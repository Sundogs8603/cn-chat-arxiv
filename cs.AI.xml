<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;Vabs-Net&#36827;&#34892;&#22810;&#32423;&#34507;&#30333;&#36136;&#39044;&#35757;&#32451;&#30340;&#26041;&#27861;&#12290;&#24403;&#21069;&#22823;&#22810;&#25968;&#22522;&#20110;&#32467;&#26500;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#20165;&#20851;&#27880;&#27531;&#22522;&#27700;&#24179;&#65292;&#20294;&#24573;&#30053;&#20102;&#20391;&#38142;&#21407;&#23376;&#30340;&#37325;&#35201;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#39044;&#35757;&#32451;&#31574;&#30053;&#65292;&#24341;&#20837;&#20102;&#36328;&#24230;&#25513;&#30721;&#65292;&#20197;&#22312;&#19977;&#32500;&#34507;&#30333;&#36136;&#39044;&#35757;&#32451;&#20013;&#21516;&#26102;&#24314;&#27169;&#27531;&#22522;&#21644;&#21407;&#23376;&#27700;&#24179;&#30340;&#20449;&#24687;&#65292;&#24182;&#25913;&#21892;&#20102;&#27531;&#22522;&#34920;&#31034;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01481</link><description>&lt;p&gt;
&#29992;Vabs-Net&#36827;&#34892;&#22810;&#32423;&#34507;&#30333;&#36136;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Multi-level protein pre-training with Vabs-Net
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01481
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;Vabs-Net&#36827;&#34892;&#22810;&#32423;&#34507;&#30333;&#36136;&#39044;&#35757;&#32451;&#30340;&#26041;&#27861;&#12290;&#24403;&#21069;&#22823;&#22810;&#25968;&#22522;&#20110;&#32467;&#26500;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#20165;&#20851;&#27880;&#27531;&#22522;&#27700;&#24179;&#65292;&#20294;&#24573;&#30053;&#20102;&#20391;&#38142;&#21407;&#23376;&#30340;&#37325;&#35201;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#39044;&#35757;&#32451;&#31574;&#30053;&#65292;&#24341;&#20837;&#20102;&#36328;&#24230;&#25513;&#30721;&#65292;&#20197;&#22312;&#19977;&#32500;&#34507;&#30333;&#36136;&#39044;&#35757;&#32451;&#20013;&#21516;&#26102;&#24314;&#27169;&#27531;&#22522;&#21644;&#21407;&#23376;&#27700;&#24179;&#30340;&#20449;&#24687;&#65292;&#24182;&#25913;&#21892;&#20102;&#27531;&#22522;&#34920;&#31034;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#65292;&#19977;&#32500;&#32467;&#26500;&#39044;&#35757;&#32451;&#34507;&#30333;&#36136;&#27169;&#22411;&#30340;&#21457;&#23637;&#36805;&#29467;&#65292;&#30456;&#36739;&#20110;&#39044;&#35757;&#32451;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#22522;&#20110;&#32467;&#26500;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#20027;&#35201;&#20851;&#27880;&#27531;&#22522;&#27700;&#24179;&#65292;&#21363;&#945;&#30899;&#21407;&#23376;&#65292;&#32780;&#24573;&#30053;&#20102;&#20854;&#20182;&#21407;&#23376;&#65292;&#22914;&#20391;&#38142;&#21407;&#23376;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#22312;&#27531;&#22522;&#21644;&#21407;&#23376;&#27700;&#24179;&#19978;&#23545;&#34507;&#30333;&#36136;&#36827;&#34892;&#24314;&#27169;&#24456;&#37325;&#35201;&#65292;&#22240;&#20026;&#20391;&#38142;&#21407;&#23376;&#23545;&#20110;&#35768;&#22810;&#19979;&#28216;&#20219;&#21153;&#65288;&#22914;&#20998;&#23376;&#23545;&#25509;&#65289;&#20063;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#39044;&#35757;&#32451;&#20013;&#22825;&#30495;&#22320;&#32452;&#21512;&#27531;&#22522;&#21644;&#21407;&#23376;&#20449;&#24687;&#36890;&#24120;&#20250;&#22833;&#36133;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20449;&#24687;&#27844;&#28431;&#26159;&#21253;&#21547;&#21407;&#23376;&#32467;&#26500;&#30340;&#36755;&#20837;&#23548;&#33268;&#27531;&#22522;&#32423;&#39044;&#35757;&#32451;&#20219;&#21153;&#21464;&#24471;&#29712;&#30862;&#24182;&#23548;&#33268;&#27531;&#22522;&#34920;&#31034;&#19981;&#22815;&#20805;&#20998;&#30340;&#19968;&#20010;&#20851;&#38190;&#21407;&#22240;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#36328;&#24230;&#25513;&#30721;&#39044;&#35757;&#32451;&#31574;&#30053;&#30340;&#19977;&#32500;&#34507;&#30333;&#36136;&#39044;&#35757;&#32451;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, there has been a surge in the development of 3D structure-based pre-trained protein models, representing a significant advancement over pre-trained protein language models in various downstream tasks. However, most existing structure-based pre-trained models primarily focus on the residue level, i.e., alpha carbon atoms, while ignoring other atoms like side chain atoms. We argue that modeling proteins at both residue and atom levels is important since the side chain atoms can also be crucial for numerous downstream tasks, for example, molecular docking. Nevertheless, we find that naively combining residue and atom information during pre-training typically fails. We identify a key reason is the information leakage caused by the inclusion of atom structure in the input, which renders residue-level pre-training tasks trivial and results in insufficiently expressive residue representations. To address this issue, we introduce a span mask pre-training strategy on 3D protein
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Diffusion Meets DAgger (DMD)&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;eye-in-hand&#27169;&#20223;&#23398;&#20064;&#65292;&#36890;&#36807;&#25193;&#25955;&#27169;&#22411;&#21019;&#24314;&#26032;&#26679;&#26412;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#34920;&#29616;&#65292;&#24182;&#20943;&#23569;&#25104;&#26412;&#12290;</title><link>https://arxiv.org/abs/2402.17768</link><description>&lt;p&gt;
&#25193;&#25955;&#36935;&#35265;DAgger: &#36229;&#32423;&#30524;&#22312;&#25163;&#27169;&#20223;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Diffusion Meets DAgger: Supercharging Eye-in-hand Imitation Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17768
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Diffusion Meets DAgger (DMD)&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;eye-in-hand&#27169;&#20223;&#23398;&#20064;&#65292;&#36890;&#36807;&#25193;&#25955;&#27169;&#22411;&#21019;&#24314;&#26032;&#26679;&#26412;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#34920;&#29616;&#65292;&#24182;&#20943;&#23569;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;Diffusion Meets DAgger (DMD)&#65292;&#29992;&#20110;eye-in-hand&#27169;&#20223;&#23398;&#20064;&#38382;&#39064;&#65292;&#36890;&#36807;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#21019;&#24314;&#26032;&#26679;&#26412;&#65292;&#20351;&#24471;&#23398;&#20064;&#31574;&#30053;&#22312;&#36935;&#21040;&#26410;&#20986;&#29616;&#22312;&#19987;&#23478;&#28436;&#31034;&#20013;&#30340;&#29366;&#24577;&#26102;&#20855;&#26377;&#40065;&#26834;&#24615;&#34920;&#29616;&#65292;&#20943;&#23569;&#20102;&#25968;&#25454;&#37319;&#38598;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17768v1 Announce Type: cross  Abstract: A common failure mode for policies trained with imitation is compounding execution errors at test time. When the learned policy encounters states that were not present in the expert demonstrations, the policy fails, leading to degenerate behavior. The Dataset Aggregation, or DAgger approach to this problem simply collects more data to cover these failure states. However, in practice, this is often prohibitively expensive. In this work, we propose Diffusion Meets DAgger (DMD), a method to reap the benefits of DAgger without the cost for eye-in-hand imitation learning problems. Instead of collecting new samples to cover out-of-distribution states, DMD uses recent advances in diffusion models to create these samples with diffusion models. This leads to robust performance from few demonstrations. In experiments conducted for non-prehensile pushing on a Franka Research 3, we show that DMD can achieve a success rate of 80% with as few as 8 e
&lt;/p&gt;</description></item><item><title>&#23454;&#29616;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#31995;&#32479;&#65292;&#20351;&#21830;&#21697;&#31227;&#21160;&#25805;&#20316;&#22120;&#25104;&#21151;&#22312;&#20197;&#21069;&#26410;&#35265;&#30340;&#30495;&#23454;&#19990;&#30028;&#29615;&#22659;&#20013;&#25171;&#24320;&#27249;&#26588;&#21644;&#25277;&#23625;&#65292;&#24863;&#30693;&#35823;&#24046;&#26159;&#20027;&#35201;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.17767</link><description>&lt;p&gt;
&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#20351;&#29992;&#21830;&#21697;&#31227;&#21160;&#25805;&#20316;&#22120;&#25171;&#24320;&#27249;&#26588;&#21644;&#25277;&#23625;
&lt;/p&gt;
&lt;p&gt;
Opening Cabinets and Drawers in the Real World using a Commodity Mobile Manipulator
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17767
&lt;/p&gt;
&lt;p&gt;
&#23454;&#29616;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#31995;&#32479;&#65292;&#20351;&#21830;&#21697;&#31227;&#21160;&#25805;&#20316;&#22120;&#25104;&#21151;&#22312;&#20197;&#21069;&#26410;&#35265;&#30340;&#30495;&#23454;&#19990;&#30028;&#29615;&#22659;&#20013;&#25171;&#24320;&#27249;&#26588;&#21644;&#25277;&#23625;&#65292;&#24863;&#30693;&#35823;&#24046;&#26159;&#20027;&#35201;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#31995;&#32479;&#65292;&#20351;&#21830;&#21697;&#31227;&#21160;&#25805;&#20316;&#22120;&#65288;Stretch RE2&#65289;&#33021;&#22815;&#22312;&#22810;&#26679;&#30340;&#20197;&#21069;&#26410;&#35265;&#30340;&#30495;&#23454;&#19990;&#30028;&#29615;&#22659;&#20013;&#25289;&#24320;&#27249;&#26588;&#21644;&#25277;&#23625;&#12290;&#25105;&#20204;&#22312;31&#20010;&#19981;&#21516;&#30340;&#29289;&#20307;&#21644;13&#20010;&#19981;&#21516;&#30495;&#23454;&#19990;&#30028;&#29615;&#22659;&#20013;&#36827;&#34892;&#20102;4&#22825;&#30340;&#23454;&#38469;&#27979;&#35797;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#22312;&#38646;&#20987;&#25171;&#19979;&#65292;&#23545;&#22312;&#26410;&#30693;&#29615;&#22659;&#20013;&#26032;&#39062;&#30340;&#27249;&#26588;&#21644;&#25277;&#23625;&#30340;&#25171;&#24320;&#29575;&#36798;&#21040;61%&#12290;&#23545;&#22833;&#36133;&#27169;&#24335;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#24863;&#30693;&#35823;&#24046;&#26159;&#25105;&#20204;&#31995;&#32479;&#38754;&#20020;&#30340;&#26368;&#37325;&#35201;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17767v1 Announce Type: cross  Abstract: Pulling open cabinets and drawers presents many difficult technical challenges in perception (inferring articulation parameters for objects from onboard sensors), planning (producing motion plans that conform to tight task constraints), and control (making and maintaining contact while applying forces on the environment). In this work, we build an end-to-end system that enables a commodity mobile manipulator (Stretch RE2) to pull open cabinets and drawers in diverse previously unseen real world environments. We conduct 4 days of real world testing of this system spanning 31 different objects from across 13 different real world environments. Our system achieves a success rate of 61% on opening novel cabinets and drawers in unseen environments zero-shot. An analysis of the failure modes suggests that errors in perception are the most significant challenge for our system. We will open source code and models for others to replicate and bui
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#37327;&#23376;&#24555;&#36895;&#26435;&#37325;&#32534;&#31243;&#22120;&#65288;QFWP&#65289;&#20316;&#20026;&#35299;&#20915;&#37327;&#23376;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;QRNNs&#65289;&#27169;&#22411;&#35757;&#32451;&#26102;&#38388;&#24310;&#38271;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2402.17760</link><description>&lt;p&gt;
&#23398;&#20064;&#20351;&#29992;&#24555;&#36895;&#26435;&#37325;&#32534;&#31243;&#21464;&#20998;&#37327;&#23376;&#30005;&#36335;
&lt;/p&gt;
&lt;p&gt;
Learning to Program Variational Quantum Circuits with Fast Weights
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17760
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#37327;&#23376;&#24555;&#36895;&#26435;&#37325;&#32534;&#31243;&#22120;&#65288;QFWP&#65289;&#20316;&#20026;&#35299;&#20915;&#37327;&#23376;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;QRNNs&#65289;&#27169;&#22411;&#35757;&#32451;&#26102;&#38388;&#24310;&#38271;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Quantum Machine Learning (QML)&#24050;&#32463;&#25104;&#20026;&#19968;&#20010;&#20027;&#35201;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#39034;&#24207;&#25511;&#21046;&#20219;&#21153;&#21644;&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;&#12290; &#29305;&#21035;&#26159;&#22312;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#21644;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#31561;&#39046;&#22495;&#65292;&#24050;&#32463;&#23637;&#31034;&#20102;&#32463;&#39564;&#37327;&#23376;&#20248;&#21183;&#12290; &#37327;&#23376;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;QRNNs&#65289;&#26159;&#19968;&#20010;&#37325;&#22823;&#36827;&#23637;&#65292;&#19987;&#38376;&#20026;&#23384;&#20648;&#23494;&#38598;&#22411;&#20219;&#21153;&#35774;&#35745;&#65292;&#21253;&#25324;&#37096;&#20998;&#21487;&#35266;&#27979;&#29615;&#22659;&#21644;&#38750;&#32447;&#24615;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#12290; &#28982;&#32780;&#65292;&#22522;&#20110;QRNN&#30340;&#27169;&#22411;&#38754;&#20020;&#25361;&#25112;&#65292;&#23588;&#20854;&#26159;&#30001;&#20110;&#38656;&#35201;&#20351;&#29992;&#36890;&#36807;&#26102;&#38388;&#21453;&#21521;&#20256;&#25773;&#65288;BPTT&#65289;&#35745;&#31639;&#37327;&#23376;&#26799;&#24230;&#32780;&#20135;&#29983;&#30340;&#35757;&#32451;&#26102;&#38388;&#24310;&#38271;&#30340;&#38382;&#39064;&#12290; &#24403;&#22312;&#37327;&#23376;&#35774;&#22791;&#19978;&#25191;&#34892;&#23436;&#25972;&#27169;&#22411;&#26102;&#65292;&#30001;&#20110;&#21442;&#25968;&#31227;&#20301;&#35268;&#21017;&#24102;&#26469;&#30340;&#30005;&#36335;&#35780;&#20272;&#38656;&#27714;&#24040;&#22823;&#65292;&#36825;&#20010;&#22256;&#22659;&#36827;&#19968;&#27493;&#21152;&#21095;&#12290; &#26412;&#25991;&#23558;&#37327;&#23376;&#24555;&#36895;&#26435;&#37325;&#31243;&#24207;&#21592;&#65288;QFWP&#65289;&#24341;&#20837;&#20316;&#20026;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17760v1 Announce Type: cross  Abstract: Quantum Machine Learning (QML) has surfaced as a pioneering framework addressing sequential control tasks and time-series modeling. It has demonstrated empirical quantum advantages notably within domains such as Reinforcement Learning (RL) and time-series prediction. A significant advancement lies in Quantum Recurrent Neural Networks (QRNNs), specifically tailored for memory-intensive tasks encompassing partially observable environments and non-linear time-series prediction. Nevertheless, QRNN-based models encounter challenges, notably prolonged training duration stemming from the necessity to compute quantum gradients using backpropagation-through-time (BPTT). This predicament exacerbates when executing the complete model on quantum devices, primarily due to the substantial demand for circuit evaluation arising from the parameter-shift rule. This paper introduces the Quantum Fast Weight Programmers (QFWP) as a solution to the temporal
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#26426;&#22120;-&#20154;&#27969;&#31243;&#65292;&#22522;&#20110;LLM&#20195;&#29702;&#26550;&#26500;&#24182;&#23558;&#20854;&#23545;&#35805;&#22522;&#20110;&#20154;&#29289;&#35282;&#33394;&#21644;&#26102;&#38388;&#20107;&#20214;&#22270;&#36827;&#34892;&#22522;&#30784;&#65292;&#25104;&#21151;&#21019;&#24314;&#20102;LoCoMo&#25968;&#25454;&#38598;&#65292;&#20026;&#38750;&#24120;&#38271;&#26399;&#23545;&#35805;&#30340;&#30740;&#31350;&#22635;&#34917;&#20102;&#31354;&#30333;&#12290;</title><link>https://arxiv.org/abs/2402.17753</link><description>&lt;p&gt;
&#35780;&#20272;LLM&#20195;&#29702;&#30340;&#38750;&#24120;&#38271;&#26399;&#23545;&#35805;&#35760;&#24518;
&lt;/p&gt;
&lt;p&gt;
Evaluating Very Long-Term Conversational Memory of LLM Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17753
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#26426;&#22120;-&#20154;&#27969;&#31243;&#65292;&#22522;&#20110;LLM&#20195;&#29702;&#26550;&#26500;&#24182;&#23558;&#20854;&#23545;&#35805;&#22522;&#20110;&#20154;&#29289;&#35282;&#33394;&#21644;&#26102;&#38388;&#20107;&#20214;&#22270;&#36827;&#34892;&#22522;&#30784;&#65292;&#25104;&#21151;&#21019;&#24314;&#20102;LoCoMo&#25968;&#25454;&#38598;&#65292;&#20026;&#38750;&#24120;&#38271;&#26399;&#23545;&#35805;&#30340;&#30740;&#31350;&#22635;&#34917;&#20102;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#26399;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#26041;&#38754;&#30340;&#29616;&#26377;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#35780;&#20272;&#27169;&#22411;&#21709;&#24212;&#65292;&#20854;&#19978;&#19979;&#25991;&#36328;&#24230;&#19981;&#36229;&#36807;&#20116;&#20010;&#32842;&#22825;&#20250;&#35805;&#12290;&#23613;&#31649;&#38271;&#19978;&#19979;&#25991;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21644;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#25216;&#26415;&#26377;&#25152;&#36827;&#23637;&#65292;&#20294;&#23427;&#20204;&#22312;&#38750;&#24120;&#38271;&#26399;&#23545;&#35805;&#20013;&#30340;&#26377;&#25928;&#24615;&#23578;&#26410;&#34987;&#25506;&#32034;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#30740;&#31350;&#31354;&#30333;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26426;&#22120;-&#20154;&#30340;&#27969;&#31243;&#65292;&#36890;&#36807;&#21033;&#29992;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#26550;&#26500;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#38750;&#24120;&#38271;&#26399;&#23545;&#35805;&#65292;&#24182;&#23558;&#20854;&#23545;&#35805;&#22522;&#20110;&#20154;&#29289;&#35282;&#33394;&#21644;&#26102;&#38388;&#20107;&#20214;&#22270;&#36827;&#34892;&#22522;&#30784;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36171;&#20104;&#27599;&#20010;&#20195;&#29702;&#20998;&#20139;&#21644;&#23545;&#22270;&#20687;&#20570;&#20986;&#21453;&#24212;&#30340;&#33021;&#21147;&#12290;&#29983;&#25104;&#30340;&#23545;&#35805;&#32463;&#20154;&#31867;&#27880;&#37322;&#21592;&#39564;&#35777;&#21644;&#32534;&#36753;&#65292;&#20197;&#30830;&#20445;&#38271;&#26399;&#19968;&#33268;&#24615;&#21644;&#19982;&#20107;&#20214;&#22270;&#30340;&#22522;&#30784;&#30456;&#32852;&#31995;&#12290;&#20351;&#29992;&#27492;&#27969;&#31243;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;LoCoMo&#65292;&#19968;&#20010;&#38750;&#24120;&#38271;&#26399;&#23545;&#35805;&#30340;&#25968;&#25454;&#38598;&#65292;&#27599;&#20010;&#25968;&#25454;&#38598;&#21253;&#21547;300&#36718;&#21644;&#24179;&#22343;9K&#20196;&#29260;&#65292;&#26368;&#22810;&#36798;&#21040;35&#20010;&#20250;&#35805;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17753v1 Announce Type: cross  Abstract: Existing works on long-term open-domain dialogues focus on evaluating model responses within contexts spanning no more than five chat sessions. Despite advancements in long-context large language models (LLMs) and retrieval augmented generation (RAG) techniques, their efficacy in very long-term dialogues remains unexplored. To address this research gap, we introduce a machine-human pipeline to generate high-quality, very long-term dialogues by leveraging LLM-based agent architectures and grounding their dialogues on personas and temporal event graphs. Moreover, we equip each agent with the capability of sharing and reacting to images. The generated conversations are verified and edited by human annotators for long-range consistency and grounding to the event graphs. Using this pipeline, we collect LoCoMo, a dataset of very long-term conversations, each encompassing 300 turns and 9K tokens on avg., over up to 35 sessions. Based on LoCoM
&lt;/p&gt;</description></item><item><title>RLHF&#22312;&#32771;&#34385;&#37096;&#20998;&#35266;&#23519;&#24615;&#26102;&#21487;&#33021;&#23548;&#33268;&#31574;&#30053;&#27450;&#39575;&#24615;&#22320;&#22840;&#22823;&#24615;&#33021;&#25110;&#36807;&#24230;&#36777;&#25252;&#34892;&#20026;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25968;&#23398;&#26465;&#20214;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#24182;&#35686;&#21578;&#19981;&#35201;&#30450;&#30446;&#24212;&#29992;RLHF&#22312;&#37096;&#20998;&#21487;&#35266;&#27979;&#24773;&#20917;&#19979;&#12290;</title><link>https://arxiv.org/abs/2402.17747</link><description>&lt;p&gt;
&#24403;&#20320;&#30340;AI&#27450;&#39575;&#20320;&#65306;&#22312;&#22870;&#21169;&#23398;&#20064;&#20013;&#20154;&#31867;&#35780;&#20272;&#32773;&#37096;&#20998;&#21487;&#35266;&#27979;&#24615;&#30340;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
When Your AI Deceives You: Challenges with Partial Observability of Human Evaluators in Reward Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17747
&lt;/p&gt;
&lt;p&gt;
RLHF&#22312;&#32771;&#34385;&#37096;&#20998;&#35266;&#23519;&#24615;&#26102;&#21487;&#33021;&#23548;&#33268;&#31574;&#30053;&#27450;&#39575;&#24615;&#22320;&#22840;&#22823;&#24615;&#33021;&#25110;&#36807;&#24230;&#36777;&#25252;&#34892;&#20026;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25968;&#23398;&#26465;&#20214;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#24182;&#35686;&#21578;&#19981;&#35201;&#30450;&#30446;&#24212;&#29992;RLHF&#22312;&#37096;&#20998;&#21487;&#35266;&#27979;&#24773;&#20917;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#65288;RLHF&#65289;&#30340;&#36807;&#21435;&#20998;&#26512;&#20551;&#35774;&#20154;&#31867;&#23436;&#20840;&#35266;&#23519;&#21040;&#29615;&#22659;&#12290;&#24403;&#20154;&#31867;&#21453;&#39304;&#20165;&#22522;&#20110;&#37096;&#20998;&#35266;&#23519;&#26102;&#20250;&#21457;&#29983;&#20160;&#20040;&#65311;&#25105;&#20204;&#23545;&#20004;&#31181;&#22833;&#36133;&#24773;&#20917;&#36827;&#34892;&#20102;&#27491;&#24335;&#23450;&#20041;&#65306;&#27450;&#39575;&#21644;&#36807;&#24230;&#36777;&#25252;&#12290;&#36890;&#36807;&#23558;&#20154;&#31867;&#24314;&#27169;&#20026;&#23545;&#36712;&#36857;&#20449;&#24565;&#30340;Boltzmann-&#29702;&#24615;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;RLHF&#20445;&#35777;&#20250;&#23548;&#33268;&#31574;&#30053;&#27450;&#39575;&#24615;&#22320;&#22840;&#22823;&#20854;&#24615;&#33021;&#12289;&#20026;&#20102;&#30041;&#19979;&#21360;&#35937;&#32780;&#36807;&#24230;&#36777;&#25252;&#25110;&#32773;&#20004;&#32773;&#20860;&#32780;&#26377;&#20043;&#30340;&#26465;&#20214;&#12290;&#20026;&#20102;&#24110;&#21161;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25968;&#23398;&#22320;&#21051;&#30011;&#20102;&#29615;&#22659;&#37096;&#20998;&#21487;&#35266;&#27979;&#24615;&#22914;&#20309;&#36716;&#21270;&#20026;&#65288;&#32570;&#20047;&#65289;&#23398;&#21040;&#30340;&#22238;&#25253;&#20989;&#25968;&#20013;&#30340;&#27169;&#31946;&#24615;&#12290;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#32771;&#34385;&#29615;&#22659;&#37096;&#20998;&#21487;&#35266;&#27979;&#24615;&#20351;&#24471;&#22312;&#29702;&#35770;&#19978;&#21487;&#33021;&#24674;&#22797;&#22238;&#25253;&#20989;&#25968;&#21644;&#26368;&#20248;&#31574;&#30053;&#65292;&#32780;&#22312;&#20854;&#20182;&#24773;&#20917;&#19979;&#65292;&#23384;&#22312;&#19981;&#21487;&#20943;&#23569;&#30340;&#27169;&#31946;&#24615;&#12290;&#25105;&#20204;&#35686;&#21578;&#19981;&#35201;&#30450;&#30446;&#24212;&#29992;RLHF&#22312;&#37096;&#20998;&#21487;&#35266;&#27979;&#24773;&#20917;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17747v1 Announce Type: cross  Abstract: Past analyses of reinforcement learning from human feedback (RLHF) assume that the human fully observes the environment. What happens when human feedback is based only on partial observations? We formally define two failure cases: deception and overjustification. Modeling the human as Boltzmann-rational w.r.t. a belief over trajectories, we prove conditions under which RLHF is guaranteed to result in policies that deceptively inflate their performance, overjustify their behavior to make an impression, or both. To help address these issues, we mathematically characterize how partial observability of the environment translates into (lack of) ambiguity in the learned return function. In some cases, accounting for partial observability makes it theoretically possible to recover the return function and thus the optimal policy, while in other cases, there is irreducible ambiguity. We caution against blindly applying RLHF in partially observa
&lt;/p&gt;</description></item><item><title>reBandit&#26159;&#19968;&#31181;&#22312;&#32447;RL&#31639;&#27861;&#65292;&#21033;&#29992;&#38543;&#26426;&#25928;&#24212;&#21644;&#36125;&#21494;&#26031;&#20808;&#39564;&#24555;&#36895;&#39640;&#25928;&#22320;&#23398;&#20064;&#65292;&#22312;&#31227;&#21160;&#20581;&#24247;&#29615;&#22659;&#20013;&#36890;&#36807;&#20010;&#24615;&#21270;&#24178;&#39044;&#26469;&#20943;&#23569;&#26032;&#20852;&#25104;&#24180;&#20154;&#30340;&#22823;&#40635;&#20351;&#29992;</title><link>https://arxiv.org/abs/2402.17739</link><description>&lt;p&gt;
reBandit&#65306;&#22522;&#20110;&#38543;&#26426;&#25928;&#24212;&#30340;&#22312;&#32447;RL&#31639;&#27861;&#29992;&#20110;&#20943;&#23569;&#22823;&#40635;&#20351;&#29992;
&lt;/p&gt;
&lt;p&gt;
reBandit: Random Effects based Online RL algorithm for Reducing Cannabis Use
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17739
&lt;/p&gt;
&lt;p&gt;
reBandit&#26159;&#19968;&#31181;&#22312;&#32447;RL&#31639;&#27861;&#65292;&#21033;&#29992;&#38543;&#26426;&#25928;&#24212;&#21644;&#36125;&#21494;&#26031;&#20808;&#39564;&#24555;&#36895;&#39640;&#25928;&#22320;&#23398;&#20064;&#65292;&#22312;&#31227;&#21160;&#20581;&#24247;&#29615;&#22659;&#20013;&#36890;&#36807;&#20010;&#24615;&#21270;&#24178;&#39044;&#26469;&#20943;&#23569;&#26032;&#20852;&#25104;&#24180;&#20154;&#30340;&#22823;&#40635;&#20351;&#29992;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#40635;&#20351;&#29992;&#21450;&#30456;&#20851;&#30340;&#22823;&#40635;&#20351;&#29992;&#38556;&#30861;&#65288;CUD&#65289;&#30340;&#19981;&#26029;&#22686;&#21152;&#22312;&#20840;&#29699;&#33539;&#22260;&#20869;&#26500;&#25104;&#20102;&#19968;&#20010;&#37325;&#22823;&#30340;&#20844;&#20849;&#21355;&#29983;&#25361;&#25112;&#12290;&#23588;&#20854;&#26159;&#22312;&#26032;&#20852;&#25104;&#24180;&#20154;&#65288;18-25&#23681;&#65289;&#20013;&#65292;&#23384;&#22312;&#26126;&#26174;&#30340;&#27835;&#30103;&#32570;&#21475;&#65292;&#22240;&#27492;&#35299;&#20915;&#22823;&#40635;&#20351;&#29992;&#21644;CUD&#20173;&#28982;&#26159;2030&#24180;&#32852;&#21512;&#22269;&#21487;&#25345;&#32493;&#21457;&#23637;&#30446;&#26631;&#65288;SDG&#65289;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#30446;&#26631;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#21517;&#20026;reBandit&#30340;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#31639;&#27861;&#65292;&#23558;&#20854;&#24212;&#29992;&#20110;&#31227;&#21160;&#20581;&#24247;&#30740;&#31350;&#20013;&#65292;&#26088;&#22312;&#36890;&#36807;&#25552;&#20379;&#20010;&#24615;&#21270;&#31227;&#21160;&#20581;&#24247;&#24178;&#39044;&#26469;&#20943;&#23569;&#26032;&#20852;&#25104;&#24180;&#20154;&#30340;&#22823;&#40635;&#20351;&#29992;&#12290;reBandit&#21033;&#29992;&#38543;&#26426;&#25928;&#24212;&#21644;&#20449;&#24687;&#20016;&#23500;&#30340;&#36125;&#21494;&#26031;&#20808;&#39564;&#20197;&#22312;&#22024;&#26434;&#30340;&#31227;&#21160;&#20581;&#24247;&#29615;&#22659;&#20013;&#24555;&#36895;&#32780;&#26377;&#25928;&#22320;&#23398;&#20064;&#12290;&#27492;&#22806;&#65292;reBandit&#37319;&#29992;&#32463;&#39564;&#36125;&#21494;&#26031;&#21644;&#20248;&#21270;&#25216;&#26415;&#26469;&#22312;&#32447;&#33258;&#20027;&#26356;&#26032;&#20854;&#36229;&#21442;&#25968;&#12290;&#20026;&#20102;&#35780;&#20272;&#25105;&#20204;&#31639;&#27861;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#21033;&#29992;&#25968;&#25454;&#26500;&#24314;&#20102;&#19968;&#20010;&#27169;&#25311;&#27979;&#35797;&#24179;&#21488;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17739v1 Announce Type: new  Abstract: The escalating prevalence of cannabis use, and associated cannabis-use disorder (CUD), poses a significant public health challenge globally. With a notably wide treatment gap, especially among emerging adults (EAs; ages 18-25), addressing cannabis use and CUD remains a pivotal objective within the 2030 United Nations Agenda for Sustainable Development Goals (SDG). In this work, we develop an online reinforcement learning (RL) algorithm called reBandit which will be utilized in a mobile health study to deliver personalized mobile health interventions aimed at reducing cannabis use among EAs. reBandit utilizes random effects and informative Bayesian priors to learn quickly and efficiently in noisy mobile health environments. Moreover, reBandit employs Empirical Bayes and optimization techniques to autonomously update its hyper-parameters online. To evaluate the performance of our algorithm, we construct a simulation testbed using data from
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#38024;&#23545;&#26410;&#30693;&#22270;&#30340;&#22270;&#25628;&#32034;&#38382;&#39064;&#30340;&#22522;&#20110;&#23398;&#20064;&#30340;&#31639;&#27861;&#65292;&#39318;&#27425;&#22312;&#26410;&#30693;&#21152;&#26435;&#22270;&#19978;&#24314;&#31435;&#20102;&#24418;&#24335;&#20445;&#35777;&#65292;&#24182;&#35774;&#35745;&#31639;&#27861;&#22312;&#39044;&#27979;&#35823;&#24046;&#19978;&#20855;&#26377;&#26368;&#20248;&#25110;&#20960;&#20046;&#26368;&#20339;&#20381;&#23384;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2402.17736</link><description>&lt;p&gt;
&#22522;&#20110;&#23398;&#20064;&#30340;&#22270;&#25628;&#32034;&#38382;&#39064;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Learning-Based Algorithms for Graph Searching Problems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17736
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#38024;&#23545;&#26410;&#30693;&#22270;&#30340;&#22270;&#25628;&#32034;&#38382;&#39064;&#30340;&#22522;&#20110;&#23398;&#20064;&#30340;&#31639;&#27861;&#65292;&#39318;&#27425;&#22312;&#26410;&#30693;&#21152;&#26435;&#22270;&#19978;&#24314;&#31435;&#20102;&#24418;&#24335;&#20445;&#35777;&#65292;&#24182;&#35774;&#35745;&#31639;&#27861;&#22312;&#39044;&#27979;&#35823;&#24046;&#19978;&#20855;&#26377;&#26368;&#20248;&#25110;&#20960;&#20046;&#26368;&#20339;&#20381;&#23384;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20102;Banerjee&#31561;&#20154;&#65288;2022&#24180;&#65289;&#26368;&#36817;&#25552;&#20986;&#30340;&#20855;&#26377;&#39044;&#27979;&#30340;&#22270;&#25628;&#32034;&#38382;&#39064;&#12290;&#22312;&#36825;&#20010;&#38382;&#39064;&#20013;&#65292;&#19968;&#20010;&#20174;&#26576;&#20010;&#39030;&#28857;$r$&#20986;&#21457;&#30340;&#20195;&#29702;&#32773;&#24517;&#39035;&#22312;&#26368;&#23567;&#21270;&#24635;&#34892;&#31243;&#30340;&#21516;&#26102;&#36941;&#21382;&#19968;&#20010;&#65288;&#28508;&#22312;&#26410;&#30693;&#30340;&#65289;&#22270;$G$&#20197;&#25214;&#21040;&#38544;&#34255;&#30340;&#30446;&#26631;&#33410;&#28857;$g$&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#35774;&#32622;&#65292;&#22312;&#36825;&#31181;&#35774;&#32622;&#20013;&#65292;&#22312;&#20219;&#24847;&#33410;&#28857;$v$&#22788;&#65292;&#20195;&#29702;&#32773;&#20250;&#25509;&#25910;&#21040;&#21040;$g$&#30340;&#36317;&#31163;&#30340;&#22122;&#22768;&#20272;&#35745;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#38024;&#23545;&#26410;&#30693;&#22270;&#30340;&#36825;&#31181;&#25628;&#32034;&#20219;&#21153;&#30340;&#31639;&#27861;&#12290;&#25105;&#20204;&#22312;&#26410;&#30693;&#21152;&#26435;&#22270;&#19978;&#24314;&#31435;&#20102;&#31532;&#19968;&#27425;&#24418;&#24335;&#20445;&#35777;&#65292;&#24182;&#25552;&#20379;&#20102;&#26174;&#31034;&#25105;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;&#22312;&#39044;&#27979;&#35823;&#24046;&#19978;&#20855;&#26377;&#26368;&#20248;&#25110;&#20960;&#20046;&#26368;&#20339;&#20381;&#23384;&#20851;&#31995;&#30340;&#19979;&#30028;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#25968;&#20540;&#23454;&#39564;&#65292;&#35777;&#26126;&#25105;&#20204;&#30340;&#31639;&#27861;&#38500;&#20102;&#23545;&#25239;&#24615;&#35823;&#24046;&#20855;&#26377;&#40065;&#26834;&#24615;&#22806;&#65292;&#36824;&#22312;&#35823;&#24046;&#26159;&#38543;&#26426;&#30340;&#20856;&#22411;&#23454;&#20363;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#23545;Banerjee&#31561;&#20154;&#31639;&#27861;&#30340;&#26367;&#20195;&#31616;&#21270;&#24615;&#33021;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17736v1 Announce Type: cross  Abstract: We consider the problem of graph searching with prediction recently introduced by Banerjee et al. (2022). In this problem, an agent, starting at some vertex $r$ has to traverse a (potentially unknown) graph $G$ to find a hidden goal node $g$ while minimizing the total distance travelled. We study a setting in which at any node $v$, the agent receives a noisy estimate of the distance from $v$ to $g$. We design algorithms for this search task on unknown graphs. We establish the first formal guarantees on unknown weighted graphs and provide lower bounds showing that the algorithms we propose have optimal or nearly-optimal dependence on the prediction error. Further, we perform numerical experiments demonstrating that in addition to being robust to adversarial error, our algorithms perform well in typical instances in which the error is stochastic. Finally, we provide alternative simpler performance bounds on the algorithms of Banerjee et 
&lt;/p&gt;</description></item><item><title>transformers&#22312;&#25968;&#23398;&#38382;&#39064;&#20013;&#37319;&#29992;&#22522;&#20110;&#26696;&#20363;&#30340;&#25512;&#29702;&#32780;&#38750;&#22522;&#20110;&#35268;&#21017;&#30340;&#25512;&#29702;&#12290;</title><link>https://arxiv.org/abs/2402.17709</link><description>&lt;p&gt;
&#22522;&#20110;&#26696;&#20363;&#36824;&#26159;&#22522;&#20110;&#35268;&#21017;&#65306;&#21464;&#21387;&#22120;&#22914;&#20309;&#36827;&#34892;&#25968;&#23398;&#35745;&#31639;&#65311;
&lt;/p&gt;
&lt;p&gt;
Case-Based or Rule-Based: How Do Transformers Do the Math?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17709
&lt;/p&gt;
&lt;p&gt;
transformers&#22312;&#25968;&#23398;&#38382;&#39064;&#20013;&#37319;&#29992;&#22522;&#20110;&#26696;&#20363;&#30340;&#25512;&#29702;&#32780;&#38750;&#22522;&#20110;&#35268;&#21017;&#30340;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#29616;&#20195;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#22797;&#26434;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#20173;&#28982;&#38590;&#20197;&#22788;&#29702;&#19968;&#20123;&#23545;&#20154;&#31867;&#26469;&#35828;&#31616;&#21333;&#19988;&#30452;&#35266;&#30340;&#25968;&#23398;&#38382;&#39064;&#65292;&#20363;&#22914;&#21152;&#27861;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21487;&#20197;&#36731;&#26494;&#23398;&#20064;&#21152;&#27861;&#30340;&#22522;&#26412;&#35268;&#21017;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#20219;&#24847;&#38271;&#24230;&#30340;&#26032;&#38382;&#39064;&#65292;&#32780;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21364;&#38590;&#20197;&#20570;&#21040;&#12290;&#30456;&#21453;&#65292;&#23427;&#20204;&#21487;&#33021;&#20381;&#36182;&#20110;&#22312;&#35757;&#32451;&#35821;&#26009;&#24211;&#20013;&#30475;&#21040;&#30340;&#31867;&#20284;&#8220;&#26696;&#20363;&#8221;&#26469;&#33719;&#21462;&#24110;&#21161;&#12290;&#25105;&#20204;&#23558;&#36825;&#20004;&#31181;&#19981;&#21516;&#30340;&#25512;&#29702;&#26426;&#21046;&#23450;&#20041;&#20026;&#8220;&#22522;&#20110;&#35268;&#21017;&#30340;&#25512;&#29702;&#8221;&#21644;&#8220;&#22522;&#20110;&#26696;&#20363;&#30340;&#25512;&#29702;&#8221;&#12290;&#30001;&#20110;&#22522;&#20110;&#35268;&#21017;&#30340;&#25512;&#29702;&#23545;&#20110;&#33719;&#24471;&#31995;&#32479;&#21270;&#27010;&#25324;&#33021;&#21147;&#33267;&#20851;&#37325;&#35201;&#65292;&#25105;&#20204;&#26088;&#22312;&#25506;&#31350;&#21464;&#21387;&#22120;&#31350;&#31455;&#26159;&#20351;&#29992;&#22522;&#20110;&#35268;&#21017;&#36824;&#26159;&#22522;&#20110;&#26696;&#20363;&#30340;&#25512;&#29702;&#26469;&#35299;&#20915;&#25968;&#23398;&#38382;&#39064;&#12290;&#36890;&#36807;&#31934;&#24515;&#35774;&#35745;&#30340;&#20116;&#20010;&#25968;&#23398;&#20219;&#21153;&#30340;&#24178;&#39044;&#23454;&#39564;&#65292;&#25105;&#20204;&#30830;&#35748;&#21464;&#21387;&#22120;&#27491;&#22312;&#25191;&#34892;&#22522;&#20110;&#26696;&#20363;&#30340;&#25512;&#29702;&#65292;&#26080;&#35770;&#26159;&#21542;&#20351;&#29992;&#33609;&#31295;&#26412;&#65292;&#36825;&#19982;&#20043;&#21069;&#30340;&#35266;&#23519;&#32467;&#26524;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17709v1 Announce Type: new  Abstract: Despite the impressive performance in a variety of complex tasks, modern large language models (LLMs) still have trouble dealing with some math problems that are simple and intuitive for humans, such as addition. While we can easily learn basic rules of addition and apply them to new problems of any length, LLMs struggle to do the same. Instead, they may rely on similar "cases" seen in the training corpus for help. We define these two different reasoning mechanisms as "rule-based reasoning" and "case-based reasoning". Since rule-based reasoning is essential for acquiring the systematic generalization ability, we aim to explore exactly whether transformers use rule-based or case-based reasoning for math problems. Through carefully designed intervention experiments on five math tasks, we confirm that transformers are performing case-based reasoning, no matter whether scratchpad is used, which aligns with the previous observations that tran
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20840;&#38754;&#25506;&#35752;&#20102;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#20013;AI&#30340;&#28436;&#36827;&#36712;&#36857;&#65292;&#20174;&#22522;&#30784;&#21407;&#29702;&#36861;&#28335;&#21040;&#26368;&#26032;&#36827;&#23637;&#65292;&#24182;&#38416;&#26126;&#20102;AI&#22312;&#22609;&#36896;&#36710;&#36742;&#33258;&#20027;&#20915;&#31574;&#33021;&#21147;&#20013;&#30340;&#22522;&#30784;&#20316;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.17690</link><description>&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#65306;&#20154;&#24037;&#26234;&#33021;&#21644;&#23398;&#20064;&#31639;&#27861;&#30340;&#28436;&#36827;
&lt;/p&gt;
&lt;p&gt;
Autonomous Vehicles: Evolution of Artificial Intelligence and Learning Algorithms
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17690
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20840;&#38754;&#25506;&#35752;&#20102;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#20013;AI&#30340;&#28436;&#36827;&#36712;&#36857;&#65292;&#20174;&#22522;&#30784;&#21407;&#29702;&#36861;&#28335;&#21040;&#26368;&#26032;&#36827;&#23637;&#65292;&#24182;&#38416;&#26126;&#20102;AI&#22312;&#22609;&#36896;&#36710;&#36742;&#33258;&#20027;&#20915;&#31574;&#33021;&#21147;&#20013;&#30340;&#22522;&#30784;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#30340;&#20986;&#29616;&#26631;&#24535;&#30528;&#20132;&#36890;&#36816;&#36755;&#39046;&#22495;&#36814;&#26469;&#20102;&#19968;&#20010;&#21464;&#38761;&#26102;&#20195;&#65292;&#36890;&#36807;&#23574;&#31471;&#25216;&#26415;&#37325;&#22609;&#20102;&#31227;&#21160;&#24615;&#30340;&#26684;&#23616;&#12290;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#21644;&#23398;&#20064;&#31639;&#27861;&#30340;&#25972;&#21512;&#26159;&#36825;&#19968;&#36827;&#21270;&#30340;&#26680;&#24515;&#65292;&#23558;&#36710;&#36742;&#25512;&#21521;&#21069;&#25152;&#26410;&#26377;&#30340;&#33258;&#20027;&#39046;&#22495;&#12290;&#26412;&#25991;&#20840;&#38754;&#25506;&#35752;&#20102;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#20013;AI&#30340;&#28436;&#36827;&#36712;&#36857;&#65292;&#20174;&#22522;&#30784;&#21407;&#29702;&#36861;&#28335;&#21040;&#26368;&#26032;&#36827;&#23637;&#12290;&#20174;&#24403;&#21069;&#26223;&#35266;&#27010;&#36848;&#24320;&#22987;&#65292;&#26412;&#25991;&#28145;&#20837;&#25506;&#35752;&#20102;AI&#22312;&#22609;&#36896;&#36710;&#36742;&#33258;&#20027;&#20915;&#31574;&#33021;&#21147;&#20013;&#30340;&#22522;&#30784;&#20316;&#29992;&#12290;&#38416;&#26126;&#20102;AI&#39537;&#21160;&#30340;&#36710;&#36742;&#24320;&#21457;&#29983;&#21629;&#21608;&#26399;&#20013;&#28041;&#21450;&#30340;&#27493;&#39588;&#65292;&#35299;&#20915;&#20102;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#20013;AI&#39537;&#21160;&#36719;&#20214;&#24320;&#21457;&#20013;&#30340;&#20262;&#29702;&#32771;&#34385;&#21644;&#20559;&#35265;&#38382;&#39064;&#12290;&#35813;&#30740;&#31350;&#25552;&#20379;&#20102;&#20851;&#20110;AI/&#23398;&#20064;&#30340;&#20351;&#29992;&#21644;&#31867;&#22411;&#30340;&#32479;&#35745;&#27934;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17690v1 Announce Type: cross  Abstract: The advent of autonomous vehicles has heralded a transformative era in transportation, reshaping the landscape of mobility through cutting-edge technologies. Central to this evolu- tion is the integration of Artificial Intelligence (AI) and learning algorithms, propelling vehicles into realms of unprecedented autonomy. This paper provides a comprehensive exploration of the evolutionary trajectory of AI within autonomous vehicles, tracing the journey from foundational principles to the most recent advancements. Commencing with a current landscape overview, the paper delves into the fundamental role of AI in shaping the autonomous decision-making capabilities of vehicles. It elucidates the steps involved in the AI-powered development life cycle in vehicles, addressing ethical considerations and bias in AI-driven software development for autonomous vehicles. The study presents statis- tical insights into the usage and types of AI/learning
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;&#20351;&#29992;&#20808;&#21069;&#25552;&#21462;&#30340;&#29992;&#25143;&#20449;&#24687;&#23545;&#36710;&#36733;&#26080;&#32447;&#29615;&#22659;&#20013;&#30340;QoS&#36827;&#34892;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;ML&#26641;&#38598;&#25104;&#26041;&#27861;&#26469;&#25552;&#39640;&#39044;&#27979;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.17689</link><description>&lt;p&gt;
&#36890;&#36807;&#20808;&#39564;&#29992;&#25143;&#20449;&#24687;&#22312;&#26080;&#32447;&#36710;&#36733;&#29615;&#22659;&#20013;&#39044;&#27979;QoS
&lt;/p&gt;
&lt;p&gt;
QoS prediction in radio vehicular environments via prior user information
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17689
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;&#20351;&#29992;&#20808;&#21069;&#25552;&#21462;&#30340;&#29992;&#25143;&#20449;&#24687;&#23545;&#36710;&#36733;&#26080;&#32447;&#29615;&#22659;&#20013;&#30340;QoS&#36827;&#34892;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;ML&#26641;&#38598;&#25104;&#26041;&#27861;&#26469;&#25552;&#39640;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#38752;&#30340;&#26080;&#32447;&#36890;&#20449;&#22312;&#27773;&#36710;&#34892;&#19994;&#20013;&#25198;&#28436;&#30528;&#37325;&#35201;&#35282;&#33394;&#65292;&#23427;&#26377;&#21161;&#20110;&#22686;&#24378;&#24403;&#21069;&#29992;&#20363;&#24182;&#23454;&#29616;&#26032;&#30340;&#29992;&#20363;&#65292;&#22914;&#36830;&#25509;&#30340;&#33258;&#21160;&#39550;&#39542;&#12289;&#32534;&#38431;&#34892;&#39542;&#12289;&#21512;&#20316;&#25805;&#32437;&#12289;&#36828;&#31243;&#39550;&#39542;&#21644;&#26234;&#33021;&#23548;&#33322;&#12290;&#36825;&#20123;&#20197;&#21450;&#20854;&#20182;&#29992;&#20363;&#36890;&#24120;&#20381;&#36182;&#20110;&#29305;&#23450;&#30340;&#36890;&#20449;&#26381;&#21153;&#36136;&#37327;&#65288;QoS&#65289;&#27700;&#24179;&#12290;&#26368;&#36817;&#65292;&#39044;&#27979;&#24615;&#26381;&#21153;&#36136;&#37327;&#65288;QoS&#65289;&#39046;&#22495;&#21463;&#21040;&#20102;&#26497;&#22823;&#20851;&#27880;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#25552;&#21069;&#36275;&#22815;&#20934;&#30830;&#22320;&#39044;&#27979;&#36890;&#20449;&#36136;&#37327;&#12290;&#28982;&#32780;&#65292;&#22312;&#21487;&#38752;&#22320;&#39044;&#27979;QoS&#26041;&#38754;&#26159;&#19968;&#39033;&#38750;&#24120;&#22256;&#38590;&#30340;&#20219;&#21153;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20351;&#29992;&#20174;&#34562;&#31389;&#27979;&#35797;&#32593;&#32476;&#25910;&#38598;&#30340;&#25968;&#25454;&#26469;&#39044;&#27979;QoS&#30340;ML&#26641;&#38598;&#25104;&#26041;&#27861;&#65292;&#33539;&#22260;&#20026;&#20960;&#20998;&#38047;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#26080;&#32447;&#29615;&#22659;&#29305;&#24449;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#36825;&#20123;&#29305;&#24449;&#26469;&#25552;&#39640;ML&#24615;&#33021;&#65292;&#24182;&#36827;&#19968;&#27493;&#25903;&#25345;ML&#22312;&#21830;&#19994;&#32593;&#32476;&#20013;&#30340;&#24212;&#29992;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17689v1 Announce Type: cross  Abstract: Reliable wireless communications play an important role in the automotive industry as it helps to enhance current use cases and enable new ones such as connected autonomous driving, platooning, cooperative maneuvering, teleoperated driving, and smart navigation. These and other use cases often rely on specific quality of service (QoS) levels for communication. Recently, the area of predictive quality of service (QoS) has received a great deal of attention as a key enabler to forecast communication quality well enough in advance. However, predicting QoS in a reliable manner is a notoriously difficult task. In this paper, we evaluate ML tree-ensemble methods to predict QoS in the range of minutes with data collected from a cellular test network. We discuss radio environment characteristics and we showcase how these can be used to improve ML performance and further support the uptake of ML in commercial networks. Specifically, we use the 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Navigator&#30340;&#26032;&#22411;&#26694;&#26550;&#65292;&#36890;&#36807;&#32479;&#19968;GPU&#20869;&#23384;&#31649;&#29702;&#21644;&#20219;&#21153;&#25918;&#32622;&#21151;&#33021;&#65292;&#20197;&#20943;&#23569;&#20316;&#19994;&#24310;&#36831;&#65292;&#21516;&#26102;&#39640;&#25928;&#21033;&#29992;&#36164;&#28304;&#65292;&#27604;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#35843;&#24230;&#22120;&#34920;&#29616;&#20986;&#26356;&#26174;&#33879;&#30340;&#23436;&#25104;&#26102;&#38388;&#32553;&#30701;&#12290;</title><link>https://arxiv.org/abs/2402.17652</link><description>&lt;p&gt;
&#23548;&#33322;&#22120;&#65306;&#38754;&#21521;&#24310;&#36831;&#25935;&#24863;ML&#24037;&#20316;&#27969;&#30340;&#20998;&#25955;&#24335;&#35843;&#24230;&#22120;
&lt;/p&gt;
&lt;p&gt;
Navigator: A Decentralized Scheduler for Latency-Sensitive ML Workflows
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17652
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Navigator&#30340;&#26032;&#22411;&#26694;&#26550;&#65292;&#36890;&#36807;&#32479;&#19968;GPU&#20869;&#23384;&#31649;&#29702;&#21644;&#20219;&#21153;&#25918;&#32622;&#21151;&#33021;&#65292;&#20197;&#20943;&#23569;&#20316;&#19994;&#24310;&#36831;&#65292;&#21516;&#26102;&#39640;&#25928;&#21033;&#29992;&#36164;&#28304;&#65292;&#27604;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#35843;&#24230;&#22120;&#34920;&#29616;&#20986;&#26356;&#26174;&#33879;&#30340;&#23436;&#25104;&#26102;&#38388;&#32553;&#30701;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#22312;&#20998;&#24067;&#24335;&#31995;&#32479;&#20013;&#36827;&#34892;ML&#26597;&#35810;&#22788;&#29702;&#65292;&#20854;&#20013;&#21551;&#29992;GPU&#30340;&#24037;&#20316;&#20154;&#21592;&#21327;&#35843;&#25191;&#34892;&#22797;&#26434;&#26597;&#35810;&#65306;&#36825;&#31181;&#35745;&#31639;&#39118;&#26684;&#32463;&#24120;&#20986;&#29616;&#22312;&#19982;&#29992;&#25143;&#20114;&#21160;&#25903;&#25345;&#22270;&#20687;&#22788;&#29702;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#24212;&#29992;&#31243;&#24207;&#20013;&#12290;&#22312;&#36825;&#31181;&#31995;&#32479;&#20013;&#65292;GPU&#20869;&#23384;&#31649;&#29702;&#21644;&#20219;&#21153;&#25918;&#32622;&#30340;&#21327;&#21516;&#35843;&#24230;&#20195;&#34920;&#30528;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#26426;&#20250;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Navigator&#65292;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#32479;&#19968;&#20102;&#36825;&#20123;&#21151;&#33021;&#65292;&#20197;&#20943;&#23569;&#20316;&#19994;&#24310;&#36831;&#65292;&#21516;&#26102;&#39640;&#25928;&#21033;&#29992;&#36164;&#28304;&#65292;&#23558;&#20219;&#21153;&#25918;&#32622;&#22312;&#25968;&#25454;&#20381;&#36182;&#20851;&#31995;&#23558;&#24471;&#21040;&#28385;&#36275;&#30340;&#22320;&#26041;&#65292;&#23558;&#26469;&#33258;&#21516;&#19968;&#20316;&#19994;&#30340;&#20219;&#21153;&#25918;&#22312;&#19968;&#36215;&#65288;&#24403;&#36825;&#19981;&#20250;&#20351;&#20027;&#26426;&#25110;&#20854;GPU&#36229;&#36733;&#26102;&#65289;&#65292;&#24182;&#39640;&#25928;&#22320;&#31649;&#29702;GPU&#20869;&#23384;&#12290;&#19982;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#35843;&#24230;&#22120;&#27604;&#36739;&#26174;&#31034;&#65292;&#22312;&#38656;&#35201;&#30456;&#21516;&#37327;&#29978;&#33267;&#26356;&#23569;&#36164;&#28304;&#30340;&#24773;&#20917;&#19979;&#65292;&#23436;&#25104;&#26102;&#38388;&#26174;&#33879;&#20943;&#23569;&#12290;&#22312;&#19968;&#20010;&#26696;&#20363;&#20013;&#65292;&#20165;&#38656;&#35201;&#19968;&#21322;&#30340;&#26381;&#21153;&#22120;&#26469;&#22788;&#29702;&#30456;&#21516;&#30340;&#24037;&#20316;&#36127;&#36733;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17652v1 Announce Type: cross  Abstract: We consider ML query processing in distributed systems where GPU-enabled workers coordinate to execute complex queries: a computing style often seen in applications that interact with users in support of image processing and natural language processing. In such systems, coscheduling of GPU memory management and task placement represents a promising opportunity. We propose Navigator, a novel framework that unifies these functions to reduce job latency while using resources efficiently, placing tasks where data dependencies will be satisfied, collocating tasks from the same job (when this will not overload the host or its GPU), and efficiently managing GPU memory. Comparison with other state of the art schedulers shows a significant reduction in completion times while requiring the same amount or even fewer resources. In one case, just half the servers were needed for processing the same workload.
&lt;/p&gt;</description></item><item><title>SongComposer&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#27468;&#26354;&#29983;&#25104;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#37319;&#29992;&#31526;&#21495;&#21270;&#30340;&#27468;&#26354;&#34920;&#31034;&#65292;&#23454;&#29616;&#20102;LLM&#21487;&#20197;&#26126;&#30830;&#21019;&#20316;&#27468;&#26354;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.17645</link><description>&lt;p&gt;
SongComposer&#65306;&#19968;&#31181;&#29992;&#20110;&#27468;&#26354;&#29983;&#25104;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#29992;&#20110;&#27468;&#35789;&#21644;&#26059;&#24459;&#21019;&#20316;
&lt;/p&gt;
&lt;p&gt;
SongComposer: A Large Language Model for Lyric and Melody Composition in Song Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17645
&lt;/p&gt;
&lt;p&gt;
SongComposer&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#27468;&#26354;&#29983;&#25104;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#37319;&#29992;&#31526;&#21495;&#21270;&#30340;&#27468;&#26354;&#34920;&#31034;&#65292;&#23454;&#29616;&#20102;LLM&#21487;&#20197;&#26126;&#30830;&#21019;&#20316;&#27468;&#26354;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;SongComposer&#65292;&#19968;&#20010;&#20026;&#27468;&#26354;&#21019;&#20316;&#32780;&#35774;&#35745;&#30340;&#21019;&#26032;&#22411;LLM&#12290;&#23427;&#33021;&#22815;&#29702;&#35299;&#21644;&#29983;&#25104;&#27468;&#26354;&#20013;&#30340;&#26059;&#24459;&#21644;&#27468;&#35789;&#65292;&#36890;&#36807;&#21033;&#29992;LLM&#30340;&#33021;&#21147;&#22312;&#31526;&#21495;&#21270;&#30340;&#27468;&#26354;&#34920;&#31034;&#20013;&#29983;&#25104;&#12290;&#29616;&#26377;&#30340;&#19982;&#38899;&#20048;&#30456;&#20851;&#30340;LLM&#23558;&#38899;&#20048;&#35270;&#20026;&#37327;&#21270;&#30340;&#38899;&#39057;&#20449;&#21495;&#65292;&#32780;&#36825;&#31181;&#38544;&#24335;&#32534;&#30721;&#23548;&#33268;&#20102;&#32534;&#30721;&#25928;&#29575;&#20302;&#19979;&#21644;&#28789;&#27963;&#24615;&#24046;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#31526;&#21495;&#21270;&#30340;&#27468;&#26354;&#34920;&#31034;&#65292;&#36825;&#26159;&#20154;&#31867;&#20026;&#38899;&#20048;&#35774;&#35745;&#30340;&#25104;&#29087;&#21644;&#39640;&#25928;&#30340;&#26041;&#24335;&#65292;&#24182;&#20351;LLM&#33021;&#22815;&#20687;&#20154;&#31867;&#19968;&#26679;&#26126;&#30830;&#22320;&#21019;&#20316;&#27468;&#26354;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20803;&#32452;&#35774;&#35745;&#65292;&#29992;&#20110;&#26684;&#24335;&#21270;&#27468;&#35789;&#21644;&#26059;&#24459;&#20013;&#30340;&#19977;&#20010;&#38899;&#31526;&#23646;&#24615;&#65288;&#38899;&#39640;&#12289;&#25345;&#32493;&#26102;&#38388;&#21644;&#20241;&#27490;&#26102;&#38388;&#65289;&#65292;&#20174;&#32780;&#20445;&#35777;LLM&#23545;&#38899;&#20048;&#31526;&#21495;&#30340;&#27491;&#30830;&#29702;&#35299;&#65292;&#24182;&#23454;&#29616;&#27468;&#35789;&#21644;&#26059;&#24459;&#20043;&#38388;&#30340;&#31934;&#30830;&#23545;&#40784;&#12290;&#20026;&#20102;&#21521;LLM&#28748;&#36755;&#22522;&#26412;&#30340;&#38899;&#20048;&#29702;&#35299;&#65292;&#25105;&#20204;&#31934;&#24515;&#25910;&#38598;&#20102;SongCompose-PT&#65292;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#27468;&#26354;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#25324;&#20102;&#27468;&#35789;&#12289;&#26059;&#24459;&#21644;&#25104;&#23545;&#30340;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17645v1 Announce Type: cross  Abstract: We present SongComposer, an innovative LLM designed for song composition. It could understand and generate melodies and lyrics in symbolic song representations, by leveraging the capability of LLM. Existing music-related LLM treated the music as quantized audio signals, while such implicit encoding leads to inefficient encoding and poor flexibility. In contrast, we resort to symbolic song representation, the mature and efficient way humans designed for music, and enable LLM to explicitly compose songs like humans. In practice, we design a novel tuple design to format lyric and three note attributes (pitch, duration, and rest duration) in the melody, which guarantees the correct LLM understanding of musical symbols and realizes precise alignment between lyrics and melody. To impart basic music understanding to LLM, we carefully collected SongCompose-PT, a large-scale song pretraining dataset that includes lyrics, melodies, and paired ly
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;QRData&#22522;&#20934;&#27979;&#35797;&#65292;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#32479;&#35745;&#21644;&#22240;&#26524;&#25512;&#29702;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#32467;&#26524;&#26174;&#31034;&#26368;&#24378;&#27169;&#22411;GPT-4&#22312;&#35813;&#27979;&#35797;&#20013;&#20934;&#30830;&#29575;&#20026;58&#65285;&#65292;&#23384;&#22312;&#25913;&#36827;&#31354;&#38388;&#12290;</title><link>https://arxiv.org/abs/2402.17644</link><description>&lt;p&gt;
LLMs&#26159;&#21542;&#20855;&#22791;&#22522;&#20110;&#25968;&#25454;&#30340;&#32479;&#35745;&#21644;&#22240;&#26524;&#25512;&#29702;&#33021;&#21147;&#65311;&#29992;&#25968;&#25454;&#23545;&#20808;&#36827;&#30340;&#23450;&#37327;&#25512;&#29702;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Are LLMs Capable of Data-based Statistical and Causal Reasoning? Benchmarking Advanced Quantitative Reasoning with Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17644
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;QRData&#22522;&#20934;&#27979;&#35797;&#65292;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#32479;&#35745;&#21644;&#22240;&#26524;&#25512;&#29702;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#32467;&#26524;&#26174;&#31034;&#26368;&#24378;&#27169;&#22411;GPT-4&#22312;&#35813;&#27979;&#35797;&#20013;&#20934;&#30830;&#29575;&#20026;58&#65285;&#65292;&#23384;&#22312;&#25913;&#36827;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#21270;&#25512;&#29702;&#26159;&#20998;&#26512;&#25968;&#25454;&#30340;&#20851;&#38190;&#25216;&#33021;&#65292;&#28982;&#32780;&#23545;&#36825;&#31181;&#33021;&#21147;&#30340;&#35780;&#20272;&#20173;&#28982;&#26377;&#38480;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Quantitative Reasoning with Data&#65288;QRData&#65289;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#32479;&#35745;&#21644;&#22240;&#26524;&#25512;&#29702;&#26041;&#38754;&#19982;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#30340;&#33021;&#21147;&#12290;&#35813;&#22522;&#20934;&#27979;&#35797;&#21253;&#25324;&#19968;&#20010;&#31934;&#24515;&#26500;&#24314;&#30340;&#21253;&#21547;&#26469;&#33258;&#25945;&#31185;&#20070;&#12289;&#22312;&#32447;&#23398;&#20064;&#26448;&#26009;&#21644;&#23398;&#26415;&#35770;&#25991;&#30340;&#25968;&#25454;&#34920;&#30340;411&#20010;&#38382;&#39064;&#30340;&#25968;&#25454;&#38598;&#12290;&#20026;&#20102;&#27604;&#36739;&#27169;&#22411;&#22312;&#25968;&#25454;&#21644;&#25991;&#26412;&#19978;&#30340;&#23450;&#37327;&#25512;&#29702;&#33021;&#21147;&#65292;&#25105;&#20204;&#36824;&#22312;&#22522;&#20934;&#27979;&#35797;&#20013;&#28155;&#21152;&#20102;&#19968;&#20010;&#21253;&#21547;290&#20010;&#20165;&#25991;&#26412;&#38382;&#39064;&#30340;&#36741;&#21161;&#25968;&#25454;&#38598;&#65292;&#21363;QRText&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#12289;&#22522;&#20110;&#31243;&#24207;&#25512;&#29702;&#21644;&#20195;&#29702;&#25512;&#29702;&#26041;&#27861;&#65292;&#21253;&#25324;Chain-of-Thought&#12289;Program-of-Thoughts&#12289;ReAct&#21644;&#20195;&#30721;&#35299;&#37322;&#22120;&#36741;&#21161;&#31561;&#22312;&#21508;&#31181;&#27169;&#22411;&#19978;&#30340;&#34920;&#29616;&#12290;&#26368;&#24378;&#30340;&#27169;&#22411;GPT-4&#30340;&#20934;&#30830;&#29575;&#36798;&#21040;&#20102;58&#65285;&#65292;&#20294;&#20173;&#26377;&#24456;&#22823;&#30340;&#25913;&#36827;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17644v1 Announce Type: cross  Abstract: Quantitative reasoning is a critical skill to analyze data, yet the assessment of such ability remains limited. To address this gap, we introduce the Quantitative Reasoning with Data (QRData) benchmark, aiming to evaluate Large Language Models' capability in statistical and causal reasoning with real-world data. The benchmark comprises a carefully constructed dataset of 411 questions accompanied by data sheets from textbooks, online learning materials, and academic papers. To compare models' quantitative reasoning abilities on data and text, we enrich the benchmark with an auxiliary set of 290 text-only questions, namely QRText. We evaluate natural language reasoning, program-based reasoning, and agent reasoning methods including Chain-of-Thought, Program-of-Thoughts, ReAct, and code interpreter assistants on diverse models. The strongest model GPT-4 achieves an accuracy of 58%, which has a large room for improvement. Among open-source
&lt;/p&gt;</description></item><item><title>&#21464;&#20998;&#23398;&#20064;&#22312;&#22823;&#22411;&#28145;&#24230;&#32593;&#32476;&#20013;&#23637;&#29616;&#20986;&#38750;&#24120;&#22909;&#30340;&#25928;&#26524;&#65292;IVON&#20248;&#21270;&#22120;&#22312;&#35757;&#32451;&#22823;&#22411;&#32593;&#32476;&#26102;&#20960;&#20046;&#33021;&#19982;Adam&#30456;&#23218;&#32654;&#29978;&#33267;&#32988;&#36807;&#23427;&#65292;&#19988;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#26356;&#20934;&#30830;&#65292;&#23545;&#27169;&#22411;&#24494;&#35843;&#12289;&#27867;&#21270;&#35823;&#24046;&#39044;&#27979;&#21644;&#25968;&#25454;&#25935;&#24863;&#24615;&#20272;&#35745;&#22343;&#26377;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.17641</link><description>&lt;p&gt;
&#21464;&#20998;&#23398;&#20064;&#23545;&#22823;&#22411;&#28145;&#24230;&#32593;&#32476;&#26377;&#25928;
&lt;/p&gt;
&lt;p&gt;
Variational Learning is Effective for Large Deep Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17641
&lt;/p&gt;
&lt;p&gt;
&#21464;&#20998;&#23398;&#20064;&#22312;&#22823;&#22411;&#28145;&#24230;&#32593;&#32476;&#20013;&#23637;&#29616;&#20986;&#38750;&#24120;&#22909;&#30340;&#25928;&#26524;&#65292;IVON&#20248;&#21270;&#22120;&#22312;&#35757;&#32451;&#22823;&#22411;&#32593;&#32476;&#26102;&#20960;&#20046;&#33021;&#19982;Adam&#30456;&#23218;&#32654;&#29978;&#33267;&#32988;&#36807;&#23427;&#65292;&#19988;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#26356;&#20934;&#30830;&#65292;&#23545;&#27169;&#22411;&#24494;&#35843;&#12289;&#27867;&#21270;&#35823;&#24046;&#39044;&#27979;&#21644;&#25968;&#25454;&#25935;&#24863;&#24615;&#20272;&#35745;&#22343;&#26377;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20379;&#20102;&#22823;&#37327;&#23454;&#35777;&#35777;&#25454;&#65292;&#21453;&#39539;&#20102;&#21464;&#20998;&#23398;&#20064;&#23545;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#26080;&#25928;&#30340;&#26222;&#36941;&#30475;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#31181;&#21517;&#20026;Improved Variational Online Newton (IVON)&#30340;&#20248;&#21270;&#22120;&#65292;&#22312;&#35757;&#32451;&#22823;&#22411;&#32593;&#32476;&#65288;&#22914;GPT-2&#21644;ResNets&#65289;&#26102;&#22987;&#32456;&#33021;&#22815;&#19982;Adam&#30456;&#21305;&#37197;&#25110;&#32988;&#36807;&#23427;&#12290;IVON&#30340;&#35745;&#31639;&#25104;&#26412;&#20960;&#20046;&#19982;Adam&#30456;&#21516;&#65292;&#20294;&#20854;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#26356;&#22909;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;IVON&#30340;&#20960;&#31181;&#26032;&#29992;&#20363;&#65292;&#20854;&#20013;&#25105;&#20204;&#25913;&#36827;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24494;&#35843;&#21644;&#27169;&#22411;&#21512;&#24182;&#65292;&#22312;&#20934;&#30830;&#39044;&#27979;&#27867;&#21270;&#35823;&#24046;&#21644;&#24544;&#23454;&#20272;&#35745;&#23545;&#25968;&#25454;&#30340;&#25935;&#24863;&#24615;&#26041;&#38754;&#12290;&#25105;&#20204;&#25214;&#21040;&#20102;&#22823;&#37327;&#25903;&#25345;&#21464;&#20998;&#23398;&#20064;&#26377;&#25928;&#24615;&#30340;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17641v1 Announce Type: cross  Abstract: We give extensive empirical evidence against the common belief that variational learning is ineffective for large neural networks. We show that an optimizer called Improved Variational Online Newton (IVON) consistently matches or outperforms Adam for training large networks such as GPT-2 and ResNets from scratch. IVON's computational costs are nearly identical to Adam but its predictive uncertainty is better. We show several new use cases of IVON where we improve fine-tuning and model merging in Large Language Models, accurately predict generalization error, and faithfully estimate sensitivity to data. We find overwhelming evidence in support of effectiveness of variational learning.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#25299;&#25169;&#24863;&#30693;&#30340;&#21452;&#21521;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#65288;TBGAT&#65289;&#65292;&#22312;&#35299;&#20915;&#36710;&#38388;&#20316;&#19994;&#35843;&#24230;&#38382;&#39064;&#20013;&#65292;&#36890;&#36807;&#23884;&#20837;&#24182;&#21457;&#22270;&#24182;&#21033;&#29992;&#21452;&#21521;&#35270;&#22270;&#23884;&#20837;&#12289;&#22270;&#27880;&#24847;&#21147;&#32858;&#21512;&#31561;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#23545;&#25299;&#25169;&#32467;&#26500;&#30340;&#26356;&#22909;&#24314;&#27169;&#21644;&#21033;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.17606</link><description>&lt;p&gt;
&#20351;&#29992;&#21452;&#21521;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#23398;&#20064;&#25299;&#25169;&#34920;&#31034;&#35299;&#20915;&#36710;&#38388;&#20316;&#19994;&#35843;&#24230;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Learning Topological Representations with Bidirectional Graph Attention Network for Solving Job Shop Scheduling Problem
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17606
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#25299;&#25169;&#24863;&#30693;&#30340;&#21452;&#21521;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#65288;TBGAT&#65289;&#65292;&#22312;&#35299;&#20915;&#36710;&#38388;&#20316;&#19994;&#35843;&#24230;&#38382;&#39064;&#20013;&#65292;&#36890;&#36807;&#23884;&#20837;&#24182;&#21457;&#22270;&#24182;&#21033;&#29992;&#21452;&#21521;&#35270;&#22270;&#23884;&#20837;&#12289;&#22270;&#27880;&#24847;&#21147;&#32858;&#21512;&#31561;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#23545;&#25299;&#25169;&#32467;&#26500;&#30340;&#26356;&#22909;&#24314;&#27169;&#21644;&#21033;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#36890;&#24120;&#20351;&#29992;&#38024;&#23545;&#26080;&#21521;&#22270;&#30340;&#29616;&#25104;GNN&#27169;&#22411;&#35299;&#20915;&#36710;&#38388;&#20316;&#19994;&#35843;&#24230;&#38382;&#39064;&#65288;JSSP&#65289;&#65292;&#24182;&#24573;&#30053;&#20102;&#24182;&#21457;&#22270;&#65288;DGs&#65289;&#30340;&#20016;&#23500;&#32780;&#26377;&#24847;&#20041;&#30340;&#25299;&#25169;&#32467;&#26500;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#25299;&#25169;&#24863;&#30693;&#30340;&#21452;&#21521;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#65288;TBGAT&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#26032;&#39062;GNN&#26550;&#26500;&#65292;&#29992;&#20110;&#22312;&#26412;&#22320;&#25628;&#32034;&#26694;&#26550;&#20013;&#23884;&#20837;DG&#20197;&#35299;&#20915;JSSP&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;TBGAT&#20998;&#21035;&#20174;&#27491;&#21521;&#21644;&#21453;&#21521;&#35270;&#22270;&#23884;&#20837;DG&#65292;&#28040;&#24687;&#36890;&#36807;&#36981;&#24490;&#19981;&#21516;&#35270;&#22270;&#30340;&#25299;&#25169;&#32467;&#26500;&#20256;&#25773;&#65292;&#24182;&#36890;&#36807;&#22270;&#27880;&#24847;&#21147;&#36827;&#34892;&#27719;&#24635;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#28040;&#24687;&#20256;&#36882;&#26426;&#21046;&#30340;&#26032;&#25805;&#20316;&#31526;&#65292;&#29992;&#20110;&#35745;&#31639;DG&#30340;&#21069;&#21521;&#21644;&#21518;&#21521;&#25299;&#25169;&#25490;&#24207;&#65292;&#36825;&#20123;&#29305;&#24449;&#29992;&#20110;&#34920;&#24449;&#25299;&#25169;&#32467;&#26500;&#24182;&#34987;&#25105;&#20204;&#30340;&#27169;&#22411;&#21033;&#29992;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20174;&#29702;&#35770;&#21644;&#23454;&#39564;&#19978;&#23637;&#31034;&#20102;TBGAT&#30340;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17606v1 Announce Type: cross  Abstract: Existing learning-based methods for solving job shop scheduling problem (JSSP) usually use off-the-shelf GNN models tailored to undirected graphs and neglect the rich and meaningful topological structures of disjunctive graphs (DGs). This paper proposes the topology-aware bidirectional graph attention network (TBGAT), a novel GNN architecture based on the attention mechanism, to embed the DG for solving JSSP in a local search framework. Specifically, TBGAT embeds the DG from a forward and a backward view, respectively, where the messages are propagated by following the different topologies of the views and aggregated via graph attention. Then, we propose a novel operator based on the message-passing mechanism to calculate the forward and backward topological sorts of the DG, which are the features for characterizing the topological structures and exploited by our model. In addition, we theoretically and experimentally show that TBGAT h
&lt;/p&gt;</description></item><item><title>DAGnosis&#20351;&#29992;&#26377;&#21521;&#26080;&#29615;&#22270;(DAGs)&#26469;&#35299;&#20915;&#25968;&#25454;&#19968;&#33268;&#24615;&#26816;&#27979;&#20013;&#30340;&#20004;&#20010;&#20851;&#38190;&#38480;&#21046;&#65292;&#24182;&#33021;&#22815;&#20934;&#30830;&#23450;&#20301;&#20026;&#20309;&#26679;&#26412;&#20250;&#34987;&#26631;&#35760;&#20026;&#19981;&#19968;&#33268;&#12290;</title><link>https://arxiv.org/abs/2402.17599</link><description>&lt;p&gt;
DAGnosis&#65306;&#20351;&#29992;&#32467;&#26500;&#36827;&#34892;&#25968;&#25454;&#19981;&#19968;&#33268;&#24615;&#30340;&#23616;&#37096;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
DAGnosis: Localized Identification of Data Inconsistencies using Structures
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17599
&lt;/p&gt;
&lt;p&gt;
DAGnosis&#20351;&#29992;&#26377;&#21521;&#26080;&#29615;&#22270;(DAGs)&#26469;&#35299;&#20915;&#25968;&#25454;&#19968;&#33268;&#24615;&#26816;&#27979;&#20013;&#30340;&#20004;&#20010;&#20851;&#38190;&#38480;&#21046;&#65292;&#24182;&#33021;&#22815;&#20934;&#30830;&#23450;&#20301;&#20026;&#20309;&#26679;&#26412;&#20250;&#34987;&#26631;&#35760;&#20026;&#19981;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#37096;&#32626;&#26102;&#35782;&#21035;&#21644;&#36866;&#24403;&#22788;&#29702;&#25968;&#25454;&#20013;&#30340;&#19981;&#19968;&#33268;&#24615;&#23545;&#21487;&#38752;&#22320;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#33267;&#20851;&#37325;&#35201;&#12290;&#36817;&#26399;&#30340;&#25968;&#25454;&#20013;&#24515;&#26041;&#27861;&#33021;&#22815;&#35782;&#21035;&#19982;&#35757;&#32451;&#38598;&#30456;&#20851;&#30340;&#36825;&#31181;&#19981;&#19968;&#33268;&#24615;&#65292;&#20294;&#23384;&#22312;&#20004;&#20010;&#20851;&#38190;&#38480;&#21046;&#65306;&#65288;1&#65289;&#22312;&#29305;&#24449;&#23637;&#29616;&#32479;&#35745;&#29420;&#31435;&#24615;&#30340;&#24773;&#20917;&#19979;&#34920;&#29616;&#19981;&#20339;&#65292;&#22240;&#20026;&#23427;&#20204;&#20351;&#29992;&#21387;&#32553;&#34920;&#31034;&#65307;&#65288;2&#65289;&#32570;&#20047;&#23616;&#37096;&#21270;&#65292;&#26080;&#27861;&#20934;&#30830;&#23450;&#20301;&#26679;&#26412;&#20026;&#20309;&#34987;&#26631;&#35760;&#20026;&#19981;&#19968;&#33268;&#65292;&#36825;&#23545;&#25351;&#23548;&#26410;&#26469;&#25968;&#25454;&#25910;&#38598;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#20351;&#29992;&#26377;&#21521;&#26080;&#29615;&#22270;&#65288;DAGs&#65289;&#26469;&#32534;&#30721;&#35757;&#32451;&#38598;&#30340;&#29305;&#24449;&#27010;&#29575;&#20998;&#24067;&#21644;&#29420;&#31435;&#24615;&#20316;&#20026;&#32467;&#26500;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#36825;&#20004;&#20010;&#22522;&#26412;&#38480;&#21046;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#34987;&#31216;&#20026;DAGnosis&#65292;&#21033;&#29992;&#36825;&#20123;&#32467;&#26500;&#20132;&#20114;&#24102;&#26469;&#26377;&#20215;&#20540;&#30340;&#12289;&#28145;&#21051;&#30340;&#25968;&#25454;&#20013;&#24515;&#32467;&#35770;&#12290;DAGnosis&#35299;&#38145;&#20102;&#22312;DAG&#19978;&#23450;&#20301;&#19981;&#19968;&#33268;&#24615;&#21407;&#22240;&#30340;&#33021;&#21147;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17599v1 Announce Type: cross  Abstract: Identification and appropriate handling of inconsistencies in data at deployment time is crucial to reliably use machine learning models. While recent data-centric methods are able to identify such inconsistencies with respect to the training set, they suffer from two key limitations: (1) suboptimality in settings where features exhibit statistical independencies, due to their usage of compressive representations and (2) lack of localization to pin-point why a sample might be flagged as inconsistent, which is important to guide future data collection. We solve these two fundamental limitations using directed acyclic graphs (DAGs) to encode the training set's features probability distribution and independencies as a structure. Our method, called DAGnosis, leverages these structural interactions to bring valuable and insightful data-centric conclusions. DAGnosis unlocks the localization of the causes of inconsistencies on a DAG, an aspec
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22312;&#26356;&#29616;&#23454;&#30340;&#31070;&#32463;&#32593;&#32476;&#32972;&#26223;&#19979;&#25506;&#35752;&#20102;&#38544;&#24335;&#27491;&#21017;&#21270;&#29616;&#35937;&#65292;&#36890;&#36807;&#30740;&#31350;&#38750;&#32447;&#24615;&#28608;&#27963;&#20989;&#25968;&#30340;&#19968;&#33324;&#31867;&#21035;&#65292;&#20005;&#26684;&#35777;&#26126;&#20102;&#22312;&#30697;&#38453;&#24863;&#30693;&#38382;&#39064;&#35774;&#32622;&#20013;&#36825;&#20123;&#32593;&#32476;&#30340;&#38544;&#24335;&#27491;&#21017;&#21270;&#29616;&#35937;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#20005;&#26684;&#30340;&#36895;&#29575;&#20445;&#35777;&#65292;&#30830;&#20445;&#26799;&#24230;&#30340;&#25351;&#25968;&#32423;&#24555;&#36895;&#25910;&#25947;&#12290;</title><link>https://arxiv.org/abs/2402.17595</link><description>&lt;p&gt;
&#36890;&#36807;&#35889;&#31070;&#32463;&#32593;&#32476;&#21644;&#38750;&#32447;&#24615;&#30697;&#38453;&#24863;&#30693;&#23454;&#29616;&#38544;&#24335;&#27491;&#21017;&#21270;
&lt;/p&gt;
&lt;p&gt;
Implicit Regularization via Spectral Neural Networks and Non-linear Matrix Sensing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17595
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22312;&#26356;&#29616;&#23454;&#30340;&#31070;&#32463;&#32593;&#32476;&#32972;&#26223;&#19979;&#25506;&#35752;&#20102;&#38544;&#24335;&#27491;&#21017;&#21270;&#29616;&#35937;&#65292;&#36890;&#36807;&#30740;&#31350;&#38750;&#32447;&#24615;&#28608;&#27963;&#20989;&#25968;&#30340;&#19968;&#33324;&#31867;&#21035;&#65292;&#20005;&#26684;&#35777;&#26126;&#20102;&#22312;&#30697;&#38453;&#24863;&#30693;&#38382;&#39064;&#35774;&#32622;&#20013;&#36825;&#20123;&#32593;&#32476;&#30340;&#38544;&#24335;&#27491;&#21017;&#21270;&#29616;&#35937;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#20005;&#26684;&#30340;&#36895;&#29575;&#20445;&#35777;&#65292;&#30830;&#20445;&#26799;&#24230;&#30340;&#25351;&#25968;&#32423;&#24555;&#36895;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38544;&#24335;&#27491;&#21017;&#21270;&#29616;&#35937;&#36817;&#24180;&#26469;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#20852;&#36259;&#65292;&#20316;&#20026;&#31070;&#32463;&#32593;&#32476;&#20986;&#33394;&#27867;&#21270;&#33021;&#21147;&#30340;&#19968;&#20010;&#22522;&#26412;&#26041;&#38754;&#12290;&#31616;&#32780;&#35328;&#20043;&#65292;&#23427;&#24847;&#21619;&#30528;&#22312;&#35768;&#22810;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#21363;&#20351;&#25439;&#22833;&#20989;&#25968;&#20013;&#27809;&#26377;&#20219;&#20309;&#26174;&#24335;&#27491;&#21017;&#21270;&#22120;&#65292;&#26799;&#24230;&#19979;&#38477;&#21160;&#24577;&#20063;&#20250;&#25910;&#25947;&#21040;&#19968;&#20010;&#27491;&#21017;&#21270;&#23398;&#20064;&#38382;&#39064;&#30340;&#35299;&#12290;&#28982;&#32780;&#65292;&#24050;&#30693;&#30340;&#35797;&#22270;&#20174;&#29702;&#35770;&#19978;&#35299;&#37322;&#36825;&#19968;&#29616;&#35937;&#30340;&#32467;&#26524;&#20027;&#35201;&#38598;&#20013;&#22312;&#32447;&#24615;&#31070;&#32463;&#32593;&#32476;&#30340;&#35774;&#32622;&#19978;&#65292;&#32447;&#24615;&#32467;&#26500;&#30340;&#31616;&#21333;&#24615;&#23545;&#29616;&#26377;&#35770;&#25454;&#29305;&#21035;&#20851;&#38190;&#12290;&#26412;&#25991;&#22312;&#26356;&#29616;&#23454;&#30340;&#31070;&#32463;&#32593;&#32476;&#21644;&#19968;&#33324;&#38750;&#32447;&#24615;&#28608;&#27963;&#20989;&#25968;&#30340;&#32972;&#26223;&#19979;&#25506;&#35752;&#20102;&#36825;&#19968;&#38382;&#39064;&#65292;&#24182;&#22312;&#30697;&#38453;&#24863;&#30693;&#38382;&#39064;&#30340;&#35774;&#32622;&#20013;&#20005;&#26684;&#35777;&#26126;&#20102;&#36825;&#20123;&#32593;&#32476;&#30340;&#38544;&#24335;&#27491;&#21017;&#21270;&#29616;&#35937;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#20005;&#26684;&#30340;&#36895;&#29575;&#20445;&#35777;&#65292;&#30830;&#20445;&#26799;&#24230;&#30340;&#25351;&#25968;&#32423;&#24555;&#36895;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17595v1 Announce Type: cross  Abstract: The phenomenon of implicit regularization has attracted interest in recent years as a fundamental aspect of the remarkable generalizing ability of neural networks. In a nutshell, it entails that gradient descent dynamics in many neural nets, even without any explicit regularizer in the loss function, converges to the solution of a regularized learning problem. However, known results attempting to theoretically explain this phenomenon focus overwhelmingly on the setting of linear neural nets, and the simplicity of the linear structure is particularly crucial to existing arguments. In this paper, we explore this problem in the context of more realistic neural networks with a general class of non-linear activation functions, and rigorously demonstrate the implicit regularization phenomenon for such networks in the setting of matrix sensing problems, together with rigorous rate guarantees that ensure exponentially fast convergence of gradi
&lt;/p&gt;</description></item><item><title>Agent-Pro&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#65292;&#36890;&#36807;&#31574;&#30053;&#32423;&#21035;&#30340;&#21453;&#24605;&#21644;&#20248;&#21270;&#65292;&#21487;&#20197;&#20174;&#20114;&#21160;&#32463;&#39564;&#20013;&#23398;&#20064;&#24182;&#36880;&#27493;&#25552;&#21319;&#20854;&#34892;&#20026;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2402.17574</link><description>&lt;p&gt;
Agent-Pro: &#36890;&#36807;&#31574;&#30053;&#32423;&#21035;&#21453;&#24605;&#21644;&#20248;&#21270;&#23398;&#20064;&#36827;&#21270;
&lt;/p&gt;
&lt;p&gt;
Agent-Pro: Learning to Evolve via Policy-Level Reflection and Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17574
&lt;/p&gt;
&lt;p&gt;
Agent-Pro&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#65292;&#36890;&#36807;&#31574;&#30053;&#32423;&#21035;&#30340;&#21453;&#24605;&#21644;&#20248;&#21270;&#65292;&#21487;&#20197;&#20174;&#20114;&#21160;&#32463;&#39564;&#20013;&#23398;&#20064;&#24182;&#36880;&#27493;&#25552;&#21319;&#20854;&#34892;&#20026;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#34920;&#29616;&#20986;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#20855;&#26377;&#24378;&#22823;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#37117;&#26159;&#29305;&#23450;&#20219;&#21153;&#27714;&#35299;&#22120;&#65292;&#24182;&#20855;&#26377;&#22797;&#26434;&#30340;&#25552;&#31034;&#24037;&#31243;&#65292;&#32780;&#19981;&#26159;&#33021;&#22815;&#36890;&#36807;&#20114;&#21160;&#23398;&#20064;&#21644;&#36827;&#21270;&#30340;&#20195;&#29702;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;Agent-Pro&#65306;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#65292;&#20855;&#26377;&#31574;&#30053;&#32423;&#21035;&#30340;&#21453;&#24605;&#21644;&#20248;&#21270;&#65292;&#21487;&#20197;&#20174;&#20114;&#21160;&#32463;&#39564;&#20013;&#23398;&#20064;&#20016;&#23500;&#30340;&#19987;&#19994;&#30693;&#35782;&#65292;&#24182;&#36880;&#28176;&#25552;&#21319;&#20854;&#34892;&#20026;&#31574;&#30053;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#23427;&#28041;&#21450;&#19968;&#20010;&#21160;&#24577;&#20449;&#24565;&#29983;&#25104;&#21644;&#21453;&#24605;&#36807;&#31243;&#65292;&#29992;&#20110;&#31574;&#30053;&#28436;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17574v1 Announce Type: new  Abstract: Large Language Models exhibit robust problem-solving capabilities for diverse tasks. However, most LLM-based agents are designed as specific task solvers with sophisticated prompt engineering, rather than agents capable of learning and evolving through interactions. These task solvers necessitate manually crafted prompts to inform task rules and regulate LLM behaviors, inherently incapacitating to address complex dynamic scenarios e.g., large interactive games. In light of this, we propose Agent-Pro: an LLM-based Agent with Policy-level Reflection and Optimization that can learn a wealth of expertise from interactive experiences and progressively elevate its behavioral policy. Specifically, it involves a dynamic belief generation and reflection process for policy evolution. Rather than action-level reflection, Agent-Pro iteratively reflects on past trajectories and beliefs, fine-tuning its irrational beliefs for a better policy. Moreover
&lt;/p&gt;</description></item><item><title>OmniACT&#26159;&#19968;&#20010;&#38024;&#23545;&#20195;&#29702;&#29983;&#25104;&#21487;&#25191;&#34892;&#31243;&#24207;&#23436;&#25104;&#35745;&#31639;&#26426;&#20219;&#21153;&#33021;&#21147;&#30340;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;&#65292;&#36229;&#36234;&#20102;&#20256;&#32479;Web&#33258;&#21160;&#21270;&#65292;&#28085;&#30422;&#20102;&#21508;&#31181;&#26700;&#38754;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.17553</link><description>&lt;p&gt;
OmniACT&#65306;&#29992;&#20110;&#21551;&#29992;&#26700;&#38754;&#21644;Web&#22810;&#27169;&#24335;&#36890;&#29992;&#20027;&#21160;&#26234;&#33021;&#20307;&#30340;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
OmniACT: A Dataset and Benchmark for Enabling Multimodal Generalist Autonomous Agents for Desktop and Web
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17553
&lt;/p&gt;
&lt;p&gt;
OmniACT&#26159;&#19968;&#20010;&#38024;&#23545;&#20195;&#29702;&#29983;&#25104;&#21487;&#25191;&#34892;&#31243;&#24207;&#23436;&#25104;&#35745;&#31639;&#26426;&#20219;&#21153;&#33021;&#21147;&#30340;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;&#65292;&#36229;&#36234;&#20102;&#20256;&#32479;Web&#33258;&#21160;&#21270;&#65292;&#28085;&#30422;&#20102;&#21508;&#31181;&#26700;&#38754;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20960;&#21313;&#24180;&#26469;&#65292;&#20154;&#26426;&#20132;&#20114;&#20174;&#26681;&#26412;&#19978;&#19968;&#30452;&#26159;&#25163;&#21160;&#30340;&#12290;&#21363;&#20351;&#22312;&#20170;&#22825;&#65292;&#20960;&#20046;&#25152;&#26377;&#22312;&#35745;&#31639;&#26426;&#19978;&#36827;&#34892;&#30340;&#39640;&#25928;&#24037;&#20316;&#37117;&#38656;&#35201;&#20154;&#31867;&#22312;&#27599;&#19968;&#27493;&#37117;&#25552;&#20379;&#36755;&#20837;&#12290;&#34394;&#25311;&#20027;&#21160;&#26234;&#33021;&#20195;&#34920;&#20102;&#33258;&#21160;&#21270;&#35768;&#22810;&#36825;&#20123;&#29712;&#30862;&#20219;&#21153;&#30340;&#19968;&#20010;&#28608;&#21160;&#20154;&#24515;&#30340;&#27493;&#39588;&#12290;&#34394;&#25311;&#20195;&#29702;&#23558;&#20351;&#25216;&#26415;&#33021;&#21147;&#26377;&#38480;&#30340;&#29992;&#25143;&#33021;&#22815;&#20805;&#20998;&#21033;&#29992;&#35745;&#31639;&#26426;&#31995;&#32479;&#30340;&#21508;&#31181;&#21487;&#33021;&#24615;&#12290;&#23427;&#20204;&#36824;&#21487;&#20197;&#23454;&#29616;&#39640;&#25928;&#22320;&#31616;&#21270;&#35768;&#22810;&#35745;&#31639;&#26426;&#20219;&#21153;&#65292;&#20174;&#26085;&#21382;&#31649;&#29702;&#21040;&#22797;&#26434;&#30340;&#26053;&#34892;&#39044;&#35746;&#65292;&#20943;&#23569;&#20154;&#31867;&#24178;&#39044;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102; OmniACT&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#20195;&#29702;&#29983;&#25104;&#21487;&#25191;&#34892;&#31243;&#24207;&#26469;&#23436;&#25104;&#35745;&#31639;&#26426;&#20219;&#21153;&#33021;&#21147;&#30340;&#39318;&#20010;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;&#12290;&#25105;&#20204;&#30340;&#33539;&#22260;&#36229;&#36234;&#20102;&#20256;&#32479;&#30340;Web&#33258;&#21160;&#21270;&#65292;&#28085;&#30422;&#20102;&#21508;&#31181;&#26700;&#38754;&#24212;&#29992;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#35832;&#22914;"&#25773;&#25918;&#19979;&#19968;&#39318;&#27468;"&#20043;&#31867;&#30340;&#22522;&#26412;&#20219;&#21153;&#65292;&#20197;&#21450;&#26356;&#20026;&#38271;&#26399;&#30340;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17553v1 Announce Type: new  Abstract: For decades, human-computer interaction has fundamentally been manual. Even today, almost all productive work done on the computer necessitates human input at every step. Autonomous virtual agents represent an exciting step in automating many of these menial tasks. Virtual agents would empower users with limited technical proficiency to harness the full possibilities of computer systems. They could also enable the efficient streamlining of numerous computer tasks, ranging from calendar management to complex travel bookings, with minimal human intervention. In this paper, we introduce OmniACT, the first-of-a-kind dataset and benchmark for assessing an agent's capability to generate executable programs to accomplish computer tasks. Our scope extends beyond traditional web automation, covering a diverse range of desktop applications. The dataset consists of fundamental tasks such as "Play the next song", as well as longer horizon tasks such
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#19977;&#23618;&#26550;&#26500;&#30340;&#32039;&#24613;&#32531;&#23384;&#32593;&#32476;&#65292;&#32467;&#21512;&#20102;&#32534;&#30721;&#32531;&#23384;&#25216;&#26415;&#65292;&#36890;&#36807;&#26080;&#20154;&#26426;&#20043;&#38388;&#21327;&#21516;&#19978;&#20256;&#23454;&#29616;&#28798;&#38590;&#22320;&#22270;&#30340;&#21487;&#38752;&#20256;&#36755;&#65292;&#21516;&#26102;&#24341;&#20837;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20248;&#21270;&#20915;&#31574;&#12290;</title><link>https://arxiv.org/abs/2402.17550</link><description>&lt;p&gt;
&#32039;&#24613;&#32531;&#23384;&#65306;&#22522;&#20110;&#32534;&#30721;&#32531;&#23384;&#30340;&#32039;&#24613;&#32593;&#32476;&#21487;&#38752;&#22320;&#22270;&#20256;&#36755;
&lt;/p&gt;
&lt;p&gt;
Emergency Caching: Coded Caching-based Reliable Map Transmission in Emergency Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17550
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#19977;&#23618;&#26550;&#26500;&#30340;&#32039;&#24613;&#32531;&#23384;&#32593;&#32476;&#65292;&#32467;&#21512;&#20102;&#32534;&#30721;&#32531;&#23384;&#25216;&#26415;&#65292;&#36890;&#36807;&#26080;&#20154;&#26426;&#20043;&#38388;&#21327;&#21516;&#19978;&#20256;&#23454;&#29616;&#28798;&#38590;&#22320;&#22270;&#30340;&#21487;&#38752;&#20256;&#36755;&#65292;&#21516;&#26102;&#24341;&#20837;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20248;&#21270;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#25937;&#25588;&#20219;&#21153;&#38656;&#35201;&#39640;&#25928;&#30340;&#24863;&#30693;&#21644;&#23454;&#26102;&#20915;&#31574;&#65292;&#36825;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#26377;&#25928;&#30340;&#25968;&#25454;&#25910;&#38598;&#21644;&#22788;&#29702;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#20391;&#37325;&#20110;&#25968;&#25454;&#25910;&#38598;&#21644;&#21487;&#38752;&#20256;&#36755;&#30340;&#32039;&#24613;&#32531;&#23384;&#32593;&#32476;&#30340;&#19977;&#23618;&#26550;&#26500;&#65292;&#21033;&#29992;&#39640;&#25928;&#24863;&#30693;&#21644;&#36793;&#32536;&#32531;&#23384;&#25216;&#26415;&#12290;&#22522;&#20110;&#36825;&#20010;&#26550;&#26500;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#25972;&#21512;&#32534;&#30721;&#32531;&#23384;&#25216;&#26415;&#30340;&#28798;&#38590;&#22320;&#22270;&#25910;&#38598;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#22312;&#26080;&#20154;&#26426;&#20043;&#38388;&#31574;&#30053;&#24615;&#22320;&#32531;&#23384;&#22320;&#22270;&#30340;&#32534;&#30721;&#29255;&#27573;&#65292;&#20419;&#36827;&#21327;&#21516;&#19978;&#20256;&#20197;&#22686;&#24378;&#20256;&#36755;&#30340;&#21487;&#38752;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#27010;&#29575;&#27169;&#22411;&#26469;&#35780;&#20272;&#28798;&#38590;&#22320;&#22270;&#30340;&#26377;&#25928;&#24674;&#22797;&#21306;&#22495;&#12290;&#20026;&#20102;&#23454;&#29616;&#25928;&#29992;&#26368;&#22823;&#21270;&#30340;&#30446;&#26631;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#30340;&#31639;&#27861;&#65292;&#20849;&#21516;&#20915;&#31574;&#21327;&#20316;&#26080;&#20154;&#26426;&#36873;&#25321;&#12289;&#24102;&#23485;&#20998;&#37197;&#31561;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17550v1 Announce Type: cross  Abstract: Many rescue missions demand effective perception and real-time decision making, which highly rely on effective data collection and processing. In this study, we propose a three-layer architecture of emergency caching networks focusing on data collection and reliable transmission, by leveraging efficient perception and edge caching technologies. Based on this architecture, we propose a disaster map collection framework that integrates coded caching technologies. Our framework strategically caches coded fragments of maps across unmanned aerial vehicles (UAVs), fostering collaborative uploading for augmented transmission reliability. Additionally, we establish a comprehensive probability model to assess the effective recovery area of disaster maps. Towards the goal of utility maximization, we propose a deep reinforcement learning (DRL) based algorithm that jointly makes decisions about cooperative UAVs selection, bandwidth allocation and 
&lt;/p&gt;</description></item><item><title>CoCoA&#26159;&#19968;&#27454;&#22522;&#20110;&#35748;&#30693;&#34892;&#20026;&#30103;&#27861;&#25216;&#26415;&#30340;&#24515;&#29702;&#36741;&#23548;&#20195;&#29702;&#65292;&#36890;&#36807;&#26500;&#24314;&#35760;&#24518;&#31995;&#32479;&#31649;&#29702;&#20449;&#24687;&#12289;&#25552;&#21462;&#39640;&#23618;&#35265;&#35299;&#65292;&#24341;&#20837;&#21160;&#24577;&#25552;&#31034;&#28789;&#27963;&#36816;&#29992;CBT&#25216;&#26415;&#65292;&#29983;&#25104;&#36866;&#24403;&#22238;&#24212;&#65292;&#24182;&#22312;&#19982;Character.ai&#35282;&#33394;&#30340;&#23545;&#35805;&#20013;&#23637;&#31034;&#20986;&#26174;&#33879;&#24046;&#24322;&#12290;</title><link>https://arxiv.org/abs/2402.17546</link><description>&lt;p&gt;
COCOA: &#22522;&#20110;&#35748;&#30693;&#22833;&#35843;&#21644;&#21160;&#24577;&#25552;&#31034;&#30340;CBT&#23545;&#35805;&#36741;&#23548;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
COCOA: CBT-based Conversational Counseling Agent using Memory Specialized in Cognitive Distortions and Dynamic Prompt
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17546
&lt;/p&gt;
&lt;p&gt;
CoCoA&#26159;&#19968;&#27454;&#22522;&#20110;&#35748;&#30693;&#34892;&#20026;&#30103;&#27861;&#25216;&#26415;&#30340;&#24515;&#29702;&#36741;&#23548;&#20195;&#29702;&#65292;&#36890;&#36807;&#26500;&#24314;&#35760;&#24518;&#31995;&#32479;&#31649;&#29702;&#20449;&#24687;&#12289;&#25552;&#21462;&#39640;&#23618;&#35265;&#35299;&#65292;&#24341;&#20837;&#21160;&#24577;&#25552;&#31034;&#28789;&#27963;&#36816;&#29992;CBT&#25216;&#26415;&#65292;&#29983;&#25104;&#36866;&#24403;&#22238;&#24212;&#65292;&#24182;&#22312;&#19982;Character.ai&#35282;&#33394;&#30340;&#23545;&#35805;&#20013;&#23637;&#31034;&#20986;&#26174;&#33879;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#23545;&#25552;&#20379;&#24515;&#29702;&#20581;&#24247;&#25252;&#29702;&#30340;&#23545;&#35805;&#20195;&#29702;&#30340;&#38656;&#27714;&#25345;&#32493;&#22686;&#21152;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#27454;&#24515;&#29702;&#36741;&#23548;&#20195;&#29702;CoCoA&#65292;&#24212;&#29992;&#35748;&#30693;&#34892;&#20026;&#30103;&#27861;&#65288;CBT&#65289;&#25216;&#26415;&#26469;&#35782;&#21035;&#21644;&#35299;&#20915;&#23458;&#25143;&#38472;&#36848;&#20013;&#22266;&#26377;&#30340;&#35748;&#30693;&#22833;&#35843;&#38382;&#39064;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#35760;&#24518;&#31995;&#32479;&#65292;&#20197;&#20415;&#26377;&#25928;&#31649;&#29702;&#36741;&#23548;&#25152;&#38656;&#30340;&#20449;&#24687;&#65292;&#24182;&#20174;&#23458;&#25143;&#30340;&#35805;&#35821;&#20013;&#25552;&#21462;&#39640;&#23618;&#27425;&#30340;&#35265;&#35299;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#30830;&#20445;&#36741;&#23548;&#20195;&#29702;&#29983;&#25104;&#36866;&#24403;&#30340;&#22238;&#24212;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#21160;&#24577;&#25552;&#31034;&#65292;&#28789;&#27963;&#24212;&#29992;CBT&#25216;&#26415;&#65292;&#24182;&#20419;&#36827;&#20449;&#24687;&#30340;&#36866;&#24403;&#26816;&#32034;&#12290;&#25105;&#20204;&#22312;CoCoA&#21644;Character.ai&#30340;&#35282;&#33394;&#20043;&#38388;&#36827;&#34892;&#20102;&#23545;&#35805;&#65292;&#21019;&#24314;&#20102;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#30340;&#25968;&#25454;&#38598;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35201;&#27714;GPT&#35780;&#20272;&#26500;&#24314;&#30340;&#36741;&#23548;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#34920;&#29616;&#20986;&#19982;&#20854;&#20182;&#26041;&#27861;&#30340;&#32479;&#35745;&#26174;&#33879;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17546v1 Announce Type: new  Abstract: The demand for conversational agents that provide mental health care is consistently increasing. In this work, we develop a psychological counseling agent, referred to as CoCoA, that applies Cognitive Behavioral Therapy (CBT) techniques to identify and address cognitive distortions inherent in the client's statements. Specifically, we construct a memory system to efficiently manage information necessary for counseling while extracting high-level insights about the client from their utterances. Additionally, to ensure that the counseling agent generates appropriate responses, we introduce dynamic prompting to flexibly apply CBT techniques and facilitate the appropriate retrieval of information. We conducted dialogues between CoCoA and characters from Character.ai, creating a dataset for evaluation. Then, we asked GPT to evaluate the constructed counseling dataset, and our model demonstrated a statistically significant difference from othe
&lt;/p&gt;</description></item><item><title>Nissist&#21033;&#29992;TSGs&#21644;&#20107;&#25925;&#32531;&#35299;&#21382;&#21490;&#25552;&#20379;&#20027;&#21160;&#24314;&#35758;&#65292;&#20943;&#23569;&#20154;&#20026;&#24178;&#39044;&#65292;&#20197;&#25552;&#39640;&#20225;&#19994;&#32423;&#20113;&#26381;&#21153;&#30340;&#20107;&#25925;&#31649;&#29702;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.17531</link><description>&lt;p&gt;
Nissist&#65306;&#22522;&#20110;&#25925;&#38556;&#25490;&#38500;&#25351;&#21335;&#30340;&#20107;&#25925;&#32531;&#35299;&#21103;&#39550;&#39542;
&lt;/p&gt;
&lt;p&gt;
Nissist: An Incident Mitigation Copilot based on Troubleshooting Guides
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17531
&lt;/p&gt;
&lt;p&gt;
Nissist&#21033;&#29992;TSGs&#21644;&#20107;&#25925;&#32531;&#35299;&#21382;&#21490;&#25552;&#20379;&#20027;&#21160;&#24314;&#35758;&#65292;&#20943;&#23569;&#20154;&#20026;&#24178;&#39044;&#65292;&#20197;&#25552;&#39640;&#20225;&#19994;&#32423;&#20113;&#26381;&#21153;&#30340;&#20107;&#25925;&#31649;&#29702;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#25928;&#30340;&#20107;&#25925;&#31649;&#29702;&#23545;&#20225;&#19994;&#32423;&#20113;&#26381;&#21153;&#30340;&#39034;&#30021;&#36816;&#20316;&#33267;&#20851;&#37325;&#35201;&#12290; &#20026;&#20102;&#21152;&#36895;&#20107;&#25925;&#32531;&#35299;&#65292;&#26381;&#21153;&#22242;&#38431;&#23558;&#25925;&#38556;&#25490;&#38500;&#30693;&#35782;&#32534;&#35793;&#25104;&#20379;&#20540;&#29677;&#24037;&#31243;&#24072;&#65288;OCEs&#65289;&#35775;&#38382;&#30340;&#25925;&#38556;&#25490;&#38500;&#25351;&#21335;&#65288;TSGs&#65289;&#12290; &#23613;&#31649;&#33258;&#21160;&#21270;&#27969;&#27700;&#32447;&#24050;&#33021;&#22815;&#35299;&#20915;&#26368;&#24120;&#35265;&#21644;&#31616;&#21333;&#30340;&#20107;&#25925;&#65292;&#20294;&#20173;&#23384;&#22312;&#38656;&#35201;OCE&#24178;&#39044;&#30340;&#22797;&#26434;&#20107;&#25925;&#12290; &#28982;&#32780;&#65292;TSGs&#36890;&#24120;&#26159;&#38750;&#32467;&#26500;&#21270;&#21644;&#19981;&#23436;&#25972;&#30340;&#65292;&#36825;&#38656;&#35201;OCE&#25163;&#21160;&#35299;&#37322;&#65292;&#23548;&#33268;&#20540;&#29677;&#30130;&#21171;&#21644;&#29983;&#20135;&#21147;&#19979;&#38477;&#65292;&#29305;&#21035;&#26159;&#26032;&#20837;&#32844;&#30340;OCE&#12290; &#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Nissist&#65292;&#23427;&#21033;&#29992;TSGs&#21644;&#20107;&#25925;&#32531;&#35299;&#21382;&#21490;&#25552;&#20379;&#20027;&#21160;&#24314;&#35758;&#65292;&#20943;&#23569;&#20154;&#20026;&#24178;&#39044;&#12290; &#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;Nissist&#20174;&#38750;&#32467;&#26500;&#21270;TSGs&#21644;&#21382;&#21490;&#20107;&#25925;&#32531;&#35299;&#35752;&#35770;&#20013;&#25552;&#21462;&#35265;&#35299;&#65292;&#24418;&#25104;&#20840;&#38754;&#30340;&#30693;&#35782;&#24211;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17531v1 Announce Type: cross  Abstract: Effective incident management is pivotal for the smooth operation of enterprises-level cloud services. In order to expedite incident mitigation, service teams compile troubleshooting knowledge into Troubleshooting Guides (TSGs) accessible to on-call engineers (OCEs). While automated pipelines are enabled to resolve the most frequent and easy incidents, there still exist complex incidents that require OCEs' intervention. However, TSGs are often unstructured and incomplete, which requires manual interpretation by OCEs, leading to on-call fatigue and decreased productivity, especially among new-hire OCEs. In this work, we propose Nissist which leverages TSGs and incident mitigation histories to provide proactive suggestions, reducing human intervention. Leveraging Large Language Models (LLM), Nissist extracts insights from unstructured TSGs and historical incident mitigation discussions, forming a comprehensive knowledge base. Its multi-a
&lt;/p&gt;</description></item><item><title>&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#22312;&#39044;&#27979;&#19979;&#19968;&#20010;&#35789;&#26102;&#65292;&#26159;&#21542;&#33021;&#22815;&#22797;&#29616;&#20154;&#31867;&#22312;&#36825;&#39033;&#20219;&#21153;&#20013;&#23637;&#31034;&#30340;&#35821;&#35328;&#21464;&#21270;&#24615;</title><link>https://arxiv.org/abs/2402.17527</link><description>&lt;p&gt;
&#39044;&#27979;&#19979;&#19968;&#20010;&#35789;&#65306;&#20154;&#31867;&#22312;&#36825;&#39033;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#19981;&#30830;&#23450;&#24615;&#65292;&#32780;&#35821;&#35328;&#27169;&#22411;_____
&lt;/p&gt;
&lt;p&gt;
Predict the Next Word: &lt;Humans exhibit uncertainty in this task and language models _____&gt;
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17527
&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#22312;&#39044;&#27979;&#19979;&#19968;&#20010;&#35789;&#26102;&#65292;&#26159;&#21542;&#33021;&#22815;&#22797;&#29616;&#20154;&#31867;&#22312;&#36825;&#39033;&#20219;&#21153;&#20013;&#23637;&#31034;&#30340;&#35821;&#35328;&#21464;&#21270;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#26159;&#35757;&#32451;&#29992;&#20110;&#20026;&#20154;&#31867;&#29983;&#25104;&#25991;&#26412;&#20998;&#37197;&#27010;&#29575;&#30340;&#32479;&#35745;&#27169;&#22411;&#12290;&#22240;&#27492;&#65292;&#21512;&#29702;&#36136;&#30097;&#23427;&#20204;&#26159;&#21542;&#24456;&#22909;&#22320;&#36817;&#20284;&#20154;&#31867;&#23637;&#31034;&#30340;&#35821;&#35328;&#21464;&#21270;&#24615;&#12290;&#36825;&#31181;&#24418;&#24335;&#30340;&#32479;&#35745;&#35780;&#20272;&#22312;&#27573;&#33853;&#32423;&#21035;&#19978;&#24456;&#38590;&#25191;&#34892;&#65292;&#22240;&#20026;&#23427;&#38656;&#35201;&#21487;&#25509;&#21463;&#24615;&#21028;&#26029;&#65288;&#21363;&#65292;&#20154;&#31867;&#35780;&#20272;&#65289;&#25110;&#19968;&#20010;&#24378;&#22823;&#30340;&#33258;&#21160;&#20195;&#29702;&#65288;&#36825;&#26159;&#19981;&#24179;&#20961;&#30340;&#65289;&#12290;&#28982;&#32780;&#65292;&#22312;&#21333;&#35789;&#32423;&#21035;&#19978;&#65292;&#36890;&#36807;&#32473;&#23450;&#19968;&#20123;&#19978;&#19979;&#25991;&#65292;&#21487;&#20197;&#36890;&#36807;&#19982;&#19968;&#20010;&#39044;&#20808;&#35760;&#24405;&#30340;&#26367;&#20195;&#21333;&#35789;&#36830;&#32493;&#25968;&#25454;&#38598;&#30340;&#31934;&#30830;&#21305;&#37197;&#26469;&#35780;&#20272;LM&#30340;&#26679;&#26412;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#19968;&#20107;&#23454;&#65292;&#24182;&#35780;&#20272;LM&#37325;&#26032;&#29983;&#25104;&#20154;&#31867;&#65288;&#29305;&#21035;&#26159;&#19968;&#32676;&#33521;&#35821;&#20351;&#29992;&#32773;&#65289;&#22312;&#8220;&#19979;&#19968;&#20010;&#35789;&#39044;&#27979;&#8221;&#20219;&#21153;&#20013;&#23637;&#31034;&#30340;&#21464;&#21270;&#30340;&#33021;&#21147;&#12290;&#36825;&#21487;&#20197;&#34987;&#35270;&#20026;&#19968;&#31181;&#26657;&#20934;&#35780;&#20272;&#65292;&#22312;&#25991;&#26412;&#20998;&#31867;&#30340;&#32972;&#26223;&#19979;&#65292;Baan&#31561;&#20154;&#65288;2022&#24180;&#65289;&#23558;&#20854;&#31216;&#20026;&#23545;&#20154;&#31867;&#19981;&#30830;&#23450;&#24615;&#30340;&#26657;&#20934;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17527v1 Announce Type: cross  Abstract: Language models (LMs) are statistical models trained to assign probability to human-generated text. As such, it is reasonable to question whether they approximate linguistic variability exhibited by humans well. This form of statistical assessment is difficult to perform at the passage level, for it requires acceptability judgements (i.e., human evaluation) or a robust automated proxy (which is non-trivial). At the word level, however, given some context, samples from an LM can be assessed via exact matching against a prerecorded dataset of alternative single-word continuations of the available context. We exploit this fact and evaluate the LM's ability to reproduce variability that humans (in particular, a population of English speakers) exhibit in the 'next word prediction' task. This can be seen as assessing a form of calibration, which, in the context of text classification, Baan et al. (2022) termed calibration to human uncertaint
&lt;/p&gt;</description></item><item><title>QUCE&#26041;&#27861;&#26088;&#22312;&#36890;&#36807;&#20943;&#23569;&#36335;&#24452;&#19981;&#30830;&#23450;&#24615;&#26469;&#37327;&#21270;&#21644;&#32531;&#35299;&#22522;&#20110;&#36335;&#24452;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#20174;&#32780;&#25913;&#21892;&#23545;&#25239;&#24615;&#21453;&#20107;&#23454;&#35299;&#37322;&#30340;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.17516</link><description>&lt;p&gt;
QUCE: &#20943;&#23569;&#21644;&#37327;&#21270;&#22522;&#20110;&#36335;&#24452;&#30340;&#19981;&#30830;&#23450;&#24615;&#20197;&#29983;&#25104;&#23545;&#25239;&#24615;&#21453;&#20107;&#23454;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
QUCE: The Minimisation and Quantification of Path-Based Uncertainty for Generative Counterfactual Explanations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17516
&lt;/p&gt;
&lt;p&gt;
QUCE&#26041;&#27861;&#26088;&#22312;&#36890;&#36807;&#20943;&#23569;&#36335;&#24452;&#19981;&#30830;&#23450;&#24615;&#26469;&#37327;&#21270;&#21644;&#32531;&#35299;&#22522;&#20110;&#36335;&#24452;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#20174;&#32780;&#25913;&#21892;&#23545;&#25239;&#24615;&#21453;&#20107;&#23454;&#35299;&#37322;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17516v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#36328;&#23398;&#31185; &#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#20316;&#20026;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#26368;&#31361;&#20986;&#30340;&#26041;&#27861;&#20043;&#19968;&#12290;DNNs&#30340;&#26377;&#25928;&#24615;&#38543;&#30528;&#26368;&#36817;&#35745;&#31639;&#33021;&#21147;&#30340;&#22686;&#21152;&#32780;&#28608;&#22686;&#65292;&#20351;&#24471;&#36825;&#20123;&#26041;&#27861;&#33021;&#22815;&#25193;&#23637;&#21040;&#22788;&#29702;&#22823;&#25968;&#25454;&#20013;&#30340;&#37325;&#35201;&#22797;&#26434;&#24615;&#20197;&#24212;&#23545;&#39044;&#27979;&#25361;&#25112;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;DNN&#27169;&#22411;&#22797;&#26434;&#24615;&#30340;&#25552;&#39640;&#65292;&#21487;&#35299;&#37322;&#24615;&#38477;&#20302;&#12290;&#38024;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#35832;&#22914;&#23545;&#25239;&#26799;&#24230;&#25972;&#21512;&#65288;AGI&#65289;&#36825;&#26679;&#30340;&#21487;&#35299;&#37322;&#27169;&#22411;&#21033;&#29992;DNN&#25552;&#20379;&#30340;&#22522;&#20110;&#36335;&#24452;&#30340;&#26799;&#24230;&#26469;&#38416;&#26126;&#23427;&#20204;&#30340;&#20915;&#31574;&#12290;&#28982;&#32780;&#65292;&#24403;&#26799;&#24230;&#22312;&#36234;&#30028;&#36335;&#24452;&#36941;&#21382;&#26399;&#38388;&#34920;&#29616;&#20986;&#19981;&#35268;&#21017;&#24615;&#26102;&#65292;&#22522;&#20110;&#36335;&#24452;&#30340;&#35299;&#37322;&#22120;&#30340;&#24615;&#33021;&#21487;&#33021;&#20250;&#21463;&#21040;&#25439;&#23475;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Quantified Uncertainty Counterfactual Explanations&#65288;QUCE&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26088;&#22312;&#20943;&#23569;&#36335;&#24452;&#19981;&#30830;&#23450;&#24615;&#30340;&#26041;&#27861;&#65292;&#20197;&#32531;&#35299;&#36234;&#30028;&#36941;&#21382;&#12290; QUCE&#19981;&#20165;&#22312;&#25552;&#20986;&#35299;&#37322;&#26102;&#37327;&#21270;&#19981;&#30830;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17516v1 Announce Type: cross  Abstract: Deep Neural Networks (DNNs) stand out as one of the most prominent approaches within the Machine Learning (ML) domain. The efficacy of DNNs has surged alongside recent increases in computational capacity, allowing these approaches to scale to significant complexities for addressing predictive challenges in big data. However, as the complexity of DNN models rises, interpretability diminishes. In response to this challenge, explainable models such as Adversarial Gradient Integration (AGI) leverage path-based gradients provided by DNNs to elucidate their decisions. Yet the performance of path-based explainers can be compromised when gradients exhibit irregularities during out-of-distribution path traversal. In this context, we introduce Quantified Uncertainty Counterfactual Explanations (QUCE), a method designed to mitigate out-of-distribution traversal by minimizing path uncertainty. QUCE not only quantifies uncertainty when presenting e
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#37325;&#26032;&#24605;&#32771;&#20114;&#20449;&#24687;&#22312;&#35821;&#35328;&#26465;&#20214;&#19979;&#30340;&#20316;&#29992;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;LCSD&#65292;&#20197;&#26080;&#30417;&#30563;&#30340;&#26041;&#24335;&#26368;&#22823;&#21270;&#35821;&#35328;&#21644;&#25216;&#33021;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#12290;</title><link>https://arxiv.org/abs/2402.17511</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#35821;&#35328;&#26465;&#20214;&#19979;&#30340;&#20114;&#20449;&#24687;&#23545;&#27169;&#20223;&#23398;&#20064;&#20013;&#25216;&#33021;&#21457;&#29616;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Rethinking Mutual Information for Language Conditioned Skill Discovery on Imitation Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17511
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#37325;&#26032;&#24605;&#32771;&#20114;&#20449;&#24687;&#22312;&#35821;&#35328;&#26465;&#20214;&#19979;&#30340;&#20316;&#29992;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;LCSD&#65292;&#20197;&#26080;&#30417;&#30563;&#30340;&#26041;&#24335;&#26368;&#22823;&#21270;&#35821;&#35328;&#21644;&#25216;&#33021;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17511v1 &#20844;&#21578;&#31867;&#22411;:&#36328;&#36234; &#25688;&#35201;:&#35821;&#35328;&#26465;&#20214;&#19979;&#30340;&#26426;&#22120;&#20154;&#34892;&#20026;&#36890;&#36807;&#23558;&#20154;&#31867;&#21629;&#20196;&#25110;&#25351;&#20196;&#19982;&#24863;&#30693;&#21644;&#21160;&#20316;&#30456;&#20851;&#32852;&#65292;&#22312;&#25191;&#34892;&#22797;&#26434;&#20219;&#21153;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#22522;&#20110;&#19981;&#21463;&#38480;&#21046;&#30340;&#35821;&#35328;&#25351;&#20196;&#26500;&#25104;&#38271;&#35270;&#36317;&#20219;&#21153;&#30340;&#33021;&#21147;&#38656;&#35201;&#33719;&#24471;&#21508;&#31181;&#36890;&#29992;&#25216;&#33021;&#38598;&#12290;&#28982;&#32780;&#65292;&#22312;&#27809;&#26377;&#22806;&#37096;&#22870;&#21169;&#25110;&#20154;&#31867;&#30417;&#30563;&#30340;&#32806;&#21512;&#21644;&#38271;&#35270;&#36317;&#29615;&#22659;&#20013;&#33719;&#24471;&#22266;&#26377;&#21407;&#22987;&#25216;&#33021;&#23384;&#22312;&#37325;&#22823;&#25361;&#25112;&#12290;&#26412;&#25991;&#20174;&#25968;&#23398;&#35282;&#24230;&#35780;&#20272;&#20102;&#25216;&#33021;&#21644;&#35821;&#35328;&#25351;&#20196;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#22312;&#35821;&#35328;&#26465;&#20214;&#31574;&#30053;&#23398;&#20064;&#26694;&#26550;&#20869;&#37319;&#29992;&#20102;&#20004;&#31181;&#24418;&#24335;&#30340;&#20114;&#20449;&#24687;&#12290;&#20026;&#20102;&#20197;&#26080;&#30417;&#30563;&#26041;&#24335;&#26368;&#22823;&#21270;&#35821;&#35328;&#21644;&#25216;&#33021;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#35821;&#35328;&#26465;&#20214;&#19979;&#25216;&#33021;&#21457;&#29616;&#65288;LCSD&#65289;&#30340;&#31471;&#21040;&#31471;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#21033;&#29992;&#30690;&#37327;&#37327;&#21270;&#26469;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17511v1 Announce Type: cross  Abstract: Language-conditioned robot behavior plays a vital role in executing complex tasks by associating human commands or instructions with perception and actions. The ability to compose long-horizon tasks based on unconstrained language instructions necessitates the acquisition of a diverse set of general-purpose skills. However, acquiring inherent primitive skills in a coupled and long-horizon environment without external rewards or human supervision presents significant challenges. In this paper, we evaluate the relationship between skills and language instructions from a mathematical perspective, employing two forms of mutual information within the framework of language-conditioned policy learning. To maximize the mutual information between language and skills in an unsupervised manner, we propose an end-to-end imitation learning approach known as Language Conditioned Skill Discovery (LCSD). Specifically, we utilize vector quantization to
&lt;/p&gt;</description></item><item><title>&#22312;&#35270;&#35273;-&#35821;&#35328;&#34920;&#31034;&#23398;&#20064;&#20013;&#65292;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35757;&#32451;&#21644;&#35780;&#20272;&#26694;&#26550;&#65292;&#24341;&#20837;&#20102;&#21512;&#25104;&#25463;&#24452;&#26469;&#25506;&#31350;&#23545;&#27604;&#35757;&#32451;&#26159;&#21542;&#36275;&#20197;&#23398;&#20064;&#21040;&#21253;&#21547;&#25152;&#26377;&#20449;&#24687;&#30340;&#20219;&#21153;&#26368;&#20248;&#34920;&#31034;&#12290;</title><link>https://arxiv.org/abs/2402.17510</link><description>&lt;p&gt;
&#31034;&#33539;&#21644;&#20943;&#23569;&#35270;&#35273;&#35821;&#35328;&#34920;&#31034;&#23398;&#20064;&#20013;&#30340;&#25463;&#24452;
&lt;/p&gt;
&lt;p&gt;
Demonstrating and Reducing Shortcuts in Vision-Language Representation Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17510
&lt;/p&gt;
&lt;p&gt;
&#22312;&#35270;&#35273;-&#35821;&#35328;&#34920;&#31034;&#23398;&#20064;&#20013;&#65292;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35757;&#32451;&#21644;&#35780;&#20272;&#26694;&#26550;&#65292;&#24341;&#20837;&#20102;&#21512;&#25104;&#25463;&#24452;&#26469;&#25506;&#31350;&#23545;&#27604;&#35757;&#32451;&#26159;&#21542;&#36275;&#20197;&#23398;&#20064;&#21040;&#21253;&#21547;&#25152;&#26377;&#20449;&#24687;&#30340;&#20219;&#21153;&#26368;&#20248;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17510v1 &#20844;&#21578;&#31867;&#22411;: &#36328;&#39046;&#22495; &#25688;&#35201;: &#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;(VLMs)&#20027;&#35201;&#20381;&#36182;&#23545;&#27604;&#35757;&#32451;&#26469;&#23398;&#20064;&#22270;&#20687;&#21644;&#26631;&#39064;&#30340;&#36890;&#29992;&#34920;&#31034;&#12290;&#25105;&#20204;&#20851;&#27880;&#30340;&#24773;&#20917;&#26159;&#24403;&#19968;&#20010;&#22270;&#20687;&#19982;&#22810;&#20010;&#26631;&#39064;&#30456;&#20851;&#32852;&#26102;&#65292;&#27599;&#20010;&#26631;&#39064;&#26082;&#21253;&#21547;&#25152;&#26377;&#26631;&#39064;&#20849;&#20139;&#30340;&#20449;&#24687;&#65292;&#21448;&#21253;&#21547;&#20851;&#20110;&#22270;&#20687;&#22330;&#26223;&#30340;&#27599;&#20010;&#26631;&#39064;&#29420;&#29305;&#30340;&#20449;&#24687;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#23578;&#19981;&#28165;&#26970;&#23545;&#27604;&#25439;&#22833;&#26159;&#21542;&#36275;&#20197;&#23398;&#20064;&#21253;&#21547;&#26631;&#39064;&#25552;&#20379;&#30340;&#25152;&#26377;&#20449;&#24687;&#30340;&#20219;&#21153;&#26368;&#20248;&#34920;&#31034;&#65292;&#36824;&#26159;&#23545;&#27604;&#23398;&#20064;&#35774;&#32622;&#26159;&#21542;&#40723;&#21169;&#23398;&#20064;&#26368;&#23567;&#21270;&#23545;&#27604;&#25439;&#22833;&#30340;&#31616;&#21333;&#25463;&#24452;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#35270;&#35273;-&#35821;&#35328;&#30340;&#21512;&#25104;&#25463;&#24452;&#65306;&#19968;&#31181;&#35757;&#32451;&#21644;&#35780;&#20272;&#26694;&#26550;&#65292;&#22312;&#20854;&#20013;&#25105;&#20204;&#21521;&#22270;&#20687;-&#25991;&#26412;&#25968;&#25454;&#27880;&#20837;&#21512;&#25104;&#25463;&#24452;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#65292;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#25110;&#29992;&#21253;&#21547;&#36825;&#20123;&#21512;&#25104;&#25463;&#24452;&#30340;&#25968;&#25454;&#24494;&#35843;&#30340;&#23545;&#27604;VLMs&#20027;&#35201;&#23398;&#20064;&#20195;&#34920;&#25463;&#24452;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17510v1 Announce Type: cross  Abstract: Vision-language models (VLMs) mainly rely on contrastive training to learn general-purpose representations of images and captions. We focus on the situation when one image is associated with several captions, each caption containing both information shared among all captions and unique information per caption about the scene depicted in the image. In such cases, it is unclear whether contrastive losses are sufficient for learning task-optimal representations that contain all the information provided by the captions or whether the contrastive learning setup encourages the learning of a simple shortcut that minimizes contrastive loss. We introduce synthetic shortcuts for vision-language: a training and evaluation framework where we inject synthetic shortcuts into image-text data. We show that contrastive VLMs trained from scratch or fine-tuned with data containing these synthetic shortcuts mainly learn features that represent the shortcu
&lt;/p&gt;</description></item><item><title>&#23558;&#21307;&#30103;&#20445;&#20581;&#35270;&#20026;&#24207;&#21015;&#24314;&#27169;&#38382;&#39064;&#65292;&#36890;&#36807;&#23558;&#24739;&#32773;&#19982;&#21307;&#30103;&#25552;&#20379;&#32773;&#20043;&#38388;&#30340;&#20132;&#20114;&#34920;&#31034;&#20026;&#20107;&#20214;&#27969;&#65292;&#23454;&#29616;&#23545;&#26410;&#26469;&#20107;&#20214;&#65288;&#22914;&#35786;&#26029;&#21644;&#27835;&#30103;&#36873;&#25321;&#65289;&#36827;&#34892;&#39044;&#27979;&#12290;</title><link>https://arxiv.org/abs/2402.17501</link><description>&lt;p&gt;
&#37325;&#30151;&#30417;&#25252;&#20316;&#20026;&#19968;&#20010;&#22823;&#22411;&#24207;&#21015;&#24314;&#27169;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Intensive Care as One Big Sequence Modeling Problem
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17501
&lt;/p&gt;
&lt;p&gt;
&#23558;&#21307;&#30103;&#20445;&#20581;&#35270;&#20026;&#24207;&#21015;&#24314;&#27169;&#38382;&#39064;&#65292;&#36890;&#36807;&#23558;&#24739;&#32773;&#19982;&#21307;&#30103;&#25552;&#20379;&#32773;&#20043;&#38388;&#30340;&#20132;&#20114;&#34920;&#31034;&#20026;&#20107;&#20214;&#27969;&#65292;&#23454;&#29616;&#23545;&#26410;&#26469;&#20107;&#20214;&#65288;&#22914;&#35786;&#26029;&#21644;&#27835;&#30103;&#36873;&#25321;&#65289;&#36827;&#34892;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21307;&#30103;&#20445;&#20581;&#20013;&#65292;&#24378;&#21270;&#23398;&#20064;&#36890;&#24120;&#28041;&#21450;&#29421;&#31364;&#30340;&#33258;&#21253;&#21547;&#20219;&#21153;&#65292;&#22914;&#33043;&#27602;&#30151;&#39044;&#27979;&#25110;&#40635;&#37257;&#25511;&#21046;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#29992;&#27169;&#22411;&#65288;&#20027;&#35201;&#31034;&#20363;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65289;&#20855;&#26377;&#36229;&#36234;&#29305;&#23450;&#20219;&#21153;&#26041;&#27861;&#30340;&#28508;&#21147;&#65292;&#22240;&#20026;&#23427;&#20204;&#20855;&#26377;&#38544;&#24335;&#36801;&#31227;&#23398;&#20064;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#23454;&#29616;&#20445;&#20581;&#22522;&#30784;&#27169;&#22411;&#30340;&#35757;&#32451;&#20197;&#21450;&#21033;&#29992;&#26368;&#20808;&#36827;&#30340;Transformer&#26550;&#26500;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20445;&#20581;&#20316;&#20026;&#24207;&#21015;&#24314;&#27169;&#30340;&#33539;&#24335;&#65292;&#20854;&#20013;&#24739;&#32773;&#21644;&#21307;&#30103;&#25552;&#20379;&#32773;&#20043;&#38388;&#30340;&#20132;&#20114;&#34987;&#34920;&#31034;&#20026;&#20107;&#20214;&#27969;&#65292;&#35786;&#26029;&#21644;&#27835;&#30103;&#36873;&#25321;&#31561;&#20219;&#21153;&#34987;&#24314;&#27169;&#20026;&#23545;&#27969;&#20013;&#26410;&#26469;&#20107;&#20214;&#30340;&#39044;&#27979;&#12290;&#20026;&#20102;&#22312;&#23454;&#39564;&#20013;&#25506;&#32034;&#36825;&#19968;&#33539;&#24335;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;MIMIC-SEQ&#65292;&#36825;&#26159;&#19968;&#20010;&#24207;&#21015;&#24314;&#27169;&#22522;&#20934;&#65292;&#36890;&#36807;&#23558;&#26469;&#33258;MIMIC-IV&#25968;&#25454;&#38598;&#30340;&#24322;&#26500;&#20020;&#24202;&#35760;&#24405;&#36716;&#25442;&#20026;&#19968;&#31181;&#32479;&#19968;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17501v1 Announce Type: cross  Abstract: Reinforcement Learning in Healthcare is typically concerned with narrow self-contained tasks such as sepsis prediction or anesthesia control. However, previous research has demonstrated the potential of generalist models (the prime example being Large Language Models) to outperform task-specific approaches due to their capability for implicit transfer learning. To enable training of foundation models for Healthcare as well as leverage the capabilities of state of the art Transformer architectures, we propose the paradigm of Healthcare as Sequence Modeling, in which interaction between the patient and the healthcare provider is represented as an event stream and tasks like diagnosis and treatment selection are modeled as prediction of future events in the stream. To explore this paradigm experimentally we develop MIMIC-SEQ, a sequence modeling benchmark derived by translating heterogenous clinical records from MIMIC-IV dataset into a un
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#20171;&#32461;&#20102;Emotional Voice Messages (EMOVOME)&#25968;&#25454;&#24211;&#65292;&#20854;&#20013;&#21253;&#21547;&#26469;&#33258;100&#21517;&#35199;&#29677;&#29273;&#35828;&#35805;&#32773;&#30340;999&#26465;&#33258;&#21457;&#35821;&#38899;&#28040;&#24687;&#65292;&#36890;&#36807;&#19987;&#23478;&#21644;&#38750;&#19987;&#23478;&#30340;&#26631;&#35760;&#23454;&#29616;&#20102;&#22312;valence&#21644;arousal&#32500;&#24230;&#19978;&#30340;&#24773;&#24863;&#35782;&#21035;&#65292;&#24182;&#23581;&#35797;&#20351;&#29992;&#35821;&#38899;&#21644;&#25991;&#26412;&#36716;&#24405;&#23454;&#29616;&#24773;&#24863;&#35782;&#21035;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.17496</link><description>&lt;p&gt;
Emotional Voice Messages (EMOVOME)&#25968;&#25454;&#24211;&#65306;&#33258;&#21457;&#24773;&#24863;&#35821;&#38899;&#28040;&#24687;&#20013;&#30340;&#24773;&#24863;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Emotional Voice Messages (EMOVOME) database: emotion recognition in spontaneous voice messages
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17496
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#20171;&#32461;&#20102;Emotional Voice Messages (EMOVOME)&#25968;&#25454;&#24211;&#65292;&#20854;&#20013;&#21253;&#21547;&#26469;&#33258;100&#21517;&#35199;&#29677;&#29273;&#35828;&#35805;&#32773;&#30340;999&#26465;&#33258;&#21457;&#35821;&#38899;&#28040;&#24687;&#65292;&#36890;&#36807;&#19987;&#23478;&#21644;&#38750;&#19987;&#23478;&#30340;&#26631;&#35760;&#23454;&#29616;&#20102;&#22312;valence&#21644;arousal&#32500;&#24230;&#19978;&#30340;&#24773;&#24863;&#35782;&#21035;&#65292;&#24182;&#23581;&#35797;&#20351;&#29992;&#35821;&#38899;&#21644;&#25991;&#26412;&#36716;&#24405;&#23454;&#29616;&#24773;&#24863;&#35782;&#21035;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Emotional Voice Messages (EMOVOME)&#26159;&#19968;&#20010;&#33258;&#21457;&#35821;&#38899;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#26469;&#33258;100&#21517;&#35199;&#29677;&#29273;&#35828;&#35805;&#32773;&#12289;&#30007;&#22899;&#24615;&#24179;&#34913;&#30340;999&#26465;&#30495;&#23454;&#20250;&#35805;&#20013;&#30340;&#38899;&#39057;&#28040;&#24687;&#65292;&#36825;&#20123;&#28040;&#24687;&#36890;&#36807;&#19968;&#20010;&#28040;&#24687;&#24212;&#29992;&#31243;&#24207;&#20135;&#29983;&#65292;&#22312;&#21442;&#19982;&#32773;&#34987;&#25307;&#21215;&#20043;&#21069;&#22312;&#37326;&#22806;&#29615;&#22659;&#20013;&#21046;&#20316;&#65292;&#36991;&#20813;&#20102;&#30001;&#20110;&#23454;&#39564;&#23460;&#29615;&#22659;&#32780;&#20135;&#29983;&#30340;&#20219;&#20309;&#24847;&#35782;&#20559;&#35265;&#12290;&#38899;&#39057;&#25353;&#29031;&#19977;&#20010;&#38750;&#19987;&#23478;&#21644;&#20004;&#20010;&#19987;&#23478;&#30340;&#35748;&#21487;&#22312;valence&#21644;arousal&#32500;&#24230;&#19978;&#36827;&#34892;&#20102;&#26631;&#35760;&#65292;&#28982;&#21518;&#23558;&#23427;&#20204;&#32467;&#21512;&#20197;&#33719;&#24471;&#27599;&#20010;&#32500;&#24230;&#30340;&#26368;&#32456;&#26631;&#31614;&#12290;&#19987;&#23478;&#36824;&#25552;&#20379;&#20102;&#23545;&#24212;&#20110;&#19971;&#31181;&#24773;&#24863;&#31867;&#21035;&#30340;&#39069;&#22806;&#26631;&#31614;&#12290;&#20026;&#20102;&#20026;&#23558;&#26469;&#20351;&#29992;EMOVOME&#36827;&#34892;&#35843;&#26597;&#35774;&#23450;&#22522;&#20934;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#35821;&#38899;&#21644;&#38899;&#39057;&#36716;&#24405;&#26469;&#23454;&#29616;&#24773;&#24863;&#35782;&#21035;&#27169;&#22411;&#12290;&#23545;&#20110;&#35821;&#38899;&#37096;&#20998;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#26631;&#20934;&#30340;eGeMAPS&#29305;&#24449;&#38598;&#21644;&#25903;&#25345;&#21521;&#37327;&#26426;&#65292;&#20998;&#21035;&#33719;&#24471;&#20102;49.27%&#21644;44.71%&#30340;valence&#21644;arousal&#26410;&#21152;&#26435;&#20934;&#30830;&#24230;&#12290;&#23545;&#20110;&#25991;&#26412;&#37096;&#20998;&#65292;&#25105;&#20204;&#23545;&#19968;&#20010;&#22810;&#35821;&#35328;BERT&#27169;&#22411;&#36827;&#34892;&#20102;&#24494;&#35843;&#65292;&#24182;&#23454;&#29616;&#20102;61%&#30340;&#24773;&#24863;&#35782;&#21035;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17496v1 Announce Type: cross  Abstract: Emotional Voice Messages (EMOVOME) is a spontaneous speech dataset containing 999 audio messages from real conversations on a messaging app from 100 Spanish speakers, gender balanced. Voice messages were produced in-the-wild conditions before participants were recruited, avoiding any conscious bias due to laboratory environment. Audios were labeled in valence and arousal dimensions by three non-experts and two experts, which were then combined to obtain a final label per dimension. The experts also provided an extra label corresponding to seven emotion categories. To set a baseline for future investigations using EMOVOME, we implemented emotion recognition models using both speech and audio transcriptions. For speech, we used the standard eGeMAPS feature set and support vector machines, obtaining 49.27% and 44.71% unweighted accuracy for valence and arousal respectively. For text, we fine-tuned a multilingual BERT model and achieved 61
&lt;/p&gt;</description></item><item><title>&#36825;&#20123;&#33402;&#26415;&#23454;&#36341;&#20851;&#27880;&#20154;&#24037;&#26234;&#33021;&#30340;&#32463;&#27982;&#21644;&#31038;&#20250;&#25919;&#27835;&#21518;&#26524;&#65292;&#36890;&#36807;&#25581;&#31034;AI&#25216;&#26415;&#30340;&#31038;&#20250;&#26681;&#28304;&#21644;&#24378;&#35843;&#20154;&#31867;&#22312;&#20854;&#20013;&#30340;&#35282;&#33394;&#65292;&#25366;&#25496;&#20102;&#25112;&#26415;&#23186;&#20307;&#33402;&#26415;&#23545;&#20844;&#21496;AI&#25919;&#27835;&#20307;&#21046;&#30340;&#24178;&#25200;&#20316;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.17490</link><description>&lt;p&gt;
&#26426;&#26800;&#22303;&#32819;&#20854;&#20154;&#65306;&#25112;&#26415;&#23186;&#20307;&#33402;&#26415;&#19982;&#23545;&#20844;&#21496;AI&#30340;&#25209;&#21028;
&lt;/p&gt;
&lt;p&gt;
The Mechanical Turkness: Tactical Media Art and the Critique of Corporate AI
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17490
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20123;&#33402;&#26415;&#23454;&#36341;&#20851;&#27880;&#20154;&#24037;&#26234;&#33021;&#30340;&#32463;&#27982;&#21644;&#31038;&#20250;&#25919;&#27835;&#21518;&#26524;&#65292;&#36890;&#36807;&#25581;&#31034;AI&#25216;&#26415;&#30340;&#31038;&#20250;&#26681;&#28304;&#21644;&#24378;&#35843;&#20154;&#31867;&#22312;&#20854;&#20013;&#30340;&#35282;&#33394;&#65292;&#25366;&#25496;&#20102;&#25112;&#26415;&#23186;&#20307;&#33402;&#26415;&#23545;&#20844;&#21496;AI&#25919;&#27835;&#20307;&#21046;&#30340;&#24178;&#25200;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;2010&#24180;&#20195;&#20013;&#26399;&#20197;&#26469;&#65292;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#30340;&#24191;&#27867;&#24037;&#19994;&#21270;&#36234;&#26469;&#36234;&#28608;&#21169;&#33402;&#26415;&#23478;&#20204;&#20851;&#27880;&#20854;&#32463;&#27982;&#21644;&#31038;&#20250;&#25919;&#27835;&#21518;&#26524;&#12290;&#26412;&#31456;&#35752;&#35770;&#20102;&#20027;&#39064;&#21270;&#21019;&#36896;&#24615;&#20195;&#29702;&#12289;&#20247;&#21253;&#21171;&#21160;&#21644;&#22996;&#25176;&#33402;&#26415;&#21019;&#20316;&#30340;&#30456;&#20851;&#33402;&#26415;&#23454;&#36341;&#65292;&#25581;&#31034;&#20102;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#30340;&#31038;&#20250;&#26681;&#28304;&#65292;&#24182;&#24378;&#35843;&#20102;&#20154;&#31867;&#22312;&#20854;&#21457;&#23637;&#20013;&#30340;&#31215;&#26497;&#20316;&#29992;&#12290;&#25105;&#20851;&#27880;&#30340;&#26159;&#37027;&#20123;&#35799;&#24847;&#29305;&#24449;&#34920;&#26126;&#24403;&#20195;AI&#24433;&#21709;&#30340;&#31185;&#23398;&#12289;&#25216;&#26415;&#12289;&#32463;&#27982;&#21644;&#31038;&#20250;&#30340;&#24191;&#27867;&#38382;&#39064;&#30340;&#20316;&#21697;&#12290;&#36890;&#36807;&#25506;&#35752;&#23427;&#20204;&#22312;&#39072;&#35206;&#20844;&#21496;AI&#25919;&#27835;&#20307;&#21046;&#20013;&#30340;&#26377;&#25928;&#24615;&#30340;&#27010;&#24565;&#12289;&#26041;&#27861;&#21644;&#36947;&#24503;&#26041;&#38754;&#65292;&#25105;&#30830;&#23450;&#20102;&#24433;&#21709;&#20854;&#25112;&#26415;&#24433;&#21709;&#21147;&#30340;&#20960;&#20010;&#38382;&#39064;&#65292;&#24182;&#27010;&#36848;&#20102;&#35299;&#20915;&#25361;&#25112;&#24182;&#25512;&#36827;&#35813;&#39046;&#22495;&#30340;&#28508;&#22312;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17490v1 Announce Type: cross  Abstract: The extensive industrialization of artificial intelligence (AI) since the mid-2010s has increasingly motivated artists to address its economic and sociopolitical consequences. In this chapter, I discuss interrelated art practices that thematize creative agency, crowdsourced labor, and delegated artmaking to reveal the social rootage of AI technologies and underline the productive human roles in their development. I focus on works whose poetic features indicate broader issues of contemporary AI-influenced science, technology, economy, and society. By exploring the conceptual, methodological, and ethical aspects of their effectiveness in disrupting the political regime of corporate AI, I identify several problems that affect their tactical impact and outline potential avenues for tackling the challenges and advancing the field.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23558;&#36229;&#22768;&#33292;&#22836;&#25104;&#20687;&#19982;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#35782;&#21035;&#24188;&#20799;&#26399;&#35821;&#38899;&#38556;&#30861;&#30340;&#25216;&#26415;&#35299;&#20915;&#26041;&#26696;&#65292;&#26088;&#22312;&#25552;&#39640;UTI&#20998;&#26512;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.17482</link><description>&lt;p&gt;
&#20351;&#29992;&#21407;&#22987;&#36229;&#22768;&#27874;&#25104;&#20687;&#25216;&#26415;&#33258;&#21160;&#20998;&#31867;&#20799;&#31461;&#35821;&#38899;&#20013;&#30340;&#38899;&#32032;&#29255;&#27573;
&lt;/p&gt;
&lt;p&gt;
Automated Classification of Phonetic Segments in Child Speech Using Raw Ultrasound Imaging
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17482
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23558;&#36229;&#22768;&#33292;&#22836;&#25104;&#20687;&#19982;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#35782;&#21035;&#24188;&#20799;&#26399;&#35821;&#38899;&#38556;&#30861;&#30340;&#25216;&#26415;&#35299;&#20915;&#26041;&#26696;&#65292;&#26088;&#22312;&#25552;&#39640;UTI&#20998;&#26512;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#38899;&#38556;&#30861;&#65288;SSD&#65289;&#34987;&#23450;&#20041;&#20026;&#35821;&#38899;&#20135;&#29983;&#30340;&#25345;&#32493;&#38556;&#30861;&#65292;&#23548;&#33268;&#35821;&#38899;&#21487;&#29702;&#35299;&#24615;&#38477;&#20302;&#65292;&#38459;&#30861;&#35328;&#35821;&#20132;&#27969;&#12290;&#26412;&#25991;&#19987;&#27880;&#20110;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#25216;&#26415;&#35299;&#20915;&#26041;&#26696;&#65292;&#23558;&#36229;&#22768;&#33292;&#22836;&#25104;&#20687;&#65288;UTI&#65289;&#19982;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#25512;&#36827;&#23545;&#24188;&#20799;&#26399;SSD&#30340;&#33258;&#21160;&#35786;&#26029;&#12290;&#24341;&#20837;&#30340;FusionNet&#27169;&#22411;&#23558;UTI&#25968;&#25454;&#19982;&#25552;&#21462;&#30340;&#32441;&#29702;&#29305;&#24449;&#32467;&#21512;&#36215;&#26469;&#65292;&#29992;&#20110;&#23545;UTI&#36827;&#34892;&#20998;&#31867;&#65292;&#26088;&#22312;&#25552;&#39640;UTI&#20998;&#26512;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17482v1 Announce Type: cross  Abstract: Speech sound disorder (SSD) is defined as a persistent impairment in speech sound production leading to reduced speech intelligibility and hindered verbal communication. Early recognition and intervention of children with SSD and timely referral to speech and language therapists (SLTs) for treatment are crucial. Automated detection of speech impairment is regarded as an efficient method for examining and screening large populations. This study focuses on advancing the automatic diagnosis of SSD in early childhood by proposing a technical solution that integrates ultrasound tongue imaging (UTI) with deep-learning models. The introduced FusionNet model combines UTI data with the extracted texture features to classify UTI. The overarching aim is to elevate the accuracy and efficiency of UTI analysis, particularly for classifying speech sounds associated with SSD. This study compared the FusionNet approach with standard deep-learning metho
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;RAGFormer&#30340;&#26032;&#26694;&#26550;&#65292;&#21516;&#26102;&#23558;&#23616;&#37096;&#21644;&#20840;&#23616;&#29305;&#24449;&#23884;&#20837;&#30446;&#26631;&#33410;&#28857;&#65292;&#20197;&#25913;&#36827;&#27450;&#35784;&#26816;&#27979;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.17472</link><description>&lt;p&gt;
&#36890;&#36807;&#34701;&#21512;&#20840;&#23616;&#21644;&#23616;&#37096;&#20851;&#31995;&#20132;&#20114;&#36827;&#34892;&#27450;&#35784;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Fraud Detection with Binding Global and Local Relational Interaction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17472
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;RAGFormer&#30340;&#26032;&#26694;&#26550;&#65292;&#21516;&#26102;&#23558;&#23616;&#37096;&#21644;&#20840;&#23616;&#29305;&#24449;&#23884;&#20837;&#30446;&#26631;&#33410;&#28857;&#65292;&#20197;&#25913;&#36827;&#27450;&#35784;&#26816;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#24050;&#34987;&#35777;&#26126;&#23545;&#20110;&#27450;&#35784;&#26816;&#27979;&#20855;&#26377;&#26377;&#25928;&#24615;&#65292;&#22240;&#20026;&#23427;&#33021;&#22815;&#22312;&#25972;&#20307;&#35270;&#35282;&#20013;&#32534;&#30721;&#33410;&#28857;&#20132;&#20114;&#21644;&#32858;&#21512;&#29305;&#24449;&#12290;&#26368;&#36817;&#65292;&#20855;&#26377;&#20986;&#33394;&#24207;&#21015;&#32534;&#30721;&#33021;&#21147;&#30340;Transformer&#32593;&#32476;&#22312;&#25991;&#29486;&#20013;&#20063;&#34920;&#29616;&#20986;&#20248;&#20110;&#20854;&#20182;&#22522;&#20110;GNN&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;GNN&#21644;&#22522;&#20110;Transformer&#30340;&#32593;&#32476;&#21482;&#32534;&#30721;&#25972;&#20010;&#22270;&#30340;&#19968;&#20010;&#35270;&#35282;&#65292;&#32780;GNN&#32534;&#30721;&#20840;&#23616;&#29305;&#24449;&#65292;Transformer&#32593;&#32476;&#32534;&#30721;&#23616;&#37096;&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;&#20808;&#21069;&#30340;&#24037;&#20316;&#24573;&#35270;&#20102;&#20351;&#29992;&#21333;&#29420;&#32593;&#32476;&#32534;&#30721;&#24322;&#26500;&#22270;&#30340;&#20840;&#23616;&#20132;&#20114;&#29305;&#24449;&#65292;&#23548;&#33268;&#24615;&#33021;&#19981;&#20339;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026;Relation-Aware GNN with transFormer&#65288;RAGFormer&#65289;&#30340;&#26032;&#39062;&#26694;&#26550;&#65292;&#23558;&#23616;&#37096;&#21644;&#20840;&#23616;&#29305;&#24449;&#21516;&#26102;&#23884;&#20837;&#30446;&#26631;&#33410;&#28857;&#20013;&#12290;&#36825;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#32593;&#32476;&#24212;&#29992;&#20102;&#19968;&#20010;&#20462;&#25913;&#21518;&#30340;GAGA&#27169;&#22359;&#65292;&#20854;&#20013;&#27599;&#20010;Transformer&#23618;&#21518;&#38754;&#37117;&#36319;&#30528;&#19968;&#20010;&#36328;&#20851;&#31995;&#32858;&#21512;&#23618;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17472v1 Announce Type: cross  Abstract: Graph Neural Network has been proved to be effective for fraud detection for its capability to encode node interaction and aggregate features in a holistic view. Recently, Transformer network with great sequence encoding ability, has also outperformed other GNN-based methods in literatures. However, both GNN-based and Transformer-based networks only encode one perspective of the whole graph, while GNN encodes global features and Transformer network encodes local ones. Furthermore, previous works ignored encoding global interaction features of the heterogeneous graph with separate networks, thus leading to suboptimal performance. In this work, we present a novel framework called Relation-Aware GNN with transFormer (RAGFormer) which simultaneously embeds local and global features into a target node. The simple yet effective network applies a modified GAGA module where each transformer layer is followed by a cross-relation aggregation lay
&lt;/p&gt;</description></item><item><title>&#35813;&#35843;&#30740;&#32508;&#36848;&#20102;&#22312;&#31526;&#21495;&#38899;&#20048;&#29983;&#25104;&#21644;&#20449;&#24687;&#26816;&#32034;&#30740;&#31350;&#20013;&#24212;&#29992;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#27861;&#65292;&#37325;&#28857;&#20851;&#27880;&#20102;&#31526;&#21495;&#38899;&#20048;&#34920;&#31034;&#30340;&#35774;&#35745;&#21644;&#22788;&#29702;&#12290;</title><link>https://arxiv.org/abs/2402.17467</link><description>&lt;p&gt;
&#29992;&#20110;&#31526;&#21495;&#38899;&#20048;&#29983;&#25104;&#21644;&#20449;&#24687;&#26816;&#32034;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#27861;&#65306;&#19968;&#39033;&#35843;&#30740;
&lt;/p&gt;
&lt;p&gt;
Natural Language Processing Methods for Symbolic Music Generation and Information Retrieval: a Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17467
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35843;&#30740;&#32508;&#36848;&#20102;&#22312;&#31526;&#21495;&#38899;&#20048;&#29983;&#25104;&#21644;&#20449;&#24687;&#26816;&#32034;&#30740;&#31350;&#20013;&#24212;&#29992;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#27861;&#65292;&#37325;&#28857;&#20851;&#27880;&#20102;&#31526;&#21495;&#38899;&#20048;&#34920;&#31034;&#30340;&#35774;&#35745;&#21644;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20174;Transformers&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#21462;&#24471;&#31361;&#30772;&#20197;&#26469;&#65292;&#35813;&#27169;&#22411;&#24050;&#22312;&#21508;&#20010;&#39046;&#22495;&#36827;&#34892;&#20102;&#22810;&#31181;&#25913;&#36827;&#12290; &#36825;&#19968;&#36235;&#21183;&#24050;&#32463;&#20256;&#25773;&#21040;&#38899;&#20048;&#20449;&#24687;&#26816;&#32034;&#65288;MIR&#65289;&#39046;&#22495;&#65292;&#21253;&#25324;&#22788;&#29702;&#38899;&#20048;&#25968;&#25454;&#30340;&#30740;&#31350;&#12290; &#20294;&#26159;&#65292;&#22312;MIR&#20013;&#65292;&#21033;&#29992;NLP&#24037;&#20855;&#22788;&#29702;&#31526;&#21495;&#38899;&#20048;&#25968;&#25454;&#30340;&#20570;&#27861;&#24182;&#19981;&#26032;&#39062;&#12290; &#38899;&#20048;&#32463;&#24120;&#34987;&#27604;&#20316;&#35821;&#35328;&#65292;&#22240;&#20026;&#23427;&#20204;&#20855;&#26377;&#22810;&#20010;&#30456;&#20284;&#20043;&#22788;&#65292;&#21253;&#25324;&#25991;&#26412;&#21644;&#38899;&#20048;&#30340;&#24207;&#21015;&#21270;&#34920;&#31034;&#12290; &#36825;&#20123;&#31867;&#27604;&#36824;&#36890;&#36807;MIR&#21644;NLP&#20013;&#30340;&#31867;&#20284;&#20219;&#21153;&#24471;&#21040;&#20307;&#29616;&#12290; &#26412;&#25991;&#32508;&#36848;&#20102;&#24212;&#29992;&#20110;&#31526;&#21495;&#38899;&#20048;&#29983;&#25104;&#21644;&#20449;&#24687;&#26816;&#32034;&#30740;&#31350;&#30340;NLP&#26041;&#27861;&#65292;&#36981;&#24490;&#20004;&#20010;&#26041;&#38754;&#12290;&#25105;&#20204;&#39318;&#20808;&#27010;&#36848;&#20102;&#20174;&#33258;&#28982;&#35821;&#35328;&#24207;&#21015;&#34920;&#31034;&#20013;&#25913;&#32534;&#32780;&#26469;&#30340;&#31526;&#21495;&#38899;&#20048;&#34920;&#31034;&#12290; &#36890;&#36807;&#32771;&#34385;&#31526;&#21495;&#38899;&#20048;&#30340;&#29305;&#23450;&#24615;&#26469;&#35774;&#35745;&#36825;&#20123;&#34920;&#31034;&#12290; &#28982;&#21518;&#36825;&#20123;&#34920;&#31034;&#34987;&#27169;&#22411;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17467v1 Announce Type: cross  Abstract: Several adaptations of Transformers models have been developed in various domains since its breakthrough in Natural Language Processing (NLP). This trend has spread into the field of Music Information Retrieval (MIR), including studies processing music data. However, the practice of leveraging NLP tools for symbolic music data is not novel in MIR. Music has been frequently compared to language, as they share several similarities, including sequential representations of text and music. These analogies are also reflected through similar tasks in MIR and NLP. This survey reviews NLP methods applied to symbolic music generation and information retrieval studies following two axes. We first propose an overview of representations of symbolic music adapted from natural language sequential representations. Such representations are designed by considering the specificities of symbolic music. These representations are then processed by models. S
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26080;&#20195;&#30721;&#32842;&#22825;&#26426;&#22120;&#20154;&#35774;&#35745;&#24037;&#20855;&#65292;&#24110;&#21161;&#25945;&#24072;&#35774;&#35745;&#23450;&#21046;&#30340;&#23545;&#35805;&#27969;&#31243;&#21644;&#32842;&#22825;&#26426;&#22120;&#20154;&#35805;&#35821;&#65292;&#25506;&#35752;&#20102;&#25945;&#24072;&#22312;&#35774;&#35745;&#32842;&#22825;&#26426;&#22120;&#20154;&#26102;&#30340;&#38656;&#27714;&#65292;&#24182;&#23637;&#31034;&#20102;&#25945;&#24072;&#23558;&#33258;&#24049;&#30475;&#20316;&#26159;&#24341;&#23548;&#23398;&#29983;&#21644;&#32842;&#22825;&#26426;&#22120;&#20154;&#34892;&#20026;&#30340;&#21095;&#20316;&#23478;&#30340;&#35266;&#28857;&#12290;</title><link>https://arxiv.org/abs/2402.17456</link><description>&lt;p&gt;
&#19968;&#37096;&#25103;&#21095;&#65306;&#25506;&#35752;&#25945;&#24072;&#22914;&#20309;&#35774;&#35745; LLM &#32842;&#22825;&#26426;&#22120;&#20154;&#20197;&#21327;&#21161;&#38738;&#23569;&#24180;&#38450;&#27490;&#32593;&#32476;&#27450;&#20940;&#25945;&#32946;
&lt;/p&gt;
&lt;p&gt;
A Piece of Theatre: Investigating How Teachers Design LLM Chatbots to Assist Adolescent Cyberbullying Education
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17456
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26080;&#20195;&#30721;&#32842;&#22825;&#26426;&#22120;&#20154;&#35774;&#35745;&#24037;&#20855;&#65292;&#24110;&#21161;&#25945;&#24072;&#35774;&#35745;&#23450;&#21046;&#30340;&#23545;&#35805;&#27969;&#31243;&#21644;&#32842;&#22825;&#26426;&#22120;&#20154;&#35805;&#35821;&#65292;&#25506;&#35752;&#20102;&#25945;&#24072;&#22312;&#35774;&#35745;&#32842;&#22825;&#26426;&#22120;&#20154;&#26102;&#30340;&#38656;&#27714;&#65292;&#24182;&#23637;&#31034;&#20102;&#25945;&#24072;&#23558;&#33258;&#24049;&#30475;&#20316;&#26159;&#24341;&#23548;&#23398;&#29983;&#21644;&#32842;&#22825;&#26426;&#22120;&#20154;&#34892;&#20026;&#30340;&#21095;&#20316;&#23478;&#30340;&#35266;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#21457;&#29616;&#65292;&#32593;&#32476;&#27450;&#20940;&#21361;&#23475;&#38738;&#23569;&#24180;&#30340;&#24515;&#29702;&#20581;&#24247;&#65292;&#25945;&#25480;&#20182;&#20204;&#27491;&#30830;&#30340;&#24178;&#39044;&#26041;&#27861;&#33267;&#20851;&#37325;&#35201;&#12290;&#24043;&#24072;-&#22885;&#20857;&#30740;&#31350;&#34920;&#26126;&#65292;&#32842;&#22825;&#26426;&#22120;&#20154;&#21487;&#20197;&#25193;&#23637;&#20010;&#24615;&#21270;&#21644;&#20114;&#21160;&#24335;&#30340;&#32593;&#32476;&#27450;&#20940;&#25945;&#32946;&#65292;&#20294;&#23454;&#26045;&#36825;&#26679;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#21644;&#24494;&#22937;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#20026; K-12 &#25945;&#24072;&#21019;&#24314;&#20102;&#19968;&#20010;&#26080;&#20195;&#30721;&#32842;&#22825;&#26426;&#22120;&#20154;&#35774;&#35745;&#24037;&#20855;&#12290;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#25552;&#31034;&#38142;&#26465;&#65292;&#25105;&#20204;&#30340;&#24037;&#20855;&#20801;&#35768;&#25945;&#24072;&#21407;&#22411;&#21270;&#23450;&#21046;&#30340;&#23545;&#35805;&#27969;&#31243;&#21644;&#32842;&#22825;&#26426;&#22120;&#20154;&#35805;&#35821;&#12290;&#36890;&#36807;&#25552;&#20379;&#36825;&#20010;&#24037;&#20855;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#25945;&#24072;&#22312;&#35774;&#35745;&#32842;&#22825;&#26426;&#22120;&#20154;&#20197;&#36741;&#21161;&#20182;&#20204;&#30340;&#25945;&#23398;&#26102;&#30340;&#29420;&#29305;&#38656;&#27714;&#65292;&#20197;&#21450;&#32842;&#22825;&#26426;&#22120;&#20154;&#35774;&#35745;&#24037;&#20855;&#22914;&#20309;&#26356;&#22909;&#22320;&#25903;&#25345;&#20182;&#20204;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#25945;&#24072;&#28909;&#24773;&#22320;&#25509;&#21463;&#36825;&#20010;&#24037;&#20855;&#12290;&#27492;&#22806;&#65292;&#20182;&#20204;&#23558;&#33258;&#24049;&#35270;&#20026;&#25351;&#23548;&#23398;&#29983;&#21644;&#32842;&#22825;&#26426;&#22120;&#20154;&#34892;&#20026;&#30340;&#21095;&#20316;&#23478;&#65292;&#21516;&#26102;&#20801;&#35768;&#19968;&#20123;&#21363;&#20852;&#28436;&#20986;&#12290;&#20182;&#20204;&#30340;&#30446;&#26631;&#26159;&#20351;&#23398;&#29983;&#33021;&#22815;&#25490;&#32451;&#23545;&#32593;&#32476;&#27450;&#20940;&#30340;&#29702;&#24819;&#21644;&#19981;&#33391;&#21453;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17456v1 Announce Type: cross  Abstract: Cyberbullying harms teenagers' mental health, and teaching them upstanding intervention is crucial. Wizard-of-Oz studies show chatbots can scale up personalized and interactive cyberbullying education, but implementing such chatbots is a challenging and delicate task. We created a no-code chatbot design tool for K-12 teachers. Using large language models and prompt chaining, our tool allows teachers to prototype bespoke dialogue flows and chatbot utterances. In offering this tool, we explore teachers' distinctive needs when designing chatbots to assist their teaching, and how chatbot design tools might better support them. Our findings reveal that teachers welcome the tool enthusiastically. Moreover, they see themselves as playwrights guiding both the students' and the chatbot's behaviors, while allowing for some improvisation. Their goal is to enable students to rehearse both desirable and undesirable reactions to cyberbullying in a s
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#65292;&#38024;&#23545;&#39135;&#35889;&#25991;&#26412;&#24320;&#21457;&#20102;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#27169;&#22411;&#65292;&#36890;&#36807;&#31995;&#32479;&#30340;&#25968;&#25454;&#22788;&#29702;&#21644;&#20998;&#26512;&#65292;&#26500;&#24314;&#20102;&#29992;&#20110;&#33258;&#21160;&#29983;&#25104;&#26032;&#39135;&#35889;&#30340;&#25968;&#25454;&#38598;&#12290;</title><link>https://arxiv.org/abs/2402.17447</link><description>&lt;p&gt;
&#39135;&#35889;&#30340;&#28145;&#24230;&#23398;&#20064;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Deep Learning Based Named Entity Recognition Models for Recipes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17447
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#65292;&#38024;&#23545;&#39135;&#35889;&#25991;&#26412;&#24320;&#21457;&#20102;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#27169;&#22411;&#65292;&#36890;&#36807;&#31995;&#32479;&#30340;&#25968;&#25454;&#22788;&#29702;&#21644;&#20998;&#26512;&#65292;&#26500;&#24314;&#20102;&#29992;&#20110;&#33258;&#21160;&#29983;&#25104;&#26032;&#39135;&#35889;&#30340;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39135;&#29289;&#36890;&#36807;&#21508;&#31181;&#21162;&#21147;&#26041;&#24335;&#24433;&#21709;&#30528;&#25105;&#20204;&#30340;&#29983;&#27963;&#65292;&#21253;&#25324;&#21475;&#21619;&#12289;&#33829;&#20859;&#12289;&#20581;&#24247;&#21644;&#21487;&#25345;&#32493;&#24615;&#12290;&#39135;&#35889;&#26159;&#36890;&#36807;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#20195;&#20195;&#30456;&#20256;&#30340;&#25991;&#21270;&#33014;&#22218;&#12290;&#33258;&#21160;&#35782;&#21035;&#21629;&#21517;&#23454;&#20307;&#30340;&#21327;&#35758;&#65292;&#21363;&#39135;&#35889;&#25991;&#26412;&#30340;&#22522;&#26412;&#32452;&#25104;&#37096;&#20998;&#65292;&#23545;&#20110;&#21508;&#31181;&#24212;&#29992;&#26469;&#35828;&#37117;&#20855;&#26377;&#24040;&#22823;&#20215;&#20540;&#65292;&#20174;&#20449;&#24687;&#25552;&#21462;&#21040;&#26032;&#39062;&#39135;&#35889;&#29983;&#25104;&#12290;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#26159;&#19968;&#31181;&#20174;&#24050;&#30693;&#26631;&#31614;&#30340;&#38750;&#32467;&#26500;&#21270;&#25110;&#21322;&#32467;&#26500;&#21270;&#25968;&#25454;&#20013;&#25552;&#21462;&#20449;&#24687;&#30340;&#25216;&#26415;&#12290;&#25105;&#20204;&#20174;&#25163;&#21160;&#27880;&#37322;&#30340;6,611&#20010;&#25104;&#20998;&#30701;&#35821;&#30340;&#25968;&#25454;&#24320;&#22987;&#65292;&#32047;&#31215;&#21019;&#24314;&#20102;26,445&#20010;&#30701;&#35821;&#30340;&#22686;&#24378;&#25968;&#25454;&#38598;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#28165;&#29702;&#21644;&#20998;&#26512;&#20102;&#26469;&#33258;RecipeDB&#30340;&#25104;&#20998;&#30701;&#35821;&#65292;&#36825;&#26159;&#40644;&#37329;&#26631;&#20934;&#30340;&#39135;&#35889;&#25968;&#25454;&#23384;&#20648;&#24211;&#65292;&#24182;&#20351;&#29992;Stanford NER&#36827;&#34892;&#20102;&#26631;&#27880;&#12290;&#22522;&#20110;&#20998;&#26512;&#65292;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;&#32858;&#31867;&#30340;&#26041;&#27861;&#23545;88,526&#20010;&#30701;&#35821;&#30340;&#23376;&#38598;&#36827;&#34892;&#20102;&#21462;&#26679;&#65292;&#21516;&#26102;&#20445;&#30041;&#20102;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17447v1 Announce Type: cross  Abstract: Food touches our lives through various endeavors, including flavor, nourishment, health, and sustainability. Recipes are cultural capsules transmitted across generations via unstructured text. Automated protocols for recognizing named entities, the building blocks of recipe text, are of immense value for various applications ranging from information extraction to novel recipe generation. Named entity recognition is a technique for extracting information from unstructured or semi-structured data with known labels. Starting with manually-annotated data of 6,611 ingredient phrases, we created an augmented dataset of 26,445 phrases cumulatively. Simultaneously, we systematically cleaned and analyzed ingredient phrases from RecipeDB, the gold-standard recipe data repository, and annotated them using the Stanford NER. Based on the analysis, we sampled a subset of 88,526 phrases using a clustering-based approach while preserving the diversity
&lt;/p&gt;</description></item><item><title>Ansible Lightspeed&#26159;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26381;&#21153;&#65292;&#19987;&#27880;&#20110;&#23558;&#33258;&#28982;&#35821;&#35328;&#36716;&#25442;&#20026;Ansible&#20195;&#30721;&#65292;&#20026;IT&#33258;&#21160;&#21270;&#39046;&#22495;&#24102;&#26469;&#20102;&#21019;&#26032;&#12290;</title><link>https://arxiv.org/abs/2402.17442</link><description>&lt;p&gt;
Ansible Lightspeed: &#19968;&#31181;&#29992;&#20110;IT&#33258;&#21160;&#21270;&#30340;&#20195;&#30721;&#29983;&#25104;&#26381;&#21153;
&lt;/p&gt;
&lt;p&gt;
Ansible Lightspeed: A Code Generation Service for IT Automation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17442
&lt;/p&gt;
&lt;p&gt;
Ansible Lightspeed&#26159;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26381;&#21153;&#65292;&#19987;&#27880;&#20110;&#23558;&#33258;&#28982;&#35821;&#35328;&#36716;&#25442;&#20026;Ansible&#20195;&#30721;&#65292;&#20026;IT&#33258;&#21160;&#21270;&#39046;&#22495;&#24102;&#26469;&#20102;&#21019;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#38382;&#19990;&#20351;&#24471;&#21019;&#24314;&#21487;&#25552;&#39640;&#24320;&#21457;&#32773;&#29983;&#20135;&#21147;&#30340;&#24037;&#20855;&#25104;&#20026;&#21487;&#33021;&#65292;&#38598;&#25104;&#24320;&#21457;&#29615;&#22659;&#65288;IDEs&#65289;&#24120;&#34987;&#29992;&#20316;&#19982;LLMs&#20132;&#20114;&#30340;&#25509;&#21475;&#12290;&#24050;&#21457;&#24067;&#35768;&#22810;&#36825;&#31867;&#24037;&#20855;&#65292;&#20294;&#20960;&#20046;&#20840;&#37096;&#37117;&#19987;&#27880;&#20110;&#36890;&#29992;&#32534;&#31243;&#35821;&#35328;&#65292;&#24456;&#23569;&#20851;&#27880;&#23545;IT&#33258;&#21160;&#21270;&#33267;&#20851;&#37325;&#35201;&#30340;&#29305;&#23450;&#39046;&#22495;&#35821;&#35328;&#12290;Ansible&#26159;&#19968;&#31181;&#22522;&#20110;YAML&#30340;IT&#33258;&#21160;&#21270;&#29305;&#23450;&#35821;&#35328;&#12290;Red Hat Ansible Lightspeed&#19982;IBM Watson Code Assistant&#21512;&#20316;&#30340;Ansible Lightspeed&#26159;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#26381;&#21153;&#65292;&#19987;&#38376;&#29992;&#20110;&#23558;&#33258;&#28982;&#35821;&#35328;&#36716;&#25442;&#20026;Ansible&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17442v1 Announce Type: cross  Abstract: The availability of Large Language Models (LLMs) which can generate code, has made it possible to create tools that improve developer productivity. Integrated development environments or IDEs which developers use to write software are often used as an interface to interact with LLMs. Although many such tools have been released, almost all of them focus on general-purpose programming languages. Domain-specific languages, such as those crucial for IT automation, have not received much attention. Ansible is one such YAML-based IT automation-specific language. Red Hat Ansible Lightspeed with IBM Watson Code Assistant, further referred to as Ansible Lightspeed, is an LLM-based service designed explicitly for natural language to Ansible code generation.   In this paper, we describe the design and implementation of the Ansible Lightspeed service and analyze feedback from thousands of real users. We examine diverse performance indicators, clas
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21160;&#24577;&#24773;&#24863;-&#35821;&#20041;&#30456;&#20851;&#24615;&#26469;&#29983;&#25104;&#20849;&#24773;&#24335;&#23545;&#35805;&#22238;&#22797;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#19978;&#19979;&#25991;&#21644;&#24773;&#24863;&#30340;&#20132;&#20114;&#26500;&#24314;&#21160;&#24577;&#24773;&#24863;-&#35821;&#20041;&#21521;&#37327;&#65292;&#25552;&#39640;&#20102;&#23545;&#24773;&#24863;&#19982;&#35821;&#20041;&#20851;&#32852;&#24615;&#30340;&#29702;&#35299;&#12290;</title><link>https://arxiv.org/abs/2402.17437</link><description>&lt;p&gt;
&#21033;&#29992;&#24773;&#24863;-&#35821;&#20041;&#30456;&#20851;&#24615;&#36827;&#34892;&#20849;&#24773;&#24335;&#22238;&#22797;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Exploiting Emotion-Semantic Correlations for Empathetic Response Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17437
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21160;&#24577;&#24773;&#24863;-&#35821;&#20041;&#30456;&#20851;&#24615;&#26469;&#29983;&#25104;&#20849;&#24773;&#24335;&#23545;&#35805;&#22238;&#22797;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#19978;&#19979;&#25991;&#21644;&#24773;&#24863;&#30340;&#20132;&#20114;&#26500;&#24314;&#21160;&#24577;&#24773;&#24863;-&#35821;&#20041;&#21521;&#37327;&#65292;&#25552;&#39640;&#20102;&#23545;&#24773;&#24863;&#19982;&#35821;&#20041;&#20851;&#32852;&#24615;&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20849;&#24773;&#24335;&#22238;&#22797;&#29983;&#25104;&#26088;&#22312;&#36890;&#36807;&#29702;&#35299;&#23545;&#35805;&#35821;&#35328;&#20013;&#35828;&#35805;&#32773;&#30340;&#24773;&#24863;&#24863;&#21463;&#26469;&#29983;&#25104;&#20849;&#24773;&#22238;&#22797;&#12290;&#26368;&#36817;&#30340;&#26041;&#27861;&#25429;&#25417;&#20132;&#38469;&#32773;&#35821;&#35328;&#20013;&#30340;&#24773;&#24863;&#35789;&#65292;&#24182;&#23558;&#20854;&#26500;&#24314;&#20026;&#38745;&#24577;&#21521;&#37327;&#65292;&#20197;&#24863;&#30693;&#24494;&#22937;&#30340;&#24773;&#24863;&#12290;&#28982;&#32780;&#65292;&#35821;&#35328;&#30740;&#31350;&#34920;&#26126;&#65292;&#35821;&#35328;&#20013;&#30340;&#24773;&#24863;&#35789;&#26159;&#21160;&#24577;&#30340;&#65292;&#24182;&#19982;&#20854;&#20182;&#35821;&#27861;&#35821;&#20041;&#35282;&#33394;&#65288;&#21363;&#20855;&#26377;&#35821;&#20041;&#21547;&#20041;&#30340;&#35789;&#35821;&#65289;&#30456;&#20851;&#32852;&#12290;&#20197;&#21069;&#30340;&#26041;&#27861;&#24573;&#30053;&#20102;&#36825;&#20004;&#20010;&#29305;&#24449;&#65292;&#36825;&#24456;&#23481;&#26131;&#23548;&#33268;&#24773;&#24863;&#35823;&#35299;&#21644;&#20851;&#38190;&#35821;&#20041;&#30340;&#24573;&#35270;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20849;&#24773;&#23545;&#35805;&#29983;&#25104;&#20219;&#21153;&#30340;&#21160;&#24577;&#24773;&#24863;-&#35821;&#20041;&#30456;&#20851;&#24615;&#27169;&#22411;&#65288;ESCM&#65289;&#12290;ESCM&#36890;&#36807;&#19978;&#19979;&#25991;&#21644;&#24773;&#24863;&#30340;&#20132;&#20114;&#26500;&#24314;&#21160;&#24577;&#24773;&#24863;-&#35821;&#20041;&#21521;&#37327;&#12290;&#25105;&#20204;&#24341;&#20837;&#20381;&#23384;&#26641;&#26469;&#21453;&#26144;&#24773;&#24863;&#19982;&#35821;&#20041;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17437v1 Announce Type: cross  Abstract: Empathetic response generation aims to generate empathetic responses by understanding the speaker's emotional feelings from the language of dialogue. Recent methods capture emotional words in the language of communicators and construct them as static vectors to perceive nuanced emotions. However, linguistic research has shown that emotional words in language are dynamic and have correlations with other grammar semantic roles, i.e., words with semantic meanings, in grammar. Previous methods overlook these two characteristics, which easily lead to misunderstandings of emotions and neglect of key semantics. To address this issue, we propose a dynamical Emotion-Semantic Correlation Model (ESCM) for empathetic dialogue generation tasks. ESCM constructs dynamic emotion-semantic vectors through the interaction of context and emotions. We introduce dependency trees to reflect the correlations between emotions and semantics. Based on dynamic em
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;KANDY&#22522;&#20934;&#26694;&#26550;&#65292;&#36890;&#36807;&#29983;&#25104;&#22522;&#20110;&#22350;&#19969;&#26031;&#22522;&#27169;&#24335;&#30340;&#23398;&#20064;&#21644;&#25512;&#29702;&#20219;&#21153;&#65292;&#25552;&#20986;&#20102;&#25345;&#32493;&#21644;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#26032;&#25361;&#25112;&#65292;&#24182;&#30528;&#37325;&#30740;&#31350;&#31526;&#21495;&#32452;&#25104;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.17431</link><description>&lt;p&gt;
KANDY&#22522;&#20934;&#65306;&#20351;&#29992;&#22350;&#19969;&#26031;&#22522;&#27169;&#24335;&#36827;&#34892;&#22686;&#37327;&#31070;&#32463;&#31526;&#21495;&#23398;&#20064;&#21644;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
The KANDY Benchmark: Incremental Neuro-Symbolic Learning and Reasoning with Kandinsky Patterns
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17431
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;KANDY&#22522;&#20934;&#26694;&#26550;&#65292;&#36890;&#36807;&#29983;&#25104;&#22522;&#20110;&#22350;&#19969;&#26031;&#22522;&#27169;&#24335;&#30340;&#23398;&#20064;&#21644;&#25512;&#29702;&#20219;&#21153;&#65292;&#25552;&#20986;&#20102;&#25345;&#32493;&#21644;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#26032;&#25361;&#25112;&#65292;&#24182;&#30528;&#37325;&#30740;&#31350;&#31526;&#21495;&#32452;&#25104;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17431v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;&#30340; &#25688;&#35201;&#65306;&#20154;&#24037;&#26234;&#33021;&#19981;&#26029;&#23547;&#27714;&#26032;&#30340;&#25361;&#25112;&#21644;&#22522;&#20934;&#65292;&#20197;&#26377;&#25928;&#34913;&#37327;&#24615;&#33021;&#24182;&#25512;&#21160;&#26368;&#26032;&#25216;&#26415;&#30340;&#21457;&#23637;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;KANDY&#65292;&#19968;&#20010;&#21487;&#29992;&#20110;&#29983;&#25104;&#21463;&#22350;&#19969;&#26031;&#22522;&#27169;&#24335;&#21551;&#21457;&#30340;&#21508;&#31181;&#23398;&#20064;&#21644;&#25512;&#29702;&#20219;&#21153;&#30340;&#22522;&#20934;&#26694;&#26550;&#12290;&#36890;&#36807;&#21019;&#24314;&#19968;&#31995;&#21015;&#20855;&#26377;&#36882;&#22686;&#22797;&#26434;&#24615;&#21644;&#31232;&#30095;&#30417;&#30563;&#30340;&#20108;&#20803;&#20998;&#31867;&#20219;&#21153;&#35838;&#31243;&#65292;KANDY&#21487;&#29992;&#20110;&#23454;&#29616;&#25345;&#32493;&#21644;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#22522;&#20934;&#65292;&#24182;&#19987;&#27880;&#20110;&#31526;&#21495;&#32452;&#25104;&#24615;&#12290;&#22522;&#26412;&#20107;&#23454;&#20013;&#36824;&#25552;&#20379;&#20102;&#20998;&#31867;&#35268;&#21017;&#65292;&#20197;&#20415;&#20998;&#26512;&#21487;&#35299;&#37322;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#38500;&#20102;&#22522;&#20934;&#29983;&#25104;&#31649;&#36947;&#65292;&#25105;&#20204;&#36824;&#21457;&#24067;&#20102;&#20004;&#20010;&#35838;&#31243;&#65292;&#19968;&#20010;&#26356;&#23481;&#26131;&#19968;&#20010;&#26356;&#38590;&#65292;&#25105;&#20204;&#25552;&#35758;&#36825;&#20123;&#20316;&#20026;&#30740;&#31350;&#31038;&#21306;&#30340;&#26032;&#25361;&#25112;&#12290;&#36890;&#36807;&#24443;&#24213;&#30340;&#23454;&#39564;&#35780;&#20272;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26368;&#20808;&#36827;&#30340;&#31070;&#32463;&#27169;&#22411;&#21644;&#32431;&#31526;&#21495;&#26041;&#27861;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17431v1 Announce Type: new  Abstract: Artificial intelligence is continuously seeking novel challenges and benchmarks to effectively measure performance and to advance the state-of-the-art. In this paper we introduce KANDY, a benchmarking framework that can be used to generate a variety of learning and reasoning tasks inspired by Kandinsky patterns. By creating curricula of binary classification tasks with increasing complexity and with sparse supervisions, KANDY can be used to implement benchmarks for continual and semi-supervised learning, with a specific focus on symbol compositionality. Classification rules are also provided in the ground truth to enable analysis of interpretable solutions. Together with the benchmark generation pipeline, we release two curricula, an easier and a harder one, that we propose as new challenges for the research community. With a thorough experimental evaluation, we show how both state-of-the-art neural models and purely symbolic approaches 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#31163;&#32447;&#25968;&#25454;&#20013;&#31471;&#21040;&#31471;&#22320;&#24378;&#21270;&#23398;&#20064;&#40657;&#30418;&#20248;&#21270;&#31639;&#27861;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#34920;&#36798;&#33021;&#21147;&#24378;&#30340;&#24207;&#21015;&#27169;&#22411;&#21644;&#21518;&#24724;-&#21069;&#36827;&#20196;&#29260;&#26469;&#33719;&#21462;&#20219;&#21153;&#20449;&#24687;&#24182;&#20570;&#20986;&#20915;&#31574;&#12290;</title><link>https://arxiv.org/abs/2402.17423</link><description>&lt;p&gt;
&#21152;&#24378;&#19978;&#19979;&#25991;&#40657;&#30418;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Reinforced In-Context Black-Box Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17423
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#31163;&#32447;&#25968;&#25454;&#20013;&#31471;&#21040;&#31471;&#22320;&#24378;&#21270;&#23398;&#20064;&#40657;&#30418;&#20248;&#21270;&#31639;&#27861;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#34920;&#36798;&#33021;&#21147;&#24378;&#30340;&#24207;&#21015;&#27169;&#22411;&#21644;&#21518;&#24724;-&#21069;&#36827;&#20196;&#29260;&#26469;&#33719;&#21462;&#20219;&#21153;&#20449;&#24687;&#24182;&#20570;&#20986;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#40657;&#30418;&#20248;&#21270;&#65288;BBO&#65289;&#24050;&#32463;&#22312;&#35768;&#22810;&#31185;&#23398;&#21644;&#24037;&#31243;&#39046;&#22495;&#21462;&#24471;&#25104;&#21151;&#24212;&#29992;&#12290;&#26368;&#36817;&#65292;&#20154;&#20204;&#36234;&#26469;&#36234;&#20851;&#27880;&#20803;&#23398;&#20064;BBO&#31639;&#27861;&#30340;&#29305;&#23450;&#32452;&#20214;&#65292;&#20197;&#21152;&#24555;&#20248;&#21270;&#36895;&#24230;&#24182;&#25670;&#33073;&#32321;&#29712;&#30340;&#25163;&#24037;&#21551;&#21457;&#24335;&#31639;&#27861;&#12290;&#20316;&#20026;&#25193;&#23637;&#65292;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#25972;&#20010;&#31639;&#27861;&#38656;&#35201;&#19987;&#23478;&#26368;&#23569;&#30340;&#24037;&#20316;&#37327;&#65292;&#24182;&#19988;&#21487;&#20197;&#25552;&#20379;&#26368;&#22823;&#30340;&#28789;&#27963;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RIBBO&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20197;&#31471;&#21040;&#31471;&#30340;&#26041;&#24335;&#20174;&#31163;&#32447;&#25968;&#25454;&#20013;&#24378;&#21270;&#23398;&#20064;BBO&#31639;&#27861;&#12290;RIBBO&#21033;&#29992;&#34920;&#36798;&#33021;&#21147;&#24378;&#30340;&#24207;&#21015;&#27169;&#22411;&#26469;&#23398;&#20064;&#22810;&#20010;&#34892;&#20026;&#31639;&#27861;&#21644;&#20219;&#21153;&#20135;&#29983;&#30340;&#20248;&#21270;&#21382;&#21490;&#65292;&#21033;&#29992;&#22823;&#22411;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#26469;&#25552;&#21462;&#20219;&#21153;&#20449;&#24687;&#24182;&#30456;&#24212;&#22320;&#20570;&#20986;&#20915;&#31574;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#26680;&#24515;&#26159;&#36890;&#36807;&#22686;&#21152;&#21518;&#24724;-&#21069;&#36827;&#20196;&#29260;&#26469;&#22686;&#24378;&#20248;&#21270;&#21382;&#21490;&#65292;&#36825;&#20123;&#20196;&#29260;&#26088;&#22312;&#22522;&#20110;&#32047;&#31215;&#34920;&#29616;&#26469;&#34920;&#31034;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17423v1 Announce Type: cross  Abstract: Black-Box Optimization (BBO) has found successful applications in many fields of science and engineering. Recently, there has been a growing interest in meta-learning particular components of BBO algorithms to speed up optimization and get rid of tedious hand-crafted heuristics. As an extension, learning the entire algorithm from data requires the least labor from experts and can provide the most flexibility. In this paper, we propose RIBBO, a method to reinforce-learn a BBO algorithm from offline data in an end-to-end fashion. RIBBO employs expressive sequence models to learn the optimization histories produced by multiple behavior algorithms and tasks, leveraging the in-context learning ability of large models to extract task information and make decisions accordingly. Central to our method is to augment the optimization histories with regret-to-go tokens, which are designed to represent the performance of an algorithm based on cumul
&lt;/p&gt;</description></item><item><title>PANDAS&#26159;&#19968;&#31181;&#29992;&#20110;&#26032;&#31867;&#21035;&#21457;&#29616;&#21644;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#26080;&#26631;&#31614;&#25968;&#25454;&#20013;&#21457;&#29616;&#20195;&#34920;&#26032;&#31867;&#21035;&#30340;&#32858;&#31867;&#65292;&#24182;&#29992;&#21407;&#22411;&#34920;&#31034;&#31867;&#21035;&#65292;&#23454;&#29616;&#20102;&#22312;&#22522;&#30784;&#31867;&#21035;&#30340;&#22522;&#30784;&#19978;&#26816;&#27979;&#26032;&#21457;&#29616;&#30340;&#31867;&#21035;&#12290;</title><link>https://arxiv.org/abs/2402.17420</link><description>&lt;p&gt;
PANDAS: &#22522;&#20110;&#21407;&#22411;&#30340;&#26032;&#31867;&#21035;&#21457;&#29616;&#19982;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
PANDAS: Prototype-based Novel Class Discovery and Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17420
&lt;/p&gt;
&lt;p&gt;
PANDAS&#26159;&#19968;&#31181;&#29992;&#20110;&#26032;&#31867;&#21035;&#21457;&#29616;&#21644;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#26080;&#26631;&#31614;&#25968;&#25454;&#20013;&#21457;&#29616;&#20195;&#34920;&#26032;&#31867;&#21035;&#30340;&#32858;&#31867;&#65292;&#24182;&#29992;&#21407;&#22411;&#34920;&#31034;&#31867;&#21035;&#65292;&#23454;&#29616;&#20102;&#22312;&#22522;&#30784;&#31867;&#21035;&#30340;&#22522;&#30784;&#19978;&#26816;&#27979;&#26032;&#21457;&#29616;&#30340;&#31867;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#26631;&#26816;&#27979;&#22120;&#36890;&#24120;&#22312;&#22266;&#23450;&#30340;&#19968;&#32452;&#31867;&#21035;&#19978;&#36827;&#34892;&#19968;&#27425;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#23553;&#38381;&#19990;&#30028;&#30340;&#20551;&#35774;&#22312;&#23454;&#36341;&#20013;&#26159;&#19981;&#29616;&#23454;&#30340;&#65292;&#22240;&#20026;&#22312;&#26816;&#27979;&#22120;&#37096;&#32626;&#22312;&#37326;&#22806;&#21518;&#65292;&#26032;&#30340;&#31867;&#21035;&#24517;&#28982;&#20250;&#20986;&#29616;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#25193;&#23637;&#35757;&#32451;&#20026;&#19968;&#32452;&#22522;&#30784;&#31867;&#21035;&#30340;&#26816;&#27979;&#22120;&#30340;&#26041;&#27861;&#65292;&#20351;&#20854;&#33021;&#22815; i) &#21457;&#29616;&#26032;&#31867;&#21035;&#30340;&#23384;&#22312;&#65292;&#24182; ii) &#33258;&#21160;&#20016;&#23500;&#20854;&#24211;&#20197;&#33021;&#22815;&#26816;&#27979;&#36825;&#20123;&#26032;&#21457;&#29616;&#30340;&#31867;&#21035;&#20197;&#21450;&#22522;&#30784;&#31867;&#21035;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102; PANDAS&#65292;&#19968;&#31181;&#29992;&#20110;&#26032;&#31867;&#21035;&#21457;&#29616;&#21644;&#26816;&#27979;&#30340;&#26041;&#27861;&#12290;&#23427;&#20174;&#26080;&#26631;&#31614;&#25968;&#25454;&#20013;&#21457;&#29616;&#20195;&#34920;&#26032;&#31867;&#21035;&#30340;&#32858;&#31867;&#65292;&#24182;&#29992;&#21407;&#22411;&#26469;&#34920;&#31034;&#26087;&#31867;&#21035;&#21644;&#26032;&#31867;&#21035;&#12290;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#65292;&#22522;&#20110;&#36317;&#31163;&#30340;&#20998;&#31867;&#22120;&#20351;&#29992;&#36825;&#20123;&#21407;&#22411;&#20026;&#27599;&#20010;&#26816;&#27979;&#21040;&#30340;&#23545;&#35937;&#23454;&#20363;&#20998;&#37197;&#26631;&#31614;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#31616;&#21333;&#24615;&#20351;&#20854;&#20855;&#26377;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#12290;&#25105;&#20204;&#22312;VOC 2012&#21644;COCO-to-LVIS&#25968;&#25454;&#38598;&#19978;&#36890;&#36807;&#23454;&#39564;&#23637;&#31034;&#20102;PANDAS&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17420v1 Announce Type: cross  Abstract: Object detectors are typically trained once and for all on a fixed set of classes. However, this closed-world assumption is unrealistic in practice, as new classes will inevitably emerge after the detector is deployed in the wild. In this work, we look at ways to extend a detector trained for a set of base classes so it can i) spot the presence of novel classes, and ii) automatically enrich its repertoire to be able to detect those newly discovered classes together with the base ones. We propose PANDAS, a method for novel class discovery and detection. It discovers clusters representing novel classes from unlabeled data, and represents old and new classes with prototypes. During inference, a distance-based classifier uses these prototypes to assign a label to each detected object instance. The simplicity of our method makes it widely applicable. We experimentally demonstrate the effectiveness of PANDAS on the VOC 2012 and COCO-to-LVIS 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;MRI&#37325;&#24314;&#30340;&#20613;&#37324;&#21494;&#22495;&#25554;&#20540;&#31070;&#32463;&#32593;&#32476;&#30340;&#22270;&#20687;&#31354;&#38388;&#24418;&#24335;&#20027;&#20041;&#65292;&#24182;&#20998;&#26512;&#20102;&#22312;CNN&#25512;&#26029;&#36807;&#31243;&#20013;&#22122;&#22768;&#20256;&#25773;&#30340;&#20272;&#35745;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.17410</link><description>&lt;p&gt;
&#20613;&#37324;&#21494;&#22495;&#25554;&#20540;&#31070;&#32463;&#32593;&#32476;&#30340;&#22270;&#20687;&#31354;&#38388;&#24418;&#24335;&#20027;&#20041;&#29992;&#20110;&#22122;&#22768;&#20256;&#25773;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
A novel image space formalism of Fourier domain interpolation neural networks for noise propagation analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17410
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;MRI&#37325;&#24314;&#30340;&#20613;&#37324;&#21494;&#22495;&#25554;&#20540;&#31070;&#32463;&#32593;&#32476;&#30340;&#22270;&#20687;&#31354;&#38388;&#24418;&#24335;&#20027;&#20041;&#65292;&#24182;&#20998;&#26512;&#20102;&#22312;CNN&#25512;&#26029;&#36807;&#31243;&#20013;&#22122;&#22768;&#20256;&#25773;&#30340;&#20272;&#35745;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26088;&#22312;&#20026;MRI&#37325;&#24314;&#20013;&#30340;&#22270;&#20687;&#22495;&#25554;&#20540;&#24320;&#21457;&#22810;&#23618;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNNs&#65289;&#30340;&#22270;&#20687;&#31354;&#38388;&#24418;&#24335;&#20027;&#20041;&#65292;&#24182;&#22312;CNN&#25512;&#26029;&#36807;&#31243;&#20013;&#23545;&#22122;&#22768;&#20256;&#25773;&#36827;&#34892;&#20998;&#26512;&#12290;&#36890;&#36807;&#20351;&#29992;&#22797;&#20540;&#25972;&#27969;&#32447;&#24615;&#21333;&#20803;&#22312;&#20613;&#37324;&#21494;&#22495;&#65288;&#20063;&#31216;&#20026;k&#31354;&#38388;&#65289;&#20013;&#30340;&#38750;&#32447;&#24615;&#28608;&#27963;&#65292;&#23558;&#20854;&#34920;&#31034;&#20026;&#19982;&#28608;&#27963;&#25513;&#27169;&#30340;&#36880;&#20803;&#32032;&#20056;&#27861;&#12290;&#36825;&#31181;&#25805;&#20316;&#22312;&#22270;&#20687;&#31354;&#38388;&#20013;&#36716;&#25442;&#20026;&#21367;&#31215;&#12290;&#22312;k&#31354;&#38388;&#32593;&#32476;&#35757;&#32451;&#21518;&#65292;&#36825;&#31181;&#26041;&#27861;&#20026;&#30456;&#23545;&#20110;&#21035;&#21517;&#32447;&#22280;&#22270;&#20687;&#30340;&#37325;&#24314;&#22270;&#20687;&#30340;&#23548;&#25968;&#25552;&#20379;&#20102;&#19968;&#20010;&#20195;&#25968;&#34920;&#36798;&#24335;&#65292;&#36825;&#20123;&#21035;&#21517;&#32447;&#22280;&#22270;&#20687;&#20316;&#20026;&#22270;&#20687;&#31354;&#38388;&#20013;&#32593;&#32476;&#30340;&#36755;&#20837;&#24352;&#37327;&#12290;&#36825;&#20351;&#24471;&#21487;&#20197;&#36890;&#36807;&#20998;&#26512;&#20272;&#35745;&#32593;&#32476;&#25512;&#26029;&#20013;&#30340;&#26041;&#24046;&#65292;&#24182;&#29992;&#20110;&#25551;&#36848;&#22122;&#22768;&#29305;&#24615;&#12290;&#36890;&#36807;&#33945;&#29305;&#21345;&#27931;&#27169;&#25311;&#21644;&#22522;&#20110;&#33258;&#21160;&#24494;&#20998;&#30340;&#25968;&#20540;&#26041;&#27861;&#36827;&#34892;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17410v1 Announce Type: cross  Abstract: Purpose: To develop an image space formalism of multi-layer convolutional neural networks (CNNs) for Fourier domain interpolation in MRI reconstructions and analytically estimate noise propagation during CNN inference. Theory and Methods: Nonlinear activations in the Fourier domain (also known as k-space) using complex-valued Rectifier Linear Units are expressed as elementwise multiplication with activation masks. This operation is transformed into a convolution in the image space. After network training in k-space, this approach provides an algebraic expression for the derivative of the reconstructed image with respect to the aliased coil images, which serve as the input tensors to the network in the image space. This allows the variance in the network inference to be estimated analytically and to be used to describe noise characteristics. Monte-Carlo simulations and numerical approaches based on auto-differentiation were used for val
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21463;&#37325;&#20889;&#31995;&#32479;&#21551;&#21457;&#30340;&#31070;&#32463;&#26550;&#26500;&#65292;&#29992;&#20110;&#23398;&#20064;&#31639;&#27861;&#20219;&#21153;&#65292;&#36890;&#36807;Selector&#12289;Solver&#21644;Combiner&#19977;&#20010;&#19987;&#38376;&#27169;&#22359;&#23454;&#29616;&#31639;&#27861;&#20219;&#21153;&#30340;&#31616;&#21270;&#65292;&#20855;&#26377;&#36739;&#22909;&#30340;&#22806;&#25512;&#33021;&#21147;</title><link>https://arxiv.org/abs/2402.17407</link><description>&lt;p&gt;
&#29992;&#31070;&#32463;&#37325;&#20889;&#31995;&#32479;&#35299;&#20915;&#31639;&#27861;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
A Neural Rewriting System to Solve Algorithmic Problems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17407
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21463;&#37325;&#20889;&#31995;&#32479;&#21551;&#21457;&#30340;&#31070;&#32463;&#26550;&#26500;&#65292;&#29992;&#20110;&#23398;&#20064;&#31639;&#27861;&#20219;&#21153;&#65292;&#36890;&#36807;Selector&#12289;Solver&#21644;Combiner&#19977;&#20010;&#19987;&#38376;&#27169;&#22359;&#23454;&#29616;&#31639;&#27861;&#20219;&#21153;&#30340;&#31616;&#21270;&#65292;&#20855;&#26377;&#36739;&#22909;&#30340;&#22806;&#25512;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#20173;&#28982;&#38590;&#20197;&#23398;&#20064;&#38656;&#35201;&#31995;&#32479;&#24212;&#29992;&#32452;&#21512;&#35268;&#21017;&#26469;&#35299;&#20915;&#36229;&#20986;&#20998;&#24067;&#38382;&#39064;&#23454;&#20363;&#30340;&#31639;&#27861;&#31243;&#24207;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21407;&#21019;&#26041;&#27861;&#26469;&#23398;&#20064;&#21463;&#37325;&#20889;&#31995;&#32479;&#21551;&#21457;&#30340;&#31639;&#27861;&#20219;&#21153;&#65292;&#37325;&#20889;&#31995;&#32479;&#26159;&#31526;&#21495;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#32463;&#20856;&#26694;&#26550;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#37325;&#20889;&#31995;&#32479;&#21487;&#20197;&#34987;&#23454;&#29616;&#20026;&#19968;&#20010;&#30001;&#19987;&#38376;&#27169;&#22359;&#32452;&#25104;&#30340;&#31070;&#32463;&#26550;&#26500;&#65306;&#36873;&#25321;&#22120;&#35782;&#21035;&#35201;&#22788;&#29702;&#30340;&#30446;&#26631;&#23376;&#34920;&#36798;&#24335;&#65292;&#27714;&#35299;&#22120;&#36890;&#36807;&#35745;&#31639;&#30456;&#24212;&#30340;&#32467;&#26524;&#31616;&#21270;&#23376;&#34920;&#36798;&#24335;&#65292;&#32452;&#21512;&#22120;&#36890;&#36807;&#29992;&#25552;&#20379;&#30340;&#35299;&#20915;&#26041;&#26696;&#26367;&#25442;&#23376;&#34920;&#36798;&#24335;&#29983;&#25104;&#21407;&#22987;&#34920;&#36798;&#24335;&#30340;&#26032;&#29256;&#26412;&#12290;&#25105;&#20204;&#22312;&#19977;&#31181;&#28041;&#21450;&#31616;&#21270;&#28041;&#21450;&#21015;&#34920;&#12289;&#31639;&#26415;&#21644;&#20195;&#25968;&#34920;&#36798;&#24335;&#30340;&#31526;&#21495;&#20844;&#24335;&#30340;&#31639;&#27861;&#20219;&#21153;&#19978;&#35780;&#20272;&#25105;&#20204;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#27979;&#35797;&#20102;&#25152;&#25552;&#26550;&#26500;&#30340;&#22806;&#25512;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17407v1 Announce Type: cross  Abstract: Modern neural network architectures still struggle to learn algorithmic procedures that require to systematically apply compositional rules to solve out-of-distribution problem instances. In this work, we propose an original approach to learn algorithmic tasks inspired by rewriting systems, a classic framework in symbolic artificial intelligence. We show that a rewriting system can be implemented as a neural architecture composed by specialized modules: the Selector identifies the target sub-expression to process, the Solver simplifies the sub-expression by computing the corresponding result, and the Combiner produces a new version of the original expression by replacing the sub-expression with the solution provided. We evaluate our model on three types of algorithmic tasks that require simplifying symbolic formulas involving lists, arithmetic, and algebraic expressions. We test the extrapolation capabilities of the proposed architectu
&lt;/p&gt;</description></item><item><title>LSPT&#26159;&#19968;&#31181;&#38761;&#21629;&#24615;&#30340;&#35270;&#35273;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#38271;&#26399;&#38376;&#25511;&#25552;&#31034;&#65292;&#24039;&#22937;&#22320;&#21033;&#29992;&#38271;&#36317;&#31163;&#20808;&#21069;&#22359;&#20316;&#20026;&#25552;&#31034;&#30340;&#28508;&#22312;&#26469;&#28304;&#65292;&#20943;&#36731;&#20102;&#36951;&#24536;&#21442;&#25968;&#30340;&#39118;&#38505;&#12290;</title><link>https://arxiv.org/abs/2402.17406</link><description>&lt;p&gt;
LSPT&#65306;&#29992;&#20110;&#35270;&#35273;&#34920;&#31034;&#23398;&#20064;&#30340;&#38271;&#26399;&#31354;&#38388;&#25552;&#31034;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
LSPT: Long-term Spatial Prompt Tuning for Visual Representation Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17406
&lt;/p&gt;
&lt;p&gt;
LSPT&#26159;&#19968;&#31181;&#38761;&#21629;&#24615;&#30340;&#35270;&#35273;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#38271;&#26399;&#38376;&#25511;&#25552;&#31034;&#65292;&#24039;&#22937;&#22320;&#21033;&#29992;&#38271;&#36317;&#31163;&#20808;&#21069;&#22359;&#20316;&#20026;&#25552;&#31034;&#30340;&#28508;&#22312;&#26469;&#28304;&#65292;&#20943;&#36731;&#20102;&#36951;&#24536;&#21442;&#25968;&#30340;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#25552;&#31034;&#35843;&#25972;&#65288;VPT&#65289;&#25216;&#26415;&#22240;&#20854;&#36890;&#36807;&#19987;&#29992;&#30340;&#21487;&#23398;&#20064;&#20196;&#29260;&#65288;&#31216;&#20026;&#25552;&#31034;&#65289;&#23558;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;Transformer&#65288;ViT&#65289;&#35843;&#25972;&#21040;&#19979;&#28216;&#35270;&#35273;&#20219;&#21153;&#32780;&#38395;&#21517;&#12290;&#22312;&#33258;&#30417;&#30563;&#35270;&#35273;Transformer&#20013;&#20351;&#29992;&#30340;&#24403;&#20195;VPT&#26041;&#27861;&#36890;&#24120;&#40664;&#35748;&#24341;&#20837;&#26469;&#28304;&#33258;&#27169;&#22411;&#20808;&#21069;&#22359;&#30340;&#26032;&#21487;&#23398;&#20064;&#25552;&#31034;&#25110;&#38376;&#25511;&#25552;&#31034;&#20196;&#29260;&#12290;&#36825;&#31181;&#26041;&#27861;&#30340;&#19968;&#20010;&#20851;&#38190;&#32570;&#22833;&#26159;&#26410;&#21033;&#29992;&#38271;&#36317;&#31163;&#20808;&#21069;&#22359;&#20316;&#20026;&#27599;&#20010;&#33258;&#30417;&#30563;ViT&#20869;&#25552;&#31034;&#30340;&#28508;&#21147;&#26469;&#28304;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#37325;&#35201;&#24046;&#36317;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#38271;&#26399;&#31354;&#38388;&#25552;&#31034;&#35843;&#25972;&#65288;LSPT&#65289;- &#19968;&#31181;&#38761;&#21629;&#24615;&#30340;&#35270;&#35273;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#12290; LSPT&#20174;&#20154;&#31867;&#22823;&#33041;&#30340;&#22797;&#26434;&#24615;&#20013;&#27762;&#21462;&#28789;&#24863;&#65292;&#24039;&#22937;&#22320;&#32467;&#21512;&#20102;&#38271;&#26399;&#38376;&#25511;&#25552;&#31034;&#12290;&#36825;&#20010;&#29305;&#24615;&#20316;&#20026;&#26102;&#38388;&#32534;&#30721;&#65292;&#20943;&#36731;&#20102;&#36951;&#24536;&#21442;&#25968;&#30340;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17406v1 Announce Type: cross  Abstract: Visual Prompt Tuning (VPT) techniques have gained prominence for their capacity to adapt pre-trained Vision Transformers (ViTs) to downstream visual tasks using specialized learnable tokens termed as prompts. Contemporary VPT methodologies, especially when employed with self-supervised vision transformers, often default to the introduction of new learnable prompts or gated prompt tokens predominantly sourced from the model's previous block. A pivotal oversight in such approaches is their failure to harness the potential of long-range previous blocks as sources of prompts within each self-supervised ViT. To bridge this crucial gap, we introduce Long-term Spatial Prompt Tuning (LSPT) - a revolutionary approach to visual representation learning. Drawing inspiration from the intricacies of the human brain, LSPT ingeniously incorporates long-term gated prompts. This feature serves as temporal coding, curbing the risk of forgetting parameter
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#37327;&#23376;&#35745;&#31639;&#25216;&#26415;&#25552;&#20986;&#20102;Quantum-SMOTE&#26041;&#27861;&#65292;&#21487;&#20197;&#35299;&#20915;&#26426;&#22120;&#23398;&#20064;&#25968;&#25454;&#38598;&#20013;&#30340;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#24182;&#24341;&#20837;&#20102;&#26059;&#36716;&#35282;&#24230;&#12289;&#23569;&#25968;&#31867;&#30334;&#20998;&#27604;&#21644;&#20998;&#35010;&#22240;&#23376;&#31561;&#36229;&#21442;&#25968;&#65292;&#23454;&#29616;&#20102;&#23545;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#30340;&#26356;&#22909;&#25511;&#21046;&#12290;</title><link>https://arxiv.org/abs/2402.17398</link><description>&lt;p&gt;
&#37327;&#23376;&#26041;&#27861;&#30740;&#31350;&#21512;&#25104;&#23569;&#25968;&#31867;&#36807;&#37319;&#26679;&#25216;&#26415;&#65288;SMOTE&#65289;
&lt;/p&gt;
&lt;p&gt;
A Quantum Approach to Synthetic Minority Oversampling Technique (SMOTE)
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17398
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#37327;&#23376;&#35745;&#31639;&#25216;&#26415;&#25552;&#20986;&#20102;Quantum-SMOTE&#26041;&#27861;&#65292;&#21487;&#20197;&#35299;&#20915;&#26426;&#22120;&#23398;&#20064;&#25968;&#25454;&#38598;&#20013;&#30340;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#24182;&#24341;&#20837;&#20102;&#26059;&#36716;&#35282;&#24230;&#12289;&#23569;&#25968;&#31867;&#30334;&#20998;&#27604;&#21644;&#20998;&#35010;&#22240;&#23376;&#31561;&#36229;&#21442;&#25968;&#65292;&#23454;&#29616;&#20102;&#23545;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#30340;&#26356;&#22909;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;Quantum-SMOTE&#26041;&#27861;&#65292;&#36825;&#26159;&#19968;&#31181;&#20351;&#29992;&#37327;&#23376;&#35745;&#31639;&#25216;&#26415;&#26469;&#35299;&#20915;&#26426;&#22120;&#23398;&#20064;&#25968;&#25454;&#38598;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#30340;&#26032;&#39062;&#35299;&#20915;&#26041;&#26696;&#12290;Quantum-SMOTE&#21463;&#21040;&#21512;&#25104;&#23569;&#25968;&#31867;&#36807;&#37319;&#26679;&#25216;&#26415;&#65288;SMOTE&#65289;&#30340;&#21551;&#21457;&#65292;&#21033;&#29992;&#37327;&#23376;&#36807;&#31243;&#22914;&#20132;&#25442;&#27979;&#35797;&#21644;&#37327;&#23376;&#26059;&#36716;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#28857;&#12290;&#35813;&#26041;&#27861;&#19982;&#20256;&#32479;&#30340;SMOTE&#31639;&#27861;&#20351;&#29992;K-&#26368;&#36817;&#37051;&#65288;KNN&#65289;&#21644;&#27431;&#27663;&#36317;&#31163;&#30340;&#26041;&#24335;&#26377;&#25152;&#19981;&#21516;&#65292;&#33021;&#22815;&#20174;&#23569;&#25968;&#31867;&#25968;&#25454;&#28857;&#29983;&#25104;&#21512;&#25104;&#23454;&#20363;&#32780;&#26080;&#38656;&#20381;&#36182;&#20110;&#37051;&#36817;&#24615;&#12290;&#31639;&#27861;&#36890;&#36807;&#24341;&#20837;&#26059;&#36716;&#35282;&#24230;&#12289;&#23569;&#25968;&#31867;&#30334;&#20998;&#27604;&#21644;&#20998;&#35010;&#22240;&#23376;&#31561;&#36229;&#21442;&#25968;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#25511;&#21046;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#29305;&#23450;&#25968;&#25454;&#38598;&#38656;&#27714;&#30340;&#23450;&#21046;&#12290;&#35813;&#26041;&#27861;&#22312;TelecomChurn&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#24182;&#19982;&#20004;&#31181;&#20027;&#35201;&#30340;&#20998;&#31867;&#31639;&#27861;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17398v1 Announce Type: cross  Abstract: The paper proposes the Quantum-SMOTE method, a novel solution that uses quantum computing techniques to solve the prevalent problem of class imbalance in machine learning datasets. Quantum-SMOTE, inspired by the Synthetic Minority Oversampling Technique (SMOTE), generates synthetic data points using quantum processes such as swap tests and quantum rotation. The process varies from the conventional SMOTE algorithm's usage of K-Nearest Neighbors (KNN) and Euclidean distances, enabling synthetic instances to be generated from minority class data points without relying on neighbor proximity. The algorithm asserts greater control over the synthetic data generation process by introducing hyperparameters such as rotation angle, minority percentage, and splitting factor, which allow for customization to specific dataset requirements. The approach is tested on a public dataset of TelecomChurn and evaluated alongside two prominent classification
&lt;/p&gt;</description></item><item><title>&#23545;GPT-4&#22312;&#31639;&#27861;&#38382;&#39064;&#19978;&#36827;&#34892;&#20102;&#31995;&#32479;&#35780;&#20272;&#65292;&#21457;&#29616;&#37319;&#29992;&#20808;&#36827;&#30340;&#25552;&#31034;&#25216;&#26415;&#21487;&#20197;&#25552;&#39640;&#20854;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.17396</link><description>&lt;p&gt;
&#22312;&#31639;&#27861;&#38382;&#39064;&#19978;&#23545;GPT-4&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65306;&#20851;&#20110;&#25552;&#31034;&#31574;&#30053;&#30340;&#31995;&#32479;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Benchmarking GPT-4 on Algorithmic Problems: A Systematic Evaluation of Prompting Strategies
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17396
&lt;/p&gt;
&lt;p&gt;
&#23545;GPT-4&#22312;&#31639;&#27861;&#38382;&#39064;&#19978;&#36827;&#34892;&#20102;&#31995;&#32479;&#35780;&#20272;&#65292;&#21457;&#29616;&#37319;&#29992;&#20808;&#36827;&#30340;&#25552;&#31034;&#25216;&#26415;&#21487;&#20197;&#25552;&#39640;&#20854;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36890;&#36807;&#22312;&#28023;&#37327;&#25991;&#26412;&#35821;&#26009;&#24211;&#19978;&#33719;&#24471;&#30340;&#30693;&#35782;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#20013;&#37325;&#26032;&#21033;&#29992;&#65292;&#20960;&#20046;&#19981;&#38656;&#35201;&#65288;&#25110;&#26681;&#26412;&#19981;&#38656;&#35201;&#65289;&#35843;&#25972;&#27493;&#39588;&#65292;&#20174;&#32780;&#38761;&#26032;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#24050;&#32463;&#21453;&#22797;&#26174;&#31034;LLMs&#32570;&#20047;&#31995;&#32479;&#21270;&#27867;&#21270;&#65292;&#36825;&#20351;&#24471;&#26080;&#27861;&#23558;&#23398;&#20064;&#21040;&#30340;&#32479;&#35745;&#35268;&#24459;&#22806;&#25512;&#21040;&#35757;&#32451;&#20998;&#24067;&#20043;&#22806;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23545;&#20854;&#20013;&#19968;&#31181;&#26368;&#20808;&#36827;&#30340;LLMs&#65292;GPT-4&#65292;&#22312;&#19977;&#20010;&#31639;&#27861;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#31995;&#32479;&#22522;&#20934;&#27979;&#35797;&#65292;&#36825;&#20123;&#20219;&#21153;&#36890;&#36807;&#20004;&#20010;&#21442;&#25968;&#25511;&#21046;&#38382;&#39064;&#38590;&#24230;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;GPT-4&#19982;&#20854;&#21069;&#36523;&#65288;GPT-3.5&#65289;&#20197;&#21450;&#26368;&#36817;&#20171;&#32461;&#30340;&#21464;&#21387;&#22120;&#32534;&#30721;&#22120;&#26550;&#26500;&#30340;&#21464;&#20307;&#65292;&#21363;&#31070;&#32463;&#25968;&#25454;&#36335;&#30001;&#22120;&#65292;&#22312;&#35299;&#20915;&#31867;&#20284;&#20219;&#21153;&#26102;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#21457;&#29616;&#37319;&#29992;&#20808;&#36827;&#30340;&#25552;&#31034;&#25216;&#26415;&#21487;&#20197;&#20351;GPT-4&#36798;&#21040;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17396v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have revolutionized the field of Natural Language Processing thanks to their ability to reuse knowledge acquired on massive text corpora on a wide variety of downstream tasks, with minimal (if any) tuning steps. At the same time, it has been repeatedly shown that LLMs lack systematic generalization, which allows to extrapolate the learned statistical regularities outside the training distribution. In this work, we offer a systematic benchmarking of GPT-4, one of the most advanced LLMs available, on three algorithmic tasks characterized by the possibility to control the problem difficulty with two parameters. We compare the performance of GPT-4 with that of its predecessor (GPT-3.5) and with a variant of the Transformer-Encoder architecture recently introduced to solve similar tasks, the Neural Data Router. We find that the deployment of advanced prompting techniques allows GPT-4 to reach superior accuracy o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;FairBelief&#65292;&#19968;&#31181;&#29992;&#20110;&#25429;&#33719;&#21644;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#20013;&#26377;&#23475;&#20449;&#24565;&#30340;&#20998;&#26512;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#20844;&#24179;&#25968;&#25454;&#38598;&#23545;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;LM&#36827;&#34892;&#35780;&#20272;&#65292;&#21457;&#29616;&#36825;&#20123;LM&#21487;&#33021;&#23384;&#22312;&#26377;&#23475;&#20449;&#24565;&#12290;</title><link>https://arxiv.org/abs/2402.17389</link><description>&lt;p&gt;
&#20844;&#24179;&#20449;&#24565; - &#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#26377;&#23475;&#20449;&#24565;
&lt;/p&gt;
&lt;p&gt;
FairBelief - Assessing Harmful Beliefs in Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17389
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;FairBelief&#65292;&#19968;&#31181;&#29992;&#20110;&#25429;&#33719;&#21644;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#20013;&#26377;&#23475;&#20449;&#24565;&#30340;&#20998;&#26512;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#20844;&#24179;&#25968;&#25454;&#38598;&#23545;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;LM&#36827;&#34892;&#35780;&#20272;&#65292;&#21457;&#29616;&#36825;&#20123;LM&#21487;&#33021;&#23384;&#22312;&#26377;&#23475;&#20449;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#24050;&#34987;&#35777;&#26126;&#23384;&#22312;&#19981;&#33391;&#20559;&#35265;&#65292;&#22914;&#26524;&#36825;&#20123;&#31995;&#32479;&#22312;&#27809;&#26377;&#20180;&#32454;&#36827;&#34892;&#20844;&#24179;&#23457;&#35745;&#30340;&#24773;&#20917;&#19979;&#38598;&#25104;&#21040;&#29616;&#23454;&#24212;&#29992;&#20013;&#65292;&#21487;&#33021;&#20250;&#20260;&#23475;&#23569;&#25968;&#32676;&#20307;&#21644;&#34987;&#24573;&#35270;&#30340;&#32676;&#20307;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;FairBelief&#65292;&#19968;&#31181;&#20998;&#26512;&#26041;&#27861;&#65292;&#29992;&#20110;&#25429;&#33719;&#21644;&#35780;&#20272;&#20449;&#24565;&#65292;&#21363;LM&#21487;&#33021;&#20197;&#19981;&#21516;&#31243;&#24230;&#30340;&#30830;&#20449;&#24230;&#23884;&#20837;&#30340;&#21629;&#39064;&#65292;&#36825;&#20123;&#21629;&#39064;&#26263;&#20013;&#24433;&#21709;&#20854;&#39044;&#27979;&#12290;&#36890;&#36807;FairBelief&#65292;&#25105;&#20204;&#21033;&#29992;&#25552;&#31034;&#26469;&#30740;&#31350;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;LM&#22312;&#20808;&#21069;&#34987;&#24573;&#35270;&#30340;&#19981;&#21516;&#36724;&#19978;&#30340;&#34892;&#20026;&#65292;&#27604;&#22914;&#27169;&#22411;&#35268;&#27169;&#21644;&#21487;&#33021;&#24615;&#65292;&#35780;&#20272;&#23545;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#37327;&#21270;LM&#36755;&#20986;&#20260;&#23475;&#31243;&#24230;&#30340;&#20844;&#24179;&#25968;&#25454;&#38598;&#30340;&#39044;&#27979;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23545;&#27169;&#22411;&#21457;&#20986;&#30340;&#20449;&#24565;&#36827;&#34892;&#28145;&#20837;&#30340;&#23450;&#24615;&#35780;&#20272;&#12290;&#25105;&#20204;&#23558;FairBelief&#24212;&#29992;&#20110;&#33521;&#35821;LMs&#65292;&#21457;&#29616;&#23613;&#31649;&#36825;&#20123;&#26550;&#26500;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#23427;&#20204;&#20063;&#21487;&#33021;&#23384;&#22312;&#26377;&#23475;&#20449;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17389v1 Announce Type: cross  Abstract: Language Models (LMs) have been shown to inherit undesired biases that might hurt minorities and underrepresented groups if such systems were integrated into real-world applications without careful fairness auditing. This paper proposes FairBelief, an analytical approach to capture and assess beliefs, i.e., propositions that an LM may embed with different degrees of confidence and that covertly influence its predictions. With FairBelief, we leverage prompting to study the behavior of several state-of-the-art LMs across different previously neglected axes, such as model scale and likelihood, assessing predictions on a fairness dataset specifically designed to quantify LMs' outputs' hurtfulness. Finally, we conclude with an in-depth qualitative assessment of the beliefs emitted by the models. We apply FairBelief to English LMs, revealing that, although these architectures enable high performances on diverse natural language processing ta
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36827;&#34892;&#20102;&#19968;&#39033;&#29992;&#20856;&#22411;GNN&#19982;&#28145;&#24230;&#20840;&#36830;&#25509;&#21069;&#39304;&#32467;&#26500;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#30340;&#26696;&#20363;&#30740;&#31350;&#65292;&#20197;&#22312;&#39640;&#33021;&#31890;&#23376;&#29289;&#29702;&#23398;&#39046;&#22495;&#20013;&#25506;&#35752;&#23427;&#20204;&#30340;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.17386</link><description>&lt;p&gt;
&#23558;&#22270;&#31070;&#32463;&#32593;&#32476;&#37325;&#26032;&#36865;&#22238;&#35797;&#39564;&#21488;&#36827;&#34892;&#22312;&#39640;&#33021;&#31890;&#23376;&#29289;&#29702;&#23398;&#20013;&#30340;&#24212;&#29992;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A case study of sending graph neural networks back to the test bench for applications in high-energy particle physics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17386
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36827;&#34892;&#20102;&#19968;&#39033;&#29992;&#20856;&#22411;GNN&#19982;&#28145;&#24230;&#20840;&#36830;&#25509;&#21069;&#39304;&#32467;&#26500;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#30340;&#26696;&#20363;&#30740;&#31350;&#65292;&#20197;&#22312;&#39640;&#33021;&#31890;&#23376;&#29289;&#29702;&#23398;&#39046;&#22495;&#20013;&#25506;&#35752;&#23427;&#20204;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#39640;&#33021;&#31890;&#23376;&#30896;&#25758;&#20013;&#65292;&#20027;&#35201;&#30340;&#30896;&#25758;&#20135;&#29289;&#36890;&#24120;&#20250;&#36827;&#19968;&#27493;&#34928;&#21464;&#65292;&#24418;&#25104;&#31867;&#20284;&#26641;&#29366;&#32467;&#26500;&#30340;&#20998;&#23618;&#32467;&#26500;&#65292;&#20854;&#20013;&#33410;&#28857;&#25968;&#37327;&#26410;&#30693;&#12290;&#22312;&#31283;&#23450;&#31890;&#23376;&#32423;&#21035;&#19978;&#65292;&#30896;&#25758;&#30340;&#25152;&#26377;&#34928;&#21464;&#20135;&#29289;&#24418;&#25104;&#20855;&#26377;&#32622;&#25442;&#19981;&#21464;&#24615;&#30340;&#26368;&#32456;&#29366;&#24577;&#23545;&#35937;&#38598;&#21512;&#12290;&#19982;&#25968;&#23398;&#22270;&#30340;&#31867;&#27604;&#24341;&#21457;&#20102;&#19968;&#20010;&#24819;&#27861;&#65292;&#21363;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#65292;&#33258;&#28982;&#22320;&#31867;&#20284;&#20110;&#36825;&#20123;&#23646;&#24615;&#65292;&#24212;&#35813;&#26368;&#36866;&#21512;&#22788;&#29702;&#35768;&#22810;&#19982;&#39640;&#33021;&#31890;&#23376;&#29289;&#29702;&#23398;&#30456;&#20851;&#30340;&#20219;&#21153;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#38024;&#23545;&#20856;&#22411;GNN&#19982;&#28145;&#24230;&#20840;&#36830;&#25509;&#21069;&#39304;&#32467;&#26500;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#30340;&#26696;&#20363;&#12290;&#25105;&#20204;&#26088;&#22312;&#20197;&#33410;&#28857;&#12289;&#38544;&#34255;&#23618;&#25110;&#25152;&#30740;&#31350;&#31070;&#32463;&#32593;&#32476;&#30340;&#21487;&#35757;&#32451;&#21442;&#25968;&#26041;&#38754;&#26368;&#22823;&#31243;&#24230;&#22320;&#19981;&#20559;&#24615;&#22320;&#36827;&#34892;&#36825;&#31181;&#27604;&#36739;&#12290;&#20316;&#20026;&#29289;&#29702;&#26696;&#20363;&#65292;&#25105;&#20204;&#20351;&#29992;&#19982;&#36136;&#23376;&#20013;&#20135;&#29983;&#30340;&#39030;&#22840;&#20811;&#23545;&#20851;&#32852;&#30340;&#26368;&#32456;&#24577;X&#30340;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17386v1 Announce Type: cross  Abstract: In high-energy particle collisions, the primary collision products usually decay further resulting in tree-like, hierarchical structures with a priori unknown multiplicity. At the stable-particle level all decay products of a collision form permutation invariant sets of final state objects. The analogy to mathematical graphs gives rise to the idea that graph neural networks (GNNs), which naturally resemble these properties, should be best-suited to address many tasks related to high-energy particle physics. In this paper we describe a benchmark test of a typical GNN against neural networks of the well-established deep fully-connected feed-forward architecture. We aim at performing this comparison maximally unbiased in terms of nodes, hidden layers, or trainable parameters of the neural networks under study. As physics case we use the classification of the final state X produced in association with top quark-antiquark pairs in proton-pr
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#24433;&#21709;LLM&#25903;&#25345;&#20915;&#31574;&#30340;&#20915;&#23450;&#22240;&#32032;&#65292;&#21253;&#25324;&#25216;&#26415;&#26041;&#38754;&#30340;&#36879;&#26126;&#24230;&#21644;&#21450;&#26102;&#24037;&#31243;&#12289;&#24515;&#29702;&#22240;&#32032;&#22914;&#24773;&#32490;&#21644;&#20915;&#31574;&#39118;&#26684;&#65292;&#20197;&#21450;&#20915;&#31574;&#29305;&#23450;&#22240;&#32032;&#22914;&#20219;&#21153;&#38590;&#24230;&#21644;&#38382;&#36131;&#21046;&#12290;</title><link>https://arxiv.org/abs/2402.17385</link><description>&lt;p&gt;
LLM&#36741;&#21161;&#20915;&#31574;&#30340;&#20915;&#23450;&#22240;&#32032;
&lt;/p&gt;
&lt;p&gt;
Determinants of LLM-assisted Decision-Making
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17385
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#24433;&#21709;LLM&#25903;&#25345;&#20915;&#31574;&#30340;&#20915;&#23450;&#22240;&#32032;&#65292;&#21253;&#25324;&#25216;&#26415;&#26041;&#38754;&#30340;&#36879;&#26126;&#24230;&#21644;&#21450;&#26102;&#24037;&#31243;&#12289;&#24515;&#29702;&#22240;&#32032;&#22914;&#24773;&#32490;&#21644;&#20915;&#31574;&#39118;&#26684;&#65292;&#20197;&#21450;&#20915;&#31574;&#29305;&#23450;&#22240;&#32032;&#22914;&#20219;&#21153;&#38590;&#24230;&#21644;&#38382;&#36131;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20915;&#31574;&#26159;&#26085;&#24120;&#29983;&#27963;&#20013;&#30340;&#19968;&#39033;&#22522;&#26412;&#33021;&#21147;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25552;&#20379;&#22810;&#26041;&#38754;&#25903;&#25345;&#65292;&#22686;&#24378;&#20154;&#31867;&#20915;&#31574;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#29702;&#35299;LLM&#36741;&#21161;&#20915;&#31574;&#30340;&#24433;&#21709;&#22240;&#32032;&#23545;&#20110;&#20351;&#20010;&#20307;&#33021;&#22815;&#21033;&#29992;LLM&#25552;&#20379;&#30340;&#20248;&#21183;&#24182;&#26368;&#23567;&#21270;&#30456;&#20851;&#39118;&#38505;&#65292;&#20197;&#20415;&#20570;&#20986;&#26356;&#21152;&#26126;&#26234;&#21644;&#26356;&#22909;&#30340;&#20915;&#31574;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#39033;&#20840;&#38754;&#25991;&#29486;&#20998;&#26512;&#30340;&#32467;&#26524;&#65292;&#25552;&#20379;&#20102;&#24433;&#21709;LLM&#25903;&#25345;&#20915;&#31574;&#30340;&#20915;&#23450;&#22240;&#32032;&#30340;&#32467;&#26500;&#27010;&#36848;&#21644;&#35814;&#32454;&#20998;&#26512;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;LLMs&#30340;&#25216;&#26415;&#26041;&#38754;&#30340;&#25928;&#24212;&#65292;&#21253;&#25324;&#36879;&#26126;&#24230;&#21644;&#21450;&#26102;&#24037;&#31243;&#65292;&#24515;&#29702;&#22240;&#32032;&#22914;&#24773;&#32490;&#21644;&#20915;&#31574;&#39118;&#26684;&#65292;&#20197;&#21450;&#29305;&#23450;&#20110;&#20915;&#31574;&#30340;&#20915;&#23450;&#22240;&#32032;&#65292;&#22914;&#20219;&#21153;&#38590;&#24230;&#21644;&#38382;&#36131;&#21046;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17385v1 Announce Type: new  Abstract: Decision-making is a fundamental capability in everyday life. Large Language Models (LLMs) provide multifaceted support in enhancing human decision-making processes. However, understanding the influencing factors of LLM-assisted decision-making is crucial for enabling individuals to utilize LLM-provided advantages and minimize associated risks in order to make more informed and better decisions. This study presents the results of a comprehensive literature analysis, providing a structural overview and detailed analysis of determinants impacting decision-making with LLM support. In particular, we explore the effects of technological aspects of LLMs, including transparency and prompt engineering, psychological factors such as emotions and decision-making styles, as well as decision-specific determinants such as task difficulty and accountability. In addition, the impact of the determinants on the decision-making process is illustrated via 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#29992;&#20110;&#35774;&#35745;&#20248;&#21270;&#38382;&#39064;&#65292;&#26088;&#22312;&#36890;&#36807;&#23547;&#25214;&#26356;&#21512;&#36866;&#30340;&#26102;&#38388;&#27493;&#38271;&#21152;&#36895;&#25193;&#25955;&#37319;&#26679;&#12290;</title><link>https://arxiv.org/abs/2402.17376</link><description>&lt;p&gt;
&#20248;&#21270;&#26102;&#38388;&#27493;&#38271;&#21152;&#36895;&#25193;&#25955;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
Accelerating Diffusion Sampling with Optimized Time Steps
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17376
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#29992;&#20110;&#35774;&#35745;&#20248;&#21270;&#38382;&#39064;&#65292;&#26088;&#22312;&#36890;&#36807;&#23547;&#25214;&#26356;&#21512;&#36866;&#30340;&#26102;&#38388;&#27493;&#38271;&#21152;&#36895;&#25193;&#25955;&#37319;&#26679;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;DPMs&#65289;&#22312;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#21512;&#25104;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#30001;&#20110;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#37319;&#26679;&#27493;&#39588;&#65292;&#20854;&#37319;&#26679;&#25928;&#29575;&#20173;&#26377;&#24453;&#25552;&#39640;&#12290;&#36817;&#26399;&#39640;&#38454;&#25968;&#20540;ODE&#27714;&#35299;&#22120;&#22312;DPMs&#20013;&#30340;&#24212;&#29992;&#20351;&#24471;&#29992;&#26356;&#23569;&#30340;&#37319;&#26679;&#27493;&#39588;&#29983;&#25104;&#39640;&#36136;&#37327;&#22270;&#20687;&#25104;&#20026;&#21487;&#33021;&#12290;&#23613;&#31649;&#36825;&#26159;&#19968;&#39033;&#37325;&#22823;&#36827;&#23637;&#65292;&#22823;&#22810;&#25968;&#37319;&#26679;&#26041;&#27861;&#20173;&#28982;&#37319;&#29992;&#22343;&#21248;&#26102;&#38388;&#27493;&#38271;&#65292;&#32780;&#22312;&#37319;&#26679;&#27493;&#39588;&#36739;&#23569;&#26102;&#24182;&#19981;&#26159;&#26368;&#20339;&#36873;&#25321;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#29992;&#20110;&#35774;&#35745;&#19968;&#20010;&#20248;&#21270;&#38382;&#39064;&#65292;&#35813;&#20248;&#21270;&#38382;&#39064;&#26088;&#22312;&#20026;DPMs&#30340;&#29305;&#23450;&#25968;&#20540;ODE&#27714;&#35299;&#22120;&#23547;&#25214;&#26356;&#21512;&#36866;&#30340;&#26102;&#38388;&#27493;&#38271;&#12290;&#27492;&#20248;&#21270;&#38382;&#39064;&#26088;&#22312;&#26368;&#23567;&#21270;&#22320;&#23454;&#29616;&#22320;&#30495;&#23454;&#35299;&#19982;&#19982;&#25968;&#20540;&#27714;&#35299;&#22120;&#23545;&#24212;&#30340;&#36817;&#20284;&#35299;&#20043;&#38388;&#30340;&#36317;&#31163;&#12290;&#23427;&#21487;&#20197;&#36890;&#36807;&#21463;&#38480;&#20449;&#36182;&#22495;&#26041;&#27861;&#36827;&#34892;&#39640;&#25928;&#27714;&#35299;&#65292;&#26102;&#38388;&#23569;&#20110;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17376v1 Announce Type: cross  Abstract: Diffusion probabilistic models (DPMs) have shown remarkable performance in high-resolution image synthesis, but their sampling efficiency is still to be desired due to the typically large number of sampling steps. Recent advancements in high-order numerical ODE solvers for DPMs have enabled the generation of high-quality images with much fewer sampling steps. While this is a significant development, most sampling methods still employ uniform time steps, which is not optimal when using a small number of steps. To address this issue, we propose a general framework for designing an optimization problem that seeks more appropriate time steps for a specific numerical ODE solver for DPMs. This optimization problem aims to minimize the distance between the ground-truth solution to the ODE and an approximate solution corresponding to the numerical solver. It can be efficiently solved using the constrained trust region method, taking less than 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;CAPT&#26041;&#27861;&#65292;&#20351;&#29992;Transformer&#20174;&#21333;&#20010;&#28857;&#20113;&#20013;&#20934;&#30830;&#20272;&#35745;&#21508;&#31181;&#32852;&#21160;&#29289;&#20307;&#30340;&#20851;&#33410;&#21442;&#25968;&#21644;&#29366;&#24577;&#65292;&#24341;&#20837;&#20102;&#36816;&#21160;&#25439;&#22833;&#26041;&#27861;&#21644;&#21452;&#37325;&#25237;&#31080;&#31574;&#30053;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#20248;&#20110;&#29616;&#26377;&#30340;&#20854;&#20182;&#36873;&#25321;</title><link>https://arxiv.org/abs/2402.17360</link><description>&lt;p&gt;
CAPT: &#20351;&#29992;Transformer&#20174;&#21333;&#20010;&#28857;&#20113;&#20272;&#35745;&#31867;&#21035;&#32423;&#32852;&#30340;&#20851;&#33410;&#21442;&#25968;
&lt;/p&gt;
&lt;p&gt;
CAPT: Category-level Articulation Estimation from a Single Point Cloud Using Transformer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17360
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;CAPT&#26041;&#27861;&#65292;&#20351;&#29992;Transformer&#20174;&#21333;&#20010;&#28857;&#20113;&#20013;&#20934;&#30830;&#20272;&#35745;&#21508;&#31181;&#32852;&#21160;&#29289;&#20307;&#30340;&#20851;&#33410;&#21442;&#25968;&#21644;&#29366;&#24577;&#65292;&#24341;&#20837;&#20102;&#36816;&#21160;&#25439;&#22833;&#26041;&#27861;&#21644;&#21452;&#37325;&#25237;&#31080;&#31574;&#30053;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#20248;&#20110;&#29616;&#26377;&#30340;&#20854;&#20182;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20272;&#35745;&#20851;&#33410;&#21442;&#25968;&#23545;&#20110;&#26426;&#22120;&#20154;&#23398;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#30340;&#21508;&#31181;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;CAPT&#65306;&#20351;&#29992;Transformer&#20174;&#28857;&#20113;&#20013;&#20272;&#35745;&#31867;&#21035;&#32423;&#32852;&#20851;&#33410;&#21442;&#25968;&#12290;CAPT&#20351;&#29992;&#31471;&#21040;&#31471;&#30340;&#22522;&#20110;Transformer&#30340;&#26550;&#26500;&#65292;&#20174;&#21333;&#20010;&#28857;&#20113;&#20013;&#23545;&#32852;&#21160;&#29289;&#20307;&#30340;&#20851;&#33410;&#21442;&#25968;&#21644;&#29366;&#24577;&#36827;&#34892;&#20272;&#35745;&#12290;&#25152;&#25552;&#20986;&#30340;CAPT&#26041;&#27861;&#21487;&#20197;&#20934;&#30830;&#20272;&#35745;&#21508;&#31181;&#32852;&#21160;&#29289;&#20307;&#30340;&#20851;&#33410;&#21442;&#25968;&#21644;&#29366;&#24577;&#65292;&#20855;&#26377;&#39640;&#31934;&#24230;&#21644;&#40065;&#26834;&#24615;&#12290;&#35813;&#35770;&#25991;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#36816;&#21160;&#25439;&#22833;&#26041;&#27861;&#65292;&#36890;&#36807;&#24378;&#35843;&#32852;&#21160;&#29289;&#20307;&#30340;&#21160;&#24577;&#29305;&#24449;&#26469;&#25913;&#21892;&#20851;&#33410;&#21442;&#25968;&#20272;&#35745;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#21452;&#37325;&#25237;&#31080;&#31574;&#30053;&#65292;&#20026;&#26694;&#26550;&#25552;&#20379;&#30001;&#31895;&#21040;&#32454;&#30340;&#21442;&#25968;&#20272;&#35745;&#12290;&#22312;&#20960;&#20010;&#31867;&#21035;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20851;&#33410;&#21442;&#25968;&#20272;&#35745;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#20854;&#20182;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17360v1 Announce Type: cross  Abstract: The ability to estimate joint parameters is essential for various applications in robotics and computer vision. In this paper, we propose CAPT: category-level articulation estimation from a point cloud using Transformer. CAPT uses an end-to-end transformer-based architecture for joint parameter and state estimation of articulated objects from a single point cloud. The proposed CAPT methods accurately estimate joint parameters and states for various articulated objects with high precision and robustness. The paper also introduces a motion loss approach, which improves articulation estimation performance by emphasizing the dynamic features of articulated objects. Additionally, the paper presents a double voting strategy to provide the framework with coarse-to-fine parameter estimation. Experimental results on several category datasets demonstrate that our methods outperform existing alternatives for articulation estimation. Our research 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LocalGCL&#30340;&#26032;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#25513;&#30721;&#24314;&#27169;&#34917;&#20805;&#22320;&#25429;&#25417;&#23616;&#37096;&#22270;&#20449;&#24687;&#65292;&#20248;&#20110;&#20256;&#32479;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.17345</link><description>&lt;p&gt;
LocalGCL&#65306;&#38754;&#21521;&#22270;&#30340;&#23616;&#37096;&#24863;&#30693;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
LocalGCL: Local-aware Contrastive Learning for Graphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17345
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LocalGCL&#30340;&#26032;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#25513;&#30721;&#24314;&#27169;&#34917;&#20805;&#22320;&#25429;&#25417;&#23616;&#37096;&#22270;&#20449;&#24687;&#65292;&#20248;&#20110;&#20256;&#32479;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#34920;&#31034;&#23398;&#20064;&#65288;GRL&#65289;&#26368;&#36817;&#21462;&#24471;&#20102;&#30456;&#24403;&#22823;&#30340;&#36827;&#23637;&#65292;&#23427;&#23558;&#22270;&#19982;&#25299;&#25169;&#32467;&#26500;&#32534;&#30721;&#20026;&#20302;&#32500;&#23884;&#20837;&#12290;&#21516;&#26102;&#65292;&#25163;&#21160;&#27880;&#37322;&#22270;&#26631;&#31614;&#30340;&#32791;&#26102;&#21644;&#25104;&#26412;&#39640;&#26114;&#20419;&#20351;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#25216;&#26415;&#30340;&#21457;&#23637;&#12290;&#20316;&#20026;SSL&#30340;&#20027;&#35201;&#26041;&#27861;&#65292;&#23545;&#27604;&#23398;&#20064;&#65288;CL&#65289;&#36890;&#36807;&#21306;&#20998;&#27491;&#36127;&#26679;&#26412;&#26469;&#23398;&#20064;&#20855;&#26377;&#21306;&#20998;&#24615;&#30340;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#24403;&#24212;&#29992;&#20110;&#22270;&#25968;&#25454;&#26102;&#65292;&#23427;&#36807;&#20998;&#24378;&#35843;&#20840;&#23616;&#27169;&#24335;&#32780;&#24573;&#35270;&#20102;&#23616;&#37096;&#32467;&#26500;&#12290;&#20026;&#20102;&#35299;&#20915;&#20197;&#19978;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#65292;&#21363;&#38754;&#21521;&#22270;&#30340;&#23616;&#37096;&#24863;&#30693;&#23545;&#27604;&#23398;&#20064;&#65288;\methname&#65289;&#65292;&#19982;&#26222;&#36890;&#23545;&#27604;&#23398;&#20064;&#30456;&#27604;&#65292;&#23427;&#36890;&#36807;&#22522;&#20110;&#25513;&#30721;&#30340;&#24314;&#27169;&#34917;&#20805;&#22320;&#25429;&#25417;&#23616;&#37096;&#22270;&#20449;&#24687;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#23454;&#20102;\methname &#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17345v1 Announce Type: cross  Abstract: Graph representation learning (GRL) makes considerable progress recently, which encodes graphs with topological structures into low-dimensional embeddings. Meanwhile, the time-consuming and costly process of annotating graph labels manually prompts the growth of self-supervised learning (SSL) techniques. As a dominant approach of SSL, Contrastive learning (CL) learns discriminative representations by differentiating between positive and negative samples. However, when applied to graph data, it overemphasizes global patterns while neglecting local structures. To tackle the above issue, we propose \underline{Local}-aware \underline{G}raph \underline{C}ontrastive \underline{L}earning (\textbf{\methnametrim}), a self-supervised learning framework that supplementarily captures local graph information with masking-based modeling compared with vanilla contrastive learning. Extensive experiments validate the superiority of \methname against st
&lt;/p&gt;</description></item><item><title>SocialCVAE&#20351;&#29992;CVAE&#27169;&#22411;&#26469;&#39044;&#27979;&#34892;&#20154;&#36712;&#36857;&#65292;&#36890;&#36807;&#25506;&#32034;&#34892;&#20026;&#19981;&#30830;&#23450;&#24615;&#24182;&#23398;&#20064;&#31038;&#20132;&#21512;&#29702;&#30340;&#36816;&#21160;&#38543;&#26426;&#24615;&#26469;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.17339</link><description>&lt;p&gt;
SocialCVAE&#65306;&#36890;&#36807;&#20132;&#20114;&#26465;&#20214;&#28508;&#21464;&#37327;&#39044;&#27979;&#34892;&#20154;&#36712;&#36857;
&lt;/p&gt;
&lt;p&gt;
SocialCVAE: Predicting Pedestrian Trajectory via Interaction Conditioned Latents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17339
&lt;/p&gt;
&lt;p&gt;
SocialCVAE&#20351;&#29992;CVAE&#27169;&#22411;&#26469;&#39044;&#27979;&#34892;&#20154;&#36712;&#36857;&#65292;&#36890;&#36807;&#25506;&#32034;&#34892;&#20026;&#19981;&#30830;&#23450;&#24615;&#24182;&#23398;&#20064;&#31038;&#20132;&#21512;&#29702;&#30340;&#36816;&#21160;&#38543;&#26426;&#24615;&#26469;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34892;&#20154;&#36712;&#36857;&#39044;&#27979;&#26159;&#35768;&#22810;&#24212;&#29992;&#31243;&#24207;&#20013;&#30340;&#20851;&#38190;&#25216;&#26415;&#65292;&#21487;&#20197;&#27934;&#23519;&#20154;&#31867;&#34892;&#20026;&#24182;&#39044;&#27979;&#20154;&#31867;&#26410;&#26469;&#21160;&#21521;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#32463;&#39564;&#27169;&#22411;&#26159;&#36890;&#36807;&#35266;&#23519;&#21040;&#30340;&#20154;&#31867;&#34892;&#20026;&#20197;&#21487;&#35299;&#37322;&#30340;&#25968;&#23398;&#26415;&#35821;&#26126;&#30830;&#34920;&#36798;&#20986;&#26469;&#30340;&#65292;&#20855;&#26377;&#30830;&#23450;&#24615;&#65292;&#32780;&#36817;&#26399;&#30340;&#24037;&#20316;&#38598;&#20013;&#22312;&#24320;&#21457;&#28151;&#21512;&#27169;&#22411;&#65292;&#32467;&#21512;&#22522;&#20110;&#23398;&#20064;&#30340;&#25216;&#26415;&#65292;&#20197;&#25552;&#20379;&#24378;&#22823;&#30340;&#34920;&#36798;&#33021;&#21147;&#21516;&#26102;&#20445;&#25345;&#21487;&#35299;&#37322;&#24615;&#12290;&#28982;&#32780;&#65292;&#20174;&#32463;&#39564;&#27169;&#22411;&#20013;&#23398;&#24471;&#30340;&#20855;&#26377;&#30830;&#23450;&#24615;&#30340;&#39550;&#39542;&#34892;&#20026;&#30340;&#24615;&#36136;&#38480;&#21046;&#20102;&#27169;&#22411;&#30340;&#23454;&#38469;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#31038;&#20132;&#26465;&#20214;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;SocialCVAE&#65289;&#29992;&#20110;&#39044;&#27979;&#34892;&#20154;&#36712;&#36857;&#65292;&#23427;&#37319;&#29992;CVAE&#26469;&#25506;&#32034;&#20154;&#31867;&#36816;&#21160;&#20915;&#31574;&#20013;&#30340;&#34892;&#20026;&#19981;&#30830;&#23450;&#24615;&#12290;SocialCVAE&#36890;&#36807;&#21033;&#29992;&#19968;&#20010;&#31038;&#20132;&#21487;&#35299;&#37322;&#30340;&#20132;&#20114;&#33021;&#37327;&#22270;&#26469;&#23398;&#20064;&#31038;&#20132;&#21512;&#29702;&#30340;&#36816;&#21160;&#38543;&#26426;&#24615;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17339v1 Announce Type: cross  Abstract: Pedestrian trajectory prediction is the key technology in many applications for providing insights into human behavior and anticipating human future motions. Most existing empirical models are explicitly formulated by observed human behaviors using explicable mathematical terms with a deterministic nature, while recent work has focused on developing hybrid models combined with learning-based techniques for powerful expressiveness while maintaining explainability. However, the deterministic nature of the learned steering behaviors from the empirical models limits the models' practical performance. To address this issue, this work proposes the social conditional variational autoencoder (SocialCVAE) for predicting pedestrian trajectories, which employs a CVAE to explore behavioral uncertainty in human motion decisions. SocialCVAE learns socially reasonable motion randomness by utilizing a socially explainable interaction energy map as the
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#21019;&#26032;&#26694;&#26550; BiVRec&#65292;&#22312;&#25512;&#33616;&#20219;&#21153;&#20013;&#32852;&#21512;&#35757;&#32451; ID &#21644;&#22810;&#27169;&#24577;&#35270;&#22270;&#65292;&#21452;&#21521;&#22686;&#24378;&#25512;&#33616;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.17334</link><description>&lt;p&gt;
BiVRec: &#21452;&#21521;&#22522;&#20110;&#35270;&#22270;&#30340;&#22810;&#27169;&#24577;&#39034;&#24207;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
BiVRec: Bidirectional View-based Multimodal Sequential Recommendation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17334
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#21019;&#26032;&#26694;&#26550; BiVRec&#65292;&#22312;&#25512;&#33616;&#20219;&#21153;&#20013;&#32852;&#21512;&#35757;&#32451; ID &#21644;&#22810;&#27169;&#24577;&#35270;&#22270;&#65292;&#21452;&#21521;&#22686;&#24378;&#25512;&#33616;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#20449;&#24687;&#34701;&#20837;&#39034;&#24207;&#25512;&#33616;&#31995;&#32479;&#36817;&#26469;&#24341;&#36215;&#20102;&#30740;&#31350;&#30340;&#24191;&#27867;&#20851;&#27880;&#12290;&#22312;&#22810;&#27169;&#24577;&#39034;&#24207;&#25512;&#33616;&#27169;&#22411;&#30340;&#21021;&#26399;&#38454;&#27573;&#65292;&#20027;&#27969;&#33539;&#24335;&#26159;ID&#20027;&#23548;&#25512;&#33616;&#65292;&#21363;&#22810;&#27169;&#24577;&#20449;&#24687;&#20316;&#20026;&#36741;&#21161;&#20449;&#24687;&#36827;&#34892;&#34701;&#21512;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#22312;&#21487;&#36716;&#31227;&#24615;&#21644;&#20449;&#24687;&#20405;&#20837;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#65292;&#21478;&#19968;&#31181;&#33539;&#24335;&#20986;&#29616;&#20102;&#65292;&#21363;&#30452;&#25509;&#21033;&#29992;&#22810;&#27169;&#24577;&#29305;&#24449;&#36827;&#34892;&#25512;&#33616;&#65292;&#23454;&#29616;&#36328;&#25968;&#25454;&#38598;&#30340;&#25512;&#33616;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#23427;&#24573;&#30053;&#20102;&#29992;&#25143;ID&#20449;&#24687;&#65292;&#23548;&#33268;&#20449;&#24687;&#21033;&#29992;&#29575;&#20302;&#21644;&#35757;&#32451;&#25104;&#26412;&#39640;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21019;&#26032;&#26694;&#26550;&#65292;BiVRec&#65292;&#36890;&#36807;&#32852;&#21512;&#35757;&#32451;ID&#21644;&#22810;&#27169;&#24577;&#35270;&#22270;&#20013;&#30340;&#25512;&#33616;&#20219;&#21153;&#65292;&#21033;&#29992;&#23427;&#20204;&#20043;&#38388;&#30340;&#21327;&#21516;&#20851;&#31995;&#21452;&#21521;&#22686;&#24378;&#25512;&#33616;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#20449;&#24687;&#24322;&#26500;&#24615;&#38382;&#39064;&#65292;&#25105;&#20204;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17334v1 Announce Type: cross  Abstract: The integration of multimodal information into sequential recommender systems has attracted significant attention in recent research. In the initial stages of multimodal sequential recommendation models, the mainstream paradigm was ID-dominant recommendations, wherein multimodal information was fused as side information. However, due to their limitations in terms of transferability and information intrusion, another paradigm emerged, wherein multimodal features were employed directly for recommendation, enabling recommendation across datasets. Nonetheless, it overlooked user ID information, resulting in low information utilization and high training costs. To this end, we propose an innovative framework, BivRec, that jointly trains the recommendation tasks in both ID and multimodal views, leveraging their synergistic relationship to enhance recommendation performance bidirectionally. To tackle the information heterogeneity issue, we fir
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30740;&#31350;&#21457;&#29616;&#65292;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20013;&#38388;&#23618;&#33021;&#22815;&#26356;&#22909;&#22320;&#32534;&#30721;&#20840;&#23616;&#35821;&#20041;&#20449;&#24687;&#65292;&#22312;&#35270;&#35273;-&#35821;&#35328;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#39030;&#23618;&#21487;&#33021;&#36807;&#22810;&#20851;&#27880;&#23616;&#37096;&#20449;&#24687;&#65292;&#23548;&#33268;&#29702;&#35299;&#20840;&#23616;&#20449;&#24687;&#30340;&#33021;&#21147;&#19979;&#38477;&#12290;</title><link>https://arxiv.org/abs/2402.17304</link><description>&lt;p&gt;
&#25506;&#31350;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#20840;&#23616;&#21644;&#23616;&#37096;&#35821;&#20041;&#34920;&#31034;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Probing Multimodal Large Language Models for Global and Local Semantic Representation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17304
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30740;&#31350;&#21457;&#29616;&#65292;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20013;&#38388;&#23618;&#33021;&#22815;&#26356;&#22909;&#22320;&#32534;&#30721;&#20840;&#23616;&#35821;&#20041;&#20449;&#24687;&#65292;&#22312;&#35270;&#35273;-&#35821;&#35328;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#39030;&#23618;&#21487;&#33021;&#36807;&#22810;&#20851;&#27880;&#23616;&#37096;&#20449;&#24687;&#65292;&#23548;&#33268;&#29702;&#35299;&#20840;&#23616;&#20449;&#24687;&#30340;&#33021;&#21147;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25104;&#21151;&#21551;&#21457;&#20102;&#30740;&#31350;&#20154;&#21592;&#23558;&#20854;&#20248;&#31168;&#30340;&#34920;&#31034;&#33021;&#21147;&#36716;&#31227;&#21040;&#20854;&#20182;&#27169;&#24577;&#12290;&#26368;&#36817;&#30340;&#19968;&#20123;&#30740;&#31350;&#21033;&#29992;&#22270;&#20687;&#25551;&#36848;&#23545;&#40784;&#25968;&#25454;&#38598;&#35757;&#32451;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#65292;&#22312;&#22270;&#20687;&#21040;&#25991;&#26412;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;&#28982;&#32780;&#65292;&#24456;&#23569;&#26377;&#30740;&#31350;&#25506;&#35752;MLLMs&#26159;&#21542;&#30495;&#27491;&#29702;&#35299;&#23436;&#25972;&#30340;&#22270;&#20687;&#20449;&#24687;&#65292;&#21363;&#20840;&#23616;&#20449;&#24687;&#65292;&#25110;&#32773;&#23427;&#20204;&#21482;&#33021;&#25429;&#25417;&#19968;&#20123;&#23616;&#37096;&#23545;&#35937;&#20449;&#24687;&#12290;&#26412;&#30740;&#31350;&#21457;&#29616;&#27169;&#22411;&#30340;&#20013;&#38388;&#23618;&#21487;&#20197;&#32534;&#30721;&#26356;&#22810;&#20840;&#23616;&#35821;&#20041;&#20449;&#24687;&#65292;&#20854;&#34920;&#31034;&#21521;&#37327;&#22312;&#35270;&#35273;-&#35821;&#35328;&#34164;&#28085;&#20219;&#21153;&#19978;&#34920;&#29616;&#26356;&#22909;&#65292;&#32780;&#19981;&#26159;&#39030;&#23618;&#12290;&#25105;&#20204;&#36890;&#36807;&#30446;&#26631;&#26816;&#27979;&#20219;&#21153;&#36827;&#19968;&#27493;&#25506;&#31350;&#27169;&#22411;&#30340;&#23616;&#37096;&#35821;&#20041;&#34920;&#31034;&#12290;&#25105;&#20204;&#24471;&#20986;&#30340;&#32467;&#35770;&#26159;&#39030;&#23618;&#21487;&#33021;&#36807;&#22810;&#19987;&#27880;&#20110;&#23616;&#37096;&#20449;&#24687;&#65292;&#23548;&#33268;&#20943;&#24369;&#20102;&#23545;&#20840;&#23616;&#20449;&#24687;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17304v1 Announce Type: cross  Abstract: The success of large language models has inspired researchers to transfer their exceptional representing ability to other modalities. Several recent works leverage image-caption alignment datasets to train multimodal large language models (MLLMs), which achieve state-of-the-art performance on image-to-text tasks. However, there are very few studies exploring whether MLLMs truly understand the complete image information, i.e., global information, or if they can only capture some local object information. In this study, we find that the intermediate layers of models can encode more global semantic information, whose representation vectors perform better on visual-language entailment tasks, rather than the topmost layers. We further probe models for local semantic representation through object detection tasks. And we draw a conclusion that the topmost layers may excessively focus on local information, leading to a diminished ability to en
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#36890;&#36807;&#20027;&#21160;&#25511;&#21046;&#21644;&#22609;&#36896;&#26059;&#32764;&#20135;&#29983;&#30340;&#39134;&#34892;&#22120;&#25512;&#36827;&#22122;&#22768;&#26469;&#26377;&#21033;&#20110;&#23450;&#20301;&#20219;&#21153;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#22122;&#22768;&#21644;&#26102;&#38388;&#21464;&#21270;&#26059;&#32764;&#30456;&#20301;&#35843;&#21046;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#23454;&#29616;&#20102;&#20934;&#30830;&#21644;&#31283;&#20581;&#30340;&#23450;&#20301;&#12290;</title><link>https://arxiv.org/abs/2402.17289</link><description>&lt;p&gt;
&#22810;&#26059;&#32764;&#39134;&#34892;&#22120;&#23450;&#20301;&#30340;&#20027;&#21160;&#25512;&#36827;&#22122;&#22768;&#22609;&#36896;
&lt;/p&gt;
&lt;p&gt;
Active propulsion noise shaping for multi-rotor aircraft localization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17289
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#36890;&#36807;&#20027;&#21160;&#25511;&#21046;&#21644;&#22609;&#36896;&#26059;&#32764;&#20135;&#29983;&#30340;&#39134;&#34892;&#22120;&#25512;&#36827;&#22122;&#22768;&#26469;&#26377;&#21033;&#20110;&#23450;&#20301;&#20219;&#21153;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#22122;&#22768;&#21644;&#26102;&#38388;&#21464;&#21270;&#26059;&#32764;&#30456;&#20301;&#35843;&#21046;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#23454;&#29616;&#20102;&#20934;&#30830;&#21644;&#31283;&#20581;&#30340;&#23450;&#20301;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26059;&#32764;&#31354;&#20013;&#33258;&#20027;&#36733;&#20855;(MAVs)&#20027;&#35201;&#20381;&#36182;&#35270;&#35273;&#36827;&#34892;&#23548;&#33322;&#12290;&#28982;&#32780;&#65292;&#35270;&#35273;&#23450;&#20301;&#21644;&#27979;&#36317;&#25216;&#26415;&#22312;&#20302;&#25110;&#30452;&#23556;&#38451;&#20809;&#19979;&#34920;&#29616;&#19981;&#20339;&#65292;&#35270;&#37326;&#26377;&#38480;&#65292;&#24182;&#19988;&#23481;&#26131;&#21463;&#21040;&#36974;&#25377;&#30340;&#24433;&#21709;&#12290;&#22768;&#23398;&#20256;&#24863;&#21487;&#20197;&#20316;&#20026;&#35270;&#35273;&#30340;&#34917;&#20805;&#25110;&#29978;&#33267;&#26367;&#20195;&#26041;&#24335;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#20351;&#29992;&#65292;&#32780;&#19988;&#36824;&#20855;&#26377;&#26356;&#20302;&#30340;&#31995;&#32479;&#25104;&#26412;&#21644;&#33021;&#28304;&#21344;&#29992;&#37327;&#65292;&#36825;&#23545;&#24494;&#22411;&#39134;&#34892;&#22120;&#23588;&#20026;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#36890;&#36807;&#20027;&#21160;&#25511;&#21046;&#21644;&#22609;&#36896;&#30001;&#26059;&#32764;&#20135;&#29983;&#30340;&#39134;&#34892;&#22120;&#25512;&#36827;&#22122;&#22768;&#65292;&#20197;&#26377;&#21033;&#20110;&#23450;&#20301;&#20219;&#21153;&#65292;&#32780;&#38750;&#23558;&#20854;&#35270;&#20026;&#26377;&#23475;&#22122;&#22768;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#22122;&#22768;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#29992;&#20110;&#22312;&#24050;&#30693;&#29615;&#22659;&#20013;&#36827;&#34892;&#23450;&#20301;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#21516;&#26102;&#35757;&#32451;&#23398;&#20064;&#26102;&#38388;&#21464;&#21270;&#30340;&#26059;&#32764;&#30456;&#20301;&#35843;&#21046;&#65292;&#21487;&#20197;&#23454;&#29616;&#20934;&#30830;&#21644;&#31283;&#20581;&#30340;&#23450;&#20301;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17289v1 Announce Type: cross  Abstract: Multi-rotor aerial autonomous vehicles (MAVs) primarily rely on vision for navigation purposes. However, visual localization and odometry techniques suffer from poor performance in low or direct sunlight, a limited field of view, and vulnerability to occlusions. Acoustic sensing can serve as a complementary or even alternative modality for vision in many situations, and it also has the added benefits of lower system cost and energy footprint, which is especially important for micro aircraft. This paper proposes actively controlling and shaping the aircraft propulsion noise generated by the rotors to benefit localization tasks, rather than considering it a harmful nuisance. We present a neural network architecture for selfnoise-based localization in a known environment. We show that training it simultaneously with learning time-varying rotor phase modulation achieves accurate and robust localization. The proposed methods are evaluated u
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;Group-Autoencoder&#26694;&#26550;&#65292;&#19982;&#25193;&#25955;&#27169;&#22411;&#21327;&#21516;&#32452;&#21512;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;HSI&#36229;&#20998;&#36776;&#27169;&#22411;&#65288;DMGASR&#65289;&#12290;</title><link>https://arxiv.org/abs/2402.17285</link><description>&lt;p&gt;
&#36890;&#36807;&#25193;&#25955;&#27169;&#22411;&#21644;&#32452;-&#33258;&#32534;&#30721;&#22120;&#36229;&#20998;&#36776;&#32593;&#32476;&#22686;&#24378;&#39640;&#20809;&#35889;&#22270;&#20687;
&lt;/p&gt;
&lt;p&gt;
Enhancing Hyperspectral Images via Diffusion Model and Group-Autoencoder Super-resolution Network
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17285
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;Group-Autoencoder&#26694;&#26550;&#65292;&#19982;&#25193;&#25955;&#27169;&#22411;&#21327;&#21516;&#32452;&#21512;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;HSI&#36229;&#20998;&#36776;&#27169;&#22411;&#65288;DMGASR&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#39640;&#20809;&#35889;&#22270;&#20687;&#65288;HSI&#65289;&#36229;&#20998;&#36776;&#65288;SR&#65289;&#26041;&#27861;&#22312;&#26377;&#25928;&#25429;&#25417;&#22797;&#26434;&#30340;&#20809;&#35889;-&#31354;&#38388;&#20851;&#31995;&#21644;&#20302;&#32423;&#32454;&#33410;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#32780;&#25193;&#25955;&#27169;&#22411;&#20197;&#20854;&#22312;&#24314;&#27169;&#22797;&#26434;&#20851;&#31995;&#21644;&#23398;&#20064;&#39640;&#20302;&#32423;&#35270;&#35273;&#29305;&#24449;&#26041;&#38754;&#30340;&#21331;&#36234;&#34920;&#29616;&#32780;&#38395;&#21517;&#12290;&#23558;&#25193;&#25955;&#27169;&#22411;&#30452;&#25509;&#24212;&#29992;&#20110;HSI SR&#21463;&#21040;&#25361;&#25112;&#65292;&#20363;&#22914;&#27169;&#22411;&#25910;&#25947;&#22256;&#38590;&#21644;&#25512;&#29702;&#26102;&#38388;&#24310;&#38271;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32452;-&#33258;&#32534;&#30721;&#22120;&#65288;GAE&#65289;&#26694;&#26550;&#65292;&#19982;&#25193;&#25955;&#27169;&#22411;&#21327;&#21516;&#32452;&#21512;&#26500;&#24314;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;HSI SR&#27169;&#22411;&#65288;DMGASR&#65289;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;GAE&#26694;&#26550;&#23558;&#39640;&#32500;HSI&#25968;&#25454;&#32534;&#30721;&#20026;&#25193;&#25955;&#27169;&#22411;&#21487;&#20197;&#36816;&#34892;&#30340;&#20302;&#32500;&#28508;&#22312;&#31354;&#38388;&#65292;&#20174;&#32780;&#20943;&#36731;&#20102;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;&#30340;&#22256;&#38590;&#65292;&#21516;&#26102;&#20445;&#25345;&#27874;&#27573;&#30456;&#20851;&#24615;&#24182;&#26174;&#33879;&#20943;&#23569;&#25512;&#29702;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17285v1 Announce Type: cross  Abstract: Existing hyperspectral image (HSI) super-resolution (SR) methods struggle to effectively capture the complex spectral-spatial relationships and low-level details, while diffusion models represent a promising generative model known for their exceptional performance in modeling complex relations and learning high and low-level visual features. The direct application of diffusion models to HSI SR is hampered by challenges such as difficulties in model convergence and protracted inference time. In this work, we introduce a novel Group-Autoencoder (GAE) framework that synergistically combines with the diffusion model to construct a highly effective HSI SR model (DMGASR). Our proposed GAE framework encodes high-dimensional HSI data into low-dimensional latent space where the diffusion model works, thereby alleviating the difficulty of training the diffusion model while maintaining band correlation and considerably reducing inference time. Ex
&lt;/p&gt;</description></item><item><title>&#35843;&#26597;&#20102;&#22810;&#26234;&#33021;&#20307;&#12289;&#20154;&#26234;&#33021;&#20307;&#21644;&#20154;&#24037;&#26234;&#33021;&#26234;&#33021;&#20307;&#22312;&#31038;&#20250;&#22256;&#22659;&#21512;&#20316;&#20013;&#30340;&#19977;&#20010;&#20851;&#38190;&#39046;&#22495;&#65292;&#35752;&#35770;&#20102;&#21512;&#20316;&#30340;&#21160;&#26426;&#12289;&#31574;&#30053;&#12289;&#20154;&#31867;&#20559;&#35265;&#65292;&#20197;&#21450;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>https://arxiv.org/abs/2402.17270</link><description>&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#12289;&#20154;&#26234;&#33021;&#20307;&#21450;&#20854;&#36827;&#23637;&#65306;&#21512;&#20316;&#22312;&#31038;&#20250;&#22256;&#22659;&#20013;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Multi-Agent, Human-Agent and Beyond: A Survey on Cooperation in Social Dilemmas
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17270
&lt;/p&gt;
&lt;p&gt;
&#35843;&#26597;&#20102;&#22810;&#26234;&#33021;&#20307;&#12289;&#20154;&#26234;&#33021;&#20307;&#21644;&#20154;&#24037;&#26234;&#33021;&#26234;&#33021;&#20307;&#22312;&#31038;&#20250;&#22256;&#22659;&#21512;&#20316;&#20013;&#30340;&#19977;&#20010;&#20851;&#38190;&#39046;&#22495;&#65292;&#35752;&#35770;&#20102;&#21512;&#20316;&#30340;&#21160;&#26426;&#12289;&#31574;&#30053;&#12289;&#20154;&#31867;&#20559;&#35265;&#65292;&#20197;&#21450;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31038;&#20250;&#22256;&#22659;&#20013;&#30740;&#31350;&#21512;&#20316;&#38271;&#26399;&#20197;&#26469;&#19968;&#30452;&#26159;&#21508;&#31181;&#23398;&#31185;&#30340;&#22522;&#26412;&#35838;&#39064;&#65292;&#21253;&#25324;&#35745;&#31639;&#26426;&#31185;&#23398;&#21644;&#31038;&#20250;&#31185;&#23398;&#12290;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#26368;&#26032;&#36827;&#23637;&#26174;&#33879;&#37325;&#22609;&#20102;&#36825;&#19968;&#39046;&#22495;&#65292;&#20026;&#29702;&#35299;&#21644;&#22686;&#24378;&#21512;&#20316;&#25552;&#20379;&#20102;&#26032;&#30340;&#35265;&#35299;&#12290;&#26412;&#35843;&#26597;&#32771;&#23519;&#20102;&#20154;&#24037;&#26234;&#33021;&#21644;&#31038;&#20250;&#22256;&#22659;&#21512;&#20316;&#20132;&#27719;&#22788;&#30340;&#19977;&#20010;&#20851;&#38190;&#39046;&#22495;&#12290;&#39318;&#20808;&#65292;&#30528;&#37325;&#20110;&#22810;&#26234;&#33021;&#20307;&#21512;&#20316;&#65292;&#25105;&#20204;&#23457;&#26597;&#20102;&#25903;&#25345;&#29702;&#24615;&#26234;&#33021;&#20307;&#20043;&#38388;&#21512;&#20316;&#30340;&#20869;&#22312;&#21644;&#22806;&#22312;&#21160;&#26426;&#65292;&#20197;&#21450;&#29992;&#20110;&#21046;&#23450;&#26377;&#25928;&#31574;&#30053;&#23545;&#25239;&#19981;&#21516;&#23545;&#25163;&#30340;&#26041;&#27861;&#12290;&#20854;&#27425;&#65292;&#25506;&#35752;&#20102;&#20154;&#26234;&#33021;&#20307;&#21512;&#20316;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#24403;&#21069;&#29992;&#20110;&#19982;&#20154;&#31867;&#21512;&#20316;&#30340;&#20154;&#24037;&#26234;&#33021;&#31639;&#27861;&#65292;&#20197;&#21450;&#20154;&#31867;&#23545;&#20154;&#24037;&#26234;&#33021;&#26234;&#33021;&#20307;&#30340;&#20559;&#35265;&#12290;&#31532;&#19977;&#65292;&#25105;&#20204;&#23457;&#26597;&#20102;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#26234;&#33021;&#20307;&#22686;&#24378;&#20154;&#31867;&#21512;&#20316;&#30340;&#26032;&#20852;&#39046;&#22495;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#65292;&#20363;&#22914; u
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17270v1 Announce Type: new  Abstract: The study of cooperation within social dilemmas has long been a fundamental topic across various disciplines, including computer science and social science. Recent advancements in Artificial Intelligence (AI) have significantly reshaped this field, offering fresh insights into understanding and enhancing cooperation. This survey examines three key areas at the intersection of AI and cooperation in social dilemmas. First, focusing on multi-agent cooperation, we review the intrinsic and external motivations that support cooperation among rational agents, and the methods employed to develop effective strategies against diverse opponents. Second, looking into human-agent cooperation, we discuss the current AI algorithms for cooperating with humans and the human biases towards AI agents. Third, we review the emergent field of leveraging AI agents to enhance cooperation among humans. We conclude by discussing future research avenues, such as u
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25506;&#35752;&#20102;&#22810;&#36718;&#23545;&#35805;&#20013;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#28431;&#27934;&#65292;&#25351;&#20986;&#20154;&#31867;&#21487;&#20197;&#36890;&#36807;&#22810;&#36718;&#23545;&#35805;&#35825;&#20351;&#20854;&#29983;&#25104;&#26377;&#23475;&#20449;&#24687;&#12290;</title><link>https://arxiv.org/abs/2402.17262</link><description>&lt;p&gt;
&#22833;&#35328;&#65306;&#22810;&#36718;&#23545;&#35805;&#20013;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23433;&#20840;&#28431;&#27934;
&lt;/p&gt;
&lt;p&gt;
Speak Out of Turn: Safety Vulnerability of Large Language Models in Multi-turn Dialogue
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17262
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25506;&#35752;&#20102;&#22810;&#36718;&#23545;&#35805;&#20013;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#28431;&#27934;&#65292;&#25351;&#20986;&#20154;&#31867;&#21487;&#20197;&#36890;&#36807;&#22810;&#36718;&#23545;&#35805;&#35825;&#20351;&#20854;&#29983;&#25104;&#26377;&#23475;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#24050;&#34987;&#35777;&#26126;&#22312;&#38754;&#20020;"&#36234;&#29425;"&#26102;&#20250;&#20135;&#29983;&#38750;&#27861;&#25110;&#19981;&#36947;&#24503;&#30340;&#22238;&#24212;&#12290; "&#36234;&#29425;"&#30740;&#31350;&#24378;&#35843;&#20102;LLMs&#30340;&#23433;&#20840;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#21333;&#36718;&#23545;&#35805;&#19978;&#65292;&#24573;&#35270;&#20102;&#22810;&#36718;&#23545;&#35805;&#21487;&#33021;&#24102;&#26469;&#30340;&#22797;&#26434;&#24615;&#21644;&#39118;&#38505;&#65292;&#36825;&#26159;&#20154;&#31867;&#20174;LLMs&#33719;&#21462;&#20449;&#24687;&#30340;&#20851;&#38190;&#26041;&#24335;&#12290;&#26412;&#25991;&#35748;&#20026;&#20154;&#31867;&#21487;&#20197;&#21033;&#29992;&#22810;&#36718;&#23545;&#35805;&#35825;&#20351;LLMs&#29983;&#25104;&#26377;&#23475;&#20449;&#24687;&#12290;LLMs&#21487;&#33021;&#19981;&#20250;&#25298;&#32477;&#35686;&#21578;&#24615;&#25110;&#36793;&#30028;&#19981;&#23433;&#20840;&#30340;&#26597;&#35810;&#65292;&#21363;&#20351;&#22312;&#22810;&#36718;&#23545;&#35805;&#20013;&#27599;&#20010;&#22238;&#21512;&#37117;&#34987;&#26381;&#21153;&#20110;&#19968;&#20010;&#24694;&#24847;&#30446;&#30340;&#12290;&#22240;&#27492;&#65292;&#36890;&#36807;&#23558;&#19968;&#20010;&#19981;&#23433;&#20840;&#26597;&#35810;&#20998;&#35299;&#20026;&#22810;&#20010;&#23376;&#26597;&#35810;&#29992;&#20110;&#22810;&#36718;&#23545;&#35805;&#65292;&#25105;&#20204;&#36880;&#28176;&#35825;&#20351;LLMs&#22238;&#31572;&#26377;&#23475;&#30340;&#23376;&#38382;&#39064;&#65292;&#26368;&#32456;&#23548;&#33268;&#24635;&#20307;&#26377;&#23475;&#21709;&#24212;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#36328;&#36234;&#20102;&#24191;&#27867;&#30340;&#33539;&#22260;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17262v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have been demonstrated to generate illegal or unethical responses, particularly when subjected to "jailbreak." Research on jailbreak has highlighted the safety issues of LLMs. However, prior studies have predominantly focused on single-turn dialogue, ignoring the potential complexities and risks presented by multi-turn dialogue, a crucial mode through which humans derive information from LLMs. In this paper, we argue that humans could exploit multi-turn dialogue to induce LLMs into generating harmful information. LLMs may not intend to reject cautionary or borderline unsafe queries, even if each turn is closely served for one malicious purpose in a multi-turn dialogue. Therefore, by decomposing an unsafe query into several sub-queries for multi-turn dialogue, we induced LLMs to answer harmful sub-questions incrementally, culminating in an overall harmful response. Our experiments, conducted across a wide ra
&lt;/p&gt;</description></item><item><title>RIME&#26159;&#19968;&#31181;&#38024;&#23545;&#22024;&#26434;&#20559;&#22909;&#30340;&#20581;&#22766;PbRL&#31639;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#36807;&#28388;&#21435;&#22122;&#20559;&#22909;&#21644;&#28909;&#21551;&#21160;&#22870;&#21169;&#27169;&#22411;&#65292;&#26497;&#22823;&#22686;&#24378;&#20102;&#29616;&#26377;&#26368;&#20808;&#36827;PbRL&#26041;&#27861;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.17257</link><description>&lt;p&gt;
RIME: &#20855;&#26377;&#22024;&#26434;&#20559;&#22909;&#30340;&#20581;&#22766;&#20559;&#22909;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
RIME: Robust Preference-based Reinforcement Learning with Noisy Preferences
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17257
&lt;/p&gt;
&lt;p&gt;
RIME&#26159;&#19968;&#31181;&#38024;&#23545;&#22024;&#26434;&#20559;&#22909;&#30340;&#20581;&#22766;PbRL&#31639;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#36807;&#28388;&#21435;&#22122;&#20559;&#22909;&#21644;&#28909;&#21551;&#21160;&#22870;&#21169;&#27169;&#22411;&#65292;&#26497;&#22823;&#22686;&#24378;&#20102;&#29616;&#26377;&#26368;&#20808;&#36827;PbRL&#26041;&#27861;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20559;&#22909;&#24378;&#21270;&#23398;&#20064;&#65288;PbRL&#65289;&#36890;&#36807;&#21033;&#29992;&#20154;&#31867;&#20559;&#22909;&#20316;&#20026;&#22870;&#21169;&#20449;&#21495;&#65292;&#36991;&#20813;&#20102;&#23545;&#22870;&#21169;&#35774;&#35745;&#30340;&#38656;&#27714;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;PbRL&#31639;&#27861;&#36807;&#24230;&#20381;&#36182;&#26469;&#33258;&#39046;&#22495;&#19987;&#23478;&#30340;&#39640;&#36136;&#37327;&#21453;&#39304;&#65292;&#23548;&#33268;&#32570;&#20047;&#40065;&#26834;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;RIME&#65292;&#19968;&#31181;&#38024;&#23545;&#22024;&#26434;&#20559;&#22909;&#30340;&#20581;&#22766;PbRL&#31639;&#27861;&#65292;&#29992;&#20110;&#26377;&#25928;&#22320;&#20174;&#22024;&#26434;&#20559;&#22909;&#20013;&#23398;&#20064;&#22870;&#21169;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#32467;&#21512;&#20102;&#22522;&#20110;&#26679;&#26412;&#36873;&#25321;&#30340;&#37492;&#21035;&#22120;&#65292;&#21160;&#24577;&#36807;&#28388;&#21435;&#22122;&#20559;&#22909;&#20197;&#36827;&#34892;&#20581;&#22766;&#35757;&#32451;&#12290;&#20026;&#20102;&#20943;&#36731;&#36873;&#25321;&#19981;&#27491;&#30830;&#36896;&#25104;&#30340;&#32047;&#31215;&#35823;&#24046;&#65292;&#25105;&#20204;&#25552;&#20986;&#28909;&#21551;&#21160;&#22870;&#21169;&#27169;&#22411;&#65292;&#27492;&#22806;&#36824;&#33021;&#22635;&#34917;PbRL&#20013;&#20174;&#39044;&#35757;&#32451;&#21040;&#22312;&#32447;&#35757;&#32451;&#36807;&#28193;&#26102;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;&#25105;&#20204;&#22312;&#26426;&#22120;&#20154;&#25805;&#32437;&#21644;&#36816;&#21160;&#20219;&#21153;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;RIME&#26174;&#33879;&#25552;&#21319;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;PbRL&#26041;&#27861;&#30340;&#40065;&#26834;&#24615;&#12290;&#28040;&#34701;&#30740;&#31350;&#36827;&#19968;&#27493;&#34920;&#26126;&#65292;&#28909;&#21551;&#21160;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17257v1 Announce Type: cross  Abstract: Preference-based Reinforcement Learning (PbRL) avoids the need for reward engineering by harnessing human preferences as the reward signal. However, current PbRL algorithms over-reliance on high-quality feedback from domain experts, which results in a lack of robustness. In this paper, we present RIME, a robust PbRL algorithm for effective reward learning from noisy preferences. Our method incorporates a sample selection-based discriminator to dynamically filter denoised preferences for robust training. To mitigate the accumulated error caused by incorrect selection, we propose to warm start the reward model, which additionally bridges the performance gap during transition from pre-training to online training in PbRL. Our experiments on robotic manipulation and locomotion tasks demonstrate that RIME significantly enhances the robustness of the current state-of-the-art PbRL method. Ablation studies further demonstrate that the warm star
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#32467;&#21512;&#28145;&#24230;&#23398;&#20064;&#21644;&#38543;&#26426;&#26862;&#26519;&#30340;&#33258;&#36866;&#24212;&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;&#22810;&#20010;&#39044;&#27979;&#23618;&#20013;&#35835;&#21462;&#22270;&#20687;&#12289;&#21512;&#25104;&#35821;&#38899;&#20197;&#21450;&#36827;&#34892;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#32593;&#32476;&#38035;&#40060;&#25915;&#20987;&#26816;&#27979;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.17249</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#35821;&#38899;&#21644;&#35270;&#35273;&#21512;&#25104;&#22312;&#22810;&#23618;&#33258;&#36866;&#24212;&#26694;&#26550;&#20013;&#25913;&#36827;&#32593;&#32476;&#38035;&#40060;&#25915;&#20987;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Deep Learning-Based Speech and Vision Synthesis to Improve Phishing Attack Detection through a Multi-layer Adaptive Framework
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17249
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#32467;&#21512;&#28145;&#24230;&#23398;&#20064;&#21644;&#38543;&#26426;&#26862;&#26519;&#30340;&#33258;&#36866;&#24212;&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;&#22810;&#20010;&#39044;&#27979;&#23618;&#20013;&#35835;&#21462;&#22270;&#20687;&#12289;&#21512;&#25104;&#35821;&#38899;&#20197;&#21450;&#36827;&#34892;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#32593;&#32476;&#38035;&#40060;&#25915;&#20987;&#26816;&#27979;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25915;&#20987;&#32773;&#19981;&#26029;&#28436;&#36827;&#30340;&#26041;&#24335;&#25345;&#32493;&#25913;&#36827;&#20854;&#27450;&#39575;&#25216;&#26415;&#65292;&#20197;&#32469;&#36807;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#32593;&#32476;&#38035;&#40060;&#26816;&#27979;&#26041;&#27861;&#65292;&#32473;&#34892;&#19994;&#21644;&#23398;&#26415;&#30740;&#31350;&#20154;&#21592;&#24102;&#26469;&#20102;&#24040;&#22823;&#25361;&#25112;&#65292;&#22240;&#20026;&#24403;&#21069;&#26041;&#27861;&#26080;&#27861;&#26816;&#27979;&#22797;&#26434;&#30340;&#32593;&#32476;&#38035;&#40060;&#25915;&#20987;&#12290;&#22240;&#27492;&#65292;&#30001;&#20110;&#25915;&#20987;&#32773;&#37319;&#29992;&#30340;&#31574;&#30053;&#26085;&#30410;&#22797;&#26434;&#19988;&#26032;&#31574;&#30053;&#19981;&#26029;&#34987;&#24320;&#21457;&#20197;&#36867;&#36991;&#26816;&#27979;&#65292;&#24403;&#21069;&#21453;&#32593;&#32476;&#38035;&#40060;&#26041;&#27861;&#20173;&#28982;&#23481;&#26131;&#21463;&#21040;&#22797;&#26434;&#32593;&#32476;&#38035;&#40060;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#35843;&#25972;&#30340;&#26694;&#26550;&#65292;&#23558;&#28145;&#24230;&#23398;&#20064;&#21644;&#38543;&#26426;&#26862;&#26519;&#32467;&#21512;&#36215;&#26469;&#65292;&#20174;&#28145;&#24230;&#20266;&#36896;&#35270;&#39057;&#20013;&#35835;&#21462;&#22270;&#20687;&#65292;&#21512;&#25104;&#35821;&#38899;&#65292;&#20197;&#21450;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65292;&#24182;&#22312;&#21508;&#31181;&#39044;&#27979;&#23618;&#20013;&#26174;&#33879;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23545;&#32593;&#32476;&#38035;&#40060;&#25915;&#20987;&#26816;&#27979;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17249v1 Announce Type: cross  Abstract: The ever-evolving ways attacker continues to im prove their phishing techniques to bypass existing state-of-the-art phishing detection methods pose a mountain of challenges to researchers in both industry and academia research due to the inability of current approaches to detect complex phishing attack. Thus, current anti-phishing methods remain vulnerable to complex phishing because of the increasingly sophistication tactics adopted by attacker coupled with the rate at which new tactics are being developed to evade detection. In this research, we proposed an adaptable framework that combines Deep learning and Randon Forest to read images, synthesize speech from deep-fake videos, and natural language processing at various predictions layered to significantly increase the performance of machine learning models for phishing attack detection.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20998;&#20139;&#20102;&#19977;&#20010;&#35265;&#35299;&#65292;&#20197;&#23454;&#29616;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#30340;&#26368;&#26032;&#32654;&#23398;&#36136;&#37327;&#65306;&#22686;&#24378;&#33394;&#24425;&#21644;&#23545;&#27604;&#24230;&#65292;&#25552;&#39640;&#36328;&#22810;&#20010;&#23485;&#39640;&#27604;&#30340;&#29983;&#25104;&#65292;&#25913;&#21892;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#32454;&#33410;&#12290;</title><link>https://arxiv.org/abs/2402.17245</link><description>&lt;p&gt;
Playground v2.5&#65306;&#22686;&#24378;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#20013;&#32654;&#23398;&#36136;&#37327;&#30340;&#19977;&#20010;&#35265;&#35299;
&lt;/p&gt;
&lt;p&gt;
Playground v2.5: Three Insights towards Enhancing Aesthetic Quality in Text-to-Image Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17245
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20998;&#20139;&#20102;&#19977;&#20010;&#35265;&#35299;&#65292;&#20197;&#23454;&#29616;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#30340;&#26368;&#26032;&#32654;&#23398;&#36136;&#37327;&#65306;&#22686;&#24378;&#33394;&#24425;&#21644;&#23545;&#27604;&#24230;&#65292;&#25552;&#39640;&#36328;&#22810;&#20010;&#23485;&#39640;&#27604;&#30340;&#29983;&#25104;&#65292;&#25913;&#21892;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#32454;&#33410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20998;&#20139;&#20102;&#19977;&#20010;&#35265;&#35299;&#65292;&#20197;&#23454;&#29616;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#30340;&#26368;&#26032;&#32654;&#23398;&#36136;&#37327;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#27169;&#22411;&#25913;&#36827;&#30340;&#19977;&#20010;&#20851;&#38190;&#26041;&#38754;&#65306;&#22686;&#24378;&#33394;&#24425;&#21644;&#23545;&#27604;&#24230;&#65292;&#25552;&#39640;&#36328;&#22810;&#20010;&#23485;&#39640;&#27604;&#30340;&#29983;&#25104;&#65292;&#25913;&#21892;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#32454;&#33410;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;&#22122;&#22768;&#35843;&#24230;&#22312;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#37325;&#35201;&#24615;&#65292;&#23637;&#31034;&#20102;&#23427;&#23545;&#29616;&#23454;&#24863;&#21644;&#35270;&#35273;&#20445;&#30495;&#24230;&#30340;&#28145;&#36828;&#24433;&#21709;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#22270;&#20687;&#29983;&#25104;&#20013;&#23481;&#32435;&#21508;&#31181;&#23485;&#39640;&#27604;&#30340;&#25361;&#25112;&#65292;&#24378;&#35843;&#20934;&#22791;&#24179;&#34913;&#30340;&#20998;&#26742;&#25968;&#25454;&#38598;&#30340;&#37325;&#35201;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#23558;&#27169;&#22411;&#36755;&#20986;&#19982;&#20154;&#31867;&#20559;&#22909;&#23545;&#40784;&#30340;&#20851;&#38190;&#20316;&#29992;&#65292;&#30830;&#20445;&#29983;&#25104;&#30340;&#22270;&#20687; resonant with&#20154;&#31867;&#24863;&#30693;&#26399;&#26395;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#20998;&#26512;&#21644;&#23454;&#39564;&#65292;Playground v2.5&#22312;&#21508;&#31181;&#26465;&#20214;&#19979;&#23637;&#31034;&#20102;&#26368;&#20808;&#36827;&#30340;&#32654;&#23398;&#36136;&#37327;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17245v1 Announce Type: cross  Abstract: In this work, we share three insights for achieving state-of-the-art aesthetic quality in text-to-image generative models. We focus on three critical aspects for model improvement: enhancing color and contrast, improving generation across multiple aspect ratios, and improving human-centric fine details. First, we delve into the significance of the noise schedule in training a diffusion model, demonstrating its profound impact on realism and visual fidelity. Second, we address the challenge of accommodating various aspect ratios in image generation, emphasizing the importance of preparing a balanced bucketed dataset. Lastly, we investigate the crucial role of aligning model outputs with human preferences, ensuring that generated images resonate with human perceptual expectations. Through extensive analysis and experiments, Playground v2.5 demonstrates state-of-the-art performance in terms of aesthetic quality under various conditions an
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#26102;&#38388;&#36923;&#36753;&#35268;&#33539;&#26465;&#20214;&#21270;&#20915;&#31574;&#36716;&#25442;&#22120;&#65288;SDT&#65289;&#26694;&#26550;&#65292;&#32467;&#21512;&#20449;&#21495;&#26102;&#38388;&#36923;&#36753;&#65288;STL&#65289;&#21644;&#20915;&#31574;&#36716;&#25442;&#22120;&#65288;DT&#65289;&#30340;&#33021;&#21147;&#65292;&#27604;&#29616;&#26377;&#26041;&#27861;&#22312;&#31163;&#32447;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#20013;&#23398;&#20064;&#20986;&#26356;&#22909;&#30340;&#23433;&#20840;&#39640;&#22870;&#21169;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2402.17217</link><description>&lt;p&gt;
&#31163;&#32447;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#26102;&#38388;&#36923;&#36753;&#35268;&#33539;&#26465;&#20214;&#21270;&#20915;&#31574;&#36716;&#25442;&#22120;
&lt;/p&gt;
&lt;p&gt;
Temporal Logic Specification-Conditioned Decision Transformer for Offline Safe Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17217
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#26102;&#38388;&#36923;&#36753;&#35268;&#33539;&#26465;&#20214;&#21270;&#20915;&#31574;&#36716;&#25442;&#22120;&#65288;SDT&#65289;&#26694;&#26550;&#65292;&#32467;&#21512;&#20449;&#21495;&#26102;&#38388;&#36923;&#36753;&#65288;STL&#65289;&#21644;&#20915;&#31574;&#36716;&#25442;&#22120;&#65288;DT&#65289;&#30340;&#33021;&#21147;&#65292;&#27604;&#29616;&#26377;&#26041;&#27861;&#22312;&#31163;&#32447;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#20013;&#23398;&#20064;&#20986;&#26356;&#22909;&#30340;&#23433;&#20840;&#39640;&#22870;&#21169;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#26088;&#22312;&#20174;&#22266;&#23450;&#25968;&#25454;&#38598;&#35757;&#32451;&#19968;&#20010;&#28385;&#36275;&#32422;&#26463;&#30340;&#31574;&#30053;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#21363;&#26102;&#38388;&#36923;&#36753;&#35268;&#33539;&#26465;&#20214;&#21270;&#20915;&#31574;&#36716;&#25442;&#22120;&#65288;SDT&#65289;&#65292;&#23427;&#21033;&#29992;&#20449;&#21495;&#26102;&#38388;&#36923;&#36753;&#65288;STL&#65289;&#30340;&#34920;&#36798;&#33021;&#21147;&#26469;&#25351;&#23450;&#20195;&#29702;&#24212;&#35813;&#36981;&#24490;&#30340;&#22797;&#26434;&#26102;&#38388;&#35268;&#21017;&#65292;&#20197;&#21450;&#20915;&#31574;&#36716;&#25442;&#22120;&#65288;DT&#65289;&#30340;&#39034;&#24207;&#24314;&#27169;&#33021;&#21147;&#12290;&#23545;DSRL&#22522;&#20934;&#27979;&#35797;&#30340;&#23454;&#35777;&#35780;&#20272;&#34920;&#26126;&#65292;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;SDT&#22312;&#23398;&#20064;&#23433;&#20840;&#39640;&#22870;&#21169;&#31574;&#30053;&#26041;&#38754;&#20855;&#26377;&#26356;&#22909;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17217v1 Announce Type: cross  Abstract: Offline safe reinforcement learning (RL) aims to train a constraint satisfaction policy from a fixed dataset. Current state-of-the-art approaches are based on supervised learning with a conditioned policy. However, these approaches fall short in real-world applications that involve complex tasks with rich temporal and logical structures. In this paper, we propose temporal logic Specification-conditioned Decision Transformer (SDT), a novel framework that harnesses the expressive power of signal temporal logic (STL) to specify complex temporal rules that an agent should follow and the sequential modeling capability of Decision Transformer (DT). Empirical evaluations on the DSRL benchmarks demonstrate the better capacity of SDT in learning safe and high-reward policies compared with existing approaches. In addition, SDT shows good alignment with respect to different desired degrees of satisfaction of the STL specification that it is condi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#20248;&#21270;&#25216;&#26415;&#35299;&#20915;&#20113;&#35745;&#31639;&#36164;&#28304;&#35843;&#24230;&#19982;&#31649;&#29702;&#20013;&#22797;&#26434;&#38382;&#39064;&#30340;&#21019;&#26032;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.17216</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#20248;&#21270;&#22312;&#20113;&#35745;&#31639;&#36164;&#28304;&#35843;&#24230;&#19982;&#31649;&#29702;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Application of Machine Learning Optimization in Cloud Computing Resource Scheduling and Management
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17216
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#20248;&#21270;&#25216;&#26415;&#35299;&#20915;&#20113;&#35745;&#31639;&#36164;&#28304;&#35843;&#24230;&#19982;&#31649;&#29702;&#20013;&#22797;&#26434;&#38382;&#39064;&#30340;&#21019;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20113;&#35745;&#31639;&#34987;&#24191;&#27867;&#24212;&#29992;&#12290;&#20113;&#35745;&#31639;&#26159;&#25351;&#38598;&#20013;&#24335;&#30340;&#35745;&#31639;&#36164;&#28304;&#65292;&#29992;&#25143;&#36890;&#36807;&#35775;&#38382;&#38598;&#20013;&#24335;&#36164;&#28304;&#23436;&#25104;&#35745;&#31639;&#65292;&#20113;&#35745;&#31639;&#20013;&#24515;&#23558;&#31243;&#24207;&#22788;&#29702;&#32467;&#26524;&#36820;&#22238;&#32473;&#29992;&#25143;&#12290;&#20113;&#35745;&#31639;&#19981;&#20165;&#36866;&#29992;&#20110;&#20010;&#20154;&#29992;&#25143;&#65292;&#20063;&#36866;&#29992;&#20110;&#20225;&#19994;&#29992;&#25143;&#12290;&#36141;&#20080;&#20113;&#26381;&#21153;&#22120;&#21518;&#65292;&#29992;&#25143;&#26080;&#38656;&#36141;&#20080;&#22823;&#37327;&#35745;&#31639;&#26426;&#65292;&#33410;&#32422;&#20102;&#35745;&#31639;&#25104;&#26412;&#12290;&#26681;&#25454;&#20013;&#22269;&#32463;&#27982;&#26032;&#38395;&#32593;&#32476;&#30340;&#25253;&#21578;&#65292;&#20013;&#22269;&#30340;&#20113;&#35745;&#31639;&#35268;&#27169;&#24050;&#36798;&#21040;2091&#20159;&#20803;&#20154;&#27665;&#24065;&#12290;&#30446;&#21069;&#65292;&#20013;&#22269;&#36739;&#20026;&#25104;&#29087;&#30340;&#20113;&#26381;&#21153;&#25552;&#20379;&#21830;&#26377;&#38463;&#37324;&#20113;&#12289;&#30334;&#24230;&#20113;&#12289;&#21326;&#20026;&#20113;&#31561;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#20248;&#21270;&#25216;&#26415;&#35299;&#20915;&#20113;&#35745;&#31639;&#36164;&#28304;&#35843;&#24230;&#19982;&#31649;&#29702;&#20013;&#30340;&#22797;&#26434;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17216v1 Announce Type: cross  Abstract: In recent years, cloud computing has been widely used. Cloud computing refers to the centralized computing resources, users through the access to the centralized resources to complete the calculation, the cloud computing center will return the results of the program processing to the user. Cloud computing is not only for individual users, but also for enterprise users. By purchasing a cloud server, users do not have to buy a large number of computers, saving computing costs. According to a report by China Economic News Network, the scale of cloud computing in China has reached 209.1 billion yuan. At present, the more mature cloud service providers in China are Ali Cloud, Baidu Cloud, Huawei Cloud and so on. Therefore, this paper proposes an innovative approach to solve complex problems in cloud computing resource scheduling and management using machine learning optimization techniques. Through in-depth study of challenges such as low r
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#30693;&#35782;&#24211;&#30340;&#22270;&#20687;&#35270;&#35273;&#24120;&#35782;&#21457;&#29616;&#65288;VCD&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#23450;&#20041;&#32454;&#31890;&#24230;&#30340;&#35270;&#35273;&#24120;&#35782;&#31867;&#22411;&#20197;&#21450;&#26500;&#24314;&#21253;&#25324;&#36229;&#36807;10&#19975;&#24352;&#22270;&#20687;&#21644;1400&#19975;&#20010;&#23545;&#35937;-&#24120;&#35782;&#23545;&#30340;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#25552;&#21319;&#35745;&#31639;&#26426;&#35270;&#35273;&#31995;&#32479;&#30340;&#25512;&#29702;&#21644;&#20915;&#31574;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.17213</link><description>&lt;p&gt;
&#22522;&#20110;&#30693;&#35782;&#24211;&#30340;&#22270;&#20687;&#35270;&#35273;&#24120;&#35782;&#21457;&#29616;&#65288;VCD&#65289;
&lt;/p&gt;
&lt;p&gt;
VCD: Knowledge Base Guided Visual Commonsense Discovery in Images
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17213
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#30693;&#35782;&#24211;&#30340;&#22270;&#20687;&#35270;&#35273;&#24120;&#35782;&#21457;&#29616;&#65288;VCD&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#23450;&#20041;&#32454;&#31890;&#24230;&#30340;&#35270;&#35273;&#24120;&#35782;&#31867;&#22411;&#20197;&#21450;&#26500;&#24314;&#21253;&#25324;&#36229;&#36807;10&#19975;&#24352;&#22270;&#20687;&#21644;1400&#19975;&#20010;&#23545;&#35937;-&#24120;&#35782;&#23545;&#30340;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#25552;&#21319;&#35745;&#31639;&#26426;&#35270;&#35273;&#31995;&#32479;&#30340;&#25512;&#29702;&#21644;&#20915;&#31574;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#20013;&#30340;&#35270;&#35273;&#24120;&#35782;&#21253;&#21547;&#26377;&#20851;&#23545;&#35937;&#23646;&#24615;&#12289;&#20851;&#31995;&#21644;&#34892;&#20026;&#30340;&#30693;&#35782;&#12290;&#21457;&#29616;&#35270;&#35273;&#24120;&#35782;&#21487;&#20197;&#25552;&#20379;&#23545;&#22270;&#20687;&#30340;&#26356;&#20840;&#38754;&#21644;&#20016;&#23500;&#30340;&#29702;&#35299;&#65292;&#24182;&#22686;&#24378;&#35745;&#31639;&#26426;&#35270;&#35273;&#31995;&#32479;&#30340;&#25512;&#29702;&#21644;&#20915;&#31574;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#35270;&#35273;&#24120;&#35782;&#21457;&#29616;&#30740;&#31350;&#20013;&#25152;&#23450;&#20041;&#30340;&#35270;&#35273;&#24120;&#35782;&#26159;&#31895;&#31890;&#24230;&#19988;&#19981;&#23436;&#25972;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20174;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#24120;&#35782;&#30693;&#35782;&#24211;ConceptNet&#20013;&#27762;&#21462;&#28789;&#24863;&#65292;&#24182;&#31995;&#32479;&#22320;&#23450;&#20041;&#20102;&#21508;&#31181;&#31867;&#22411;&#30340;&#35270;&#35273;&#24120;&#35782;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#20219;&#21153;&#65292;&#21363;&#35270;&#35273;&#24120;&#35782;&#21457;&#29616;&#65288;VCD&#65289;&#65292;&#26088;&#22312;&#25552;&#21462;&#22270;&#20687;&#20013;&#19981;&#21516;&#23545;&#35937;&#25152;&#21253;&#21547;&#30340;&#19981;&#21516;&#31867;&#22411;&#30340;&#32454;&#31890;&#24230;&#24120;&#35782;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20174;Visual Genome&#21644;ConceptNet&#20013;&#26500;&#24314;&#20102;&#19968;&#20010;&#21517;&#20026;VCDD&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#36229;&#36807;10&#19975;&#24352;&#22270;&#20687;&#21644;1400&#19975;&#20010;&#23545;&#35937;-&#24120;&#35782;&#23545;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17213v1 Announce Type: cross  Abstract: Visual commonsense contains knowledge about object properties, relationships, and behaviors in visual data. Discovering visual commonsense can provide a more comprehensive and richer understanding of images, and enhance the reasoning and decision-making capabilities of computer vision systems. However, the visual commonsense defined in existing visual commonsense discovery studies is coarse-grained and incomplete. In this work, we draw inspiration from a commonsense knowledge base ConceptNet in natural language processing, and systematically define the types of visual commonsense. Based on this, we introduce a new task, Visual Commonsense Discovery (VCD), aiming to extract fine-grained commonsense of different types contained within different objects in the image. We accordingly construct a dataset (VCDD) from Visual Genome and ConceptNet for VCD, featuring over 100,000 images and 14 million object-commonsense pairs. We furthermore pro
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#25361;&#25112;&#65292;&#29992;&#20110;&#27979;&#35797;&#31070;&#32463;&#27169;&#22411;&#30340;STEM&#25216;&#33021;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#21253;&#21547;&#22823;&#37327;&#22522;&#30784;&#25216;&#33021;&#21644;&#38382;&#39064;&#30340;&#25968;&#25454;&#38598;&#65292;&#38656;&#35201;&#29702;&#35299;STEM&#30340;&#22810;&#27169;&#24335;&#35270;&#35273;&#35821;&#35328;&#20449;&#24687;&#65292;&#24182;&#23637;&#31034;&#20102;&#26368;&#26032;&#27169;&#22411;&#23545;&#20110;&#20302;&#24180;&#32423;&#25216;&#33021;&#30340;&#26377;&#38480;&#25484;&#25569;&#12290;</title><link>https://arxiv.org/abs/2402.17205</link><description>&lt;p&gt;
&#27979;&#37327;&#31070;&#32463;&#27169;&#22411;&#30340;&#35270;&#35273;&#35821;&#35328;STEM&#25216;&#33021;
&lt;/p&gt;
&lt;p&gt;
Measuring Vision-Language STEM Skills of Neural Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17205
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#25361;&#25112;&#65292;&#29992;&#20110;&#27979;&#35797;&#31070;&#32463;&#27169;&#22411;&#30340;STEM&#25216;&#33021;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#21253;&#21547;&#22823;&#37327;&#22522;&#30784;&#25216;&#33021;&#21644;&#38382;&#39064;&#30340;&#25968;&#25454;&#38598;&#65292;&#38656;&#35201;&#29702;&#35299;STEM&#30340;&#22810;&#27169;&#24335;&#35270;&#35273;&#35821;&#35328;&#20449;&#24687;&#65292;&#24182;&#23637;&#31034;&#20102;&#26368;&#26032;&#27169;&#22411;&#23545;&#20110;&#20302;&#24180;&#32423;&#25216;&#33021;&#30340;&#26377;&#38480;&#25484;&#25569;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#25361;&#25112;&#65292;&#29992;&#20110;&#27979;&#35797;&#31070;&#32463;&#27169;&#22411;&#30340;STEM&#25216;&#33021;&#12290;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#38382;&#39064;&#36890;&#24120;&#38656;&#35201;&#32467;&#21512;STEM&#65288;&#31185;&#23398;&#12289;&#25216;&#26415;&#12289;&#24037;&#31243;&#21644;&#25968;&#23398;&#65289;&#30693;&#35782;&#26469;&#35299;&#20915;&#12290;&#19982;&#29616;&#26377;&#25968;&#25454;&#38598;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#38656;&#35201;&#29702;&#35299;STEM&#30340;&#22810;&#27169;&#24335;&#35270;&#35273;&#35821;&#35328;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#26159;&#25361;&#25112;&#24615;&#38382;&#39064;&#20013;&#26368;&#22823;&#12289;&#26368;&#20840;&#38754;&#30340;&#25968;&#25454;&#38598;&#20043;&#19968;&#12290;&#23427;&#21253;&#25324;448&#39033;&#25216;&#33021;&#21644;1,073,146&#20010;&#36328;&#36234;&#25152;&#26377;STEM&#31185;&#30446;&#30340;&#38382;&#39064;&#12290;&#19982;&#36890;&#24120;&#20391;&#37325;&#20110;&#26816;&#39564;&#19987;&#23478;&#27700;&#24179;&#33021;&#21147;&#30340;&#29616;&#26377;&#25968;&#25454;&#38598;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#21253;&#25324;&#22522;&#30784;&#25216;&#33021;&#21644;&#26681;&#25454;K-12&#35838;&#31243;&#35774;&#35745;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#36824;&#23558;&#26368;&#20808;&#36827;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#22914;CLIP&#21644;GPT-3.5-Turbo&#65292;&#28155;&#21152;&#21040;&#25105;&#20204;&#30340;&#22522;&#20934;&#20013;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#26368;&#36817;&#30340;&#27169;&#22411;&#36827;&#23637;&#21482;&#26377;&#21161;&#20110;&#25484;&#25569;&#25968;&#25454;&#38598;&#20013;&#38750;&#24120;&#26377;&#38480;&#25968;&#37327;&#30340;&#20302;&#24180;&#32423;&#25216;&#33021;&#65288;&#19977;&#24180;&#32423;&#20013;&#30340;2.5%&#65289;&#12290;&#20107;&#23454;&#19978;&#65292;&#36825;&#20123;&#27169;&#22411;&#20173;&#36828;&#27809;&#26377;&#23436;&#20840;&#25484;&#25569;&#23398;&#21069;&#25945;&#32946;&#38454;&#27573;&#30340;&#25216;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17205v1 Announce Type: cross  Abstract: We introduce a new challenge to test the STEM skills of neural models. The problems in the real world often require solutions, combining knowledge from STEM (science, technology, engineering, and math). Unlike existing datasets, our dataset requires the understanding of multimodal vision-language information of STEM. Our dataset features one of the largest and most comprehensive datasets for the challenge. It includes 448 skills and 1,073,146 questions spanning all STEM subjects. Compared to existing datasets that often focus on examining expert-level ability, our dataset includes fundamental skills and questions designed based on the K-12 curriculum. We also add state-of-the-art foundation models such as CLIP and GPT-3.5-Turbo to our benchmark. Results show that the recent model advances only help master a very limited number of lower grade-level skills (2.5% in the third grade) in our dataset. In fact, these models are still well bel
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#30340;&#24046;&#20998;&#38544;&#31169;&#20445;&#25252;&#31639;&#27861;&#65292;&#23454;&#29616;&#20010;&#20154;&#25968;&#25454;&#38544;&#31169;&#20445;&#25252;&#21644;&#26816;&#27979;&#12290;</title><link>https://arxiv.org/abs/2402.17191</link><description>&lt;p&gt;
AI&#39537;&#21160;&#30340;&#21311;&#21517;&#21270;&#65306;&#22312;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#30340;&#21516;&#26102;&#20445;&#25252;&#20010;&#20154;&#25968;&#25454;&#38544;&#31169;
&lt;/p&gt;
&lt;p&gt;
AI-Driven Anonymization: Protecting Personal Data Privacy While Leveraging Machine Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17191
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#30340;&#24046;&#20998;&#38544;&#31169;&#20445;&#25252;&#31639;&#27861;&#65292;&#23454;&#29616;&#20010;&#20154;&#25968;&#25454;&#38544;&#31169;&#20445;&#25252;&#21644;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#30340;&#21457;&#23637;&#26174;&#33879;&#25913;&#21464;&#20102;&#20154;&#20204;&#30340;&#29983;&#27963;&#12290;&#28982;&#32780;&#65292;&#23427;&#20063;&#23545;&#38544;&#31169;&#21644;&#23433;&#20840;&#26500;&#25104;&#20102;&#37325;&#22823;&#23041;&#32961;&#65292;&#35768;&#22810;&#20010;&#20154;&#20449;&#24687;&#34987;&#20844;&#24320;&#65292;&#24182;&#26377;&#29359;&#32618;&#25915;&#20987;&#21644;&#31363;&#21462;&#30340;&#25253;&#36947;&#12290;&#22240;&#27492;&#65292;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#23454;&#29616;&#20010;&#20154;&#20449;&#24687;&#30340;&#26234;&#33021;&#20445;&#25252;&#24050;&#32463;&#25104;&#20026;&#39318;&#35201;&#20851;&#27880;&#30340;&#38382;&#39064;&#12290;&#20154;&#24037;&#26234;&#33021;&#21033;&#29992;&#20808;&#36827;&#31639;&#27861;&#21644;&#25216;&#26415;&#26377;&#25928;&#21152;&#23494;&#21644;&#21311;&#21517;&#21270;&#20010;&#20154;&#25968;&#25454;&#65292;&#23454;&#29616;&#26377;&#20215;&#20540;&#30340;&#25968;&#25454;&#20998;&#26512;&#21644;&#21033;&#29992;&#21516;&#26102;&#32500;&#25252;&#38544;&#31169;&#12290;&#26412;&#25991;&#30528;&#30524;&#20110;&#20010;&#20154;&#25968;&#25454;&#38544;&#31169;&#20445;&#25252;&#21644;&#21311;&#21517;&#21270;&#25512;&#24191;&#20316;&#20026;&#20854;&#26680;&#24515;&#30740;&#31350;&#30446;&#26631;&#12290;&#23427;&#36890;&#36807;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#30340;&#24046;&#20998;&#38544;&#31169;&#20445;&#25252;&#31639;&#27861;&#23454;&#29616;&#20102;&#20010;&#20154;&#25968;&#25454;&#38544;&#31169;&#20445;&#25252;&#21644;&#26816;&#27979;&#12290;&#35813;&#35770;&#25991;&#36824;&#35299;&#20915;&#20102;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#20013;&#29616;&#26377;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17191v1 Announce Type: cross  Abstract: The development of artificial intelligence has significantly transformed people's lives. However, it has also posed a significant threat to privacy and security, with numerous instances of personal information being exposed online and reports of criminal attacks and theft. Consequently, the need to achieve intelligent protection of personal information through machine learning algorithms has become a paramount concern. Artificial intelligence leverages advanced algorithms and technologies to effectively encrypt and anonymize personal data, enabling valuable data analysis and utilization while safeguarding privacy. This paper focuses on personal data privacy protection and the promotion of anonymity as its core research objectives. It achieves personal data privacy protection and detection through the use of machine learning's differential privacy protection algorithm. The paper also addresses existing challenges in machine learning rel
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#35299;&#32544;&#25439;&#22833;&#24182;&#25913;&#36827;&#22768;&#23398;&#32534;&#30721;&#22120;&#65292;&#26412;&#25991;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#22788;&#29702;&#20195;&#30721;&#20999;&#25442;&#29616;&#35937;&#26102;&#34920;&#29616;&#20986;&#33394;&#65292;&#26174;&#33879;&#20248;&#20110;&#20808;&#21069;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.17189</link><description>&lt;p&gt;
&#19968;&#31181;&#26377;&#25928;&#30340;&#28151;&#21512;&#19987;&#23478;&#26041;&#27861;&#65292;&#21033;&#29992;&#32534;&#30721;&#22120;&#35299;&#32544;&#26469;&#36827;&#34892;&#20195;&#30721;&#20999;&#25442;&#35821;&#38899;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
An Effective Mixture-Of-Experts Approach For Code-Switching Speech Recognition Leveraging Encoder Disentanglement
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17189
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#35299;&#32544;&#25439;&#22833;&#24182;&#25913;&#36827;&#22768;&#23398;&#32534;&#30721;&#22120;&#65292;&#26412;&#25991;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#22788;&#29702;&#20195;&#30721;&#20999;&#25442;&#29616;&#35937;&#26102;&#34920;&#29616;&#20986;&#33394;&#65292;&#26174;&#33879;&#20248;&#20110;&#20808;&#21069;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#31471;&#21040;&#31471;&#65288;E2E&#65289;&#31070;&#32463;&#32593;&#32476;&#30340;&#22823;&#35268;&#27169;&#21457;&#23637;&#65292;&#36817;&#24180;&#26469;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#21462;&#24471;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#31361;&#30772;&#12290;&#28982;&#32780;&#65292;&#20195;&#30721;&#20999;&#25442;&#29616;&#35937;&#20173;&#28982;&#26159;&#38459;&#30861;ASR&#23436;&#32654;&#30340;&#20027;&#35201;&#38556;&#30861;&#65292;&#22240;&#20026;&#32570;&#20047;&#26631;&#35760;&#25968;&#25454;&#21644;&#35821;&#35328;&#20043;&#38388;&#30340;&#21464;&#21270;&#32463;&#24120;&#23548;&#33268;ASR&#24615;&#33021;&#19979;&#38477;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#25913;&#36827;E2E ASR&#30340;&#22768;&#23398;&#32534;&#30721;&#22120;&#65292;&#20197;&#24212;&#23545;&#20195;&#30721;&#20999;&#25442;&#29616;&#35937;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26377;&#19977;&#20010;&#65306;&#39318;&#20808;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35299;&#32544;&#25439;&#22833;&#65292;&#20351;&#24471;&#32534;&#30721;&#22120;&#30340;&#36739;&#20302;&#23618;&#33021;&#22815;&#25429;&#33719;&#36328;&#35821;&#35328;&#30340;&#22768;&#23398;&#20449;&#24687;&#65292;&#21516;&#26102;&#22312;&#32534;&#30721;&#22120;&#30340;&#36739;&#39640;&#23618;&#20943;&#36731;&#35821;&#35328;&#28151;&#28102;&#12290;&#20854;&#27425;&#65292;&#36890;&#36807;&#20840;&#38754;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#20351;&#29992;&#39044;&#35757;&#32451;&#21452;&#32534;&#30721;&#22120;&#30340;&#29616;&#26377;&#26041;&#27861;&#65292;&#21516;&#26102;&#21482;&#35775;&#38382;&#20195;&#30721;&#20999;&#25442;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17189v1 Announce Type: cross  Abstract: With the massive developments of end-to-end (E2E) neural networks, recent years have witnessed unprecedented breakthroughs in automatic speech recognition (ASR). However, the codeswitching phenomenon remains a major obstacle that hinders ASR from perfection, as the lack of labeled data and the variations between languages often lead to degradation of ASR performance. In this paper, we focus exclusively on improving the acoustic encoder of E2E ASR to tackle the challenge caused by the codeswitching phenomenon. Our main contributions are threefold: First, we introduce a novel disentanglement loss to enable the lower-layer of the encoder to capture inter-lingual acoustic information while mitigating linguistic confusion at the higher-layer of the encoder. Second, through comprehensive experiments, we verify that our proposed method outperforms the prior-art methods using pretrained dual-encoders, meanwhile having access only to the codesw
&lt;/p&gt;</description></item><item><title>Sora&#26159;&#19968;&#31181;&#25991;&#26412;&#21040;&#35270;&#39057;&#29983;&#25104;&#30340;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#65292;&#23637;&#31034;&#20986;&#22312;&#27169;&#25311;&#29289;&#29702;&#19990;&#30028;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#21069;&#26223;&#21644;&#25361;&#25112;&#65292;&#26410;&#26469;&#21457;&#23637;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>https://arxiv.org/abs/2402.17177</link><description>&lt;p&gt;
Sora: &#22823;&#22411;&#35270;&#35273;&#27169;&#22411;&#32972;&#26223;&#12289;&#25216;&#26415;&#12289;&#23616;&#38480;&#24615;&#21644;&#26426;&#36935;&#30340;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Sora: A Review on Background, Technology, Limitations, and Opportunities of Large Vision Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17177
&lt;/p&gt;
&lt;p&gt;
Sora&#26159;&#19968;&#31181;&#25991;&#26412;&#21040;&#35270;&#39057;&#29983;&#25104;&#30340;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#65292;&#23637;&#31034;&#20986;&#22312;&#27169;&#25311;&#29289;&#29702;&#19990;&#30028;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#21069;&#26223;&#21644;&#25361;&#25112;&#65292;&#26410;&#26469;&#21457;&#23637;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Sora&#26159;&#30001;OpenAI&#20110;2024&#24180;2&#26376;&#21457;&#24067;&#30340;&#19968;&#31181;&#25991;&#26412;&#21040;&#35270;&#39057;&#29983;&#25104;&#30340;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#12290;&#36825;&#20010;&#27169;&#22411;&#32463;&#36807;&#35757;&#32451;&#65292;&#21487;&#20197;&#26681;&#25454;&#25991;&#26412;&#25351;&#20196;&#29983;&#25104;&#36924;&#30495;&#25110;&#24819;&#35937;&#30340;&#22330;&#26223;&#35270;&#39057;&#65292;&#24182;&#22312;&#27169;&#25311;&#29289;&#29702;&#19990;&#30028;&#26041;&#38754;&#26174;&#31034;&#20986;&#28508;&#21147;&#12290;&#26412;&#25991;&#22522;&#20110;&#20844;&#24320;&#30340;&#25216;&#26415;&#25253;&#21578;&#21644;&#36870;&#21521;&#24037;&#31243;&#65292;&#23545;&#36825;&#20010;&#27169;&#22411;&#30340;&#32972;&#26223;&#12289;&#30456;&#20851;&#25216;&#26415;&#12289;&#24212;&#29992;&#12289;&#23578;&#23384;&#30340;&#25361;&#25112;&#20197;&#21450;&#25991;&#26412;&#21040;&#35270;&#39057;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#30340;&#26410;&#26469;&#26041;&#21521;&#36827;&#34892;&#20102;&#20840;&#38754;&#22238;&#39038;&#12290;&#39318;&#20808;&#25105;&#20204;&#36861;&#28335;&#20102;Sora&#30340;&#21457;&#23637;&#21382;&#31243;&#65292;&#24182;&#35843;&#26597;&#20102;&#29992;&#20110;&#26500;&#24314;&#36825;&#20010;"&#19990;&#30028;&#27169;&#25311;&#22120;"&#30340;&#22522;&#30784;&#25216;&#26415;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35814;&#32454;&#25551;&#36848;&#20102;Sora&#22312;&#20174;&#30005;&#24433;&#21046;&#20316;&#21644;&#25945;&#32946;&#21040;&#33829;&#38144;&#31561;&#22810;&#20010;&#34892;&#19994;&#20013;&#30340;&#24212;&#29992;&#21644;&#28508;&#22312;&#24433;&#21709;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#38656;&#35201;&#35299;&#20915;&#30340;&#20027;&#35201;&#25361;&#25112;&#21644;&#23616;&#38480;&#24615;&#65292;&#20197;&#20415;&#24191;&#27867;&#37096;&#32626;Sora&#65292;&#22914;&#30830;&#20445;&#23433;&#20840;&#21644;&#26080;&#20559;&#35265;&#30340;&#35270;&#39057;&#29983;&#25104;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;Sora&#20197;&#21450;&#35270;&#39057;&#29983;&#25104;&#25216;&#26415;&#26410;&#26469;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17177v1 Announce Type: cross  Abstract: Sora is a text-to-video generative AI model, released by OpenAI in February 2024. The model is trained to generate videos of realistic or imaginative scenes from text instructions and show potential in simulating the physical world. Based on public technical reports and reverse engineering, this paper presents a comprehensive review of the model's background, related technologies, applications, remaining challenges, and future directions of text-to-video AI models. We first trace Sora's development and investigate the underlying technologies used to build this "world simulator". Then, we describe in detail the applications and potential impact of Sora in multiple industries ranging from film-making and education to marketing. We discuss the main challenges and limitations that need to be addressed to widely deploy Sora, such as ensuring safe and unbiased video generation. Lastly, we discuss the future development of Sora and video gene
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;DSEval&#35780;&#20272;&#33539;&#24335;&#21644;&#19968;&#31995;&#21015;&#21019;&#26032;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#25968;&#25454;&#31185;&#23398;&#20195;&#29702;&#22312;&#25972;&#20010;&#25968;&#25454;&#31185;&#23398;&#29983;&#21629;&#21608;&#26399;&#20013;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#24341;&#20837;&#33258;&#20030;&#27880;&#37322;&#26041;&#27861;&#31616;&#21270;&#25968;&#25454;&#38598;&#20934;&#22791;&#27969;&#31243;&#65292;&#25913;&#36827;&#35780;&#20272;&#35206;&#30422;&#33539;&#22260;&#65292;&#25193;&#23637;&#22522;&#20934;&#27979;&#35797;&#30340;&#20840;&#38754;&#24615;&#65292;&#25581;&#31034;&#20102;&#26222;&#36941;&#23384;&#22312;&#30340;&#38556;&#30861;&#24182;&#25552;&#20379;&#20102;&#20851;&#38190;&#35265;&#35299;</title><link>https://arxiv.org/abs/2402.17168</link><description>&lt;p&gt;
&#25968;&#25454;&#31185;&#23398;&#20195;&#29702;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Benchmarking Data Science Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17168
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;DSEval&#35780;&#20272;&#33539;&#24335;&#21644;&#19968;&#31995;&#21015;&#21019;&#26032;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#25968;&#25454;&#31185;&#23398;&#20195;&#29702;&#22312;&#25972;&#20010;&#25968;&#25454;&#31185;&#23398;&#29983;&#21629;&#21608;&#26399;&#20013;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#24341;&#20837;&#33258;&#20030;&#27880;&#37322;&#26041;&#27861;&#31616;&#21270;&#25968;&#25454;&#38598;&#20934;&#22791;&#27969;&#31243;&#65292;&#25913;&#36827;&#35780;&#20272;&#35206;&#30422;&#33539;&#22260;&#65292;&#25193;&#23637;&#22522;&#20934;&#27979;&#35797;&#30340;&#20840;&#38754;&#24615;&#65292;&#25581;&#31034;&#20102;&#26222;&#36941;&#23384;&#22312;&#30340;&#38556;&#30861;&#24182;&#25552;&#20379;&#20102;&#20851;&#38190;&#35265;&#35299;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25968;&#25454;&#39537;&#21160;&#20915;&#31574;&#30340;&#26102;&#20195;&#65292;&#25968;&#25454;&#20998;&#26512;&#30340;&#22797;&#26434;&#24615;&#38656;&#35201;&#25968;&#25454;&#31185;&#23398;&#30340;&#39640;&#32423;&#19987;&#19994;&#30693;&#35782;&#21644;&#24037;&#20855;&#65292;&#36825;&#23545;&#19987;&#23478;&#26469;&#35828;&#20063;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#20316;&#20026;&#25968;&#25454;&#31185;&#23398;&#20195;&#29702;&#65292;&#24050;&#32463;&#25104;&#20026;&#21327;&#21161;&#20154;&#31867;&#36827;&#34892;&#25968;&#25454;&#20998;&#26512;&#21644;&#22788;&#29702;&#30340;&#26377;&#24076;&#26395;&#30340;&#36741;&#21161;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#23454;&#38469;&#26377;&#25928;&#24615;&#20173;&#21463;&#38480;&#20110;&#29616;&#23454;&#24212;&#29992;&#30340;&#22810;&#26679;&#38656;&#27714;&#21644;&#22797;&#26434;&#30340;&#20998;&#26512;&#36807;&#31243;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;DSEval--&#19968;&#31181;&#26032;&#39062;&#30340;&#35780;&#20272;&#33539;&#24335;&#65292;&#20197;&#21450;&#19968;&#31995;&#21015;&#38024;&#23545;&#25972;&#20010;&#25968;&#25454;&#31185;&#23398;&#29983;&#21629;&#21608;&#26399;&#30340;&#20195;&#29702;&#24615;&#33021;&#35780;&#20272;&#30340;&#21019;&#26032;&#22522;&#20934;&#12290;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#20030;&#27880;&#37322;&#26041;&#27861;&#65292;&#25105;&#20204;&#31616;&#21270;&#20102;&#25968;&#25454;&#38598;&#20934;&#22791;&#27969;&#31243;&#65292;&#25913;&#36827;&#20102;&#35780;&#20272;&#35206;&#30422;&#33539;&#22260;&#65292;&#24182;&#25193;&#23637;&#20102;&#22522;&#20934;&#27979;&#35797;&#30340;&#20840;&#38754;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#25581;&#31034;&#20102;&#26222;&#36941;&#23384;&#22312;&#30340;&#38556;&#30861;&#65292;&#24182;&#25552;&#20379;&#20102;&#20851;&#38190;&#35265;&#35299;&#65292;&#20197;&#25351;&#23548;&#26410;&#26469;&#22312;&#36825;&#19968;&#39046;&#22495;&#30340;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17168v1 Announce Type: new  Abstract: In the era of data-driven decision-making, the complexity of data analysis necessitates advanced expertise and tools of data science, presenting significant challenges even for specialists. Large Language Models (LLMs) have emerged as promising aids as data science agents, assisting humans in data analysis and processing. Yet their practical efficacy remains constrained by the varied demands of real-world applications and complicated analytical process. In this paper, we introduce DSEval -- a novel evaluation paradigm, as well as a series of innovative benchmarks tailored for assessing the performance of these agents throughout the entire data science lifecycle. Incorporating a novel bootstrapped annotation method, we streamline dataset preparation, improve the evaluation coverage, and expand benchmarking comprehensiveness. Our findings uncover prevalent obstacles and provide critical insights to inform future advancements in the field.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#20195;&#29702;&#21327;&#20316;&#26694;&#26550;&#65292;&#29992;&#20110;&#21442;&#19982;&#24335;&#22478;&#24066;&#35268;&#21010;&#65292;&#21487;&#20197;&#29983;&#25104;&#32771;&#34385;&#23621;&#27665;&#22810;&#26679;&#21270;&#38656;&#27714;&#30340;&#22478;&#24066;&#22303;&#22320;&#21033;&#29992;&#35268;&#21010;&#12290;</title><link>https://arxiv.org/abs/2402.17161</link><description>&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#21442;&#19982;&#24335;&#22478;&#24066;&#35268;&#21010;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Large Language Model for Participatory Urban Planning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17161
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#20195;&#29702;&#21327;&#20316;&#26694;&#26550;&#65292;&#29992;&#20110;&#21442;&#19982;&#24335;&#22478;&#24066;&#35268;&#21010;&#65292;&#21487;&#20197;&#29983;&#25104;&#32771;&#34385;&#23621;&#27665;&#22810;&#26679;&#21270;&#38656;&#27714;&#30340;&#22478;&#24066;&#22303;&#22320;&#21033;&#29992;&#35268;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21442;&#19982;&#24335;&#22478;&#24066;&#35268;&#21010;&#26159;&#29616;&#20195;&#22478;&#24066;&#35268;&#21010;&#30340;&#20027;&#27969;&#65292;&#28041;&#21450;&#23621;&#27665;&#30340;&#31215;&#26497;&#21442;&#19982;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#21442;&#19982;&#24335;&#33539;&#24335;&#38656;&#35201;&#32463;&#39564;&#20016;&#23500;&#30340;&#35268;&#21010;&#19987;&#23478;&#65292;&#36890;&#24120;&#32791;&#26102;&#19988;&#26114;&#36149;&#12290;&#24184;&#36816;&#30340;&#26159;&#65292;&#26032;&#20852;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;(LLMs)&#24050;&#32463;&#26174;&#31034;&#20986;&#30456;&#24403;&#22823;&#30340;&#33021;&#21147;&#26469;&#27169;&#25311;&#31867;&#20154;&#20195;&#29702;&#65292;&#21487;&#20197;&#29992;&#20110;&#36731;&#26494;&#27169;&#25311;&#21442;&#19982;&#24615;&#36807;&#31243;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#22522;&#20110;LLM&#30340;&#22810;&#20195;&#29702;&#21327;&#20316;&#26694;&#26550;&#65292;&#29992;&#20110;&#21442;&#19982;&#24335;&#22478;&#24066;&#35268;&#21010;&#65292;&#21487;&#20197;&#32771;&#34385;&#23621;&#27665;&#30340;&#22810;&#26679;&#21270;&#38656;&#27714;&#29983;&#25104;&#22478;&#24066;&#22320;&#21306;&#30340;&#22303;&#22320;&#21033;&#29992;&#35268;&#21010;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;LLM&#20195;&#29702;&#26469;&#27169;&#25311;&#35268;&#21010;&#32773;&#21644;&#25104;&#21315;&#19978;&#19975;&#20855;&#26377;&#19981;&#21516;&#32972;&#26223;&#21644;&#29305;&#28857;&#30340;&#23621;&#27665;&#12290;&#25105;&#20204;&#39318;&#20808;&#35201;&#27714;&#35268;&#21010;&#32773;&#21046;&#23450;&#19968;&#39033;&#21021;&#27493;&#22303;&#22320;&#21033;&#29992;&#35268;&#21010;&#12290;&#20026;&#20102;&#35299;&#20915;&#23621;&#27665;&#23545;&#19981;&#21516;&#35774;&#26045;&#38656;&#27714;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#22312;&#27599;&#20010;&#31038;&#21306;&#20013;&#35753;&#23621;&#27665;&#23637;&#24320;&#35752;&#35770;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17161v1 Announce Type: new  Abstract: Participatory urban planning is the mainstream of modern urban planning that involves the active engagement of residents. However, the traditional participatory paradigm requires experienced planning experts and is often time-consuming and costly. Fortunately, the emerging Large Language Models (LLMs) have shown considerable ability to simulate human-like agents, which can be used to emulate the participatory process easily. In this work, we introduce an LLM-based multi-agent collaboration framework for participatory urban planning, which can generate land-use plans for urban regions considering the diverse needs of residents. Specifically, we construct LLM agents to simulate a planner and thousands of residents with diverse profiles and backgrounds. We first ask the planner to carry out an initial land-use plan. To deal with the different facilities needs of residents, we initiate a discussion among the residents in each community about
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TaxDiff&#30340;&#20998;&#31867;&#24341;&#23548;&#25193;&#25955;&#27169;&#22411;&#65292;&#32467;&#21512;&#20102;&#29983;&#29289;&#29289;&#31181;&#20449;&#24687;&#21644;&#25193;&#25955;&#27169;&#22411;&#30340;&#29983;&#25104;&#33021;&#21147;&#65292;&#29992;&#20110;&#21487;&#25511;&#29983;&#25104;&#32467;&#26500;&#31283;&#23450;&#30340;&#34507;&#30333;&#36136;&#24207;&#21015;&#12290;</title><link>https://arxiv.org/abs/2402.17156</link><description>&lt;p&gt;
TaxDiff&#65306;&#29992;&#20110;&#34507;&#30333;&#36136;&#24207;&#21015;&#29983;&#25104;&#30340;&#20998;&#31867;&#24341;&#23548;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
TaxDiff: Taxonomic-Guided Diffusion Model for Protein Sequence Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17156
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TaxDiff&#30340;&#20998;&#31867;&#24341;&#23548;&#25193;&#25955;&#27169;&#22411;&#65292;&#32467;&#21512;&#20102;&#29983;&#29289;&#29289;&#31181;&#20449;&#24687;&#21644;&#25193;&#25955;&#27169;&#22411;&#30340;&#29983;&#25104;&#33021;&#21147;&#65292;&#29992;&#20110;&#21487;&#25511;&#29983;&#25104;&#32467;&#26500;&#31283;&#23450;&#30340;&#34507;&#30333;&#36136;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#20855;&#26377;&#29305;&#23450;&#29983;&#29289;&#21151;&#33021;&#21644;&#32467;&#26500;&#31283;&#23450;&#24615;&#30340;&#34507;&#30333;&#36136;&#24207;&#21015;&#22312;&#29983;&#29289;&#23398;&#21644;&#21270;&#23398;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;&#29983;&#25104;&#27169;&#22411;&#24050;&#32463;&#23637;&#31034;&#20102;&#23427;&#20204;&#22312;&#21487;&#38752;&#34507;&#30333;&#36136;&#35774;&#35745;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#27169;&#22411;&#20165;&#38480;&#20110;&#26080;&#26465;&#20214;&#29983;&#25104;&#34507;&#30333;&#36136;&#24207;&#21015;&#65292;&#32570;&#20047;&#23545;&#29983;&#29289;&#20219;&#21153;&#33267;&#20851;&#37325;&#35201;&#30340;&#21487;&#25511;&#29983;&#25104;&#33021;&#21147;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TaxDiff&#65292;&#19968;&#31181;&#29992;&#20110;&#21487;&#25511;&#34507;&#30333;&#36136;&#24207;&#21015;&#29983;&#25104;&#30340;&#20998;&#31867;&#24341;&#23548;&#25193;&#25955;&#27169;&#22411;&#65292;&#23427;&#23558;&#29983;&#29289;&#29289;&#31181;&#20449;&#24687;&#19982;&#25193;&#25955;&#27169;&#22411;&#30340;&#29983;&#25104;&#33021;&#21147;&#30456;&#32467;&#21512;&#65292;&#20197;&#22312;&#24207;&#21015;&#31354;&#38388;&#20869;&#29983;&#25104;&#32467;&#26500;&#31283;&#23450;&#30340;&#34507;&#30333;&#36136;&#12290;&#20855;&#20307;&#22320;&#65292;&#20998;&#31867;&#25511;&#21046;&#20449;&#24687;&#34987;&#25554;&#20837;&#21040;&#21464;&#21387;&#22120;&#22359;&#30340;&#27599;&#19968;&#23618;&#65292;&#20197;&#23454;&#29616;&#32454;&#31890;&#24230;&#25511;&#21046;&#12290;&#20840;&#23616;&#21644;&#23616;&#37096;&#20851;&#27880;&#30340;&#32467;&#21512;&#30830;&#20445;&#20102;&#20998;&#31867;&#29305;&#23450;&#34507;&#30333;&#36136;&#30340;&#24207;&#21015;&#19968;&#33268;&#24615;&#21644;&#32467;&#26500;&#21487;&#25240;&#21472;&#24615;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17156v1 Announce Type: cross  Abstract: Designing protein sequences with specific biological functions and structural stability is crucial in biology and chemistry. Generative models already demonstrated their capabilities for reliable protein design. However, previous models are limited to the unconditional generation of protein sequences and lack the controllable generation ability that is vital to biological tasks. In this work, we propose TaxDiff, a taxonomic-guided diffusion model for controllable protein sequence generation that combines biological species information with the generative capabilities of diffusion models to generate structurally stable proteins within the sequence space. Specifically, taxonomic control information is inserted into each layer of the transformer block to achieve fine-grained control. The combination of global and local attention ensures the sequence consistency and structural foldability of taxonomic-specific proteins. Extensive experimen
&lt;/p&gt;</description></item><item><title>Metasql&#26159;&#19968;&#31181;&#32479;&#19968;&#30340;&#29983;&#25104;-&#25490;&#24207;&#26694;&#26550;&#65292;&#21033;&#29992;&#26597;&#35810;&#20803;&#25968;&#25454;&#21644;&#23398;&#20064;-&#25490;&#24207;&#31639;&#27861;&#19981;&#26029;&#25552;&#39640;&#33258;&#28982;&#35821;&#35328;&#21040;SQL&#32763;&#35793;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.17144</link><description>&lt;p&gt;
Metasql&#65306;&#19968;&#31181;&#29992;&#20110;&#33258;&#28982;&#35821;&#35328;&#21040;SQL&#32763;&#35793;&#30340;&#29983;&#25104;-&#25490;&#24207;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Metasql: A Generate-then-Rank Framework for Natural Language to SQL Translation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17144
&lt;/p&gt;
&lt;p&gt;
Metasql&#26159;&#19968;&#31181;&#32479;&#19968;&#30340;&#29983;&#25104;-&#25490;&#24207;&#26694;&#26550;&#65292;&#21033;&#29992;&#26597;&#35810;&#20803;&#25968;&#25454;&#21644;&#23398;&#20064;-&#25490;&#24207;&#31639;&#27861;&#19981;&#26029;&#25552;&#39640;&#33258;&#28982;&#35821;&#35328;&#21040;SQL&#32763;&#35793;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17144v1 &#20844;&#21578;&#31867;&#22411;:&#36328;&#23398;&#31185; &#25688;&#35201;: &#25968;&#25454;&#24211;&#33258;&#28982;&#35821;&#35328;&#25509;&#21475;&#65288;NLIDB&#65289;&#36890;&#36807;&#30452;&#35266;&#30340;&#33258;&#28982;&#35821;&#35328;&#65288;NL&#65289;&#20132;&#20114;&#20351;&#38750;&#25216;&#26415;&#29992;&#25143;&#33021;&#22815;&#35775;&#38382;&#25968;&#25454;&#24211;&#12290;&#20808;&#36827;&#30340;&#26041;&#27861;&#36890;&#24120;&#21033;&#29992;&#31070;&#32463;&#24207;&#21015;&#21040;&#24207;&#21015;&#27169;&#22411;&#25110;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#24120;&#20351;&#29992;&#33258;&#22238;&#24402;&#35299;&#30721;&#26469;&#36880;&#20010;&#29983;&#25104;&#29420;&#29305;&#30340;SQL&#26597;&#35810;&#12290;&#34429;&#28982;&#36825;&#20123;&#32763;&#35793;&#27169;&#22411;&#22823;&#22823;&#25552;&#39640;&#20102;&#25972;&#20307;&#32763;&#35793;&#20934;&#30830;&#24615;&#65292;&#22312;NLIDB&#22522;&#20934;&#27979;&#35797;&#20013;&#36229;&#36807;&#20102;70&#65285;&#65292;&#20294;&#20351;&#29992;&#33258;&#22238;&#24402;&#35299;&#30721;&#29983;&#25104;&#21333;&#20010;SQL&#26597;&#35810;&#21487;&#33021;&#23548;&#33268;&#27425;&#20248;&#36755;&#20986;&#65292;&#28508;&#22312;&#23548;&#33268;&#38169;&#35823;&#32763;&#35793;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;Metasql&#65292;&#19968;&#20010;&#32479;&#19968;&#30340;&#29983;&#25104;-&#25490;&#24207;&#26694;&#26550;&#65292;&#21487;&#20197;&#28789;&#27963;&#19982;&#29616;&#26377;NLIDB&#38598;&#25104;&#65292;&#20174;&#32780;&#19981;&#26029;&#25552;&#39640;&#20854;&#32763;&#35793;&#20934;&#30830;&#24615;&#12290;Metasql&#24341;&#20837;&#20102;&#26597;&#35810;&#20803;&#25968;&#25454;&#26469;&#25511;&#21046;&#29983;&#25104;&#26356;&#22909;&#30340;SQL&#26597;&#35810;&#20505;&#36873;&#39033;&#65292;&#24182;&#20351;&#29992;&#23398;&#20064;-&#25490;&#24207;&#31639;&#27861;&#26816;&#32034;&#20840;&#23616;&#26368;&#20248;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17144v1 Announce Type: cross  Abstract: The Natural Language Interface to Databases (NLIDB) empowers non-technical users with database access through intuitive natural language (NL) interactions. Advanced approaches, utilizing neural sequence-to-sequence models or large-scale language models, typically employ auto-regressive decoding to generate unique SQL queries sequentially. While these translation models have greatly improved the overall translation accuracy, surpassing 70% on NLIDB benchmarks, the use of auto-regressive decoding to generate single SQL queries may result in sub-optimal outputs, potentially leading to erroneous translations. In this paper, we propose Metasql, a unified generate-then-rank framework that can be flexibly incorporated with existing NLIDBs to consistently improve their translation accuracy. Metasql introduces query metadata to control the generation of better SQL query candidates and uses learning-to-rank algorithms to retrieve globally optimi
&lt;/p&gt;</description></item><item><title>&#35270;&#39057;&#29983;&#25104;&#22312;&#35299;&#20915;&#30495;&#23454;&#19990;&#30028;&#20219;&#21153;&#20013;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#65292;&#31867;&#20284;&#20110;&#35821;&#35328;&#27169;&#22411;&#65292;&#23427;&#21487;&#20197;&#20316;&#20026;&#35268;&#21010;&#32773;&#12289;&#20195;&#29702;&#12289;&#35745;&#31639;&#24341;&#25806;&#21644;&#29615;&#22659;&#27169;&#25311;&#22120;&#12290;</title><link>https://arxiv.org/abs/2402.17139</link><description>&lt;p&gt;
&#35270;&#39057;&#20316;&#20026;&#30495;&#23454;&#19990;&#30028;&#20915;&#31574;&#30340;&#26032;&#35821;&#35328;
&lt;/p&gt;
&lt;p&gt;
Video as the New Language for Real-World Decision Making
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17139
&lt;/p&gt;
&lt;p&gt;
&#35270;&#39057;&#29983;&#25104;&#22312;&#35299;&#20915;&#30495;&#23454;&#19990;&#30028;&#20219;&#21153;&#20013;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#65292;&#31867;&#20284;&#20110;&#35821;&#35328;&#27169;&#22411;&#65292;&#23427;&#21487;&#20197;&#20316;&#20026;&#35268;&#21010;&#32773;&#12289;&#20195;&#29702;&#12289;&#35745;&#31639;&#24341;&#25806;&#21644;&#29615;&#22659;&#27169;&#25311;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20114;&#32852;&#32593;&#19978;&#23384;&#22312;&#22823;&#37327;&#25991;&#26412;&#21644;&#35270;&#39057;&#25968;&#25454;&#65292;&#36890;&#36807;&#19979;&#19968;&#20010;&#20196;&#29260;&#25110;&#24103;&#39044;&#27979;&#25903;&#25345;&#22823;&#35268;&#27169;&#33258;&#30417;&#30563;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#24182;&#27809;&#26377;&#34987;&#20805;&#20998;&#21033;&#29992;&#65306;&#35821;&#35328;&#27169;&#22411;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#20135;&#29983;&#20102;&#26174;&#33879;&#24433;&#21709;&#65292;&#32780;&#35270;&#39057;&#29983;&#25104;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20165;&#38480;&#20110;&#23186;&#20307;&#23089;&#20048;&#12290;&#28982;&#32780;&#65292;&#35270;&#39057;&#25968;&#25454;&#25429;&#25417;&#20102;&#20851;&#20110;&#29289;&#29702;&#19990;&#30028;&#30340;&#37325;&#35201;&#20449;&#24687;&#65292;&#36825;&#20123;&#20449;&#24687;&#24456;&#38590;&#29992;&#35821;&#35328;&#34920;&#36798;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#23558;&#35270;&#39057;&#29983;&#25104;&#25193;&#23637;&#21040;&#35299;&#20915;&#30495;&#23454;&#19990;&#30028;&#20219;&#21153;&#30340;&#19968;&#20010;&#34987;&#20302;&#20272;&#30340;&#26426;&#20250;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#31867;&#20284;&#20110;&#35821;&#35328;&#65292;&#35270;&#39057;&#21487;&#20197;&#20316;&#20026;&#19968;&#20010;&#32479;&#19968;&#30340;&#25509;&#21475;&#65292;&#21487;&#20197;&#21560;&#25910;&#20114;&#32852;&#32593;&#30693;&#35782;&#24182;&#34920;&#31034;&#22810;&#26679;&#21270;&#30340;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#21508;&#31181;&#25216;&#26415;&#65288;&#22914;&#19978;&#19979;&#25991;&#23398;&#20064;&#12289;&#35268;&#21010;&#21644;&#24378;&#21270;&#23398;&#20064;&#65289;&#23558;&#35270;&#39057;&#29983;&#25104;&#29992;&#20316;&#35268;&#21010;&#32773;&#12289;&#20195;&#29702;&#12289;&#35745;&#31639;&#24341;&#25806;&#21644;&#29615;&#22659;&#27169;&#25311;&#22120;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#19968;&#20123;&#37325;&#35201;&#30340;&#21019;&#26032;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17139v1 Announce Type: cross  Abstract: Both text and video data are abundant on the internet and support large-scale self-supervised learning through next token or frame prediction. However, they have not been equally leveraged: language models have had significant real-world impact, whereas video generation has remained largely limited to media entertainment. Yet video data captures important information about the physical world that is difficult to express in language. To address this gap, we discuss an under-appreciated opportunity to extend video generation to solve tasks in the real world. We observe how, akin to language, video can serve as a unified interface that can absorb internet knowledge and represent diverse tasks. Moreover, we demonstrate how, like language models, video generation can serve as planners, agents, compute engines, and environment simulators through techniques such as in-context learning, planning and reinforcement learning. We identify major im
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21151;&#33021;&#22870;&#21169;&#32534;&#30721;&#23454;&#29616;&#30340;&#26080;&#30417;&#30563;&#38646;&#26679;&#26412;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#33021;&#22815;&#22312;&#21508;&#31181;&#27169;&#25311;&#26426;&#22120;&#20154;&#22522;&#20934;&#27979;&#35797;&#20013;&#35757;&#32451;&#20195;&#29702;&#24182;&#25104;&#21151;&#35299;&#20915;&#26032;&#20219;&#21153;&#65292;&#30456;&#27604;&#20197;&#24448;&#30340;&#38646;&#26679;&#26412;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#34920;&#29616;&#26356;&#20248;&#31168;&#12290;</title><link>https://arxiv.org/abs/2402.17135</link><description>&lt;p&gt;
&#36890;&#36807;&#21151;&#33021;&#22870;&#21169;&#32534;&#30721;&#23454;&#29616;&#30340;&#26080;&#30417;&#30563;&#38646;&#26679;&#26412;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Zero-Shot Reinforcement Learning via Functional Reward Encodings
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17135
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21151;&#33021;&#22870;&#21169;&#32534;&#30721;&#23454;&#29616;&#30340;&#26080;&#30417;&#30563;&#38646;&#26679;&#26412;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#33021;&#22815;&#22312;&#21508;&#31181;&#27169;&#25311;&#26426;&#22120;&#20154;&#22522;&#20934;&#27979;&#35797;&#20013;&#35757;&#32451;&#20195;&#29702;&#24182;&#25104;&#21151;&#35299;&#20915;&#26032;&#20219;&#21153;&#65292;&#30456;&#27604;&#20197;&#24448;&#30340;&#38646;&#26679;&#26412;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#34920;&#29616;&#26356;&#20248;&#31168;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#21151;&#33021;&#22870;&#21169;&#32534;&#30721;&#65288;FRE&#65289;&#30340;&#36890;&#29992;&#12289;&#21487;&#25193;&#23637;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#38646;&#26679;&#26412;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#24819;&#27861;&#26159;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;transformer&#30340;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#23545;&#20219;&#24847;&#20219;&#21153;&#30340;&#29366;&#24577;-&#22870;&#21169;&#26679;&#26412;&#36827;&#34892;&#32534;&#30721;&#65292;&#20174;&#32780;&#23398;&#20064;&#20219;&#24847;&#20219;&#21153;&#30340;&#21151;&#33021;&#34920;&#31034;&#12290;&#36825;&#31181;&#21151;&#33021;&#32534;&#30721;&#19981;&#20165;&#20351;&#24471;&#33021;&#22815;&#20174;&#21508;&#31181;&#36890;&#29992;&#26080;&#30417;&#30563;&#22870;&#21169;&#20989;&#25968;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#32780;&#19988;&#36824;&#25552;&#20379;&#20102;&#19968;&#31181;&#22312;&#38646;&#26679;&#26412;&#24773;&#20917;&#19979;&#35299;&#20915;&#20219;&#20309;&#26032;&#30340;&#19979;&#28216;&#20219;&#21153;&#30340;&#26041;&#27861;&#65292;&#21482;&#38656;&#23569;&#37327;&#22870;&#21169;&#27880;&#37322;&#26679;&#26412;&#12290;&#25105;&#20204;&#22312;&#23454;&#39564;&#20013;&#26174;&#31034;&#65292;&#38024;&#23545;&#22810;&#26679;&#30340;&#38543;&#26426;&#26080;&#30417;&#30563;&#22870;&#21169;&#20989;&#25968;&#36827;&#34892;&#35757;&#32451;&#30340;FRE&#20195;&#29702;&#33021;&#22815;&#25512;&#24191;&#21040;&#35299;&#20915;&#19968;&#31995;&#21015;&#27169;&#25311;&#26426;&#22120;&#20154;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#26032;&#20219;&#21153;&#65292;&#36890;&#24120;&#20248;&#20110;&#20808;&#21069;&#30340;&#38646;&#26679;&#26412;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17135v1 Announce Type: cross  Abstract: Can we pre-train a generalist agent from a large amount of unlabeled offline trajectories such that it can be immediately adapted to any new downstream tasks in a zero-shot manner? In this work, we present a functional reward encoding (FRE) as a general, scalable solution to this zero-shot RL problem. Our main idea is to learn functional representations of any arbitrary tasks by encoding their state-reward samples using a transformer-based variational auto-encoder. This functional encoding not only enables the pre-training of an agent from a wide diversity of general unsupervised reward functions, but also provides a way to solve any new downstream tasks in a zero-shot manner, given a small number of reward-annotated samples. We empirically show that FRE agents trained on diverse random unsupervised reward functions can generalize to solve novel tasks in a range of simulated robotic benchmarks, often outperforming previous zero-shot RL
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#24320;&#21457;&#19968;&#20010;&#20998;&#31867;&#27861;&#30740;&#31350;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#38382;&#39064;&#20851;&#32852;&#65292;&#25506;&#35752;&#20102;&#22312;&#27169;&#22411;&#32423;&#21035;&#24494;&#35843;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#25351;&#20986;&#20102;&#35270;&#35273;&#36136;&#37327;&#38477;&#20302;&#21487;&#33021;&#26159;&#19968;&#20010;&#23616;&#38480;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.17101</link><description>&lt;p&gt;
T-HITL&#26377;&#25928;&#35299;&#20915;&#22270;&#20687;&#29983;&#25104;&#20013;&#30340;&#38382;&#39064;&#20851;&#32852;&#24182;&#20445;&#25345;&#24635;&#20307;&#35270;&#35273;&#36136;&#37327;
&lt;/p&gt;
&lt;p&gt;
T-HITL Effectively Addresses Problematic Associations in Image Generation and Maintains Overall Visual Quality
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17101
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#24320;&#21457;&#19968;&#20010;&#20998;&#31867;&#27861;&#30740;&#31350;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#38382;&#39064;&#20851;&#32852;&#65292;&#25506;&#35752;&#20102;&#22312;&#27169;&#22411;&#32423;&#21035;&#24494;&#35843;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#25351;&#20986;&#20102;&#35270;&#35273;&#36136;&#37327;&#38477;&#20302;&#21487;&#33021;&#26159;&#19968;&#20010;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24615;&#20154;&#24037;&#26234;&#33021;&#22270;&#20687;&#27169;&#22411;&#21487;&#33021;&#20250;&#26080;&#24847;&#20013;&#29983;&#25104;&#26377;&#20851;&#20154;&#29289;&#30340;&#38382;&#39064;&#34920;&#24449;&#12290;&#36807;&#21435;&#30340;&#30740;&#31350;&#25351;&#20986;&#65292;&#20840;&#29699;&#26377;&#25968;&#30334;&#19975;&#29992;&#25143;&#27599;&#22825;&#19982;&#36825;&#20123;&#27169;&#22411;&#20114;&#21160;&#65292;&#24182;&#19988;&#36825;&#20123;&#27169;&#22411;&#65292;&#21253;&#25324;&#36890;&#36807;&#26377;&#20851;&#20154;&#29289;&#30340;&#38382;&#39064;&#34920;&#24449;&#65292;&#20855;&#26377;&#21487;&#33021;&#21152;&#21095;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#27495;&#35270;&#21644;&#20854;&#20182;&#21361;&#23475;&#30340;&#28508;&#21147;&#12290; &#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30528;&#37325;&#35299;&#20915;&#20154;&#21475;&#32676;&#20307;&#21644;&#35821;&#20041;&#27010;&#24565;&#20043;&#38388;&#29983;&#25104;&#38382;&#39064;&#20851;&#32852;&#30340;&#38382;&#39064;&#65292;&#36825;&#20123;&#20851;&#32852;&#21487;&#33021;&#21453;&#26144;&#21644;&#24378;&#21270;&#31038;&#20250;&#25968;&#25454;&#20013;&#23884;&#20837;&#30340;&#36127;&#38754;&#21465;&#36848;&#12290;&#22312;&#31038;&#20250;&#23398;&#25991;&#29486;&#65288;Blumer, 1958&#65289;&#30340;&#22522;&#30784;&#19978;&#65292;&#36890;&#36807;&#23558;&#34920;&#24449;&#26144;&#23556;&#21040;&#27169;&#22411;&#34892;&#20026;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#20998;&#31867;&#27861;&#26469;&#30740;&#31350;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#38382;&#39064;&#20851;&#32852;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#22312;&#27169;&#22411;&#32423;&#21035;&#24494;&#35843;&#30340;&#26377;&#25928;&#24615;&#20316;&#20026;&#35299;&#20915;&#36825;&#20123;&#20851;&#32852;&#30340;&#26041;&#27861;&#65292;&#30830;&#23450;&#35270;&#35273;&#36136;&#37327;&#38477;&#20302;&#21487;&#33021;&#26159;&#19968;&#20010;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17101v1 Announce Type: cross  Abstract: Generative AI image models may inadvertently generate problematic representations of people. Past research has noted that millions of users engage daily across the world with these models and that the models, including through problematic representations of people, have the potential to compound and accelerate real-world discrimination and other harms (Bianchi et al, 2023). In this paper, we focus on addressing the generation of problematic associations between demographic groups and semantic concepts that may reflect and reinforce negative narratives embedded in social data. Building on sociological literature (Blumer, 1958) and mapping representations to model behaviors, we have developed a taxonomy to study problematic associations in image generation models. We explore the effectiveness of fine tuning at the model level as a method to address these associations, identifying a potential reduction in visual quality as a limitation of
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Re-Ex&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#20107;&#23454;&#38169;&#35823;&#35828;&#26126;&#27493;&#39588;&#26469;&#20462;&#27491;LLM&#29983;&#25104;&#25991;&#26412;&#20013;&#30340;&#20107;&#23454;&#38169;&#35823;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#25552;&#31034;&#25216;&#26415;&#26469;&#20943;&#23569;&#25152;&#38656;&#30340;&#26631;&#35760;&#25968;&#37327;&#21644;&#25346;&#38047;&#26102;&#38388;</title><link>https://arxiv.org/abs/2402.17097</link><description>&lt;p&gt;
&#20462;&#22797;: &#22312;&#35828;&#26126;&#21518;&#20462;&#27491;LLM&#21709;&#24212;&#20013;&#30340;&#20107;&#23454;&#38169;&#35823;
&lt;/p&gt;
&lt;p&gt;
Re-Ex: Revising after Explanation Reduces the Factual Errors in LLM Responses
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17097
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Re-Ex&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#20107;&#23454;&#38169;&#35823;&#35828;&#26126;&#27493;&#39588;&#26469;&#20462;&#27491;LLM&#29983;&#25104;&#25991;&#26412;&#20013;&#30340;&#20107;&#23454;&#38169;&#35823;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#25552;&#31034;&#25216;&#26415;&#26469;&#20943;&#23569;&#25152;&#38656;&#30340;&#26631;&#35760;&#25968;&#37327;&#21644;&#25346;&#38047;&#26102;&#38388;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32531;&#35299;&#24187;&#35273;&#38382;&#39064;&#26159;LLM&#30340;&#20027;&#35201;&#25361;&#25112;&#20043;&#19968;&#65292;&#25105;&#20204;&#38656;&#35201;&#20811;&#26381;&#36825;&#19968;&#25361;&#25112;&#65292;&#20197;&#20415;&#21487;&#38752;&#22320;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#20351;&#29992;&#23427;&#20204;&#12290;&#26368;&#36817;&#65292;&#25552;&#20986;&#20102;&#21508;&#31181;&#26041;&#27861;&#26469;&#26816;&#26597;LLM&#29983;&#25104;&#30340;&#25991;&#26412;&#20013;&#30340;&#20107;&#23454;&#38169;&#35823;&#65292;&#24182;&#30456;&#24212;&#22320;&#36827;&#34892;&#20462;&#35746;&#65292;&#20197;&#20943;&#23569;&#24187;&#35273;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Re-Ex&#65292;&#19968;&#31181;&#20462;&#35746;LLM&#29983;&#25104;&#25991;&#26412;&#30340;&#26041;&#27861;&#65292;&#23427;&#24341;&#20837;&#20102;&#19968;&#20010;&#31216;&#20026;&#20107;&#23454;&#38169;&#35823;&#35828;&#26126;&#27493;&#39588;&#30340;&#26032;&#27493;&#39588;&#12290; Re-Ex&#20351;&#29992;3&#20010;&#27493;&#39588;&#23545;LLM&#30340;&#21021;&#22987;&#21709;&#24212;&#36827;&#34892;&#20462;&#35746;&#65306;&#39318;&#20808;&#65292;&#20351;&#29992;&#22806;&#37096;&#24037;&#20855;&#33719;&#21462;&#21709;&#24212;&#20013;&#20107;&#23454;&#38169;&#35823;&#30340;&#35777;&#25454;&#65307;&#31532;&#20108;&#65292;&#35201;&#27714;LLM&#26681;&#25454;&#31532;&#19968;&#27493;&#20013;&#25910;&#38598;&#30340;&#35777;&#25454;&#35299;&#37322;&#21709;&#24212;&#20013;&#30340;&#38382;&#39064;&#37096;&#20998;&#65307;&#26368;&#21518;&#65292;LLM&#20351;&#29992;&#22312;&#31532;&#20108;&#27493;&#20013;&#33719;&#24471;&#30340;&#35299;&#37322;&#23545;&#21709;&#24212;&#36827;&#34892;&#20462;&#35746;&#12290;&#38500;&#20102;&#35828;&#26126;&#27493;&#39588;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#26032;&#30340;&#25552;&#31034;&#25216;&#26415;&#65292;&#20197;&#20943;&#23569;&#25152;&#38656;&#30340;&#26631;&#35760;&#25968;&#37327;&#21644;&#25346;&#38047;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17097v1 Announce Type: cross  Abstract: Mitigating hallucination issues is one of the main challenges of LLMs we need to overcome, in order to reliably use them in real-world scenarios. Recently, various methods are proposed to check the factual errors in the LLM-generated texts and revise them accordingly, to reduce the hallucination issue. In this paper, we propose Re-Ex, a method of revising LLM-generated texts, which introduces a novel step dubbed as the factual error explanation step. Re-Ex revises the initial response of LLMs using 3-steps: first, external tools are used to get the evidences on the factual errors in the response; second, LLMs are instructed to explain the problematic parts of the response based on the evidences gathered in the first step; finally, LLMs revise the response using the explanation obtained in the second step. In addition to the explanation step, we propose new prompting techniques to reduce the amount of tokens and wall-clock time required
&lt;/p&gt;</description></item><item><title>&#20174;&#36125;&#21494;&#26031;&#32593;&#32476;&#35745;&#31639;&#20986;&#30340;&#25968;&#25454;&#38598;&#30340;&#20284;&#28982;&#24615;&#20027;&#35201;&#30001;&#32463;&#39564;&#32593;&#32476;&#30340;&#20284;&#28982;&#24615;&#30340;&#20840;&#23616;&#26368;&#22823;&#20540;&#25152;&#20027;&#23548;&#65292;&#24182;&#19988;&#20165;&#24403;&#36125;&#21494;&#26031;&#32593;&#32476;&#30340;&#21442;&#25968;&#19982;&#32463;&#39564;&#27169;&#22411;&#30340;&#21442;&#25968;&#19968;&#33268;&#26102;&#65292;&#36825;&#26679;&#30340;&#26368;&#22823;&#20540;&#25165;&#20250;&#34987;&#23454;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.17087</link><description>&lt;p&gt;
&#20851;&#20110;&#20855;&#26377;&#28508;&#22312;&#26681;&#21464;&#37327;&#30340;&#36125;&#21494;&#26031;&#32593;&#32476;&#30340;&#27880;&#35299;
&lt;/p&gt;
&lt;p&gt;
A Note on Bayesian Networks with Latent Root Variables
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17087
&lt;/p&gt;
&lt;p&gt;
&#20174;&#36125;&#21494;&#26031;&#32593;&#32476;&#35745;&#31639;&#20986;&#30340;&#25968;&#25454;&#38598;&#30340;&#20284;&#28982;&#24615;&#20027;&#35201;&#30001;&#32463;&#39564;&#32593;&#32476;&#30340;&#20284;&#28982;&#24615;&#30340;&#20840;&#23616;&#26368;&#22823;&#20540;&#25152;&#20027;&#23548;&#65292;&#24182;&#19988;&#20165;&#24403;&#36125;&#21494;&#26031;&#32593;&#32476;&#30340;&#21442;&#25968;&#19982;&#32463;&#39564;&#27169;&#22411;&#30340;&#21442;&#25968;&#19968;&#33268;&#26102;&#65292;&#36825;&#26679;&#30340;&#26368;&#22823;&#20540;&#25165;&#20250;&#34987;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#34920;&#24449;&#20102;&#20174;&#20855;&#26377;&#28508;&#22312;&#26681;&#33410;&#28857;&#30340;&#36125;&#21494;&#26031;&#32593;&#32476;&#35745;&#31639;&#20986;&#30340;&#20284;&#28982;&#20989;&#25968;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#23545;&#20110;&#21097;&#20313;&#30340;&#26174;&#24615;&#21464;&#37327;&#30340;&#36793;&#32536;&#20998;&#24067;&#20063;&#20250;&#20687;&#19968;&#20010;&#36125;&#21494;&#26031;&#32593;&#32476;&#19968;&#26679;&#20998;&#35299;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#32463;&#39564;&#21270;&#12290;&#19968;&#32452;&#35266;&#27979;&#21040;&#30340;&#26174;&#24615;&#21464;&#37327;&#30340;&#25968;&#25454;&#38598;&#20351;&#25105;&#20204;&#33021;&#22815;&#37327;&#21270;&#32463;&#39564;&#36125;&#21494;&#26031;&#32593;&#32476;&#30340;&#21442;&#25968;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;(i)&#20174;&#21407;&#22987;&#36125;&#21494;&#26031;&#32593;&#32476;&#35745;&#31639;&#20986;&#36825;&#26679;&#19968;&#20010;&#25968;&#25454;&#38598;&#30340;&#20284;&#28982;&#24615;&#30001;&#32463;&#39564;&#21270;&#27169;&#22411;&#30340;&#20284;&#28982;&#24615;&#30340;&#20840;&#23616;&#26368;&#22823;&#20540;&#25152;&#20027;&#23548;&#65307;&#20197;&#21450;(ii)&#36825;&#26679;&#19968;&#20010;&#26368;&#22823;&#20540;&#20165;&#22312;&#36125;&#21494;&#26031;&#32593;&#32476;&#30340;&#21442;&#25968;&#19982;&#32463;&#39564;&#27169;&#22411;&#30340;&#21442;&#25968;&#19968;&#33268;&#26102;&#25165;&#20250;&#36798;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17087v1 Announce Type: cross  Abstract: We characterise the likelihood function computed from a Bayesian network with latent variables as root nodes. We show that the marginal distribution over the remaining, manifest, variables also factorises as a Bayesian network, which we call empirical. A dataset of observations of the manifest variables allows us to quantify the parameters of the empirical Bayesian net. We prove that (i) the likelihood of such a dataset from the original Bayesian network is dominated by the global maximum of the likelihood from the empirical one; and that (ii) such a maximum is attained if and only if the parameters of the Bayesian network are consistent with those of the empirical model.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#19982;&#24403;&#22320;&#20225;&#19994;&#23478;&#21512;&#20316;&#20849;&#21516;&#35774;&#35745;&#20132;&#20114;&#24335;&#30740;&#35752;&#20250;&#65292;&#24110;&#21161;&#20182;&#20204;&#20102;&#35299;&#29983;&#25104; AI &#24179;&#21488;&#30340;&#37325;&#35201;&#24615;&#65292;&#25903;&#25345;&#21487;&#25805;&#20316;&#20351;&#29992;&#24182;&#25581;&#31034;&#21019;&#19994;&#21147;&#37327;&#12290;</title><link>https://arxiv.org/abs/2402.17082</link><description>&lt;p&gt;
&#25286;&#35299;&#31616;&#21333;&#22806;&#34920;&#65306;&#19982;&#24403;&#22320;&#20225;&#19994;&#23478;&#20849;&#21516;&#35774;&#35745;&#21021;&#32423;&#29983;&#25104; AI &#30740;&#35752;&#20250;
&lt;/p&gt;
&lt;p&gt;
Deconstructing the Veneer of Simplicity: Co-Designing Introductory Generative AI Workshops with Local Entrepreneurs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17082
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#19982;&#24403;&#22320;&#20225;&#19994;&#23478;&#21512;&#20316;&#20849;&#21516;&#35774;&#35745;&#20132;&#20114;&#24335;&#30740;&#35752;&#20250;&#65292;&#24110;&#21161;&#20182;&#20204;&#20102;&#35299;&#29983;&#25104; AI &#24179;&#21488;&#30340;&#37325;&#35201;&#24615;&#65292;&#25903;&#25345;&#21487;&#25805;&#20316;&#20351;&#29992;&#24182;&#25581;&#31034;&#21019;&#19994;&#21147;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104; AI &#24179;&#21488;&#21644;&#21151;&#33021;&#27491;&#22312;&#28183;&#36879;&#24037;&#20316;&#30340;&#35768;&#22810;&#26041;&#38754;&#12290;&#29305;&#21035;&#26159;&#26469;&#33258;&#23454;&#21147;&#34180;&#24369;&#30340;&#32463;&#27982;&#20307;&#30340;&#20225;&#19994;&#23478;&#30001;&#20110;&#36164;&#28304;&#26377;&#38480;&#65292;&#22240;&#27492;&#24456;&#26377;&#21487;&#33021;&#23558;&#20219;&#21153;&#22806;&#21253;&#32473;&#29983;&#25104; AI&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#19982;&#24403;&#22320;&#21019;&#19994;&#20013;&#24515;&#24320;&#23637;&#38271;&#36798;&#22235;&#24180;&#30340;&#21512;&#20316;&#65292;&#33268;&#21147;&#20110;&#35299;&#20915;&#23545;&#36825;&#20123;&#25216;&#26415;&#20351;&#29992;&#30340;&#19981;&#24179;&#31561;&#29616;&#35937;&#12290;&#25105;&#20204;&#20849;&#21516;&#35774;&#35745;&#20102;&#19968;&#31995;&#21015;&#20114;&#21160;&#30740;&#35752;&#20250;&#65292;&#26088;&#22312;&#24110;&#21161;&#24403;&#22320;&#20225;&#19994;&#23478;&#29087;&#24713;&#29983;&#25104; AI &#24179;&#21488;&#12290;&#38500;&#27492;&#20043;&#22806;&#65292;&#25105;&#20204;&#36824;&#19982;15&#20301;&#24403;&#22320;&#20225;&#19994;&#23478;&#21644;&#31038;&#21306;&#25552;&#20379;&#32773;&#36827;&#34892;&#20102;&#35775;&#35848;&#12290;&#25105;&#20204;&#35814;&#32454;&#20171;&#32461;&#20102;&#23545;&#24403;&#22320;&#20225;&#19994;&#23478;&#36827;&#34892;&#22242;&#20307;&#21644;&#25903;&#25345;&#24615;&#26333;&#20809;&#30340;&#37325;&#35201;&#24615;&#65292;&#20026;&#20182;&#20204;&#25552;&#20379;&#29983;&#25104; AI &#24037;&#20855;&#30340;&#21487;&#25805;&#20316;&#20351;&#29992;&#65288;&#20197;&#21450;&#23545;&#19981;&#20351;&#29992;&#30340;&#25903;&#25345;&#65289;&#65292;&#36890;&#36807;&#24378;&#35843;&#21019;&#19994;&#21147;&#37327;&#26469;&#25581;&#24320;&#29983;&#25104; AI &#25216;&#26415;&#30340;&#31070;&#31192;&#38754;&#32433;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17082v1 Announce Type: cross  Abstract: Generative AI platforms and features are permeating many aspects of work. Entrepreneurs from lean economies in particular are well positioned to outsource tasks to generative AI given limited resources. In this paper, we work to address a growing disparity in use of these technologies by building on a four-year partnership with a local entrepreneurial hub dedicated to equity in tech and entrepreneurship. Together, we co-designed an interactive workshops series aimed to onboard local entrepreneurs to generative AI platforms. Alongside four community-driven and iterative workshops with entrepreneurs across five months, we conducted interviews with 15 local entrepreneurs and community providers. We detail the importance of communal and supportive exposure to generative AI tools for local entrepreneurs, scaffolding actionable use (and supporting non-use), demystifying generative AI technologies by emphasizing entrepreneurial power, while s
&lt;/p&gt;</description></item><item><title>&#35813;&#26041;&#27861;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36229;&#39640;&#32500;&#35745;&#31639;&#36827;&#34892;&#21333;&#27425;&#22270;&#34920;&#31034;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#25968;&#25454;&#25237;&#24433;&#21040;&#39640;&#32500;&#31354;&#38388;&#24182;&#21033;&#29992;HD&#36816;&#31639;&#31526;&#36827;&#34892;&#20449;&#24687;&#32858;&#21512;&#65292;&#23454;&#29616;&#20102;&#19982;&#26368;&#20808;&#36827;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30456;&#31454;&#20105;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#35745;&#31639;&#26114;&#36149;&#30340;&#35757;&#32451;&#12290;</title><link>https://arxiv.org/abs/2402.17073</link><description>&lt;p&gt;
&#20351;&#29992;&#36229;&#39640;&#32500;&#35745;&#31639;&#36827;&#34892;&#21333;&#27425;&#22270;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
One-Shot Graph Representation Learning Using Hyperdimensional Computing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17073
&lt;/p&gt;
&lt;p&gt;
&#35813;&#26041;&#27861;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36229;&#39640;&#32500;&#35745;&#31639;&#36827;&#34892;&#21333;&#27425;&#22270;&#34920;&#31034;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#25968;&#25454;&#25237;&#24433;&#21040;&#39640;&#32500;&#31354;&#38388;&#24182;&#21033;&#29992;HD&#36816;&#31639;&#31526;&#36827;&#34892;&#20449;&#24687;&#32858;&#21512;&#65292;&#23454;&#29616;&#20102;&#19982;&#26368;&#20808;&#36827;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30456;&#31454;&#20105;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#35745;&#31639;&#26114;&#36149;&#30340;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#12289;&#31616;&#21333;&#12289;&#24555;&#36895;&#12289;&#39640;&#25928;&#30340;&#21322;&#30417;&#30563;&#22270;&#23398;&#20064;&#26041;&#27861;&#12290;&#25152;&#25552;&#26041;&#27861;&#21033;&#29992;&#36229;&#39640;&#32500;&#35745;&#31639;&#65292;&#23558;&#25968;&#25454;&#26679;&#26412;&#20351;&#29992;&#38543;&#26426;&#25237;&#24433;&#32534;&#30721;&#21040;&#39640;&#32500;&#31354;&#38388;&#65288;&#31616;&#31216;HD&#31354;&#38388;&#65289;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#33410;&#28857;&#34920;&#31034;&#30340;&#21333;&#23556;&#24615;&#36136;&#30340;&#36229;&#39640;&#32500;&#22270;&#23398;&#20064;&#65288;HDGL&#65289;&#31639;&#27861;&#12290;HDGL&#23558;&#33410;&#28857;&#29305;&#24449;&#26144;&#23556;&#21040;HD&#31354;&#38388;&#65292;&#28982;&#21518;&#20351;&#29992;HD&#36816;&#31639;&#31526;&#65288;&#22914;&#25414;&#32465;&#21644;&#32465;&#23450;&#65289;&#26469;&#32858;&#21512;&#27599;&#20010;&#33410;&#28857;&#30340;&#23616;&#37096;&#37051;&#22495;&#20449;&#24687;&#12290;&#23545;&#24191;&#27867;&#20351;&#29992;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;HDGL&#23454;&#29616;&#20102;&#19982;&#26368;&#20808;&#36827;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30456;&#31454;&#20105;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#35745;&#31639;&#26114;&#36149;&#30340;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17073v1 Announce Type: cross  Abstract: We present a novel, simple, fast, and efficient approach for semi-supervised learning on graphs. The proposed approach takes advantage of hyper-dimensional computing which encodes data samples using random projections into a high dimensional space (HD space for short). Specifically, we propose a Hyper-dimensional Graph Learning (HDGL) algorithm that leverages the injectivity property of the node representations of a family of graph neural networks. HDGL maps node features to the HD space and then uses HD operators such as bundling and binding to aggregate information from the local neighborhood of each node. Results of experiments with widely used benchmark data sets show that HDGL achieves predictive performance that is competitive with the state-of-the-art deep learning methods, without the need for computationally expensive training.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22312;&#36739;&#20302;&#20998;&#36776;&#29575;&#36827;&#34892;&#26080;&#26465;&#20214;&#35757;&#32451;&#65292;&#20801;&#35768;&#38271;&#23614;&#31867;&#21035;&#20174;&#20449;&#24687;&#26356;&#20016;&#23500;&#30340;&#31867;&#21035;&#20013;&#20849;&#20139;&#30693;&#35782;&#65292;&#20197;&#25913;&#21892;&#38271;&#23614;&#25968;&#25454;&#19979;&#31867;&#21035;&#26465;&#20214;GANs&#30340;&#35757;&#32451;</title><link>https://arxiv.org/abs/2402.17065</link><description>&lt;p&gt;
&#39535;&#26381;&#31867;&#21035;&#26465;&#20214;GAN&#20013;&#30340;&#38271;&#23614;&#38382;&#39064;&#65306;&#36890;&#36807;&#22312;&#36739;&#20302;&#20998;&#36776;&#29575;&#36827;&#34892;&#26080;&#26465;&#20214;&#35757;&#32451;&#36827;&#34892;&#30693;&#35782;&#20849;&#20139;
&lt;/p&gt;
&lt;p&gt;
Taming the Tail in Class-Conditional GANs: Knowledge Sharing via Unconditional Training at Lower Resolutions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17065
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22312;&#36739;&#20302;&#20998;&#36776;&#29575;&#36827;&#34892;&#26080;&#26465;&#20214;&#35757;&#32451;&#65292;&#20801;&#35768;&#38271;&#23614;&#31867;&#21035;&#20174;&#20449;&#24687;&#26356;&#20016;&#23500;&#30340;&#31867;&#21035;&#20013;&#20849;&#20139;&#30693;&#35782;&#65292;&#20197;&#25913;&#21892;&#38271;&#23614;&#25968;&#25454;&#19979;&#31867;&#21035;&#26465;&#20214;GANs&#30340;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#23545;&#20110;&#20351;&#29992;&#26377;&#38480;&#35757;&#32451;&#25968;&#25454;&#35757;&#32451;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#30740;&#31350;&#65292;&#20294;&#20174;&#38271;&#23614;&#35757;&#32451;&#20998;&#24067;&#29983;&#25104;&#22270;&#20687;&#30340;&#25216;&#26415;&#20173;&#28982;&#30456;&#24403;&#26410;&#34987;&#25506;&#32034;&#12290;&#22312;&#23384;&#22312;&#19981;&#24179;&#34913;&#30340;&#22810;&#31867;&#21035;&#35757;&#32451;&#25968;&#25454;&#26102;&#65292;GANs&#20542;&#21521;&#20110;&#20559;&#29233;&#26679;&#26412;&#26356;&#22810;&#30340;&#31867;&#21035;&#65292;&#23548;&#33268;&#23614;&#37096;&#31867;&#21035;&#30340;&#29983;&#25104;&#20302;&#36136;&#37327;&#19988;&#26679;&#26412;&#19981;&#22815;&#22810;&#26679;&#21270;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#25913;&#36827;&#20351;&#29992;&#38271;&#23614;&#25968;&#25454;&#35757;&#32451;&#31867;&#21035;&#26465;&#20214;GANs&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#30693;&#35782;&#20849;&#20139;&#26041;&#27861;&#65292;&#20801;&#35768;&#23614;&#37096;&#31867;&#21035;&#20174;&#35757;&#32451;&#25968;&#25454;&#26356;&#20016;&#23500;&#30340;&#31867;&#21035;&#20013;&#20511;&#37492;&#20016;&#23500;&#30340;&#20449;&#24687;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#23545;&#29616;&#26377;&#30340;&#31867;&#21035;&#26465;&#20214;GAN&#26550;&#26500;&#36827;&#34892;&#20102;&#20462;&#25913;&#65292;&#20197;&#30830;&#20445;&#29983;&#25104;&#22120;&#30340;&#36739;&#20302;&#20998;&#36776;&#29575;&#23618;&#23436;&#20840;&#26080;&#26465;&#20214;&#22320;&#36827;&#34892;&#35757;&#32451;&#65292;&#21516;&#26102;&#23558;&#31867;&#21035;&#26465;&#20214;&#29983;&#25104;&#20445;&#30041;&#32473;&#36739;&#39640;&#20998;&#36776;&#29575;&#23618;&#12290;&#22312;&#22810;&#20010;&#23454;&#39564;&#20013;&#36827;&#34892;&#20102;&#23454;&#39564;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17065v1 Announce Type: cross  Abstract: Despite the extensive research on training generative adversarial networks (GANs) with limited training data, learning to generate images from long-tailed training distributions remains fairly unexplored. In the presence of imbalanced multi-class training data, GANs tend to favor classes with more samples, leading to the generation of low-quality and less diverse samples in tail classes. In this study, we aim to improve the training of class-conditional GANs with long-tailed data. We propose a straightforward yet effective method for knowledge sharing, allowing tail classes to borrow from the rich information from classes with more abundant training data. More concretely, we propose modifications to existing class-conditional GAN architectures to ensure that the lower-resolution layers of the generator are trained entirely unconditionally while reserving class-conditional generation for the higher-resolution layers. Experiments on seve
&lt;/p&gt;</description></item><item><title>&#20998;&#26512;&#20102;&#36807;&#21435;10&#24180;&#38024;&#23545;&#19981;&#21516;&#31867;&#22411;&#32593;&#32476;&#25915;&#20987;&#26816;&#27979;&#30340;&#21508;&#31181;&#26368;&#20808;&#36827;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#30528;&#37325;&#27604;&#36739;&#20102;&#26368;&#26032;&#30340;&#24037;&#20316;&#12290;</title><link>https://arxiv.org/abs/2402.17045</link><description>&lt;p&gt;
&#23545;&#21508;&#31867;&#32593;&#32476;&#25915;&#20987;&#26816;&#27979;&#30340;&#26368;&#20808;&#36827;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#24615;&#33021;&#30340;&#30740;&#31350;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
An Investigation into the Performances of the State-of-the-art Machine Learning Approaches for Various Cyber-attack Detection: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17045
&lt;/p&gt;
&lt;p&gt;
&#20998;&#26512;&#20102;&#36807;&#21435;10&#24180;&#38024;&#23545;&#19981;&#21516;&#31867;&#22411;&#32593;&#32476;&#25915;&#20987;&#26816;&#27979;&#30340;&#21508;&#31181;&#26368;&#20808;&#36827;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#30528;&#37325;&#27604;&#36739;&#20102;&#26368;&#26032;&#30340;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#20445;&#25252;&#35745;&#31639;&#26426;&#21644;&#20449;&#24687;&#31995;&#32479;&#20813;&#21463;&#25915;&#20987;&#32773;&#21033;&#29992;&#31995;&#32479;&#20013;&#30340;&#28431;&#27934;&#36827;&#34892;&#32593;&#32476;&#29359;&#32618;&#30340;&#20405;&#23475;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#20960;&#31181;&#29992;&#20110;&#23454;&#26102;&#26816;&#27979;&#28431;&#27934;&#20197;&#25552;&#39640;&#20449;&#24687;&#31995;&#32479;&#23433;&#20840;&#24615;&#30340;&#26041;&#27861;&#12290;&#22312;&#25152;&#26377;&#25552;&#20986;&#30340;&#26041;&#27861;&#20013;&#65292;&#26426;&#22120;&#23398;&#20064;&#26159;&#23454;&#29616;&#31995;&#32479;&#23433;&#20840;&#30340;&#25928;&#26524;&#26368;&#22909;&#30340;&#26041;&#27861;&#65292;&#20854;&#33021;&#21147;&#33539;&#22260;&#20174;&#26089;&#26399;&#26816;&#27979;&#36719;&#20214;&#28431;&#27934;&#21040;&#23454;&#26102;&#26816;&#27979;&#31995;&#32479;&#20013;&#27491;&#22312;&#36827;&#34892;&#30340;&#22949;&#21327;&#12290;&#30001;&#20110;&#23384;&#22312;&#19981;&#21516;&#31867;&#22411;&#30340;&#32593;&#32476;&#25915;&#20987;&#65292;&#27599;&#31181;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#37117;&#20381;&#36182;&#20110;&#19981;&#21516;&#30340;&#35757;&#32451;&#31639;&#27861;&#65292;&#36825;&#20063;&#24433;&#21709;&#20102;&#23427;&#20204;&#23545;&#29305;&#23450;&#31867;&#22411;&#32593;&#32476;&#25915;&#20987;&#30340;&#36866;&#29992;&#24615;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#36807;&#21435;10&#24180;&#38024;&#23545;&#19981;&#21516;&#31867;&#22411;&#32593;&#32476;&#25915;&#20987;&#26816;&#27979;&#30340;&#27599;&#19968;&#20010;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#37325;&#28857;&#25918;&#22312;&#26368;&#36817;&#30340;&#24037;&#20316;&#19978;&#20197;&#36827;&#34892;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17045v1 Announce Type: cross  Abstract: To secure computers and information systems from attackers taking advantage of vulnerabilities in the system to commit cybercrime, several methods have been proposed for real-time detection of vulnerabilities to improve security around information systems. Of all the proposed methods, machine learning had been the most effective method in securing a system with capabilities ranging from early detection of software vulnerabilities to real-time detection of ongoing compromise in a system. As there are different types of cyberattacks, each of the existing state-of-the-art machine learning models depends on different algorithms for training which also impact their suitability for detection of a particular type of cyberattack. In this research, we analyzed each of the current state-of-theart machine learning models for different types of cyberattack detection from the past 10 years with a major emphasis on the most recent works for comparat
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22312;&#35797;&#22270;&#35299;&#20915;&#20174;&#35797;&#39564;&#32467;&#26524;&#25512;&#24191;&#21040;&#30446;&#26631;&#31181;&#32676;&#30340;&#22806;&#37096;&#26377;&#25928;&#24615;&#25361;&#25112;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;</title><link>https://arxiv.org/abs/2402.17042</link><description>&lt;p&gt;
&#36890;&#21521;&#20174;&#35797;&#39564;&#25512;&#24191;&#25512;&#29702;&#21040;&#30446;&#26631;&#31181;&#32676;&#30340;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Towards Generalizing Inferences from Trials to Target Populations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17042
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22312;&#35797;&#22270;&#35299;&#20915;&#20174;&#35797;&#39564;&#32467;&#26524;&#25512;&#24191;&#21040;&#30446;&#26631;&#31181;&#32676;&#30340;&#22806;&#37096;&#26377;&#25928;&#24615;&#25361;&#25112;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#23545;&#29031;&#35797;&#39564;&#65288;RCTs&#65289;&#22312;&#20135;&#29983;&#20869;&#37096;&#26377;&#25928;&#20272;&#35745;&#26041;&#38754;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#32780;&#23545;&#25193;&#23637;&#36825;&#20123;&#21457;&#29616;&#20197;&#33719;&#24471;&#22806;&#37096;&#26377;&#25928;&#20272;&#35745;&#33267;&#20851;&#37325;&#35201;&#65292;&#20197;&#20419;&#36827;&#26356;&#24191;&#27867;&#30340;&#31185;&#23398;&#25506;&#31350;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#24212;&#23545;&#36825;&#20123;&#22806;&#37096;&#26377;&#25928;&#24615;&#25361;&#25112;&#30340;&#21069;&#27839;&#65292;&#27010;&#25324;&#20102;2023&#24180;&#31179;&#23395;&#22312;&#24067;&#26391;&#22823;&#23398;&#35745;&#31639;&#19982;&#23454;&#39564;&#25968;&#23398;&#30740;&#31350;&#25152;&#65288;ICERM&#65289;&#20030;&#34892;&#30340;&#19968;&#27425;&#36328;&#23398;&#31185;&#30740;&#35752;&#20250;&#30340;&#31934;&#21326;&#12290;&#35813;&#30740;&#35752;&#20250;&#27719;&#38598;&#20102;&#26469;&#33258;&#31038;&#20250;&#31185;&#23398;&#12289;&#21307;&#23398;&#12289;&#20844;&#20849;&#21355;&#29983;&#12289;&#32479;&#35745;&#23398;&#12289;&#35745;&#31639;&#26426;&#31185;&#23398;&#21644;&#25945;&#32946;&#31561;&#21508;&#20010;&#39046;&#22495;&#30340;&#19987;&#23478;&#65292;&#20197;&#35299;&#20915;&#27599;&#20010;&#23398;&#31185;&#22312;&#25512;&#26029;&#23454;&#39564;&#32467;&#26524;&#26041;&#38754;&#38754;&#20020;&#30340;&#29420;&#29305;&#38556;&#30861;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#19977;&#20010;&#20851;&#38190;&#36129;&#29486;&#65306;&#25105;&#20204;&#25972;&#21512;&#27491;&#22312;&#36827;&#34892;&#30340;&#21162;&#21147;&#65292;&#31361;&#20986;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17042v1 Announce Type: cross  Abstract: Randomized Controlled Trials (RCTs) are pivotal in generating internally valid estimates with minimal assumptions, serving as a cornerstone for researchers dedicated to advancing causal inference methods. However, extending these findings beyond the experimental cohort to achieve externally valid estimates is crucial for broader scientific inquiry. This paper delves into the forefront of addressing these external validity challenges, encapsulating the essence of a multidisciplinary workshop held at the Institute for Computational and Experimental Research in Mathematics (ICERM), Brown University, in Fall 2023. The workshop congregated experts from diverse fields including social science, medicine, public health, statistics, computer science, and education, to tackle the unique obstacles each discipline faces in extrapolating experimental findings. Our study presents three key contributions: we integrate ongoing efforts, highlighting me
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;REFACTOR&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#20174;&#35777;&#26126;&#20013;&#25552;&#21462;&#23450;&#29702;&#65292;&#26032;&#23450;&#29702;&#30340;&#24341;&#20837;&#24110;&#21161;&#32553;&#30701;&#35777;&#26126;&#38271;&#24230;&#24182;&#25552;&#39640;&#35777;&#26126;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.17032</link><description>&lt;p&gt;
&#20174;&#35777;&#26126;&#20013;&#25552;&#21462;&#23450;&#29702;&#30340;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
REFACTOR: Learning to Extract Theorems from Proofs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17032
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;REFACTOR&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#20174;&#35777;&#26126;&#20013;&#25552;&#21462;&#23450;&#29702;&#65292;&#26032;&#23450;&#29702;&#30340;&#24341;&#20837;&#24110;&#21161;&#32553;&#30701;&#35777;&#26126;&#38271;&#24230;&#24182;&#25552;&#39640;&#35777;&#26126;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#25968;&#23398;&#23478;&#36890;&#24120;&#25797;&#38271;&#35782;&#21035;&#27169;&#22359;&#21270;&#21644;&#21487;&#37325;&#29992;&#30340;&#23450;&#29702;&#65292;&#36825;&#20123;&#23450;&#29702;&#20351;&#22797;&#26434;&#30340;&#25968;&#23398;&#32467;&#26524;&#26131;&#20110;&#33719;&#24471;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#8220;&#23450;&#29702;&#20174;&#35777;&#26126;&#20013;&#25552;&#21462;&#22120;&#65288;REFACTOR&#65289;&#8221;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#27169;&#20223;&#36825;&#31181;&#24418;&#24335;&#25968;&#23398;&#23450;&#29702;&#35777;&#26126;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#19968;&#32452;&#26410;&#35265;&#35777;&#26126;&#19978;&#65292;REFACTOR&#33021;&#22815;&#25552;&#21462;&#20986;&#20154;&#31867;&#22312;&#20889;&#35777;&#26126;&#26102;&#20250;&#20351;&#29992;&#30340;19.6%&#30340;&#23450;&#29702;&#12290;&#24403;&#23558;&#27169;&#22411;&#24212;&#29992;&#20110;&#29616;&#26377;&#30340;Metamath&#24211;&#26102;&#65292;REFACTOR&#25552;&#21462;&#20986;&#20102;16&#20010;&#26032;&#23450;&#29702;&#12290;&#36890;&#36807;&#26032;&#25552;&#21462;&#30340;&#23450;&#29702;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;MetaMath&#25968;&#25454;&#24211;&#20013;&#30340;&#29616;&#26377;&#35777;&#26126;&#21487;&#20197;&#34987;&#37325;&#26500;&#12290;&#32463;&#37325;&#26500;&#21518;&#65292;&#36825;&#20123;&#26032;&#23450;&#29702;&#34987;&#38750;&#24120;&#39057;&#32321;&#22320;&#20351;&#29992;&#65292;&#24179;&#22343;&#20351;&#29992;&#27425;&#25968;&#20026;733.5&#27425;&#65292;&#24182;&#26377;&#21161;&#20110;&#32553;&#30701;&#35777;&#26126;&#38271;&#24230;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#32463;&#36807;&#26032;&#23450;&#29702;&#37325;&#26500;&#30340;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#35777;&#26126;&#32773;&#35777;&#26126;&#20102;&#26356;&#22810;&#27979;&#35797;&#23450;&#29702;&#65292;&#24182;&#36890;&#36807;&#39057;&#32321;&#21033;&#29992;&#36229;&#36234;&#20102;&#26368;&#20808;&#36827;&#30340;&#22522;&#20934;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17032v1 Announce Type: new  Abstract: Human mathematicians are often good at recognizing modular and reusable theorems that make complex mathematical results within reach. In this paper, we propose a novel method called theoREm-from-prooF extrACTOR (REFACTOR) for training neural networks to mimic this ability in formal mathematical theorem proving. We show on a set of unseen proofs, REFACTOR is able to extract 19.6% of the theorems that humans would use to write the proofs. When applying the model to the existing Metamath library, REFACTOR extracted 16 new theorems. With newly extracted theorems, we show that the existing proofs in the MetaMath database can be refactored. The new theorems are used very frequently after refactoring, with an average usage of 733.5 times, and help shorten the proof lengths. Lastly, we demonstrate that the prover trained on the new-theorem refactored dataset proves more test theorems and outperforms state-of-the-art baselines by frequently lever
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22312;&#31070;&#32463;&#27169;&#22411;&#20013;&#24341;&#20837;&#19981;&#21516;iable&#21644;&#23436;&#20840;&#21367;&#31215;&#30340;&#21069;&#31471;&#27169;&#22411;&#65292;&#24182;&#32467;&#21512;&#36339;&#36291;&#36830;&#25509;&#65292;&#25104;&#21151;&#23454;&#29616;&#23545;&#26799;&#24230;&#25915;&#20987;&#30340;&#26174;&#33879;&#38887;&#24615;&#65292;&#24182;&#36890;&#36807;&#23558;&#27169;&#22411;&#32452;&#21512;&#25104;&#38543;&#26426;&#38598;&#21512;&#65292;&#26377;&#25928;&#23545;&#25239;&#40657;&#30418;&#25915;&#20987;&#12290;</title><link>https://arxiv.org/abs/2402.17018</link><description>&lt;p&gt;
&#36890;&#36807;&#23436;&#20840;&#21367;&#31215;&#21644;&#21487;&#24494;&#30340;&#21069;&#31471;&#19982;&#36339;&#36291;&#36830;&#25509;&#23545;&#26799;&#24230;&#25915;&#20987;&#34920;&#29616;&#20986;&#26174;&#33879;&#38887;&#24615;&#30340;&#32784;&#20154;&#23547;&#21619;&#26696;&#20363;
&lt;/p&gt;
&lt;p&gt;
A Curious Case of Remarkable Resilience to Gradient Attacks via Fully Convolutional and Differentiable Front End with a Skip Connection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17018
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22312;&#31070;&#32463;&#27169;&#22411;&#20013;&#24341;&#20837;&#19981;&#21516;iable&#21644;&#23436;&#20840;&#21367;&#31215;&#30340;&#21069;&#31471;&#27169;&#22411;&#65292;&#24182;&#32467;&#21512;&#36339;&#36291;&#36830;&#25509;&#65292;&#25104;&#21151;&#23454;&#29616;&#23545;&#26799;&#24230;&#25915;&#20987;&#30340;&#26174;&#33879;&#38887;&#24615;&#65292;&#24182;&#36890;&#36807;&#23558;&#27169;&#22411;&#32452;&#21512;&#25104;&#38543;&#26426;&#38598;&#21512;&#65292;&#26377;&#25928;&#23545;&#25239;&#40657;&#30418;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#27979;&#35797;&#20102;&#36890;&#36807;&#22312;&#19968;&#20010;&#20923;&#32467;&#30340;&#20998;&#31867;&#22120;&#20043;&#21069;&#22686;&#21152;&#19968;&#20010;&#21487;&#24494;&#19988;&#23436;&#20840;&#21367;&#31215;&#30340;&#27169;&#22411;&#65292;&#24182;&#20855;&#26377;&#36339;&#36291;&#36830;&#25509;&#30340;&#21069;&#31471;&#22686;&#24378;&#31070;&#32463;&#27169;&#22411;&#12290;&#36890;&#36807;&#20351;&#29992;&#36739;&#23567;&#30340;&#23398;&#20064;&#29575;&#36827;&#34892;&#22823;&#32422;&#19968;&#20010;epoch&#30340;&#35757;&#32451;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#19968;&#20123;&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#20445;&#25345;&#39592;&#24178;&#20998;&#31867;&#22120;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#65292;&#23545;&#21253;&#25324;AutoAttack&#36719;&#20214;&#21253;&#20013;&#30340;APGD&#21644;FAB-T&#25915;&#20987;&#22312;&#20869;&#30340;&#26799;&#24230;&#25915;&#20987;&#20855;&#26377;&#24322;&#24120;&#30340;&#25269;&#25239;&#21147;&#65292;&#36825;&#24402;&#22240;&#20110;&#26799;&#24230;&#25513;&#30422;&#12290;&#26799;&#24230;&#25513;&#30422;&#29616;&#35937;&#24182;&#19981;&#26032;&#40092;&#65292;&#20294;&#23545;&#20110;&#36825;&#20123;&#27809;&#26377;&#26799;&#24230;&#30772;&#22351;&#37096;&#20998;&#65288;&#22914;JPEG&#21387;&#32553;&#25110;&#39044;&#35745;&#23548;&#33268;&#26799;&#24230;&#20943;&#23567;&#30340;&#37096;&#20998;&#65289;&#30340;&#23436;&#20840;&#21487;&#24494;&#27169;&#22411;&#26469;&#35828;&#65292;&#25513;&#30422;&#30340;&#31243;&#24230;&#30456;&#24403;&#26174;&#33879;&#12290;&#23613;&#31649;&#40657;&#30418;&#25915;&#20987;&#23545;&#26799;&#24230;&#25513;&#30422;&#21487;&#33021;&#37096;&#20998;&#26377;&#25928;&#65292;&#20294;&#36890;&#36807;&#23558;&#27169;&#22411;&#32452;&#21512;&#25104;&#38543;&#26426;&#38598;&#21512;&#65292;&#21487;&#20197;&#36731;&#26494;&#20987;&#36133;&#23427;&#20204;&#12290;&#25105;&#20204;&#20272;&#35745;&#36825;&#26679;&#30340;&#38598;&#21512;&#22312;CIFAR10&#21644;CIF&#31561;&#19978;&#23454;&#29616;&#20102;&#20960;&#20046;SOTA&#32423;&#21035;&#30340;AutoAttack&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17018v1 Announce Type: cross  Abstract: We tested front-end enhanced neural models where a frozen classifier was prepended by a differentiable and fully convolutional model with a skip connection. By training them using a small learning rate for about one epoch, we obtained models that retained the accuracy of the backbone classifier while being unusually resistant to gradient attacks including APGD and FAB-T attacks from the AutoAttack package, which we attributed to gradient masking. The gradient masking phenomenon is not new, but the degree of masking was quite remarkable for fully differentiable models that did not have gradient-shattering components such as JPEG compression or components that are expected to cause diminishing gradients.   Though black box attacks can be partially effective against gradient masking, they are easily defeated by combining models into randomized ensembles. We estimate that such ensembles achieve near-SOTA AutoAttack accuracy on CIFAR10, CIF
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#29420;&#29305;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#30446;&#26631;&#65292;&#30740;&#31350;&#32773;&#35774;&#35745;&#20102;&#29992;&#20110;&#25903;&#25345;&#33521;&#35821;&#21644;&#20854;&#20182;&#30446;&#26631;&#35821;&#35328;&#30340;&#26368;&#20808;&#36827;&#30340;&#21452;&#35821;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#27169;&#22411;&#22312;STS&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#21516;&#26102;&#22312;&#30446;&#26631;&#35821;&#35328;&#29702;&#35299;&#21644;&#36328;&#35821;&#35328;&#35780;&#20272;&#20219;&#21153;&#20013;&#36229;&#36234;&#20102;&#29616;&#26377;&#22810;&#35821;&#35328;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.17016</link><description>&lt;p&gt;
&#22810;&#20219;&#21153;&#23545;&#27604;&#23398;&#20064;&#29992;&#20110;8192&#26631;&#35760;&#30340;&#21452;&#35821;&#25991;&#26412;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Multi-Task Contrastive Learning for 8192-Token Bilingual Text Embeddings
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17016
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#29420;&#29305;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#30446;&#26631;&#65292;&#30740;&#31350;&#32773;&#35774;&#35745;&#20102;&#29992;&#20110;&#25903;&#25345;&#33521;&#35821;&#21644;&#20854;&#20182;&#30446;&#26631;&#35821;&#35328;&#30340;&#26368;&#20808;&#36827;&#30340;&#21452;&#35821;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#27169;&#22411;&#22312;STS&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#21516;&#26102;&#22312;&#30446;&#26631;&#35821;&#35328;&#29702;&#35299;&#21644;&#36328;&#35821;&#35328;&#35780;&#20272;&#20219;&#21153;&#20013;&#36229;&#36234;&#20102;&#29616;&#26377;&#22810;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#22871;&#26032;&#39062;&#30340;&#26368;&#20808;&#36827;&#30340;&#21452;&#35821;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#65292;&#26088;&#22312;&#25903;&#25345;&#33521;&#35821;&#21644;&#21478;&#19968;&#31181;&#30446;&#26631;&#35821;&#35328;&#12290;&#36825;&#20123;&#27169;&#22411;&#33021;&#22815;&#22788;&#29702;&#38271;&#36798;8192&#20010;&#26631;&#35760;&#30340;&#25991;&#26412;&#36755;&#20837;&#65292;&#22240;&#27492;&#38750;&#24120;&#36866;&#29992;&#20110;&#19968;&#31995;&#21015;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#65292;&#22914;&#25991;&#26412;&#26816;&#32034;&#12289;&#32858;&#31867;&#21644;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24230;&#65288;STS&#65289;&#35745;&#31639;&#12290;&#36890;&#36807;&#19987;&#27880;&#20110;&#21452;&#35821;&#27169;&#22411;&#24182;&#24341;&#20837;&#29420;&#29305;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#30446;&#26631;&#65292;&#25105;&#20204;&#26174;&#33879;&#25913;&#21892;&#20102;&#22312;STS&#20219;&#21153;&#19978;&#30340;&#27169;&#22411;&#34920;&#29616;&#65292;&#36229;&#36807;&#20102;&#29616;&#26377;&#22810;&#35821;&#35328;&#27169;&#22411;&#22312;&#30446;&#26631;&#35821;&#35328;&#29702;&#35299;&#21644;&#36328;&#35821;&#35328;&#35780;&#20272;&#20219;&#21153;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#21452;&#35821;&#27169;&#22411;&#26356;&#21152;&#39640;&#25928;&#65292;&#38656;&#35201;&#36739;&#23569;&#30340;&#21442;&#25968;&#21644;&#26356;&#23569;&#30340;&#20869;&#23384;&#65292;&#22240;&#20026;&#23427;&#20204;&#38656;&#35201;&#36739;&#23567;&#30340;&#35789;&#27719;&#37327;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25193;&#23637;&#20102;&#22823;&#35268;&#27169;&#25991;&#26412;&#23884;&#20837;&#22522;&#20934;&#65288;MTEB&#65289;&#65292;&#21253;&#25324;&#24503;&#35821;&#21644;&#35199;&#29677;&#29273;&#35821;&#23884;&#20837;&#30340;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17016v1 Announce Type: cross  Abstract: We introduce a novel suite of state-of-the-art bilingual text embedding models that are designed to support English and another target language. These models are capable of processing lengthy text inputs with up to 8192 tokens, making them highly versatile for a range of natural language processing tasks such as text retrieval, clustering, and semantic textual similarity (STS) calculations.   By focusing on bilingual models and introducing a unique multi-task learning objective, we have significantly improved the model performance on STS tasks, which outperforms the capabilities of existing multilingual models in both target language understanding and cross-lingual evaluation tasks. Moreover, our bilingual models are more efficient, requiring fewer parameters and less memory due to their smaller vocabulary needs. Furthermore, we have expanded the Massive Text Embedding Benchmark (MTEB) to include benchmarks for German and Spanish embed
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#28145;&#20837;&#25506;&#35752;&#20102;&#22312;&#29790;&#22763;&#21496;&#27861;&#39044;&#27979;&#20013;&#23454;&#29616;&#21487;&#35299;&#37322;&#24615;&#21644;&#20844;&#24179;&#24615;&#30340;&#37325;&#35201;&#24615;&#65292;&#21033;&#29992;&#20102;&#21807;&#19968;&#21487;&#29992;&#30340;&#22810;&#35821;&#35328;LJP&#25968;&#25454;&#38598;&#65292;&#24182;&#23545;&#26368;&#26032;&#30340;&#21333;&#35821;&#21644;&#22810;&#35821;BERT-based LJP&#27169;&#22411;&#36827;&#34892;&#20102;&#21487;&#35299;&#37322;&#24615;&#33021;&#35780;&#20272;&#12290;</title><link>https://arxiv.org/abs/2402.17013</link><description>&lt;p&gt;
&#22312;&#29790;&#22763;&#21496;&#27861;&#39044;&#27979;&#20013;&#23454;&#29616;&#21487;&#35299;&#37322;&#24615;&#21644;&#20844;&#24179;&#24615;&#65306;&#22312;&#19968;&#20010;&#22810;&#35821;&#35328;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Towards Explainability and Fairness in Swiss Judgement Prediction: Benchmarking on a Multilingual Dataset
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17013
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#28145;&#20837;&#25506;&#35752;&#20102;&#22312;&#29790;&#22763;&#21496;&#27861;&#39044;&#27979;&#20013;&#23454;&#29616;&#21487;&#35299;&#37322;&#24615;&#21644;&#20844;&#24179;&#24615;&#30340;&#37325;&#35201;&#24615;&#65292;&#21033;&#29992;&#20102;&#21807;&#19968;&#21487;&#29992;&#30340;&#22810;&#35821;&#35328;LJP&#25968;&#25454;&#38598;&#65292;&#24182;&#23545;&#26368;&#26032;&#30340;&#21333;&#35821;&#21644;&#22810;&#35821;BERT-based LJP&#27169;&#22411;&#36827;&#34892;&#20102;&#21487;&#35299;&#37322;&#24615;&#33021;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17013v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#36328;&#39046;&#22495; &#25688;&#35201;&#65306;&#23545;&#27861;&#24459;&#35009;&#20915;&#39044;&#27979;&#65288;LJP&#65289;&#31995;&#32479;&#20013;&#30340;&#21487;&#35299;&#37322;&#24615;&#36827;&#34892;&#35780;&#20272;&#22312;&#26500;&#24314;&#20540;&#24471;&#20449;&#36182;&#21644;&#36879;&#26126;&#31995;&#32479;&#26041;&#38754;&#33267;&#20851;&#37325;&#35201;&#65292;&#29305;&#21035;&#26159;&#32771;&#34385;&#21040;&#36825;&#20123;&#31995;&#32479;&#20381;&#36182;&#21487;&#33021;&#32570;&#20047;&#27861;&#24459;&#30456;&#20851;&#24615;&#25110;&#28041;&#21450;&#25935;&#24863;&#23646;&#24615;&#30340;&#22240;&#32032;&#12290;&#26412;&#30740;&#31350;&#28145;&#20837;&#25506;&#35752;&#20102;LJP&#27169;&#22411;&#20013;&#21487;&#35299;&#37322;&#24615;&#21644;&#20844;&#24179;&#24615;&#30340;&#39046;&#22495;&#65292;&#21033;&#29992;&#29790;&#22763;&#35009;&#20915;&#39044;&#27979;&#65288;SJP&#65289;&#36825;&#19968;&#21807;&#19968;&#21487;&#29992;&#30340;&#22810;&#35821;&#35328;LJP&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#25972;&#29702;&#20102;&#19968;&#20010;&#21253;&#25324;108&#20010;&#26696;&#20363;&#30340;&#25903;&#25345;&#21644;&#21453;&#23545;&#27861;&#24459;&#19987;&#23478;&#35009;&#20915;&#30340;&#29702;&#30001;&#30340;&#20840;&#38754;&#25910;&#38598;&#65292;&#22312;&#24503;&#35821;&#12289;&#27861;&#35821;&#21644;&#24847;&#22823;&#21033;&#35821;&#20013;&#25552;&#20379;&#12290;&#36890;&#36807;&#37319;&#29992;&#22522;&#20110;&#36974;&#25377;&#30340;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#26368;&#26032;&#30340;&#21333;&#35821;&#21644;&#22810;&#35821;BERT-based LJP&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#34920;&#29616;&#65292;&#20197;&#21450;&#21033;&#29992;&#25968;&#25454;&#22686;&#24378;&#21644;&#36328;&#35821;&#35328;&#36716;&#31227;&#31561;&#25216;&#26415;&#24320;&#21457;&#30340;&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#23637;&#31034;&#20102;&#39044;&#27979;&#24615;&#33021;&#30340;&#25552;&#39640;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17013v1 Announce Type: cross  Abstract: The assessment of explainability in Legal Judgement Prediction (LJP) systems is of paramount importance in building trustworthy and transparent systems, particularly considering the reliance of these systems on factors that may lack legal relevance or involve sensitive attributes. This study delves into the realm of explainability and fairness in LJP models, utilizing Swiss Judgement Prediction (SJP), the only available multilingual LJP dataset. We curate a comprehensive collection of rationales that `support' and `oppose' judgement from legal experts for 108 cases in German, French, and Italian. By employing an occlusion-based explainability approach, we evaluate the explainability performance of state-of-the-art monolingual and multilingual BERT-based LJP models, as well as models developed with techniques such as data augmentation and cross-lingual transfer, which demonstrated prediction performance improvement. Notably, our finding
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#24320;&#28304;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#20102;&#38544;&#31169;&#25915;&#20987;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#39318;&#20010;&#33021;&#21516;&#26102;&#23454;&#29616;&#39640;&#30495;&#27491;&#29575;&#21644;&#20302;&#35823;&#20998;&#31867;&#29575;&#30340;&#39044;&#35757;&#32451;LLMs&#20250;&#21592;&#25512;&#29702;&#25915;&#20987;&#65288;MIAs&#65289;&#65292;&#20197;&#21450;&#23637;&#31034;&#20102;&#22312;&#33258;&#28982;&#29615;&#22659;&#20013;&#21487;&#20197;&#20174;&#24494;&#35843;LLM&#20013;&#25552;&#21462;&#36229;&#36807;50%&#30340;&#24494;&#35843;&#25968;&#25454;&#38598;&#12290;</title><link>https://arxiv.org/abs/2402.17012</link><description>&lt;p&gt;
Pandora's White-Box&#65306;&#24320;&#25918;LLMs&#20013;&#35757;&#32451;&#25968;&#25454;&#27844;&#28431;&#30340;&#22686;&#21152;
&lt;/p&gt;
&lt;p&gt;
Pandora's White-Box: Increased Training Data Leakage in Open LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17012
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#24320;&#28304;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#20102;&#38544;&#31169;&#25915;&#20987;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#39318;&#20010;&#33021;&#21516;&#26102;&#23454;&#29616;&#39640;&#30495;&#27491;&#29575;&#21644;&#20302;&#35823;&#20998;&#31867;&#29575;&#30340;&#39044;&#35757;&#32451;LLMs&#20250;&#21592;&#25512;&#29702;&#25915;&#20987;&#65288;MIAs&#65289;&#65292;&#20197;&#21450;&#23637;&#31034;&#20102;&#22312;&#33258;&#28982;&#29615;&#22659;&#20013;&#21487;&#20197;&#20174;&#24494;&#35843;LLM&#20013;&#25552;&#21462;&#36229;&#36807;50%&#30340;&#24494;&#35843;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;&#24320;&#28304;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36973;&#21463;&#30340;&#38544;&#31169;&#25915;&#20987;&#36827;&#34892;&#20102;&#31995;&#32479;&#30740;&#31350;&#65292;&#20854;&#20013;&#23545;&#25163;&#21487;&#20197;&#35775;&#38382;&#27169;&#22411;&#26435;&#37325;&#12289;&#26799;&#24230;&#25110;&#25439;&#22833;&#65292;&#35797;&#22270;&#21033;&#29992;&#23427;&#20204;&#26469;&#20102;&#35299;&#24213;&#23618;&#35757;&#32451;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#32467;&#26524;&#26159;&#38024;&#23545;&#39044;&#35757;&#32451;LLMs&#30340;&#31532;&#19968;&#20010;&#20250;&#21592;&#25512;&#29702;&#25915;&#20987;&#65288;MIAs&#65289;&#65292;&#33021;&#22815;&#21516;&#26102;&#23454;&#29616;&#39640;TPR&#21644;&#20302;FPR&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#33258;&#28982;&#29615;&#22659;&#20013;&#21487;&#20197;&#20174;&#24494;&#35843;LLM&#20013;&#25552;&#21462;&#36229;&#36807;50%&#30340;&#24494;&#35843;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#23545;&#24213;&#23618;&#27169;&#22411;&#30340;&#19981;&#21516;&#35775;&#38382;&#31243;&#24230;&#12289;&#35821;&#35328;&#27169;&#22411;&#30340;&#23450;&#21046;&#21270;&#20197;&#21450;&#25915;&#20987;&#32773;&#21487;&#20197;&#20351;&#29992;&#30340;&#36164;&#28304;&#12290;&#22312;&#39044;&#35757;&#32451;&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#31181;&#26032;&#30340;&#30333;&#30418;MIAs&#65306;&#22522;&#20110;&#26799;&#24230;&#33539;&#25968;&#30340;&#25915;&#20987;&#12289;&#30417;&#30563;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#21644;&#21333;&#27493;&#25439;&#22833;&#27604;&#25915;&#20987;&#12290;&#25152;&#26377;&#36825;&#20123;&#37117;&#20248;&#20110;&#29616;&#26377;&#30340;&#40657;&#30418;&#22522;&#32447;&#65292;&#24182;&#19988;&#25105;&#20204;&#30340;.....
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17012v1 Announce Type: cross  Abstract: In this paper we undertake a systematic study of privacy attacks against open source Large Language Models (LLMs), where an adversary has access to either the model weights, gradients, or losses, and tries to exploit them to learn something about the underlying training data. Our headline results are the first membership inference attacks (MIAs) against pre-trained LLMs that are able to simultaneously achieve high TPRs and low FPRs, and a pipeline showing that over $50\%$ (!) of the fine-tuning dataset can be extracted from a fine-tuned LLM in natural settings. We consider varying degrees of access to the underlying model, customization of the language model, and resources available to the attacker. In the pre-trained setting, we propose three new white-box MIAs: an attack based on the gradient norm, a supervised neural network classifier, and a single step loss ratio attack. All outperform existing black-box baselines, and our supervi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#21033;&#29992;&#39044;&#35757;&#32451;&#38454;&#27573;&#30340;&#30693;&#35782;&#22238;&#24518;&#21442;&#32771;&#27573;&#33853;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#26694;&#26550;&#27169;&#25311;&#20154;&#31867;&#22238;&#24518;&#21442;&#32771;&#30340;&#36807;&#31243;&#12290;</title><link>https://arxiv.org/abs/2402.17010</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#20687;&#20154;&#31867;&#19968;&#26679;&#22238;&#24518;&#21442;&#32771;&#20301;&#32622;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Large Language Models Recall Reference Location Like Humans?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17010
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#21033;&#29992;&#39044;&#35757;&#32451;&#38454;&#27573;&#30340;&#30693;&#35782;&#22238;&#24518;&#21442;&#32771;&#27573;&#33853;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#26694;&#26550;&#27169;&#25311;&#20154;&#31867;&#22238;&#24518;&#21442;&#32771;&#30340;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23436;&#25104;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#26102;&#65292;&#20154;&#31867;&#26377;&#26102;&#19981;&#20165;&#38656;&#35201;&#19968;&#20010;&#31572;&#26696;&#65292;&#36824;&#38656;&#35201;&#30456;&#24212;&#30340;&#21442;&#32771;&#27573;&#33853;&#20379;&#36741;&#21161;&#38405;&#35835;&#12290;&#20808;&#21069;&#30340;&#26041;&#27861;&#38656;&#35201;&#36890;&#36807;&#39069;&#22806;&#30340;&#26816;&#32034;&#27169;&#22411;&#33719;&#21462;&#39044;&#20998;&#27573;&#30340;&#25991;&#31456;&#22359;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#39044;&#35757;&#32451;&#38454;&#27573;&#23384;&#20648;&#30340;&#21442;&#25968;&#21270;&#30693;&#35782;&#65292;&#29420;&#31435;&#20110;&#20219;&#20309;&#36215;&#22987;&#20301;&#32622;&#22238;&#24518;&#21442;&#32771;&#27573;&#33853;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#25311;&#20154;&#31867;&#22238;&#24518;&#26131;&#34987;&#36951;&#24536;&#21442;&#32771;&#30340;&#24773;&#26223;&#30340;&#20004;&#38454;&#27573;&#26694;&#26550;&#12290;&#39318;&#20808;&#65292;LLM&#34987;&#25552;&#31034;&#22238;&#24518;&#25991;&#26723;&#26631;&#39064;&#26631;&#35782;&#31526;&#20197;&#33719;&#21462;&#31895;&#31890;&#24230;&#25991;&#26723;&#38598;&#12290;&#28982;&#21518;&#65292;&#22522;&#20110;&#33719;&#24471;&#30340;&#31895;&#31890;&#24230;&#25991;&#26723;&#38598;&#65292;&#23427;&#22238;&#24518;&#32454;&#31890;&#24230;&#27573;&#33853;&#12290;&#22312;&#20004;&#38454;&#27573;&#22238;&#24518;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#32422;&#26463;&#35299;&#30721;&#26469;&#30830;&#20445;&#19981;&#29983;&#25104;&#23384;&#20648;&#25991;&#26723;&#20043;&#22806;&#30340;&#20869;&#23481;&#12290;&#20026;&#20102;&#22686;&#21152;&#36895;&#24230;&#65292;&#25105;&#20204;&#21482;&#22238;&#24518;&#30701;&#21069;&#32512;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17010v1 Announce Type: cross  Abstract: When completing knowledge-intensive tasks, humans sometimes need not just an answer but also a corresponding reference passage for auxiliary reading. Previous methods required obtaining pre-segmented article chunks through additional retrieval models. This paper explores leveraging the parameterized knowledge stored during the pre-training phase of large language models (LLMs) to independently recall reference passage from any starting position. We propose a two-stage framework that simulates the scenario of humans recalling easily forgotten references. Initially, the LLM is prompted to recall document title identifiers to obtain a coarse-grained document set. Then, based on the acquired coarse-grained document set, it recalls fine-grained passage. In the two-stage recall process, we use constrained decoding to ensure that content outside of the stored documents is not generated. To increase speed, we only recall a short prefix in the 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#31639;&#27861;&#20934;&#30830;&#24615;&#20316;&#20026;&#22312;&#20020;&#24202;&#35797;&#39564;&#20013;&#37096;&#32626;&#22312;&#32447;RL&#31639;&#27861;&#30340;&#20851;&#38190;&#35201;&#27714;&#65292;&#24378;&#35843;&#20102;&#23545;&#21442;&#19982;&#32773;&#20445;&#25252;&#21644;&#25968;&#25454;&#31185;&#23398;&#25928;&#29992;&#30340;&#20445;&#30041;&#36131;&#20219;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#36827;&#34892;&#39044;&#37096;&#32626;&#35268;&#21010;&#21644;&#23454;&#26102;&#30417;&#27979;&#20197;&#30830;&#20445;&#31639;&#27861;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.17003</link><description>&lt;p&gt;
&#22312;&#20020;&#24202;&#35797;&#39564;&#20013;&#30417;&#27979;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;
Monitoring Fidelity of Online Reinforcement Learning Algorithms in Clinical Trials
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17003
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#31639;&#27861;&#20934;&#30830;&#24615;&#20316;&#20026;&#22312;&#20020;&#24202;&#35797;&#39564;&#20013;&#37096;&#32626;&#22312;&#32447;RL&#31639;&#27861;&#30340;&#20851;&#38190;&#35201;&#27714;&#65292;&#24378;&#35843;&#20102;&#23545;&#21442;&#19982;&#32773;&#20445;&#25252;&#21644;&#25968;&#25454;&#31185;&#23398;&#25928;&#29992;&#30340;&#20445;&#30041;&#36131;&#20219;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#36827;&#34892;&#39044;&#37096;&#32626;&#35268;&#21010;&#21644;&#23454;&#26102;&#30417;&#27979;&#20197;&#30830;&#20445;&#31639;&#27861;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#31639;&#27861;&#20026;&#20010;&#24615;&#21270;&#20020;&#24202;&#35797;&#39564;&#20013;&#21442;&#19982;&#32773;&#30340;&#27835;&#30103;&#25552;&#20379;&#20102;&#24040;&#22823;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#22312;&#39640;&#39118;&#38505;&#21307;&#30103;&#39046;&#22495;&#37096;&#32626;&#22312;&#32447;&#33258;&#20027;&#31639;&#27861;&#20351;&#24471;&#36136;&#37327;&#25511;&#21046;&#21644;&#25968;&#25454;&#36136;&#37327;&#29305;&#21035;&#38590;&#20197;&#23454;&#29616;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#20316;&#20026;&#22312;&#20020;&#24202;&#35797;&#39564;&#20013;&#37096;&#32626;&#22312;&#32447;RL&#31639;&#27861;&#30340;&#20851;&#38190;&#35201;&#27714;&#30340;&#31639;&#27861;&#20934;&#30830;&#24615;&#12290;&#23427;&#24378;&#35843;&#20102;&#31639;&#27861;&#23545;&#65288;1&#65289;&#20445;&#25252;&#21442;&#19982;&#32773;&#21644;&#65288;2&#65289;&#20445;&#30041;&#25968;&#25454;&#22312;&#35797;&#39564;&#21518;&#20998;&#26512;&#20013;&#30340;&#31185;&#23398;&#25928;&#29992;&#30340;&#36131;&#20219;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#37096;&#32626;&#21069;&#35268;&#21010;&#21644;&#23454;&#26102;&#30417;&#27979;&#30340;&#26694;&#26550;&#65292;&#20197;&#21327;&#21161;&#31639;&#27861;&#24320;&#21457;&#32773;&#21644;&#20020;&#24202;&#30740;&#31350;&#20154;&#21592;&#30830;&#20445;&#31639;&#27861;&#30340;&#20934;&#30830;&#24615;&#12290;&#20026;&#20102;&#35828;&#26126;&#25105;&#20204;&#26694;&#26550;&#30340;&#23454;&#38469;&#24212;&#29992;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#26469;&#33258;Oralytics&#20020;&#24202;&#35797;&#39564;&#30340;&#30495;&#23454;&#26696;&#20363;&#12290;&#33258;2023&#24180;&#26149;&#23395;&#20197;&#26469;&#65292;&#36825;&#39033;&#35797;&#39564;&#25104;&#21151;&#22320;&#37096;&#32626;&#20102;&#19968;&#31181;&#33258;&#20027;&#30340;&#22312;&#32447;RL&#31639;&#27861;&#26469;&#36827;&#34892;&#20010;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17003v1 Announce Type: cross  Abstract: Online reinforcement learning (RL) algorithms offer great potential for personalizing treatment for participants in clinical trials. However, deploying an online, autonomous algorithm in the high-stakes healthcare setting makes quality control and data quality especially difficult to achieve. This paper proposes algorithm fidelity as a critical requirement for deploying online RL algorithms in clinical trials. It emphasizes the responsibility of the algorithm to (1) safeguard participants and (2) preserve the scientific utility of the data for post-trial analyses. We also present a framework for pre-deployment planning and real-time monitoring to help algorithm developers and clinical researchers ensure algorithm fidelity. To illustrate our framework's practical application, we present real-world examples from the Oralytics clinical trial. Since Spring 2023, this trial successfully deployed an autonomous, online RL algorithm to persona
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#32447;&#24615;&#25506;&#38024;&#65292;&#23558;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25991;&#26412;&#34920;&#31034;&#21644;&#39044;&#35757;&#32451;&#38899;&#39057;&#27169;&#22411;&#20013;&#30340;&#22768;&#38899;&#34920;&#31034;&#32852;&#31995;&#22312;&#19968;&#36215;&#65292;&#30740;&#31350;&#21457;&#29616;&#23613;&#31649;&#20165;&#22312;&#21407;&#22987;&#25991;&#26412;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#35821;&#35328;&#27169;&#22411;&#23545;&#20110;&#19968;&#20123;&#23545;&#35937;&#30340;&#22768;&#38899;&#30693;&#35782;&#26377;&#30528;&#22522;&#20110;&#23454;&#36136;&#30340;&#32534;&#30721;&#12290;</title><link>https://arxiv.org/abs/2402.16998</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#21548;&#21040;&#20102;&#20160;&#20040;&#65311;&#25506;&#31350;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#21548;&#35273;&#34920;&#24449;
&lt;/p&gt;
&lt;p&gt;
What Do Language Models Hear? Probing for Auditory Representations in Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16998
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#32447;&#24615;&#25506;&#38024;&#65292;&#23558;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25991;&#26412;&#34920;&#31034;&#21644;&#39044;&#35757;&#32451;&#38899;&#39057;&#27169;&#22411;&#20013;&#30340;&#22768;&#38899;&#34920;&#31034;&#32852;&#31995;&#22312;&#19968;&#36215;&#65292;&#30740;&#31350;&#21457;&#29616;&#23613;&#31649;&#20165;&#22312;&#21407;&#22987;&#25991;&#26412;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#35821;&#35328;&#27169;&#22411;&#23545;&#20110;&#19968;&#20123;&#23545;&#35937;&#30340;&#22768;&#38899;&#30693;&#35782;&#26377;&#30528;&#22522;&#20110;&#23454;&#36136;&#30340;&#32534;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25506;&#35752;&#20102;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#23545;&#29289;&#20307;&#30340;&#22768;&#38899;&#20855;&#26377;&#21547;&#20041;&#28145;&#21051;&#19988;&#22522;&#20110;&#23454;&#36136;&#30340;&#34920;&#24449;&#12290;&#25105;&#20204;&#23398;&#20064;&#20102;&#19968;&#20010;&#32447;&#24615;&#25506;&#38024;&#65292;&#36890;&#36807;&#19968;&#20010;&#39044;&#35757;&#32451;&#30340;&#38899;&#39057;&#27169;&#22411;&#32473;&#20986;&#19968;&#20010;&#23545;&#35937;&#30340;&#22768;&#38899;&#34920;&#31034;&#65292;&#20174;&#32780;&#22312;&#32473;&#23450;&#19982;&#35813;&#23545;&#35937;&#30456;&#20851;&#30340;&#38899;&#39057;&#29255;&#27573;&#30340;&#24773;&#20917;&#19979;&#26816;&#32034;&#20986;&#35813;&#23545;&#35937;&#30340;&#27491;&#30830;&#25991;&#26412;&#34920;&#31034;&#12290;&#36825;&#20010;&#25506;&#38024;&#26159;&#36890;&#36807;&#23545;&#27604;&#25439;&#22833;&#36827;&#34892;&#35757;&#32451;&#30340;&#65292;&#25512;&#21160;&#23545;&#35937;&#30340;&#35821;&#35328;&#34920;&#31034;&#21644;&#22768;&#38899;&#34920;&#31034;&#24444;&#27492;&#25509;&#36817;&#12290;&#22312;&#35757;&#32451;&#20043;&#21518;&#65292;&#25105;&#20204;&#27979;&#35797;&#20102;&#25506;&#38024;&#23545;&#20110;&#19968;&#20123;&#22312;&#35757;&#32451;&#20013;&#27809;&#26377;&#35265;&#36807;&#30340;&#23545;&#35937;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#22312;&#19981;&#21516;&#30340;&#35821;&#35328;&#27169;&#22411;&#21644;&#38899;&#39057;&#27169;&#22411;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#25506;&#38024;&#30340;&#27867;&#21270;&#33021;&#21147;&#36229;&#36807;&#20102;&#38543;&#26426;&#29468;&#27979;&#30340;&#27700;&#24179;&#65292;&#36825;&#34920;&#26126;&#23613;&#31649;&#20165;&#22312;&#21407;&#22987;&#25991;&#26412;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#35821;&#35328;&#27169;&#22411;&#23545;&#20110;&#19968;&#20123;&#23545;&#35937;&#30340;&#22768;&#38899;&#30693;&#35782;&#20855;&#26377;&#22522;&#20110;&#23454;&#36136;&#30340;&#32534;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16998v1 Announce Type: cross  Abstract: This work explores whether language models encode meaningfully grounded representations of sounds of objects. We learn a linear probe that retrieves the correct text representation of an object given a snippet of audio related to that object, where the sound representation is given by a pretrained audio model. This probe is trained via a contrastive loss that pushes the language representations and sound representations of an object to be close to one another. After training, the probe is tested on its ability to generalize to objects that were not seen during training. Across different language models and audio models, we find that the probe generalization is above chance in many cases, indicating that despite being trained only on raw text, language models encode grounded knowledge of sounds for some objects.
&lt;/p&gt;</description></item><item><title>GEM3D&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#12289;&#25299;&#25169;&#24863;&#30693;&#30340;&#19977;&#32500;&#24418;&#29366;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#31070;&#32463;&#39592;&#26550;&#32534;&#30721;&#20102;&#25299;&#25169;&#21644;&#20960;&#20309;&#20449;&#24687;&#65292;&#36890;&#36807;&#39592;&#26550;&#39537;&#21160;&#30340;&#31070;&#32463;&#38544;&#24335;&#20844;&#24335;&#29983;&#25104;&#20934;&#30830;&#21644;&#22810;&#26679;&#21270;&#30340;&#34920;&#38754;&#12290;</title><link>https://arxiv.org/abs/2402.16994</link><description>&lt;p&gt;
GEM3D&#65306;&#19977;&#32500;&#24418;&#29366;&#21512;&#25104;&#30340;&#29983;&#25104;&#23186;&#20307;&#25277;&#35937;
&lt;/p&gt;
&lt;p&gt;
GEM3D: GEnerative Medial Abstractions for 3D Shape Synthesis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16994
&lt;/p&gt;
&lt;p&gt;
GEM3D&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#12289;&#25299;&#25169;&#24863;&#30693;&#30340;&#19977;&#32500;&#24418;&#29366;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#31070;&#32463;&#39592;&#26550;&#32534;&#30721;&#20102;&#25299;&#25169;&#21644;&#20960;&#20309;&#20449;&#24687;&#65292;&#36890;&#36807;&#39592;&#26550;&#39537;&#21160;&#30340;&#31070;&#32463;&#38544;&#24335;&#20844;&#24335;&#29983;&#25104;&#20934;&#30830;&#21644;&#22810;&#26679;&#21270;&#30340;&#34920;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;GEM3D&#8212;&#8212;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#12289;&#25299;&#25169;&#24863;&#30693;&#30340;&#19977;&#32500;&#24418;&#29366;&#29983;&#25104;&#27169;&#22411;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#20851;&#38190;&#37096;&#20998;&#26159;&#22522;&#20110;&#31070;&#32463;&#39592;&#26550;&#30340;&#34920;&#31034;&#65292;&#32534;&#30721;&#20102;&#20851;&#20110;&#24418;&#29366;&#25299;&#25169;&#21644;&#20960;&#20309;&#30340;&#20449;&#24687;&#12290;&#36890;&#36807;&#19968;&#20010;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#39318;&#20808;&#29983;&#25104;&#36981;&#24490;&#20013;&#36724;&#21464;&#25442;&#65288;MAT&#65289;&#30340;&#22522;&#20110;&#39592;&#26550;&#30340;&#34920;&#31034;&#65292;&#28982;&#21518;&#36890;&#36807;&#19968;&#20010;&#39592;&#26550;&#39537;&#21160;&#30340;&#31070;&#32463;&#38544;&#24335;&#20844;&#24335;&#29983;&#25104;&#34920;&#38754;&#12290;&#31070;&#32463;&#38544;&#24335;&#32771;&#34385;&#20102;&#22312;&#29983;&#25104;&#30340;&#39592;&#26550;&#34920;&#31034;&#20013;&#23384;&#20648;&#30340;&#25299;&#25169;&#21644;&#20960;&#20309;&#20449;&#24687;&#65292;&#20135;&#29983;&#30340;&#34920;&#38754;&#19982;&#20043;&#21069;&#30340;&#31070;&#32463;&#22330;&#20844;&#24335;&#30456;&#27604;&#26356;&#21152;&#25299;&#25169;&#21644;&#20960;&#20309;&#20934;&#30830;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#24418;&#29366;&#21512;&#25104;&#21644;&#28857;&#20113;&#37325;&#24314;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#23545;&#25105;&#20204;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#23450;&#24615;&#21644;&#23450;&#37327;&#35780;&#20272;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#27604;&#20197;&#21069;&#26356;&#20026;&#20934;&#30830;&#21644;&#22810;&#26679;&#21270;&#30340;&#24418;&#29366;&#37325;&#24314;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16994v1 Announce Type: cross  Abstract: We introduce GEM3D -- a new deep, topology-aware generative model of 3D shapes. The key ingredient of our method is a neural skeleton-based representation encoding information on both shape topology and geometry. Through a denoising diffusion probabilistic model, our method first generates skeleton-based representations following the Medial Axis Transform (MAT), then generates surfaces through a skeleton-driven neural implicit formulation. The neural implicit takes into account the topological and geometric information stored in the generated skeleton representations to yield surfaces that are more topologically and geometrically accurate compared to previous neural field formulations. We discuss applications of our method in shape synthesis and point cloud reconstruction tasks, and evaluate our method both qualitatively and quantitatively. We demonstrate significantly more faithful surface reconstruction and diverse shape generation r
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26816;&#27979;&#28508;&#22312;&#24187;&#35273;&#24182;&#24314;&#35758;&#26367;&#20195;&#26041;&#26696;&#30340;&#36890;&#20449;&#26426;&#21046;&#65292;&#25104;&#21151;&#20943;&#23569;&#20154;&#31867;&#23548;&#33322;&#38169;&#35823;&#39640;&#36798;29%&#32780;&#19981;&#22686;&#21152;&#35748;&#30693;&#36127;&#25285;</title><link>https://arxiv.org/abs/2402.16973</link><description>&lt;p&gt;
&#36890;&#36807;&#31361;&#20986;&#28508;&#22312;&#38169;&#35823;&#24182;&#24314;&#35758;&#32416;&#27491;&#25104;&#21151;&#24341;&#23548;&#20154;&#31867;&#20570;&#20986;&#20915;&#31574;&#30340;&#19981;&#23436;&#32654;&#35828;&#26126;
&lt;/p&gt;
&lt;p&gt;
Successfully Guiding Humans with Imperfect Instructions by Highlighting Potential Errors and Suggesting Corrections
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16973
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26816;&#27979;&#28508;&#22312;&#24187;&#35273;&#24182;&#24314;&#35758;&#26367;&#20195;&#26041;&#26696;&#30340;&#36890;&#20449;&#26426;&#21046;&#65292;&#25104;&#21151;&#20943;&#23569;&#20154;&#31867;&#23548;&#33322;&#38169;&#35823;&#39640;&#36798;29%&#32780;&#19981;&#22686;&#21152;&#35748;&#30693;&#36127;&#25285;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#21033;&#29992;&#19981;&#23436;&#32654;&#35821;&#35328;&#27169;&#22411;&#26469;&#22312;&#22522;&#20110;&#23450;&#20301;&#23548;&#33322;&#20219;&#21153;&#30340;&#32972;&#26223;&#19979;&#24341;&#23548;&#20154;&#31867;&#20915;&#31574;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19981;&#23436;&#32654;&#30340;&#35828;&#26126;&#29983;&#25104;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#26377;&#25928;&#30340;&#36890;&#20449;&#26426;&#21046;&#26469;&#26356;&#25104;&#21151;&#22320;&#24341;&#23548;&#20154;&#31867;&#12290;&#25105;&#20204;&#26500;&#24314;&#30340;&#36890;&#20449;&#26426;&#21046;&#21253;&#25324;&#21487;&#20197;&#26816;&#27979;&#35828;&#26126;&#20013;&#28508;&#22312;&#24187;&#35273;&#24182;&#24314;&#35758;&#23454;&#38469;&#26367;&#20195;&#26041;&#26696;&#30340;&#27169;&#22411;&#65292;&#20197;&#21450;&#19968;&#20010;&#30452;&#35266;&#30340;&#30028;&#38754;&#23558;&#35813;&#20449;&#24687;&#21576;&#29616;&#32473;&#29992;&#25143;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#23558;&#20154;&#31867;&#23548;&#33322;&#38169;&#35823;&#38477;&#20302;&#39640;&#36798;29%&#65292;&#32780;&#19981;&#22686;&#21152;&#39069;&#22806;&#30340;&#35748;&#30693;&#36127;&#25285;&#12290;&#36825;&#19968;&#32467;&#26524;&#31361;&#26174;&#20102;&#23558;&#22810;&#26679;&#21270;&#30340;&#36890;&#20449;&#28192;&#36947;&#25972;&#21512;&#21040;AI&#31995;&#32479;&#20013;&#26469;&#24357;&#34917;&#20854;&#32570;&#38519;&#24182;&#22686;&#24378;&#20854;&#23545;&#20154;&#31867;&#30340;&#23454;&#29992;&#24615;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16973v1 Announce Type: new  Abstract: This paper addresses the challenge of leveraging imperfect language models to guide human decision-making in the context of a grounded navigation task. We show that an imperfect instruction generation model can be complemented with an effective communication mechanism to become more successful at guiding humans. The communication mechanism we build comprises models that can detect potential hallucinations in instructions and suggest practical alternatives, and an intuitive interface to present that information to users. We show that this approach reduces the human navigation error by up to 29% with no additional cognitive burden. This result underscores the potential of integrating diverse communication channels into AI systems to compensate for their imperfections and enhance their utility for humans.
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#32593;&#32476;&#23433;&#20840;&#39046;&#22495;&#30340;&#24212;&#29992;&#21450;&#20854;&#23616;&#38480;&#24615;&#30340;&#35843;&#26597;&#21644;&#23637;&#26395;&#12290;</title><link>https://arxiv.org/abs/2402.16968</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#32593;&#32476;&#23433;&#20840;&#20013;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
A Survey of Large Language Models in Cybersecurity
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16968
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#32593;&#32476;&#23433;&#20840;&#39046;&#22495;&#30340;&#24212;&#29992;&#21450;&#20854;&#23616;&#38480;&#24615;&#30340;&#35843;&#26597;&#21644;&#23637;&#26395;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#36805;&#36895;&#23835;&#36215;&#65292;&#22240;&#20854;&#33021;&#22815;&#22312;&#22810;&#20010;&#39046;&#22495;&#20197;&#25509;&#36817;&#25110;&#36798;&#21040;&#26368;&#20808;&#36827;&#27700;&#24179;&#30340;&#34920;&#29616;&#65292;&#24182;&#22788;&#29702;&#33258;&#28982;&#35821;&#35328;&#12290;&#30740;&#31350;&#30340;&#19968;&#20010;&#37325;&#35201;&#39046;&#22495;&#26159;&#23558;&#36825;&#20123;&#27169;&#22411;&#24212;&#29992;&#20110;&#32593;&#32476;&#23433;&#20840;&#39046;&#22495;&#12290;&#26412;&#35843;&#26597;&#26088;&#22312;&#30830;&#23450;&#22312;&#32593;&#32476;&#23433;&#20840;&#39046;&#22495;LLMs&#24050;&#32463;&#34987;&#24212;&#29992;&#30340;&#22320;&#26041;&#65292;&#23427;&#20204;&#34987;&#29992;&#20110;&#30340;&#26041;&#24335;&#20197;&#21450;&#23427;&#20204;&#22312;&#35813;&#39046;&#22495;&#20013;&#30340;&#23616;&#38480;&#24615;&#12290;&#26368;&#21518;&#65292;&#25552;&#20986;&#20102;&#22914;&#20309;&#25913;&#21892;&#36825;&#20123;&#23616;&#38480;&#24615;&#20197;&#21450;&#22312;&#20811;&#26381;&#36825;&#20123;&#23616;&#38480;&#24615;&#21518;&#21487;&#20197;&#20174;&#36825;&#20123;&#31995;&#32479;&#20013;&#26399;&#24453;&#20160;&#20040;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16968v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have quickly risen to prominence due to their ability to perform at or close to the state-of-the-art in a variety of fields while handling natural language. An important field of research is the application of such models at the cybersecurity context. This survey aims to identify where in the field of cybersecurity LLMs have already been applied, the ways in which they are being used and their limitations in the field. Finally, suggestions are made on how to improve such limitations and what can be expected from these systems once these limitations are overcome.
&lt;/p&gt;</description></item><item><title>WIPI&#26159;&#19968;&#31181;&#26032;&#22411;&#23041;&#32961;&#65292;&#21487;&#20197;&#38388;&#25509;&#25511;&#21046;Web&#20195;&#29702;&#25191;&#34892;&#24694;&#24847;&#25351;&#20196;&#65292;&#20174;&#32780;&#25552;&#39640;&#25915;&#20987;&#30340;&#25928;&#29575;&#21644;&#38544;&#34109;&#24615;</title><link>https://arxiv.org/abs/2402.16965</link><description>&lt;p&gt;
WIPI: &#19968;&#31181;&#26032;&#30340;&#29992;&#20110;LLM&#39537;&#21160;Web&#20195;&#29702;&#30340;&#32593;&#32476;&#23041;&#32961;
&lt;/p&gt;
&lt;p&gt;
WIPI: A New Web Threat for LLM-Driven Web Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16965
&lt;/p&gt;
&lt;p&gt;
WIPI&#26159;&#19968;&#31181;&#26032;&#22411;&#23041;&#32961;&#65292;&#21487;&#20197;&#38388;&#25509;&#25511;&#21046;Web&#20195;&#29702;&#25191;&#34892;&#24694;&#24847;&#25351;&#20196;&#65292;&#20174;&#32780;&#25552;&#39640;&#25915;&#20987;&#30340;&#25928;&#29575;&#21644;&#38544;&#34109;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;LLM&#39537;&#21160;&#30340;Web&#20195;&#29702;&#65288;&#31616;&#31216;Web&#20195;&#29702;&#65289;&#30001;&#20110;&#20854;&#20248;&#36234;&#30340;&#33021;&#21147;&#32780;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#65292;&#20854;&#20013;LLMs&#20316;&#20026;&#20915;&#31574;&#30340;&#26680;&#24515;&#37096;&#20998;&#65292;&#31867;&#20284;&#20110;&#20154;&#33041;&#65292;&#37197;&#22791;&#20102;&#22810;&#20010;&#32593;&#32476;&#24037;&#20855;&#65292;&#21487;&#20197;&#19982;&#22806;&#37096;&#37096;&#32626;&#30340;&#32593;&#31449;&#36827;&#34892;&#31215;&#26497;&#20132;&#20114;&#12290;&#38543;&#30528;&#26080;&#25968;Web&#20195;&#29702;&#30340;&#21457;&#24067;&#65292;&#36825;&#31181;LLM&#31995;&#32479;&#27491;&#22312;&#24555;&#36895;&#21457;&#23637;&#65292;&#24182;&#19988;&#36880;&#28176;&#25509;&#36817;&#22312;&#25105;&#20204;&#26085;&#24120;&#29983;&#27963;&#20013;&#30340;&#24191;&#27867;&#37096;&#32626;&#65292;&#19968;&#20010;&#37325;&#35201;&#32780;&#36843;&#20999;&#30340;&#38382;&#39064;&#20986;&#29616;&#20102;&#65306;&#8220;&#36825;&#20123;Web&#20195;&#29702;&#23433;&#20840;&#21527;&#65311;&#8221;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#22411;&#23041;&#32961;WIPI&#65292;&#23427;&#38388;&#25509;&#25511;&#21046;Web&#20195;&#29702;&#25191;&#34892;&#23884;&#20837;&#22312;&#20844;&#24320;&#21487;&#35775;&#38382;&#32593;&#39029;&#20013;&#30340;&#24694;&#24847;&#25351;&#20196;&#12290;&#20026;&#20102;&#25104;&#21151;&#21551;&#21160;WIPI&#65292;&#38656;&#35201;&#22312;&#40657;&#30418;&#29615;&#22659;&#20013;&#24037;&#20316;&#12290;&#36825;&#31181;&#26041;&#27861;&#20851;&#27880;&#20110;&#22806;&#37096;&#32593;&#39029;&#20013;&#38388;&#25509;&#25351;&#20196;&#30340;&#24418;&#24335;&#21644;&#20869;&#23481;&#65292;&#22686;&#24378;&#20102;&#25915;&#20987;&#30340;&#25928;&#29575;&#21644;&#38544;&#34109;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16965v1 Announce Type: cross  Abstract: With the fast development of large language models (LLMs), LLM-driven Web Agents (Web Agents for short) have obtained tons of attention due to their superior capability where LLMs serve as the core part of making decisions like the human brain equipped with multiple web tools to actively interact with external deployed websites. As uncountable Web Agents have been released and such LLM systems are experiencing rapid development and drawing closer to widespread deployment in our daily lives, an essential and pressing question arises: "Are these Web Agents secure?". In this paper, we introduce a novel threat, WIPI, that indirectly controls Web Agent to execute malicious instructions embedded in publicly accessible webpages. To launch a successful WIPI works in a black-box environment. This methodology focuses on the form and content of indirect instructions within external webpages, enhancing the efficiency and stealthiness of the attack
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;FedReview&#26426;&#21046;&#65292;&#36890;&#36807;&#38543;&#26426;&#20998;&#37197;&#35780;&#23457;&#21592;&#23458;&#25143;&#31471;&#26469;&#35782;&#21035;&#21644;&#25298;&#32477;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#28508;&#22312;&#27602;&#21270;&#26356;&#26032;&#65292;&#24182;&#37319;&#29992;&#22810;&#25968;&#34920;&#20915;&#26426;&#21046;&#26469;&#25972;&#21512;&#25490;&#21517;&#24182;&#31227;&#38500;&#36825;&#20123;&#26356;&#26032;&#12290;</title><link>https://arxiv.org/abs/2402.16934</link><description>&lt;p&gt;
FedReview: &#19968;&#31181;&#29992;&#20110;&#25298;&#32477;&#27602;&#21270;&#26356;&#26032;&#30340;&#32852;&#37030;&#23398;&#20064;&#23457;&#26597;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
FedReview: A Review Mechanism for Rejecting Poisoned Updates in Federated Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16934
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;FedReview&#26426;&#21046;&#65292;&#36890;&#36807;&#38543;&#26426;&#20998;&#37197;&#35780;&#23457;&#21592;&#23458;&#25143;&#31471;&#26469;&#35782;&#21035;&#21644;&#25298;&#32477;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#28508;&#22312;&#27602;&#21270;&#26356;&#26032;&#65292;&#24182;&#37319;&#29992;&#22810;&#25968;&#34920;&#20915;&#26426;&#21046;&#26469;&#25972;&#21512;&#25490;&#21517;&#24182;&#31227;&#38500;&#36825;&#20123;&#26356;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Federated learning&#26368;&#36817;&#24050;&#32463;&#34987;&#25552;&#20986;&#20316;&#20026;&#19968;&#31181;&#21435;&#20013;&#24515;&#21270;&#30340;&#26041;&#27861;&#65292;&#22312;&#19981;&#35775;&#38382;&#29992;&#25143;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#19968;&#20010;&#39640;&#24615;&#33021;&#27169;&#22411;&#12290;&#23613;&#31649;&#20854;&#26377;&#25928;&#24615;&#65292;&#20294;&#32852;&#37030;&#23398;&#20064;&#32473;&#24694;&#24847;&#29992;&#25143;&#25552;&#20379;&#20102;&#26426;&#20250;&#36890;&#36807;&#21521;&#26381;&#21153;&#22120;&#19978;&#20256;&#27602;&#21270;&#27169;&#22411;&#26356;&#26032;&#26469;&#25805;&#32437;&#27169;&#22411;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedReview&#30340;&#23457;&#26597;&#26426;&#21046;&#65292;&#29992;&#20110;&#35782;&#21035;&#21644;&#25298;&#32477;&#32852;&#37030;&#23398;&#20064;&#20013;&#28508;&#22312;&#30340;&#27602;&#21270;&#26356;&#26032;&#12290;&#22312;&#25105;&#20204;&#30340;&#26426;&#21046;&#19979;&#65292;&#26381;&#21153;&#22120;&#27599;&#36718;&#38543;&#26426;&#20998;&#37197;&#23376;&#38598;&#23458;&#25143;&#31471;&#20316;&#20026;&#35780;&#23457;&#21592;&#65292;&#22312;&#20854;&#35757;&#32451;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#27169;&#22411;&#26356;&#26032;&#12290;&#35780;&#23457;&#21592;&#26681;&#25454;&#35780;&#20215;&#32467;&#26524;&#23545;&#27169;&#22411;&#26356;&#26032;&#36827;&#34892;&#25490;&#21517;&#65292;&#32479;&#35745;&#30456;&#23545;&#20302;&#36136;&#37327;&#30340;&#26356;&#26032;&#25968;&#37327;&#20316;&#20026;&#20272;&#35745;&#30340;&#27602;&#21270;&#26356;&#26032;&#25968;&#37327;&#12290;&#22522;&#20110;&#23457;&#26597;&#25253;&#21578;&#65292;&#26381;&#21153;&#22120;&#37319;&#29992;&#22810;&#25968;&#34920;&#20915;&#26426;&#21046;&#25972;&#21512;&#25490;&#21517;&#24182;&#22312;&#27169;&#22411;&#32858;&#21512;&#36807;&#31243;&#20013;&#21435;&#38500;&#28508;&#22312;&#30340;&#27602;&#21270;&#26356;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16934v1 Announce Type: cross  Abstract: Federated learning has recently emerged as a decentralized approach to learn a high-performance model without access to user data. Despite its effectiveness, federated learning gives malicious users opportunities to manipulate the model by uploading poisoned model updates to the server. In this paper, we propose a review mechanism called FedReview to identify and decline the potential poisoned updates in federated learning. Under our mechanism, the server randomly assigns a subset of clients as reviewers to evaluate the model updates on their training datasets in each round. The reviewers rank the model updates based on the evaluation results and count the number of the updates with relatively low quality as the estimated number of poisoned updates. Based on review reports, the server employs a majority voting mechanism to integrate the rankings and remove the potential poisoned updates in the model aggregation process. Extensive evalu
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Cobweb4V&#30340;&#26032;&#39062;&#35270;&#35273;&#20998;&#31867;&#26041;&#27861;&#65292;&#21033;&#29992;&#20154;&#31867;&#31867;&#20284;&#23398;&#20064;&#31995;&#32479;&#65292;&#36991;&#20813;&#20102;&#28798;&#38590;&#24615;&#36951;&#24536;&#25928;&#24212;&#65292;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#27604;&#65292;&#38656;&#35201;&#26356;&#23569;&#30340;&#25968;&#25454;&#26469;&#23454;&#29616;&#26377;&#25928;&#23398;&#20064;&#25104;&#26524;&#65292;&#24182;&#20445;&#25345;&#31283;&#23450;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.16933</link><description>&lt;p&gt;
&#20351;&#29992;&#20154;&#31867;&#27010;&#24565;&#24418;&#25104;&#36991;&#20813;&#35270;&#35273;&#20998;&#31867;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;
&lt;/p&gt;
&lt;p&gt;
Avoiding Catastrophic Forgetting in Visual Classification Using Human Concept Formation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16933
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Cobweb4V&#30340;&#26032;&#39062;&#35270;&#35273;&#20998;&#31867;&#26041;&#27861;&#65292;&#21033;&#29992;&#20154;&#31867;&#31867;&#20284;&#23398;&#20064;&#31995;&#32479;&#65292;&#36991;&#20813;&#20102;&#28798;&#38590;&#24615;&#36951;&#24536;&#25928;&#24212;&#65292;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#27604;&#65292;&#38656;&#35201;&#26356;&#23569;&#30340;&#25968;&#25454;&#26469;&#23454;&#29616;&#26377;&#25928;&#23398;&#20064;&#25104;&#26524;&#65292;&#24182;&#20445;&#25345;&#31283;&#23450;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#29305;&#21035;&#26159;&#22312;&#35270;&#35273;&#20219;&#21153;&#20013;&#65292;&#28982;&#32780;&#65292;&#24403;&#25353;&#39034;&#24207;&#23398;&#20064;&#26032;&#20219;&#21153;&#26102;&#65292;&#23427;&#20204;&#32463;&#24120;&#38754;&#20020;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;Cobweb4V&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#35270;&#35273;&#20998;&#31867;&#26041;&#27861;&#65292;&#23427;&#22522;&#20110;Cobweb&#65292;&#36825;&#26159;&#19968;&#31181;&#20154;&#31867;&#31867;&#20284;&#30340;&#23398;&#20064;&#31995;&#32479;&#65292;&#21463;&#21040;&#20154;&#31867;&#38543;&#26102;&#38388;&#36880;&#28176;&#23398;&#20064;&#26032;&#27010;&#24565;&#30340;&#21551;&#21457;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#23637;&#31034;&#20102;Cobweb4V&#22312;&#23398;&#20064;&#35270;&#35273;&#27010;&#24565;&#26041;&#38754;&#30340;&#29087;&#32451;&#31243;&#24230;&#65292;&#30456;&#36739;&#20110;&#20256;&#32479;&#26041;&#27861;&#65292;&#38656;&#35201;&#26356;&#23569;&#30340;&#25968;&#25454;&#26469;&#23454;&#29616;&#26377;&#25928;&#30340;&#23398;&#20064;&#25104;&#26524;&#65292;&#38543;&#26102;&#38388;&#20445;&#25345;&#31283;&#23450;&#30340;&#24615;&#33021;&#65292;&#24182;&#23454;&#29616;&#20102;&#20196;&#20154;&#31216;&#36190;&#30340;&#28176;&#36817;&#34892;&#20026;&#65292;&#36991;&#20813;&#20102;&#28798;&#38590;&#24615;&#36951;&#24536;&#25928;&#24212;&#12290;&#36825;&#20123;&#29305;&#24449;&#19982;&#20154;&#31867;&#35748;&#30693;&#20013;&#30340;&#23398;&#20064;&#31574;&#30053;&#19968;&#33268;&#65292;&#23558;Cobweb4V&#23450;&#20301;&#20026;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#30340;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16933v1 Announce Type: cross  Abstract: Deep neural networks have excelled in machine learning, particularly in vision tasks, however, they often suffer from catastrophic forgetting when learning new tasks sequentially. In this work, we propose Cobweb4V, a novel visual classification approach that builds on Cobweb, a human like learning system that is inspired by the way humans incrementally learn new concepts over time. In this research, we conduct a comprehensive evaluation, showcasing the proficiency of Cobweb4V in learning visual concepts, requiring less data to achieve effective learning outcomes compared to traditional methods, maintaining stable performance over time, and achieving commendable asymptotic behavior, without catastrophic forgetting effects. These characteristics align with learning strategies in human cognition, positioning Cobweb4V as a promising alternative to neural network approaches.
&lt;/p&gt;</description></item><item><title>LangGPT&#25552;&#20986;&#20102;&#19968;&#20010;&#21452;&#23618;&#25552;&#31034;&#35774;&#35745;&#26694;&#26550;&#65292;&#20316;&#20026;LLMs&#30340;&#32534;&#31243;&#35821;&#35328;&#65292;&#22823;&#22823;&#22686;&#24378;&#20102;LLMs&#20135;&#29983;&#39640;&#36136;&#37327;&#21709;&#24212;&#30340;&#33021;&#21147;&#65292;&#24182;&#22312;&#24341;&#23548;LLMs&#29983;&#25104;&#39640;&#36136;&#37327;&#25552;&#31034;&#26041;&#38754;&#20855;&#26377;&#26174;&#33879;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.16929</link><description>&lt;p&gt;
LangGPT&#65306;&#37325;&#26032;&#24605;&#32771;&#38754;&#21521;LLMs&#30340;&#32467;&#26500;&#21270;&#21487;&#37325;&#22797;&#20351;&#29992;&#25552;&#31034;&#35774;&#35745;&#26694;&#26550;&#20174;&#32534;&#31243;&#35821;&#35328;&#20986;&#21457;
&lt;/p&gt;
&lt;p&gt;
LangGPT: Rethinking Structured Reusable Prompt Design Framework for LLMs from the Programming Language
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16929
&lt;/p&gt;
&lt;p&gt;
LangGPT&#25552;&#20986;&#20102;&#19968;&#20010;&#21452;&#23618;&#25552;&#31034;&#35774;&#35745;&#26694;&#26550;&#65292;&#20316;&#20026;LLMs&#30340;&#32534;&#31243;&#35821;&#35328;&#65292;&#22823;&#22823;&#22686;&#24378;&#20102;LLMs&#20135;&#29983;&#39640;&#36136;&#37327;&#21709;&#24212;&#30340;&#33021;&#21147;&#65292;&#24182;&#22312;&#24341;&#23548;LLMs&#29983;&#25104;&#39640;&#36136;&#37327;&#25552;&#31034;&#26041;&#38754;&#20855;&#26377;&#26174;&#33879;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
LLMs&#24050;&#32463;&#23637;&#31034;&#20986;&#22312;&#19981;&#21516;&#39046;&#22495;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#26377;&#25928;&#25351;&#23548;LLMs&#21046;&#23450;&#39640;&#36136;&#37327;&#30340;&#25552;&#31034;&#23545;&#20110;&#38750;AI&#19987;&#23478;&#26469;&#35828;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#29616;&#26377;&#30340;&#25552;&#31034;&#24037;&#31243;&#30740;&#31350;&#24314;&#35758;&#20102;&#19968;&#20123;&#30053;&#26174;&#38646;&#30862;&#30340;&#20248;&#21270;&#21407;&#21017;&#21644;&#35774;&#35745;&#65292;&#20197;&#21450;&#20973;&#32463;&#39564;&#20381;&#36182;&#30340;&#25552;&#31034;&#20248;&#21270;&#22120;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#36825;&#20123;&#21162;&#21147;&#32570;&#20047;&#19968;&#20010;&#32467;&#26500;&#21270;&#30340;&#35774;&#35745;&#27169;&#26495;&#65292;&#23548;&#33268;&#23398;&#20064;&#25104;&#26412;&#39640;&#65292;&#37325;&#22797;&#20351;&#29992;&#24615;&#20302;&#12290;&#21463;&#32467;&#26500;&#21270;&#21487;&#37325;&#22797;&#20351;&#29992;&#30340;&#32534;&#31243;&#35821;&#35328;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LangGPT&#65292;&#20316;&#20026;LLMs&#30340;&#32534;&#31243;&#35821;&#35328;&#30340;&#21452;&#23618;&#25552;&#31034;&#35774;&#35745;&#26694;&#26550;&#12290;LangGPT&#20855;&#26377;&#26131;&#20110;&#23398;&#20064;&#30340;&#35268;&#33539;&#32467;&#26500;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#25193;&#23637;&#32467;&#26500;&#20197;&#36827;&#34892;&#36801;&#31227;&#21644;&#37325;&#29992;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#19982;&#22522;&#20934;&#30456;&#27604;&#65292;LangGPT&#26174;&#33879;&#22686;&#24378;&#20102;LLMs&#20135;&#29983;&#39640;&#36136;&#37327;&#21709;&#24212;&#30340;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;LangGPT&#24050;&#34987;&#35777;&#26126;&#22312;&#24341;&#23548;LLMs&#29983;&#25104;&#39640;&#36136;&#37327;&#25552;&#31034;&#26041;&#38754;&#26159;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16929v1 Announce Type: cross  Abstract: LLMs have demonstrated commendable performance across diverse domains. Nevertheless, formulating high-quality prompts to effectively instruct LLMs poses a challenge for non-AI experts. Existing research in prompt engineering suggests somewhat fragmented optimization principles and designs empirically dependent prompt optimizers. Unfortunately, these endeavors lack a structured design template, incurring high learning costs and resulting in low reusability. Inspired by structured reusable programming languages, we propose LangGPT, a dual-layer prompt design framework as the programming language for LLMs. LangGPT has an easy-to-learn normative structure and provides an extended structure for migration and reuse. Experiments illustrate that LangGPT significantly enhances the capacity of LLMs to produce responses of superior quality compared to baselines. Moreover, LangGPT has proven effective in guiding LLMs to generate high-quality promp
&lt;/p&gt;</description></item><item><title>CLAP&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#30417;&#30563;&#23398;&#20064;&#20108;&#36827;&#21046;&#20195;&#30721;&#30340;&#36716;&#31227;&#34920;&#31034;&#65292;&#25552;&#39640;&#20102;&#22312;&#23569;&#26679;&#26412;&#21644;&#38646;&#26679;&#26412;&#24773;&#26223;&#19979;&#30340;&#36801;&#31227;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.16928</link><description>&lt;p&gt;
&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#30417;&#30563;&#23398;&#20064;&#21487;&#36716;&#31227;&#30340;&#20108;&#36827;&#21046;&#20195;&#30721;&#34920;&#31034;&#30340;CLAP
&lt;/p&gt;
&lt;p&gt;
CLAP: Learning Transferable Binary Code Representations with Natural Language Supervision
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16928
&lt;/p&gt;
&lt;p&gt;
CLAP&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#30417;&#30563;&#23398;&#20064;&#20108;&#36827;&#21046;&#20195;&#30721;&#30340;&#36716;&#31227;&#34920;&#31034;&#65292;&#25552;&#39640;&#20102;&#22312;&#23569;&#26679;&#26412;&#21644;&#38646;&#26679;&#26412;&#24773;&#26223;&#19979;&#30340;&#36801;&#31227;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20108;&#36827;&#21046;&#20195;&#30721;&#34920;&#31034;&#23398;&#20064;&#22312;&#20108;&#36827;&#21046;&#20998;&#26512;&#20219;&#21153;&#20013;&#26174;&#31034;&#20986;&#26174;&#33879;&#24615;&#33021;&#12290;&#20294;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#22312;&#36801;&#31227;&#24615;&#19978;&#24448;&#24448;&#36739;&#24046;&#65292;&#29305;&#21035;&#26159;&#22312;&#23569;&#26679;&#26412;&#21644;&#38646;&#26679;&#26412;&#24773;&#26223;&#19979;&#65292;&#20219;&#21153;&#30340;&#35757;&#32451;&#26679;&#26412;&#24456;&#23569;&#25110;&#19981;&#23384;&#22312;&#26102;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CLAP&#65288;&#23545;&#27604;&#35821;&#35328;-&#27719;&#32534;&#39044;&#35757;&#32451;&#65289;&#65292;&#23427;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#30417;&#30563;&#26469;&#23398;&#20064;&#26356;&#22909;&#30340;&#20108;&#36827;&#21046;&#20195;&#30721;&#65288;&#21363;&#27719;&#32534;&#20195;&#30721;&#65289;&#34920;&#31034;&#65292;&#24182;&#33719;&#24471;&#26356;&#22909;&#30340;&#36801;&#31227;&#24615;&#12290;&#20174;&#26680;&#24515;&#19978;&#35762;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#26377;&#25928;&#22320;&#23558;&#20108;&#36827;&#21046;&#20195;&#30721;&#19982;&#23427;&#20204;&#30340;&#35821;&#20041;&#35299;&#37322;&#65288;&#33258;&#28982;&#35821;&#35328;&#20013;&#65289;&#30456;&#23545;&#40784;&#65292;&#25552;&#39640;&#20102;&#21331;&#36234;&#30340;&#36801;&#31227;&#23398;&#20064;&#33021;&#21147;&#65292;&#20174;&#32780;&#20351;&#27169;&#22411;&#33021;&#22815;&#20026;&#20108;&#36827;&#21046;&#20195;&#30721;&#29983;&#25104;&#26356;&#22909;&#30340;&#23884;&#20837;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#31181;&#23545;&#40784;&#35757;&#32451;&#65292;&#25105;&#20204;&#38543;&#21518;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#33258;&#21160;&#29983;&#25104;&#22823;&#37327;&#21644;&#22810;&#26679;&#21270;&#30340;&#25968;&#25454;&#38598;&#30340;&#39640;&#25928;&#25968;&#25454;&#38598;&#24341;&#25806;&#65292;&#21253;&#25324;&#20108;&#36827;&#21046;&#20195;&#30721;&#21644;&#30456;&#24212;&#30340;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16928v1 Announce Type: cross  Abstract: Binary code representation learning has shown significant performance in binary analysis tasks. But existing solutions often have poor transferability, particularly in few-shot and zero-shot scenarios where few or no training samples are available for the tasks. To address this problem, we present CLAP (Contrastive Language-Assembly Pre-training), which employs natural language supervision to learn better representations of binary code (i.e., assembly code) and get better transferability. At the core, our approach boosts superior transfer learning capabilities by effectively aligning binary code with their semantics explanations (in natural language), resulting a model able to generate better embeddings for binary code. To enable this alignment training, we then propose an efficient dataset engine that could automatically generate a large and diverse dataset comprising of binary code and corresponding natural language explanations. We 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#20013;&#21518;&#38376;&#26816;&#27979;&#38382;&#39064;&#30340;&#27491;&#24335;&#32479;&#35745;&#23450;&#20041;&#65292;&#24182;&#24471;&#20986;&#20102;&#21518;&#38376;&#26816;&#27979;&#30340;&#19981;&#21487;&#33021;&#24615;&#19982;&#21487;&#23454;&#29616;&#24615;&#32467;&#26524;&#65292;&#25351;&#20986;&#20102;&#36890;&#29992;&#21518;&#38376;&#26816;&#27979;&#30340;&#23616;&#38480;&#24615;&#65292;&#24378;&#35843;&#21518;&#38376;&#26816;&#27979;&#26041;&#27861;&#38656;&#35201;&#32771;&#34385;&#25932;&#23545;&#22240;&#32032;&#12290;</title><link>https://arxiv.org/abs/2402.16926</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#21518;&#38376;&#26816;&#27979;&#30340;&#21487;&#34892;&#24615;&#38382;&#39064;&#20316;&#20026;&#20551;&#35774;&#26816;&#39564;&#38382;&#39064;&#30340;&#25506;&#35752;
&lt;/p&gt;
&lt;p&gt;
On the (In)feasibility of ML Backdoor Detection as an Hypothesis Testing Problem
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16926
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#20013;&#21518;&#38376;&#26816;&#27979;&#38382;&#39064;&#30340;&#27491;&#24335;&#32479;&#35745;&#23450;&#20041;&#65292;&#24182;&#24471;&#20986;&#20102;&#21518;&#38376;&#26816;&#27979;&#30340;&#19981;&#21487;&#33021;&#24615;&#19982;&#21487;&#23454;&#29616;&#24615;&#32467;&#26524;&#65292;&#25351;&#20986;&#20102;&#36890;&#29992;&#21518;&#38376;&#26816;&#27979;&#30340;&#23616;&#38480;&#24615;&#65292;&#24378;&#35843;&#21518;&#38376;&#26816;&#27979;&#26041;&#27861;&#38656;&#35201;&#32771;&#34385;&#25932;&#23545;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#27491;&#24335;&#30340;&#32479;&#35745;&#23398;&#23450;&#20041;&#65292;&#29992;&#20110;&#20998;&#26512;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#20013;&#30340;&#21518;&#38376;&#26816;&#27979;&#38382;&#39064;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#20351;&#29992;&#23427;&#26469;&#20998;&#26512;&#36825;&#20123;&#38382;&#39064;&#30340;&#21487;&#34892;&#24615;&#65292;&#25552;&#20379;&#20102;&#23545;&#25105;&#20204;&#23450;&#20041;&#30340;&#23454;&#29992;&#24615;&#21644;&#36866;&#29992;&#24615;&#30340;&#35777;&#25454;&#12290;&#36825;&#39033;&#24037;&#20316;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#23545;&#21518;&#38376;&#26816;&#27979;&#30340;&#19981;&#21487;&#33021;&#24615;&#32467;&#26524;&#21644;&#21487;&#23454;&#29616;&#24615;&#32467;&#26524;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20010;&#27809;&#26377;&#20813;&#36153;&#21320;&#39184;&#23450;&#29702;&#65292;&#35777;&#26126;&#20102;&#36890;&#29992;&#65288;&#23545;&#25932;&#26041;&#19981;&#30693;&#24773;&#65289;&#21518;&#38376;&#26816;&#27979;&#26159;&#19981;&#21487;&#33021;&#30340;&#65292;&#38500;&#38750;Alphabet&#22823;&#23567;&#38750;&#24120;&#23567;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#35748;&#20026;&#65292;&#21518;&#38376;&#26816;&#27979;&#26041;&#27861;&#38656;&#35201;&#26126;&#30830;&#22320;&#25110;&#38544;&#24335;&#22320;&#32771;&#34385;&#25932;&#23545;&#22240;&#32032;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#24182;&#19981;&#24847;&#21619;&#30528;&#21518;&#38376;&#26816;&#27979;&#22312;&#29305;&#23450;&#22330;&#26223;&#19979;&#19981;&#33021;&#36215;&#20316;&#29992;&#65292;&#22240;&#20026;&#31185;&#23398;&#25991;&#29486;&#20013;&#25104;&#21151;&#30340;&#21518;&#38376;&#26816;&#27979;&#26041;&#27861;&#35777;&#26126;&#20102;&#36825;&#19968;&#28857;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#23450;&#20041;&#19982;&#22823;&#27010;&#36817;&#20284;&#27491;&#30830;&#65288;PAC&#65289;&#23398;&#20064;&#19982;&#22806;&#20998;&#24067;&#26816;&#27979;&#38382;&#39064;&#32852;&#31995;&#36215;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16926v1 Announce Type: cross  Abstract: We introduce a formal statistical definition for the problem of backdoor detection in machine learning systems and use it to analyze the feasibility of such problems, providing evidence for the utility and applicability of our definition. The main contributions of this work are an impossibility result and an achievability result for backdoor detection. We show a no-free-lunch theorem, proving that universal (adversary-unaware) backdoor detection is impossible, except for very small alphabet sizes. Thus, we argue, that backdoor detection methods need to be either explicitly, or implicitly adversary-aware. However, our work does not imply that backdoor detection cannot work in specific scenarios, as evidenced by successful backdoor detection methods in the scientific literature. Furthermore, we connect our definition to the probably approximately correct (PAC) learnability of the out-of-distribution detection problem.
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#24102;&#26377;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#23558;&#22270;&#30528;&#33394;&#38382;&#39064;&#24418;&#24335;&#21270;&#20026;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#24378;&#32467;&#26500;&#21487;&#25511;&#24615;&#26465;&#20214;&#19979;&#26368;&#23567;&#21270;&#25511;&#21046;&#36755;&#20837;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.16925</link><description>&lt;p&gt;
&#20351;&#29992;&#24102;&#26377;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#24378;&#32467;&#26500;&#21487;&#25511;&#24615;&#24378;&#21270;&#23398;&#20064;&#26368;&#23567;&#21270;&#25511;&#21046;&#36755;&#20837;
&lt;/p&gt;
&lt;p&gt;
Minimize Control Inputs for Strong Structural Controllability Using Reinforcement Learning with Graph Neural Network
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16925
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#24102;&#26377;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#23558;&#22270;&#30528;&#33394;&#38382;&#39064;&#24418;&#24335;&#21270;&#20026;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#24378;&#32467;&#26500;&#21487;&#25511;&#24615;&#26465;&#20214;&#19979;&#26368;&#23567;&#21270;&#25511;&#21046;&#36755;&#20837;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#32467;&#26500;&#21487;&#25511;&#24615;(SSC)&#20445;&#35777;&#20855;&#26377;&#32447;&#24615;&#19981;&#21464;&#21160;&#21147;&#23398;&#30340;&#32593;&#32476;&#31995;&#32479;&#23545;&#20110;&#25152;&#26377;&#21442;&#25968;&#30340;&#25968;&#20540;&#23454;&#29616;&#37117;&#26159;&#21487;&#25511;&#30340;&#12290;&#24403;&#21069;&#30740;&#31350;&#24050;&#32463;&#20026;&#38646;/&#38750;&#38646;&#25110;&#38646;/&#38750;&#38646;/&#20219;&#24847;&#32467;&#26500;&#30340;SSC&#24314;&#31435;&#20102;&#20195;&#25968;&#21644;&#22270;&#35770;&#26465;&#20214;&#12290;&#19968;&#20010;&#30456;&#20851;&#30340;&#23454;&#38469;&#38382;&#39064;&#26159;&#22914;&#20309;&#29992;&#26368;&#23569;&#30340;&#36755;&#20837;&#20449;&#21495;&#23436;&#20840;&#25511;&#21046;&#31995;&#32479;&#65292;&#24182;&#30830;&#23450;&#24517;&#39035;&#26045;&#21152;&#20449;&#21495;&#30340;&#33410;&#28857;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#36825;&#20010;&#20248;&#21270;&#38382;&#39064;&#26159;NP&#38590;&#30340;&#65292;&#24456;&#38590;&#25214;&#21040;&#35299;&#20915;&#26041;&#26696;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#26681;&#25454;&#38646;/&#38750;&#38646;&#21644;&#38646;/&#38750;&#38646;/&#20219;&#24847;&#32467;&#26500;&#30340;SSC&#30340;&#22270;&#35770;&#26465;&#20214;&#65292;&#23558;&#22270;&#30528;&#33394;&#36807;&#31243;&#26500;&#24314;&#20026;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;(MDP)&#12290;&#25105;&#20204;&#20351;&#29992;&#24102;&#26377;&#26377;&#21521;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;Actor-critic&#26041;&#27861;&#26469;&#34920;&#31034;&#22270;&#30340;&#30528;&#33394;&#20449;&#24687;&#20197;&#20248;&#21270;MDP&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19968;&#20010;&#31038;&#20132;&#24433;&#21709;&#32593;&#32476;&#20013;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16925v1 Announce Type: cross  Abstract: Strong structural controllability (SSC) guarantees networked system with linear-invariant dynamics controllable for all numerical realizations of parameters. Current research has established algebraic and graph-theoretic conditions of SSC for zero/nonzero or zero/nonzero/arbitrary structure. One relevant practical problem is how to fully control the system with the minimal number of input signals and identify which nodes must be imposed signals. Previous work shows that this optimization problem is NP-hard and it is difficult to find the solution. To solve this problem, we formulate the graph coloring process as a Markov decision process (MDP) according to the graph-theoretical condition of SSC for both zero/nonzero and zero/nonzero/arbitrary structure. We use Actor-critic method with Directed graph neural network which represents the color information of graph to optimize MDP. Our method is validated in a social influence network with
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#26088;&#22312;&#28040;&#38500;&#19982;&#20449;&#24687;&#30740;&#31350;&#30456;&#20851;&#30340;&#35748;&#30693;&#38556;&#30861;&#65292;&#32479;&#19968;&#21508;&#26041;&#38754;&#30340;&#20449;&#24687;&#29702;&#35770;&#65292;&#25670;&#33073;&#19981;&#24517;&#35201;&#30340;&#26041;&#27861;&#35770;&#20551;&#35774;&#65292;&#20026;&#21457;&#23637;&#32479;&#19968;&#30340;&#20449;&#24687;&#29702;&#35770;&#25552;&#20379;&#20102;&#21487;&#33021;&#30340;&#24212;&#29992;&#20363;&#23376;&#12290;</title><link>https://arxiv.org/abs/2402.16924</link><description>&lt;p&gt;
&#20449;&#24687;&#30862;&#29255;&#30340;&#29702;&#35770;&#32479;&#19968;
&lt;/p&gt;
&lt;p&gt;
Theoretical Unification of the Fractured Aspects of Information
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16924
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#26088;&#22312;&#28040;&#38500;&#19982;&#20449;&#24687;&#30740;&#31350;&#30456;&#20851;&#30340;&#35748;&#30693;&#38556;&#30861;&#65292;&#32479;&#19968;&#21508;&#26041;&#38754;&#30340;&#20449;&#24687;&#29702;&#35770;&#65292;&#25670;&#33073;&#19981;&#24517;&#35201;&#30340;&#26041;&#27861;&#35770;&#20551;&#35774;&#65292;&#20026;&#21457;&#23637;&#32479;&#19968;&#30340;&#20449;&#24687;&#29702;&#35770;&#25552;&#20379;&#20102;&#21487;&#33021;&#30340;&#24212;&#29992;&#20363;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#30830;&#23450;&#19982;&#20449;&#24687;&#30740;&#31350;&#30456;&#20851;&#30340;&#22522;&#26412;&#35748;&#35782;&#35770;&#38556;&#30861;&#65292;&#28040;&#38500;&#20851;&#20110;&#20449;&#24687;&#21508;&#26041;&#38754;&#22522;&#26412;&#20998;&#27495;&#30340;&#27969;&#34892;&#20449;&#24565;&#65292;&#24182;&#21487;&#29702;&#35299;&#20026;&#23545;&#35748;&#35782;&#35770;&#38556;&#30861;&#30340;Bachelardian&#20999;&#26029;&#30340;&#35299;&#24785;&#26080;&#24517;&#35201;&#30340;&#26041;&#27861;&#35770;&#20551;&#35774;&#12290; &#22312;&#36825;&#20123;&#19968;&#33324;&#24615;&#32771;&#34385;&#20043;&#21069;&#65292;&#25991;&#31456;&#23545;&#30740;&#31350;&#20449;&#24687;&#30340;&#21160;&#26426;&#21450;&#20449;&#24687;&#27010;&#24565;&#22312;&#26234;&#33021;&#12289;&#22797;&#26434;&#24615;&#21644;&#24847;&#35782;&#30340;&#27010;&#24565;&#21270;&#20013;&#30340;&#20316;&#29992;&#36827;&#34892;&#20102;&#27010;&#36848;&#65292;&#35777;&#26126;&#20102;&#22312;&#20449;&#24687;&#30740;&#31350;&#20013;&#38656;&#35201;&#19968;&#20010;&#36275;&#22815;&#19968;&#33324;&#30340;&#35270;&#35282;&#65292;&#24182;&#22312;&#25991;&#31456;&#26411;&#23614;&#38472;&#36848;&#20102;&#19968;&#20010;&#21487;&#33021;&#24212;&#29992;&#30340;&#20363;&#23376;&#65292;&#25351;&#20986;&#22312;&#32479;&#19968;&#20449;&#24687;&#29702;&#35770;&#30340;&#21457;&#23637;&#20013;&#38656;&#35201;&#25670;&#33073;&#19981;&#24517;&#35201;&#30340;&#20998;&#27495;&#21644;&#23545;&#29616;&#26377;&#26041;&#27861;&#20559;&#22909;&#30340;&#20248;&#36234;&#24615;&#20027;&#24352;&#12290; &#25176; B.&#30340;&#21442;&#32771;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16924v1 Announce Type: new  Abstract: The article has as its main objective the identification of fundamental epistemological obstacles in the study of information related to unnecessary methodological assumptions and the demystification of popular beliefs in the fundamental divisions of the aspects of information that can be understood as Bachelardian rupture of epistemological obstacles. These general considerations are preceded by an overview of the motivations for the study of information and the role of the concept of information in the conceptualization of intelligence, complexity, and consciousness justifying the need for a sufficiently general perspective in the study of information, and are followed at the end of the article by a brief exposition of an example of a possible application in the development of the unified theory of information free from unnecessary divisions and claims of superiority of the existing preferences in methodology. The reference to Gaston B
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32852;&#21512;GPS&#21644;&#36335;&#30001;&#24314;&#27169;&#30340;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#30417;&#30563;&#25216;&#26415;&#23454;&#29616;&#65292;&#21033;&#29992;&#20004;&#20010;&#32534;&#30721;&#22120;&#20998;&#21035;&#25429;&#33719;&#36335;&#30001;&#21644;GPS&#36712;&#36857;&#30340;&#34920;&#31034;&#65292;&#24182;&#36890;&#36807;&#20849;&#20139;&#30340;&#21464;&#21387;&#22120;&#36827;&#34892;&#27169;&#24577;&#38388;&#20449;&#24687;&#20132;&#20114;&#12290;</title><link>https://arxiv.org/abs/2402.16915</link><description>&lt;p&gt;
&#36229;&#36234;&#36335;&#30001;&#65306;&#32852;&#21512;GPS&#21644;&#36335;&#30001;&#24314;&#27169;&#20197;&#20248;&#21270;&#36712;&#36857;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
More Than Routing: Joint GPS and Route Modeling for Refine Trajectory Representation Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16915
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32852;&#21512;GPS&#21644;&#36335;&#30001;&#24314;&#27169;&#30340;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#30417;&#30563;&#25216;&#26415;&#23454;&#29616;&#65292;&#21033;&#29992;&#20004;&#20010;&#32534;&#30721;&#22120;&#20998;&#21035;&#25429;&#33719;&#36335;&#30001;&#21644;GPS&#36712;&#36857;&#30340;&#34920;&#31034;&#65292;&#24182;&#36890;&#36807;&#20849;&#20139;&#30340;&#21464;&#21387;&#22120;&#36827;&#34892;&#27169;&#24577;&#38388;&#20449;&#24687;&#20132;&#20114;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36712;&#36857;&#34920;&#31034;&#23398;&#20064;&#22312;&#25903;&#25345;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#20013;&#21457;&#25381;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#20256;&#32479;&#26041;&#27861;&#20026;&#20102;&#36807;&#28388;GPS&#36712;&#36857;&#20013;&#30340;&#22122;&#22768;&#24448;&#24448;&#20391;&#37325;&#20110;&#22522;&#20110;&#36335;&#30001;&#30340;&#26041;&#27861;&#29992;&#20110;&#31616;&#21270;&#36712;&#36857;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#24573;&#30053;&#20102;GPS&#25968;&#25454;&#20013;&#21253;&#21547;&#30340;&#36816;&#21160;&#32454;&#33410;&#65292;&#38480;&#21046;&#20102;&#36712;&#36857;&#34920;&#31034;&#23398;&#20064;&#30340;&#34920;&#31034;&#33021;&#21147;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#33258;&#30417;&#30563;&#25216;&#26415;&#30340;&#32852;&#21512;GPS&#21644;&#36335;&#30001;&#24314;&#27169;&#30340;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#21363;JGRM&#12290;&#25105;&#20204;&#23558;GPS&#36712;&#36857;&#21644;&#36335;&#30001;&#35270;&#20026;&#21333;&#20010;&#31227;&#21160;&#35266;&#23519;&#30340;&#20004;&#31181;&#27169;&#24335;&#65292;&#24182;&#36890;&#36807;&#27169;&#24577;&#38388;&#20449;&#24687;&#20132;&#20114;&#26469;&#34701;&#21512;&#20449;&#24687;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#20004;&#20010;&#32534;&#30721;&#22120;&#65292;&#20998;&#21035;&#29992;&#20110;&#25429;&#33719;&#36335;&#30001;&#21644;GPS&#36712;&#36857;&#30340;&#34920;&#31034;&#12290;&#26469;&#33258;&#36825;&#20004;&#31181;&#27169;&#24577;&#30340;&#34920;&#31034;&#34987;&#36865;&#20837;&#19968;&#20010;&#20849;&#20139;&#30340;&#21464;&#21387;&#22120;&#36827;&#34892;&#27169;&#24577;&#38388;&#20449;&#24687;&#20132;&#20114;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16915v1 Announce Type: cross  Abstract: Trajectory representation learning plays a pivotal role in supporting various downstream tasks. Traditional methods in order to filter the noise in GPS trajectories tend to focus on routing-based methods used to simplify the trajectories. However, this approach ignores the motion details contained in the GPS data, limiting the representation capability of trajectory representation learning. To fill this gap, we propose a novel representation learning framework that Joint GPS and Route Modelling based on self-supervised technology, namely JGRM. We consider GPS trajectory and route as the two modes of a single movement observation and fuse information through inter-modal information interaction. Specifically, we develop two encoders, each tailored to capture representations of route and GPS trajectories respectively. The representations from the two modalities are fed into a shared transformer for inter-modal information interaction. Eve
&lt;/p&gt;</description></item><item><title>&#23558;&#24694;&#24847;&#25552;&#31034;&#20998;&#35299;&#20026;&#29420;&#31435;&#30340;&#23376;&#25552;&#31034;&#20351;&#24471;LLM&#36234;&#29425;&#25915;&#20987;&#26356;&#38590;&#34987;&#26816;&#27979;</title><link>https://arxiv.org/abs/2402.16914</link><description>&lt;p&gt;
DrAttack: &#25552;&#31034;&#20998;&#35299;&#21644;&#37325;&#26500;&#20351;&#24378;&#22823;&#30340;LLM&#36234;&#29425;&#32773;
&lt;/p&gt;
&lt;p&gt;
DrAttack: Prompt Decomposition and Reconstruction Makes Powerful LLM Jailbreakers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16914
&lt;/p&gt;
&lt;p&gt;
&#23558;&#24694;&#24847;&#25552;&#31034;&#20998;&#35299;&#20026;&#29420;&#31435;&#30340;&#23376;&#25552;&#31034;&#20351;&#24471;LLM&#36234;&#29425;&#25915;&#20987;&#26356;&#38590;&#34987;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21457;&#29616;&#23558;&#24694;&#24847;&#25552;&#31034;&#20998;&#35299;&#20026;&#29420;&#31435;&#30340;&#23376;&#25552;&#31034;&#33021;&#22815;&#26377;&#25928;&#27169;&#31946;&#20854;&#28508;&#22312;&#30340;&#24694;&#24847;&#24847;&#22270;&#65292;&#20351;&#20043;&#20197;&#29255;&#27573;&#21270;&#12289;&#19981;&#26131;&#26816;&#27979;&#30340;&#24418;&#24335;&#21576;&#29616;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#36825;&#20123;&#23616;&#38480;&#24615;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#29992;&#20110;&#36234;&#29425;&#25915;&#20987;&#30340;&#33258;&#21160;&#25552;&#31034;&#20998;&#35299;&#21644;&#37325;&#26500;&#26694;&#26550;&#65288;DrAttack&#65289;&#12290;DrAttack&#21253;&#25324;&#19977;&#20010;&#20851;&#38190;&#32452;&#20214;&#65306;(a) &#23558;&#21407;&#22987;&#25552;&#31034;&#36827;&#34892;&#8220;&#20998;&#35299;&#8221;&#20026;&#23376;&#25552;&#31034;&#65292;(b) &#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#30340;&#35821;&#20041;&#19978;&#30456;&#20284;&#20294;&#38544;&#21547;&#30340;&#8220;&#37325;&#26500;&#8221;&#36825;&#20123;&#23376;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16914v1 Announce Type: cross  Abstract: The safety alignment of Large Language Models (LLMs) is vulnerable to both manual and automated jailbreak attacks, which adversarially trigger LLMs to output harmful content. However, current methods for jailbreaking LLMs, which nest entire harmful prompts, are not effective at concealing malicious intent and can be easily identified and rejected by well-aligned LLMs. This paper discovers that decomposing a malicious prompt into separated sub-prompts can effectively obscure its underlying malicious intent by presenting it in a fragmented, less detectable form, thereby addressing these limitations. We introduce an automatic prompt \textbf{D}ecomposition and \textbf{R}econstruction framework for jailbreak \textbf{Attack} (DrAttack). DrAttack includes three key components: (a) `Decomposition' of the original prompt into sub-prompts, (b) `Reconstruction' of these sub-prompts implicitly by in-context learning with semantically similar but h
&lt;/p&gt;</description></item><item><title>&#32467;&#21512;&#31526;&#21495;&#21270;&#23398;&#20064;&#25216;&#26415;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#21270;&#24037;&#20316;&#27969;&#29992;&#20110;&#25913;&#36827;&#20195;&#30721;&#27880;&#37322;&#25968;&#25454;&#29983;&#25104;&#21644;&#20998;&#31867;&#65292;&#24182;&#36890;&#36807;&#29983;&#25104;&#21463;&#25511;&#21512;&#25104;&#25968;&#25454;&#20462;&#22797;LLM&#29983;&#25104;&#20013;&#30340;&#24369;&#28857;&#65292;&#25552;&#39640;&#20102;&#32463;&#20856;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#20195;&#30721;&#27880;&#37322;&#20998;&#31867;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.16910</link><description>&lt;p&gt;
NeSy&#29369;&#22312;&#65306;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#31526;&#21495;&#21270;&#26041;&#27861;&#29992;&#20110;&#25913;&#36827;&#20195;&#30721;&#27880;&#37322;&#25968;&#25454;&#29983;&#25104;&#21644;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
NeSy is alive and well: A LLM-driven symbolic approach for better code comment data generation and classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16910
&lt;/p&gt;
&lt;p&gt;
&#32467;&#21512;&#31526;&#21495;&#21270;&#23398;&#20064;&#25216;&#26415;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#21270;&#24037;&#20316;&#27969;&#29992;&#20110;&#25913;&#36827;&#20195;&#30721;&#27880;&#37322;&#25968;&#25454;&#29983;&#25104;&#21644;&#20998;&#31867;&#65292;&#24182;&#36890;&#36807;&#29983;&#25104;&#21463;&#25511;&#21512;&#25104;&#25968;&#25454;&#20462;&#22797;LLM&#29983;&#25104;&#20013;&#30340;&#24369;&#28857;&#65292;&#25552;&#39640;&#20102;&#32463;&#20856;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#20195;&#30721;&#27880;&#37322;&#20998;&#31867;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#21270;&#65288;NeSy&#65289;&#24037;&#20316;&#27969;&#65292;&#23558;&#22522;&#20110;&#31526;&#21495;&#30340;&#23398;&#20064;&#25216;&#26415;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20195;&#29702;&#30456;&#32467;&#21512;&#65292;&#20197;&#29983;&#25104;C&#32534;&#31243;&#35821;&#35328;&#20013;&#20195;&#30721;&#27880;&#37322;&#20998;&#31867;&#30340;&#21512;&#25104;&#25968;&#25454;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#36825;&#31181;&#24037;&#20316;&#27969;&#29983;&#25104;&#21463;&#25511;&#21512;&#25104;&#25968;&#25454;&#26469;&#20462;&#22797;LLM&#22522;&#20110;&#29983;&#25104;&#30340;&#19968;&#20123;&#26126;&#26174;&#24369;&#28857;&#65292;&#24182;&#25552;&#39640;&#32463;&#20856;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#20195;&#30721;&#27880;&#37322;&#20998;&#31867;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#26368;&#20339;&#27169;&#22411;&#65292;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#65292;&#22312;&#25968;&#25454;&#22686;&#24378;&#21518;&#23454;&#29616;&#20102;91.412&#65285;&#30340;Macro-F1&#20998;&#25968;&#65292;&#22686;&#21152;&#20102;1.033&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16910v1 Announce Type: cross  Abstract: We present a neuro-symbolic (NeSy) workflow combining a symbolic-based learning technique with a large language model (LLM) agent to generate synthetic data for code comment classification in the C programming language. We also show how generating controlled synthetic data using this workflow fixes some of the notable weaknesses of LLM-based generation and increases the performance of classical machine learning models on the code comment classification task. Our best model, a Neural Network, achieves a Macro-F1 score of 91.412% with an increase of 1.033% after data augmentation.
&lt;/p&gt;</description></item><item><title>LDB&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#35843;&#35797;&#26694;&#26550;&#65292;&#21487;&#20197;&#35753;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#36816;&#34892;&#26102;&#25191;&#34892;&#20449;&#24687;&#26469;&#23436;&#21892;&#29983;&#25104;&#30340;&#31243;&#24207;&#12290;</title><link>https://arxiv.org/abs/2402.16906</link><description>&lt;p&gt;
LDB&#65306;&#36890;&#36807;&#36880;&#27493;&#39564;&#35777;&#36816;&#34892;&#26102;&#25191;&#34892;&#26469;&#35843;&#35797;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
LDB: A Large Language Model Debugger via Verifying Runtime Execution Step-by-step
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16906
&lt;/p&gt;
&lt;p&gt;
LDB&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#35843;&#35797;&#26694;&#26550;&#65292;&#21487;&#20197;&#35753;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#36816;&#34892;&#26102;&#25191;&#34892;&#20449;&#24687;&#26469;&#23436;&#21892;&#29983;&#25104;&#30340;&#31243;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20195;&#30721;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#19981;&#20165;&#23558;&#21333;&#27425;&#20195;&#30721;&#29983;&#25104;&#65292;&#32780;&#19988;&#36824;&#23558;&#21333;&#20803;&#27979;&#35797;&#21644;&#31243;&#24207;&#39564;&#35777;&#22120;&#25972;&#21512;&#21040;LLMs&#20013;&#65292;&#20197;&#36845;&#20195;&#22320;&#23436;&#21892;&#29983;&#25104;&#30340;&#31243;&#24207;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#24037;&#20316;&#23558;&#29983;&#25104;&#30340;&#31243;&#24207;&#35270;&#20026;&#19981;&#21487;&#20998;&#21106;&#30340;&#23454;&#20307;&#65292;&#36825;&#23545;LLMs&#22312;&#35843;&#35797;&#31243;&#24207;&#26102;&#23384;&#22312;&#19981;&#36275;&#65292;&#29305;&#21035;&#26159;&#24403;&#31243;&#24207;&#21253;&#21547;&#22797;&#26434;&#30340;&#36923;&#36753;&#27969;&#31243;&#21644;&#25968;&#25454;&#25805;&#20316;&#26102;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#24403;&#20154;&#31867;&#24320;&#21457;&#20154;&#21592;&#35843;&#35797;&#31243;&#24207;&#26102;&#65292;&#20182;&#20204;&#36890;&#24120;&#35774;&#32622;&#26029;&#28857;&#24182;&#26377;&#36873;&#25321;&#22320;&#26816;&#26597;&#36816;&#34892;&#26102;&#25191;&#34892;&#20449;&#24687;&#12290;&#25191;&#34892;&#27969;&#21644;&#20013;&#38388;&#21464;&#37327;&#22312;&#35843;&#35797;&#36807;&#31243;&#20013;&#21457;&#25381;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#28982;&#32780;&#29616;&#26377;&#30340;&#20195;&#30721;&#29983;&#25104;&#25991;&#29486;&#20013;&#26410;&#20805;&#20998;&#21033;&#29992;&#23427;&#20204;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35843;&#35797;&#22120;&#65288;LDB&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#35843;&#35797;&#26694;&#26550;&#65292;&#21487;&#20197;&#35753;LLMs&#36890;&#36807;&#36816;&#34892;&#26102;&#25191;&#34892;&#20449;&#24687;&#23436;&#21892;&#20854;&#29983;&#25104;&#30340;&#31243;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16906v1 Announce Type: cross  Abstract: Large language models (LLMs) are leading significant progress in code generation. Beyond one-pass code generation, recent works further integrate unit tests and program verifiers into LLMs to iteratively refine the generated programs. However, these works consider the generated programs as an indivisible entity, which falls short for LLMs in debugging the programs, especially when the programs contain complex logic flows and data operations. In contrast, when human developers debug programs, they typically set breakpoints and selectively examine runtime execution information. The execution flow and the intermediate variables play a crucial role in the debugging process, yet they are underutilized in the existing literature on code generation. In this study, we introduce Large Language Model Debugger (LDB), a novel debugging framework that enables LLMs to refine their generated programs with the runtime execution information. Specifical
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#24418;&#24335;&#36923;&#36753;&#20026;&#22522;&#30784;&#30340;&#31243;&#24207;&#21512;&#25104;&#21644;LLM&#20869;&#23481;&#29983;&#25104;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#26102;&#38388;&#27969;&#36923;&#36753;&#65288;TSL&#65289;&#23545;&#29983;&#25104;&#24335;&#20195;&#29702;&#26045;&#21152;&#26102;&#38388;&#32422;&#26463;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#20195;&#29702;&#34892;&#20026;&#30340;&#20445;&#35777;&#27700;&#24179;&#12289;&#31995;&#32479;&#30340;&#35299;&#37322;&#24615;&#21644;&#20195;&#29702;&#30340;&#27169;&#22359;&#21270;&#26500;&#24314;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.16905</link><description>&lt;p&gt;
&#21033;&#29992;&#21453;&#24212;&#21512;&#25104;&#23545;&#29983;&#25104;&#24335;&#20195;&#29702;&#34892;&#20026;&#26045;&#21152;&#26102;&#38388;&#32422;&#26463;
&lt;/p&gt;
&lt;p&gt;
Enforcing Temporal Constraints on Generative Agent Behavior with Reactive Synthesis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16905
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#24418;&#24335;&#36923;&#36753;&#20026;&#22522;&#30784;&#30340;&#31243;&#24207;&#21512;&#25104;&#21644;LLM&#20869;&#23481;&#29983;&#25104;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#26102;&#38388;&#27969;&#36923;&#36753;&#65288;TSL&#65289;&#23545;&#29983;&#25104;&#24335;&#20195;&#29702;&#26045;&#21152;&#26102;&#38388;&#32422;&#26463;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#20195;&#29702;&#34892;&#20026;&#30340;&#20445;&#35777;&#27700;&#24179;&#12289;&#31995;&#32479;&#30340;&#35299;&#37322;&#24615;&#21644;&#20195;&#29702;&#30340;&#27169;&#22359;&#21270;&#26500;&#24314;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#27969;&#34892;&#24341;&#21457;&#20102;&#23545;&#21019;&#24314;&#20132;&#20114;&#20195;&#29702;&#26032;&#26041;&#27861;&#30340;&#25506;&#32034;&#12290;&#28982;&#32780;&#65292;&#22312;&#20114;&#21160;&#36807;&#31243;&#20013;&#31649;&#29702;&#36825;&#20123;&#20195;&#29702;&#30340;&#26102;&#38388;&#34892;&#20026;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#24418;&#24335;&#36923;&#36753;&#20026;&#22522;&#30784;&#30340;&#31243;&#24207;&#21512;&#25104;&#19982;LLM&#20869;&#23481;&#29983;&#25104;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#20197;&#21019;&#24314;&#36981;&#23432;&#26102;&#38388;&#32422;&#26463;&#30340;&#29983;&#25104;&#24335;&#20195;&#29702;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#26102;&#38388;&#27969;&#36923;&#36753;&#65288;Temporal Stream Logic&#65292;TSL&#65289;&#29983;&#25104;&#19968;&#20010;&#33258;&#21160;&#26426;&#65292;&#23545;&#20195;&#29702;&#26045;&#21152;&#26102;&#38388;&#32467;&#26500;&#65292;&#24182;&#23558;&#27599;&#20010;&#21160;&#20316;&#30340;&#32454;&#33410;&#30041;&#32473;LLM&#12290;&#36890;&#36807;&#20351;&#29992;TSL&#65292;&#25105;&#20204;&#33021;&#22815;&#22686;&#24378;&#29983;&#25104;&#20195;&#29702;&#65292;&#20351;&#29992;&#25143;&#22312;&#34892;&#20026;&#19978;&#26377;&#26356;&#39640;&#30340;&#20445;&#35777;&#27700;&#24179;&#65292;&#31995;&#32479;&#26356;&#26131;&#35299;&#37322;&#65292;&#24182;&#19988;&#26356;&#33021;&#20197;&#27169;&#22359;&#21270;&#26041;&#24335;&#26500;&#24314;&#20195;&#29702;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#8230;&#8230;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16905v1 Announce Type: new  Abstract: The surge in popularity of Large Language Models (LLMs) has opened doors for new approaches to the creation of interactive agents. However, managing the temporal behavior of such agents over the course of an interaction remains challenging. The stateful, long-term horizon and quantitative reasoning required for coherent agent behavior does not fit well into the LLM paradigm. We propose a combination of formal logic-based program synthesis and LLM content generation to create generative agents that adhere to temporal constraints. Our approach uses Temporal Stream Logic (TSL) to generate an automaton that enforces a temporal structure on an agent and leaves the details of each action for a moment in time to an LLM. By using TSL, we are able to augment the generative agent where users have a higher level of guarantees on behavior, better interpretability of the system, and more ability to build agents in a modular way. We evaluate our appro
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36731;&#37327;&#32423;&#28151;&#21512;&#36951;&#20256;&#31639;&#27861;&#65288;LGSTO&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#38754;&#21521;&#23454;&#26102;&#29289;&#32852;&#32593;&#20256;&#24863;&#31995;&#32479;&#30340;&#36873;&#25321;&#24615;&#20219;&#21153;&#21368;&#36733;&#38382;&#39064;&#65292;&#20197;&#22312;&#26102;&#38388;&#21644;&#33021;&#37327;&#32422;&#26463;&#19979;&#26368;&#22823;&#21270;&#25512;&#29702;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.16904</link><description>&lt;p&gt;
&#38754;&#21521;&#26368;&#22823;&#25512;&#29702;&#20934;&#30830;&#24615;&#21644;&#33021;&#25928;&#23454;&#26102;&#29289;&#32852;&#32593;&#20256;&#24863;&#31995;&#32479;&#30340;&#36873;&#25321;&#24615;&#20219;&#21153;&#21368;&#36733;
&lt;/p&gt;
&lt;p&gt;
Selective Task offloading for Maximum Inference Accuracy and Energy efficient Real-Time IoT Sensing Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16904
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36731;&#37327;&#32423;&#28151;&#21512;&#36951;&#20256;&#31639;&#27861;&#65288;LGSTO&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#38754;&#21521;&#23454;&#26102;&#29289;&#32852;&#32593;&#20256;&#24863;&#31995;&#32479;&#30340;&#36873;&#25321;&#24615;&#20219;&#21153;&#21368;&#36733;&#38382;&#39064;&#65292;&#20197;&#22312;&#26102;&#38388;&#21644;&#33021;&#37327;&#32422;&#26463;&#19979;&#26368;&#22823;&#21270;&#25512;&#29702;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23567;&#22411;&#25512;&#29702;&#27169;&#22411;&#30340;&#26368;&#26032;&#36827;&#23637;&#20419;&#36827;&#20102;&#20154;&#24037;&#26234;&#33021;&#22312;&#36793;&#32536;&#37096;&#32626;&#12290;&#28982;&#32780;&#65292;&#36793;&#32536;&#35774;&#22791;&#26377;&#38480;&#30340;&#36164;&#28304;&#29305;&#24615;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#23454;&#26102;&#24212;&#29992;&#31243;&#24207;&#65292;&#24102;&#26469;&#20102;&#26032;&#30340;&#25361;&#25112;&#12290;&#37096;&#32626;&#22810;&#20010;&#25512;&#29702;&#27169;&#22411;&#65288;&#25110;&#32773;&#19968;&#20010;&#23610;&#23544;&#21487;&#35843;&#30340;&#27169;&#22411;&#65289;&#21464;&#21270;&#22823;&#23567;&#65292;&#22240;&#27492;&#20934;&#30830;&#24615;&#21644;&#21151;&#32791;&#65292;&#20197;&#21450;&#36793;&#32536;&#26381;&#21153;&#22120;&#25512;&#29702;&#27169;&#22411;&#65292;&#21487;&#20197;&#25552;&#20379;&#19968;&#20010;&#21160;&#24577;&#31995;&#32479;&#65292;&#22312;&#20854;&#20013;&#26681;&#25454;&#24403;&#21069;&#36164;&#28304;&#26465;&#20214;&#25191;&#34892;&#23545;&#25512;&#29702;&#27169;&#22411;&#21040;&#25512;&#29702;&#20219;&#21153;&#30340;&#20998;&#37197;&#12290;&#22240;&#27492;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#23558;&#25512;&#29702;&#27169;&#22411;&#36873;&#25321;&#24615;&#20998;&#37197;&#32473;&#20219;&#21153;&#25110;&#23558;&#20854;&#21368;&#36733;&#21040;&#36793;&#32536;&#26381;&#21153;&#22120;&#20197;&#22312;&#26102;&#38388;&#21644;&#33021;&#37327;&#32422;&#26463;&#19979;&#26368;&#22823;&#21270;&#25512;&#29702;&#20934;&#30830;&#24615;&#30340;&#38382;&#39064;&#12290;&#35813;&#38382;&#39064;&#34987;&#35777;&#26126;&#26159;&#26080;&#30028;&#22810;&#32500;&#32972;&#21253;&#38382;&#39064;&#30340;&#19968;&#20010;&#23454;&#20363;&#65292;&#34987;&#35748;&#20026;&#26159;&#19968;&#20010;&#24378; NP-&#38590;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36731;&#37327;&#32423;&#28151;&#21512;&#36951;&#20256;&#31639;&#27861;&#65288;LGSTO&#65289;&#26469;&#35299;&#20915;&#27492;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16904v1 Announce Type: cross  Abstract: The recent advancements in small-size inference models facilitated AI deployment on the edge. However, the limited resource nature of edge devices poses new challenges especially for real-time applications. Deploying multiple inference models (or a single tunable model) varying in size and therefore accuracy and power consumption, in addition to an edge server inference model, can offer a dynamic system in which the allocation of inference models to inference jobs is performed according to the current resource conditions. Therefore, in this work, we tackle the problem of selectively allocating inference models to jobs or offloading them to the edge server to maximize inference accuracy under time and energy constraints. This problem is shown to be an instance of the unbounded multidimensional knapsack problem which is considered a strongly NP-hard problem. We propose a lightweight hybrid genetic algorithm (LGSTO) to solve this problem.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#34507;&#30333;&#36136;&#30340;&#22522;&#22240;&#34920;&#31034;&#20316;&#20026;&#19968;&#31181;&#19978;&#19979;&#25991;&#24863;&#30693;&#21644;&#32467;&#26500;&#30456;&#20851;&#30340;&#26631;&#35760;&#22120;&#65292;&#36890;&#36807;Masked Gene Modeling&#65288;MGM&#65289;&#21644;Triple Enhanced Metagenomic Contrastive Learning&#65288;TEM-CL&#65289;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#23439;&#22522;&#22240;&#32452;&#35821;&#35328;&#27169;&#22411;FGBERT&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#25429;&#25417;&#22522;&#22240;&#24207;&#21015;&#19982;&#21151;&#33021;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2402.16901</link><description>&lt;p&gt;
FGBERT&#65306;&#22522;&#20110;&#21151;&#33021;&#39537;&#21160;&#30340;&#23439;&#22522;&#22240;&#32452;&#39044;&#35757;&#32451;&#22522;&#22240;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
FGBERT: Function-Driven Pre-trained Gene Language Model for Metagenomics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16901
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#34507;&#30333;&#36136;&#30340;&#22522;&#22240;&#34920;&#31034;&#20316;&#20026;&#19968;&#31181;&#19978;&#19979;&#25991;&#24863;&#30693;&#21644;&#32467;&#26500;&#30456;&#20851;&#30340;&#26631;&#35760;&#22120;&#65292;&#36890;&#36807;Masked Gene Modeling&#65288;MGM&#65289;&#21644;Triple Enhanced Metagenomic Contrastive Learning&#65288;TEM-CL&#65289;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#23439;&#22522;&#22240;&#32452;&#35821;&#35328;&#27169;&#22411;FGBERT&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#25429;&#25417;&#22522;&#22240;&#24207;&#21015;&#19982;&#21151;&#33021;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Metagenomic data, comprising mixed multi-species genomes, are prevalent in diverse environments like oceans and soils, significantly impacting human health and ecological functions. However, current research relies on K-mer representations, limiting the capture of structurally relevant gene contexts. To address these limitations and further our understanding of complex relationships between metagenomic sequences and their functions, we introduce a protein-based gene representation as a context-aware and structure-relevant tokenizer. Our approach includes Masked Gene Modeling (MGM) for gene group-level pre-training, providing insights into inter-gene contextual information, and Triple Enhanced Metagenomic Contrastive Learning (TEM-CL) for gene-level pre-training to model gene sequence-function relationships. MGM and TEM-CL constitute our novel metagenomic language model FGBERT, pre-trained on 100 million metagenomic sequences.
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16901v1 Announce Type: cross  Abstract: Metagenomic data, comprising mixed multi-species genomes, are prevalent in diverse environments like oceans and soils, significantly impacting human health and ecological functions. However, current research relies on K-mer representations, limiting the capture of structurally relevant gene contexts. To address these limitations and further our understanding of complex relationships between metagenomic sequences and their functions, we introduce a protein-based gene representation as a context-aware and structure-relevant tokenizer. Our approach includes Masked Gene Modeling (MGM) for gene group-level pre-training, providing insights into inter-gene contextual information, and Triple Enhanced Metagenomic Contrastive Learning (TEM-CL) for gene-level pre-training to model gene sequence-function relationships. MGM and TEM-CL constitute our novel metagenomic language model {\NAME}, pre-trained on 100 million metagenomic sequences. We demon
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#36830;&#32493;&#26102;&#38388;&#25511;&#21046;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20197;&#30452;&#25509;&#20998;&#26512;Bellman&#26368;&#20248;&#25439;&#22833;\emph{&#20808;&#39564;}&#27867;&#21270;&#35823;&#24046;&#30340;&#26041;&#27861;&#65292;&#36991;&#20813;&#20102;&#26377;&#30028;&#24615;&#20551;&#35774;&#65292;&#24182;&#36890;&#36807;&#26368;&#22823;&#31639;&#23376;&#30340;&#20998;&#35299;&#26041;&#27861;&#23454;&#29616;&#20102;&#25439;&#22833;&#20989;&#25968;&#30340;&#36716;&#25442;&#12290;</title><link>https://arxiv.org/abs/2402.16899</link><description>&lt;p&gt;
&#36830;&#32493;&#26102;&#38388;&#24378;&#21270;&#23398;&#20064;&#20013;&#28145;&#24230;&#27531;&#24046;&#32593;&#32476;&#30340;\emph{&#20808;&#39564;&#20272;&#35745;}
&lt;/p&gt;
&lt;p&gt;
A prior Estimates for Deep Residual Network in Continuous-time Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16899
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#36830;&#32493;&#26102;&#38388;&#25511;&#21046;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20197;&#30452;&#25509;&#20998;&#26512;Bellman&#26368;&#20248;&#25439;&#22833;\emph{&#20808;&#39564;}&#27867;&#21270;&#35823;&#24046;&#30340;&#26041;&#27861;&#65292;&#36991;&#20813;&#20102;&#26377;&#30028;&#24615;&#20551;&#35774;&#65292;&#24182;&#36890;&#36807;&#26368;&#22823;&#31639;&#23376;&#30340;&#20998;&#35299;&#26041;&#27861;&#23454;&#29616;&#20102;&#25439;&#22833;&#20989;&#25968;&#30340;&#36716;&#25442;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22312;&#35768;&#22810;&#22823;&#35268;&#27169;&#23454;&#38469;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#24615;&#33021;&#20998;&#26512;&#24573;&#30053;&#20102;&#36830;&#32493;&#26102;&#38388;&#25511;&#21046;&#38382;&#39064;&#30340;&#29420;&#29305;&#29305;&#24449;&#65292;&#26080;&#27861;&#30452;&#25509;&#20272;&#35745;Bellman&#26368;&#20248;&#25439;&#22833;&#30340;&#27867;&#21270;&#35823;&#24046;&#65292;&#24182;&#19988;&#38656;&#35201;&#19968;&#20010;&#26377;&#30028;&#24615;&#20551;&#35774;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#20391;&#37325;&#20110;&#36830;&#32493;&#26102;&#38388;&#25511;&#21046;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#25152;&#26377;&#28385;&#36275;&#21322;&#32676;&#21644;Lipschitz&#24615;&#36136;&#30340;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#22312;&#35813;&#26041;&#27861;&#19979;&#65292;&#25105;&#20204;&#33021;&#22815;&#30452;&#25509;&#20998;&#26512;Bellman&#26368;&#20248;&#25439;&#22833;&#30340;\emph{&#20808;&#39564;}&#27867;&#21270;&#35823;&#24046;&#12290;&#35813;&#26041;&#27861;&#30340;&#26680;&#24515;&#22312;&#20110;&#25439;&#22833;&#20989;&#25968;&#30340;&#20004;&#27425;&#36716;&#25442;&#12290;&#20026;&#20102;&#23436;&#25104;&#36716;&#25442;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26368;&#22823;&#31639;&#23376;&#30340;&#20998;&#35299;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#36825;&#20010;&#20998;&#26512;&#26041;&#27861;&#19981;&#38656;&#35201;&#26377;&#30028;&#24615;&#20551;&#35774;&#12290;&#26368;&#32456;&#25105;&#20204;&#32500;&#24471;&#21040;&#20102;&#19968;&#20010;&#27809;&#26377;&#8220;&#32500;&#24230;&#35781;&#21650;&#8221;&#30340;\emph{&#20808;&#39564;}&#27867;&#21270;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16899v1 Announce Type: cross  Abstract: Deep reinforcement learning excels in numerous large-scale practical applications. However, existing performance analyses ignores the unique characteristics of continuous-time control problems, is unable to directly estimate the generalization error of the Bellman optimal loss and require a boundedness assumption. Our work focuses on continuous-time control problems and proposes a method that is applicable to all such problems where the transition function satisfies semi-group and Lipschitz properties. Under this method, we can directly analyze the \emph{a priori} generalization error of the Bellman optimal loss. The core of this method lies in two transformations of the loss function. To complete the transformation, we propose a decomposition method for the maximum operator. Additionally, this analysis method does not require a boundedness assumption. Finally, we obtain an \emph{a priori} generalization error without the curse of dime
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;MIM-Reasoner&#65292;&#32467;&#21512;&#24378;&#21270;&#23398;&#20064;&#21644;&#27010;&#29575;&#22270;&#27169;&#22411;&#65292;&#26377;&#25928;&#22320;&#25429;&#25417;&#20102;&#32473;&#23450;&#22810;&#37325;&#32593;&#32476;&#20869;&#37096;&#21644;&#23618;&#38388;&#30340;&#22797;&#26434;&#20256;&#25773;&#36807;&#31243;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;MIM&#20013;&#26368;&#20855;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.16898</link><description>&lt;p&gt;
MIM-Reasoner: &#20855;&#26377;&#29702;&#35770;&#20445;&#35777;&#30340;&#22810;&#37325;&#24433;&#21709;&#26368;&#22823;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
MIM-Reasoner: Learning with Theoretical Guarantees for Multiplex Influence Maximization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16898
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;MIM-Reasoner&#65292;&#32467;&#21512;&#24378;&#21270;&#23398;&#20064;&#21644;&#27010;&#29575;&#22270;&#27169;&#22411;&#65292;&#26377;&#25928;&#22320;&#25429;&#25417;&#20102;&#32473;&#23450;&#22810;&#37325;&#32593;&#32476;&#20869;&#37096;&#21644;&#23618;&#38388;&#30340;&#22797;&#26434;&#20256;&#25773;&#36807;&#31243;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;MIM&#20013;&#26368;&#20855;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#37325;&#24433;&#21709;&#26368;&#22823;&#21270;&#65288;MIM&#65289;&#35201;&#27714;&#25105;&#20204;&#35782;&#21035;&#19968;&#32452;&#31181;&#23376;&#29992;&#25143;&#65292;&#20197;&#26368;&#22823;&#21270;&#22810;&#37325;&#32593;&#32476;&#20013;&#21463;&#24433;&#21709;&#29992;&#25143;&#30340;&#39044;&#26399;&#25968;&#37327;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;MIM-Reasoner&#65292;&#23558;&#24378;&#21270;&#23398;&#20064;&#19982;&#27010;&#29575;&#22270;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#26377;&#25928;&#25429;&#25417;&#32473;&#23450;&#22810;&#37325;&#32593;&#32476;&#20869;&#37096;&#21644;&#23618;&#38388;&#30340;&#22797;&#26434;&#20256;&#25773;&#36807;&#31243;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;MIM&#20013;&#26368;&#20855;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16898v1 Announce Type: cross  Abstract: Multiplex influence maximization (MIM) asks us to identify a set of seed users such as to maximize the expected number of influenced users in a multiplex network. MIM has been one of central research topics, especially in nowadays social networking landscape where users participate in multiple online social networks (OSNs) and their influences can propagate among several OSNs simultaneously. Although there exist a couple combinatorial algorithms to MIM, learning-based solutions have been desired due to its generalization ability to heterogeneous networks and their diversified propagation characteristics. In this paper, we introduce MIM-Reasoner, coupling reinforcement learning with probabilistic graphical model, which effectively captures the complex propagation process within and between layers of a given multiplex network, thereby tackling the most challenging problem in MIM. We establish a theoretical guarantee for MIM-Reasoner as w
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#21487;&#38752;&#30340;&#20914;&#31361;&#22810;&#35270;&#35282;&#23398;&#20064;&#65288;RCML&#65289;&#38382;&#39064;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;Evidential Conflictive Multi-view Learning (ECML)&#26041;&#27861;&#26469;&#22788;&#29702;&#20855;&#26377;&#20914;&#31361;&#20449;&#24687;&#30340;&#22810;&#35270;&#35282;&#25968;&#25454;&#12290;</title><link>https://arxiv.org/abs/2402.16897</link><description>&lt;p&gt;
&#21487;&#38752;&#30340;&#20914;&#31361;&#22810;&#35270;&#35282;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Reliable Conflictive Multi-View Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16897
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#21487;&#38752;&#30340;&#20914;&#31361;&#22810;&#35270;&#35282;&#23398;&#20064;&#65288;RCML&#65289;&#38382;&#39064;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;Evidential Conflictive Multi-view Learning (ECML)&#26041;&#27861;&#26469;&#22788;&#29702;&#20855;&#26377;&#20914;&#31361;&#20449;&#24687;&#30340;&#22810;&#35270;&#35282;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#35270;&#35282;&#23398;&#20064;&#26088;&#22312;&#32467;&#21512;&#22810;&#20010;&#29305;&#24449;&#65292;&#20197;&#23454;&#29616;&#23545;&#25968;&#25454;&#30340;&#26356;&#20840;&#38754;&#25551;&#36848;&#12290;&#20043;&#21069;&#30340;&#22823;&#37096;&#20998;&#24037;&#20316;&#37117;&#20551;&#35774;&#22810;&#20010;&#35270;&#22270;&#26159;&#20005;&#26684;&#23545;&#40784;&#30340;&#12290;&#28982;&#32780;&#65292;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#22810;&#35270;&#35282;&#25968;&#25454;&#21487;&#33021;&#21253;&#21547;&#20302;&#36136;&#37327;&#30340;&#20914;&#31361;&#23454;&#20363;&#65292;&#21363;&#22312;&#19981;&#21516;&#35270;&#22270;&#20013;&#26174;&#31034;&#20914;&#31361;&#20449;&#24687;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#21487;&#38752;&#30340;&#20914;&#31361;&#22810;&#35270;&#35282;&#23398;&#20064;&#65288;RCML&#65289;&#38382;&#39064;&#65292;&#35201;&#27714;&#27169;&#22411;&#20026;&#20914;&#31361;&#30340;&#22810;&#35270;&#35282;&#25968;&#25454;&#25552;&#20379;&#20915;&#31574;&#32467;&#26524;&#21644;&#38468;&#21152;&#30340;&#21487;&#38752;&#24615;&#12290;&#25105;&#20204;&#20026;&#36825;&#20010;&#38382;&#39064;&#24320;&#21457;&#20102;&#19968;&#31181;Evidential Conflictive Multi-view Learning (ECML)&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16897v1 Announce Type: cross  Abstract: Multi-view learning aims to combine multiple features to achieve more comprehensive descriptions of data. Most previous works assume that multiple views are strictly aligned. However, real-world multi-view data may contain low-quality conflictive instances, which show conflictive information in different views. Previous methods for this problem mainly focus on eliminating the conflictive data instances by removing them or replacing conflictive views. Nevertheless, real-world applications usually require making decisions for conflictive instances rather than only eliminating them. To solve this, we point out a new Reliable Conflictive Multi-view Learning (RCML) problem, which requires the model to provide decision results and attached reliabilities for conflictive multi-view data. We develop an Evidential Conflictive Multi-view Learning (ECML) method for this problem. ECML first learns view-specific evidence, which could be termed as th
&lt;/p&gt;</description></item><item><title>RAG&#25216;&#26415;&#26377;&#21487;&#33021;&#25913;&#21464;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#34892;&#20026;&#65292;&#24102;&#26469;&#26032;&#30340;&#38544;&#31169;&#38382;&#39064;&#65292;&#20294;&#21516;&#26102;&#20063;&#21487;&#20197;&#20943;&#23569;&#35821;&#35328;&#27169;&#22411;&#30340;&#35757;&#32451;&#25968;&#25454;&#27844;&#28431;&#12290;</title><link>https://arxiv.org/abs/2402.16893</link><description>&lt;p&gt;
&#25506;&#35752;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#20013;&#30340;&#38544;&#31169;&#38382;&#39064;&#30340;&#21033;&#19982;&#24330;
&lt;/p&gt;
&lt;p&gt;
The Good and The Bad: Exploring Privacy Issues in Retrieval-Augmented Generation (RAG)
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16893
&lt;/p&gt;
&lt;p&gt;
RAG&#25216;&#26415;&#26377;&#21487;&#33021;&#25913;&#21464;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#34892;&#20026;&#65292;&#24102;&#26469;&#26032;&#30340;&#38544;&#31169;&#38382;&#39064;&#65292;&#20294;&#21516;&#26102;&#20063;&#21487;&#20197;&#20943;&#23569;&#35821;&#35328;&#27169;&#22411;&#30340;&#35757;&#32451;&#25968;&#25454;&#27844;&#28431;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#25216;&#26415;&#65292;&#29992;&#20110;&#20419;&#36827;&#20855;&#26377;&#19987;&#26377;&#21644;&#31169;&#23494;&#25968;&#25454;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#20854;&#20013;&#25968;&#25454;&#38544;&#31169;&#26159;&#19968;&#20010;&#20851;&#38190;&#20851;&#27880;&#28857;&#12290;&#23613;&#31649;&#22823;&#37327;&#30740;&#31350;&#24050;&#32463;&#35777;&#26126;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23384;&#22312;&#30340;&#38544;&#31169;&#39118;&#38505;&#65292;RAG&#25216;&#26415;&#26377;&#21487;&#33021;&#25913;&#21464;LLM&#29983;&#25104;&#30340;&#22266;&#26377;&#34892;&#20026;&#65292;&#20174;&#32780;&#24341;&#21457;&#30446;&#21069;&#23578;&#26410;&#20805;&#20998;&#25506;&#35752;&#30340;&#26032;&#38544;&#31169;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#30340;&#23454;&#35777;&#30740;&#31350;&#65292;&#24182;&#37319;&#29992;&#20102;&#26032;&#39062;&#30340;&#25915;&#20987;&#26041;&#27861;&#65292;&#35777;&#26126;&#20102;RAG&#31995;&#32479;&#27844;&#38706;&#31169;&#23494;&#26816;&#32034;&#25968;&#25454;&#24211;&#30340;&#33030;&#24369;&#24615;&#12290;&#23613;&#31649;RAG&#24102;&#26469;&#20102;&#26816;&#32034;&#25968;&#25454;&#30340;&#26032;&#39118;&#38505;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25581;&#31034;&#20102;RAG&#21487;&#20197;&#20943;&#23569;LLM&#35757;&#32451;&#25968;&#25454;&#30340;&#27844;&#28431;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#22312;&#26412;&#25991;&#20013;&#25552;&#20379;&#20102;&#20851;&#20110;&#26816;&#32034;&#22686;&#24378;LLMs&#38544;&#31169;&#20445;&#25252;&#30340;&#26032;&#35265;&#35299;&#65292;&#36825;&#26377;&#21033;&#20110;LLMs&#21644;RAG&#31995;&#32479;&#30340;&#26500;&#24314;&#32773;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21487;&#22312;https://github.com/phycholosogy/RAG-privacy&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16893v1 Announce Type: cross  Abstract: Retrieval-augmented generation (RAG) is a powerful technique to facilitate language model with proprietary and private data, where data privacy is a pivotal concern. Whereas extensive research has demonstrated the privacy risks of large language models (LLMs), the RAG technique could potentially reshape the inherent behaviors of LLM generation, posing new privacy issues that are currently under-explored. In this work, we conduct extensive empirical studies with novel attack methods, which demonstrate the vulnerability of RAG systems on leaking the private retrieval database. Despite the new risk brought by RAG on the retrieval data, we further reveal that RAG can mitigate the leakage of the LLMs' training data. Overall, we provide new insights in this paper for privacy protection of retrieval-augmented LLMs, which benefit both LLMs and RAG systems builders. Our code is available at https://github.com/phycholosogy/RAG-privacy.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#39318;&#27425;&#23581;&#35797;&#35299;&#20915;&#36328;&#38382;&#39064;&#27867;&#21270;&#30340;&#20851;&#38190;&#25361;&#25112;&#65292;&#36890;&#36807;&#23558;VRPs&#23450;&#20041;&#20026;&#20849;&#20139;&#22522;&#30784;&#23646;&#24615;&#30340;&#19981;&#21516;&#32452;&#21512;&#65292;&#24182;&#36890;&#36807;&#23646;&#24615;&#32452;&#21512;&#21516;&#26102;&#35299;&#20915;&#23427;&#20204;&#65292;&#23454;&#29616;&#20102;&#38646;&#26679;&#26412;&#27867;&#21270;&#30340;&#36335;&#24452;&#38382;&#39064;&#35299;&#20915;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.16891</link><description>&lt;p&gt;
&#22810;&#20219;&#21153;&#23398;&#20064;&#29992;&#20110;&#20855;&#26377;&#36328;&#38382;&#39064;&#38646;&#26679;&#26412;&#27867;&#21270;&#30340;&#36335;&#24452;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Multi-Task Learning for Routing Problem with Cross-Problem Zero-Shot Generalization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16891
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#39318;&#27425;&#23581;&#35797;&#35299;&#20915;&#36328;&#38382;&#39064;&#27867;&#21270;&#30340;&#20851;&#38190;&#25361;&#25112;&#65292;&#36890;&#36807;&#23558;VRPs&#23450;&#20041;&#20026;&#20849;&#20139;&#22522;&#30784;&#23646;&#24615;&#30340;&#19981;&#21516;&#32452;&#21512;&#65292;&#24182;&#36890;&#36807;&#23646;&#24615;&#32452;&#21512;&#21516;&#26102;&#35299;&#20915;&#23427;&#20204;&#65292;&#23454;&#29616;&#20102;&#38646;&#26679;&#26412;&#27867;&#21270;&#30340;&#36335;&#24452;&#38382;&#39064;&#35299;&#20915;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#65288;VRPs&#65289;&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#37117;&#33021;&#25214;&#21040;&#65292;&#24050;&#32463;&#25104;&#20026;&#20960;&#21313;&#24180;&#30340;&#37325;&#35201;&#30740;&#31350;&#35838;&#39064;&#12290;&#26368;&#36817;&#65292;&#21033;&#29992;&#22522;&#20110;&#23398;&#20064;&#30340;&#27169;&#22411;&#26469;&#35299;&#20915;VRPs&#30340;&#31070;&#32463;&#32452;&#21512;&#20248;&#21270;&#65288;NCO&#65289;&#26041;&#27861;&#24341;&#36215;&#20102;&#30456;&#24403;&#22823;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;NCO&#26041;&#27861;&#36890;&#24120;&#38656;&#35201;&#20026;&#27599;&#20010;&#36335;&#24452;&#38382;&#39064;&#26500;&#24314;&#19968;&#20010;&#27169;&#22411;&#65292;&#36825;&#26174;&#33879;&#38459;&#30861;&#20102;&#23427;&#20204;&#22312;&#20855;&#26377;&#19981;&#21516;&#23646;&#24615;&#30340;&#30495;&#23454;&#24037;&#19994;&#38382;&#39064;&#20013;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#23581;&#35797;&#35299;&#20915;&#36328;&#38382;&#39064;&#27867;&#21270;&#30340;&#20851;&#38190;&#25361;&#25112;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;VRPs&#23450;&#20041;&#20026;&#19968;&#32452;&#20849;&#20139;&#30340;&#22522;&#30784;&#23646;&#24615;&#30340;&#19981;&#21516;&#32452;&#21512;&#65292;&#24182;&#36890;&#36807;&#23646;&#24615;&#32452;&#21512;&#21516;&#26102;&#36890;&#36807;&#21333;&#19968;&#27169;&#22411;&#35299;&#20915;&#23427;&#20204;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#33021;&#22815;&#25104;&#21151;&#35299;&#20915;&#20855;&#26377;&#26410;&#35265;&#23646;&#24615;&#32452;&#21512;&#30340;VRPs&#65292;&#23454;&#29616;&#38646;&#26679;&#26412;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16891v1 Announce Type: cross  Abstract: Vehicle routing problems (VRPs), which can be found in numerous real-world applications, have been an important research topic for several decades. Recently, the neural combinatorial optimization (NCO) approach that leverages a learning-based model to solve VRPs without manual algorithm design has gained substantial attention. However, current NCO methods typically require building one model for each routing problem, which significantly hinders their practical application for real-world industry problems with diverse attributes. In this work, we make the first attempt to tackle the crucial challenge of cross-problem generalization. In particular, we formulate VRPs as different combinations of a set of shared underlying attributes and solve them simultaneously via a single model through attribute composition. In this way, our proposed model can successfully solve VRPs with unseen attribute combinations in a zero-shot generalization mann
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20877;&#29983;&#25104;&#35782;&#21035;&#27169;&#22411;&#25968;&#25454;&#25152;&#26377;&#26435;&#30340;&#26041;&#27861;&#65292;&#36991;&#20813;&#20102;&#20256;&#32479;&#25968;&#23383;&#27700;&#21360;&#25216;&#26415;&#21487;&#33021;&#30772;&#22351;&#36755;&#20986;&#36136;&#37327;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.16889</link><description>&lt;p&gt;
&#29983;&#25104;&#27169;&#22411;&#20855;&#26377;&#33258;&#36523;&#27700;&#21360;&#65306;&#36890;&#36807;&#20877;&#29983;&#25104;&#22768;&#26126;&#27169;&#22411;&#35748;&#35777;
&lt;/p&gt;
&lt;p&gt;
Generative Models are Self-Watermarked: Declaring Model Authentication through Re-Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16889
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20877;&#29983;&#25104;&#35782;&#21035;&#27169;&#22411;&#25968;&#25454;&#25152;&#26377;&#26435;&#30340;&#26041;&#27861;&#65292;&#36991;&#20813;&#20102;&#20256;&#32479;&#25968;&#23383;&#27700;&#21360;&#25216;&#26415;&#21487;&#33021;&#30772;&#22351;&#36755;&#20986;&#36136;&#37327;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26426;&#22120;&#21644;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#30340;&#20869;&#23481;&#19981;&#26029;&#22686;&#21152;&#65292;&#20445;&#25252;&#29983;&#25104;&#27169;&#22411;&#30340;&#30693;&#35782;&#20135;&#26435;&#24050;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#65292;&#28982;&#32780;&#39564;&#35777;&#25968;&#25454;&#25152;&#26377;&#26435;&#22312;&#26410;&#32463;&#25480;&#26435;&#37325;&#22797;&#20351;&#29992;&#29983;&#25104;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#38754;&#20020;&#30528;&#24040;&#22823;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#33268;&#21147;&#20110;&#26816;&#27979;&#21363;&#20351;&#26159;&#20010;&#21035;&#26679;&#26412;&#30340;&#25968;&#25454;&#37325;&#22797;&#20351;&#29992;&#12290;&#20256;&#32479;&#19978;&#65292;&#25968;&#23383;&#27700;&#21360;&#25216;&#26415;&#34987;&#21033;&#29992;&#26469;&#26816;&#27979;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#30340;&#20869;&#23481;&#12290;&#28982;&#32780;&#65292;&#19982;&#23558;&#38468;&#21152;&#20449;&#24687;&#23884;&#20837;&#27169;&#22411;&#25110;&#29983;&#25104;&#20869;&#23481;&#20197;&#20316;&#20026;&#35302;&#21457;&#22120;&#30340;&#27700;&#21360;&#25216;&#26415;&#19981;&#21516;&#65292;&#28508;&#22312;&#25439;&#23475;&#36755;&#20986;&#36136;&#37327;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#37325;&#26032;&#29983;&#25104;&#35782;&#21035;&#22312;&#36755;&#20986;&#20013;&#22266;&#26377;&#23384;&#22312;&#30340;&#28508;&#22312;&#25351;&#32441;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#39564;&#35777;&#36807;&#31243;&#65292;&#36890;&#36807;&#20877;&#29983;&#25104;&#26469;&#24402;&#23646;&#25968;&#25454;&#25152;&#26377;&#26435;&#65292;&#36827;&#19968;&#27493;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16889v1 Announce Type: cross  Abstract: As machine- and AI-generated content proliferates, protecting the intellectual property of generative models has become imperative, yet verifying data ownership poses formidable challenges, particularly in cases of unauthorized reuse of generated data. The challenge of verifying data ownership is further amplified by using Machine Learning as a Service (MLaaS), which often functions as a black-box system.   Our work is dedicated to detecting data reuse from even an individual sample. Traditionally, watermarking has been leveraged to detect AI-generated content. However, unlike watermarking techniques that embed additional information as triggers into models or generated content, potentially compromising output quality, our approach identifies latent fingerprints inherently present within the outputs through re-generation. We propose an explainable verification procedure that attributes data ownership through re-generation, and further 
&lt;/p&gt;</description></item><item><title>&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#19982;&#20016;&#23500;&#30495;&#23454;&#32593;&#32476;&#25968;&#25454;&#30340;&#23384;&#22312;&#24320;&#21551;&#20102;&#22797;&#26434;&#32593;&#32476;&#31185;&#23398;&#30740;&#31350;&#30340;&#26032;&#26102;&#20195;&#65292;&#26377;&#26395;&#20811;&#26381;&#29616;&#23384;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.16887</link><description>&lt;p&gt;
&#22797;&#26434;&#32593;&#32476;&#30340;&#20154;&#24037;&#26234;&#33021;&#65306;&#28508;&#21147;&#12289;&#26041;&#27861;&#35770;&#21644;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Artificial Intelligence for Complex Network: Potential, Methodology and Application
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16887
&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#19982;&#20016;&#23500;&#30495;&#23454;&#32593;&#32476;&#25968;&#25454;&#30340;&#23384;&#22312;&#24320;&#21551;&#20102;&#22797;&#26434;&#32593;&#32476;&#31185;&#23398;&#30740;&#31350;&#30340;&#26032;&#26102;&#20195;&#65292;&#26377;&#26395;&#20811;&#26381;&#29616;&#23384;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22797;&#26434;&#32593;&#32476;&#23384;&#22312;&#20110;&#21508;&#31181;&#30495;&#23454;&#19990;&#30028;&#31995;&#32479;&#20013;&#65292;&#20174;&#33258;&#28982;&#29615;&#22659;&#21040;&#20154;&#31867;&#31038;&#20250;&#12290;&#36825;&#20123;&#32593;&#32476;&#30340;&#26412;&#36136;&#22312;&#20110;&#23427;&#20204;&#33021;&#22815;&#20174;&#24494;&#35266;&#28151;&#20081;-&#20854;&#20013;&#32593;&#32476;&#25299;&#25169;&#21644;&#33410;&#28857;&#21160;&#24577;&#20132;&#32455;-&#36716;&#21464;&#21644;&#28436;&#21270;&#20026;&#20855;&#26377;&#29305;&#23450;&#38598;&#20307;&#34892;&#20026;&#30340;&#23439;&#35266;&#31209;&#24207;&#12290;&#22312;&#36807;&#21435;&#30340;&#20108;&#21313;&#24180;&#37324;&#65292;&#22797;&#26434;&#32593;&#32476;&#31185;&#23398;&#26174;&#33879;&#22686;&#24378;&#20102;&#25105;&#20204;&#23545;&#30495;&#23454;&#19990;&#30028;&#32593;&#32476;&#28508;&#22312;&#26426;&#21046;&#12289;&#32467;&#26500;&#21644;&#21160;&#24577;&#30340;&#29702;&#35299;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#36825;&#20123;&#36827;&#23637;&#65292;&#20294;&#22312;&#25506;&#32034;&#26356;&#21152;&#30495;&#23454;&#31995;&#32479;&#21644;&#25552;&#21319;&#23454;&#38469;&#24212;&#29992;&#26041;&#38754;&#20173;&#28982;&#23384;&#22312;&#30528;&#30456;&#24403;&#22823;&#30340;&#25361;&#25112;&#12290;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#30340;&#20986;&#29616;&#65292;&#20197;&#21450;&#20016;&#23500;&#22810;&#26679;&#30340;&#30495;&#23454;&#19990;&#30028;&#32593;&#32476;&#25968;&#25454;&#30340;&#23384;&#22312;&#65292;&#24320;&#21551;&#20102;&#22797;&#26434;&#32593;&#32476;&#31185;&#23398;&#30740;&#31350;&#30340;&#26032;&#26102;&#20195;&#12290;&#26412;&#35843;&#26597;&#26088;&#22312;&#31995;&#32479;&#22320;&#25506;&#35752;&#20154;&#24037;&#26234;&#33021;&#22312;&#20811;&#26381;&#22797;&#26434;&#32593;&#32476;&#31185;&#23398;&#30740;&#31350;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#26041;&#38754;&#30340;&#28508;&#22312;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16887v1 Announce Type: cross  Abstract: Complex networks pervade various real-world systems, from the natural environment to human societies. The essence of these networks is in their ability to transition and evolve from microscopic disorder-where network topology and node dynamics intertwine-to a macroscopic order characterized by certain collective behaviors. Over the past two decades, complex network science has significantly enhanced our understanding of the statistical mechanics, structures, and dynamics underlying real-world networks. Despite these advancements, there remain considerable challenges in exploring more realistic systems and enhancing practical applications. The emergence of artificial intelligence (AI) technologies, coupled with the abundance of diverse real-world network data, has heralded a new era in complex network science research. This survey aims to systematically address the potential advantages of AI in overcoming the lingering challenges of com
&lt;/p&gt;</description></item><item><title>&#21521;&#37327;&#25968;&#25454;&#24211;&#21644;&#23884;&#20837;&#27169;&#22411;&#30340;&#24212;&#29992;&#20026;&#25991;&#26412;&#20998;&#31867;&#22120;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#26041;&#24335;&#26469;&#34920;&#36798;&#25968;&#25454;&#27169;&#24335;&#65292;&#29305;&#21035;&#26159;&#22312;&#21307;&#30103;&#39046;&#22495;&#20013;&#24320;&#22987;&#26377;&#30528;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.16886</link><description>&lt;p&gt;
&#20351;&#29992;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#21644;&#21521;&#37327;&#25968;&#25454;&#24211;&#20316;&#20026;&#25991;&#26412;&#20998;&#31867;&#22120;&#30340;&#30740;&#31350;&#65292;&#20197;&#21307;&#30103;&#25968;&#25454;&#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
Using text embedding models and vector databases as text classifiers with the example of medical data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16886
&lt;/p&gt;
&lt;p&gt;
&#21521;&#37327;&#25968;&#25454;&#24211;&#21644;&#23884;&#20837;&#27169;&#22411;&#30340;&#24212;&#29992;&#20026;&#25991;&#26412;&#20998;&#31867;&#22120;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#26041;&#24335;&#26469;&#34920;&#36798;&#25968;&#25454;&#27169;&#24335;&#65292;&#29305;&#21035;&#26159;&#22312;&#21307;&#30103;&#39046;&#22495;&#20013;&#24320;&#22987;&#26377;&#30528;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#26159;&#20196;&#20154;&#20852;&#22859;&#30340;&#65292;&#24182;&#24050;&#22312;&#35768;&#22810;&#39046;&#22495;&#25214;&#21040;&#24212;&#29992;&#65292;&#20294;&#36890;&#24120;&#24773;&#20917;&#19979;&#65292;&#21307;&#23398;&#39046;&#22495;&#30340;&#26631;&#20934;&#35201;&#27714;&#38750;&#24120;&#39640;&#12290;&#19982;LLMs&#37197;&#21512;&#20351;&#29992;&#65292;&#21521;&#37327;&#23884;&#20837;&#27169;&#22411;&#21644;&#21521;&#37327;&#25968;&#25454;&#24211;&#25552;&#20379;&#20102;&#19968;&#31181;&#24378;&#22823;&#30340;&#26041;&#24335;&#26469;&#34920;&#36798;&#21508;&#31181;&#25968;&#25454;&#27169;&#24335;&#65292;&#36825;&#20123;&#25968;&#25454;&#27169;&#24335;&#23481;&#26131;&#34987;&#20856;&#22411;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25152;&#29702;&#35299;&#12290;&#38500;&#20102;&#26041;&#20415;&#22320;&#21521;&#36825;&#20123;&#21521;&#37327;&#25968;&#25454;&#24211;&#28155;&#21152;&#20449;&#24687;&#12289;&#30693;&#35782;&#21644;&#25968;&#25454;&#22806;&#65292;&#23427;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#20196;&#20154;&#20449;&#26381;&#30340;&#29702;&#30001;&#65292;&#21363;&#23558;&#20854;&#24212;&#29992;&#20110;&#36890;&#24120;&#30001;&#20154;&#31867;&#23436;&#25104;&#30340;&#26816;&#32034;&#20449;&#24687;&#20219;&#21153;&#30340;&#21508;&#31181;&#39046;&#22495;&#12290;Google&#30340;&#30740;&#31350;&#20154;&#21592;&#24320;&#21457;&#20102;&#19968;&#20010;&#28165;&#26224;&#30340;&#26367;&#20195;&#27169;&#22411;Med-PaLM&#65292;&#19987;&#38376;&#26088;&#22312;&#19982;&#20020;&#24202;&#21307;&#24072;&#30340;&#21307;&#23398;&#30693;&#35782;&#27700;&#24179;&#21305;&#37197;&#12290;&#22312;&#35757;&#32451;&#20998;&#31867;&#22120;&#21644;&#24320;&#21457;&#27169;&#22411;&#26102;&#65292;&#20445;&#25345;&#20107;&#23454;&#21644;&#20943;&#23569;&#20559;&#35265;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#21521;&#37327;&#25968;&#25454;&#24211;&#21644;&#23884;&#20837;&#27169;&#22411;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16886v1 Announce Type: cross  Abstract: The advent of Large Language Models (LLMs) is promising and has found application in numerous fields, but as it often is with the medical field, the bar is typically quite high [5]. In tandem with LLMs, vector embedding models and vector databases provide a robust way of expressing numerous modes of data that are easily digestible by typical machine learning models. Along with the ease of adding information, knowledge, and data to these vector databases, they provide a compelling reason to apply them in numerous fields where the task of retrieving information is typically done by humans. Researchers at Google have developed a clear alternative model, Med-PaLM [6] specifically designed to match a clinician's level of accuracy when it comes to medical knowledge. When training classifiers, and developing models, it is imperative to maintain factuality and reduce bias [4]. Here, we explore the use of vector databases and embedding models a
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#39044;&#35757;&#32451;&#31574;&#30053;&#65292;&#24213;&#29289;&#33539;&#22260;&#23545;&#27604;&#23398;&#20064;&#65292;&#20197;&#23398;&#20064;&#36866;&#21512;&#21270;&#23398;&#21453;&#24212;&#24615;&#30340;&#21407;&#23376;&#34920;&#31034;&#12290;</title><link>https://arxiv.org/abs/2402.16882</link><description>&lt;p&gt;
&#24213;&#29289;&#33539;&#22260;&#23545;&#27604;&#23398;&#20064;&#65306;&#37325;&#26032;&#21033;&#29992;&#20154;&#31867;&#20559;&#35265;&#23398;&#20064;&#21407;&#23376;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Substrate Scope Contrastive Learning: Repurposing Human Bias to Learn Atomic Representations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16882
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#39044;&#35757;&#32451;&#31574;&#30053;&#65292;&#24213;&#29289;&#33539;&#22260;&#23545;&#27604;&#23398;&#20064;&#65292;&#20197;&#23398;&#20064;&#36866;&#21512;&#21270;&#23398;&#21453;&#24212;&#24615;&#30340;&#21407;&#23376;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#20998;&#23376;&#34920;&#31034;&#26159;&#20998;&#23376;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#20851;&#38190;&#27493;&#39588;&#65292;&#23545;&#24314;&#27169;&#25104;&#21151;&#20135;&#29983;&#26174;&#33879;&#24433;&#21709;&#65292;&#23588;&#20854;&#22312;&#25968;&#25454;&#31232;&#32570;&#24773;&#20917;&#19979;&#12290;&#24191;&#20041;&#39044;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#30340;&#27010;&#24565;&#25512;&#21160;&#20102;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#34507;&#30333;&#36136;&#24037;&#31243;&#31561;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#31867;&#20284;&#30340;&#26041;&#27861;&#22312;&#23567;&#26377;&#26426;&#20998;&#23376;&#26041;&#38754;&#24182;&#26410;&#21462;&#24471;&#31867;&#20284;&#30340;&#25104;&#21151;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#19968;&#31181;&#26032;&#39062;&#30340;&#39044;&#35757;&#32451;&#31574;&#30053;&#65292;&#21363;&#24213;&#29289;&#33539;&#22260;&#23545;&#27604;&#23398;&#20064;&#65292;&#23427;&#23398;&#20064;&#36866;&#21512;&#21270;&#23398;&#21453;&#24212;&#24615;&#30340;&#21407;&#23376;&#34920;&#31034;&#12290;&#36825;&#31181;&#26041;&#27861;&#20197;&#24050;&#21457;&#34920;&#30340;&#24213;&#29289;&#33539;&#22260;&#34920;&#20013;&#24213;&#29289;&#30340;&#20998;&#32452;&#21644;&#20135;&#29289;&#25910;&#29575;&#20316;&#20026;&#21270;&#23398;&#21453;&#24212;&#24615;&#30456;&#20284;&#24615;&#25110;&#19981;&#30456;&#20284;&#24615;&#30340;&#34913;&#37327;&#12290;&#25105;&#20204;&#20851;&#27880; CAS Content Collection &#20013;&#30340; 20,798 &#20010;&#33459;&#39321;&#21348;&#20195;&#28867;&#65292;&#28085;&#30422;&#25968;&#21315;&#31687;&#20986;&#29256;&#29289;&#65292;&#20197;&#23398;&#20064;&#33459;&#39321;&#21348;&#20195;&#28867;&#30340;&#21453;&#24212;&#24615;&#34920;&#31034;&#12290;&#25105;&#20204;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16882v1 Announce Type: cross  Abstract: Learning molecular representation is a critical step in molecular machine learning that significantly influences modeling success, particularly in data-scarce situations. The concept of broadly pre-training neural networks has advanced fields such as computer vision, natural language processing, and protein engineering. However, similar approaches for small organic molecules have not achieved comparable success. In this work, we introduce a novel pre-training strategy, substrate scope contrastive learning, which learns atomic representations tailored to chemical reactivity. This method considers the grouping of substrates and their yields in published substrate scope tables as a measure of their similarity or dissimilarity in terms of chemical reactivity. We focus on 20,798 aryl halides in the CAS Content Collection spanning thousands of publications to learn a representation of aryl halide reactivity. We validate our pre-training appr
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BESA&#30340;&#26032;&#22411;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20462;&#21098;&#25216;&#26415;&#65292;&#36890;&#36807;&#24212;&#29992;&#20998;&#22359;&#37325;&#26500;&#25439;&#22833;&#65292;&#19982;&#20256;&#32479;&#30340;&#36880;&#23618;&#20462;&#21098;&#25216;&#26415;&#19981;&#21516;&#65292;BESA&#20855;&#26377;&#20248;&#21183;</title><link>https://arxiv.org/abs/2402.16880</link><description>&lt;p&gt;
BESA: &#20351;&#29992;&#20998;&#22359;&#21442;&#25968;&#39640;&#25928;&#31232;&#30095;&#20998;&#37197;&#20462;&#21098;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
BESA: Pruning Large Language Models with Blockwise Parameter-Efficient Sparsity Allocation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16880
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BESA&#30340;&#26032;&#22411;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20462;&#21098;&#25216;&#26415;&#65292;&#36890;&#36807;&#24212;&#29992;&#20998;&#22359;&#37325;&#26500;&#25439;&#22833;&#65292;&#19982;&#20256;&#32479;&#30340;&#36880;&#23618;&#20462;&#21098;&#25216;&#26415;&#19981;&#21516;&#65292;BESA&#20855;&#26377;&#20248;&#21183;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#25991;&#26412;&#25688;&#35201;&#12289;&#25991;&#26412;&#38382;&#31572;&#31561;&#21508;&#31181;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#23613;&#31649;&#23427;&#20204;&#30340;&#24615;&#33021;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#65292;&#20294;&#30001;&#20110;&#22823;&#37327;&#21442;&#25968;&#36896;&#25104;&#30340;&#35745;&#31639;&#21344;&#29992;&#21487;&#33021;&#26159;&#31105;&#38178;&#30340;&#12290;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#65288;&#22914;SparseGPT&#21644;Wanda&#65289;&#23581;&#35797;&#36890;&#36807;&#26435;&#37325;&#20462;&#21098;&#32531;&#35299;&#27492;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#36880;&#23618;&#26041;&#27861;&#20250;&#23548;&#33268;&#27169;&#22411;&#36755;&#20986;&#26174;&#33879;&#25200;&#21160;&#65292;&#24182;&#38656;&#35201;&#32454;&#33268;&#30340;&#36229;&#21442;&#25968;&#35843;&#25972;&#65292;&#22914;&#20462;&#21098;&#36895;&#29575;&#65292;&#36825;&#21487;&#33021;&#20250;&#23545;&#25972;&#20307;&#27169;&#22411;&#24615;&#33021;&#20135;&#29983;&#19981;&#21033;&#24433;&#21709;&#12290;&#20026;&#35299;&#20915;&#27492;&#38382;&#39064;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;LLM&#20462;&#21098;&#25216;&#26415;&#65292;&#31216;&#20026;&#20998;&#22359;&#21442;&#25968;&#39640;&#25928;&#31232;&#30095;&#20998;&#37197;&#65288;BESA&#65289;&#65292;&#36890;&#36807;&#24212;&#29992;&#20998;&#22359;&#37325;&#26500;&#25439;&#22833;&#12290;&#19982;&#20856;&#22411;&#30340;&#36880;&#23618;&#20462;&#21098;&#25216;&#26415;&#30456;&#27604;&#65292;BESA&#20855;&#26377;&#20004;&#20010;&#29420;&#29305;&#30340;&#29305;&#28857;&#65306;i&#65289;&#23427;&#23450;&#20301;&#20110;&#25972;&#20307;&#20462;&#21098;&#35823;&#24046;&#30456;&#23545;&#20110;&#27599;&#20010;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16880v1 Announce Type: cross  Abstract: Large language models (LLMs) have demonstrated outstanding performance in various tasks, such as text summarization, text question-answering, and etc. While their performance is impressive, the computational footprint due to their vast number of parameters can be prohibitive. Existing solutions such as SparseGPT and Wanda attempt to alleviate this issue through weight pruning. However, their layer-wise approach results in significant perturbation to the model's output and requires meticulous hyperparameter tuning, such as the pruning rate, which can adversely affect overall model performance. To address this, this paper introduces a novel LLM pruning technique dubbed blockwise parameter-efficient sparsity allocation (BESA) by applying a blockwise reconstruction loss. In contrast to the typical layer-wise pruning techniques, BESA is characterized by two distinctive attributes: i) it targets the overall pruning error with respect to indi
&lt;/p&gt;</description></item><item><title>EvoGPT-f&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#36827;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#23545;&#20116;&#20010;&#24418;&#24335;&#25968;&#23398;&#35821;&#26009;&#24211;&#36827;&#34892;&#24046;&#24322;&#26426;&#22120;&#21487;&#23398;&#20064;&#24615;&#30340;&#31995;&#32479;&#37327;&#21270;&#20998;&#26512;&#65292;&#20026;&#24418;&#24335;&#25968;&#23398;&#35821;&#35328;&#30340;&#22522;&#20934;&#27979;&#35797;&#25552;&#20379;&#20102;&#26032;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.16878</link><description>&lt;p&gt;
EvoGPT-f: &#19968;&#31181;&#29992;&#20110;&#22522;&#20934;&#27979;&#35797;&#24418;&#24335;&#25968;&#23398;&#35821;&#35328;&#30340;&#36827;&#21270;GPT&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
EvoGPT-f: An Evolutionary GPT Framework for Benchmarking Formal Math Languages
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16878
&lt;/p&gt;
&lt;p&gt;
EvoGPT-f&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#36827;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#23545;&#20116;&#20010;&#24418;&#24335;&#25968;&#23398;&#35821;&#26009;&#24211;&#36827;&#34892;&#24046;&#24322;&#26426;&#22120;&#21487;&#23398;&#20064;&#24615;&#30340;&#31995;&#32479;&#37327;&#21270;&#20998;&#26512;&#65292;&#20026;&#24418;&#24335;&#25968;&#23398;&#35821;&#35328;&#30340;&#22522;&#20934;&#27979;&#35797;&#25552;&#20379;&#20102;&#26032;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16878v1 &#20844;&#21578;&#31867;&#22411;: &#26032;&#30340; &#25688;&#35201;: &#24418;&#24335;&#25968;&#23398;&#26159;&#23558;&#25968;&#23398;&#36716;&#21270;&#20026;&#32534;&#31243;&#35821;&#35328;&#30340;&#23398;&#31185;&#65292;&#22312;&#36825;&#31181;&#32534;&#31243;&#35821;&#35328;&#20013;&#65292;&#20219;&#20309;&#38472;&#36848;&#37117;&#21487;&#20197;&#34987;&#35745;&#31639;&#26426;&#26126;&#30830;&#22320;&#26816;&#26597;&#12290;&#25968;&#23398;&#23478;&#21644;&#35745;&#31639;&#26426;&#31185;&#23398;&#23478;&#33457;&#36153;&#20102;&#25968;&#21313;&#24180;&#36827;&#34892;&#33392;&#33510;&#30340;&#24418;&#24335;&#21270;&#24037;&#20316;&#65292;&#24320;&#21457;&#20102;&#35832;&#22914;Coq&#12289;HOL&#21644;Lean&#31561;&#35821;&#35328;&#12290;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#24050;&#32463;&#27719;&#38598;&#21040;&#36825;&#20123;&#24418;&#24335;&#21270;&#25968;&#23398;&#35821;&#26009;&#24211;&#19978;&#65292;&#24182;&#20135;&#29983;&#20102;&#21508;&#31181;&#26041;&#27861;&#26469;&#24110;&#21161;&#20132;&#20114;&#24335;&#21644;&#33258;&#21160;&#23450;&#29702;&#35777;&#26126;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#35770;&#25991;&#20027;&#35201;&#38598;&#20013;&#22312;&#19968;&#20010;&#26041;&#27861;&#12289;&#19968;&#20010;&#35777;&#26126;&#20219;&#21153;&#12289;&#19968;&#20010;&#35821;&#35328;&#19978;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;EvoGPT-f: &#19968;&#31181;&#26032;&#39062;&#30340;&#36827;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#39318;&#27425;&#31995;&#32479;&#37327;&#21270;&#20998;&#26512;&#20116;&#20010;&#24418;&#24335;&#25968;&#23398;&#35821;&#26009;&#24211;(Lean 3&#12289;Lean 4&#12289;Coq&#12289;HOL 4&#12289;HOL Light)&#30340;&#24046;&#24322;&#26426;&#22120;&#21487;&#23398;&#20064;&#24615;&#65292;&#20351;&#29992;&#22235;&#31181;&#35760;&#21495;&#21270;&#26041;&#27861;(&#23383;&#31526;&#12289;&#21333;&#35789;&#32423;&#12289;&#23383;&#33410;&#23545;&#32534;&#30721;&#21644;StarCoder&#35760;&#21495;&#21270;&#22120;)&#12290;&#26412;&#25991;&#24182;&#26410;&#32467;&#26463;&#20851;&#20110;&#8220;&#26368;&#20339;&#8221;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16878v1 Announce Type: new  Abstract: Formal mathematics is the discipline of translating mathematics into a programming language in which any statement can be unequivocally checked by a computer. Mathematicians and computer scientists have spent decades of painstaking formalization efforts developing languages such as Coq, HOL, and Lean. Machine learning research has converged on these formal math corpora and given rise to an assortment of methodologies to aid in interactive and automated theorem proving. However, these papers have primarily focused on one method, for one proof task, in one language. This paper introduces EvoGPT-f: a novel evolutionary framework for the first systematic quantitative analysis of the differential machine learnability of five formal math corpora (Lean 3, Lean 4, Coq, HOL 4, HOL Light) using four tokenization methods (character, word-level, Byte Pair Encoding and StarCoder tokenizer). This paper does not put to rest the question of the "best" o
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21033;&#29992;&#29983;&#25104;&#33021;&#21147;&#26469;&#21512;&#25104;&#20551;&#35774;&#32451;&#20064;&#65292;&#20197;&#24357;&#21512;&#23398;&#20064;&#32773;&#38656;&#27714;&#19982;&#32451;&#20064;&#20869;&#23481;&#20043;&#38388;&#30340;&#35821;&#20041;&#40511;&#27807;&#65292;&#25552;&#39640;&#20010;&#24615;&#21270;&#35821;&#35328;&#23398;&#20064;&#32451;&#20064;&#26816;&#32034;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.16877</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#30340;&#20010;&#24615;&#21270;&#35821;&#35328;&#23398;&#20064;&#32451;&#20064;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Large Language Model Augmented Exercise Retrieval for Personalized Language Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16877
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21033;&#29992;&#29983;&#25104;&#33021;&#21147;&#26469;&#21512;&#25104;&#20551;&#35774;&#32451;&#20064;&#65292;&#20197;&#24357;&#21512;&#23398;&#20064;&#32773;&#38656;&#27714;&#19982;&#32451;&#20064;&#20869;&#23481;&#20043;&#38388;&#30340;&#35821;&#20041;&#40511;&#27807;&#65292;&#25552;&#39640;&#20010;&#24615;&#21270;&#35821;&#35328;&#23398;&#20064;&#32451;&#20064;&#26816;&#32034;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#32447;&#35821;&#35328;&#23398;&#20064;&#29615;&#22659;&#20013;&#30340;&#38646;&#26679;&#26412;&#32451;&#20064;&#26816;&#32034;&#38382;&#39064;&#65292;&#20197;&#36171;&#20104;&#23398;&#20064;&#32773;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#26126;&#30830;&#35831;&#27714;&#20010;&#24615;&#21270;&#32451;&#20064;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#25910;&#38598;&#33258;&#35821;&#35328;&#23398;&#20064;&#32773;&#30340;&#30495;&#23454;&#25968;&#25454;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#30690;&#37327;&#30456;&#20284;&#24615;&#26041;&#27861;&#24456;&#38590;&#25429;&#25417;&#32451;&#20064;&#20869;&#23481;&#19982;&#23398;&#20064;&#32773;&#29992;&#20110;&#34920;&#36798;&#20182;&#20204;&#24819;&#35201;&#23398;&#20064;&#20869;&#23481;&#30340;&#35821;&#35328;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#29983;&#25104;&#33021;&#21147;&#26469;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#36890;&#36807;&#22522;&#20110;&#23398;&#20064;&#32773;&#36755;&#20837;&#21512;&#25104;&#20551;&#35774;&#32451;&#20064;&#65292;&#28982;&#21518;&#29992;&#20110;&#25628;&#32034;&#30456;&#20851;&#32451;&#20064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;mHyER&#20811;&#26381;&#20102;&#19977;&#20010;&#25361;&#25112;&#65306;&#65288;1&#65289;&#32570;&#20047;&#29992;&#20110;&#35757;&#32451;&#30340;&#30456;&#20851;&#24615;&#26631;&#31614;&#65292;&#65288;2&#65289;&#21463;&#38480;&#30340;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16877v1 Announce Type: cross  Abstract: We study the problem of zero-shot exercise retrieval in the context of online language learning, to give learners the ability to explicitly request personalized exercises via natural language. Using real-world data collected from language learners, we observe that vector similarity approaches poorly capture the relationship between exercise content and the language that learners use to express what they want to learn. This semantic gap between queries and content dramatically reduces the effectiveness of general-purpose retrieval models pretrained on large scale information retrieval datasets like MS MARCO. We leverage the generative capabilities of large language models to bridge the gap by synthesizing hypothetical exercises based on the learner's input, which are then used to search for relevant exercises. Our approach, which we call mHyER, overcomes three challenges: (1) lack of relevance labels for training, (2) unrestricted learn
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#20219;&#21153;&#65306;&#23398;&#26415;&#22242;&#38431;&#25104;&#21592;&#25512;&#33616;&#27169;&#22411;&#65292;&#36890;&#36807;&#32467;&#21512;&#26597;&#35810;&#21644;&#35770;&#25991;&#19978;&#19979;&#25991;&#19982;&#22270;&#25299;&#25169;&#32467;&#26500;&#65292;&#24418;&#25104;&#36866;&#29992;&#20110;&#29305;&#23450;&#30740;&#31350;&#20852;&#36259;&#21644;&#20219;&#21153;&#30340;&#26032;&#22270;&#65288;CQBG-R&#65289;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#36825;&#31181;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.16876</link><description>&lt;p&gt;
&#39640;&#32423;&#23398;&#26415;&#22242;&#38431;&#25104;&#21592;&#25512;&#33616;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Advanced Academic Team Worker Recommendation Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16876
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#20219;&#21153;&#65306;&#23398;&#26415;&#22242;&#38431;&#25104;&#21592;&#25512;&#33616;&#27169;&#22411;&#65292;&#36890;&#36807;&#32467;&#21512;&#26597;&#35810;&#21644;&#35770;&#25991;&#19978;&#19979;&#25991;&#19982;&#22270;&#25299;&#25169;&#32467;&#26500;&#65292;&#24418;&#25104;&#36866;&#29992;&#20110;&#29305;&#23450;&#30740;&#31350;&#20852;&#36259;&#21644;&#20219;&#21153;&#30340;&#26032;&#22270;&#65288;CQBG-R&#65289;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#36825;&#31181;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21512;&#20316;&#20249;&#20276;&#25512;&#33616;&#26159;&#23398;&#26415;&#39046;&#22495;&#20013;&#30340;&#37325;&#35201;&#20219;&#21153;&#12290;&#29616;&#26377;&#26041;&#27861;&#22823;&#22810;&#20551;&#35774;&#25512;&#33616;&#31995;&#32479;&#20165;&#38656;&#20026;&#20219;&#21153;&#25512;&#33616;&#29305;&#23450;&#30340;&#30740;&#31350;&#20154;&#21592;&#12290;&#28982;&#32780;&#65292;&#23398;&#26415;&#25104;&#21151;&#21487;&#33021;&#24402;&#21151;&#20110;&#25972;&#20010;&#23398;&#26415;&#22242;&#38431;&#30340;&#39640;&#25928;&#21512;&#20316;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#20219;&#21153;&#65306;&#23398;&#26415;&#22242;&#38431;&#25104;&#21592;&#25512;&#33616;&#12290;&#26681;&#25454;&#32473;&#23450;&#30340;&#36523;&#20221;&#65288;&#23398;&#29983;&#65292;&#21161;&#29702;&#25945;&#25480;&#25110;&#20027;&#25945;&#25480;&#65289;&#12289;&#30740;&#31350;&#20852;&#36259;&#21644;&#29305;&#23450;&#20219;&#21153;&#65292;&#25105;&#20204;&#21487;&#20197;&#25512;&#33616;&#19968;&#20010;&#30001;&#65288;&#20027;&#25945;&#25480;&#65292;&#21161;&#29702;&#25945;&#25480;&#65292;&#23398;&#29983;&#65289;&#32452;&#25104;&#30340;&#23398;&#26415;&#22242;&#38431;&#12290;&#20026;&#20102;&#23436;&#25104;&#36825;&#20010;&#20219;&#21153;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;CQBG-R&#65288;&#24341;&#29992;-&#26597;&#35810;&#28151;&#21512;&#22270;-&#25490;&#21517;&#65289;&#30340;&#27169;&#22411;&#12290;&#20854;&#20851;&#38190;&#24605;&#24819;&#26159;&#23558;&#26597;&#35810;&#21644;&#35770;&#25991;&#30340;&#19978;&#19979;&#25991;&#19982;&#22270;&#25299;&#25169;&#32467;&#26500;&#32467;&#21512;&#36215;&#26469;&#24418;&#25104;&#19968;&#20010;&#26032;&#30340;&#22270;&#65288;CQBG&#65289;&#65292;&#36825;&#20010;&#22270;&#21487;&#20197;&#38024;&#23545;&#36825;&#27425;&#30340;&#30740;&#31350;&#20852;&#36259;&#21644;&#29305;&#23450;&#30740;&#31350;&#20219;&#21153;&#12290;&#23454;&#39564;&#32467;&#26524;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16876v1 Announce Type: cross  Abstract: Collaborator recommendation is an important task in academic domain. Most of the existing approaches have the assumption that the recommendation system only need to recommend a specific researcher for the task. However, academic successes can be owed to productive collaboration of a whole academic team. In this work, we propose a new task: academic team worker recommendation: with a given status: student, assistant professor or prime professor, research interests and specific task, we can recommend an academic team formed as (prime professor, assistant professor, student). For this task, we propose a model CQBG-R(Citation-Query Blended Graph-Ranking). The key ideas is to combine the context of the query and the papers with the graph topology to form a new graph(CQBG), which can target at the research interests and the specific research task for this time. The experiment results show the effectiveness of the proposed method.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#25216;&#26415;&#35299;&#20915;&#20102;&#35821;&#35328;&#29983;&#25104;&#20013;&#8220;&#24187;&#35273;&#8221;&#38382;&#39064;&#65292;&#24182;&#20511;&#21161;&#26597;&#35810;&#20248;&#21270;&#36807;&#31243;&#23558;&#29992;&#25143;&#26597;&#35810;&#19982;&#39640;&#32423;&#35821;&#35328;&#27169;&#22411;&#36830;&#25509;&#65292;&#26174;&#33879;&#25552;&#21319;&#20102;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.16874</link><description>&lt;p&gt;
&#36890;&#36807;&#22686;&#24378;&#26597;&#35810;&#25552;&#21319;&#35821;&#35328;&#29983;&#25104;&#30340;&#26816;&#32034;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
Enhancing Retrieval Processes for Language Generation with Augmented Queries
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16874
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#25216;&#26415;&#35299;&#20915;&#20102;&#35821;&#35328;&#29983;&#25104;&#20013;&#8220;&#24187;&#35273;&#8221;&#38382;&#39064;&#65292;&#24182;&#20511;&#21161;&#26597;&#35810;&#20248;&#21270;&#36807;&#31243;&#23558;&#29992;&#25143;&#26597;&#35810;&#19982;&#39640;&#32423;&#35821;&#35328;&#27169;&#22411;&#36830;&#25509;&#65292;&#26174;&#33879;&#25552;&#21319;&#20102;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26234;&#33021;&#25216;&#26415;&#26085;&#26032;&#26376;&#24322;&#30340;&#19990;&#30028;&#20013;&#65292;&#30001;&#20110;&#20808;&#36827;&#35821;&#35328;&#27169;&#22411;&#30340;&#23835;&#36215;&#65292;&#25628;&#32034;&#25991;&#26723;&#21464;&#24471;&#26356;&#20855;&#25361;&#25112;&#24615;&#12290;&#36825;&#20123;&#27169;&#22411;&#26377;&#26102;&#20250;&#38754;&#20020;&#22256;&#38590;&#65292;&#27604;&#22914;&#25552;&#20379;&#19981;&#20934;&#30830;&#30340;&#20449;&#24687;&#65292;&#36890;&#24120;&#34987;&#31216;&#20026;&#8220;&#24187;&#35273;&#8221;&#12290;&#26412;&#30740;&#31350;&#33268;&#21147;&#20110;&#36890;&#36807;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#25216;&#26415;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#35813;&#25216;&#26415;&#25351;&#23548;&#27169;&#22411;&#22522;&#20110;&#30495;&#23454;&#20107;&#23454;&#25552;&#20379;&#20934;&#30830;&#31572;&#22797;&#12290;&#20026;&#20102;&#35299;&#20915;&#21487;&#20280;&#32553;&#24615;&#38382;&#39064;&#65292;&#30740;&#31350;&#25506;&#35752;&#20102;&#23558;&#29992;&#25143;&#26597;&#35810;&#19982;&#35832;&#22914;BERT&#21644;Orca2&#31561;&#22797;&#26434;&#35821;&#35328;&#27169;&#22411;&#36830;&#25509;&#36215;&#26469;&#30340;&#21019;&#26032;&#26597;&#35810;&#20248;&#21270;&#36807;&#31243;&#12290;&#30740;&#31350;&#23637;&#24320;&#22312;&#19977;&#31181;&#24773;&#22659;&#20013;&#65306;&#39318;&#20808;&#65292;&#27809;&#26377;RAG&#65292;&#20854;&#27425;&#65292;&#27809;&#26377;&#39069;&#22806;&#24110;&#21161;&#65292;&#26368;&#21518;&#65292;&#21152;&#20837;&#39069;&#22806;&#24110;&#21161;&#12290;&#36873;&#25321;&#32039;&#20945;&#32780;&#39640;&#25928;&#30340;Orca2 7B&#27169;&#22411;&#23637;&#31034;&#20986;&#23545;&#35745;&#31639;&#36164;&#28304;&#30340;&#26234;&#33021;&#20351;&#29992;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#21021;&#22987;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#26377;&#20102;&#26174;&#33879;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16874v1 Announce Type: cross  Abstract: In the rapidly changing world of smart technology, searching for documents has become more challenging due to the rise of advanced language models. These models sometimes face difficulties, like providing inaccurate information, commonly known as "hallucination." This research focuses on addressing this issue through Retrieval-Augmented Generation (RAG), a technique that guides models to give accurate responses based on real facts. To overcome scalability issues, the study explores connecting user queries with sophisticated language models such as BERT and Orca2, using an innovative query optimization process. The study unfolds in three scenarios: first, without RAG, second, without additional assistance, and finally, with extra help. Choosing the compact yet efficient Orca2 7B model demonstrates a smart use of computing resources. The empirical results indicate a significant improvement in the initial language model's performance unde
&lt;/p&gt;</description></item><item><title>Bike3S&#26159;&#19968;&#20010;&#38024;&#23545;&#22522;&#20110;&#31449;&#28857;&#30340;&#33258;&#34892;&#36710;&#20849;&#20139;&#31995;&#32479;&#30340;&#27169;&#25311;&#22120;&#65292;&#21487;&#20197;&#36827;&#34892;&#21322;&#29616;&#23454;&#30340;&#20195;&#29702;&#22522;&#30784;&#27169;&#25311;&#65292;&#24110;&#21161;&#35780;&#20272;&#21644;&#27979;&#35797;&#19981;&#21516;&#31649;&#29702;&#20915;&#31574;&#21644;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2402.16871</link><description>&lt;p&gt;
Bike3S&#65306;&#33258;&#34892;&#36710;&#20849;&#20139;&#31995;&#32479;&#27169;&#25311;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
Bike3S: A Tool for Bike Sharing Systems Simulation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16871
&lt;/p&gt;
&lt;p&gt;
Bike3S&#26159;&#19968;&#20010;&#38024;&#23545;&#22522;&#20110;&#31449;&#28857;&#30340;&#33258;&#34892;&#36710;&#20849;&#20139;&#31995;&#32479;&#30340;&#27169;&#25311;&#22120;&#65292;&#21487;&#20197;&#36827;&#34892;&#21322;&#29616;&#23454;&#30340;&#20195;&#29702;&#22522;&#30784;&#27169;&#25311;&#65292;&#24110;&#21161;&#35780;&#20272;&#21644;&#27979;&#35797;&#19981;&#21516;&#31649;&#29702;&#20915;&#31574;&#21644;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36710;&#36742;&#20849;&#20139;&#31995;&#32479;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#36825;&#20123;&#31995;&#32479;&#30340;&#26377;&#25928;&#24615;&#21462;&#20915;&#20110;&#19981;&#21516;&#30340;&#25112;&#30053;&#21644;&#36816;&#33829;&#31649;&#29702;&#20915;&#31574;&#21644;&#25919;&#31574;&#65292;&#27604;&#22914;&#36710;&#38431;&#35268;&#27169;&#25110;&#36710;&#36742;&#20998;&#24067;&#12290;&#33021;&#22815;&#39044;&#27979;&#21644;&#35780;&#20272;&#36825;&#20123;&#31574;&#30053;&#30340;&#28508;&#22312;&#24433;&#21709;&#33267;&#20851;&#37325;&#35201;&#65292;&#28982;&#21518;&#25165;&#33021;&#25104;&#21151;&#37096;&#32626;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;Bike3S&#65292;&#19968;&#20010;&#29992;&#20110;&#22522;&#20110;&#31449;&#28857;&#30340;&#33258;&#34892;&#36710;&#20849;&#20139;&#31995;&#32479;&#30340;&#27169;&#25311;&#22120;&#12290;&#35813;&#27169;&#25311;&#22120;&#25191;&#34892;&#21322;&#29616;&#23454;&#30340;&#33258;&#34892;&#36710;&#20849;&#20139;&#31995;&#32479;&#25805;&#20316;&#27169;&#25311;&#65292;&#24182;&#20801;&#35768;&#35780;&#20272;&#21644;&#27979;&#35797;&#19981;&#21516;&#30340;&#31649;&#29702;&#20915;&#31574;&#21644;&#31574;&#30053;&#12290;&#29305;&#21035;&#26159;&#65292;&#35813;&#27169;&#25311;&#22120;&#34987;&#35774;&#35745;&#29992;&#20110;&#27979;&#35797;&#19981;&#21516;&#30340;&#31449;&#28857;&#23481;&#37327;&#12289;&#31449;&#28857;&#20998;&#24067;&#21644;&#24179;&#34913;&#31574;&#30053;&#12290;&#35813;&#27169;&#25311;&#22120;&#36827;&#34892;&#24494;&#35266;&#22522;&#20110;&#20195;&#29702;&#30340;&#27169;&#25311;&#65292;&#21487;&#20197;&#23450;&#20041;&#19981;&#21516;&#31867;&#22411;&#30340;&#29992;&#25143;&#65292;&#26681;&#25454;&#20854;&#33258;&#36523;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16871v1 Announce Type: cross  Abstract: Vehicle sharing systems are becoming increasingly popular. The effectiveness of such systems depends, among other factors, on different strategic and operational management decisions and policies, like the dimension of the fleet or the distribution of vehicles. It is of foremost importance to be able to anticipate and evaluate the potential effects of such strategies before they can be successfully deployed. In this paper we present Bike3S, a simulator for a station-based bike sharing system. The simulator performs semi-realistic simulations of the operation of a bike sharing system and allows for evaluating and testing different management decisions and strategies. In particular, the simulator has been designed to test different station capacities, station distributions, and balancing strategies. The simulator carries out microscopic agent-based simulations, where users of different types can be defined that act according to their ind
&lt;/p&gt;</description></item><item><title>&#26410;&#26469;&#27431;&#27954;&#20154;&#24037;&#26234;&#33021;&#26631;&#20934;&#21270;&#24212;&#32771;&#34385;&#22522;&#26412;&#26435;&#21033;&#65292;&#20197;&#32531;&#35299;&#26576;&#20123;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#24102;&#26469;&#30340;&#39640;&#39118;&#38505;&#65292;&#21453;&#26144;&#22522;&#26412;&#26435;&#21033;&#32771;&#34385;&#65292;&#24182;&#35299;&#20915;&#23545;&#27431;&#27954;&#26631;&#20934;&#21270;&#36807;&#31243;&#30340;&#25209;&#35780;&#12290;</title><link>https://arxiv.org/abs/2402.16869</link><description>&lt;p&gt;
&#22312;&#27431;&#27954;&#20154;&#24037;&#26234;&#33021;&#26631;&#20934;&#21270;&#20013;&#32771;&#34385;&#22522;&#26412;&#26435;&#21033;&#65306;&#32993;&#25199;&#36824;&#26159;&#25112;&#30053;&#32852;&#30431;&#65311;
&lt;/p&gt;
&lt;p&gt;
Considering Fundamental Rights in the European Standardisation of Artificial Intelligence: Nonsense or Strategic Alliance?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16869
&lt;/p&gt;
&lt;p&gt;
&#26410;&#26469;&#27431;&#27954;&#20154;&#24037;&#26234;&#33021;&#26631;&#20934;&#21270;&#24212;&#32771;&#34385;&#22522;&#26412;&#26435;&#21033;&#65292;&#20197;&#32531;&#35299;&#26576;&#20123;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#24102;&#26469;&#30340;&#39640;&#39118;&#38505;&#65292;&#21453;&#26144;&#22522;&#26412;&#26435;&#21033;&#32771;&#34385;&#65292;&#24182;&#35299;&#20915;&#23545;&#27431;&#27954;&#26631;&#20934;&#21270;&#36807;&#31243;&#30340;&#25209;&#35780;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#27431;&#27954;&#32972;&#26223;&#19979;&#65292;&#27431;&#30431;&#20154;&#24037;&#26234;&#33021;&#27861;&#26696;&#25552;&#26696;&#21644;&#26377;&#20851;&#23433;&#20840;&#21487;&#20449;&#20154;&#24037;&#26234;&#33021;&#30340;&#26631;&#20934;&#21270;&#35831;&#27714;&#33609;&#26696;&#65292;&#23558;&#26631;&#20934;&#21270;&#19982;&#22522;&#26412;&#26435;&#21033;&#32852;&#31995;&#36215;&#26469;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25991;&#26412;&#24182;&#26410;&#25552;&#20379;&#20219;&#20309;&#25351;&#23548;&#26041;&#38024;&#65292;&#26126;&#30830;&#38416;&#26126;&#20154;&#24037;&#26234;&#33021;&#26631;&#20934;&#19982;&#22522;&#26412;&#26435;&#21033;&#20043;&#38388;&#30340;&#20851;&#31995;&#12289;&#21547;&#20041;&#25110;&#24433;&#21709;&#12290;&#26412;&#31456;&#26088;&#22312;&#28548;&#28165;&#36825;&#19968;&#37325;&#35201;&#30340;&#30417;&#31649;&#30450;&#28857;&#12290;&#20027;&#35201;&#35752;&#35770;&#30340;&#38382;&#39064;&#26159;&#22522;&#20110;&#26410;&#26469;&#20154;&#24037;&#26234;&#33021;&#27861;&#26696;&#37319;&#32435;&#20154;&#24037;&#26234;&#33021;&#21327;&#35843;&#26631;&#20934;&#26102;&#26159;&#21542;&#24212;&#32771;&#34385;&#22522;&#26412;&#26435;&#21033;&#12290;&#22312;&#25105;&#20204;&#30475;&#26469;&#65292;&#31572;&#26696;&#26159;&#32943;&#23450;&#30340;&#12290;&#26576;&#20123;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#24102;&#26469;&#30340;&#39640;&#39118;&#38505;&#29305;&#21035;&#28041;&#21450;&#22522;&#26412;&#26435;&#21033;&#30340;&#20405;&#29359;&#12290;&#22240;&#27492;&#65292;&#20943;&#36731;&#36825;&#20123;&#39118;&#38505;&#28041;&#21450;&#22522;&#26412;&#26435;&#21033;&#32771;&#34385;&#65292;&#26410;&#26469;&#30340;&#21327;&#35843;&#26631;&#20934;&#24212;&#35813;&#21453;&#26144;&#36825;&#19968;&#28857;&#12290;&#21516;&#26102;&#65292;&#23545;&#27431;&#27954;&#26631;&#20934;&#21270;&#36807;&#31243;&#30340;&#26377;&#25928;&#25209;&#35780;&#24517;&#39035;&#24471;&#21040;&#35299;&#20915;&#12290;&#26368;&#21518;&#65292;&#23454;&#38469;&#25972;&#21512;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16869v1 Announce Type: cross  Abstract: In the European context, both the EU AI Act proposal and the draft Standardisation Request on safe and trustworthy AI link standardisation to fundamental rights. However, these texts do not provide any guidelines that specify and detail the relationship between AI standards and fundamental rights, its meaning or implication. This chapter aims to clarify this critical regulatory blind spot. The main issue tackled is whether the adoption of AI harmonised standards, based on the future AI Act, should take into account fundamental rights. In our view, the response is yes. The high risks posed by certain AI systems relate in particular to infringements of fundamental rights. Therefore, mitigating such risks involves fundamental rights considerations and this is what future harmonised standards should reflect. At the same time, valid criticisms of the European standardisation process have to be addressed. Finally, the practical incorporation
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24378;&#22823;&#30340;&#30721;&#20070;&#36741;&#21161;&#22270;&#20687;&#35821;&#20041;&#36890;&#20449;&#31995;&#32479;&#65292;&#36890;&#36807;&#32852;&#21512;&#26500;&#24314;&#35821;&#20041;&#32534;&#35299;&#30721;&#22120;&#21644;&#30721;&#20070;&#12289;&#35774;&#35745;&#21521;&#37327;-&#32034;&#24341;&#21464;&#25442;&#22120;&#26469;&#23454;&#29616;&#22270;&#20687;&#29983;&#25104;&#65292;&#24182;&#19988;&#20511;&#21161;&#39640;&#36136;&#37327;&#30721;&#20070;&#24110;&#21161;Transformer&#65292;&#25552;&#39640;&#31995;&#32479;&#23545;&#25239;&#20449;&#36947;&#22122;&#22768;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.16868</link><description>&lt;p&gt;
&#30001;Transformer&#39537;&#21160;&#30340;&#31471;&#21040;&#31471;&#35821;&#20041;&#36890;&#20449;&#30340;&#30721;&#20070;&#29983;&#25104;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Codebook-enabled Generative End-to-end Semantic Communication Powered by Transformer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16868
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24378;&#22823;&#30340;&#30721;&#20070;&#36741;&#21161;&#22270;&#20687;&#35821;&#20041;&#36890;&#20449;&#31995;&#32479;&#65292;&#36890;&#36807;&#32852;&#21512;&#26500;&#24314;&#35821;&#20041;&#32534;&#35299;&#30721;&#22120;&#21644;&#30721;&#20070;&#12289;&#35774;&#35745;&#21521;&#37327;-&#32034;&#24341;&#21464;&#25442;&#22120;&#26469;&#23454;&#29616;&#22270;&#20687;&#29983;&#25104;&#65292;&#24182;&#19988;&#20511;&#21161;&#39640;&#36136;&#37327;&#30721;&#20070;&#24110;&#21161;Transformer&#65292;&#25552;&#39640;&#31995;&#32479;&#23545;&#25239;&#20449;&#36947;&#22122;&#22768;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#30721;&#20070;&#30340;&#29983;&#25104;&#24335;&#35821;&#20041;&#36890;&#20449;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#22240;&#20026;&#24403;&#30721;&#20070;&#22312;&#21457;&#36865;&#32773;&#21644;&#25509;&#25910;&#32773;&#20043;&#38388;&#20849;&#20139;&#26102;&#65292;&#21482;&#38656;&#35201;&#20256;&#36755;&#32034;&#24341;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#30721;&#21521;&#37327;&#20043;&#38388;&#30340;&#35821;&#20041;&#20851;&#31995;&#26410;&#24517;&#19982;&#23545;&#24212;&#30721;&#32034;&#24341;&#30340;&#36317;&#31163;&#30456;&#20851;&#65292;&#30721;&#20070;&#21551;&#29992;&#30340;&#35821;&#20041;&#36890;&#20449;&#31995;&#32479;&#24615;&#33021;&#23481;&#26131;&#21463;&#21040;&#20449;&#36947;&#22122;&#22768;&#30340;&#24433;&#21709;&#12290;&#22240;&#27492;&#65292;&#22914;&#20309;&#25552;&#39640;&#31995;&#32479;&#23545;&#25239;&#22122;&#22768;&#30340;&#40065;&#26834;&#24615;&#38656;&#35201;&#20180;&#32454;&#35774;&#35745;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24378;&#22823;&#30340;&#30721;&#20070;&#36741;&#21161;&#22270;&#20687;&#35821;&#20041;&#36890;&#20449;&#31995;&#32479;&#65292;&#20854;&#20013;&#39318;&#20808;&#32852;&#21512;&#26500;&#24314;&#35821;&#20041;&#32534;&#35299;&#30721;&#22120;&#21644;&#30721;&#20070;&#65292;&#28982;&#21518;&#35774;&#35745;&#20102;&#21521;&#37327;-&#32034;&#24341;&#21464;&#25442;&#22120;&#65292;&#26681;&#25454;&#30721;&#20070;&#24341;&#23548;&#20197;&#28040;&#38500;&#20449;&#36947;&#22122;&#22768;&#30340;&#24433;&#21709;&#65292;&#24182;&#23454;&#29616;&#22270;&#20687;&#29983;&#25104;&#12290;&#30001;&#20110;&#39640;&#36136;&#37327;&#30721;&#20070;&#23545;Transformer&#30340;&#36741;&#21161;&#65292;&#25509;&#25910;&#31471;&#29983;&#25104;&#30340;&#22270;&#20687;&#25928;&#26524;&#20248;&#20110;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16868v1 Announce Type: cross  Abstract: Codebook-based generative semantic communication attracts increasing attention, since only indices are required to be transmitted when the codebook is shared between transmitter and receiver. However, due to the fact that the semantic relations among code vectors are not necessarily related to the distance of the corresponding code indices, the performance of the codebook-enabled semantic communication system is susceptible to the channel noise. Thus, how to improve the system robustness against the noise requires careful design. This paper proposes a robust codebook-assisted image semantic communication system, where semantic codec and codebook are first jointly constructed, and then vector-to-index transformer is designed guided by the codebook to eliminate the effects of channel noise, and achieve image generation. Thanks to the assistance of the high-quality codebook to the Transformer, the generated images at the receiver outperfo
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#29992;&#25143;&#21327;&#20316;&#26041;&#26696;&#65292;&#26088;&#22312;&#26368;&#22823;&#21270;&#32593;&#32476;&#20013;&#25152;&#26377;IoT&#35774;&#22791;&#30340;&#21152;&#26435;&#24635;&#35745;&#31639;&#36895;&#29575;&#65288;WSCR&#65289;&#12290;</title><link>https://arxiv.org/abs/2402.16866</link><description>&lt;p&gt;
&#29992;&#20110;&#22810;&#29992;&#25143;&#21327;&#20316;&#30340;&#26080;&#32447;&#20379;&#30005;&#36793;&#32536;&#35745;&#31639;&#30340;&#35745;&#31639;&#36895;&#29575;&#26368;&#22823;&#21270;
&lt;/p&gt;
&lt;p&gt;
Computation Rate Maximization for Wireless Powered Edge Computing With Multi-User Cooperation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16866
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#29992;&#25143;&#21327;&#20316;&#26041;&#26696;&#65292;&#26088;&#22312;&#26368;&#22823;&#21270;&#32593;&#32476;&#20013;&#25152;&#26377;IoT&#35774;&#22791;&#30340;&#21152;&#26435;&#24635;&#35745;&#31639;&#36895;&#29575;&#65288;WSCR&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16866v1 &#20844;&#21578;&#31867;&#22411;: &#36328;&#23398;&#31185; &#25688;&#35201;: &#31227;&#21160;&#36793;&#32536;&#35745;&#31639;&#65288;MEC&#65289;&#21644;&#22522;&#20110;&#23556;&#39057;&#30340;&#26080;&#32447;&#21151;&#29575;&#20256;&#36755;&#65288;WPT&#65289;&#30340;&#32467;&#21512;&#21576;&#29616;&#20986;&#19968;&#31181;&#20026;&#32593;&#32476;&#36793;&#32536;&#25552;&#20379;&#21487;&#25345;&#32493;&#33021;&#28304;&#20379;&#24212;&#21644;&#35745;&#31639;&#26381;&#21153;&#30340;&#26377;&#21069;&#26223;&#30340;&#25216;&#26415;&#12290;&#26412;&#30740;&#31350;&#32771;&#34385;&#20102;&#19968;&#20010;&#21253;&#25324;&#20855;&#26377;&#35745;&#31639;&#21333;&#20803;&#21644;&#22810;&#20010;&#29289;&#32852;&#32593;&#65288;IoT&#65289;&#35774;&#22791;&#30340;&#28151;&#21512;&#25509;&#20837;&#28857;&#65288;HAP&#65289;&#30340;&#26080;&#32447;&#20379;&#30005;&#31227;&#21160;&#36793;&#32536;&#35745;&#31639;&#31995;&#32479;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#29992;&#25143;&#21327;&#20316;&#26041;&#26696;&#26469;&#25913;&#21892;&#35745;&#31639;&#24615;&#33021;&#65292;&#20854;&#20013;&#21512;&#20316;&#38598;&#32676;&#26159;&#21160;&#24577;&#24418;&#25104;&#30340;&#12290;&#27599;&#20010;&#21512;&#20316;&#38598;&#32676;&#21253;&#25324;&#19968;&#20010;&#28304;&#35774;&#22791;&#65288;SD&#65289;&#21644;&#19968;&#20010;&#36741;&#21161;&#35774;&#22791;&#65288;AD&#65289;&#65292;&#20854;&#20013;SD&#21487;&#20197;&#23558;&#35745;&#31639;&#20219;&#21153;&#20998;&#25104;&#21508;&#31181;&#37096;&#20998;&#36827;&#34892;&#26412;&#22320;&#22788;&#29702;&#65292;&#21521;HAP&#21368;&#36733;&#65292;&#24182;&#22312;HAP&#30340;&#24110;&#21161;&#19979;&#30001;AD&#36828;&#31243;&#25191;&#34892;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#26088;&#22312;&#26368;&#22823;&#21270;&#32593;&#32476;&#20013;&#25152;&#26377;IoT&#35774;&#22791;&#30340;&#21152;&#26435;&#24635;&#35745;&#31639;&#36895;&#29575;&#65288;WSCR&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16866v1 Announce Type: cross  Abstract: The combination of mobile edge computing (MEC) and radio frequency-based wireless power transfer (WPT) presents a promising technique for providing sustainable energy supply and computing services at the network edge. This study considers a wireless-powered mobile edge computing system that includes a hybrid access point (HAP) equipped with a computing unit and multiple Internet of Things (IoT) devices. In particular, we propose a novel muti-user cooperation scheme to improve computation performance, where collaborative clusters are dynamically formed. Each collaborative cluster comprises a source device (SD) and an auxiliary device (AD), where the SD can partition the computation task into various segments for local processing, offloading to the HAP, and remote execution by the AD with the assistance of the HAP. Specifically, we aims to maximize the weighted sum computation rate (WSCR) of all the IoT devices in the network. This invol
&lt;/p&gt;</description></item><item><title>&#37327;&#23376;&#21551;&#21457;&#30340;&#28151;&#27788;&#33832;&#23572;&#26222;&#32676;&#20307;&#20248;&#21270;&#31639;&#27861;&#29992;&#20110;&#21160;&#24577;&#20248;&#21270;&#38382;&#39064;&#30340;&#25913;&#36827;&#24615;&#33021;&#30740;&#31350;</title><link>https://arxiv.org/abs/2402.16863</link><description>&lt;p&gt;
&#22522;&#20110;&#37327;&#23376;&#21551;&#21457;&#30340;&#28151;&#27788;&#33832;&#23572;&#26222;&#32676;&#20307;&#20248;&#21270;&#29992;&#20110;&#21160;&#24577;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Quantum Inspired Chaotic Salp Swarm Optimization for Dynamic Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16863
&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#21551;&#21457;&#30340;&#28151;&#27788;&#33832;&#23572;&#26222;&#32676;&#20307;&#20248;&#21270;&#31639;&#27861;&#29992;&#20110;&#21160;&#24577;&#20248;&#21270;&#38382;&#39064;&#30340;&#25913;&#36827;&#24615;&#33021;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#23454;&#38469;&#38382;&#39064;&#26159;&#26410;&#30693;&#30340;&#21160;&#24577;&#20248;&#21270;&#38382;&#39064;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#20687;&#26032;&#24037;&#20316;&#30340;&#21040;&#26469;&#12289;&#25130;&#27490;&#26085;&#26399;&#21464;&#26356;&#12289;&#39044;&#35746;&#21462;&#28040;&#20197;&#21450;&#21442;&#25968;&#25110;&#32422;&#26463;&#30340;&#26356;&#25913;&#31561;&#19981;&#21487;&#39044;&#27979;&#20107;&#20214;&#20351;&#24471;&#25628;&#32034;&#29615;&#22659;&#21160;&#24577;&#21270;&#12290;&#35768;&#22810;&#31639;&#27861;&#35774;&#35745;&#29992;&#20110;&#22788;&#29702;&#38745;&#24577;&#20248;&#21270;&#38382;&#39064;&#65292;&#20294;&#36825;&#20123;&#31639;&#27861;&#26080;&#27861;&#24456;&#22909;&#22320;&#22788;&#29702;&#21160;&#24577;&#20248;&#21270;&#38382;&#39064;&#12290;&#23613;&#31649;&#19968;&#20123;&#20248;&#21270;&#31639;&#27861;&#34987;&#25552;&#20986;&#26469;&#19981;&#21516;&#22320;&#22788;&#29702;&#21160;&#24577;&#29615;&#22659;&#30340;&#21464;&#21270;&#65292;&#20294;&#30001;&#20110;&#26576;&#20123;&#38480;&#21046;&#25110;&#32570;&#38519;&#65292;&#29616;&#26377;&#31639;&#27861;&#20173;&#26377;&#25913;&#36827;&#31354;&#38388;&#65292;&#29305;&#21035;&#26159;&#22312;&#23450;&#20301;&#21644;&#36319;&#36394;&#20808;&#21069;&#30830;&#23450;&#30340;&#26368;&#20248;&#35299;&#26041;&#38754;&#12290;&#32771;&#34385;&#21040;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#38598;&#25104;&#20102;&#37327;&#23376;&#35745;&#31639;&#21407;&#29702;&#30340;SSA&#21464;&#20307;&#65292;&#21517;&#20026;QSSO&#12290;&#25105;&#20204;&#33268;&#21147;&#20110;&#25913;&#36827;&#26631;&#20934;SSA&#30340;&#25972;&#20307;&#24615;&#33021;&#65292;&#20197;&#22788;&#29702;&#21160;&#24577;&#29615;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16863v1 Announce Type: cross  Abstract: Many real-world problems are dynamic optimization problems that are unknown beforehand. In practice, unpredictable events such as the arrival of new jobs, due date changes, and reservation cancellations, changes in parameters or constraints make the search environment dynamic. Many algorithms are designed to deal with stationary optimization problems, but these algorithms do not face dynamic optimization problems or manage them correctly. Although some optimization algorithms are proposed to deal with the changes in dynamic environments differently, there are still areas of improvement in existing algorithms due to limitations or drawbacks, especially in terms of locating and following the previously identified optima. With this in mind, we studied a variant of SSA known as QSSO, which integrates the principles of quantum computing. An attempt is made to improve the overall performance of standard SSA to deal with the dynamic environme
&lt;/p&gt;</description></item><item><title>&#23558;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#32479;&#19968;&#25551;&#36848;&#20026;&#35745;&#31639;&#22270;&#65292;&#25552;&#20986;&#26032;&#39062;&#30340;&#33258;&#21160;&#22270;&#20248;&#21270;&#22120;&#26469;&#25913;&#36827;&#33410;&#28857;&#21644;&#36793;&#65292;&#23454;&#29616;&#20102;&#20195;&#29702;&#20043;&#38388;&#30340;&#33258;&#21160;&#21327;&#20316;&#21644;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.16823</link><description>&lt;p&gt;
&#20316;&#20026;&#21487;&#20248;&#21270;&#22270;&#30340;&#35821;&#35328;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
Language Agents as Optimizable Graphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16823
&lt;/p&gt;
&lt;p&gt;
&#23558;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#32479;&#19968;&#25551;&#36848;&#20026;&#35745;&#31639;&#22270;&#65292;&#25552;&#20986;&#26032;&#39062;&#30340;&#33258;&#21160;&#22270;&#20248;&#21270;&#22120;&#26469;&#25913;&#36827;&#33410;&#28857;&#21644;&#36793;&#65292;&#23454;&#29616;&#20102;&#20195;&#29702;&#20043;&#38388;&#30340;&#33258;&#21160;&#21327;&#20316;&#21644;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#31181;&#20154;&#31867;&#35774;&#35745;&#30340;&#25552;&#21319;&#25216;&#26415;&#34987;&#25552;&#20986;&#65292;&#29992;&#20110;&#25913;&#36827;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#38382;&#39064;&#27714;&#35299;&#22120;&#65292;&#20135;&#29983;&#20102;&#35768;&#22810;&#19981;&#21516;&#30340;&#20195;&#30721;&#24211;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;LLM&#20195;&#29702;&#25551;&#36848;&#20026;&#35745;&#31639;&#22270;&#26469;&#32479;&#19968;&#36825;&#20123;&#26041;&#27861;&#12290;&#33410;&#28857;&#23454;&#29616;&#22788;&#29702;&#22810;&#27169;&#24577;&#25968;&#25454;&#25110;&#26597;&#35810;LLMs&#30340;&#21151;&#33021;&#65292;&#24182;&#19988;&#36793;&#25551;&#36848;&#25805;&#20316;&#20043;&#38388;&#30340;&#20449;&#24687;&#27969;&#21160;&#12290;&#22270;&#24418;&#21487;&#20197;&#36882;&#24402;&#22320;&#32452;&#21512;&#25104;&#20195;&#34920;&#19981;&#21516;&#20195;&#29702;&#20043;&#38388;&#21327;&#20316;&#23618;&#27425;&#30340;&#26356;&#22823;&#32452;&#21512;&#22270;&#65288;&#20854;&#20013;&#36793;&#36830;&#25509;&#19981;&#21516;&#20195;&#29702;&#30340;&#25805;&#20316;&#65289;&#12290;&#25105;&#20204;&#30340;&#26032;&#39062;&#33258;&#21160;&#22270;&#20248;&#21270;&#22120;&#65288;1&#65289;&#20248;&#21270;&#33410;&#28857;&#32423;LLM&#25552;&#31034;&#65288;&#33410;&#28857;&#20248;&#21270;&#65289;&#24182;&#65288;2&#65289;&#36890;&#36807;&#25913;&#21464;&#22270;&#36830;&#25509;&#24615;&#26469;&#25913;&#21892;&#20195;&#29702;&#21327;&#35843;&#65288;&#36793;&#32536;&#20248;&#21270;&#65289;&#12290;&#23454;&#39564;&#35777;&#26126;&#25105;&#20204;&#30340;&#26694;&#26550;&#21487;&#29992;&#20110;&#39640;&#25928;&#24320;&#21457;&#12289;&#38598;&#25104;&#21644;&#33258;&#21160;&#25913;&#36827;&#21508;&#31181;LLM&#20195;&#29702;&#12290;&#20195;&#30721;&#21487;&#22312;https://github.com/metauto-ai/gptswarm&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16823v1 Announce Type: cross  Abstract: Various human-designed prompt engineering techniques have been proposed to improve problem solvers based on Large Language Models (LLMs), yielding many disparate code bases. We unify these approaches by describing LLM-based agents as computational graphs. The nodes implement functions to process multimodal data or query LLMs, and the edges describe the information flow between operations. Graphs can be recursively combined into larger composite graphs representing hierarchies of inter-agent collaboration (where edges connect operations of different agents). Our novel automatic graph optimizers (1) refine node-level LLM prompts (node optimization) and (2) improve agent orchestration by changing graph connectivity (edge optimization). Experiments demonstrate that our framework can be used to efficiently develop, integrate, and automatically improve various LLM agents. The code can be found at https://github.com/metauto-ai/gptswarm.
&lt;/p&gt;</description></item><item><title>Nemotron-4 15B&#26159;&#19968;&#20010;150&#20159;&#21442;&#25968;&#30340;&#22823;&#22411;&#22810;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#22810;&#35821;&#35328;&#33021;&#21147;&#19978;&#34920;&#29616;&#20248;&#24322;&#65292;&#36229;&#36807;&#20854;&#20182;&#35268;&#27169;&#30456;&#20284;&#30340;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.16819</link><description>&lt;p&gt;
Nemotron-4 15B&#25216;&#26415;&#25253;&#21578;
&lt;/p&gt;
&lt;p&gt;
Nemotron-4 15B Technical Report
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16819
&lt;/p&gt;
&lt;p&gt;
Nemotron-4 15B&#26159;&#19968;&#20010;150&#20159;&#21442;&#25968;&#30340;&#22823;&#22411;&#22810;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#22810;&#35821;&#35328;&#33021;&#21147;&#19978;&#34920;&#29616;&#20248;&#24322;&#65292;&#36229;&#36807;&#20854;&#20182;&#35268;&#27169;&#30456;&#20284;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;Nemotron-4 15B&#65292;&#36825;&#26159;&#19968;&#20010;&#25317;&#26377;150&#20159;&#21442;&#25968;&#30340;&#22823;&#22411;&#22810;&#35821;&#35328;&#27169;&#22411;&#65292;&#35757;&#32451;&#36807;&#31243;&#20013;&#20351;&#29992;&#20102;8000&#19975;&#20159;&#20010;&#25991;&#26412;&#26631;&#35760;&#12290;Nemotron-4 15B&#22312;&#33521;&#35821;&#12289;&#22810;&#35821;&#35328;&#21644;&#32534;&#30721;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#65306;&#22312;7&#20010;&#19979;&#28216;&#35780;&#20272;&#39046;&#22495;&#20013;&#65292;&#23427;&#22312;4&#20010;&#39046;&#22495;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#22312;&#20854;&#20313;&#39046;&#22495;&#20013;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#34920;&#29616;&#65292;&#36229;&#36807;&#20102;&#25152;&#26377;&#29616;&#26377;&#35268;&#27169;&#30456;&#20284;&#30340;&#24320;&#25918;&#27169;&#22411;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;Nemotron-4 15B&#23637;&#29616;&#20986;&#20102;&#25152;&#26377;&#35268;&#27169;&#30456;&#20284;&#27169;&#22411;&#20013;&#26368;&#24378;&#30340;&#22810;&#35821;&#35328;&#33021;&#21147;&#65292;&#29978;&#33267;&#22312;&#22810;&#35821;&#35328;&#20219;&#21153;&#19978;&#20248;&#20110;&#22235;&#20493;&#20197;&#19978;&#30340;&#22823;&#22411;&#27169;&#22411;&#65292;&#20197;&#21450;&#19987;&#38376;&#29992;&#20110;&#22810;&#35821;&#35328;&#20219;&#21153;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16819v1 Announce Type: new  Abstract: We introduce Nemotron-4 15B, a 15-billion-parameter large multilingual language model trained on 8 trillion text tokens. Nemotron-4 15B demonstrates strong performance when assessed on English, multilingual, and coding tasks: it outperforms all existing similarly-sized open models on 4 out of 7 downstream evaluation areas and achieves competitive performance to the leading open models in the remaining ones. Specifically, Nemotron-4 15B exhibits the best multilingual capabilities of all similarly-sized models, even outperforming models over four times larger and those explicitly specialized for multilingual tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#21487;&#35299;&#37322;&#30340;&#36870;&#21521;&#24037;&#31243;&#22312;&#22797;&#26434;&#27169;&#22359;&#21270;&#31639;&#26415;&#20013;&#35266;&#23519;&#20102;Transformer&#20869;&#37096;&#30005;&#36335;&#23398;&#20064;&#36807;&#31243;&#65292;&#24182;&#21457;&#29616;&#20943;&#27861;&#22312;Transformer&#19978;&#36896;&#25104;&#20102;&#24378;&#28872;&#30340;&#19981;&#23545;&#31216;&#24615;&#65292;&#20056;&#27861;&#38656;&#35201;&#20313;&#24358;&#20559;&#32622;&#20998;&#37327;&#65292;&#22810;&#39033;&#24335;&#21472;&#21152;&#20102;&#22522;&#26412;&#31639;&#26415;&#27169;&#24335;&#65292;&#20294;&#22312;&#25361;&#25112;&#24615;&#24773;&#20917;&#19979;&#24182;&#19981;&#28165;&#26224;&#65292;Grokking&#29978;&#33267;&#21487;&#20197;&#22312;&#20855;&#26377;&#22522;&#26412;&#23545;&#31216;&#21644;&#20132;&#26367;&#34920;&#36798;&#24335;&#30340;&#39640;&#27425;&#20844;&#24335;&#20013;&#36731;&#26494;&#21457;&#29983;&#12290;</title><link>https://arxiv.org/abs/2402.16726</link><description>&lt;p&gt;
&#22312;&#22797;&#26434;&#27169;&#22359;&#21270;&#31639;&#26415;&#20013;&#35299;&#37322;&#29702;&#35299;&#30340;Transformer
&lt;/p&gt;
&lt;p&gt;
Interpreting Grokked Transformers in Complex Modular Arithmetic
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16726
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#21487;&#35299;&#37322;&#30340;&#36870;&#21521;&#24037;&#31243;&#22312;&#22797;&#26434;&#27169;&#22359;&#21270;&#31639;&#26415;&#20013;&#35266;&#23519;&#20102;Transformer&#20869;&#37096;&#30005;&#36335;&#23398;&#20064;&#36807;&#31243;&#65292;&#24182;&#21457;&#29616;&#20943;&#27861;&#22312;Transformer&#19978;&#36896;&#25104;&#20102;&#24378;&#28872;&#30340;&#19981;&#23545;&#31216;&#24615;&#65292;&#20056;&#27861;&#38656;&#35201;&#20313;&#24358;&#20559;&#32622;&#20998;&#37327;&#65292;&#22810;&#39033;&#24335;&#21472;&#21152;&#20102;&#22522;&#26412;&#31639;&#26415;&#27169;&#24335;&#65292;&#20294;&#22312;&#25361;&#25112;&#24615;&#24773;&#20917;&#19979;&#24182;&#19981;&#28165;&#26224;&#65292;Grokking&#29978;&#33267;&#21487;&#20197;&#22312;&#20855;&#26377;&#22522;&#26412;&#23545;&#31216;&#21644;&#20132;&#26367;&#34920;&#36798;&#24335;&#30340;&#39640;&#27425;&#20844;&#24335;&#20013;&#36731;&#26494;&#21457;&#29983;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Grokking&#19968;&#30452;&#26159;&#35299;&#24320;&#24310;&#36831;&#27867;&#21270;&#20043;&#35868;&#30340;&#31215;&#26497;&#25506;&#32034;&#12290;&#22312;&#24050;&#35299;&#23494;&#27169;&#22411;&#20013;&#35782;&#21035;&#21487;&#35299;&#37322;&#30340;&#31639;&#27861;&#26159;&#29702;&#35299;&#20854;&#26426;&#21046;&#30340;&#26263;&#31034;&#24615;&#32447;&#32034;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#38500;&#20102;&#26368;&#31616;&#21333;&#21644;&#24191;&#20026;&#30740;&#31350;&#30340;&#27169;&#22359;&#21270;&#21152;&#27861;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#21487;&#35299;&#37322;&#30340;&#36870;&#21521;&#24037;&#31243;&#35266;&#23519;&#20102;&#36890;&#36807;Grokking&#22312;&#22797;&#26434;&#27169;&#22359;&#21270;&#31639;&#26415;&#20013;&#23398;&#21040;&#30340;&#20869;&#37096;&#30005;&#36335;&#65292;&#31361;&#20986;&#26174;&#31034;&#20102;&#23427;&#20204;&#21160;&#21147;&#23398;&#19978;&#30340;&#37325;&#22823;&#24046;&#24322;&#65306;&#20943;&#27861;&#23545;Transformer&#20135;&#29983;&#24378;&#28872;&#30340;&#19981;&#23545;&#31216;&#24615;&#65307;&#20056;&#27861;&#22312;&#20613;&#31435;&#21494;&#22495;&#30340;&#25152;&#26377;&#39057;&#29575;&#19978;&#38656;&#35201;&#20313;&#24358;&#20559;&#32622;&#20998;&#37327;&#65307;&#22810;&#39033;&#24335;&#36890;&#24120;&#23548;&#33268;&#22522;&#26412;&#31639;&#26415;&#27169;&#24335;&#30340;&#21472;&#21152;&#65292;&#20294;&#22312;&#25361;&#25112;&#24615;&#24773;&#20917;&#19979;&#28165;&#26224;&#30340;&#27169;&#24335;&#24182;&#19981;&#26174;&#29616;&#65307;&#21363;&#20351;&#22312;&#20855;&#26377;&#22522;&#26412;&#23545;&#31216;&#21644;&#20132;&#26367;&#34920;&#36798;&#24335;&#30340;&#39640;&#27425;&#20844;&#24335;&#20013;&#65292;Grokking&#20063;&#24456;&#23481;&#26131;&#21457;&#29983;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#27169;&#22359;&#21270;&#31639;&#26415;&#30340;&#26032;&#39062;&#36827;&#23637;&#24230;&#37327;&#65307;&#20613;&#31435;&#21494;&#39057;&#29575;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16726v2 Announce Type: replace-cross  Abstract: Grokking has been actively explored to reveal the mystery of delayed generalization. Identifying interpretable algorithms inside the grokked models is a suggestive hint to understanding its mechanism. In this work, beyond the simplest and well-studied modular addition, we observe the internal circuits learned through grokking in complex modular arithmetic via interpretable reverse engineering, which highlights the significant difference in their dynamics: subtraction poses a strong asymmetry on Transformer; multiplication requires cosine-biased components at all the frequencies in a Fourier domain; polynomials often result in the superposition of the patterns from elementary arithmetic, but clear patterns do not emerge in challenging cases; grokking can easily occur even in higher-degree formulas with basic symmetric and alternating expressions. We also introduce the novel progress measure for modular arithmetic; Fourier Freque
&lt;/p&gt;</description></item><item><title>RoboGrind&#26159;&#19968;&#20010;&#38598;&#25104;&#31995;&#32479;&#65292;&#36890;&#36807;3D&#24863;&#30693;&#12289;&#20132;&#20114;&#24335;&#35821;&#38899;&#25511;&#21046;&#21521;&#23548;&#31995;&#32479;&#21644;&#33258;&#21160;&#35268;&#21010;&#25191;&#34892;&#27969;&#27700;&#32447;&#23454;&#29616;&#24037;&#19994;&#26426;&#22120;&#20154;&#23545;&#34920;&#38754;&#22788;&#29702;&#20219;&#21153;&#30340;&#30452;&#35266;&#12289;&#20132;&#20114;&#24335;&#33258;&#21160;&#21270;&#65292;&#20026;&#37325;&#21046;&#29627;&#29827;&#32420;&#32500;&#39118;&#21147;&#28065;&#36718;&#21494;&#29255;&#25552;&#20379;&#20102;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2402.16542</link><description>&lt;p&gt;
RoboGrind&#65306;&#24037;&#19994;&#26426;&#22120;&#20154;&#30340;&#30452;&#35266;&#20132;&#20114;&#24335;&#34920;&#38754;&#22788;&#29702;
&lt;/p&gt;
&lt;p&gt;
RoboGrind: Intuitive and Interactive Surface Treatment with Industrial Robots
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16542
&lt;/p&gt;
&lt;p&gt;
RoboGrind&#26159;&#19968;&#20010;&#38598;&#25104;&#31995;&#32479;&#65292;&#36890;&#36807;3D&#24863;&#30693;&#12289;&#20132;&#20114;&#24335;&#35821;&#38899;&#25511;&#21046;&#21521;&#23548;&#31995;&#32479;&#21644;&#33258;&#21160;&#35268;&#21010;&#25191;&#34892;&#27969;&#27700;&#32447;&#23454;&#29616;&#24037;&#19994;&#26426;&#22120;&#20154;&#23545;&#34920;&#38754;&#22788;&#29702;&#20219;&#21153;&#30340;&#30452;&#35266;&#12289;&#20132;&#20114;&#24335;&#33258;&#21160;&#21270;&#65292;&#20026;&#37325;&#21046;&#29627;&#29827;&#32420;&#32500;&#39118;&#21147;&#28065;&#36718;&#21494;&#29255;&#25552;&#20379;&#20102;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16542v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#36328;&#39046;&#22495; &#25688;&#35201;&#65306;&#35832;&#22914;&#30952;&#21066;&#12289;&#25171;&#30952;&#25110;&#25243;&#20809;&#20043;&#31867;&#30340;&#34920;&#38754;&#22788;&#29702;&#20219;&#21153;&#26159;&#35768;&#22810;&#34892;&#19994;&#20215;&#20540;&#38142;&#20013;&#33267;&#20851;&#37325;&#35201;&#30340;&#19968;&#27493;&#65292;&#20294;&#33258;&#21160;&#21270;&#22788;&#29702;&#36825;&#20123;&#20219;&#21153;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;RoboGrind&#65292;&#36825;&#26159;&#19968;&#20010;&#38598;&#25104;&#31995;&#32479;&#65292;&#29992;&#20110;&#36890;&#36807;&#24037;&#19994;&#26426;&#22120;&#20154;&#30452;&#35266;&#12289;&#20132;&#20114;&#24335;&#22320;&#33258;&#21160;&#21270;&#34920;&#38754;&#22788;&#29702;&#20219;&#21153;&#12290;&#35813;&#31995;&#32479;&#32467;&#21512;&#20102;&#19968;&#20010;&#22797;&#26434;&#30340;3D&#24863;&#30693;&#27969;&#27700;&#32447;&#65292;&#29992;&#20110;&#34920;&#38754;&#25195;&#25551;&#21644;&#33258;&#21160;&#32570;&#38519;&#35782;&#21035;&#65292;&#19968;&#20010;&#20132;&#20114;&#24335;&#35821;&#38899;&#25511;&#21046;&#21521;&#23548;&#31995;&#32479;&#65292;&#29992;&#20110;AI&#36741;&#21161;&#21021;&#22987;&#21270;&#21644;&#21442;&#25968;&#21270;&#26426;&#22120;&#20154;&#31243;&#24207;&#65292;&#20197;&#21450;&#19968;&#20010;&#29992;&#20110;&#21147;&#25511;&#21046;&#24335;&#26426;&#22120;&#20154;&#34920;&#38754;&#22788;&#29702;&#30340;&#33258;&#21160;&#35268;&#21010;&#21644;&#25191;&#34892;&#27969;&#27700;&#32447;&#12290;RoboGrind&#22312;&#23454;&#39564;&#23460;&#21644;&#23454;&#38469;&#29615;&#22659;&#20013;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#20027;&#35201;&#26159;&#29992;&#20110;&#37325;&#21046;&#29627;&#29827;&#32420;&#32500;&#39118;&#21147;&#28065;&#36718;&#21494;&#29255;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16542v1 Announce Type: cross  Abstract: Surface treatment tasks such as grinding, sanding or polishing are a vital step of the value chain in many industries, but are notoriously challenging to automate. We present RoboGrind, an integrated system for the intuitive, interactive automation of surface treatment tasks with industrial robots. It combines a sophisticated 3D perception pipeline for surface scanning and automatic defect identification, an interactive voice-controlled wizard system for the AI-assisted bootstrapping and parameterization of robot programs, and an automatic planning and execution pipeline for force-controlled robotic surface treatment. RoboGrind is evaluated both under laboratory and real-world conditions in the context of refabricating fiberglass wind turbine blades.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21453;&#39304;&#39640;&#25928;&#30340;&#22312;&#32447;&#24494;&#35843;&#25193;&#25955;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#31243;&#24207;</title><link>https://arxiv.org/abs/2402.16359</link><description>&lt;p&gt;
&#21453;&#39304;&#39640;&#25928;&#22312;&#32447;&#24494;&#35843;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Feedback Efficient Online Fine-Tuning of Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16359
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21453;&#39304;&#39640;&#25928;&#30340;&#22312;&#32447;&#24494;&#35843;&#25193;&#25955;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#31243;&#24207;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#22312;&#24314;&#27169;&#22797;&#26434;&#25968;&#25454;&#20998;&#24067;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#21253;&#25324;&#22270;&#20687;&#65292;&#34507;&#30333;&#36136;&#21644;&#23567;&#20998;&#23376;&#30340;&#20998;&#24067;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#27169;&#25311;&#26368;&#22823;&#21270;&#26576;&#20123;&#23646;&#24615;&#30340;&#20998;&#24067;&#30340;&#37096;&#20998;&#65306;&#20363;&#22914;&#65292;&#25105;&#20204;&#21487;&#33021;&#24076;&#26395;&#29983;&#25104;&#20855;&#26377;&#39640;&#23457;&#32654;&#36136;&#37327;&#30340;&#22270;&#20687;&#65292;&#25110;&#20855;&#26377;&#39640;&#29983;&#29289;&#27963;&#24615;&#30340;&#20998;&#23376;&#12290;&#33258;&#28982;&#22320;&#65292;&#25105;&#20204;&#21487;&#20197;&#23558;&#36825;&#35270;&#20026;&#19968;&#20010;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#38382;&#39064;&#65292;&#20854;&#30446;&#26631;&#26159;&#24494;&#35843;&#25193;&#25955;&#27169;&#22411;&#20197;&#26368;&#22823;&#21270;&#19982;&#26576;&#20123;&#23646;&#24615;&#23545;&#24212;&#30340;&#22870;&#21169;&#20989;&#25968;&#12290;&#21363;&#20351;&#21487;&#20197;&#35775;&#38382;&#22320;&#38754;&#30495;&#23454;&#22870;&#21169;&#20989;&#25968;&#30340;&#22312;&#32447;&#26597;&#35810;&#65292;&#26377;&#25928;&#22320;&#21457;&#29616;&#39640;&#22870;&#21169;&#26679;&#26412;&#20063;&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#65306;&#23427;&#20204;&#22312;&#21021;&#22987;&#20998;&#24067;&#20013;&#30340;&#27010;&#29575;&#21487;&#33021;&#24456;&#20302;&#65292;&#24182;&#19988;&#21487;&#33021;&#23384;&#22312;&#35768;&#22810;&#19981;&#21487;&#34892;&#30340;&#26679;&#26412;&#65292;&#29978;&#33267;&#27809;&#26377;&#23450;&#20041;&#33391;&#22909;&#30340;&#22870;&#21169;&#65288;&#20363;&#22914;&#65292;&#19981;&#33258;&#28982;&#30340;&#22270;&#20687;&#25110;&#29289;&#29702;&#19978;&#19981;&#21487;&#33021;&#30340;&#20998;&#23376;&#65289;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24378;&#21270;&#23398;&#20064;&#31243;&#24207;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#21457;&#29616;&#39640;&#22870;&#21169;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16359v1 Announce Type: cross  Abstract: Diffusion models excel at modeling complex data distributions, including those of images, proteins, and small molecules. However, in many cases, our goal is to model parts of the distribution that maximize certain properties: for example, we may want to generate images with high aesthetic quality, or molecules with high bioactivity. It is natural to frame this as a reinforcement learning (RL) problem, in which the objective is to fine-tune a diffusion model to maximize a reward function that corresponds to some property. Even with access to online queries of the ground-truth reward function, efficiently discovering high-reward samples can be challenging: they might have a low probability in the initial distribution, and there might be many infeasible samples that do not even have a well-defined reward (e.g., unnatural images or physically impossible molecules). In this work, we propose a novel reinforcement learning procedure that effi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#33258;&#25105;&#35757;&#32451;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#29983;&#25104;&#35757;&#32451;&#25968;&#25454;&#23558;&#28304;&#39046;&#22495;&#21477;&#27861;&#35268;&#21017;&#19982;&#30446;&#26631;&#39046;&#22495;&#21477;&#23376;&#30456;&#32467;&#21512;&#65292;&#22686;&#24378;&#21477;&#24335;&#32467;&#26500;&#35299;&#26512;&#22120;&#23545;&#21508;&#31181;&#39046;&#22495;&#30340;&#36866;&#24212;&#33021;&#21147;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#22312;&#25945;&#31185;&#20070;&#21644;&#26032;&#38395;&#39046;&#22495;&#30340;&#25928;&#26524;&#20248;&#20110;&#22522;&#20110;&#35268;&#21017;&#30340;&#22522;&#20934;&#27169;&#22411;1.68&#20010;&#30334;&#20998;&#28857;&#12290;</title><link>https://arxiv.org/abs/2402.16311</link><description>&lt;p&gt;
&#36328;&#39046;&#22495;&#30340;&#20013;&#25991;&#21477;&#24335;&#32467;&#26500;&#35299;&#26512;
&lt;/p&gt;
&lt;p&gt;
Cross-domain Chinese Sentence Pattern Parsing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16311
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#33258;&#25105;&#35757;&#32451;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#29983;&#25104;&#35757;&#32451;&#25968;&#25454;&#23558;&#28304;&#39046;&#22495;&#21477;&#27861;&#35268;&#21017;&#19982;&#30446;&#26631;&#39046;&#22495;&#21477;&#23376;&#30456;&#32467;&#21512;&#65292;&#22686;&#24378;&#21477;&#24335;&#32467;&#26500;&#35299;&#26512;&#22120;&#23545;&#21508;&#31181;&#39046;&#22495;&#30340;&#36866;&#24212;&#33021;&#21147;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#22312;&#25945;&#31185;&#20070;&#21644;&#26032;&#38395;&#39046;&#22495;&#30340;&#25928;&#26524;&#20248;&#20110;&#22522;&#20110;&#35268;&#21017;&#30340;&#22522;&#20934;&#27169;&#22411;1.68&#20010;&#30334;&#20998;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16311v1 &#20844;&#21578;&#31867;&#22411;: &#36328;&#39046;&#22495; &#21477;&#24335;&#32467;&#26500;&#65288;SPS&#65289;&#35299;&#26512;&#26159;&#19968;&#31181;&#20027;&#35201;&#29992;&#20110;&#35821;&#35328;&#25945;&#23398;&#30340;&#21477;&#27861;&#20998;&#26512;&#26041;&#27861;&#12290;&#29616;&#26377;&#30340;SPS&#35299;&#26512;&#22120;&#20027;&#35201;&#20381;&#36182;&#20110;&#25945;&#31185;&#20070;&#35821;&#26009;&#24211;&#36827;&#34892;&#35757;&#32451;&#65292;&#32570;&#20047;&#36328;&#39046;&#22495;&#33021;&#21147;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#38480;&#21046;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#33258;&#25105;&#35757;&#32451;&#26694;&#26550;&#20869;&#12290;&#20174;&#28304;&#39046;&#22495;&#20013;&#25552;&#21462;&#37096;&#20998;&#21477;&#27861;&#35268;&#21017;&#65292;&#19982;&#30446;&#26631;&#39046;&#22495;&#21477;&#23376;&#32467;&#21512;&#21160;&#24577;&#29983;&#25104;&#35757;&#32451;&#25968;&#25454;&#65292;&#22686;&#24378;&#20102;&#35299;&#26512;&#22120;&#23545;&#19981;&#21516;&#39046;&#22495;&#30340;&#36866;&#24212;&#33021;&#21147;&#12290;&#22312;&#25945;&#31185;&#20070;&#21644;&#26032;&#38395;&#39046;&#22495;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#25928;&#26524;&#26174;&#33879;&#65292;F1&#25351;&#26631;&#27604;&#22522;&#20110;&#35268;&#21017;&#30340;&#22522;&#20934;&#27169;&#22411;&#39640;&#20986;1.68&#20010;&#30334;&#20998;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16311v1 Announce Type: cross  Abstract: Sentence Pattern Structure (SPS) parsing is a syntactic analysis method primarily employed in language teaching.Existing SPS parsers rely heavily on textbook corpora for training, lacking cross-domain capability.To overcome this constraint, this paper proposes an innovative approach leveraging large language models (LLMs) within a self-training framework. Partial syntactic rules from a source domain are combined with target domain sentences to dynamically generate training data, enhancing the adaptability of the parser to diverse domains.Experiments conducted on textbook and news domains demonstrate the effectiveness of the proposed method, outperforming rule-based baselines by 1.68 points on F1 metrics.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#24341;&#25991;&#22686;&#24378;&#30340;LLM&#32842;&#22825;&#26426;&#22120;&#20154;&#29983;&#25104;&#26041;&#27861;&#65292;&#37319;&#29992;&#26816;&#32034;&#27169;&#22359;&#25628;&#32034;&#25903;&#25345;&#25991;&#26723;&#26469;&#35299;&#20915;&#24187;&#35273;&#20869;&#23481;&#20135;&#29983;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.16063</link><description>&lt;p&gt;
&#22522;&#20110;&#24341;&#25991;&#22686;&#24378;&#30340;LLM&#32842;&#22825;&#26426;&#22120;&#20154;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Citation-Enhanced Generation for LLM-based Chatbot
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16063
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#24341;&#25991;&#22686;&#24378;&#30340;LLM&#32842;&#22825;&#26426;&#22120;&#20154;&#29983;&#25104;&#26041;&#27861;&#65292;&#37319;&#29992;&#26816;&#32034;&#27169;&#22359;&#25628;&#32034;&#25903;&#25345;&#25991;&#26723;&#26469;&#35299;&#20915;&#24187;&#35273;&#20869;&#23481;&#20135;&#29983;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#24773;&#26223;&#19979;&#23637;&#29616;&#20986;&#24378;&#22823;&#30340;&#36890;&#29992;&#26234;&#33021;&#65292;&#21253;&#25324;&#23558;&#23427;&#20204;&#38598;&#25104;&#21040;&#32842;&#22825;&#26426;&#22120;&#20154;&#20013;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;LLM&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#38754;&#20020;&#30340;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#26159;&#22312;&#22238;&#22797;&#20013;&#21487;&#33021;&#20135;&#29983;&#34394;&#26500;&#20869;&#23481;&#65292;&#36825;&#20005;&#37325;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#36866;&#29992;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21518;&#32493;&#24341;&#29992;&#22686;&#24378;&#29983;&#25104;&#65288;CEG&#65289;&#26041;&#27861;&#65292;&#32467;&#21512;&#26816;&#32034;&#35770;&#35777;&#12290;&#19982;&#20808;&#21069;&#20391;&#37325;&#20110;&#39044;&#38450;&#29983;&#25104;&#36807;&#31243;&#20013;&#24187;&#35273;&#30340;&#30740;&#31350;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20197;&#21518;&#32493;&#26041;&#24335;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;&#23427;&#32467;&#21512;&#20102;&#19968;&#20010;&#26816;&#32034;&#27169;&#22359;&#26469;&#25628;&#32034;&#19982;&#29983;&#25104;&#20869;&#23481;&#30456;&#20851;&#30340;&#25903;&#25345;&#25991;&#26723;&#65292;&#24182;&#37319;&#29992;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16063v1 Announce Type: cross  Abstract: Large language models (LLMs) exhibit powerful general intelligence across diverse scenarios, including their integration into chatbots. However, a vital challenge of LLM-based chatbots is that they may produce hallucinated content in responses, which significantly limits their applicability. Various efforts have been made to alleviate hallucination, such as retrieval augmented generation and reinforcement learning with human feedback, but most of them require additional training and data annotation. In this paper, we propose a novel post-hoc \textbf{C}itation-\textbf{E}nhanced \textbf{G}eneration (\textbf{CEG}) approach combined with retrieval argumentation. Unlike previous studies that focus on preventing hallucinations during generation, our method addresses this issue in a post-hoc way. It incorporates a retrieval module to search for supporting documents relevant to the generated content, and employs a natural language inference-ba
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#23545;&#27604;&#22270;&#23398;&#20064;&#65288;DCGL&#65289;&#27169;&#22411;&#65292;&#36890;&#36807;&#32467;&#21512;&#33258;&#32534;&#30721;&#22120;&#21644;GCN&#65292;&#22312;&#22788;&#29702;&#19968;&#33324;&#25968;&#25454;&#32858;&#31867;&#26102;&#24378;&#35843;&#20102;&#22270;&#32467;&#26500;&#21644;&#21407;&#22987;&#29305;&#24449;&#12290;</title><link>https://arxiv.org/abs/2402.16012</link><description>&lt;p&gt;
&#20855;&#26377;&#38754;&#21521;&#32858;&#31867;&#30340;&#24341;&#23548;&#30340;&#28145;&#24230;&#23545;&#27604;&#22270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Deep Contrastive Graph Learning with Clustering-Oriented Guidance
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16012
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#23545;&#27604;&#22270;&#23398;&#20064;&#65288;DCGL&#65289;&#27169;&#22411;&#65292;&#36890;&#36807;&#32467;&#21512;&#33258;&#32534;&#30721;&#22120;&#21644;GCN&#65292;&#22312;&#22788;&#29702;&#19968;&#33324;&#25968;&#25454;&#32858;&#31867;&#26102;&#24378;&#35843;&#20102;&#22270;&#32467;&#26500;&#21644;&#21407;&#22987;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCN&#65289;&#22312;&#25913;&#21892;&#22522;&#20110;&#22270;&#30340;&#32858;&#31867;&#20013;&#23637;&#29616;&#20986;&#20102;&#26174;&#33879;&#28508;&#21147;&#12290;&#20026;&#20102;&#22788;&#29702;&#27809;&#26377;&#20808;&#39564;&#22270;&#30340;&#19968;&#33324;&#32858;&#31867;&#22330;&#26223;&#65292;&#36825;&#20123;&#27169;&#22411;&#20808;&#20272;&#35745;&#19968;&#20010;&#21021;&#22987;&#22270;&#65292;&#28982;&#21518;&#24212;&#29992;GCN&#12290;&#25991;&#29486;&#20013;&#26174;&#31034;&#65292;&#22823;&#22810;&#25968;&#27169;&#22411;&#20851;&#27880;&#20110;&#21021;&#22987;&#22270;&#32780;&#24573;&#30053;&#20102;&#21407;&#22987;&#29305;&#24449;&#12290;&#22240;&#27492;&#65292;&#23398;&#21040;&#30340;&#34920;&#31034;&#30340;&#21487;&#36776;&#35782;&#24615;&#21487;&#33021;&#20250;&#21463;&#21040;&#20302;&#36136;&#37327;&#21021;&#22987;&#22270;&#30340;&#30772;&#22351;&#65307;&#35757;&#32451;&#36807;&#31243;&#32570;&#20047;&#26377;&#25928;&#30340;&#32858;&#31867;&#24341;&#23548;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#23558;&#19982;&#32858;&#31867;&#26080;&#20851;&#30340;&#20449;&#24687;&#21512;&#24182;&#21040;&#23398;&#21040;&#30340;&#22270;&#20013;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#29992;&#20110;&#19968;&#33324;&#25968;&#25454;&#32858;&#31867;&#30340;&#28145;&#24230;&#23545;&#27604;&#22270;&#23398;&#20064;&#65288;DCGL&#65289;&#27169;&#22411;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#20266;&#23402;&#29983;&#32593;&#32476;&#65292;&#23558;&#33258;&#32534;&#30721;&#22120;&#19982;GCN&#30456;&#32467;&#21512;&#65292;&#20197;&#24378;&#35843;&#22270;&#32467;&#26500;&#21644;&#21407;&#22987;&#29305;&#24449;&#12290;&#22522;&#20110;&#27492;&#65292;&#29305;&#24449;&#32423;&#23545;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16012v1 Announce Type: new  Abstract: Graph Convolutional Network (GCN) has exhibited remarkable potential in improving graph-based clustering. To handle the general clustering scenario without a prior graph, these models estimate an initial graph beforehand to apply GCN. Throughout the literature, we have witnessed that 1) most models focus on the initial graph while neglecting the original features. Therefore, the discriminability of the learned representation may be corrupted by a low-quality initial graph; 2) the training procedure lacks effective clustering guidance, which may lead to the incorporation of clustering-irrelevant information into the learned graph. To tackle these problems, the Deep Contrastive Graph Learning (DCGL) model is proposed for general data clustering. Specifically, we establish a pseudo-siamese network, which incorporates auto-encoder with GCN to emphasize both the graph structure and the original features. On this basis, feature-level contrasti
&lt;/p&gt;</description></item><item><title>CoDream&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22312;&#36755;&#20837;&#25968;&#25454;&#31354;&#38388;&#20013;&#21327;&#20316;&#20248;&#21270;&#25968;&#25454;&#26469;&#20132;&#25442;&#30693;&#35782;&#30340;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#27169;&#22411;&#20043;&#38388;&#30340;&#21512;&#20316;&#23398;&#20064;&#65292;&#23454;&#29616;&#20102;&#27169;&#22411;&#26550;&#26500;&#26080;&#20851;&#12289;&#36890;&#20449;&#19981;&#21463;&#27169;&#22411;&#22823;&#23567;&#24433;&#21709;&#12289;&#20860;&#23481;&#23433;&#20840;&#32858;&#21512;&#30340;&#20248;&#28857;&#12290;</title><link>https://arxiv.org/abs/2402.15968</link><description>&lt;p&gt;
CoDream&#65306;&#20351;&#29992;&#24322;&#26500;&#27169;&#22411;&#20132;&#25442;&#26790;&#24819;&#32780;&#19981;&#26159;&#27169;&#22411;&#36827;&#34892;&#32852;&#21512;&#32858;&#21512;
&lt;/p&gt;
&lt;p&gt;
CoDream: Exchanging dreams instead of models for federated aggregation with heterogeneous models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15968
&lt;/p&gt;
&lt;p&gt;
CoDream&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22312;&#36755;&#20837;&#25968;&#25454;&#31354;&#38388;&#20013;&#21327;&#20316;&#20248;&#21270;&#25968;&#25454;&#26469;&#20132;&#25442;&#30693;&#35782;&#30340;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#27169;&#22411;&#20043;&#38388;&#30340;&#21512;&#20316;&#23398;&#20064;&#65292;&#23454;&#29616;&#20102;&#27169;&#22411;&#26550;&#26500;&#26080;&#20851;&#12289;&#36890;&#20449;&#19981;&#21463;&#27169;&#22411;&#22823;&#23567;&#24433;&#21709;&#12289;&#20860;&#23481;&#23433;&#20840;&#32858;&#21512;&#30340;&#20248;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#36890;&#36807;&#32858;&#21512;&#27169;&#22411;&#21442;&#25968;&#23454;&#29616;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#20998;&#25955;&#25968;&#25454;&#19978;&#30340;&#21327;&#20316;&#20248;&#21270;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#32858;&#21512;&#27169;&#22411;&#20135;&#29983;&#30340;&#8220;&#30693;&#35782;&#8221;&#65292;&#32780;&#19981;&#26159;&#27169;&#22411;&#21442;&#25968;&#26469;&#25193;&#23637;&#36825;&#19968;&#27010;&#24565;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026; \codream &#30340;&#26032;&#39062;&#26694;&#26550;&#65292;&#22312;&#36825;&#20010;&#26694;&#26550;&#20013;&#65292;&#23458;&#25143;&#31471;&#36890;&#36807;&#22312;&#36755;&#20837;&#25968;&#25454;&#31354;&#38388;&#20013;&#20351;&#29992;&#32852;&#21512;&#20248;&#21270;&#26469;&#21327;&#20316;&#20248;&#21270;&#38543;&#26426;&#21021;&#22987;&#21270;&#30340;&#25968;&#25454;&#65292;&#31867;&#20284;&#20110;&#22312;FL&#20013;&#20248;&#21270;&#38543;&#26426;&#21021;&#22987;&#21270;&#30340;&#27169;&#22411;&#21442;&#25968;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#35265;&#35299;&#26159;&#65292;&#32852;&#21512;&#20248;&#21270;&#36825;&#20123;&#25968;&#25454;&#21487;&#20197;&#26377;&#25928;&#25429;&#33719;&#20840;&#23616;&#25968;&#25454;&#20998;&#24067;&#30340;&#29305;&#24615;&#12290;&#22312;&#25968;&#25454;&#31354;&#38388;&#20849;&#20139;&#30693;&#35782;&#20855;&#26377;&#35768;&#22810;&#22909;&#22788;&#65306;&#65288;1&#65289;&#19982;&#27169;&#22411;&#26080;&#20851;&#30340;&#21327;&#20316;&#23398;&#20064;&#65292;&#21363;&#19981;&#21516;&#23458;&#25143;&#31471;&#21487;&#20197;&#20855;&#26377;&#19981;&#21516;&#30340;&#27169;&#22411;&#26550;&#26500;&#65307;&#65288;2&#65289;&#36890;&#20449;&#19981;&#21463;&#27169;&#22411;&#22823;&#23567;&#24433;&#21709;&#65292;&#28040;&#38500;&#20102;&#27169;&#22411;&#21442;&#25968;&#30340;&#21487;&#20280;&#32553;&#24615;&#38382;&#39064;&#65307;&#65288;3&#65289;&#19982;&#23433;&#20840;&#32858;&#21512;&#20860;&#23481;&#65292;&#22240;&#27492;&#21487;&#39044;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15968v1 Announce Type: cross  Abstract: Federated Learning (FL) enables collaborative optimization of machine learning models across decentralized data by aggregating model parameters. Our approach extends this concept by aggregating "knowledge" derived from models, instead of model parameters. We present a novel framework called \codream, where clients collaboratively optimize randomly initialized data using federated optimization in the input data space, similar to how randomly initialized model parameters are optimized in FL. Our key insight is that jointly optimizing this data can effectively capture the properties of the global data distribution. Sharing knowledge in data space offers numerous benefits: (1) model-agnostic collaborative learning, i.e., different clients can have different model architectures; (2) communication that is independent of the model size, eliminating scalability concerns with model parameters; (3) compatibility with secure aggregation, thus pre
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#36873;&#25321;&#30456;&#20851;&#30340;&#23383;&#33410;&#23376;&#38598;&#26367;&#20195;&#39640;&#26031;&#22122;&#22768;&#65292;&#22312;&#35757;&#32451;&#20013;&#36827;&#34892;&#22522;&#20110;&#28040;&#34701;&#30340;&#24179;&#28369;&#26041;&#26696;&#65292;&#21152;&#24378;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#22120;&#23545;&#25239;&#24615;&#24694;&#24847;&#36719;&#20214;&#31034;&#20363;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.15267</link><description>&lt;p&gt;
&#36890;&#36807;&#65288;&#21435;&#65289;&#38543;&#26426;&#24179;&#28369;&#25552;&#39640;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#22120;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Adversarial Robustness of Deep Learning-based Malware Detectors via (De)Randomized Smoothing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15267
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#36873;&#25321;&#30456;&#20851;&#30340;&#23383;&#33410;&#23376;&#38598;&#26367;&#20195;&#39640;&#26031;&#22122;&#22768;&#65292;&#22312;&#35757;&#32451;&#20013;&#36827;&#34892;&#22522;&#20110;&#28040;&#34701;&#30340;&#24179;&#28369;&#26041;&#26696;&#65292;&#21152;&#24378;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#22120;&#23545;&#25239;&#24615;&#24694;&#24847;&#36719;&#20214;&#31034;&#20363;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#22120;&#24050;&#34987;&#35777;&#26126;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#24694;&#24847;&#36719;&#20214;&#31034;&#20363;&#30340;&#25915;&#20987;&#65292;&#21363;&#24694;&#24847;&#36719;&#20214;&#31034;&#20363;&#32463;&#36807;&#25925;&#24847;&#25805;&#32437;&#20197;&#36991;&#20813;&#26816;&#27979;&#12290;&#37492;&#20110;&#28145;&#24230;&#23398;&#20064;&#26816;&#27979;&#22120;&#23545;&#24494;&#22937;&#36755;&#20837;&#25991;&#20214;&#20462;&#25913;&#30340;&#33030;&#24369;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21463;&#65288;&#21435;&#65289;&#38543;&#26426;&#24179;&#28369;&#21551;&#21457;&#30340;&#38024;&#23545;&#23545;&#25239;&#24615;&#24694;&#24847;&#36719;&#20214;&#31034;&#20363;&#30340;&#23454;&#29992;&#38450;&#24481;&#26041;&#27861;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#36873;&#25321;&#30456;&#20851;&#30340;&#23383;&#33410;&#23376;&#38598;&#32780;&#19981;&#26159;&#20687;&#35745;&#31639;&#26426;&#35270;&#35273;&#65288;CV&#65289;&#39046;&#22495;&#37027;&#26679;&#20351;&#29992;&#39640;&#26031;&#22122;&#22768;&#26469;&#38543;&#26426;&#21270;&#36755;&#20837;&#65292;&#26469;&#38477;&#20302;&#34987;&#24694;&#24847;&#36719;&#20214;&#20316;&#32773;&#27880;&#20837;&#30340;&#23545;&#25239;&#20869;&#23481;&#34987;&#37319;&#26679;&#30340;&#20960;&#29575;&#12290;&#22312;&#35757;&#32451;&#26399;&#38388;&#65292;&#25105;&#20204;&#30340;&#21435;&#38500;&#22522;&#20110;&#28040;&#34701;&#30340;&#24179;&#28369;&#26041;&#26696;&#35757;&#32451;&#19968;&#20010;&#22522;&#26412;&#20998;&#31867;&#22120;&#23545;&#19968;&#37096;&#20998;&#36830;&#32493;&#23383;&#33410;&#25110;&#23383;&#33410;&#22359;&#36827;&#34892;&#20998;&#31867;&#12290;&#22312;&#27979;&#35797;&#26102;&#65292;&#22522;&#26412;&#20998;&#31867;&#22120;&#23545;&#22823;&#37327;&#23383;&#33410;&#22359;&#36827;&#34892;&#20998;&#31867;&#65292;&#26368;&#21518;&#39044;&#27979;&#32467;&#26524;&#26159;&#36825;&#20123;&#20998;&#31867;&#20013;&#30340;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15267v1 Announce Type: cross  Abstract: Deep learning-based malware detectors have been shown to be susceptible to adversarial malware examples, i.e. malware examples that have been deliberately manipulated in order to avoid detection. In light of the vulnerability of deep learning detectors to subtle input file modifications, we propose a practical defense against adversarial malware examples inspired by (de)randomized smoothing. In this work, we reduce the chances of sampling adversarial content injected by malware authors by selecting correlated subsets of bytes, rather than using Gaussian noise to randomize inputs like in the Computer Vision (CV) domain. During training, our ablation-based smoothing scheme trains a base classifier to make classifications on a subset of contiguous bytes or chunk of bytes. At test time, a large number of chunks are then classified by a base classifier and the consensus among these classifications is then reported as the final prediction. W
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GraphEdit&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23398;&#20064;&#22797;&#26434;&#30340;&#22270;&#32467;&#26500;&#21270;&#25968;&#25454;&#20013;&#30340;&#33410;&#28857;&#20851;&#31995;&#65292;&#36890;&#36807;&#22312;&#22270;&#32467;&#26500;&#19978;&#36827;&#34892;&#25351;&#23548;&#35843;&#25972;&#65292;&#22686;&#24378;LLMs&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#20174;&#32780;&#25552;&#39640;&#22270;&#32467;&#26500;&#23398;&#20064;&#30340;&#21487;&#38752;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.15183</link><description>&lt;p&gt;
GraphEdit&#65306;&#29992;&#20110;&#22270;&#32467;&#26500;&#23398;&#20064;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
GraphEdit: Large Language Models for Graph Structure Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15183
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GraphEdit&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23398;&#20064;&#22797;&#26434;&#30340;&#22270;&#32467;&#26500;&#21270;&#25968;&#25454;&#20013;&#30340;&#33410;&#28857;&#20851;&#31995;&#65292;&#36890;&#36807;&#22312;&#22270;&#32467;&#26500;&#19978;&#36827;&#34892;&#25351;&#23548;&#35843;&#25972;&#65292;&#22686;&#24378;LLMs&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#20174;&#32780;&#25552;&#39640;&#22270;&#32467;&#26500;&#23398;&#20064;&#30340;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#32467;&#26500;&#23398;&#20064;&#65288;GSL&#65289;&#33268;&#21147;&#20110;&#36890;&#36807;&#29983;&#25104;&#26032;&#39062;&#30340;&#22270;&#32467;&#26500;&#26469;&#25429;&#25417;&#22270;&#32467;&#26500;&#25968;&#25454;&#20013;&#33410;&#28857;&#20043;&#38388;&#30340;&#22266;&#26377;&#20381;&#36182;&#24615;&#21644;&#30456;&#20114;&#20316;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GraphEdit&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23398;&#20064;&#22270;&#32467;&#26500;&#21270;&#25968;&#25454;&#20013;&#22797;&#26434;&#30340;&#33410;&#28857;&#20851;&#31995;&#12290;&#36890;&#36807;&#22312;&#22270;&#32467;&#26500;&#19978;&#36827;&#34892;&#25351;&#23548;&#35843;&#25972;&#65292;&#22686;&#24378;LLMs&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#25105;&#20204;&#26088;&#22312;&#20811;&#26381;&#26174;&#24335;&#22270;&#32467;&#26500;&#20449;&#24687;&#24102;&#26469;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#39640;&#22270;&#32467;&#26500;&#23398;&#20064;&#30340;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15183v1 Announce Type: cross  Abstract: Graph Structure Learning (GSL) focuses on capturing intrinsic dependencies and interactions among nodes in graph-structured data by generating novel graph structures. Graph Neural Networks (GNNs) have emerged as promising GSL solutions, utilizing recursive message passing to encode node-wise inter-dependencies. However, many existing GSL methods heavily depend on explicit graph structural information as supervision signals, leaving them susceptible to challenges such as data noise and sparsity. In this work, we propose GraphEdit, an approach that leverages large language models (LLMs) to learn complex node relationships in graph-structured data. By enhancing the reasoning capabilities of LLMs through instruction-tuning over graph structures, we aim to overcome the limitations associated with explicit graph structural information and enhance the reliability of graph structure learning. Our approach not only effectively denoises noisy co
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#25506;&#35752;&#20102;&#8220;&#34987;&#36951;&#24536;&#26435;&#8221;&#30340;&#27010;&#24565;&#65292;&#25552;&#20986;&#26426;&#22120;&#36951;&#24536;&#20316;&#20026;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#22312;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#24314;&#31435;&#20102;&#20840;&#38754;&#30340;&#36951;&#24536;&#26694;&#26550;&#21450;&#39640;&#25928;&#30340;&#36951;&#24536;&#26041;&#27861;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#25913;&#36827;&#36229;&#21442;&#25968;&#40065;&#26834;&#24615;&#20197;&#21450;&#39640;&#25928;&#35843;&#25972;&#36229;&#21442;&#25968;&#30340;&#25351;&#21335;&#12290;</title><link>https://arxiv.org/abs/2402.15159</link><description>&lt;p&gt;
&#38754;&#21521;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26426;&#22120;&#36951;&#24536;
&lt;/p&gt;
&lt;p&gt;
Machine Unlearning of Pre-trained Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15159
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#25506;&#35752;&#20102;&#8220;&#34987;&#36951;&#24536;&#26435;&#8221;&#30340;&#27010;&#24565;&#65292;&#25552;&#20986;&#26426;&#22120;&#36951;&#24536;&#20316;&#20026;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#22312;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#24314;&#31435;&#20102;&#20840;&#38754;&#30340;&#36951;&#24536;&#26694;&#26550;&#21450;&#39640;&#25928;&#30340;&#36951;&#24536;&#26041;&#27861;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#25913;&#36827;&#36229;&#21442;&#25968;&#40065;&#26834;&#24615;&#20197;&#21450;&#39640;&#25928;&#35843;&#25972;&#36229;&#21442;&#25968;&#30340;&#25351;&#21335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#32972;&#26223;&#19979;&#8220;&#34987;&#36951;&#24536;&#26435;&#8221;&#30340;&#27010;&#24565;&#12290;&#25105;&#20204;&#20197;&#26426;&#22120;&#36951;&#24536;&#20316;&#20026;&#19968;&#20010;&#20851;&#38190;&#35299;&#20915;&#26041;&#26696;&#65292;&#37325;&#28857;&#20851;&#27880;&#39044;&#35757;&#32451;&#27169;&#22411;&#8212;&#8212;&#19968;&#20010;&#26126;&#26174;&#32570;&#20047;&#30740;&#31350;&#30340;&#39046;&#22495;&#12290;&#25105;&#20204;&#22312;&#39044;&#35757;&#32451;LLMs&#20013;&#21246;&#21202;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#26426;&#22120;&#36951;&#24536;&#26694;&#26550;&#65292;&#21253;&#25324;&#23545;&#19971;&#31181;&#19981;&#21516;&#36951;&#24536;&#26041;&#27861;&#30340;&#25209;&#21028;&#24615;&#20998;&#26512;&#12290;&#36890;&#36807;&#20351;&#29992;&#26469;&#33258;arXiv&#12289;&#20070;&#31821;&#21644;GitHub&#30340;&#31574;&#21010;&#25968;&#25454;&#38598;&#36827;&#34892;&#20005;&#26684;&#35780;&#20272;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#26377;&#21147;&#30340;&#26426;&#22120;&#36951;&#24536;&#24615;&#33021;&#22522;&#20934;&#65292;&#34920;&#26126;&#36825;&#20123;&#26041;&#27861;&#30340;&#35745;&#31639;&#25928;&#29575;&#27604;&#37325;&#26032;&#35757;&#32451;&#39640;&#20986; $10^5$ &#20493;&#20197;&#19978;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#20998;&#24067;&#25968;&#25454;&#19978;&#23558;&#26799;&#24230;&#19978;&#21319;&#19982;&#26799;&#24230;&#19979;&#38477;&#32467;&#21512;&#21487;&#20197;&#25913;&#21892;&#36229;&#21442;&#25968;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#20851;&#20110;&#22312;&#36951;&#24536;&#36807;&#31243;&#20013;&#36827;&#34892;&#39640;&#25928;&#36229;&#21442;&#25968;&#35843;&#25972;&#30340;&#35814;&#32454;&#25351;&#21335;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25512;&#21160;&#20102;&#26377;&#20851;&#20262;&#29702;&#20154;&#24037;&#26234;&#33021;&#23454;&#36341;&#30340;&#35752;&#35770;&#65292;&#25552;&#20379;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15159v1 Announce Type: cross  Abstract: This study investigates the concept of the `right to be forgotten' within the context of large language models (LLMs). We explore machine unlearning as a pivotal solution, with a focus on pre-trained models--a notably under-researched area. Our research delineates a comprehensive framework for machine unlearning in pre-trained LLMs, encompassing a critical analysis of seven diverse unlearning methods. Through rigorous evaluation using curated datasets from arXiv, books, and GitHub, we establish a robust benchmark for unlearning performance, demonstrating that these methods are over $10^5$ times more computationally efficient than retraining. Our results show that integrating gradient ascent with gradient descent on in-distribution data improves hyperparameter robustness. We also provide detailed guidelines for efficient hyperparameter tuning in the unlearning process. Our findings advance the discourse on ethical AI practices, offering
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35821;&#20041;&#38236;&#20687;&#36234;&#29425;&#65288;SMJ&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#22312;&#35821;&#20041;&#19978;&#31867;&#20284;&#20110;&#21407;&#22987;&#38382;&#39064;&#30340;&#36234;&#29425;&#25552;&#31034;&#26469;&#32469;&#36807;LLMs&#12290;</title><link>https://arxiv.org/abs/2402.14872</link><description>&lt;p&gt;
&#35821;&#20041;&#38236;&#20687;&#36234;&#29425;:&#22522;&#20110;&#36951;&#20256;&#31639;&#27861;&#30340;&#38024;&#23545;&#24320;&#28304;LLM&#30340;&#36234;&#29425;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
Semantic Mirror Jailbreak: Genetic Algorithm Based Jailbreak Prompts Against Open-source LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14872
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35821;&#20041;&#38236;&#20687;&#36234;&#29425;&#65288;SMJ&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#22312;&#35821;&#20041;&#19978;&#31867;&#20284;&#20110;&#21407;&#22987;&#38382;&#39064;&#30340;&#36234;&#29425;&#25552;&#31034;&#26469;&#32469;&#36807;LLMs&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36890;&#24120;&#29992;&#20110;&#21019;&#24847;&#20889;&#20316;&#12289;&#20195;&#30721;&#29983;&#25104;&#21644;&#32763;&#35793;&#65292;&#26681;&#25454;&#36755;&#20837;&#24207;&#21015;&#29983;&#25104;&#25991;&#26412;&#65292;&#20294;&#23481;&#26131;&#21463;&#21040;&#36234;&#29425;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#20854;&#20013;&#31934;&#24515;&#35774;&#35745;&#30340;&#25552;&#31034;&#20250;&#23548;&#33268;&#26377;&#23475;&#36755;&#20986;&#12290;&#22823;&#22810;&#25968;&#36234;&#29425;&#25552;&#31034;&#26041;&#27861;&#20351;&#29992;&#19968;&#32452;&#36234;&#29425;&#27169;&#26495;&#65292;&#28982;&#21518;&#36319;&#38543;&#25552;&#20986;&#38382;&#39064;&#65292;&#21019;&#24314;&#36234;&#29425;&#25552;&#31034;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#36234;&#29425;&#25552;&#31034;&#35774;&#35745;&#36890;&#24120;&#23384;&#22312;&#36807;&#22810;&#30340;&#35821;&#20041;&#24046;&#24322;&#65292;&#23548;&#33268;&#26080;&#27861;&#25269;&#24481;&#20351;&#29992;&#31616;&#21333;&#35821;&#20041;&#24230;&#37327;&#20316;&#20026;&#38408;&#20540;&#30340;&#38450;&#24481;&#12290;&#36234;&#29425;&#25552;&#31034;&#22312;&#35821;&#20041;&#19978;&#27604;&#29992;&#20110;&#26597;&#35810;&#30340;&#21407;&#22987;&#38382;&#39064;&#26356;&#21152;&#22810;&#26679;&#21270;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;&#35821;&#20041;&#38236;&#20687;&#36234;&#29425;&#65288;SMJ&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#22312;&#35821;&#20041;&#19978;&#31867;&#20284;&#20110;&#21407;&#22987;&#38382;&#39064;&#30340;&#36234;&#29425;&#25552;&#31034;&#26469;&#32469;&#36807;LLMs&#12290;&#25105;&#20204;&#23558;&#23547;&#25214;&#26082;&#28385;&#36275;&#35821;&#20041;&#30456;&#20284;&#24615;&#21448;&#20855;&#26377;&#36234;&#29425;&#26377;&#25928;&#24615;&#30340;&#36234;&#29425;&#25552;&#31034;&#24314;&#27169;&#20026;&#19968;&#20010;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14872v1 Announce Type: cross  Abstract: Large Language Models (LLMs), used in creative writing, code generation, and translation, generate text based on input sequences but are vulnerable to jailbreak attacks, where crafted prompts induce harmful outputs. Most jailbreak prompt methods use a combination of jailbreak templates followed by questions to ask to create jailbreak prompts. However, existing jailbreak prompt designs generally suffer from excessive semantic differences, resulting in an inability to resist defenses that use simple semantic metrics as thresholds. Jailbreak prompts are semantically more varied than the original questions used for queries. In this paper, we introduce a Semantic Mirror Jailbreak (SMJ) approach that bypasses LLMs by generating jailbreak prompts that are semantically similar to the original question. We model the search for jailbreak prompts that satisfy both semantic similarity and jailbreak validity as a multi-objective optimization proble
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Continual Optimal Policy Regularization (COPR) &#26041;&#27861;&#65292;&#36890;&#36807;&#20511;&#37492;&#26368;&#20248;&#31574;&#30053;&#29702;&#35770;&#65292;&#21033;&#29992;&#37319;&#26679;&#20998;&#24067;&#20316;&#20026;&#31034;&#33539;&#21644;&#27491;&#21017;&#21270;&#32422;&#26463;&#65292;&#20197;&#21160;&#24577;&#22320;&#23545;&#24403;&#21069;&#31574;&#30053;&#36827;&#34892;&#27491;&#21017;&#21270;&#65292;&#20174;&#32780;&#20351;&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;&#22312;&#25345;&#32493;&#23398;&#20064;&#24773;&#22659;&#19979;&#26356;&#21152;&#31283;&#20581;</title><link>https://arxiv.org/abs/2402.14228</link><description>&lt;p&gt;
COPR:&#36890;&#36807;&#26368;&#20248;&#31574;&#30053;&#27491;&#21017;&#21270;&#23454;&#29616;&#25345;&#32493;&#20154;&#31867;&#20559;&#22909;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
COPR: Continual Human Preference Learning via Optimal Policy Regularization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14228
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Continual Optimal Policy Regularization (COPR) &#26041;&#27861;&#65292;&#36890;&#36807;&#20511;&#37492;&#26368;&#20248;&#31574;&#30053;&#29702;&#35770;&#65292;&#21033;&#29992;&#37319;&#26679;&#20998;&#24067;&#20316;&#20026;&#31034;&#33539;&#21644;&#27491;&#21017;&#21270;&#32422;&#26463;&#65292;&#20197;&#21160;&#24577;&#22320;&#23545;&#24403;&#21069;&#31574;&#30053;&#36827;&#34892;&#27491;&#21017;&#21270;&#65292;&#20174;&#32780;&#20351;&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;&#22312;&#25345;&#32493;&#23398;&#20064;&#24773;&#22659;&#19979;&#26356;&#21152;&#31283;&#20581;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14228v1 &#20844;&#21578;&#31867;&#22411;:&#36328;&#30028; &#25688;&#35201;: &#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;&#65288;RLHF&#65289;&#36890;&#24120;&#29992;&#20110;&#25913;&#21892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#20154;&#31867;&#20559;&#22909;&#30340;&#23545;&#40784;&#12290;&#37492;&#20110;&#20154;&#31867;&#20559;&#22909;&#30340;&#19981;&#26029;&#21464;&#21270;&#65292;&#25345;&#32493;&#23545;&#40784;&#30456;&#23545;&#20110;&#20256;&#32479;&#38745;&#24577;&#23545;&#40784;&#21464;&#24471;&#26356;&#21152;&#37325;&#35201;&#21644;&#23454;&#38469;&#12290;&#28982;&#32780;&#65292;&#20351;RLHF&#19982;&#25345;&#32493;&#23398;&#20064;&#65288;CL&#65289;&#20860;&#23481;&#30001;&#20110;&#20854;&#22797;&#26434;&#36807;&#31243;&#32780;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#21516;&#26102;&#65292;&#30452;&#25509;&#23398;&#20064;&#26032;&#30340;&#20154;&#31867;&#20559;&#22909;&#21487;&#33021;&#23548;&#33268;&#21382;&#21490;&#20559;&#22909;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#65288;CF&#65289;&#65292;&#23548;&#33268;&#26080;&#21161;&#25110;&#26377;&#23475;&#30340;&#32467;&#26524;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Continual Optimal Policy Regularization (COPR) &#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20511;&#37492;&#20102;&#26368;&#20248;&#31574;&#30053;&#29702;&#35770;&#12290;COPR&#21033;&#29992;&#37319;&#26679;&#20998;&#24067;&#20316;&#20026;&#31034;&#33539;&#21644;&#27491;&#21017;&#21270;&#32422;&#26463;&#29992;&#20110;&#25345;&#32493;&#23398;&#20064;&#12290;&#23427;&#37319;&#29992;Lagrange&#23545;&#20598;&#65288;LD&#65289;&#26041;&#27861;&#26681;&#25454;&#21382;&#21490;&#19978;&#30340;&#26368;&#20248;&#31574;&#30053;&#21160;&#24577;&#22320;&#27491;&#21017;&#21270;&#24403;&#21069;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14228v1 Announce Type: cross  Abstract: Reinforcement Learning from Human Feedback (RLHF) is commonly utilized to improve the alignment of Large Language Models (LLMs) with human preferences. Given the evolving nature of human preferences, continual alignment becomes more crucial and practical in comparison to traditional static alignment. Nevertheless, making RLHF compatible with Continual Learning (CL) is challenging due to its complex process. Meanwhile, directly learning new human preferences may lead to Catastrophic Forgetting (CF) of historical preferences, resulting in helpless or harmful outputs. To overcome these challenges, we propose the Continual Optimal Policy Regularization (COPR) method, which draws inspiration from the optimal policy theory. COPR utilizes a sampling distribution as a demonstration and regularization constraints for CL. It adopts the Lagrangian Duality (LD) method to dynamically regularize the current policy based on the historically optimal p
&lt;/p&gt;</description></item><item><title>&#23558;&#22522;&#30784;&#27169;&#22411;&#30340;&#30693;&#35782;&#36716;&#31227;&#21040;&#19987;&#29992;&#24212;&#29992;&#27169;&#22411;&#20013;&#23384;&#22312;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#36890;&#36807;&#21019;&#24314;&#25945;&#23398;&#22996;&#21592;&#20250;&#26469;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.14035</link><description>&lt;p&gt;
&#22996;&#21592;&#20250;&#30340;&#26234;&#24935;&#65306;&#20174;&#22522;&#30784;&#27169;&#22411;&#21040;&#19987;&#29992;&#24212;&#29992;&#27169;&#22411;&#30340;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Wisdom of Committee: Distilling from Foundation Model to SpecializedApplication Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14035
&lt;/p&gt;
&lt;p&gt;
&#23558;&#22522;&#30784;&#27169;&#22411;&#30340;&#30693;&#35782;&#36716;&#31227;&#21040;&#19987;&#29992;&#24212;&#29992;&#27169;&#22411;&#20013;&#23384;&#22312;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#36890;&#36807;&#21019;&#24314;&#25945;&#23398;&#22996;&#21592;&#20250;&#26469;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22522;&#30784;&#27169;&#22411;&#30340;&#36827;&#23637;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#65292;&#19982;&#27492;&#21516;&#26102;&#65292;&#20026;&#29305;&#23450;&#24212;&#29992;&#65292;&#20174;&#19994;&#32773;&#20204;&#19968;&#30452;&#22312;&#24320;&#21457;&#19987;&#38376;&#30340;&#24212;&#29992;&#27169;&#22411;&#12290;&#20026;&#20102;&#20139;&#21463;&#36825;&#20004;&#31181;&#27169;&#22411;&#30340;&#22909;&#22788;&#65292;&#19968;&#20010;&#33258;&#28982;&#30340;&#36335;&#24452;&#26159;&#23558;&#22522;&#30784;&#27169;&#22411;&#20013;&#30340;&#30693;&#35782;&#36716;&#31227;&#21040;&#19987;&#29992;&#24212;&#29992;&#27169;&#22411;&#20013;&#65292;&#21518;&#32773;&#36890;&#24120;&#26356;&#26377;&#25928;&#22320;&#25552;&#20379;&#26381;&#21153;&#12290;&#30693;&#35782;&#33976;&#39311;&#30340;&#25216;&#26415;&#21487;&#20197;&#22312;&#36825;&#37324;&#24212;&#29992;&#65292;&#20854;&#20013;&#24212;&#29992;&#27169;&#22411;&#23398;&#20250;&#27169;&#20223;&#22522;&#30784;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#19987;&#29992;&#24212;&#29992;&#27169;&#22411;&#21644;&#22522;&#30784;&#27169;&#22411;&#22312;&#23481;&#37327;&#19978;&#23384;&#22312;&#23454;&#36136;&#24615;&#24046;&#36317;&#65292;&#37319;&#29992;&#19981;&#21516;&#30340;&#26550;&#26500;&#65292;&#20351;&#29992;&#26469;&#33258;&#19981;&#21516;&#27169;&#24577;&#30340;&#19981;&#21516;&#36755;&#20837;&#29305;&#24449;&#65292;&#24182;&#22312;&#19981;&#21516;&#30340;&#20998;&#24067;&#19978;&#36827;&#34892;&#20248;&#21270;&#12290;&#27169;&#22411;&#29305;&#24449;&#19978;&#30340;&#36825;&#20123;&#24046;&#24322;&#23548;&#33268;&#20102;&#33976;&#39311;&#26041;&#27861;&#38754;&#20020;&#37325;&#22823;&#25361;&#25112;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#21019;&#24314;&#19968;&#20010;&#25945;&#23398;&#22996;&#21592;&#20250;&#65292;&#21253;&#25324;&#22522;&#30784;&#27169;&#22411;&#21644;&#19987;&#29992;&#24212;&#29992;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14035v1 Announce Type: cross  Abstract: Recent advancements in foundation models have yielded impressive performance across a wide range of tasks. Meanwhile, for specific applications, practitioners have been developing specialized application models. To enjoy the benefits of both kinds of models, one natural path is to transfer the knowledge in foundation models into specialized application models, which are generally more efficient for serving. Techniques from knowledge distillation may be applied here, where the application model learns to mimic the foundation model. However, specialized application models and foundation models have substantial gaps in capacity, employing distinct architectures, using different input features from different modalities, and being optimized on different distributions. These differences in model characteristics lead to significant challenges for distillation methods. In this work, we propose creating a teaching committee comprising both foun
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;"ProSparse"&#30340;&#26377;&#25928;&#31232;&#30095;&#21270;&#26041;&#27861;&#65292;&#20197;&#25512;&#21160;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#26356;&#39640;&#30340;&#28608;&#27963;&#31232;&#30095;&#24615;&#32780;&#19981;&#38477;&#20302;&#27169;&#22411;&#24615;&#33021;</title><link>https://arxiv.org/abs/2402.13516</link><description>&lt;p&gt;
ProSparse: &#24341;&#20837;&#21644;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20869;&#37096;&#28608;&#27963;&#31232;&#30095;&#24615;
&lt;/p&gt;
&lt;p&gt;
ProSparse: Introducing and Enhancing Intrinsic Activation Sparsity within Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13516
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;"ProSparse"&#30340;&#26377;&#25928;&#31232;&#30095;&#21270;&#26041;&#27861;&#65292;&#20197;&#25512;&#21160;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#26356;&#39640;&#30340;&#28608;&#27963;&#31232;&#30095;&#24615;&#32780;&#19981;&#38477;&#20302;&#27169;&#22411;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Activation sparsity&#25351;&#30340;&#26159;&#28608;&#27963;&#36755;&#20986;&#20013;&#23384;&#22312;&#35768;&#22810;&#24369;&#36129;&#29486;&#20803;&#32032;&#12290;&#20316;&#20026;&#20351;&#29992;ReLU&#28608;&#27963;&#20989;&#25968;&#30340;&#27169;&#22411;&#30340;&#26222;&#36941;&#23646;&#24615;&#65292;&#24050;&#34987;&#35777;&#26126;&#26159;&#25552;&#39640;&#27169;&#22411;&#25512;&#29702;&#25928;&#29575;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#33539;&#20363;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#37319;&#29992;&#20102;&#27809;&#26377;&#20869;&#22312;&#28608;&#27963;&#31232;&#30095;&#24615;&#30340;&#28608;&#27963;&#20989;&#25968;&#65288;&#20363;&#22914;GELU&#21644;Swish&#65289;&#12290;&#19968;&#20123;&#26368;&#36817;&#30340;&#21162;&#21147;&#23581;&#35797;&#24341;&#20837;ReLU&#25110;&#20854;&#21464;&#20307;&#20316;&#20026;&#26367;&#20195;&#28608;&#27963;&#20989;&#25968;&#65292;&#20197;&#24110;&#21161;LLMs&#23454;&#29616;&#28608;&#27963;&#31232;&#30095;&#24615;&#21644;&#25512;&#29702;&#21152;&#36895;&#65292;&#20294;&#24456;&#23569;&#33021;&#21516;&#26102;&#33719;&#24471;&#39640;&#31232;&#30095;&#24230;&#21644;&#21487;&#27604;&#36739;&#30340;&#27169;&#22411;&#24615;&#33021;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;"ProSparse"&#30340;&#26377;&#25928;&#31232;&#30095;&#21270;&#26041;&#27861;&#65292;&#20197;&#25512;&#21160;LLMs&#23454;&#29616;&#26356;&#39640;&#30340;&#28608;&#27963;&#31232;&#30095;&#24615;&#32780;&#19981;&#38477;&#20302;&#27169;&#22411;&#24615;&#33021;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#23558;LLMs&#30340;&#28608;&#27963;&#20989;&#25968;&#26367;&#25442;&#20026;ReLU&#21518;&#65292;ProSparse&#37319;&#29992;&#28176;&#36827;&#31232;&#30095;&#27491;&#21017;&#21270;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13516v1 Announce Type: cross  Abstract: Activation sparsity refers to the existence of considerable weakly-contributed elements among activation outputs. As a prevalent property of the models using the ReLU activation function, it has been proven a promising paradigm to boost model inference efficiency. Nevertheless, most large language models (LLMs) adopt activation functions without intrinsic activation sparsity (e.g., GELU and Swish). Some recent efforts have explored introducing ReLU or its variants as the substitutive activation function to help LLMs achieve activation sparsity and inference acceleration, but few can simultaneously obtain high sparsity and comparable model performance. This paper introduces an effective sparsification method named "ProSparse" to push LLMs for higher activation sparsity without decreasing model performance. Specifically, after substituting the activation function of LLMs with ReLU, ProSparse adopts progressive sparsity regularization wit
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#32852;&#37030;&#22240;&#26524;&#21457;&#29616;&#26041;&#27861;&#65292;&#26088;&#22312;&#36866;&#24212;&#20219;&#24847;&#22240;&#26524;&#27169;&#22411;&#21644;&#24322;&#26500;&#25968;&#25454;&#65292;&#36890;&#36807;&#20351;&#29992;&#26367;&#20195;&#21464;&#37327;&#21644;&#32852;&#37030;&#26465;&#20214;&#29420;&#31435;&#24615;&#26816;&#39564;&#26469;&#35299;&#20915;&#25968;&#25454;&#24322;&#36136;&#24615;&#65292;&#24182;&#24314;&#31435;&#20102;&#32852;&#37030;&#29420;&#31435;&#21464;&#21270;&#21407;&#21017;&#29992;&#20110;&#30830;&#23450;&#22240;&#26524;&#26041;&#21521;&#12290;</title><link>https://arxiv.org/abs/2402.13241</link><description>&lt;p&gt;
&#26469;&#33258;&#24322;&#26500;&#25968;&#25454;&#30340;&#32852;&#37030;&#22240;&#26524;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Federated Causal Discovery from Heterogeneous Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13241
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#32852;&#37030;&#22240;&#26524;&#21457;&#29616;&#26041;&#27861;&#65292;&#26088;&#22312;&#36866;&#24212;&#20219;&#24847;&#22240;&#26524;&#27169;&#22411;&#21644;&#24322;&#26500;&#25968;&#25454;&#65292;&#36890;&#36807;&#20351;&#29992;&#26367;&#20195;&#21464;&#37327;&#21644;&#32852;&#37030;&#26465;&#20214;&#29420;&#31435;&#24615;&#26816;&#39564;&#26469;&#35299;&#20915;&#25968;&#25454;&#24322;&#36136;&#24615;&#65292;&#24182;&#24314;&#31435;&#20102;&#32852;&#37030;&#29420;&#31435;&#21464;&#21270;&#21407;&#21017;&#29992;&#20110;&#30830;&#23450;&#22240;&#26524;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#22240;&#26524;&#21457;&#29616;&#26041;&#27861;&#20381;&#36182;&#20110;&#38598;&#20013;&#24335;&#25968;&#25454;&#65292;&#36825;&#19982;&#35768;&#22810;&#23454;&#38469;&#24773;&#20917;&#19979;&#25968;&#25454;&#30340;&#20998;&#25955;&#24615;&#36136;&#19981;&#19968;&#33268;&#12290;&#36825;&#31181;&#24046;&#24322;&#25512;&#21160;&#20102;&#32852;&#37030;&#22240;&#26524;&#21457;&#29616;&#65288;FCD&#65289;&#26041;&#27861;&#30340;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;FCD&#26041;&#27861;&#21487;&#33021;&#21463;&#21040;&#20854;&#23545;&#21487;&#35782;&#21035;&#21151;&#33021;&#22240;&#26524;&#27169;&#22411;&#25110; homogeneous&#25968;&#25454;&#20998;&#24067;&#30340;&#28508;&#22312;&#38480;&#21046;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#21508;&#31181;&#22330;&#26223;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23581;&#35797;&#36866;&#24212;&#20219;&#24847;&#22240;&#26524;&#27169;&#22411;&#21644;&#24322;&#26500;&#25968;&#25454;&#30340;&#26032;&#22411;FCD&#26041;&#27861;&#12290;&#25105;&#20204;&#39318;&#20808;&#21033;&#29992;&#19982;&#23458;&#25143;&#31471;&#32034;&#24341;&#23545;&#24212;&#30340;&#26367;&#20195;&#21464;&#37327;&#65292;&#20197;&#35299;&#20915;&#19981;&#21516;&#23458;&#25143;&#31471;&#20043;&#38388;&#30340;&#25968;&#25454;&#24322;&#36136;&#24615;&#12290;&#28982;&#21518;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#29992;&#20110;&#22240;&#26524;&#39592;&#26550;&#21457;&#29616;&#30340;&#32852;&#37030;&#26465;&#20214;&#29420;&#31435;&#24615;&#26816;&#39564;&#65288;FCIT&#65289;&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#20010;&#29992;&#20110;&#30830;&#23450;&#22240;&#26524;&#26041;&#21521;&#30340;&#32852;&#37030;&#29420;&#31435;&#21464;&#21270;&#21407;&#21017;&#65288;FICP&#65289;&#12290;&#36825;&#20123;&#26041;&#27861;&#28041;&#21450;&#26500;&#24314;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13241v1 Announce Type: cross  Abstract: Conventional causal discovery methods rely on centralized data, which is inconsistent with the decentralized nature of data in many real-world situations. This discrepancy has motivated the development of federated causal discovery (FCD) approaches. However, existing FCD methods may be limited by their potentially restrictive assumptions of identifiable functional causal models or homogeneous data distributions, narrowing their applicability in diverse scenarios. In this paper, we propose a novel FCD method attempting to accommodate arbitrary causal models and heterogeneous data. We first utilize a surrogate variable corresponding to the client index to account for the data heterogeneity across different clients. We then develop a federated conditional independence test (FCIT) for causal skeleton discovery and establish a federated independent change principle (FICP) to determine causal directions. These approaches involve constructing
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20869;&#22312;&#33258;&#25105;&#26657;&#27491;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#8220;&#22914;&#26524;-&#21542;&#21017;&#8221;&#65288;IoE&#65289;&#25552;&#31034;&#26694;&#26550;&#65292;&#24110;&#21161;&#27169;&#22411;&#35780;&#20272;&#33258;&#36523;&#8220;&#20449;&#24515;&#8221;&#24182;&#36827;&#34892;&#33258;&#25105;&#26657;&#27491;&#12290;</title><link>https://arxiv.org/abs/2402.12563</link><description>&lt;p&gt;
&#20449;&#24515;&#33267;&#20851;&#37325;&#35201;&#65306;&#37325;&#26032;&#23457;&#35270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20869;&#22312;&#33258;&#25105;&#26657;&#27491;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Confidence Matters: Revisiting Intrinsic Self-Correction Capabilities of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12563
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20869;&#22312;&#33258;&#25105;&#26657;&#27491;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#8220;&#22914;&#26524;-&#21542;&#21017;&#8221;&#65288;IoE&#65289;&#25552;&#31034;&#26694;&#26550;&#65292;&#24110;&#21161;&#27169;&#22411;&#35780;&#20272;&#33258;&#36523;&#8220;&#20449;&#24515;&#8221;&#24182;&#36827;&#34892;&#33258;&#25105;&#26657;&#27491;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26368;&#36817;&#25104;&#21151;&#28608;&#21457;&#20102;&#23545;&#23427;&#20204;&#33258;&#25105;&#26657;&#27491;&#33021;&#21147;&#30340;&#36234;&#26469;&#36234;&#22810;&#30340;&#20852;&#36259;&#12290;&#26412;&#25991;&#23545;LLMs&#30340;&#20869;&#22312;&#33258;&#25105;&#26657;&#27491;&#36827;&#34892;&#20102;&#20840;&#38754;&#35843;&#26597;&#65292;&#35797;&#22270;&#35299;&#20915;&#20851;&#20110;&#20854;&#21487;&#34892;&#24615;&#30340;&#25345;&#32493;&#20105;&#35770;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#30830;&#23450;&#20102;&#19968;&#20010;&#37325;&#35201;&#30340;&#28508;&#22312;&#22240;&#32032; - LLMs&#30340;&#8220;&#20449;&#24515;&#8221; - &#22312;&#33258;&#25105;&#26657;&#27491;&#36807;&#31243;&#20013;&#12290;&#24573;&#35270;&#36825;&#19968;&#22240;&#32032;&#21487;&#33021;&#23548;&#33268;&#27169;&#22411;&#36807;&#24230;&#25209;&#35780;&#33258;&#24049;&#65292;&#20174;&#32780;&#23548;&#33268;&#23545;&#33258;&#26657;&#27491;&#25928;&#26524;&#30340;&#21487;&#38752;&#32467;&#35770;&#19981;&#20934;&#30830;&#12290;&#25105;&#20204;&#23454;&#39564;&#35266;&#23519;&#21040;LLMs&#20855;&#26377;&#29702;&#35299;&#20854;&#33258;&#36523;&#22238;&#24212;&#8220;&#20449;&#24515;&#8221;&#30340;&#33021;&#21147;&#12290;&#36825;&#28608;&#21169;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#8220;&#22914;&#26524;-&#21542;&#21017;&#8221;&#65288;IoE&#65289;&#25552;&#31034;&#26694;&#26550;&#65292;&#26088;&#22312;&#24341;&#23548;LLMs&#35780;&#20272;&#20854;&#33258;&#36523;&#8220;&#20449;&#24515;&#8221;&#65292;&#20419;&#36827;&#20869;&#22312;&#33258;&#25105;&#26657;&#27491;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#22522;&#20110;IoE&#30340;&#25552;&#31034;&#21487;&#20197;&#23454;&#29616;&#19968;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12563v1 Announce Type: cross  Abstract: The recent success of Large Language Models (LLMs) has catalyzed an increasing interest in their self-correction capabilities. This paper presents a comprehensive investigation into the intrinsic self-correction of LLMs, attempting to address the ongoing debate about its feasibility. Our research has identified an important latent factor - the ``confidence'' of LLMs - during the self-correction process. Overlooking this factor may cause the models to over-criticize themselves, resulting in unreliable conclusions regarding the efficacy of self-correction. We have experimentally observed that LLMs possess the capability to understand the ``confidence'' in their own responses. It motivates us to develop an ``If-or-Else'' (IoE) prompting framework, designed to guide LLMs in assessing their own ``confidence'', facilitating intrinsic self-corrections. We conduct extensive experiments and demonstrate that our IoE-based Prompt can achieve a co
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;&#39057;&#29575;&#31354;&#38388;&#30340;&#20998;&#26512;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#23610;&#24230;&#20302;&#31209;&#36866;&#24212;&#65288;MuScleLoRA&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22312;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#26102;&#21463;&#21040;&#21518;&#38376;&#25915;&#20987;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.12026</link><description>&lt;p&gt;
&#20174;&#21518;&#38376;&#27602;&#21270;&#25968;&#25454;&#38598;&#20013;&#36890;&#36807;&#38477;&#39057;&#31354;&#38388;&#33719;&#21462;&#28165;&#27905;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Acquiring Clean Language Models from Backdoor Poisoned Datasets by Downscaling Frequency Space
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12026
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;&#39057;&#29575;&#31354;&#38388;&#30340;&#20998;&#26512;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#23610;&#24230;&#20302;&#31209;&#36866;&#24212;&#65288;MuScleLoRA&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22312;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#26102;&#21463;&#21040;&#21518;&#38376;&#25915;&#20987;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#21151;&#65292;&#20294;LMs&#30340;&#21487;&#38752;&#24615;&#23481;&#26131;&#21463;&#21040;&#21518;&#38376;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#23581;&#35797;&#22312;&#27602;&#21270;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;LMs&#26102;&#20943;&#36731;&#21518;&#38376;&#23398;&#20064;&#65292;&#20294;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#25269;&#24481;&#22797;&#26434;&#30340;&#21518;&#38376;&#25915;&#20987;&#26102;&#20173;&#28982;&#38754;&#20020;&#22256;&#38590;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#20613;&#37324;&#21494;&#20998;&#26512;&#30740;&#31350;&#20102;&#39057;&#29575;&#31354;&#38388;&#20013;&#21518;&#38376;LMs&#30340;&#23398;&#20064;&#26426;&#21046;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#27602;&#21270;&#25968;&#25454;&#38598;&#19978;&#21576;&#29616;&#30340;&#21518;&#38376;&#26144;&#23556;&#30456;&#27604;&#28165;&#27905;&#26144;&#23556;&#26356;&#20542;&#21521;&#20110;&#36739;&#20302;&#39057;&#29575;&#65292;&#23548;&#33268;&#21518;&#38376;&#26144;&#23556;&#26356;&#24555;&#22320;&#25910;&#25947;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#22256;&#22659;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22810;&#23610;&#24230;&#20302;&#31209;&#36866;&#24212;&#65288;MuScleLoRA&#65289;&#65292;&#23427;&#22312;&#39057;&#29575;&#31354;&#38388;&#20013;&#37096;&#32626;&#22810;&#20010;&#24452;&#21521;&#32553;&#25918;&#65292;&#20302;&#31209;&#36866;&#24212;&#30446;&#26631;&#27169;&#22411;&#65292;&#24182;&#22312;&#26356;&#26032;&#21442;&#25968;&#26102;&#36827;&#19968;&#27493;&#35843;&#25972;&#26799;&#24230;&#12290;&#36890;&#36807;&#38477;&#39057;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12026v1 Announce Type: cross  Abstract: Despite the notable success of language models (LMs) in various natural language processing (NLP) tasks, the reliability of LMs is susceptible to backdoor attacks. Prior research attempts to mitigate backdoor learning while training the LMs on the poisoned dataset, yet struggles against complex backdoor attacks in real-world scenarios. In this paper, we investigate the learning mechanisms of backdoor LMs in the frequency space by Fourier analysis. Our findings indicate that the backdoor mapping presented on the poisoned datasets exhibits a more discernible inclination towards lower frequency compared to clean mapping, resulting in the faster convergence of backdoor mapping. To alleviate this dilemma, we propose Multi-Scale Low-Rank Adaptation (MuScleLoRA), which deploys multiple radial scalings in the frequency space with low-rank adaptation to the target model and further aligns the gradients when updating parameters. Through downscal
&lt;/p&gt;</description></item><item><title>&#21457;&#29616;&#28145;&#23618;ReLU&#32593;&#32476;&#34920;&#29616;&#20986;&#36807;&#24230;&#27867;&#21270;&#29616;&#35937;&#65292;&#21033;&#29992;&#36825;&#19968;&#29305;&#24615;&#35774;&#35745;&#20102;&#8220;&#29983;&#25104;&#19975;&#33457;&#31570;&#32593;&#32476;&#8221;&#65292;&#36890;&#36807;&#36882;&#24402;&#26144;&#23556;&#38543;&#26426;&#36755;&#20837;&#22122;&#22768;&#29983;&#25104;&#26679;&#26412;&#12290;</title><link>https://arxiv.org/abs/2402.11793</link><description>&lt;p&gt;
&#29983;&#25104;&#19975;&#33457;&#31570;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Generative Kaleidoscopic Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11793
&lt;/p&gt;
&lt;p&gt;
&#21457;&#29616;&#28145;&#23618;ReLU&#32593;&#32476;&#34920;&#29616;&#20986;&#36807;&#24230;&#27867;&#21270;&#29616;&#35937;&#65292;&#21033;&#29992;&#36825;&#19968;&#29305;&#24615;&#35774;&#35745;&#20102;&#8220;&#29983;&#25104;&#19975;&#33457;&#31570;&#32593;&#32476;&#8221;&#65292;&#36890;&#36807;&#36882;&#24402;&#26144;&#23556;&#38543;&#26426;&#36755;&#20837;&#22122;&#22768;&#29983;&#25104;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21457;&#29616;&#28145;&#23618;ReLU&#32593;&#32476;&#65288;&#25110;&#22810;&#23618;&#24863;&#30693;&#22120;&#26550;&#26500;&#65289;&#34920;&#29616;&#20986;&#8220;&#36807;&#24230;&#27867;&#21270;&#8221;&#29616;&#35937;&#12290;&#20063;&#23601;&#26159;&#35828;&#65292;&#37027;&#20123;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#27809;&#26377;&#30475;&#21040;&#30340;&#36755;&#20837;&#30340;&#36755;&#20986;&#20540;&#34987;&#26144;&#23556;&#21040;&#20102;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#35266;&#23519;&#21040;&#30340;&#36755;&#20986;&#33539;&#22260;&#38468;&#36817;&#12290;&#25442;&#21477;&#35805;&#35828;&#65292;&#22810;&#23618;&#24863;&#30693;&#22120;&#23398;&#20064;&#20102;&#19968;&#23545;&#22810;&#30340;&#26144;&#23556;&#65292;&#36825;&#31181;&#25928;&#24212;&#22312;&#22686;&#21152;&#23618;&#25968;&#25110;&#22810;&#23618;&#24863;&#30693;&#22120;&#30340;&#28145;&#24230;&#26102;&#26356;&#20026;&#26126;&#26174;&#12290;&#25105;&#20204;&#21033;&#29992;&#20102;&#28145;&#23618;ReLU&#32593;&#32476;&#30340;&#36825;&#19968;&#29305;&#24615;&#26469;&#35774;&#35745;&#19968;&#20010;&#25968;&#25454;&#38598;&#19975;&#33457;&#31570;&#65292;&#31216;&#20026;&#8220;&#29983;&#25104;&#19975;&#33457;&#31570;&#32593;&#32476;&#8221;&#12290;&#31616;&#32780;&#35328;&#20043;&#65292;&#22914;&#26524;&#25105;&#20204;&#23398;&#20064;&#19968;&#20010;&#22810;&#23618;&#24863;&#30693;&#22120;&#23558;&#36755;&#20837; $x\in\mathbb{R}^D$ &#26144;&#23556;&#21040;&#33258;&#36523; $f_\mathcal{N}(x)\rightarrow x$&#65292;&#37027;&#20040;&#8220;&#19975;&#33457;&#31570;&#37319;&#26679;&#8221;&#36807;&#31243;&#23558;&#20174;&#38543;&#26426;&#36755;&#20837;&#22122;&#22768; $z\in\mathbb{R}^D$ &#24320;&#22987;&#65292;&#24182;&#36882;&#24402;&#22320;&#24212;&#29992; $f_\mathcal{N}(\cdots f_\mathcal{N}(z)\cdots )$&#12290;&#32463;&#36807;&#29123;&#28903;&#26399;&#21518;&#65292;&#25105;&#20204;&#24320;&#22987;&#35266;&#23519;&#26469;&#33258;&#36755;&#20837;&#20998;&#24067;&#30340;&#26679;&#26412;&#65292;&#25105;&#20204;&#21457;&#29616;&#26356;&#28145;&#30340;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11793v1 Announce Type: cross  Abstract: We discovered that the Deep ReLU networks (or Multilayer Perceptron architecture) demonstrate an 'over-generalization' phenomenon. That is, the output values for the inputs that were not seen during training are mapped close to the output range that were observed during the learning process. In other words, the MLP learns a many-to-one mapping and this effect is more prominent as we increase the number of layers or depth of the MLP. We utilize this property of Deep ReLU networks to design a dataset kaleidoscope, termed as 'Generative Kaleidoscopic Networks'. Briefly, if we learn a MLP to map from input $x\in\mathbb{R}^D$ to itself $f_\mathcal{N}(x)\rightarrow x$, the 'Kaleidoscopic sampling' procedure starts with a random input noise $z\in\mathbb{R}^D$ and recursively applies $f_\mathcal{N}(\cdots f_\mathcal{N}(z)\cdots )$. After a burn-in period duration, we start observing samples from the input distribution and we found that deeper 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#31995;&#32479;&#24615;&#22238;&#39038;&#20840;&#38754;&#20998;&#26512;&#20102;&#25968;&#25454;&#21040;&#25991;&#26412;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#30740;&#31350;&#30340;&#29616;&#29366;&#65292;&#25552;&#20986;&#26410;&#26469;&#26041;&#21521;&#65292;&#24182;&#35299;&#20915;&#20102;&#30456;&#20851;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.08496</link><description>&lt;p&gt;
&#25968;&#25454;&#21040;&#25991;&#26412;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#30740;&#31350;&#30340;&#31995;&#32479;&#24615;&#22238;&#39038;
&lt;/p&gt;
&lt;p&gt;
A Systematic Review of Data-to-Text NLG
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08496
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#31995;&#32479;&#24615;&#22238;&#39038;&#20840;&#38754;&#20998;&#26512;&#20102;&#25968;&#25454;&#21040;&#25991;&#26412;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#30740;&#31350;&#30340;&#29616;&#29366;&#65292;&#25552;&#20986;&#26410;&#26469;&#26041;&#21521;&#65292;&#24182;&#35299;&#20915;&#20102;&#30456;&#20851;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#31995;&#32479;&#24615;&#22238;&#39038;&#26088;&#22312;&#20840;&#38754;&#20998;&#26512;&#25968;&#25454;&#21040;&#25991;&#26412;&#29983;&#25104;&#30740;&#31350;&#30340;&#29616;&#29366;&#65292;&#37325;&#28857;&#26159;&#30830;&#23450;&#30740;&#31350;&#31354;&#30333;&#65292;&#25552;&#20379;&#26410;&#26469;&#26041;&#21521;&#65292;&#24182;&#35299;&#20915;&#22238;&#39038;&#20013;&#21457;&#29616;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#23545;&#25991;&#29486;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#26816;&#26597;&#65292;&#21253;&#25324;&#26041;&#27861;&#12289;&#25968;&#25454;&#38598;&#12289;&#35780;&#20272;&#25351;&#26631;&#12289;&#24212;&#29992;&#12289;&#22810;&#35821;&#35328;&#24615;&#21644;&#24187;&#35273;&#32531;&#35299;&#25514;&#26045;&#12290;&#25105;&#20204;&#30340;&#22238;&#39038;&#20026;&#36825;&#20010;&#24555;&#36895;&#21457;&#23637;&#30340;&#39046;&#22495;&#30340;&#26410;&#26469;&#30740;&#31350;&#25552;&#20379;&#20102;&#36335;&#32447;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
This systematic review aims to provide a comprehensive analysis of the state of data-to-text generation research, focusing on identifying research gaps, offering future directions, and addressing challenges found during the review. We thoroughly examined the literature, including approaches, datasets, evaluation metrics, applications, multilingualism, and hallucination mitigation measures. Our review provides a roadmap for future research in this rapidly evolving field.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#36890;&#20449;&#22797;&#26434;&#24615;&#35777;&#26126;&#20102;Transformer&#23618;&#22312;&#22788;&#29702;&#20989;&#25968;&#32452;&#21512;&#20219;&#21153;&#26102;&#30340;&#23616;&#38480;&#24615;&#65292;&#25351;&#20986;&#23545;&#20110;&#22823;&#22411;&#23450;&#20041;&#22495;&#21644;&#26576;&#20123;&#25968;&#23398;&#20219;&#21153;&#65292;Transformers&#21487;&#33021;&#26080;&#27861;&#35299;&#20915;&#12290;</title><link>https://arxiv.org/abs/2402.08164</link><description>&lt;p&gt;
&#20851;&#20110;Transformer&#26550;&#26500;&#30340;&#38480;&#21046;
&lt;/p&gt;
&lt;p&gt;
On Limitations of the Transformer Architecture
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08164
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#36890;&#20449;&#22797;&#26434;&#24615;&#35777;&#26126;&#20102;Transformer&#23618;&#22312;&#22788;&#29702;&#20989;&#25968;&#32452;&#21512;&#20219;&#21153;&#26102;&#30340;&#23616;&#38480;&#24615;&#65292;&#25351;&#20986;&#23545;&#20110;&#22823;&#22411;&#23450;&#20041;&#22495;&#21644;&#26576;&#20123;&#25968;&#23398;&#20219;&#21153;&#65292;Transformers&#21487;&#33021;&#26080;&#27861;&#35299;&#20915;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#24187;&#35273;&#30340;&#26681;&#26412;&#21407;&#22240;&#26159;&#20160;&#20040;&#65311;&#25105;&#20204;&#20351;&#29992;&#36890;&#20449;&#22797;&#26434;&#24615;&#26469;&#35777;&#26126;&#65292;&#22914;&#26524;&#20989;&#25968;&#30340;&#23450;&#20041;&#22495;&#36275;&#22815;&#22823;&#65292;Transformer&#23618;&#26080;&#27861;&#32452;&#21512;&#20989;&#25968;&#65288;&#20363;&#22914;&#65292;&#22312;&#23478;&#35889;&#20013;&#26597;&#25214;&#19968;&#20010;&#20154;&#30340;&#31062;&#29238;&#65289;&#65307;&#25105;&#20204;&#36890;&#36807;&#31034;&#20363;&#26174;&#31034;&#65292;&#24403;&#23450;&#20041;&#22495;&#30456;&#24403;&#23567;&#30340;&#26102;&#20505;&#65292;&#36825;&#31181;&#33021;&#21147;&#30340;&#32570;&#20047;&#24050;&#32463;&#22312;&#32463;&#39564;&#19978;&#23384;&#22312;&#12290;&#25105;&#20204;&#36824;&#25351;&#20986;&#65292;&#35768;&#22810;&#22312;&#25152;&#35859;&#30340;&#32452;&#21512;&#20219;&#21153;&#20013;&#30340;&#25968;&#23398;&#20219;&#21153;&#65292;&#35748;&#20026;&#23427;&#20204;&#23545;LLMs&#26469;&#35828;&#24456;&#38590;&#35299;&#20915;&#65292;&#23545;&#20110;&#36275;&#22815;&#22823;&#30340;&#23454;&#20363;&#26469;&#35828;&#65292;&#19988;&#20551;&#35774;&#35745;&#31639;&#22797;&#26434;&#24615;&#39046;&#22495;&#30340;&#26576;&#20123;&#34987;&#24191;&#27867;&#25509;&#21463;&#30340;&#29468;&#24819;&#26159;&#27491;&#30830;&#30340;&#65292;Transformers&#20063;&#19981;&#22826;&#21487;&#33021;&#35299;&#20915;&#12290;
&lt;/p&gt;
&lt;p&gt;
What are the root causes of hallucinations in large language models (LLMs)? We use Communication Complexity to prove that the Transformer layer is incapable of composing functions (e.g., identify a grandparent of a person in a genealogy) if the domains of the functions are large enough; we show through examples that this inability is already empirically present when the domains are quite small. We also point out that several mathematical tasks that are at the core of the so-called compositional tasks thought to be hard for LLMs are unlikely to be solvable by Transformers, for large enough instances and assuming that certain well accepted conjectures in the field of Computational Complexity are true.
&lt;/p&gt;</description></item><item><title>TriAug&#26159;&#19968;&#20010;&#29992;&#20110;&#20083;&#33146;&#36229;&#22768;&#22270;&#20687;&#30340;&#24322;&#24120;&#26679;&#26412;&#26816;&#27979;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#19977;&#20803;&#29366;&#24577;&#22686;&#24378;&#21644;&#24179;&#34913;&#30340;&#29699;&#24418;&#25439;&#22833;&#26469;&#25552;&#39640;&#31034;&#36394;&#20998;&#31867;&#30340;&#20934;&#30830;&#24615;&#21644;&#24322;&#24120;&#26679;&#26412;&#26816;&#27979;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.07452</link><description>&lt;p&gt;
TriAug&#65306;&#29992;&#20110;&#36229;&#22768;&#20083;&#33146;&#30149;&#21464;&#19981;&#24179;&#34913;&#20998;&#31867;&#30340;&#24322;&#24120;&#26679;&#26412;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
TriAug: Out-of-Distribution Detection for Robust Classification of Imbalanced Breast Lesion in Ultrasound
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07452
&lt;/p&gt;
&lt;p&gt;
TriAug&#26159;&#19968;&#20010;&#29992;&#20110;&#20083;&#33146;&#36229;&#22768;&#22270;&#20687;&#30340;&#24322;&#24120;&#26679;&#26412;&#26816;&#27979;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#19977;&#20803;&#29366;&#24577;&#22686;&#24378;&#21644;&#24179;&#34913;&#30340;&#29699;&#24418;&#25439;&#22833;&#26469;&#25552;&#39640;&#31034;&#36394;&#20998;&#31867;&#30340;&#20934;&#30830;&#24615;&#21644;&#24322;&#24120;&#26679;&#26412;&#26816;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19981;&#21516;&#30340;&#30142;&#30149;&#65292;&#22914;&#20083;&#33146;&#30149;&#21464;&#30340;&#32452;&#32455;&#20122;&#22411;&#65292;&#20855;&#26377;&#20005;&#37325;&#19981;&#21516;&#30340;&#21457;&#30149;&#29575;&#12290;&#21363;&#20351;&#36890;&#36807;&#22823;&#37327;&#30340;&#31034;&#36394;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#27169;&#22411;&#22312;&#20020;&#24202;&#23454;&#38469;&#20013;&#36890;&#24120;&#36935;&#21040;&#23646;&#20110;&#26410;&#35265;&#31867;&#21035;&#30340;&#24322;&#24120;&#26679;&#26412;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#22522;&#20110;&#20083;&#33146;&#36229;&#22768;&#22270;&#20687;&#30340;&#38271;&#23614;&#24322;&#24120;&#26679;&#26412;&#26816;&#27979;&#20219;&#21153;&#65292;&#24182;&#37197;&#22791;&#20102;&#19968;&#31181;&#19977;&#20803;&#29366;&#24577;&#22686;&#24378;&#65288;TriAug&#65289;&#65292;&#23427;&#21487;&#20197;&#25552;&#39640;&#31034;&#36394;&#20998;&#31867;&#30340;&#20934;&#30830;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#33391;&#22909;&#30340;&#24322;&#24120;&#26679;&#26412;&#26816;&#27979;&#24615;&#33021;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#24179;&#34913;&#30340;&#29699;&#24418;&#25439;&#22833;&#26469;&#22788;&#29702;&#31867;&#19981;&#24179;&#34913;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Different diseases, such as histological subtypes of breast lesions, have severely varying incidence rates. Even trained with substantial amount of in-distribution (ID) data, models often encounter out-of-distribution (OOD) samples belonging to unseen classes in clinical reality. To address this, we propose a novel framework built upon a long-tailed OOD detection task for breast ultrasound images. It is equipped with a triplet state augmentation (TriAug) which improves ID classification accuracy while maintaining a promising OOD detection performance. Meanwhile, we designed a balanced sphere loss to handle the class imbalanced problem.
&lt;/p&gt;</description></item><item><title>HarmBench&#26159;&#19968;&#20010;&#20026;&#33258;&#21160;&#32418;&#38431;&#21644;&#24378;&#22823;&#25298;&#32477;&#35774;&#35745;&#30340;&#26631;&#20934;&#21270;&#35780;&#20272;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;18&#31181;&#32418;&#38431;&#26041;&#27861;&#21644;33&#20010;&#30446;&#26631;LLM&#21644;&#38450;&#24481;&#30340;&#27604;&#36739;&#65292;&#24471;&#20986;&#20102;&#26032;&#30340;&#35265;&#35299;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#23545;&#25239;&#35757;&#32451;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;LLM&#22312;&#21508;&#31181;&#25915;&#20987;&#19979;&#30340;&#31283;&#20581;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.04249</link><description>&lt;p&gt;
HarmBench&#65306;&#29992;&#20110;&#33258;&#21160;&#32418;&#38431;&#21644;&#24378;&#22823;&#25298;&#32477;&#30340;&#26631;&#20934;&#21270;&#35780;&#20272;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
HarmBench: A Standardized Evaluation Framework for Automated Red Teaming and Robust Refusal
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04249
&lt;/p&gt;
&lt;p&gt;
HarmBench&#26159;&#19968;&#20010;&#20026;&#33258;&#21160;&#32418;&#38431;&#21644;&#24378;&#22823;&#25298;&#32477;&#35774;&#35745;&#30340;&#26631;&#20934;&#21270;&#35780;&#20272;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;18&#31181;&#32418;&#38431;&#26041;&#27861;&#21644;33&#20010;&#30446;&#26631;LLM&#21644;&#38450;&#24481;&#30340;&#27604;&#36739;&#65292;&#24471;&#20986;&#20102;&#26032;&#30340;&#35265;&#35299;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#23545;&#25239;&#35757;&#32451;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;LLM&#22312;&#21508;&#31181;&#25915;&#20987;&#19979;&#30340;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#32418;&#38431;&#20855;&#26377;&#21457;&#29616;&#21644;&#20943;&#36731;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24694;&#24847;&#20351;&#29992;&#30340;&#39118;&#38505;&#30340;&#24040;&#22823;&#28508;&#21147;&#65292;&#28982;&#32780;&#35813;&#39046;&#22495;&#32570;&#20047;&#19968;&#20010;&#26631;&#20934;&#21270;&#30340;&#35780;&#20272;&#26694;&#26550;&#26469;&#20005;&#26684;&#35780;&#20272;&#26032;&#26041;&#27861;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;HarmBench&#65292;&#19968;&#20010;&#29992;&#20110;&#33258;&#21160;&#32418;&#38431;&#30340;&#26631;&#20934;&#21270;&#35780;&#20272;&#26694;&#26550;&#12290;&#25105;&#20204;&#22312;&#32418;&#38431;&#35780;&#20272;&#20013;&#30830;&#23450;&#20102;&#20960;&#20010;&#20197;&#21069;&#26410;&#32771;&#34385;&#30340;&#26377;&#21560;&#24341;&#21147;&#30340;&#29305;&#24615;&#65292;&#24182;&#31995;&#32479;&#22320;&#35774;&#35745;&#20102;HarmBench&#20197;&#28385;&#36275;&#36825;&#20123;&#26631;&#20934;&#12290;&#20351;&#29992;HarmBench&#65292;&#25105;&#20204;&#23545;18&#31181;&#32418;&#38431;&#26041;&#27861;&#21644;33&#20010;&#30446;&#26631;LLM&#21644;&#38450;&#24481;&#36827;&#34892;&#20102;&#22823;&#35268;&#27169;&#27604;&#36739;&#65292;&#24471;&#21040;&#20102;&#26032;&#30340;&#35265;&#35299;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#23545;&#25239;&#35757;&#32451;&#26041;&#27861;&#65292;&#26174;&#33879;&#22686;&#24378;&#20102;LLM&#22312;&#21508;&#31181;&#25915;&#20987;&#19979;&#30340;&#31283;&#20581;&#24615;&#65292;&#23637;&#31034;&#20102;HarmBench&#22914;&#20309;&#20419;&#36827;&#25915;&#20987;&#21644;&#38450;&#24481;&#30340;&#20849;&#21516;&#24320;&#21457;&#12290;&#25105;&#20204;&#22312;https://github.com/centerforaisafety/HarmBench&#19978;&#24320;&#28304;&#20102;HarmBench&#12290;
&lt;/p&gt;
&lt;p&gt;
Automated red teaming holds substantial promise for uncovering and mitigating the risks associated with the malicious use of large language models (LLMs), yet the field lacks a standardized evaluation framework to rigorously assess new methods. To address this issue, we introduce HarmBench, a standardized evaluation framework for automated red teaming. We identify several desirable properties previously unaccounted for in red teaming evaluations and systematically design HarmBench to meet these criteria. Using HarmBench, we conduct a large-scale comparison of 18 red teaming methods and 33 target LLMs and defenses, yielding novel insights. We also introduce a highly efficient adversarial training method that greatly enhances LLM robustness across a wide range of attacks, demonstrating how HarmBench enables codevelopment of attacks and defenses. We open source HarmBench at https://github.com/centerforaisafety/HarmBench.
&lt;/p&gt;</description></item><item><title>ANLS*&#26159;&#19968;&#31181;&#29992;&#20110;&#29983;&#25104;&#22411;&#27169;&#22411;&#30340;&#26032;&#24230;&#37327;&#26041;&#27861;&#65292;&#38024;&#23545;&#21508;&#31181;&#20219;&#21153;&#21253;&#25324;&#20449;&#24687;&#25552;&#21462;&#21644;&#20998;&#31867;&#20219;&#21153;&#36827;&#34892;&#35780;&#20272;&#12290;&#23427;&#25193;&#23637;&#20102;&#29616;&#26377;&#30340;ANLS&#24230;&#37327;&#26041;&#27861;&#65292;&#21487;&#20197;&#20316;&#20026;&#26367;&#20195;&#26041;&#26696;&#20351;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.03848</link><description>&lt;p&gt;
ANLS* -- &#19968;&#31181;&#36866;&#29992;&#20110;&#29983;&#25104;&#22411;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#36890;&#29992;&#25991;&#26723;&#22788;&#29702;&#24230;&#37327;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
ANLS* -- A Universal Document Processing Metric for Generative Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03848
&lt;/p&gt;
&lt;p&gt;
ANLS*&#26159;&#19968;&#31181;&#29992;&#20110;&#29983;&#25104;&#22411;&#27169;&#22411;&#30340;&#26032;&#24230;&#37327;&#26041;&#27861;&#65292;&#38024;&#23545;&#21508;&#31181;&#20219;&#21153;&#21253;&#25324;&#20449;&#24687;&#25552;&#21462;&#21644;&#20998;&#31867;&#20219;&#21153;&#36827;&#34892;&#35780;&#20272;&#12290;&#23427;&#25193;&#23637;&#20102;&#29616;&#26377;&#30340;ANLS&#24230;&#37327;&#26041;&#27861;&#65292;&#21487;&#20197;&#20316;&#20026;&#26367;&#20195;&#26041;&#26696;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#19978;&#65292;&#22312;&#25991;&#26723;&#20998;&#31867;&#21644;&#20449;&#24687;&#25552;&#21462;&#31561;&#20219;&#21153;&#20013;&#65292;&#21306;&#20998;&#27169;&#22411;&#19968;&#30452;&#26159;&#20027;&#35201;&#36873;&#25321;&#12290;&#36825;&#20123;&#27169;&#22411;&#20570;&#20986;&#30340;&#39044;&#27979;&#21487;&#20197;&#20998;&#20026;&#26377;&#38480;&#25968;&#37327;&#30340;&#39044;&#23450;&#20041;&#31867;&#21035;&#65292;&#20415;&#20110;&#36827;&#34892;&#20108;&#20803;&#30495;&#20551;&#35780;&#20272;&#65292;&#24182;&#33021;&#30452;&#25509;&#35745;&#31639;F1&#20998;&#25968;&#31561;&#25351;&#26631;&#12290;&#28982;&#32780;&#65292;&#29983;&#25104;&#22411;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;GLLMs&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#20419;&#20351;&#39046;&#22495;&#21457;&#29983;&#20102;&#36716;&#21464;&#65292;&#22240;&#20026;&#23427;&#20204;&#20855;&#22791;&#20102;&#24378;&#22823;&#30340;&#38646;-shot&#33021;&#21147;&#65292;&#28040;&#38500;&#20102;&#19979;&#28216;&#25968;&#25454;&#38598;&#21644;&#35745;&#31639;&#26114;&#36149;&#30340;&#24494;&#35843;&#30340;&#38656;&#27714;&#12290;&#28982;&#32780;&#65292;&#35780;&#20272;GLLMs&#23384;&#22312;&#25361;&#25112;&#65292;&#22240;&#20026;&#23545;&#20110;GLLMs&#30340;&#39044;&#27979;&#65292;&#19981;&#33021;&#24212;&#29992;&#20110;&#21306;&#20998;&#27169;&#22411;&#25152;&#20351;&#29992;&#30340;&#20108;&#20803;&#30495;&#20551;&#35780;&#20272;&#26041;&#27861;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#29992;&#20110;&#29983;&#25104;&#22411;&#27169;&#22411;&#30340;&#26032;&#24230;&#37327;&#26041;&#27861;&#65292;&#31216;&#20026;ANLS*&#65292;&#29992;&#20110;&#35780;&#20272;&#21508;&#31181;&#20219;&#21153;&#65292;&#21253;&#25324;&#20449;&#24687;&#25552;&#21462;&#21644;&#20998;&#31867;&#20219;&#21153;&#12290;ANLS*&#24230;&#37327;&#26041;&#27861;&#25193;&#23637;&#20102;&#29616;&#26377;ANLS&#24230;&#37327;&#26041;&#27861;&#65292;&#21487;&#20316;&#20026;&#19968;&#31181;&#21363;&#25554;&#21363;&#29992;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traditionally, discriminative models have been the predominant choice for tasks like document classification and information extraction. These models make predictions that fall into a limited number of predefined classes, facilitating a binary true or false evaluation and enabling the direct calculation of metrics such as the F1 score. However, recent advancements in generative large language models (GLLMs) have prompted a shift in the field due to their enhanced zero-shot capabilities, which eliminate the need for a downstream dataset and computationally expensive fine-tuning. However, evaluating GLLMs presents a challenge as the binary true or false evaluation used for discriminative models is not applicable to the predictions made by GLLMs. This paper introduces a new metric for generative models called ANLS* for evaluating a wide variety of tasks, including information extraction and classification tasks. The ANLS* metric extends existing ANLS metrics as a drop-in-replacement and i
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30740;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#33258;&#30001;&#25991;&#26412;&#20013;&#35782;&#21035;&#20316;&#20026;&#20195;&#29702;&#30340;&#27169;&#22411;&#21644;&#20923;&#32467;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#23884;&#20837;&#30340;&#23567;&#22411;&#32447;&#24615;&#25506;&#27979;&#22120;&#21487;&#20197;&#20934;&#30830;&#39044;&#27979;&#26356;&#22823;&#27169;&#22411;&#20196;&#29260;&#32423;&#21035;&#19978;&#30340;&#33258;&#20449;&#24230;&#65292;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#26041;&#27861;&#22312;&#30456;&#21516;&#20219;&#21153;&#19978;&#36798;&#21040;&#20102;&#38750;&#24179;&#20961;&#30340;&#20934;&#30830;&#24230;&#65292;&#36825;&#35777;&#26126;&#20102;&#35821;&#35328;&#27169;&#22411;&#20013;&#23384;&#22312;&#19981;&#21516;&#31867;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#34920;&#31034;&#12290;</title><link>https://arxiv.org/abs/2402.03563</link><description>&lt;p&gt;
&#29992;&#35821;&#35328;&#27169;&#22411;&#21306;&#20998;&#21487;&#30693;&#19982;&#19981;&#21487;&#30693;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Distinguishing the Knowable from the Unknowable with Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03563
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30740;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#33258;&#30001;&#25991;&#26412;&#20013;&#35782;&#21035;&#20316;&#20026;&#20195;&#29702;&#30340;&#27169;&#22411;&#21644;&#20923;&#32467;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#23884;&#20837;&#30340;&#23567;&#22411;&#32447;&#24615;&#25506;&#27979;&#22120;&#21487;&#20197;&#20934;&#30830;&#39044;&#27979;&#26356;&#22823;&#27169;&#22411;&#20196;&#29260;&#32423;&#21035;&#19978;&#30340;&#33258;&#20449;&#24230;&#65292;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#26041;&#27861;&#22312;&#30456;&#21516;&#20219;&#21153;&#19978;&#36798;&#21040;&#20102;&#38750;&#24179;&#20961;&#30340;&#20934;&#30830;&#24230;&#65292;&#36825;&#35777;&#26126;&#20102;&#35821;&#35328;&#27169;&#22411;&#20013;&#23384;&#22312;&#19981;&#21516;&#31867;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#30340;&#33258;&#30001;&#25991;&#26412;&#36755;&#20986;&#20013;&#65292;&#26159;&#21542;&#21487;&#20197;&#37492;&#21035;&#20986;&#35748;&#30693;&#19981;&#30830;&#23450;&#24615;&#65288;&#21453;&#26144;&#32570;&#20047;&#30693;&#35782;&#30340;&#19981;&#30830;&#23450;&#24615;&#65289;&#21644;&#20598;&#28982;&#19981;&#30830;&#23450;&#24615;&#65288;&#21453;&#26144;&#22522;&#30784;&#20998;&#24067;&#20013;&#30340;&#29109;&#65289;&#12290;&#22312;&#27809;&#26377;&#30495;&#23454;&#27010;&#29575;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#19968;&#20010;&#35774;&#32622;&#65292;&#22312;&#36825;&#20010;&#35774;&#32622;&#20013;&#65292;&#20026;&#20102;&#65288;&#36817;&#20284;&#22320;&#65289;&#20998;&#35299;&#32473;&#23450;LLM&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#19968;&#20010;&#26126;&#26174;&#26356;&#22823;&#30340;&#27169;&#22411;&#20805;&#24403;&#22320;&#38754;&#30495;&#30456;&#30340;&#20195;&#29702;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#22522;&#20110;&#20923;&#32467;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#23884;&#20837;&#30340;&#23567;&#22411;&#32447;&#24615;&#25506;&#27979;&#22120;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#22312;&#20196;&#29260;&#32423;&#21035;&#19978;&#26356;&#22823;&#27169;&#22411;&#23558;&#26356;&#33258;&#20449;&#30340;&#24773;&#20917;&#65292;&#24182;&#19988;&#22312;&#19968;&#20010;&#25991;&#26412;&#39046;&#22495;&#19978;&#35757;&#32451;&#30340;&#25506;&#27979;&#22120;&#21487;&#20197;&#27867;&#21270;&#21040;&#20854;&#20182;&#39046;&#22495;&#12290;&#36827;&#19968;&#27493;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23436;&#20840;&#26080;&#30417;&#30563;&#30340;&#26041;&#27861;&#65292;&#22312;&#30456;&#21516;&#20219;&#21153;&#19978;&#36798;&#21040;&#20102;&#38750;&#24179;&#20961;&#30340;&#20934;&#30830;&#24230;&#12290;&#32508;&#21512;&#32771;&#34385;&#36825;&#20123;&#32467;&#26524;&#65292;&#25105;&#20204;&#35299;&#37322;&#36825;&#20123;&#32467;&#26524;&#20316;&#20026;LLMs&#20869;&#37096;&#33258;&#28982;&#22320;&#21253;&#21547;&#20102;&#19981;&#21516;&#31867;&#22411;&#19981;&#30830;&#23450;&#24615;&#30340;&#34920;&#31034;&#65292;&#36825;&#21487;&#33021;&#26377;&#21161;&#20110;&#21046;&#23450;&#26356;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the feasibility of identifying epistemic uncertainty (reflecting a lack of knowledge), as opposed to aleatoric uncertainty (reflecting entropy in the underlying distribution), in the outputs of large language models (LLMs) over free-form text. In the absence of ground-truth probabilities, we explore a setting where, in order to (approximately) disentangle a given LLM's uncertainty, a significantly larger model stands in as a proxy for the ground truth. We show that small linear probes trained on the embeddings of frozen, pretrained models accurately predict when larger models will be more confident at the token level and that probes trained on one text domain generalize to others. Going further, we propose a fully unsupervised method that achieves non-trivial accuracy on the same task. Taken together, we interpret these results as evidence that LLMs naturally contain internal representations of different types of uncertainty that could potentially be leveraged to devise more i
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#20998;&#23376;&#37197;&#32622;&#36716;&#25442;&#22120;&#29983;&#25104;&#39640;&#31934;&#24230;&#21147;&#22330;&#65292;&#20197;&#35299;&#20915;&#35745;&#31639;&#36895;&#24230;&#38480;&#21046;&#65292;&#20174;&#32780;&#22312;&#20998;&#23376;&#21160;&#21147;&#23398;&#27169;&#25311;&#20013;&#30740;&#31350;&#21270;&#23398;&#21453;&#24212;&#26426;&#29702;&#12290;</title><link>https://arxiv.org/abs/2401.00499</link><description>&lt;p&gt;
&#20351;&#29992;&#20998;&#23376;&#37197;&#32622;&#36716;&#25442;&#22120;&#29983;&#25104;&#29992;&#20110;&#20998;&#23376;&#21160;&#21147;&#23398;&#27169;&#25311;&#30340;&#39640;&#31934;&#24230;&#21147;&#22330;&#65292;&#30740;&#31350;&#21270;&#23398;&#21453;&#24212;&#26426;&#29702;
&lt;/p&gt;
&lt;p&gt;
Generating High-Precision Force Fields for Molecular Dynamics Simulations to Study Chemical Reaction Mechanisms using Molecular Configuration Transformer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.00499
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#20998;&#23376;&#37197;&#32622;&#36716;&#25442;&#22120;&#29983;&#25104;&#39640;&#31934;&#24230;&#21147;&#22330;&#65292;&#20197;&#35299;&#20915;&#35745;&#31639;&#36895;&#24230;&#38480;&#21046;&#65292;&#20174;&#32780;&#22312;&#20998;&#23376;&#21160;&#21147;&#23398;&#27169;&#25311;&#20013;&#30740;&#31350;&#21270;&#23398;&#21453;&#24212;&#26426;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26377;&#26426;&#21270;&#23398;&#20013;&#65292;&#21270;&#23398;&#21453;&#24212;&#26426;&#29702;&#30340;&#29702;&#35770;&#30740;&#31350;&#33267;&#20851;&#37325;&#35201;&#12290;&#20256;&#32479;&#19978;&#65292;&#20351;&#29992;&#37327;&#23376;&#21270;&#23398;&#35745;&#31639;&#35745;&#31639;&#25163;&#21160;&#26500;&#24314;&#30340;&#36807;&#28193;&#24577;&#20998;&#23376;&#26500;&#22411;&#26159;&#26368;&#24120;&#29992;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#20010;&#20154;&#32463;&#39564;&#21644;&#21270;&#23398;&#30452;&#35273;&#12290;&#22312;&#25105;&#20204;&#20043;&#21069;&#30340;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20998;&#23376;&#21160;&#21147;&#23398;&#27169;&#25311;&#20013;&#30340;&#22686;&#24378;&#37319;&#26679;&#26469;&#30740;&#31350;&#21270;&#23398;&#21453;&#24212;&#30340;&#30740;&#31350;&#33539;&#24335;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#30452;&#25509;&#27169;&#25311;&#21270;&#23398;&#21453;&#24212;&#30340;&#25972;&#20010;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#35745;&#31639;&#36895;&#24230;&#38480;&#21046;&#20102;&#29992;&#20110;&#27169;&#25311;&#30340;&#39640;&#31934;&#24230;&#21183;&#33021;&#20989;&#25968;&#30340;&#20351;&#29992;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#26696;&#65292;&#20351;&#29992;&#20808;&#21069;&#24320;&#21457;&#30340;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#20998;&#23376;&#27169;&#22411;-&#20998;&#23376;&#37197;&#32622;&#36716;&#25442;&#22120;&#26469;&#35757;&#32451;&#39640;&#31934;&#24230;&#20998;&#23376;&#24314;&#27169;&#21147;&#22330;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.00499v2 Announce Type: replace-cross  Abstract: Theoretical studies on chemical reaction mechanisms have been crucial in organic chemistry. Traditionally, calculating the manually constructed molecular conformations of transition states for chemical reactions using quantum chemical calculations is the most commonly used method. However, this way is heavily dependent on individual experience and chemical intuition. In our previous study, we proposed a research paradigm that uses enhanced sampling in molecular dynamics simulations to study chemical reactions. This approach can directly simulate the entire process of a chemical reaction. However, the computational speed limits the use of high-precision potential energy functions for simulations. To address this issue, we present a scheme for training high-precision force fields for molecular modeling using a previously developed graph-neural-network-based molecular model, molecular configuration transformer. This potential ener
&lt;/p&gt;</description></item><item><title>KnowGPT&#26159;&#19968;&#31181;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#40657;&#30418;&#30693;&#35782;&#27880;&#20837;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#21644;&#22810;&#33218;&#32769;&#34382;&#26426;&#26500;&#24314;&#26368;&#36866;&#21512;&#27599;&#20010;&#38382;&#39064;&#30340;&#25552;&#31034;&#65292;&#22312;&#19977;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#39564;&#35777;&#26126;&#20854;&#26174;&#33879;&#25552;&#21319;&#20102;&#30693;&#35782;&#27880;&#20837;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2312.06185</link><description>&lt;p&gt;
KnowGPT&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#40657;&#30418;&#30693;&#35782;&#27880;&#20837;
&lt;/p&gt;
&lt;p&gt;
KnowGPT: Black-Box Knowledge Injection for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.06185
&lt;/p&gt;
&lt;p&gt;
KnowGPT&#26159;&#19968;&#31181;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#40657;&#30418;&#30693;&#35782;&#27880;&#20837;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#21644;&#22810;&#33218;&#32769;&#34382;&#26426;&#26500;&#24314;&#26368;&#36866;&#21512;&#27599;&#20010;&#38382;&#39064;&#30340;&#25552;&#31034;&#65292;&#22312;&#19977;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#39564;&#35777;&#26126;&#20854;&#26174;&#33879;&#25552;&#21319;&#20102;&#30693;&#35782;&#27880;&#20837;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22914;ChatGPT&#65292;&#25552;&#20379;&#20114;&#21160;&#24335;API&#65292;&#21487;&#20197;&#20197;&#20154;&#31867;&#19987;&#23478;&#27700;&#24179;&#22238;&#31572;&#24120;&#35265;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#24403;&#38754;&#20020;&#38656;&#35201;&#29305;&#23450;&#39046;&#22495;&#25110;&#19987;&#19994;&#39046;&#22495;&#30693;&#35782;&#30340;&#38382;&#39064;&#26102;&#65292;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#20250;&#32473;&#20986;&#19981;&#20934;&#30830;&#25110;&#19981;&#27491;&#30830;&#30340;&#21709;&#24212;&#65292;&#36825;&#20123;&#30693;&#35782;&#24182;&#26410;&#21253;&#21547;&#22312;&#23427;&#20204;&#30340;&#35757;&#32451;&#35821;&#26009;&#24211;&#20013;&#12290;&#27492;&#22806;&#65292;&#35768;&#22810;&#26368;&#20808;&#36827;&#30340;LLMs&#24182;&#38750;&#24320;&#28304;&#65292;&#36825;&#20351;&#24471;&#20165;&#20351;&#29992;&#27169;&#22411;API&#27880;&#20837;&#30693;&#35782;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;KnowGPT&#65292;&#19968;&#31181;&#29992;&#20110;LLMs&#22312;&#38382;&#31572;&#20013;&#30340;&#40657;&#30418;&#30693;&#35782;&#27880;&#20837;&#26694;&#26550;&#12290;KnowGPT&#21033;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20174;&#30693;&#35782;&#22270;&#20013;&#25552;&#21462;&#30456;&#20851;&#30693;&#35782;&#65292;&#24182;&#20351;&#29992;&#22810;&#33218;&#32769;&#34382;&#26426;&#65288;MAB&#65289;&#20026;&#27599;&#20010;&#38382;&#39064;&#26500;&#24314;&#26368;&#21512;&#36866;&#30340;&#25552;&#31034;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#23637;&#31034;&#20102;KnowGPT&#26174;&#33879;&#22686;&#24378;&#20102;&#29616;&#26377;&#26041;&#27861;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;KnowGPT&#24179;&#22343;&#25913;&#36827;&#20102;23%&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.06185v2 Announce Type: replace-cross  Abstract: Generative Large Language Models (LLMs), such as ChatGPT, offer interactive APIs that can answer common questions at a human-expert level. However, these models often give inaccurate or incorrect responses when faced with questions requiring domain-specific or professional-specific knowledge not covered in their training corpus. Furthermore, many state-of-the-art LLMs are not open-source, making it challenging to inject knowledge with model APIs only. In this work, we introduce KnowGPT, a black-box knowledge injection framework for LLMs in question answering. KnowGPT leverages deep reinforcement learning (RL) to extract relevant knowledge from Knowledge Graphs (KGs) and use Multi-Armed Bandit (MAB) to construct the most suitable prompt for each question. Our extensive experiments on three benchmark datasets showcase that KnowGPT significantly enhances the existing methods. Notably, KnowGPT achieves an average improvement of 23.
&lt;/p&gt;</description></item><item><title>&#31070;&#32463;&#35821;&#38899;&#23884;&#20837;&#22312;&#22522;&#20110;&#28145;&#24230;&#29983;&#25104;&#32593;&#32476;&#30340;&#35821;&#38899;&#21512;&#25104;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#21487;&#20197;&#30452;&#25509;&#23558;&#33041;&#20449;&#21495;&#36716;&#21270;&#20026;&#35821;&#38899;&#65292;&#26497;&#22823;&#22320;&#22686;&#24378;&#20102;&#20132;&#27969;&#30340;&#33258;&#28982;&#24615;&#12290;</title><link>https://arxiv.org/abs/2312.05814</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#29983;&#25104;&#32593;&#32476;&#30340;&#35821;&#38899;&#29983;&#25104;&#30340;&#31070;&#32463;&#35821;&#38899;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Neural Speech Embeddings for Speech Synthesis Based on Deep Generative Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.05814
&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#35821;&#38899;&#23884;&#20837;&#22312;&#22522;&#20110;&#28145;&#24230;&#29983;&#25104;&#32593;&#32476;&#30340;&#35821;&#38899;&#21512;&#25104;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#21487;&#20197;&#30452;&#25509;&#23558;&#33041;&#20449;&#21495;&#36716;&#21270;&#20026;&#35821;&#38899;&#65292;&#26497;&#22823;&#22320;&#22686;&#24378;&#20102;&#20132;&#27969;&#30340;&#33258;&#28982;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2312.05814v2 &#20844;&#21578;&#31867;&#22411;: &#26367;&#25442; &#25688;&#35201;: &#33041;&#21040;&#35821;&#38899;&#25216;&#26415;&#20195;&#34920;&#20102;&#36328;&#23398;&#31185;&#24212;&#29992;&#30340;&#34701;&#21512;&#65292;&#28085;&#30422;&#20102;&#20154;&#24037;&#26234;&#33021;&#12289;&#33041;-&#35745;&#31639;&#26426;&#30028;&#38754;&#21644;&#35821;&#38899;&#21512;&#25104;&#39046;&#22495;&#12290;&#22522;&#20110;&#31070;&#32463;&#34920;&#24449;&#23398;&#20064;&#30340;&#24847;&#22270;&#35299;&#30721;&#21644;&#35821;&#38899;&#21512;&#25104;&#30452;&#25509;&#23558;&#31070;&#32463;&#27963;&#21160;&#19982;&#20154;&#31867;&#35821;&#35328;&#20132;&#27969;&#26041;&#24335;&#32852;&#31995;&#36215;&#26469;&#65292;&#36825;&#21487;&#33021;&#26497;&#22823;&#22686;&#24378;&#20132;&#27969;&#30340;&#33258;&#28982;&#24615;&#12290;&#38543;&#30528;&#23545;&#34920;&#24449;&#23398;&#20064;&#30340;&#24403;&#21069;&#21457;&#29616;&#21644;&#35821;&#38899;&#21512;&#25104;&#25216;&#26415;&#30340;&#21457;&#23637;&#65292;&#30452;&#25509;&#23558;&#33041;&#20449;&#21495;&#32763;&#35793;&#25104;&#35821;&#38899;&#26174;&#31034;&#20986;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;&#29305;&#21035;&#26159;&#65292;&#36755;&#20837;&#29305;&#24449;&#21644;&#31070;&#32463;&#35821;&#38899;&#23884;&#20837;&#22312;&#20351;&#29992;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#20174;&#33041;&#20449;&#21495;&#29983;&#25104;&#35821;&#38899;&#26102;&#21457;&#25381;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#24403;&#21069;&#30340;&#33041;&#21040;&#35821;&#38899;&#25216;&#26415;&#65292;&#20197;&#21450;&#20174;&#33041;&#20449;&#21495;&#21512;&#25104;&#35821;&#38899;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.05814v2 Announce Type: replace  Abstract: Brain-to-speech technology represents a fusion of interdisciplinary applications encompassing fields of artificial intelligence, brain-computer interfaces, and speech synthesis. Neural representation learning based intention decoding and speech synthesis directly connects the neural activity to the means of human linguistic communication, which may greatly enhance the naturalness of communication. With the current discoveries on representation learning and the development of the speech synthesis technologies, direct translation of brain signals into speech has shown great promise. Especially, the processed input features and neural speech embeddings which are given to the neural network play a significant role in the overall performance when using deep generative models for speech generation from brain signals. In this paper, we introduce the current brain-to-speech technology with the possibility of speech synthesis from brain signa
&lt;/p&gt;</description></item><item><title>&#20998;&#26512;&#20102;&#22240;&#26524;&#20844;&#24179;&#24615;&#23545;&#26410;&#35266;&#23519;&#21040;&#28151;&#26434;&#30340;&#25935;&#24863;&#24615;&#65292;&#25512;&#23548;&#20986;&#22240;&#26524;&#20844;&#24179;&#24615;&#25351;&#26631;&#30340;&#30028;&#38480;&#65292;&#25552;&#20986;&#31070;&#32463;&#26694;&#26550;&#29992;&#20110;&#23398;&#20064;&#20844;&#24179;&#39044;&#27979;&#65292;&#23637;&#31034;&#20102;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;</title><link>https://arxiv.org/abs/2311.18460</link><description>&lt;p&gt;
&#26410;&#35266;&#23519;&#21040;&#30340;&#28151;&#26434;&#19979;&#30340;&#22240;&#26524;&#20844;&#24179;&#24615;&#65306;&#19968;&#31181;&#31070;&#32463;&#25935;&#24863;&#24615;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Causal Fairness under Unobserved Confounding: A Neural Sensitivity Framework
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.18460
&lt;/p&gt;
&lt;p&gt;
&#20998;&#26512;&#20102;&#22240;&#26524;&#20844;&#24179;&#24615;&#23545;&#26410;&#35266;&#23519;&#21040;&#28151;&#26434;&#30340;&#25935;&#24863;&#24615;&#65292;&#25512;&#23548;&#20986;&#22240;&#26524;&#20844;&#24179;&#24615;&#25351;&#26631;&#30340;&#30028;&#38480;&#65292;&#25552;&#20986;&#31070;&#32463;&#26694;&#26550;&#29992;&#20110;&#23398;&#20064;&#20844;&#24179;&#39044;&#27979;&#65292;&#23637;&#31034;&#20102;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#39044;&#27979;&#20013;&#30340;&#20844;&#24179;&#24615;&#30001;&#20110;&#27861;&#24459;&#12289;&#36947;&#24503;&#21644;&#31038;&#20250;&#21407;&#22240;&#22312;&#23454;&#36341;&#20013;&#34987;&#24191;&#27867;&#35201;&#27714;&#12290;&#29616;&#26377;&#24037;&#20316;&#36890;&#24120;&#38598;&#20013;&#22312;&#27809;&#26377;&#26410;&#35266;&#23519;&#21040;&#30340;&#28151;&#26434;&#30340;&#35774;&#32622;&#19978;&#65292;&#23613;&#31649;&#26410;&#35266;&#23519;&#21040;&#30340;&#28151;&#26434;&#21487;&#33021;&#23548;&#33268;&#20005;&#37325;&#36829;&#21453;&#22240;&#26524;&#20844;&#24179;&#24615;&#65292;&#20174;&#32780;&#20135;&#29983;&#19981;&#20844;&#24179;&#30340;&#39044;&#27979;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#22240;&#26524;&#20844;&#24179;&#24615;&#23545;&#26410;&#35266;&#23519;&#21040;&#30340;&#28151;&#26434;&#30340;&#25935;&#24863;&#24615;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#26377;&#19977;&#20010;&#26041;&#38754;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#19981;&#21516;&#26469;&#28304;&#30340;&#26410;&#35266;&#23519;&#21040;&#28151;&#26434;&#19979;&#22240;&#26524;&#20844;&#24179;&#24615;&#25351;&#26631;&#30340;&#30028;&#38480;&#12290;&#36825;&#20351;&#20174;&#19994;&#32773;&#33021;&#22815;&#26816;&#26597;&#20854;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23545;&#22312;&#20844;&#24179;&#20851;&#38190;&#24212;&#29992;&#20013;&#30340;&#26410;&#35266;&#23519;&#21040;&#30340;&#28151;&#26434;&#30340;&#25935;&#24863;&#24615;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#23398;&#20064;&#20844;&#24179;&#39044;&#27979;&#30340;&#26032;&#22411;&#31070;&#32463;&#26694;&#26550;&#65292;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#25552;&#20379;&#23545;&#22240;&#26524;&#20844;&#24179;&#24615;&#21487;&#33021;&#30001;&#20110;&#26410;&#35266;&#23519;&#21040;&#30340;&#28151;&#26434;&#32780;&#21463;&#21040;&#36829;&#21453;&#30340;&#31243;&#24230;&#30340;&#26368;&#22351;&#24773;&#20917;&#20445;&#35777;&#12290;&#31532;&#19977;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.18460v2 Announce Type: replace-cross  Abstract: Fairness for machine learning predictions is widely required in practice for legal, ethical, and societal reasons. Existing work typically focuses on settings without unobserved confounding, even though unobserved confounding can lead to severe violations of causal fairness and, thus, unfair predictions. In this work, we analyze the sensitivity of causal fairness to unobserved confounding. Our contributions are three-fold. First, we derive bounds for causal fairness metrics under different sources of unobserved confounding. This enables practitioners to examine the sensitivity of their machine learning models to unobserved confounding in fairness-critical applications. Second, we propose a novel neural framework for learning fair predictions, which allows us to offer worst-case guarantees of the extent to which causal fairness can be violated due to unobserved confounding. Third, we demonstrate the effectiveness of our framewor
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;EarnMore&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;RL&#26041;&#27861;&#65292;&#21487;&#20197;&#20801;&#35768;RL&#20195;&#29702;&#19982;&#21487;&#23450;&#21046;&#32929;&#31080;&#27744;&#65288;CSPs&#65289;&#20132;&#20114;&#65292;&#32780;&#19981;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#12290;</title><link>https://arxiv.org/abs/2311.10801</link><description>&lt;p&gt;
&#20351;&#29992;&#21487;&#23631;&#34109;&#32929;&#31080;&#34920;&#31034;&#30340;&#24378;&#21270;&#23398;&#20064;&#22312;&#21487;&#23450;&#21046;&#32929;&#31080;&#27744;&#20013;&#36827;&#34892;&#25237;&#36164;&#32452;&#21512;&#31649;&#29702;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning with Maskable Stock Representation for Portfolio Management in Customizable Stock Pools
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.10801
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;EarnMore&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;RL&#26041;&#27861;&#65292;&#21487;&#20197;&#20801;&#35768;RL&#20195;&#29702;&#19982;&#21487;&#23450;&#21046;&#32929;&#31080;&#27744;&#65288;CSPs&#65289;&#20132;&#20114;&#65292;&#32780;&#19981;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25237;&#36164;&#32452;&#21512;&#31649;&#29702;&#65288;PM&#65289;&#26159;&#19968;&#39033;&#22522;&#26412;&#30340;&#37329;&#34701;&#20132;&#26131;&#20219;&#21153;&#65292;&#25506;&#32034;&#23450;&#26399;&#23558;&#36164;&#37329;&#37325;&#26032;&#37197;&#32622;&#21040;&#19981;&#21516;&#32929;&#31080;&#20013;&#20197;&#36861;&#27714;&#38271;&#26399;&#21033;&#28070;&#12290;&#26368;&#36817;&#65292;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26174;&#31034;&#20986;&#20854;&#28508;&#21147;&#65292;&#36890;&#36807;&#19982;&#37329;&#34701;&#24066;&#22330;&#20114;&#21160;&#26469;&#35757;&#32451;&#20855;&#26377;&#30408;&#21033;&#33021;&#21147;&#30340;PM&#20195;&#29702;&#12290;&#20294;&#26159;&#65292;&#29616;&#26377;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#22266;&#23450;&#32929;&#31080;&#27744;&#19978;&#65292;&#36825;&#19982;&#25237;&#36164;&#32773;&#30340;&#23454;&#38469;&#38656;&#27714;&#19981;&#19968;&#33268;&#12290;&#20026;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;EarnMore&#65292;&#19968;&#31181;&#26032;&#30340;RL&#26041;&#27861;&#65292;&#21487;&#20197;&#20801;&#35768;RL&#20195;&#29702;&#19982;&#21487;&#23450;&#21046;&#32929;&#31080;&#27744;&#65288;CSPs&#65289;&#20132;&#20114;&#65292;&#32780;&#19981;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.10801v3 Announce Type: replace-cross  Abstract: Portfolio management (PM) is a fundamental financial trading task, which explores the optimal periodical reallocation of capitals into different stocks to pursue long-term profits. Reinforcement learning (RL) has recently shown its potential to train profitable agents for PM through interacting with financial markets. However, existing work mostly focuses on fixed stock pools, which is inconsistent with investors' practical demand. Specifically, the target stock pool of different investors varies dramatically due to their discrepancy on market states and individual investors may temporally adjust stocks they desire to trade (e.g., adding one popular stocks), which lead to customizable stock pools (CSPs). Existing RL methods require to retrain RL agents even with a tiny change of the stock pool, which leads to high computational cost and unstable performance. To tackle this challenge, we propose EarnMore, a rEinforcement leARNin
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;KBQA&#26550;&#26500;FuSIC-KBQA&#65292;&#36890;&#36807;&#22810;&#20010;&#28304;&#35757;&#32451;&#30340;&#21484;&#22238;&#22120;&#25191;&#34892;KB&#26816;&#32034;&#65292;&#22312;LLM&#30340;&#37325;&#26032;&#25490;&#24207;&#21518;&#20197;&#27492;&#20316;&#20026;LLM&#23569;&#26679;&#26412;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#36755;&#20837;&#26469;&#29983;&#25104;&#36923;&#36753;&#24418;&#24335;&#65292;&#24182;&#21033;&#29992;&#25191;&#34892;&#24341;&#23548;&#21453;&#39304;&#36827;&#19968;&#27493;&#20248;&#21270;&#12290;</title><link>https://arxiv.org/abs/2311.08894</link><description>&lt;p&gt;
&#23569;&#26679;&#26412;&#36801;&#31227;&#23398;&#20064;&#29992;&#20110;&#30693;&#35782;&#24211;&#38382;&#31572;&#65306;&#34701;&#21512;&#30417;&#30563;&#27169;&#22411;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Few-shot Transfer Learning for Knowledge Base Question Answering: Fusing Supervised Models with In-Context Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.08894
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;KBQA&#26550;&#26500;FuSIC-KBQA&#65292;&#36890;&#36807;&#22810;&#20010;&#28304;&#35757;&#32451;&#30340;&#21484;&#22238;&#22120;&#25191;&#34892;KB&#26816;&#32034;&#65292;&#22312;LLM&#30340;&#37325;&#26032;&#25490;&#24207;&#21518;&#20197;&#27492;&#20316;&#20026;LLM&#23569;&#26679;&#26412;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#36755;&#20837;&#26469;&#29983;&#25104;&#36923;&#36753;&#24418;&#24335;&#65292;&#24182;&#21033;&#29992;&#25191;&#34892;&#24341;&#23548;&#21453;&#39304;&#36827;&#19968;&#27493;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#30693;&#35782;&#24211;&#38382;&#31572;&#65288;KBQA&#65289;&#26550;&#26500;&#38656;&#35201;&#22823;&#37327;&#27880;&#37322;&#25968;&#25454;&#65292;&#36825;&#20351;&#24471;&#23427;&#20204;&#22312;&#37096;&#32626;&#26102;&#25104;&#26412;&#39640;&#19988;&#32791;&#26102;&#12290;&#25105;&#20204;&#25552;&#20986;&#23569;&#26679;&#26412;&#36801;&#31227;&#23398;&#20064;&#29992;&#20110;KBQA&#30340;&#38382;&#39064;&#65292;&#30446;&#26631;&#22495;&#20165;&#25552;&#20379;&#23569;&#37327;&#26631;&#35760;&#31034;&#20363;&#65292;&#20294;&#22312;&#28304;&#22495;&#20013;&#26377;&#22823;&#37327;&#26631;&#35760;&#35757;&#32451;&#25968;&#25454;&#38598;&#21487;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FuSIC-KBQA&#30340;&#26032;&#22411;KBQA&#26550;&#26500;&#65292;&#23427;&#20351;&#29992;&#22810;&#20010;&#32463;&#36807;&#28304;&#22521;&#35757;&#30340;&#21484;&#22238;&#22120;&#25191;&#34892;KB&#26816;&#32034;&#65292;&#20351;&#29992;LLM&#37325;&#26032;&#25490;&#24207;&#65292;&#23558;&#27492;&#20316;&#20026;LLM&#23569;&#26679;&#26412;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#36755;&#20837;&#20197;&#29983;&#25104;&#36923;&#36753;&#24418;&#24335;&#65292;&#36827;&#19968;&#27493;&#20351;&#29992;&#25191;&#34892;&#24341;&#23548;&#21453;&#39304;&#36827;&#34892;&#32454;&#21270;&#12290;&#22312;&#22235;&#23545;&#19981;&#21516;&#22797;&#26434;&#24230;&#30340;&#28304;-&#30446;&#26631;KBQA&#23545;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;FuSIC-KBQA&#26126;&#26174;&#20248;&#20110;&#20026;&#27492;&#35774;&#32622;&#35843;&#25972;&#30340;SoTA KBQA&#27169;&#22411;&#12290;&#22312;&#39046;&#22495;&#20869;&#35774;&#32622;&#30340;&#39069;&#22806;&#23454;&#39564;&#34920;&#26126;&#65292;&#24403;&#35757;&#32451;&#25968;&#25454;&#26377;&#38480;&#26102;&#65292;FuSIC-KBQA&#20063;&#20248;&#20110;SoTA KBQA&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.08894v2 Announce Type: replace-cross  Abstract: Existing Knowledge Base Question Answering (KBQA) architectures are hungry for annotated data, which make them costly and time-consuming to deploy. We introduce the problem of few-shot transfer learning for KBQA, where the target domain offers only a few labeled examples, but a large labeled training dataset is available in a source domain. We propose a novel KBQA architecture called FuSIC-KBQA that performs KB-retrieval using multiple source-trained retrievers, re-ranks using an LLM and uses this as input for LLM few-shot in-context learning to generate logical forms, which are further refined using execution-guided feedback. Experiments over four source-target KBQA pairs of varying complexity show that FuSIC-KBQA significantly outperforms adaptations of SoTA KBQA models for this setting. Additional experiments in the in-domain setting show that FuSIC-KBQA also outperforms SoTA KBQA models when training data is limited.
&lt;/p&gt;</description></item><item><title>"Labor Space"&#26159;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#31934;&#32454;&#35843;&#25972;&#32780;&#25104;&#30340;&#30690;&#37327;&#31354;&#38388;&#23884;&#20837;&#65292;&#25581;&#31034;&#20102;&#21171;&#21160;&#21147;&#24066;&#22330;&#21508;&#31181;&#32452;&#25104;&#37096;&#20998;&#30340;&#22797;&#26434;&#20851;&#31995;&#32467;&#26500;&#65292;&#20419;&#36827;&#20102;&#23545;&#34892;&#19994;&#12289;&#32844;&#19994;&#12289;&#25216;&#33021;&#21644;&#20844;&#21496;&#30340;&#32508;&#21512;&#20998;&#26512;&#12290;</title><link>https://arxiv.org/abs/2311.06310</link><description>&lt;p&gt;
Labor Space: &#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#21171;&#21160;&#21147;&#24066;&#22330;&#36827;&#34892;&#32479;&#19968;&#34920;&#24449;
&lt;/p&gt;
&lt;p&gt;
Labor Space: A Unifying Representation of the Labor Market via Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.06310
&lt;/p&gt;
&lt;p&gt;
"Labor Space"&#26159;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#31934;&#32454;&#35843;&#25972;&#32780;&#25104;&#30340;&#30690;&#37327;&#31354;&#38388;&#23884;&#20837;&#65292;&#25581;&#31034;&#20102;&#21171;&#21160;&#21147;&#24066;&#22330;&#21508;&#31181;&#32452;&#25104;&#37096;&#20998;&#30340;&#22797;&#26434;&#20851;&#31995;&#32467;&#26500;&#65292;&#20419;&#36827;&#20102;&#23545;&#34892;&#19994;&#12289;&#32844;&#19994;&#12289;&#25216;&#33021;&#21644;&#20844;&#21496;&#30340;&#32508;&#21512;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21171;&#21160;&#21147;&#24066;&#22330;&#26159;&#19968;&#20010;&#22797;&#26434;&#30340;&#29983;&#24577;&#31995;&#32479;&#65292;&#21253;&#25324;&#21508;&#31181;&#30456;&#20114;&#20851;&#32852;&#30340;&#23454;&#20307;&#65292;&#22914;&#34892;&#19994;&#12289;&#32844;&#19994;&#12289;&#25216;&#33021;&#21644;&#20844;&#21496;&#12290;&#30001;&#20110;&#32570;&#20047;&#23558;&#36825;&#20123;&#24322;&#36136;&#23454;&#20307;&#31995;&#32479;&#22320;&#26144;&#23556;&#22312;&#19968;&#36215;&#30340;&#26041;&#27861;&#65292;&#27599;&#20010;&#23454;&#20307;&#37117;&#34987;&#23396;&#31435;&#22320;&#20998;&#26512;&#25110;&#20165;&#36890;&#36807;&#25104;&#23545;&#20851;&#31995;&#65292;&#25233;&#21046;&#20102;&#23545;&#25972;&#20010;&#29983;&#24577;&#31995;&#32479;&#30340;&#20840;&#38754;&#29702;&#35299;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;"Labor Space"&#65292;&#36825;&#26159;&#36890;&#36807;&#24212;&#29992;&#20855;&#26377;&#24494;&#35843;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23548;&#20986;&#30340;&#24322;&#36136;&#21171;&#21160;&#21147;&#24066;&#22330;&#23454;&#20307;&#30340;&#30690;&#37327;&#31354;&#38388;&#23884;&#20837;&#12290;Labor Space&#23637;&#31034;&#20102;&#21508;&#31181;&#21171;&#21160;&#21147;&#24066;&#22330;&#25104;&#20998;&#30340;&#22797;&#26434;&#20851;&#31995;&#32467;&#26500;&#65292;&#20419;&#36827;&#20102;&#23545;&#34892;&#19994;&#12289;&#32844;&#19994;&#12289;&#25216;&#33021;&#21644;&#20844;&#21496;&#30340;&#19968;&#33268;&#32508;&#21512;&#20998;&#26512;&#65292;&#21516;&#26102;&#20445;&#30041;&#20102;&#29305;&#23450;&#31867;&#22411;&#30340;&#32858;&#31867;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20854;&#21069;&#25152;&#26410;&#26377;&#30340;&#20998;&#26512;&#33021;&#21147;&#65292;&#21253;&#25324;&#23558;&#24322;&#36136;&#23454;&#20307;&#23450;&#20301;&#22312;&#32463;&#27982;&#36724;&#19978;&#65292;&#22914;&#8220;&#21046;&#36896;&#19994;-&#21307;&#30103;&#20445;&#20581;&#8221;&#12290;&#27492;&#22806;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.06310v3 Announce Type: replace-cross  Abstract: The labor market is a complex ecosystem comprising diverse, interconnected entities, such as industries, occupations, skills, and firms. Due to the lack of a systematic method to map these heterogeneous entities together, each entity has been analyzed in isolation or only through pairwise relationships, inhibiting comprehensive understanding of the whole ecosystem. Here, we introduce $\textit{Labor Space}$, a vector-space embedding of heterogeneous labor market entities, derived through applying a large language model with fine-tuning. Labor Space exposes the complex relational fabric of various labor market constituents, facilitating coherent integrative analysis of industries, occupations, skills, and firms, while retaining type-specific clustering. We demonstrate its unprecedented analytical capacities, including positioning heterogeneous entities on an economic axes, such as `Manufacturing--Healthcare'. Furthermore, by allo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#24247;&#24503;&#30340;&#20041;&#21153;&#35770;&#22312;&#20844;&#24179;&#24230;&#37327;&#20013;&#30340;&#24212;&#29992;&#65292;&#20027;&#24352;&#20844;&#24179;&#21407;&#21017;&#24212;&#19982;&#24247;&#24503;&#20041;&#21153;&#35770;&#26694;&#26550;&#30456;&#22865;&#21512;&#65292;&#20197;&#23454;&#29616;&#26356;&#20855;&#36947;&#24503;&#31435;&#36275;&#30340;&#20154;&#24037;&#26234;&#33021;&#21644;&#26356;&#24179;&#34913;&#32467;&#26524;&#19982;&#31243;&#24207;&#30340;&#20844;&#24179;&#19982;&#27491;&#20041;&#12290;</title><link>https://arxiv.org/abs/2311.05227</link><description>&lt;p&gt;
&#24247;&#24503;&#30340;&#20041;&#21153;&#35770;&#19982;&#20154;&#24037;&#26234;&#33021;&#23545;&#40784;&#65306;&#36808;&#21521;&#36947;&#24503;&#31435;&#36275;&#30340;&#20844;&#24179;&#24230;&#37327;
&lt;/p&gt;
&lt;p&gt;
Kantian Deontology Meets AI Alignment: Towards Morally Grounded Fairness Metrics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.05227
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#24247;&#24503;&#30340;&#20041;&#21153;&#35770;&#22312;&#20844;&#24179;&#24230;&#37327;&#20013;&#30340;&#24212;&#29992;&#65292;&#20027;&#24352;&#20844;&#24179;&#21407;&#21017;&#24212;&#19982;&#24247;&#24503;&#20041;&#21153;&#35770;&#26694;&#26550;&#30456;&#22865;&#21512;&#65292;&#20197;&#23454;&#29616;&#26356;&#20855;&#36947;&#24503;&#31435;&#36275;&#30340;&#20154;&#24037;&#26234;&#33021;&#21644;&#26356;&#24179;&#34913;&#32467;&#26524;&#19982;&#31243;&#24207;&#30340;&#20844;&#24179;&#19982;&#27491;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24247;&#24503;&#30340;&#20041;&#21153;&#35770;&#21363;&#20234;&#26364;&#32445;&#23572;&#183;&#24247;&#24503;&#29702;&#35299;&#30340;&#37027;&#31181;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#24378;&#35843;&#32844;&#36131;&#21644;&#21407;&#21017;&#37325;&#35201;&#24615;&#32780;&#38750;&#34892;&#21160;&#21518;&#26524;&#30340;&#36947;&#24503;&#26694;&#26550;&#12290;&#26412;&#25991;&#35748;&#20026;&#65292;&#23613;&#31649;&#20041;&#21153;&#35770;&#39047;&#20855;&#31361;&#20986;&#65292;&#20294;&#22312;&#20844;&#24179;&#24230;&#37327;&#39046;&#22495;&#21364;&#40092;&#26377;&#30740;&#31350;&#65292;&#23427;&#25506;&#35752;&#20102;&#24247;&#24503;&#20041;&#21153;&#35770;&#26694;&#26550;&#19982;&#20844;&#24179;&#24230;&#37327;&#30340;&#20860;&#23481;&#24615;&#65292;&#22312;AI&#23545;&#40784;&#39046;&#22495;&#20013;&#12290;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;&#24247;&#24503;&#23545;&#21151;&#21033;&#20027;&#20041;&#30340;&#25209;&#21028;&#65292;&#32780;&#21151;&#21033;&#20027;&#20041;&#26159;AI&#20844;&#24179;&#24230;&#37327;&#30340;&#20027;&#35201;&#26041;&#27861;&#65292;&#20027;&#24352;&#20844;&#24179;&#21407;&#21017;&#24212;&#19982;&#24247;&#24503;&#20041;&#21153;&#35770;&#26694;&#26550;&#30456;&#22865;&#21512;&#12290;&#36890;&#36807;&#23558;&#24247;&#24503;&#20262;&#29702;&#34701;&#20837;AI&#23545;&#40784;&#65292;&#25105;&#20204;&#19981;&#20165;&#24341;&#20837;&#20102;&#19968;&#20010;&#34987;&#24191;&#27867;&#25509;&#21463;&#30340;&#37325;&#35201;&#36947;&#24503;&#29702;&#35770;&#65292;&#32780;&#19988;&#21162;&#21147;&#23454;&#29616;&#19968;&#20010;&#26356;&#20855;&#36947;&#24503;&#31435;&#36275;&#30340;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#65292;&#26356;&#22909;&#22320;&#24179;&#34913;&#32467;&#26524;&#21644;&#31243;&#24207;&#65292;&#36861;&#27714;&#20844;&#24179;&#19982;&#27491;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.05227v2 Announce Type: replace  Abstract: Deontological ethics, specifically understood through Immanuel Kant, provides a moral framework that emphasizes the importance of duties and principles, rather than the consequences of action. Understanding that despite the prominence of deontology, it is currently an overlooked approach in fairness metrics, this paper explores the compatibility of a Kantian deontological framework in fairness metrics, part of the AI alignment field. We revisit Kant's critique of utilitarianism, which is the primary approach in AI fairness metrics and argue that fairness principles should align with the Kantian deontological framework. By integrating Kantian ethics into AI alignment, we not only bring in a widely-accepted prominent moral theory but also strive for a more morally grounded AI landscape that better balances outcomes and procedures in pursuit of fairness and justice.
&lt;/p&gt;</description></item><item><title>&#32454;&#35843;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#20998;&#35299;&#29983;&#25104;&#22120;&#65292;&#20351;&#29992;&#31574;&#30053;&#26799;&#24230;&#20248;&#21270;&#26469;&#21327;&#35843;&#26356;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#22797;&#26434;&#25512;&#29702;&#33021;&#21147;</title><link>https://arxiv.org/abs/2310.18338</link><description>&lt;p&gt;
&#32454;&#35843;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#20197;&#21327;&#35843;&#26356;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#39640;&#22797;&#26434;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Small Language Models Fine-tuned to Coordinate Larger Language Models improve Complex Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.18338
&lt;/p&gt;
&lt;p&gt;
&#32454;&#35843;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#20998;&#35299;&#29983;&#25104;&#22120;&#65292;&#20351;&#29992;&#31574;&#30053;&#26799;&#24230;&#20248;&#21270;&#26469;&#21327;&#35843;&#26356;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#22797;&#26434;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#29983;&#25104;&#24605;&#32500;&#38142;&#65288;CoT&#65289;&#26102;&#23637;&#29616;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#26368;&#36817;&#23545;&#25552;&#31034;&#20998;&#35299;&#20197;&#35299;&#20915;&#22797;&#26434;&#30340;&#22810;&#27493;&#25512;&#29702;&#38382;&#39064;&#30340;&#23581;&#35797;&#21462;&#20915;&#20110;LLM&#21516;&#26102;&#20998;&#35299;&#21644;&#35299;&#20915;&#38382;&#39064;&#30340;&#33021;&#21147;&#12290;&#19968;&#20010;&#37325;&#35201;&#30340;&#32570;&#28857;&#26159;&#65292;&#22522;&#30784;LLMs&#36890;&#24120;&#19981;&#21487;&#29992;&#20110;&#24494;&#35843;&#65292;&#20351;&#24471;&#36866;&#24212;&#24615;&#22312;&#35745;&#31639;&#19978;&#26159;&#31105;&#38178;&#30340;&#12290;&#25105;&#20204;&#35748;&#20026;&#65288;&#24182;&#35777;&#26126;&#65289;&#38382;&#39064;&#30340;&#20998;&#35299;&#21644;&#35299;&#20915;&#26041;&#26696;&#29983;&#25104;&#26159;&#19981;&#21516;&#30340;&#33021;&#21147;&#65292;&#26368;&#22909;&#36890;&#36807;&#21333;&#20010;&#24222;&#22823;&#30340;LLM&#32780;&#19981;&#26159;&#19968;&#20010;&#25972;&#20307;&#27169;&#22359;&#26469;&#35299;&#20915;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;DaSLaM&#65292;&#23427;&#20351;&#29992;&#19968;&#20010;&#20998;&#35299;&#29983;&#25104;&#22120;&#23558;&#22797;&#26434;&#38382;&#39064;&#20998;&#35299;&#25104;&#38656;&#35201;&#26356;&#23569;&#25512;&#29702;&#27493;&#39588;&#30340;&#23376;&#38382;&#39064;&#12290;&#36825;&#20123;&#23376;&#38382;&#39064;&#30001;&#19968;&#20010;&#27714;&#35299;&#22120;&#26469;&#22238;&#31572;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#30456;&#23545;&#23567;&#22411;&#65288;13B&#21442;&#25968;&#65289;LM&#20316;&#20026;&#20998;&#35299;&#29983;&#25104;&#22120;&#65292;&#25105;&#20204;&#20351;&#29992;&#31574;&#30053;&#26799;&#24230;&#20248;&#21270;&#26469;&#19982;&#20043;&#20132;&#20114;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.18338v2 Announce Type: replace-cross  Abstract: Large Language Models (LLMs) prompted to generate chain-of-thought (CoT) exhibit impressive reasoning capabilities. Recent attempts at prompt decomposition toward solving complex, multi-step reasoning problems depend on the ability of the LLM to simultaneously decompose and solve the problem. A significant disadvantage is that foundational LLMs are typically not available for fine-tuning, making adaptation computationally prohibitive. We believe (and demonstrate) that problem decomposition and solution generation are distinct capabilites, better addressed in separate modules, than by one monolithic LLM. We introduce DaSLaM, which uses a decomposition generator to decompose complex problems into subproblems that require fewer reasoning steps. These subproblems are answered by a solver. We use a relatively small (13B parameters) LM as the decomposition generator, which we train using policy gradient optimization to interact with 
&lt;/p&gt;</description></item><item><title>&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#24341;&#20837;&#20102;&#22810;&#27169;&#24577;&#32852;&#37030;&#23398;&#20064;&#65292;&#32467;&#21512;&#26368;&#26032;&#30340;&#26426;&#22120;&#23398;&#20064;&#36827;&#23637;&#65292;&#30830;&#20445;&#20102;&#24739;&#32773;&#25968;&#25454;&#38544;&#31169;&#21644;&#23433;&#20840;&#65292;&#20026;&#20248;&#21270;&#21307;&#30103;AI&#31995;&#32479;&#25552;&#20379;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>https://arxiv.org/abs/2310.09650</link><description>&lt;p&gt;
&#21307;&#30103;&#20445;&#20581;&#20013;&#30340;&#22810;&#27169;&#24577;&#32852;&#37030;&#23398;&#20064;&#65306;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Multimodal Federated Learning in Healthcare: a Review
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.09650
&lt;/p&gt;
&lt;p&gt;
&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#24341;&#20837;&#20102;&#22810;&#27169;&#24577;&#32852;&#37030;&#23398;&#20064;&#65292;&#32467;&#21512;&#26368;&#26032;&#30340;&#26426;&#22120;&#23398;&#20064;&#36827;&#23637;&#65292;&#30830;&#20445;&#20102;&#24739;&#32773;&#25968;&#25454;&#38544;&#31169;&#21644;&#23433;&#20840;&#65292;&#20026;&#20248;&#21270;&#21307;&#30103;AI&#31995;&#32479;&#25552;&#20379;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#26426;&#22120;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#36171;&#20104;&#20102;&#21307;&#23398;&#39046;&#22495;&#20934;&#30830;&#32780;&#31283;&#20581;&#30340;AI&#31995;&#32479;&#30340;&#21457;&#23637;&#65292;&#23588;&#20854;&#26159;&#22312;&#20013;&#24515;&#21270;&#25968;&#25454;&#24211;&#31995;&#32479;&#20869;&#12290;&#21516;&#26102;&#65292;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20063;&#24471;&#21040;&#20102;&#36827;&#23637;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#25968;&#25454;&#26080;&#38656;&#25972;&#21512;&#30340;&#21435;&#20013;&#24515;&#21270;&#26426;&#21046;&#65292;&#22686;&#24378;&#20102;&#23545;&#25935;&#24863;&#21307;&#30103;&#25968;&#25454;&#30340;&#38544;&#31169;&#21644;&#23433;&#20840;&#24615;&#12290;&#36825;&#20004;&#20010;&#27010;&#24565;&#30340;&#25972;&#21512;&#25903;&#25345;&#20102;&#21307;&#30103;&#20445;&#20581;&#20013;&#22810;&#27169;&#24577;&#23398;&#20064;&#30340;&#25345;&#32493;&#36827;&#23637;&#65292;&#21516;&#26102;&#30830;&#20445;&#20102;&#24739;&#32773;&#35760;&#24405;&#22312;&#26412;&#22320;&#25968;&#25454;&#25345;&#26377;&#26426;&#26500;&#20869;&#30340;&#23433;&#20840;&#21644;&#38544;&#31169;&#12290;&#26412;&#25991;&#31616;&#35201;&#27010;&#36848;&#20102;FL&#22312;&#21307;&#30103;&#20445;&#20581;&#20013;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#27010;&#36848;&#20102;&#21307;&#30103;&#39046;&#22495;&#20869;&#22810;&#27169;&#24577;&#32852;&#37030;&#23398;&#20064;&#65288;MMFL&#65289;&#30340;&#26368;&#26032;&#25216;&#26415;&#26041;&#27861;&#12290;&#25991;&#31456;&#20840;&#38754;&#23457;&#35270;&#20102;&#35813;&#39046;&#22495;&#20013;&#23384;&#22312;&#30340;&#25361;&#25112;&#65292;&#25581;&#31034;&#20102;&#29616;&#26377;&#27169;&#22411;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.09650v2 Announce Type: replace-cross  Abstract: Recent advancements in multimodal machine learning have empowered the development of accurate and robust AI systems in the medical domain, especially within centralized database systems. Simultaneously, Federated Learning (FL) has progressed, providing a decentralized mechanism where data need not be consolidated, thereby enhancing the privacy and security of sensitive healthcare data. The integration of these two concepts supports the ongoing progress of multimodal learning in healthcare while ensuring the security and privacy of patient records within local data-holding agencies. This paper offers a concise overview of the significance of FL in healthcare and outlines the current state-of-the-art approaches to Multimodal Federated Learning (MMFL) within the healthcare domain. It comprehensively examines the existing challenges in the field, shedding light on the limitations of present models. Finally, the paper outlines poten
&lt;/p&gt;</description></item><item><title>&#30446;&#21069;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#38754;&#23545;&#21518;&#32493;&#38382;&#39064;&#26102;&#24120;&#24120;&#25671;&#25670;&#19981;&#23450;&#65292;&#30740;&#31350;&#32773;&#25552;&#20986;&#20102;&#19968;&#20010;&#21518;&#32493;&#38382;&#39064;&#26426;&#21046;&#21644;&#20004;&#20010;&#24230;&#37327;&#26631;&#20934;&#26469;&#37327;&#21270;&#36825;&#31181;&#19981;&#19968;&#33268;&#24615;&#65292;&#24182;&#24320;&#21457;&#20986;Unwavering-FQ&#26694;&#26550;&#26469;&#25945;&#23548;&#27169;&#22411;&#20445;&#25345;&#26368;&#21021;&#30340;&#27491;&#30830;&#21028;&#26029;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2310.02174</link><description>&lt;p&gt;
&#35753;&#24490;&#29615;&#30340;&#35810;&#38382;: &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21028;&#26029;&#20013;&#30340;&#25671;&#25670;
&lt;/p&gt;
&lt;p&gt;
Ask Again, Then Fail: Large Language Models' Vacillations in Judgement
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.02174
&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#38754;&#23545;&#21518;&#32493;&#38382;&#39064;&#26102;&#24120;&#24120;&#25671;&#25670;&#19981;&#23450;&#65292;&#30740;&#31350;&#32773;&#25552;&#20986;&#20102;&#19968;&#20010;&#21518;&#32493;&#38382;&#39064;&#26426;&#21046;&#21644;&#20004;&#20010;&#24230;&#37327;&#26631;&#20934;&#26469;&#37327;&#21270;&#36825;&#31181;&#19981;&#19968;&#33268;&#24615;&#65292;&#24182;&#24320;&#21457;&#20986;Unwavering-FQ&#26694;&#26550;&#26469;&#25945;&#23548;&#27169;&#22411;&#20445;&#25345;&#26368;&#21021;&#30340;&#27491;&#30830;&#21028;&#26029;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35266;&#23519;&#21040;&#30446;&#21069;&#30340;&#20250;&#35805;&#24335;&#35821;&#35328;&#27169;&#22411;&#22312;&#38754;&#23545;&#21518;&#32493;&#38382;&#39064;&#26102;&#24448;&#24448;&#22312;&#20854;&#21028;&#26029;&#19978;&#25671;&#25670;&#19981;&#23450;&#65292;&#21363;&#20351;&#21407;&#22987;&#21028;&#26029;&#26159;&#27491;&#30830;&#30340;&#12290;&#36825;&#31181;&#25671;&#25670;&#23545;&#20110;&#29983;&#25104;&#21487;&#38752;&#22238;&#22797;&#21644;&#24314;&#31435;&#29992;&#25143;&#20449;&#20219;&#26500;&#25104;&#20102;&#37325;&#35201;&#25361;&#25112;&#12290;&#20026;&#20102;&#20840;&#38754;&#35780;&#20272;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21518;&#32493;&#38382;&#39064;&#26426;&#21046;&#20197;&#21450;&#20004;&#20010;&#24230;&#37327;&#26631;&#20934;&#26469;&#37327;&#21270;&#36825;&#31181;&#19981;&#19968;&#33268;&#24615;&#65292;&#30830;&#35748;&#20102;&#24403;&#21069;&#35821;&#35328;&#27169;&#22411;&#26222;&#36941;&#23384;&#22312;&#36825;&#31181;&#24773;&#20917;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#21508;&#31181;&#25552;&#31034;&#31574;&#30053;&#29992;&#20110;&#38381;&#28304;&#27169;&#22411;&#65307;&#27492;&#22806;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;&#35757;&#32451;&#30340;&#26694;&#26550;Unwavering-FQ&#65292;&#36890;&#36807;&#21512;&#25104;&#39640;&#36136;&#37327;&#30340;&#20559;&#22909;&#25968;&#25454;&#26469;&#25945;&#23548;&#35821;&#35328;&#27169;&#22411;&#20445;&#25345;&#20854;&#26368;&#21021;&#30340;&#27491;&#30830;&#21028;&#26029;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#39564;&#35777;&#20102;&#25105;&#20204;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#20197;&#21450;&#20854;&#22686;&#24378;&#27169;&#22411;&#36890;&#29992;&#33021;&#21147;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.02174v2 Announce Type: replace-cross  Abstract: We observe that current conversational language models often waver in their judgements when faced with follow-up questions, even if the original judgement was correct. This wavering presents a significant challenge for generating reliable responses and building user trust. To comprehensively assess this issue, we introduce a Follow-up Questioning Mechanism along with two metrics to quantify this inconsistency, confirming its widespread presence in current language models. To mitigate this issue, we explore various prompting strategies for closed-source models; moreover, we develop a training-based framework Unwavering-FQ that teaches language models to maintain their originally correct judgements through synthesized high-quality preference data. Our experimental results confirm the effectiveness of our framework and its ability to enhance the general capabilities of models (https://github.com/NUSTM/LLMs-Waver-In-Judgements).
&lt;/p&gt;</description></item><item><title>EvalLM&#26159;&#19968;&#20010;&#20132;&#20114;&#24335;&#31995;&#32479;&#65292;&#24110;&#21161;&#24320;&#21457;&#20154;&#21592;&#36890;&#36807;&#35780;&#20272;&#22810;&#20010;&#36755;&#20986;&#26469;&#25913;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#65292;&#30456;&#27604;&#25163;&#21160;&#35780;&#20272;&#65292;&#33021;&#22815;&#24110;&#21161;&#29992;&#25143;&#25776;&#20889;&#26356;&#22810;&#26679;&#21270;&#30340;&#26631;&#20934;&#12289;&#26816;&#26597;&#26356;&#22810;&#36755;&#20986;&#65292;&#36798;&#21040;&#26356;&#28385;&#24847;&#30340;&#25552;&#31034;&#12290;</title><link>https://arxiv.org/abs/2309.13633</link><description>&lt;p&gt;
EvalLM: &#20132;&#20114;&#24335;&#35780;&#20272;&#22522;&#20110;&#29992;&#25143;&#23450;&#20041;&#26631;&#20934;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
EvalLM: Interactive Evaluation of Large Language Model Prompts on User-Defined Criteria
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2309.13633
&lt;/p&gt;
&lt;p&gt;
EvalLM&#26159;&#19968;&#20010;&#20132;&#20114;&#24335;&#31995;&#32479;&#65292;&#24110;&#21161;&#24320;&#21457;&#20154;&#21592;&#36890;&#36807;&#35780;&#20272;&#22810;&#20010;&#36755;&#20986;&#26469;&#25913;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#65292;&#30456;&#27604;&#25163;&#21160;&#35780;&#20272;&#65292;&#33021;&#22815;&#24110;&#21161;&#29992;&#25143;&#25776;&#20889;&#26356;&#22810;&#26679;&#21270;&#30340;&#26631;&#20934;&#12289;&#26816;&#26597;&#26356;&#22810;&#36755;&#20986;&#65292;&#36798;&#21040;&#26356;&#28385;&#24847;&#30340;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#31616;&#21333;&#22320;&#32452;&#21512;&#25552;&#31034;&#65292;&#24320;&#21457;&#20154;&#21592;&#21487;&#20197;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21407;&#22411;&#21270;&#26032;&#39062;&#30340;&#29983;&#25104;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#35201;&#23558;&#21407;&#22411;&#32454;&#21270;&#20026;&#20135;&#21697;&#65292;&#24320;&#21457;&#20154;&#21592;&#24517;&#39035;&#36890;&#36807;&#35780;&#20272;&#36755;&#20986;&#20197;&#35786;&#26029;&#24369;&#28857;&#26469;&#36845;&#20195;&#20462;&#35746;&#25552;&#31034;&#12290;&#24418;&#25104;&#24615;&#35775;&#35848;&#65288;N=8&#65289;&#26174;&#31034;&#65292;&#24320;&#21457;&#20154;&#21592;&#22312;&#35780;&#20272;&#36755;&#20986;&#26102;&#25237;&#20837;&#20102;&#22823;&#37327;&#31934;&#21147;&#65292;&#22240;&#20026;&#20182;&#20204;&#35780;&#20272;&#29305;&#23450;&#19978;&#19979;&#25991;&#21644;&#20027;&#35266;&#26631;&#20934;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;EvalLM&#65292;&#36825;&#26159;&#19968;&#20010;&#20132;&#20114;&#24335;&#31995;&#32479;&#65292;&#21487;&#20197;&#36890;&#36807;&#35780;&#20272;&#29992;&#25143;&#23450;&#20041;&#26631;&#20934;&#19978;&#30340;&#22810;&#20010;&#36755;&#20986;&#26469;&#36845;&#20195;&#25913;&#36827;&#25552;&#31034;&#12290;&#29992;&#25143;&#21487;&#20197;&#36890;&#36807;&#29992;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#26631;&#20934;&#65292;&#20351;&#29992;&#31995;&#32479;&#22522;&#20110;LLM&#30340;&#35780;&#20272;&#22120;&#26469;&#33719;&#24471;&#25552;&#31034;&#22312;&#21738;&#20123;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#25110;&#22833;&#36133;&#30340;&#27010;&#36848;&#65292;&#24182;&#26681;&#25454;&#35780;&#20272;&#22120;&#30340;&#21453;&#39304;&#36827;&#34892;&#25913;&#36827;&#12290;&#19968;&#39033;&#27604;&#36739;&#30740;&#31350;&#65288;N=12&#65289;&#26174;&#31034;&#65292;&#19982;&#25163;&#21160;&#35780;&#20272;&#30456;&#27604;&#65292;EvalLM&#26377;&#21161;&#20110;&#24110;&#21161;&#21442;&#19982;&#32773;&#25776;&#20889;&#26356;&#22810;&#26679;&#21270;&#30340;&#26631;&#20934;&#12289;&#26816;&#26597;&#20004;&#20493;&#25968;&#37327;&#30340;&#36755;&#20986;&#65292;&#24182;&#36798;&#21040;&#20196;&#20154;&#28385;&#24847;&#30340;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2309.13633v2 Announce Type: replace-cross  Abstract: By simply composing prompts, developers can prototype novel generative applications with Large Language Models (LLMs). To refine prototypes into products, however, developers must iteratively revise prompts by evaluating outputs to diagnose weaknesses. Formative interviews (N=8) revealed that developers invest significant effort in manually evaluating outputs as they assess context-specific and subjective criteria. We present EvalLM, an interactive system for iteratively refining prompts by evaluating multiple outputs on user-defined criteria. By describing criteria in natural language, users can employ the system's LLM-based evaluator to get an overview of where prompts excel or fail, and improve these based on the evaluator's feedback. A comparative study (N=12) showed that EvalLM, when compared to manual evaluation, helped participants compose more diverse criteria, examine twice as many outputs, and reach satisfactory promp
&lt;/p&gt;</description></item><item><title>&#20197;&#23545;&#35937;&#20026;&#20013;&#24515;&#30340;Atari 2600&#24378;&#21270;&#23398;&#20064;&#29615;&#22659;OCAtari&#25193;&#23637;&#20102;Atari Learning Environments&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#23545;&#28216;&#25103;&#20013;&#22522;&#20110;&#23545;&#35937;&#30340;&#29366;&#24577;&#36827;&#34892;&#36164;&#28304;&#39640;&#25928;&#25552;&#21462;&#65292;&#24182;&#20801;&#35768;&#23545;&#35937;&#21457;&#29616;&#12289;&#23545;&#35937;&#34920;&#24449;&#23398;&#20064;&#20197;&#21450;&#23545;&#35937;&#20026;&#20013;&#24515;&#30340;&#24378;&#21270;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2306.08649</link><description>&lt;p&gt;
OCAtari: &#20197;&#23545;&#35937;&#20026;&#20013;&#24515;&#30340;Atari 2600&#24378;&#21270;&#23398;&#20064;&#29615;&#22659;
&lt;/p&gt;
&lt;p&gt;
OCAtari: Object-Centric Atari 2600 Reinforcement Learning Environments
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2306.08649
&lt;/p&gt;
&lt;p&gt;
&#20197;&#23545;&#35937;&#20026;&#20013;&#24515;&#30340;Atari 2600&#24378;&#21270;&#23398;&#20064;&#29615;&#22659;OCAtari&#25193;&#23637;&#20102;Atari Learning Environments&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#23545;&#28216;&#25103;&#20013;&#22522;&#20110;&#23545;&#35937;&#30340;&#29366;&#24577;&#36827;&#34892;&#36164;&#28304;&#39640;&#25928;&#25552;&#21462;&#65292;&#24182;&#20801;&#35768;&#23545;&#35937;&#21457;&#29616;&#12289;&#23545;&#35937;&#34920;&#24449;&#23398;&#20064;&#20197;&#21450;&#23545;&#35937;&#20026;&#20013;&#24515;&#30340;&#24378;&#21270;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35748;&#30693;&#31185;&#23398;&#21644;&#24515;&#29702;&#23398;&#34920;&#26126;&#65292;&#22797;&#26434;&#22330;&#26223;&#30340;&#20197;&#23545;&#35937;&#20026;&#20013;&#24515;&#30340;&#34920;&#24449;&#26159;&#23454;&#29616;&#20174;&#20302;&#32423;&#24863;&#30693;&#29305;&#24449;&#26377;&#25928;&#25277;&#35937;&#25512;&#29702;&#30340;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#27493;&#39588;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#21482;&#20381;&#36182;&#20110;&#20687;&#32032;&#32423;&#34920;&#31034;&#65292;&#26080;&#27861;&#25429;&#25417;&#33258;&#28982;&#22330;&#26223;&#30340;&#32452;&#21512;&#29305;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#38656;&#35201;&#20801;&#35768;&#25105;&#20204;&#22788;&#29702;&#21644;&#35780;&#20272;&#20197;&#23545;&#35937;&#20026;&#20013;&#24515;&#26041;&#27861;&#30340;&#29615;&#22659;&#21644;&#25968;&#25454;&#38598;&#12290;&#22312;&#25105;&#20204;&#30340;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;OCAtari&#26469;&#25193;&#23637;Atari&#23398;&#20064;&#29615;&#22659;&#65292;&#36825;&#26159;&#28145;&#24230;RL&#26041;&#27861;&#26368;&#24120;&#29992;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;OCAtari&#23545;&#36825;&#20123;&#28216;&#25103;&#36827;&#34892;&#20102;&#36164;&#28304;&#39640;&#25928;&#30340;&#23545;&#35937;&#20013;&#24515;&#29366;&#24577;&#25552;&#21462;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#20801;&#35768;&#23545;&#35937;&#21457;&#29616;&#12289;&#23545;&#35937;&#34920;&#24449;&#23398;&#20064;&#20197;&#21450;&#23545;&#35937;&#20026;&#20013;&#24515;&#30340;RL&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;OCAtari&#30340;&#26816;&#27979;&#33021;&#21147;&#21644;&#36164;&#28304;&#25928;&#29575;&#12290;&#25105;&#20204;&#30340;&#28304;&#20195;&#30721;&#21487;&#22312;github.com/k4ntz/OC_Atari&#19978;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2306.08649v2 Announce Type: replace-cross  Abstract: Cognitive science and psychology suggest that object-centric representations of complex scenes are a promising step towards enabling efficient abstract reasoning from low-level perceptual features. Yet, most deep reinforcement learning approaches only rely on pixel-based representations that do not capture the compositional properties of natural scenes. For this, we need environments and datasets that allow us to work and evaluate object-centric approaches. In our work, we extend the Atari Learning Environments, the most-used evaluation framework for deep RL approaches, by introducing OCAtari, that performs resource-efficient extractions of the object-centric states for these games. Our framework allows for object discovery, object representation learning, as well as object-centric RL. We evaluate OCAtari's detection capabilities and resource efficiency. Our source code is available at github.com/k4ntz/OC_Atari.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#8220;&#26681;&#25454;&#8221;&#25552;&#31034;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21709;&#24212;&#20013;&#21442;&#32771;&#20808;&#21069;&#35266;&#23519;&#21040;&#30340;&#25991;&#26412;&#26469;&#25913;&#36827;&#24341;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35780;&#20272;&#25351;&#26631;&#65288;QUIP-Score&#65289;&#26469;&#24230;&#37327;&#27169;&#22411;&#29983;&#25104;&#30340;&#31572;&#26696;&#22312;&#25991;&#26412;&#35821;&#26009;&#24211;&#20013;&#30340;&#30452;&#25509;&#21457;&#29616;&#31243;&#24230;&#12290;</title><link>https://arxiv.org/abs/2305.13252</link><description>&lt;p&gt;
&#26681;&#25454;...&#65306;&#20419;&#20351;&#35821;&#35328;&#27169;&#22411;&#25552;&#39640;&#24341;&#29992;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
"According to ...": Prompting Language Models Improves Quoting from Pre-Training Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2305.13252
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#8220;&#26681;&#25454;&#8221;&#25552;&#31034;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21709;&#24212;&#20013;&#21442;&#32771;&#20808;&#21069;&#35266;&#23519;&#21040;&#30340;&#25991;&#26412;&#26469;&#25913;&#36827;&#24341;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35780;&#20272;&#25351;&#26631;&#65288;QUIP-Score&#65289;&#26469;&#24230;&#37327;&#27169;&#22411;&#29983;&#25104;&#30340;&#31572;&#26696;&#22312;&#25991;&#26412;&#35821;&#26009;&#24211;&#20013;&#30340;&#30452;&#25509;&#21457;&#29616;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21487;&#33021;&#20250;&#20135;&#29983;&#24187;&#35273;&#24182;&#29983;&#25104;&#34394;&#20551;&#20449;&#24687;&#65292;&#23613;&#31649;&#23427;&#20204;&#20107;&#20808;&#22312;&#20107;&#23454;&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#35757;&#32451;&#12290;&#21463;&#8220;&#25454;&#24713;&#8221;&#30340;&#26032;&#38395;&#35774;&#22791;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;&#26681;&#25454;&#8221;&#25552;&#31034;&#65306;&#25351;&#23548;LLMs&#23558;&#21709;&#24212;&#19982;&#20808;&#21069;&#35266;&#23519;&#21040;&#30340;&#25991;&#26412;&#30456;&#32852;&#31995;&#12290;&#20026;&#20102;&#37327;&#21270;&#36825;&#31181;&#22522;&#30784;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35780;&#20272;&#25351;&#26631;&#65288;QUIP-Score&#65289;&#65292;&#29992;&#20110;&#34913;&#37327;&#27169;&#22411;&#29983;&#25104;&#30340;&#31572;&#26696;&#22312;&#22522;&#30784;&#25991;&#26412;&#35821;&#26009;&#24211;&#20013;&#30452;&#25509;&#25214;&#21040;&#30340;&#31243;&#24230;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#19977;&#20010;&#35821;&#26009;&#24211;&#65288;&#32500;&#22522;&#30334;&#31185;&#12289;PubMed&#21644;&#32654;&#22269;&#27861;&#24459;&#31246;&#27861;&#20856;&#65289;&#36827;&#34892;&#23454;&#39564;&#26469;&#35828;&#26126;&#36825;&#20123;&#25552;&#31034;&#26681;&#25454;&#25105;&#20204;&#30340;&#24230;&#37327;&#26631;&#20934;&#25913;&#21892;&#20102;&#22522;&#30784;&#24615;&#65292;&#32780;&#19988;&#36890;&#24120;&#36824;&#25552;&#39640;&#20102;&#26368;&#32456;&#20219;&#21153;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#35201;&#27714;&#27169;&#22411;&#20943;&#23569;&#22522;&#30784;&#24615;&#65288;&#25110;&#32773;&#26681;&#25454;&#20854;&#20182;&#35821;&#26009;&#24211;&#65289;&#30340;&#25552;&#31034;&#30830;&#23454;&#38477;&#20302;&#20102;QUIP-Score&#65292;&#34920;&#26126;LLMs&#26377;&#33021;&#21147;&#26681;&#25454;&#35201;&#27714;&#22686;&#21152;&#25110;&#20943;&#23569;&#22522;&#30784;&#24615;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2305.13252v2 Announce Type: replace-cross  Abstract: Large Language Models (LLMs) may hallucinate and generate fake information, despite pre-training on factual data. Inspired by the journalistic device of "according to sources", we propose according-to prompting: directing LLMs to ground responses against previously observed text. To quantify this grounding, we propose a novel evaluation metric (QUIP-Score) that measures the extent to which model-produced answers are directly found in underlying text corpora. We illustrate with experiments on three corpora (Wikipedia, PubMed, and the U.S. legal tax code) that these prompts improve grounding under our metrics, with the additional benefit of often improving end-task performance. Furthermore, prompts that ask the model to decrease grounding (or to ground to other corpora) indeed decrease QUIP-Score, indicating the ability of LLMs to increase or decrease grounded generations on request.
&lt;/p&gt;</description></item><item><title>&#21512;&#25104;&#25968;&#25454;&#20316;&#20026;&#19968;&#31181;&#38544;&#31169;&#22686;&#24378;&#25216;&#26415;&#65292;&#36890;&#36807;&#21457;&#24067;&#20154;&#24037;&#29983;&#25104;&#30340;&#25968;&#25454;&#38598;&#26469;&#20195;&#26367;&#25935;&#24863;&#30495;&#23454;&#25968;&#25454;&#65292;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#65292;&#20294;&#20173;&#38754;&#20020;&#26410;&#35299;&#20915;&#30340;&#38544;&#31169;&#25361;&#25112;&#21644;&#23616;&#38480;&#24615;&#12290;</title><link>https://arxiv.org/abs/2303.01230</link><description>&lt;p&gt;
&#21512;&#25104;&#25968;&#25454;&#65306;&#26041;&#27861;&#12289;&#29992;&#20363;&#21644;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;
Synthetic Data: Methods, Use Cases, and Risks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2303.01230
&lt;/p&gt;
&lt;p&gt;
&#21512;&#25104;&#25968;&#25454;&#20316;&#20026;&#19968;&#31181;&#38544;&#31169;&#22686;&#24378;&#25216;&#26415;&#65292;&#36890;&#36807;&#21457;&#24067;&#20154;&#24037;&#29983;&#25104;&#30340;&#25968;&#25454;&#38598;&#26469;&#20195;&#26367;&#25935;&#24863;&#30495;&#23454;&#25968;&#25454;&#65292;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#65292;&#20294;&#20173;&#38754;&#20020;&#26410;&#35299;&#20915;&#30340;&#38544;&#31169;&#25361;&#25112;&#21644;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#20139;&#25968;&#25454;&#36890;&#24120;&#21487;&#20197;&#23454;&#29616;&#24341;&#20154;&#27880;&#30446;&#30340;&#24212;&#29992;&#31243;&#24207;&#21644;&#20998;&#26512;&#12290;&#28982;&#32780;&#65292;&#26356;&#22810;&#26102;&#20505;&#65292;&#26377;&#20215;&#20540;&#30340;&#25968;&#25454;&#38598;&#21253;&#21547;&#25935;&#24863;&#20449;&#24687;&#65292;&#22240;&#27492;&#20998;&#20139;&#23427;&#20204;&#21487;&#33021;&#20250;&#21361;&#21450;&#29992;&#25143;&#21644;&#32452;&#32455;&#30340;&#38544;&#31169;&#12290;&#22312;&#30740;&#31350;&#30028;&#21644;&#20135;&#19994;&#30028;&#37117;&#26085;&#30410;&#21463;&#21040;&#20851;&#27880;&#30340;&#19968;&#20010;&#21487;&#33021;&#26367;&#20195;&#26041;&#26696;&#26159;&#20998;&#20139;&#21512;&#25104;&#25968;&#25454;&#12290;&#36825;&#20010;&#24819;&#27861;&#26159;&#21457;&#24067;&#20154;&#24037;&#29983;&#25104;&#30340;&#25968;&#25454;&#38598;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#31867;&#20284;&#20110;&#23454;&#38469;&#25968;&#25454; -- &#26356;&#20934;&#30830;&#22320;&#35828;&#65292;&#20855;&#26377;&#31867;&#20284;&#30340;&#32479;&#35745;&#29305;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#21512;&#25104;&#25968;&#25454;&#30340;&#31616;&#35201;&#20171;&#32461;&#65292;&#24182;&#35752;&#35770;&#20102;&#23427;&#30340;&#29992;&#20363;&#12289;&#20173;&#26410;&#35299;&#20915;&#30340;&#38544;&#31169;&#25361;&#25112;&#20197;&#21450;&#20316;&#20026;&#19968;&#31181;&#26377;&#25928;&#30340;&#38544;&#31169;&#22686;&#24378;&#25216;&#26415;&#30340;&#20869;&#22312;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2303.01230v3 Announce Type: replace-cross  Abstract: Sharing data can often enable compelling applications and analytics. However, more often than not, valuable datasets contain information of a sensitive nature, and thus, sharing them can endanger the privacy of users and organizations. A possible alternative gaining momentum in both the research community and industry is to share synthetic data instead. The idea is to release artificially generated datasets that resemble the actual data -- more precisely, having similar statistical properties. In this article, we provide a gentle introduction to synthetic data and discuss its use cases, the privacy challenges that are still unaddressed, and its inherent limitations as an effective privacy-enhancing technology.
&lt;/p&gt;</description></item><item><title>&#22312;&#20010;&#24615;&#21270;&#25512;&#33616;&#39046;&#22495;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#23558;&#25512;&#33616;&#20844;&#24179;&#24615;&#34920;&#36798;&#20026;&#20998;&#37197;&#21644;&#32858;&#21512;&#38382;&#39064;&#30340;&#26032;&#27169;&#22411;&#65292;&#38598;&#25104;&#20102;&#20844;&#24179;&#20851;&#27880;&#28857;&#21644;&#20010;&#24615;&#21270;&#25512;&#33616;&#35268;&#23450;&#65292;&#24182;&#25512;&#23548;&#20986;&#20102;&#26032;&#30340;&#25512;&#33616;&#25216;&#26415;&#12290;</title><link>https://arxiv.org/abs/2303.00968</link><description>&lt;p&gt;
&#21160;&#24577;&#20844;&#24179;&#24863;&#30693;&#25512;&#33616;&#36890;&#36807;&#22810;&#26234;&#33021;&#20307;&#31038;&#20250;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Dynamic fairness-aware recommendation through multi-agent social choice
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2303.00968
&lt;/p&gt;
&lt;p&gt;
&#22312;&#20010;&#24615;&#21270;&#25512;&#33616;&#39046;&#22495;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#23558;&#25512;&#33616;&#20844;&#24179;&#24615;&#34920;&#36798;&#20026;&#20998;&#37197;&#21644;&#32858;&#21512;&#38382;&#39064;&#30340;&#26032;&#27169;&#22411;&#65292;&#38598;&#25104;&#20102;&#20844;&#24179;&#20851;&#27880;&#28857;&#21644;&#20010;&#24615;&#21270;&#25512;&#33616;&#35268;&#23450;&#65292;&#24182;&#25512;&#23548;&#20986;&#20102;&#26032;&#30340;&#25512;&#33616;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20010;&#24615;&#21270;&#25512;&#33616;&#30340;&#31639;&#27861;&#20844;&#24179;&#24615;&#20013;&#65292;&#31639;&#27861;&#20844;&#24179;&#24615;&#38754;&#20020;&#19982;&#20998;&#31867;&#20219;&#21153;&#20013;&#24120;&#35265;&#25361;&#25112;&#26174;&#33879;&#19981;&#21516;&#30340;&#38382;&#39064;&#12290;&#30740;&#31350;&#20998;&#31867;&#30340;&#30740;&#31350;&#20154;&#21592;&#36890;&#24120;&#35748;&#20026;&#20844;&#24179;&#24615;&#22312;&#20110;&#23454;&#29616;&#21463;&#20445;&#25252;&#32452;&#21644;&#26410;&#21463;&#20445;&#25252;&#32452;&#20043;&#38388;&#32467;&#26524;&#30340;&#24179;&#31561;&#65292;&#24182;&#22522;&#20110;&#27492;&#26500;&#24314;&#31639;&#27861;&#24178;&#39044;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#29615;&#22659;&#20013;&#36890;&#24120;&#26356;&#20026;&#22797;&#26434;&#19988;&#22810;&#26041;&#38754;&#65292;&#22312;&#20010;&#24615;&#21270;&#25512;&#33616;&#30340;&#32972;&#26223;&#19979;&#23588;&#20854;&#22914;&#27492;&#65292;&#38656;&#35201;&#26356;&#19968;&#33324;&#21270;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22411;&#65292;&#23558;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#22810;&#21033;&#30410;&#30456;&#20851;&#32773;&#20844;&#24179;&#24615;&#24418;&#24335;&#21270;&#20026;&#20004;&#38454;&#27573;&#31038;&#20250;&#36873;&#25321;&#38382;&#39064;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#23558;&#25512;&#33616;&#20844;&#24179;&#24615;&#34920;&#36798;&#20026;&#19968;&#20010;&#20998;&#37197;&#21644;&#32858;&#21512;&#38382;&#39064;&#30340;&#26032;&#39062;&#32452;&#21512;&#65292;&#23558;&#20844;&#24179;&#20851;&#27880;&#28857;&#21644;&#20010;&#24615;&#21270;&#25512;&#33616;&#35268;&#23450;&#34701;&#21512;&#22312;&#19968;&#36215;&#65292;&#24182;&#25512;&#23548;&#20986;&#26032;&#30340;&#25512;&#33616;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2303.00968v3 Announce Type: replace  Abstract: Algorithmic fairness in the context of personalized recommendation presents significantly different challenges to those commonly encountered in classification tasks. Researchers studying classification have generally considered fairness to be a matter of achieving equality of outcomes between a protected and unprotected group, and built algorithmic interventions on this basis. We argue that fairness in real-world application settings in general, and especially in the context of personalized recommendation, is much more complex and multi-faceted, requiring a more general approach. We propose a model to formalize multistakeholder fairness in recommender systems as a two stage social choice problem. In particular, we express recommendation fairness as a novel combination of an allocation and an aggregation problem, which integrate both fairness concerns and personalized recommendation provisions, and derive new recommendation techniques
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#24863;&#30693;&#35823;&#24046;&#27169;&#22411;&#65288;PEM&#65289;&#65292;&#29992;&#20110;&#24110;&#21161;&#20998;&#26512;&#24863;&#30693;&#35823;&#24046;&#23545;&#33258;&#21160;&#39550;&#39542;&#26080;&#20154;&#36710;&#23433;&#20840;&#24615;&#30340;&#24433;&#21709;&#65292;&#26080;&#38656;&#23545;&#20256;&#24863;&#22120;&#26412;&#36523;&#36827;&#34892;&#24314;&#27169;&#65292;&#36890;&#36807;&#25968;&#25454;&#39537;&#21160;&#30340;&#36807;&#31243;&#23454;&#29616;&#21442;&#25968;&#21270;&#24314;&#27169;&#65292;&#24182;&#22312;&#24320;&#28304;&#36719;&#20214;Apollo&#21644;&#20844;&#20849;&#25968;&#25454;&#38598;nuScenes&#20013;&#36827;&#34892;&#35780;&#20272;&#65292;&#23637;&#31034;&#20102;&#22522;&#20110;PEM&#30340;&#34394;&#25311;&#27979;&#35797;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2302.11919</link><description>&lt;p&gt;
PEM: &#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#26080;&#20154;&#36710;&#34394;&#25311;&#27979;&#35797;&#30340;&#24863;&#30693;&#35823;&#24046;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
PEM: Perception Error Model for Virtual Testing of Autonomous Vehicles
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2302.11919
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#24863;&#30693;&#35823;&#24046;&#27169;&#22411;&#65288;PEM&#65289;&#65292;&#29992;&#20110;&#24110;&#21161;&#20998;&#26512;&#24863;&#30693;&#35823;&#24046;&#23545;&#33258;&#21160;&#39550;&#39542;&#26080;&#20154;&#36710;&#23433;&#20840;&#24615;&#30340;&#24433;&#21709;&#65292;&#26080;&#38656;&#23545;&#20256;&#24863;&#22120;&#26412;&#36523;&#36827;&#34892;&#24314;&#27169;&#65292;&#36890;&#36807;&#25968;&#25454;&#39537;&#21160;&#30340;&#36807;&#31243;&#23454;&#29616;&#21442;&#25968;&#21270;&#24314;&#27169;&#65292;&#24182;&#22312;&#24320;&#28304;&#36719;&#20214;Apollo&#21644;&#20844;&#20849;&#25968;&#25454;&#38598;nuScenes&#20013;&#36827;&#34892;&#35780;&#20272;&#65292;&#23637;&#31034;&#20102;&#22522;&#20110;PEM&#30340;&#34394;&#25311;&#27979;&#35797;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#34394;&#25311;&#27979;&#35797;&#33258;&#21160;&#39550;&#39542;&#26080;&#20154;&#36710;&#65288;AVs&#65289;&#24050;&#34987;&#20844;&#35748;&#20026;&#23433;&#20840;&#35780;&#20272;&#20013;&#33267;&#20851;&#37325;&#35201;&#65292;AV&#27169;&#25311;&#22120;&#20173;&#22312;&#31215;&#26497;&#24320;&#21457;&#20013;&#12290;&#20854;&#20013;&#19968;&#20010;&#29305;&#21035;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#26159;&#22914;&#20309;&#26377;&#25928;&#22320;&#23558;&#24863;&#30693;&#19982;&#30693;&#35273;&#65288;S&amp;P&#65289;&#23376;&#31995;&#32479;&#32435;&#20837;&#27169;&#25311;&#24490;&#29615;&#20013;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#24863;&#30693;&#35823;&#24046;&#27169;&#22411;&#65288;PEM&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#34394;&#25311;&#27169;&#25311;&#32452;&#20214;&#65292;&#21487;&#20197;&#20998;&#26512;&#24863;&#30693;&#35823;&#24046;&#23545;AV&#23433;&#20840;&#24615;&#30340;&#24433;&#21709;&#65292;&#32780;&#26080;&#38656;&#23545;&#20256;&#24863;&#22120;&#26412;&#36523;&#36827;&#34892;&#24314;&#27169;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#25968;&#25454;&#39537;&#21160;&#36807;&#31243;&#29992;&#20110;&#21442;&#25968;&#24314;&#27169;&#65292;&#24182;&#20351;&#29992;Apollo&#65288;&#19968;&#31181;&#24320;&#28304;&#39550;&#39542;&#36719;&#20214;&#65289;&#21644;nuScenes&#65288;&#19968;&#20010;&#20844;&#20849;AV&#25968;&#25454;&#38598;&#65289;&#36827;&#34892;&#35780;&#20272;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;SVL&#65288;&#19968;&#31181;&#24320;&#28304;&#36710;&#36742;&#27169;&#25311;&#22120;&#65289;&#20013;&#23454;&#29616;&#20102;PEMs&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#35780;&#20272;&#25668;&#20687;&#22836;&#12289;LiDAR&#21644;&#25668;&#20687;&#22836;-LiDAR&#35774;&#32622;&#65292;&#23637;&#31034;&#20102;&#22522;&#20110;PEM&#30340;&#34394;&#25311;&#27979;&#35797;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#34394;&#25311;&#27979;&#35797;&#20984;&#26174;&#20102;&#22312;
&lt;/p&gt;
&lt;p&gt;
arXiv:2302.11919v2 Announce Type: replace-cross  Abstract: Even though virtual testing of Autonomous Vehicles (AVs) has been well recognized as essential for safety assessment, AV simulators are still undergoing active development. One particularly challenging question is to effectively include the Sensing and Perception (S&amp;P) subsystem into the simulation loop. In this article, we define Perception Error Models (PEM), a virtual simulation component that can enable the analysis of the impact of perception errors on AV safety, without the need to model the sensors themselves. We propose a generalized data-driven procedure towards parametric modeling and evaluate it using Apollo, an open-source driving software, and nuScenes, a public AV dataset. Additionally, we implement PEMs in SVL, an open-source vehicle simulator. Furthermore, we demonstrate the usefulness of PEM-based virtual tests, by evaluating camera, LiDAR, and camera-LiDAR setups. Our virtual tests highlight limitations in the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25511;&#21046;&#22120;&#24341;&#23548;&#30340;&#37096;&#20998;&#26631;&#31614;&#19968;&#33268;&#24615;&#35268;&#33539;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#26410;&#26631;&#35760;&#25968;&#25454;&#26469;&#25913;&#21892;&#37096;&#20998;&#27880;&#37322;&#19981;&#36275;&#30340;&#24773;&#20917;&#12290;</title><link>https://arxiv.org/abs/2210.11194</link><description>&lt;p&gt;
&#20351;&#29992;&#25511;&#21046;&#22120;&#24341;&#23548;&#30340;&#37096;&#20998;&#26631;&#31614;&#19968;&#33268;&#24615;&#35268;&#33539;&#19982;&#26410;&#26631;&#35760;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Controller-Guided Partial Label Consistency Regularization with Unlabeled Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2210.11194
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25511;&#21046;&#22120;&#24341;&#23548;&#30340;&#37096;&#20998;&#26631;&#31614;&#19968;&#33268;&#24615;&#35268;&#33539;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#26410;&#26631;&#35760;&#25968;&#25454;&#26469;&#25913;&#21892;&#37096;&#20998;&#27880;&#37322;&#19981;&#36275;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37096;&#20998;&#26631;&#31614;&#23398;&#20064;&#65288;PLL&#65289;&#20174;&#35757;&#32451;&#31034;&#20363;&#20013;&#23398;&#20064;&#65292;&#27599;&#20010;&#31034;&#20363;&#37117;&#19982;&#22810;&#20010;&#20505;&#36873;&#26631;&#31614;&#30456;&#20851;&#32852;&#65292;&#20854;&#20013;&#21482;&#26377;&#19968;&#20010;&#26159;&#26377;&#25928;&#30340;&#12290;&#36817;&#24180;&#26469;&#65292;&#21463;&#30410;&#20110;&#22788;&#29702;&#27169;&#31946;&#30417;&#30563;&#30340;&#24378;&#22823;&#33021;&#21147;&#21644;&#29616;&#20195;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#30340;&#25512;&#21160;&#65292;&#22522;&#20110;&#19968;&#33268;&#24615;&#35268;&#33539;&#30340;PLL&#26041;&#27861;&#21462;&#24471;&#20102;&#19968;&#31995;&#21015;&#25104;&#21151;&#24182;&#25104;&#20026;&#20027;&#27969;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#37096;&#20998;&#27880;&#37322;&#19981;&#36275;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#26174;&#33879;&#19979;&#38477;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#26131;&#20110;&#35775;&#38382;&#30340;&#26410;&#26631;&#35760;&#31034;&#20363;&#20419;&#36827;&#37096;&#20998;&#26631;&#31614;&#19968;&#33268;&#24615;&#35268;&#33539;&#12290;&#38500;&#20102;&#37096;&#20998;&#30417;&#30563;&#25439;&#22833;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#25511;&#21046;&#22120;&#22312;&#26631;&#31614;&#32423;&#21035;&#21644;&#34920;&#31034;&#32423;&#21035;&#23545;&#26410;&#26631;&#35760;&#25968;&#25454;&#36827;&#34892;&#19968;&#33268;&#24615;&#35268;&#33539;&#12290;&#20026;&#20102;&#26368;&#23567;&#21270;&#21021;&#22987;&#30417;&#30563;&#27169;&#22411;&#33021;&#21147;&#19981;&#36275;&#30340;&#32570;&#28857;&#65292;&#25105;&#20204;&#20351;&#29992;&#25511;&#21046;&#22120;&#20272;&#35745;&#27599;&#20010;
&lt;/p&gt;
&lt;p&gt;
arXiv:2210.11194v4 Announce Type: replace  Abstract: Partial label learning (PLL) learns from training examples each associated with multiple candidate labels, among which only one is valid. In recent years, benefiting from the strong capability of dealing with ambiguous supervision and the impetus of modern data augmentation methods, consistency regularization-based PLL methods have achieved a series of successes and become mainstream. However, as the partial annotation becomes insufficient, their performances drop significantly. In this paper, we leverage easily accessible unlabeled examples to facilitate the partial label consistency regularization. In addition to a partial supervised loss, our method performs a controller-guided consistency regularization at both the label-level and representation-level with the help of unlabeled data. To minimize the disadvantages of insufficient capabilities of the initial supervised model, we use the controller to estimate the confidence of each
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22312;&#19968;&#32452;&#65288;&#25277;&#35937;&#30340;&#65289;&#23376;&#30446;&#26631;&#19978;&#36827;&#34892;&#32422;&#26463;&#21644;&#23398;&#20064;&#26412;&#22320;&#12289;&#23376;&#30446;&#26631;&#26465;&#20214;&#30340;&#27169;&#22411;&#65292;&#26412;&#25991;&#25552;&#20986;&#30340;&#30446;&#26631;&#31354;&#38388;&#35268;&#21010;&#65288;GSP&#65289;&#26041;&#27861;&#26356;&#20855;&#35745;&#31639;&#25928;&#29575;&#65292;&#33258;&#28982;&#22320;&#32467;&#21512;&#20102;&#26102;&#38388;&#25277;&#35937;&#65292;&#36991;&#20813;&#20102;&#23398;&#20064;&#36716;&#25442;&#21160;&#21147;&#23398;&#12290;</title><link>https://arxiv.org/abs/2206.02902</link><description>&lt;p&gt;
&#20855;&#26377;&#23376;&#30446;&#26631;&#27169;&#22411;&#30340;&#30446;&#26631;&#31354;&#38388;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Goal-Space Planning with Subgoal Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2206.02902
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22312;&#19968;&#32452;&#65288;&#25277;&#35937;&#30340;&#65289;&#23376;&#30446;&#26631;&#19978;&#36827;&#34892;&#32422;&#26463;&#21644;&#23398;&#20064;&#26412;&#22320;&#12289;&#23376;&#30446;&#26631;&#26465;&#20214;&#30340;&#27169;&#22411;&#65292;&#26412;&#25991;&#25552;&#20986;&#30340;&#30446;&#26631;&#31354;&#38388;&#35268;&#21010;&#65288;GSP&#65289;&#26041;&#27861;&#26356;&#20855;&#35745;&#31639;&#25928;&#29575;&#65292;&#33258;&#28982;&#22320;&#32467;&#21512;&#20102;&#26102;&#38388;&#25277;&#35937;&#65292;&#36991;&#20813;&#20102;&#23398;&#20064;&#36716;&#25442;&#21160;&#21147;&#23398;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#20351;&#29992;&#32972;&#26223;&#35268;&#21010;&#65306;&#28151;&#21512;&#65288;&#36817;&#20284;&#30340;&#65289;&#21160;&#24577;&#35268;&#21010;&#26356;&#26032;&#21644;&#27169;&#22411;&#26080;&#20851;&#30340;&#26356;&#26032;&#65292;&#31867;&#20284;&#20110;Dyna&#26550;&#26500;&#12290;&#21033;&#29992;&#23398;&#20064;&#30340;&#27169;&#22411;&#36827;&#34892;&#32972;&#26223;&#35268;&#21010;&#36890;&#24120;&#27604;&#27169;&#22411;&#26080;&#20851;&#30340;&#26367;&#20195;&#26041;&#27861;&#65288;&#22914;Double DQN&#65289;&#26356;&#24046;&#65292;&#23613;&#31649;&#21069;&#32773;&#20351;&#29992;&#20102;&#26174;&#33879;&#26356;&#22810;&#30340;&#20869;&#23384;&#21644;&#35745;&#31639;&#36164;&#28304;&#12290;&#26681;&#26412;&#38382;&#39064;&#22312;&#20110;&#65292;&#23398;&#20064;&#30340;&#27169;&#22411;&#21487;&#33021;&#19981;&#20934;&#30830;&#65292;&#24182;&#19988;&#22312;&#36845;&#20195;&#22810;&#20010;&#27493;&#39588;&#26102;&#36890;&#24120;&#20250;&#20135;&#29983;&#26080;&#25928;&#29366;&#24577;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;&#32972;&#26223;&#35268;&#21010;&#38480;&#21046;&#22312;&#19968;&#32452;&#65288;&#25277;&#35937;&#30340;&#65289;&#23376;&#30446;&#26631;&#65292;&#24182;&#20165;&#23398;&#20064;&#26412;&#22320;&#12289;&#23376;&#30446;&#26631;&#26465;&#20214;&#30340;&#27169;&#22411;&#26469;&#36991;&#20813;&#36825;&#31181;&#38480;&#21046;&#12290;&#36825;&#31181;&#30446;&#26631;&#31354;&#38388;&#35268;&#21010;&#65288;GSP&#65289;&#26041;&#27861;&#26356;&#20855;&#35745;&#31639;&#25928;&#29575;&#65292;&#33258;&#28982;&#22320;&#32467;&#21512;&#20102;&#29992;&#20110;&#26356;&#24555;&#38271;&#26102;&#31243;&#35268;&#21010;&#30340;&#26102;&#38388;&#25277;&#35937;&#65292;&#24182;&#23436;&#20840;&#36991;&#20813;&#20102;&#23398;&#20064;&#36716;&#25442;&#21160;&#21147;&#23398;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;GSP&#31639;&#27861;&#21487;&#20197;&#20256;&#25773;&#20215;&#20540;
&lt;/p&gt;
&lt;p&gt;
arXiv:2206.02902v5 Announce Type: replace-cross  Abstract: This paper investigates a new approach to model-based reinforcement learning using background planning: mixing (approximate) dynamic programming updates and model-free updates, similar to the Dyna architecture. Background planning with learned models is often worse than model-free alternatives, such as Double DQN, even though the former uses significantly more memory and computation. The fundamental problem is that learned models can be inaccurate and often generate invalid states, especially when iterated many steps. In this paper, we avoid this limitation by constraining background planning to a set of (abstract) subgoals and learning only local, subgoal-conditioned models. This goal-space planning (GSP) approach is more computationally efficient, naturally incorporates temporal abstraction for faster long-horizon planning and avoids learning the transition dynamics entirely. We show that our GSP algorithm can propagate value
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#28151;&#21512;&#25163;&#21183;&#35782;&#21035;&#31995;&#32479;&#65292;&#33021;&#22815;&#23398;&#20064;&#38745;&#24577;&#21644;&#21160;&#24577;&#25163;&#21183;&#65292;&#24182;&#36890;&#36807;&#25429;&#25417;&#25163;&#21183;&#34920;&#29616;&#39640;&#23792;&#30340;&#8220;&#24555;&#29031;&#8221;&#26469;&#34701;&#21512;&#38745;&#24577;&#23039;&#21183;&#21644;&#21160;&#24577;&#36816;&#21160;&#12290;</title><link>https://arxiv.org/abs/2205.15862</link><description>&lt;p&gt;
Snapture -- &#19968;&#31181;&#29992;&#20110;&#38745;&#24577;&#21644;&#21160;&#24577;&#25163;&#21183;&#35782;&#21035;&#30340;&#26032;&#22411;&#31070;&#32463;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
Snapture -- A Novel Neural Architecture for Combined Static and Dynamic Hand Gesture Recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2205.15862
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#28151;&#21512;&#25163;&#21183;&#35782;&#21035;&#31995;&#32479;&#65292;&#33021;&#22815;&#23398;&#20064;&#38745;&#24577;&#21644;&#21160;&#24577;&#25163;&#21183;&#65292;&#24182;&#36890;&#36807;&#25429;&#25417;&#25163;&#21183;&#34920;&#29616;&#39640;&#23792;&#30340;&#8220;&#24555;&#29031;&#8221;&#26469;&#34701;&#21512;&#38745;&#24577;&#23039;&#21183;&#21644;&#21160;&#24577;&#36816;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26426;&#22120;&#20154;&#39044;&#26399;&#26356;&#22810;&#22320;&#21442;&#19982;&#20154;&#20204;&#30340;&#26085;&#24120;&#29983;&#27963;&#65292;&#38656;&#35201;&#33021;&#22815;&#25552;&#20379;&#30452;&#35266;&#29992;&#25143;&#30028;&#38754;&#30340;&#26694;&#26550;&#12290;&#25163;&#21183;&#35782;&#21035;&#31995;&#32479;&#25552;&#20379;&#20102;&#19968;&#31181;&#33258;&#28982;&#30340;&#20132;&#27969;&#26041;&#24335;&#65292;&#22240;&#27492;&#26159;&#26080;&#32541;&#20154;&#26426;&#20132;&#20114;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#36817;&#24180;&#26469;&#65292;&#30001;&#28145;&#24230;&#23398;&#20064;&#39537;&#21160;&#30340;&#35745;&#31639;&#27169;&#22411;&#21457;&#29983;&#20102;&#24040;&#22823;&#30340;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#22312;&#36328;&#19981;&#21516;&#25163;&#21183;&#39046;&#22495;&#65288;&#22914;&#35937;&#24449;&#25163;&#21183;&#21644;&#20849;&#35821;&#25163;&#21183;&#65289;&#26041;&#38754;&#20173;&#23384;&#22312;&#19981;&#36275;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28151;&#21512;&#25163;&#21183;&#35782;&#21035;&#31995;&#32479;&#12290;&#25105;&#20204;&#30340;&#26550;&#26500;&#23454;&#29616;&#20102;&#23545;&#38745;&#24577;&#21644;&#21160;&#24577;&#25163;&#21183;&#30340;&#23398;&#20064;&#65306;&#36890;&#36807;&#25429;&#25417;&#25163;&#21183;&#22312;&#20854;&#39640;&#23792;&#34920;&#29616;&#26102;&#30340;&#25152;&#35859;&#8220;&#24555;&#29031;&#8221;&#65292;&#25105;&#20204;&#23558;&#25163;&#21183;&#23039;&#21183;&#19982;&#21160;&#24577;&#36816;&#21160;&#34701;&#21512;&#22312;&#19968;&#36215;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#26512;&#25163;&#21183;&#36816;&#21160;&#36712;&#36857;&#30340;&#26041;&#27861;&#65292;&#20197;&#25581;&#31034;&#20854;&#21160;&#24577;&#29305;&#24449;&#65292;&#24182;&#20801;&#35768;&#35843;&#33410;&#19968;&#20010;&#38745;&#24577;&#36890;&#36947;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2205.15862v2 Announce Type: replace-cross  Abstract: As robots are expected to get more involved in people's everyday lives, frameworks that enable intuitive user interfaces are in demand. Hand gesture recognition systems provide a natural way of communication and, thus, are an integral part of seamless Human-Robot Interaction (HRI). Recent years have witnessed an immense evolution of computational models powered by deep learning. However, state-of-the-art models fall short in expanding across different gesture domains, such as emblems and co-speech. In this paper, we propose a novel hybrid hand gesture recognition system. Our architecture enables learning both static and dynamic gestures: by capturing a so-called "snapshot" of the gesture performance at its peak, we integrate the hand pose along with the dynamic movement. Moreover, we present a method for analyzing the motion profile of a gesture to uncover its dynamic characteristics and which allows regulating a static channel
&lt;/p&gt;</description></item><item><title>&#33258;&#21160;&#21270;&#26426;&#22120;&#23398;&#20064;&#65288;AutoML&#65289;&#26088;&#22312;&#20197;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#24335;&#29983;&#25104;&#20196;&#20154;&#28385;&#24847;&#30340;ML&#37197;&#32622;&#65292;&#26412;&#25991;&#25552;&#20379;&#20102;&#23545;AutoML&#21407;&#29702;&#21644;&#23454;&#36341;&#30340;&#20840;&#38754;&#35843;&#30740;&#12290;</title><link>https://arxiv.org/abs/1810.13306</link><description>&lt;p&gt;
&#33258;&#21160;&#21270;&#26426;&#22120;&#23398;&#20064;&#65306;&#20174;&#21407;&#29702;&#21040;&#23454;&#36341;
&lt;/p&gt;
&lt;p&gt;
Automated Machine Learning: From Principles to Practices
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/1810.13306
&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#21270;&#26426;&#22120;&#23398;&#20064;&#65288;AutoML&#65289;&#26088;&#22312;&#20197;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#24335;&#29983;&#25104;&#20196;&#20154;&#28385;&#24847;&#30340;ML&#37197;&#32622;&#65292;&#26412;&#25991;&#25552;&#20379;&#20102;&#23545;AutoML&#21407;&#29702;&#21644;&#23454;&#36341;&#30340;&#20840;&#38754;&#35843;&#30740;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#26041;&#27861;&#21457;&#23637;&#36805;&#36895;&#65292;&#20294;&#37197;&#32622;&#21644;&#36873;&#25321;&#21512;&#36866;&#30340;&#26041;&#27861;&#20197;&#36798;&#21040;&#25152;&#38656;&#24615;&#33021;&#27491;&#21464;&#24471;&#36234;&#26469;&#36234;&#22256;&#38590;&#21644;&#20047;&#21619;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#33258;&#21160;&#21270;&#26426;&#22120;&#23398;&#20064;&#65288;AutoML&#65289;&#24212;&#36816;&#32780;&#29983;&#65292;&#26088;&#22312;&#20197;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#24335;&#20026;&#32473;&#23450;&#20219;&#21153;&#29983;&#25104;&#20196;&#20154;&#28385;&#24847;&#30340;ML&#37197;&#32622;&#12290;&#26412;&#25991;&#23601;&#35813;&#20027;&#39064;&#36827;&#34892;&#20102;&#20840;&#38754;&#35843;&#30740;&#12290;&#25105;&#20204;&#20174;AutoML&#30340;&#27491;&#24335;&#23450;&#20041;&#24320;&#22987;&#65292;&#28982;&#21518;&#20171;&#32461;&#20854;&#21407;&#29702;&#65292;&#21253;&#25324;&#21452;&#23618;&#23398;&#20064;&#30446;&#26631;&#12289;&#23398;&#20064;&#31574;&#30053;&#21644;&#29702;&#35770;&#35299;&#37322;&#12290;&#25509;&#30528;&#65292;&#25105;&#20204;&#36890;&#36807;&#22522;&#20110;&#19977;&#20010;&#20027;&#35201;&#22240;&#32032;&#8212;&#8212;&#25628;&#32034;&#31354;&#38388;&#12289;&#25628;&#32034;&#31639;&#27861;&#21644;&#35780;&#20272;&#31574;&#30053;&#8212;&#8212;&#35774;&#31435;&#29616;&#26377;&#24037;&#20316;&#30340;&#20998;&#31867;&#27861;&#26469;&#24635;&#32467;AutoML&#23454;&#36341;&#12290;&#27599;&#20010;&#31867;&#21035;&#36824;&#20250;&#36890;&#36807;&#20195;&#34920;&#24615;&#26041;&#27861;&#36827;&#34892;&#35299;&#37322;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#37197;&#32622;ML&#31649;&#32447;&#31561;&#31034;&#20363;&#24212;&#29992;&#35828;&#26126;&#20102;AutoML&#30340;&#21407;&#29702;&#21644;&#23454;&#36341;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:1810.13306v5 Announce Type: replace  Abstract: Machine learning (ML) methods have been developing rapidly, but configuring and selecting proper methods to achieve a desired performance is increasingly difficult and tedious. To address this challenge, automated machine learning (AutoML) has emerged, which aims to generate satisfactory ML configurations for given tasks in a data-driven way. In this paper, we provide a comprehensive survey on this topic. We begin with the formal definition of AutoML and then introduce its principles, including the bi-level learning objective, the learning strategy, and the theoretical interpretation. Then, we summarize the AutoML practices by setting up the taxonomy of existing works based on three main factors: the search space, the search algorithm, and the evaluation strategy. Each category is also explained with the representative methods. Then, we illustrate the principles and practices with exemplary applications from configuring ML pipeline, 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21021;&#27493;&#25506;&#32034;&#20102;&#22522;&#20110;GPT-4&#30340;ChatGPT&#22312;&#38754;&#37096;&#29983;&#29289;&#35782;&#21035;&#20013;&#30340;&#34920;&#29616;&#12290;&#30740;&#31350;&#20998;&#26512;&#20102;ChatGPT&#22312;&#38754;&#37096;&#39564;&#35777;&#12289;&#36719;&#29983;&#29289;&#29305;&#24449;&#20272;&#35745;&#21644;&#32467;&#26524;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;ChatGPT&#30340;&#24212;&#29992;&#26377;&#26395;&#25552;&#39640;&#33258;&#21160;&#20915;&#31574;&#22312;&#20154;&#31867;&#22330;&#26223;&#20013;&#30340;&#35299;&#37322;&#24615;&#21644;&#36879;&#26126;&#24230;&#12290;</title><link>http://arxiv.org/abs/2401.13641</link><description>&lt;p&gt;
ChatGPT&#22312;&#38754;&#37096;&#29983;&#29289;&#35782;&#21035;&#20013;&#30340;&#34920;&#29616;&#26377;&#22810;&#22909;&#65311;&#23545;&#35782;&#21035;&#12289;&#36719;&#29983;&#29289;&#29305;&#24449;&#21644;&#21487;&#35299;&#37322;&#24615;&#30340;&#21021;&#27493;&#25506;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
How Good is ChatGPT at Face Biometrics? A First Look into Recognition, Soft Biometrics, and Explainability. (arXiv:2401.13641v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13641
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21021;&#27493;&#25506;&#32034;&#20102;&#22522;&#20110;GPT-4&#30340;ChatGPT&#22312;&#38754;&#37096;&#29983;&#29289;&#35782;&#21035;&#20013;&#30340;&#34920;&#29616;&#12290;&#30740;&#31350;&#20998;&#26512;&#20102;ChatGPT&#22312;&#38754;&#37096;&#39564;&#35777;&#12289;&#36719;&#29983;&#29289;&#29305;&#24449;&#20272;&#35745;&#21644;&#32467;&#26524;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;ChatGPT&#30340;&#24212;&#29992;&#26377;&#26395;&#25552;&#39640;&#33258;&#21160;&#20915;&#31574;&#22312;&#20154;&#31867;&#22330;&#26223;&#20013;&#30340;&#35299;&#37322;&#24615;&#21644;&#36879;&#26126;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35832;&#22914;OpenAI&#24320;&#21457;&#30340;GPT&#36825;&#26679;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#23637;&#29616;&#20986;&#20196;&#20154;&#24778;&#35766;&#30340;&#32467;&#26524;&#65292;&#20026;&#25105;&#20204;&#30340;&#31038;&#20250;&#24341;&#20837;&#20102;&#24555;&#36895;&#21464;&#38761;&#12290;ChatGPT&#30340;&#21457;&#24067;&#36827;&#19968;&#27493;&#21152;&#24378;&#20102;&#36825;&#19968;&#24433;&#21709;&#65292;&#23427;&#20351;&#20219;&#20309;&#20154;&#37117;&#33021;&#20197;&#31616;&#21333;&#30340;&#23545;&#35805;&#26041;&#24335;&#19982;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20132;&#20114;&#65292;&#19981;&#38656;&#35201;&#20219;&#20309;&#39046;&#22495;&#32463;&#39564;&#12290;&#22240;&#27492;&#65292;ChatGPT&#24050;&#34987;&#36805;&#36895;&#24212;&#29992;&#20110;&#35768;&#22810;&#19981;&#21516;&#30340;&#20219;&#21153;&#65292;&#22914;&#20195;&#30721;&#21644;&#27468;&#26354;&#21019;&#20316;&#12289;&#25945;&#32946;&#12289;&#34394;&#25311;&#21161;&#25163;&#31561;&#65292;&#23637;&#31034;&#20102;&#23545;&#20110;&#26410;&#32463;&#36807;&#35757;&#32451;&#30340;&#20219;&#21153;&#32780;&#35328;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#65288;&#38646;&#26679;&#26412;&#23398;&#20064;&#65289;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#35752;&#22522;&#20110;&#26368;&#26032;&#30340;GPT-4&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#30340;ChatGPT&#22312;&#38754;&#37096;&#29983;&#29289;&#35782;&#21035;&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;ChatGPT&#22312;&#38754;&#37096;&#39564;&#35777;&#12289;&#36719;&#29983;&#29289;&#29305;&#24449;&#20272;&#35745;&#21644;&#32467;&#26524;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;ChatGPT&#23545;&#20110;&#36827;&#19968;&#27493;&#22686;&#21152;&#20154;&#31867;&#22330;&#26223;&#20013;&#33258;&#21160;&#20915;&#31574;&#30340;&#35299;&#37322;&#24615;&#21644;&#36879;&#26126;&#24230;&#38750;&#24120;&#26377;&#20215;&#20540;&#12290;&#23454;&#39564;&#34987;&#36827;&#34892;&#20197;&#35780;&#20272;ChatGPT&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) such as GPT developed by OpenAI, have already shown astonishing results, introducing quick changes in our society. This has been intensified by the release of ChatGPT which allows anyone to interact in a simple conversational way with LLMs, without any experience in the field needed. As a result, ChatGPT has been rapidly applied to many different tasks such as code- and song-writer, education, virtual assistants, etc., showing impressive results for tasks for which it was not trained (zero-shot learning).  The present study aims to explore the ability of ChatGPT, based on the recent GPT-4 multimodal LLM, for the task of face biometrics. In particular, we analyze the ability of ChatGPT to perform tasks such as face verification, soft-biometrics estimation, and explainability of the results. ChatGPT could be very valuable to further increase the explainability and transparency of the automatic decisions in human scenarios. Experiments are carried out in order
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#23545;&#25239;&#35757;&#32451;&#65288;FAT&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#25552;&#39640;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#22312;&#26080;&#21516;&#20041;&#35789;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#21333;&#27493;&#26799;&#24230;&#19978;&#21319;&#22312;&#23884;&#20837;&#31354;&#38388;&#20013;&#29983;&#25104;&#23545;&#25239;&#24615;&#31034;&#20363;&#65292;&#20197;&#21152;&#36895;&#35757;&#32451;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2401.12461</link><description>&lt;p&gt;
&#24555;&#36895;&#23545;&#25239;&#35757;&#32451;&#19982;&#25991;&#26412;&#23545;&#25239;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Fast Adversarial Training against Textual Adversarial Attacks. (arXiv:2401.12461v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12461
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#23545;&#25239;&#35757;&#32451;&#65288;FAT&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#25552;&#39640;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#22312;&#26080;&#21516;&#20041;&#35789;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#21333;&#27493;&#26799;&#24230;&#19978;&#21319;&#22312;&#23884;&#20837;&#31354;&#38388;&#20013;&#29983;&#25104;&#23545;&#25239;&#24615;&#31034;&#20363;&#65292;&#20197;&#21152;&#36895;&#35757;&#32451;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#23545;&#25239;&#24615;&#38450;&#24481;&#26041;&#27861;&#24050;&#34987;&#25552;&#20986;&#65292;&#20197;&#22686;&#24378;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#26041;&#27861;&#24341;&#20837;&#20102;&#39069;&#22806;&#30340;&#39044;&#35774;&#35821;&#35328;&#30693;&#35782;&#65292;&#24182;&#20551;&#35774;&#25915;&#20987;&#32773;&#20351;&#29992;&#30340;&#21516;&#20041;&#35789;&#20505;&#36873;&#35789;&#26159;&#21487;&#35775;&#38382;&#30340;&#65292;&#36825;&#26159;&#19968;&#20010;&#29702;&#24819;&#30340;&#20551;&#35774;&#12290;&#25105;&#20204;&#28145;&#20837;&#30740;&#31350;&#20102;&#23884;&#20837;&#31354;&#38388;&#20013;&#30340;&#23545;&#25239;&#24615;&#35757;&#32451;&#65292;&#24182;&#20174;&#21333;&#27493;&#25200;&#21160;&#29983;&#25104;&#21644;&#25200;&#21160;&#21021;&#22987;&#21270;&#35282;&#24230;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#23545;&#25239;&#35757;&#32451;&#65288;FAT&#65289;&#26041;&#27861;&#65292;&#20197;&#25913;&#21892;&#22312;&#26080;&#21516;&#20041;&#35789;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;&#22522;&#20110;&#21333;&#27493;&#21644;&#22810;&#27493;&#26799;&#24230;&#19978;&#21319;&#29983;&#25104;&#30340;&#23545;&#25239;&#25200;&#21160;&#30456;&#20284;&#30340;&#35266;&#23519;&#65292;FAT&#20351;&#29992;&#21333;&#27493;&#26799;&#24230;&#19978;&#21319;&#22312;&#23884;&#20837;&#31354;&#38388;&#20013;&#29983;&#25104;&#23545;&#25239;&#24615;&#31034;&#20363;&#65292;&#20197;&#21152;&#36895;&#35757;&#32451;&#36807;&#31243;&#12290;&#22522;&#20110;&#36830;&#32493;&#32426;&#20803;&#20013;&#21516;&#19968;&#35757;&#32451;&#26679;&#26412;&#19978;&#29983;&#25104;&#30340;&#25200;&#21160;&#30456;&#20284;&#30340;&#35266;&#23519;&#65292;FAT&#20805;&#20998;&#21033;&#29992;&#21382;&#21490;&#20449;&#24687;&#26469;&#21021;&#22987;&#21270;&#25200;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many adversarial defense methods have been proposed to enhance the adversarial robustness of natural language processing models. However, most of them introduce additional pre-set linguistic knowledge and assume that the synonym candidates used by attackers are accessible, which is an ideal assumption. We delve into adversarial training in the embedding space and propose a Fast Adversarial Training (FAT) method to improve the model robustness in the synonym-unaware scenario from the perspective of single-step perturbation generation and perturbation initialization. Based on the observation that the adversarial perturbations crafted by single-step and multi-step gradient ascent are similar, FAT uses single-step gradient ascent to craft adversarial examples in the embedding space to expedite the training process. Based on the observation that the perturbations generated on the identical training sample in successive epochs are similar, FAT fully utilizes historical information when initi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#25193;&#25955;&#26041;&#31243;&#26694;&#26550;&#65292;&#36890;&#36807;&#24102;&#26377;&#20445;&#30495;&#24230;&#39033;&#30340;&#26041;&#31243;&#65292;&#27491;&#24335;&#24314;&#31435;&#20102;GNN&#19982;&#25193;&#25955;&#36807;&#31243;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#25551;&#36848;&#39640;&#38454;&#37051;&#23621;&#30340;&#26631;&#31614;&#30456;&#20284;&#24615;&#12290;</title><link>http://arxiv.org/abs/2312.08616</link><description>&lt;p&gt;
&#22270;&#19978;&#30340;&#36890;&#29992;&#31070;&#32463;&#25193;&#25955;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Generalized Neural Diffusion Framework on Graphs. (arXiv:2312.08616v2 [cs.SI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.08616
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#25193;&#25955;&#26041;&#31243;&#26694;&#26550;&#65292;&#36890;&#36807;&#24102;&#26377;&#20445;&#30495;&#24230;&#39033;&#30340;&#26041;&#31243;&#65292;&#27491;&#24335;&#24314;&#31435;&#20102;GNN&#19982;&#25193;&#25955;&#36807;&#31243;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#25551;&#36848;&#39640;&#38454;&#37051;&#23621;&#30340;&#26631;&#31614;&#30456;&#20284;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;GNN&#21644;&#25193;&#25955;&#36807;&#31243;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#36825;&#28608;&#21457;&#20102;&#35768;&#22810;&#22522;&#20110;&#25193;&#25955;&#30340;GNN&#30340;&#25552;&#20986;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#36825;&#20004;&#31181;&#26426;&#21046;&#23494;&#20999;&#30456;&#20851;&#65292;&#19968;&#20010;&#22522;&#26412;&#30340;&#38382;&#39064;&#33258;&#28982;&#22320;&#20135;&#29983;&#65306;&#26159;&#21542;&#23384;&#22312;&#19968;&#20010;&#21487;&#20197;&#27491;&#24335;&#32479;&#19968;&#36825;&#20123;GNN&#30340;&#36890;&#29992;&#25193;&#25955;&#26694;&#26550;&#65311;&#36825;&#20010;&#38382;&#39064;&#30340;&#31572;&#26696;&#19981;&#20165;&#21487;&#20197;&#21152;&#28145;&#25105;&#20204;&#23545;GNN&#23398;&#20064;&#36807;&#31243;&#30340;&#29702;&#35299;&#65292;&#32780;&#19988;&#36824;&#21487;&#33021;&#25171;&#24320;&#19968;&#20010;&#35774;&#35745;&#24191;&#27867;&#26032;&#30340;GNN&#31867;&#21035;&#30340;&#26032;&#22823;&#38376;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24102;&#26377;&#20445;&#30495;&#24230;&#39033;&#30340;&#36890;&#29992;&#25193;&#25955;&#26041;&#31243;&#26694;&#26550;&#65292;&#23427;&#27491;&#24335;&#24314;&#31435;&#20102;&#25193;&#25955;&#36807;&#31243;&#19982;&#26356;&#22810;GNN&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#36825;&#20010;&#26694;&#26550;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#22270;&#25193;&#25955;&#32593;&#32476;&#30340;&#19968;&#20010;&#29305;&#24449;&#65292;&#21363;&#24403;&#21069;&#31070;&#32463;&#25193;&#25955;&#36807;&#31243;&#21482;&#23545;&#24212;&#20110;&#19968;&#38454;&#25193;&#25955;&#26041;&#31243;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#39640;&#38454;&#37051;&#23621;&#30340;&#26631;&#31614;&#23454;&#38469;&#19978;&#34920;&#29616;&#20986;&#21333;&#19968;&#24615;&#29305;&#24449;&#65292;&#36825;&#24341;&#21457;&#20102;&#30456;&#20284;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent studies reveal the connection between GNNs and the diffusion process, which motivates many diffusion-based GNNs to be proposed. However, since these two mechanisms are closely related, one fundamental question naturally arises: Is there a general diffusion framework that can formally unify these GNNs? The answer to this question can not only deepen our understanding of the learning process of GNNs, but also may open a new door to design a broad new class of GNNs. In this paper, we propose a general diffusion equation framework with the fidelity term, which formally establishes the relationship between the diffusion process with more GNNs. Meanwhile, with this framework, we identify one characteristic of graph diffusion networks, i.e., the current neural diffusion process only corresponds to the first-order diffusion equation. However, by an experimental investigation, we show that the labels of high-order neighbors actually exhibit monophily property, which induces the similarit
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#37327;&#23376;&#26497;&#22352;&#26631;&#24230;&#37327;&#23398;&#20064;(QPMeL)&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32463;&#20856;&#27169;&#22411;&#23398;&#20064;&#37327;&#23376;&#27604;&#29305;&#30340;&#26497;&#22352;&#26631;&#24418;&#24335;&#30340;&#21442;&#25968;&#65292;&#28982;&#21518;&#20351;&#29992;&#27973;&#23618;PQC&#21644;&#21487;&#35757;&#32451;&#30340;&#38376;&#23618;&#26469;&#21019;&#24314;&#37327;&#23376;&#24577;&#21644;&#23398;&#20064;&#32416;&#32544;&#12290;&#19982;QMeL&#30456;&#27604;&#65292;QPMeL&#20855;&#26377;&#26356;&#39640;&#25928;&#30340;&#35745;&#31639;&#24615;&#33021;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;</title><link>http://arxiv.org/abs/2312.01655</link><description>&lt;p&gt;
&#37327;&#23376;&#26497;&#22352;&#26631;&#24230;&#37327;&#23398;&#20064;: &#39640;&#25928;&#32463;&#20856;&#23398;&#20064;&#30340;&#37327;&#23376;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Quantum Polar Metric Learning: Efficient Classically Learned Quantum Embeddings. (arXiv:2312.01655v2 [quant-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.01655
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#37327;&#23376;&#26497;&#22352;&#26631;&#24230;&#37327;&#23398;&#20064;(QPMeL)&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32463;&#20856;&#27169;&#22411;&#23398;&#20064;&#37327;&#23376;&#27604;&#29305;&#30340;&#26497;&#22352;&#26631;&#24418;&#24335;&#30340;&#21442;&#25968;&#65292;&#28982;&#21518;&#20351;&#29992;&#27973;&#23618;PQC&#21644;&#21487;&#35757;&#32451;&#30340;&#38376;&#23618;&#26469;&#21019;&#24314;&#37327;&#23376;&#24577;&#21644;&#23398;&#20064;&#32416;&#32544;&#12290;&#19982;QMeL&#30456;&#27604;&#65292;QPMeL&#20855;&#26377;&#26356;&#39640;&#25928;&#30340;&#35745;&#31639;&#24615;&#33021;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24230;&#37327;&#23398;&#20064;&#22312;&#32463;&#20856;&#25968;&#25454;&#33539;&#30068;&#20013;&#34920;&#29616;&#20986;&#26497;&#26377;&#28508;&#21147;&#30340;&#32467;&#26524;&#65292;&#21019;&#24314;&#20102;&#20998;&#31163;&#26126;&#26174;&#30340;&#29305;&#24449;&#31354;&#38388;&#12290;&#36825;&#20010;&#24819;&#27861;&#20063;&#34987;&#24212;&#29992;&#21040;&#37327;&#23376;&#35745;&#31639;&#26426;&#20013;&#65292;&#36890;&#36807;&#37327;&#23376;&#24230;&#37327;&#23398;&#20064;(QMeL)&#12290;QMeL&#21253;&#25324;&#20004;&#20010;&#27493;&#39588;&#65292;&#39318;&#20808;&#20351;&#29992;&#32463;&#20856;&#27169;&#22411;&#23558;&#25968;&#25454;&#21387;&#32553;&#20197;&#36866;&#24212;&#26377;&#38480;&#25968;&#37327;&#30340;&#37327;&#23376;&#27604;&#29305;&#65292;&#28982;&#21518;&#20351;&#29992;&#21442;&#25968;&#21270;&#37327;&#23376;&#30005;&#36335;(PQC)&#22312;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20013;&#21019;&#24314;&#26356;&#22909;&#30340;&#20998;&#31163;&#25928;&#26524;&#12290;&#28982;&#32780;&#65292;&#22312;&#22024;&#26434;&#20013;&#38388;&#35268;&#27169;&#37327;&#23376;(NISQ)&#35774;&#22791;&#19978;&#65292;QMeL&#35299;&#20915;&#26041;&#26696;&#23548;&#33268;&#30005;&#36335;&#23485;&#24230;&#21644;&#28145;&#24230;&#36739;&#22823;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#21487;&#25193;&#23637;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#37327;&#23376;&#26497;&#22352;&#26631;&#24230;&#37327;&#23398;&#20064;(QPMeL)&#30340;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;&#32463;&#20856;&#27169;&#22411;&#23398;&#20064;&#19968;&#20010;&#37327;&#23376;&#27604;&#29305;&#30340;&#26497;&#22352;&#26631;&#24418;&#24335;&#30340;&#21442;&#25968;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#20165;&#21253;&#21547;$R_y$&#21644;$R_z$&#38376;&#30340;&#27973;&#23618;PQC&#21019;&#24314;&#37327;&#23376;&#24577;&#65292;&#24182;&#21033;&#29992;&#21487;&#35757;&#32451;&#30340;$ZZ(\theta)$&#38376;&#23618;&#23398;&#20064;&#32416;&#32544;&#12290;&#30005;&#36335;&#36824;&#36890;&#36807;SWAP&#27979;&#35797;&#35745;&#31639;&#20445;&#30495;&#24230;&#65292;&#29992;&#20110;&#25105;&#20204;&#25552;&#20986;&#30340;&#20445;&#30495;&#24230;&#19977;&#20803;&#25439;&#22833;&#20989;&#25968;&#30340;&#35757;&#32451;&#65292;&#29992;&#20110;&#21516;&#26102;&#35757;&#32451;&#32463;&#20856;&#21644;&#37327;&#23376;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep metric learning has recently shown extremely promising results in the classical data domain, creating well-separated feature spaces. This idea was also adapted to quantum computers via Quantum Metric Learning(QMeL). QMeL consists of a 2 step process with a classical model to compress the data to fit into the limited number of qubits, then train a Parameterized Quantum Circuit(PQC) to create better separation in Hilbert Space. However, on Noisy Intermediate Scale Quantum (NISQ) devices. QMeL solutions result in high circuit width and depth, both of which limit scalability. We propose Quantum Polar Metric Learning (QPMeL) that uses a classical model to learn the parameters of the polar form of a qubit. We then utilize a shallow PQC with $R_y$ and $R_z$ gates to create the state and a trainable layer of $ZZ(\theta)$-gates to learn entanglement. The circuit also computes fidelity via a SWAP Test for our proposed Fidelity Triplet Loss function, used to train both classical and quantum 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#26465;&#20214;&#38750;&#32447;&#24615;&#33258;&#21160;&#32534;&#30721;&#22120;(CVAE)&#36827;&#34892;&#36712;&#36857;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;(VAE)&#20013;&#30340;&#38750;&#32447;&#24615;&#37319;&#26679;&#36807;&#31243;&#21644;&#20854;&#20182;&#25913;&#36827;&#65292;&#36229;&#36234;&#20102;&#29616;&#26377;&#25216;&#26415;&#65292;&#20026;&#33258;&#21160;&#39550;&#39542;&#39046;&#22495;&#30340;&#36712;&#36857;&#39044;&#27979;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.19944</link><description>&lt;p&gt;
&#26465;&#20214;&#38750;&#32447;&#24615;&#33258;&#21160;&#32534;&#30721;&#22120;&#29992;&#20110;&#36712;&#36857;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Conditional Unscented Autoencoders for Trajectory Prediction. (arXiv:2310.19944v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19944
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#26465;&#20214;&#38750;&#32447;&#24615;&#33258;&#21160;&#32534;&#30721;&#22120;(CVAE)&#36827;&#34892;&#36712;&#36857;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;(VAE)&#20013;&#30340;&#38750;&#32447;&#24615;&#37319;&#26679;&#36807;&#31243;&#21644;&#20854;&#20182;&#25913;&#36827;&#65292;&#36229;&#36234;&#20102;&#29616;&#26377;&#25216;&#26415;&#65292;&#20026;&#33258;&#21160;&#39550;&#39542;&#39046;&#22495;&#30340;&#36712;&#36857;&#39044;&#27979;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26465;&#20214;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;(CVAE)&#26159;&#33258;&#21160;&#39550;&#39542;&#36712;&#36857;&#39044;&#27979;&#20013;&#26368;&#24120;&#29992;&#30340;&#27169;&#22411;&#20043;&#19968;&#12290;&#23427;&#23558;&#39550;&#39542;&#29615;&#22659;&#21644;&#30495;&#23454;&#26410;&#26469;&#20851;&#31995;&#24314;&#31435;&#22312;&#27010;&#29575;&#38544;&#31354;&#38388;&#20013;&#65292;&#24182;&#21033;&#29992;&#27492;&#31354;&#38388;&#29983;&#25104;&#39044;&#27979;&#12290;&#26412;&#25991;&#23545;CVAE&#30340;&#20851;&#38190;&#32452;&#20214;&#25552;&#20986;&#20102;&#25361;&#25112;&#12290;&#25105;&#20204;&#21033;&#29992;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;(VAE)&#39046;&#22495;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#21457;&#29616;&#21464;&#21270;&#37319;&#26679;&#36807;&#31243;&#21487;&#20197;&#26497;&#22823;&#22320;&#25552;&#39640;&#24615;&#33021;&#12290;&#25105;&#20204;&#21457;&#29616;&#38750;&#32447;&#24615;&#37319;&#26679;&#33021;&#22815;&#26356;&#36866;&#21512;&#20110;&#36712;&#36857;&#39044;&#27979;&#65292;&#32780;&#38543;&#26426;&#37319;&#26679;&#21487;&#33021;&#24102;&#26469;&#28508;&#22312;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#20854;&#20182;&#25913;&#36827;&#65292;&#21253;&#25324;&#26356;&#32467;&#26500;&#21270;&#30340;&#28151;&#21512;&#38544;&#31354;&#38388;&#65292;&#20197;&#21450;&#19968;&#31181;&#26032;&#39062;&#12289;&#21487;&#33021;&#26356;&#20855;&#34920;&#36798;&#21147;&#30340;CVAE&#25512;&#29702;&#26041;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;INTERACTION&#39044;&#27979;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35780;&#20272;&#65292;&#23637;&#31034;&#20102;&#25105;&#20204;&#27169;&#22411;&#30340;&#24191;&#27867;&#36866;&#29992;&#24615;&#65292;&#36229;&#36807;&#20102;&#29616;&#26377;&#25216;&#26415;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
The \ac{CVAE} is one of the most widely-used models in trajectory prediction for \ac{AD}. It captures the interplay between a driving context and its ground-truth future into a probabilistic latent space and uses it to produce predictions. In this paper, we challenge key components of the CVAE. We leverage recent advances in the space of the VAE, the foundation of the CVAE, which show that a simple change in the sampling procedure can greatly benefit performance. We find that unscented sampling, which draws samples from any learned distribution in a deterministic manner, can naturally be better suited to trajectory prediction than potentially dangerous random sampling. We go further and offer additional improvements, including a more structured mixture latent space, as well as a novel, potentially more expressive way to do inference with CVAEs. We show wide applicability of our models by evaluating them on the INTERACTION prediction dataset, outperforming the state of the art, as well 
&lt;/p&gt;</description></item><item><title>NECO&#26159;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#22349;&#22604;&#30340;&#26032;&#39062;&#30340;&#36229;&#20986;&#20998;&#24067;&#26816;&#27979;&#26041;&#27861;&#65292;&#21033;&#29992;&#20960;&#20309;&#23646;&#24615;&#21644;&#20027;&#25104;&#20998;&#31354;&#38388;&#35782;&#21035;OOD&#25968;&#25454;&#65292;&#22312;&#23567;&#35268;&#27169;&#21644;&#22823;&#35268;&#27169;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#24182;&#23637;&#31034;&#20102;&#24378;&#22823;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.06823</link><description>&lt;p&gt;
NECO: &#22522;&#20110;&#31070;&#32463;&#22349;&#22604;&#30340;&#36229;&#20986;&#20998;&#24067;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
NECO: NEural Collapse Based Out-of-distribution detection. (arXiv:2310.06823v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06823
&lt;/p&gt;
&lt;p&gt;
NECO&#26159;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#22349;&#22604;&#30340;&#26032;&#39062;&#30340;&#36229;&#20986;&#20998;&#24067;&#26816;&#27979;&#26041;&#27861;&#65292;&#21033;&#29992;&#20960;&#20309;&#23646;&#24615;&#21644;&#20027;&#25104;&#20998;&#31354;&#38388;&#35782;&#21035;OOD&#25968;&#25454;&#65292;&#22312;&#23567;&#35268;&#27169;&#21644;&#22823;&#35268;&#27169;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#24182;&#23637;&#31034;&#20102;&#24378;&#22823;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#27169;&#22411;&#36807;&#20110;&#33258;&#20449;&#24182;&#19988;&#27809;&#26377;&#24847;&#35782;&#21040;&#20854;&#35748;&#35782;&#35770;&#38480;&#21046;&#65292;&#26816;&#27979;&#36229;&#20986;&#20998;&#24067;&#65288;OOD&#65289;&#25968;&#25454;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#12290;&#25105;&#20204;&#20551;&#35774;&#8220;&#31070;&#32463;&#22349;&#22604;&#8221;&#65292;&#19968;&#31181;&#24433;&#21709;&#36229;&#20986;&#20998;&#24067;&#25968;&#25454;&#30340;&#29616;&#35937;&#65292;&#20063;&#20250;&#24433;&#21709;&#36229;&#20986;&#20998;&#24067;&#25968;&#25454;&#12290;&#20026;&#20102;&#20174;&#36825;&#31181;&#30456;&#20114;&#20316;&#29992;&#20013;&#21463;&#30410;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;NECO&#65292;&#19968;&#31181;&#29992;&#20110;OOD&#26816;&#27979;&#30340;&#26032;&#39062;&#30340;&#20107;&#21518;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#8220;&#31070;&#32463;&#22349;&#22604;&#8221;&#21644;&#20027;&#25104;&#20998;&#31354;&#38388;&#30340;&#20960;&#20309;&#23646;&#24615;&#26469;&#35782;&#21035;OOD&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;NECO&#22312;&#23567;&#35268;&#27169;&#21644;&#22823;&#35268;&#27169;OOD&#26816;&#27979;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#21516;&#26102;&#22312;&#19981;&#21516;&#30340;&#32593;&#32476;&#26550;&#26500;&#19978;&#23637;&#31034;&#20102;&#24378;&#22823;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23545;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;OOD&#26816;&#27979;&#20013;&#30340;&#26377;&#25928;&#24615;&#25552;&#20379;&#20102;&#29702;&#35770;&#35299;&#37322;&#12290;&#25105;&#20204;&#35745;&#21010;&#22312;&#21311;&#21517;&#26399;&#32467;&#26463;&#21518;&#21457;&#24067;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
Detecting out-of-distribution (OOD) data is a critical challenge in machine learning due to model overconfidence, often without awareness of their epistemological limits. We hypothesize that ``neural collapse'', a phenomenon affecting in-distribution data for models trained beyond loss convergence, also influences OOD data. To benefit from this interplay, we introduce NECO, a novel post-hoc method for OOD detection, which leverages the geometric properties of ``neural collapse'' and of principal component spaces to identify OOD data. Our extensive experiments demonstrate that NECO achieves state-of-the-art results on both small and large-scale OOD detection tasks while exhibiting strong generalization capabilities across different network architectures. Furthermore, we provide a theoretical explanation for the effectiveness of our method in OOD detection. We plan to release the code after the anonymity period.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#30740;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#35299;&#37322;&#20854;&#20182;&#39044;&#27979;&#27169;&#22411;&#26377;&#25928;&#24615;&#30340;&#26694;&#26550;&#65292;&#24182;&#19988;&#25552;&#20986;&#20102;&#22810;&#20010;&#25552;&#31034;&#31574;&#30053;&#65292;&#22635;&#34917;&#20102;&#24403;&#21069;&#23545;&#20110;LLMs&#22312;&#35299;&#37322;&#20854;&#20182;&#27169;&#22411;&#34892;&#20026;&#26041;&#38754;&#30340;&#32570;&#22833;&#12290;</title><link>http://arxiv.org/abs/2310.05797</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#20107;&#21518;&#35299;&#37322;&#22120;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Are Large Language Models Post Hoc Explainers?. (arXiv:2310.05797v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05797
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#30740;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#35299;&#37322;&#20854;&#20182;&#39044;&#27979;&#27169;&#22411;&#26377;&#25928;&#24615;&#30340;&#26694;&#26550;&#65292;&#24182;&#19988;&#25552;&#20986;&#20102;&#22810;&#20010;&#25552;&#31034;&#31574;&#30053;&#65292;&#22635;&#34917;&#20102;&#24403;&#21069;&#23545;&#20110;LLMs&#22312;&#35299;&#37322;&#20854;&#20182;&#27169;&#22411;&#34892;&#20026;&#26041;&#38754;&#30340;&#32570;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36234;&#26469;&#36234;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#24212;&#29992;&#20013;&#12290;&#26368;&#36817;&#30340;&#19968;&#39033;&#21019;&#26032;&#65292;&#21363;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#65292;&#20351;&#24471;LLM&#33021;&#22815;&#22312;&#25512;&#29702;&#38454;&#27573;&#36890;&#36807;&#22312;&#25552;&#31034;&#20013;&#25552;&#20379;&#23569;&#37327;&#31034;&#20363;&#26469;&#23398;&#20064;&#26032;&#20219;&#21153;&#65292;&#20174;&#32780;&#28040;&#38500;&#20102;&#27169;&#22411;&#24494;&#35843;&#30340;&#38656;&#35201;&#12290;&#34429;&#28982;LLM&#24050;&#32463;&#34987;&#24212;&#29992;&#20110;&#22810;&#20010;&#39046;&#22495;&#65292;&#20294;&#20854;&#22312;&#35299;&#37322;&#20854;&#20182;&#27169;&#22411;&#34892;&#20026;&#26041;&#38754;&#30340;&#36866;&#29992;&#24615;&#20173;&#30456;&#23545;&#26410;&#34987;&#25506;&#32034;&#12290;&#23613;&#31649;&#23384;&#22312;&#36234;&#26469;&#36234;&#22810;&#30340;&#26032;&#35299;&#37322;&#25216;&#26415;&#65292;&#20294;&#24456;&#22810;&#25216;&#26415;&#35201;&#27714;&#23545;&#27169;&#22411;&#20855;&#26377;&#30333;&#30418;&#35775;&#38382;&#26435;&#38480;&#21644;/&#25110;&#35745;&#31639;&#25104;&#26412;&#36739;&#39640;&#65292;&#20984;&#26174;&#20102;&#19979;&#19968;&#20195;&#20107;&#21518;&#35299;&#37322;&#22120;&#30340;&#38656;&#27714;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#30740;&#31350;LLM&#35299;&#37322;&#20854;&#20182;&#39044;&#27979;&#27169;&#22411;&#26377;&#25928;&#24615;&#30340;&#26694;&#26550;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21253;&#21547;&#22810;&#31181;&#25552;&#31034;&#31574;&#30053;&#30340;&#26032;&#39062;&#26694;&#26550;&#65306;i&#65289;&#22522;&#20110;&#25200;&#21160;&#30340;ICL&#65292;ii&#65289;&#22522;&#20110;&#39044;&#27979;&#30340;ICL&#65292;iii&#65289;&#22522;&#20110;&#25351;&#20196;&#30340;ICL&#65292;&#21644;iv&#65289;&#22522;&#20110;&#35299;&#37322;&#30340;ICL&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) are increasingly used as powerful tools for a plethora of natural language processing (NLP) applications. A recent innovation, in-context learning (ICL), enables LLMs to learn new tasks by supplying a few examples in the prompt during inference time, thereby eliminating the need for model fine-tuning. While LLMs have been utilized in several applications, their applicability in explaining the behavior of other models remains relatively unexplored. Despite the growing number of new explanation techniques, many require white-box access to the model and/or are computationally expensive, highlighting a need for next-generation post hoc explainers. In this work, we present the first framework to study the effectiveness of LLMs in explaining other predictive models. More specifically, we propose a novel framework encompassing multiple prompting strategies: i) Perturbation-based ICL, ii) Prediction-based ICL, iii) Instruction-based ICL, and iv) Explanation-based I
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#21307;&#23398;&#22270;&#20687;&#20013;&#31227;&#21160;&#30446;&#26631;&#20998;&#21106;&#30340;&#22522;&#30784;&#27169;&#22411;iMOS&#65292;&#36890;&#36807;&#23545;&#24207;&#21015;&#20013;&#21482;&#26377;&#23569;&#37327;&#22270;&#20687;&#36827;&#34892;&#27880;&#37322;&#65292;&#21363;&#21487;&#23454;&#29616;&#39640;&#31934;&#24230;&#30340;&#20998;&#21106;&#25928;&#26524;</title><link>http://arxiv.org/abs/2309.17264</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#21307;&#23398;&#22270;&#20687;&#20013;&#19968;&#33324;&#31227;&#21160;&#30446;&#26631;&#20998;&#21106;&#30340;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Foundation Model for General Moving Object Segmentation in Medical Images. (arXiv:2309.17264v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17264
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#21307;&#23398;&#22270;&#20687;&#20013;&#31227;&#21160;&#30446;&#26631;&#20998;&#21106;&#30340;&#22522;&#30784;&#27169;&#22411;iMOS&#65292;&#36890;&#36807;&#23545;&#24207;&#21015;&#20013;&#21482;&#26377;&#23569;&#37327;&#22270;&#20687;&#36827;&#34892;&#27880;&#37322;&#65292;&#21363;&#21487;&#23454;&#29616;&#39640;&#31934;&#24230;&#30340;&#20998;&#21106;&#25928;&#26524;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#26088;&#22312;&#25551;&#32472;&#24863;&#20852;&#36259;&#30340;&#35299;&#21078;&#25110;&#30149;&#29702;&#32467;&#26500;&#65292;&#22312;&#20020;&#24202;&#35786;&#26029;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#26500;&#24314;&#39640;&#31934;&#24230;&#30340;&#28145;&#24230;&#20998;&#21106;&#27169;&#22411;&#38656;&#35201;&#22823;&#37327;&#39640;&#36136;&#37327;&#30340;&#27880;&#37322;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#21307;&#23398;&#27880;&#37322;&#38750;&#24120;&#32321;&#29712;&#32791;&#26102;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#21307;&#23398;&#35270;&#39057;&#25110;3D&#20307;&#31215;&#65292;&#30001;&#20110;&#24040;&#22823;&#30340;&#26631;&#31614;&#31354;&#38388;&#21644;&#24046;&#30340;&#24103;&#38388;&#19968;&#33268;&#24615;&#12290;&#26368;&#36817;&#65292;&#22312;&#33258;&#28982;&#22270;&#20687;&#20013;&#65292;&#19968;&#20010;&#21517;&#20026;Moving Object Segmentation (MOS)&#30340;&#22522;&#26412;&#20219;&#21153;&#22312;&#25216;&#26415;&#19978;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#23427;&#30340;&#30446;&#26631;&#26159;&#22312;&#22270;&#20687;&#24207;&#21015;&#20013;&#20174;&#32972;&#26223;&#20013;&#25551;&#32472;&#31227;&#21160;&#29289;&#20307;&#65292;&#21482;&#38656;&#35201;&#26368;&#23567;&#30340;&#27880;&#37322;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#29992;&#20110;&#21307;&#23398;&#22270;&#20687;&#20013;MOS&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#21517;&#20026;iMOS&#12290;&#23545;&#19968;&#20010;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#21307;&#23398;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#39564;&#35777;&#20102;&#25152;&#25552;&#20986;&#30340;iMOS&#30340;&#26377;&#25928;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#21482;&#38656;&#23545;&#24207;&#21015;&#20013;&#23569;&#37327;&#30340;&#22270;&#20687;&#36827;&#34892;&#27880;&#37322;&#65292;iMOS&#23601;&#21487;&#20197;&#23454;&#29616;&#20102;
&lt;/p&gt;
&lt;p&gt;
Medical image segmentation aims to delineate the anatomical or pathological structures of interest, playing a crucial role in clinical diagnosis. A substantial amount of high-quality annotated data is crucial for constructing high-precision deep segmentation models. However, medical annotation is highly cumbersome and time-consuming, especially for medical videos or 3D volumes, due to the huge labeling space and poor inter-frame consistency. Recently, a fundamental task named Moving Object Segmentation (MOS) has made significant advancements in natural images. Its objective is to delineate moving objects from the background within image sequences, requiring only minimal annotations. In this paper, we propose the first foundation model, named iMOS, for MOS in medical images. Extensive experiments on a large multi-modal medical dataset validate the effectiveness of the proposed iMOS. Specifically, with the annotation of only a small number of images in the sequence, iMOS can achieve sati
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#22270;&#25968;&#25454;&#20013;&#30340;&#24212;&#29992;&#65292;&#21457;&#29616;LLM&#21487;&#20197;&#20174;&#32467;&#26500;&#20449;&#24687;&#20013;&#21463;&#30410;&#65292;&#23588;&#20854;&#26159;&#22312;&#25991;&#26412;&#33410;&#28857;&#29305;&#24449;&#32570;&#20047;&#30340;&#24773;&#20917;&#19979;&#65292;&#32780;LLM&#30340;&#24615;&#33021;&#19982;&#25968;&#25454;&#27844;&#38706;&#27809;&#26377;&#26174;&#33879;&#30456;&#20851;&#12290;</title><link>http://arxiv.org/abs/2309.16595</link><description>&lt;p&gt;
LLM&#33021;&#21542;&#26377;&#25928;&#21033;&#29992;&#32467;&#26500;&#20449;&#24687;&#36827;&#34892;&#22270;&#23398;&#20064;&#65306;&#20309;&#26102;&#20309;&#22320;&#12290;
&lt;/p&gt;
&lt;p&gt;
Can LLMs Effectively Leverage Structural Information for Graph Learning: When and Why. (arXiv:2309.16595v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16595
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#22270;&#25968;&#25454;&#20013;&#30340;&#24212;&#29992;&#65292;&#21457;&#29616;LLM&#21487;&#20197;&#20174;&#32467;&#26500;&#20449;&#24687;&#20013;&#21463;&#30410;&#65292;&#23588;&#20854;&#26159;&#22312;&#25991;&#26412;&#33410;&#28857;&#29305;&#24449;&#32570;&#20047;&#30340;&#24773;&#20917;&#19979;&#65292;&#32780;LLM&#30340;&#24615;&#33021;&#19982;&#25968;&#25454;&#27844;&#38706;&#27809;&#26377;&#26174;&#33879;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#32467;&#26500;&#21270;&#25968;&#25454;&#65288;&#29305;&#21035;&#26159;&#22270;&#25968;&#25454;&#65289;&#19978;&#30340;&#24212;&#29992;&#65292;&#36825;&#26159;LLM&#25991;&#29486;&#20013;&#23578;&#26410;&#20805;&#20998;&#25506;&#32034;&#30340;&#37325;&#35201;&#25968;&#25454;&#24418;&#24577;&#12290;&#25105;&#20204;&#26088;&#22312;&#20102;&#35299;&#22312;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#20013;&#65292;&#20309;&#26102;&#20309;&#22320;&#24341;&#20837;&#22270;&#25968;&#25454;&#20013;&#30340;&#32467;&#26500;&#20449;&#24687;&#21487;&#20197;&#25552;&#39640;LLM&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#8220;&#20309;&#26102;&#8221;&#38382;&#39064;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22810;&#31181;&#32534;&#30721;&#32467;&#26500;&#20449;&#24687;&#30340;&#25552;&#31034;&#26041;&#27861;&#65292;&#35774;&#32622;&#20013;&#25991;&#26412;&#33410;&#28857;&#29305;&#24449;&#20016;&#23500;&#25110;&#31232;&#32570;&#12290;&#23545;&#20110;&#8220;&#20026;&#20160;&#20040;&#8221;&#38382;&#39064;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;LLM&#24615;&#33021;&#30340;&#20004;&#20010;&#28508;&#22312;&#22240;&#32032;&#65306;&#25968;&#25454;&#27844;&#38706;&#21644;&#21516;&#36136;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65306;&#65288;i&#65289;LLM&#21487;&#20197;&#20174;&#32467;&#26500;&#20449;&#24687;&#20013;&#21463;&#30410;&#65292;&#23588;&#20854;&#26159;&#22312;&#25991;&#26412;&#33410;&#28857;&#29305;&#24449;&#32570;&#20047;&#30340;&#24773;&#20917;&#19979;&#65307;&#65288;ii&#65289;&#27809;&#26377;&#23454;&#36136;&#24615;&#30340;&#35777;&#25454;&#34920;&#26126;LLM&#24615;&#33021;&#19982;&#25968;&#25454;&#27844;&#38706;&#26377;&#26174;&#33879;&#30456;&#20851;&#65307;&#65288;iii&#65289;LLM&#22312;&#30446;&#26631;&#33410;&#28857;&#19978;&#30340;&#24615;&#33021;&#19982;&#27491;&#21521;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies Large Language Models (LLMs) for structured data--particularly graphs--a crucial data modality that remains underexplored in the LLM literature. We aim to understand when and why the incorporation of structural information inherent in graph data can improve the prediction performance of LLMs on node classification tasks. To address the ``when'' question, we examine a variety of prompting methods for encoding structural information, in settings where textual node features are either rich or scarce. For the ``why'' questions, we probe into two potential contributing factors to the LLM performance: data leakage and homophily. Our exploration of these questions reveals that (i) LLMs can benefit from structural information, especially when textual node features are scarce; (ii) there is no substantial evidence indicating that the performance of LLMs is significantly attributed to data leakage; and (iii) the performance of LLMs on a target node is strongly positively relat
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;11&#31181;&#25193;&#23637;&#25216;&#26415;&#12289;12&#20010;&#19981;&#21516;&#20998;&#24067;&#21464;&#21270;&#30340;&#25968;&#25454;&#38598;&#21644;24&#20010;&#26816;&#32034;&#27169;&#22411;&#30340;&#20840;&#38754;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#26597;&#35810;&#25110;&#25991;&#26723;&#25193;&#23637;&#30340;&#25928;&#26524;&#19982;&#26816;&#32034;&#22120;&#24615;&#33021;&#30456;&#20851;&#65292;&#23545;&#20110;&#24369;&#27169;&#22411;&#26469;&#35828;&#25193;&#23637;&#25552;&#39640;&#20102;&#20998;&#25968;&#65292;&#20294;&#23545;&#20110;&#24378;&#27169;&#22411;&#26469;&#35828;&#25193;&#23637;&#36890;&#24120;&#20250;&#25439;&#23475;&#20998;&#25968;&#12290;</title><link>http://arxiv.org/abs/2309.08541</link><description>&lt;p&gt;
&#29983;&#25104;&#24335;&#26597;&#35810;&#21644;&#25991;&#26723;&#25193;&#23637;&#20309;&#26102;&#22833;&#36133;&#65311;&#26041;&#27861;&#12289;&#26816;&#32034;&#22120;&#21644;&#25968;&#25454;&#38598;&#30340;&#20840;&#38754;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
When do Generative Query and Document Expansions Fail? A Comprehensive Study Across Methods, Retrievers, and Datasets. (arXiv:2309.08541v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08541
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;11&#31181;&#25193;&#23637;&#25216;&#26415;&#12289;12&#20010;&#19981;&#21516;&#20998;&#24067;&#21464;&#21270;&#30340;&#25968;&#25454;&#38598;&#21644;24&#20010;&#26816;&#32034;&#27169;&#22411;&#30340;&#20840;&#38754;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#26597;&#35810;&#25110;&#25991;&#26723;&#25193;&#23637;&#30340;&#25928;&#26524;&#19982;&#26816;&#32034;&#22120;&#24615;&#33021;&#30456;&#20851;&#65292;&#23545;&#20110;&#24369;&#27169;&#22411;&#26469;&#35828;&#25193;&#23637;&#25552;&#39640;&#20102;&#20998;&#25968;&#65292;&#20294;&#23545;&#20110;&#24378;&#27169;&#22411;&#26469;&#35828;&#25193;&#23637;&#36890;&#24120;&#20250;&#25439;&#23475;&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#36827;&#34892;&#26597;&#35810;&#25110;&#25991;&#26723;&#25193;&#23637;&#21487;&#20197;&#25913;&#21892;&#20449;&#24687;&#26816;&#32034;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23578;&#19981;&#28165;&#26970;&#36825;&#20123;&#25216;&#26415;&#26159;&#21542;&#26222;&#36941;&#26377;&#30410;&#65292;&#36824;&#26159;&#20165;&#22312;&#29305;&#23450;&#35774;&#32622;&#19979;&#26377;&#25928;&#65292;&#20363;&#22914;&#23545;&#20110;&#29305;&#23450;&#30340;&#26816;&#32034;&#27169;&#22411;&#12289;&#25968;&#25454;&#38598;&#39046;&#22495;&#25110;&#26597;&#35810;&#31867;&#22411;&#12290;&#20026;&#20102;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#31532;&#19968;&#27425;&#23545;&#22522;&#20110;LM&#30340;&#25193;&#23637;&#30340;&#20840;&#38754;&#20998;&#26512;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#26816;&#32034;&#22120;&#24615;&#33021;&#19982;&#25193;&#23637;&#30340;&#22686;&#30410;&#20043;&#38388;&#23384;&#22312;&#24378;&#28872;&#30340;&#36127;&#30456;&#20851;&#20851;&#31995;&#65306;&#25193;&#23637;&#25913;&#21892;&#20102;&#36739;&#24369;&#27169;&#22411;&#30340;&#20998;&#25968;&#65292;&#20294;&#36890;&#24120;&#20250;&#25439;&#23475;&#36739;&#24378;&#27169;&#22411;&#30340;&#20998;&#25968;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#19968;&#36235;&#21183;&#22312;11&#31181;&#25193;&#23637;&#25216;&#26415;&#12289;12&#20010;&#20855;&#26377;&#19981;&#21516;&#20998;&#24067;&#21464;&#21270;&#30340;&#25968;&#25454;&#38598;&#21644;24&#20010;&#26816;&#32034;&#27169;&#22411;&#30340;&#19968;&#32452;&#23454;&#39564;&#20013;&#25104;&#31435;&#12290;&#36890;&#36807;&#23450;&#24615;&#38169;&#35823;&#20998;&#26512;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20551;&#35774;&#65292;&#21363;&#23613;&#31649;&#25193;&#23637;&#25552;&#20379;&#20102;&#39069;&#22806;&#30340;&#20449;&#24687;&#65288;&#21487;&#33021;&#25913;&#21892;&#20102;&#21484;&#22238;&#29575;&#65289;&#65292;&#20294;&#23427;&#20204;&#20063;&#22686;&#21152;&#20102;&#22122;&#22768;&#65292;&#20351;&#24471;&#24456;&#38590;&#21306;&#20998;&#20986;&#39030;&#32423;&#30456;&#20851;&#25991;&#26723;&#65288;&#20174;&#32780;&#24341;&#20837;&#20102;&#38169;&#35823;&#30340;&#27491;&#20363;&#65289;
&lt;/p&gt;
&lt;p&gt;
Using large language models (LMs) for query or document expansion can improve generalization in information retrieval. However, it is unknown whether these techniques are universally beneficial or only effective in specific settings, such as for particular retrieval models, dataset domains, or query types. To answer this, we conduct the first comprehensive analysis of LM-based expansion. We find that there exists a strong negative correlation between retriever performance and gains from expansion: expansion improves scores for weaker models, but generally harms stronger models. We show this trend holds across a set of eleven expansion techniques, twelve datasets with diverse distribution shifts, and twenty-four retrieval models. Through qualitative error analysis, we hypothesize that although expansions provide extra information (potentially improving recall), they add additional noise that makes it difficult to discern between the top relevant documents (thus introducing false positiv
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#36830;&#25509;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#36827;&#21270;&#31639;&#27861;&#36827;&#34892;&#25552;&#31034;&#20248;&#21270;&#30340;&#26694;&#26550;&#65292;&#21517;&#20026;EvoPrompt&#12290;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35821;&#35328;&#22788;&#29702;&#33021;&#21147;&#21644;&#36827;&#21270;&#31639;&#27861;&#30340;&#20248;&#21270;&#24615;&#33021;&#65292;EvoPrompt&#21487;&#20197;&#33258;&#21160;&#21270;&#22788;&#29702;&#38656;&#35201;&#36830;&#36143;&#21644;&#21487;&#35835;&#24615;&#33391;&#22909;&#30340;&#25552;&#31034;&#65292;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.08532</link><description>&lt;p&gt;
&#36890;&#36807;&#36827;&#21270;&#31639;&#27861;&#36830;&#25509;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#24378;&#22823;&#30340;&#25552;&#31034;&#20248;&#21270;&#22120;
&lt;/p&gt;
&lt;p&gt;
Connecting Large Language Models with Evolutionary Algorithms Yields Powerful Prompt Optimizers. (arXiv:2309.08532v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08532
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#36830;&#25509;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#36827;&#21270;&#31639;&#27861;&#36827;&#34892;&#25552;&#31034;&#20248;&#21270;&#30340;&#26694;&#26550;&#65292;&#21517;&#20026;EvoPrompt&#12290;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35821;&#35328;&#22788;&#29702;&#33021;&#21147;&#21644;&#36827;&#21270;&#31639;&#27861;&#30340;&#20248;&#21270;&#24615;&#33021;&#65292;EvoPrompt&#21487;&#20197;&#33258;&#21160;&#21270;&#22788;&#29702;&#38656;&#35201;&#36830;&#36143;&#21644;&#21487;&#35835;&#24615;&#33391;&#22909;&#30340;&#25552;&#31034;&#65292;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#23427;&#20204;&#20381;&#36182;&#20110;&#31934;&#24515;&#35774;&#35745;&#30340;&#25552;&#31034;&#65292;&#36825;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#30340;&#20154;&#21147;&#21162;&#21147;&#12290;&#20026;&#20102;&#33258;&#21160;&#21270;&#36825;&#20010;&#36807;&#31243;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31163;&#25955;&#25552;&#31034;&#20248;&#21270;&#26694;&#26550;&#65292;&#31216;&#20026;EvoPrompt&#65292;&#23427;&#20511;&#37492;&#20102;&#36827;&#21270;&#31639;&#27861;&#30340;&#24605;&#24819;&#65292;&#22240;&#20026;&#23427;&#20204;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#21644;&#24555;&#36895;&#30340;&#25910;&#25947;&#24615;&#12290;&#20026;&#20102;&#20351;&#36827;&#21270;&#31639;&#27861;&#33021;&#22815;&#22788;&#29702;&#38656;&#35201;&#36830;&#36143;&#24182;&#19988;&#21487;&#35835;&#24615;&#33391;&#22909;&#30340;&#33258;&#28982;&#35821;&#35328;&#34920;&#36798;&#30340;&#31163;&#25955;&#25552;&#31034;&#65292;&#25105;&#20204;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#36827;&#21270;&#31639;&#27861;&#36827;&#34892;&#20102;&#36830;&#25509;&#12290;&#36825;&#31181;&#26041;&#27861;&#20351;&#25105;&#20204;&#21487;&#20197;&#21516;&#26102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24378;&#22823;&#35821;&#35328;&#22788;&#29702;&#33021;&#21147;&#21644;&#36827;&#21270;&#31639;&#27861;&#30340;&#39640;&#25928;&#20248;&#21270;&#24615;&#33021;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;EvoPrompt&#22312;&#19981;&#20351;&#29992;&#20219;&#20309;&#26799;&#24230;&#25110;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#65292;&#20174;&#19968;&#32452;&#25552;&#31034;&#20013;&#24320;&#22987;&#65292;&#24182;&#22522;&#20110;&#36827;&#21270;&#31639;&#23376;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#26032;&#30340;&#25552;&#31034;&#65292;&#26681;&#25454;&#24320;&#21457;&#38598;&#25913;&#36827;&#25552;&#31034;&#30340;&#31181;&#32676;&#12290;&#25105;&#20204;&#23545;&#38381;&#28304;&#21644;&#24320;&#28304;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#21253;&#25324;GPT-3&#36827;&#34892;&#25552;&#31034;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) excel in various tasks, but they rely on carefully crafted prompts that often demand substantial human effort. To automate this process, in this paper, we propose a novel framework for discrete prompt optimization, called EvoPrompt, which borrows the idea of evolutionary algorithms (EAs) as they exhibit good performance and fast convergence. To enable EAs to work on discrete prompts, which are natural language expressions that need to be coherent and human-readable, we connect LLMs with EAs. This approach allows us to simultaneously leverage the powerful language processing capabilities of LLMs and the efficient optimization performance of EAs. Specifically, abstaining from any gradients or parameters, EvoPrompt starts from a population of prompts and iteratively generates new prompts with LLMs based on the evolutionary operators, improving the population based on the development set. We optimize prompts for both closed- and open-source LLMs including GPT-3
&lt;/p&gt;</description></item><item><title>FedSoL&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#37030;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26088;&#22312;&#35299;&#20915;&#25968;&#25454;&#20998;&#24067;&#19981;&#22343;&#21248;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;&#23427;&#36890;&#36807;&#24179;&#34913;&#20840;&#23616;&#23545;&#40784;&#21644;&#26412;&#22320;&#19968;&#33324;&#24615;&#26469;&#25913;&#21892;FL&#30340;&#23398;&#20064;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.12532</link><description>&lt;p&gt;
FedSoL: &#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#35299;&#20915;&#20840;&#23616;&#23545;&#40784;&#21644;&#26412;&#22320;&#19968;&#33324;&#24615;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
FedSoL: Bridging Global Alignment and Local Generality in Federated Learning. (arXiv:2308.12532v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12532
&lt;/p&gt;
&lt;p&gt;
FedSoL&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#37030;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26088;&#22312;&#35299;&#20915;&#25968;&#25454;&#20998;&#24067;&#19981;&#22343;&#21248;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;&#23427;&#36890;&#36807;&#24179;&#34913;&#20840;&#23616;&#23545;&#40784;&#21644;&#26412;&#22320;&#19968;&#33324;&#24615;&#26469;&#25913;&#21892;FL&#30340;&#23398;&#20064;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;(Federated Learning, FL)&#36890;&#36807;&#32858;&#21512;&#26469;&#33258;&#20010;&#20307;&#23458;&#25143;&#31471;&#30340;&#26412;&#22320;&#35757;&#32451;&#27169;&#22411;&#26469;&#26500;&#24314;&#20840;&#23616;&#27169;&#22411;&#12290;&#34429;&#28982;FL&#21487;&#20197;&#22312;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#27169;&#22411;&#65292;&#20294;&#24403;&#23458;&#25143;&#31471;&#25968;&#25454;&#20998;&#24067;&#19981;&#22343;&#21248;&#26102;&#65292;&#24120;&#24120;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#12290;&#35768;&#22810;&#20808;&#21069;&#30340;FL&#31639;&#27861;&#36890;&#36807;&#24341;&#20837;&#21508;&#31181;&#36817;&#20284;&#32422;&#26463;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#36825;&#20123;&#32422;&#26463;&#26088;&#22312;&#36890;&#36807;&#38480;&#21046;&#23616;&#37096;&#23398;&#20064;&#19982;&#20840;&#23616;&#30446;&#26631;&#30340;&#20559;&#31163;&#26469;&#20419;&#36827;&#20840;&#23616;&#23545;&#40784;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#26412;&#36136;&#19978;&#36890;&#36807;&#24178;&#25200;&#21407;&#22987;&#30340;&#23616;&#37096;&#30446;&#26631;&#32780;&#38480;&#21046;&#20102;&#23616;&#37096;&#23398;&#20064;&#12290;&#26368;&#36817;&#65292;&#20986;&#29616;&#20102;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#26469;&#25913;&#21892;&#26412;&#22320;&#23398;&#20064;&#30340;&#19968;&#33324;&#24615;&#12290;&#36890;&#36807;&#22312;&#24179;&#28369;&#30340;&#25439;&#22833;&#31354;&#38388;&#20013;&#33719;&#24471;&#26412;&#22320;&#27169;&#22411;&#65292;&#36825;&#31181;&#26041;&#27861;&#20943;&#36731;&#20102;&#23458;&#25143;&#31471;&#19981;&#21516;&#26412;&#22320;&#30446;&#26631;&#20043;&#38388;&#30340;&#20914;&#31361;&#12290;&#28982;&#32780;&#65292;&#23427;&#19981;&#33021;&#30830;&#20445;&#31283;&#23450;&#30340;&#20840;&#23616;&#23545;&#40784;&#65292;&#22240;&#20026;&#26412;&#22320;&#23398;&#20064;&#19981;&#32771;&#34385;&#20840;&#23616;&#30446;&#26631;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#32852;&#37030;&#23398;&#20064;&#30340;&#31283;&#23450;&#24615;(FedSoL)&#26041;&#27861;&#26469;&#22312;FL&#20013;&#35299;&#20915;&#20840;&#23616;&#23545;&#40784;&#21644;&#26412;&#22320;&#19968;&#33324;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) aggregates locally trained models from individual clients to construct a global model. While FL enables learning a model with data privacy, it often suffers from significant performance degradation when client data distributions are heterogeneous. Many previous FL algorithms have addressed this issue by introducing various proximal restrictions. These restrictions aim to encourage global alignment by constraining the deviation of local learning from the global objective. However, they inherently limit local learning by interfering with the original local objectives. Recently, an alternative approach has emerged to improve local learning generality. By obtaining local models within a smooth loss landscape, this approach mitigates conflicts among different local objectives of the clients. Yet, it does not ensure stable global alignment, as local learning does not take the global objective into account. In this study, we propose Federated Stability on Learning (Fed
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FlexPredict&#30340;&#38543;&#26426;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#27169;&#22411;&#20013;&#21152;&#20837;&#20301;&#32622;&#19981;&#30830;&#23450;&#24615;&#65292;&#20197;&#39044;&#27979;&#25513;&#30422;&#30340;&#26631;&#35760;&#20301;&#32622;&#65292;&#20174;&#32780;&#25913;&#21892;&#20102;&#25513;&#30422;&#22270;&#20687;&#24314;&#27169;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.00566</link><description>&lt;p&gt;
&#22312;&#38543;&#26426;&#20301;&#32622;&#39044;&#27979;&#25513;&#30422;&#30340;&#26631;&#35760;&#25913;&#21892;&#20102;&#25513;&#30422;&#22270;&#20687;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Predicting masked tokens in stochastic locations improves masked image modeling. (arXiv:2308.00566v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00566
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FlexPredict&#30340;&#38543;&#26426;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#27169;&#22411;&#20013;&#21152;&#20837;&#20301;&#32622;&#19981;&#30830;&#23450;&#24615;&#65292;&#20197;&#39044;&#27979;&#25513;&#30422;&#30340;&#26631;&#35760;&#20301;&#32622;&#65292;&#20174;&#32780;&#25913;&#21892;&#20102;&#25513;&#30422;&#22270;&#20687;&#24314;&#27169;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#26159;&#28145;&#24230;&#23398;&#20064;&#20013;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#33539;&#24335;&#65292;&#36890;&#36807;&#26500;&#24314;&#38656;&#35201;&#23398;&#20064;&#26377;&#29992;&#34920;&#31034;&#30340;&#39044;&#35757;&#32451;&#20219;&#21153;&#65292;&#21487;&#20197;&#20174;&#26080;&#26631;&#31614;&#25968;&#25454;&#20013;&#36827;&#34892;&#23398;&#20064;&#12290;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#65292;&#20027;&#35201;&#30340;&#39044;&#35757;&#32451;&#20219;&#21153;&#26159;&#25513;&#30422;&#35821;&#35328;&#24314;&#27169;&#65288;MLM&#65289;&#65292;&#32780;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#23384;&#22312;&#30456;&#24212;&#30340;&#25513;&#30422;&#22270;&#20687;&#24314;&#27169;&#65288;MIM&#65289;&#12290;&#28982;&#32780;&#65292;MIM&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#23427;&#38656;&#35201;&#22312;&#20934;&#30830;&#20301;&#32622;&#19978;&#39044;&#27979;&#35821;&#20041;&#20869;&#23481;&#12290;&#20363;&#22914;&#65292;&#32473;&#23450;&#19968;&#24352;&#19981;&#23436;&#25972;&#30340;&#29399;&#30340;&#22270;&#29255;&#65292;&#25105;&#20204;&#21487;&#20197;&#29468;&#27979;&#26377;&#19968;&#20010;&#23614;&#24052;&#65292;&#20294;&#25105;&#20204;&#26080;&#27861;&#30830;&#23450;&#23427;&#30340;&#30830;&#20999;&#20301;&#32622;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FlexPredict&#65292;&#36825;&#26159;&#19968;&#20010;&#32771;&#34385;&#20301;&#32622;&#19981;&#30830;&#23450;&#24615;&#30340;&#38543;&#26426;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#27169;&#22411;&#26465;&#20214;&#21270;&#21040;&#38543;&#26426;&#25513;&#30422;&#30340;&#26631;&#35760;&#20301;&#32622;&#19978;&#65292;&#24341;&#23548;&#27169;&#22411;&#23398;&#20064;&#26356;&#21152;&#40065;&#26834;&#23545;&#20301;&#32622;&#19981;&#30830;&#23450;&#24615;&#30340;&#29305;&#24449;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25913;&#36827;&#20102;&#22810;&#20010;&#20219;&#21153;&#30340;&#19979;&#28216;&#24615;&#33021;&#65292;&#20363;&#22914;&#19982;MIM&#22522;&#20934;&#30456;&#27604;&#65292;FlexPredict&#22312;&#19968;&#31995;&#21015;&#20219;&#21153;&#19978;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning is a promising paradigm in deep learning that enables learning from unlabeled data by constructing pretext tasks that require learning useful representations. In natural language processing, the dominant pretext task has been masked language modeling (MLM), while in computer vision there exists an equivalent called Masked Image Modeling (MIM). However, MIM is challenging because it requires predicting semantic content in accurate locations. E.g, given an incomplete picture of a dog, we can guess that there is a tail, but we cannot determine its exact location. In this work, we propose FlexPredict, a stochastic model that addresses this challenge by incorporating location uncertainty into the model. Specifically, we condition the model on stochastic masked token positions to guide the model toward learning features that are more robust to location uncertainties. Our approach improves downstream performance on a range of tasks, e.g, compared to MIM baselines, Fle
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#22312;&#26631;&#31614;&#31232;&#32570;&#30340;Learning-To-Rank&#38382;&#39064;&#20013;&#65292;&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#30340;&#28145;&#24230;&#27169;&#22411;&#26159;&#21542;&#33021;&#32988;&#36807;GBDTs&#21644;&#20854;&#20182;&#38750;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#20351;&#29992;SimCLR-Rank&#26041;&#27861;&#36827;&#34892;&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#65292;&#25105;&#20204;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#22823;&#37327;&#26080;&#26631;&#31614;&#25968;&#25454;&#21644;&#26377;&#38480;&#26631;&#31614;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#21462;&#24471;&#20102;&#26174;&#33879;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2308.00177</link><description>&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#28145;&#24230;&#27169;&#22411;&#22312;&#26631;&#31614;&#31232;&#32570;&#30340;Learning-To-Rank&#20013;&#32988;&#36807;GBDTs
&lt;/p&gt;
&lt;p&gt;
Pretrained deep models outperform GBDTs in Learning-To-Rank under label scarcity. (arXiv:2308.00177v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00177
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#22312;&#26631;&#31614;&#31232;&#32570;&#30340;Learning-To-Rank&#38382;&#39064;&#20013;&#65292;&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#30340;&#28145;&#24230;&#27169;&#22411;&#26159;&#21542;&#33021;&#32988;&#36807;GBDTs&#21644;&#20854;&#20182;&#38750;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#20351;&#29992;SimCLR-Rank&#26041;&#27861;&#36827;&#34892;&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#65292;&#25105;&#20204;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#22823;&#37327;&#26080;&#26631;&#31614;&#25968;&#25454;&#21644;&#26377;&#38480;&#26631;&#31614;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#21462;&#24471;&#20102;&#26174;&#33879;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#25991;&#26412;&#21644;&#22270;&#20687;&#39046;&#22495;&#26159;&#26368;&#20808;&#36827;&#30340;&#65292;&#20294;&#23427;&#20204;&#22312;&#34920;&#26684;&#24418;&#24335;&#30340;Learning-To-Rank&#38382;&#39064;&#19978;&#23578;&#26410;&#19968;&#33268;&#22320;&#32988;&#36807;&#26799;&#24230;&#25552;&#21319;&#20915;&#31574;&#26641;(GBDTs)&#12290;&#36817;&#26399;&#22312;&#25991;&#26412;&#21644;&#22270;&#20687;&#20219;&#21153;&#19978;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21462;&#24471;&#30340;&#24615;&#33021;&#25552;&#21319;&#20027;&#35201;&#20381;&#36182;&#20110;&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#65292;&#36825;&#31181;&#26041;&#27861;&#21033;&#29992;&#20102;&#27604;&#26377;&#26631;&#31614;&#25968;&#25454;&#22810;&#20960;&#20010;&#25968;&#37327;&#32423;&#30340;&#26080;&#26631;&#31614;&#25968;&#25454;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#36824;&#26410;&#24212;&#29992;&#20110;Learning-To-Rank&#38382;&#39064;&#65292;&#32780;&#35813;&#38382;&#39064;&#36890;&#24120;&#20135;&#29983;&#22823;&#37327;&#26080;&#26631;&#31614;&#25968;&#25454;&#12290;&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#26159;&#21542;&#33021;&#25552;&#39640;LTR&#24615;&#33021;&#65292;&#19982;GBDTs&#21644;&#20854;&#20182;&#38750;&#39044;&#35757;&#32451;&#27169;&#22411;&#30456;&#27604;&#12290;&#36890;&#36807;&#20351;&#29992;&#31616;&#21333;&#30340;&#35774;&#35745;&#36873;&#25321;(&#21253;&#25324;SimCLR-Rank&#65292;&#36825;&#26159;&#25105;&#20204;&#38024;&#23545;&#25490;&#21517;&#38382;&#39064;&#20462;&#25913;&#30340;SimCLR&#26041;&#27861;)&#65292;&#25105;&#20204;&#20135;&#29983;&#20102;&#39044;&#35757;&#32451;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#22312;&#26377;&#22823;&#37327;&#26080;&#26631;&#31614;&#25968;&#25454;&#19988;&#26377;&#38480;&#26631;&#31614;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#26174;&#33879;&#20248;&#20110;GBDTs(&#21644;&#20854;&#20182;&#38750;&#39044;&#35757;&#32451;&#27169;&#22411;)&#12290;
&lt;/p&gt;
&lt;p&gt;
While deep learning (DL) models are state-of-the-art in text and image domains, they have not yet consistently outperformed Gradient Boosted Decision Trees (GBDTs) on tabular Learning-To-Rank (LTR) problems. Most of the recent performance gains attained by DL models in text and image tasks have used unsupervised pretraining, which exploits orders of magnitude more unlabeled data than labeled data. To the best of our knowledge, unsupervised pretraining has not been applied to the LTR problem, which often produces vast amounts of unlabeled data. In this work, we study whether unsupervised pretraining can improve LTR performance over GBDTs and other non-pretrained models. Using simple design choices--including SimCLR-Rank, our ranking-specific modification of SimCLR (an unsupervised pretraining method for images)--we produce pretrained deep learning models that soundly outperform GBDTs (and other non-pretrained models) in the case where labeled data is vastly outnumbered by unlabeled data
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#27169;&#22411;&#21152;&#36895;Benders&#20998;&#35299;&#26041;&#27861;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#30456;&#23545;&#20110;&#20854;&#20182;&#21152;&#36895;&#26041;&#26696;&#30340;30%&#26356;&#24555;&#30340;&#24179;&#22343;&#25910;&#25947;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2307.08816</link><description>&lt;p&gt;
&#21152;&#36895;Benders&#20998;&#35299;&#26041;&#27861;&#30340;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#27169;&#22411;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards Accelerating Benders Decomposition via Reinforcement Learning Surrogate Models. (arXiv:2307.08816v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08816
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#27169;&#22411;&#21152;&#36895;Benders&#20998;&#35299;&#26041;&#27861;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#30456;&#23545;&#20110;&#20854;&#20182;&#21152;&#36895;&#26041;&#26696;&#30340;30%&#26356;&#24555;&#30340;&#24179;&#22343;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#20248;&#21270;&#35797;&#22270;&#22312;&#23384;&#22312;&#19981;&#30830;&#23450;&#24615;&#30340;&#24773;&#20917;&#19979;&#25552;&#20379;&#26368;&#20248;&#20915;&#31574;&#12290;&#36890;&#24120;&#65292;&#30001;&#20110;&#38656;&#35201;&#25429;&#25417;&#19981;&#30830;&#23450;&#24615;&#30340;&#24773;&#26223;&#25968;&#37327;&#20197;&#21450;&#29616;&#23454;&#35268;&#21010;&#38382;&#39064;&#30340;&#31163;&#25955;&#24615;&#36136;&#65292;&#36825;&#20123;&#38382;&#39064;&#30340;&#32463;&#20856;&#24418;&#24335;&#21464;&#24471;&#38590;&#20197;&#22788;&#29702;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#21487;&#34892;&#24615;&#38382;&#39064;&#65292;&#23454;&#36341;&#32773;&#20204;&#36716;&#21521;&#20998;&#35299;&#26041;&#27861;&#65292;&#23558;&#38382;&#39064;&#20998;&#35299;&#20026;&#26356;&#23567;&#12289;&#26356;&#26131;&#22788;&#29702;&#30340;&#23376;&#38382;&#39064;&#12290;&#26412;&#25991;&#30340;&#20027;&#35201;&#20998;&#35299;&#26041;&#27861;&#26159;Benders&#20998;&#35299;&#65288;BD&#65289;&#65292;&#23427;&#26681;&#25454;&#24773;&#26223;&#29420;&#31435;&#24615;&#23545;&#38543;&#26426;&#20248;&#21270;&#38382;&#39064;&#36827;&#34892;&#20998;&#35299;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20195;&#29702;&#27169;&#22411;&#21152;&#36895;BD&#30340;&#26041;&#27861;&#65292;&#35813;&#20195;&#29702;&#27169;&#22411;&#21462;&#20195;&#20102;NP&#38590;&#30340;&#25972;&#25968;&#20027;&#38382;&#39064;&#12290;&#36890;&#36807;&#21152;&#36895;&#26041;&#27861;&#65292;&#19982;&#20854;&#20182;&#21152;&#36895;&#30340;BD&#23454;&#29616;&#30456;&#27604;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#24179;&#22343;&#25910;&#25947;&#36895;&#24230;&#25552;&#39640;&#20102;30%&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#20316;&#20026;&#26367;&#20195;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#23427;&#26469;&#35299;&#20915;&#38543;&#26426;&#24211;&#23384;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stochastic optimization (SO) attempts to offer optimal decisions in the presence of uncertainty. Often, the classical formulation of these problems becomes intractable due to (a) the number of scenarios required to capture the uncertainty and (b) the discrete nature of real-world planning problems. To overcome these tractability issues, practitioners turn to decomposition methods that divide the problem into smaller, more tractable sub-problems. The focal decomposition method of this paper is Benders decomposition (BD), which decomposes stochastic optimization problems on the basis of scenario independence. In this paper we propose a method of accelerating BD with the aid of a surrogate model in place of an NP-hard integer master problem. Through the acceleration method we observe 30% faster average convergence when compared to other accelerated BD implementations. We introduce a reinforcement learning agent as a surrogate and demonstrate how it can be used to solve a stochastic invent
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Preference Ranking Optimization (PRO)&#26041;&#27861;&#65292;&#36890;&#36807;&#25193;&#23637;&#24067;&#25289;&#24503;&#21033;-&#29305;&#37324;&#27604;&#36739;&#65292;&#37319;&#29992;&#20559;&#22909;&#25490;&#24207;&#30340;&#26041;&#24335;&#26469;&#30452;&#25509;&#23545;&#40784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#35299;&#20915;&#20102;&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;&#30340;&#22797;&#26434;&#24615;&#12289;&#19981;&#31283;&#23450;&#24615;&#21644;&#23545;&#36229;&#21442;&#25968;&#30340;&#25935;&#24863;&#24615;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.17492</link><description>&lt;p&gt;
&#20154;&#31867;&#23545;&#40784;&#30340;&#20559;&#22909;&#25490;&#24207;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Preference Ranking Optimization for Human Alignment. (arXiv:2306.17492v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17492
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Preference Ranking Optimization (PRO)&#26041;&#27861;&#65292;&#36890;&#36807;&#25193;&#23637;&#24067;&#25289;&#24503;&#21033;-&#29305;&#37324;&#27604;&#36739;&#65292;&#37319;&#29992;&#20559;&#22909;&#25490;&#24207;&#30340;&#26041;&#24335;&#26469;&#30452;&#25509;&#23545;&#40784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#35299;&#20915;&#20102;&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;&#30340;&#22797;&#26434;&#24615;&#12289;&#19981;&#31283;&#23450;&#24615;&#21644;&#23545;&#36229;&#21442;&#25968;&#30340;&#25935;&#24863;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#32463;&#24120;&#21253;&#21547;&#35823;&#23548;&#24615;&#20869;&#23481;&#65292;&#24378;&#35843;&#20102;&#23558;&#20854;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#23545;&#40784;&#20197;&#30830;&#20445;&#23433;&#20840;&#30340;AI&#31995;&#32479;&#30340;&#24517;&#35201;&#24615;&#12290;&#37319;&#29992;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#26469;&#23454;&#29616;&#36825;&#31181;&#23545;&#40784;&#65292;&#36890;&#36807;&#23558;&#22522;&#20110;&#24067;&#25289;&#24503;&#21033;-&#29305;&#37324;&#37197;&#23545;&#27604;&#36739;&#30340;&#22870;&#21169;&#27169;&#22411;&#19982;Proximal Policy Optimization&#65288;PPO&#65289;&#31561;RL&#31639;&#27861;&#32467;&#21512;&#36215;&#26469;&#26469;&#20248;&#21270;LLM&#30340;&#21709;&#24212;&#12290;&#28982;&#32780;&#65292;RLHF&#34920;&#29616;&#20986;&#22797;&#26434;&#24615;&#12289;&#19981;&#31283;&#23450;&#24615;&#21644;&#23545;&#36229;&#21442;&#25968;&#30340;&#25935;&#24863;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Preference Ranking Optimization&#65288;PRO&#65289;&#20316;&#20026;PPO&#30340;&#21478;&#19968;&#31181;&#30452;&#25509;&#23558;LLM&#19982;&#24067;&#25289;&#24503;&#21033;-&#29305;&#37324;&#27604;&#36739;&#23545;&#40784;&#30340;&#26041;&#27861;&#12290;PRO&#23558;&#37197;&#23545;&#30340;&#24067;&#25289;&#24503;&#21033;-&#29305;&#37324;&#27604;&#36739;&#25193;&#23637;&#21040;&#36866;&#24212;&#20219;&#24847;&#38271;&#24230;&#30340;&#20559;&#22909;&#25490;&#24207;&#12290;&#36890;&#36807;&#21453;&#22797;&#23545;&#27604;&#29983;&#25104;&#21709;&#24212;&#30340;&#21487;&#33021;&#24615;&#65292;PRO&#25351;&#23548;LLM&#20248;&#20808;&#32771;&#34385;&#26368;&#20339;&#21709;&#24212;&#65292;&#24182;&#36880;&#28176;&#23545;&#21097;&#20313;&#30340;&#21709;&#24212;&#36827;&#34892;&#25490;&#24207;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;PRO&#23558;&#20154;&#31867;&#23545;&#40784;&#26377;&#25928;&#22320;&#36716;&#21270;&#20026;&#27010;&#29575;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) often contain misleading content, emphasizing the need to align them with human values to ensure secur AI systems. Reinforcement learning from human feedback (RLHF) has been employed to achieve this alignment by combining a reward model, typically based on Bradley-Terry paired comparison, with an RL algorithm such as Proximal Policy Optimization (PPO) to optimize LLM responses. However, RLHF exhibits complexity, instability, and sensitivity to hyperparameters. In this paper, we propose Preference Ranking Optimization (PRO) as an alternative to PPO for directly aligning LLMs with the Bradley-Terry comparison. PRO extends the pairwise Bradley-Terry comparison to accommodate preference rankings of any length. By iteratively contrasting the likelihood of generating responses, PRO instructs the LLM to prioritize the best response while progressively ranking the remaining responses. In this manner, PRO effectively transforms human alignment into aligning the prob
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#23545;GPT&#27169;&#22411;&#36827;&#34892;&#20840;&#38754;&#21487;&#20449;&#24230;&#35780;&#20272;&#65292;&#32771;&#34385;&#20102;&#22810;&#20010;&#26041;&#38754;&#30340;&#39118;&#38505;&#65292;&#21457;&#29616;&#20102;&#20197;&#21069;&#26410;&#20844;&#24320;&#30340;&#23041;&#32961;&#28431;&#27934;&#65292;&#20363;&#22914;&#23545;&#27602;&#24615;&#36755;&#20986;&#21644;&#20010;&#20154;&#20449;&#24687;&#27844;&#28431;&#30340;&#26131;&#34987;&#35823;&#23548;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.11698</link><description>&lt;p&gt;
DecodingTrust: GPT&#27169;&#22411;&#30340;&#20840;&#38754;&#21487;&#20449;&#24230;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
DecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT Models. (arXiv:2306.11698v4 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11698
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#23545;GPT&#27169;&#22411;&#36827;&#34892;&#20840;&#38754;&#21487;&#20449;&#24230;&#35780;&#20272;&#65292;&#32771;&#34385;&#20102;&#22810;&#20010;&#26041;&#38754;&#30340;&#39118;&#38505;&#65292;&#21457;&#29616;&#20102;&#20197;&#21069;&#26410;&#20844;&#24320;&#30340;&#23041;&#32961;&#28431;&#27934;&#65292;&#20363;&#22914;&#23545;&#27602;&#24615;&#36755;&#20986;&#21644;&#20010;&#20154;&#20449;&#24687;&#27844;&#28431;&#30340;&#26131;&#34987;&#35823;&#23548;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#39044;&#35757;&#32451;&#21464;&#21387;&#22120;&#65288;GPT&#65289;&#27169;&#22411;&#22312;&#20854;&#33021;&#21147;&#26041;&#38754;&#21462;&#24471;&#20102;&#20196;&#20154;&#20852;&#22859;&#30340;&#36827;&#23637;&#65292;&#24341;&#36215;&#20102;&#20174;&#20174;&#19994;&#32773;&#21040;&#20844;&#20247;&#30340;&#20852;&#36259;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#20851;&#20110;GPT&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;&#30340;&#25991;&#29486;&#20173;&#28982;&#26377;&#38480;&#65292;&#20174;&#19994;&#32773;&#20204;&#25552;&#35758;&#23558;&#24378;&#22823;&#30340;GPT&#27169;&#22411;&#29992;&#20110;&#25935;&#24863;&#24212;&#29992;&#65292;&#22914;&#21307;&#30103;&#20445;&#20581;&#21644;&#37329;&#34701;&#39046;&#22495;&#65292;&#20854;&#20013;&#38169;&#35823;&#21487;&#33021;&#20195;&#20215;&#39640;&#26114;&#12290;&#20026;&#27492;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#37325;&#28857;&#25918;&#22312;GPT-4&#21644;GPT-3.5&#19978;&#65289;&#36827;&#34892;&#20840;&#38754;&#30340;&#21487;&#20449;&#24230;&#35780;&#20272;&#65292;&#32771;&#34385;&#20102;&#22810;&#26679;&#30340;&#35266;&#28857; - &#21253;&#25324;&#26377;&#27602;&#24615;&#12289;&#38472;&#35268;&#20559;&#35265;&#12289;&#23545;&#25239;&#24378;&#24230;&#12289;&#36229;&#20986;&#20998;&#24067;&#30340;&#24378;&#24230;&#12289;&#23545;&#25239;&#31034;&#33539;&#30340;&#24378;&#24230;&#12289;&#38544;&#31169;&#12289;&#26426;&#22120;&#20262;&#29702;&#21644;&#20844;&#24179;&#24615;&#12290;&#26681;&#25454;&#25105;&#20204;&#30340;&#35780;&#20272;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#20197;&#21069;&#26410;&#20844;&#24320;&#30340;&#21487;&#20449;&#24230;&#23041;&#32961;&#28431;&#27934;&#12290;&#20363;&#22914;&#65292;&#25105;&#20204;&#21457;&#29616;GPT&#27169;&#22411;&#21487;&#20197;&#36731;&#26494;&#34987;&#35823;&#23548;&#29983;&#25104;&#26377;&#27602;&#21644;&#20559;&#35265;&#30340;&#36755;&#20986;&#65292;&#24182;&#22312;&#35757;&#32451;&#25968;&#25454;&#21644;&#19978;&#19979;&#25991;&#20013;&#27844;&#28431;&#31169;&#20154;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative Pre-trained Transformer (GPT) models have exhibited exciting progress in their capabilities, capturing the interest of practitioners and the public alike. Yet, while the literature on the trustworthiness of GPT models remains limited, practitioners have proposed employing capable GPT models for sensitive applications such as healthcare and finance -- where mistakes can be costly. To this end, this work proposes a comprehensive trustworthiness evaluation for large language models with a focus on GPT-4 and GPT-3.5, considering diverse perspectives -- including toxicity, stereotype bias, adversarial robustness, out-of-distribution robustness, robustness on adversarial demonstrations, privacy, machine ethics, and fairness. Based on our evaluations, we discover previously unpublished vulnerabilities to trustworthiness threats. For instance, we find that GPT models can be easily misled to generate toxic and biased outputs and leak private information in both training data and conv
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30417;&#30563;&#24335;&#27880;&#24847;&#21147;&#22810;&#23454;&#20363;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#33258;&#21160;&#20998;&#26512;&#36229;&#22768;&#22270;&#20687;&#65292;&#23454;&#29616;AS&#30340;&#31934;&#30830;&#31579;&#26597;&#19988;&#31934;&#24230;&#20248;&#20110;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.00003</link><description>&lt;p&gt;
&#36890;&#36807;&#30417;&#30563;&#24335;&#27880;&#24847;&#21147;&#22810;&#23454;&#20363;&#23398;&#20064;&#20174;&#22810;&#35270;&#35282;&#36229;&#22768;&#22270;&#20687;&#26816;&#27979;&#24515;&#33039;&#30149;
&lt;/p&gt;
&lt;p&gt;
Detecting Heart Disease from Multi-View Ultrasound Images via Supervised Attention Multiple Instance Learning. (arXiv:2306.00003v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00003
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30417;&#30563;&#24335;&#27880;&#24847;&#21147;&#22810;&#23454;&#20363;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#33258;&#21160;&#20998;&#26512;&#36229;&#22768;&#22270;&#20687;&#65292;&#23454;&#29616;AS&#30340;&#31934;&#30830;&#31579;&#26597;&#19988;&#31934;&#24230;&#20248;&#20110;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#21160;&#33033;&#29923;&#29421;&#31364;(AS)&#26159;&#19968;&#31181;&#23548;&#33268;&#20005;&#37325;&#21457;&#30149;&#29575;&#21644;&#27515;&#20129;&#29575;&#30340;&#36864;&#34892;&#24615;&#29923;&#33180;&#30142;&#30149;&#12290;&#36825;&#31181;&#24773;&#20917;&#32463;&#24120;&#34987;&#20302;&#20272;&#21644;&#20302;&#27835;&#30103;&#12290;&#22312;&#20020;&#24202;&#23454;&#36341;&#20013;&#65292;AS&#26159;&#36890;&#36807;&#36229;&#22768;&#24515;&#21160;&#22270;&#30340;&#19987;&#23478;&#23457;&#26597;&#26469;&#35786;&#26029;&#30340;&#65292;&#36825;&#20250;&#20135;&#29983;&#25968;&#21313;&#20010;&#19979;&#32954;&#37319;&#26679;&#30340;&#36229;&#22768;&#22270;&#20687;&#12290;&#21482;&#26377;&#19968;&#20123;&#35270;&#22270;&#26174;&#31034;&#20027;&#21160;&#33033;&#29923;&#12290;&#20026;&#20102;&#33258;&#21160;&#21270;&#31579;&#26597;AS&#65292;&#28145;&#24230;&#32593;&#32476;&#24517;&#39035;&#23398;&#20064;&#27169;&#20223;&#20154;&#31867;&#19987;&#23478;&#35782;&#21035;&#20027;&#21160;&#33033;&#29923;&#35270;&#22270;&#30340;&#33021;&#21147;&#65292;&#28982;&#21518;&#27719;&#24635;&#36825;&#20123;&#30456;&#20851;&#22270;&#20687;&#20197;&#20135;&#29983;&#30740;&#31350;&#32423;&#35786;&#26029;&#12290;&#25105;&#20204;&#21457;&#29616;&#20808;&#21069;&#30340;AS&#26816;&#27979;&#26041;&#27861;&#30001;&#20110;&#20381;&#36182;&#20110;&#36328;&#22270;&#20687;&#30340;&#19981;&#28789;&#27963;&#24179;&#22343;&#20540;&#32780;&#23548;&#33268;&#31934;&#24230;&#19981;&#36275;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#21457;&#29616;&#65292;&#29616;&#25104;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#22810;&#23454;&#20363;(MIL)&#23398;&#20064;&#34920;&#29616;&#19981;&#20339;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31471;&#21040;&#31471;MIL&#26041;&#27861;&#65292;&#21253;&#21547;&#20004;&#20010;&#20851;&#38190;&#26041;&#27861;&#21019;&#26032;&#12290;&#39318;&#20808;&#65292;&#36890;&#36807;&#30417;&#30563;&#24335;&#27880;&#24847;&#25216;&#26415;&#65292;&#24341;&#23548;&#23398;&#20064;&#30340;&#27880;&#24847;&#26426;&#21046;&#20559;&#29233;&#30456;&#20851;&#35270;&#22270;&#12290;&#20854;&#27425;&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#25552;&#39640;&#20102;&#27599;&#20010;&#21333;&#29420;&#22270;&#20687;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19968;&#20010;&#30495;&#23454;&#30340;&#20020;&#24202;&#25968;&#25454;&#38598;&#65288;4569&#21517;&#24739;&#32773;&#65289;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#22312;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#20043;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
Aortic stenosis (AS) is a degenerative valve condition that causes substantial morbidity and mortality. This condition is under-diagnosed and under-treated. In clinical practice, AS is diagnosed with expert review of transthoracic echocardiography, which produces dozens of ultrasound images of the heart. Only some of these views show the aortic valve. To automate screening for AS, deep networks must learn to mimic a human expert's ability to identify views of the aortic valve then aggregate across these relevant images to produce a study-level diagnosis. We find previous approaches to AS detection yield insufficient accuracy due to relying on inflexible averages across images. We further find that off-the-shelf attention-based multiple instance learning (MIL) performs poorly. We contribute a new end-to-end MIL approach with two key methodological innovations. First, a supervised attention technique guides the learned attention mechanism to favor relevant views. Second, a novel self-sup
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#26080;&#27861;&#22788;&#29702;&#30340;&#32467;&#26500;&#21270;&#22823;&#31163;&#25955;&#21160;&#20316;&#31354;&#38388;&#65288;SLDAS&#65289;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21160;&#24577;&#37051;&#22495;&#26500;&#24314;&#65288;DNC&#65289;&#30340;&#26032;&#22411;&#21033;&#29992;&#31574;&#30053;&#65292;&#36890;&#36807;&#21487;&#25193;&#23637;&#30340;&#37051;&#22495;&#25506;&#32034;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#39640;&#25928;&#22320;&#25506;&#32034;&#36830;&#32493;&#20195;&#29702;&#21160;&#20316;&#21608;&#22260;&#30340;&#31163;&#25955;&#37051;&#22495;&#12290;</title><link>http://arxiv.org/abs/2305.19891</link><description>&lt;p&gt;
&#32467;&#26500;&#21270;&#22823;&#31163;&#25955;&#21160;&#20316;&#31354;&#38388;&#30340;&#21160;&#24577;&#37051;&#22495;&#26500;&#24314;
&lt;/p&gt;
&lt;p&gt;
Dynamic Neighborhood Construction for Structured Large Discrete Action Spaces. (arXiv:2305.19891v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19891
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#26080;&#27861;&#22788;&#29702;&#30340;&#32467;&#26500;&#21270;&#22823;&#31163;&#25955;&#21160;&#20316;&#31354;&#38388;&#65288;SLDAS&#65289;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21160;&#24577;&#37051;&#22495;&#26500;&#24314;&#65288;DNC&#65289;&#30340;&#26032;&#22411;&#21033;&#29992;&#31574;&#30053;&#65292;&#36890;&#36807;&#21487;&#25193;&#23637;&#30340;&#37051;&#22495;&#25506;&#32034;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#39640;&#25928;&#22320;&#25506;&#32034;&#36830;&#32493;&#20195;&#29702;&#21160;&#20316;&#21608;&#22260;&#30340;&#31163;&#25955;&#37051;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#31163;&#25955;&#21160;&#20316;&#31354;&#38388;&#65288;LDAS&#65289;&#26159;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#26680;&#24515;&#25361;&#25112;&#12290;&#29616;&#26377;&#30340;&#35299;&#20915;&#26041;&#26696;&#21487;&#20197;&#22788;&#29702;&#22810;&#36798;&#20960;&#30334;&#19975;&#20010;&#21160;&#20316;&#30340;&#38750;&#32467;&#26500;&#21270;LDAS&#12290;&#28982;&#32780;&#65292;&#22312;&#29289;&#27969;&#12289;&#29983;&#20135;&#21644;&#36816;&#36755;&#31995;&#32479;&#31561;&#35768;&#22810;&#29616;&#23454;&#24212;&#29992;&#20013;&#65292;&#21160;&#20316;&#31354;&#38388;&#20855;&#26377;&#32452;&#21512;&#32467;&#26500;&#65292;&#20854;&#35268;&#27169;&#29978;&#33267;&#22312;&#23567;&#35268;&#27169;&#23454;&#20363;&#19978;&#20063;&#36229;&#36807;&#20102;&#25968;&#30334;&#19975;&#20010;&#21160;&#20316;&#12290;&#24184;&#36816;&#30340;&#26159;&#65292;&#36825;&#26679;&#30340;&#21160;&#20316;&#31354;&#38388;&#21576;&#29616;&#20986;&#19968;&#23450;&#30340;&#32467;&#26500;&#65292;&#20363;&#22914;&#31561;&#38388;&#36317;&#30340;&#31163;&#25955;&#36164;&#28304;&#21333;&#20301;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#22788;&#29702;&#24403;&#21069;&#22522;&#20934;&#27979;&#35797;&#26080;&#27861;&#22788;&#29702;&#30340;&#32467;&#26500;&#21270;LDAS&#65288;SLDAS&#65289;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21160;&#24577;&#37051;&#22495;&#26500;&#24314;&#65288;DNC&#65289;&#30340;&#26032;&#22411;&#21033;&#29992;&#31574;&#30053;&#65292;&#29992;&#20110;SLDAS&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#37051;&#22495;&#25506;&#32034;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#21033;&#29992;&#36825;&#31181;&#31574;&#30053;&#65292;&#22312;&#20855;&#26377;&#39640;&#36798;$10^{73}$&#20010;&#21160;&#20316;&#30340;&#32467;&#26500;&#21270;&#21160;&#20316;&#31354;&#38388;&#20013;&#39640;&#25928;&#22320;&#25506;&#32034;&#36830;&#32493;&#20195;&#29702;&#21160;&#20316;&#21608;&#22260;&#30340;&#31163;&#25955;&#37051;&#22495;&#12290;&#25105;&#20204;&#36890;&#36807;&#19982;&#19977;&#20010;&#26631;&#26438;&#31639;&#27861;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#26469;&#23637;&#31034;&#25105;&#20204;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large discrete action spaces (LDAS) remain a central challenge in reinforcement learning. Existing solution approaches can handle unstructured LDAS with up to a few million actions. However, many real-world applications in logistics, production, and transportation systems have combinatorial action spaces, whose size grows well beyond millions of actions, even on small instances. Fortunately, such action spaces exhibit structure, e.g., equally spaced discrete resource units. With this work, we focus on handling structured LDAS (SLDAS) with sizes that cannot be handled by current benchmarks: we propose Dynamic Neighborhood Construction (DNC), a novel exploitation paradigm for SLDAS. We present a scalable neighborhood exploration heuristic that utilizes this paradigm and efficiently explores the discrete neighborhood around the continuous proxy action in structured action spaces with up to $10^{73}$ actions. We demonstrate the performance of our method by benchmarking it against three sta
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;SUFO&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22686;&#24378;&#24494;&#35843;&#30340;&#21464;&#21387;&#22120;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#35813;&#26694;&#26550;&#21033;&#29992;&#22810;&#31181;&#20998;&#26512;&#21644;&#21487;&#35270;&#21270;&#25216;&#26415;&#65292;&#35299;&#20915;&#20102;&#27169;&#22411;&#20449;&#20219;&#21644;&#21487;&#35299;&#37322;&#24615;&#30340;&#20851;&#38190;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#26696;&#20363;&#30740;&#31350;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.17588</link><description>&lt;p&gt;
&#35786;&#26029;&#21464;&#21387;&#22120;&#65306;&#25581;&#31034;&#20020;&#24202;&#20915;&#31574;&#20013;&#30340;&#29305;&#24449;&#31354;&#38388;&#12290; (arXiv:2305.17588v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
Diagnosing Transformers: Illuminating Feature Spaces for Clinical Decision-Making. (arXiv:2305.17588v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17588
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;SUFO&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22686;&#24378;&#24494;&#35843;&#30340;&#21464;&#21387;&#22120;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#35813;&#26694;&#26550;&#21033;&#29992;&#22810;&#31181;&#20998;&#26512;&#21644;&#21487;&#35270;&#21270;&#25216;&#26415;&#65292;&#35299;&#20915;&#20102;&#27169;&#22411;&#20449;&#20219;&#21644;&#21487;&#35299;&#37322;&#24615;&#30340;&#20851;&#38190;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#26696;&#20363;&#30740;&#31350;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21307;&#23398;&#31561;&#39640;&#39118;&#38505;&#39046;&#22495;&#65292;&#20026;&#20102;&#24314;&#31435;&#20449;&#20219;&#21644;&#30830;&#20445;&#23433;&#20840;&#65292;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#33267;&#20851;&#37325;&#35201;&#65292;&#32780;&#20351;&#29992;&#26377;&#38480;&#30340;&#20020;&#24202;&#35760;&#24405;&#23545;&#39044;&#35757;&#32451;&#30340;&#21464;&#21387;&#22120;&#36827;&#34892;&#24494;&#35843;&#20197;&#36741;&#21161;&#20020;&#24202;&#20915;&#31574;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;SUFO&#30340;&#31995;&#32479;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#22686;&#24378;&#20102;&#24494;&#35843;&#30340;&#21464;&#21387;&#22120;&#29305;&#24449;&#31354;&#38388;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;SUFO&#21033;&#29992;&#19968;&#31995;&#21015;&#20998;&#26512;&#21644;&#21487;&#35270;&#21270;&#25216;&#26415;&#65292;&#21253;&#25324;&#30417;&#30563;&#25506;&#32034;&#12289;&#26080;&#30417;&#30563;&#30456;&#20284;&#24615;&#20998;&#26512;&#12289;&#29305;&#24449;&#21160;&#24577;&#21644;&#24322;&#24120;&#20540;&#20998;&#26512;&#65292;&#26469;&#35299;&#20915;&#20851;&#20110;&#27169;&#22411;&#20449;&#20219;&#21644;&#21487;&#35299;&#37322;&#24615;&#30340;&#20851;&#38190;&#38382;&#39064;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#20010;&#26696;&#20363;&#30740;&#31350;&#65292;&#30740;&#31350;&#20102;&#39044;&#35757;&#32451;&#25968;&#25454;&#23545;&#30495;&#23454;&#19990;&#30028;&#30149;&#29702;&#20998;&#31867;&#20219;&#21153;&#30340;&#24433;&#21709;&#65292;&#24182;&#22312;MedNLI&#19978;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#21457;&#29616;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#20116;&#20010;110M&#35268;&#27169;&#30340;&#39044;&#35757;&#32451;&#21464;&#21387;&#22120;&#27169;&#22411;&#65292;&#20998;&#20026;&#36890;&#29992;&#39046;&#22495;&#65288;BERT, TNLR&#65289;&#12289;&#28151;&#21512;&#39046;&#22495;&#65288;BioBERT, Clinical BioBERT&#65289;&#21644;&#39046;&#22495;&#29305;&#23450;&#65288;PubMedBERT&#65289;&#32452;&#12290;&#25105;&#20204;&#30340;SUFO&#20998;&#26512;&#25581;&#31034;&#20102;&#65306;(1)
&lt;/p&gt;
&lt;p&gt;
Pre-trained transformers are often fine-tuned to aid clinical decision-making using limited clinical notes. Model interpretability is crucial, especially in high-stakes domains like medicine, to establish trust and ensure safety, which requires human engagement. We introduce SUFO, a systematic framework that enhances interpretability of fine-tuned transformer feature spaces. SUFO utilizes a range of analytic and visualization techniques, including Supervised probing, Unsupervised similarity analysis, Feature dynamics, and Outlier analysis to address key questions about model trust and interpretability. We conduct a case study investigating the impact of pre-training data where we focus on real-world pathology classification tasks, and validate our findings on MedNLI. We evaluate five 110M-sized pre-trained transformer models, categorized into general-domain (BERT, TNLR), mixed-domain (BioBERT, Clinical BioBERT), and domain-specific (PubMedBERT) groups. Our SUFO analyses reveal that: (1
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#38024;&#23545;&#28145;&#24230;&#23398;&#20064;&#27979;&#35797;&#30340;&#20845;&#20010;&#20855;&#20307;&#25351;&#26631;&#26469;&#26816;&#27979;&#21644;&#32531;&#35299;&#20854;&#28431;&#27934;&#65292;&#35748;&#20026;&#35768;&#22810;DNN&#27979;&#35797;&#20219;&#21153;&#24212;&#35813;&#26159;&#23450;&#21521;&#27979;&#35797;&#38382;&#39064;&#32780;&#19981;&#26159;&#36890;&#29992;&#27979;&#35797;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2305.15698</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#28145;&#24230;&#23398;&#20064;&#27979;&#35797;&#20013;&#30340;&#22810;&#26679;&#24615;
&lt;/p&gt;
&lt;p&gt;
Rethink Diversity in Deep Learning Testing. (arXiv:2305.15698v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15698
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#38024;&#23545;&#28145;&#24230;&#23398;&#20064;&#27979;&#35797;&#30340;&#20845;&#20010;&#20855;&#20307;&#25351;&#26631;&#26469;&#26816;&#27979;&#21644;&#32531;&#35299;&#20854;&#28431;&#27934;&#65292;&#35748;&#20026;&#35768;&#22810;DNN&#27979;&#35797;&#20219;&#21153;&#24212;&#35813;&#26159;&#23450;&#21521;&#27979;&#35797;&#38382;&#39064;&#32780;&#19981;&#26159;&#36890;&#29992;&#27979;&#35797;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#23637;&#29616;&#20102;&#38750;&#20961;&#30340;&#33021;&#21147;&#65292;&#26159;&#29616;&#20195;&#36719;&#20214;&#31995;&#32479;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20063;&#38754;&#20020;&#30528;&#21508;&#31181;&#28431;&#27934;&#65292;&#22914;&#23545;&#25239;&#25915;&#20987;&#21644;&#19981;&#20844;&#24179;&#24615;&#12290;&#22240;&#27492;&#65292;&#27979;&#35797;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#31995;&#32479;&#26159;&#19968;&#39033;&#37325;&#35201;&#20219;&#21153;&#65292;&#20197;&#26816;&#27979;&#21644;&#20943;&#36731;&#36825;&#20123;&#28431;&#27934;&#12290;&#21463;&#20256;&#32479;&#36719;&#20214;&#27979;&#35797;&#25104;&#21151;&#30340;&#21551;&#21457;&#65292;&#25552;&#20986;&#20102;&#21508;&#31181;DNN&#30340;&#22810;&#26679;&#24615;&#24230;&#37327;&#65292;&#20197;&#24110;&#21161;&#26377;&#25928;&#22320;&#26292;&#38706;DNNs&#30340;buggy&#34892;&#20026;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35748;&#20026;&#35768;&#22810;DNN&#27979;&#35797;&#20219;&#21153;&#24212;&#35813;&#34987;&#35270;&#20026;&#23450;&#21521;&#27979;&#35797;&#38382;&#39064;&#32780;&#19981;&#26159;&#36890;&#29992;&#27979;&#35797;&#20219;&#21153;&#65292;&#22240;&#20026;&#36825;&#20123;&#20219;&#21153;&#26159;&#20855;&#20307;&#21644;&#26126;&#30830;&#23450;&#20041;&#30340;&#12290;&#22240;&#27492;&#65292;&#22522;&#20110;&#22810;&#26679;&#24615;&#30340;&#26041;&#27861;&#25928;&#26524;&#36739;&#24046;&#12290;&#36981;&#24490;&#25105;&#20204;&#30340;&#27979;&#35797;&#30446;&#26631;&#21644;DNN&#35821;&#20041;&#30340;&#35770;&#35777;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;6&#20010;&#25351;&#26631;&#65292;&#21487;&#20197;&#29992;&#20110;DNN&#27979;&#35797;&#65292;&#24182;&#20180;&#32454;&#20998;&#26512;&#23427;&#20204;&#30340;&#24212;&#29992;&#33539;&#22260;&#12290;&#25105;&#20204;&#36890;&#36807;&#32463;&#39564;&#24615;&#32467;&#26524;&#26174;&#31034;&#23427;&#20204;&#30340;&#24212;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks (DNNs) have demonstrated extraordinary capabilities and are an integral part of modern software systems. However, they also suffer from various vulnerabilities such as adversarial attacks and unfairness. Testing deep learning (DL) systems is therefore an important task, to detect and mitigate those vulnerabilities. Motivated by the success of traditional software testing, which often employs diversity heuristics, various diversity measures on DNNs have been proposed to help efficiently expose the buggy behavior of DNNs. In this work, we argue that many DNN testing tasks should be treated as directed testing problems rather than general-purpose testing tasks, because these tasks are specific and well-defined. Hence, the diversity-based approach is less effective.  Following our argument based on the semantics of DNNs and the testing goal, we derive $6$ metrics that can be used for DNN testing and carefully analyze their application scopes. We empirically show their 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#36973;&#36935;&#30693;&#35782;&#20914;&#31361;&#26102;&#30340;&#34892;&#20026;&#12290;&#32467;&#26524;&#21457;&#29616;&#65292;LLMs&#21487;&#20197;&#39640;&#24230;&#25509;&#21463;&#22806;&#37096;&#36830;&#36143;&#19988;&#26377;&#35828;&#26381;&#21147;&#30340;&#35777;&#25454;&#65292;&#21363;&#20351;&#19982;&#20854;&#21442;&#25968;&#21270;&#20869;&#23384;&#23384;&#22312;&#20914;&#31361;&#65292;&#20294;&#20063;&#21487;&#33021;&#26377;&#23616;&#38480;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.13300</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#30693;&#35782;&#20914;&#31361;&#20013;&#30340;&#34892;&#20026;&#25581;&#31192;&#65306;&#33258;&#36866;&#24212;&#21464;&#33394;&#40857;&#36824;&#26159;&#22266;&#25191;&#30340;&#26641;&#29549;
&lt;/p&gt;
&lt;p&gt;
Adaptive Chameleon or Stubborn Sloth: Unraveling the Behavior of Large Language Models in Knowledge Clashes. (arXiv:2305.13300v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13300
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#36973;&#36935;&#30693;&#35782;&#20914;&#31361;&#26102;&#30340;&#34892;&#20026;&#12290;&#32467;&#26524;&#21457;&#29616;&#65292;LLMs&#21487;&#20197;&#39640;&#24230;&#25509;&#21463;&#22806;&#37096;&#36830;&#36143;&#19988;&#26377;&#35828;&#26381;&#21147;&#30340;&#35777;&#25454;&#65292;&#21363;&#20351;&#19982;&#20854;&#21442;&#25968;&#21270;&#20869;&#23384;&#23384;&#22312;&#20914;&#31361;&#65292;&#20294;&#20063;&#21487;&#33021;&#26377;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21521;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25552;&#20379;&#22806;&#37096;&#20449;&#24687;&#65292;&#24037;&#20855;&#22686;&#24378;&#65288;&#21253;&#25324;&#26816;&#32034;&#22686;&#24378;&#65289;&#24050;&#25104;&#20026;&#35299;&#20915;LLMs&#38745;&#24577;&#21442;&#25968;&#21270;&#20869;&#23384;&#38480;&#21046;&#30340;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#24403;&#36825;&#20123;&#35777;&#25454;&#19982;&#23427;&#20204;&#30340;&#21442;&#25968;&#21270;&#20869;&#23384;&#21457;&#29983;&#20914;&#31361;&#26102;&#65292;LLMs&#23545;&#36825;&#20123;&#22806;&#37096;&#35777;&#25454;&#26377;&#22810;&#23569;&#25509;&#21463;&#33021;&#21147;&#65311;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31995;&#32479;&#24615;&#30340;&#26694;&#26550;&#26469;&#20174;LLMs&#20013;&#33719;&#21462;&#39640;&#36136;&#37327;&#30340;&#21442;&#25968;&#21270;&#20869;&#23384;&#65292;&#24182;&#26500;&#24314;&#30456;&#24212;&#30340;&#23545;&#31435;&#20869;&#23384;&#65292;&#20174;&#32780;&#20351;&#25105;&#20204;&#33021;&#22815;&#36827;&#34892;&#19968;&#31995;&#21015;&#21463;&#25511;&#23454;&#39564;&#12290;&#25105;&#20204;&#30340;&#35843;&#26597;&#25581;&#31034;&#20102;LLMs&#34920;&#29616;&#20986;&#30475;&#20284;&#30683;&#30462;&#30340;&#34892;&#20026;&#12290;&#19968;&#26041;&#38754;&#65292;&#19982;&#20197;&#24448;&#30340;&#35266;&#24565;&#19981;&#21516;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;&#21482;&#35201;&#22806;&#37096;&#35777;&#25454;&#26159;&#36830;&#36143;&#19988;&#26377;&#35828;&#26381;&#21147;&#30340;&#65292;LLMs&#21363;&#20351;&#19982;&#20854;&#21442;&#25968;&#21270;&#20869;&#23384;&#23384;&#22312;&#20914;&#31361;&#20063;&#21487;&#20197;&#39640;&#24230;&#25509;&#21463;&#22806;&#37096;&#35777;&#25454;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;LLMs&#20063;&#21487;&#33021;&#20250;&#34920;&#29616;&#20986;&#23616;&#38480;&#24615;&#65292;&#23588;&#20854;&#26159;&#24403;&#20854;&#21442;&#25968;&#21270;&#20869;&#23384;&#21463;&#21040;&#23041;&#32961;&#26102;&#12290;
&lt;/p&gt;
&lt;p&gt;
By providing external information to large language models (LLMs), tool augmentation (including retrieval augmentation) has emerged as a promising solution for addressing the limitations of LLMs' static parametric memory. However, how receptive are LLMs to such external evidence, especially when the evidence conflicts with their parametric memory? We present the first comprehensive and controlled investigation into the behavior of LLMs when encountering knowledge conflicts. We propose a systematic framework to elicit high-quality parametric memory from LLMs and construct the corresponding counter-memory, which enables us to conduct a series of controlled experiments. Our investigation reveals seemingly contradicting behaviors of LLMs. On the one hand, different from prior wisdom, we find that LLMs can be highly receptive to external evidence even when that conflicts with their parametric memory, given that the external evidence is coherent and convincing. On the other hand, LLMs also d
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#36923;&#36753;&#39537;&#21160;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#26469;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36923;&#36753;&#25512;&#29702;&#33021;&#21147;&#12290;&#36890;&#36807;&#23558;&#21407;&#22987;&#25991;&#26412;&#36716;&#25442;&#20026;&#25277;&#35937;&#24847;&#20041;&#34920;&#36848;&#22270;&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#36923;&#36753;&#20462;&#25913;&#21644;&#36716;&#25442;&#65292;&#29983;&#25104;&#22686;&#24378;&#25968;&#25454;&#65292;&#20174;&#32780;&#25552;&#21319;&#27169;&#22411;&#24615;&#33021;&#12290;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#21508;&#31181;&#20307;&#31995;&#32467;&#26500;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.12599</link><description>&lt;p&gt;
&#36890;&#36807;&#36923;&#36753;&#39537;&#21160;&#30340;&#25968;&#25454;&#22686;&#24378;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36923;&#36753;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Enhancing Logical Reasoning of Large Language Models through Logic-Driven Data Augmentation. (arXiv:2305.12599v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12599
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#36923;&#36753;&#39537;&#21160;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#26469;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36923;&#36753;&#25512;&#29702;&#33021;&#21147;&#12290;&#36890;&#36807;&#23558;&#21407;&#22987;&#25991;&#26412;&#36716;&#25442;&#20026;&#25277;&#35937;&#24847;&#20041;&#34920;&#36848;&#22270;&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#36923;&#36753;&#20462;&#25913;&#21644;&#36716;&#25442;&#65292;&#29983;&#25104;&#22686;&#24378;&#25968;&#25454;&#65292;&#20174;&#32780;&#25552;&#21319;&#27169;&#22411;&#24615;&#33021;&#12290;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#21508;&#31181;&#20307;&#31995;&#32467;&#26500;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#36923;&#36753;&#25512;&#29702;&#30456;&#32467;&#21512;&#21487;&#20197;&#22686;&#24378;&#23427;&#20204;&#22312;&#38382;&#39064;&#35299;&#20915;&#20013;&#30340;&#33021;&#21147;&#65292;&#20351;&#20854;&#26356;&#21152;&#24378;&#22823;&#21644;&#21487;&#38752;&#12290;&#28982;&#32780;&#65292;&#36923;&#36753;&#25512;&#29702;&#30340;&#22797;&#26434;&#24615;&#20351;&#24471;&#20174;&#32593;&#39029;&#19978;&#25910;&#38598;&#21487;&#38752;&#30340;&#25968;&#25454;&#26469;&#24314;&#31435;&#20840;&#38754;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#38754;&#20020;&#22256;&#38590;&#65292;&#36827;&#32780;&#24433;&#21709;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36923;&#36753;&#39537;&#21160;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;AMR-LDA&#12290;AMR-LDA&#23558;&#21407;&#22987;&#25991;&#26412;&#36716;&#25442;&#25104;&#25277;&#35937;&#24847;&#20041;&#34920;&#31034;&#65288;AMR&#65289;&#22270;&#65292;&#36825;&#26159;&#19968;&#31181;&#32467;&#26500;&#21270;&#30340;&#35821;&#20041;&#34920;&#31034;&#65292;&#21253;&#21547;&#20102;&#21477;&#23376;&#30340;&#36923;&#36753;&#32467;&#26500;&#65292;&#28982;&#21518;&#23545;&#35813;&#22270;&#36827;&#34892;&#25805;&#20316;&#20197;&#29983;&#25104;&#36923;&#36753;&#20462;&#25913;&#21518;&#30340;AMR&#22270;&#12290;&#20462;&#25913;&#21518;&#30340;AMR&#22270;&#38543;&#21518;&#34987;&#36716;&#25442;&#22238;&#25991;&#26412;&#65292;&#20174;&#32780;&#21019;&#24314;&#22686;&#24378;&#25968;&#25454;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#20307;&#31995;&#32467;&#26500;&#26080;&#20851;&#65292;&#24182;&#36890;&#36807;&#25552;&#31034;&#22686;&#24378;&#26469;&#22686;&#24378;&#29983;&#25104;&#22411;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;GPT-3.5&#21644;GPT-4&#65289;&#65292;&#24182;&#36890;&#36807;&#24494;&#35843;&#26469;&#22686;&#24378;&#21028;&#21035;&#22411;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Combining large language models with logical reasoning enhance their capacity to address problems in a robust and reliable manner. Nevertheless, the intricate nature of logical reasoning poses challenges to gathering reliable data from web for building comprehensive training datasets, subsequently affecting the performance on downstream tasks. To address this, we introduce a novel logic-driven data augmentation approach, AMR-LDA. AMR-LDA converts the original text into an Abstract Meaning Representation (AMR) graph, a structured semantic representation that encapsulates the logic structure of the sentence, upon which operations are performed to generate logically modified AMR graphs. The modified AMR graphs are subsequently converted back into texts to create augmented data. Notably, our methodology is architecture-agnostic and enhances generative large language models, such as GPT-3.5 and GPT-4, through prompt augmentation, and fine-tuning discriminative large language models through 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#31350;&#20102;&#22522;&#20110;LLMs&#27169;&#25311;&#20195;&#29702;&#30340;&#34892;&#20026;&#65292;&#31216;&#20043;&#20026;LLM Personas&#65292;&#22312;&#20998;&#37197;&#22823;&#20116;&#20154;&#26684;&#31867;&#22411;&#21644;&#24615;&#21035;&#35282;&#33394;&#26102;&#26159;&#21542;&#21487;&#20197;&#29983;&#25104;&#20855;&#26377;&#19968;&#33268;&#24615;&#30340;&#20010;&#24615;&#21270;&#29305;&#36136;&#30340;&#20869;&#23481;&#12290;</title><link>http://arxiv.org/abs/2305.02547</link><description>&lt;p&gt;
PersonaLLM: &#25506;&#31350;GPT-3.5&#34920;&#36798;&#20010;&#24615;&#29305;&#24449;&#21644;&#24615;&#21035;&#24046;&#24322;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
PersonaLLM: Investigating the Ability of GPT-3.5 to Express Personality Traits and Gender Differences. (arXiv:2305.02547v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02547
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#31350;&#20102;&#22522;&#20110;LLMs&#27169;&#25311;&#20195;&#29702;&#30340;&#34892;&#20026;&#65292;&#31216;&#20043;&#20026;LLM Personas&#65292;&#22312;&#20998;&#37197;&#22823;&#20116;&#20154;&#26684;&#31867;&#22411;&#21644;&#24615;&#21035;&#35282;&#33394;&#26102;&#26159;&#21542;&#21487;&#20197;&#29983;&#25104;&#20855;&#26377;&#19968;&#33268;&#24615;&#30340;&#20010;&#24615;&#21270;&#29305;&#36136;&#30340;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#20010;&#34892;&#19994;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#35774;&#35745;&#20013;&#26377;&#35768;&#22810;&#29992;&#36884;&#65292;&#24182;&#19988;&#30740;&#31350;&#34920;&#26126;&#20010;&#24615;&#21270;&#32842;&#22825;&#26426;&#22120;&#20154;&#22312;&#28385;&#36275;&#19981;&#21516;&#20154;&#26684;&#29305;&#24449;&#26041;&#38754;&#30340;&#37325;&#35201;&#24615;&#65292;&#20294;&#24456;&#23569;&#26377;&#30740;&#31350;&#35780;&#20272;&#20010;&#24615;&#21270;LLM&#30340;&#34892;&#20026;&#26159;&#21542;&#33021;&#22815;&#20934;&#30830;&#12289;&#19968;&#33268;&#22320;&#21453;&#26144;&#26576;&#20123;&#20154;&#26684;&#29305;&#24449;&#12290;&#25105;&#20204;&#32771;&#34385;&#30740;&#31350;&#22522;&#20110;LLM&#30340;&#27169;&#25311;&#20195;&#29702;&#30340;&#34892;&#20026;&#65292;&#31216;&#20043;&#20026;LLM personas&#65292;&#24182;&#20351;&#29992;GPT-3.5&#65288;text-davinci-003&#65289;&#36827;&#34892;&#26696;&#20363;&#30740;&#31350;&#65292;&#20197;&#30740;&#31350;LLM&#22312;&#20998;&#37197;&#22823;&#20116;&#20154;&#26684;&#31867;&#22411;&#21644;&#24615;&#21035;&#35282;&#33394;&#26102;&#26159;&#21542;&#21487;&#20197;&#29983;&#25104;&#20855;&#26377;&#19968;&#33268;&#24615;&#30340;&#20010;&#24615;&#21270;&#29305;&#36136;&#30340;&#20869;&#23481;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;320&#20010;LLM personas&#65288;&#27599;&#31181;&#22823;&#20116;&#20154;&#26684;&#31867;&#22411;&#26377;5&#20010;&#22899;&#24615;&#21644;5&#20010;&#30007;&#24615;&#65289;&#65292;&#24182;&#25552;&#31034;&#20182;&#20204;&#23436;&#25104;&#32463;&#20856;&#30340;44&#39033;&#22823;&#20116;&#20154;&#26684;&#38382;&#21367;&#65288;BFI&#65289;&#65292;&#28982;&#21518;&#25776;&#20889;&#19968;&#20010;&#20851;&#20110;&#20182;&#20204;&#31461;&#24180;&#30340;800&#23383;&#25925;&#20107;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;LLM personas&#30340;&#33258;&#25105;&#25253;&#21578;&#30340;BFI&#20998;&#25968;&#19982;&#20182;&#20204;&#20998;&#37197;&#30340;&#20154;&#26684;&#31867;&#22411;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the many use cases for large language models (LLMs) in the design of chatbots in various industries and the research showing the importance of personalizing chatbots to cater to different personality traits, little work has been done to evaluate whether the behaviors of personalized LLMs can reflect certain personality traits accurately and consistently. We consider studying the behavior of LLM-based simulated agents which refer to as LLM personas and present a case study with GPT-3.5 (text-davinci-003) to investigate whether LLMs can generate content with consistent, personalized traits when assigned Big Five personality types and gender roles. We created 320 LLM personas (5 females and 5 males for each of the 32 Big Five personality types) and prompted them to complete the classic 44-item Big Five Inventory (BFI) and then write an 800-word story about their childhood. Results showed that LLM personas' self-reported BFI scores are consistent with their assigned personality typ
&lt;/p&gt;</description></item><item><title>HeySQuAD &#26159;&#19968;&#20010;&#22823;&#35268;&#27169;&#21475;&#35821;&#21270;&#38382;&#31572;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#34913;&#37327;&#26426;&#22120;&#29702;&#35299;&#24182;&#22238;&#31572;&#22024;&#26434;&#30340;&#21475;&#35821;&#25552;&#38382;&#30340;&#33021;&#21147;&#65292;&#21516;&#26102;&#20351;&#29992;&#36716;&#24405;&#30340;&#20154;&#31867;&#21475;&#35821;&#25552;&#38382;&#36827;&#34892;&#35757;&#32451;&#33021;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2304.13689</link><description>&lt;p&gt;
HeySQuAD: &#19968;&#20010;&#21475;&#35821;&#21270;&#38382;&#31572;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
HeySQuAD: A Spoken Question Answering Dataset. (arXiv:2304.13689v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13689
&lt;/p&gt;
&lt;p&gt;
HeySQuAD &#26159;&#19968;&#20010;&#22823;&#35268;&#27169;&#21475;&#35821;&#21270;&#38382;&#31572;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#34913;&#37327;&#26426;&#22120;&#29702;&#35299;&#24182;&#22238;&#31572;&#22024;&#26434;&#30340;&#21475;&#35821;&#25552;&#38382;&#30340;&#33021;&#21147;&#65292;&#21516;&#26102;&#20351;&#29992;&#36716;&#24405;&#30340;&#20154;&#31867;&#21475;&#35821;&#25552;&#38382;&#36827;&#34892;&#35757;&#32451;&#33021;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#21475;&#35821;&#25552;&#38382;&#23545;&#20110;&#35780;&#20272;&#21475;&#35821;&#38382;&#31572;&#31995;&#32479;&#30340;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#65292;&#23588;&#20854;&#26159;&#25968;&#23383;&#21161;&#25163;&#31561;&#22810;&#20010;&#23454;&#38469;&#24212;&#29992;&#22330;&#26223;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22823;&#35268;&#27169;&#31038;&#21306;&#20849;&#20139;&#30340;&#21475;&#35821;&#38382;&#31572;&#25968;&#25454;&#38598; HeySQuAD&#65292;&#23427;&#30001;76k&#20010;&#20154;&#31867;&#21475;&#35821;&#25552;&#38382;&#12289;97k&#20010;&#26426;&#22120;&#29983;&#25104;&#30340;&#38382;&#39064;&#20197;&#21450;&#30456;&#24212;&#30340;&#25991;&#26412;&#31572;&#26696;&#32452;&#25104;&#65292;&#36825;&#20123;&#31572;&#26696;&#28304;&#33258; SQuAD QA &#25968;&#25454;&#38598;&#12290;HeySQuAD &#30340;&#30446;&#26631;&#26159;&#34913;&#37327;&#26426;&#22120;&#29702;&#35299;&#22024;&#26434;&#30340;&#21475;&#35821;&#25552;&#38382;&#24182;&#20934;&#30830;&#22238;&#31572;&#38382;&#39064;&#30340;&#33021;&#21147;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#22312;&#20154;&#31867;&#21475;&#35821;&#21644;&#26426;&#22120;&#29983;&#25104;&#30340;&#38382;&#39064;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#20197;&#37327;&#21270;&#26469;&#33258;&#20004;&#26041;&#38754;&#22122;&#22768;&#30340;&#24046;&#24322;&#21450;&#23545;&#27169;&#22411;&#21644;&#22238;&#31572;&#20934;&#30830;&#24230;&#30340;&#24433;&#21709;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#22312;&#21475;&#35821;&#38382;&#31572;&#20219;&#21153;&#20013;&#65292;&#25105;&#20204;&#24076;&#26395;&#22238;&#31572;&#30340;&#26159;&#20154;&#31867;&#21475;&#35821;&#25552;&#38382;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#20351;&#29992;&#36716;&#24405;&#30340;&#20154;&#31867;&#21475;&#35821;&#25552;&#38382;&#21644;&#21407;&#22987; SQuAD &#38382;&#39064;&#36827;&#34892;&#35757;&#32451;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#65288;12.51%&#65289;&#27169;&#22411;&#30340;&#34920;&#29616;&#65292;&#32780;&#19981;&#26159;&#20165;&#20351;&#29992;&#21407;&#22987; SQuAD &#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human-spoken questions are critical to evaluating the performance of spoken question answering (SQA) systems that serve several real-world use cases including digital assistants. We present a new large-scale community-shared SQA dataset, HeySQuAD that consists of 76k human-spoken questions and 97k machine-generated questions and corresponding textual answers derived from the SQuAD QA dataset. The goal of HeySQuAD is to measure the ability of machines to understand noisy spoken questions and answer the questions accurately. To this end, we run extensive benchmarks on the human-spoken and machine-generated questions to quantify the differences in noise from both sources and its subsequent impact on the model and answering accuracy. Importantly, for the task of SQA, where we want to answer human-spoken questions, we observe that training using the transcribed human-spoken and original SQuAD questions leads to significant improvements (12.51%) over training using only the original SQuAD te
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38544;&#24335;&#20943;&#36731;&#35270;&#35273;&#20559;&#35265;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#21518;&#39564;&#20272;&#35745;&#23574;&#38160;&#21270;&#65292;&#40723;&#21169;&#32593;&#32476;&#32858;&#28966;&#20110;&#19981;&#23548;&#33268;&#39640;&#19981;&#30830;&#23450;&#24615;&#30340;&#20013;&#24515;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2303.16564</link><description>&lt;p&gt;
&#36890;&#36807;&#21518;&#39564;&#20272;&#35745;&#23574;&#38160;&#21270;&#26469;&#38544;&#24335;&#20943;&#36731;&#35270;&#35273;&#20559;&#35265;&#65306;&#19968;&#31181;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Implicit Visual Bias Mitigation by Posterior Estimate Sharpening of a Bayesian Neural Network. (arXiv:2303.16564v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16564
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38544;&#24335;&#20943;&#36731;&#35270;&#35273;&#20559;&#35265;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#21518;&#39564;&#20272;&#35745;&#23574;&#38160;&#21270;&#65292;&#40723;&#21169;&#32593;&#32476;&#32858;&#28966;&#20110;&#19981;&#23548;&#33268;&#39640;&#19981;&#30830;&#23450;&#24615;&#30340;&#20013;&#24515;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#20844;&#24179;&#24615;&#21463;&#21040;&#25968;&#25454;&#38598;&#20559;&#35265;&#21644;&#34394;&#20551;&#30456;&#20851;&#24615;&#30340;&#24378;&#28872;&#24433;&#21709;&#65292;&#32780;&#36825;&#20123;&#23545;&#29616;&#20195;&#29305;&#24449;&#20016;&#23500;&#21644;&#22797;&#26434;&#30340;&#35270;&#35273;&#25968;&#25454;&#38598;&#36890;&#24120;&#37117;&#26159;&#23384;&#22312;&#30340;&#12290;&#30001;&#20110;&#20219;&#21153;&#30340;&#38590;&#24230;&#21644;&#21487;&#21464;&#24615;&#65292;&#27809;&#26377;&#19968;&#31181;&#21333;&#19968;&#30340;&#21435;&#20559;&#35265;&#26041;&#27861;&#26159;&#26222;&#36941;&#25104;&#21151;&#30340;&#12290;&#29305;&#21035;&#26159;&#65292;&#22312;&#19981;&#38656;&#35201;&#26174;&#24335;&#30693;&#36947;&#20559;&#24046;&#21464;&#37327;&#30340;&#24773;&#20917;&#19979;&#30340;&#38544;&#24335;&#26041;&#27861;&#23545;&#20110;&#29616;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#23588;&#20026;&#30456;&#20851;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38544;&#24335;&#20943;&#32531;&#26041;&#27861;&#65292;&#20351;&#29992;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#65292;&#20801;&#35768;&#25105;&#20204;&#21033;&#29992;&#33268;&#24615;&#19981;&#30830;&#23450;&#24615;&#19982;&#26679;&#26412;&#20013;&#20559;&#24046;&#25110;&#34394;&#20551;&#30456;&#20851;&#24615;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#21518;&#39564;&#20272;&#35745;&#23574;&#38160;&#21270;&#31243;&#24207;&#40723;&#21169;&#32593;&#32476;&#32858;&#28966;&#20110;&#19981;&#23548;&#33268;&#39640;&#19981;&#30830;&#23450;&#24615;&#30340;&#20013;&#24515;&#29305;&#24449;&#12290;&#22312;&#19977;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20855;&#26377;&#32463;&#36807;&#23574;&#38160;&#21270;&#21518;&#39564;&#20272;&#35745;&#30340;&#36125;&#21494;&#26031;&#32593;&#32476;&#34920;&#29616;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#24403;&#65292;&#24182;&#26174;&#31034;&#20986;&#36827;&#19968;&#27493;&#30740;&#31350;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The fairness of a deep neural network is strongly affected by dataset bias and spurious correlations, both of which are usually present in modern feature-rich and complex visual datasets. Due to the difficulty and variability of the task, no single de-biasing method has been universally successful. In particular, implicit methods not requiring explicit knowledge of bias variables are especially relevant for real-world applications. We propose a novel implicit mitigation method using a Bayesian neural network, allowing us to leverage the relationship between epistemic uncertainties and the presence of bias or spurious correlations in a sample. Our proposed posterior estimate sharpening procedure encourages the network to focus on core features that do not contribute to high uncertainties. Experimental results on three benchmark datasets demonstrate that Bayesian networks with sharpened posterior estimates perform comparably to prior existing methods and show potential worthy of further 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#38544;&#31169;&#20445;&#25252;&#35757;&#32451;&#21307;&#23398;&#24433;&#20687;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#20844;&#24179;&#24615;&#65292;&#24182;&#19982;&#38750;&#38544;&#31169;&#35757;&#32451;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#30740;&#31350;&#32467;&#26524;&#21487;&#20026;&#38544;&#31169;&#20445;&#25252;&#25216;&#26415;&#30340;&#24191;&#27867;&#24212;&#29992;&#25552;&#20379;&#37325;&#35201;&#21442;&#32771;&#12290;</title><link>http://arxiv.org/abs/2302.01622</link><description>&lt;p&gt;
&#31169;&#23494;&#12289;&#20844;&#24179;&#19988;&#31934;&#30830;&#65306;&#22312;&#21307;&#23398;&#24433;&#20687;&#20013;&#35757;&#32451;&#22823;&#35268;&#27169;&#38544;&#31169;&#20445;&#25252;&#30340;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Private, fair and accurate: Training large-scale, privacy-preserving AI models in medical imaging. (arXiv:2302.01622v3 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.01622
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#38544;&#31169;&#20445;&#25252;&#35757;&#32451;&#21307;&#23398;&#24433;&#20687;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#20844;&#24179;&#24615;&#65292;&#24182;&#19982;&#38750;&#38544;&#31169;&#35757;&#32451;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#30740;&#31350;&#32467;&#26524;&#21487;&#20026;&#38544;&#31169;&#20445;&#25252;&#25216;&#26415;&#30340;&#24191;&#27867;&#24212;&#29992;&#25552;&#20379;&#37325;&#35201;&#21442;&#32771;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#22312;&#21307;&#23398;&#39046;&#22495;&#30340;&#24212;&#29992;&#36234;&#26469;&#36234;&#22810;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#21307;&#23398;&#25968;&#25454;&#30340;&#39640;&#24230;&#25935;&#24863;&#24615;&#65292;&#38656;&#35201;&#37319;&#21462;&#29305;&#27530;&#25514;&#26045;&#30830;&#20445;&#20854;&#20445;&#25252;&#12290;&#20445;&#25252;&#38544;&#31169;&#30340;&#40644;&#37329;&#26631;&#20934;&#26159;&#24341;&#20837;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#26469;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;DP&#23545;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#20844;&#24179;&#24615;&#26377;&#36127;&#38754;&#24433;&#21709;&#65292;&#36825;&#22312;&#21307;&#23398;&#20013;&#26159;&#19981;&#21487;&#25509;&#21463;&#30340;&#65292;&#24182;&#19988;&#26159;&#38544;&#31169;&#20445;&#25252;&#25216;&#26415;&#24191;&#27867;&#24212;&#29992;&#30340;&#20027;&#35201;&#38556;&#30861;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#38544;&#31169;&#20445;&#25252;&#35757;&#32451;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#23545;&#20934;&#30830;&#24615;&#21644;&#20844;&#24179;&#24615;&#30340;&#24433;&#21709;&#65292;&#19982;&#38750;&#38544;&#31169;&#35757;&#32451;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#20004;&#20010;&#25968;&#25454;&#38598;&#65306;&#65288;1&#65289;&#19968;&#20010;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65288;N=193,311&#65289;&#30340;&#39640;&#36136;&#37327;&#20020;&#24202;&#33016;&#37096;X&#23556;&#32447;&#22270;&#20687;&#65292;&#21644;&#65288;2&#65289;&#19968;&#20010;&#25968;&#25454;&#38598;&#65288;N=1,625&#65289;&#30340;3D&#33145;&#37096;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#65288;CT&#65289;&#22270;&#20687;&#65292;&#29992;&#20110;&#20998;&#31867;&#33008;&#33146;&#23548;&#31649;&#33146;&#30284;&#65288;PDAC&#65289;&#30340;&#23384;&#22312;&#12290;&#20004;&#20010;&#25968;&#25454;&#38598;&#22343;&#20026;&#22238;&#39038;&#24615;&#37319;&#38598;&#65292;&#24182;&#30001;&#32463;&#39564;&#20016;&#23500;&#30340;&#21307;&#23398;&#24433;&#20687;&#19987;&#23478;&#36827;&#34892;&#25163;&#21160;&#26631;&#27880;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial intelligence (AI) models are increasingly used in the medical domain. However, as medical data is highly sensitive, special precautions to ensure its protection are required. The gold standard for privacy preservation is the introduction of differential privacy (DP) to model training. Prior work indicates that DP has negative implications on model accuracy and fairness, which are unacceptable in medicine and represent a main barrier to the widespread use of privacy-preserving techniques. In this work, we evaluated the effect of privacy-preserving training of AI models regarding accuracy and fairness compared to non-private training. For this, we used two datasets: (1) A large dataset (N=193,311) of high quality clinical chest radiographs, and (2) a dataset (N=1,625) of 3D abdominal computed tomography (CT) images, with the task of classifying the presence of pancreatic ductal adenocarcinoma (PDAC). Both were retrospectively collected and manually labeled by experienced radio
&lt;/p&gt;</description></item></channel></rss>