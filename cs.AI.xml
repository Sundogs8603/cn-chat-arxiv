<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#24418;&#24335;&#21270;&#8220;&#20462;&#27491;&#26426;&#22120;&#28040;&#38500;&#8221;&#26469;&#35299;&#20915;&#21463;&#26410;&#30693;&#25805;&#32437;&#24433;&#21709;&#30340;&#25968;&#25454;&#23545;&#35757;&#32451;&#27169;&#22411;&#30340;&#24433;&#21709;&#38382;&#39064;&#65292;&#21487;&#33021;&#20165;&#30693;&#36947;&#19968;&#37096;&#20998;&#21463;&#24433;&#21709;&#26679;&#26412;&#12290;&#21457;&#29616;&#32416;&#27491;&#28040;&#38500;&#38382;&#39064;&#19982;&#20256;&#32479;&#20197;&#38544;&#31169;&#20026;&#23548;&#21521;&#30340;&#28040;&#38500;&#26041;&#27861;&#26377;&#26174;&#33879;&#19981;&#21516;&#30340;&#35201;&#27714;&#12290;</title><link>https://arxiv.org/abs/2402.14015</link><description>&lt;p&gt;
&#20462;&#27491;&#26426;&#22120;&#28040;&#38500;
&lt;/p&gt;
&lt;p&gt;
Corrective Machine Unlearning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14015
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#24418;&#24335;&#21270;&#8220;&#20462;&#27491;&#26426;&#22120;&#28040;&#38500;&#8221;&#26469;&#35299;&#20915;&#21463;&#26410;&#30693;&#25805;&#32437;&#24433;&#21709;&#30340;&#25968;&#25454;&#23545;&#35757;&#32451;&#27169;&#22411;&#30340;&#24433;&#21709;&#38382;&#39064;&#65292;&#21487;&#33021;&#20165;&#30693;&#36947;&#19968;&#37096;&#20998;&#21463;&#24433;&#21709;&#26679;&#26412;&#12290;&#21457;&#29616;&#32416;&#27491;&#28040;&#38500;&#38382;&#39064;&#19982;&#20256;&#32479;&#20197;&#38544;&#31169;&#20026;&#23548;&#21521;&#30340;&#28040;&#38500;&#26041;&#27861;&#26377;&#26174;&#33879;&#19981;&#21516;&#30340;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36234;&#26469;&#36234;&#38754;&#20020;&#25968;&#25454;&#23436;&#25972;&#24615;&#25361;&#25112;&#65292;&#22240;&#20026;&#23427;&#20204;&#20351;&#29992;&#20102;&#22823;&#35268;&#27169;&#30340;&#20174;&#20114;&#32852;&#32593;&#20013;&#33719;&#21462;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#26524;&#27169;&#22411;&#24320;&#21457;&#32773;&#21457;&#29616;&#26576;&#20123;&#25968;&#25454;&#34987;&#31713;&#25913;&#25110;&#38169;&#35823;&#65292;&#20182;&#20204;&#21487;&#20197;&#37319;&#21462;&#20160;&#20040;&#25514;&#26045;&#12290;&#36825;&#20123;&#34987;&#31713;&#25913;&#30340;&#25968;&#25454;&#20250;&#23548;&#33268;&#19981;&#21033;&#24433;&#21709;&#65292;&#22914;&#23481;&#26131;&#21463;&#21040;&#21518;&#38376;&#26679;&#26412;&#30340;&#25915;&#20987;&#12289;&#31995;&#32479;&#24615;&#20559;&#35265;&#65292;&#20197;&#21450;&#22312;&#26576;&#20123;&#36755;&#20837;&#39046;&#22495;&#30340;&#20934;&#30830;&#24230;&#38477;&#20302;&#12290;&#36890;&#24120;&#65292;&#24182;&#38750;&#25152;&#26377;&#34987;&#31713;&#25913;&#30340;&#35757;&#32451;&#26679;&#26412;&#37117;&#26159;&#24050;&#30693;&#30340;&#65292;&#32780;&#21482;&#26377;&#19968;&#23567;&#37096;&#20998;&#20195;&#34920;&#24615;&#30340;&#21463;&#24433;&#21709;&#25968;&#25454;&#34987;&#26631;&#35760;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14015v1 Announce Type: cross  Abstract: Machine Learning models increasingly face data integrity challenges due to the use of large-scale training datasets drawn from the internet. We study what model developers can do if they detect that some data was manipulated or incorrect. Such manipulated data can cause adverse effects like vulnerability to backdoored samples, systematic biases, and in general, reduced accuracy on certain input domains. Often, all manipulated training samples are not known, and only a small, representative subset of the affected data is flagged.   We formalize "Corrective Machine Unlearning" as the problem of mitigating the impact of data affected by unknown manipulations on a trained model, possibly knowing only a subset of impacted samples. We demonstrate that the problem of corrective unlearning has significantly different requirements from traditional privacy-oriented unlearning. We find most existing unlearning methods, including the gold-standard
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;&#25991;&#26412;&#27700;&#21360;&#20013;&#30340;&#8220;&#36328;&#35821;&#35328;&#19968;&#33268;&#24615;&#8221;&#27010;&#24565;&#65292;&#21457;&#29616;&#24403;&#21069;&#25991;&#26412;&#27700;&#21360;&#25216;&#26415;&#22312;&#25991;&#26412;&#34987;&#32763;&#35793;&#25104;&#20854;&#20182;&#35821;&#35328;&#21518;&#22833;&#21435;&#20102;&#19968;&#33268;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36328;&#35821;&#35328;&#27700;&#21360;&#21435;&#38500;&#25915;&#20987;&#26041;&#27861;&#65292;&#26377;&#25928;&#32469;&#36807;&#27700;&#21360;&#65292;&#38477;&#20302;AUC&#20540;&#65292;&#21516;&#26102;&#25351;&#20986;&#20102;&#23548;&#33268;&#36825;&#31181;&#24046;&#24322;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;</title><link>https://arxiv.org/abs/2402.14007</link><description>&lt;p&gt;
&#27700;&#21360;&#26159;&#21542;&#33021;&#22815;&#22312;&#32763;&#35793;&#20013;&#23384;&#27963;&#65311;&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25991;&#26412;&#27700;&#21360;&#30340;&#36328;&#35821;&#35328;&#19968;&#33268;&#24615;
&lt;/p&gt;
&lt;p&gt;
Can Watermarks Survive Translation? On the Cross-lingual Consistency of Text Watermark for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14007
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;&#25991;&#26412;&#27700;&#21360;&#20013;&#30340;&#8220;&#36328;&#35821;&#35328;&#19968;&#33268;&#24615;&#8221;&#27010;&#24565;&#65292;&#21457;&#29616;&#24403;&#21069;&#25991;&#26412;&#27700;&#21360;&#25216;&#26415;&#22312;&#25991;&#26412;&#34987;&#32763;&#35793;&#25104;&#20854;&#20182;&#35821;&#35328;&#21518;&#22833;&#21435;&#20102;&#19968;&#33268;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36328;&#35821;&#35328;&#27700;&#21360;&#21435;&#38500;&#25915;&#20987;&#26041;&#27861;&#65292;&#26377;&#25928;&#32469;&#36807;&#27700;&#21360;&#65292;&#38477;&#20302;AUC&#20540;&#65292;&#21516;&#26102;&#25351;&#20986;&#20102;&#23548;&#33268;&#36825;&#31181;&#24046;&#24322;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#27700;&#21360;&#25216;&#26415;&#26088;&#22312;&#26631;&#35760;&#21644;&#35782;&#21035;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#30340;&#20869;&#23481;&#65292;&#20197;&#38450;&#27490;&#28389;&#29992;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#25991;&#26412;&#27700;&#21360;&#20013;&#30340;&#8220;&#36328;&#35821;&#35328;&#19968;&#33268;&#24615;&#8221;&#27010;&#24565;&#65292;&#35780;&#20272;&#20102;&#25991;&#26412;&#27700;&#21360;&#22312;&#34987;&#32763;&#35793;&#25104;&#20854;&#20182;&#35821;&#35328;&#21518;&#20445;&#25345;&#26377;&#25928;&#24615;&#30340;&#33021;&#21147;&#12290;&#20004;&#20010;LLM&#21644;&#19977;&#31181;&#27700;&#21360;&#26041;&#27861;&#30340;&#21021;&#27493;&#23454;&#35777;&#32467;&#26524;&#26174;&#31034;&#65292;&#24403;&#21069;&#30340;&#25991;&#26412;&#27700;&#21360;&#25216;&#26415;&#22312;&#25991;&#26412;&#34987;&#32763;&#35793;&#25104;&#19981;&#21516;&#35821;&#35328;&#26102;&#32570;&#20047;&#19968;&#33268;&#24615;&#12290;&#22522;&#20110;&#36825;&#19968;&#35266;&#23519;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36328;&#35821;&#35328;&#27700;&#21360;&#21435;&#38500;&#25915;&#20987;&#65288;CWRA&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#39318;&#20808;&#20174;&#19968;&#20010;LLM&#20013;&#33719;&#21462;&#26469;&#33258;&#20013;&#20171;&#35821;&#35328;&#30340;&#21709;&#24212;&#65292;&#28982;&#21518;&#23558;&#20854;&#32763;&#35793;&#25104;&#30446;&#26631;&#35821;&#35328;&#26469;&#32469;&#36807;&#27700;&#21360;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#20943;&#23569;AUC&#20540;&#20174;0.95&#38477;&#33267;0.67&#32780;&#26080;&#24615;&#33021;&#25439;&#22833;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#23548;&#33268;&#20132;&#21449;&#19968;&#33268;&#24615;&#24046;&#24322;&#30340;&#20004;&#20010;&#20851;&#38190;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14007v1 Announce Type: cross  Abstract: Text watermarking technology aims to tag and identify content produced by large language models (LLMs) to prevent misuse. In this study, we introduce the concept of ''cross-lingual consistency'' in text watermarking, which assesses the ability of text watermarks to maintain their effectiveness after being translated into other languages. Preliminary empirical results from two LLMs and three watermarking methods reveal that current text watermarking technologies lack consistency when texts are translated into various languages. Based on this observation, we propose a Cross-lingual Watermark Removal Attack (CWRA) to bypass watermarking by first obtaining a response from an LLM in a pivot language, which is then translated into the target language. CWRA can effectively remove watermarks by reducing the Area Under the Curve (AUC) from 0.95 to 0.67 without performance loss. Furthermore, we analyze two key factors that contribute to the cros
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#23398;&#20064;&#22312;&#27668;&#20505;&#31185;&#23398;&#24212;&#29992;&#20013;&#26550;&#26500;&#36873;&#25321;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#22312;&#22810;&#26679;&#21270;&#30340;&#27668;&#20505;&#24773;&#26223;&#19979;&#21487;&#39044;&#27979;&#22823;&#35199;&#27915;&#32463;&#21521;&#32763;&#36716;&#29615;&#27969;&#65288;AMOC&#65289;&#65292;&#36827;&#19968;&#27493;&#25581;&#31034;MLP&#21644;&#28145;&#24230;&#38598;&#25104;&#21487;&#20197;&#23398;&#20064;AMOC&#30340;&#29289;&#29702;&#36807;&#31243;&#32780;&#38750;&#27169;&#25311;&#20854;&#36827;&#23637;&#12290;</title><link>https://arxiv.org/abs/2402.13979</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;&#27668;&#20505;&#24212;&#29992;&#20013;&#30340;&#26550;&#26500;&#36873;&#25321;&#30340;&#37325;&#35201;&#24615;
&lt;/p&gt;
&lt;p&gt;
The Importance of Architecture Choice in Deep Learning for Climate Applications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13979
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#23398;&#20064;&#22312;&#27668;&#20505;&#31185;&#23398;&#24212;&#29992;&#20013;&#26550;&#26500;&#36873;&#25321;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#22312;&#22810;&#26679;&#21270;&#30340;&#27668;&#20505;&#24773;&#26223;&#19979;&#21487;&#39044;&#27979;&#22823;&#35199;&#27915;&#32463;&#21521;&#32763;&#36716;&#29615;&#27969;&#65288;AMOC&#65289;&#65292;&#36827;&#19968;&#27493;&#25581;&#31034;MLP&#21644;&#28145;&#24230;&#38598;&#25104;&#21487;&#20197;&#23398;&#20064;AMOC&#30340;&#29289;&#29702;&#36807;&#31243;&#32780;&#38750;&#27169;&#25311;&#20854;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#24050;&#32463;&#25104;&#20026;&#27668;&#20505;&#31185;&#23398;&#24212;&#29992;&#20013;&#26222;&#36941;&#20351;&#29992;&#30340;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;&#27169;&#22411;&#26410;&#33021;&#35299;&#20915;&#30001;&#20154;&#20026;&#25913;&#21464;&#28201;&#23460;&#27668;&#20307;&#25490;&#25918;&#24341;&#36215;&#30340;&#38750;&#24179;&#31283;&#24615;&#65292;&#24182;&#19988;&#19981;&#20250;&#23450;&#26399;&#37327;&#21270;&#25152;&#25552;&#20986;&#30340;&#39044;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;&#22823;&#35199;&#27915;&#32463;&#21521;&#32763;&#36716;&#29615;&#27969;&#65288;AMOC&#65289;&#36827;&#34892;&#24314;&#27169;&#65292;&#36890;&#36807;&#23558;&#28201;&#26262;&#27700;&#36755;&#36865;&#21040;&#27431;&#27954;&#21644;&#32654;&#22269;&#19996;&#28023;&#23736;&#65292;&#23545;&#36825;&#20123;&#22320;&#21306;&#30340;&#27668;&#20505;&#33267;&#20851;&#37325;&#35201;&#65292;&#24182;&#19988;&#26377;&#28508;&#22312;&#30340;&#31361;&#28982;&#23849;&#28291;&#39118;&#38505;&#12290;&#25105;&#20204;&#21487;&#20197;&#29983;&#25104;&#36890;&#36807;&#20219;&#24847;&#26102;&#38388;&#23610;&#24230;&#30340;&#20219;&#24847;&#26497;&#31471;&#27668;&#20505;&#22330;&#26223;&#65292;&#28982;&#21518;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#39044;&#27979;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#26174;&#31034;&#65292;&#22312;&#22810;&#26679;&#21270;&#30340;&#27668;&#20505;&#24773;&#26223;&#19979;&#65292;AMOC&#21487;&#20197;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#21487;&#39044;&#27979;&#12290;&#36827;&#19968;&#27493;&#30340;&#23454;&#39564;&#26174;&#31034;&#65292;MLP&#21644;&#28145;&#24230;&#38598;&#25104;&#21487;&#20197;&#23398;&#20064;AMOC&#30340;&#29289;&#29702;&#36807;&#31243;&#65292;&#32780;&#19981;&#26159;&#36890;&#36807;&#33258;&#30456;&#20851;&#27169;&#25311;&#20854;&#36827;&#23637;&#12290;&#36890;&#36807;&#37327;&#21270;&#19981;&#30830;&#23450;&#24615;&#65292;&#21457;&#29616;&#20102;&#19968;&#20010;&#26377;&#36259;&#30340;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13979v1 Announce Type: cross  Abstract: Machine Learning has become a pervasive tool in climate science applications. However, current models fail to address nonstationarity induced by anthropogenic alterations in greenhouse emissions and do not routinely quantify the uncertainty of proposed projections. In this paper, we model the Atlantic Meridional Overturning Circulation (AMOC) which is of major importance to climate in Europe and the US East Coast by transporting warm water to these regions, and has the potential for abrupt collapse. We can generate arbitrarily extreme climate scenarios through arbitrary time scales which we then predict using neural networks. Our analysis shows that the AMOC is predictable using neural networks under a diverse set of climate scenarios. Further experiments reveal that MLPs and Deep Ensembles can learn the physics of the AMOC instead of imitating its progression through autocorrelation. With quantified uncertainty, an intriguing pattern 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;&#27010;&#29575;&#31070;&#32463;&#32593;&#32476;&#65288;PNNs&#65289;&#26469;&#24314;&#27169;Aleatoric&#19981;&#30830;&#23450;&#24615;&#65292;&#36890;&#36807;&#24320;&#21457;&#27010;&#29575;&#36317;&#31163;&#24230;&#37327;&#26469;&#20248;&#21270;PNN&#26550;&#26500;&#65292;&#35777;&#23454;&#20102;PNNs&#22312;&#27169;&#25311;Aleatoric&#19981;&#30830;&#23450;&#24615;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.13945</link><description>&lt;p&gt;
&#29992;&#20110;&#24314;&#27169;&#31185;&#23398;&#26426;&#22120;&#23398;&#20064;&#20013;Aleatoric&#19981;&#30830;&#23450;&#24615;&#30340;&#27010;&#29575;&#31070;&#32463;&#32593;&#32476;&#65288;PNNs&#65289;
&lt;/p&gt;
&lt;p&gt;
Probabilistic Neural Networks (PNNs) for Modeling Aleatoric Uncertainty in Scientific Machine Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13945
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;&#27010;&#29575;&#31070;&#32463;&#32593;&#32476;&#65288;PNNs&#65289;&#26469;&#24314;&#27169;Aleatoric&#19981;&#30830;&#23450;&#24615;&#65292;&#36890;&#36807;&#24320;&#21457;&#27010;&#29575;&#36317;&#31163;&#24230;&#37327;&#26469;&#20248;&#21270;PNN&#26550;&#26500;&#65292;&#35777;&#23454;&#20102;PNNs&#22312;&#27169;&#25311;Aleatoric&#19981;&#30830;&#23450;&#24615;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;&#27010;&#29575;&#31070;&#32463;&#32593;&#32476;&#65288;PNNs&#65289;&#26469;&#24314;&#27169;Aleatoric&#19981;&#30830;&#23450;&#24615;&#65292;&#35813;&#19981;&#30830;&#23450;&#24615;&#26159;&#25351;&#31995;&#32479;&#36755;&#20837;&#36755;&#20986;&#20851;&#31995;&#20013;&#22266;&#26377;&#30340;&#21464;&#24322;&#24615;&#65292;&#36890;&#24120;&#34920;&#29616;&#20026;&#19981;&#22343;&#31561;&#30340;&#26041;&#24046;&#25110;&#24322;&#26041;&#24046;&#24615;&#12290;&#19981;&#21516;&#20110;&#20135;&#29983;&#30830;&#23450;&#24615;&#36755;&#20986;&#30340;&#20256;&#32479;&#31070;&#32463;&#32593;&#32476;&#65292;PNNs&#20026;&#30446;&#26631;&#21464;&#37327;&#29983;&#25104;&#27010;&#29575;&#20998;&#24067;&#65292;&#20801;&#35768;&#22312;&#22238;&#24402;&#22330;&#26223;&#20013;&#30830;&#23450;&#39044;&#27979;&#22343;&#20540;&#21644;&#21306;&#38388;&#12290;&#26412;&#25991;&#30340;&#36129;&#29486;&#21253;&#25324;&#24320;&#21457;&#27010;&#29575;&#36317;&#31163;&#24230;&#37327;&#26469;&#20248;&#21270;PNN&#26550;&#26500;&#65292;&#20197;&#21450;&#22312;&#21463;&#25511;&#25968;&#25454;&#38598;&#21644;&#28041;&#21450;&#32420;&#32500;&#22686;&#24378;&#22797;&#21512;&#26448;&#26009;&#30340;&#23454;&#38469;&#26448;&#26009;&#31185;&#23398;&#26696;&#20363;&#20013;&#37096;&#32626;PNNs&#12290;&#30740;&#31350;&#32467;&#26524;&#35777;&#23454;&#65292;PNNs&#26377;&#25928;&#22320;&#27169;&#25311;&#20102;Aleatoric&#19981;&#30830;&#23450;&#24615;&#65292;&#35777;&#26126;&#22312;&#36825;&#19968;&#30446;&#30340;&#19978;&#65292;&#23427;&#27604;&#36890;&#24120;&#37319;&#29992;&#30340;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#26356;&#20026;&#21512;&#36866;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#22312;&#19968;&#20010;&#30495;&#23454;&#30340;&#31185;&#23398;&#29615;&#22659;&#20013;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13945v1 Announce Type: cross  Abstract: This paper investigates the use of probabilistic neural networks (PNNs) to model aleatoric uncertainty, which refers to the inherent variability in the input-output relationships of a system, often characterized by unequal variance or heteroscedasticity. Unlike traditional neural networks that produce deterministic outputs, PNNs generate probability distributions for the target variable, allowing the determination of both predicted means and intervals in regression scenarios. Contributions of this paper include the development of a probabilistic distance metric to optimize PNN architecture, and the deployment of PNNs in controlled data sets as well as a practical material science case involving fiber-reinforced composites. The findings confirm that PNNs effectively model aleatoric uncertainty, proving to be more appropriate than the commonly employed Gaussian process regression for this purpose. Specifically, in a real-world scientific
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#29702;&#35299;&#39640;&#25928;Transformer&#65288;&#20363;&#22914;&#31232;&#30095;Transformer&#21644;&#32447;&#24615;Transformer&#65289;&#30340;&#33021;&#21147;&#21644;&#38480;&#21046;&#65292;&#21457;&#29616;&#23427;&#20204;&#36866;&#21512;&#35299;&#20915;&#19968;&#33324;DP&#20219;&#21153;&#65292;&#20294;&#19981;&#21516;&#20110;&#26631;&#20934;Transformer&#12290;</title><link>https://arxiv.org/abs/2402.13934</link><description>&lt;p&gt;
&#30830;&#23454;&#39640;&#25928;&#30340;Transformer&#33021;&#22815;&#33410;&#32422;&#35745;&#31639;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Do Efficient Transformers Really Save Computation?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13934
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#29702;&#35299;&#39640;&#25928;Transformer&#65288;&#20363;&#22914;&#31232;&#30095;Transformer&#21644;&#32447;&#24615;Transformer&#65289;&#30340;&#33021;&#21147;&#21644;&#38480;&#21046;&#65292;&#21457;&#29616;&#23427;&#20204;&#36866;&#21512;&#35299;&#20915;&#19968;&#33324;DP&#20219;&#21153;&#65292;&#20294;&#19981;&#21516;&#20110;&#26631;&#20934;Transformer&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#36234;&#26469;&#36234;&#22823;&#30340;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#65292;&#24182;&#25317;&#26377;&#22823;&#37327;&#21442;&#25968;&#65292;&#25214;&#21040;&#26356;&#39640;&#25928;&#30340;&#26367;&#20195;&#26631;&#20934;Transformer&#21464;&#24471;&#38750;&#24120;&#26377;&#20215;&#20540;&#12290;&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#39640;&#25928;&#30340;Transformer&#21644;Transformer&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#20294;&#27809;&#26377;&#19968;&#20010;&#33021;&#22815;&#25552;&#20379;&#23427;&#20204;&#36866;&#21512;&#26367;&#20195;&#26631;&#20934;Transformer&#30340;&#29702;&#35770;&#20445;&#35777;&#12290;&#36825;&#20351;&#24471;&#24456;&#38590;&#30830;&#23450;&#20309;&#26102;&#20351;&#29992;&#29305;&#23450;&#27169;&#22411;&#20197;&#21450;&#36827;&#19968;&#27493;&#30740;&#31350;&#30340;&#37325;&#28857;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#29702;&#35299;&#39640;&#25928;Transformer&#30340;&#33021;&#21147;&#21644;&#23616;&#38480;&#24615;&#65292;&#29305;&#21035;&#26159;&#31232;&#30095;Transformer&#21644;&#32447;&#24615;Transformer&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#23427;&#20204;&#22312;Chain-of-Thought (CoT)&#25552;&#31034;&#20013;&#23637;&#31034;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#36981;&#24490;&#20808;&#21069;&#30340;&#30740;&#31350;&#23558;&#23427;&#20204;&#24314;&#27169;&#20026;&#21160;&#24577;&#35268;&#21010;&#65288;DP&#65289;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#34429;&#28982;&#36825;&#20123;&#27169;&#22411;&#36275;&#22815;&#34920;&#36798;&#35299;&#20915;&#19968;&#33324;DP&#20219;&#21153;&#30340;&#33021;&#21147;&#65292;&#20294;&#19982;&#26631;&#20934;Transformer&#19981;&#21516;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13934v1 Announce Type: cross  Abstract: As transformer-based language models are trained on increasingly large datasets and with vast numbers of parameters, finding more efficient alternatives to the standard Transformer has become very valuable. While many efficient Transformers and Transformer alternatives have been proposed, none provide theoretical guarantees that they are a suitable replacement for the standard Transformer. This makes it challenging to identify when to use a specific model and what directions to prioritize for further investigation. In this paper, we aim to understand the capabilities and limitations of efficient Transformers, specifically the Sparse Transformer and the Linear Transformer. We focus on their reasoning capability as exhibited by Chain-of-Thought (CoT) prompts and follow previous works to model them as Dynamic Programming (DP) problems. Our results show that while these models are expressive enough to solve general DP tasks, contrary to ex
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#28176;&#36827;&#21644;&#23545;&#25239;&#24615;&#33976;&#39311;&#30340;&#25193;&#25955;&#33976;&#39311;&#26041;&#27861;&#65292;&#22312;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#65292;&#24182;&#24320;&#28304;&#20102;&#30456;&#24212;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.13929</link><description>&lt;p&gt;
SDXL-Lightning: &#28176;&#36827;&#24335;&#23545;&#25239;&#24615;&#25193;&#25955;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
SDXL-Lightning: Progressive Adversarial Diffusion Distillation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13929
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#28176;&#36827;&#21644;&#23545;&#25239;&#24615;&#33976;&#39311;&#30340;&#25193;&#25955;&#33976;&#39311;&#26041;&#27861;&#65292;&#22312;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#65292;&#24182;&#24320;&#28304;&#20102;&#30456;&#24212;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25193;&#25955;&#33976;&#39311;&#26041;&#27861;&#65292;&#22312;&#22522;&#20110;SDXL&#30340;&#19968;&#27493;/&#20960;&#27493;1024&#20687;&#32032;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#20840;&#26032;&#30340;&#26368;&#20808;&#36827;&#27700;&#24179;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#32467;&#21512;&#20102;&#28176;&#36827;&#21644;&#23545;&#25239;&#24615;&#33976;&#39311;&#65292;&#23454;&#29616;&#20102;&#36136;&#37327;&#21644;&#27169;&#24335;&#35206;&#30422;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;&#26412;&#25991;&#35752;&#35770;&#20102;&#29702;&#35770;&#20998;&#26512;&#12289;&#21028;&#21035;&#22120;&#35774;&#35745;&#12289;&#27169;&#22411;&#20844;&#24335;&#21644;&#35757;&#32451;&#25216;&#24039;&#12290;&#25105;&#20204;&#20197;LoRA&#21644;&#23436;&#25972;UNet&#26435;&#37325;&#30340;&#24418;&#24335;&#24320;&#28304;&#20102;&#25105;&#20204;&#30340;&#33976;&#39311;SDXL-Lightning&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13929v1 Announce Type: cross  Abstract: We propose a diffusion distillation method that achieves new state-of-the-art in one-step/few-step 1024px text-to-image generation based on SDXL. Our method combines progressive and adversarial distillation to achieve a balance between quality and mode coverage. In this paper, we discuss the theoretical analysis, discriminator design, model formulation, and training techniques. We open-source our distilled SDXL-Lightning models both as LoRA and full UNet weights.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25193;&#23637;&#22916;&#24819;&#23545;&#20914;&#31639;&#27861;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22411;&#65292;&#33021;&#22815;&#24110;&#21161;&#20154;&#20204;&#20174;&#22810;&#20803;&#20449;&#24687;&#28304;&#20013;&#23398;&#20064;&#24182;&#21028;&#26029;&#21738;&#20123;&#35266;&#28857;&#20540;&#24471;&#20449;&#20219;&#12290;</title><link>https://arxiv.org/abs/2402.13927</link><description>&lt;p&gt;
&#20197;&#22916;&#24819;&#30340;&#23545;&#20914;&#31639;&#27861;&#20316;&#20026;&#20154;&#31867;&#20174;&#22810;&#20803;&#35266;&#28857;&#23398;&#20064;&#30340;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
The Delusional Hedge Algorithm as a Model of Human Learning from Diverse Opinions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13927
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25193;&#23637;&#22916;&#24819;&#23545;&#20914;&#31639;&#27861;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22411;&#65292;&#33021;&#22815;&#24110;&#21161;&#20154;&#20204;&#20174;&#22810;&#20803;&#20449;&#24687;&#28304;&#20013;&#23398;&#20064;&#24182;&#21028;&#26029;&#21738;&#20123;&#35266;&#28857;&#20540;&#24471;&#20449;&#20219;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35748;&#30693;&#23398;&#20064;&#27169;&#22411;&#36890;&#24120;&#20551;&#35774;&#30452;&#25509;&#20307;&#39564;&#20107;&#20214;&#30340;&#29305;&#24449;&#21644;&#30495;&#23454;&#26631;&#31614;&#25110;&#32467;&#26524;&#65292;&#20294;&#22823;&#37096;&#20998;&#26085;&#24120;&#23398;&#20064;&#26469;&#28304;&#20110;&#21548;&#21462;&#20182;&#20154;&#35266;&#28857;&#65292;&#27809;&#26377;&#30452;&#25509;&#25509;&#35302;&#20307;&#39564;&#25110;&#20934;&#30830;&#32467;&#26524;&#12290;&#25105;&#20204;&#36890;&#36807;&#25193;&#23637;&#23545;&#20914;&#31639;&#27861;&#26469;&#32771;&#34385;&#20154;&#20204;&#22312;&#36825;&#31181;&#22330;&#26223;&#19979;&#22914;&#20309;&#23398;&#20250;&#20449;&#20219;&#21738;&#20123;&#35266;&#28857;&#65306;&#36825;&#26159;&#19968;&#20010;&#32463;&#20856;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#20174;&#19981;&#21516;&#20449;&#24687;&#28304;&#23398;&#20064;&#12290;&#25105;&#20204;&#39318;&#27425;&#24341;&#20837;&#20102;&#19968;&#20010;&#25105;&#20204;&#31216;&#20043;&#20026;&#22916;&#24819;&#23545;&#20914;&#30340;&#21322;&#30417;&#30563;&#21464;&#20307;&#65292;&#23427;&#33021;&#22815;&#20174;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#32463;&#39564;&#20013;&#23398;&#20064;&#12290;&#36890;&#36807;&#20004;&#20010;&#23454;&#39564;&#65292;&#25105;&#20204;&#26816;&#39564;&#20102;&#20154;&#31867;&#21028;&#26029;&#21644;&#26631;&#20934;&#23545;&#20914;&#12289;&#22916;&#24819;&#23545;&#20914;&#20197;&#21450;&#21551;&#21457;&#24335;&#22522;&#32447;&#27169;&#22411;&#39044;&#27979;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20154;&#31867;&#26377;&#25928;&#22320;&#32467;&#21512;&#20102;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#20449;&#24687;&#65292;&#19982;&#22916;&#24819;&#23545;&#20914;&#31639;&#27861;&#19968;&#33268;&#22320;&#23398;&#20064;&#8212;&#8212;&#36825;&#34920;&#26126;&#20154;&#31867;&#23398;&#20064;&#32773;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13927v1 Announce Type: new  Abstract: Whereas cognitive models of learning often assume direct experience with both the features of an event and with a true label or outcome, much of everyday learning arises from hearing the opinions of others, without direct access to either the experience or the ground truth outcome. We consider how people can learn which opinions to trust in such scenarios by extending the hedge algorithm: a classic solution for learning from diverse information sources. We first introduce a semi-supervised variant we call the delusional hedge capable of learning from both supervised and unsupervised experiences. In two experiments, we examine the alignment between human judgments and predictions from the standard hedge, the delusional hedge, and a heuristic baseline model. Results indicate that humans effectively incorporate both labeled and unlabeled information in a manner consistent with the delusional hedge algorithm -- suggesting that human learners
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#33021;&#21463;&#21040;&#35825;&#39285;-&#36716;&#25442;&#25915;&#20987;&#30340;&#23041;&#32961;&#65292;&#29978;&#33267;&#23433;&#20840;&#29983;&#25104;&#30340;&#25991;&#26412;&#20063;&#33021;&#36731;&#26131;&#36716;&#21464;&#20026;&#26377;&#23475;&#20869;&#23481;&#65292;&#24378;&#35843;&#22312;LLMs&#30340;&#23433;&#20840;&#38450;&#25252;&#20013;&#38656;&#35201;&#32771;&#34385;&#21518;&#22788;&#29702;&#36716;&#25442;&#12290;</title><link>https://arxiv.org/abs/2402.13926</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26131;&#21463;&#35825;&#39285;-&#36716;&#25442;&#25915;&#20987;&#30340;&#21361;&#23475;&#20869;&#23481;&#29983;&#25104;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Large Language Models are Vulnerable to Bait-and-Switch Attacks for Generating Harmful Content
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13926
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#33021;&#21463;&#21040;&#35825;&#39285;-&#36716;&#25442;&#25915;&#20987;&#30340;&#23041;&#32961;&#65292;&#29978;&#33267;&#23433;&#20840;&#29983;&#25104;&#30340;&#25991;&#26412;&#20063;&#33021;&#36731;&#26131;&#36716;&#21464;&#20026;&#26377;&#23475;&#20869;&#23481;&#65292;&#24378;&#35843;&#22312;LLMs&#30340;&#23433;&#20840;&#38450;&#25252;&#20013;&#38656;&#35201;&#32771;&#34385;&#21518;&#22788;&#29702;&#36716;&#25442;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#27450;&#39575;&#24615;&#21644;&#26377;&#23475;&#20869;&#23481;&#25152;&#24102;&#26469;&#30340;&#39118;&#38505;&#24050;&#32463;&#24341;&#36215;&#20102;&#30456;&#24403;&#22810;&#30340;&#30740;&#31350;&#65292;&#20294;&#21363;&#20351;&#26159;&#23433;&#20840;&#30340;&#29983;&#25104;&#20063;&#21487;&#33021;&#23548;&#33268;&#38382;&#39064;&#38477;&#32423;&#24433;&#21709;&#12290;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23558;&#28966;&#28857;&#36716;&#31227;&#21040;&#21363;&#20351;&#26469;&#33258;LLMs&#30340;&#23433;&#20840;&#25991;&#26412;&#20063;&#21487;&#20197;&#36890;&#36807;&#35825;&#39285;-&#36716;&#25442;&#25915;&#20987;&#36731;&#26494;&#36716;&#21464;&#20026;&#28508;&#22312;&#21361;&#38505;&#20869;&#23481;&#12290;&#22312;&#36825;&#31181;&#25915;&#20987;&#20013;&#65292;&#29992;&#25143;&#39318;&#20808;&#29992;&#23433;&#20840;&#38382;&#39064;&#25552;&#31034;LLMs&#65292;&#28982;&#21518;&#21033;&#29992;&#31616;&#21333;&#30340;&#26597;&#25214;&#21644;&#26367;&#25442;&#21518;&#22788;&#29702;&#25216;&#26415;&#23558;&#36755;&#20986;&#25805;&#32437;&#25104;&#26377;&#23475;&#21465;&#20107;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#29983;&#25104;&#26377;&#27602;&#20869;&#23481;&#26041;&#38754;&#30340;&#24778;&#20154;&#26377;&#25928;&#24615;&#31361;&#20986;&#20102;&#22312;&#24320;&#21457;&#21487;&#38752;&#30340;LLMs&#23433;&#20840;&#38450;&#25252;&#26639;&#26102;&#38754;&#20020;&#30340;&#37325;&#22823;&#25361;&#25112;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#24378;&#35843;&#65292;&#19987;&#27880;&#20110;&#36880;&#23383;&#30340;LLMs&#36755;&#20986;&#30340;&#23433;&#20840;&#24615;&#26159;&#19981;&#22815;&#30340;&#65292;&#25105;&#20204;&#36824;&#38656;&#35201;&#32771;&#34385;&#21518;&#22788;&#29702;&#36716;&#25442;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13926v1 Announce Type: cross  Abstract: The risks derived from large language models (LLMs) generating deceptive and damaging content have been the subject of considerable research, but even safe generations can lead to problematic downstream impacts. In our study, we shift the focus to how even safe text coming from LLMs can be easily turned into potentially dangerous content through Bait-and-Switch attacks. In such attacks, the user first prompts LLMs with safe questions and then employs a simple find-and-replace post-hoc technique to manipulate the outputs into harmful narratives. The alarming efficacy of this approach in generating toxic content highlights a significant challenge in developing reliable safety guardrails for LLMs. In particular, we stress that focusing on the safety of the verbatim LLM outputs is insufficient and that we also need to consider post-hoc transformations.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#27969;&#31243;&#65292;&#21033;&#29992;GPT-3.5&#21644;GPT-4&#29983;&#25104;&#39640;&#36136;&#37327;&#21453;&#39304;&#65292;&#20197;&#22686;&#24378;&#20020;&#24202;&#31508;&#35760;&#25688;&#35201;&#20013;&#30340;&#20107;&#23454;&#19968;&#33268;&#24615;&#65292;&#24357;&#34917;&#20102;&#19987;&#23478;&#27880;&#37322;&#25968;&#25454;&#30340;&#39640;&#25104;&#26412;&#21644;&#26377;&#38480;&#21487;&#29992;&#24615;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.13919</link><description>&lt;p&gt;
SYNFAC-EDIT: &#29992;&#20110;&#20020;&#24202;&#25688;&#35201;&#20013;&#30340;&#20107;&#23454;&#23545;&#40784;&#30340;&#21512;&#25104;&#27169;&#20223;&#32534;&#36753;&#21453;&#39304;
&lt;/p&gt;
&lt;p&gt;
SYNFAC-EDIT: Synthetic Imitation Edit Feedback for Factual Alignment in Clinical Summarization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13919
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#27969;&#31243;&#65292;&#21033;&#29992;GPT-3.5&#21644;GPT-4&#29983;&#25104;&#39640;&#36136;&#37327;&#21453;&#39304;&#65292;&#20197;&#22686;&#24378;&#20020;&#24202;&#31508;&#35760;&#25688;&#35201;&#20013;&#30340;&#20107;&#23454;&#19968;&#33268;&#24615;&#65292;&#24357;&#34917;&#20102;&#19987;&#23478;&#27880;&#37322;&#25968;&#25454;&#30340;&#39640;&#25104;&#26412;&#21644;&#26377;&#38480;&#21487;&#29992;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;GPT&#21644;Llama&#22312;&#25688;&#35201;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;&#22312;&#20107;&#23454;&#19981;&#20934;&#30830;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#36825;&#26159;&#20020;&#24202;NLP&#24212;&#29992;&#20013;&#30340;&#20851;&#38190;&#38382;&#39064;&#65292;&#38169;&#35823;&#21487;&#33021;&#23548;&#33268;&#20005;&#37325;&#21518;&#26524;&#12290;&#20026;&#20102;&#35299;&#20915;&#20107;&#23454;&#23545;&#40784;&#30340;&#19987;&#23478;&#27880;&#37322;&#25968;&#25454;&#25104;&#26412;&#39640;&#26114;&#19988;&#26377;&#38480;&#30340;&#38382;&#39064;&#65292;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#27969;&#31243;&#65292;&#21033;&#29992;GPT-3.5&#21644;GPT-4&#29983;&#25104;&#39640;&#36136;&#37327;&#21453;&#39304;&#65292;&#26088;&#22312;&#22686;&#24378;&#20020;&#24202;&#31508;&#35760;&#25688;&#35201;&#20013;&#30340;&#20107;&#23454;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#32534;&#36753;&#21453;&#39304;&#65292;&#22312;&#27809;&#26377;&#39069;&#22806;&#27880;&#37322;&#30340;&#24773;&#20917;&#19979;&#65292;&#27169;&#25311;&#20102;&#21307;&#30103;&#19987;&#19994;&#20154;&#21592;&#25913;&#21892;AI&#31995;&#32479;&#36755;&#20986;&#30340;&#23454;&#38469;&#22330;&#26223;&#12290;&#23613;&#31649;GPT&#22312;&#21508;&#31181;&#20020;&#24202;NLP&#20219;&#21153;&#20013;&#37117;&#34920;&#29616;&#20986;&#20102;&#19987;&#19994;&#27700;&#24179;&#65292;&#27604;&#22914;&#21307;&#23398;&#25191;&#29031;&#32771;&#35797;&#65292;&#20294;&#23545;&#20854;&#25552;&#20379;&#25913;&#21892;&#36739;&#24369;LM&#25110;LLM&#29983;&#25104;&#36136;&#37327;&#30340;&#19987;&#19994;&#32423;&#32534;&#36753;&#21453;&#39304;&#30340;&#30740;&#31350;&#24456;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13919v1 Announce Type: cross  Abstract: Large Language Models (LLMs) such as GPT and Llama have demonstrated significant achievements in summarization tasks but struggle with factual inaccuracies, a critical issue in clinical NLP applications where errors could lead to serious consequences. To counter the high costs and limited availability of expert-annotated data for factual alignment, this study introduces an innovative pipeline that utilizes GPT-3.5 and GPT-4 to generate high-quality feedback aimed at enhancing factual consistency in clinical note summarization. Our research primarily focuses on edit feedback, mirroring the practical scenario in which medical professionals refine AI system outputs without the need for additional annotations. Despite GPT's proven expertise in various clinical NLP tasks, such as the Medical Licensing Examination, there is scant research on its capacity to deliver expert-level edit feedback for improving weaker LMs or LLMs generation qualit
&lt;/p&gt;</description></item><item><title>Llama2&#27169;&#22411;&#22312;&#32763;&#35793;&#20013;&#34920;&#29616;&#20986;&#20934;&#30830;&#24230;&#39640;&#65292;&#37096;&#20998;&#26410;&#35265;&#35821;&#35328;&#38656;&#35201;&#26356;&#22823;&#35268;&#27169;&#30340;&#27169;&#22411;&#26469;&#25552;&#21319;&#32763;&#35793;&#36136;&#37327;&#65292;&#21478;&#22806;&#35821;&#35328;&#30340;&#21477;&#27861;&#30456;&#20284;&#24615;&#24182;&#38750;&#32763;&#35793;&#36136;&#37327;&#30340;&#20027;&#35201;&#22240;&#32032;&#65292;&#26576;&#20123;&#35821;&#35328;&#21363;&#20351;&#25968;&#25454;&#23569;&#20381;&#28982;&#34920;&#29616;&#20986;&#24378;&#30456;&#20851;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.13917</link><description>&lt;p&gt;
LLM&#32763;&#35793;&#20013;&#30340;&#35821;&#35328;&#29305;&#24449;&#21644;&#37325;&#35201;&#35821;&#35328;&#26159;&#20160;&#20040;&#65311;
&lt;/p&gt;
&lt;p&gt;
What Linguistic Features and Languages are Important in LLM Translation?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13917
&lt;/p&gt;
&lt;p&gt;
Llama2&#27169;&#22411;&#22312;&#32763;&#35793;&#20013;&#34920;&#29616;&#20986;&#20934;&#30830;&#24230;&#39640;&#65292;&#37096;&#20998;&#26410;&#35265;&#35821;&#35328;&#38656;&#35201;&#26356;&#22823;&#35268;&#27169;&#30340;&#27169;&#22411;&#26469;&#25552;&#21319;&#32763;&#35793;&#36136;&#37327;&#65292;&#21478;&#22806;&#35821;&#35328;&#30340;&#21477;&#27861;&#30456;&#20284;&#24615;&#24182;&#38750;&#32763;&#35793;&#36136;&#37327;&#30340;&#20027;&#35201;&#22240;&#32032;&#65292;&#26576;&#20123;&#35821;&#35328;&#21363;&#20351;&#25968;&#25454;&#23569;&#20381;&#28982;&#34920;&#29616;&#20986;&#24378;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv&#65306;2402.13917v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#36328;&#39046;&#22495; &#25688;&#35201;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23637;&#31034;&#20102;&#22312;&#22810;&#20010;&#20219;&#21153;&#20013;&#20855;&#26377;&#24378;&#22823;&#33021;&#21147;&#65292;&#21253;&#25324;&#26426;&#22120;&#32763;&#35793;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#37325;&#28857;&#22312;&#20110;&#35780;&#20272;Llama2&#30340;&#26426;&#22120;&#32763;&#35793;&#33021;&#21147;&#65292;&#24182;&#25506;&#32034;&#32763;&#35793;&#22914;&#20309;&#21462;&#20915;&#20110;&#20854;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#35821;&#35328;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;7B Llama2&#27169;&#22411;&#23545;&#20854;&#25152;&#35265;&#30340;&#25152;&#26377;&#35821;&#35328;&#37117;&#21487;&#20197;&#33719;&#24471;&#36229;&#36807;10&#30340;BLEU&#20998;&#25968;&#65292;&#20294;&#24182;&#38750;&#24635;&#26159;&#23545;&#20854;&#26410;&#35265;&#30340;&#35821;&#35328;&#12290;&#23545;&#20110;&#36825;&#20123;&#26410;&#35265;&#35821;&#35328;&#65292;&#19982;&#20351;&#29992;&#32842;&#22825;&#29256;&#26412;&#25110;&#28155;&#21152;&#23569;&#37327;&#25968;&#25454;&#30456;&#27604;&#65292;&#22312;&#27169;&#22411;&#35268;&#27169;&#19978;&#35266;&#23519;&#21040;&#30340;&#26368;&#22823;&#25910;&#30410;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#35821;&#35328;&#36317;&#31163;&#20998;&#26512;&#26174;&#31034;&#65292;&#21477;&#27861;&#30456;&#20284;&#24615;&#24182;&#38750;&#22987;&#32456;&#26159;&#20915;&#23450;&#32763;&#35793;&#36136;&#37327;&#30340;&#20027;&#35201;&#35821;&#35328;&#22240;&#32032;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#29305;&#23450;&#24773;&#20917;&#19979;&#65292;&#19968;&#20123;&#35821;&#35328;&#65292;&#23613;&#31649;&#35757;&#32451;&#25968;&#25454;&#26126;&#26174;&#23569;&#20110;&#33521;&#35821;&#65292;&#21364;&#34920;&#29616;&#20986;&#19982;&#33521;&#35821;&#21487;&#27604;&#30340;&#24378;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#22312;&#36825;&#37324;&#30340;&#21457;&#29616;&#20026;&#30740;&#31350;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13917v1 Announce Type: cross  Abstract: Large Language Models (LLMs) demonstrate strong capability across multiple tasks, including machine translation. Our study focuses on evaluating Llama2's machine translation capabilities and exploring how translation depends on languages in its training data. Our experiments show that the 7B Llama2 model yields above 10 BLEU score for all languages it has seen, but not always for languages it has not seen. Most gains for those unseen languages are observed the most with the model scale compared to using chat versions or adding shot count. Furthermore, our linguistic distance analysis reveals that syntactic similarity is not always the primary linguistic factor in determining translation quality. Interestingly, we discovered that under specific circumstances, some languages, despite having significantly less training data than English, exhibit strong correlations comparable to English. Our discoveries here give new perspectives for the 
&lt;/p&gt;</description></item><item><title>XAI&#39046;&#22495;&#34987;&#21010;&#20998;&#20026;&#34013;&#33394;XAI&#21644;&#32418;&#33394;XAI&#20004;&#31181;&#35299;&#37322;&#25991;&#21270;&#65292;&#25351;&#20986;&#20102;&#32418;&#33394;XAI&#39046;&#22495;&#30340;&#37325;&#35201;&#24615;&#21644;&#30740;&#31350;&#28508;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.13914</link><description>&lt;p&gt;
&#19981;&#26159;&#20026;&#20102;&#36777;&#35299;&#32780;&#26159;&#20026;&#20102;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Explain to Question not to Justify
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13914
&lt;/p&gt;
&lt;p&gt;
XAI&#39046;&#22495;&#34987;&#21010;&#20998;&#20026;&#34013;&#33394;XAI&#21644;&#32418;&#33394;XAI&#20004;&#31181;&#35299;&#37322;&#25991;&#21270;&#65292;&#25351;&#20986;&#20102;&#32418;&#33394;XAI&#39046;&#22495;&#30340;&#37325;&#35201;&#24615;&#21644;&#30740;&#31350;&#28508;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#26159;&#19968;&#20010;&#24180;&#36731;&#20294;&#38750;&#24120;&#26377;&#21069;&#36884;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#35813;&#39046;&#22495;&#30446;&#21069;&#30340;&#36827;&#23637;&#21463;&#21040;&#20102;&#19981;&#21516;&#21644;&#19981;&#20860;&#23481;&#30446;&#26631;&#30340;&#38480;&#21046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;XAI&#39046;&#22495;&#20869;&#32416;&#32544;&#22312;&#19968;&#36215;&#30340;&#21508;&#31181;&#32447;&#32034;&#20998;&#20026;&#20004;&#31181;&#20114;&#34917;&#30340;&#25991;&#21270;&#65292;&#21363;&#20154;&#31867;/&#20215;&#20540;&#21462;&#21521;&#35299;&#37322;&#65288;&#34013;&#33394;XAI&#65289;&#21644;&#27169;&#22411;/&#39564;&#35777;&#21462;&#21521;&#35299;&#37322;&#65288;&#32418;&#33394;XAI&#65289;&#12290;&#25105;&#20204;&#36824;&#35748;&#20026;&#65292;&#32418;&#33394;XAI&#39046;&#22495;&#30446;&#21069;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#65292;&#38544;&#34255;&#30528;&#24040;&#22823;&#30340;&#26426;&#36935;&#21644;&#37325;&#35201;&#30740;&#31350;&#30340;&#28508;&#21147;&#65292;&#20197;&#30830;&#20445;AI&#31995;&#32479;&#30340;&#23433;&#20840;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#36825;&#19968;&#39046;&#22495;&#30340;&#26377;&#21069;&#36884;&#30340;&#25361;&#25112;&#26469;&#24635;&#32467;&#26412;&#25991;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13914v1 Announce Type: new  Abstract: Explainable Artificial Intelligence (XAI) is a young but very promising field of research. Unfortunately, the progress in this field is currently slowed down by divergent and incompatible goals. In this paper, we separate various threads tangled within the area of XAI into two complementary cultures of human/value-oriented explanations (BLUE XAI) and model/validation-oriented explanations (RED XAI). We also argue that the area of RED XAI is currently under-explored and hides great opportunities and potential for important research necessary to ensure the safety of AI systems. We conclude this paper by presenting promising challenges in this area.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#22359;&#24335;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#38271;&#25991;&#26723;&#20013;&#20449;&#24687;&#26816;&#32034;&#39046;&#22495;&#30340;&#25361;&#25112;&#65292;&#24182;&#23454;&#29616;&#20102;&#21452;&#21521;&#20132;&#20114;</title><link>https://arxiv.org/abs/2402.13897</link><description>&lt;p&gt;
&#31185;&#23398;&#26816;&#26597;&#32773;&#20877;&#24230;&#21319;&#32423;&#65306;&#36879;&#26126;&#24230;&#21644;&#36923;&#36753;&#25512;&#29702;&#30340;&#21452;&#21521;&#33539;&#24335;
&lt;/p&gt;
&lt;p&gt;
Science Checker Reloaded: A Bidirectional Paradigm for Transparency and Logical Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13897
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#22359;&#24335;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#38271;&#25991;&#26723;&#20013;&#20449;&#24687;&#26816;&#32034;&#39046;&#22495;&#30340;&#25361;&#25112;&#65292;&#24182;&#23454;&#29616;&#20102;&#21452;&#21521;&#20132;&#20114;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20449;&#24687;&#26816;&#32034;&#26159;&#19968;&#20010;&#24555;&#36895;&#21457;&#23637;&#30340;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#23427;&#20173;&#28982;&#38754;&#20020;&#30528;&#22312;&#31185;&#23398;&#21644;&#24037;&#19994;&#30340;&#28023;&#37327;&#20449;&#24687;&#20013;&#30340;&#35832;&#22810;&#38480;&#21046;&#65292;&#27604;&#22914;&#35821;&#20041;&#20998;&#27495;&#21644;&#26816;&#32034;&#20013;&#30340;&#35789;&#27719;&#24046;&#36317;&#12289;&#35821;&#20041;&#25628;&#32034;&#20013;&#30340;&#20302;&#31934;&#24230;&#21644;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#65292;&#25110;&#32773;&#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;&#21644;&#36807;&#26102;&#20449;&#24687;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#22359;&#24335;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#38271;&#25991;&#26723;&#30340;&#36825;&#20123;&#38556;&#30861;&#12290;&#31532;&#19968;&#20010;&#27169;&#22359;&#36890;&#36807;&#26597;&#35810;&#25193;&#23637;&#22686;&#24378;&#20102;&#22312;&#31232;&#30095;&#26816;&#32034;&#20013;&#30340;&#35821;&#35328;&#29702;&#35299;&#65292;&#20197;&#26816;&#32034;&#30456;&#20851;&#25991;&#26723;&#12290;&#31532;&#20108;&#20010;&#27169;&#22359;&#36890;&#36807;&#21482;&#20351;&#29992;&#38271;&#25991;&#26723;&#20013;&#20256;&#25773;&#30340;&#20449;&#24687;&#65292;&#20026;&#22797;&#26434;&#38382;&#39064;&#25552;&#20379;&#20840;&#38754;&#21644;&#20449;&#24687;&#20016;&#23500;&#30340;&#31572;&#26696;&#26469;&#21152;&#28145;&#32467;&#26524;&#65292;&#23454;&#29616;&#21452;&#21521;&#20132;&#20114;&#12290;&#22312;&#31649;&#36947;&#30340;&#21508;&#20010;&#38454;&#27573;&#65292;&#21521;&#29992;&#25143;&#21576;&#29616;&#20013;&#38388;&#32467;&#26524;&#20197;&#20419;&#36827;&#23545;&#31995;&#32479;&#25512;&#29702;&#30340;&#29702;&#35299;&#12290;&#25105;&#20204;&#30456;&#20449;&#36825;&#31181;&#21452;&#21521;&#26041;&#27861;&#24102;&#26469;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13897v1 Announce Type: cross  Abstract: Information retrieval is a rapidly evolving field. However it still faces significant limitations in the scientific and industrial vast amounts of information, such as semantic divergence and vocabulary gaps in sparse retrieval, low precision and lack of interpretability in semantic search, or hallucination and outdated information in generative models. In this paper, we introduce a two-block approach to tackle these hurdles for long documents. The first block enhances language understanding in sparse retrieval by query expansion to retrieve relevant documents. The second block deepens the result by providing comprehensive and informative answers to the complex question using only the information spread in the long document, enabling bidirectional engagement. At various stages of the pipeline, intermediate results are presented to users to facilitate understanding of the system's reasoning. We believe this bidirectional approach brings
&lt;/p&gt;</description></item><item><title>&#20248;&#21270;&#30340;Transformer&#27169;&#22411;DistilBERT&#29992;&#20110;&#26816;&#27979;&#38035;&#40060;&#37038;&#20214;&#65292;&#36890;&#36807;&#39044;&#22788;&#29702;&#25216;&#26415;&#35299;&#20915;&#20102;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;</title><link>https://arxiv.org/abs/2402.13871</link><description>&lt;p&gt;
&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#22522;&#20110;Transformer&#30340;&#38035;&#40060;&#37038;&#20214;&#26816;&#27979;&#27169;&#22411;&#65306;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
An Explainable Transformer-based Model for Phishing Email Detection: A Large Language Model Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13871
&lt;/p&gt;
&lt;p&gt;
&#20248;&#21270;&#30340;Transformer&#27169;&#22411;DistilBERT&#29992;&#20110;&#26816;&#27979;&#38035;&#40060;&#37038;&#20214;&#65292;&#36890;&#36807;&#39044;&#22788;&#29702;&#25216;&#26415;&#35299;&#20915;&#20102;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38035;&#40060;&#37038;&#20214;&#26159;&#19968;&#31181;&#20005;&#37325;&#30340;&#32593;&#32476;&#23041;&#32961;&#65292;&#35797;&#22270;&#36890;&#36807;&#21457;&#36865;&#34394;&#20551;&#37038;&#20214;&#26469;&#27450;&#39575;&#29992;&#25143;&#65292;&#24847;&#22270;&#26159;&#31363;&#21462;&#26426;&#23494;&#20449;&#24687;&#25110;&#36896;&#25104;&#36130;&#21153;&#25439;&#22833;&#12290;&#25915;&#20987;&#32773;&#24120;&#24120;&#20882;&#20805;&#21487;&#20449;&#23454;&#20307;&#65292;&#21033;&#29992;&#25216;&#26415;&#36827;&#27493;&#21644;&#22797;&#26434;&#24615;&#20351;&#24471;&#38035;&#40060;&#30340;&#26816;&#27979;&#21644;&#39044;&#38450;&#26356;&#20855;&#25361;&#25112;&#24615;&#12290;&#23613;&#31649;&#36827;&#34892;&#20102;&#22823;&#37327;&#23398;&#26415;&#30740;&#31350;&#65292;&#20294;&#38035;&#40060;&#37038;&#20214;&#26816;&#27979;&#22312;&#32593;&#32476;&#23433;&#20840;&#39046;&#22495;&#20173;&#28982;&#26159;&#19968;&#20010;&#25345;&#32493;&#19988;&#20005;&#23803;&#30340;&#25361;&#25112;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21644;&#25513;&#30422;&#35821;&#35328;&#27169;&#22411;&#65288;MLMs&#65289;&#25317;&#26377;&#24040;&#22823;&#28508;&#21147;&#65292;&#33021;&#22815;&#25552;&#20379;&#21019;&#26032;&#35299;&#20915;&#26041;&#26696;&#26469;&#35299;&#20915;&#38271;&#26399;&#23384;&#22312;&#30340;&#25361;&#25112;&#12290;&#22312;&#26412;&#30740;&#31350;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32463;&#36807;&#20248;&#21270;&#30340;&#12289;&#32463;&#36807;&#24494;&#35843;&#30340;&#22522;&#20110;Transformer&#30340;DistilBERT&#27169;&#22411;&#65292;&#29992;&#20110;&#26816;&#27979;&#38035;&#40060;&#37038;&#20214;&#12290;&#22312;&#26816;&#27979;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#32452;&#38035;&#40060;&#37038;&#20214;&#25968;&#25454;&#38598;&#65292;&#24182;&#21033;&#29992;&#39044;&#22788;&#29702;&#25216;&#26415;&#26469;&#28165;&#29702;&#21644;&#35299;&#20915;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;&#36890;&#36807;&#25105;&#20204;&#30340;&#23454;&#39564;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13871v1 Announce Type: cross  Abstract: Phishing email is a serious cyber threat that tries to deceive users by sending false emails with the intention of stealing confidential information or causing financial harm. Attackers, often posing as trustworthy entities, exploit technological advancements and sophistication to make detection and prevention of phishing more challenging. Despite extensive academic research, phishing detection remains an ongoing and formidable challenge in the cybersecurity landscape. Large Language Models (LLMs) and Masked Language Models (MLMs) possess immense potential to offer innovative solutions to address long-standing challenges. In this research paper, we present an optimized, fine-tuned transformer-based DistilBERT model designed for the detection of phishing emails. In the detection process, we work with a phishing email dataset and utilize the preprocessing techniques to clean and solve the imbalance class issues. Through our experiments, 
&lt;/p&gt;</description></item><item><title>Kuaiji&#26159;&#31532;&#19968;&#20010;&#20013;&#22269;&#20250;&#35745;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;Baichuan&#26694;&#26550;&#31934;&#24515;&#35843;&#25972;&#65292;&#25903;&#25345;&#30340;CAtAcctQA&#25968;&#25454;&#38598;&#65292;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#20934;&#30830;&#24615;&#21644;&#21709;&#24212;&#36895;&#24230;&#65292;&#20855;&#26377;&#24320;&#21019;&#24615;&#22320;&#21019;&#24314;&#20102;&#20013;&#22269;&#20250;&#35745;&#25968;&#25454;&#38598;&#65292;&#24182;&#35777;&#23454;&#20102;&#22312;&#30495;&#23454;&#20250;&#35745;&#22330;&#26223;&#20013;&#30340;&#39640;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.13866</link><description>&lt;p&gt;
Kuaiji&#65306;&#31532;&#19968;&#20010;&#20013;&#22269;&#20250;&#35745;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Kuaiji: the First Chinese Accounting Large Language Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13866
&lt;/p&gt;
&lt;p&gt;
Kuaiji&#26159;&#31532;&#19968;&#20010;&#20013;&#22269;&#20250;&#35745;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;Baichuan&#26694;&#26550;&#31934;&#24515;&#35843;&#25972;&#65292;&#25903;&#25345;&#30340;CAtAcctQA&#25968;&#25454;&#38598;&#65292;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#20934;&#30830;&#24615;&#21644;&#21709;&#24212;&#36895;&#24230;&#65292;&#20855;&#26377;&#24320;&#21019;&#24615;&#22320;&#21019;&#24314;&#20102;&#20013;&#22269;&#20250;&#35745;&#25968;&#25454;&#38598;&#65292;&#24182;&#35777;&#23454;&#20102;&#22312;&#30495;&#23454;&#20250;&#35745;&#22330;&#26223;&#20013;&#30340;&#39640;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;ChatGPT&#21644;GPT-4&#24050;&#32463;&#23637;&#31034;&#20986;&#22312;&#29702;&#35299;&#21644;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#26041;&#38754;&#30340;&#20986;&#33394;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#24403;&#38754;&#20020;&#20219;&#21153;&#35201;&#27714;&#36866;&#24212;&#20250;&#35745;&#31561;&#19987;&#19994;&#39046;&#22495;&#26102;&#65292;&#23427;&#20204;&#20250;&#36935;&#21040;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Kuaiji&#65292;&#19968;&#20010;&#19987;&#38376;&#23450;&#21046;&#30340;&#20250;&#35745;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;Kuaiji&#32463;&#36807;&#31934;&#24515;&#35843;&#25972;&#65292;&#20351;&#29992;&#21253;&#21547;&#36830;&#32493;&#39044;&#35757;&#32451;&#21644;&#30417;&#30563;&#24494;&#35843;&#36807;&#31243;&#30340;Baichuan&#26694;&#26550;&#12290;&#22312;CAtAcctQA&#30340;&#25903;&#25345;&#19979;&#65292;&#36825;&#26159;&#19968;&#20010;&#21253;&#21547;&#22823;&#37327;&#30495;&#23454;&#20250;&#35745;&#24072;&#19982;&#23458;&#25143;&#23545;&#35805;&#30340;&#25968;&#25454;&#38598;&#65292;Kuaiji&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#20934;&#30830;&#24615;&#21644;&#21709;&#24212;&#36895;&#24230;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#21253;&#25324;&#21019;&#24314;&#20102;&#31532;&#19968;&#20010;&#20013;&#22269;&#20250;&#35745;&#25968;&#25454;&#38598;&#65292;&#23558;Kuaiji&#24314;&#31435;&#20026;&#19968;&#31181;&#39046;&#20808;&#30340;&#24320;&#28304;&#20013;&#22269;&#20250;&#35745;LLM&#65292;&#24182;&#36890;&#36807;&#30495;&#23454;&#20250;&#35745;&#22330;&#26223;&#23545;&#20854;&#26377;&#25928;&#24615;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13866v1 Announce Type: cross  Abstract: Large Language Models (LLMs) like ChatGPT and GPT-4 have demonstrated impressive proficiency in comprehending and generating natural language. However, they encounter difficulties when tasked with adapting to specialized domains such as accounting. To address this challenge, we introduce Kuaiji, a tailored Accounting Large Language Model. Kuaiji is meticulously fine-tuned using the Baichuan framework, which encompasses continuous pre-training and supervised fine-tuning processes. Supported by CAtAcctQA, a dataset containing large genuine accountant-client dialogues, Kuaiji exhibits exceptional accuracy and response speed. Our contributions encompass the creation of the first Chinese accounting dataset, the establishment of Kuaiji as a leading open-source Chinese accounting LLM, and the validation of its efficacy through real-world accounting scenarios.
&lt;/p&gt;</description></item><item><title>RealDex&#25968;&#25454;&#38598;&#25429;&#25417;&#20102;&#30495;&#23454;&#30340;&#28789;&#24039;&#25163;&#25235;&#21462;&#21160;&#20316;&#65292;&#21033;&#29992;&#22810;&#27169;&#24577;&#25968;&#25454;&#20351;&#24471;&#35757;&#32451;&#28789;&#24039;&#25163;&#26356;&#21152;&#33258;&#28982;&#21644;&#31934;&#30830;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#20808;&#36827;&#30340;&#28789;&#24039;&#25235;&#21462;&#21160;&#20316;&#29983;&#25104;&#26694;&#26550;&#65292;&#26377;&#25928;&#21033;&#29992;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#31867;&#20154;&#26426;&#22120;&#20154;&#30340;&#33258;&#21160;&#24863;&#30693;&#12289;&#35748;&#30693;&#21644;&#25805;&#32437;&#26041;&#38754;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.13853</link><description>&lt;p&gt;
RealDex: &#23454;&#29616;&#26426;&#22120;&#20154;&#28789;&#24039;&#25163;&#31867;&#20154;&#24335;&#25235;&#21462;
&lt;/p&gt;
&lt;p&gt;
RealDex: Towards Human-like Grasping for Robotic Dexterous Hand
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13853
&lt;/p&gt;
&lt;p&gt;
RealDex&#25968;&#25454;&#38598;&#25429;&#25417;&#20102;&#30495;&#23454;&#30340;&#28789;&#24039;&#25163;&#25235;&#21462;&#21160;&#20316;&#65292;&#21033;&#29992;&#22810;&#27169;&#24577;&#25968;&#25454;&#20351;&#24471;&#35757;&#32451;&#28789;&#24039;&#25163;&#26356;&#21152;&#33258;&#28982;&#21644;&#31934;&#30830;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#20808;&#36827;&#30340;&#28789;&#24039;&#25235;&#21462;&#21160;&#20316;&#29983;&#25104;&#26694;&#26550;&#65292;&#26377;&#25928;&#21033;&#29992;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#31867;&#20154;&#26426;&#22120;&#20154;&#30340;&#33258;&#21160;&#24863;&#30693;&#12289;&#35748;&#30693;&#21644;&#25805;&#32437;&#26041;&#38754;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;RealDex&#65292;&#19968;&#20010;&#24320;&#21019;&#24615;&#30340;&#25968;&#25454;&#38598;&#65292;&#25429;&#25417;&#20102;&#34701;&#20837;&#20102;&#20154;&#31867;&#34892;&#20026;&#27169;&#24335;&#30340;&#30495;&#23454;&#28789;&#24039;&#25163;&#25235;&#21462;&#21160;&#20316;&#65292;&#21516;&#26102;&#36890;&#36807;&#22810;&#35270;&#35282;&#21644;&#22810;&#27169;&#24577;&#35270;&#35273;&#25968;&#25454;&#36827;&#34892;&#20102;&#20016;&#23500;&#12290;&#21033;&#29992;&#36828;&#31243;&#25805;&#20316;&#31995;&#32479;&#65292;&#25105;&#20204;&#21487;&#20197;&#23454;&#26102;&#26080;&#32541;&#21516;&#27493;&#20154;-&#26426;&#22120;&#20154;&#25163;&#23039;&#21183;&#12290;&#36825;&#20123;&#31867;&#20154;&#21160;&#20316;&#30340;&#38598;&#21512;&#23545;&#20110;&#35757;&#32451;&#28789;&#24039;&#25163;&#26356;&#33258;&#28982;&#12289;&#26356;&#31934;&#30830;&#22320;&#27169;&#20223;&#20154;&#31867;&#21160;&#20316;&#33267;&#20851;&#37325;&#35201;&#12290;RealDex&#22312;&#25512;&#21160;&#31867;&#20154;&#26426;&#22120;&#20154;&#22312;&#30495;&#23454;&#22330;&#26223;&#20013;&#33258;&#21160;&#24863;&#30693;&#12289;&#35748;&#30693;&#21644;&#25805;&#32437;&#26041;&#38754;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21069;&#27839;&#30340;&#28789;&#24039;&#25235;&#21462;&#21160;&#20316;&#29983;&#25104;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#31526;&#21512;&#20154;&#31867;&#32463;&#39564;&#65292;&#24182;&#36890;&#36807;&#26377;&#25928;&#21033;&#29992;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#20102;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;RealDex&#21644;&#20854;&#20182;&#24320;&#25918;&#25968;&#25454;&#38598;&#19978;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;&#23436;&#25972;&#30340;&#25968;&#25454;&#38598;&#21644;&#20195;&#30721;&#23558;&#20250;&#20844;&#24320;&#21457;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13853v1 Announce Type: cross  Abstract: In this paper, we introduce RealDex, a pioneering dataset capturing authentic dexterous hand grasping motions infused with human behavioral patterns, enriched by multi-view and multimodal visual data. Utilizing a teleoperation system, we seamlessly synchronize human-robot hand poses in real time. This collection of human-like motions is crucial for training dexterous hands to mimic human movements more naturally and precisely. RealDex holds immense promise in advancing humanoid robot for automated perception, cognition, and manipulation in real-world scenarios. Moreover, we introduce a cutting-edge dexterous grasping motion generation framework, which aligns with human experience and enhances real-world applicability through effectively utilizing Multimodal Large Language Models. Extensive experiments have demonstrated the superior performance of our method on RealDex and other open datasets. The complete dataset and code will be made 
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#25511;&#21046;&#31995;&#32479;&#65292;&#29992;&#20110;&#36830;&#32493;&#33889;&#33796;&#31958;&#30417;&#27979;&#21644;&#32500;&#25252;&#65292;&#23454;&#26102;&#21160;&#24577;&#35843;&#25972;&#33008;&#23707;&#32032;&#36755;&#36865;&#65292;&#22686;&#24378;&#33889;&#33796;&#31958;&#20248;&#21270;&#65292;&#26368;&#22823;&#21270;&#25928;&#29575;&#24182;&#30830;&#20445;&#20010;&#24615;&#21270;&#25252;&#29702;&#12290;</title><link>https://arxiv.org/abs/2402.13852</link><description>&lt;p&gt;
&#36830;&#32493;&#33889;&#33796;&#31958;&#30417;&#27979;&#21644;&#32500;&#25252;&#30340;&#31070;&#32463;&#25511;&#21046;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Neural Control System for Continuous Glucose Monitoring and Maintenance
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13852
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#25511;&#21046;&#31995;&#32479;&#65292;&#29992;&#20110;&#36830;&#32493;&#33889;&#33796;&#31958;&#30417;&#27979;&#21644;&#32500;&#25252;&#65292;&#23454;&#26102;&#21160;&#24577;&#35843;&#25972;&#33008;&#23707;&#32032;&#36755;&#36865;&#65292;&#22686;&#24378;&#33889;&#33796;&#31958;&#20248;&#21270;&#65292;&#26368;&#22823;&#21270;&#25928;&#29575;&#24182;&#30830;&#20445;&#20010;&#24615;&#21270;&#25252;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31934;&#30830;&#30340;&#33889;&#33796;&#31958;&#27700;&#24179;&#31649;&#29702;&#23545;&#20110;&#31958;&#23615;&#30149;&#24739;&#32773;&#33267;&#20851;&#37325;&#35201;&#65292;&#21487;&#20197;&#36991;&#20813;&#20005;&#37325;&#24182;&#21457;&#30151;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#25511;&#21046;&#31995;&#32479;&#65292;&#29992;&#20110;&#36830;&#32493;&#33889;&#33796;&#31958;&#30417;&#27979;&#21644;&#32500;&#25252;&#65292;&#21033;&#29992;&#24494;&#20998;&#39044;&#27979;&#25511;&#21046;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#21463;&#21040;&#22797;&#26434;&#31070;&#32463;&#31574;&#30053;&#21644;&#21487;&#21306;&#20998;&#24314;&#27169;&#30340;&#25351;&#23548;&#65292;&#23454;&#26102;&#21160;&#24577;&#35843;&#25972;&#33008;&#23707;&#32032;&#36755;&#36865;&#65292;&#22686;&#24378;&#33889;&#33796;&#31958;&#20248;&#21270;&#12290;&#36825;&#31181;&#31471;&#21040;&#31471;&#26041;&#27861;&#26368;&#22823;&#21270;&#25928;&#29575;&#65292;&#30830;&#20445;&#20010;&#24615;&#21270;&#25252;&#29702;&#21644;&#25913;&#21892;&#20581;&#24247;&#32467;&#26524;&#65292;&#22914;&#32463;&#39564;&#21457;&#29616;&#25152;&#35777;&#23454;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13852v1 Announce Type: cross  Abstract: Precise glucose level management is pivotal for individuals with diabetes, averting severe complications. In this work, we introduce a novel neural control system for continuous glucose monitoring and maintenance, utilizing differential predictive control. Our system, guided by a sophisticated neural policy and differentiable modeling, dynamically adjusts insulin delivery in real-time, enhancing glucose optimization. This end-to-end approach maximizes efficiency, ensuring personalized care and improved health outcomes, as affirmed by empirical findings.
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20445;&#25252;&#20010;&#20154;&#25968;&#25454;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#25239;&#24615;LLM&#25512;&#26029;&#30340;&#21311;&#21517;&#21270;&#26694;&#26550;&#12290;</title><link>https://arxiv.org/abs/2402.13846</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#20808;&#36827;&#30340;&#21311;&#21517;&#21270;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
Large Language Models are Advanced Anonymizers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13846
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20445;&#25252;&#20010;&#20154;&#25968;&#25454;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#25239;&#24615;LLM&#25512;&#26029;&#30340;&#21311;&#21517;&#21270;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#38544;&#31169;&#30740;&#31350;&#39046;&#22495;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#23427;&#20204;&#22312;&#25512;&#26029;&#30495;&#23454;&#19990;&#30028;&#22312;&#32447;&#25991;&#26412;&#20013;&#30340;&#20010;&#20154;&#25968;&#25454;&#26041;&#38754;&#34920;&#29616;&#20986;&#25509;&#36817;&#20154;&#31867;&#27700;&#24179;&#30340;&#24615;&#33021;&#12290;&#38543;&#30528;&#27169;&#22411;&#33021;&#21147;&#30340;&#19981;&#26029;&#22686;&#24378;&#65292;&#29616;&#26377;&#30340;&#25991;&#26412;&#21311;&#21517;&#21270;&#26041;&#27861;&#24403;&#21069;&#24050;&#32463;&#33853;&#21518;&#20110;&#30417;&#31649;&#35201;&#27714;&#21644;&#23545;&#25239;&#23041;&#32961;&#12290;&#36825;&#24341;&#20986;&#20102;&#19968;&#20010;&#38382;&#39064;&#65306;&#20010;&#20154;&#22914;&#20309;&#26377;&#25928;&#22320;&#20445;&#25252;&#20182;&#20204;&#22312;&#20998;&#20139;&#22312;&#32447;&#25991;&#26412;&#26102;&#30340;&#20010;&#20154;&#25968;&#25454;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#37319;&#21462;&#20102;&#20004;&#27493;&#26469;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65306;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35774;&#32622;&#65292;&#29992;&#20110;&#35780;&#20272;&#38754;&#23545;&#23545;&#25239;&#24615;LLM&#30340;&#25512;&#26029;&#26102;&#30340;&#21311;&#21517;&#21270;&#25928;&#26524;&#65292;&#20174;&#32780;&#20801;&#35768;&#33258;&#28982;&#22320;&#27979;&#37327;&#21311;&#21517;&#21270;&#24615;&#33021;&#65292;&#21516;&#26102;&#32416;&#27491;&#20102;&#20197;&#21069;&#25351;&#26631;&#30340;&#19968;&#20123;&#32570;&#38519;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;LLM&#30340;&#23545;&#25239;&#24615;&#21311;&#21517;&#21270;&#26694;&#26550;&#65292;&#21033;&#29992;LLM&#30340;&#24378;&#22823;&#25512;&#26029;&#33021;&#21147;&#26469;&#25351;&#23548;&#25105;&#20204;&#30340;&#21311;&#21517;&#21270;&#36807;&#31243;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#35780;&#20272;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#30340;&#21311;&#21517;&#21270;&#23454;&#36341;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13846v1 Announce Type: cross  Abstract: Recent work in privacy research on large language models has shown that they achieve near human-level performance at inferring personal data from real-world online texts. With consistently increasing model capabilities, existing text anonymization methods are currently lacking behind regulatory requirements and adversarial threats. This raises the question of how individuals can effectively protect their personal data in sharing online texts. In this work, we take two steps to answer this question: We first present a new setting for evaluating anonymizations in the face of adversarial LLMs inferences, allowing for a natural measurement of anonymization performance while remedying some of the shortcomings of previous metrics. We then present our LLM-based adversarial anonymization framework leveraging the strong inferential capabilities of LLMs to inform our anonymization procedure. In our experimental evaluation, we show on real-world 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;LLM4SBR&#26694;&#26550;&#65292;&#26159;&#31532;&#19968;&#20010;&#36866;&#21512;&#22312;&#22522;&#20110;&#20250;&#35805;&#30340;&#25512;&#33616;&#20013;&#38598;&#25104;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36731;&#37327;&#19988;&#26377;&#25928;&#26694;&#26550;&#12290;</title><link>https://arxiv.org/abs/2402.13840</link><description>&lt;p&gt;
LLM4SBR: &#19968;&#20010;&#36731;&#37327;&#19988;&#26377;&#25928;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#22522;&#20110;&#20250;&#35805;&#30340;&#25512;&#33616;&#20013;&#38598;&#25104;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
LLM4SBR: A Lightweight and Effective Framework for Integrating Large Language Models in Session-based Recommendation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13840
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;LLM4SBR&#26694;&#26550;&#65292;&#26159;&#31532;&#19968;&#20010;&#36866;&#21512;&#22312;&#22522;&#20110;&#20250;&#35805;&#30340;&#25512;&#33616;&#20013;&#38598;&#25104;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36731;&#37327;&#19988;&#26377;&#25928;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#22522;&#20110;&#20250;&#35805;&#30340;&#25512;&#33616;(SBR)&#21033;&#29992;&#26469;&#33258;&#21311;&#21517;&#29992;&#25143;&#30340;&#20250;&#35805;&#34892;&#20026;&#24207;&#21015;&#36827;&#34892;&#25512;&#33616;&#12290;&#34429;&#28982;&#36825;&#31181;&#31574;&#30053;&#38750;&#24120;&#39640;&#25928;&#65292;&#20294;&#29306;&#29298;&#20102;&#21830;&#21697;&#30340;&#22266;&#26377;&#35821;&#20041;&#20449;&#24687;&#65292;&#20351;&#27169;&#22411;&#38590;&#20197;&#29702;&#35299;&#20250;&#35805;&#30340;&#30495;&#27491;&#24847;&#22270;&#65292;&#23548;&#33268;&#25512;&#33616;&#32467;&#26524;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#12290;&#36817;&#24180;&#26469;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#21508;&#20010;&#39046;&#22495;&#34028;&#21187;&#21457;&#23637;&#65292;&#20026;&#35299;&#20915;&#19978;&#36848;&#25361;&#25112;&#24102;&#26469;&#20102;&#19968;&#32447;&#24076;&#26395;&#12290;&#21463;LLMs&#24433;&#21709;&#65292;&#25506;&#35752;LLMs&#19982;&#25512;&#33616;&#31995;&#32479;(RS)&#38598;&#25104;&#30340;&#30740;&#31350;&#22914;&#38632;&#21518;&#26149;&#31499;&#33324;&#28044;&#29616;&#12290;&#28982;&#32780;&#65292;&#21463;&#38480;&#20110;&#39640;&#26102;&#38388;&#21644;&#31354;&#38388;&#25104;&#26412;&#65292;&#20197;&#21450;&#20250;&#35805;&#25968;&#25454;&#30701;&#26242;&#19988;&#21311;&#21517;&#30340;&#29305;&#24615;&#65292;&#31532;&#19968;&#20010;&#36866;&#21512;&#24037;&#19994;&#37096;&#32626;&#30340;LLM&#25512;&#33616;&#26694;&#26550;&#22312;SBR&#39046;&#22495;&#23578;&#26410;&#20986;&#29616;&#12290;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#25361;&#25112;&#65292;&#25105;&#20204;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13840v1 Announce Type: cross  Abstract: Traditional session-based recommendation (SBR) utilizes session behavior sequences from anonymous users for recommendation. Although this strategy is highly efficient, it sacrifices the inherent semantic information of the items, making it difficult for the model to understand the true intent of the session and resulting in a lack of interpretability in the recommended results. Recently, large language models (LLMs) have flourished across various domains, offering a glimpse of hope in addressing the aforementioned challenges. Inspired by the impact of LLMs, research exploring the integration of LLMs with the Recommender system (RS) has surged like mushrooms after rain. However, constrained by high time and space costs, as well as the brief and anonymous nature of session data, the first LLM recommendation framework suitable for industrial deployment has yet to emerge in the field of SBR. To address the aforementioned challenges, we hav
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#30340;&#12289;&#32467;&#26500;&#21270;&#30340;&#34920;&#31034;&#21644;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#20613;&#31435;&#21494;&#28508;&#21160;&#21147;&#23398;&#25552;&#39640;&#20102;&#36816;&#21160;&#23398;&#20064;&#31639;&#27861;&#30340;&#25554;&#20540;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.13820</link><description>&lt;p&gt;
FLD&#65306;&#20613;&#31435;&#21494;&#28508;&#21160;&#21147;&#23398;&#29992;&#20110;&#32467;&#26500;&#21270;&#36816;&#21160;&#34920;&#31034;&#21644;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
FLD: Fourier Latent Dynamics for Structured Motion Representation and Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13820
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#30340;&#12289;&#32467;&#26500;&#21270;&#30340;&#34920;&#31034;&#21644;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#20613;&#31435;&#21494;&#28508;&#21160;&#21147;&#23398;&#25552;&#39640;&#20102;&#36816;&#21160;&#23398;&#20064;&#31639;&#27861;&#30340;&#25554;&#20540;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36816;&#21160;&#36712;&#36857;&#20026;&#22522;&#20110;&#29289;&#29702;&#30340;&#36816;&#21160;&#23398;&#20064;&#25552;&#20379;&#21487;&#38752;&#21442;&#32771;&#65292;&#20294;&#22312;&#32570;&#20047;&#36275;&#22815;&#25968;&#25454;&#35206;&#30422;&#30340;&#21306;&#22495;&#65292;&#23384;&#22312;&#31232;&#30095;&#24615;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#30340;&#32467;&#26500;&#21270;&#34920;&#31034;&#21644;&#29983;&#25104;&#26041;&#27861;&#65292;&#25552;&#21462;&#21608;&#26399;&#24615;&#25110;&#20934;&#21608;&#26399;&#24615;&#36816;&#21160;&#30340;&#26102;&#31354;&#20851;&#31995;&#12290;&#22312;&#36830;&#32493;&#21442;&#25968;&#21270;&#30340;&#28508;&#31354;&#38388;&#20013;&#30340;&#36816;&#21160;&#21160;&#21147;&#23398;&#20351;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#22686;&#24378;&#36816;&#21160;&#23398;&#20064;&#31639;&#27861;&#30340;&#25554;&#20540;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#21463;&#36816;&#21160;&#21442;&#25968;&#21270;&#21551;&#21457;&#30340;&#36816;&#21160;&#23398;&#20064;&#25511;&#21046;&#22120;&#21487;&#20197;&#22312;&#32447;&#36319;&#36394;&#21508;&#31181;&#36816;&#21160;&#65292;&#21253;&#25324;&#35757;&#32451;&#26102;&#26410;&#35265;&#36807;&#30340;&#30446;&#26631;&#12290;&#36890;&#36807;&#19968;&#20010;&#22238;&#36864;&#26426;&#21046;&#65292;&#25511;&#21046;&#22120;&#21487;&#20197;&#21160;&#24577;&#35843;&#25972;&#20854;&#36319;&#36394;&#31574;&#30053;&#65292;&#24182;&#22312;&#25552;&#20986;&#28508;&#22312;&#21361;&#38505;&#30446;&#26631;&#26102;&#33258;&#21160;&#37319;&#21462;&#23433;&#20840;&#34892;&#21160;&#25191;&#34892;&#12290;&#36890;&#36807;&#21033;&#29992;&#35782;&#21035;&#30340;&#26102;&#31354;&#32467;&#26500;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#24320;&#21551;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13820v1 Announce Type: cross  Abstract: Motion trajectories offer reliable references for physics-based motion learning but suffer from sparsity, particularly in regions that lack sufficient data coverage. To address this challenge, we introduce a self-supervised, structured representation and generation method that extracts spatial-temporal relationships in periodic or quasi-periodic motions. The motion dynamics in a continuously parameterized latent space enable our method to enhance the interpolation and generalization capabilities of motion learning algorithms. The motion learning controller, informed by the motion parameterization, operates online tracking of a wide range of motions, including targets unseen during training. With a fallback mechanism, the controller dynamically adapts its tracking strategy and automatically resorts to safe action execution when a potentially risky target is proposed. By leveraging the identified spatial-temporal structure, our work open
&lt;/p&gt;</description></item><item><title>NeuralDiffuser&#24341;&#20837;&#20027;&#35270;&#35273;&#29305;&#24449;&#24341;&#23548;&#65292;&#25193;&#23637;&#20102;LDM&#26041;&#27861;&#30340;&#33258;&#19979;&#32780;&#19978;&#36807;&#31243;&#65292;&#20197;&#23454;&#29616;&#24544;&#23454;&#30340;&#35821;&#20041;&#21644;&#32454;&#33410;&#12290;</title><link>https://arxiv.org/abs/2402.13809</link><description>&lt;p&gt;
NeuralDiffuser&#65306;&#20855;&#26377;&#20027;&#35270;&#35273;&#29305;&#24449;&#24341;&#23548;&#25193;&#25955;&#30340;&#21487;&#25511;fMRI&#37325;&#24314;
&lt;/p&gt;
&lt;p&gt;
NeuralDiffuser: Controllable fMRI Reconstruction with Primary Visual Feature Guided Diffusion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13809
&lt;/p&gt;
&lt;p&gt;
NeuralDiffuser&#24341;&#20837;&#20027;&#35270;&#35273;&#29305;&#24449;&#24341;&#23548;&#65292;&#25193;&#23637;&#20102;LDM&#26041;&#27861;&#30340;&#33258;&#19979;&#32780;&#19978;&#36807;&#31243;&#65292;&#20197;&#23454;&#29616;&#24544;&#23454;&#30340;&#35821;&#20041;&#21644;&#32454;&#33410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;(LDM)&#20174;&#21151;&#33021;&#24615;&#30913;&#20849;&#25391;&#25104;&#20687;(fMRI)&#20013;&#37325;&#24314;&#35270;&#35273;&#21050;&#28608;&#65292;&#20026;&#22823;&#33041;&#25552;&#20379;&#20102;&#32454;&#31890;&#24230;&#30340;&#26816;&#32034;&#12290;&#19968;&#20010;&#25361;&#25112;&#22312;&#20110;&#37325;&#24314;&#32454;&#33410;&#30340;&#36830;&#36143;&#23545;&#40784;&#65288;&#22914;&#32467;&#26500;&#12289;&#32972;&#26223;&#12289;&#32441;&#29702;&#12289;&#39068;&#33394;&#31561;&#65289;&#12290;&#27492;&#22806;&#65292;&#21363;&#20351;&#22312;&#30456;&#21516;&#26465;&#20214;&#19979;&#65292;LDM&#20063;&#20250;&#29983;&#25104;&#19981;&#21516;&#30340;&#22270;&#20687;&#32467;&#26524;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#25581;&#31034;&#20102;&#22522;&#20110;LDM&#30340;&#31070;&#32463;&#31185;&#23398;&#35270;&#35282;&#65292;&#21363;&#22522;&#20110;&#26469;&#33258;&#28023;&#37327;&#22270;&#20687;&#30340;&#39044;&#35757;&#32451;&#30693;&#35782;&#36827;&#34892;&#33258;&#19978;&#32780;&#19979;&#30340;&#21019;&#24314;&#65292;&#20294;&#32570;&#20047;&#22522;&#20110;&#32454;&#33410;&#39537;&#21160;&#30340;&#33258;&#19979;&#32780;&#19978;&#24863;&#30693;&#65292;&#23548;&#33268;&#32454;&#33410;&#19981;&#24544;&#23454;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;NeuralDiffuser&#65292;&#24341;&#20837;&#20027;&#35270;&#35273;&#29305;&#24449;&#24341;&#23548;&#65292;&#20197;&#28176;&#21464;&#24418;&#24335;&#25552;&#20379;&#32454;&#33410;&#32447;&#32034;&#65292;&#25193;&#23637;&#20102;LDM&#26041;&#27861;&#30340;&#33258;&#19979;&#32780;&#19978;&#36807;&#31243;&#65292;&#20197;&#23454;&#29616;&#24544;&#23454;&#30340;&#35821;&#20041;&#21644;&#32454;&#33410;&#12290;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24341;&#23548;&#31574;&#30053;&#65292;&#20197;&#30830;&#20445;&#37325;&#22797;&#37325;&#24314;&#30340;&#19968;&#33268;&#24615;&#65292;&#32780;&#19981;&#26159;&#38543;&#26426;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13809v1 Announce Type: cross  Abstract: Reconstructing visual stimuli from functional Magnetic Resonance Imaging (fMRI) based on Latent Diffusion Models (LDM) provides a fine-grained retrieval of the brain. A challenge persists in reconstructing a cohesive alignment of details (such as structure, background, texture, color, etc.). Moreover, LDMs would generate different image results even under the same conditions. For these, we first uncover the neuroscientific perspective of LDM-based methods that is top-down creation based on pre-trained knowledge from massive images but lack of detail-driven bottom-up perception resulting in unfaithful details. We propose NeuralDiffuser which introduces primary visual feature guidance to provide detail cues in the form of gradients, extending the bottom-up process for LDM-based methods to achieve faithful semantics and details. We also developed a novel guidance strategy to ensure the consistency of repeated reconstructions rather than a
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#30340;&#20998;&#23618;&#25511;&#21046;&#22120;&#35774;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#31616;&#27905;&#30340;&#8220;&#28508;&#22312;&#8221;&#31574;&#30053;&#26469;&#35299;&#20915;&#25151;&#38388;&#24314;&#27169;&#38382;&#39064;&#65292;&#26080;&#38656;&#27169;&#22411;&#25552;&#28860;&#27493;&#39588;&#65292;&#20811;&#26381;&#20102;DRL&#20013;&#30340;&#31232;&#30095;&#22870;&#21169;&#65292;&#23454;&#29616;&#20102;&#20302;&#32423;&#31574;&#30053;&#30340;&#21487;&#37325;&#29992;&#24615;</title><link>https://arxiv.org/abs/2402.13785</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#30340;&#20998;&#23618;&#25511;&#21046;&#22120;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
Synthesis of Hierarchical Controllers Based on Deep Reinforcement Learning Policies
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13785
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#30340;&#20998;&#23618;&#25511;&#21046;&#22120;&#35774;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#31616;&#27905;&#30340;&#8220;&#28508;&#22312;&#8221;&#31574;&#30053;&#26469;&#35299;&#20915;&#25151;&#38388;&#24314;&#27169;&#38382;&#39064;&#65292;&#26080;&#38656;&#27169;&#22411;&#25552;&#28860;&#27493;&#39588;&#65292;&#20811;&#26381;&#20102;DRL&#20013;&#30340;&#31232;&#30095;&#22870;&#21169;&#65292;&#23454;&#29616;&#20102;&#20302;&#32423;&#31574;&#30053;&#30340;&#21487;&#37325;&#29992;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#23558;&#29615;&#22659;&#24314;&#27169;&#20026;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDPs&#65289;&#30340;&#25511;&#21046;&#22120;&#35774;&#35745;&#38382;&#39064;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#20998;&#23618;MDP&#65292;&#19968;&#20010;&#30001;&#31216;&#20026;&#8220;&#25151;&#38388;&#8221;&#30340;MDP&#22635;&#20805;&#30340;&#22270;&#24418;&#12290;&#25105;&#20204;&#39318;&#20808;&#24212;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#26469;&#33719;&#24471;&#27599;&#20010;&#25151;&#38388;&#30340;&#20302;&#32423;&#31574;&#30053;&#65292;&#20197;&#36866;&#24212;&#26410;&#30693;&#32467;&#26500;&#30340;&#22823;&#25151;&#38388;&#12290;&#28982;&#21518;&#25105;&#20204;&#24212;&#29992;&#21453;&#24212;&#21512;&#25104;&#26469;&#33719;&#24471;&#19968;&#20010;&#39640;&#32423;&#35268;&#21010;&#32773;&#65292;&#36873;&#25321;&#22312;&#27599;&#20010;&#25151;&#38388;&#25191;&#34892;&#21738;&#20010;&#20302;&#32423;&#31574;&#30053;&#12290;&#22312;&#21512;&#25104;&#35268;&#21010;&#32773;&#26041;&#38754;&#30340;&#20013;&#24515;&#25361;&#25112;&#26159;&#38656;&#35201;&#23545;&#25151;&#38388;&#36827;&#34892;&#24314;&#27169;&#12290;&#25105;&#20204;&#36890;&#36807;&#24320;&#21457;&#19968;&#20010;DRL&#36807;&#31243;&#26469;&#35757;&#32451;&#31616;&#27905;&#30340;&#8220;&#28508;&#22312;&#8221;&#31574;&#30053;&#20197;&#21450;&#20851;&#20110;&#20854;&#24615;&#33021;&#30340;PAC&#20445;&#35777;&#26469;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#12290;&#19982;&#20808;&#21069;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#35268;&#36991;&#20102;&#27169;&#22411;&#25552;&#28860;&#27493;&#39588;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23545;&#25239;DRL&#20013;&#30340;&#31232;&#30095;&#22870;&#21169;&#65292;&#24182;&#23454;&#29616;&#20102;&#20302;&#32423;&#31574;&#30053;&#30340;&#21487;&#37325;&#29992;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#20010;&#28041;&#21450;&#20195;&#29702;&#23548;&#33322;&#30340;&#26696;&#20363;&#30740;&#31350;&#35777;&#26126;&#20102;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13785v1 Announce Type: new  Abstract: We propose a novel approach to the problem of controller design for environments modeled as Markov decision processes (MDPs). Specifically, we consider a hierarchical MDP a graph with each vertex populated by an MDP called a "room". We first apply deep reinforcement learning (DRL) to obtain low-level policies for each room, scaling to large rooms of unknown structure. We then apply reactive synthesis to obtain a high-level planner that chooses which low-level policy to execute in each room. The central challenge in synthesizing the planner is the need for modeling rooms. We address this challenge by developing a DRL procedure to train concise "latent" policies together with PAC guarantees on their performance. Unlike previous approaches, ours circumvents a model distillation step. Our approach combats sparse rewards in DRL and enables reusability of low-level policies. We demonstrate feasibility in a case study involving agent navigation
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#27010;&#29575;&#21644;&#31070;&#32463;&#31526;&#21495;&#36923;&#36753;&#32534;&#31243;&#30340;&#32479;&#19968;&#20195;&#25968;&#35270;&#35282;&#65292;&#23558;&#35768;&#22810;PLP&#30340;&#25193;&#23637;&#37117;&#32435;&#20837;&#19968;&#20010;&#20849;&#21516;&#30340;&#20195;&#25968;&#36923;&#36753;&#32534;&#31243;&#26694;&#26550;&#20013;&#12290;</title><link>https://arxiv.org/abs/2402.13782</link><description>&lt;p&gt;
&#29992;&#20110;&#27010;&#29575;&#21644;&#31070;&#32463;&#31526;&#21495;&#36923;&#36753;&#32534;&#31243;&#30340;&#21322;&#29615;
&lt;/p&gt;
&lt;p&gt;
Semirings for Probabilistic and Neuro-Symbolic Logic Programming
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13782
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#27010;&#29575;&#21644;&#31070;&#32463;&#31526;&#21495;&#36923;&#36753;&#32534;&#31243;&#30340;&#32479;&#19968;&#20195;&#25968;&#35270;&#35282;&#65292;&#23558;&#35768;&#22810;PLP&#30340;&#25193;&#23637;&#37117;&#32435;&#20837;&#19968;&#20010;&#20849;&#21516;&#30340;&#20195;&#25968;&#36923;&#36753;&#32534;&#31243;&#26694;&#26550;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27010;&#29575;&#36923;&#36753;&#32534;&#31243;(PLP)&#39046;&#22495;&#33268;&#21147;&#20110;&#23558;&#27010;&#29575;&#27169;&#22411;&#38598;&#25104;&#21040;&#22522;&#20110;&#36923;&#36753;&#30340;&#32534;&#31243;&#35821;&#35328;&#20013;&#12290;&#22312;&#36807;&#21435;&#30340;30&#24180;&#20013;&#65292;&#24050;&#32463;&#24320;&#21457;&#20986;&#35768;&#22810;&#29992;&#20110;&#22312;&#27010;&#29575;&#36923;&#36753;&#31243;&#24207;&#20013;&#24314;&#27169;&#12289;&#25512;&#29702;&#21644;&#23398;&#20064;&#30340;&#35821;&#35328;&#21644;&#26694;&#26550;&#12290;&#23613;&#31649;&#26368;&#21021;PLP&#19987;&#27880;&#20110;&#31163;&#25955;&#27010;&#29575;&#65292;&#20294;&#26356;&#36817;&#26399;&#30340;&#26041;&#27861;&#24050;&#32463;&#23558;&#36830;&#32493;&#20998;&#24067;&#20197;&#21450;&#31070;&#32463;&#32593;&#32476;&#32435;&#20837;&#20854;&#20013;&#65292;&#26377;&#25928;&#22320;&#20135;&#29983;&#20102;&#31070;&#32463;&#31526;&#21495;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#20195;&#25968;&#36879;&#35270;&#26469;&#30475;&#24453;PLP&#65292;&#34920;&#26126;&#35768;&#22810;PLP&#30340;&#25193;&#23637;&#21487;&#20197;&#22312;&#19968;&#20010;&#20849;&#21516;&#30340;&#20195;&#25968;&#36923;&#36753;&#32534;&#31243;&#26694;&#26550;&#20869;&#36827;&#34892;&#36716;&#25442;&#65292;&#20854;&#20013;&#20107;&#23454;&#34987;&#26631;&#35760;&#20026;&#21322;&#29615;&#30340;&#20803;&#32032;&#65292;&#32780;&#26512;&#21462;&#21644;&#21512;&#21462;&#34987;&#26367;&#25442;&#20026;&#21152;&#27861;&#21644;&#20056;&#27861;&#12290;&#36825;&#19981;&#20165;&#36866;&#29992;&#20110;PLP&#30340;&#21464;&#20307;&#26412;&#36523;&#65292;&#20063;&#36866;&#29992;&#20110;&#22522;&#20110;(&#20195;&#25968;)&#27169;&#22411;&#35745;&#25968;&#30340;&#22522;&#30784;&#25191;&#34892;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13782v1 Announce Type: new  Abstract: The field of probabilistic logic programming (PLP) focuses on integrating probabilistic models into programming languages based on logic. Over the past 30 years, numerous languages and frameworks have been developed for modeling, inference and learning in probabilistic logic programs. While originally PLP focused on discrete probability, more recent approaches have incorporated continuous distributions as well as neural networks, effectively yielding neural-symbolic methods. We provide a unified algebraic perspective on PLP, showing that many if not most of the extensions of PLP can be cast within a common algebraic logic programming framework, in which facts are labeled with elements of a semiring and disjunction and conjunction are replaced by addition and multiplication. This does not only hold for the PLP variations itself but also for the underlying execution mechanism that is based on (algebraic) model counting.
&lt;/p&gt;</description></item><item><title>REMO&#26159;&#19968;&#20010;&#33258;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#24120;&#35265;&#21270;&#23398;&#20013;&#26126;&#30830;&#23450;&#20041;&#30340;&#21407;&#23376;&#32452;&#21512;&#35268;&#21017;&#65292;&#22312;1.7&#30334;&#19975;&#20010;&#24050;&#30693;&#21270;&#23398;&#21453;&#24212;&#19978;&#39044;&#35757;&#32451;&#22270;&#24418;/Transformer&#32534;&#30721;&#22120;&#65292;&#24182;&#25552;&#20986;&#20102;Masked Reaction Centre Reconstruction (MRCR)&#21644;Reaction Centre Identification (RCI)&#20004;&#20010;&#39044;&#35757;&#32451;&#30446;&#26631;&#65292;&#20026;&#20998;&#23376;&#34920;&#31034;&#23398;&#20064;&#25552;&#20379;&#20102;&#26032;&#39062;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2402.13779</link><description>&lt;p&gt;
&#20174;&#21270;&#23398;&#21453;&#24212;&#30693;&#35782;&#20013;&#23398;&#20064;&#19978;&#19979;&#25991;&#20998;&#23376;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Contextual Molecule Representation Learning from Chemical Reaction Knowledge
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13779
&lt;/p&gt;
&lt;p&gt;
REMO&#26159;&#19968;&#20010;&#33258;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#24120;&#35265;&#21270;&#23398;&#20013;&#26126;&#30830;&#23450;&#20041;&#30340;&#21407;&#23376;&#32452;&#21512;&#35268;&#21017;&#65292;&#22312;1.7&#30334;&#19975;&#20010;&#24050;&#30693;&#21270;&#23398;&#21453;&#24212;&#19978;&#39044;&#35757;&#32451;&#22270;&#24418;/Transformer&#32534;&#30721;&#22120;&#65292;&#24182;&#25552;&#20986;&#20102;Masked Reaction Centre Reconstruction (MRCR)&#21644;Reaction Centre Identification (RCI)&#20004;&#20010;&#39044;&#35757;&#32451;&#30446;&#26631;&#65292;&#20026;&#20998;&#23376;&#34920;&#31034;&#23398;&#20064;&#25552;&#20379;&#20102;&#26032;&#39062;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#33258;&#30417;&#30563;&#23398;&#20064;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#24378;&#22823;&#30340;&#24037;&#20855;&#65292;&#29992;&#20110;&#21033;&#29992;&#20016;&#23500;&#30340;&#26410;&#26631;&#35760;&#25968;&#25454;&#36827;&#34892;&#34920;&#31034;&#23398;&#20064;&#65292;&#24182;&#24050;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#20010;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#24403;&#24212;&#29992;&#20110;&#20998;&#23376;&#34920;&#31034;&#23398;&#20064;&#65288;MRL&#65289;&#26102;&#65292;&#27969;&#34892;&#30340;&#25216;&#26415;(&#22914;&#25513;&#30721;&#20122;&#21333;&#20301;&#37325;&#24314;)&#24448;&#24448;&#34920;&#29616;&#19981;&#20339;&#65292;&#36825;&#26159;&#22240;&#20026;&#20998;&#23376;&#20013;&#21487;&#33021;&#30340;&#21407;&#23376;&#32452;&#21512;&#26041;&#24335;&#33258;&#30001;&#24230;&#36739;&#39640;&#65292;&#32473;&#25513;&#30721;&#37325;&#24314;&#33539;&#24335;&#24102;&#26469;&#20102;&#38590;&#20197;&#36926;&#36234;&#30340;&#22797;&#26434;&#24615;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;REMO&#65292;&#36825;&#26159;&#19968;&#20010;&#33258;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#65292;&#21033;&#29992;&#24120;&#35265;&#21270;&#23398;&#20013;&#30340;&#26126;&#30830;&#23450;&#20041;&#30340;&#21407;&#23376;&#32452;&#21512;&#35268;&#21017;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;REMO&#22312;&#25991;&#29486;&#20013;&#24050;&#30693;&#30340;170&#19975;&#20010;&#21270;&#23398;&#21453;&#24212;&#19978;&#36827;&#34892;&#20102;&#22270;/&#21464;&#25442;&#22120;&#32534;&#30721;&#22120;&#30340;&#39044;&#35757;&#32451;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#39044;&#35757;&#32451;&#30446;&#26631;: &#25513;&#30721;&#21453;&#24212;&#20013;&#24515;&#37325;&#24314;&#65288;MRCR&#65289;&#21644;&#21453;&#24212;&#20013;&#24515;&#35782;&#21035;&#65288;RCI&#65289;&#12290;REMO&#36890;&#36807;&#21033;&#29992;&#26032;&#39062;&#30340;&#35299;&#20915;&#26041;&#26696;&#22312;MRL&#26041;&#38754;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13779v1 Announce Type: cross  Abstract: In recent years, self-supervised learning has emerged as a powerful tool to harness abundant unlabelled data for representation learning and has been broadly adopted in diverse areas. However, when applied to molecular representation learning (MRL), prevailing techniques such as masked sub-unit reconstruction often fall short, due to the high degree of freedom in the possible combinations of atoms within molecules, which brings insurmountable complexity to the masking-reconstruction paradigm. To tackle this challenge, we introduce REMO, a self-supervised learning framework that takes advantage of well-defined atom-combination rules in common chemistry. Specifically, REMO pre-trains graph/Transformer encoders on 1.7 million known chemical reactions in the literature. We propose two pre-training objectives: Masked Reaction Centre Reconstruction (MRCR) and Reaction Centre Identification (RCI). REMO offers a novel solution to MRL by exploi
&lt;/p&gt;</description></item><item><title>&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#22312;&#31163;&#32447;&#31574;&#30053;&#23398;&#20064;&#20013;&#23637;&#29616;&#20102;&#24040;&#22823;&#28508;&#21147;&#65292;&#26412;&#25991;&#25552;&#20379;&#20102;&#39318;&#20010;&#31995;&#32479;&#24615;&#32508;&#36848;&#65292;&#28085;&#30422;&#20102;&#20116;&#31181;&#20027;&#27969;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#21450;&#20854;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.13777</link><description>&lt;p&gt;
&#31163;&#32447;&#31574;&#30053;&#23398;&#20064;&#30340;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65306;&#25945;&#31243;&#12289;&#35843;&#26597;&#21644;&#26410;&#26469;&#26041;&#21521;&#23637;&#26395;
&lt;/p&gt;
&lt;p&gt;
Deep Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future Directions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13777
&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#22312;&#31163;&#32447;&#31574;&#30053;&#23398;&#20064;&#20013;&#23637;&#29616;&#20102;&#24040;&#22823;&#28508;&#21147;&#65292;&#26412;&#25991;&#25552;&#20379;&#20102;&#39318;&#20010;&#31995;&#32479;&#24615;&#32508;&#36848;&#65292;&#28085;&#30422;&#20102;&#20116;&#31181;&#20027;&#27969;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#21450;&#20854;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;(DGMs)&#22312;&#21508;&#20010;&#39046;&#22495;&#23637;&#31034;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#29305;&#21035;&#26159;&#22312;&#20351;&#29992;&#20174;&#31163;&#32447;&#25968;&#25454;&#35757;&#32451;&#30340;&#27169;&#22411;&#29983;&#25104;&#25991;&#26412;&#12289;&#22270;&#20687;&#21644;&#35270;&#39057;&#26041;&#38754;&#12290;&#31867;&#20284;&#22320;&#65292;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#20915;&#31574;&#21644;&#26426;&#22120;&#20154;&#25511;&#21046;&#20063;&#38656;&#35201;&#20174;&#31163;&#32447;&#25968;&#25454;&#20013;&#23398;&#20064;&#19968;&#20010;&#29983;&#25104;&#20989;&#25968;&#20316;&#20026;&#31574;&#30053;&#25110;&#25919;&#31574;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#23558;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#24212;&#29992;&#20110;&#31163;&#32447;&#31574;&#30053;&#23398;&#20064;&#23637;&#29616;&#20986;&#24040;&#22823;&#28508;&#21147;&#65292;&#35768;&#22810;&#30740;&#31350;&#22312;&#36825;&#20010;&#26041;&#21521;&#19978;&#36827;&#34892;&#20102;&#25506;&#32034;&#12290;&#28982;&#32780;&#65292;&#36825;&#19968;&#39046;&#22495;&#20173;&#28982;&#32570;&#20047;&#20840;&#38754;&#30340;&#35780;&#20272;&#65292;&#22240;&#27492;&#19981;&#21516;&#20998;&#25903;&#30340;&#21457;&#23637;&#30456;&#23545;&#29420;&#31435;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#22312;&#31163;&#32447;&#31574;&#30053;&#23398;&#20064;&#24212;&#29992;&#26041;&#38754;&#30340;&#31532;&#19968;&#27425;&#31995;&#32479;&#24615;&#32508;&#36848;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#28085;&#30422;&#20102;&#20116;&#31181;&#20027;&#27969;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65292;&#21253;&#25324;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#12289;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#12289;&#24402;&#19968;&#21270;&#27969;&#12289;&#21464;&#21387;&#22120;&#21644;&#25193;&#25955;&#27169;&#22411;&#65292;&#20197;&#21450;&#23427;&#20204;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13777v1 Announce Type: cross  Abstract: Deep generative models (DGMs) have demonstrated great success across various domains, particularly in generating texts, images, and videos using models trained from offline data. Similarly, data-driven decision-making and robotic control also necessitate learning a generator function from the offline data to serve as the strategy or policy. In this case, applying deep generative models in offline policy learning exhibits great potential, and numerous studies have explored in this direction. However, this field still lacks a comprehensive review and so developments of different branches are relatively independent. Thus, we provide the first systematic review on the applications of deep generative models for offline policy learning. In particular, we cover five mainstream deep generative models, including Variational Auto-Encoders, Generative Adversarial Networks, Normalizing Flows, Transformers, and Diffusion Models, and their applicati
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#23457;&#35745;&#20102;&#21830;&#19994;&#21644;&#24320;&#28304;&#20154;&#33080;&#35782;&#21035;&#31995;&#32479;&#22312;&#25140;&#21475;&#32617;&#20154;&#33080;&#20877;&#35782;&#21035;&#20013;&#30340;&#20559;&#35265;&#65292;&#25581;&#31034;&#20102;&#36825;&#20123;&#31995;&#32479;&#22312;&#24212;&#23545;&#25140;&#21475;&#32617;&#24102;&#26469;&#30340;&#25361;&#25112;&#26041;&#38754;&#30340;&#19981;&#36275;&#12290;</title><link>https://arxiv.org/abs/2402.13771</link><description>&lt;p&gt;
Mask-up: &#25506;&#31350;&#25140;&#21475;&#32617;&#20154;&#33080;&#20877;&#35782;&#21035;&#20013;&#30340;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Mask-up: Investigating Biases in Face Re-identification for Masked Faces
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13771
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#23457;&#35745;&#20102;&#21830;&#19994;&#21644;&#24320;&#28304;&#20154;&#33080;&#35782;&#21035;&#31995;&#32479;&#22312;&#25140;&#21475;&#32617;&#20154;&#33080;&#20877;&#35782;&#21035;&#20013;&#30340;&#20559;&#35265;&#65292;&#25581;&#31034;&#20102;&#36825;&#20123;&#31995;&#32479;&#22312;&#24212;&#23545;&#25140;&#21475;&#32617;&#24102;&#26469;&#30340;&#25361;&#25112;&#26041;&#38754;&#30340;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
AI&#22522;&#20110;&#30340;&#20154;&#33080;&#35782;&#21035;&#31995;&#32479;&#65288;FRSs&#65289;&#30446;&#21069;&#24050;&#34987;&#24191;&#27867;&#20998;&#21457;&#65292;&#24182;&#37096;&#32626;&#20026;&#36941;&#24067;&#20840;&#29699;&#30340;MLaaS&#35299;&#20915;&#26041;&#26696;&#65292;&#23588;&#20854;&#26159;&#33258;COVID-19&#22823;&#27969;&#34892;&#20197;&#26469;&#65292;&#29992;&#20110;&#39564;&#35777;&#20010;&#20154;&#36141;&#20080;SIM&#21345;&#26102;&#30340;&#38754;&#37096;&#65292;&#21040;&#23545;&#20844;&#27665;&#36827;&#34892;&#30417;&#35270;&#12290;&#24050;&#32463;&#25253;&#21578;&#20102;&#36825;&#20123;&#31995;&#32479;&#23545;&#36793;&#32536;&#21270;&#32676;&#20307;&#23384;&#22312;&#24191;&#27867;&#30340;&#20559;&#35265;&#65292;&#24182;&#23548;&#33268;&#39640;&#24230;&#27495;&#35270;&#24615;&#30340;&#32467;&#26524;&#12290;&#21518;&#30123;&#24773;&#26102;&#20195;&#24050;&#32463;&#20351;&#25140;&#21475;&#32617;&#21464;&#24471;&#27491;&#24120;&#65292;&#20294;FRSs&#24182;&#27809;&#26377;&#36319;&#19978;&#28526;&#27969;&#12290;&#22240;&#27492;&#65292;&#36825;&#20123;&#31995;&#32479;&#23481;&#26131;&#21463;&#21040;&#22522;&#20110;&#21475;&#32617;&#30340;&#33080;&#37096;&#36974;&#30422;&#30340;&#24433;&#21709;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23457;&#35745;&#20102;&#22235;&#20010;&#21830;&#19994;FRSs&#21644;&#20061;&#20010;&#24320;&#28304;FRSs&#65292;&#29992;&#20110;&#22312;&#20116;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#65288;&#20849;14,722&#24352;&#22270;&#20687;&#65289;&#20043;&#38388;&#23545;&#19981;&#21516;&#31867;&#22411;&#30340;&#25140;&#21475;&#32617;&#21644;&#19981;&#25140;&#21475;&#32617;&#22270;&#20687;&#36827;&#34892;&#20154;&#33080;&#20877;&#35782;&#21035;&#30340;&#20219;&#21153;&#12290;&#36825;&#20123;&#25968;&#25454;&#38598;&#27169;&#25311;&#20102;&#37096;&#32626;&#22312;&#20840;&#29699;&#20027;&#35201;&#22269;&#23478;&#20013;&#30340;&#29616;&#23454;&#39564;&#35777;/&#30417;&#35270;&#20219;&#21153;&#12290;&#20854;&#20013;&#19977;&#20010;&#21830;&#19994;&#21644;&#20116;&#20010;&#24320;&#28304;&#30340;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13771v1 Announce Type: cross  Abstract: AI based Face Recognition Systems (FRSs) are now widely distributed and deployed as MLaaS solutions all over the world, moreso since the COVID-19 pandemic for tasks ranging from validating individuals' faces while buying SIM cards to surveillance of citizens. Extensive biases have been reported against marginalized groups in these systems and have led to highly discriminatory outcomes. The post-pandemic world has normalized wearing face masks but FRSs have not kept up with the changing times. As a result, these systems are susceptible to mask based face occlusion. In this study, we audit four commercial and nine open-source FRSs for the task of face re-identification between different varieties of masked and unmasked images across five benchmark datasets (total 14,722 images). These simulate a realistic validation/surveillance task as deployed in all major countries around the world. Three of the commercial and five of the open-source 
&lt;/p&gt;</description></item><item><title>CriticBench&#26159;&#19968;&#20010;&#26088;&#22312;&#20840;&#38754;&#21644;&#21487;&#38752;&#22320;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35780;&#35770;&#33021;&#21147;&#30340;&#26032;&#22411;&#22522;&#20934;&#65292;&#23637;&#31034;&#20102;&#35780;&#35770;&#33021;&#21147;&#19982;&#20219;&#21153;&#12289;&#21709;&#24212;&#36136;&#37327;&#21644;&#27169;&#22411;&#35268;&#27169;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2402.13764</link><description>&lt;p&gt;
CriticBench: &#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#35780;&#35770;&#23478;&#36827;&#34892;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
CriticBench: Evaluating Large Language Models as Critic
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13764
&lt;/p&gt;
&lt;p&gt;
CriticBench&#26159;&#19968;&#20010;&#26088;&#22312;&#20840;&#38754;&#21644;&#21487;&#38752;&#22320;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35780;&#35770;&#33021;&#21147;&#30340;&#26032;&#22411;&#22522;&#20934;&#65292;&#23637;&#31034;&#20102;&#35780;&#35770;&#33021;&#21147;&#19982;&#20219;&#21153;&#12289;&#21709;&#24212;&#36136;&#37327;&#21644;&#27169;&#22411;&#35268;&#27169;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25552;&#20986;&#20102; CriticBench&#65292;&#36825;&#26159;&#19968;&#20010;&#26088;&#22312;&#20840;&#38754;&#21644;&#21487;&#38752;&#22320;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#22235;&#20010;&#20851;&#38190;&#35780;&#35770;&#33021;&#21147;&#32500;&#24230;&#65288;&#21453;&#39304;&#12289;&#27604;&#36739;&#12289;&#25913;&#36827;&#21644;&#20803;&#21453;&#39304;&#65289;&#30340;&#26032;&#22411;&#22522;&#20934;&#12290;CriticBench&#21253;&#21547;&#20061;&#20010;&#19981;&#21516;&#30340;&#20219;&#21153;&#65292;&#27599;&#20010;&#20219;&#21153;&#35780;&#20272;LLMs&#22312;&#19981;&#21516;&#36136;&#37327;&#32454;&#31890;&#24230;&#27700;&#24179;&#19978;&#35780;&#35770;&#21709;&#24212;&#30340;&#33021;&#21147;&#12290;&#23545;&#24320;&#28304;&#21644;&#38381;&#28304;LLMs&#36827;&#34892;&#30340;&#24191;&#27867;&#35780;&#20272;&#25581;&#31034;&#20102;&#35780;&#35770;&#33021;&#21147;&#19982;&#20219;&#21153;&#12289;&#21709;&#24212;&#36136;&#37327;&#21644;&#27169;&#22411;&#35268;&#27169;&#20043;&#38388;&#26377;&#36259;&#30340;&#20851;&#31995;&#12290;CriticBench&#30340;&#25968;&#25454;&#38598;&#12289;&#36164;&#28304;&#21644;&#35780;&#20272;&#24037;&#20855;&#21253;&#23558;&#22312;https://github.com/gmftbyGMFTBY/Cri&#19978;&#20844;&#24320;&#21457;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13764v1 Announce Type: cross  Abstract: Critique ability are crucial in the scalable oversight and self-improvement of Large Language Models (LLMs). While many recent studies explore the critique ability of LLMs to judge and refine flaws in generations, how to comprehensively and reliably measure the critique abilities of LLMs is under-explored. This paper introduces \shortname, a novel benchmark designed to comprehensively and reliably evaluate four key critique ability dimensions of LLMs: feedback, comparison, refinement and meta-feedback. \shortname~encompasses nine diverse tasks, each assessing the LLMs' ability to critique responses at varying levels of quality granularity. Our extensive evaluations of open-source and closed-source LLMs reveal intriguing relationships between the critique ability and tasks, response qualities, and model scales. Datasets, resources and evaluation toolkit for \shortname~will be publicly released at \url{https://github.com/gmftbyGMFTBY/Cri
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#33258;&#21160;&#25628;&#32034;&#21464;&#20998;&#30005;&#36335;&#30340;&#26368;&#20339;&#32467;&#26500;&#65292;&#25913;&#21892;&#20102;VQAs&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.13754</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#36741;&#21161;&#30340;&#21464;&#20998;&#37327;&#23376;&#31639;&#27861;&#37327;&#23376;&#26550;&#26500;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning-assisted quantum architecture search for variational quantum algorithms
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13754
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#33258;&#21160;&#25628;&#32034;&#21464;&#20998;&#30005;&#36335;&#30340;&#26368;&#20339;&#32467;&#26500;&#65292;&#25913;&#21892;&#20102;VQAs&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22024;&#26434;&#20013;&#31561;&#35268;&#27169;&#37327;&#23376;&#65288;NISQ&#65289;&#26102;&#20195;&#65292;&#19968;&#20010;&#37325;&#35201;&#38556;&#30861;&#26159;&#30830;&#23450;&#21151;&#33021;&#24615;&#37327;&#23376;&#30005;&#36335;&#12290;&#36825;&#20123;&#30005;&#36335;&#24517;&#39035;&#21516;&#26102;&#31526;&#21512;&#24403;&#21069;&#37327;&#23376;&#30828;&#20214;&#38480;&#21046;&#25152;&#26045;&#21152;&#30340;&#32422;&#26463;&#12290;&#21464;&#20998;&#37327;&#23376;&#31639;&#27861;&#65288;VQA&#65289;&#26159;&#19968;&#31867;&#37327;&#23376;-&#32463;&#20856;&#20248;&#21270;&#31639;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#24403;&#21069;&#21487;&#29992;&#37327;&#23376;&#35774;&#22791;&#20013;&#30340;&#36825;&#20123;&#25361;&#25112;&#12290;&#26412;&#35770;&#25991;&#20391;&#37325;&#20110;&#30005;&#36335;&#32467;&#26500;&#65292;&#36890;&#36807;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#33258;&#21160;&#25628;&#32034;&#21464;&#20998;&#30005;&#36335;&#30340;&#26368;&#20248;&#32467;&#26500;&#65292;&#25913;&#21892;&#20102;VQAs&#30340;&#24615;&#33021;&#12290;&#35770;&#25991;&#20869;&#36890;&#36807;&#35780;&#20272;&#30005;&#36335;&#30340;&#28145;&#24230;&#12289;&#38376;&#21644;&#21442;&#25968;&#30340;&#24635;&#25968;&#20197;&#21450;&#20934;&#30830;&#24615;&#26469;&#30830;&#23450;&#30005;&#36335;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13754v1 Announce Type: cross  Abstract: A significant hurdle in the noisy intermediate-scale quantum (NISQ) era is identifying functional quantum circuits. These circuits must also adhere to the constraints imposed by current quantum hardware limitations. Variational quantum algorithms (VQAs), a class of quantum-classical optimization algorithms, were developed to address these challenges in the currently available quantum devices. However, the overall performance of VQAs depends on the initialization strategy of the variational circuit, the structure of the circuit (also known as ansatz), and the configuration of the cost function. Focusing on the structure of the circuit, in this thesis, we improve the performance of VQAs by automating the search for an optimal structure for the variational circuits using reinforcement learning (RL). Within the thesis, the optimality of a circuit is determined by evaluating its depth, the overall count of gates and parameters, and its accu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23457;&#26597;&#20102;&#22914;&#20309;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#23545;&#29983;&#20135;&#32773;&#20860;&#28040;&#36153;&#32773;&#31038;&#21306;&#30340;&#30005;&#21147;&#36127;&#36733;&#36827;&#34892;&#39044;&#27979;&#30340;&#26041;&#27861;&#21644;&#21487;&#34892;&#24615;</title><link>https://arxiv.org/abs/2402.13752</link><description>&lt;p&gt;
AI-Powered Predictions for Electricity Load in Prosumer Communities
&lt;/p&gt;
&lt;p&gt;
AI-Powered Predictions for Electricity Load in Prosumer Communities
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13752
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23457;&#26597;&#20102;&#22914;&#20309;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#23545;&#29983;&#20135;&#32773;&#20860;&#28040;&#36153;&#32773;&#31038;&#21306;&#30340;&#30005;&#21147;&#36127;&#36733;&#36827;&#34892;&#39044;&#27979;&#30340;&#26041;&#27861;&#21644;&#21487;&#34892;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20303;&#23429;&#27004;&#23431;&#31038;&#21306;&#30340;&#30005;&#21147;&#28040;&#32791;&#21644;&#20135;&#20986;&#28789;&#27963;&#24615;&#65292;&#21253;&#25324;&#20855;&#26377;&#21487;&#20877;&#29983;&#33021;&#28304;&#21644;&#33021;&#28304;&#20648;&#23384;&#35774;&#26045;&#65288;&#20063;&#31216;&#20026;&#29983;&#20135;&#32773;&#20860;&#28040;&#36153;&#32773;&#65289;&#65292;&#21487;&#20197;&#36890;&#36807;&#20808;&#36827;&#30340;&#30701;&#26399;&#38656;&#27714;&#21709;&#24212;&#26426;&#21046;&#24471;&#21040;&#26377;&#25928;&#21033;&#29992;&#12290;&#20247;&#25152;&#21608;&#30693;&#65292;&#22914;&#26524;&#22312;&#29983;&#20135;&#32773;&#20860;&#28040;&#36153;&#32773;&#31038;&#21306;&#23618;&#38754;&#36827;&#34892;&#38656;&#27714;&#21709;&#24212;&#65292;&#28789;&#27963;&#24615;&#23601;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#65292;&#22240;&#20026;&#32858;&#21512;&#30340;&#32676;&#20307;&#21487;&#20197;&#26356;&#22909;&#22320;&#21327;&#35843;&#30005;&#21147;&#28040;&#36153;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#30701;&#26399;&#20248;&#21270;&#30340;&#25928;&#26524;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#21462;&#20915;&#20110;&#23545;&#27599;&#26635;&#24314;&#31569;&#29289;&#20197;&#21450;&#25972;&#20010;&#31038;&#21306;&#30340;&#30005;&#21147;&#36127;&#36733;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;&#30005;&#21147;&#36127;&#36733;&#26354;&#32447;&#20013;&#30340;&#32467;&#26500;&#21464;&#21270;&#21487;&#33021;&#19982;&#19981;&#21516;&#30340;&#22806;&#37096;&#22240;&#32032;&#30456;&#20851;&#32852;&#65292;&#20363;&#22914;&#22825;&#27668;&#26465;&#20214;&#12289;&#26085;&#21382;&#20449;&#24687;&#12289;&#26143;&#26399;&#20960;&#20197;&#21450;&#29992;&#25143;&#34892;&#20026;&#12290;&#26412;&#25991;&#23457;&#26597;&#20102;&#19968;&#31995;&#21015;&#24191;&#27867;&#30340;&#30005;&#21147;&#36127;&#36733;&#39044;&#27979;&#25216;&#26415;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13752v1 Announce Type: cross  Abstract: The flexibility in electricity consumption and production in communities of residential buildings, including those with renewable energy sources and energy storage (a.k.a., prosumers), can effectively be utilized through the advancement of short-term demand response mechanisms. It is known that flexibility can further be increased if demand response is performed at the level of communities of prosumers, since aggregated groups can better coordinate electricity consumption. However, the effectiveness of such short-term optimization is highly dependent on the accuracy of electricity load forecasts both for each building as well as for the whole community. Structural variations in the electricity load profile can be associated with different exogenous factors, such as weather conditions, calendar information and day of the week, as well as user behavior. In this paper, we review a wide range of electricity load forecasting techniques, tha
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20114;&#34917;&#30693;&#35782;&#22686;&#24378;&#25512;&#33616;&#31995;&#32479;&#65288;LLM-KERec&#65289;&#65292;&#36890;&#36807;&#24341;&#20837;&#23454;&#20307;&#25552;&#21462;&#22120;&#21644;&#26500;&#24314;&#20114;&#34917;&#30693;&#35782;&#22270;&#65292;&#35299;&#20915;&#20102;&#25512;&#33616;&#31995;&#32479;&#38590;&#20197;&#25429;&#25417;&#29992;&#25143;&#24847;&#22270;&#36716;&#21464;&#21644;&#36866;&#24212;&#26032;&#21830;&#21697;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.13750</link><description>&lt;p&gt;
&#25171;&#30772;&#38556;&#30861;&#65306;&#36890;&#36807;&#25512;&#29702;&#30693;&#35782;&#22270;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24037;&#19994;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Breaking the Barrier: Utilizing Large Language Models for Industrial Recommendation Systems through an Inferential Knowledge Graph
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13750
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20114;&#34917;&#30693;&#35782;&#22686;&#24378;&#25512;&#33616;&#31995;&#32479;&#65288;LLM-KERec&#65289;&#65292;&#36890;&#36807;&#24341;&#20837;&#23454;&#20307;&#25552;&#21462;&#22120;&#21644;&#26500;&#24314;&#20114;&#34917;&#30693;&#35782;&#22270;&#65292;&#35299;&#20915;&#20102;&#25512;&#33616;&#31995;&#32479;&#38590;&#20197;&#25429;&#25417;&#29992;&#25143;&#24847;&#22270;&#36716;&#21464;&#21644;&#36866;&#24212;&#26032;&#21830;&#21697;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#22312;&#30005;&#23376;&#21830;&#21153;&#32593;&#31449;&#21644;&#22312;&#32447;&#24179;&#21488;&#20013;&#34987;&#24191;&#27867;&#20351;&#29992;&#65292;&#20197;&#24212;&#23545;&#20449;&#24687;&#36807;&#36733;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#31995;&#32479;&#20027;&#35201;&#20381;&#36182;&#21382;&#21490;&#25968;&#25454;&#21644;&#29992;&#25143;&#21453;&#39304;&#65292;&#38590;&#20197;&#25429;&#25417;&#29992;&#25143;&#24847;&#22270;&#36716;&#21464;&#12290;&#26368;&#36817;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#30693;&#35782;&#24211;&#65288;KB&#65289;&#30340;&#27169;&#22411;&#26469;&#25972;&#21512;&#19987;&#23478;&#30693;&#35782;&#65292;&#20294;&#23427;&#20204;&#38590;&#20197;&#36866;&#24212;&#26032;&#21830;&#21697;&#21644;&#19981;&#26029;&#21457;&#23637;&#30340;&#30005;&#23376;&#21830;&#21153;&#29615;&#22659;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20114;&#34917;&#30693;&#35782;&#22686;&#24378;&#25512;&#33616;&#31995;&#32479;&#65288;LLM-KERec&#65289;&#12290;&#23427;&#24341;&#20837;&#20102;&#19968;&#20010;&#23454;&#20307;&#25552;&#21462;&#22120;&#65292;&#20174;&#21830;&#21697;&#21644;&#29992;&#25143;&#20449;&#24687;&#20013;&#25552;&#21462;&#32479;&#19968;&#27010;&#24565;&#26415;&#35821;&#12290;&#20026;&#20102;&#25552;&#20379;&#20855;&#26377;&#25104;&#26412;&#25928;&#30410;&#19988;&#21487;&#38752;&#30340;&#20808;&#39564;&#30693;&#35782;&#65292;&#26681;&#25454;&#23454;&#20307;&#30340;&#27969;&#34892;&#24230;&#21644;&#29305;&#23450;&#31574;&#30053;&#29983;&#25104;&#23454;&#20307;&#23545;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30830;&#23450;&#27599;&#20010;&#23454;&#20307;&#23545;&#20013;&#30340;&#20114;&#34917;&#20851;&#31995;&#65292;&#26500;&#24314;&#19968;&#20010;&#20114;&#34917;&#30693;&#35782;&#22270;&#12290;&#27492;&#22806;&#65292;&#19968;&#20010;&#26032;&#30340;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13750v1 Announce Type: cross  Abstract: Recommendation systems are widely used in e-commerce websites and online platforms to address information overload. However, existing systems primarily rely on historical data and user feedback, making it difficult to capture user intent transitions. Recently, Knowledge Base (KB)-based models are proposed to incorporate expert knowledge, but it struggle to adapt to new items and the evolving e-commerce environment. To address these challenges, we propose a novel Large Language Model based Complementary Knowledge Enhanced Recommendation System (LLM-KERec). It introduces an entity extractor that extracts unified concept terms from item and user information. To provide cost-effective and reliable prior knowledge, entity pairs are generated based on entity popularity and specific strategies. The large language model determines complementary relationships in each entity pair, constructing a complementary knowledge graph. Furthermore, a new 
&lt;/p&gt;</description></item><item><title>&#35774;&#35745;&#20102;&#34920;&#26684;&#25552;&#31034;&#20197;&#35299;&#20915;&#20851;&#31995;&#19977;&#20803;&#32452;&#25277;&#21462;&#20013;&#30340;&#25552;&#31034;&#35774;&#35745;&#21644;&#26679;&#26412;&#36873;&#25321;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.13741</link><description>&lt;p&gt;
&#20351;&#29992;&#34920;&#26684;&#25552;&#31034;&#35299;&#38145;&#20851;&#31995;&#19977;&#20803;&#32452;&#25277;&#21462;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Unlocking Instructive In-Context Learning with Tabular Prompting for Relational Triple Extraction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13741
&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#20102;&#34920;&#26684;&#25552;&#31034;&#20197;&#35299;&#20915;&#20851;&#31995;&#19977;&#20803;&#32452;&#25277;&#21462;&#20013;&#30340;&#25552;&#31034;&#35774;&#35745;&#21644;&#26679;&#26412;&#36873;&#25321;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#31995;&#19977;&#20803;&#32452;&#25277;&#21462;&#65288;RTE&#65289;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#21462;&#24471;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#34920;&#29616;&#65292;&#20294;&#20173;&#28982;&#38754;&#20020;&#20004;&#20010;&#20851;&#38190;&#25361;&#25112;&#65306;&#65288;1&#65289;&#22914;&#20309;&#35774;&#35745;&#26377;&#25928;&#30340;&#25552;&#31034;&#21644;&#65288;2&#65289;&#22914;&#20309;&#36873;&#25321;&#36866;&#24403;&#30340;&#28436;&#31034;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#26410;&#33021;&#36866;&#24403;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;&#19968;&#26041;&#38754;&#65292;&#23427;&#20204;&#36890;&#24120;&#23558;RTE&#20219;&#21153;&#37325;&#26032;&#23450;&#20041;&#20026;&#25991;&#26412;-&#25991;&#26412;&#25552;&#31034;&#26684;&#24335;&#65292;&#36825;&#26159;&#19981;&#33258;&#28982;&#30340;&#65292;&#23548;&#33268;&#22312;&#39044;&#35757;&#32451;&#26102;&#30340;&#36755;&#20986;&#26684;&#24335;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25512;&#26029;&#26102;&#38388;&#20043;&#38388;&#19981;&#21305;&#37197;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#23427;&#20204;&#21482;&#21033;&#29992;&#34920;&#38754;&#33258;&#28982;&#35821;&#35328;&#29305;&#24449;&#65292;&#32570;&#20047;&#22312;&#26679;&#26412;&#36873;&#25321;&#20013;&#32771;&#34385;&#19977;&#20803;&#32452;&#35821;&#20041;&#12290;&#36825;&#20123;&#38382;&#39064;&#38459;&#30861;&#20102;ICL&#23545;RTE&#24615;&#33021;&#30340;&#25913;&#36827;&#65292;&#22240;&#27492;&#25105;&#20204;&#26088;&#22312;&#21516;&#26102;&#35299;&#20915;&#25552;&#31034;&#35774;&#35745;&#21644;&#26679;&#26412;&#36873;&#25321;&#25361;&#25112;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#29992;&#20110;RTE&#30340;&#34920;&#26684;&#25552;&#31034;&#65288;TableIE&#65289;&#65292;&#23558;RTE&#20219;&#21153;&#26500;&#24314;&#25104;&#19968;&#20010;&#34920;&#26684;&#29983;&#25104;&#20219;&#21153;&#20197;&#35299;&#20915;&#19978;&#36848;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13741v1 Announce Type: cross  Abstract: The in-context learning (ICL) for relational triple extraction (RTE) has achieved promising performance, but still encounters two key challenges: (1) how to design effective prompts and (2) how to select proper demonstrations. Existing methods, however, fail to address these challenges appropriately. On the one hand, they usually recast RTE task to text-to-text prompting formats, which is unnatural and results in a mismatch between the output format at the pre-training time and the inference time for large language models (LLMs). On the other hand, they only utilize surface natural language features and lack consideration of triple semantics in sample selection. These issues are blocking improved performance in ICL for RTE, thus we aim to tackle prompt designing and sample selection challenges simultaneously. To this end, we devise a tabular prompting for RTE (\textsc{TableIE}) which frames RTE task into a table generation task to inco
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#23545;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#36864;&#21270;&#30693;&#35782;&#31070;&#32463;&#20803;&#65288;DKNs&#65289;&#30340;&#20840;&#38754;&#23450;&#20041;&#65292;&#24341;&#20837;&#20102;&#31070;&#32463;&#25299;&#25169;&#32858;&#31867;&#26041;&#27861;&#21644;&#31070;&#32463;&#36864;&#21270;&#20998;&#26512;&#26694;&#26550;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#20934;&#30830;&#30340;DKN&#33719;&#21462;&#12290;</title><link>https://arxiv.org/abs/2402.13731</link><description>&lt;p&gt;
&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#36798;&#33452;&#22855;&#23494;&#30721;&#65306;&#35299;&#35835;&#36864;&#21270;&#30693;&#35782;&#31070;&#32463;&#20803;
&lt;/p&gt;
&lt;p&gt;
The Da Vinci Code of Large Pre-trained Language Models: Deciphering Degenerate Knowledge Neurons
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13731
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#23545;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#36864;&#21270;&#30693;&#35782;&#31070;&#32463;&#20803;&#65288;DKNs&#65289;&#30340;&#20840;&#38754;&#23450;&#20041;&#65292;&#24341;&#20837;&#20102;&#31070;&#32463;&#25299;&#25169;&#32858;&#31867;&#26041;&#27861;&#21644;&#31070;&#32463;&#36864;&#21270;&#20998;&#26512;&#26694;&#26550;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#20934;&#30830;&#30340;DKN&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#20013;&#20107;&#23454;&#30693;&#35782;&#23384;&#20648;&#30340;&#26426;&#21046;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20107;&#23454;&#30693;&#35782;&#23384;&#20648;&#22312;&#22810;&#23618;&#24863;&#30693;&#22120;&#26435;&#37325;&#20013;&#65292;&#26576;&#20123;&#23384;&#20648;&#21333;&#20803;&#34920;&#29616;&#20986;&#36864;&#21270;&#24615;&#65292;&#31216;&#20026;&#36864;&#21270;&#30693;&#35782;&#31070;&#32463;&#20803;&#65288;Degenerate Knowledge Neurons, DKNs&#65289;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#28085;&#30422;&#32467;&#26500;&#21644;&#21151;&#33021;&#26041;&#38754;&#30340;DKNs&#20840;&#38754;&#23450;&#20041;&#65292;&#24320;&#21019;&#20102;&#23545;PLMs&#20107;&#23454;&#30693;&#35782;&#23384;&#20648;&#21333;&#20803;&#32467;&#26500;&#30340;&#30740;&#31350;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#31070;&#32463;&#25299;&#25169;&#32858;&#31867;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20801;&#35768;&#24418;&#25104;&#20219;&#24847;&#25968;&#37327;&#21644;&#32467;&#26500;&#30340;DKNs&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#20934;&#30830;&#30340;DKN&#33719;&#21462;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#31070;&#32463;&#36864;&#21270;&#20998;&#26512;&#26694;&#26550;&#65292;&#29420;&#29305;&#22320;&#25972;&#21512;&#20102;&#27169;&#22411;&#40065;&#26834;&#24615;&#12289;&#21487;&#36827;&#21270;&#24615;&#21644;&#22797;&#26434;&#24615;&#65292;&#20197;&#23545;PLMs&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#12290;&#22312;&#35813;&#26694;&#26550;&#20869;&#65292;&#25105;&#20204;&#25191;&#34892;&#20102;34&#20010;&#23454;&#39564;&#65292;&#36328;&#36234;2&#20010;PLMs&#12289;4&#20010;&#25968;&#25454;&#38598;&#21644;6&#20010;&#35774;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13731v1 Announce Type: cross  Abstract: This study explores the mechanism of factual knowledge storage in pre-trained language models (PLMs). Previous research suggests that factual knowledge is stored within multi-layer perceptron weights, and some storage units exhibit degeneracy, referred to as Degenerate Knowledge Neurons (DKNs). This paper provides a comprehensive definition of DKNs that covers both structural and functional aspects, pioneering the study of structures in PLMs' factual knowledge storage units. Based on this, we introduce the Neurological Topology Clustering method, which allows the formation of DKNs in any numbers and structures, leading to a more accurate DKN acquisition. Furthermore, we introduce the Neuro-Degeneracy Analytic Analysis Framework, which uniquely integrates model robustness, evolvability, and complexity for a holistic assessment of PLMs. Within this framework, our execution of 34 experiments across 2 PLMs, 4 datasets, and 6 settings highl
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#29289;&#20449;&#24687;&#23398;&#30740;&#31350;&#20013;&#23637;&#29616;&#20986;&#20102;&#28508;&#21147;&#65292;&#21487;&#25104;&#21151;&#22788;&#29702;&#22810;&#39033;&#20851;&#38190;&#20219;&#21153;&#65292;&#20294;&#22312;&#22797;&#26434;&#20219;&#21153;&#20013;&#20173;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.13714</link><description>&lt;p&gt;
&#29983;&#29289;&#20449;&#24687;&#23398;&#30740;&#31350;&#20013;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
An Evaluation of Large Language Models in Bioinformatics Research
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13714
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#29289;&#20449;&#24687;&#23398;&#30740;&#31350;&#20013;&#23637;&#29616;&#20986;&#20102;&#28508;&#21147;&#65292;&#21487;&#25104;&#21151;&#22788;&#29702;&#22810;&#39033;&#20851;&#38190;&#20219;&#21153;&#65292;&#20294;&#22312;&#22797;&#26434;&#20219;&#21153;&#20013;&#20173;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;ChatGPT&#22312;&#21508;&#20010;&#30740;&#31350;&#39046;&#22495;&#24341;&#36215;&#20102;&#26497;&#22823;&#20851;&#27880;&#12290;&#23427;&#20204;&#22312;&#25991;&#26412;&#23436;&#25104;&#21644;&#29983;&#25104;&#26041;&#38754;&#30340;&#26174;&#33879;&#33021;&#21147;&#24320;&#21019;&#20102;&#19968;&#31181;&#26032;&#30340;&#29992;&#20110;&#35299;&#20915;&#38382;&#39064;&#30340;&#35821;&#35328;&#30028;&#38754;&#33539;&#24335;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#29983;&#29289;&#20449;&#24687;&#23398;&#20013;&#30340;&#28508;&#21147;&#21644;&#21151;&#25928;&#23578;&#26410;&#23436;&#20840;&#25506;&#32034;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;LLMs&#22312;&#21508;&#31181;&#20851;&#38190;&#29983;&#29289;&#20449;&#24687;&#23398;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#12290;&#36825;&#20123;&#20219;&#21153;&#21253;&#25324;&#28508;&#22312;&#32534;&#30721;&#21306;&#22495;&#30340;&#35782;&#21035;&#65292;&#22522;&#22240;&#21644;&#34507;&#30333;&#36136;&#30340;&#21629;&#21517;&#23454;&#20307;&#25552;&#21462;&#65292;&#25239;&#24494;&#29983;&#29289;&#21644;&#25239;&#30284;&#32957;&#30340;&#26816;&#27979;&#65292;&#20998;&#23376;&#20248;&#21270;&#20197;&#21450;&#35299;&#20915;&#25945;&#32946;&#24615;&#29983;&#29289;&#20449;&#24687;&#23398;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#22312;&#32473;&#23450;&#36866;&#24403;&#25552;&#31034;&#30340;&#24773;&#20917;&#19979;&#65292;&#20687;GPT&#21464;&#31181;&#36825;&#26679;&#30340;LLMs&#21487;&#20197;&#25104;&#21151;&#22788;&#29702;&#22823;&#22810;&#25968;&#36825;&#20123;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545;&#23427;&#20204;&#22312;&#22797;&#26434;&#29983;&#29289;&#20449;&#24687;&#23398;&#20219;&#21153;&#32972;&#26223;&#19979;&#30340;&#38480;&#21046;&#36827;&#34892;&#20102;&#24443;&#24213;&#30340;&#20998;&#26512;&#12290;&#22312;&#32467;&#35770;&#37096;&#20998;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13714v1 Announce Type: cross  Abstract: Large language models (LLMs) such as ChatGPT have gained considerable interest across diverse research communities. Their notable ability for text completion and generation has inaugurated a novel paradigm for language-interfaced problem solving. However, the potential and efficacy of these models in bioinformatics remain incompletely explored. In this work, we study the performance LLMs on a wide spectrum of crucial bioinformatics tasks. These tasks include the identification of potential coding regions, extraction of named entities for genes and proteins, detection of antimicrobial and anti-cancer peptides, molecular optimization, and resolution of educational bioinformatics problems. Our findings indicate that, given appropriate prompts, LLMs like GPT variants can successfully handle most of these tasks. In addition, we provide a thorough analysis of their limitations in the context of complicated bioinformatics tasks. In conclusion
&lt;/p&gt;</description></item><item><title>DSLR&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35206;&#30422;&#33539;&#22260;&#30340;&#22810;&#26679;&#24615;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#22522;&#20110;&#37325;&#25773;&#30340;&#22270;&#25345;&#32493;&#23398;&#20064;&#20013;&#22238;&#25918;&#33410;&#28857;&#36807;&#20110;&#38598;&#20013;&#23548;&#33268;&#36807;&#25311;&#21512;&#21644;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.13711</link><description>&lt;p&gt;
DSLR&#65306;&#22810;&#26679;&#24615;&#22686;&#24378;&#21644;&#32467;&#26500;&#23398;&#20064;&#29992;&#20110;&#22522;&#20110;&#37325;&#25773;&#30340;&#22270;&#25345;&#32493;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
DSLR: Diversity Enhancement and Structure Learning for Rehearsal-based Graph Continual Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13711
&lt;/p&gt;
&lt;p&gt;
DSLR&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35206;&#30422;&#33539;&#22260;&#30340;&#22810;&#26679;&#24615;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#22522;&#20110;&#37325;&#25773;&#30340;&#22270;&#25345;&#32493;&#23398;&#20064;&#20013;&#22238;&#25918;&#33410;&#28857;&#36807;&#20110;&#38598;&#20013;&#23548;&#33268;&#36807;&#25311;&#21512;&#21644;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22522;&#20110;&#37325;&#25773;&#26041;&#27861;&#20013;&#22238;&#25918;&#32531;&#20914;&#21306;&#23545;&#22270;&#25345;&#32493;&#23398;&#20064;&#65288;GCL&#65289;&#26041;&#27861;&#30340;&#24433;&#21709;&#12290;&#29616;&#26377;&#30340;&#22522;&#20110;&#37325;&#25773;&#30340;GCL&#26041;&#27861;&#20026;&#27599;&#20010;&#31867;&#21035;&#36873;&#25321;&#26368;&#20855;&#20195;&#34920;&#24615;&#30340;&#33410;&#28857;&#24182;&#23558;&#23427;&#20204;&#23384;&#20648;&#22312;&#37325;&#25773;&#32531;&#20914;&#21306;&#20013;&#65292;&#20197;&#20379;&#22312;&#35757;&#32451;&#21518;&#32493;&#20219;&#21153;&#26102;&#20351;&#29992;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;&#20165;&#32771;&#34385;&#27599;&#20010;&#22238;&#25918;&#33410;&#28857;&#30340;&#31867;&#21035;&#20195;&#34920;&#24615;&#20250;&#20351;&#22238;&#25918;&#33410;&#28857;&#38598;&#20013;&#22312;&#27599;&#20010;&#31867;&#21035;&#30340;&#20013;&#24515;&#21608;&#22260;&#65292;&#21487;&#33021;&#23384;&#22312;&#36807;&#25311;&#21512;&#20110;&#20301;&#20110;&#37027;&#20123;&#21306;&#22495;&#30340;&#33410;&#28857;&#30340;&#39118;&#38505;&#65292;&#20174;&#32780;&#21152;&#21095;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#22522;&#20110;&#37325;&#25773;&#26041;&#27861;&#20005;&#37325;&#20381;&#36182;&#20110;&#23569;&#25968;&#22238;&#25918;&#33410;&#28857;&#26469;&#20445;&#30041;&#20174;&#20808;&#21069;&#20219;&#21153;&#20013;&#33719;&#24471;&#30340;&#30693;&#35782;&#65292;&#28041;&#21450;&#22312;&#27169;&#22411;&#35757;&#32451;&#20013;&#20855;&#26377;&#19981;&#30456;&#20851;&#37051;&#23621;&#30340;&#22238;&#25918;&#33410;&#28857;&#21487;&#33021;&#23545;&#27169;&#22411;&#24615;&#33021;&#20135;&#29983;&#26174;&#30528;&#30340;&#36127;&#38754;&#24433;&#21709;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DSLR&#30340;GCL&#27169;&#22411;&#65292;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;&#35206;&#30422;&#33539;&#22260;&#30340;&#22810;&#26679;&#24615;&#65288;CD&#65289;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13711v1 Announce Type: cross  Abstract: We investigate the replay buffer in rehearsal-based approaches for graph continual learning (GCL) methods. Existing rehearsal-based GCL methods select the most representative nodes for each class and store them in a replay buffer for later use in training subsequent tasks. However, we discovered that considering only the class representativeness of each replayed node makes the replayed nodes to be concentrated around the center of each class, incurring a potential risk of overfitting to nodes residing in those regions, which aggravates catastrophic forgetting. Moreover, as the rehearsal-based approach heavily relies on a few replayed nodes to retain knowledge obtained from previous tasks, involving the replayed nodes that have irrelevant neighbors in the model training may have a significant detrimental impact on model performance. In this paper, we propose a GCL model named DSLR, specifically, we devise a coverage-based diversity (CD)
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;SaGE&#26041;&#27861;&#65292;&#36890;&#36807;&#35821;&#20041;&#22270;&#29109;&#26469;&#34913;&#37327;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36947;&#24503;&#19968;&#33268;&#24615;&#65292;&#26500;&#24314;&#20102;MCC&#35821;&#26009;&#24211;&#12290;</title><link>https://arxiv.org/abs/2402.13709</link><description>&lt;p&gt;
SaGE&#65306;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36947;&#24503;&#19968;&#33268;&#24615;
&lt;/p&gt;
&lt;p&gt;
SaGE: Evaluating Moral Consistency in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13709
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;SaGE&#26041;&#27861;&#65292;&#36890;&#36807;&#35821;&#20041;&#22270;&#29109;&#26469;&#34913;&#37327;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36947;&#24503;&#19968;&#33268;&#24615;&#65292;&#26500;&#24314;&#20102;MCC&#35821;&#26009;&#24211;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#26368;&#36817;&#23637;&#31034;&#20986;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20250;&#35805;&#31995;&#32479;&#20013;&#30340;&#21360;&#35937;&#28145;&#21051;&#33021;&#21147;&#65292;&#20294;&#25105;&#20204;&#34920;&#26126;&#21363;&#20351;&#26159;&#26368;&#20808;&#36827;&#30340;LLMs&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#20063;&#23384;&#22312;&#36947;&#24503;&#19981;&#19968;&#33268;&#65292;&#23545;&#20854;&#21487;&#38752;&#24615;&#65288;&#20197;&#21450;&#24635;&#20307;&#21487;&#20449;&#36182;&#24615;&#65289;&#25552;&#20986;&#20102;&#36136;&#30097;&#12290;&#20197;&#24448;&#22312;LLM&#35780;&#20272;&#39046;&#22495;&#30340;&#24037;&#20316;&#20391;&#37325;&#20110;&#24320;&#21457;&#22320;&#38754;&#30495;&#23454;&#25968;&#25454;&#65292;&#20197;&#34913;&#37327;&#22312;&#29305;&#23450;&#20219;&#21153;&#19978;&#30340;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#36947;&#24503;&#24773;&#26223;&#24448;&#24448;&#32570;&#20047;&#26222;&#36941;&#35748;&#21516;&#31572;&#26696;&#30340;&#24773;&#20917;&#65292;&#27169;&#22411;&#21709;&#24212;&#30340;&#19968;&#33268;&#24615;&#23545;&#20110;&#20854;&#21487;&#38752;&#24615;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20449;&#24687;&#29702;&#35770;&#24230;&#37327;&#26041;&#27861;&#65292;&#31216;&#20026;&#35821;&#20041;&#22270;&#29109;&#65288;SaGE&#65289;&#65292;&#22522;&#20110;&#8220;&#32463;&#39564;&#27861;&#21017;&#8221;&#65288;RoTs&#65289;&#30340;&#27010;&#24565;&#26469;&#34913;&#37327;&#27169;&#22411;&#30340;&#36947;&#24503;&#19968;&#33268;&#24615;&#12290;RoTs&#26159;&#27169;&#22411;&#23398;&#20064;&#21040;&#30340;&#25277;&#35937;&#21407;&#21017;&#65292;&#21487;&#26377;&#25928;&#24110;&#21161;&#35299;&#37322;&#20854;&#20915;&#31574;&#31574;&#30053;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#36947;&#24503;&#19968;&#33268;&#24615;&#35821;&#26009;&#24211;&#65288;MCC&#65289;&#65292;&#21253;&#21547;50K&#20010;&#36947;&#24503;&#38382;&#39064;&#12289;&#22238;&#31572;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13709v1 Announce Type: cross  Abstract: Despite recent advancements showcasing the impressive capabilities of Large Language Models (LLMs) in conversational systems, we show that even state-of-the-art LLMs are morally inconsistent in their generations, questioning their reliability (and trustworthiness in general). Prior works in LLM evaluation focus on developing ground-truth data to measure accuracy on specific tasks. However, for moral scenarios that often lack universally agreed-upon answers, consistency in model responses becomes crucial for their reliability. To address this issue, we propose an information-theoretic measure called Semantic Graph Entropy (SaGE), grounded in the concept of "Rules of Thumb" (RoTs) to measure a model's moral consistency. RoTs are abstract principles learned by a model and can help explain their decision-making strategies effectively. To this extent, we construct the Moral Consistency Corpus (MCC), containing 50K moral questions, responses
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#38024;&#23545;SemEval-2024&#20219;&#21153;8&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24494;&#35843;LLMs&#36827;&#34892;&#22810;&#35821;&#35328;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22810;&#31181;&#26041;&#24335;&#22788;&#29702;&#35813;&#20219;&#21153;&#24182;&#23558;&#32479;&#35745;&#26816;&#27979;&#25351;&#26631;&#19982;&#27169;&#22411;&#39044;&#27979;&#30456;&#32467;&#21512;&#65292;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.13671</link><description>&lt;p&gt;
KInIT&#21442;&#21152;SemEval-2024&#20219;&#21153;8&#65306;&#38024;&#23545;&#22810;&#35821;&#35328;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#30340;&#24494;&#35843;LLMs
&lt;/p&gt;
&lt;p&gt;
KInIT at SemEval-2024 Task 8: Fine-tuned LLMs for Multilingual Machine-Generated Text Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13671
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#38024;&#23545;SemEval-2024&#20219;&#21153;8&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24494;&#35843;LLMs&#36827;&#34892;&#22810;&#35821;&#35328;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22810;&#31181;&#26041;&#24335;&#22788;&#29702;&#35813;&#20219;&#21153;&#24182;&#23558;&#32479;&#35745;&#26816;&#27979;&#25351;&#26631;&#19982;&#27169;&#22411;&#39044;&#27979;&#30456;&#32467;&#21512;&#65292;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13671v1 &#21457;&#34920;&#31867;&#22411;&#65306;&#20132;&#21449;&#20256;&#25773; &#25688;&#35201;&#65306;SemEval-2024&#20219;&#21153;8&#20391;&#37325;&#20110;&#22810;&#29983;&#25104;&#22120;&#12289;&#22810;&#39046;&#22495;&#21644;&#22810;&#35821;&#35328;&#30340;&#40657;&#30418;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#12290;&#36825;&#26679;&#30340;&#26816;&#27979;&#23545;&#20110;&#38450;&#27490;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#28508;&#22312;&#28389;&#29992;&#38750;&#24120;&#37325;&#35201;&#65292;&#20854;&#20013;&#26368;&#26032;&#30340;LLMs&#38750;&#24120;&#25797;&#38271;&#29983;&#25104;&#22810;&#35821;&#35328;&#30340;&#31867;&#20284;&#20154;&#31867;&#30340;&#25991;&#26412;&#12290;&#25105;&#20204;&#20197;&#22810;&#31181;&#26041;&#24335;&#22788;&#29702;&#20102;&#36825;&#20010;&#20219;&#21153;&#65292;&#21033;&#29992;&#35821;&#35328;&#35782;&#21035;&#21644;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#36739;&#23567;&#30340;LLMs&#36827;&#34892;&#25991;&#26412;&#20998;&#31867;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#20351;&#29992;&#27599;&#31181;&#35821;&#35328;&#30340;&#20998;&#31867;&#38408;&#20540;&#26657;&#20934;&#65292;&#23558;&#24494;&#35843;&#30340;&#27169;&#22411;&#39044;&#27979;&#19982;&#32479;&#35745;&#26816;&#27979;&#25351;&#26631;&#29420;&#29305;&#32467;&#21512;&#65292;&#20197;&#25552;&#39640;&#31995;&#32479;&#26816;&#27979;&#24615;&#33021;&#30340;&#27867;&#21270;&#12290;&#25105;&#20204;&#25552;&#20132;&#30340;&#26041;&#27861;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#30340;&#32467;&#26524;&#65292;&#25490;&#21517;&#31532;&#22235;&#65292;&#20165;&#33853;&#21518;&#20110;&#33719;&#32988;&#32773;&#19981;&#21040;1&#20010;&#30334;&#20998;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13671v1 Announce Type: cross  Abstract: SemEval-2024 Task 8 is focused on multigenerator, multidomain, and multilingual black-box machine-generated text detection. Such a detection is important for preventing a potential misuse of large language models (LLMs), the newest of which are very capable in generating multilingual human-like texts. We have coped with this task in multiple ways, utilizing language identification and parameter-efficient fine-tuning of smaller LLMs for text classification. We have further used the per-language classification-threshold calibration to uniquely combine fine-tuned models predictions with statistical detection metrics to improve generalization of the system detection performance. Our submitted method achieved competitive results, ranking at the fourth place, just under 1 percentage point behind the winner.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#32452;&#21512;&#27880;&#24847;&#21147;&#36974;&#32617;&#26041;&#27861;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25552;&#20986;&#22810;&#31181;&#20132;&#20114;&#26041;&#24335;&#65292;&#21487;&#20197;&#25913;&#36827;&#26080;&#30417;&#30563;&#25991;&#26412;&#39118;&#26684;&#36716;&#31227;&#20219;&#21153;&#12290;</title><link>https://arxiv.org/abs/2402.13647</link><description>&lt;p&gt;
&#36890;&#36807;LLMs&#21644;&#27880;&#24847;&#21147;&#36974;&#32617;&#23436;&#25104;&#26080;&#30417;&#30563;&#25991;&#26412;&#39118;&#26684;&#36716;&#31227;&#19982;&#22810;&#36335;&#20132;&#20114;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Text Style Transfer via LLMs and Attention Masking with Multi-way Interactions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13647
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#32452;&#21512;&#27880;&#24847;&#21147;&#36974;&#32617;&#26041;&#27861;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25552;&#20986;&#22810;&#31181;&#20132;&#20114;&#26041;&#24335;&#65292;&#21487;&#20197;&#25913;&#36827;&#26080;&#30417;&#30563;&#25991;&#26412;&#39118;&#26684;&#36716;&#31227;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#25991;&#26412;&#39118;&#26684;&#36716;&#31227;&#65288;UTST&#65289;&#24050;&#25104;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#20219;&#21153;&#65292;&#26088;&#22312;&#23558;&#21477;&#23376;&#30340;&#19968;&#31181;&#39118;&#26684;&#26041;&#38754;&#36716;&#25442;&#20026;&#21478;&#19968;&#31181;&#39118;&#26684;&#65292;&#21516;&#26102;&#19981;&#25913;&#21464;&#20854;&#35821;&#20041;&#12289;&#21477;&#27861;&#25110;&#20854;&#20182;&#23646;&#24615;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#26377;&#25928;&#32467;&#21512;&#27880;&#24847;&#21147;&#36974;&#32617;&#26041;&#27861;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#25552;&#20986;&#20102;&#22235;&#31181;&#20132;&#20114;&#26041;&#24335;&#65306;&#20855;&#26377;&#35843;&#25972;&#39034;&#24207;&#30340;&#27969;&#27700;&#32447;&#26694;&#26550;&#65307;&#30693;&#35782;&#33976;&#39311;&#20174;LLMs&#21040;&#27880;&#24847;&#21147;&#36974;&#32617;&#27169;&#22411;&#65307;&#20351;&#29992;&#26500;&#24314;&#30340;&#24182;&#34892;&#31034;&#20363;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#12290;&#25105;&#20204;&#23454;&#39564;&#35777;&#26126;&#36825;&#20123;&#22810;&#36335;&#20132;&#20114;&#21487;&#20197;&#25913;&#36827;&#24615;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13647v1 Announce Type: cross  Abstract: Unsupervised Text Style Transfer (UTST) has emerged as a critical task within the domain of Natural Language Processing (NLP), aiming to transfer one stylistic aspect of a sentence into another style without changing its semantics, syntax, or other attributes. This task is especially challenging given the intrinsic lack of parallel text pairings. Among existing methods for UTST tasks, attention masking approach and Large Language Models (LLMs) are deemed as two pioneering methods. However, they have shortcomings in generating unsmooth sentences and changing the original contents, respectively. In this paper, we investigate if we can combine these two methods effectively. We propose four ways of interactions, that are pipeline framework with tuned orders; knowledge distillation from LLMs to attention masking model; in-context learning with constructed parallel examples. We empirically show these multi-way interactions can improve the ba
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#29992;&#20110;&#35780;&#20272;&#21307;&#30103;&#20154;&#24037;&#26234;&#33021;&#25968;&#25454;&#36136;&#37327;&#30340;METRIC&#26694;&#26550;&#65292;&#30528;&#37325;&#25506;&#35752;&#25968;&#25454;&#36136;&#37327;&#22312;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#20013;&#30340;&#37325;&#35201;&#24615;&#65292;&#24378;&#35843;&#25968;&#25454;&#36136;&#37327;&#23545;&#20110;&#21307;&#23398;&#20154;&#24037;&#26234;&#33021;&#20135;&#21697;&#30417;&#31649;&#25209;&#20934;&#30340;&#20851;&#38190;&#20316;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.13635</link><description>&lt;p&gt;
&#29992;&#20110;&#35780;&#20272;&#21487;&#20449;&#21307;&#30103;&#20154;&#24037;&#26234;&#33021;&#25968;&#25454;&#36136;&#37327;&#30340;METRIC&#26694;&#26550;: &#19968;&#39033;&#31995;&#32479;&#24615;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
The METRIC-framework for assessing data quality for trustworthy AI in medicine: a systematic review
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13635
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#29992;&#20110;&#35780;&#20272;&#21307;&#30103;&#20154;&#24037;&#26234;&#33021;&#25968;&#25454;&#36136;&#37327;&#30340;METRIC&#26694;&#26550;&#65292;&#30528;&#37325;&#25506;&#35752;&#25968;&#25454;&#36136;&#37327;&#22312;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#20013;&#30340;&#37325;&#35201;&#24615;&#65292;&#24378;&#35843;&#25968;&#25454;&#36136;&#37327;&#23545;&#20110;&#21307;&#23398;&#20154;&#24037;&#26234;&#33021;&#20135;&#21697;&#30417;&#31649;&#25209;&#20934;&#30340;&#20851;&#38190;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13635v1 &#20844;&#21578;&#31867;&#22411;:&#36328;&#39046;&#22495; &#25688;&#35201;: &#26426;&#22120;&#23398;&#20064;(ML)&#30340;&#37319;&#29992;&#65292;&#26356;&#20855;&#20307;&#22320;&#35828;&#26159;&#28145;&#24230;&#23398;&#20064;(DL)&#24212;&#29992;&#27491;&#22312;&#34067;&#24310;&#21040;&#25105;&#20204;&#29983;&#27963;&#30340;&#21508;&#20010;&#20027;&#35201;&#39046;&#22495;&#20013;&#12290;&#22312;&#21307;&#23398;&#39046;&#22495;&#65292;&#21457;&#23637;&#21487;&#20449;&#20154;&#24037;&#26234;&#33021;&#23588;&#20026;&#37325;&#35201;&#65292;&#22240;&#20026;&#36825;&#23545;&#24739;&#32773;&#30340;&#29983;&#27963;&#26377;&#30528;&#37325;&#22823;&#24433;&#21709;&#12290;&#34429;&#28982;&#21487;&#20449;&#24615;&#28041;&#21450;&#22810;&#20010;&#26041;&#38754;&#65292;&#21253;&#25324;&#36947;&#24503;&#12289;&#25216;&#26415;&#21644;&#38544;&#31169;&#35201;&#27714;&#65292;&#20294;&#25105;&#20204;&#30528;&#37325;&#20110;DL&#20013;&#25968;&#25454;&#36136;&#37327;(&#35757;&#32451;/&#27979;&#35797;)&#30340;&#37325;&#35201;&#24615;&#12290;&#30001;&#20110;&#25968;&#25454;&#36136;&#37327;&#20915;&#23450;&#20102;ML&#20135;&#21697;&#30340;&#34892;&#20026;&#65292;&#35780;&#20272;&#25968;&#25454;&#36136;&#37327;&#23558;&#22312;&#21307;&#30103;&#20154;&#24037;&#26234;&#33021;&#20135;&#21697;&#30340;&#30417;&#31649;&#25209;&#20934;&#20013;&#36215;&#20851;&#38190;&#20316;&#29992;&#12290;&#25105;&#20204;&#25353;&#29031;PRISMA&#25351;&#21335;&#36827;&#34892;&#31995;&#32479;&#24615;&#32508;&#36848;&#65292;&#20351;&#29992;PubMed&#21644;ACM&#25968;&#23383;&#22270;&#20070;&#39302;&#25968;&#25454;&#24211;&#12290;&#25105;&#20204;&#21457;&#29616;&#20102;2362&#39033;&#30740;&#31350;&#65292;&#20854;&#20013;62&#39033;&#31526;&#21512;&#25105;&#20204;&#30340;&#36164;&#26684;&#26631;&#20934;&#12290;&#22312;&#36825;&#19968;&#25991;&#29486;&#20013;&#65292;&#25105;&#20204;&#32508;&#21512;&#29616;&#26377;&#30340;&#20851;&#20110;&#25968;&#25454;&#36136;&#37327;&#26694;&#26550;&#30340;&#30693;&#35782;&#65292;&#24182;&#32467;&#21512;ML&#22312;&#21307;&#23398;&#20013;&#30340;&#24212;&#29992;&#35270;&#35282;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#24635;&#32467;&#20102;&#21307;&#30103;AI&#39046;&#22495;&#25968;&#25454;&#36136;&#37327;&#26694;&#26550;&#30340;&#29616;&#26377;&#30693;&#35782;&#65292;&#24182;&#23558;&#20854;&#19982;ML&#24212;&#29992;&#30340;&#35270;&#35282;&#32467;&#21512;&#22312;&#19968;&#36215;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13635v1 Announce Type: cross  Abstract: The adoption of machine learning (ML) and, more specifically, deep learning (DL) applications into all major areas of our lives is underway. The development of trustworthy AI is especially important in medicine due to the large implications for patients' lives. While trustworthiness concerns various aspects including ethical, technical and privacy requirements, we focus on the importance of data quality (training/test) in DL. Since data quality dictates the behaviour of ML products, evaluating data quality will play a key part in the regulatory approval of medical AI products. We perform a systematic review following PRISMA guidelines using the databases PubMed and ACM Digital Library. We identify 2362 studies, out of which 62 records fulfil our eligibility criteria. From this literature, we synthesise the existing knowledge on data quality frameworks and combine it with the perspective of ML applications in medicine. As a result, we p
&lt;/p&gt;</description></item><item><title>&#20998;&#26512;&#36830;&#35789;&#35884;&#35823;&#30340;&#20107;&#23454;&#21487;&#33021;&#24615;&#33539;&#22260;&#65292;&#25581;&#31034;&#22823;&#22810;&#25968;&#30740;&#31350;&#23384;&#22312;&#23545;&#21487;&#33021;&#24615;&#29421;&#38552;&#30340;&#20559;&#35265;&#12290;</title><link>https://arxiv.org/abs/2402.13615</link><description>&lt;p&gt;
&#23558;&#36830;&#35789;&#35884;&#35823;&#35270;&#20026;&#19968;&#20010;&#20107;&#23454;&#36827;&#34892;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Analyizing the Conjunction Fallacy as a Fact
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13615
&lt;/p&gt;
&lt;p&gt;
&#20998;&#26512;&#36830;&#35789;&#35884;&#35823;&#30340;&#20107;&#23454;&#21487;&#33021;&#24615;&#33539;&#22260;&#65292;&#25581;&#31034;&#22823;&#22810;&#25968;&#30740;&#31350;&#23384;&#22312;&#23545;&#21487;&#33021;&#24615;&#29421;&#38552;&#30340;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20174;Tversky&#21644;Kahneman&#30340;&#24320;&#21019;&#24615;&#35770;&#25991;&#20197;&#26469;&#65292;&#36830;&#35789;&#35884;&#35823;&#19968;&#30452;&#26159;&#22810;&#27425;&#36777;&#35770;&#30340;&#20027;&#39064;&#65292;&#24182;&#25104;&#20026;&#20915;&#31574;&#21046;&#23450;&#20013;&#35748;&#30693;&#29702;&#35770;&#38754;&#20020;&#30340;&#22522;&#26412;&#25361;&#25112;&#12290;&#26412;&#25991;&#20174;&#19968;&#31181;&#19981;&#22826;&#24120;&#35265;&#30340;&#35282;&#24230;&#26469;&#30475;&#24453;&#36825;&#19968;&#29616;&#35937;&#12290;&#25105;&#20204;&#20998;&#26512;&#20854;&#20107;&#23454;&#21487;&#33021;&#24615;&#30340;&#33539;&#22260;&#65292;&#32780;&#19981;&#26159;&#35797;&#22270;&#35299;&#37322;&#36830;&#35789;&#35884;&#35823;&#30340;&#26412;&#36136;&#25110;&#21407;&#22240;&#65288;&#20869;&#28085;&#23450;&#20041;&#65289;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#26681;&#25454;&#25105;&#20204;&#25152;&#23457;&#26597;&#30340;&#28085;&#30422;&#20102;1983&#24180;&#33267;2016&#24180;&#25991;&#29486;&#30340;&#23454;&#39564;&#26679;&#26412;&#65292;&#22823;&#22810;&#25968;&#20851;&#20110;&#36830;&#35789;&#35884;&#35823;&#30340;&#30740;&#31350;&#38598;&#20013;&#22312;&#20808;&#39564;&#20107;&#23454;&#21487;&#33021;&#24615;&#30340;&#29421;&#31364;&#37096;&#20998;&#65292;&#26263;&#31034;&#20102;&#36830;&#35789;&#35884;&#35823;&#30340;&#35299;&#37322;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#21463;&#38480;&#20110;&#25506;&#32034;&#30340;&#21487;&#33021;&#24615;&#33539;&#22260;&#29421;&#38552;&#12290;&#21518;&#32773;&#26159;&#36830;&#35789;&#35884;&#35823;&#30740;&#31350;&#36827;&#21270;&#20013;&#30456;&#24403;&#22855;&#29305;&#30340;&#19968;&#20010;&#26041;&#38754;&#65292;&#32771;&#34385;&#21040;&#23427;&#26412;&#36136;&#19978;&#26159;&#30001;&#20854;&#21487;&#33021;&#24615;&#30340;&#25193;&#23637;&#25152;&#25512;&#21160;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13615v1 Announce Type: new  Abstract: Since the seminal paper by Tversky and Kahneman, the conjunction fallacy has been the subject of multiple debates and become a fundamental challenge for cognitive theories in decision-making. In this article, we take a rather uncommon perspective on this phenomenon. Instead of trying to explain the nature or causes of the conjunction fallacy (intensional definition), we analyze its range of factual possibilities (extensional definition). We show that the majority of research on the conjunction fallacy, according to our sample of experiments reviewed which covers literature between 1983 and 2016, has focused on a narrow part of the a priori factual possibilities, implying that explanations of the conjunction fallacy are fundamentally biased by the short scope of possibilities explored. The latter is a rather curious aspect of the research evolution in the conjunction fallacy considering that the very nature of it is motivated by extension
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#29983;&#25104;&#27169;&#22411;&#22312;&#25968;&#25454;&#39537;&#21160;&#21457;&#29616;&#20013;&#30340;&#24212;&#29992;&#24320;&#21019;&#20102;&#31471;&#21040;&#31471;&#21457;&#29616;&#31995;&#32479;&#30340;&#26032;&#27169;&#24335;&#65292;&#21033;&#29992;&#25552;&#20379;&#30340;&#25968;&#25454;&#38598;&#25628;&#23547;&#21644;&#39564;&#35777;&#20551;&#35774;&#65292;&#31361;&#26174;&#20102;&#33258;&#21160;&#21270;&#31995;&#32479;&#30340;&#37325;&#35201;&#24615;&#21644;&#23616;&#38480;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.13610</link><description>&lt;p&gt;
&#25968;&#25454;&#39537;&#21160;&#30340;&#22823;&#22411;&#29983;&#25104;&#27169;&#22411;&#22312;&#31185;&#23398;&#21457;&#29616;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Data-driven Discovery with Large Generative Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13610
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#29983;&#25104;&#27169;&#22411;&#22312;&#25968;&#25454;&#39537;&#21160;&#21457;&#29616;&#20013;&#30340;&#24212;&#29992;&#24320;&#21019;&#20102;&#31471;&#21040;&#31471;&#21457;&#29616;&#31995;&#32479;&#30340;&#26032;&#27169;&#24335;&#65292;&#21033;&#29992;&#25552;&#20379;&#30340;&#25968;&#25454;&#38598;&#25628;&#23547;&#21644;&#39564;&#35777;&#20551;&#35774;&#65292;&#31361;&#26174;&#20102;&#33258;&#21160;&#21270;&#31995;&#32479;&#30340;&#37325;&#35201;&#24615;&#21644;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#25968;&#25454;&#20197;&#21069;&#25152;&#26410;&#26377;&#30340;&#36895;&#24230;&#32047;&#31215;&#65292;&#23427;&#20316;&#20026;&#20419;&#36827;&#31185;&#23398;&#21457;&#29616;&#30340;&#28508;&#21147;&#21576;&#25351;&#25968;&#22686;&#38271;&#12290;&#36825;&#31687;&#31435;&#22330;&#35770;&#25991;&#25958;&#20419;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#31038;&#21306;&#21033;&#29992;&#22823;&#22411;&#29983;&#25104;&#27169;&#22411;&#65288;LGMs&#65289;&#30340;&#33021;&#21147;&#65292;&#24320;&#21457;&#33258;&#21160;&#21270;&#31995;&#32479;&#29992;&#20110;&#31471;&#21040;&#31471;&#30340;&#25968;&#25454;&#39537;&#21160;&#21457;&#29616; -- &#19968;&#31181;&#33539;&#24335;&#65292;&#20174;&#25152;&#25552;&#20379;&#30340;&#25968;&#25454;&#38598;&#20013;&#32431;&#31929;&#25628;&#32034;&#21644;&#39564;&#35777;&#20551;&#35774;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#25968;&#25454;&#25910;&#38598;&#25110;&#29289;&#29702;&#23454;&#39564;&#12290;&#25105;&#20204;&#39318;&#20808;&#27010;&#36848;&#20102;&#29702;&#24819;&#25968;&#25454;&#39537;&#21160;&#21457;&#29616;&#31995;&#32479;&#30340;&#20960;&#20010;&#26399;&#26395;&#26465;&#20214;&#12290;&#28982;&#21518;&#65292;&#36890;&#36807;&#20351;&#29992;GPT-4&#30340;DATAVOYAGER&#20316;&#20026;&#27010;&#24565;&#39564;&#35777;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;LGMs&#22914;&#20309;&#23454;&#29616;&#20960;&#39033;&#36825;&#20123;&#26399;&#26395;&#26465;&#20214; -- &#36825;&#26159;&#20197;&#21069;&#26080;&#27861;&#20570;&#21040;&#30340;&#25104;&#23601; -- &#21516;&#26102;&#20063;&#31361;&#26174;&#20102;&#24403;&#21069;&#31995;&#32479;&#20013;&#30340;&#37325;&#35201;&#23616;&#38480;&#24615;&#65292;&#20174;&#32780;&#20026;&#24320;&#23637;&#26032;&#22411;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#25552;&#20379;&#20102;&#26426;&#36935;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13610v1 Announce Type: cross  Abstract: With the accumulation of data at an unprecedented rate, its potential to fuel scientific discovery is growing exponentially. This position paper urges the Machine Learning (ML) community to exploit the capabilities of large generative models (LGMs) to develop automated systems for end-to-end data-driven discovery -- a paradigm encompassing the search and verification of hypotheses purely from a set of provided datasets, without the need for additional data collection or physical experiments. We first outline several desiderata for an ideal data-driven discovery system. Then, through DATAVOYAGER, a proof-of-concept utilizing GPT-4, we demonstrate how LGMs fulfill several of these desiderata -- a feat previously unattainable -- while also highlighting important limitations in the current system that open up opportunities for novel ML research. We contend that achieving accurate, reliable, and robust end-to-end discovery systems solely th
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#21160;&#39550;&#39542;&#20013;&#30340;&#28151;&#21512;&#25512;&#29702;&#33021;&#21147;&#21487;&#20197;&#36890;&#36807;&#20998;&#26512;&#25968;&#25454;&#12289;&#29702;&#35299;&#35268;&#21017;&#21644;&#27861;&#21017;&#12289;&#25552;&#20379;&#35821;&#22659;&#31561;&#26041;&#24335;&#25552;&#39640;&#33258;&#21160;&#39542;&#39542;&#30340;&#20915;&#31574;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.13602</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#28151;&#21512;&#25512;&#29702;&#22312;&#33258;&#21160;&#39550;&#39542;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Hybrid Reasoning Based on Large Language Models for Autonomous Car Driving
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13602
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#21160;&#39550;&#39542;&#20013;&#30340;&#28151;&#21512;&#25512;&#29702;&#33021;&#21147;&#21487;&#20197;&#36890;&#36807;&#20998;&#26512;&#25968;&#25454;&#12289;&#29702;&#35299;&#35268;&#21017;&#21644;&#27861;&#21017;&#12289;&#25552;&#20379;&#35821;&#22659;&#31561;&#26041;&#24335;&#25552;&#39640;&#33258;&#21160;&#39542;&#39542;&#30340;&#20915;&#31574;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22240;&#20854;&#29702;&#35299;&#25991;&#26412;&#21644;&#22270;&#20687;&#12289;&#29983;&#25104;&#31867;&#20154;&#25991;&#26412;&#20197;&#21450;&#25191;&#34892;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#30340;&#33021;&#21147;&#32780;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#23558;&#36825;&#31181;&#39640;&#32423;&#25512;&#29702;&#19982;&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;&#30456;&#32467;&#21512;&#20197;&#29992;&#20110;&#21160;&#24577;&#24773;&#20917;&#19979;&#30340;&#20915;&#31574;&#30340;&#27867;&#21270;&#33021;&#21147;&#38656;&#35201;&#36827;&#19968;&#27493;&#25506;&#32034;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;LLMs&#22312;&#28151;&#21512;&#31639;&#26415;&#21644;&#24120;&#35782;&#25512;&#29702;&#26041;&#38754;&#30340;&#36866;&#24212;&#33021;&#21147;&#21644;&#24212;&#29992;&#33021;&#21147;&#65292;&#29305;&#21035;&#26159;&#22312;&#33258;&#21160;&#39550;&#39542;&#22330;&#26223;&#20013;&#12290;&#25105;&#20204;&#20551;&#35774;LLMs&#30340;&#28151;&#21512;&#25512;&#29702;&#33021;&#21147;&#21487;&#20197;&#36890;&#36807;&#20351;&#23427;&#20204;&#20998;&#26512;&#26816;&#27979;&#21040;&#30340;&#29289;&#20307;&#21644;&#20256;&#24863;&#22120;&#25968;&#25454;&#12289;&#29702;&#35299;&#39550;&#39542;&#35268;&#23450;&#21644;&#29289;&#29702;&#27861;&#21017;&#65292;&#24182;&#25552;&#20379;&#39069;&#22806;&#30340;&#35821;&#22659;&#26469;&#25913;&#21892;&#33258;&#21160;&#39550;&#39542;&#12290;&#36825;&#35299;&#20915;&#20102;&#22797;&#26434;&#24773;&#26223;&#65292;&#22914;&#20302;&#33021;&#35265;&#24230;&#65288;&#30001;&#20110;&#22825;&#27668;&#26465;&#20214;&#65289;&#19979;&#30340;&#20915;&#31574;&#65292;&#20256;&#32479;&#26041;&#27861;&#21487;&#33021;&#19981;&#36275;&#20197;&#32988;&#20219;&#12290;&#25105;&#20204;&#36890;&#36807;&#20934;&#30830;&#24615;&#35780;&#20272;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36825;&#31181;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13602v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have garnered significant attention for their ability to understand text and images, generate human-like text, and perform complex reasoning tasks. However, their ability to generalize this advanced reasoning with a combination of natural language text for decision-making in dynamic situations requires further exploration. In this study, we investigate how well LLMs can adapt and apply a combination of arithmetic and common-sense reasoning, particularly in autonomous driving scenarios. We hypothesize that LLMs hybrid reasoning abilities can improve autonomous driving by enabling them to analyze detected object and sensor data, understand driving regulations and physical laws, and offer additional context. This addresses complex scenarios, like decisions in low visibility (due to weather conditions), where traditional methods might fall short. We evaluated Large Language Models (LLMs) based on accuracy by co
&lt;/p&gt;</description></item><item><title>User-LLM&#26694;&#26550;&#21033;&#29992;&#29992;&#25143;&#23884;&#20837;&#23545;LLMs&#36827;&#34892;&#35821;&#22659;&#21270;&#65292;&#20351;&#20854;&#33021;&#22815;&#21160;&#24577;&#36866;&#24212;&#29992;&#25143;&#19978;&#19979;&#25991;&#65292;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23454;&#29616;&#26174;&#33879;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2402.13598</link><description>&lt;p&gt;
User-LLM: &#21033;&#29992;&#29992;&#25143;&#23884;&#20837;&#23454;&#29616;&#26377;&#25928;&#30340;LLM&#35821;&#22659;&#21270;
&lt;/p&gt;
&lt;p&gt;
User-LLM: Efficient LLM Contextualization with User Embeddings
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13598
&lt;/p&gt;
&lt;p&gt;
User-LLM&#26694;&#26550;&#21033;&#29992;&#29992;&#25143;&#23884;&#20837;&#23545;LLMs&#36827;&#34892;&#35821;&#22659;&#21270;&#65292;&#20351;&#20854;&#33021;&#22815;&#21160;&#24577;&#36866;&#24212;&#29992;&#25143;&#19978;&#19979;&#25991;&#65292;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23454;&#29616;&#26174;&#33879;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;(LLMs)&#24050;&#32463;&#24443;&#24213;&#25913;&#21464;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12290;&#28982;&#32780;&#65292;&#26377;&#25928;&#22320;&#25972;&#21512;&#22797;&#26434;&#19988;&#28508;&#22312;&#22024;&#26434;&#30340;&#29992;&#25143;&#20132;&#20114;&#25968;&#25454;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;User-LLM&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#29992;&#25143;&#23884;&#20837;&#26469;&#23545;LLMs&#36827;&#34892;&#35821;&#22659;&#21270;&#12290;&#36825;&#20123;&#23884;&#20837;&#26159;&#36890;&#36807;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#20174;&#21508;&#31181;&#29992;&#25143;&#20132;&#20114;&#20013;&#31934;&#28860;&#20986;&#26469;&#30340;&#65292;&#33021;&#22815;&#25429;&#25417;&#28508;&#22312;&#29992;&#25143;&#20559;&#22909;&#21450;&#20854;&#38543;&#26102;&#38388;&#30340;&#28436;&#21464;&#12290;&#25105;&#20204;&#36890;&#36807;&#20132;&#21449;&#27880;&#24847;&#21147;&#21644;&#36719;&#25552;&#31034;&#23558;&#36825;&#20123;&#29992;&#25143;&#23884;&#20837;&#19982;LLMs&#38598;&#25104;&#36215;&#26469;&#65292;&#20351;LLMs&#33021;&#22815;&#21160;&#24577;&#36866;&#24212;&#29992;&#25143;&#19978;&#19979;&#25991;&#12290;&#25105;&#20204;&#22312;MovieLens&#12289;&#20122;&#39532;&#36874;&#35780;&#35770;&#21644;&#35895;&#27468;&#26412;&#22320;&#35780;&#35770;&#31561;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#20840;&#38754;&#23454;&#39564;&#65292;&#23637;&#31034;&#20102;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#30340;&#26174;&#33879;&#24615;&#33021;&#25552;&#21319;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#38271;&#24207;&#21015;&#20219;&#21153;&#21644;&#38656;&#35201;&#28145;&#20837;&#29702;&#35299;&#29992;&#25143;&#30340;&#20219;&#21153;&#19978;&#36229;&#36807;&#20102;&#22522;&#20110;&#25991;&#26412;&#25552;&#31034;&#30340;&#35821;&#22659;&#21270;&#65292;&#21516;&#26102;&#22312;&#35745;&#31639;&#19978;&#20063;&#26356;&#21152;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13598v1 Announce Type: cross  Abstract: Large language models (LLMs) have revolutionized natural language processing. However, effectively incorporating complex and potentially noisy user interaction data remains a challenge. To address this, we propose User-LLM, a novel framework that leverages user embeddings to contextualize LLMs. These embeddings, distilled from diverse user interactions using self-supervised pretraining, capture latent user preferences and their evolution over time. We integrate these user embeddings with LLMs through cross-attention and soft-prompting, enabling LLMs to dynamically adapt to user context. Our comprehensive experiments on MovieLens, Amazon Review, and Google Local Review datasets demonstrate significant performance gains across various tasks. Notably, our approach outperforms text-prompt-based contextualization on long sequence tasks and tasks that require deep user understanding while being computationally efficient. We further incorpora
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20027;&#35201;&#36129;&#29486;&#26159;&#36890;&#36807;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21644;&#34892;&#20026;&#35843;&#25511;&#26041;&#26696;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;GuanoZero&#65292;&#20351;AI&#20195;&#29702;&#33021;&#22815;&#25484;&#25569;&#12298;&#20851;&#26086;&#12299;&#28216;&#25103;&#12290;</title><link>https://arxiv.org/abs/2402.13582</link><description>&lt;p&gt;
&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#21644;&#34892;&#20026;&#35843;&#25511;&#25484;&#25569;&#20851;&#26086;&#28216;&#25103;
&lt;/p&gt;
&lt;p&gt;
Mastering the Game of Guandan with Deep Reinforcement Learning and Behavior Regulating
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13582
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20027;&#35201;&#36129;&#29486;&#26159;&#36890;&#36807;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21644;&#34892;&#20026;&#35843;&#25511;&#26041;&#26696;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;GuanoZero&#65292;&#20351;AI&#20195;&#29702;&#33021;&#22815;&#25484;&#25569;&#12298;&#20851;&#26086;&#12299;&#28216;&#25103;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28216;&#25103;&#26159;&#29616;&#23454;&#30340;&#31616;&#21270;&#27169;&#22411;&#65292;&#36890;&#24120;&#34987;&#35270;&#20026;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#30340;&#39318;&#36873;&#24179;&#21488;&#12290; &#24456;&#22810;&#30740;&#31350;&#20851;&#27880;&#20110;&#28216;&#25103;&#20195;&#29702;&#21644;&#23427;&#20204;&#30340;&#20915;&#31574;&#36807;&#31243;&#12290;&#12298;&#20851;&#26086;&#12299;&#26159;&#19968;&#31181;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#28216;&#25103;&#65292;&#21363;&#20351;&#26159;&#19987;&#19994;&#30340;&#20154;&#31867;&#29609;&#23478;&#26377;&#26102;&#20063;&#38590;&#20197;&#20570;&#20986;&#27491;&#30830;&#30340;&#20915;&#31574;&#12290; &#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;GuanZero&#30340;&#26694;&#26550;&#65292;&#35753;AI&#20195;&#29702;&#36890;&#36807;&#33945;&#29305;&#21345;&#27931;&#26041;&#27861;&#21644;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26469;&#25484;&#25569;&#36825;&#20010;&#28216;&#25103;&#12290; &#26412;&#25991;&#30340;&#20027;&#35201;&#36129;&#29486;&#22312;&#20110;&#36890;&#36807;&#31934;&#24515;&#35774;&#35745;&#30340;&#31070;&#32463;&#32593;&#32476;&#32534;&#30721;&#26041;&#26696;&#26469;&#35843;&#25511;&#20195;&#29702;&#30340;&#34892;&#20026;&#12290; &#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#65292;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13582v1 Announce Type: new  Abstract: Games are a simplified model of reality and often serve as a favored platform for Artificial Intelligence (AI) research. Much of the research is concerned with game-playing agents and their decision making processes. The game of Guandan (literally, "throwing eggs") is a challenging game where even professional human players struggle to make the right decision at times. In this paper we propose a framework named GuanZero for AI agents to master this game using Monte-Carlo methods and deep neural networks. The main contribution of this paper is about regulating agents' behavior through a carefully designed neural network encoding scheme. We then demonstrate the effectiveness of the proposed framework by comparing it with state-of-the-art approaches.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#28210;&#26579;&#26041;&#27861;&#65292;&#21517;&#20026;FPA&#65292;&#36890;&#36807;&#23398;&#20064;&#23545;&#25239;&#27169;&#24335;&#24182;&#32467;&#21512;&#29305;&#27530;&#35774;&#35745;&#30340;&#23545;&#25239;&#25439;&#22833;&#21644;&#38544;&#34109;&#32422;&#26463;&#25439;&#22833;&#65292;&#21487;&#20197;&#29983;&#25104;&#29289;&#29702;&#19990;&#30028;&#20013;&#20855;&#26377;&#23545;&#25239;&#24615;&#21644;&#38544;&#34109;&#24615;&#36136;&#30340;&#20266;&#35013;&#12290;</title><link>https://arxiv.org/abs/2402.13575</link><description>&lt;p&gt;
&#22522;&#20110;&#24046;&#24322;&#26041;&#27861;&#30340;&#28789;&#27963;&#29289;&#29702;&#20266;&#35013;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Flexible Physical Camouflage Generation Based on a Differential Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13575
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#28210;&#26579;&#26041;&#27861;&#65292;&#21517;&#20026;FPA&#65292;&#36890;&#36807;&#23398;&#20064;&#23545;&#25239;&#27169;&#24335;&#24182;&#32467;&#21512;&#29305;&#27530;&#35774;&#35745;&#30340;&#23545;&#25239;&#25439;&#22833;&#21644;&#38544;&#34109;&#32422;&#26463;&#25439;&#22833;&#65292;&#21487;&#20197;&#29983;&#25104;&#29289;&#29702;&#19990;&#30028;&#20013;&#20855;&#26377;&#23545;&#25239;&#24615;&#21644;&#38544;&#34109;&#24615;&#36136;&#30340;&#20266;&#35013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#28210;&#26579;&#26041;&#27861;&#65292;&#19987;&#38376;&#38024;&#23545;&#23545;&#25239;&#20266;&#35013;&#65292;&#22312;&#24191;&#27867;&#30340;&#19977;&#32500;&#28210;&#26579;&#26694;&#26550;&#20869;&#36827;&#34892;&#20102;&#23450;&#21046;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#21517;&#20026;FPA&#65292;&#36890;&#36807;&#24544;&#23454;&#22320;&#27169;&#25311;&#20809;&#29031;&#26465;&#20214;&#21644;&#26448;&#26009;&#21464;&#21270;&#65292;&#30830;&#20445;&#22312;&#19977;&#32500;&#30446;&#26631;&#19978;&#23545;&#32441;&#29702;&#36827;&#34892;&#24494;&#22937;&#32780;&#36924;&#30495;&#30340;&#34920;&#29616;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#37319;&#29992;&#19968;&#31181;&#29983;&#25104;&#26041;&#27861;&#65292;&#20174;&#25193;&#25955;&#27169;&#22411;&#20013;&#23398;&#20064;&#23545;&#25239;&#27169;&#24335;&#12290;&#36825;&#28041;&#21450;&#23558;&#19968;&#20010;&#29305;&#21035;&#35774;&#35745;&#30340;&#23545;&#25239;&#25439;&#22833;&#21644;&#38544;&#34109;&#32422;&#26463;&#25439;&#22833;&#32467;&#21512;&#22312;&#19968;&#36215;&#65292;&#20197;&#30830;&#20445;&#20266;&#35013;&#22312;&#29289;&#29702;&#19990;&#30028;&#20013;&#30340;&#23545;&#25239;&#24615;&#21644;&#38544;&#34109;&#24615;&#36136;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;&#20266;&#35013;&#22312;&#36148;&#32440;&#27169;&#24335;&#19979;&#30340;&#26377;&#25928;&#24615;&#65292;&#23637;&#31034;&#20102;&#20854;&#35206;&#30422;&#30446;&#26631;&#32780;&#19981;&#24433;&#21709;&#23545;&#25239;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#23454;&#35777;&#21644;&#29289;&#29702;&#23454;&#39564;&#65292;FPA&#22312;&#25915;&#20987;&#25104;&#21151;&#29575;&#21644;&#21487;&#36716;&#31227;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#24456;&#24378;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13575v1 Announce Type: cross  Abstract: This study introduces a novel approach to neural rendering, specifically tailored for adversarial camouflage, within an extensive 3D rendering framework. Our method, named FPA, goes beyond traditional techniques by faithfully simulating lighting conditions and material variations, ensuring a nuanced and realistic representation of textures on a 3D target. To achieve this, we employ a generative approach that learns adversarial patterns from a diffusion model. This involves incorporating a specially designed adversarial loss and covert constraint loss to guarantee the adversarial and covert nature of the camouflage in the physical world. Furthermore, we showcase the effectiveness of the proposed camouflage in sticker mode, demonstrating its ability to cover the target without compromising adversarial information. Through empirical and physical experiments, FPA exhibits strong performance in terms of attack success rate and transferabili
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;-free &#26041;&#27861; ToDo&#65292;&#36890;&#36807;&#20196;&#29260;&#19979;&#37319;&#26679;&#21152;&#36895; Stable Diffusion &#25512;&#29702;&#65292;&#20197;&#23454;&#29616;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#30340;&#39640;&#25928;&#29983;&#25104;&#12290;</title><link>https://arxiv.org/abs/2402.13573</link><description>&lt;p&gt;
&#20219;&#21153;&#24453;&#21150;&#65306;&#29992;&#20110;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#39640;&#25928;&#29983;&#25104;&#30340;&#20196;&#29260;&#19979;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
ToDo: Token Downsampling for Efficient Generation of High-Resolution Images
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13573
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;-free &#26041;&#27861; ToDo&#65292;&#36890;&#36807;&#20196;&#29260;&#19979;&#37319;&#26679;&#21152;&#36895; Stable Diffusion &#25512;&#29702;&#65292;&#20197;&#23454;&#29616;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#30340;&#39640;&#25928;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27880;&#24847;&#21147;&#26426;&#21046;&#23545;&#20110;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#33267;&#20851;&#37325;&#35201;&#65292;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#20108;&#27425;&#35745;&#31639;&#22797;&#26434;&#24615;&#38480;&#21046;&#20102;&#25105;&#20204;&#21487;&#20197;&#22312;&#21512;&#29702;&#26102;&#38388;&#21644;&#20869;&#23384;&#38480;&#21046;&#20869;&#22788;&#29702;&#30340;&#22270;&#20687;&#22823;&#23567;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#29983;&#25104;&#22270;&#20687;&#27169;&#22411;&#20013;&#23494;&#38598;&#27880;&#24847;&#21147;&#30340;&#37325;&#35201;&#24615;&#65292;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#21253;&#21547;&#20887;&#20313;&#29305;&#24449;&#65292;&#20351;&#23427;&#20204;&#36866;&#21512;&#31232;&#30095;&#27880;&#24847;&#21147;&#26426;&#21046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26080;&#38656;&#35757;&#32451;&#30340;&#26041;&#27861; ToDo&#65292;&#35813;&#26041;&#27861;&#20381;&#36182;&#20110;&#20851;&#38190;&#21644;&#20540;&#20196;&#29260;&#30340;&#20196;&#29260;&#19979;&#37319;&#26679;&#65292;&#21487;&#23558;&#24120;&#35265;&#22823;&#23567;&#30340; Stable Diffusion &#25512;&#29702;&#21152;&#36895;&#33267;&#22810;&#36798;2&#20493;&#65292;&#23545;&#20110;2048x2048&#31561;&#39640;&#20998;&#36776;&#29575;&#65292;&#21152;&#36895;&#27604;&#21487;&#36798;4.5&#20493;&#25110;&#26356;&#39640;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#24179;&#34913;&#39640;&#25928;&#21534;&#21520;&#37327;&#21644;&#20445;&#30495;&#24230;&#26041;&#38754;&#20248;&#20110;&#20808;&#21069;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13573v1 Announce Type: cross  Abstract: Attention mechanism has been crucial for image diffusion models, however, their quadratic computational complexity limits the sizes of images we can process within reasonable time and memory constraints. This paper investigates the importance of dense attention in generative image models, which often contain redundant features, making them suitable for sparser attention mechanisms. We propose a novel training-free method ToDo that relies on token downsampling of key and value tokens to accelerate Stable Diffusion inference by up to 2x for common sizes and up to 4.5x or more for high resolutions like 2048x2048. We demonstrate that our approach outperforms previous methods in balancing efficient throughput and fidelity.
&lt;/p&gt;</description></item><item><title>&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#22411;Transformer&#22359;AlgoFormer&#65292;&#30456;&#27604;&#26631;&#20934;Transformer&#21644;Looped Transformer&#65292;AlgoFormer&#22312;&#30456;&#21516;&#21442;&#25968;&#25968;&#37327;&#19979;&#33021;&#22815;&#23454;&#29616;&#26356;&#39640;&#30340;&#31639;&#27861;&#34920;&#36798;&#33021;&#21147;</title><link>https://arxiv.org/abs/2402.13572</link><description>&lt;p&gt;
&#35770;&#19968;&#31181;&#21464;&#31181;Looped Transformer&#30340;&#34920;&#36798;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
On the Expressive Power of a Variant of the Looped Transformer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13572
&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#22411;Transformer&#22359;AlgoFormer&#65292;&#30456;&#27604;&#26631;&#20934;Transformer&#21644;Looped Transformer&#65292;AlgoFormer&#22312;&#30456;&#21516;&#21442;&#25968;&#25968;&#37327;&#19979;&#33021;&#22815;&#23454;&#29616;&#26356;&#39640;&#30340;&#31639;&#27861;&#34920;&#36798;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38500;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65292;&#22312;&#35299;&#20915;&#26356;&#24191;&#27867;&#30340;&#24212;&#29992;&#31243;&#24207;&#65288;&#21253;&#25324;&#31185;&#23398;&#35745;&#31639;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#65289;&#26041;&#38754;&#65292;Transformer&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#35797;&#22270;&#20174;&#34920;&#36798;&#33021;&#21147;&#21644;&#21151;&#33021;&#24615;&#35282;&#24230;&#35299;&#37322;&#65292;&#26631;&#20934;&#30340;Transformer&#33021;&#22815;&#25191;&#34892;&#19968;&#20123;&#31639;&#27861;&#12290;&#20026;&#20102;&#36171;&#20104;Transformer&#31639;&#27861;&#33021;&#21147;&#65292;&#24182;&#21463;&#21040;&#26368;&#36817;&#25552;&#20986;&#30340;Looped Transformer&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;Transformer&#22359;&#65292;&#21517;&#20026;Algorithm Transformer&#65288;&#31616;&#31216;AlgoFormer&#65289;&#12290;&#19982;&#26631;&#20934;Transformer&#21644;&#32431;&#31929;&#30340;Looped Transformer&#30456;&#27604;&#65292;&#25152;&#25552;&#20986;&#30340;AlgoFormer&#22312;&#20351;&#29992;&#30456;&#21516;&#25968;&#37327;&#30340;&#21442;&#25968;&#26102;&#21487;&#20197;&#23454;&#29616;&#26356;&#39640;&#30340;&#31639;&#27861;&#34920;&#31034;&#34920;&#36798;&#33021;&#21147;&#12290;&#29305;&#21035;&#26159;&#65292;&#21463;&#20154;&#31867;&#35774;&#35745;&#30340;&#23398;&#20064;&#31639;&#27861;&#32467;&#26500;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#30340;Transformer&#22359;&#21253;&#25324;&#19968;&#20010;&#36127;&#36131;&#36827;&#34892;ta
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13572v1 Announce Type: cross  Abstract: Besides natural language processing, transformers exhibit extraordinary performance in solving broader applications, including scientific computing and computer vision. Previous works try to explain this from the expressive power and capability perspectives that standard transformers are capable of performing some algorithms. To empower transformers with algorithmic capabilities and motivated by the recently proposed looped transformer (Yang et al., 2024; Giannou et al., 2023), we design a novel transformer block, dubbed Algorithm Transformer (abbreviated as AlgoFormer). Compared with the standard transformer and vanilla looped transformer, the proposed AlgoFormer can achieve significantly higher expressiveness in algorithm representation when using the same number of parameters. In particular, inspired by the structure of human-designed learning algorithms, our transformer block consists of a pre-transformer that is responsible for ta
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#19968;&#20010;&#29992;&#20110;31&#31181;&#21335;&#20122;&#35821;&#35328;&#30340;&#22810;&#35821;&#35328;&#20849;&#25351;&#35299;&#26512;&#32763;&#35793;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#21033;&#29992;&#29616;&#25104;&#24037;&#20855;&#36827;&#34892;&#35757;&#32451;&#21644;&#23545;&#40784;&#65292;&#22312;&#20302;&#36164;&#28304;&#26465;&#20214;&#19979;&#23454;&#29616;&#20102;&#36739;&#22909;&#30340;&#20849;&#25351;&#35299;&#26512;&#27169;&#22411;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2402.13571</link><description>&lt;p&gt;
&#20302;&#36164;&#28304;&#26465;&#20214;&#19979;&#21335;&#20122;&#35821;&#35328;&#30340;&#22810;&#35821;&#35328;&#20849;&#25351;&#35299;&#26512;
&lt;/p&gt;
&lt;p&gt;
Multilingual Coreference Resolution in Low-resource South Asian Languages
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13571
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#19968;&#20010;&#29992;&#20110;31&#31181;&#21335;&#20122;&#35821;&#35328;&#30340;&#22810;&#35821;&#35328;&#20849;&#25351;&#35299;&#26512;&#32763;&#35793;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#21033;&#29992;&#29616;&#25104;&#24037;&#20855;&#36827;&#34892;&#35757;&#32451;&#21644;&#23545;&#40784;&#65292;&#22312;&#20302;&#36164;&#28304;&#26465;&#20214;&#19979;&#23454;&#29616;&#20102;&#36739;&#22909;&#30340;&#20849;&#25351;&#35299;&#26512;&#27169;&#22411;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20849;&#25351;&#35299;&#26512;&#28041;&#21450;&#35782;&#21035;&#22312;&#35805;&#35821;&#20013;&#25351;&#21521;&#21516;&#19968;&#29616;&#23454;&#23454;&#20307;&#30340;&#25991;&#26412;&#29255;&#27573;&#30340;&#20219;&#21153;&#12290;&#34429;&#28982;&#36825;&#19968;&#20219;&#21153;&#22312;&#33521;&#35821;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#22312;&#21335;&#20122;&#35821;&#35328;&#20013;&#65292;&#20844;&#24320;&#21487;&#35775;&#38382;&#30340;&#20849;&#25351;&#35299;&#26512;&#36164;&#28304;&#21644;&#27169;&#22411;&#30456;&#23545;&#31232;&#32570;&#12290;&#25105;&#20204;&#21033;&#29992;&#29616;&#25104;&#30340;&#32763;&#35793;&#21644;&#35789;&#23545;&#40784;&#24037;&#20855;&#65292;&#22312;31&#31181;&#21335;&#20122;&#35821;&#35328;&#20013;&#24341;&#20837;&#20102;&#19968;&#20010;&#29992;&#20110;&#22810;&#35821;&#35328;&#20849;&#25351;&#35299;&#26512;&#30340;&#32763;&#35793;&#25968;&#25454;&#38598;&#65288;TransMuCoRes&#65289;&#12290;&#20960;&#20046;&#25152;&#26377;&#39044;&#27979;&#30340;&#32763;&#35793;&#37117;&#36890;&#36807;&#20102;&#21512;&#29702;&#24615;&#26816;&#26597;&#65292;75%&#30340;&#33521;&#35821;&#21442;&#32771;&#25991;&#29486;&#19982;&#20854;&#39044;&#27979;&#30340;&#32763;&#35793;&#30456;&#23545;&#24212;&#12290;&#21033;&#29992;&#22810;&#35821;&#35328;&#32534;&#30721;&#22120;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#20004;&#31181;&#29616;&#25104;&#30340;&#20849;&#25351;&#35299;&#26512;&#27169;&#22411;&#65292;&#23558;TransMuCoRes&#19982;&#24102;&#26377;&#25163;&#21160;&#27880;&#37322;&#30340;&#21360;&#22320;&#35821;&#20849;&#25351;&#35299;&#26512;&#25968;&#25454;&#38598;&#25340;&#25509;&#22312;&#19968;&#36215;&#12290;&#26368;&#20339;&#34920;&#29616;&#27169;&#22411;&#22312;LEA F1&#21644;CoNLL F1&#19978;&#20998;&#21035;&#36798;&#21040;&#20102;64&#21644;68&#30340;&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13571v1 Announce Type: cross  Abstract: Coreference resolution involves the task of identifying text spans within a discourse that pertain to the same real-world entity. While this task has been extensively explored in the English language, there has been a notable scarcity of publicly accessible resources and models for coreference resolution in South Asian languages. We introduce a Translated dataset for Multilingual Coreference Resolution (TransMuCoRes) in 31 South Asian languages using off-the-shelf tools for translation and word-alignment. Nearly all of the predicted translations successfully pass a sanity check, and 75% of English references align with their predicted translations. Using multilingual encoders, two off-the-shelf coreference resolution models were trained on a concatenation of TransMuCoRes and a Hindi coreference resolution dataset with manual annotations. The best performing model achieved a score of 64 and 68 for LEA F1 and CoNLL F1, respectively, on o
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;"&#28857;&#26816;&#26597;&#31561;&#20215;&#24615;"&#65292;&#20026;&#21516;&#34892;&#39044;&#27979;&#26426;&#21046;&#30340;&#25928;&#21147;&#25552;&#20379;&#20102;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#24230;&#37327;</title><link>https://arxiv.org/abs/2402.13567</link><description>&lt;p&gt;
Spot Check Equivalence&#65306;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#20449;&#24687;&#24341;&#20986;&#26426;&#21046;&#24230;&#37327;
&lt;/p&gt;
&lt;p&gt;
Spot Check Equivalence: an Interpretable Metric for Information Elicitation Mechanisms
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13567
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;"&#28857;&#26816;&#26597;&#31561;&#20215;&#24615;"&#65292;&#20026;&#21516;&#34892;&#39044;&#27979;&#26426;&#21046;&#30340;&#25928;&#21147;&#25552;&#20379;&#20102;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#24230;&#37327;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#39640;&#36136;&#37327;&#25968;&#25454;&#23545;&#20110;AI&#31995;&#32479;&#22914;&#21516;&#27687;&#27668;&#19968;&#33324;&#37325;&#35201;&#65292;&#26377;&#25928;&#22320;&#20174;&#20247;&#21253;&#24037;&#20316;&#32773;&#20013;&#24341;&#20986;&#20449;&#24687;&#24050;&#25104;&#20026;&#24320;&#21457;&#39640;&#24615;&#33021;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#39318;&#35201;&#38382;&#39064;&#12290;&#20004;&#31181;&#26222;&#36941;&#30340;&#33539;&#24335;&#65292;&#21363;&#28857;&#26816;&#26597;&#21644;&#21516;&#34892;&#39044;&#27979;&#65292;&#20351;&#24471;&#35774;&#35745;&#26426;&#21046;&#26469;&#35780;&#20272;&#21644;&#28608;&#21169;&#26469;&#33258;&#20154;&#31867;&#26631;&#27880;&#32773;&#30340;&#39640;&#36136;&#37327;&#25968;&#25454;&#25104;&#20026;&#21487;&#33021;&#12290;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#33267;&#23569;&#25552;&#20986;&#20102;&#19977;&#31181;&#25351;&#26631;&#26469;&#27604;&#36739;&#36825;&#20123;&#25216;&#26415;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#19981;&#21516;&#30340;&#25351;&#26631;&#22312;&#19981;&#21516;&#32972;&#26223;&#19979;&#23548;&#33268;&#20102;&#20998;&#27495;&#29978;&#33267;&#30683;&#30462;&#30340;&#32467;&#26524;&#12290;&#26412;&#25991;&#23558;&#35843;&#21644;&#36825;&#20123;&#19981;&#21516;&#30340;&#25925;&#20107;&#65292;&#23637;&#31034;&#20854;&#20013;&#20004;&#20010;&#25351;&#26631;&#22312;&#26576;&#20123;&#32972;&#26223;&#19979;&#23454;&#38469;&#19978;&#26159;&#30456;&#21516;&#30340;&#65292;&#24182;&#35299;&#37322;&#31532;&#19977;&#20010;&#30340;&#20998;&#27495;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;"&#28857;&#26816;&#26597;&#31561;&#20215;&#24615;"&#26469;&#32479;&#19968;&#36825;&#20123;&#19981;&#21516;&#30340;&#32972;&#26223;&#65292;&#20026;&#21516;&#34892;&#39044;&#27979;&#26426;&#21046;&#30340;&#26377;&#25928;&#24615;&#25552;&#20379;&#20102;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#24230;&#37327;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;...&#65288;&#26410;&#23436;&#65289;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13567v1 Announce Type: cross  Abstract: Because high-quality data is like oxygen for AI systems, effectively eliciting information from crowdsourcing workers has become a first-order problem for developing high-performance machine learning algorithms. Two prevalent paradigms, spot-checking and peer prediction, enable the design of mechanisms to evaluate and incentivize high-quality data from human labelers. So far, at least three metrics have been proposed to compare the performances of these techniques [33, 8, 3]. However, different metrics lead to divergent and even contradictory results in various contexts. In this paper, we harmonize these divergent stories, showing that two of these metrics are actually the same within certain contexts and explain the divergence of the third. Moreover, we unify these different contexts by introducing \textit{Spot Check Equivalence}, which offers an interpretable metric for the effectiveness of a peer prediction mechanism. Finally, we pr
&lt;/p&gt;</description></item><item><title>IGAP&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#22522;&#20110;&#22270;&#25552;&#31034;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23558;&#22270;&#39044;&#35757;&#32451;&#27867;&#21270;&#21040;&#24402;&#32435;&#22330;&#26223;&#65292;&#24357;&#21512;&#20102;&#39044;&#35757;&#32451;&#22270;&#21644;&#24494;&#35843;&#22270;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;</title><link>https://arxiv.org/abs/2402.13556</link><description>&lt;p&gt;
&#24402;&#32435;&#22270;&#23545;&#40784;&#25552;&#31034;&#65306;&#20174;&#35889;&#35282;&#24230;&#24357;&#21512;&#22270;&#39044;&#35757;&#32451;&#21644;&#24402;&#32435;&#24494;&#35843;&#20043;&#38388;&#30340;&#24046;&#36317;
&lt;/p&gt;
&lt;p&gt;
Inductive Graph Alignment Prompt: Bridging the Gap between Graph Pre-training and Inductive Fine-tuning From Spectral Perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13556
&lt;/p&gt;
&lt;p&gt;
IGAP&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#22522;&#20110;&#22270;&#25552;&#31034;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23558;&#22270;&#39044;&#35757;&#32451;&#27867;&#21270;&#21040;&#24402;&#32435;&#22330;&#26223;&#65292;&#24357;&#21512;&#20102;&#39044;&#35757;&#32451;&#22270;&#21644;&#24494;&#35843;&#22270;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#8220;&#22270;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#8221;&#33539;&#24335;&#36890;&#36807;&#25429;&#25417;&#19979;&#28216;&#20219;&#21153;&#26080;&#38656;&#25163;&#21160;&#26631;&#27880;&#30340;&#36890;&#29992;&#30693;&#35782;&#65292;&#26174;&#33879;&#25913;&#36827;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;(GNNs)&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#22312;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#38454;&#27573;&#20043;&#38388;&#30340;&#25968;&#25454;&#21644;&#20219;&#21153;&#24040;&#22823;&#24046;&#36317;&#65292;&#27169;&#22411;&#24615;&#33021;&#20173;&#28982;&#21463;&#38480;&#12290;&#21463;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#20013;&#25552;&#31034;&#24494;&#35843;&#30340;&#21551;&#21457;&#65292;&#35768;&#22810;&#21162;&#21147;&#24050;&#32463;&#20026;&#22312;&#22270;&#39046;&#22495;&#20013;&#24357;&#21512;&#24046;&#36317;&#20570;&#20986;&#20102;&#21162;&#21147;&#12290;&#20294;&#29616;&#26377;&#26041;&#27861;&#20165;&#20165;&#23558;&#24494;&#35843;&#20219;&#21153;&#30340;&#24418;&#24335;&#37325;&#26032;&#34920;&#36848;&#20026;&#39044;&#35757;&#32451;&#20219;&#21153;&#12290;&#22312;&#39044;&#35757;&#32451;&#22270;&#19982;&#24494;&#35843;&#22270;&#20860;&#23481;&#30340;&#21069;&#25552;&#19979;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#22312;&#36716;&#23548;&#35774;&#32622;&#20013;&#36816;&#34892;&#12290;&#20026;&#20102;&#23558;&#22270;&#39044;&#35757;&#32451;&#27867;&#21270;&#21040;&#24402;&#32435;&#22330;&#26223;&#65292;&#20854;&#20013;&#24494;&#35843;&#22270;&#21487;&#33021;&#19982;&#39044;&#35757;&#32451;&#22270;&#26174;&#33879;&#19981;&#21516;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#24402;&#32435;&#22270;&#23545;&#40784;&#25552;&#31034;(IGAP)&#30340;&#26032;&#22411;&#22522;&#20110;&#22270;&#25552;&#31034;&#30340;&#26041;&#27861;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#32479;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13556v1 Announce Type: cross  Abstract: The "Graph pre-training and fine-tuning" paradigm has significantly improved Graph Neural Networks(GNNs) by capturing general knowledge without manual annotations for downstream tasks. However, due to the immense gap of data and tasks between the pre-training and fine-tuning stages, the model performance is still limited. Inspired by prompt fine-tuning in Natural Language Processing(NLP), many endeavors have been made to bridge the gap in graph domain. But existing methods simply reformulate the form of fine-tuning tasks to the pre-training ones. With the premise that the pre-training graphs are compatible with the fine-tuning ones, these methods typically operate in transductive setting. In order to generalize graph pre-training to inductive scenario where the fine-tuning graphs might significantly differ from pre-training ones, we propose a novel graph prompt based method called Inductive Graph Alignment Prompt(IGAP). Firstly, we uni
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#31995;&#32479;&#35780;&#20272;&#20102;LLMs&#22312;&#35848;&#21028;&#23545;&#35805;&#20013;&#30340;&#22810;&#26041;&#38754;&#33021;&#21147;&#65292;&#25581;&#31034;&#20102;&#23427;&#20204;&#22312;&#35848;&#21028;&#30740;&#31350;&#20013;&#30340;&#28508;&#21147;&#21644;&#23616;&#38480;&#12290;</title><link>https://arxiv.org/abs/2402.13550</link><description>&lt;p&gt;
LLM&#20204;&#26159;&#26377;&#25928;&#30340;&#35848;&#21028;&#32773;&#21527;&#65311;&#23545;LLM&#22312;&#35848;&#21028;&#23545;&#35805;&#20013;&#22810;&#26041;&#38754;&#33021;&#21147;&#30340;&#31995;&#32479;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Are LLMs Effective Negotiators? Systematic Evaluation of the Multifaceted Capabilities of LLMs in Negotiation Dialogues
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13550
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#31995;&#32479;&#35780;&#20272;&#20102;LLMs&#22312;&#35848;&#21028;&#23545;&#35805;&#20013;&#30340;&#22810;&#26041;&#38754;&#33021;&#21147;&#65292;&#25581;&#31034;&#20102;&#23427;&#20204;&#22312;&#35848;&#21028;&#30740;&#31350;&#20013;&#30340;&#28508;&#21147;&#21644;&#23616;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#27425;&#25104;&#21151;&#30340;&#35848;&#21028;&#38656;&#35201;&#23545;&#35848;&#35805;&#32972;&#26223;&#26377;&#28145;&#21051;&#29702;&#35299;&#65292;&#20855;&#22791;&#25512;&#26029;&#23545;&#26041;&#21160;&#26426;&#30340;&#24515;&#29702;&#29702;&#35770;&#25216;&#33021;&#65292;&#20197;&#21450;&#25112;&#30053;&#25512;&#29702;&#21644;&#26377;&#25928;&#27807;&#36890;&#65292;&#36825;&#20351;&#24471;&#33258;&#21160;&#21270;&#31995;&#32479;&#38754;&#20020;&#25361;&#25112;&#12290;&#37492;&#20110;LLMs&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#32034;LLMs&#22914;&#20309;&#25512;&#21160;&#35848;&#21028;&#30740;&#31350;&#30340;&#19981;&#21516;&#26041;&#38754;&#65292;&#21253;&#25324;&#35774;&#35745;&#23545;&#35805;&#31995;&#32479;&#12289;&#25552;&#20379;&#25945;&#23398;&#21453;&#39304;&#21644;&#25193;&#22823;&#25968;&#25454;&#25910;&#38598;&#23454;&#36341;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#20998;&#26512;LLMs&#22312;&#21508;&#31181;&#23545;&#35805;&#24773;&#26223;&#20013;&#30340;&#22810;&#26041;&#38754;&#33021;&#21147;&#65292;&#28085;&#30422;&#20856;&#22411;&#35848;&#21028;&#20114;&#21160;&#30340;&#25152;&#26377;&#26102;&#38388;&#38454;&#27573;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#36827;&#19968;&#27493;&#35777;&#26126;&#20102;GPT-4&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#30340;&#20248;&#36234;&#24615;&#65292;&#21516;&#26102;&#20063;&#25581;&#31034;&#20102;LLMs&#22312;&#26576;&#20123;&#20219;&#21153;&#19978;&#20173;&#28982;&#22256;&#38590;&#30340;&#32454;&#33410;&#12290;&#20363;&#22914;&#65292;&#36825;&#20123;&#27169;&#22411;&#19982;&#20154;&#31867;&#30340;&#30456;&#20851;&#24615;&#36739;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13550v1 Announce Type: cross  Abstract: A successful negotiation demands a deep comprehension of the conversation context, Theory-of-Mind (ToM) skills to infer the partner's motives, as well as strategic reasoning and effective communication, making it challenging for automated systems. Given the remarkable performance of LLMs across a variety of NLP tasks, in this work, we aim to understand how LLMs can advance different aspects of negotiation research, ranging from designing dialogue systems to providing pedagogical feedback and scaling up data collection practices. To this end, we devise a methodology to analyze the multifaceted capabilities of LLMs across diverse dialogue scenarios covering all the time stages of a typical negotiation interaction. Our analysis adds to the increasing evidence for the superiority of GPT-4 across various tasks while also providing insights into specific tasks that remain difficult for LLMs. For instance, the models correlate poorly with hum
&lt;/p&gt;</description></item><item><title>ARL2&#25552;&#20986;&#20102;&#19968;&#31181;&#26816;&#32034;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#21033;&#29992;LLMs&#20316;&#20026;&#26631;&#27880;&#32773;&#65292;&#24182;&#37319;&#29992;&#33258;&#36866;&#24212;&#33258;&#35757;&#32451;&#31574;&#30053;&#65292;&#33021;&#22815;&#26377;&#25928;&#20943;&#23569;&#27880;&#37322;&#25104;&#26412;&#65292;&#24182;&#22312;NQ&#21644;MMLU&#19978;&#21462;&#24471;&#20102;5.4%&#21644;4.6%&#30340;&#20934;&#30830;&#24230;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2402.13542</link><description>&lt;p&gt;
ARL2: &#36890;&#36807;&#33258;&#23548;&#33258;&#36866;&#24212;&#30456;&#20851;&#24615;&#26631;&#35760;&#23558;&#26816;&#32034;&#22120;&#19982;&#40657;&#30418;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
ARL2: Aligning Retrievers for Black-box Large Language Models via Self-guided Adaptive Relevance Labeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13542
&lt;/p&gt;
&lt;p&gt;
ARL2&#25552;&#20986;&#20102;&#19968;&#31181;&#26816;&#32034;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#21033;&#29992;LLMs&#20316;&#20026;&#26631;&#27880;&#32773;&#65292;&#24182;&#37319;&#29992;&#33258;&#36866;&#24212;&#33258;&#35757;&#32451;&#31574;&#30053;&#65292;&#33021;&#22815;&#26377;&#25928;&#20943;&#23569;&#27880;&#37322;&#25104;&#26412;&#65292;&#24182;&#22312;NQ&#21644;MMLU&#19978;&#21462;&#24471;&#20102;5.4%&#21644;4.6%&#30340;&#20934;&#30830;&#24230;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13542v1 &#20844;&#21578;&#31867;&#22411;: &#20132;&#21449; &#25688;&#35201;: &#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#36890;&#36807;&#25972;&#21512;&#22806;&#37096;&#30693;&#35782;&#28304;&#30340;&#30456;&#20851;&#20449;&#24687;&#25913;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#20351;LLMs&#33021;&#22815;&#36866;&#24212;&#29305;&#23450;&#39046;&#22495;&#65292;&#24182;&#20943;&#36731;&#30693;&#35782;&#23494;&#38598;&#20219;&#21153;&#20013;&#30340;&#24187;&#35273;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#20998;&#24320;&#30340;&#35757;&#32451;&#36807;&#31243;&#21644;LLMs&#30340;&#40657;&#30418;&#29305;&#24615;&#65292;&#29616;&#26377;&#30340;&#26816;&#32034;&#22120;&#36890;&#24120;&#19982;LLMs&#19981;&#21305;&#37197;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ARL2&#65292;&#19968;&#31181;&#21033;&#29992;LLMs&#20316;&#20026;&#26631;&#27880;&#32773;&#30340;&#26816;&#32034;&#22120;&#23398;&#20064;&#25216;&#26415;&#12290;ARL2&#21033;&#29992;LLMs&#27880;&#37322;&#21644;&#35780;&#20998;&#30456;&#20851;&#35777;&#25454;&#65292;&#20174;&#32780;&#33021;&#22815;&#20174;&#24378;&#22823;&#30340;LLM&#30417;&#30563;&#20013;&#23398;&#20064;&#26816;&#32034;&#22120;&#12290;&#27492;&#22806;&#65292;ARL2&#20351;&#29992;&#33258;&#36866;&#24212;&#33258;&#35757;&#32451;&#31574;&#30053;&#26469;&#31574;&#21010;&#39640;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#30456;&#20851;&#24615;&#25968;&#25454;&#65292;&#21487;&#20197;&#26377;&#25928;&#38477;&#20302;&#26631;&#27880;&#25104;&#26412;&#12290;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;ARL2&#30340;&#26377;&#25928;&#24615;&#65292;&#19982;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#27604;&#65292;&#22312;NQ&#19978;&#25552;&#39640;&#20102;5.4%&#30340;&#20934;&#30830;&#29575;&#65292;&#22312;MMLU&#19978;&#25552;&#39640;&#20102;4.6%&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13542v1 Announce Type: cross  Abstract: Retrieval-augmented generation enhances large language models (LLMs) by incorporating relevant information from external knowledge sources. This enables LLMs to adapt to specific domains and mitigate hallucinations in knowledge-intensive tasks. However, existing retrievers are often misaligned with LLMs due to their separate training processes and the black-box nature of LLMs. To address this challenge, we propose ARL2, a retriever learning technique that harnesses LLMs as labelers. ARL2 leverages LLMs to annotate and score relevant evidence, enabling learning the retriever from robust LLM supervision. Furthermore, ARL2 uses an adaptive self-training strategy for curating high-quality and diverse relevance data, which can effectively reduce the annotation cost. Extensive experiments demonstrate the effectiveness of ARL2, achieving accuracy improvements of 5.4% on NQ and 4.6% on MMLU compared to the state-of-the-art methods. Additionall
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;GPT-4V&#21644;DALL-E3&#26469;&#25506;&#32034;&#22270;&#20687;&#21387;&#32553;&#30340;&#36136;&#37327;&#21644;&#21387;&#32553;&#30028;&#38480;&#65292;&#25512;&#21160;&#35821;&#20041;&#21387;&#32553;&#38477;&#33267;&#27599;&#20687;&#32032;100&#24494;&#27604;&#29305;&#65292;&#24182;&#36890;&#36807;&#36845;&#20195;&#21453;&#23556;&#36807;&#31243;&#25913;&#21892;&#35299;&#30721;&#22270;&#20687;&#12290;</title><link>https://arxiv.org/abs/2402.13536</link><description>&lt;p&gt;
&#25506;&#32034;&#27599;&#24494;&#27604;&#29305;&#35821;&#20041;&#22270;&#20687;&#21387;&#32553;&#30340;&#26497;&#38480;
&lt;/p&gt;
&lt;p&gt;
Exploring the Limits of Semantic Image Compression at Micro-bits per Pixel
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13536
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;GPT-4V&#21644;DALL-E3&#26469;&#25506;&#32034;&#22270;&#20687;&#21387;&#32553;&#30340;&#36136;&#37327;&#21644;&#21387;&#32553;&#30028;&#38480;&#65292;&#25512;&#21160;&#35821;&#20041;&#21387;&#32553;&#38477;&#33267;&#27599;&#20687;&#32032;100&#24494;&#27604;&#29305;&#65292;&#24182;&#36890;&#36807;&#36845;&#20195;&#21453;&#23556;&#36807;&#31243;&#25913;&#21892;&#35299;&#30721;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#26041;&#27861;&#65292;&#22914;JPEG&#65292;&#36890;&#36807;&#22788;&#29702;&#32467;&#26500;&#20449;&#24687;&#65288;&#22914;&#20687;&#32032;&#20540;&#25110;&#39057;&#29575;&#20869;&#23481;&#65289;&#26469;&#36827;&#34892;&#22270;&#20687;&#21387;&#32553;&#12290;&#22312;&#26631;&#20934;&#22270;&#20687;&#23610;&#23544;&#19979;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#27599;&#20687;&#32032;&#32422;&#19968;&#27604;&#29305;&#65288;bpp&#65289;&#21450;&#20197;&#19978;&#30340;&#27604;&#29305;&#29575;&#19978;&#26159;&#26377;&#25928;&#30340;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#22522;&#20110;&#25991;&#26412;&#30340;&#35821;&#20041;&#21387;&#32553;&#30452;&#25509;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#23384;&#20648;&#27010;&#24565;&#21450;&#20854;&#20851;&#31995;&#65292;&#36825;&#20123;&#20851;&#31995;&#24050;&#19982;&#20154;&#31867;&#19968;&#36215;&#28436;&#21464;&#65292;&#20197;&#26377;&#25928;&#34920;&#31034;&#36825;&#20123;&#31361;&#20986;&#30340;&#27010;&#24565;&#12290;&#36890;&#36807;&#24573;&#30053;&#20301;&#32622;&#12289;&#22823;&#23567;&#21644;&#26041;&#21521;&#31561;&#32467;&#26500;&#20449;&#24687;&#65292;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#22312;&#26497;&#20302;&#27604;&#29305;&#29575;&#19979;&#36816;&#34892;&#12290;&#26412;&#25991;&#21033;&#29992;OpenAI&#30340;GPT-4V&#21644;DALL-E3&#25506;&#32034;&#22270;&#20687;&#21387;&#32553;&#30340;&#36136;&#37327;-&#21387;&#32553;&#21069;&#27839;&#65292;&#24182;&#30830;&#23450;&#24403;&#21069;&#25216;&#26415;&#30340;&#38480;&#21046;&#12290;&#36890;&#36807;&#24341;&#20837;&#36845;&#20195;&#21453;&#23556;&#36807;&#31243;&#26469;&#25913;&#21892;&#35299;&#30721;&#22270;&#20687;&#65292;&#25105;&#20204;&#23558;&#35821;&#20041;&#21387;&#32553;&#25512;&#33267;&#27599;&#20687;&#32032;100&#24494;&#27604;&#29305;&#65288;&#27604;JPEG&#23567;$10,000\times$&#65289;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#20551;&#35774;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13536v1 Announce Type: cross  Abstract: Traditional methods, such as JPEG, perform image compression by operating on structural information, such as pixel values or frequency content. These methods are effective to bitrates around one bit per pixel (bpp) and higher at standard image sizes. In contrast, text-based semantic compression directly stores concepts and their relationships using natural language, which has evolved with humans to efficiently represent these salient concepts. These methods can operate at extremely low bitrates by disregarding structural information like location, size, and orientation. In this work, we use GPT-4V and DALL-E3 from OpenAI to explore the quality-compression frontier for image compression and identify the limitations of current technology. We push semantic compression as low as 100 $\mu$bpp (up to $10,000\times$ smaller than JPEG) by introducing an iterative reflection process to improve the decoded image. We further hypothesize this 100 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#19987;&#20026;&#24207;&#21015;&#26631;&#27880;&#20219;&#21153;&#35774;&#35745;&#30340;&#20004;&#38454;&#27573;&#35838;&#31243;&#23398;&#20064;&#65288;TCL&#65289;&#26694;&#26550;&#65292;&#36880;&#28176;&#24341;&#20837;&#25968;&#25454;&#23454;&#20363;&#20174;&#31616;&#21333;&#21040;&#22256;&#38590;&#65292;&#26088;&#22312;&#25552;&#39640;&#24615;&#33021;&#21644;&#35757;&#32451;&#36895;&#24230;&#65292;&#24182;&#19988;&#23545;&#20845;&#20010;&#20013;&#25991;&#20998;&#35789;&#21644;&#35789;&#24615;&#26631;&#27880;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#24191;&#27867;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.13534</link><description>&lt;p&gt;
&#19968;&#31181;&#26377;&#25928;&#34701;&#21512;&#24322;&#26500;&#30693;&#35782;&#30340;&#35838;&#31243;&#23398;&#20064;&#26041;&#27861;&#29992;&#20110;&#24207;&#21015;&#26631;&#27880;
&lt;/p&gt;
&lt;p&gt;
An Effective Incorporating Heterogeneous Knowledge Curriculum Learning for Sequence Labeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13534
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#19987;&#20026;&#24207;&#21015;&#26631;&#27880;&#20219;&#21153;&#35774;&#35745;&#30340;&#20004;&#38454;&#27573;&#35838;&#31243;&#23398;&#20064;&#65288;TCL&#65289;&#26694;&#26550;&#65292;&#36880;&#28176;&#24341;&#20837;&#25968;&#25454;&#23454;&#20363;&#20174;&#31616;&#21333;&#21040;&#22256;&#38590;&#65292;&#26088;&#22312;&#25552;&#39640;&#24615;&#33021;&#21644;&#35757;&#32451;&#36895;&#24230;&#65292;&#24182;&#19988;&#23545;&#20845;&#20010;&#20013;&#25991;&#20998;&#35789;&#21644;&#35789;&#24615;&#26631;&#27880;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#24191;&#27867;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24207;&#21015;&#26631;&#27880;&#27169;&#22411;&#24120;&#24120;&#21463;&#30410;&#20110;&#25972;&#21512;&#22806;&#37096;&#30693;&#35782;&#12290;&#28982;&#32780;&#65292;&#36825;&#19968;&#20570;&#27861;&#24341;&#20837;&#20102;&#25968;&#25454;&#24322;&#26500;&#24615;&#65292;&#24182;&#36890;&#36807;&#39069;&#22806;&#27169;&#22359;&#20351;&#27169;&#22411;&#21464;&#24471;&#22797;&#26434;&#65292;&#23548;&#33268;&#35757;&#32451;&#39640;&#24615;&#33021;&#27169;&#22411;&#30340;&#25104;&#26412;&#22686;&#21152;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#19987;&#20026;&#24207;&#21015;&#26631;&#27880;&#20219;&#21153;&#35774;&#35745;&#30340;&#20004;&#38454;&#27573;&#35838;&#31243;&#23398;&#20064;&#65288;TCL&#65289;&#26694;&#26550;&#12290;TCL&#26694;&#26550;&#36890;&#36807;&#36880;&#28176;&#24341;&#20837;&#20174;&#31616;&#21333;&#21040;&#22256;&#38590;&#30340;&#25968;&#25454;&#23454;&#20363;&#26469;&#22686;&#24378;&#35757;&#32451;&#65292;&#26088;&#22312;&#25552;&#39640;&#24615;&#33021;&#21644;&#35757;&#32451;&#36895;&#24230;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25506;&#32034;&#20102;&#29992;&#20110;&#35780;&#20272;&#24207;&#21015;&#26631;&#27880;&#20219;&#21153;&#38590;&#24230;&#32423;&#21035;&#30340;&#19981;&#21516;&#25351;&#26631;&#12290;&#36890;&#36807;&#22312;&#20845;&#20010;&#20013;&#25991;&#20998;&#35789;&#65288;CWS&#65289;&#21644;&#35789;&#24615;&#26631;&#27880;&#65288;POS&#65289;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#25552;&#39640;&#24207;&#21015;&#26631;&#27880;&#27169;&#22411;&#24615;&#33021;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;TCL&#21152;&#36895;&#20102;&#35757;&#32451;&#24182;&#32531;&#35299;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13534v1 Announce Type: cross  Abstract: Sequence labeling models often benefit from incorporating external knowledge. However, this practice introduces data heterogeneity and complicates the model with additional modules, leading to increased expenses for training a high-performing model. To address this challenge, we propose a two-stage curriculum learning (TCL) framework specifically designed for sequence labeling tasks. The TCL framework enhances training by gradually introducing data instances from easy to hard, aiming to improve both performance and training speed. Furthermore, we explore different metrics for assessing the difficulty levels of sequence labeling tasks. Through extensive experimentation on six Chinese word segmentation (CWS) and Part-of-speech tagging (POS) datasets, we demonstrate the effectiveness of our model in enhancing the performance of sequence labeling models. Additionally, our analysis indicates that TCL accelerates training and alleviates the 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39640;&#24615;&#33021;GPU&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#20302;&#31209;&#32467;&#26500;&#26469;&#39640;&#25928;&#22320;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#32447;&#24615;&#23618;&#20887;&#20313;&#24615;&#12289;GPU&#20869;&#23384;&#21344;&#29992;&#21644;&#20998;&#24067;&#24335;&#35757;&#32451;&#20013;GPU&#21033;&#29992;&#29575;&#19981;&#36275;&#30340;&#25361;&#25112;</title><link>https://arxiv.org/abs/2402.13533</link><description>&lt;p&gt;
FinGPT-HPC: &#39640;&#24615;&#33021;&#35745;&#31639;&#19979;&#29992;&#20110;&#37329;&#34701;&#24212;&#29992;&#30340;&#39640;&#25928;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
FinGPT-HPC: Efficient Pretraining and Finetuning Large Language Models for Financial Applications with High-Performance Computing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13533
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39640;&#24615;&#33021;GPU&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#20302;&#31209;&#32467;&#26500;&#26469;&#39640;&#25928;&#22320;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#32447;&#24615;&#23618;&#20887;&#20313;&#24615;&#12289;GPU&#20869;&#23384;&#21344;&#29992;&#21644;&#20998;&#24067;&#24335;&#35757;&#32451;&#20013;GPU&#21033;&#29992;&#29575;&#19981;&#36275;&#30340;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#35745;&#31639;&#23494;&#38598;&#24615;&#24456;&#39640;&#12290;&#35745;&#31639;&#24037;&#20316;&#37327;&#21644;&#20869;&#23384;&#21344;&#29992;&#37327;&#38543;&#32500;&#24230;(&#23618;&#23485;&#24230;)&#30340;&#22686;&#21152;&#21576;&#20108;&#27425;&#22686;&#38271;&#12290;&#22823;&#22810;&#25968;LLM&#21442;&#25968;&#26469;&#33258;&#21464;&#21387;&#22120;&#32467;&#26500;&#30340;&#32447;&#24615;&#23618;&#65292;&#20855;&#26377;&#39640;&#24230;&#20887;&#20313;&#24615;&#12290;&#36825;&#20123;&#32447;&#24615;&#23618;&#36129;&#29486;&#20102;&#36229;&#36807;80%&#30340;&#35745;&#31639;&#24037;&#20316;&#37327;&#21644;99%&#30340;&#27169;&#22411;&#22823;&#23567;&#12290;&#20026;&#20102;&#39640;&#25928;&#22320;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;LLMs&#65292;&#38656;&#35201;&#35299;&#20915;&#19977;&#20010;&#20027;&#35201;&#25361;&#25112;&#65306;1) &#20943;&#23569;&#32447;&#24615;&#23618;&#30340;&#20887;&#20313;&#24615;&#65307;2) &#20943;&#23569;GPU&#20869;&#23384;&#21344;&#29992;&#65307;3) &#22312;&#20351;&#29992;&#20998;&#24067;&#24335;&#35757;&#32451;&#26102;&#25552;&#39640;GPU&#21033;&#29992;&#29575;&#12290;&#20043;&#21069;&#30340;&#26041;&#27861;&#65292;&#22914;LoRA&#21644;QLoRA&#65292;&#21033;&#29992;&#20302;&#31209;&#30697;&#38453;&#21644;&#37327;&#21270;&#26469;&#20998;&#21035;&#20943;&#23569;&#21487;&#35757;&#32451;&#21442;&#25968;&#30340;&#25968;&#37327;&#21644;&#27169;&#22411;&#22823;&#23567;&#12290;&#28982;&#32780;&#65292; resulting model &#20173;&#28982;&#28040;&#32791;&#22823;&#37327;GPU&#20869;&#23384;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#39640;&#24615;&#33021;GPU&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#20302;&#31209;&#32467;&#26500;&#26469;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13533v1 Announce Type: cross  Abstract: Large language models (LLMs) are computationally intensive. The computation workload and the memory footprint grow quadratically with the dimension (layer width). Most of LLMs' parameters come from the linear layers of the transformer structure and are highly redundant. These linear layers contribute more than 80% of the computation workload and 99% of the model size. To pretrain and finetune LLMs efficiently, there are three major challenges to address: 1) reducing redundancy of the linear layers; 2) reducing GPU memory footprint; 3) improving GPU utilization when using distributed training. Prior methods, such as LoRA and QLoRA, utilized low-rank matrices and quantization to reduce the number of trainable parameters and model size, respectively. However, the resulting model still consumes a large amount of GPU memory. In this paper, we present high-performance GPU-based methods that exploit low-rank structures to pretrain and finetun
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#30740;&#31350;&#36890;&#36807;&#27979;&#35797;&#39537;&#21160;&#24320;&#21457;&#65288;TDD&#65289;&#26041;&#27861;&#65292;&#23558;&#38382;&#39064;&#25551;&#36848;&#21644;&#27979;&#35797;&#20316;&#20026;&#36755;&#20837;&#26159;&#21542;&#20248;&#20110;&#20165;&#23558;&#38382;&#39064;&#25551;&#36848;&#20316;&#20026;&#36755;&#20837;&#65292;&#20197;&#25552;&#39640;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#20195;&#30721;&#29983;&#25104;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.13521</link><description>&lt;p&gt;
&#38754;&#21521;&#20195;&#30721;&#29983;&#25104;&#30340;&#27979;&#35797;&#39537;&#21160;&#24320;&#21457;
&lt;/p&gt;
&lt;p&gt;
Test-Driven Development for Code Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13521
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#30740;&#31350;&#36890;&#36807;&#27979;&#35797;&#39537;&#21160;&#24320;&#21457;&#65288;TDD&#65289;&#26041;&#27861;&#65292;&#23558;&#38382;&#39064;&#25551;&#36848;&#21644;&#27979;&#35797;&#20316;&#20026;&#36755;&#20837;&#26159;&#21542;&#20248;&#20110;&#20165;&#23558;&#38382;&#39064;&#25551;&#36848;&#20316;&#20026;&#36755;&#20837;&#65292;&#20197;&#25552;&#39640;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#20195;&#30721;&#29983;&#25104;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13521v1 &#20844;&#21578;&#31867;&#22411;:&#20132;&#21449;&#25688;&#35201;:&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;GPT4&#24050;&#32463;&#23637;&#31034;&#20986;&#20102;&#20174;&#38382;&#39064;&#25551;&#36848;&#20013;&#29983;&#25104;&#20195;&#30721;&#29255;&#27573;&#30340;&#33021;&#21147;&#12290;&#20256;&#32479;&#19978;&#65292;&#20154;&#31867;&#32534;&#20889;&#36719;&#20214;&#30340;&#26041;&#27861;&#31867;&#20284;&#20110;&#20174;&#38382;&#39064;&#25551;&#36848;&#25110;&#38656;&#27714;&#20013;&#32534;&#20889;&#20195;&#30721;&#12290;&#28982;&#32780;&#65292;&#36807;&#21435;&#26377;&#20960;&#39033;&#30740;&#31350;&#24050;&#32463;&#26174;&#31034;&#20102;&#27979;&#35797;&#39537;&#21160;&#24320;&#21457;&#65288;TDD&#65289;&#30340;&#20215;&#20540;&#65292;&#21363;&#22312;&#32534;&#20889;&#21151;&#33021;&#20195;&#30721;&#20043;&#21069;&#65292;&#20154;&#31867;&#26681;&#25454;&#38382;&#39064;&#25551;&#36848;&#32534;&#20889;&#27979;&#35797;&#12290;&#22312;&#22522;&#20110;LLM&#30340;&#20195;&#30721;&#29983;&#25104;&#29615;&#22659;&#20013;&#65292;TDD&#30340;&#19968;&#20010;&#26126;&#26174;&#22909;&#22788;&#26159;&#24320;&#21457;&#20154;&#21592;&#21487;&#20197;&#30830;&#20999;&#30693;&#36947;&#29983;&#25104;&#30340;&#20195;&#30721;&#26159;&#21542;&#36890;&#36807;&#20102;&#25152;&#26377;&#32473;&#23450;&#30340;&#27979;&#35797;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24076;&#26395;&#36890;&#36807;&#23454;&#35777;&#35780;&#20272;&#20551;&#35774;&#65306;&#23558;&#38382;&#39064;&#25551;&#36848;&#21644;&#27979;&#35797;&#20316;&#20026;GPT4&#30340;&#36755;&#20837;&#35201;&#20248;&#20110;&#20165;&#23558;&#38382;&#39064;&#25551;&#36848;&#20316;&#20026;&#36755;&#20837;&#12290;&#20026;&#20102;&#27979;&#35797;&#25105;&#20204;&#30340;&#20551;&#35774;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#21517;&#20026;TGen&#30340;&#26694;&#26550;&#12290;&#22312;MBPP&#12289;HumanEval&#21644;CodeChef&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#20013;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13521v1 Announce Type: cross  Abstract: Large language models (LLMs) like GPT4, have shown proficiency in generating code snippets from problem statements. Traditionally software development by humans followed a similar methodology of writing code from problem statements or requirements. However, in the past, there have been several studies that have shown the value of test-driven development (TDD) where humans write tests based on problem statements before the code for the functionality is written. In the context of LLM-based code generation, one obvious benefit of TDD is that the developer then knows for sure if the generated code has passed all the given tests or not. Therefore, in this paper, we want to empirically evaluate the hypothesis: giving the problem statements and tests as input to GPT4 is better than just giving the problem statement as input. To test our hypothesis, we build a framework TGen. In our experiments on the MBPP, HumanEval and CodeChef datasets, we 
&lt;/p&gt;</description></item><item><title>&#24448;&#36820;&#32763;&#35793;&#65288;RTT&#65289;&#26041;&#27861;&#26159;&#31532;&#19968;&#20010;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#25269;&#24481;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#31038;&#20132;&#24037;&#31243;&#25915;&#20987;&#30340;&#31639;&#27861;&#65292;&#25104;&#21151;&#22320;&#20943;&#23569;&#20102;&#22810;&#31181;&#25915;&#20987;&#24418;&#24335;&#30340;&#25104;&#21151;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.13517</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36870;&#21521;&#32763;&#35793;&#38450;&#24481;&#23545;&#25239;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Round Trip Translation Defence against Large Language Model Jailbreaking Attacks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13517
&lt;/p&gt;
&lt;p&gt;
&#24448;&#36820;&#32763;&#35793;&#65288;RTT&#65289;&#26041;&#27861;&#26159;&#31532;&#19968;&#20010;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#25269;&#24481;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#31038;&#20132;&#24037;&#31243;&#25915;&#20987;&#30340;&#31639;&#27861;&#65292;&#25104;&#21151;&#22320;&#20943;&#23569;&#20102;&#22810;&#31181;&#25915;&#20987;&#24418;&#24335;&#30340;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23481;&#26131;&#21463;&#21040;&#31038;&#20132;&#24037;&#31243;&#25915;&#20987;&#65292;&#36825;&#20123;&#25915;&#20987;&#23545;&#20154;&#31867;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#65292;&#20294;&#38656;&#35201;LLMs&#20855;&#26377;&#39640;&#27700;&#24179;&#30340;&#29702;&#35299;&#33021;&#21147;&#25165;&#33021;&#25269;&#25239;&#12290;&#29616;&#26377;&#30340;&#38450;&#24481;&#25514;&#26045;&#26368;&#22810;&#21482;&#33021;&#32531;&#35299;&#36825;&#20123;&#25915;&#20987;&#30340;&#19981;&#21040;&#19968;&#21322;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24448;&#36820;&#32763;&#35793;&#65288;RTT&#65289;&#26041;&#27861;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#25269;&#24481;LLMs&#31038;&#20132;&#24037;&#31243;&#25915;&#20987;&#30340;&#31639;&#27861;&#12290;RTT&#20250;&#25913;&#20889;&#23545;&#25239;&#24615;&#25552;&#31034;&#24182;&#25512;&#24191;&#34920;&#36798;&#30340;&#24605;&#24819;&#65292;&#20351;LLMs&#26356;&#23481;&#26131;&#26816;&#27979;&#20986;&#35825;&#21457;&#26377;&#23475;&#34892;&#20026;&#12290;&#36825;&#31181;&#26041;&#27861;&#28789;&#27963;&#12289;&#36731;&#37327;&#19988;&#21487;&#36716;&#31227;&#33267;&#19981;&#21516;&#30340;LLMs&#12290;&#25105;&#20204;&#30340;&#38450;&#24481;&#25104;&#21151;&#22320;&#32531;&#35299;&#20102;&#36229;&#36807;70%&#30340;Prompt Automatic Iterative Refinement (PAIR)&#25915;&#20987;&#65292;&#36825;&#26159;&#30446;&#21069;&#25105;&#20204;&#25152;&#30693;&#26368;&#26377;&#25928;&#30340;&#38450;&#24481;&#12290;&#25105;&#20204;&#20063;&#26159;&#39318;&#27425;&#23581;&#35797;&#32531;&#35299;MathsAttack&#65292;&#24182;&#23558;&#20854;&#25915;&#20987;&#25104;&#21151;&#29575;&#38477;&#20302;&#20102;&#36817;40%&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#24050;&#20844;&#24320;&#21457;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13517v1 Announce Type: cross  Abstract: Large language models (LLMs) are susceptible to social-engineered attacks that are human-interpretable but require a high level of comprehension for LLMs to counteract. Existing defensive measures can only mitigate less than half of these attacks at most. To address this issue, we propose the Round Trip Translation (RTT) method, the first algorithm specifically designed to defend against social-engineered attacks on LLMs. RTT paraphrases the adversarial prompt and generalizes the idea conveyed, making it easier for LLMs to detect induced harmful behavior. This method is versatile, lightweight, and transferrable to different LLMs. Our defense successfully mitigated over 70% of Prompt Automatic Iterative Refinement (PAIR) attacks, which is currently the most effective defense to the best of our knowledge. We are also the first to attempt mitigating the MathsAttack and reduced its attack success rate by almost 40%. Our code is publicly av
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;"ProSparse"&#30340;&#26377;&#25928;&#31232;&#30095;&#21270;&#26041;&#27861;&#65292;&#20197;&#25512;&#21160;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#26356;&#39640;&#30340;&#28608;&#27963;&#31232;&#30095;&#24615;&#32780;&#19981;&#38477;&#20302;&#27169;&#22411;&#24615;&#33021;</title><link>https://arxiv.org/abs/2402.13516</link><description>&lt;p&gt;
ProSparse: &#24341;&#20837;&#21644;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20869;&#37096;&#28608;&#27963;&#31232;&#30095;&#24615;
&lt;/p&gt;
&lt;p&gt;
ProSparse: Introducing and Enhancing Intrinsic Activation Sparsity within Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13516
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;"ProSparse"&#30340;&#26377;&#25928;&#31232;&#30095;&#21270;&#26041;&#27861;&#65292;&#20197;&#25512;&#21160;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#26356;&#39640;&#30340;&#28608;&#27963;&#31232;&#30095;&#24615;&#32780;&#19981;&#38477;&#20302;&#27169;&#22411;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Activation sparsity&#25351;&#30340;&#26159;&#28608;&#27963;&#36755;&#20986;&#20013;&#23384;&#22312;&#35768;&#22810;&#24369;&#36129;&#29486;&#20803;&#32032;&#12290;&#20316;&#20026;&#20351;&#29992;ReLU&#28608;&#27963;&#20989;&#25968;&#30340;&#27169;&#22411;&#30340;&#26222;&#36941;&#23646;&#24615;&#65292;&#24050;&#34987;&#35777;&#26126;&#26159;&#25552;&#39640;&#27169;&#22411;&#25512;&#29702;&#25928;&#29575;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#33539;&#20363;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#37319;&#29992;&#20102;&#27809;&#26377;&#20869;&#22312;&#28608;&#27963;&#31232;&#30095;&#24615;&#30340;&#28608;&#27963;&#20989;&#25968;&#65288;&#20363;&#22914;GELU&#21644;Swish&#65289;&#12290;&#19968;&#20123;&#26368;&#36817;&#30340;&#21162;&#21147;&#23581;&#35797;&#24341;&#20837;ReLU&#25110;&#20854;&#21464;&#20307;&#20316;&#20026;&#26367;&#20195;&#28608;&#27963;&#20989;&#25968;&#65292;&#20197;&#24110;&#21161;LLMs&#23454;&#29616;&#28608;&#27963;&#31232;&#30095;&#24615;&#21644;&#25512;&#29702;&#21152;&#36895;&#65292;&#20294;&#24456;&#23569;&#33021;&#21516;&#26102;&#33719;&#24471;&#39640;&#31232;&#30095;&#24230;&#21644;&#21487;&#27604;&#36739;&#30340;&#27169;&#22411;&#24615;&#33021;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;"ProSparse"&#30340;&#26377;&#25928;&#31232;&#30095;&#21270;&#26041;&#27861;&#65292;&#20197;&#25512;&#21160;LLMs&#23454;&#29616;&#26356;&#39640;&#30340;&#28608;&#27963;&#31232;&#30095;&#24615;&#32780;&#19981;&#38477;&#20302;&#27169;&#22411;&#24615;&#33021;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#23558;LLMs&#30340;&#28608;&#27963;&#20989;&#25968;&#26367;&#25442;&#20026;ReLU&#21518;&#65292;ProSparse&#37319;&#29992;&#28176;&#36827;&#31232;&#30095;&#27491;&#21017;&#21270;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13516v1 Announce Type: cross  Abstract: Activation sparsity refers to the existence of considerable weakly-contributed elements among activation outputs. As a prevalent property of the models using the ReLU activation function, it has been proven a promising paradigm to boost model inference efficiency. Nevertheless, most large language models (LLMs) adopt activation functions without intrinsic activation sparsity (e.g., GELU and Swish). Some recent efforts have explored introducing ReLU or its variants as the substitutive activation function to help LLMs achieve activation sparsity and inference acceleration, but few can simultaneously obtain high sparsity and comparable model performance. This paper introduces an effective sparsification method named "ProSparse" to push LLMs for higher activation sparsity without decreasing model performance. Specifically, after substituting the activation function of LLMs with ReLU, ProSparse adopts progressive sparsity regularization wit
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#38754;&#21521;&#32452;&#21512;&#26410;&#30693;&#38382;&#39064;&#30340;&#33258;&#25105;&#20998;&#32780;&#27835;&#20043;&#31639;&#27861;&#65292;&#24341;&#20837;&#20102;&#31532;&#19968;&#20010;&#32452;&#21512;&#26410;&#30693;&#38382;&#39064;&#38382;&#31572;&#25968;&#25454;&#38598;&#65288;CuQA&#65289;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#35843;&#29992;&#19981;&#21516;&#26041;&#27861;&#23454;&#29616;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.13514</link><description>&lt;p&gt;
&#33258;&#25105;&#20998;&#32780;&#27835;&#20043;&#65306;&#20309;&#26102;&#26816;&#32034;&#12289;&#20309;&#26102;&#29983;&#25104;&#65311;&#38754;&#21521;&#32452;&#21512;&#26410;&#30693;&#38382;&#39064;&#30340;&#33258;&#25105;&#20998;&#32780;&#27835;&#20043;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Self-DC: When to retrieve and When to generate? Self Divide-and-Conquer for Compositional Unknown Questions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13514
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#38754;&#21521;&#32452;&#21512;&#26410;&#30693;&#38382;&#39064;&#30340;&#33258;&#25105;&#20998;&#32780;&#27835;&#20043;&#31639;&#27861;&#65292;&#24341;&#20837;&#20102;&#31532;&#19968;&#20010;&#32452;&#21512;&#26410;&#30693;&#38382;&#39064;&#38382;&#31572;&#25968;&#25454;&#38598;&#65288;CuQA&#65289;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#35843;&#29992;&#19981;&#21516;&#26041;&#27861;&#23454;&#29616;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#32034;-&#28982;&#21518;&#38405;&#35835;&#21644;&#29983;&#25104;-&#28982;&#21518;&#38405;&#35835;&#26159;&#22788;&#29702;&#24320;&#25918;&#22495;&#38382;&#31572;&#20013;&#26410;&#30693;&#21644;&#24050;&#30693;&#38382;&#39064;&#30340;&#20004;&#31181;&#20856;&#22411;&#35299;&#20915;&#26041;&#26696;&#65292;&#21069;&#32773;&#26816;&#32034;&#24517;&#35201;&#30340;&#22806;&#37096;&#30693;&#35782;&#65292;&#21518;&#32773;&#21017;&#20419;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#21442;&#25968;&#20013;&#32534;&#30721;&#30340;&#20869;&#37096;&#24050;&#30693;&#30693;&#35782;&#12290;&#28982;&#32780;&#65292;&#36807;&#21435;&#24456;&#23569;&#26377;&#20316;&#21697;&#32771;&#34385;&#21040;&#32452;&#21512;&#26410;&#30693;&#38382;&#39064;&#65292;&#36825;&#20123;&#38382;&#39064;&#30001;&#20960;&#20010;&#24050;&#30693;&#25110;&#26410;&#30693;&#30340;&#23376;&#38382;&#39064;&#32452;&#25104;&#12290;&#22240;&#27492;&#65292;&#31616;&#21333;&#30340;&#20108;&#20803;&#20998;&#31867;&#65288;&#24050;&#30693;&#25110;&#26410;&#30693;&#65289;&#21464;&#24471;&#27425;&#20248;&#21644;&#20302;&#25928;&#65292;&#22240;&#20026;&#23427;&#20250;&#23545;&#27599;&#20010;&#32452;&#21512;&#26410;&#30693;&#38382;&#39064;&#36807;&#24230;&#35843;&#29992;&#22806;&#37096;&#26816;&#32034;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#32452;&#21512;&#26410;&#30693;&#38382;&#39064;&#38382;&#31572;&#25968;&#25454;&#38598;&#65288;CuQA&#65289;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#33258;&#25105;&#20998;&#32780;&#27835;&#20043;&#65288;Self-DC&#65289;&#26694;&#26550;&#65292;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#33258;&#36866;&#24212;&#22320;&#35843;&#29992;&#19981;&#21516;&#30340;&#26041;&#27861;&#65292;&#20174;&#32780;&#25552;&#39640;&#24615;&#33021;&#21644;&#25928;&#29575;&#12290;&#23454;&#39564;&#32467;&#26524;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#65288;CuQA&#21644;FreshQA&#65289;&#19978;&#34920;&#26126;&#8230;&#8230;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13514v1 Announce Type: cross  Abstract: Retrieve-then-read and generate-then-read are two typical solutions to handle unknown and known questions in open-domain question-answering, while the former retrieves necessary external knowledge and the later prompt the large language models to generate internal known knowledge encoded in the parameters. However, few of previous works consider the compositional unknown questions, which consist of several known or unknown sub-questions. Thus, simple binary classification (known or unknown) becomes sub-optimal and inefficient since it will call external retrieval excessively for each compositional unknown question. To this end, we propose the first Compositional unknown Question-Answering dataset (CuQA), and introduce a Self Divide-and-Conquer (Self-DC) framework to empower LLMs to adaptively call different methods on-demand, resulting in better performance and efficiency. Experimental results on two datasets (CuQA and FreshQA) demonst
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20174;&#33258;&#27880;&#24847;&#21147;&#27169;&#22411;&#21040;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#30340;&#36716;&#21464;&#65292;&#25581;&#31034;&#20102;&#29983;&#25104;Transformer&#21160;&#24577;&#30340;&#26426;&#29702;&#21644;&#30456;&#20851;&#26465;&#20214;&#65292;&#20026;&#19968;&#33268;&#20272;&#35745;&#25552;&#20379;&#20102;&#20445;&#35777;&#65292;&#24182;&#22312;IID&#26679;&#26412;&#19979;&#24314;&#31435;&#20102;&#26679;&#26412;&#22797;&#26434;&#24615;&#20445;&#35777;&#12290;</title><link>https://arxiv.org/abs/2402.13512</link><description>&lt;p&gt;
&#20174;&#33258;&#27880;&#24847;&#21147;&#21040;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#65306;&#25581;&#31034;&#29983;&#25104;Transformer&#30340;&#21160;&#24577;
&lt;/p&gt;
&lt;p&gt;
From Self-Attention to Markov Models: Unveiling the Dynamics of Generative Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13512
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20174;&#33258;&#27880;&#24847;&#21147;&#27169;&#22411;&#21040;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#30340;&#36716;&#21464;&#65292;&#25581;&#31034;&#20102;&#29983;&#25104;Transformer&#21160;&#24577;&#30340;&#26426;&#29702;&#21644;&#30456;&#20851;&#26465;&#20214;&#65292;&#20026;&#19968;&#33268;&#20272;&#35745;&#25552;&#20379;&#20102;&#20445;&#35777;&#65292;&#24182;&#22312;IID&#26679;&#26412;&#19979;&#24314;&#31435;&#20102;&#26679;&#26412;&#22797;&#26434;&#24615;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#35821;&#35328;&#27169;&#22411;&#20381;&#36182;Transformer&#26550;&#26500;&#21644;&#27880;&#24847;&#21147;&#26426;&#21046;&#26469;&#36827;&#34892;&#35821;&#35328;&#29702;&#35299;&#21644;&#25991;&#26412;&#29983;&#25104;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20174;&#19968;&#32452;&#25552;&#31034;&#21644;&#19982;&#27169;&#22411;&#37319;&#26679;&#30340;&#20851;&#32852;&#36755;&#20986;&#25968;&#25454;&#20013;&#23398;&#20064;&#19968;&#20010;&#21333;&#23618;&#33258;&#27880;&#24847;&#27169;&#22411;&#12290;&#25105;&#20204;&#39318;&#20808;&#24314;&#31435;&#20102;&#33258;&#27880;&#24847;&#26426;&#21046;&#21644;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#20043;&#38388;&#30340;&#31934;&#30830;&#26144;&#23556;&#65306;&#23558;&#25552;&#31034;&#36755;&#20837;&#27169;&#22411;&#20250;&#26681;&#25454;&#19978;&#19979;&#25991;&#26465;&#20214;&#30340;&#39532;&#23572;&#21487;&#22827;&#38142;&#65288;CCMC&#65289;&#23545;&#36755;&#20986;&#26631;&#35760;&#36827;&#34892;&#37319;&#26679;&#65292;&#35813;&#38142;&#21152;&#26435;&#20102;&#22522;&#26412;&#39532;&#23572;&#21487;&#22827;&#38142;&#30340;&#36716;&#31227;&#30697;&#38453;&#12290;&#27492;&#22806;&#65292;&#24341;&#20837;&#20301;&#32622;&#32534;&#30721;&#23548;&#33268;&#20102;&#36716;&#31227;&#27010;&#29575;&#30340;&#20301;&#32622;&#30456;&#20851;&#32553;&#25918;&#12290;&#22522;&#20110;&#36825;&#31181;&#24418;&#24335;&#20027;&#20041;&#65292;&#25105;&#20204;&#20026;&#25552;&#31034;&#20998;&#24067;&#24320;&#21457;&#20102;&#21487;&#36776;&#35782;&#24615;/&#35206;&#30422;&#26465;&#20214;&#65292;&#30830;&#20445;&#19968;&#33268;&#20272;&#35745;&#65292;&#24182;&#22312;IID&#26679;&#26412;&#19979;&#24314;&#31435;&#20102;&#26679;&#26412;&#22797;&#26434;&#24615;&#20445;&#35777;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20174;&#21333;&#20010;&#36755;&#20986;&#36712;&#36857;&#29983;&#25104;&#20013;&#23398;&#20064;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13512v1 Announce Type: cross  Abstract: Modern language models rely on the transformer architecture and attention mechanism to perform language understanding and text generation. In this work, we study learning a 1-layer self-attention model from a set of prompts and associated output data sampled from the model. We first establish a precise mapping between the self-attention mechanism and Markov models: Inputting a prompt to the model samples the output token according to a context-conditioned Markov chain (CCMC) which weights the transition matrix of a base Markov chain. Additionally, incorporating positional encoding results in position-dependent scaling of the transition probabilities. Building on this formalism, we develop identifiability/coverage conditions for the prompt distribution that guarantee consistent estimation and establish sample complexity guarantees under IID samples. Finally, we study the problem of learning from a single output trajectory generated from
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20302;&#36164;&#28304;&#39046;&#22495;&#20219;&#21153;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#26469;&#33258;&#20854;&#20182;&#25968;&#25454;&#38598;&#30340;&#30456;&#20851;&#31034;&#20363;&#26469;&#22686;&#24378;&#35757;&#32451;&#25968;&#25454;&#65292;&#20197;&#35299;&#20915;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#20013;&#29983;&#25104;&#26679;&#26412;&#19981;&#22815;&#29702;&#24819;&#21644;&#32570;&#20047;&#22810;&#26679;&#24615;&#30340;&#25361;&#25112;</title><link>https://arxiv.org/abs/2402.13482</link><description>&lt;p&gt;
&#29992;&#20110;&#20302;&#36164;&#28304;&#39046;&#22495;&#20219;&#21153;&#30340;&#26816;&#32034;&#22686;&#24378;&#25968;&#25454;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Retrieval-Augmented Data Augmentation for Low-Resource Domain Tasks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13482
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20302;&#36164;&#28304;&#39046;&#22495;&#20219;&#21153;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#26469;&#33258;&#20854;&#20182;&#25968;&#25454;&#38598;&#30340;&#30456;&#20851;&#31034;&#20363;&#26469;&#22686;&#24378;&#35757;&#32451;&#25968;&#25454;&#65292;&#20197;&#35299;&#20915;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#20013;&#29983;&#25104;&#26679;&#26412;&#19981;&#22815;&#29702;&#24819;&#21644;&#32570;&#20047;&#22810;&#26679;&#24615;&#30340;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#26368;&#36817;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#26679;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#20294;&#22312;&#35757;&#32451;&#25968;&#25454;&#26377;&#38480;&#30340;&#20302;&#36164;&#28304;&#29615;&#22659;&#20013;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#20250;&#20005;&#37325;&#19979;&#38477;&#12290;&#35768;&#22810;&#29616;&#26377;&#20316;&#21697;&#36890;&#36807;&#20174;&#35757;&#32451;&#25968;&#25454;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#65292;&#28982;&#21518;&#22312;&#20854;&#19978;&#35757;&#32451;&#27169;&#22411;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26368;&#36817;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36827;&#34892;&#12290;&#28982;&#32780;&#65292;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#20013;&#65292;&#29992;&#20110;&#25968;&#25454;&#22686;&#24378;&#30340;&#31181;&#23376;&#25968;&#25454;&#26679;&#26412;&#25968;&#37327;&#38750;&#24120;&#23569;&#65292;&#36825;&#20351;&#24471;&#29983;&#25104;&#30340;&#26679;&#26412;&#19981;&#22815;&#29702;&#24819;&#19988;&#32570;&#20047;&#22810;&#26679;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#20854;&#20182;&#25968;&#25454;&#38598;&#20013;&#20016;&#23500;&#30340;&#31034;&#20363;&#19982;&#32473;&#23450;&#30340;&#35757;&#32451;&#25968;&#25454;&#32467;&#21512;&#36215;&#26469;&#65292;&#26469;&#22686;&#24378;&#35757;&#32451;&#25968;&#25454;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#19982;&#32473;&#23450;&#31181;&#23376;&#25968;&#25454;&#30456;&#20284;&#24615;&#22522;&#20110;&#20854;&#20182;&#25968;&#25454;&#38598;&#26816;&#32034;&#30456;&#20851;&#23454;&#20363;&#65292;&#20363;&#22914;&#23427;&#20204;&#30340;&#36755;&#20837;-&#36755;&#20986;&#23545;&#25110;&#19978;&#19979;&#25991;&#65292;&#28982;&#21518;&#25552;&#31034;LLM&#20351;&#29992;&#19978;&#19979;&#25991;&#20449;&#24687;&#29983;&#25104;&#26032;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13482v1 Announce Type: cross  Abstract: Despite large successes of recent language models on diverse tasks, they suffer from severe performance degeneration in low-resource settings with limited training data available. Many existing works tackle this problem by generating synthetic data from the training data and then training models on them, recently using Large Language Models (LLMs). However, in low-resource settings, the amount of seed data samples to use for data augmentation is very small, which makes generated samples suboptimal and less diverse. To tackle this challenge, we propose a novel method that augments training data by incorporating a wealth of examples from other datasets, along with the given training data. Specifically, we first retrieve the relevant instances from other datasets, such as their input-output pairs or contexts, based on their similarities with the given seed data, and then prompt LLMs to generate new samples with the contextual information 
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;Personality Modeling Network (PeMN)&#26469;&#27169;&#25311;&#39640;&#24230;&#20114;&#21160;&#22330;&#26223;&#20013;&#21508;&#31181;&#39550;&#39542;&#20132;&#20114;&#65292;&#36890;&#36807;&#35757;&#32451;&#32972;&#26223;&#20132;&#36890;&#27969;&#25913;&#21892;&#33258;&#36710;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;</title><link>https://arxiv.org/abs/2402.13481</link><description>&lt;p&gt;
&#29992;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#22312;&#39640;&#24230;&#20114;&#21160;&#33258;&#21160;&#39550;&#39542;&#22330;&#26223;&#20013;&#23398;&#20064;&#24314;&#27169;&#22810;&#26679;&#30340;&#39550;&#39542;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Learning to Model Diverse Driving Behaviors in Highly Interactive Autonomous Driving Scenarios with Multi-Agent Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13481
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;Personality Modeling Network (PeMN)&#26469;&#27169;&#25311;&#39640;&#24230;&#20114;&#21160;&#22330;&#26223;&#20013;&#21508;&#31181;&#39550;&#39542;&#20132;&#20114;&#65292;&#36890;&#36807;&#35757;&#32451;&#32972;&#26223;&#20132;&#36890;&#27969;&#25913;&#21892;&#33258;&#36710;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#35757;&#32451;&#30340;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#22312;&#35768;&#22810;&#39550;&#39542;&#22330;&#26223;&#20013;&#23637;&#31034;&#20986;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#22312;&#38754;&#23545;&#22810;&#26679;&#30340;&#39550;&#39542;&#39118;&#26684;&#21644;&#20010;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#39640;&#24230;&#20114;&#21160;&#30340;&#24773;&#20917;&#19979;&#65292;&#36825;&#20123;&#35757;&#32451;&#31574;&#30053;&#30340;&#24615;&#33021;&#21487;&#33021;&#20250;&#21463;&#21040;&#24433;&#21709;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Personality Modeling Network&#65288;PeMN&#65289;&#65292;&#20854;&#20013;&#21253;&#25324;&#19968;&#20010;&#21512;&#20316;&#20540;&#20989;&#25968;&#21644;&#20010;&#24615;&#21442;&#25968;&#65292;&#29992;&#26469;&#24314;&#27169;&#39640;&#24230;&#20114;&#21160;&#22330;&#26223;&#20013;&#30340;&#21508;&#31181;&#20132;&#20114;&#12290;PeMN&#36824;&#21487;&#20197;&#35757;&#32451;&#20855;&#26377;&#19981;&#21516;&#34892;&#20026;&#30340;&#32972;&#26223;&#20132;&#36890;&#27969;&#65292;&#20174;&#32780;&#25552;&#39640;&#33258;&#36710;&#30340;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#24191;&#27867;&#23454;&#39564;&#30740;&#31350;&#34701;&#20837;&#20102;&#19981;&#21516;&#30340;&#20010;&#24615;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13481v1 Announce Type: cross  Abstract: Autonomous vehicles trained through Multi-Agent Reinforcement Learning (MARL) have shown impressive results in many driving scenarios. However, the performance of these trained policies can be impacted when faced with diverse driving styles and personalities, particularly in highly interactive situations. This is because conventional MARL algorithms usually operate under the assumption of fully cooperative behavior among all agents and focus on maximizing team rewards during training. To address this issue, we introduce the Personality Modeling Network (PeMN), which includes a cooperation value function and personality parameters to model the varied interactions in high-interactive scenarios. The PeMN also enables the training of a background traffic flow with diverse behaviors, thereby improving the performance and generalization of the ego vehicle. Our extensive experimental studies, which incorporate different personality parameters
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;Multi-scale Spatio-temporal Transformer&#32593;&#32476;&#30340;&#22833;&#34913;&#32437;&#21521;&#23398;&#20064;&#26041;&#27861;&#65292;&#38024;&#23545;&#38738;&#20809;&#30524;&#39044;&#27979;&#20013;&#30340;&#19981;&#35268;&#21017;&#26102;&#38388;&#24207;&#21015;&#22270;&#20687;&#65292;&#33021;&#26377;&#25928;&#23398;&#20064;&#20195;&#34920;&#24615;&#35821;&#20041;&#20449;&#24687;.</title><link>https://arxiv.org/abs/2402.13475</link><description>&lt;p&gt;
&#22810;&#23610;&#24230;&#26102;&#31354;&#21464;&#21387;&#22120;&#30340;&#22833;&#34913;&#32437;&#21521;&#23398;&#20064;&#65292;&#29992;&#20110;&#20174;&#19981;&#35268;&#21017;&#26102;&#38388;&#24207;&#21015;&#22270;&#20687;&#20013;&#39044;&#27979;&#38738;&#20809;&#30524;
&lt;/p&gt;
&lt;p&gt;
Multi-scale Spatio-temporal Transformer-based Imbalanced Longitudinal Learning for Glaucoma Forecasting from Irregular Time Series Images
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13475
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;Multi-scale Spatio-temporal Transformer&#32593;&#32476;&#30340;&#22833;&#34913;&#32437;&#21521;&#23398;&#20064;&#26041;&#27861;&#65292;&#38024;&#23545;&#38738;&#20809;&#30524;&#39044;&#27979;&#20013;&#30340;&#19981;&#35268;&#21017;&#26102;&#38388;&#24207;&#21015;&#22270;&#20687;&#65292;&#33021;&#26377;&#25928;&#23398;&#20064;&#20195;&#34920;&#24615;&#35821;&#20041;&#20449;&#24687;.
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38738;&#20809;&#30524;&#26159;&#23548;&#33268;&#36827;&#34892;&#24615;&#35270;&#31070;&#32463;&#32420;&#32500;&#25439;&#20260;&#21644;&#19981;&#21487;&#36870;&#30450;&#30446;&#30340;&#20027;&#35201;&#30524;&#30149;&#20043;&#19968;&#65292;&#24433;&#21709;&#30528;&#25968;&#30334;&#19975;&#20154;&#12290;&#38738;&#20809;&#30524;&#39044;&#27979;&#26159;&#26089;&#26399;&#31579;&#26597;&#21644;&#24178;&#39044;&#28508;&#22312;&#24739;&#32773;&#30340;&#33391;&#22909;&#35299;&#20915;&#26041;&#26696;&#65292;&#26377;&#21161;&#20110;&#38450;&#27490;&#30142;&#30149;&#30340;&#36827;&#19968;&#27493;&#24694;&#21270;&#12290;&#23427;&#21033;&#29992;&#30524;&#30555;&#30340;&#19968;&#31995;&#21015;&#21382;&#21490;&#24213;&#29255;&#22270;&#20687;&#65292;&#24182;&#39044;&#27979;&#26410;&#26469;&#38738;&#20809;&#30524;&#21457;&#29983;&#30340;&#21487;&#33021;&#24615;&#12290;&#28982;&#32780;&#65292;&#19981;&#35268;&#21017;&#37319;&#26679;&#30340;&#24615;&#36136;&#21644;&#22833;&#34913;&#30340;&#31867;&#20998;&#24067;&#26159;&#30142;&#30149;&#39044;&#27979;&#26041;&#27861;&#24320;&#21457;&#20013;&#30340;&#20004;&#20010;&#25361;&#25112;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#21464;&#21387;&#22120;&#26550;&#26500;&#30340;&#36866;&#29992;&#20110;&#39034;&#24207;&#22270;&#20687;&#36755;&#20837;&#30340;&#22810;&#23610;&#24230;&#26102;&#31354;&#21464;&#21387;&#22120;&#32593;&#32476;&#65288;MST-former&#65289;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#20174;&#39034;&#24207;&#22270;&#20687;&#20013;&#22312;&#26102;&#38388;&#21644;&#31354;&#38388;&#32500;&#24230;&#19978;&#23398;&#20064;&#20195;&#34920;&#24615;&#35821;&#20041;&#20449;&#24687;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#37319;&#29992;&#22810;&#23610;&#24230;&#32467;&#26500;&#26469;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13475v1 Announce Type: cross  Abstract: Glaucoma is one of the major eye diseases that leads to progressive optic nerve fiber damage and irreversible blindness, afflicting millions of individuals. Glaucoma forecast is a good solution to early screening and intervention of potential patients, which is helpful to prevent further deterioration of the disease. It leverages a series of historical fundus images of an eye and forecasts the likelihood of glaucoma occurrence in the future. However, the irregular sampling nature and the imbalanced class distribution are two challenges in the development of disease forecasting approaches. To this end, we introduce the Multi-scale Spatio-temporal Transformer Network (MST-former) based on the transformer architecture tailored for sequential image inputs, which can effectively learn representative semantic information from sequential images on both temporal and spatial dimensions. Specifically, we employ a multi-scale structure to extract
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;RefuteBench&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#21453;&#39539;&#25351;&#20196;&#30340;&#36981;&#24490;&#33021;&#21147;&#65292;&#21457;&#29616;LLMs&#20542;&#21521;&#20110;&#22266;&#25191;&#20110;&#20854;&#20869;&#37096;&#30693;&#35782;&#32780;&#26080;&#27861;&#36981;&#20174;&#29992;&#25143;&#21453;&#39304;&#12290;</title><link>https://arxiv.org/abs/2402.13463</link><description>&lt;p&gt;
RefuteBench&#65306;&#35780;&#20272;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21453;&#39539;&#25351;&#20196;&#36981;&#24490;
&lt;/p&gt;
&lt;p&gt;
RefuteBench: Evaluating Refuting Instruction-Following for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13463
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;RefuteBench&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#21453;&#39539;&#25351;&#20196;&#30340;&#36981;&#24490;&#33021;&#21147;&#65292;&#21457;&#29616;LLMs&#20542;&#21521;&#20110;&#22266;&#25191;&#20110;&#20854;&#20869;&#37096;&#30693;&#35782;&#32780;&#26080;&#27861;&#36981;&#20174;&#29992;&#25143;&#21453;&#39304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24212;&#29992;&#33539;&#22260;&#26085;&#30410;&#25193;&#22823;&#12290;&#22312;&#23454;&#38469;&#20351;&#29992;&#20013;&#65292;&#29992;&#25143;&#21487;&#33021;&#26681;&#25454;&#27169;&#22411;&#30340;&#36755;&#20986;&#25552;&#20379;&#21453;&#39304;&#65292;&#24076;&#26395;&#24471;&#21040;&#19968;&#20010;&#21487;&#20197;&#26681;&#25454;&#20182;&#20204;&#30340;&#21453;&#39304;&#23436;&#25104;&#21709;&#24212;&#30340;&#21709;&#24212;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#27169;&#22411;&#33021;&#21542;&#24688;&#24403;&#22320;&#21709;&#24212;&#29992;&#25143;&#30340;&#21453;&#39539;&#21453;&#39304;&#24182;&#22987;&#32456;&#25191;&#34892;&#19979;&#21435;&#23578;&#26410;&#24471;&#21040;&#24443;&#24213;&#20998;&#26512;&#12290;&#22522;&#20110;&#36825;&#19968;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;RefuteBench&#65292;&#28085;&#30422;&#20102;&#35832;&#22914;&#38382;&#31572;&#12289;&#26426;&#22120;&#32763;&#35793;&#21644;&#30005;&#23376;&#37038;&#20214;&#25776;&#20889;&#31561;&#20219;&#21153;&#12290;&#35780;&#20272;&#26088;&#22312;&#35780;&#20272;&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#31215;&#26497;&#25509;&#21463;&#21453;&#39539;&#25351;&#20196;&#24418;&#24335;&#30340;&#21453;&#39304;&#65292;&#24182;&#26159;&#21542;&#33021;&#22815;&#22312;&#23545;&#35805;&#20013;&#22987;&#32456;&#36981;&#24490;&#29992;&#25143;&#38656;&#27714;&#12290;&#25105;&#20204;&#23545;&#20247;&#22810;LLMs&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#24182;&#21457;&#29616;LLMs&#20542;&#21521;&#22266;&#25191;&#65292;&#21363;&#20542;&#21521;&#20110;&#20854;&#20869;&#37096;&#30693;&#35782;&#65292;&#32463;&#24120;&#26410;&#33021;&#36981;&#23432;&#29992;&#25143;&#21453;&#39304;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13463v1 Announce Type: cross  Abstract: The application scope of large language models (LLMs) is increasingly expanding. In practical use, users might provide feedback based on the model's output, hoping for a responsive model that can complete responses according to their feedback. Whether the model can appropriately respond to users' refuting feedback and consistently follow through with execution has not been thoroughly analyzed. In light of this, this paper proposes a comprehensive benchmark, RefuteBench, covering tasks such as question answering, machine translation, and email writing. The evaluation aims to assess whether models can positively accept feedback in form of refuting instructions and whether they can consistently adhere to user demands throughout the conversation. We conduct evaluations on numerous LLMs and find that LLMs are stubborn, i.e. exhibit inclination to their internal knowledge, often failing to comply with user feedback. Additionally, as the leng
&lt;/p&gt;</description></item><item><title>&#27169;&#22411;&#32534;&#36753;&#26041;&#27861;&#22312;&#31038;&#20132;&#21435;&#20559;&#35265;&#20013;&#20855;&#26377;&#28508;&#21147;&#65292;&#20294;&#20063;&#38754;&#20020;&#25361;&#25112;&#65292;&#23588;&#20854;&#26159;&#22312;&#25903;&#25345;&#19981;&#21516;&#20559;&#35265;&#31867;&#22411;&#21644;&#29702;&#35299;&#32534;&#36753;&#26041;&#27861;&#24212;&#29992;&#20110;&#21435;&#20559;&#35265;&#36807;&#31243;&#20013;&#30340;&#21033;&#24330;&#26041;&#38754;&#12290;</title><link>https://arxiv.org/abs/2402.13462</link><description>&lt;p&gt;
&#27169;&#22411;&#32534;&#36753;&#22312;&#31038;&#20132;&#21435;&#20559;&#35265;&#20013;&#30340;&#28508;&#21147;&#19982;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Potential and Challenges of Model Editing for Social Debiasing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13462
&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#32534;&#36753;&#26041;&#27861;&#22312;&#31038;&#20132;&#21435;&#20559;&#35265;&#20013;&#20855;&#26377;&#28508;&#21147;&#65292;&#20294;&#20063;&#38754;&#20020;&#25361;&#25112;&#65292;&#23588;&#20854;&#26159;&#22312;&#25903;&#25345;&#19981;&#21516;&#20559;&#35265;&#31867;&#22411;&#21644;&#29702;&#35299;&#32534;&#36753;&#26041;&#27861;&#24212;&#29992;&#20110;&#21435;&#20559;&#35265;&#36807;&#31243;&#20013;&#30340;&#21033;&#24330;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#37327;&#35821;&#26009;&#24211;&#19978;&#35757;&#32451;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19981;&#21487;&#36991;&#20813;&#22320;&#23384;&#22312;&#21051;&#26495;&#21360;&#35937;&#20559;&#35265;&#12290;&#36890;&#36807;&#24494;&#35843;&#26469;&#20943;&#36731;&#36825;&#20123;&#20559;&#35265;&#21487;&#33021;&#26082;&#26114;&#36149;&#21448;&#38656;&#35201;&#22823;&#37327;&#25968;&#25454;&#12290;&#27169;&#22411;&#32534;&#36753;&#26041;&#27861;&#19987;&#27880;&#20110;&#20197;&#20107;&#21518;&#26041;&#24335;&#20462;&#25913;LLMs&#65292;&#23545;&#20110;&#35299;&#20915;&#21435;&#20559;&#35265;&#38382;&#39064;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#32570;&#20047;&#25903;&#25345;&#21508;&#31181;&#20559;&#35265;&#31867;&#22411;&#65292;&#24182;&#20102;&#35299;&#24212;&#29992;&#32534;&#36753;&#26041;&#27861;&#20110;&#21435;&#20559;&#35265;&#36807;&#31243;&#20013;&#30340;&#21033;&#24330;&#30340;&#32508;&#21512;&#30740;&#31350;&#12290;&#20026;&#22635;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#23558;&#31038;&#20132;&#21435;&#20559;&#35265;&#20180;&#32454;&#26500;&#24314;&#20026;&#19968;&#20010;&#32534;&#36753;&#38382;&#39064;&#65292;&#24182;&#22312;&#21051;&#26495;&#21360;&#35937;&#21435;&#20559;&#35265;&#19978;&#23545;&#19971;&#31181;&#29616;&#26377;&#30340;&#27169;&#22411;&#32534;&#36753;&#31639;&#27861;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;&#21363;&#21435;&#20559;&#35265;&#32534;&#36753;&#12290;&#25105;&#20204;&#22312;&#19977;&#31181;&#24773;&#26223;&#19979;&#30340;&#30740;&#31350;&#32467;&#26524;&#23637;&#31034;&#20102;&#21435;&#20559;&#35265;&#32534;&#36753;&#30340;&#28508;&#21147;&#19982;&#25361;&#25112;&#65306;&#65288;1&#65289;&#29616;&#26377;&#30340;&#27169;&#22411;&#32534;&#36753;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#20445;&#30041;&#30693;&#35782;&#24182;&#20943;&#36731;&#20559;&#35265;&#65292;&#21516;&#26102;&#20063;&#25581;&#31034;&#20102;&#21435;&#20559;&#35265;&#25928;&#26524;&#20174;&#32534;&#36753;&#21040;&#24212;&#29992;&#30340;&#19968;&#33324;&#21270;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13462v1 Announce Type: cross  Abstract: Large language models (LLMs) trained on vast corpora suffer from inevitable stereotype biases. Mitigating these biases with fine-tuning could be both costly and data-hungry. Model editing methods, which focus on modifying LLMs in a post-hoc manner, are of great potential to address debiasing. However, it lacks a comprehensive study that facilitates both internal and external model editing methods, supports various bias types, as well as understands the pros and cons of applying editing methods to stereotypical debiasing. To mitigate this gap, we carefully formulate social debiasing into an editing problem and benchmark seven existing model editing algorithms on stereotypical debiasing, i.e., debias editing. Our findings in three scenarios reveal both the potential and challenges of debias editing: (1) Existing model editing methods can effectively preserve knowledge and mitigate biases, while the generalization of debias effect from ed
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545;LLM&#27169;&#22411;&#30340;&#36234;&#29425;&#25915;&#20987;&#21644;&#38450;&#24481;&#25216;&#26415;&#36827;&#34892;&#20102;&#20840;&#38754;&#30740;&#31350;&#65292;&#25581;&#31034;&#20102;&#29616;&#26377;&#25915;&#20987;&#21644;&#38450;&#24481;&#25216;&#26415;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.13457</link><description>&lt;p&gt;
LLM&#36234;&#29425;&#25915;&#20987;&#19982;&#38450;&#24481;&#25216;&#26415;&#8212;&#19968;&#39033;&#20840;&#38754;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
LLM Jailbreak Attack versus Defense Techniques -- A Comprehensive Study
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13457
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;LLM&#27169;&#22411;&#30340;&#36234;&#29425;&#25915;&#20987;&#21644;&#38450;&#24481;&#25216;&#26415;&#36827;&#34892;&#20102;&#20840;&#38754;&#30740;&#31350;&#65292;&#25581;&#31034;&#20102;&#29616;&#26377;&#25915;&#20987;&#21644;&#38450;&#24481;&#25216;&#26415;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#36234;&#26469;&#36234;&#25104;&#20026;&#20135;&#29983;&#20855;&#26377;&#28508;&#22312;&#31038;&#20250;&#24433;&#21709;&#20869;&#23481;&#30340;&#26680;&#24515;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#36825;&#20123;&#27169;&#22411;&#23637;&#31034;&#20102;&#29983;&#25104;&#21487;&#33021;&#34987;&#35270;&#20026;&#26377;&#23475;&#30340;&#20869;&#23481;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#20943;&#36731;&#36825;&#20123;&#39118;&#38505;&#65292;&#30740;&#31350;&#20154;&#21592;&#37319;&#29992;&#20102;&#23433;&#20840;&#35757;&#32451;&#25216;&#26415;&#65292;&#20197;&#20351;&#27169;&#22411;&#36755;&#20986;&#19982;&#31038;&#20250;&#20215;&#20540;&#35266;&#20445;&#25345;&#19968;&#33268;&#65292;&#20197;&#36943;&#21046;&#23545;&#24694;&#24847;&#20869;&#23481;&#30340;&#29983;&#25104;&#12290;&#28982;&#32780;&#65292;&#8220;&#36234;&#29425;&#8221;&#29616;&#35937;&#65292;&#21363;&#31934;&#24515;&#21046;&#20316;&#30340;&#25552;&#31034;&#24341;&#21457;&#27169;&#22411;&#20135;&#29983;&#26377;&#23475;&#22238;&#24212;&#30340;&#24773;&#20917;&#65292;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#12290;&#26412;&#30740;&#31350;&#23545;&#29616;&#26377;&#20851;&#20110;&#36234;&#29425;LLMs&#21450;&#20854;&#38450;&#24481;&#25216;&#26415;&#30340;&#30740;&#31350;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#12290;&#25105;&#20204;&#23545;&#19977;&#31181;&#19981;&#21516;&#35821;&#35328;&#27169;&#22411;&#30340;&#20061;&#31181;&#25915;&#20987;&#25216;&#26415;&#21644;&#19971;&#31181;&#38450;&#24481;&#25216;&#26415;&#36827;&#34892;&#20102;&#32454;&#33268;&#35843;&#26597;&#65306;Vicuna&#12289;LLama&#21644;GPT-3.5 Turbo&#12290;&#25105;&#20204;&#26088;&#22312;&#35780;&#20272;&#36825;&#20123;&#25915;&#20987;&#21644;&#38450;&#24481;&#25216;&#26415;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#29616;&#26377;&#30340;&#30333;&#30418;&#25915;&#20987;u
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13457v1 Announce Type: cross  Abstract: Large Language Models (LLMS) have increasingly become central to generating content with potential societal impacts. Notably, these models have demonstrated capabilities for generating content that could be deemed harmful. To mitigate these risks, researchers have adopted safety training techniques to align model outputs with societal values to curb the generation of malicious content. However, the phenomenon of "jailbreaking", where carefully crafted prompts elicit harmful responses from models, persists as a significant challenge. This research conducts a comprehensive analysis of existing studies on jailbreaking LLMs and their defense techniques. We meticulously investigate nine attack techniques and seven defense techniques applied across three distinct language models: Vicuna, LLama, and GPT-3.5 Turbo. We aim to evaluate the effectiveness of these attack and defense techniques. Our findings reveal that existing white-box attacks u
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#24613;&#35786;&#31185;&#20013;&#20943;&#23569;&#31561;&#24453;&#26102;&#38388;&#30340;&#35786;&#26029;&#36741;&#21161;&#26041;&#27861;&#65292;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#24110;&#21161;&#21307;&#29983;&#36827;&#34892;&#24555;&#36895;&#20934;&#30830;&#30340;&#35786;&#26029;&#65292;&#24182;&#24320;&#21457;&#20102;ED-Copilot&#31995;&#32479;&#26469;&#25512;&#33616;&#23454;&#39564;&#23460;&#26816;&#27979;&#24182;&#36827;&#34892;&#35786;&#26029;&#39044;&#27979;&#12290;</title><link>https://arxiv.org/abs/2402.13448</link><description>&lt;p&gt;
ED-Copilot: &#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#35786;&#26029;&#36741;&#21161;&#20943;&#23569;&#24613;&#35786;&#31185;&#31561;&#24453;&#26102;&#38388;
&lt;/p&gt;
&lt;p&gt;
ED-Copilot: Reduce Emergency Department Wait Time with Language Model Diagnostic Assistance
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13448
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#24613;&#35786;&#31185;&#20013;&#20943;&#23569;&#31561;&#24453;&#26102;&#38388;&#30340;&#35786;&#26029;&#36741;&#21161;&#26041;&#27861;&#65292;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#24110;&#21161;&#21307;&#29983;&#36827;&#34892;&#24555;&#36895;&#20934;&#30830;&#30340;&#35786;&#26029;&#65292;&#24182;&#24320;&#21457;&#20102;ED-Copilot&#31995;&#32479;&#26469;&#25512;&#33616;&#23454;&#39564;&#23460;&#26816;&#27979;&#24182;&#36827;&#34892;&#35786;&#26029;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24613;&#35786;&#31185;&#65288;ED&#65289;&#20013;&#65292;&#24739;&#32773;&#22312;&#35786;&#26029;&#21069;&#38656;&#35201;&#36827;&#34892;&#20998;&#35786;&#21644;&#22810;&#31181;&#23454;&#39564;&#23460;&#26816;&#27979;&#12290;&#36825;&#20010;&#36807;&#31243;&#32791;&#26102;&#65292;&#23548;&#33268;&#24613;&#35786;&#31185;&#25317;&#25380;&#65292;&#26174;&#33879;&#24433;&#21709;&#24739;&#32773;&#27515;&#20129;&#29575;&#12289;&#21307;&#30103;&#38169;&#35823;&#12289;&#20154;&#21592;&#26543;&#31469;&#31561;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#65288;&#26102;&#38388;&#65289;&#25104;&#26412;&#26377;&#25928;&#30340;&#35786;&#26029;&#36741;&#21161;&#26041;&#27861;&#65292;&#25506;&#32034;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#22312;&#21327;&#21161;&#24613;&#35786;&#31185;&#20020;&#24202;&#21307;&#29983;&#36827;&#34892;&#39640;&#25928;&#20934;&#30830;&#35786;&#26029;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;&#20351;&#29992;&#20844;&#24320;&#21487;&#33719;&#24471;&#30340;&#24739;&#32773;&#25968;&#25454;&#65292;&#25105;&#20204;&#19982;&#24613;&#35786;&#31185;&#20020;&#24202;&#21307;&#29983;&#21512;&#20316;&#31574;&#21010;&#20102;MIMIC-ED-Assist&#65292;&#36825;&#26159;&#19968;&#20010;&#34913;&#37327;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#22312;&#24314;&#35758;&#26368;&#22823;&#31243;&#24230;&#20943;&#23569;&#24613;&#35786;&#31561;&#24453;&#26102;&#38388;&#30340;&#23454;&#39564;&#23460;&#26816;&#27979;&#65292;&#24182;&#22312;&#27491;&#30830;&#39044;&#27979;&#35832;&#22914;&#27515;&#20129;&#20043;&#31867;&#20851;&#38190;&#32467;&#26524;&#26041;&#38754;&#30340;&#33021;&#21147;&#30340;&#22522;&#20934;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;ED-Copilot&#65292;&#23427;&#20381;&#27425;&#24314;&#35758;&#24739;&#32773;&#29305;&#23450;&#30340;&#23454;&#39564;&#23460;&#26816;&#27979;&#24182;&#36827;&#34892;&#35786;&#26029;&#39044;&#27979;&#12290;ED-Copilot&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#29983;&#29289;&#21307;&#23398;&#35821;&#35328;&#27169;&#22411;&#23545;&#24739;&#32773;&#20449;&#24687;&#36827;&#34892;&#32534;&#30721;&#24182;&#36827;&#34892;&#22686;&#24378;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13448v1 Announce Type: cross  Abstract: In the emergency department (ED), patients undergo triage and multiple laboratory tests before diagnosis. This process is time-consuming, and causes ED crowding which significantly impacts patient mortality, medical errors, staff burnout, etc. This work proposes (time) cost-effective diagnostic assistance that explores the potential of artificial intelligence (AI) systems in assisting ED clinicians to make time-efficient and accurate diagnoses. Using publicly available patient data, we collaborate with ED clinicians to curate MIMIC-ED-Assist, a benchmark that measures the ability of AI systems in suggesting laboratory tests that minimize ED wait times, while correctly predicting critical outcomes such as death. We develop ED-Copilot which sequentially suggests patient-specific laboratory tests and makes diagnostic predictions. ED-Copilot uses a pre-trained bio-medical language model to encode patient information and reinforcement learn
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#31070;&#32463;&#31526;&#21495;&#36923;&#36753;&#31070;&#32463;&#32593;&#32476;&#65288;LNN&#65289;&#26694;&#26550;&#21644;&#27010;&#29575;&#36923;&#36753;&#31070;&#32463;&#32593;&#32476;&#65288;PLNN&#65289;&#26694;&#26550;&#65292;&#22312;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#23454;&#29616;&#21487;&#35299;&#37322;&#30340;&#20915;&#31574;&#21046;&#23450;&#21644;&#22788;&#29702;&#19981;&#30830;&#23450;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.13440</link><description>&lt;p&gt;
&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#21270;&#26041;&#27861;&#29992;&#20110;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65292;&#23454;&#29616;&#21487;&#35299;&#37322;&#24615;&#21644;&#27010;&#29575;&#20915;&#31574;
&lt;/p&gt;
&lt;p&gt;
A Neuro-Symbolic Approach to Multi-Agent RL for Interpretability and Probabilistic Decision Making
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13440
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#31070;&#32463;&#31526;&#21495;&#36923;&#36753;&#31070;&#32463;&#32593;&#32476;&#65288;LNN&#65289;&#26694;&#26550;&#21644;&#27010;&#29575;&#36923;&#36753;&#31070;&#32463;&#32593;&#32476;&#65288;PLNN&#65289;&#26694;&#26550;&#65292;&#22312;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#23454;&#29616;&#21487;&#35299;&#37322;&#30340;&#20915;&#31574;&#21046;&#23450;&#21644;&#22788;&#29702;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#38750;&#24120;&#36866;&#21512;&#22312;&#20248;&#21270;&#31995;&#32479;&#24615;&#33021;&#30340;&#23454;&#26102;&#20915;&#31574;&#20013;&#65292;&#20854;&#20013;&#22810;&#20010;&#26234;&#33021;&#20307;&#20849;&#23384;&#24182;&#31454;&#20105;&#20849;&#20139;&#36164;&#28304;&#12290;&#28982;&#32780;&#65292;&#23558;&#24120;&#35265;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;MARL&#35299;&#20915;&#26041;&#26696;&#24212;&#29992;&#20110;&#30495;&#23454;&#19990;&#30028;&#38382;&#39064;&#23384;&#22312;&#35299;&#37322;&#24615;&#12289;&#26679;&#26412;&#25928;&#29575;&#12289;&#37096;&#20998;&#21487;&#35266;&#27979;&#24615;&#31561;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20107;&#20214;&#39537;&#21160;&#30340;&#24418;&#24335;&#21270;&#26041;&#27861;&#65292;&#20915;&#31574;&#30001;&#20351;&#29992;&#31070;&#32463;&#31526;&#21495;&#26041;&#27861;&#30340;&#20998;&#24067;&#24335;&#21512;&#20316;MARL&#26234;&#33021;&#20307;&#22788;&#29702;&#12290;&#26368;&#36817;&#24341;&#20837;&#30340;&#31070;&#32463;&#31526;&#21495;&#36923;&#36753;&#31070;&#32463;&#32593;&#32476;&#65288;LNN&#65289;&#26694;&#26550;&#20316;&#20026;RL&#30340;&#20989;&#25968;&#36924;&#36817;&#22120;&#65292;&#35757;&#32451;&#19968;&#20010;&#22522;&#20110;&#35268;&#21017;&#30340;&#31574;&#30053;&#65292;&#36890;&#36807;&#32467;&#26500;&#26500;&#24314;&#26082;&#36923;&#36753;&#21448;&#21487;&#35299;&#37322;&#12290;&#20026;&#20102;&#22312;&#19981;&#30830;&#23450;&#24615;&#21644;&#37096;&#20998;&#21487;&#35266;&#27979;&#24615;&#19979;&#23454;&#29616;&#20915;&#31574;&#21046;&#23450;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27010;&#29575;&#31070;&#32463;&#31526;&#21495;&#26694;&#26550;&#65292;&#27010;&#29575;&#36923;&#36753;&#31070;&#32463;&#32593;&#32476;&#65288;PLNN&#65289;&#65292;&#32467;&#21512;&#20102;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13440v1 Announce Type: new  Abstract: Multi-agent reinforcement learning (MARL) is well-suited for runtime decision-making in optimizing the performance of systems where multiple agents coexist and compete for shared resources. However, applying common deep learning-based MARL solutions to real-world problems suffers from issues of interpretability, sample efficiency, partial observability, etc. To address these challenges, we present an event-driven formulation, where decision-making is handled by distributed co-operative MARL agents using neuro-symbolic methods. The recently introduced neuro-symbolic Logical Neural Networks (LNN) framework serves as a function approximator for the RL, to train a rules-based policy that is both logical and interpretable by construction. To enable decision-making under uncertainty and partial observability, we developed a novel probabilistic neuro-symbolic framework, Probabilistic Logical Neural Networks (PLNN), which combines the capabiliti
&lt;/p&gt;</description></item><item><title>DrBenchmark&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;&#27861;&#35821;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#30340;&#22823;&#22411;&#35821;&#35328;&#29702;&#35299;&#35780;&#20272;&#22522;&#20934;&#65292;&#26088;&#22312;&#24357;&#34917;&#23545;&#26368;&#26032;&#27861;&#35821;&#29983;&#29289;&#21307;&#23398;&#27169;&#22411;&#35780;&#20272;&#30340;&#19981;&#36275;&#65292;&#24182;&#32771;&#34385;&#21040;&#27861;&#35821;&#30340;&#29420;&#29305;&#25935;&#24863;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.13432</link><description>&lt;p&gt;
DrBenchmark: &#19968;&#20010;&#38024;&#23545;&#27861;&#35821;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#30340;&#22823;&#22411;&#35821;&#35328;&#29702;&#35299;&#35780;&#20272;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
DrBenchmark: A Large Language Understanding Evaluation Benchmark for French Biomedical Domain
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13432
&lt;/p&gt;
&lt;p&gt;
DrBenchmark&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;&#27861;&#35821;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#30340;&#22823;&#22411;&#35821;&#35328;&#29702;&#35299;&#35780;&#20272;&#22522;&#20934;&#65292;&#26088;&#22312;&#24357;&#34917;&#23545;&#26368;&#26032;&#27861;&#35821;&#29983;&#29289;&#21307;&#23398;&#27169;&#22411;&#35780;&#20272;&#30340;&#19981;&#36275;&#65292;&#24182;&#32771;&#34385;&#21040;&#27861;&#35821;&#30340;&#29420;&#29305;&#25935;&#24863;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#24341;&#36215;&#20102;&#26497;&#22823;&#30340;&#20852;&#36259;&#65292;&#38543;&#30528;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#30340;&#23454;&#36136;&#24615;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#19981;&#21516;&#27169;&#22411;&#20043;&#38388;&#35780;&#20272;&#21327;&#35758;&#30340;&#21464;&#21270;&#65292;&#27604;&#36739;&#36825;&#20123;&#27169;&#22411;&#24050;&#32463;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#19968;&#20010;&#20844;&#24179;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#23558;&#19981;&#21516;&#30340;&#19979;&#28216;&#20219;&#21153;&#32858;&#21512;&#21040;&#19968;&#20010;&#22522;&#20934;&#20013;&#65292;&#20801;&#35768;&#20174;&#21508;&#31181;&#35282;&#24230;&#35780;&#20272;PLMs&#30340;&#20869;&#22312;&#21697;&#36136;&#12290;&#23613;&#31649;&#36825;&#19968;&#20513;&#35758;&#20173;&#28982;&#23616;&#38480;&#20110;&#23569;&#25968;&#35821;&#35328;&#65292;&#29305;&#21035;&#26159;&#33521;&#35821;&#21644;&#20013;&#25991;&#65292;&#20294;&#24050;&#32463;&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#23637;&#24320;&#12290;&#36825;&#19968;&#38480;&#21046;&#38459;&#30861;&#20102;&#23545;&#26368;&#26032;&#30340;&#27861;&#35821;&#29983;&#29289;&#21307;&#23398;&#27169;&#22411;&#30340;&#35780;&#20215;&#65292;&#22240;&#20026;&#23427;&#20204;&#35201;&#20040;&#22312;&#23569;&#37327;&#20219;&#21153;&#19978;&#36827;&#34892;&#35780;&#20272;&#65292;&#32780;&#19988;&#20351;&#29992;&#30340;&#21327;&#35758;&#19981;&#22815;&#26631;&#20934;&#21270;&#65292;&#35201;&#20040;&#20351;&#29992;&#19968;&#33324;&#30340;&#19979;&#28216;&#20219;&#21153;&#36827;&#34892;&#35780;&#20272;&#12290;&#20026;&#24357;&#34917;&#36825;&#19968;&#30740;&#31350;&#24046;&#36317;&#65292;&#24182;&#32771;&#34385;&#21040;&#27861;&#35821;&#30340;&#29420;&#29305;&#25935;&#24863;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#39318;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#27861;&#35821;&#29983;&#29289;&#21307;&#23398;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13432v1 Announce Type: cross  Abstract: The biomedical domain has sparked a significant interest in the field of Natural Language Processing (NLP), which has seen substantial advancements with pre-trained language models (PLMs). However, comparing these models has proven challenging due to variations in evaluation protocols across different models. A fair solution is to aggregate diverse downstream tasks into a benchmark, allowing for the assessment of intrinsic PLMs qualities from various perspectives. Although still limited to few languages, this initiative has been undertaken in the biomedical field, notably English and Chinese. This limitation hampers the evaluation of the latest French biomedical models, as they are either assessed on a minimal number of tasks with non-standardized protocols or evaluated using general downstream tasks. To bridge this research gap and account for the unique sensitivities of French, we present the first-ever publicly available French biom
&lt;/p&gt;</description></item><item><title>LinkSAGE&#26159;&#19968;&#20010;&#37319;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#20248;&#21270;&#24037;&#20316;&#21305;&#37197;&#26694;&#26550;&#65292;&#36890;&#36807;&#29420;&#29305;&#30340;&#35757;&#32451;&#21644;&#26381;&#21153;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#22312;&#24222;&#22823;&#32780;&#22797;&#26434;&#30340;&#39046;&#33521;&#19987;&#19994;&#32593;&#32476;&#20013;&#36827;&#34892;&#20010;&#24615;&#21270;&#24037;&#20316;&#21305;&#37197;&#12290;</title><link>https://arxiv.org/abs/2402.13430</link><description>&lt;p&gt;
LinkSAGE: &#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#20248;&#21270;&#24037;&#20316;&#21305;&#37197;
&lt;/p&gt;
&lt;p&gt;
LinkSAGE: Optimizing Job Matching Using Graph Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13430
&lt;/p&gt;
&lt;p&gt;
LinkSAGE&#26159;&#19968;&#20010;&#37319;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#20248;&#21270;&#24037;&#20316;&#21305;&#37197;&#26694;&#26550;&#65292;&#36890;&#36807;&#29420;&#29305;&#30340;&#35757;&#32451;&#21644;&#26381;&#21153;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#22312;&#24222;&#22823;&#32780;&#22797;&#26434;&#30340;&#39046;&#33521;&#19987;&#19994;&#32593;&#32476;&#20013;&#36827;&#34892;&#20010;&#24615;&#21270;&#24037;&#20316;&#21305;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;LinkSAGE&#65292;&#19968;&#20010;&#21019;&#26032;&#24615;&#26694;&#26550;&#65292;&#23558;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#38598;&#25104;&#21040;&#22823;&#35268;&#27169;&#20010;&#24615;&#21270;&#24037;&#20316;&#21305;&#37197;&#31995;&#32479;&#20013;&#65292;&#26088;&#22312;&#24212;&#23545;&#39046;&#33521;&#24222;&#22823;&#19987;&#19994;&#32593;&#32476;&#30340;&#22797;&#26434;&#21160;&#24577;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;&#19968;&#20010;&#20840;&#26032;&#30340;&#24037;&#20316;&#24066;&#22330;&#22270;&#65292;&#36825;&#26159;&#24037;&#19994;&#30028;&#35268;&#27169;&#26368;&#22823;&#12289;&#26368;&#22797;&#26434;&#30340;&#22270;&#20043;&#19968;&#65292;&#25317;&#26377;&#25968;&#21313;&#20159;&#20010;&#33410;&#28857;&#21644;&#36793;&#12290;&#36825;&#20010;&#22270;&#19981;&#20165;&#24191;&#27867;&#65292;&#32780;&#19988;&#35814;&#32454;&#20016;&#23500;&#65292;&#21253;&#21547;&#20250;&#21592;&#21644;&#24037;&#20316;&#33410;&#28857;&#20197;&#21450;&#20851;&#38190;&#23646;&#24615;&#65292;&#20174;&#32780;&#21019;&#24314;&#20102;&#19968;&#20010;&#24191;&#38420;&#32780;&#20132;&#32455;&#30340;&#32593;&#32476;&#12290;LinkSAGE&#30340;&#19968;&#20010;&#20851;&#38190;&#21019;&#26032;&#22312;&#20110;&#20854;&#35757;&#32451;&#21644;&#26381;&#21153;&#26041;&#27861;&#65292;&#23427;&#26377;&#25928;&#22320;&#23558;&#24863;&#30693;&#22270;&#23398;&#20064;&#19982;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;GNN&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#35757;&#32451;GNN&#27169;&#22411;&#19982;&#29616;&#26377;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#27169;&#22411;&#30340;&#35757;&#32451;&#20998;&#31163;&#65292;&#28040;&#38500;&#20102;&#39057;&#32321;&#37325;&#26032;&#35757;&#32451;GNN&#30340;&#38656;&#35201;&#65292;&#21516;&#26102;&#20445;&#25345;&#22270;&#20449;&#21495;&#26368;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13430v1 Announce Type: cross  Abstract: We present LinkSAGE, an innovative framework that integrates Graph Neural Networks (GNNs) into large-scale personalized job matching systems, designed to address the complex dynamics of LinkedIns extensive professional network. Our approach capitalizes on a novel job marketplace graph, the largest and most intricate of its kind in industry, with billions of nodes and edges. This graph is not merely extensive but also richly detailed, encompassing member and job nodes along with key attributes, thus creating an expansive and interwoven network. A key innovation in LinkSAGE is its training and serving methodology, which effectively combines inductive graph learning on a heterogeneous, evolving graph with an encoder-decoder GNN model. This methodology decouples the training of the GNN model from that of existing Deep Neural Nets (DNN) models, eliminating the need for frequent GNN retraining while maintaining up-to-date graph signals in ne
&lt;/p&gt;</description></item><item><title>&#22240;&#26524;&#20998;&#26512;&#22312;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#20013;&#30340;&#24212;&#29992;&#25361;&#25112;&#24050;&#24471;&#21040;&#22522;&#26412;&#35299;&#20915;&#65292;&#24314;&#31435;&#20102;&#20005;&#26684;&#30340;&#22240;&#26524;&#20998;&#26512;&#24418;&#24335;&#20027;&#20041;&#65292;&#25512;&#21160;&#20102;&#22240;&#26524;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#30340;&#24314;&#31435;&#65292;&#20419;&#36827;&#20102;&#20174;&#22823;&#27668;&#28023;&#27915;&#31185;&#23398;&#21040;&#37327;&#23376;&#21147;&#23398;&#12289;&#31070;&#32463;&#31185;&#23398;&#12289;&#37329;&#34701;&#32463;&#27982;&#31561;&#39046;&#22495;&#30340;&#31185;&#23398;&#21457;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.13427</link><description>&lt;p&gt;
&#23450;&#37327;&#22240;&#26524;&#20851;&#31995;&#12289;&#22240;&#26524;&#24341;&#23548;&#30340;&#31185;&#23398;&#21457;&#29616;&#21644;&#22240;&#26524;&#26426;&#22120;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Quantitative causality, causality-guided scientific discovery, and causal machine learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13427
&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#20998;&#26512;&#22312;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#20013;&#30340;&#24212;&#29992;&#25361;&#25112;&#24050;&#24471;&#21040;&#22522;&#26412;&#35299;&#20915;&#65292;&#24314;&#31435;&#20102;&#20005;&#26684;&#30340;&#22240;&#26524;&#20998;&#26512;&#24418;&#24335;&#20027;&#20041;&#65292;&#25512;&#21160;&#20102;&#22240;&#26524;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#30340;&#24314;&#31435;&#65292;&#20419;&#36827;&#20102;&#20174;&#22823;&#27668;&#28023;&#27915;&#31185;&#23398;&#21040;&#37327;&#23376;&#21147;&#23398;&#12289;&#31070;&#32463;&#31185;&#23398;&#12289;&#37329;&#34701;&#32463;&#27982;&#31561;&#39046;&#22495;&#30340;&#31185;&#23398;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#20204;&#26366;&#35828;&#65292;&#22240;&#26524;&#20998;&#26512;&#24212;&#35813;&#20026;&#21487;&#35299;&#37322;&#30340;&#28145;&#24230;&#23398;&#20064;&#21644;&#27867;&#21270;&#38138;&#24179;&#19968;&#26465;&#20805;&#28385;&#24076;&#26395;&#30340;&#36947;&#36335;&#12290;&#28982;&#32780;&#65292;&#23558;&#22240;&#26524;&#24615;&#32435;&#20837;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#31639;&#27861;&#20013;&#38754;&#20020;&#30528;&#20854;&#27169;&#31946;&#24615;&#12289;&#38750;&#23450;&#37327;&#24615;&#12289;&#35745;&#31639;&#25928;&#29575;&#20302;&#31561;&#25361;&#25112;&#12290;&#22312;&#36807;&#21435;&#30340;18&#24180;&#20013;&#65292;&#36825;&#20123;&#25361;&#25112;&#24050;&#22522;&#26412;&#24471;&#21040;&#35299;&#20915;&#65292;&#24314;&#31435;&#20102;&#19968;&#20010;&#20005;&#26684;&#30340;&#22240;&#26524;&#20998;&#26512;&#24418;&#24335;&#20027;&#20041;&#65292;&#26368;&#21021;&#26159;&#21463;&#22823;&#27668;&#21487;&#39044;&#27979;&#24615;&#21551;&#21457;&#32780;&#24314;&#31435;&#30340;&#12290;&#36825;&#19981;&#20165;&#24320;&#21551;&#20102;&#22823;&#27668;&#28023;&#27915;&#31185;&#23398;&#20013;&#30340;&#19968;&#20010;&#26032;&#39046;&#22495;&#65292;&#21363;&#20449;&#24687;&#27969;&#65292;&#36824;&#36890;&#36807;&#21508;&#31181;&#24212;&#29992;&#22312;&#20854;&#20182;&#23398;&#31185;&#20013;&#21462;&#24471;&#20102;&#31185;&#23398;&#21457;&#29616;&#65292;&#22914;&#37327;&#23376;&#21147;&#23398;&#12289;&#31070;&#32463;&#31185;&#23398;&#12289;&#37329;&#34701;&#32463;&#27982;&#31561;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#23545;&#36807;&#21435;&#21313;&#24180;&#21162;&#21147;&#30340;&#31616;&#35201;&#22238;&#39038;&#65292;&#21253;&#25324;&#20027;&#35201;&#29702;&#35770;&#32467;&#26524;&#30340;&#21015;&#34920;&#12289;&#22240;&#26524;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#30340;&#27010;&#36848;&#20197;&#21450;&#19968;&#20123;&#20195;&#34920;&#24615;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13427v1 Announce Type: new  Abstract: It has been said, arguably, that causality analysis should pave a promising way to interpretable deep learning and generalization. Incorporation of causality into artificial intelligence (AI) algorithms, however, is challenged with its vagueness, non-quantitiveness, computational inefficiency, etc. During the past 18 years, these challenges have been essentially resolved, with the establishment of a rigorous formalism of causality analysis initially motivated from atmospheric predictability. This not only opens a new field in the atmosphere-ocean science, namely, information flow, but also has led to scientific discoveries in other disciplines, such as quantum mechanics, neuroscience, financial economics, etc., through various applications. This note provides a brief review of the decade-long effort, including a list of major theoretical results, a sketch of the causal deep learning framework, and some representative real-world applicati
&lt;/p&gt;</description></item><item><title>&#23398;&#20064;&#25972;&#20010;&#20998;&#24067;&#22312;&#22238;&#24402;&#20013;&#30340;&#24615;&#33021;&#25552;&#21319;&#20027;&#35201;&#26469;&#33258;&#20110;&#20248;&#21270;&#30340;&#25913;&#36827;&#65292;&#32780;&#19981;&#26159;&#23398;&#20064;&#26356;&#22909;&#30340;&#34920;&#31034;&#12290;</title><link>https://arxiv.org/abs/2402.13425</link><description>&lt;p&gt;
&#22312;&#22238;&#24402;&#20013;&#25506;&#35752;&#30452;&#26041;&#22270;&#25439;&#22833;
&lt;/p&gt;
&lt;p&gt;
Investigating the Histogram Loss in Regression
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13425
&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#25972;&#20010;&#20998;&#24067;&#22312;&#22238;&#24402;&#20013;&#30340;&#24615;&#33021;&#25552;&#21319;&#20027;&#35201;&#26469;&#33258;&#20110;&#20248;&#21270;&#30340;&#25913;&#36827;&#65292;&#32780;&#19981;&#26159;&#23398;&#20064;&#26356;&#22909;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36234;&#26469;&#36234;&#24120;&#35265;&#30340;&#26159;&#65292;&#22312;&#22238;&#24402;&#20013;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#26469;&#24314;&#27169;&#25972;&#20010;&#20998;&#24067;&#65292;&#21363;&#20351;&#21482;&#38656;&#35201;&#22343;&#20540;&#26469;&#36827;&#34892;&#39044;&#27979;&#12290; &#36825;&#31181;&#39069;&#22806;&#30340;&#24314;&#27169;&#36890;&#24120;&#20250;&#24102;&#26469;&#24615;&#33021;&#22686;&#30410;&#65292;&#20294;&#32972;&#21518;&#30340;&#21407;&#22240;&#23578;&#19981;&#23436;&#20840;&#28165;&#26970;&#12290; &#26412;&#25991;&#30740;&#31350;&#20102;&#22238;&#24402;&#20013;&#30340;&#19968;&#31181;&#26368;&#26032;&#26041;&#27861;&#65292;&#21363;&#30452;&#26041;&#22270;&#25439;&#22833;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#26368;&#23567;&#21270;&#30446;&#26631;&#20998;&#24067;&#21644;&#28789;&#27963;&#30452;&#26041;&#22270;&#39044;&#27979;&#20043;&#38388;&#30340;&#20132;&#21449;&#29109;&#26469;&#23398;&#20064;&#30446;&#26631;&#21464;&#37327;&#30340;&#26465;&#20214;&#20998;&#24067;&#12290; &#25105;&#20204;&#35774;&#35745;&#20102;&#29702;&#35770;&#21644;&#23454;&#35777;&#20998;&#26512;&#65292;&#20197;&#30830;&#23450;&#20026;&#20160;&#20040;&#20197;&#21450;&#20309;&#26102;&#20250;&#20986;&#29616;&#24615;&#33021;&#22686;&#30410;&#65292;&#20197;&#21450;&#25439;&#22833;&#30340;&#19981;&#21516;&#32452;&#20214;&#22914;&#20309;&#20026;&#27492;&#20570;&#20986;&#36129;&#29486;&#12290; &#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#36825;&#31181;&#35774;&#32622;&#20013;&#23398;&#20064;&#20998;&#24067;&#30340;&#22909;&#22788;&#26469;&#33258;&#20110;&#20248;&#21270;&#30340;&#25913;&#36827;&#65292;&#32780;&#19981;&#26159;&#23398;&#20064;&#26356;&#22909;&#30340;&#34920;&#31034;&#12290; &#28982;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#30452;&#26041;&#22270;&#25439;&#22833;&#22312;&#24120;&#35265;&#30340;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#20013;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13425v1 Announce Type: cross  Abstract: It is becoming increasingly common in regression to train neural networks that model the entire distribution even if only the mean is required for prediction. This additional modeling often comes with performance gain and the reasons behind the improvement are not fully known. This paper investigates a recent approach to regression, the Histogram Loss, which involves learning the conditional distribution of the target variable by minimizing the cross-entropy between a target distribution and a flexible histogram prediction. We design theoretical and empirical analyses to determine why and when this performance gain appears, and how different components of the loss contribute to it. Our results suggest that the benefits of learning distributions in this setup come from improvements in optimization rather than learning a better representation. We then demonstrate the viability of the Histogram Loss in common deep learning applications wi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22914;&#20309;&#30830;&#20445;&#27169;&#22411;&#35268;&#21010;&#20195;&#29702;&#22312;&#29305;&#23450;&#26102;&#38388;&#27493;&#20869;&#36798;&#21040;&#30446;&#26631;&#29366;&#24577;&#30340;&#34892;&#20026;&#20445;&#35777;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#30446;&#26631;&#29366;&#24577;&#22870;&#21169;&#30340;&#19979;&#30028;&#65292;&#23545;&#20110;&#20302;&#20110;&#35813;&#22870;&#21169;&#30340;&#24773;&#20917;&#26080;&#27861;&#33719;&#24471;&#20445;&#35777;&#12290;</title><link>https://arxiv.org/abs/2402.13419</link><description>&lt;p&gt;
&#22522;&#20110;&#34892;&#20026;&#20445;&#35777;&#30340;&#27169;&#22411;&#35268;&#21010;&#20195;&#29702;&#30340;&#22870;&#21169;&#19979;&#30028;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Reward Bound for Behavioral Guarantee of Model-based Planning Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13419
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22914;&#20309;&#30830;&#20445;&#27169;&#22411;&#35268;&#21010;&#20195;&#29702;&#22312;&#29305;&#23450;&#26102;&#38388;&#27493;&#20869;&#36798;&#21040;&#30446;&#26631;&#29366;&#24577;&#30340;&#34892;&#20026;&#20445;&#35777;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#30446;&#26631;&#29366;&#24577;&#22870;&#21169;&#30340;&#19979;&#30028;&#65292;&#23545;&#20110;&#20302;&#20110;&#35813;&#22870;&#21169;&#30340;&#24773;&#20917;&#26080;&#27861;&#33719;&#24471;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22312;&#26426;&#22120;&#23398;&#20064;&#20026;&#22522;&#30784;&#30340;&#20195;&#29702;&#22312;&#29616;&#23454;&#29615;&#22659;&#20013;&#30340;&#21487;&#20449;&#24230;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#20852;&#36259;&#65292;&#29305;&#21035;&#26159;&#22312;&#26426;&#22120;&#20154;&#39046;&#22495;&#65292;&#20026;&#35813;&#34892;&#19994;&#25552;&#20379;&#23433;&#20840;&#20445;&#35777;&#12290;&#20026;&#36825;&#20123;&#20195;&#29702;&#33719;&#21462;&#34892;&#20026;&#20445;&#35777;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#20445;&#35777;&#27169;&#22411;&#35268;&#21010;&#20195;&#29702;&#22312;&#29305;&#23450;&#26410;&#26469;&#26102;&#38388;&#27493;&#20869;&#36798;&#21040;&#30446;&#26631;&#29366;&#24577;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#22312;&#30446;&#26631;&#29366;&#24577;&#23384;&#22312;&#19968;&#20010;&#22870;&#21169;&#30340;&#19979;&#30028;&#65292;&#22914;&#26524;&#35813;&#22870;&#21169;&#20302;&#20110;&#35813;&#19979;&#30028;&#65292;&#21017;&#26080;&#27861;&#33719;&#24471;&#27492;&#20445;&#35777;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#23545;&#22810;&#20010;&#30446;&#26631;&#36827;&#34892;&#20248;&#20808;&#25490;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13419v1 Announce Type: new  Abstract: Recent years have seen an emerging interest in the trustworthiness of machine learning-based agents in the wild, especially in robotics, to provide safety assurance for the industry. Obtaining behavioral guarantees for these agents remains an important problem. In this work, we focus on guaranteeing a model-based planning agent reaches a goal state within a specific future time step. We show that there exists a lower bound for the reward at the goal state, such that if the said reward is below that bound, it is impossible to obtain such a guarantee. By extension, we show how to enforce preferences over multiple goals.
&lt;/p&gt;</description></item><item><title>&#26032;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#25193;&#23637;&#29289;&#29702;&#20449;&#24687;&#30340;&#30828;&#32422;&#26463;&#65292;&#25552;&#39640;&#20102;&#23545;&#22797;&#26434;&#21160;&#21147;&#31995;&#32479;&#30340;&#24314;&#27169;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.13412</link><description>&lt;p&gt;
&#20351;&#29992;&#19987;&#23478;&#28151;&#21512;&#27169;&#22411;&#25193;&#23637;&#29289;&#29702;&#20449;&#24687;&#30340;&#30828;&#32422;&#26463;
&lt;/p&gt;
&lt;p&gt;
Scaling physics-informed hard constraints with mixture-of-experts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13412
&lt;/p&gt;
&lt;p&gt;
&#26032;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#25193;&#23637;&#29289;&#29702;&#20449;&#24687;&#30340;&#30828;&#32422;&#26463;&#65292;&#25552;&#39640;&#20102;&#23545;&#22797;&#26434;&#21160;&#21147;&#31995;&#32479;&#30340;&#24314;&#27169;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#20013;&#24378;&#21152;&#24050;&#30693;&#30340;&#29289;&#29702;&#32422;&#26463;&#65292;&#27604;&#22914;&#23432;&#24658;&#23450;&#24459;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#24402;&#32435;&#20559;&#24046;&#65292;&#21487;&#20197;&#25552;&#39640;&#27169;&#25311;&#29289;&#29702;&#21160;&#24577;&#30340;&#20934;&#30830;&#24615;&#12289;&#21487;&#38752;&#24615;&#12289;&#25910;&#25947;&#24615;&#21644;&#25968;&#25454;&#25928;&#29575;&#12290;&#34429;&#28982;&#36825;&#20123;&#32422;&#26463;&#21487;&#20197;&#36890;&#36807;&#25439;&#22833;&#20989;&#25968;&#24809;&#32602;&#36719;&#24615;&#22320;&#24378;&#21152;&#65292;&#20294;&#26368;&#36817;&#19981;&#21516;iable&#29289;&#29702;&#21644;&#20248;&#21270;&#30340;&#36827;&#23637;&#36890;&#36807;&#23558;PDE&#32422;&#26463;&#20248;&#21270;&#20316;&#20026;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#21333;&#29420;&#23618;&#26469;&#25913;&#21892;&#24615;&#33021;&#12290;&#36825;&#20351;&#24471;&#23545;&#29289;&#29702;&#32422;&#26463;&#30340;&#36981;&#23432;&#26356;&#21152;&#20005;&#26684;&#12290;&#28982;&#32780;&#65292;&#24378;&#21152;&#30828;&#32422;&#26463;&#26174;&#33879;&#22686;&#21152;&#20102;&#35745;&#31639;&#21644;&#20869;&#23384;&#25104;&#26412;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#22797;&#26434;&#30340;&#21160;&#21147;&#31995;&#32479;&#12290;&#36825;&#26159;&#22240;&#20026;&#23427;&#38656;&#35201;&#22312;&#32593;&#26684;&#20013;&#30340;&#22823;&#37327;&#28857;&#19978;&#27714;&#35299;&#20248;&#21270;&#38382;&#39064;&#65292;&#34920;&#31034;&#31354;&#38388;&#21644;&#26102;&#38388;&#30340;&#31163;&#25955;&#21270;&#65292;&#20174;&#32780;&#26497;&#22823;&#22320;&#22686;&#21152;&#20102;&#32422;&#26463;&#30340;&#22797;&#26434;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#26041;&#27861;&#26469;&#23454;&#26045;&#30828;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13412v1 Announce Type: cross  Abstract: Imposing known physical constraints, such as conservation laws, during neural network training introduces an inductive bias that can improve accuracy, reliability, convergence, and data efficiency for modeling physical dynamics. While such constraints can be softly imposed via loss function penalties, recent advancements in differentiable physics and optimization improve performance by incorporating PDE-constrained optimization as individual layers in neural networks. This enables a stricter adherence to physical constraints. However, imposing hard constraints significantly increases computational and memory costs, especially for complex dynamical systems. This is because it requires solving an optimization problem over a large number of points in a mesh, representing spatial and temporal discretizations, which greatly increases the complexity of the constraint. To address this challenge, we develop a scalable approach to enforce hard 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#36125;&#21494;&#26031;&#35268;&#21017;&#24402;&#32435;&#65292;&#26032;&#24341;&#20837;&#30340;&#26234;&#33021;&#20307;&#21487;&#20197;&#25512;&#26029;&#29616;&#26377;&#20154;&#32676;&#30340;&#35268;&#33539;&#65292;&#20351;&#26234;&#33021;&#20307;&#25910;&#25947;&#21040;&#20849;&#20139;&#30340;&#35268;&#33539;&#65292;&#20174;&#32780;&#23454;&#29616;&#35268;&#33539;&#20307;&#31995;&#30340;&#31283;&#23450;&#24615;</title><link>https://arxiv.org/abs/2402.13399</link><description>&lt;p&gt;
&#36890;&#36807;&#36125;&#21494;&#26031;&#35268;&#21017;&#24402;&#32435;&#22312;&#39532;&#23572;&#31185;&#22827;&#21338;&#24328;&#20013;&#23398;&#20064;&#21644;&#32500;&#25345;&#20849;&#20139;&#30340;&#35268;&#33539;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Learning and Sustaining Shared Normative Systems via Bayesian Rule Induction in Markov Games
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13399
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#36125;&#21494;&#26031;&#35268;&#21017;&#24402;&#32435;&#65292;&#26032;&#24341;&#20837;&#30340;&#26234;&#33021;&#20307;&#21487;&#20197;&#25512;&#26029;&#29616;&#26377;&#20154;&#32676;&#30340;&#35268;&#33539;&#65292;&#20351;&#26234;&#33021;&#20307;&#25910;&#25947;&#21040;&#20849;&#20139;&#30340;&#35268;&#33539;&#65292;&#20174;&#32780;&#23454;&#29616;&#35268;&#33539;&#20307;&#31995;&#30340;&#31283;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#31038;&#20250;&#30340;&#19968;&#20010;&#26222;&#36941;&#29305;&#24449;&#26159;&#37319;&#29992;&#35268;&#21017;&#21644;&#35268;&#33539;&#20307;&#31995;&#26469;&#26381;&#21153;&#20110;&#21512;&#20316;&#30446;&#30340;&#12290;&#25105;&#20204;&#22914;&#20309;&#26500;&#24314;&#21487;&#20197;&#23398;&#20064;&#24182;&#36981;&#23432;&#36825;&#19968;&#20307;&#31995;&#30340;&#26234;&#33021;&#20307;&#65292;&#20197;&#20415;&#23427;&#20204;&#21487;&#20197;&#28789;&#27963;&#22320;&#19982;&#20154;&#31867;&#26426;&#26500;&#21512;&#20316;&#65311;&#25105;&#20204;&#20551;&#35774;&#65292;&#36890;&#36807;&#20551;&#23450;&#23384;&#22312;&#19968;&#20010;&#20849;&#20139;&#30340;&#35268;&#33539;&#38598;&#65292;&#22823;&#22810;&#25968;&#20854;&#20182;&#20154;&#20250;&#36981;&#23432;&#36825;&#20123;&#35268;&#33539;&#65292;&#21516;&#26102;&#36861;&#27714;&#20182;&#20204;&#20010;&#20154;&#30340;&#24895;&#26395;&#65292;&#21363;&#20351;&#20182;&#20204;&#19981;&#30693;&#36947;&#36825;&#20123;&#35268;&#33539;&#30340;&#30830;&#20999;&#20869;&#23481;&#12290;&#36890;&#36807;&#20551;&#35774;&#20849;&#20139;&#35268;&#33539;&#65292;&#26032;&#24341;&#20837;&#30340;&#26234;&#33021;&#20307;&#21487;&#20197;&#20174;&#36981;&#23432;&#21644;&#36829;&#21453;&#30340;&#35266;&#23519;&#20013;&#25512;&#26029;&#29616;&#26377;&#20154;&#32676;&#30340;&#35268;&#33539;&#12290;&#27492;&#22806;&#65292;&#21363;&#20351;&#26368;&#21021;&#22312;&#23545;&#35268;&#33539;&#30340;&#20449;&#24565;&#19978;&#23384;&#22312;&#20998;&#27495;&#65292;&#19968;&#32452;&#26234;&#33021;&#20307;&#20063;&#21487;&#20197;&#25910;&#25947;&#21040;&#20849;&#20139;&#30340;&#35268;&#33539;&#65292;&#20174;&#32780;&#23454;&#29616;&#35268;&#33539;&#20307;&#31995;&#30340;&#31283;&#23450;&#24615;&#65306;&#30001;&#20110;&#26234;&#33021;&#20307;&#21487;&#20197;&#20351;&#35268;&#33539;&#21464;&#20026;&#20849;&#35782;&#30693;&#35782;&#65292;&#36825;&#23548;&#33268;&#35268;&#33539;&#24471;&#21040;&#24191;&#27867;&#36981;&#23432;&#65292;&#20174;&#32780;&#20351;&#26032;&#30340;&#21442;&#19982;&#32773;&#24471;&#20197;&#21152;&#20837;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13399v1 Announce Type: new  Abstract: A universal feature of human societies is the adoption of systems of rules and norms in the service of cooperative ends. How can we build learning agents that do the same, so that they may flexibly cooperate with the human institutions they are embedded in? We hypothesize that agents can achieve this by assuming there exists a shared set of norms that most others comply with while pursuing their individual desires, even if they do not know the exact content of those norms. By assuming shared norms, a newly introduced agent can infer the norms of an existing population from observations of compliance and violation. Furthermore, groups of agents can converge to a shared set of norms, even if they initially diverge in their beliefs about what the norms are. This in turn enables the stability of the normative system: since agents can bootstrap common knowledge of the norms, this leads the norms to be widely adhered to, enabling new entrants 
&lt;/p&gt;</description></item><item><title>Xling&#26159;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#21033;&#29992;&#23398;&#20064;&#27169;&#22411;&#26500;&#24314;&#24230;&#37327;&#31354;&#38388;&#36807;&#28388;&#22120;&#65292;&#29992;&#20110;&#21152;&#36895;&#39640;&#32500;&#36817;&#20284;&#30456;&#20284;&#24615;&#36830;&#25509;&#65292;&#25552;&#20379;&#20248;&#21270;&#31574;&#30053;&#20197;&#25552;&#39640;&#39044;&#27979;&#36136;&#37327;</title><link>https://arxiv.org/abs/2402.13397</link><description>&lt;p&gt;
Xling: &#29992;&#20110;&#21152;&#36895;&#39640;&#32500;&#36817;&#20284;&#30456;&#20284;&#24615;&#36830;&#25509;&#30340;&#23398;&#20064;&#36807;&#28388;&#22120;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Xling: A Learned Filter Framework for Accelerating High-Dimensional Approximate Similarity Join
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13397
&lt;/p&gt;
&lt;p&gt;
Xling&#26159;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#21033;&#29992;&#23398;&#20064;&#27169;&#22411;&#26500;&#24314;&#24230;&#37327;&#31354;&#38388;&#36807;&#28388;&#22120;&#65292;&#29992;&#20110;&#21152;&#36895;&#39640;&#32500;&#36817;&#20284;&#30456;&#20284;&#24615;&#36830;&#25509;&#65292;&#25552;&#20379;&#20248;&#21270;&#31574;&#30053;&#20197;&#25552;&#39640;&#39044;&#27979;&#36136;&#37327;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30456;&#20284;&#24615;&#36830;&#25509;&#26597;&#25214;&#36317;&#31163;&#38408;&#20540;&#20869;&#30340;&#25152;&#26377;&#25509;&#36817;&#28857;&#23545;&#12290;&#35768;&#22810;&#30456;&#20284;&#24615;&#36830;&#25509;&#26041;&#27861;&#24050;&#34987;&#25552;&#20986;&#65292;&#20294;&#22312;&#39640;&#32500;&#31354;&#38388;&#20013;&#36890;&#24120;&#30001;&#20110;&#32500;&#24230;&#28798;&#38590;&#21644;&#25968;&#25454;&#19981;&#21487;&#24863;&#30693;&#32780;&#25928;&#29575;&#20302;&#19979;&#12290;&#25105;&#20204;&#35843;&#26597;&#20102;&#20351;&#29992;&#24230;&#37327;&#31354;&#38388;&#24067;&#38534;&#36807;&#28388;&#22120;&#65288;MSBF&#65289;&#30340;&#21487;&#33021;&#24615;&#65292;&#23427;&#26159;&#19968;&#32452;&#25968;&#25454;&#32467;&#26500;&#65292;&#26816;&#26597;&#26597;&#35810;&#28857;&#22312;&#22810;&#32500;&#31354;&#38388;&#20013;&#26159;&#21542;&#26377;&#37051;&#23621;&#65292;&#20197;&#21152;&#36895;&#30456;&#20284;&#24615;&#36830;&#25509;&#12290;&#28982;&#32780;&#65292;&#23558;MSBF&#24212;&#29992;&#20110;&#30456;&#20284;&#24615;&#36830;&#25509;&#26102;&#23384;&#22312;&#20960;&#20010;&#25361;&#25112;&#65292;&#21253;&#25324;&#20449;&#24687;&#20005;&#37325;&#20002;&#22833;&#65292;&#25968;&#25454;&#19981;&#21487;&#24863;&#30693;&#21644;&#23545;&#36317;&#31163;&#24230;&#37327;&#30340;&#20005;&#26684;&#32422;&#26463;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Xling&#65292;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#29992;&#20219;&#20309;&#29616;&#26377;&#22238;&#24402;&#27169;&#22411;&#26500;&#24314;&#22522;&#20110;&#23398;&#20064;&#30340;&#24230;&#37327;&#31354;&#38388;&#36807;&#28388;&#22120;&#65292;&#26088;&#22312;&#20934;&#30830;&#39044;&#27979;&#26597;&#35810;&#28857;&#26159;&#21542;&#20855;&#26377;&#36275;&#22815;&#25968;&#37327;&#30340;&#37051;&#23621;&#12290;&#35813;&#26694;&#26550;&#25552;&#20379;&#20102;&#19968;&#22871;&#20248;&#21270;&#31574;&#30053;&#26469;&#36827;&#19968;&#27493;&#25552;&#39640;&#39044;&#27979;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13397v1 Announce Type: cross  Abstract: Similarity join finds all pairs of close points within a given distance threshold. Many similarity join methods have been proposed, but they are usually not efficient on high-dimensional space due to the curse of dimensionality and data-unawareness. We investigate the possibility of using metric space Bloom filter (MSBF), a family of data structures checking if a query point has neighbors in a multi-dimensional space, to speed up similarity join. However, there are several challenges when applying MSBF to similarity join, including excessive information loss, data-unawareness and hard constraint on the distance metric. In this paper, we propose Xling, a generic framework to build a learning-based metric space filter with any existing regression model, aiming at accurately predicting whether a query point has enough number of neighbors. The framework provides a suite of optimization strategies to further improve the prediction quality b
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#21033;&#29992;&#21464;&#21387;&#22120;&#27169;&#22411;&#35299;&#20915;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#38382;&#39064;&#65292;&#39318;&#27425;&#37319;&#29992;&#21464;&#21387;&#22120;&#39044;&#27979;&#20108;&#36827;&#21046;&#21464;&#37327;&#65292;&#25552;&#20986;&#30340;&#31639;&#27861;&#22312;&#35299;&#20915;&#26102;&#38388;&#19978;&#36229;&#36234;&#20102;&#20256;&#32479;CPLEX&#21644;LSTM&#12290;</title><link>https://arxiv.org/abs/2402.13380</link><description>&lt;p&gt;
&#36808;&#21521;&#21464;&#21387;&#22120;&#65306;&#29992;&#21464;&#21387;&#22120;&#24443;&#24213;&#25913;&#21464;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#30340;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Toward TransfORmers: Revolutionizing the Solution of Mixed Integer Programs with Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13380
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#21033;&#29992;&#21464;&#21387;&#22120;&#27169;&#22411;&#35299;&#20915;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#38382;&#39064;&#65292;&#39318;&#27425;&#37319;&#29992;&#21464;&#21387;&#22120;&#39044;&#27979;&#20108;&#36827;&#21046;&#21464;&#37327;&#65292;&#25552;&#20986;&#30340;&#31639;&#27861;&#22312;&#35299;&#20915;&#26102;&#38388;&#19978;&#36229;&#36234;&#20102;&#20256;&#32479;CPLEX&#21644;LSTM&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#21033;&#29992;&#21464;&#21387;&#22120;&#27169;&#22411;&#26469;&#35299;&#20915;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#30340;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#19987;&#27880;&#20110;&#23481;&#37327;&#38480;&#21046;&#25209;&#37327;&#29983;&#20135;&#38382;&#39064;&#65288;CLSP&#65289;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#39318;&#20010;&#21033;&#29992;&#21464;&#21387;&#22120;&#26469;&#39044;&#27979;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#38382;&#39064;&#20013;&#30340;&#20108;&#36827;&#21046;&#21464;&#37327;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#21464;&#21387;&#22120;&#22788;&#29702;&#39034;&#24207;&#25968;&#25454;&#30340;&#33021;&#21147;&#65292;&#38750;&#24120;&#36866;&#21512;&#39044;&#27979;&#27599;&#20010;CLSP&#21608;&#26399;&#20013;&#34920;&#31034;&#29983;&#20135;&#35774;&#32622;&#20915;&#31574;&#30340;&#20108;&#36827;&#21046;&#21464;&#37327;&#12290;&#36825;&#20010;&#38382;&#39064;&#26412;&#36136;&#19978;&#26159;&#21160;&#24577;&#30340;&#65292;&#25105;&#20204;&#38656;&#35201;&#22312;&#32422;&#26463;&#26465;&#20214;&#19979;&#22788;&#29702;&#39034;&#24207;&#20915;&#31574;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#21464;&#21387;&#22120;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;CLSP&#35299;&#20915;&#26041;&#26696;&#12290;&#25152;&#25552;&#20986;&#30340;&#21518;&#22788;&#29702;&#21464;&#21387;&#22120;&#31639;&#27861;&#22312;&#35299;&#20915;&#26102;&#38388;&#19978;&#36229;&#36234;&#20102;&#26368;&#20808;&#36827;&#30340;&#27714;&#35299;&#22120;CPLEX&#21644;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13380v1 Announce Type: new  Abstract: In this study, we introduce an innovative deep learning framework that employs a transformer model to address the challenges of mixed-integer programs, specifically focusing on the Capacitated Lot Sizing Problem (CLSP). Our approach, to our knowledge, is the first to utilize transformers to predict the binary variables of a mixed-integer programming (MIP) problem. Specifically, our approach harnesses the encoder decoder transformer's ability to process sequential data, making it well-suited for predicting binary variables indicating production setup decisions in each period of the CLSP. This problem is inherently dynamic, and we need to handle sequential decision making under constraints. We present an efficient algorithm in which CLSP solutions are learned through a transformer neural network. The proposed post-processed transformer algorithm surpasses the state-of-the-art solver, CPLEX and Long Short-Term Memory (LSTM) in solution time
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;&#25193;&#25955;&#27169;&#22411;&#30340;&#32508;&#21512;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#20915;&#23450;&#27169;&#22411;&#24615;&#33021;&#30340;&#38544;&#34255;&#20851;&#38190;&#22240;&#32032;&#65292;&#20026;DMs&#30340;&#25512;&#36827;&#25552;&#20379;&#20102;&#35265;&#35299;&#12290;</title><link>https://arxiv.org/abs/2402.13369</link><description>&lt;p&gt;
&#24322;&#35758;&#23665;&#35895;&#65306;&#25193;&#25955;&#27169;&#22411;&#30340;&#32508;&#21512;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
The Uncanny Valley: A Comprehensive Analysis of Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13369
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;&#25193;&#25955;&#27169;&#22411;&#30340;&#32508;&#21512;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#20915;&#23450;&#27169;&#22411;&#24615;&#33021;&#30340;&#38544;&#34255;&#20851;&#38190;&#22240;&#32032;&#65292;&#20026;DMs&#30340;&#25512;&#36827;&#25552;&#20379;&#20102;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25193;&#25955;&#27169;&#22411;&#65288;DMs&#65289;&#65292;&#25105;&#20204;&#22312;&#29983;&#25104;&#39640;&#36136;&#37327;&#22270;&#20687;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#12290;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;&#36825;&#20123;&#27169;&#22411;&#30340;&#26680;&#24515;&#25805;&#20316;&#21407;&#21017;&#65292;&#31995;&#32479;&#22320;&#35843;&#26597;&#20102;&#19981;&#21516;DM&#26550;&#26500;&#20013;&#30340;&#20851;&#38190;&#26041;&#38754;&#65306;i&#65289;&#22122;&#22768;&#26102;&#38388;&#34920;&#65292;ii&#65289;&#37319;&#26679;&#22120;&#21644;iii&#65289;&#24341;&#23548;&#12290;&#25105;&#20204;&#23545;&#36825;&#20123;&#27169;&#22411;&#30340;&#20840;&#38754;&#23457;&#26597;&#25581;&#31034;&#20102;&#23427;&#20204;&#38544;&#34255;&#30340;&#22522;&#26412;&#26426;&#21046;&#65292;&#25581;&#31034;&#20102;&#23545;&#20854;&#26377;&#25928;&#24615;&#33267;&#20851;&#37325;&#35201;&#30340;&#38544;&#34255;&#22522;&#30784;&#35201;&#32032;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#24378;&#35843;&#20102;&#20915;&#23450;&#27169;&#22411;&#24615;&#33021;&#30340;&#38544;&#34255;&#20851;&#38190;&#22240;&#32032;&#65292;&#25552;&#20379;&#20102;&#26377;&#21161;&#20110;&#25512;&#21160;DMs&#21457;&#23637;&#30340;&#35265;&#35299;&#12290;&#36807;&#21435;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#22122;&#22768;&#26102;&#38388;&#34920;&#12289;&#37319;&#26679;&#22120;&#21644;&#24341;&#23548;&#30340;&#37197;&#32622;&#23545;&#29983;&#25104;&#22270;&#20687;&#30340;&#36136;&#37327;&#33267;&#20851;&#37325;&#35201;&#65307;&#28982;&#32780;&#65292;&#27169;&#22411;&#22312;&#19981;&#21516;&#37197;&#32622;&#19979;&#22312;&#19968;&#20010;&#38750;&#24120;&#30456;&#20284;&#30340;&#31283;&#23450;&#36136;&#37327;&#27700;&#24179;&#19978;&#36798;&#21040;&#65292;&#25581;&#31034;&#20102;&#20915;&#23450;&#26368;&#20339;&#24615;&#33021;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13369v1 Announce Type: cross  Abstract: Through Diffusion Models (DMs), we have made significant advances in generating high-quality images. Our exploration of these models delves deeply into their core operational principles by systematically investigating key aspects across various DM architectures: i) noise schedules, ii) samplers, and iii) guidance. Our comprehensive examination of these models sheds light on their hidden fundamental mechanisms, revealing the concealed foundational elements that are essential for their effectiveness. Our analyses emphasize the hidden key factors that determine model performance, offering insights that contribute to the advancement of DMs. Past findings show that the configuration of noise schedules, samplers, and guidance is vital to the quality of generated images; however, models reach a stable level of quality across different configurations at a remarkably similar point, revealing that the decisive factors for optimal performance pre
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#21033;&#29992;Transformer&#26426;&#22120;&#23398;&#20064;&#26550;&#26500;&#29983;&#25104;&#8220;&#30475;&#36215;&#26469;&#30495;&#23454;&#8221;&#30340;&#37327;&#23376;&#30005;&#36335;&#65292;&#20197;&#22686;&#24378;&#29616;&#26377;&#30340;&#37327;&#23376;&#30005;&#36335;&#25968;&#25454;&#38598;&#12290;</title><link>https://arxiv.org/abs/2402.13352</link><description>&lt;p&gt;
KetGPT -- &#20351;&#29992;Transformer&#23545;&#37327;&#23376;&#30005;&#36335;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
KetGPT -- Dataset Augmentation of Quantum Circuits using Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13352
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#21033;&#29992;Transformer&#26426;&#22120;&#23398;&#20064;&#26550;&#26500;&#29983;&#25104;&#8220;&#30475;&#36215;&#26469;&#30495;&#23454;&#8221;&#30340;&#37327;&#23376;&#30005;&#36335;&#65292;&#20197;&#22686;&#24378;&#29616;&#26377;&#30340;&#37327;&#23376;&#30005;&#36335;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#31639;&#27861;&#65292;&#34920;&#31034;&#20026;&#37327;&#23376;&#30005;&#36335;&#65292;&#21487;&#29992;&#20316;&#35780;&#20272;&#37327;&#23376;&#31995;&#32479;&#24615;&#33021;&#30340;&#22522;&#20934;&#12290;&#29616;&#26377;&#25968;&#25454;&#38598;&#22312;&#35268;&#27169;&#21644;&#22810;&#26679;&#24615;&#26041;&#38754;&#23384;&#22312;&#38480;&#21046;&#65292;&#22312;&#35813;&#39046;&#22495;&#24191;&#27867;&#20351;&#29992;&#65292;&#23548;&#33268;&#30740;&#31350;&#20154;&#21592;&#20351;&#29992;&#38543;&#26426;&#29983;&#25104;&#30340;&#30005;&#36335;&#12290;&#28982;&#32780;&#65292;&#38543;&#26426;&#30005;&#36335;&#24182;&#19981;&#26159;&#20195;&#34920;&#24615;&#22522;&#20934;&#65292;&#22240;&#20026;&#23427;&#20204;&#32570;&#20047;&#37327;&#23376;&#31995;&#32479;&#21046;&#36896;&#30340;&#30495;&#23454;&#37327;&#23376;&#31639;&#27861;&#30340;&#22266;&#26377;&#23646;&#24615;&#12290;&#36825;&#31181;&#32570;&#20047;&#8220;&#26377;&#29992;&#8221;&#30340;&#37327;&#23376;&#22522;&#20934;&#26500;&#25104;&#20102;&#25512;&#21160;&#37327;&#23376;&#32534;&#35793;&#22120;&#21644;&#30828;&#20214;&#24320;&#21457;&#19982;&#27604;&#36739;&#30340;&#25361;&#25112;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;Transformer&#26426;&#22120;&#23398;&#20064;&#26550;&#26500;&#29983;&#25104;&#25105;&#20204;&#31216;&#20043;&#20026;&#8220;&#30475;&#36215;&#26469;&#30495;&#23454;&#8221;&#30340;&#30005;&#36335;&#65292;&#20197;&#22686;&#24378;&#29616;&#26377;&#30340;&#37327;&#23376;&#30005;&#36335;&#25968;&#25454;&#38598;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;KetGPT&#65292;&#19968;&#31181;&#20197;OpenQASM&#35821;&#35328;&#29983;&#25104;&#21512;&#25104;&#30005;&#36335;&#30340;&#24037;&#20855;&#65292;&#20854;&#32467;&#26500;&#26159;&#22522;&#20110;&#25512;&#23548;&#33258;&#37327;&#23376;&#30005;&#36335;&#30340;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13352v1 Announce Type: cross  Abstract: Quantum algorithms, represented as quantum circuits, can be used as benchmarks for assessing the performance of quantum systems. Existing datasets, widely utilized in the field, suffer from limitations in size and versatility, leading researchers to employ randomly generated circuits. Random circuits are, however, not representative benchmarks as they lack the inherent properties of real quantum algorithms for which the quantum systems are manufactured. This shortage of `useful' quantum benchmarks poses a challenge to advancing the development and comparison of quantum compilers and hardware.   This research aims to enhance the existing quantum circuit datasets by generating what we refer to as `realistic-looking' circuits by employing the Transformer machine learning architecture. For this purpose, we introduce KetGPT, a tool that generates synthetic circuits in OpenQASM language, whose structure is based on quantum circuits derived f
&lt;/p&gt;</description></item><item><title>AEA&#25968;&#25454;&#38598;&#26159;&#20351;&#29992;Project Aria&#30524;&#38236;&#35760;&#24405;&#30340;&#31532;&#19968;&#20154;&#31216;&#22810;&#27169;&#24577;&#24320;&#25918;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#22810;&#20010;&#20329;&#25140;&#32773;&#22312;&#23460;&#20869;&#19981;&#21516;&#20301;&#32622;&#35760;&#24405;&#30340;&#26085;&#24120;&#27963;&#21160;&#24207;&#21015;&#65292;&#20026;&#30740;&#31350;&#25552;&#20379;&#20102;3D&#36712;&#36857;&#12289;&#22330;&#26223;&#28857;&#20113;&#12289;&#30524;&#29699;&#27880;&#35270;&#21521;&#37327;&#21644;&#35821;&#38899;&#36716;&#24405;&#31561;&#26426;&#22120;&#24863;&#30693;&#25968;&#25454;&#65292;&#25903;&#25345;&#31070;&#32463;&#22330;&#26223;&#37325;&#24314;&#21644;&#25552;&#31034;&#20998;&#21106;&#12290;</title><link>https://arxiv.org/abs/2402.13349</link><description>&lt;p&gt;
Aria Everyday Activities &#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Aria Everyday Activities Dataset
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13349
&lt;/p&gt;
&lt;p&gt;
AEA&#25968;&#25454;&#38598;&#26159;&#20351;&#29992;Project Aria&#30524;&#38236;&#35760;&#24405;&#30340;&#31532;&#19968;&#20154;&#31216;&#22810;&#27169;&#24577;&#24320;&#25918;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#22810;&#20010;&#20329;&#25140;&#32773;&#22312;&#23460;&#20869;&#19981;&#21516;&#20301;&#32622;&#35760;&#24405;&#30340;&#26085;&#24120;&#27963;&#21160;&#24207;&#21015;&#65292;&#20026;&#30740;&#31350;&#25552;&#20379;&#20102;3D&#36712;&#36857;&#12289;&#22330;&#26223;&#28857;&#20113;&#12289;&#30524;&#29699;&#27880;&#35270;&#21521;&#37327;&#21644;&#35821;&#38899;&#36716;&#24405;&#31561;&#26426;&#22120;&#24863;&#30693;&#25968;&#25454;&#65292;&#25903;&#25345;&#31070;&#32463;&#22330;&#26223;&#37325;&#24314;&#21644;&#25552;&#31034;&#20998;&#21106;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;Aria Everyday Activities (AEA)&#25968;&#25454;&#38598;&#65292;&#36825;&#26159;&#19968;&#20010;&#20351;&#29992;Project Aria&#30524;&#38236;&#35760;&#24405;&#30340;&#31532;&#19968;&#20154;&#31216;&#22810;&#27169;&#24577;&#24320;&#25918;&#25968;&#25454;&#38598;&#12290;AEA&#21253;&#21547;&#20102;&#30001;&#22810;&#21517;&#20329;&#25140;&#32773;&#22312;&#20116;&#20010;&#22320;&#29702;&#19978;&#22810;&#26679;&#30340;&#23460;&#20869;&#20301;&#32622;&#35760;&#24405;&#30340;143&#20010;&#26085;&#24120;&#27963;&#21160;&#24207;&#21015;&#12290;&#27599;&#20010;&#35760;&#24405;&#37117;&#21253;&#21547;&#36890;&#36807;Project Aria&#30524;&#38236;&#35760;&#24405;&#30340;&#22810;&#27169;&#24577;&#20256;&#24863;&#22120;&#25968;&#25454;&#12290;&#27492;&#22806;&#65292;AEA&#36824;&#25552;&#20379;&#20102;&#26426;&#22120;&#24863;&#30693;&#25968;&#25454;&#65292;&#21253;&#25324;&#39640;&#39057;&#20840;&#23616;&#23545;&#40784;&#30340;3D&#36712;&#36857;&#65292;&#22330;&#26223;&#28857;&#20113;&#65292;&#36880;&#24103;3D&#30524;&#29699;&#27880;&#35270;&#21521;&#37327;&#21644;&#26102;&#38388;&#23545;&#40784;&#30340;&#35821;&#38899;&#36716;&#24405;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#36825;&#19968;&#25968;&#25454;&#38598;&#23454;&#29616;&#30340;&#19968;&#20123;&#31034;&#20363;&#30740;&#31350;&#24212;&#29992;&#65292;&#21253;&#25324;&#31070;&#32463;&#22330;&#26223;&#37325;&#24314;&#21644;&#25552;&#31034;&#20998;&#21106;&#12290;AEA&#26159;&#19968;&#20010;&#21487;&#20197;&#20174;projectaria.com&#19979;&#36733;&#30340;&#24320;&#28304;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#22914;&#20309;&#22312;Project Aria Tools&#20013;&#20351;&#29992;&#25968;&#25454;&#38598;&#30340;&#24320;&#28304;&#23454;&#29616;&#21644;&#31034;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13349v1 Announce Type: cross  Abstract: We present Aria Everyday Activities (AEA) Dataset, an egocentric multimodal open dataset recorded using Project Aria glasses. AEA contains 143 daily activity sequences recorded by multiple wearers in five geographically diverse indoor locations. Each of the recording contains multimodal sensor data recorded through the Project Aria glasses. In addition, AEA provides machine perception data including high frequency globally aligned 3D trajectories, scene point cloud, per-frame 3D eye gaze vector and time aligned speech transcription. In this paper, we demonstrate a few exemplar research applications enabled by this dataset, including neural scene reconstruction and prompted segmentation. AEA is an open source dataset that can be downloaded from projectaria.com. We are also providing open-source implementations and examples of how to use the dataset in Project Aria Tools.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#24066;&#22330;&#24433;&#21709;&#21160;&#24577;&#22871;&#26399;&#20445;&#20540;&#27169;&#22411;&#65292;&#32771;&#34385;&#20102;&#20984;&#24066;&#22330;&#24433;&#21709;&#21644;&#38543;&#26102;&#38388;&#25345;&#32493;&#30340;&#24433;&#21709;&#65292;&#36890;&#36807;&#19982;&#24120;&#29992;&#31243;&#24207;&#27604;&#36739;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#26399;&#26435;&#22871;&#26399;&#20445;&#20540;&#26041;&#38754;&#30340;&#20248;&#36234;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.13326</link><description>&lt;p&gt;
&#20855;&#26377;&#24066;&#22330;&#24433;&#21709;&#21147;&#30340;&#28145;&#24230;&#22871;&#26399;&#20445;&#20540;
&lt;/p&gt;
&lt;p&gt;
Deep Hedging with Market Impact
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13326
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#24066;&#22330;&#24433;&#21709;&#21160;&#24577;&#22871;&#26399;&#20445;&#20540;&#27169;&#22411;&#65292;&#32771;&#34385;&#20102;&#20984;&#24066;&#22330;&#24433;&#21709;&#21644;&#38543;&#26102;&#38388;&#25345;&#32493;&#30340;&#24433;&#21709;&#65292;&#36890;&#36807;&#19982;&#24120;&#29992;&#31243;&#24207;&#27604;&#36739;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#26399;&#26435;&#22871;&#26399;&#20445;&#20540;&#26041;&#38754;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#24577;&#22871;&#26399;&#20445;&#20540;&#26159;&#23450;&#26399;&#36827;&#34892;&#37329;&#34701;&#24037;&#20855;&#20132;&#26131;&#65292;&#20197;&#25269;&#28040;&#25237;&#36164;&#25110;&#36127;&#20538;&#25152;&#24102;&#26469;&#39118;&#38505;&#30340;&#23454;&#36341;&#12290;&#21160;&#24577;&#22871;&#26399;&#20445;&#20540;&#20248;&#21270;&#21487;&#20197;&#34987;&#35270;&#20026;&#19968;&#20010;&#39034;&#24207;&#20915;&#31574;&#38382;&#39064;&#65307;&#22240;&#27492;&#65292;&#26368;&#36817;&#25552;&#20986;&#20102;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#27169;&#22411;&#26469;&#35299;&#20915;&#36825;&#19968;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22871;&#26399;&#20445;&#20540;RL&#24037;&#20316;&#24182;&#26410;&#32771;&#34385;&#30001;&#20132;&#26131;&#24037;&#20855;&#30340;&#26377;&#38480;&#27969;&#21160;&#24615;&#24341;&#36215;&#30340;&#24066;&#22330;&#24433;&#21709;&#12290;&#25972;&#21512;&#36825;&#26679;&#30340;&#29305;&#24449;&#23545;&#20110;&#22312;&#20855;&#26377;&#26377;&#38480;&#27969;&#21160;&#24615;&#30340;&#32929;&#31080;&#26399;&#26435;&#22871;&#26399;&#20445;&#20540;&#26102;&#23454;&#29616;&#26368;&#20339;&#24615;&#33021;&#21487;&#33021;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#30340;&#26032;&#22411;&#36890;&#29992;&#24066;&#22330;&#24433;&#21709;&#21160;&#24577;&#22871;&#26399;&#20445;&#20540;&#27169;&#22411;&#65292;&#32771;&#34385;&#20102;&#20960;&#20010;&#36924;&#30495;&#30340;&#29305;&#24449;&#65292;&#20363;&#22914;&#20984;&#24066;&#22330;&#24433;&#21709;&#21644;&#38543;&#26102;&#38388;&#25345;&#32493;&#30340;&#24433;&#21709;&#12290;&#20174;DRL&#27169;&#22411;&#24471;&#21040;&#30340;&#26368;&#20248;&#31574;&#30053;&#36890;&#36807;&#20960;&#20010;&#26399;&#26435;&#22871;&#26399;&#20445;&#20540;&#27169;&#25311;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#24182;&#19982;&#24120;&#29992;&#31243;&#24207;&#65288;&#22914;&#24503;&#23572;&#22612;&#22871;&#26399;&#20445;&#20540;&#65289;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13326v1 Announce Type: cross  Abstract: Dynamic hedging is the practice of periodically transacting financial instruments to offset the risk caused by an investment or a liability. Dynamic hedging optimization can be framed as a sequential decision problem; thus, Reinforcement Learning (RL) models were recently proposed to tackle this task. However, existing RL works for hedging do not consider market impact caused by the finite liquidity of traded instruments. Integrating such feature can be crucial to achieve optimal performance when hedging options on stocks with limited liquidity. In this paper, we propose a novel general market impact dynamic hedging model based on Deep Reinforcement Learning (DRL) that considers several realistic features such as convex market impacts, and impact persistence through time. The optimal policy obtained from the DRL model is analysed using several option hedging simulations and compared to commonly used procedures such as delta hedging. Re
&lt;/p&gt;</description></item><item><title>&#27969;&#24335;&#23398;&#20064;&#26159;&#26377;&#26395;&#35299;&#20915;&#26102;&#38388;&#24207;&#21015;&#38382;&#39064;&#20013;&#27010;&#24565;&#28418;&#31227;&#30340;&#26368;&#26377;&#24076;&#26395;&#26041;&#27861;&#20043;&#19968;&#65292;&#26412;&#30740;&#31350;&#23545;&#20854;&#22312;&#39044;&#27979;&#26377;&#23475;&#34299;&#21326;&#32454;&#32990;&#25439;&#23475;&#26041;&#38754;&#30340;&#25928;&#21147;&#36827;&#34892;&#20102;&#27979;&#35797;&#24182;&#19982;&#25209;&#22788;&#29702;&#23398;&#20064;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;</title><link>https://arxiv.org/abs/2402.13304</link><description>&lt;p&gt;
&#26377;&#23475;&#34299;&#21326;&#32454;&#32990;&#25439;&#23475;(DSP)&#30340;&#39044;&#27979;&#65306;&#27969;&#24335;&#23398;&#20064;&#21644;&#25209;&#22788;&#29702;&#23398;&#20064;&#30340;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
Harmful algal bloom forecasting. A comparison between stream and batch learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13304
&lt;/p&gt;
&lt;p&gt;
&#27969;&#24335;&#23398;&#20064;&#26159;&#26377;&#26395;&#35299;&#20915;&#26102;&#38388;&#24207;&#21015;&#38382;&#39064;&#20013;&#27010;&#24565;&#28418;&#31227;&#30340;&#26368;&#26377;&#24076;&#26395;&#26041;&#27861;&#20043;&#19968;&#65292;&#26412;&#30740;&#31350;&#23545;&#20854;&#22312;&#39044;&#27979;&#26377;&#23475;&#34299;&#21326;&#32454;&#32990;&#25439;&#23475;&#26041;&#38754;&#30340;&#25928;&#21147;&#36827;&#34892;&#20102;&#27979;&#35797;&#24182;&#19982;&#25209;&#22788;&#29702;&#23398;&#20064;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#23475;&#34299;&#21326;&#32454;&#32990;&#25439;&#23475;(DSP)&#26159;&#19968;&#31181;&#20840;&#29699;&#20581;&#24247;&#23041;&#32961;&#65292;&#28304;&#20110;&#36125;&#31867;&#21463;&#21040;&#30002;&#34299;&#20135;&#29983;&#30340;&#27602;&#32032;&#27745;&#26579;&#12290;&#36825;&#31181;&#30142;&#30149;&#30001;&#20110;&#26222;&#36941;&#24615;&#21457;&#29983;&#12289;&#39640;&#33268;&#30149;&#29575;&#21644;&#36125;&#31867;&#25345;&#32493;&#30340;&#27602;&#24615;&#65292;&#23545;&#20844;&#20849;&#21355;&#29983;&#21644;&#36125;&#31867;&#20135;&#19994;&#26500;&#25104;&#21361;&#38505;&#12290;&#27602;&#32032;&#20135;&#29983;&#34299;&#31867;&#29983;&#29289;&#37327;&#39640;&#30340;&#24773;&#20917;&#65292;&#22914;DSP&#65292;&#34987;&#31216;&#20026;&#26377;&#23475;&#34299;&#21326;&#32454;&#32990;&#25439;&#23475;(HABs)&#12290;&#30417;&#27979;&#21644;&#39044;&#27979;&#31995;&#32479;&#23545;&#20110;&#20943;&#36731;HABs&#24433;&#21709;&#33267;&#20851;&#37325;&#35201;&#12290;&#39044;&#27979;&#26377;&#23475;&#34299;&#21326;&#32454;&#32990;&#25439;&#23475;&#28041;&#21450;&#19968;&#20010;&#20197;&#26102;&#38388;&#24207;&#21015;&#20026;&#22522;&#30784;&#30340;&#38382;&#39064;&#65292;&#20854;&#20013;&#26377;&#19968;&#20010;&#24378;&#28872;&#30340;&#21382;&#21490;&#23395;&#33410;&#24615;&#32452;&#25104;&#37096;&#20998;&#65292;&#28982;&#32780;&#65292;&#36817;&#26399;&#30001;&#20110;&#27668;&#35937;&#21644;&#28023;&#27915;&#20107;&#20214;&#21464;&#21270;&#32780;&#24341;&#36215;&#30340;&#24322;&#24120;&#29616;&#35937;&#24050;&#34987;&#35266;&#23519;&#21040;&#12290;&#27969;&#24335;&#23398;&#20064;&#26159;&#35299;&#20915;&#20855;&#26377;&#27010;&#24565;&#28418;&#31227;&#30340;&#26102;&#38388;&#24207;&#21015;&#38382;&#39064;&#30340;&#26368;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#20854;&#22312;&#39044;&#27979;HABs&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#23578;&#26410;&#24471;&#21040;&#35777;&#26126;&#65292;&#24182;&#19988;&#38656;&#35201;&#19982;&#25209;&#22788;&#29702;&#23398;&#20064;&#36827;&#34892;&#27604;&#36739;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13304v1 Announce Type: cross  Abstract: Diarrhetic Shellfish Poisoning (DSP) is a global health threat arising from shellfish contaminated with toxins produced by dinoflagellates. The condition, with its widespread incidence, high morbidity rate, and persistent shellfish toxicity, poses risks to public health and the shellfish industry. High biomass of toxin-producing algae such as DSP are known as Harmful Algal Blooms (HABs). Monitoring and forecasting systems are crucial for mitigating HABs impact. Predicting harmful algal blooms involves a time-series-based problem with a strong historical seasonal component, however, recent anomalies due to changes in meteorological and oceanographic events have been observed. Stream Learning stands out as one of the most promising approaches for addressing time-series-based problems with concept drifts. However, its efficacy in predicting HABs remains unproven and needs to be tested in comparison with Batch Learning. Historical data ava
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#38899;&#20048;&#29983;&#25104;&#30340;&#32467;&#26500;&#24863;&#30693;&#20301;&#32622;&#32534;&#30721;&#26694;&#26550;&#65292;&#36890;&#36807;&#32477;&#23545;&#12289;&#30456;&#23545;&#21644;&#38750;&#24179;&#31283;&#20301;&#32622;&#20449;&#24687;&#65292;&#26174;&#33879;&#25913;&#21892;&#20102;&#29983;&#25104;&#20316;&#21697;&#30340;&#26059;&#24459;&#21644;&#32467;&#26500;&#19968;&#33268;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.13301</link><description>&lt;p&gt;
&#38024;&#23545;&#38899;&#20048;&#29983;&#25104;&#30340;&#32467;&#26500;&#24863;&#30693;&#20301;&#32622;&#32534;&#30721;
&lt;/p&gt;
&lt;p&gt;
Structure-informed Positional Encoding for Music Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13301
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#38899;&#20048;&#29983;&#25104;&#30340;&#32467;&#26500;&#24863;&#30693;&#20301;&#32622;&#32534;&#30721;&#26694;&#26550;&#65292;&#36890;&#36807;&#32477;&#23545;&#12289;&#30456;&#23545;&#21644;&#38750;&#24179;&#31283;&#20301;&#32622;&#20449;&#24687;&#65292;&#26174;&#33879;&#25913;&#21892;&#20102;&#29983;&#25104;&#20316;&#21697;&#30340;&#26059;&#24459;&#21644;&#32467;&#26500;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#29983;&#25104;&#30340;&#38899;&#20048;&#24448;&#24448;&#32570;&#20047;&#36830;&#36143;&#24615;&#21644;&#38271;&#26399;&#32452;&#32455;&#65292;&#32780;&#22810;&#23610;&#24230;&#23618;&#27425;&#32467;&#26500;&#26159;&#38899;&#20048;&#20449;&#21495;&#30340;&#26174;&#33879;&#29305;&#24449;&#12290;&#20026;&#20102;&#21033;&#29992;&#36825;&#19968;&#20449;&#24687;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;Transformer&#38899;&#20048;&#29983;&#25104;&#30340;&#32467;&#26500;&#24863;&#30693;&#20301;&#32622;&#32534;&#30721;&#26694;&#26550;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19977;&#31181;&#21464;&#20307;&#65292;&#28041;&#21450;&#32477;&#23545;&#12289;&#30456;&#23545;&#21644;&#38750;&#24179;&#31283;&#20301;&#32622;&#20449;&#24687;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#31526;&#21495;&#38899;&#20048;&#29983;&#25104;&#20219;&#21153;&#19978;&#23545;&#23427;&#20204;&#36827;&#34892;&#20102;&#20840;&#38754;&#27979;&#35797;&#65306;&#19979;&#19968;&#20010;&#26102;&#38388;&#27493;&#39044;&#27979;&#21644;&#20276;&#22863;&#29983;&#25104;&#12290;&#20316;&#20026;&#23545;&#27604;&#65292;&#25105;&#20204;&#36873;&#25321;&#20102;&#25991;&#29486;&#20013;&#30340;&#22810;&#20010;&#22522;&#32447;&#65292;&#20351;&#29992;&#20960;&#20010;&#20197;&#38899;&#20048;&#20026;&#21160;&#26426;&#30340;&#35780;&#20272;&#25351;&#26631;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#20248;&#28857;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#25913;&#21892;&#20102;&#29983;&#25104;&#20316;&#21697;&#30340;&#26059;&#24459;&#21644;&#32467;&#26500;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13301v1 Announce Type: cross  Abstract: Music generated by deep learning methods often suffers from a lack of coherence and long-term organization. Yet, multi-scale hierarchical structure is a distinctive feature of music signals. To leverage this information, we propose a structure-informed positional encoding framework for music generation with Transformers. We design three variants in terms of absolute, relative and non-stationary positional information. We comprehensively test them on two symbolic music generation tasks: next-timestep prediction and accompaniment generation. As a comparison, we choose multiple baselines from the literature and demonstrate the merits of our methods using several musically-motivated evaluation metrics. In particular, our methods improve the melodic and structural consistency of the generated pieces.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;/&#21512;&#25104;&#29983;&#29289;&#23398;&#21327;&#21516;&#35774;&#35745;&#30340;&#23569;&#26679;&#26412;&#35757;&#32451;&#24037;&#20316;&#27969;&#31243;&#65292;&#29992;&#20110;N&#31471;&#32534;&#30721;&#24207;&#21015;&#65288;NCS&#65289;&#20248;&#21270;&#65292;&#36890;&#36807;&#21033;&#29992;k&#26368;&#36817;&#32534;&#30721;&#21644;word2vec&#23545;NCS&#36827;&#34892;&#32534;&#30721;&#65292;&#24182;&#21033;&#29992;&#27880;&#24847;&#26426;&#21046;&#36827;&#34892;&#29305;&#24449;&#25552;&#21462;&#65292;&#26500;&#24314;&#26102;&#38388;&#24207;&#21015;&#32593;&#32476;&#39044;&#27979;&#22522;&#22240;&#34920;&#36798;&#24378;&#24230;&#65292;&#26368;&#32456;&#36890;&#36807;&#30452;&#25509;&#25628;&#32034;&#31639;&#27861;&#30830;&#23450;&#20855;&#26377;&#26377;&#38480;&#35757;&#32451;&#25968;&#25454;&#30340;&#26368;&#20339;NCS&#12290;</title><link>https://arxiv.org/abs/2402.13297</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#19982;&#21512;&#25104;&#29983;&#29289;&#23398;&#30340;&#38598;&#25104;: &#36890;&#36807;N&#31471;&#32534;&#30721;&#24207;&#21015;&#22686;&#24378;&#22522;&#22240;&#34920;&#36798;&#30340;&#21327;&#21516;&#35774;&#35745;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Integrating Deep Learning and Synthetic Biology: A Co-Design Approach for Enhancing Gene Expression via N-terminal Coding Sequences
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13297
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;/&#21512;&#25104;&#29983;&#29289;&#23398;&#21327;&#21516;&#35774;&#35745;&#30340;&#23569;&#26679;&#26412;&#35757;&#32451;&#24037;&#20316;&#27969;&#31243;&#65292;&#29992;&#20110;N&#31471;&#32534;&#30721;&#24207;&#21015;&#65288;NCS&#65289;&#20248;&#21270;&#65292;&#36890;&#36807;&#21033;&#29992;k&#26368;&#36817;&#32534;&#30721;&#21644;word2vec&#23545;NCS&#36827;&#34892;&#32534;&#30721;&#65292;&#24182;&#21033;&#29992;&#27880;&#24847;&#26426;&#21046;&#36827;&#34892;&#29305;&#24449;&#25552;&#21462;&#65292;&#26500;&#24314;&#26102;&#38388;&#24207;&#21015;&#32593;&#32476;&#39044;&#27979;&#22522;&#22240;&#34920;&#36798;&#24378;&#24230;&#65292;&#26368;&#32456;&#36890;&#36807;&#30452;&#25509;&#25628;&#32034;&#31639;&#27861;&#30830;&#23450;&#20855;&#26377;&#26377;&#38480;&#35757;&#32451;&#25968;&#25454;&#30340;&#26368;&#20339;NCS&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
N&#31471;&#32534;&#30721;&#24207;&#21015;&#65288;NCS&#65289;&#36890;&#36807;&#24433;&#21709;&#32763;&#35793;&#36215;&#22987;&#36895;&#29575;&#32780;&#24433;&#21709;&#22522;&#22240;&#34920;&#36798;&#12290; NCS&#20248;&#21270;&#38382;&#39064;&#26159;&#25214;&#21040;&#26368;&#22823;&#21270;&#22522;&#22240;&#34920;&#36798;&#30340;NCS&#12290; &#35813;&#38382;&#39064;&#22312;&#36951;&#20256;&#24037;&#31243;&#20013;&#38750;&#24120;&#37325;&#35201;&#12290; &#20294;&#26159;&#65292;&#30446;&#21069;&#29992;&#20110;NCS&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;&#22914;&#26377;&#29702;&#35774;&#35745;&#21644;&#32479;&#35745;&#24341;&#23548;&#26041;&#27861;&#65292;&#21171;&#21160;&#23494;&#38598;&#19988;&#20165;&#20135;&#29983;&#30456;&#23545;&#36739;&#23567;&#30340;&#25913;&#36827;&#12290; &#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;NCS&#20248;&#21270;&#30340;&#28145;&#24230;&#23398;&#20064;/&#21512;&#25104;&#29983;&#29289;&#23398;&#21327;&#21516;&#35774;&#35745;&#23569;&#26679;&#26412;&#35757;&#32451;&#24037;&#20316;&#27969;&#31243;&#12290; &#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;k&#26368;&#36817;&#32534;&#30721;&#65292;&#28982;&#21518;&#21033;&#29992;word2vec&#23545;NCS&#36827;&#34892;&#32534;&#30721;&#65292;&#28982;&#21518;&#20351;&#29992;&#27880;&#24847;&#26426;&#21046;&#36827;&#34892;&#29305;&#24449;&#25552;&#21462;&#65292;&#28982;&#21518;&#26500;&#24314;&#26102;&#38388;&#24207;&#21015;&#32593;&#32476;&#20197;&#39044;&#27979;&#22522;&#22240;&#34920;&#36798;&#24378;&#24230;&#65292;&#26368;&#21518;&#36890;&#36807;&#30452;&#25509;&#25628;&#32034;&#31639;&#27861;&#30830;&#23450;&#20855;&#26377;&#26377;&#38480;&#35757;&#32451;&#25968;&#25454;&#30340;&#26368;&#20339;NCS&#12290; &#25105;&#20204;&#20197;&#26543;&#33639;&#20809;&#34507;&#30333;&#65288;GFP&#65289;&#30001;&#26543;&#33639;&#26438;&#33740;&#34920;&#36798;&#20316;&#20026;NCS&#30340;&#25253;&#21578;&#34507;&#30333;&#65292;&#24182;&#37319;&#29992;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13297v1 Announce Type: cross  Abstract: N-terminal coding sequence (NCS) influences gene expression by impacting the translation initiation rate. The NCS optimization problem is to find an NCS that maximizes gene expression. The problem is important in genetic engineering. However, current methods for NCS optimization such as rational design and statistics-guided approaches are labor-intensive yield only relatively small improvements. This paper introduces a deep learning/synthetic biology co-designed few-shot training workflow for NCS optimization. Our method utilizes k-nearest encoding followed by word2vec to encode the NCS, then performs feature extraction using attention mechanisms, before constructing a time-series network for predicting gene expression intensity, and finally a direct search algorithm identifies the optimal NCS with limited training data. We took green fluorescent protein (GFP) expressed by Bacillus subtilis as a reporting protein of NCSs, and employed 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#38024;&#23545;&#22810;&#26426;&#22120;&#20154;&#31995;&#32479;&#30340;&#20914;&#31361;&#24863;&#30693;&#26368;&#20248;&#30446;&#26631;&#20998;&#37197;&#31639;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#20914;&#31361;&#24341;&#23548;&#26041;&#27861;&#21644;&#20854;&#20182;&#20248;&#21270;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.13292</link><description>&lt;p&gt;
&#19968;&#20010;&#38024;&#23545;&#22810;&#26426;&#22120;&#20154;&#31995;&#32479;&#30340;&#20914;&#31361;&#24863;&#30693;&#26368;&#20248;&#30446;&#26631;&#20998;&#37197;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Conflict-Aware Optimal Goal Assignment Algorithm for Multi-Robot Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13292
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#38024;&#23545;&#22810;&#26426;&#22120;&#20154;&#31995;&#32479;&#30340;&#20914;&#31361;&#24863;&#30693;&#26368;&#20248;&#30446;&#26631;&#20998;&#37197;&#31639;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#20914;&#31361;&#24341;&#23548;&#26041;&#27861;&#21644;&#20854;&#20182;&#20248;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26426;&#22120;&#20154;&#24212;&#29992;&#20013;&#30340;&#22522;&#26412;&#30446;&#26631;&#20998;&#37197;&#38382;&#39064;&#26088;&#22312;&#20026;&#27599;&#20010;&#26426;&#22120;&#20154;&#20998;&#37197;&#19968;&#20010;&#21807;&#19968;&#30446;&#26631;&#65292;&#21516;&#26102;&#30830;&#20445;&#36335;&#24452;&#26080;&#30896;&#25758;&#65292;&#26368;&#23567;&#21270;&#24635;&#31227;&#21160;&#25104;&#26412;&#12290;&#36825;&#20010;NP-hard&#38382;&#39064;&#30340;&#19968;&#20010;&#21487;&#34892;&#31639;&#27861;&#35299;&#20915;&#26041;&#26696;&#28041;&#21450;&#19968;&#20010;&#36845;&#20195;&#36807;&#31243;&#65292;&#35813;&#36807;&#31243;&#25972;&#21512;&#20102;&#19968;&#20010;&#20219;&#21153;&#35268;&#21010;&#22120;&#26469;&#35745;&#31639;&#30446;&#26631;&#20998;&#37197;&#65292;&#21516;&#26102;&#24573;&#30053;&#26426;&#22120;&#20154;&#20043;&#38388;&#30340;&#30896;&#25758;&#21487;&#33021;&#24615;&#65292;&#20197;&#21450;&#19968;&#20010;&#22810;&#20195;&#29702;&#36335;&#24452;&#35268;&#21010;&#31639;&#27861;&#26469;&#25214;&#21040;&#32473;&#23450;&#20998;&#37197;&#30340;&#26080;&#30896;&#25758;&#36712;&#36857;&#12290;&#20026;&#20102;&#36991;&#20813;&#36825;&#19968;&#29942;&#39048;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#20914;&#31361;&#24341;&#23548;&#26041;&#27861;&#26469;&#35745;&#31639;&#19979;&#19968;&#20010;&#26368;&#20339;&#20998;&#37197;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#21478;&#22806;&#20004;&#31181;&#20248;&#21270;&#26041;&#27861;&#21040;&#31639;&#27861;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13292v1 Announce Type: cross  Abstract: The fundamental goal assignment problem for a multi-robot application aims to assign a unique goal to each robot while ensuring collision-free paths, minimizing the total movement cost. A plausible algorithmic solution to this NP-hard problem involves an iterative process that integrates a task planner to compute the goal assignment while ignoring the collision possibilities among the robots and a multi-agent path-finding algorithm to find the collision-free trajectories for a given assignment. This procedure involves a method for computing the next best assignment given the current best assignment. A naive way of computing the next best assignment, as done in the state-of-the-art solutions, becomes a roadblock to achieving scalability in solving the overall problem. To obviate this bottleneck, we propose an efficient conflict-guided method to compute the next best assignment. Additionally, we introduce two more optimizations to the al
&lt;/p&gt;</description></item><item><title>&#20174;&#35748;&#30693;&#31185;&#23398;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#35270;&#35282;&#25506;&#35752;&#22522;&#30784;&#24314;&#31435;&#30340;&#24494;&#22937;&#20043;&#22788;&#65292;&#25506;&#35752;&#31070;&#32463;&#31526;&#21495;&#26041;&#27861;&#22312;&#22914;&#20309;&#26356;&#20840;&#38754;&#22320;&#35299;&#20915;&#22522;&#30784;&#24314;&#31435;&#38382;&#39064;&#65292;&#35752;&#35770;&#22522;&#30784;&#24314;&#31435;&#39046;&#22495;&#30340;&#36827;&#19968;&#27493;&#21457;&#23637;&#26041;&#21521;&#12290;</title><link>https://arxiv.org/abs/2402.13290</link><description>&lt;p&gt;
&#20174;&#20154;&#24037;&#26234;&#33021;&#21644;&#35748;&#30693;&#31185;&#23398;&#30340;&#35270;&#35282;&#30475;&#22522;&#30784;&#24314;&#31435;
&lt;/p&gt;
&lt;p&gt;
Grounding from an AI and Cognitive Science Lens
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13290
&lt;/p&gt;
&lt;p&gt;
&#20174;&#35748;&#30693;&#31185;&#23398;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#35270;&#35282;&#25506;&#35752;&#22522;&#30784;&#24314;&#31435;&#30340;&#24494;&#22937;&#20043;&#22788;&#65292;&#25506;&#35752;&#31070;&#32463;&#31526;&#21495;&#26041;&#27861;&#22312;&#22914;&#20309;&#26356;&#20840;&#38754;&#22320;&#35299;&#20915;&#22522;&#30784;&#24314;&#31435;&#38382;&#39064;&#65292;&#35752;&#35770;&#22522;&#30784;&#24314;&#31435;&#39046;&#22495;&#30340;&#36827;&#19968;&#27493;&#21457;&#23637;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#30784;&#24314;&#31435;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#38656;&#35201;&#19968;&#20010;&#27491;&#24335;&#30340;&#23450;&#20041;&#21644;&#19981;&#21516;&#23618;&#27425;&#30340;&#25277;&#35937;&#12290;&#26412;&#25991;&#20174;&#35748;&#30693;&#31185;&#23398;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#35270;&#35282;&#25506;&#35752;&#20102;&#22522;&#30784;&#24314;&#31435;&#12290;&#23427;&#30830;&#23450;&#20102;&#22522;&#30784;&#24314;&#31435;&#30340;&#24494;&#22937;&#20043;&#22788;&#65292;&#23427;&#23545;&#21327;&#20316;&#20195;&#29702;&#30340;&#37325;&#35201;&#24615;&#65292;&#20197;&#21450;&#22312;&#20004;&#20010;&#31038;&#21306;&#20013;&#22522;&#30784;&#24314;&#31435;&#26041;&#27861;&#30340;&#30456;&#20284;&#20043;&#22788;&#21644;&#19981;&#21516;&#20043;&#22788;&#12290;&#26412;&#25991;&#36824;&#30740;&#31350;&#20102;&#19987;&#20026;&#22522;&#30784;&#24314;&#31435;&#20219;&#21153;&#37327;&#36523;&#23450;&#21046;&#30340;&#31070;&#32463;&#31526;&#21495;&#26041;&#27861;&#30340;&#28508;&#21147;&#65292;&#23637;&#31034;&#20102;&#23427;&#20204;&#22914;&#20309;&#26356;&#20840;&#38754;&#22320;&#35299;&#20915;&#22522;&#30784;&#24314;&#31435;&#38382;&#39064;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#22522;&#30784;&#24314;&#31435;&#39046;&#22495;&#30340;&#36827;&#19968;&#27493;&#25506;&#32034;&#21644;&#21457;&#23637;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13290v1 Announce Type: new  Abstract: Grounding is a challenging problem, requiring a formal definition and different levels of abstraction. This article explores grounding from both cognitive science and machine learning perspectives. It identifies the subtleties of grounding, its significance for collaborative agents, and similarities and differences in grounding approaches in both communities. The article examines the potential of neuro-symbolic approaches tailored for grounding tasks, showcasing how they can more comprehensively address grounding. Finally, we discuss areas for further exploration and development in grounding.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23398;&#20064;&#27169;&#25311;SQL&#26679;&#24335;&#20195;&#25968;&#25805;&#20316;&#30340;&#21463;&#38480;&#37096;&#20998;&#65292;&#25552;&#20379;&#20102;&#20013;&#38388;&#30417;&#30563;&#27493;&#39588;&#65292;&#30456;&#36739;&#20110;&#20256;&#32479;&#26041;&#27861;&#22686;&#21152;&#20102;&#27867;&#21270;&#24615;&#21644;&#32467;&#26500;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.13288</link><description>&lt;p&gt;
&#36890;&#36807;SQL&#26597;&#35810;&#20998;&#35299;&#36827;&#34892;&#22521;&#35757;&#34920;&#26684;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
Training Table Question Answering via SQL Query Decomposition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13288
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23398;&#20064;&#27169;&#25311;SQL&#26679;&#24335;&#20195;&#25968;&#25805;&#20316;&#30340;&#21463;&#38480;&#37096;&#20998;&#65292;&#25552;&#20379;&#20102;&#20013;&#38388;&#30417;&#30563;&#27493;&#39588;&#65292;&#30456;&#36739;&#20110;&#20256;&#32479;&#26041;&#27861;&#22686;&#21152;&#20102;&#27867;&#21270;&#24615;&#21644;&#32467;&#26500;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34920;&#26684;&#38382;&#31572;&#28041;&#21450;&#29702;&#35299;&#33258;&#28982;&#35821;&#35328;&#26597;&#35810;&#24182;&#23558;&#20854;&#25166;&#26681;&#20110;&#36755;&#20837;&#34920;&#26684;&#30340;&#19978;&#19979;&#25991;&#20013;&#65292;&#20197;&#25552;&#21462;&#30456;&#20851;&#20449;&#24687;&#12290;&#35768;&#22810;&#26041;&#27861;&#24378;&#35843;&#20102;&#20174;SQL&#26597;&#35810;&#36827;&#34892;&#20013;&#38388;&#39044;&#35757;&#32451;&#30340;&#22909;&#22788;&#12290;&#28982;&#32780;&#65292;&#34429;&#28982;&#22823;&#22810;&#25968;&#26041;&#27861;&#26088;&#22312;&#30452;&#25509;&#20174;&#36755;&#20837;&#29983;&#25104;&#26368;&#32456;&#31572;&#26696;&#65292;&#25105;&#20204;&#20027;&#24352;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#26356;&#22909;&#22320;&#21033;&#29992;SQL&#26597;&#35810;&#12290;&#36890;&#36807;&#23398;&#20064;&#27169;&#25311;SQL&#26679;&#24335;&#20195;&#25968;&#25805;&#20316;&#30340;&#21463;&#38480;&#37096;&#20998;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23427;&#20204;&#30340;&#25191;&#34892;&#27969;&#31243;&#25552;&#20379;&#20102;&#20013;&#38388;&#30417;&#30563;&#27493;&#39588;&#65292;&#20801;&#35768;&#30456;&#23545;&#20110;&#35813;&#39046;&#22495;&#30340;&#20256;&#32479;&#26041;&#27861;&#32780;&#35328;&#22686;&#21152;&#20102;&#27867;&#21270;&#24615;&#21644;&#32467;&#26500;&#25512;&#29702;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#24357;&#21512;&#20102;&#35821;&#20041;&#35299;&#26512;&#21644;&#30452;&#25509;&#22238;&#31572;&#26041;&#27861;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#24182;&#23601;&#29983;&#25104;&#26550;&#26500;&#24212;&#39044;&#27979;&#21738;&#20123;&#31867;&#22411;&#30340;&#25805;&#20316;&#25110;&#26368;&#22909;&#30001;&#22806;&#37096;&#31639;&#27861;&#25191;&#34892;&#25552;&#20379;&#20102;&#26377;&#29992;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13288v1 Announce Type: cross  Abstract: Table Question-Answering involves both understanding the natural language query and grounding it in the context of the input table to extract the relevant information. In this context, many methods have highlighted the benefits of intermediate pre-training from SQL queries. However, while most approaches aim at generating final answers from inputs directly, we claim that there is better to do with SQL queries during training. By learning to imitate a restricted portion of SQL-like algebraic operations, we show that their execution flow provides intermediate supervision steps that allow increased generalization and structural reasoning compared with classical approaches of the field. Our study bridges the gap between semantic parsing and direct answering methods and provides useful insights regarding what types of operations should be predicted by a generative architecture or be preferably executed by an external algorithm.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#27745;&#26579;&#25968;&#25454;&#65292;&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27010;&#29575;&#35270;&#35282;&#65292;&#29992;&#20110;&#25805;&#32437;&#38544;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#30340;&#25512;&#26029;&#65292;&#24182;&#24320;&#21457;&#20102;&#22810;&#31181;&#35299;&#20915;&#26041;&#27861;&#36827;&#34892;&#24212;&#29992;&#21644;&#23454;&#35777;&#27979;&#35797;&#12290;</title><link>https://arxiv.org/abs/2402.13287</link><description>&lt;p&gt;
&#36890;&#36807;&#27745;&#26579;&#25209;&#37327;&#25968;&#25454;&#25805;&#32437;&#38544;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#30340;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Manipulating hidden-Markov-model inferences by corrupting batch data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13287
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#27745;&#26579;&#25968;&#25454;&#65292;&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27010;&#29575;&#35270;&#35282;&#65292;&#29992;&#20110;&#25805;&#32437;&#38544;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#30340;&#25512;&#26029;&#65292;&#24182;&#24320;&#21457;&#20102;&#22810;&#31181;&#35299;&#20915;&#26041;&#27861;&#36827;&#34892;&#24212;&#29992;&#21644;&#23454;&#35777;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#36890;&#24120;&#20551;&#35774;&#25968;&#25454;&#27969;&#26159;&#26410;&#34987;&#27745;&#26579;&#21644;&#21512;&#27861;&#30340;&#12290;&#28982;&#32780;&#65292;&#19968;&#20010;&#33258;&#31169;&#30340;&#23545;&#25163;&#21487;&#33021;&#26377;&#21160;&#26426;&#26469;&#30772;&#22351;&#36825;&#20123;&#25968;&#25454;&#65292;&#20174;&#32780;&#25913;&#21464;&#20915;&#31574;&#32773;&#30340;&#25512;&#26029;&#12290;&#22312;&#26356;&#24191;&#27867;&#30340;&#23545;&#25239;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#65292;&#36825;&#39033;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27010;&#29575;&#35270;&#35282;&#65292;&#29992;&#20110;&#36890;&#36807;&#27745;&#26579;&#25968;&#25454;&#26469;&#25805;&#32437;&#38544;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#30340;&#25512;&#26029;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#22871;&#27745;&#26579;&#38382;&#39064;&#65292;&#29992;&#20110;&#36807;&#28388;&#12289;&#24179;&#28369;&#21644;&#35299;&#30721;&#25512;&#26029;&#65292;&#21033;&#29992;&#23545;&#25239;&#39118;&#38505;&#20998;&#26512;&#26041;&#27861;&#12290;&#25552;&#20986;&#20102;&#22810;&#20010;&#38543;&#26426;&#35268;&#21010;&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#32467;&#21512;&#20102;&#29616;&#23454;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#21644;&#19981;&#21516;&#30340;&#25915;&#20987;&#32773;&#30446;&#26631;&#12290;&#36890;&#36807;&#20174;&#39057;&#29575;&#20027;&#20041;&#21644;&#36125;&#21494;&#26031;&#35282;&#24230;&#20132;&#26367;&#22320;&#35266;&#23519;&#38382;&#39064;&#65292;&#24320;&#21457;&#20102;&#19977;&#31181;&#19968;&#33324;&#30340;&#35299;&#20915;&#26041;&#27861;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#35777;&#27979;&#35797;&#65292;&#35828;&#26126;&#20102;&#27599;&#31181;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#24320;&#21457;&#30340;&#26041;&#27861;&#20197;&#20854;&#35299;&#20915;&#36136;&#37327;&#20026;&#29305;&#28857;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13287v1 Announce Type: cross  Abstract: Time-series models typically assume untainted and legitimate streams of data. However, a self-interested adversary may have incentive to corrupt this data, thereby altering a decision maker's inference. Within the broader field of adversarial machine learning, this research provides a novel, probabilistic perspective toward the manipulation of hidden Markov model inferences via corrupted data. In particular, we provision a suite of corruption problems for filtering, smoothing, and decoding inferences leveraging an adversarial risk analysis approach. Multiple stochastic programming models are set forth that incorporate realistic uncertainties and varied attacker objectives. Three general solution methods are developed by alternatively viewing the problem from frequentist and Bayesian perspectives. The efficacy of each method is illustrated via extensive, empirical testing. The developed methods are characterized by their solution qualit
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#32467;&#26500;&#20449;&#24687;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#32467;&#26500;&#24341;&#23548;&#30340;SQL&#29983;&#25104;&#27169;&#22411;&#65292;&#20197;&#25913;&#21892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;SQL&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#25191;&#34892;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.13284</link><description>&lt;p&gt;
&#32467;&#26500;&#24341;&#23548;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;SQL&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Structure Guided Large Language Model for SQL Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13284
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#32467;&#26500;&#20449;&#24687;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#32467;&#26500;&#24341;&#23548;&#30340;SQL&#29983;&#25104;&#27169;&#22411;&#65292;&#20197;&#25913;&#21892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;SQL&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#25191;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#20934;&#30830;&#30340;&#32467;&#26500;&#21270;&#26597;&#35810;&#35821;&#35328;&#65288;SQL&#65289;&#26159;&#19968;&#20010;&#38271;&#26399;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#22312;&#23558;&#29992;&#25143;&#30340;&#35821;&#20041;&#26597;&#35810;&#19982;&#32467;&#26500;&#21270;&#25968;&#25454;&#24211;&#21305;&#37197;&#65292;&#28982;&#21518;&#29983;&#25104;&#32467;&#26500;&#21270;SQL&#26041;&#38754;&#12290;&#29616;&#26377;&#27169;&#22411;&#36890;&#24120;&#23558;&#26597;&#35810;&#21644;&#25968;&#25454;&#24211;&#27169;&#24335;&#36755;&#20837;&#21040;LLM&#20013;&#65292;&#24182;&#20381;&#36182;LLM&#25191;&#34892;&#35821;&#20041;-&#32467;&#26500;&#21305;&#37197;&#24182;&#29983;&#25104;&#32467;&#26500;&#21270;SQL&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#35299;&#20915;&#26041;&#26696;&#24573;&#30053;&#20102;&#29992;&#25143;&#26597;&#35810;&#21644;&#25968;&#25454;&#24211;&#20013;&#30340;&#32467;&#26500;&#20449;&#24687;&#65292;&#32780;&#36825;&#20123;&#20449;&#24687;&#21487;&#20197;&#29992;&#26469;&#22686;&#24378;&#32467;&#26500;&#21270;SQL&#30340;&#29983;&#25104;&#12290;&#36825;&#19968;&#30095;&#24573;&#21487;&#33021;&#23548;&#33268;&#19981;&#20934;&#30830;&#25110;&#26080;&#27861;&#25191;&#34892;&#30340;SQL&#29983;&#25104;&#12290;&#20026;&#20102;&#20805;&#20998;&#21033;&#29992;&#32467;&#26500;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32467;&#26500;&#21040;SQL&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#22266;&#26377;&#30340;&#32467;&#26500;&#20449;&#24687;&#26469;&#25913;&#21892;LLM&#30340;SQL&#29983;&#25104;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#25105;&#20204;&#30340;&#32467;&#26500;&#24341;&#23548;SQL&#65288;SGU-SQL&#65289;&#29983;&#25104;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13284v1 Announce Type: cross  Abstract: Generating accurate Structured Querying Language (SQL) is a long-standing problem, especially in matching users' semantic queries with structured databases and then generating structured SQL. Existing models typically input queries and database schemas into the LLM and rely on the LLM to perform semantic-structure matching and generate structured SQL. However, such solutions overlook the structural information within user queries and databases, which can be utilized to enhance the generation of structured SQL. This oversight can lead to inaccurate or unexecutable SQL generation. To fully exploit the structure, we propose a structure-to-SQL framework, which leverages the inherent structure information to improve the SQL generation of LLMs. Specifically, we introduce our Structure Guided SQL~(SGU-SQL) generation model. SGU-SQL first links user queries and databases in a structure-enhanced manner. It then decomposes complicated linked str
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#65292;&#23558;&#22768;&#23398;&#35821;&#38899;&#20449;&#24687;&#38598;&#25104;&#21040;LLMs&#26694;&#26550;&#20013;&#65292;&#20197;&#29992;&#20110;&#22810;&#27169;&#24335;&#25233;&#37057;&#26816;&#27979;&#12290;</title><link>https://arxiv.org/abs/2402.13276</link><description>&lt;p&gt;
&#24403;LLMs&#36935;&#21040;&#22768;&#23398;&#26631;&#24535;&#65306;&#19968;&#31181;&#39640;&#25928;&#22320;&#23558;&#35821;&#38899;&#38598;&#25104;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#29992;&#20110;&#25233;&#37057;&#26816;&#27979;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
When LLMs Meets Acoustic Landmarks: An Efficient Approach to Integrate Speech into Large Language Models for Depression Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13276
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#65292;&#23558;&#22768;&#23398;&#35821;&#38899;&#20449;&#24687;&#38598;&#25104;&#21040;LLMs&#26694;&#26550;&#20013;&#65292;&#20197;&#29992;&#20110;&#22810;&#27169;&#24335;&#25233;&#37057;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25233;&#37057;&#26159;&#20840;&#29699;&#24515;&#29702;&#20581;&#24247;&#20013;&#30340;&#19968;&#20010;&#20005;&#37325;&#20851;&#20999;&#65292;&#20419;&#20351;&#36827;&#34892;&#22823;&#37327;&#30740;&#31350;&#26469;&#25506;&#35752;&#22522;&#20110;AI&#30340;&#26816;&#27979;&#26041;&#27861;&#12290;&#22312;&#21508;&#31181;AI&#25216;&#26415;&#20013;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22240;&#20854;&#22312;&#24515;&#29702;&#21355;&#29983;&#24212;&#29992;&#20013;&#30340;&#22810;&#21151;&#33021;&#24615;&#32780;&#33073;&#39062;&#32780;&#20986;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#20027;&#35201;&#23616;&#38480;&#24615;&#22312;&#20110;&#23427;&#20204;&#20165;&#20381;&#36182;&#20110;&#25991;&#26412;&#36755;&#20837;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#25972;&#20307;&#21151;&#33021;&#12290;&#27492;&#22806;&#65292;LLMs&#22312;&#35782;&#21035;&#21644;&#20998;&#26512;&#25233;&#37057;&#29366;&#24577;&#26041;&#38754;&#30340;&#21033;&#29992;&#20173;&#30456;&#23545;&#26410;&#24320;&#21457;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#65292;&#23558;&#22768;&#23398;&#35821;&#38899;&#20449;&#24687;&#38598;&#25104;&#21040;LLMs&#26694;&#26550;&#20013;&#65292;&#20197;&#29992;&#20110;&#22810;&#27169;&#24335;&#25233;&#37057;&#26816;&#27979;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#36890;&#36807;&#21033;&#29992;&#22768;&#23398;&#26631;&#24535;&#23558;&#35821;&#38899;&#20449;&#21495;&#38598;&#25104;&#21040;LLMs&#20013;&#30340;&#39640;&#25928;&#25233;&#37057;&#26816;&#27979;&#26041;&#27861;&#12290;&#36890;&#36807;&#25972;&#21512;&#22768;&#23398;&#26631;&#24535;&#65292;&#36825;&#20123;&#26631;&#24535;&#26159;&#29305;&#23450;&#20110;&#21475;&#35821;&#21333;&#35789;&#21457;&#38899;&#30340;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20026;&#25991;&#26412;&#36716;&#24405;&#28155;&#21152;&#20102;&#20851;&#38190;&#32500;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13276v1 Announce Type: cross  Abstract: Depression is a critical concern in global mental health, prompting extensive research into AI-based detection methods. Among various AI technologies, Large Language Models (LLMs) stand out for their versatility in mental healthcare applications. However, their primary limitation arises from their exclusive dependence on textual input, which constrains their overall capabilities. Furthermore, the utilization of LLMs in identifying and analyzing depressive states is still relatively untapped. In this paper, we present an innovative approach to integrating acoustic speech information into the LLMs framework for multimodal depression detection. We investigate an efficient method for depression detection by integrating speech signals into LLMs utilizing Acoustic Landmarks. By incorporating acoustic landmarks, which are specific to the pronunciation of spoken words, our method adds critical dimensions to text transcripts. This integration a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#30382;&#23618;&#22522;&#24213;&#31070;&#32463;&#33410;&#29615;&#36335;&#27169;&#22411;&#30340;&#23454;&#29616;&#65292;&#29992;&#20110;&#34892;&#21160;&#36873;&#25321;&#21644;&#25191;&#34892;&#65292;&#22522;&#20110;&#22823;&#33041;&#30382;&#23618;&#39044;&#27979;&#34892;&#21160;&#65292;&#22522;&#24213;&#31070;&#32463;&#33410;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#20915;&#23450;&#25191;&#34892;&#21738;&#20123;&#34892;&#21160;&#12290;</title><link>https://arxiv.org/abs/2402.13275</link><description>&lt;p&gt;
&#30382;&#23618;&#22522;&#24213;&#31070;&#32463;&#33410;&#29615;&#36335;&#27169;&#22411;&#30340;&#23454;&#29616;
&lt;/p&gt;
&lt;p&gt;
Implementation of a Model of the Cortex Basal Ganglia Loop
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13275
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#30382;&#23618;&#22522;&#24213;&#31070;&#32463;&#33410;&#29615;&#36335;&#27169;&#22411;&#30340;&#23454;&#29616;&#65292;&#29992;&#20110;&#34892;&#21160;&#36873;&#25321;&#21644;&#25191;&#34892;&#65292;&#22522;&#20110;&#22823;&#33041;&#30382;&#23618;&#39044;&#27979;&#34892;&#21160;&#65292;&#22522;&#24213;&#31070;&#32463;&#33410;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#20915;&#23450;&#25191;&#34892;&#21738;&#20123;&#34892;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#30382;&#23618;-&#22522;&#24213;&#31070;&#32463;&#33410;-&#19992;&#33041;&#29615;&#36335;&#30340;&#31616;&#21333;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#34987;&#35748;&#20026;&#29992;&#20110;&#34892;&#21160;&#36873;&#25321;&#21644;&#25191;&#34892;&#65292;&#24182;&#25253;&#21578;&#20102;&#20854;&#23454;&#29616;&#30340;&#32467;&#26524;&#12290;&#35813;&#27169;&#22411;&#22522;&#20110;&#22823;&#33041;&#30382;&#23618;&#39044;&#27979;&#34892;&#21160;&#65292;&#32780;&#22522;&#24213;&#31070;&#32463;&#33410;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#26469;&#20915;&#23450;&#26159;&#21542;&#25191;&#34892;&#30382;&#23618;&#39044;&#27979;&#30340;&#34892;&#21160;&#12290;&#35813;&#23454;&#29616;&#26088;&#22312;&#29992;&#20316;&#30001;&#22823;&#33041;&#30382;&#23618;&#21306;&#22495;&#25110;&#21463;&#21551;&#21457;&#35748;&#30693;&#26550;&#26500;&#32452;&#25104;&#30340;&#27169;&#22411;&#30340;&#32452;&#25104;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13275v1 Announce Type: cross  Abstract: This article presents a simple model of the cortex-basal ganglia-thalamus loop, which is thought to serve for action selection and executions, and reports the results of its implementation. The model is based on the hypothesis that the cerebral cortex predicts actions, while the basal ganglia use reinforcement learning to decide whether to perform the actions predicted by the cortex. The implementation is intended to be used as a component of models of the brain consisting of cortical regions or brain-inspired cognitive architectures.
&lt;/p&gt;</description></item><item><title>&#32858;&#21512;&#20247;&#21253;&#39044;&#27979;&#20316;&#20026;&#38598;&#20307;&#26234;&#33021;&#30340;&#37325;&#35201;&#36827;&#23637;&#65292;&#20419;&#36827;&#20102;&#20154;&#31867;&#21644;AI&#30340;&#36830;&#25509;&#65292;&#20351;&#20915;&#31574;&#20855;&#26377;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2402.13273</link><description>&lt;p&gt;
&#20154;&#31867;&#19982;&#26426;&#22120;&#30340;&#36816;&#33829;&#38598;&#20307;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
Operational Collective Intelligence of Humans and Machines
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13273
&lt;/p&gt;
&lt;p&gt;
&#32858;&#21512;&#20247;&#21253;&#39044;&#27979;&#20316;&#20026;&#38598;&#20307;&#26234;&#33021;&#30340;&#37325;&#35201;&#36827;&#23637;&#65292;&#20419;&#36827;&#20102;&#20154;&#31867;&#21644;AI&#30340;&#36830;&#25509;&#65292;&#20351;&#20915;&#31574;&#20855;&#26377;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25506;&#35752;&#20102;&#32858;&#21512;&#24335;&#20247;&#21253;&#39044;&#27979;&#65288;ACF&#65289;&#30340;&#20351;&#29992;&#65292;&#20316;&#20026;&#24110;&#21161;&#36816;&#29992;&#20154;&#26426;&#22242;&#38431;&#30340;&#8220;&#38598;&#20307;&#26234;&#33021;&#8221;&#26469;&#21327;&#35843;&#34892;&#21160;&#30340;&#26426;&#21046;&#12290;&#25105;&#20204;&#37319;&#32435;&#20102;&#38598;&#20307;&#26234;&#33021;&#30340;&#23450;&#20041;&#65306;&#8220;&#19968;&#31181;&#20174;&#25968;&#25454;-&#20449;&#24687;-&#30693;&#35782;&#12289;&#36719;&#20214;-&#30828;&#20214;&#12289;&#20010;&#20154;&#65288;&#37027;&#20123;&#20855;&#26377;&#26032;&#35265;&#35299;&#20197;&#21450;&#20844;&#35748;&#26435;&#23041;&#65289;&#20043;&#38388;&#30340;&#21327;&#21516;&#20316;&#29992;&#20135;&#29983;&#30340;&#32676;&#20307;&#23646;&#24615;&#65292;&#20351;&#24471;&#23454;&#26102;&#30693;&#35782;&#33021;&#22815;&#27604;&#36825;&#19977;&#20010;&#20803;&#32032;&#21333;&#29420;&#34892;&#21160;&#20570;&#20986;&#26356;&#22909;&#30340;&#20915;&#31574;&#12290;&#8221;&#38598;&#20307;&#26234;&#33021;&#28304;&#33258;&#20110;&#36830;&#25509;&#20154;&#31867;&#21644;&#20154;&#24037;&#26234;&#33021;&#30340;&#26032;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#20915;&#31574;&#20248;&#21183;&#65292;&#37096;&#20998;&#21407;&#22240;&#22312;&#20110;&#21019;&#36896;&#21644;&#21033;&#29992;&#21487;&#33021;&#21542;&#21017;&#19981;&#20250;&#34987;&#21253;&#25324;&#30340;&#39069;&#22806;&#20449;&#24687;&#26469;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13273v1 Announce Type: new  Abstract: We explore the use of aggregative crowdsourced forecasting (ACF) as a mechanism to help operationalize ``collective intelligence'' of human-machine teams for coordinated actions. We adopt the definition for Collective Intelligence as: ``A property of groups that emerges from synergies among data-information-knowledge, software-hardware, and individuals (those with new insights as well as recognized authorities) that enables just-in-time knowledge for better decisions than these three elements acting alone.'' Collective Intelligence emerges from new ways of connecting humans and AI to enable decision-advantage, in part by creating and leveraging additional sources of information that might otherwise not be included. Aggregative crowdsourced forecasting (ACF) is a recent key advancement towards Collective Intelligence wherein predictions (X\% probability that Y will happen) and rationales (why I believe it is this probability that X will h
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25552;&#20986;&#20102;&#20154;&#24037;&#26234;&#33021;&#20013;&#33258;&#21457;&#24515;&#28789;&#29702;&#35770;&#30340;&#27010;&#24565;&#65292;&#24378;&#35843;&#20102;&#33258;&#21457;ToM&#30456;&#23545;&#20110;&#21551;&#21457;&#24335;ToM&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#20027;&#24352;&#21457;&#23637;AI ToM&#38656;&#35201;&#37319;&#21462;&#21407;&#21017;&#24615;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#26356;&#20855;&#24377;&#24615;&#30340;&#20154;&#24037;&#31038;&#20250;&#26234;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.13272</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#30340;&#33258;&#21457;&#24515;&#28789;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
Spontaneous Theory of Mind for Artificial Intelligence
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13272
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25552;&#20986;&#20102;&#20154;&#24037;&#26234;&#33021;&#20013;&#33258;&#21457;&#24515;&#28789;&#29702;&#35770;&#30340;&#27010;&#24565;&#65292;&#24378;&#35843;&#20102;&#33258;&#21457;ToM&#30456;&#23545;&#20110;&#21551;&#21457;&#24335;ToM&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#20027;&#24352;&#21457;&#23637;AI ToM&#38656;&#35201;&#37319;&#21462;&#21407;&#21017;&#24615;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#26356;&#20855;&#24377;&#24615;&#30340;&#20154;&#24037;&#31038;&#20250;&#26234;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#29616;&#26377;&#30340;&#24515;&#28789;&#29702;&#35770;&#65288;ToM&#65289;&#26041;&#27861;&#36807;&#20998;&#24378;&#35843;&#21551;&#21457;&#24335;&#25110;&#22522;&#20110;&#32447;&#32034;&#30340;ToM&#65292;&#21487;&#33021;&#38480;&#21046;&#25105;&#20204;&#21457;&#23637;&#20154;&#24037;&#31038;&#20250;&#26234;&#33021;&#65288;ASI&#65289;&#30340;&#38598;&#20307;&#33021;&#21147;&#12290;&#20511;&#37492;&#35745;&#31639;&#26426;&#31185;&#23398;&#12289;&#35748;&#30693;&#31185;&#23398;&#21644;&#30456;&#20851;&#23398;&#31185;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#23558;&#21551;&#21457;&#24335;ToM&#19982;&#25105;&#20204;&#25152;&#31216;&#30340;&#33258;&#21457;ToM&#36827;&#34892;&#23545;&#27604; - &#21363;&#20851;&#20110;&#20182;&#20154;&#24515;&#26234;&#29366;&#24577;&#30340;&#25512;&#29702;&#65292;&#22522;&#20110;&#26080;&#24847;&#35782;&#12289;&#21487;&#33021;&#26080;&#27861;&#25511;&#21046;&#30340;&#35748;&#30693;&#21151;&#33021;&#12290;&#25105;&#20204;&#20027;&#24352;&#23545;&#30740;&#31350;&#21644;&#21457;&#23637;AI ToM&#37319;&#21462;&#21407;&#21017;&#24615;&#30340;&#26041;&#27861;&#65292;&#24182;&#24314;&#35758;&#20581;&#20840;&#30340;&#25110;&#36890;&#29992;&#30340;ASI&#23558;&#23545;&#25552;&#31034;&#20316;&#20986;&#22238;&#24212;&#65292;&#24182;&#33258;&#21457;&#22320;&#21442;&#19982;&#31038;&#20250;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13272v1 Announce Type: new  Abstract: Existing approaches to Theory of Mind (ToM) in Artificial Intelligence (AI) overemphasize prompted, or cue-based, ToM, which may limit our collective ability to develop Artificial Social Intelligence (ASI). Drawing from research in computer science, cognitive science, and related disciplines, we contrast prompted ToM with what we call spontaneous ToM -- reasoning about others' mental states that is grounded in unintentional, possibly uncontrollable cognitive functions. We argue for a principled approach to studying and developing AI ToM and suggest that a robust, or general, ASI will respond to prompts \textit{and} spontaneously engage in social reasoning.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#29699;&#28909;&#24102;&#27668;&#26059;&#24378;&#24230;&#39044;&#27979;&#27169;&#22411;MSCAR&#65292;&#39318;&#27425;&#23558;&#22240;&#26524;&#20851;&#31995;&#19982;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#25968;&#25454;&#30456;&#32467;&#21512;&#65292;&#26377;&#25928;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.13270</link><description>&lt;p&gt;
&#20840;&#29699;&#28909;&#24102;&#27668;&#26059;&#24378;&#24230;&#39044;&#27979;&#30340;&#22810;&#27169;&#24577;&#22810;&#23610;&#24230;&#22240;&#26524;&#33258;&#22238;&#24402;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Global Tropical Cyclone Intensity Forecasting with Multi-modal Multi-scale Causal Autoregressive Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13270
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#29699;&#28909;&#24102;&#27668;&#26059;&#24378;&#24230;&#39044;&#27979;&#27169;&#22411;MSCAR&#65292;&#39318;&#27425;&#23558;&#22240;&#26524;&#20851;&#31995;&#19982;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#25968;&#25454;&#30456;&#32467;&#21512;&#65292;&#26377;&#25928;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31934;&#30830;&#39044;&#27979;&#28909;&#24102;&#27668;&#26059;&#65288;TC&#65289;&#24378;&#24230;&#23545;&#20110;&#21046;&#23450;&#28798;&#23475;&#39118;&#38505;&#20943;&#23569;&#31574;&#30053;&#33267;&#20851;&#37325;&#35201;&#12290;&#30446;&#21069;&#30340;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#26469;&#33258;ERA5&#25968;&#25454;&#30340;&#26377;&#38480;&#26102;&#31354;&#20449;&#24687;&#65292;&#24182;&#24573;&#35270;&#36825;&#20123;&#29289;&#29702;&#21464;&#37327;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#65292;&#26410;&#33021;&#20805;&#20998;&#25429;&#25417;&#24378;&#24230;&#39044;&#27979;&#25152;&#38656;&#30340;&#31354;&#38388;&#21644;&#26102;&#38388;&#27169;&#24335;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#22810;&#23610;&#24230;&#22240;&#26524;&#33258;&#22238;&#24402;&#27169;&#22411;&#65288;MSCAR&#65289;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#23558;&#22240;&#26524;&#20851;&#31995;&#19982;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#25968;&#25454;&#32467;&#21512;&#36215;&#26469;&#36827;&#34892;&#20840;&#29699;TC&#24378;&#24230;&#33258;&#22238;&#24402;&#39044;&#27979;&#30340;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#37492;&#20110;&#30446;&#21069;&#32570;&#20047;&#19968;&#20010;&#25552;&#20379;&#24191;&#27867;&#31354;&#38388;&#21464;&#37327;&#30340;TC&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#25512;&#20986;&#20102;&#22522;&#20110;&#21355;&#26143;&#21644;ERA5&#30340;&#28909;&#24102;&#27668;&#26059;&#25968;&#25454;&#38598;&#65288;SETCD&#65289;&#65292;&#23427;&#26159;&#19982;TC&#26377;&#20851;&#30340;&#26368;&#38271;&#21644;&#26368;&#20840;&#38754;&#30340;&#20840;&#29699;&#25968;&#25454;&#38598;&#12290;&#23545;&#35813;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#34920;&#26126;MSCAR&#32988;&#36807;&#20102;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13270v1 Announce Type: cross  Abstract: Accurate forecasting of Tropical cyclone (TC) intensity is crucial for formulating disaster risk reduction strategies. Current methods predominantly rely on limited spatiotemporal information from ERA5 data and neglect the causal relationships between these physical variables, failing to fully capture the spatial and temporal patterns required for intensity forecasting. To address this issue, we propose a Multi-modal multi-Scale Causal AutoRegressive model (MSCAR), which is the first model that combines causal relationships with large-scale multi-modal data for global TC intensity autoregressive forecasting. Furthermore, given the current absence of a TC dataset that offers a wide range of spatial variables, we present the Satellite and ERA5-based Tropical Cyclone Dataset (SETCD), which stands as the longest and most comprehensive global dataset related to TCs. Experiments on the dataset show that MSCAR outperforms the state-of-the-art
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30693;&#35782;&#22270;&#35889;&#21644;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;KGroot&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#26681;&#26412;&#21407;&#22240;&#20998;&#26512;&#30340;&#26041;&#27861;&#65292;&#24212;&#29992;&#20110;&#22312;&#32447;&#24494;&#26381;&#21153;&#20013;&#30340;&#25925;&#38556;&#23450;&#20301;&#65292;&#26088;&#22312;&#35299;&#20915;&#30417;&#25511;&#25968;&#25454;&#22810;&#26679;&#24615;&#12289;&#20107;&#20214;&#20256;&#25773;&#31561;&#25361;&#25112;&#65292;&#25552;&#39640;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.13264</link><description>&lt;p&gt;
KGroot&#65306;&#36890;&#36807;&#30693;&#35782;&#22270;&#35889;&#21644;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#22686;&#24378;&#26681;&#26412;&#21407;&#22240;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
KGroot: Enhancing Root Cause Analysis through Knowledge Graphs and Graph Convolutional Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13264
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30693;&#35782;&#22270;&#35889;&#21644;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;KGroot&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#26681;&#26412;&#21407;&#22240;&#20998;&#26512;&#30340;&#26041;&#27861;&#65292;&#24212;&#29992;&#20110;&#22312;&#32447;&#24494;&#26381;&#21153;&#20013;&#30340;&#25925;&#38556;&#23450;&#20301;&#65292;&#26088;&#22312;&#35299;&#20915;&#30417;&#25511;&#25968;&#25454;&#22810;&#26679;&#24615;&#12289;&#20107;&#20214;&#20256;&#25773;&#31561;&#25361;&#25112;&#65292;&#25552;&#39640;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25925;&#38556;&#23450;&#20301;&#22312;&#22312;&#32447;&#24494;&#26381;&#21153;&#20013;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#30417;&#25511;&#25968;&#25454;&#30340;&#25968;&#37327;&#12289;&#31867;&#22411;&#12289;&#20107;&#20214;&#20197;&#21450;&#26381;&#21153;&#21644;&#32452;&#20214;&#20043;&#38388;&#22797;&#26434;&#30340;&#30456;&#20114;&#20381;&#36182;&#24615;&#22810;&#31181;&#22810;&#26679;&#12290;&#26381;&#21153;&#20013;&#30340;&#25925;&#38556;&#20107;&#20214;&#26159;&#20855;&#26377;&#20256;&#25773;&#24615;&#30340;&#65292;&#21487;&#20197;&#22312;&#30701;&#26102;&#38388;&#20869;&#35302;&#21457;&#19968;&#31995;&#21015;&#35686;&#25253;&#12290;&#22312;&#24037;&#19994;&#30028;&#65292;&#25925;&#38556;&#23450;&#20301;&#36890;&#24120;&#30001;&#32463;&#39564;&#20016;&#23500;&#30340;&#20154;&#21592;&#25163;&#21160;&#36827;&#34892;&#12290;&#36825;&#31181;&#20381;&#36182;&#20110;&#32463;&#39564;&#30340;&#26041;&#24335;&#19981;&#21487;&#38752;&#19988;&#32570;&#20047;&#33258;&#21160;&#21270;&#12290;&#19981;&#21516;&#30340;&#27169;&#22359;&#22312;&#25163;&#21160;&#23450;&#20301;&#36807;&#31243;&#20013;&#23384;&#22312;&#20449;&#24687;&#38556;&#30861;&#65292;&#20351;&#24471;&#22312;&#32039;&#24613;&#25925;&#38556;&#26399;&#38388;&#24456;&#38590;&#24555;&#36895;&#23545;&#40784;&#12290;&#36825;&#31181;&#20302;&#25928;&#23548;&#33268;&#20102;&#31283;&#23450;&#24615;&#20445;&#35777;&#28382;&#21518;&#65292;&#26080;&#27861;&#21450;&#26102;&#26368;&#23567;&#21270;&#25925;&#38556;&#26816;&#27979;&#21644;&#20462;&#22797;&#26102;&#38388;&#12290;&#34429;&#28982;&#24050;&#32463;&#26377;&#21487;&#25805;&#20316;&#30340;&#26041;&#27861;&#26088;&#22312;&#33258;&#21160;&#21270;&#36825;&#20010;&#36807;&#31243;&#65292;&#20294;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#20173;&#19981;&#23613;&#22914;&#20154;&#24847;&#12290;&#25925;&#38556;&#23450;&#20301;&#32467;&#26524;&#30340;&#31934;&#30830;&#24615;&#38750;&#24120;&#37325;&#35201;&#65292;&#22240;&#20026;&#23427;&#20026;&#24037;&#31243;&#24072;&#23545;&#20174;&#22810;&#20010;&#26041;&#38754;&#24471;&#20986;&#30340;&#35786;&#26029;&#32467;&#35770;&#30340;&#20449;&#20219;&#22880;&#23450;&#20102;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13264v1 Announce Type: new  Abstract: Fault localization is challenging in online micro-service due to the wide variety of monitoring data volume, types, events and complex interdependencies in service and components. Faults events in services are propagative and can trigger a cascade of alerts in a short period of time. In the industry, fault localization is typically conducted manually by experienced personnel. This reliance on experience is unreliable and lacks automation. Different modules present information barriers during manual localization, making it difficult to quickly align during urgent faults. This inefficiency lags stability assurance to minimize fault detection and repair time. Though actionable methods aimed to automatic the process, the accuracy and efficiency are less than satisfactory. The precision of fault localization results is of paramount importance as it underpins engineers trust in the diagnostic conclusions, which are derived from multiple perspe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#30005;&#21160;&#27773;&#36710;&#20805;&#30005;&#31449;&#27169;&#22411;&#65292;&#36890;&#36807;&#29992;&#25143;&#34892;&#20026;&#24314;&#27169;&#21644;&#38543;&#26426;&#35268;&#21010;&#65292;&#35299;&#20915;&#20102;&#20805;&#30005;&#20250;&#35805;&#19981;&#30830;&#23450;&#24615;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#26041;&#27861;&#26469;&#20248;&#21270;&#25104;&#26412;&#24182;&#25552;&#39640;&#29992;&#25143;&#28385;&#24847;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.13224</link><description>&lt;p&gt;
&#36890;&#36807;&#29992;&#25143;&#34892;&#20026;&#24314;&#27169;&#21644;&#38543;&#26426;&#35268;&#21010;&#25511;&#21046;&#22823;&#22411;&#30005;&#21160;&#27773;&#36710;&#20805;&#30005;&#31449;
&lt;/p&gt;
&lt;p&gt;
Controlling Large Electric Vehicle Charging Stations via User Behavior Modeling and Stochastic Programming
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13224
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#30005;&#21160;&#27773;&#36710;&#20805;&#30005;&#31449;&#27169;&#22411;&#65292;&#36890;&#36807;&#29992;&#25143;&#34892;&#20026;&#24314;&#27169;&#21644;&#38543;&#26426;&#35268;&#21010;&#65292;&#35299;&#20915;&#20102;&#20805;&#30005;&#20250;&#35805;&#19981;&#30830;&#23450;&#24615;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#26041;&#27861;&#26469;&#20248;&#21270;&#25104;&#26412;&#24182;&#25552;&#39640;&#29992;&#25143;&#28385;&#24847;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#30005;&#21160;&#27773;&#36710;&#20805;&#30005;&#31449;&#65288;EVCS&#65289;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#34701;&#21512;&#20102;&#30495;&#23454;&#19990;&#30028;&#30340;&#32422;&#26463;&#26465;&#20214;&#65292;&#22914;&#25554;&#27133;&#21151;&#29575;&#38480;&#21046;&#12289;&#21512;&#21516;&#38408;&#20540;&#36229;&#38480;&#24809;&#32602;&#20197;&#21450;&#30005;&#21160;&#27773;&#36710;&#65288;EVs&#65289;&#30340;&#26089;&#26399;&#26029;&#24320;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22312;&#19981;&#30830;&#23450;&#24615;&#19979;&#25511;&#21046;EVCS&#30340;&#38382;&#39064;&#24418;&#24335;&#65292;&#24182;&#23454;&#26045;&#20102;&#20004;&#31181;&#22810;&#38454;&#27573;&#38543;&#26426;&#35268;&#21010;&#26041;&#27861;&#65292;&#21033;&#29992;&#29992;&#25143;&#25552;&#20379;&#30340;&#20449;&#24687;&#65292;&#21363;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#21644;&#20108;&#38454;&#27573;&#38543;&#26426;&#35268;&#21010;&#12290;&#35813;&#27169;&#22411;&#35299;&#20915;&#20102;&#20805;&#30005;&#20250;&#35805;&#24320;&#22987;&#21644;&#32467;&#26463;&#26102;&#38388;&#20197;&#21450;&#33021;&#37327;&#38656;&#27714;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#22522;&#20110;&#39547;&#30041;&#26102;&#38388;&#20381;&#36182;&#38543;&#26426;&#36807;&#31243;&#30340;&#29992;&#25143;&#34892;&#20026;&#27169;&#22411;&#22686;&#24378;&#20102;&#25104;&#26412;&#38477;&#20302;&#30340;&#21516;&#26102;&#20445;&#25345;&#23458;&#25143;&#28385;&#24847;&#24230;&#12290;&#36890;&#36807;&#20351;&#29992;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;22&#22825;&#27169;&#25311;&#23637;&#31034;&#20102;&#20004;&#31181;&#25552;&#20986;&#26041;&#27861;&#30456;&#23545;&#20110;&#20004;&#20010;&#22522;&#32447;&#30340;&#20248;&#21183;&#12290;&#20004;&#38454;&#27573;&#26041;&#27861;&#35777;&#26126;&#20102;&#38024;&#23545;&#26089;&#26399;&#26029;&#24320;&#30340;&#40065;&#26834;&#24615;&#65292;&#32771;&#34385;&#20102;&#26356;&#22810;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13224v1 Announce Type: cross  Abstract: This paper introduces an Electric Vehicle Charging Station (EVCS) model that incorporates real-world constraints, such as slot power limitations, contract threshold overruns penalties, or early disconnections of electric vehicles (EVs). We propose a formulation of the problem of EVCS control under uncertainty, and implement two Multi-Stage Stochastic Programming approaches that leverage user-provided information, namely, Model Predictive Control and Two-Stage Stochastic Programming. The model addresses uncertainties in charging session start and end times, as well as in energy demand. A user's behavior model based on a sojourn-time-dependent stochastic process enhances cost reduction while maintaining customer satisfaction. The benefits of the two proposed methods are showcased against two baselines over a 22-day simulation using a real-world dataset. The two-stage approach proves robust against early disconnections, considering a more
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#39640;&#36136;&#37327;&#30340;&#24102;&#27880;&#37322;&#20013;&#25991;&#38544;&#21947;&#35821;&#26009;&#24211;&#65292;&#24378;&#35843;&#38544;&#21947;&#29983;&#25104;&#20013;&#30340;&#22522;&#30784;&#21450;&#20854;&#29420;&#29305;&#29305;&#24449;&#65292;&#32780;&#38750;&#20256;&#32479;&#30340;&#23545;&#35937;&#21644;&#36733;&#20307;&#32452;&#21512;&#12290;</title><link>https://arxiv.org/abs/2402.13145</link><description>&lt;p&gt;
CMDAG: &#19968;&#20010;&#24102;&#26377;&#27880;&#37322;&#30340;&#20013;&#25991;&#38544;&#21947;&#25968;&#25454;&#38598;&#20316;&#20026;&#8220;CoT&#8221;&#26469;&#25552;&#21319;&#38544;&#21947;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
CMDAG: A Chinese Metaphor Dataset with Annotated Grounds as CoT for Boosting Metaphor Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13145
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#39640;&#36136;&#37327;&#30340;&#24102;&#27880;&#37322;&#20013;&#25991;&#38544;&#21947;&#35821;&#26009;&#24211;&#65292;&#24378;&#35843;&#38544;&#21947;&#29983;&#25104;&#20013;&#30340;&#22522;&#30784;&#21450;&#20854;&#29420;&#29305;&#29305;&#24449;&#65292;&#32780;&#38750;&#20256;&#32479;&#30340;&#23545;&#35937;&#21644;&#36733;&#20307;&#32452;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38544;&#21947;&#26159;&#20154;&#31867;&#35821;&#35328;&#21644;&#25991;&#23398;&#20013;&#26174;&#33879;&#30340;&#20462;&#36766;&#25163;&#27861;&#65292;&#22240;&#20026;&#23427;&#20204;&#22686;&#28155;&#20102;&#33394;&#24425;&#12289;&#24418;&#35937;&#21644;&#24378;&#35843;&#65292;&#20197;&#22686;&#24378;&#26377;&#25928;&#20132;&#27969;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#39640;&#36136;&#37327;&#30340;&#24102;&#27880;&#37322;&#20013;&#25991;&#38544;&#21947;&#35821;&#26009;&#24211;&#65292;&#21253;&#25324;&#32422;28K&#21477;&#26469;&#33258;&#21508;&#31181;&#20013;&#25991;&#25991;&#23398;&#26469;&#28304;&#65288;&#22914;&#35799;&#27468;&#12289;&#25955;&#25991;&#12289;&#27468;&#35789;&#31561;&#65289;&#12290;&#20026;&#30830;&#20445;&#27880;&#37322;&#30340;&#20934;&#30830;&#24615;&#21644;&#19968;&#33268;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#22871;&#20840;&#38754;&#30340;&#25351;&#21335;&#12290;&#36825;&#20123;&#25351;&#21335;&#28085;&#30422;&#20102;&#38544;&#21947;&#26631;&#27880;&#30340;&#26041;&#38754;&#65292;&#21253;&#25324;&#35782;&#21035;&#23545;&#35937;&#12289;&#36733;&#20307;&#21644;&#22522;&#30784;&#65292;&#20197;&#22788;&#29702;&#27604;&#21947;&#12289;&#25311;&#20154;&#12289;&#24182;&#21015;&#21644;&#22840;&#24352;&#31561;&#22797;&#26434;&#24615;&#12290;&#25171;&#30772;&#20256;&#32479;&#65292;&#25105;&#20204;&#30340;&#38544;&#21947;&#29983;&#25104;&#26041;&#27861;&#24378;&#35843;&#22522;&#30784;&#21450;&#20854;&#29420;&#29305;&#29305;&#24449;&#65292;&#32780;&#19981;&#26159;&#20256;&#32479;&#30340;&#23545;&#35937;&#21644;&#36733;&#20307;&#32452;&#21512;&#12290;&#36890;&#36807;&#23558;&#8220;&#22522;&#30784;&#8221;&#20316;&#20026;&#8220;CoT&#8221;&#65288;&#24605;&#32500;&#38142;&#65289;&#36755;&#20837;&#36827;&#34892;&#25972;&#21512;&#65292;&#25105;&#20204;&#33021;&#22815;&#29983;&#25104;&#37325;&#26032;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13145v1 Announce Type: cross  Abstract: Metaphor is a prominent linguistic device in human language and literature, as they add color, imagery, and emphasis to enhance effective communication. This paper introduces a large-scale high quality annotated Chinese Metaphor Corpus, which comprises around 28K sentences drawn from a diverse range of Chinese literary sources, such as poems, prose, song lyrics, etc. To ensure the accuracy and consistency of our annotations, we introduce a comprehensive set of guidelines. These guidelines address the facets of metaphor annotation, including identifying tenors, vehicles, and grounds to handling the complexities of similes, personifications, juxtapositions, and hyperboles. Breaking tradition, our approach to metaphor generation emphasizes grounds and their distinct features rather than the conventional combination of tenors and vehicles. By integrating "ground" as a CoT (Chain of Thoughts) input, we are able to generate metaphors that re
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Rectify-Router&#35299;&#20915;&#20102;MoE&#27169;&#22411;&#20013;&#24120;&#29992;&#30340;Top-k&#36335;&#30001;&#26426;&#21046;&#25152;&#24102;&#26469;&#30340;&#20196;&#29260;&#20002;&#22833;&#21644;&#22635;&#20805;&#38382;&#39064;&#65292;&#36890;&#36807;Intra-GPU&#30699;&#27491;&#21644;Fill-in&#30699;&#27491;&#26469;&#23454;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.12399</link><description>&lt;p&gt;
&#23558;&#24223;&#26009;&#21464;&#24223;&#20026;&#23453;&#65306;&#30699;&#27491;MoE&#30340;Top-k&#36335;&#30001;&#22120;
&lt;/p&gt;
&lt;p&gt;
Turn Waste into Worth: Rectifying Top-$k$ Router of MoE
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12399
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Rectify-Router&#35299;&#20915;&#20102;MoE&#27169;&#22411;&#20013;&#24120;&#29992;&#30340;Top-k&#36335;&#30001;&#26426;&#21046;&#25152;&#24102;&#26469;&#30340;&#20196;&#29260;&#20002;&#22833;&#21644;&#22635;&#20805;&#38382;&#39064;&#65292;&#36890;&#36807;Intra-GPU&#30699;&#27491;&#21644;Fill-in&#30699;&#27491;&#26469;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31232;&#30095;&#28151;&#21512;&#19987;&#23478;&#65288;MoE&#65289;&#27169;&#22411;&#22240;&#20854;&#35745;&#31639;&#25928;&#29575;&#32780;&#21463;&#21040;&#27426;&#36814;&#65292;&#29992;&#20110;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#24120;&#29992;&#30340;Top-k&#36335;&#30001;&#26426;&#21046;&#30001;&#20110;&#19981;&#24179;&#34913;&#30340;&#36335;&#30001;&#23548;&#33268;&#20887;&#20313;&#35745;&#31639;&#21644;&#20869;&#23384;&#25104;&#26412;&#36807;&#39640;&#12290;&#19968;&#20123;&#19987;&#23478;&#20250;&#28322;&#20986;&#65292;&#20854;&#20013;&#36229;&#20986;&#30340;&#20196;&#29260;&#20250;&#34987;&#20002;&#24323;&#12290;&#32780;&#19968;&#20123;&#19987;&#23478;&#26159;&#31354;&#38386;&#30340;&#65292;&#36825;&#20123;&#19987;&#23478;&#20250;&#22635;&#20805;&#20026;&#38646;&#65292;&#36127;&#38754;&#24433;&#21709;&#20102;&#27169;&#22411;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#20002;&#24323;&#20196;&#29260;&#21644;&#22635;&#20805;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Rectify-Router&#65292;&#21253;&#25324;Intra-GPU&#30699;&#27491;&#21644;Fill-in&#30699;&#27491;&#12290;Intra-GPU&#30699;&#27491;&#22788;&#29702;&#20002;&#24323;&#30340;&#20196;&#29260;&#65292;&#23558;&#23427;&#20204;&#26377;&#25928;&#22320;&#36335;&#30001;&#21040;GPU&#20869;&#30340;&#19987;&#23478;&#65292;&#36991;&#20813;&#36328;GPU&#36890;&#20449;&#12290;Fill-in&#30699;&#27491;&#36890;&#36807;&#29992;&#20855;&#26377;&#39640;&#36335;&#30001;&#20998;&#25968;&#30340;&#20196;&#29260;&#26367;&#25442;&#22635;&#20805;&#20196;&#29260;&#26469;&#35299;&#20915;&#22635;&#20805;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;Intra-GPU&#30699;&#27491;&#21644;Fill-in&#30699;&#27491;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12399v1 Announce Type: cross  Abstract: Sparse Mixture of Experts (MoE) models are popular for training large language models due to their computational efficiency. However, the commonly used top-$k$ routing mechanism suffers from redundancy computation and memory costs due to the unbalanced routing. Some experts are overflow, where the exceeding tokens are dropped. While some experts are vacant, which are padded with zeros, negatively impacting model performance. To address the dropped tokens and padding, we propose the Rectify-Router, comprising the Intra-GPU Rectification and the Fill-in Rectification. The Intra-GPU Rectification handles dropped tokens, efficiently routing them to experts within the GPU where they are located to avoid inter-GPU communication. The Fill-in Rectification addresses padding by replacing padding tokens with the tokens that have high routing scores. Our experimental results demonstrate that the Intra-GPU Rectification and the Fill-in Rectificati
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;AI&#31185;&#23398;&#23478;&#22242;&#38431;&#65288;TAIS&#65289;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#31616;&#21270;&#31185;&#23398;&#21457;&#29616;&#27969;&#31243;&#65292;&#30001;&#27169;&#25311;&#35282;&#33394;&#21327;&#20316;&#65292;&#29305;&#21035;&#20851;&#27880;&#20110;&#35782;&#21035;&#20855;&#26377;&#30142;&#30149;&#39044;&#27979;&#20215;&#20540;&#30340;&#22522;&#22240;</title><link>https://arxiv.org/abs/2402.12391</link><description>&lt;p&gt;
&#23454;&#29616;&#22522;&#22240;&#34920;&#36798;&#25968;&#25454;&#31185;&#23398;&#21457;&#29616;&#30340;AI&#31185;&#23398;&#23478;&#22242;&#38431;
&lt;/p&gt;
&lt;p&gt;
Toward a Team of AI-made Scientists for Scientific Discovery from Gene Expression Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12391
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;AI&#31185;&#23398;&#23478;&#22242;&#38431;&#65288;TAIS&#65289;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#31616;&#21270;&#31185;&#23398;&#21457;&#29616;&#27969;&#31243;&#65292;&#30001;&#27169;&#25311;&#35282;&#33394;&#21327;&#20316;&#65292;&#29305;&#21035;&#20851;&#27880;&#20110;&#35782;&#21035;&#20855;&#26377;&#30142;&#30149;&#39044;&#27979;&#20215;&#20540;&#30340;&#22522;&#22240;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#24050;&#25104;&#20026;&#31185;&#23398;&#21457;&#29616;&#30340;&#24378;&#22823;&#24037;&#20855;&#65292;&#20351;&#30740;&#31350;&#20154;&#21592;&#33021;&#22815;&#20174;&#22797;&#26434;&#25968;&#25454;&#38598;&#20013;&#25552;&#21462;&#26377;&#24847;&#20041;&#30340;&#35265;&#35299;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#21517;&#20026;AI&#31185;&#23398;&#23478;&#22242;&#38431;&#65288;TAIS&#65289;&#65292;&#26088;&#22312;&#31616;&#21270;&#31185;&#23398;&#21457;&#29616;&#27969;&#31243;&#12290;TAIS&#21253;&#25324;&#27169;&#25311;&#35282;&#33394;&#65292;&#21253;&#25324;&#39033;&#30446;&#32463;&#29702;&#12289;&#25968;&#25454;&#24037;&#31243;&#24072;&#21644;&#39046;&#22495;&#19987;&#23478;&#65292;&#27599;&#20010;&#35282;&#33394;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20195;&#34920;&#12290;&#36825;&#20123;&#35282;&#33394;&#21327;&#20316;&#20197;&#22797;&#21046;&#25968;&#25454;&#31185;&#23398;&#23478;&#36890;&#24120;&#25191;&#34892;&#30340;&#20219;&#21153;&#65292;&#29305;&#21035;&#20851;&#27880;&#20110;&#35782;&#21035;&#20855;&#26377;&#30142;&#30149;&#39044;&#27979;&#20215;&#20540;&#30340;&#22522;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12391v1 Announce Type: cross  Abstract: Machine learning has emerged as a powerful tool for scientific discovery, enabling researchers to extract meaningful insights from complex datasets. For instance, it has facilitated the identification of disease-predictive genes from gene expression data, significantly advancing healthcare. However, the traditional process for analyzing such datasets demands substantial human effort and expertise for the data selection, processing, and analysis. To address this challenge, we introduce a novel framework, a Team of AI-made Scientists (TAIS), designed to streamline the scientific discovery pipeline. TAIS comprises simulated roles, including a project manager, data engineer, and domain expert, each represented by a Large Language Model (LLM). These roles collaborate to replicate the tasks typically performed by data scientists, with a specific focus on identifying disease-predictive genes. Furthermore, we have curated a benchmark dataset t
&lt;/p&gt;</description></item><item><title>&#23433;&#20840;&#23545;&#40784;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#33021;&#20250;&#36890;&#36807;&#27169;&#25311;&#22833;&#35843;&#26694;&#26550;&#65292;&#22312;&#23545;&#25239;&#24615;&#25805;&#32437;&#19979;&#20135;&#29983;&#21361;&#38505;&#32467;&#26524;&#65292;&#23545;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#21452;&#20493;&#26377;&#23475;&#24615;&#65292;&#39640;&#20110;&#24378;&#22522;&#32447;&#65292;&#24378;&#35843;&#20102;&#21363;&#20351;&#22312;&#23433;&#20840;&#23545;&#40784;&#21518;&#20063;&#38656;&#35201;&#37325;&#26032;&#35780;&#20272;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.12343</link><description>&lt;p&gt;
&#27169;&#25311;&#22833;&#35843;: &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23433;&#20840;&#23545;&#40784;&#21487;&#33021;&#20250;&#36866;&#24471;&#20854;&#21453;&#65281;
&lt;/p&gt;
&lt;p&gt;
Emulated Disalignment: Safety Alignment for Large Language Models May Backfire!
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12343
&lt;/p&gt;
&lt;p&gt;
&#23433;&#20840;&#23545;&#40784;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#33021;&#20250;&#36890;&#36807;&#27169;&#25311;&#22833;&#35843;&#26694;&#26550;&#65292;&#22312;&#23545;&#25239;&#24615;&#25805;&#32437;&#19979;&#20135;&#29983;&#21361;&#38505;&#32467;&#26524;&#65292;&#23545;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#21452;&#20493;&#26377;&#23475;&#24615;&#65292;&#39640;&#20110;&#24378;&#22522;&#32447;&#65292;&#24378;&#35843;&#20102;&#21363;&#20351;&#22312;&#23433;&#20840;&#23545;&#40784;&#21518;&#20063;&#38656;&#35201;&#37325;&#26032;&#35780;&#20272;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#38656;&#35201;&#36827;&#34892;&#23433;&#20840;&#23545;&#40784;&#65292;&#20197;&#30830;&#20445;&#19982;&#20154;&#31867;&#36827;&#34892;&#23433;&#20840;&#30340;&#23545;&#35805;&#12290;&#28982;&#32780;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#25512;&#29702;&#26102;&#25915;&#20987;&#26694;&#26550;&#65292;&#34920;&#26126;&#23433;&#20840;&#23545;&#40784;&#20063;&#21487;&#33021;&#22312;&#23545;&#25239;&#24615;&#25805;&#32437;&#19979;&#26080;&#24847;&#20013;&#20419;&#25104;&#26377;&#23475;&#32467;&#26524;&#12290;&#36825;&#20010;&#26694;&#26550;&#34987;&#21629;&#21517;&#20026;&#27169;&#25311;&#22833;&#35843;&#65288;ED&#65289;&#65292;&#22312;&#36755;&#20986;&#31354;&#38388;&#20013;&#19981;&#33391;&#22320;&#32452;&#21512;&#20102;&#19968;&#23545;&#24320;&#28304;&#39044;&#35757;&#32451;&#21644;&#23433;&#20840;&#23545;&#40784;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#20135;&#29983;&#20102;&#19968;&#20010;&#26377;&#23475;&#30340;&#35821;&#35328;&#27169;&#22411;&#32780;&#26080;&#38656;&#20219;&#20309;&#35757;&#32451;&#12290;&#25105;&#20204;&#23545;ED&#22312;&#19977;&#20010;&#25968;&#25454;&#38598;&#21644;&#22235;&#20010;&#27169;&#22411;&#31995;&#21015;&#65288;Llama-1&#12289;Llama-2&#12289;Mistral&#21644;Alpaca&#65289;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;ED&#20351;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#26377;&#23475;&#24615;&#22686;&#21152;&#20102;&#19968;&#20493;&#65292;&#24182;&#32988;&#36807;&#24378;&#22522;&#32447;&#65292;&#20197;&#36739;&#22823;&#20248;&#21183;&#22312;48&#20010;&#35780;&#20272;&#23376;&#38598;&#20013;&#30340;43&#20010;&#20013;&#23454;&#29616;&#20102;&#26368;&#39640;&#30340;&#26377;&#23475;&#29575;&#12290;&#33267;&#20851;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#20984;&#26174;&#20102;&#21363;&#20351;&#22312;&#23433;&#20840;&#23545;&#40784;&#21518;&#65292;&#37325;&#26032;&#35780;&#20272;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#23454;&#36341;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12343v1 Announce Type: new  Abstract: Large language models (LLMs) need to undergo safety alignment to ensure safe conversations with humans. However, in this work, we introduce an inference-time attack framework, demonstrating that safety alignment can also unintentionally facilitate harmful outcomes under adversarial manipulation. This framework, named Emulated Disalignment (ED), adversely combines a pair of open-source pre-trained and safety-aligned language models in the output space to produce a harmful language model without any training. Our experiments with ED across three datasets and four model families (Llama-1, Llama-2, Mistral, and Alpaca) show that ED doubles the harmfulness of pre-trained models and outperforms strong baselines, achieving the highest harmful rate in 43 out of 48 evaluation subsets by a large margin. Crucially, our findings highlight the importance of reevaluating the practice of open-sourcing language models even after safety alignment.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#20844;&#24179;&#24615;&#21407;&#21017;&#8212;&#8212;&#24179;&#31561;&#20445;&#25252;&#65292;&#20854;&#20851;&#38190;&#22312;&#20110;&#23558;&#38169;&#35823;&#20998;&#31867;&#30340;&#39118;&#38505;&#22343;&#31561;&#21270;&#65292;&#36991;&#20813;&#20102;&#35768;&#22810;&#23545;&#20256;&#32479;&#20998;&#31867;&#24179;&#31561;&#21407;&#21017;&#30340;&#21453;&#20363;&#12290;</title><link>https://arxiv.org/abs/2402.12062</link><description>&lt;p&gt;
&#22240;&#26524;&#24179;&#31561;&#20445;&#25252;&#19982;&#31639;&#27861;&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;
Causal Equal Protection as Algorithmic Fairness
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12062
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#20844;&#24179;&#24615;&#21407;&#21017;&#8212;&#8212;&#24179;&#31561;&#20445;&#25252;&#65292;&#20854;&#20851;&#38190;&#22312;&#20110;&#23558;&#38169;&#35823;&#20998;&#31867;&#30340;&#39118;&#38505;&#22343;&#31561;&#21270;&#65292;&#36991;&#20813;&#20102;&#35768;&#22810;&#23545;&#20256;&#32479;&#20998;&#31867;&#24179;&#31561;&#21407;&#21017;&#30340;&#21453;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#21313;&#24180;&#65292;&#35745;&#31639;&#26426;&#31185;&#23398;&#21644;&#21746;&#23398;&#30340;&#25991;&#29486;&#24418;&#25104;&#20102;&#19981;&#21516;&#30340;&#31639;&#27861;&#20844;&#24179;&#24615;&#26631;&#20934;&#12290;&#20854;&#20013;&#26368;&#21463;&#20105;&#35758;&#30340;&#20998;&#31867;&#24179;&#31561;&#35201;&#27714;&#65292;&#39044;&#27979;&#31639;&#27861;&#30340;&#38169;&#35823;&#20998;&#31867;&#22312;&#34987;&#20445;&#25252;&#29305;&#24449;&#25152;&#25351;&#31034;&#30340;&#32676;&#20307;&#20013;&#20197;&#30456;&#31561;&#39057;&#29575;&#21457;&#29983;&#12290;&#23613;&#31649;&#20998;&#31867;&#24179;&#31561;&#20855;&#26377;&#30452;&#35266;&#21560;&#24341;&#21147;&#65292;&#20294;&#24050;&#21463;&#21040;&#25915;&#20987;&#12290;&#25105;&#20204;&#36716;&#21521;&#19968;&#20010;&#30456;&#20851;&#21407;&#21017;&#65292;&#21363;&#24179;&#31561;&#20445;&#25252;&#65292;&#35813;&#21407;&#21017;&#26368;&#21021;&#26159;&#22312;&#21009;&#20107;&#21496;&#27861;&#39046;&#22495;&#21457;&#23637;&#36215;&#26469;&#30340;&#12290;&#24179;&#31561;&#20445;&#25252;&#30340;&#20851;&#38190;&#22312;&#20110;&#23558;&#38169;&#35823;&#20998;&#31867;&#30340;&#39118;&#38505;&#65288;&#23558;&#22312;&#35268;&#23450;&#30340;&#24847;&#20041;&#19978;&#20855;&#20307;&#35828;&#26126;&#65289;&#36827;&#34892;&#22343;&#31561;&#21270;&#65292;&#32780;&#19981;&#26159;&#23558;&#38169;&#35823;&#20998;&#31867;&#30340;&#27604;&#29575;&#22343;&#31561;&#21270;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#24179;&#31561;&#20445;&#25252;&#36991;&#20813;&#20102;&#35768;&#22810;&#23545;&#20998;&#31867;&#24179;&#31561;&#30340;&#21453;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12062v1 Announce Type: cross  Abstract: Over the last ten years the literature in computer science and philosophy has formulated different criteria of algorithmic fairness. One of the most discussed, classification parity, requires that the erroneous classifications of a predictive algorithm occur with equal frequency for groups picked out by protected characteristics. Despite its intuitive appeal, classification parity has come under attack. Multiple scenarios can be imagined in which - intuitively - a predictive algorithm does not treat any individual unfairly, and yet classification parity is violated. To make progress, we turn to a related principle, equal protection, originally developed in the context of criminal justice. Key to equal protection is equalizing the risks of erroneous classifications (in a sense to be specified) as opposed to equalizing the rates of erroneous classifications. We show that equal protection avoids many of the counterexamples to classificati
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#26410;&#26631;&#35760;&#39640;&#32500;&#23454;&#20540;&#26426;&#22120;&#20154;&#36712;&#36857;&#24320;&#22987;&#33258;&#20027;&#23398;&#20064;&#36890;&#29992;&#30340;&#36923;&#36753;&#30456;&#20851;&#34920;&#31034;&#65292;&#36825;&#20123;&#34920;&#31034;&#26500;&#25104;&#20102;&#33258;&#21160;&#21457;&#26126;&#30340;PDDL-like&#22495;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.11871</link><description>&lt;p&gt;
&#20174;&#23454;&#38469;&#21040;&#36923;&#36753;&#20877;&#21040;&#23454;&#38469;&#65306;&#20026;&#35268;&#21010;&#20174;&#21407;&#22987;&#25968;&#25454;&#20013;&#21457;&#26126;&#31526;&#21495;&#35789;&#27719;&#12289;&#21160;&#20316;&#21644;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
From Reals to Logic and Back: Inventing Symbolic Vocabularies, Actions and Models for Planning from Raw Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11871
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#26410;&#26631;&#35760;&#39640;&#32500;&#23454;&#20540;&#26426;&#22120;&#20154;&#36712;&#36857;&#24320;&#22987;&#33258;&#20027;&#23398;&#20064;&#36890;&#29992;&#30340;&#36923;&#36753;&#30456;&#20851;&#34920;&#31034;&#65292;&#36825;&#20123;&#34920;&#31034;&#26500;&#25104;&#20102;&#33258;&#21160;&#21457;&#26126;&#30340;PDDL-like&#22495;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25163;&#24037;&#21046;&#20316;&#30340;&#22522;&#20110;&#36923;&#36753;&#30340;&#29366;&#24577;&#21644;&#21160;&#20316;&#34920;&#31034;&#24050;&#34987;&#24191;&#27867;&#29992;&#20110;&#20811;&#26381;&#38271;&#26399;&#20154;&#24037;&#26234;&#33021;&#26426;&#22120;&#20154;&#35268;&#21010;&#38382;&#39064;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#65292;&#21253;&#25324;&#20219;&#21153;&#21644;&#21160;&#20316;&#35268;&#21010;&#38382;&#39064;&#12290;&#20294;&#26159;&#65292;&#21019;&#24314;&#36825;&#26679;&#30340;&#34920;&#31034;&#38656;&#35201;&#20855;&#26377;&#24378;&#28872;&#30452;&#35273;&#21644;&#35814;&#32454;&#30693;&#35782;&#30340;&#19987;&#23478;&#65292;&#20182;&#20204;&#20102;&#35299;&#26426;&#22120;&#20154;&#21644;&#22312;&#29305;&#23450;&#29615;&#22659;&#20013;&#21487;&#33021;&#38656;&#35201;&#23436;&#25104;&#30340;&#20219;&#21153;&#12290;&#28040;&#38500;&#23545;&#20154;&#31867;&#30452;&#35273;&#30340;&#20381;&#36182;&#26159;&#19968;&#20010;&#26497;&#20026;&#27963;&#36291;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290; &#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#20027;&#23398;&#20064;&#36890;&#29992;&#36923;&#36753;&#30456;&#20851;&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#35813;&#34920;&#31034;&#20174;&#26410;&#26631;&#35760;&#30340;&#39640;&#32500;&#23454;&#20540;&#26426;&#22120;&#20154;&#36712;&#36857;&#24320;&#22987;&#12290;&#25152;&#23398;&#34920;&#31034;&#26500;&#25104;&#20102;&#33258;&#21160;&#21457;&#26126;&#30340;&#31867;PDDL&#22495;&#27169;&#22411;&#12290;&#30830;&#23450;&#24615;&#35774;&#32622;&#19979;&#30340;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#20165;&#20174;&#23569;&#25968;&#26426;&#22120;&#20154;&#36712;&#36857;&#20013;&#21487;&#20197;&#23398;&#21040;&#24378;&#22823;&#30340;&#25277;&#35937;&#34920;&#31034;&#65307;&#25152;&#23398;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11871v1 Announce Type: cross  Abstract: Hand-crafted, logic-based state and action representations have been widely used to overcome the intractable computational complexity of long-horizon robot planning problems, including task and motion planning problems. However, creating such representations requires experts with strong intuitions and detailed knowledge about the robot and the tasks it may need to accomplish in a given setting. Removing this dependency on human intuition is a highly active research area.   This paper presents the first approach for autonomously learning generalizable, logic-based relational representations for abstract states and actions starting from unannotated high-dimensional, real-valued robot trajectories. The learned representations constitute auto-invented PDDL-like domain models. Empirical results in deterministic settings show that powerful abstract representations can be learned from just a handful of robot trajectories; the learned relation
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25193;&#25955;&#26725;&#29983;&#25104;&#27169;&#22411; Re-Dock&#65292;&#29992;&#20110;&#28789;&#27963;&#21644;&#29616;&#23454;&#30340;&#20998;&#23376;&#23545;&#25509;&#65292;&#36890;&#36807;&#33021;&#37327;&#21040;&#20960;&#20309;&#26144;&#23556;&#26469;&#20849;&#21516;&#24314;&#27169;&#32467;&#21512;&#33021;&#21644;&#26500;&#35937;&#65292;&#22635;&#34917;&#20102;&#23545;&#25509;&#20013;&#30340;&#23454;&#29992;&#24615;&#21644;&#26500;&#35937;&#39044;&#27979;&#26041;&#38754;&#30340;&#24046;&#36317;</title><link>https://arxiv.org/abs/2402.11459</link><description>&lt;p&gt;
Re-Dock: &#26397;&#21521;&#20855;&#26377;&#25193;&#25955;&#26725;&#30340;&#28789;&#27963;&#21644;&#29616;&#23454;&#20998;&#23376;&#23545;&#25509;
&lt;/p&gt;
&lt;p&gt;
Re-Dock: Towards Flexible and Realistic Molecular Docking with Diffusion Bridge
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11459
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25193;&#25955;&#26725;&#29983;&#25104;&#27169;&#22411; Re-Dock&#65292;&#29992;&#20110;&#28789;&#27963;&#21644;&#29616;&#23454;&#30340;&#20998;&#23376;&#23545;&#25509;&#65292;&#36890;&#36807;&#33021;&#37327;&#21040;&#20960;&#20309;&#26144;&#23556;&#26469;&#20849;&#21516;&#24314;&#27169;&#32467;&#21512;&#33021;&#21644;&#26500;&#35937;&#65292;&#22635;&#34917;&#20102;&#23545;&#25509;&#20013;&#30340;&#23454;&#29992;&#24615;&#21644;&#26500;&#35937;&#39044;&#27979;&#26041;&#38754;&#30340;&#24046;&#36317;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#39044;&#27979;&#34507;&#30333;&#36136;-&#37197;&#20307;&#32467;&#21512;&#32467;&#26500;&#65292;&#21363;&#20998;&#23376;&#23545;&#25509;&#20219;&#21153;&#23545;&#20110;&#33647;&#29289;&#35774;&#35745;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#23613;&#31649;&#28145;&#24230;&#23398;&#20064;&#26174;&#31034;&#20986;&#20102;&#28508;&#21147;&#65292;&#20294;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#23436;&#25972;&#34507;&#30333;&#36136;&#32467;&#26500;&#65288;&#23545;&#25509;&#65292;&#19988;&#22312;&#29616;&#23454;&#20219;&#21153;&#20013;&#19981;&#21487;&#36798;&#65289;&#25110;&#24573;&#30053;&#21475;&#34955;&#20391;&#38142;&#26500;&#35937;&#65292;&#23548;&#33268;&#26377;&#38480;&#30340;&#23454;&#29992;&#24615;&#21644;&#19981;&#20999;&#23454;&#38469;&#30340;&#26500;&#35937;&#39044;&#27979;&#12290;&#20026;&#22635;&#34917;&#36825;&#20123;&#24046;&#36317;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26410;&#32463;&#25506;&#32034;&#30340;&#20219;&#21153;&#65292;&#21629;&#21517;&#20026;&#26580;&#24615;&#23545;&#25509;&#65292;&#20197;&#21516;&#26102;&#39044;&#27979;&#37197;&#20307;&#21644;&#21475;&#34955;&#20391;&#38142;&#30340;&#23039;&#21183;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#25193;&#23637;&#21040;&#20960;&#20309;&#27969;&#24418;&#30340;&#26032;&#22411;&#25193;&#25955;&#26725;&#29983;&#25104;&#27169;&#22411; Re-Dock&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21463;&#29275;&#39039;-&#27431;&#25289;&#26041;&#31243;&#21551;&#21457;&#30340;&#33021;&#37327;&#21040;&#20960;&#20309;&#26144;&#23556;&#65292;&#20197;&#20849;&#21516;&#24314;&#27169;&#32467;&#21512;&#33021;&#21644;&#26500;&#35937;&#65292;&#20197;&#21453;&#26144;&#33021;&#37327;&#32422;&#26463;&#23545;&#25509;&#29983;&#25104;&#36807;&#31243;&#12290;&#25105;&#20204;&#22312;&#35774;&#35745;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23454;&#39564;&#65292;&#21253;&#25324;apo-dock&#21644;cross-dock d
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11459v1 Announce Type: cross  Abstract: Accurate prediction of protein-ligand binding structures, a task known as molecular docking is crucial for drug design but remains challenging. While deep learning has shown promise, existing methods often depend on holo-protein structures (docked, and not accessible in realistic tasks) or neglect pocket sidechain conformations, leading to limited practical utility and unrealistic conformation predictions. To fill these gaps, we introduce an under-explored task, named flexible docking to predict poses of ligand and pocket sidechains simultaneously and introduce Re-Dock, a novel diffusion bridge generative model extended to geometric manifolds. Specifically, we propose energy-to-geometry mapping inspired by the Newton-Euler equation to co-model the binding energy and conformations for reflecting the energy-constrained docking generative process. Comprehensive experiments on designed benchmark datasets including apo-dock and cross-dock d
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#24037;&#20855;&#22686;&#24378;&#22411;&#31185;&#23398;&#25512;&#29702;&#30340;&#26032;&#20219;&#21153;&#35774;&#32622;&#65292;&#36890;&#36807;&#25552;&#20379;&#21487;&#25193;&#23637;&#30340;&#24037;&#20855;&#38598;&#65292;&#24110;&#21161;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#31185;&#23398;&#38382;&#39064;&#35299;&#20915;&#20013;&#21464;&#24471;&#26356;&#21152;&#23454;&#29992;&#21644;&#21487;&#35299;&#20915;&#12290;</title><link>https://arxiv.org/abs/2402.11451</link><description>&lt;p&gt;
SciAgent: &#24037;&#20855;&#22686;&#24378;&#22411;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#31185;&#23398;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
SciAgent: Tool-augmented Language Models for Scientific Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11451
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#24037;&#20855;&#22686;&#24378;&#22411;&#31185;&#23398;&#25512;&#29702;&#30340;&#26032;&#20219;&#21153;&#35774;&#32622;&#65292;&#36890;&#36807;&#25552;&#20379;&#21487;&#25193;&#23637;&#30340;&#24037;&#20855;&#38598;&#65292;&#24110;&#21161;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#31185;&#23398;&#38382;&#39064;&#35299;&#20915;&#20013;&#21464;&#24471;&#26356;&#21152;&#23454;&#29992;&#21644;&#21487;&#35299;&#20915;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31185;&#23398;&#25512;&#29702;&#23545;&#20110;&#21363;&#20351;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#35828;&#20063;&#26159;&#19968;&#39033;&#24040;&#22823;&#25361;&#25112;&#12290;&#20026;&#20102;&#20351;LLMs&#26356;&#21152;&#23454;&#29992;&#21644;&#21487;&#35299;&#20915;&#27492;&#20219;&#21153;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;&#24037;&#20855;&#22686;&#24378;&#22411;&#31185;&#23398;&#25512;&#29702;&#30340;&#26032;&#20219;&#21153;&#35774;&#32622;&#12290;&#36825;&#31181;&#35774;&#32622;&#36890;&#36807;&#20026;LLMs&#25552;&#20379;&#21487;&#25193;&#23637;&#30340;&#24037;&#20855;&#38598;&#65292;&#23558;&#37325;&#28857;&#20174;&#36861;&#27714;&#20840;&#30693;&#38382;&#39064;&#27714;&#35299;&#22120;&#36716;&#21464;&#20026;&#29087;&#32451;&#20351;&#29992;&#24037;&#20855;&#30340;&#20154;&#12290;&#20026;&#20102;&#20419;&#36827;&#36825;&#31181;&#35774;&#32622;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#21517;&#20026;MathFunc&#30340;&#24037;&#20855;&#22686;&#24378;&#22411;&#35757;&#32451;&#35821;&#26009;&#24211;&#65292;&#28085;&#30422;&#20102;&#36229;&#36807;30,000&#20010;&#26679;&#26412;&#21644;&#22823;&#32422;6,000&#20010;&#24037;&#20855;&#12290;&#22522;&#20110;MathFunc&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;SciAgent&#65292;&#29992;&#20110;&#26816;&#32034;&#12289;&#29702;&#35299;&#65292;&#20197;&#21450;&#24517;&#35201;&#26102;&#20351;&#29992;&#24037;&#20855;&#36827;&#34892;&#31185;&#23398;&#38382;&#39064;&#35299;&#20915;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#21517;&#20026;SciToolBench&#30340;&#22522;&#20934;&#65292;&#28085;&#30422;&#20116;&#20010;&#31185;&#23398;&#39046;&#22495;&#65292;&#20197;&#35780;&#20272;LLMs&#22312;&#24037;&#20855;&#36741;&#21161;&#19979;&#30340;&#33021;&#21147;&#12290;&#23545;SciToolBench&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#39564;&#35777;&#20102;SciAgent&#30340;&#26377;&#25928;&#24615;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;SciAgent-Mistral-7B&#36229;&#36807;&#20102;&#20854;&#20182;LLMs&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11451v1 Announce Type: cross  Abstract: Scientific reasoning poses an excessive challenge for even the most advanced Large Language Models (LLMs). To make this task more practical and solvable for LLMs, we introduce a new task setting named tool-augmented scientific reasoning. This setting supplements LLMs with scalable toolsets, and shifts the focus from pursuing an omniscient problem solver to a proficient tool-user. To facilitate the research of such setting, we construct a tool-augmented training corpus named MathFunc which encompasses over 30,000 samples and roughly 6,000 tools. Building on MathFunc, we develop SciAgent to retrieve, understand and, if necessary, use tools for scientific problem solving. Additionally, we craft a benchmark, SciToolBench, spanning five scientific domains to evaluate LLMs' abilities with tool assistance. Extensive experiments on SciToolBench confirm the effectiveness of SciAgent. Notably, SciAgent-Mistral-7B surpasses other LLMs with the sa
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21033;&#29992;LLM&#22686;&#24378;&#35821;&#20041;&#20998;&#26512;&#65292;&#24320;&#21457;&#20102;&#29992;&#20110;&#25991;&#26412;&#30340;&#30456;&#20284;&#24230;&#24230;&#37327;&#26694;&#26550;&#65292;&#21487;&#26174;&#33879;&#25913;&#21892;&#25991;&#26412;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#35780;&#20272;&#65292;&#24182;&#21487;&#25193;&#23637;&#21040;&#20854;&#20182;&#19987;&#19994;&#39046;&#22495;&#12290;</title><link>https://arxiv.org/abs/2402.11398</link><description>&lt;p&gt;
&#22312;&#27604;&#36739;&#20043;&#21069;&#36827;&#34892;&#25512;&#29702;&#65306;LLM&#22686;&#24378;&#30340;&#35821;&#20041;&#30456;&#20284;&#24230;&#24230;&#37327;&#29992;&#20110;&#39046;&#22495;&#19987;&#38376;&#25991;&#26412;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Reasoning before Comparison: LLM-Enhanced Semantic Similarity Metrics for Domain Specialized Text Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11398
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;LLM&#22686;&#24378;&#35821;&#20041;&#20998;&#26512;&#65292;&#24320;&#21457;&#20102;&#29992;&#20110;&#25991;&#26412;&#30340;&#30456;&#20284;&#24230;&#24230;&#37327;&#26694;&#26550;&#65292;&#21487;&#26174;&#33879;&#25913;&#21892;&#25991;&#26412;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#35780;&#20272;&#65292;&#24182;&#21487;&#25193;&#23637;&#21040;&#20854;&#20182;&#19987;&#19994;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;LLM&#26469;&#22686;&#24378;&#35821;&#20041;&#20998;&#26512;&#65292;&#20026;&#25991;&#26412;&#24320;&#21457;&#30456;&#20284;&#24230;&#24230;&#37327;&#65292;&#35299;&#20915;&#20256;&#32479;&#26080;&#30417;&#30563;NLP&#24230;&#37327;&#65288;&#22914;ROUGE&#21644;BLEU&#65289;&#30340;&#23616;&#38480;&#24615;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#20854;&#20013;LLM&#65288;&#20363;&#22914;GPT-4&#65289;&#29992;&#20110;&#38646;&#26679;&#26412;&#25991;&#26412;&#35782;&#21035;&#21644;&#25918;&#23556;&#23398;&#25253;&#21578;&#30340;&#26631;&#31614;&#29983;&#25104;&#65292;&#22312;&#37027;&#37324;&#36825;&#20123;&#26631;&#31614;&#28982;&#21518;&#34987;&#29992;&#20316;&#25991;&#26412;&#30456;&#20284;&#24615;&#30340;&#24230;&#37327;&#12290;&#36890;&#36807;&#22312;MIMIC&#25968;&#25454;&#38598;&#19978;&#27979;&#35797;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#65292;&#25105;&#20204;&#21457;&#29616;GPT-4&#29983;&#25104;&#30340;&#26631;&#31614;&#33021;&#22815;&#26174;&#33879;&#25913;&#21892;&#35821;&#20041;&#30456;&#20284;&#24230;&#35780;&#20272;&#65292;&#24471;&#20998;&#26356;&#25509;&#36817;&#20020;&#24202;&#23454;&#38469;&#24773;&#20917;&#27604;&#20256;&#32479;NLP&#24230;&#37327;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#23637;&#31034;&#20102;&#20351;&#29992;LLM&#36827;&#34892;&#39640;&#24230;&#19987;&#19994;&#39046;&#22495;&#30340;&#25991;&#26412;&#25968;&#25454;&#30340;&#35821;&#20041;&#20998;&#26512;&#30340;&#21487;&#33021;&#24615;&#65292;&#20855;&#26377;&#21322;&#23450;&#37327;&#25512;&#29702;&#32467;&#26524;&#12290;&#34429;&#28982;&#35813;&#26694;&#26550;&#38024;&#23545;&#25918;&#23556;&#23398;&#25253;&#21578;&#30456;&#20284;&#24615;&#20998;&#26512;&#36827;&#34892;&#20102;&#23454;&#26045;&#65292;&#20294;&#20854;&#27010;&#24565;&#21487;&#20197;&#25193;&#23637;&#21040;&#20854;&#20182;&#19987;&#38376;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11398v1 Announce Type: cross  Abstract: In this study, we leverage LLM to enhance the semantic analysis and develop similarity metrics for texts, addressing the limitations of traditional unsupervised NLP metrics like ROUGE and BLEU. We develop a framework where LLMs such as GPT-4 are employed for zero-shot text identification and label generation for radiology reports, where the labels are then used as measurements for text similarity. By testing the proposed framework on the MIMIC data, we find that GPT-4 generated labels can significantly improve the semantic similarity assessment, with scores more closely aligned with clinical ground truth than traditional NLP metrics. Our work demonstrates the possibility of conducting semantic analysis of the text data using semi-quantitative reasoning results by the LLMs for highly specialized domains. While the framework is implemented for radiology report similarity analysis, its concept can be extended to other specialized domains 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#40657;&#30418;&#27010;&#29575;&#35748;&#35777;&#35299;&#37322;&#30340;&#20449;&#20219;&#21306;&#22495;&#33021;&#22815;&#26377;&#25928;&#22320;&#27934;&#23519;&#27169;&#22411;&#34892;&#20026;&#12289;&#20445;&#35777;&#35299;&#37322;&#30340;&#31283;&#23450;&#24615;&#65292;&#24182;&#23454;&#29616;&#35299;&#37322;&#30340;&#37325;&#29992;</title><link>https://arxiv.org/abs/2402.11168</link><description>&lt;p&gt;
&#22522;&#20110;&#20449;&#20219;&#21306;&#22495;&#30340;&#40657;&#30418;&#27010;&#29575;&#35748;&#35777;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Trust Regions for Explanations via Black-Box Probabilistic Certification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11168
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#40657;&#30418;&#27010;&#29575;&#35748;&#35777;&#35299;&#37322;&#30340;&#20449;&#20219;&#21306;&#22495;&#33021;&#22815;&#26377;&#25928;&#22320;&#27934;&#23519;&#27169;&#22411;&#34892;&#20026;&#12289;&#20445;&#35777;&#35299;&#37322;&#30340;&#31283;&#23450;&#24615;&#65292;&#24182;&#23454;&#29616;&#35299;&#37322;&#30340;&#37325;&#29992;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#40657;&#30418;&#24615;&#36136;&#65292;&#20154;&#20204;&#24320;&#21457;&#20102;&#22823;&#37327;&#30340;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#26469;&#35299;&#26512;&#20010;&#21035;&#20915;&#31574;&#32972;&#21518;&#30340;&#22240;&#32032;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#40657;&#30418;&#65288;&#27010;&#29575;&#24615;&#65289;&#35299;&#37322;&#35748;&#35777;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#38382;&#39064;&#65306;&#32473;&#23450;&#19968;&#20010;&#40657;&#30418;&#27169;&#22411;&#65292;&#21482;&#26377;&#26597;&#35810;&#35775;&#38382;&#26435;&#65292;&#19968;&#20010;&#31034;&#20363;&#30340;&#35299;&#37322;&#20197;&#21450;&#19968;&#20010;&#36136;&#37327;&#24230;&#37327;&#65288;&#22914;&#36924;&#30495;&#24230;&#12289;&#31283;&#23450;&#24615;&#65289;&#65292;&#25105;&#20204;&#26159;&#21542;&#33021;&#25214;&#21040;&#26368;&#22823;&#30340;&#36229;&#31435;&#26041;&#20307;&#65288;&#21363; $\ell_{\infty}$ &#29699;&#65289;&#65292;&#20197;&#31034;&#20363;&#20026;&#20013;&#24515;&#65292;&#20351;&#24471;&#24403;&#35299;&#37322;&#34987;&#24212;&#29992;&#20110;&#36229;&#31435;&#26041;&#20307;&#20869;&#30340;&#25152;&#26377;&#31034;&#20363;&#26102;&#65288;&#39640;&#27010;&#29575;&#19979;&#65289;&#36136;&#37327;&#26631;&#20934;&#24471;&#21040;&#28385;&#36275;&#65288;&#27604;&#22914;&#36924;&#30495;&#24230;&#39640;&#20110;&#26576;&#20010;&#20540;&#65289;&#65311;&#33021;&#22815;&#39640;&#25928;&#22320;&#25214;&#21040;&#36825;&#26679;&#19968;&#20010;&#20449;&#20219;&#21306;&#22495;&#26377;&#22810;&#37325;&#22909;&#22788;&#65306;i&#65289;&#27934;&#23519;&#27169;&#22411;&#22312;&#19968;&#20010;&#21306;&#22495;&#20869;&#30340;&#34892;&#20026;&#65292;&#20855;&#26377;&#20445;&#35777;&#65307;ii&#65289;&#35299;&#37322;&#30340;&#31283;&#23450;&#24615;&#24471;&#21040;&#20445;&#35777;&#65307;iii&#65289;&#35299;&#37322;&#30340;&#37325;&#29992;&#65292;&#21487;&#20197;&#33410;&#30465;&#26102;&#38388;&#12289;&#31934;&#21147;&#21644;&#37329;&#38065;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11168v1 Announce Type: cross  Abstract: Given the black box nature of machine learning models, a plethora of explainability methods have been developed to decipher the factors behind individual decisions. In this paper, we introduce a novel problem of black box (probabilistic) explanation certification. We ask the question: Given a black box model with only query access, an explanation for an example and a quality metric (viz. fidelity, stability), can we find the largest hypercube (i.e., $\ell_{\infty}$ ball) centered at the example such that when the explanation is applied to all examples within the hypercube, (with high probability) a quality criterion is met (viz. fidelity greater than some value)? Being able to efficiently find such a \emph{trust region} has multiple benefits: i) insight into model behavior in a \emph{region}, with a \emph{guarantee}; ii) ascertained \emph{stability} of the explanation; iii) \emph{explanation reuse}, which can save time, energy and mone
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#36129;&#29486;&#30340;&#24322;&#27493;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#21160;&#24577;&#35843;&#25972;&#25509;&#25910;&#21040;&#30340;&#26356;&#26032;&#30340;&#22788;&#29702;&#26041;&#24335;&#65292;&#20197;&#35299;&#20915;&#29616;&#23454;&#24773;&#20917;&#19979;&#21516;&#27493;&#19978;&#20256;&#25968;&#25454;&#21487;&#33021;&#20986;&#29616;&#30340;&#32531;&#24930;&#21644;&#19981;&#21487;&#38752;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.10991</link><description>&lt;p&gt;
&#21152;&#36895;&#21322;&#24322;&#27493;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Accelerating Semi-Asynchronous Federated Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10991
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#36129;&#29486;&#30340;&#24322;&#27493;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#21160;&#24577;&#35843;&#25972;&#25509;&#25910;&#21040;&#30340;&#26356;&#26032;&#30340;&#22788;&#29702;&#26041;&#24335;&#65292;&#20197;&#35299;&#20915;&#29616;&#23454;&#24773;&#20917;&#19979;&#21516;&#27493;&#19978;&#20256;&#25968;&#25454;&#21487;&#33021;&#20986;&#29616;&#30340;&#32531;&#24930;&#21644;&#19981;&#21487;&#38752;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26159;&#19968;&#31181;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#33539;&#20363;&#65292;&#20801;&#35768;&#23458;&#25143;&#31471;&#22312;&#20445;&#25252;&#38544;&#31169;&#30340;&#21516;&#26102;&#22312;&#20854;&#25968;&#25454;&#19978;&#35757;&#32451;&#27169;&#22411;&#12290;&#29616;&#26377;&#30340;FL&#31639;&#27861;&#65292;&#22914;Federated Averaging&#65288;FedAvg&#65289;&#21450;&#20854;&#21464;&#31181;&#65292;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#24050;&#32463;&#34987;&#35777;&#26126;&#25910;&#25947;&#33391;&#22909;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#38656;&#35201;&#23458;&#25143;&#31471;&#20197;&#21516;&#27493;&#26041;&#24335;&#23558;&#20854;&#26412;&#22320;&#26356;&#26032;&#19978;&#20256;&#33267;&#26381;&#21153;&#22120;&#65292;&#36825;&#22312;&#29616;&#23454;&#24773;&#20917;&#19979;&#21487;&#33021;&#20250;&#21464;&#24471;&#32531;&#24930;&#21644;&#19981;&#21487;&#38752;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#30740;&#31350;&#20154;&#21592;&#24320;&#21457;&#20102;&#24322;&#27493;FL&#26041;&#27861;&#65292;&#20801;&#35768;&#23458;&#25143;&#31471;&#32487;&#32493;&#20351;&#29992;&#38472;&#26087;&#30340;&#20840;&#23616;&#27169;&#22411;&#23545;&#20854;&#26412;&#22320;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#36825;&#20123;&#26041;&#27861;&#20165;&#20165;&#32858;&#21512;&#20102;&#25152;&#26377;&#25509;&#25910;&#21040;&#30340;&#26356;&#26032;&#65292;&#32780;&#27809;&#26377;&#32771;&#34385;&#20854;&#30456;&#23545;&#36129;&#29486;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#25910;&#25947;&#36895;&#24230;&#21464;&#24930;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#36129;&#29486;&#30340;&#24322;&#27493;FL&#26041;&#27861;&#65292;&#32771;&#34385;&#20102;&#25509;&#25910;&#21040;&#30340;&#26356;&#26032;&#30340;&#38472;&#26087;&#31243;&#24230;&#21644;&#32479;&#35745;&#24322;&#36136;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21160;&#24577;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10991v1 Announce Type: cross  Abstract: Federated Learning (FL) is a distributed machine learning paradigm that allows clients to train models on their data while preserving their privacy. FL algorithms, such as Federated Averaging (FedAvg) and its variants, have been shown to converge well in many scenarios. However, these methods require clients to upload their local updates to the server in a synchronous manner, which can be slow and unreliable in realistic FL settings. To address this issue, researchers have developed asynchronous FL methods that allow clients to continue training on their local data using a stale global model. However, most of these methods simply aggregate all of the received updates without considering their relative contributions, which can slow down convergence. In this paper, we propose a contribution-aware asynchronous FL method that takes into account the staleness and statistical heterogeneity of the received updates. Our method dynamically adju
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#19982;&#37327;&#23376;&#21270;&#23398;&#21453;&#39304;&#30456;&#32467;&#21512;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;AI&#24341;&#23548;&#30340;&#35745;&#31639;&#31579;&#36873;&#26694;&#26550;&#65292;&#23558;&#20652;&#21270;&#21058;&#21457;&#29616;&#24418;&#24335;&#21270;&#20026;&#19968;&#20010;&#19981;&#30830;&#23450;&#29615;&#22659;&#65292;&#20174;&#32780;&#23454;&#29616;&#39640;&#25928;&#20652;&#21270;&#21058;&#30340;&#31215;&#26497;&#25628;&#32034;</title><link>https://arxiv.org/abs/2402.10980</link><description>&lt;p&gt;
CHEMREASONER&#65306;&#20351;&#29992;&#37327;&#23376;&#21270;&#23398;&#21453;&#39304;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#31354;&#38388;&#20013;&#36827;&#34892;&#21551;&#21457;&#24335;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
CHEMREASONER: Heuristic Search over a Large Language Model's Knowledge Space using Quantum-Chemical Feedback
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10980
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#19982;&#37327;&#23376;&#21270;&#23398;&#21453;&#39304;&#30456;&#32467;&#21512;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;AI&#24341;&#23548;&#30340;&#35745;&#31639;&#31579;&#36873;&#26694;&#26550;&#65292;&#23558;&#20652;&#21270;&#21058;&#21457;&#29616;&#24418;&#24335;&#21270;&#20026;&#19968;&#20010;&#19981;&#30830;&#23450;&#29615;&#22659;&#65292;&#20174;&#32780;&#23454;&#29616;&#39640;&#25928;&#20652;&#21270;&#21058;&#30340;&#31215;&#26497;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10980v1 &#31867;&#22411;&#20844;&#21578;&#65306;&#36328;&#39046;&#22495; &#25688;&#35201;&#65306;&#21457;&#29616;&#26032;&#30340;&#20652;&#21270;&#21058;&#23545;&#20110;&#35774;&#35745;&#26032;&#30340;&#26356;&#39640;&#25928;&#30340;&#21270;&#23398;&#36807;&#31243;&#33267;&#20851;&#37325;&#35201;&#65292;&#20197;&#23454;&#29616;&#21521;&#21487;&#25345;&#32493;&#26410;&#26469;&#30340;&#36807;&#28193;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#20154;&#24037;&#26234;&#33021;&#24341;&#23548;&#30340;&#35745;&#31639;&#31579;&#36873;&#26694;&#26550;&#65292;&#23558;&#35821;&#35328;&#25512;&#29702;&#19982;&#22522;&#20110;&#37327;&#23376;&#21270;&#23398;&#30340;&#19977;&#32500;&#21407;&#23376;&#34920;&#31034;&#30340;&#21453;&#39304;&#32479;&#19968;&#36215;&#26469;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#20652;&#21270;&#21058;&#21457;&#29616;&#26500;&#24314;&#20026;&#19968;&#20010;&#19981;&#30830;&#23450;&#29615;&#22659;&#65292;&#20854;&#20013;&#19968;&#20010;&#20195;&#29702;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#25512;&#23548;&#30340;&#20551;&#35774;&#19982;&#22522;&#20110;&#21407;&#23376;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#21453;&#39304;&#30340;&#36845;&#20195;&#32452;&#21512;&#65292;&#31215;&#26497;&#25628;&#32034;&#39640;&#25928;&#20652;&#21270;&#21058;&#12290;&#22312;&#20013;&#38388;&#25628;&#32034;&#27493;&#39588;&#30830;&#23450;&#30340;&#20652;&#21270;&#21058;&#32463;&#36807;&#22522;&#20110;&#31354;&#38388;&#23450;&#21521;&#12289;&#21453;&#24212;&#36884;&#24452;&#21644;&#31283;&#23450;&#24615;&#30340;&#32467;&#26500;&#35780;&#20272;&#12290;&#22522;&#20110;&#21560;&#38468;&#33021;&#21644;&#21183;&#22418;&#30340;&#35780;&#20998;&#20989;&#25968;&#24341;&#23548;&#22312;LLM&#30340;&#30693;&#35782;&#31354;&#38388;&#20013;&#21521;&#33021;&#37327;&#26377;&#21033;&#12289;&#39640;&#25928;&#30340;&#20652;&#21270;&#21058;&#25506;&#32034;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#21487;&#20197;&#33258;&#21160;&#35268;&#21010;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10980v1 Announce Type: cross  Abstract: The discovery of new catalysts is essential for the design of new and more efficient chemical processes in order to transition to a sustainable future. We introduce an AI-guided computational screening framework unifying linguistic reasoning with quantum-chemistry based feedback from 3D atomistic representations. Our approach formulates catalyst discovery as an uncertain environment where an agent actively searches for highly effective catalysts via the iterative combination of large language model (LLM)-derived hypotheses and atomistic graph neural network (GNN)-derived feedback. Identified catalysts in intermediate search steps undergo structural evaluation based on spatial orientation, reaction pathways, and stability. Scoring functions based on adsorption energies and barriers steer the exploration in the LLM's knowledge space toward energetically favorable, high-efficiency catalysts. We introduce planning methods that automaticall
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#24490;&#29615;&#35760;&#24518;&#22686;&#24378;&#23545; GPT-2 &#36827;&#34892;&#24494;&#35843;&#65292;&#20351;&#20854;&#33021;&#22815;&#22788;&#29702;&#38271;&#36798; 1000 &#19975;&#20010;&#20803;&#32032;&#30340;&#20219;&#21153;&#65292;&#36825;&#26159;&#36804;&#20170;&#20026;&#27490;&#22788;&#29702;&#26368;&#38271;&#36755;&#20837;&#30340;&#24320;&#25918;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#24182;&#23637;&#31034;&#20102;&#23545;&#38271;&#24207;&#21015;&#22788;&#29702;&#33021;&#21147;&#30340;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.10790</link><description>&lt;p&gt;
&#22312;&#19968;&#20010; 1000 &#19975;&#26681;&#33609;&#22427;&#20013;&#23547;&#25214;&#38024;&#65306;&#24490;&#29615;&#35760;&#24518;&#25214;&#21040;&#20102;&#35821;&#35328;&#27169;&#22411;&#19981;&#25797;&#38271;&#30340;&#20869;&#23481;
&lt;/p&gt;
&lt;p&gt;
In Search of Needles in a 10M Haystack: Recurrent Memory Finds What LLMs Miss
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10790
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#24490;&#29615;&#35760;&#24518;&#22686;&#24378;&#23545; GPT-2 &#36827;&#34892;&#24494;&#35843;&#65292;&#20351;&#20854;&#33021;&#22815;&#22788;&#29702;&#38271;&#36798; 1000 &#19975;&#20010;&#20803;&#32032;&#30340;&#20219;&#21153;&#65292;&#36825;&#26159;&#36804;&#20170;&#20026;&#27490;&#22788;&#29702;&#26368;&#38271;&#36755;&#20837;&#30340;&#24320;&#25918;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#24182;&#23637;&#31034;&#20102;&#23545;&#38271;&#24207;&#21015;&#22788;&#29702;&#33021;&#21147;&#30340;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#20351;&#29992;&#29983;&#25104;&#24335; Transformer &#27169;&#22411;&#22788;&#29702;&#38271;&#25991;&#26723;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#35780;&#20272;&#19981;&#21516;&#26041;&#27861;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102; BABILong&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#65292;&#26088;&#22312;&#35780;&#20272;&#27169;&#22411;&#22312;&#25552;&#21462;&#21644;&#22788;&#29702;&#24191;&#27867;&#25991;&#26412;&#20013;&#20998;&#24067;&#24335;&#20107;&#23454;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#21253;&#25324; GPT-4 &#21644; RAG &#30340;&#22522;&#20934;&#65292;&#32467;&#26524;&#26174;&#31034;&#24120;&#35265;&#26041;&#27861;&#20165;&#36866;&#29992;&#20110;&#26368;&#22810; $10^4$ &#20010;&#20803;&#32032;&#30340;&#24207;&#21015;&#12290;&#30456;&#21453;&#65292;&#36890;&#36807;&#20351;&#29992;&#24490;&#29615;&#35760;&#24518;&#22686;&#24378;&#23545; GPT-2 &#36827;&#34892;&#24494;&#35843;&#65292;&#20351;&#20854;&#33021;&#22815;&#22788;&#29702;&#28041;&#21450;&#26368;&#22810; $10^7$ &#20010;&#20803;&#32032;&#30340;&#20219;&#21153;&#12290;&#36825;&#19968;&#25104;&#23601;&#26631;&#24535;&#30528;&#36804;&#20170;&#20026;&#27490;&#20219;&#20309;&#24320;&#28304;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#22788;&#29702;&#30340;&#26368;&#38271;&#36755;&#20837;&#65292;&#26174;&#31034;&#20102;&#23545;&#38271;&#24207;&#21015;&#22788;&#29702;&#33021;&#21147;&#30340;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10790v1 Announce Type: cross  Abstract: This paper addresses the challenge of processing long documents using generative transformer models. To evaluate different approaches, we introduce BABILong, a new benchmark designed to assess model capabilities in extracting and processing distributed facts within extensive texts. Our evaluation, which includes benchmarks for GPT-4 and RAG, reveals that common methods are effective only for sequences up to $10^4$ elements. In contrast, fine-tuning GPT-2 with recurrent memory augmentations enables it to handle tasks involving up to $10^7$ elements. This achievement marks a substantial leap, as it is by far the longest input processed by any open neural network model to date, demonstrating a significant improvement in the processing capabilities for long sequences.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22312;&#21360;&#24230;&#27861;&#24459;&#39046;&#22495;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22788;&#29702;&#31038;&#20250;&#22240;&#32032;&#26102;&#30340;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#32467;&#21512;&#20844;&#24179;&#24615;&#21644;&#20934;&#30830;&#24615;&#30340;&#26032;&#25351;&#26631;$LSS_{\beta}$&#65292;&#24182;&#35780;&#20272;&#20102;&#27169;&#22411;&#22312;&#20108;&#20803;&#27861;&#24459;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#20197;&#21450;&#22312;&#21360;&#24230;&#31038;&#20250;&#21508;&#31181;&#19981;&#24179;&#31561;&#26041;&#38754;&#30340;&#20844;&#24179;&#24615;&#23637;&#31034;&#12290;</title><link>https://arxiv.org/abs/2402.10567</link><description>&lt;p&gt;
&#22312;InSaAF&#20013;&#34701;&#20837;&#23433;&#20840;&#24615;&#65292;&#36890;&#36807;&#20934;&#30830;&#24615;&#21644;&#20844;&#24179;&#24615; | LLM&#26159;&#21542;&#24050;&#32463;&#20934;&#22791;&#22909;&#36827;&#20837;&#21360;&#24230;&#27861;&#24459;&#39046;&#22495;&#65311;
&lt;/p&gt;
&lt;p&gt;
InSaAF: Incorporating Safety through Accuracy and Fairness | Are LLMs ready for the Indian Legal Domain?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10567
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22312;&#21360;&#24230;&#27861;&#24459;&#39046;&#22495;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22788;&#29702;&#31038;&#20250;&#22240;&#32032;&#26102;&#30340;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#32467;&#21512;&#20844;&#24179;&#24615;&#21644;&#20934;&#30830;&#24615;&#30340;&#26032;&#25351;&#26631;$LSS_{\beta}$&#65292;&#24182;&#35780;&#20272;&#20102;&#27169;&#22411;&#22312;&#20108;&#20803;&#27861;&#24459;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#20197;&#21450;&#22312;&#21360;&#24230;&#31038;&#20250;&#21508;&#31181;&#19981;&#24179;&#31561;&#26041;&#38754;&#30340;&#20844;&#24179;&#24615;&#23637;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#25216;&#26415;&#21644;&#20154;&#24037;&#26234;&#33021;&#30340;&#26368;&#26032;&#36827;&#23637;&#24050;&#32463;&#23548;&#33268;&#25552;&#20986;&#20102;&#20247;&#22810;&#35821;&#35328;&#27169;&#22411;&#65292;&#29992;&#20110;&#25191;&#34892;&#27861;&#24459;&#39046;&#22495;&#30340;&#21508;&#31181;&#20219;&#21153;&#65292;&#20174;&#39044;&#27979;&#21028;&#20915;&#21040;&#29983;&#25104;&#25688;&#35201;&#12290;&#23613;&#31649;&#23427;&#20204;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#65292;&#20294;&#24050;&#32463;&#35777;&#26126;&#36825;&#20123;&#27169;&#22411;&#23398;&#20064;&#24182;&#23637;&#31034;&#31038;&#20250;&#20559;&#35265;&#65292;&#24182;&#20570;&#20986;&#19981;&#20844;&#24179;&#30340;&#39044;&#27979;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#24403;&#28041;&#21450;&#31038;&#20250;&#22240;&#32032;&#26102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21360;&#24230;&#27861;&#24459;&#39046;&#22495;&#25191;&#34892;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;$\beta$-&#21152;&#26435;&#30340;$\textit{&#27861;&#24459;&#23433;&#20840;&#20998;&#25968;($LSS_{\beta}$)}$&#65292;&#23558;LLM&#30340;&#20844;&#24179;&#24615;&#21644;&#20934;&#30830;&#24615;&#20004;&#20010;&#26041;&#38754;&#32467;&#21512;&#36215;&#26469;&#12290;&#25105;&#20204;&#36890;&#36807;&#32771;&#34385;LLM&#22312;$\textit{&#20108;&#20803;&#27861;&#24459;&#25512;&#29702;}$&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#20197;&#21450;&#20854;&#22312;&#21360;&#24230;&#31038;&#20250;&#21508;&#31181;&#19981;&#24179;&#31561;&#26041;&#38754;&#30340;&#20844;&#24179;&#23637;&#31034;&#26469;&#35780;&#20272;LLMs&#30340;&#23433;&#20840;&#24615;&#12290;LLaMA&#21644;LLaMA--2&#27169;&#22411;&#30340;&#20219;&#21153;&#34920;&#29616;&#21644;&#20844;&#24179;&#24471;&#20998;&#34920;&#26126;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10567v1 Announce Type: cross  Abstract: Recent advancements in language technology and Artificial Intelligence have resulted in numerous Language Models being proposed to perform various tasks in the legal domain ranging from predicting judgments to generating summaries. Despite their immense potential, these models have been proven to learn and exhibit societal biases and make unfair predictions. In this study, we explore the ability of Large Language Models (LLMs) to perform legal tasks in the Indian landscape when social factors are involved. We present a novel metric, $\beta$-weighted $\textit{Legal Safety Score ($LSS_{\beta}$)}$, which encapsulates both the fairness and accuracy aspects of the LLM. We assess LLMs' safety by considering its performance in the $\textit{Binary Statutory Reasoning}$ task and its fairness exhibition with respect to various axes of disparities in the Indian society. Task performance and fairness scores of LLaMA and LLaMA--2 models indicate th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20998;&#24067;&#20559;&#22909;&#22870;&#21169;&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#22810;&#26679;&#30340;&#20154;&#31867;&#20559;&#22909;&#23545;&#40784;&#12290;&#35813;&#26694;&#26550;&#20351;&#29992;&#36125;&#22612;&#20998;&#24067;&#21051;&#30011;&#20559;&#22909;&#65292;&#24182;&#35774;&#35745;&#20102;&#22522;&#20110;&#26368;&#20248;&#36755;&#36816;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#26657;&#20934;&#27169;&#22411;&#19982;&#20559;&#22909;&#30340;&#23545;&#40784;&#31243;&#24230;&#12290;&#26368;&#32456;&#21033;&#29992;&#26399;&#26395;&#22870;&#21169;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#30340;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2402.09764</link><description>&lt;p&gt;
&#36890;&#36807;&#20998;&#24067;&#20559;&#22909;&#22870;&#21169;&#24314;&#27169;&#23545;&#40784;&#20247;&#21253;&#21453;&#39304;
&lt;/p&gt;
&lt;p&gt;
Aligning Crowd Feedback via Distributional Preference Reward Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09764
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20998;&#24067;&#20559;&#22909;&#22870;&#21169;&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#22810;&#26679;&#30340;&#20154;&#31867;&#20559;&#22909;&#23545;&#40784;&#12290;&#35813;&#26694;&#26550;&#20351;&#29992;&#36125;&#22612;&#20998;&#24067;&#21051;&#30011;&#20559;&#22909;&#65292;&#24182;&#35774;&#35745;&#20102;&#22522;&#20110;&#26368;&#20248;&#36755;&#36816;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#26657;&#20934;&#27169;&#22411;&#19982;&#20559;&#22909;&#30340;&#23545;&#40784;&#31243;&#24230;&#12290;&#26368;&#32456;&#21033;&#29992;&#26399;&#26395;&#22870;&#21169;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#24191;&#27867;&#29992;&#20110;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#31867;&#20559;&#22909;&#23545;&#40784;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#22870;&#21169;&#24314;&#27169;&#20027;&#35201;&#20381;&#36182;&#20110;&#19968;&#32452;&#20010;&#20307;&#25552;&#20379;&#30340;&#20154;&#31867;&#26631;&#27880;&#12290;&#36825;&#31181;&#20381;&#36182;&#21487;&#33021;&#20250;&#23548;&#33268;&#27169;&#22411;&#20542;&#21521;&#20110;&#21453;&#26144;&#36825;&#20123;&#26631;&#27880;&#32773;&#30340;&#20542;&#21521;&#65292;&#20174;&#32780;&#26410;&#33021;&#20805;&#20998;&#20195;&#34920;&#26356;&#24191;&#27867;&#20154;&#32676;&#30340;&#26399;&#26395;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26694;&#26550;&#8212;&#8212;&#20998;&#24067;&#20559;&#22909;&#22870;&#21169;&#27169;&#22411;(DPRM)&#65292;&#20197;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#22810;&#26679;&#30340;&#20154;&#31867;&#20559;&#22909;&#23545;&#40784;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20351;&#29992;&#36125;&#22612;&#20998;&#24067;&#26469;&#21051;&#30011;&#20559;&#22909;&#65292;&#35813;&#20998;&#24067;&#33021;&#22815;&#21160;&#24577;&#36866;&#24212;&#20559;&#22909;&#36235;&#21183;&#30340;&#27874;&#21160;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#22522;&#20110;&#26368;&#20248;&#36755;&#36816;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#20197;&#26657;&#20934;DPRM&#19982;&#20559;&#22909;&#20998;&#24067;&#30340;&#23545;&#40784;&#24230;&#12290;&#26368;&#21518;&#65292;&#21033;&#29992;&#26399;&#26395;&#22870;&#21169;&#26469;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09764v1 Announce Type: new  Abstract: Deep Reinforcement Learning is widely used for aligning Large Language Models (LLM) with human preference. However, the conventional reward modelling has predominantly depended on human annotations provided by a select cohort of individuals. Such dependence may unintentionally result in models that are skewed to reflect the inclinations of these annotators, thereby failing to represent the expectations of the wider population adequately. In this paper, we introduce the Distributional Preference Reward Model (DPRM), a simple yet effective framework to align large language models with a diverse set of human preferences. To this end, we characterize the preferences by a beta distribution, which can dynamically adapt to fluctuations in preference trends. On top of that, we design an optimal-transportation-based loss to calibrate DPRM to align with the preference distribution. Finally, the expected reward is utilized to fine-tune an LLM polic
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#32508;&#36848;&#35770;&#25991;&#20171;&#32461;&#20102;&#29992;&#25143;&#24314;&#27169;&#19982;&#29992;&#25143;&#30011;&#20687;&#30740;&#31350;&#30340;&#29616;&#29366;&#12289;&#21457;&#23637;&#21644;&#26410;&#26469;&#26041;&#21521;&#12290;&#35813;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#22312;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#20013;&#26500;&#24314;&#20934;&#30830;&#30340;&#29992;&#25143;&#34920;&#31034;&#65292;&#21253;&#25324;&#21033;&#29992;&#22823;&#37327;&#25968;&#25454;&#36827;&#34892;&#24314;&#27169;&#20197;&#21450;&#37319;&#29992;&#28145;&#24230;&#23398;&#20064;&#21644;&#22270;&#25968;&#25454;&#25216;&#26415;&#31561;&#20808;&#36827;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.09660</link><description>&lt;p&gt;
&#29992;&#25143;&#24314;&#27169;&#19982;&#29992;&#25143;&#30011;&#20687;&#65306;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
User Modeling and User Profiling: A Comprehensive Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09660
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#32508;&#36848;&#35770;&#25991;&#20171;&#32461;&#20102;&#29992;&#25143;&#24314;&#27169;&#19982;&#29992;&#25143;&#30011;&#20687;&#30740;&#31350;&#30340;&#29616;&#29366;&#12289;&#21457;&#23637;&#21644;&#26410;&#26469;&#26041;&#21521;&#12290;&#35813;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#22312;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#20013;&#26500;&#24314;&#20934;&#30830;&#30340;&#29992;&#25143;&#34920;&#31034;&#65292;&#21253;&#25324;&#21033;&#29992;&#22823;&#37327;&#25968;&#25454;&#36827;&#34892;&#24314;&#27169;&#20197;&#21450;&#37319;&#29992;&#28145;&#24230;&#23398;&#20064;&#21644;&#22270;&#25968;&#25454;&#25216;&#26415;&#31561;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#34701;&#20837;&#26085;&#24120;&#29983;&#27963;&#65292;&#29305;&#21035;&#26159;&#36890;&#36807;&#20449;&#24687;&#26816;&#32034;&#21644;&#25512;&#33616;&#31995;&#32479;&#65292;&#24050;&#32463;&#20419;&#20351;&#20808;&#36827;&#30340;&#29992;&#25143;&#24314;&#27169;&#21644;&#29992;&#25143;&#30011;&#20687;&#25216;&#26415;&#65292;&#20197;&#25552;&#20379;&#20010;&#24615;&#21270;&#20307;&#39564;&#12290;&#36825;&#20123;&#25216;&#26415;&#26088;&#22312;&#22522;&#20110;&#19982;&#36825;&#20123;&#31995;&#32479;&#30340;&#20114;&#21160;&#20013;&#29983;&#25104;&#30340;&#22823;&#37327;&#25968;&#25454;&#26500;&#24314;&#20934;&#30830;&#30340;&#29992;&#25143;&#34920;&#31034;&#12290;&#26412;&#25991;&#23545;&#29992;&#25143;&#24314;&#27169;&#21644;&#29992;&#25143;&#30011;&#20687;&#30740;&#31350;&#30340;&#29616;&#29366;&#12289;&#21457;&#23637;&#21644;&#26410;&#26469;&#26041;&#21521;&#36827;&#34892;&#20102;&#20840;&#38754;&#32508;&#36848;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#21382;&#21490;&#27010;&#36848;&#65292;&#36861;&#28335;&#20102;&#20174;&#26089;&#26399;&#30340;&#21051;&#26495;&#27169;&#22411;&#21040;&#26368;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20998;&#31867;&#20307;&#31995;&#65292;&#28085;&#30422;&#20102;&#36825;&#19968;&#30740;&#31350;&#39046;&#22495;&#20013;&#30340;&#25152;&#26377;&#27963;&#21160;&#20027;&#39064;&#65292;&#21253;&#25324;&#26368;&#36817;&#30340;&#36235;&#21183;&#12290;&#25105;&#20204;&#30340;&#32508;&#36848;&#31361;&#20986;&#20102;&#21521;&#26356;&#22797;&#26434;&#30340;&#29992;&#25143;&#30011;&#20687;&#26041;&#27861;&#30340;&#33539;&#24335;&#36716;&#21464;&#65292;&#24378;&#35843;&#20102;&#38544;&#24335;&#25968;&#25454;&#25910;&#38598;&#12289;&#22810;&#34892;&#20026;&#24314;&#27169;&#20197;&#21450;&#22270;&#25968;&#25454;&#30340;&#25972;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09660v1 Announce Type: new  Abstract: The integration of artificial intelligence (AI) into daily life, particularly through information retrieval and recommender systems, has necessitated advanced user modeling and profiling techniques to deliver personalized experiences. These techniques aim to construct accurate user representations based on the rich amounts of data generated through interactions with these systems. This paper presents a comprehensive survey of the current state, evolution, and future directions of user modeling and profiling research. We provide a historical overview, tracing the development from early stereotype models to the latest deep learning techniques, and propose a novel taxonomy that encompasses all active topics in this research area, including recent trends. Our survey highlights the paradigm shifts towards more sophisticated user profiling methods, emphasizing implicit data collection, multi-behavior modeling, and the integration of graph data
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#22522;&#20110;&#33258;&#39537;&#21160;&#20256;&#24863;&#22120;&#21644;&#28145;&#24230;&#23398;&#20064;&#30340;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#37325;&#28857;&#35752;&#35770;&#20102;&#20351;&#29992;TENG&#20316;&#20026;&#33258;&#39537;&#21160;&#20256;&#24863;&#22120;&#30340;&#20248;&#21183;&#65292;&#21253;&#25324;&#31616;&#21333;&#32467;&#26500;&#21644;&#39640;&#30636;&#26102;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.09442</link><description>&lt;p&gt;
&#22522;&#20110;&#33258;&#39537;&#21160;&#20256;&#24863;&#22120;&#21644;&#28145;&#24230;&#23398;&#20064;&#30340;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#36827;&#23637;
&lt;/p&gt;
&lt;p&gt;
Progress in artificial intelligence applications based on the combination of self-driven sensors and deep learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09442
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22522;&#20110;&#33258;&#39537;&#21160;&#20256;&#24863;&#22120;&#21644;&#28145;&#24230;&#23398;&#20064;&#30340;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#37325;&#28857;&#35752;&#35770;&#20102;&#20351;&#29992;TENG&#20316;&#20026;&#33258;&#39537;&#21160;&#20256;&#24863;&#22120;&#30340;&#20248;&#21183;&#65292;&#21253;&#25324;&#31616;&#21333;&#32467;&#26500;&#21644;&#39640;&#30636;&#26102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29289;&#32852;&#32593;&#26102;&#20195;&#65292;&#22914;&#20309;&#24320;&#21457;&#20855;&#26377;&#21487;&#25345;&#32493;&#30005;&#28304;&#20379;&#24212;&#12289;&#26131;&#20110;&#37096;&#32626;&#21644;&#28789;&#27963;&#20351;&#29992;&#30340;&#26234;&#33021;&#20256;&#24863;&#22120;&#31995;&#32479;&#24050;&#25104;&#20026;&#19968;&#20010;&#38590;&#39064;&#12290;&#20256;&#32479;&#30340;&#30005;&#28304;&#20379;&#24212;&#23384;&#22312;&#39057;&#32321;&#26356;&#25442;&#25110;&#20351;&#29992;&#26102;&#20805;&#30005;&#31561;&#38382;&#39064;&#65292;&#36825;&#38480;&#21046;&#20102;&#21487;&#31359;&#25140;&#35774;&#22791;&#30340;&#21457;&#23637;&#12290;&#36890;&#36807;&#20351;&#29992;&#32858;&#22235;&#27679;&#20057;&#28911;&#65288;PTFE&#65289;&#21644;&#38109;&#31636;&#65288;AI&#65289;&#21046;&#22791;&#25509;&#35302;-&#20998;&#31163;&#25705;&#25830;&#32435;&#31859;&#21457;&#30005;&#26426;&#65288;TENG&#65289;&#26469;&#25910;&#38598;&#20154;&#20307;&#36816;&#21160;&#33021;&#37327;&#65292;&#26681;&#25454;&#36755;&#20986;&#30005;&#20449;&#21495;&#30340;&#21464;&#21270;&#26469;&#30417;&#27979;&#20154;&#20307;&#36816;&#21160;&#23039;&#21183;&#12290; 2012&#24180;&#65292;&#29579;&#20013;&#26519;&#38498;&#22763;&#21450;&#20854;&#22242;&#38431;&#21457;&#26126;&#20102;&#25705;&#25830;&#30005;&#32435;&#31859;&#21457;&#30005;&#26426;&#65288;TENG&#65289;&#65292;&#23427;&#21033;&#29992;&#26368;&#22823;&#20301;&#31227;&#30005;&#27969;&#20316;&#20026;&#39537;&#21160;&#21147;&#65292;&#23558;&#26426;&#26800;&#21050;&#28608;&#30452;&#25509;&#36716;&#25442;&#20026;&#30005;&#20449;&#21495;&#65292;&#22240;&#27492;&#21487;&#20197;&#29992;&#20316;&#33258;&#39537;&#21160;&#20256;&#24863;&#22120;&#12290;TENG&#20256;&#24863;&#22120;&#20855;&#26377;&#32467;&#26500;&#31616;&#21333;&#21644;&#30636;&#26102;&#24615;&#39640;&#30340;&#20248;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09442v1 Announce Type: cross  Abstract: In the era of Internet of Things, how to develop a smart sensor system with sustainable power supply, easy deployment and flexible use has become a difficult problem to be solved. The traditional power supply has problems such as frequent replacement or charging when in use, which limits the development of wearable devices. The contact-to-separate friction nanogenerator (TENG) was prepared by using polychotomy thy lene (PTFE) and aluminum (AI) foils. Human motion energy was collected by human body arrangement, and human motion posture was monitored according to the changes of output electrical signals. In 2012, Academician Wang Zhong lin and his team invented the triboelectric nanogenerator (TENG), which uses Maxwell displacement current as a driving force to directly convert mechanical stimuli into electrical signals, so it can be used as a self-driven sensor. Teng-based sensors have the advantages of simple structure and high instant
&lt;/p&gt;</description></item><item><title>&#19968;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#31639;&#27861;&#36741;&#21161;&#20915;&#31574;&#20013;&#65292;&#22914;&#20309;&#35774;&#35745;&#26368;&#20248;&#30340;&#39044;&#27979;&#31639;&#27861;&#21644;&#22996;&#25176;&#35268;&#21017;&#12290;&#20851;&#38190;&#21457;&#29616;&#21253;&#25324;&#65306;&#22996;&#25176;&#30340;&#26368;&#20248;&#24615;&#19982;&#22996;&#25176;&#20154;&#26159;&#21542;&#20250;&#20570;&#20986;&#19982;&#20195;&#29702;&#20154;&#30456;&#21516;&#30340;&#20915;&#31574;&#26377;&#20851;&#12289;&#26368;&#20855;&#20449;&#24687;&#37327;&#30340;&#31639;&#27861;&#19981;&#19968;&#23450;&#26159;&#26368;&#20248;&#30340;&#12289;&#24120;&#35265;&#30340;&#31639;&#27861;&#38480;&#21046;&#20250;&#38477;&#20302;&#20915;&#31574;&#36136;&#37327;&#12290;</title><link>https://arxiv.org/abs/2402.09384</link><description>&lt;p&gt;
&#35828;&#26381;&#12289;&#22996;&#25176;&#21644;&#31169;&#26377;&#20449;&#24687;&#22312;&#31639;&#27861;&#36741;&#21161;&#20915;&#31574;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Persuasion, Delegation, and Private Information in Algorithm-Assisted Decisions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09384
&lt;/p&gt;
&lt;p&gt;
&#19968;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#31639;&#27861;&#36741;&#21161;&#20915;&#31574;&#20013;&#65292;&#22914;&#20309;&#35774;&#35745;&#26368;&#20248;&#30340;&#39044;&#27979;&#31639;&#27861;&#21644;&#22996;&#25176;&#35268;&#21017;&#12290;&#20851;&#38190;&#21457;&#29616;&#21253;&#25324;&#65306;&#22996;&#25176;&#30340;&#26368;&#20248;&#24615;&#19982;&#22996;&#25176;&#20154;&#26159;&#21542;&#20250;&#20570;&#20986;&#19982;&#20195;&#29702;&#20154;&#30456;&#21516;&#30340;&#20915;&#31574;&#26377;&#20851;&#12289;&#26368;&#20855;&#20449;&#24687;&#37327;&#30340;&#31639;&#27861;&#19981;&#19968;&#23450;&#26159;&#26368;&#20248;&#30340;&#12289;&#24120;&#35265;&#30340;&#31639;&#27861;&#38480;&#21046;&#20250;&#38477;&#20302;&#20915;&#31574;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#20301;&#22996;&#25176;&#20154;&#35774;&#35745;&#20102;&#19968;&#20010;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#29983;&#25104;&#19968;&#20010;&#20844;&#24320;&#21487;&#35265;&#30340;&#20108;&#36827;&#21046;&#29366;&#24577;&#39044;&#27979;&#12290;&#22905;&#24517;&#39035;&#20915;&#23450;&#26159;&#26681;&#25454;&#39044;&#27979;&#30452;&#25509;&#34892;&#21160;&#36824;&#26159;&#23558;&#20915;&#31574;&#22996;&#25176;&#32473;&#19968;&#20010;&#20195;&#29702;&#20154;&#65292;&#35813;&#20195;&#29702;&#20154;&#20855;&#26377;&#31169;&#26377;&#20449;&#24687;&#20294;&#21487;&#33021;&#23384;&#22312;&#19981;&#23545;&#40784;&#38382;&#39064;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#36825;&#31181;&#29615;&#22659;&#19979;&#39044;&#27979;&#31639;&#27861;&#21644;&#22996;&#25176;&#35268;&#21017;&#30340;&#26368;&#20248;&#35774;&#35745;&#12290;&#24471;&#20986;&#20102;&#19977;&#20010;&#20851;&#38190;&#21457;&#29616;&#65306;(1)&#21482;&#26377;&#24403;&#22996;&#25176;&#20154;&#22312;&#35266;&#23519;&#21040;&#20195;&#29702;&#20154;&#30340;&#20449;&#24687;&#26102;&#20250;&#20570;&#20986;&#19982;&#20195;&#29702;&#20154;&#30456;&#21516;&#30340;&#20108;&#36827;&#21046;&#20915;&#31574;&#26102;&#65292;&#22996;&#25176;&#25165;&#26159;&#26368;&#20248;&#30340;&#12290;(2)&#25552;&#20379;&#26368;&#20855;&#20449;&#24687;&#37327;&#30340;&#31639;&#27861;&#21487;&#33021;&#24182;&#19981;&#26159;&#26368;&#20248;&#30340;&#65292;&#21363;&#20351;&#22996;&#25176;&#20154;&#21487;&#20197;&#26681;&#25454;&#31639;&#27861;&#30340;&#39044;&#27979;&#26469;&#34892;&#21160;&#12290;&#30456;&#21453;&#65292;&#26368;&#20248;&#31639;&#27861;&#21487;&#33021;&#25552;&#20379;&#26356;&#22810;&#20851;&#20110;&#19968;&#20010;&#29366;&#24577;&#30340;&#20449;&#24687;&#65292;&#24182;&#38480;&#21046;&#20851;&#20110;&#21478;&#19968;&#20010;&#29366;&#24577;&#30340;&#20449;&#24687;&#12290;(3)&#22312;&#27809;&#26377;&#23436;&#32654;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#30340;&#24773;&#20917;&#19979;&#65292;&#24120;&#35265;&#30340;&#23545;&#31639;&#27861;&#30340;&#38480;&#21046;&#65292;&#22914;&#20445;&#25345;"&#20154;&#26426;&#21512;&#20316;"&#25110;&#35201;&#27714;&#26368;&#22823;&#39044;&#27979;&#31934;&#24230;&#65292;&#20250;&#20005;&#37325;&#38477;&#20302;&#20915;&#31574;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09384v1 Announce Type: cross Abstract: A principal designs an algorithm that generates a publicly observable prediction of a binary state. She must decide whether to act directly based on the prediction or to delegate the decision to an agent with private information but potential misalignment. We study the optimal design of the prediction algorithm and the delegation rule in such environments. Three key findings emerge: (1) Delegation is optimal if and only if the principal would make the same binary decision as the agent had she observed the agent's information. (2) Providing the most informative algorithm may be suboptimal even if the principal can act on the algorithm's prediction. Instead, the optimal algorithm may provide more information about one state and restrict information about the other. (3) Common restrictions on algorithms, such as keeping a "human-in-the-loop" or requiring maximal prediction accuracy, strictly worsen decision quality in the absence of perfec
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20851;&#20110;&#35777;&#25454;&#28145;&#24230;&#23398;&#20064;&#30340;&#26032;&#29702;&#35770;&#27934;&#35265;, &#39640;&#20142;&#20102;&#22312;&#20248;&#21270;&#20108;&#38454;&#25439;&#22833;&#20989;&#25968;&#21644;&#35299;&#37322;&#24471;&#20986;&#30340;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#24230;&#37327;&#19978;&#30340;&#22256;&#38590;&#24615;</title><link>https://arxiv.org/abs/2402.09056</link><description>&lt;p&gt;
&#35777;&#25454;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#26159;&#21542;&#20934;&#30830;&#22320;&#34920;&#31034;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#65311;
&lt;/p&gt;
&lt;p&gt;
Is Epistemic Uncertainty Faithfully Represented by Evidential Deep Learning Methods?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09056
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20851;&#20110;&#35777;&#25454;&#28145;&#24230;&#23398;&#20064;&#30340;&#26032;&#29702;&#35770;&#27934;&#35265;, &#39640;&#20142;&#20102;&#22312;&#20248;&#21270;&#20108;&#38454;&#25439;&#22833;&#20989;&#25968;&#21644;&#35299;&#37322;&#24471;&#20986;&#30340;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#24230;&#37327;&#19978;&#30340;&#22256;&#38590;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#20449;&#30340;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#19981;&#20165;&#24212;&#36820;&#22238;&#20934;&#30830;&#30340;&#39044;&#27979;&#32467;&#26524;&#65292;&#36824;&#24212;&#25552;&#20379;&#21487;&#38752;&#30340;&#19981;&#30830;&#23450;&#24615;&#34920;&#31034;&#12290;&#36125;&#21494;&#26031;&#26041;&#27861;&#24120;&#29992;&#20110;&#37327;&#21270;&#19981;&#30830;&#23450;&#24615;&#65292;&#20294;&#36817;&#24180;&#26469;&#65292;&#35777;&#25454;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#31561;&#26367;&#20195;&#26041;&#27861;&#20063;&#21464;&#24471;&#27969;&#34892;&#36215;&#26469;&#12290;&#21518;&#32773;&#26412;&#36136;&#19978;&#25193;&#23637;&#20102;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#65288;ERM&#65289;&#65292;&#29992;&#20110;&#39044;&#27979;&#32467;&#26524;&#30340;&#20108;&#38454;&#27010;&#29575;&#20998;&#24067;&#65292;&#20174;&#20013;&#21487;&#20197;&#25552;&#21462;&#35748;&#35782;&#65288;&#21644;&#38543;&#26426;&#65289;&#19981;&#30830;&#23450;&#24615;&#30340;&#24230;&#37327;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#35777;&#25454;&#28145;&#24230;&#23398;&#20064;&#30340;&#26032;&#29702;&#35770;&#27934;&#35265;&#65292;&#24378;&#35843;&#20102;&#20248;&#21270;&#20108;&#38454;&#25439;&#22833;&#20989;&#25968;&#20197;&#21450;&#35299;&#37322;&#32467;&#26524;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#24230;&#37327;&#30340;&#22256;&#38590;&#24615;&#12290;&#36890;&#36807;&#31995;&#32479;&#21270;&#30340;&#35774;&#32622;&#65292;&#28085;&#30422;&#20102;&#20998;&#31867;&#12289;&#22238;&#24402;&#21644;&#35745;&#25968;&#30340;&#24191;&#27867;&#26041;&#27861;&#65292;&#36825;&#31687;&#35770;&#25991;&#20026;&#21487;&#36776;&#35782;&#24615;&#21644;&#25910;&#25947;&#24615;&#38382;&#39064;&#25552;&#20379;&#20102;&#26032;&#30340;&#27934;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09056v1 Announce Type: new Abstract: Trustworthy ML systems should not only return accurate predictions, but also a reliable representation of their uncertainty. Bayesian methods are commonly used to quantify both aleatoric and epistemic uncertainty, but alternative approaches, such as evidential deep learning methods, have become popular in recent years. The latter group of methods in essence extends empirical risk minimization (ERM) for predicting second-order probability distributions over outcomes, from which measures of epistemic (and aleatoric) uncertainty can be extracted. This paper presents novel theoretical insights of evidential deep learning, highlighting the difficulties in optimizing second-order loss functions and interpreting the resulting epistemic uncertainty measures. With a systematic setup that covers a wide range of approaches for classification, regression and counts, it provides novel insights into issues of identifiability and convergence in second-o
&lt;/p&gt;</description></item><item><title>SAGMAN&#26159;&#19968;&#31181;&#29992;&#20110;&#26816;&#39564;&#22270;&#31070;&#32463;&#32593;&#32476;&#31283;&#23450;&#24615;&#30340;&#35889;&#26694;&#26550;&#65292;&#23427;&#36890;&#36807;&#35780;&#20272;&#38750;&#32447;&#24615;&#26144;&#23556;&#20013;&#30340;&#36317;&#31163;&#22833;&#30495;&#26469;&#34913;&#37327;GNN&#30340;&#31283;&#23450;&#24615;&#12290;&#20026;&#20102;&#36827;&#34892;&#26377;&#24847;&#20041;&#30340;&#31283;&#23450;&#24615;&#20998;&#26512;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36317;&#31163;&#20445;&#25345;&#30340;&#22270;&#38477;&#32500;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.08653</link><description>&lt;p&gt;
SAGMAN: &#29992;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#27969;&#24418;&#19978;&#30340;&#31283;&#23450;&#24615;&#20998;&#26512;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
SAGMAN: Stability Analysis of Graph Neural Networks on the Manifolds
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08653
&lt;/p&gt;
&lt;p&gt;
SAGMAN&#26159;&#19968;&#31181;&#29992;&#20110;&#26816;&#39564;&#22270;&#31070;&#32463;&#32593;&#32476;&#31283;&#23450;&#24615;&#30340;&#35889;&#26694;&#26550;&#65292;&#23427;&#36890;&#36807;&#35780;&#20272;&#38750;&#32447;&#24615;&#26144;&#23556;&#20013;&#30340;&#36317;&#31163;&#22833;&#30495;&#26469;&#34913;&#37327;GNN&#30340;&#31283;&#23450;&#24615;&#12290;&#20026;&#20102;&#36827;&#34892;&#26377;&#24847;&#20041;&#30340;&#31283;&#23450;&#24615;&#20998;&#26512;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36317;&#31163;&#20445;&#25345;&#30340;&#22270;&#38477;&#32500;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#23545;&#36755;&#20837;&#22270;&#32467;&#26500;&#21644;&#33410;&#28857;&#29305;&#24449;&#30340;&#21464;&#21270;&#25935;&#24863;&#65292;&#21487;&#33021;&#23548;&#33268;&#19981;&#21487;&#39044;&#27979;&#30340;&#34892;&#20026;&#21644;&#24615;&#33021;&#19979;&#38477;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#31216;&#20026;SAGMAN&#30340;&#35889;&#26694;&#26550;&#65292;&#29992;&#20110;&#26816;&#39564;GNN&#30340;&#31283;&#23450;&#24615;&#12290;&#35813;&#26694;&#26550;&#35780;&#20272;&#38750;&#32447;&#24615;&#26144;&#23556;&#20013;GNN&#22312;&#36755;&#20837;&#21644;&#36755;&#20986;&#27969;&#24418;&#20043;&#38388;&#24341;&#36215;&#30340;&#36317;&#31163;&#22833;&#30495;: &#24403;&#36755;&#20837;&#27969;&#34892;&#20013;&#20004;&#20010;&#38468;&#36817;&#30340;&#33410;&#28857;&#65288;&#36890;&#36807;GNN&#27169;&#22411;&#65289;&#34987;&#26144;&#23556;&#21040;&#36755;&#20986;&#27969;&#34892;&#19978;&#30340;&#20004;&#20010;&#36828;&#31163;&#30340;&#33410;&#28857;&#26102;&#65292;&#24847;&#21619;&#30528;&#23384;&#22312;&#36739;&#22823;&#30340;&#36317;&#31163;&#22833;&#30495;&#65292;&#20174;&#32780;&#23548;&#33268;GNN&#30340;&#31283;&#23450;&#24615;&#36739;&#24046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36317;&#31163;&#20445;&#25345;&#30340;&#22270;&#38477;&#32500;&#65288;GDR&#65289;&#26041;&#27861;&#65292;&#21033;&#29992;&#35889;&#22270;&#23884;&#20837;&#21644;&#27010;&#29575;&#22270;&#27169;&#22411;&#65288;PGMs&#65289;&#26469;&#21019;&#24314;&#20302;&#32500;&#30340;&#36755;&#20837;/&#36755;&#20986;&#22522;&#20110;&#22270;&#30340;&#27969;&#24418;&#65292;&#20197;&#36827;&#34892;&#26377;&#24847;&#20041;&#30340;&#31283;&#23450;&#24615;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#35780;&#20272;&#34920;&#26126;&#65292;SAGMAN&#33021;&#22815;&#26377;&#25928;&#35780;&#20272;&#27599;&#20010;&#33410;&#28857;&#22312;&#38754;&#23545;&#19981;&#21516;&#36793;&#32536;&#25110;&#29305;&#24449;&#25200;&#21160;&#26102;&#30340;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern graph neural networks (GNNs) can be sensitive to changes in the input graph structure and node features, potentially resulting in unpredictable behavior and degraded performance. In this work, we introduce a spectral framework known as SAGMAN for examining the stability of GNNs. This framework assesses the distance distortions that arise from the nonlinear mappings of GNNs between the input and output manifolds: when two nearby nodes on the input manifold are mapped (through a GNN model) to two distant ones on the output manifold, it implies a large distance distortion and thus a poor GNN stability. We propose a distance-preserving graph dimension reduction (GDR) approach that utilizes spectral graph embedding and probabilistic graphical models (PGMs) to create low-dimensional input/output graph-based manifolds for meaningful stability analysis. Our empirical evaluations show that SAGMAN effectively assesses the stability of each node when subjected to various edge or feature pe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#24046;&#20998;&#38544;&#31169;&#38646;&#38454;&#26041;&#27861;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#20013;&#30340;&#24212;&#29992;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#38646;&#38454;&#26799;&#24230;&#26469;&#36991;&#20813;&#20256;&#32479;&#20248;&#21270;&#26041;&#27861;&#30340;&#21487;&#25193;&#23637;&#24615;&#29942;&#39048;&#65292;&#23454;&#29616;&#20102;&#22312;&#38544;&#31169;&#12289;&#25928;&#29992;&#21644;&#21487;&#25193;&#23637;&#24615;&#20043;&#38388;&#30340;&#33391;&#22909;&#24179;&#34913;&#12290;</title><link>https://arxiv.org/abs/2402.07818</link><description>&lt;p&gt;
&#21487;&#25193;&#23637;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#30340;&#24046;&#20998;&#38544;&#31169;&#38646;&#38454;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Differentially Private Zeroth-Order Methods for Scalable Large Language Model Finetuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07818
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24046;&#20998;&#38544;&#31169;&#38646;&#38454;&#26041;&#27861;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#20013;&#30340;&#24212;&#29992;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#38646;&#38454;&#26799;&#24230;&#26469;&#36991;&#20813;&#20256;&#32479;&#20248;&#21270;&#26041;&#27861;&#30340;&#21487;&#25193;&#23637;&#24615;&#29942;&#39048;&#65292;&#23454;&#29616;&#20102;&#22312;&#38544;&#31169;&#12289;&#25928;&#29992;&#21644;&#21487;&#25193;&#23637;&#24615;&#20043;&#38388;&#30340;&#33391;&#22909;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29305;&#23450;&#20219;&#21153;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#26159;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#24378;&#22823;&#33021;&#21147;&#36827;&#34892;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#30340;&#24191;&#27867;&#25509;&#21463;&#30340;&#33539;&#20363;&#12290;&#30001;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#30340;&#26222;&#21450;&#20197;&#21450;&#19982;&#20043;&#30456;&#20851;&#30340;&#38544;&#31169;&#38382;&#39064;&#65292;&#24046;&#20998;&#38544;&#31169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#20197;&#20445;&#25252;&#29305;&#23450;&#20219;&#21153;&#25968;&#25454;&#38598;&#30340;&#38544;&#31169;&#12290;&#24046;&#20998;&#38544;&#31169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#26041;&#27861;&#30340;&#35774;&#35745;&#26680;&#24515;&#26159;&#22312;&#38544;&#31169;&#12289;&#25928;&#29992;&#21644;&#21487;&#25193;&#23637;&#24615;&#20043;&#38388;&#36798;&#21040;&#28385;&#24847;&#30340;&#26435;&#34913;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#37117;&#26159;&#22522;&#20110;DP-SGD&#30340;&#21019;&#26032;&#24615;&#24037;&#20316;&#12290;&#23613;&#31649;&#23558;DP-SGD&#30340;&#21487;&#25193;&#23637;&#24615;&#25512;&#21040;&#20102;&#26497;&#38480;&#65292;&#20294;&#22522;&#20110;DP-SGD&#30340;&#24494;&#35843;&#26041;&#27861;&#19981;&#24184;&#22320;&#21463;&#21040;&#20102;SGD&#22266;&#26377;&#20302;&#25928;&#29575;&#30340;&#38480;&#21046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;DP&#38646;&#38454;&#26041;&#27861;&#22312;LLM&#39044;&#35757;&#32451;&#20013;&#30340;&#28508;&#21147;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#29992;&#26356;&#39640;&#25928;&#30340;&#38646;&#38454;&#26799;&#24230;&#26469;&#36817;&#20284;&#26799;&#24230;&#65292;&#36991;&#20813;&#20102;SGD&#30340;&#21487;&#25193;&#23637;&#24615;&#29942;&#39048;&#12290;&#19982;&#23558;&#38646;&#38454;&#26041;&#27861;&#20316;&#20026;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#36827;&#34892;&#22788;&#29702;&#19981;&#21516;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#21106;&#25509;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#20197;&#38750;&#24120;&#25509;&#36817;&#30340;&#26041;&#24335;&#27169;&#25311;DP-SGD&#30340;&#22522;&#26412;&#25805;&#20316;&#65292;&#28982;&#21518;&#21033;&#29992;&#38646;&#38454;&#20248;&#21270;&#26041;&#27861;&#26469;&#36817;&#20284;&#26799;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Finetuning on task-specific datasets is a widely-embraced paradigm of harnessing the powerful capability of pretrained LLMs for various downstream tasks. Due to the popularity of LLMs finetuning and its accompanying privacy concerns, differentially private (DP) finetuning of pretrained LLMs has garnered increasing attention to safeguarding the privacy of task-specific datasets. Lying at the design core of DP LLM finetuning methods is the satisfactory tradeoff between privacy, utility, and scalability. Most existing methods build upon the seminal work of DP-SGD. Despite pushing the scalability of DP-SGD to its limit, DP-SGD-based finetuning methods are unfortunately limited by the inherent inefficiency of SGD. In this paper, we investigate the potential of DP zeroth-order methods for LLM pretraining, which avoids the scalability bottleneck of SGD by approximating the gradient with the more efficient zeroth-order gradient. Rather than treating the zeroth-order method as a drop-in replace
&lt;/p&gt;</description></item><item><title>DeAL&#26159;&#19968;&#20010;&#20801;&#35768;&#29992;&#25143;&#33258;&#23450;&#20041;&#22870;&#21169;&#20989;&#25968;&#24182;&#23454;&#29616;&#35299;&#30721;&#26102;&#23545;&#40784;LLMs&#30340;&#26694;&#26550;&#12290;</title><link>https://arxiv.org/abs/2402.06147</link><description>&lt;p&gt;
DeAL&#65306;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35299;&#30721;&#26102;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
DeAL: Decoding-time Alignment for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06147
&lt;/p&gt;
&lt;p&gt;
DeAL&#26159;&#19968;&#20010;&#20801;&#35768;&#29992;&#25143;&#33258;&#23450;&#20041;&#22870;&#21169;&#20989;&#25968;&#24182;&#23454;&#29616;&#35299;&#30721;&#26102;&#23545;&#40784;LLMs&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29616;&#22312;&#26399;&#26395;&#29983;&#25104;&#19982;&#20154;&#31867;&#20559;&#22909;&#23545;&#40784;&#30340;&#20869;&#23481;&#12290;&#30446;&#21069;&#30340;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#27169;&#22411;&#35757;&#32451;&#26102;&#38388;&#23545;&#40784;&#19978;&#65292;&#36890;&#36807;&#35832;&#22914;&#24378;&#21270;&#23398;&#20064;&#19982;&#20154;&#31867;&#21453;&#39304;&#65288;RLHF&#65289;&#31561;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#36824;&#19981;&#28165;&#26970;&#36825;&#20123;&#26041;&#27861;&#26159;&#21542;&#26377;&#25928;&#22320;&#25945;&#23548;&#27169;&#22411;&#23545;&#40784;&#30446;&#26631;&#12290;&#39318;&#20808;&#65292;&#26080;&#27861;&#25972;&#21512;&#22810;&#20010;&#33258;&#23450;&#20041;&#22870;&#21169;&#21644;&#20381;&#36182;&#27169;&#22411;&#24320;&#21457;&#32773;&#23545;&#36890;&#29992;&#21644;&#38745;&#24577;&#21407;&#21017;&#30340;&#29702;&#35299;&#26159;&#20027;&#35201;&#23616;&#38480;&#12290;&#20854;&#27425;&#65292;&#27169;&#22411;&#35757;&#32451;&#20013;&#30340;&#27531;&#30041;&#24046;&#36317;&#20197;&#21450;&#36825;&#20123;&#26041;&#27861;&#30340;&#21487;&#38752;&#24615;&#20063;&#20540;&#24471;&#36136;&#30097;&#65288;&#20363;&#22914;&#65292;&#21363;&#20351;&#22312;&#23433;&#20840;&#35757;&#32451;&#21518;&#20173;&#28982;&#23481;&#26131;&#34987;&#36234;&#29425;&#65289;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DeAL&#65292;&#19968;&#20010;&#20801;&#35768;&#29992;&#25143;&#33258;&#23450;&#20041;&#22870;&#21169;&#20989;&#25968;&#24182;&#23454;&#29616;&#35299;&#30721;&#26102;&#23545;&#40784;LLMs&#65288;DeAL&#65289;&#30340;&#26694;&#26550;&#12290;&#26680;&#24515;&#24605;&#24819;&#22312;&#20110;&#23558;&#35299;&#30721;&#35270;&#20026;&#19968;&#20010;&#21551;&#21457;&#24335;&#24341;&#23548;&#30340;&#25628;&#32034;&#36807;&#31243;&#65292;&#24182;&#20419;&#20351;&#20351;&#29992;&#21508;&#31181;&#23545;&#40784;&#30446;&#26631;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#20197;&#32534;&#31243;&#32422;&#26463;&#20026;&#20363;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) are nowadays expected to generate content aligned with human preferences. Current work focuses on alignment at model training time, through techniques such as Reinforcement Learning with Human Feedback (RLHF). However, it is unclear if such methods are an effective choice to teach alignment objectives to the model. First, the inability to incorporate multiple, custom rewards and reliance on a model developer's view of universal and static principles are key limitations. Second, the residual gaps in model training and the reliability of such approaches are also questionable (e.g. susceptibility to jail-breaking even after safety training). To address these, we propose DeAL, a framework that allows the user to customize reward functions and enables Decoding-time Alignment of LLMs (DeAL). At its core, we view decoding as a heuristic-guided search process and facilitate the use of a wide variety of alignment objectives. Our experiments with programmatic constra
&lt;/p&gt;</description></item><item><title>InkSight&#26159;&#19968;&#20010;&#21487;&#20197;&#23558;&#31163;&#32447;&#25163;&#20889;&#36716;&#25442;&#20026;&#22312;&#32447;&#25163;&#20889;&#30340;&#31995;&#32479;&#65292;&#36890;&#36807;&#32467;&#21512;&#38405;&#35835;&#21644;&#20070;&#20889;&#20808;&#39564;&#30693;&#35782;&#65292;&#22312;&#22810;&#26679;&#21270;&#30340;&#29031;&#29255;&#20013;&#26377;&#25928;&#22320;Derendering&#25163;&#20889;&#25991;&#26412;&#12290;</title><link>https://arxiv.org/abs/2402.05804</link><description>&lt;p&gt;
InkSight&#65306;&#36890;&#36807;&#23398;&#20064;&#38405;&#35835;&#21644;&#20070;&#20889;&#23454;&#29616;&#31163;&#32447;&#21040;&#22312;&#32447;&#25163;&#20889;&#36716;&#25442;
&lt;/p&gt;
&lt;p&gt;
InkSight: Offline-to-Online Handwriting Conversion by Learning to Read and Write
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05804
&lt;/p&gt;
&lt;p&gt;
InkSight&#26159;&#19968;&#20010;&#21487;&#20197;&#23558;&#31163;&#32447;&#25163;&#20889;&#36716;&#25442;&#20026;&#22312;&#32447;&#25163;&#20889;&#30340;&#31995;&#32479;&#65292;&#36890;&#36807;&#32467;&#21512;&#38405;&#35835;&#21644;&#20070;&#20889;&#20808;&#39564;&#30693;&#35782;&#65292;&#22312;&#22810;&#26679;&#21270;&#30340;&#29031;&#29255;&#20013;&#26377;&#25928;&#22320;Derendering&#25163;&#20889;&#25991;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23383;&#31508;&#35760;&#27491;&#22312;&#21464;&#24471;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#32784;&#29992;&#12289;&#21487;&#32534;&#36753;&#21644;&#26131;&#20110;&#32034;&#24341;&#30340;&#23384;&#20648;&#31508;&#35760;&#30340;&#26041;&#24335;&#65292;&#21363;&#30690;&#37327;&#21270;&#24418;&#24335;&#30340;&#25968;&#23383;&#22696;&#27700;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#31508;&#35760;&#26041;&#24335;&#19982;&#20256;&#32479;&#30340;&#32440;&#31508;&#35760;&#26041;&#24335;&#20043;&#38388;&#20173;&#23384;&#22312;&#26174;&#33879;&#24046;&#36317;&#65292;&#32780;&#20256;&#32479;&#32440;&#31508;&#35760;&#26041;&#24335;&#20173;&#21463;&#21040;&#32477;&#22823;&#22810;&#25968;&#20154;&#30340;&#38738;&#30544;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;InkSight&#26088;&#22312;&#24357;&#21512;&#36825;&#31181;&#24046;&#36317;&#65292;&#20351;&#23454;&#20307;&#31508;&#35760;&#32773;&#33021;&#22815;&#36731;&#26494;&#22320;&#23558;&#20182;&#20204;&#30340;&#20316;&#21697;&#65288;&#31163;&#32447;&#25163;&#20889;&#65289;&#36716;&#25442;&#20026;&#25968;&#23383;&#22696;&#27700;&#65288;&#22312;&#32447;&#25163;&#20889;&#65289;&#65292;&#36825;&#20010;&#36807;&#31243;&#25105;&#20204;&#31216;&#20043;&#20026;Derendering&#12290;&#20043;&#21069;&#20851;&#20110;&#27492;&#20027;&#39064;&#30340;&#30740;&#31350;&#38598;&#20013;&#22312;&#22270;&#20687;&#30340;&#20960;&#20309;&#23646;&#24615;&#19978;&#65292;&#23548;&#33268;&#20102;&#22312;&#35757;&#32451;&#39046;&#22495;&#20043;&#22806;&#30340;&#26377;&#38480;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#32467;&#21512;&#20102;&#38405;&#35835;&#21644;&#20070;&#20889;&#30340;&#20808;&#39564;&#30693;&#35782;&#65292;&#20801;&#35768;&#22312;&#32570;&#20047;&#22823;&#37327;&#37197;&#23545;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;&#27169;&#22411;&#65292;&#32780;&#36825;&#20123;&#37197;&#23545;&#26679;&#26412;&#24456;&#38590;&#33719;&#21462;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#26377;&#25928;&#22320;&#23545;&#20855;&#26377;&#22810;&#26679;&#21270;&#35270;&#35273;&#29305;&#24449;&#21644;&#32972;&#26223;&#30340;&#20219;&#24847;&#29031;&#29255;&#20013;&#30340;&#25163;&#20889;&#25991;&#26412;&#36827;&#34892;Derendering&#30340;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Digital note-taking is gaining popularity, offering a durable, editable, and easily indexable way of storing notes in the vectorized form, known as digital ink. However, a substantial gap remains between this way of note-taking and traditional pen-and-paper note-taking, a practice still favored by a vast majority. Our work, InkSight, aims to bridge the gap by empowering physical note-takers to effortlessly convert their work (offline handwriting) to digital ink (online handwriting), a process we refer to as Derendering. Prior research on the topic has focused on the geometric properties of images, resulting in limited generalization beyond their training domains. Our approach combines reading and writing priors, allowing training a model in the absence of large amounts of paired samples, which are difficult to obtain. To our knowledge, this is the first work that effectively derenders handwritten text in arbitrary photos with diverse visual characteristics and backgrounds. Furthermore,
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#20174;&#26410;&#26631;&#35760;&#30340;&#22810;&#24863;&#23448;&#25968;&#25454;&#20013;&#23398;&#20064;&#20016;&#23500;&#32780;&#26377;&#24847;&#20041;&#30340;3D&#22330;&#26223;&#34920;&#31034;&#12290;&#36890;&#36807;&#21033;&#29992;&#19981;&#21516;&#24863;&#35273;&#27169;&#24577;&#20043;&#38388;&#30340;&#26102;&#38388;&#19968;&#33268;&#24615;&#21644;&#20960;&#20309;&#23545;&#40784;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#33021;&#22815;&#23398;&#20064;&#21040;&#24378;&#22823;&#32780;&#20934;&#30830;&#30340;&#34920;&#31034;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;&#21508;&#31181;3D&#24863;&#30693;&#20219;&#21153;&#65292;&#24182;&#19982;&#30417;&#30563;&#22522;&#32447;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#23637;&#31034;&#20102;&#31454;&#20105;&#24615;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#25105;&#20204;&#23398;&#21040;&#30340;&#34920;&#31034;&#22312;&#19981;&#21516;&#30340;&#20256;&#24863;&#22120;&#35774;&#32622;&#19979;&#20855;&#26377;&#24456;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#36827;&#19968;&#27493;&#31361;&#26174;&#20102;&#25105;&#20204;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#22810;&#21151;&#33021;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.05650</link><description>&lt;p&gt;
&#12298;&#23721;&#30707;&#32534;&#30721;&#65292;&#19981;&#26159;&#24320;&#21457;-&#19968;&#20010;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;LLM&#22312;&#36719;&#20214;&#24037;&#31243;&#20219;&#21153;&#20013;&#30340;&#23454;&#39564;&#35780;&#20272;&#12299;
&lt;/p&gt;
&lt;p&gt;
Rocks Coding, Not Development--A Human-Centric, Experimental Evaluation of LLM-Supported SE Tasks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05650
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#20174;&#26410;&#26631;&#35760;&#30340;&#22810;&#24863;&#23448;&#25968;&#25454;&#20013;&#23398;&#20064;&#20016;&#23500;&#32780;&#26377;&#24847;&#20041;&#30340;3D&#22330;&#26223;&#34920;&#31034;&#12290;&#36890;&#36807;&#21033;&#29992;&#19981;&#21516;&#24863;&#35273;&#27169;&#24577;&#20043;&#38388;&#30340;&#26102;&#38388;&#19968;&#33268;&#24615;&#21644;&#20960;&#20309;&#23545;&#40784;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#33021;&#22815;&#23398;&#20064;&#21040;&#24378;&#22823;&#32780;&#20934;&#30830;&#30340;&#34920;&#31034;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;&#21508;&#31181;3D&#24863;&#30693;&#20219;&#21153;&#65292;&#24182;&#19982;&#30417;&#30563;&#22522;&#32447;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#23637;&#31034;&#20102;&#31454;&#20105;&#24615;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#25105;&#20204;&#23398;&#21040;&#30340;&#34920;&#31034;&#22312;&#19981;&#21516;&#30340;&#20256;&#24863;&#22120;&#35774;&#32622;&#19979;&#20855;&#26377;&#24456;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#36827;&#19968;&#27493;&#31361;&#26174;&#20102;&#25105;&#20204;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#22810;&#21151;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#29983;&#25104;&#22411;AI&#22240;&#20854;&#22312;&#22810;&#20010;&#39046;&#22495;&#20013;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#39640;&#36136;&#37327;&#34920;&#29616;&#32780;&#22791;&#21463;&#20851;&#27880;&#65292;&#29305;&#21035;&#26159;&#22312;ChatGPT&#21457;&#24067;&#20043;&#21518;&#12290;&#35768;&#22810;&#20154;&#35748;&#20026;&#23427;&#20204;&#26377;&#28508;&#21147;&#22312;&#36719;&#20214;&#24320;&#21457;&#20013;&#25191;&#34892;&#36890;&#29992;&#38382;&#39064;&#35299;&#20915;&#65292;&#24182;&#21462;&#20195;&#20154;&#31867;&#36719;&#20214;&#24320;&#21457;&#20154;&#21592;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#27809;&#26377;&#23545;&#36825;&#20123;LLM&#25216;&#26415;&#22312;&#23436;&#25104;&#36719;&#20214;&#24320;&#21457;&#20219;&#21153;&#26041;&#38754;&#30340;&#33021;&#21147;&#36827;&#34892;&#28145;&#20837;&#35843;&#26597;&#12290;&#22312;&#19968;&#39033;&#26377;109&#21517;&#21442;&#19982;&#32773;&#30340;&#21463;&#25511; 2x2 &#21463;&#35797;&#32773;&#38388;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19982;ChatGPT&#21512;&#20316;&#22312;&#32534;&#30721;&#20219;&#21153;&#21644;&#20856;&#22411;&#36719;&#20214;&#24320;&#21457;&#20219;&#21153;&#20013;&#30340;&#25928;&#29992;&#31243;&#24230;&#20197;&#21450;&#20154;&#20204;&#22914;&#20309;&#20351;&#29992;ChatGPT&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#23613;&#31649;ChatGPT&#22312;&#35299;&#20915;&#31616;&#21333;&#30340;&#32534;&#30721;&#38382;&#39064;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#23427;&#22312;&#25903;&#25345;&#20856;&#22411;&#30340;&#36719;&#20214;&#24320;&#21457;&#20219;&#21153;&#26041;&#38754;&#30340;&#34920;&#29616;&#24182;&#19981;&#29702;&#24819;&#12290;&#25105;&#20204;&#36824;&#35266;&#23519;&#20102;&#21442;&#19982;&#32773;&#19982;ChatGPT&#20043;&#38388;&#30340;&#20114;&#21160;&#65292;&#24182;&#25214;&#21040;&#20102;&#30456;&#20114;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, large language models (LLM) based generative AI has been gaining momentum for their impressive high-quality performances in multiple domains, particularly after the release of the ChatGPT. Many believe that they have the potential to perform general-purpose problem-solving in software development and replace human software developers. Nevertheless, there are in a lack of serious investigation into the capability of these LLM techniques in fulfilling software development tasks. In a controlled 2 $\times$ 2 between-subject experiment with 109 participants, we examined whether and to what degree working with ChatGPT was helpful in the coding task and typical software development task and how people work with ChatGPT. We found that while ChatGPT performed well in solving simple coding problems, its performance in supporting typical software development tasks was not that good. We also observed the interactions between participants and ChatGPT and found the relations between the i
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#35302;&#35273;&#21453;&#39304;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#39063;&#31890;&#20171;&#36136;&#20013;&#26816;&#32034;&#22475;&#34255;&#30340;&#29289;&#20307;&#12290;&#36890;&#36807;&#27169;&#25311;&#20256;&#24863;&#22120;&#22122;&#22768;&#36827;&#34892;&#31471;&#21040;&#31471;&#35757;&#32451;&#65292;&#23454;&#29616;&#20102;&#33258;&#28982;&#20986;&#29616;&#30340;&#23398;&#20064;&#25512;&#21160;&#34892;&#20026;&#65292;&#24182;&#25104;&#21151;&#23558;&#20854;&#36801;&#31227;&#21040;&#23454;&#38469;&#30828;&#20214;&#19978;&#12290;</title><link>https://arxiv.org/abs/2402.04536</link><description>&lt;p&gt;
&#22522;&#20110;&#35302;&#35273;&#30340;&#20174;&#39063;&#31890;&#20171;&#36136;&#20013;&#26816;&#32034;&#29289;&#20307;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Tactile-based Object Retrieval From Granular Media
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04536
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#35302;&#35273;&#21453;&#39304;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#39063;&#31890;&#20171;&#36136;&#20013;&#26816;&#32034;&#22475;&#34255;&#30340;&#29289;&#20307;&#12290;&#36890;&#36807;&#27169;&#25311;&#20256;&#24863;&#22120;&#22122;&#22768;&#36827;&#34892;&#31471;&#21040;&#31471;&#35757;&#32451;&#65292;&#23454;&#29616;&#20102;&#33258;&#28982;&#20986;&#29616;&#30340;&#23398;&#20064;&#25512;&#21160;&#34892;&#20026;&#65292;&#24182;&#25104;&#21151;&#23558;&#20854;&#36801;&#31227;&#21040;&#23454;&#38469;&#30828;&#20214;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;GEOTACT&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#39063;&#31890;&#20171;&#36136;&#20013;&#26816;&#32034;&#22475;&#34255;&#30340;&#29289;&#20307;&#12290;&#36825;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#38656;&#35201;&#19982;&#39063;&#31890;&#20171;&#36136;&#36827;&#34892;&#20132;&#20114;&#65292;&#24182;&#19988;&#20165;&#20381;&#38752;&#35302;&#35273;&#21453;&#39304;&#26469;&#23436;&#25104;&#65292;&#22240;&#20026;&#19968;&#20010;&#22475;&#34255;&#30340;&#29289;&#20307;&#21487;&#33021;&#23436;&#20840;&#34987;&#35270;&#35273;&#38544;&#34255;&#12290;&#22312;&#36825;&#31181;&#29615;&#22659;&#20013;&#65292;&#35302;&#35273;&#21453;&#39304;&#26412;&#36523;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#38656;&#35201;&#19982;&#21608;&#22260;&#20171;&#36136;&#36827;&#34892;&#26222;&#36941;&#25509;&#35302;&#65292;&#24182;&#19988;&#30001;&#35302;&#35273;&#35835;&#25968;&#24341;&#36215;&#30340;&#22266;&#26377;&#22122;&#22768;&#27700;&#24179;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#31181;&#36890;&#36807;&#27169;&#25311;&#20256;&#24863;&#22120;&#22122;&#22768;&#36827;&#34892;&#31471;&#21040;&#31471;&#35757;&#32451;&#30340;&#23398;&#20064;&#26041;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#38382;&#39064;&#34920;&#36848;&#23548;&#33268;&#20102;&#23398;&#20064;&#25512;&#21160;&#34892;&#20026;&#30340;&#33258;&#28982;&#20986;&#29616;&#65292;&#25805;&#20316;&#22120;&#20351;&#29992;&#36825;&#20123;&#34892;&#20026;&#26469;&#20943;&#23569;&#19981;&#30830;&#23450;&#24615;&#24182;&#23558;&#29289;&#20307;&#24341;&#23548;&#21040;&#31283;&#23450;&#30340;&#25235;&#21462;&#20301;&#32622;&#65292;&#23613;&#31649;&#23384;&#22312;&#20551;&#30340;&#21644;&#22122;&#22768;&#30340;&#35302;&#35273;&#35835;&#25968;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#22521;&#35757;&#26041;&#26696;&#65292;&#21487;&#20197;&#22312;&#20223;&#30495;&#20013;&#23398;&#20064;&#36825;&#20123;&#34892;&#20026;&#65292;&#24182;&#22312;&#23454;&#38469;&#30828;&#20214;&#19978;&#36827;&#34892;&#38646;&#26679;&#26412;&#36801;&#31227;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;GEOTACT&#26159;&#31532;&#19968;&#20010;&#36825;&#26679;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce GEOTACT, a robotic manipulation method capable of retrieving objects buried in granular media. This is a challenging task due to the need to interact with granular media, and doing so based exclusively on tactile feedback, since a buried object can be completely hidden from vision. Tactile feedback is in itself challenging in this context, due to ubiquitous contact with the surrounding media, and the inherent noise level induced by the tactile readings. To address these challenges, we use a learning method trained end-to-end with simulated sensor noise. We show that our problem formulation leads to the natural emergence of learned pushing behaviors that the manipulator uses to reduce uncertainty and funnel the object to a stable grasp despite spurious and noisy tactile readings. We also introduce a training curriculum that enables learning these behaviors in simulation, followed by zero-shot transfer to real hardware. To the best of our knowledge, GEOTACT is the first meth
&lt;/p&gt;</description></item><item><title>HEAM&#26159;&#19968;&#31181;&#37319;&#29992;&#24322;&#26500;&#20869;&#23384;&#26550;&#26500;&#30340;&#26041;&#27861;&#65292;&#23558;3D&#22534;&#21472;DRAM&#19982;DIMM&#38598;&#25104;&#65292;&#29992;&#20110;&#21152;&#36895;&#22788;&#29702;&#22823;&#35268;&#27169;&#20010;&#24615;&#21270;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#23884;&#20837;&#25805;&#20316;&#12290;</title><link>https://arxiv.org/abs/2402.04032</link><description>&lt;p&gt;
HEAM: &#20351;&#29992;&#22788;&#29702;-&#20869;&#23384;&#36827;&#34892;&#25955;&#21015;&#23884;&#20837;&#21152;&#36895;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
HEAM : Hashed Embedding Acceleration using Processing-In-Memory
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04032
&lt;/p&gt;
&lt;p&gt;
HEAM&#26159;&#19968;&#31181;&#37319;&#29992;&#24322;&#26500;&#20869;&#23384;&#26550;&#26500;&#30340;&#26041;&#27861;&#65292;&#23558;3D&#22534;&#21472;DRAM&#19982;DIMM&#38598;&#25104;&#65292;&#29992;&#20110;&#21152;&#36895;&#22788;&#29702;&#22823;&#35268;&#27169;&#20010;&#24615;&#21270;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#23884;&#20837;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24403;&#20170;&#30340;&#25968;&#25454;&#20013;&#24515;&#20013;&#65292;&#20010;&#24615;&#21270;&#25512;&#33616;&#31995;&#32479;&#38754;&#20020;&#30528;&#35832;&#22810;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#22312;&#25191;&#34892;&#23884;&#20837;&#25805;&#20316;&#26102;&#38656;&#35201;&#22823;&#23481;&#37327;&#30340;&#20869;&#23384;&#21644;&#39640;&#24102;&#23485;&#12290;&#20043;&#21069;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;DIMM-based&#36817;&#20869;&#23384;&#22788;&#29702;&#25216;&#26415;&#25110;&#24341;&#20837;3D&#22534;&#21472;DRAM&#26469;&#35299;&#20915;&#20869;&#23384;&#38480;&#21046;&#21644;&#25193;&#23637;&#20869;&#23384;&#24102;&#23485;&#30340;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#35299;&#20915;&#26041;&#26696;&#22312;&#22788;&#29702;&#26085;&#30410;&#25193;&#22823;&#30340;&#20010;&#24615;&#21270;&#25512;&#33616;&#31995;&#32479;&#22823;&#23567;&#26102;&#23384;&#22312;&#19981;&#36275;&#20043;&#22788;&#12290;&#25512;&#33616;&#27169;&#22411;&#24050;&#32463;&#22686;&#38271;&#21040;&#36229;&#36807;&#25968;&#21313;TB&#30340;&#22823;&#23567;&#65292;&#23548;&#33268;&#22312;&#20256;&#32479;&#21333;&#33410;&#28857;&#25512;&#26029;&#26381;&#21153;&#22120;&#19978;&#39640;&#25928;&#36816;&#34892;&#21464;&#24471;&#22256;&#38590;&#12290;&#23613;&#31649;&#24050;&#32463;&#25552;&#20986;&#20102;&#21508;&#31181;&#31639;&#27861;&#26041;&#27861;&#26469;&#20943;&#23567;&#23884;&#20837;&#34920;&#23481;&#37327;&#65292;&#20294;&#36890;&#24120;&#20250;&#23548;&#33268;&#20869;&#23384;&#35775;&#38382;&#22686;&#21152;&#25110;&#20869;&#23384;&#36164;&#28304;&#21033;&#29992;&#20302;&#25928;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;HEAM&#65292;&#19968;&#31181;&#24322;&#26500;&#20869;&#23384;&#26550;&#26500;&#65292;&#23558;3D&#22534;&#21472;DRAM&#19982;DIMM&#38598;&#25104;&#22312;&#19968;&#36215;&#65292;&#20197;&#21152;&#36895;&#32452;&#21512;&#23884;&#20837;&#30340;&#25512;&#33616;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
In today's data centers, personalized recommendation systems face challenges such as the need for large memory capacity and high bandwidth, especially when performing embedding operations. Previous approaches have relied on DIMM-based near-memory processing techniques or introduced 3D-stacked DRAM to address memory-bound issues and expand memory bandwidth. However, these solutions fall short when dealing with the expanding size of personalized recommendation systems. Recommendation models have grown to sizes exceeding tens of terabytes, making them challenging to run efficiently on traditional single-node inference servers. Although various algorithmic methods have been proposed to reduce embedding table capacity, they often result in increased memory access or inefficient utilization of memory resources. This paper introduces HEAM, a heterogeneous memory architecture that integrates 3D-stacked DRAM with DIMM to accelerate recommendation systems in which compositional embedding is util
&lt;/p&gt;</description></item><item><title>SEABO&#26159;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#22522;&#20110;&#25628;&#32034;&#30340;&#31163;&#32447;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#65292;&#23427;&#20197;&#26080;&#30417;&#30563;&#23398;&#20064;&#30340;&#26041;&#24335;&#65292;&#26681;&#25454;&#19987;&#23478;&#25968;&#25454;&#21644;&#26080;&#26631;&#31614;&#25968;&#25454;&#24471;&#21040;&#22870;&#21169;&#20989;&#25968;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#24615;&#33021;&#19982;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30456;&#24403;&#12290;</title><link>https://arxiv.org/abs/2402.03807</link><description>&lt;p&gt;
SEABO: &#19968;&#31181;&#31616;&#21333;&#30340;&#22522;&#20110;&#25628;&#32034;&#30340;&#31163;&#32447;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
SEABO: A Simple Search-Based Method for Offline Imitation Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03807
&lt;/p&gt;
&lt;p&gt;
SEABO&#26159;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#22522;&#20110;&#25628;&#32034;&#30340;&#31163;&#32447;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#65292;&#23427;&#20197;&#26080;&#30417;&#30563;&#23398;&#20064;&#30340;&#26041;&#24335;&#65292;&#26681;&#25454;&#19987;&#23478;&#25968;&#25454;&#21644;&#26080;&#26631;&#31614;&#25968;&#25454;&#24471;&#21040;&#22870;&#21169;&#20989;&#25968;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#24615;&#33021;&#19982;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#30001;&#20110;&#33021;&#22815;&#20174;&#38745;&#24577;&#31163;&#32447;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#24182;&#28040;&#38500;&#19982;&#29615;&#22659;&#20132;&#20114;&#30340;&#38656;&#27714;&#65292;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#31163;&#32447;RL&#30340;&#25104;&#21151;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#21462;&#20915;&#20110;&#26631;&#26377;&#22870;&#21169;&#26631;&#31614;&#30340;&#31163;&#32447;&#36716;&#25442;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#25105;&#20204;&#32463;&#24120;&#38656;&#35201;&#25163;&#24037;&#35774;&#35745;&#22870;&#21169;&#20989;&#25968;&#65292;&#36825;&#26377;&#26102;&#26159;&#22256;&#38590;&#30340;&#12289;&#21171;&#21160;&#23494;&#38598;&#30340;&#25110;&#20302;&#25928;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25226;&#37325;&#28857;&#25918;&#22312;&#31163;&#32447;&#27169;&#20223;&#23398;&#20064;&#65288;IL&#65289;&#35774;&#32622;&#19978;&#65292;&#26088;&#22312;&#22522;&#20110;&#19987;&#23478;&#25968;&#25454;&#21644;&#26080;&#26631;&#31614;&#25968;&#25454;&#24471;&#21040;&#19968;&#20010;&#22870;&#21169;&#20989;&#25968;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#22522;&#20110;&#25628;&#32034;&#30340;&#31163;&#32447;IL&#26041;&#27861;&#65292;&#31216;&#20026;SEABO&#12290;SEABO&#20197;&#26080;&#30417;&#30563;&#23398;&#20064;&#30340;&#26041;&#24335;&#65292;&#23558;&#36739;&#22823;&#30340;&#22870;&#21169;&#20998;&#37197;&#32473;&#19982;&#19987;&#23478;&#28436;&#31034;&#20013;&#26368;&#25509;&#36817;&#30340;&#36716;&#25442;&#65292;&#21542;&#21017;&#20998;&#37197;&#36739;&#23567;&#30340;&#22870;&#21169;&#12290;&#22312;&#22810;&#20010;D4RL&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;SEABO&#33021;&#22815;&#36798;&#21040;&#19982;&#31163;&#32447;RL&#30456;&#24403;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Offline reinforcement learning (RL) has attracted much attention due to its ability in learning from static offline datasets and eliminating the need of interacting with the environment. Nevertheless, the success of offline RL relies heavily on the offline transitions annotated with reward labels. In practice, we often need to hand-craft the reward function, which is sometimes difficult, labor-intensive, or inefficient. To tackle this challenge, we set our focus on the offline imitation learning (IL) setting, and aim at getting a reward function based on the expert data and unlabeled data. To that end, we propose a simple yet effective search-based offline IL method, tagged SEABO. SEABO allocates a larger reward to the transition that is close to its closest neighbor in the expert demonstration, and a smaller reward otherwise, all in an unsupervised learning manner. Experimental results on a variety of D4RL datasets indicate that SEABO can achieve competitive performance to offline RL 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#27169;&#24577;&#26694;&#26550;MolTC&#65292;&#29992;&#20110;&#20998;&#23376;&#30456;&#20114;&#20316;&#29992;&#39044;&#27979;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#39640;&#25928;&#22320;&#25972;&#21512;&#20998;&#23376;&#23545;&#30340;&#20016;&#23500;&#22270;&#24418;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#24605;&#32500;&#38142;&#29702;&#35770;&#23454;&#29616;&#32479;&#19968;&#30340;&#20998;&#23376;&#20851;&#31995;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2402.03781</link><description>&lt;p&gt;
MolTC: &#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#36827;&#34892;&#20998;&#23376;&#20851;&#31995;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
MolTC: Towards Molecular Relational Modeling In Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03781
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#27169;&#24577;&#26694;&#26550;MolTC&#65292;&#29992;&#20110;&#20998;&#23376;&#30456;&#20114;&#20316;&#29992;&#39044;&#27979;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#39640;&#25928;&#22320;&#25972;&#21512;&#20998;&#23376;&#23545;&#30340;&#20016;&#23500;&#22270;&#24418;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#24605;&#32500;&#38142;&#29702;&#35770;&#23454;&#29616;&#32479;&#19968;&#30340;&#20998;&#23376;&#20851;&#31995;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#23376;&#20851;&#31995;&#23398;&#20064;&#65288;MRL&#65289;&#26088;&#22312;&#29702;&#35299;&#20998;&#23376;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#22312;&#25512;&#36827;&#29983;&#29289;&#21270;&#23398;&#30740;&#31350;&#26041;&#38754;&#36215;&#21040;&#20102;&#20851;&#38190;&#20316;&#29992;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#37319;&#29992;&#24050;&#25104;&#20026;&#19968;&#31181;&#26377;&#25928;&#21644;&#39640;&#25928;&#30340;MRL&#26041;&#27861;&#65292;&#36825;&#20123;&#27169;&#22411;&#20197;&#20854;&#24222;&#22823;&#30340;&#30693;&#35782;&#23384;&#20648;&#24211;&#21644;&#20808;&#36827;&#30340;&#36923;&#36753;&#25512;&#29702;&#33021;&#21147;&#32780;&#38395;&#21517;&#12290;&#23613;&#31649;&#20855;&#26377;&#28508;&#21147;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#25991;&#26412;&#25968;&#25454;&#65292;&#22240;&#27492;&#27809;&#26377;&#20805;&#20998;&#21033;&#29992;&#20998;&#23376;&#22270;&#20013;&#22266;&#26377;&#30340;&#20016;&#23500;&#32467;&#26500;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#32570;&#20047;&#32479;&#19968;&#30340;&#26694;&#26550;&#21152;&#21095;&#20102;&#20449;&#24687;&#30340;&#28010;&#36153;&#65292;&#22240;&#20026;&#23427;&#38459;&#30861;&#20102;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#20043;&#38388;&#20849;&#20139;&#23398;&#20064;&#21040;&#30340;&#30456;&#20114;&#20316;&#29992;&#29702;&#30001;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#22810;&#27169;&#24577;&#26694;&#26550;&#65292;&#29992;&#20110;&#26681;&#25454;&#24605;&#32500;&#38142;&#65288;CoT&#65289;&#29702;&#35770;&#23545;&#20998;&#23376;&#30456;&#20114;&#20316;&#29992;&#36827;&#34892;&#39044;&#27979;&#65292;&#31216;&#20026;MolTC&#65292;&#23427;&#21487;&#20197;&#39640;&#25928;&#22320;&#25972;&#21512;&#20998;&#23376;&#23545;&#30340;&#20016;&#23500;&#22270;&#24418;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Molecular Relational Learning (MRL), aiming to understand interactions between molecular pairs, plays a pivotal role in advancing biochemical research. Recently, the adoption of large language models (LLMs), known for their vast knowledge repositories and advanced logical inference capabilities, has emerged as a promising way for efficient and effective MRL. Despite their potential, these methods predominantly rely on the textual data, thus not fully harnessing the wealth of structural information inherent in molecular graphs. Moreover, the absence of a unified framework exacerbates the information underutilization, as it hinders the sharing of interaction rationale learned across diverse datasets. To address these challenges, this work proposes a novel LLM-based multi-modal framework for Molecular inTeraction prediction following Chain-of-Thought (CoT) theory, termed MolTC, which can efficiently integrate rich graphical information of molecular pairs. For achieving a unified MRL, MolT
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#40657;&#30418;&#23376;&#21477;&#32423;&#25915;&#20987;&#20013;&#21033;&#29992;&#31867;&#21035;&#27010;&#29575;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#36827;&#34892;&#25915;&#20987;&#12290;&#36890;&#36807;&#19982;&#22522;&#32447;&#26041;&#27861;&#36827;&#34892;&#23545;&#27604;&#65292;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#35780;&#20272;&#12290;</title><link>https://arxiv.org/abs/2402.02695</link><description>&lt;p&gt;
&#21033;&#29992;&#31867;&#21035;&#27010;&#29575;&#36827;&#34892;&#40657;&#30418;&#23376;&#21477;&#32423;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Exploiting Class Probabilities for Black-box Sentence-level Attacks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02695
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#40657;&#30418;&#23376;&#21477;&#32423;&#25915;&#20987;&#20013;&#21033;&#29992;&#31867;&#21035;&#27010;&#29575;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#36827;&#34892;&#25915;&#20987;&#12290;&#36890;&#36807;&#19982;&#22522;&#32447;&#26041;&#27861;&#36827;&#34892;&#23545;&#27604;&#65292;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21477;&#32423;&#25915;&#20987;&#26159;&#38024;&#23545;&#25991;&#26412;&#20998;&#31867;&#22120;&#30340;&#23545;&#25239;&#24615;&#21477;&#23376;&#29983;&#25104;&#26041;&#27861;&#65292;&#36825;&#20123;&#21477;&#23376;&#19982;&#27491;&#30830;&#20998;&#31867;&#30340;&#21477;&#23376;&#21516;&#20041;&#65292;&#20294;&#34987;&#20998;&#31867;&#22120;&#38169;&#35823;&#22320;&#20998;&#31867;&#12290;&#22312;&#40657;&#30418;&#35774;&#32622;&#19979;&#65292;&#20998;&#31867;&#22120;&#21482;&#33021;&#36890;&#36807;&#23545;&#26597;&#35810;&#36755;&#20837;&#30340;&#21453;&#39304;&#36827;&#34892;&#35775;&#38382;&#65292;&#36825;&#20027;&#35201;&#20197;&#31867;&#21035;&#27010;&#29575;&#30340;&#24418;&#24335;&#25552;&#20379;&#12290;&#23613;&#31649;&#21033;&#29992;&#31867;&#21035;&#27010;&#29575;&#21487;&#20197;&#33719;&#24471;&#26356;&#24378;&#22823;&#30340;&#25915;&#20987;&#25928;&#26524;&#65292;&#20294;&#30001;&#20110;&#22312;&#21477;&#32423;&#25915;&#20987;&#20013;&#20351;&#29992;&#31867;&#21035;&#27010;&#29575;&#23384;&#22312;&#25361;&#25112;&#65292;&#29616;&#26377;&#30340;&#25915;&#20987;&#26041;&#27861;&#35201;&#20040;&#19981;&#20351;&#29992;&#21453;&#39304;&#65292;&#35201;&#20040;&#20165;&#20351;&#29992;&#31867;&#21035;&#26631;&#31614;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#20351;&#29992;&#31867;&#21035;&#27010;&#29575;&#36827;&#34892;&#40657;&#30418;&#21477;&#32423;&#25915;&#20987;&#65292;&#24182;&#30740;&#31350;&#20102;&#22312;&#25915;&#20987;&#25104;&#21151;&#29575;&#19978;&#20351;&#29992;&#31867;&#21035;&#27010;&#29575;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#25506;&#35752;&#20102;&#22312;&#40657;&#30418;&#21477;&#32423;&#25915;&#20987;&#20013;&#20351;&#29992;&#31867;&#21035;&#27010;&#29575;&#26159;&#21542;&#20540;&#24471;&#25110;&#21487;&#34892;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#20998;&#31867;&#22120;&#21644;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23545;&#25552;&#20986;&#30340;&#25915;&#20987;&#26041;&#27861;&#36827;&#34892;&#20102;&#24191;&#27867;&#35780;&#20272;&#65292;&#24182;&#19982;&#22522;&#32447;&#36827;&#34892;&#20102;&#23545;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sentence-level attacks craft adversarial sentences that are synonymous with correctly-classified sentences but are misclassified by the text classifiers. Under the black-box setting, classifiers are only accessible through their feedback to queried inputs, which is predominately available in the form of class probabilities. Even though utilizing class probabilities results in stronger attacks, due to the challenges of using them for sentence-level attacks, existing attacks use either no feedback or only the class labels. Overcoming the challenges, we develop a novel algorithm that uses class probabilities for black-box sentence-level attacks, investigate the effectiveness of using class probabilities on the attack's success, and examine the question if it is worthy or practical to use class probabilities by black-box sentence-level attacks. We conduct extensive evaluations of the proposed attack comparing with the baselines across various classifiers and benchmark datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CTAug&#30340;&#26032;&#26694;&#26550;&#65292;&#23558;&#20869;&#32858;&#23376;&#22270;&#24847;&#35782;&#26080;&#32541;&#25972;&#21512;&#21040;&#22270;&#23545;&#27604;&#23398;&#20064;&#20013;&#12290;&#36890;&#36807;&#25913;&#36827;&#22270;&#25299;&#25169;&#22686;&#24378;&#21644;&#22270;&#23398;&#20064;&#36807;&#31243;&#65292;&#25552;&#39640;&#20102;&#23545;&#21508;&#31181;&#22270;&#30340;&#34920;&#24449;&#23398;&#20064;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2401.17580</link><description>&lt;p&gt;
&#20855;&#26377;&#20869;&#32858;&#23376;&#22270;&#24847;&#35782;&#30340;&#22270;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Graph Contrastive Learning with Cohesive Subgraph Awareness
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17580
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CTAug&#30340;&#26032;&#26694;&#26550;&#65292;&#23558;&#20869;&#32858;&#23376;&#22270;&#24847;&#35782;&#26080;&#32541;&#25972;&#21512;&#21040;&#22270;&#23545;&#27604;&#23398;&#20064;&#20013;&#12290;&#36890;&#36807;&#25913;&#36827;&#22270;&#25299;&#25169;&#22686;&#24378;&#21644;&#22270;&#23398;&#20064;&#36807;&#31243;&#65292;&#25552;&#39640;&#20102;&#23545;&#21508;&#31181;&#22270;&#30340;&#34920;&#24449;&#23398;&#20064;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#23545;&#27604;&#23398;&#20064;&#65288;GCL&#65289;&#24050;&#25104;&#20026;&#23398;&#20064;&#21508;&#31181;&#22270;&#34920;&#24449;&#30340;&#20808;&#36827;&#31574;&#30053;&#65292;&#21253;&#25324;&#31038;&#20132;&#21644;&#29983;&#29289;&#21307;&#23398;&#32593;&#32476;&#12290;GCL&#24191;&#27867;&#20351;&#29992;&#38543;&#26426;&#22270;&#25299;&#25169;&#22686;&#24378;&#65292;&#22914;&#22343;&#21248;&#33410;&#28857;&#20002;&#22833;&#65292;&#29983;&#25104;&#22686;&#24378;&#22270;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#38543;&#26426;&#22686;&#24378;&#21487;&#33021;&#20005;&#37325;&#25439;&#23475;&#22270;&#30340;&#20869;&#22312;&#23646;&#24615;&#24182;&#24694;&#21270;&#21518;&#32493;&#30340;&#34920;&#24449;&#23398;&#20064;&#36807;&#31243;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#22312;&#22270;&#22686;&#24378;&#21644;&#23398;&#20064;&#36807;&#31243;&#20013;&#24341;&#20837;&#20869;&#32858;&#23376;&#22270;&#24847;&#35782;&#26377;&#21487;&#33021;&#25552;&#39640;GCL&#30340;&#24615;&#33021;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;CTAug&#30340;&#26032;&#39062;&#32479;&#19968;&#26694;&#26550;&#65292;&#20197;&#26080;&#32541;&#22320;&#23558;&#20869;&#32858;&#24847;&#35782;&#25972;&#21512;&#21040;&#21508;&#31181;&#29616;&#26377;&#30340;GCL&#26426;&#21046;&#20013;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;CTAug&#21253;&#25324;&#20004;&#20010;&#19987;&#38376;&#30340;&#27169;&#22359;&#65306;&#25299;&#25169;&#22686;&#24378;&#22686;&#24378;&#21644;&#22270;&#23398;&#20064;&#22686;&#24378;&#12290;&#21069;&#32773;&#29983;&#25104;&#35880;&#24910;&#20445;&#30041;&#20869;&#32858;&#24615;&#36136;&#30340;&#22686;&#24378;&#22270;&#65292;&#32780;&#21518;&#32773;&#22686;&#24378;&#20102;&#22270;&#30340;&#23398;&#20064;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph contrastive learning (GCL) has emerged as a state-of-the-art strategy for learning representations of diverse graphs including social and biomedical networks. GCL widely uses stochastic graph topology augmentation, such as uniform node dropping, to generate augmented graphs. However, such stochastic augmentations may severely damage the intrinsic properties of a graph and deteriorate the following representation learning process. We argue that incorporating an awareness of cohesive subgraphs during the graph augmentation and learning processes has the potential to enhance GCL performance. To this end, we propose a novel unified framework called CTAug, to seamlessly integrate cohesion awareness into various existing GCL mechanisms. In particular, CTAug comprises two specialized modules: topology augmentation enhancement and graph learning enhancement. The former module generates augmented graphs that carefully preserve cohesion properties, while the latter module bolsters the grap
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#24773;&#24863;&#24605;&#32500;&#38142;&#65288;ECoT&#65289;&#30340;&#25552;&#31034;&#26041;&#27861;&#65292;&#36890;&#36807;&#19982;&#20154;&#31867;&#24773;&#24863;&#26234;&#24935;&#20934;&#21017;&#23545;&#40784;&#65292;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24773;&#24863;&#29983;&#25104;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2401.06836</link><description>&lt;p&gt;
&#36890;&#36807;&#24773;&#32490;&#24605;&#32500;&#38142;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24773;&#32490;&#29983;&#25104;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Enhancing Emotional Generation Capability of Large Language Models via Emotional Chain-of-Thought
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.06836
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#24773;&#24863;&#24605;&#32500;&#38142;&#65288;ECoT&#65289;&#30340;&#25552;&#31034;&#26041;&#27861;&#65292;&#36890;&#36807;&#19982;&#20154;&#31867;&#24773;&#24863;&#26234;&#24935;&#20934;&#21017;&#23545;&#40784;&#65292;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24773;&#24863;&#29983;&#25104;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#24773;&#32490;&#35782;&#21035;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#24341;&#36215;&#20102;&#30740;&#31350;&#30028;&#23545;&#25506;&#32034;&#23427;&#20204;&#22312;&#24773;&#24863;&#26234;&#33021;&#20013;&#28508;&#21147;&#30340;&#22909;&#22855;&#24515;&#12290;&#28982;&#32780;&#65292;&#24773;&#24863;&#29983;&#25104;&#20219;&#21153;&#39046;&#22495;&#20173;&#23384;&#22312;&#19968;&#20123;&#38382;&#39064;&#65292;&#21253;&#25324;&#20154;&#31867;&#20559;&#22909;&#30340;&#23545;&#40784;&#21644;&#24773;&#24863;&#29983;&#25104;&#35780;&#20272;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#24773;&#24863;&#24605;&#32500;&#38142;&#65288;ECoT&#65289;&#30340;&#21363;&#25554;&#21363;&#29992;&#25552;&#31034;&#26041;&#27861;&#65292;&#36890;&#36807;&#19982;&#20154;&#31867;&#24773;&#24863;&#26234;&#21147;&#25351;&#21335;&#23545;&#40784;&#26469;&#22686;&#24378;LLMs&#22312;&#21508;&#31181;&#24773;&#24863;&#29983;&#25104;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#12290;&#20026;&#20102;&#35780;&#20272;ECoT&#30340;&#21487;&#38752;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#24773;&#24863;&#29983;&#25104;&#24471;&#20998;&#65288;EGS&#65289;&#30340;&#33258;&#21160;&#21270;&#22522;&#20110;&#27169;&#22411;&#30340;&#35780;&#20272;&#26041;&#27861;&#12290;EGS&#23558;&#25096;&#23572;&#26364;&#30340;&#24773;&#32490;&#26234;&#21147;&#29702;&#35770;&#20316;&#20026;&#20154;&#31867;&#19987;&#23478;&#20849;&#35782;&#65292;&#20026;&#24773;&#24863;&#29983;&#25104;&#20219;&#21153;&#30340;&#35780;&#20272;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#12290;&#22823;&#37327;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.06836v2 Announce Type: replace-cross  Abstract: Large Language Models (LLMs) have shown remarkable performance in various emotion recognition tasks, thereby piquing the research community's curiosity for exploring their potential in emotional intelligence. However, several issues in the field of emotional generation tasks remain unresolved, including human preference alignment and emotional generation assessment. In this paper, we propose the Emotional Chain-of-Thought (ECoT), a plug-and-play prompting method that enhances the performance of LLMs on various emotional generation tasks by aligning with human emotional intelligence guidelines. To assess the reliability of ECoT, we propose an automated model-based evaluation method called Emotional Generation Score (EGS). EGS incorporates Goleman's Emotional Intelligence Theory as a consensus of human experts, providing a new perspective on the evaluation of emotional generation tasks. Extensive experimental results demonstrate 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#22522;&#20110;&#22810;&#23618;&#20132;&#21449;&#27880;&#24847;&#21147;&#34701;&#21512;&#30340;&#38899;&#39057;-&#35270;&#35273;&#35821;&#38899;&#35782;&#21035;&#65288;MLCA-AVSR&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#19981;&#21516;&#32423;&#21035;&#30340;&#38899;&#39057;/&#35270;&#35273;&#32534;&#30721;&#22120;&#19978;&#34701;&#21512;&#27169;&#24577;&#29305;&#24449;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#31995;&#32479;&#30340;&#31283;&#20581;&#24615;&#12290;</title><link>https://arxiv.org/abs/2401.03424</link><description>&lt;p&gt;
&#22522;&#20110;&#22810;&#23618;&#20132;&#21449;&#27880;&#24847;&#21147;&#34701;&#21512;&#30340;&#38899;&#39057;-&#35270;&#35273;&#35821;&#38899;&#35782;&#21035;&#65288;MLCA-AVSR&#65289;
&lt;/p&gt;
&lt;p&gt;
MLCA-AVSR: Multi-Layer Cross Attention Fusion based Audio-Visual Speech Recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.03424
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#22522;&#20110;&#22810;&#23618;&#20132;&#21449;&#27880;&#24847;&#21147;&#34701;&#21512;&#30340;&#38899;&#39057;-&#35270;&#35273;&#35821;&#38899;&#35782;&#21035;&#65288;MLCA-AVSR&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#19981;&#21516;&#32423;&#21035;&#30340;&#38899;&#39057;/&#35270;&#35273;&#32534;&#30721;&#22120;&#19978;&#34701;&#21512;&#27169;&#24577;&#29305;&#24449;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#31995;&#32479;&#30340;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#31995;&#32479;&#22312;&#22024;&#26434;&#29615;&#22659;&#20013;&#26126;&#26174;&#36864;&#21270;&#65292;&#32780;&#38899;&#39057;-&#35270;&#35273;&#35821;&#38899;&#35782;&#21035;&#65288;AVSR&#65289;&#31995;&#32479;&#26088;&#22312;&#29992;&#25239;&#22122;&#38899;&#30340;&#35270;&#35273;&#32447;&#32034;&#34917;&#20805;&#38899;&#39057;&#27969;&#65292;&#24182;&#25552;&#39640;&#31995;&#32479;&#30340;&#31283;&#20581;&#24615;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#34701;&#21512;&#22909;&#23398;&#20064;&#30340;&#27169;&#24577;&#29305;&#24449;&#65292;&#22914;&#27169;&#24577;&#29305;&#23450;&#32534;&#30721;&#22120;&#30340;&#36755;&#20986;&#65292;&#32780;&#27809;&#26377;&#32771;&#34385;&#27169;&#24577;&#29305;&#24449;&#23398;&#20064;&#26399;&#38388;&#30340;&#19978;&#19979;&#25991;&#20851;&#31995;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#23618;&#20132;&#21449;&#27880;&#24847;&#21147;&#34701;&#21512;&#30340;AVSR&#65288;MLCA-AVSR&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#19981;&#21516;&#32423;&#21035;&#30340;&#38899;&#39057;/&#35270;&#35273;&#32534;&#30721;&#22120;&#19978;&#34701;&#21512;&#23427;&#20204;&#26469;&#20419;&#36827;&#27599;&#20010;&#27169;&#24577;&#30340;&#34920;&#31034;&#23398;&#20064;&#12290;&#23545;MISP2022-AVSR&#25361;&#25112;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#31995;&#32479;&#30340;&#26377;&#25928;&#24615;&#65292;&#22312;Eval&#38598;&#19978;&#23454;&#29616;&#20102;30.57%&#30340;&#25340;&#25509;&#26368;&#23567;&#32622;&#25442;&#23383;&#31526;&#35823;&#24046;&#29575;&#65288;cpCER&#65289;&#65292;&#30456;&#23545;&#20110;&#25105;&#20204;&#30340;p&#21462;&#24471;&#20102;&#39640;&#36798;3.17%&#30340;&#30456;&#23545;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.03424v2 Announce Type: replace-cross  Abstract: While automatic speech recognition (ASR) systems degrade significantly in noisy environments, audio-visual speech recognition (AVSR) systems aim to complement the audio stream with noise-invariant visual cues and improve the system's robustness. However, current studies mainly focus on fusing the well-learned modality features, like the output of modality-specific encoders, without considering the contextual relationship during the modality feature learning. In this study, we propose a multi-layer cross-attention fusion based AVSR (MLCA-AVSR) approach that promotes representation learning of each modality by fusing them at different levels of audio/visual encoders. Experimental results on the MISP2022-AVSR Challenge dataset show the efficacy of our proposed system, achieving a concatenated minimum permutation character error rate (cpCER) of 30.57% on the Eval set and yielding up to 3.17% relative improvement compared with our p
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#35843;&#26597;&#20102;&#20256;&#32479;&#21644;&#29616;&#20195;&#28040;&#24687;&#20195;&#29702;&#65292;&#27604;&#36739;&#20998;&#26512;&#20102;&#27969;&#34892;&#24179;&#21488;&#65292;&#20026;&#25968;&#25454;&#20013;&#24515;GenAI&#27169;&#22411;&#30340;&#38656;&#27714;&#22686;&#21152;&#25552;&#20379;&#20102;&#20581;&#22766;&#30340;&#25968;&#25454;&#36890;&#20449;&#22522;&#30784;&#35774;&#26045;</title><link>https://arxiv.org/abs/2312.14647</link><description>&lt;p&gt;
&#38754;&#21521;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#30340;&#28040;&#24687;&#20195;&#29702;&#65306;&#35843;&#30740;&#12289;&#25361;&#25112;&#21644;&#26426;&#36935;
&lt;/p&gt;
&lt;p&gt;
Towards Message Brokers for Generative AI: Survey, Challenges, and Opportunities
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.14647
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#35843;&#26597;&#20102;&#20256;&#32479;&#21644;&#29616;&#20195;&#28040;&#24687;&#20195;&#29702;&#65292;&#27604;&#36739;&#20998;&#26512;&#20102;&#27969;&#34892;&#24179;&#21488;&#65292;&#20026;&#25968;&#25454;&#20013;&#24515;GenAI&#27169;&#22411;&#30340;&#38656;&#27714;&#22686;&#21152;&#25552;&#20379;&#20102;&#20581;&#22766;&#30340;&#25968;&#25454;&#36890;&#20449;&#22522;&#30784;&#35774;&#26045;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24403;&#20170;&#25968;&#23383;&#21270;&#19990;&#30028;&#20013;&#65292;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#65288;GenAI&#65289;&#22914;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#27491;&#21464;&#24471;&#36234;&#26469;&#36234;&#26222;&#36941;&#65292;&#25193;&#23637;&#20854;&#24433;&#21709;&#33539;&#22260;&#33267;&#21508;&#31181;&#24212;&#29992;&#12290;&#36825;&#31181;&#37319;&#29992;&#28608;&#22686;&#24341;&#21457;&#20102;&#23545;&#25968;&#25454;&#20013;&#24515;GenAI&#27169;&#22411;&#30340;&#38656;&#27714;&#26174;&#33879;&#22686;&#21152;&#65292;&#31361;&#26174;&#20986;&#20581;&#22766;&#30340;&#25968;&#25454;&#36890;&#20449;&#22522;&#30784;&#35774;&#26045;&#30340;&#24517;&#35201;&#24615;&#12290;&#28040;&#24687;&#20195;&#29702;&#22312;&#36825;&#19968;&#38656;&#27714;&#20013;&#33267;&#20851;&#37325;&#35201;&#65292;&#23427;&#20204;&#20316;&#20026;&#21508;&#20010;&#31995;&#32479;&#32452;&#20214;&#20043;&#38388;&#25968;&#25454;&#20256;&#36755;&#30340;&#37325;&#35201;&#36890;&#36947;&#12290;&#26412;&#35843;&#26597;&#26088;&#22312;&#28145;&#20837;&#20998;&#26512;&#20256;&#32479;&#21644;&#29616;&#20195;&#28040;&#24687;&#20195;&#29702;&#65292;&#25552;&#20379;&#27969;&#34892;&#24179;&#21488;&#30340;&#27604;&#36739;&#30740;&#31350;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32771;&#34385;&#20102;&#35768;&#22810;&#26631;&#20934;&#65292;&#21253;&#25324;&#20294;&#19981;&#38480;&#20110;&#24320;&#28304;&#21487;&#29992;&#24615;&#12289;&#38598;&#25104;&#30417;&#25511;&#24037;&#20855;&#12289;&#28040;&#24687;&#20248;&#20808;&#32423;&#26426;&#21046;&#12289;&#24182;&#34892;&#22788;&#29702;&#33021;&#21147;&#12289;&#21487;&#38752;&#24615;&#12289;&#20998;&#21457;&#21644;&#38598;&#32676;&#21151;&#33021;&#12289;&#35748;&#35777;&#27969;&#31243;&#12289;&#25968;&#25454;&#25345;&#20037;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.14647v2 Announce Type: replace-cross  Abstract: In today's digital world, Generative Artificial Intelligence (GenAI) such as Large Language Models (LLMs) is becoming increasingly prevalent, extending its reach across diverse applications. This surge in adoption has sparked a significant increase in demand for data-centric GenAI models, highlighting the necessity for robust data communication infrastructures. Central to this need are message brokers, which serve as essential channels for data transfer within various system components. This survey aims to delve into a comprehensive analysis of traditional and modern message brokers, offering a comparative study of prevalent platforms. Our study considers numerous criteria including, but not limited to, open-source availability, integrated monitoring tools, message prioritization mechanisms, capabilities for parallel processing, reliability, distribution and clustering functionalities, authentication processes, data persistence
&lt;/p&gt;</description></item><item><title>&#20998;&#26512;&#20102;&#19978;&#19979;&#25991;&#24863;&#30693;&#39046;&#22495;&#27867;&#21270;&#30340;&#26465;&#20214;&#65292;&#25552;&#20986;&#20102;&#29702;&#35770;&#20998;&#26512;&#21644;&#23454;&#35777;&#20998;&#26512;&#25152;&#38656;&#30340;&#26631;&#20934;&#65292;&#24182;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#21487;&#20197;&#26816;&#27979;&#38750;&#24120;&#25968;&#22495;&#30340;&#22330;&#26223;&#12290;</title><link>https://arxiv.org/abs/2312.10107</link><description>&lt;p&gt;
&#36808;&#21521;&#38754;&#21521;&#19978;&#19979;&#25991;&#24863;&#30693;&#39046;&#22495;&#27867;&#21270;&#65306;&#29702;&#35299;&#36793;&#32536;&#20256;&#36882;&#23398;&#20064;&#30340;&#22909;&#22788;&#21644;&#38480;&#21046;
&lt;/p&gt;
&lt;p&gt;
Towards Context-Aware Domain Generalization: Understanding the Benefits and Limits of Marginal Transfer Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.10107
&lt;/p&gt;
&lt;p&gt;
&#20998;&#26512;&#20102;&#19978;&#19979;&#25991;&#24863;&#30693;&#39046;&#22495;&#27867;&#21270;&#30340;&#26465;&#20214;&#65292;&#25552;&#20986;&#20102;&#29702;&#35770;&#20998;&#26512;&#21644;&#23454;&#35777;&#20998;&#26512;&#25152;&#38656;&#30340;&#26631;&#20934;&#65292;&#24182;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#21487;&#20197;&#26816;&#27979;&#38750;&#24120;&#25968;&#22495;&#30340;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#20851;&#20110;&#36755;&#20837;$X$&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#22914;&#20309;&#25913;&#21892;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#26032;&#39046;&#22495;&#20013;&#30340;&#39044;&#27979;&#30340;&#26465;&#20214;&#12290;&#22312;&#39046;&#22495;&#27867;&#21270;&#20013;&#36793;&#32536;&#20256;&#36882;&#23398;&#20064;&#30340;&#30740;&#31350;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#23558;&#19978;&#19979;&#25991;&#30340;&#27010;&#24565;&#24418;&#24335;&#21270;&#20026;&#19968;&#32452;&#25968;&#25454;&#28857;&#30340;&#25490;&#21015;&#19981;&#21464;&#34920;&#31034;&#65292;&#36825;&#20123;&#25968;&#25454;&#28857;&#26469;&#33258;&#20110;&#19982;&#36755;&#20837;&#26412;&#36523;&#30456;&#21516;&#30340;&#22495;&#12290;&#25105;&#20204;&#23545;&#36825;&#31181;&#26041;&#27861;&#22312;&#21407;&#21017;&#19978;&#21487;&#20197;&#20135;&#29983;&#22909;&#22788;&#30340;&#26465;&#20214;&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#65292;&#24182;&#21046;&#23450;&#20102;&#20004;&#20010;&#22312;&#23454;&#36341;&#20013;&#21487;&#20197;&#36731;&#26494;&#39564;&#35777;&#30340;&#24517;&#35201;&#26631;&#20934;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20851;&#20110;&#36793;&#32536;&#20256;&#36882;&#23398;&#20064;&#26041;&#27861;&#26377;&#26395;&#20855;&#26377;&#31283;&#20581;&#24615;&#30340;&#20998;&#24067;&#21464;&#21270;&#31867;&#22411;&#30340;&#35265;&#35299;&#12290;&#23454;&#35777;&#20998;&#26512;&#34920;&#26126;&#25105;&#20204;&#30340;&#26631;&#20934;&#26377;&#25928;&#22320;&#21306;&#20998;&#20102;&#26377;&#21033;&#21644;&#19981;&#21033;&#30340;&#22330;&#26223;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#21487;&#20197;&#21487;&#38752;&#22320;&#26816;&#27979;&#27169;&#22411;&#38754;&#20020;&#38750;&#24120;&#25968;&#22495;&#30340;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.10107v2 Announce Type: replace-cross  Abstract: In this work, we analyze the conditions under which information about the context of an input $X$ can improve the predictions of deep learning models in new domains. Following work in marginal transfer learning in Domain Generalization (DG), we formalize the notion of context as a permutation-invariant representation of a set of data points that originate from the same domain as the input itself. We offer a theoretical analysis of the conditions under which this approach can, in principle, yield benefits, and formulate two necessary criteria that can be easily verified in practice. Additionally, we contribute insights into the kind of distribution shifts for which the marginal transfer learning approach promises robustness. Empirical analysis shows that our criteria are effective in discerning both favorable and unfavorable scenarios. Finally, we demonstrate that we can reliably detect scenarios where a model is tasked with unw
&lt;/p&gt;</description></item><item><title>&#22312;&#20020;&#24202;&#25991;&#26412;&#30340;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#30740;&#31350;&#20013;&#65292;&#36890;&#36807;&#20351;&#29992;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;ClinSpEn-2022&#33521;&#35199;&#20020;&#24202;&#39046;&#22495;&#25968;&#25454;&#19978;&#21462;&#24471;&#20102;&#39030;&#32423;&#24615;&#33021;&#65292;&#24182;&#21457;&#29616;&#23567;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#20020;&#24202;&#39046;&#22495;&#24494;&#35843;&#20013;&#32988;&#36807;&#20854;&#20182;&#36229;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2312.07250</link><description>&lt;p&gt;
&#20020;&#24202;&#25991;&#26412;&#30340;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65306;&#23545;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#36801;&#31227;&#23398;&#20064;&#30340;&#32463;&#39564;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Neural Machine Translation of Clinical Text: An Empirical Investigation into Multilingual Pre-Trained Language Models and Transfer-Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.07250
&lt;/p&gt;
&lt;p&gt;
&#22312;&#20020;&#24202;&#25991;&#26412;&#30340;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#30740;&#31350;&#20013;&#65292;&#36890;&#36807;&#20351;&#29992;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;ClinSpEn-2022&#33521;&#35199;&#20020;&#24202;&#39046;&#22495;&#25968;&#25454;&#19978;&#21462;&#24471;&#20102;&#39030;&#32423;&#24615;&#33021;&#65292;&#24182;&#21457;&#29616;&#23567;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#20020;&#24202;&#39046;&#22495;&#24494;&#35843;&#20013;&#32988;&#36807;&#20854;&#20182;&#36229;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#36890;&#36807;&#26816;&#39564;&#20351;&#29992;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#22810;&#35821;&#35328;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#22914;&#22522;&#20110;Transformer&#32467;&#26500;&#30340;&#27169;&#22411;&#65292;&#22312;&#20020;&#24202;&#25991;&#26412;&#26426;&#22120;&#32763;&#35793;&#26041;&#38754;&#36827;&#34892;&#20102;&#35843;&#26597;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#35299;&#20915;&#35821;&#35328;&#36164;&#28304;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#25105;&#20204;&#36824;&#20351;&#29992;&#22522;&#20110;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;MMPLMs&#65289;&#30340;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;&#22312;&#20020;&#24202;&#26696;&#20363;&#65288;CC&#65289;&#12289;&#20020;&#24202;&#26415;&#35821;&#65288;CT&#65289;&#21644;&#26412;&#20307;&#27010;&#24565;&#65288;OC&#65289;&#31561;&#19977;&#20010;&#23376;&#20219;&#21153;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;ClinSpEn-2022&#33521;&#35199;&#20020;&#24202;&#39046;&#22495;&#25968;&#25454;&#20849;&#20139;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#39030;&#32423;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22522;&#20110;&#19987;&#23478;&#30340;&#20154;&#24037;&#35780;&#20272;&#26174;&#31034;&#65292;&#22312;&#20020;&#24202;&#39046;&#22495;&#24494;&#35843;&#20013;&#65292;&#23567;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#26126;&#26174;&#32988;&#36807;&#20854;&#20182;&#20004;&#20010;&#36229;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36825;&#19968;&#21457;&#29616;&#22312;&#35813;&#39046;&#22495;&#20174;&#26410;&#26377;&#36807;&#25253;&#36947;&#12290;&#26368;&#21518;&#65292;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#34920;&#29616;&#20986;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.07250v2 Announce Type: replace-cross  Abstract: We conduct investigations on clinical text machine translation by examining multilingual neural network models using deep learning such as Transformer based structures. Furthermore, to address the language resource imbalance issue, we also carry out experiments using a transfer learning methodology based on massive multilingual pre-trained language models (MMPLMs). The experimental results on three subtasks including 1) clinical case (CC), 2) clinical terminology (CT), and 3) ontological concept (OC) show that our models achieved top-level performances in the ClinSpEn-2022 shared task on English-Spanish clinical domain data. Furthermore, our expert-based human evaluations demonstrate that the small-sized pre-trained language model (PLM) won over the other two extra-large language models by a large margin, in the clinical domain fine-tuning, which finding was never reported in the field. Finally, the transfer learning method wor
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#20855;&#26377;&#22823;&#37327;&#21160;&#20316;&#30340;&#37325;&#22797;&#28216;&#25103;&#20013;&#23454;&#29616;&#36951;&#25022;&#26368;&#23567;&#21270;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;Oracle&#30340;&#31639;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#20869;&#37096;&#36951;&#25022;&#26368;&#23567;&#21270;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#22312;&#22823;&#22411;&#28216;&#25103;&#20013;&#35745;&#31639;&#30456;&#20851;&#22343;&#34913;&#30340;&#39640;&#25928;&#24615;&#12290;&#36890;&#36807;&#22312;AI&#23433;&#20840;&#36777;&#35770;&#29615;&#22659;&#20013;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2312.04792</link><description>&lt;p&gt;
&#20351;&#29992;Oracle&#21644;AI&#36777;&#35770;&#36827;&#34892;&#22823;&#22411;&#28216;&#25103;&#30340;&#29609;&#27861;
&lt;/p&gt;
&lt;p&gt;
Playing Large Games with Oracles and AI Debate
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.04792
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#20855;&#26377;&#22823;&#37327;&#21160;&#20316;&#30340;&#37325;&#22797;&#28216;&#25103;&#20013;&#23454;&#29616;&#36951;&#25022;&#26368;&#23567;&#21270;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;Oracle&#30340;&#31639;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#20869;&#37096;&#36951;&#25022;&#26368;&#23567;&#21270;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#22312;&#22823;&#22411;&#28216;&#25103;&#20013;&#35745;&#31639;&#30456;&#20851;&#22343;&#34913;&#30340;&#39640;&#25928;&#24615;&#12290;&#36890;&#36807;&#22312;AI&#23433;&#20840;&#36777;&#35770;&#29615;&#22659;&#20013;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#22312;&#20855;&#26377;&#22823;&#37327;&#21160;&#20316;&#30340;&#37325;&#22797;&#28216;&#25103;&#20013;&#23454;&#29616;&#36951;&#25022;&#26368;&#23567;&#21270;&#12290;&#36825;&#31181;&#28216;&#25103;&#22312;&#36890;&#36807;&#36777;&#35770;&#30830;&#20445;AI&#23433;&#20840;&#30340;&#29615;&#22659;&#20013;&#26159;&#22266;&#26377;&#30340;&#65292;&#24182;&#19988;&#26356;&#19968;&#33324;&#22320;&#24212;&#29992;&#20110;&#21160;&#20316;&#22522;&#20110;&#35821;&#35328;&#30340;&#28216;&#25103;&#20013;&#12290;&#29616;&#26377;&#30340;&#22312;&#32447;&#28216;&#25103;&#31639;&#27861;&#38656;&#35201;&#22810;&#39033;&#24335;&#35745;&#31639;&#25968;&#37327;&#30340;&#21160;&#20316;&#65292;&#32780;&#23545;&#20110;&#22823;&#22411;&#28216;&#25103;&#26469;&#35828;&#65292;&#36825;&#21487;&#33021;&#26159;&#38590;&#20197;&#23454;&#29616;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#32771;&#34385;&#20351;&#29992;&#22522;&#20110;Oracle&#30340;&#31639;&#27861;&#65292;&#22240;&#20026;Oracle&#33258;&#28982;&#22320;&#27169;&#25311;&#20102;&#23545;AI&#20195;&#29702;&#30340;&#35775;&#38382;&#12290;&#36890;&#36807;&#23545;Oracle&#35775;&#38382;&#36827;&#34892;&#29305;&#24449;&#21270;&#65292;&#25105;&#20204;&#21487;&#20197;&#26377;&#25928;&#22320;&#23454;&#29616;&#20869;&#37096;&#21644;&#22806;&#37096;&#36951;&#25022;&#30340;&#26368;&#23567;&#21270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20869;&#37096;&#36951;&#25022;&#26368;&#23567;&#21270;&#31639;&#27861;&#65292;&#20854;&#36951;&#25022;&#21644;&#35745;&#31639;&#22797;&#26434;&#24230;&#23545;&#25968;&#22320;&#20381;&#36182;&#20110;&#21160;&#20316;&#25968;&#37327;&#12290;&#36825;&#24847;&#21619;&#30528;&#21487;&#20197;&#39640;&#25928;&#22320;&#22522;&#20110;Oracle&#35745;&#31639;&#22823;&#22411;&#28216;&#25103;&#20013;&#30340;&#30456;&#20851;&#22343;&#34913;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;AI&#23433;&#20840;&#36777;&#35770;&#29615;&#22659;&#20013;&#36827;&#34892;&#23454;&#39564;&#65292;&#23637;&#31034;&#20102;&#25105;&#20204;&#31639;&#27861;&#20998;&#26512;&#30340;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider regret minimization in repeated games with a very large number of actions. Such games are inherent in the setting of AI safety via debate, and more generally games whose actions are language-based. Existing algorithms for online game playing require computation polynomial in the number of actions, which can be prohibitive for large games.   We thus consider oracle-based algorithms, as oracles naturally model access to AI agents. With oracle access, we characterize when internal and external regret can be minimized efficiently. We give a novel efficient algorithm for internal regret minimization whose regret and computation complexity depend logarithmically on the number of actions. This implies efficient oracle-based computation of a correlated equilibrium in large games.   We conclude with experiments in the setting of AI Safety via Debate that shows the benefit of insights from our algorithmic analysis.
&lt;/p&gt;</description></item><item><title>SynthScribe&#26159;&#19968;&#20010;&#21033;&#29992;&#22810;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#30340;&#20840;&#26632;&#31995;&#32479;&#65292;&#35753;&#29992;&#25143;&#20197;&#26356;&#39640;&#32423;&#21035;&#34920;&#36798;&#24847;&#22270;&#65292;&#23454;&#29616;&#20102;&#25628;&#32034;&#29616;&#26377;&#22768;&#38899;&#12289;&#21019;&#24314;&#20840;&#26032;&#22768;&#38899;&#12289;&#23545;&#32473;&#23450;&#22768;&#38899;&#36827;&#34892;&#26377;&#24847;&#20041;&#20462;&#25913;&#30340;&#21151;&#33021;</title><link>https://arxiv.org/abs/2312.04690</link><description>&lt;p&gt;
SynthScribe&#65306;&#29992;&#20110;&#21512;&#25104;&#22120;&#22768;&#38899;&#26816;&#32034;&#21644;&#25506;&#32034;&#30340;&#28145;&#24230;&#22810;&#27169;&#24577;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
SynthScribe: Deep Multimodal Tools for Synthesizer Sound Retrieval and Exploration
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.04690
&lt;/p&gt;
&lt;p&gt;
SynthScribe&#26159;&#19968;&#20010;&#21033;&#29992;&#22810;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#30340;&#20840;&#26632;&#31995;&#32479;&#65292;&#35753;&#29992;&#25143;&#20197;&#26356;&#39640;&#32423;&#21035;&#34920;&#36798;&#24847;&#22270;&#65292;&#23454;&#29616;&#20102;&#25628;&#32034;&#29616;&#26377;&#22768;&#38899;&#12289;&#21019;&#24314;&#20840;&#26032;&#22768;&#38899;&#12289;&#23545;&#32473;&#23450;&#22768;&#38899;&#36827;&#34892;&#26377;&#24847;&#20041;&#20462;&#25913;&#30340;&#21151;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21512;&#25104;&#22120;&#26159;&#33021;&#22815;&#35753;&#38899;&#20048;&#23478;&#21019;&#20316;&#20986;&#21160;&#24577;&#21644;&#21407;&#21019;&#22768;&#38899;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;&#29616;&#26377;&#30340;&#29992;&#20110;&#21512;&#25104;&#22120;&#30340;&#21830;&#29992;&#30028;&#38754;&#36890;&#24120;&#35201;&#27714;&#38899;&#20048;&#23478;&#19982;&#22797;&#26434;&#30340;&#20302;&#32423;&#21442;&#25968;&#36827;&#34892;&#20132;&#20114;&#65292;&#25110;&#32773;&#31649;&#29702;&#22823;&#37327;&#39044;&#21046;&#22768;&#38899;&#24211;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;SynthScribe&#8212;&#8212;&#19968;&#20010;&#21033;&#29992;&#22810;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#30340;&#20840;&#26632;&#31995;&#32479;&#65292;&#35753;&#29992;&#25143;&#21487;&#20197;&#20197;&#26356;&#39640;&#32423;&#21035;&#34920;&#36798;&#20182;&#20204;&#30340;&#24847;&#22270;&#12290;&#25105;&#20204;&#23454;&#29616;&#20102;&#20960;&#20010;&#21151;&#33021;&#65292;&#20197;&#35299;&#20915;&#19968;&#20123;&#22256;&#38590;&#65292;&#21363;1&#65289;&#25628;&#32034;&#29616;&#26377;&#22768;&#38899;&#65292;2&#65289;&#21019;&#24314;&#20840;&#26032;&#22768;&#38899;&#65292;3&#65289;&#23545;&#32473;&#23450;&#22768;&#38899;&#36827;&#34892;&#26377;&#24847;&#20041;&#30340;&#20462;&#25913;&#12290;&#36825;&#36890;&#36807;&#19977;&#20010;&#20027;&#35201;&#21151;&#33021;&#23454;&#29616;&#65306;&#19968;&#20010;&#29992;&#20110;&#22823;&#22411;&#21512;&#25104;&#22120;&#22768;&#38899;&#24211;&#30340;&#22810;&#27169;&#24577;&#25628;&#32034;&#24341;&#25806;&#65307;&#19968;&#20010;&#20197;&#29992;&#25143;&#20026;&#20013;&#24515;&#30340;&#36951;&#20256;&#31639;&#27861;&#65292;&#21487;&#20197;&#26681;&#25454;&#29992;&#25143;&#30340;&#20559;&#22909;&#21019;&#24314;&#21644;&#36873;&#25321;&#20840;&#26032;&#22768;&#38899;&#65307;&#19968;&#20010;&#22768;&#38899;&#32534;&#36753;&#25903;&#25345;&#21151;&#33021;&#65292;&#31361;&#20986;&#26174;&#31034;&#24182;&#25552;&#20379;&#20851;&#38190;&#31034;&#20363;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.04690v2 Announce Type: replace-cross  Abstract: Synthesizers are powerful tools that allow musicians to create dynamic and original sounds. Existing commercial interfaces for synthesizers typically require musicians to interact with complex low-level parameters or to manage large libraries of premade sounds. To address these challenges, we implement SynthScribe -- a fullstack system that uses multimodal deep learning to let users express their intentions at a much higher level. We implement features which address a number of difficulties, namely 1) searching through existing sounds, 2) creating completely new sounds, 3) making meaningful modifications to a given sound. This is achieved with three main features: a multimodal search engine for a large library of synthesizer sounds; a user centered genetic algorithm by which completely new sounds can be created and selected given the users preferences; a sound editing support feature which highlights and gives examples for key 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Tree of Attacks with Pruning (TAP)&#30340;&#33258;&#21160;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#21482;&#38656;&#35201;&#23545;&#30446;&#26631;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#40657;&#30418;&#35775;&#38382;&#30340;&#36234;&#29425;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#24605;&#32500;&#26641;&#25512;&#29702;&#21644;&#20462;&#21098;&#29983;&#25104;&#20934;&#30830;&#30340;&#36234;&#29425;&#25552;&#31034;&#12290;</title><link>https://arxiv.org/abs/2312.02119</link><description>&lt;p&gt;
&#25915;&#20987;&#26641;&#65306;&#33258;&#21160;&#30772;&#35299;&#40657;&#30418;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Tree of Attacks: Jailbreaking Black-Box LLMs Automatically
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.02119
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Tree of Attacks with Pruning (TAP)&#30340;&#33258;&#21160;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#21482;&#38656;&#35201;&#23545;&#30446;&#26631;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#40657;&#30418;&#35775;&#38382;&#30340;&#36234;&#29425;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#24605;&#32500;&#26641;&#25512;&#29702;&#21644;&#20462;&#21098;&#29983;&#25104;&#20934;&#30830;&#30340;&#36234;&#29425;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#23637;&#31034;&#20102;&#22810;&#21151;&#33021;&#24615;&#65292;&#20294;&#20173;&#22312;&#29983;&#25104;&#26377;&#23475;&#12289;&#24102;&#20559;&#35265;&#21644;&#26377;&#27602;&#20869;&#23481;&#65292;&#36825;&#19968;&#28857;&#30001;&#20154;&#20026;&#35774;&#35745;&#30340;&#36234;&#29425;&#34892;&#20026;&#30340;&#26222;&#36941;&#23384;&#22312;&#24471;&#20197;&#35777;&#26126;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Tree of Attacks with Pruning (TAP)&#30340;&#33258;&#21160;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#36234;&#29425;&#65292;&#20165;&#38656;&#35201;&#23545;&#30446;&#26631;LLM&#36827;&#34892;&#40657;&#30418;&#35775;&#38382;&#12290;TAP&#21033;&#29992;LLM&#26469;&#36890;&#36807;&#24605;&#32500;&#26641;&#25512;&#29702;&#36845;&#20195;&#22320;&#20248;&#21270;&#20505;&#36873;&#65288;&#25915;&#20987;&#65289;&#25552;&#31034;&#65292;&#30452;&#21040;&#29983;&#25104;&#30340;&#25552;&#31034;&#20043;&#19968;&#36234;&#29425;&#30446;&#26631;&#12290;&#20851;&#38190;&#22312;&#20110;&#65292;&#22312;&#23558;&#25552;&#31034;&#21457;&#36865;&#32473;&#30446;&#26631;&#20043;&#21069;&#65292;TAP&#23545;&#20854;&#36827;&#34892;&#35780;&#20272;&#24182;&#31227;&#38500;&#21487;&#33021;&#19981;&#20250;&#23548;&#33268;&#36234;&#29425;&#30340;&#25552;&#31034;&#12290;&#20351;&#29992;&#24605;&#32500;&#26641;&#25512;&#29702;&#20351;TAP&#33021;&#22815;&#22312;&#22823;&#37327;&#25552;&#31034;&#30340;&#25628;&#32034;&#31354;&#38388;&#20013;&#23548;&#33322;&#65292;&#32780;&#20462;&#21098;&#21017;&#20943;&#23569;&#20102;&#21457;&#36865;&#32473;&#30446;&#26631;&#30340;&#24635;&#26597;&#35810;&#25968;&#37327;&#12290;&#22312;&#23454;&#35777;&#35780;&#20272;&#20013;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;TAP&#29983;&#25104;&#30340;&#25552;&#31034;&#36234;&#29425;&#20102;&#36229;&#36807;80%&#30340;&#26368;&#20808;&#36827;LLMs&#65288;&#21253;&#25324;GPT4&#21644;GPT4-Turbo&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.02119v2 Announce Type: replace-cross  Abstract: While Large Language Models (LLMs) display versatile functionality, they continue to generate harmful, biased, and toxic content, as demonstrated by the prevalence of human-designed jailbreaks. In this work, we present Tree of Attacks with Pruning (TAP), an automated method for generating jailbreaks that only requires black-box access to the target LLM. TAP utilizes an LLM to iteratively refine candidate (attack) prompts using tree-of-thought reasoning until one of the generated prompts jailbreaks the target. Crucially, before sending prompts to the target, TAP assesses them and prunes the ones unlikely to result in jailbreaks. Using tree-of-thought reasoning allows TAP to navigate a large search space of prompts and pruning reduces the total number of queries sent to the target. In empirical evaluations, we observe that TAP generates prompts that jailbreak state-of-the-art LLMs (including GPT4 and GPT4-Turbo) for more than 80%
&lt;/p&gt;</description></item><item><title>CAMRA&#26159;&#19968;&#20010;&#21019;&#26032;&#30340;&#22522;&#20110;web&#30340;&#24037;&#20855;&#65292;&#29992;&#20110;&#20174;&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;&#26500;&#24314;&#25277;&#35937;&#24847;&#20041;&#34920;&#31034;&#65288;AMR&#65289;&#65292;&#36890;&#36807;&#25972;&#21512;&#32534;&#31243;&#35821;&#35328;&#30340;&#32534;&#30721;&#26041;&#27861;&#21644;AMR&#35299;&#26512;&#22120;&#27169;&#22411;&#20316;&#20026;&#21103;&#39550;&#39542;&#65292;&#26497;&#22823;&#25552;&#39640;&#20102;AMR&#27880;&#37322;&#30340;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2311.10928</link><description>&lt;p&gt;
CAMRA&#65306;AMR&#27880;&#37322;&#30340;&#21103;&#39550;&#39542;
&lt;/p&gt;
&lt;p&gt;
CAMRA: Copilot for AMR Annotation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.10928
&lt;/p&gt;
&lt;p&gt;
CAMRA&#26159;&#19968;&#20010;&#21019;&#26032;&#30340;&#22522;&#20110;web&#30340;&#24037;&#20855;&#65292;&#29992;&#20110;&#20174;&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;&#26500;&#24314;&#25277;&#35937;&#24847;&#20041;&#34920;&#31034;&#65288;AMR&#65289;&#65292;&#36890;&#36807;&#25972;&#21512;&#32534;&#31243;&#35821;&#35328;&#30340;&#32534;&#30721;&#26041;&#27861;&#21644;AMR&#35299;&#26512;&#22120;&#27169;&#22411;&#20316;&#20026;&#21103;&#39550;&#39542;&#65292;&#26497;&#22823;&#25552;&#39640;&#20102;AMR&#27880;&#37322;&#30340;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;CAMRA&#65288;Copilot for AMR Annotations&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;web&#30340;&#24037;&#20855;&#65292;&#26088;&#22312;&#20174;&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;&#26500;&#24314;&#25277;&#35937;&#24847;&#20041;&#34920;&#31034;&#65288;AMR&#65289;&#12290;CAMRA&#25552;&#20379;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#28145;&#23618;&#35789;&#27719;&#35821;&#20041;&#27880;&#37322;&#26041;&#27861;&#65292;&#22914;AMR&#65292;&#23558;AMR&#27880;&#37322;&#35270;&#20026;&#32534;&#31243;&#35821;&#35328;&#20013;&#30340;&#32534;&#30721;&#12290;&#20511;&#21161;&#32534;&#31243;&#33539;&#24335;&#30340;&#29087;&#24713;&#24230;&#65292;CAMRA&#21253;&#21547;&#20102;&#25152;&#26377;&#29616;&#26377;AMR&#32534;&#36753;&#22120;&#30340;&#22522;&#26412;&#21151;&#33021;&#65292;&#21253;&#25324;&#31034;&#20363;&#26597;&#25214;&#65292;&#21516;&#26102;&#36890;&#36807;&#23558;Propbank&#35282;&#33394;&#38598;&#26597;&#25214;&#38598;&#25104;&#20026;&#24037;&#20855;&#20013;&#30340;&#33258;&#21160;&#23436;&#25104;&#21151;&#33021;&#65292;&#26356;&#36827;&#19968;&#27493;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;CAMRA&#23558;AMR&#35299;&#26512;&#22120;&#27169;&#22411;&#20316;&#20026;&#32534;&#30721;&#21103;&#39550;&#39542;&#65292;&#26497;&#22823;&#22320;&#25552;&#39640;&#20102;AMR&#27880;&#37322;&#32773;&#30340;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;&#20026;&#20102;&#23637;&#31034;&#24037;&#20855;&#30340;&#21151;&#33021;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#21487;&#35775;&#38382;&#30340;&#23454;&#26102;&#28436;&#31034;&#65306;https://camra.colorado.edu
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.10928v2 Announce Type: replace-cross  Abstract: In this paper, we introduce CAMRA (Copilot for AMR Annotatations), a cutting-edge web-based tool designed for constructing Abstract Meaning Representation (AMR) from natural language text. CAMRA offers a novel approach to deep lexical semantics annotation such as AMR, treating AMR annotation akin to coding in programming languages. Leveraging the familiarity of programming paradigms, CAMRA encompasses all essential features of existing AMR editors, including example lookup, while going a step further by integrating Propbank roleset lookup as an autocomplete feature within the tool. Notably, CAMRA incorporates AMR parser models as coding co-pilots, greatly enhancing the efficiency and accuracy of AMR annotators. To demonstrate the tool's capabilities, we provide a live demo accessible at: https://camra.colorado.edu
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20462;&#25913;&#36807;&#30340;Thompson&#25277;&#26679;&#31639;&#27861;&#65292;&#24378;&#35843;&#36890;&#36807;&#20248;&#21270;&#20010;&#24615;&#21270;&#22870;&#21169;&#20989;&#25968;&#23454;&#29616;&#20010;&#24615;&#21270;&#30446;&#26631;&#35774;&#23450;&#65292;&#20026;&#25903;&#25345;&#30446;&#26631;&#35774;&#23450;&#25552;&#20379;&#20102;&#19968;&#20010;&#24179;&#34913;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#27492;&#20462;&#25913;&#20165;&#23545;&#32047;&#31215;&#36951;&#25022;&#20135;&#29983;&#24658;&#23450;&#30340;&#24809;&#32602;&#12290;</title><link>https://arxiv.org/abs/2311.09483</link><description>&lt;p&gt;
&#20855;&#26377;&#29992;&#25143;&#23450;&#20041;&#30446;&#26631;&#30340;&#33258;&#36866;&#24212;&#24178;&#39044;&#29992;&#20110;&#20581;&#24247;&#34892;&#20026;&#25913;&#21464;
&lt;/p&gt;
&lt;p&gt;
Adaptive Interventions with User-Defined Goals for Health Behavior Change
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.09483
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20462;&#25913;&#36807;&#30340;Thompson&#25277;&#26679;&#31639;&#27861;&#65292;&#24378;&#35843;&#36890;&#36807;&#20248;&#21270;&#20010;&#24615;&#21270;&#22870;&#21169;&#20989;&#25968;&#23454;&#29616;&#20010;&#24615;&#21270;&#30446;&#26631;&#35774;&#23450;&#65292;&#20026;&#25903;&#25345;&#30446;&#26631;&#35774;&#23450;&#25552;&#20379;&#20102;&#19968;&#20010;&#24179;&#34913;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#27492;&#20462;&#25913;&#20165;&#23545;&#32047;&#31215;&#36951;&#25022;&#20135;&#29983;&#24658;&#23450;&#30340;&#24809;&#32602;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36523;&#20307;&#27963;&#21160;&#19981;&#36275;&#20173;&#28982;&#26159;&#19968;&#20010;&#20027;&#35201;&#30340;&#20844;&#20849;&#20581;&#24247;&#38382;&#39064;&#65292;&#19982;&#24515;&#34880;&#31649;&#30142;&#30149;&#21644;2&#22411;&#31958;&#23615;&#30149;&#31561;&#19981;&#33391;&#20581;&#24247;&#32467;&#26524;&#30456;&#20851;&#12290;&#31227;&#21160;&#20581;&#24247;&#24212;&#29992;&#31243;&#24207;&#20026;&#20302;&#25104;&#26412;&#12289;&#21487;&#25193;&#23637;&#30340;&#36523;&#20307;&#27963;&#21160;&#20419;&#36827;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#36884;&#24452;&#65292;&#28982;&#32780;&#36890;&#24120;&#25928;&#26524;&#36739;&#23567;&#65292;&#31896;&#38468;&#29575;&#20302;&#65292;&#29305;&#21035;&#26159;&#19982;&#20154;&#31867;&#36741;&#23548;&#30456;&#27604;&#12290;&#30446;&#26631;&#35774;&#23450;&#26159;&#20581;&#24247;&#36741;&#23548;&#30340;&#19968;&#20010;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65292;&#22312;&#31227;&#21160;&#20581;&#24247;&#24178;&#39044;&#30340;&#33258;&#36866;&#24212;&#31639;&#27861;&#20013;&#19968;&#30452;&#26410;&#20805;&#20998;&#21033;&#29992;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#23545;Thompson&#25277;&#26679;&#31639;&#27861;&#30340;&#20462;&#25913;&#65292;&#37325;&#28857;&#25918;&#22312;&#36890;&#36807;&#20248;&#21270;&#20010;&#24615;&#21270;&#22870;&#21169;&#20989;&#25968;&#23454;&#29616;&#20010;&#24615;&#21270;&#30446;&#26631;&#35774;&#23450;&#12290;&#20316;&#20026;&#25903;&#25345;&#30446;&#26631;&#35774;&#23450;&#30340;&#19968;&#27493;&#65292;&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#21487;&#20197;&#21033;&#29992;&#20849;&#20139;&#32467;&#26500;&#21516;&#26102;&#20248;&#21270;&#20010;&#20154;&#20559;&#22909;&#21644;&#30446;&#26631;&#30340;&#24179;&#34913;&#26041;&#27861;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#20462;&#25913;&#21482;&#23545;&#32047;&#31215;&#36951;&#25022;&#36896;&#25104;&#19968;&#20010;&#24120;&#25968;&#24809;&#32602;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.09483v2 Announce Type: replace-cross  Abstract: Physical inactivity remains a major public health concern, having associations with adverse health outcomes such as cardiovascular disease and type-2 diabetes. Mobile health applications present a promising avenue for low-cost, scalable physical activity promotion, yet often suffer from small effect sizes and low adherence rates, particularly in comparison to human coaching. Goal-setting is a critical component of health coaching that has been underutilized in adaptive algorithms for mobile health interventions. This paper introduces a modification to the Thompson sampling algorithm that places emphasis on individualized goal-setting by optimizing personalized reward functions. As a step towards supporting goal-setting, this paper offers a balanced approach that can leverage shared structure while optimizing individual preferences and goals. We prove that our modification incurs only a constant penalty on the cumulative regret 
&lt;/p&gt;</description></item><item><title>&#22312;&#39640;&#39118;&#38505;&#29615;&#22659;&#20013;&#65292;&#30740;&#31350;&#20154;&#21592;&#36890;&#36807;&#20998;&#26512;&#30005;&#35270;&#28216;&#25103;&#33410;&#30446;&#25968;&#25454;&#21457;&#29616;&#65292;&#21363;&#20351;&#21482;&#20351;&#29992;&#35821;&#35328;&#32447;&#32034;&#65292;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26500;&#24314;&#30340;&#27169;&#22411;&#21487;&#20197;&#19982;&#20154;&#31867;&#20027;&#20307;&#20855;&#26377;&#31867;&#20284;&#30340;&#30495;&#30456;&#26816;&#27979;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2311.07092</link><description>&lt;p&gt;
&#25581;&#31034;&#30495;&#30456;&#65306;&#27450;&#39575;&#35821;&#35328;&#21644;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
To Tell The Truth: Language of Deception and Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.07092
&lt;/p&gt;
&lt;p&gt;
&#22312;&#39640;&#39118;&#38505;&#29615;&#22659;&#20013;&#65292;&#30740;&#31350;&#20154;&#21592;&#36890;&#36807;&#20998;&#26512;&#30005;&#35270;&#28216;&#25103;&#33410;&#30446;&#25968;&#25454;&#21457;&#29616;&#65292;&#21363;&#20351;&#21482;&#20351;&#29992;&#35821;&#35328;&#32447;&#32034;&#65292;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26500;&#24314;&#30340;&#27169;&#22411;&#21487;&#20197;&#19982;&#20154;&#31867;&#20027;&#20307;&#20855;&#26377;&#31867;&#20284;&#30340;&#30495;&#30456;&#26816;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2311.07092v2 &#20844;&#21578;&#31867;&#22411;&#65306;&#26367;&#25442;-cross &#25688;&#35201;&#65306;&#22522;&#20110;&#25991;&#26412;&#30340;&#38169;&#35823;&#20449;&#24687;&#28183;&#36879;&#21040;&#22312;&#32447;&#35752;&#35770;&#20013;&#65292;&#28982;&#32780;&#20154;&#20204;&#33021;&#22815;&#20174;&#36825;&#31181;&#27450;&#39575;&#24615;&#25991;&#26412;&#20869;&#23481;&#20013;&#36776;&#21035;&#30495;&#30456;&#30340;&#35777;&#25454;&#21364;&#24456;&#23569;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#19968;&#26723;&#26032;&#39062;&#30340;&#30005;&#35270;&#28216;&#25103;&#33410;&#30446;&#25968;&#25454;&#65292;&#20854;&#20013;&#39640;&#39118;&#38505;&#29615;&#22659;&#20013;&#30456;&#20114;&#20043;&#38388;&#23384;&#22312;&#20914;&#31361;&#30446;&#26631;&#30340;&#20010;&#20307;&#20043;&#38388;&#30340;&#23545;&#35805;&#23548;&#33268;&#35854;&#35328;&#12290;&#25105;&#20204;&#35843;&#26597;&#20102;&#27450;&#39575;&#35821;&#35328;&#28508;&#22312;&#21487;&#39564;&#35777;&#35821;&#35328;&#32447;&#32034;&#22312;&#23458;&#35266;&#30495;&#30456;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#30340;&#34920;&#29616;&#65292;&#36825;&#26159;&#20197;&#24448;&#22522;&#20110;&#25991;&#26412;&#30340;&#27450;&#39575;&#25968;&#25454;&#38598;&#20013;&#32570;&#23569;&#30340;&#19968;&#20010;&#26174;&#33879;&#29305;&#24449;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#23384;&#22312;&#19968;&#31867;&#25506;&#27979;&#22120;&#65288;&#31639;&#27861;&#65289;&#65292;&#20854;&#30495;&#30456;&#26816;&#27979;&#24615;&#33021;&#19982;&#20154;&#31867;&#20027;&#20307;&#30456;&#20284;&#65292;&#21363;&#20351;&#21069;&#32773;&#21482;&#20351;&#29992;&#35821;&#35328;&#32447;&#32034;&#65292;&#32780;&#21518;&#32773;&#21017;&#36890;&#36807;&#23436;&#20840;&#35775;&#38382;&#25152;&#26377;&#28508;&#22312;&#32447;&#32034;&#28304;&#65288;&#35821;&#35328;&#21644;&#35270;&#21548;&#65289;&#36827;&#34892;&#23545;&#35805;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#65292;&#24314;&#31435;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20043;&#19978;&#65292;&#37319;&#29992;&#29942;&#39048;&#26694;&#26550;&#26469;&#23398;&#20064;&#21487;&#36776;&#21035;&#30340;&#32447;&#32034;&#65292;&#20197;&#30830;&#23450;&#30495;&#30456;&#30340;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.07092v2 Announce Type: replace-cross  Abstract: Text-based misinformation permeates online discourses, yet evidence of people's ability to discern truth from such deceptive textual content is scarce. We analyze a novel TV game show data where conversations in a high-stake environment between individuals with conflicting objectives result in lies. We investigate the manifestation of potentially verifiable language cues of deception in the presence of objective truth, a distinguishing feature absent in previous text-based deception datasets. We show that there exists a class of detectors (algorithms) that have similar truth detection performance compared to human subjects, even when the former accesses only the language cues while the latter engages in conversations with complete access to all potential sources of cues (language and audio-visual). Our model, built on a large language model, employs a bottleneck framework to learn discernible cues to determine truth, an act of 
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#29289;&#20307;&#30340;&#36816;&#21160;&#23398;&#32467;&#26500;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36816;&#21160;&#23398;&#24863;&#30693;&#25552;&#31034;&#26694;&#26550;&#65292;&#29992;&#20110;&#29983;&#25104;LLMs&#30340;&#20302;&#32423;&#36816;&#21160;&#36712;&#36857;&#20301;&#28857;&#65292;&#20197;&#23454;&#29616;&#23545;&#21487;&#31227;&#21160;&#29289;&#20307;&#30340;&#27867;&#21270;&#25805;&#20316;</title><link>https://arxiv.org/abs/2311.02847</link><description>&lt;p&gt;
&#20351;&#29992;LLMs&#30340;&#36816;&#21160;&#23398;&#24863;&#30693;&#25552;&#31034;&#23454;&#29616;&#23545;&#21487;&#31227;&#21160;&#29289;&#20307;&#30340;&#27867;&#21270;&#25805;&#20316;
&lt;/p&gt;
&lt;p&gt;
Kinematic-aware Prompting for Generalizable Articulated Object Manipulation with LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.02847
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#29289;&#20307;&#30340;&#36816;&#21160;&#23398;&#32467;&#26500;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36816;&#21160;&#23398;&#24863;&#30693;&#25552;&#31034;&#26694;&#26550;&#65292;&#29992;&#20110;&#29983;&#25104;LLMs&#30340;&#20302;&#32423;&#36816;&#21160;&#36712;&#36857;&#20301;&#28857;&#65292;&#20197;&#23454;&#29616;&#23545;&#21487;&#31227;&#21160;&#29289;&#20307;&#30340;&#27867;&#21270;&#25805;&#20316;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27867;&#21270;&#30340;&#21487;&#31227;&#21160;&#29289;&#20307;&#25805;&#20316;&#23545;&#20110;&#23478;&#24237;&#21161;&#25163;&#26426;&#22120;&#20154;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#20174;&#28436;&#31034;&#20013;&#36827;&#34892;&#27169;&#20223;&#23398;&#20064;&#25110;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#65292;&#28982;&#32780;&#65292;&#30001;&#20110;&#23454;&#38469;&#25968;&#25454;&#25910;&#38598;&#21644;&#31934;&#30830;&#23545;&#35937;&#27169;&#25311;&#25104;&#26412;&#39640;&#26114;&#65292;&#36825;&#20123;&#24037;&#20316;&#20173;&#28982;&#38590;&#20197;&#22312;&#22810;&#26679;&#30340;&#21487;&#31227;&#21160;&#29289;&#20307;&#19978;&#23454;&#29616;&#24191;&#27867;&#30340;&#36866;&#24212;&#24615;&#12290;&#26368;&#36817;&#65292;&#35768;&#22810;&#30740;&#31350;&#23581;&#35797;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24378;&#22823;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#23454;&#29616;&#21487;&#27867;&#21270;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#65292;&#20294;&#22823;&#22810;&#25968;&#30740;&#31350;&#20391;&#37325;&#20110;&#39640;&#23618;&#20219;&#21153;&#35268;&#21010;&#65292;&#24573;&#35270;&#20102;&#20302;&#23618;&#30340;&#26426;&#22120;&#20154;&#25511;&#21046;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#22522;&#20110;&#19968;&#20010;&#29702;&#24565;&#65292;&#21363;&#29289;&#20307;&#30340;&#36816;&#21160;&#23398;&#32467;&#26500;&#20915;&#23450;&#20102;&#25105;&#20204;&#22914;&#20309;&#25805;&#32437;&#23427;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36816;&#21160;&#23398;&#24863;&#30693;&#25552;&#31034;&#26694;&#26550;&#65292;&#29992;&#29289;&#20307;&#30340;&#36816;&#21160;&#23398;&#30693;&#35782;&#25552;&#31034;LLMs&#29983;&#25104;&#20302;&#23618;&#36816;&#21160;&#36712;&#36857;&#20301;&#28857;&#65292;&#25903;&#25345;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.02847v3 Announce Type: replace-cross  Abstract: Generalizable articulated object manipulation is essential for home-assistant robots. Recent efforts focus on imitation learning from demonstrations or reinforcement learning in simulation, however, due to the prohibitive costs of real-world data collection and precise object simulation, it still remains challenging for these works to achieve broad adaptability across diverse articulated objects. Recently, many works have tried to utilize the strong in-context learning ability of Large Language Models (LLMs) to achieve generalizable robotic manipulation, but most of these researches focus on high-level task planning, sidelining low-level robotic control. In this work, building on the idea that the kinematic structure of the object determines how we can manipulate it, we propose a kinematic-aware prompting framework that prompts LLMs with kinematic knowledge of objects to generate low-level motion trajectory waypoints, supportin
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21033;&#29992;AIS&#25968;&#25454;&#39044;&#27979;&#33337;&#33334;&#36712;&#36857;&#65292;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#20943;&#23569;&#33337;&#33334;&#19982;&#40120;&#40060;&#30896;&#25758;&#26469;&#24314;&#31435;&#26356;&#23433;&#20840;&#30340;&#28023;&#27915;&#29615;&#22659;&#12290;</title><link>https://arxiv.org/abs/2310.18948</link><description>&lt;p&gt;
&#36890;&#36807;&#22810;&#36335;&#24452;&#38271;&#26399;&#33337;&#33334;&#36712;&#36857;&#39044;&#27979;&#24314;&#31435;&#26356;&#23433;&#20840;&#30340;&#28023;&#27915;&#29615;&#22659;
&lt;/p&gt;
&lt;p&gt;
Building a Safer Maritime Environment Through Multi-Path Long-Term Vessel Trajectory Forecasting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.18948
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;AIS&#25968;&#25454;&#39044;&#27979;&#33337;&#33334;&#36712;&#36857;&#65292;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#20943;&#23569;&#33337;&#33334;&#19982;&#40120;&#40060;&#30896;&#25758;&#26469;&#24314;&#31435;&#26356;&#23433;&#20840;&#30340;&#28023;&#27915;&#29615;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28023;&#19978;&#20132;&#36890;&#23545;&#20110;&#23454;&#29616;&#20840;&#29699;&#32463;&#27982;&#22686;&#38271;&#33267;&#20851;&#37325;&#35201;&#65292;&#21516;&#26102;&#20063;&#38656;&#35201;&#22312;&#21487;&#25345;&#32493;&#24615;&#21644;&#20445;&#25252;&#28626;&#21361;&#28023;&#27915;&#29289;&#31181;&#26041;&#38754;&#23653;&#34892;&#29983;&#24577;&#20041;&#21153;&#65292;&#23588;&#20854;&#26159;&#20445;&#25252;&#22823;&#22411;&#40120;&#31867;&#31181;&#32676;&#12290;&#22312;&#36825;&#26041;&#38754;&#65292;&#33258;&#21160;&#35782;&#21035;&#31995;&#32479;(AIS)&#25968;&#25454;&#36890;&#36807;&#25552;&#20379;&#33337;&#33334;&#36816;&#21160;&#30340;&#23454;&#26102;&#27969;&#25968;&#25454;&#65292;&#21487;&#20197;&#23454;&#29616;&#24378;&#21270;&#30340;&#20132;&#36890;&#30417;&#25511;&#65292;&#20174;&#32780;&#36991;&#20813;&#33337;&#33334;&#19982;&#40120;&#40060;&#30896;&#25758;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#21033;&#29992;AIS&#25968;&#25454;&#39044;&#27979;&#38271;&#26399;&#33337;&#33334;&#36712;&#36857;&#65292;&#20174;&#32780;&#39044;&#38450;&#33337;&#33334;&#19982;&#40120;&#40060;&#30340;&#30896;&#25758;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#37319;&#29992;&#21452;&#21521;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;(Bi-LSTM)&#26500;&#24314;&#20102;&#19968;&#31181;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#26550;&#26500;&#65292;&#36890;&#36807;&#23558;1&#21040;3&#23567;&#26102;&#30340;AIS&#25968;&#25454;&#20316;&#20026;&#36755;&#20837;&#65292;&#39044;&#27979;&#25509;&#19979;&#26469;12&#23567;&#26102;&#30340;&#33337;&#33334;&#36712;&#36857;&#12290;&#25105;&#20204;&#20174;&#21382;&#21490;AIS&#25968;&#25454;&#20013;&#25552;&#21462;&#28508;&#22312;&#36335;&#32447;&#21644;&#30446;&#30340;&#22320;&#30340;&#27010;&#29575;&#29305;&#24449;&#65292;&#24182;&#23558;&#20854;&#20316;&#20026;&#27169;&#22411;&#30340;&#36755;&#20837;&#12290;&#27169;&#22411;&#38543;&#21518;&#39044;&#27979;&#33337;&#33334;&#30340;&#36712;&#36857;&#65292;&#32771;&#34385;&#21040;&#28508;&#22312;&#36335;&#32447;&#21644;&#30446;&#30340;&#22320;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Maritime transportation is paramount in achieving global economic growth, entailing concurrent ecological obligations in sustainability and safeguarding endangered marine species, most notably preserving large whale populations. In this regard, the Automatic Identification System (AIS) data plays a significant role by offering real-time streaming data on vessel movement, allowing enhanced traffic monitoring. This study explores using AIS data to prevent vessel-to-whale collisions by forecasting long-term vessel trajectories from engineered AIS data sequences. For such a task, we have developed an encoder-decoder model architecture using Bidirectional Long Short-Term Memory Networks (Bi-LSTM) to predict the next 12 hours of vessel trajectories using 1 to 3 hours of AIS data as input. We feed the model with probabilistic features engineered from historical AIS data that refer to each trajectory's potential route and destination. The model then predicts the vessel's trajectory, considerin
&lt;/p&gt;</description></item><item><title>RGI-Net&#36890;&#36807;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#21644;&#21033;&#29992;&#25151;&#38388;&#33033;&#20914;&#21709;&#24212;&#20013;&#39640;&#38454;&#21453;&#23556;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#23454;&#29616;&#22312;&#27809;&#26377;&#20256;&#32479;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#25512;&#26029;&#25151;&#38388;&#20960;&#20309;&#20449;&#24687;&#12290;</title><link>https://arxiv.org/abs/2309.01513</link><description>&lt;p&gt;
RGI-Net&#65306;&#22312;&#27809;&#26377;&#19968;&#38454;&#22238;&#22768;&#30340;&#24773;&#20917;&#19979;&#20174;&#25151;&#38388;&#33033;&#20914;&#21709;&#24212;&#20013;&#25512;&#26029;3D&#25151;&#38388;&#20960;&#20309;
&lt;/p&gt;
&lt;p&gt;
RGI-Net: 3D Room Geometry Inference from Room Impulse Responses in the Absence of First-order Echoes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2309.01513
&lt;/p&gt;
&lt;p&gt;
RGI-Net&#36890;&#36807;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#21644;&#21033;&#29992;&#25151;&#38388;&#33033;&#20914;&#21709;&#24212;&#20013;&#39640;&#38454;&#21453;&#23556;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#23454;&#29616;&#22312;&#27809;&#26377;&#20256;&#32479;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#25512;&#26029;&#25151;&#38388;&#20960;&#20309;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25151;&#38388;&#20960;&#20309;&#26159;&#23454;&#29616;&#36924;&#30495;&#30340;3D&#38899;&#39057;&#28210;&#26579;&#30340;&#37325;&#35201;&#20808;&#39564;&#20449;&#24687;&#12290;&#20026;&#27492;&#65292;&#21033;&#29992;&#25151;&#38388;&#33033;&#20914;&#21709;&#24212;&#20013;&#21040;&#36798;&#26102;&#38388;&#65288;TOA&#65289;&#25110;&#21040;&#36798;&#26102;&#38388;&#24046;&#65288;TDOA&#65289;&#20449;&#24687;&#21457;&#23637;&#20102;&#21508;&#31181;&#25151;&#38388;&#20960;&#20309;&#25512;&#26029;&#65288;RGI&#65289;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;RGI&#25216;&#26415;&#25552;&#20986;&#20102;&#19968;&#20123;&#20551;&#35774;&#65292;&#22914;&#20984;&#25151;&#38388;&#24418;&#29366;&#12289;&#24050;&#30693;&#22681;&#22721;&#25968;&#37327;&#21644;&#19968;&#38454;&#21453;&#23556;&#30340;&#21487;&#35265;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;RGI-Net&#65292;&#23427;&#21487;&#20197;&#22312;&#27809;&#26377;&#19978;&#36848;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#20272;&#35745;&#25151;&#38388;&#20960;&#20309;&#12290;RGI-Net&#23398;&#20064;&#24182;&#21033;&#29992;&#25151;&#38388;&#33033;&#20914;&#21709;&#24212;&#65288;RIRs&#65289;&#20013;&#30340;&#39640;&#38454;&#21453;&#23556;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#65292;&#22240;&#27492;&#21487;&#20197;&#22312;&#24418;&#29366;&#20026;&#38750;&#20984;&#24418;&#25110;RIRs&#20013;&#32570;&#23569;&#19968;&#38454;&#21453;&#23556;&#30340;&#24773;&#20917;&#19979;&#20272;&#35745;&#25151;&#38388;&#24418;&#29366;&#12290;&#35813;&#32593;&#32476;&#37319;&#29992;&#20174;&#35013;&#26377;&#22278;&#24418;&#40614;&#20811;&#39118;&#30340;&#32039;&#20945;&#38899;&#39057;&#35774;&#22791;&#27979;&#37327;&#30340;RIRs&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2309.01513v2 Announce Type: replace-cross  Abstract: Room geometry is important prior information for implementing realistic 3D audio rendering. For this reason, various room geometry inference (RGI) methods have been developed by utilizing the time of arrival (TOA) or time difference of arrival (TDOA) information in room impulse responses. However, the conventional RGI technique poses several assumptions, such as convex room shapes, the number of walls known in priori, and the visibility of first-order reflections. In this work, we introduce the deep neural network (DNN), RGI-Net, which can estimate room geometries without the aforementioned assumptions. RGI-Net learns and exploits complex relationships between high-order reflections in room impulse responses (RIRs) and, thus, can estimate room shapes even when the shape is non-convex or first-order reflections are missing in the RIRs. The network takes RIRs measured from a compact audio device equipped with a circular microphon
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23457;&#26597;&#20102;&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20844;&#24179;&#24615;&#30340;&#30740;&#31350;&#65292;&#38024;&#23545;&#20013;&#31561;&#35268;&#27169;LLMs&#21644;&#22823;&#35268;&#27169;LLMs&#25552;&#20986;&#20102;&#35780;&#20272;&#25351;&#26631;&#21644;&#21435;&#20559;&#35265;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2308.10149</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20844;&#24179;&#24615;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
A Survey on Fairness in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2308.10149
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23457;&#26597;&#20102;&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20844;&#24179;&#24615;&#30340;&#30740;&#31350;&#65292;&#38024;&#23545;&#20013;&#31561;&#35268;&#27169;LLMs&#21644;&#22823;&#35268;&#27169;LLMs&#25552;&#20986;&#20102;&#35780;&#20272;&#25351;&#26631;&#21644;&#21435;&#20559;&#35265;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#23637;&#29616;&#20102;&#24378;&#22823;&#30340;&#24615;&#33021;&#21644;&#21457;&#23637;&#21069;&#26223;&#65292;&#24182;&#24191;&#27867;&#37096;&#32626;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#12290;&#28982;&#32780;&#65292;LLMs&#21487;&#33021;&#20250;&#25429;&#25417;&#21040;&#26410;&#32463;&#22788;&#29702;&#30340;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#31038;&#20250;&#20559;&#35265;&#65292;&#24182;&#23558;&#36825;&#20123;&#20559;&#35265;&#20256;&#25773;&#21040;&#19979;&#28216;&#20219;&#21153;&#12290;&#19981;&#20844;&#24179;&#30340;LLM&#31995;&#32479;&#20250;&#20135;&#29983;&#19981;&#33391;&#30340;&#31038;&#20250;&#24433;&#21709;&#21644;&#28508;&#22312;&#21361;&#23475;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20840;&#38754;&#22238;&#39038;&#20102;&#26377;&#20851;LLMs&#20013;&#20844;&#24179;&#24615;&#30340;&#30740;&#31350;&#12290;&#32771;&#34385;&#21040;&#21442;&#25968;&#22823;&#23567;&#21644;&#35757;&#32451;&#33539;&#24335;&#23545;&#30740;&#31350;&#31574;&#30053;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#23558;&#29616;&#26377;&#30340;&#20844;&#24179;&#24615;&#30740;&#31350;&#20998;&#20026;&#38024;&#23545;&#20013;&#31561;&#35268;&#27169;LLMs&#22312;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#33539;&#24335;&#19979;&#30340;&#30740;&#31350;&#20197;&#21450;&#38024;&#23545;&#22823;&#35268;&#27169;LLMs&#22312;&#25552;&#31034;&#33539;&#24335;&#19979;&#30340;&#30740;&#31350;&#12290;&#39318;&#20808;&#65292;&#23545;&#20110;&#20013;&#31561;&#35268;&#27169;LLMs&#65292;&#25105;&#20204;&#20174;&#20869;&#22312;&#20559;&#35265;&#21644;&#22806;&#22312;&#20559;&#35265;&#30340;&#35282;&#24230;&#20171;&#32461;&#20102;&#35780;&#20272;&#25351;&#26631;&#21644;&#21435;&#20559;&#35265;&#26041;&#27861;&#12290;&#28982;&#21518;&#65292;&#23545;&#20110;&#22823;&#35268;&#27169;LLMs&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#26368;&#36817;&#30340;&#20844;&#24179;&#24615;&#30740;&#31350;&#65292;&#21253;&#25324;&#20844;&#24179;&#24615;&#35780;&#20272;&#12289;&#21407;&#22240;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2308.10149v2 Announce Type: replace-cross  Abstract: Large Language Models (LLMs) have shown powerful performance and development prospects and are widely deployed in the real world. However, LLMs can capture social biases from unprocessed training data and propagate the biases to downstream tasks. Unfair LLM systems have undesirable social impacts and potential harms. In this paper, we provide a comprehensive review of related research on fairness in LLMs. Considering the influence of parameter magnitude and training paradigm on research strategy, we divide existing fairness research into oriented to medium-sized LLMs under pre-training and fine-tuning paradigms and oriented to large-sized LLMs under prompting paradigms. First, for medium-sized LLMs, we introduce evaluation metrics and debiasing methods from the perspectives of intrinsic bias and extrinsic bias, respectively. Then, for large-sized LLMs, we introduce recent fairness research, including fairness evaluation, reason
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#36890;&#36807;&#35821;&#20041;&#21305;&#37197;&#20462;&#22797;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#20013;&#30340;&#30830;&#35748;&#20559;&#35265;&#38382;&#39064;&#65292;&#24341;&#20837;&#20102;&#20154;&#31867;&#27010;&#24565;&#19982;&#65288;&#20122;&#31526;&#21495;&#65289;&#35299;&#37322;&#20043;&#38388;&#30340;&#27010;&#24565;&#26694;&#26550;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#26500;&#21270;&#26041;&#27861;&#26469;&#35780;&#20272;&#35821;&#20041;&#21305;&#37197;&#12290;</title><link>https://arxiv.org/abs/2307.00897</link><description>&lt;p&gt;
&#36890;&#36807;&#35821;&#20041;&#21305;&#37197;&#20462;&#22797;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#20013;&#30340;&#30830;&#35748;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Fixing confirmation bias in feature attribution methods via semantic match
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2307.00897
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#36890;&#36807;&#35821;&#20041;&#21305;&#37197;&#20462;&#22797;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#20013;&#30340;&#30830;&#35748;&#20559;&#35265;&#38382;&#39064;&#65292;&#24341;&#20837;&#20102;&#20154;&#31867;&#27010;&#24565;&#19982;&#65288;&#20122;&#31526;&#21495;&#65289;&#35299;&#37322;&#20043;&#38388;&#30340;&#27010;&#24565;&#26694;&#26550;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#26500;&#21270;&#26041;&#27861;&#26469;&#35780;&#20272;&#35821;&#20041;&#21305;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#24050;&#32463;&#25104;&#20026;&#35299;&#26512;&#40657;&#30418;&#27169;&#22411;&#22797;&#26434;&#34892;&#20026;&#30340;&#37325;&#35201;&#26041;&#27861;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#19968;&#20123;&#23398;&#32773;&#25351;&#20986;&#36825;&#31867;&#26041;&#27861;&#23384;&#22312;&#20005;&#37325;&#32570;&#38519;&#65306;&#23427;&#20204;&#19981;&#33021;&#21487;&#38752;&#22320;&#29992;&#20154;&#31867;&#27010;&#24565;&#36827;&#34892;&#35299;&#37322;&#12290;&#31616;&#32780;&#35328;&#20043;&#65292;&#20165;&#20165;&#21487;&#35270;&#21270;&#19968;&#31995;&#21015;&#29305;&#24449;&#36129;&#29486;&#23545;&#20110;&#20154;&#31867;&#26469;&#35828;&#26080;&#27861;&#24471;&#20986;&#20851;&#20110;&#27169;&#22411;&#20869;&#37096;&#34920;&#31034;&#30340;&#32467;&#35770;&#65292;&#32780;&#30830;&#35748;&#20559;&#35265;&#21487;&#33021;&#20250;&#35753;&#29992;&#25143;&#20135;&#29983;&#20851;&#20110;&#27169;&#22411;&#34892;&#20026;&#30340;&#38169;&#35823;&#20449;&#24565;&#12290;&#25105;&#20204;&#35748;&#20026;&#38656;&#35201;&#19968;&#31181;&#32467;&#26500;&#21270;&#26041;&#27861;&#26469;&#39564;&#35777;&#25105;&#20204;&#23545;&#27169;&#22411;&#30340;&#20551;&#35774;&#26159;&#21542;&#24471;&#21040;&#20102;&#29305;&#24449;&#24402;&#22240;&#30340;&#30830;&#35748;&#12290;&#36825;&#23601;&#26159;&#25105;&#20204;&#25152;&#35828;&#30340;&#20154;&#31867;&#27010;&#24565;&#19982;&#65288;&#20122;&#31526;&#21495;&#65289;&#35299;&#37322;&#20043;&#38388;&#30340;&#8220;&#35821;&#20041;&#21305;&#37197;&#8221;&#12290;&#22312; Cin\`a&#31561;&#20154;[2023]&#25552;&#20986;&#30340;&#27010;&#24565;&#26694;&#26550;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#26500;&#21270;&#26041;&#27861;&#26469;&#22312;&#23454;&#36341;&#20013;&#35780;&#20272;&#35821;&#20041;&#21305;&#37197;&#12290;&#25105;&#20204;&#22312;&#19968;&#31995;&#21015;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#36825;&#19968;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2307.00897v2 Announce Type: replace-cross  Abstract: Feature attribution methods have become a staple method to disentangle the complex behavior of black box models. Despite their success, some scholars have argued that such methods suffer from a serious flaw: they do not allow a reliable interpretation in terms of human concepts. Simply put, visualizing an array of feature contributions is not enough for humans to conclude something about a model's internal representations, and confirmation bias can trick users into false beliefs about model behavior. We argue that a structured approach is required to test whether our hypotheses on the model are confirmed by the feature attributions. This is what we call the "semantic match" between human concepts and (sub-symbolic) explanations. Building on the conceptual framework put forward in Cin\`a et al. [2023], we propose a structured approach to evaluate semantic match in practice. We showcase the procedure in a suite of experiments spa
&lt;/p&gt;</description></item><item><title>Sin3DM&#25552;&#20986;&#20102;&#19968;&#31181;&#25193;&#25955;&#27169;&#22411;&#65292;&#33021;&#22815;&#20174;&#21333;&#20010;3D&#32441;&#29702;&#24418;&#29366;&#20013;&#23398;&#20064;&#20869;&#37096;&#34917;&#19969;&#20998;&#24067;&#65292;&#22312;&#20302;&#32500;&#28508;&#22312;&#31354;&#38388;&#20013;&#35757;&#32451;&#24182;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#21464;&#20307;&#12290;</title><link>https://arxiv.org/abs/2305.15399</link><description>&lt;p&gt;
Sin3DM: &#20174;&#21333;&#20010;3D&#32441;&#29702;&#24418;&#29366;&#20013;&#23398;&#20064;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Sin3DM: Learning a Diffusion Model from a Single 3D Textured Shape
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2305.15399
&lt;/p&gt;
&lt;p&gt;
Sin3DM&#25552;&#20986;&#20102;&#19968;&#31181;&#25193;&#25955;&#27169;&#22411;&#65292;&#33021;&#22815;&#20174;&#21333;&#20010;3D&#32441;&#29702;&#24418;&#29366;&#20013;&#23398;&#20064;&#20869;&#37096;&#34917;&#19969;&#20998;&#24067;&#65292;&#22312;&#20302;&#32500;&#28508;&#22312;&#31354;&#38388;&#20013;&#35757;&#32451;&#24182;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#21464;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21512;&#25104;&#21151;&#33021;&#19981;&#20165;&#21487;&#20197;&#38543;&#26426;&#29983;&#25104;&#26032;&#26679;&#26412;&#65292;&#36824;&#21487;&#20197;&#20174;&#23454;&#38469;&#36755;&#20837;&#26679;&#26412;&#20013;&#29983;&#25104;&#31867;&#20284;&#30340;&#39640;&#36136;&#37327;&#21464;&#21270;&#65292;&#21253;&#25324;&#31934;&#32454;&#30340;&#20960;&#20309;&#21644;&#32441;&#29702;&#32454;&#33410;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2305.15399v2 Announce Type: replace-cross  Abstract: Synthesizing novel 3D models that resemble the input example has long been pursued by graphics artists and machine learning researchers. In this paper, we present Sin3DM, a diffusion model that learns the internal patch distribution from a single 3D textured shape and generates high-quality variations with fine geometry and texture details. Training a diffusion model directly in 3D would induce large memory and computational cost. Therefore, we first compress the input into a lower-dimensional latent space and then train a diffusion model on it. Specifically, we encode the input 3D textured shape into triplane feature maps that represent the signed distance and texture fields of the input. The denoising network of our diffusion model has a limited receptive field to avoid overfitting, and uses triplane-aware 2D convolution blocks to improve the result quality. Aside from randomly generating new samples, our model also facilitat
&lt;/p&gt;</description></item><item><title>InPars-Light&#26159;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#20462;&#25913;&#65292;&#36890;&#36807;&#20351;&#29992;&#23567;&#24471;&#22810;&#30340;&#25490;&#21517;&#27169;&#22411;&#21644;&#20813;&#36153;&#35821;&#35328;&#27169;&#22411;BLOOM&#65292;&#22312;&#22810;&#20010;&#33521;&#25991;&#26816;&#32034;&#38598;&#21512;&#19978;&#26174;&#33879;&#25913;&#36827;&#20102;&#25490;&#21517;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2301.02998</link><description>&lt;p&gt;
InPars-Light:&#25104;&#26412;&#25928;&#30410;&#39640;&#30340;&#26080;&#30417;&#30563;&#35757;&#32451;&#39640;&#25928;&#25490;&#21517;&#22120;
&lt;/p&gt;
&lt;p&gt;
InPars-Light: Cost-Effective Unsupervised Training of Efficient Rankers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2301.02998
&lt;/p&gt;
&lt;p&gt;
InPars-Light&#26159;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#20462;&#25913;&#65292;&#36890;&#36807;&#20351;&#29992;&#23567;&#24471;&#22810;&#30340;&#25490;&#21517;&#27169;&#22411;&#21644;&#20813;&#36153;&#35821;&#35328;&#27169;&#22411;BLOOM&#65292;&#22312;&#22810;&#20010;&#33521;&#25991;&#26816;&#32034;&#38598;&#21512;&#19978;&#26174;&#33879;&#25913;&#36827;&#20102;&#25490;&#21517;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24320;&#23637;&#20102;&#23545;InPars&#30340;&#21487;&#37325;&#29616;&#24615;&#30740;&#31350;&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;&#26080;&#30417;&#30563;&#35757;&#32451;&#31070;&#32463;&#25490;&#21517;&#22120;&#30340;&#26041;&#27861;&#12290;&#20316;&#20026;&#21103;&#20135;&#21697;&#65292;&#25105;&#20204;&#24320;&#21457;&#20986;&#20102;InPars-Light&#65292;&#36825;&#26159;&#23545;InPars&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#20462;&#25913;&#12290;&#19982;InPars&#19981;&#21516;&#65292;InPars-Light&#20351;&#29992;7-100&#20493;&#26356;&#23567;&#30340;&#25490;&#21517;&#27169;&#22411;&#65292;&#24182;&#19988;&#21482;&#38656;&#35201;&#19968;&#20010;&#20813;&#36153;&#25552;&#20379;&#30340;&#35821;&#35328;&#27169;&#22411;BLOOM&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;&#19982;&#19987;&#26377;&#30340;GPT-3&#27169;&#22411;&#30456;&#27604;&#65292;BLOOM&#33021;&#22815;&#20135;&#29983;&#26356;&#20934;&#30830;&#30340;&#25490;&#21517;&#22120;&#12290;&#22312;&#25152;&#26377;&#20116;&#20010;&#33521;&#25991;&#26816;&#32034;&#38598;&#21512;&#19978;&#65292;&#25105;&#20204;&#20165;&#20351;&#29992;&#19968;&#20010;30M&#21442;&#25968;&#20845;&#23618;MiniLM-30M&#25490;&#21517;&#22120;&#21644;&#19968;&#20010;&#19977;&#36873;&#20457;&#30340;&#25552;&#31034;&#65292;&#22312;nDCG&#21644;MRR&#26041;&#38754;&#65292;&#30456;&#27604;BM25&#65292;&#25105;&#20204;&#37117;&#33719;&#24471;&#20102;&#26174;&#33879;&#30340;&#65288;7%-30%&#65289;&#19988;&#20855;&#26377;&#32479;&#35745;&#23398;&#24847;&#20041;&#30340;&#25913;&#36827;&#12290;&#30456;&#21453;&#65292;&#22312;InPars&#30340;&#30740;&#31350;&#20013;&#65292;&#21482;&#26377;&#19968;&#20010;&#22823;100&#20493;&#30340;monoT5-3B&#27169;&#22411;&#33021;&#22815;&#22987;&#32456;&#32988;&#36807;BM25&#65292;&#32780;&#23567;&#24471;&#22810;&#30340;monoT5-220M&#27169;&#22411;&#65288;&#20173;&#28982;&#27604;&#25105;&#20204;&#30340;MiniLM&#25490;&#21517;&#22120;&#22823;7&#20493;&#65289;&#21482;&#26159;&#22312;MS MAR&#19978;&#32988;&#36807;BM25&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2301.02998v2 Announce Type: replace-cross  Abstract: We carried out a reproducibility study of InPars, which is a method for unsupervised training of neural rankers (Bonifacio et al., 2022). As a by-product, we developed InPars-light, which is a simple-yet-effective modification of InPars. Unlike InPars, InPars-light uses 7x-100x smaller ranking models and only a freely available language model BLOOM, which -- as we found out -- produced more accurate rankers compared to a proprietary GPT-3 model. On all five English retrieval collections (used in the original InPars study) we obtained substantial (7%-30%) and statistically significant improvements over BM25 (in nDCG and MRR) using only a 30M parameter six-layer MiniLM-30M ranker and a single three-shot prompt. In contrast, in the InPars study only a 100x larger monoT5-3B model consistently outperformed BM25, whereas their smaller monoT5-220M model (which is still 7x larger than our MiniLM ranker) outperformed BM25 only on MS MAR
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#38598;&#25104;&#35838;&#31243;&#23398;&#20064;&#21644;&#21327;&#20316;&#35757;&#32451;&#30340;&#20004;&#38454;&#27573;&#27169;&#22411;&#38598;&#25104;&#26550;&#26500;&#65292;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#25439;&#22833;&#39033;&#24212;&#29992;&#35838;&#31243;&#23398;&#20064;&#24182;&#20801;&#35768;&#21327;&#20316;&#35299;&#35835;&#65292;&#35299;&#20915;&#20102;&#36328;&#20027;&#20307;&#36816;&#21160;&#24819;&#35937;&#35299;&#30721;&#30340;&#25361;&#25112;&#24615;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2211.11460</link><description>&lt;p&gt;
&#20351;&#29992;&#38598;&#25104;&#35838;&#31243;&#23398;&#20064;&#21644;&#21327;&#20316;&#35757;&#32451;&#36827;&#34892;&#36816;&#21160;&#24819;&#35937;&#35299;&#30721;
&lt;/p&gt;
&lt;p&gt;
Motor Imagery Decoding Using Ensemble Curriculum Learning and Collaborative Training
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2211.11460
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#38598;&#25104;&#35838;&#31243;&#23398;&#20064;&#21644;&#21327;&#20316;&#35757;&#32451;&#30340;&#20004;&#38454;&#27573;&#27169;&#22411;&#38598;&#25104;&#26550;&#26500;&#65292;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#25439;&#22833;&#39033;&#24212;&#29992;&#35838;&#31243;&#23398;&#20064;&#24182;&#20801;&#35768;&#21327;&#20316;&#35299;&#35835;&#65292;&#35299;&#20915;&#20102;&#36328;&#20027;&#20307;&#36816;&#21160;&#24819;&#35937;&#35299;&#30721;&#30340;&#25361;&#25112;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20351;&#29992;&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#25968;&#25454;&#36827;&#34892;&#36328;&#20027;&#20307;&#36816;&#21160;&#24819;&#35937;&#65288;MI&#65289;&#35299;&#30721;&#30340;&#38382;&#39064;&#12290;&#22810;&#20027;&#20307;EEG&#25968;&#25454;&#38598;&#30001;&#20110;&#21508;&#31181;&#19981;&#21516;&#30340;&#20027;&#20307;&#38388;&#24046;&#24322;&#65288;&#22914;&#22823;&#33041;&#35299;&#21078;&#12289;&#20010;&#24615;&#21644;&#35748;&#30693;&#29305;&#24449;&#65289;&#32780;&#20135;&#29983;&#22810;&#31181;&#39046;&#22495;&#36716;&#31227;&#12290;&#21463;&#21040;&#39046;&#22495;&#27867;&#21270;&#25216;&#26415;&#23545;&#35299;&#20915;&#27492;&#31867;&#38382;&#39064;&#30340;&#37325;&#35201;&#24615;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#27169;&#22411;&#38598;&#25104;&#26550;&#26500;&#65292;&#30001;&#22810;&#20010;&#29305;&#24449;&#25552;&#21462;&#22120;&#65288;&#31532;&#19968;&#38454;&#27573;&#65289;&#21644;&#19968;&#20010;&#20849;&#20139;&#20998;&#31867;&#22120;&#65288;&#31532;&#20108;&#38454;&#27573;&#65289;&#26500;&#24314;&#65292;&#25105;&#20204;&#29992;&#20004;&#20010;&#26032;&#39062;&#30340;&#25439;&#22833;&#39033;&#31471;&#21040;&#31471;&#22320;&#35757;&#32451;&#23427;&#20204;&#12290;&#31532;&#19968;&#20010;&#25439;&#22833;&#24212;&#29992;&#35838;&#31243;&#23398;&#20064;&#65292;&#20419;&#20351;&#27599;&#20010;&#29305;&#24449;&#25552;&#21462;&#22120;&#19987;&#38376;&#38024;&#23545;&#19968;&#37096;&#20998;&#35757;&#32451;&#20027;&#20307;&#24182;&#20419;&#36827;&#29305;&#24449;&#22810;&#26679;&#24615;&#12290;&#31532;&#20108;&#20010;&#25439;&#22833;&#26159;&#19968;&#20010;&#20869;&#37096;&#38598;&#25104;&#33976;&#39311;&#30446;&#26631;&#65292;&#20801;&#35768;&#21327;&#20316;&#35299;&#35835;
&lt;/p&gt;
&lt;p&gt;
arXiv:2211.11460v2 Announce Type: replace-cross  Abstract: In this work, we study the problem of cross-subject motor imagery (MI) decoding from electroencephalography (EEG) data. Multi-subject EEG datasets present several kinds of domain shifts due to various inter-individual differences (e.g. brain anatomy, personality and cognitive profile). These domain shifts render multi-subject training a challenging task and also impede robust cross-subject generalization. Inspired by the importance of domain generalization techniques for tackling such issues, we propose a two-stage model ensemble architecture built with multiple feature extractors (first stage) and a shared classifier (second stage), which we train end-to-end with two novel loss terms. The first loss applies curriculum learning, forcing each feature extractor to specialize to a subset of the training subjects and promoting feature diversity. The second loss is an intra-ensemble distillation objective that allows collaborative e
&lt;/p&gt;</description></item><item><title>&#35780;&#20272;&#30456;&#20284;&#24230;&#35780;&#20998;&#20989;&#25968;&#24615;&#33021;&#21644;&#20844;&#24179;&#24615;&#36136;&#30340;&#20851;&#38190;&#24037;&#20855;&#26159;ROC&#26354;&#32447;&#65292;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#20934;&#30830;&#35780;&#20272;&#19982;ROC&#26354;&#32447;&#30456;&#20851;&#19981;&#30830;&#23450;&#24615;&#27700;&#24179;&#30340;&#26041;&#27861;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#38754;&#37096;&#35782;&#21035;&#31561;&#20855;&#26377;&#31038;&#20250;&#24433;&#21709;&#30340;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2211.07245</link><description>&lt;p&gt;
&#35780;&#20272;&#30456;&#20284;&#24230;&#35780;&#20998;&#30340;&#19981;&#30830;&#23450;&#24615;&#65306;&#38754;&#37096;&#35782;&#21035;&#20013;&#30340;&#24615;&#33021;&#19982;&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;
Assessing Uncertainty in Similarity Scoring: Performance &amp; Fairness in Face Recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2211.07245
&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#30456;&#20284;&#24230;&#35780;&#20998;&#20989;&#25968;&#24615;&#33021;&#21644;&#20844;&#24179;&#24615;&#36136;&#30340;&#20851;&#38190;&#24037;&#20855;&#26159;ROC&#26354;&#32447;&#65292;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#20934;&#30830;&#35780;&#20272;&#19982;ROC&#26354;&#32447;&#30456;&#20851;&#19981;&#30830;&#23450;&#24615;&#27700;&#24179;&#30340;&#26041;&#27861;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#38754;&#37096;&#35782;&#21035;&#31561;&#20855;&#26377;&#31038;&#20250;&#24433;&#21709;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ROC&#26354;&#32447;&#26159;&#35780;&#20272;&#30456;&#20284;&#24230;&#35780;&#20998;&#20989;&#25968;&#24615;&#33021;&#21644;&#20844;&#24179;&#24615;&#36136;&#30340;&#20027;&#35201;&#24037;&#20855;&#12290;&#20026;&#20102;&#22522;&#20110;&#32463;&#39564;ROC&#20998;&#26512;&#24471;&#20986;&#21487;&#38752;&#32467;&#35770;&#65292;&#20934;&#30830;&#35780;&#20272;&#19982;&#24863;&#20852;&#36259;&#30340;ROC&#26354;&#32447;&#30340;&#32479;&#35745;&#29256;&#26412;&#30456;&#20851;&#30340;&#19981;&#30830;&#23450;&#24615;&#27700;&#24179;&#26159;&#32477;&#23545;&#24517;&#35201;&#30340;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#20855;&#26377;&#37325;&#35201;&#31038;&#20250;&#24433;&#21709;&#30340;&#24212;&#29992;&#65292;&#22914;&#38754;&#37096;&#35782;&#21035;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#30456;&#20284;&#24615;&#20989;&#25968;&#30340;&#32463;&#39564;ROC&#26354;&#32447;&#20197;&#21450;&#29992;&#20110;&#35780;&#20272;&#20844;&#24179;&#24615;&#30340;&#21103;&#20135;&#21697;&#25351;&#26631;&#30340;&#28176;&#36817;&#20445;&#35777;&#12290;&#25105;&#20204;&#36824;&#35299;&#37322;&#65292;&#30001;&#20110;&#22312;&#30456;&#20284;&#24230;&#35780;&#20998;&#24773;&#20917;&#19979;&#65292;&#35823;&#25509;&#21463;/&#25298;&#32477;&#29575;&#30340;&#24418;&#24335;&#20026;U-&#32479;&#35745;&#37327;&#65292;&#25152;&#20197;&#22825;&#30495;&#30340;&#33258;&#21161;&#27861;&#21487;&#33021;&#20250;&#21361;&#21450;&#35780;&#20272;&#36807;&#31243;&#12290;&#24517;&#39035;&#20351;&#29992;&#19987;&#38376;&#30340;&#37325;&#26032;&#23621;&#20013;&#25216;&#26415;&#12290;&#38500;&#36827;&#34892;&#30340;&#29702;&#35770;&#20998;&#26512;&#22806;&#65292;&#36824;&#20351;&#29992;&#30495;&#23454;&#20154;&#33080;&#22270;&#20687;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#21508;&#31181;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2211.07245v2 Announce Type: replace-cross  Abstract: The ROC curve is the major tool for assessing not only the performance but also the fairness properties of a similarity scoring function. In order to draw reliable conclusions based on empirical ROC analysis, accurately evaluating the uncertainty level related to statistical versions of the ROC curves of interest is absolutely necessary, especially for applications with considerable societal impact such as Face Recognition. In this article, we prove asymptotic guarantees for empirical ROC curves of similarity functions as well as for by-product metrics useful to assess fairness. We also explain that, because the false acceptance/rejection rates are of the form of U-statistics in the case of similarity scoring, the naive bootstrap approach may jeopardize the assessment procedure. A dedicated recentering technique must be used instead. Beyond the theoretical analysis carried out, various experiments using real face image datasets
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22522;&#20110;Mamdani&#26041;&#27861;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20110;&#22810;&#36755;&#20837;&#22810;&#36755;&#20986;&#30340;&#27169;&#31946;&#36923;&#36753;&#25511;&#21046;&#22120;&#31639;&#27861;&#65292;&#24182;&#22312;Python&#20013;&#23454;&#29616;&#65292;&#32467;&#26524;&#26174;&#31034;&#22312;&#20302;&#35745;&#31639;&#25104;&#26412;&#19979;&#27927;&#34915;&#26426;&#34920;&#29616;&#26356;&#22909;</title><link>https://arxiv.org/abs/2210.00187</link><description>&lt;p&gt;
&#27169;&#31946;&#36923;&#36753;&#25511;&#21046;&#22120;&#30340;&#27927;&#34915;&#26426;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Design of Fuzzy Logic Controller for Washing Machine
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2210.00187
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22522;&#20110;Mamdani&#26041;&#27861;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20110;&#22810;&#36755;&#20837;&#22810;&#36755;&#20986;&#30340;&#27169;&#31946;&#36923;&#36753;&#25511;&#21046;&#22120;&#31639;&#27861;&#65292;&#24182;&#22312;Python&#20013;&#23454;&#29616;&#65292;&#32467;&#26524;&#26174;&#31034;&#22312;&#20302;&#35745;&#31639;&#25104;&#26412;&#19979;&#27927;&#34915;&#26426;&#34920;&#29616;&#26356;&#22909;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#25216;&#26415;&#30340;&#36827;&#27493;&#65292;&#20107;&#29289;&#21464;&#24471;&#26356;&#21152;&#20808;&#36827;&#65292;&#26426;&#22120;&#29616;&#22312;&#25191;&#34892;&#22823;&#37096;&#20998;&#25163;&#21160;&#24037;&#20316;&#12290;&#26368;&#24120;&#29992;&#30340;&#23478;&#29992;&#30005;&#22120;&#26159;&#27927;&#34915;&#26426;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;Mamdani&#26041;&#27861;&#65292;&#24182;&#22522;&#20110;&#22810;&#36755;&#20837;&#22810;&#36755;&#20986;&#21019;&#24314;&#20102;&#19968;&#20010;&#31639;&#27861;&#12290;&#35813;&#31639;&#27861;&#26159;&#29992;Python&#23454;&#29616;&#30340;&#12290;&#27169;&#25311;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27927;&#34915;&#26426;&#22312;&#20302;&#35745;&#31639;&#25104;&#26412;&#19979;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#25191;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2210.00187v2 Announce Type: replace-cross  Abstract: Things are becoming more advanced as technology advances,and machines now perform the majority of the manual work. The most often used home appliance is the washing machine for cloths. In this paper, we used the Mamdani approach and created an algorithm based on multi-input multi-output. The algorithm is implemented in Python.The results of this simulation show that the washing machine provides better execution at a low computation cost
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;RIS&#21644;ADMM&#30340;&#34987;&#21160;&#31232;&#30095;&#24863;&#30693;&#26041;&#27861;&#65292;&#36890;&#36807;&#26032;&#39062;&#30340;&#36845;&#20195;&#26041;&#27861;&#26377;&#25928;&#25233;&#21046;&#24178;&#25200;&#20449;&#21495;&#65292;&#26174;&#33879;&#25552;&#39640;&#21040;&#36798;&#26041;&#21521;&#65288;DOA&#65289;&#20272;&#35745;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2206.06172</link><description>&lt;p&gt;
&#22522;&#20110;RIS&#21644;ADMM&#30340;&#34987;&#21160;&#31232;&#30095;&#24863;&#30693;&#26041;&#27861;&#21450;&#24178;&#25200;&#21435;&#38500;
&lt;/p&gt;
&lt;p&gt;
RIS-ADMM: A RIS and ADMM-Based Passive and Sparse Sensing Method With Interference Removal
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2206.06172
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;RIS&#21644;ADMM&#30340;&#34987;&#21160;&#31232;&#30095;&#24863;&#30693;&#26041;&#27861;&#65292;&#36890;&#36807;&#26032;&#39062;&#30340;&#36845;&#20195;&#26041;&#27861;&#26377;&#25928;&#25233;&#21046;&#24178;&#25200;&#20449;&#21495;&#65292;&#26174;&#33879;&#25552;&#39640;&#21040;&#36798;&#26041;&#21521;&#65288;DOA&#65289;&#20272;&#35745;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#37325;&#26500;&#26234;&#33021;&#34920;&#38754;&#65288;RIS&#65289;&#22312;&#26410;&#26469;&#38647;&#36798;&#21644;&#26080;&#32447;&#36890;&#20449;&#39046;&#22495;&#23835;&#36215;&#20026;&#26377;&#21069;&#36884;&#30340;&#25216;&#26415;&#12290;&#26412;&#25991;&#35299;&#20915;&#20102;&#22312;&#26080;&#32447;&#36890;&#20449;&#20449;&#21495;&#21644;RIS&#20013;&#34987;&#26080;&#32447;&#25509;&#20837;&#28857;&#65288;AP&#65289;&#24178;&#25200;&#30340;&#24773;&#20917;&#19979;&#21033;&#29992;&#34987;&#21160;&#24863;&#30693;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21407;&#23376;&#33539;&#25968;&#26368;&#23567;&#21270;&#65288;ANM&#65289;&#26041;&#27861;&#26469;&#21033;&#29992;&#31354;&#38388;&#22495;&#30446;&#26631;&#31232;&#30095;&#24615;&#24182;&#20272;&#35745;&#21040;&#36798;&#26041;&#21521;&#65288;DOA&#65289;&#12290;&#28982;&#32780;&#65292;&#29992;&#20110;ANM&#38382;&#39064;&#30340;&#20256;&#32479;&#21322;&#23450;&#35268;&#21010;&#65288;SDP&#65289;&#35299;&#20915;&#26041;&#26696;&#22797;&#26434;&#19988;&#32570;&#20047;&#39640;&#25928;&#23454;&#29616;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;RIS-ADMM&#26041;&#27861;&#65292;&#36825;&#26159;&#19968;&#31181;&#21019;&#26032;&#30340;&#22522;&#20110;&#20132;&#26367;&#26041;&#21521;&#20056;&#23376;&#26041;&#27861;&#65288;ADMM&#65289;&#30340;&#36845;&#20195;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#20135;&#29983;&#38381;&#24335;&#34920;&#36798;&#24335;&#24182;&#26377;&#25928;&#25233;&#21046;&#24178;&#25200;&#20449;&#21495;&#12290;&#20223;&#30495;&#32467;&#26524;&#35777;&#23454;&#20102;&#25105;&#20204;&#30340;RIS-ADMM&#26041;&#27861;&#22312;DOA&#20272;&#35745;&#31934;&#24230;&#26041;&#38754;&#36229;&#36807;&#20102;&#29616;&#26377;&#25216;&#26415;&#65292;&#21516;&#26102;&#20445;&#25345;&#20302;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2206.06172v2 Announce Type: replace-cross  Abstract: Reconfigurable Intelligent Surfaces (RIS) emerge as promising technologies in future radar and wireless communication domains. This letter addresses the passive sensing issue utilizing wireless communication signals and RIS amidst interference from wireless access points (APs). We introduce an atomic norm minimization (ANM) approach to leverage spatial domain target sparsity and estimate the direction of arrival (DOA). However, the conventional semidefinite programming (SDP)-based solutions for the ANM problem are complex and lack efficient realization. Consequently, we propose a RIS-ADMM method, an innovative alternating direction method of multipliers (ADMM)-based iterative approach. This method yields closed-form expressions and effectively suppresses interference signals. Simulation outcomes affirm that our RIS-ADMM method surpasses existing techniques in DOA estimation accuracy while maintaining low computational complexit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#28201;&#21644;&#20445;&#23432;Q&#23398;&#20064;&#65288;MCQ&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#20026;OOD&#21160;&#20316;&#20998;&#37197;&#36866;&#24403;&#30340;&#20266;Q&#20540;&#26469;&#35757;&#32451;&#65292;&#20174;&#32780;&#22312;&#19981;&#25439;&#23475;&#27867;&#21270;&#33021;&#21147;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20215;&#20540;&#20989;&#25968;&#30340;&#20445;&#23432;&#24615;&#65292;&#36991;&#20813;&#36807;&#24230;&#39640;&#20272;&#36229;&#20986;&#20998;&#24067;&#30340;&#21160;&#20316;&#12290;</title><link>https://arxiv.org/abs/2206.04745</link><description>&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#28201;&#21644;&#20445;&#23432;Q&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Mildly Conservative Q-Learning for Offline Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2206.04745
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#28201;&#21644;&#20445;&#23432;Q&#23398;&#20064;&#65288;MCQ&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#20026;OOD&#21160;&#20316;&#20998;&#37197;&#36866;&#24403;&#30340;&#20266;Q&#20540;&#26469;&#35757;&#32451;&#65292;&#20174;&#32780;&#22312;&#19981;&#25439;&#23475;&#27867;&#21270;&#33021;&#21147;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20215;&#20540;&#20989;&#25968;&#30340;&#20445;&#23432;&#24615;&#65292;&#36991;&#20813;&#36807;&#24230;&#39640;&#20272;&#36229;&#20986;&#20998;&#24067;&#30340;&#21160;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#23450;&#20041;&#20102;&#20174;&#38745;&#24577;&#35760;&#24405;&#30340;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#32780;&#26080;&#38656;&#25345;&#32493;&#19982;&#29615;&#22659;&#36827;&#34892;&#20132;&#20114;&#30340;&#20219;&#21153;&#12290;&#23398;&#20064;&#31574;&#30053;&#19982;&#34892;&#20026;&#31574;&#30053;&#20043;&#38388;&#30340;&#20998;&#24067;&#36716;&#31227;&#20351;&#24471;&#20215;&#20540;&#20989;&#25968;&#20445;&#25345;&#20445;&#23432;&#25104;&#20026;&#24517;&#35201;&#65292;&#20197;&#30830;&#20445;&#36229;&#20986;&#20998;&#24067;&#65288;OOD&#65289;&#30340;&#21160;&#20316;&#19981;&#20250;&#34987;&#20005;&#37325;&#39640;&#20272;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#65292;&#22914;&#23545;&#26410;&#35265;&#21160;&#20316;&#36827;&#34892;&#24809;&#32602;&#25110;&#19982;&#34892;&#20026;&#31574;&#30053;&#36827;&#34892;&#27491;&#21017;&#21270;&#65292;&#37117;&#36807;&#20110;&#24754;&#35266;&#65292;&#25233;&#21046;&#20102;&#20215;&#20540;&#20989;&#25968;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#38459;&#30861;&#20102;&#24615;&#33021;&#30340;&#25552;&#39640;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#31163;&#32447;&#23398;&#20064;&#20013;&#30340;&#28201;&#21644;&#20294;&#36275;&#22815;&#20445;&#23432;&#65292;&#21516;&#26102;&#19981;&#25439;&#23475;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#28201;&#21644;&#20445;&#23432;Q&#23398;&#20064;&#65288;MCQ&#65289;&#65292;&#36890;&#36807;&#20026;OOD&#21160;&#20316;&#20998;&#37197;&#36866;&#24403;&#30340;&#20266;Q&#20540;&#26469;&#31215;&#26497;&#35757;&#32451;&#23427;&#20204;&#12290;&#25105;&#20204;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;MCQ&#20250;&#20135;&#29983;&#19968;&#20010;&#33267;&#23569;&#19982;&#34892;&#20026;&#31574;&#30053;&#19968;&#26679;&#22909;&#30340;&#31574;&#30053;&#65292;&#24182;&#19988;&#19981;&#20250;&#21457;&#29983;&#38169;&#35823;&#30340;&#36807;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2206.04745v3 Announce Type: replace-cross  Abstract: Offline reinforcement learning (RL) defines the task of learning from a static logged dataset without continually interacting with the environment. The distribution shift between the learned policy and the behavior policy makes it necessary for the value function to stay conservative such that out-of-distribution (OOD) actions will not be severely overestimated. However, existing approaches, penalizing the unseen actions or regularizing with the behavior policy, are too pessimistic, which suppresses the generalization of the value function and hinders the performance improvement. This paper explores mild but enough conservatism for offline learning while not harming generalization. We propose Mildly Conservative Q-learning (MCQ), where OOD actions are actively trained by assigning them proper pseudo Q values. We theoretically show that MCQ induces a policy that behaves at least as well as the behavior policy and no erroneous ov
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25552;&#20986;&#20102;&#26500;&#24314;&#21487;&#20449;&#36182;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#32508;&#21512;&#36335;&#32447;&#22270;&#65292;&#20851;&#27880;&#35299;&#20915;&#24615;&#33021;&#23548;&#21521;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#21487;&#33021;&#23384;&#22312;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#12289;&#27495;&#35270;&#38382;&#39064;&#21644;&#36164;&#28304;&#28040;&#32791;&#36807;&#22810;&#31561;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2205.07424</link><description>&lt;p&gt;
&#20540;&#24471;&#20449;&#36182;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65306;&#26041;&#38754;&#12289;&#26041;&#27861;&#21644;&#36235;&#21183;
&lt;/p&gt;
&lt;p&gt;
Trustworthy Graph Neural Networks: Aspects, Methods and Trends
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2205.07424
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25552;&#20986;&#20102;&#26500;&#24314;&#21487;&#20449;&#36182;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#32508;&#21512;&#36335;&#32447;&#22270;&#65292;&#20851;&#27880;&#35299;&#20915;&#24615;&#33021;&#23548;&#21521;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#21487;&#33021;&#23384;&#22312;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#12289;&#27495;&#35270;&#38382;&#39064;&#21644;&#36164;&#28304;&#28040;&#32791;&#36807;&#22810;&#31561;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25506;&#35752;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#21508;&#31181;&#29616;&#23454;&#22330;&#26223;&#20013;&#30340;&#24212;&#29992;&#65292;&#19981;&#20165;&#20165;&#20851;&#27880;&#20219;&#21153;&#34920;&#29616;&#65292;&#36824;&#30528;&#37325;&#25351;&#20986;&#24615;&#33021;&#23548;&#21521;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#21487;&#33021;&#23384;&#22312;&#23545;&#25239;&#24615;&#25915;&#20987;&#12289;&#27495;&#35270;&#24369;&#21183;&#32676;&#20307;&#12289;&#22312;&#36793;&#32536;&#35745;&#31639;&#29615;&#22659;&#20013;&#28040;&#32791;&#36164;&#28304;&#36807;&#22810;&#31561;&#38382;&#39064;&#12290;&#20026;&#20102;&#36991;&#20813;&#36825;&#20123;&#24847;&#22806;&#20260;&#23475;&#65292;&#26377;&#24517;&#35201;&#26500;&#24314;&#20855;&#26377;&#20449;&#36182;&#24615;&#30340;&#39640;&#25928;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#25552;&#20986;&#20102;&#20174;&#28041;&#21450;&#30340;&#21508;&#31181;&#35745;&#31639;&#25216;&#26415;&#35270;&#35282;&#26500;&#24314;&#21487;&#20449;&#36182;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#20840;&#38754;&#36335;&#32447;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2205.07424v2 Announce Type: replace-cross  Abstract: Graph neural networks (GNNs) have emerged as a series of competent graph learning methods for diverse real-world scenarios, ranging from daily applications like recommendation systems and question answering to cutting-edge technologies such as drug discovery in life sciences and n-body simulation in astrophysics. However, task performance is not the only requirement for GNNs. Performance-oriented GNNs have exhibited potential adverse effects like vulnerability to adversarial attacks, unexplainable discrimination against disadvantaged groups, or excessive resource consumption in edge computing environments. To avoid these unintentional harms, it is necessary to build competent GNNs characterised by trustworthiness. To this end, we propose a comprehensive roadmap to build trustworthy GNNs from the view of the various computing technologies involved. In this survey, we introduce basic concepts and comprehensively summarise existin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23398;&#20064;&#26694;&#26550;&#26469;&#35299;&#20915;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#65292;&#36890;&#36807;&#23398;&#20064;&#20943;&#23569;&#30340;&#21704;&#23494;&#39039;&#21160;&#21147;&#23398;&#21644;&#20276;&#38543;&#21464;&#37327;&#65292;&#21033;&#29992;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#36880;&#28176;&#23398;&#20064;&#21518;&#39564;&#20998;&#24067;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#36335;&#24452;&#25506;&#32034;&#36807;&#31243;&#30340;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2111.08108</link><description>&lt;p&gt;
&#20351;&#29992;&#21704;&#23494;&#39039;&#21160;&#21147;&#23398;&#38543;&#26426;&#27169;&#22411;&#23398;&#20064;&#26368;&#20248;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Learning Optimal Control with Stochastic Models of Hamiltonian Dynamics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2111.08108
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23398;&#20064;&#26694;&#26550;&#26469;&#35299;&#20915;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#65292;&#36890;&#36807;&#23398;&#20064;&#20943;&#23569;&#30340;&#21704;&#23494;&#39039;&#21160;&#21147;&#23398;&#21644;&#20276;&#38543;&#21464;&#37327;&#65292;&#21033;&#29992;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#36880;&#28176;&#23398;&#20064;&#21518;&#39564;&#20998;&#24067;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#36335;&#24452;&#25506;&#32034;&#36807;&#31243;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24212;&#29992;&#24222;&#29305;&#37324;&#20122;&#37329;&#26368;&#22823;&#20540;&#21407;&#29702;&#65292;&#28982;&#21518;&#27714;&#35299;&#21704;&#23494;&#39039;&#21160;&#21147;&#31995;&#32479;&#65292;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#21487;&#20197;&#24471;&#21040;&#35299;&#20915;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#26032;&#39062;&#30340;&#23398;&#20064;&#26694;&#26550;&#26469;&#35299;&#20915;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#12290;&#36890;&#36807;&#23558;&#24222;&#29305;&#37324;&#20122;&#37329;&#26368;&#22823;&#20540;&#21407;&#29702;&#24212;&#29992;&#20110;&#21407;&#22987;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#65292;&#23398;&#20064;&#37325;&#28857;&#36716;&#31227;&#21040;&#20102;&#20943;&#23569;&#30340;&#21704;&#23494;&#39039;&#21160;&#21147;&#23398;&#21644;&#30456;&#24212;&#30340;&#20276;&#38543;&#21464;&#37327;&#19978;&#12290;&#20943;&#23569;&#30340;&#21704;&#23494;&#39039;&#32593;&#32476;&#21487;&#20197;&#36890;&#36807;&#21521;&#21518;&#25512;&#36827;&#26102;&#38388;&#65292;&#28982;&#21518;&#26368;&#23567;&#21270;&#20174;&#24222;&#29305;&#37324;&#20122;&#37329;&#26368;&#22823;&#20540;&#21407;&#29702;&#26465;&#20214;&#25512;&#23548;&#20986;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#23398;&#20064;&#12290;&#36890;&#36807;&#36880;&#28176;&#23398;&#20064;&#20943;&#23569;&#30340;&#21704;&#23494;&#39039;&#30340;&#21518;&#39564;&#20998;&#24067;&#65292;&#21033;&#29992;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#36827;&#19968;&#27493;&#25913;&#36827;&#20102;&#23398;&#20064;&#36807;&#31243;&#65292;&#20174;&#32780;&#23548;&#33268;&#26356;&#26377;&#25928;&#30340;&#36335;&#24452;&#25506;&#32034;&#36807;&#31243;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#23398;&#20064;&#26694;&#26550;&#24212;&#29992;&#20110;&#25511;&#21046;&#20219;&#21153;&#24182;&#33719;&#24471;&#20102;&#31454;&#20105;&#21147;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2111.08108v2 Announce Type: replace-cross  Abstract: Optimal control problems can be solved by applying the Pontryagin maximum principle and then solving for a Hamiltonian dynamical system. In this paper, we propose novel learning frameworks to tackle optimal control problems. By applying the Pontryagin maximum principle to the original optimal control problem, the learning focus shifts to reduced Hamiltonian dynamics and corresponding adjoint variables. The reduced Hamiltonian networks can be learned by going backward in time and then minimizing loss function deduced from the Pontryagin maximum principle's conditions. The learning process is further improved by progressively learning a posterior distribution of reduced Hamiltonians, utilizing a variational autoencoder which leads to more effective path exploration process. We apply our learning frameworks to control tasks and obtain competitive results.
&lt;/p&gt;</description></item><item><title>&#20914;&#31361;&#22238;&#36991;&#26799;&#24230;&#19979;&#38477;&#65288;CAGrad&#65289;&#26159;&#38024;&#23545;&#22810;&#20219;&#21153;&#23398;&#20064;&#20013;&#26799;&#24230;&#20914;&#31361;&#38382;&#39064;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#19981;&#21516;&#20219;&#21153;&#26799;&#24230;&#19981;&#19968;&#33268;&#23548;&#33268;&#30340;&#24615;&#33021;&#19979;&#38477;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2110.14048</link><description>&lt;p&gt;
&#20914;&#31361;&#22238;&#36991;&#26799;&#24230;&#19979;&#38477;&#29992;&#20110;&#22810;&#20219;&#21153;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Conflict-Averse Gradient Descent for Multi-task Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2110.14048
&lt;/p&gt;
&lt;p&gt;
&#20914;&#31361;&#22238;&#36991;&#26799;&#24230;&#19979;&#38477;&#65288;CAGrad&#65289;&#26159;&#38024;&#23545;&#22810;&#20219;&#21153;&#23398;&#20064;&#20013;&#26799;&#24230;&#20914;&#31361;&#38382;&#39064;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#19981;&#21516;&#20219;&#21153;&#26799;&#24230;&#19981;&#19968;&#33268;&#23548;&#33268;&#30340;&#24615;&#33021;&#19979;&#38477;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#30446;&#26631;&#26159;&#36890;&#36807;&#20849;&#20139;&#27169;&#22411;&#32467;&#26500;&#26469;&#23454;&#29616;&#27604;&#21333;&#20219;&#21153;&#23398;&#20064;&#26356;&#39640;&#25928;&#30340;&#23398;&#20064;&#65292;&#20197;&#36866;&#24212;&#21508;&#31181;&#20219;&#21153;&#12290;&#26631;&#20934;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#30446;&#26631;&#26159;&#26368;&#23567;&#21270;&#25152;&#26377;&#20219;&#21153;&#30340;&#24179;&#22343;&#25439;&#22833;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#36825;&#20010;&#30446;&#26631;&#36890;&#24120;&#20250;&#23548;&#33268;&#27599;&#20010;&#20219;&#21153;&#30340;&#26368;&#32456;&#34920;&#29616;&#27604;&#29420;&#31435;&#23398;&#20064;&#23427;&#20204;&#26102;&#26356;&#24046;&#12290;&#22312;&#20248;&#21270;&#22810;&#20219;&#21153;&#27169;&#22411;&#20013;&#30340;&#19968;&#20010;&#20027;&#35201;&#25361;&#25112;&#26159;&#20914;&#31361;&#26799;&#24230;&#65292;&#21363;&#19981;&#21516;&#20219;&#21153;&#30446;&#26631;&#30340;&#26799;&#24230;&#19981;&#22826;&#19968;&#33268;&#65292;&#22240;&#27492;&#36981;&#24490;&#24179;&#22343;&#26799;&#24230;&#26041;&#21521;&#21487;&#33021;&#23545;&#29305;&#23450;&#20219;&#21153;&#30340;&#24615;&#33021;&#26377;&#23475;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#20960;&#31181;&#21551;&#21457;&#24335;&#26041;&#27861;&#26469;&#25805;&#32437;&#20219;&#21153;&#26799;&#24230;&#20197;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#12290;&#20294;&#26159;&#22823;&#22810;&#25968;&#26041;&#27861;&#32570;&#20047;&#25910;&#25947;&#20445;&#35777;&#21644;/&#25110;&#21487;&#33021;&#25910;&#25947;&#21040;&#20219;&#20309;&#24085;&#32047;&#25176;&#31283;&#23450;&#28857;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21483;&#20570;&#20914;&#31361;&#22238;&#36991;&#26799;&#24230;&#19979;&#38477;&#65288;CAGrad&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#24179;&#22343;&#25439;&#22833;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
arXiv:2110.14048v2 Announce Type: replace-cross  Abstract: The goal of multi-task learning is to enable more efficient learning than single task learning by sharing model structures for a diverse set of tasks. A standard multi-task learning objective is to minimize the average loss across all tasks. While straightforward, using this objective often results in much worse final performance for each task than learning them independently. A major challenge in optimizing a multi-task model is the conflicting gradients, where gradients of different task objectives are not well aligned so that following the average gradient direction can be detrimental to specific tasks' performance. Previous work has proposed several heuristics to manipulate the task gradients for mitigating this problem. But most of them lack convergence guarantee and/or could converge to any Pareto-stationary point. In this paper, we introduce Conflict-Averse Gradient descent (CAGrad) which minimizes the average loss funct
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25193;&#23637;&#20102;&#19968;&#31181;&#29992;&#20110;&#20195;&#29702;&#24314;&#27169;&#30340;&#26412;&#20307;OASIS&#65292;&#24341;&#20837;&#20102;&#26465;&#20214;&#21644;&#26412;&#20307;&#26234;&#33021;&#21512;&#32422;&#65288;OSCs&#65289;&#65292;&#23558;&#20854;&#24212;&#29992;&#20110;&#25193;&#23637;&#21306;&#22359;&#38142;&#21644;&#26234;&#33021;&#21512;&#32422;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;OASIS OSCs&#23450;&#20041;&#30340;&#26694;&#26550;&#26550;&#26500;&#12290;</title><link>https://arxiv.org/abs/2012.01410</link><description>&lt;p&gt;
OASIS&#20013;&#30340;&#26412;&#20307;&#26234;&#33021;&#21512;&#32422;&#65306;&#20195;&#29702;&#12289;&#31995;&#32479;&#21644;&#26381;&#21153;&#38598;&#25104;&#30340;&#26412;&#20307;&#65288;&#25193;&#23637;&#29256;&#65289;
&lt;/p&gt;
&lt;p&gt;
Ontological Smart Contracts in OASIS: Ontology for Agents, Systems, and Integration of Services (Extended Version)
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2012.01410
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25193;&#23637;&#20102;&#19968;&#31181;&#29992;&#20110;&#20195;&#29702;&#24314;&#27169;&#30340;&#26412;&#20307;OASIS&#65292;&#24341;&#20837;&#20102;&#26465;&#20214;&#21644;&#26412;&#20307;&#26234;&#33021;&#21512;&#32422;&#65288;OSCs&#65289;&#65292;&#23558;&#20854;&#24212;&#29992;&#20110;&#25193;&#23637;&#21306;&#22359;&#38142;&#21644;&#26234;&#33021;&#21512;&#32422;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;OASIS OSCs&#23450;&#20041;&#30340;&#26694;&#26550;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#26465;&#20214;&#21644;&#26412;&#20307;&#26234;&#33021;&#21512;&#32422;&#65288;&#31616;&#31216;&#20026;OSCs&#65289;&#25193;&#23637;&#20102;&#19968;&#31181;&#29992;&#20110;&#24314;&#27169;&#20195;&#29702;&#21450;&#20854;&#20132;&#20114;&#30340;&#26412;&#20307;&#65292;&#31216;&#20026;&#20195;&#29702;&#12289;&#31995;&#32479;&#21644;&#26381;&#21153;&#38598;&#25104;&#30340;&#26412;&#20307;&#65288;&#31616;&#31216;OASIS&#65289;&#12290; OASIS&#20013;&#23450;&#20041;&#30340;&#26465;&#20214;&#21644;OSCs&#34987;&#24212;&#29992;&#20110;&#25193;&#23637;&#20855;&#26377;&#26412;&#20307;&#33021;&#21147;&#30340;&#25968;&#23383;&#20844;&#20849;&#36134;&#26412;&#65292;&#22914;&#21306;&#22359;&#38142;&#21644;&#20854;&#19978;&#23454;&#26045;&#30340;&#26234;&#33021;&#21512;&#32422;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2012.01410v4 Announce Type: replace  Abstract: In this contribution we extend an ontology for modelling agents and their interactions, called Ontology for Agents, Systems, and Integration of Services (in short, OASIS), with conditionals and ontological smart contracts (in short, OSCs). OSCs are ontological representations of smart contracts that allow to establish responsibilities and authorizations among agents and set agreements, whereas conditionals allow one to restrict and limit agent interactions, define activation mechanisms that trigger agent actions, and define constraints and contract terms on OSCs. Conditionals and OSCs, as defined in OASIS, are applied to extend with ontological capabilities digital public ledgers such as the blockchain and smart contracts implemented on it. We will also sketch the architecture of a framework based on the OASIS definition of OSCs that exploits the Ethereum platform and the Interplanetary File System.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22810;&#20803;&#36125;&#22612;&#28151;&#21512;&#27169;&#22411;&#65288;MBMM&#65289;&#30340;&#26032;&#30340;&#27010;&#29575;&#27169;&#22411;&#65292;&#29992;&#20110;&#36719;&#32858;&#31867;&#12290;MBMM&#36890;&#36807;&#20854;&#28789;&#27963;&#30340;&#22810;&#20803;&#36125;&#22612;&#20998;&#24067;&#30340;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;&#36866;&#24212;&#19981;&#21516;&#30340;&#32858;&#31867;&#24418;&#29366;&#65292;&#24182;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#20854;&#36866;&#24212;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.16708</link><description>&lt;p&gt;
&#22810;&#20803;&#36125;&#22612;&#28151;&#21512;&#27169;&#22411;&#65306;&#20855;&#26377;&#28789;&#27963;&#32858;&#31867;&#24418;&#29366;&#30340;&#27010;&#29575;&#32858;&#31867;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Multivariate Beta Mixture Model: Probabilistic Clustering With Flexible Cluster Shapes. (arXiv:2401.16708v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16708
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22810;&#20803;&#36125;&#22612;&#28151;&#21512;&#27169;&#22411;&#65288;MBMM&#65289;&#30340;&#26032;&#30340;&#27010;&#29575;&#27169;&#22411;&#65292;&#29992;&#20110;&#36719;&#32858;&#31867;&#12290;MBMM&#36890;&#36807;&#20854;&#28789;&#27963;&#30340;&#22810;&#20803;&#36125;&#22612;&#20998;&#24067;&#30340;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;&#36866;&#24212;&#19981;&#21516;&#30340;&#32858;&#31867;&#24418;&#29366;&#65292;&#24182;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#20854;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22810;&#20803;&#36125;&#22612;&#28151;&#21512;&#27169;&#22411;&#65288;MBMM&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#30340;&#27010;&#29575;&#27169;&#22411;&#29992;&#20110;&#36719;&#32858;&#31867;&#12290;MBMM&#36890;&#36807;&#22810;&#20803;&#36125;&#22612;&#20998;&#24067;&#30340;&#28789;&#27963;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;&#36866;&#24212;&#19981;&#21516;&#30340;&#32858;&#31867;&#24418;&#29366;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;MBMM&#30340;&#23646;&#24615;&#65292;&#25551;&#36848;&#20102;&#21442;&#25968;&#23398;&#20064;&#36807;&#31243;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#36866;&#21512;&#21508;&#31181;&#32858;&#31867;&#24418;&#29366;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;&#20195;&#30721;&#21311;&#21517;&#21457;&#24067;&#22312;\url{https://github.com/hhchen1105/mbmm/}&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces the multivariate beta mixture model (MBMM), a new probabilistic model for soft clustering. MBMM adapts to diverse cluster shapes because of the flexible probability density function of the multivariate beta distribution. We introduce the properties of MBMM, describe the parameter learning procedure, and present the experimental results, showing that MBMM fits diverse cluster shapes on synthetic and real datasets. The code is released anonymously at \url{https://github.com/hhchen1105/mbmm/}.
&lt;/p&gt;</description></item><item><title>&#33258;&#25105;&#31361;&#20986;&#24335;&#29369;&#35947;&#65288;SH2&#65289;&#26159;&#19968;&#31181;&#25512;&#29702;&#26102;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#39044;&#27979;&#27010;&#29575;&#36739;&#20302;&#30340;&#26631;&#35760;&#65292;&#24182;&#24378;&#35843;&#23427;&#20204;&#30340;&#24046;&#24322;&#65292;&#20174;&#32780;&#24110;&#21161;&#35821;&#35328;&#27169;&#22411;&#26356;&#20934;&#30830;&#22320;&#35299;&#30721;&#12290;</title><link>http://arxiv.org/abs/2401.05930</link><description>&lt;p&gt;
SH2: &#33258;&#25105;&#31361;&#20986;&#24335;&#29369;&#35947;&#24110;&#21161;&#24744;&#26356;&#20934;&#30830;&#35299;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
SH2: Self-Highlighted Hesitation Helps You Decode More Truthfully. (arXiv:2401.05930v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05930
&lt;/p&gt;
&lt;p&gt;
&#33258;&#25105;&#31361;&#20986;&#24335;&#29369;&#35947;&#65288;SH2&#65289;&#26159;&#19968;&#31181;&#25512;&#29702;&#26102;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#39044;&#27979;&#27010;&#29575;&#36739;&#20302;&#30340;&#26631;&#35760;&#65292;&#24182;&#24378;&#35843;&#23427;&#20204;&#30340;&#24046;&#24322;&#65292;&#20174;&#32780;&#24110;&#21161;&#35821;&#35328;&#27169;&#22411;&#26356;&#20934;&#30830;&#22320;&#35299;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#25991;&#26412;&#29983;&#25104;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;LLMs&#20173;&#28982;&#23384;&#22312;&#24187;&#35273;&#38382;&#39064;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25512;&#29702;&#26102;&#26041;&#27861;&#65292;&#21363;&#33258;&#25105;&#31361;&#20986;&#24335;&#29369;&#35947;(SH2)&#65292;&#20197;&#24110;&#21161;LLMs&#26356;&#20934;&#30830;&#22320;&#35299;&#30721;&#12290;SH2&#22522;&#20110;&#20449;&#24687;&#29702;&#35770;&#20013;&#19968;&#20010;&#31616;&#21333;&#30340;&#20107;&#23454;&#65292;&#21363;&#23545;&#20110;LLMs&#32780;&#35328;&#65292;&#39044;&#27979;&#27010;&#29575;&#36739;&#20302;&#30340;&#26631;&#35760;&#24448;&#24448;&#26356;&#20855;&#20449;&#24687;&#37327;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;LLMs&#32473;&#20104;&#36739;&#20302;&#27010;&#29575;&#30340;&#26631;&#35760;&#26356;&#26377;&#21487;&#33021;&#19982;&#20107;&#23454;&#20449;&#24687;&#65288;&#22914;&#21517;&#35789;&#12289;&#19987;&#26377;&#21517;&#35789;&#21644;&#24418;&#23481;&#35789;&#65289;&#23494;&#20999;&#30456;&#20851;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#36873;&#25321;&#27010;&#29575;&#26368;&#20302;&#30340;&#26631;&#35760;&#24182;&#23558;&#20854;&#36830;&#25509;&#21040;&#21407;&#22987;&#19978;&#19979;&#25991;&#20013;&#26469;&#8220;&#31361;&#20986;&#8221;&#20107;&#23454;&#20449;&#24687;&#65292;&#20174;&#32780;&#36843;&#20351;&#27169;&#22411;&#22312;&#29983;&#25104;&#20043;&#21069;&#22810;&#27425;&#38405;&#35835;&#21644;&#29369;&#35947;&#36825;&#20123;&#26631;&#35760;&#12290;&#22312;&#35299;&#30721;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#36824;&#37319;&#29992;&#23545;&#27604;&#35299;&#30721;&#30340;&#26041;&#24335;&#26469;&#24378;&#35843;&#30001;&#29369;&#35947;&#24102;&#26469;&#30340;&#36755;&#20986;&#27010;&#29575;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) demonstrate great performance in text generation. However, LLMs are still suffering from hallucinations. In this work, we propose an inference-time method, Self-Highlighted Hesitation (SH2), to help LLMs decode more truthfully. SH2 is based on a simple fact rooted in information theory that for an LLM, the tokens predicted with lower probabilities are prone to be more informative than others. Our analysis shows that the tokens assigned with lower probabilities by an LLM are more likely to be closely related to factual information, such as nouns, proper nouns, and adjectives. Therefore, we propose to ''highlight'' the factual information by selecting the tokens with the lowest probabilities and concatenating them to the original context, thus forcing the model to repeatedly read and hesitate on these tokens before generation. During decoding, we also adopt contrastive decoding to emphasize the difference in the output probabilities brought by the hesitation.
&lt;/p&gt;</description></item><item><title>ANGO&#26159;&#19968;&#20010;&#20013;&#25991;&#39046;&#22495;&#29983;&#25104;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#22522;&#20934;&#65292;&#24341;&#20837;&#20102;&#20851;&#38190;&#28857;&#20998;&#31867;&#26631;&#20934;&#65292;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#21516;&#26102;&#24314;&#31435;&#20102;&#21487;&#37327;&#21270;&#30340;&#38382;&#39064;&#38590;&#24230;&#26631;&#20934;&#65292;&#23545;&#27169;&#22411;&#35757;&#32451;&#25552;&#20379;&#20102;&#26356;&#31934;&#30830;&#30340;&#25351;&#23548;&#12290;</title><link>http://arxiv.org/abs/2401.04898</link><description>&lt;p&gt;
ANGO: &#19968;&#20010;&#38754;&#21521;&#29983;&#25104;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20013;&#25991;&#39046;&#22495;&#35780;&#20272;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
ANGO: A Next-Level Evaluation Benchmark For Generation-Oriented Language Models In Chinese Domain. (arXiv:2401.04898v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04898
&lt;/p&gt;
&lt;p&gt;
ANGO&#26159;&#19968;&#20010;&#20013;&#25991;&#39046;&#22495;&#29983;&#25104;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#22522;&#20934;&#65292;&#24341;&#20837;&#20102;&#20851;&#38190;&#28857;&#20998;&#31867;&#26631;&#20934;&#65292;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#21516;&#26102;&#24314;&#31435;&#20102;&#21487;&#37327;&#21270;&#30340;&#38382;&#39064;&#38590;&#24230;&#26631;&#20934;&#65292;&#23545;&#27169;&#22411;&#35757;&#32451;&#25552;&#20379;&#20102;&#26356;&#31934;&#30830;&#30340;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20986;&#29616;&#20102;&#21508;&#31181;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#35780;&#20272;&#25968;&#25454;&#38598;&#65292;&#20294;&#20854;&#20013;&#22823;&#22810;&#25968;&#23384;&#22312;&#25490;&#21517;&#22833;&#30495;&#21644;&#27169;&#22411;&#33021;&#21147;&#20998;&#26512;&#22256;&#38590;&#30340;&#38382;&#39064;&#12290;&#38024;&#23545;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;ANGO&#65292;&#19968;&#20010;&#20013;&#25991;&#22810;&#39033;&#36873;&#25321;&#39064;&#35780;&#20272;&#22522;&#20934;&#12290;ANGO&#39318;&#27425;&#25552;&#20986;&#20102;&#8220;&#20851;&#38190;&#28857;&#8221;&#20998;&#31867;&#26631;&#20934;&#65292;ANGO&#20013;&#30340;&#27599;&#20010;&#38382;&#39064;&#21487;&#20197;&#23545;&#24212;&#22810;&#20010;&#20851;&#38190;&#28857;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#35780;&#20272;&#32467;&#26524;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#22522;&#20110;&#30495;&#20154;&#34920;&#29616;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#21487;&#37327;&#21270;&#30340;&#38382;&#39064;&#38590;&#24230;&#26631;&#20934;&#65292;&#24182;&#23558;ANGO&#38382;&#39064;&#20998;&#20026;9&#20010;&#38590;&#24230;&#32423;&#21035;&#65292;&#20026;&#27169;&#22411;&#35757;&#32451;&#25552;&#20379;&#20102;&#26356;&#31934;&#30830;&#30340;&#25351;&#23548;&#12290;&#20026;&#20102;&#26368;&#23567;&#21270;&#25968;&#25454;&#27844;&#28431;&#30340;&#24433;&#21709;&#24182;&#20805;&#20998;&#21033;&#29992;ANGO&#30340;&#21019;&#26032;&#29305;&#28857;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#29420;&#23478;&#25277;&#26679;&#31574;&#30053;&#21644;&#26032;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#25903;&#25345;&#24555;&#36895;&#27979;&#35797;&#38598;&#36845;&#20195;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;ANGO&#23545;&#27169;&#22411;&#25552;&#20986;&#20102;&#26356;&#22823;&#30340;&#25361;&#25112;&#65292;&#24182;&#22312;&#35780;&#20272;&#32467;&#26524;&#20013;&#25581;&#31034;&#20986;&#26356;&#22810;&#32454;&#33410;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, various Large Language Models (LLMs) evaluation datasets have emerged, but most of them have issues with distorted rankings and difficulty in model capabilities analysis. Addressing these concerns, this paper introduces ANGO, a Chinese multi-choice question evaluation benchmark. ANGO proposes \textit{Keypoint} categorization standard for the first time, each question in ANGO can correspond to multiple keypoints, effectively enhancing interpretability of evaluation results. Base on performance of real humans, we build a quantifiable question difficulty standard and divide ANGO questions into 9 difficulty levels, which provide more precise guidance for model training. To minimize data leakage impact and fully leverage ANGO's innovative features, we have engineered exclusive sampling strategies and a new evaluation framework that support swift testset iteration. Our experiments demonstrate that ANGO poses a stronger challenge to models and reveals more details in evaluation resu
&lt;/p&gt;</description></item><item><title>ICMC-ASR&#25361;&#25112;&#36187;&#26159;&#20026;&#20102;&#20419;&#36827;&#39550;&#39542;&#22330;&#26223;&#19979;&#30340;&#35821;&#38899;&#22788;&#29702;&#21644;&#35782;&#21035;&#30740;&#31350;&#32780;&#20030;&#21150;&#30340;&#65292;&#21253;&#25324;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#21644;&#33258;&#21160;&#35821;&#38899;&#20998;&#31163;&#21644;&#35782;&#21035;&#65288;ASDR&#65289;&#20004;&#20010;&#36187;&#36947;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#21892;&#12290;&#26368;&#32456;&#65292;USTCiflytek&#38431;&#22312;ASR&#36187;&#36947;&#19978;&#33719;&#24471;&#20102;13.16%&#30340;CER&#65292;ASDR&#36187;&#36947;&#19978;&#33719;&#24471;&#20102;21.48%&#30340;cpCER&#12290;</title><link>http://arxiv.org/abs/2401.03473</link><description>&lt;p&gt;
ICMC-ASR&#65306;ICASSP 2024&#24180;&#27773;&#36710;&#22810;&#36890;&#36947;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#25361;&#25112;&#36187;
&lt;/p&gt;
&lt;p&gt;
ICMC-ASR: The ICASSP 2024 In-Car Multi-Channel Automatic Speech Recognition Challenge. (arXiv:2401.03473v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03473
&lt;/p&gt;
&lt;p&gt;
ICMC-ASR&#25361;&#25112;&#36187;&#26159;&#20026;&#20102;&#20419;&#36827;&#39550;&#39542;&#22330;&#26223;&#19979;&#30340;&#35821;&#38899;&#22788;&#29702;&#21644;&#35782;&#21035;&#30740;&#31350;&#32780;&#20030;&#21150;&#30340;&#65292;&#21253;&#25324;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#21644;&#33258;&#21160;&#35821;&#38899;&#20998;&#31163;&#21644;&#35782;&#21035;&#65288;ASDR&#65289;&#20004;&#20010;&#36187;&#36947;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#21892;&#12290;&#26368;&#32456;&#65292;USTCiflytek&#38431;&#22312;ASR&#36187;&#36947;&#19978;&#33719;&#24471;&#20102;13.16%&#30340;CER&#65292;ASDR&#36187;&#36947;&#19978;&#33719;&#24471;&#20102;21.48%&#30340;cpCER&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#20419;&#36827;&#39550;&#39542;&#22330;&#26223;&#19979;&#30340;&#35821;&#38899;&#22788;&#29702;&#21644;&#35782;&#21035;&#30740;&#31350;&#65292;&#25105;&#20204;&#22522;&#20110;ISCSLP 2022&#24180;&#24230;&#20030;&#21150;&#30340;&#26234;&#33021;&#24231;&#33329;&#35821;&#38899;&#35782;&#21035;&#25361;&#25112;&#36187;&#65288;ICSRC&#65289;&#30340;&#25104;&#21151;&#32463;&#39564;&#65292;&#25512;&#20986;&#20102;ICASSP 2024&#24180;&#27773;&#36710;&#22810;&#36890;&#36947;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ICMC-ASR&#65289;&#25361;&#25112;&#36187;&#12290;&#35813;&#25361;&#25112;&#25910;&#38598;&#20102;100&#22810;&#23567;&#26102;&#30340;&#26032;&#33021;&#28304;&#27773;&#36710;&#20869;&#37096;&#22810;&#36890;&#36947;&#35821;&#38899;&#25968;&#25454;&#20197;&#21450;40&#23567;&#26102;&#30340;&#22122;&#22768;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#12290;&#35774;&#31435;&#20102;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#21644;&#33258;&#21160;&#35821;&#38899;&#20998;&#31163;&#21644;&#35782;&#21035;&#65288;ASDR&#65289;&#20004;&#20010;&#36187;&#36947;&#65292;&#24182;&#20998;&#21035;&#20351;&#29992;&#23383;&#31526;&#38169;&#35823;&#29575;&#65288;CER&#65289;&#21644;&#36830;&#25509;&#26368;&#23567;&#32622;&#25442;&#23383;&#31526;&#38169;&#35823;&#29575;&#65288;cpCER&#65289;&#20316;&#20026;&#35780;&#20272;&#25351;&#26631;&#12290;&#24635;&#20307;&#19978;&#65292;ICMC-ASR&#25361;&#25112;&#36187;&#21560;&#24341;&#20102;98&#25903;&#21442;&#36187;&#38431;&#20237;&#65292;&#24182;&#22312;&#20004;&#20010;&#36187;&#36947;&#19978;&#25910;&#21040;&#20102;53&#20010;&#26377;&#25928;&#32467;&#26524;&#12290;&#26368;&#32456;&#65292;USTCiflytek&#38431;&#22312;ASR&#36187;&#36947;&#19978;&#23454;&#29616;&#20102;13.16%&#30340;CER&#65292;&#22312;ASDR&#36187;&#36947;&#19978;&#23454;&#29616;&#20102;21.48%&#30340;cpCER&#65292;&#20998;&#21035;&#30456;&#23545;&#20110;&#25105;&#20204;&#25361;&#25112;&#36187;&#20934;&#21017;&#30340;&#32477;&#23545;&#25913;&#21892;&#29575;&#20026;13.08%&#21644;51.4%&#12290;
&lt;/p&gt;
&lt;p&gt;
To promote speech processing and recognition research in driving scenarios, we build on the success of the Intelligent Cockpit Speech Recognition Challenge (ICSRC) held at ISCSLP 2022 and launch the ICASSP 2024 In-Car Multi-Channel Automatic Speech Recognition (ICMC-ASR) Challenge. This challenge collects over 100 hours of multi-channel speech data recorded inside a new energy vehicle and 40 hours of noise for data augmentation. Two tracks, including automatic speech recognition (ASR) and automatic speech diarization and recognition (ASDR) are set up, using character error rate (CER) and concatenated minimum permutation character error rate (cpCER) as evaluation metrics, respectively. Overall, the ICMC-ASR Challenge attracts 98 participating teams and receives 53 valid results in both tracks. In the end, first-place team USTCiflytek achieves a CER of 13.16% in the ASR track and a cpCER of 21.48% in the ASDR track, showing an absolute improvement of 13.08% and 51.4% compared to our chal
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#25991;&#26412;&#20998;&#31867;&#20013;&#27010;&#24565;&#32423;&#21035;&#30340;&#35823;&#30456;&#20851;&#24615;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;ChatGPT&#20998;&#37197;&#27010;&#24565;&#26631;&#31614;&#21644;&#24341;&#20837;&#25968;&#25454;&#20877;&#24179;&#34913;&#25216;&#26415;&#26469;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2311.08648</link><description>&lt;p&gt;
&#22312;&#25991;&#26412;&#20998;&#31867;&#20013;&#25506;&#32034;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#27010;&#24565;&#32423;&#21035;&#30340;&#35823;&#30456;&#20851;&#24615;
&lt;/p&gt;
&lt;p&gt;
Explore Spurious Correlations at the Concept Level in Language Models for Text Classification. (arXiv:2311.08648v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.08648
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#25991;&#26412;&#20998;&#31867;&#20013;&#27010;&#24565;&#32423;&#21035;&#30340;&#35823;&#30456;&#20851;&#24615;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;ChatGPT&#20998;&#37197;&#27010;&#24565;&#26631;&#31614;&#21644;&#24341;&#20837;&#25968;&#25454;&#20877;&#24179;&#34913;&#25216;&#26415;&#26469;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#22312;&#20247;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#37319;&#29992;&#20102;&#24494;&#35843;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#27861;&#12290;&#34429;&#28982;&#35821;&#35328;&#27169;&#22411;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#20294;&#30001;&#20110;&#35757;&#32451;&#25968;&#25454;&#20013;&#26631;&#31614;&#20998;&#24067;&#19981;&#24179;&#34913;&#25110;&#19978;&#19979;&#25991;&#23398;&#20064;&#23454;&#20363;&#20135;&#29983;&#30340;&#35823;&#30456;&#20851;&#24615;&#65292;&#23427;&#20204;&#38754;&#20020;&#30528;&#40065;&#26834;&#24615;&#25361;&#25112;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#35789;&#35821;&#12289;&#30701;&#35821;&#21644;&#21477;&#27861;&#29305;&#24449;&#19978;&#65292;&#24573;&#35270;&#20102;&#27010;&#24565;&#32423;&#21035;&#30340;&#30740;&#31350;&#65292;&#36825;&#24448;&#24448;&#26159;&#30001;&#20110;&#32570;&#20047;&#27010;&#24565;&#26631;&#31614;&#21644;&#38590;&#20197;&#30830;&#23450;&#36755;&#20837;&#25991;&#26412;&#20013;&#30340;&#27010;&#24565;&#20869;&#23481;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#20027;&#35201;&#36129;&#29486;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;ChatGPT&#20026;&#25991;&#26412;&#20998;&#37197;&#27010;&#24565;&#26631;&#31614;&#65292;&#35780;&#20272;&#27169;&#22411;&#22312;&#24494;&#35843;&#25110;&#19978;&#19979;&#25991;&#23398;&#20064;&#27979;&#35797;&#25968;&#25454;&#20013;&#30340;&#27010;&#24565;&#20559;&#24046;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#24403;&#35821;&#35328;&#27169;&#22411;&#22312;&#35757;&#32451;&#25110;&#25552;&#31034;&#20013;&#36935;&#21040;&#27010;&#24565;&#21644;&#26631;&#31614;&#20043;&#38388;&#30340;&#35823;&#30456;&#20851;&#24615;&#26102;&#65292;&#20250;&#37319;&#21462;&#39044;&#27979;&#30340;&#25463;&#24452;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#25968;&#25454;&#20877;&#24179;&#34913;&#25216;&#26415;&#65292;&#23558;ChatGPT&#29983;&#25104;&#30340;&#21453;&#20107;&#23454;&#25968;&#25454;&#32435;&#20837;&#20854;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models (LMs) have achieved notable success in numerous NLP tasks, employing both fine-tuning and in-context learning (ICL) methods. While language models demonstrate exceptional performance, they face robustness challenges due to spurious correlations arising from imbalanced label distributions in training data or ICL exemplars. Previous research has primarily concentrated on word, phrase, and syntax features, neglecting the concept level, often due to the absence of concept labels and difficulty in identifying conceptual content in input texts. This paper introduces two main contributions. First, we employ ChatGPT to assign concept labels to texts, assessing concept bias in models during fine-tuning or ICL on test data. We find that LMs, when encountering spurious correlations between a concept and a label in training or prompts, resort to shortcuts for predictions. Second, we introduce a data rebalancing technique that incorporates ChatGPT-generated counterfactual data, ther
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#22240;&#26524;&#22270;&#25277;&#35937;&#20174;&#35266;&#27979;&#26102;&#38388;&#24207;&#21015;&#20013;&#35782;&#21035;&#24178;&#39044;&#24635;&#25928;&#24212;&#30340;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#25193;&#23637;&#25688;&#35201;&#22240;&#26524;&#22270;&#20013;&#24635;&#25928;&#24212;&#24635;&#26159;&#21487;&#35782;&#21035;&#30340;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#25688;&#35201;&#22240;&#26524;&#22270;&#20013;&#24635;&#25928;&#24212;&#21487;&#35782;&#21035;&#30340;&#24517;&#35201;&#21644;&#20805;&#20998;&#30340;&#22270;&#24418;&#26465;&#20214;&#65292;&#24182;&#25552;&#20379;&#20102;&#35843;&#25972;&#38598;&#21512;&#20197;&#20272;&#35745;&#24635;&#25928;&#24212;&#12290;</title><link>http://arxiv.org/abs/2310.14691</link><description>&lt;p&gt;
&#26102;&#24207;&#22240;&#26524;&#22270;&#30340;&#25277;&#35937;&#20013;&#24635;&#25928;&#24212;&#30340;&#21487;&#35782;&#21035;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Identifiability of total effects from abstractions of time series causal graphs. (arXiv:2310.14691v2 [math.ST] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.14691
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#22240;&#26524;&#22270;&#25277;&#35937;&#20174;&#35266;&#27979;&#26102;&#38388;&#24207;&#21015;&#20013;&#35782;&#21035;&#24178;&#39044;&#24635;&#25928;&#24212;&#30340;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#25193;&#23637;&#25688;&#35201;&#22240;&#26524;&#22270;&#20013;&#24635;&#25928;&#24212;&#24635;&#26159;&#21487;&#35782;&#21035;&#30340;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#25688;&#35201;&#22240;&#26524;&#22270;&#20013;&#24635;&#25928;&#24212;&#21487;&#35782;&#21035;&#30340;&#24517;&#35201;&#21644;&#20805;&#20998;&#30340;&#22270;&#24418;&#26465;&#20214;&#65292;&#24182;&#25552;&#20379;&#20102;&#35843;&#25972;&#38598;&#21512;&#20197;&#20272;&#35745;&#24635;&#25928;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#20165;&#22522;&#20110;&#31995;&#32479;&#30340;&#22240;&#26524;&#22270;&#25277;&#35937;&#20174;&#35266;&#27979;&#26102;&#38388;&#24207;&#21015;&#20013;&#35782;&#21035;&#24178;&#39044;&#24635;&#25928;&#24212;&#30340;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#20004;&#31181;&#31867;&#22411;&#30340;&#25277;&#35937;&#65306;&#25193;&#23637;&#25688;&#35201;&#22240;&#26524;&#22270;&#23558;&#25152;&#26377;&#28382;&#21518;&#22240;&#26524;&#20851;&#31995;&#28151;&#28102;&#22312;&#19968;&#36215;&#65292;&#20294;&#21306;&#20998;&#28382;&#21518;&#21644;&#30636;&#26102;&#20851;&#31995;&#65307;&#32780;&#25688;&#35201;&#22240;&#26524;&#22270;&#21017;&#19981;&#25552;&#20379;&#20219;&#20309;&#20851;&#20110;&#22240;&#26524;&#20851;&#31995;&#28382;&#21518;&#30340;&#25351;&#31034;&#12290;&#25105;&#20204;&#35777;&#26126;&#25193;&#23637;&#25688;&#35201;&#22240;&#26524;&#22270;&#20013;&#24635;&#25928;&#24212;&#24635;&#26159;&#21487;&#35782;&#21035;&#30340;&#65292;&#24182;&#19988;&#25105;&#20204;&#25552;&#20379;&#20102;&#25688;&#35201;&#22240;&#26524;&#22270;&#20013;&#30340;&#21487;&#35782;&#21035;&#24615;&#25152;&#24517;&#38656;&#21644;&#20805;&#20998;&#30340;&#22270;&#24418;&#26465;&#20214;&#12290;&#27492;&#22806;&#65292;&#22312;&#24635;&#25928;&#24212;&#21487;&#35782;&#21035;&#26102;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#29992;&#20110;&#20272;&#35745;&#24635;&#25928;&#24212;&#30340;&#35843;&#25972;&#38598;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the problem of identifiability of the total effect of an intervention from observational time series only given an abstraction of the causal graph of the system. Specifically, we consider two types of abstractions: the extended summary causal graph which conflates all lagged causal relations but distinguishes between lagged and instantaneous relations; and the summary causal graph which does not give any indication about the lag between causal relations. We show that the total effect is always identifiable in extended summary causal graphs and we provide necessary and sufficient graphical conditions for identifiability in summary causal graphs. Furthermore, we provide adjustment sets allowing to estimate the total effect whenever it is identifiable.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#36816;&#21160;&#24847;&#21521;&#35299;&#30721;&#39046;&#22495;&#24212;&#29992;&#19981;&#21516;&#36890;&#36947;&#20851;&#27880;&#26426;&#21046;&#30340;&#21487;&#34892;&#24615;&#65292;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#36731;&#37327;&#32423;&#26550;&#26500;&#26694;&#26550;&#65292;&#24182;&#22312;&#21516;&#19968;&#29615;&#22659;&#20013;&#27604;&#36739;&#23427;&#20204;&#30340;&#24433;&#21709;&#65292;&#32467;&#26524;&#34920;&#26126;&#36825;&#20123;&#26426;&#21046;&#30340;&#26131;&#38598;&#25104;&#24615;&#21644;&#20302;&#35745;&#31639;&#22797;&#26434;&#24230;&#20351;&#20854;&#25104;&#20026;BCI&#20013;&#36816;&#21160;&#24847;&#21521;&#35299;&#30721;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.11198</link><description>&lt;p&gt;
EEG&#36816;&#21160;&#24847;&#21521;&#35299;&#30721;&#65306;&#19968;&#31181;&#19982;&#36890;&#36947;&#20851;&#27880;&#26426;&#21046;&#30456;&#27604;&#36739;&#30340;&#20998;&#26512;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
EEG motor imagery decoding: A framework for comparative analysis with channel attention mechanisms. (arXiv:2310.11198v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11198
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#36816;&#21160;&#24847;&#21521;&#35299;&#30721;&#39046;&#22495;&#24212;&#29992;&#19981;&#21516;&#36890;&#36947;&#20851;&#27880;&#26426;&#21046;&#30340;&#21487;&#34892;&#24615;&#65292;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#36731;&#37327;&#32423;&#26550;&#26500;&#26694;&#26550;&#65292;&#24182;&#22312;&#21516;&#19968;&#29615;&#22659;&#20013;&#27604;&#36739;&#23427;&#20204;&#30340;&#24433;&#21709;&#65292;&#32467;&#26524;&#34920;&#26126;&#36825;&#20123;&#26426;&#21046;&#30340;&#26131;&#38598;&#25104;&#24615;&#21644;&#20302;&#35745;&#31639;&#22797;&#26434;&#24230;&#20351;&#20854;&#25104;&#20026;BCI&#20013;&#36816;&#21160;&#24847;&#21521;&#35299;&#30721;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30340;&#30446;&#26631;&#26159;&#25506;&#35752;&#22312;&#22823;&#33041;-&#35745;&#31639;&#26426;&#25509;&#21475;&#65288;BCI&#65289;&#39046;&#22495;&#20013;&#24212;&#29992;&#21508;&#31181;&#36890;&#36947;&#20851;&#27880;&#26426;&#21046;&#20110;&#36816;&#21160;&#24847;&#21521;&#35299;&#30721;&#12290;&#36890;&#36947;&#20851;&#27880;&#26426;&#21046;&#21487;&#20197;&#35270;&#20026;&#20256;&#32479;&#29992;&#20110;&#36816;&#21160;&#24847;&#21521;&#35299;&#30721;&#30340;&#31354;&#38388;&#28388;&#27874;&#22120;&#30340;&#24378;&#22823;&#28436;&#36827;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#23558;&#36825;&#20123;&#26426;&#21046;&#25972;&#21512;&#21040;&#19968;&#20010;&#36731;&#37327;&#32423;&#26550;&#26500;&#26694;&#26550;&#20013;&#65292;&#31995;&#32479;&#22320;&#27604;&#36739;&#23427;&#20204;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#31934;&#24515;&#26500;&#24314;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#36731;&#37327;&#32423;&#30340;&#22522;&#20934;&#26550;&#26500;&#65292;&#26088;&#22312;&#26080;&#32541;&#38598;&#25104;&#19981;&#21516;&#30340;&#36890;&#36947;&#20851;&#27880;&#26426;&#21046;&#12290;&#36825;&#31181;&#26041;&#27861;&#19982;&#20043;&#21069;&#30340;&#30740;&#31350;&#30456;&#21453;&#65292;&#20043;&#21069;&#30340;&#30740;&#31350;&#21482;&#30740;&#31350;&#19968;&#20010;&#20851;&#27880;&#26426;&#21046;&#65292;&#24182;&#19988;&#36890;&#24120;&#26500;&#24314;&#19968;&#20010;&#38750;&#24120;&#22797;&#26434;&#12289;&#26377;&#26102;&#23884;&#22871;&#30340;&#26550;&#26500;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#20351;&#25105;&#20204;&#33021;&#22815;&#22312;&#30456;&#21516;&#24773;&#20917;&#19979;&#35780;&#20272;&#21644;&#27604;&#36739;&#19981;&#21516;&#30340;&#20851;&#27880;&#26426;&#21046;&#30340;&#24433;&#21709;&#12290;&#26131;&#20110;&#38598;&#25104;&#19981;&#21516;&#30340;&#36890;&#36947;&#20851;&#27880;&#26426;&#21046;&#20197;&#21450;&#20302;&#35745;&#31639;&#22797;&#26434;&#24230;&#20351;&#25105;&#20204;&#33021;&#22815;&#39640;&#25928;&#22320;&#30740;&#31350;&#21644;&#25512;&#36827;BCI&#20013;&#30340;&#36816;&#21160;&#24847;&#21521;&#35299;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
The objective of this study is to investigate the application of various channel attention mechanisms within the domain of brain-computer interface (BCI) for motor imagery decoding. Channel attention mechanisms can be seen as a powerful evolution of spatial filters traditionally used for motor imagery decoding. This study systematically compares such mechanisms by integrating them into a lightweight architecture framework to evaluate their impact. We carefully construct a straightforward and lightweight baseline architecture designed to seamlessly integrate different channel attention mechanisms. This approach is contrary to previous works which only investigate one attention mechanism and usually build a very complex, sometimes nested architecture. Our framework allows us to evaluate and compare the impact of different attention mechanisms under the same circumstances. The easy integration of different channel attention mechanisms as well as the low computational complexity enables us
&lt;/p&gt;</description></item><item><title>QLLM&#26159;&#19968;&#31181;&#20026;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#35774;&#35745;&#30340;&#20934;&#30830;&#39640;&#25928;&#30340;&#20302;&#20301;&#23485;&#21518;&#35757;&#32451;&#37327;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#33258;&#36866;&#24212;&#36890;&#36947;&#37325;&#32452;&#25216;&#26415;&#65292;&#23558;&#31163;&#32676;&#20540;&#30340;&#22823;&#23567;&#37325;&#26032;&#20998;&#37197;&#32473;&#20854;&#20182;&#36890;&#36947;&#65292;&#20174;&#32780;&#20943;&#36731;&#23427;&#20204;&#23545;&#37327;&#21270;&#33539;&#22260;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2310.08041</link><description>&lt;p&gt;
QLLM: &#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#20934;&#30830;&#39640;&#25928;&#20302;&#20301;&#23485;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
QLLM: Accurate and Efficient Low-Bitwidth Quantization for Large Language Models. (arXiv:2310.08041v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08041
&lt;/p&gt;
&lt;p&gt;
QLLM&#26159;&#19968;&#31181;&#20026;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#35774;&#35745;&#30340;&#20934;&#30830;&#39640;&#25928;&#30340;&#20302;&#20301;&#23485;&#21518;&#35757;&#32451;&#37327;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#33258;&#36866;&#24212;&#36890;&#36947;&#37325;&#32452;&#25216;&#26415;&#65292;&#23558;&#31163;&#32676;&#20540;&#30340;&#22823;&#23567;&#37325;&#26032;&#20998;&#37197;&#32473;&#20854;&#20182;&#36890;&#36947;&#65292;&#20174;&#32780;&#20943;&#36731;&#23427;&#20204;&#23545;&#37327;&#21270;&#33539;&#22260;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#30001;&#20110;&#20854;&#25152;&#38656;&#36164;&#28304;&#36807;&#22823;&#65292;&#38480;&#21046;&#20102;&#20854;&#24191;&#27867;&#24212;&#29992;&#12290;&#34429;&#28982;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;&#65288;Quantization-Aware Training&#65292;QAT&#65289;&#25552;&#20379;&#20102;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#23427;&#30340;&#35757;&#32451;&#25104;&#26412;&#36807;&#39640;&#65292;&#22240;&#27492;&#21518;&#35757;&#32451;&#37327;&#21270;&#65288;Post-Training Quantization&#65292;PTQ&#65289;&#25104;&#20026;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#26356;&#23454;&#38469;&#30340;&#26041;&#27861;&#12290;&#22312;&#29616;&#26377;&#30740;&#31350;&#20013;&#65292;&#29305;&#23450;&#36890;&#36947;&#20013;&#30340;&#28608;&#27963;&#31163;&#32676;&#20540;&#34987;&#35748;&#20026;&#26159;&#23548;&#33268;&#21518;&#35757;&#32451;&#37327;&#21270;&#20934;&#30830;&#24615;&#19979;&#38477;&#30340;&#29942;&#39048;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;QLLM&#65292;&#19968;&#31181;&#20026;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#35774;&#35745;&#30340;&#20934;&#30830;&#39640;&#25928;&#30340;&#20302;&#20301;&#23485;&#21518;&#35757;&#32451;&#37327;&#21270;&#26041;&#27861;&#12290;QLLM&#24341;&#20837;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#36890;&#36947;&#37325;&#32452;&#25216;&#26415;&#65292;&#23558;&#31163;&#32676;&#20540;&#30340;&#22823;&#23567;&#37325;&#26032;&#20998;&#37197;&#32473;&#20854;&#20182;&#36890;&#36947;&#65292;&#20174;&#32780;&#20943;&#36731;&#23427;&#20204;&#23545;&#37327;&#21270;&#33539;&#22260;&#30340;&#24433;&#21709;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#36890;&#36807;&#36890;&#36947;&#25286;&#20998;&#21644;&#36890;&#36947;&#32452;&#35013;&#65292;&#22312;&#20445;&#35777;&#20302;&#20301;&#23485;&#30340;&#24773;&#20917;&#19979;&#23558;&#31163;&#32676;&#36890;&#36947;&#20998;&#35299;&#25104;&#22810;&#20010;&#23376;&#36890;&#36947;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) excel in NLP, but their demands hinder their widespread deployment. While Quantization-Aware Training (QAT) offers a solution, its extensive training costs make Post-Training Quantization (PTQ) a more practical approach for LLMs. In existing studies, activation outliers in particular channels are identified as the bottleneck to PTQ accuracy. They propose to transform the magnitudes from activations to weights, which however offers limited alleviation or suffers from unstable gradients, resulting in a severe performance drop at low-bitwidth. In this paper, we propose QLLM, an accurate and efficient low-bitwidth PTQ method designed for LLMs. QLLM introduces an adaptive channel reassembly technique that reallocates the magnitude of outliers to other channels, thereby mitigating their impact on the quantization range. This is achieved by channel disassembly and channel assembly, which first breaks down the outlier channels into several sub-channels to ensure a 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#35266;&#23519;&#25193;&#25955;&#21644;&#20449;&#24687;&#20998;&#35299;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#25581;&#31034;&#20102;&#25193;&#25955;&#27169;&#22411;&#23398;&#20064;&#21040;&#30340;&#32454;&#31890;&#24230;&#20851;&#31995;&#65292;&#36827;&#19968;&#27493;&#35299;&#20915;&#20102;&#39640;&#32500;&#31354;&#38388;&#20013;&#20449;&#24687;&#25658;&#24102;&#21464;&#37327;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.07972</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#25193;&#25955;&#27169;&#22411;&#36890;&#36807;&#20449;&#24687;&#20998;&#35299;
&lt;/p&gt;
&lt;p&gt;
Interpretable Diffusion via Information Decomposition. (arXiv:2310.07972v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07972
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#35266;&#23519;&#25193;&#25955;&#21644;&#20449;&#24687;&#20998;&#35299;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#25581;&#31034;&#20102;&#25193;&#25955;&#27169;&#22411;&#23398;&#20064;&#21040;&#30340;&#32454;&#31890;&#24230;&#20851;&#31995;&#65292;&#36827;&#19968;&#27493;&#35299;&#20915;&#20102;&#39640;&#32500;&#31354;&#38388;&#20013;&#20449;&#24687;&#25658;&#24102;&#21464;&#37327;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#33021;&#22815;&#29992;&#20110;&#22797;&#26434;&#20851;&#31995;&#30340;&#26465;&#20214;&#29983;&#25104;&#21644;&#23494;&#24230;&#24314;&#27169;&#65292;&#22914;&#22270;&#20687;&#21644;&#25991;&#26412;&#12290;&#28982;&#32780;&#65292;&#23398;&#20064;&#21040;&#30340;&#20851;&#31995;&#30340;&#26412;&#36136;&#26159;&#19981;&#36879;&#26126;&#30340;&#65292;&#22240;&#27492;&#24456;&#38590;&#20934;&#30830;&#29702;&#35299;&#21333;&#35789;&#21644;&#22270;&#20687;&#37096;&#20998;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#25110;&#32773;&#39044;&#27979;&#24178;&#39044;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#36890;&#36807;&#35266;&#23519;&#25193;&#25955;&#21644;&#20449;&#24687;&#20998;&#35299;&#20043;&#38388;&#30340;&#31934;&#30830;&#20851;&#31995;&#65292;&#25581;&#31034;&#20102;&#25193;&#25955;&#27169;&#22411;&#23398;&#20064;&#21040;&#30340;&#32454;&#31890;&#24230;&#20851;&#31995;&#12290;&#20114;&#20449;&#24687;&#21644;&#26465;&#20214;&#20114;&#20449;&#24687;&#30340;&#31934;&#30830;&#34920;&#36798;&#21487;&#20197;&#36890;&#36807;&#21435;&#22122;&#27169;&#22411;&#26469;&#35745;&#31639;&#12290;&#27492;&#22806;&#65292;&#20063;&#21487;&#20197;&#36731;&#26494;&#20272;&#35745;&#22312;&#29305;&#23450;&#22270;&#20687;&#21644;&#26631;&#39064;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#36827;&#19968;&#27493;&#23545;&#20449;&#24687;&#36827;&#34892;&#20998;&#35299;&#65292;&#20197;&#29702;&#35299;&#39640;&#32500;&#31354;&#38388;&#20013;&#21738;&#20123;&#21464;&#37327;&#25658;&#24102;&#20449;&#24687;&#65292;&#26159;&#19968;&#20010;&#38271;&#26399;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;&#23545;&#20110;&#25193;&#25955;&#27169;&#22411;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#31181;&#33258;&#28982;&#30340;&#38750;&#36127;&#20449;&#24687;&#20998;&#35299;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Denoising diffusion models enable conditional generation and density modeling of complex relationships like images and text. However, the nature of the learned relationships is opaque making it difficult to understand precisely what relationships between words and parts of an image are captured, or to predict the effect of an intervention. We illuminate the fine-grained relationships learned by diffusion models by noticing a precise relationship between diffusion and information decomposition. Exact expressions for mutual information and conditional mutual information can be written in terms of the denoising model. Furthermore, pointwise estimates can be easily estimated as well, allowing us to ask questions about the relationships between specific images and captions. Decomposing information even further to understand which variables in a high-dimensional space carry information is a long-standing problem. For diffusion models, we show that a natural non-negative decomposition of mutu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21435;&#22122;&#20219;&#21153;&#36335;&#30001;&#30340;&#31574;&#30053;&#65292;&#36890;&#36807;&#20026;&#25193;&#25955;&#27169;&#22411;&#30340;&#19981;&#21516;&#20219;&#21153;&#24314;&#31435;&#29420;&#31435;&#30340;&#20449;&#24687;&#36335;&#24452;&#65292;&#23454;&#29616;&#20102;&#23545;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#26126;&#30830;&#32435;&#20837;&#12290;&#35813;&#26041;&#27861;&#23558;&#21435;&#22122;&#20219;&#21153;&#30340;&#20808;&#39564;&#30693;&#35782;&#26080;&#32541;&#38598;&#25104;&#21040;&#26694;&#26550;&#20013;&#65292;&#36890;&#36807;&#28608;&#27963;&#30456;&#20284;&#30340;&#36890;&#36947;&#21644;&#28369;&#21160;&#31383;&#21475;&#30340;&#26041;&#24335;&#65292;&#20805;&#20998;&#21033;&#29992;&#20102;&#30456;&#37051;&#26102;&#38388;&#27493;&#20219;&#21153;&#38388;&#30340;&#20146;&#21644;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2310.07138</link><description>&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#30340;&#21435;&#22122;&#20219;&#21153;&#36335;&#30001;
&lt;/p&gt;
&lt;p&gt;
Denoising Task Routing for Diffusion Models. (arXiv:2310.07138v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07138
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21435;&#22122;&#20219;&#21153;&#36335;&#30001;&#30340;&#31574;&#30053;&#65292;&#36890;&#36807;&#20026;&#25193;&#25955;&#27169;&#22411;&#30340;&#19981;&#21516;&#20219;&#21153;&#24314;&#31435;&#29420;&#31435;&#30340;&#20449;&#24687;&#36335;&#24452;&#65292;&#23454;&#29616;&#20102;&#23545;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#26126;&#30830;&#32435;&#20837;&#12290;&#35813;&#26041;&#27861;&#23558;&#21435;&#22122;&#20219;&#21153;&#30340;&#20808;&#39564;&#30693;&#35782;&#26080;&#32541;&#38598;&#25104;&#21040;&#26694;&#26550;&#20013;&#65292;&#36890;&#36807;&#28608;&#27963;&#30456;&#20284;&#30340;&#36890;&#36947;&#21644;&#28369;&#21160;&#31383;&#21475;&#30340;&#26041;&#24335;&#65292;&#20805;&#20998;&#21033;&#29992;&#20102;&#30456;&#37051;&#26102;&#38388;&#27493;&#20219;&#21153;&#38388;&#30340;&#20146;&#21644;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#36890;&#36807;&#23398;&#20064;&#22810;&#27493;&#21435;&#22122;&#36807;&#31243;&#29983;&#25104;&#39640;&#24230;&#36924;&#30495;&#30340;&#22270;&#20687;&#65292;&#33258;&#28982;&#22320;&#20307;&#29616;&#20102;&#22810;&#20219;&#21153;&#23398;&#20064;&#65288;MTL&#65289;&#30340;&#21407;&#29702;&#12290;&#23613;&#31649;&#25193;&#25955;&#27169;&#22411;&#21644;MTL&#20043;&#38388;&#23384;&#22312;&#22266;&#26377;&#30340;&#36830;&#25509;&#65292;&#20294;&#22312;&#35774;&#35745;&#26126;&#30830;&#23558;MTL&#32435;&#20837;&#25193;&#25955;&#27169;&#22411;&#26694;&#26550;&#30340;&#31070;&#32463;&#32467;&#26500;&#26041;&#38754;&#20173;&#23384;&#22312;&#19968;&#20010;&#26410;&#34987;&#25506;&#32034;&#30340;&#39046;&#22495;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21435;&#22122;&#20219;&#21153;&#36335;&#30001;&#65288;DTR&#65289;&#65292;&#19968;&#31181;&#23545;&#29616;&#26377;&#25193;&#25955;&#27169;&#22411;&#26550;&#26500;&#36827;&#34892;&#31616;&#21333;&#38468;&#21152;&#30340;&#31574;&#30053;&#65292;&#36890;&#36807;&#36873;&#25321;&#24615;&#22320;&#28608;&#27963;&#27169;&#22411;&#20013;&#30340;&#23376;&#36890;&#36947;&#26469;&#20026;&#21333;&#20010;&#20219;&#21153;&#24314;&#31435;&#29420;&#31435;&#30340;&#20449;&#24687;&#36335;&#24452;&#12290;DTR&#30340;&#29305;&#21035;&#21560;&#24341;&#20154;&#20043;&#22788;&#22312;&#20110;&#23427;&#23558;&#21435;&#22122;&#20219;&#21153;&#30340;&#20808;&#39564;&#30693;&#35782;&#26080;&#32541;&#38598;&#25104;&#21040;&#26694;&#26550;&#20013;&#65306;&#65288;1&#65289;&#20219;&#21153;&#20146;&#21644;&#24615;&#65306;DTR&#20026;&#30456;&#37051;&#26102;&#38388;&#27493;&#30340;&#20219;&#21153;&#28608;&#27963;&#30456;&#20284;&#30340;&#36890;&#36947;&#65292;&#24182;&#23558;&#28608;&#27963;&#30340;&#36890;&#36947;&#20316;&#20026;&#28369;&#21160;&#31383;&#21475;&#36890;&#36807;&#26102;&#38388;&#27493;&#36827;&#34892;&#31227;&#21160;&#65292;&#21033;&#29992;&#30456;&#37051;&#26102;&#38388;&#27493;&#20219;&#21153;&#38388;&#22266;&#26377;&#30340;&#24378;&#20146;&#21644;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models generate highly realistic images through learning a multi-step denoising process, naturally embodying the principles of multi-task learning (MTL). Despite the inherent connection between diffusion models and MTL, there remains an unexplored area in designing neural architectures that explicitly incorporate MTL into the framework of diffusion models. In this paper, we present Denoising Task Routing (DTR), a simple add-on strategy for existing diffusion model architectures to establish distinct information pathways for individual tasks within a single architecture by selectively activating subsets of channels in the model. What makes DTR particularly compelling is its seamless integration of prior knowledge of denoising tasks into the framework: (1) Task Affinity: DTR activates similar channels for tasks at adjacent timesteps and shifts activated channels as sliding windows through timesteps, capitalizing on the inherent strong affinity between tasks at adjacent timestep
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#24212;&#29992;&#20110;&#32852;&#24819;&#35760;&#24518;&#20013;&#30340;&#32553;&#25918;&#23450;&#24459;&#65292;&#36890;&#36807;&#39640;&#32500;&#30697;&#38453;&#21644;&#23884;&#20837;&#30340;&#22806;&#31215;&#26469;&#27169;&#25311;&#20869;&#23618;Transformer&#35821;&#35328;&#27169;&#22411;&#12290;&#20316;&#32773;&#25512;&#23548;&#20986;&#20102;&#19982;&#26679;&#26412;&#25968;&#37327;&#21644;&#21442;&#25968;&#22823;&#23567;&#30456;&#20851;&#30340;&#31934;&#30830;&#32553;&#25918;&#23450;&#24459;&#65292;&#24182;&#39564;&#35777;&#20102;&#29702;&#35770;&#32467;&#26524;&#30340;&#26377;&#25928;&#24615;&#12290;&#21516;&#26102;&#65292;&#20316;&#32773;&#36824;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#23637;&#31034;&#20102;&#23384;&#20648;&#35760;&#24518;&#20851;&#32852;&#30340;&#32454;&#31890;&#24230;&#21487;&#35270;&#21270;&#12290;</title><link>http://arxiv.org/abs/2310.02984</link><description>&lt;p&gt;
&#32553;&#25918;&#23450;&#24459;&#22312;&#32852;&#24819;&#35760;&#24518;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Scaling Laws for Associative Memories. (arXiv:2310.02984v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02984
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24212;&#29992;&#20110;&#32852;&#24819;&#35760;&#24518;&#20013;&#30340;&#32553;&#25918;&#23450;&#24459;&#65292;&#36890;&#36807;&#39640;&#32500;&#30697;&#38453;&#21644;&#23884;&#20837;&#30340;&#22806;&#31215;&#26469;&#27169;&#25311;&#20869;&#23618;Transformer&#35821;&#35328;&#27169;&#22411;&#12290;&#20316;&#32773;&#25512;&#23548;&#20986;&#20102;&#19982;&#26679;&#26412;&#25968;&#37327;&#21644;&#21442;&#25968;&#22823;&#23567;&#30456;&#20851;&#30340;&#31934;&#30830;&#32553;&#25918;&#23450;&#24459;&#65292;&#24182;&#39564;&#35777;&#20102;&#29702;&#35770;&#32467;&#26524;&#30340;&#26377;&#25928;&#24615;&#12290;&#21516;&#26102;&#65292;&#20316;&#32773;&#36824;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#23637;&#31034;&#20102;&#23384;&#20648;&#35760;&#24518;&#20851;&#32852;&#30340;&#32454;&#31890;&#24230;&#21487;&#35270;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#24456;&#21487;&#33021;&#28041;&#21450;&#21040;&#25277;&#35937;&#35268;&#21017;&#30340;&#21457;&#29616;&#21644;&#35760;&#24518;&#12290;&#26412;&#25991;&#26088;&#22312;&#30740;&#31350;&#32852;&#24819;&#35760;&#24518;&#26426;&#21046;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22522;&#20110;&#39640;&#32500;&#30697;&#38453;&#65292;&#30001;&#23884;&#20837;&#30340;&#22806;&#31215;&#32452;&#25104;&#65292;&#19982;Transformer&#35821;&#35328;&#27169;&#22411;&#30340;&#20869;&#23618;&#30456;&#20851;&#12290;&#25105;&#20204;&#25512;&#23548;&#20986;&#20851;&#20110;&#26679;&#26412;&#25968;&#37327;&#21644;&#21442;&#25968;&#35268;&#27169;&#30340;&#31934;&#30830;&#32553;&#25918;&#23450;&#24459;&#65292;&#24182;&#35752;&#35770;&#20102;&#19981;&#21516;&#20272;&#35745;&#22120;&#30340;&#32479;&#35745;&#25928;&#29575;&#65292;&#21253;&#25324;&#22522;&#20110;&#20248;&#21270;&#30340;&#31639;&#27861;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#30340;&#25968;&#20540;&#23454;&#39564;&#65292;&#20197;&#39564;&#35777;&#21644;&#35299;&#37322;&#29702;&#35770;&#32467;&#26524;&#65292;&#21253;&#25324;&#23545;&#23384;&#20648;&#35760;&#24518;&#20851;&#32852;&#30340;&#32454;&#31890;&#24230;&#21487;&#35270;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning arguably involves the discovery and memorization of abstract rules. The aim of this paper is to study associative memory mechanisms. Our model is based on high-dimensional matrices consisting of outer products of embeddings, which relates to the inner layers of transformer language models. We derive precise scaling laws with respect to sample size and parameter size, and discuss the statistical efficiency of different estimators, including optimization-based algorithms. We provide extensive numerical experiments to validate and interpret theoretical results, including fine-grained visualizations of the stored memory associations.
&lt;/p&gt;</description></item><item><title>MIDDAG&#26159;&#19968;&#20010;&#20132;&#20114;&#24335;&#31995;&#32479;&#65292;&#36890;&#36807;&#21487;&#35270;&#21270;&#31038;&#20132;&#23186;&#20307;&#19978;&#30001;COVID-19&#30456;&#20851;&#26032;&#38395;&#35302;&#21457;&#30340;&#20449;&#24687;&#20256;&#25773;&#36335;&#24452;&#65292;&#25552;&#20379;&#20840;&#38754;&#30340;&#27934;&#23519;&#21147;&#65292;&#24182;&#33021;&#26500;&#24314;&#29992;&#25143;&#31038;&#21306;&#21644;&#39044;&#27979;&#20449;&#24687;&#20256;&#25773;&#65292;&#20174;&#32780;&#36861;&#36394;&#21644;&#29702;&#35299;&#20449;&#24687;&#30340;&#20256;&#25773;&#26041;&#24335;&#12290;</title><link>http://arxiv.org/abs/2310.02529</link><description>&lt;p&gt;
MIDDAG&#65306;&#26032;&#38395;&#36208;&#21521;&#20309;&#26041;&#65311;&#36890;&#36807;&#31038;&#21306;&#32423;&#20449;&#24687;&#36335;&#24452;&#30740;&#31350;&#20449;&#24687;&#20256;&#25773;
&lt;/p&gt;
&lt;p&gt;
MIDDAG: Where Does Our News Go? Investigating Information Diffusion via Community-Level Information Pathways. (arXiv:2310.02529v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02529
&lt;/p&gt;
&lt;p&gt;
MIDDAG&#26159;&#19968;&#20010;&#20132;&#20114;&#24335;&#31995;&#32479;&#65292;&#36890;&#36807;&#21487;&#35270;&#21270;&#31038;&#20132;&#23186;&#20307;&#19978;&#30001;COVID-19&#30456;&#20851;&#26032;&#38395;&#35302;&#21457;&#30340;&#20449;&#24687;&#20256;&#25773;&#36335;&#24452;&#65292;&#25552;&#20379;&#20840;&#38754;&#30340;&#27934;&#23519;&#21147;&#65292;&#24182;&#33021;&#26500;&#24314;&#29992;&#25143;&#31038;&#21306;&#21644;&#39044;&#27979;&#20449;&#24687;&#20256;&#25773;&#65292;&#20174;&#32780;&#36861;&#36394;&#21644;&#29702;&#35299;&#20449;&#24687;&#30340;&#20256;&#25773;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;MIDDAG&#65292;&#19968;&#20010;&#30452;&#35266;&#12289;&#20132;&#20114;&#24335;&#30340;&#31995;&#32479;&#65292;&#21487;&#20197;&#21487;&#35270;&#21270;&#30001;COVID-19&#30456;&#20851;&#26032;&#38395;&#25991;&#31456;&#35302;&#21457;&#30340;&#31038;&#20132;&#23186;&#20307;&#20449;&#24687;&#20256;&#25773;&#36335;&#24452;&#65292;&#24182;&#25552;&#20379;&#21253;&#25324;&#29992;&#25143;/&#31038;&#21306;&#26131;&#24863;&#24615;&#27700;&#24179;&#12289;&#20197;&#21450;&#20449;&#24687;&#20256;&#25773;&#36807;&#31243;&#20013;&#24341;&#21457;&#30340;&#20107;&#20214;&#21644;&#24191;&#22823;&#32676;&#20247;&#30340;&#28909;&#38376;&#35266;&#28857;&#30340;&#20840;&#38754;&#27934;&#23519;&#12290;&#38500;&#20102;&#21457;&#29616;&#29992;&#25143;&#20043;&#38388;&#30340;&#20449;&#24687;&#27969;&#27169;&#24335;&#65292;&#25105;&#20204;&#36824;&#26500;&#24314;&#20102;&#29992;&#25143;&#20043;&#38388;&#30340;&#31038;&#21306;&#65292;&#24182;&#24320;&#21457;&#20102;&#20256;&#25773;&#39044;&#27979;&#33021;&#21147;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#39640;&#23618;&#27425;&#20449;&#24687;&#20256;&#25773;&#26041;&#24335;&#30340;&#36861;&#36394;&#21644;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present MIDDAG, an intuitive, interactive system that visualizes the information propagation paths on social media triggered by COVID-19-related news articles accompanied by comprehensive insights including user/community susceptibility level, as well as events and popular opinions raised by the crowd while propagating the information. Besides discovering information flow patterns among users, we construct communities among users and develop the propagation forecasting capability, enabling tracing and understanding of how information is disseminated at a higher level.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#35780;&#20272;&#26694;&#26550;&#65292;&#36890;&#36807;&#21521;&#27861;&#23448;&#25552;&#20986;&#19968;&#31995;&#21015;&#26597;&#35810;&#26469;&#35780;&#20272;LLMs&#30340;&#23545;&#35805;&#25512;&#29702;&#21644;&#35268;&#21010;&#33021;&#21147;&#12290;&#25105;&#20204;&#21457;&#29616;&#19981;&#21516;&#30340;LLMs&#22312;&#36825;&#20010;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#26174;&#33879;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2310.01468</link><description>&lt;p&gt;
&#23454;&#20307;&#25512;&#26029;&#31454;&#25216;&#22330;&#65306;&#25506;&#31350;LLMs&#30340;&#23545;&#35805;&#25512;&#29702;&#21644;&#35268;&#21010;&#33021;&#21147;&#30340;&#24179;&#21488;
&lt;/p&gt;
&lt;p&gt;
The Entity-Deduction Arena: A playground for probing the conversational reasoning and planning capabilities of LLMs. (arXiv:2310.01468v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01468
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#35780;&#20272;&#26694;&#26550;&#65292;&#36890;&#36807;&#21521;&#27861;&#23448;&#25552;&#20986;&#19968;&#31995;&#21015;&#26597;&#35810;&#26469;&#35780;&#20272;LLMs&#30340;&#23545;&#35805;&#25512;&#29702;&#21644;&#35268;&#21010;&#33021;&#21147;&#12290;&#25105;&#20204;&#21457;&#29616;&#19981;&#21516;&#30340;LLMs&#22312;&#36825;&#20010;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#26174;&#33879;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22238;&#31572;&#26126;&#30830;&#25552;&#38382;&#26102;&#38750;&#24120;&#26377;&#25928;&#12290;&#28982;&#32780;&#65292;&#24403;&#38754;&#20020;&#21547;&#31946;&#19981;&#28165;&#30340;&#26597;&#35810;&#26102;&#65292;&#23427;&#20204;&#21487;&#33021;&#34892;&#20026;&#38590;&#20197;&#39044;&#27979;&#24182;&#20135;&#29983;&#38169;&#35823;&#30340;&#36755;&#20986;&#12290;&#36825;&#20984;&#26174;&#20102;&#38656;&#35201;&#24320;&#21457;&#33021;&#22815;&#25552;&#20986;&#28548;&#28165;&#38382;&#39064;&#20197;&#26377;&#25928;&#35299;&#20915;&#27495;&#20041;&#30340;&#26234;&#33021;&#20195;&#29702;&#30340;&#38656;&#27714;&#12290;&#36825;&#31181;&#33021;&#21147;&#38656;&#35201;&#23545;&#22810;&#20010;&#23545;&#35805;&#36718;&#27425;&#36827;&#34892;&#22797;&#26434;&#30340;&#29702;&#35299;&#12289;&#29366;&#24577;&#36319;&#36394;&#12289;&#25512;&#29702;&#21644;&#35268;&#21010;&#12290;&#28982;&#32780;&#65292;&#30452;&#25509;&#27979;&#37327;&#36825;&#31181;&#33021;&#21147;&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#26367;&#20195;&#24615;&#38382;&#39064;&#65292;&#36890;&#36807;&#21521;&#27861;&#23448;&#25552;&#20986;&#19968;&#31995;&#21015;&#26597;&#35810;&#65292;&#35780;&#20272;&#20102;LLMs&#25512;&#26029;&#33258;&#24049;&#19981;&#30693;&#36947;&#20294;&#34987;&#27861;&#23448;&#25581;&#31034;&#30340;&#23454;&#20307;&#30340;&#33021;&#21147;&#12290;&#36825;&#20010;&#8220;&#23454;&#20307;&#25512;&#26029;&#28216;&#25103;&#8221;&#21487;&#20197;&#20316;&#20026;&#19968;&#20010;&#35780;&#20272;&#26694;&#26550;&#65292;&#29992;&#20110;&#25506;&#31350;&#35821;&#35328;&#27169;&#22411;&#30340;&#23545;&#35805;&#25512;&#29702;&#21644;&#35268;&#21010;&#33021;&#21147;&#12290;&#25105;&#20204;&#31995;&#32479;&#22320;&#35780;&#20272;&#20102;&#21508;&#31181;LLMs&#65292;&#24182;&#21457;&#29616;&#22312;&#36825;&#20010;&#20219;&#21153;&#19978;&#23427;&#20204;&#30340;&#24615;&#33021;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#12290;&#25105;&#20204;&#21457;&#29616;&#24378;&#22823;&#30340;LLMs...
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) are currently effective at answering questions that are clearly asked. However, when faced with ambiguous queries they can act unpredictably and produce incorrect outputs. This underscores the need for the development of intelligent agents capable of asking clarification questions to resolve ambiguities effectively. This capability requires complex understanding, state tracking, reasoning and planning over multiple conversational turns. However, directly measuring this can be challenging. In this paper, we offer a surrogate problem which assesses an LLMs's capability to deduce an entity unknown to itself, but revealed to a judge, by asking the judge a series of queries. This \textit{entity-deducing game} can serve as an evaluation framework to probe the conversational reasoning and planning capabilities of language models. We systematically evaluate various LLMs and discover significant differences in their performance on this task. We find that strong LLMs
&lt;/p&gt;</description></item><item><title>ToRA&#26159;&#19968;&#31181;&#38598;&#25104;&#24037;&#20855;&#30340;&#25968;&#23398;&#38382;&#39064;&#27714;&#35299;&#25512;&#29702;&#20195;&#29702;&#65292;&#36890;&#36807;&#32467;&#21512;&#35821;&#35328;&#30340;&#20998;&#26512;&#33021;&#21147;&#21644;&#24037;&#20855;&#30340;&#35745;&#31639;&#25928;&#29575;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#25968;&#23398;&#25512;&#29702;&#30340;&#24615;&#33021;&#65292;&#22312;&#22810;&#20010;&#25968;&#23398;&#25512;&#29702;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;13%-19%&#30340;&#24179;&#22343;&#32477;&#23545;&#25913;&#36827;&#29575;&#65292;&#24182;&#22312;&#31454;&#36187;&#32423;&#25968;&#25454;&#38598;MATH&#19978;&#36798;&#21040;&#20102;44.6%&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.17452</link><description>&lt;p&gt;
ToRA&#65306;&#19968;&#31181;&#38598;&#25104;&#24037;&#20855;&#30340;&#25968;&#23398;&#38382;&#39064;&#27714;&#35299;&#25512;&#29702;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
ToRA: A Tool-Integrated Reasoning Agent for Mathematical Problem Solving. (arXiv:2309.17452v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17452
&lt;/p&gt;
&lt;p&gt;
ToRA&#26159;&#19968;&#31181;&#38598;&#25104;&#24037;&#20855;&#30340;&#25968;&#23398;&#38382;&#39064;&#27714;&#35299;&#25512;&#29702;&#20195;&#29702;&#65292;&#36890;&#36807;&#32467;&#21512;&#35821;&#35328;&#30340;&#20998;&#26512;&#33021;&#21147;&#21644;&#24037;&#20855;&#30340;&#35745;&#31639;&#25928;&#29575;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#25968;&#23398;&#25512;&#29702;&#30340;&#24615;&#33021;&#65292;&#22312;&#22810;&#20010;&#25968;&#23398;&#25512;&#29702;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;13%-19%&#30340;&#24179;&#22343;&#32477;&#23545;&#25913;&#36827;&#29575;&#65292;&#24182;&#22312;&#31454;&#36187;&#32423;&#25968;&#25454;&#38598;MATH&#19978;&#36798;&#21040;&#20102;44.6%&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#35821;&#35328;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;&#22312;&#22797;&#26434;&#30340;&#25968;&#23398;&#38382;&#39064;&#19978;&#20173;&#28982;&#23384;&#22312;&#22256;&#38590;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#38598;&#25104;&#24037;&#20855;&#30340;&#25512;&#29702;&#20195;&#29702;ToRA&#65292;&#23427;&#36890;&#36807;&#26080;&#32541;&#22320;&#23558;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#19982;&#22806;&#37096;&#24037;&#20855;&#65288;&#20363;&#22914;&#35745;&#31639;&#24211;&#21644;&#31526;&#21495;&#27714;&#35299;&#22120;&#65289;&#30340;&#21033;&#29992;&#30456;&#32467;&#21512;&#65292;&#20174;&#32780;&#23558;&#35821;&#35328;&#30340;&#20998;&#26512;&#33021;&#21147;&#19982;&#24037;&#20855;&#30340;&#35745;&#31639;&#25928;&#29575;&#34701;&#21512;&#22312;&#19968;&#36215;&#65292;&#29992;&#20110;&#35299;&#20915;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25968;&#23398;&#38382;&#39064;&#12290;&#20026;&#20102;&#35757;&#32451;ToRA&#65292;&#25105;&#20204;&#31934;&#36873;&#20102;&#25968;&#23398;&#25968;&#25454;&#38598;&#19978;&#30340;&#20114;&#21160;&#24037;&#20855;&#20351;&#29992;&#36712;&#36857;&#65292;&#24212;&#29992;&#27169;&#20223;&#23398;&#20064;&#20110;&#27880;&#37322;&#65292;&#24182;&#25552;&#20986;&#36755;&#20986;&#31354;&#38388;&#25972;&#24418;&#26469;&#36827;&#19968;&#27493;&#25913;&#36827;&#27169;&#22411;&#30340;&#25512;&#29702;&#34892;&#20026;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;ToRA&#27169;&#22411;&#22312;10&#20010;&#28085;&#30422;&#21508;&#31181;&#35268;&#27169;&#30340;&#25968;&#23398;&#25512;&#29702;&#25968;&#25454;&#38598;&#19978;&#26174;&#33879;&#20248;&#20110;&#24320;&#28304;&#27169;&#22411;&#65292;&#24179;&#22343;&#32477;&#23545;&#25913;&#36827;&#29575;&#36798;&#21040;13%&#33267;19%&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;ToRA-7B &#22312;&#31454;&#36187;&#32423;&#25968;&#25454;&#38598;MATH&#19978;&#36798;&#21040;&#20102;44.6%&#65292;&#36229;&#36234;&#20102;&#26368;&#20339;&#24320;&#28304;&#27169;&#22411;WizardMath&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models have made significant progress in various language tasks, yet they still struggle with complex mathematics. In this paper, we propose ToRA a series of Tool-integrated Reasoning Agents designed to solve challenging mathematical problems by seamlessly integrating natural language reasoning with the utilization of external tools (e.g., computation libraries and symbolic solvers), thereby amalgamating the analytical prowess of language and the computational efficiency of tools. To train ToRA, we curate interactive tool-use trajectories on mathematical datasets, apply imitation learning on the annotations, and propose output space shaping to further refine models' reasoning behavior. As a result, ToRA models significantly outperform open-source models on 10 mathematical reasoning datasets across all scales with 13%-19% absolute improvements on average. Notably, ToRA-7B reaches 44.6% on the competition-level dataset MATH, surpassing the best open-source model WizardMath
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#22810;&#35282;&#24230;&#33258;&#19968;&#33268;&#24615;&#65288;MPSC&#65289;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#25552;&#21319;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#22797;&#26434;&#30340;&#20195;&#30721;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#20174;&#22810;&#20010;&#35282;&#24230;&#37319;&#26679;&#22810;&#20010;&#36755;&#20986;&#24182;&#26500;&#24314;&#19968;&#20010;&#22810;&#37096;&#20998;&#22270;&#65292;&#21033;&#29992;&#20132;&#21449;&#19968;&#33268;&#24615;&#21644;&#20869;&#19968;&#33268;&#24615;&#20449;&#24687;&#26469;&#36873;&#25321;&#26368;&#20248;&#36755;&#20986;&#12290;</title><link>http://arxiv.org/abs/2309.17272</link><description>&lt;p&gt;
&#25552;&#21319;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#32534;&#30721;&#20013;&#30340;&#33021;&#21147;&#36890;&#36807;&#22810;&#35282;&#24230;&#33258;&#19968;&#33268;&#24615;
&lt;/p&gt;
&lt;p&gt;
Enhancing Large Language Models in Coding Through Multi-Perspective Self-Consistency. (arXiv:2309.17272v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17272
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#22810;&#35282;&#24230;&#33258;&#19968;&#33268;&#24615;&#65288;MPSC&#65289;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#25552;&#21319;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#22797;&#26434;&#30340;&#20195;&#30721;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#20174;&#22810;&#20010;&#35282;&#24230;&#37319;&#26679;&#22810;&#20010;&#36755;&#20986;&#24182;&#26500;&#24314;&#19968;&#20010;&#22810;&#37096;&#20998;&#22270;&#65292;&#21033;&#29992;&#20132;&#21449;&#19968;&#33268;&#24615;&#21644;&#20869;&#19968;&#33268;&#24615;&#20449;&#24687;&#26469;&#36873;&#25321;&#26368;&#20248;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#25991;&#26412;&#29983;&#25104;&#26041;&#38754;&#23637;&#29616;&#20102;&#21331;&#36234;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#22312;&#22797;&#26434;&#30340;&#25512;&#29702;&#20219;&#21153;&#65292;&#22914;&#20195;&#30721;&#29983;&#25104;&#20013;&#65292;LLMs&#20173;&#28982;&#38590;&#20197;&#22312;&#19968;&#27425;&#23581;&#35797;&#20013;&#29983;&#25104;&#27491;&#30830;&#30340;&#31572;&#26696;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#36890;&#36807;&#32858;&#21512;&#22810;&#20010;&#36755;&#20986;&#65292;&#21033;&#29992;&#23427;&#20204;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#26469;&#25506;&#32034;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#30740;&#31350;&#27809;&#26377;&#20840;&#38754;&#22320;&#20174;&#19981;&#21516;&#30340;&#35282;&#24230;&#25429;&#25417;&#36825;&#31181;&#19968;&#33268;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22810;&#35282;&#24230;&#33258;&#19968;&#33268;&#24615;&#65288;MPSC&#65289;&#26694;&#26550;&#30340;&#26032;&#30340;&#35299;&#30721;&#31574;&#30053;&#65292;&#29992;&#20110;LLM&#65292;&#23427;&#23558;&#26469;&#33258;&#22810;&#20010;&#35282;&#24230;&#30340;&#36755;&#20986;&#20043;&#38388;&#30340;&#20132;&#21449;&#19968;&#33268;&#24615;&#21644;&#21333;&#20010;&#35282;&#24230;&#20869;&#30340;&#20869;&#19968;&#33268;&#24615;&#32467;&#21512;&#36215;&#26469;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35201;&#27714;LLMs&#23545;&#32473;&#23450;&#26597;&#35810;&#20174;&#21508;&#20010;&#35282;&#24230;&#37319;&#26679;&#22810;&#20010;&#22810;&#26679;&#21270;&#30340;&#36755;&#20986;&#65292;&#24182;&#22522;&#20110;&#23427;&#20204;&#26500;&#24314;&#19968;&#20010;&#22810;&#37096;&#20998;&#22270;&#12290;&#36890;&#36807;&#20004;&#20010;&#39044;&#23450;&#20041;&#30340;&#19968;&#33268;&#24615;&#24230;&#37327;&#65292;&#25105;&#20204;&#23558;&#20132;&#21449;&#19968;&#33268;&#24615;&#21644;&#20869;&#19968;&#33268;&#24615;&#20449;&#24687;&#23884;&#20837;&#21040;&#22270;&#20013;&#12290;&#26368;&#20339;&#36873;&#25321;&#26159;&#26681;&#25454;&#36825;&#20123;&#19968;&#33268;&#24615;&#24230;&#37327;&#26469;&#36873;&#25321;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have exhibited remarkable ability in textual generation. However, in complex reasoning tasks such as code generation, generating the correct answer in a single attempt remains a formidable challenge for LLMs. Previous research has explored solutions by aggregating multiple outputs, leveraging the consistency among them. However, none of them have comprehensively captured this consistency from different perspectives. In this paper, we propose the Multi-Perspective Self-Consistency (MPSC) framework, a novel decoding strategy for LLM that incorporates both inter-consistency across outputs from multiple perspectives and intra-consistency within a single perspective. Specifically, we ask LLMs to sample multiple diverse outputs from various perspectives for a given query and then construct a multipartite graph based on them. With two predefined measures of consistency, we embed both inter- and intra-consistency information into the graph. The optimal choice is th
&lt;/p&gt;</description></item><item><title>TranDRL&#26159;&#19968;&#31181;&#22522;&#20110;Transformer&#39537;&#21160;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#25903;&#25345;&#30340;&#39044;&#38450;&#24615;&#32500;&#25252;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#22797;&#26434;&#26102;&#38388;&#27169;&#24335;&#25429;&#25417;&#21644;&#32463;&#27982;&#39640;&#25928;&#32500;&#25252;&#24314;&#35758;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#21097;&#20313;&#23551;&#21629;&#65288;RUL&#65289;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#32500;&#25252;&#34892;&#21160;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2309.16935</link><description>&lt;p&gt;
TranDRL&#65306;&#19968;&#31181;&#22522;&#20110;Transformer&#39537;&#21160;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#25903;&#25345;&#30340;&#39044;&#38450;&#24615;&#32500;&#25252;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
TranDRL: A Transformer-Driven Deep Reinforcement Learning Enabled Prescriptive Maintenance Framework. (arXiv:2309.16935v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16935
&lt;/p&gt;
&lt;p&gt;
TranDRL&#26159;&#19968;&#31181;&#22522;&#20110;Transformer&#39537;&#21160;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#25903;&#25345;&#30340;&#39044;&#38450;&#24615;&#32500;&#25252;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#22797;&#26434;&#26102;&#38388;&#27169;&#24335;&#25429;&#25417;&#21644;&#32463;&#27982;&#39640;&#25928;&#32500;&#25252;&#24314;&#35758;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#21097;&#20313;&#23551;&#21629;&#65288;RUL&#65289;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#32500;&#25252;&#34892;&#21160;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24037;&#19994;&#31995;&#32479;&#38656;&#35201;&#21487;&#38752;&#30340;&#39044;&#27979;&#24615;&#32500;&#25252;&#31574;&#30053;&#26469;&#25552;&#39640;&#36816;&#33829;&#25928;&#29575;&#24182;&#20943;&#23569;&#20572;&#26426;&#26102;&#38388;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32508;&#21512;&#26694;&#26550;&#65292;&#21033;&#29992;Transformer&#31070;&#32463;&#32593;&#32476;&#21644;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#31639;&#27861;&#26469;&#20248;&#21270;&#32500;&#25252;&#34892;&#21160;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#37319;&#29992;Transformer&#27169;&#22411;&#26469;&#26377;&#25928;&#25429;&#25417;&#20256;&#24863;&#22120;&#25968;&#25454;&#20013;&#30340;&#22797;&#26434;&#26102;&#38388;&#27169;&#24335;&#65292;&#20174;&#32780;&#20934;&#30830;&#39044;&#27979;&#35774;&#22791;&#30340;&#21097;&#20313;&#23551;&#21629;&#65288;RUL&#65289;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#26694;&#26550;&#20013;&#30340;DRL&#32452;&#20214;&#25552;&#20379;&#20102;&#32463;&#27982;&#39640;&#25928;&#21644;&#21450;&#26102;&#30340;&#32500;&#25252;&#24314;&#35758;&#12290;&#25105;&#20204;&#22312;NASA C-MPASS&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20102;&#25105;&#20204;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#65292;&#32467;&#26524;&#26174;&#31034;&#22312;RUL&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#32500;&#25252;&#34892;&#21160;&#20248;&#21270;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#21019;&#26032;&#26041;&#27861;&#20026;&#39044;&#38450;&#24615;&#32500;&#25252;&#25552;&#20379;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#24037;&#19994;&#36816;&#33829;&#20013;&#30340;&#20851;&#38190;&#25361;&#25112;&#65292;&#24182;&#24102;&#26469;&#20102;&#26356;&#22810;&#21457;&#23637;&#26426;&#36935;&#12290;
&lt;/p&gt;
&lt;p&gt;
Industrial systems demand reliable predictive maintenance strategies to enhance operational efficiency and reduce downtime. This paper introduces a novel, integrated framework that leverages the power of transformer neural networks and deep reinforcement learning (DRL) algorithms to optimize maintenance actions. Our approach employs the transformer model to effectively capture complex temporal patterns in sensor data, thereby accurately predicting the Remaining Useful Life (RUL) of equipment. Simultaneously, the DRL component of our framework provides cost-effective and timely maintenance recommendations. We validate the efficacy of our framework on the NASA C-MPASS dataset, where it demonstrates significant advancements in both RUL prediction accuracy and the optimization of maintenance actions. Consequently, our pioneering approach provides an innovative data-driven methodology for prescriptive maintenance, addressing key challenges in industrial operations and leading the way to mor
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#20154;&#24037;&#29983;&#25104;&#30340;&#28436;&#31034;&#26159;&#21542;&#26377;&#24517;&#35201;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#21453;&#24605;&#25552;&#31034;&#31574;&#30053;&#65288;SEC&#65289;&#65292;&#36890;&#36807;&#36825;&#31181;&#31574;&#30053;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21487;&#20197;&#33258;&#34892;&#29983;&#25104;&#28436;&#31034;&#21644;&#26368;&#32456;&#36755;&#20986;&#65292;&#36991;&#20813;&#20102;&#25163;&#21160;&#29983;&#25104;&#36807;&#31243;&#30340;&#22797;&#26434;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.14681</link><description>&lt;p&gt;
&#20154;&#24037;&#29983;&#25104;&#30340;&#28436;&#31034;&#26159;&#21542;&#23545;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#26377;&#24517;&#35201;&#65311;
&lt;/p&gt;
&lt;p&gt;
Are Human-generated Demonstrations Necessary for In-context Learning?. (arXiv:2309.14681v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14681
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#20154;&#24037;&#29983;&#25104;&#30340;&#28436;&#31034;&#26159;&#21542;&#26377;&#24517;&#35201;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#21453;&#24605;&#25552;&#31034;&#31574;&#30053;&#65288;SEC&#65289;&#65292;&#36890;&#36807;&#36825;&#31181;&#31574;&#30053;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21487;&#20197;&#33258;&#34892;&#29983;&#25104;&#28436;&#31034;&#21644;&#26368;&#32456;&#36755;&#20986;&#65292;&#36991;&#20813;&#20102;&#25163;&#21160;&#29983;&#25104;&#36807;&#31243;&#30340;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20855;&#22791;&#33391;&#22909;&#30340;&#23569;&#26679;&#26412;&#33021;&#21147;&#65292;&#20294;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#30340;&#26631;&#20934;&#33539;&#24335;&#20013;&#23384;&#22312;&#20197;&#19979;&#24330;&#31471;&#65306;&#26131;&#21463;&#36873;&#23450;&#28436;&#31034;&#30340;&#24433;&#21709;&#65292;&#29983;&#25104;&#36825;&#20123;&#28436;&#31034;&#30340;&#22797;&#26434;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#23545;&#20110;ICL&#65292;&#20154;&#24037;&#29983;&#25104;&#30340;&#28436;&#31034;&#26159;&#21542;&#26377;&#24517;&#35201;&#30340;&#22522;&#26412;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#33258;&#21453;&#24605;&#25552;&#31034;&#31574;&#30053;&#65288;SEC&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#19981;&#20381;&#36182;&#20154;&#24037;&#28436;&#31034;&#30340;&#33539;&#20363;&#12290;SEC&#30340;&#20851;&#38190;&#28857;&#22312;&#20110;&#65292;&#19981;&#20351;&#29992;&#25163;&#24037;&#21046;&#20316;&#30340;&#31034;&#20363;&#20316;&#20026;ICL&#20013;&#30340;&#28436;&#31034;&#65292;&#32780;&#26159;&#35201;&#27714;LLMs&#39318;&#20808;&#33258;&#34892;&#21019;&#24314;&#28436;&#31034;&#65292;&#28982;&#21518;&#29983;&#25104;&#26368;&#32456;&#36755;&#20986;&#12290;SEC&#26159;&#19968;&#31181;&#28789;&#27963;&#30340;&#26694;&#26550;&#65292;&#21487;&#36866;&#24212;&#21407;&#22987;ICL&#21644;&#8220;&#24605;&#32500;&#38142;&#8221;&#65288;CoT&#65289;&#65292;&#24182;&#19988;&#26356;&#21152;&#20415;&#25463;&#65306;&#22240;&#20026;&#21487;&#20197;&#33410;&#30465;&#31034;&#20363;&#21644;&#29702;&#30001;&#30340;&#25163;&#21160;&#29983;&#25104;&#36807;&#31243;&#12290;&#22312;&#31639;&#26415;&#25512;&#29702;&#12289;&#24120;&#35782;&#25512;&#29702;&#21644;&#22810;&#20219;&#21153;&#35821;&#35328;&#29702;&#35299;&#26041;&#38754;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the promising few-shot ability of large language models (LLMs), the standard paradigm of In-context Learning (ICL) suffers the disadvantages of susceptibility to selected demonstrations and the intricacy to generate these demonstrations. In this paper, we raise the fundamental question that whether human-generated demonstrations are necessary for ICL. To answer this question, we propose self-contemplation prompting strategy (SEC), a paradigm free from human-crafted demonstrations. The key point of SEC is that, instead of using hand-crafted examples as demonstrations in ICL, SEC asks LLMs to first create demonstrations on their own, based on which the final output is generated. SEC is a flexible framework and can be adapted to both the vanilla ICL and the chain-of-thought (CoT), but with greater ease: as the manual-generation process of both examples and rationale can be saved. Extensive experiments in arithmetic reasoning, commonsense reasoning, multi-task language understandin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24809;&#32602;&#30340;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;P-CSG&#65292;&#32467;&#21512;&#35821;&#20041;&#29983;&#25104;&#20256;&#24863;&#22120;&#34701;&#21512;&#25216;&#26415;&#65292;&#20197;&#25552;&#39640;&#31471;&#21040;&#31471;&#33258;&#21160;&#39550;&#39542;&#30340;&#25972;&#20307;&#24615;&#33021;&#65292;&#24182;&#35299;&#20915;&#20102;&#20132;&#36890;&#35268;&#21017;&#36981;&#23432;&#21644;&#20256;&#24863;&#22120;&#24863;&#30693;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.07808</link><description>&lt;p&gt;
&#25552;&#21319;&#27169;&#20223;&#23398;&#20064;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#30340;&#20132;&#36890;&#35268;&#21017;&#36981;&#23432;&#30340;&#20851;&#38190;&#22240;&#32032;
&lt;/p&gt;
&lt;p&gt;
What Matters to Enhance Traffic Rule Compliance of Imitation Learning for Automated Driving. (arXiv:2309.07808v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07808
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24809;&#32602;&#30340;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;P-CSG&#65292;&#32467;&#21512;&#35821;&#20041;&#29983;&#25104;&#20256;&#24863;&#22120;&#34701;&#21512;&#25216;&#26415;&#65292;&#20197;&#25552;&#39640;&#31471;&#21040;&#31471;&#33258;&#21160;&#39550;&#39542;&#30340;&#25972;&#20307;&#24615;&#33021;&#65292;&#24182;&#35299;&#20915;&#20102;&#20132;&#36890;&#35268;&#21017;&#36981;&#23432;&#21644;&#20256;&#24863;&#22120;&#24863;&#30693;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#36234;&#26469;&#36234;&#22810;&#30340;&#30740;&#31350;&#20851;&#27880;&#20110;&#20840;&#31471;&#21040;&#31471;&#30340;&#33258;&#21160;&#39550;&#39542;&#25216;&#26415;&#65292;&#22312;&#36825;&#31181;&#25216;&#26415;&#20013;&#65292;&#25972;&#20010;&#39550;&#39542;&#27969;&#31243;&#34987;&#26367;&#25442;&#20026;&#19968;&#20010;&#31616;&#21333;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#30001;&#20110;&#20854;&#32467;&#26500;&#31616;&#21333;&#21644;&#25512;&#29702;&#26102;&#38388;&#24555;&#65292;&#22240;&#27492;&#21464;&#24471;&#38750;&#24120;&#21560;&#24341;&#20154;&#12290;&#23613;&#31649;&#36825;&#31181;&#26041;&#27861;&#22823;&#22823;&#20943;&#23569;&#20102;&#39550;&#39542;&#27969;&#31243;&#20013;&#30340;&#32452;&#20214;&#65292;&#20294;&#20854;&#31616;&#21333;&#24615;&#20063;&#23548;&#33268;&#35299;&#37322;&#24615;&#38382;&#39064;&#21644;&#23433;&#20840;&#38382;&#39064;&#12290;&#35757;&#32451;&#24471;&#21040;&#30340;&#31574;&#30053;&#24182;&#19981;&#24635;&#26159;&#31526;&#21512;&#20132;&#36890;&#35268;&#21017;&#65292;&#21516;&#26102;&#20063;&#24456;&#38590;&#21457;&#29616;&#20854;&#38169;&#35823;&#30340;&#21407;&#22240;&#65292;&#22240;&#20026;&#32570;&#20047;&#20013;&#38388;&#36755;&#20986;&#12290;&#21516;&#26102;&#65292;&#20256;&#24863;&#22120;&#23545;&#20110;&#33258;&#21160;&#39550;&#39542;&#30340;&#23433;&#20840;&#24615;&#21644;&#21487;&#34892;&#24615;&#20063;&#33267;&#20851;&#37325;&#35201;&#65292;&#21487;&#20197;&#24110;&#21161;&#24863;&#30693;&#22797;&#26434;&#39550;&#39542;&#22330;&#26223;&#19979;&#30340;&#21608;&#22260;&#29615;&#22659;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#26032;&#30340;&#22522;&#20110;&#24809;&#32602;&#30340;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;P-CSG&#65292;&#32467;&#21512;&#35821;&#20041;&#29983;&#25104;&#20256;&#24863;&#22120;&#34701;&#21512;&#25216;&#26415;&#65292;&#20197;&#25552;&#39640;&#31471;&#21040;&#31471;&#33258;&#21160;&#39550;&#39542;&#30340;&#25972;&#20307;&#24615;&#33021;&#12290;&#25105;&#20204;&#23545;&#27169;&#22411;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
More research attention has recently been given to end-to-end autonomous driving technologies where the entire driving pipeline is replaced with a single neural network because of its simpler structure and faster inference time. Despite this appealing approach largely reducing the components in driving pipeline, its simplicity also leads to interpretability problems and safety issues arXiv:2003.06404. The trained policy is not always compliant with the traffic rules and it is also hard to discover the reason for the misbehavior because of the lack of intermediate outputs. Meanwhile, Sensors are also critical to autonomous driving's security and feasibility to perceive the surrounding environment under complex driving scenarios. In this paper, we proposed P-CSG, a novel penalty-based imitation learning approach with cross semantics generation sensor fusion technologies to increase the overall performance of End-to-End Autonomous Driving. We conducted an assessment of our model's perform
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#24179;&#28369;&#36807;&#24230;&#21644;&#29305;&#24449;&#20851;&#32852;&#36807;&#39640;&#29616;&#35937;&#65292;&#21457;&#29616;&#22266;&#23450;&#19981;&#21464;&#30340;&#23376;&#31354;&#38388;&#23548;&#33268;&#20102;&#33410;&#28857;&#34920;&#31034;&#30340;&#31561;&#32423;&#23849;&#22604;&#12290;&#22312;&#35813;&#23376;&#31354;&#38388;&#20013;&#24179;&#28369;&#21521;&#37327;&#30340;&#23384;&#22312;&#23548;&#33268;&#36807;&#24230;&#24179;&#28369;&#65292;&#21363;&#20351;&#36991;&#20813;&#36807;&#24230;&#24179;&#28369;&#20063;&#20250;&#23548;&#33268;&#36807;&#39640;&#30340;&#20851;&#32852;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20811;&#32599;&#20869;&#20811;&#31215;&#20043;&#21644;&#20316;&#20026;&#19968;&#31181;&#26377;&#25928;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.16800</link><description>&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#31561;&#32423;&#23849;&#22604;&#23548;&#33268;&#24179;&#28369;&#36807;&#24230;&#21644;&#20851;&#32852;&#36807;&#39640;
&lt;/p&gt;
&lt;p&gt;
Rank Collapse Causes Over-Smoothing and Over-Correlation in Graph Neural Networks. (arXiv:2308.16800v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16800
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#24179;&#28369;&#36807;&#24230;&#21644;&#29305;&#24449;&#20851;&#32852;&#36807;&#39640;&#29616;&#35937;&#65292;&#21457;&#29616;&#22266;&#23450;&#19981;&#21464;&#30340;&#23376;&#31354;&#38388;&#23548;&#33268;&#20102;&#33410;&#28857;&#34920;&#31034;&#30340;&#31561;&#32423;&#23849;&#22604;&#12290;&#22312;&#35813;&#23376;&#31354;&#38388;&#20013;&#24179;&#28369;&#21521;&#37327;&#30340;&#23384;&#22312;&#23548;&#33268;&#36807;&#24230;&#24179;&#28369;&#65292;&#21363;&#20351;&#36991;&#20813;&#36807;&#24230;&#24179;&#28369;&#20063;&#20250;&#23548;&#33268;&#36807;&#39640;&#30340;&#20851;&#32852;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20811;&#32599;&#20869;&#20811;&#31215;&#20043;&#21644;&#20316;&#20026;&#19968;&#31181;&#26377;&#25928;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#28145;&#24230;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#24179;&#28369;&#36807;&#24230;&#21644;&#29305;&#24449;&#20851;&#32852;&#36807;&#39640;&#30340;&#26032;&#29702;&#35770;&#35265;&#35299;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22266;&#23450;&#19981;&#21464;&#23376;&#31354;&#38388;&#30340;&#26222;&#36941;&#23384;&#22312;&#65292;&#23427;&#34920;&#29616;&#20986;&#19968;&#31181;&#30456;&#23545;&#30340;&#34892;&#20026;&#65292;&#19981;&#21463;&#29305;&#24449;&#36716;&#25442;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#38416;&#26126;&#20102;&#19982;&#25910;&#25947;&#21040;&#24120;&#25968;&#29366;&#24577;&#21644;&#33410;&#28857;&#29366;&#24577;&#30340;&#36807;&#20998;&#20998;&#31163;&#30456;&#20851;&#30340;&#26368;&#26032;&#35266;&#23519;&#32467;&#26524;&#65292;&#22240;&#20026;&#23376;&#31354;&#38388;&#30340;&#25918;&#22823;&#21482;&#21462;&#20915;&#20110;&#32858;&#21512;&#20989;&#25968;&#30340;&#39057;&#35889;&#12290;&#22312;&#32447;&#24615;&#22330;&#26223;&#20013;&#65292;&#36825;&#23548;&#33268;&#33410;&#28857;&#34920;&#31034;&#30001;&#20302;&#32500;&#23376;&#31354;&#38388;&#20027;&#23548;&#65292;&#24182;&#19988;&#20855;&#26377;&#19982;&#29305;&#24449;&#36716;&#25442;&#26080;&#20851;&#30340;&#28176;&#36817;&#25910;&#25947;&#36895;&#29575;&#12290;&#24403;&#24179;&#28369;&#21521;&#37327;&#36328;&#36234;&#36825;&#20010;&#23376;&#31354;&#38388;&#26102;&#65292;&#36825;&#20250;&#23548;&#33268;&#33410;&#28857;&#34920;&#31034;&#30340;&#31561;&#32423;&#23849;&#22604;&#65292;&#20174;&#32780;&#23548;&#33268;&#36807;&#24230;&#24179;&#28369;&#65292;&#21363;&#20351;&#36991;&#20813;&#36807;&#24230;&#24179;&#28369;&#20063;&#20250;&#23548;&#33268;&#36807;&#39640;&#30340;&#20851;&#32852;&#12290;&#22312;&#25105;&#20204;&#30340;&#29702;&#35770;&#25351;&#23548;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20811;&#32599;&#20869;&#20811;&#31215;&#20043;&#21644;&#20316;&#20026;&#19968;&#31181;&#26377;&#30410;&#29305;&#24615;&#65292;&#21487;&#20197;&#21487;&#38752;&#22320;&#38450;&#27490;&#36807;&#24230;&#24179;&#28369;&#12289;&#36807;&#39640;&#20851;&#32852;&#21644;&#31561;&#32423;&#23849;&#22604;&#12290;
&lt;/p&gt;
&lt;p&gt;
Our study reveals new theoretical insights into over-smoothing and feature over-correlation in deep graph neural networks. We show the prevalence of invariant subspaces, demonstrating a fixed relative behavior that is unaffected by feature transformations. Our work clarifies recent observations related to convergence to a constant state and a potential over-separation of node states, as the amplification of subspaces only depends on the spectrum of the aggregation function. In linear scenarios, this leads to node representations being dominated by a low-dimensional subspace with an asymptotic convergence rate independent of the feature transformations. This causes a rank collapse of the node representations, resulting in over-smoothing when smooth vectors span this subspace, and over-correlation even when over-smoothing is avoided. Guided by our theory, we propose a sum of Kronecker products as a beneficial property that can provably prevent over-smoothing, over-correlation, and rank c
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#23454;&#29616;&#21327;&#20316;&#20449;&#24687;&#20256;&#25773;&#12290;&#36890;&#36807;&#25552;&#20986;&#20998;&#24067;&#24335;POMDP&#24418;&#24335;&#65292;&#22312;&#28040;&#24687;&#36716;&#21457;&#19978;&#23454;&#29616;&#20102;&#27599;&#20010;&#26234;&#33021;&#20307;&#30340;&#29420;&#31435;&#20915;&#31574;&#65292;&#30456;&#27604;&#20256;&#32479;&#30340;&#22522;&#20110;&#22810;&#28857;&#20013;&#32487;&#36873;&#25321;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#20855;&#26377;&#37325;&#22823;&#21019;&#26032;&#21644;&#36129;&#29486;&#12290;&#21516;&#26102;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#22270;&#21367;&#31215;&#24378;&#21270;&#23398;&#20064;&#21644;&#21160;&#24577;&#27880;&#24847;&#21147;&#26426;&#21046;&#25429;&#25417;&#20851;&#38190;&#32593;&#32476;&#29305;&#24449;&#65292;&#24182;&#25552;&#20986;&#20102;&#19981;&#21516;&#20449;&#24687;&#20132;&#25442;&#26041;&#24335;&#30340;&#20004;&#31181;&#26041;&#27861;&#36827;&#34892;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2308.16198</link><description>&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;&#22270;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#23398;&#20064;&#21327;&#20316;&#20449;&#24687;&#20256;&#25773;
&lt;/p&gt;
&lt;p&gt;
Learning Collaborative Information Dissemination with Graph-based Multi-Agent Reinforcement Learning. (arXiv:2308.16198v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16198
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#23454;&#29616;&#21327;&#20316;&#20449;&#24687;&#20256;&#25773;&#12290;&#36890;&#36807;&#25552;&#20986;&#20998;&#24067;&#24335;POMDP&#24418;&#24335;&#65292;&#22312;&#28040;&#24687;&#36716;&#21457;&#19978;&#23454;&#29616;&#20102;&#27599;&#20010;&#26234;&#33021;&#20307;&#30340;&#29420;&#31435;&#20915;&#31574;&#65292;&#30456;&#27604;&#20256;&#32479;&#30340;&#22522;&#20110;&#22810;&#28857;&#20013;&#32487;&#36873;&#25321;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#20855;&#26377;&#37325;&#22823;&#21019;&#26032;&#21644;&#36129;&#29486;&#12290;&#21516;&#26102;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#22270;&#21367;&#31215;&#24378;&#21270;&#23398;&#20064;&#21644;&#21160;&#24577;&#27880;&#24847;&#21147;&#26426;&#21046;&#25429;&#25417;&#20851;&#38190;&#32593;&#32476;&#29305;&#24449;&#65292;&#24182;&#25552;&#20986;&#20102;&#19981;&#21516;&#20449;&#24687;&#20132;&#25442;&#26041;&#24335;&#30340;&#20004;&#31181;&#26041;&#27861;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#20195;&#36890;&#20449;&#31995;&#32479;&#20013;&#65292;&#39640;&#25928;&#21487;&#38752;&#30340;&#20449;&#24687;&#20256;&#25773;&#23545;&#25903;&#25345;&#20851;&#38190;&#25805;&#20316;&#33267;&#20851;&#37325;&#35201;&#65292;&#22914;&#28798;&#38590;&#21709;&#24212;&#12289;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#21644;&#20256;&#24863;&#22120;&#32593;&#32476;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#26041;&#27861;&#65292;&#20316;&#20026;&#23454;&#29616;&#26356;&#20026;&#20998;&#25955;&#12289;&#39640;&#25928;&#21644;&#21327;&#20316;&#35299;&#20915;&#26041;&#26696;&#30340;&#37325;&#35201;&#36827;&#23637;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20449;&#24687;&#20256;&#25773;&#30340;&#20998;&#24067;&#24335;POMDP&#65288;Decentralized-POMDP&#65289;&#24418;&#24335;&#65292;&#20351;&#24471;&#27599;&#20010;&#26234;&#33021;&#20307;&#21487;&#20197;&#29420;&#31435;&#20915;&#23450;&#28040;&#24687;&#30340;&#36716;&#21457;&#12290;&#36825;&#26500;&#25104;&#20102;&#19968;&#31181;&#20174;&#20256;&#32479;&#22522;&#20110;&#22810;&#28857;&#20013;&#32487;&#65288;MPR&#65289;&#36873;&#25321;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#30340;&#37325;&#22823;&#33539;&#24335;&#36716;&#31227;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#22270;&#21367;&#31215;&#24378;&#21270;&#23398;&#20064;&#65292;&#37319;&#29992;&#20855;&#26377;&#21160;&#24577;&#27880;&#24847;&#21147;&#30340;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#65288;GAT&#65289;&#26469;&#25429;&#25417;&#20851;&#38190;&#32593;&#32476;&#29305;&#24449;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26041;&#27861;&#65292;L-DGN&#21644;HL-DGN&#65292;&#23427;&#20204;&#22312;&#26234;&#33021;&#20307;&#20043;&#38388;&#20132;&#25442;&#30340;&#20449;&#24687;&#19978;&#26377;&#25152;&#19981;&#21516;&#12290;&#36890;&#36807;&#23558;&#25105;&#20204;&#30340;&#20998;&#25955;&#26041;&#27861;&#19982;&#22522;&#20110;MPR&#30340;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#20854;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In modern communication systems, efficient and reliable information dissemination is crucial for supporting critical operations across domains like disaster response, autonomous vehicles, and sensor networks. This paper introduces a Multi-Agent Reinforcement Learning (MARL) approach as a significant step forward in achieving more decentralized, efficient, and collaborative solutions. We propose a Decentralized-POMDP formulation for information dissemination, empowering each agent to independently decide on message forwarding. This constitutes a significant paradigm shift from traditional heuristics based on Multi-Point Relay (MPR) selection. Our approach harnesses Graph Convolutional Reinforcement Learning, employing Graph Attention Networks (GAT) with dynamic attention to capture essential network features. We propose two approaches, L-DGN and HL-DGN, which differ in the information that is exchanged among agents. We evaluate the performance of our decentralized approaches, by compari
&lt;/p&gt;</description></item><item><title>chatGPT ADA&#26159;&#19968;&#31181;&#33021;&#22815;&#33258;&#20027;&#24320;&#21457;&#20020;&#24202;&#30740;&#31350;&#25152;&#38656;&#30340;&#26368;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#21487;&#23558;&#39640;&#32423;&#20998;&#26512;&#24037;&#20855;&#27665;&#20027;&#21270;&#65292;&#20351;&#38750;&#25968;&#25454;&#31185;&#23398;&#23478;&#30340;&#20020;&#24202;&#21307;&#29983;&#33021;&#22815;&#36731;&#26494;&#24212;&#29992;&#20110;&#21307;&#23398;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2308.14120</link><description>&lt;p&gt;
&#25480;&#26435;&#20020;&#24202;&#21307;&#29983;&#24182;&#27665;&#20027;&#21270;&#25968;&#25454;&#31185;&#23398;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#21270;&#20020;&#24202;&#30740;&#31350;&#30340;&#26426;&#22120;&#23398;&#20064;&#12290; (arXiv:2308.14120v2 [cs.LG] &#26356;&#26032;&#29256;)
&lt;/p&gt;
&lt;p&gt;
Empowering Clinicians and Democratizing Data Science: Large Language Models Automate Machine Learning for Clinical Studies. (arXiv:2308.14120v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14120
&lt;/p&gt;
&lt;p&gt;
chatGPT ADA&#26159;&#19968;&#31181;&#33021;&#22815;&#33258;&#20027;&#24320;&#21457;&#20020;&#24202;&#30740;&#31350;&#25152;&#38656;&#30340;&#26368;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#21487;&#23558;&#39640;&#32423;&#20998;&#26512;&#24037;&#20855;&#27665;&#20027;&#21270;&#65292;&#20351;&#38750;&#25968;&#25454;&#31185;&#23398;&#23478;&#30340;&#20020;&#24202;&#21307;&#29983;&#33021;&#22815;&#36731;&#26494;&#24212;&#29992;&#20110;&#21307;&#23398;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#24320;&#21457;&#32773;&#65288;&#22914;&#25968;&#25454;&#31185;&#23398;&#23478;&#65289;&#21644;&#20174;&#19994;&#32773;&#65288;&#22914;&#20020;&#24202;&#21307;&#29983;&#65289;&#20043;&#38388;&#23384;&#22312;&#30693;&#35782;&#24046;&#36317;&#65292;&#38459;&#30861;&#20102;ML&#22312;&#20020;&#24202;&#25968;&#25454;&#20998;&#26512;&#20013;&#30340;&#20805;&#20998;&#21033;&#29992;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;chatGPT Advanced Data Analysis&#65288;ADA&#65289;&#65292;&#21363;GPT-4&#30340;&#25193;&#23637;&#65292;&#26469;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#24182;&#39640;&#25928;&#25191;&#34892;ML&#20998;&#26512;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#21521;chatGPT ADA&#25552;&#20379;&#20102;&#21508;&#31181;&#21307;&#23398;&#19987;&#19994;&#30340;&#22823;&#22411;&#35797;&#39564;&#30340;&#30495;&#23454;&#20020;&#24202;&#25968;&#25454;&#21644;&#30740;&#31350;&#35814;&#32454;&#20449;&#24687;&#65292;&#27809;&#26377;&#32473;&#20986;&#20855;&#20307;&#25351;&#23548;&#12290;ChatGPT ADA&#22522;&#20110;&#21407;&#22987;&#30740;&#31350;&#30340;&#35757;&#32451;&#25968;&#25454;&#33258;&#20027;&#24320;&#21457;&#20102;&#26368;&#20808;&#36827;&#30340;ML&#27169;&#22411;&#65292;&#29992;&#20110;&#39044;&#27979;&#20020;&#24202;&#32467;&#26524;&#65292;&#22914;&#30284;&#30151;&#21457;&#23637;&#12289;&#30284;&#30151;&#36827;&#23637;&#12289;&#30142;&#30149;&#24182;&#21457;&#30151;&#25110;&#33268;&#30149;&#22522;&#22240;&#24207;&#21015;&#31561;&#29983;&#29289;&#26631;&#24535;&#29289;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#36825;&#20123;ML&#27169;&#22411;&#19982;&#20854;&#24050;&#21457;&#34920;&#30340;&#23545;&#24212;&#29289;&#30456;&#21305;&#37197;&#29978;&#33267;&#34920;&#29616;&#26356;&#22909;&#12290;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65292;chatGPT ADA&#20026;&#27665;&#20027;&#21270;&#21307;&#23398;&#20013;&#30340;ML&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#21069;&#26223;&#30340;&#36884;&#24452;&#65292;&#20351;&#38750;ML&#19987;&#23478;&#33021;&#22815;&#33719;&#24471;&#20808;&#36827;&#30340;&#20998;&#26512;&#24037;&#20855;&#24182;&#25512;&#21160;&#24191;&#27867;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
A knowledge gap persists between Machine Learning (ML) developers (e.g., data scientists) and practitioners (e.g., clinicians), hampering the full utilization of ML for clinical data analysis. We investigated the potential of the chatGPT Advanced Data Analysis (ADA), an extension of GPT-4, to bridge this gap and perform ML analyses efficiently. Real-world clinical datasets and study details from large trials across various medical specialties were presented to chatGPT ADA without specific guidance. ChatGPT ADA autonomously developed state-of-the-art ML models based on the original study's training data to predict clinical outcomes such as cancer development, cancer progression, disease complications, or biomarkers such as pathogenic gene sequences. Strikingly, these ML models matched or outperformed their published counterparts. We conclude that chatGPT ADA offers a promising avenue to democratize ML in medicine, making advanced analytics accessible to non-ML experts and promoting broa
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22870;&#21169;&#26426;&#22120;&#25277;&#35937;&#26469;&#34920;&#31034;&#24403;&#21069;&#20219;&#21153;&#65292;&#24182;&#22312;&#36801;&#31227;&#23398;&#20064;&#20013;&#25552;&#21319;DRL&#20195;&#29702;&#30340;&#24615;&#33021;&#30340;&#26041;&#27861;&#65292;&#23454;&#39564;&#34920;&#26126;&#35813;&#26041;&#27861;&#33021;&#22815;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#24182;&#22312;&#22810;&#20010;&#39046;&#22495;&#20013;&#36827;&#34892;&#23569;&#26679;&#26412;&#36801;&#31227;&#12290;</title><link>http://arxiv.org/abs/2307.05209</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20013;&#22522;&#20110;&#22870;&#21169;&#26426;&#22120;&#25277;&#35937;&#30340;&#19978;&#19979;&#25991;&#39044;&#35268;&#21010;&#20197;&#22686;&#24378;&#36801;&#31227;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Contextual Pre-Planning on Reward Machine Abstractions for Enhanced Transfer in Deep Reinforcement Learning. (arXiv:2307.05209v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05209
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22870;&#21169;&#26426;&#22120;&#25277;&#35937;&#26469;&#34920;&#31034;&#24403;&#21069;&#20219;&#21153;&#65292;&#24182;&#22312;&#36801;&#31227;&#23398;&#20064;&#20013;&#25552;&#21319;DRL&#20195;&#29702;&#30340;&#24615;&#33021;&#30340;&#26041;&#27861;&#65292;&#23454;&#39564;&#34920;&#26126;&#35813;&#26041;&#27861;&#33021;&#22815;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#24182;&#22312;&#22810;&#20010;&#39046;&#22495;&#20013;&#36827;&#34892;&#23569;&#26679;&#26412;&#36801;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#20195;&#29702;&#20542;&#21521;&#20110;&#36807;&#25311;&#21512;&#35757;&#32451;&#20219;&#21153;&#65292;&#24182;&#19988;&#26080;&#27861;&#36866;&#24212;&#36731;&#24494;&#30340;&#29615;&#22659;&#21464;&#21270;&#12290;&#20026;&#20102;&#22312;&#36716;&#31227;&#21040;&#26410;&#35265;&#20219;&#21153;&#26102;&#21152;&#24555;&#23398;&#20064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22870;&#21169;&#26426;&#22120;&#65288;RM&#65289;&#26469;&#34920;&#31034;&#24403;&#21069;&#20219;&#21153;&#30340;&#26032;&#26041;&#27861;&#65292;&#22870;&#21169;&#26426;&#22120;&#26159;&#22522;&#20110;&#24403;&#21069;&#20219;&#21153;&#30340;&#22870;&#21169;&#21644;&#21160;&#24577;&#29983;&#25104;&#23376;&#20219;&#21153;&#30340;&#29366;&#24577;&#26426;&#25277;&#35937;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20026;&#20195;&#29702;&#25552;&#20379;&#20102;&#24403;&#21069;&#25277;&#35937;&#29366;&#24577;&#30340;&#31526;&#21495;&#34920;&#31034;&#65292;&#24182;&#22870;&#21169;&#23427;&#20204;&#36798;&#25104;&#36825;&#20123;&#36716;&#25442;&#12290;&#36825;&#20123;&#34920;&#31034;&#22312;&#20219;&#21153;&#20043;&#38388;&#20849;&#20139;&#65292;&#20351;&#20195;&#29702;&#33021;&#22815;&#21033;&#29992;&#20808;&#21069;&#36935;&#21040;&#30340;&#31526;&#21495;&#21644;&#36716;&#25442;&#30340;&#30693;&#35782;&#65292;&#20174;&#32780;&#22686;&#24378;&#36801;&#31227;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#35780;&#20272;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#34920;&#31034;&#22312;&#21508;&#31181;&#39046;&#22495;&#20013;&#25552;&#39640;&#20102;&#26679;&#26412;&#25928;&#29575;&#21644;&#23569;&#26679;&#26412;&#36801;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent studies show that deep reinforcement learning (DRL) agents tend to overfit to the task on which they were trained and fail to adapt to minor environment changes. To expedite learning when transferring to unseen tasks, we propose a novel approach to representing the current task using reward machines (RM), state machine abstractions that induce subtasks based on the current task's rewards and dynamics. Our method provides agents with symbolic representations of optimal transitions from their current abstract state and rewards them for achieving these transitions. These representations are shared across tasks, allowing agents to exploit knowledge of previously encountered symbols and transitions, thus enhancing transfer. Our empirical evaluation shows that our representations improve sample efficiency and few-shot transfer in a variety of domains.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102; OASIS 2 &#26412;&#20307;&#35770;&#65292;&#19968;&#31181;&#20026;&#20195;&#29702;&#25552;&#20379;&#35821;&#20041;&#34920;&#31034;&#21644;&#36890;&#20449;&#30340;&#34892;&#20026;&#20027;&#20041;&#26041;&#27861;&#12290;&#35813;&#26412;&#20307;&#35770;&#24050;&#24212;&#29992;&#20110;&#21306;&#22359;&#38142;&#21450;&#20854;&#20182;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2306.10061</link><description>&lt;p&gt;
&#20195;&#29702;&#12289;&#31995;&#32479;&#21644;&#26381;&#21153;&#38598;&#25104;&#30340;&#26412;&#20307;&#35770;&#65306;OASIS 2 &#29256;&#26412;
&lt;/p&gt;
&lt;p&gt;
The Ontology for Agents, Systems and Integration of Services: OASIS version 2. (arXiv:2306.10061v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10061
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102; OASIS 2 &#26412;&#20307;&#35770;&#65292;&#19968;&#31181;&#20026;&#20195;&#29702;&#25552;&#20379;&#35821;&#20041;&#34920;&#31034;&#21644;&#36890;&#20449;&#30340;&#34892;&#20026;&#20027;&#20041;&#26041;&#27861;&#12290;&#35813;&#26412;&#20307;&#35770;&#24050;&#24212;&#29992;&#20110;&#21306;&#22359;&#38142;&#21450;&#20854;&#20182;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#20041;&#34920;&#31034;&#26159;&#22810;&#20010;&#24212;&#29992;&#39046;&#22495;&#30340;&#20851;&#38190;&#26041;&#27861;&#65292;&#20854;&#20013;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#39046;&#22495;&#20063;&#19981;&#20363;&#22806;&#12290;&#22312;&#20026;&#20195;&#29702;&#36827;&#34892;&#35821;&#20041;&#34920;&#31034;&#30340;&#26041;&#27861;&#20013;&#65292;&#19968;&#20010;&#22522;&#26412;&#23454;&#29616;&#25163;&#27573;&#26159;&#37319;&#21462;&#34892;&#20026;&#20027;&#20041;&#35270;&#35282;&#65292;&#36890;&#36807;&#25551;&#36848;&#26234;&#33021;&#20307;&#22914;&#20309;&#25805;&#20316;&#24182;&#19982;&#20854;&#21516;&#34892;&#20114;&#21160;&#12290;&#36825;&#31181;&#26041;&#27861;&#20027;&#35201;&#26088;&#22312;&#36890;&#36807;&#19982;&#20219;&#21153;&#23436;&#25104;&#30456;&#20851;&#30340;&#24515;&#29702;&#29366;&#24577;&#26469;&#23450;&#20041;&#20195;&#29702;&#30340;&#25805;&#20316;&#33021;&#21147;&#12290;OASIS &#26412;&#20307;&#35770;&#65288;&#19968;&#31181;&#20195;&#29702;&#12289;&#31995;&#32479;&#21644;&#26381;&#21153;&#38598;&#25104;&#30340;&#26412;&#20307;&#35770;&#65289;&#65292;&#20110;2019&#24180;&#25552;&#20986;&#20102;&#36825;&#31181;&#34892;&#20026;&#20027;&#20041;&#26041;&#27861;&#65292;&#20197;&#25552;&#20379;&#26234;&#33021;&#20195;&#29702;&#21450;&#20854;&#25215;&#35834;&#30340;&#35821;&#20041;&#34920;&#31034;&#31995;&#32479;&#21644;&#36890;&#20449;&#21327;&#35758;&#12290;&#26412;&#25991;&#25253;&#21578;&#20102; OASIS 2 &#20013;&#26377;&#20851;&#20195;&#29702;&#34920;&#31034;&#30340;&#20027;&#35201;&#24314;&#27169;&#36873;&#25321;&#65292;&#36825;&#26159; OASIS &#30340;&#26368;&#26032;&#37325;&#22823;&#21319;&#32423;&#29256;&#26412;&#65292;&#24182;&#20171;&#32461;&#20102;&#33258;&#20854;&#39318;&#27425;&#24341;&#20837;&#20197;&#26469;&#26412;&#20307;&#35770;&#22312;&#21306;&#22359;&#38142;&#26412;&#20307;&#35770;&#31561;&#39046;&#22495;&#20013;&#30340;&#24212;&#29992;&#21450;&#25104;&#23601;&#12290;
&lt;/p&gt;
&lt;p&gt;
Semantic representation is a key enabler for several application domains, and the multi-agent systems realm makes no exception. Among the methods for semantically representing agents, one has been essentially achieved by taking a behaviouristic vision, through which one can describe how they operate and engage with their peers. The approach essentially aims at defining the operational capabilities of agents through the mental states related with the achievement of tasks. The OASIS ontology -- An Ontology for Agent, Systems, and Integration of Services, presented in 2019 -- pursues the behaviouristic approach to deliver a semantic representation system and a communication protocol for agents and their commitments. This paper reports on the main modeling choices concerning the representation of agents in OASIS 2, the latest major upgrade of OASIS, and the achievement reached by the ontology since it was first introduced, in particular in the context of ontologies for blockchains.
&lt;/p&gt;</description></item><item><title>SUNG&#26159;&#19968;&#31181;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#24341;&#23548;&#30340;&#31163;&#32447;&#21040;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#22312;&#36890;&#36807;&#37327;&#21270;&#19981;&#30830;&#23450;&#24615;&#36827;&#34892;&#25506;&#32034;&#21644;&#24212;&#29992;&#20445;&#23432;Q&#20540;&#20272;&#35745;&#30340;&#25351;&#23548;&#19979;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#32769;&#21270;&#24378;&#21270;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2306.07541</link><description>&lt;p&gt;
&#19968;&#31181;&#31616;&#21333;&#32479;&#19968;&#30340;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#24341;&#23548;&#30340;&#31163;&#32447;&#21040;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Simple Unified Uncertainty-Guided Framework for Offline-to-Online Reinforcement Learning. (arXiv:2306.07541v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07541
&lt;/p&gt;
&lt;p&gt;
SUNG&#26159;&#19968;&#31181;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#24341;&#23548;&#30340;&#31163;&#32447;&#21040;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#22312;&#36890;&#36807;&#37327;&#21270;&#19981;&#30830;&#23450;&#24615;&#36827;&#34892;&#25506;&#32034;&#21644;&#24212;&#29992;&#20445;&#23432;Q&#20540;&#20272;&#35745;&#30340;&#25351;&#23548;&#19979;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#32769;&#21270;&#24378;&#21270;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20026;&#20381;&#38752;&#25968;&#25454;&#39537;&#21160;&#33539;&#20363;&#23398;&#20064;&#26234;&#33021;&#20307;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290; &#28982;&#32780;&#65292;&#21463;&#38480;&#20110;&#31163;&#32447;&#25968;&#25454;&#38598;&#30340;&#26377;&#38480;&#36136;&#37327;&#65292;&#20854;&#24615;&#33021;&#24120;&#24120;&#19981;&#22815;&#20248;&#31168;&#12290;&#22240;&#27492;&#65292;&#22312;&#37096;&#32626;&#20043;&#21069;&#36890;&#36807;&#39069;&#22806;&#30340;&#22312;&#32447;&#20132;&#20114;&#36827;&#19968;&#27493;&#24494;&#35843;&#26234;&#33021;&#20307;&#26159;&#26377;&#24517;&#35201;&#30340;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#30001;&#20110;&#21463;&#21040;&#20004;&#20010;&#20027;&#35201;&#25361;&#25112;&#30340;&#21046;&#32422;&#65292;&#21363;&#21463;&#38480;&#30340;&#25506;&#32034;&#34892;&#20026;&#21644;&#29366;&#24577;-&#21160;&#20316;&#20998;&#24067;&#20559;&#31227;&#65292;&#31163;&#32447;&#21040;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32479;&#19968;&#30340;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#24341;&#23548;&#30340;&#65288;SUNG&#65289;&#26694;&#26550;&#65292;&#20854;&#36890;&#36807;&#19981;&#30830;&#23450;&#24615;&#24037;&#20855;&#33258;&#28982;&#22320;&#32479;&#19968;&#20102;&#36825;&#20004;&#20010;&#25361;&#25112;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;SUNG&#36890;&#36807;&#22522;&#20110;VAE&#30340;&#29366;&#24577;-&#21160;&#20316;&#35775;&#38382;&#23494;&#24230;&#20272;&#35745;&#22120;&#37327;&#21270;&#19981;&#30830;&#23450;&#24615;&#12290;&#20026;&#20102;&#20419;&#36827;&#39640;&#25928;&#25506;&#32034;&#65292;SUNG&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#29992;&#30340;&#20048;&#35266;&#25506;&#32034;&#31574;&#30053;&#65292;&#20197;&#36873;&#25321;&#20855;&#26377;&#39640;&#20215;&#20540;&#21644;&#39640;&#19981;&#30830;&#23450;&#24615;&#30340;&#20449;&#24687;&#21160;&#20316;&#12290;&#27492;&#22806;&#65292;SUNG&#36890;&#36807;&#22312;&#19981;&#30830;&#23450;&#24615;&#25351;&#23548;&#19979;&#24212;&#29992;&#20445;&#23432;Q&#20540;&#20272;&#35745;&#26469;&#24320;&#21457;&#19968;&#31181;&#33258;&#36866;&#24212;&#21033;&#29992;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;Atari&#21644;MuJoCo&#22522;&#20934;&#27979;&#35797;&#19978;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;SUNG&#22987;&#32456;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#31163;&#32447;&#21040;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#22312;&#35768;&#22810;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#25509;&#36817;&#22312;&#32447;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Offline reinforcement learning (RL) provides a promising solution to learning an agent fully relying on a data-driven paradigm. However, constrained by the limited quality of the offline dataset, its performance is often sub-optimal. Therefore, it is desired to further finetune the agent via extra online interactions before deployment. Unfortunately, offline-to-online RL can be challenging due to two main challenges: constrained exploratory behavior and state-action distribution shift. To this end, we propose a Simple Unified uNcertainty-Guided (SUNG) framework, which naturally unifies the solution to both challenges with the tool of uncertainty. Specifically, SUNG quantifies uncertainty via a VAE-based state-action visitation density estimator. To facilitate efficient exploration, SUNG presents a practical optimistic exploration strategy to select informative actions with both high value and high uncertainty. Moreover, SUNG develops an adaptive exploitation method by applying conserva
&lt;/p&gt;</description></item><item><title>STAR&#26159;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21512;&#25104;&#25968;&#25454;&#23454;&#20363;&#30340;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#36827;&#20302;&#36164;&#28304;&#20449;&#24687;&#25277;&#21462;&#65292;&#20026;&#23454;&#38469;&#24212;&#29992;&#25552;&#20379;&#20102;&#38656;&#35201;&#26368;&#23569;&#20154;&#24037;&#26631;&#27880;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2305.15090</link><description>&lt;p&gt;
STAR: &#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#32467;&#26500;&#21040;&#25991;&#26412;&#25968;&#25454;&#29983;&#25104;&#25913;&#36827;&#20302;&#36164;&#28304;&#20449;&#24687;&#25277;&#21462;
&lt;/p&gt;
&lt;p&gt;
STAR: Improving Low-Resource Information Extraction by Structure-to-Text Data Generation with Large Language Models. (arXiv:2305.15090v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15090
&lt;/p&gt;
&lt;p&gt;
STAR&#26159;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21512;&#25104;&#25968;&#25454;&#23454;&#20363;&#30340;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#36827;&#20302;&#36164;&#28304;&#20449;&#24687;&#25277;&#21462;&#65292;&#20026;&#23454;&#38469;&#24212;&#29992;&#25552;&#20379;&#20102;&#38656;&#35201;&#26368;&#23569;&#20154;&#24037;&#26631;&#27880;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20449;&#24687;&#25277;&#21462;&#20219;&#21153;&#65292;&#22914;&#20107;&#20214;&#25277;&#21462;&#65292;&#38656;&#35201;&#23545;&#36755;&#20986;&#32467;&#26500;&#21644;&#23376;&#20219;&#21153;&#20381;&#36182;&#36827;&#34892;&#28145;&#20837;&#29702;&#35299;&#12290;&#20026;&#20102;&#33719;&#24471;&#21512;&#29702;&#30340;&#24615;&#33021;&#65292;&#23427;&#20204;&#20005;&#37325;&#20381;&#36182;&#20110;&#20197;&#65288;&#27573;&#33853;&#65292;&#30446;&#26631;&#32467;&#26500;&#65289;&#23545;&#30340;&#24418;&#24335;&#30340;&#20219;&#21153;&#29305;&#23450;&#35757;&#32451;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;&#20154;&#24037;&#27880;&#37322;&#33719;&#24471;&#36825;&#26679;&#30340;&#25968;&#25454;&#26159;&#26114;&#36149;&#30340;&#65292;&#22240;&#27492;&#23545;&#20110;&#23454;&#38469;&#24212;&#29992;&#65292;&#25105;&#20204;&#36843;&#20999;&#38656;&#35201;&#38656;&#35201;&#26368;&#23569;&#20154;&#24037;&#26631;&#27880;&#30340;&#20302;&#36164;&#28304;&#20449;&#24687;&#25277;&#21462;&#26041;&#27861;&#12290;&#20351;&#29992;&#21512;&#25104;&#35757;&#32451;&#25968;&#25454;&#23545;&#30417;&#30563;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#21487;&#33021;&#26159;&#19968;&#31181;&#36890;&#29992;&#26041;&#27861;&#65292;&#20294;&#29616;&#26377;&#30340;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#35201;&#20040;&#20173;&#28982;&#20381;&#36182;&#20110;&#22823;&#35268;&#27169;&#30340;&#30495;&#23454;&#25968;&#25454;&#65292;&#35201;&#20040;&#30001;&#20110;&#24615;&#33021;&#24046;&#32780;&#26080;&#27861;&#24212;&#29992;&#20110;&#22797;&#26434;&#30340;&#20449;&#24687;&#25277;&#21462;&#20219;&#21153;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;STAR&#65292;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26681;&#25454;&#26377;&#38480;&#30340;&#31181;&#23376;&#31034;&#20363;&#21512;&#25104;&#25968;&#25454;&#23454;&#20363;&#65292;&#20174;&#32780;&#25552;&#39640;&#20302;&#36164;&#28304;&#20449;&#24687;&#25277;&#21462;&#24615;&#33021;&#30340;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Information extraction tasks such as event extraction require an in-depth understanding of the output structure and sub-task dependencies. They heavily rely on task-specific training data in the form of (passage, target structure) pairs to obtain reasonable performance. However, obtaining such data through human annotation is costly, leading to a pressing need for low-resource information extraction approaches that require minimal human labeling for real-world applications. Fine-tuning supervised models with synthesized training data would be a generalizable method, but the existing data generation methods either still rely on large-scale ground-truth data or cannot be applied to complicated IE tasks due to their poor performance. To address these challenges, we propose STAR, a data generation method that leverages Large Language Models (LLMs) to synthesize data instances given limited seed demonstrations, thereby boosting low-resource information extraction performance. Our approach i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;CRITIC&#30340;&#26694;&#26550;&#65292;&#20351;&#24471;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#19982;&#24037;&#20855;&#30340;&#20132;&#20114;&#26657;&#27491;&#33258;&#24049;&#30340;&#38169;&#35823;&#65292;&#20174;&#32780;&#36991;&#20813;&#29983;&#25104;&#20986;&#29616;&#19981;&#19968;&#33268;&#21644;&#38382;&#39064;&#34892;&#20026;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.11738</link><description>&lt;p&gt;
CRITIC&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#24037;&#20855;&#20132;&#20114;&#25209;&#35780;&#36827;&#34892;&#33258;&#25105;&#26657;&#27491;
&lt;/p&gt;
&lt;p&gt;
CRITIC: Large Language Models Can Self-Correct with Tool-Interactive Critiquing. (arXiv:2305.11738v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11738
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;CRITIC&#30340;&#26694;&#26550;&#65292;&#20351;&#24471;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#19982;&#24037;&#20855;&#30340;&#20132;&#20114;&#26657;&#27491;&#33258;&#24049;&#30340;&#38169;&#35823;&#65292;&#20174;&#32780;&#36991;&#20813;&#29983;&#25104;&#20986;&#29616;&#19981;&#19968;&#33268;&#21644;&#38382;&#39064;&#34892;&#20026;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21457;&#23637;&#38750;&#24120;&#24341;&#20154;&#27880;&#30446;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#26377;&#26102;&#20250;&#20986;&#29616;&#19981;&#19968;&#33268;&#21644;&#38382;&#39064;&#34892;&#20026;&#65292;&#20363;&#22914;&#20986;&#29616;&#24187;&#35273;&#20107;&#23454;&#65292;&#29983;&#25104;&#26377;&#32570;&#38519;&#30340;&#20195;&#30721;&#25110;&#21019;&#24314;&#20882;&#29359;&#21644;&#26377;&#23475;&#30340;&#20869;&#23481;&#12290;&#19982;&#36825;&#20123;&#27169;&#22411;&#19981;&#21516;&#65292;&#20154;&#31867;&#36890;&#24120;&#20351;&#29992;&#22806;&#37096;&#24037;&#20855;&#26469;&#20132;&#21449;&#26816;&#26597;&#21644;&#31934;&#28860;&#20182;&#20204;&#30340;&#21021;&#27493;&#20869;&#23481;&#65292;&#20363;&#22914;&#20351;&#29992;&#25628;&#32034;&#24341;&#25806;&#36827;&#34892;&#20107;&#23454;&#26816;&#26597;&#25110;&#20351;&#29992;&#20195;&#30721;&#35299;&#37322;&#22120;&#36827;&#34892;&#35843;&#35797;&#12290;&#21463;&#36825;&#19968;&#35266;&#23519;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;CRITIC&#30340;&#26694;&#26550;&#65292;&#20801;&#35768;LLMs&#65288;&#23454;&#36136;&#19978;&#26159;&#8220;&#40657;&#30418;&#23376;&#8221;&#65289;&#20197;&#31867;&#20284;&#20110;&#20154;&#31867;&#19982;&#24037;&#20855;&#20132;&#20114;&#30340;&#26041;&#24335;&#39564;&#35777;&#21644;&#36880;&#27493;&#20462;&#27491;&#33258;&#24049;&#30340;&#36755;&#20986;&#12290;&#26356;&#20855;&#20307;&#22320;&#35828;&#65292;&#20174;&#21021;&#22987;&#36755;&#20986;&#24320;&#22987;&#65292;CRITIC&#19982;&#36866;&#24403;&#30340;&#24037;&#20855;&#20132;&#20114;&#20197;&#35780;&#20272;&#25991;&#26412;&#30340;&#26576;&#20123;&#26041;&#38754;&#65292;&#28982;&#21518;&#26681;&#25454;&#22312;&#27492;&#39564;&#35777;&#36807;&#31243;&#20013;&#33719;&#24471;&#30340;&#21453;&#39304;&#20462;&#25913;&#36755;&#20986;&#12290;&#28041;&#21450;&#33258;&#30001;&#24418;&#24335;&#38382;&#31572;&#12289;&#25968;&#23398;&#31243;&#24207;&#32508;&#21512;&#21644;&#27602;&#24615;&#26816;&#27979;&#30340;&#20840;&#38754;&#35780;&#20272;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#20351;LLMs&#33021;&#22815;&#20174;&#38169;&#35823;&#20013;&#23398;&#20064;&#24182;&#32416;&#27491;&#33258;&#24049;&#30340;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent developments in large language models (LLMs) have been impressive. However, these models sometimes show inconsistencies and problematic behavior, such as hallucinating facts, generating flawed code, or creating offensive and toxic content. Unlike these models, humans typically utilize external tools to cross-check and refine their initial content, like using a search engine for fact-checking, or a code interpreter for debugging. Inspired by this observation, we introduce a framework called CRITIC that allows LLMs, which are essentially "black boxes" to validate and progressively amend their own outputs in a manner similar to human interaction with tools. More specifically, starting with an initial output, CRITIC interacts with appropriate tools to evaluate certain aspects of the text, and then revises the output based on the feedback obtained during this validation process. Comprehensive evaluations involving free-form question answering, mathematical program synthesis, and toxi
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#20221;&#20013;&#25991;&#30340;&#22522;&#20110;&#25351;&#20196;&#30340;&#20449;&#24687;&#25552;&#21462;&#25968;&#25454;&#38598;InstructIE&#65292;&#20854;&#20013;&#21253;&#25324;&#20102;270,000&#20010;&#24369;&#30417;&#30563;&#30340;&#25968;&#25454;&#21644;1,000&#20010;&#39640;&#36136;&#37327;&#27880;&#37322;&#23454;&#20363;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#24403;&#21069;&#30340;&#27169;&#22411;&#34920;&#29616;&#26377;&#24453;&#25913;&#36827;&#65292;&#35813;&#20219;&#21153;&#20173;&#23384;&#22312;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2305.11527</link><description>&lt;p&gt;
InstructIE: &#19968;&#20221;&#22522;&#20110;&#25351;&#20196;&#30340;&#20013;&#25991;&#20449;&#24687;&#25552;&#21462;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
InstructIE: A Chinese Instruction-based Information Extraction Dataset. (arXiv:2305.11527v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11527
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#20221;&#20013;&#25991;&#30340;&#22522;&#20110;&#25351;&#20196;&#30340;&#20449;&#24687;&#25552;&#21462;&#25968;&#25454;&#38598;InstructIE&#65292;&#20854;&#20013;&#21253;&#25324;&#20102;270,000&#20010;&#24369;&#30417;&#30563;&#30340;&#25968;&#25454;&#21644;1,000&#20010;&#39640;&#36136;&#37327;&#27880;&#37322;&#23454;&#20363;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#24403;&#21069;&#30340;&#27169;&#22411;&#34920;&#29616;&#26377;&#24453;&#25913;&#36827;&#65292;&#35813;&#20219;&#21153;&#20173;&#23384;&#22312;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#39033;&#26032;&#30340;&#20449;&#24687;&#25552;&#21462;&#20219;&#21153;&#65292;&#31216;&#20026;&#22522;&#20110;&#25351;&#20196;&#30340;&#20449;&#24687;&#25552;&#21462; (Instruction-based IE)&#65292;&#23427;&#26088;&#22312;&#35201;&#27714;&#31995;&#32479;&#36981;&#24490;&#29305;&#23450;&#30340;&#25351;&#20196;&#25110;&#25351;&#21335;&#26469;&#25552;&#21462;&#20449;&#24687;&#12290;&#20026;&#20102;&#20419;&#36827;&#35813;&#39046;&#22495;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;&#65292;&#31216;&#20026;InstructIE&#65292;&#20854;&#20013;&#21253;&#25324;&#26469;&#33258;&#20013;&#25991;&#32500;&#22522;&#30334;&#31185;&#30340; 270,000 &#20010;&#24369;&#30417;&#30563;&#25968;&#25454;&#21644; 1,000 &#20010;&#39640;&#36136;&#37327;&#20247;&#21253;&#27880;&#37322;&#23454;&#20363;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35780;&#20272;&#20102;&#21508;&#31181;&#22522;&#32447;&#27169;&#22411;&#22312;InstructIE&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#23613;&#31649;&#24403;&#21069;&#30340;&#27169;&#22411;&#34920;&#29616;&#24456;&#26377;&#24076;&#26395;&#65292;&#20294;&#20173;&#26377;&#25913;&#36827;&#30340;&#31354;&#38388;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#26696;&#20363;&#30740;&#31350;&#20998;&#26512;&#65292;&#24378;&#35843;&#20102;&#22522;&#20110;&#25351;&#20196;&#30340;&#20449;&#24687;&#25552;&#21462;&#20219;&#21153;&#20013;&#22266;&#26377;&#30340;&#25361;&#25112;&#12290;&#20195;&#30721;&#21644;&#25968;&#25454;&#38598;&#21487;&#22312; https://github.com/zjunlp/DeepKE/tree/main/example/llm &#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a new Information Extraction (IE) task dubbed Instruction-based IE, which aims to ask the system to follow specific instructions or guidelines to extract information. To facilitate research in this area, we construct a dataset called InstructIE, consisting of 270,000 weakly supervised data from Chinese Wikipedia and 1,000 high-quality crowdsourced annotated instances. We further evaluate the performance of various baseline models on the InstructIE dataset. The results reveal that although current models exhibit promising performance, there is still room for improvement. Furthermore, we conduct a comprehensive case study analysis, underlining the challenges inherent in the Instruction-based IE task. Code and dataset are available at https://github.com/zjunlp/DeepKE/tree/main/example/llm.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;AMS-DRL&#26041;&#27861;&#29992;&#20110;&#35757;&#32451;&#23545;&#25239;&#24615;&#31070;&#32463;&#32593;&#32476;&#65292;&#20197;&#23398;&#20064;&#21644;&#24555;&#36895;&#36866;&#24212;&#22810;&#20010;&#25915;&#20987;&#32773;&#30340;&#34892;&#20026;&#65292;&#20174;&#32780;&#23454;&#29616;&#26080;&#20154;&#26426;&#30340;&#23433;&#20840;&#23548;&#33322;&#21644;&#21040;&#36798;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2304.03443</link><description>&lt;p&gt;
AMS-DRL: &#23398;&#20064;&#22810;&#30446;&#26631;&#36867;&#36991;&#20197;&#23454;&#29616;&#26080;&#20154;&#26426;&#23433;&#20840;&#23548;&#33322;
&lt;/p&gt;
&lt;p&gt;
AMS-DRL: Learning Multi-Pursuit Evasion for Safe Targeted Navigation of Drones. (arXiv:2304.03443v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03443
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;AMS-DRL&#26041;&#27861;&#29992;&#20110;&#35757;&#32451;&#23545;&#25239;&#24615;&#31070;&#32463;&#32593;&#32476;&#65292;&#20197;&#23398;&#20064;&#21644;&#24555;&#36895;&#36866;&#24212;&#22810;&#20010;&#25915;&#20987;&#32773;&#30340;&#34892;&#20026;&#65292;&#20174;&#32780;&#23454;&#29616;&#26080;&#20154;&#26426;&#30340;&#23433;&#20840;&#23548;&#33322;&#21644;&#21040;&#36798;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#20010;&#34989;&#20987;&#32773;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#65292;&#26080;&#20154;&#26426;&#30340;&#23433;&#20840;&#23548;&#33322;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#24322;&#27493;&#22810;&#38454;&#27573;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;(AMS-DRL)&#65292;&#26469;&#35757;&#32451;&#23545;&#25239;&#24615;&#31070;&#32463;&#32593;&#32476;&#65292;&#35813;&#32593;&#32476;&#21487;&#20197;&#20174;&#22810;&#20010;&#25915;&#20987;&#32773;&#30340;&#34892;&#21160;&#20013;&#23398;&#20064;&#21644;&#24555;&#36895;&#36866;&#24212;&#23427;&#20204;&#30340;&#34892;&#20026;&#65292;&#20351;&#26080;&#20154;&#26426;&#33021;&#22815;&#36991;&#20813;&#25915;&#20987;&#24182;&#21040;&#36798;&#30446;&#26631;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#30830;&#20445;&#21338;&#24328;&#35770;&#20998;&#26512;&#20013;&#30340;&#20195;&#29702;&#20043;&#38388;&#30340;Nash&#22343;&#34913;&#26469;&#20445;&#35777;&#25910;&#25947;&#24615;&#12290;&#25105;&#20204;&#22312;&#24191;&#27867;&#30340;&#27169;&#25311;&#20013;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#27604;&#22522;&#32447;&#26041;&#27861;&#20855;&#26377;&#26356;&#39640;&#30340;&#23548;&#33322;&#25104;&#21151;&#29575;&#12290;&#25105;&#20204;&#36824;&#20998;&#26512;&#20102;&#19968;&#20123;&#21442;&#25968;&#22914;&#30456;&#23545;&#26368;&#22823;&#36895;&#24230;&#22914;&#20309;&#24433;&#21709;&#23548;&#33322;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#29289;&#29702;&#23454;&#39564;&#65292;&#24182;&#39564;&#35777;&#20102;&#23454;&#26102;&#39134;&#34892;&#20013;&#21463;&#35757;&#31574;&#30053;&#30340;&#26377;&#25928;&#24615;&#12290;&#20171;&#32461;&#20102;&#25104;&#21151;&#29575;&#28909;&#22270;&#65292;&#20197;&#35828;&#26126;&#31354;&#38388;&#20960;&#20309;&#23545;&#23548;&#33322;&#32467;&#26524;&#30340;&#24433;&#21709;&#12290;&#39033;&#30446;&#32593;&#31449;&#65306;https://gi
&lt;/p&gt;
&lt;p&gt;
Safe navigation of drones in the presence of adversarial physical attacks from multiple pursuers is a challenging task. This paper proposes a novel approach, asynchronous multi-stage deep reinforcement learning (AMS-DRL), to train an adversarial neural network that can learn from the actions of multiple pursuers and adapt quickly to their behavior, enabling the drone to avoid attacks and reach its target. Our approach guarantees convergence by ensuring Nash Equilibrium among agents from the game-theory analysis. We evaluate our method in extensive simulations and show that it outperforms baselines with higher navigation success rates. We also analyze how parameters such as the relative maximum speed affect navigation performance. Furthermore, we have conducted physical experiments and validated the effectiveness of the trained policies in real-time flights. A success rate heatmap is introduced to elucidate how spatial geometry influences navigation outcomes. Project website: https://gi
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#36880;&#23618;&#32858;&#21512;&#24322;&#24120;&#24471;&#20998;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#26356;&#22909;&#22320;&#36827;&#34892;&#25991;&#26412;OOD&#26816;&#27979;&#12290;&#20854;&#33021;&#21457;&#25496;&#19981;&#21516;&#23618;&#36755;&#20986;&#30340;&#20248;&#21183;&#65292;&#36798;&#21040;&#26356;&#40065;&#26834;&#30340;&#24615;&#33021;&#65292;&#24182;&#25193;&#23637;&#32463;&#20856;&#22522;&#20934;&#27979;&#35797;&#20197;&#21453;&#26144;&#26356;&#29616;&#23454;&#30340;&#35774;&#32622;&#12290;</title><link>http://arxiv.org/abs/2302.09852</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#30340;&#36880;&#23618;&#25991;&#26412;OOD&#26816;&#27979;&#24471;&#20998;&#32858;&#21512;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Layer-wise Score Aggregation for Textual OOD Detection. (arXiv:2302.09852v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.09852
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#36880;&#23618;&#32858;&#21512;&#24322;&#24120;&#24471;&#20998;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#26356;&#22909;&#22320;&#36827;&#34892;&#25991;&#26412;OOD&#26816;&#27979;&#12290;&#20854;&#33021;&#21457;&#25496;&#19981;&#21516;&#23618;&#36755;&#20986;&#30340;&#20248;&#21183;&#65292;&#36798;&#21040;&#26356;&#40065;&#26834;&#30340;&#24615;&#33021;&#65292;&#24182;&#25193;&#23637;&#32463;&#20856;&#22522;&#20934;&#27979;&#35797;&#20197;&#21453;&#26144;&#26356;&#29616;&#23454;&#30340;&#35774;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#36234;&#26469;&#36234;&#22810;&#22522;&#20110;AI&#30340;&#31995;&#32479;&#22686;&#21152;&#65292;OOD&#26816;&#27979;&#26159;&#19968;&#20010;&#36805;&#36895;&#21457;&#23637;&#30340;&#39046;&#22495;&#65292;&#30001;&#20110;&#26032;&#30340;&#40065;&#26834;&#24615;&#21644;&#23433;&#20840;&#24615;&#35201;&#27714;&#12290;&#29616;&#26377;&#30340;OOD&#25991;&#26412;&#26816;&#27979;&#22120;&#36890;&#24120;&#20381;&#36182;&#20110;&#22312;&#32534;&#30721;&#22120;&#30340;&#26368;&#21518;&#19968;&#23618;&#36755;&#20986;&#19978;&#35745;&#31639;&#30340;&#24322;&#24120;&#24471;&#20998;&#65288;&#20363;&#22914;&#39532;&#27663;&#36317;&#31163;&#65289;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;OOD&#26816;&#27979;&#24615;&#33021;&#22240;&#20219;&#21153;&#21644;&#23618;&#36755;&#20986;&#32780;&#24322;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#34920;&#26126;&#36890;&#24120;&#30340;&#36873;&#25321;&#65288;&#26368;&#21518;&#19968;&#23618;&#65289;&#24456;&#23569;&#26159;OOD&#26816;&#27979;&#30340;&#26368;&#20339;&#36873;&#25321;&#65292;&#22914;&#26524;&#36873;&#25321;&#26368;&#20339;&#23618;&#65292;&#21017;&#21487;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;&#20026;&#20102;&#21033;&#29992;&#36825;&#20010;&#35266;&#23519;&#32467;&#26524;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#39537;&#21160;&#30340;&#26080;&#30417;&#30563;&#26041;&#27861;&#26469;&#32467;&#21512;&#36880;&#23618;&#30340;&#24322;&#24120;&#24471;&#20998;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#21253;&#25324;&#26356;&#22810;&#31867;&#21035;&#30340;&#20998;&#31867;&#20219;&#21153;&#65288;&#39640;&#36798;77&#65289;&#25193;&#23637;&#20102;&#32463;&#20856;&#25991;&#26412;OOD&#22522;&#20934;&#27979;&#35797;&#65292;&#20174;&#32780;&#21453;&#26144;&#26356;&#29616;&#23454;&#30340;&#35774;&#32622;&#12290;&#22312;&#36825;&#20010;&#22686;&#24378;&#30340;&#22522;&#20934;&#27979;&#35797;&#19978;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;&#21518;&#32858;&#21512;&#26041;&#27861;&#23454;&#29616;&#20102;&#40065;&#26834;&#30340;OOD&#26816;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Out-of-distribution (OOD) detection is a rapidly growing field due to new robustness and security requirements driven by an increased number of AI-based systems. Existing OOD textual detectors often rely on an anomaly score (e.g., Mahalanobis distance) computed on the embedding output of the last layer of the encoder. In this work, we observe that OOD detection performance varies greatly depending on the task and layer output. More importantly, we show that the usual choice (the last layer) is rarely the best one for OOD detection and that far better results could be achieved if the best layer were picked. To leverage this observation, we propose a data-driven, unsupervised method to combine layer-wise anomaly scores. In addition, we extend classical textual OOD benchmarks by including classification tasks with a greater number of classes (up to 77), which reflects more realistic settings. On this augmented benchmark, we show that the proposed post-aggregation methods achieve robust an
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#37327;&#21270;&#38543;&#26426;&#36807;&#31243;&#32479;&#35745;&#20381;&#36182;&#20851;&#31995;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#20132;&#26367;&#21327;&#26041;&#24046;&#20272;&#35745;&#21644;&#35268;&#33539;&#21270;&#20132;&#21449;&#23494;&#24230;&#26469;&#34913;&#37327;&#22810;&#21464;&#37327;&#32479;&#35745;&#20381;&#36182;&#24615;&#65292;&#24182;&#24212;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#26550;&#26500;&#20013;&#12290;</title><link>http://arxiv.org/abs/2212.04631</link><description>&lt;p&gt;
&#35268;&#33539;&#21270;&#20132;&#21449;&#23494;&#24230;&#20989;&#25968;&#65306;&#19968;&#31181;&#29992;&#20110;&#37327;&#21270;&#38543;&#26426;&#36807;&#31243;&#32479;&#35745;&#20381;&#36182;&#20851;&#31995;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
The Normalized Cross Density Functional: A Framework to Quantify Statistical Dependence for Random Processes. (arXiv:2212.04631v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.04631
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#37327;&#21270;&#38543;&#26426;&#36807;&#31243;&#32479;&#35745;&#20381;&#36182;&#20851;&#31995;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#20132;&#26367;&#21327;&#26041;&#24046;&#20272;&#35745;&#21644;&#35268;&#33539;&#21270;&#20132;&#21449;&#23494;&#24230;&#26469;&#34913;&#37327;&#22810;&#21464;&#37327;&#32479;&#35745;&#20381;&#36182;&#24615;&#65292;&#24182;&#24212;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#26550;&#26500;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38463;&#23572;&#24343;&#38647;&#24503;&#183;&#38647;&#23612;&#65288;Alfr\'ed R\'enyi&#65289;&#30340;&#21151;&#33021;&#26041;&#27861;&#35770;&#65292;&#36890;&#36807;&#23545;&#20004;&#20010;&#36830;&#32493;&#38543;&#26426;&#36807;&#31243;&#65288;r.p.&#65289;&#20043;&#38388;&#30340;&#32479;&#35745;&#20381;&#36182;&#20851;&#31995;&#36827;&#34892;&#26032;&#39062;&#30340;&#22810;&#21464;&#37327;&#23450;&#20041;&#12290;&#23558;&#38543;&#26426;&#36807;&#31243;&#26679;&#26412;&#23545;&#30340;&#20114;&#20449;&#24687;&#30340;&#23545;&#25968;&#35770;&#35777;&#21629;&#21517;&#20026;&#35268;&#33539;&#21270;&#20132;&#21449;&#23494;&#24230;&#65288;NCD&#65289;&#65292;&#23450;&#20041;&#20102;&#19968;&#31181;&#23545;&#31216;&#21644;&#33258;&#20276;&#30340;&#27491;&#23450;&#20989;&#25968;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#26368;&#22823;&#21270;&#20132;&#26367;&#21327;&#26041;&#24046;&#20272;&#35745;&#65288;ACE&#65289;&#36882;&#24402;&#24212;&#29992;&#20110;&#36755;&#20837;&#26679;&#26412;&#23545;&#30340;&#32852;&#21512;&#27010;&#29575;&#23494;&#24230;&#65292;&#31526;&#21512;&#38647;&#23612;&#30340;&#26368;&#22823;&#30456;&#20851;&#24615;&#30340;&#25152;&#26377;&#24615;&#36136;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;NCD&#30340;&#29305;&#24449;&#35889;&#20316;&#20026;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#21464;&#37327;&#24230;&#37327;&#65292;&#29992;&#20110;&#34913;&#37327;&#36755;&#20837;&#21644;&#36755;&#20986;r.p.&#20043;&#38388;&#30340;&#32479;&#35745;&#20381;&#36182;&#20851;&#31995;&#12290;&#21033;&#29992;r.p.&#30340;&#23454;&#29616;&#65292;&#20063;&#21487;&#20197;&#30452;&#25509;&#20272;&#35745;&#22810;&#21464;&#37327;&#32479;&#35745;&#20381;&#36182;&#24615;&#12290;&#25552;&#20986;&#30340;&#21151;&#33021;&#26368;&#22823;&#30456;&#20851;&#31639;&#27861;&#65288;FMCA&#65289;&#24212;&#29992;&#20110;&#30001;&#20004;&#20010;&#31070;&#32463;&#32593;&#32476;&#26500;&#24314;&#30340;&#26426;&#22120;&#23398;&#20064;&#26550;&#26500;&#19978;&#65292;&#36890;&#36807;&#36924;&#36817;&#32852;&#21512;&#35757;&#32451;&#26469;&#21516;&#26102;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a novel multivariate definition of statistical dependence between two continuous random processes (r.p.) using a functional methodology inspired by Alfr\'ed R\'enyi. The argument of the logarithm of mutual information between pairs of samples of a r.p., named here the normalized cross density (NCD), defines a symmetric and self-adjoint positive definite function. We show that maximizing the alternating covariance estimation (ACE) recursion, applied to each of the joint probability density of input sample pairs, obeys all the properties of Renyi's maximal correlation. We propose the NCD's eigenspectrum as a novel multivariate measure of the statistical dependence between the input and output r.p.  The multivariate statistical dependence can also be estimated directly from r.p. realizations. The proposed functional maximum correlation algorithm (FMCA) is applied to a machine learning architecture built from two neural networks that learn concurrently by approximating 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#35843;&#26597;&#32508;&#36848;&#20102;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#25298;&#32477;&#36873;&#39033;&#12290;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36991;&#20813;&#22312;&#21487;&#33021;&#29359;&#38169;&#35823;&#26102;&#20570;&#20986;&#39044;&#27979;&#65292;&#21487;&#20197;&#22312;&#20915;&#31574;&#25903;&#25345;&#24212;&#29992;&#20013;&#36991;&#20813;&#20005;&#37325;&#21518;&#26524;&#12290;&#35843;&#26597;&#20171;&#32461;&#20102;&#25298;&#32477;&#36873;&#39033;&#30340;&#26465;&#20214;&#12289;&#35780;&#20272;&#31574;&#30053;&#20197;&#21450;&#30456;&#20851;&#24212;&#29992;&#39046;&#22495;&#65292;&#24182;&#25506;&#35752;&#20102;&#23427;&#19982;&#20854;&#20182;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2107.11277</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#25298;&#32477;&#36873;&#39033;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Machine Learning with a Reject Option: A survey. (arXiv:2107.11277v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2107.11277
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#35843;&#26597;&#32508;&#36848;&#20102;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#25298;&#32477;&#36873;&#39033;&#12290;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36991;&#20813;&#22312;&#21487;&#33021;&#29359;&#38169;&#35823;&#26102;&#20570;&#20986;&#39044;&#27979;&#65292;&#21487;&#20197;&#22312;&#20915;&#31574;&#25903;&#25345;&#24212;&#29992;&#20013;&#36991;&#20813;&#20005;&#37325;&#21518;&#26524;&#12290;&#35843;&#26597;&#20171;&#32461;&#20102;&#25298;&#32477;&#36873;&#39033;&#30340;&#26465;&#20214;&#12289;&#35780;&#20272;&#31574;&#30053;&#20197;&#21450;&#30456;&#20851;&#24212;&#29992;&#39046;&#22495;&#65292;&#24182;&#25506;&#35752;&#20102;&#23427;&#19982;&#20854;&#20182;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24635;&#26159;&#20570;&#20986;&#39044;&#27979;&#65292;&#21363;&#20351;&#21487;&#33021;&#26159;&#19981;&#20934;&#30830;&#30340;&#12290;&#22312;&#35768;&#22810;&#20915;&#31574;&#25903;&#25345;&#24212;&#29992;&#20013;&#65292;&#24212;&#36991;&#20813;&#36825;&#31181;&#34892;&#20026;&#65292;&#22240;&#20026;&#38169;&#35823;&#21487;&#33021;&#24102;&#26469;&#20005;&#37325;&#21518;&#26524;&#12290;&#23613;&#31649;&#22312;1970&#24180;&#24050;&#32463;&#30740;&#31350;&#36807;&#65292;&#20294;&#36817;&#24180;&#26469;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#25298;&#32477;&#36873;&#39033;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#20851;&#27880;&#12290;&#36825;&#20010;&#26426;&#22120;&#23398;&#20064;&#23376;&#39046;&#22495;&#20351;&#24471;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#33021;&#22815;&#22312;&#21487;&#33021;&#29359;&#38169;&#35823;&#26102;&#36991;&#20813;&#20570;&#20986;&#39044;&#27979;&#12290;&#26412;&#35843;&#26597;&#26088;&#22312;&#25552;&#20379;&#26426;&#22120;&#23398;&#20064;&#20013;&#25298;&#32477;&#36873;&#39033;&#30340;&#27010;&#36848;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#23548;&#33268;&#20004;&#31181;&#25298;&#32477;&#24773;&#20917;&#65288;&#27169;&#31946;&#21644;&#26032;&#22855;&#25298;&#32477;&#65289;&#30340;&#26465;&#20214;&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#20102;&#20180;&#32454;&#30340;&#24418;&#24335;&#21270;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#22238;&#39038;&#21644;&#20998;&#31867;&#20102;&#35780;&#20272;&#27169;&#22411;&#39044;&#27979;&#21644;&#25298;&#32477;&#36136;&#37327;&#30340;&#31574;&#30053;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#29616;&#26377;&#30340;&#24102;&#26377;&#25298;&#32477;&#36873;&#39033;&#30340;&#27169;&#22411;&#26550;&#26500;&#65292;&#24182;&#25551;&#36848;&#20102;&#23398;&#20064;&#36825;&#20123;&#27169;&#22411;&#30340;&#26631;&#20934;&#25216;&#26415;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#30456;&#20851;&#24212;&#29992;&#39046;&#22495;&#30340;&#31034;&#20363;&#65292;&#24182;&#23637;&#31034;&#20102;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#25298;&#32477;&#36873;&#39033;&#19982;&#20854;&#20182;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning models always make a prediction, even when it is likely to be inaccurate. This behavior should be avoided in many decision support applications, where mistakes can have severe consequences. Albeit already studied in 1970, machine learning with rejection recently gained interest. This machine learning subfield enables machine learning models to abstain from making a prediction when likely to make a mistake.  This survey aims to provide an overview on machine learning with rejection. We introduce the conditions leading to two types of rejection, ambiguity and novelty rejection, which we carefully formalize. Moreover, we review and categorize strategies to evaluate a model's predictive and rejective quality. Additionally, we define the existing architectures for models with rejection and describe the standard techniques for learning such models. Finally, we provide examples of relevant application domains and show how machine learning with rejection relates to other machi
&lt;/p&gt;</description></item></channel></rss>