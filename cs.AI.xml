<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;ViewRefer&#65292;&#36825;&#26159;&#19968;&#20010;&#22810;&#35270;&#35282;&#30340;&#19977;&#32500;&#35270;&#35273;&#23450;&#20301;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#21644;&#22810;&#35270;&#35282;&#21407;&#22411;&#65292;&#20174;&#25991;&#26412;&#21644;3D&#27169;&#24577;&#20013;&#33719;&#21462;&#35270;&#35282;&#30693;&#35782;&#24182;&#22686;&#24378;&#26694;&#26550;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2303.16894</link><description>&lt;p&gt;
ViewRefer: &#22522;&#20110;GPT&#21644;&#26679;&#20363;&#24341;&#23548;&#30340;&#22810;&#35270;&#35282;&#30693;&#35782;&#22788;&#29702;&#30340;&#19977;&#32500;&#35270;&#35273;&#23450;&#20301;
&lt;/p&gt;
&lt;p&gt;
ViewRefer: Grasp the Multi-view Knowledge for 3D Visual Grounding with GPT and Prototype Guidance. (arXiv:2303.16894v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16894
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;ViewRefer&#65292;&#36825;&#26159;&#19968;&#20010;&#22810;&#35270;&#35282;&#30340;&#19977;&#32500;&#35270;&#35273;&#23450;&#20301;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#21644;&#22810;&#35270;&#35282;&#21407;&#22411;&#65292;&#20174;&#25991;&#26412;&#21644;3D&#27169;&#24577;&#20013;&#33719;&#21462;&#35270;&#35282;&#30693;&#35782;&#24182;&#22686;&#24378;&#26694;&#26550;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#22810;&#35270;&#35282;&#36755;&#20837;&#30340;3D&#22330;&#26223;&#65292;&#21487;&#20197;&#32531;&#35299;3D&#35270;&#35273;&#23450;&#20301;&#20013;&#30340;&#35270;&#35282;&#24046;&#24322;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#24573;&#30053;&#20102;&#23884;&#20837;&#22312;&#25991;&#26412;&#27169;&#24577;&#20013;&#30340;&#35270;&#35282;&#32447;&#32034;&#65292;&#24182;&#19988;&#26410;&#33021;&#26435;&#34913;&#19981;&#21516;&#35270;&#22270;&#30340;&#30456;&#23545;&#37325;&#35201;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;ViewRefer&#65292;&#36825;&#26159;&#19968;&#20010;&#22810;&#35270;&#35282;&#30340;&#19977;&#32500;&#35270;&#35273;&#23450;&#20301;&#26694;&#26550;&#65292;&#25506;&#32034;&#22914;&#20309;&#20174;&#25991;&#26412;&#21644;3D&#27169;&#24577;&#20013;&#33719;&#21462;&#35270;&#35282;&#30693;&#35782;&#12290;&#20854;&#20013;&#65292;ViewRefer&#21033;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;&#20363;&#22914;GPT&#65289;&#30340;&#22810;&#26679;&#21270;&#35821;&#35328;&#30693;&#35782;&#65292;&#23558;&#21333;&#19968;&#30340;&#23450;&#20301;&#25991;&#26412;&#25193;&#23637;&#20026;&#22810;&#20010;&#20960;&#20309;&#19968;&#33268;&#30340;&#25551;&#36848;&#65307;&#21516;&#26102;&#65292;&#22312;3D&#27169;&#24577;&#20013;&#65292;&#24341;&#20837;&#20102;&#22522;&#20110;Transformer&#30340;&#34701;&#21512;&#27169;&#22359;&#21644;&#35270;&#22270;&#38388;&#27880;&#24847;&#21147;&#65292;&#20197;&#22686;&#24378;&#35270;&#22270;&#20043;&#38388;&#29289;&#20307;&#30340;&#20132;&#20114;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#32452;&#21487;&#23398;&#20064;&#30340;&#22810;&#35270;&#35282;&#21407;&#22411;&#65292;&#29992;&#20110;&#35760;&#24518;&#19981;&#21516;&#35270;&#35282;&#19979;&#30340;&#22330;&#26223;&#26080;&#20851;&#30693;&#35782;&#65292;&#20174;&#20004;&#20010;&#26041;&#38754;&#22686;&#24378;&#20102;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding 3D scenes from multi-view inputs has been proven to alleviate the view discrepancy issue in 3D visual grounding. However, existing methods normally neglect the view cues embedded in the text modality and fail to weigh the relative importance of different views. In this paper, we propose ViewRefer, a multi-view framework for 3D visual grounding exploring how to grasp the view knowledge from both text and 3D modalities. For the text branch, ViewRefer leverages the diverse linguistic knowledge of large-scale language models, e.g., GPT, to expand a single grounding text to multiple geometry-consistent descriptions. Meanwhile, in the 3D modality, a transformer fusion module with inter-view attention is introduced to boost the interaction of objects across views. On top of that, we further present a set of learnable multi-view prototypes, which memorize scene-agnostic knowledge for different views, and enhance the framework from two perspectives: a view-guided attention module 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#33258;&#21160;&#25366;&#25496; Empirical AI Research &#39046;&#22495;&#25490;&#34892;&#27036;&#36825;&#19968;&#20219;&#21153;&#31867;&#21035;&#20013;&#30340;&#38646;&#26679;&#26412;&#23398;&#20064;&#29616;&#35937;&#12290;&#23454;&#39564;&#27979;&#35797;&#20102;&#20808;&#21069;&#25253;&#21578;&#30340;&#26368;&#26032;&#25216;&#26415;&#27169;&#22411;&#22312;&#20854;&#26410;&#35265;&#36807;&#30340;&#25490;&#34892;&#27036;&#26631;&#31614;&#19978;&#30340;&#27867;&#21270;&#33021;&#21147;&#25110; entailment &#33021;&#21147;&#12290;&#26412;&#25991;&#21019;&#24314;&#20102;&#19968;&#20010;&#38646;&#26679;&#26412;&#26631;&#35760;&#30340;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2303.16835</link><description>&lt;p&gt;
&#38646;&#26679;&#26412; Entrailment &#29992;&#20110; Empirical AI Research &#39046;&#22495;&#25490;&#34892;&#27036;
&lt;/p&gt;
&lt;p&gt;
Zero-shot Entailment of Leaderboards for Empirical AI Research. (arXiv:2303.16835v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16835
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#33258;&#21160;&#25366;&#25496; Empirical AI Research &#39046;&#22495;&#25490;&#34892;&#27036;&#36825;&#19968;&#20219;&#21153;&#31867;&#21035;&#20013;&#30340;&#38646;&#26679;&#26412;&#23398;&#20064;&#29616;&#35937;&#12290;&#23454;&#39564;&#27979;&#35797;&#20102;&#20808;&#21069;&#25253;&#21578;&#30340;&#26368;&#26032;&#25216;&#26415;&#27169;&#22411;&#22312;&#20854;&#26410;&#35265;&#36807;&#30340;&#25490;&#34892;&#27036;&#26631;&#31614;&#19978;&#30340;&#27867;&#21270;&#33021;&#21147;&#25110; entailment &#33021;&#21147;&#12290;&#26412;&#25991;&#21019;&#24314;&#20102;&#19968;&#20010;&#38646;&#26679;&#26412;&#26631;&#35760;&#30340;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22312;&#19968;&#20010;&#29305;&#23450;&#30340;&#25991;&#26412;&#34164;&#21547;&#65288;RTE&#65289;&#20219;&#21153;&#31867;&#21035;&#20013;&#36827;&#34892;&#20102;&#38646;&#26679;&#26412;&#23398;&#20064;&#29616;&#35937;&#30340;&#22823;&#35268;&#27169;&#23454;&#35777;&#30740;&#31350;&#65292;&#21363;&#33258;&#21160;&#25366;&#25496; Empirical AI Research &#39046;&#22495;&#25490;&#34892;&#27036;&#12290;&#35813;&#39046;&#22495;&#30340;&#25490;&#34892;&#27036;&#25552;&#21462;&#20808;&#21069;&#25253;&#21578;&#30340;&#26368;&#26032;&#25216;&#26415;&#27169;&#22411;&#65292;&#22312;&#38750;&#38646;&#26679;&#26412;&#35774;&#32622;&#19979;&#34920;&#29616;&#33391;&#22909;&#65292;&#25253;&#21578;&#20102;&#39640;&#20110;90%&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#19968;&#20010;&#26680;&#24515;&#30340;&#30740;&#31350;&#38382;&#39064;&#20173;&#26410;&#34987;&#26816;&#39564;&#65306;&#36825;&#20123;&#27169;&#22411;&#30495;&#30340;&#23398;&#20064;&#20102; entailment &#21527;&#65311;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#30340;&#23454;&#39564;&#20013;&#65292;&#27979;&#35797;&#20102;&#20004;&#20010;&#20808;&#21069;&#25253;&#21578;&#30340;&#26368;&#26032;&#25216;&#26415;&#27169;&#22411;&#65292;&#22312;&#20854;&#35757;&#32451;&#36807;&#31243;&#20013;&#27809;&#26377;&#35265;&#36807;&#30340;&#25490;&#34892;&#27036;&#26631;&#31614;&#19978;&#65292;&#27979;&#35797;&#23427;&#20204;&#30340;&#27867;&#21270;&#33021;&#21147;&#25110; entailment &#33021;&#21147;&#12290;&#25105;&#20204;&#20551;&#35774;&#65292;&#22914;&#26524;&#27169;&#22411;&#23398;&#20064;&#20102; entailment&#65292;&#23427;&#20204;&#30340;&#38646;&#26679;&#26412;&#24615;&#33021;&#20063;&#21487;&#33021;&#26159;&#20013;&#31561;&#30340;&#65292;&#25110;&#32773;&#20855;&#20307;&#26469;&#35828;&#65292;&#22909;&#20110;&#38543;&#26426;&#29468;&#27979;&#12290;&#26412;&#25991;&#36890;&#36807;&#36828;&#31243;&#26631;&#27880;&#21019;&#24314;&#20102;&#38646;&#26679;&#26412;&#26631;&#35760;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a large-scale empirical investigation of the zero-shot learning phenomena in a specific recognizing textual entailment (RTE) task category, i.e. the automated mining of leaderboards for Empirical AI Research. The prior reported state-of-the-art models for leaderboards extraction formulated as an RTE task, in a non-zero-shot setting, are promising with above 90% reported performances. However, a central research question remains unexamined: did the models actually learn entailment? Thus, for the experiments in this paper, two prior reported state-of-the-art models are tested out-of-the-box for their ability to generalize or their capacity for entailment, given leaderboard labels that were unseen during training. We hypothesize that if the models learned entailment, their zero-shot performances can be expected to be moderately high as well--perhaps, concretely, better than chance. As a result of this work, a zero-shot labeled dataset is created via distant labeling formulating
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22312;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#19982;&#20854;&#20182;&#36710;&#36742;&#20132;&#20114;&#21512;&#27969;&#30340;&#24773;&#20917;&#19979;&#65292;&#22522;&#20110;&#23398;&#20064;&#30340;&#39044;&#27979;&#30340;&#20915;&#31574;&#21046;&#23450;&#26041;&#27861;&#26469;&#35299;&#20915;&#39640;&#24230;&#25361;&#25112;&#30340;&#20915;&#31574;&#38382;&#39064;&#21644;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#37319;&#29992;&#37096;&#20998;&#21487;&#35266;&#23519;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;POMDP&#65289;&#21644;&#33945;&#29305;&#21345;&#32599;&#26641;&#25628;&#32034;&#31639;&#27861;&#26469;&#25191;&#34892;&#39640;&#32423;&#39550;&#39542;&#25805;&#20316;&#12290;</title><link>http://arxiv.org/abs/2303.16821</link><description>&lt;p&gt;
&#22522;&#20110;&#23398;&#20064;&#30340;&#39044;&#27979;&#30340;&#20132;&#20114;&#21512;&#27969;&#24773;&#20917;&#19979;&#33258;&#21160;&#39550;&#39542;&#20915;&#31574;&#21046;&#23450;
&lt;/p&gt;
&lt;p&gt;
Decision Making for Autonomous Driving in Interactive Merge Scenarios via Learning-based Prediction. (arXiv:2303.16821v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16821
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22312;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#19982;&#20854;&#20182;&#36710;&#36742;&#20132;&#20114;&#21512;&#27969;&#30340;&#24773;&#20917;&#19979;&#65292;&#22522;&#20110;&#23398;&#20064;&#30340;&#39044;&#27979;&#30340;&#20915;&#31574;&#21046;&#23450;&#26041;&#27861;&#26469;&#35299;&#20915;&#39640;&#24230;&#25361;&#25112;&#30340;&#20915;&#31574;&#38382;&#39064;&#21644;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#37319;&#29992;&#37096;&#20998;&#21487;&#35266;&#23519;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;POMDP&#65289;&#21644;&#33945;&#29305;&#21345;&#32599;&#26641;&#25628;&#32034;&#31639;&#27861;&#26469;&#25191;&#34892;&#39640;&#32423;&#39550;&#39542;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19982;&#20154;&#31867;&#39550;&#39542;&#21592;&#20849;&#20139;&#36947;&#36335;&#30340;&#33258;&#21160;&#20195;&#29702;&#26041;&#38754;&#65292;&#24517;&#39035;&#32771;&#34385;&#20132;&#36890;&#21442;&#19982;&#32773;&#20043;&#38388;&#24494;&#22937;&#30340;&#20114;&#21160;&#12290;&#36825;&#26159;&#19968;&#20010;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20915;&#31574;&#38382;&#39064;&#65292;&#22240;&#20026;&#20154;&#31867;&#34892;&#20026;&#21463;&#21040;&#38590;&#20197;&#24314;&#27169;&#30340;&#22810;&#31181;&#22240;&#32032;&#65288;&#20363;&#22914;&#20154;&#31867;&#24847;&#22270;&#21644;&#24773;&#32490;&#65289;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#39550;&#39542;&#30340;&#20915;&#31574;&#26041;&#27861;&#65292;&#37325;&#28857;&#20851;&#27880;&#22797;&#26434;&#30340;&#21512;&#27969;&#20132;&#36890;&#20219;&#21153;&#65292;&#20854;&#20013;&#19981;&#30830;&#23450;&#24615;&#26469;&#33258;&#20854;&#20182;&#39550;&#39542;&#21592;&#30340;&#34892;&#20026;&#21644;&#19981;&#23436;&#32654;&#30340;&#20256;&#24863;&#22120;&#27979;&#37327;&#12290;&#25105;&#20204;&#23558;&#38382;&#39064;&#26694;&#26550;&#21270;&#20026;&#37096;&#20998;&#21487;&#35266;&#23519;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;POMDP&#65289;&#65292;&#24182;&#20351;&#29992;&#33945;&#29305;&#21345;&#32599;&#26641;&#25628;&#32034;&#22312;&#32447;&#27714;&#35299;&#12290; POMDP&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#25191;&#34892;&#39640;&#32423;&#39550;&#39542;&#25805;&#20316;&#30340;&#31574;&#30053;&#65292;&#20363;&#22914;&#35753;&#36947;&#32473;&#36924;&#36817;&#30340;&#36710;&#36742;&#65292;&#19982;&#21069;&#38754;&#30340;&#36710;&#36742;&#20445;&#25345;&#23433;&#20840;&#36317;&#31163;&#25110;&#21512;&#24182;&#21040;&#20132;&#36890;&#20013;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#30340;&#27169;&#22411;&#26469;&#39044;&#27979;&#26410;&#26469;&#30340;&#20132;&#36890;&#29366;&#24577;&#65292;&#21516;&#26102;&#26126;&#30830;&#32771;&#34385;&#20132;&#20114;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autonomous agents that drive on roads shared with human drivers must reason about the nuanced interactions among traffic participants. This poses a highly challenging decision making problem since human behavior is influenced by a multitude of factors (e.g., human intentions and emotions) that are hard to model. This paper presents a decision making approach for autonomous driving, focusing on the complex task of merging into moving traffic where uncertainty emanates from the behavior of other drivers and imperfect sensor measurements. We frame the problem as a partially observable Markov decision process (POMDP) and solve it online with Monte Carlo tree search. The solution to the POMDP is a policy that performs high-level driving maneuvers, such as giving way to an approaching car, keeping a safe distance from the vehicle in front or merging into traffic. Our method leverages a model learned from data to predict the future states of traffic while explicitly accounting for interaction
&lt;/p&gt;</description></item><item><title>Thistle&#26159;&#19968;&#20010;&#23436;&#20840;&#21151;&#33021;&#30340;&#21521;&#37327;&#25968;&#25454;&#24211;&#65292;&#26088;&#22312;&#35299;&#20915;&#22238;&#31572;&#25628;&#32034;&#26597;&#35810;&#20013;&#30340;&#28508;&#22312;&#30693;&#35782;&#39046;&#22495;&#38382;&#39064;&#65292;&#24050;&#32463;&#22312;MS MARCO&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#19988;&#26377;&#21161;&#20110;&#25512;&#36827;Rust ML&#29983;&#24577;&#31995;&#32479;&#30340;&#21457;&#23637;&#12290;</title><link>http://arxiv.org/abs/2303.16780</link><description>&lt;p&gt;
Thistle: Rust&#20013;&#30340;&#21521;&#37327;&#25968;&#25454;&#24211;
&lt;/p&gt;
&lt;p&gt;
Thistle: A Vector Database in Rust. (arXiv:2303.16780v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16780
&lt;/p&gt;
&lt;p&gt;
Thistle&#26159;&#19968;&#20010;&#23436;&#20840;&#21151;&#33021;&#30340;&#21521;&#37327;&#25968;&#25454;&#24211;&#65292;&#26088;&#22312;&#35299;&#20915;&#22238;&#31572;&#25628;&#32034;&#26597;&#35810;&#20013;&#30340;&#28508;&#22312;&#30693;&#35782;&#39046;&#22495;&#38382;&#39064;&#65292;&#24050;&#32463;&#22312;MS MARCO&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#19988;&#26377;&#21161;&#20110;&#25512;&#36827;Rust ML&#29983;&#24577;&#31995;&#32479;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;Thistle&#65292;&#19968;&#20010;&#23436;&#20840;&#21151;&#33021;&#30340;&#21521;&#37327;&#25968;&#25454;&#24211;&#12290;Thistle&#26159;Latent Knowledge Use&#22312;&#22238;&#31572;&#25628;&#32034;&#26597;&#35810;&#26041;&#38754;&#30340;&#20998;&#25903;&#65292;&#36825;&#26159;&#21021;&#21019;&#20844;&#21496;&#21644;&#25628;&#32034;&#24341;&#25806;&#20844;&#21496;&#30340;&#25345;&#32493;&#30740;&#31350;&#35838;&#39064;&#12290;&#25105;&#20204;&#20351;&#29992;&#25968;&#20010;&#33879;&#21517;&#31639;&#27861;&#23454;&#29616;Thistle&#65292;&#24182;&#22312;MS MARCO&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#12290;&#32467;&#26524;&#26377;&#21161;&#20110;&#28548;&#28165;&#28508;&#22312;&#30693;&#35782;&#39046;&#22495;&#20197;&#21450;&#19981;&#26029;&#22686;&#38271;&#30340;Rust ML&#29983;&#24577;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Thistle, a fully functional vector database. Thistle is an entry into the domain of latent knowledge use in answering search queries, an ongoing research topic at both start-ups and search engine companies. We implement Thistle with several well-known algorithms, and benchmark results on the MS MARCO dataset. Results help clarify the latent knowledge domain as well as the growing Rust ML ecosystem.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#39046;&#22495;&#19987;&#23478;&#30340;&#30693;&#35782;&#21644;&#20027;&#21160;&#23398;&#20064;&#25216;&#26415;&#21576;&#29616;&#20102;&#19968;&#20010;&#20004;&#30334;&#19975;&#20221;&#28921;&#39274;&#39135;&#35889;&#26032;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#23558;30&#19975;&#20221;&#39135;&#35889;&#25353;&#29031;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#36827;&#34892;&#20998;&#31867;&#21040;9&#20010;&#31867;&#21035;&#20013;&#65292;&#28982;&#21518;&#20351;&#29992;&#28151;&#21512;&#26041;&#27861;&#23545;&#20854;&#20313;&#30340;1900K&#20221;&#39135;&#35889;&#36827;&#34892;&#20998;&#31867;&#12290;</title><link>http://arxiv.org/abs/2303.16778</link><description>&lt;p&gt;
&#22522;&#20110;&#20027;&#21160;&#23398;&#20064;&#31574;&#30053;&#30340;&#20004; &#30334;&#19975;&#20221;&#26631;&#35760;&#32654;&#39135;&#39135;&#35889;&#25968;&#25454;&#38598; - 3A2M
&lt;/p&gt;
&lt;p&gt;
Assorted, Archetypal and Annotated Two Million (3A2M) Cooking Recipes Dataset based on Active Learning. (arXiv:2303.16778v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16778
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#39046;&#22495;&#19987;&#23478;&#30340;&#30693;&#35782;&#21644;&#20027;&#21160;&#23398;&#20064;&#25216;&#26415;&#21576;&#29616;&#20102;&#19968;&#20010;&#20004;&#30334;&#19975;&#20221;&#28921;&#39274;&#39135;&#35889;&#26032;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#23558;30&#19975;&#20221;&#39135;&#35889;&#25353;&#29031;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#36827;&#34892;&#20998;&#31867;&#21040;9&#20010;&#31867;&#21035;&#20013;&#65292;&#28982;&#21518;&#20351;&#29992;&#28151;&#21512;&#26041;&#27861;&#23545;&#20854;&#20313;&#30340;1900K&#20221;&#39135;&#35889;&#36827;&#34892;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28921;&#39274;&#39135;&#35889;&#21487;&#20197;&#20132;&#25442;&#28921;&#39274;&#24605;&#24819;&#65292;&#24182;&#25552;&#20379;&#39135;&#21697;&#30340;&#21046;&#20316;&#35828;&#26126;&#12290;&#28982;&#32780;&#65292;&#22312;&#35813;&#39046;&#22495;&#20869;&#30001;&#20110;&#32570;&#20047;&#36275;&#22815;&#30340;&#26631;&#35760;&#25968;&#25454;&#65292;&#23558;&#22312;&#32447;&#25214;&#21040;&#30340;&#21407;&#22987;&#39135;&#35889;&#20998;&#31867;&#21040;&#21512;&#36866;&#30340;&#39135;&#21697;&#31867;&#22411;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#26412;&#30740;&#31350;&#21033;&#29992;&#39046;&#22495;&#19987;&#23478;&#30340;&#30693;&#35782;&#23558;&#39135;&#35889;&#20998;&#31867;&#21487;&#33021;&#26159;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#22522;&#20110;&#19968;&#20010;&#20027;&#21160;&#23398;&#20064;&#25216;&#26415;&#21576;&#29616;&#20102;&#19968;&#20010;&#20004;&#30334;&#19975;&#20221;&#28921;&#39274;&#39135;&#35889;&#30340;&#26032;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#21033;&#29992;&#39046;&#22495;&#19987;&#23478;&#30340;&#30693;&#35782;&#23558;&#20854;&#26631;&#35760;&#22312;&#21508;&#33258;&#30340;&#31867;&#21035;&#20013;&#12290;&#20026;&#20102;&#26500;&#24314;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#20174;RecipeNLG&#25968;&#25454;&#38598;&#20013;&#33719;&#21462;&#39135;&#35889;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#19977;&#20010;&#21487;&#20449;&#24230;&#24471;&#20998;&#39640;&#20110;86.667&#65285;&#30340;&#20154;&#31867;&#19987;&#23478;&#25353;&#29031;&#20854;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#23558;30&#19975;&#20221;&#39135;&#35889;&#20998;&#31867;&#21040;&#20061;&#20010;&#31867;&#21035;&#20043;&#19968;&#65306;&#28888;&#28953;&#12289;&#39278;&#26009;&#12289;&#33636;&#33756;&#12289;&#34092;&#33756;&#12289;&#24555;&#39184;&#12289;&#40614;&#29255;&#12289;&#39184;&#28857;&#12289;&#37197;&#33756;&#21644;&#34701;&#21512;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;Query-by-Committee&#21644;Human&#30340;&#28151;&#21512;&#26041;&#27861;&#65292;&#23558;&#21097;&#20313;&#30340;1900K&#20221;&#39135;&#35889;&#36827;&#34892;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cooking recipes allow individuals to exchange culinary ideas and provide food preparation instructions. Due to a lack of adequate labeled data, categorizing raw recipes found online to the appropriate food genres is a challenging task in this domain. Utilizing the knowledge of domain experts to categorize recipes could be a solution. In this study, we present a novel dataset of two million culinary recipes labeled in respective categories leveraging the knowledge of food experts and an active learning technique. To construct the dataset, we collect the recipes from the RecipeNLG dataset. Then, we employ three human experts whose trustworthiness score is higher than 86.667% to categorize 300K recipe by their Named Entity Recognition (NER) and assign it to one of the nine categories: bakery, drinks, non-veg, vegetables, fast food, cereals, meals, sides and fusion. Finally, we categorize the remaining 1900K recipes using Active Learning method with a blend of Query-by-Committee and Human 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#27979;&#37327;&#19987;&#21033;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65292;&#21516;&#26102;&#32771;&#34385;&#35821;&#20041;&#21644;&#25216;&#26415;&#30456;&#20284;&#24615;&#65292;&#24182;&#19988;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#20248;&#20110;&#20165;&#32771;&#34385;&#35821;&#20041;&#30456;&#20284;&#24615;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.16767</link><description>&lt;p&gt;
&#19968;&#31181;&#26032;&#30340;&#19987;&#21033;&#30456;&#20284;&#24230;&#27979;&#37327;&#26041;&#27861;&#65306;&#35821;&#20041;&#36317;&#31163;&#21644;&#25216;&#26415;&#36317;&#31163;
&lt;/p&gt;
&lt;p&gt;
A Novel Patent Similarity Measurement Methodology: Semantic Distance and Technological Distance. (arXiv:2303.16767v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16767
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#27979;&#37327;&#19987;&#21033;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65292;&#21516;&#26102;&#32771;&#34385;&#35821;&#20041;&#21644;&#25216;&#26415;&#30456;&#20284;&#24615;&#65292;&#24182;&#19988;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#20248;&#20110;&#20165;&#32771;&#34385;&#35821;&#20041;&#30456;&#20284;&#24615;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27979;&#37327;&#19987;&#21033;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#26159;&#30830;&#20445;&#21019;&#26032;&#30340;&#26032;&#39062;&#24615;&#30340;&#20851;&#38190;&#27493;&#39588;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#22823;&#22810;&#25968;&#19987;&#21033;&#30456;&#20284;&#24230;&#27979;&#37327;&#26041;&#27861;&#20173;&#28982;&#20381;&#36182;&#20110;&#19987;&#23478;&#25163;&#21160;&#20998;&#31867;&#19987;&#21033;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#19968;&#20123;&#30740;&#31350;&#25552;&#20986;&#20102;&#33258;&#21160;&#21270;&#26041;&#27861;&#65307;&#28982;&#32780;&#65292;&#22823;&#37096;&#20998;&#33258;&#21160;&#21270;&#26041;&#27861;&#21482;&#20851;&#27880;&#19987;&#21033;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#27979;&#37327;&#19987;&#21033;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65292;&#21516;&#26102;&#32771;&#34385;&#35821;&#20041;&#21644;&#25216;&#26415;&#30340;&#30456;&#20284;&#24615;&#12290;&#25105;&#20204;&#22522;&#20110;&#19987;&#21033;&#25991;&#26412;&#20351;&#29992;BERT&#27979;&#37327;&#35821;&#20041;&#30456;&#20284;&#24615;&#65292;&#20351;&#29992;Jaccard&#30456;&#20284;&#24615;&#35745;&#31639;&#19987;&#21033;&#30340;&#25216;&#26415;&#30456;&#20284;&#24615;&#65292;&#24182;&#36890;&#36807;&#20998;&#37197;&#26435;&#37325;&#26469;&#23454;&#29616;&#28151;&#21512;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20248;&#20110;&#20165;&#32771;&#34385;&#35821;&#20041;&#30456;&#20284;&#24230;&#30340;&#22522;&#20934;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Measuring similarity between patents is an essential step to ensure novelty of innovation. However, a large number of methods of measuring the similarity between patents still rely on manual classification of patents by experts. Another body of research has proposed automated methods; nevertheless, most of it solely focuses on the semantic similarity of patents. In order to tackle these limitations, we propose a hybrid method for automatically measuring the similarity between patents, considering both semantic and technological similarities. We measure the semantic similarity based on patent texts using BERT, calculate the technological similarity with IPC codes using Jaccard similarity, and perform hybridization by assigning weights to the two similarity methods. Our evaluation result demonstrates that the proposed method outperforms the baseline that considers the semantic similarity only.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22522;&#20110;&#38750;&#20020;&#24202;&#21644;&#20813;&#36153;&#21487;&#29992;&#30340;&#20449;&#24687;&#65292;&#32467;&#21512;&#20998;&#24067;&#24335;&#35745;&#31639;&#12289;&#25991;&#26412;&#26816;&#32034;&#12289;&#32858;&#31867;&#21644;&#20998;&#31867;&#26041;&#27861;&#24320;&#21457;&#20102;&#19968;&#20010;&#33021;&#22815;&#26816;&#32034;&#12289;&#32858;&#31867;&#21644;&#23637;&#31034;&#20851;&#20110;&#30284;&#30151;&#30149;&#31243;&#20449;&#24687;&#30340;&#35745;&#31639;&#26377;&#25928;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2303.16766</link><description>&lt;p&gt;
&#29992;&#38750;&#20020;&#24202;&#25991;&#26412;&#20449;&#24687;&#26816;&#32034;&#23454;&#29616;&#32959;&#30244;&#30456;&#20851;&#35770;&#22363;&#24086;&#23376;&#30340;&#35745;&#31639;&#26377;&#25928;&#26631;&#35760;
&lt;/p&gt;
&lt;p&gt;
Computationally Efficient Labeling of Cancer Related Forum Posts by Non-Clinical Text Information Retrieval. (arXiv:2303.16766v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16766
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22522;&#20110;&#38750;&#20020;&#24202;&#21644;&#20813;&#36153;&#21487;&#29992;&#30340;&#20449;&#24687;&#65292;&#32467;&#21512;&#20998;&#24067;&#24335;&#35745;&#31639;&#12289;&#25991;&#26412;&#26816;&#32034;&#12289;&#32858;&#31867;&#21644;&#20998;&#31867;&#26041;&#27861;&#24320;&#21457;&#20102;&#19968;&#20010;&#33021;&#22815;&#26816;&#32034;&#12289;&#32858;&#31867;&#21644;&#23637;&#31034;&#20851;&#20110;&#30284;&#30151;&#30149;&#31243;&#20449;&#24687;&#30340;&#35745;&#31639;&#26377;&#25928;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#19978;&#23384;&#22312;&#30528;&#22823;&#37327;&#20851;&#20110;&#30284;&#30151;&#30340;&#20449;&#24687;&#65292;&#20294;&#20998;&#31867;&#21644;&#25552;&#21462;&#26377;&#29992;&#20449;&#24687;&#24456;&#22256;&#38590;&#12290;&#20960;&#20046;&#25152;&#26377;&#30340;&#21307;&#30103;&#20445;&#20581;&#25968;&#25454;&#22788;&#29702;&#30740;&#31350;&#37117;&#28041;&#21450;&#27491;&#24335;&#30340;&#20020;&#24202;&#25968;&#25454;&#65292;&#20294;&#38750;&#20020;&#24202;&#25968;&#25454;&#20013;&#20063;&#26377;&#26377;&#20215;&#20540;&#30340;&#20449;&#24687;&#12290;&#26412;&#30740;&#31350;&#23558;&#20998;&#24067;&#24335;&#35745;&#31639;&#12289;&#25991;&#26412;&#26816;&#32034;&#12289;&#32858;&#31867;&#21644;&#20998;&#31867;&#26041;&#27861;&#32467;&#21512;&#25104;&#19968;&#20010;&#36830;&#36143;&#12289;&#35745;&#31639;&#26377;&#25928;&#30340;&#31995;&#32479;&#65292;&#22522;&#20110;&#38750;&#20020;&#24202;&#21644;&#20813;&#36153;&#21487;&#29992;&#30340;&#20449;&#24687;&#65292;&#21487;&#20197;&#28548;&#28165;&#30284;&#30151;&#24739;&#32773;&#30340;&#30149;&#31243;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#23436;&#20840;&#21151;&#33021;&#30340;&#21407;&#22411;&#65292;&#21487;&#20197;&#20174;&#38750;&#20020;&#24202;&#35770;&#22363;&#24086;&#23376;&#20013;&#26816;&#32034;&#12289;&#32858;&#31867;&#21644;&#23637;&#31034;&#20851;&#20110;&#30284;&#30151;&#30149;&#31243;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#19977;&#31181;&#32858;&#31867;&#31639;&#27861;&#65288;MR-DBSCAN&#12289;DBSCAN&#21644;HDBSCAN&#65289;&#65292;&#24182;&#27604;&#36739;&#20102;&#23427;&#20204;&#22312;&#35843;&#25972;&#21518;&#30340;&#20848;&#24503;&#25351;&#25968;&#21644;&#24635;&#36816;&#34892;&#26102;&#38388;&#26041;&#38754;&#30340;&#34920;&#29616;&#65292;&#20316;&#20026;&#26816;&#32034;&#30340;&#24086;&#23376;&#25968;&#37327;&#21644;&#37051;&#22495;&#21322;&#24452;&#20989;&#25968;&#12290;&#32858;&#31867;&#32467;&#26524;&#26174;&#31034;&#65292;&#37051;&#22495;&#21322;&#24452;&#23545;&#32858;&#31867;&#32467;&#26524;&#26377;&#26368;&#26174;&#33879;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
An abundance of information about cancer exists online, but categorizing and extracting useful information from it is difficult. Almost all research within healthcare data processing is concerned with formal clinical data, but there is valuable information in non-clinical data too. The present study combines methods within distributed computing, text retrieval, clustering, and classification into a coherent and computationally efficient system, that can clarify cancer patient trajectories based on non-clinical and freely available information. We produce a fully-functional prototype that can retrieve, cluster and present information about cancer trajectories from non-clinical forum posts. We evaluate three clustering algorithms (MR-DBSCAN, DBSCAN, and HDBSCAN) and compare them in terms of Adjusted Rand Index and total run time as a function of the number of posts retrieved and the neighborhood radius. Clustering results show that neighborhood radius has the most significant impact on c
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#35805;&#30340;&#35270;&#39057;&#26816;&#32034;&#31995;&#32479;&#65292;&#20351;&#29992;&#23545;&#35805;&#20316;&#20026;&#25628;&#32034;&#25551;&#36848;&#31526;&#65292;&#26377;&#25928;&#22320;&#25552;&#39640;&#20102;&#35270;&#39057;&#26816;&#32034;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.16761</link><description>&lt;p&gt;
&#22522;&#20110;&#23545;&#35805;&#30340;&#35270;&#39057;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Dialogue-to-Video Retrieval. (arXiv:2303.16761v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16761
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#35805;&#30340;&#35270;&#39057;&#26816;&#32034;&#31995;&#32479;&#65292;&#20351;&#29992;&#23545;&#35805;&#20316;&#20026;&#25628;&#32034;&#25551;&#36848;&#31526;&#65292;&#26377;&#25928;&#22320;&#25552;&#39640;&#20102;&#35270;&#39057;&#26816;&#32034;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22312;&#31038;&#20132;&#23186;&#20307;&#31561;&#32593;&#32476;&#24179;&#21488;&#19978;&#65292;&#20154;&#20204;&#36827;&#34892;&#30528;&#36234;&#26469;&#36234;&#22810;&#30340;&#23545;&#35805;&#12290;&#36825;&#21551;&#21457;&#20102;&#22522;&#20110;&#23545;&#35805;&#30340;&#26816;&#32034;&#30340;&#21457;&#23637;&#65292;&#20854;&#20013;&#22522;&#20110;&#23545;&#35805;&#30340;&#35270;&#39057;&#26816;&#32034;&#23545;&#20110;&#25512;&#33616;&#31995;&#32479;&#20855;&#26377;&#36234;&#26469;&#36234;&#22823;&#30340;&#20852;&#36259;&#12290;&#19981;&#21516;&#20110;&#20854;&#20182;&#35270;&#39057;&#26816;&#32034;&#20219;&#21153;&#65292;&#23545;&#35805;&#21040;&#35270;&#39057;&#26816;&#32034;&#20351;&#29992;&#20197;&#29992;&#25143;&#29983;&#25104;&#30340;&#23545;&#35805;&#20026;&#25628;&#32034;&#25551;&#36848;&#31526;&#30340;&#32467;&#26500;&#21270;&#26597;&#35810;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#22522;&#20110;&#23545;&#35805;&#30340;&#35270;&#39057;&#26816;&#32034;&#31995;&#32479;&#65292;&#34701;&#21512;&#20102;&#32467;&#26500;&#21270;&#30340;&#23545;&#35805;&#20449;&#24687;&#12290;&#22312;AVSD&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#20351;&#29992;&#32431;&#25991;&#26412;&#26597;&#35810;&#30340;&#26041;&#27861;&#22312;R@1&#19978;&#27604;&#20197;&#21069;&#30340;&#27169;&#22411;&#25552;&#39640;&#20102;15.8%&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;&#23545;&#35805;&#20316;&#20026;&#26597;&#35810;&#30340;&#26041;&#27861;&#65292;&#22312;R@1&#12289;R@5&#21644;R@10&#19978;&#20998;&#21035;&#25552;&#39640;&#20102;4.2%&#12289;6.2%&#21644;8.6%&#65292;&#22312;R@1&#12289;R@5&#21644;R@10&#19978;&#20998;&#21035;&#27604;&#22522;&#20934;&#27169;&#22411;&#25552;&#39640;&#20102;0.7%&#12289;3.6%&#21644;6.0%&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent years have witnessed an increasing amount of dialogue/conversation on the web especially on social media. That inspires the development of dialogue-based retrieval, in which retrieving videos based on dialogue is of increasing interest for recommendation systems. Different from other video retrieval tasks, dialogue-to-video retrieval uses structured queries in the form of user-generated dialogue as the search descriptor. We present a novel dialogue-to-video retrieval system, incorporating structured conversational information. Experiments conducted on the AVSD dataset show that our proposed approach using plain-text queries improves over the previous counterpart model by 15.8% on R@1. Furthermore, our approach using dialogue as a query, improves retrieval performance by 4.2%, 6.2%, 8.6% on R@1, R@5 and R@10 and outperforms the state-of-the-art model by 0.7%, 3.6% and 6.0% on R@1, R@5 and R@10 respectively.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#34433;&#32676;&#20248;&#21270;&#30340;&#39640;&#25928;&#35789;&#24615;&#26631;&#27880;&#26041;&#27861;ACO-tagger&#65292;&#23454;&#29616;&#20102;&#39640;&#36798;96.867&#65285;&#30340;&#20934;&#30830;&#29575;&#65292;&#20248;&#20110;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.16760</link><description>&lt;p&gt;
&#20351;&#29992;&#34433;&#32676;&#20248;&#21270;&#30340;&#26032;&#22411;&#35789;&#24615;&#26631;&#27880;&#26041;&#27861;ACO-tagger
&lt;/p&gt;
&lt;p&gt;
ACO-tagger: A Novel Method for Part-of-Speech Tagging using Ant Colony Optimization. (arXiv:2303.16760v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16760
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#34433;&#32676;&#20248;&#21270;&#30340;&#39640;&#25928;&#35789;&#24615;&#26631;&#27880;&#26041;&#27861;ACO-tagger&#65292;&#23454;&#29616;&#20102;&#39640;&#36798;96.867&#65285;&#30340;&#20934;&#30830;&#29575;&#65292;&#20248;&#20110;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#32676;&#26234;&#33021;&#31639;&#27861;&#22240;&#20854;&#35299;&#20915;&#22797;&#26434;&#21644;&#38750;&#30830;&#23450;&#24615;&#38382;&#39064;&#30340;&#33021;&#21147;&#32780;&#22791;&#21463;&#20851;&#27880;&#12290;&#36825;&#20123;&#31639;&#27861;&#21463;&#33258;&#28982;&#29983;&#29289;&#30340;&#38598;&#20307;&#34892;&#20026;&#21551;&#21457;&#65292;&#27169;&#25311;&#36825;&#31181;&#34892;&#20026;&#20197;&#24320;&#21457;&#29992;&#20110;&#35745;&#31639;&#20219;&#21153;&#30340;&#26234;&#33021; agent&#12290;&#20854;&#20013;&#19968;&#31181;&#31639;&#27861;&#26159;&#21463;&#21040;&#34434;&#34433;&#35269;&#39135;&#34892;&#20026;&#21450;&#20854;&#20449;&#24687;&#32032;&#37322;&#25918;&#26426;&#21046;&#21551;&#21457;&#30340;&#34433;&#32676;&#20248;&#21270;&#65288;ACO&#65289;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#31163;&#25955;&#21644;&#32452;&#21512;&#24615;&#30340;&#22256;&#38590;&#38382;&#39064;&#12290;&#35789;&#24615;&#26631;&#27880;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#22522;&#30784;&#20219;&#21153;&#65292;&#26088;&#22312;&#20026;&#21477;&#23376;&#20013;&#30340;&#27599;&#20010;&#21333;&#35789;&#20998;&#37197;&#19968;&#20010;&#35789;&#24615;&#35282;&#33394;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;ACO&#30340;&#39640;&#24615;&#33021;&#35789;&#24615;&#26631;&#27880;&#26041;&#27861;ACO-tagger&#12290;&#35813;&#26041;&#27861;&#23454;&#29616;&#20102;&#39640;&#36798;96.867&#65285;&#30340;&#20934;&#30830;&#29575;&#65292;&#20248;&#20110;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#24555;&#36895;&#39640;&#25928;&#65292;&#26159;&#23454;&#38469;&#24212;&#29992;&#30340;&#21487;&#34892;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;
Swarm Intelligence algorithms have gained significant attention in recent years as a means of solving complex and non-deterministic problems. These algorithms are inspired by the collective behavior of natural creatures, and they simulate this behavior to develop intelligent agents for computational tasks. One such algorithm is Ant Colony Optimization (ACO), which is inspired by the foraging behavior of ants and their pheromone laying mechanism. ACO is used for solving difficult problems that are discrete and combinatorial in nature. Part-of-Speech (POS) tagging is a fundamental task in natural language processing that aims to assign a part-of-speech role to each word in a sentence. In this research paper, proposed a high-performance POS-tagging method based on ACO called ACO-tagger. This method achieved a high accuracy rate of 96.867%, outperforming several state-of-the-art methods. The proposed method is fast and efficient, making it a viable option for practical applications.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#20174;&#30005;&#23376;&#30149;&#21382;&#20013;&#26816;&#32034;&#28431;&#35786;&#30340;&#39069;&#22806;&#35786;&#26029;&#65292;&#20197;&#36866;&#29992;&#20110;DRG&#20998;&#32452;&#65292;&#35299;&#20915;&#20102;&#28431;&#35786;&#38382;&#39064;&#23548;&#33268;&#21307;&#30103;&#35760;&#24405;&#19981;&#23436;&#25972;&#12289;&#24433;&#21709;DRG&#25307;&#29983;&#27491;&#30830;&#29575;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.16757</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22914;&#20309;&#20174;&#30005;&#23376;&#30149;&#21382;&#20013;&#26816;&#27979;&#28431;&#35786;&#30340;&#39069;&#22806;&#35786;&#26029;&#20197;&#36866;&#29992;&#20110;DRG
&lt;/p&gt;
&lt;p&gt;
How can Deep Learning Retrieve the Write-Missing Additional Diagnosis from Chinese Electronic Medical Record For DRG. (arXiv:2303.16757v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16757
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#20174;&#30005;&#23376;&#30149;&#21382;&#20013;&#26816;&#32034;&#28431;&#35786;&#30340;&#39069;&#22806;&#35786;&#26029;&#65292;&#20197;&#36866;&#29992;&#20110;DRG&#20998;&#32452;&#65292;&#35299;&#20915;&#20102;&#28431;&#35786;&#38382;&#39064;&#23548;&#33268;&#21307;&#30103;&#35760;&#24405;&#19981;&#23436;&#25972;&#12289;&#24433;&#21709;DRG&#25307;&#29983;&#27491;&#30830;&#29575;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28431;&#35786;&#30340;&#26816;&#27979;&#30340;&#30446;&#30340;&#26159;&#25214;&#21040;&#24050;&#32463;&#22312;&#21307;&#30103;&#35760;&#24405;&#20013;&#28165;&#26224;&#35786;&#26029;&#20294;&#34987;&#28431;&#25481;&#30340;&#30142;&#30149;&#12290;&#19981;&#21516;&#20110;&#28431;&#35786;&#30340;&#23450;&#20041;&#65292;&#28431;&#35786;&#22312;&#21307;&#30103;&#35760;&#24405;&#20013;&#26126;&#26174;&#34920;&#29616;&#65292;&#26080;&#39035;&#36827;&#19968;&#27493;&#25512;&#29702;&#12290;&#28431;&#35786;&#38382;&#39064;&#24456;&#24120;&#35265;&#65292;&#36890;&#24120;&#26159;&#30001;&#20110;&#21307;&#29983;&#30095;&#24573;&#36896;&#25104;&#30340;&#12290;&#28431;&#35786;&#20250;&#23548;&#33268;&#21307;&#30103;&#35760;&#24405;&#30340;&#19981;&#23436;&#25972;&#24615;&#12290;&#22312;DRG&#30340;&#20998;&#32452;&#19979;&#65292;&#28431;&#35786;&#23558;&#38169;&#36807;&#37325;&#35201;&#30340;&#39069;&#22806;&#35786;&#26029;&#65288;CC&#65292;MCC&#65289;&#65292;&#20174;&#32780;&#24433;&#21709;DRG&#25307;&#29983;&#30340;&#27491;&#30830;&#29575;&#12290;&#22312;&#22269;&#23478;&#26222;&#36941;&#24320;&#22987;&#37319;&#29992;DRG&#25307;&#29983;&#21644;&#25903;&#20184;&#30340;&#24773;&#20917;&#19979;&#65292;&#28431;&#35786;&#38382;&#39064;&#26159;&#26222;&#36941;&#23384;&#22312;&#30340;&#20005;&#37325;&#38382;&#39064;&#12290;&#24403;&#21069;&#30340;&#22522;&#20110;&#25163;&#21160;&#26041;&#27861;&#30001;&#20110;&#20840;&#38754;&#21307;&#30103;&#35760;&#24405;&#30340;&#22797;&#26434;&#20869;&#23481;&#32780;&#26114;&#36149;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#35748;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21487;&#20197;&#29992;&#20110;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#20294;&#25454;&#25105;&#25152;&#30693;&#65292;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#20174;&#20013;&#22269;&#30005;&#23376;&#30149;&#21382;&#20013;&#26816;&#32034;&#28431;&#35786;&#30340;&#39069;&#22806;&#35786;&#26029;&#65292;&#20197;&#36866;&#29992;&#20110;DRG&#20998;&#32452;&#12290;
&lt;/p&gt;
&lt;p&gt;
The purpose of write-missing diagnosis detection is to find diseases that have been clearly diagnosed from medical records but are missed in the discharge diagnosis. Unlike the definition of missed diagnosis, the write-missing diagnosis is clearly manifested in the medical record without further reasoning. The write-missing diagnosis is a common problem, often caused by physician negligence. The write-missing diagnosis will result in an incomplete diagnosis of medical records. While under DRG grouping, the write-missing diagnoses will miss important additional diagnoses (CC, MCC), thus affecting the correct rate of DRG enrollment.  Under the circumstance that countries generally start to adopt DRG enrollment and payment, the problem of write-missing diagnosis is a common and serious problem. The current manual-based method is expensive due to the complex content of the full medical record. We think this problem is suitable to be solved as natural language processing. But to the best of
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38544;&#31169;&#24863;&#30693;&#25968;&#25454;&#22686;&#24378;&#30340;LLM-PTM&#26041;&#27861;&#65292;&#26377;&#25928;&#22320;&#25552;&#39640;&#20102;&#24739;&#32773;-&#35797;&#39564;&#21305;&#37197;&#30340;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.16756</link><description>&lt;p&gt;
LLM&#29992;&#20110;&#24739;&#32773;-&#35797;&#39564;&#21305;&#37197;: &#38754;&#21521;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#30340;&#38544;&#31169;&#24863;&#30693;&#25968;&#25454;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
LLM for Patient-Trial Matching: Privacy-Aware Data Augmentation Towards Better Performance and Generalizability. (arXiv:2303.16756v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16756
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38544;&#31169;&#24863;&#30693;&#25968;&#25454;&#22686;&#24378;&#30340;LLM-PTM&#26041;&#27861;&#65292;&#26377;&#25928;&#22320;&#25552;&#39640;&#20102;&#24739;&#32773;-&#35797;&#39564;&#21305;&#37197;&#30340;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#24739;&#32773;&#19982;&#36866;&#21512;&#30340;&#20020;&#24202;&#35797;&#39564;&#36827;&#34892;&#21305;&#37197;&#26159;&#25512;&#36827;&#21307;&#23398;&#30740;&#31350;&#21644;&#25552;&#20379;&#26368;&#20339;&#25252;&#29702;&#30340;&#20851;&#38190;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#38754;&#20020;&#25968;&#25454;&#26631;&#20934;&#21270;&#12289;&#20262;&#29702;&#32771;&#34385;&#21644;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#19982;&#20020;&#24202;&#35797;&#39564;&#26631;&#20934;&#20043;&#38388;&#20114;&#25805;&#20316;&#24615;&#32570;&#20047;&#31561;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#30340;&#28508;&#21147;&#65292;&#36890;&#36807;&#21033;&#29992;&#20854;&#20808;&#36827;&#30340;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#33021;&#21147;&#26469;&#25913;&#21892;EHRs&#21644;&#20020;&#24202;&#35797;&#39564;&#25551;&#36848;&#20043;&#38388;&#30340;&#20860;&#23481;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#22522;&#20110;LLM&#30340;&#24739;&#32773;-&#35797;&#39564;&#21305;&#37197;&#65288;LLM-PTM&#65289;&#30340;&#38544;&#31169;&#24863;&#30693;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#24179;&#34913;&#20102;LLMs&#30340;&#22909;&#22788;&#65292;&#21516;&#26102;&#30830;&#20445;&#25935;&#24863;&#24739;&#32773;&#25968;&#25454;&#30340;&#23433;&#20840;&#21644;&#20445;&#23494;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#20351;&#29992;&#25152;&#25552;&#20986;&#30340;LLM-PTM&#26041;&#27861;&#65292;&#24615;&#33021;&#24179;&#22343;&#25552;&#39640;&#20102;7.32&#65285;&#65292;&#26032;&#25968;&#25454;&#30340;&#27867;&#21270;&#33021;&#21147;&#25552;&#39640;&#20102;12.12&#65285;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#26696;&#20363;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
The process of matching patients with suitable clinical trials is essential for advancing medical research and providing optimal care. However, current approaches face challenges such as data standardization, ethical considerations, and a lack of interoperability between Electronic Health Records (EHRs) and clinical trial criteria. In this paper, we explore the potential of large language models (LLMs) to address these challenges by leveraging their advanced natural language generation capabilities to improve compatibility between EHRs and clinical trial descriptions. We propose an innovative privacy-aware data augmentation approach for LLM-based patient-trial matching (LLM-PTM), which balances the benefits of LLMs while ensuring the security and confidentiality of sensitive patient data. Our experiments demonstrate a 7.32% average improvement in performance using the proposed LLM-PTM method, and the generalizability to new data is improved by 12.12%. Additionally, we present case stud
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21363;&#21033;&#29992;&#26356;&#20016;&#23500;&#30340;&#35821;&#35328;&#21453;&#39304;&#36827;&#34892;&#27169;&#20223;&#23398;&#20064;&#65292;&#36890;&#36807;&#19977;&#20010;&#36845;&#20195;&#27493;&#39588;&#23545;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#20197;&#29983;&#25104;&#26356;&#31526;&#21512;&#20154;&#31867;&#20559;&#22909;&#30340;&#36755;&#20986;&#12290;</title><link>http://arxiv.org/abs/2303.16755</link><description>&lt;p&gt;
&#20351;&#29992;&#35821;&#35328;&#21453;&#39304;&#35268;&#27169;&#21270;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Training Language Models with Language Feedback at Scale. (arXiv:2303.16755v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16755
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21363;&#21033;&#29992;&#26356;&#20016;&#23500;&#30340;&#35821;&#35328;&#21453;&#39304;&#36827;&#34892;&#27169;&#20223;&#23398;&#20064;&#65292;&#36890;&#36807;&#19977;&#20010;&#36845;&#20195;&#27493;&#39588;&#23545;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#20197;&#29983;&#25104;&#26356;&#31526;&#21512;&#20154;&#31867;&#20559;&#22909;&#30340;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#32463;&#24120;&#29983;&#25104;&#19981;&#31526;&#21512;&#20154;&#31867;&#20559;&#22909;&#30340;&#36755;&#20986;&#65292;&#20363;&#22914;&#26377;&#23475;&#30340;&#25991;&#26412;&#25110;&#20107;&#23454;&#19981;&#27491;&#30830;&#30340;&#25688;&#35201;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#23581;&#35797;&#36890;&#36807;&#23398;&#20064;&#19968;&#31181;&#31616;&#21333;&#30340;&#20154;&#31867;&#21453;&#39304;&#24418;&#24335;&#65288;&#21363;&#27169;&#22411;&#29983;&#25104;&#36755;&#20986;&#20043;&#38388;&#30340;&#27604;&#36739;&#65289;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#20294;&#26159;&#65292;&#27604;&#36739;&#21453;&#39304;&#21482;&#33021;&#20256;&#36798;&#26377;&#38480;&#30340;&#20851;&#20110;&#20154;&#31867;&#20559;&#22909;&#30340;&#20449;&#24687;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#8212;&#8212;&#20351;&#29992;&#35821;&#35328;&#21453;&#39304;&#36827;&#34892;&#27169;&#20223;&#23398;&#20064;&#65288;ILF&#65289;&#65292;&#23427;&#21033;&#29992;&#20102;&#26356;&#20016;&#23500;&#30340;&#35821;&#35328;&#21453;&#39304;&#12290;ILF&#30001;&#19977;&#20010;&#36845;&#20195;&#27493;&#39588;&#32452;&#25104;&#65306;&#31532;&#19968;&#27493;&#65292;&#26681;&#25454;&#36755;&#20837;&#65292;&#21021;&#22987;LM&#36755;&#20986;&#21644;&#21453;&#39304;&#23545;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35843;&#33410;&#20197;&#29983;&#25104;&#25913;&#36827;&#12290;&#31532;&#20108;&#27493;&#65292;&#36873;&#25321;&#26368;&#22810;&#21453;&#39304;&#30340;&#25913;&#36827;&#12290;&#31532;&#19977;&#27493;&#65292;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#65292;&#20197;&#26368;&#22823;&#21270;&#22312;&#32473;&#23450;&#36755;&#20837;&#30340;&#24773;&#20917;&#19979;&#36873;&#25321;&#30340;&#25913;&#36827;&#30340;&#21487;&#33021;&#24615;&#12290;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;ILF&#21487;&#20197;&#34987;&#30475;&#20316;&#26159;&#36125;&#21494;&#26031;&#25512;&#26029;&#65292;&#31867;&#20284;&#20110;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#12290;&#25105;&#20204;&#36824;&#35780;&#20272;&#20102;ILF&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pretrained language models often generate outputs that are not in line with human preferences, such as harmful text or factually incorrect summaries. Recent work approaches the above issues by learning from a simple form of human feedback: comparisons between pairs of model-generated outputs. However, comparison feedback only conveys limited information about human preferences. In this paper, we introduce Imitation learning from Language Feedback (ILF), a new approach that utilizes more informative language feedback. ILF consists of three steps that are applied iteratively: first, conditioning the language model on the input, an initial LM output, and feedback to generate refinements. Second, selecting the refinement incorporating the most feedback. Third, finetuning the language model to maximize the likelihood of the chosen refinement given the input. We show theoretically that ILF can be viewed as Bayesian Inference, similar to Reinforcement Learning from human feedback. We evaluate
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20004;&#36718;&#26631;&#27880;&#20107;&#20214;&#25552;&#21462;&#25216;&#26415;&#30340;&#31163;&#23130;&#26696;&#20214;&#20105;&#35758;&#26816;&#27979;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#21496;&#27861;&#26234;&#33021;&#21161;&#25163;&#65288;JIA&#65289;&#31995;&#32479;&#65292;&#20197;&#33258;&#21160;&#20174;&#31163;&#23130;&#26696;&#20214;&#26448;&#26009;&#20013;&#25552;&#21462;&#37325;&#28857;&#20107;&#20214;&#65292;&#36890;&#36807;&#35782;&#21035;&#20854;&#20013;&#30340;&#20849;&#25351;&#26469;&#23545;&#20107;&#20214;&#36827;&#34892;&#23545;&#40784;&#65292;&#24182;&#26816;&#27979;&#20914;&#31361;&#12290;</title><link>http://arxiv.org/abs/2303.16751</link><description>&lt;p&gt;
&#21496;&#27861;&#26234;&#33021;&#21161;&#25163;&#31995;&#32479;&#65306;&#20174;&#31163;&#23130;&#26696;&#20214;&#20013;&#25552;&#21462;&#20107;&#20214;&#20197;&#26816;&#27979;&#35009;&#21028;&#20013;&#30340;&#20105;&#35758;
&lt;/p&gt;
&lt;p&gt;
Judicial Intelligent Assistant System: Extracting Events from Divorce Cases to Detect Disputes for the Judge. (arXiv:2303.16751v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16751
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20004;&#36718;&#26631;&#27880;&#20107;&#20214;&#25552;&#21462;&#25216;&#26415;&#30340;&#31163;&#23130;&#26696;&#20214;&#20105;&#35758;&#26816;&#27979;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#21496;&#27861;&#26234;&#33021;&#21161;&#25163;&#65288;JIA&#65289;&#31995;&#32479;&#65292;&#20197;&#33258;&#21160;&#20174;&#31163;&#23130;&#26696;&#20214;&#26448;&#26009;&#20013;&#25552;&#21462;&#37325;&#28857;&#20107;&#20214;&#65292;&#36890;&#36807;&#35782;&#21035;&#20854;&#20013;&#30340;&#20849;&#25351;&#26469;&#23545;&#20107;&#20214;&#36827;&#34892;&#23545;&#40784;&#65292;&#24182;&#26816;&#27979;&#20914;&#31361;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#27665;&#20107;&#26696;&#20214;&#30340;&#27491;&#24335;&#31243;&#24207;&#20013;&#65292;&#30001;&#19981;&#21516;&#24403;&#20107;&#20154;&#25552;&#20379;&#30340;&#25991;&#26412;&#36164;&#26009;&#25551;&#36848;&#20102;&#26696;&#20214;&#30340;&#21457;&#23637;&#36807;&#31243;&#12290;&#20174;&#36825;&#20123;&#25991;&#26412;&#26448;&#26009;&#20013;&#25552;&#21462;&#26696;&#20214;&#30340;&#20851;&#38190;&#20449;&#24687;&#24182;&#28548;&#28165;&#30456;&#20851;&#24403;&#20107;&#20154;&#30340;&#20105;&#35758;&#28966;&#28857;&#26159;&#19968;&#39033;&#22256;&#38590;&#32780;&#24517;&#35201;&#30340;&#20219;&#21153;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20004;&#36718;&#26631;&#27880;&#20107;&#20214;&#25552;&#21462;&#25216;&#26415;&#30340;&#31163;&#23130;&#26696;&#20214;&#20105;&#35758;&#26816;&#27979;&#26041;&#27861;&#12290;&#25105;&#20204;&#25353;&#29031;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#23454;&#29616;&#20102;&#21496;&#27861;&#26234;&#33021;&#21161;&#25163;&#65288;JIA&#65289;&#31995;&#32479;&#65292;&#20197;&#33258;&#21160;&#20174;&#31163;&#23130;&#26696;&#20214;&#26448;&#26009;&#20013;&#25552;&#21462;&#37325;&#28857;&#20107;&#20214;&#65292;&#36890;&#36807;&#35782;&#21035;&#20854;&#20013;&#30340;&#20849;&#25351;&#26469;&#23545;&#20107;&#20214;&#36827;&#34892;&#23545;&#40784;&#65292;&#24182;&#26816;&#27979;&#20914;&#31361;&#12290;
&lt;/p&gt;
&lt;p&gt;
In formal procedure of civil cases, the textual materials provided by different parties describe the development process of the cases. It is a difficult but necessary task to extract the key information for the cases from these textual materials and to clarify the dispute focus of related parties. Currently, officers read the materials manually and use methods, such as keyword searching and regular matching, to get the target information. These approaches are time-consuming and heavily depending on prior knowledge and carefulness of the officers. To assist the officers to enhance working efficiency and accuracy, we propose an approach to detect disputes from divorce cases based on a two-round-labeling event extracting technique in this paper. We implement the Judicial Intelligent Assistant (JIA) system according to the proposed approach to 1) automatically extract focus events from divorce case materials, 2) align events by identifying co-reference among them, and 3) detect conflicts a
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;ILF&#65292;&#36890;&#36807;&#20174;&#33258;&#28982;&#35821;&#35328;&#21453;&#39304;&#20013;&#36827;&#34892;&#23398;&#20064;&#26469;&#26174;&#33879;&#25552;&#39640;&#20195;&#30721;&#29983;&#25104;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#21363;&#20351;&#21482;&#26377;&#23569;&#37327;&#21453;&#39304;&#65292;&#20063;&#21487;&#20197;&#33719;&#24471;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.16749</link><description>&lt;p&gt;
&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#21453;&#39304;&#36827;&#34892;&#20195;&#30721;&#29983;&#25104;&#30340;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;
Improving Code Generation by Training with Natural Language Feedback. (arXiv:2303.16749v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16749
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;ILF&#65292;&#36890;&#36807;&#20174;&#33258;&#28982;&#35821;&#35328;&#21453;&#39304;&#20013;&#36827;&#34892;&#23398;&#20064;&#26469;&#26174;&#33879;&#25552;&#39640;&#20195;&#30721;&#29983;&#25104;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#21363;&#20351;&#21482;&#26377;&#23569;&#37327;&#21453;&#39304;&#65292;&#20063;&#21487;&#20197;&#33719;&#24471;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#20808;&#35757;&#32451;&#22909;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#25512;&#29702;&#26102;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#21453;&#39304;&#30340;&#28508;&#21147;&#26159;&#26368;&#36817;&#30340;&#19968;&#20010;&#20196;&#20154;&#20852;&#22859;&#30340;&#21457;&#23637;&#12290;&#25105;&#20204;&#22312;&#27492;&#22522;&#30784;&#19978;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;Language Feedback&#65288;ILF&#65289;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#20174;&#33258;&#28982;&#35821;&#35328;&#21453;&#39304;&#20013;&#36827;&#34892;&#23398;&#20064;&#12290;ILF&#22312;&#35757;&#32451;&#26399;&#38388;&#20165;&#38656;&#35201;&#23569;&#37327;&#30340;&#20154;&#24037;&#32534;&#20889;&#21453;&#39304;&#65292;&#24182;&#19988;&#22312;&#27979;&#35797;&#26102;&#19981;&#38656;&#35201;&#30456;&#21516;&#30340;&#21453;&#39304;&#65292;&#22240;&#27492;&#20351;&#29992;&#36215;&#26469;&#26082;&#26041;&#20415;&#21448;&#39640;&#25928;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#35777;&#26126;ILF&#21487;&#20197;&#34987;&#35270;&#20026;&#26368;&#23567;&#21270;&#19982;&#22522;&#20934;&#20998;&#24067;&#30340;KL&#25955;&#24230;&#30340;&#19968;&#31181;&#24418;&#24335;&#65292;&#24182;&#22312;&#31070;&#32463;&#31243;&#24207;&#21512;&#25104;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#27010;&#24565;&#39564;&#35777;&#12290;&#25105;&#20204;&#20351;&#29992;ILF&#22312;Mostly Basic Python Problems(MBPP)&#22522;&#20934;&#27979;&#35797;&#19978;&#23558;Codegen-Mono 6.1B&#27169;&#22411;&#30340;pass @ 1&#35206;&#30422;&#29575;&#30456;&#23545;&#25552;&#39640;&#20102;38%&#65288;&#32477;&#23545;&#25552;&#39640;&#20102;10%&#65289;&#65292;&#32988;&#36807;&#20102;&#22312;MBPP&#19978;&#24494;&#35843;&#21644;&#22312;&#20154;&#31867;&#20462;&#22797;&#30340;&#31243;&#24207;&#19978;&#24494;&#35843;&#30340;&#27169;&#22411;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#21482;&#26377;&#23569;&#37327;&#21453;&#39304;&#65292;&#20174;&#20154;&#31867;&#32534;&#20889;&#30340;&#33258;&#28982;&#35821;&#35328;&#21453;&#39304;&#20013;&#36827;&#34892;&#23398;&#20064;&#20063;&#21487;&#20197;&#26174;&#33879;&#25913;&#36827;&#20195;&#30721;&#29983;&#25104;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
The potential for pre-trained large language models (LLMs) to use natural language feedback at inference time has been an exciting recent development. We build upon this observation by formalizing an algorithm for learning from natural language feedback at training time instead, which we call Imitation learning from Language Feedback (ILF). ILF requires only a small amount of human-written feedback during training and does not require the same feedback at test time, making it both user-friendly and sample-efficient. We further show that ILF can be seen as a form of minimizing the KL divergence to the ground truth distribution and demonstrate a proof-of-concept on a neural program synthesis task. We use ILF to improve a Codegen-Mono 6.1B model's pass@1 rate by 38% relative (and 10% absolute) on the Mostly Basic Python Problems (MBPP) benchmark, outperforming both fine-tuning on MBPP and fine-tuning on repaired programs written by humans. Overall, our results suggest that learning from h
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;&#36870;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#25104;&#21151;&#35299;&#20915;&#20102;&#36890;&#20449;&#36127;&#36733;&#22343;&#34913;&#38382;&#39064;&#65292;&#39318;&#27425;&#23558;&#36870;&#24378;&#21270;&#23398;&#20064;&#24212;&#29992;&#20110;&#35813;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2303.16686</link><description>&lt;p&gt;
&#36890;&#36807;&#39640;&#25928;&#30340;&#36870;&#24378;&#21270;&#23398;&#20064;&#23454;&#29616;&#36890;&#20449;&#36127;&#36733;&#22343;&#34913;
&lt;/p&gt;
&lt;p&gt;
Communication Load Balancing via Efficient Inverse Reinforcement Learning. (arXiv:2303.16686v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16686
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#36870;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#25104;&#21151;&#35299;&#20915;&#20102;&#36890;&#20449;&#36127;&#36733;&#22343;&#34913;&#38382;&#39064;&#65292;&#39318;&#27425;&#23558;&#36870;&#24378;&#21270;&#23398;&#20064;&#24212;&#29992;&#20110;&#35813;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#20449;&#36127;&#36733;&#22343;&#34913;&#26088;&#22312;&#24179;&#34913;&#19981;&#21516;&#21487;&#29992;&#36164;&#28304;&#20043;&#38388;&#30340;&#36127;&#36733;&#65292;&#20174;&#32780;&#25552;&#39640;&#32593;&#32476;&#31995;&#32479;&#30340;&#26381;&#21153;&#36136;&#37327;&#12290;&#26412;&#25991;&#23558;&#36127;&#36733;&#22343;&#34913;&#38382;&#39064;&#25551;&#36848;&#20026;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65292;&#24182;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#26377;&#25928;&#35299;&#20915;&#36127;&#36733;&#22343;&#34913;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#21033;&#29992;&#32463;&#20856;&#24378;&#21270;&#23398;&#20064;&#35299;&#20915;&#36127;&#36733;&#22343;&#34913;&#38382;&#39064;&#65292;&#25105;&#20204;&#38656;&#35201;&#26126;&#30830;&#30340;&#22870;&#21169;&#23450;&#20041;&#12290;&#26500;&#24314;&#36825;&#31181;&#22870;&#21169;&#20989;&#25968;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#23427;&#28041;&#21450;&#21040;&#19987;&#23478;&#30693;&#35782;&#30340;&#38656;&#27714;&#65292;&#32780;&#19988;&#23545;&#20110;&#26368;&#20248;&#22870;&#21169;&#20989;&#25968;&#30340;&#24418;&#24335;&#32570;&#20047;&#20849;&#35782;&#12290;&#26412;&#25991;&#37319;&#29992;&#36870;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#20174;&#19968;&#32452;&#28436;&#31034;&#20013;&#25512;&#26029;&#22870;&#21169;&#20989;&#25968;&#65292;&#25104;&#21151;&#22320;&#35299;&#20915;&#20102;&#36890;&#20449;&#36127;&#36733;&#22343;&#34913;&#38382;&#39064;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#27425;&#22312;&#36890;&#20449;&#36127;&#36733;&#22343;&#34913;&#39046;&#22495;&#25104;&#21151;&#24212;&#29992;&#36870;&#24378;&#21270;&#23398;&#20064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#20174;&#19968;&#32452;&#28436;&#31034;&#20013;&#25512;&#26029;&#22870;&#21169;&#20989;&#25968;&#65292;&#28982;&#21518;&#23398;&#20064;&#24378;&#21270;&#23398;&#20064;&#36127;&#36733;&#22343;&#34913;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Communication load balancing aims to balance the load between different available resources, and thus improve the quality of service for network systems. After formulating the load balancing (LB) as a Markov decision process problem, reinforcement learning (RL) has recently proven effective in addressing the LB problem. To leverage the benefits of classical RL for load balancing, however, we need an explicit reward definition. Engineering this reward function is challenging, because it involves the need for expert knowledge and there lacks a general consensus on the form of an optimal reward function. In this work, we tackle the communication load balancing problem from an inverse reinforcement learning (IRL) approach. To the best of our knowledge, this is the first time IRL has been successfully applied in the field of communication load balancing. Specifically, first, we infer a reward function from a set of demonstrations, and then learn a reinforcement learning load balancing polic
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#31574;&#30053;&#37325;&#29992;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#36873;&#25321;&#26368;&#36866;&#21512;&#25191;&#34892;&#30340;&#39044;&#35757;&#32451;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#26469;&#35299;&#20915;&#36890;&#20449;&#36127;&#36733;&#24179;&#34913;&#38382;&#39064;&#65292;&#20854;&#26681;&#25454;&#19981;&#21516;&#27969;&#37327;&#22330;&#26223;&#19979;&#30340;&#31574;&#30053;&#35757;&#32451;&#24211;&#36827;&#34892;&#36873;&#25321;&#65292;&#21487;&#20197;&#32988;&#36807;&#20256;&#32479;&#30340;&#36127;&#36733;&#24179;&#34913;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.16685</link><description>&lt;p&gt;
&#26410;&#30693;&#27969;&#37327;&#22330;&#26223;&#19979;&#30340;&#36890;&#20449;&#36127;&#36733;&#22343;&#34913;&#20013;&#30340;&#31574;&#30053;&#37325;&#29992;
&lt;/p&gt;
&lt;p&gt;
Policy Reuse for Communication Load Balancing in Unseen Traffic Scenarios. (arXiv:2303.16685v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16685
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#31574;&#30053;&#37325;&#29992;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#36873;&#25321;&#26368;&#36866;&#21512;&#25191;&#34892;&#30340;&#39044;&#35757;&#32451;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#26469;&#35299;&#20915;&#36890;&#20449;&#36127;&#36733;&#24179;&#34913;&#38382;&#39064;&#65292;&#20854;&#26681;&#25454;&#19981;&#21516;&#27969;&#37327;&#22330;&#26223;&#19979;&#30340;&#31574;&#30053;&#35757;&#32451;&#24211;&#36827;&#34892;&#36873;&#25321;&#65292;&#21487;&#20197;&#32988;&#36807;&#20256;&#32479;&#30340;&#36127;&#36733;&#24179;&#34913;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#36890;&#20449;&#32593;&#32476;&#22797;&#26434;&#24615;&#21644;&#27969;&#37327;&#22686;&#38271;&#30340;&#25345;&#32493;&#22686;&#21152;&#65292;&#36890;&#20449;&#36127;&#36733;&#24179;&#34913;&#35299;&#20915;&#26041;&#26696;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#30340;&#26041;&#27861;&#19982;&#20256;&#32479;&#30340;&#22522;&#20110;&#35268;&#21017;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#34920;&#29616;&#20986;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#26631;&#20934;&#30340;RL&#26041;&#27861;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#30340;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#19988;&#23545;&#20110;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#26410;&#36935;&#21040;&#30340;&#22330;&#26223;&#30340;&#27867;&#21270;&#33021;&#21147;&#36739;&#24046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31574;&#30053;&#37325;&#29992;&#26694;&#26550;&#65292;&#20854;&#20013;&#31574;&#30053;&#36873;&#25321;&#22120;&#22522;&#20110;&#24403;&#21069;&#30340;&#27969;&#37327;&#29366;&#20917;&#36873;&#25321;&#26368;&#36866;&#21512;&#30340;&#39044;&#35757;&#32451;RL&#31574;&#30053;&#26469;&#25191;&#34892;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#19968;&#20010;&#31574;&#30053;&#24211;&#65292;&#20854;&#20013;&#21253;&#21547;&#22312;&#19981;&#21516;&#27969;&#37327;&#22330;&#26223;&#19979;&#35757;&#32451;&#30340;&#31574;&#30053;&#12290;&#22312;&#37096;&#32626;&#21040;&#26410;&#30693;&#30340;&#27969;&#37327;&#22330;&#26223;&#26102;&#65292;&#25105;&#20204;&#26681;&#25454;&#24403;&#21069;&#22330;&#26223;&#30340;&#21069;&#19968;&#22825;&#30340;&#27969;&#37327;&#19982;&#35757;&#32451;&#26399;&#38388;&#35266;&#23519;&#21040;&#30340;&#27969;&#37327;&#20043;&#38388;&#30340;&#30456;&#20284;&#24230;&#20174;&#31574;&#30053;&#24211;&#20013;&#36873;&#25321;&#31574;&#30053;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#32988;&#36807;&#20256;&#32479;&#30340;&#36127;&#36733;&#24179;&#34913;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the continuous growth in communication network complexity and traffic volume, communication load balancing solutions are receiving increasing attention. Specifically, reinforcement learning (RL)-based methods have shown impressive performance compared with traditional rule-based methods. However, standard RL methods generally require an enormous amount of data to train, and generalize poorly to scenarios that are not encountered during training. We propose a policy reuse framework in which a policy selector chooses the most suitable pre-trained RL policy to execute based on the current traffic condition. Our method hinges on a policy bank composed of policies trained on a diverse set of traffic scenarios. When deploying to an unknown traffic scenario, we select a policy from the policy bank based on the similarity between the previous-day traffic of the current scenario and the traffic observed during training. Experiments demonstrate that this framework can outperform classical a
&lt;/p&gt;</description></item><item><title>OCPD&#33539;&#20363;&#36716;&#21464;&#20102;&#27969;&#31243;&#25366;&#25496;&#65292;&#21487;&#20197;&#22788;&#29702;&#19982;&#19968;&#31995;&#21015;&#23545;&#35937;&#30456;&#20851;&#32852;&#30340;&#20107;&#20214;&#65292;&#26412;&#25991;&#25552;&#20986;&#30340;&#25193;&#23637;OCPD&#26041;&#27861;&#21487;&#20197;&#36991;&#20813;&#21407;&#26041;&#27861;&#20013;&#20851;&#20110;&#22810;&#23545;&#35937;&#20132;&#20114;&#24490;&#29615;&#30340;&#38169;&#35823;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.16680</link><description>&lt;p&gt;
&#38024;&#23545;&#24490;&#29615;&#21327;&#21516;&#31995;&#32479;&#20013;&#23545;&#35937;&#20132;&#20114;&#30340;&#19981;&#33391;&#36807;&#31243;&#27169;&#22411;&#30340;&#30446;&#26631;&#23548;&#21521;&#38450;&#27490;&#26041;&#27861;&#65306;&#25193;&#23637;&#29256;
&lt;/p&gt;
&lt;p&gt;
Preventing Object-centric Discovery of Unsound Process Models for Object Interactions with Loops in Collaborative Systems: Extended Version. (arXiv:2303.16680v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16680
&lt;/p&gt;
&lt;p&gt;
OCPD&#33539;&#20363;&#36716;&#21464;&#20102;&#27969;&#31243;&#25366;&#25496;&#65292;&#21487;&#20197;&#22788;&#29702;&#19982;&#19968;&#31995;&#21015;&#23545;&#35937;&#30456;&#20851;&#32852;&#30340;&#20107;&#20214;&#65292;&#26412;&#25991;&#25552;&#20986;&#30340;&#25193;&#23637;OCPD&#26041;&#27861;&#21487;&#20197;&#36991;&#20813;&#21407;&#26041;&#27861;&#20013;&#20851;&#20110;&#22810;&#23545;&#35937;&#20132;&#20114;&#24490;&#29615;&#30340;&#38169;&#35823;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35937;&#20013;&#24515;&#30340;&#36807;&#31243;&#21457;&#29616;&#65288;OCPD&#65289;&#26159;&#27969;&#31243;&#25366;&#25496;&#20013;&#30340;&#33539;&#24335;&#36716;&#21464;&#12290; OCPD&#33021;&#22815;&#22788;&#29702;&#19981;&#20855;&#26377;&#21333;&#20010;&#26696;&#20363;&#27010;&#24565;&#20294;&#19982;&#20855;&#26377;&#29305;&#23450;&#31867;&#22411;&#30340;&#19968;&#31995;&#21015;&#23545;&#35937;&#30456;&#20851;&#32852;&#30340;&#20107;&#20214;&#12290;&#23545;&#35937;&#31867;&#22411;&#26500;&#25104;&#22810;&#20010;&#20132;&#20114;&#26696;&#20363;&#27010;&#24565;&#12290; OCPD&#30340;&#36755;&#20986;&#26159;&#19968;&#20010;&#23545;&#35937;&#20013;&#24515;Petri&#32593;&#65292;&#21363;&#20855;&#26377;&#23545;&#35937;&#31867;&#22411;&#20301;&#32622;&#30340;Petri&#32593;&#65292;&#34920;&#31034;&#19982;&#23545;&#35937;&#31867;&#22411;&#23545;&#24212;&#30340;&#22810;&#20010;&#25191;&#34892;&#27969;&#30340;&#24182;&#34892;&#25191;&#34892;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25193;&#23637;OCPD&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20854;&#19981;&#20250;&#21463;&#21040;&#21407;&#22987;&#26041;&#27861;&#20013;&#20986;&#29616;&#30340;&#19981;&#33391;&#38382;&#39064;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Object-centric process discovery (OCPD) constitutes a paradigm shift in process mining. Instead of assuming a single case notion present in the event log, OCPD can handle events without a single case notion, but that are instead related to a collection of objects each having a certain type. The object types constitute multiple, interacting case notions. The output of OCPD is an object-centric Petri net, i.e. a Petri net with object-typed places, that represents the parallel execution of multiple execution flows corresponding to object types. Similar to classical process discovery, where we aim for behaviorally sound process models as a result, in OCPD, we aim for soundness of the resulting object-centric Petri nets. However, the existing OCPD approach can result in violations of soundness. As we will show, one violation arises for multiple interacting object types with loops that arise in collaborative systems. This paper proposes an extended OCPD approach and proves that it does not s
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23558;&#31070;&#32463;&#33539;&#24335;&#27169;&#22359;&#25193;&#23637;&#21040;&#22810;&#31867;&#21644;&#22810;&#26631;&#31614;&#23454;&#38469;&#20998;&#31867;&#20219;&#21153;&#20013;&#65292;&#20197;&#21450;&#24378;&#21046;&#23454;&#29616;&#31526;&#21495;&#20114;&#26021;&#23646;&#24615;&#65292;&#25552;&#39640;&#20102;&#31070;&#32463;&#31526;&#21495;&#35268;&#21017;&#23398;&#20064;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.16674</link><description>&lt;p&gt;
&#23454;&#38469;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#31070;&#32463;&#31526;&#21495;&#35268;&#21017;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Neuro-symbolic Rule Learning in Real-world Classification Tasks. (arXiv:2303.16674v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16674
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23558;&#31070;&#32463;&#33539;&#24335;&#27169;&#22359;&#25193;&#23637;&#21040;&#22810;&#31867;&#21644;&#22810;&#26631;&#31614;&#23454;&#38469;&#20998;&#31867;&#20219;&#21153;&#20013;&#65292;&#20197;&#21450;&#24378;&#21046;&#23454;&#29616;&#31526;&#21495;&#20114;&#26021;&#23646;&#24615;&#65292;&#25552;&#39640;&#20102;&#31070;&#32463;&#31526;&#21495;&#35268;&#21017;&#23398;&#20064;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#31526;&#21495;&#35268;&#21017;&#23398;&#20064;&#22240;&#20854;&#27604;&#32431;&#31070;&#32463;&#27169;&#22411;&#20855;&#26377;&#26356;&#22909;&#30340;&#21487;&#35299;&#37322;&#24615;&#19988;&#27604;&#31526;&#21495;&#35268;&#21017;&#23398;&#20064;&#25193;&#23637;&#24615;&#26356;&#22909;&#32780;&#21463;&#21040;&#20851;&#27880;&#12290;&#26368;&#36817;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; pix2rule &#30340;&#26041;&#27861;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#31070;&#32463;&#33539;&#24335;&#27169;&#22359;&#26469;&#20351;&#29992;&#21069;&#21521;&#23618;&#23398;&#20064;&#31526;&#21495;&#35268;&#21017;&#12290;&#23613;&#31649;&#22312;&#21512;&#25104;&#20108;&#20998;&#31867;&#26041;&#38754;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#65292;&#20294; pix2rule &#23578;&#26410;&#24212;&#29992;&#20110;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#22914;&#22522;&#20110;&#23454;&#38469;&#25968;&#25454;&#30340;&#22810;&#26631;&#31614;&#21644;&#22810;&#31867;&#20998;&#31867;&#12290;&#26412;&#25991;&#36890;&#36807;&#23558;&#31070;&#32463;&#33539;&#24335;&#27169;&#22359;&#25193;&#23637;&#21040;&#20197;&#19979;&#19977;&#20010;&#26041;&#38754;&#26469;&#35299;&#20915;&#36825;&#19968;&#38480;&#21046;&#65306;(i) &#25903;&#25345;&#23454;&#38469;&#22810;&#31867;&#21644;&#22810;&#26631;&#31614;&#20998;&#31867;&#20219;&#21153;&#30340;&#35268;&#21017;&#23398;&#20064;&#65292;(ii) &#22312;&#22810;&#31867;&#20998;&#31867;&#20013;&#24378;&#21046;&#31526;&#21495;&#20114;&#26021;&#23646;&#24615;(&#21363;&#39044;&#27979;&#24688;&#22909;&#19968;&#20010;&#31867;)&#65292;&#20197;&#21450;(iii) &#25506;&#32034;&#20854;&#22312;&#22823;&#36755;&#20837;&#36755;&#20986;&#26041;&#38754;&#30340;&#25193;&#23637;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;&#31867;&#20284;&#20110; pix2rule &#30340;&#31070;&#32463; DNF &#27169;&#22359;&#35757;&#32451;&#20102;&#19968;&#20010;&#26222;&#36890;&#30340;&#31070;&#32463; DNF &#27169;&#22411;&#65292;&#29992;&#20110;&#22810;&#26631;&#31614;&#20998;&#31867;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#26469;&#36890;&#36807;&#25913;&#36827;&#29616;&#26377;&#30340;&#31070;&#32463; DNF &#27169;&#22359;&#23454;&#29616;&#22810;&#31867;&#20998;&#31867;&#20013;&#30340;&#20114;&#26021;&#24615;&#12290;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25193;&#23637;&#30340;&#31070;&#32463; DNF &#27169;&#22411;&#22312;&#20934;&#30830;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#30340;&#31070;&#32463;&#31526;&#21495;&#35268;&#21017;&#23398;&#20064;&#27169;&#22411;&#20197;&#21450;&#32431;&#31070;&#32463;&#27169;&#22411;&#21644;&#31526;&#21495;&#35268;&#21017;&#23398;&#20064;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neuro-symbolic rule learning has attracted lots of attention as it offers better interpretability than pure neural models and scales better than symbolic rule learning. A recent approach named pix2rule proposes a neural Disjunctive Normal Form (neural DNF) module to learn symbolic rules with feed-forward layers. Although proved to be effective in synthetic binary classification, pix2rule has not been applied to more challenging tasks such as multi-label and multi-class classifications over real-world data. In this paper, we address this limitation by extending the neural DNF module to (i) support rule learning in real-world multi-class and multi-label classification tasks, (ii) enforce the symbolic property of mutual exclusivity (i.e. predicting exactly one class) in multi-class classification, and (iii) explore its scalability over large inputs and outputs. We train a vanilla neural DNF model similar to pix2rule's neural DNF module for multi-label classification, and we propose a nove
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;FLANDERS&#65292;&#19968;&#31181;&#22522;&#20110;&#30697;&#38453;&#33258;&#22238;&#24402;&#30340;&#32852;&#37030;&#23398;&#20064;&#32858;&#21512;&#26041;&#26696;&#65292;&#21487;&#20197;&#35782;&#21035;&#24694;&#24847;&#23458;&#25143;&#31471;&#65292;&#24182;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#25308;&#21344;&#24237;&#25915;&#20987;&#38450;&#24481;&#12290;</title><link>http://arxiv.org/abs/2303.16668</link><description>&lt;p&gt;
&#22522;&#20110;&#30697;&#38453;&#33258;&#22238;&#24402;&#30340;&#32852;&#37030;&#23398;&#20064;&#25308;&#21344;&#24237;&#23481;&#38169;&#32858;&#21512;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
A Byzantine-Resilient Aggregation Scheme for Federated Learning via Matrix Autoregression on Client Updates. (arXiv:2303.16668v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16668
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;FLANDERS&#65292;&#19968;&#31181;&#22522;&#20110;&#30697;&#38453;&#33258;&#22238;&#24402;&#30340;&#32852;&#37030;&#23398;&#20064;&#32858;&#21512;&#26041;&#26696;&#65292;&#21487;&#20197;&#35782;&#21035;&#24694;&#24847;&#23458;&#25143;&#31471;&#65292;&#24182;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#25308;&#21344;&#24237;&#25915;&#20987;&#38450;&#24481;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;FLANDERS&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#32858;&#21512;&#26041;&#26696;&#65292;&#21487;&#20197;&#25269;&#24481;&#25308;&#21344;&#24237;&#25915;&#20987;&#12290;FLANDERS&#23558;&#27599;&#20010;FL&#36718;&#27425;&#20013;&#30001;&#23458;&#25143;&#31471;&#21457;&#36865;&#30340;&#26412;&#22320;&#27169;&#22411;&#26356;&#26032;&#35270;&#20026;&#30697;&#38453;&#20540;&#26102;&#38388;&#24207;&#21015;&#12290;&#28982;&#21518;&#65292;&#36890;&#36807;&#23558;&#23454;&#38469;&#35266;&#27979;&#19982;&#30001;&#30697;&#38453;&#33258;&#22238;&#24402;&#39044;&#27979;&#27169;&#22411;&#20272;&#35745;&#30340;&#35266;&#27979;&#36827;&#34892;&#27604;&#36739;&#65292;&#35782;&#21035;&#24694;&#24847;&#23458;&#25143;&#31471;&#20316;&#20026;&#36825;&#20010;&#26102;&#38388;&#24207;&#21015;&#30340;&#24322;&#24120;&#20540;&#12290;&#22312;&#19981;&#21516;FL&#35774;&#32622;&#19979;&#23545;&#22810;&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;FLANDERS&#22312;&#25269;&#24481;&#25308;&#21344;&#24237;&#25915;&#20987;&#26041;&#38754;&#19982;&#26368;&#24378;&#22823;&#30340;&#22522;&#32447;&#30456;&#21305;&#37197;&#12290;&#27492;&#22806;&#65292;&#19982;&#29616;&#26377;&#30340;&#38450;&#24481;&#31574;&#30053;&#30456;&#27604;&#65292; FLANDERS&#21363;&#20351;&#22312;&#26497;&#20854;&#20005;&#37325;&#30340;&#25915;&#20987;&#22330;&#26223;&#19979;&#20173;&#28982;&#38750;&#24120;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we propose FLANDERS, a novel federated learning (FL) aggregation scheme robust to Byzantine attacks. FLANDERS considers the local model updates sent by clients at each FL round as a matrix-valued time series. Then, it identifies malicious clients as outliers of this time series by comparing actual observations with those estimated by a matrix autoregressive forecasting model. Experiments conducted on several datasets under different FL settings demonstrate that FLANDERS matches the robustness of the most powerful baselines against Byzantine clients. Furthermore, FLANDERS remains highly effective even under extremely severe attack scenarios, as opposed to existing defense strategies.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GUT&#30340;&#26032;&#22411;&#20998;&#23618;&#24335;&#21338;&#24328;&#35770;&#23454;&#29992;&#26641;&#27169;&#22411;&#65292;&#22312;&#22810;&#26234;&#33021;&#20307;&#20915;&#31574;&#20013;&#23454;&#29616;&#39640;&#20302;&#23618;&#27425;&#31574;&#30053;&#20998;&#35299;&#65292;&#20351;&#29992;&#20195;&#29702;&#38656;&#27714;&#20316;&#20026;&#26032;&#30340;&#25910;&#30410;&#24230;&#37327;&#65292;&#36890;&#36807;&#24191;&#27867;&#30340;&#25968;&#20540;&#27169;&#25311;&#34920;&#26126;&#21487;&#20197;&#24110;&#21161;MAS&#20197;&#26356;&#20302;&#30340;&#25104;&#26412;&#21644;&#26356;&#39640;&#30340;&#33719;&#32988;&#29575;&#23454;&#29616;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2303.16641</link><description>&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20013;&#21338;&#24328;&#35770;&#20915;&#31574;&#30340;&#20998;&#23618;&#24335;&#23454;&#29992;&#26641;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Hierarchical Game-Theoretic Decision-Making for Cooperative Multi-Agent Systems Under the Presence of Adversarial Agents. (arXiv:2303.16641v1 [cs.MA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16641
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GUT&#30340;&#26032;&#22411;&#20998;&#23618;&#24335;&#21338;&#24328;&#35770;&#23454;&#29992;&#26641;&#27169;&#22411;&#65292;&#22312;&#22810;&#26234;&#33021;&#20307;&#20915;&#31574;&#20013;&#23454;&#29616;&#39640;&#20302;&#23618;&#27425;&#31574;&#30053;&#20998;&#35299;&#65292;&#20351;&#29992;&#20195;&#29702;&#38656;&#27714;&#20316;&#20026;&#26032;&#30340;&#25910;&#30410;&#24230;&#37327;&#65292;&#36890;&#36807;&#24191;&#27867;&#30340;&#25968;&#20540;&#27169;&#25311;&#34920;&#26126;&#21487;&#20197;&#24110;&#21161;MAS&#20197;&#26356;&#20302;&#30340;&#25104;&#26412;&#21644;&#26356;&#39640;&#30340;&#33719;&#32988;&#29575;&#23454;&#29616;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21361;&#38505;&#24773;&#22659;&#20013;&#30340;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#65288;MAS&#65289;&#20013;&#65292;&#22522;&#30784;&#30340;&#20851;&#31995;&#21487;&#20197;&#34987;&#34920;&#31034;&#25104;&#21338;&#24328;&#35770;&#27169;&#22411;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#32593;&#32476;&#30340;&#20998;&#23618;&#27169;&#22411;&#65292;&#31216;&#20026;&#21338;&#24328;&#35770;&#23454;&#29992;&#26641;&#65288;GUT&#65289;&#65292;&#23427;&#23558;&#39640;&#23618;&#27425;&#31574;&#30053;&#20998;&#35299;&#20026;&#21487;&#25191;&#34892;&#30340;&#20302;&#23618;&#27425;&#21160;&#20316;&#65292;&#20197;&#23454;&#29616;&#21512;&#20316;MAS&#20915;&#31574;&#12290;&#23427;&#32467;&#21512;&#20102;&#19968;&#31181;&#22522;&#20110;&#20195;&#29702;&#38656;&#27714;&#30340;&#23454;&#26102;&#31574;&#30053;&#28216;&#25103;&#30340;&#26032;&#25910;&#30410;&#24230;&#37327;&#12290;&#25105;&#20204;&#22312;Explore&#28216;&#25103;&#39046;&#22495;&#20013;&#65292;&#20174;&#24179;&#34913;&#25104;&#21151;&#27010;&#29575;&#21644;&#31995;&#32479;&#25104;&#26412;&#30340;&#35282;&#24230;&#34913;&#37327;&#20102;MAS&#30340;&#32489;&#25928;&#65292;&#35780;&#20272;&#20102;GUT&#26041;&#27861;&#30456;&#23545;&#20110;&#36138;&#24515;&#22320;&#20381;&#36182;&#20110;&#32452;&#21512;&#21160;&#20316;&#22870;&#21169;&#30340;&#26368;&#26032;&#26041;&#27861;&#12290;&#24191;&#27867;&#30340;&#25968;&#20540;&#27169;&#25311;&#30340;&#30830;&#23450;&#32467;&#26524;&#34920;&#26126;GUT&#21487;&#20197;&#32452;&#32455;&#26356;&#22797;&#26434;&#30340;MAS&#21327;&#20316;&#20851;&#31995;&#65292;&#24110;&#21161;&#22242;&#38431;&#20197;&#26356;&#20302;&#30340;&#25104;&#26412;&#21644;&#26356;&#39640;&#30340;&#33719;&#32988;&#29575;&#23454;&#29616;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#28436;&#31034;&#20102;&#20351;&#29992;&#27169;&#25311;&#24212;&#29992;GUT&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Underlying relationships among Multi-Agent Systems (MAS) in hazardous scenarios can be represented as Game-theoretic models. This paper proposes a new hierarchical network-based model called Game-theoretic Utility Tree (GUT), which decomposes high-level strategies into executable low-level actions for cooperative MAS decisions. It combines with a new payoff measure based on agent needs for real-time strategy games. We present an Explore game domain, where we measure the performance of MAS achieving tasks from the perspective of balancing the success probability and system costs. We evaluate the GUT approach against state-of-the-art methods that greedily rely on rewards of the composite actions. Conclusive results on extensive numerical simulations indicate that GUT can organize more complex relationships among MAS cooperation, helping the group achieve challenging tasks with lower costs and higher winning rates. Furthermore, we demonstrated the applicability of the GUT using the simula
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;GPTEval&#65292;&#19968;&#20010;&#21033;&#29992;&#38142;&#24335;&#24605;&#32771;&#21644;&#24418;&#24335;&#22635;&#20805;&#35780;&#20215;NLG&#29983;&#25104;&#30340;&#36136;&#37327;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#25991;&#26412;&#25688;&#35201;&#20219;&#21153;&#20013;&#65292;GPTEval&#32467;&#21512;GPT-4&#21462;&#24471;&#20102;0.514&#30340;&#26031;&#30382;&#23572;&#26364;&#30456;&#20851;&#31995;&#25968;&#65292;&#32988;&#36807;&#20854;&#20182;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.16634</link><description>&lt;p&gt;
GPTEval&#65306;&#20351;&#29992;GPT-4&#21644;&#26356;&#22909;&#30340;&#20154;&#31867;&#23545;&#40784;&#26469;&#35780;&#20272;NLG
&lt;/p&gt;
&lt;p&gt;
GPTEval: NLG Evaluation using GPT-4 with Better Human Alignment. (arXiv:2303.16634v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16634
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;GPTEval&#65292;&#19968;&#20010;&#21033;&#29992;&#38142;&#24335;&#24605;&#32771;&#21644;&#24418;&#24335;&#22635;&#20805;&#35780;&#20215;NLG&#29983;&#25104;&#30340;&#36136;&#37327;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#25991;&#26412;&#25688;&#35201;&#20219;&#21153;&#20013;&#65292;GPTEval&#32467;&#21512;GPT-4&#21462;&#24471;&#20102;0.514&#30340;&#26031;&#30382;&#23572;&#26364;&#30456;&#20851;&#31995;&#25968;&#65292;&#32988;&#36807;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#65288;NLG&#65289;&#31995;&#32479;&#29983;&#25104;&#30340;&#25991;&#26412;&#36136;&#37327;&#24456;&#38590;&#36827;&#34892;&#33258;&#21160;&#27979;&#37327;&#12290;&#20256;&#32479;&#30340;&#22522;&#20110;&#21442;&#32771;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#22914;BLEU&#21644;ROUGE&#24050;&#34987;&#35777;&#26126;&#22312;&#38656;&#35201;&#21019;&#36896;&#24615;&#21644;&#22810;&#26679;&#24615;&#30340;&#20219;&#21153;&#20013;&#19982;&#20154;&#31867;&#21028;&#26029;&#30340;&#30456;&#20851;&#24615;&#30456;&#23545;&#36739;&#20302;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#24314;&#35758;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#26080;&#21442;&#32771;&#30340;NLG&#35780;&#20272;&#24230;&#37327;&#26631;&#20934;&#65292;&#36825;&#20123;&#27169;&#22411;&#36866;&#29992;&#20110;&#32570;&#20047;&#20154;&#31867;&#21442;&#32771;&#30340;&#26032;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#22522;&#20110;LLM&#30340;&#35780;&#20272;&#22120;&#20173;&#28982;&#27604;&#20013;&#31561;&#35268;&#27169;&#30340;&#31070;&#32463;&#35780;&#20272;&#22120;&#30340;&#20154;&#31867;&#23545;&#24212;&#24230;&#20302;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GPTEval&#65292;&#19968;&#20010;&#20351;&#29992;&#38142;&#24335;&#24605;&#32771;&#65288;CoT&#65289;&#21644;&#24418;&#24335;&#22635;&#20805;&#33539;&#24335;&#26469;&#35780;&#20272;NLG&#36755;&#20986;&#36136;&#37327;&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#38024;&#23545;&#20004;&#20010;&#29983;&#25104;&#20219;&#21153;&#65292;&#25991;&#26412;&#25688;&#35201;&#21644;&#23545;&#35805;&#29983;&#25104;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;&#25105;&#20204;&#23637;&#31034;&#20986;&#65292;GPTEval&#32467;&#21512;GPT-4&#20316;&#20026;&#39592;&#24178;&#27169;&#22411;&#65292;&#22312;&#25688;&#35201;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;0.514&#30340;&#26031;&#30382;&#23572;&#26364;&#30456;&#20851;&#31995;&#25968;&#65292;&#32988;&#36807;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The quality of texts generated by natural language generation (NLG) systems is hard to measure automatically. Conventional reference-based metrics, such as BLEU and ROUGE, have been shown to have relatively low correlation with human judgments, especially for tasks that require creativity and diversity. Recent studies suggest using large language models (LLMs) as reference-free metrics for NLG evaluation, which have the benefit of being applicable to new tasks that lack human references. However, these LLM-based evaluators still have lower human correspondence than medium-size neural evaluators. In this work, we present GPTEval, a framework of using large language models with chain-of-thoughts (CoT) and a form-filling paradigm, to assess the quality of NLG outputs. We experiment with two generation tasks, text summarization and dialogue generation. We show that GPTEval with GPT-4 as the backbone model achieves a Spearman correlation of 0.514 with human on summarization task, outperform
&lt;/p&gt;</description></item><item><title>Fairlearn&#26159;&#19968;&#20010;&#26377;&#21161;&#20110;&#35780;&#20272;&#21644;&#25552;&#39640;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#20844;&#24179;&#24615;&#30340;&#24320;&#28304;&#39033;&#30446;&#65292;&#36890;&#36807;&#21487;&#35270;&#21270;&#22320;&#35780;&#20272;&#21463;&#24433;&#21709;&#20154;&#32676;&#30340;&#27169;&#22411;&#36755;&#20986;&#65292;&#24182;&#25552;&#20379;&#22810;&#31181;&#31639;&#27861;&#26469;&#32531;&#35299;&#19981;&#20844;&#24179;&#38382;&#39064;&#65292;&#21516;&#26102;&#36824;&#22312;&#32771;&#34385;&#31995;&#32479;&#30340;&#26356;&#24191;&#27867;&#31038;&#20250;&#32972;&#26223;&#26041;&#38754;&#25552;&#20379;&#20102;&#23398;&#20064;&#36164;&#28304;&#12290;</title><link>http://arxiv.org/abs/2303.16626</link><description>&lt;p&gt;
Fairlearn: &#35780;&#20272;&#21644;&#25552;&#21319;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;
Fairlearn: Assessing and Improving Fairness of AI Systems. (arXiv:2303.16626v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16626
&lt;/p&gt;
&lt;p&gt;
Fairlearn&#26159;&#19968;&#20010;&#26377;&#21161;&#20110;&#35780;&#20272;&#21644;&#25552;&#39640;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#20844;&#24179;&#24615;&#30340;&#24320;&#28304;&#39033;&#30446;&#65292;&#36890;&#36807;&#21487;&#35270;&#21270;&#22320;&#35780;&#20272;&#21463;&#24433;&#21709;&#20154;&#32676;&#30340;&#27169;&#22411;&#36755;&#20986;&#65292;&#24182;&#25552;&#20379;&#22810;&#31181;&#31639;&#27861;&#26469;&#32531;&#35299;&#19981;&#20844;&#24179;&#38382;&#39064;&#65292;&#21516;&#26102;&#36824;&#22312;&#32771;&#34385;&#31995;&#32479;&#30340;&#26356;&#24191;&#27867;&#31038;&#20250;&#32972;&#26223;&#26041;&#38754;&#25552;&#20379;&#20102;&#23398;&#20064;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Fairlearn&#26159;&#19968;&#20010;&#24320;&#28304;&#39033;&#30446;&#65292;&#26088;&#22312;&#24110;&#21161;&#20174;&#19994;&#20154;&#21592;&#35780;&#20272;&#21644;&#25552;&#39640;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#31995;&#32479;&#30340;&#20844;&#24179;&#24615;&#12290;&#35813;&#30456;&#20851;&#30340;Python&#24211;&#65292;&#20063;&#21517;&#20026;Fairlearn&#65292;&#25903;&#25345;&#36328;&#21463;&#24433;&#21709;&#30340;&#20154;&#21475;&#35780;&#20272;&#27169;&#22411;&#30340;&#36755;&#20986;&#65292;&#24182;&#21253;&#25324;&#20960;&#31181;&#31639;&#27861;&#20197;&#32531;&#35299;&#20844;&#24179;&#24615;&#38382;&#39064;&#12290;&#22522;&#20110;&#20844;&#24179;&#26159;&#19968;&#20010;&#31038;&#20250;&#25216;&#26415;&#25361;&#25112;&#30340;&#35748;&#35782;&#65292;&#35813;&#39033;&#30446;&#38598;&#25104;&#20102;&#23398;&#20064;&#36164;&#28304;&#65292;&#20197;&#24110;&#21161;&#20174;&#19994;&#20154;&#21592;&#32771;&#34385;&#19968;&#20010;&#31995;&#32479;&#30340;&#26356;&#24191;&#27867;&#31038;&#20250;&#32972;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fairlearn is an open source project to help practitioners assess and improve fairness of artificial intelligence (AI) systems. The associated Python library, also named fairlearn, supports evaluation of a model's output across affected populations and includes several algorithms for mitigating fairness issues. Grounded in the understanding that fairness is a sociotechnical challenge, the project integrates learning resources that aid practitioners in considering a system's broader societal context.
&lt;/p&gt;</description></item><item><title>AraSpot&#26159;&#19968;&#27454;&#20351;&#29992;ConformerGRU&#27169;&#22411;&#26550;&#26500;&#35757;&#32451;40&#20010;&#38463;&#25289;&#20271;&#35821;&#20851;&#38190;&#35789;&#30340;&#21475;&#35821;&#21629;&#20196;&#35782;&#21035;&#24037;&#20855;&#65292;&#20854;&#36890;&#36807;&#22312;&#32447;&#25968;&#25454;&#22686;&#24378;&#21644;&#25991;&#26412;&#21040;&#35821;&#38899;&#27169;&#22411;&#30340;&#35757;&#32451;&#25552;&#39640;&#20102;&#24615;&#33021;&#65292;&#24182;&#20197;99.59%&#30340;&#20934;&#30830;&#29575;&#36229;&#20986;&#20197;&#24448;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.16621</link><description>&lt;p&gt;
AraSpot&#65306;&#38463;&#25289;&#20271;&#35821;&#21475;&#35821;&#21629;&#20196;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
AraSpot: Arabic Spoken Command Spotting. (arXiv:2303.16621v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16621
&lt;/p&gt;
&lt;p&gt;
AraSpot&#26159;&#19968;&#27454;&#20351;&#29992;ConformerGRU&#27169;&#22411;&#26550;&#26500;&#35757;&#32451;40&#20010;&#38463;&#25289;&#20271;&#35821;&#20851;&#38190;&#35789;&#30340;&#21475;&#35821;&#21629;&#20196;&#35782;&#21035;&#24037;&#20855;&#65292;&#20854;&#36890;&#36807;&#22312;&#32447;&#25968;&#25454;&#22686;&#24378;&#21644;&#25991;&#26412;&#21040;&#35821;&#38899;&#27169;&#22411;&#30340;&#35757;&#32451;&#25552;&#39640;&#20102;&#24615;&#33021;&#65292;&#24182;&#20197;99.59%&#30340;&#20934;&#30830;&#29575;&#36229;&#20986;&#20197;&#24448;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21475;&#35821;&#20851;&#38190;&#35782;&#21035;&#65288;KWS&#65289;&#26159;&#25351;&#22312;&#38899;&#39057;&#27969;&#20013;&#35782;&#21035;&#20851;&#38190;&#35789;&#65292;&#24191;&#27867;&#29992;&#20110;&#26234;&#33021;&#36793;&#32536;&#35774;&#22791;&#19978;&#65292;&#20197;&#21551;&#21160;&#35821;&#38899;&#21161;&#25163;&#21644;&#36827;&#34892;&#20813;&#25552;&#20219;&#21153;&#12290;&#35813;&#20219;&#21153;&#38754;&#20020;&#30528;&#39640;&#31934;&#24230;&#21644;&#22312;&#20302;&#21151;&#29575;&#21644;&#21487;&#33021;&#30340;&#26377;&#38480;&#35745;&#31639;&#33021;&#21147;&#35774;&#22791;&#19978;&#20445;&#25345;&#31995;&#32479;&#36816;&#34892;&#25928;&#29575;&#30340;&#38656;&#27714;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;&#19981;&#21516;&#30340;&#22312;&#32447;&#25968;&#25454;&#22686;&#24378;&#21644;&#24341;&#20837;ConformerGRU&#27169;&#22411;&#26550;&#26500;&#30340;AraSpot&#65292;&#29992;&#20110;&#35757;&#32451;40&#20010;&#38463;&#25289;&#20271;&#35821;&#20851;&#38190;&#35789;&#30340;&#35782;&#21035;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#35757;&#32451;&#25991;&#26412;&#21040;&#35821;&#38899;&#27169;&#22411;&#36827;&#34892;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;AraSpot&#20197;SOTA 99.59&#65285;&#36229;&#36807;&#20197;&#24448;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spoken keyword spotting (KWS) is the task of identifying a keyword in an audio stream and is widely used in smart devices at the edge in order to activate voice assistants and perform hands-free tasks. The task is daunting as there is a need, on the one hand, to achieve high accuracy while at the same time ensuring that such systems continue to run efficiently on low power and possibly limited computational capabilities devices. This work presents AraSpot for Arabic keyword spotting trained on 40 Arabic keywords, using different online data augmentation, and introducing ConformerGRU model architecture. Finally, we further improve the performance of the model by training a text-to-speech model for synthetic data generation. AraSpot achieved a State-of-the-Art SOTA 99.59% result outperforming previous approaches.
&lt;/p&gt;</description></item><item><title>&#26412;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#20351;&#29992;&#20016;&#23500;&#30340;&#20803;&#25968;&#25454;&#27880;&#37322;&#30340;&#20449;&#24687;&#36827;&#34892;&#23631;&#24149;&#35282;&#33394;&#30340;&#20010;&#24615;&#21270;&#35821;&#35328;&#24314;&#27169;&#65292;&#27979;&#35797;&#34920;&#26126;&#36825;&#26679;&#21487;&#20197;&#26377;&#25928;&#22320;&#36827;&#34892;&#20010;&#24615;&#21270;&#35821;&#35328;&#27169;&#22411;&#30340;&#26500;&#24314;&#65292;&#21363;&#20351;&#23545;&#20110;&#38646;&#26679;&#26412;&#30340;&#28436;&#35828;&#23478;&#20063;&#21487;&#20197;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2303.16618</link><description>&lt;p&gt;
&#20351;&#29992;&#20016;&#23500;&#30340;&#20803;&#25968;&#25454;&#27880;&#37322;&#30340;&#23631;&#24149;&#35282;&#33394;&#30340;&#20010;&#24615;&#21270;&#35821;&#35328;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Personalised Language Modelling of Screen Characters Using Rich Metadata Annotations. (arXiv:2303.16618v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16618
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#20351;&#29992;&#20016;&#23500;&#30340;&#20803;&#25968;&#25454;&#27880;&#37322;&#30340;&#20449;&#24687;&#36827;&#34892;&#23631;&#24149;&#35282;&#33394;&#30340;&#20010;&#24615;&#21270;&#35821;&#35328;&#24314;&#27169;&#65292;&#27979;&#35797;&#34920;&#26126;&#36825;&#26679;&#21487;&#20197;&#26377;&#25928;&#22320;&#36827;&#34892;&#20010;&#24615;&#21270;&#35821;&#35328;&#27169;&#22411;&#30340;&#26500;&#24314;&#65292;&#21363;&#20351;&#23545;&#20110;&#38646;&#26679;&#26412;&#30340;&#28436;&#35828;&#23478;&#20063;&#21487;&#20197;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#30340;&#20010;&#24615;&#21270;&#20026;&#23545;&#35805;&#25935;&#24863;&#65292;&#33021;&#26356;&#22909;&#22320;&#25429;&#25417;&#29305;&#23450;&#29305;&#24449;&#30340;&#20154;&#21592;&#21644;/&#25110;&#29305;&#23450;&#29615;&#22659;&#20013;&#30340;&#35828;&#35805;&#27169;&#24335;&#12290;&#28982;&#32780;&#65292;&#20016;&#23500;&#30340;&#35282;&#33394;&#27880;&#37322;&#38590;&#20197;&#24471;&#21040;&#21644;&#25104;&#21151;&#21033;&#29992;&#12290;&#22312;&#27492;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21457;&#24067;&#24182;&#25551;&#36848;&#20102;&#19968;&#32452;&#26032;&#39062;&#30340;&#25163;&#21160;&#27880;&#37322;&#65292;&#28085;&#30422;&#20102;&#26469;&#33258;&#27969;&#34892;&#30340; Cornell &#30005;&#24433;&#23545;&#35805;&#35821;&#26009;&#24211;&#30340; 863 &#21517;&#28436;&#35762;&#32773;&#65292;&#21253;&#25324;&#29305;&#24449;&#24341;&#29992;&#21644;&#35282;&#33394;&#25551;&#36848;&#65292;&#20197;&#21450;&#36229;&#36807; 95&#65285; &#30340;&#29305;&#33394;&#30005;&#24433;&#30340;&#19968;&#32452;&#33258;&#21160;&#25552;&#21462;&#30340;&#20845;&#20010;&#20803;&#25968;&#25454;&#12290;&#25105;&#20204;&#23545;&#20004;&#20010;&#35821;&#26009;&#24211;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#24182;&#34920;&#26126;&#21487;&#20197;&#26377;&#25928;&#22320;&#20351;&#29992;&#27492;&#31867;&#27880;&#37322;&#26469;&#20010;&#24615;&#21270;&#35821;&#35328;&#27169;&#22411;&#65292;&#23558;&#22256;&#24785;&#20943;&#23569;&#39640;&#36798; 8.5&#65285;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#29978;&#33267;&#21487;&#20197;&#24212;&#29992;&#20110;&#38646;&#26679;&#26412;&#30340;&#28436;&#35762;&#32773;&#65292;&#21363;&#23545;&#20110;&#27809;&#26377;&#20808;&#21069;&#22521;&#35757;&#25968;&#25454;&#30340;&#28436;&#35762;&#32773;&#65292;&#20381;&#36182;&#20110;&#35282;&#33394;&#30340;&#20154;&#21475;&#29305;&#24449;&#30340;&#32452;&#21512;&#12290;&#30001;&#20110;&#25910;&#38598;&#27492;&#31867;&#20803;&#25968;&#25454;&#25104;&#26412;&#39640;&#26114;&#65292;&#22240;&#27492;&#25105;&#20204;&#36824;&#36129;&#29486;&#20102;&#19968;&#39033;&#25104;&#26412;&#25928;&#30410;&#20998;&#26512;&#65292;&#20197;&#31361;&#20986;&#26174;&#31034;
&lt;/p&gt;
&lt;p&gt;
Personalisation of language models for dialogue sensitises them to better capture the speaking patterns of people of specific characteristics, and/or in specific environments. However, rich character annotations are difficult to come by and to successfully leverage. In this work, we release and describe a novel set of manual annotations for 863 speakers from the popular Cornell Movie Dialog Corpus, including features like characteristic quotes and character descriptions, and a set of six automatically extracted metadata for over 95% of the featured films. We perform extensive experiments on two corpora and show that such annotations can be effectively used to personalise language models, reducing perplexity by up to 8.5%. Our method can be applied even zero-shot for speakers for whom no prior training data is available, by relying on combinations of characters' demographic characteristics. Since collecting such metadata is costly, we also contribute a cost-benefit analysis to highlight
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38544;&#24335;&#20943;&#36731;&#35270;&#35273;&#20559;&#35265;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#21518;&#39564;&#20272;&#35745;&#23574;&#38160;&#21270;&#65292;&#40723;&#21169;&#32593;&#32476;&#32858;&#28966;&#20110;&#19981;&#23548;&#33268;&#39640;&#19981;&#30830;&#23450;&#24615;&#30340;&#20013;&#24515;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2303.16564</link><description>&lt;p&gt;
&#36890;&#36807;&#21518;&#39564;&#20272;&#35745;&#23574;&#38160;&#21270;&#26469;&#38544;&#24335;&#20943;&#36731;&#35270;&#35273;&#20559;&#35265;&#65306;&#19968;&#31181;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Implicit Visual Bias Mitigation by Posterior Estimate Sharpening of a Bayesian Neural Network. (arXiv:2303.16564v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16564
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38544;&#24335;&#20943;&#36731;&#35270;&#35273;&#20559;&#35265;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#21518;&#39564;&#20272;&#35745;&#23574;&#38160;&#21270;&#65292;&#40723;&#21169;&#32593;&#32476;&#32858;&#28966;&#20110;&#19981;&#23548;&#33268;&#39640;&#19981;&#30830;&#23450;&#24615;&#30340;&#20013;&#24515;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#20844;&#24179;&#24615;&#21463;&#21040;&#25968;&#25454;&#38598;&#20559;&#35265;&#21644;&#34394;&#20551;&#30456;&#20851;&#24615;&#30340;&#24378;&#28872;&#24433;&#21709;&#65292;&#32780;&#36825;&#20123;&#23545;&#29616;&#20195;&#29305;&#24449;&#20016;&#23500;&#21644;&#22797;&#26434;&#30340;&#35270;&#35273;&#25968;&#25454;&#38598;&#36890;&#24120;&#37117;&#26159;&#23384;&#22312;&#30340;&#12290;&#30001;&#20110;&#20219;&#21153;&#30340;&#38590;&#24230;&#21644;&#21487;&#21464;&#24615;&#65292;&#27809;&#26377;&#19968;&#31181;&#21333;&#19968;&#30340;&#21435;&#20559;&#35265;&#26041;&#27861;&#26159;&#26222;&#36941;&#25104;&#21151;&#30340;&#12290;&#29305;&#21035;&#26159;&#65292;&#22312;&#19981;&#38656;&#35201;&#26174;&#24335;&#30693;&#36947;&#20559;&#24046;&#21464;&#37327;&#30340;&#24773;&#20917;&#19979;&#30340;&#38544;&#24335;&#26041;&#27861;&#23545;&#20110;&#29616;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#23588;&#20026;&#30456;&#20851;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38544;&#24335;&#20943;&#32531;&#26041;&#27861;&#65292;&#20351;&#29992;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#65292;&#20801;&#35768;&#25105;&#20204;&#21033;&#29992;&#33268;&#24615;&#19981;&#30830;&#23450;&#24615;&#19982;&#26679;&#26412;&#20013;&#20559;&#24046;&#25110;&#34394;&#20551;&#30456;&#20851;&#24615;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#21518;&#39564;&#20272;&#35745;&#23574;&#38160;&#21270;&#31243;&#24207;&#40723;&#21169;&#32593;&#32476;&#32858;&#28966;&#20110;&#19981;&#23548;&#33268;&#39640;&#19981;&#30830;&#23450;&#24615;&#30340;&#20013;&#24515;&#29305;&#24449;&#12290;&#22312;&#19977;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20855;&#26377;&#32463;&#36807;&#23574;&#38160;&#21270;&#21518;&#39564;&#20272;&#35745;&#30340;&#36125;&#21494;&#26031;&#32593;&#32476;&#34920;&#29616;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#24403;&#65292;&#24182;&#26174;&#31034;&#20986;&#36827;&#19968;&#27493;&#30740;&#31350;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The fairness of a deep neural network is strongly affected by dataset bias and spurious correlations, both of which are usually present in modern feature-rich and complex visual datasets. Due to the difficulty and variability of the task, no single de-biasing method has been universally successful. In particular, implicit methods not requiring explicit knowledge of bias variables are especially relevant for real-world applications. We propose a novel implicit mitigation method using a Bayesian neural network, allowing us to leverage the relationship between epistemic uncertainties and the presence of bias or spurious correlations in a sample. Our proposed posterior estimate sharpening procedure encourages the network to focus on core features that do not contribute to high uncertainties. Experimental results on three benchmark datasets demonstrate that Bayesian networks with sharpened posterior estimates perform comparably to prior existing methods and show potential worthy of further 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25216;&#33021;&#24378;&#21270;&#23398;&#20064;&#21644;&#35268;&#21010;&#30340; Minecraft &#24320;&#25918;&#24335;&#20219;&#21153;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#23398;&#20064;&#22522;&#26412;&#25216;&#33021;&#21644;&#25216;&#33021;&#35268;&#21010;&#30340;&#26041;&#27861;&#26377;&#25928;&#25552;&#39640;&#20102;&#20219;&#21153;&#35299;&#20915;&#25928;&#29575;&#65292;&#23454;&#29616;&#20102;24&#20010;&#19981;&#21516;&#30340;&#20219;&#21153;&#24182;&#22312;&#22823;&#22810;&#25968;&#20219;&#21153;&#20013;&#20248;&#20110;&#22522;&#32447;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.16563</link><description>&lt;p&gt;
Plan4MC: &#22522;&#20110;&#25216;&#33021;&#24378;&#21270;&#23398;&#20064;&#21644;&#35268;&#21010;&#30340; Minecraft &#24320;&#25918;&#24335;&#20219;&#21153;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Plan4MC: Skill Reinforcement Learning and Planning for Open-World Minecraft Tasks. (arXiv:2303.16563v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16563
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25216;&#33021;&#24378;&#21270;&#23398;&#20064;&#21644;&#35268;&#21010;&#30340; Minecraft &#24320;&#25918;&#24335;&#20219;&#21153;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#23398;&#20064;&#22522;&#26412;&#25216;&#33021;&#21644;&#25216;&#33021;&#35268;&#21010;&#30340;&#26041;&#27861;&#26377;&#25928;&#25552;&#39640;&#20102;&#20219;&#21153;&#35299;&#20915;&#25928;&#29575;&#65292;&#23454;&#29616;&#20102;24&#20010;&#19981;&#21516;&#30340;&#20219;&#21153;&#24182;&#22312;&#22823;&#22810;&#25968;&#20219;&#21153;&#20013;&#20248;&#20110;&#22522;&#32447;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#22312; Minecraft &#20013;&#26500;&#24314;&#19968;&#20010;&#22810;&#20219;&#21153;&#26234;&#33021;&#20307;&#12290;&#22312;&#27809;&#26377;&#20154;&#24037;&#28436;&#31034;&#30340;&#24773;&#20917;&#19979;&#65292;&#29992;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#35299;&#20915;&#36825;&#20010;&#24320;&#25918;&#24335;&#29615;&#22659;&#20013;&#30340;&#38271;&#31243;&#20219;&#21153;&#26159;&#26497;&#20854;&#26679;&#26412;&#20302;&#25928;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#23558; Minecraft &#20219;&#21153;&#30340;&#35299;&#20915;&#20998;&#35299;&#25104;&#23398;&#20064;&#22522;&#26412;&#25216;&#33021;&#21644;&#22522;&#20110;&#25216;&#33021;&#36827;&#34892;&#35268;&#21010;&#20004;&#20010;&#38454;&#27573;&#12290;&#25105;&#20204;&#22312; Minecraft &#20013;&#25552;&#20986;&#20102;&#19977;&#31181;&#31867;&#22411;&#30340;&#32454;&#31890;&#24230;&#22522;&#26412;&#25216;&#33021;&#65292;&#24182;&#20351;&#29992;&#20855;&#26377;&#20869;&#22312;&#22870;&#21169;&#30340; RL &#26041;&#27861;&#26469;&#23454;&#29616;&#25104;&#21151;&#29575;&#39640;&#30340;&#22522;&#26412;&#25216;&#33021;&#23398;&#20064;&#12290;&#22312;&#25216;&#33021;&#35268;&#21010;&#26041;&#38754;&#65292;&#25105;&#20204;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#21457;&#29616;&#25216;&#33021;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#39044;&#20808;&#26500;&#24314;&#25216;&#33021;&#22270;&#12290;&#24403;&#26234;&#33021;&#20307;&#35299;&#20915;&#20219;&#21153;&#26102;&#65292;&#25105;&#20204;&#30340;&#25216;&#33021;&#25628;&#32034;&#31639;&#27861;&#22312;&#25216;&#33021;&#22270;&#19978;&#34892;&#36208;&#24182;&#29983;&#25104;&#36866;&#24403;&#30340;&#25216;&#33021;&#35745;&#21010;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#35299;&#20915;&#20102; 24 &#20010;&#19981;&#21516;&#30340; Minecraft &#20219;&#21153;&#65292;&#20854;&#20013;&#35768;&#22810;&#20219;&#21153;&#38656;&#35201;&#36830;&#32493;&#25191;&#34892;&#36229;&#36807; 10 &#20010;&#25216;&#33021;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22823;&#22810;&#25968;&#20219;&#21153;&#20013;&#20248;&#20110;&#22522;&#32447;&#31639;&#27861;&#12290;&#39033;&#30446;&#30340;&#32593;&#22336;&#21644;&#20195;&#30721;&#21487;&#20197;&#22312; https://www.rocwang.me/plan4mc.html &#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study building a multi-task agent in Minecraft. Without human demonstrations, solving long-horizon tasks in this open-ended environment with reinforcement learning (RL) is extremely sample inefficient. To tackle the challenge, we decompose solving Minecraft tasks into learning basic skills and planning over the skills. We propose three types of fine-grained basic skills in Minecraft, and use RL with intrinsic rewards to accomplish basic skills with high success rates. For skill planning, we use Large Language Models to find the relationships between skills and build a skill graph in advance. When the agent is solving a task, our skill search algorithm walks on the skill graph and generates the proper skill plans for the agent. In experiments, our method accomplishes 24 diverse Minecraft tasks, where many tasks require sequentially executing for more than 10 skills. Our method outperforms baselines in most tasks by a large margin. The project's website and code can be found at https:
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#32047;&#31215;&#35270;&#35273;&#21464;&#25442;&#22120;&#30340;&#26041;&#27861;&#65292;&#24212;&#29992;&#20110;&#39592;&#40836;&#35780;&#20272;&#12290;&#36890;&#36807;&#24212;&#29992;&#26631;&#35760;&#37325;&#25918;&#21644;&#21306;&#22495;&#27880;&#24847;&#20559;&#24046;&#65292;&#35813;&#26041;&#27861;&#26377;&#25928;&#22320;&#25366;&#25496;&#22320;&#26631;&#20043;&#38388;&#30340;&#20851;&#31995;&#24182;&#23398;&#20064;&#20840;&#23616;&#24418;&#24577;&#29305;&#24449;&#65292;&#20351;&#39592;&#40836;&#35780;&#20272;&#30340;&#24179;&#22343;&#32477;&#23545;&#35823;&#24046;&#27604;&#20043;&#21069;&#30340;&#24037;&#20316;&#38477;&#20302;&#20102;0.11&#12290;</title><link>http://arxiv.org/abs/2303.16557</link><description>&lt;p&gt;
Bone Age Assessment&#30340;&#33258;&#32047;&#31215;&#35270;&#35273;&#21464;&#25442;&#22120;&#65292;&#20351;&#29992;Sauvegrain&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Self-accumulative Vision Transformer for Bone Age Assessment Using the Sauvegrain Method. (arXiv:2303.16557v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16557
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#32047;&#31215;&#35270;&#35273;&#21464;&#25442;&#22120;&#30340;&#26041;&#27861;&#65292;&#24212;&#29992;&#20110;&#39592;&#40836;&#35780;&#20272;&#12290;&#36890;&#36807;&#24212;&#29992;&#26631;&#35760;&#37325;&#25918;&#21644;&#21306;&#22495;&#27880;&#24847;&#20559;&#24046;&#65292;&#35813;&#26041;&#27861;&#26377;&#25928;&#22320;&#25366;&#25496;&#22320;&#26631;&#20043;&#38388;&#30340;&#20851;&#31995;&#24182;&#23398;&#20064;&#20840;&#23616;&#24418;&#24577;&#29305;&#24449;&#65292;&#20351;&#39592;&#40836;&#35780;&#20272;&#30340;&#24179;&#22343;&#32477;&#23545;&#35823;&#24046;&#27604;&#20043;&#21069;&#30340;&#24037;&#20316;&#38477;&#20302;&#20102;0.11&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#22522;&#20110;Sauvegrain&#26041;&#27861;&#65292;&#20351;&#29992;&#22810;&#35270;&#35282;&#12289;&#22810;&#20219;&#21153;&#20998;&#31867;&#27169;&#22411;&#36827;&#34892;&#39592;&#40836;&#35780;&#20272;&#65288;BAA&#65289;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#35757;&#32451;&#20998;&#31867;&#22120;&#26469;&#35780;&#20998;&#27599;&#20010;&#19968;&#28857;&#30340;&#25104;&#29087;&#24230;&#24182;&#39044;&#27979;&#39592;&#40836;&#65292;&#20294;&#26159;&#36825;&#31181;&#26041;&#27861;&#23616;&#38480;&#20110;&#26412;&#22320;&#24418;&#24577;&#65292;&#24182;&#22686;&#21152;&#20102;&#35745;&#31639;&#25104;&#26412;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#32047;&#31215;&#35270;&#35273;&#21464;&#25442;&#22120;&#65288;SAT&#65289;&#65292;&#36890;&#36807;&#24212;&#29992;&#26631;&#35760;&#37325;&#25918;&#21644;&#21306;&#22495;&#27880;&#24847;&#20559;&#24046;&#65292;&#28040;&#20943;&#20102;&#22810;&#35270;&#35282;&#12289;&#22810;&#20219;&#21153;&#38382;&#39064;&#20013;&#36890;&#24120;&#21457;&#29983;&#30340;&#21508;&#21521;&#24322;&#24615;&#34892;&#20026;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#25366;&#25496;&#22320;&#26631;&#20043;&#38388;&#30340;&#20851;&#31995;&#24182;&#23398;&#20064;&#20840;&#23616;&#24418;&#24577;&#29305;&#24449;&#12290;&#36890;&#36807;&#22810;&#27425;&#23454;&#39564;&#34920;&#26126;&#65292;SAT&#25104;&#21151;&#22320;&#23558;&#22320;&#26631;&#20043;&#38388;&#30340;&#20851;&#31995;&#21644;&#20840;&#23616;&#24418;&#24577;&#29305;&#24449;&#34701;&#21512;&#65292;&#20351;&#39592;&#40836;&#35780;&#20272;&#30340;&#24179;&#22343;&#32477;&#23545;&#35823;&#24046;&#27604;&#20043;&#21069;&#30340;&#24037;&#20316;&#38477;&#20302;&#20102;0.11&#12290;
&lt;/p&gt;
&lt;p&gt;
This study presents a novel approach to bone age assessment (BAA) using a multi-view, multi-task classification model based on the Sauvegrain method. A straightforward solution to automating the Sauvegrain method, which assesses a maturity score for each landmark in the elbow and predicts the bone age, is to train classifiers independently to score each region of interest (RoI), but this approach limits the accessible information to local morphologies and increases computational costs. As a result, this work proposes a self-accumulative vision transformer (SAT) that mitigates anisotropic behavior, which usually occurs in multi-view, multi-task problems and limits the effectiveness of a vision transformer, by applying token replay and regional attention bias. A number of experiments show that SAT successfully exploits the relationships between landmarks and learns global morphological features, resulting in a mean absolute error of BAA that is 0.11 lower than that of the previous work. 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24314;&#31435;&#20102;&#19968;&#20010;&#21253;&#25324;&#23433;&#20840;&#12289;&#24212;&#29992;&#39046;&#22495;&#12289;&#26631;&#20934;&#27861;&#35268;&#31561;&#26041;&#38754;&#30340;&#20998;&#24067;&#24335;&#20998;&#31867;&#36134;&#25216;&#26415;&#30340;&#30693;&#35782;&#22270;&#35889;&#65292;&#26377;&#21161;&#20110;&#25299;&#23637;&#20154;&#20204;&#23545;&#35813;&#25216;&#26415;&#30340;&#29702;&#35299;&#21644;&#23547;&#25214;&#26032;&#30340;&#29992;&#20363;&#12290;</title><link>http://arxiv.org/abs/2303.16528</link><description>&lt;p&gt;
&#24314;&#31435;&#20998;&#24067;&#24335;&#20998;&#31867;&#36134;&#25216;&#26415;&#30693;&#35782;&#22270;&#35889;
&lt;/p&gt;
&lt;p&gt;
Building a Knowledge Graph of Distributed Ledger Technologies. (arXiv:2303.16528v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16528
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24314;&#31435;&#20102;&#19968;&#20010;&#21253;&#25324;&#23433;&#20840;&#12289;&#24212;&#29992;&#39046;&#22495;&#12289;&#26631;&#20934;&#27861;&#35268;&#31561;&#26041;&#38754;&#30340;&#20998;&#24067;&#24335;&#20998;&#31867;&#36134;&#25216;&#26415;&#30340;&#30693;&#35782;&#22270;&#35889;&#65292;&#26377;&#21161;&#20110;&#25299;&#23637;&#20154;&#20204;&#23545;&#35813;&#25216;&#26415;&#30340;&#29702;&#35299;&#21644;&#23547;&#25214;&#26032;&#30340;&#29992;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20998;&#24067;&#24335;&#20998;&#31867;&#36134;&#31995;&#32479;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#21644;&#25104;&#21151;&#25512;&#24191;&#65292;&#23588;&#20854;&#26159;&#21306;&#22359;&#38142;&#21644;&#21152;&#23494;&#36135;&#24065;&#26041;&#38754;&#12290;&#36825;&#23548;&#33268;&#20102;&#20154;&#20204;&#23545;&#35813;&#25216;&#26415;&#21450;&#20854;&#33021;&#21147;&#30340;&#21508;&#31181;&#35823;&#35299;&#65292;&#22240;&#20026;&#24456;&#22810;&#24773;&#20917;&#19979;&#65292;&#21306;&#22359;&#38142;&#21644;&#21152;&#23494;&#36135;&#24065;&#34987;&#35270;&#20316;&#21516;&#20041;&#35789;&#65292;&#20854;&#20182;&#29992;&#36884;&#20063;&#34987;&#32463;&#24120;&#24573;&#35270;&#12290;&#22240;&#27492;&#65292;&#23545;&#20998;&#24067;&#24335;&#20998;&#31867;&#36134;&#25216;&#26415;&#30340;&#35748;&#35782;&#21644;&#24212;&#29992;&#34987;&#38480;&#21046;&#20110;&#21306;&#22359;&#38142;&#21644;&#21152;&#23494;&#36135;&#24065;&#39046;&#22495;&#12290;&#29616;&#26377;&#30340;&#35789;&#27719;&#34920;&#21644;&#26412;&#20307;&#35770;&#24448;&#24448;&#21482;&#20851;&#27880;&#25216;&#26415;&#30340;&#26576;&#20123;&#26041;&#38754;&#65292;&#26377;&#26102;&#29978;&#33267;&#21482;&#20851;&#27880;&#21333;&#20010;&#20135;&#21697;&#65292;&#36825;&#21487;&#33021;&#20250;&#24573;&#30053;&#20854;&#20182;&#31867;&#22411;&#30340;&#20998;&#24067;&#24335;&#20998;&#31867;&#36134;&#21450;&#20854;&#28508;&#22312;&#30340;&#29992;&#36884;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#24067;&#24335;&#20998;&#31867;&#36134;&#25216;&#26415;&#30340;&#30693;&#35782;&#22270;&#35889;&#21644;&#26412;&#20307;&#35770;&#65292;&#21253;&#25324;&#23433;&#20840;&#32771;&#34385;&#12289;&#24212;&#29992;&#39046;&#22495;&#12289;&#30456;&#20851;&#26631;&#20934;&#21644;&#27861;&#35268;&#31561;&#65292;&#20197;&#25913;&#21892;&#23545;&#35813;&#25216;&#26415;&#30340;&#25972;&#20307;&#29702;&#35299;&#65292;&#24182;&#26377;&#21161;&#20110;&#21457;&#29616;&#21306;&#22359;&#38142;&#21644;&#21152;&#23494;&#36135;&#24065;&#20197;&#22806;&#30340;&#26032;&#29992;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
Distributed ledger systems have become more prominent and successful in recent years, with a focus on blockchains and cryptocurrency. This has led to various misunderstandings about both the technology itself and its capabilities, as in many cases blockchain and cryptocurrency is used synonymously and other applications are often overlooked. Therefore, as a whole, the view of distributed ledger technology beyond blockchains and cryptocurrencies is very limited. Existing vocabularies and ontologies often focus on single aspects of the technology, or in some cases even just on one product. This potentially leads to other types of distributed ledgers and their possible use cases being neglected. In this paper, we present a knowledge graph and an ontology for distributed ledger technologies, which includes security considerations to model aspects such as threats and vulnerabilities, application domains, as well as relevant standards and regulations. Such a knowledge graph improves the over
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#20260;&#21475;&#20998;&#31867;&#24037;&#20855;&#65292;&#21487;&#20197;&#21327;&#21161;&#21307;&#21153;&#20154;&#21592;&#23545;&#20116;&#31181;&#20851;&#38190;&#20260;&#21475;&#24773;&#20917;&#36827;&#34892;&#20998;&#31867;&#12290;&#20351;&#29992;&#20102;&#22810;&#20219;&#21153;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#20351;&#29992;Cohen's kappa&#31995;&#25968;&#26469;&#35780;&#20272;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#65292;&#27169;&#22411;&#30340;&#34920;&#29616;&#20248;&#20110;&#25110;&#19981;&#21155;&#20110;&#25152;&#26377;&#21307;&#21153;&#20154;&#21592;&#12290;</title><link>http://arxiv.org/abs/2303.16522</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#20260;&#21475;&#20998;&#31867;&#36741;&#21161;&#24037;&#20855;&#24320;&#21457;
&lt;/p&gt;
&lt;p&gt;
Development of a deep learning-based tool to assist wound classification. (arXiv:2303.16522v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16522
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#20260;&#21475;&#20998;&#31867;&#24037;&#20855;&#65292;&#21487;&#20197;&#21327;&#21161;&#21307;&#21153;&#20154;&#21592;&#23545;&#20116;&#31181;&#20851;&#38190;&#20260;&#21475;&#24773;&#20917;&#36827;&#34892;&#20998;&#31867;&#12290;&#20351;&#29992;&#20102;&#22810;&#20219;&#21153;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#20351;&#29992;Cohen's kappa&#31995;&#25968;&#26469;&#35780;&#20272;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#65292;&#27169;&#22411;&#30340;&#34920;&#29616;&#20248;&#20110;&#25110;&#19981;&#21155;&#20110;&#25152;&#26377;&#21307;&#21153;&#20154;&#21592;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#20260;&#21475;&#20998;&#31867;&#24037;&#20855;&#65292;&#33021;&#22815;&#24110;&#21161;&#38750;&#20260;&#21475;&#25252;&#29702;&#19987;&#19994;&#30340;&#21307;&#21153;&#20154;&#21592;&#23545;&#20116;&#31181;&#20851;&#38190;&#20260;&#21475;&#24773;&#20917;&#36827;&#34892;&#20998;&#31867;&#65292;&#21253;&#25324;&#28145;&#21019;&#21475;&#12289;&#24863;&#26579;&#24615;&#20260;&#21475;&#12289;&#21160;&#33033;&#24615;&#20260;&#21475;&#12289;&#38745;&#33033;&#24615;&#20260;&#21475;&#21644;&#21387;&#21147;&#24615;&#20260;&#21475;&#65292;&#20351;&#29992;&#26131;&#20110;&#33719;&#21462;&#30340;&#30456;&#26426;&#25293;&#25668;&#30340;&#24425;&#33394;&#22270;&#20687;&#12290;&#20998;&#31867;&#30340;&#20934;&#30830;&#24615;&#23545;&#20110;&#36866;&#24403;&#30340;&#20260;&#21475;&#31649;&#29702;&#33267;&#20851;&#37325;&#35201;&#12290;&#25152;&#25552;&#20986;&#30340;&#20260;&#21475;&#20998;&#31867;&#26041;&#27861;&#37319;&#29992;&#20102;&#22810;&#20219;&#21153;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#21033;&#29992;&#20116;&#31181;&#20851;&#38190;&#20260;&#21475;&#24773;&#20917;&#20043;&#38388;&#30340;&#20851;&#31995;&#26469;&#23454;&#29616;&#32479;&#19968;&#30340;&#20260;&#21475;&#20998;&#31867;&#26550;&#26500;&#12290;&#36890;&#36807;&#20351;&#29992;Cohen&#30340;kappa&#31995;&#25968;&#20316;&#20026;&#25351;&#26631;&#23558;&#25105;&#20204;&#30340;&#27169;&#22411;&#19982;&#20154;&#31867;&#36827;&#34892;&#27604;&#36739;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#30340;&#34920;&#29616;&#20248;&#20110;&#25110;&#19981;&#21155;&#20110;&#25152;&#26377;&#21307;&#21153;&#20154;&#21592;&#12290;&#25105;&#20204;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#26159;&#31532;&#19968;&#20010;&#33021;&#22815;&#21516;&#26102;&#20934;&#30830;&#20998;&#31867;&#28145;&#21019;&#21475;&#12289;&#24863;&#26579;&#24615;&#20260;&#21475;&#12289;&#21160;&#33033;&#24615;&#20260;&#21475;&#12289;&#38745;&#33033;&#24615;&#20260;&#21475;&#21644;&#21387;&#21147;&#24615;&#20260;&#21475;&#30340;&#27169;&#22411;&#12290;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#26159;&#21327;&#21161;&#21307;&#25252;&#20154;&#21592;&#36827;&#34892;&#20260;&#21475;&#20998;&#31867;&#30340;&#37325;&#35201;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a deep learning-based wound classification tool that can assist medical personnel in non-wound care specialization to classify five key wound conditions, namely deep wound, infected wound, arterial wound, venous wound, and pressure wound, given color images captured using readily available cameras. The accuracy of the classification is vital for appropriate wound management. The proposed wound classification method adopts a multi-task deep learning framework that leverages the relationships among the five key wound conditions for a unified wound classification architecture. With differences in Cohen's kappa coefficients as the metrics to compare our proposed model with humans, the performance of our model was better or non-inferior to those of all human medical personnel. Our convolutional neural network-based model is the first to classify five tasks of deep, infected, arterial, venous, and pressure wounds simultaneously with good accuracy. The proposed model is co
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#22312;&#23458;&#25143;&#36129;&#29486;&#35780;&#20272;&#19979;&#21516;&#26102;&#20248;&#21270;&#21327;&#20316;&#20844;&#24179;&#24615;&#21644;&#24615;&#33021;&#20844;&#24179;&#24615;&#65292;&#20197;&#33719;&#24471;&#39640;&#36136;&#37327;&#30340;&#20840;&#23616;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2303.16520</link><description>&lt;p&gt;
&#23458;&#25143;&#36129;&#29486;&#35780;&#20272;&#19979;&#30340;&#20844;&#24179;&#32852;&#37030;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Fair Federated Medical Image Segmentation via Client Contribution Estimation. (arXiv:2303.16520v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16520
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#22312;&#23458;&#25143;&#36129;&#29486;&#35780;&#20272;&#19979;&#21516;&#26102;&#20248;&#21270;&#21327;&#20316;&#20844;&#24179;&#24615;&#21644;&#24615;&#33021;&#20844;&#24179;&#24615;&#65292;&#20197;&#33719;&#24471;&#39640;&#36136;&#37327;&#30340;&#20840;&#23616;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20309;&#30830;&#20445;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#20844;&#24179;&#24615;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#35805;&#39064;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#25506;&#35752;&#20102;&#22914;&#20309;&#26681;&#25454;&#23458;&#25143;&#30340;&#36129;&#29486;&#65288;&#21327;&#20316;&#20844;&#24179;&#24615;&#65289;&#26469;&#22870;&#21169;&#23458;&#25143;&#65292;&#20197;&#21450;&#22914;&#20309;&#23454;&#29616;&#23458;&#25143;&#20043;&#38388;&#30340;&#24615;&#33021;&#22343;&#34913;&#65288;&#24615;&#33021;&#20844;&#24179;&#24615;&#65289;&#12290;&#23613;&#31649;&#24050;&#32463;&#22312;&#20854;&#20013;&#19968;&#20010;&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#25105;&#20204;&#35748;&#20026;&#23558;&#20108;&#32773;&#32771;&#34385;&#22312;&#19968;&#36215;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#20197;&#21560;&#24341;&#21644;&#28608;&#21169;&#26356;&#22810;&#19981;&#21516;&#31867;&#22411;&#30340;&#23458;&#25143;&#21152;&#20837;&#32852;&#37030;&#23398;&#20064;&#65292;&#20197;&#33719;&#24471;&#39640;&#36136;&#37327;&#30340;&#20840;&#23616;&#27169;&#22411;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21516;&#26102;&#20248;&#21270;&#20004;&#31181;&#20844;&#24179;&#24615;&#30340;&#26032;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26799;&#24230;&#21644;&#25968;&#25454;&#31354;&#38388;&#20013;&#30340;&#23458;&#25143;&#36129;&#29486;&#35780;&#20272;&#26041;&#27861;&#12290;&#22312;&#26799;&#24230;&#31354;&#38388;&#20013;&#65292;&#25105;&#20204;&#30417;&#27979;&#27599;&#20010;&#23458;&#25143;&#30456;&#23545;&#20110;&#20854;&#20182;&#23458;&#25143;&#30340;&#26799;&#24230;&#26041;&#21521;&#24046;&#24322;&#12290;&#22312;&#25968;&#25454;&#31354;&#38388;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#36741;&#21161;&#27169;&#22411;&#26469;&#27979;&#37327;&#23458;&#25143;&#25968;&#25454;&#30340;&#39044;&#27979;&#35823;&#24046;&#12290;&#22522;&#20110;&#36825;&#31181;&#36129;&#29486;&#35780;&#20272;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#21363;&#36890;&#36807;&#36129;&#29486;&#35780;&#20272;&#36827;&#34892;&#30340;&#32852;&#37030;&#35757;&#32451;&#65288;FedCE&#65289;&#65292;&#21363;&#20351;&#29992;&#20272;&#35745;&#30340;&#26041;&#24335;&#20316;&#20026;&#20840;&#23616;&#27169;&#22411;&#32858;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
How to ensure fairness is an important topic in federated learning (FL). Recent studies have investigated how to reward clients based on their contribution (collaboration fairness), and how to achieve uniformity of performance across clients (performance fairness). Despite achieving progress on either one, we argue that it is critical to consider them together, in order to engage and motivate more diverse clients joining FL to derive a high-quality global model. In this work, we propose a novel method to optimize both types of fairness simultaneously. Specifically, we propose to estimate client contribution in gradient and data space. In gradient space, we monitor the gradient direction differences of each client with respect to others. And in data space, we measure the prediction error on client data using an auxiliary model. Based on this contribution estimation, we propose a FL method, federated training via contribution estimation (FedCE), i.e., using estimation as global model agg
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20960;&#31181;&#22270;&#24418;&#25237;&#24433;&#26041;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#36873;&#23450;&#36866;&#24403;&#22270;&#24418;&#25237;&#24433;&#30340;&#25351;&#23548;&#26041;&#38024;&#12290;</title><link>http://arxiv.org/abs/2303.16519</link><description>&lt;p&gt;
&#20174;&#22270;&#24418;&#20844;&#29702;&#22238;&#21040;&#21521;&#37327;&#65306;&#35780;&#20272;&#22522;&#20110;&#22270;&#24418;&#26412;&#20307;&#23884;&#20837;&#30340;&#23646;&#24615;
&lt;/p&gt;
&lt;p&gt;
From axioms over graphs to vectors, and back again: evaluating the properties of graph-based ontology embeddings. (arXiv:2303.16519v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16519
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20960;&#31181;&#22270;&#24418;&#25237;&#24433;&#26041;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#36873;&#23450;&#36866;&#24403;&#22270;&#24418;&#25237;&#24433;&#30340;&#25351;&#23548;&#26041;&#38024;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24050;&#32463;&#21457;&#23637;&#20986;&#22810;&#31181;&#26041;&#27861;&#29983;&#25104;&#25551;&#36848;&#36923;&#36753;&#26412;&#20307;&#23884;&#20837;&#65292;&#24182;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#20351;&#29992;&#36825;&#20123;&#23884;&#20837;&#12290;&#23558;&#26412;&#20307;&#23884;&#20837;&#21040;&#22270;&#24418;&#32467;&#26500;&#20013;&#65292;&#21363;&#24341;&#20837;&#19968;&#32452;&#33410;&#28857;&#21644;&#36793;&#32536;&#29992;&#20110;&#21629;&#21517;&#23454;&#20307;&#21644;&#36923;&#36753;&#20844;&#29702;&#65292;&#28982;&#21518;&#24212;&#29992;&#22270;&#24418;&#23884;&#20837;&#23558;&#22270;&#24418;&#23884;&#20837;&#21040;$\mathbb{R}^n$&#20013;&#65292;&#26159;&#19968;&#31181;&#29983;&#25104;&#26412;&#20307;&#23884;&#20837;&#30340;&#26041;&#27861;&#12290;&#23884;&#20837;&#26412;&#20307;&#21040;&#22270;&#24418;&#20013;&#30340;&#26041;&#27861;&#65288;&#22270;&#24418;&#25237;&#24433;&#65289;&#20855;&#26377;&#19981;&#21516;&#30340;&#24418;&#24335;&#23646;&#24615;&#65292;&#28041;&#21450;&#23427;&#20204;&#21487;&#20197;&#21033;&#29992;&#30340;&#20844;&#24335;&#31867;&#22411;&#12289;&#25237;&#24433;&#26159;&#21542;&#21487;&#36870;&#20197;&#21450;&#26159;&#21542;&#21487;&#20197;&#24212;&#29992;&#20110;&#26029;&#35328;&#20844;&#24335;&#25110;&#20854;&#28436;&#32462;&#38381;&#21253;&#12290;&#25105;&#20204;&#23450;&#24615;&#22320;&#21644;&#23450;&#37327;&#22320;&#20998;&#26512;&#20102;&#24050;&#29992;&#20110;&#23884;&#20837;&#26412;&#20307;&#30340;&#20960;&#31181;&#22270;&#24418;&#25237;&#24433;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#22270;&#24418;&#25237;&#24433;&#23646;&#24615;&#23545;&#20174;&#26412;&#20307;&#23884;&#20837;&#20013;&#39044;&#27979;&#20844;&#24335;&#30340;&#34920;&#29616;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#19981;&#21516;&#30340;&#25237;&#24433;&#26041;&#27861;&#20043;&#38388;&#23384;&#22312;&#37325;&#22823;&#24046;&#24322;&#65292;&#24182;&#25552;&#20379;&#20102;&#36873;&#23450;&#36866;&#24403;&#22270;&#24418;&#25237;&#24433;&#30340;&#25351;&#23548;&#26041;&#38024;&#12290;
&lt;/p&gt;
&lt;p&gt;
Several approaches have been developed that generate embeddings for Description Logic ontologies and use these embeddings in machine learning. One approach of generating ontologies embeddings is by first embedding the ontologies into a graph structure, i.e., introducing a set of nodes and edges for named entities and logical axioms, and then applying a graph embedding to embed the graph in $\mathbb{R}^n$. Methods that embed ontologies in graphs (graph projections) have different formal properties related to the type of axioms they can utilize, whether the projections are invertible or not, and whether they can be applied to asserted axioms or their deductive closure. We analyze, qualitatively and quantitatively, several graph projection methods that have been used to embed ontologies, and we demonstrate the effect of the properties of graph projections on the performance of predicting axioms from ontology embeddings. We find that there are substantial differences between different proj
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#23616;&#37096;&#38544;&#24335;&#21464;&#25442;&#22120;&#65288;LIT&#65289;&#21644;&#32423;&#32852;LIT (CLIT) &#65292;&#32467;&#21512;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#22810;&#23610;&#24230;&#29305;&#24449;&#65292;&#23454;&#29616;&#20219;&#24847;&#23610;&#24230;&#36229;&#20998;&#36776;&#29575;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#21644;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.16513</link><description>&lt;p&gt;
&#38024;&#23545;&#20219;&#24847;&#23610;&#24230;&#36229;&#20998;&#36776;&#29575;&#30340;&#32423;&#32852;&#23616;&#37096;&#38544;&#24335;&#21464;&#25442;&#22120;
&lt;/p&gt;
&lt;p&gt;
Cascaded Local Implicit Transformer for Arbitrary-Scale Super-Resolution. (arXiv:2303.16513v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16513
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#23616;&#37096;&#38544;&#24335;&#21464;&#25442;&#22120;&#65288;LIT&#65289;&#21644;&#32423;&#32852;LIT (CLIT) &#65292;&#32467;&#21512;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#22810;&#23610;&#24230;&#29305;&#24449;&#65292;&#23454;&#29616;&#20219;&#24847;&#23610;&#24230;&#36229;&#20998;&#36776;&#29575;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#21644;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#23637;&#31034;&#20986;&#22312;&#34920;&#31034;&#20219;&#24847;&#20998;&#36776;&#29575;&#22270;&#20687;&#26041;&#38754;&#30340;&#26377;&#21147;&#33021;&#21147;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23616;&#37096;&#38544;&#24335;&#21464;&#25442;&#22120;&#65288;LIT&#65289;&#65292;&#23558;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#39057;&#29575;&#32534;&#30721;&#25216;&#26415;&#34701;&#21512;&#21040;&#23616;&#37096;&#38544;&#24335;&#22270;&#20687;&#20989;&#25968;&#20013;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#36328;&#23610;&#24230;&#30340;&#23616;&#37096;&#27880;&#24847;&#21147;&#22359;&#65292;&#20197;&#26377;&#25928;&#22320;&#32858;&#21512;&#23616;&#37096;&#29305;&#24449;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25552;&#39640;&#20195;&#34920;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22810;&#23610;&#24230;&#29305;&#24449;&#30340;&#32423;&#32852;LIT&#65288;CLIT&#65289;&#65292;&#20197;&#21450;&#36880;&#27493;&#22686;&#21152;&#19978;&#37319;&#26679;&#27604;&#20363;&#30340;&#32047;&#35745;&#35757;&#32451;&#31574;&#30053;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#20123;&#32452;&#20214;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#20998;&#26512;&#20102;&#21508;&#31181;&#35757;&#32451;&#31574;&#30053;&#12290;&#23450;&#24615;&#21644;&#23450;&#37327;&#32467;&#26524;&#34920;&#26126;&#65292;LIT&#21644;CLIT&#22312;&#20219;&#24847;&#36229;&#20998;&#36776;&#29575;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#32467;&#26524;&#65292;&#24182;&#36229;&#36807;&#20102;&#20808;&#21069;&#30340;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Implicit neural representation has recently shown a promising ability in representing images with arbitrary resolutions. In this paper, we present a Local Implicit Transformer (LIT), which integrates the attention mechanism and frequency encoding technique into a local implicit image function. We design a cross-scale local attention block to effectively aggregate local features. To further improve representative power, we propose a Cascaded LIT (CLIT) that exploits multi-scale features, along with a cumulative training strategy that gradually increases the upsampling scales during training. We have conducted extensive experiments to validate the effectiveness of these components and analyze various training strategies. The qualitative and quantitative results demonstrate that LIT and CLIT achieve favorable results and outperform the prior works in arbitrary super-resolution tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#38543;&#26426;&#26862;&#26519;&#27169;&#22411;&#22312;&#22810;&#30446;&#26631;&#22238;&#24402;&#20013;&#20135;&#29983;&#22522;&#20110;&#35268;&#21017;&#35299;&#37322;&#30340;&#25216;&#26415;&#65292;&#24182;&#22312;&#24191;&#27867;&#23454;&#39564;&#35780;&#20272;&#20013;&#35777;&#26126;&#20854;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.16506</link><description>&lt;p&gt;
&#38543;&#26426;&#26862;&#26519;&#22312;&#22810;&#30446;&#26631;&#22238;&#24402;&#20013;&#30340;&#23616;&#37096;&#21487;&#35299;&#37322;&#24615;
&lt;/p&gt;
&lt;p&gt;
Local Interpretability of Random Forests for Multi-Target Regression. (arXiv:2303.16506v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16506
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#38543;&#26426;&#26862;&#26519;&#27169;&#22411;&#22312;&#22810;&#30446;&#26631;&#22238;&#24402;&#20013;&#20135;&#29983;&#22522;&#20110;&#35268;&#21017;&#35299;&#37322;&#30340;&#25216;&#26415;&#65292;&#24182;&#22312;&#24191;&#27867;&#23454;&#39564;&#35780;&#20272;&#20013;&#35777;&#26126;&#20854;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#30446;&#26631;&#22238;&#24402;&#22312;&#20247;&#22810;&#24212;&#29992;&#20013;&#38750;&#24120;&#26377;&#29992;&#12290;&#34429;&#28982;&#38543;&#26426;&#26862;&#26519;&#27169;&#22411;&#22312;&#36825;&#20123;&#20219;&#21153;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#24448;&#24448;&#38590;&#20197;&#35299;&#37322;&#12290;&#21487;&#35299;&#37322;&#24615;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#38750;&#24120;&#37325;&#35201;&#65292;&#29305;&#21035;&#26159;&#24403;&#23427;&#33021;&#30452;&#25509;&#24433;&#21709;&#20154;&#31867;&#31119;&#31049;&#26102;&#26356;&#20026;&#37325;&#35201;&#12290;&#34429;&#28982;&#23384;&#22312;&#38024;&#23545;&#22810;&#30446;&#26631;&#22238;&#24402;&#30340;&#27169;&#22411;&#26080;&#20851;&#35299;&#37322;&#25216;&#26415;&#65292;&#20294;&#27809;&#26377;&#29305;&#23450;&#20110;&#38543;&#26426;&#26862;&#26519;&#27169;&#22411;&#30340;&#25216;&#26415;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25216;&#26415;&#65292;&#29992;&#20110;&#20026;&#38543;&#26426;&#26862;&#26519;&#27169;&#22411;&#22312;&#22810;&#30446;&#26631;&#22238;&#24402;&#20013;&#20135;&#29983;&#22522;&#20110;&#35268;&#21017;&#30340;&#35299;&#37322;&#65292;&#21463;&#26368;&#36817;&#29992;&#20110;&#38543;&#26426;&#26862;&#26519;&#35299;&#37322;&#24615;&#30340;&#27169;&#22411;&#29305;&#23450;&#25216;&#26415;&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#35780;&#20272;&#65292;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#25216;&#26415;&#22312;&#35299;&#37322;&#26041;&#38754;&#19982;&#26368;&#20808;&#36827;&#30340;&#25216;&#26415;&#30456;&#27604;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-target regression is useful in a plethora of applications. Although random forest models perform well in these tasks, they are often difficult to interpret. Interpretability is crucial in machine learning, especially when it can directly impact human well-being. Although model-agnostic techniques exist for multi-target regression, specific techniques tailored to random forest models are not available. To address this issue, we propose a technique that provides rule-based interpretations for instances made by a random forest model for multi-target regression, influenced by a recent model-specific technique for random forest interpretability. The proposed technique was evaluated through extensive experiments and shown to offer competitive interpretations compared to state-of-the-art techniques.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;XAI&#20013;&#30340;&#20449;&#20219;&#19982;&#19981;&#20449;&#20219;&#20043;&#38388;&#30340;&#28508;&#22312;&#21306;&#21035;&#65292;&#20026;&#25506;&#35752;&#19981;&#21516;&#30340;&#20449;&#20219;&#27010;&#24565;&#21270;&#21644;&#32570;&#20047;&#39564;&#35777;&#30340;&#20449;&#20219;&#38382;&#21367;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24605;&#32771;&#35282;&#24230;&#12290;</title><link>http://arxiv.org/abs/2303.16495</link><description>&lt;p&gt;
XAI&#20013;&#30340;&#19981;&#20449;&#20219;--&#27979;&#37327;&#24037;&#20855;&#36824;&#26159;&#19981;&#21516;&#30340;&#27010;&#24565;&#65311;
&lt;/p&gt;
&lt;p&gt;
Distrust in (X)AI -- Measurement Artifact or Distinct Construct?. (arXiv:2303.16495v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16495
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;XAI&#20013;&#30340;&#20449;&#20219;&#19982;&#19981;&#20449;&#20219;&#20043;&#38388;&#30340;&#28508;&#22312;&#21306;&#21035;&#65292;&#20026;&#25506;&#35752;&#19981;&#21516;&#30340;&#20449;&#20219;&#27010;&#24565;&#21270;&#21644;&#32570;&#20047;&#39564;&#35777;&#30340;&#20449;&#20219;&#38382;&#21367;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24605;&#32771;&#35282;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20449;&#20219;&#26159;&#24320;&#21457;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#30340;&#20851;&#38190;&#21160;&#26426;&#65292;&#28982;&#32780;&#65292;&#30740;&#31350;&#20154;&#21592;&#38754;&#20020;&#35768;&#22810;&#25361;&#25112;&#65292;&#20363;&#22914;&#19981;&#21516;&#30340;&#20449;&#20219;&#27010;&#24565;&#21270;&#21644;&#32570;&#20047;&#22312;AI&#19978;&#19979;&#25991;&#20013;&#32463;&#36807;&#39564;&#35777;&#30340;&#20449;&#20219;&#38382;&#21367;&#12290;&#35813;&#31687;&#20301;&#32622;&#35770;&#25991;&#30528;&#37325;&#20110;&#25506;&#35752;distrust&#20316;&#20026;&#19968;&#20010;&#26500;&#24314;&#21644;&#29420;&#31435;&#20110;&#20449;&#20219;&#30340;&#31532;&#20108;&#20010;&#26500;&#24314;&#20043;&#38388;&#30340;&#28508;&#22312;&#21306;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
Trust is a key motivation in developing explainable artificial intelligence (XAI). However, researchers attempting to measure trust in AI face numerous challenges, such as different trust conceptualizations, simplified experimental tasks that may not induce uncertainty as a prerequisite for trust, and the lack of validated trust questionnaires in the context of AI. While acknowledging these issues, we have identified a further challenge that currently seems underappreciated - the potential distinction between trust as one construct and \emph{distrust} as a second construct independent of trust. While there has been long-standing academic discourse for this distinction and arguments for both the one-dimensional and two-dimensional conceptualization of trust, distrust seems relatively understudied in XAI. In this position paper, we not only highlight the theoretical arguments for distrust as a distinct construct from trust but also contextualize psychometric evidence that likewise favors
&lt;/p&gt;</description></item><item><title>GNNBuilder&#26159;&#19968;&#20010;&#33258;&#21160;&#21270;&#30340;&#12289;&#36890;&#29992;&#30340;&#12289;&#31471;&#21040;&#31471;&#30340;GNN&#21152;&#36895;&#22120;&#29983;&#25104;&#26694;&#26550;&#65292;&#23427;&#21487;&#20197;&#20026;&#29992;&#25143;&#20219;&#24847;&#23450;&#20041;&#30340;&#24191;&#27867;&#30340;GNN&#27169;&#22411;&#33258;&#21160;&#29983;&#25104;&#21152;&#36895;&#22120;&#65292;&#19988;&#36816;&#34892;&#36895;&#24230;&#27604;&#36719;&#20214;&#22522;&#20934;&#24555;&#22810;&#36798;12.95&#20493;&#12290;</title><link>http://arxiv.org/abs/2303.16459</link><description>&lt;p&gt;
GNNBuilder&#65306;&#36890;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#21152;&#36895;&#22120;&#29983;&#25104;&#12289;&#27169;&#25311;&#21644;&#20248;&#21270;&#30340;&#33258;&#21160;&#21270;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
GNNBuilder: An Automated Framework for Generic Graph Neural Network Accelerator Generation, Simulation, and Optimization. (arXiv:2303.16459v1 [cs.AR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16459
&lt;/p&gt;
&lt;p&gt;
GNNBuilder&#26159;&#19968;&#20010;&#33258;&#21160;&#21270;&#30340;&#12289;&#36890;&#29992;&#30340;&#12289;&#31471;&#21040;&#31471;&#30340;GNN&#21152;&#36895;&#22120;&#29983;&#25104;&#26694;&#26550;&#65292;&#23427;&#21487;&#20197;&#20026;&#29992;&#25143;&#20219;&#24847;&#23450;&#20041;&#30340;&#24191;&#27867;&#30340;GNN&#27169;&#22411;&#33258;&#21160;&#29983;&#25104;&#21152;&#36895;&#22120;&#65292;&#19988;&#36816;&#34892;&#36895;&#24230;&#27604;&#36719;&#20214;&#22522;&#20934;&#24555;&#22810;&#36798;12.95&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#26377;&#24456;&#22810;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#21152;&#36895;&#22120;&#34987;&#25552;&#20986;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#39640;&#24230;&#20381;&#36182;&#20110;&#29992;&#25143;&#30340;&#30828;&#20214;&#19987;&#19994;&#30693;&#35782;&#65292;&#24182;&#19988;&#36890;&#24120;&#38024;&#23545;&#19968;&#31181;&#29305;&#23450;&#30340;GNN&#27169;&#22411;&#36827;&#34892;&#20248;&#21270;&#65292;&#20351;&#23427;&#20204;&#38590;&#20197;&#23454;&#38469;&#20351;&#29992;&#12290;&#22240;&#27492;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GNNBuilder&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#33258;&#21160;&#21270;&#30340;&#12289;&#36890;&#29992;&#30340;&#12289;&#31471;&#21040;&#31471;&#30340;GNN&#21152;&#36895;&#22120;&#29983;&#25104;&#26694;&#26550;&#12290;&#23427;&#20855;&#26377;&#22235;&#20010;&#20248;&#28857;&#65306;&#65288;1&#65289;GNNBuilder&#21487;&#20197;&#33258;&#21160;&#20026;&#29992;&#25143;&#20219;&#24847;&#23450;&#20041;&#30340;&#24191;&#27867;&#30340;GNN&#27169;&#22411;&#29983;&#25104;GNN&#21152;&#36895;&#22120;&#65307;&#65288;2&#65289;GNNBuilder&#37319;&#29992;&#26631;&#20934;&#30340;PyTorch&#32534;&#31243;&#25509;&#21475;&#65292;&#20026;&#31639;&#27861;&#24320;&#21457;&#20154;&#21592;&#25552;&#20379;&#38646;&#24320;&#38144;&#65307;&#65288;3&#65289;GNNBuilder&#25903;&#25345;&#31471;&#21040;&#31471;&#30340;&#20195;&#30721;&#29983;&#25104;&#12289;&#27169;&#25311;&#12289;&#21152;&#36895;&#22120;&#20248;&#21270;&#21644;&#30828;&#20214;&#37096;&#32626;&#65292;&#23454;&#29616;&#20102;GNN&#21152;&#36895;&#22120;&#35774;&#35745;&#30340;&#19968;&#38190;&#24335;&#25805;&#20316;&#65307;&#65288;4&#65289;GNNBuilder&#37197;&#22791;&#20102;&#20854;&#25152;&#29983;&#25104;&#30340;&#21152;&#36895;&#22120;&#30340;&#20934;&#30830;&#24615;&#33021;&#27169;&#22411;&#65292;&#20351;&#24471;&#35774;&#35745;&#31354;&#38388;&#25506;&#32034;&#65288;DSE&#65289;&#24555;&#36895;&#32780;&#28789;&#27963;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#39318;&#20808;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#21152;&#36895;&#22120;&#22312;6&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#19982;&#26368;&#20808;&#36827;&#30340;GNN&#21152;&#36895;&#22120;&#30456;&#27604;&#34920;&#29616;&#20986;&#33394;&#65292;&#36816;&#34892;&#36895;&#24230;&#27604;&#36719;&#20214;&#22522;&#20934;&#24555;&#20102;&#22810;&#36798;12.95&#20493;&#12290;&#20854;&#27425;&#65292;&#23545;&#19981;&#35268;&#21017;&#22270;&#22788;&#29702;&#30340;&#26696;&#20363;&#30740;&#31350;&#23637;&#31034;&#20102;GNNBuilder&#30340;&#20986;&#33394;&#21487;&#25193;&#23637;&#24615;&#21644;&#36866;&#24212;&#24615;&#12290;&#26368;&#21518;&#65292;&#22312;&#21333;&#20010;GPU&#30340;&#26381;&#21153;&#22120;&#19978;&#65292;&#25105;&#20204;&#23545;400&#20010;GNN&#27169;&#22411;&#36827;&#34892;&#20102;30&#20998;&#38047;&#30340;DSE&#30740;&#31350;&#65292;&#39564;&#35777;&#20102;GNNBuilder&#30340;&#25928;&#29575;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
There are plenty of graph neural network (GNN) accelerators being proposed. However, they highly rely on users' hardware expertise and are usually optimized for one specific GNN model, making them challenging for practical use . Therefore, in this work, we propose GNNBuilder, the first automated, generic, end-to-end GNN accelerator generation framework. It features four advantages: (1) GNNBuilder can automatically generate GNN accelerators for a wide range of GNN models arbitrarily defined by users; (2) GNNBuilder takes standard PyTorch programming interface, introducing zero overhead for algorithm developers; (3) GNNBuilder supports end-to-end code generation, simulation, accelerator optimization, and hardware deployment, realizing a push-button fashion for GNN accelerator design; (4) GNNBuilder is equipped with accurate performance models of its generated accelerator, enabling fast and flexible design space exploration (DSE). In the experiments, first, we show that our accelerator pe
&lt;/p&gt;</description></item><item><title>TaskMatrix.AI&#26159;&#19968;&#20010;&#20219;&#21153;&#23436;&#25104;&#31995;&#32479;&#65292;&#21487;&#20197;&#23558;&#22522;&#30784;&#27169;&#22411;&#19982;&#25968;&#30334;&#19975;&#20010;API&#36830;&#25509;&#65292;&#25552;&#39640;&#23436;&#25104;&#20219;&#21153;&#30340;&#25928;&#29575;&#21644;&#20840;&#38754;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.16434</link><description>&lt;p&gt;
TaskMatrix.AI&#65306;&#23558;&#22522;&#30784;&#27169;&#22411;&#19982;&#25968;&#30334;&#19975;&#20010;API&#36830;&#25509;&#20197;&#23436;&#25104;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
TaskMatrix.AI: Completing Tasks by Connecting Foundation Models with Millions of APIs. (arXiv:2303.16434v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16434
&lt;/p&gt;
&lt;p&gt;
TaskMatrix.AI&#26159;&#19968;&#20010;&#20219;&#21153;&#23436;&#25104;&#31995;&#32479;&#65292;&#21487;&#20197;&#23558;&#22522;&#30784;&#27169;&#22411;&#19982;&#25968;&#30334;&#19975;&#20010;API&#36830;&#25509;&#65292;&#25552;&#39640;&#23436;&#25104;&#20219;&#21153;&#30340;&#25928;&#29575;&#21644;&#20840;&#38754;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#36817;&#24180;&#26469;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#36827;&#23637;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20219;&#21153;&#23436;&#25104;&#31995;&#32479;TaskMatrix.AI&#65292;&#23427;&#21487;&#20197;&#33258;&#21160;&#23558;&#22522;&#30784;&#27169;&#22411;&#19982;&#25968;&#30334;&#19975;&#20010;API&#36830;&#25509;&#20197;&#23436;&#25104;&#19981;&#21516;&#30340;&#20219;&#21153;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;TaskMatrix.AI&#20351;&#29992;API&#25968;&#25454;&#38598;&#26469;&#23558;&#22522;&#30784;&#27169;&#22411;&#30340;&#36755;&#20837;&#19982;&#21487;&#29992;&#30340;API&#36827;&#34892;&#21305;&#37197;&#65292;&#20174;&#32780;&#23436;&#25104;&#20219;&#21153;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#24320;&#25918;&#22495;&#21644;&#39046;&#22495;&#29305;&#23450;&#20219;&#21153;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;TaskMatrix.AI&#30340;&#25928;&#26524;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial Intelligence (AI) has made incredible progress recently. On the one hand, advanced foundation models like ChatGPT can offer powerful conversation, in-context learning and code generation abilities on a broad range of open-domain tasks. They can also generate high-level solution outlines for domain-specific tasks based on the common sense knowledge they have acquired. However, they still face difficulties with some specialized tasks because they lack enough domain-specific data during pre-training or they often have errors in their neural network computations on those tasks that need accurate executions. On the other hand, there are also many existing models and systems (symbolic-based or neural-based) that can do some domain-specific tasks very well. However, due to the different implementation or working mechanisms, they are not easily accessible or compatible with foundation models. Therefore, there is a clear and pressing need for a mechanism that can leverage foundation 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Product Autoencoder&#26469;&#36890;&#36807;&#21487;&#31649;&#29702;&#30340;&#35757;&#32451;&#22797;&#26434;&#24615;&#23454;&#29616;&#30456;&#23545;&#36739;&#22823;&#30340;&#20195;&#30721;&#30340;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2303.16424</link><description>&lt;p&gt;
ProductAE&#65306;&#38754;&#21521;&#22823;&#32500;&#25968;&#30340;&#28145;&#24230;&#23398;&#20064;&#39537;&#21160;&#32416;&#38169;&#30721;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
ProductAE: Toward Deep Learning Driven Error-Correction Codes of Large Dimensions. (arXiv:2303.16424v1 [cs.IT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16424
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Product Autoencoder&#26469;&#36890;&#36807;&#21487;&#31649;&#29702;&#30340;&#35757;&#32451;&#22797;&#26434;&#24615;&#23454;&#29616;&#30456;&#23545;&#36739;&#22823;&#30340;&#20195;&#30721;&#30340;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20960;&#21313;&#24180;&#30340;&#29702;&#35770;&#30740;&#31350;&#24050;&#32463;&#21457;&#26126;&#20102;&#20960;&#20010;&#32416;&#38169;&#30721;&#31867;&#21035;&#65292;&#20294;&#36825;&#20123;&#30721;&#30340;&#35774;&#35745;&#21364;&#26159;&#19968;&#39033;&#26497;&#20855;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#20027;&#35201;&#20381;&#38752;&#20154;&#31867;&#26234;&#24935;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#36825;&#26679;&#30340;&#35774;&#35745;&#21487;&#20197;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#24037;&#20855;&#26377;&#25928;&#22320;&#33258;&#21160;&#21270;&#21644;&#21152;&#36895;&#65292;&#20174;&#32780;&#23454;&#29616;&#19982;&#32463;&#20856;&#35774;&#35745;&#30456;&#27604;&#20855;&#26377;&#33391;&#22909;&#24615;&#33021;&#22686;&#30410;&#30340;ML&#39537;&#21160;&#30340;&#32416;&#38169;&#30721;&#31867;&#21035;&#12290;&#28982;&#32780;&#65292;&#19968;&#20010;&#26681;&#26412;&#24615;&#30340;&#25361;&#25112;&#26159;&#65292;&#23545;&#20110;&#22823;&#30721;&#32500;&#24230;&#26469;&#35828;&#65292;&#35774;&#35745;&#21644;&#35757;&#32451;&#23436;&#20840;&#30340;ML&#39537;&#21160;&#30340;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#23545;&#26469;&#35828;&#26159;&#38750;&#24120;&#22797;&#26434;&#30340;&#65292;&#22914;&#26524;&#19981;&#26159;&#19981;&#21487;&#33021;&#30340;&#30340;&#35805;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Product Autoencoder&#65288;ProductAE&#65289;-&#19968;&#31181;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#28145;&#24230;&#23398;&#20064;&#39537;&#21160;&#65288;&#32534;&#30721;&#22120;&#65292;&#35299;&#30721;&#22120;&#65289;&#23545;&#30340;&#31995;&#21015;-&#26088;&#22312;&#36890;&#36807;&#21487;&#31649;&#29702;&#30340;&#35757;&#32451;&#22797;&#26434;&#24615;&#23454;&#29616;&#30456;&#23545;&#36739;&#22823;&#30340;&#20195;&#30721;&#65288;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#65289;&#30340;&#35757;&#32451;&#12290;&#25105;&#20204;&#20511;&#37492;&#20102;&#32463;&#20856;&#20056;&#31215;&#30721;&#30340;&#24605;&#24819;&#65292;&#24182;&#25552;&#20986;&#20102;&#20351;&#29992;ProductAE&#26500;&#24314;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#30340;&#32534;&#30721;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
While decades of theoretical research have led to the invention of several classes of error-correction codes, the design of such codes is an extremely challenging task, mostly driven by human ingenuity. Recent studies demonstrate that such designs can be effectively automated and accelerated via tools from machine learning (ML), thus enabling ML-driven classes of error-correction codes with promising performance gains compared to classical designs. A fundamental challenge, however, is that it is prohibitively complex, if not impossible, to design and train fully ML-driven encoder and decoder pairs for large code dimensions. In this paper, we propose Product Autoencoder (ProductAE) -- a computationally-efficient family of deep learning driven (encoder, decoder) pairs -- aimed at enabling the training of relatively large codes (both encoder and decoder) with a manageable training complexity. We build upon ideas from classical product codes and propose constructing large neural codes usin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#29289;&#29702;&#23646;&#24615;&#21644;&#21270;&#23398;&#25968;&#25454;&#39044;&#27979;&#20808;&#36827;&#26448;&#26009;&#30340;&#29305;&#24615;&#65292;&#22312;&#22788;&#29702;&#39640;&#32500;&#20449;&#24687;&#31354;&#38388;&#26041;&#38754;&#20855;&#26377;&#24456;&#22823;&#20248;&#21183;&#65292;&#25104;&#21151;&#39044;&#27979;&#20102;114210&#31181;&#32452;&#25104;&#26465;&#20214;&#19979;&#30340;913680&#20010;&#23646;&#24615;&#25968;&#25454;&#28857;&#65292;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#19981;&#21516;&#26448;&#26009;&#21644;&#23610;&#24230;&#12290;</title><link>http://arxiv.org/abs/2303.16412</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#39044;&#27979;&#20808;&#36827;&#26448;&#26009;&#22810;&#31181;&#29305;&#24615;&#30340;&#32508;&#21512;&#22810;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive and Versatile Multimodal Deep Learning Approach for Predicting Diverse Properties of Advanced Materials. (arXiv:2303.16412v1 [cond-mat.soft])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16412
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#29289;&#29702;&#23646;&#24615;&#21644;&#21270;&#23398;&#25968;&#25454;&#39044;&#27979;&#20808;&#36827;&#26448;&#26009;&#30340;&#29305;&#24615;&#65292;&#22312;&#22788;&#29702;&#39640;&#32500;&#20449;&#24687;&#31354;&#38388;&#26041;&#38754;&#20855;&#26377;&#24456;&#22823;&#20248;&#21183;&#65292;&#25104;&#21151;&#39044;&#27979;&#20102;114210&#31181;&#32452;&#25104;&#26465;&#20214;&#19979;&#30340;913680&#20010;&#23646;&#24615;&#25968;&#25454;&#28857;&#65292;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#19981;&#21516;&#26448;&#26009;&#21644;&#23610;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#65288;MDL&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#21512;&#24182;&#29289;&#29702;&#23646;&#24615;&#21644;&#21270;&#23398;&#25968;&#25454;&#26469;&#39044;&#27979;10&#32500;&#19993;&#28911;&#37240;&#32858;&#21512;&#29289;&#22797;&#21512;&#26448;&#26009;&#30340;&#29289;&#29702;&#29305;&#24615;&#12290;&#25105;&#20204;&#30340;MDL&#27169;&#22411;&#21253;&#25324;&#22235;&#20010;&#27169;&#22359;&#65292;&#20854;&#20013;&#19977;&#20010;&#27169;&#22359;&#26159;&#29992;&#20110;&#26448;&#26009;&#32467;&#26500;&#34920;&#24449;&#30340;&#29983;&#25104;&#24335;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#31532;&#22235;&#20010;&#27169;&#22359;&#29992;&#20110;&#23646;&#24615;&#39044;&#27979;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22788;&#29702;&#20102;18&#32500;&#30340;&#22797;&#26434;&#24615;&#65292;&#21253;&#25324;10&#20010;&#32452;&#25104;&#36755;&#20837;&#21644;8&#20010;&#23646;&#24615;&#36755;&#20986;&#65292;&#25104;&#21151;&#39044;&#27979;&#20102;114210&#31181;&#32452;&#25104;&#26465;&#20214;&#19979;&#30340;913680&#20010;&#23646;&#24615;&#25968;&#25454;&#28857;&#12290;&#23545;&#20110;&#35745;&#31639;&#26448;&#26009;&#31185;&#23398;&#29305;&#21035;&#26159;&#23545;&#20110;&#32467;&#26500;&#26410;&#23450;&#20041;&#30340;&#26448;&#26009;&#65292;&#36825;&#31181;&#22797;&#26434;&#24615;&#26159;&#21069;&#25152;&#26410;&#26377;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#20998;&#26512;&#39640;&#32500;&#20449;&#24687;&#31354;&#38388;&#65292;&#20197;&#23454;&#29616;&#36870;&#21521;&#26448;&#26009;&#35774;&#35745;&#65292;&#23637;&#31034;&#20102;&#23545;&#21508;&#31181;&#26448;&#26009;&#21644;&#23610;&#24230;&#30340;&#28789;&#27963;&#24615;&#21644;&#36866;&#24212;&#24615;&#65292;&#21069;&#25552;&#26159;&#26377;&#36275;&#22815;&#30340;&#25968;&#25454;&#12290;&#36825;&#39033;&#30740;&#31350;&#25512;&#21160;&#20102;&#26410;&#26469;&#30740;&#31350;&#19981;&#21516;&#26448;&#26009;&#21644;&#24320;&#21457;&#26356;&#22797;&#26434;&#30340;MDL&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a multimodal deep learning (MDL) framework for predicting physical properties of a 10-dimensional acrylic polymer composite material by merging physical attributes and chemical data. Our MDL model comprises four modules, including three generative deep learning models for material structure characterization and a fourth model for property prediction. Our approach handles an 18-dimensional complexity, with 10 compositional inputs and 8 property outputs, successfully predicting 913,680 property data points across 114,210 composition conditions. This level of complexity is unprecedented in computational materials science, particularly for materials with undefined structures. We propose a framework to analyze the high-dimensional information space for inverse material design, demonstrating flexibility and adaptability to various materials and scales, provided sufficient data is available. This study advances future research on different materials and the development of more soph
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LMDA-Net&#30340;&#36731;&#37327;&#32423;&#22810;&#32500;&#27880;&#24847;&#21147;&#32593;&#32476;&#65292;&#26088;&#22312;&#25552;&#39640;&#36890;&#29992;EEG&#33041;&#26426;&#25509;&#21475;&#33539;&#20363;&#30340;&#20998;&#31867;&#24615;&#33021;&#12290;&#35813;&#32593;&#32476;&#20351;&#29992;&#20004;&#20010;&#26032;&#22411;&#27880;&#24847;&#21147;&#27169;&#22359;&#8212;&#8212;&#36890;&#36947;&#27880;&#24847;&#21147;&#27169;&#22359;&#21644;&#28145;&#24230;&#27880;&#24847;&#21147;&#27169;&#22359;&#65292;&#24182;&#22312;&#22810;&#20010;&#20844;&#24320;&#39640;&#24433;&#21709;&#21147;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;LMDA-Net&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#20195;&#34920;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#20915;&#31574;&#36807;&#31243;&#30340;&#39640;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.16407</link><description>&lt;p&gt;
LMDA-Net: &#29992;&#20110;&#36890;&#29992;EEG&#33041;&#26426;&#25509;&#21475;&#33539;&#20363;&#30340;&#36731;&#37327;&#32423;&#22810;&#32500;&#27880;&#24847;&#21147;&#32593;&#32476;&#21450;&#20854;&#21487;&#35299;&#37322;&#24615;
&lt;/p&gt;
&lt;p&gt;
LMDA-Net:A lightweight multi-dimensional attention network for general EEG-based brain-computer interface paradigms and interpretability. (arXiv:2303.16407v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16407
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LMDA-Net&#30340;&#36731;&#37327;&#32423;&#22810;&#32500;&#27880;&#24847;&#21147;&#32593;&#32476;&#65292;&#26088;&#22312;&#25552;&#39640;&#36890;&#29992;EEG&#33041;&#26426;&#25509;&#21475;&#33539;&#20363;&#30340;&#20998;&#31867;&#24615;&#33021;&#12290;&#35813;&#32593;&#32476;&#20351;&#29992;&#20004;&#20010;&#26032;&#22411;&#27880;&#24847;&#21147;&#27169;&#22359;&#8212;&#8212;&#36890;&#36947;&#27880;&#24847;&#21147;&#27169;&#22359;&#21644;&#28145;&#24230;&#27880;&#24847;&#21147;&#27169;&#22359;&#65292;&#24182;&#22312;&#22810;&#20010;&#20844;&#24320;&#39640;&#24433;&#21709;&#21147;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;LMDA-Net&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#20195;&#34920;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#20915;&#31574;&#36807;&#31243;&#30340;&#39640;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;EEG&#30340;&#27963;&#21160;&#21644;&#29366;&#24577;&#35782;&#21035;&#38656;&#35201;&#20351;&#29992;&#31070;&#32463;&#31185;&#23398;&#20808;&#21069;&#30693;&#35782;&#29983;&#25104;&#23450;&#37327;EEG&#29305;&#24449;&#65292;&#36825;&#21487;&#33021;&#38480;&#21046;BCI&#30340;&#24615;&#33021;&#12290;&#34429;&#28982;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#25552;&#21462;&#29305;&#24449;&#65292;&#20294;&#23427;&#20204;&#32463;&#24120;&#20250;&#36935;&#21040;&#35832;&#22914;&#36328;&#25968;&#25454;&#38598;&#30340;&#36890;&#29992;&#24615;&#24046;&#12289;&#39640;&#39044;&#27979;&#27874;&#21160;&#24615;&#21644;&#20302;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#31561;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36731;&#37327;&#32423;&#22810;&#32500;&#27880;&#24847;&#21147;&#32593;&#32476;&#65292;&#31216;&#20026;LMDA-Net&#12290;&#36890;&#36807;&#32467;&#21512;&#20004;&#20010;&#19987;&#20026;EEG&#20449;&#21495;&#35774;&#35745;&#30340;&#26032;&#22411;&#27880;&#24847;&#21147;&#27169;&#22359;&#8212;&#8212;&#36890;&#36947;&#27880;&#24847;&#21147;&#27169;&#22359;&#21644;&#28145;&#24230;&#27880;&#24847;&#21147;&#27169;&#22359;&#65292;LMDA-Net&#21487;&#20197;&#26377;&#25928;&#22320;&#25972;&#21512;&#26469;&#33258;&#22810;&#20010;&#32500;&#24230;&#30340;&#29305;&#24449;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#22312;&#21508;&#31181;BCI&#20219;&#21153;&#19978;&#30340;&#20998;&#31867;&#24615;&#33021;&#12290; LMDA-Net&#22312;&#21253;&#25324;&#36816;&#21160;&#24819;&#35937;&#65288;MI&#65289;&#21644;P300-Speller&#33539;&#20363;&#22312;&#20869;&#30340;&#22235;&#20010;&#20844;&#24320;&#39640;&#24433;&#21709;&#21147;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#24182;&#19982;&#20854;&#20182;&#20195;&#34920;&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;LMDA-Net&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#20195;&#34920;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#20915;&#31574;&#36807;&#31243;&#30340;&#39640;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
EEG-based recognition of activities and states involves the use of prior neuroscience knowledge to generate quantitative EEG features, which may limit BCI performance. Although neural network-based methods can effectively extract features, they often encounter issues such as poor generalization across datasets, high predicting volatility, and low model interpretability. Hence, we propose a novel lightweight multi-dimensional attention network, called LMDA-Net. By incorporating two novel attention modules designed specifically for EEG signals, the channel attention module and the depth attention module, LMDA-Net can effectively integrate features from multiple dimensions, resulting in improved classification performance across various BCI tasks. LMDA-Net was evaluated on four high-impact public datasets, including motor imagery (MI) and P300-Speller paradigms, and was compared with other representative models. The experimental results demonstrate that LMDA-Net outperforms other represen
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;HiREST&#25968;&#25454;&#38598;&#21644;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#65292;&#23558;&#20998;&#23618;&#20449;&#24687;&#26816;&#32034;&#21644;&#35270;&#35273;/&#25991;&#26412;&#36880;&#27493;&#24635;&#32467;&#20174;&#25945;&#23398;&#35270;&#39057;&#35821;&#26009;&#24211;&#20013;&#25512;&#24191;&#65292;&#20351;&#24471;&#22312;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#35774;&#32622;&#19979;&#21487;&#20197;&#20849;&#21516;&#25628;&#32034;&#35270;&#39057;&#35821;&#26009;&#24211;&#65292;&#24182;&#29983;&#25104;&#25688;&#35201;&#12290;</title><link>http://arxiv.org/abs/2303.16406</link><description>&lt;p&gt;
&#23618;&#27425;&#21270;&#35270;&#39057;&#30636;&#38388;&#26816;&#32034;&#21644;&#20998;&#27493;&#23383;&#24149;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Video-Moment Retrieval and Step-Captioning. (arXiv:2303.16406v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16406
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;HiREST&#25968;&#25454;&#38598;&#21644;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#65292;&#23558;&#20998;&#23618;&#20449;&#24687;&#26816;&#32034;&#21644;&#35270;&#35273;/&#25991;&#26412;&#36880;&#27493;&#24635;&#32467;&#20174;&#25945;&#23398;&#35270;&#39057;&#35821;&#26009;&#24211;&#20013;&#25512;&#24191;&#65292;&#20351;&#24471;&#22312;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#35774;&#32622;&#19979;&#21487;&#20197;&#20849;&#21516;&#25628;&#32034;&#35270;&#39057;&#35821;&#26009;&#24211;&#65292;&#24182;&#29983;&#25104;&#25688;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#20154;&#20204;&#22312;&#23547;&#25214;&#22823;&#22411;&#35270;&#39057;&#35821;&#26009;&#24211;&#20013;&#30340;&#20449;&#24687;&#26041;&#38754;&#36234;&#26469;&#36234;&#24863;&#20852;&#36259;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#29420;&#31435;&#30740;&#31350;&#20102;&#30456;&#20851;&#20219;&#21153;&#65292;&#22914;&#22522;&#20110;&#25991;&#26412;&#30340;&#35270;&#39057;&#26816;&#32034;&#12289;&#30636;&#38388;&#26816;&#32034;&#12289;&#35270;&#39057;&#25688;&#35201;&#21644;&#35270;&#39057;&#23383;&#24149;&#65292;&#27809;&#26377;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#35774;&#32622;&#21487;&#20197;&#20849;&#21516;&#25628;&#32034;&#35270;&#39057;&#35821;&#26009;&#24211;&#65292;&#24182;&#29983;&#25104;&#25688;&#35201;&#12290;&#36825;&#26679;&#30340;&#31471;&#21040;&#31471;&#35774;&#32622;&#23558;&#20801;&#35768;&#35768;&#22810;&#26377;&#36259;&#30340;&#24212;&#29992;&#31243;&#24207;&#65292;&#20363;&#22914;&#22522;&#20110;&#25991;&#26412;&#30340;&#25628;&#32034;&#65292;&#20174;&#35270;&#39057;&#35821;&#26009;&#24211;&#20013;&#25214;&#21040;&#30456;&#20851;&#30340;&#35270;&#39057;&#65292;&#25552;&#21462;&#26368;&#30456;&#20851;&#30340;&#30636;&#38388;&#65292;&#24182;&#23558;&#30636;&#38388;&#20998;&#25104;&#37325;&#35201;&#30340;&#27493;&#39588;&#65292;&#24182;&#21152;&#19978;&#23383;&#24149;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;HiREST(Hierarchical REtrieval and STep-captioning)&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#65292;&#28085;&#30422;&#20102;&#26469;&#33258;&#25945;&#23398;&#35270;&#39057;&#35821;&#26009;&#24211;&#30340;&#20998;&#23618;&#20449;&#24687;&#26816;&#32034;&#21644;&#35270;&#35273;/&#25991;&#26412;&#20998;&#38454;&#27573;&#24635;&#32467;&#12290;HiREST&#30001;&#26469;&#33258;&#25945;&#23398;&#35270;&#39057;&#25968;&#25454;&#38598;&#30340;3.4K&#20010;&#25991;&#26412;-&#35270;&#39057;&#23545;&#32452;&#25104;&#65292;&#20854;&#20013;1.1K&#20010;&#35270;&#39057;&#20855;&#26377;&#19982;&#25991;&#26412;&#26597;&#35810;&#30456;&#20851;&#30340;&#30636;&#38388;&#36328;&#24230;&#27880;&#37322;&#21644;&#32454;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
There is growing interest in searching for information from large video corpora. Prior works have studied relevant tasks, such as text-based video retrieval, moment retrieval, video summarization, and video captioning in isolation, without an end-to-end setup that can jointly search from video corpora and generate summaries. Such an end-to-end setup would allow for many interesting applications, e.g., a text-based search that finds a relevant video from a video corpus, extracts the most relevant moment from that video, and segments the moment into important steps with captions. To address this, we present the HiREST (HIerarchical REtrieval and STep-captioning) dataset and propose a new benchmark that covers hierarchical information retrieval and visual/textual stepwise summarization from an instructional video corpus. HiREST consists of 3.4K text-video pairs from an instructional video dataset, where 1.1K videos have annotations of moment spans relevant to text query and breakdown of e
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#26597;&#35810;&#26469;&#36827;&#34892;&#38899;&#39057;&#28304;&#20998;&#31163;&#65292;&#23454;&#29616;&#20102;&#35821;&#35328;&#12289;&#35270;&#35273;&#21644;&#38899;&#39057;&#30340;&#19968;&#33268;&#24615;&#23545;&#40784;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.16342</link><description>&lt;p&gt;
&#21033;&#29992;&#35821;&#35328;&#25351;&#23548;&#30340;&#19977;&#27169;&#24577;&#19968;&#33268;&#24615;&#36827;&#34892;&#38899;&#39057;-&#35270;&#39057;&#28304;&#20998;&#31163;
&lt;/p&gt;
&lt;p&gt;
Language-Guided Audio-Visual Source Separation via Trimodal Consistency. (arXiv:2303.16342v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16342
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#26597;&#35810;&#26469;&#36827;&#34892;&#38899;&#39057;&#28304;&#20998;&#31163;&#65292;&#23454;&#29616;&#20102;&#35821;&#35328;&#12289;&#35270;&#35273;&#21644;&#38899;&#39057;&#30340;&#19968;&#33268;&#24615;&#23545;&#40784;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#26597;&#35810;&#22312;&#35270;&#39057;&#20013;&#23398;&#20064;&#25191;&#34892;&#38899;&#39057;&#28304;&#20998;&#31163;&#65292;&#20165;&#20351;&#29992;&#26410;&#26631;&#35760;&#30340;&#35270;&#39057;&#21644;&#38899;&#39057;&#23545;&#20316;&#20026;&#35757;&#32451;&#25968;&#25454;&#12290;&#36825;&#39033;&#20219;&#21153;&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#26159;&#23398;&#20064;&#23558;&#21457;&#22768;&#29289;&#20307;&#30340;&#35821;&#35328;&#25551;&#36848;&#19982;&#20854;&#35270;&#35273;&#29305;&#24449;&#21644;&#30456;&#24212;&#30340;&#38899;&#39057;&#27874;&#24418;&#32452;&#20214;&#32852;&#31995;&#36215;&#26469;&#65292;&#32780;&#22312;&#35757;&#32451;&#26399;&#38388;&#27809;&#26377;&#35775;&#38382;&#27880;&#37322;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25913;&#36827;&#20102;&#29616;&#25104;&#30340;&#35270;&#35273;&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#65292;&#36890;&#36807;&#20004;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#25552;&#20379;&#20266;&#30446;&#26631;&#30417;&#30563;&#65292;&#24182;&#20419;&#36827;&#38899;&#39057;&#12289;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#27169;&#24577;&#20043;&#38388;&#26356;&#24378;&#30340;&#23545;&#40784;&#12290;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#32473;&#23450;&#25991;&#26412;&#12289;&#35270;&#39057;&#21644;&#38899;&#39057;&#36755;&#20837;&#25110;&#20165;&#32473;&#23450;&#25991;&#26412;&#21644;&#38899;&#39057;&#36755;&#20837;&#26102;&#20998;&#31163;&#22768;&#38899;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#38899;&#39057;-&#35270;&#39057;&#20998;&#31163;&#25968;&#25454;&#38598;&#65288;&#21253;&#25324;MUSIC&#12289;SOLOS&#21644;AudioSet&#65289;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#33258;&#30417;&#30563;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#20854;&#20013;&#25105;&#20204;&#30340;&#27169;&#22411;&#32988;&#36807;&#20102;&#29616;&#26377;&#24378;&#30417;&#30563;&#26041;&#27861;&#30340;&#26368;&#26032;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a self-supervised approach for learning to perform audio source separation in videos based on natural language queries, using only unlabeled video and audio pairs as training data. A key challenge in this task is learning to associate the linguistic description of a sound-emitting object to its visual features and the corresponding components of the audio waveform, all without access to annotations during training. To overcome this challenge, we adapt off-the-shelf vision-language foundation models to provide pseudo-target supervision via two novel loss functions and encourage a stronger alignment between the audio, visual and natural language modalities. During inference, our approach can separate sounds given text, video and audio input, or given text and audio input alone. We demonstrate the effectiveness of our self-supervised approach on three audio-visual separation datasets, including MUSIC, SOLOS and AudioSet, where we outperform state-of-the-art strongly supervised 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19977;&#31181;&#26412;&#22320;&#32531;&#23384;&#26356;&#26032;&#35268;&#21017;&#26469;&#31649;&#29702;&#21160;&#24577;&#25968;&#25454;&#20998;&#24067;&#21644;&#26377;&#38480;&#30340;&#32531;&#23384;&#23481;&#37327;&#65292;&#20197;&#24212;&#23545;&#27969;&#24335;&#32852;&#37030;&#23398;&#20064;&#20013;&#26412;&#22320;&#35757;&#32451;&#25968;&#25454;&#38598;&#19982;&#38271;&#26399;&#25968;&#25454;&#20998;&#24067;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#25105;&#20204;&#36824;&#25512;&#23548;&#20986;&#20102;&#35813;&#31639;&#27861;&#30340;&#25910;&#25947;&#30028;&#38480;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#30340;&#31639;&#27861;&#25928;&#26524;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2303.16340</link><description>&lt;p&gt;
&#20851;&#20110;&#27969;&#24335;&#32852;&#37030;&#23398;&#20064;&#20013;&#26412;&#22320;&#32531;&#23384;&#26356;&#26032;&#35268;&#21017;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Local Cache Update Rules in Streaming Federated Learning. (arXiv:2303.16340v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16340
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19977;&#31181;&#26412;&#22320;&#32531;&#23384;&#26356;&#26032;&#35268;&#21017;&#26469;&#31649;&#29702;&#21160;&#24577;&#25968;&#25454;&#20998;&#24067;&#21644;&#26377;&#38480;&#30340;&#32531;&#23384;&#23481;&#37327;&#65292;&#20197;&#24212;&#23545;&#27969;&#24335;&#32852;&#37030;&#23398;&#20064;&#20013;&#26412;&#22320;&#35757;&#32451;&#25968;&#25454;&#38598;&#19982;&#38271;&#26399;&#25968;&#25454;&#20998;&#24067;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#25105;&#20204;&#36824;&#25512;&#23548;&#20986;&#20102;&#35813;&#31639;&#27861;&#30340;&#25910;&#25947;&#30028;&#38480;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#30340;&#31639;&#27861;&#25928;&#26524;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#27969;&#24335;&#32852;&#37030;&#23398;&#20064;&#65288;SFL&#65289;&#39046;&#22495;&#65292;&#25552;&#20986;&#20102;&#26412;&#22320;&#32531;&#23384;&#26356;&#26032;&#35268;&#21017;&#65292;&#20197;&#31649;&#29702;&#21160;&#24577;&#25968;&#25454;&#20998;&#24067;&#21644;&#26377;&#38480;&#30340;&#32531;&#23384;&#23481;&#37327;&#12290;&#20256;&#32479;&#30340;&#32852;&#37030;&#23398;&#20064;&#20381;&#36182;&#20110;&#22266;&#23450;&#30340;&#25968;&#25454;&#38598;&#65292;&#32780;&#22312;SFL&#20013;&#65292;&#25968;&#25454;&#26159;&#27969;&#24335;&#30340;&#65292;&#24182;&#19988;&#20854;&#20998;&#24067;&#38543;&#26102;&#38388;&#32780;&#21464;&#21270;&#65292;&#23548;&#33268;&#26412;&#22320;&#35757;&#32451;&#25968;&#25454;&#38598;&#19982;&#38271;&#26399;&#20998;&#24067;&#20043;&#38388;&#23384;&#22312;&#24046;&#24322;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#20010;&#26412;&#22320;&#32531;&#23384;&#26356;&#26032;&#35268;&#21017;&#8212;&#8212;&#20808;&#36827;&#20808;&#20986;&#65288;FIFO&#65289;&#12289;&#38745;&#24577;&#27604;&#20363;&#36873;&#25321;&#24615;&#26367;&#25442;&#65288;SRSR&#65289;&#21644;&#21160;&#24577;&#27604;&#20363;&#36873;&#25321;&#24615;&#26367;&#25442;&#65288;DRSR&#65289;&#8212;&#8212;&#22312;&#32771;&#34385;&#26377;&#38480;&#30340;&#32531;&#23384;&#23481;&#37327;&#30340;&#21516;&#26102;&#26356;&#26032;&#27599;&#20010;&#23458;&#25143;&#31471;&#30340;&#26412;&#22320;&#32531;&#23384;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25512;&#23548;&#20986;&#20102;&#22522;&#20110;&#38271;&#26399;&#25968;&#25454;&#20998;&#24067;&#21644;&#23458;&#25143;&#31471;&#26412;&#22320;&#35757;&#32451;&#25968;&#25454;&#38598;&#20043;&#38388;&#20998;&#24067;&#19981;&#19968;&#33268;&#24230;&#30340;&#25910;&#25947;&#30028;&#38480;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;&#65306;&#32593;&#32476;&#27969;&#37327;&#20998;&#31867;&#25968;&#25454;&#38598;&#21644;&#22270;&#20687;&#20998;&#31867;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we address the emerging field of Streaming Federated Learning (SFL) and propose local cache update rules to manage dynamic data distributions and limited cache capacity. Traditional federated learning relies on fixed data sets, whereas in SFL, data is streamed, and its distribution changes over time, leading to discrepancies between the local training dataset and long-term distribution. To mitigate this problem, we propose three local cache update rules - First-In-First-Out (FIFO), Static Ratio Selective Replacement (SRSR), and Dynamic Ratio Selective Replacement (DRSR) - that update the local cache of each client while considering the limited cache capacity. Furthermore, we derive a convergence bound for our proposed SFL algorithm as a function of the distribution discrepancy between the long-term data distribution and the client's local training dataset. We then evaluate our proposed algorithm on two datasets: a network traffic classification dataset and an image class
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#24555;&#36895;&#22810;&#30446;&#26631;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#26694;&#26550;FMAS&#65292;&#25628;&#32034;&#26377;&#25928;&#30340;&#35821;&#20041;&#20998;&#21106;&#27169;&#22411;&#65292;&#33021;&#22815;&#24555;&#36895;&#25214;&#21040;&#26377;&#31454;&#20105;&#21147;&#30340;&#35774;&#35745;&#65292;&#24182;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#25214;&#21040;&#26356;&#24555;&#30340;&#32593;&#32476;&#12290;</title><link>http://arxiv.org/abs/2303.16322</link><description>&lt;p&gt;
FMAS&#65306;&#29992;&#20110;&#35821;&#20041;&#20998;&#21106;&#30340;&#24555;&#36895;&#22810;&#30446;&#26631;&#36229;&#32423;&#32593;&#32476;&#26550;&#26500;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
FMAS: Fast Multi-Objective SuperNet Architecture Search for Semantic Segmentation. (arXiv:2303.16322v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16322
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#24555;&#36895;&#22810;&#30446;&#26631;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#26694;&#26550;FMAS&#65292;&#25628;&#32034;&#26377;&#25928;&#30340;&#35821;&#20041;&#20998;&#21106;&#27169;&#22411;&#65292;&#33021;&#22815;&#24555;&#36895;&#25214;&#21040;&#26377;&#31454;&#20105;&#21147;&#30340;&#35774;&#35745;&#65292;&#24182;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#25214;&#21040;&#26356;&#24555;&#30340;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;FMAS&#65292;&#19968;&#31181;&#38024;&#23545;&#35821;&#20041;&#20998;&#21106;&#30340;&#24555;&#36895;&#22810;&#30446;&#26631;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#26694;&#26550;&#12290;FMAS&#23545;DeepLabV3+&#30340;&#32467;&#26500;&#21644;&#39044;&#35757;&#32451;&#21442;&#25968;&#36827;&#34892;&#23376;&#37319;&#26679;&#65292;&#26080;&#38656;&#24494;&#35843;&#65292;&#22823;&#22823;&#20943;&#23569;&#25628;&#32034;&#26399;&#38388;&#30340;&#35757;&#32451;&#26102;&#38388;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#38477;&#20302;&#20505;&#36873;&#27169;&#22411;&#30340;&#35780;&#20272;&#26102;&#38388;&#65292;&#22312;&#25628;&#32034;&#36807;&#31243;&#20013;&#20165;&#20351;&#29992;&#39564;&#35777;&#38598;&#30340;&#23376;&#38598;&#12290;&#21482;&#26377;&#26368;&#32456;&#30340; Pareto&#38750;&#25903;&#37197; &#20505;&#36873;&#27169;&#22411;&#26368;&#32456;&#20351;&#29992;&#23436;&#25972;&#30340;&#35757;&#32451;&#38598;&#36827;&#34892;&#24494;&#35843;&#12290;&#25105;&#20204;&#22312;PASCAL VOC 2012&#25968;&#25454;&#38598;&#19978;&#25628;&#32034;&#20102;&#26377;&#25928;&#30340;&#31934;&#24230;&#21644;&#35745;&#31639;&#25104;&#26412;&#20132;&#25442;&#27169;&#22411;&#65292;&#24182;&#35780;&#20272;&#20102;FMAS&#30340;&#24615;&#33021;&#12290;FMAS&#33021;&#22815;&#24555;&#36895;&#25214;&#21040;&#26377;&#31454;&#20105;&#21147;&#30340;&#35774;&#35745;&#65292;&#20363;&#22914;&#21482;&#38656;0.5&#20010;GPU&#22825;&#21363;&#21487;&#21457;&#29616;DeepLabV3+&#21464;&#20307;&#65292;&#23558;FLOPs&#21644;&#21442;&#25968;&#20998;&#21035;&#20943;&#23569;10%&#21644;20%&#65292;&#35823;&#24046;&#20165;&#22686;&#21152;&#19981;&#21040;3%&#12290;&#25105;&#20204;&#22312;&#19968;&#31181;&#21517;&#20026;GAP8&#30340;&#36793;&#32536;&#35774;&#22791;&#19978;&#36827;&#34892;&#20102;&#25628;&#32034;&#65292;&#24182;&#23558;&#20854;&#24310;&#36831;&#20316;&#20026;&#24230;&#37327;&#26631;&#20934;&#65292;FMAS&#33021;&#22815;&#25214;&#21040;2.2&#20493;&#26356;&#24555;&#30340;&#32593;&#32476;&#65292;&#20002;&#22833;7.61% MIoU&#12290;
&lt;/p&gt;
&lt;p&gt;
We present FMAS, a fast multi-objective neural architecture search framework for semantic segmentation. FMAS subsamples the structure and pre-trained parameters of DeepLabV3+, without fine-tuning, dramatically reducing training time during search. To further reduce candidate evaluation time, we use a subset of the validation dataset during the search. Only the final, Pareto non-dominated, candidates are ultimately fine-tuned using the complete training set. We evaluate FMAS by searching for models that effectively trade accuracy and computational cost on the PASCAL VOC 2012 dataset. FMAS finds competitive designs quickly, e.g., taking just 0.5 GPU days to discover a DeepLabV3+ variant that reduces FLOPs and parameters by 10$\%$ and 20$\%$ respectively, for less than 3$\%$ increased error. We also search on an edge device called GAP8 and use its latency as the metric. FMAS is capable of finding 2.2$\times$ faster network with 7.61$\%$ MIoU loss.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#26080;&#38480;&#26102;&#38388;&#35270;&#35282;&#19979;&#37096;&#20998;&#35266;&#27979;&#31995;&#32479;&#30340;&#26368;&#22351;&#24773;&#20917;&#25511;&#21046;&#21644;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#21033;&#29992;&#26410;&#30693;&#27010;&#29575;&#20998;&#24067;&#30340;&#26377;&#38480;&#20540;&#19981;&#30830;&#23450;&#21464;&#37327;&#65292;&#24182;&#33021;&#22815;&#36890;&#36807;&#23450;&#20041;&#20449;&#24687;&#29366;&#24577;&#26469;&#25552;&#39640;&#21160;&#24577;&#35268;&#21010;&#30340;&#35745;&#31639;&#21487;&#22788;&#29702;&#24615;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#21487;&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#26500;&#24314;&#25110;&#23398;&#20064;&#30340;&#36817;&#20284;&#20449;&#24687;&#29366;&#24577;&#12290;</title><link>http://arxiv.org/abs/2303.16321</link><description>&lt;p&gt;
&#26080;&#38480;&#26102;&#38388;&#35270;&#35282;&#19979;&#21033;&#29992;&#37096;&#20998;&#35266;&#27979;&#36827;&#34892;&#26368;&#22351;&#24773;&#20917;&#25511;&#21046;&#19982;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Worst-Case Control and Learning Using Partial Observations Over an Infinite Time-Horizon. (arXiv:2303.16321v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16321
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#26080;&#38480;&#26102;&#38388;&#35270;&#35282;&#19979;&#37096;&#20998;&#35266;&#27979;&#31995;&#32479;&#30340;&#26368;&#22351;&#24773;&#20917;&#25511;&#21046;&#21644;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#21033;&#29992;&#26410;&#30693;&#27010;&#29575;&#20998;&#24067;&#30340;&#26377;&#38480;&#20540;&#19981;&#30830;&#23450;&#21464;&#37327;&#65292;&#24182;&#33021;&#22815;&#36890;&#36807;&#23450;&#20041;&#20449;&#24687;&#29366;&#24577;&#26469;&#25552;&#39640;&#21160;&#24577;&#35268;&#21010;&#30340;&#35745;&#31639;&#21487;&#22788;&#29702;&#24615;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#21487;&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#26500;&#24314;&#25110;&#23398;&#20064;&#30340;&#36817;&#20284;&#20449;&#24687;&#29366;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23433;&#20840;&#20851;&#38190;&#30340;&#32593;&#32476;&#29289;&#29702;&#31995;&#32479;&#38656;&#35201;&#33391;&#22909;&#30340;&#25511;&#21046;&#31574;&#30053;&#26469;&#24212;&#23545;&#25932;&#23545;&#24178;&#25200;&#21644;&#24314;&#27169;&#19981;&#30830;&#23450;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#21033;&#29992;&#37096;&#20998;&#35266;&#27979;&#31995;&#32479;&#36827;&#34892;&#36817;&#20284;&#25511;&#21046;&#21644;&#23398;&#20064;&#65292;&#20197;&#26368;&#23567;&#21270;&#26080;&#38480;&#26102;&#38388;&#35270;&#35282;&#19979;&#30340;&#26368;&#22351;&#24773;&#20917;&#36148;&#29616;&#25104;&#26412;&#12290;&#25105;&#20204;&#23558;&#23545;&#31995;&#32479;&#30340;&#24178;&#25200;&#24314;&#27169;&#20026;&#20855;&#26377;&#26410;&#30693;&#27010;&#29575;&#20998;&#24067;&#30340;&#26377;&#38480;&#20540;&#19981;&#30830;&#23450;&#21464;&#37327;&#12290;&#23545;&#20110;&#24050;&#30693;&#31995;&#32479;&#21160;&#21147;&#23398;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#21160;&#24577;&#35268;&#21010;&#65288;DP&#65289;&#20998;&#35299;&#26469;&#35745;&#31639;&#26368;&#20248;&#25511;&#21046;&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#31532;&#19968;&#36129;&#29486;&#26159;&#23450;&#20041;&#20449;&#24687;&#29366;&#24577;&#65292;&#25552;&#39640;&#20102;DP&#30340;&#35745;&#31639;&#21487;&#22788;&#29702;&#24615;&#65292;&#32780;&#19981;&#25439;&#22833;&#26368;&#20248;&#24615;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#31867;&#22312;&#27599;&#20010;&#26102;&#38388;&#28857;&#20135;&#29983;&#21487;&#35266;&#27979;&#25104;&#26412;&#30340;&#38382;&#39064;&#30340;&#31616;&#21270;&#12290;&#25105;&#20204;&#30340;&#31532;&#20108;&#20010;&#36129;&#29486;&#26159;&#23450;&#20041;&#20102;&#21487;&#20197;&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#30452;&#25509;&#26500;&#24314;&#25110;&#23398;&#20064;&#30340;&#36817;&#20284;&#20449;&#24687;&#29366;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;
Safety-critical cyber-physical systems require control strategies whose worst-case performance is robust against adversarial disturbances and modeling uncertainties. In this paper, we present a framework for approximate control and learning in partially observed systems to minimize the worst-case discounted cost over an infinite time-horizon. We model disturbances to the system as finite-valued uncertain variables with unknown probability distributions. For problems with known system dynamics, we construct a dynamic programming (DP) decomposition to compute the optimal control strategy. Our first contribution is to define information states that improve the computational tractability of this DP without loss of optimality. Then, we describe a simplification for a class of problems where the incurred cost is observable at each time-instance. Our second contribution is a definition of approximate information states that can be constructed or learned directly from observed data for problem
&lt;/p&gt;</description></item><item><title>&#35813;&#32508;&#36848;&#25991;&#31456;&#35843;&#26597;&#20102;&#36229;&#36807;150&#31687;&#25991;&#31456;&#65292;&#25506;&#35752;&#20102;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#22312;&#39044;&#27979;&#29359;&#32618;&#26041;&#38754;&#30340;&#24212;&#29992;&#65292;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#21442;&#32771;&#36164;&#26009;&#12290;</title><link>http://arxiv.org/abs/2303.16310</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#20110;&#29359;&#32618;&#39044;&#27979;&#65306;&#31995;&#32479;&#32508;&#36848;&#19982;&#26410;&#26469;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;
Crime Prediction Using Machine Learning and Deep Learning: A Systematic Review and Future Directions. (arXiv:2303.16310v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16310
&lt;/p&gt;
&lt;p&gt;
&#35813;&#32508;&#36848;&#25991;&#31456;&#35843;&#26597;&#20102;&#36229;&#36807;150&#31687;&#25991;&#31456;&#65292;&#25506;&#35752;&#20102;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#22312;&#39044;&#27979;&#29359;&#32618;&#26041;&#38754;&#30340;&#24212;&#29992;&#65292;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#21442;&#32771;&#36164;&#26009;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#29359;&#32618;&#22312;&#26368;&#36817;&#20960;&#24180;&#24341;&#36215;&#20102;&#30740;&#31350;&#20154;&#21592;&#30340;&#30456;&#24403;&#20851;&#27880;&#65292;&#20027;&#35201;&#20851;&#27880;&#35782;&#21035;&#29359;&#32618;&#20107;&#20214;&#20013;&#30340;&#27169;&#24335;&#21644;&#36235;&#21183;&#12290;&#26412;&#32508;&#36848;&#25991;&#31456;&#35843;&#26597;&#20102;&#36229;&#36807;150&#31687;&#25991;&#31456;&#65292;&#25506;&#35752;&#20102;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#24212;&#29992;&#20110;&#39044;&#27979;&#29359;&#32618;&#30340;&#24773;&#20917;&#12290;&#35813;&#30740;&#31350;&#25552;&#20379;&#20102;&#30740;&#31350;&#20154;&#21592;&#29992;&#20110;&#29359;&#32618;&#39044;&#27979;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#20998;&#26512;&#20102;&#24212;&#29992;&#20110;&#39044;&#27979;&#29359;&#32618;&#30340;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#30340;&#26480;&#20986;&#26041;&#27861;&#65292;&#25552;&#20379;&#20102;&#26377;&#20851;&#29359;&#32618;&#27963;&#21160;&#30456;&#20851;&#19981;&#21516;&#36235;&#21183;&#21644;&#22240;&#32032;&#30340;&#35265;&#35299;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#36824;&#24378;&#35843;&#20102;&#21487;&#33021;&#23384;&#22312;&#30340;&#24046;&#36317;&#21644;&#26410;&#26469;&#26041;&#21521;&#65292;&#20197;&#22686;&#24378;&#29359;&#32618;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;&#26368;&#21518;&#65292;&#26412;&#25991;&#20171;&#32461;&#30340;&#20851;&#20110;&#24212;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#39044;&#27979;&#29359;&#32618;&#30340;&#30740;&#31350;&#32508;&#36848;&#65292;&#20026;&#36825;&#19968;&#39046;&#22495;&#30340;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#21442;&#32771;&#36164;&#26009;&#12290;
&lt;/p&gt;
&lt;p&gt;
Predicting crime using machine learning and deep learning techniques has gained considerable attention from researchers in recent years, focusing on identifying patterns and trends in crime occurrences. This review paper examines over 150 articles to explore the various machine learning and deep learning algorithms applied to predict crime. The study provides access to the datasets used for crime prediction by researchers and analyzes prominent approaches applied in machine learning and deep learning algorithms to predict crime, offering insights into different trends and factors related to criminal activities. Additionally, the paper highlights potential gaps and future directions that can enhance the accuracy of crime prediction. Finally, the comprehensive overview of research discussed in this paper on crime prediction using machine learning and deep learning approaches serves as a valuable reference for researchers in this field. By gaining a deeper understanding of crime predictio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#30340;Dice&#21322;&#24230;&#37327;&#25439;&#22833;&#20989;&#25968;&#21487;&#22312;&#36719;&#26631;&#31614;&#35774;&#32622;&#20013;&#20351;&#29992;&#65292;&#22312;&#21307;&#30103;&#25104;&#20687;&#39046;&#22495;&#30340;&#20998;&#21106;&#26041;&#26696;&#20013;&#19982;&#20351;&#29992;&#36719;&#26631;&#31614;&#30340;&#30740;&#31350;&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;Dice&#20998;&#25968;&#21644;&#27169;&#22411;&#26657;&#20934;&#12290;</title><link>http://arxiv.org/abs/2303.16296</link><description>&lt;p&gt;
Dice&#21322;&#24230;&#37327;&#25439;&#22833;&#20989;&#25968;&#65306;&#29992;&#36719;&#26631;&#31614;&#20248;&#21270;Dice&#20998;&#25968;
&lt;/p&gt;
&lt;p&gt;
Dice Semimetric Losses: Optimizing the Dice Score with Soft Labels. (arXiv:2303.16296v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16296
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#30340;Dice&#21322;&#24230;&#37327;&#25439;&#22833;&#20989;&#25968;&#21487;&#22312;&#36719;&#26631;&#31614;&#35774;&#32622;&#20013;&#20351;&#29992;&#65292;&#22312;&#21307;&#30103;&#25104;&#20687;&#39046;&#22495;&#30340;&#20998;&#21106;&#26041;&#26696;&#20013;&#19982;&#20351;&#29992;&#36719;&#26631;&#31614;&#30340;&#30740;&#31350;&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;Dice&#20998;&#25968;&#21644;&#27169;&#22411;&#26657;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21307;&#23398;&#25104;&#20687;&#39046;&#22495;&#30340;&#35768;&#22810;&#33258;&#21160;&#20998;&#21106;&#26041;&#26696;&#20013;&#65292;&#36719;Dice&#25439;&#22833;&#65288;SDL&#65289;&#21457;&#25381;&#20102;&#20851;&#38190;&#20316;&#29992;&#12290;&#22312;&#36807;&#21435;&#20960;&#24180;&#20013;&#65292;&#20154;&#20204;&#24050;&#32463;&#25581;&#31034;&#20102;&#20854;&#20248;&#36234;&#24615;&#33021;&#32972;&#21518;&#30340;&#19968;&#20123;&#21407;&#22240;&#24182;&#36827;&#19968;&#27493;&#25506;&#32034;&#20102;&#20854;&#20248;&#21270;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#36824;&#27809;&#26377;&#23454;&#29616;&#25903;&#25345;&#30452;&#25509;&#22312;&#36719;&#26631;&#31614;&#35774;&#32622;&#20013;&#20351;&#29992;&#23427;&#30340;&#26041;&#26696;&#12290;&#22240;&#27492;&#65292;&#22312;&#20351;&#29992;SDL&#21644;&#30740;&#31350;&#21033;&#29992;&#36719;&#26631;&#31614;&#30340;&#21516;&#26102;&#36827;&#34892;&#27169;&#22411;&#26657;&#20934;&#30340;&#21327;&#21516;&#20316;&#29992;&#20173;&#28982;&#32570;&#22833;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Dice&#21322;&#24230;&#37327;&#25439;&#22833;&#20989;&#25968;&#65288;DMLs&#65289;&#65292;&#23427;&#20204;&#65288;i&#65289;&#22312;&#30828;&#26631;&#31614;&#30340;&#26631;&#20934;&#35774;&#32622;&#19979;&#19982;SDL&#30456;&#21516;&#65292;&#20294;&#65288;ii&#65289;&#20063;&#21487;&#22312;&#36719;&#26631;&#31614;&#35774;&#32622;&#20013;&#20351;&#29992;&#12290;&#25105;&#20204;&#22312;&#20844;&#20849;&#30340;QUBIQ&#12289;LiTS&#21644;KiTS&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#23454;&#39564;&#35777;&#23454;&#20102;DMLs&#19982;&#36719;&#26631;&#31614;&#65288;&#22914;&#24179;&#22343;&#12289;&#26631;&#31614;&#24179;&#28369;&#21644;&#30693;&#35782;&#33976;&#39311;&#65289;&#30340;&#28508;&#22312;&#21327;&#21516;&#20316;&#29992;&#65292;&#32780;DMLs&#19982;&#30828;&#26631;&#31614;&#65288;&#22914;&#22823;&#22810;&#25968;&#25237;&#31080;&#21644;&#38543;&#26426;&#36873;&#25321;&#65289;&#30456;&#27604;&#65292;&#20135;&#29983;&#20102;&#26356;&#20248;&#31168;&#30340;Dice&#20998;&#25968;&#21644;&#27169;&#22411;&#26657;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
The soft Dice loss (SDL) has taken a pivotal role in many automated segmentation pipelines in the medical imaging community. Over the last years, some reasons behind its superior functioning have been uncovered and further optimizations have been explored. However, there is currently no implementation that supports its direct use in settings with soft labels. Hence, a synergy between the use of SDL and research leveraging the use of soft labels, also in the context of model calibration, is still missing. In this work, we introduce Dice semimetric losses (DMLs), which (i) are by design identical to SDL in a standard setting with hard labels, but (ii) can be used in settings with soft labels. Our experiments on the public QUBIQ, LiTS and KiTS benchmarks confirm the potential synergy of DMLs with soft labels (e.g. averaging, label smoothing, and knowledge distillation) over hard labels (e.g. majority voting and random selection). As a result, we obtain superior Dice scores and model calib
&lt;/p&gt;</description></item><item><title>XAIR&#26159;&#19968;&#20010;&#35299;&#20915;AR&#20013;XAI&#36755;&#20986;&#35299;&#37322;&#30340;&#35774;&#35745;&#26694;&#26550;&#65292;&#21487;&#20197;&#20026;&#35774;&#35745;&#32773;&#25552;&#20379;&#25351;&#21335;&#21644;&#25903;&#25345;&#65292;&#23454;&#29616;&#26377;&#25928;&#30340;XAI&#35774;&#35745;&#12290;</title><link>http://arxiv.org/abs/2303.16292</link><description>&lt;p&gt;
XAIR&#65306;&#22686;&#24378;&#29616;&#23454;&#20013;&#21487;&#35299;&#37322;AI&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
XAIR: A Framework of Explainable AI in Augmented Reality. (arXiv:2303.16292v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16292
&lt;/p&gt;
&lt;p&gt;
XAIR&#26159;&#19968;&#20010;&#35299;&#20915;AR&#20013;XAI&#36755;&#20986;&#35299;&#37322;&#30340;&#35774;&#35745;&#26694;&#26550;&#65292;&#21487;&#20197;&#20026;&#35774;&#35745;&#32773;&#25552;&#20379;&#25351;&#21335;&#21644;&#25903;&#25345;&#65292;&#23454;&#29616;&#26377;&#25928;&#30340;XAI&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#24050;&#32463;&#25104;&#20026;AI&#39537;&#21160;&#20132;&#20114;&#24335;&#31995;&#32479;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#38543;&#30528;&#22686;&#24378;&#29616;&#23454;&#65288;AR&#65289;&#36234;&#26469;&#36234;&#22810;&#22320;&#34701;&#20837;&#26085;&#24120;&#29983;&#27963;&#65292;XAI&#22312;AR&#20013;&#30340;&#20316;&#29992;&#20063;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#20026;&#26368;&#32456;&#29992;&#25143;&#23558;&#32463;&#24120;&#19982;&#26234;&#33021;&#26381;&#21153;&#20114;&#21160;&#12290;&#28982;&#32780;&#65292;&#22914;&#20309;&#35774;&#35745;&#26377;&#25928;&#30340;AR XAI&#20307;&#39564;&#23578;&#19981;&#28165;&#26970;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;XAIR&#65292;&#36825;&#26159;&#19968;&#20010;&#35774;&#35745;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#20309;&#26102;&#65292;&#20160;&#20040;&#21644;&#22914;&#20309;&#22312;AR&#20013;&#25552;&#20379;AI&#36755;&#20986;&#35299;&#37322;&#30340;&#38382;&#39064;&#12290;&#35813;&#26694;&#26550;&#26159;&#22522;&#20110;&#23545;XAI&#21644;HCI&#30740;&#31350;&#30340;&#36328;&#23398;&#31185;&#25991;&#29486;&#32508;&#36848;&#12289;&#23545;500&#22810;&#21517;&#26368;&#32456;&#29992;&#25143;&#23545;&#22522;&#20110;AR&#30340;&#35299;&#37322;&#20559;&#22909;&#30340;&#22823;&#35268;&#27169;&#35843;&#26597;&#20197;&#21450;&#23545;12&#20301;&#19987;&#23478;&#25910;&#38598;&#20182;&#20204;&#22312;AR XAI&#35774;&#35745;&#26041;&#38754;&#30340;&#35265;&#35299;&#30340;&#19977;&#20010;&#30740;&#35752;&#20250;&#32780;&#26500;&#24314;&#30340;&#12290;&#36890;&#36807;&#19982;10&#21517;&#35774;&#35745;&#24072;&#21644;&#21478;&#22806;12&#21517;&#26368;&#32456;&#29992;&#25143;&#36827;&#34892;&#30340;&#30740;&#31350;&#39564;&#35777;&#20102;XAIR&#30340;&#25928;&#29992;&#21644;&#26377;&#25928;&#24615;&#12290;XAIR&#21487;&#20197;&#20026;&#35774;&#35745;&#24072;&#25552;&#20379;&#25351;&#21335;&#65292;&#28608;&#21457;&#20182;&#20204;&#21457;&#29616;&#26032;&#30340;&#35774;&#35745;&#26426;&#20250;&#65292;&#24182;&#22312;AR&#20013;&#23454;&#29616;&#26377;&#25928;&#30340;XAI&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
Explainable AI (XAI) has established itself as an important component of AI-driven interactive systems. With Augmented Reality (AR) becoming more integrated in daily lives, the role of XAI also becomes essential in AR because end-users will frequently interact with intelligent services. However, it is unclear how to design effective XAI experiences for AR. We propose XAIR, a design framework that addresses "when", "what", and "how" to provide explanations of AI output in AR. The framework was based on a multi-disciplinary literature review of XAI and HCI research, a large-scale survey probing 500+ end-users' preferences for AR-based explanations, and three workshops with 12 experts collecting their insights about XAI design in AR. XAIR's utility and effectiveness was verified via a study with 10 designers and another study with 12 end-users. XAIR can provide guidelines for designers, inspiring them to identify new design opportunities and achieve effective XAI designs in AR.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#21457;&#29616;&#22312;Google&#12289;ChatGPT&#12289;&#32500;&#22522;&#30334;&#31185;&#21644;YouTube&#19978;&#65292;&#25628;&#32034;&#32467;&#26524;&#21463;&#38480;&#20110;&#35821;&#35328;&#65292;&#21453;&#26144;&#20102;&#19982;&#22797;&#26434;&#20027;&#39064;&#30456;&#20851;&#30340;&#25991;&#21270;&#21051;&#26495;&#21360;&#35937;&#65292;&#32570;&#20047;&#36328;&#25991;&#21270;&#35270;&#35282;&#12290;</title><link>http://arxiv.org/abs/2303.16281</link><description>&lt;p&gt;
&#22823;&#35937;&#30340;&#36879;&#35270;&#38236;&#65306;&#35843;&#26597;&#35895;&#27468;&#12289;ChatGPT&#12289;&#32500;&#22522;&#30334;&#31185;&#21644;YouTube&#19978;&#30340;&#35821;&#35328;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
A Perspectival Mirror of the Elephant: Investigating Language Bias on Google, ChatGPT, Wikipedia, and YouTube. (arXiv:2303.16281v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16281
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#21457;&#29616;&#22312;Google&#12289;ChatGPT&#12289;&#32500;&#22522;&#30334;&#31185;&#21644;YouTube&#19978;&#65292;&#25628;&#32034;&#32467;&#26524;&#21463;&#38480;&#20110;&#35821;&#35328;&#65292;&#21453;&#26144;&#20102;&#19982;&#22797;&#26434;&#20027;&#39064;&#30456;&#20851;&#30340;&#25991;&#21270;&#21051;&#26495;&#21360;&#35937;&#65292;&#32570;&#20047;&#36328;&#25991;&#21270;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;&#35895;&#27468;&#25628;&#32034;&#8220;&#20174;&#22810;&#20010;&#35282;&#24230;&#33719;&#21462;&#20449;&#24687;&#65292;&#20197;&#20415;&#20320;&#21487;&#20197;&#24418;&#25104;&#33258;&#24049;&#23545;&#19990;&#30028;&#30340;&#29702;&#35299;&#8221;&#30340;&#20219;&#21153;&#30456;&#21453;&#65292;&#25105;&#20204;&#21457;&#29616;&#35895;&#27468;&#21450;&#20854;&#26368;&#31361;&#20986;&#30340;&#25628;&#32034;&#32467;&#26524; - &#32500;&#22522;&#30334;&#31185;&#21644;YouTube&#65292;&#20165;&#21453;&#26144;&#19982;&#8220;&#20315;&#25945;&#8221;&#12289;&#8220;&#33258;&#30001;&#20027;&#20041;&#8221;&#12289;&#8220;&#27542;&#27665;&#21270;&#8221;&#12289;&#8220;&#20234;&#26391;&#8221;&#21644;&#8220;&#32654;&#22269;&#8221;&#31561;&#22797;&#26434;&#20027;&#39064;&#30456;&#20851;&#30340;&#25991;&#21270;&#21051;&#26495;&#21360;&#35937;&#12290;&#31616;&#21333;&#22320;&#35828;&#65292;&#22312;&#19981;&#21516;&#35821;&#35328;&#30340;&#30456;&#21516;&#25628;&#32034;&#20013;&#65292;&#23427;&#20204;&#20197;&#19981;&#21516;&#31243;&#24230;&#21576;&#29616;&#19981;&#21516;&#30340;&#20449;&#24687;&#65288;&#25105;&#20204;&#31216;&#20043;&#20026;&#8220;&#35821;&#35328;&#20559;&#35265;&#8221;&#65289;&#65292;&#32780;&#19981;&#26159;&#21576;&#29616;&#22797;&#26434;&#20027;&#39064;&#30340;&#20840;&#29699;&#22270;&#29255;&#12290;&#25105;&#20204;&#30340;&#22312;&#32447;&#25628;&#32034;&#20351;&#25105;&#20204;&#25104;&#20026;&#35866;&#35821;&#20013;&#30340;&#30450;&#20154;&#65292;&#20165;&#35302;&#25720;&#23567;&#35937;&#30340;&#19968;&#23567;&#37096;&#20998;&#65292;&#19981;&#30693;&#36947;&#20854;&#20182;&#25991;&#21270;&#30340;&#35270;&#35282;&#30340;&#23384;&#22312;&#12290;&#25105;&#20204;&#29992;&#20110;&#25628;&#32034;&#30340;&#35821;&#35328;&#26368;&#32456;&#25104;&#20026;&#20419;&#36827;&#26412;&#26063;&#20013;&#24515;&#20027;&#20041;&#35266;&#28857;&#30340;&#25991;&#21270;&#36807;&#28388;&#22120;&#65292;&#20854;&#20013;&#19968;&#20010;&#20154;&#26681;&#25454;&#33258;&#24049;&#30340;&#25991;&#21270;&#35780;&#20272;&#20854;&#20182;&#20154;&#25110;&#24605;&#24819;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;ChatGPT&#20013;&#28145;&#28145;&#23884;&#20837;&#20102;&#35821;&#35328;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contrary to Google Search's mission of delivering information from "many angles so you can form your own understanding of the world," we find that Google and its most prominent returned results -- Wikipedia and YouTube, simply reflect the narrow set of cultural stereotypes tied to the search language for complex topics like "Buddhism," "Liberalism," "colonization," "Iran" and "America." Simply stated, they present, to varying degrees, distinct information across the same search in different languages (we call it 'language bias'). Instead of presenting a global picture of a complex topic, our online searches turn us into the proverbial blind person touching a small portion of an elephant, ignorant of the existence of other cultural perspectives. The language we use to search ends up as a cultural filter to promote ethnocentric views, where a person evaluates other people or ideas based on their own culture. We also find that language bias is deeply embedded in ChatGPT. As it is primaril
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20351;&#29992;&#21551;&#21457;&#24335;&#31639;&#27861;&#26500;&#36896;&#35206;&#30422;&#32534;&#30721;&#26144;&#23556;&#65292;&#23558;&#22823;&#22411;&#25628;&#32034;&#31354;&#38388;&#20013;&#30340;&#36807;&#31243;&#26144;&#23556;&#21040;&#21407;&#22987;&#25628;&#32034;&#31354;&#38388;&#30340;&#23376;&#38598;&#20013;&#65292;&#24182;&#20351;&#29992;&#37325;&#25972;&#21270;&#32676;&#30340;&#35270;&#35282;&#26469;&#22788;&#29702;&#31163;&#25955;&#20248;&#21270;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#30456;&#23545;&#20110;&#21333;&#29420;&#30340;&#23616;&#37096;&#25628;&#32034;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.16258</link><description>&lt;p&gt;
&#36890;&#36807;&#32534;&#30721;&#23454;&#29616;&#20248;&#21270;&#65306;&#37325;&#25972;&#21270;&#32676;&#30340;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Optimisation via encodings: a renormalisation group perspective. (arXiv:2303.16258v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16258
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20351;&#29992;&#21551;&#21457;&#24335;&#31639;&#27861;&#26500;&#36896;&#35206;&#30422;&#32534;&#30721;&#26144;&#23556;&#65292;&#23558;&#22823;&#22411;&#25628;&#32034;&#31354;&#38388;&#20013;&#30340;&#36807;&#31243;&#26144;&#23556;&#21040;&#21407;&#22987;&#25628;&#32034;&#31354;&#38388;&#30340;&#23376;&#38598;&#20013;&#65292;&#24182;&#20351;&#29992;&#37325;&#25972;&#21270;&#32676;&#30340;&#35270;&#35282;&#26469;&#22788;&#29702;&#31163;&#25955;&#20248;&#21270;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#30456;&#23545;&#20110;&#21333;&#29420;&#30340;&#23616;&#37096;&#25628;&#32034;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#20915;&#31163;&#25955;&#20248;&#21270;&#38382;&#39064;&#30340;&#20256;&#32479;&#26041;&#27861;&#26159;&#20351;&#29992;&#36866;&#24403;&#23450;&#20041;&#30340;&#25104;&#26412;&#25110;&#36866;&#24212;&#24615;&#26223;&#35266;&#19978;&#30340;&#23616;&#37096;&#25628;&#32034;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#21463;&#21040;&#20856;&#22411;&#23822;&#23702;&#26223;&#35266;&#20013;&#30340;&#23616;&#37096;&#26368;&#23567;&#20540;&#21046;&#32422;&#65292;&#20196;&#25628;&#32034;&#36807;&#31243;&#21464;&#24930;&#12290;&#21478;&#19968;&#31181;&#35299;&#20915;&#20248;&#21270;&#38382;&#39064;&#30340;&#26041;&#27861;&#26159;&#36890;&#36807;&#20351;&#29992;&#21551;&#21457;&#24335;&#36924;&#36817;&#20272;&#35745;&#20840;&#23616;&#26368;&#23567;&#20540;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#36825;&#20004;&#31181;&#26041;&#27861;&#30340;&#32452;&#21512;&#65292;&#20351;&#29992;&#35206;&#30422;&#32534;&#30721;&#26144;&#23556;&#23558;&#22823;&#22411;&#25628;&#32034;&#31354;&#38388;&#20013;&#30340;&#36807;&#31243;&#26144;&#23556;&#21040;&#21407;&#22987;&#25628;&#32034;&#31354;&#38388;&#30340;&#23376;&#38598;&#20013;&#12290;&#20851;&#38190;&#24605;&#24819;&#26159;&#21033;&#29992;&#36866;&#24403;&#30340;&#21551;&#21457;&#24335;&#31639;&#27861;&#26500;&#36896;&#35206;&#30422;&#32534;&#30721;&#26144;&#23556;&#65292;&#22312;&#22823;&#22411;&#25628;&#32034;&#31354;&#38388;&#19978;&#29983;&#25104;&#19981;&#20877;&#26174;&#31034;&#38519;&#38449;&#23616;&#37096;&#26368;&#23567;&#20540;&#30340;&#26223;&#35266;&#12290;&#36890;&#24120;&#37319;&#29992;&#30340;&#36807;&#31243;&#28041;&#21450;&#26576;&#31181;&#24418;&#24335;&#30340;&#31895;&#31890;&#21270;&#65292;&#25105;&#20204;&#35748;&#20026;&#23427;&#20204;&#21487;&#20197;&#34987;&#35270;&#20026;&#32479;&#35745;&#21147;&#23398;&#20013;&#37325;&#25972;&#21270;&#32676;&#30340;&#21270;&#36523;&#12290;&#25105;&#20204;&#20351;&#29992;&#32452;&#21512;&#20248;&#21270;&#39046;&#22495;&#30340;&#27979;&#35797;&#38382;&#39064;&#26469;&#35828;&#26126;&#36825;&#31181;&#26041;&#27861;&#65292;&#21457;&#29616;&#35206;&#30422;&#32534;&#30721;&#26144;&#23556;&#19982;&#23616;&#37096;&#25628;&#32034;&#30340;&#32452;&#21512;&#22987;&#32456;&#20248;&#20110;&#21333;&#29420;&#30340;&#23616;&#37096;&#25628;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
The traditional way of tackling discrete optimization problems is by using local search on suitably defined cost or fitness landscapes. Such approaches are however limited by the slowing down that occurs when local minima, that are a feature of the typically rugged landscapes encountered, arrest the progress of the search process. Another way of tackling optimization problems is by the use of heuristic approximations to estimate a global cost minimum. Here we present a combination of these two approaches by using cover-encoding maps which map processes from a larger search space to subsets of the original search space. The key idea is to construct cover-encoding maps with the help of suitable heuristics that single out near-optimal solutions and result in landscapes on the larger search space that no longer exhibit trapping local minima. The processes that are typically employed involve some form of coarse-graining, and we suggest here that they can be viewed as avatars of renormalisat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;Codex&#29983;&#25104;OCL&#32422;&#26463;&#65292;&#36890;&#36807;&#25552;&#39640;&#25552;&#31034;&#27169;&#26495;&#30340;&#39046;&#22495;&#29305;&#23450;&#20449;&#24687;&#21644;&#23569;&#37327;&#26679;&#26412;&#23398;&#20064;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#29983;&#25104;&#32422;&#26463;&#30340;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2303.16244</link><description>&lt;p&gt;
Codex&#25552;&#31034;&#24037;&#31243;&#29992;&#20110;OCL&#29983;&#25104;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On Codex Prompt Engineering for OCL Generation: An Empirical Study. (arXiv:2303.16244v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16244
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;Codex&#29983;&#25104;OCL&#32422;&#26463;&#65292;&#36890;&#36807;&#25552;&#39640;&#25552;&#31034;&#27169;&#26495;&#30340;&#39046;&#22495;&#29305;&#23450;&#20449;&#24687;&#21644;&#23569;&#37327;&#26679;&#26412;&#23398;&#20064;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#29983;&#25104;&#32422;&#26463;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35937;&#32422;&#26463;&#35821;&#35328;&#65288;OCL&#65289;&#26159;&#19968;&#31181;&#22768;&#26126;&#24615;&#35821;&#35328;&#65292;&#23427;&#22312;MOF&#27169;&#22411;&#20013;&#28155;&#21152;&#20102;&#32422;&#26463;&#21644;&#23545;&#35937;&#26597;&#35810;&#34920;&#36798;&#24335;&#12290;&#23613;&#31649;OCL&#26377;&#28508;&#21147;&#20026;UML&#27169;&#22411;&#25552;&#20379;&#31934;&#24230;&#21644;&#31616;&#27905;&#24615;&#65292;&#20294;&#20854;&#19981;&#29087;&#24713;&#30340;&#35821;&#27861;&#38459;&#30861;&#20102;&#20854;&#34987;&#37319;&#29992;&#12290;&#26368;&#36817;LLM&#65288;&#22914;GPT-3&#65289;&#30340;&#36827;&#23637;&#26174;&#31034;&#20102;&#23427;&#20204;&#22312;&#35768;&#22810;NLP&#20219;&#21153;&#65288;&#21253;&#25324;&#35821;&#20041;&#35299;&#26512;&#21644;&#25991;&#26412;&#29983;&#25104;&#65289;&#20013;&#30340;&#33021;&#21147;&#12290;Codex&#26159;GPT-3&#30340;&#21518;&#20195;&#65292;&#24050;&#32463;&#22312;GitHub&#19978;&#20844;&#24320;&#21487;&#29992;&#30340;&#20195;&#30721;&#19978;&#36827;&#34892;&#20102;&#24494;&#35843;&#65292;&#24182;&#19988;&#21487;&#20197;&#29992;&#35768;&#22810;&#32534;&#31243;&#35821;&#35328;&#29983;&#25104;&#20195;&#30721;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#20174;&#33258;&#28982;&#35821;&#35328;&#35268;&#33539;&#20013;&#30001;Codex&#29983;&#25104;&#30340;OCL&#32422;&#26463;&#30340;&#21487;&#38752;&#24615;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#32534;&#21046;&#20102;&#19968;&#20010;&#21253;&#21547;&#25554;&#27133;&#30340;&#25552;&#31034;&#27169;&#26495;&#65292;&#29992;UML&#20449;&#24687;&#21644;&#30446;&#26631;&#20219;&#21153;&#22635;&#20805;&#65292;&#20351;&#29992;&#38646;&#25110;&#23569;&#37327;&#26679;&#26412;&#23398;&#20064;&#26041;&#27861;&#12290;&#36890;&#36807;&#34913;&#37327;OCL&#32422;&#26463;&#30340;&#35821;&#27861;&#26377;&#25928;&#24615;&#21644;&#25191;&#34892;&#20934;&#30830;&#24615;&#25351;&#26631;&#65292;&#25105;&#20204;&#21457;&#29616;&#20016;&#23500;&#25552;&#31034;&#27169;&#26495;&#30340;&#39046;&#22495;&#29305;&#23450;&#20449;&#24687;&#20197;&#21450;&#20351;&#29992;&#23569;&#37327;&#26679;&#26412;&#23398;&#20064;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#29983;&#25104;&#30340;OCL&#32422;&#26463;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Object Constraint Language (OCL) is a declarative language that adds constraints and object query expressions to MOF models. Despite its potential to provide precision and conciseness to UML models, the unfamiliar syntax of OCL has hindered its adoption. Recent advancements in LLMs, such as GPT-3, have shown their capability in many NLP tasks, including semantic parsing and text generation. Codex, a GPT-3 descendant, has been fine-tuned on publicly available code from GitHub and can generate code in many programming languages. We investigate the reliability of OCL constraints generated by Codex from natural language specifications. To achieve this, we compiled a dataset of 15 UML models and 168 specifications and crafted a prompt template with slots to populate with UML information and the target task, using both zero- and few-shot learning methods. By measuring the syntactic validity and execution accuracy metrics of the generated OCL constraints, we found that enriching the promp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#23376;&#32593;&#32476;&#30340;EMO&#32852;&#21512;&#21098;&#26525;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#21487;&#20197;&#20943;&#23569;&#31354;&#38388;&#21644;&#36164;&#28304;&#28040;&#32791;&#12290;&#35813;&#31639;&#27861;&#37319;&#29992;&#20998;&#27835;&#30340;EMO&#32593;&#32476;&#21098;&#26525;&#26694;&#26550;&#65292;&#23558;&#25972;&#20010;&#32593;&#32476;&#19978;&#22797;&#26434;&#30340;EMO&#21098;&#26525;&#20219;&#21153;&#20998;&#35299;&#20026;&#22810;&#20010;&#23376;&#32593;&#32476;&#19978;&#26356;&#31616;&#21333;&#30340;&#23376;&#20219;&#21153;&#12290;&#22522;&#20110;&#36328;&#32593;&#32476;&#32422;&#26463;&#30340;&#23376;&#32593;&#32476;&#35757;&#32451;&#26041;&#27861;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.16212</link><description>&lt;p&gt;
&#22522;&#20110;&#22810;&#23376;&#32593;&#32476;&#30340;EMO&#32852;&#21512;&#21098;&#26525;&#65306;&#39640;&#25928;&#24555;&#36895;
&lt;/p&gt;
&lt;p&gt;
An EMO Joint Pruning with Multiple Sub-networks: Fast and Effect. (arXiv:2303.16212v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16212
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#23376;&#32593;&#32476;&#30340;EMO&#32852;&#21512;&#21098;&#26525;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#21487;&#20197;&#20943;&#23569;&#31354;&#38388;&#21644;&#36164;&#28304;&#28040;&#32791;&#12290;&#35813;&#31639;&#27861;&#37319;&#29992;&#20998;&#27835;&#30340;EMO&#32593;&#32476;&#21098;&#26525;&#26694;&#26550;&#65292;&#23558;&#25972;&#20010;&#32593;&#32476;&#19978;&#22797;&#26434;&#30340;EMO&#21098;&#26525;&#20219;&#21153;&#20998;&#35299;&#20026;&#22810;&#20010;&#23376;&#32593;&#32476;&#19978;&#26356;&#31616;&#21333;&#30340;&#23376;&#20219;&#21153;&#12290;&#22522;&#20110;&#36328;&#32593;&#32476;&#32422;&#26463;&#30340;&#23376;&#32593;&#32476;&#35757;&#32451;&#26041;&#27861;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#36827;&#21270;&#22810;&#30446;&#26631;&#65288;EMO&#65289;&#30340;&#32593;&#32476;&#21098;&#26525;&#31639;&#27861;&#21487;&#20197;&#24179;&#34913;&#32593;&#32476;&#30340;&#21098;&#26525;&#29575;&#21644;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#22522;&#20110;&#31181;&#32676;&#30340;&#29305;&#24615;&#65292;&#23427;&#32463;&#24120;&#21463;&#21040;&#22797;&#26434;&#30340;&#21098;&#26525;&#20248;&#21270;&#31354;&#38388;&#21644;&#39640;&#24230;&#36164;&#28304;&#28040;&#32791;&#30340;&#21098;&#26525;&#32467;&#26500;&#39564;&#35777;&#36807;&#31243;&#30340;&#38480;&#21046;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#23427;&#30340;&#24212;&#29992;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#23376;&#32593;&#32476;&#30340;EMO&#32852;&#21512;&#21098;&#26525;&#65288;EMO-PMS&#65289;&#65292;&#20197;&#20943;&#23569;&#31354;&#38388;&#22797;&#26434;&#24230;&#21644;&#36164;&#28304;&#28040;&#32791;&#12290;&#39318;&#20808;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#27835;&#30340;EMO&#32593;&#32476;&#21098;&#26525;&#26694;&#26550;&#65292;&#23558;&#25972;&#20010;&#32593;&#32476;&#19978;&#22797;&#26434;&#30340;EMO&#21098;&#26525;&#20219;&#21153;&#20998;&#35299;&#20026;&#22810;&#20010;&#23376;&#32593;&#32476;&#19978;&#26356;&#31616;&#21333;&#30340;&#23376;&#20219;&#21153;&#12290;&#19968;&#26041;&#38754;&#65292;&#36825;&#31181;&#20998;&#35299;&#20943;&#23569;&#20102;&#21098;&#26525;&#20248;&#21270;&#31354;&#38388;&#24182;&#38477;&#20302;&#20102;&#20248;&#21270;&#38590;&#24230;&#65307;&#21478;&#19968;&#26041;&#38754;&#65292;&#36739;&#23567;&#30340;&#32593;&#32476;&#32467;&#26500;&#25910;&#25947;&#26356;&#24555;&#65292;&#22240;&#27492;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#30340;&#35745;&#31639;&#36164;&#28304;&#28040;&#32791;&#36739;&#20302;&#12290;&#20854;&#27425;&#65292;&#22522;&#20110;&#36328;&#32593;&#32476;&#32422;&#26463;&#30340;&#23376;&#32593;&#32476;&#35757;&#32451;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The network pruning algorithm based on evolutionary multi-objective (EMO) can balance the pruning rate and performance of the network. However, its population-based nature often suffers from the complex pruning optimization space and the highly resource-consuming pruning structure verification process, which limits its application. To this end, this paper proposes an EMO joint pruning with multiple sub-networks (EMO-PMS) to reduce space complexity and resource consumption. First, a divide-and-conquer EMO network pruning framework is proposed, which decomposes the complex EMO pruning task on the whole network into easier sub-tasks on multiple sub-networks. On the one hand, this decomposition reduces the pruning optimization space and decreases the optimization difficulty; on the other hand, the smaller network structure converges faster, so the computational resource consumption of the proposed algorithm is lower. Secondly, a sub-network training method based on cross-network constraint
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36136;&#37327;&#22810;&#26679;&#24615;&#31639;&#27861;&#21644; Transformer &#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20004;&#31181;&#26426;&#21046;&#20197;&#23454;&#29616;&#36136;&#37327;&#19968;&#33268;&#30340;&#29983;&#25104;&#34892;&#20026;&#26465;&#20214;&#19979;&#30340;&#36712;&#36857;&#12290;</title><link>http://arxiv.org/abs/2303.16207</link><description>&lt;p&gt;
&#36136;&#37327;&#22810;&#26679;&#24615;&#21464;&#24418;&#22120;&#65306;&#22522;&#20110;&#20915;&#31574;Transformer&#29983;&#25104;&#34892;&#20026;&#26465;&#20214;&#19979;&#30340;&#36712;&#36857;
&lt;/p&gt;
&lt;p&gt;
The Quality-Diversity Transformer: Generating Behavior-Conditioned Trajectories with Decision Transformers. (arXiv:2303.16207v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16207
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36136;&#37327;&#22810;&#26679;&#24615;&#31639;&#27861;&#21644; Transformer &#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20004;&#31181;&#26426;&#21046;&#20197;&#23454;&#29616;&#36136;&#37327;&#19968;&#33268;&#30340;&#29983;&#25104;&#34892;&#20026;&#26465;&#20214;&#19979;&#30340;&#36712;&#36857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31070;&#32463;&#36827;&#21270;&#35745;&#31639;&#30340;&#32972;&#26223;&#19979;&#65292;&#36136;&#37327;&#22810;&#26679;&#24615;&#31639;&#27861;&#36890;&#36807;&#20381;&#36182;&#34892;&#20026;&#31354;&#38388;&#30340;&#23450;&#20041;&#26469;&#29983;&#25104;&#21508;&#31181;&#19981;&#21516;&#21644;&#39640;&#25928;&#30340;&#31574;&#30053;&#38598;&#21512;&#65292;&#21462;&#24471;&#20102;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;&#28982;&#32780;&#65292;&#22312;&#19981;&#30830;&#23450;&#30340;&#29615;&#22659;&#20013;&#65292;&#20250;&#26377;&#20004;&#20010;&#38382;&#39064;&#20986;&#29616;&#12290;&#31532;&#19968;&#65292;&#31574;&#30053;&#21487;&#33021;&#32570;&#20047;&#40065;&#26834;&#24615;&#21644;&#21487;&#37325;&#22797;&#24615;&#65292;&#21363;&#22312;&#30053;&#24494;&#19981;&#21516;&#30340;&#24773;&#20917;&#19979;&#65292;&#22810;&#20010; episodes &#24448;&#24448;&#20250;&#23548;&#33268;&#38750;&#24120;&#19981;&#21516;&#30340;&#34892;&#20026;&#32467;&#26524;&#12290;&#31532;&#20108;&#65292;&#30001;&#20110;&#31574;&#30053;&#38598;&#30340;&#31163;&#25955;&#24615;&#65292;&#35299;&#20915;&#26041;&#26696;&#30340;&#21464;&#21270;&#26159;&#19981;&#36830;&#32493;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#23454;&#29616;&#22522;&#20110;&#34892;&#20026;&#26465;&#20214;&#19979;&#30340;&#36712;&#36857;&#29983;&#25104;&#65292;&#20854;&#22522;&#20110;&#20004;&#20010;&#26426;&#21046;&#65306;&#39318;&#20808;&#26159; MAP-Elites Low-Spread (ME-LS)&#65292;&#23427;&#38480;&#21046;&#20102;&#36873;&#25321;&#37027;&#20123;&#22312;&#34892;&#20026;&#31354;&#38388;&#19978;&#26368;&#19968;&#33268;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#20854;&#27425;&#26159;&#36136;&#37327;&#22810;&#26679;&#24615;&#21464;&#24418;&#22120; (QDT)&#65292;&#23427;&#26159;&#22522;&#20110; Transformer &#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the context of neuroevolution, Quality-Diversity algorithms have proven effective in generating repertoires of diverse and efficient policies by relying on the definition of a behavior space. A natural goal induced by the creation of such a repertoire is trying to achieve behaviors on demand, which can be done by running the corresponding policy from the repertoire. However, in uncertain environments, two problems arise. First, policies can lack robustness and repeatability, meaning that multiple episodes under slightly different conditions often result in very different behaviors. Second, due to the discrete nature of the repertoire, solutions vary discontinuously. Here we present a new approach to achieve behavior-conditioned trajectory generation based on two mechanisms: First, MAP-Elites Low-Spread (ME-LS), which constrains the selection of solutions to those that are the most consistent in the behavior space. Second, the Quality-Diversity Transformer (QDT), a Transformer-based 
&lt;/p&gt;</description></item><item><title>&#25193;&#25955;&#27169;&#22411;&#30340;&#23494;&#24230;&#20272;&#35745;&#21487;&#20197;&#34987;&#29992;&#20316;&#38646;&#26679;&#26412;&#20998;&#31867;&#65292;&#20316;&#32773;&#30340;&#29983;&#25104;&#24335;&#20998;&#31867;&#26041;&#27861;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#24378;&#22823;&#30340;&#32467;&#26524;&#65292;&#24182;&#20855;&#26377;&#26356;&#24378;&#30340;&#22810;&#27169;&#24335;&#20851;&#31995;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.16203</link><description>&lt;p&gt;
&#24744;&#30340;&#25193;&#25955;&#27169;&#22411;&#26263;&#20013;&#26159;&#19968;&#31181;&#38646;&#26679;&#26412;&#20998;&#31867;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Your Diffusion Model is Secretly a Zero-Shot Classifier. (arXiv:2303.16203v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16203
&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#30340;&#23494;&#24230;&#20272;&#35745;&#21487;&#20197;&#34987;&#29992;&#20316;&#38646;&#26679;&#26412;&#20998;&#31867;&#65292;&#20316;&#32773;&#30340;&#29983;&#25104;&#24335;&#20998;&#31867;&#26041;&#27861;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#24378;&#22823;&#30340;&#32467;&#26524;&#65292;&#24182;&#20855;&#26377;&#26356;&#24378;&#30340;&#22810;&#27169;&#24335;&#20851;&#31995;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#35268;&#27169;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#26497;&#22823;&#22320;&#22686;&#24378;&#20102;&#25105;&#20204;&#30340;&#22522;&#20110;&#25991;&#26412;&#29983;&#25104;&#22270;&#20687;&#30340;&#33021;&#21147;&#12290;&#36825;&#20123;&#27169;&#22411;&#21487;&#20197;&#20026;&#22823;&#37327;&#25552;&#31034;&#29983;&#25104;&#36924;&#30495;&#30340;&#22270;&#20687;&#65292;&#24182;&#23637;&#31034;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32452;&#21512;&#27867;&#21270;&#33021;&#21147;&#12290;&#20960;&#20046;&#25152;&#26377;&#30340;&#29992;&#20363;&#21040;&#30446;&#21069;&#20026;&#27490;&#37117;&#21482;&#20851;&#27880;&#25277;&#26679;&#65292;&#28982;&#32780;&#65292;&#25193;&#25955;&#27169;&#22411;&#36824;&#21487;&#20197;&#25552;&#20379;&#26377;&#29992;&#20110;&#22270;&#20687;&#29983;&#25104;&#20043;&#22806;&#30340;&#26465;&#20214;&#23494;&#24230;&#20272;&#35745;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#31867;&#20284;&#20110;Stable Diffusion&#30340;&#22823;&#35268;&#27169;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#30340;&#23494;&#24230;&#20272;&#35745;&#21487;&#20197;&#34987;&#21033;&#29992;&#26469;&#25191;&#34892;&#38646;&#26679;&#26412;&#20998;&#31867;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#29983;&#25104;&#24335;&#20998;&#31867;&#26041;&#27861;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#24378;&#22823;&#30340;&#32467;&#26524;&#65292;&#24182;&#20248;&#20110;&#20174;&#25193;&#25955;&#27169;&#22411;&#20013;&#25552;&#21462;&#30693;&#35782;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;&#25105;&#20204;&#22522;&#20110;&#25193;&#25955;&#30340;&#26041;&#27861;&#27604;&#31454;&#20105;&#24615;&#30340;&#23545;&#27604;&#26041;&#27861;&#20855;&#26377;&#26356;&#24378;&#30340;&#22810;&#27169;&#24335;&#20851;&#31995;&#25512;&#29702;&#33021;&#21147;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#24182;&#21576;&#29616;&#20102;&#23450;&#24615;&#32467;&#26524;&#65292;&#35777;&#26126;&#23427;&#23398;&#20064;&#20102;&#26377;&#24847;&#20041;&#30340;&#22270;&#20687;-&#25991;&#26412;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent wave of large-scale text-to-image diffusion models has dramatically increased our text-based image generation abilities. These models can generate realistic images for a staggering variety of prompts and exhibit impressive compositional generalization abilities. Almost all use cases thus far have solely focused on sampling; however, diffusion models can also provide conditional density estimates, which are useful for tasks beyond image generation. In this paper, we show that the density estimates from large-scale text-to-image diffusion models like Stable Diffusion can be leveraged to perform zero-shot classification without any additional training. Our generative approach to classification attains strong results on a variety of benchmarks and outperforms alternative methods of extracting knowledge from diffusion models. We also find that our diffusion-based approach has stronger multimodal relational reasoning abilities than competing contrastive approaches. Finally, we eva
&lt;/p&gt;</description></item><item><title>&#22312;NLP&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#19981;&#33021;&#20165;&#20973;&#24863;&#30693;&#36136;&#37327;&#20551;&#23450;&#20195;&#30721;&#27491;&#30830;&#24615;&#65292;&#24212;&#35813;&#25512;&#21160;&#37319;&#29992;&#32534;&#30721;&#26368;&#20339;&#23454;&#36341;&#20197;&#25552;&#39640;&#23454;&#39564;&#32467;&#26524;&#30340;&#27491;&#30830;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.16166</link><description>&lt;p&gt;
&#27809;&#26377;&#27491;&#30830;&#24615;&#30340;&#21487;&#37325;&#22797;&#24615;&#24182;&#19981;&#37325;&#35201;&#65306;&#22312;NLP&#39046;&#22495;&#20013;&#27979;&#35797;&#20195;&#30721;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reproducibility is Nothing without Correctness: The Importance of Testing Code in NLP. (arXiv:2303.16166v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16166
&lt;/p&gt;
&lt;p&gt;
&#22312;NLP&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#19981;&#33021;&#20165;&#20973;&#24863;&#30693;&#36136;&#37327;&#20551;&#23450;&#20195;&#30721;&#27491;&#30830;&#24615;&#65292;&#24212;&#35813;&#25512;&#21160;&#37319;&#29992;&#32534;&#30721;&#26368;&#20339;&#23454;&#36341;&#20197;&#25552;&#39640;&#23454;&#39564;&#32467;&#26524;&#30340;&#27491;&#30830;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20854;&#22312;&#30740;&#31350;&#23454;&#39564;&#20013;&#21457;&#25381;&#20102;&#20851;&#38190;&#20316;&#29992;&#65292;&#20294;&#20195;&#30721;&#27491;&#30830;&#24615;&#24448;&#24448;&#20165;&#22522;&#20110;&#32467;&#26524;&#30340;&#24863;&#30693;&#36136;&#37327;&#32780;&#34987;&#20551;&#23450;&#12290;&#36825;&#24102;&#26469;&#20102;&#38169;&#35823;&#32467;&#26524;&#21644;&#28508;&#22312;&#35823;&#23548;&#24615;&#21457;&#29616;&#30340;&#39118;&#38505;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#35748;&#20026;&#24403;&#21069;&#20851;&#27880;&#32467;&#26524;&#37325;&#29616;&#24212;&#35813;&#19982;&#24378;&#35843;&#32534;&#30721;&#26368;&#20339;&#23454;&#36341;&#30456;&#36741;&#30456;&#25104;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#20010;&#26696;&#20363;&#30740;&#31350;&#26469;&#25903;&#25345;&#25105;&#20204;&#21521;NLP&#31038;&#21306;&#21457;&#20986;&#30340;&#21495;&#21484;&#65292;&#22312;&#36825;&#20010;&#26696;&#20363;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35782;&#21035;&#20986;&#24182;&#32416;&#27491;&#20102;&#24191;&#27867;&#20351;&#29992;&#30340;&#26368;&#20808;&#36827;Conformer&#26550;&#26500;&#30340;&#24320;&#28304;&#23454;&#29616;&#20013;&#30340;&#19977;&#20010;Bug&#12290;&#36890;&#36807;&#22312;&#21508;&#31181;&#35821;&#35328;&#29615;&#22659;&#19979;&#36827;&#34892;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#21644;&#32763;&#35793;&#30340;&#27604;&#36739;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;Bug&#30340;&#23384;&#22312;&#24182;&#19981;&#20250;&#22952;&#30861;&#33719;&#24471;&#33391;&#22909;&#30340;&#21644;&#21487;&#37325;&#22797;&#30340;&#32467;&#26524;&#65292;&#21453;&#32780;&#21487;&#33021;&#23548;&#33268;&#19981;&#27491;&#30830;&#30340;&#32467;&#35770;&#65292;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#21487;&#33021;&#25552;&#20379;&#38169;&#35823;&#30340;&#25351;&#23548;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#38382;&#39064;&#65292;&#36825;&#39033;&#30740;&#31350;&#21628;&#21505;&#37319;&#29992;&#26088;&#22312;&#20419;&#36827;NLP&#30740;&#31350;&#20013;&#27491;&#30830;&#24615;&#30340;&#32534;&#30721;&#26368;&#20339;&#23454;&#36341;&#65292;&#24182;&#25552;&#39640;&#23454;&#39564;&#32467;&#26524;&#30340;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite its pivotal role in research experiments, code correctness is often presumed only on the basis of the perceived quality of the results. This comes with the risk of erroneous outcomes and potentially misleading findings. To address this issue, we posit that the current focus on result reproducibility should go hand in hand with the emphasis on coding best practices. We bolster our call to the NLP community by presenting a case study, in which we identify (and correct) three bugs in widely used open-source implementations of the state-of-the-art Conformer architecture. Through comparative experiments on automatic speech recognition and translation in various language settings, we demonstrate that the existence of bugs does not prevent the achievement of good and reproducible results and can lead to incorrect conclusions that potentially misguide future research. In response to this, this study is a call to action toward the adoption of coding best practices aimed at fostering cor
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#26426;&#22120;&#23398;&#20064;&#22312;&#23454;&#39564;&#27969;&#20307;&#21147;&#23398;&#20013;&#30340;&#19977;&#20010;&#26041;&#38754;&#30340;&#24212;&#29992;&#28508;&#21147;&#65306;&#25552;&#39640;&#27979;&#37327;&#25216;&#26415;&#31934;&#24230;&#65292;&#25913;&#36827;&#23454;&#39564;&#35774;&#35745;&#21644;&#23454;&#29616;&#23454;&#26102;&#20272;&#35745;&#21644;&#25511;&#21046;&#12290;</title><link>http://arxiv.org/abs/2303.15832</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22312;&#27969;&#20307;&#21147;&#23398;&#23454;&#39564;&#20013;&#30340;&#21464;&#38761;&#24615;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;
The transformative potential of machine learning for experiments in fluid mechanics. (arXiv:2303.15832v1 [physics.flu-dyn])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15832
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#26426;&#22120;&#23398;&#20064;&#22312;&#23454;&#39564;&#27969;&#20307;&#21147;&#23398;&#20013;&#30340;&#19977;&#20010;&#26041;&#38754;&#30340;&#24212;&#29992;&#28508;&#21147;&#65306;&#25552;&#39640;&#27979;&#37327;&#25216;&#26415;&#31934;&#24230;&#65292;&#25913;&#36827;&#23454;&#39564;&#35774;&#35745;&#21644;&#23454;&#29616;&#23454;&#26102;&#20272;&#35745;&#21644;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#22312;&#35768;&#22810;&#31185;&#23398;&#21644;&#24037;&#31243;&#39046;&#22495;&#21462;&#24471;&#20102;&#24555;&#36895;&#36827;&#23637;&#65292;&#20854;&#20013;&#21253;&#25324;&#23454;&#39564;&#27969;&#20307;&#21147;&#23398;&#65292;&#36825;&#26159;&#26368;&#21021;&#30340;&#22823;&#25968;&#25454;&#23398;&#31185;&#20043;&#19968;&#12290;&#26412;&#25991;&#23558;&#37325;&#28857;&#20171;&#32461;&#23454;&#39564;&#27969;&#20307;&#21147;&#23398;&#20013;&#20960;&#20010;&#21463;&#30410;&#20110;&#26426;&#22120;&#23398;&#20064;&#36827;&#23637;&#30340;&#26041;&#38754;&#65292;&#21253;&#25324;&#65306;1&#65289;&#22686;&#24378;&#27979;&#37327;&#25216;&#26415;&#30340;&#31934;&#24230;&#21644;&#36136;&#37327;&#65292;2&#65289;&#25913;&#36827;&#23454;&#39564;&#35774;&#35745;&#21644;&#26367;&#20195;&#25968;&#20540;&#23402;&#29983;&#27169;&#22411;&#21644;3&#65289;&#23454;&#29616;&#23454;&#26102;&#20272;&#35745;&#21644;&#25511;&#21046;&#12290;&#23545;&#20110;&#27599;&#20010;&#26041;&#38754;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#26368;&#36817;&#30340;&#25104;&#21151;&#26696;&#20363;&#21644;&#27491;&#22312;&#36827;&#34892;&#30340;&#25361;&#25112;&#65292;&#20197;&#21450;&#27880;&#24847;&#20107;&#39033;&#21644;&#38480;&#21046;&#65292;&#24182;&#27010;&#36848;&#20102;ML&#22686;&#24378;&#21644;ML&#33021;&#21147;&#30340;&#23454;&#39564;&#27969;&#20307;&#21147;&#23398;&#30340;&#26032;&#36884;&#24452;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The field of machine learning has rapidly advanced the state of the art in many fields of science and engineering, including experimental fluid dynamics, which is one of the original big-data disciplines. This perspective will highlight several aspects of experimental fluid mechanics that stand to benefit from progress advances in machine learning, including: 1) augmenting the fidelity and quality of measurement techniques, 2) improving experimental design and surrogate digital-twin models and 3) enabling real-time estimation and control. In each case, we discuss recent success stories and ongoing challenges, along with caveats and limitations, and outline the potential for new avenues of ML-augmented and ML-enabled experimental fluid mechanics.
&lt;/p&gt;</description></item><item><title>&#22522;&#30784;&#27169;&#22411;&#24320;&#21457;&#21644;&#37096;&#32626;&#38656;&#30830;&#20445;&#22312;&#21512;&#29702;&#20351;&#29992;&#30340;&#33539;&#22260;&#20869;&#65292;&#20294;&#36825;&#24182;&#19981;&#20445;&#35777;&#12290;&#22312;&#20351;&#29992;&#26377;&#29256;&#26435;&#20869;&#23481;&#26102;&#38656;&#35201;&#27880;&#24847;&#39118;&#38505;&#24182;&#21487;&#33021;&#38656;&#35201;&#36827;&#19968;&#27493;&#30340;&#24037;&#20316;&#12290;</title><link>http://arxiv.org/abs/2303.15715</link><description>&lt;p&gt;
&#22522;&#30784;&#27169;&#22411;&#19982;&#21512;&#29702;&#20351;&#29992;
&lt;/p&gt;
&lt;p&gt;
Foundation Models and Fair Use. (arXiv:2303.15715v1 [cs.CY] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15715
&lt;/p&gt;
&lt;p&gt;
&#22522;&#30784;&#27169;&#22411;&#24320;&#21457;&#21644;&#37096;&#32626;&#38656;&#30830;&#20445;&#22312;&#21512;&#29702;&#20351;&#29992;&#30340;&#33539;&#22260;&#20869;&#65292;&#20294;&#36825;&#24182;&#19981;&#20445;&#35777;&#12290;&#22312;&#20351;&#29992;&#26377;&#29256;&#26435;&#20869;&#23481;&#26102;&#38656;&#35201;&#27880;&#24847;&#39118;&#38505;&#24182;&#21487;&#33021;&#38656;&#35201;&#36827;&#19968;&#27493;&#30340;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#22522;&#30784;&#27169;&#22411;&#26159;&#22522;&#20110;&#26377;&#29256;&#26435;&#30340;&#26448;&#26009;&#35757;&#32451;&#20986;&#26469;&#30340;&#12290;&#22312;&#25968;&#25454;&#21019;&#24314;&#32773;&#27809;&#26377;&#24471;&#21040;&#36866;&#24403;&#30340;&#24402;&#23646;&#25110;&#36180;&#20607;&#26102;&#37096;&#32626;&#36825;&#20123;&#27169;&#22411;&#21487;&#33021;&#24102;&#26469;&#27861;&#24459;&#21644;&#36947;&#24503;&#39118;&#38505;&#12290;&#22312;&#32654;&#22269;&#21644;&#20854;&#20182;&#20960;&#20010;&#22269;&#23478;&#65292;&#29256;&#26435;&#20869;&#23481;&#21487;&#33021;&#34987;&#29992;&#20110;&#26500;&#24314;&#22522;&#30784;&#27169;&#22411;&#32780;&#19981;&#20250;&#20135;&#29983;&#27861;&#24459;&#36131;&#20219;&#65292;&#36825;&#26159;&#22240;&#20026;&#21512;&#29702;&#20351;&#29992;&#21407;&#21017;&#30340;&#23384;&#22312;&#12290;&#20294;&#26159;&#38656;&#35201;&#27880;&#24847;&#30340;&#26159;&#65292;&#22914;&#26524;&#36825;&#20123;&#27169;&#22411;&#30340;&#36755;&#20986;&#20135;&#29983;&#20102;&#19982;&#29256;&#26435;&#25968;&#25454;&#30456;&#20284;&#30340;&#32467;&#26524;&#65292;&#23588;&#20854;&#26159;&#22312;&#24433;&#21709;&#21040;&#36825;&#20123;&#25968;&#25454;&#24066;&#22330;&#30340;&#24773;&#20917;&#19979;&#65292;&#21512;&#29702;&#20351;&#29992;&#21407;&#21017;&#23558;&#19981;&#20877;&#36866;&#29992;&#20110;&#27169;&#22411;&#30340;&#36755;&#20986;&#12290;&#26412;&#25991;&#24378;&#35843;&#21512;&#29702;&#20351;&#29992;&#21407;&#21017;&#24182;&#19981;&#20445;&#35777;&#65292;&#21487;&#33021;&#38656;&#35201;&#36827;&#19968;&#27493;&#30340;&#24037;&#20316;&#25165;&#33021;&#30830;&#20445;&#27169;&#22411;&#30340;&#24320;&#21457;&#21644;&#37096;&#32626;&#22312;&#21512;&#29702;&#20351;&#29992;&#30340;&#33539;&#22260;&#20869;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#22522;&#20110;&#26377;&#29256;&#26435;&#20869;&#23481;&#30340;&#24320;&#21457;&#21644;&#37096;&#32626;&#22522;&#30784;&#27169;&#22411;&#30340;&#28508;&#22312;&#39118;&#38505;&#12290;&#25105;&#20204;&#22238;&#39038;&#20102;&#30456;&#20851;&#30340;&#32654;&#22269;&#26696;&#20363;&#27861;&#65292;&#24182;&#31867;&#27604;&#29983;&#25104;&#25991;&#26412;&#12289;&#28304;&#20195;&#30721;&#21644;&#21487;&#35270;&#21270;&#31561;&#21487;&#33021;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing foundation models are trained on copyrighted material. Deploying these models can pose both legal and ethical risks when data creators fail to receive appropriate attribution or compensation. In the United States and several other countries, copyrighted content may be used to build foundation models without incurring liability due to the fair use doctrine. However, there is a caveat: If the model produces output that is similar to copyrighted data, particularly in scenarios that affect the market of that data, fair use may no longer apply to the output of the model. In this work, we emphasize that fair use is not guaranteed, and additional work may be necessary to keep model development and deployment squarely in the realm of fair use. First, we survey the potential risks of developing and deploying foundation models based on copyrighted content. We review relevant U.S. case law, drawing parallels to existing and potential applications for generating text, source code, and vis
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26041;&#27861;&#26469;&#35777;&#26126;$\ell_2$-norm&#19979;&#30340;&#26679;&#26412;&#22806;&#26816;&#27979;&#30340;&#40065;&#26834;&#24615;&#65292;&#22312;&#19981;&#32771;&#34385;&#32593;&#32476;&#26550;&#26500;&#21644;&#20855;&#20307;&#32452;&#20214;&#30340;&#24773;&#20917;&#19979;&#65292;&#25913;&#21892;&#20102;&#23545;&#25239;&#25915;&#20987;&#30340;&#26816;&#27979;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2303.14961</link><description>&lt;p&gt;
&#36890;&#36807;&#25193;&#25955;&#21435;&#22122;&#24179;&#28369;&#36827;&#34892;&#35748;&#35777;&#21644;&#23545;&#25239;&#24615;&#30340;&#40065;&#26834;&#30340;&#26679;&#26412;&#22806;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Diffusion Denoised Smoothing for Certified and Adversarial Robust Out-Of-Distribution Detection. (arXiv:2303.14961v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14961
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26041;&#27861;&#26469;&#35777;&#26126;$\ell_2$-norm&#19979;&#30340;&#26679;&#26412;&#22806;&#26816;&#27979;&#30340;&#40065;&#26834;&#24615;&#65292;&#22312;&#19981;&#32771;&#34385;&#32593;&#32476;&#26550;&#26500;&#21644;&#20855;&#20307;&#32452;&#20214;&#30340;&#24773;&#20917;&#19979;&#65292;&#25913;&#21892;&#20102;&#23545;&#25239;&#25915;&#20987;&#30340;&#26816;&#27979;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#30340;&#24212;&#29992;&#19981;&#26029;&#25193;&#23637;&#65292;&#30830;&#20445;&#20854;&#23433;&#20840;&#24615;&#21464;&#24471;&#23588;&#20026;&#37325;&#35201;&#12290;&#20854;&#20013;&#19968;&#20010;&#20027;&#35201;&#20851;&#27880;&#28857;&#26159;&#35782;&#21035;&#32473;&#23450;&#26679;&#26412;&#26159;&#21542;&#26469;&#33258;&#35757;&#32451;&#20998;&#24067;&#65292;&#25110;&#32773;&#26159;&#19968;&#20010;&#8220;&#26679;&#26412;&#22806;&#8221;&#65288;OOD&#65289;&#26679;&#26412;&#12290;&#27492;&#22806;&#65292;&#23545;&#25163;&#21487;&#20197;&#20197;&#19968;&#31181;&#23548;&#33268;&#20998;&#31867;&#22120;&#20570;&#20986;&#33258;&#20449;&#39044;&#27979;&#30340;&#26041;&#24335;&#25805;&#32437;OOD&#26679;&#26412;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#36755;&#20837;&#30340;L2&#33539;&#22260;&#20869;&#35777;&#26126;&#22312;&#19981;&#32771;&#34385;&#32593;&#32476;&#26550;&#26500;&#20197;&#21450;&#19981;&#38656;&#35201;&#29305;&#23450;&#32452;&#20214;&#25110;&#39069;&#22806;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#65292;&#23545;OOD&#26816;&#27979;&#30340;&#40065;&#26834;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25913;&#36827;&#20102;&#26816;&#27979;OOD&#26679;&#26412;&#30340;&#23545;&#25239;&#25915;&#20987;&#30340;&#25216;&#26415;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#23545;&#20110;&#20998;&#24067;&#26679;&#26412;&#30340;&#39640;&#27700;&#24179;&#30340;&#35748;&#35777;&#21644;&#23545;&#25239;&#30340;&#32467;&#26524;&#12290;&#22312;CIFAR10/100&#30340;&#25152;&#26377;OOD&#26816;&#27979;&#25351;&#26631;&#30340;&#24179;&#22343;&#20540;&#26174;&#31034;&#65292;&#19982;&#20197;&#21069;&#30340;&#26041;&#27861;&#30456;&#27604;&#25552;&#39640;&#20102;&#32422;13&#65285;/ 5&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
As the use of machine learning continues to expand, the importance of ensuring its safety cannot be overstated. A key concern in this regard is the ability to identify whether a given sample is from the training distribution, or is an "Out-Of-Distribution" (OOD) sample. In addition, adversaries can manipulate OOD samples in ways that lead a classifier to make a confident prediction. In this study, we present a novel approach for certifying the robustness of OOD detection within a $\ell_2$-norm around the input, regardless of network architecture and without the need for specific components or additional training. Further, we improve current techniques for detecting adversarial attacks on OOD samples, while providing high levels of certified and adversarial robustness on in-distribution samples. The average of all OOD detection metrics on CIFAR10/100 shows an increase of $\sim 13 \% / 5\%$ relative to previous approaches.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#20107;&#20214;&#30340;&#26102;&#31354;&#25554;&#20540;&#26469;&#23454;&#29616;&#38543;&#26426;&#23610;&#24230;&#19979;&#30340;&#35270;&#39057;&#36229;&#20998;&#36776;&#29575;&#12290;&#26041;&#27861;&#21253;&#25324;&#20174;RGB&#24103;&#21644;&#20107;&#20214;&#30340;&#26597;&#35810;&#31354;&#38388;-&#26102;&#38388;&#22352;&#26631;&#21644;&#29305;&#24449;&#20013;&#23398;&#20064;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2303.13767</link><description>&lt;p&gt;
&#23398;&#20064;&#26102;&#31354;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#26469;&#23454;&#29616;&#20107;&#20214;&#24341;&#23548;&#30340;&#35270;&#39057;&#36229;&#20998;&#36776;&#29575;
&lt;/p&gt;
&lt;p&gt;
Learning Spatial-Temporal Implicit Neural Representations for Event-Guided Video Super-Resolution. (arXiv:2303.13767v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13767
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#20107;&#20214;&#30340;&#26102;&#31354;&#25554;&#20540;&#26469;&#23454;&#29616;&#38543;&#26426;&#23610;&#24230;&#19979;&#30340;&#35270;&#39057;&#36229;&#20998;&#36776;&#29575;&#12290;&#26041;&#27861;&#21253;&#25324;&#20174;RGB&#24103;&#21644;&#20107;&#20214;&#30340;&#26597;&#35810;&#31354;&#38388;-&#26102;&#38388;&#22352;&#26631;&#21644;&#29305;&#24449;&#20013;&#23398;&#20064;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20107;&#20214;&#30456;&#26426;&#24322;&#27493;&#24863;&#30693;&#24378;&#24230;&#21464;&#21270;&#65292;&#20135;&#29983;&#20855;&#26377;&#39640;&#21160;&#24577;&#33539;&#22260;&#21644;&#20302;&#24310;&#36831;&#30340;&#20107;&#20214;&#27969;&#12290;&#36825;&#28608;&#21457;&#20102;&#21033;&#29992;&#20107;&#20214;&#24341;&#23548;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#35270;&#39057;&#36229;&#20998;&#36776;&#29575;&#65288;VSR&#65289;&#20219;&#21153;&#30340;&#30740;&#31350;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#23581;&#35797;&#21033;&#29992;&#20107;&#20214;&#30340;&#39640;&#26102;&#24207;&#20998;&#36776;&#29575;&#24615;&#36136;&#65292;&#36890;&#36807;&#21033;&#29992;&#20107;&#20214;&#30340;&#26102;&#31354;&#25554;&#20540;&#26469;&#23454;&#29616;&#38543;&#26426;&#23610;&#24230;&#19979;&#30340;VSR&#12290;&#24403;&#24341;&#23548;VSR&#26102;&#65292;&#20107;&#20214;&#30340;&#26102;&#31354;&#20449;&#24687;&#30340;&#34920;&#31034;&#20855;&#26377;&#22256;&#38590;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#23558;&#20107;&#20214;&#30340;&#26102;&#31354;&#25554;&#20540;&#19982;VSR&#32467;&#21512;&#22312;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#20013;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#24605;&#24819;&#26159;&#20174;RGB&#24103;&#21644;&#20107;&#20214;&#30340;&#26597;&#35810;&#31354;&#38388;-&#26102;&#38388;&#22352;&#26631;&#21644;&#29305;&#24449;&#20013;&#23398;&#20064;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20998;&#20026;&#19977;&#37096;&#20998;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#31354;&#26102;&#34701;&#21512;&#65288;STF&#65289;&#27169;&#22359;&#39318;&#20808;&#23398;&#20064;&#20107;&#20214;&#21644;RGB&#24103;&#30340;3D&#29305;&#24449;&#12290;&#28982;&#21518;&#65292;&#26102;&#22495;&#28388;&#27874;&#22120;&#65288;TF&#65289;&#27169;&#22359;&#35299;&#38145;&#20102;&#26356;&#22810;&#29305;&#24449;&#24182;&#32473;&#20986;&#20102;&#31934;&#32454;&#30340;VSR&#32467;&#26524;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;VSR&#25968;&#25454;&#38598;&#65292;&#20197;&#20415;&#28145;&#24230;&#23398;&#20064;&#30340;VSR&#30740;&#31350;&#20154;&#21592;&#21487;&#20197;&#35780;&#20272;&#20854;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Event cameras sense the intensity changes asynchronously and produce event streams with high dynamic range and low latency. This has inspired research endeavors utilizing events to guide the challenging video superresolution (VSR) task. In this paper, we make the first attempt to address a novel problem of achieving VSR at random scales by taking advantages of the high temporal resolution property of events. This is hampered by the difficulties of representing the spatial-temporal information of events when guiding VSR. To this end, we propose a novel framework that incorporates the spatial-temporal interpolation of events to VSR in a unified framework. Our key idea is to learn implicit neural representations from queried spatial-temporal coordinates and features from both RGB frames and events. Our method contains three parts. Specifically, the Spatial-Temporal Fusion (STF) module first learns the 3D features from events and RGB frames. Then, the Temporal Filter (TF) module unlocks mo
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#39537;&#21160;&#21464;&#21387;&#22120;&#30340;&#35270;&#39057; HMR &#26694;&#26550;&#65288;DDT&#65289;&#65292;&#23427;&#26088;&#22312;&#20174;&#36755;&#20837;&#24207;&#21015;&#20013;&#35299;&#30721;&#29305;&#23450;&#30340;&#36816;&#21160;&#27169;&#24335;&#65292;&#22686;&#24378;&#36816;&#21160;&#24179;&#28369;&#24615;&#21644;&#26102;&#38388;&#19968;&#33268;&#24615;&#65292;&#24182;&#36755;&#20986;&#25152;&#26377;&#24103;&#30340;&#20154;&#20307;&#32593;&#26684;&#65292;&#20351;&#24471; DDT &#26356;&#36866;&#29992;&#20110;&#26102;&#38388;&#25928;&#29575;&#33267;&#20851;&#37325;&#35201;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2303.13397</link><description>&lt;p&gt;
DDT&#65306;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#39537;&#21160;&#21464;&#21387;&#22120;&#30340;&#20174;&#35270;&#39057;&#20013;&#24674;&#22797;&#20154;&#20307;&#32593;&#26684;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
DDT: A Diffusion-Driven Transformer-based Framework for Human Mesh Recovery from a Video. (arXiv:2303.13397v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13397
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#39537;&#21160;&#21464;&#21387;&#22120;&#30340;&#35270;&#39057; HMR &#26694;&#26550;&#65288;DDT&#65289;&#65292;&#23427;&#26088;&#22312;&#20174;&#36755;&#20837;&#24207;&#21015;&#20013;&#35299;&#30721;&#29305;&#23450;&#30340;&#36816;&#21160;&#27169;&#24335;&#65292;&#22686;&#24378;&#36816;&#21160;&#24179;&#28369;&#24615;&#21644;&#26102;&#38388;&#19968;&#33268;&#24615;&#65292;&#24182;&#36755;&#20986;&#25152;&#26377;&#24103;&#30340;&#20154;&#20307;&#32593;&#26684;&#65292;&#20351;&#24471; DDT &#26356;&#36866;&#29992;&#20110;&#26102;&#38388;&#25928;&#29575;&#33267;&#20851;&#37325;&#35201;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#20307;&#32593;&#26684;&#24674;&#22797;&#65288;HMR&#65289;&#20026;&#21508;&#31181;&#23454;&#38469;&#24212;&#29992;&#25552;&#20379;&#20102;&#20016;&#23500;&#30340;&#20154;&#20307;&#20449;&#24687;&#65292;&#20363;&#22914;&#28216;&#25103;&#12289;&#20154;&#26426;&#20132;&#20114;&#21644;&#34394;&#25311;&#29616;&#23454;&#12290;&#19982;&#21333;&#19968;&#22270;&#20687;&#26041;&#27861;&#30456;&#27604;&#65292;&#22522;&#20110;&#35270;&#39057;&#30340;&#26041;&#27861;&#21487;&#20197;&#21033;&#29992;&#26102;&#38388;&#20449;&#24687;&#36890;&#36807;&#34701;&#21512;&#20154;&#20307;&#36816;&#21160;&#20808;&#39564;&#36827;&#19968;&#27493;&#25552;&#39640;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#20687; VIBE &#36825;&#26679;&#30340;&#22810;&#23545;&#22810;&#26041;&#27861;&#23384;&#22312;&#36816;&#21160;&#24179;&#28369;&#24615;&#21644;&#26102;&#38388;&#19968;&#33268;&#24615;&#30340;&#25361;&#25112;&#12290;&#32780;&#20687; TCMR &#21644; MPS-Net &#36825;&#26679;&#30340;&#22810;&#23545;&#19968;&#26041;&#27861;&#21017;&#20381;&#36182;&#20110;&#26410;&#26469;&#24103;&#65292;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#26159;&#38750;&#22240;&#26524;&#21644;&#26102;&#38388;&#25928;&#29575;&#20302;&#19979;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#25193;&#25955;&#39537;&#21160;&#21464;&#21387;&#22120;&#30340;&#35270;&#39057; HMR &#26694;&#26550;&#65288;DDT&#65289;&#12290;DDT &#26088;&#22312;&#20174;&#36755;&#20837;&#24207;&#21015;&#20013;&#35299;&#30721;&#29305;&#23450;&#30340;&#36816;&#21160;&#27169;&#24335;&#65292;&#22686;&#24378;&#36816;&#21160;&#24179;&#28369;&#24615;&#21644;&#26102;&#38388;&#19968;&#33268;&#24615;&#12290;&#20316;&#20026;&#19968;&#31181;&#22810;&#23545;&#22810;&#26041;&#27861;&#65292;DDT &#30340;&#35299;&#30721;&#22120;&#36755;&#20986;&#25152;&#26377;&#24103;&#30340;&#20154;&#20307;&#32593;&#26684;&#65292;&#20351; DDT &#26356;&#36866;&#29992;&#20110;&#26102;&#38388;&#25928;&#29575;&#33267;&#20851;&#37325;&#35201;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human mesh recovery (HMR) provides rich human body information for various real-world applications such as gaming, human-computer interaction, and virtual reality. Compared to single image-based methods, video-based methods can utilize temporal information to further improve performance by incorporating human body motion priors. However, many-to-many approaches such as VIBE suffer from motion smoothness and temporal inconsistency. While many-to-one approaches such as TCMR and MPS-Net rely on the future frames, which is non-causal and time inefficient during inference. To address these challenges, a novel Diffusion-Driven Transformer-based framework (DDT) for video-based HMR is presented. DDT is designed to decode specific motion patterns from the input sequence, enhancing motion smoothness and temporal consistency. As a many-to-many approach, the decoder of our DDT outputs the human mesh of all the frames, making DDT more viable for real-world applications where time efficiency is cruc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#29616;&#26377;&#30340;&#33258;&#21160;&#39550;&#39542;&#36335;&#24452;&#35268;&#21010;&#26041;&#27861;&#65292;&#21253;&#25324;&#31649;&#36947;&#35268;&#21010;&#21644;&#31471;&#21040;&#31471;&#35268;&#21010;&#26041;&#27861;&#12290;&#22312;&#25361;&#25112;&#21644;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#26041;&#38754;&#25552;&#20379;&#20102;&#35752;&#35770;&#65292;&#26377;&#21161;&#20110;&#20026;&#26234;&#33021;&#27773;&#36710;&#30340;&#21457;&#23637;&#25552;&#20379;&#26356;&#22909;&#30340;&#35268;&#21010;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.09824</link><description>&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#36335;&#24452;&#35268;&#21010;&#65306;&#29616;&#29366;&#19982;&#23637;&#26395;
&lt;/p&gt;
&lt;p&gt;
Path Planning for Autonomous Driving: The State of the Art and Perspectives. (arXiv:2303.09824v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09824
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#29616;&#26377;&#30340;&#33258;&#21160;&#39550;&#39542;&#36335;&#24452;&#35268;&#21010;&#26041;&#27861;&#65292;&#21253;&#25324;&#31649;&#36947;&#35268;&#21010;&#21644;&#31471;&#21040;&#31471;&#35268;&#21010;&#26041;&#27861;&#12290;&#22312;&#25361;&#25112;&#21644;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#26041;&#38754;&#25552;&#20379;&#20102;&#35752;&#35770;&#65292;&#26377;&#21161;&#20110;&#20026;&#26234;&#33021;&#27773;&#36710;&#30340;&#21457;&#23637;&#25552;&#20379;&#26356;&#22909;&#30340;&#35268;&#21010;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26234;&#33021;&#27773;&#36710;&#30001;&#20110;&#25552;&#39640;&#30340;&#20415;&#21033;&#24615;&#12289;&#23433;&#20840;&#24615;&#21644;&#28508;&#22312;&#30340;&#21830;&#19994;&#20215;&#20540;&#32780;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#12290;&#20294;&#30001;&#20110;&#21508;&#31181;&#38382;&#39064;&#65292;&#22914;&#23433;&#20840;&#24615;&#12289;&#21487;&#38752;&#24615;&#21644;&#35268;&#21010;&#26041;&#27861;&#30340;&#27867;&#21270;&#31561;&#38480;&#21046;&#65292;&#23427;&#20204;&#30340;&#37096;&#32626;&#20173;&#23616;&#38480;&#20110;&#23567;&#35268;&#27169;&#39564;&#35777;&#38454;&#27573;&#12290;&#26412;&#25991;&#26088;&#22312;&#32508;&#36848;&#26368;&#20808;&#36827;&#30340;&#36335;&#24452;&#35268;&#21010;&#26041;&#27861;&#65292;&#21253;&#25324;&#31649;&#36947;&#35268;&#21010;&#21644;&#31471;&#21040;&#31471;&#35268;&#21010;&#26041;&#27861;&#12290;&#38024;&#23545;&#31649;&#36947;&#26041;&#27861;&#65292;&#26412;&#25991;&#25552;&#20379;&#20102;&#36873;&#21462;&#31639;&#27861;&#30340;&#27010;&#36848;&#65292;&#24182;&#35752;&#35770;&#20102;&#25193;&#23637;&#21644;&#20248;&#21270;&#26426;&#21046;&#65307;&#38024;&#23545;&#31471;&#21040;&#31471;&#26041;&#27861;&#65292;&#26412;&#25991;&#24378;&#35843;&#22521;&#35757;&#21644;&#39564;&#35777;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#36824;&#35752;&#35770;&#20102;&#25361;&#25112;&#21644;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#65292;&#36825;&#26377;&#21161;&#20110;&#20026;&#26234;&#33021;&#27773;&#36710;&#30340;&#21457;&#23637;&#25552;&#20379;&#26356;&#22909;&#30340;&#35268;&#21010;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Intelligent vehicles (IVs) have attracted wide attention thanks to the augmented convenience, safety advantages, and potential commercial value. Although a few of autonomous driving unicorns assert that IVs will be commercially deployable by 2025, their deployment is still restricted to small-scale validation due to various issues, among which safety, reliability, and generalization of planning methods are prominent concerns. Precise computation of control commands or trajectories by planning methods remains a prerequisite for IVs, owing to perceptual imperfections under complex environments, which pose an obstacle to the successful commercialization of IVs. This paper aims to review state-of-the-art planning methods, including pipeline planning and end-to-end planning methods. In terms of pipeline methods, a survey of selecting algorithms is provided along with a discussion of the expansion and optimization mechanisms, whereas in end-to-end methods, the training approaches and verific
&lt;/p&gt;</description></item><item><title>&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#33021;&#21147;&#19981;&#26029;&#25552;&#21319;&#65292;&#25511;&#21046;&#26576;&#20123;&#33021;&#21147;&#23558;&#26377;&#21161;&#20110;&#38450;&#27490;&#20854;&#28389;&#29992;&#65292;&#36825;&#20123;&#38480;&#21046;&#21487;&#33021;&#21253;&#25324;&#25511;&#21046;&#35775;&#38382;&#12289;&#20351;&#29992;&#30446;&#30340;&#12289;&#36755;&#20986;&#19982;&#28335;&#28304;&#20197;&#21450;&#24320;&#21457;&#36164;&#28304;&#65292;&#38750;AI&#33021;&#21147;&#38480;&#21046;&#20063;&#26159;&#24517;&#35201;&#30340;&#12290;&#23613;&#31649;&#21487;&#33021;&#20250;&#38477;&#20302;&#20351;&#29992;&#29575;&#32780;&#22686;&#21152;&#28389;&#29992;&#39118;&#38505;&#65292;&#20294;&#36825;&#20123;&#38480;&#21046;&#26159;&#24403;&#20854;&#20182;&#24178;&#39044;&#34892;&#19981;&#36890;&#12289;&#28508;&#22312;&#21361;&#23475;&#24615;&#39640;&#12289;&#26377;&#26377;&#38024;&#23545;&#24615;&#26041;&#24335;&#24178;&#39044;&#26102;&#25152;&#24517;&#35201;&#30340;&#12290;</title><link>http://arxiv.org/abs/2303.09377</link><description>&lt;p&gt;
&#20445;&#25252;&#31038;&#20250;&#20813;&#21463;AI&#28389;&#29992;&#65306;&#20309;&#26102;&#38480;&#21046;AI&#33021;&#21147;&#26159;&#24517;&#35201;&#30340;&#65311;
&lt;/p&gt;
&lt;p&gt;
Protecting Society from AI Misuse: When are Restrictions on Capabilities Warranted?. (arXiv:2303.09377v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09377
&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#33021;&#21147;&#19981;&#26029;&#25552;&#21319;&#65292;&#25511;&#21046;&#26576;&#20123;&#33021;&#21147;&#23558;&#26377;&#21161;&#20110;&#38450;&#27490;&#20854;&#28389;&#29992;&#65292;&#36825;&#20123;&#38480;&#21046;&#21487;&#33021;&#21253;&#25324;&#25511;&#21046;&#35775;&#38382;&#12289;&#20351;&#29992;&#30446;&#30340;&#12289;&#36755;&#20986;&#19982;&#28335;&#28304;&#20197;&#21450;&#24320;&#21457;&#36164;&#28304;&#65292;&#38750;AI&#33021;&#21147;&#38480;&#21046;&#20063;&#26159;&#24517;&#35201;&#30340;&#12290;&#23613;&#31649;&#21487;&#33021;&#20250;&#38477;&#20302;&#20351;&#29992;&#29575;&#32780;&#22686;&#21152;&#28389;&#29992;&#39118;&#38505;&#65292;&#20294;&#36825;&#20123;&#38480;&#21046;&#26159;&#24403;&#20854;&#20182;&#24178;&#39044;&#34892;&#19981;&#36890;&#12289;&#28508;&#22312;&#21361;&#23475;&#24615;&#39640;&#12289;&#26377;&#26377;&#38024;&#23545;&#24615;&#26041;&#24335;&#24178;&#39044;&#26102;&#25152;&#24517;&#35201;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#31995;&#32479;&#19981;&#26029;&#25552;&#39640;&#33021;&#21147;&#65292;&#20854;&#34987;&#29992;&#20110;&#36896;&#25104;&#20260;&#23475;&#30340;&#24773;&#20917;&#23558;&#20250;&#36234;&#26469;&#36234;&#22810;&#12290;&#20107;&#23454;&#19978;&#65292;AI&#31995;&#32479;&#24050;&#32463;&#24320;&#22987;&#29992;&#20110;&#33258;&#21160;&#21270;&#30340;&#27450;&#35784;&#27963;&#21160;&#12289;&#20405;&#29359;&#20154;&#26435;&#12289;&#21019;&#24314;&#26377;&#23475;&#30340;&#34394;&#20551;&#22270;&#20687;&#20197;&#21450;&#35782;&#21035;&#21361;&#38505;&#27602;&#32032;&#12290;&#20026;&#20102;&#38450;&#27490;AI&#30340;&#26576;&#20123;&#28389;&#29992;&#65292;&#25105;&#20204;&#35748;&#20026;&#26377;&#24517;&#35201;&#23545;&#26576;&#20123;&#33021;&#21147;&#36827;&#34892;&#26377;&#38024;&#23545;&#24615;&#30340;&#24178;&#39044;&#12290;&#36825;&#20123;&#38480;&#21046;&#21487;&#33021;&#21253;&#25324;&#25511;&#21046;&#35841;&#33021;&#35775;&#38382;&#26576;&#20123;&#31867;&#22411;&#30340;AI&#27169;&#22411;&#12289;&#23427;&#20204;&#21487;&#20197;&#29992;&#20110;&#20160;&#20040;&#12289;&#26159;&#21542;&#36807;&#28388;&#36755;&#20986;&#25110;&#32773;&#21487;&#20197;&#36861;&#28335;&#21040;&#20351;&#29992;&#32773;&#20197;&#21450;&#24320;&#21457;&#23427;&#20204;&#25152;&#38656;&#30340;&#36164;&#28304;&#12290;&#25105;&#20204;&#36824;&#35748;&#20026;&#65292;&#19968;&#20123;&#23545;&#28389;&#29992;&#25152;&#38656;&#30340;&#38750;AI&#33021;&#21147;&#38480;&#21046;&#20063;&#26159;&#24517;&#35201;&#30340;&#12290;&#34429;&#28982;&#33021;&#21147;&#38480;&#21046;&#21487;&#33021;&#20250;&#38477;&#20302;&#20351;&#29992;&#29575;&#32780;&#19981;&#26159;&#28389;&#29992;&#29575;&#65288;&#23384;&#22312;&#19981;&#21033;&#30340;&#28389;&#29992;-&#20351;&#29992;&#26435;&#34913;&#65289;&#65292;&#20294;&#25105;&#20204;&#35748;&#20026;&#24403;&#20854;&#20182;&#24178;&#39044;&#34892;&#19981;&#36890;&#12289;&#28508;&#22312;&#28389;&#29992;&#30340;&#21361;&#23475;&#24615;&#24456;&#39640;&#65292;&#24182;&#19988;&#26377;&#26377;&#38024;&#23545;&#24615;&#30340;&#26041;&#24335;&#26469;&#24178;&#39044;&#33021;&#21147;&#26102;&#65292;&#24178;&#39044;&#33021;&#21147;&#26159;&#24517;&#35201;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial intelligence (AI) systems will increasingly be used to cause harm as they grow more capable. In fact, AI systems are already starting to be used to automate fraudulent activities, violate human rights, create harmful fake images, and identify dangerous toxins. To prevent some misuses of AI, we argue that targeted interventions on certain capabilities will be warranted. These restrictions may include controlling who can access certain types of AI models, what they can be used for, whether outputs are filtered or can be traced back to their user, and the resources needed to develop them. We also contend that some restrictions on non-AI capabilities needed to cause harm will be required. Though capability restrictions risk reducing use more than misuse (facing an unfavorable Misuse-Use Tradeoff), we argue that interventions on capabilities are warranted when other interventions are insufficient, the potential harm from misuse is high, and there are targeted ways to intervene on
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#25506;&#35752;&#21033;&#29992;ChatGPT&#23558;&#25918;&#23556;&#23398;&#25253;&#21578;&#32763;&#35793;&#25104;&#36890;&#20439;&#26131;&#25026;&#30340;&#35821;&#35328;&#65292;&#24179;&#22343;&#24471;&#20998;&#20026;5&#20998;&#21046;&#30340;4.1&#20998;&#65292;&#20449;&#24687;&#32570;&#22833;&#29575;&#21644;&#20449;&#24687;&#38169;&#35823;&#29575;&#22343;&#36739;&#20302;&#65292;ChatGPT&#25552;&#20379;&#30340;&#24314;&#35758;&#22823;&#37117;&#19982;&#25918;&#23556;&#23398;&#25253;&#21578;&#30456;&#20851;&#12290;</title><link>http://arxiv.org/abs/2303.09038</link><description>&lt;p&gt;
&#21033;&#29992;ChatGPT&#21644;Prompt Learning&#23558;&#25918;&#23556;&#23398;&#25253;&#21578;&#32763;&#35793;&#25104;&#36890;&#20439;&#26131;&#25026;&#30340;&#35821;&#35328;&#65306;&#32467;&#26524;&#12289;&#38480;&#21046;&#21644;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Translating Radiology Reports into Plain Language using ChatGPT and GPT-4 with Prompt Learning: Promising Results, Limitations, and Potential. (arXiv:2303.09038v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09038
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#25506;&#35752;&#21033;&#29992;ChatGPT&#23558;&#25918;&#23556;&#23398;&#25253;&#21578;&#32763;&#35793;&#25104;&#36890;&#20439;&#26131;&#25026;&#30340;&#35821;&#35328;&#65292;&#24179;&#22343;&#24471;&#20998;&#20026;5&#20998;&#21046;&#30340;4.1&#20998;&#65292;&#20449;&#24687;&#32570;&#22833;&#29575;&#21644;&#20449;&#24687;&#38169;&#35823;&#29575;&#22343;&#36739;&#20302;&#65292;ChatGPT&#25552;&#20379;&#30340;&#24314;&#35758;&#22823;&#37117;&#19982;&#25918;&#23556;&#23398;&#25253;&#21578;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#20316;&#20026;&#19968;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20197;&#20854;&#31867;&#20284;&#20154;&#31867;&#34920;&#36798;&#21644;&#25512;&#29702;&#33021;&#21147;&#32780;&#22791;&#21463;&#20851;&#27880;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20351;&#29992;ChatGPT&#23558;&#25918;&#23556;&#23398;&#25253;&#21578;&#32763;&#35793;&#25104;&#36890;&#20439;&#26131;&#25026;&#30340;&#35821;&#35328;&#30340;&#21487;&#34892;&#24615;&#65292;&#20197;&#20415;&#24739;&#32773;&#21644;&#21307;&#30103;&#26381;&#21153;&#25552;&#20379;&#32773;&#24471;&#21040;&#26356;&#22909;&#30340;&#21307;&#30103;&#25945;&#32946;&#12290;&#30740;&#31350;&#37319;&#38598;&#20102;62&#20221;&#20302;&#21058;&#37327;&#33016;&#37096;CT&#32954;&#30284;&#31579;&#26597;&#25195;&#25551;&#21644;76&#20221;&#33041;MRI&#36716;&#31227;&#24615;&#31579;&#26597;&#25195;&#25551;&#30340;&#25918;&#23556;&#23398;&#25253;&#21578;&#12290;&#26681;&#25454;&#25918;&#23556;&#31185;&#21307;&#24072;&#30340;&#35780;&#20215;&#65292;ChatGPT&#21487;&#20197;&#25104;&#21151;&#23558;&#25918;&#23556;&#23398;&#25253;&#21578;&#32763;&#35793;&#25104;&#36890;&#20439;&#26131;&#25026;&#30340;&#35821;&#35328;&#65292;&#24179;&#22343;&#24471;&#20998;&#20026;5&#20998;&#21046;&#30340;4.1&#20998;&#65292;&#20449;&#24687;&#32570;&#22833;0.07&#22788;&#65292;&#20449;&#24687;&#38169;&#35823;0.11&#22788;&#12290;&#23601;ChatGPT&#25552;&#20379;&#30340;&#24314;&#35758;&#32780;&#35328;&#65292;&#23427;&#20204;&#26159;&#19968;&#33324;&#24615;&#30340;&#30456;&#20851;&#24314;&#35758;&#65292;&#20363;&#22914;&#20445;&#25345;&#19982;&#21307;&#29983;&#30340;&#38543;&#35775;&#21644;&#23494;&#20999;&#30417;&#27979;&#20219;&#20309;&#30151;&#29366;&#65292;&#23545;&#20110;&#20849;138&#20010;&#30149;&#20363;&#20013;&#30340;&#32422;37&#65285;&#65292;ChatGPT&#25552;&#20379;&#20102;&#19982;&#25918;&#23556;&#23398;&#25253;&#21578;&#26377;&#20851;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
The large language model called ChatGPT has drawn extensively attention because of its human-like expression and reasoning abilities. In this study, we investigate the feasibility of using ChatGPT in experiments on using ChatGPT to translate radiology reports into plain language for patients and healthcare providers so that they are educated for improved healthcare. Radiology reports from 62 low-dose chest CT lung cancer screening scans and 76 brain MRI metastases screening scans were collected in the first half of February for this study. According to the evaluation by radiologists, ChatGPT can successfully translate radiology reports into plain language with an average score of 4.1 in the five-point system with 0.07 places of information missing and 0.11 places of misinformation. In terms of the suggestions provided by ChatGPT, they are general relevant such as keeping following-up with doctors and closely monitoring any symptoms, and for about 37% of 138 cases in total ChatGPT offer
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102; I$^2$-SDF &#26041;&#27861;&#65292;&#20351;&#29992;&#21487;&#24494;&#20998;&#30340;&#33945;&#29305;&#21345;&#32599;&#20809;&#32447;&#36319;&#36394;&#25216;&#26415;&#23454;&#29616;&#20102;&#23460;&#20869;&#22330;&#26223;&#30340;&#37325;&#24314;&#21644;&#32534;&#36753;&#65292;&#37319;&#29992;&#27668;&#27873;&#25439;&#22833;&#20989;&#25968;&#21644;&#38169;&#35823;&#24341;&#23548;&#30340;&#33258;&#36866;&#24212;&#37319;&#26679;&#26041;&#26696;&#25552;&#39640;&#20102;&#37325;&#24314;&#36136;&#37327;&#65292;&#21516;&#26102;&#20998;&#35299;&#31070;&#32463;&#36752;&#23556;&#22330;&#20026;&#22330;&#26223;&#30340;&#31354;&#38388;&#21464;&#21270;&#26448;&#26009;&#23454;&#29616;&#20102;&#29289;&#29702;&#21644;&#36924;&#30495;&#30340;&#22330;&#26223;&#32534;&#36753;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2303.07634</link><description>&lt;p&gt;
I$^2$-SDF: &#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#20809;&#32447;&#36861;&#36394;&#23454;&#29616;&#20869;&#37096;&#22330;&#26223;&#37325;&#24314;&#21644;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
I$^2$-SDF: Intrinsic Indoor Scene Reconstruction and Editing via Raytracing in Neural SDFs. (arXiv:2303.07634v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07634
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102; I$^2$-SDF &#26041;&#27861;&#65292;&#20351;&#29992;&#21487;&#24494;&#20998;&#30340;&#33945;&#29305;&#21345;&#32599;&#20809;&#32447;&#36319;&#36394;&#25216;&#26415;&#23454;&#29616;&#20102;&#23460;&#20869;&#22330;&#26223;&#30340;&#37325;&#24314;&#21644;&#32534;&#36753;&#65292;&#37319;&#29992;&#27668;&#27873;&#25439;&#22833;&#20989;&#25968;&#21644;&#38169;&#35823;&#24341;&#23548;&#30340;&#33258;&#36866;&#24212;&#37319;&#26679;&#26041;&#26696;&#25552;&#39640;&#20102;&#37325;&#24314;&#36136;&#37327;&#65292;&#21516;&#26102;&#20998;&#35299;&#31070;&#32463;&#36752;&#23556;&#22330;&#20026;&#22330;&#26223;&#30340;&#31354;&#38388;&#21464;&#21270;&#26448;&#26009;&#23454;&#29616;&#20102;&#29289;&#29702;&#21644;&#36924;&#30495;&#30340;&#22330;&#26223;&#32534;&#36753;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; I$^2$-SDF &#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20351;&#29992;&#21487;&#24494;&#20998;&#30340;&#33945;&#29305;&#21345;&#32599;&#20809;&#32447;&#36861;&#36394;&#25216;&#26415;&#23545;&#31070;&#32463;&#32593;&#32476;&#26377;&#31526;&#21495;&#36317;&#31163;&#22330;&#20013;&#30340;&#22810;&#35270;&#22270;&#22270;&#20687;&#36827;&#34892;&#29289;&#20307;&#37325;&#24314;&#12289;&#36752;&#23556;&#21644;&#26448;&#36136;&#24674;&#22797;&#12290;&#38024;&#23545;&#23567;&#22411;&#29289;&#20307;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#26032;&#30340;&#27668;&#27873;&#25439;&#22833;&#20989;&#25968;&#65292;&#20197;&#21450;&#35823;&#24046;&#24341;&#23548;&#33258;&#36866;&#24212;&#21462;&#26679;&#26041;&#26696;&#65292;&#22823;&#22823;&#25552;&#39640;&#20102;&#22823;&#35268;&#27169;&#23460;&#20869;&#22330;&#26223;&#30340;&#37325;&#24314;&#36136;&#37327;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;&#31070;&#32463;&#36752;&#23556;&#22330;&#20998;&#35299;&#20026;&#22330;&#26223;&#30340;&#31354;&#38388;&#21464;&#21270;&#26448;&#26009;&#65292;&#24182;&#36890;&#36807;&#22522;&#20110;&#34920;&#38754;&#30340;&#21487;&#24494;&#33945;&#29305;&#21345;&#32599;&#20809;&#32447;&#36861;&#36394;&#21644;&#21457;&#23556;&#22120;&#35821;&#20041;&#20998;&#21106;&#26469;&#23454;&#29616;&#22522;&#20110;&#29289;&#29702;&#21644;&#36924;&#30495;&#30340;&#22330;&#26223;&#37325;&#29031;&#21644;&#32534;&#36753;&#24212;&#29992;&#12290;&#36890;&#36807;&#22823;&#37327;&#30340;&#23450;&#37327;&#21644;&#23450;&#24615;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#23460;&#20869;&#22330;&#26223;&#37325;&#24314;&#12289;&#26032;&#35270;&#35282;&#21512;&#25104;&#21644;&#22330;&#26223;&#32534;&#36753;&#26041;&#38754;&#30340;&#21331;&#36234;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we present I$^2$-SDF, a new method for intrinsic indoor scene reconstruction and editing using differentiable Monte Carlo raytracing on neural signed distance fields (SDFs). Our holistic neural SDF-based framework jointly recovers the underlying shapes, incident radiance and materials from multi-view images. We introduce a novel bubble loss for fine-grained small objects and error-guided adaptive sampling scheme to largely improve the reconstruction quality on large-scale indoor scenes. Further, we propose to decompose the neural radiance field into spatially-varying material of the scene as a neural field through surface-based, differentiable Monte Carlo raytracing and emitter semantic segmentations, which enables physically based and photorealistic scene relighting and editing applications. Through a number of qualitative and quantitative experiments, we demonstrate the superior quality of our method on indoor scene reconstruction, novel view synthesis, and scene editin
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#31890;&#24230;&#39063;&#31890;&#29699;&#21644;&#26368;&#23567;&#29983;&#25104;&#26641;&#30340;&#32858;&#31867;&#31639;&#27861;&#65292;&#36890;&#36807;&#23454;&#29616;&#22522;&#20110;&#8220;&#22823;&#35268;&#27169;&#20248;&#20808;&#32423;&#8221;&#30340;&#32858;&#31867;&#26041;&#27861;&#65292;&#21487;&#20197;&#22823;&#22823;&#36991;&#20813;&#24322;&#24120;&#20540;&#30340;&#24433;&#21709;&#65292;&#24182;&#21152;&#36895;&#20102;MST&#30340;&#26500;&#24314;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2303.01082</link><description>&lt;p&gt;
&#22522;&#20110;&#39063;&#31890;&#29699;&#35745;&#31639;&#30340;&#39640;&#25928;&#26368;&#23567;&#29983;&#25104;&#26641;&#32858;&#31867;&#31639;&#27861;GBMST
&lt;/p&gt;
&lt;p&gt;
GBMST: An Efficient Minimum Spanning Tree Clustering Based on Granular-Ball Computing. (arXiv:2303.01082v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.01082
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#31890;&#24230;&#39063;&#31890;&#29699;&#21644;&#26368;&#23567;&#29983;&#25104;&#26641;&#30340;&#32858;&#31867;&#31639;&#27861;&#65292;&#36890;&#36807;&#23454;&#29616;&#22522;&#20110;&#8220;&#22823;&#35268;&#27169;&#20248;&#20808;&#32423;&#8221;&#30340;&#32858;&#31867;&#26041;&#27861;&#65292;&#21487;&#20197;&#22823;&#22823;&#36991;&#20813;&#24322;&#24120;&#20540;&#30340;&#24433;&#21709;&#65292;&#24182;&#21152;&#36895;&#20102;MST&#30340;&#26500;&#24314;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#32858;&#31867;&#26041;&#27861;&#37117;&#22522;&#20110;&#21333;&#19968;&#31890;&#24230;&#30340;&#20449;&#24687;&#65292;&#22914;&#27599;&#20010;&#25968;&#25454;&#30340;&#36317;&#31163;&#21644;&#23494;&#24230;&#12290;&#36825;&#31181;&#26368;&#32454;&#31890;&#24230;&#30340;&#26041;&#27861;&#36890;&#24120;&#25928;&#29575;&#20302;&#19979;&#19988;&#23481;&#26131;&#21463;&#21040;&#22122;&#22768;&#30340;&#24433;&#21709;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32858;&#31867;&#31639;&#27861;&#65292;&#23427;&#32467;&#21512;&#20102;&#22810;&#31890;&#24230;&#39063;&#31890;&#29699;&#21644;&#26368;&#23567;&#29983;&#25104;&#26641;(MST)&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#31895;&#31890;&#24230;&#39063;&#31890;&#29699;&#65292;&#28982;&#21518;&#20351;&#29992;&#39063;&#31890;&#29699;&#21644;MST&#23454;&#29616;&#22522;&#20110;&#8220;&#22823;&#35268;&#27169;&#20248;&#20808;&#32423;&#8221;&#30340;&#32858;&#31867;&#26041;&#27861;&#65292;&#21487;&#20197;&#22823;&#22823;&#36991;&#20813;&#24322;&#24120;&#20540;&#30340;&#24433;&#21709;&#65292;&#24182;&#21152;&#24555;&#20102;MST&#30340;&#26500;&#24314;&#36807;&#31243;&#12290;&#22312;&#20960;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#35813;&#31639;&#27861;&#30340;&#25928;&#26524;&#12290;&#25152;&#26377;&#20195;&#30721;&#24050;&#22312;https://github.com/xjnine/GBMST&#21457;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most of the existing clustering methods are based on a single granularity of information, such as the distance and density of each data. This most fine-grained based approach is usually inefficient and susceptible to noise. Therefore, we propose a clustering algorithm that combines multi-granularity Granular-Ball and minimum spanning tree (MST). We construct coarsegrained granular-balls, and then use granular-balls and MST to implement the clustering method based on "large-scale priority", which can greatly avoid the influence of outliers and accelerate the construction process of MST. Experimental results on several data sets demonstrate the power of the algorithm. All codes have been released at https://github.com/xjnine/GBMST.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;ChatGPT&#30340;&#40065;&#26834;&#24615;&#65292;&#21457;&#29616;&#20854;&#22312;&#23545;&#25239;&#24615;&#21644;&#36229;&#20986;&#20998;&#24067;&#20219;&#21153;&#19978;&#26377;&#19968;&#33268;&#30340;&#20248;&#21183;&#65292;&#20294;&#32477;&#23545;&#34920;&#29616;&#20173;&#26377;&#25552;&#39640;&#31354;&#38388;&#65292;&#40065;&#26834;&#24615;&#20173;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2302.12095</link><description>&lt;p&gt;
&#35770;ChatGPT&#30340;&#40065;&#26834;&#24615;&#65306;&#23545;&#25239;&#24615;&#21644;&#36229;&#20986;&#20998;&#24067;&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
On the Robustness of ChatGPT: An Adversarial and Out-of-distribution Perspective. (arXiv:2302.12095v4 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.12095
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;ChatGPT&#30340;&#40065;&#26834;&#24615;&#65292;&#21457;&#29616;&#20854;&#22312;&#23545;&#25239;&#24615;&#21644;&#36229;&#20986;&#20998;&#24067;&#20219;&#21153;&#19978;&#26377;&#19968;&#33268;&#30340;&#20248;&#21183;&#65292;&#20294;&#32477;&#23545;&#34920;&#29616;&#20173;&#26377;&#25552;&#39640;&#31354;&#38388;&#65292;&#40065;&#26834;&#24615;&#20173;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#26159;OpenAI&#26368;&#36817;&#21457;&#24067;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#26381;&#21153;&#65292;&#24182;&#22312;&#36807;&#21435;&#20960;&#20010;&#26376;&#20013;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#34429;&#28982;&#24050;&#23545;ChatGPT&#30340;&#21508;&#20010;&#26041;&#38754;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#20294;&#20854;&#40065;&#26834;&#24615;&#65292;&#21363;&#23545;&#20110;&#26410;&#39044;&#26399;&#36755;&#20837;&#30340;&#34920;&#29616;&#65292;&#20173;&#19981;&#28165;&#26970;&#12290;&#40065;&#26834;&#24615;&#22312;&#36127;&#36131;&#20219;&#30340;AI&#20013;&#29305;&#21035;&#21463;&#20851;&#27880;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#31243;&#24207;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20174;&#23545;&#25239;&#24615;&#21644;&#36229;&#20986;&#20998;&#24067;&#65288;OOD&#65289;&#30340;&#35282;&#24230;&#23545;ChatGPT&#30340;&#40065;&#26834;&#24615;&#36827;&#34892;&#20102;&#24443;&#24213;&#35780;&#20272;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;AdvGLUE&#21644;ANLI&#22522;&#20934;&#26469;&#35780;&#20272;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#65292;&#37319;&#29992;Flipkart&#35780;&#35770;&#21644;DDXPlus&#21307;&#23398;&#35786;&#26029;&#25968;&#25454;&#38598;&#36827;&#34892;OOD&#35780;&#20272;&#12290;&#25105;&#20204;&#36873;&#25321;&#20102;&#20960;&#20010;&#27969;&#34892;&#30340;&#22522;&#30784;&#27169;&#22411;&#20316;&#20026;&#22522;&#20934;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;ChatGPT&#22312;&#22823;&#22810;&#25968;&#23545;&#25239;&#24615;&#21644;OOD&#20998;&#31867;&#21644;&#32763;&#35793;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#19968;&#33268;&#30340;&#20248;&#21183;&#12290;&#20294;&#26159;&#65292;&#32477;&#23545;&#30340;&#34920;&#29616;&#36828;&#38750;&#23436;&#32654;&#65292;&#36825;&#34920;&#26126;&#23545;&#25239;&#24615;&#21644;OOD&#40065;&#26834;&#24615;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#23041;&#32961;&#12290;
&lt;/p&gt;
&lt;p&gt;
ChatGPT is a recent chatbot service released by OpenAI and is receiving increasing attention over the past few months. While evaluations of various aspects of ChatGPT have been done, its robustness, i.e., the performance to unexpected inputs, is still unclear to the public. Robustness is of particular concern in responsible AI, especially for safety-critical applications. In this paper, we conduct a thorough evaluation of the robustness of ChatGPT from the adversarial and out-of-distribution (OOD) perspective. To do so, we employ the AdvGLUE and ANLI benchmarks to assess adversarial robustness and the Flipkart review and DDXPlus medical diagnosis datasets for OOD evaluation. We select several popular foundation models as baselines. Results show that ChatGPT shows consistent advantages on most adversarial and OOD classification and translation tasks. However, the absolute performance is far from perfection, which suggests that adversarial and OOD robustness remains a significant threat 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Jaccard&#24230;&#37327;&#25439;&#22833;&#65288;JMLs&#65289;&#26469;&#20248;&#21270;Jaccard&#25351;&#25968;&#65292;&#35813;&#25439;&#22833;&#22312;&#36719;&#26631;&#31614;&#19979;&#20173;&#28982;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2302.05666</link><description>&lt;p&gt;
Jaccard&#24230;&#37327;&#25439;&#22833;&#65306;&#20351;&#29992;&#36719;&#26631;&#31614;&#20248;&#21270;Jaccard&#25351;&#25968;
&lt;/p&gt;
&lt;p&gt;
Jaccard Metric Losses: Optimizing the Jaccard Index with Soft Labels. (arXiv:2302.05666v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.05666
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Jaccard&#24230;&#37327;&#25439;&#22833;&#65288;JMLs&#65289;&#26469;&#20248;&#21270;Jaccard&#25351;&#25968;&#65292;&#35813;&#25439;&#22833;&#22312;&#36719;&#26631;&#31614;&#19979;&#20173;&#28982;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
IoU&#25439;&#22833;&#26159;&#30452;&#25509;&#20248;&#21270;Jaccard&#25351;&#25968;&#30340;&#26367;&#20195;&#21697;&#12290;&#22312;&#35821;&#20041;&#20998;&#21106;&#20013;&#65292;&#23558;IoU&#25439;&#22833;&#20316;&#20026;&#25439;&#22833;&#20989;&#25968;&#30340;&#19968;&#37096;&#20998;&#65292;&#19982;&#20165;&#20248;&#21270;&#20687;&#32032;&#25439;&#22833;&#65288;&#22914;&#20132;&#21449;&#29109;&#25439;&#22833;&#65289;&#30456;&#27604;&#65292;&#23545;&#20110;Jaccard&#25351;&#25968;&#27979;&#37327;&#34920;&#29616;&#26356;&#22909;&#12290;&#26368;&#26174;&#30528;&#30340;IoU&#25439;&#22833;&#26159;&#36719;Jaccard&#25439;&#22833;&#21644;Lovasz-Softmax&#25439;&#22833;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25439;&#22833;&#19982;&#26426;&#22120;&#23398;&#20064;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#36719;&#26631;&#31614;&#19981;&#20860;&#23481;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Jaccard&#24230;&#37327;&#25439;&#22833;&#65288;JMLs&#65289;&#65292;&#23427;&#20204;&#22312;&#26631;&#20934;&#35774;&#32622;&#19979;&#19982;&#36719;&#26631;&#31614;&#20860;&#23481;&#65292;&#19982;&#36719;Jaccard&#25439;&#22833;&#30456;&#21516;&#12290;&#20351;&#29992;JMLs&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20004;&#31181;&#26368;&#27969;&#34892;&#30340;&#36719;&#26631;&#31614;&#29992;&#20363;&#65306;&#26631;&#31614;&#24179;&#28369;&#21644;&#30693;&#35782;&#33976;&#39311;&#12290;&#22312;&#19977;&#20010;&#35821;&#20041;&#20998;&#21106;&#25968;&#25454;&#38598;&#65288;Cityscapes&#12289;PASCAL VOC&#21644;DeepGlobe Land&#65289;&#19978;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#19982;&#20132;&#21449;&#29109;&#25439;&#22833;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#31616;&#21333;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#65292;&#24182;&#19988;&#22312;DeepGlobe Land&#25968;&#25454;&#38598;&#19978;&#36229;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
IoU losses are surrogates that directly optimize the Jaccard index. In semantic segmentation, leveraging IoU losses as part of the loss function is shown to perform better with respect to the Jaccard index measure than optimizing pixel-wise losses such as the cross-entropy loss alone. The most notable IoU losses are the soft Jaccard loss and the Lovasz-Softmax loss. However, these losses are incompatible with soft labels which are ubiquitous in machine learning. In this paper, we propose Jaccard metric losses (JMLs), which are identical to the soft Jaccard loss in a standard setting with hard labels, but are compatible with soft labels. With JMLs, we study two of the most popular use cases of soft labels: label smoothing and knowledge distillation. With a variety of architectures, our experiments show significant improvements over the cross-entropy loss on three semantic segmentation datasets (Cityscapes, PASCAL VOC and DeepGlobe Land), and our simple approach outperforms state-of-the-
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#31070;&#32463;&#36890;&#36947;&#25193;&#23637;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#33258;&#36866;&#24212;&#22810;&#20010;&#27969;&#34892;&#32593;&#32476;&#65292;&#36890;&#36807;&#36873;&#25321;&#24615;&#25193;&#23637;&#36890;&#36947;&#21644;&#28385;&#36275;&#30828;&#20214;&#32422;&#26463;&#65292;&#23454;&#29616;&#20102;&#36229;&#20302;&#22343;&#21248;&#31934;&#24230;&#37327;&#21270;&#23545;&#25512;&#29702;&#20934;&#30830;&#24615;&#30340;&#20943;&#36731;&#21644;&#25552;&#21319;&#65292;&#20854;&#20013;2bit ResNet50&#20934;&#30830;&#29575;&#36798;&#21040;&#20102;&#30446;&#21069;&#26368;&#20339;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2212.10878</link><description>&lt;p&gt;
&#36229;&#20302;&#22343;&#21248;&#31934;&#24230;&#37327;&#21270;&#30340;&#33258;&#21160;&#32593;&#32476;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Automatic Network Adaptation for Ultra-Low Uniform-Precision Quantization. (arXiv:2212.10878v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10878
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#31070;&#32463;&#36890;&#36947;&#25193;&#23637;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#33258;&#36866;&#24212;&#22810;&#20010;&#27969;&#34892;&#32593;&#32476;&#65292;&#36890;&#36807;&#36873;&#25321;&#24615;&#25193;&#23637;&#36890;&#36947;&#21644;&#28385;&#36275;&#30828;&#20214;&#32422;&#26463;&#65292;&#23454;&#29616;&#20102;&#36229;&#20302;&#22343;&#21248;&#31934;&#24230;&#37327;&#21270;&#23545;&#25512;&#29702;&#20934;&#30830;&#24615;&#30340;&#20943;&#36731;&#21644;&#25552;&#21319;&#65292;&#20854;&#20013;2bit ResNet50&#20934;&#30830;&#29575;&#36798;&#21040;&#20102;&#30446;&#21069;&#26368;&#20339;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22343;&#21248;&#31934;&#24230;&#31070;&#32463;&#32593;&#32476;&#37327;&#21270;&#22240;&#20854;&#31616;&#21270;&#20102;&#39640;&#35745;&#31639;&#24615;&#33021;&#30340;&#32039;&#23494;&#22635;&#20805;&#31639;&#26415;&#21333;&#20803;&#32780;&#21464;&#24471;&#27969;&#34892;&#12290;&#28982;&#32780;&#65292;&#23427;&#24573;&#30053;&#20102;&#23618;&#38388;&#23545;&#37327;&#21270;&#35823;&#24046;&#30340;&#24322;&#26500;&#25935;&#24863;&#24615;&#65292;&#23548;&#33268;&#25512;&#29702;&#20934;&#30830;&#24615;&#19981;&#20339;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#31070;&#32463;&#36890;&#36947;&#25193;&#23637;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#26041;&#27861;&#65292;&#35843;&#25972;&#32593;&#32476;&#32467;&#26500;&#20197;&#20943;&#36731;&#36229;&#20302;&#22343;&#21248;&#31934;&#24230;&#37327;&#21270;&#23545;&#25512;&#29702;&#20934;&#30830;&#24615;&#30340;&#24433;&#21709;&#12290;&#35813;&#26041;&#27861;&#20248;&#21270;&#20102;&#21508;&#23618;&#30340;&#36890;&#36947;&#25193;&#23637;&#65292;&#21516;&#26102;&#28385;&#36275;&#30828;&#20214;&#32422;&#26463;&#65288;&#20363;&#22914;FLOPs&#65292;PARAMs&#65289;&#12290;&#22522;&#20110;&#28145;&#20837;&#30340;&#20998;&#26512;&#21644;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#21487;&#20197;&#35843;&#25972;&#22810;&#20010;&#27969;&#34892;&#32593;&#32476;&#65292;&#20197;&#22312;CIFAR10&#21644;ImageNet&#19978;&#23454;&#29616;&#26356;&#22909;&#30340;2&#20301;&#37327;&#21270;&#20934;&#30830;&#24615;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#20855;&#26377;&#26356;&#23567;&#30340;FLOPs&#21644;&#21442;&#25968;&#22823;&#23567;&#30340;2bit ResNet50&#26368;&#20339;&#30340;Top-1/Top-5&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Uniform-precision neural network quantization has gained popularity since it simplifies densely packed arithmetic unit for high computing capability. However, it ignores heterogeneous sensitivity to the impact of quantization errors across the layers, resulting in sub-optimal inference accuracy. This work proposes a novel neural architecture search called neural channel expansion that adjusts the network structure to alleviate accuracy degradation from ultra-low uniform-precision quantization. The proposed method selectively expands channels for the quantization sensitive layers while satisfying hardware constraints (e.g., FLOPs, PARAMs). Based on in-depth analysis and experiments, we demonstrate that the proposed method can adapt several popular networks channels to achieve superior 2-bit quantization accuracy on CIFAR10 and ImageNet. In particular, we achieve the best-to-date Top-1/Top-5 accuracy for 2-bit ResNet50 with smaller FLOPs and the parameter size.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;CLIP&#30340;&#32452;&#21512;&#24615;&#33021;&#21147;&#20197;&#21450;&#20197;&#32467;&#26500;&#25935;&#24863;&#30340;&#26041;&#24335;&#25414;&#32465;&#21464;&#37327;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#20854;&#33021;&#22815;&#22312;&#21333;&#19968;&#23545;&#35937;&#30340;&#24773;&#20917;&#19979;&#32452;&#21512;&#27010;&#24565;&#65292;&#20294;&#22312;&#38656;&#35201;&#27010;&#24565;&#25414;&#32465;&#30340;&#24773;&#20917;&#19979;&#24615;&#33021;&#26174;&#33879;&#19979;&#38477;&#12290;</title><link>http://arxiv.org/abs/2212.10537</link><description>&lt;p&gt;
CLIP&#26159;&#21542;&#25414;&#32465;&#27010;&#24565;&#65311;&#25506;&#32034;&#22823;&#22411;&#22270;&#20687;&#27169;&#22411;&#30340;&#32452;&#21512;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Does CLIP Bind Concepts? Probing Compositionality in Large Image Models. (arXiv:2212.10537v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10537
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;CLIP&#30340;&#32452;&#21512;&#24615;&#33021;&#21147;&#20197;&#21450;&#20197;&#32467;&#26500;&#25935;&#24863;&#30340;&#26041;&#24335;&#25414;&#32465;&#21464;&#37327;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#20854;&#33021;&#22815;&#22312;&#21333;&#19968;&#23545;&#35937;&#30340;&#24773;&#20917;&#19979;&#32452;&#21512;&#27010;&#24565;&#65292;&#20294;&#22312;&#38656;&#35201;&#27010;&#24565;&#25414;&#32465;&#30340;&#24773;&#20917;&#19979;&#24615;&#33021;&#26174;&#33879;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#32467;&#21512;&#25991;&#26412;&#21644;&#22270;&#20687;&#30340;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#32534;&#30721;&#20102;&#23427;&#20204;&#25805;&#20316;&#30340;&#27010;&#24565;&#30340;&#32452;&#25104;&#24615;&#34920;&#31034;&#65292;&#22914;&#36890;&#36807;&#23545;&#8220;&#32418;&#33394;&#31435;&#26041;&#20307;&#8221;&#36827;&#34892;&#25512;&#29702;&#20197;&#27491;&#30830;&#35782;&#21035;&#8220;&#32418;&#33394;&#8221;&#21644;&#8220;&#31435;&#26041;&#20307;&#8221;&#36825;&#20123;&#25104;&#20998;&#65292;&#36825;&#20173;&#28982;&#26159;&#19968;&#20010;&#24320;&#25918;&#24615;&#38382;&#39064;&#12290;&#26412;&#25991;&#20851;&#27880;&#19968;&#20010;&#22823;&#22411;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#22411;&#65288;CLIP&#65289;&#32534;&#30721;&#32452;&#21512;&#27010;&#24565;&#30340;&#33021;&#21147;&#20197;&#21450;&#20197;&#32467;&#26500;&#25935;&#24863;&#30340;&#26041;&#24335;&#25414;&#32465;&#21464;&#37327;&#30340;&#33021;&#21147;&#65288;&#20363;&#22914;&#21306;&#20998;&#8220;&#31435;&#26041;&#20307;&#22312;&#29699;&#20307;&#21518;&#38754;&#8221;&#21644;&#8220;&#29699;&#20307;&#22312;&#31435;&#26041;&#20307;&#21518;&#38754;&#8221;&#65289;&#12290;&#20026;&#20102;&#26816;&#26597;CLIP&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#35768;&#22810;&#26469;&#33258;&#32452;&#21512;&#20998;&#24067;&#35821;&#20041;&#27169;&#22411;&#65288;CDSMs&#65289;&#30340;&#26550;&#26500;&#65292;&#36825;&#26159;&#19968;&#31181;&#35797;&#22270;&#22312;&#23884;&#20837;&#31354;&#38388;&#20013;&#23454;&#29616;&#20256;&#32479;&#32452;&#21512;&#35821;&#35328;&#32467;&#26500;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#25105;&#20204;&#21457;&#29616;CLIP&#33021;&#22815;&#22312;&#21333;&#19968;&#23545;&#35937;&#30340;&#24773;&#20917;&#19979;&#32452;&#21512;&#27010;&#24565;&#65292;&#20294;&#22312;&#38656;&#35201;&#27010;&#24565;&#25414;&#32465;&#30340;&#24773;&#20917;&#19979;&#24615;&#33021;&#26174;&#33879;&#19979;&#38477;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#20984;&#26174;&#20102;&#35780;&#20272;&#22823;&#22411;&#27169;&#22411;&#32452;&#21512;&#24615;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#20026;&#26410;&#26469;&#30740;&#31350;&#25552;&#20986;&#20102;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large-scale neural network models combining text and images have made incredible progress in recent years. However, it remains an open question to what extent such models encode compositional representations of the concepts over which they operate, such as correctly identifying ''red cube'' by reasoning over the constituents ''red'' and ''cube''. In this work, we focus on the ability of a large pretrained vision and language model (CLIP) to encode compositional concepts and to bind variables in a structure-sensitive way (e.g., differentiating ''cube behind sphere'' from ''sphere behind cube''). In order to inspect the performance of CLIP, we compare several architectures from research on compositional distributional semantics models (CDSMs), a line of research that attempts to implement traditional compositional linguistic structures within embedding spaces. We find that CLIP can compose concepts in a single-object setting, but in situations where concept binding is needed, performance
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#25105;&#39564;&#35777;&#26041;&#27861;&#65292;&#20351;&#29992;CoT&#30340;&#32467;&#35770;&#26469;&#26500;&#24314;&#26032;&#26679;&#26412;&#24182;&#35201;&#27714;LLM&#37325;&#26032;&#39044;&#27979;&#21407;&#22987;&#26465;&#20214;&#65292;&#20197;&#25552;&#39640;&#25512;&#29702;&#20934;&#30830;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;LLMs&#21487;&#20197;&#23545;&#20854;&#33258;&#24049;&#30340;&#32467;&#35770;&#36827;&#34892;&#33258;&#25105;&#39564;&#35777;&#24182;&#23454;&#29616;&#31454;&#20105;&#24615;&#30340;&#25512;&#29702;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2212.09561</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#24102;&#26377;&#33258;&#25105;&#39564;&#35777;&#30340;&#25512;&#29702;&#22120;
&lt;/p&gt;
&lt;p&gt;
Large Language Models are reasoners with Self-Verification. (arXiv:2212.09561v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09561
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#25105;&#39564;&#35777;&#26041;&#27861;&#65292;&#20351;&#29992;CoT&#30340;&#32467;&#35770;&#26469;&#26500;&#24314;&#26032;&#26679;&#26412;&#24182;&#35201;&#27714;LLM&#37325;&#26032;&#39044;&#27979;&#21407;&#22987;&#26465;&#20214;&#65292;&#20197;&#25552;&#39640;&#25512;&#29702;&#20934;&#30830;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;LLMs&#21487;&#20197;&#23545;&#20854;&#33258;&#24049;&#30340;&#32467;&#35770;&#36827;&#34892;&#33258;&#25105;&#39564;&#35777;&#24182;&#23454;&#29616;&#31454;&#20105;&#24615;&#30340;&#25512;&#29702;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36890;&#36807;&#24605;&#32500;&#38142;&#65288;CoT&#65289;&#36827;&#34892;&#22797;&#26434;&#25512;&#29702;&#26102;&#65292;&#23427;&#38750;&#24120;&#25935;&#24863;&#20110;&#20010;&#21035;&#38169;&#35823;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24517;&#39035;&#35757;&#32451;&#39564;&#35777;&#22120;&#12290;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#31216;&#20026;&#33258;&#25105;&#39564;&#35777;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;CoT&#30340;&#32467;&#35770;&#20316;&#20026;&#26465;&#20214;&#26469;&#26500;&#24314;&#19968;&#20010;&#26032;&#26679;&#26412;&#65292;&#24182;&#35201;&#27714;LLM&#37325;&#26032;&#39044;&#27979;&#34987;&#25513;&#30422;&#30340;&#21407;&#22987;&#26465;&#20214;&#12290;&#25105;&#20204;&#22522;&#20110;&#20934;&#30830;&#24615;&#35745;&#31639;&#21487;&#35299;&#37322;&#30340;&#39564;&#35777;&#20998;&#25968;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#20351;&#29992;&#23569;&#37327;&#26679;&#26412;&#23398;&#20064;&#26102;&#25552;&#39640;&#22810;&#20010;&#31639;&#26415;&#21644;&#36923;&#36753;&#25512;&#29702;&#25968;&#25454;&#38598;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#24050;&#32463;&#35777;&#26126;LLM&#21487;&#20197;&#23545;&#20854;&#33258;&#24049;&#30340;&#32467;&#35770;&#36827;&#34892;&#21487;&#35299;&#37322;&#30340;&#33258;&#25105;&#39564;&#35777;&#24182;&#23454;&#29616;&#31454;&#20105;&#24615;&#30340;&#25512;&#29702;&#24615;&#33021;&#12290;&#20840;&#38754;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#24110;&#21161;&#22810;&#31181;&#24102;&#26377;&#33258;&#25105;&#39564;&#35777;&#21151;&#33021;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36991;&#20813;&#28151;&#28102;&#12290;
&lt;/p&gt;
&lt;p&gt;
When a large language model (LLM) performs complex reasoning by chain of thought (CoT), it can be highly sensitive to individual mistakes. We have had to train verifiers to address this issue. As we all know, after human inferring a conclusion, they often check it by re-verifying it, which can avoid some mistakes. We propose a new method called self-verification that uses the conclusion of the CoT as a condition to build a new sample and asks the LLM to re-predict the original conditions which be masked. We calculate an explainable verification score based on the accuracy. This method can improve the accuracy of multiple arithmetics and logical reasoning datasets when using few-shot learning. we have demonstrated that LLMs can conduct explainable self-verification of their own conclusions and achieve competitive reasoning performance. Extensive experimentals have demonstrated that our method can help multiple large language models with self-verification can avoid interference from inco
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#21367;&#31215;&#32593;&#32476;&#21644;&#24179;&#34913;&#29702;&#35770;&#30340;&#21152;&#23494;&#36135;&#24065;&#20449;&#20219;&#32593;&#32476;&#27450;&#35784;&#26816;&#27979;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;&#27169;&#24335;&#30697;&#38453;&#25429;&#25417;&#23616;&#37096;&#25299;&#25169;&#20449;&#24687;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#30495;&#23454;&#25968;&#25454;&#21644;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#22343;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2211.13123</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#21367;&#31215;&#32593;&#32476;&#30340;&#23494;&#30721;&#36135;&#24065;&#20449;&#20219;&#32593;&#32476;&#20013;&#30340;&#27450;&#35784;&#26816;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Motif-aware temporal GCN for fraud detection in signed cryptocurrency trust networks. (arXiv:2211.13123v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.13123
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#21367;&#31215;&#32593;&#32476;&#21644;&#24179;&#34913;&#29702;&#35770;&#30340;&#21152;&#23494;&#36135;&#24065;&#20449;&#20219;&#32593;&#32476;&#27450;&#35784;&#26816;&#27979;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;&#27169;&#24335;&#30697;&#38453;&#25429;&#25417;&#23616;&#37096;&#25299;&#25169;&#20449;&#24687;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#30495;&#23454;&#25968;&#25454;&#21644;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#22343;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#21367;&#31215;&#32593;&#32476;&#26159;&#19968;&#31181;&#22788;&#29702;&#21487;&#34920;&#31034;&#20026;&#22270;&#30340;&#25968;&#25454;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#12290;&#30001;&#20110;&#37329;&#34701;&#20132;&#26131;&#21487;&#20197;&#33258;&#28982;&#22320;&#26500;&#36896;&#25104;&#22270;&#24418;&#65292;&#22240;&#27492;GCN&#22312;&#37329;&#34701;&#34892;&#19994;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#24212;&#29992;&#65292;&#29305;&#21035;&#26159;&#22312;&#37329;&#34701;&#27450;&#35784;&#26816;&#27979;&#26041;&#38754;&#12290;&#26412;&#25991;&#37325;&#28857;&#30740;&#31350;&#21152;&#23494;&#36135;&#24065;&#20449;&#20219;&#32593;&#32476;&#20013;&#30340;&#27450;&#35784;&#26816;&#27979;&#12290;&#22312;&#29616;&#26377;&#25991;&#29486;&#20013;&#65292;&#22823;&#37096;&#20998;&#24037;&#20316;&#37117;&#38598;&#20013;&#22312;&#38745;&#24577;&#22270;&#19978;&#12290;&#32780;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#21152;&#23494;&#36135;&#24065;&#32593;&#32476;&#30340;&#28436;&#21270;&#29305;&#24615;&#65292;&#24182;&#21033;&#29992;&#26412;&#22320;&#32467;&#26500;&#21644;&#24179;&#34913;&#29702;&#35770;&#26469;&#25351;&#23548;&#35757;&#32451;&#36807;&#31243;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35745;&#31639;&#21160;&#24577;&#32593;&#32476;&#20013;&#30340;&#27169;&#24335;&#30697;&#38453;&#26469;&#25429;&#25417;&#23616;&#37096;&#25299;&#25169;&#20449;&#24687;&#65292;&#28982;&#21518;&#22312;GCN&#32858;&#21512;&#36807;&#31243;&#20013;&#20351;&#29992;&#23427;&#20204;&#12290;&#27599;&#20010;&#24555;&#29031;&#29983;&#25104;&#30340;&#23884;&#20837;&#26159;&#26102;&#38388;&#31383;&#21475;&#20869;&#23884;&#20837;&#30340;&#21152;&#26435;&#24179;&#22343;&#20540;&#65292;&#20854;&#20013;&#26435;&#37325;&#26159;&#21487;&#23398;&#20064;&#30340;&#21442;&#25968;&#12290;&#30001;&#20110;&#20449;&#20219;&#32593;&#32476;&#22312;&#27599;&#20010;&#36793;&#32536;&#19978;&#37117;&#26377;&#31614;&#21517;&#65292;&#22240;&#27492;&#20351;&#29992;&#24179;&#34913;&#29702;&#35770;&#26469;&#25351;&#23548;&#35757;&#32451;&#36807;&#31243;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#22522;&#20110;&#27169;&#24335;&#24863;&#30693;&#30340;&#26102;&#24577;GCN&#22312;&#30495;&#23454;&#25968;&#25454;&#21644;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#22343;&#27604;&#29616;&#26377;&#30340;&#26041;&#27861;&#20855;&#26377;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph convolutional networks (GCNs) is a class of artificial neural networks for processing data that can be represented as graphs. Since financial transactions can naturally be constructed as graphs, GCNs are widely applied in the financial industry, especially for financial fraud detection. In this paper, we focus on fraud detection on cryptocurrency truct networks. In the literature, most works focus on static networks. Whereas in this study, we consider the evolving nature of cryptocurrency networks, and use local structural as well as the balance theory to guide the training process. More specifically, we compute motif matrices to capture the local topological information, then use them in the GCN aggregation process. The generated embedding at each snapshot is a weighted average of embeddings within a time window, where the weights are learnable parameters. Since the trust networks is signed on each edge, balance theory is used to guide the training process. Experimental results 
&lt;/p&gt;</description></item><item><title>CRAFT&#36890;&#36807;&#29983;&#25104;&#22522;&#20110;&#27010;&#24565;&#30340;&#35299;&#37322;&#26469;&#22635;&#34917;&#24402;&#22240;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;&#23454;&#29616;&#20102;&#23545;&#22270;&#20687;&#8220;&#26159;&#20160;&#20040;&#8221;&#21644;&#8220;&#22312;&#21738;&#37324;&#8221;&#30340;&#21516;&#26102;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2211.10154</link><description>&lt;p&gt;
CRAFT: &#27010;&#24565;&#36882;&#24402;&#28608;&#27963;&#20998;&#35299;&#29992;&#20110;&#21487;&#35299;&#37322;&#24615;
&lt;/p&gt;
&lt;p&gt;
CRAFT: Concept Recursive Activation FacTorization for Explainability. (arXiv:2211.10154v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.10154
&lt;/p&gt;
&lt;p&gt;
CRAFT&#36890;&#36807;&#29983;&#25104;&#22522;&#20110;&#27010;&#24565;&#30340;&#35299;&#37322;&#26469;&#22635;&#34917;&#24402;&#22240;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;&#23454;&#29616;&#20102;&#23545;&#22270;&#20687;&#8220;&#26159;&#20160;&#20040;&#8221;&#21644;&#8220;&#22312;&#21738;&#37324;&#8221;&#30340;&#21516;&#26102;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24402;&#22240;&#26041;&#27861;&#36890;&#36807;&#28909;&#22270;&#35782;&#21035;&#24433;&#21709;&#27169;&#22411;&#20915;&#31574;&#30340;&#26368;&#20855;&#24433;&#21709;&#21147;&#21306;&#22495;&#65292;&#24050;&#25104;&#20026;&#19968;&#31181;&#26222;&#36941;&#30340;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#24050;&#32463;&#25581;&#31034;&#20102;&#36825;&#20123;&#26041;&#27861;&#26377;&#38480;&#30340;&#23454;&#38469;&#20215;&#20540;&#65292;&#37096;&#20998;&#24402;&#22240;&#20110;&#23427;&#20204;&#23545;&#22270;&#20687;&#26368;&#26174;&#33879;&#21306;&#22495;&#30340;&#29421;&#31364;&#20851;&#27880;,&#25581;&#31034;&#20102;&#27169;&#22411;&#8220;&#27880;&#35270;&#8221;&#21738;&#37324;&#65292;&#20294;&#26410;&#33021;&#38416;&#26126;&#27169;&#22411;&#22312;&#36825;&#20123;&#21306;&#22495;&#8220;&#30475;&#21040;&#8221;&#30340;&#20869;&#23481;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#29983;&#25104;&#22522;&#20110;&#27010;&#24565;&#30340;&#35299;&#37322;&#65292;&#23581;&#35797;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#26469;&#30830;&#23450;&#8220;&#26159;&#20160;&#20040;&#8221;&#21644;&#8220;&#22312;&#21738;&#37324;&#8221;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19977;&#20010;&#26032;&#35201;&#32032;&#26469;&#33258;&#21160;&#25552;&#21462;&#27010;&#24565;&#65306;&#65288;i&#65289;&#19968;&#31181;&#36882;&#24402;&#31574;&#30053;&#26469;&#26816;&#27979;&#21644;&#20998;&#35299;&#36328;&#23618;&#30340;&#27010;&#24565;&#65292;&#65288;ii&#65289;&#29992; Sobol &#25351;&#25968;&#26356;&#20934;&#30830;&#22320;&#20272;&#35745;&#27010;&#24565;&#37325;&#35201;&#24615;&#30340;&#26032;&#26041;&#27861;&#65292;&#21644;&#65288;iii&#65289;&#20351;&#29992;&#38544;&#24335;&#24494;&#20998;&#26469;&#35299;&#38145;&#27010;&#24565;&#24402;&#22240;&#22270;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#20154;&#31867;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#23454;&#39564;&#26469;&#35777;&#26126;&#35813;&#26041;&#27861;&#30340;&#21487;&#34892;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Attribution methods, which employ heatmaps to identify the most influential regions of an image that impact model decisions, have gained widespread popularity as a type of explainability method. However, recent research has exposed the limited practical value of these methods, attributed in part to their narrow focus on the most prominent regions of an image -- revealing "where" the model looks, but failing to elucidate "what" the model sees in those areas. In this work, we try to fill in this gap with CRAFT -- a novel approach to identify both "what" and "where" by generating concept-based explanations. We introduce 3 new ingredients to the automatic concept extraction literature: (i) a recursive strategy to detect and decompose concepts across layers, (ii) a novel method for a more faithful estimation of concept importance using Sobol indices, and (iii) the use of implicit differentiation to unlock Concept Attribution Maps.  We conduct both human and computer vision experiments to de
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21487;&#35265;&#24615;&#24341;&#23548;&#27969;&#27169;&#22359;&#30340;&#27969;&#32593;&#32476;&#27169;&#22411;VGFlow&#65292;&#21487;&#20197;&#20998;&#31163;&#20986;&#30446;&#26631;&#30340;&#21487;&#35265;&#21644;&#19981;&#21487;&#35265;&#37096;&#20998;&#20197;&#23454;&#29616;&#32441;&#29702;&#20445;&#30041;&#21644;&#39118;&#26684;&#25805;&#20316;&#65292;&#21516;&#26102;&#37319;&#29992;&#20102;&#22810;&#36335;&#24452;&#32467;&#26500;&#20197;&#22312;&#19981;&#21516;&#32423;&#21035;&#30340;&#32454;&#33410;&#19978;&#25805;&#20316;&#12290;&#22312;&#29983;&#25104;&#36924;&#30495;&#20154;&#20307;&#23039;&#21183;&#26041;&#38754;&#34920;&#29616;&#20986;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2211.08540</link><description>&lt;p&gt;
VGFlow: &#38024;&#23545;&#20154;&#20307;&#37325;&#23450;&#20301;&#30340;&#21487;&#35265;&#24615;&#24341;&#23548;&#27969;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
VGFlow: Visibility guided Flow Network for Human Reposing. (arXiv:2211.08540v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.08540
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21487;&#35265;&#24615;&#24341;&#23548;&#27969;&#27169;&#22359;&#30340;&#27969;&#32593;&#32476;&#27169;&#22411;VGFlow&#65292;&#21487;&#20197;&#20998;&#31163;&#20986;&#30446;&#26631;&#30340;&#21487;&#35265;&#21644;&#19981;&#21487;&#35265;&#37096;&#20998;&#20197;&#23454;&#29616;&#32441;&#29702;&#20445;&#30041;&#21644;&#39118;&#26684;&#25805;&#20316;&#65292;&#21516;&#26102;&#37319;&#29992;&#20102;&#22810;&#36335;&#24452;&#32467;&#26500;&#20197;&#22312;&#19981;&#21516;&#32423;&#21035;&#30340;&#32454;&#33410;&#19978;&#25805;&#20316;&#12290;&#22312;&#29983;&#25104;&#36924;&#30495;&#20154;&#20307;&#23039;&#21183;&#26041;&#38754;&#34920;&#29616;&#20986;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#20307;&#37325;&#23450;&#20301;&#30340;&#20219;&#21153;&#28041;&#21450;&#29983;&#25104;&#19968;&#20010;&#31449;&#31435;&#22312;&#20219;&#24847;&#26500;&#24819;&#23039;&#21183;&#19979;&#30340;&#30495;&#23454;&#20154;&#29289;&#22270;&#20687;&#12290;&#29983;&#25104;&#24863;&#30693;&#20934;&#30830;&#22270;&#20687;&#23384;&#22312;&#22810;&#20010;&#22256;&#38590;&#65292;&#29616;&#26377;&#26041;&#27861;&#22312;&#20445;&#30041;&#32441;&#29702;&#12289;&#32500;&#25345;&#22270;&#26696;&#19968;&#33268;&#24615;&#12289;&#32771;&#34385;&#26381;&#35013;&#36793;&#30028;&#12289;&#22788;&#29702;&#36974;&#25377;&#12289;&#35843;&#25972;&#30382;&#32932;&#29983;&#25104;&#31561;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#36825;&#20123;&#22256;&#38590;&#36824;&#22240;&#21487;&#33021;&#30340;&#20154;&#20307;&#23039;&#21183;&#26041;&#21521;&#31354;&#38388;&#24040;&#22823;&#19988;&#22810;&#21464;&#12289;&#26381;&#35013;&#29289;&#21697;&#30340;&#26412;&#36136;&#39640;&#24230;&#38750;&#21018;&#24615;&#12289;&#36523;&#20307;&#24418;&#29366;&#30340;&#22810;&#26679;&#24615;&#22823;&#22823;&#19981;&#21516;&#20110;&#20154;&#21475;&#20013;&#30340;&#22810;&#26679;&#24615;&#32780;&#36827;&#19968;&#27493;&#24694;&#21270;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20123;&#22256;&#38590;&#24182;&#21512;&#25104;&#24863;&#30693;&#20934;&#30830;&#30340;&#22270;&#20687;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;VGFlow&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#20351;&#29992;&#21487;&#35265;&#24615;&#24341;&#23548;&#27969;&#27169;&#22359;&#23558;&#27969;&#20998;&#31163;&#20026;&#30446;&#26631;&#30340;&#21487;&#35265;&#21644;&#19981;&#21487;&#35265;&#37096;&#20998;&#65292;&#20197;&#23454;&#29616;&#21516;&#26102;&#32441;&#29702;&#20445;&#30041;&#21644;&#39118;&#26684;&#25805;&#20316;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#35299;&#20915;&#19981;&#21516;&#30340;&#36523;&#20307;&#24418;&#29366;&#21644;&#36991;&#20813;&#32593;&#32476;&#20266;&#24433;&#65292;&#25105;&#20204;&#36824;&#37319;&#29992;&#20102;&#19968;&#20010;&#22810;&#36335;&#24452;&#32467;&#26500;&#65292;&#21253;&#25324;&#20840;&#23616;&#32467;&#26500;&#36335;&#24452;&#21644;&#22810;&#20010;&#23616;&#37096;&#32454;&#33410;&#36335;&#24452;&#65292;&#21487;&#20197;&#22312;&#19981;&#21516;&#32423;&#21035;&#30340;&#32454;&#33410;&#19978;&#36827;&#34892;&#25805;&#20316;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#22312;&#29983;&#25104;&#20855;&#26377;&#31934;&#32454;&#32454;&#33410;&#21644;&#20934;&#30830;&#32441;&#29702;&#30340;&#36924;&#30495;&#20154;&#20307;&#23039;&#21183;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The task of human reposing involves generating a realistic image of a person standing in an arbitrary conceivable pose. There are multiple difficulties in generating perceptually accurate images, and existing methods suffer from limitations in preserving texture, maintaining pattern coherence, respecting cloth boundaries, handling occlusions, manipulating skin generation, etc. These difficulties are further exacerbated by the fact that the possible space of pose orientation for humans is large and variable, the nature of clothing items is highly non-rigid, and the diversity in body shape differs largely among the population. To alleviate these difficulties and synthesize perceptually accurate images, we propose VGFlow. Our model uses a visibility-guided flow module to disentangle the flow into visible and invisible parts of the target for simultaneous texture preservation and style manipulation. Furthermore, to tackle distinct body shapes and avoid network artifacts, we also incorporat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23454;&#20363;&#24863;&#30693;&#22270;&#20687;&#20462;&#22797;&#27169;&#22411;ImComplete&#65292;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#24187;&#35937;&#20986;&#19982;&#29615;&#22659;&#32972;&#26223;&#30456;&#21327;&#35843;&#30340;&#35270;&#35273;&#23454;&#20363;&#65292;&#25552;&#20379;&#20102;&#22522;&#20110;&#35821;&#20041;&#21644;&#32467;&#26500;&#30340;&#20687;&#32032;&#32423;&#25351;&#23548;&#12290;</title><link>http://arxiv.org/abs/2210.12350</link><description>&lt;p&gt;
&#23454;&#20363;&#24863;&#30693;&#22270;&#20687;&#20462;&#22797;
&lt;/p&gt;
&lt;p&gt;
Instance-Aware Image Completion. (arXiv:2210.12350v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.12350
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23454;&#20363;&#24863;&#30693;&#22270;&#20687;&#20462;&#22797;&#27169;&#22411;ImComplete&#65292;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#24187;&#35937;&#20986;&#19982;&#29615;&#22659;&#32972;&#26223;&#30456;&#21327;&#35843;&#30340;&#35270;&#35273;&#23454;&#20363;&#65292;&#25552;&#20379;&#20102;&#22522;&#20110;&#35821;&#20041;&#21644;&#32467;&#26500;&#30340;&#20687;&#32032;&#32423;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#20462;&#22797;&#26159;&#19968;&#39033;&#26088;&#22312;&#22635;&#34917;&#24102;&#26377;&#32570;&#22833;&#21306;&#22495;&#30340;&#22270;&#20687;&#30340;&#20219;&#21153;&#65292;&#20351;&#23427;&#20204;&#20855;&#26377;&#21512;&#29702;&#30340;&#20869;&#23481;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22270;&#20687;&#20462;&#22797;&#26041;&#27861;&#24448;&#24448;&#36890;&#36807;&#22635;&#20805;&#21608;&#22260;&#32441;&#29702;&#26469;&#22635;&#34917;&#32570;&#22833;&#21306;&#22495;&#65292;&#32780;&#19981;&#26159;&#21435;&#24187;&#35937;&#19968;&#20010;&#19982;&#29615;&#22659;&#32972;&#26223;&#30456;&#21327;&#35843;&#30340;&#35270;&#35273;&#23454;&#20363;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#20687;&#20462;&#22797;&#27169;&#22411;&#65292;&#21517;&#20026;ImComplete&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#24187;&#35937;&#32570;&#22833;&#30340;&#23454;&#20363;&#65292;&#20174;&#32780;&#19982;&#21407;&#22987;&#32972;&#26223;&#21327;&#35843;&#12290;ImComplete&#39318;&#20808;&#37319;&#29992;&#20102;&#19968;&#20010;&#21464;&#21387;&#22120;&#26550;&#26500;&#65292;&#32771;&#34385;&#21040;&#21487;&#35265;&#23454;&#20363;&#21644;&#32570;&#22833;&#21306;&#22495;&#30340;&#20301;&#32622;&#12290;&#28982;&#21518;&#65292;ImComplete&#23436;&#25104;&#20102;&#32570;&#22833;&#21306;&#22495;&#20869;&#30340;&#35821;&#20041;&#20998;&#21106;&#25513;&#27169;&#65292;&#25552;&#20379;&#20687;&#32032;&#32423;&#30340;&#35821;&#20041;&#21644;&#32467;&#26500;&#25351;&#23548;&#12290;&#26368;&#21518;&#65292;&#22270;&#20687;&#21512;&#25104;&#22359;&#29983;&#25104;&#20102;&#36924;&#30495;&#30340;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
Image completion is a task that aims to fill in the missing region of a masked image with plausible contents. However, existing image completion methods tend to fill in the missing region with the surrounding texture instead of hallucinating a visual instance that is suitable in accordance with the context of the scene. In this work, we propose a novel image completion model, dubbed ImComplete, that hallucinates the missing instance that harmonizes well with - and thus preserves - the original context. ImComplete first adopts a transformer architecture that considers the visible instances and the location of the missing region. Then, ImComplete completes the semantic segmentation masks within the missing region, providing pixel-level semantic and structural guidance. Finally, the image synthesis blocks generate photo-realistic content. We perform a comprehensive evaluation of the results in terms of visual quality (LPIPS and FID) and contextual preservation scores (CLIPscore and object
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20840;&#38754;&#22238;&#39038;&#20102;AI&#21644;&#25968;&#23383;&#23402;&#29983;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#37325;&#28857;&#20171;&#32461;&#20102;&#25968;&#23383;&#23402;&#29983;&#32593;&#32476;&#30340;&#21508;&#31181;&#29992;&#20363;&#21644;&#24212;&#29992;&#65292;&#20197;&#21450;&#20854;&#22914;&#20309;&#24357;&#21512;&#25968;&#25454;&#39537;&#21160;&#21644;&#27169;&#22411;&#39537;&#21160;&#26041;&#27861;&#30340;&#24046;&#36317;&#12290;</title><link>http://arxiv.org/abs/2209.12423</link><description>&lt;p&gt;
AI&#19982;&#25968;&#23383;&#23402;&#29983;&#30340;&#30456;&#20114;&#20316;&#29992;&#65306;&#24357;&#21512;&#25968;&#25454;&#39537;&#21160;&#21644;&#27169;&#22411;&#39537;&#21160;&#26041;&#27861;&#30340;&#24046;&#36317;
&lt;/p&gt;
&lt;p&gt;
The Interplay of AI and Digital Twin: Bridging the Gap between Data-Driven and Model-Driven Approaches. (arXiv:2209.12423v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.12423
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20840;&#38754;&#22238;&#39038;&#20102;AI&#21644;&#25968;&#23383;&#23402;&#29983;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#37325;&#28857;&#20171;&#32461;&#20102;&#25968;&#23383;&#23402;&#29983;&#32593;&#32476;&#30340;&#21508;&#31181;&#29992;&#20363;&#21644;&#24212;&#29992;&#65292;&#20197;&#21450;&#20854;&#22914;&#20309;&#24357;&#21512;&#25968;&#25454;&#39537;&#21160;&#21644;&#27169;&#22411;&#39537;&#21160;&#26041;&#27861;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32593;&#32476;&#34394;&#25311;&#21270;&#21644;&#21407;&#29983;&#20154;&#24037;&#26234;&#33021;(AI)&#33539;&#20363;&#30340;&#21457;&#23637;&#24050;&#32463;&#26500;&#24314;&#20102;&#26410;&#26469;&#26080;&#32447;&#32593;&#32476;&#20316;&#20026;&#19968;&#20010;&#22312;&#25968;&#23383;&#24179;&#21488;&#19978;&#25805;&#20316;&#30340;&#32508;&#21512;&#23454;&#20307;&#30340;&#24895;&#26223;&#65292;&#19982;&#29289;&#29702;&#39046;&#22495;&#26234;&#33021;&#20114;&#21160;&#65292;&#20026;&#25968;&#23383;&#23402;&#29983;(DT)&#27010;&#24565;&#30340;&#34028;&#21187;&#21457;&#23637;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;DT&#32593;&#32476;&#30340;&#26368;&#26032;&#20852;&#36259;&#34987;&#26032;&#39062;&#30340;&#26080;&#32447;&#25216;&#26415;&#21644;&#29992;&#20363;&#30340;&#20986;&#29616;&#25152;&#25512;&#21160;&#65292;&#36825;&#20123;&#25216;&#26415;&#21644;&#29992;&#20363;&#21152;&#21095;&#20102;&#32593;&#32476;&#21327;&#35843;&#21644;&#36164;&#28304;&#31649;&#29702;&#30340;&#22797;&#26434;&#24230;&#12290;&#30001;AI&#39537;&#21160;&#65292;DT&#30340;&#20851;&#38190;&#21407;&#21017;&#26159;&#20026;&#29289;&#29702;&#23454;&#20307;&#21644;&#32593;&#32476;&#21160;&#21147;&#23398;&#21019;&#24314;&#34394;&#25311;&#23402;&#29983;&#20307;&#65292;&#20854;&#20013;&#34394;&#25311;&#23402;&#29983;&#20307;&#23558;&#34987;&#21033;&#29992;&#26469;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#24182;&#25552;&#20379;&#19987;&#20026;AI&#27169;&#22411;&#35757;&#32451;&#32780;&#35774;&#35745;&#30340;&#24179;&#21488;&#12290;&#23613;&#31649;&#20154;&#20204;&#26222;&#36941;&#35748;&#20026;AI&#26159;DT&#30340;&#31181;&#23376;&#65292;&#20294;&#25105;&#20204;&#39044;&#35745;DT&#21644;AI&#23558;&#20114;&#30456;&#20419;&#36827;&#65292;&#20197;&#19968;&#31181;&#20811;&#26381;&#24444;&#27492;&#38480;&#21046;&#24182;&#30456;&#20114;&#34917;&#20805;&#20248;&#21183;&#30340;&#26041;&#24335;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20840;&#38754;&#22238;&#39038;&#20102;AI&#21644;&#25968;&#23383;&#23402;&#29983;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#35752;&#35770;&#20102;DT&#32593;&#32476;&#30340;&#21508;&#31181;&#29992;&#20363;&#21644;&#24212;&#29992;&#65292;&#24182;&#24378;&#35843;&#20102;&#36825;&#20010;&#39046;&#22495;&#30340;&#25361;&#25112;&#21644;&#26426;&#36935;&#12290;
&lt;/p&gt;
&lt;p&gt;
The evolution of network virtualization and native artificial intelligence (AI) paradigms have conceptualized the vision of future wireless networks as a comprehensive entity operating in whole over a digital platform, with smart interaction with the physical domain, paving the way for the blooming of the Digital Twin (DT) concept. The recent interest in the DT networks is fueled by the emergence of novel wireless technologies and use-cases, that exacerbate the level of complexity to orchestrate the network and to manage its resources. Driven by AI, the key principle of the DT is to create a virtual twin for the physical entities and network dynamics, where the virtual twin will be leveraged to generate synthetic data and offer an on-demand platform for AI model training. Despite the common understanding that AI is the seed for DT, we anticipate that the DT and AI will be enablers for each other, in a way that overcome their limitations and complement each other benefits. In this artic
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#21367;&#31215;&#27744;&#21270;&#21464;&#25442;&#22120;&#29992;&#20110;Deepfake&#26816;&#27979;&#65292;&#33021;&#22815;&#20840;&#38754;&#22320;&#34701;&#21512;&#23616;&#37096;&#21644;&#20840;&#23616;&#30340;&#22270;&#20687;&#29305;&#24449;&#65292;&#24182;&#37319;&#29992;&#27880;&#24847;&#21147;&#25513;&#27169;&#20002;&#22833;&#26041;&#27861;&#25552;&#39640;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#65292;&#33719;&#24471;&#20102;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#26368;&#20248;&#26816;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2209.05299</link><description>&lt;p&gt;
&#28145;&#24230;&#21367;&#31215;&#27744;&#21270;&#21464;&#25442;&#22120;&#29992;&#20110;Deepfake&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Deep Convolutional Pooling Transformer for Deepfake Detection. (arXiv:2209.05299v4 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.05299
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#21367;&#31215;&#27744;&#21270;&#21464;&#25442;&#22120;&#29992;&#20110;Deepfake&#26816;&#27979;&#65292;&#33021;&#22815;&#20840;&#38754;&#22320;&#34701;&#21512;&#23616;&#37096;&#21644;&#20840;&#23616;&#30340;&#22270;&#20687;&#29305;&#24449;&#65292;&#24182;&#37319;&#29992;&#27880;&#24847;&#21147;&#25513;&#27169;&#20002;&#22833;&#26041;&#27861;&#25552;&#39640;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#65292;&#33719;&#24471;&#20102;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#26368;&#20248;&#26816;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#30001;&#20110;&#31038;&#20132;&#23186;&#20307;&#25968;&#23383;&#21462;&#35777;&#20013;&#30340;&#23433;&#20840;&#21644;&#38544;&#31169;&#38382;&#39064;&#65292;Deepfake&#24341;&#36215;&#20102;&#24191;&#27867;&#30340;&#20844;&#20247;&#20851;&#27880;&#12290;&#38543;&#30528;&#20114;&#32852;&#32593;&#19978;&#36234;&#26469;&#36234;&#22810;&#36924;&#30495;&#30340;Deepfake&#35270;&#39057;&#30340;&#20256;&#25773;&#65292;&#20256;&#32479;&#30340;&#26816;&#27979;&#25216;&#26415;&#24050;&#32463;&#26080;&#27861;&#21306;&#20998;&#30495;&#20266;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#39592;&#24178;&#65292;&#20197;&#23616;&#37096;&#29305;&#24449;&#21644;&#33080;&#37096;&#22270;&#20687;&#20869;&#20851;&#31995;&#20026;&#37325;&#28857;&#12290;&#28982;&#32780;&#65292;&#23616;&#37096;&#29305;&#24449;&#21644;&#20851;&#31995;&#23545;&#20110;&#27169;&#22411;&#35757;&#32451;&#23398;&#20064;&#36275;&#22815;&#30340;&#19968;&#33324;&#20449;&#24687;&#26469;&#36827;&#34892;Deepfake&#26816;&#27979;&#26159;&#19981;&#20805;&#20998;&#30340;&#12290;&#22240;&#27492;&#65292;&#29616;&#26377;&#30340;Deepfake&#26816;&#27979;&#26041;&#27861;&#24050;&#32463;&#36798;&#21040;&#20102;&#36827;&#19968;&#27493;&#25552;&#39640;&#26816;&#27979;&#24615;&#33021;&#30340;&#29942;&#39048;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#21367;&#31215;&#21464;&#25442;&#22120;&#65292;&#20197;&#21516;&#26102;&#22312;&#23616;&#37096;&#21644;&#20840;&#23616;&#19978;&#21512;&#24182;&#20851;&#38190;&#30340;&#22270;&#20687;&#29305;&#24449;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#24212;&#29992;&#20102;&#21367;&#31215;&#27744;&#21270;&#21644;&#37325;&#26032;&#27880;&#24847;&#21147;&#26426;&#21046;&#26469;&#20016;&#23500;&#25552;&#21462;&#30340;&#29305;&#24449;&#21644;&#22686;&#24378;&#25928;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#40092;&#20026;&#20154;&#30693;&#30340;&#27880;&#24847;&#21147;&#25513;&#27169;&#20002;&#22833;&#26041;&#27861;&#26469;&#36827;&#19968;&#27493;&#25552;&#39640;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, Deepfake has drawn considerable public attention due to security and privacy concerns in social media digital forensics. As the wildly spreading Deepfake videos on the Internet become more realistic, traditional detection techniques have failed in distinguishing between real and fake. Most existing deep learning methods mainly focus on local features and relations within the face image using convolutional neural networks as a backbone. However, local features and relations are insufficient for model training to learn enough general information for Deepfake detection. Therefore, the existing Deepfake detection methods have reached a bottleneck to further improve the detection performance. To address this issue, we propose a deep convolutional Transformer to incorporate the decisive image features both locally and globally. Specifically, we apply convolutional pooling and re-attention to enrich the extracted features and enhance efficacy. Moreover, we employ the barely discusse
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#38024;&#23545;&#22270;&#24418;&#32467;&#26500;&#30340;&#23545;&#25239;&#25915;&#20987;&#65292;&#21457;&#29616;&#25915;&#20987;&#32773;&#26356;&#20542;&#21521;&#20110;&#22686;&#21152;&#31867;&#38388;&#36793;&#32536;&#65292;&#36890;&#36807;&#36830;&#25509;&#19981;&#21516;&#31867;&#30340;&#33410;&#28857;&#26469;&#26356;&#26377;&#25928;&#22320;&#30772;&#22351;&#33410;&#28857;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;GNN &#30340;&#22266;&#26377;&#24179;&#28369;&#24615;&#20250;&#23548;&#33268;&#22312;&#21069;&#21521;&#36807;&#31243;&#20013;&#20002;&#22833;&#20851;&#38190;&#20449;&#24687;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20855;&#26377;&#22810;&#32423;&#20256;&#25773;&#30340;&#26032;&#22411;&#20195;&#29702;&#27169;&#22411;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2208.12815</link><description>&lt;p&gt;
&#24403;&#25915;&#20987;&#22270;&#24418;&#32467;&#26500;&#26102;&#65292;&#26799;&#24230;&#21578;&#35785;&#25105;&#20204;&#20160;&#20040;
&lt;/p&gt;
&lt;p&gt;
What Does the Gradient Tell When Attacking the Graph Structure. (arXiv:2208.12815v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.12815
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#38024;&#23545;&#22270;&#24418;&#32467;&#26500;&#30340;&#23545;&#25239;&#25915;&#20987;&#65292;&#21457;&#29616;&#25915;&#20987;&#32773;&#26356;&#20542;&#21521;&#20110;&#22686;&#21152;&#31867;&#38388;&#36793;&#32536;&#65292;&#36890;&#36807;&#36830;&#25509;&#19981;&#21516;&#31867;&#30340;&#33410;&#28857;&#26469;&#26356;&#26377;&#25928;&#22320;&#30772;&#22351;&#33410;&#28857;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;GNN &#30340;&#22266;&#26377;&#24179;&#28369;&#24615;&#20250;&#23548;&#33268;&#22312;&#21069;&#21521;&#36807;&#31243;&#20013;&#20002;&#22833;&#20851;&#38190;&#20449;&#24687;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20855;&#26377;&#22810;&#32423;&#20256;&#25773;&#30340;&#26032;&#22411;&#20195;&#29702;&#27169;&#22411;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476; (GNNs) &#23481;&#26131;&#21463;&#21040;&#38024;&#23545;&#22270;&#24418;&#32467;&#26500;&#30340;&#23545;&#25239;&#25915;&#20987;&#12290;&#19968;&#20010;&#24694;&#24847;&#25915;&#20987;&#32773;&#21487;&#20197;&#22312;&#26377;&#38480;&#30340;&#36793;&#32536;&#33539;&#22260;&#20869;&#65292;&#36890;&#36807;&#32473;&#20986;&#30340;&#35757;&#32451;&#26631;&#31614;&#65292;&#26469;&#30772;&#22351;&#21463;&#23475;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#20043;&#21069;&#30340;&#32463;&#39564;&#24615;&#30740;&#31350;&#34920;&#26126;&#65292;&#22522;&#20110;&#26799;&#24230;&#30340;&#25915;&#20987;&#32773;&#26356;&#20542;&#21521;&#20110;&#28155;&#21152;&#36793;&#32536;&#32780;&#19981;&#26159;&#21024;&#38500;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35770;&#35777;&#26126;&#65292;&#25581;&#31034;&#20102;&#25915;&#20987;&#32773;&#20542;&#21521;&#20110;&#22686;&#21152;&#31867;&#38388;&#36793;&#32536;&#65292;&#36825;&#26159;&#30001;&#20110; GNN &#30340;&#28040;&#24687;&#20256;&#36882;&#26426;&#21046;&#65292;&#36825;&#20063;&#35299;&#37322;&#20102;&#20043;&#21069;&#30340;&#19968;&#20123;&#32463;&#39564;&#35266;&#23519;&#12290;&#36890;&#36807;&#36830;&#25509;&#19981;&#21516;&#31867;&#30340;&#33410;&#28857;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#26356;&#26377;&#25928;&#22320;&#30772;&#22351;&#33410;&#28857;&#29305;&#24449;&#65292;&#20174;&#32780;&#20351;&#27492;&#31867;&#25915;&#20987;&#26356;&#20855;&#20248;&#21183;&#12290;&#20294;&#26159;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102; GNN &#28040;&#24687;&#20256;&#36882;&#30340;&#22266;&#26377;&#24179;&#28369;&#24615;&#20250;&#23558;&#29305;&#24449;&#31354;&#38388;&#20013;&#30340;&#33410;&#28857;&#24046;&#24322;&#27169;&#31946;&#21270;&#65292;&#23548;&#33268;&#22312;&#21069;&#21521;&#36807;&#31243;&#20013;&#20002;&#22833;&#20851;&#38190;&#20449;&#24687;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20855;&#26377;&#22810;&#32423;&#20256;&#25773;&#30340;&#26032;&#22411;&#20195;&#29702;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent research has revealed that Graph Neural Networks (GNNs) are susceptible to adversarial attacks targeting the graph structure. A malicious attacker can manipulate a limited number of edges, given the training labels, to impair the victim model's performance. Previous empirical studies indicate that gradient-based attackers tend to add edges rather than remove them. In this paper, we present a theoretical demonstration revealing that attackers tend to increase inter-class edges due to the message passing mechanism of GNNs, which explains some previous empirical observations. By connecting dissimilar nodes, attackers can more effectively corrupt node features, making such attacks more advantageous. However, we demonstrate that the inherent smoothness of GNN's message passing tends to blur node dissimilarity in the feature space, leading to the loss of crucial information during the forward process. To address this issue, we propose a novel surrogate model with multi-level propagati
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#26816;&#27979;&#21644;&#34920;&#24449;&#25289;&#19969;&#32654;&#27954;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31995;&#32479;&#20013;&#20559;&#35265;&#21644;&#26377;&#23475;&#21051;&#26495;&#21360;&#35937;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#21333;&#35789;&#23884;&#20837;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#21644;&#25968;&#25454;&#28304;&#30340;&#20998;&#26512;&#65292;&#24182;&#22312;&#35199;&#29677;&#29273;&#35821;&#12289;&#33889;&#33796;&#29273;&#35821;&#21644;&#20811;&#19992;&#20122;&#35821;&#30340;&#26377;&#23475;&#21051;&#26495;&#21360;&#35937;&#26816;&#27979;&#26696;&#20363;&#20013;&#36827;&#34892;&#27979;&#35797;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#19981;&#21516;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31995;&#32479;&#20197;&#19981;&#21516;&#29978;&#33267;&#24847;&#24819;&#19981;&#21040;&#30340;&#26041;&#24335;&#23384;&#22312;&#20559;&#35265;&#21644;&#25918;&#22823;&#26377;&#23475;&#30340;&#21051;&#26495;&#21360;&#35937;&#12290;</title><link>http://arxiv.org/abs/2207.06591</link><description>&lt;p&gt;
&#22312;&#25289;&#19969;&#32654;&#27954;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#34920;&#29616;&#20986;&#30340;&#20559;&#35265;&#21644;&#26377;&#23475;&#21051;&#26495;&#21360;&#35937;&#30340;&#29305;&#24449;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A methodology to characterize bias and harmful stereotypes in natural language processing in Latin America. (arXiv:2207.06591v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.06591
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#26816;&#27979;&#21644;&#34920;&#24449;&#25289;&#19969;&#32654;&#27954;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31995;&#32479;&#20013;&#20559;&#35265;&#21644;&#26377;&#23475;&#21051;&#26495;&#21360;&#35937;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#21333;&#35789;&#23884;&#20837;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#21644;&#25968;&#25454;&#28304;&#30340;&#20998;&#26512;&#65292;&#24182;&#22312;&#35199;&#29677;&#29273;&#35821;&#12289;&#33889;&#33796;&#29273;&#35821;&#21644;&#20811;&#19992;&#20122;&#35821;&#30340;&#26377;&#23475;&#21051;&#26495;&#21360;&#35937;&#26816;&#27979;&#26696;&#20363;&#20013;&#36827;&#34892;&#27979;&#35797;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#19981;&#21516;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31995;&#32479;&#20197;&#19981;&#21516;&#29978;&#33267;&#24847;&#24819;&#19981;&#21040;&#30340;&#26041;&#24335;&#23384;&#22312;&#20559;&#35265;&#21644;&#25918;&#22823;&#26377;&#23475;&#30340;&#21051;&#26495;&#21360;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#21270;&#20915;&#31574;&#31995;&#32479;&#65292;&#23588;&#20854;&#26159;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#31995;&#32479;&#22312;&#25105;&#20204;&#30340;&#29983;&#27963;&#20013;&#26080;&#22788;&#19981;&#22312;&#12290;&#23427;&#20204;&#19981;&#20165;&#26159;&#25105;&#20204;&#27599;&#22825;&#20351;&#29992;&#30340;&#20114;&#32852;&#32593;&#25628;&#32034;&#24341;&#25806;&#30340;&#32972;&#21518;&#65292;&#36824;&#26377;&#26356;&#20026;&#20851;&#38190;&#30340;&#20316;&#29992;&#65306;&#31579;&#36873;&#24037;&#20316;&#20505;&#36873;&#20154;&#65292;&#30830;&#23450;&#29359;&#32618;&#23244;&#30097;&#20154;&#65292;&#35786;&#26029;&#33258;&#38381;&#30151;&#31561;&#12290;&#36825;&#20123;&#33258;&#21160;&#21270;&#31995;&#32479;&#20250;&#20986;&#29616;&#38169;&#35823;&#65292;&#36825;&#20123;&#38169;&#35823;&#21487;&#33021;&#22312;&#24456;&#22810;&#26041;&#38754;&#37117;&#26159;&#26377;&#23475;&#30340;&#65292;&#26080;&#35770;&#26159;&#22240;&#20026;&#21518;&#26524;&#30340;&#20005;&#37325;&#24615;&#65288;&#20363;&#22914;&#20581;&#24247;&#38382;&#39064;&#65289;&#36824;&#26159;&#22240;&#20026;&#25152;&#28041;&#21450;&#30340;&#20154;&#25968;&#20043;&#22810;&#12290;&#24403;&#30001;&#33258;&#21160;&#21270;&#31995;&#32479;&#36896;&#25104;&#30340;&#38169;&#35823;&#23545;&#26576;&#20010;&#32676;&#20307;&#30340;&#24433;&#21709;&#36229;&#36807;&#20854;&#20182;&#32676;&#20307;&#26102;&#65292;&#25105;&#20204;&#31216;&#27492;&#31995;&#32479;&#23384;&#22312;&#20559;&#35265;&#12290;&#22823;&#22810;&#25968;&#29616;&#20195;&#33258;&#28982;&#35821;&#35328;&#25216;&#26415;&#26159;&#22522;&#20110;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#20174;&#22823;&#37327;&#25991;&#26412;&#20013;&#33719;&#24471;&#30340;&#20998;&#26512;&#32467;&#26524;&#65292;&#21363;&#35821;&#35328;&#27169;&#22411;&#21644;&#21333;&#35789;&#23884;&#20837;&#12290;&#30001;&#20110;&#23427;&#20204;&#26159;&#36890;&#36807;&#24212;&#29992;&#23376;&#31526;&#21495;&#26426;&#22120;&#23398;&#20064;&#21019;&#24314;&#30340;&#65292;&#20027;&#35201;&#26159;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65292;&#25152;&#20197;&#23427;&#20204;&#26159;&#19981;&#36879;&#26126;&#30340;&#65292;&#19988;&#26080;&#27861;&#36890;&#36807;&#30452;&#25509;&#26816;&#26597;&#26469;&#35299;&#37322;&#65292;&#22240;&#27492;&#24456;&#38590;&#29702;&#35299;&#36825;&#20123;&#31995;&#32479;&#20309;&#26102;&#23384;&#22312;&#20559;&#35265;&#25110;&#20309;&#26102;&#25918;&#22823;&#26377;&#23475;&#30340;&#21051;&#26495;&#21360;&#35937;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#26816;&#27979;&#21644;&#34920;&#24449;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31995;&#32479;&#20013;&#30340;&#20559;&#35265;&#21644;&#26377;&#23475;&#21051;&#26495;&#21360;&#35937;&#65292;&#24212;&#29992;&#20110;&#25289;&#19969;&#32654;&#27954;&#35821;&#35328;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#20998;&#26512;&#26469;&#33258;&#19981;&#21516;&#22320;&#21306;&#30340;&#21333;&#35789;&#23884;&#20837;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65292;&#24182;&#20998;&#26512;&#29992;&#20110;&#35757;&#32451;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#30340;&#25968;&#25454;&#28304;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#38024;&#23545;&#35199;&#29677;&#29273;&#35821;&#12289;&#33889;&#33796;&#29273;&#35821;&#21644;&#20811;&#19992;&#20122;&#35821;&#30340;&#26377;&#23475;&#21051;&#26495;&#21360;&#35937;&#26816;&#27979;&#26696;&#20363;&#30740;&#31350;&#20013;&#27979;&#35797;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#19981;&#21516;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31995;&#32479;&#20197;&#19981;&#21516;&#29978;&#33267;&#24847;&#24819;&#19981;&#21040;&#30340;&#26041;&#24335;&#23384;&#22312;&#20559;&#35265;&#21644;&#25918;&#22823;&#26377;&#23475;&#30340;&#21051;&#26495;&#21360;&#35937;&#65292;&#36825;&#20984;&#26174;&#20102;&#27979;&#35797;&#21644;&#25913;&#21892;&#27492;&#31867;&#31995;&#32479;&#36136;&#37327;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automated decision-making systems, especially those based on natural language processing, are pervasive in our lives. They are not only behind the internet search engines we use daily, but also take more critical roles: selecting candidates for a job, determining suspects of a crime, diagnosing autism and more. Such automated systems make errors, which may be harmful in many ways, be it because of the severity of the consequences (as in health issues) or because of the sheer number of people they affect. When errors made by an automated system affect a population more than others, we call the system \textit{biased}.  Most modern natural language technologies are based on artifacts obtained from enormous volumes of text using machine learning, namely language models and word embeddings. Since they are created by applying subsymbolic machine learning, mostly artificial neural networks, they are opaque and practically uninterpretable by direct inspection, thus making it very difficult to 
&lt;/p&gt;</description></item><item><title>NovelCraft&#25968;&#25454;&#38598;&#25552;&#20379;&#20102;&#24320;&#25918;&#19990;&#30028;&#20013;&#26032;&#39062;&#24615;&#26816;&#27979;&#19982;&#21457;&#29616;&#20219;&#21153;&#30340;&#25361;&#25112;&#12290;&#22312;&#22797;&#26434;&#30340;&#22330;&#26223;&#20013;&#25554;&#20837;&#26032;&#39062;&#29289;&#20307;&#30340;&#26816;&#27979;&#38656;&#35201;&#26356;&#22909;&#30340;&#22522;&#20934;&#65292;&#24182;&#21457;&#29616;&#20102;&#25511;&#21046;&#20551;&#38451;&#24615;&#26102;&#26356;&#31616;&#21333;&#30340;&#26041;&#27861;&#21487;&#33021;&#27604;&#22797;&#26434;&#30340;&#26041;&#27861;&#26356;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2206.11736</link><description>&lt;p&gt;
NovelCraft&#65306;&#24320;&#25918;&#19990;&#30028;&#20013;&#30340;&#26032;&#39062;&#24615;&#26816;&#27979;&#21644;&#21457;&#29616;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
NovelCraft: A Dataset for Novelty Detection and Discovery in Open Worlds. (arXiv:2206.11736v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.11736
&lt;/p&gt;
&lt;p&gt;
NovelCraft&#25968;&#25454;&#38598;&#25552;&#20379;&#20102;&#24320;&#25918;&#19990;&#30028;&#20013;&#26032;&#39062;&#24615;&#26816;&#27979;&#19982;&#21457;&#29616;&#20219;&#21153;&#30340;&#25361;&#25112;&#12290;&#22312;&#22797;&#26434;&#30340;&#22330;&#26223;&#20013;&#25554;&#20837;&#26032;&#39062;&#29289;&#20307;&#30340;&#26816;&#27979;&#38656;&#35201;&#26356;&#22909;&#30340;&#22522;&#20934;&#65292;&#24182;&#21457;&#29616;&#20102;&#25511;&#21046;&#20551;&#38451;&#24615;&#26102;&#26356;&#31616;&#21333;&#30340;&#26041;&#27861;&#21487;&#33021;&#27604;&#22797;&#26434;&#30340;&#26041;&#27861;&#26356;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#35753;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#22312;&#19981;&#26029;&#21464;&#21270;&#30340;&#29615;&#22659;&#20013;&#25104;&#21151;&#25191;&#34892;&#20219;&#21153;&#65292;&#24517;&#39035;&#33021;&#22815;&#26816;&#27979;&#21644;&#36866;&#24212;&#26032;&#39062;&#24615;&#12290;&#28982;&#32780;&#65292;&#35270;&#35273;&#26032;&#39062;&#24615;&#26816;&#27979;&#30740;&#31350;&#36890;&#24120;&#21482;&#35780;&#20272;&#26088;&#22312;&#36827;&#34892;&#23545;&#35937;&#20998;&#31867;&#30340;&#37325;&#22797;&#21033;&#29992;&#25968;&#25454;&#38598;&#65288;&#22914;CIFAR-10&#65289;&#65292;&#20854;&#20013;&#22270;&#20687;&#32858;&#28966;&#20110;&#19968;&#20010;&#26126;&#26174;&#12289;&#23621;&#20013;&#30340;&#23545;&#35937;&#12290;&#38656;&#35201;&#26032;&#30340;&#22522;&#20934;&#26469;&#20195;&#34920;&#22312;&#24320;&#25918;&#19990;&#30028;&#20013;&#23548;&#33322;&#22797;&#26434;&#22330;&#26223;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#26032;NovelCraft&#25968;&#25454;&#38598;&#21253;&#21547;&#23436;&#25104;&#20462;&#25913;&#21518;&#30340;Minecraft&#29615;&#22659;&#20013;&#30340;&#36339;&#36339;&#29699;&#35013;&#37197;&#20219;&#21153;&#30340;&#20195;&#29702;&#25152;&#30475;&#21040;&#30340;&#22270;&#20687;&#21644;&#31526;&#21495;&#19990;&#30028;&#29366;&#24577;&#30340;&#22810;&#27169;&#24335;&#24773;&#33410;&#25968;&#25454;&#12290;&#22312;&#26576;&#20123;&#24773;&#33410;&#20013;&#65292;&#25105;&#20204;&#22312;&#22797;&#26434;&#30340;3D&#22330;&#26223;&#20013;&#25554;&#20837;&#26032;&#39062;&#29289;&#20307;&#65292;&#36825;&#20123;&#29289;&#20307;&#21487;&#33021;&#24433;&#21709;&#28216;&#25103;&#29609;&#27861;&#24182;&#20986;&#29616;&#22312;&#21508;&#31181;&#22823;&#23567;&#21644;&#20301;&#32622;&#20013;&#12290;&#25105;&#20204;&#30340;&#35270;&#35273;&#26032;&#39062;&#24615;&#26816;&#27979;&#22522;&#20934;&#21457;&#29616;&#65292;&#25511;&#21046;&#20551;&#38451;&#24615;&#26102;&#65292;&#26368;&#22909;&#30340;&#38754;&#31215;&#19979;&#26354;&#32447;&#24230;&#37327;&#30340;&#26041;&#27861;&#21487;&#33021;&#20250;&#34987;&#26356;&#31616;&#21333;&#30340;&#26367;&#20195;&#26041;&#27861;&#36229;&#36807;&#12290;
&lt;/p&gt;
&lt;p&gt;
In order for artificial agents to successfully perform tasks in changing environments, they must be able to both detect and adapt to novelty. However, visual novelty detection research often only evaluates on repurposed datasets such as CIFAR-10 originally intended for object classification, where images focus on one distinct, well-centered object. New benchmarks are needed to represent the challenges of navigating the complex scenes of an open world. Our new NovelCraft dataset contains multimodal episodic data of the images and symbolic world-states seen by an agent completing a pogo stick assembly task within a modified Minecraft environment. In some episodes, we insert novel objects within the complex 3D scene that may impact gameplay and appear in a variety of sizes and positions. Our visual novelty detection benchmark finds that methods that rank best on popular area-under-the-curve metrics may be outperformed by simpler alternatives when controlling false positives matters most. 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;IOP-FL&#65292;&#19968;&#31181;&#32852;&#37030;&#23398;&#20064;&#30340;&#20869;&#22806;&#20010;&#24615;&#21270;&#26041;&#27861;&#65292;&#20197;&#22686;&#24378;&#21307;&#23398;&#25104;&#20687;&#20219;&#21153;&#20013;&#21333;&#20010;&#23458;&#25143;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2204.08467</link><description>&lt;p&gt;
IOP-FL: Inside-Outside Personalization for Federated Medical Image Segmentation&#65288;&#32852;&#37030;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#30340;&#20869;&#22806;&#20010;&#24615;&#21270;&#26041;&#27861;&#65289;
&lt;/p&gt;
&lt;p&gt;
IOP-FL: Inside-Outside Personalization for Federated Medical Image Segmentation. (arXiv:2204.08467v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.08467
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;IOP-FL&#65292;&#19968;&#31181;&#32852;&#37030;&#23398;&#20064;&#30340;&#20869;&#22806;&#20010;&#24615;&#21270;&#26041;&#27861;&#65292;&#20197;&#22686;&#24378;&#21307;&#23398;&#25104;&#20687;&#20219;&#21153;&#20013;&#21333;&#20010;&#23458;&#25143;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20801;&#35768;&#22810;&#20010;&#21307;&#30103;&#26426;&#26500;&#22312;&#26410;&#38598;&#20013;&#23458;&#25143;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#21327;&#21516;&#23398;&#20064;&#20840;&#23616;&#27169;&#22411;&#12290;&#30001;&#20110;&#26469;&#33258;&#21508;&#31181;&#25195;&#25551;&#20202;&#21644;&#24739;&#32773;&#20154;&#21475;&#32479;&#35745;&#23398;&#30340;&#21307;&#23398;&#22270;&#20687;&#30340;&#24322;&#36136;&#24615;&#65292;&#36825;&#31181;&#20840;&#23616;&#27169;&#22411;&#26222;&#36941;&#23454;&#29616;&#27599;&#20010;&#23458;&#25143;&#30340;&#26368;&#20339;&#24615;&#33021;&#26159;&#22256;&#38590;&#30340;&#65292;&#22914;&#26524;&#21487;&#33021;&#30340;&#35805;&#12290;&#24403;&#23558;&#20840;&#23616;&#27169;&#22411;&#37096;&#32626;&#21040;&#22312;FL&#26399;&#38388;&#26410;&#21576;&#29616;&#30340;&#20998;&#24067;&#19978;&#30475;&#19981;&#35265;&#30340;&#23458;&#25143;&#26102;&#65292;&#36825;&#20010;&#38382;&#39064;&#21464;&#24471;&#26356;&#20026;&#37325;&#35201;&#12290;&#20026;&#20102;&#20248;&#21270;&#27599;&#20010;&#23458;&#25143;&#22312;&#21307;&#23398;&#25104;&#20687;&#20219;&#21153;&#20013;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#29992;&#20110;FL&#20013;&#30340;&#8220;&#20869;&#37096;&#21644;&#22806;&#37096;&#27169;&#22411;&#20010;&#24615;&#21270;&#8221;&#65288;IOP-FL&#65289;&#12290;&#25105;&#20204;&#30340;&#20869;&#37096;&#20010;&#24615;&#21270;&#20351;&#29992;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#22522;&#20110;&#26799;&#24230;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32047;&#31215;&#36890;&#29992;&#30693;&#35782;&#30340;&#20840;&#23616;&#26799;&#24230;&#21644;&#23458;&#25143;&#29305;&#23450;&#20248;&#21270;&#30340;&#26412;&#22320;&#26799;&#24230;&#65292;&#21033;&#29992;&#27599;&#20010;&#23458;&#25143;&#30340;&#26412;&#22320;&#36866;&#24212;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#22806;&#37096;&#20010;&#24615;&#21270;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#19968;&#20010;&#23458;&#25143;&#33258;&#36866;&#24212;&#26426;&#21046;&#26469;&#24212;&#23545;&#26469;&#33258;&#26410;&#30693;&#23458;&#25143;&#30340;&#22270;&#20687;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#22823;&#22823;&#25552;&#39640;&#21333;&#20010;&#23458;&#25143;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#24182;&#19988;&#21487;&#20197;&#21516;&#26102;&#20445;&#25345;&#22312;&#19981;&#21516;&#23458;&#25143;&#20043;&#38388;&#30340;&#36890;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) allows multiple medical institutions to collaboratively learn a global model without centralizing client data. It is difficult, if possible at all, for such a global model to commonly achieve optimal performance for each individual client, due to the heterogeneity of medical images from various scanners and patient demographics. This problem becomes even more significant when deploying the global model to unseen clients outside the FL with unseen distributions not presented during federated training. To optimize the prediction accuracy of each individual client for medical imaging tasks, we propose a novel unified framework for both \textit{Inside and Outside model Personalization in FL} (IOP-FL). Our inside personalization uses a lightweight gradient-based approach that exploits the local adapted model for each client, by accumulating both the global gradients for common knowledge and the local gradients for client-specific optimization. Moreover, and important
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20154;&#24037;&#26234;&#33021;&#22312; COVID-19 &#20013;&#30340;&#24212;&#29992;&#65292;&#25506;&#35752;&#20102;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#21487;&#24110;&#21161;&#26089;&#26399;&#21644;&#24265;&#20215;&#22320;&#35786;&#26029;&#35813;&#30149;&#65292;&#26377;&#21161;&#20110;&#21307;&#29983;&#21644;&#30740;&#31350;&#20154;&#21592;&#23545;&#25239;&#35813;&#30149;&#12290;</title><link>http://arxiv.org/abs/2111.09537</link><description>&lt;p&gt;
&#26032;&#20896;&#32954;&#28814;&#20013;&#30340;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
The Prominence of Artificial Intelligence in COVID-19. (arXiv:2111.09537v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.09537
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20154;&#24037;&#26234;&#33021;&#22312; COVID-19 &#20013;&#30340;&#24212;&#29992;&#65292;&#25506;&#35752;&#20102;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#21487;&#24110;&#21161;&#26089;&#26399;&#21644;&#24265;&#20215;&#22320;&#35786;&#26029;&#35813;&#30149;&#65292;&#26377;&#21161;&#20110;&#21307;&#29983;&#21644;&#30740;&#31350;&#20154;&#21592;&#23545;&#25239;&#35813;&#30149;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
2019&#24180;12&#26376;&#65292;&#19968;&#31181;&#26032;&#22411;&#30149;&#27602;&#65292;COVID-19&#24050;&#32463;&#23548;&#33268;&#20102;&#26497;&#22810;&#30340;&#27515;&#20129;&#12290;&#19982;&#35199;&#29677;&#29273;&#27969;&#24863;1918&#24180;&#30456;&#27604;&#65292;&#19982;&#36825;&#31181;&#26032;&#20896;&#30149;&#27602;&#30340;&#26007;&#20105;&#20196;&#20154;&#22256;&#24785;&#21644;&#24656;&#24807;&#12290;&#32780;&#22312;&#21069;&#32447;&#21307;&#29983;&#21644;&#21307;&#23398;&#30740;&#31350;&#20154;&#21592;&#22312;&#25511;&#21046;&#36825;&#31181;&#39640;&#24230;&#20256;&#26579;&#30149;&#27602;&#30340;&#20256;&#25773;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#30340;&#21516;&#26102;&#65292;&#25216;&#26415;&#22312;&#36825;&#22330;&#25112;&#26007;&#20013;&#20063;&#35777;&#26126;&#20102;&#20854;&#37325;&#35201;&#24615;&#12290;&#27492;&#22806;&#65292;&#20154;&#24037;&#26234;&#33021;&#22312;&#35768;&#22810;&#21307;&#30103;&#24212;&#29992;&#20013;&#24471;&#21040;&#20102;&#37319;&#29992;&#65292;&#21487;&#20197;&#35786;&#26029;&#35768;&#22810;&#30142;&#30149;&#65292;&#29978;&#33267;&#20351;&#32463;&#39564;&#20016;&#23500;&#30340;&#21307;&#29983;&#24863;&#21040;&#22256;&#24785;&#12290;&#22240;&#27492;&#65292;&#26412;&#35843;&#26597;&#35770;&#25991;&#25506;&#35752;&#20102;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#21487;&#24110;&#21161;&#21307;&#29983;&#21644;&#30740;&#31350;&#20154;&#21592;&#26089;&#26399;&#21644;&#24265;&#20215;&#22320;&#35786;&#26029;&#35813;&#30149;&#12290;&#22823;&#22810;&#25968;&#21457;&#23637;&#20013;&#22269;&#23478;&#38590;&#20197;&#20351;&#29992;&#20256;&#32479;&#26041;&#24335;&#36827;&#34892;&#27979;&#35797;&#65292;&#20294;&#21487;&#20197;&#37319;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#33719;&#24471;&#19981;&#21516;&#31867;&#22411;&#30340;&#21307;&#23398;&#22270;&#20687;&#20063;&#28608;&#21457;&#20102;&#30740;&#31350;&#20154;&#21592;&#30340;&#21160;&#21147;&#12290;&#22240;&#27492;&#65292;&#22823;&#37327;&#30340;&#25216;&#26415;&#21019;&#26032;&#34987;&#24341;&#20837;&#24182;&#24212;&#29992;&#20110;&#23454;&#36341;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
In December 2019, a novel virus called COVID-19 had caused an enormous number of causalities to date. The battle with the novel Coronavirus is baffling and horrifying after the Spanish Flu 2019. While the front-line doctors and medical researchers have made significant progress in controlling the spread of the highly contiguous virus, technology has also proved its significance in the battle. Moreover, Artificial Intelligence has been adopted in many medical applications to diagnose many diseases, even baffling experienced doctors. Therefore, this survey paper explores the methodologies proposed that can aid doctors and researchers in early and inexpensive methods of diagnosis of the disease. Most developing countries have difficulties carrying out tests using the conventional manner, but a significant way can be adopted with Machine and Deep Learning. On the other hand, the access to different types of medical images has motivated the researchers. As a result, a mammoth number of tech
&lt;/p&gt;</description></item><item><title>&#26412;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#23398;&#20064;&#25512;&#29702;&#20013;&#30340;&#35745;&#31639;&#21644;&#33021;&#37327;&#28040;&#32791;&#36235;&#21183;&#65292;&#37325;&#28857;&#20851;&#27880;&#25512;&#29702;&#25104;&#26412;&#32780;&#38750;&#35757;&#32451;&#25104;&#26412;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#38500;&#20102;&#31639;&#27861;&#21019;&#26032;&#22806;&#65292;&#26356;&#20855;&#20307;&#21644;&#24378;&#22823;&#30340;&#30828;&#20214;&#36890;&#24120;&#20276;&#38543;&#30528;&#37325;&#35201;&#30340;&#33021;&#37327;&#25928;&#29575;&#20248;&#21270;&#65292;&#23548;&#33268;&#25512;&#29702;&#25104;&#26412;&#30340;&#25552;&#39640;&#21576;&#29616;&#26580;&#21644;&#30340;&#36235;&#21183;&#12290;</title><link>http://arxiv.org/abs/2109.05472</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#25512;&#29702;&#20013;&#30340;&#35745;&#31639;&#21644;&#33021;&#37327;&#28040;&#32791;&#36235;&#21183;
&lt;/p&gt;
&lt;p&gt;
Compute and Energy Consumption Trends in Deep Learning Inference. (arXiv:2109.05472v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2109.05472
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#23398;&#20064;&#25512;&#29702;&#20013;&#30340;&#35745;&#31639;&#21644;&#33021;&#37327;&#28040;&#32791;&#36235;&#21183;&#65292;&#37325;&#28857;&#20851;&#27880;&#25512;&#29702;&#25104;&#26412;&#32780;&#38750;&#35757;&#32451;&#25104;&#26412;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#38500;&#20102;&#31639;&#27861;&#21019;&#26032;&#22806;&#65292;&#26356;&#20855;&#20307;&#21644;&#24378;&#22823;&#30340;&#30828;&#20214;&#36890;&#24120;&#20276;&#38543;&#30528;&#37325;&#35201;&#30340;&#33021;&#37327;&#25928;&#29575;&#20248;&#21270;&#65292;&#23548;&#33268;&#25512;&#29702;&#25104;&#26412;&#30340;&#25552;&#39640;&#21576;&#29616;&#26580;&#21644;&#30340;&#36235;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#20123;&#20154;&#35748;&#20026;&#65292;&#28145;&#24230;&#23398;&#20064;&#31561;AI&#33539;&#24335;&#30340;&#36827;&#23637;&#19982;&#21442;&#25968;&#25968;&#37327;&#25351;&#25968;&#22686;&#38271;&#26377;&#20851;&#12290;&#26377;&#35768;&#22810;&#30740;&#31350;&#35777;&#23454;&#36825;&#20123;&#36235;&#21183;&#65292;&#20294;&#36825;&#26159;&#21542;&#24847;&#21619;&#30528;&#33021;&#37327;&#28040;&#32791;&#21576;&#25351;&#25968;&#22686;&#38271;&#65311;&#20026;&#20102;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#23558;&#37325;&#28857;&#25918;&#22312;&#25512;&#29702;&#25104;&#26412;&#19978;&#65292;&#32780;&#19981;&#26159;&#35757;&#32451;&#25104;&#26412;&#65292;&#22240;&#20026;&#21069;&#32773;&#21344;&#25454;&#20102;&#22823;&#37096;&#20998;&#30340;&#35745;&#31639;&#24037;&#20316;&#37327;&#65292;&#20165;&#22240;&#20026;&#26377;&#20056;&#27861;&#22240;&#32032;&#23384;&#22312;&#12290;&#27492;&#22806;&#65292;&#38500;&#20102;&#31639;&#27861;&#21019;&#26032;&#22806;&#65292;&#25105;&#20204;&#36824;&#32771;&#34385;&#20102;&#26356;&#20855;&#20307;&#21644;&#24378;&#22823;&#30340;&#30828;&#20214;&#65288;&#23548;&#33268;&#26356;&#39640;&#30340;FLOPS&#65289;&#65292;&#36825;&#36890;&#24120;&#20276;&#38543;&#30528;&#37325;&#35201;&#30340;&#33021;&#37327;&#25928;&#29575;&#20248;&#21270;&#12290;&#25105;&#20204;&#36824;&#23558;&#28966;&#28857;&#20174;&#31361;&#30772;&#24615;&#35770;&#25991;&#30340;&#31532;&#19968;&#27425;&#23454;&#29616;&#36716;&#31227;&#21040;&#20102;&#19968;&#20004;&#24180;&#21518;&#30340;&#25216;&#26415;&#29256;&#26412;&#30340;&#24041;&#22266;&#29256;&#26412;&#12290;&#22312;&#36825;&#20010;&#29420;&#29305;&#21644;&#20840;&#38754;&#30340;&#35270;&#35282;&#19979;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#30456;&#20851;&#27169;&#22411;&#65306;&#23545;&#20110;&#25345;&#32493;&#25552;&#39640;&#24615;&#33021;&#65292;&#25105;&#20204;&#30475;&#21040;&#19968;&#20010;&#26356;&#26580;&#21644;&#30340;&#36235;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
The progress of some AI paradigms such as deep learning is said to be linked to an exponential growth in the number of parameters. There are many studies corroborating these trends, but does this translate into an exponential increase in energy consumption? In order to answer this question we focus on inference costs rather than training costs, as the former account for most of the computing effort, solely because of the multiplicative factors. Also, apart from algorithmic innovations, we account for more specific and powerful hardware (leading to higher FLOPS) that is usually accompanied with important energy efficiency optimisations. We also move the focus from the first implementation of a breakthrough paper towards the consolidated version of the techniques one or two year later. Under this distinctive and comprehensive perspective, we study relevant models in the areas of computer vision and natural language processing: for a sustained increase in performance we see a much softer 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20154;&#31867;&#22914;&#20309;&#36890;&#36807;&#31616;&#35201;&#25945;&#31243;&#21644;&#35299;&#37322;&#24615;&#21453;&#39304;&#20174;&#23569;&#37327;&#30340;&#35757;&#32451;&#31034;&#20363;&#20013;&#23398;&#20064;&#25277;&#35937;&#25512;&#29702;&#20219;&#21153;&#65292;&#24182;&#33021;&#25104;&#21151;&#22320;&#23558;&#20854;&#25512;&#24191;&#21040;&#35757;&#32451;&#33539;&#22260;&#20043;&#22806;&#30340;&#24773;&#20917;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#23454;&#29616;&#20154;&#31867;&#23398;&#20064;&#26426;&#21046;&#23545;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#21457;&#23637;&#38750;&#24120;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2107.06994</link><description>&lt;p&gt;
&#31995;&#32479;&#21270;&#30340;&#20154;&#31867;&#23398;&#20064;&#19982;&#25512;&#24191;&#65306;&#36890;&#36807;&#31616;&#35201;&#25945;&#31243;&#21644;&#35299;&#37322;&#24615;&#21453;&#39304;&#23454;&#29616;
&lt;/p&gt;
&lt;p&gt;
Systematic human learning and generalization from a brief tutorial with explanatory feedback. (arXiv:2107.06994v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2107.06994
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20154;&#31867;&#22914;&#20309;&#36890;&#36807;&#31616;&#35201;&#25945;&#31243;&#21644;&#35299;&#37322;&#24615;&#21453;&#39304;&#20174;&#23569;&#37327;&#30340;&#35757;&#32451;&#31034;&#20363;&#20013;&#23398;&#20064;&#25277;&#35937;&#25512;&#29702;&#20219;&#21153;&#65292;&#24182;&#33021;&#25104;&#21151;&#22320;&#23558;&#20854;&#25512;&#24191;&#21040;&#35757;&#32451;&#33539;&#22260;&#20043;&#22806;&#30340;&#24773;&#20917;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#23454;&#29616;&#20154;&#31867;&#23398;&#20064;&#26426;&#21046;&#23545;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#21457;&#23637;&#38750;&#24120;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#38271;&#26399;&#20197;&#26469;&#34987;&#29992;&#26469;&#27169;&#25311;&#20154;&#31867;&#26234;&#33021;&#65292;&#25429;&#25417;&#34892;&#20026;&#21644;&#35748;&#30693;&#30340;&#20803;&#32032;&#21450;&#20854;&#31070;&#32463;&#22522;&#30784;&#12290; &#28145;&#24230;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#20351;&#24471;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#22312;&#35768;&#22810;&#26041;&#38754;&#36798;&#21040;&#29978;&#33267;&#36229;&#36807;&#20154;&#31867;&#30340;&#26234;&#33021;&#27700;&#24179;&#65292;&#28982;&#32780;&#19982;&#20154;&#31867;&#19981;&#21516;&#30340;&#26159;&#65292;&#23427;&#20204;&#24555;&#36895;&#23398;&#20064;&#26032;&#20219;&#21153;&#30340;&#33021;&#21147;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#20154;&#20204;&#19981;&#20165;&#21487;&#20197;&#22312;&#29087;&#24713;&#30340;&#39046;&#22495;&#36827;&#34892;&#25512;&#29702;&#65292;&#32780;&#19988;&#36824;&#21487;&#20197;&#24555;&#36895;&#23398;&#20064;&#25512;&#29702;&#26032;&#38382;&#39064;&#21644;&#24773;&#20917;&#65292;&#36825;&#23601;&#24341;&#21457;&#20102;&#19968;&#20010;&#38382;&#39064;&#65306;&#29616;&#20195;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#22914;&#20309;&#25429;&#25417;&#20154;&#31867;&#26234;&#33021;&#20197;&#21450;&#23427;&#20204;&#30340;&#21738;&#20123;&#26041;&#38754;&#19982;&#20154;&#31867;&#19981;&#21516;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#30740;&#31350;&#25104;&#24180;&#20154;&#20174;&#19968;&#20010;&#22522;&#20110;&#25968;&#29420;&#30340;&#25277;&#35937;&#25512;&#29702;&#20219;&#21153;&#30340;&#31616;&#35201;&#25945;&#23398;&#20013;&#23398;&#20064;&#30340;&#33021;&#21147;&#12289;&#36890;&#36807;&#35299;&#37322;&#24615;&#21453;&#39304;&#32416;&#27491;&#38169;&#35823;&#31572;&#26696;&#21644;&#29421;&#31364;&#33539;&#22260;&#30340;&#35757;&#32451;&#31034;&#20363;&#65292;&#26469;&#25506;&#35752;&#36825;&#20010;&#24046;&#36317;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#25484;&#25569;&#35813;&#20219;&#21153;&#30340;&#21442;&#19982;&#32773;&#21487;&#20197;&#22312;&#23569;&#25968;&#20960;&#27425;&#35797;&#39564;&#20013;&#23454;&#29616;&#65292;&#24182;&#21487;&#20197;&#25512;&#24191;&#21040;&#35757;&#32451;&#33539;&#22260;&#22806;&#30340;&#35868;&#39064;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#31361;&#20986;&#20102;&#22312;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#20013;&#23454;&#29616;&#20154;&#31867;&#23398;&#20064;&#26426;&#21046;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural networks have long been used to model human intelligence, capturing elements of behavior and cognition, and their neural basis. Recent advancements in deep learning have enabled neural network models to reach and even surpass human levels of intelligence in many respects, yet unlike humans, their ability to learn new tasks quickly remains a challenge. People can reason not only in familiar domains, but can also rapidly learn to reason through novel problems and situations, raising the question of how well modern neural network models capture human intelligence and in which ways they diverge. In this work, we explore this gap by investigating human adults' ability to learn an abstract reasoning task based on Sudoku from a brief instructional tutorial with explanatory feedback for incorrect responses using a narrow range of training examples. We find that participants who master the task do so within a small number of trials and generalize well to puzzles outside of the training r
&lt;/p&gt;</description></item></channel></rss>