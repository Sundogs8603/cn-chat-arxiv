<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#22312;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#20013;&#25552;&#39640;&#27867;&#21270;&#24615;&#33021;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22686;&#24378;&#23545;&#35805;&#19978;&#19979;&#25991;&#65292;&#20351;&#29992;&#26377;&#25928;&#30340;&#24037;&#20316;&#27969;&#21517;&#31216;&#21644;&#34892;&#21160;&#35745;&#21010;&#32534;&#30721;&#26469;&#23436;&#25104;&#22810;&#27493;&#39588;&#20219;&#21153;&#30340;&#25191;&#34892;&#65292;&#35299;&#20915;&#20102;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#26080;&#27861;&#21487;&#38752;&#25191;&#34892;&#35757;&#32451;&#26399;&#38388;&#26410;&#35265;&#36807;&#30340;&#26032;&#22810;&#27493;&#39588;&#20219;&#21153;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.01729</link><description>&lt;p&gt;
&#29992;&#24037;&#20316;&#27969;&#31243;&#19982;&#34892;&#21160;&#35745;&#21010;&#22312;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#20013;&#25552;&#39640;&#27867;&#21270;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Improving Generalization in Task-oriented Dialogues with Workflows and Action Plans. (arXiv:2306.01729v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01729
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#22312;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#20013;&#25552;&#39640;&#27867;&#21270;&#24615;&#33021;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22686;&#24378;&#23545;&#35805;&#19978;&#19979;&#25991;&#65292;&#20351;&#29992;&#26377;&#25928;&#30340;&#24037;&#20316;&#27969;&#21517;&#31216;&#21644;&#34892;&#21160;&#35745;&#21010;&#32534;&#30721;&#26469;&#23436;&#25104;&#22810;&#27493;&#39588;&#20219;&#21153;&#30340;&#25191;&#34892;&#65292;&#35299;&#20915;&#20102;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#26080;&#27861;&#21487;&#38752;&#25191;&#34892;&#35757;&#32451;&#26399;&#38388;&#26410;&#35265;&#36807;&#30340;&#26032;&#22810;&#27493;&#39588;&#20219;&#21153;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#30340;&#38590;&#28857;&#22312;&#20110;&#28041;&#21450;&#29702;&#35299;&#29992;&#25143;&#24847;&#22270;&#12289;&#20174;&#29992;&#25143;&#22788;&#25910;&#38598;&#20449;&#24687;&#12289;&#25191;&#34892;API&#35843;&#29992;&#20197;&#21450;&#29983;&#25104;&#26377;&#29992;&#27969;&#30021;&#30340;&#22238;&#24212;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#22797;&#26434;&#30340;&#20219;&#21153;&#65292;&#25105;&#20204;&#24517;&#39035;&#25353;&#29305;&#23450;&#39034;&#24207;&#22312;&#22810;&#20010;&#27493;&#39588;&#20013;&#27491;&#30830;&#22320;&#23436;&#25104;&#25152;&#26377;&#36825;&#20123;&#20107;&#24773;&#12290;&#34429;&#28982;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36827;&#34892;&#31471;&#21040;&#31471;&#30340;&#24494;&#35843;&#26469;&#21019;&#24314;&#29983;&#25104;&#27969;&#30021;&#25991;&#26412;&#30340;&#22810;&#27493;&#39588;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#20195;&#29702;&#65292;&#20294;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#23454;&#65292;&#20165;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#26080;&#27861;&#21487;&#38752;&#22320;&#25191;&#34892;&#35757;&#32451;&#26399;&#38388;&#26410;&#35265;&#36807;&#30340;&#26032;&#22810;&#27493;&#39588;&#20219;&#21153;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#22686;&#24378;&#20102;&#32473;&#20986;&#21040;text2text&#36716;&#25442;&#22120;&#30340;&#23545;&#35805;&#19978;&#19979;&#25991;&#65292;&#20854;&#20013;&#21253;&#25324;&#24050;&#30693;&#30340;&#26377;&#25928;&#24037;&#20316;&#27969;&#21517;&#31216;&#21644;&#34892;&#21160;&#35745;&#21010;&#12290;&#34892;&#21160;&#35745;&#21010;&#21253;&#25324;&#23436;&#25104;&#20219;&#21153;&#25152;&#38656;&#30340;&#34892;&#21160;&#24207;&#21015;&#65292;&#24182;&#32534;&#30721;&#20026;&#31616;&#21333;&#30340;&#20851;&#38190;&#23383;&#24207;&#21015;&#65288;&#20363;&#22914;&#65292;&#39564;&#35777;&#36523;&#20221;&#65292;&#25289;&#36215;&#36134;&#25143;&#65292;&#37325;&#32622;&#23494;&#30721;&#31561;&#65289;&#12290;&#25105;&#20204;&#22312;&#22522;&#20110;&#34892;&#21160;&#30340;&#23545;&#35805;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Task-oriented dialogue is difficult in part because it involves understanding user intent, collecting information from the user, executing API calls, and generating helpful and fluent responses. However, for complex tasks one must also correctly do all of these things over multiple steps, and in a specific order. While large pre-trained language models can be fine-tuned end-to-end to create multi-step task-oriented dialogue agents that generate fluent text, our experiments confirm that this approach alone cannot reliably perform new multi-step tasks that are unseen during training. To address these limitations, we augment the dialogue contexts given to \textmd{text2text} transformers with known \textit{valid workflow names} and \textit{action plans}. Action plans consist of sequences of actions required to accomplish a task, and are encoded as simple sequences of keywords (e.g. verify-identity, pull-up-account, reset-password, etc.). We perform extensive experiments on the Action-Based
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21435;&#22122;&#25193;&#25955;&#29983;&#25104;&#27169;&#22411;&#24314;&#27169;&#30340;mask&#20808;&#39564;&#65292;&#25913;&#36827;&#29616;&#26377;&#30340;&#35821;&#20041;&#20998;&#21106;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36798;&#21040;&#20102;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#26368;&#20808;&#36827;&#25928;&#26524;&#65292;&#36229;&#21442;&#25968;&#35843;&#25972;&#36739;&#23569;&#12290;</title><link>http://arxiv.org/abs/2306.01721</link><description>&lt;p&gt;
&#21033;&#29992;mask&#20808;&#39564;&#27169;&#22411;&#21435;&#22122;&#25193;&#25955;&#35821;&#20041;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Denoising Diffusion Semantic Segmentation with Mask Prior Modeling. (arXiv:2306.01721v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01721
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21435;&#22122;&#25193;&#25955;&#29983;&#25104;&#27169;&#22411;&#24314;&#27169;&#30340;mask&#20808;&#39564;&#65292;&#25913;&#36827;&#29616;&#26377;&#30340;&#35821;&#20041;&#20998;&#21106;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36798;&#21040;&#20102;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#26368;&#20808;&#36827;&#25928;&#26524;&#65292;&#36229;&#21442;&#25968;&#35843;&#25972;&#36739;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#20041;&#20998;&#21106;&#19968;&#30452;&#20197;&#26469;&#37117;&#26159;&#23398;&#20064;&#26356;&#20855;&#26377;&#21306;&#20998;&#24615;&#30340;&#22270;&#20687;&#34920;&#31034;&#29992;&#20110;&#23545;&#27599;&#20010;&#20687;&#32032;&#36827;&#34892;&#20998;&#31867;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#20294;&#20998;&#21106;&#25513;&#27169;&#26412;&#36523;&#30340;&#20808;&#39564;&#65292;&#20363;&#22914;&#20960;&#20309;&#32422;&#26463;&#21644;&#35821;&#20041;&#32422;&#26463;&#20173;&#26410;&#34987;&#28145;&#20837;&#25506;&#32034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26368;&#36817;&#24320;&#21457;&#30340;&#21435;&#22122;&#25193;&#25955;&#29983;&#25104;&#27169;&#22411;&#24314;&#27169;&#30340;mask&#20808;&#39564;&#65292;&#25913;&#21892;&#29616;&#26377;&#21306;&#20998;&#24615;&#26041;&#27861;&#30340;&#35821;&#20041;&#20998;&#21106;&#36136;&#37327;&#12290;&#25105;&#20204;&#20174;&#20026;mask&#20808;&#39564;&#24314;&#27169;&#35843;&#25972;&#25193;&#25955;&#27169;&#22411;&#30340;&#32479;&#19968;&#26550;&#26500;&#24320;&#22987;&#65292;&#23558;&#26412;&#25991;&#37325;&#28857;&#25918;&#22312;&#20855;&#26377;&#31163;&#25955;&#25193;&#25955;&#30340;&#20855;&#20307;&#23454;&#20363;&#19978;&#65292;&#24182;&#30830;&#23450;&#20854;&#25104;&#21151;&#24212;&#29992;&#30340;&#21508;&#31181;&#20851;&#38190;&#35774;&#35745;&#36873;&#25321;&#12290;&#25105;&#20204;&#30340;&#21021;&#27493;&#20998;&#26512;&#25581;&#31034;&#20102;&#20960;&#20010;&#37325;&#35201;&#21457;&#29616;&#65292;&#21253;&#25324;&#65306;&#65288;1&#65289;&#31616;&#21333;&#22320;&#23558;&#25193;&#25955;&#27169;&#22411;&#38598;&#25104;&#21040;&#35821;&#20041;&#20998;&#21106;&#20013;&#19981;&#36275;&#20197;&#36798;&#21040;&#26368;&#20339;&#25928;&#26524;&#65292;&#35774;&#35745;&#19981;&#33391;&#30340;&#25193;&#25955;&#36807;&#31243;&#21487;&#33021;&#23548;&#33268;&#20998;&#21106;&#24615;&#33021;&#19979;&#38477;;&#65288;2&#65289;&#22312;&#25193;&#25955;&#36807;&#31243;&#20013;&#65292;&#22312;&#20445;&#30041;&#36974;&#32617;&#32467;&#26500;&#21644;&#21512;&#24182;&#36755;&#20837;&#22270;&#20687;&#20013;&#30340;&#20854;&#20182;&#20449;&#24687;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#33267;&#20851;&#37325;&#35201;;&#65288;3&#65289;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#32780;&#35843;&#25972;&#30340;&#36229;&#21442;&#25968;&#36739;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;
The evolution of semantic segmentation has long been dominated by learning more discriminative image representations for classifying each pixel. Despite the prominent advancements, the priors of segmentation masks themselves, e.g., geometric and semantic constraints, are still under-explored. In this paper, we propose to ameliorate the semantic segmentation quality of existing discriminative approaches with a mask prior modeled by a recently-developed denoising diffusion generative model. Beginning with a unified architecture that adapts diffusion models for mask prior modeling, we focus this work on a specific instantiation with discrete diffusion and identify a variety of key design choices for its successful application. Our exploratory analysis revealed several important findings, including: (1) a simple integration of diffusion models into semantic segmentation is not sufficient, and a poorly-designed diffusion process might lead to degradation in segmentation performance; (2) dur
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25581;&#31034;&#20102;&#29616;&#26377;&#27169;&#22411;&#21512;&#24182;&#25216;&#26415;&#23384;&#22312;&#30340;&#24178;&#25200;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#20855;&#26377;&#24191;&#27867;&#36866;&#29992;&#24615;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#26174;&#30528;&#25552;&#39640;&#21512;&#24182;&#21518;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.01708</link><description>&lt;p&gt;
&#21512;&#24182;&#27169;&#22411;&#26102;&#22914;&#20309;&#35299;&#20915;&#24178;&#25200;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Resolving Interference When Merging Models. (arXiv:2306.01708v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01708
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25581;&#31034;&#20102;&#29616;&#26377;&#27169;&#22411;&#21512;&#24182;&#25216;&#26415;&#23384;&#22312;&#30340;&#24178;&#25200;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#20855;&#26377;&#24191;&#27867;&#36866;&#29992;&#24615;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#26174;&#30528;&#25552;&#39640;&#21512;&#24182;&#21518;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36801;&#31227;&#23398;&#20064;&#21487;&#20197;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#36827;&#19968;&#27493;&#24494;&#35843;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#20174;&#32780;&#33719;&#24471;&#26174;&#33879;&#30340;&#20248;&#21183;&#65292;&#21253;&#25324;&#25913;&#36827;&#19979;&#28216;&#24615;&#33021;&#65292;&#21152;&#24555;&#25910;&#25947;&#36895;&#24230;&#21644;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#24050;&#26377;&#30340;&#27169;&#22411;&#21512;&#24182;&#25216;&#26415;&#24448;&#24448;&#24573;&#35270;&#20102;&#19981;&#21516;&#27169;&#22411;&#21442;&#25968;&#20043;&#38388;&#30340;&#24178;&#25200;&#65292;&#23548;&#33268;&#21512;&#24182;&#22810;&#20010;&#27169;&#22411;&#26102;&#24615;&#33021;&#22823;&#24133;&#19979;&#38477;&#12290;&#26412;&#25991;&#35777;&#26126;&#65292;&#20808;&#21069;&#30340;&#21512;&#24182;&#25216;&#26415;&#30001;&#20110;&#20004;&#20010;&#20027;&#35201;&#24178;&#25200;&#26469;&#28304;&#32780;&#19981;&#24910;&#20002;&#22833;&#26377;&#20215;&#20540;&#30340;&#20449;&#24687;&#65306;(a)&#20887;&#20313;&#21442;&#25968;&#20540;&#24341;&#36215;&#30340;&#24178;&#25200;&#21644;(b)&#34920;&#31034;&#21516;&#19968;&#21442;&#25968;&#20540;&#30340;&#31526;&#21495;&#22312;&#19981;&#21516;&#27169;&#22411;&#20013;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transfer learning - i.e., further fine-tuning a pre-trained model on a downstream task - can confer significant advantages, including improved downstream performance, faster convergence, and better sample efficiency. These advantages have led to a proliferation of task-specific fine-tuned models, which typically can only perform a single task and do not benefit from one another. Recently, model merging techniques have emerged as a solution to combine multiple task-specific models into a single multitask model without performing additional training. However, existing merging methods often ignore the interference between parameters of different models, resulting in large performance drops when merging multiple models. In this paper, we demonstrate that prior merging techniques inadvertently lose valuable information due to two major sources of interference: (a) interference due to redundant parameter values and (b) disagreement on the sign of a given parameter's values across models. To 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#29983;&#25104;&#24314;&#27169;&#39118;&#26684;&#21270;&#25216;&#26415;&#22312;&#39046;&#22495;&#33258;&#36866;&#24212;&#22238;&#24402;&#20219;&#21153;&#20013;&#26159;&#21542;&#24517;&#35201;&#65292;&#21457;&#29616;&#19982;&#20998;&#31867;&#20219;&#21153;&#30456;&#27604;&#65292;&#20854;&#23545;&#20110;&#22238;&#24402;&#20219;&#21153;&#30340;&#24433;&#21709;&#36739;&#23567;&#12290;</title><link>http://arxiv.org/abs/2306.01706</link><description>&lt;p&gt;
&#22522;&#20110;&#29983;&#25104;&#24314;&#27169;&#39118;&#26684;&#21270;&#25216;&#26415;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#22238;&#24402;&#20219;&#21153;&#20013;&#26159;&#21542;&#24517;&#35201;&#65311;
&lt;/p&gt;
&lt;p&gt;
Is Generative Modeling-based Stylization Necessary for Domain Adaptation in Regression Tasks?. (arXiv:2306.01706v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01706
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#29983;&#25104;&#24314;&#27169;&#39118;&#26684;&#21270;&#25216;&#26415;&#22312;&#39046;&#22495;&#33258;&#36866;&#24212;&#22238;&#24402;&#20219;&#21153;&#20013;&#26159;&#21542;&#24517;&#35201;&#65292;&#21457;&#29616;&#19982;&#20998;&#31867;&#20219;&#21153;&#30456;&#27604;&#65292;&#20854;&#23545;&#20110;&#22238;&#24402;&#20219;&#21153;&#30340;&#24433;&#21709;&#36739;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#26088;&#22312;&#22312;&#27809;&#26377;&#30446;&#26631;&#22495;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#20351;&#29992;&#20004;&#31181;&#20027;&#35201;&#25216;&#26415;&#65306;&#36755;&#20837;&#32423;&#21035;&#23545;&#40784;&#65288;&#22914;&#29983;&#25104;&#24314;&#27169;&#21644;&#39118;&#26684;&#21270;&#65289;&#21644;&#29305;&#24449;&#32423;&#21035;&#23545;&#40784;&#65288;&#21305;&#37197;&#29305;&#24449;&#26144;&#23556;&#30340;&#20998;&#24067;&#65292;&#20363;&#22914;&#26799;&#24230;&#21453;&#36716;&#23618;&#65289;&#26469;&#24357;&#21512;&#28304;&#22495;&#21644;&#30446;&#26631;&#22495;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#36755;&#20837;&#32423;&#21035;&#23545;&#40784;&#23545;&#20110;&#39046;&#22495;&#33258;&#36866;&#24212;&#22238;&#24402;&#20219;&#21153;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unsupervised domain adaptation (UDA) aims to bridge the gap between source and target domains in the absence of target domain labels using two main techniques: input-level alignment (such as generative modeling and stylization) and feature-level alignment (which matches the distribution of the feature maps, e.g. gradient reversal layers). Motivated from the success of generative modeling for image classification, stylization-based methods were recently proposed for regression tasks, such as pose estimation. However, use of input-level alignment via generative modeling and stylization incur additional overhead and computational complexity which limit their use in real-world DA tasks. To investigate the role of input-level alignment for DA, we ask the following question: Is generative modeling-based stylization necessary for visual domain adaptation in regression? Surprisingly, we find that input-alignment has little effect on regression tasks as compared to classification. Based on thes
&lt;/p&gt;</description></item><item><title>Transformer&#20351;&#29992;&#31264;&#23494;&#30340;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#20351;&#24471;&#22312;&#28145;&#24230;&#32593;&#32476;&#20013;&#21487;&#33021;&#30340;&#36830;&#25509;&#27169;&#24335;&#25968;&#37327;&#21576;&#25351;&#25968;&#32423;&#22686;&#38271;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20449;&#24687;&#36890;&#36335;&#20551;&#35828;&#65292;&#21033;&#29992;&#36825;&#31181;&#29305;&#24615;&#23558;&#36830;&#25509;&#27169;&#24335;&#20998;&#35299;&#20026;&#30456;&#20114;&#29420;&#31435;&#30340;&#20449;&#24687;&#36890;&#36335;&#65292;&#20943;&#23569;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#20869;&#23384;&#21644;&#35745;&#31639;&#25104;&#26412;&#20197;&#21450;&#25552;&#39640;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.01705</link><description>&lt;p&gt;
&#20449;&#24687;&#36890;&#36335;&#20551;&#35828;&#65306;Transformer&#26159;&#21160;&#24577;&#33258;&#32452;&#32455;
&lt;/p&gt;
&lt;p&gt;
The Information Pathways Hypothesis: Transformers are Dynamic Self-Ensembles. (arXiv:2306.01705v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01705
&lt;/p&gt;
&lt;p&gt;
Transformer&#20351;&#29992;&#31264;&#23494;&#30340;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#20351;&#24471;&#22312;&#28145;&#24230;&#32593;&#32476;&#20013;&#21487;&#33021;&#30340;&#36830;&#25509;&#27169;&#24335;&#25968;&#37327;&#21576;&#25351;&#25968;&#32423;&#22686;&#38271;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20449;&#24687;&#36890;&#36335;&#20551;&#35828;&#65292;&#21033;&#29992;&#36825;&#31181;&#29305;&#24615;&#23558;&#36830;&#25509;&#27169;&#24335;&#20998;&#35299;&#20026;&#30456;&#20114;&#29420;&#31435;&#30340;&#20449;&#24687;&#36890;&#36335;&#65292;&#20943;&#23569;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#20869;&#23384;&#21644;&#35745;&#31639;&#25104;&#26412;&#20197;&#21450;&#25552;&#39640;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#27169;&#22411;&#20351;&#29992;&#20102;&#31264;&#23494;&#30340;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#36171;&#20104;&#20102;&#23427;&#22312;&#36828;&#36317;&#31163;&#36830;&#25509;&#26041;&#38754;&#26356;&#39640;&#30340;&#28789;&#27963;&#24615;&#12290;&#22312;&#28145;&#24230;Transformer&#30340;&#22810;&#20010;&#23618;&#20013;&#65292;&#21487;&#33021;&#30340;&#36830;&#25509;&#27169;&#24335;&#25968;&#37327;&#21576;&#25351;&#25968;&#32423;&#22686;&#38271;&#12290;&#28982;&#32780;&#65292;&#20854;&#20013;&#24456;&#23569;&#26377;&#36830;&#25509;&#27169;&#24335;&#23545;&#27169;&#22411;&#24615;&#33021;&#26377;&#25152;&#36129;&#29486;&#65292;&#20854;&#20013;&#26377;&#29992;&#30340;&#26356;&#23569;&#12290;&#25105;&#20204;&#25552;&#20986;&#20551;&#35774;&#65292;&#22312;&#19968;&#20010;Transformer&#20013;&#23384;&#22312;&#30528;&#21517;&#20026;&#20449;&#24687;&#36890;&#36335;&#30340;&#31232;&#30095;&#23376;&#32593;&#32476;&#65292;&#21487;&#20197;&#29420;&#31435;&#22320;&#36827;&#34892;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#36890;&#36335;&#30340;&#21160;&#24577;&#65288;&#21363;&#20381;&#36182;&#20110;&#36755;&#20837;&#65289;&#29305;&#24615;&#20351;&#24471;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#24456;&#38590;&#21098;&#26525;&#31264;&#23494;&#30340;&#33258;&#27880;&#24847;&#21147;&#12290;&#20294;&#26159;&#65292;&#36825;&#20123;&#36890;&#36335;&#30340;&#25972;&#20307;&#20998;&#24067;&#24448;&#24448;&#26159;&#21487;&#20197;&#39044;&#27979;&#30340;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#19968;&#20107;&#23454;&#25552;&#20986;&#20102;&#38543;&#26426;&#23376;&#37319;&#26679;&#33258;&#27880;&#24847;&#21147;&#65288;SSA&#65289;&#8212;&#8212;&#19968;&#31181;&#29992;&#20110;Transformer&#30340;&#36890;&#29992;&#35757;&#32451;&#31574;&#30053;&#65292;&#21487;&#20197;&#22312;&#35757;&#32451;&#26399;&#38388;&#23558;&#33258;&#27880;&#24847;&#21147;&#30340;&#20869;&#23384;&#21644;&#35745;&#31639;&#25104;&#26412;&#20943;&#23569;4&#21040;8&#20493;&#65292;&#21516;&#26102;&#36824;&#21487;&#20197;&#20316;&#20026;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#25552;&#39640;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformers use the dense self-attention mechanism which gives a lot of flexibility for long-range connectivity. Over multiple layers of a deep transformer, the number of possible connectivity patterns increases exponentially. However, very few of these contribute to the performance of the network, and even fewer are essential. We hypothesize that there are sparsely connected sub-networks within a transformer, called information pathways which can be trained independently. However, the dynamic (i.e., input-dependent) nature of these pathways makes it difficult to prune dense self-attention during training. But the overall distribution of these pathways is often predictable. We take advantage of this fact to propose Stochastically Subsampled self-Attention (SSA) - a general-purpose training strategy for transformers that can reduce both the memory and computational cost of self-attention by 4 to 8 times during training while also serving as a regularization method - improving generaliz
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20146;&#21644;&#24615;&#32858;&#31867;&#30340;&#25968;&#25454;&#25193;&#20805;&#26041;&#27861;MASC&#65292;&#36890;&#36807;&#21516;&#31867;&#25968;&#25454;&#38598;&#20013;&#30340;&#20146;&#21644;&#32858;&#31867;&#21644;&#20445;&#25252;&#25968;&#25454;&#30340;&#20849;&#20139;&#36798;&#21040;&#25968;&#25454;&#38598;&#30340;&#24179;&#34913;&#65292;&#20174;&#32780;&#35299;&#20915;&#25968;&#25454;&#38598;&#20195;&#34920;&#24615;&#20559;&#35265;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.01699</link><description>&lt;p&gt;
&#22522;&#20110;&#25104;&#23545;&#20998;&#24067;&#24046;&#24322;&#30340;&#25968;&#25454;&#21435;&#20559;&#35265;&#20146;&#21644;&#24615;&#32858;&#31867;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Affinity Clustering Framework for Data Debiasing Using Pairwise Distribution Discrepancy. (arXiv:2306.01699v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01699
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20146;&#21644;&#24615;&#32858;&#31867;&#30340;&#25968;&#25454;&#25193;&#20805;&#26041;&#27861;MASC&#65292;&#36890;&#36807;&#21516;&#31867;&#25968;&#25454;&#38598;&#20013;&#30340;&#20146;&#21644;&#32858;&#31867;&#21644;&#20445;&#25252;&#25968;&#25454;&#30340;&#20849;&#20139;&#36798;&#21040;&#25968;&#25454;&#38598;&#30340;&#24179;&#34913;&#65292;&#20174;&#32780;&#35299;&#20915;&#25968;&#25454;&#38598;&#20195;&#34920;&#24615;&#20559;&#35265;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#37319;&#38598;&#26041;&#27861;&#19981;&#36275;&#25110;&#19981;&#20855;&#20195;&#34920;&#24615;&#24120;&#23548;&#33268;&#36523;&#20221;&#32452;&#19981;&#24179;&#34913;&#65292;&#24418;&#25104;&#25968;&#25454;&#38598;&#20195;&#34920;&#24615;&#20559;&#35265;&#12290;&#36825;&#31181;&#20559;&#35265;&#21487;&#33021;&#23384;&#22312;&#20110;&#19968;&#20010;&#25110;&#22810;&#20010;&#21463;&#20445;&#25252;&#23646;&#24615;&#30340;&#19981;&#21516;&#32452;&#20043;&#38388;&#65292;&#24182;&#21487;&#33021;&#23548;&#33268;&#23545;&#26576;&#20123;&#20154;&#32676;&#30340;&#20559;&#35265;&#21644;&#27495;&#35270;&#24615;&#32467;&#26524;&#12290;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#25193;&#20805;&#26041;&#27861;MASC&#65292;&#21033;&#29992;&#20146;&#21644;&#24615;&#32858;&#31867;&#24179;&#34913;&#30446;&#26631;&#25968;&#25454;&#38598;&#30340;&#38750;&#20445;&#25252;&#32452;&#21644;&#20445;&#25252;&#32452;&#34920;&#24449;&#12290;&#36890;&#36807;&#23558;&#21516;&#19968;&#20445;&#25252;&#23646;&#24615;&#30340;&#23454;&#20363;&#20174;&#30456;&#20284;&#25968;&#25454;&#38598;&#20013;&#36827;&#34892;&#32858;&#31867;&#65292;&#20849;&#20139;&#26469;&#33258;&#21463;&#20445;&#25252;&#23646;&#24615;&#30340;&#23454;&#20363;&#12290;&#35813;&#26041;&#27861;&#21253;&#25324;&#36890;&#36807;&#37327;&#21270;&#25968;&#25454;&#38598;&#38388;&#30340;&#20998;&#24067;&#24046;&#24322;&#26500;&#24314;&#20146;&#21644;&#30697;&#38453;&#65292;&#24182;&#23558;&#20854;&#36716;&#25442;&#20026;&#23545;&#31216;&#25104;&#23545;&#30456;&#20284;&#24615;&#30697;&#38453;&#12290;&#20351;&#29992;&#38750;&#21442;&#25968;&#30340;&#35889;&#32858;&#31867;&#31639;&#27861;&#23545;&#30446;&#26631;&#25968;&#25454;&#38598;&#36827;&#34892;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
Group imbalance, resulting from inadequate or unrepresentative data collection methods, is a primary cause of representation bias in datasets. Representation bias can exist with respect to different groups of one or more protected attributes and might lead to prejudicial and discriminatory outcomes toward certain groups of individuals; in cases where a learning model is trained on such biased data. This paper presents MASC, a data augmentation approach that leverages affinity clustering to balance the representation of non-protected and protected groups of a target dataset by utilizing instances of the same protected attributes from similar datasets that are categorized in the same cluster as the target dataset by sharing instances of the protected attribute. The proposed method involves constructing an affinity matrix by quantifying distribution discrepancies between dataset pairs and transforming them into a symmetric pairwise similarity matrix. A non-parametric spectral clustering i
&lt;/p&gt;</description></item><item><title>GateON&#26159;&#19968;&#31181;&#29992;&#20110;&#22823;&#35268;&#27169;&#36830;&#32493;&#23398;&#20064;&#30340;&#26080;&#30417;&#30563;&#26041;&#27861;&#65292;&#36890;&#36807;&#21487;&#23398;&#20064;&#30340;&#27963;&#21160;&#38376;&#25511;&#21644;&#21442;&#25968;&#30456;&#20851;&#24615;&#30340;&#22312;&#32447;&#20272;&#35745;&#26469;&#38450;&#27490;&#37325;&#35201;&#30693;&#35782;&#34987;&#35206;&#30422;&#65292;&#21516;&#26102;&#36890;&#36807;&#23450;&#28857;&#31070;&#32463;&#20803;&#30340;&#37325;&#26032;&#28608;&#27963;&#26426;&#21046;&#35299;&#20915;&#20102;&#32593;&#32476;&#39281;&#21644;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.01690</link><description>&lt;p&gt;
GateON: &#19968;&#31181;&#29992;&#20110;&#22823;&#35268;&#27169;&#36830;&#32493;&#23398;&#20064;&#30340;&#26080;&#30417;&#30563;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
GateON: an unsupervised method for large scale continual learning. (arXiv:2306.01690v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01690
&lt;/p&gt;
&lt;p&gt;
GateON&#26159;&#19968;&#31181;&#29992;&#20110;&#22823;&#35268;&#27169;&#36830;&#32493;&#23398;&#20064;&#30340;&#26080;&#30417;&#30563;&#26041;&#27861;&#65292;&#36890;&#36807;&#21487;&#23398;&#20064;&#30340;&#27963;&#21160;&#38376;&#25511;&#21644;&#21442;&#25968;&#30456;&#20851;&#24615;&#30340;&#22312;&#32447;&#20272;&#35745;&#26469;&#38450;&#27490;&#37325;&#35201;&#30693;&#35782;&#34987;&#35206;&#30422;&#65292;&#21516;&#26102;&#36890;&#36807;&#23450;&#28857;&#31070;&#32463;&#20803;&#30340;&#37325;&#26032;&#28608;&#27963;&#26426;&#21046;&#35299;&#20915;&#20102;&#32593;&#32476;&#39281;&#21644;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36830;&#32493;&#23398;&#20064;&#65288;CL&#65289;&#30340;&#30446;&#26631;&#26159;&#22312;&#19981;&#23545;&#26089;&#26399;&#20219;&#21153;&#36827;&#34892;&#37325;&#26032;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#25353;&#39034;&#24207;&#23398;&#20064;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#31070;&#32463;&#32593;&#32476;&#22312;&#32463;&#36807;CL&#35757;&#32451;&#21518;&#20250;&#20986;&#29616;&#28798;&#38590;&#24615;&#36951;&#24536;&#21644;&#26377;&#38480;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;'Gate and Obstruct Network'&#65288;GateON&#65289;&#12290;GateON&#23558;&#21487;&#23398;&#20064;&#30340;&#27963;&#21160;&#38376;&#25511;&#19982;&#21442;&#25968;&#30456;&#20851;&#24615;&#30340;&#22312;&#32447;&#20272;&#35745;&#30456;&#32467;&#21512;&#65292;&#20197;&#38450;&#27490;&#37325;&#35201;&#30693;&#35782;&#34987;&#35206;&#30422;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20219;&#21153;&#20043;&#38388;&#29983;&#25104;&#37096;&#20998;&#37325;&#21472;&#30340;&#36335;&#24452;&#65292;&#20801;&#35768;&#22312;&#39034;&#24207;&#23398;&#20064;&#36807;&#31243;&#20013;&#36827;&#34892;&#27491;&#21521;&#21644;&#21453;&#21521;&#36716;&#31227;&#12290;GateON&#36890;&#36807;&#23450;&#28857;&#31070;&#32463;&#20803;&#30340;&#37325;&#26032;&#28608;&#27963;&#26426;&#21046;&#26469;&#35299;&#20915;&#21442;&#25968;&#22266;&#23450;&#21518;&#32593;&#32476;&#39281;&#21644;&#30340;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#22823;&#35268;&#27169;&#36830;&#32493;&#23398;&#20064;&#12290;GateON&#36866;&#29992;&#20110;&#21508;&#31181;&#32593;&#32476;&#65288;&#20840;&#36830;&#25509;&#12289;CNN&#12289;Transformers&#65289;&#65292;&#35745;&#31639;&#22797;&#26434;&#24230;&#20302;&#65292;&#26377;&#25928;&#22320;&#23398;&#20064;&#20102;&#39640;&#36798;100&#20010;MNIST&#23398;&#20064;&#20219;&#21153;&#65292;&#24182;&#22312;&#39044;&#35757;&#32451;BERT&#20013;&#21462;&#24471;&#20102;&#39030;&#23574;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The objective of continual learning (CL) is to learn tasks sequentially without retraining on earlier tasks. However, when subjected to CL, traditional neural networks exhibit catastrophic forgetting and limited generalization. To overcome these problems, we introduce a novel method called 'Gate and Obstruct Network' (GateON). GateON combines learnable gating of activity and online estimation of parameter relevance to safeguard crucial knowledge from being overwritten. Our method generates partially overlapping pathways between tasks which permits forward and backward transfer during sequential learning. GateON addresses the issue of network saturation after parameter fixation by a re-activation mechanism of fixed neurons, enabling large-scale continual learning. GateON is implemented on a wide range of networks (fully-connected, CNN, Transformers), has low computational complexity, effectively learns up to 100 MNIST learning tasks, and achieves top-tier results for pre-trained BERT in
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#22312;&#38761;&#26032;&#21307;&#23398;&#35786;&#26029;&#27169;&#22411;&#30340;&#21019;&#26032;&#26041;&#27861;&#21644;&#26041;&#27861;&#35770;&#65292;&#24182;&#36890;&#36807;&#25581;&#31034;&#28508;&#22312;&#30340;&#20915;&#31574;&#36807;&#31243;&#26469;&#36171;&#20104;&#21307;&#30103;&#20445;&#20581;&#19987;&#19994;&#20154;&#21592;&#29702;&#35299;&#21644;&#20449;&#20219;&#36825;&#20123;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#36825;&#20123;&#26032;&#25216;&#26415;&#28508;&#21147;&#24040;&#22823;&#65292;&#26377;&#26395;&#25913;&#21464;&#21307;&#30103;&#20445;&#20581;&#30340;&#26684;&#23616;&#65292;&#20174;&#32780;&#25552;&#39640;&#24739;&#32773;&#30340;&#32467;&#26524;&#24182;&#24314;&#31435;&#23545;AI&#39537;&#21160;&#35786;&#26029;&#31995;&#32479;&#30340;&#20449;&#20219;&#12290;</title><link>http://arxiv.org/abs/2306.01668</link><description>&lt;p&gt;
XAI&#25991;&#33402;&#22797;&#20852;&#65306;&#37325;&#26032;&#23450;&#20041;&#21307;&#23398;&#35786;&#26029;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
XAI Renaissance: Redefining Interpretability in Medical Diagnostic Models. (arXiv:2306.01668v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01668
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#22312;&#38761;&#26032;&#21307;&#23398;&#35786;&#26029;&#27169;&#22411;&#30340;&#21019;&#26032;&#26041;&#27861;&#21644;&#26041;&#27861;&#35770;&#65292;&#24182;&#36890;&#36807;&#25581;&#31034;&#28508;&#22312;&#30340;&#20915;&#31574;&#36807;&#31243;&#26469;&#36171;&#20104;&#21307;&#30103;&#20445;&#20581;&#19987;&#19994;&#20154;&#21592;&#29702;&#35299;&#21644;&#20449;&#20219;&#36825;&#20123;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#36825;&#20123;&#26032;&#25216;&#26415;&#28508;&#21147;&#24040;&#22823;&#65292;&#26377;&#26395;&#25913;&#21464;&#21307;&#30103;&#20445;&#20581;&#30340;&#26684;&#23616;&#65292;&#20174;&#32780;&#25552;&#39640;&#24739;&#32773;&#30340;&#32467;&#26524;&#24182;&#24314;&#31435;&#23545;AI&#39537;&#21160;&#35786;&#26029;&#31995;&#32479;&#30340;&#20449;&#20219;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36234;&#26469;&#36234;&#22810;&#22320;&#24212;&#29992;&#20110;&#21307;&#23398;&#35786;&#26029;&#65292;&#21487;&#35299;&#37322;&#24615;&#21644;&#36879;&#26126;&#24230;&#30340;&#38656;&#27714;&#21464;&#24471;&#23588;&#20026;&#37325;&#35201;&#12290;XAI&#25991;&#33402;&#22797;&#20852;&#26631;&#24535;&#30528;&#35813;&#39046;&#22495;&#21457;&#29983;&#20102;&#37325;&#22823;&#36716;&#21464;&#65292;&#26088;&#22312;&#37325;&#26032;&#23450;&#20041;&#21307;&#23398;&#35786;&#26029;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#39046;&#22495;&#20869;&#27491;&#22312;&#38761;&#26032;&#21307;&#23398;&#35786;&#26029;&#27169;&#22411;&#30340;&#21019;&#26032;&#26041;&#27861;&#21644;&#26041;&#27861;&#35770;&#12290;&#36890;&#36807;&#25581;&#31034;&#28508;&#22312;&#30340;&#20915;&#31574;&#36807;&#31243;&#65292;XAI&#25216;&#26415;&#36171;&#20104;&#21307;&#30103;&#20445;&#20581;&#19987;&#19994;&#20154;&#21592;&#29702;&#35299;&#12289;&#20449;&#20219;&#24182;&#26377;&#25928;&#21033;&#29992;&#36825;&#20123;&#27169;&#22411;&#36827;&#34892;&#20934;&#30830;&#21487;&#38752;&#30340;&#21307;&#23398;&#35786;&#26029;&#12290;&#26412;&#25991;&#37325;&#28857;&#20171;&#32461;&#20102;XAI&#22312;&#21307;&#23398;&#35786;&#26029;&#26041;&#38754;&#30340;&#20851;&#38190;&#36827;&#23637;&#21450;&#20854;&#25913;&#21464;&#21307;&#30103;&#20445;&#20581;&#26684;&#23616;&#30340;&#28508;&#21147;&#65292;&#20174;&#32780;&#25913;&#21892;&#24739;&#32773;&#32467;&#26524;&#24182;&#24314;&#31435;&#23545;AI&#39537;&#21160;&#35786;&#26029;&#31995;&#32479;&#30340;&#20449;&#20219;&#12290;
&lt;/p&gt;
&lt;p&gt;
As machine learning models become increasingly prevalent in medical diagnostics, the need for interpretability and transparency becomes paramount. The XAI Renaissance signifies a significant shift in the field, aiming to redefine the interpretability of medical diagnostic models. This paper explores the innovative approaches and methodologies within the realm of Explainable AI (XAI) that are revolutionizing the interpretability of medical diagnostic models. By shedding light on the underlying decision-making process, XAI techniques empower healthcare professionals to understand, trust, and effectively utilize these models for accurate and reliable medical diagnoses. This review highlights the key advancements in XAI for medical diagnostics and their potential to transform the healthcare landscape, ultimately improving patient outcomes and fostering trust in AI-driven diagnostic systems.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#25968;&#25454;&#27969;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26234;&#33021;&#21512;&#32422;&#30340;&#28304;&#20195;&#30721;&#29305;&#24449;&#65292;&#23454;&#29616;&#26816;&#27979;&#20197;&#22826;&#22346;&#19978;&#30340;&#26234;&#33021;&#24222;&#20857;&#39575;&#23616;&#12290;&#35813;&#26041;&#27861;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#38477;&#20302;&#20102;&#25968;&#25454;&#33719;&#21462;&#21644;&#29305;&#24449;&#25552;&#21462;&#30340;&#38590;&#24230;&#12290;</title><link>http://arxiv.org/abs/2306.01665</link><description>&lt;p&gt;
SourceP&#65306;&#20351;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#25968;&#25454;&#27969;&#26234;&#33021;&#26816;&#27979;&#20197;&#22826;&#22346;&#19978;&#30340;&#26234;&#33021;&#24222;&#20857;&#39575;&#23616;
&lt;/p&gt;
&lt;p&gt;
SourceP: Smart Ponzi Schemes Detection on Ethereum Using Pre-training Model with Data Flow. (arXiv:2306.01665v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01665
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#25968;&#25454;&#27969;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26234;&#33021;&#21512;&#32422;&#30340;&#28304;&#20195;&#30721;&#29305;&#24449;&#65292;&#23454;&#29616;&#26816;&#27979;&#20197;&#22826;&#22346;&#19978;&#30340;&#26234;&#33021;&#24222;&#20857;&#39575;&#23616;&#12290;&#35813;&#26041;&#27861;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#38477;&#20302;&#20102;&#25968;&#25454;&#33719;&#21462;&#21644;&#29305;&#24449;&#25552;&#21462;&#30340;&#38590;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#21306;&#22359;&#38142;&#25216;&#26415;&#36234;&#26469;&#36234;&#27969;&#34892;&#65292;&#20856;&#22411;&#30340;&#37329;&#34701;&#39575;&#23616;&#24222;&#20857;&#39575;&#23616;&#20063;&#22312;&#21306;&#22359;&#38142;&#24179;&#21488;&#20197;&#22826;&#22346;&#19978;&#20986;&#29616;&#12290;&#36890;&#36807;&#26234;&#33021;&#21512;&#32422;&#37096;&#32626;&#30340;&#36825;&#31181;&#24222;&#20857;&#39575;&#23616;&#65292;&#20063;&#31216;&#20026;&#26234;&#33021;&#24222;&#20857;&#39575;&#23616;&#65292;&#24050;&#32463;&#36896;&#25104;&#20102;&#22823;&#37327;&#30340;&#32463;&#27982;&#25439;&#22833;&#21644;&#36127;&#38754;&#24433;&#21709;&#12290;&#29616;&#26377;&#30340;&#20197;&#22826;&#22346;&#26234;&#33021;&#24222;&#20857;&#39575;&#23616;&#26816;&#27979;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#26234;&#33021;&#21512;&#32422;&#30340;&#23383;&#33410;&#30721;&#29305;&#24449;&#12289;&#25805;&#20316;&#30721;&#29305;&#24449;&#12289;&#36134;&#25143;&#29305;&#24449;&#21644;&#20132;&#26131;&#34892;&#20026;&#29305;&#24449;&#65292;&#36825;&#20123;&#26041;&#27861;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#21644;&#21487;&#25345;&#32493;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;SourceP&#65292;&#19968;&#31181;&#20351;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#25968;&#25454;&#27969;&#22312;&#20197;&#22826;&#22346;&#24179;&#21488;&#19978;&#26816;&#27979;&#26234;&#33021;&#24222;&#20857;&#39575;&#23616;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#21482;&#38656;&#35201;&#21033;&#29992;&#26234;&#33021;&#21512;&#32422;&#30340;&#28304;&#20195;&#30721;&#20316;&#20026;&#29305;&#24449;&#65292;&#20174;&#21478;&#19968;&#20010;&#35282;&#24230;&#25506;&#32034;&#26816;&#27979;&#26234;&#33021;&#24222;&#20857;&#39575;&#23616;&#30340;&#21487;&#33021;&#24615;&#12290;SourceP&#38477;&#20302;&#20102;&#29616;&#26377;&#26816;&#27979;&#26041;&#27861;&#30340;&#25968;&#25454;&#33719;&#21462;&#21644;&#29305;&#24449;&#25552;&#21462;&#38590;&#24230;&#65292;&#21516;&#26102;&#22686;&#21152;&#20102;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
As blockchain technology becomes more and more popular, a typical financial scam, the Ponzi scheme, has also emerged in the blockchain platform Ethereum. This Ponzi scheme deployed through smart contracts, also known as the smart Ponzi scheme, has caused a lot of economic losses and negative impacts. Existing methods for detecting smart Ponzi schemes on Ethereum mainly rely on bytecode features, opcode features, account features, and transaction behavior features of smart contracts, and such methods lack interpretability and sustainability. In this paper, we propose SourceP, a method to detect smart Ponzi schemes on the Ethereum platform using pre-training models and data flow, which only requires using the source code of smart contracts as features to explore the possibility of detecting smart Ponzi schemes from another direction. SourceP reduces the difficulty of data acquisition and feature extraction of existing detection methods while increasing the interpretability of the model. 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DiffusEmp&#30340;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#22522;&#20110;&#26465;&#20214;&#25193;&#25955;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#22810;&#32423;&#25511;&#21046;&#20449;&#21495;&#26469;&#29983;&#25104;&#26356;&#21152;&#32454;&#33268;&#21644;&#20010;&#24615;&#21270;&#30340;&#20849;&#24773;&#22238;&#24212;&#12290;</title><link>http://arxiv.org/abs/2306.01657</link><description>&lt;p&gt;
DiffusEmp:&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#22810;&#32423;&#25511;&#21046;&#20849;&#24773;&#22238;&#24212;&#29983;&#25104;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
DiffusEmp: A Diffusion Model-Based Framework with Multi-Grained Control for Empathetic Response Generation. (arXiv:2306.01657v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01657
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DiffusEmp&#30340;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#22522;&#20110;&#26465;&#20214;&#25193;&#25955;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#22810;&#32423;&#25511;&#21046;&#20449;&#21495;&#26469;&#29983;&#25104;&#26356;&#21152;&#32454;&#33268;&#21644;&#20010;&#24615;&#21270;&#30340;&#20849;&#24773;&#22238;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20849;&#24773;&#26159;&#24320;&#25918;&#24335;&#20132;&#27969;&#20013;&#33267;&#20851;&#37325;&#35201;&#30340;&#22240;&#32032;&#65292;&#23427;&#33258;&#28982;&#22320;&#23637;&#31034;&#20102;&#19968;&#20010;&#20154;&#23545;&#20182;&#20154;&#30340;&#20851;&#24515;&#21644;&#29702;&#35299;&#12290;&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#20102;&#20960;&#31181;&#29983;&#25104;&#20849;&#24773;&#21709;&#24212;&#30340;&#26041;&#27861;&#65292;&#20294;&#29616;&#26377;&#30340;&#20316;&#21697;&#24448;&#24448;&#23548;&#33268;&#21333;&#35843;&#30340;&#20849;&#24773;&#65292;&#21363;&#25351;&#36890;&#29992;&#21644;&#23433;&#20840;&#30340;&#34920;&#36798;&#26041;&#24335;&#12290;&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#26174;&#24335;&#25511;&#21046;&#26469;&#24341;&#23548;&#20849;&#24773;&#34920;&#36798;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;DiffusEmp&#26694;&#26550;&#65292;&#22522;&#20110;&#26465;&#20214;&#25193;&#25955;&#35821;&#35328;&#27169;&#22411;&#65292;&#32479;&#19968;&#20102;&#23545;&#35805;&#19978;&#19979;&#25991;&#21644;&#23646;&#24615;&#23548;&#21521;&#25511;&#21046;&#20449;&#21495;&#30340;&#21033;&#29992;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#24341;&#20837;&#36890;&#20449;&#26426;&#21046;&#12289;&#24847;&#22270;&#21644;&#35821;&#20041;&#26694;&#26550;&#20316;&#20026;&#22810;&#23618;&#27425;&#20449;&#21495;&#65292;&#21487;&#20174;&#31895;&#31961;&#21040;&#32454;&#33268;&#22320;&#25511;&#21046;&#20849;&#24773;&#23454;&#29616;&#12290;&#28982;&#21518;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#29305;&#23450;&#30340;&#23631;&#34109;&#31574;&#30053;&#65292;&#20197;&#21453;&#26144;&#22810;&#23618;&#27425;&#20449;&#21495;&#19982;&#21709;&#24212;&#20196;&#29260;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#23558;&#20854;&#25972;&#21512;&#21040;&#25193;&#25955;&#27169;&#22411;&#20013;&#20197;&#24433;&#21709;&#29983;&#25104;&#36807;&#31243;&#12290;&#23454;&#39564;&#32467;&#26524;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;EmpatheticDialogue&#19978;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#20248;&#20110;&#20854;&#20182;&#20849;&#24773;&#22238;&#24212;&#29983;&#25104;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Empathy is a crucial factor in open-domain conversations, which naturally shows one's caring and understanding to others. Though several methods have been proposed to generate empathetic responses, existing works often lead to monotonous empathy that refers to generic and safe expressions. In this paper, we propose to use explicit control to guide the empathy expression and design a framework DiffusEmp based on conditional diffusion language model to unify the utilization of dialogue context and attribute-oriented control signals. Specifically, communication mechanism, intent, and semantic frame are imported as multi-grained signals that control the empathy realization from coarse to fine levels. We then design a specific masking strategy to reflect the relationship between multi-grained signals and response tokens, and integrate it into the diffusion model to influence the generative process. Experimental results on a benchmark dataset EmpatheticDialogue show that our framework outper
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#22312;&#20998;&#23376;&#32467;&#26500;&#21644;&#29983;&#29289;&#21307;&#23398;&#30693;&#35782;&#22270;&#35889;&#20013;&#38598;&#25104;&#22810;&#20010;&#39046;&#22495;&#20449;&#24687;&#65292;&#36890;&#36807;&#33258;&#25105;&#30417;&#30563;&#31574;&#30053;&#39044;&#20808;&#35757;&#32451;&#26356;&#24191;&#27867;&#21644;&#26356;&#24378;&#22823;&#30340;&#34920;&#31034;&#65292;&#24182;&#22312;&#21270;&#23398;&#23646;&#24615;&#39044;&#27979;&#20219;&#21153;&#19978;&#23637;&#31034;&#20986;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.01631</link><description>&lt;p&gt;
Gode -- &#23558;&#29983;&#29289;&#21270;&#23398;&#30693;&#35782;&#22270;&#35889;&#38598;&#25104;&#21040;&#20998;&#23376;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#39044;&#35757;&#32451;&#20013;
&lt;/p&gt;
&lt;p&gt;
Gode -- Integrating Biochemical Knowledge Graph into Pre-training Molecule Graph Neural Network. (arXiv:2306.01631v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01631
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#22312;&#20998;&#23376;&#32467;&#26500;&#21644;&#29983;&#29289;&#21307;&#23398;&#30693;&#35782;&#22270;&#35889;&#20013;&#38598;&#25104;&#22810;&#20010;&#39046;&#22495;&#20449;&#24687;&#65292;&#36890;&#36807;&#33258;&#25105;&#30417;&#30563;&#31574;&#30053;&#39044;&#20808;&#35757;&#32451;&#26356;&#24191;&#27867;&#21644;&#26356;&#24378;&#22823;&#30340;&#34920;&#31034;&#65292;&#24182;&#22312;&#21270;&#23398;&#23646;&#24615;&#39044;&#27979;&#20219;&#21153;&#19978;&#23637;&#31034;&#20986;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#23376;&#23646;&#24615;&#30340;&#20934;&#30830;&#39044;&#27979;&#23545;&#20110;&#20419;&#36827;&#21019;&#26032;&#27835;&#30103;&#26041;&#27861;&#30340;&#21457;&#23637;&#21644;&#29702;&#35299;&#21270;&#23398;&#29289;&#36136;&#21644;&#29983;&#29289;&#31995;&#32479;&#20043;&#38388;&#22797;&#26434;&#30340;&#30456;&#20114;&#20316;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#23558;&#21333;&#20010;&#20998;&#23376;&#32467;&#26500;&#30340;&#22270;&#34920;&#31034;&#19982;&#29983;&#29289;&#21307;&#23398;&#30693;&#35782;&#22270;&#35889; (KG) &#30340;&#22810;&#20010;&#39046;&#22495;&#20449;&#24687;&#36827;&#34892;&#38598;&#25104;&#12290;&#36890;&#36807;&#38598;&#25104;&#20004;&#20010;&#32423;&#21035;&#30340;&#20449;&#24687;&#65292;&#25105;&#20204;&#21487;&#20197;&#20351;&#29992;&#33258;&#25105;&#30417;&#30563;&#31574;&#30053;&#39044;&#20808;&#35757;&#32451;&#26356;&#24191;&#27867;&#21644;&#26356;&#24378;&#22823;&#30340;&#34920;&#31034;&#65292;&#29992;&#20110;&#20998;&#23376;&#32423;&#21644; KG &#32423;&#39044;&#27979;&#20219;&#21153;&#12290;&#22312;&#24615;&#33021;&#35780;&#20272;&#26041;&#38754;&#65292;&#25105;&#20204;&#22312; 11 &#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#21270;&#23398;&#23646;&#24615;&#39044;&#27979;&#20219;&#21153;&#19978;&#24494;&#35843;&#25105;&#20204;&#39044;&#20808;&#35757;&#32451;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#24494;&#35843;&#30340;&#27169;&#22411;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
The precise prediction of molecular properties holds paramount importance in facilitating the development of innovative treatments and comprehending the intricate interplay between chemicals and biological systems. In this study, we propose a novel approach that integrates graph representations of individual molecular structures with multi-domain information from biomedical knowledge graphs (KGs). Integrating information from both levels, we can pre-train a more extensive and robust representation for both molecule-level and KG-level prediction tasks with our novel self-supervision strategy. For performance evaluation, we fine-tune our pre-trained model on 11 challenging chemical property prediction tasks. Results from our framework demonstrate our fine-tuned models outperform existing state-of-the-art models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#35270;&#35282;&#35270;&#39057;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#38544;&#24335;&#26144;&#23556;&#65292;&#24418;&#25104;&#20102;&#19968;&#20010;&#20855;&#26377;&#21516;&#21464;&#24615;&#30340;&#34920;&#31034;&#31354;&#38388;&#65292;&#33021;&#22815;&#23545;&#34892;&#21160;&#35782;&#21035;&#21644;&#34892;&#20154;&#24847;&#22270;&#39044;&#27979;&#20219;&#21153;&#33719;&#24471;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.01623</link><description>&lt;p&gt;
HomE: &#21516;&#21464;&#24615;&#21333;&#24212;&#24615;&#35270;&#39057;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
HomE: Homography-Equivariant Video Representation Learning. (arXiv:2306.01623v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01623
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#35270;&#35282;&#35270;&#39057;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#38544;&#24335;&#26144;&#23556;&#65292;&#24418;&#25104;&#20102;&#19968;&#20010;&#20855;&#26377;&#21516;&#21464;&#24615;&#30340;&#34920;&#31034;&#31354;&#38388;&#65292;&#33021;&#22815;&#23545;&#34892;&#21160;&#35782;&#21035;&#21644;&#34892;&#20154;&#24847;&#22270;&#39044;&#27979;&#20219;&#21153;&#33719;&#24471;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#30340;&#24555;&#36895;&#21457;&#23637;&#20351;&#24471;&#27169;&#22411;&#22312;&#19981;&#38656;&#35201;&#22823;&#37327;&#26631;&#27880;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#24471;&#21040;&#20102;&#26356;&#39640;&#25928;&#21644;&#26356;&#24378;&#20581;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22823;&#37096;&#20998;&#24037;&#20316;&#20173;&#28982;&#38598;&#20013;&#22312;&#22270;&#20687;&#19978;&#65292;&#24456;&#23569;&#26377;&#24037;&#20316;&#20851;&#27880;&#35270;&#39057;&#65292;&#29978;&#33267;&#26356;&#23569;&#30340;&#24037;&#20316;&#20851;&#27880;&#22810;&#35270;&#35282;&#35270;&#39057;&#65292;&#20854;&#20013;&#21487;&#20197;&#21033;&#29992;&#26356;&#24378;&#22823;&#30340;&#24402;&#32435;&#20559;&#24046;&#36827;&#34892;&#33258;&#30417;&#30563;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#35270;&#35282;&#35270;&#39057;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#20854;&#20013;&#25105;&#20204;&#26174;&#24335;&#22320;&#27169;&#25311;&#34920;&#31034;&#31354;&#38388;&#20197;&#20445;&#25345;&#21516;&#21464;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23398;&#20064;&#19981;&#21516;&#35270;&#35282;&#20043;&#38388;&#30340;&#38544;&#24335;&#26144;&#23556;&#65292;&#26368;&#32456;&#24418;&#25104;&#19968;&#20010;&#34920;&#31034;&#31354;&#38388;&#65292;&#20854;&#20013;&#20445;&#25345;&#30456;&#37051;&#35270;&#35282;&#20043;&#38388;&#30340;&#21333;&#24212;&#20851;&#31995;&#12290;&#25105;&#20204;&#36890;&#36807;&#34892;&#21160;&#35782;&#21035;&#21644;&#34892;&#20154;&#24847;&#22270;&#39044;&#27979;&#20316;&#20026;&#19979;&#28216;&#20219;&#21153;&#26469;&#35780;&#20272;&#25105;&#20204;&#30340;HomE&#34920;&#31034;&#12290;&#22312;UCF101&#25968;&#25454;&#38598;&#19978;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;3&#27425;&#20132;&#21449;&#39564;&#35777;&#30340;&#20934;&#30830;&#24230;&#36798;&#21040;&#20102;96.4&#65285;&#65292;&#20248;&#20110;&#22823;&#22810;&#25968;&#26368;&#20808;&#36827;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#12290;&#21516;&#26679;&#65292;&#22312;&#34892;&#20154;&#24847;&#22270;&#39044;&#27979;&#30340;STIP&#25968;&#25454;&#38598;&#19978;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20063;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in self-supervised representation learning have enabled more efficient and robust model performance without relying on extensive labeled data. However, most works are still focused on images, with few working on videos and even fewer on multi-view videos, where more powerful inductive biases can be leveraged for self-supervision. In this work, we propose a novel method for representation learning of multi-view videos, where we explicitly model the representation space to maintain Homography Equivariance (HomE). Our method learns an implicit mapping between different views, culminating in a representation space that maintains the homography relationship between neighboring views. We evaluate our HomE representation via action recognition and pedestrian intent prediction as downstream tasks. On action classification, our method obtains 96.4% 3-fold accuracy on the UCF101 dataset, better than most state-of-the-art self-supervised learning methods. Similarly, on the STIP da
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;ChatGPT&#22312;&#26085;&#24535;&#35299;&#26512;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#32467;&#26524;&#34920;&#26126;&#23427;&#21487;&#20197;&#36890;&#36807;&#36866;&#24403;&#30340;&#25552;&#31034;&#23454;&#29616;&#26377;&#21069;&#36884;&#30340;&#26085;&#24535;&#35299;&#26512;&#32467;&#26524;&#65292;&#29305;&#21035;&#26159;&#22312;few-shot&#25552;&#31034;&#19979;&#12290;</title><link>http://arxiv.org/abs/2306.01590</link><description>&lt;p&gt;
&#21033;&#29992;ChatGPT&#36827;&#34892;&#26085;&#24535;&#35299;&#26512;&#30340;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
An Evaluation of Log Parsing with ChatGPT. (arXiv:2306.01590v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01590
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;ChatGPT&#22312;&#26085;&#24535;&#35299;&#26512;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#32467;&#26524;&#34920;&#26126;&#23427;&#21487;&#20197;&#36890;&#36807;&#36866;&#24403;&#30340;&#25552;&#31034;&#23454;&#29616;&#26377;&#21069;&#36884;&#30340;&#26085;&#24535;&#35299;&#26512;&#32467;&#26524;&#65292;&#29305;&#21035;&#26159;&#22312;few-shot&#25552;&#31034;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36719;&#20214;&#26085;&#24535;&#22312;&#30830;&#20445;&#22823;&#35268;&#27169;&#36719;&#20214;&#31995;&#32479;&#30340;&#21487;&#38752;&#24615;&#21644;&#21487;&#32500;&#25252;&#24615;&#26041;&#38754;&#25198;&#28436;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#35282;&#33394;&#65292;&#22240;&#20026;&#23427;&#20204;&#36890;&#24120;&#26159;&#36816;&#34892;&#26102;&#20449;&#24687;&#30340;&#21807;&#19968;&#26469;&#28304;&#12290;&#26085;&#24535;&#35299;&#26512;&#23558;&#21407;&#22987;&#26085;&#24535;&#28040;&#24687;&#36716;&#25442;&#20026;&#32467;&#26500;&#21270;&#25968;&#25454;&#65292;&#26159;&#21521;&#19979;&#28216;&#26085;&#24535;&#20998;&#26512;&#30340;&#37325;&#35201;&#21021;&#22987;&#27493;&#39588;&#12290;&#22312;&#26368;&#36817;&#30340;&#30740;&#31350;&#20013;&#65292;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;ChatGPT&#24050;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#31181;&#36719;&#20214;&#24037;&#31243;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#23427;&#22312;&#33258;&#21160;&#21270;&#26085;&#24535;&#35299;&#26512;&#26041;&#38754;&#30340;&#24615;&#33021;&#20173;&#19981;&#28165;&#26970;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#35299;&#20915;&#20004;&#20010;&#30740;&#31350;&#38382;&#39064;&#35780;&#20272;&#20102;ChatGPT&#36827;&#34892;&#26085;&#24535;&#35299;&#26512;&#30340;&#33021;&#21147;&#12290; &#65288;1&#65289;ChatGPT&#33021;&#21542;&#26377;&#25928;&#22320;&#35299;&#26512;&#26085;&#24535;&#65311;&#65288;2&#65289;ChatGPT&#22312;&#19981;&#21516;&#25552;&#31034;&#26041;&#27861;&#19979;&#30340;&#34920;&#29616;&#22914;&#20309;&#65311;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;ChatGPT&#21487;&#20197;&#36890;&#36807;&#36866;&#24403;&#30340;&#25552;&#31034;&#23454;&#29616;&#26377;&#21069;&#36884;&#30340;&#26085;&#24535;&#35299;&#26512;&#32467;&#26524;&#65292;&#29305;&#21035;&#26159;&#22312;few-shot&#25552;&#31034;&#19979;&#12290;&#22522;&#20110;&#25105;&#20204;&#30340;&#21457;&#29616;&#65292;&#25105;&#20204;&#27010;&#36848;&#20102;&#20960;&#20010;&#22522;&#20110;ChatGPT&#30340;&#26085;&#24535;&#35299;&#26512;&#30340;&#25361;&#25112;&#21644;&#26426;&#36935;&#12290;
&lt;/p&gt;
&lt;p&gt;
Software logs play an essential role in ensuring the reliability and maintainability of large-scale software systems, as they are often the sole source of runtime information. Log parsing, which converts raw log messages into structured data, is an important initial step towards downstream log analytics. In recent studies, ChatGPT, the current cutting-edge large language model (LLM), has been widely applied to a wide range of software engineering tasks. However, its performance in automated log parsing remains unclear. In this paper, we evaluate ChatGPT's ability to undertake log parsing by addressing two research questions. (1) Can ChatGPT effectively parse logs? (2) How does ChatGPT perform with different prompting methods? Our results show that ChatGPT can achieve promising results for log parsing with appropriate prompts, especially with few-shot prompting. Based on our findings, we outline several challenges and opportunities for ChatGPT-based log parsing.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#27010;&#29575;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#65288;ProbCBM&#65289;&#65292;&#36890;&#36807;&#24314;&#31435;&#27010;&#29575;&#27010;&#24565;&#23884;&#20837;&#26469;&#35299;&#20915;&#25968;&#25454;&#20013;&#27010;&#24565;&#23384;&#22312;&#27169;&#31946;&#24615;&#30340;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;CBM&#27169;&#22411;&#30340;&#21487;&#38752;&#24615;&#21450;&#35299;&#37322;&#30340;&#21487;&#20449;&#24230;&#12290;</title><link>http://arxiv.org/abs/2306.01574</link><description>&lt;p&gt;
&#27010;&#29575;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Probabilistic Concept Bottleneck Models. (arXiv:2306.01574v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01574
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#27010;&#29575;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#65288;ProbCBM&#65289;&#65292;&#36890;&#36807;&#24314;&#31435;&#27010;&#29575;&#27010;&#24565;&#23884;&#20837;&#26469;&#35299;&#20915;&#25968;&#25454;&#20013;&#27010;&#24565;&#23384;&#22312;&#27169;&#31946;&#24615;&#30340;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;CBM&#27169;&#22411;&#30340;&#21487;&#38752;&#24615;&#21450;&#35299;&#37322;&#30340;&#21487;&#20449;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#26088;&#22312;&#20197;&#21487;&#35835;&#30340;&#26041;&#24335;&#20570;&#20986;&#20915;&#31574;&#12290;&#20854;&#20013;&#65292;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#65288;CBM&#65289;&#26681;&#25454;&#39044;&#27979;&#30340;&#27010;&#24565;&#36827;&#34892;&#27010;&#24565;&#39044;&#27979;&#21644;&#31867;&#39044;&#27979;&#20004;&#27493;&#39588;&#12290;CBM&#20351;&#29992;&#20174;&#27010;&#24565;&#39044;&#27979;&#20013;&#24471;&#20986;&#30340;&#39640;&#32423;&#27010;&#24565;&#25552;&#20379;&#35299;&#37322;&#65307;&#22240;&#27492;&#65292;&#21487;&#38752;&#30340;&#27010;&#24565;&#39044;&#27979;&#23545;&#20110;&#21487;&#20449;&#24230;&#38750;&#24120;&#37325;&#35201;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#21487;&#33021;&#25439;&#23475;&#21487;&#38752;&#24615;&#30340;&#27169;&#31946;&#24615;&#38382;&#39064;&#12290;&#34429;&#28982;&#25968;&#25454;&#20013;&#27010;&#24565;&#30340;&#23384;&#22312;&#24448;&#24448;&#26159;&#27169;&#31946;&#30340;&#65292;&#20294;CBM&#22312;&#19981;&#32771;&#34385;&#27492;&#27169;&#31946;&#24615;&#30340;&#24773;&#20917;&#19979;&#20351;&#29992;&#30830;&#23450;&#24615;&#26041;&#27861;&#39044;&#27979;&#27010;&#24565;&#12290;&#20026;&#20102;&#38024;&#23545;&#36825;&#31181;&#27169;&#31946;&#24615;&#25552;&#20379;&#21487;&#38752;&#30340;&#35299;&#37322;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#27010;&#29575;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#65288;ProbCBM&#65289;&#12290;&#36890;&#36807;&#21033;&#29992;&#27010;&#29575;&#27010;&#24565;&#23884;&#20837;&#65292;ProbCBM&#23545;&#27010;&#24565;&#39044;&#27979;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#36827;&#34892;&#24314;&#27169;&#65292;&#24182;&#22522;&#20110;&#27010;&#24565;&#21450;&#20854;&#30456;&#24212;&#30340;&#19981;&#30830;&#23450;&#24615;&#25552;&#20379;&#35299;&#37322;&#12290;&#36825;&#31181;&#19981;&#30830;&#23450;&#24615;&#25552;&#39640;&#20102;&#35299;&#37322;&#30340;&#21487;&#38752;&#24615;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#31867;&#21035;&#39044;&#27979;&#30340;&#27010;&#29575;&#24615;&#36136;&#65292;ProbCBM&#36824;&#25552;&#20379;&#20102;&#31867;&#21035;&#39044;&#27979;&#30340;&#19981;&#30830;&#23450;&#24230;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
Interpretable models are designed to make decisions in a human-interpretable manner. Representatively, Concept Bottleneck Models (CBM) follow a two-step process of concept prediction and class prediction based on the predicted concepts. CBM provides explanations with high-level concepts derived from concept predictions; thus, reliable concept predictions are important for trustworthiness. In this study, we address the ambiguity issue that can harm reliability. While the existence of a concept can often be ambiguous in the data, CBM predicts concepts deterministically without considering this ambiguity. To provide a reliable interpretation against this ambiguity, we propose Probabilistic Concept Bottleneck Models (ProbCBM). By leveraging probabilistic concept embeddings, ProbCBM models uncertainty in concept prediction and provides explanations based on the concept and its corresponding uncertainty. This uncertainty enhances the reliability of the explanations. Furthermore, as class unc
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;PassGPT&#36827;&#34892;&#23494;&#30721;&#24314;&#27169;&#21644;&#29983;&#25104;&#65292;&#35813;&#27169;&#22411;&#27604;&#22522;&#20110;GAN&#30340;&#29616;&#26377;&#26041;&#27861;&#26356;&#20934;&#30830;&#65292;&#33021;&#29983;&#25104;&#31526;&#21512;&#20219;&#24847;&#38480;&#21046;&#30340;&#23494;&#30721;&#65292;&#20026;&#25552;&#39640;&#23494;&#30721;&#24378;&#24230;&#20272;&#35745;&#22120;&#25552;&#20379;&#20102;&#28508;&#22312;&#30340;&#24110;&#21161;&#12290;</title><link>http://arxiv.org/abs/2306.01545</link><description>&lt;p&gt;
PassGPT: &#22823;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#23494;&#30721;&#24314;&#27169;&#21644;&#65288;&#24341;&#23548;&#24335;&#65289;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
PassGPT: Password Modeling and (Guided) Generation with Large Language Models. (arXiv:2306.01545v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01545
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;PassGPT&#36827;&#34892;&#23494;&#30721;&#24314;&#27169;&#21644;&#29983;&#25104;&#65292;&#35813;&#27169;&#22411;&#27604;&#22522;&#20110;GAN&#30340;&#29616;&#26377;&#26041;&#27861;&#26356;&#20934;&#30830;&#65292;&#33021;&#29983;&#25104;&#31526;&#21512;&#20219;&#24847;&#38480;&#21046;&#30340;&#23494;&#30721;&#65292;&#20026;&#25552;&#39640;&#23494;&#30721;&#24378;&#24230;&#20272;&#35745;&#22120;&#25552;&#20379;&#20102;&#28508;&#22312;&#30340;&#24110;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21487;&#20197;&#25104;&#21151;&#22320;&#27169;&#25311;&#33258;&#28982;&#35821;&#35328;&#65292;&#26080;&#38656;&#26126;&#30830;&#30340;&#30417;&#30563;&#65292;&#20165;&#36890;&#36807;&#22823;&#37327;&#30340;&#25991;&#26412;&#36827;&#34892;&#35757;&#32451;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;LLMs&#22312;&#24314;&#27169;&#23494;&#30721;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;PassGPT&#65292;&#23427;&#26159;&#19968;&#20010;&#22312;&#23494;&#30721;&#27844;&#38706;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#30340;LLM&#65292;&#29992;&#20110;&#29983;&#25104;&#23494;&#30721;&#12290;PassGPT&#36890;&#36807;&#29468;&#27979;&#20004;&#20493;&#20110;&#22522;&#20110;&#29983;&#25104;&#24615;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#30340;&#29616;&#26377;&#26041;&#27861;&#20013;&#30340;&#20197;&#21069;&#26410;&#35265;&#36807;&#30340;&#23494;&#30721;&#32780;&#32988;&#36807;&#20854;&#23427;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#24341;&#23548;&#24335;&#23494;&#30721;&#29983;&#25104;&#30340;&#27010;&#24565;&#65292;&#21033;&#29992;PassGPT&#30340;&#25277;&#26679;&#36807;&#31243;&#29983;&#25104;&#31526;&#21512;&#20219;&#24847;&#38480;&#21046;&#30340;&#23494;&#30721;&#65292;&#36825;&#22312;&#24403;&#21069;&#22522;&#20110;GAN&#30340;&#31574;&#30053;&#20013;&#26159;&#32570;&#20047;&#30340;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23545;PassGPT&#23545;&#23494;&#30721;&#23450;&#20041;&#30340;&#29109;&#21644;&#27010;&#29575;&#20998;&#24067;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#65292;&#24182;&#35752;&#35770;&#20102;&#20854;&#22312;&#22686;&#24378;&#29616;&#26377;&#23494;&#30721;&#24378;&#24230;&#20272;&#35745;&#22120;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) successfully model natural language from vast amounts of text without the need for explicit supervision. In this paper, we investigate the efficacy of LLMs in modeling passwords. We present PassGPT, a LLM trained on password leaks for password generation. PassGPT outperforms existing methods based on generative adversarial networks (GAN) by guessing twice as many previously unseen passwords. Furthermore, we introduce the concept of guided password generation, where we leverage PassGPT sampling procedure to generate passwords matching arbitrary constraints, a feat lacking in current GAN-based strategies. Lastly, we conduct an in-depth analysis of the entropy and probability distribution that PassGPT defines over passwords and discuss their use in enhancing existing password strength estimators.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#23545;&#35937;&#26816;&#27979;&#30340;&#19977;&#38454;&#27573;&#27169;&#22411;&#21387;&#32553;&#26041;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#31232;&#30095;&#35757;&#32451;&#12289;&#32452;&#36890;&#36947;&#21098;&#26525;&#21644;&#31354;&#38388;&#20851;&#27880;&#25552;&#21462;&#22823;&#24133;&#32553;&#23567;&#20102;&#27169;&#22411;&#22823;&#23567;&#12289;&#21442;&#25968;&#25968;&#37327;&#21644;&#35745;&#31639;&#22797;&#26434;&#24230;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26377;&#31454;&#20105;&#21147;&#30340;&#23545;&#35937;&#26816;&#27979;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2306.01526</link><description>&lt;p&gt;
&#23545;&#35937;&#26816;&#27979;&#30340;&#32452;&#36890;&#36947;&#21098;&#26525;&#21644;&#31354;&#38388;&#20851;&#27880;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Group channel pruning and spatial attention distilling for object detection. (arXiv:2306.01526v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01526
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#23545;&#35937;&#26816;&#27979;&#30340;&#19977;&#38454;&#27573;&#27169;&#22411;&#21387;&#32553;&#26041;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#31232;&#30095;&#35757;&#32451;&#12289;&#32452;&#36890;&#36947;&#21098;&#26525;&#21644;&#31354;&#38388;&#20851;&#27880;&#25552;&#21462;&#22823;&#24133;&#32553;&#23567;&#20102;&#27169;&#22411;&#22823;&#23567;&#12289;&#21442;&#25968;&#25968;&#37327;&#21644;&#35745;&#31639;&#22797;&#26434;&#24230;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26377;&#31454;&#20105;&#21147;&#30340;&#23545;&#35937;&#26816;&#27979;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#36229;&#21442;&#25968;&#21270;&#65292;&#35768;&#22810;&#22522;&#20110;&#21098;&#26525;&#21644;&#37327;&#21270;&#30340;&#27169;&#22411;&#21387;&#32553;&#26041;&#27861;&#24050;&#32463;&#20986;&#29616;&#12290;&#36825;&#20123;&#26041;&#27861;&#22312;&#20943;&#23567;&#27169;&#22411;&#30340;&#22823;&#23567;&#12289;&#21442;&#25968;&#25968;&#37327;&#21644;&#35745;&#31639;&#22797;&#26434;&#24230;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#34987;&#36825;&#20123;&#26041;&#27861;&#21387;&#32553;&#30340;&#27169;&#22411;&#38656;&#35201;&#29305;&#27530;&#30340;&#30828;&#20214;&#21644;&#36719;&#20214;&#25903;&#25345;&#65292;&#36825;&#22686;&#21152;&#20102;&#37096;&#32626;&#25104;&#26412;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#26041;&#27861;&#20027;&#35201;&#29992;&#20110;&#20998;&#31867;&#20219;&#21153;&#65292;&#24456;&#23569;&#30452;&#25509;&#29992;&#20110;&#26816;&#27979;&#20219;&#21153;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#19977;&#38454;&#27573;&#30340;&#27169;&#22411;&#21387;&#32553;&#26041;&#27861;&#65306;&#21160;&#24577;&#31232;&#30095;&#35757;&#32451;&#12289;&#32452;&#36890;&#36947;&#21098;&#26525;&#21644;&#31354;&#38388;&#20851;&#27880;&#25552;&#21462;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20943;&#23567;&#27169;&#22411;&#22823;&#23567;&#12289;&#21442;&#25968;&#25968;&#37327;&#21644;&#35745;&#31639;&#22797;&#26434;&#24230;&#30340;&#21516;&#26102;&#65292;&#23454;&#29616;&#20102;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#23545;&#35937;&#26816;&#27979;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to the over-parameterization of neural networks, many model compression methods based on pruning and quantization have emerged. They are remarkable in reducing the size, parameter number, and computational complexity of the model. However, most of the models compressed by such methods need the support of special hardware and software, which increases the deployment cost. Moreover, these methods are mainly used in classification tasks, and rarely directly used in detection tasks. To address these issues, for the object detection network we introduce a three-stage model compression method: dynamic sparse training, group channel pruning, and spatial attention distilling. Firstly, to select out the unimportant channels in the network and maintain a good balance between sparsity and accuracy, we put forward a dynamic sparse training method, which introduces a variable sparse rate, and the sparse rate will change with the training process of the network. Secondly, to reduce the effect of
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#23616;&#27719;&#38598;&#21644;&#38745;&#24577;&#20869;&#23481;&#24847;&#35782;&#30340;&#31471;&#21040;&#31471;&#26694;&#26550;DynEformer&#65292;&#20854;&#20013;&#35774;&#35745;&#20102;&#20840;&#23616;&#27719;&#38598;&#21644;&#20449;&#24687;&#21512;&#24182;&#26426;&#21046;&#65292;&#21487;&#20026;&#21160;&#24577;&#22810;&#31199;&#25143;&#36793;&#32536;&#20113;&#24179;&#21488;&#25552;&#20379;&#32479;&#19968;&#30340;&#24037;&#20316;&#36127;&#36733;&#39044;&#27979;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2306.01507</link><description>&lt;p&gt;
&#19968;&#20307;&#21270;&#21160;&#24577;&#22810;&#31199;&#25143;&#36793;&#32536;&#20113;&#24179;&#21488;&#30340;&#32479;&#19968;&#24037;&#20316;&#36127;&#36733;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
One for All: Unified Workload Prediction for Dynamic Multi-tenant Edge Cloud Platforms. (arXiv:2306.01507v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01507
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#23616;&#27719;&#38598;&#21644;&#38745;&#24577;&#20869;&#23481;&#24847;&#35782;&#30340;&#31471;&#21040;&#31471;&#26694;&#26550;DynEformer&#65292;&#20854;&#20013;&#35774;&#35745;&#20102;&#20840;&#23616;&#27719;&#38598;&#21644;&#20449;&#24687;&#21512;&#24182;&#26426;&#21046;&#65292;&#21487;&#20026;&#21160;&#24577;&#22810;&#31199;&#25143;&#36793;&#32536;&#20113;&#24179;&#21488;&#25552;&#20379;&#32479;&#19968;&#30340;&#24037;&#20316;&#36127;&#36733;&#39044;&#27979;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#31199;&#25143;&#36793;&#32536;&#20113;&#24179;&#21488;&#20013;&#30340;&#24037;&#20316;&#36127;&#36733;&#39044;&#27979;&#23545;&#20110;&#39640;&#25928;&#30340;&#24212;&#29992;&#37096;&#32626;&#21644;&#36164;&#28304;&#37197;&#32622;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;MT-ECP&#20013;&#24322;&#26500;&#30340;&#24212;&#29992;&#31243;&#24207;&#27169;&#24335;&#12289;&#21487;&#21464;&#30340;&#22522;&#30784;&#26550;&#26500;&#24615;&#33021;&#20197;&#21450;&#39057;&#32321;&#30340;&#37096;&#32626;&#32473;&#20934;&#30830;&#21644;&#39640;&#25928;&#30340;&#24037;&#20316;&#36127;&#36733;&#39044;&#27979;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#24050;&#26377;&#30340;&#22522;&#20110;&#32858;&#31867;&#30340;&#21160;&#24577;MT-ECP&#24314;&#27169;&#26041;&#27861;&#24120;&#24120;&#20250;&#30001;&#20110;&#38656;&#35201;&#32500;&#25252;&#22823;&#37327;&#25968;&#25454;&#38598;&#32676;&#21644;&#27169;&#22411;&#32780;&#20135;&#29983;&#36807;&#39640;&#30340;&#25104;&#26412;&#12290;&#29616;&#26377;&#30340;&#31471;&#21040;&#31471;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#27861;&#22312;&#21160;&#24577;MT-ECP&#20013;&#26080;&#27861;&#25552;&#20379;&#19968;&#33268;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#20840;&#23616;&#27719;&#38598;&#21644;&#38745;&#24577;&#20869;&#23481;&#24847;&#35782;&#30340;&#31471;&#21040;&#31471;&#26694;&#26550;DynEformer&#65292;&#20026;&#21160;&#24577;MT-ECP&#25552;&#20379;&#32479;&#19968;&#30340;&#24037;&#20316;&#36127;&#36733;&#39044;&#27979;&#26041;&#26696;&#12290;&#31934;&#24515;&#35774;&#35745;&#30340;&#20840;&#23616;&#27719;&#38598;&#21644;&#20449;&#24687;&#21512;&#24182;&#26426;&#21046;&#21487;&#20197;&#26377;&#25928;&#22320;&#35782;&#21035;&#21644;&#21033;&#29992;&#20840;&#23616;&#24212;&#29992;&#31243;&#24207;&#27169;&#24335;&#26469;&#39537;&#21160;&#26412;&#22320;&#24037;&#20316;&#36127;&#36733;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Workload prediction in multi-tenant edge cloud platforms (MT-ECP) is vital for efficient application deployment and resource provisioning. However, the heterogeneous application patterns, variable infrastructure performance, and frequent deployments in MT-ECP pose significant challenges for accurate and efficient workload prediction. Clustering-based methods for dynamic MT-ECP modeling often incur excessive costs due to the need to maintain numerous data clusters and models, which leads to excessive costs. Existing end-to-end time series prediction methods are challenging to provide consistent prediction performance in dynamic MT-ECP. In this paper, we propose an end-to-end framework with global pooling and static content awareness, DynEformer, to provide a unified workload prediction scheme for dynamic MT-ECP. Meticulously designed global pooling and information merging mechanisms can effectively identify and utilize global application patterns to drive local workload predictions. The
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30417;&#30563;&#24335;&#23545;&#25239;&#24615;&#23545;&#27604;&#23398;&#20064;&#65288;SACL&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#23398;&#20064;&#31867;&#21035;&#20998;&#24067;&#32467;&#26500;&#34920;&#31034;&#65292;&#36890;&#36807;&#32852;&#21512;&#31867;&#21035;&#20998;&#24067;&#23545;&#27604;&#23398;&#20064;&#30446;&#26631;&#65292;&#26377;&#25928;&#21033;&#29992;&#26631;&#31614;&#32423;&#29305;&#24615;&#19968;&#33268;&#24615;&#24182;&#20445;&#30041;&#32454;&#31890;&#24230;&#30340;&#31867;&#20869;&#29305;&#24615;&#65292;&#23454;&#29616;&#20102;&#22312;&#23545;&#35805;&#24773;&#24863;&#35782;&#21035;&#20013;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.01505</link><description>&lt;p&gt;
&#22312;&#23545;&#35805;&#24773;&#24863;&#35782;&#21035;&#20013;&#20351;&#29992;&#30417;&#30563;&#24335;&#23545;&#25239;&#24615;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Supervised Adversarial Contrastive Learning for Emotion Recognition in Conversations. (arXiv:2306.01505v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01505
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30417;&#30563;&#24335;&#23545;&#25239;&#24615;&#23545;&#27604;&#23398;&#20064;&#65288;SACL&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#23398;&#20064;&#31867;&#21035;&#20998;&#24067;&#32467;&#26500;&#34920;&#31034;&#65292;&#36890;&#36807;&#32852;&#21512;&#31867;&#21035;&#20998;&#24067;&#23545;&#27604;&#23398;&#20064;&#30446;&#26631;&#65292;&#26377;&#25928;&#21033;&#29992;&#26631;&#31614;&#32423;&#29305;&#24615;&#19968;&#33268;&#24615;&#24182;&#20445;&#30041;&#32454;&#31890;&#24230;&#30340;&#31867;&#20869;&#29305;&#24615;&#65292;&#23454;&#29616;&#20102;&#22312;&#23545;&#35805;&#24773;&#24863;&#35782;&#21035;&#20013;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24773;&#24863;&#35782;&#21035;&#22312;&#23545;&#35805;&#20013;&#26159;&#25552;&#21462;&#27867;&#21270;&#21644;&#31283;&#20581;&#34920;&#31034;&#30340;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30417;&#30563;&#24335;&#23545;&#25239;&#24615;&#23545;&#27604;&#23398;&#20064;&#65288;SACL&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#23398;&#20064;&#31867;&#21035;&#20998;&#24067;&#32467;&#26500;&#34920;&#31034;&#12290;&#35813;&#26694;&#26550;&#24212;&#29992;&#20110;&#23545;&#27604;&#24863;&#30693;&#23545;&#25239;&#24615;&#35757;&#32451;&#20197;&#29983;&#25104;&#26368;&#22351;&#24773;&#20917;&#30340;&#26679;&#26412;&#65292;&#24182;&#22312;&#21407;&#22987;&#21644;&#23545;&#25239;&#26679;&#26412;&#19978;&#20351;&#29992;&#32852;&#21512;&#31867;&#21035;&#20998;&#24067;&#23545;&#27604;&#23398;&#20064;&#30446;&#26631;&#12290;&#23427;&#21487;&#20197;&#26377;&#25928;&#22320;&#21033;&#29992;&#26631;&#31614;&#32423;&#29305;&#24615;&#19968;&#33268;&#24615;&#24182;&#20445;&#30041;&#32454;&#31890;&#24230;&#30340;&#31867;&#20869;&#29305;&#24615;&#12290;&#20026;&#20102;&#36991;&#20813;&#23545;&#19978;&#19979;&#25991;&#30456;&#20851;&#25968;&#25454;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#19978;&#19979;&#25991;&#23545;&#25239;&#24615;&#35757;&#32451;&#31574;&#30053;&#65292;&#20174;&#19978;&#19979;&#25991;&#20013;&#23398;&#20064;&#26356;&#22810;&#19981;&#21516;&#30340;&#29305;&#24449;&#65292;&#24182;&#22686;&#24378;&#27169;&#22411;&#23545;&#19978;&#19979;&#25991;&#30340;&#23481;&#38169;&#24615;&#12290;&#22312;&#35813;&#26694;&#26550;&#19979;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;&#24207;&#21015;&#30340;&#26041;&#27861;SACL-LSTM&#65292;&#29992;&#20110;&#23398;&#20064;&#38024;&#23545;ERC&#30340;&#26631;&#31614;&#19968;&#33268;&#21644;&#19978;&#19979;&#25991;&#31283;&#20581;&#30340;&#24773;&#24863;&#29305;&#24449;&#12290;&#22312;&#19977;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;SACL-LSTM&#22312;&#23545;&#35805;&#24773;&#24863;&#35782;&#21035;&#26041;&#38754;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#20248;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Extracting generalized and robust representations is a major challenge in emotion recognition in conversations (ERC). To address this, we propose a supervised adversarial contrastive learning (SACL) framework for learning class-spread structured representations. The framework applies contrast-aware adversarial training to generate worst-case samples and uses a joint class-spread contrastive learning objective on both original and adversarial samples. It can effectively utilize label-level feature consistency and retain fine-grained intra-class features. To avoid the negative impact of adversarial perturbations on context-dependent data, we design a contextual adversarial training strategy to learn more diverse features from context and enhance the model's context robustness. We develop a sequence-based method SACL-LSTM under this framework, to learn label-consistent and context-robust emotional features for ERC. Experiments on three datasets demonstrate that SACL-LSTM achieves state-of
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#22914;&#20309;&#36890;&#36807;&#32435;&#20837;&#20154;&#31867;&#19987;&#19994;&#30693;&#35782;&#20998;&#24067;&#12289;&#35757;&#32451;&#26080;&#30417;&#30563;&#27169;&#22411;&#36827;&#34892;&#27169;&#25311;&#25512;&#29702;&#65292;&#26174;&#33879;&#25913;&#21892;AI&#23545;&#26410;&#26469;&#21457;&#29616;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#24182;&#20135;&#29983;&#31185;&#23398;&#19978;&#26377;&#21069;&#36884;&#12289;&#24819;&#35937;&#21147;&#24102;&#26469;&#30340;&#33258;&#28982;&#20551;&#35774;&#65292;&#20174;&#32780;&#21152;&#36895;&#20154;&#31867;&#21457;&#29616;&#25110;&#25506;&#32034;&#20854;&#30450;&#28857;&#12290;</title><link>http://arxiv.org/abs/2306.01495</link><description>&lt;p&gt;
&#20154;&#31867;&#24863;&#30693;&#30340;&#20154;&#24037;&#26234;&#33021;&#22312;&#31185;&#23398;&#21152;&#36895;&#26041;&#38754;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Accelerating science with human-aware artificial intelligence. (arXiv:2306.01495v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01495
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#22914;&#20309;&#36890;&#36807;&#32435;&#20837;&#20154;&#31867;&#19987;&#19994;&#30693;&#35782;&#20998;&#24067;&#12289;&#35757;&#32451;&#26080;&#30417;&#30563;&#27169;&#22411;&#36827;&#34892;&#27169;&#25311;&#25512;&#29702;&#65292;&#26174;&#33879;&#25913;&#21892;AI&#23545;&#26410;&#26469;&#21457;&#29616;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#24182;&#20135;&#29983;&#31185;&#23398;&#19978;&#26377;&#21069;&#36884;&#12289;&#24819;&#35937;&#21147;&#24102;&#26469;&#30340;&#33258;&#28982;&#20551;&#35774;&#65292;&#20174;&#32780;&#21152;&#36895;&#20154;&#31867;&#21457;&#29616;&#25110;&#25506;&#32034;&#20854;&#30450;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21457;&#26126;&#26377;&#20215;&#20540;&#30340;&#26448;&#26009;&#21644;&#38774;&#21521;&#27835;&#30103;&#26041;&#38754;&#65292;&#20351;&#29992;&#24050;&#21457;&#34920;&#30340;&#31185;&#23398;&#25104;&#26524;&#35757;&#32451;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#27169;&#22411;&#24050;&#32463;&#24471;&#21040;&#20102;&#24212;&#29992;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#24573;&#30053;&#19981;&#26029;&#25913;&#21464;&#21457;&#29616;&#39046;&#22495;&#30340;&#20154;&#31867;&#31185;&#23398;&#23478;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#35757;&#32451;&#26080;&#30417;&#30563;&#27169;&#22411;&#36827;&#34892;&#27169;&#25311;&#25512;&#29702;&#65288;&#19987;&#23478;&#35748;&#20026;&#26131;&#20110;&#29702;&#35299;&#65289;&#24182;&#32435;&#20837;&#20154;&#31867;&#19987;&#19994;&#30693;&#35782;&#20998;&#24067;&#65292;&#21487;&#20197;&#26174;&#33879;&#25913;&#21892;&#65288;&#39640;&#36798;400&#65285;&#65289;AI&#23545;&#26410;&#26469;&#21457;&#29616;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#30456;&#20851;&#25991;&#29486;&#31232;&#32570;&#30340;&#24773;&#20917;&#19979;&#12290;&#36825;&#20123;&#27169;&#22411;&#36890;&#36807;&#39044;&#27979;&#20154;&#31867;&#30340;&#39044;&#27979;&#21644;&#23558;&#20250;&#25552;&#20986;&#36825;&#20123;&#39044;&#27979;&#30340;&#31185;&#23398;&#23478;&#26469;&#25104;&#21151;&#12290;&#36890;&#36807;&#35843;&#25972;&#20154;&#31867;&#24863;&#30693;&#30340;AI&#20197;&#36991;&#24320;&#24120;&#35268;&#24605;&#36335;&#65292;&#25105;&#20204;&#21487;&#20197;&#20135;&#29983;&#31185;&#23398;&#19978;&#26377;&#21069;&#36884;&#12289;&#24819;&#35937;&#21147;&#24102;&#26469;&#30340;&#33258;&#28982;&#20551;&#35774;&#65292;&#24182;&#19988;&#25317;&#26377;&#36229;&#36234;&#24403;&#21069;&#38382;&#39064;&#30340;&#31185;&#23398;&#36827;&#23637;&#28508;&#21147;&#65292;&#36825;&#20123;&#38382;&#39064;&#30452;&#21040;&#36965;&#36828;&#26410;&#26469;&#25165;&#21487;&#33021;&#24471;&#21040;&#35299;&#20915;&#12290;&#20154;&#31867;&#24863;&#30693;&#30340;&#20154;&#24037;&#26234;&#33021;&#21487;&#20197;&#22686;&#24378;&#31185;&#23398;&#21019;&#36896;&#21147;&#65292;&#24182;&#25193;&#22823;&#30740;&#31350;&#38382;&#39064;&#30340;&#33539;&#22260;&#65292;&#20174;&#32780;&#21152;&#36895;&#20154;&#31867;&#21457;&#29616;&#25110;&#25506;&#32034;&#20854;&#30450;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial intelligence (AI) models trained on published scientific findings have been used to invent valuable materials and targeted therapies, but they typically ignore the human scientists who continually alter the landscape of discovery. Here we show that incorporating the distribution of human expertise by training unsupervised models on simulated inferences cognitively accessible to experts dramatically improves (up to 400%) AI prediction of future discoveries beyond those focused on research content alone, especially when relevant literature is sparse. These models succeed by predicting human predictions and the scientists who will make them. By tuning human-aware AI to avoid the crowd, we can generate scientifically promising "alien" hypotheses unlikely to be imagined or pursued without intervention until the distant future, which hold promise to punctuate scientific advance beyond questions currently pursued. Accelerating human discovery or probing its blind spots, human-aware
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#36861;&#21152;&#27491;&#20132;&#32422;&#26463;&#65292;&#20174;&#32780;&#22312;&#20445;&#25345;&#20302;&#31209;&#30697;&#38453;&#20998;&#35299;&#21069;&#25552;&#19979;&#25552;&#39640;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#40065;&#26834;&#24615;&#19982;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2306.01485</link><description>&lt;p&gt;
&#36890;&#36807;&#36817;&#20284;&#30340;&#27491;&#20132;&#32422;&#26463;&#23454;&#29616;&#31283;&#20581;&#30340;&#20302;&#31209;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robust low-rank training via approximate orthonormal constraints. (arXiv:2306.01485v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01485
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#36861;&#21152;&#27491;&#20132;&#32422;&#26463;&#65292;&#20174;&#32780;&#22312;&#20445;&#25345;&#20302;&#31209;&#30697;&#38453;&#20998;&#35299;&#21069;&#25552;&#19979;&#25552;&#39640;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#40065;&#26834;&#24615;&#19982;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#27169;&#22411;&#21644;&#25968;&#25454;&#35268;&#27169;&#30340;&#22686;&#38271;&#65292;&#35774;&#35745;&#21098;&#26525;&#25216;&#26415;&#20197;&#38477;&#20302;&#28145;&#24230;&#23398;&#20064;&#27969;&#31243;&#30340;&#36164;&#28304;&#38656;&#27714;&#24182;&#20445;&#25345;&#27169;&#22411;&#24615;&#33021;&#24050;&#25104;&#20026;&#24191;&#27867;&#21162;&#21147;&#30340;&#30446;&#26631;&#12290;&#20026;&#20102;&#38477;&#20302;&#25512;&#29702;&#21644;&#35757;&#32451;&#25104;&#26412;&#65292;&#20027;&#35201;&#30340;&#24037;&#20316;&#26041;&#21521;&#20043;&#19968;&#20351;&#29992;&#20302;&#31209;&#30697;&#38453;&#20998;&#35299;&#26469;&#34920;&#31034;&#32593;&#32476;&#26435;&#37325;&#12290;&#23613;&#31649;&#33021;&#22815;&#20445;&#25345;&#20934;&#30830;&#24615;&#65292;&#20294;&#25105;&#20204;&#35266;&#23519;&#21040;&#20302;&#31209;&#26041;&#27861;&#24448;&#24448;&#20250;&#25439;&#23475;&#27169;&#22411;&#23545;&#25239;&#25200;&#21160;&#30340;&#40065;&#26834;&#24615;&#12290;&#36890;&#36807;&#23558;&#31283;&#20581;&#24615;&#24314;&#27169;&#20026;&#31070;&#32463;&#32593;&#32476;&#30340;&#26465;&#20214;&#25968;&#65292;&#25105;&#20204;&#35748;&#20026;&#36825;&#31181;&#31283;&#20581;&#24615;&#25439;&#22833;&#26159;&#30001;&#20110;&#20302;&#31209;&#26435;&#37325;&#30697;&#38453;&#30340;&#22855;&#24322;&#20540;&#29190;&#28856;&#24341;&#36215;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31283;&#20581;&#30340;&#20302;&#31209;&#35757;&#32451;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#22312;&#20445;&#25345;&#32593;&#32476;&#26435;&#37325;&#20301;&#20110;&#20302;&#31209;&#30697;&#38453;&#27969;&#24418;&#19978;&#30340;&#21516;&#26102;&#65292;&#21516;&#26102;&#24378;&#21046;&#26045;&#21152;&#36817;&#20284;&#30340;&#27491;&#20132;&#32422;&#26463;&#12290;&#22240;&#27492;&#65292;&#35813;&#27169;&#22411;&#38477;&#20302;&#20102;&#35757;&#32451;&#21644;&#25512;&#29702;&#25104;&#26412;&#65292;&#21516;&#26102;&#30830;&#20445;&#20102;&#33391;&#22909;&#30340;&#26465;&#20214;&#24615;&#21644;&#26356;&#22909;&#30340;&#25239;&#24178;&#25200;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the growth of model and data sizes, a broad effort has been made to design pruning techniques that reduce the resource demand of deep learning pipelines, while retaining model performance. In order to reduce both inference and training costs, a prominent line of work uses low-rank matrix factorizations to represent the network weights. Although able to retain accuracy, we observe that low-rank methods tend to compromise model robustness against adversarial perturbations. By modeling robustness in terms of the condition number of the neural network, we argue that this loss of robustness is due to the exploding singular values of the low-rank weight matrices. Thus, we introduce a robust low-rank training algorithm that maintains the network's weights on the low-rank matrix manifold while simultaneously enforcing approximate orthonormal constraints. The resulting model reduces both training and inference costs while ensuring well-conditioning and thus better adversarial robustness, w
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#32508;&#21512;&#30740;&#31350;&#34920;&#26126;&#65292;XAI &#26041;&#27861;&#22312;&#23384;&#22312;&#25233;&#21046;&#21464;&#37327;&#26102;&#35299;&#37322;&#21487;&#33021;&#20986;&#29616;&#35823;&#23548;&#24615;&#65292;&#38656;&#35201;&#36827;&#34892;&#26356;&#21152;&#29702;&#35770;&#21270;&#21644;&#32463;&#39564;&#21270;&#30340;&#30740;&#31350;&#65292;&#30830;&#20445;&#20854;&#24212;&#29992;&#30340;&#27491;&#30830;&#24615;&#21644;&#21487;&#33021;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.01464</link><description>&lt;p&gt;
&#23384;&#22312;&#25233;&#21046;&#21464;&#37327;&#26102; XAI &#26041;&#27861;&#30340;&#29702;&#35770;&#34892;&#20026;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Theoretical Behavior of XAI Methods in the Presence of Suppressor Variables. (arXiv:2306.01464v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01464
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#32508;&#21512;&#30740;&#31350;&#34920;&#26126;&#65292;XAI &#26041;&#27861;&#22312;&#23384;&#22312;&#25233;&#21046;&#21464;&#37327;&#26102;&#35299;&#37322;&#21487;&#33021;&#20986;&#29616;&#35823;&#23548;&#24615;&#65292;&#38656;&#35201;&#36827;&#34892;&#26356;&#21152;&#29702;&#35770;&#21270;&#21644;&#32463;&#39564;&#21270;&#30340;&#30740;&#31350;&#65292;&#30830;&#20445;&#20854;&#24212;&#29992;&#30340;&#27491;&#30830;&#24615;&#21644;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#8220;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#8221;&#65288;XAI&#65289;&#31038;&#21306;&#24050;&#32463;&#21019;&#24314;&#20102;&#19968;&#20010;&#22823;&#37327;&#30340;&#26041;&#27861;&#26469;&#24357;&#21512;&#27169;&#22411;&#8220;&#22797;&#26434;&#24230;&#8221;&#21644;&#8220;&#21487;&#35299;&#37322;&#24615;&#8221;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#28982;&#32780;&#65292;XAI &#26041;&#27861;&#38656;&#35201;&#35299;&#20915;&#30340;&#20855;&#20307;&#38382;&#39064;&#23578;&#26410;&#24471;&#21040;&#27491;&#24335;&#35828;&#26126;&#12290;&#22240;&#27492;&#65292;XAI &#26041;&#27861;&#32570;&#20047;&#29702;&#35770;&#21644;&#23454;&#35777;&#35777;&#25454;&#65292;&#20197;&#39564;&#35777;&#20854;&#35299;&#37322;&#30340;&#8220;&#27491;&#30830;&#24615;&#8221;&#65292;&#38480;&#21046;&#20102;&#20854;&#29992;&#20110;&#36136;&#37327;&#25511;&#21046;&#21644;&#36879;&#26126;&#24230;&#30446;&#30340;&#30340;&#28508;&#21147;&#12290;&#21516;&#26102;&#65292;Haufe&#31561;&#20154;&#65288;2014&#65289;&#20351;&#29992;&#31616;&#21333;&#30340;&#29609;&#20855;&#20363;&#23376;&#23637;&#31034;&#20102;&#21363;&#20351;&#26159;&#32447;&#24615;&#27169;&#22411;&#30340;&#26631;&#20934;&#35299;&#37322;&#20063;&#21487;&#33021;&#26497;&#20855;&#35823;&#23548;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#21487;&#33021;&#20250;&#34987;&#24402;&#22240;&#20110;&#25152;&#35859;&#30340;&#25233;&#21046;&#21464;&#37327;&#65292;&#36825;&#20123;&#21464;&#37327;&#19982;&#39044;&#27979;&#30446;&#26631;&#32570;&#20047;&#20219;&#20309;&#32479;&#35745;&#20851;&#31995;&#12290;Wilming&#31561;&#20154;&#65288;2022&#65289;&#24050;&#32463;&#32463;&#39564;&#35777;&#20102;&#36825;&#31181;&#34892;&#20026;&#22312;&#22823;&#37327; XAI &#26041;&#27861;&#20013;&#30340;&#23454;&#35777;&#30740;&#31350;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25512;&#23548;&#20102;&#22810;&#31181;&#27969;&#34892;&#30340; XAI &#26041;&#27861;&#22312;&#31616;&#21333;&#30340; toy dataset &#19978;&#30340;&#34892;&#20026;&#30340;&#35299;&#26512;&#34920;&#36798;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, the community of 'explainable artificial intelligence' (XAI) has created a vast body of methods to bridge a perceived gap between model 'complexity' and 'interpretability'. However, a concrete problem to be solved by XAI methods has not yet been formally stated. As a result, XAI methods are lacking theoretical and empirical evidence for the 'correctness' of their explanations, limiting their potential use for quality-control and transparency purposes. At the same time, Haufe et al. (2014) showed, using simple toy examples, that even standard interpretations of linear models can be highly misleading. Specifically, high importance may be attributed to so-called suppressor variables lacking any statistical relation to the prediction target. This behavior has been confirmed empirically for a large array of XAI methods in Wilming et al. (2022). Here, we go one step further by deriving analytical expressions for the behavior of a variety of popular XAI methods on a simple tw
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27604;&#36739;&#20102;&#28145;&#24230; Q-Learning &#21644;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#31639;&#27861;&#22312;&#26448;&#26009;&#25490;&#24207;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#32467;&#26524;&#34920;&#26126;PPO&#22312;&#25152;&#26377;&#35780;&#20272;&#25351;&#26631;&#26041;&#38754;&#30340;&#34920;&#29616;&#37117;&#26356;&#20248;&#12290;&#31574;&#30053;&#20248;&#21270;&#31639;&#27861;&#22312;&#39640;&#32500;&#29366;&#24577;&#21644;&#21160;&#20316;&#31354;&#38388;&#38382;&#39064;&#20013;&#26377;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2306.01451</link><description>&lt;p&gt;
&#28145;&#24230; Q-Learning &#21644;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#31639;&#27861;&#22312;&#26448;&#26009;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
Deep Q-Learning versus Proximal Policy Optimization: Performance Comparison in a Material Sorting Task. (arXiv:2306.01451v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01451
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27604;&#36739;&#20102;&#28145;&#24230; Q-Learning &#21644;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#31639;&#27861;&#22312;&#26448;&#26009;&#25490;&#24207;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#32467;&#26524;&#34920;&#26126;PPO&#22312;&#25152;&#26377;&#35780;&#20272;&#25351;&#26631;&#26041;&#38754;&#30340;&#34920;&#29616;&#37117;&#26356;&#20248;&#12290;&#31574;&#30053;&#20248;&#21270;&#31639;&#27861;&#22312;&#39640;&#32500;&#29366;&#24577;&#21644;&#21160;&#20316;&#31354;&#38388;&#38382;&#39064;&#20013;&#26377;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22312;&#19968;&#20010;&#27169;&#25311;&#29983;&#20135;&#31995;&#32479;&#20013;&#27604;&#36739;&#20102;&#20004;&#31181;&#30693;&#21517;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65306;&#28145;&#24230; Q-Learning&#65288;DQN&#65289;&#21644;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#31639;&#27861;&#65288;PPO&#65289;&#12290;&#25105;&#20204;&#21033;&#29992;&#20102;&#19968;&#20010;&#22522;&#20110; Petri &#32593;&#30340;&#27169;&#25311;&#29615;&#22659;&#65292;&#22312;&#30456;&#20851;&#24037;&#20316;&#20013;&#27492;&#29615;&#22659;&#24050;&#34987;&#25552;&#20986;&#12290;&#26681;&#25454;&#22810;&#20010;&#35780;&#20272;&#25351;&#26631;&#65292;&#21253;&#25324;&#24179;&#22343;&#27491;&#30830;&#32452;&#35013;&#21644;&#20998;&#31867;&#20135;&#21697;&#30340;&#30334;&#20998;&#27604;&#12289;&#24179;&#22343;&#24773;&#33410;&#38271;&#24230;&#21644;&#25104;&#21151;&#24773;&#33410;&#30340;&#30334;&#20998;&#27604;&#65292;&#27604;&#36739;&#20102;&#36825;&#20004;&#31181;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;PPO&#22312;&#25152;&#26377;&#35780;&#20272;&#25351;&#26631;&#26041;&#38754;&#37117;&#34920;&#29616;&#20248;&#20110;DQN&#12290;&#35813;&#30740;&#31350;&#31361;&#20986;&#20102;&#22522;&#20110;&#31574;&#30053;&#30340;&#31639;&#27861;&#22312;&#39640;&#32500;&#29366;&#24577;&#21644;&#21160;&#20316;&#31354;&#38388;&#38382;&#39064;&#20013;&#30340;&#20248;&#21183;&#12290;&#35813;&#30740;&#31350;&#36890;&#36807;&#25552;&#20379;&#26377;&#20851;&#19981;&#21516;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#21450;&#20854;&#36866;&#29992;&#20110;&#19981;&#21516;&#20219;&#21153;&#30340;&#35265;&#35299;&#65292;&#20026;&#28145;&#24230; RL &#22312;&#29983;&#20135;&#31995;&#32479;&#39046;&#22495;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a comparison between two well-known deep Reinforcement Learning (RL) algorithms: Deep Q-Learning (DQN) and Proximal Policy Optimization (PPO) in a simulated production system. We utilize a Petri Net (PN)-based simulation environment, which was previously proposed in related work. The performance of the two algorithms is compared based on several evaluation metrics, including average percentage of correctly assembled and sorted products, average episode length, and percentage of successful episodes. The results show that PPO outperforms DQN in terms of all evaluation metrics. The study highlights the advantages of policy-based algorithms in problems with high-dimensional state and action spaces. The study contributes to the field of deep RL in context of production systems by providing insights into the effectiveness of different algorithms and their suitability for different tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#22810;&#35789;&#34920;&#36798;&#24335;&#25913;&#20889;&#26041;&#27861;&#65292;&#20351;&#29992;&#30340;&#25968;&#25454;&#21644;&#24037;&#20855;&#38750;&#24120;&#31616;&#21333;&#65292;&#19988;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#29087;&#35821;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24230;&#20219;&#21153;&#20013;&#24615;&#33021;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2306.01443</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#30340;&#22810;&#35789;&#34920;&#36798;&#25913;&#20889;&#26041;&#27861;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Paraphrasing of Multiword Expressions. (arXiv:2306.01443v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01443
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#22810;&#35789;&#34920;&#36798;&#24335;&#25913;&#20889;&#26041;&#27861;&#65292;&#20351;&#29992;&#30340;&#25968;&#25454;&#21644;&#24037;&#20855;&#38750;&#24120;&#31616;&#21333;&#65292;&#19988;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#29087;&#35821;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24230;&#20219;&#21153;&#20013;&#24615;&#33021;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#25913;&#20889;&#22810;&#35789;&#34920;&#36798;&#24335;&#65288;MWEs&#65289;&#30340;&#26041;&#27861;&#65292;&#21482;&#20351;&#29992;&#21333;&#35821;&#26009;&#24211;&#25968;&#25454;&#21644;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;&#26080;&#38656;&#24494;&#35843;&#65289;&#65292;&#19981;&#20351;&#29992;&#20219;&#20309;&#22806;&#37096;&#36164;&#28304;&#65292;&#22914;&#23383;&#20856;&#12290;&#25105;&#20204;&#22312;SemEval 2022&#29087;&#35821;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24230;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#34920;&#26126;&#23427;&#20248;&#20110;&#25152;&#26377;&#26080;&#30417;&#30563;&#31995;&#32479;&#24182;&#19982;&#26377;&#30417;&#30563;&#31995;&#32479;&#30456;&#21305;&#25932;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose an unsupervised approach to paraphrasing multiword expressions (MWEs) in context. Our model employs only monolingual corpus data and pre-trained language models (without fine-tuning), and does not make use of any external resources such as dictionaries. We evaluate our method on the SemEval 2022 idiomatic semantic text similarity task, and show that it outperforms all unsupervised systems and rivals supervised systems.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25506;&#31350;&#20102;&#22914;&#20309;&#36816;&#29992;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#25552;&#39640;&#24037;&#19994;&#24212;&#29992;&#30340;&#25928;&#29575;&#65292;&#20197;&#20256;&#36755;&#21644;&#32452;&#35013;&#36135;&#29289;&#20026;&#20363;&#65292;&#23454;&#29616;&#32553;&#30701;&#19978;&#24066;&#26102;&#38388;&#12289;&#22823;&#35268;&#27169;&#23450;&#21046;&#21644;&#25209;&#37327;&#32423;&#21035;&#30340;&#29983;&#20135;&#12290;</title><link>http://arxiv.org/abs/2306.01440</link><description>&lt;p&gt;
&#29992;&#20110;&#24037;&#19994;&#24212;&#29992;&#20013;&#24378;&#21270;&#23398;&#20064;&#38598;&#25104;&#30340;&#27169;&#22359;&#21270;&#27979;&#35797;&#24179;&#21488;
&lt;/p&gt;
&lt;p&gt;
A Modular Test Bed for Reinforcement Learning Incorporation into Industrial Applications. (arXiv:2306.01440v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01440
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25506;&#31350;&#20102;&#22914;&#20309;&#36816;&#29992;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#25552;&#39640;&#24037;&#19994;&#24212;&#29992;&#30340;&#25928;&#29575;&#65292;&#20197;&#20256;&#36755;&#21644;&#32452;&#35013;&#36135;&#29289;&#20026;&#20363;&#65292;&#23454;&#29616;&#32553;&#30701;&#19978;&#24066;&#26102;&#38388;&#12289;&#22823;&#35268;&#27169;&#23450;&#21046;&#21644;&#25209;&#37327;&#32423;&#21035;&#30340;&#29983;&#20135;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#24212;&#29992;&#35770;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26469;&#24212;&#23545;&#24037;&#19994;4.0&#30340;&#38656;&#27714;&#65292;&#21253;&#25324;&#32553;&#30701;&#19978;&#24066;&#26102;&#38388;&#12289;&#22823;&#35268;&#27169;&#23450;&#21046;&#21644;&#25209;&#37327;&#32423;&#21035;&#30340;&#29983;&#20135;&#30340;&#28508;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#26696;&#20363;&#65292;&#20219;&#21153;&#26159;&#25353;&#29031;&#39044;&#23450;&#20041;&#30340;&#35268;&#21017;&#36890;&#36807;&#27169;&#22411;&#24037;&#21378;&#20256;&#36755;&#21644;&#32452;&#35013;&#36135;&#29289;&#12290;&#27599;&#27425;&#27169;&#25311;&#36816;&#34892;&#28041;&#21450;&#23558;&#29305;&#23450;&#25968;&#37327;&#30340;&#38543;&#26426;&#39068;&#33394;&#30340;&#36135;&#29289;&#25918;&#32622;&#22312;&#20837;&#21475;&#22788;&#12290;&#30446;&#26631;&#26159;&#23558;&#21830;&#21697;&#36816;&#36755;&#21040;&#35013;&#37197;&#31449;&#65292;&#22312;&#20854;&#20013;&#23433;&#35013;&#20004;&#20010;&#38086;&#38025;&#65292;&#23558;&#19978;&#37096;&#36830;&#25509;&#21040;&#19979;&#37096;&#12290;&#22312;&#38086;&#38025;&#23433;&#35013;&#21518;&#65292;&#34013;&#33394;&#20135;&#21697;&#24517;&#39035;&#34987;&#36816;&#36755;&#21040;&#20986;&#21475;&#65292;&#32780;&#32511;&#33394;&#20135;&#21697;&#21017;&#24517;&#39035;&#34987;&#36816;&#36755;&#21040;&#23384;&#20648;&#21306;&#12290;&#26412;&#30740;&#31350;&#37325;&#28857;&#20851;&#27880;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#22312;&#35299;&#20915;&#38382;&#39064;&#21644;&#25552;&#39640;&#29983;&#20135;&#25928;&#29575;&#26041;&#38754;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
This application paper explores the potential of using reinforcement learning (RL) to address the demands of Industry 4.0, including shorter time-to-market, mass customization, and batch size one production. Specifically, we present a use case in which the task is to transport and assemble goods through a model factory following predefined rules. Each simulation run involves placing a specific number of goods of random color at the entry point. The objective is to transport the goods to the assembly station, where two rivets are installed in each product, connecting the upper part to the lower part. Following the installation of rivets, blue products must be transported to the exit, while green products are to be transported to storage. The study focuses on the application of reinforcement learning techniques to address this problem and improve the efficiency of the production process.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;NUDGE&#30340;&#31574;&#30053;&#65292;&#21033;&#29992;&#35757;&#32451;&#22909;&#30340;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#20195;&#29702;&#26469;&#24341;&#23548;&#36923;&#36753;&#35268;&#21017;&#30340;&#25628;&#32034;&#65292;&#23454;&#29616;&#20102;&#21487;&#35299;&#37322;&#21644;&#21487;&#35299;&#37322;&#30340;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2306.01439</link><description>&lt;p&gt;
&#36890;&#36807;&#31070;&#32463;&#24341;&#23548;&#31526;&#21495;&#25277;&#35937;&#23454;&#29616;&#21487;&#35299;&#37322;&#21644;&#21487;&#35299;&#37322;&#36923;&#36753;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Interpretable and Explainable Logical Policies via Neurally Guided Symbolic Abstraction. (arXiv:2306.01439v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01439
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;NUDGE&#30340;&#31574;&#30053;&#65292;&#21033;&#29992;&#35757;&#32451;&#22909;&#30340;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#20195;&#29702;&#26469;&#24341;&#23548;&#36923;&#36753;&#35268;&#21017;&#30340;&#25628;&#32034;&#65292;&#23454;&#29616;&#20102;&#21487;&#35299;&#37322;&#21644;&#21487;&#35299;&#37322;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#25152;&#38656;&#35201;&#30340;&#26377;&#38480;&#20808;&#39564;&#20351;&#20854;&#25104;&#20026;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#32534;&#30721;&#21644;&#23398;&#20064;&#31574;&#30053;&#30340;&#20027;&#35201;&#36873;&#25321;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20063;&#26159;&#40657;&#21283;&#23376;&#65292;&#22312;&#24037;&#20316;&#22312;&#22270;&#20687;&#32423;&#21035;&#26102;&#38590;&#20197;&#29702;&#35299;&#20195;&#29702;&#34892;&#20026;&#12290;&#22240;&#27492;&#65292;&#31070;&#32463;&#31526;&#21495;RL&#26088;&#22312;&#39318;&#20808;&#21019;&#24314;&#21487;&#35299;&#37322;&#30340;&#31574;&#30053;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#21487;&#35299;&#37322;&#24615;&#19981;&#24847;&#21619;&#30528;&#21487;&#35299;&#37322;&#24615;&#12290;&#20026;&#20102;&#23454;&#29616;&#35299;&#37322;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#31070;&#32463;&#24341;&#23548;&#21487;&#24494;&#20998;&#36923;&#36753;&#31574;&#30053;&#65288;NUDGE&#65289;&#12290;NUDGE&#21033;&#29992;&#35757;&#32451;&#22909;&#30340;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#20195;&#29702;&#26469;&#24341;&#23548;&#20505;&#36873;&#21152;&#26435;&#36923;&#36753;&#35268;&#21017;&#30340;&#25628;&#32034;&#65292;&#28982;&#21518;&#20351;&#29992;&#21487;&#24494;&#20998;&#30340;&#36923;&#36753;&#26469;&#35757;&#32451;&#36923;&#36753;&#20195;&#29702;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35780;&#20272;&#34920;&#26126;&#65292;NUDGE&#20195;&#29702;&#21487;&#20197;&#20135;&#29983;&#21487;&#35299;&#37322;&#21644;&#21487;&#35299;&#37322;&#30340;&#31574;&#30053;&#65292;&#21516;&#26102;&#32988;&#36807;&#32431;&#31070;&#32463;&#20195;&#29702;&#65292;&#24182;&#23637;&#29616;&#20986;&#33391;&#22909;&#30340;&#28789;&#27963;&#24615;&#65292;&#20197;&#36866;&#24212;&#19981;&#21516;&#21021;&#22987;&#29366;&#24577;&#21644;&#38382;&#39064;&#22823;&#23567;&#30340;&#29615;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;
The limited priors required by neural networks make them the dominating choice to encode and learn policies using reinforcement learning (RL). However, they are also black-boxes, making it hard to understand the agent's behaviour, especially when working on the image level. Therefore, neuro-symbolic RL aims at creating policies that are interpretable in the first place. Unfortunately, interpretability is not explainability. To achieve both, we introduce Neurally gUided Differentiable loGic policiEs (NUDGE). NUDGE exploits trained neural network-based agents to guide the search of candidate-weighted logic rules, then uses differentiable logic to train the logic agents. Our experimental evaluation demonstrates that NUDGE agents can induce interpretable and explainable policies while outperforming purely neural ones and showing good flexibility to environments of different initial states and problem sizes.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20221;&#20851;&#20110;&#32852;&#37030;&#23398;&#20064;&#20013;&#30693;&#35782;&#32534;&#36753;&#30340;&#24191;&#27867;&#35843;&#26597;&#65292;&#24635;&#32467;&#20102;&#26368;&#26032;&#25216;&#26415;&#65292;&#30830;&#23450;&#20102;&#25361;&#25112;&#21644;&#26426;&#36935;&#65292;&#24182;&#27010;&#36848;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2306.01431</link><description>&lt;p&gt;
&#20851;&#20110;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#30693;&#35782;&#32534;&#36753;&#65306;&#23637;&#26395;&#65292;&#25361;&#25112;&#21644;&#26410;&#26469;&#26041;&#21521;&#30340;&#36879;&#35270;
&lt;/p&gt;
&lt;p&gt;
On Knowledge Editing in Federated Learning: Perspectives, Challenges, and Future Directions. (arXiv:2306.01431v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01431
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20221;&#20851;&#20110;&#32852;&#37030;&#23398;&#20064;&#20013;&#30693;&#35782;&#32534;&#36753;&#30340;&#24191;&#27867;&#35843;&#26597;&#65292;&#24635;&#32467;&#20102;&#26368;&#26032;&#25216;&#26415;&#65292;&#30830;&#23450;&#20102;&#25361;&#25112;&#21644;&#26426;&#36935;&#65292;&#24182;&#27010;&#36848;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#24341;&#36215;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#20154;&#20204;&#26222;&#36941;&#35748;&#20026;&#22312;&#23398;&#20064;&#19968;&#31995;&#21015;&#20219;&#21153;&#26102;&#65292;&#22312;&#25972;&#20010;&#26694;&#26550;&#19978;&#31616;&#21333;&#22320;&#24212;&#29992;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#20250;&#23548;&#33268;&#25152;&#35859;&#30340;&#8220;&#28798;&#38590;&#24615;&#36951;&#24536;&#8221;&#12290;&#22240;&#27492;&#65292;&#35768;&#22810;FL&#30740;&#31350;&#38598;&#20013;&#20110;&#35774;&#35745;&#32852;&#37030;&#22686;&#37327;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#20943;&#36731;&#36951;&#24536;&#24182;&#22686;&#21152;&#30693;&#35782;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#36951;&#24536;&#24182;&#19981;&#24635;&#26159;&#26377;&#23475;&#30340;&#12290;&#36873;&#25321;&#24615;&#36951;&#24536;&#65292;&#20063;&#31216;&#20026;&#32852;&#37030;&#36951;&#24536;&#65292;&#21253;&#25324;&#28040;&#38500;&#29305;&#23450;&#30693;&#35782;&#65292;&#21487;&#20197;&#35299;&#20915;&#38544;&#31169;&#38382;&#39064;&#24182;&#20026;&#33719;&#21462;&#26032;&#30693;&#35782;&#21019;&#36896;&#39069;&#22806;&#30340;&#8220;&#31354;&#38388;&#8221;&#12290;&#28982;&#32780;&#65292;&#32570;&#20047;&#24191;&#27867;&#35843;&#26597;&#28085;&#30422;&#26368;&#26032;&#36827;&#23637;&#24182;&#23545;&#27492;&#38382;&#39064;&#36827;&#34892;&#20840;&#38754;&#26816;&#26597;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20221;&#20851;&#20110;&#32852;&#37030;&#23398;&#20064;&#20013;&#30693;&#35782;&#32534;&#36753;&#65288;&#22686;&#24378;/&#21024;&#38500;&#65289;&#30340;&#24191;&#27867;&#35843;&#26597;&#65292;&#26088;&#22312;&#24635;&#32467;&#26368;&#26032;&#25216;&#26415;&#65292;&#30830;&#23450;&#25361;&#25112;&#21644;&#26426;&#36935;&#65292;&#24182;&#27010;&#36848;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
As Federated Learning (FL) has gained increasing attention, it has become widely acknowledged that straightforwardly applying stochastic gradient descent (SGD) on the overall framework when learning over a sequence of tasks results in the phenomenon known as ``catastrophic forgetting''. Consequently, much FL research has centered on devising federated increasing learning methods to alleviate forgetting while augmenting knowledge. On the other hand, forgetting is not always detrimental. The selective amnesia, also known as federated unlearning, which entails the elimination of specific knowledge, can address privacy concerns and create additional ``space'' for acquiring new knowledge. However, there is a scarcity of extensive surveys that encompass recent advancements and provide a thorough examination of this issue. In this manuscript, we present an extensive survey on the topic of knowledge editing (augmentation/removal) in Federated Learning, with the goal of summarizing the state-of
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22312;&#24037;&#19994;&#29615;&#22659;&#20013;&#37096;&#32626;&#24378;&#21270;&#23398;&#20064;&#30340;&#26550;&#26500;&#65292;&#35813;&#26550;&#26500;&#22522;&#20110;OPC UA&#65292;&#32467;&#21512;&#25968;&#23383;&#23402;&#29983;&#30340;&#35774;&#32622;&#25193;&#23637;&#20102;&#26631;&#20934;RL&#29366;&#24577;&#65292;&#24182;&#23450;&#20041;&#20102;&#19968;&#31181;&#20449;&#24687;&#27169;&#22411;&#12290;&#25105;&#20204;&#36890;&#36807;&#21019;&#24314;&#27010;&#24565;&#39564;&#35777;&#24182;&#22312;&#25511;&#21046;&#31995;&#32479;&#20013;&#30830;&#23450;&#26368;&#20248;&#31574;&#30053;&#65292;&#39564;&#35777;&#20102;&#35813;&#26550;&#26500;&#30340;&#21487;&#34892;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.01420</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#22312;&#24037;&#19994;&#29615;&#22659;&#20013;&#37096;&#32626;&#24378;&#21270;&#23398;&#20064;&#30340;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
An Architecture for Deploying Reinforcement Learning in Industrial Environments. (arXiv:2306.01420v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01420
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22312;&#24037;&#19994;&#29615;&#22659;&#20013;&#37096;&#32626;&#24378;&#21270;&#23398;&#20064;&#30340;&#26550;&#26500;&#65292;&#35813;&#26550;&#26500;&#22522;&#20110;OPC UA&#65292;&#32467;&#21512;&#25968;&#23383;&#23402;&#29983;&#30340;&#35774;&#32622;&#25193;&#23637;&#20102;&#26631;&#20934;RL&#29366;&#24577;&#65292;&#24182;&#23450;&#20041;&#20102;&#19968;&#31181;&#20449;&#24687;&#27169;&#22411;&#12290;&#25105;&#20204;&#36890;&#36807;&#21019;&#24314;&#27010;&#24565;&#39564;&#35777;&#24182;&#22312;&#25511;&#21046;&#31995;&#32479;&#20013;&#30830;&#23450;&#26368;&#20248;&#31574;&#30053;&#65292;&#39564;&#35777;&#20102;&#35813;&#26550;&#26500;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24037;&#19994;4.0&#21463;&#21040;&#20102;&#32553;&#30701;&#19978;&#24066;&#26102;&#38388;&#12289;&#20135;&#21697;&#25209;&#37327;&#23450;&#21046;&#20197;&#21450;&#25209;&#37327;&#20026;&#19968;&#29983;&#20135;&#31561;&#38656;&#27714;&#30340;&#25512;&#21160;&#12290;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26159;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#33539;&#20363;&#65292;&#24050;&#32463;&#22312;&#24456;&#22810;&#22797;&#26434;&#20219;&#21153;&#20013;&#26174;&#31034;&#20986;&#20855;&#26377;&#20248;&#20110;&#20154;&#31867;&#30340;&#28508;&#21147;&#65292;&#21487;&#20197;&#24212;&#23545;&#19978;&#36848;&#38656;&#27714;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;OPC UA&#30340;&#25805;&#20316;&#25216;&#26415;&#24863;&#30693;RL&#26550;&#26500;&#65292;&#23427;&#25193;&#23637;&#20102;&#26631;&#20934;RL&#35774;&#32622;&#65292;&#24182;&#23558;&#20854;&#19982;&#25968;&#23383;&#23402;&#29983;&#30340;&#35774;&#32622;&#30456;&#32467;&#21512;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23450;&#20041;&#20102;&#19968;&#31181;OPC UA&#20449;&#24687;&#27169;&#22411;&#65292;&#20801;&#35768;&#37319;&#29992;&#24191;&#20041;&#30340;&#21363;&#25554;&#21363;&#29992;&#26041;&#27861;&#20132;&#25442;&#25152;&#20351;&#29992;&#30340;RL&#20195;&#29702;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#21019;&#24314;&#19968;&#20010;&#27010;&#24565;&#39564;&#35777;&#26469;&#23637;&#31034;&#21644;&#35780;&#20272;&#36825;&#20010;&#26550;&#26500;&#12290;&#36890;&#36807;&#35299;&#20915;&#19968;&#20010;&#29609;&#20855;&#20363;&#23376;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20010;&#26550;&#26500;&#21487;&#20197;&#29992;&#20110;&#20351;&#29992;&#23454;&#38469;&#25511;&#21046;&#31995;&#32479;&#30830;&#23450;&#26368;&#20248;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Industry 4.0 is driven by demands like shorter time-to-market, mass customization of products, and batch size one production. Reinforcement Learning (RL), a machine learning paradigm shown to possess a great potential in improving and surpassing human level performance in numerous complex tasks, allows coping with the mentioned demands. In this paper, we present an OPC UA based Operational Technology (OT)-aware RL architecture, which extends the standard RL setting, combining it with the setting of digital twins. Moreover, we define an OPC UA information model allowing for a generalized plug-and-play like approach for exchanging the RL agent used. In conclusion, we demonstrate and evaluate the architecture, by creating a proof of concept. By means of solving a toy example, we show that this architecture can be used to determine the optimal policy using a real control system.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#23558;&#25968;&#20540;&#25512;&#29702;&#32435;&#20837;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#65292;&#23558;&#25968;&#20540;&#35270;&#20026;&#19968;&#31561;&#20844;&#27665;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23545;&#23454;&#20307;&#21644;&#25968;&#20540;&#36827;&#34892;&#25512;&#29702;&#65292;&#23454;&#29616;&#20102;&#23545;&#22797;&#26434;&#26597;&#35810;&#30340;&#26377;&#25928;&#21644;&#39640;&#25928;&#22238;&#31572;&#12290;</title><link>http://arxiv.org/abs/2306.01399</link><description>&lt;p&gt;
&#23454;&#20307;&#21644;&#25968;&#23383;&#20540;&#30340;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Knowledge Graph Reasoning over Entities and Numerical Values. (arXiv:2306.01399v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01399
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#23558;&#25968;&#20540;&#25512;&#29702;&#32435;&#20837;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#65292;&#23558;&#25968;&#20540;&#35270;&#20026;&#19968;&#31561;&#20844;&#27665;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23545;&#23454;&#20307;&#21644;&#25968;&#20540;&#36827;&#34892;&#25512;&#29702;&#65292;&#23454;&#29616;&#20102;&#23545;&#22797;&#26434;&#26597;&#35810;&#30340;&#26377;&#25928;&#21644;&#39640;&#25928;&#22238;&#31572;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#22797;&#26434;&#36923;&#36753;&#26597;&#35810;&#25351;&#30340;&#26159;&#20197;&#36923;&#36753;&#24418;&#24335;&#34920;&#36798;&#30340;&#26597;&#35810;&#65292;&#20256;&#36798;&#22797;&#26434;&#30340;&#21547;&#20041;&#65292;&#27604;&#22914;&#21152;&#25343;&#22823;&#30340;&#22270;&#28789;&#22870;&#24471;&#20027;&#27605;&#19994;&#20110;&#21738;&#37324;&#65311;&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#30340;&#24212;&#29992;&#31243;&#24207;&#65292;&#20363;&#22914;&#23545;&#35805;&#31995;&#32479;&#21644;&#20132;&#20114;&#24335;&#25628;&#32034;&#24341;&#25806;&#65292;&#20381;&#36182;&#20110;&#22238;&#31572;&#22797;&#26434;&#36923;&#36753;&#26597;&#35810;&#20316;&#20026;&#22522;&#26412;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;&#22312;&#22823;&#22810;&#25968;&#30693;&#35782;&#22270;&#35889;&#20013;&#65292;&#36793;&#32536;&#36890;&#24120;&#29992;&#20110;&#25551;&#36848;&#23454;&#20307;&#20043;&#38388;&#30340;&#20851;&#31995;&#25110;&#23427;&#20204;&#30340;&#20851;&#32852;&#23646;&#24615;&#20540;&#12290;&#23646;&#24615;&#20540;&#21487;&#20197;&#26159;&#20998;&#31867;&#25110;&#25968;&#20540;&#26684;&#24335;&#65292;&#20363;&#22914;&#26085;&#26399;&#12289;&#24180;&#20221;&#12289;&#23610;&#23544;&#31561;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22797;&#26434;&#26597;&#35810;&#31572;&#26696;&#65288;CQA&#65289;&#26041;&#27861;&#20165;&#23558;&#25968;&#20540;&#19982;&#23454;&#20307;&#35270;&#20026;&#30456;&#21516;&#12290;&#36825;&#21487;&#33021;&#20250;&#23548;&#33268;&#22238;&#31572;&#26576;&#20123;&#26597;&#35810;&#30340;&#22256;&#38590;&#65292;&#20363;&#22914;&#21738;&#20010;&#28595;&#22823;&#21033;&#20122;&#30340;&#26222;&#21033;&#31574;&#22870;&#24471;&#20027;&#20986;&#29983;&#20110;1927&#24180;&#20043;&#21069;&#65292;&#21738;&#31181;&#33647;&#29289;&#26159;&#19968;&#31181;&#27490;&#30171;&#21058;&#65292;&#24182;&#19988;&#21103;&#20316;&#29992;&#27604;&#24067;&#27931;&#33452;&#26356;&#23569;&#12290;&#26412;&#25991;&#21463;&#21040;&#25968;&#20540;&#32534;&#30721;&#26368;&#36817;&#30340;&#36827;&#23637;&#30340;&#21551;&#21457;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#25968;&#20540;&#25512;&#29702;&#32435;&#20837;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#25968;&#20540;&#35270;&#20026;&#19968;&#31561;&#20844;&#27665;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23545;&#23454;&#20307;&#21644;&#25968;&#20540;&#36827;&#34892;&#25512;&#29702;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;WIKWD&#21644;WebQSP&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#22238;&#31572;&#28041;&#21450;&#25968;&#20540;&#30340;&#22797;&#26434;&#26597;&#35810;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
A complex logic query in a knowledge graph refers to a query expressed in logic form that conveys a complex meaning, such as where did the Canadian Turing award winner graduate from? Knowledge graph reasoning-based applications, such as dialogue systems and interactive search engines, rely on the ability to answer complex logic queries as a fundamental task. In most knowledge graphs, edges are typically used to either describe the relationships between entities or their associated attribute values. An attribute value can be in categorical or numerical format, such as dates, years, sizes, etc. However, existing complex query answering (CQA) methods simply treat numerical values in the same way as they treat entities. This can lead to difficulties in answering certain queries, such as which Australian Pulitzer award winner is born before 1927, and which drug is a pain reliever and has fewer side effects than Paracetamol. In this work, inspired by the recent advances in numerical encoding
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;GNN&#35757;&#32451;&#31995;&#32479;AdaQP&#65292;&#36890;&#36807;&#38543;&#26426;&#37327;&#21270;&#36328;&#35774;&#22791;&#20256;&#36755;&#30340;&#28040;&#24687;&#20197;&#38477;&#20302;&#36890;&#20449;&#27969;&#37327;&#65292;&#24182;&#25552;&#20513;&#36793;&#32536;&#33410;&#28857;&#21644;&#20013;&#24515;&#33410;&#28857;&#20043;&#38388;&#30340;&#36890;&#20449;-&#35745;&#31639;&#24182;&#34892;&#21270;&#65292;&#20197;&#21152;&#24555;&#20998;&#24067;&#24335;&#20840;&#22270;GNN&#35757;&#32451;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2306.01381</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#28040;&#24687;&#37327;&#21270;&#21644;&#24182;&#34892;&#21270;&#22312;&#20998;&#24067;&#24335;&#20840;&#22270;GNN&#35757;&#32451;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Adaptive Message Quantization and Parallelization for Distributed Full-graph GNN Training. (arXiv:2306.01381v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01381
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;GNN&#35757;&#32451;&#31995;&#32479;AdaQP&#65292;&#36890;&#36807;&#38543;&#26426;&#37327;&#21270;&#36328;&#35774;&#22791;&#20256;&#36755;&#30340;&#28040;&#24687;&#20197;&#38477;&#20302;&#36890;&#20449;&#27969;&#37327;&#65292;&#24182;&#25552;&#20513;&#36793;&#32536;&#33410;&#28857;&#21644;&#20013;&#24515;&#33410;&#28857;&#20043;&#38388;&#30340;&#36890;&#20449;-&#35745;&#31639;&#24182;&#34892;&#21270;&#65292;&#20197;&#21152;&#24555;&#20998;&#24067;&#24335;&#20840;&#22270;GNN&#35757;&#32451;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#22270;&#32593;&#32476;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#30340;&#20998;&#24067;&#24335;&#20840;&#22270;&#35757;&#32451;&#20855;&#26377;&#24102;&#23485;&#38656;&#27714;&#39640;&#21644;&#32791;&#26102;&#38271;&#30340;&#29305;&#28857;&#12290;&#36328;&#35774;&#22791;&#39057;&#32321;&#20132;&#25442;&#33410;&#28857;&#29305;&#24449;&#12289;&#23884;&#20837;&#21644;&#23884;&#20837;&#26799;&#24230;&#65288;&#22343;&#31216;&#20026;&#28040;&#24687;&#65289;&#24102;&#26469;&#20102;&#26174;&#33879;&#30340;&#36890;&#20449;&#24320;&#38144;&#65292;&#23545;&#20110;&#22312;&#20854;&#20182;&#35774;&#22791;&#19978;&#20855;&#26377;&#36828;&#31243;&#37051;&#23621;&#30340;&#33410;&#28857;&#65288;&#36793;&#32536;&#33410;&#28857;&#65289;&#32780;&#35328;&#65292;&#32780;&#23545;&#20110;&#27809;&#26377;&#36828;&#31243;&#37051;&#23621;&#30340;&#33410;&#28857;&#65288;&#20013;&#24515;&#33410;&#28857;&#65289;&#32780;&#35328;&#21017;&#24102;&#26469;&#20102;&#19981;&#24517;&#35201;&#30340;&#31561;&#24453;&#26102;&#38388;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;GNN&#35757;&#32451;&#31995;&#32479;AdaQP&#65292;&#36890;&#36807;&#38543;&#26426;&#37327;&#21270;&#36328;&#35774;&#22791;&#20256;&#36755;&#30340;&#28040;&#24687;&#20197;&#38477;&#20302;&#36890;&#20449;&#27969;&#37327;&#65292;&#24182;&#25552;&#20513;&#36793;&#32536;&#33410;&#28857;&#21644;&#20013;&#24515;&#33410;&#28857;&#20043;&#38388;&#30340;&#36890;&#20449;-&#35745;&#31639;&#24182;&#34892;&#21270;&#65292;&#20197;&#21152;&#24555;&#20998;&#24067;&#24335;&#20840;&#22270;GNN&#35757;&#32451;&#36895;&#24230;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#29702;&#35770;&#20998;&#26512;&#65292;&#35777;&#26126;&#20102;&#24555;&#36895;&#35757;&#32451;&#25910;&#25947;&#65288;&#20197;O&#65288;T ^ {-1}&#65289;&#30340;&#36895;&#29575;&#65292;&#20854;&#20013;T&#20026;&#35757;&#32451;&#21608;&#26399;&#30340;&#24635;&#25968;&#65289;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#37327;&#21270;&#20301;&#23485;&#20998;&#37197;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Distributed full-graph training of Graph Neural Networks (GNNs) over large graphs is bandwidth-demanding and time-consuming. Frequent exchanges of node features, embeddings and embedding gradients (all referred to as messages) across devices bring significant communication overhead for nodes with remote neighbors on other devices (marginal nodes) and unnecessary waiting time for nodes without remote neighbors (central nodes) in the training graph. This paper proposes an efficient GNN training system, AdaQP, to expedite distributed full-graph GNN training. We stochastically quantize messages transferred across devices to lower-precision integers for communication traffic reduction and advocate communication-computation parallelization between marginal nodes and central nodes. We provide theoretical analysis to prove fast training convergence (at the rate of O(T^{-1}) with T being the total number of training epochs) and design an adaptive quantization bit-width assignment scheme for eac
&lt;/p&gt;</description></item><item><title>&#20195;&#30721;&#24322;&#21619;&#26816;&#27979;&#24037;&#20855;&#30340;&#20934;&#30830;&#24615;&#21462;&#20915;&#20110;&#25152;&#20351;&#29992;&#30340;&#25968;&#25454;&#38598;&#65292;&#29616;&#26377;&#25968;&#25454;&#38598;&#23384;&#22312;&#26679;&#26412;&#19981;&#24179;&#34913;&#12289;&#32570;&#20047;&#25903;&#25345;&#20005;&#37325;&#32423;&#21035;&#21644;&#20165;&#38480;&#20110;Java&#35821;&#35328;&#31561;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.01377</link><description>&lt;p&gt;
&#19968;&#20221;&#20851;&#20110;&#20195;&#30721;&#24322;&#21619;&#25968;&#25454;&#38598;&#21644;&#39564;&#35777;&#26426;&#21046;&#30340;&#31995;&#32479;&#25991;&#29486;&#32508;&#36848; (arXiv:2306.01377v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
A systematic literature review on the code smells datasets and validation mechanisms. (arXiv:2306.01377v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01377
&lt;/p&gt;
&lt;p&gt;
&#20195;&#30721;&#24322;&#21619;&#26816;&#27979;&#24037;&#20855;&#30340;&#20934;&#30830;&#24615;&#21462;&#20915;&#20110;&#25152;&#20351;&#29992;&#30340;&#25968;&#25454;&#38598;&#65292;&#29616;&#26377;&#25968;&#25454;&#38598;&#23384;&#22312;&#26679;&#26412;&#19981;&#24179;&#34913;&#12289;&#32570;&#20047;&#25903;&#25345;&#20005;&#37325;&#32423;&#21035;&#21644;&#20165;&#38480;&#20110;Java&#35821;&#35328;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20195;&#30721;&#24322;&#21619;&#26816;&#27979;&#24037;&#20855;&#30340;&#20934;&#30830;&#24615;&#21462;&#20915;&#20110;&#29992;&#20110;&#35780;&#20272;&#36825;&#20123;&#24037;&#20855;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#35843;&#26597;&#20102;45&#20010;&#29616;&#26377;&#25968;&#25454;&#38598;&#65292;&#21457;&#29616;&#25968;&#25454;&#38598;&#30340;&#20805;&#20998;&#24615;&#39640;&#24230;&#21462;&#20915;&#20110;&#30456;&#20851;&#23646;&#24615;&#65292;&#20363;&#22914;&#22823;&#23567;&#65292;&#20005;&#37325;&#32423;&#21035;&#65292;&#39033;&#30446;&#31867;&#22411;&#65292;&#27599;&#31181;&#24322;&#21619;&#30340;&#25968;&#37327;&#65292;&#24322;&#21619;&#30340;&#25968;&#37327;&#65292;&#20197;&#21450;&#25968;&#25454;&#38598;&#20013;&#21457;&#29983;&#24322;&#21619;&#21644;&#38750;&#24322;&#21619;&#26679;&#26412;&#30340;&#27604;&#20363;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#25968;&#25454;&#38598;&#25903;&#25345;God Class&#65292;Long Method&#21644;Feature Envy&#65292;&#32780;Fowler&#21644;Beck&#30446;&#24405;&#20013;&#30340;&#20845;&#31181;&#24322;&#21619;&#27809;&#26377;&#34987;&#20219;&#20309;&#25968;&#25454;&#38598;&#25903;&#25345;&#12290;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65292;&#29616;&#26377;&#25968;&#25454;&#38598;&#23384;&#22312;&#26679;&#26412;&#19981;&#24179;&#34913;&#65292;&#32570;&#20047;&#25903;&#25345;&#20005;&#37325;&#32423;&#21035;&#65292;&#19988;&#20165;&#38480;&#20110;Java&#35821;&#35328;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
The accuracy reported for code smell-detecting tools varies depending on the dataset used to evaluate the tools. Our survey of 45 existing datasets reveals that the adequacy of a dataset for detecting smells highly depends on relevant properties such as the size, severity level, project types, number of each type of smell, number of smells, and the ratio of smelly to non-smelly samples in the dataset. Most existing datasets support God Class, Long Method, and Feature Envy while six smells in Fowler and Beck's catalog are not supported by any datasets. We conclude that existing datasets suffer from imbalanced samples, lack of supporting severity level, and restriction to Java language.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;TSP&#30340;UAV&#36741;&#21161;&#29289;&#32852;&#32593;&#25968;&#25454;&#25910;&#38598;&#26694;&#26550;&#65292;&#36890;&#36807;&#35299;&#31354;&#38388;&#32553;&#20943;&#21644;&#31639;&#27861;&#20248;&#21270;&#65292;&#23454;&#29616;&#20102;&#33021;&#28304;&#39640;&#25928;&#12289;&#20302;&#22797;&#26434;&#24230;&#12289;&#39640;&#25928;&#25910;&#38598;&#20998;&#24067;&#24335;&#20256;&#24863;&#22120;&#25968;&#25454;&#30340;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2306.01355</link><description>&lt;p&gt;
&#22522;&#20110;TSP&#35299;&#31354;&#38388;&#32553;&#20943;&#30340;&#33021;&#28304;&#39640;&#25928;&#30340;&#26080;&#20154;&#26426;&#36741;&#21161;&#29289;&#32852;&#32593;&#25968;&#25454;&#25910;&#38598;
&lt;/p&gt;
&lt;p&gt;
Energy-Efficient UAV-Assisted IoT Data Collection via TSP-Based Solution Space Reduction. (arXiv:2306.01355v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01355
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;TSP&#30340;UAV&#36741;&#21161;&#29289;&#32852;&#32593;&#25968;&#25454;&#25910;&#38598;&#26694;&#26550;&#65292;&#36890;&#36807;&#35299;&#31354;&#38388;&#32553;&#20943;&#21644;&#31639;&#27861;&#20248;&#21270;&#65292;&#23454;&#29616;&#20102;&#33021;&#28304;&#39640;&#25928;&#12289;&#20302;&#22797;&#26434;&#24230;&#12289;&#39640;&#25928;&#25910;&#38598;&#20998;&#24067;&#24335;&#20256;&#24863;&#22120;&#25968;&#25454;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#32447;&#25968;&#25454;&#25910;&#38598;&#26694;&#26550;&#65292;&#23427;&#21033;&#29992;&#26080;&#20154;&#26426;(UAV)&#39640;&#25928;&#22320;&#20174;&#37096;&#32626;&#22312;&#22823;&#33539;&#22260;&#20869;&#30340;&#20998;&#24067;&#24335;&#29289;&#32852;&#32593;&#20256;&#24863;&#22120;&#20013;&#25910;&#38598;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#32771;&#34385;&#21040;&#20256;&#24863;&#22120;&#30340;&#38750;&#38646;&#36890;&#20449;&#33539;&#22260;&#65292;&#20197;&#20248;&#21270;UAV&#30340;&#39134;&#34892;&#36335;&#24452;&#65292;&#20174;&#32780;&#20135;&#29983;&#20102;TSP&#21464;&#20307;&#12290;&#25105;&#20204;&#22312;&#25968;&#23398;&#19978;&#35777;&#26126;&#20102;&#36825;&#31181;TSP&#21464;&#22411;&#38382;&#39064;&#30340;&#26368;&#20248;&#33322;&#28857;&#26159;&#21463;&#38480;&#20110;&#20256;&#24863;&#22120;&#36890;&#20449;&#33539;&#22260;&#30340;&#36793;&#30028;&#65292;&#26497;&#22823;&#22320;&#32553;&#23567;&#20102;&#35299;&#31354;&#38388;&#12290;&#22522;&#20110;&#36825;&#20010;&#21457;&#29616;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#20302;&#22797;&#26434;&#24230;&#30340;UAV&#36741;&#21161;&#20256;&#24863;&#22120;&#25968;&#25454;&#25910;&#38598;&#31639;&#27861;&#65292;&#24182;&#22312;&#36873;&#25321;&#30340;&#29992;&#20363;&#20013;&#23637;&#31034;&#20102;&#23427;&#30340;&#26377;&#25928;&#24615;&#65292;&#22312;&#32852;&#21512;&#20248;&#21270;UAV&#30340;&#34892;&#39542;&#36317;&#31163;&#21644;&#20256;&#24863;&#22120;&#30340;&#36890;&#20449;&#33539;&#22260;&#30340;&#21516;&#26102;&#26368;&#23567;&#21270;UAV&#21644;&#20256;&#24863;&#22120;&#30340;&#24635;&#33021;&#37327;&#28040;&#32791;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a wireless data collection framework that employs an unmanned aerial vehicle (UAV) to efficiently gather data from distributed IoT sensors deployed in a large area. Our approach takes into account the non-zero communication ranges of the sensors to optimize the flight path of the UAV, resulting in a variation of the Traveling Salesman Problem (TSP). We prove mathematically that the optimal waypoints for this TSP-variant problem are restricted to the boundaries of the sensor communication ranges, greatly reducing the solution space. Building on this finding, we develop a low-complexity UAV-assisted sensor data collection algorithm, and demonstrate its effectiveness in a selected use case where we minimize the total energy consumption of the UAV and sensors by jointly optimizing the UAV's travel distance and the sensors' communication ranges.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35843;&#26597;&#20102;&#32852;&#37030;&#39046;&#22495;&#27867;&#21270;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#25552;&#21040;&#20102;&#32852;&#37030;&#23398;&#20064;&#21644;&#39046;&#22495;&#27867;&#21270;&#25216;&#26415;&#30340;&#20248;&#21183;&#65292;&#20197;&#21450;&#22312;&#27867;&#21270;&#32852;&#37030;&#27169;&#22411;&#26102;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2306.01334</link><description>&lt;p&gt;
&#32852;&#37030;&#39046;&#22495;&#27867;&#21270;&#65306;&#19968;&#39033;&#35843;&#26597;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Federated Domain Generalization: A Survey. (arXiv:2306.01334v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01334
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35843;&#26597;&#20102;&#32852;&#37030;&#39046;&#22495;&#27867;&#21270;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#25552;&#21040;&#20102;&#32852;&#37030;&#23398;&#20064;&#21644;&#39046;&#22495;&#27867;&#21270;&#25216;&#26415;&#30340;&#20248;&#21183;&#65292;&#20197;&#21450;&#22312;&#27867;&#21270;&#32852;&#37030;&#27169;&#22411;&#26102;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#36890;&#24120;&#20381;&#36182;&#20110;&#19968;&#20010;&#20551;&#35774;&#65292;&#21363;&#35757;&#32451;&#21644;&#27979;&#35797;&#30340;&#20998;&#24067;&#26159;&#30456;&#21516;&#30340;&#65292;&#25968;&#25454;&#26159;&#38598;&#20013;&#23384;&#20648;&#20379;&#35757;&#32451;&#21644;&#27979;&#35797;&#20043;&#29992;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#65292;&#20998;&#24067;&#21487;&#33021;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#65292;&#24182;&#19988;&#25968;&#25454;&#36890;&#24120;&#20998;&#24067;&#22312;&#19981;&#21516;&#30340;&#35774;&#22791;&#12289;&#32452;&#32455;&#25110;&#36793;&#32536;&#33410;&#28857;&#19978;&#12290;&#22240;&#27492;&#65292;&#24517;&#39035;&#24320;&#21457;&#33021;&#22815;&#26377;&#25928;&#27867;&#21270;&#21040;&#26410;&#35265;&#36807;&#30340;&#20998;&#24067;&#65292;&#24182;&#19988;&#25968;&#25454;&#20998;&#24067;&#22312;&#19981;&#21516;&#39046;&#22495;&#30340;&#27169;&#22411;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#36817;&#24180;&#26469;&#20986;&#29616;&#20102;&#23545;&#32852;&#37030;&#39046;&#22495;&#27867;&#21270; (FDG) &#30340;&#26497;&#22823;&#20852;&#36259;&#12290;FDG &#32467;&#21512;&#20102;&#32852;&#37030;&#23398;&#20064; (FL) &#21644;&#39046;&#22495;&#27867;&#21270; (DG) &#25216;&#26415;&#30340;&#20248;&#28857;&#65292;&#20351;&#22810;&#20010;&#28304;&#39046;&#22495;&#33021;&#22815;&#21327;&#20316;&#23398;&#20064;&#19968;&#20010;&#33021;&#22815;&#30452;&#25509;&#27867;&#21270;&#21040;&#26410;&#35265;&#36807;&#30340;&#39046;&#22495;&#32780;&#21448;&#20445;&#25345;&#25968;&#25454;&#38544;&#31169;&#30340;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#22312;&#39046;&#22495;&#36716;&#31227;&#19979;&#27867;&#21270;&#32852;&#37030;&#27169;&#22411;&#26159;&#19968;&#20010;&#25216;&#26415;&#19978;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#30446;&#21069;&#36824;&#27809;&#26377;&#24471;&#21040;&#20805;&#20998;&#30340;&#20851;&#27880;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning typically relies on the assumption that training and testing distributions are identical and that data is centrally stored for training and testing. However, in real-world scenarios, distributions may differ significantly and data is often distributed across different devices, organizations, or edge nodes. Consequently, it is imperative to develop models that can effectively generalize to unseen distributions where data is distributed across different domains. In response to this challenge, there has been a surge of interest in federated domain generalization (FDG) in recent years. FDG combines the strengths of federated learning (FL) and domain generalization (DG) techniques to enable multiple source domains to collaboratively learn a model capable of directly generalizing to unseen domains while preserving data privacy. However, generalizing the federated model under domain shifts is a technically challenging problem that has received scant attention in the research 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#25918;&#23556;&#23398;AI&#20013;&#30340;&#20844;&#24179;&#24615;&#38382;&#39064;&#65292;&#20171;&#32461;&#20102;Aequitas&#20559;&#24046;&#23457;&#35745;&#24037;&#20855;&#21253;&#24182;&#38416;&#26126;&#20102;&#20844;&#24179;&#24615;&#35780;&#20272;&#30340;&#22522;&#26412;&#25351;&#26631;&#65292;&#24378;&#35843;&#20102;&#32771;&#34385;AI&#20262;&#29702;&#24433;&#21709;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.01333</link><description>&lt;p&gt;
&#25918;&#23556;&#23398;AI&#20013;&#30340;&#20844;&#24179;&#24615;&#23548;&#33322;&#65306;&#27010;&#24565;&#12289;&#21518;&#26524;&#21644;&#20851;&#38190;&#32771;&#34385;
&lt;/p&gt;
&lt;p&gt;
Navigating Fairness in Radiology AI: Concepts, Consequences,and Crucial Considerations. (arXiv:2306.01333v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01333
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#25918;&#23556;&#23398;AI&#20013;&#30340;&#20844;&#24179;&#24615;&#38382;&#39064;&#65292;&#20171;&#32461;&#20102;Aequitas&#20559;&#24046;&#23457;&#35745;&#24037;&#20855;&#21253;&#24182;&#38416;&#26126;&#20102;&#20844;&#24179;&#24615;&#35780;&#20272;&#30340;&#22522;&#26412;&#25351;&#26631;&#65292;&#24378;&#35843;&#20102;&#32771;&#34385;AI&#20262;&#29702;&#24433;&#21709;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#24050;&#32463;&#22312;&#25918;&#23556;&#23398;&#39046;&#22495;&#21462;&#24471;&#20102;&#37325;&#22823;&#38761;&#21629;&#24615;&#36827;&#23637;&#65292;&#25215;&#35834;&#25913;&#21892;&#24739;&#32773;&#32467;&#26524;&#21644;&#31616;&#21270;&#27969;&#31243;&#12290;&#28982;&#32780;&#65292;&#30830;&#20445;AI&#27169;&#22411;&#30340;&#20844;&#24179;&#24615;&#33267;&#20851;&#37325;&#35201;&#65292;&#20197;&#38450;&#27490;&#38544;&#34109;&#30340;&#20559;&#35265;&#21644;&#19981;&#24179;&#31561;&#29616;&#35937;&#23548;&#33268;&#19981;&#24179;&#31561;&#32467;&#26524;&#12290;&#26412;&#32508;&#36848;&#35752;&#35770;&#20102;AI&#20013;&#30340;&#20844;&#24179;&#27010;&#24565;&#65292;&#37325;&#28857;&#35752;&#35770;&#20351;&#29992;Aequitas&#24037;&#20855;&#21253;&#36827;&#34892;&#20559;&#24046;&#23457;&#35745;&#21450;&#20854;&#22312;&#25918;&#23556;&#23398;&#20013;&#30340;&#23454;&#38469;&#24433;&#21709;&#65292;&#29305;&#21035;&#26159;&#22312;&#30142;&#30149;&#31579;&#26597;&#22330;&#26223;&#20013;&#30340;&#24433;&#21709;&#12290;Aequitas&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;&#20559;&#24046;&#23457;&#35745;&#24037;&#20855;&#21253;&#65292;&#23457;&#26597;AI&#27169;&#22411;&#30340;&#20915;&#31574;&#65292;&#35782;&#21035;&#21487;&#33021;&#23548;&#33268;&#19981;&#21516;&#20154;&#21475;&#32676;&#20307;&#21644;&#25104;&#20687;&#35774;&#22791;&#21697;&#29260;&#20043;&#38388;&#24046;&#24322;&#30340;&#38544;&#34255;&#20559;&#35265;&#12290;&#35813;&#24037;&#20855;&#21253;&#22522;&#20110;&#32479;&#35745;&#29702;&#35770;&#65292;&#20998;&#26512;&#22823;&#22411;&#25968;&#25454;&#38598;&#20197;&#25581;&#31034;&#27169;&#22411;&#30340;&#20844;&#24179;&#24615;&#12290;&#23427;&#22312;&#21516;&#26102;&#22788;&#29702;&#22810;&#31181;&#21464;&#37327;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#29305;&#21035;&#26159;&#22312;&#25918;&#23556;&#23398;&#36825;&#26679;&#30340;&#22810;&#20803;&#21270;&#39046;&#22495;&#20013;&#12290;&#26412;&#32508;&#36848;&#38416;&#26126;&#20102;&#20844;&#24179;&#24615;&#35780;&#20272;&#30340;&#22522;&#26412;&#25351;&#26631;&#65306;&#24179;&#31561;&#21644;&#27604;&#20363;&#24179;&#31561;&#12289;&#20551;&#38451;&#24615;&#29575;&#24179;&#31561;&#12289;&#20551;&#38452;&#24615;&#29575;&#24179;&#31561;&#21644;&#39044;&#27979;&#24179;&#31561;&#12290;&#20316;&#32773;&#36824;&#24378;&#35843;&#20102;&#22312;&#25918;&#23556;&#23398;AI&#20013;&#32771;&#34385;&#20262;&#29702;&#24433;&#21709;&#30340;&#37325;&#35201;&#24615;&#65292;&#21253;&#25324;&#23545;&#24739;&#32773;&#36896;&#25104;&#20260;&#23475;&#30340;&#39118;&#38505;&#20197;&#21450;&#22312;AI&#20915;&#31574;&#36807;&#31243;&#20013;&#38656;&#35201;&#36879;&#26126;&#24230;&#30340;&#24517;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial Intelligence (AI) has significantly revolutionized radiology, promising improved patient outcomes and streamlined processes. However, it's critical to ensure the fairness of AI models to prevent stealthy bias and disparities from leading to unequal outcomes. This review discusses the concept of fairness in AI, focusing on bias auditing using the Aequitas toolkit, and its real-world implications in radiology, particularly in disease screening scenarios. Aequitas, an open-source bias audit toolkit, scrutinizes AI models' decisions, identifying hidden biases that may result in disparities across different demographic groups and imaging equipment brands. This toolkit operates on statistical theories, analyzing a large dataset to reveal a model's fairness. It excels in its versatility to handle various variables simultaneously, especially in a field as diverse as radiology. The review explicates essential fairness metrics: Equal and Proportional Parity, False Positive Rate Parity
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35777;&#26126;GNN&#22312;&#21516;&#26500;&#22270;&#20013;&#30340;&#21516;&#26500;&#33410;&#28857;&#21644;&#24322;&#26500;&#22270;&#20013;&#30340;&#24322;&#26500;&#33410;&#28857;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#32780;&#22312;&#21478;&#19968;&#32452;&#33410;&#28857;&#19978;&#34920;&#29616;&#19981;&#20339;&#12290;&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#21152;&#26435;&#32858;&#21512;&#25552;&#39640;GNN&#22312;&#19981;&#21516;&#32467;&#26500;&#27169;&#24335;&#33410;&#28857;&#19978;&#30340;&#34920;&#29616;&#65292;&#26377;&#25928;&#35299;&#20915;&#32467;&#26500;&#24046;&#24322;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.01323</link><description>&lt;p&gt;
&#25581;&#31034;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#32467;&#26500;&#24046;&#24322;&#24615;&#65306;&#19968;&#20010;&#23610;&#30721;&#36866;&#29992;&#20110;&#25152;&#26377;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Demystifying Structural Disparity in Graph Neural Networks: Can One Size Fit All?. (arXiv:2306.01323v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01323
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35777;&#26126;GNN&#22312;&#21516;&#26500;&#22270;&#20013;&#30340;&#21516;&#26500;&#33410;&#28857;&#21644;&#24322;&#26500;&#22270;&#20013;&#30340;&#24322;&#26500;&#33410;&#28857;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#32780;&#22312;&#21478;&#19968;&#32452;&#33410;&#28857;&#19978;&#34920;&#29616;&#19981;&#20339;&#12290;&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#21152;&#26435;&#32858;&#21512;&#25552;&#39640;GNN&#22312;&#19981;&#21516;&#32467;&#26500;&#27169;&#24335;&#33410;&#28857;&#19978;&#30340;&#34920;&#29616;&#65292;&#26377;&#25928;&#35299;&#20915;&#32467;&#26500;&#24046;&#24322;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#20851;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#23454;&#35777;&#21644;&#29702;&#35770;&#35777;&#25454;&#65292;&#25903;&#25345;&#23427;&#20204;&#22312;&#25429;&#25417;&#21516;&#26500;&#21644;&#26576;&#20123;&#24322;&#26500;&#22270;&#19978;&#30340;&#32467;&#26500;&#27169;&#24335;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#22823;&#22810;&#25968;&#23454;&#38469;&#20013;&#30340;&#21516;&#26500;&#21644;&#24322;&#26500;&#22270;&#37117;&#30001;&#21516;&#26500;&#21644;&#24322;&#26500;&#32467;&#26500;&#27169;&#24335;&#30340;&#28151;&#21512;&#33410;&#28857;&#32452;&#25104;&#65292;&#34920;&#29616;&#20986;&#19968;&#23450;&#30340;&#32467;&#26500;&#24046;&#24322;&#24615;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#19981;&#21516;&#32467;&#26500;&#27169;&#24335;&#19979;&#30340;&#33410;&#28857;&#65288;&#20363;&#22914;&#22312;&#24322;&#26500;&#22270;&#20013;&#30340;&#21516;&#26500;&#33410;&#28857;&#65289;&#22312;GNN&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#20998;&#26512;&#20173;&#28982;&#24456;&#26377;&#38480;&#12290;&#26412;&#25991;&#36890;&#36807;&#29702;&#35770;&#21644;&#23454;&#35777;&#30740;&#31350;&#35777;&#26126;&#65292;GNN&#22312;&#21516;&#26500;&#22270;&#20013;&#30340;&#21516;&#26500;&#33410;&#28857;&#21644;&#24322;&#26500;&#22270;&#20013;&#30340;&#24322;&#26500;&#33410;&#28857;&#19978;&#30340;&#34920;&#29616;&#36890;&#24120;&#26159;&#20986;&#33394;&#30340;&#65292;&#32780;&#22312;&#21478;&#19968;&#32452;&#33410;&#28857;&#19978;&#34920;&#29616;&#19981;&#20339;&#65292;&#34920;&#29616;&#20986;&#24615;&#33021;&#24046;&#24322;&#24615;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35782;&#21035;&#20102;&#27979;&#35797;&#23637;&#31034;&#19981;&#21516;&#32467;&#26500;&#27169;&#24335;&#33410;&#28857;&#26102;GNN&#30340;&#25928;&#24212;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20351;&#29992;GNN&#30340;&#21152;&#26435;&#32858;&#21512;&#20197;&#36866;&#24212;&#24615;&#32467;&#26500;&#24046;&#24322;&#24615;&#30340;&#26032;&#26694;&#26550;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#35299;&#20915;&#32467;&#26500;&#24046;&#24322;&#24615;&#21644;&#25552;&#39640;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#30340;&#24615;&#33021;&#26041;&#38754;&#26159;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent studies on Graph Neural Networks(GNNs) provide both empirical and theoretical evidence supporting their effectiveness in capturing structural patterns on both homophilic and certain heterophilic graphs. Notably, most real-world homophilic and heterophilic graphs are comprised of a mixture of nodes in both homophilic and heterophilic structural patterns, exhibiting a structural disparity. However, the analysis of GNN performance with respect to nodes exhibiting different structural patterns, e.g., homophilic nodes in heterophilic graphs, remains rather limited. In the present study, we provide evidence that Graph Neural Networks(GNNs) on node classification typically perform admirably on homophilic nodes within homophilic graphs and heterophilic nodes within heterophilic graphs while struggling on the opposite node set, exhibiting a performance disparity. We theoretically and empirically identify effects of GNNs on testing nodes exhibiting distinct structural patterns. We then pr
&lt;/p&gt;</description></item><item><title>EPIC&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25554;&#20540;&#30340;&#26041;&#27861;&#26469;&#22686;&#24378;&#22270;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#21033;&#29992;&#22270;&#32534;&#36753;&#36317;&#31163;&#29983;&#25104;&#19982;&#21407;&#22987;&#22270;&#30456;&#20284;&#20294;&#26377;&#32467;&#26500;&#21464;&#21270;&#30340;&#26032;&#22270;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#20998;&#31867;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.01310</link><description>&lt;p&gt;
EPIC: &#36890;&#36807;&#21487;&#23398;&#20064;&#30340;&#20195;&#20215;&#23454;&#29616;&#30340;&#32534;&#36753;&#36335;&#24452;&#25554;&#20540;&#30340;&#22270;&#24418;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
EPIC: Graph Augmentation with Edit Path Interpolation via Learnable Cost. (arXiv:2306.01310v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01310
&lt;/p&gt;
&lt;p&gt;
EPIC&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25554;&#20540;&#30340;&#26041;&#27861;&#26469;&#22686;&#24378;&#22270;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#21033;&#29992;&#22270;&#32534;&#36753;&#36317;&#31163;&#29983;&#25104;&#19982;&#21407;&#22987;&#22270;&#30456;&#20284;&#20294;&#26377;&#32467;&#26500;&#21464;&#21270;&#30340;&#26032;&#22270;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#20998;&#31867;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22270;&#30340;&#27169;&#22411;&#22312;&#21508;&#20010;&#39046;&#22495;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#65292;&#20294;&#29616;&#26377;&#22270;&#25968;&#25454;&#38598;&#30340;&#26377;&#38480;&#35268;&#27169;&#21644;&#22810;&#26679;&#24615;&#32463;&#24120;&#38480;&#21046;&#23427;&#20204;&#30340;&#24615;&#33021;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;EPIC&#65288;&#36890;&#36807;&#21487;&#23398;&#20064;&#30340;&#20195;&#20215;&#23454;&#29616;&#30340;&#32534;&#36753;&#36335;&#24452;&#25554;&#20540;&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#25554;&#20540;&#30340;&#22686;&#24378;&#22270;&#25968;&#25454;&#38598;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;&#22270;&#32534;&#36753;&#36317;&#31163;&#26469;&#29983;&#25104;&#19982;&#21407;&#22987;&#22270;&#30456;&#20284;&#20294;&#32467;&#26500;&#26377;&#25152;&#21464;&#21270;&#30340;&#26032;&#22270;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#36890;&#36807;&#27604;&#36739;&#24102;&#26631;&#31614;&#30340;&#22270;&#26469;&#23398;&#20064;&#22270;&#32534;&#36753;&#36317;&#31163;&#65292;&#24182;&#21033;&#29992;&#36825;&#19968;&#30693;&#35782;&#22312;&#21407;&#22987;&#22270;&#23545;&#20043;&#38388;&#21019;&#24314;&#20102;&#22270;&#32534;&#36753;&#36335;&#24452;&#12290;&#36890;&#36807;&#20174;&#22270;&#32534;&#36753;&#36335;&#24452;&#20013;&#38543;&#26426;&#25277;&#26679;&#30340;&#22270;&#24418;&#65292;&#25105;&#20204;&#20016;&#23500;&#20102;&#35757;&#32451;&#38598;&#20197;&#22686;&#24378;&#20998;&#31867;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#34920;&#26126;&#23427;&#22312;&#22270;&#20998;&#31867;&#20219;&#21153;&#20013;&#20248;&#20110;&#29616;&#26377;&#30340;&#22686;&#24378;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph-based models have become increasingly important in various domains, but the limited size and diversity of existing graph datasets often limit their performance. To address this issue, we propose EPIC (Edit Path Interpolation via learnable Cost), a novel interpolation-based method for augmenting graph datasets. Our approach leverages graph edit distance to generate new graphs that are similar to the original ones but exhibit some variation in their structures. To achieve this, we learn the graph edit distance through a comparison of labeled graphs and utilize this knowledge to create graph edit paths between pairs of original graphs. With randomly sampled graphs from a graph edit path, we enrich the training set to enhance the generalization capability of classification models. We demonstrate the effectiveness of our approach on several benchmark datasets and show that it outperforms existing augmentation methods in graph classification tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#25105;&#20013;&#24515;&#35268;&#21010;&#30340;&#26041;&#27861;&#65292;&#32467;&#21512;&#31526;&#21495;&#35268;&#21010;&#21644;&#38754;&#21521;&#23545;&#35937;&#30340;POMDP&#27169;&#22411;&#65292;&#25104;&#20026;&#39640;&#25193;&#23637;&#24615;&#20219;&#21153;&#23436;&#25104;&#30340;&#19968;&#31181;&#26032;&#36884;&#24452;&#12290;</title><link>http://arxiv.org/abs/2306.01295</link><description>&lt;p&gt;
&#39640;&#25193;&#23637;&#24615;&#20219;&#21153;&#23436;&#25104;&#30340;&#33258;&#25105;&#20013;&#24515;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Egocentric Planning for Scalable Embodied Task Achievement. (arXiv:2306.01295v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01295
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#25105;&#20013;&#24515;&#35268;&#21010;&#30340;&#26041;&#27861;&#65292;&#32467;&#21512;&#31526;&#21495;&#35268;&#21010;&#21644;&#38754;&#21521;&#23545;&#35937;&#30340;POMDP&#27169;&#22411;&#65292;&#25104;&#20026;&#39640;&#25193;&#23637;&#24615;&#20219;&#21153;&#23436;&#25104;&#30340;&#19968;&#31181;&#26032;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19981;&#21516;&#29615;&#22659;&#20013;&#23436;&#25104;&#20219;&#21153;&#23545;&#20110;&#37319;&#21462;&#34892;&#21160;&#30340;&#26426;&#22120;&#20154;&#20195;&#29702;&#26469;&#35828;&#38754;&#20020;&#30528;&#37325;&#22823;&#25361;&#25112;&#65292;&#23588;&#20854;&#26159;&#22312;&#23545;&#35937;&#31867;&#22411;&#19978;&#30340;&#27867;&#21270;&#20197;&#21450;&#25191;&#34892;&#36866;&#24403;&#30340;&#34892;&#21160;&#20197;&#23436;&#25104;&#20219;&#21153;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#33258;&#25105;&#20013;&#24515;&#35268;&#21010;&#65292;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#23558;&#31526;&#21495;&#35268;&#21010;&#21644;&#38754;&#21521;&#23545;&#35937;&#30340;POMDP&#32467;&#21512;&#36215;&#26469;&#65292;&#20197;&#35299;&#20915;&#22797;&#26434;&#29615;&#22659;&#19979;&#30340;&#20219;&#21153;&#65292;&#21033;&#29992;&#29616;&#26377;&#30340;&#35270;&#35273;&#24863;&#30693;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;ALFRED&#65288;&#19968;&#20010;&#27169;&#25311;&#30340;&#23478;&#24237;&#20219;&#21153;&#29615;&#22659;&#65289;&#20013;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#39640;&#21487;&#25193;&#23637;&#24615;&#65292;&#22312;ALFRED&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#24778;&#20154;&#30340;36.07%&#30340;&#26410;&#35265;&#36807;&#30340;&#25104;&#21151;&#29575;&#65292;&#24182;&#22312;CVPR Emobodied AI workshop&#20013;&#33719;&#24471;&#20102;ALFRED&#25361;&#25112;&#36187;&#30340;&#32988;&#21033;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#38656;&#35201;&#21487;&#38752;&#30340;&#24863;&#30693;&#65292;&#24182;&#35268;&#23450;&#25110;&#23398;&#20064;&#20851;&#20110;&#20195;&#29702;&#30340;&#34892;&#21160;&#21069;&#25552;&#21644;&#25928;&#26524;&#65292;&#20197;&#21450;&#23545;&#35937;&#31867;&#22411;&#30340;&#31526;&#21495;&#25551;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
Embodied agents face significant challenges when tasked with performing actions in diverse environments, particularly in generalizing across object types and executing suitable actions to accomplish tasks. Furthermore, agents should exhibit robustness, minimizing the execution of illegal actions. In this work, we present Egocentric Planning, an innovative approach that combines symbolic planning and Object-oriented POMDPs to solve tasks in complex environments, harnessing existing models for visual perception and natural language processing. We evaluated our approach in ALFRED, a simulated environment designed for domestic tasks, and demonstrated its high scalability, achieving an impressive 36.07% unseen success rate in the ALFRED benchmark and winning the ALFRED challenge at CVPR Embodied AI workshop. Our method requires reliable perception and the specification or learning of a symbolic description of the preconditions and effects of the agent's actions, as well as what object types
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28201;&#24230;&#37319;&#26679;&#31639;&#27861;&#65292;&#36890;&#36807;KL-&#25955;&#24230;&#24341;&#23548;&#21160;&#24577;&#35843;&#25972;&#28201;&#24230;&#65292;&#20174;&#32780;&#32531;&#35299;&#22810;&#26679;&#24615;&#21644;&#21487;&#24402;&#22240;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#31639;&#27861;&#22312;&#23545;&#35805;&#38382;&#31572;&#21644;&#25688;&#35201;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>http://arxiv.org/abs/2306.01286</link><description>&lt;p&gt;
KL-Divergence&#24341;&#23548;&#19979;&#30340;&#28201;&#24230;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
KL-Divergence Guided Temperature Sampling. (arXiv:2306.01286v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01286
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28201;&#24230;&#37319;&#26679;&#31639;&#27861;&#65292;&#36890;&#36807;KL-&#25955;&#24230;&#24341;&#23548;&#21160;&#24577;&#35843;&#25972;&#28201;&#24230;&#65292;&#20174;&#32780;&#32531;&#35299;&#22810;&#26679;&#24615;&#21644;&#21487;&#24402;&#22240;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#31639;&#27861;&#22312;&#23545;&#35805;&#38382;&#31572;&#21644;&#25688;&#35201;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28201;&#24230;&#37319;&#26679;&#26159;&#19968;&#31181;&#24120;&#35268;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#27979;&#22810;&#26679;&#21270;&#12290;&#38543;&#30528;&#28201;&#24230;&#30340;&#21319;&#39640;&#65292;&#39044;&#27979;&#21464;&#24471;&#26356;&#21152;&#22810;&#26679;&#21270;&#65292;&#20294;&#20063;&#23481;&#26131;&#20135;&#29983;&#24187;&#35273;&#8212;&#8212;&#29983;&#25104;&#30475;&#20284;&#21512;&#29702;&#20294;&#19981;&#27491;&#30830;&#30340;&#20196;&#29260;&#12290;&#32531;&#35299;&#24187;&#35273;&#30340;&#19968;&#31181;&#24120;&#35265;&#26041;&#27861;&#26159;&#25552;&#20379;&#28304;/&#22522;&#30784;&#25991;&#26723;&#65292;&#24182;&#20351;&#27169;&#22411;&#35757;&#32451;&#29983;&#25104;&#19982;&#25552;&#20379;&#30340;&#26469;&#28304;&#30456;&#20851;&#19988;&#21487;&#24402;&#22240;&#30340;&#39044;&#27979;&#12290;&#30475;&#26469;&#23384;&#22312;&#22810;&#26679;&#24615;&#21644;&#21487;&#24402;&#22240;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#31181;&#26435;&#34913;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26494;&#24347;&#22266;&#23450;&#28201;&#24230;&#21644;&#36890;&#36807;KL-&#25955;&#24230;&#26681;&#25454;&#20854;&#19982;&#28304;&#30340;&#30456;&#20851;&#24615;&#24341;&#23548;&#21160;&#24577;&#28201;&#24230;&#30340;&#26426;&#21046;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#23454;&#20102;&#36825;&#31181;&#26435;&#34913;&#65292;&#24182;&#34920;&#26126;&#25105;&#20204;&#30340;&#37319;&#26679;&#31639;&#27861;&#22312;&#23545;&#35805;&#38382;&#31572;&#21644;&#25688;&#35201;&#20219;&#21153;&#20013;&#20248;&#20110;&#24120;&#35268;&#30340;top-k&#21644;top-p&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Temperature sampling is a conventional approach to diversify large language model predictions. As temperature increases, the prediction becomes diverse but also vulnerable to hallucinations -- generating tokens that are sensible but not factual. One common approach to mitigate hallucinations is to provide source/grounding documents and the model is trained to produce predictions that bind to and are attributable to the provided source. It appears that there is a trade-off between diversity and attribution. To mitigate any such trade-off, we propose to relax the constraint of having a fixed temperature over decoding steps, and a mechanism to guide the dynamic temperature according to its relevance to the source through KL-divergence. Our experiments justifies the trade-off, and shows that our sampling algorithm outperforms the conventional top-k and top-p algorithms in conversational question-answering and summarization tasks.
&lt;/p&gt;</description></item><item><title>&#26234;&#33021;&#20132;&#36890;&#31995;&#32479;&#65288;ITS&#65289;&#26159;&#29616;&#20195;&#20132;&#36890;&#22522;&#30784;&#35774;&#26045;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#24050;&#20986;&#29616;&#22522;&#20110;&#22797;&#26434;&#25968;&#25454;&#30340;&#25968;&#25454;&#39537;&#21160;&#35299;&#20915;&#26041;&#26696;&#20197;&#35299;&#20915;&#21508;&#31181;&#19982;ITS&#30456;&#20851;&#30340;&#25361;&#25112;&#12290;&#22270;&#24418;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#25104;&#20026;&#20102;&#26234;&#33021;&#20132;&#36890;&#31995;&#32479;&#39046;&#22495;&#30340;&#26085;&#30410;&#37325;&#35201;&#30340;&#30740;&#31350;&#37325;&#28857;&#12290;</title><link>http://arxiv.org/abs/2306.01282</link><description>&lt;p&gt;
&#26234;&#33021;&#22478;&#24066;&#20132;&#36890;&#31995;&#32479;&#20013;&#22522;&#20110;&#22270;&#30340;&#26426;&#22120;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent Advances in Graph-based Machine Learning for Applications in Smart Urban Transportation Systems. (arXiv:2306.01282v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01282
&lt;/p&gt;
&lt;p&gt;
&#26234;&#33021;&#20132;&#36890;&#31995;&#32479;&#65288;ITS&#65289;&#26159;&#29616;&#20195;&#20132;&#36890;&#22522;&#30784;&#35774;&#26045;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#24050;&#20986;&#29616;&#22522;&#20110;&#22797;&#26434;&#25968;&#25454;&#30340;&#25968;&#25454;&#39537;&#21160;&#35299;&#20915;&#26041;&#26696;&#20197;&#35299;&#20915;&#21508;&#31181;&#19982;ITS&#30456;&#20851;&#30340;&#25361;&#25112;&#12290;&#22270;&#24418;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#25104;&#20026;&#20102;&#26234;&#33021;&#20132;&#36890;&#31995;&#32479;&#39046;&#22495;&#30340;&#26085;&#30410;&#37325;&#35201;&#30340;&#30740;&#31350;&#37325;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26234;&#33021;&#20132;&#36890;&#31995;&#32479;&#65288;ITS&#65289;&#26159;&#29616;&#20195;&#20132;&#36890;&#22522;&#30784;&#35774;&#26045;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#37319;&#29992;&#36890;&#20449;&#25216;&#26415;&#65292;&#20449;&#24687;&#22788;&#29702;&#21644;&#25511;&#21046;&#31995;&#32479;&#26469;&#31649;&#29702;&#20132;&#36890;&#32593;&#32476;&#12290;&#36825;&#31181;&#38598;&#25104;&#21508;&#31181;&#32452;&#20214;&#65288;&#22914;&#36947;&#36335;&#65292;&#36710;&#36742;&#21644;&#36890;&#20449;&#31995;&#32479;&#65289;&#30340;&#26041;&#27861;&#65292;&#39044;&#35745;&#36890;&#36807;&#25552;&#20379;&#26356;&#22909;&#30340;&#20449;&#24687;&#65292;&#26381;&#21153;&#21644;&#20132;&#36890;&#27169;&#24335;&#30340;&#21327;&#35843;&#26469;&#25552;&#39640;&#25928;&#29575;&#21644;&#23433;&#20840;&#24615;&#12290;&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;&#22270;&#30340;&#26426;&#22120;&#23398;&#20064;&#24050;&#25104;&#20026;&#26234;&#33021;&#20132;&#36890;&#31995;&#32479;&#39046;&#22495;&#26085;&#30410;&#37325;&#35201;&#30340;&#30740;&#31350;&#37325;&#28857;&#65292;&#26088;&#22312;&#24320;&#21457;&#22522;&#20110;&#22797;&#26434;&#25968;&#25454;&#30340;&#25968;&#25454;&#39537;&#21160;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#35299;&#20915;&#21508;&#31181;&#19982;ITS&#30456;&#20851;&#30340;&#25361;&#25112;&#12290;&#26412;&#31456;&#33410;&#25552;&#20379;ITS&#35774;&#35745;&#30340;&#20851;&#38190;&#25216;&#26415;&#25361;&#25112;&#30340;&#32972;&#26223;&#20449;&#24687;&#65292;&#20197;&#21450;&#28085;&#30422;&#20174;&#32463;&#20856;&#32479;&#35745;&#26041;&#27861;&#21040;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#21644;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#30340;&#30740;&#31350;&#26041;&#27861;&#30340;&#32508;&#36848;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#22522;&#20110;&#22270;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#28145;&#20837;&#23457;&#26597;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Intelligent Transportation System (ITS) is an important part of modern transportation infrastructure, employing a combination of communication technology, information processing and control systems to manage transportation networks. This integration of various components such as roads, vehicles, and communication systems, is expected to improve efficiency and safety by providing better information, services, and coordination of transportation modes. In recent years, graph-based machine learning has become an increasingly important research focus in the field of ITS aiming at the development of complex, data-driven solutions to address various ITS-related challenges. This chapter presents background information on the key technical challenges for ITS design, along with a review of research methods ranging from classic statistical approaches to modern machine learning and deep learning-based approaches. Specifically, we provide an in-depth review of graph-based machine learning metho
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#26426;&#22120;&#20154;&#36335;&#24452;&#35268;&#21010;&#26041;&#27861;MAPPOHR&#65292;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;&#21551;&#21457;&#24335;&#25628;&#32034;&#12289;&#32463;&#39564;&#35268;&#21017;&#21644;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#35268;&#21010;&#25928;&#29575;&#21644;&#36991;&#30896;&#33021;&#21147;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.01270</link><description>&lt;p&gt;
&#32452;&#21512;&#21551;&#21457;&#24335;&#21644;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#22810;&#26426;&#22120;&#20154;&#36335;&#24452;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Multi-Robot Path Planning Combining Heuristics and Multi-Agent Reinforcement Learning. (arXiv:2306.01270v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01270
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#26426;&#22120;&#20154;&#36335;&#24452;&#35268;&#21010;&#26041;&#27861;MAPPOHR&#65292;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;&#21551;&#21457;&#24335;&#25628;&#32034;&#12289;&#32463;&#39564;&#35268;&#21017;&#21644;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#35268;&#21010;&#25928;&#29575;&#21644;&#36991;&#30896;&#33021;&#21147;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#24577;&#29615;&#22659;&#19979;&#30340;&#22810;&#26426;&#22120;&#20154;&#36335;&#24452;&#35268;&#21010;&#26159;&#19968;&#20010;&#26497;&#20855;&#25361;&#25112;&#24615;&#30340;&#32463;&#20856;&#38382;&#39064;&#12290;&#22312;&#31227;&#21160;&#36807;&#31243;&#20013;&#65292;&#26426;&#22120;&#20154;&#38656;&#35201;&#36991;&#20813;&#19982;&#20854;&#20182;&#31227;&#21160;&#26426;&#22120;&#20154;&#21457;&#29983;&#30896;&#25758;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#23427;&#20204;&#30340;&#34892;&#39542;&#36317;&#31163;&#12290;&#20197;&#24448;&#30340;&#26041;&#27861;&#35201;&#20040;&#20351;&#29992;&#21551;&#21457;&#24335;&#25628;&#32034;&#26041;&#27861;&#19981;&#26029;&#37325;&#26032;&#35268;&#21010;&#36335;&#24452;&#20197;&#36991;&#20813;&#20914;&#31361;&#65292;&#35201;&#20040;&#22522;&#20110;&#23398;&#20064;&#26041;&#27861;&#36873;&#25321;&#36866;&#24403;&#30340;&#36991;&#30896;&#31574;&#30053;&#12290;&#21069;&#32773;&#21487;&#33021;&#30001;&#20110;&#39057;&#32321;&#30340;&#37325;&#26032;&#35268;&#21010;&#23548;&#33268;&#34892;&#39542;&#36317;&#31163;&#36739;&#38271;&#65292;&#32780;&#21518;&#32773;&#21487;&#33021;&#30001;&#20110;&#20302;&#26679;&#26412;&#25506;&#32034;&#21644;&#21033;&#29992;&#29575;&#32780;&#23548;&#33268;&#23398;&#20064;&#25928;&#29575;&#20302;&#65292;&#20174;&#32780;&#20351;&#27169;&#22411;&#30340;&#35757;&#32451;&#25104;&#26412;&#36739;&#39640;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36335;&#24452;&#35268;&#21010;&#26041;&#27861;MAPPOHR&#65292;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;&#21551;&#21457;&#24335;&#25628;&#32034;&#12289;&#32463;&#39564;&#35268;&#21017;&#21644;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#12290;&#35813;&#26041;&#27861;&#21253;&#21547;&#20004;&#20010;&#23618;&#27425;&#65306;&#22522;&#20110;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;MAPPO&#30340;&#23454;&#26102;&#35268;&#21010;&#22120;&#65292;&#20854;&#23558;&#32463;&#39564;&#35268;&#21017;&#23884;&#20837;&#21040;&#21160;&#20316;&#36755;&#20986;&#23618;&#21644;&#22870;&#21169;&#20989;&#25968;&#20013;&#65307;&#20197;&#21450;&#19968;&#20010;&#21551;&#21457;&#24335;&#35268;&#21010;&#22120;&#65292;&#23427;&#29983;&#25104;&#21021;&#22987;&#36335;&#24452;&#24182;&#21521;MAPPO&#35268;&#21010;&#22120;&#28155;&#21152;&#32422;&#26463;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#35268;&#21010;&#25928;&#29575;&#21644;&#36991;&#30896;&#33021;&#21147;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-robot path finding in dynamic environments is a highly challenging classic problem. In the movement process, robots need to avoid collisions with other moving robots while minimizing their travel distance. Previous methods for this problem either continuously replan paths using heuristic search methods to avoid conflicts or choose appropriate collision avoidance strategies based on learning approaches. The former may result in long travel distances due to frequent replanning, while the latter may have low learning efficiency due to low sample exploration and utilization, and causing high training costs for the model. To address these issues, we propose a path planning method, MAPPOHR, which combines heuristic search, empirical rules, and multi-agent reinforcement learning. The method consists of two layers: a real-time planner based on the multi-agent reinforcement learning algorithm, MAPPO, which embeds empirical rules in the action output layer and reward functions, and a heuri
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#30784;&#26694;&#26550;-"&#36127;&#36131;&#20219;&#30340;&#20219;&#21153;&#33258;&#21160;&#21270;&#65288;ResponsibleTA&#65289;"&#65292;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36127;&#36131;&#20219;&#22320;&#20316;&#20026;&#20219;&#21153;&#21327;&#21516;&#24037;&#20855;&#12290;&#35813;&#26694;&#26550;&#22686;&#24378;&#20102;LLM&#30340;&#19977;&#31181;&#33021;&#21147;&#65306;&#39044;&#27979;&#20219;&#21153;&#21487;&#34892;&#24615;&#12289;&#39564;&#35777;&#20219;&#21153;&#23436;&#25972;&#24615;&#20197;&#21450;&#22686;&#24378;&#20219;&#21153;&#23433;&#20840;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.01242</link><description>&lt;p&gt;
&#36127;&#36131;&#20219;&#30340;&#20219;&#21153;&#33258;&#21160;&#21270;: &#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25104;&#20026;&#36127;&#36131;&#20219;&#30340;&#20219;&#21153;&#33258;&#21160;&#21270;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
Responsible Task Automation: Empowering Large Language Models as Responsible Task Automators. (arXiv:2306.01242v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01242
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#30784;&#26694;&#26550;-"&#36127;&#36131;&#20219;&#30340;&#20219;&#21153;&#33258;&#21160;&#21270;&#65288;ResponsibleTA&#65289;"&#65292;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36127;&#36131;&#20219;&#22320;&#20316;&#20026;&#20219;&#21153;&#21327;&#21516;&#24037;&#20855;&#12290;&#35813;&#26694;&#26550;&#22686;&#24378;&#20102;LLM&#30340;&#19977;&#31181;&#33021;&#21147;&#65306;&#39044;&#27979;&#20219;&#21153;&#21487;&#34892;&#24615;&#12289;&#39564;&#35777;&#20219;&#21153;&#23436;&#25972;&#24615;&#20197;&#21450;&#22686;&#24378;&#20219;&#21153;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25104;&#21151;&#20026;&#20154;&#24037;&#26234;&#33021;&#36808;&#20986;&#20102;&#37325;&#35201;&#30340;&#19968;&#27493;&#12290;&#23427;&#20204;&#23637;&#29616;&#20102;&#22312;&#29992;&#25143;&#25351;&#20196;&#19979;&#33258;&#21160;&#23436;&#25104;&#20219;&#21153;&#30340;&#33391;&#22909;&#21069;&#26223;&#65292;&#21487;&#20197;&#20316;&#20026;&#31867;&#20284;&#22823;&#33041;&#30340;&#21327;&#35843;&#32773;&#12290;&#38543;&#30528;&#25105;&#20204;&#23558;&#36234;&#26469;&#36234;&#22810;&#30340;&#20219;&#21153;&#20132;&#32473;&#26426;&#22120;&#33258;&#21160;&#23436;&#25104;&#65292;&#30456;&#20851;&#30340;&#39118;&#38505;&#20063;&#36880;&#28176;&#26174;&#29616;&#12290;&#19968;&#20010;&#37325;&#35201;&#30340;&#38382;&#39064;&#20986;&#29616;&#20102;: &#24403;&#26426;&#22120;&#20687;&#20154;&#31867;&#39550;&#39542;&#21327;&#21516;&#19968;&#26679;&#24110;&#21161;&#20154;&#20204;&#33258;&#21160;&#23436;&#25104;&#20219;&#21153;&#26102;&#65292;&#25105;&#20204;&#22914;&#20309;&#30830;&#20445;&#26426;&#22120;&#30340;&#36131;&#20219;&#34892;&#20026;&#65311;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20174;&#21487;&#34892;&#24615;&#12289;&#23436;&#25972;&#24615;&#21644;&#23433;&#20840;&#24615;&#30340;&#35282;&#24230;&#65292;&#28145;&#20837;&#25506;&#35752;&#36825;&#20010;&#38382;&#39064;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;&#36127;&#36131;&#20219;&#30340;&#20219;&#21153;&#33258;&#21160;&#21270;&#8221;&#65288;ResponsibleTA&#65289;&#20316;&#20026;&#19968;&#20010;&#22522;&#30784;&#26694;&#26550;&#65292;&#20197;&#20419;&#36827;LLM&#21327;&#35843;&#32773;&#21644;&#25191;&#34892;&#32773;&#20043;&#38388;&#30340;&#36127;&#36131;&#20219;&#21327;&#20316;&#65292;&#23454;&#29616;&#20219;&#21153;&#33258;&#21160;&#21270;&#12290;&#35813;&#26694;&#26550;&#25317;&#26377;&#19977;&#31181;&#22686;&#24378;&#33021;&#21147;: 1&#65289;&#39044;&#27979;&#25191;&#34892;&#32773;&#21629;&#20196;&#30340;&#21487;&#34892;&#24615;&#65307;2&#65289;&#39564;&#35777;&#25191;&#34892;&#32773;&#30340;&#23436;&#25972;&#24615;&#65307;3&#65289;&#22686;&#24378;&#23433;&#20840;&#24615;&#65288;&#20363;&#22914;&#65292;&#20445;&#25252;&#38544;&#31169;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent success of Large Language Models (LLMs) signifies an impressive stride towards artificial general intelligence. They have shown a promising prospect in automatically completing tasks upon user instructions, functioning as brain-like coordinators. The associated risks will be revealed as we delegate an increasing number of tasks to machines for automated completion. A big question emerges: how can we make machines behave responsibly when helping humans automate tasks as personal copilots? In this paper, we explore this question in depth from the perspectives of feasibility, completeness and security. In specific, we present Responsible Task Automation (ResponsibleTA) as a fundamental framework to facilitate responsible collaboration between LLM-based coordinators and executors for task automation with three empowered capabilities: 1) predicting the feasibility of the commands for executors; 2) verifying the completeness of executors; 3) enhancing the security (e.g., the prote
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22522;&#20110;&#19968;&#20010;&#22995;&#27663;&#20146;&#32536;&#32593;&#32476;&#65292;&#36890;&#36807;&#24314;&#31435;&#30693;&#35782;&#24211;&#23436;&#25104;&#27169;&#22411;&#39044;&#27979;&#26032;&#32852;&#31995;&#30340;&#24418;&#25104;&#24182;&#21457;&#29616;&#20849;&#20139;&#37051;&#23621;&#21644;&#23884;&#20837;&#31354;&#38388;&#20013;&#30340;&#37051;&#23621;&#23545;&#20110;&#39044;&#27979;&#37117;&#21313;&#20998;&#37325;&#35201;&#65292;&#36825;&#19968;&#21457;&#29616;&#23545;&#20110;&#35299;&#37322;&#22307;&#22320;&#20122;&#21733;&#31934;&#33521;&#20869;&#23130;&#20013;&#30340;&#39640;&#27700;&#24179;&#24456;&#26377;&#24110;&#21161;&#12290;</title><link>http://arxiv.org/abs/2306.01218</link><description>&lt;p&gt;
&#22312;&#19968;&#20010;&#22995;&#27663;&#32593;&#32476;&#20013;&#39044;&#27979;&#20146;&#32536;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
Predicting affinity ties in a surname network. (arXiv:2306.01218v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01218
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22522;&#20110;&#19968;&#20010;&#22995;&#27663;&#20146;&#32536;&#32593;&#32476;&#65292;&#36890;&#36807;&#24314;&#31435;&#30693;&#35782;&#24211;&#23436;&#25104;&#27169;&#22411;&#39044;&#27979;&#26032;&#32852;&#31995;&#30340;&#24418;&#25104;&#24182;&#21457;&#29616;&#20849;&#20139;&#37051;&#23621;&#21644;&#23884;&#20837;&#31354;&#38388;&#20013;&#30340;&#37051;&#23621;&#23545;&#20110;&#39044;&#27979;&#37117;&#21313;&#20998;&#37325;&#35201;&#65292;&#36825;&#19968;&#21457;&#29616;&#23545;&#20110;&#35299;&#37322;&#22307;&#22320;&#20122;&#21733;&#31934;&#33521;&#20869;&#23130;&#20013;&#30340;&#39640;&#27700;&#24179;&#24456;&#26377;&#24110;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#26234;&#21033;&#22307;&#22320;&#20122;&#21733;&#30340;&#22995;&#27663;&#31649;&#29702;&#30331;&#35760;&#20013;&#21019;&#24314;&#19968;&#20010;&#22995;&#27663;&#20146;&#32536;&#32593;&#32476;&#65292;&#32534;&#30721;&#20102;&#31038;&#20250;&#32463;&#27982;&#25968;&#25454;&#12290;&#36825;&#20010;&#32593;&#32476;&#26159;&#19968;&#20010;&#22810;&#37325;&#20851;&#31995;&#22270;&#65292;&#33410;&#28857;&#20195;&#34920;&#22995;&#27663;&#65292;&#36793;&#20195;&#34920;&#19981;&#21516;&#31038;&#20250;&#32463;&#27982;&#20998;&#20301;&#25968;&#20043;&#38388;&#22995;&#27663;&#38388;&#20114;&#21160;&#30340;&#30427;&#34892;&#31243;&#24230;&#12290;&#25105;&#20204;&#23558;&#38142;&#25509;&#39044;&#27979;&#24314;&#27169;&#20026;&#30693;&#35782;&#24211;&#23436;&#25104;&#38382;&#39064;&#65292;&#24182;&#21457;&#29616;&#20849;&#20139;&#37051;&#23621;&#39640;&#24230;&#39044;&#27979;&#26032;&#38142;&#25509;&#30340;&#24418;&#25104;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#21306;&#20998;&#20102;&#23884;&#20837;&#31354;&#38388;&#20013;&#30340;&#26377;&#26681;&#37051;&#23621;&#21644;&#37051;&#23621;&#65292;&#24182;&#21457;&#29616;&#21518;&#32773;&#26356;&#33021;&#39044;&#27979;&#32852;&#31995;&#30340;&#24418;&#25104;&#12290;&#26412;&#25991;&#35752;&#35770;&#20102;&#36825;&#19968;&#21457;&#29616;&#22312;&#35299;&#37322;&#22307;&#22320;&#20122;&#21733;&#31934;&#33521;&#20869;&#23130;&#30340;&#39640;&#27700;&#24179;&#20013;&#30340;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
From administrative registers of last names in Santiago, Chile, we create a surname affinity network that encodes socioeconomic data. This network is a multi-relational graph with nodes representing surnames and edges representing the prevalence of interactions between surnames by socioeconomic decile. We model the prediction of links as a knowledge base completion problem, and find that sharing neighbors is highly predictive of the formation of new links. Importantly, We distinguish between grounded neighbors and neighbors in the embedding space, and find that the latter is more predictive of tie formation. The paper discusses the implications of this finding in explaining the high levels of elite endogamy in Santiago.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#22914;&#20309;&#20197;&#21407;&#21017;&#24615;&#30340;&#26041;&#24335;&#20998;&#26512;&#27169;&#22411;&#22312;&#22495;&#20869;&#21644;&#22495;&#22806;&#35774;&#32622;&#19979;&#30340;&#24615;&#33021;&#65292;&#26368;&#32456;&#25214;&#20986;&#19968;&#31181;&#26080;&#30417;&#30563;&#26041;&#27861;&#35782;&#21035;OOD / OODist&#26679;&#26412;&#12290;</title><link>http://arxiv.org/abs/2306.01206</link><description>&lt;p&gt;
&#22312;&#22495;&#20869;&#21644;&#22495;&#22806;&#26679;&#26412;&#20043;&#38388;&#20272;&#35745;&#35821;&#20041;&#30456;&#20284;&#24230;
&lt;/p&gt;
&lt;p&gt;
Estimating Semantic Similarity between In-Domain and Out-of-Domain Samples. (arXiv:2306.01206v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01206
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#22914;&#20309;&#20197;&#21407;&#21017;&#24615;&#30340;&#26041;&#24335;&#20998;&#26512;&#27169;&#22411;&#22312;&#22495;&#20869;&#21644;&#22495;&#22806;&#35774;&#32622;&#19979;&#30340;&#24615;&#33021;&#65292;&#26368;&#32456;&#25214;&#20986;&#19968;&#31181;&#26080;&#30417;&#30563;&#26041;&#27861;&#35782;&#21035;OOD / OODist&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#30340;&#24037;&#20316;&#36890;&#24120;&#23558;&#26469;&#33258;&#25968;&#25454;&#38598;&#25110;&#28304;&#19982;&#35757;&#32451;&#38598;&#19981;&#21516;&#20294;&#29992;&#20110;&#21516;&#19968;&#20219;&#21153;&#30340;&#22495;&#22806;&#65288;OOD&#65289;&#25110;&#22495;&#22806;&#20998;&#24067;&#65288;OODist&#65289;&#26679;&#26412;&#25551;&#36848;&#20026;&#22495;&#22806;&#65292;&#19982;&#22495;&#20869;&#65288;ID&#65289;&#26679;&#26412;&#30456;&#27604;&#65292;&#27169;&#22411;&#22312;OOD&#26679;&#26412;&#19978;&#30340;&#34920;&#29616;&#36890;&#24120;&#36739;&#24046;&#65292;&#23613;&#31649;&#36825;&#31181;&#35266;&#23519;&#32467;&#26524;&#24182;&#19981;&#19968;&#33268;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#19968;&#20123;&#30740;&#31350;&#20851;&#27880;&#20110;OOD&#26816;&#27979;&#65292;&#20294;&#22823;&#22810;&#20351;&#29992;&#26377;&#30417;&#30563;&#30340;&#26041;&#27861;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#25972;&#21512;&#24182;&#21576;&#29616;&#20102;&#22810;&#20010;&#20851;&#20110;OOD&#21644;OODist&#30340;&#22810;&#37325;&#23450;&#20041;&#65292;&#24182;&#20197;&#21407;&#21017;&#24615;&#30340;&#26041;&#24335;&#20998;&#26512;&#20102;&#27169;&#22411;&#22312;ID&#21644;OOD / OODist&#35774;&#32622;&#19979;&#30340;&#24615;&#33021;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35797;&#22270;&#35782;&#21035;&#19968;&#31181;&#26080;&#30417;&#30563;&#26041;&#27861;&#65292;&#21487;&#22312;&#19981;&#20351;&#29992;&#35757;&#32451;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#21487;&#38752;&#22320;&#35782;&#21035;OOD / OODist&#26679;&#26412;&#12290;&#25105;&#20204;&#20351;&#29992;4&#20010;&#19981;&#21516;&#20219;&#21153;&#30340;12&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#24191;&#27867;&#35780;&#20272;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#26080;&#30417;&#30563;&#24230;&#37327;&#22312;&#35813;&#20219;&#21153;&#20013;&#20855;&#26377;&#33391;&#22909;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prior work typically describes out-of-domain (OOD) or out-of-distribution (OODist) samples as those that originate from dataset(s) or source(s) different from the training set but for the same task. When compared to in-domain (ID) samples, the models have been known to usually perform poorer on OOD samples, although this observation is not consistent. Another thread of research has focused on OOD detection, albeit mostly using supervised approaches. In this work, we first consolidate and present a systematic analysis of multiple definitions of OOD and OODist as discussed in prior literature. Then, we analyze the performance of a model under ID and OOD/OODist settings in a principled way. Finally, we seek to identify an unsupervised method for reliably identifying OOD/OODist samples without using a trained model. The results of our extensive evaluation using 12 datasets from 4 different tasks suggest the promising potential of unsupervised metrics in this task.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#35780;&#20272;&#29983;&#23384;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#20351;&#29992;&#20266;&#35266;&#23519;&#20540;&#30340;MAE&#25351;&#26631;&#33021;&#22815;&#20934;&#30830;&#22320;&#25490;&#21517;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#21457;&#29616;&#36825;&#31181;&#26041;&#27861;&#27604;&#20854;&#20182;&#26367;&#20195;&#26041;&#27861;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2306.01196</link><description>&lt;p&gt;
&#19968;&#31181;&#26377;&#25928;&#30340;&#35780;&#20272;&#29983;&#23384;&#27169;&#22411;&#30340;&#26377;&#24847;&#20041;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
An Effective Meaningful Way to Evaluate Survival Models. (arXiv:2306.01196v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01196
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#35780;&#20272;&#29983;&#23384;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#20351;&#29992;&#20266;&#35266;&#23519;&#20540;&#30340;MAE&#25351;&#26631;&#33021;&#22815;&#20934;&#30830;&#22320;&#25490;&#21517;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#21457;&#29616;&#36825;&#31181;&#26041;&#27861;&#27604;&#20854;&#20182;&#26367;&#20195;&#26041;&#27861;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#29983;&#23384;&#39044;&#27979;&#27169;&#22411;&#30340;&#19968;&#31181;&#30452;&#25509;&#25351;&#26631;&#26159;&#22522;&#20110;&#24179;&#22343;&#32477;&#23545;&#35823;&#24046;&#65288;MAE&#65289;-&#27169;&#22411;&#39044;&#27979;&#26102;&#38388;&#19982;&#30495;&#23454;&#20107;&#20214;&#26102;&#38388;&#20043;&#38388;&#30340;&#32477;&#23545;&#24046;&#20540;&#30340;&#24179;&#22343;&#20540;&#65292;&#23545;&#25152;&#26377;&#20010;&#20307;&#36827;&#34892;&#12290;&#28982;&#32780;&#65292;&#36825;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#22312;&#23454;&#36341;&#20013;&#65292;&#27979;&#35797;&#38598;&#21253;&#25324;&#65288;&#27491;&#30830;&#65289;&#34987;&#23457;&#26597;&#30340;&#20010;&#20307;&#65292;&#36825;&#24847;&#21619;&#30528;&#25105;&#20204;&#19981;&#30693;&#36947;&#34987;&#23457;&#26597;&#20010;&#20307;&#23454;&#38469;&#32463;&#21382;&#20107;&#20214;&#30340;&#26102;&#38388;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#29992;&#20110;&#35780;&#20272;&#21253;&#25324;&#65288;&#35768;&#22810;&#65289;&#34987;&#23457;&#26597;&#20010;&#20307;&#30340;&#29983;&#23384;&#25968;&#25454;&#38598;&#30340;&#21508;&#31181;&#25351;&#26631;&#26469;&#20272;&#35745;MAE&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#29983;&#25104;&#36924;&#30495;&#30340;&#21322;&#21512;&#25104;&#29983;&#23384;&#25968;&#25454;&#38598;&#65292;&#20197;&#20415;&#35780;&#20272;&#25351;&#26631;&#12290;&#22522;&#20110;&#21322;&#21512;&#25104;&#25968;&#25454;&#38598;&#30340;&#20998;&#26512;&#65292;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#25351;&#26631;&#65288;&#20351;&#29992;&#20266;&#35266;&#23519;&#27861;&#30340;MAE&#65289;&#33021;&#22815;&#20934;&#30830;&#22320;&#25490;&#21517;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#36890;&#24120;&#19982;&#30495;&#23454;&#30340;MAE&#38750;&#24120;&#25509;&#36817;-&#29305;&#21035;&#26159;&#20248;&#20110;&#20960;&#31181;&#26367;&#20195;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
One straightforward metric to evaluate a survival prediction model is based on the Mean Absolute Error (MAE) -- the average of the absolute difference between the time predicted by the model and the true event time, over all subjects. Unfortunately, this is challenging because, in practice, the test set includes (right) censored individuals, meaning we do not know when a censored individual actually experienced the event. In this paper, we explore various metrics to estimate MAE for survival datasets that include (many) censored individuals. Moreover, we introduce a novel and effective approach for generating realistic semi-synthetic survival datasets to facilitate the evaluation of metrics. Our findings, based on the analysis of the semi-synthetic datasets, reveal that our proposed metric (MAE using pseudo-observations) is able to rank models accurately based on their performance, and often closely matches the true MAE -- in particular, is better than several alternative methods.
&lt;/p&gt;</description></item><item><title>&#38754;&#21521;&#36793;&#32536;&#20154;&#24037;&#26234;&#33021;&#30340;&#38598;&#25104;&#24863;&#30693;-&#36890;&#20449;-&#35745;&#31639;&#25216;&#26415;&#23545;&#20110;&#25552;&#39640;&#36164;&#28304;&#21033;&#29992;&#29575;&#20197;&#21450;&#23454;&#29616;&#36793;&#32536;&#23398;&#20064;&#21644;&#36793;&#32536;&#20154;&#24037;&#26234;&#33021;&#25512;&#29702;&#20219;&#21153;&#30340;&#23450;&#21046;&#30446;&#26631;&#33267;&#20851;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2306.01162</link><description>&lt;p&gt;
&#38754;&#21521;&#36793;&#32536;&#20154;&#24037;&#26234;&#33021;&#30340;&#38598;&#25104;&#24863;&#30693;-&#36890;&#20449;-&#35745;&#31639; &#65288;Integrated Sensing-Communication-Computation&#65289; &#65288;arXiv&#65306;2306.01162v1 [cs.IT]&#65289;
&lt;/p&gt;
&lt;p&gt;
Integrated Sensing-Communication-Computation for Edge Artificial Intelligence. (arXiv:2306.01162v1 [cs.IT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01162
&lt;/p&gt;
&lt;p&gt;
&#38754;&#21521;&#36793;&#32536;&#20154;&#24037;&#26234;&#33021;&#30340;&#38598;&#25104;&#24863;&#30693;-&#36890;&#20449;-&#35745;&#31639;&#25216;&#26415;&#23545;&#20110;&#25552;&#39640;&#36164;&#28304;&#21033;&#29992;&#29575;&#20197;&#21450;&#23454;&#29616;&#36793;&#32536;&#23398;&#20064;&#21644;&#36793;&#32536;&#20154;&#24037;&#26234;&#33021;&#25512;&#29702;&#20219;&#21153;&#30340;&#23450;&#21046;&#30446;&#26631;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36793;&#32536;&#20154;&#24037;&#26234;&#33021;&#26159;&#23454;&#29616;&#19975;&#29289;&#26234;&#33021;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#29992;&#20110;&#25968;&#23383;&#23402;&#29983;&#12289;&#20840;&#24687;&#25237;&#24433;&#12289;&#35821;&#20041;&#36890;&#20449;&#21644;&#33258;&#21160;&#39550;&#39542;&#31561;&#39640;&#32423;&#25216;&#26415;&#12290;&#36793;&#32536;&#20154;&#24037;&#26234;&#33021;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;&#36793;&#32536;&#23398;&#20064;&#21644;&#36793;&#32536;&#20154;&#24037;&#26234;&#33021;&#25512;&#29702;&#65292;&#21462;&#20915;&#20110;&#19977;&#20010;&#39640;&#24230;&#32806;&#21512;&#30340;&#36807;&#31243;&#30340;&#36136;&#37327;&#65292;&#21363;&#25968;&#25454;&#33719;&#21462;&#30340;&#24863;&#30693;&#12289;&#20449;&#24687;&#25552;&#21462;&#30340;&#35745;&#31639;&#21644;&#20449;&#24687;&#20256;&#36755;&#30340;&#36890;&#20449;&#12290;&#28982;&#32780;&#65292;&#36825;&#19977;&#20010;&#27169;&#22359;&#38656;&#35201;&#20026;&#22686;&#24378;&#33258;&#24049;&#30340;&#26381;&#21153;&#36136;&#37327;&#32780;&#31454;&#20105;&#32593;&#32476;&#36164;&#28304;&#12290;&#20026;&#27492;&#65292;&#38598;&#25104;&#24863;&#30693;-&#36890;&#20449;-&#35745;&#31639;&#65288;ISCC&#65289;&#23545;&#20110;&#25552;&#39640;&#36164;&#28304;&#21033;&#29992;&#29575;&#20197;&#21450;&#23454;&#29616;&#36793;&#32536;&#20154;&#24037;&#26234;&#33021;&#20219;&#21153;&#30340;&#23450;&#21046;&#30446;&#26631;&#33267;&#20851;&#37325;&#35201;&#12290;&#36890;&#36807;&#30740;&#31350;&#19977;&#20010;&#27169;&#22359;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#21508;&#31181; ISCC &#26041;&#26696;&#65292;&#36866;&#29992;&#20110;&#32852;&#37030;&#36793;&#32536;&#23398;&#20064;&#20219;&#21153;&#21644;&#36793;&#32536;&#20154;&#24037;&#26234;&#33021;&#25512;&#29702;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Edge artificial intelligence (AI) has been a promising solution towards 6G to empower a series of advanced techniques such as digital twin, holographic projection, semantic communications, and auto-driving, for achieving intelligence of everything. The performance of edge AI tasks, including edge learning and edge AI inference, depends on the quality of three highly coupled processes, i.e., sensing for data acquisition, computation for information extraction, and communication for information transmission. However, these three modules need to compete for network resources for enhancing their own quality-of-services. To this end, integrated sensing-communication-computation (ISCC) is of paramount significance for improving resource utilization as well as achieving the customized goals of edge AI tasks. By investigating the interplay among the three modules, this article presents various kinds of ISCC schemes for federated edge learning tasks and edge AI inference tasks in both applicati
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#31232;&#30095;Flash&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#33021;&#22815;&#24555;&#36895;&#22788;&#29702;&#22823;&#24207;&#21015;&#20013;&#30340;&#22240;&#26524;&#20851;&#31995;&#65292;&#19988;&#36895;&#24230;&#25552;&#39640;&#20102;&#22810;&#20493;&#65292;&#21487;&#20197;&#20316;&#20026;&#20219;&#20309;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#23494;&#38598;&#33258;&#25105;&#27880;&#24847;&#21147;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#24182;&#22312;&#22810;&#20010;&#35774;&#32622;&#20013;&#20135;&#29983;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.01160</link><description>&lt;p&gt;
&#36890;&#36807;&#31232;&#30095;Flash&#27880;&#24847;&#21147;&#21152;&#36895;&#22788;&#29702;&#22823;&#24207;&#21015;&#20013;&#30340;&#22240;&#26524;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
Faster Causal Attention Over Large Sequences Through Sparse Flash Attention. (arXiv:2306.01160v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01160
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#31232;&#30095;Flash&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#33021;&#22815;&#24555;&#36895;&#22788;&#29702;&#22823;&#24207;&#21015;&#20013;&#30340;&#22240;&#26524;&#20851;&#31995;&#65292;&#19988;&#36895;&#24230;&#25552;&#39640;&#20102;&#22810;&#20493;&#65292;&#21487;&#20197;&#20316;&#20026;&#20219;&#20309;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#23494;&#38598;&#33258;&#25105;&#27880;&#24847;&#21147;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#24182;&#22312;&#22810;&#20010;&#35774;&#32622;&#20013;&#20135;&#29983;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#22312;&#22788;&#29702;&#36234;&#26469;&#36234;&#38271;&#24207;&#21015;&#30340;&#20219;&#21153;&#20013;&#34987;&#24191;&#27867;&#24212;&#29992;&#12290;&#23545;&#20110;&#36825;&#20123;&#24212;&#29992;&#65292;&#24207;&#21015;&#38271;&#24230;&#20851;&#20110;&#30340;&#22240;&#26524;&#33258;&#27880;&#24847;&#21147;&#25104;&#20026;&#19968;&#20010;&#26680;&#24515;&#38382;&#39064;&#65292;&#22240;&#20026;&#23427;&#26159;&#21807;&#19968;&#19968;&#20010;&#19982;&#24207;&#21015;&#38271;&#24230;&#21576;&#20108;&#27425;&#20851;&#31995;&#30340;&#32452;&#20214;&#12290;&#34429;&#28982;&#35768;&#22810;&#30740;&#31350;&#24050;&#32463;&#25552;&#20986;&#26041;&#26696;&#26469;&#20351;&#33258;&#27880;&#24847;&#21147;&#30340;&#27880;&#24847;&#21147;&#27169;&#24335;&#31232;&#30095;&#21270;&#65292;&#20294;&#36825;&#20123;&#26041;&#26696;&#36890;&#24120;&#21463;&#21040;&#23454;&#29616;&#26041;&#38754;&#30340;&#38480;&#21046;&#65292;&#24182;&#26368;&#32456;&#24378;&#21152;&#19968;&#20010;&#31616;&#21333;&#19988;&#38745;&#24577;&#30340;&#32467;&#26500;&#22312;&#20851;&#27880;&#30697;&#38453;&#19978;&#12290;&#30456;&#21453;&#65292;&#23454;&#29616;&#26356;&#21160;&#24577;&#30340;&#31232;&#30095;&#27880;&#24847;&#21147;&#36890;&#24120;&#20250;&#23548;&#33268;&#36816;&#34892;&#26102;&#38388;&#26174;&#30528;&#24930;&#20110;&#20351;&#29992;Dao&#31561;&#20154;&#65288;2022&#65289;&#30340;Flash&#23454;&#29616;&#35745;&#31639;&#23436;&#25972;&#27880;&#24847;&#21147;&#12290;&#25105;&#20204;&#25193;&#23637;&#20102;FlashAttention&#65292;&#20197;&#36866;&#24212;&#21253;&#21547;&#38190;/&#26597;&#35810;&#20002;&#24323;&#21644;&#22522;&#20110;&#21704;&#24076;&#30340;&#27880;&#24847;&#21147;&#22312;&#20869;&#30340;&#22823;&#31867;&#31232;&#30095;&#27880;&#24847;&#24615;&#27169;&#24335;&#12290;&#36825;&#23548;&#33268;&#23454;&#29616;&#27809;&#26377;&#20219;&#20309;&#35745;&#31639;&#22797;&#26434;&#24230;&#24320;&#38144;&#65292;&#24182;&#19988;&#19982;&#20197;&#21069;&#30340;&#21160;&#24577;&#31232;&#30095;&#27880;&#24847;&#24615;&#30456;&#27604;&#65292;&#36895;&#24230;&#25552;&#39640;&#20102;&#22810;&#20493;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#29992;&#20316;&#20219;&#20309;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#23494;&#38598;&#33258;&#25105;&#27880;&#24847;&#21147;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#24182;&#22312;&#22810;&#20010;&#35774;&#32622;&#20013;&#20135;&#29983;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#21253;&#25324;&#29983;&#25104;&#35821;&#35328;&#24314;&#27169;&#21644;&#38271;&#26684;&#24335;&#38382;&#31572;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer-based language models have found many diverse applications requiring them to process sequences of increasing length. For these applications, the causal self-attention -- which is the only component scaling quadratically w.r.t. the sequence length -- becomes a central concern. While many works have proposed schemes to sparsify the attention patterns and reduce the computational overhead of self-attention, those are often limited by implementations concerns and end up imposing a simple and static structure over the attention matrix. Conversely, implementing more dynamic sparse attentions often results in runtimes significantly slower than computing the full attention using the Flash implementation from Dao et al. (2022). We extend FlashAttention to accommodate a large class of attention sparsity patterns that, in particular, encompass key/query dropping and hashing-based attention. This leads to implementations with no computational complexity overhead and a multi-fold runtim
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22686;&#24378;&#27169;&#22359;&#21270;&#24378;&#21270;&#23398;&#20064;&#65288;AMRL&#65289;&#65292;&#20351;&#29992;&#20210;&#35009;&#22120;&#26469;&#36873;&#25321;&#24322;&#26500;&#27169;&#22359;&#65292;&#24182;&#26080;&#32541;&#22320;&#25972;&#21512;&#19981;&#21516;&#31867;&#22411;&#30340;&#30693;&#35782;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#20943;&#32531;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#19968;&#20123;&#20302;&#25928;&#38382;&#39064;&#65292;&#26377;&#26395;&#22312;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#24471;&#21040;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2306.01158</link><description>&lt;p&gt;
&#22522;&#20110;&#24322;&#26500;&#30693;&#35782;&#30340;&#22686;&#24378;&#27169;&#22359;&#21270;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Augmented Modular Reinforcement Learning based on Heterogeneous Knowledge. (arXiv:2306.01158v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01158
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22686;&#24378;&#27169;&#22359;&#21270;&#24378;&#21270;&#23398;&#20064;&#65288;AMRL&#65289;&#65292;&#20351;&#29992;&#20210;&#35009;&#22120;&#26469;&#36873;&#25321;&#24322;&#26500;&#27169;&#22359;&#65292;&#24182;&#26080;&#32541;&#22320;&#25972;&#21512;&#19981;&#21516;&#31867;&#22411;&#30340;&#30693;&#35782;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#20943;&#32531;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#19968;&#20123;&#20302;&#25928;&#38382;&#39064;&#65292;&#26377;&#26395;&#22312;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#24471;&#21040;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#20943;&#32531;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#19968;&#20123;&#20302;&#25928;&#38382;&#39064;&#65292;&#23398;&#32773;&#20204;&#25552;&#20986;&#20102;&#27169;&#22359;&#21270;&#26041;&#27861;&#65292;&#23558;&#19981;&#21516;&#30340;&#20915;&#31574;&#21046;&#23450;&#31574;&#30053;&#32452;&#21512;&#36215;&#26469;&#20197;&#34893;&#29983;&#20986;&#21487;&#20197;&#25191;&#34892;&#22810;&#31181;&#20219;&#21153;&#30340;&#20195;&#29702;&#12290;&#36825;&#20123;&#20307;&#31995;&#32467;&#26500;&#30340;&#22522;&#30784;&#27169;&#22359;&#36890;&#24120;&#26159;&#21487;&#37325;&#22797;&#20351;&#29992;&#30340;&#65292;&#20063;&#20801;&#35768;&#8220;&#21363;&#25554;&#21363;&#29992;&#8221;&#30340;&#38598;&#25104;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#35299;&#20915;&#26041;&#26696;&#20173;&#28982;&#32570;&#20047;&#22788;&#29702;&#21644;&#25972;&#21512;&#22810;&#31181;&#31867;&#22411;&#20449;&#24687;&#65288;&#30693;&#35782;&#65289;&#30340;&#33021;&#21147;&#65292;&#20363;&#22914;&#35268;&#21017;&#65292;&#23376;&#30446;&#26631;&#21644;&#25216;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22686;&#24378;&#27169;&#22359;&#21270;&#24378;&#21270;&#23398;&#20064;&#65288;AMRL&#65289;&#26469;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#12290;&#36825;&#31181;&#26032;&#30340;&#26694;&#26550;&#20351;&#29992;&#20210;&#35009;&#22120;&#26469;&#36873;&#25321;&#24322;&#26500;&#27169;&#22359;&#65292;&#24182;&#26080;&#32541;&#22320;&#25972;&#21512;&#19981;&#21516;&#31867;&#22411;&#30340;&#30693;&#35782;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#36873;&#25321;&#26426;&#21046;&#30340;&#21464;&#20307;&#65292;&#21363;&#22686;&#24378;&#35760;&#24518;&#30340;&#20210;&#35009;&#22120;&#65292;&#23427;&#22686;&#21152;&#20102;&#21033;&#29992;&#26102;&#38388;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;&#24050;&#26377;&#30340;&#29615;&#22659;&#20013;&#35780;&#20272;&#20102;&#25152;&#25552;&#20986;&#30340;&#26426;&#21046;&#65292;&#21516;&#26102;&#20063;&#22312;&#26032;&#29615;&#22659;&#20013;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#65292;&#32467;&#26524;&#34920;&#26126;&#20854;&#22312;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#20855;&#26377;&#33391;&#22909;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
In order to mitigate some of the inefficiencies of Reinforcement Learning (RL), modular approaches composing different decision-making policies to derive agents capable of performing a variety of tasks have been proposed. The modules at the basis of these architectures are generally reusable, also allowing for "plug-and-play" integration. However, such solutions still lack the ability to process and integrate multiple types of information (knowledge), such as rules, sub-goals, and skills. We propose Augmented Modular Reinforcement Learning (AMRL) to address these limitations. This new framework uses an arbitrator to select heterogeneous modules and seamlessly incorporate different types of knowledge. Additionally, we introduce a variation of the selection mechanism, namely the Memory-Augmented Arbitrator, which adds the capability of exploiting temporal information. We evaluate the proposed mechanisms on established as well as new environments and benchmark them against prominent deep 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Delphic&#19981;&#30830;&#23450;&#24615;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#38544;&#34255;&#28151;&#28102;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#38750;&#21487;&#35782;&#21035;&#30340;&#38544;&#34255;&#28151;&#28102;&#24773;&#20917;&#19979;&#20248;&#20110;&#29616;&#26377;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.01157</link><description>&lt;p&gt;
&#38750;&#21487;&#35782;&#21035;&#30340;&#38544;&#21464;&#37327;&#19979;&#30340;Delphic&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Delphic Offline Reinforcement Learning under Nonidentifiable Hidden Confounding. (arXiv:2306.01157v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01157
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Delphic&#19981;&#30830;&#23450;&#24615;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#38544;&#34255;&#28151;&#28102;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#38750;&#21487;&#35782;&#21035;&#30340;&#38544;&#34255;&#28151;&#28102;&#24773;&#20917;&#19979;&#20248;&#20110;&#29616;&#26377;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#31361;&#20986;&#25361;&#25112;&#26159;&#38544;&#34255;&#30340;&#28151;&#28102;&#38382;&#39064;&#65306;&#26410;&#35266;&#23519;&#21040;&#30340;&#21464;&#37327;&#21487;&#33021;&#24433;&#21709;&#21040;&#26234;&#33021;&#20307;&#37319;&#21462;&#30340;&#34892;&#21160;&#21644;&#35266;&#23519;&#21040;&#30340;&#32467;&#26524;&#12290;&#38544;&#34255;&#30340;&#28151;&#28102;&#21487;&#33021;&#25439;&#23475;&#20174;&#25968;&#25454;&#20013;&#24471;&#20986;&#30340;&#20219;&#20309;&#22240;&#26524;&#32467;&#35770;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#19988;&#26159;&#26377;&#25928;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#19968;&#20010;&#20027;&#35201;&#38556;&#30861;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#38750;&#21487;&#35782;&#21035;&#35774;&#32622;&#20013;&#30340;&#38544;&#34255;&#28151;&#28102;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19982;&#35266;&#23519;&#20860;&#23481;&#30340;&#19990;&#30028;&#27169;&#22411;&#30340;&#24046;&#24322;&#26469;&#23450;&#20041;&#30001;&#38544;&#34255;&#28151;&#28102;&#20559;&#24046;&#24341;&#36215;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#31216;&#20026;Delphic&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#23558;&#20854;&#19982;&#20247;&#25152;&#21608;&#30693;&#30340;&#35748;&#30693;&#21644;&#38543;&#26426;&#19981;&#30830;&#23450;&#24615;&#21306;&#20998;&#24320;&#26469;&#12290;&#25105;&#20204;&#23548;&#20986;&#20102;&#19968;&#31181;&#23454;&#38469;&#26041;&#27861;&#26469;&#20272;&#35745;&#36825;&#19977;&#31181;&#31867;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#31181;&#24754;&#35266;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#26469;&#35299;&#20915;&#23427;&#20204;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20551;&#23450;&#26410;&#35266;&#23519;&#21040;&#30340;&#28151;&#28102;&#22240;&#23376;&#26159;&#21487;&#35782;&#21035;&#30340;&#65292;&#24182;&#19988;&#35797;&#22270;&#20943;&#23569;&#28151;&#28102;&#20559;&#24046;&#30340;&#37327;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#23384;&#22312;&#38750;&#21487;&#35782;&#21035;&#30340;&#38544;&#34255;&#28151;&#28102;&#26102;&#20248;&#20110;&#29616;&#26377;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
A prominent challenge of offline reinforcement learning (RL) is the issue of hidden confounding: unobserved variables may influence both the actions taken by the agent and the observed outcomes. Hidden confounding can compromise the validity of any causal conclusion drawn from data and presents a major obstacle to effective offline RL. In the present paper, we tackle the problem of hidden confounding in the nonidentifiable setting. We propose a definition of uncertainty due to hidden confounding bias, termed delphic uncertainty, which uses variation over world models compatible with the observations, and differentiate it from the well-known epistemic and aleatoric uncertainties. We derive a practical method for estimating the three types of uncertainties, and construct a pessimistic offline RL algorithm to account for them. Our method does not assume identifiability of the unobserved confounders, and attempts to reduce the amount of confounding bias. We demonstrate through extensive ex
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20219;&#21153;&#23450;&#20041;&#22312;&#25351;&#20196;&#23398;&#20064;&#20013;&#30340;&#20316;&#29992;&#65292;&#21457;&#29616;&#24403;&#21024;&#38500;&#25551;&#36848;&#20219;&#21153;&#36755;&#20986;&#30340;&#20869;&#23481;&#26102;&#27169;&#22411;&#24615;&#33021;&#25165;&#20250;&#26174;&#33879;&#19979;&#38477;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#31639;&#27861;&#21487;&#20197;&#21387;&#32553;&#20219;&#21153;&#23450;&#20041;&#65292;&#21024;&#38500;60\%&#26631;&#35760;&#20173;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.01150</link><description>&lt;p&gt;
&#20320;&#35835;&#25026;&#20102;&#35828;&#26126;&#20070;&#21527;&#65311;&#37325;&#26032;&#24605;&#32771;&#25351;&#20196;&#23398;&#20064;&#20013;&#20219;&#21153;&#23450;&#20041;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Did You Read the Instructions? Rethinking the Effectiveness of Task Definitions in Instruction Learning. (arXiv:2306.01150v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01150
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20219;&#21153;&#23450;&#20041;&#22312;&#25351;&#20196;&#23398;&#20064;&#20013;&#30340;&#20316;&#29992;&#65292;&#21457;&#29616;&#24403;&#21024;&#38500;&#25551;&#36848;&#20219;&#21153;&#36755;&#20986;&#30340;&#20869;&#23481;&#26102;&#27169;&#22411;&#24615;&#33021;&#25165;&#20250;&#26174;&#33879;&#19979;&#38477;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#31639;&#27861;&#21487;&#20197;&#21387;&#32553;&#20219;&#21153;&#23450;&#20041;&#65292;&#21024;&#38500;60\%&#26631;&#35760;&#20173;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36981;&#24490;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#35299;&#20915;&#26410;&#35265;&#36807;&#30340;&#20219;&#21153;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#27169;&#22411;&#26159;&#21542;&#30495;&#27491;&#29702;&#35299;&#20219;&#21153;&#23450;&#20041;&#20197;&#21450;&#20154;&#31867;&#32534;&#20889;&#30340;&#23450;&#20041;&#26159;&#21542;&#26368;&#20339;&#20173;&#28982;&#19981;&#28165;&#26970;&#12290;&#26412;&#25991;&#31995;&#32479;&#30740;&#31350;&#20102;&#20219;&#21153;&#23450;&#20041;&#22312;&#25351;&#20196;&#23398;&#20064;&#20013;&#30340;&#20316;&#29992;&#12290;&#25105;&#20204;&#39318;&#20808;&#36827;&#34892;&#28040;&#34701;&#20998;&#26512;&#65292;&#36890;&#36807;&#20154;&#31867;&#27880;&#37322;&#21457;&#29616;&#21482;&#26377;&#24403;&#21024;&#38500;&#25551;&#36848;&#20219;&#21153;&#36755;&#20986;&#30340;&#20869;&#23481;&#65292;&#23588;&#20854;&#26159;&#26631;&#31614;&#20449;&#24687;&#26102;&#65292;&#27169;&#22411;&#30340;&#24615;&#33021;&#25165;&#20250;&#26174;&#33879;&#19979;&#38477;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#31639;&#27861;&#65292;&#23558;&#20219;&#21153;&#23450;&#20041;&#21387;&#32553;&#21040;&#26368;&#23567;&#30340;&#25903;&#25345;&#26631;&#35760;&#38598;&#65292;&#24182;&#21457;&#29616;&#21487;&#20197;&#21024;&#38500;60\%&#30340;&#26631;&#35760;&#32780;&#32500;&#25345;&#25110;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;&#22522;&#20110;&#36825;&#20123;&#32467;&#26524;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#31574;&#30053;&#26469;&#24110;&#21161;&#27169;&#22411;&#26356;&#22909;&#22320;&#21033;&#29992;&#20219;&#21153;&#25351;&#20196;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have shown impressive performance in following natural language instructions to solve unseen tasks. However, it remains unclear whether models truly understand task definitions and whether the human-written definitions are optimal. In this paper, we systematically study the role of task definitions in instruction learning. We first conduct an ablation analysis informed by human annotations to understand which parts of a task definition are most important, and find that model performance only drops substantially when removing contents describing the task output, in particular label information. Next, we propose an automatic algorithm to compress task definitions to a minimal supporting set of tokens, and find that 60\% of tokens can be removed while maintaining or even improving model performance. Based on these results, we propose two strategies to help models better leverage task instructions: (1) providing only key information for tasks in a common struct
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#31456;&#30740;&#31350;&#20102;AI&#36131;&#20219;&#20445;&#38505;&#65292;&#24182;&#20197;AI&#36741;&#21161;&#30340;&#30005;&#23376;&#35786;&#26029;&#31995;&#32479;&#20026;&#20363;&#36827;&#34892;&#20102;&#23450;&#37327;&#39118;&#38505;&#35780;&#20272;&#12290;AI&#36131;&#20219;&#20445;&#38505;&#21487;&#20197;&#20316;&#20026;&#30417;&#31649;&#26426;&#21046;&#26469;&#28608;&#21169;&#21512;&#35268;&#34892;&#20026;&#65292;&#24182;&#20026;&#39640;&#36136;&#37327;AI&#31995;&#32479;&#25552;&#20379;&#35777;&#20070;&#12290;</title><link>http://arxiv.org/abs/2306.01149</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#36131;&#20219;&#20445;&#38505;&#65306;&#20197;AI&#36741;&#21161;&#30005;&#23376;&#35786;&#26029;&#31995;&#32479;&#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
AI Liability Insurance With an Example in AI-Powered E-diagnosis System. (arXiv:2306.01149v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01149
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#31456;&#30740;&#31350;&#20102;AI&#36131;&#20219;&#20445;&#38505;&#65292;&#24182;&#20197;AI&#36741;&#21161;&#30340;&#30005;&#23376;&#35786;&#26029;&#31995;&#32479;&#20026;&#20363;&#36827;&#34892;&#20102;&#23450;&#37327;&#39118;&#38505;&#35780;&#20272;&#12290;AI&#36131;&#20219;&#20445;&#38505;&#21487;&#20197;&#20316;&#20026;&#30417;&#31649;&#26426;&#21046;&#26469;&#28608;&#21169;&#21512;&#35268;&#34892;&#20026;&#65292;&#24182;&#20026;&#39640;&#36136;&#37327;AI&#31995;&#32479;&#25552;&#20379;&#35777;&#20070;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#22312;&#35768;&#22810;&#39046;&#22495;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#30001;&#20110;AI-powered&#31995;&#32479;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#21644;&#39118;&#38505;&#65292;&#22312;&#37326;&#22806;&#36816;&#29992;&#20013;&#24341;&#36215;&#20102;&#29369;&#35947;&#12290;&#20316;&#20026;&#19968;&#31181;&#32463;&#27982;&#35299;&#20915;&#26041;&#26696;&#26469;&#24357;&#34917;&#28508;&#22312;&#25439;&#23475;&#65292;AI&#36131;&#20219;&#20445;&#38505;&#26159;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#24066;&#22330;&#65292;&#21487;&#20197;&#22686;&#24378;AI&#23545;&#26085;&#24120;&#29983;&#27963;&#30340;&#25972;&#21512;&#12290;&#26412;&#25991;&#20197;&#19968;&#20010;AI&#36741;&#21161;&#30340;&#30005;&#23376;&#35786;&#26029;&#31995;&#32479;&#20026;&#20363;&#65292;&#30740;&#31350;AI&#36131;&#20219;&#20445;&#38505;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#23450;&#37327;&#39118;&#38505;&#35780;&#20272;&#27169;&#22411;&#65292;&#24182;&#36827;&#34892;&#20102;&#22522;&#20110;&#35777;&#25454;&#30340;&#25968;&#20540;&#20998;&#26512;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;AI&#25216;&#26415;&#30340;&#21487;&#20445;&#38505;&#24615;&#26631;&#20934;&#65292;&#24182;&#24314;&#35758;&#24517;&#35201;&#30340;&#35843;&#25972;&#26469;&#36866;&#24212;AI&#20135;&#21697;&#30340;&#29305;&#28857;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;AI&#36131;&#20219;&#20445;&#38505;&#21487;&#20197;&#20316;&#20026;&#19968;&#31181;&#30417;&#31649;&#26426;&#21046;&#26469;&#28608;&#21169;&#21512;&#35268;&#34892;&#20026;&#65292;&#24182;&#20316;&#20026;&#39640;&#36136;&#37327;AI&#31995;&#32479;&#30340;&#35777;&#20070;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24314;&#35758;&#35843;&#25972;&#20445;&#36153;&#20197;&#21453;&#26144;AI&#20869;&#22312;&#19981;&#30830;&#23450;&#24615;&#30340;&#21160;&#24577;&#28436;&#21464;&#12290;&#35752;&#35770;&#20102;&#36947;&#24503;&#39118;&#38505;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#23545;AI&#20135;&#21697;&#36827;&#34892;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial Intelligence (AI) has received an increasing amount of attention in multiple areas. The uncertainties and risks in AI-powered systems have created reluctance in their wild adoption. As an economic solution to compensate for potential damages, AI liability insurance is a promising market to enhance the integration of AI into daily life. In this work, we use an AI-powered E-diagnosis system as an example to study AI liability insurance. We provide a quantitative risk assessment model with evidence-based numerical analysis. We discuss the insurability criteria for AI technologies and suggest necessary adjustments to accommodate the features of AI products. We show that AI liability insurance can act as a regulatory mechanism to incentivize compliant behaviors and serve as a certificate of high-quality AI systems. Furthermore, we suggest premium adjustment to reflect the dynamic evolution of the inherent uncertainty in AI. Moral hazard problems are discussed and suggestions for 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22359;--&#24179;&#28369;min-max(SMM)&#32593;&#32476;&#65292;&#30456;&#27604;&#20110;&#20256;&#32479;&#30340;min-max(MM)&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#31616;&#21333;&#26131;&#29992;&#65292;&#22312;&#21333;&#35843;&#24314;&#27169;&#26041;&#38754;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>http://arxiv.org/abs/2306.01147</link><description>&lt;p&gt;
&#24179;&#28369;&#21333;&#35843;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Smooth Monotonic Networks. (arXiv:2306.01147v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01147
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22359;--&#24179;&#28369;min-max(SMM)&#32593;&#32476;&#65292;&#30456;&#27604;&#20110;&#20256;&#32479;&#30340;min-max(MM)&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#31616;&#21333;&#26131;&#29992;&#65292;&#22312;&#21333;&#35843;&#24314;&#27169;&#26041;&#38754;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21333;&#35843;&#24615;&#32422;&#26463;&#26159;&#32479;&#35745;&#24314;&#27169;&#20013;&#30340;&#24378;&#21147;&#27491;&#21017;&#21270;&#24037;&#20855;&#12290;&#23427;&#20204;&#21487;&#20197;&#22312;&#35745;&#31639;&#26426;&#25903;&#25345;&#30340;&#20915;&#31574;&#21046;&#23450;&#20013;&#25903;&#25345;&#20844;&#24179;&#24615;&#65292;&#24182;&#22686;&#21152;&#25968;&#25454;&#39537;&#21160;&#31185;&#23398;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;&#12290;&#32463;&#20856;&#30340;min-max(MM)&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#30830;&#20445;&#20102;&#21333;&#35843;&#24615;&#65292;&#20294;&#30001;&#20110;&#26799;&#24230;&#28040;&#22833;&#32780;&#24448;&#24448;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#38519;&#20837;&#19981;&#33391;&#23616;&#37096;&#26368;&#20248;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;MM&#32593;&#32476;&#30340;&#31616;&#21333;&#20462;&#25913;&#65292;&#20351;&#29992;&#20005;&#26684;&#36882;&#22686;&#30340;&#24179;&#28369;&#38750;&#32447;&#24615;&#20989;&#25968;&#26469;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#12290;&#24471;&#21040;&#30340;&#24179;&#28369;min-max(SMM)&#32593;&#32476;&#27169;&#22359;&#32487;&#25215;&#20102;MM&#26550;&#26500;&#30340;&#28176;&#36817;&#36924;&#36817;&#24615;&#36136;&#12290;&#23427;&#21487;&#20197;&#23884;&#20837;&#21040;&#26356;&#22823;&#30340;&#31471;&#21040;&#31471;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#20013;&#36827;&#34892;&#35757;&#32451;&#12290;&#22312;&#21333;&#35843;&#24314;&#27169;&#30340;&#31070;&#32463;&#32593;&#32476;&#26041;&#38754;&#65292;SMM&#27169;&#22359;&#35201;&#31616;&#21333;&#24471;&#22810;&#65292;&#35745;&#31639;&#38656;&#27714;&#20063;&#35201;&#23569;&#24471;&#22810;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;&#23427;&#22312;&#27867;&#21270;&#24615;&#33021;&#26041;&#38754;&#19982;&#26367;&#20195;&#31070;&#32463;&#21644;&#38750;&#31070;&#32463;&#26041;&#27861;&#30456;&#27604;&#34920;&#29616;&#24471;&#26356;&#20026;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Monotonicity constraints are powerful regularizers in statistical modelling. They can support fairness in computer supported decision making and increase plausibility in data-driven scientific models. The seminal min-max (MM) neural network architecture ensures monotonicity, but often gets stuck in undesired local optima during training because of vanishing gradients. We propose a simple modification of the MM network using strictly-increasing smooth non-linearities that alleviates this problem. The resulting smooth min-max (SMM) network module inherits the asymptotic approximation properties from the MM architecture. It can be used within larger deep learning systems trained end-to-end. The SMM module is considerably simpler and less computationally demanding than state-of-the-art neural networks for monotonic modelling. Still, in our experiments, it compared favorably to alternative neural and non-neural approaches in terms of generalization performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#19981;&#21487;&#23519;&#35273;&#25915;&#20987;&#26041;&#27861;&#65292;&#26088;&#22312;&#26377;&#25928;&#22320;&#38477;&#20302;&#23398;&#20064;&#22270;&#20687;&#21387;&#32553;&#30340;&#37325;&#26500;&#36136;&#37327;&#65292;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;Frobenius&#33539;&#25968;&#30340;&#25439;&#22833;&#20989;&#25968;&#29983;&#25104;&#23545;&#25239;&#26679;&#26412;&#65292;&#36827;&#19968;&#27493;&#21033;&#29992;&#39640;&#39057;&#20998;&#37327;&#23545;&#20154;&#30524;&#35270;&#35273;&#19981;&#25935;&#24863;&#30340;&#29305;&#28857;&#65292;&#20197;&#30830;&#20445;&#25200;&#21160;&#20445;&#25345;&#19981;&#26174;&#30524;&#65292;&#35813;&#26041;&#27861;&#22312;Kodak&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#20102;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.01125</link><description>&lt;p&gt;
&#21033;&#29992;&#19981;&#21487;&#24863;&#30693;&#30340;&#25200;&#21160;&#25913;&#21892;&#30340;&#23398;&#20064;&#22270;&#20687;&#21387;&#32553;&#30340;&#37325;&#26500;&#22833;&#30495;
&lt;/p&gt;
&lt;p&gt;
Reconstruction Distortion of Learned Image Compression with Imperceptible Perturbations. (arXiv:2306.01125v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01125
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#19981;&#21487;&#23519;&#35273;&#25915;&#20987;&#26041;&#27861;&#65292;&#26088;&#22312;&#26377;&#25928;&#22320;&#38477;&#20302;&#23398;&#20064;&#22270;&#20687;&#21387;&#32553;&#30340;&#37325;&#26500;&#36136;&#37327;&#65292;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;Frobenius&#33539;&#25968;&#30340;&#25439;&#22833;&#20989;&#25968;&#29983;&#25104;&#23545;&#25239;&#26679;&#26412;&#65292;&#36827;&#19968;&#27493;&#21033;&#29992;&#39640;&#39057;&#20998;&#37327;&#23545;&#20154;&#30524;&#35270;&#35273;&#19981;&#25935;&#24863;&#30340;&#29305;&#28857;&#65292;&#20197;&#30830;&#20445;&#25200;&#21160;&#20445;&#25345;&#19981;&#26174;&#30524;&#65292;&#35813;&#26041;&#27861;&#22312;Kodak&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#20102;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#23398;&#20064;&#22270;&#20687;&#21387;&#32553;&#65288;LIC&#65289;&#22240;&#20854;&#38750;&#20961;&#30340;&#24615;&#33021;&#32780;&#25104;&#20026;&#22270;&#20687;&#20256;&#36755;&#30340;&#36235;&#21183;&#25216;&#26415;&#12290;&#23613;&#31649;&#20854;&#21463;&#27426;&#36814;&#24230;&#24456;&#39640;&#65292;&#20294;&#26159;LIC&#22312;&#22270;&#20687;&#37325;&#26500;&#36136;&#37327;&#26041;&#38754;&#30340;&#31283;&#20581;&#24615;&#20173;&#26410;&#24471;&#21040;&#25506;&#32034;&#12290;&#26412;&#25991;&#24341;&#20837;&#19968;&#31181;&#19981;&#21487;&#24863;&#30693;&#25915;&#20987;&#26041;&#27861;&#65292;&#26088;&#22312;&#26377;&#25928;&#38477;&#20302;LIC&#30340;&#37325;&#26500;&#36136;&#37327;&#65292;&#23548;&#33268;&#37325;&#26500;&#22270;&#20687;&#34987;&#22122;&#22768;&#20005;&#37325;&#24178;&#25200;&#65292;&#37325;&#26500;&#22270;&#20687;&#20013;&#30340;&#20219;&#20309;&#23545;&#35937;&#37117;&#20960;&#20046;&#19981;&#21487;&#33021;&#24674;&#22797;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;Frobenius&#33539;&#25968;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#29983;&#25104;&#23545;&#25239;&#26679;&#26412;&#65292;&#20197;&#26368;&#22823;&#21270;&#21407;&#22987;&#22270;&#20687;&#19982;&#37325;&#26500;&#23545;&#25239;&#26679;&#26412;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#36827;&#19968;&#27493;&#21033;&#29992;&#39640;&#39057;&#20998;&#37327;&#23545;&#20154;&#30524;&#35270;&#35273;&#19981;&#25935;&#24863;&#30340;&#29305;&#28857;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19981;&#21487;&#23519;&#35273;&#24615;&#32422;&#26463;&#65288;IC&#65289;&#65292;&#20197;&#30830;&#20445;&#25200;&#21160;&#20445;&#25345;&#19981;&#26174;&#30524;&#12290;&#22312;Kodak&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;&#21508;&#31181;LIC&#27169;&#22411;&#36827;&#34892;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learned Image Compression (LIC) has recently become the trending technique for image transmission due to its notable performance. Despite its popularity, the robustness of LIC with respect to the quality of image reconstruction remains under-explored. In this paper, we introduce an imperceptible attack approach designed to effectively degrade the reconstruction quality of LIC, resulting in the reconstructed image being severely disrupted by noise where any object in the reconstructed images is virtually impossible. More specifically, we generate adversarial examples by introducing a Frobenius norm-based loss function to maximize the discrepancy between original images and reconstructed adversarial examples. Further, leveraging the insensitivity of high-frequency components to human vision, we introduce Imperceptibility Constraint (IC) to ensure that the perturbations remain inconspicuous. Experiments conducted on the Kodak dataset using various LIC models demonstrate effectiveness. In 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#37325;&#23614;&#22870;&#21169;&#30340;&#26377;&#38480;&#27493;&#39588;&#34920;&#26684;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#38382;&#39064;&#25506;&#35752;&#20102;&#24046;&#20998;&#38544;&#31169;&#38480;&#21046;&#19979;&#30340;&#20004;&#31181;&#26694;&#26550;&#65292;&#21363;&#20215;&#20540;&#36845;&#20195;&#21644;&#31574;&#30053;&#20248;&#21270;&#65292;&#21516;&#26102;&#32771;&#34385;&#20102;&#32852;&#21512;&#24046;&#20998;&#38544;&#31169;&#21644;&#26412;&#22320;&#24046;&#20998;&#38544;&#31169;&#27169;&#22411;&#65292;&#24182;&#20026;&#20004;&#31181;&#24773;&#20917;&#25552;&#20379;&#20102;&#36951;&#25022;&#19978;&#38480;&#12290;</title><link>http://arxiv.org/abs/2306.01121</link><description>&lt;p&gt;
&#24102;&#26377;&#37325;&#23614;&#22870;&#21169;&#30340;&#24046;&#20998;&#38544;&#31169;&#24335;&#24773;&#33410;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Differentially Private Episodic Reinforcement Learning with Heavy-tailed Rewards. (arXiv:2306.01121v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01121
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#37325;&#23614;&#22870;&#21169;&#30340;&#26377;&#38480;&#27493;&#39588;&#34920;&#26684;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#38382;&#39064;&#25506;&#35752;&#20102;&#24046;&#20998;&#38544;&#31169;&#38480;&#21046;&#19979;&#30340;&#20004;&#31181;&#26694;&#26550;&#65292;&#21363;&#20215;&#20540;&#36845;&#20195;&#21644;&#31574;&#30053;&#20248;&#21270;&#65292;&#21516;&#26102;&#32771;&#34385;&#20102;&#32852;&#21512;&#24046;&#20998;&#38544;&#31169;&#21644;&#26412;&#22320;&#24046;&#20998;&#38544;&#31169;&#27169;&#22411;&#65292;&#24182;&#20026;&#20004;&#31181;&#24773;&#20917;&#25552;&#20379;&#20102;&#36951;&#25022;&#19978;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24046;&#20998;&#38544;&#31169;(DP)&#38480;&#21046;&#19979;&#30340;&#37325;&#23614;&#22870;&#21169;&#30340;&#65288;&#26377;&#38480;&#27493;&#39588;&#34920;&#26684;&#65289;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;(MDP)&#38382;&#39064;&#12290;&#19982;&#20808;&#21069;&#30340;&#31169;&#26377;&#24378;&#21270;&#23398;&#20064;&#30740;&#31350;&#36890;&#24120;&#20551;&#35774;&#22870;&#21169;&#26469;&#33258;&#19968;&#20123;&#26377;&#30028;&#25110;&#27425;&#39640;&#26031;&#20998;&#24067;&#20197;&#30830;&#20445;DP&#30456;&#27604;&#65292;&#25105;&#20204;&#32771;&#34385;&#22870;&#21169;&#20998;&#24067;&#21482;&#26377;&#26377;&#38480;&#30340;$(1+v)$&#38454;&#30697;&#30340;&#24773;&#20917;&#65292;$v \in (0,1]$&#12290;&#36890;&#36807;&#20351;&#29992;&#22870;&#21169;&#30340;&#20581;&#22766;&#22343;&#20540;&#20272;&#35745;&#22120;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#20004;&#31181;&#38024;&#23545;&#37325;&#23614;MDP&#30340;&#26694;&#26550;&#65292;&#21363;&#19968;&#20010;&#29992;&#20110;&#20215;&#20540;&#36845;&#20195;&#65292;&#21478;&#19968;&#20010;&#29992;&#20110;&#31574;&#30053;&#20248;&#21270;&#12290;&#22312;&#27599;&#20010;&#26694;&#26550;&#19979;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#32852;&#21512;&#24046;&#20998;&#38544;&#31169;(JDP)&#21644;&#26412;&#22320;&#24046;&#20998;&#38544;&#31169;(LDP)&#27169;&#22411;&#12290;&#22522;&#20110;&#25105;&#20204;&#30340;&#26694;&#26550;&#65292;&#25105;&#20204;&#20026;JDP&#21644;LDP&#24773;&#20917;&#25552;&#20379;&#20102;&#36951;&#25022;&#19978;&#38480;&#65292;&#24182;&#34920;&#26126;&#20998;&#24067;&#30340;&#30697;&#21644;&#38544;&#31169;&#39044;&#31639;&#37117;&#23545;&#36951;&#25022;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#36951;&#25022;&#26368;&#23567;&#21270;&#30340;&#19979;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we study the problem of (finite horizon tabular) Markov decision processes (MDPs) with heavy-tailed rewards under the constraint of differential privacy (DP). Compared with the previous studies for private reinforcement learning that typically assume rewards are sampled from some bounded or sub-Gaussian distributions to ensure DP, we consider the setting where reward distributions have only finite $(1+v)$-th moments with some $v \in (0,1]$. By resorting to robust mean estimators for rewards, we first propose two frameworks for heavy-tailed MDPs, i.e., one is for value iteration and another is for policy optimization. Under each framework, we consider both joint differential privacy (JDP) and local differential privacy (LDP) models. Based on our frameworks, we provide regret upper bounds for both JDP and LDP cases and show that the moment of distribution and privacy budget both have significant impacts on regrets. Finally, we establish a lower bound of regret minimization
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21457;&#29616;&#32463;&#36807;&#36866;&#24403;&#30340;&#31579;&#36873;&#21644;&#21435;&#37325;&#21518;&#65292;&#20165;&#20973;&#32593;&#32476;&#25968;&#25454;&#23601;&#33021;&#35757;&#32451;&#20986;&#24378;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#36825;&#29978;&#33267;&#26126;&#26174;&#20248;&#20110;&#20351;&#29992;&#31579;&#36873;&#21518;&#30340;&#35821;&#26009;&#24211;&#35757;&#32451;&#12290;&#30740;&#31350;&#36824;&#34920;&#26126;&#65292;&#21363;&#20351;&#22312;&#24191;&#27867;&#31579;&#36873;&#21518;&#65292;&#20174;&#32593;&#32476;&#20013;&#25552;&#21462;&#30340;&#39640;&#36136;&#37327;&#25968;&#25454;&#20173;&#28982;&#20805;&#36275;&#65292;&#36825;&#20026;&#35757;&#32451;&#26356;&#22823;&#12289;&#26356;&#24378;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#24102;&#26469;&#20102;&#24076;&#26395;&#12290;</title><link>http://arxiv.org/abs/2306.01116</link><description>&lt;p&gt;
Falcon LLM&#30340;RefinedWeb&#25968;&#25454;&#38598;&#65306;&#20165;&#20973;&#32593;&#32476;&#25968;&#25454;&#32988;&#36807;&#31579;&#36873;&#21518;&#30340;&#35821;&#26009;&#24211;
&lt;/p&gt;
&lt;p&gt;
The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only. (arXiv:2306.01116v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01116
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21457;&#29616;&#32463;&#36807;&#36866;&#24403;&#30340;&#31579;&#36873;&#21644;&#21435;&#37325;&#21518;&#65292;&#20165;&#20973;&#32593;&#32476;&#25968;&#25454;&#23601;&#33021;&#35757;&#32451;&#20986;&#24378;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#36825;&#29978;&#33267;&#26126;&#26174;&#20248;&#20110;&#20351;&#29992;&#31579;&#36873;&#21518;&#30340;&#35821;&#26009;&#24211;&#35757;&#32451;&#12290;&#30740;&#31350;&#36824;&#34920;&#26126;&#65292;&#21363;&#20351;&#22312;&#24191;&#27867;&#31579;&#36873;&#21518;&#65292;&#20174;&#32593;&#32476;&#20013;&#25552;&#21462;&#30340;&#39640;&#36136;&#37327;&#25968;&#25454;&#20173;&#28982;&#20805;&#36275;&#65292;&#36825;&#20026;&#35757;&#32451;&#26356;&#22823;&#12289;&#26356;&#24378;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#24102;&#26469;&#20102;&#24076;&#26395;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#24120;&#30001;&#32463;&#36807;&#31579;&#36873;&#30340;&#32593;&#32476;&#25968;&#25454;&#21644;&#32463;&#36807;&#31579;&#36873;&#30340;&#39640;&#36136;&#37327;&#35821;&#26009;&#24211;&#65288;&#22914;&#31038;&#20132;&#23186;&#20307;&#23545;&#35805;&#12289;&#20070;&#31821;&#25110;&#25216;&#26415;&#35770;&#25991;&#65289;&#28151;&#21512;&#35757;&#32451;&#12290;&#36825;&#31181;&#31579;&#36873;&#36807;&#31243;&#34987;&#35748;&#20026;&#26159;&#20135;&#29983;&#20855;&#26377;&#24191;&#27867;&#38646;-shot&#27867;&#21270;&#33021;&#21147;&#30340;&#39640;&#24615;&#33021;&#27169;&#22411;&#25152;&#24517;&#38656;&#30340;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#32771;&#34385;&#38656;&#35201;&#39044;&#20808;&#35757;&#32451;&#25968;&#19975;&#20159;&#20010;&#26631;&#35760;&#30340;&#26356;&#22823;&#27169;&#22411;&#65292;&#31579;&#36873;&#30340;&#21487;&#25193;&#23637;&#24615;&#26159;&#21542;&#20250;&#20986;&#29616;&#29942;&#39048;&#20197;&#21450;&#25105;&#20204;&#26159;&#21542;&#20250;&#24456;&#24555;&#29992;&#23613;&#29420;&#29305;&#30340;&#39640;&#36136;&#37327;&#25968;&#25454;&#20173;&#28982;&#19981;&#28165;&#26970;&#12290;&#19982;&#20197;&#24448;&#30340;&#24819;&#27861;&#30456;&#21453;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#32463;&#36807;&#36866;&#24403;&#31579;&#36873;&#21644;&#21435;&#37325;&#30340;&#32593;&#32476;&#25968;&#25454;&#21487;&#20197;&#23548;&#33268;&#21151;&#33021;&#24378;&#22823;&#30340;&#27169;&#22411;&#65307;&#29978;&#33267;&#26126;&#26174;&#20248;&#20110;&#35757;&#32451;&#22312;The Pile &#19978;&#30340;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#12290;&#23613;&#31649;&#32463;&#36807;&#20102;&#24191;&#27867;&#30340;&#36807;&#28388;&#65292;&#25105;&#20204;&#20174;&#32593;&#32476;&#20013;&#25552;&#21462;&#30340;&#39640;&#36136;&#37327;&#25968;&#25454;&#20173;&#28982;&#24456;&#20016;&#23500;&#65292;&#25105;&#20204;&#33021;&#22815;&#20174;CommonCrawl&#20013;&#33719;&#24471;&#20116;&#19975;&#20159;&#20010;&#26631;&#35760;&#12290;&#25105;&#20204;&#20844;&#24320;&#21457;&#24067;&#20102;&#20174;&#25105;&#20204;&#30340;RefinedWeb&#25968;&#25454;&#38598;&#20013;&#25552;&#21462;&#30340;6000&#20159;&#20010;&#26631;&#35760;&#30340;&#29255;&#27573;&#65292;&#20197;&#21450;&#22312;&#27492;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;1.3 / 7.5B&#21442;&#25968;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models are commonly trained on a mixture of filtered web data and curated high-quality corpora, such as social media conversations, books, or technical papers. This curation process is believed to be necessary to produce performant models with broad zero-shot generalization abilities. However, as larger models requiring pretraining on trillions of tokens are considered, it is unclear how scalable is curation and whether we will run out of unique high-quality data soon. At variance with previous beliefs, we show that properly filtered and deduplicated web data alone can lead to powerful models; even significantly outperforming models from the state-of-the-art trained on The Pile. Despite extensive filtering, the high-quality data we extract from the web is still plentiful, and we are able to obtain five trillion tokens from CommonCrawl. We publicly release an extract of 600 billion tokens from our RefinedWeb dataset, and 1.3/7.5B parameters language models trained on it.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#22122;&#22768;&#22914;&#20309;&#24433;&#21709;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#28966;&#34385;&#26816;&#27979;&#27169;&#22411;&#65292;&#24182;&#24320;&#21457;&#20986;&#22312;&#22024;&#26434;&#30340;&#29616;&#23454;&#29615;&#22659;&#20013;&#20855;&#26377;&#25239;&#24178;&#25200;&#24615;&#21644;&#36866;&#24212;&#24615;&#30340;&#27169;&#22411;&#65292;&#20197;&#25512;&#36827;&#35813;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;</title><link>http://arxiv.org/abs/2306.01110</link><description>&lt;p&gt;
&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#28966;&#34385;&#26816;&#27979;&#20013;&#22122;&#22768;&#24433;&#21709;&#30340;&#27604;&#36739;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Comparative Study on the Effects of Noise in ML-Based Anxiety Detection. (arXiv:2306.01110v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01110
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#22122;&#22768;&#22914;&#20309;&#24433;&#21709;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#28966;&#34385;&#26816;&#27979;&#27169;&#22411;&#65292;&#24182;&#24320;&#21457;&#20986;&#22312;&#22024;&#26434;&#30340;&#29616;&#23454;&#29615;&#22659;&#20013;&#20855;&#26377;&#25239;&#24178;&#25200;&#24615;&#21644;&#36866;&#24212;&#24615;&#30340;&#27169;&#22411;&#65292;&#20197;&#25512;&#36827;&#35813;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31359;&#25140;&#24335;&#20581;&#24247;&#35774;&#22791;&#27491;&#22312;&#24341;&#39046;&#19968;&#31181;&#26032;&#26102;&#20195;&#30340;&#36830;&#32493;&#21644;&#38750;&#20405;&#20837;&#24615;&#36828;&#31243;&#30417;&#27979;&#12290;&#20854;&#20013;&#19968;&#39033;&#24212;&#29992;&#26159;&#29992;&#20110;&#28966;&#34385;&#26816;&#27979;&#12290;&#35768;&#22810;&#28966;&#34385;&#26816;&#27979;&#26041;&#38754;&#30340;&#36827;&#23637;&#21457;&#29983;&#22312;&#21463;&#25511;&#23454;&#39564;&#23460;&#29615;&#22659;&#20013;&#65292;&#20294;&#22122;&#22768;&#38459;&#27490;&#20102;&#36825;&#20123;&#36827;&#23637;&#25512;&#24191;&#21040;&#29616;&#23454;&#19990;&#30028;&#30340;&#26465;&#20214;&#19979;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#30740;&#31350;&#22122;&#22768;&#22914;&#20309;&#24433;&#21709;&#27169;&#22411;&#24615;&#33021;&#24182;&#24320;&#21457;&#23545;&#22024;&#26434;&#30340;&#29616;&#23454;&#29615;&#22659;&#20855;&#26377;&#25239;&#24178;&#25200;&#24615;&#21644;&#36866;&#24212;&#26085;&#24120;&#29983;&#27963;&#20013;&#28151;&#20081;&#30340;&#27169;&#22411;&#65292;&#20174;&#32780;&#25512;&#36827;&#35813;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;&#25105;&#20204;&#23581;&#35797;&#30740;&#31350;&#20808;&#21069;&#30340;&#26041;&#27861;&#20026;&#20309;&#22833;&#36133;&#65292;&#24182;&#20351;&#29992;&#21487;&#31359;&#25140;&#36127;&#33655;&#19982;&#24773;&#24863;&#26816;&#27979;&#65288;WESAD&#65289;&#25968;&#25454;&#38598;&#65292;&#22312;&#19977;&#31867;&#20998;&#31867;&#38382;&#39064;&#65288;&#22522;&#20934;&#20540; vs. &#21387;&#21147; vs. &#24841;&#24742;&#65289;&#20013;&#27604;&#36739;&#19981;&#21516;&#24378;&#24230;&#22122;&#22768;&#23545;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20998;&#31867;&#29983;&#29702;&#21796;&#37266;&#31561;&#32423;&#30340;&#24433;&#21709;&#12290;&#22312;&#24341;&#20837;&#22122;&#22768;&#20043;&#21069;&#65292;&#25105;&#20204;&#22522;&#20934;&#27169;&#22411;&#30340;&#24615;&#33021;&#36798;&#21040;&#20102;98.7&#65285;&#65292;&#32780;Schmidt 2018&#24180;&#30340;&#27169;&#22411;&#20165;&#36798;&#21040;&#20102;80.3&#65285;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#21487;&#33021;&#30340;&#35299;&#20915;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Wearable health devices are ushering in a new age of continuous and noninvasive remote monitoring. One application of this technology is in anxiety detection. Many advancements in anxiety detection have happened in controlled lab settings, but noise prevents these advancements from generalizing to real-world conditions. We seek to progress the field by studying how noise impacts model performance and developing models that are robust to noisy, real-world conditions and, hence, attuned to the commotion of everyday life. In this study we look to investigate why and how previous methods have failed. Using the wearable stress and affect detection (WESAD) dataset, we compare the effect of various intensities of noise on machine learning models classifying levels of physiological arousal in the three-class classification problem: baseline vs. stress vs. amusement. Before introducing noise, our baseline model performance reaches 98.7%, compared to Schmidt 2018's 80.3%. We discuss potential so
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#26631;&#31614;&#21644;&#29615;&#22659;&#22240;&#26524;&#29420;&#31435;&#24615;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#22270;&#24418;&#36229;&#20986;&#20998;&#24067;&#65288;OOD&#65289;&#27867;&#21270;&#38382;&#39064;&#65292;&#36890;&#36807;&#25932;&#23545;&#35757;&#32451;&#31574;&#30053;&#26469;&#32852;&#21512;&#20248;&#21270;&#23646;&#24615;&#20197;&#33719;&#24471;&#26377;&#25928;&#32467;&#26524;&#65292;&#23454;&#39564;&#35777;&#26126;LECI&#26174;&#30528;&#20248;&#20110;&#20043;&#21069;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.01103</link><description>&lt;p&gt;
&#22312;&#22270;&#24418;&#30340;&#36229;&#20986;&#20998;&#24067;&#27867;&#21270;&#20013;&#23398;&#20064;&#26631;&#31614;&#21644;&#29615;&#22659;&#22240;&#26524;&#29420;&#31435;&#24615;
&lt;/p&gt;
&lt;p&gt;
Joint Learning of Label and Environment Causal Independence for Graph Out-of-Distribution Generalization. (arXiv:2306.01103v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01103
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#26631;&#31614;&#21644;&#29615;&#22659;&#22240;&#26524;&#29420;&#31435;&#24615;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#22270;&#24418;&#36229;&#20986;&#20998;&#24067;&#65288;OOD&#65289;&#27867;&#21270;&#38382;&#39064;&#65292;&#36890;&#36807;&#25932;&#23545;&#35757;&#32451;&#31574;&#30053;&#26469;&#32852;&#21512;&#20248;&#21270;&#23646;&#24615;&#20197;&#33719;&#24471;&#26377;&#25928;&#32467;&#26524;&#65292;&#23454;&#39564;&#35777;&#26126;LECI&#26174;&#30528;&#20248;&#20110;&#20043;&#21069;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35299;&#20915;&#20102;&#22270;&#24418;&#30340;&#36229;&#20986;&#20998;&#24067;&#65288;OOD&#65289;&#27867;&#21270;&#38382;&#39064;&#12290;&#29616;&#26377;&#30340;&#22270;&#24418;OOD&#31639;&#27861;&#35201;&#20040;&#20381;&#36182;&#20110;&#21463;&#38480;&#30340;&#20551;&#35774;&#65292;&#35201;&#20040;&#26080;&#27861;&#21033;&#29992;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#29615;&#22659;&#20449;&#24687;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#21516;&#26102;&#32435;&#20837;&#26631;&#31614;&#21644;&#29615;&#22659;&#22240;&#26524;&#29420;&#31435;&#65288;LECI&#65289;&#65292;&#20805;&#20998;&#21033;&#29992;&#26631;&#31614;&#21644;&#29615;&#22659;&#20449;&#24687;&#65292;&#20174;&#32780;&#35299;&#20915;&#20043;&#21069;&#30340;&#26041;&#27861;&#22312;&#35782;&#21035;&#22240;&#26524;&#21644;&#19981;&#21464;&#23376;&#22270;&#26102;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#24320;&#21457;&#20102;&#19968;&#31181;&#25932;&#23545;&#35757;&#32451;&#31574;&#30053;&#65292;&#20197;&#32852;&#21512;&#20248;&#21270;&#36825;&#20004;&#20010;&#23646;&#24615;&#65292;&#29992;&#20110;&#20855;&#26377;&#29702;&#35770;&#20445;&#35777;&#30340;&#23548;&#33268;&#23376;&#22270;&#21457;&#29616;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#21644;&#20998;&#26512;&#34920;&#26126;&#65292;LECI&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#37117;&#26174;&#30528;&#20248;&#20110;&#20043;&#21069;&#30340;&#26041;&#27861;&#65292;&#23558;LECI&#30830;&#31435;&#20026;&#22270;&#24418;OOD&#27867;&#21270;&#30340;&#23454;&#29992;&#26377;&#25928;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
We tackle the problem of graph out-of-distribution (OOD) generalization. Existing graph OOD algorithms either rely on restricted assumptions or fail to exploit environment information in training data. In this work, we propose to simultaneously incorporate label and environment causal independence (LECI) to fully make use of label and environment information, thereby addressing the challenges faced by prior methods on identifying causal and invariant subgraphs. We further develop an adversarial training strategy to jointly optimize these two properties for casual subgraph discovery with theoretical guarantees. Extensive experiments and analysis show that LECI significantly outperforms prior methods on both synthetic and real-world datasets, establishing LECI as a practical and effective solution for graph OOD generalization.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#21033;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#21644;&#22810;&#26679;&#24615;&#20248;&#21270;&#31639;&#27861;&#30456;&#32467;&#21512;&#30340; LLMatic &#31070;&#32463;&#32467;&#26500;&#25628;&#32034;&#31639;&#27861;&#12290;&#35813;&#31639;&#27861;&#22312;CIFAR-10&#25968;&#25454;&#38598;&#36827;&#34892;&#27979;&#35797;&#65292;&#20165;&#36827;&#34892;2000&#27425;&#25628;&#32034;&#21363;&#21487;&#20135;&#29983;&#39640;&#24615;&#33021;&#32593;&#32476;&#65292;&#21363;&#20351;&#27809;&#26377;&#35813;&#22522;&#20934;&#39046;&#22495;&#30340;&#20808;&#21069;&#30693;&#35782;&#25110;&#20219;&#20309;&#20808;&#21069;&#30340;&#26368;&#20339;&#32467;&#26524;&#30340;&#26333;&#20809;&#12290;</title><link>http://arxiv.org/abs/2306.01102</link><description>&lt;p&gt;
LLMatic: &#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#21644;&#22810;&#26679;&#24615;&#20248;&#21270;&#30340;&#31070;&#32463;&#32467;&#26500;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
LLMatic: Neural Architecture Search via Large Language Models and Quality-Diversity Optimization. (arXiv:2306.01102v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01102
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#21033;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#21644;&#22810;&#26679;&#24615;&#20248;&#21270;&#31639;&#27861;&#30456;&#32467;&#21512;&#30340; LLMatic &#31070;&#32463;&#32467;&#26500;&#25628;&#32034;&#31639;&#27861;&#12290;&#35813;&#31639;&#27861;&#22312;CIFAR-10&#25968;&#25454;&#38598;&#36827;&#34892;&#27979;&#35797;&#65292;&#20165;&#36827;&#34892;2000&#27425;&#25628;&#32034;&#21363;&#21487;&#20135;&#29983;&#39640;&#24615;&#33021;&#32593;&#32476;&#65292;&#21363;&#20351;&#27809;&#26377;&#35813;&#22522;&#20934;&#39046;&#22495;&#30340;&#20808;&#21069;&#30693;&#35782;&#25110;&#20219;&#20309;&#20808;&#21069;&#30340;&#26368;&#20339;&#32467;&#26524;&#30340;&#26333;&#20809;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLMs) &#24050;&#25104;&#20026;&#19968;&#31181;&#24378;&#22823;&#30340;&#24037;&#20855;&#65292;&#21487;&#20197;&#23436;&#25104;&#24191;&#27867;&#30340;&#20219;&#21153;&#12290;&#23427;&#20204;&#30340;&#33021;&#21147;&#28085;&#30422;&#20102;&#35768;&#22810;&#39046;&#22495;&#65292;&#23427;&#20204;&#22312;&#20195;&#30721;&#29983;&#25104;&#39046;&#22495;&#20135;&#29983;&#20102;&#37325;&#22823;&#24433;&#21709;&#12290;&#22312;&#27492;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#23558; LLMs &#35270;&#20026;&#21464;&#24322;&#21644;&#20132;&#21449;&#24037;&#20855;&#12290;&#21516;&#26102;&#65292;&#22810;&#26679;&#24615;&#20248;&#21270;&#31639;&#27861;&#24050;&#30693;&#21487;&#20197;&#21457;&#29616;&#22810;&#26679;&#24615;&#21644;&#31283;&#20581;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#36890;&#36807;&#23558; LLMs &#30340;&#20195;&#30721;&#29983;&#25104;&#33021;&#21147;&#19982; QD &#35299;&#20915;&#26041;&#26696;&#30340;&#22810;&#26679;&#24615;&#21644;&#40065;&#26834;&#24615;&#30456;&#32467;&#21512;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102; LLMatic&#65292;&#19968;&#20010;&#31070;&#32463;&#32467;&#26500;&#25628;&#32034; (NAS) &#31639;&#27861;&#12290;&#34429;&#28982; LLMs &#36890;&#36807;&#25552;&#31034;&#30452;&#25509;&#36827;&#34892; NAS &#32771;&#39564;&#22256;&#38590;&#65292;&#20294; LLMatic &#21033;&#29992;&#31243;&#24207;&#21270;&#26041;&#27861;&#65292;&#21033;&#29992; QD &#26469;&#36827;&#34892;&#25552;&#31034;&#21644;&#32593;&#32476;&#32467;&#26500;&#65292;&#20174;&#32780;&#21019;&#24314;&#22810;&#26679;&#24615;&#21644;&#39640;&#24615;&#33021;&#32593;&#32476;&#12290;&#25105;&#20204;&#22312; CIFAR-10 &#22270;&#20687;&#20998;&#31867;&#22522;&#20934;&#27979;&#35797;&#20013;&#27979;&#35797;&#20102; LLMatic&#65292;&#35777;&#26126;&#23427;&#21487;&#20197;&#22312;&#20165;&#36827;&#34892; 2000 &#27425;&#25628;&#32034;&#30340;&#24773;&#20917;&#19979;&#20135;&#29983;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#32593;&#32476;&#65292;&#21363;&#20351;&#27809;&#26377;&#35813;&#22522;&#20934;&#39046;&#22495;&#30340;&#20808;&#21069;&#30693;&#35782;&#25110;&#20219;&#20309;&#20808;&#21069;&#30340;&#26368;&#20339;&#32467;&#26524;&#30340;&#26333;&#20809;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have emerged as powerful tools capable of accomplishing a broad spectrum of tasks. Their abilities span numerous areas, and one area where they have made a significant impact is in the domain of code generation. In this context, we view LLMs as mutation and crossover tools. Meanwhile, Quality-Diversity (QD) algorithms are known to discover diverse and robust solutions. By merging the code-generating abilities of LLMs with the diversity and robustness of QD solutions, we introduce LLMatic, a Neural Architecture Search (NAS) algorithm. While LLMs struggle to conduct NAS directly through prompts, LLMatic uses a procedural approach, leveraging QD for prompts and network architecture to create diverse and highly performant networks. We test LLMatic on the CIFAR-10 image classification benchmark, demonstrating that it can produce competitive networks with just $2,000$ searches, even without prior knowledge of the benchmark domain or exposure to any previous top-p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32422;&#26463;&#32534;&#31243;&#30340;&#26041;&#27861;&#65292;&#20197;&#23547;&#25214;&#24555;&#36895;&#30697;&#38453;&#20056;&#27861;&#30340;&#38750;&#20132;&#25442;&#31639;&#27861;&#25110;&#25552;&#20379;&#19981;&#21487;&#34892;&#24615;&#35777;&#26126;&#12290;&#25105;&#20204;&#36890;&#36807;&#25171;&#30772;&#23545;&#31216;&#24615;&#30340;&#32422;&#26463;&#26465;&#20214;&#21644;&#26377;&#25928;&#30340;&#19981;&#31561;&#24335;&#32422;&#26463;&#26469;&#20462;&#21098;&#25628;&#32034;&#31354;&#38388;&#65292;&#21487;&#20197;&#22312;&#21512;&#29702;&#30340;&#26102;&#38388;&#20869;&#25214;&#21040;&#24050;&#30693;&#30340;&#26368;&#20339;&#31639;&#27861;&#25110;&#25913;&#36827;&#30340;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.01097</link><description>&lt;p&gt;
&#19981;&#23481;&#26131;&#20986;&#38169;&#30340;&#24555;&#36895;&#30697;&#38453;&#20056;&#27861;:&#22522;&#20110;&#32422;&#26463;&#32534;&#31243;&#30340;&#35299;&#27861;
&lt;/p&gt;
&lt;p&gt;
Fast Matrix Multiplication Without Tears: A Constraint Programming Approach. (arXiv:2306.01097v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01097
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32422;&#26463;&#32534;&#31243;&#30340;&#26041;&#27861;&#65292;&#20197;&#23547;&#25214;&#24555;&#36895;&#30697;&#38453;&#20056;&#27861;&#30340;&#38750;&#20132;&#25442;&#31639;&#27861;&#25110;&#25552;&#20379;&#19981;&#21487;&#34892;&#24615;&#35777;&#26126;&#12290;&#25105;&#20204;&#36890;&#36807;&#25171;&#30772;&#23545;&#31216;&#24615;&#30340;&#32422;&#26463;&#26465;&#20214;&#21644;&#26377;&#25928;&#30340;&#19981;&#31561;&#24335;&#32422;&#26463;&#26469;&#20462;&#21098;&#25628;&#32034;&#31354;&#38388;&#65292;&#21487;&#20197;&#22312;&#21512;&#29702;&#30340;&#26102;&#38388;&#20869;&#25214;&#21040;&#24050;&#30693;&#30340;&#26368;&#20339;&#31639;&#27861;&#25110;&#25913;&#36827;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24050;&#30693;&#23558;&#19968;&#20010; $N \times M$ &#30697;&#38453;&#19982;&#19968;&#20010; $M \times P$ &#30340;&#30697;&#38453;&#30456;&#20056;&#26102;&#65292;&#21487;&#20197;&#20351;&#29992;&#23569;&#20110;&#26420;&#32032; $NMP$ &#26041;&#27861;&#24314;&#35758;&#30340;&#20056;&#27861;&#27425;&#25968;&#12290;&#20854;&#20013;&#26368;&#33879;&#21517;&#30340;&#26159; Strassen &#31639;&#27861;&#65292;&#21487;&#23558;&#20004;&#20010; $2\times 2$ &#30340;&#30697;&#38453;&#30456;&#20056;&#65292;&#21482;&#38656; 7 &#27425;&#32780;&#38750; 8 &#27425;&#20056;&#27861;&#12290;&#36825;&#24341;&#20986;&#20102;&#24555;&#36895;&#30697;&#38453;&#20056;&#27861;&#30340;&#32422;&#26463;&#28385;&#36275;&#38382;&#39064;&#65292;&#20854;&#20013;&#24517;&#39035;&#36873;&#25321;&#19968;&#32452; $R &lt;NMP$ &#20056;&#27861;&#39033;&#65292;&#24182;&#23558;&#23427;&#20204;&#32452;&#21512;&#20197;&#28385;&#36275;&#36755;&#20986;&#30697;&#38453;&#30340;&#27491;&#30830;&#24615;&#32422;&#26463;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26032;&#39062;&#30340;&#22522;&#20110;&#32422;&#26463;&#32534;&#31243;&#30340;&#26041;&#27861;&#65292;&#20197;&#23547;&#25214;&#24555;&#36895;&#30697;&#38453;&#20056;&#27861;&#30340;&#38750;&#20132;&#25442;&#31639;&#27861;&#25110;&#25552;&#20379;&#19981;&#21487;&#34892;&#24615;&#35777;&#26126;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#32452;&#25171;&#30772;&#23545;&#31216;&#24615;&#30340;&#32422;&#26463;&#26465;&#20214;&#21644;&#26377;&#25928;&#30340;&#19981;&#31561;&#24335;&#32422;&#26463;&#26469;&#20462;&#21098;&#25628;&#32034;&#31354;&#38388;&#65292;&#24182;&#20943;&#23569;&#35201;&#26816;&#26597;&#30340;&#35299;&#20915;&#26041;&#26696;&#25968;&#37327;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#21512;&#29702;&#30340;&#26102;&#38388;&#20869;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#25214;&#21040;&#24050;&#30693;&#30340;&#26368;&#20339;&#31639;&#27861;&#25110;&#25913;&#36827;&#30340;&#31639;&#27861;&#65292;&#36866;&#29992;&#20110;&#27979;&#35797;&#30340;&#25152;&#26377;&#30697;&#38453;&#22823;&#23567;&#65292;&#26368;&#22823;&#20026; $768 \times 768$&#12290;
&lt;/p&gt;
&lt;p&gt;
It is known that the multiplication of an $N \times M$ matrix with an $M \times P$ matrix can be performed using fewer multiplications than what the naive $NMP$ approach suggests. The most famous instance of this is Strassen's algorithm for multiplying two $2\times 2$ matrices in 7 instead of 8 multiplications. This gives rise to the constraint satisfaction problem of fast matrix multiplication, where a set of $R &lt; NMP$ multiplication terms must be chosen and combined such that they satisfy correctness constraints on the output matrix. Despite its highly combinatorial nature, this problem has not been exhaustively examined from that perspective, as evidenced for example by the recent deep reinforcement learning approach of AlphaTensor. In this work, we propose a simple yet novel Constraint Programming approach to find non-commutative algorithms for fast matrix multiplication or provide proof of infeasibility otherwise. We propose a set of symmetry-breaking constraints and valid inequal
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#25968;&#25454;&#23494;&#38598;&#22411;&#38382;&#39064;&#21644;&#22810;&#30446;&#26631;&#20248;&#21270;&#35774;&#32622;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#26694;&#26550;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#20195;&#29702;&#24314;&#27169;&#21644;&#21487;&#25193;&#23637;&#12289;&#20855;&#26377;&#19981;&#30830;&#23450;&#24615;&#30340;&#25910;&#36141;&#31574;&#30053;&#65292;&#33021;&#22815;&#22312;&#26368;&#23569;&#36845;&#20195;&#27425;&#25968;&#30340;&#24773;&#20917;&#19979;&#39640;&#25928;&#22320;&#36827;&#34892;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2306.01095</link><description>&lt;p&gt;
&#22823;&#25209;&#37327;&#31070;&#32463;&#22810;&#30446;&#26631;&#36125;&#21494;&#26031;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Large-Batch, Neural Multi-Objective Bayesian Optimization. (arXiv:2306.01095v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01095
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#25968;&#25454;&#23494;&#38598;&#22411;&#38382;&#39064;&#21644;&#22810;&#30446;&#26631;&#20248;&#21270;&#35774;&#32622;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#26694;&#26550;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#20195;&#29702;&#24314;&#27169;&#21644;&#21487;&#25193;&#23637;&#12289;&#20855;&#26377;&#19981;&#30830;&#23450;&#24615;&#30340;&#25910;&#36141;&#31574;&#30053;&#65292;&#33021;&#22815;&#22312;&#26368;&#23569;&#36845;&#20195;&#27425;&#25968;&#30340;&#24773;&#20917;&#19979;&#39640;&#25928;&#22320;&#36827;&#34892;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#20248;&#21270;&#22312;&#20840;&#23616;&#20248;&#21270;&#40657;&#30418;&#39640;&#25104;&#26412;&#20989;&#25968;&#26041;&#38754;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#26694;&#26550;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#40664;&#35748;&#39640;&#26031;&#36807;&#31243;&#20195;&#29702;&#30340;&#21487;&#25193;&#23637;&#24615;&#24046;&#65292;&#23427;&#22312;&#22788;&#29702;&#25968;&#25454;&#23494;&#38598;&#22411;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#22312;&#22810;&#30446;&#26631;&#35774;&#32622;&#20013;&#30340;&#33021;&#21147;&#26377;&#38480;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#26694;&#26550;&#65292;&#19987;&#20026;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#32780;&#35774;&#35745;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#36827;&#34892;&#20195;&#29702;&#24314;&#27169;&#12290;&#36825;&#20351;&#24471;&#23427;&#33021;&#22815;&#26377;&#25928;&#22320;&#22788;&#29702;&#22823;&#25209;&#37327;&#25968;&#25454;&#65292;&#24314;&#27169;&#22797;&#26434;&#38382;&#39064;&#20197;&#21450;&#20135;&#29983;&#39044;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#32467;&#21512;&#20102;&#19968;&#31181;&#22522;&#20110;&#20247;&#25152;&#21608;&#30693;&#19988;&#26131;&#20110;&#37096;&#32626;&#30340;NSGA-II&#30340;&#21487;&#25193;&#23637;&#30340;&#12289;&#20855;&#26377;&#19981;&#30830;&#23450;&#24615;&#30340;&#25910;&#36141;&#31574;&#30053;&#12290;&#36825;&#31181;&#23436;&#20840;&#21487;&#24182;&#34892;&#21270;&#30340;&#31574;&#30053;&#20419;&#36827;&#20102;&#26410;&#21208;&#25506;&#21306;&#22495;&#30340;&#26377;&#25928;&#25506;&#32034;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#20801;&#35768;&#22312;&#26368;&#23569;&#36845;&#20195;&#27425;&#25968;&#30340;&#24773;&#20917;&#19979;&#22312;&#25968;&#25454;&#23494;&#38598;&#29615;&#22659;&#20013;&#36827;&#34892;&#26377;&#25928;&#30340;&#20248;&#21270;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian optimization provides a powerful framework for global optimization of black-box, expensive-to-evaluate functions. However, it has a limited capacity in handling data-intensive problems, especially in multi-objective settings, due to the poor scalability of default Gaussian Process surrogates. We present a novel Bayesian optimization framework specifically tailored to address these limitations. Our method leverages a Bayesian neural networks approach for surrogate modeling. This enables efficient handling of large batches of data, modeling complex problems, and generating the uncertainty of the predictions. In addition, our method incorporates a scalable, uncertainty-aware acquisition strategy based on the well-known, easy-to-deploy NSGA-II. This fully parallelizable strategy promotes efficient exploration of uncharted regions. Our framework allows for effective optimization in data-intensive environments with a minimum number of iterations. We demonstrate the superiority of ou
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102; UCAS-IIE-NLP &#35774;&#35745;&#30340; SACL-XLMR &#31995;&#32479;&#65292;&#23427;&#21033;&#29992;&#35789;&#20856;&#24335;&#22810;&#35821;&#35328;BERT&#36827;&#34892;&#24773;&#24863;&#24863;&#30693;&#34920;&#31034;&#23398;&#20064;&#65292;&#20351;&#29992;&#30417;&#30563;&#24615;&#23545;&#25239;&#24335;&#23545;&#27604;&#23398;&#20064;&#25216;&#26415;&#36827;&#34892;&#24773;&#24863;&#20256;&#25773;&#32467;&#26500;&#21270;&#34920;&#31034;&#23398;&#20064;&#65292;&#29992;&#20110;&#20302;&#36164;&#28304;&#24773;&#24863;&#20998;&#26512;&#65292;&#19988;&#22312;&#22810;&#35821;&#35328;&#21644;&#38646;&#26679;&#26412;&#24773;&#24863;&#20998;&#31867;&#23376;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#19988;&#22312;&#38646;&#26679;&#26412;&#20998;&#31867;&#23376;&#20219;&#21153;&#20013;&#33719;&#24471;&#20102;&#31532;&#19968;&#21517;&#12290;</title><link>http://arxiv.org/abs/2306.01093</link><description>&lt;p&gt;
UCAS-IIE-NLP&#22312;SemEval-2023&#20219;&#21153;12&#20013;&#30340;&#24212;&#29992;&#65306;&#22686;&#24378;&#20302;&#36164;&#28304;&#24773;&#24863;&#20998;&#26512;&#30340;&#22810;&#35821;&#35328;BERT&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
UCAS-IIE-NLP at SemEval-2023 Task 12: Enhancing Generalization of Multilingual BERT for Low-resource Sentiment Analysis. (arXiv:2306.01093v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01093
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102; UCAS-IIE-NLP &#35774;&#35745;&#30340; SACL-XLMR &#31995;&#32479;&#65292;&#23427;&#21033;&#29992;&#35789;&#20856;&#24335;&#22810;&#35821;&#35328;BERT&#36827;&#34892;&#24773;&#24863;&#24863;&#30693;&#34920;&#31034;&#23398;&#20064;&#65292;&#20351;&#29992;&#30417;&#30563;&#24615;&#23545;&#25239;&#24335;&#23545;&#27604;&#23398;&#20064;&#25216;&#26415;&#36827;&#34892;&#24773;&#24863;&#20256;&#25773;&#32467;&#26500;&#21270;&#34920;&#31034;&#23398;&#20064;&#65292;&#29992;&#20110;&#20302;&#36164;&#28304;&#24773;&#24863;&#20998;&#26512;&#65292;&#19988;&#22312;&#22810;&#35821;&#35328;&#21644;&#38646;&#26679;&#26412;&#24773;&#24863;&#20998;&#31867;&#23376;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#19988;&#22312;&#38646;&#26679;&#26412;&#20998;&#31867;&#23376;&#20219;&#21153;&#20013;&#33719;&#24471;&#20102;&#31532;&#19968;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;&#25105;&#20204;&#20026;SemEval-2023&#20219;&#21153;12&#35774;&#35745;&#30340;&#31995;&#32479;&#65306;&#38750;&#27954;&#35821;&#35328;&#30340;&#24773;&#24863;&#20998;&#26512;&#12290;&#35813;&#20219;&#21153;&#38754;&#20020;&#30340;&#25361;&#25112;&#26159;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#19979;&#26631;&#27880;&#25968;&#25454;&#21644;&#35821;&#35328;&#36164;&#28304;&#30340;&#31232;&#32570;&#24615;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;SACL-XLMR&#30340;&#36890;&#29992;&#22810;&#35821;&#35328;&#31995;&#32479;&#65292;&#29992;&#20110;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#24773;&#24863;&#20998;&#26512;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20110;&#35789;&#20856;&#30340;&#22810;&#35821;&#35328;BERT&#65292;&#20197;&#20415;&#36827;&#34892;&#35821;&#35328;&#36866;&#24212;&#21644;&#24773;&#24863;&#24863;&#30693;&#34920;&#31034;&#23398;&#20064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#37319;&#29992;&#19968;&#20010;&#30417;&#30563;&#24615;&#30340;&#23545;&#25239;&#24335;&#23545;&#27604;&#23398;&#20064;&#25216;&#26415;&#26469;&#23398;&#20064;&#24773;&#24863;&#20256;&#25773;&#32467;&#26500;&#21270;&#34920;&#31034;&#24182;&#22686;&#24378;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#22312;&#22810;&#35821;&#35328;&#21644;&#38646;&#26679;&#26412;&#24773;&#24863;&#20998;&#31867;&#23376;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#36828;&#36828;&#36229;&#36807;&#22522;&#32447;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#22312;&#23448;&#26041;&#25490;&#21517;&#20013;&#65292;&#35813;&#31995;&#32479;&#22312;&#38646;&#26679;&#26412;&#20998;&#31867;&#23376;&#20219;&#21153;&#20013;&#33719;&#24471;&#20102;&#31532;&#19968;&#21517;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#20102;&#25105;&#20204;&#31995;&#32479;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper describes our system designed for SemEval-2023 Task 12: Sentiment analysis for African languages. The challenge faced by this task is the scarcity of labeled data and linguistic resources in low-resource settings. To alleviate these, we propose a generalized multilingual system SACL-XLMR for sentiment analysis on low-resource languages. Specifically, we design a lexicon-based multilingual BERT to facilitate language adaptation and sentiment-aware representation learning. Besides, we apply a supervised adversarial contrastive learning technique to learn sentiment-spread structured representations and enhance model generalization. Our system achieved competitive results, largely outperforming baselines on both multilingual and zero-shot sentiment classification subtasks. Notably, the system obtained the 1st rank on the zero-shot classification subtask in the official ranking. Extensive experiments demonstrate the effectiveness of our system.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#22270;&#21367;&#31215;&#32593;&#32476;&#30340;4D&#35270;&#39057;&#28857;&#20113;&#19978;&#37319;&#26679;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#36136;&#37327;&#21644;&#23450;&#37327;&#19978;&#30342;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2306.01081</link><description>&lt;p&gt;
4DSR-GCN: &#20351;&#29992;&#22270;&#21367;&#31215;&#32593;&#32476;&#30340;4D&#35270;&#39057;&#28857;&#20113;&#19978;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
4DSR-GCN: 4D Video Point Cloud Upsampling using Graph Convolutional Networks. (arXiv:2306.01081v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01081
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#22270;&#21367;&#31215;&#32593;&#32476;&#30340;4D&#35270;&#39057;&#28857;&#20113;&#19978;&#37319;&#26679;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#36136;&#37327;&#21644;&#23450;&#37327;&#19978;&#30342;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19977;&#32500;&#28857;&#20113;&#30340;&#26102;&#38388;&#24207;&#21015;&#65292;&#25110;&#31216;&#20026;4D&#28857;&#20113;&#65292;&#29616;&#22312;&#22312;&#22810;&#20010;&#24212;&#29992;&#20013;&#20197;&#26085;&#30410;&#22686;&#38271;&#30340;&#36895;&#24230;&#34987;&#33719;&#21462;&#65288;&#20363;&#22914;&#65292;&#33258;&#20027;&#39550;&#39542;&#25110;&#36741;&#21161;&#39550;&#39542;&#20013;&#30340;&#28608;&#20809;&#38647;&#36798;&#65289;&#12290;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#38656;&#35201;&#20256;&#36755;&#22823;&#37327;&#25968;&#25454;&#65292;&#22240;&#27492;&#38656;&#35201;&#24212;&#29992;&#36866;&#24403;&#30340;&#21387;&#32553;&#24037;&#20855;&#26469;&#38477;&#20302;&#20998;&#36776;&#29575;&#25110;&#24102;&#23485;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#23545;&#34987;&#22823;&#37327;&#21387;&#32553;&#21518;&#30340;&#26102;&#38388;&#21464;&#21270;&#30340;3D&#35270;&#39057;&#28857;&#20113;&#36827;&#34892;&#21319;&#37319;&#26679;&#21644;&#24674;&#22797;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#30001;&#19968;&#20010;&#19987;&#38376;&#35774;&#35745;&#30340;&#22270;&#21367;&#31215;&#32593;&#32476;&#32452;&#25104;&#65292;&#35813;&#32593;&#32476;&#32467;&#21512;&#20102;&#21160;&#24577;&#36793;&#32536;&#21367;&#31215;&#21644;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#36827;&#34892;&#29305;&#24449;&#32858;&#21512;&#65292;&#22312;&#29983;&#25104;&#23545;&#25239;&#35774;&#32622;&#20013;&#36827;&#34892;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#31181;&#19981;&#20381;&#36182;&#20110;&#20256;&#32479;&#20307;&#32032;&#21270;&#25805;&#20316;&#30340;&#23494;&#38598;&#28857;&#20113;&#25277;&#26679;&#26041;&#27861;&#65292;&#26469;&#33258;PointNet ++ &#30340;&#21551;&#21457;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;4D&#35270;&#39057;&#28857;&#20113;&#19978;&#37319;&#26679;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#25216;&#26415;&#65292;&#26080;&#35770;&#26159;&#20174;&#23450;&#37327;&#36824;&#26159;&#23450;&#24615;&#19978;&#37117;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Time varying sequences of 3D point clouds, or 4D point clouds, are now being acquired at an increasing pace in several applications (e.g., LiDAR in autonomous or assisted driving). In many cases, such volume of data is transmitted, thus requiring that proper compression tools are applied to either reduce the resolution or the bandwidth. In this paper, we propose a new solution for upscaling and restoration of time-varying 3D video point clouds after they have been heavily compressed. In consideration of recent growing relevance of 3D applications, %We focused on a model allowing user-side upscaling and artifact removal for 3D video point clouds, a real-time stream of which would require . Our model consists of a specifically designed Graph Convolutional Network (GCN) that combines Dynamic Edge Convolution and Graph Attention Networks for feature aggregation in a Generative Adversarial setting. By taking inspiration PointNet++, We present a different way to sample dense point clouds wit
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#37327;&#21270;&#24863;&#30693;&#24352;&#37327;&#21387;&#32553;&#35757;&#32451;&#26041;&#27861;&#65292;&#29992;&#20110;&#20943;&#23569;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#30340;&#27169;&#22411;&#22823;&#23567;&#65292;&#31639;&#26415;&#36816;&#31639;&#21644;&#26368;&#32456;&#36816;&#34892;&#26102;&#24310;&#12290;&#35813;&#26041;&#27861;&#32463;&#36807;&#36880;&#23618;&#33976;&#39311;&#21518;&#22312;&#25991;&#26412;&#34164;&#21547;&#21644;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33391;&#22909;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.01076</link><description>&lt;p&gt;
&#37327;&#21270;&#24863;&#30693;&#21644;&#24352;&#37327;&#21387;&#32553;&#35757;&#32451;&#65306;&#29992;&#20110;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#30340;Transformer
&lt;/p&gt;
&lt;p&gt;
Quantization-Aware and Tensor-Compressed Training of Transformers for Natural Language Understanding. (arXiv:2306.01076v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01076
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#37327;&#21270;&#24863;&#30693;&#24352;&#37327;&#21387;&#32553;&#35757;&#32451;&#26041;&#27861;&#65292;&#29992;&#20110;&#20943;&#23569;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#30340;&#27169;&#22411;&#22823;&#23567;&#65292;&#31639;&#26415;&#36816;&#31639;&#21644;&#26368;&#32456;&#36816;&#34892;&#26102;&#24310;&#12290;&#35813;&#26041;&#27861;&#32463;&#36807;&#36880;&#23618;&#33976;&#39311;&#21518;&#22312;&#25991;&#26412;&#34164;&#21547;&#21644;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33391;&#22909;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32454;&#35843;&#30340;Transformer&#27169;&#22411;&#22312;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#24222;&#22823;&#30340;&#27169;&#22411;&#22823;&#23567;&#38459;&#27490;&#20102;&#22312;&#36164;&#28304;&#21463;&#38480;&#35774;&#22791;&#19978;&#37096;&#32626;&#39640;&#24615;&#33021;Transformer&#27169;&#22411;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37327;&#21270;&#24863;&#30693;&#24352;&#37327;&#21387;&#32553;&#35757;&#32451;&#26041;&#27861;&#65292;&#20197;&#20943;&#23569;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#30340;&#27169;&#22411;&#22823;&#23567;&#65292;&#31639;&#26415;&#36816;&#31639;&#21644;&#26368;&#32456;&#36816;&#34892;&#26102;&#24310;&#12290;&#25105;&#20204;&#23558;Transformer&#30340;&#23884;&#20837;&#21644;&#32447;&#24615;&#23618;&#21387;&#32553;&#20026;&#23567;&#22411;&#20302;&#31209;&#24352;&#37327;&#26680;&#65292;&#20174;&#32780;&#26174;&#33879;&#20943;&#23569;&#27169;&#22411;&#21442;&#25968;&#12290;&#37319;&#29992;&#21487;&#23398;&#20064;&#27604;&#20363;&#22240;&#23376;&#30340;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;&#26469;&#36827;&#19968;&#27493;&#33719;&#24471;&#24352;&#37327;&#21387;&#32553;&#27169;&#22411;&#30340;&#20302;&#31934;&#24230;&#34920;&#31034;&#12290;&#35813;&#26041;&#27861;&#21487;&#29992;&#20110;&#31471;&#21040;&#31471;&#35757;&#32451;&#21644;&#33976;&#39311;&#35757;&#32451;&#12290;&#20026;&#20102;&#25552;&#39640;&#25910;&#25947;&#24615;&#65292;&#37319;&#29992;&#36880;&#23618;&#33976;&#39311;&#26041;&#27861;&#20174;&#39044;&#35757;&#32451;Transformer&#20013;&#25552;&#21462;&#20986;&#19968;&#20010;&#32463;&#36807;&#37327;&#21270;&#21644;&#24352;&#37327;&#21387;&#32553;&#30340;&#23398;&#29983;&#27169;&#22411;&#12290;&#26412;&#30740;&#31350;&#22312;&#20004;&#20010;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#65292;&#21363;&#25991;&#26412;&#34164;&#21547;&#21644;&#24773;&#24863;&#20998;&#26512;&#20013;&#23637;&#31034;&#20102;&#20854;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fine-tuned transformer models have shown superior performances in many natural language tasks. However, the large model size prohibits deploying high-performance transformer models on resource-constrained devices. This paper proposes a quantization-aware tensor-compressed training approach to reduce the model size, arithmetic operations, and ultimately runtime latency of transformer-based models. We compress the embedding and linear layers of transformers into small low-rank tensor cores, which significantly reduces model parameters. A quantization-aware training with learnable scale factors is used to further obtain low-precision representations of the tensor-compressed models. The developed approach can be used for both end-to-end training and distillation-based training. To improve the convergence, a layer-by-layer distillation is applied to distill a quantized and tensor-compressed student model from a pre-trained transformer. The performance is demonstrated in two natural language
&lt;/p&gt;</description></item><item><title>TimelineQA&#26159;&#19968;&#20010;&#29992;&#20110;&#26597;&#35810;&#29983;&#27963;&#26085;&#24535;&#30340;&#21152;&#36895;&#36827;&#23637;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#28041;&#21450;&#26102;&#38388;&#21644;&#22320;&#29702;&#20449;&#24687;&#65292;&#24050;&#32463;&#20844;&#24320;&#21457;&#24067;&#65292;&#24182;&#20351;&#29992;&#20004;&#31181;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#20294;&#36825;&#20004;&#31181;&#27169;&#22411;&#22343;&#26410;&#36798;&#21040;&#20154;&#31867;&#34920;&#29616;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2306.01069</link><description>&lt;p&gt;
TimelineQA: &#19968;&#20010;&#29992;&#20110;&#26102;&#38388;&#32447;&#38382;&#31572;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
TimelineQA: A Benchmark for Question Answering over Timelines. (arXiv:2306.01069v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01069
&lt;/p&gt;
&lt;p&gt;
TimelineQA&#26159;&#19968;&#20010;&#29992;&#20110;&#26597;&#35810;&#29983;&#27963;&#26085;&#24535;&#30340;&#21152;&#36895;&#36827;&#23637;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#28041;&#21450;&#26102;&#38388;&#21644;&#22320;&#29702;&#20449;&#24687;&#65292;&#24050;&#32463;&#20844;&#24320;&#21457;&#24067;&#65292;&#24182;&#20351;&#29992;&#20004;&#31181;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#20294;&#36825;&#20004;&#31181;&#27169;&#22411;&#22343;&#26410;&#36798;&#21040;&#20154;&#31867;&#34920;&#29616;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Lifelogs&#65288;&#29983;&#27963;&#26085;&#24535;&#65289;&#26159;&#20154;&#20204;&#29983;&#27963;&#32463;&#21382;&#30340;&#25551;&#36848;&#65292;&#36825;&#20123;&#26085;&#24535;&#36890;&#36807;&#32467;&#21512;&#26469;&#33258;&#22810;&#20010;&#25968;&#23383;&#26381;&#21153;&#65288;&#22914;&#22312;&#32447;&#29031;&#29255;&#12289;&#22320;&#22270;&#12289;&#36141;&#29289;&#21644;&#20869;&#23481;&#27969;&#23186;&#20307;&#26381;&#21153;&#65289;&#30340;&#25968;&#25454;&#26469;&#21019;&#24314;&#12290;&#38382;&#31572;&#25216;&#26415;&#22312;&#29983;&#27963;&#26085;&#24535;&#19978;&#30340;&#24212;&#29992;&#21487;&#20197;&#20026;&#20010;&#20154;&#21161;&#25163;&#22312;&#25552;&#20379;&#19978;&#19979;&#25991;&#26041;&#38754;&#25552;&#20379;&#20851;&#38190;&#36164;&#28304;&#12290;&#20294;&#26159;&#65292;&#30001;&#20110;&#29983;&#27963;&#26085;&#24535;&#32467;&#21512;&#20102;&#33258;&#30001;&#25991;&#26412;&#19982;&#19968;&#23450;&#31243;&#24230;&#30340;&#32467;&#26500;&#65292;&#22914;&#26102;&#38388;&#21644;&#22320;&#29702;&#20449;&#24687;&#65292;&#22240;&#27492;&#24403;&#21069;&#30340;&#38382;&#31572;&#25216;&#26415;&#26080;&#27861;&#22238;&#31572;&#27492;&#31867;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#21019;&#24314;&#24182;&#20844;&#24320;&#21457;&#24067;&#20102;TimelineQA&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#26597;&#35810;&#29983;&#27963;&#26085;&#24535;&#30340;&#21152;&#36895;&#36827;&#23637;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;TimelineQA&#29983;&#25104;&#34394;&#26500;&#20154;&#29289;&#30340;&#29983;&#27963;&#26085;&#24535;&#12290;&#29983;&#27963;&#26085;&#24535;&#20013;&#30340;&#20107;&#20214;&#20174;&#39640;&#20013;&#27605;&#19994;&#31561;&#37325;&#22823;&#29983;&#27963;&#20107;&#20214;&#21040;&#26085;&#24120;&#27963;&#21160;&#22914;&#24930;&#36305;&#37117;&#26377;&#25152;&#35206;&#30422;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#20851;&#20110;TimelineQA&#30340;&#23454;&#39564;&#65292;&#20351;&#29992;&#20102;&#20004;&#31181;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#65292;&#21457;&#29616;&#36825;&#20004;&#31181;&#27169;&#22411;&#22312;&#36825;&#19968;&#20219;&#21153;&#19978;&#22343;&#26410;&#36798;&#21040;&#20154;&#31867;&#34920;&#29616;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Lifelogs are descriptions of experiences that a person had during their life. Lifelogs are created by fusing data from the multitude of digital services, such as online photos, maps, shopping and content streaming services. Question answering over lifelogs can offer personal assistants a critical resource when they try to provide advice in context. However, obtaining answers to questions over lifelogs is beyond the current state of the art of question answering techniques for a variety of reasons, the most pronounced of which is that lifelogs combine free text with some degree of structure such as temporal and geographical information.  We create and publicly release TimelineQA1, a benchmark for accelerating progress on querying lifelogs. TimelineQA generates lifelogs of imaginary people. The episodes in the lifelog range from major life episodes such as high school graduation to those that occur on a daily basis such as going for a run. We describe a set of experiments on TimelineQA w
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;FPGA&#26550;&#26500;&#30340;&#22312;&#32447;&#23398;&#20064;&#31995;&#32479;&#65292;&#36890;&#36807;&#37319;&#29992;&#20302;&#22797;&#26434;&#24230;&#30340;Tsetlin Machine&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#22312;&#36816;&#34892;&#26102;&#25552;&#20379;&#20102;&#31163;&#32447;&#21644;&#22312;&#32447;&#23398;&#20064;&#31649;&#29702;&#12290;&#36825;&#31181;&#26550;&#26500;&#33021;&#22815;&#25353;&#38656;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#19988;&#33021;&#22815;&#22312;&#25805;&#20316;&#26399;&#38388;&#20132;&#38169;&#22320;&#36827;&#34892;&#25512;&#29702;&#21644;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2306.01027</link><description>&lt;p&gt;
&#20351;&#29992;Tsetlin&#26426;&#22120;&#30340;&#22312;&#32447;&#23398;&#20064;&#30340;FPGA&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
An FPGA Architecture for Online Learning using the Tsetlin Machine. (arXiv:2306.01027v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01027
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;FPGA&#26550;&#26500;&#30340;&#22312;&#32447;&#23398;&#20064;&#31995;&#32479;&#65292;&#36890;&#36807;&#37319;&#29992;&#20302;&#22797;&#26434;&#24230;&#30340;Tsetlin Machine&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#22312;&#36816;&#34892;&#26102;&#25552;&#20379;&#20102;&#31163;&#32447;&#21644;&#22312;&#32447;&#23398;&#20064;&#31649;&#29702;&#12290;&#36825;&#31181;&#26550;&#26500;&#33021;&#22815;&#25353;&#38656;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#19988;&#33021;&#22815;&#22312;&#25805;&#20316;&#26399;&#38388;&#20132;&#38169;&#22320;&#36827;&#34892;&#25512;&#29702;&#21644;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#38656;&#35201;&#22312;&#26080;&#30417;&#30563;&#24773;&#20917;&#19979;&#19981;&#26029;&#28436;&#21270;&#30340;&#38656;&#27714;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;FPGA&#26550;&#26500;&#30340;&#22312;&#32447;&#23398;&#20064;&#31995;&#32479;&#65292;&#37319;&#29992;&#20302;&#22797;&#26434;&#24230;&#30340;Tsetlin Machine&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#12290;&#35813;&#26550;&#26500;&#25552;&#20379;&#20102;&#23450;&#21046;&#21270;&#30340;&#36816;&#34892;&#26102;&#23398;&#20064;&#31649;&#29702;&#65292;&#23454;&#29616;&#20102;&#33455;&#29255;&#20869;&#37096;&#30340;&#31163;&#32447;&#21644;&#22312;&#32447;&#23398;&#20064;&#65292;&#33021;&#22815;&#22312;&#25512;&#29702;&#20043;&#21069;&#36890;&#36807;&#39044;&#20998;&#31867;&#25968;&#25454;&#22312;FPGA&#19978;&#36827;&#34892;&#25353;&#38656;&#35757;&#32451;&#65292;&#24182;&#19988;&#33021;&#22815;&#22312;&#25805;&#20316;&#26399;&#38388;&#20132;&#38169;&#22320;&#36827;&#34892;&#25512;&#29702;&#21644;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
There is a need for machine learning models to evolve in unsupervised circumstances. New classifications may be introduced, unexpected faults may occur, or the initial dataset may be small compared to the data-points presented to the system during normal operation. Implementing such a system using neural networks involves significant mathematical complexity, which is a major issue in power-critical edge applications.  This paper proposes a novel field-programmable gate-array infrastructure for online learning, implementing a low-complexity machine learning algorithm called the Tsetlin Machine. This infrastructure features a custom-designed architecture for run-time learning management, providing on-chip offline and online learning. Using this architecture, training can be carried out on-demand on the \ac{FPGA} with pre-classified data before inference takes place. Additionally, our architecture provisions online learning, where training can be interleaved with inference during operatio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25551;&#36848;&#20102;&#19968;&#31181;&#22522;&#20110;&#34394;&#25311;&#21147;&#31995;&#32479;&#30340;&#32676;&#26234;&#33021;&#31639;&#27861;&#65292;&#29992;&#20197;&#35299;&#20915;&#24179;&#34913;&#22278;&#24418;&#35013;&#31665;&#38382;&#39064;&#65292;&#24182;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#24471;&#21040;&#39564;&#35777;&#65292;&#20855;&#26377;&#24456;&#22909;&#30340;&#35299;&#20915;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.01021</link><description>&lt;p&gt;
&#22522;&#20110;&#34394;&#25311;&#21147;&#30340;&#24179;&#34913;&#22278;&#24418;&#35013;&#31665;&#38382;&#39064;&#30340;&#32676;&#26234;&#33021;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Vitual-Force Based Swarm Algorithm for Balanced Circular Bin Packing Problems. (arXiv:2306.01021v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01021
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;&#19968;&#31181;&#22522;&#20110;&#34394;&#25311;&#21147;&#31995;&#32479;&#30340;&#32676;&#26234;&#33021;&#31639;&#27861;&#65292;&#29992;&#20197;&#35299;&#20915;&#24179;&#34913;&#22278;&#24418;&#35013;&#31665;&#38382;&#39064;&#65292;&#24182;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#24471;&#21040;&#39564;&#35777;&#65292;&#20855;&#26377;&#24456;&#22909;&#30340;&#35299;&#20915;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24179;&#34913;&#22278;&#24418;&#35013;&#31665;&#38382;&#39064;&#28041;&#21450;&#23558;&#32473;&#23450;&#25968;&#37327;&#30340;&#21152;&#26435;&#22278;&#25918;&#32622;&#22312;&#22278;&#24418;&#23481;&#22120;&#20013;&#65292;&#20197;&#26368;&#23567;&#21270;&#21322;&#24452;&#24182;&#28385;&#36275;&#24179;&#34913;&#32422;&#26463;&#26465;&#20214;&#12290;&#26412;&#25991;&#25551;&#36848;&#20102;&#19968;&#31181;&#22522;&#20110;&#34394;&#25311;&#21147;&#31995;&#32479;&#30340;&#32676;&#26234;&#33021;&#31639;&#27861;&#65292;&#20197;&#35299;&#20915;&#24179;&#34913;&#22278;&#24418;&#35013;&#31665;&#38382;&#39064;&#12290;&#22312;&#25552;&#20986;&#30340;&#26041;&#27861;&#20013;&#65292;&#23545;&#27599;&#20010;&#32452;&#20214;&#26045;&#21152;&#19968;&#32452;&#21147;&#65292;&#20197;&#32771;&#34385;&#32422;&#26463;&#26465;&#20214;&#24182;&#20351;&#29992;&#21160;&#21147;&#23398;&#30340;&#22522;&#26412;&#21407;&#29702;&#26368;&#23567;&#21270;&#30446;&#26631;&#20989;&#25968;&#12290;&#25552;&#20986;&#30340;&#31639;&#27861;&#22312;&#21508;&#31181;&#24179;&#34913;&#22278;&#24418;&#35013;&#31665;&#38382;&#39064;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#24471;&#21040;&#39564;&#35777;&#65292;&#24182;&#39564;&#35777;&#20102;&#20855;&#26377;&#39640;&#36798;300&#20010;&#22278;&#30340;&#38382;&#39064;&#12290;&#25253;&#21578;&#30340;&#32467;&#26524;&#20801;&#35768;&#35780;&#20272;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#19982;&#25991;&#29486;&#20013;&#29616;&#26377;&#32467;&#26524;&#20043;&#38388;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Balanced circular bin packing problems consist in positioning a given number of weighted circles in order to minimize the radius of a circular container while satisfying equilibrium constraints. These problems are NP-hard, highly constrained and dimensional. This paper describes a swarm algorithm based on a virtual-force system in order to solve balanced circular bin packing problems. In the proposed approach, a system of forces is applied to each component allowing to take into account the constraints and minimizing the objective function using the fundamental principle of dynamics. The proposed algorithm is experimented and validated on benchmarks of various balanced circular bin packing problems with up to 300 circles. The reported results allow to assess the effectiveness of the proposed approach compared to existing results from the literature.
&lt;/p&gt;</description></item><item><title>PV2TEA&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#30340;&#20449;&#24687;&#25277;&#21462;&#27169;&#22411;&#65292;&#22312;&#22810;&#27169;&#24577;&#27880;&#37322;&#22256;&#38590;&#30340;&#24773;&#20917;&#19979;&#35299;&#20915;&#20102;&#36328;&#27169;&#24577;&#38598;&#25104;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19977;&#31181;&#20559;&#24046;&#38477;&#20302;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2306.01016</link><description>&lt;p&gt;
PV2TEA&#65306;&#23558;&#35270;&#35273;&#27169;&#24577;&#19982;&#22522;&#20110;&#25991;&#26412;&#30340;&#20449;&#24687;&#25277;&#21462;&#30456;&#32467;&#21512;
&lt;/p&gt;
&lt;p&gt;
PV2TEA: Patching Visual Modality to Textual-Established Information Extraction. (arXiv:2306.01016v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01016
&lt;/p&gt;
&lt;p&gt;
PV2TEA&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#30340;&#20449;&#24687;&#25277;&#21462;&#27169;&#22411;&#65292;&#22312;&#22810;&#27169;&#24577;&#27880;&#37322;&#22256;&#38590;&#30340;&#24773;&#20917;&#19979;&#35299;&#20915;&#20102;&#36328;&#27169;&#24577;&#38598;&#25104;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19977;&#31181;&#20559;&#24046;&#38477;&#20302;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20449;&#24687;&#25277;&#21462;&#65288;&#20363;&#22914;&#23646;&#24615;&#20540;&#25552;&#21462;&#65289;&#24050;&#34987;&#24191;&#27867;&#30740;&#31350;&#21644;&#24314;&#27169;&#65292;&#20294;&#20165;&#22522;&#20110;&#25991;&#26412;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#23646;&#24615;&#21487;&#20197;&#20174;&#22522;&#20110;&#22270;&#20687;&#30340;&#25552;&#21462;&#20013;&#21463;&#30410;&#65292;&#22914;&#39068;&#33394;&#12289;&#24418;&#29366;&#12289;&#22270;&#26696;&#31561;&#12290;&#35270;&#35273;&#27169;&#24577;&#38271;&#26399;&#20197;&#26469;&#19968;&#30452;&#26410;&#34987;&#20805;&#20998;&#21033;&#29992;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#22810;&#27169;&#24577;&#27880;&#37322;&#30340;&#38590;&#24230;&#12290;&#26412;&#25991;&#26088;&#22312;&#23558;&#35270;&#35273;&#27169;&#24577;&#19982;&#22522;&#20110;&#25991;&#26412;&#30340;&#23646;&#24615;&#20449;&#24687;&#25552;&#21462;&#22120;&#30456;&#32467;&#21512;&#12290;&#36328;&#27169;&#24577;&#38598;&#25104;&#38754;&#20020;&#20960;&#20010;&#29420;&#29305;&#30340;&#25361;&#25112;&#65306;&#65288;C1&#65289;&#22270;&#20687;&#21644;&#25991;&#26412;&#25551;&#36848;&#22312;&#26679;&#26412;&#20869;&#21644;&#26679;&#26412;&#38388;&#26494;&#25955;&#21305;&#37197;&#65307;&#65288;C2&#65289;&#22270;&#20687;&#36890;&#24120;&#21253;&#21547;&#20016;&#23500;&#30340;&#32972;&#26223;&#65292;&#21487;&#33021;&#20250;&#35823;&#23548;&#39044;&#27979;&#65307;&#65288;C3&#65289;&#26469;&#33258;&#22522;&#20110;&#25991;&#26412;&#30340;&#25552;&#21462;&#22120;&#30340;&#24369;&#30417;&#30563;&#26631;&#31614;&#23545;&#20110;&#22810;&#27169;&#24577;&#35757;&#32451;&#23384;&#22312;&#20559;&#24046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;PV2TEA&#65292;&#36825;&#26159;&#19968;&#31181;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#65292;&#37197;&#22791;&#20102;&#19977;&#31181;&#20559;&#24046;&#38477;&#20302;&#26041;&#26696;&#65306;&#65288;S1&#65289;&#22686;&#24378;&#30340;&#26631;&#31614;&#24179;&#28369;&#23545;&#27604;&#65292;&#20197;&#25913;&#36827;&#26494;&#25955;&#21305;&#37197;&#30340;&#22270;&#20687;&#21644;&#25991;&#26412;&#30340;&#20132;&#21449;&#27169;&#24577;&#23545;&#40784;; &#65288;S2&#65289;&#27880;&#24847;&#21147;&#21098;&#26525;&#26041;&#26696;&#29992;&#20110;&#22312;&#20445;&#30041;&#27491;&#30830;&#20449;&#24687;&#30340;&#21516;&#26102;&#28040;&#38500;&#19968;&#20123;&#19981;&#24517;&#35201;&#30340;&#32454;&#33410;&#65307;&#65288;S3&#65289;&#22522;&#20110;&#23545;&#25239;&#35757;&#32451;&#30340;&#21487;&#37325;&#32452;&#21367;&#31215;&#33258;&#36866;&#24212;&#27169;&#22359;&#65292;&#20197;&#24110;&#21161;&#28040;&#38500;&#26469;&#33258;&#25991;&#26412;&#25552;&#21462;&#22120;&#30340;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Information extraction, e.g., attribute value extraction, has been extensively studied and formulated based only on text. However, many attributes can benefit from image-based extraction, like color, shape, pattern, among others. The visual modality has long been underutilized, mainly due to multimodal annotation difficulty. In this paper, we aim to patch the visual modality to the textual-established attribute information extractor. The cross-modality integration faces several unique challenges: (C1) images and textual descriptions are loosely paired intra-sample and inter-samples; (C2) images usually contain rich backgrounds that can mislead the prediction; (C3) weakly supervised labels from textual-established extractors are biased for multimodal training. We present PV2TEA, an encoder-decoder architecture equipped with three bias reduction schemes: (S1) Augmented label-smoothed contrast to improve the cross-modality alignment for loosely-paired image and text; (S2) Attention-prunin
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#21160;&#24577;&#32593;&#32476;&#30340;&#22270;&#32423;&#23884;&#20837;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#29983;&#25104;&#33410;&#28857;&#30340;&#26102;&#38388;&#19978;&#19979;&#25991;&#26469;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#20197;&#29983;&#25104;&#20302;&#32500;&#24230;&#30340;&#22270;&#32423;&#34920;&#31034;&#65292;&#23545;&#20110;&#19979;&#28216;&#30340;&#22270;&#24418;&#30456;&#20284;&#24615;&#25490;&#24207;&#12289;&#22270;&#24418;&#21516;&#26500;&#21644;&#24322;&#24120;&#26816;&#27979;&#31561;&#20219;&#21153;&#20855;&#26377;&#37325;&#35201;&#30340;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2306.01012</link><description>&lt;p&gt;
&#26102;&#38388;&#28436;&#21270;&#22270;&#30340;&#22270;&#32423;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Graph-Level Embedding for Time-Evolving Graphs. (arXiv:2306.01012v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01012
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#21160;&#24577;&#32593;&#32476;&#30340;&#22270;&#32423;&#23884;&#20837;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#29983;&#25104;&#33410;&#28857;&#30340;&#26102;&#38388;&#19978;&#19979;&#25991;&#26469;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#20197;&#29983;&#25104;&#20302;&#32500;&#24230;&#30340;&#22270;&#32423;&#34920;&#31034;&#65292;&#23545;&#20110;&#19979;&#28216;&#30340;&#22270;&#24418;&#30456;&#20284;&#24615;&#25490;&#24207;&#12289;&#22270;&#24418;&#21516;&#26500;&#21644;&#24322;&#24120;&#26816;&#27979;&#31561;&#20219;&#21153;&#20855;&#26377;&#37325;&#35201;&#30340;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#34920;&#31034;&#23398;&#20064;&#24050;&#32463;&#24471;&#21040;&#24191;&#27867;&#30340;&#30740;&#31350;&#65292;&#20174;&#33410;&#28857;&#21040;&#22270;&#30340;&#31890;&#24230;&#21508;&#19981;&#30456;&#21516;&#12290;&#34429;&#28982;&#22312;&#33410;&#28857;&#32423;&#21035;&#34920;&#31034;&#26041;&#38754;&#30340;&#22823;&#37096;&#20998;&#24037;&#20316;&#37117;&#24050;&#32463;&#34987;&#30740;&#31350;&#65292;&#20294;&#26159;&#23545;&#20110;&#21160;&#24577;&#25110;&#26102;&#24577;&#32593;&#32476;&#30340;&#22270;&#32423;&#23884;&#20837;&#21364;&#40092;&#26377;&#30740;&#31350;&#12290;&#28982;&#32780;&#65292;&#22312;&#21160;&#24577;&#32593;&#32476;&#20013;&#23398;&#20064;&#20302;&#32500;&#24230;&#30340;&#22270;&#32423;&#34920;&#31034;&#23545;&#21508;&#31181;&#19979;&#28216;&#22270;&#26816;&#32034;&#20219;&#21153;&#38750;&#24120;&#37325;&#35201;&#65292;&#20363;&#22914;&#26102;&#38388;&#22270;&#30456;&#20284;&#24615;&#25490;&#24207;&#12289;&#26102;&#38388;&#22270;&#21516;&#26500;&#21644;&#24322;&#24120;&#26816;&#27979;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#21363;&#24314;&#31435;&#19968;&#20010;&#22810;&#23618;&#22270;&#24182;&#20351;&#29992;&#20855;&#26377;&#26102;&#38388;&#22238;&#28335;&#30340;&#20462;&#25913;&#21518;&#38543;&#26426;&#28216;&#36208;&#29983;&#25104;&#33410;&#28857;&#30340;&#26102;&#38388;&#19978;&#19979;&#25991;&#65292;&#28982;&#21518;&#22312;&#36825;&#20123;&#19978;&#19979;&#25991;&#19978;&#35757;&#32451;&#8220;&#25991;&#26723;&#32423;&#8221;&#35821;&#35328;&#27169;&#22411;&#20197;&#29983;&#25104;&#22270;&#32423;&#23884;&#20837;&#12290;&#25105;&#20204;&#23558;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;&#20116;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph representation learning (also known as network embedding) has been extensively researched with varying levels of granularity, ranging from nodes to graphs. While most prior work in this area focuses on node-level representation, limited research has been conducted on graph-level embedding, particularly for dynamic or temporal networks. However, learning low-dimensional graph-level representations for dynamic networks is critical for various downstream graph retrieval tasks such as temporal graph similarity ranking, temporal graph isomorphism, and anomaly detection. In this paper, we present a novel method for temporal graph-level embedding that addresses this gap. Our approach involves constructing a multilayer graph and using a modified random walk with temporal backtracking to generate temporal contexts for the graph's nodes. We then train a "document-level" language model on these contexts to generate graph-level embeddings. We evaluate our proposed model on five publicly avai
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#19981;&#21516;&#35268;&#27169;&#30340;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#30340;&#28436;&#32462;&#25512;&#29702;&#33021;&#21147;&#65292;&#21457;&#29616;&#38543;&#30528;&#35268;&#27169;&#30340;&#22686;&#21152;&#65292;&#25512;&#29702;&#33021;&#21147;&#22686;&#24378;&#65292;&#38271;&#24230;&#19981;&#20250;&#24433;&#21709;&#22823;&#37096;&#20998;&#27169;&#22411;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2306.01009</link><description>&lt;p&gt;
&#25506;&#31350;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#30340;&#28436;&#32462;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Examining the Emergence of Deductive Reasoning in Generative Language Models. (arXiv:2306.01009v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01009
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#19981;&#21516;&#35268;&#27169;&#30340;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#30340;&#28436;&#32462;&#25512;&#29702;&#33021;&#21147;&#65292;&#21457;&#29616;&#38543;&#30528;&#35268;&#27169;&#30340;&#22686;&#21152;&#65292;&#25512;&#29702;&#33021;&#21147;&#22686;&#24378;&#65292;&#38271;&#24230;&#19981;&#20250;&#24433;&#21709;&#22823;&#37096;&#20998;&#27169;&#22411;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23545;&#29983;&#25104;&#21464;&#21387;&#22120;&#27169;&#22411;&#20174;&#21069;&#25552;&#20013;&#36827;&#34892;&#28436;&#32462;&#25512;&#29702;&#30340;&#33021;&#21147;&#36827;&#34892;&#21021;&#27493;&#35843;&#26597;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#19981;&#21516;&#35757;&#32451;&#35774;&#32622;&#30340;&#27169;&#22411;&#24615;&#33021;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#65292;&#24182;&#21457;&#29616;&#28436;&#32462;&#25512;&#29702;&#33021;&#21147;&#38543;&#35268;&#27169;&#22686;&#21152;&#32780;&#22686;&#24378;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;&#38500;&#20102;OpenAI GPT-3&#21644;GPT-3.5&#27169;&#22411;&#65292;&#27169;&#22411;&#30340;&#34920;&#29616;&#36890;&#24120;&#19981;&#20250;&#38543;&#30528;&#25512;&#29702;&#38142;&#30340;&#38271;&#24230;&#32780;&#20943;&#24369;&#12290;&#26412;&#30740;&#31350;&#32771;&#34385;&#20102;&#20174;1.17&#20159;&#21040;1750&#20159;&#20010;&#21442;&#25968;&#30340;&#21508;&#31181;&#21464;&#21387;&#22120;&#35299;&#30721;&#22120;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
We conduct a preliminary inquiry into the ability of generative transformer models to deductively reason from premises provided. We observe notable differences in the performance of models coming from different training setups and find that the deductive reasoning ability increases with scale. Further, we discover that the performance generally does not decrease with the length of the deductive chain needed to reach the conclusion, with the exception of OpenAI GPT-3 and GPT-3.5 models. Our study considers a wide variety of transformer-decoder models, ranging from 117 million to 175 billion parameters in size.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#26080;&#24615;&#32321;&#27542;&#20248;&#21270;&#31639;&#27861;&#36827;&#34892;&#20449;&#29992;&#21345;&#27450;&#35784;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#19982;&#20854;&#20182;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26174;&#33879;&#38477;&#20302;&#35757;&#32451;&#26102;&#38388;&#65292;&#24182;&#22312;&#37325;&#35201;&#30340;&#27450;&#35784;&#26816;&#27979;&#38382;&#39064;&#20013;&#22686;&#21152;&#21484;&#22238;&#29575;&#12290;</title><link>http://arxiv.org/abs/2306.01008</link><description>&lt;p&gt;
&#21033;&#29992;&#26080;&#24615;&#32321;&#27542;&#20248;&#21270;&#31639;&#27861;&#36827;&#34892;&#20449;&#29992;&#21345;&#27450;&#35784;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Credit Card Fraud Detection Using Asexual Reproduction Optimization. (arXiv:2306.01008v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01008
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#26080;&#24615;&#32321;&#27542;&#20248;&#21270;&#31639;&#27861;&#36827;&#34892;&#20449;&#29992;&#21345;&#27450;&#35784;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#19982;&#20854;&#20182;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26174;&#33879;&#38477;&#20302;&#35757;&#32451;&#26102;&#38388;&#65292;&#24182;&#22312;&#37325;&#35201;&#30340;&#27450;&#35784;&#26816;&#27979;&#38382;&#39064;&#20013;&#22686;&#21152;&#21484;&#22238;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20449;&#29992;&#21345;&#29992;&#25143;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;&#22312;&#36825;&#19968;&#39046;&#22495;&#26816;&#27979;&#27450;&#35784;&#24050;&#25104;&#20026;&#33267;&#20851;&#37325;&#35201;&#30340;&#38382;&#39064;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#24212;&#29992;&#20102;&#21508;&#31181;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#26469;&#23547;&#27714;&#26377;&#25928;&#30340;&#27450;&#35784;&#26816;&#27979;&#31995;&#32479;&#12290;&#28982;&#32780;&#65292;&#19968;&#20123;&#26041;&#27861;&#38656;&#35201;&#22823;&#37327;&#26102;&#38388;&#25165;&#33021;&#36798;&#21040;&#21512;&#29702;&#30340;&#20934;&#30830;&#24615;&#12290;&#26412;&#25991;&#37319;&#29992;&#20102;&#26080;&#24615;&#32321;&#27542;&#20248;&#21270;&#31639;&#27861;&#65288;ARO&#65289;&#26041;&#27861;&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;&#26816;&#27979;&#20449;&#29992;&#21345;&#27450;&#35784;&#30340;&#30417;&#30563;&#26041;&#27861;&#12290;&#36890;&#36807;&#20165;&#20174;&#22810;&#25968;&#31867;&#36827;&#34892;&#37319;&#26679;&#65292;ARO&#26041;&#27861;&#33021;&#22815;&#25552;&#39640;&#20998;&#31867;&#30340;&#26377;&#25928;&#24615;&#12290;&#19982;&#24403;&#21069;&#25968;&#25454;&#38598;&#19978;&#23454;&#26045;&#30340;&#26368;&#20339;&#26041;&#27861;&#20043;&#19968;&#30340;&#20154;&#24037;&#20813;&#30123;&#31995;&#32479;&#65288;AIS&#65289;&#36827;&#34892;&#27604;&#36739;&#65292;&#32467;&#26524;&#34920;&#26126;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#33021;&#22815;&#26174;&#30528;&#38477;&#20302;&#25152;&#38656;&#30340;&#35757;&#32451;&#26102;&#38388;&#65292;&#24182;&#22312;&#37325;&#35201;&#30340;&#27450;&#35784;&#26816;&#27979;&#38382;&#39064;&#20013;&#22686;&#21152;&#21484;&#22238;&#29575;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;ARO&#26041;&#27861;&#22312;&#26102;&#38388;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
As the number of credit card users has increased, detecting fraud in this domain has become a vital issue. Previous literature has applied various supervised and unsupervised machine learning methods to find an effective fraud detection system. However, some of these methods require an enormous amount of time to achieve reasonable accuracy. In this paper, an Asexual Reproduction Optimization (ARO) approach was employed, which is a supervised method to detect credit card fraud. ARO refers to a kind of production in which one parent produces some offspring. By applying this method and sampling just from the majority class, the effectiveness of the classification is increased. A comparison to Artificial Immune Systems (AIS), which is one of the best methods implemented on current datasets, has shown that the proposed method is able to remarkably reduce the required training time and at the same time increase the recall that is important in fraud detection problems. The obtained results sh
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#21464;&#21270;&#29615;&#22659;&#30340;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#23558;&#27169;&#22411;&#21442;&#25968;&#21010;&#20998;&#20026;&#29615;&#22659;&#19981;&#21464;&#37096;&#20998;&#21644;&#29615;&#22659;&#29305;&#23450;&#37096;&#20998;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#25968;&#25454;&#20844;&#24179;&#24615;&#12290;&#36890;&#36807;&#22823;&#37327;&#30340;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#35813;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.01007</link><description>&lt;p&gt;
&#38754;&#21521;&#21464;&#21270;&#29615;&#22659;&#30340;&#20844;&#24179;&#35299;&#32544;&#22312;&#32447;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Towards Fair Disentangled Online Learning for Changing Environments. (arXiv:2306.01007v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01007
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#21464;&#21270;&#29615;&#22659;&#30340;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#23558;&#27169;&#22411;&#21442;&#25968;&#21010;&#20998;&#20026;&#29615;&#22659;&#19981;&#21464;&#37096;&#20998;&#21644;&#29615;&#22659;&#29305;&#23450;&#37096;&#20998;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#25968;&#25454;&#20844;&#24179;&#24615;&#12290;&#36890;&#36807;&#22823;&#37327;&#30340;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#35813;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#38754;&#23545;&#21464;&#21270;&#29615;&#22659;&#30340;&#22312;&#32447;&#23398;&#20064;&#38382;&#39064;&#20013;&#65292;&#25968;&#25454;&#25353;&#26102;&#38388;&#39034;&#24207;&#19968;&#20010;&#25509;&#19968;&#20010;&#22320;&#25509;&#25910;&#65292;&#24182;&#19988;&#23427;&#20204;&#30340;&#20998;&#24067;&#20551;&#35774;&#21487;&#33021;&#32463;&#24120;&#21464;&#21270;&#12290;&#34429;&#28982;&#29616;&#26377;&#26041;&#27861;&#36890;&#36807;&#25552;&#20379;&#23545;&#21160;&#24577;&#36951;&#25022;&#25110;&#33258;&#36866;&#24212;&#36951;&#25022;&#30340;&#20005;&#26684;&#30028;&#38480;&#26469;&#23637;&#31034;&#20854;&#23398;&#20064;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#20294;&#23427;&#20204;&#22823;&#22810;&#23436;&#20840;&#24573;&#30053;&#20102;&#24102;&#26377;&#27169;&#22411;&#20844;&#24179;&#24615;&#30340;&#23398;&#20064;&#65292;&#20854;&#23450;&#20041;&#20026;&#36328;&#19981;&#21516;&#23376;&#26063;&#32676;&#65288;&#20363;&#22914;&#65292;&#31181;&#26063;&#21644;&#24615;&#21035;&#65289;&#30340;&#32479;&#35745;&#24179;&#31561;&#12290;&#21478;&#19968;&#20010;&#32570;&#28857;&#26159;&#65292;&#22312;&#36866;&#24212;&#26032;&#29615;&#22659;&#26102;&#65292;&#22312;&#32447;&#23398;&#20064;&#32773;&#38656;&#35201;&#20351;&#29992;&#20840;&#23616;&#26356;&#25913;&#26356;&#26032;&#27169;&#22411;&#21442;&#25968;&#65292;&#36825;&#26159;&#26114;&#36149;&#21644;&#20302;&#25928;&#30340;&#12290;&#21463;&#21040;&#31232;&#30095;&#26426;&#21046;&#36716;&#31227;&#20551;&#35774;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#22768;&#31216;&#22312;&#32447;&#23398;&#20064;&#20013;&#30340;&#21464;&#21270;&#29615;&#22659;&#21487;&#20197;&#24402;&#22240;&#20110;&#29305;&#23450;&#20110;&#29615;&#22659;&#30340;&#37096;&#20998;&#23398;&#20064;&#21442;&#25968;&#30340;&#37096;&#20998;&#21464;&#21270;&#65292;&#20854;&#20313;&#37096;&#20998;&#20445;&#25345;&#19981;&#21464;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#22312;&#20551;&#35774;&#20174;&#19981;&#21516;&#23376;&#20154;&#32676;&#25910;&#38598;&#30340;&#25968;&#25454;&#20855;&#26377;&#20844;&#24179;&#30340;&#27169;&#22411;&#34920;&#31034;&#30340;&#21069;&#25552;&#19979;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#65292;&#23558;&#27169;&#22411;&#21442;&#25968;&#20998;&#20026;&#29615;&#22659;&#19981;&#21464;&#37096;&#20998;&#21644;&#29615;&#22659;&#29305;&#23450;&#37096;&#20998;&#12290;&#25105;&#20204;&#25512;&#23548;&#20102;&#27599;&#20010;&#23376;&#20154;&#32676;&#27169;&#22411;&#34920;&#31034;&#20844;&#27491;&#24615;&#30340;&#32479;&#35745;&#20445;&#35777;&#65292;&#24182;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;&#30340;&#25910;&#25947;&#36895;&#29575;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#23545;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#30340;&#24191;&#27867;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the problem of online learning for changing environments, data are sequentially received one after another over time, and their distribution assumptions may vary frequently. Although existing methods demonstrate the effectiveness of their learning algorithms by providing a tight bound on either dynamic regret or adaptive regret, most of them completely ignore learning with model fairness, defined as the statistical parity across different sub-population (e.g., race and gender). Another drawback is that when adapting to a new environment, an online learner needs to update model parameters with a global change, which is costly and inefficient. Inspired by the sparse mechanism shift hypothesis, we claim that changing environments in online learning can be attributed to partial changes in learned parameters that are specific to environments and the rest remain invariant to changing environments. To this end, in this paper, we propose a novel algorithm under the assumption that data coll
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20174;&#22836;&#35774;&#35745;&#25239;&#20307;&#30340;&#26041;&#27861;AbODE&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#36830;&#32493;&#24494;&#20998;&#27880;&#24847;&#21147;&#26469;&#24212;&#23545;&#34507;&#30333;&#25240;&#21472;&#12289;&#36870;&#21521;&#25240;&#21472;&#21644;&#23545;&#25509;&#31561;&#25361;&#25112;&#65292;&#24182;&#19982;&#26102;&#38388;&#32593;&#32476;&#21644;&#22270;&#21305;&#37197;&#32593;&#32476;&#20855;&#26377;&#22522;&#26412;&#32852;&#31995;&#12290;</title><link>http://arxiv.org/abs/2306.01005</link><description>&lt;p&gt;
AbODE&#65306;&#20351;&#29992;&#32852;&#21512;ODE&#30340;&#20174;&#22836;&#35774;&#35745;&#25239;&#20307;
&lt;/p&gt;
&lt;p&gt;
AbODE: Ab Initio Antibody Design using Conjoined ODEs. (arXiv:2306.01005v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01005
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20174;&#22836;&#35774;&#35745;&#25239;&#20307;&#30340;&#26041;&#27861;AbODE&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#36830;&#32493;&#24494;&#20998;&#27880;&#24847;&#21147;&#26469;&#24212;&#23545;&#34507;&#30333;&#25240;&#21472;&#12289;&#36870;&#21521;&#25240;&#21472;&#21644;&#23545;&#25509;&#31561;&#25361;&#25112;&#65292;&#24182;&#19982;&#26102;&#38388;&#32593;&#32476;&#21644;&#22270;&#21305;&#37197;&#32593;&#32476;&#20855;&#26377;&#22522;&#26412;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25239;&#20307;&#26159;&#19968;&#31181;Y&#24418;&#34507;&#30333;&#65292;&#21487;&#20197;&#20013;&#21644;&#30149;&#21407;&#20307;&#65292;&#24182;&#26500;&#25104;&#25105;&#20204;&#36866;&#24212;&#24615;&#20813;&#30123;&#31995;&#32479;&#30340;&#26680;&#24515;&#12290;&#26032;&#30340;&#25239;&#20307;&#30340;&#20174;&#22836;&#35774;&#35745;&#65292;&#20197;&#29305;&#23450;&#25239;&#21407;&#20026;&#38774;&#26631;&#65292;&#26159;&#21152;&#36895;&#30123;&#33495;&#21457;&#29616;&#30340;&#20851;&#38190;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#27688;&#22522;&#37240;&#24207;&#21015;&#21644;&#19977;&#32500;&#32467;&#26500;&#30340;&#32852;&#21512;&#35774;&#35745;&#28085;&#30422;&#24182;&#24378;&#35843;&#20102;&#26469;&#33258;&#22810;&#20010;&#20219;&#21153;&#30340;&#19968;&#20123;&#26680;&#24515;&#25361;&#25112;&#65292;&#21253;&#25324;&#34507;&#30333;&#25240;&#21472;&#65288;&#24207;&#21015;&#21040;&#32467;&#26500;&#65289;&#65292;&#36870;&#21521;&#25240;&#21472;&#65288;&#32467;&#26500;&#21040;&#24207;&#21015;&#65289;&#21644;&#23545;&#25509;&#65288;&#32467;&#21512;&#65289;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#20010;&#26032;&#30340;&#29983;&#25104;&#27169;&#22411;AbODE&#26469;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#35813;&#27169;&#22411;&#25193;&#23637;&#20102;&#22270;&#24418;PDE&#20197;&#36866;&#24212;&#19978;&#19979;&#25991;&#20449;&#24687;&#21644;&#22806;&#37096;&#30456;&#20114;&#20316;&#29992;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#65292;AbODE&#20351;&#29992;&#19968;&#36718;&#23436;&#25972;&#30340;&#35299;&#30721;&#65292;&#24182;&#24341;&#20986;&#36830;&#32493;&#24494;&#20998;&#27880;&#24847;&#21147;&#65292;&#20854;&#20013;&#21253;&#25324;&#25239;&#20307;&#20869;&#37096;&#21644;&#25239;&#21407;&#30456;&#20851;&#30340;&#28508;&#22312;&#30456;&#20114;&#20316;&#29992;&#12290;&#25105;&#20204;&#25581;&#31034;&#20102;AbODE&#19982;&#26102;&#38388;&#32593;&#32476;&#20197;&#21450;&#22270;&#21305;&#37197;&#32593;&#32476;&#20043;&#38388;&#30340;&#22522;&#26412;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Antibodies are Y-shaped proteins that neutralize pathogens and constitute the core of our adaptive immune system. De novo generation of new antibodies that target specific antigens holds the key to accelerating vaccine discovery. However, this co-design of the amino acid sequence and the 3D structure subsumes and accentuates some central challenges from multiple tasks, including protein folding (sequence to structure), inverse folding (structure to sequence), and docking (binding). We strive to surmount these challenges with a new generative model AbODE that extends graph PDEs to accommodate both contextual information and external interactions. Unlike existing approaches, AbODE uses a single round of full-shot decoding and elicits continuous differential attention that encapsulates and evolves with latent interactions within the antibody as well as those involving the antigen. We unravel fundamental connections between AbODE and temporal networks as well as graph-matching networks. Th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38754;&#21521;&#26041;&#38754;&#30340;&#26041;&#27861;(AoM)&#26469;&#26816;&#27979;&#19982;&#26041;&#38754;&#30456;&#20851;&#30340;&#35821;&#20041;&#21644;&#24773;&#24863;&#20449;&#24687;&#65292;&#24182;&#26126;&#30830;&#22320;&#23558;&#24773;&#24863;&#23884;&#20837;&#21040;AoM&#20013;&#12290;&#23454;&#39564;&#35777;&#26126;AoM&#22312;&#25913;&#36827;&#22810;&#27169;&#24577;&#22522;&#20110;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#26512;(MABSA)&#26041;&#38754;&#38750;&#24120;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2306.01004</link><description>&lt;p&gt;
AoM&#65306;&#29992;&#20110;&#22810;&#27169;&#24577;&#22522;&#20110;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#26512;&#20013;&#26816;&#27979;&#38754;&#21521;&#26041;&#38754;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
AoM: Detecting Aspect-oriented Information for Multimodal Aspect-Based Sentiment Analysis. (arXiv:2306.01004v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01004
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38754;&#21521;&#26041;&#38754;&#30340;&#26041;&#27861;(AoM)&#26469;&#26816;&#27979;&#19982;&#26041;&#38754;&#30456;&#20851;&#30340;&#35821;&#20041;&#21644;&#24773;&#24863;&#20449;&#24687;&#65292;&#24182;&#26126;&#30830;&#22320;&#23558;&#24773;&#24863;&#23884;&#20837;&#21040;AoM&#20013;&#12290;&#23454;&#39564;&#35777;&#26126;AoM&#22312;&#25913;&#36827;&#22810;&#27169;&#24577;&#22522;&#20110;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#26512;(MABSA)&#26041;&#38754;&#38750;&#24120;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#22522;&#20110;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#26512;(MABSA)&#26088;&#22312;&#20174;&#25991;&#26412;-&#22270;&#20687;&#23545;&#20013;&#25552;&#21462;&#26041;&#38754;&#24182;&#35782;&#21035;&#23427;&#20204;&#30340;&#24773;&#24863;&#12290;&#29616;&#26377;&#26041;&#27861;&#33268;&#21147;&#20110;&#23558;&#25972;&#20010;&#22270;&#20687;&#19982;&#30456;&#24212;&#30340;&#26041;&#38754;&#23545;&#40784;&#12290;&#28982;&#32780;&#65292;&#22270;&#20687;&#30340;&#19981;&#21516;&#21306;&#22495;&#21487;&#33021;&#19982;&#21516;&#19968;&#21477;&#23376;&#20013;&#30340;&#19981;&#21516;&#26041;&#38754;&#30456;&#20851;&#65292;&#31895;&#30053;&#22320;&#24314;&#31435;&#22270;&#20687;-&#26041;&#38754;&#23545;&#40784;&#20250;&#24341;&#20837;&#22122;&#22768;&#21040;&#22522;&#20110;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#26512;&#20013;(&#21363;&#35270;&#35273;&#22122;&#22768;)&#12290;&#27492;&#22806;&#65292;&#29305;&#23450;&#26041;&#38754;&#30340;&#24773;&#24863;&#20063;&#21487;&#33021;&#34987;&#20854;&#20182;&#26041;&#38754;&#30340;&#25551;&#36848;&#25152;&#24178;&#25200;(&#21363;&#25991;&#26412;&#22122;&#22768;)&#12290;&#32771;&#34385;&#21040;&#19978;&#36848;&#22122;&#22768;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#26041;&#38754;&#30340;&#26041;&#27861;(AoM)&#26469;&#26816;&#27979;&#19982;&#26041;&#38754;&#30456;&#20851;&#30340;&#35821;&#20041;&#21644;&#24773;&#24863;&#20449;&#24687;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#38754;&#21521;&#26041;&#38754;&#30340;&#27880;&#24847;&#21147;&#27169;&#22359;&#65292;&#21516;&#26102;&#36873;&#25321;&#19982;&#26041;&#38754;&#26377;&#35821;&#20041;&#20851;&#32852;&#30340;&#25991;&#26412;&#20196;&#29260;&#21644;&#22270;&#20687;&#22359;&#12290;&#20026;&#20102;&#20934;&#30830;&#22320;&#32858;&#21512;&#24773;&#24863;&#20449;&#24687;&#65292;&#25105;&#20204;&#26126;&#30830;&#22320;&#23558;&#24773;&#24863;&#23884;&#20837;&#21040;AoM&#20013;&#65292;&#24182;&#20351;&#29992;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#22312;&#19981;&#21516;&#27169;&#24577;&#20013;&#25429;&#33719;&#20381;&#36182;&#20110;&#26041;&#38754;&#30340;&#24773;&#24863;&#34920;&#36798;&#24335;&#12290;&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20102;AoM&#22312;&#25913;&#36827;MABSA&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal aspect-based sentiment analysis (MABSA) aims to extract aspects from text-image pairs and recognize their sentiments. Existing methods make great efforts to align the whole image to corresponding aspects. However, different regions of the image may relate to different aspects in the same sentence, and coarsely establishing image-aspect alignment will introduce noise to aspect-based sentiment analysis (i.e., visual noise). Besides, the sentiment of a specific aspect can also be interfered by descriptions of other aspects (i.e., textual noise). Considering the aforementioned noises, this paper proposes an Aspect-oriented Method (AoM) to detect aspect-relevant semantic and sentiment information. Specifically, an aspect-aware attention module is designed to simultaneously select textual tokens and image blocks that are semantically related to the aspects. To accurately aggregate sentiment information, we explicitly introduce sentiment embedding into AoM, and use a graph convolut
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#36127;&#33655;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#27861;&#65292;&#37319;&#29992;Seq2Seq&#32593;&#32476;&#32467;&#26500;&#26469;&#20998;&#31163;&#20004;&#31181;&#31867;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#24182;&#22788;&#29702;&#24322;&#24120;&#24773;&#20917;&#65292;&#19981;&#20165;&#30528;&#30524;&#20110;&#39044;&#27979;&#26465;&#20214;&#26399;&#26395;&#20540;&#12290;</title><link>http://arxiv.org/abs/2306.01001</link><description>&lt;p&gt;
DiffLoad:&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#36127;&#33655;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
DiffLoad: Uncertainty Quantification in Load Forecasting with Diffusion Model. (arXiv:2306.01001v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01001
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#36127;&#33655;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#27861;&#65292;&#37319;&#29992;Seq2Seq&#32593;&#32476;&#32467;&#26500;&#26469;&#20998;&#31163;&#20004;&#31181;&#31867;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#24182;&#22788;&#29702;&#24322;&#24120;&#24773;&#20917;&#65292;&#19981;&#20165;&#30528;&#30524;&#20110;&#39044;&#27979;&#26465;&#20214;&#26399;&#26395;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#21147;&#36127;&#33655;&#39044;&#27979;&#23545;&#30005;&#21147;&#31995;&#32479;&#30340;&#20915;&#31574;&#21046;&#23450;&#65292;&#22914;&#26426;&#32452;&#25237;&#20837;&#21644;&#33021;&#28304;&#31649;&#29702;&#31561;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#36817;&#24180;&#26469;&#65292;&#21508;&#31181;&#22522;&#20110;&#33258;&#30417;&#30563;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#24050;&#32463;&#34987;&#24212;&#29992;&#20110;&#30005;&#21147;&#36127;&#33655;&#39044;&#27979;&#65292;&#20197;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#25429;&#25417;&#19981;&#30830;&#23450;&#24615;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#26041;&#27861;&#26159;&#22522;&#20110;&#39640;&#26031;&#20284;&#28982;&#26041;&#27861;&#30340;&#65292;&#23427;&#26088;&#22312;&#22312;&#32473;&#23450;&#30340;&#21327;&#21464;&#37327;&#19979;&#20934;&#30830;&#20272;&#35745;&#20998;&#24067;&#26399;&#26395;&#20540;&#12290;&#36825;&#31181;&#26041;&#27861;&#24456;&#38590;&#36866;&#24212;&#23384;&#22312;&#20998;&#24067;&#20559;&#31227;&#21644;&#24322;&#24120;&#20540;&#30340;&#26102;&#38388;&#25968;&#25454;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#30340;Seq2seq&#32467;&#26500;&#26469;&#20272;&#35745;&#26412;&#20307;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#20351;&#29992;&#40065;&#26834;&#30340;&#21152;&#24615;&#26607;&#35199;&#20998;&#24067;&#26469;&#20272;&#35745;&#29289;&#35937;&#19981;&#30830;&#23450;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#20998;&#31163;&#20004;&#31181;&#31867;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#24182;&#22788;&#29702;&#31361;&#21464;&#24773;&#20917;&#65292;&#32780;&#19981;&#26159;&#20934;&#30830;&#39044;&#27979;&#26465;&#20214;&#26399;&#26395;&#12290;
&lt;/p&gt;
&lt;p&gt;
Electrical load forecasting is of great significance for the decision makings in power systems, such as unit commitment and energy management. In recent years, various self-supervised neural network-based methods have been applied to electrical load forecasting to improve forecasting accuracy and capture uncertainties. However, most current methods are based on Gaussian likelihood methods, which aim to accurately estimate the distribution expectation under a given covariate. This kind of approach is difficult to adapt to situations where temporal data has a distribution shift and outliers. In this paper, we propose a diffusion-based Seq2seq structure to estimate epistemic uncertainty and use the robust additive Cauchy distribution to estimate aleatoric uncertainty. Rather than accurately forecasting conditional expectations, we demonstrate our method's ability in separating two types of uncertainties and dealing with the mutant scenarios.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#22270;&#24418;&#24322;&#24120;&#30417;&#27979;&#25968;&#25454;&#38598;&#20013;&#23384;&#22312;&#30340;&#19968;&#31867;&#21516;&#22411;&#29616;&#35937;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#30417;&#30563;&#24322;&#24120;&#35780;&#20998;&#24230;&#37327;&#8212;&#8212;&#24403;&#21069;&#33410;&#28857;&#20146;&#21644;&#21147;&#65292;&#24182;&#36890;&#36807;&#23398;&#20064;&#37327;&#36523;&#23450;&#21046;&#30340;&#33410;&#28857;&#34920;&#31034;&#65292;&#23454;&#29616;&#20102;&#25130;&#26029;&#20146;&#21644;&#21147;&#26368;&#22823;&#21270;&#65288;TAM&#65289;&#26041;&#27861;&#65292;&#20248;&#21270;&#22312;&#21407;&#22987;&#22270;&#24418;&#32467;&#26500;&#19978;&#36827;&#34892;&#65292;&#33021;&#22815;&#26377;&#25928;&#36827;&#34892;&#21452;&#37325;One-Class&#30340;GAD&#12290;</title><link>http://arxiv.org/abs/2306.00006</link><description>&lt;p&gt;
&#25130;&#26029;&#20146;&#21644;&#21147;&#26368;&#22823;&#21270;&#65306;&#29992;&#20110;&#22270;&#24418;&#24322;&#24120;&#30417;&#27979;&#30340;&#21333;&#31867;&#21516;&#22411;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Truncated Affinity Maximization: One-class Homophily Modeling for Graph Anomaly Detection. (arXiv:2306.00006v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00006
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#22270;&#24418;&#24322;&#24120;&#30417;&#27979;&#25968;&#25454;&#38598;&#20013;&#23384;&#22312;&#30340;&#19968;&#31867;&#21516;&#22411;&#29616;&#35937;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#30417;&#30563;&#24322;&#24120;&#35780;&#20998;&#24230;&#37327;&#8212;&#8212;&#24403;&#21069;&#33410;&#28857;&#20146;&#21644;&#21147;&#65292;&#24182;&#36890;&#36807;&#23398;&#20064;&#37327;&#36523;&#23450;&#21046;&#30340;&#33410;&#28857;&#34920;&#31034;&#65292;&#23454;&#29616;&#20102;&#25130;&#26029;&#20146;&#21644;&#21147;&#26368;&#22823;&#21270;&#65288;TAM&#65289;&#26041;&#27861;&#65292;&#20248;&#21270;&#22312;&#21407;&#22987;&#22270;&#24418;&#32467;&#26500;&#19978;&#36827;&#34892;&#65292;&#33021;&#22815;&#26377;&#25928;&#36827;&#34892;&#21452;&#37325;One-Class&#30340;GAD&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#22270;&#24418;&#24322;&#24120;&#30417;&#27979;&#65288;GAD&#65289;&#25968;&#25454;&#38598;&#20013;&#32463;&#24120;&#21457;&#29616;&#19968;&#31181;&#26222;&#36941;&#30340;&#23646;&#24615;......&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#30417;&#30563;&#24322;&#24120;&#35780;&#20998;&#24230;&#37327; - &#24403;&#21069;&#33410;&#28857;&#20146;&#21644;&#21147;......&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#25130;&#26029;&#20146;&#21644;&#21147;&#26368;&#22823;&#21270; (TAM)&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#26368;&#22823;&#21270;&#19982;_neighbors&#30340;&#26412;&#22320;&#20146;&#21644;&#21147;&#26469;&#23398;&#20064;&#37327;&#36523;&#23450;&#21046;&#30340;&#33410;&#28857;&#34920;&#31034;&#12290;&#26412;&#25991;&#25152;&#25552;&#26041;&#27861;&#22312;&#21407;&#22987;&#22270;&#24418;&#32467;&#26500;&#19978;&#36827;&#34892;&#20248;&#21270;&#65292;&#21487;&#20197;&#36827;&#34892;&#21452;&#37325;One-Class&#30340;GAD&#12290;
&lt;/p&gt;
&lt;p&gt;
One prevalent property we find empirically in real-world graph anomaly detection (GAD) datasets is a one-class homophily, i.e., normal nodes tend to have strong connection/affinity with each other, while the homophily in abnormal nodes is significantly weaker than normal nodes. However, this anomaly-discriminative property is ignored by existing GAD methods that are typically built using a conventional anomaly detection objective, such as data reconstruction. In this work, we explore this property to introduce a novel unsupervised anomaly scoring measure for GAD -- local node affinity -- that assigns a larger anomaly score to nodes that are less affiliated with their neighbors, with the affinity defined as similarity on node attributes/representations. We further propose Truncated Affinity Maximization (TAM) that learns tailored node representations for our anomaly measure by maximizing the local affinity of nodes to their neighbors. Optimizing on the original graph structure can be bi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#22522;&#20110;&#26041;&#24046;&#32422;&#31616;&#30340;&#20248;&#21270;&#31639;&#27861;&#65292;&#20197;&#23454;&#29616;&#23545;&#22810;&#22359;&#21452;&#23618;&#20248;&#21270;&#38382;&#39064;&#30340;&#39640;&#25928;&#27714;&#35299;&#65292;&#21516;&#26102;&#21305;&#37197;&#21333;&#22359;&#26631;&#20934; BO &#38382;&#39064;&#30340;&#26368;&#20248;&#22797;&#26434;&#24230;&#12289;&#23454;&#29616;&#24182;&#34892;&#21270;&#21152;&#36895;&#65292;&#20197;&#21450;&#36991;&#20813;&#35745;&#31639;&#39640;&#32500;&#24230;&#30340; Hessian &#30697;&#38453;&#30340;&#36870;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2305.18730</link><description>&lt;p&gt;
&#22810;&#22359;&#21452;&#23618;&#20248;&#21270;&#30340;&#20998;&#22359;&#38543;&#26426;&#26041;&#24046;&#32422;&#31616;&#26041;&#27861;&#21450;&#24182;&#34892;&#21152;&#36895;
&lt;/p&gt;
&lt;p&gt;
Blockwise Stochastic Variance-Reduced Methods with Parallel Speedup for Multi-Block Bilevel Optimization. (arXiv:2305.18730v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18730
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#22522;&#20110;&#26041;&#24046;&#32422;&#31616;&#30340;&#20248;&#21270;&#31639;&#27861;&#65292;&#20197;&#23454;&#29616;&#23545;&#22810;&#22359;&#21452;&#23618;&#20248;&#21270;&#38382;&#39064;&#30340;&#39640;&#25928;&#27714;&#35299;&#65292;&#21516;&#26102;&#21305;&#37197;&#21333;&#22359;&#26631;&#20934; BO &#38382;&#39064;&#30340;&#26368;&#20248;&#22797;&#26434;&#24230;&#12289;&#23454;&#29616;&#24182;&#34892;&#21270;&#21152;&#36895;&#65292;&#20197;&#21450;&#36991;&#20813;&#35745;&#31639;&#39640;&#32500;&#24230;&#30340; Hessian &#30697;&#38453;&#30340;&#36870;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#38750;&#20984;&#30340;&#22810;&#22359;&#21452;&#23618;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#22522;&#20110;&#20998;&#22359;&#26041;&#24046;&#32422;&#31616;&#30340;&#20248;&#21270;&#31639;&#27861;&#12290;&#20026;&#20102;&#36798;&#21040;&#31639;&#27861;&#30340;&#19977;&#20010;&#26399;&#26395;&#65306;&#65288;a&#65289;&#33021;&#21305;&#37197;&#21333;&#22359;&#26631;&#20934; BO &#38382;&#39064;&#30340;&#26368;&#20248;&#22797;&#26434;&#24230;&#65307;&#65288;b&#65289;&#23454;&#29616;&#24182;&#34892;&#21270;&#21152;&#36895;&#65292;&#27599;&#20010;&#36845;&#20195;&#20013;&#37319;&#26679; $I$ &#22359;&#24182;&#23545;&#27599;&#20010;&#37319;&#26679;&#22359;&#37319;&#26679; $B$ &#20010;&#26679;&#26412;&#65307;&#65288;c&#65289;&#36991;&#20813;&#35745;&#31639;&#39640;&#32500;&#24230;&#30340; Hessian &#30697;&#38453;&#30340;&#36870;&#20272;&#35745;&#12290;&#26412;&#25991;&#26088;&#22312;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#24182;&#25506;&#35752;&#20102;&#29616;&#26377;&#31639;&#27861;&#30340;&#20851;&#32852;&#24615;&#20197;&#21450;&#19981;&#36275;&#20043;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we consider non-convex multi-block bilevel optimization (MBBO) problems, which involve $m\gg 1$ lower level problems and have important applications in machine learning. Designing a stochastic gradient and controlling its variance is more intricate due to the hierarchical sampling of blocks and data and the unique challenge of estimating hyper-gradient. We aim to achieve three nice properties for our algorithm: (a) matching the state-of-the-art complexity of standard BO problems with a single block; (b) achieving parallel speedup by sampling $I$ blocks and sampling $B$ samples for each sampled block per-iteration; (c) avoiding the computation of the inverse of a high-dimensional Hessian matrix estimator. However, it is non-trivial to achieve all of these by observing that existing works only achieve one or two of these properties. To address the involved challenges for achieving (a, b, c), we propose two stochastic algorithms by using advanced blockwise variance-reductio
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;Transformer&#27169;&#22411;&#22312;&#19977;&#20010;&#20195;&#34920;&#24615;&#32452;&#21512;&#22411;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#20854;&#36890;&#36807;&#32447;&#24615;&#23376;&#22270;&#21305;&#37197;&#35299;&#20915;&#22810;&#27493;&#32452;&#21512;&#25512;&#29702;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.18654</link><description>&lt;p&gt;
&#20449;&#20208;&#19982;&#21629;&#36816;&#65306;Transformer&#22312;&#32452;&#21512;&#24615;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Faith and Fate: Limits of Transformers on Compositionality. (arXiv:2305.18654v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18654
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;Transformer&#27169;&#22411;&#22312;&#19977;&#20010;&#20195;&#34920;&#24615;&#32452;&#21512;&#22411;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#20854;&#36890;&#36807;&#32447;&#24615;&#23376;&#22270;&#21305;&#37197;&#35299;&#20915;&#22810;&#27493;&#32452;&#21512;&#25512;&#29702;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38656;&#35201;&#22797;&#26434;&#22810;&#27493;&#25512;&#29702;&#30340;&#20219;&#21153;&#19978;&#34920;&#29616;&#21331;&#36234;&#65292;&#20294;&#21516;&#26102;&#22312;&#19968;&#20123;&#31616;&#21333;&#38382;&#39064;&#19978;&#20063;&#20250;&#20986;&#29616;&#22833;&#36133;&#12290;&#36825;&#24341;&#21457;&#20102;&#30097;&#38382;&#65306;&#36825;&#20123;&#38169;&#35823;&#26159;&#20598;&#28982;&#30340;&#65292;&#36824;&#26159;&#23427;&#20204;&#34920;&#26126;&#20102;&#26356;&#23454;&#36136;&#24615;&#30340;&#38480;&#21046;&#65311;&#20026;&#20102;&#25581;&#31034;Transformer&#30340;&#31070;&#31192;&#38754;&#32433;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#36825;&#20123;&#27169;&#22411;&#22312;&#19977;&#20010;&#20195;&#34920;&#24615;&#30340;&#32452;&#21512;&#22411;&#20219;&#21153;&#20013;&#30340;&#26497;&#38480; - &#22810;&#20301;&#25968;&#20056;&#27861;&#12289;&#36923;&#36753;&#32593;&#26684;&#35868;&#39064;&#21644;&#19968;&#20010;&#32463;&#20856;&#30340;&#21160;&#24577;&#35268;&#21010;&#38382;&#39064;&#12290; &#36825;&#20123;&#20219;&#21153;&#38656;&#35201;&#23558;&#38382;&#39064;&#20998;&#35299;&#20026;&#23376;&#27493;&#39588;&#65292;&#24182;&#23558;&#36825;&#20123;&#27493;&#39588;&#32508;&#21512;&#25104;&#31934;&#30830;&#30340;&#31572;&#26696;&#12290;&#25105;&#20204;&#23558;&#32452;&#21512;&#22411;&#20219;&#21153;&#36716;&#21270;&#20026;&#35745;&#31639;&#22270;&#65292;&#20197;&#31995;&#32479;&#22320;&#37327;&#21270;&#20854;&#22797;&#26434;&#24615;&#65292;&#24182;&#23558;&#25512;&#29702;&#27493;&#39588;&#20998;&#35299;&#20026;&#20013;&#38388;&#23376;&#31243;&#24207;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;Transformer&#36890;&#36807;&#23558;&#22810;&#27493;&#32452;&#21512;&#25512;&#29702;&#36716;&#21270;&#20026;&#32447;&#24615;&#23376;&#22270;&#21305;&#37197;&#26469;&#35299;&#20915;&#32452;&#21512;&#22411;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer large language models (LLMs) have sparked admiration for their exceptional performance on tasks that demand intricate multi-step reasoning. Yet, these models simultaneously show failures on surprisingly trivial problems. This begs the question: Are these errors incidental, or do they signal more substantial limitations? In an attempt to demystify Transformers, we investigate the limits of these models across three representative compositional tasks -- multi-digit multiplication, logic grid puzzles, and a classic dynamic programming problem. These tasks require breaking problems down into sub-steps and synthesizing these steps into a precise answer. We formulate compositional tasks as computation graphs to systematically quantify the level of complexity, and break down reasoning steps into intermediate sub-procedures. Our empirical findings suggest that Transformers solve compositional tasks by reducing multi-step compositional reasoning into linearized subgraph matching, wi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#23610;&#24230;&#27491;&#36127;&#26679;&#26412;&#30340;&#35757;&#32451;&#26694;&#26550;&#65292;&#20197;&#35299;&#20915;&#22810;&#23610;&#24230;AI&#29983;&#25104;&#25991;&#26412;&#30340;&#26816;&#27979;&#38382;&#39064;&#12290;&#36890;&#36807;&#23558;&#30701;&#26426;&#22120;&#25991;&#26412;&#26631;&#35760;&#20026;&#8220;&#26410;&#26631;&#35760;&#8221;&#26469;&#37325;&#26032;&#34920;&#36848;&#25991;&#26412;&#20998;&#31867;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#35268;&#21017;&#21270;&#25439;&#22833;&#20989;&#25968;&#26469;&#20248;&#21270;&#26816;&#27979;&#24615;&#33021;&#65292;&#26377;&#25928;&#24615;&#33021;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.18149</link><description>&lt;p&gt;
&#22810;&#23610;&#24230;&#27491;&#36127;&#26679;&#26412;&#26816;&#27979;AI&#29983;&#25104;&#25991;&#26412;
&lt;/p&gt;
&lt;p&gt;
Multiscale Positive-Unlabeled Detection of AI-Generated Texts. (arXiv:2305.18149v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18149
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#23610;&#24230;&#27491;&#36127;&#26679;&#26412;&#30340;&#35757;&#32451;&#26694;&#26550;&#65292;&#20197;&#35299;&#20915;&#22810;&#23610;&#24230;AI&#29983;&#25104;&#25991;&#26412;&#30340;&#26816;&#27979;&#38382;&#39064;&#12290;&#36890;&#36807;&#23558;&#30701;&#26426;&#22120;&#25991;&#26412;&#26631;&#35760;&#20026;&#8220;&#26410;&#26631;&#35760;&#8221;&#26469;&#37325;&#26032;&#34920;&#36848;&#25991;&#26412;&#20998;&#31867;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#35268;&#21017;&#21270;&#25439;&#22833;&#20989;&#25968;&#26469;&#20248;&#21270;&#26816;&#27979;&#24615;&#33021;&#65292;&#26377;&#25928;&#24615;&#33021;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#21457;&#24067;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22914;ChatGPT&#31561;&#22312;&#29983;&#25104;&#31867;&#20284;&#20110;&#20154;&#31867;&#30340;&#25991;&#26412;&#26041;&#38754;&#20196;&#20154;&#24778;&#35766;&#65292;&#20294;&#23427;&#20204;&#21487;&#33021;&#34987;&#29992;&#20110;&#21046;&#36896;&#34394;&#20551;&#30340;&#23398;&#26415;&#25991;&#26412;&#12289;&#34394;&#20551;&#26032;&#38395;&#12289;&#34394;&#20551;&#25512;&#29305;&#31561;&#12290;&#20808;&#21069;&#30340;&#20316;&#21697;&#25552;&#20986;&#20102;&#26816;&#27979;&#36825;&#20123;&#22810;&#23610;&#24230;AI&#29983;&#25104;&#25991;&#26412;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#31616;&#21333;&#30340;ML&#20998;&#31867;&#22120;&#12289;&#22522;&#20110;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#35757;&#32451;&#19981;&#21487;&#30693;&#26041;&#27861;&#21644;&#31934;&#35843;&#30340;&#35821;&#35328;&#20998;&#31867;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#20027;&#27969;&#26816;&#27979;&#22120;&#22312;&#26500;&#24314;&#26102;&#27809;&#26377;&#32771;&#34385;&#21040;&#25991;&#26412;&#38271;&#24230;&#30340;&#22240;&#32032;&#65306;&#30701;&#25991;&#26412;&#30340;&#32570;&#20047;&#20449;&#24687;&#29305;&#24449;&#65292;&#20351;&#20854;&#26356;&#38590;&#26816;&#27979;&#12290;&#38024;&#23545;&#22810;&#23610;&#24230;&#25991;&#26412;&#26816;&#27979;&#30340;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#23610;&#24230;&#27491;&#36127;&#26679;&#26412;&#65288;MPU&#65289;&#35757;&#32451;&#26694;&#26550;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25215;&#35748;&#30701;&#30340;&#26426;&#22120;&#25991;&#26412;&#20855;&#26377;&#31867;&#20154;&#23646;&#24615;&#65292;&#24182;&#23558;&#25991;&#26412;&#20998;&#31867;&#37325;&#26032;&#34920;&#36848;&#20026;&#19968;&#20010;&#27491;&#36127;&#26679;&#26412;&#38382;&#39064;&#65292;&#21363;&#36890;&#36807;&#22312;&#35757;&#32451;&#26399;&#38388;&#26631;&#35760;&#36825;&#20123;&#30701;&#30340;&#26426;&#22120;&#25991;&#26412;&#20026;"unlabeled"&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#22312;&#36825;&#20010;&#27491;&#36127;&#26679;&#26412;&#30340;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;
&lt;/p&gt;
&lt;p&gt;
Recent releases of Large Language Models (LLMs), e.g. ChatGPT, are astonishing at generating human-like texts, but they may get misused for fake scholarly texts, fake news, fake tweets, et cetera. Previous works have proposed methods to detect these multiscale AI-generated texts, including simple ML classifiers, pretrained-model-based training-agnostic methods, and finetuned language classification models. However, mainstream detectors are formulated without considering the factor of corpus length: shorter corpuses are harder to detect compared with longer ones for shortage of informative features. In this paper, a Multiscale Positive-Unlabeled (MPU) training framework is proposed to address the challenge of multiscale text detection. Firstly, we acknowledge the human-resemblance property of short machine texts, and rephrase text classification as a Positive-Unlabeled (PU) problem by marking these short machine texts as "unlabeled" during training. In this PU context, we propose the le
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22810;&#27169;&#24577;&#30721;&#26412;&#30340;&#25991;&#26412;&#22270;&#29255;&#32763;&#35793;&#27169;&#22411;&#12290;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#22810;&#38454;&#27573;&#35757;&#32451;&#26694;&#26550;&#65292;&#21033;&#29992;&#20102;&#39069;&#22806;&#30340;&#21452;&#35821;&#25991;&#26412;&#21644;&#20809;&#23398;&#23383;&#31526;&#35782;&#21035;&#25968;&#25454;&#38598;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#23558;&#22270;&#20687;&#19982;&#30456;&#20851;&#25991;&#26412;&#20851;&#32852;&#36215;&#26469;&#65292;&#25552;&#20379;&#26377;&#29992;&#30340;&#34917;&#20805;&#20449;&#24687;&#65292;&#21462;&#24471;&#20102;&#27604;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.17415</link><description>&lt;p&gt;
&#22522;&#20110;&#22810;&#27169;&#24577;&#30721;&#26412;&#30340;&#25991;&#26412;&#22270;&#29255;&#32763;&#35793;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Exploring Better Text Image Translation with Multimodal Codebook. (arXiv:2305.17415v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17415
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22810;&#27169;&#24577;&#30721;&#26412;&#30340;&#25991;&#26412;&#22270;&#29255;&#32763;&#35793;&#27169;&#22411;&#12290;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#22810;&#38454;&#27573;&#35757;&#32451;&#26694;&#26550;&#65292;&#21033;&#29992;&#20102;&#39069;&#22806;&#30340;&#21452;&#35821;&#25991;&#26412;&#21644;&#20809;&#23398;&#23383;&#31526;&#35782;&#21035;&#25968;&#25454;&#38598;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#23558;&#22270;&#20687;&#19982;&#30456;&#20851;&#25991;&#26412;&#20851;&#32852;&#36215;&#26469;&#65292;&#25552;&#20379;&#26377;&#29992;&#30340;&#34917;&#20805;&#20449;&#24687;&#65292;&#21462;&#24471;&#20102;&#27604;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#22270;&#29255;&#32763;&#35793;&#26159;&#23558;&#22270;&#20687;&#20013;&#23884;&#20837;&#30340;&#21407;&#22987;&#25991;&#26412;&#32763;&#35793;&#25104;&#30446;&#26631;&#35821;&#35328;&#30340;&#20219;&#21153;&#12290;&#35813;&#20219;&#21153;&#26377;&#30528;&#24191;&#27867;&#30340;&#24212;&#29992;&#65292;&#24182;&#20855;&#26377;&#37325;&#35201;&#30340;&#30740;&#31350;&#20215;&#20540;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;&#25991;&#26412;&#22270;&#29255;&#32763;&#35793;&#30740;&#31350;&#38754;&#20020;&#20004;&#20010;&#20027;&#35201;&#29942;&#39048;&#65306;1&#65289;&#32570;&#23569;&#20844;&#24320;&#30340;&#25991;&#26412;&#22270;&#29255;&#32763;&#35793;&#25968;&#25454;&#38598;&#65307;2&#65289;&#20027;&#27969;&#27169;&#22411;&#37319;&#29992;&#32423;&#32852;&#27169;&#24335;&#26500;&#24314;&#65292;&#23481;&#26131;&#21463;&#21040;&#20809;&#23383;&#31526;&#35782;&#21035;&#38169;&#35823;&#20256;&#25773;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#39318;&#20808;&#27880;&#37322;&#20102;&#19968;&#20010;&#21517;&#20026;OCRMT30K&#30340;&#20013;&#33521;&#25991;&#25991;&#26412;&#22270;&#29255;&#32763;&#35793;&#25968;&#25454;&#38598;&#65292;&#20026;&#21518;&#32493;&#30740;&#31350;&#25552;&#20379;&#20102;&#20415;&#21033;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#30721;&#26412;&#30340;&#25991;&#26412;&#22270;&#29255;&#32763;&#35793;&#27169;&#22411;&#65292;&#33021;&#22815;&#23558;&#22270;&#20687;&#19982;&#30456;&#20851;&#25991;&#26412;&#20851;&#32852;&#36215;&#26469;&#65292;&#25552;&#20379;&#26377;&#29992;&#30340;&#34917;&#20805;&#20449;&#24687;&#36827;&#34892;&#32763;&#35793;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#38454;&#27573;&#35757;&#32451;&#26694;&#26550;&#65292;&#21253;&#25324;&#25991;&#26412;&#26426;&#22120;&#32763;&#35793;&#12289;&#22270;&#20687;&#25991;&#26412;&#23545;&#40784;&#21644;&#25991;&#26412;&#22270;&#29255;&#32763;&#35793;&#20219;&#21153;&#65292;&#20805;&#20998;&#21033;&#29992;&#20102;&#39069;&#22806;&#30340;&#21452;&#35821;&#25991;&#26412;&#12289;&#20809;&#23383;&#31526;&#35782;&#21035;&#25968;&#25454;&#38598;&#21644;OCRMT30K&#25968;&#25454;&#38598;&#26469;&#35757;&#32451;&#25105;&#20204;&#30340;&#27169;&#22411;&#12290;&#25193;&#23637;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#24182;&#36798;&#21040;&#20102;&#31454;&#20105;&#24615;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text image translation (TIT) aims to translate the source texts embedded in the image to target translations, which has a wide range of applications and thus has important research value. However, current studies on TIT are confronted with two main bottlenecks: 1) this task lacks a publicly available TIT dataset, 2) dominant models are constructed in a cascaded manner, which tends to suffer from the error propagation of optical character recognition (OCR). In this work, we first annotate a Chinese-English TIT dataset named OCRMT30K, providing convenience for subsequent studies. Then, we propose a TIT model with a multimodal codebook, which is able to associate the image with relevant texts, providing useful supplementary information for translation. Moreover, we present a multi-stage training framework involving text machine translation, image-text alignment, and TIT tasks, which fully exploits additional bilingual texts, OCR dataset and our OCRMT30K dataset to train our model. Extensi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#31616;&#35201;&#27010;&#36848;&#20102;&#38271;&#25991;&#26412;&#30340;&#31070;&#32463;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#29616;&#29366;&#65292;&#20027;&#35201;&#21253;&#25324;&#25991;&#26723;&#20998;&#31867;&#21644;&#25688;&#35201;&#65292;&#28085;&#30422;&#20102;&#24773;&#24863;&#20998;&#26512;&#65292;&#21516;&#26102;&#36824;&#25506;&#35752;&#20102;&#38271;&#25991;&#26412;NLP&#30340;&#20027;&#35201;&#25361;&#25112;&#12289;&#38382;&#39064;&#21644;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2305.16259</link><description>&lt;p&gt;
&#38271;&#25991;&#26412;&#30340;&#31070;&#32463;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65306;&#29616;&#29366;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Neural Natural Language Processing for Long Texts: A Survey of the State-of-the-Art. (arXiv:2305.16259v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16259
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#31616;&#35201;&#27010;&#36848;&#20102;&#38271;&#25991;&#26412;&#30340;&#31070;&#32463;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#29616;&#29366;&#65292;&#20027;&#35201;&#21253;&#25324;&#25991;&#26723;&#20998;&#31867;&#21644;&#25688;&#35201;&#65292;&#28085;&#30422;&#20102;&#24773;&#24863;&#20998;&#26512;&#65292;&#21516;&#26102;&#36824;&#25506;&#35752;&#20102;&#38271;&#25991;&#26412;NLP&#30340;&#20027;&#35201;&#25361;&#25112;&#12289;&#38382;&#39064;&#21644;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#20013;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#30340;&#37319;&#29992;&#26497;&#22823;&#22320;&#20419;&#36827;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#30340;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#38271;&#25991;&#26412;&#20998;&#26512;&#30340;&#38656;&#27714;&#19982;&#30701;&#25991;&#26412;&#26377;&#24456;&#22823;&#19981;&#21516;&#65292;&#32780;&#32593;&#32476;&#19978;&#20256;&#36755;&#30340;&#25991;&#26723;&#22823;&#23567;&#19981;&#26029;&#22686;&#21152;&#65292;&#20351;&#38271;&#25991;&#26412;&#30340;&#33258;&#21160;&#29702;&#35299;&#25104;&#20026;&#19968;&#39033;&#20851;&#38190;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#26412;&#25991;&#30340;&#20004;&#20010;&#30446;&#26631;&#26159;&#65306;a&#65289;&#27010;&#36848;&#30456;&#20851;&#30340;&#31070;&#32463;&#26500;&#24314;&#27169;&#22359;&#65292;&#20316;&#20026;&#30701;&#25945;&#31243;&#65307;b&#65289;&#24635;&#32467;&#38271;&#25991;&#26412;NLP&#30340;&#29616;&#29366;&#65292;&#20027;&#35201;&#20851;&#27880;&#20004;&#20010;&#26680;&#24515;&#20219;&#21153;&#65306;&#25991;&#26723;&#20998;&#31867;&#21644;&#25991;&#26723;&#25688;&#35201;&#12290;&#24773;&#24863;&#20998;&#26512;&#20063;&#28085;&#30422;&#22312;&#20869;&#65292;&#22240;&#20026;&#23427;&#36890;&#24120;&#34987;&#35270;&#20026;&#25991;&#26723;&#20998;&#31867;&#30340;&#29305;&#20363;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#36824;&#35752;&#35770;&#20102;&#38271;&#25991;&#26412;NLP&#30456;&#20851;&#30340;&#20027;&#35201;&#25361;&#25112;&#12289;&#38382;&#39064;&#21644;&#35299;&#20915;&#26041;&#26696;&#12290;&#26368;&#21518;&#65292;&#20171;&#32461;&#20102;&#30456;&#20851;&#30340;&#20844;&#24320;&#30340;&#27880;&#37322;&#25968;&#25454;&#38598;&#65292;&#20197;&#20415;&#20419;&#36827;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
The adoption of Deep Neural Networks (DNNs) has greatly benefited Natural Language Processing (NLP) during the past decade. However, the demands of long document analysis are quite different from those of shorter texts, while the ever increasing size of documents uploaded on-line renders automated understanding of long texts a critical area of research. This article has two goals: a) it overviews the relevant neural building blocks, thus serving as a short tutorial, and b) it surveys the state-of-the-art in long document NLP, mainly focusing on two central tasks: document classification and document summarization. Sentiment analysis for long texts is also covered, since it is typically treated as a particular case of document classification. Additionally, this article discusses the main challenges, issues and current solutions related to long document NLP. Finally, the relevant, publicly available, annotated datasets are presented, in order to facilitate further research.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;LLM&#30340;&#19981;&#30495;&#23454;&#22238;&#31572;&#29616;&#35937;&#65292;&#21457;&#29616;GPT-3&#27169;&#22411;&#23545;&#32473;&#23450;&#25552;&#31034;&#30340;&#22238;&#31572;&#22312;&#35821;&#35328;&#29305;&#24615;&#19978;&#24456;&#30456;&#20284;&#12290;&#21516;&#26102;&#65292;&#35813;&#35770;&#25991;&#35777;&#26126;&#20102;&#22312;&#27809;&#26377;&#35780;&#20272;&#20869;&#23481;&#26412;&#36523;&#30340;&#24773;&#20917;&#19979;&#65292;&#20165;&#20381;&#36182;&#27169;&#22411;&#22238;&#31572;&#30340;&#39118;&#26684;&#25104;&#20998;&#21363;&#21487;&#20998;&#31867;&#30495;&#23454;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.15875</link><description>&lt;p&gt;
&#30495;&#23454;&#22238;&#31572;&#30340;&#35821;&#35328;&#29305;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Linguistic Properties of Truthful Response. (arXiv:2305.15875v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15875
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;LLM&#30340;&#19981;&#30495;&#23454;&#22238;&#31572;&#29616;&#35937;&#65292;&#21457;&#29616;GPT-3&#27169;&#22411;&#23545;&#32473;&#23450;&#25552;&#31034;&#30340;&#22238;&#31572;&#22312;&#35821;&#35328;&#29305;&#24615;&#19978;&#24456;&#30456;&#20284;&#12290;&#21516;&#26102;&#65292;&#35813;&#35770;&#25991;&#35777;&#26126;&#20102;&#22312;&#27809;&#26377;&#35780;&#20272;&#20869;&#23481;&#26412;&#36523;&#30340;&#24773;&#20917;&#19979;&#65292;&#20165;&#20381;&#36182;&#27169;&#22411;&#22238;&#31572;&#30340;&#39118;&#26684;&#25104;&#20998;&#21363;&#21487;&#20998;&#31867;&#30495;&#23454;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20351;&#29992;220&#20010;&#25163;&#24037;&#21046;&#20316;&#30340;&#35821;&#35328;&#29305;&#24615;&#23545;LLM&#19981;&#30495;&#23454;&#22238;&#31572;&#30340;&#29616;&#35937;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;GPT-3&#27169;&#22411;&#65292;&#24182;&#21457;&#29616;&#19981;&#21516;&#22823;&#23567;&#30340;LLM&#23545;&#32473;&#23450;&#25552;&#31034;&#30340;&#22238;&#31572;&#22312;&#35821;&#35328;&#29305;&#24615;&#19978;&#24456;&#30456;&#20284;&#12290;&#25105;&#20204;&#36890;&#36807;&#35757;&#32451;&#21482;&#20381;&#36182;&#27169;&#22411;&#22238;&#31572;&#30340;&#39118;&#26684;&#25104;&#20998;&#26469;&#20998;&#31867;&#38472;&#36848;&#30495;&#23454;&#24615;&#30340;&#25903;&#25345;&#21521;&#37327;&#26426;&#25193;&#23637;&#20102;&#36825;&#19968;&#21457;&#29616;&#12290;&#34429;&#28982;&#25968;&#25454;&#38598;&#22823;&#23567;&#38480;&#21046;&#20102;&#25105;&#20204;&#30340;&#24403;&#21069;&#30740;&#31350;&#25104;&#26524;&#65292;&#20294;&#25105;&#20204;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#35777;&#25454;&#65292;&#34920;&#26126;&#21487;&#20197;&#22312;&#19981;&#35780;&#20272;&#20869;&#23481;&#26412;&#36523;&#30340;&#24773;&#20917;&#19979;&#26816;&#27979;&#30495;&#23454;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the phenomenon of an LLM's untruthful response using a large set of 220 handcrafted linguistic features. We focus on GPT-3 models and find that the linguistic profiles of responses are similar across model sizes. That is, how varying-sized LLMs respond to given prompts stays similar on the linguistic properties level. We expand upon this finding by training support vector machines that rely only upon the stylistic components of model responses to classify the truthfulness of statements. Though the dataset size limits our current findings, we present promising evidence that truthfulness detection is possible without evaluating the content itself.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#27169;&#24335;&#32852;&#37030;&#20154;&#31867;&#27963;&#21160;&#35782;&#21035;&#20013;&#30340;&#38544;&#31169;&#38382;&#39064;&#12290;&#36890;&#36807;&#19968;&#20010;&#29305;&#23450;&#30340;&#31995;&#32479;&#65292;&#32852;&#37030;&#23398;&#20064;&#21487;&#20197;&#25552;&#20379;&#26356;&#22909;&#30340;&#38544;&#31169;&#20445;&#25252;&#65292;&#21516;&#26102;&#19981;&#20250;&#25439;&#22833;&#20154;&#31867;&#27963;&#21160;&#35782;&#21035;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.12134</link><description>&lt;p&gt;
&#22810;&#27169;&#24335;&#32852;&#37030;&#20154;&#31867;&#27963;&#21160;&#35782;&#21035;&#20013;&#30340;&#38544;&#31169;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Privacy in Multimodal Federated Human Activity Recognition. (arXiv:2305.12134v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12134
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#27169;&#24335;&#32852;&#37030;&#20154;&#31867;&#27963;&#21160;&#35782;&#21035;&#20013;&#30340;&#38544;&#31169;&#38382;&#39064;&#12290;&#36890;&#36807;&#19968;&#20010;&#29305;&#23450;&#30340;&#31995;&#32479;&#65292;&#32852;&#37030;&#23398;&#20064;&#21487;&#20197;&#25552;&#20379;&#26356;&#22909;&#30340;&#38544;&#31169;&#20445;&#25252;&#65292;&#21516;&#26102;&#19981;&#20250;&#25439;&#22833;&#20154;&#31867;&#27963;&#21160;&#35782;&#21035;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#27963;&#21160;&#35782;&#21035;&#65288;HAR&#65289;&#30340;&#35757;&#32451;&#25968;&#25454;&#24448;&#24448;&#21253;&#21547;&#38544;&#31169;&#20449;&#24687;&#25110;&#30001;&#19981;&#21512;&#20316;&#23454;&#20307;&#25345;&#26377;&#12290;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#36890;&#36807;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#29992;&#25143;&#12289;&#29615;&#22659;&#21644;&#20256;&#24863;&#22120;&#32423;&#21035;&#19978;&#38544;&#31169;&#23545;&#32852;&#37030;HAR&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;FL&#23545;HAR&#30340;&#24615;&#33021;&#21462;&#20915;&#20110;FL&#31995;&#32479;&#30340;&#38544;&#31169;&#20445;&#25252;&#31243;&#24230;&#65292;&#24182;&#19988;&#20027;&#35201;&#21462;&#20915;&#20110;&#26469;&#33258;&#19981;&#21516;&#20256;&#24863;&#22120;&#30340;&#25968;&#25454;&#30340;&#37197;&#32622;&#12290;&#23613;&#31649;&#36991;&#20813;&#25968;&#25454;&#20849;&#20139;&#24182;&#22312;&#20154;&#31867;&#25110;&#29615;&#22659;&#32423;&#21035;&#19978;&#20551;&#35774;&#38544;&#31169;&#65292;&#22914;&#20043;&#21069;&#30340;&#24037;&#20316;&#25152;&#20570;&#30340;&#37027;&#26679;&#65292;&#31934;&#24230;&#20250;&#38477;&#20302;5-7&#65285;&#12290;&#28982;&#32780;&#65292;&#23558;&#36825;&#31181;&#38544;&#31169;&#24310;&#20280;&#21040;&#27169;&#24577;&#32423;&#21035;&#24182;&#20005;&#26684;&#20998;&#31163;&#22810;&#20010;&#23458;&#25143;&#31471;&#20043;&#38388;&#30340;&#20256;&#24863;&#22120;&#25968;&#25454;&#21487;&#33021;&#20250;&#23548;&#33268;&#31934;&#24230;&#38477;&#20302;19-42&#65285;&#12290;&#30001;&#20110;&#36825;&#31181;&#24418;&#24335;&#30340;&#38544;&#31169;&#26159;HAR&#20013;&#34987;&#35201;&#27714;&#30340;&#36947;&#24503;&#21033;&#29992;&#34987;&#21160;&#20256;&#24863;&#26041;&#27861;&#25152;&#24517;&#38656;&#30340;&#65292;&#22240;&#27492;&#25105;&#20204;&#23454;&#29616;&#20102;&#19968;&#31181;&#31995;&#32479;&#65292;&#22312;&#35813;&#31995;&#32479;&#20013;&#23458;&#25143;&#31471;&#30456;&#20114;&#35757;&#32451;&#19968;&#20010;&#36890;&#29992;&#30340;FL&#27169;&#22411;&#21644;&#19968;&#20010;&#27599;&#31181;&#27169;&#24577;&#19968;&#20010;&#30340;&#32452;&#32423;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#22312;&#19981;&#29306;&#29298;HAR&#20934;&#30830;&#24615;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#38544;&#31169;&#20445;&#25252;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human Activity Recognition (HAR) training data is often privacy-sensitive or held by non-cooperative entities. Federated Learning (FL) addresses such concerns by training ML models on edge clients. This work studies the impact of privacy in federated HAR at a user, environment, and sensor level. We show that the performance of FL for HAR depends on the assumed privacy level of the FL system and primarily upon the colocation of data from different sensors. By avoiding data sharing and assuming privacy at the human or environment level, as prior works have done, the accuracy decreases by 5-7%. However, extending this to the modality level and strictly separating sensor data between multiple clients may decrease the accuracy by 19-42%. As this form of privacy is necessary for the ethical utilisation of passive sensing methods in HAR, we implement a system where clients mutually train both a general FL model and a group-level one per modality. Our evaluation shows that this method leads to
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#26368;&#36817;&#25991;&#29486;&#20013;&#30340;&#21487;&#35270;&#21270;&#38382;&#31572;&#38382;&#39064;&#65292;&#25552;&#20379;&#20102;&#23545;&#35813;&#39046;&#22495;&#30340;&#28145;&#20837;&#20998;&#26512;&#21644;&#27604;&#36739;&#65292;&#21253;&#25324;&#32467;&#26524;&#12289;&#26368;&#26032;&#25216;&#26415;&#12289;&#24120;&#35265;&#38169;&#35823;&#20197;&#21450;&#26410;&#26469;&#30740;&#31350;&#30340;&#28508;&#22312;&#25913;&#36827;&#28857;&#12290;</title><link>http://arxiv.org/abs/2305.11033</link><description>&lt;p&gt;
&#21487;&#35270;&#21270;&#38382;&#31572;&#65306;&#26368;&#36817;&#25991;&#29486;&#20013;&#25216;&#26415;&#21644;&#24120;&#35265;&#36235;&#21183;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Visual Question Answering: A Survey on Techniques and Common Trends in Recent Literature. (arXiv:2305.11033v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11033
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#26368;&#36817;&#25991;&#29486;&#20013;&#30340;&#21487;&#35270;&#21270;&#38382;&#31572;&#38382;&#39064;&#65292;&#25552;&#20379;&#20102;&#23545;&#35813;&#39046;&#22495;&#30340;&#28145;&#20837;&#20998;&#26512;&#21644;&#27604;&#36739;&#65292;&#21253;&#25324;&#32467;&#26524;&#12289;&#26368;&#26032;&#25216;&#26415;&#12289;&#24120;&#35265;&#38169;&#35823;&#20197;&#21450;&#26410;&#26469;&#30740;&#31350;&#30340;&#28508;&#22312;&#25913;&#36827;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35270;&#21270;&#38382;&#31572;&#65288;VQA&#65289;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#22270;&#20687;&#39044;&#27979;&#20013;&#30340;&#19968;&#20010;&#26032;&#20852;&#38382;&#39064;&#65292;&#38656;&#35201;&#31639;&#27861;&#22238;&#31572;&#26377;&#20851;&#29305;&#23450;&#22270;&#20687;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#20316;&#32773;&#20998;&#26512;&#20102;25&#39033;&#26368;&#26032;&#30740;&#31350;&#21644;6&#20010;&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20379;&#20102;&#19979;&#36733;&#38142;&#25509;&#12290;&#20316;&#32773;&#28145;&#20837;&#35843;&#30740;&#20102;&#35813;&#39046;&#22495;&#30340;&#22810;&#39033;&#30740;&#31350;&#65292;&#24182;&#25552;&#20379;&#20102;&#20998;&#26512;&#27604;&#36739;&#65292;&#21253;&#25324;&#32467;&#26524;&#12289;&#26368;&#26032;&#25216;&#26415;&#12289;&#24120;&#35265;&#38169;&#35823;&#20197;&#21450;&#26410;&#26469;&#30740;&#31350;&#30340;&#28508;&#22312;&#25913;&#36827;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visual Question Answering (VQA) is an emerging area of interest for researches, being a recent problem in natural language processing and image prediction. In this area, an algorithm needs to answer questions about certain images. As of the writing of this survey, 25 recent studies were analyzed. Besides, 6 datasets were analyzed and provided their link to download. In this work, several recent pieces of research in this area were investigated and a deeper analysis and comparison among them were provided, including results, the state-of-the-art, common errors, and possible points of improvement for future researchers.
&lt;/p&gt;</description></item><item><title>&#38646;&#26679;&#26412;&#22810;&#35821;&#35328;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#23481;&#26131;&#20986;&#29616;&#8220;&#31163;&#35889;&#38382;&#39064;&#8221;&#65292;&#26412;&#25991;&#25552;&#20986;&#30340;&#31616;&#21333;&#19988;&#26377;&#25928;&#30340;&#31639;&#27861;LAVS&#21487;&#20197;&#36890;&#36807;&#22686;&#21152;&#35821;&#35328;&#20043;&#38388;&#30340;KL&#20998;&#27495;&#26174;&#33879;&#38477;&#20302;&#36825;&#20010;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.10930</link><description>&lt;p&gt;
&#38646;&#26679;&#26412;&#22810;&#35821;&#35328;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#8220;&#31163;&#35889;&#38382;&#39064;&#8221;
&lt;/p&gt;
&lt;p&gt;
On the Off-Target Problem of Zero-Shot Multilingual Neural Machine Translation. (arXiv:2305.10930v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10930
&lt;/p&gt;
&lt;p&gt;
&#38646;&#26679;&#26412;&#22810;&#35821;&#35328;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#23481;&#26131;&#20986;&#29616;&#8220;&#31163;&#35889;&#38382;&#39064;&#8221;&#65292;&#26412;&#25991;&#25552;&#20986;&#30340;&#31616;&#21333;&#19988;&#26377;&#25928;&#30340;&#31639;&#27861;LAVS&#21487;&#20197;&#36890;&#36807;&#22686;&#21152;&#35821;&#35328;&#20043;&#38388;&#30340;KL&#20998;&#27495;&#26174;&#33879;&#38477;&#20302;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22810;&#35821;&#35328;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#20294;&#23427;&#20173;&#28982;&#23384;&#22312;&#8220;&#31163;&#35889;&#38382;&#39064;&#8221;&#65292;&#21363;&#23558;&#32763;&#35793;&#36755;&#20986;&#21040;&#38169;&#35823;&#30340;&#35821;&#35328;&#20013;&#12290;&#36825;&#20010;&#38382;&#39064;&#22312;&#38646;&#26679;&#26412;&#32763;&#35793;&#20219;&#21153;&#20013;&#26356;&#21152;&#26126;&#26174;&#12290;&#26412;&#25991;&#21457;&#29616;&#65292;&#24403;&#32534;&#30721;&#30446;&#26631;&#35821;&#35328;&#20449;&#21495;&#26102;&#22833;&#25928;&#65292;&#20250;&#23548;&#33268;&#31163;&#35889;&#38382;&#39064;&#65292;&#24182;&#19988;&#20004;&#31181;&#35821;&#35328;&#35789;&#27719;&#20043;&#38388;&#26356;&#25509;&#36817;&#30340;&#35789;&#27719;&#36317;&#31163;&#65288;&#21363;KL&#20998;&#27495;&#65289;&#19982;&#26356;&#39640;&#30340;&#31163;&#35889;&#29575;&#26377;&#20851;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#36824;&#21457;&#29616;&#65292;&#20165;&#38548;&#31163;&#35299;&#30721;&#22120;&#20013;&#19981;&#21516;&#35821;&#35328;&#30340;&#35789;&#27719;&#21487;&#20197;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#12290;&#22522;&#20110;&#36825;&#20123;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#26377;&#25928;&#30340;&#31639;&#27861;Language Aware Vocabulary Sharing (LAVS)&#26469;&#26500;&#24314;&#22810;&#35821;&#35328;&#35789;&#27719;&#34920;&#65292;&#36890;&#36807;&#22686;&#21152;&#35821;&#35328;&#20043;&#38388;&#30340;KL&#20998;&#27495;&#65292;&#22823;&#22823;&#20943;&#36731;&#20102;&#32763;&#35793;&#27169;&#22411;&#30340;&#31163;&#35889;&#38382;&#39064;&#12290;&#25105;&#20204;&#22312;11&#31181;&#35821;&#35328;&#30340;&#22810;&#35821;&#35328;&#32763;&#35793;&#22522;&#20934;&#27979;&#35797;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#23545;&#20110;90&#20010;&#32763;&#35793;&#20219;&#21153;&#65292;&#37319;&#29992;LAVS&#30340;&#31163;&#35889;&#29575;&#38477;&#20302;&#20102;37&#65285;&#33267;90&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
While multilingual neural machine translation has achieved great success, it suffers from the off-target issue, where the translation is in the wrong language. This problem is more pronounced on zero-shot translation tasks. In this work, we find that failing in encoding discriminative target language signal will lead to off-target and a closer lexical distance (i.e., KL-divergence) between two languages' vocabularies is related with a higher off-target rate. We also find that solely isolating the vocab of different languages in the decoder can alleviate the problem. Motivated by the findings, we propose Language Aware Vocabulary Sharing (LAVS), a simple and effective algorithm to construct the multilingual vocabulary, that greatly alleviates the off-target problem of the translation model by increasing the KL-divergence between languages. We conduct experiments on a multilingual machine translation benchmark in 11 languages. Experiments show that the off-target rate for 90 translation 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#20197;TGNB&#20154;&#32676;&#30340;&#22768;&#38899;&#20026;&#20013;&#24515;&#65292;&#35780;&#20272;&#24320;&#25918;&#24335;&#35821;&#35328;&#29983;&#25104;&#20013;&#30340;&#20559;&#35265;&#12290;&#36890;&#36807;&#29702;&#35299;TGNB&#20010;&#20307;&#30340;&#32463;&#21382;&#65292;&#25552;&#20986;&#20102;&#20197;TGNB&#20154;&#32676;&#20026;&#20013;&#24515;&#30340;OLG&#31995;&#32479;&#35780;&#20272;&#26694;&#26550;&#65292;&#24182;&#19988;&#21253;&#25324;&#19968;&#20010;&#20026;TGNB&#20154;&#32676;&#35774;&#35745;&#30340;&#35843;&#26597;&#24037;&#20855;&#21644;&#20998;&#26512;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.09941</link><description>&lt;p&gt;
&#8220;&#25105;&#20840;&#28982;&#25104;&#20026;&#25105;&#33258;&#24049;&#8221;&#65306;&#20197;TGNB&#20154;&#32676;&#20026;&#20013;&#24515;&#65292;&#35780;&#20272;&#24320;&#25918;&#24335;&#35821;&#35328;&#29983;&#25104;&#20013;&#30340;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
"I'm fully who I am": Towards Centering Transgender and Non-Binary Voices to Measure Biases in Open Language Generation. (arXiv:2305.09941v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09941
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#20197;TGNB&#20154;&#32676;&#30340;&#22768;&#38899;&#20026;&#20013;&#24515;&#65292;&#35780;&#20272;&#24320;&#25918;&#24335;&#35821;&#35328;&#29983;&#25104;&#20013;&#30340;&#20559;&#35265;&#12290;&#36890;&#36807;&#29702;&#35299;TGNB&#20010;&#20307;&#30340;&#32463;&#21382;&#65292;&#25552;&#20986;&#20102;&#20197;TGNB&#20154;&#32676;&#20026;&#20013;&#24515;&#30340;OLG&#31995;&#32479;&#35780;&#20272;&#26694;&#26550;&#65292;&#24182;&#19988;&#21253;&#25324;&#19968;&#20010;&#20026;TGNB&#20154;&#32676;&#35774;&#35745;&#30340;&#35843;&#26597;&#24037;&#20855;&#21644;&#20998;&#26512;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36328;&#24615;&#21035;&#21644;&#38750;&#20108;&#20803;&#65288;TGNB&#65289;&#20154;&#32676;&#22312;&#26085;&#24120;&#29983;&#27963;&#20013;&#32463;&#21382;&#20102;&#19981;&#25104;&#27604;&#20363;&#30340;&#27495;&#35270;&#21644;&#25490;&#26021;&#12290;&#38543;&#30528;&#35821;&#35328;&#29983;&#25104;&#25216;&#26415;&#30340;&#26085;&#30410;&#26222;&#21450;&#21644;&#24212;&#29992;&#65292;&#36827;&#19968;&#27493;&#36793;&#32536;&#21270;&#36825;&#19968;&#20154;&#32676;&#30340;&#21487;&#33021;&#24615;&#20063;&#22312;&#22686;&#21152;&#12290;&#34429;&#28982;&#22823;&#37327;&#30340;NLP&#20844;&#24179;&#25991;&#29486;&#30528;&#37325;&#20110;&#38416;&#26126;&#21644;&#35299;&#20915;&#24615;&#21035;&#20559;&#35265;&#65292;&#20294;&#35780;&#20272;TGNB&#36523;&#20221;&#25152;&#24102;&#26469;&#30340;&#24615;&#21035;&#20260;&#23475;&#38656;&#35201;&#29702;&#35299;&#36825;&#20123;&#36523;&#20221;&#22914;&#20309;&#29420;&#29305;&#22320;&#19982;&#31038;&#20250;&#24615;&#21035;&#35268;&#33539;&#20114;&#21160;&#20197;&#21450;&#19982;&#24615;&#21035;&#20108;&#20803;&#20013;&#24515;&#30340;&#35270;&#35282;&#30456;&#21306;&#20998;&#12290;&#36825;&#26679;&#30340;&#27979;&#37327;&#26694;&#26550;&#26412;&#36136;&#19978;&#38656;&#35201;&#20197;TGNB&#22768;&#38899;&#20026;&#20013;&#24515;&#65292;&#24110;&#21161;&#25351;&#23548;&#21253;&#23481;&#24615;&#21035;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24212;&#35813;&#20026;&#35841;&#26381;&#21153;&#12290;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#20197;TGNB&#31038;&#21306;&#21644;&#29616;&#26377;&#30340;&#36328;&#23398;&#31185;&#25991;&#29486;&#20026;&#22522;&#30784;&#65292;&#35780;&#20272;&#20102;TGNB&#20010;&#20307;&#32463;&#21382;&#36793;&#32536;&#21270;&#25152;&#24418;&#25104;&#30340;&#31038;&#20250;&#29616;&#23454;&#26159;&#22914;&#20309;&#24433;&#21709;&#21644;&#23384;&#22312;&#20110;&#24320;&#25918;&#24335;&#35821;&#35328;&#29983;&#25104;&#65288;OLG&#65289;&#20013;&#12290;&#39318;&#20808;&#29702;&#35299;TGNB&#20010;&#20307;&#30340;&#32463;&#21382;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#35780;&#20272;OLG&#31995;&#32479;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#20197;TGNB&#20154;&#32676;&#20026;&#20013;&#24515;&#65292;&#24230;&#37327;&#19982;&#35813;&#20154;&#32676;&#30456;&#20851;&#30340;&#20559;&#35265;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21253;&#25324;&#29305;&#21035;&#20026;TGNB&#20154;&#32676;&#35774;&#35745;&#30340;&#35843;&#26597;&#24037;&#20855;&#65292;&#20197;&#21450;&#20132;&#21449;&#20998;&#26512;&#32467;&#26524;&#30340;&#20132;&#21449;&#26041;&#27861;&#12290;&#25105;&#20204;&#30456;&#20449;&#65292;&#36825;&#39033;&#24037;&#20316;&#23558;&#26377;&#21161;&#20110;&#23454;&#29616;&#26356;&#20844;&#24179;&#12289;&#26356;&#21253;&#23481;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31038;&#21306;&#65292;&#24182;&#28508;&#22312;&#22320;&#35299;&#20915;NLP&#30740;&#31350;&#20013;&#24191;&#27867;&#30340;&#20132;&#21449;&#36523;&#20221;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transgender and non-binary (TGNB) individuals disproportionately experience discrimination and exclusion from daily life. Given the recent popularity and adoption of language generation technologies, the potential to further marginalize this population only grows. Although a multitude of NLP fairness literature focuses on illuminating and addressing gender biases, assessing gender harms for TGNB identities requires understanding how such identities uniquely interact with societal gender norms and how they differ from gender binary-centric perspectives. Such measurement frameworks inherently require centering TGNB voices to help guide the alignment between gender-inclusive NLP and whom they are intended to serve. Towards this goal, we ground our work in the TGNB community and existing interdisciplinary literature to assess how the social reality surrounding experienced marginalization by TGNB persons contributes to and persists within Open Language Generation (OLG). By first understandi
&lt;/p&gt;</description></item><item><title>&#37329;&#34701;&#39044;&#27979;&#20013;&#27169;&#22411;&#30340;&#19968;&#33268;&#24615;&#23545;&#20110;&#24314;&#31435;&#29992;&#25143;&#20449;&#20219;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#30446;&#21069;&#37329;&#34701;&#39044;&#27979;&#26041;&#27861;&#24456;&#23569;&#32771;&#34385;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#20272;&#37329;&#34701;&#25991;&#26412;&#36923;&#36753;&#19968;&#33268;&#24615;&#30340;&#35780;&#20272;&#24037;&#20855;FinTrust&#65292;&#24182;&#20351;&#29992;&#23427;&#21457;&#29616;&#26368;&#20808;&#36827;&#30340;&#37329;&#34701;&#39044;&#27979;NLP&#27169;&#22411;&#30340;&#19968;&#33268;&#24615;&#36739;&#24046;&#12290;</title><link>http://arxiv.org/abs/2305.08524</link><description>&lt;p&gt;
&#22522;&#20110;&#25991;&#26412;&#30340;&#37329;&#34701;&#39044;&#27979;&#27169;&#22411;&#30340;&#19968;&#33268;&#24615;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Measuring Consistency in Text-based Financial Forecasting Models. (arXiv:2305.08524v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08524
&lt;/p&gt;
&lt;p&gt;
&#37329;&#34701;&#39044;&#27979;&#20013;&#27169;&#22411;&#30340;&#19968;&#33268;&#24615;&#23545;&#20110;&#24314;&#31435;&#29992;&#25143;&#20449;&#20219;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#30446;&#21069;&#37329;&#34701;&#39044;&#27979;&#26041;&#27861;&#24456;&#23569;&#32771;&#34385;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#20272;&#37329;&#34701;&#25991;&#26412;&#36923;&#36753;&#19968;&#33268;&#24615;&#30340;&#35780;&#20272;&#24037;&#20855;FinTrust&#65292;&#24182;&#20351;&#29992;&#23427;&#21457;&#29616;&#26368;&#20808;&#36827;&#30340;&#37329;&#34701;&#39044;&#27979;NLP&#27169;&#22411;&#30340;&#19968;&#33268;&#24615;&#36739;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37329;&#34701;&#39044;&#27979;&#26159;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#20013;&#19968;&#20010;&#37325;&#35201;&#19988;&#27963;&#36291;&#30340;&#39046;&#22495;&#65292;&#22240;&#20026;&#21363;&#20351;&#26159;&#26368;&#23567;&#30340;&#39044;&#27979;&#20934;&#30830;&#29575;&#25552;&#39640;&#20063;&#21487;&#20197;&#36716;&#21270;&#20026;&#24040;&#22823;&#30340;&#36130;&#21153;&#25910;&#30410;&#12290;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#38754;&#30340;&#26368;&#26032;&#36827;&#23637;&#24102;&#26469;&#20102;&#21033;&#29992;&#25991;&#26412;&#25968;&#25454;&#65288;&#22914;&#19978;&#24066;&#20844;&#21496;&#30340;&#30408;&#21033;&#25253;&#21578;&#65289;&#26469;&#39044;&#27979;&#36164;&#20135;&#25910;&#30410;&#29575;&#30340;&#26426;&#20250;&#12290;&#28982;&#32780;&#65292;&#22312;&#22788;&#29702;&#36825;&#31181;&#25935;&#24863;&#20219;&#21153;&#26102;&#65292;&#27169;&#22411;&#30340;&#19968;&#33268;&#24615;&#8212;&#8212;&#21363;&#22312;&#36755;&#20837;&#30340;&#20445;&#30041;&#24847;&#20041;&#25913;&#21464;&#26102;&#23545;&#27169;&#22411;&#30340;&#24433;&#21709;&#26159;&#19968;&#20010;&#24314;&#31435;&#29992;&#25143;&#20449;&#20219;&#30340;&#20851;&#38190;&#23646;&#24615;&#65292;&#20294;&#24403;&#21069;&#30340;&#37329;&#34701;&#39044;&#27979;&#26041;&#27861;&#21364;&#27809;&#26377;&#32771;&#34385;&#21040;&#36825;&#19968;&#28857;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FinTrust&#65292;&#19968;&#31181;&#35780;&#20272;&#37329;&#34701;&#25991;&#26412;&#20013;&#36923;&#36753;&#19968;&#33268;&#24615;&#30340;&#35780;&#20272;&#24037;&#20855;&#12290;&#20351;&#29992;FinTrust&#65292;&#25105;&#20204;&#21457;&#29616;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#37329;&#34701;&#39044;&#27979;NLP&#27169;&#22411;&#30340;&#19968;&#33268;&#24615;&#36739;&#24046;&#12290;&#25105;&#20204;&#23545;&#20445;&#30041;&#24847;&#20041;&#25913;&#21464;&#24341;&#36215;&#30340;&#24615;&#33021;&#38477;&#32423;&#36827;&#34892;&#20998;&#26512;&#65292;&#32467;&#26524;&#34920;&#26126;...
&lt;/p&gt;
&lt;p&gt;
Financial forecasting has been an important and active area of machine learning research, as even the most modest advantage in predictive accuracy can be parlayed into significant financial gains. Recent advances in natural language processing (NLP) bring the opportunity to leverage textual data, such as earnings reports of publicly traded companies, to predict the return rate for an asset. However, when dealing with such a sensitive task, the consistency of models -- their invariance under meaning-preserving alternations in input -is a crucial property for building user trust. Despite this, current financial forecasting methods do not consider consistency. To address this problem, we propose FinTrust, an evaluation tool that assesses logical consistency in financial text. Using FinTrust, we show that the consistency of state-of-the-art NLP models for financial forecasting is poor. Our analysis of the performance degradation caused by meaning-preserving alternations suggests that cur
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#30495;&#23454;&#19990;&#30028;&#35013;&#31665;&#38382;&#39064;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#23454;&#20363;&#29983;&#25104;&#22120;&#65292;&#21487;&#20197;&#29992;&#26469;&#27604;&#36739;&#19981;&#21516;&#35013;&#31665;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.14712</link><description>&lt;p&gt;
&#30495;&#23454;&#19990;&#30028;&#19977;&#32500;&#35013;&#31665;&#38382;&#39064;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#23454;&#20363;&#29983;&#25104;&#22120;
&lt;/p&gt;
&lt;p&gt;
Benchmark dataset and instance generator for Real-World Three-Dimensional Bin Packing Problems. (arXiv:2304.14712v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14712
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#30495;&#23454;&#19990;&#30028;&#35013;&#31665;&#38382;&#39064;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#23454;&#20363;&#29983;&#25104;&#22120;&#65292;&#21487;&#20197;&#29992;&#26469;&#27604;&#36739;&#19981;&#21516;&#35013;&#31665;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#30495;&#23454;&#19990;&#30028;&#35013;&#31665;&#38382;&#39064;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;&#35813;&#25968;&#25454;&#38598;&#30001;12&#20010;&#23454;&#20363;&#32452;&#25104;&#65292;&#28085;&#30422;&#20102;&#19981;&#21516;&#22823;&#23567;&#21644;&#29992;&#25143;&#38656;&#27714;&#30340;&#38382;&#39064;&#22797;&#26434;&#24230;&#27700;&#24179;&#65288;&#21253;&#21547;&#20174;38&#21040;53&#20010;&#21253;&#35065;&#30340;&#25968;&#37327;&#65289;&#12290;&#23454;&#38469;&#19978;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#20960;&#20010;&#38754;&#21521;&#30495;&#23454;&#19990;&#30028;&#30340;&#38480;&#21046;&#26465;&#20214;&#26469;&#26500;&#24314;&#36825;&#20123;&#23454;&#20363;&#65306;i)&#29289;&#21697;&#21644;&#31665;&#23376;&#23610;&#23544;&#65292;ii)&#37325;&#37327;&#38480;&#21046;&#65292;iii)&#21253;&#31867;&#21035;&#20043;&#38388;&#30340;&#20146;&#21644;&#24615;&#65292;iv)&#21253;&#35013;&#39034;&#24207;&#30340;&#20559;&#22909;&#21644;v)&#36127;&#36733;&#24179;&#34913;&#12290;&#38500;&#20102;&#25968;&#25454;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#33258;&#20027;&#24320;&#21457;&#30340;Python&#33050;&#26412;&#29992;&#20110;&#25968;&#25454;&#38598;&#29983;&#25104;&#65292;&#31216;&#20026;Q4RealBPP-DataGen&#12290;&#35813;&#22522;&#20934;&#39318;&#20808;&#34987;&#35774;&#35745;&#29992;&#20110;&#35780;&#20272;&#37327;&#23376;&#27714;&#35299;&#22120;&#65292;&#22240;&#27492;&#36825;&#32452;&#23454;&#20363;&#30340;&#29305;&#24449;&#26159;&#25353;&#29031;&#37327;&#23376;&#35774;&#22791;&#30340;&#24403;&#21069;&#38480;&#21046;&#35774;&#35745;&#30340;&#12290;&#27492;&#22806;&#65292;&#25968;&#25454;&#38598;&#29983;&#25104;&#22120;&#21253;&#21547;&#22312;&#20869;&#65292;&#20801;&#35768;&#26500;&#24314;&#36890;&#29992;&#22522;&#20934;&#12290;&#26412;&#25991;&#20171;&#32461;&#30340;&#25968;&#25454;&#25552;&#20379;&#20102;&#19968;&#20010;&#22522;&#20934;&#65292;&#21487;&#20197;&#29992;&#26469;&#27604;&#36739;&#19981;&#21516;&#26041;&#27861;&#30340;&#24615;&#33021;&#21644;&#23545;&#27604;&#31639;&#27861;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, a benchmark for real-world bin packing problems is proposed. This dataset is composed of 12 instances comprehending different levels of problem complexity regarding size (with the number of packages ranging from 38 to 53) and user-defined requirements. In fact, several real-world oriented restrictions have been considered for building these instances: i) items and bins dimensions, ii) weight restrictions, iii) affinities among packages categories iv) preferences for package ordering and v) load balancing. Besides the data, we also provide an own-developed Python script for the dataset generation, coined as Q4RealBPP-DataGen. The benchmark was firstly proposed to evaluate quantum solvers, therefore the characteristic of this set of instances were designed according to the current limitations of quantum devices. Additionally, the dataset generator is included to allow the construction of general-purpose benchmarks. The data introduced on this paper provides a baseline that
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;VGOS&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#24555;&#36895;&#20174;&#31232;&#30095;&#36755;&#20837;&#65288;3-10&#20010;&#35270;&#22270;&#65289;&#20013;&#37325;&#24314;&#25918;&#23556;&#22330;&#65292;&#20197;&#35299;&#20915;&#20307;&#32032;&#32593;&#26684;&#26356;&#23481;&#26131;&#36807;&#24230;&#25311;&#21512;&#35757;&#32451;&#35270;&#22270;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#22686;&#37327;&#20307;&#32032;&#35757;&#32451;&#31574;&#30053;&#26469;&#36991;&#20813;&#36807;&#24230;&#25311;&#21512;&#12290;</title><link>http://arxiv.org/abs/2304.13386</link><description>&lt;p&gt;
VGOS&#65306;&#26469;&#33258;&#31232;&#30095;&#36755;&#20837;&#30340;&#20307;&#32032;&#32593;&#26684;&#20248;&#21270;&#35270;&#35282;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
VGOS: Voxel Grid Optimization for View Synthesis from Sparse Inputs. (arXiv:2304.13386v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13386
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;VGOS&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#24555;&#36895;&#20174;&#31232;&#30095;&#36755;&#20837;&#65288;3-10&#20010;&#35270;&#22270;&#65289;&#20013;&#37325;&#24314;&#25918;&#23556;&#22330;&#65292;&#20197;&#35299;&#20915;&#20307;&#32032;&#32593;&#26684;&#26356;&#23481;&#26131;&#36807;&#24230;&#25311;&#21512;&#35757;&#32451;&#35270;&#22270;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#22686;&#37327;&#20307;&#32032;&#35757;&#32451;&#31574;&#30053;&#26469;&#36991;&#20813;&#36807;&#24230;&#25311;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#25918;&#23556;&#22330;&#65288;NeRF&#65289;&#30001;&#20110;&#20854;&#20248;&#24322;&#30340;&#36136;&#37327;&#21644;&#28789;&#27963;&#24615;&#22312;&#26032;&#39062;&#35270;&#35282;&#21512;&#25104;&#20013;&#21462;&#24471;&#20102;&#24456;&#22823;&#30340;&#25104;&#21151;&#12290;&#20294;&#26159;&#65292;NeRF&#38656;&#35201;&#23494;&#38598;&#30340;&#36755;&#20837;&#35270;&#22270;&#65288;&#20960;&#21313;&#21040;&#20960;&#30334;&#20010;&#65289;&#21644;&#38271;&#26102;&#38388;&#30340;&#35757;&#32451;&#26102;&#38388;&#65288;&#20960;&#23567;&#26102;&#21040;&#20960;&#22825;&#65289;&#25165;&#33021;&#20026;&#21333;&#20010;&#22330;&#26223;&#29983;&#25104;&#39640;&#20445;&#30495;&#24230;&#30340;&#22270;&#20687;&#12290;&#34429;&#28982;&#20351;&#29992;&#20307;&#32032;&#32593;&#26684;&#34920;&#31034;&#25918;&#23556;&#22330;&#21487;&#20197;&#26174;&#33879;&#21152;&#36895;&#20248;&#21270;&#36807;&#31243;&#65292;&#20294;&#25105;&#20204;&#27880;&#24847;&#21040;&#65292;&#22312;&#31232;&#30095;&#36755;&#20837;&#30340;&#24773;&#20917;&#19979;&#65292;&#20307;&#32032;&#32593;&#26684;&#26356;&#23481;&#26131;&#36807;&#24230;&#25311;&#21512;&#35757;&#32451;&#35270;&#22270;&#65292;&#20250;&#20135;&#29983;&#31354;&#27934;&#21644;&#28418;&#28014;&#29289;&#65292;&#23548;&#33268;&#20266;&#24433;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;VGOS&#65292;&#19968;&#31181;&#20174;&#31232;&#30095;&#36755;&#20837;&#65288;3-10&#20010;&#35270;&#22270;&#65289;&#24555;&#36895;&#65288;3-5&#20998;&#38047;&#65289;&#37325;&#24314;&#25918;&#23556;&#22330;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#20026;&#20102;&#25913;&#21892;&#22522;&#20110;&#20307;&#32032;&#30340;&#25918;&#23556;&#22330;&#22312;&#31232;&#30095;&#36755;&#20837;&#22330;&#26223;&#20013;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26041;&#27861;&#65306;&#65288;a&#65289;&#25105;&#20204;&#24341;&#20837;&#20102;&#22686;&#37327;&#20307;&#32032;&#35757;&#32451;&#31574;&#30053;&#65292;&#22312;&#37325;&#24314;&#26089;&#26399;&#25233;&#21046;&#21608;&#36793;&#20307;&#32032;&#30340;&#20248;&#21270;&#65292;&#20197;&#38450;&#27490;&#36807;&#24230;&#25311;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural Radiance Fields (NeRF) has shown great success in novel view synthesis due to its state-of-the-art quality and flexibility. However, NeRF requires dense input views (tens to hundreds) and a long training time (hours to days) for a single scene to generate high-fidelity images. Although using the voxel grids to represent the radiance field can significantly accelerate the optimization process, we observe that for sparse inputs, the voxel grids are more prone to overfitting to the training views and will have holes and floaters, which leads to artifacts. In this paper, we propose VGOS, an approach for fast (3-5 minutes) radiance field reconstruction from sparse inputs (3-10 views) to address these issues. To improve the performance of voxel-based radiance field in sparse input scenarios, we propose two methods: (a) We introduce an incremental voxel training strategy, which prevents overfitting by suppressing the optimization of peripheral voxels in the early stage of reconstructio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#20102;&#29616;&#26377;&#30340;CVaR&#21644;EVaR&#39118;&#38505;&#24230;&#37327;&#21160;&#24577;&#20998;&#35299;&#26159;&#30495;&#23454;&#39118;&#38505;&#20540;&#30340;&#20005;&#26684;&#39640;&#20272;&#35745;&#65292;&#28982;&#32780;VaR&#23384;&#22312;&#31934;&#30830;&#30340;&#21160;&#24577;&#20998;&#35299;&#12290;</title><link>http://arxiv.org/abs/2304.12477</link><description>&lt;p&gt;
&#20851;&#20110;&#38745;&#24577;&#39118;&#38505;&#24230;&#37327;&#30340;&#21160;&#24577;&#35268;&#21010;&#20998;&#35299;
&lt;/p&gt;
&lt;p&gt;
On Dynamic Program Decompositions of Static Risk Measures. (arXiv:2304.12477v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12477
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;&#29616;&#26377;&#30340;CVaR&#21644;EVaR&#39118;&#38505;&#24230;&#37327;&#21160;&#24577;&#20998;&#35299;&#26159;&#30495;&#23454;&#39118;&#38505;&#20540;&#30340;&#20005;&#26684;&#39640;&#20272;&#35745;&#65292;&#28982;&#32780;VaR&#23384;&#22312;&#31934;&#30830;&#30340;&#21160;&#24577;&#20998;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#20248;&#21270;&#38745;&#24577;&#39118;&#38505;&#35268;&#36991;&#30446;&#26631;&#20855;&#26377;&#19968;&#23450;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#23427;&#20204;&#19981;&#23481;&#26131;&#25509;&#21463;&#21160;&#24577;&#35268;&#21010;&#20998;&#35299;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#24314;&#35758;&#20351;&#29992;&#39118;&#38505;&#24230;&#37327;&#30340;&#21160;&#24577;&#20998;&#35299;&#26469;&#21046;&#23450;&#25193;&#23637;&#29366;&#24577;&#31354;&#38388;&#19978;&#30340;&#21160;&#24577;&#35268;&#21010;&#12290;&#26412;&#25991;&#34920;&#26126;&#20960;&#31181;&#29616;&#26377;&#30340;&#20998;&#35299;&#26412;&#36136;&#19978;&#26159;&#19981;&#31934;&#30830;&#30340;&#65292;&#36825;&#19982;&#25991;&#29486;&#20013;&#30340;&#20960;&#20010;&#22768;&#26126;&#30456;&#30683;&#30462;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#20030;&#20986;&#20102;&#19968;&#20123;&#20363;&#23376;&#65292;&#35777;&#26126;&#20102;CVaR&#21644;EVaR&#39118;&#38505;&#24230;&#37327;&#30340;&#27969;&#34892;&#20998;&#35299;&#26159;&#30495;&#23454;&#39118;&#38505;&#20540;&#30340;&#20005;&#26684;&#39640;&#20272;&#35745;&#12290;&#28982;&#32780;&#65292;VaR&#30830;&#23454;&#23384;&#22312;&#31934;&#30830;&#30340;&#20998;&#35299;&#65292;&#25105;&#20204;&#25552;&#20379;&#19968;&#20010;&#31616;&#21333;&#30340;&#35777;&#26126;&#65292;&#38416;&#26126;&#20102;VaR&#21644;CVaR&#21160;&#24577;&#35268;&#21010;&#23646;&#24615;&#20043;&#38388;&#30340;&#22522;&#26412;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Optimizing static risk-averse objectives in Markov decision processes is challenging because they do not readily admit dynamic programming decompositions. Prior work has proposed to use a dynamic decomposition of risk measures that help to formulate dynamic programs on an augmented state space. This paper shows that several existing decompositions are inherently inexact, contradicting several claims in the literature. In particular, we give examples that show that popular decompositions for CVaR and EVaR risk measures are strict overestimates of the true risk values. However, an exact decomposition is possible for VaR, and we give a simple proof that illustrates the fundamental difference between VaR and CVaR dynamic programming properties.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;AI&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#30340;&#21487;&#33021;&#24615;&#65292;&#25552;&#20986;&#20102;&#31934;&#30830;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#30028;&#38480;&#65292;&#24182;&#25351;&#20986;&#20102;&#35774;&#35745;&#26356;&#20934;&#30830;&#30340;&#26816;&#27979;&#26041;&#27861;&#21644;&#25552;&#39640;&#36879;&#26126;&#24230;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2304.04736</link><description>&lt;p&gt;
&#20851;&#20110;AI&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#30340;&#21487;&#33021;&#24615;&#30340;&#25506;&#35752;
&lt;/p&gt;
&lt;p&gt;
On the Possibilities of AI-Generated Text Detection. (arXiv:2304.04736v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04736
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;AI&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#30340;&#21487;&#33021;&#24615;&#65292;&#25552;&#20986;&#20102;&#31934;&#30830;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#30028;&#38480;&#65292;&#24182;&#25351;&#20986;&#20102;&#35774;&#35745;&#26356;&#20934;&#30830;&#30340;&#26816;&#27979;&#26041;&#27861;&#21644;&#25552;&#39640;&#36879;&#26126;&#24230;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30340;&#24037;&#20316;&#30528;&#30524;&#20110;&#26816;&#27979;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#29983;&#25104;&#30340;&#36755;&#20986;&#65292;&#20197;&#21306;&#20998;&#20854;&#19982;&#20154;&#31867;&#29983;&#25104;&#30340;&#36755;&#20986;&#12290;&#36825;&#39033;&#33021;&#21147;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#38750;&#24120;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#36825;&#31181;&#21306;&#20998;&#30340;&#21487;&#33021;&#24615;&#19968;&#30452;&#26159;&#35813;&#39046;&#22495;&#20869;&#30340;&#20105;&#35758;&#35805;&#39064;&#12290;&#22240;&#27492;&#65292;&#19968;&#20010;&#26680;&#24515;&#38382;&#39064;&#26159;&#25105;&#20204;&#26159;&#21542;&#33021;&#22815;&#26816;&#27979;&#21040;AI&#29983;&#25104;&#30340;&#25991;&#26412;&#65292;&#22914;&#26524;&#33021;&#65292;&#20309;&#26102;&#33021;&#26816;&#27979;&#21040;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#35777;&#25454;&#34920;&#26126;&#65292;&#38500;&#38750;&#20154;&#31867;&#21644;&#26426;&#22120;&#29983;&#25104;&#30340;&#25991;&#26412;&#20998;&#24067;&#22312;&#25972;&#20010;&#25903;&#25345;&#20013;&#23436;&#20840;&#30456;&#21516;&#65292;&#21542;&#21017;&#20960;&#20046;&#24635;&#26159;&#21487;&#20197;&#26816;&#27979;&#21040;AI&#29983;&#25104;&#30340;&#25991;&#26412;&#12290;&#36825;&#20010;&#35266;&#23519;&#32467;&#26524;&#26469;&#33258;&#20110;&#20449;&#24687;&#35770;&#20013;&#30340;&#26631;&#20934;&#32467;&#26524;&#65292;&#24182;&#20381;&#36182;&#20110;&#26426;&#22120;&#29983;&#25104;&#30340;&#25991;&#26412;&#36234;&#20687;&#20154;&#31867;&#65292;&#25105;&#20204;&#23601;&#38656;&#35201;&#26356;&#22810;&#30340;&#26679;&#26412;&#26469;&#26816;&#27979;&#23427;&#12290;&#25105;&#20204;&#24471;&#20986;&#20102;AI&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#30340;&#31934;&#30830;&#26679;&#26412;&#22797;&#26434;&#24230;&#30028;&#38480;&#65292;&#21578;&#35785;&#38656;&#35201;&#22810;&#23569;&#20010;&#26679;&#26412;&#25165;&#33021;&#26816;&#27979;&#21040;AI&#29983;&#25104;&#30340;&#25991;&#26412;&#12290;&#36825;&#24341;&#36215;&#20102;&#26356;&#22810;&#35774;&#35745;&#26356;&#20934;&#30830;&#30340;AI&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#26041;&#27861;&#21644;&#25552;&#39640;LLM&#36879;&#26126;&#24230;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Our work focuses on the challenge of detecting outputs generated by Large Language Models (LLMs) to distinguish them from those generated by humans. This ability is of the utmost importance in numerous applications. However, the possibility of such discernment has been the subject of debate within the community. Therefore, a central question is whether we can detect AI-generated text and, if so, when. In this work, we provide evidence that it should almost always be possible to detect AI-generated text unless the distributions of human and machine-generated texts are exactly the same over the entire support. This observation follows from the standard results in information theory and relies on the fact that if the machine text becomes more human-like, we need more samples to detect it. We derive a precise sample complexity bound of AI-generated text detection, which tells how many samples are needed to detect AI-generated text. This gives rise to additional challenges of designing more
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#32467;&#26500;&#21387;&#32553;&#21644;&#27169;&#22411;&#23610;&#23544;&#19981;&#23545;&#31216;&#30340;&#21452;&#32534;&#30721;&#22120;&#27169;&#22411; KALE&#65292;&#26377;&#25928;&#25552;&#39640;&#23494;&#38598;&#20449;&#24687;&#26816;&#32034;&#30340;&#25512;&#29702;&#25928;&#29575;&#65292;&#21516;&#26102;&#20801;&#35768;&#26597;&#35810;&#32534;&#30721;&#22120;&#30340;&#26377;&#25928;&#21387;&#32553;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#20840;&#37096;&#30340;&#20877;&#35757;&#32451;&#25110;&#32034;&#24341;&#29983;&#25104;&#65292;&#27492;&#26041;&#27861;&#33021;&#22815;&#29983;&#25104;&#36229;&#36807;DistilBERT&#24615;&#33021;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2304.01016</link><description>&lt;p&gt;
&#24555;&#36895;&#23494;&#38598;&#20449;&#24687;&#26816;&#32034;&#22120;&#21033;&#29992;KALE&#36827;&#34892;&#21518;&#32622;KL&#23545;&#40784;&#30340;&#24322;&#24418;&#21452;&#32534;&#30721;&#22120;&#27169;&#22411;&#35757;&#32451; (arXiv:2304.01016v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
Quick Dense Retrievers Consume KALE: Post Training Kullback Leibler Alignment of Embeddings for Asymmetrical dual encoders. (arXiv:2304.01016v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01016
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#32467;&#26500;&#21387;&#32553;&#21644;&#27169;&#22411;&#23610;&#23544;&#19981;&#23545;&#31216;&#30340;&#21452;&#32534;&#30721;&#22120;&#27169;&#22411; KALE&#65292;&#26377;&#25928;&#25552;&#39640;&#23494;&#38598;&#20449;&#24687;&#26816;&#32034;&#30340;&#25512;&#29702;&#25928;&#29575;&#65292;&#21516;&#26102;&#20801;&#35768;&#26597;&#35810;&#32534;&#30721;&#22120;&#30340;&#26377;&#25928;&#21387;&#32553;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#20840;&#37096;&#30340;&#20877;&#35757;&#32451;&#25110;&#32034;&#24341;&#29983;&#25104;&#65292;&#27492;&#26041;&#27861;&#33021;&#22815;&#29983;&#25104;&#36229;&#36807;DistilBERT&#24615;&#33021;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#32467;&#26500;&#21387;&#32553;&#21644;&#27169;&#22411;&#23610;&#23544;&#19981;&#23545;&#31216;&#30340;&#21452;&#32534;&#30721;&#22120;&#27169;&#22411;&#65292;&#26088;&#22312;&#25552;&#39640;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#23494;&#38598;&#20449;&#24687;&#26816;&#32034;&#31995;&#32479;&#30340;&#25512;&#29702;&#36895;&#24230;&#12290;&#36890;&#36807;&#23545;MSMARCO&#12289;&#33258;&#28982;&#38382;&#31572;&#12289;&#38382;&#31572;&#28216;&#25103;&#31561;&#22810;&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#21069;&#21518;&#35757;&#32451;&#21387;&#32553;&#23454;&#39564;&#65292;&#30740;&#31350;&#20102;&#21387;&#32553;&#23545;&#31995;&#32479;&#25512;&#29702;&#25928;&#29575;&#30340;&#24433;&#21709;&#65292;&#32467;&#26524;&#34920;&#26126;&#23494;&#38598;&#20449;&#24687;&#26816;&#32034;&#22120;&#30340;&#21452;&#32534;&#30721;&#22120;&#32467;&#26500;&#24322;&#24418;&#21270;&#26377;&#21161;&#20110;&#25552;&#39640;&#20854;&#25512;&#29702;&#25928;&#29575;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;Kullback Leibler Alignment of Embeddings (KALE)&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35009;&#21098;&#21644;&#23545;&#40784;&#26597;&#35810;&#32534;&#30721;&#22120;&#65292;&#25552;&#39640;&#20102;&#23494;&#38598;&#20449;&#24687;&#26816;&#32034;&#30340;&#25512;&#29702;&#25928;&#29575;&#12290;KALE&#25193;&#23637;&#20102;&#20256;&#32479;&#30340;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#20351;&#24471;&#22312;&#21452;&#32534;&#30721;&#22120;&#35757;&#32451;&#21518;&#21487;&#20197;&#26377;&#25928;&#22320;&#23545;&#26597;&#35810;&#32534;&#30721;&#22120;&#36827;&#34892;&#21387;&#32553;&#32780;&#26080;&#38656;&#36827;&#34892;&#23436;&#25972;&#30340;&#20877;&#35757;&#32451;&#25110;&#32034;&#24341;&#29983;&#25104;&#12290;&#20351;&#29992;KALE&#21644;&#19981;&#23545;&#31216;&#35757;&#32451;&#65292;&#25105;&#20204;&#21487;&#20197;&#29983;&#25104;&#36229;&#36807;DistilBERT&#24615;&#33021;&#30340;&#27169;&#22411;&#65292;&#21516;&#26102;&#27169;&#22411;&#23610;&#23544;&#26356;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we consider the problem of improving the inference latency of language model-based dense retrieval systems by introducing structural compression and model size asymmetry between the context and query encoders. First, we investigate the impact of pre and post-training compression on the MSMARCO, Natural Questions, TriviaQA, SQUAD, and SCIFACT, finding that asymmetry in the dual encoders in dense retrieval can lead to improved inference efficiency. Knowing this, we introduce Kullback Leibler Alignment of Embeddings (KALE), an efficient and accurate method for increasing the inference efficiency of dense retrieval methods by pruning and aligning the query encoder after training. Specifically, KALE extends traditional Knowledge Distillation after bi-encoder training, allowing for effective query encoder compression without full retraining or index generation. Using KALE and asymmetric training, we can generate models which exceed the performance of DistilBERT despite having 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24466;&#24351;&#23398;&#20064;&#30340;&#38754;&#21521;&#20027;&#39064;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#22120;SuTI&#65292;&#33021;&#22815;&#36890;&#36807;&#23558;&#22823;&#37327;&#22522;&#20110;&#20027;&#39064;&#30340;&#19987;&#23478;&#27169;&#22411;&#30340;&#25968;&#25454;&#36755;&#20837;&#24466;&#24351;&#27169;&#22411;&#65292;&#23398;&#20064;&#24182;&#25512;&#26029;&#20986;&#26032;&#20027;&#39064;&#30340;&#26368;&#20339;&#19987;&#23478;&#27169;&#22411;&#65292;&#20174;&#32780;&#29983;&#25104;&#39640;&#21697;&#36136;&#30340;&#33258;&#23450;&#20041;&#22270;&#20687;&#65292;&#19988;&#36895;&#24230;&#27604;&#20256;&#32479;&#26041;&#27861;&#26356;&#24555;&#12290;</title><link>http://arxiv.org/abs/2304.00186</link><description>&lt;p&gt;
&#22522;&#20110;&#24466;&#24351;&#23398;&#20064;&#30340;&#38754;&#21521;&#20027;&#39064;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#22120;
&lt;/p&gt;
&lt;p&gt;
Subject-driven Text-to-Image Generation via Apprenticeship Learning. (arXiv:2304.00186v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00186
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24466;&#24351;&#23398;&#20064;&#30340;&#38754;&#21521;&#20027;&#39064;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#22120;SuTI&#65292;&#33021;&#22815;&#36890;&#36807;&#23558;&#22823;&#37327;&#22522;&#20110;&#20027;&#39064;&#30340;&#19987;&#23478;&#27169;&#22411;&#30340;&#25968;&#25454;&#36755;&#20837;&#24466;&#24351;&#27169;&#22411;&#65292;&#23398;&#20064;&#24182;&#25512;&#26029;&#20986;&#26032;&#20027;&#39064;&#30340;&#26368;&#20339;&#19987;&#23478;&#27169;&#22411;&#65292;&#20174;&#32780;&#29983;&#25104;&#39640;&#21697;&#36136;&#30340;&#33258;&#23450;&#20041;&#22270;&#20687;&#65292;&#19988;&#36895;&#24230;&#27604;&#20256;&#32479;&#26041;&#27861;&#26356;&#24555;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#65288;&#22914;DreamBooth&#65289;&#22312;&#36890;&#36807;&#38024;&#23545;&#30446;&#26631;&#20027;&#39064;&#24494;&#35843;&#8220;&#19987;&#23478;&#27169;&#22411;&#8221;&#65292;&#29983;&#25104;&#39640;&#24230;&#33258;&#23450;&#20041;&#30340;&#22270;&#20687;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#36825;&#20010;&#36807;&#31243;&#24456;&#26114;&#36149;&#65292;&#22240;&#20026;&#27599;&#20010;&#20027;&#39064;&#37117;&#24517;&#39035;&#23398;&#20064;&#19968;&#20010;&#26032;&#30340;&#19987;&#23478;&#27169;&#22411;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26367;&#20195;&#20027;&#39064;&#29305;&#23450;&#24494;&#35843;&#30340;&#38754;&#21521;&#20027;&#39064;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#22120;SuTI&#12290;&#32473;&#23450;&#19968;&#20010;&#26032;&#20027;&#39064;&#30340;&#23569;&#37327;&#28436;&#31034;&#65292;SuTI&#21487;&#20197;&#21363;&#26102;&#29983;&#25104;&#19981;&#21516;&#22330;&#26223;&#20013;&#20027;&#39064;&#30340;&#26032;&#29256;&#26412;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#20219;&#20309;&#20027;&#39064;&#29305;&#23450;&#30340;&#20248;&#21270;&#12290;SuTI&#30001;&#8220;&#24466;&#24351;&#23398;&#20064;&#8221;&#39537;&#21160;&#65292;&#20854;&#20013;&#20174;&#22823;&#37327;&#22522;&#20110;&#20027;&#39064;&#30340;&#19987;&#23478;&#27169;&#22411;&#29983;&#25104;&#30340;&#25968;&#25454;&#20013;&#23398;&#20064;&#21333;&#20010;&#30340;&#24466;&#24351;&#27169;&#22411;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20174;&#20114;&#32852;&#32593;&#25366;&#25496;&#20102;&#25968;&#30334;&#19975;&#20010;&#22270;&#20687;&#31751;&#65292;&#27599;&#20010;&#22270;&#20687;&#31751;&#37117;&#32858;&#28966;&#20110;&#19968;&#20010;&#29305;&#23450;&#30340;&#35270;&#35273;&#20027;&#39064;&#12290;&#25105;&#20204;&#37319;&#29992;&#36825;&#20123;&#31751;&#26469;&#35757;&#32451;&#22823;&#37327;&#19987;&#38376;&#38024;&#23545;&#19981;&#21516;&#35270;&#35273;&#20027;&#39064;&#30340;&#19987;&#23478;&#27169;&#22411;&#12290;&#24466;&#24351;&#27169;&#22411;&#36890;&#36807;&#25512;&#26029;&#22522;&#20110;&#20854;&#25991;&#26412;&#25551;&#36848;&#30340;&#26032;&#20027;&#39064;&#30340;&#26368;&#20339;&#19987;&#23478;&#27169;&#22411;&#24182;&#29983;&#25104;&#22270;&#20687;&#26469;&#23398;&#20064;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;SuTI&#30340;&#26377;&#25928;&#24615;&#65292;&#34920;&#26126;&#23427;&#21487;&#20197;&#29983;&#25104;&#39640;&#21697;&#36136;&#30340;&#19981;&#21516;&#20027;&#39064;&#30340;&#22270;&#20687;&#65292;&#21516;&#26102;&#27604;&#22522;&#20110;&#24494;&#35843;&#30340;&#26041;&#27861;&#24555;&#24471;&#22810;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent text-to-image generation models like DreamBooth have made remarkable progress in generating highly customized images of a target subject, by fine-tuning an ``expert model'' for a given subject from a few examples. However, this process is expensive, since a new expert model must be learned for each subject. In this paper, we present SuTI, a Subject-driven Text-to-Image generator that replaces subject-specific fine tuning with \emph{in-context} learning. Given a few demonstrations of a new subject, SuTI can instantly generate novel renditions of the subject in different scenes, without any subject-specific optimization. SuTI is powered by {\em apprenticeship learning}, where a single apprentice model is learned from data generated by massive amount of subject-specific expert models. Specifically, we mine millions of image clusters from the Internet, each centered around a specific visual subject. We adopt these clusters to train massive amount of expert models specialized on diff
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#20013;&#38388;&#27010;&#24565;&#30340;&#32423;&#21035;&#32467;&#26500;&#65292;&#21033;&#29992;&#39046;&#22495;&#19987;&#23478;&#24314;&#31435;&#37096;&#20998;-&#25972;&#20307;&#20851;&#31995;&#65292;&#20174;&#32780;&#22312;&#19981;&#21516;&#25277;&#35937;&#32423;&#21035;&#19978;&#24110;&#21161;&#35299;&#37322;&#40657;&#30418;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2303.11920</link><description>&lt;p&gt;
&#20013;&#38388;&#29305;&#24449;&#32852;&#30431;&#33021;&#24110;&#21161;&#35299;&#37322;&#40657;&#30418;&#27169;&#22411;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Do intermediate feature coalitions aid explainability of black-box models?. (arXiv:2303.11920v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11920
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#20013;&#38388;&#27010;&#24565;&#30340;&#32423;&#21035;&#32467;&#26500;&#65292;&#21033;&#29992;&#39046;&#22495;&#19987;&#23478;&#24314;&#31435;&#37096;&#20998;-&#25972;&#20307;&#20851;&#31995;&#65292;&#20174;&#32780;&#22312;&#19981;&#21516;&#25277;&#35937;&#32423;&#21035;&#19978;&#24110;&#21161;&#35299;&#37322;&#40657;&#30418;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#22522;&#20110;&#32423;&#21035;&#32467;&#26500;&#30340;&#20013;&#38388;&#27010;&#24565;&#65292;&#20197;&#24110;&#21161;&#40657;&#30418;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#32423;&#21035;&#32467;&#26500;&#26159;&#19968;&#31181;&#20998;&#23618;&#32467;&#26500;&#65292;&#27599;&#20010;&#32423;&#21035;&#23545;&#24212;&#25968;&#25454;&#38598;&#30340;&#29305;&#24449;&#65288;&#21363;&#29609;&#23478;&#38598;&#20998;&#21306;&#65289;&#12290;&#20174;&#21482;&#21253;&#21547;&#21333;&#20803;&#32032;&#30340;&#24179;&#20961;&#38598;&#21512;&#21040;&#21482;&#21253;&#21547;&#22823;&#32852;&#30431;&#30340;&#38598;&#21512;&#65292;&#31895;&#31961;&#24230;&#30340;&#32423;&#21035;&#36880;&#28176;&#22686;&#21152;&#12290;&#27492;&#22806;&#65292;&#21487;&#20197;&#36890;&#36807;&#39046;&#22495;&#19987;&#23478;&#24314;&#31435;&#37096;&#20998;-&#25972;&#20307;&#20851;&#31995;&#26469;&#29983;&#25104;&#25277;&#35937;&#32423;&#21035;&#30340;&#35299;&#37322;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#23454;&#38469;&#30340;&#27773;&#36710;&#27169;&#22411;&#31034;&#20363;&#21644;&#27888;&#22374;&#23612;&#20811;&#21495;&#30340;&#25968;&#25454;&#38598;&#20013;&#35828;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#21487;&#29992;&#24615;&#65292;&#20854;&#20013;&#20013;&#38388;&#27010;&#24565;&#22312;&#19981;&#21516;&#25277;&#35937;&#32423;&#21035;&#19978;&#24110;&#21161;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work introduces the notion of intermediate concepts based on levels structure to aid explainability for black-box models. The levels structure is a hierarchical structure in which each level corresponds to features of a dataset (i.e., a player-set partition). The level of coarseness increases from the trivial set, which only comprises singletons, to the set, which only contains the grand coalition. In addition, it is possible to establish meronomies, i.e., part-whole relationships, via a domain expert that can be utilised to generate explanations at an abstract level. We illustrate the usability of this approach in a real-world car model example and the Titanic dataset, where intermediate concepts aid in explainability at different levels of abstraction.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23616;&#37096;&#27491;&#21017;&#21270;&#30340;&#31070;&#32463;&#24494;&#20998;&#26041;&#31243;&#27169;&#22411;&#65292;&#36890;&#36807;&#21551;&#21457;&#24335;&#25104;&#26412;&#25511;&#21046;&#35757;&#32451;&#36807;&#31243;&#20197;&#23398;&#20064;&#26131;&#20110;&#31215;&#20998;&#30340;&#21160;&#21147;&#31995;&#32479;&#65292;&#21516;&#26102;&#25552;&#39640;&#39044;&#27979;&#36895;&#24230;&#65292;&#24182;&#20445;&#30041;&#20102;&#27169;&#22411;&#30340;&#33258;&#36866;&#24212;&#29305;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.02262</link><description>&lt;p&gt;
&#23616;&#37096;&#27491;&#21017;&#21270;&#30340;&#31070;&#32463;&#24494;&#20998;&#26041;&#31243;&#65306;&#26377;&#20123;&#40657;&#30418;&#23376;&#26159;&#35201;&#20445;&#25345;&#23553;&#38381;&#30340;&#65281;
&lt;/p&gt;
&lt;p&gt;
Locally Regularized Neural Differential Equations: Some Black Boxes Were Meant to Remain Closed!. (arXiv:2303.02262v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.02262
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23616;&#37096;&#27491;&#21017;&#21270;&#30340;&#31070;&#32463;&#24494;&#20998;&#26041;&#31243;&#27169;&#22411;&#65292;&#36890;&#36807;&#21551;&#21457;&#24335;&#25104;&#26412;&#25511;&#21046;&#35757;&#32451;&#36807;&#31243;&#20197;&#23398;&#20064;&#26131;&#20110;&#31215;&#20998;&#30340;&#21160;&#21147;&#31995;&#32479;&#65292;&#21516;&#26102;&#25552;&#39640;&#39044;&#27979;&#36895;&#24230;&#65292;&#24182;&#20445;&#30041;&#20102;&#27169;&#22411;&#30340;&#33258;&#36866;&#24212;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#33258;&#21160;&#36866;&#24212;&#30340;&#33021;&#21147;&#65292;&#31867;&#20284;&#31070;&#32463;&#24494;&#20998;&#26041;&#31243;&#30340;&#38544;&#24335;&#23618;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#24050;&#32463;&#25104;&#20026;&#37325;&#35201;&#30340;&#24314;&#27169;&#26694;&#26550;&#12290;&#20294;&#26159;&#65292;&#25511;&#21046;&#36825;&#20123;&#27169;&#22411;&#30340;&#35745;&#31639;&#25104;&#26412;&#26159;&#22256;&#38590;&#30340;&#65292;&#22240;&#20026;&#23427;&#20381;&#36182;&#20110;&#33258;&#36866;&#24212;&#27714;&#35299;&#22120;&#30340;&#27493;&#39588;&#25968;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#22312;&#38543;&#26426;&#26102;&#38388;&#28857;&#20351;&#29992;&#33258;&#36866;&#24212;&#24494;&#20998;&#26041;&#31243;&#27714;&#35299;&#22120;&#30340;&#20869;&#37096;&#25104;&#26412;&#21551;&#21457;&#24335;&#26469;&#24341;&#23548;&#35757;&#32451;&#20197;&#23398;&#20064;&#26356;&#26131;&#20110;&#31215;&#20998;&#30340;&#21160;&#21147;&#31995;&#32479;&#12290;&#25105;&#20204;&#22686;&#21152;&#23616;&#37096;&#27491;&#21017;&#21270;&#39033;&#21040;&#30446;&#26631;&#20989;&#25968;&#20013;&#65292;&#20351;&#24471;&#35757;&#32451;&#36807;&#31243;&#36981;&#24490;&#32463;&#39564;&#26465;&#20214;&#24182;&#25552;&#39640;&#39044;&#27979;&#36895;&#24230;&#65292;&#21516;&#26102;&#20173;&#20445;&#30041;&#20102;&#27169;&#22411;&#30340;&#33258;&#36866;&#24212;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Implicit layer deep learning techniques, like Neural Differential Equations, have become an important modeling framework due to their ability to adapt to new problems automatically. Training a neural differential equation is effectively a search over a space of plausible dynamical systems. However, controlling the computational cost for these models is difficult since it relies on the number of steps the adaptive solver takes. Most prior works have used higher-order methods to reduce prediction timings while greatly increasing training time or reducing both training and prediction timings by relying on specific training algorithms, which are harder to use as a drop-in replacement due to strict requirements on automatic differentiation. In this manuscript, we use internal cost heuristics of adaptive differential equation solvers at stochastic time points to guide the training toward learning a dynamical system that is easier to integrate. We "close the black-box" and allow the use of ou
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#31995;&#32479;&#22320;&#25506;&#35752;&#20102;&#22823;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#36328;&#39046;&#22495;&#31867;&#27604;&#21019;&#36896;&#21147;&#30340;&#33021;&#21147;&#65292;&#32467;&#26524;&#34920;&#26126;LLM&#29983;&#25104;&#30340;&#36328;&#39046;&#22495;&#31867;&#27604;&#22312;&#38382;&#39064;&#37325;&#26500;&#20013;&#20855;&#26377;&#23454;&#38469;&#24110;&#21161;&#65292;&#24182;&#19988;&#23384;&#22312;&#19968;&#20123;&#28508;&#22312;&#30340;&#21361;&#23475;&#65292;&#38656;&#35201;&#27880;&#24847;&#12290;</title><link>http://arxiv.org/abs/2302.12832</link><description>&lt;p&gt;
&#27969;&#20307;&#21464;&#21387;&#22120;&#19982;&#21019;&#36896;&#24615;&#31867;&#27604;&#65306;&#25506;&#32034;&#22823;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#36328;&#39046;&#22495;&#31867;&#27604;&#21019;&#36896;&#21147;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Fluid Transformers and Creative Analogies: Exploring Large Language Models' Capacity for Augmenting Cross-Domain Analogical Creativity. (arXiv:2302.12832v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.12832
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#31995;&#32479;&#22320;&#25506;&#35752;&#20102;&#22823;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#36328;&#39046;&#22495;&#31867;&#27604;&#21019;&#36896;&#21147;&#30340;&#33021;&#21147;&#65292;&#32467;&#26524;&#34920;&#26126;LLM&#29983;&#25104;&#30340;&#36328;&#39046;&#22495;&#31867;&#27604;&#22312;&#38382;&#39064;&#37325;&#26500;&#20013;&#20855;&#26377;&#23454;&#38469;&#24110;&#21161;&#65292;&#24182;&#19988;&#23384;&#22312;&#19968;&#20123;&#28508;&#22312;&#30340;&#21361;&#23475;&#65292;&#38656;&#35201;&#27880;&#24847;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36328;&#39046;&#22495;&#30340;&#31867;&#27604;&#25512;&#29702;&#26159;&#19968;&#31181;&#23545;&#20154;&#31867;&#26469;&#35828;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#26680;&#24515;&#21019;&#36896;&#33021;&#21147;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#24050;&#32463;&#35777;&#26126;&#20102;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#36328;&#39046;&#22495;&#31867;&#27604;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#33021;&#21147;&#30340;&#21487;&#38752;&#24615;&#21644;&#28508;&#22312;&#29992;&#36884;&#30340;&#25506;&#32034;&#21364;&#40092;&#26377;&#31995;&#32479;&#30340;&#30740;&#31350;&#12290;&#26412;&#25991;&#31995;&#32479;&#22320;&#25506;&#35752;&#20102;LLMs&#22686;&#24378;&#36328;&#39046;&#22495;&#31867;&#27604;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;&#22312;&#19977;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#65306;1&#65289;LLM&#29983;&#25104;&#30340;&#36328;&#39046;&#22495;&#31867;&#27604;&#22312;&#38382;&#39064;&#37325;&#26500;&#20219;&#21153;&#30340;&#32972;&#26223;&#19979;&#32463;&#24120;&#34987;&#35780;&#20026;&#26377;&#24110;&#21161;&#65288;&#20013;&#20301;&#25968;5&#20010;&#35780;&#20998;&#20013;&#26377;4&#20010;&#26159;&#26377;&#24110;&#21161;&#30340;&#65289;&#65292;&#24182;&#19988;&#36890;&#24120;&#65288;&#32422;80&#65285;&#30340;&#24773;&#20917;&#65289;&#23548;&#33268;&#38382;&#39064;&#37325;&#26032;&#21046;&#23450;&#26041;&#38754;&#30340;&#21487;&#35266;&#23519;&#21464;&#21270;&#65307;2&#65289;&#23384;&#22312;&#26368;&#22810;25&#65285;&#30340;&#36755;&#20986;&#34987;&#35780;&#20026;&#26377;&#28508;&#22312;&#21361;&#23475;&#30340;&#19978;&#38480;&#65292;&#20854;&#20013;&#22823;&#37096;&#20998;&#26159;&#30001;&#20110;&#28508;&#22312;&#30340;&#19981;&#23433;&#20869;&#23481;&#65292;&#32780;&#19981;&#26159;&#26377;&#20559;&#35265;&#25110;&#26377;&#27602;&#20869;&#23481;&#12290;&#36825;&#20123;&#32467;&#26524;&#35777;&#26126;&#20102;LLMs&#30340;&#28508;&#22312;&#25928;&#29992;&#21644;&#39118;&#38505;&#65292;&#20197;&#22686;&#24378;&#36328;&#39046;&#22495;&#31867;&#27604;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cross-domain analogical reasoning is a core creative ability that can be challenging for humans. Recent work has shown some proofs-of concept of Large language Models' (LLMs) ability to generate cross-domain analogies. However, the reliability and potential usefulness of this capacity for augmenting human creative work has received little systematic exploration. In this paper, we systematically explore LLMs capacity to augment cross-domain analogical reasoning. Across three studies, we found: 1) LLM-generated cross-domain analogies were frequently judged as helpful in the context of a problem reformulation task (median 4 out of 5 helpfulness rating), and frequently (~80% of cases) led to observable changes in problem formulations, and 2) there was an upper bound of 25% of outputs bring rated as potentially harmful, with a majority due to potentially upsetting content, rather than biased or toxic content. These results demonstrate the potential utility -- and risks -- of LLMs for augmen
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#20844;&#24179;&#25193;&#25955;&#8221;&#30340;&#26032;&#31574;&#30053;&#65292;&#21487;&#20197;&#22312;&#29983;&#25104;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#37096;&#32626;&#21518;&#20943;&#36731;&#20559;&#35265;&#24182;&#20351;&#27169;&#22411;&#25509;&#21463;&#20844;&#24179;&#24615;&#25351;&#23548;&#12290;</title><link>http://arxiv.org/abs/2302.10893</link><description>&lt;p&gt;
&#20844;&#24179;&#25193;&#25955;&#65306;&#35757;&#32451;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#23454;&#29616;&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;
Fair Diffusion: Instructing Text-to-Image Generation Models on Fairness. (arXiv:2302.10893v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.10893
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#20844;&#24179;&#25193;&#25955;&#8221;&#30340;&#26032;&#31574;&#30053;&#65292;&#21487;&#20197;&#22312;&#29983;&#25104;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#37096;&#32626;&#21518;&#20943;&#36731;&#20559;&#35265;&#24182;&#20351;&#27169;&#22411;&#25509;&#21463;&#20844;&#24179;&#24615;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#29983;&#25104;&#24335;AI&#27169;&#22411;&#22312;&#36136;&#37327;&#26041;&#38754;&#21462;&#24471;&#20102;&#24778;&#20154;&#30340;&#25104;&#26524;&#65292;&#24182;&#22240;&#27492;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#36234;&#26469;&#36234;&#22810;&#30340;&#24212;&#29992;&#20013;&#12290;&#20294;&#30001;&#20110;&#23427;&#20204;&#39640;&#24230;&#20381;&#36182;&#20110;&#20174;&#20114;&#32852;&#32593;&#19978;&#38543;&#26426;&#25277;&#21462;&#30340;&#21313;&#20159;&#32423;&#25968;&#25454;&#38598;&#65292;&#22240;&#27492;&#23427;&#20204;&#20063;&#20250;&#21463;&#21040;&#36864;&#21270;&#21644;&#20559;&#35265;&#30340;&#20154;&#31867;&#34892;&#20026;&#30340;&#24433;&#21709;&#65292;&#27491;&#22914;&#25105;&#20204;&#25152;&#23637;&#31034;&#30340;&#37027;&#26679;&#12290;&#20107;&#23454;&#19978;&#65292;&#23427;&#20204;&#29978;&#33267;&#21487;&#33021;&#21152;&#21095;&#36825;&#20123;&#20559;&#35265;&#12290;&#20026;&#20102;&#19981;&#20165;&#25581;&#31034;&#32780;&#19988;&#23545;&#25239;&#36825;&#20123;&#19981;&#33391;&#24433;&#21709;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31574;&#30053;&#65292;&#31216;&#20026;&#20844;&#24179;&#25193;&#25955;&#65292;&#20197;&#22312;&#29983;&#25104;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#37096;&#32626;&#21518;&#20943;&#36731;&#20559;&#35265;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22522;&#20110;&#20154;&#31867;&#25351;&#23548;&#30340;&#20559;&#24046;&#36716;&#31227;&#65292;&#21487;&#22312;&#20219;&#20309;&#26041;&#21521;&#19978;&#20135;&#29983;&#20219;&#24847;&#26032;&#30340;&#27604;&#20363;&#65292;&#20363;&#22914;&#65292;&#36523;&#20221;&#32452;&#12290;&#27491;&#22914;&#25105;&#20204;&#30340;&#23454;&#35777;&#35780;&#20272;&#25152;&#31034;&#65292;&#36825;&#31181;&#25511;&#21046;&#20351;&#29983;&#25104;&#22270;&#20687;&#27169;&#22411;&#22312;&#20844;&#24179;&#24615;&#26041;&#38754;&#33021;&#22815;&#25509;&#21463;&#25351;&#23548;&#65292;&#26080;&#38656;&#25968;&#25454;&#36807;&#28388;&#21644;&#39069;&#22806;&#30340;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative AI models have recently achieved astonishing results in quality and are consequently employed in a fast-growing number of applications. However, since they are highly data-driven, relying on billion-sized datasets randomly scraped from the internet, they also suffer from degenerated and biased human behavior, as we demonstrate. In fact, they may even reinforce such biases. To not only uncover but also combat these undesired effects, we present a novel strategy, called Fair Diffusion, to attenuate biases after the deployment of generative text-to-image models. Specifically, we demonstrate shifting a bias, based on human instructions, in any direction yielding arbitrarily new proportions for, e.g., identity groups. As our empirical evaluation demonstrates, this introduced control enables instructing generative image models on fairness, with no data filtering and additional training required.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26816;&#39564;&#20102; ChatGPT &#22312; 25 &#20010;&#19981;&#21516;&#30340; NLP &#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#65292;&#23427;&#26159;&#19968;&#20010;&#19975;&#33021;&#30340; AI &#27169;&#22411;&#65292;&#20294;&#26080;&#20851;&#32039;&#35201;&#30340;&#34920;&#29616;&#21487;&#33021;&#20250;&#23545;&#26576;&#20123;&#20219;&#21153;&#30340;&#34920;&#29616;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2302.10724</link><description>&lt;p&gt;
ChatGPT&#65306;&#24212;&#20184;&#21315;&#20107;&#30340;&#19975;&#33021;&#22411; AI&#65292;&#20294;&#26080;&#25152;&#19987;&#31934;
&lt;/p&gt;
&lt;p&gt;
ChatGPT: Jack of all trades, master of none. (arXiv:2302.10724v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.10724
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26816;&#39564;&#20102; ChatGPT &#22312; 25 &#20010;&#19981;&#21516;&#30340; NLP &#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#65292;&#23427;&#26159;&#19968;&#20010;&#19975;&#33021;&#30340; AI &#27169;&#22411;&#65292;&#20294;&#26080;&#20851;&#32039;&#35201;&#30340;&#34920;&#29616;&#21487;&#33021;&#20250;&#23545;&#26576;&#20123;&#20219;&#21153;&#30340;&#34920;&#29616;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
OpenAI &#25512;&#20986;&#20102;&#32842;&#22825;&#29983;&#25104;&#39044;&#35757;&#32451; Transformer&#65288;ChatGPT&#65289;&#65292;&#38761;&#26032;&#20102;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#20114;&#21160;&#30340;&#26041;&#27861;&#12290;&#35768;&#22810;&#30740;&#31350;&#36890;&#36807;&#27979;&#35797; ChatGPT &#22312;&#20247;&#25152;&#21608;&#30693;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#30340;&#25928;&#26524;&#65292;&#26469;&#35780;&#20272;&#35813;&#27169;&#22411;&#30340;&#25928;&#33021;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#30740;&#31350;&#22823;&#22810;&#38750;&#33258;&#21160;&#21270;&#65292;&#24182;&#19988;&#35268;&#27169;&#38750;&#24120;&#26377;&#38480;&#12290;&#26412;&#30740;&#31350;&#22312; 25 &#20010;&#19981;&#21516;&#30340; NLP &#20219;&#21153;&#19978;&#26816;&#39564;&#20102; ChatGPT &#30340;&#24615;&#33021;&#65292;&#20854;&#20013;&#22823;&#22810;&#25968;&#20219;&#21153;&#29978;&#33267;&#23545;&#20154;&#31867;&#32780;&#35328;&#37117;&#26159;&#20027;&#35266;&#30340;&#65292;&#20363;&#22914;&#24773;&#24863;&#20998;&#26512;&#12289;&#24773;&#32490;&#35782;&#21035;&#12289;&#25915;&#20987;&#24615;&#21644;&#31435;&#22330;&#26816;&#27979;&#12290;&#21478;&#19968;&#20123;&#20219;&#21153;&#21017;&#38656;&#35201;&#26356;&#23458;&#35266;&#30340;&#25512;&#29702;&#65292;&#22914;&#35789;&#20041;&#28040;&#27495;&#12289;&#35821;&#35328;&#21487;&#25509;&#21463;&#24615;&#21644;&#38382;&#31572;&#12290;&#25105;&#20204;&#36824;&#23545; GPT-4 &#27169;&#22411;&#22312;&#20116;&#20010;&#36873;&#23450;&#30340; NLP &#20219;&#21153;&#23376;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#25105;&#20204;&#33258;&#21160;&#21270;&#20102; ChatGPT &#21644; GPT-4 &#30340;&#24341;&#23548;&#36807;&#31243;&#65292;&#24182;&#20998;&#26512;&#20102;&#36229;&#36807; 49k &#20010;&#21709;&#24212;&#12290;&#19982;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#35299;&#20915;&#26041;&#26696;&#65288;SOTA&#65289;&#36827;&#34892;&#27604;&#36739;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#19968;&#20123;&#20219;&#21153;&#19978; ChatGPT &#30340;&#24615;&#33021;&#23384;&#22312;&#19968;&#23450;&#30340;&#32570;&#38519;&#12290;
&lt;/p&gt;
&lt;p&gt;
OpenAI has released the Chat Generative Pre-trained Transformer (ChatGPT) and revolutionized the approach in artificial intelligence to human-model interaction. Several publications on ChatGPT evaluation test its effectiveness on well-known natural language processing (NLP) tasks. However, the existing studies are mostly non-automated and tested on a very limited scale. In this work, we examined ChatGPT's capabilities on 25 diverse analytical NLP tasks, most of them subjective even to humans, such as sentiment analysis, emotion recognition, offensiveness, and stance detection. In contrast, the other tasks require more objective reasoning like word sense disambiguation, linguistic acceptability, and question answering. We also evaluated GPT-4 model on five selected subsets of NLP tasks. We automated ChatGPT and GPT-4 prompting process and analyzed more than 49k responses. Our comparison of its results with available State-of-the-Art (SOTA) solutions showed that the average loss in quali
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#20102;&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#65288;MPNNs&#65289;&#19981;&#33021;&#23398;&#20064;&#20960;&#20309;&#20449;&#24687;&#65292;&#25552;&#20986;&#20102;$k$-DisGNNs&#21487;&#20197;&#21033;&#29992;&#36317;&#31163;&#30697;&#38453;&#20013;&#30340;&#20449;&#24687;&#65292;&#24182;&#24314;&#31435;&#20102;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#21644;&#20256;&#32479;&#22270;&#34920;&#31034;&#23398;&#20064;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;</title><link>http://arxiv.org/abs/2302.05743</link><description>&lt;p&gt;
&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#20165;&#20381;&#38752;&#36317;&#31163;&#30697;&#38453;&#36275;&#22815;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Is Distance Matrix Enough for Geometric Deep Learning?. (arXiv:2302.05743v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.05743
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#65288;MPNNs&#65289;&#19981;&#33021;&#23398;&#20064;&#20960;&#20309;&#20449;&#24687;&#65292;&#25552;&#20986;&#20102;$k$-DisGNNs&#21487;&#20197;&#21033;&#29992;&#36317;&#31163;&#30697;&#38453;&#20013;&#30340;&#20449;&#24687;&#65292;&#24182;&#24314;&#31435;&#20102;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#21644;&#20256;&#32479;&#22270;&#34920;&#31034;&#23398;&#20064;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#24120;&#29992;&#20110;&#28041;&#21450;&#22270;&#24418;&#20960;&#20309;&#30340;&#20219;&#21153;&#65292;&#20363;&#22914;&#20998;&#23376;&#21160;&#21147;&#23398;&#27169;&#25311;&#12290;&#34429;&#28982;&#20960;&#20309;&#22270;&#30340;&#36317;&#31163;&#30697;&#38453;&#21253;&#21547;&#23436;&#25972;&#30340;&#20960;&#20309;&#20449;&#24687;&#65292;&#20294;&#24050;&#32463;&#35777;&#26126;&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#65288;MPNNs&#65289;&#26080;&#27861;&#23398;&#20064;&#36825;&#31181;&#20960;&#20309;&#20449;&#24687;&#12290;&#26412;&#25991;&#36890;&#36807;&#26500;&#36896;&#26032;&#39062;&#30340;&#23545;&#31216;&#20960;&#20309;&#22270;&#30340;&#23478;&#26063;&#65292;&#25193;&#23637;&#20102;MPNN&#26080;&#27861;&#21306;&#20998;&#20854;&#36317;&#31163;&#30697;&#38453;&#30340;&#21453;&#20363;&#23478;&#26063;&#65292;&#24182;&#25552;&#20986;$k$-DisGNNs&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#21033;&#29992;&#36317;&#31163;&#30697;&#38453;&#20013;&#20016;&#23500;&#30340;&#20960;&#20309;&#32467;&#26500;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#27169;&#22411;&#30340;&#39640;&#34920;&#36798;&#33021;&#21147;&#65292;&#24182;&#35777;&#26126;&#20102;&#19968;&#20123;&#29616;&#26377;&#30340;&#31934;&#24515;&#35774;&#35745;&#30340;&#20960;&#20309;&#27169;&#22411;&#21487;&#20197;&#20316;&#20026;$k$-DisGNNs&#30340;&#29305;&#27530;&#24773;&#20917;&#32479;&#19968;&#36215;&#26469;&#12290;&#26368;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#21644;&#20256;&#32479;&#22270;&#34920;&#31034;&#23398;&#20064;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#23637;&#31034;&#20102;&#37027;&#20123;&#26368;&#21021;&#20026;&#20302;&#24230;&#34920;&#36798;&#33021;&#21147;&#30340;GNN&#27169;&#22411;&#35774;&#35745;&#30340;&#39640;&#24230;&#34920;&#36798;&#21147;&#30340;GNN&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) are often used for tasks involving the geometry of a given graph, such as molecular dynamics simulation. Although the distance matrix of a geometric graph contains complete geometric information, it has been demonstrated that Message Passing Neural Networks (MPNNs) are insufficient for learning this geometry. In this work, we expand on the families of counterexamples that MPNNs are unable to distinguish from their distance matrices, by constructing families of novel and symmetric geometric graphs. We then propose $k$-DisGNNs, which can effectively exploit the rich geometry contained in the distance matrix. We demonstrate the high expressive power of our models and prove that some existing well-designed geometric models can be unified by $k$-DisGNNs as special cases. Most importantly, we establish a connection between geometric deep learning and traditional graph representation learning, showing that those highly expressive GNN models originally designed for
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#21457;&#29616;&#65292;&#21487;&#23398;&#20064;&#27604;&#20363;&#22240;&#23376;&#21152;&#21095;&#20102;&#26435;&#37325;&#25391;&#33633;&#65292;&#26412;&#25991;&#25552;&#20986;&#19977;&#31181;&#25216;&#26415;&#20197;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#19978;&#26174;&#33879;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2302.02210</link><description>&lt;p&gt;
&#20302;&#20301;&#35270;&#35273;&#21464;&#21387;&#22120;&#30340;&#26080;&#25391;&#33633;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Oscillation-free Quantization for Low-bit Vision Transformers. (arXiv:2302.02210v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.02210
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#21457;&#29616;&#65292;&#21487;&#23398;&#20064;&#27604;&#20363;&#22240;&#23376;&#21152;&#21095;&#20102;&#26435;&#37325;&#25391;&#33633;&#65292;&#26412;&#25991;&#25552;&#20986;&#19977;&#31181;&#25216;&#26415;&#20197;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#19978;&#26174;&#33879;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#21270;&#24847;&#35782;&#35757;&#32451;&#30340;&#19968;&#20010;&#19981;&#33391;&#21103;&#20316;&#29992;&#26159;&#26435;&#37325;&#25391;&#33633;&#65292;&#20854;&#20013;&#37327;&#21270;&#26435;&#37325;&#32463;&#24120;&#22312;&#20004;&#20010;&#37327;&#21270;&#32423;&#21035;&#20043;&#38388;&#36339;&#21160;&#65292;&#23548;&#33268;&#35757;&#32451;&#19981;&#31283;&#23450;&#21644;&#23376;&#20248;&#21270;&#30340;&#26368;&#32456;&#27169;&#22411;&#12290;&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#21487;&#23398;&#20064;&#30340;&#27604;&#20363;&#22240;&#23376;&#8212;&#8212;&#22312;&#37327;&#21270;&#20013;&#24191;&#27867;&#20351;&#29992;&#30340;$\textit{de facto}$&#35774;&#32622;&#8212;&#8212;&#21152;&#21095;&#20102;&#26435;&#37325;&#25391;&#33633;&#12290;&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#21487;&#23398;&#20064;&#27604;&#20363;&#22240;&#23376;&#19982;&#37327;&#21270;&#26435;&#37325;&#25391;&#33633;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#20197;ViT&#20026;&#26696;&#20363;&#26469;&#35828;&#26126;&#21457;&#29616;&#21644;&#35299;&#20915;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#21457;&#29616;&#33258;&#27880;&#24847;&#21147;&#23618;&#20013;&#37327;&#21270;&#26435;&#37325;&#30340;$\textit{query}$&#21644;$\textit{key}$&#20043;&#38388;&#30340;&#30456;&#20114;&#20381;&#23384;&#20351;ViT&#23481;&#26131;&#21463;&#21040;&#25391;&#33633;&#30340;&#24433;&#21709;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30456;&#24212;&#22320;&#25552;&#20986;&#20102;&#19977;&#31181;&#25216;&#26415;&#65306;&#32479;&#35745;&#26435;&#37325;&#37327;&#21270;&#65288;$\rm StatsQ$&#65289;&#20197;&#25913;&#21892;&#37327;&#21270;&#40065;&#26834;&#24615;&#65292;&#19982;&#26222;&#36941;&#20351;&#29992;&#30340;&#21487;&#23398;&#20064;&#27604;&#20363;&#22240;&#23376;&#26041;&#27861;&#30456;&#27604;&#65307;&#32622;&#20449;&#24230;&#24341;&#23548;&#30340;&#36864;&#28779;&#65288;$\rm CGA$&#65289;&#22312;&#35757;&#32451;&#26399;&#38388;&#20923;&#32467;&#20855;&#26377;$\textit{&#39640;&#32622;&#20449;&#24230;}$&#30340;&#26435;&#37325;&#65292;&#20197;&#20943;&#23569;&#26435;&#37325;&#25391;&#33633;&#65307;&#20197;&#21450;&#30456;&#20114;&#20381;&#36182;&#26435;&#37325;&#30340;&#22343;&#34913;&#65288;$\rm IWEqual$&#65289;&#65292;&#20197;&#26377;&#25928;&#22788;&#29702;&#30456;&#20114;&#20381;&#36182;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30456;&#27604;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#19978;&#26174;&#33879;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Weight oscillation is an undesirable side effect of quantization-aware training, in which quantized weights frequently jump between two quantized levels, resulting in training instability and a sub-optimal final model. We discover that the learnable scaling factor, a widely-used $\textit{de facto}$ setting in quantization aggravates weight oscillation. In this study, we investigate the connection between the learnable scaling factor and quantized weight oscillation and use ViT as a case driver to illustrate the findings and remedies. In addition, we also found that the interdependence between quantized weights in $\textit{query}$ and $\textit{key}$ of a self-attention layer makes ViT vulnerable to oscillation. We, therefore, propose three techniques accordingly: statistical weight quantization ($\rm StatsQ$) to improve quantization robustness compared to the prevalent learnable-scale-based method; confidence-guided annealing ($\rm CGA$) that freezes the weights with $\textit{high confi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#21487;&#33021;&#20219;&#20309;&#26102;&#20505;&#23433;&#20840;&#30340;&#38543;&#26426;&#32452;&#21512;&#21322;&#33218;&#36172;&#21338;&#38382;&#39064;&#65292;&#24182;&#35774;&#35745;&#20986;&#31639;&#27861;PASCombUCB&#22312;&#26102;&#38388;&#36724;&#19978;&#26368;&#23567;&#21270;&#21518;&#24724;&#20540;&#12290;</title><link>http://arxiv.org/abs/2301.13393</link><description>&lt;p&gt;
&#21487;&#33021;&#20219;&#20309;&#26102;&#20505;&#23433;&#20840;&#30340;&#38543;&#26426;&#32452;&#21512;&#21322;&#33218;&#36172;&#21338;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Probably Anytime-Safe Stochastic Combinatorial Semi-Bandits. (arXiv:2301.13393v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.13393
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#21487;&#33021;&#20219;&#20309;&#26102;&#20505;&#23433;&#20840;&#30340;&#38543;&#26426;&#32452;&#21512;&#21322;&#33218;&#36172;&#21338;&#38382;&#39064;&#65292;&#24182;&#35774;&#35745;&#20986;&#31639;&#27861;PASCombUCB&#22312;&#26102;&#38388;&#36724;&#19978;&#26368;&#23567;&#21270;&#21518;&#24724;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#35299;&#20915;&#22312;&#32447;&#20915;&#31574;&#20013;&#21487;&#33021;&#36896;&#25104;&#36807;&#24230;&#39118;&#38505;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#21487;&#33021;&#20219;&#20309;&#26102;&#20505;&#23433;&#20840;&#30340;&#38543;&#26426;&#32452;&#21512;&#21322;&#33218;&#36172;&#21338;&#38382;&#39064;&#12290;&#22312;&#36825;&#20010;&#38382;&#39064;&#20013;&#65292;&#26234;&#33021;&#20307;&#26377;&#36873;&#25321;&#20174;$L$&#20010;&#22522;&#30784;&#39033;&#20013;&#19981;&#36229;&#36807;$K$&#20010;&#36827;&#34892;&#23376;&#38598;&#30340;&#36873;&#39033;&#12290;&#27599;&#20010;&#20803;&#32032;&#37117;&#19982;&#26576;&#20010;&#24179;&#22343;&#22870;&#21169;&#21644;&#34920;&#31034;&#20854;&#39118;&#38505;&#30340;&#26041;&#24046;&#30456;&#20851;&#32852;&#12290;&#20026;&#20102;&#20943;&#23569;&#20195;&#29702;&#20154;&#25152;&#36973;&#21463;&#30340;&#39118;&#38505;&#65292;&#25105;&#20204;&#35201;&#27714;&#65292;&#22312;&#25972;&#20010;&#26102;&#38388;$T$&#30340;&#26102;&#38388;&#36328;&#24230;&#19978;&#65292;&#26234;&#33021;&#20307;&#25152;&#20570;&#30340;&#27599;&#20010;&#36873;&#25321;&#37117;&#24212;&#21253;&#21547;&#20854;&#26041;&#24046;&#20043;&#21644;&#19981;&#36229;&#36807;&#26576;&#20010;&#26041;&#24046;&#39044;&#31639;&#30340;&#20803;&#32032;&#65292;&#19988;&#20854;&#21487;&#33021;&#20219;&#20309;&#26102;&#20505;&#28385;&#36275;&#27492;&#32422;&#26463;&#12290;&#22312;&#27492;&#32422;&#26463;&#26465;&#20214;&#19979;&#65292;&#25105;&#20204;&#35774;&#35745;&#21644;&#20998;&#26512;&#20102;&#19968;&#31181;&#31639;&#27861;PASCombUCB&#65292;&#20197;&#22312;&#26102;&#38388;&#36724;&#19978;&#26368;&#23567;&#21270;&#21518;&#24724;&#20540;&#12290;&#36890;&#36807;&#24320;&#21457;&#37197;&#22871;&#20449;&#24687;&#29702;&#35770;&#19979;&#30028;&#65292;&#25105;&#20204;&#35777;&#26126;&#22312;&#38382;&#39064;&#30456;&#20851;&#21644;&#38382;&#39064;&#26080;&#20851;&#30340;&#20004;&#31181;&#33539;&#20363;&#19979;&#65292;&#31639;&#27861;&#37117;&#26159;&#26368;&#20248;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Motivated by concerns about making online decisions that incur undue amount of risk at each time step, in this paper, we formulate the probably anytime-safe stochastic combinatorial semi-bandits problem. In this problem, the agent is given the option to select a subset of size at most $K$ from a set of $L$ ground items. Each item is associated to a certain mean reward as well as a variance that represents its risk. To mitigate the risk that the agent incurs, we require that with probability at least $1-\delta$, over the entire horizon of time $T$, each of the choices that the agent makes should contain items whose sum of variances does not exceed a certain variance budget. We call this probably anytime-safe constraint. Under this constraint, we design and analyze an algorithm {\sc PASCombUCB} that minimizes the regret over the horizon of time $T$. By developing accompanying information-theoretic lower bounds, we show that under both the problem-dependent and problem-independent paradig
&lt;/p&gt;</description></item><item><title>CHAD&#26159;&#19968;&#20010;&#21830;&#19994;&#20572;&#36710;&#22330;&#29615;&#22659;&#30340;&#39640;&#20998;&#36776;&#29575;&#12289;&#22810;&#30456;&#26426;&#24322;&#24120;&#25968;&#25454;&#38598;&#65292;&#26159;&#31532;&#19968;&#20010;&#20026;&#27599;&#20010;&#28436;&#21592;&#21253;&#25324;&#36793;&#30028;&#26694;&#12289;&#36523;&#20221;&#21644;&#23039;&#21183;&#27880;&#37322;&#30340;&#24322;&#24120;&#25968;&#25454;&#38598;&#65292;&#21516;&#26102;&#21253;&#25324;&#21508;&#31181;&#29615;&#22659;&#22240;&#32032;&#12290;&#20351;&#29992;CHAD&#24378;&#35843;&#20102;&#26356;&#20934;&#30830;&#30340;&#24322;&#24120;&#26816;&#27979;&#31639;&#27861;&#25152;&#38656;&#30340;&#29616;&#23454;&#25968;&#25454;&#38598;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2212.09258</link><description>&lt;p&gt;
CHAD&#65306;&#22799;&#27931;&#29305;&#24322;&#24120;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
CHAD: Charlotte Anomaly Dataset. (arXiv:2212.09258v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09258
&lt;/p&gt;
&lt;p&gt;
CHAD&#26159;&#19968;&#20010;&#21830;&#19994;&#20572;&#36710;&#22330;&#29615;&#22659;&#30340;&#39640;&#20998;&#36776;&#29575;&#12289;&#22810;&#30456;&#26426;&#24322;&#24120;&#25968;&#25454;&#38598;&#65292;&#26159;&#31532;&#19968;&#20010;&#20026;&#27599;&#20010;&#28436;&#21592;&#21253;&#25324;&#36793;&#30028;&#26694;&#12289;&#36523;&#20221;&#21644;&#23039;&#21183;&#27880;&#37322;&#30340;&#24322;&#24120;&#25968;&#25454;&#38598;&#65292;&#21516;&#26102;&#21253;&#25324;&#21508;&#31181;&#29615;&#22659;&#22240;&#32032;&#12290;&#20351;&#29992;CHAD&#24378;&#35843;&#20102;&#26356;&#20934;&#30830;&#30340;&#24322;&#24120;&#26816;&#27979;&#31639;&#27861;&#25152;&#38656;&#30340;&#29616;&#23454;&#25968;&#25454;&#38598;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#35270;&#39057;&#24322;&#24120;&#26816;&#27979;&#20013;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#20854;&#20013;&#31639;&#27861;&#24517;&#39035;&#30830;&#23450;&#35270;&#39057;&#30340;&#29305;&#23450;&#24103;&#26159;&#21542;&#21253;&#21547;&#24322;&#24120;&#34892;&#20026;&#12290;&#28982;&#32780;&#65292;&#35270;&#39057;&#24322;&#24120;&#26816;&#27979;&#29305;&#21035;&#22240;&#24773;&#22659;&#32780;&#24322;&#65292;&#32780;&#20195;&#34920;&#24615;&#25968;&#25454;&#38598;&#30340;&#21487;&#29992;&#24615;&#20005;&#37325;&#38480;&#21046;&#20102;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#20934;&#30830;&#24615;&#12290;&#27492;&#22806;&#65292;&#30446;&#21069;&#22823;&#22810;&#25968;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#25253;&#21578;&#30340;&#25351;&#26631;&#36890;&#24120;&#19981;&#33021;&#21453;&#26144;&#27169;&#22411;&#22312;&#30495;&#23454;&#22330;&#26223;&#20013;&#30340;&#34920;&#29616;&#22914;&#20309;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#22799;&#27931;&#29305;&#24322;&#24120;&#25968;&#25454;&#38598;&#65288;CHAD&#65289;&#12290;CHAD&#26159;&#21830;&#19994;&#20572;&#36710;&#22330;&#29615;&#22659;&#20013;&#30340;&#39640;&#20998;&#36776;&#29575;&#12289;&#22810;&#30456;&#26426;&#24322;&#24120;&#25968;&#25454;&#38598;&#12290;&#38500;&#20102;&#24103;&#32423;&#21035;&#30340;&#24322;&#24120;&#26631;&#31614;&#22806;&#65292;CHAD&#36824;&#26159;&#31532;&#19968;&#20010;&#20026;&#27599;&#20010;&#28436;&#21592;&#21253;&#25324;&#36793;&#30028;&#26694;&#12289;&#36523;&#20221;&#21644;&#23039;&#21183;&#27880;&#37322;&#30340;&#24322;&#24120;&#25968;&#25454;&#38598;&#12290;&#36825;&#23545;&#20110;&#22522;&#20110;&#39592;&#39612;&#30340;&#24322;&#24120;&#26816;&#27979;&#29305;&#21035;&#26377;&#30410;&#65292;&#22240;&#20026;&#23427;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#20855;&#26377;&#36739;&#20302;&#30340;&#35745;&#31639;&#38656;&#27714;&#12290;CHAD&#20063;&#26159;&#31532;&#19968;&#20010;&#21253;&#25324;&#21508;&#31181;&#29615;&#22659;&#22240;&#32032;&#65288;&#20363;&#22914;&#20809;&#29031;&#21464;&#21270;&#12289;&#22825;&#27668;&#26465;&#20214;&#21464;&#21270;&#21644;&#25668;&#20687;&#26426;&#35270;&#35282;&#65289;&#30340;&#24322;&#24120;&#25968;&#25454;&#38598;&#12290;&#20351;&#29992;CHAD&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20123;&#29615;&#22659;&#22240;&#32032;&#22914;&#20309;&#23545;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#25552;&#20986;&#20102;&#37325;&#22823;&#25361;&#25112;&#65292;&#26368;&#32456;&#24378;&#35843;&#20102;&#26356;&#20934;&#30830;&#30340;&#24322;&#24120;&#26816;&#27979;&#31639;&#27861;&#25152;&#38656;&#30340;&#29616;&#23454;&#25968;&#25454;&#38598;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, we have seen a significant interest in data-driven deep learning approaches for video anomaly detection, where an algorithm must determine if specific frames of a video contain abnormal behaviors. However, video anomaly detection is particularly context-specific, and the availability of representative datasets heavily limits real-world accuracy. Additionally, the metrics currently reported by most state-of-the-art methods often do not reflect how well the model will perform in real-world scenarios. In this article, we present the Charlotte Anomaly Dataset (CHAD). CHAD is a high-resolution, multi-camera anomaly dataset in a commercial parking lot setting. In addition to frame-level anomaly labels, CHAD is the first anomaly dataset to include bounding box, identity, and pose annotations for each actor. This is especially beneficial for skeleton-based anomaly detection, which is useful for its lower computational demand in real-world settings. CHAD is also the first anoma
&lt;/p&gt;</description></item><item><title>BKinD-3D&#26159;&#19968;&#31181;&#26032;&#30340;&#26080;&#38656;2D&#25110;3D&#30417;&#30563;&#30340;&#33258;&#30417;&#30563;&#19977;&#32500;&#20851;&#38190;&#28857;&#21457;&#29616;&#26041;&#27861;&#65292;&#20351;&#29992;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#21644;3D&#20307;&#31215;&#28909;&#21147;&#22270;&#23545;&#22810;&#20010;&#35270;&#22270;&#20043;&#38388;&#30340;&#26102;&#31354;&#24046;&#24322;&#36827;&#34892;&#37325;&#24314;&#65292;&#36824;&#20351;&#29992;&#20102;&#19968;&#20010;&#23398;&#20064;&#21040;&#30340;3D&#20027;&#20307;&#39592;&#26550;&#30340;&#20851;&#33410;&#38271;&#24230;&#32422;&#26463;&#12290;</title><link>http://arxiv.org/abs/2212.07401</link><description>&lt;p&gt;
BKinD-3D&#65306;&#33258;&#30417;&#30563;&#22810;&#35270;&#28857;&#35270;&#39057;&#20013;&#30340;&#19977;&#32500;&#20851;&#38190;&#28857;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
BKinD-3D: Self-Supervised 3D Keypoint Discovery from Multi-View Videos. (arXiv:2212.07401v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.07401
&lt;/p&gt;
&lt;p&gt;
BKinD-3D&#26159;&#19968;&#31181;&#26032;&#30340;&#26080;&#38656;2D&#25110;3D&#30417;&#30563;&#30340;&#33258;&#30417;&#30563;&#19977;&#32500;&#20851;&#38190;&#28857;&#21457;&#29616;&#26041;&#27861;&#65292;&#20351;&#29992;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#21644;3D&#20307;&#31215;&#28909;&#21147;&#22270;&#23545;&#22810;&#20010;&#35270;&#22270;&#20043;&#38388;&#30340;&#26102;&#31354;&#24046;&#24322;&#36827;&#34892;&#37325;&#24314;&#65292;&#36824;&#20351;&#29992;&#20102;&#19968;&#20010;&#23398;&#20064;&#21040;&#30340;3D&#20027;&#20307;&#39592;&#26550;&#30340;&#20851;&#33410;&#38271;&#24230;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#21270;&#19977;&#32500;&#36816;&#21160;&#23545;&#20110;&#30740;&#31350;&#20154;&#31867;&#21644;&#20854;&#20182;&#21160;&#29289;&#30340;&#34892;&#20026;&#38750;&#24120;&#37325;&#35201;&#65292;&#20294;&#25163;&#21160;&#23039;&#24577;&#27880;&#37322;&#33719;&#21462;&#25104;&#26412;&#39640;&#19988;&#32791;&#26102;&#12290;&#33258;&#30417;&#30563;&#20851;&#38190;&#28857;&#21457;&#29616;&#26159;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#31574;&#30053;&#65292;&#21487;&#20197;&#22312;&#27809;&#26377;&#27880;&#37322;&#30340;&#24773;&#20917;&#19979;&#20272;&#35745;&#19977;&#32500;&#23039;&#24577;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#20851;&#38190;&#28857;&#21457;&#29616;&#26041;&#27861;&#36890;&#24120;&#20165;&#22788;&#29702;&#21333;&#20010;2D&#35270;&#22270;&#24182;&#19988;&#19981;&#22312;3D&#31354;&#38388;&#25805;&#20316;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#34892;&#20026;&#20027;&#20307;&#30340;&#22810;&#35270;&#28857;&#35270;&#39057;&#20013;&#25191;&#34892;&#26080;2D&#25110;3D&#20851;&#38190;&#28857;&#25110;&#36793;&#30028;&#26694;&#30417;&#30563;&#30340;&#33258;&#30417;&#30563;&#19977;&#32500;&#20851;&#38190;&#28857;&#21457;&#29616;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;BKinD-3D&#20351;&#29992;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#21644;3D&#20307;&#31215;&#28909;&#21147;&#22270;&#65292;&#35757;&#32451;&#20197;&#37325;&#24314;&#22810;&#20010;&#35270;&#22270;&#20043;&#38388;&#30340;&#26102;&#31354;&#24046;&#24322;&#65292;&#27492;&#22806;&#36824;&#20351;&#29992;&#20102;&#23398;&#20064;&#21040;&#30340;&#20027;&#20307;&#19977;&#32500;&#39592;&#26550;&#30340;&#20851;&#33410;&#38271;&#24230;&#32422;&#26463;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#22312;&#27809;&#26377;&#25163;&#21160;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#65292;&#22312;&#20154;&#31867;&#21644;&#32769;&#40736;&#30340;&#35270;&#39057;&#20013;&#21457;&#29616;&#20102;&#20851;&#38190;&#28857;&#65292;&#23637;&#31034;&#20102;&#19977;&#32500;&#20851;&#38190;&#28857;&#21457;&#29616;&#22312;&#30740;&#31350;&#34892;&#20026;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantifying motion in 3D is important for studying the behavior of humans and other animals, but manual pose annotations are expensive and time-consuming to obtain. Self-supervised keypoint discovery is a promising strategy for estimating 3D poses without annotations. However, current keypoint discovery approaches commonly process single 2D views and do not operate in the 3D space. We propose a new method to perform self-supervised keypoint discovery in 3D from multi-view videos of behaving agents, without any keypoint or bounding box supervision in 2D or 3D. Our method, BKinD-3D, uses an encoder-decoder architecture with a 3D volumetric heatmap, trained to reconstruct spatiotemporal differences across multiple views, in addition to joint length constraints on a learned 3D skeleton of the subject. In this way, we discover keypoints without requiring manual supervision in videos of humans and rats, demonstrating the potential of 3D keypoint discovery for studying behavior.
&lt;/p&gt;</description></item><item><title>PATO&#26159;&#19968;&#20010;&#31574;&#30053;&#36741;&#21161;&#30340;&#36828;&#31243;&#25805;&#20316;&#31995;&#32479;&#65292;&#36890;&#36807;&#33258;&#21160;&#25191;&#34892;&#37096;&#20998;&#28436;&#31034;&#37319;&#38598;&#36807;&#31243;&#65292;&#24182;&#20165;&#22312;&#19981;&#30830;&#23450;&#35201;&#25191;&#34892;&#21738;&#20010;&#23376;&#20219;&#21153;&#25110;&#34892;&#20026;&#26102;&#35831;&#27714;&#20154;&#31867;&#36755;&#20837;&#65292;&#25552;&#39640;&#20102;&#25968;&#25454;&#37319;&#38598;&#25928;&#29575;&#65292;&#21516;&#26102;&#20943;&#23569;&#20102;&#20154;&#24037;&#25805;&#20316;&#21592;&#30340;&#24515;&#29702;&#36127;&#25285;&#65292;&#23454;&#29616;&#21333;&#20010;&#25805;&#20316;&#21592;&#24182;&#34892;&#25511;&#21046;&#22810;&#20010;&#26426;&#22120;&#20154;&#30340;&#21151;&#33021;&#12290;</title><link>http://arxiv.org/abs/2212.04708</link><description>&lt;p&gt;
PATO: &#22522;&#20110;&#31574;&#30053;&#36741;&#21161;&#30340;&#21487;&#20280;&#32553;&#26426;&#22120;&#20154;&#25968;&#25454;&#37319;&#38598;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
PATO: Policy Assisted TeleOperation for Scalable Robot Data Collection. (arXiv:2212.04708v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.04708
&lt;/p&gt;
&lt;p&gt;
PATO&#26159;&#19968;&#20010;&#31574;&#30053;&#36741;&#21161;&#30340;&#36828;&#31243;&#25805;&#20316;&#31995;&#32479;&#65292;&#36890;&#36807;&#33258;&#21160;&#25191;&#34892;&#37096;&#20998;&#28436;&#31034;&#37319;&#38598;&#36807;&#31243;&#65292;&#24182;&#20165;&#22312;&#19981;&#30830;&#23450;&#35201;&#25191;&#34892;&#21738;&#20010;&#23376;&#20219;&#21153;&#25110;&#34892;&#20026;&#26102;&#35831;&#27714;&#20154;&#31867;&#36755;&#20837;&#65292;&#25552;&#39640;&#20102;&#25968;&#25454;&#37319;&#38598;&#25928;&#29575;&#65292;&#21516;&#26102;&#20943;&#23569;&#20102;&#20154;&#24037;&#25805;&#20316;&#21592;&#30340;&#24515;&#29702;&#36127;&#25285;&#65292;&#23454;&#29616;&#21333;&#20010;&#25805;&#20316;&#21592;&#24182;&#34892;&#25511;&#21046;&#22810;&#20010;&#26426;&#22120;&#20154;&#30340;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#25968;&#25454;&#26159;&#26426;&#22120;&#23398;&#20064;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#27491;&#22914;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#30740;&#31350;&#30340;&#26368;&#26032;&#36827;&#23637;&#20013;&#25152;&#35777;&#26126;&#30340;&#37027;&#26679;&#12290;&#20294;&#26159;&#65292;&#37319;&#38598;&#22823;&#35268;&#27169;&#26426;&#22120;&#20154;&#25968;&#25454;&#30340;&#25104;&#26412;&#26356;&#39640;&#65292;&#36895;&#24230;&#26356;&#24930;&#65292;&#22240;&#20026;&#27599;&#20010;&#25805;&#20316;&#21592;&#21482;&#33021;&#21516;&#26102;&#25511;&#21046;&#19968;&#20010;&#26426;&#22120;&#20154;&#12290;&#20026;&#20102;&#20351;&#36825;&#20010;&#26114;&#36149;&#30340;&#25968;&#25454;&#37319;&#38598;&#36807;&#31243;&#39640;&#25928;&#21644;&#21487;&#20280;&#32553;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31574;&#30053;&#36741;&#21161;&#30340;&#36828;&#31243;&#25805;&#20316;&#31995;&#32479;&#65288;PATO&#65289;&#65292;&#35813;&#31995;&#32479;&#21033;&#29992;&#23398;&#20064;&#21040;&#30340;&#36741;&#21161;&#31574;&#30053;&#33258;&#21160;&#25191;&#34892;&#37096;&#20998;&#28436;&#31034;&#37319;&#38598;&#36807;&#31243;&#12290;PATO&#22312;&#25968;&#25454;&#37319;&#38598;&#20013;&#33258;&#21160;&#25191;&#34892;&#37325;&#22797;&#30340;&#34892;&#20026;&#65292;&#24182;&#20165;&#22312;&#19981;&#30830;&#23450;&#35201;&#25191;&#34892;&#21738;&#20010;&#23376;&#20219;&#21153;&#25110;&#34892;&#20026;&#26102;&#35831;&#27714;&#20154;&#31867;&#36755;&#20837;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19982;&#30495;&#23454;&#26426;&#22120;&#20154;&#21644;&#27169;&#25311;&#26426;&#22120;&#20154;&#32676;&#20307;&#30340;&#36828;&#31243;&#25805;&#20316;&#29992;&#25143;&#30740;&#31350;&#65292;&#24182;&#35777;&#26126;&#25105;&#20204;&#30340;&#36741;&#21161;&#36828;&#31243;&#25805;&#20316;&#31995;&#32479;&#33021;&#22815;&#20943;&#23569;&#20154;&#24037;&#25805;&#20316;&#21592;&#30340;&#24515;&#29702;&#36127;&#25285;&#65292;&#21516;&#26102;&#25552;&#39640;&#25968;&#25454;&#37319;&#38598;&#25928;&#29575;&#12290;&#27492;&#22806;&#65292;&#23427;&#20351;&#21333;&#20010;&#25805;&#20316;&#21592;&#33021;&#22815;&#24182;&#34892;&#25511;&#21046;&#22810;&#20010;&#26426;&#22120;&#20154;&#65292;&#36825;&#26159;&#39318;&#27425;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large-scale data is an essential component of machine learning as demonstrated in recent advances in natural language processing and computer vision research. However, collecting large-scale robotic data is much more expensive and slower as each operator can control only a single robot at a time. To make this costly data collection process efficient and scalable, we propose Policy Assisted TeleOperation (PATO), a system which automates part of the demonstration collection process using a learned assistive policy. PATO autonomously executes repetitive behaviors in data collection and asks for human input only when it is uncertain about which subtask or behavior to execute. We conduct teleoperation user studies both with a real robot and a simulated robot fleet and demonstrate that our assisted teleoperation system reduces human operators' mental load while improving data collection efficiency. Further, it enables a single operator to control multiple robots in parallel, which is a first
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20154;&#31867;&#20559;&#22909;&#30340;&#25512;&#33616;&#31995;&#32479;&#33539;&#24335;PrefRec&#65292;&#20801;&#35768;&#24378;&#21270;&#23398;&#20064;&#25512;&#33616;&#31995;&#32479;&#20174;&#29992;&#25143;&#21382;&#21490;&#34892;&#20026;&#20559;&#22909;&#20013;&#23398;&#20064;&#65292;&#20197;&#20248;&#21270;&#38271;&#26399;&#29992;&#25143;&#21442;&#19982;&#24230;&#12290;</title><link>http://arxiv.org/abs/2212.02779</link><description>&lt;p&gt;
PrefRec&#65306;&#21033;&#29992;&#20154;&#31867;&#20559;&#22909;&#21152;&#24378;&#38271;&#26399;&#29992;&#25143;&#21442;&#19982;&#30340;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
PrefRec: Recommender Systems with Human Preferences for Reinforcing Long-term User Engagement. (arXiv:2212.02779v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.02779
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20154;&#31867;&#20559;&#22909;&#30340;&#25512;&#33616;&#31995;&#32479;&#33539;&#24335;PrefRec&#65292;&#20801;&#35768;&#24378;&#21270;&#23398;&#20064;&#25512;&#33616;&#31995;&#32479;&#20174;&#29992;&#25143;&#21382;&#21490;&#34892;&#20026;&#20559;&#22909;&#20013;&#23398;&#20064;&#65292;&#20197;&#20248;&#21270;&#38271;&#26399;&#29992;&#25143;&#21442;&#19982;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#65292;&#25512;&#33616;&#31995;&#32479;&#22312;&#20248;&#21270;&#21363;&#26102;&#21442;&#19982;&#26041;&#38754;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#26356;&#21487;&#21462;&#30340;&#32489;&#25928;&#25351;&#26631;&#38271;&#26399;&#29992;&#25143;&#21442;&#19982;&#24230;&#30340;&#25552;&#39640;&#20173;&#28982;&#24456;&#38590;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#26368;&#36817;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;&#21508;&#31181;&#38271;&#26399;&#30446;&#26631;&#20248;&#21270;&#20219;&#21153;&#20013;&#23637;&#29616;&#20102;&#26377;&#25928;&#24615;&#12290;&#22240;&#27492;&#65292;&#24378;&#21270;&#23398;&#20064;&#34987;&#24191;&#27867;&#35748;&#20026;&#26159;&#20248;&#21270;&#25512;&#33616;&#20013;&#38271;&#26399;&#29992;&#25143;&#21442;&#19982;&#24230;&#30340;&#26377;&#21069;&#36884;&#30340;&#26694;&#26550;&#12290;&#34429;&#28982;&#26377;&#21069;&#36884;&#65292;&#20294;&#24212;&#29992;&#24378;&#21270;&#23398;&#20064;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#31934;&#24515;&#35774;&#35745;&#30340;&#22870;&#21169;&#65292;&#20294;&#35774;&#35745;&#19982;&#38271;&#26399;&#29992;&#25143;&#21442;&#19982;&#26377;&#20851;&#30340;&#22870;&#21169;&#30456;&#24403;&#22256;&#38590;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33539;&#24335;&#65292;&#21363;&#20197;&#20154;&#31867;&#20559;&#22909;&#20026;&#22522;&#30784;&#30340;&#25512;&#33616;&#31995;&#32479;&#65292;&#20801;&#35768;&#24378;&#21270;&#23398;&#20064;&#25512;&#33616;&#31995;&#32479;&#20174;&#26377;&#20851;&#29992;&#25143;&#21382;&#21490;&#34892;&#20026;&#30340;&#20559;&#22909;&#20013;&#23398;&#20064;&#65292;&#32780;&#19981;&#26159;&#20174;&#26126;&#30830;&#23450;&#20041;&#30340;&#22870;&#21169;&#20013;&#23398;&#20064;&#12290;&#36825;&#20123;&#20559;&#22909;&#21487;&#20197;&#36890;&#36807;&#20247;&#21253;&#31561;&#25216;&#26415;&#36731;&#26494;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current advances in recommender systems have been remarkably successful in optimizing immediate engagement. However, long-term user engagement, a more desirable performance metric, remains difficult to improve. Meanwhile, recent reinforcement learning (RL) algorithms have shown their effectiveness in a variety of long-term goal optimization tasks. For this reason, RL is widely considered as a promising framework for optimizing long-term user engagement in recommendation. Though promising, the application of RL heavily relies on well-designed rewards, but designing rewards related to long-term user engagement is quite difficult. To mitigate the problem, we propose a novel paradigm, recommender systems with human preferences (or Preference-based Recommender systems), which allows RL recommender systems to learn from preferences about users historical behaviors rather than explicitly defined rewards. Such preferences are easily accessible through techniques such as crowdsourcing, as they 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#23618;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#34701;&#21512;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#35757;&#32451;&#21644;&#24494;&#35843;&#26041;&#27861;&#26469;&#36827;&#34892;&#25991;&#26412;&#20013;&#30340;&#31163;&#32676;&#26816;&#27979;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2211.11300</link><description>&lt;p&gt;
&#25991;&#26412;&#20013;&#30340;&#31163;&#32676;&#26816;&#27979;&#30340;&#22810;&#23618;&#30693;&#35782;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Multi-Level Knowledge Distillation for Out-of-Distribution Detection in Text. (arXiv:2211.11300v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.11300
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#23618;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#34701;&#21512;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#35757;&#32451;&#21644;&#24494;&#35843;&#26041;&#27861;&#26469;&#36827;&#34892;&#25991;&#26412;&#20013;&#30340;&#31163;&#32676;&#26816;&#27979;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#24050;&#32463;&#35777;&#26126;&#26159;&#21482;&#20351;&#29992;&#20869;&#20998;&#24067;(ID)&#26679;&#20363;&#25991;&#26412;&#36827;&#34892;&#31163;&#32676;&#26816;&#27979;&#30340;&#23453;&#36149;&#32452;&#25104;&#37096;&#20998;&#12290;&#36825;&#20123;&#26041;&#27861;&#35201;&#20040;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#35201;&#20040;&#36890;&#36807;&#20351;&#29992;ID&#26679;&#20363;&#24494;&#35843;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#28982;&#21518;&#23558;&#35821;&#35328;&#27169;&#22411;&#36755;&#20986;&#30340;&#22256;&#24785;&#24230;&#20316;&#20026;&#31163;&#32676;&#24471;&#20998;&#12290;&#26412;&#25991;&#20998;&#26512;&#20102;&#20004;&#31181;&#31163;&#32676;&#26816;&#27979;&#26041;&#27861;&#30340;&#20114;&#34917;&#29305;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#34701;&#21512;&#23427;&#20204;&#20248;&#21183;&#24182;&#20943;&#36731;&#23427;&#20204;&#30340;&#23616;&#38480;&#24615;&#30340;&#22810;&#23618;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20351;&#29992;&#24494;&#35843;&#27169;&#22411;&#20316;&#20026;&#32769;&#24072;&#65292;&#22312;ID&#31034;&#20363;&#19978;&#25945;&#25480;&#19968;&#20010;&#38543;&#26426;&#21021;&#22987;&#21270;&#30340;&#23398;&#29983;&#27169;&#22411;&#12290;&#38500;&#20102;&#39044;&#27979;&#23618;&#33976;&#39311;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30456;&#20284;&#24615;&#30340;&#20013;&#38388;&#23618;&#33976;&#39311;&#26041;&#27861;&#65292;&#20197;&#20840;&#38754;&#25506;&#32034;&#32769;&#24072;&#27169;&#22411;&#30340;&#34920;&#31034;&#31354;&#38388;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#23398;&#20064;&#30340;&#23398;&#29983;&#21487;&#20197;&#26356;&#22909;&#22320;&#34920;&#31034;ID&#25968;&#25454;&#27969;&#24418;&#65292;&#21516;&#26102;&#33719;&#24471;&#26356;&#24378;&#30340;&#23558;OoD&#31034;&#20363;&#26144;&#23556;&#21040;&#27969;&#24418;&#20043;&#22806;&#30340;&#33021;&#21147;&#12290;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#25105;&#20204;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#19982;&#31454;&#20105;&#22522;&#32447;&#30456;&#27604;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised representation learning has proved to be a valuable component for out-of-distribution (OoD) detection with only the texts of in-distribution (ID) examples. These approaches either train a language model from scratch or fine-tune a pre-trained language model using ID examples, and then take the perplexity output by the language model as OoD scores. In this paper, we analyze the complementary characteristics of both OoD detection methods and propose a multi-level knowledge distillation approach that integrates their strengths while mitigating their limitations. Specifically, we use a fine-tuned model as the teacher to teach a randomly initialized student model on the ID examples. Besides the prediction layer distillation, we present a similarity-based intermediate layer distillation method to thoroughly explore the representation space of the teacher model. In this way, the learned student can better represent the ID data manifold while gaining a stronger ability to map O
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#24577;&#36923;&#36753;&#30340;&#24418;&#24335;&#35821;&#35328;&#65292;&#29992;&#20110;&#25551;&#36848;&#21644;&#35299;&#37322;&#32479;&#35745;&#22240;&#26524;&#20851;&#31995;&#65292;&#24182;&#19988;&#33021;&#22815;&#25351;&#23450;&#21644;&#35299;&#37322;&#32479;&#35745;&#22240;&#26524;&#25512;&#26029;&#30340;&#27491;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2210.16751</link><description>&lt;p&gt;
&#22522;&#20110;&#27169;&#24577;&#36923;&#36753;&#30340;&#32479;&#35745;&#22240;&#26524;&#20851;&#31995;&#24418;&#24335;&#21270;
&lt;/p&gt;
&lt;p&gt;
Formalizing Statistical Causality via Modal Logic. (arXiv:2210.16751v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.16751
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#24577;&#36923;&#36753;&#30340;&#24418;&#24335;&#35821;&#35328;&#65292;&#29992;&#20110;&#25551;&#36848;&#21644;&#35299;&#37322;&#32479;&#35745;&#22240;&#26524;&#20851;&#31995;&#65292;&#24182;&#19988;&#33021;&#22815;&#25351;&#23450;&#21644;&#35299;&#37322;&#32479;&#35745;&#22240;&#26524;&#25512;&#26029;&#30340;&#27491;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25551;&#36848;&#21644;&#35299;&#37322;&#32479;&#35745;&#22240;&#26524;&#20851;&#31995;&#30340;&#24418;&#24335;&#35821;&#35328;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#32479;&#35745;&#22240;&#26524;&#35821;&#35328;&#65288;StaCL&#65289;&#65292;&#29992;&#20110;&#34920;&#36798;&#38543;&#26426;&#21464;&#37327;&#19978;&#30340;&#22240;&#26524;&#25928;&#24212;&#24182;&#25351;&#23450;&#22240;&#26524;&#25512;&#26029;&#30340;&#35201;&#27714;&#12290;StaCL&#36890;&#36807;&#24178;&#39044;&#30340;&#27169;&#24577;&#36816;&#31639;&#31526;&#65292;&#22312;&#19981;&#21516;&#21487;&#33021;&#30340;&#19990;&#30028;&#30340;&#27010;&#29575;&#20998;&#24067;&#20043;&#38388;&#34920;&#36798;&#22240;&#26524;&#23646;&#24615;&#65292;&#22312;Kripke&#27169;&#22411;&#20013;&#35299;&#37322;&#12290;&#25105;&#20204;&#20351;&#29992;StaCL&#20844;&#24335;&#27491;&#24335;&#21270;&#27010;&#29575;&#20998;&#24067;&#12289;&#24178;&#39044;&#21644;&#22240;&#26524;&#35859;&#35789;&#30340;&#20844;&#29702;&#12290;&#36825;&#20123;&#20844;&#29702;&#36275;&#22815;&#34920;&#36798;Pearl&#30340;do-calculus&#35268;&#21017;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#31034;&#20363;&#35777;&#26126;&#20102;StaCL&#21487;&#20197;&#29992;&#20110;&#25351;&#23450;&#21644;&#35299;&#37322;&#32479;&#35745;&#22240;&#26524;&#25512;&#26029;&#30340;&#27491;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a formal language for describing and explaining statistical causality. Concretely, we define Statistical Causality Language (StaCL) for expressing causal effects on random variables and specifying the requirements for causal inference. StaCL incorporates modal operators for interventions to express causal properties between probability distributions in different possible worlds in a Kripke model. We formalize axioms for probability distributions, interventions, and causal predicates using StaCL formulas. These axioms are expressive enough to derive the rules of Pearl's do-calculus. Finally, we demonstrate by examples that StaCL can be used to specify and explain the correctness of statistical causal inference.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#26412;&#22320;&#27169;&#22411;&#37325;&#26500;&#25915;&#20987;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#36890;&#36807;&#37325;&#26500;&#23458;&#25143;&#31471;&#30340;&#27169;&#22411;&#26356;&#26377;&#25928;&#22320;&#35302;&#21457;&#20854;&#20182;&#25915;&#20987;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#23646;&#24615;&#25512;&#23548;&#25915;&#20987;&#65292;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#25915;&#20987;&#26377;&#25928;&#65292;&#21487;&#24212;&#29992;&#20110;&#22238;&#24402;&#21644;&#20998;&#31867;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2210.16205</link><description>&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#26412;&#22320;&#27169;&#22411;&#37325;&#26500;&#25915;&#20987;&#21450;&#20854;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Local Model Reconstruction Attacks in Federated Learning and their Uses. (arXiv:2210.16205v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.16205
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#26412;&#22320;&#27169;&#22411;&#37325;&#26500;&#25915;&#20987;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#36890;&#36807;&#37325;&#26500;&#23458;&#25143;&#31471;&#30340;&#27169;&#22411;&#26356;&#26377;&#25928;&#22320;&#35302;&#21457;&#20854;&#20182;&#25915;&#20987;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#23646;&#24615;&#25512;&#23548;&#25915;&#20987;&#65292;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#25915;&#20987;&#26377;&#25928;&#65292;&#21487;&#24212;&#29992;&#20110;&#22238;&#24402;&#21644;&#20998;&#31867;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#26412;&#22320;&#27169;&#22411;&#37325;&#26500;&#25915;&#20987;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#36890;&#36807;&#31363;&#21548;&#30446;&#26631;&#23458;&#25143;&#31471;&#19982;&#26381;&#21153;&#22120;&#20043;&#38388;&#30340;&#36890;&#20449;&#24182;&#37325;&#26500;&#21463;&#23475;&#32773;&#30340;&#26412;&#22320;/&#20010;&#24615;&#21270;&#27169;&#22411;&#65292;&#20174;&#32780;&#26356;&#26377;&#25928;&#22320;&#35302;&#21457;&#20854;&#20182;&#32463;&#20856;&#25915;&#20987;&#12290;&#26412;&#22320;&#27169;&#22411;&#20165;&#20381;&#36182;&#20110;&#23458;&#25143;&#31471;&#30340;&#25968;&#25454;&#65292;&#20294;&#21487;&#33021;&#27844;&#28431;&#27604;&#26381;&#21153;&#22120;&#23398;&#20064;&#30340;&#20840;&#23616;&#27169;&#22411;&#26356;&#22810;&#30340;&#31169;&#26377;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#23646;&#24615;&#25512;&#23548;&#25915;&#20987;&#65292;&#21033;&#29992;&#20102;&#26412;&#22320;&#27169;&#22411;&#37325;&#26500;&#25915;&#20987;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#36825;&#31181;&#25915;&#20987;&#30340;&#20998;&#26512;&#19979;&#30028;&#12290;&#20351;&#29992;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#30340;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26412;&#22320;&#37325;&#26500;&#25915;&#20987;&#23545;&#20110;&#22238;&#24402;&#21644;&#20998;&#31867;&#20219;&#21153;&#37117;&#26377;&#25928;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26032;&#39062;&#30340;&#23646;&#24615;&#25512;&#23548;&#25915;&#20987;&#19982;&#26368;&#20808;&#36827;&#30340;&#25915;&#20987;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we initiate the study of local model reconstruction attacks for federated learning, where a honest-but-curious adversary eavesdrops the messages exchanged between a targeted client and the server, and then reconstructs the local/personalized model of the victim. The local model reconstruction attack allows the adversary to trigger other classical attacks in a more effective way, since the local model only depends on the client's data and can leak more private information than the global model learned by the server. Additionally, we propose a novel model-based attribute inference attack in federated learning leveraging the local model reconstruction attack. We provide an analytical lower-bound for this attribute inference attack. Empirical results using real world datasets confirm that our local reconstruction attack works well for both regression and classification tasks. Moreover, we benchmark our novel attribute inference attack against the state-of-the-art attacks in 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24179;&#28369;&#30772;&#30862;&#30340;&#24130;&#24459;&#20989;&#25968;&#24418;&#24335;&#65292;&#21487;&#20197;&#20934;&#30830;&#22320;&#27169;&#25311;&#21644;&#22806;&#25512;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#32553;&#25918;&#34892;&#20026;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#26550;&#26500;&#21644;&#22823;&#37327;&#19981;&#21516;&#20219;&#21153;&#65292;&#21253;&#25324;&#35270;&#35273;&#12289;&#35821;&#35328;&#12289;&#38899;&#39057;&#12289;&#35270;&#39057;&#12289;&#29983;&#25104;&#24314;&#27169;&#12289;&#23545;&#27604;&#23398;&#20064;&#12289;&#26426;&#22120;&#20154;&#12289;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;/&#26657;&#20934;&#12289;&#23545;&#25239;&#40065;&#26834;&#24615;&#12289;&#20998;&#23376;&#12289;&#35745;&#31639;&#26426;&#32534;&#31243;/&#32534;&#30721;&#12289;&#25968;&#23398;&#21333;&#35789;&#38382;&#39064;&#12289;&#31639;&#26415;&#12289;&#26080;&#30417;&#30563;/&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2210.14891</link><description>&lt;p&gt;
&#30772;&#30862;&#30340;&#31070;&#32463;&#32553;&#25918;&#23450;&#24459;
&lt;/p&gt;
&lt;p&gt;
Broken Neural Scaling Laws. (arXiv:2210.14891v7 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.14891
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24179;&#28369;&#30772;&#30862;&#30340;&#24130;&#24459;&#20989;&#25968;&#24418;&#24335;&#65292;&#21487;&#20197;&#20934;&#30830;&#22320;&#27169;&#25311;&#21644;&#22806;&#25512;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#32553;&#25918;&#34892;&#20026;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#26550;&#26500;&#21644;&#22823;&#37327;&#19981;&#21516;&#20219;&#21153;&#65292;&#21253;&#25324;&#35270;&#35273;&#12289;&#35821;&#35328;&#12289;&#38899;&#39057;&#12289;&#35270;&#39057;&#12289;&#29983;&#25104;&#24314;&#27169;&#12289;&#23545;&#27604;&#23398;&#20064;&#12289;&#26426;&#22120;&#20154;&#12289;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;/&#26657;&#20934;&#12289;&#23545;&#25239;&#40065;&#26834;&#24615;&#12289;&#20998;&#23376;&#12289;&#35745;&#31639;&#26426;&#32534;&#31243;/&#32534;&#30721;&#12289;&#25968;&#23398;&#21333;&#35789;&#38382;&#39064;&#12289;&#31639;&#26415;&#12289;&#26080;&#30417;&#30563;/&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a smoothly broken power law functional form (referred to as a Broken Neural Scaling Law (BNSL)) that accurately models and extrapolates the scaling behaviors of deep neural networks for various architectures and a large and diverse set of tasks, including vision, language, audio, video, generative modeling, contrastive learning, robotics, uncertainty estimation/calibration, adversarial robustness, molecules, computer programming/coding, math word problems, arithmetic, unsupervised/self-supervised learning, and reinforcement learning.
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24179;&#28369;&#30772;&#30862;&#30340;&#24130;&#24459;&#20989;&#25968;&#24418;&#24335;&#65288;&#25105;&#20204;&#31216;&#20043;&#20026;&#30772;&#30862;&#30340;&#31070;&#32463;&#32553;&#25918;&#23450;&#24459;&#65288;BNSL&#65289;&#65289;&#65292;&#23427;&#20934;&#30830;&#22320;&#27169;&#25311;&#21644;&#22806;&#25512;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#32553;&#25918;&#34892;&#20026;&#65288;&#21363;&#24863;&#20852;&#36259;&#30340;&#35780;&#20272;&#25351;&#26631;&#38543;&#29992;&#20110;&#35757;&#32451;&#30340;&#35745;&#31639;&#37327;&#12289;&#27169;&#22411;&#21442;&#25968;&#25968;&#37327;&#12289;&#35757;&#32451;&#25968;&#25454;&#38598;&#22823;&#23567;&#25110;&#19978;&#28216;&#24615;&#33021;&#21464;&#21270;&#32780;&#21464;&#21270;&#65289;&#23545;&#20110;&#21508;&#31181;&#26550;&#26500;&#21644;&#22823;&#37327;&#19981;&#21516;&#20219;&#21153;&#20013;&#30340;&#27599;&#20010;&#20219;&#21153;&#65292;&#21253;&#25324;&#22823;&#35268;&#27169;&#35270;&#35273;&#12289;&#35821;&#35328;&#12289;&#38899;&#39057;&#12289;&#35270;&#39057;&#12289;&#25193;&#25955;&#12289;&#29983;&#25104;&#24314;&#27169;&#12289;&#22810;&#27169;&#24577;&#23398;&#20064;&#12289;&#23545;&#27604;&#23398;&#20064;&#12289;AI&#23545;&#40784;&#12289;&#26426;&#22120;&#20154;&#12289;&#36229;&#20986;&#20998;&#24067;&#65288;OOD&#65289;&#27867;&#21270;&#12289;&#25345;&#32493;&#23398;&#20064;&#12289;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;/&#26657;&#20934;&#12289;&#36229;&#20986;&#20998;&#24067;&#26816;&#27979;&#12289;&#23545;&#25239;&#40065;&#26834;&#24615;&#12289;&#33976;&#39311;&#12289;&#20998;&#23376;&#12289;&#35745;&#31639;&#26426;&#32534;&#31243;/&#32534;&#30721;&#12289;&#25968;&#23398;&#21333;&#35789;&#38382;&#39064;&#12289;&#31639;&#26415;&#12289;&#26080;&#30417;&#30563;/&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a smoothly broken power law functional form (referred to by us as a Broken Neural Scaling Law (BNSL)) that accurately models and extrapolates the scaling behaviors of deep neural networks (i.e. how the evaluation metric of interest varies as the amount of compute used for training, number of model parameters, training dataset size, or upstream performance varies) for various architectures and for each of various tasks within a large and diverse set of upstream and downstream tasks, in zero-shot, prompted, and fine-tuned settings. This set includes large-scale vision, language, audio, video, diffusion, generative modeling, multimodal learning, contrastive learning, AI alignment, robotics, out-of-distribution (OOD) generalization, continual learning, uncertainty estimation / calibration, out-of-distribution detection, adversarial robustness, distillation, molecules, computer programming/coding, math word problems, arithmetic, unsupervised/self-supervised learning, and reinforc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#23558;Stackelberg&#22343;&#34913;&#25628;&#32034;&#20316;&#20026;&#22810;&#26234;&#33021;&#20307;RL&#38382;&#39064;&#36827;&#34892;&#23454;&#29616;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#19978;&#19979;&#25991;&#31574;&#30053;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#26631;&#20934;&#21644;&#26032;&#39062;&#30340;&#22522;&#20934;&#39046;&#22495;&#19978;&#36827;&#34892;&#23454;&#39564;&#35780;&#20272;&#65292;&#35777;&#26126;&#30456;&#27604;&#20043;&#21069;&#30340;&#26041;&#27861;&#65292;&#26679;&#26412;&#25928;&#29575;&#24471;&#21040;&#20102;&#26497;&#22823;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2210.11942</link><description>&lt;p&gt;
&#39046;&#34966;&#19982;&#36861;&#38543;&#32773;&#65306;&#28145;&#24230;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;Stackelberg&#22343;&#34913;
&lt;/p&gt;
&lt;p&gt;
Oracles &amp; Followers: Stackelberg Equilibria in Deep Multi-Agent Reinforcement Learning. (arXiv:2210.11942v4 [cs.GT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.11942
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#23558;Stackelberg&#22343;&#34913;&#25628;&#32034;&#20316;&#20026;&#22810;&#26234;&#33021;&#20307;RL&#38382;&#39064;&#36827;&#34892;&#23454;&#29616;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#19978;&#19979;&#25991;&#31574;&#30053;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#26631;&#20934;&#21644;&#26032;&#39062;&#30340;&#22522;&#20934;&#39046;&#22495;&#19978;&#36827;&#34892;&#23454;&#39564;&#35780;&#20272;&#65292;&#35777;&#26126;&#30456;&#27604;&#20043;&#21069;&#30340;&#26041;&#27861;&#65292;&#26679;&#26412;&#25928;&#29575;&#24471;&#21040;&#20102;&#26497;&#22823;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Stackelberg&#22343;&#34913;&#22312;&#22810;&#20010;&#23398;&#20064;&#38382;&#39064;&#20013;&#33258;&#28982;&#20986;&#29616;&#65292;&#20363;&#22914;&#22312;&#23433;&#20840;&#21338;&#24328;&#25110;&#38388;&#25509;&#26426;&#21046;&#35774;&#35745;&#20013;&#65292;&#24182;&#22312;&#24378;&#21270;&#23398;&#20064;&#25991;&#29486;&#20013;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23454;&#29616;Stackelberg&#22343;&#34913;&#25628;&#32034;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#23558;&#20854;&#20316;&#20026;&#22810;&#26234;&#33021;&#20307;RL&#38382;&#39064;&#65292;&#20801;&#35768;&#36827;&#34892;&#21508;&#31181;&#31639;&#27861;&#35774;&#35745;&#36873;&#25321;&#65292;&#24182;&#23558;&#20043;&#21069;&#30340;&#26041;&#27861;&#35270;&#20026;&#27492;&#26694;&#26550;&#30340;&#29305;&#23450;&#23454;&#20363;&#12290;&#21478;&#22806;&#65292;&#25105;&#20204;&#27880;&#24847;&#21040;&#35774;&#35745;&#31354;&#38388;&#20801;&#35768;&#20351;&#29992;&#20197;&#21069;&#22312;&#25991;&#29486;&#20013;&#27809;&#26377;&#35265;&#36807;&#30340;&#26041;&#27861;&#65292;&#20363;&#22914;&#21033;&#29992;&#22810;&#20219;&#21153;&#21644;&#20803;-RL&#25216;&#26415;&#23454;&#29616;&#36861;&#38543;&#32773;&#30340;&#25910;&#25947;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#19978;&#19979;&#25991;&#31574;&#30053;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#26631;&#20934;&#21644;&#26032;&#39062;&#30340;&#22522;&#20934;&#39046;&#22495;&#19978;&#36827;&#34892;&#23454;&#39564;&#35780;&#20272;&#65292;&#35777;&#26126;&#20102;&#30456;&#27604;&#20197;&#21069;&#30340;&#26041;&#27861;&#65292;&#26679;&#26412;&#25928;&#29575;&#22823;&#22823;&#25552;&#39640;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36824;&#25506;&#32034;&#20102;&#37319;&#29992;&#26694;&#26550;&#36793;&#30028;&#22806;&#30340;&#31639;&#27861;&#35774;&#35745;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stackelberg equilibria arise naturally in a range of popular learning problems, such as in security games or indirect mechanism design, and have received increasing attention in the reinforcement learning literature. We present a general framework for implementing Stackelberg equilibria search as a multi-agent RL problem, allowing a wide range of algorithmic design choices. We discuss how previous approaches can be seen as specific instantiations of this framework. As a key insight, we note that the design space allows for approaches not previously seen in the literature, for instance by leveraging multitask and meta-RL techniques for follower convergence. We propose one such approach using contextual policies, and evaluate it experimentally on both standard and novel benchmark domains, showing greatly improved sample efficiency compared to previous approaches. Finally, we explore the effect of adopting algorithm designs outside the borders of our framework.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#21517;&#20026;UDDIA&#30340;&#32479;&#19968;&#21435;&#27602;&#21270;&#21644;&#21435;&#20559;&#35265;&#26694;&#26550;&#65292;&#23427;&#33021;&#22815;&#26377;&#25928;&#22320;&#28040;&#38500;&#26377;&#27602;&#35821;&#35328;&#21644;&#20943;&#23569;&#31038;&#20250;&#20559;&#35265;&#65292;&#21516;&#26102;&#20445;&#25345;&#27969;&#30021;&#24615;&#12290;</title><link>http://arxiv.org/abs/2210.04492</link><description>&lt;p&gt;
&#36890;&#36807;&#25512;&#29702;&#26102;&#33258;&#36866;&#24212;&#20248;&#21270;&#23454;&#29616;&#35821;&#35328;&#29983;&#25104;&#20013;&#30340;&#32479;&#19968;&#21435;&#27602;&#21270;&#21644;&#21435;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Unified Detoxifying and Debiasing in Language Generation via Inference-time Adaptive Optimization. (arXiv:2210.04492v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.04492
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#21517;&#20026;UDDIA&#30340;&#32479;&#19968;&#21435;&#27602;&#21270;&#21644;&#21435;&#20559;&#35265;&#26694;&#26550;&#65292;&#23427;&#33021;&#22815;&#26377;&#25928;&#22320;&#28040;&#38500;&#26377;&#27602;&#35821;&#35328;&#21644;&#20943;&#23569;&#31038;&#20250;&#20559;&#35265;&#65292;&#21516;&#26102;&#20445;&#25345;&#27969;&#30021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#24456;&#22909;&#30340;&#34920;&#29616;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#35757;&#32451;&#35821;&#26009;&#24211;&#20013;&#25429;&#25417;&#21644;&#22797;&#21046;&#26377;&#23475;&#20869;&#23481;&#30340;&#24773;&#20917;&#26222;&#36941;&#23384;&#22312;&#65292;&#29305;&#21035;&#26159;&#27602;&#24615;&#35821;&#35328;&#21644;&#31038;&#20250;&#20559;&#35265;&#65292;&#24341;&#36215;&#20102;&#20005;&#37325;&#30340;&#36947;&#24503;&#38382;&#39064;&#12290;&#27492;&#21069;&#22312;&#36947;&#24503;&#19978;&#30340;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#26041;&#38754;&#30340;&#24037;&#20316;&#37117;&#26159;&#20998;&#24320;&#35299;&#20915;&#21435;&#27602;&#21270;&#21644;&#21435;&#20559;&#35265;&#36825;&#20004;&#20010;&#38382;&#39064;&#30340;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#21435;&#20559;&#35265;&#30340;&#27169;&#22411;&#20173;&#28982;&#23384;&#22312;&#27602;&#24615;&#65292;&#32780;&#32463;&#36807;&#21435;&#27602;&#21270;&#30340;&#27169;&#22411;&#29978;&#33267;&#20250;&#21152;&#21095;&#31038;&#20250;&#20559;&#35265;&#65292;&#36825;&#26159;&#26377;&#38382;&#39064;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21517;&#20026;UDDIA&#30340;&#32479;&#19968;&#21435;&#27602;&#21270;&#21644;&#21435;&#20559;&#35265;&#26694;&#26550;&#65292;&#23558;&#36825;&#20004;&#20010;&#38382;&#39064;&#20316;&#20026;&#32416;&#27491;&#36755;&#20986;&#31354;&#38388;&#30340;&#20851;&#38190;&#38382;&#39064;&#32852;&#21512;&#24418;&#24335;&#21270;&#12290;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#23558;&#25105;&#20204;&#30340;&#26694;&#26550;&#35299;&#37322;&#20026;&#23398;&#20064;&#28151;&#21512;&#21152;&#26435;&#23646;&#24615;&#30340;&#25991;&#26412;&#20998;&#24067;&#12290;&#27492;&#22806;&#65292;UDDIA&#23545;&#23569;&#37327;&#21442;&#25968;&#36827;&#34892;&#33258;&#36866;&#24212;&#20248;&#21270;&#65292;&#20174;&#32780;&#25511;&#21046;&#27599;&#20010;&#23646;&#24615;&#30340;&#36129;&#29486;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;UDDIA&#33021;&#22815;&#26377;&#25928;&#22320;&#28040;&#38500;&#26377;&#27602;&#35821;&#35328;&#24182;&#20943;&#23569;&#31038;&#20250;&#20559;&#35265;&#65292;&#21516;&#26102;&#20445;&#25345;&#27969;&#30021;&#24615;&#12290;&#27492;&#22806;&#65292;UDDIA&#22312;&#24191;&#27867;&#30340;&#35780;&#20272;&#20013;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#21435;&#27602;&#21270;&#21644;&#21435;&#20559;&#35265;&#26041;&#27861;&#12290;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#28145;&#20837;&#30340;&#20998;&#26512;&#65292;&#35299;&#37322;&#20102;UDDIA&#30340;&#34892;&#20026;&#65292;&#24182;&#20026;&#26410;&#26469;&#22312;&#36947;&#24503;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#26041;&#38754;&#30340;&#24037;&#20316;&#25552;&#20379;&#20102;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Warning: this paper contains model outputs exhibiting offensiveness and biases. Recently pre-trained language models (PLMs) have prospered in various natural language generation (NLG) tasks due to their ability to generate fairly fluent text. Nevertheless, these models are observed to capture and reproduce harmful contents in training corpora, typically toxic language and social biases, raising severe moral issues. Prior works on ethical NLG tackle detoxifying and debiasing separately, which is problematic since we find debiased models still exhibit toxicity while detoxified ones even exacerbate social biases. To address such a challenge, we propose the first unified framework of detoxifying and debiasing called UDDIA, which jointly formalizes these two problems as rectifying the output space. We theoretically interpret our framework as learning a text distribution mixing weighted attributes. Besides, UDDIA conducts adaptive optimization of only a few parameters during decoding based o
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25506;&#31350;&#21592;&#24037;&#23545;(X)AI&#30340;&#38656;&#27714;&#21644;&#24577;&#24230;&#12290;&#30740;&#31350;&#21457;&#29616;&#21592;&#24037;&#26222;&#36941;&#35748;&#20026;&#35299;&#37322;&#24615;AI&#23545;&#20844;&#21496;&#30340;&#25104;&#21151;&#38750;&#24120;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2210.03527</link><description>&lt;p&gt;
&#20844;&#21496;&#38656;&#35201;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#21527;&#65311;&#21592;&#24037;&#35282;&#24230;&#30340;&#25361;&#25112;&#12289;&#26399;&#26395;&#21644;&#26426;&#20250;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Do We Need Explainable AI in Companies? Investigation of Challenges, Expectations, and Chances from Employees' Perspective. (arXiv:2210.03527v2 [cs.HC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.03527
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25506;&#31350;&#21592;&#24037;&#23545;(X)AI&#30340;&#38656;&#27714;&#21644;&#24577;&#24230;&#12290;&#30740;&#31350;&#21457;&#29616;&#21592;&#24037;&#26222;&#36941;&#35748;&#20026;&#35299;&#37322;&#24615;AI&#23545;&#20844;&#21496;&#30340;&#25104;&#21151;&#38750;&#24120;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#23545;&#20110;&#20225;&#19994;&#30340;&#37319;&#29992;&#36234;&#26469;&#36234;&#25104;&#20026;&#19994;&#21153;&#25104;&#21151;&#30340;&#24517;&#35201;&#20803;&#32032;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992; AI &#25552;&#20986;&#20102;&#20844;&#21496;&#21644;&#20854;&#21592;&#24037;&#30340;&#26032;&#35201;&#27714;&#65292;&#21253;&#25324; AI &#31995;&#32479;&#30340;&#36879;&#26126;&#24230;&#21644;&#21487;&#29702;&#35299;&#24615;&#12290;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#39046;&#22495;&#26088;&#22312;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30740;&#31350;&#20027;&#35201;&#21253;&#25324;&#23454;&#39564;&#23460;&#30740;&#31350;&#65292;&#38656;&#35201;&#25913;&#36827;&#30740;&#31350;&#32467;&#26524;&#22312;&#23454;&#38469;&#24773;&#20917;&#19979;&#30340;&#36866;&#29992;&#24615;&#12290;&#22240;&#27492;&#65292;&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#21592;&#24037;&#23545;&#65288;X&#65289;AI &#38656;&#27714;&#21644;&#24577;&#24230;&#30340;&#35265;&#35299;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;AI &#21644; XAI &#26159;&#20247;&#25152;&#21608;&#30693;&#30340;&#26415;&#35821;&#65292;&#34987;&#35748;&#20026;&#23545;&#21592;&#24037;&#38750;&#24120;&#37325;&#35201;&#12290;&#36825;&#19968;&#35748;&#30693;&#26159;&#25512;&#21160; AI &#25104;&#21151;&#20351;&#29992;&#30340;&#20851;&#38190;&#31532;&#19968;&#27493;&#65292;&#20026;&#25552;&#20379;&#21487;&#29702;&#35299;&#30340; AI &#25216;&#26415;&#27934;&#35265;&#20570;&#20986;&#36129;&#29486;&#12290;&#22312;&#8220;&#25945;&#35757;&#23398;&#20064;&#8221;&#37096;&#20998;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#21457;&#29616;&#30340;&#24320;&#25918;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Companies' adoption of artificial intelligence (AI) is increasingly becoming an essential element of business success. However, using AI poses new requirements for companies and their employees, including transparency and comprehensibility of AI systems. The field of Explainable AI (XAI) aims to address these issues. Yet, the current research primarily consists of laboratory studies, and there is a need to improve the applicability of the findings to real-world situations. Therefore, this project report paper provides insights into employees' needs and attitudes towards (X)AI. For this, we investigate employees' perspectives on (X)AI. Our findings suggest that AI and XAI are well-known terms perceived as important for employees. This recognition is a critical first step for XAI to potentially drive successful usage of AI by providing comprehensible insights into AI technologies. In a lessons-learned section, we discuss the open questions identified and suggest future research direction
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#29289;&#20307;&#32423;&#21035;&#30693;&#35782;&#20316;&#20026;&#21151;&#33021;&#23545;&#35937;&#23548;&#21521;&#32593;&#32476;&#36827;&#34892;&#20219;&#21153;&#35268;&#21010;&#21644;&#25191;&#34892;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#28436;&#31034;&#20102;&#22312;CoppeliaSim&#20013;&#24212;&#29992;&#35813;&#26041;&#27861;&#36827;&#34892;&#38271;&#26399;&#20219;&#21153;&#35268;&#21010;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2207.05800</link><description>&lt;p&gt;
&#38271;&#31243;&#35268;&#21010;&#19982;&#25191;&#34892;&#30340;&#21151;&#33021;&#23545;&#35937;&#23548;&#21521;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Long-Horizon Planning and Execution with Functional Object-Oriented Networks. (arXiv:2207.05800v5 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.05800
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#29289;&#20307;&#32423;&#21035;&#30693;&#35782;&#20316;&#20026;&#21151;&#33021;&#23545;&#35937;&#23548;&#21521;&#32593;&#32476;&#36827;&#34892;&#20219;&#21153;&#35268;&#21010;&#21644;&#25191;&#34892;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#28436;&#31034;&#20102;&#22312;CoppeliaSim&#20013;&#24212;&#29992;&#35813;&#26041;&#27861;&#36827;&#34892;&#38271;&#26399;&#20219;&#21153;&#35268;&#21010;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32852;&#21512;&#23545;&#35937;-&#21160;&#20316;&#34920;&#31034;&#26041;&#38754;&#30340;&#24037;&#20316;&#20043;&#21518;&#65292;&#23558;&#21151;&#33021;&#23545;&#35937;&#23548;&#21521;&#32593;&#32476;&#65288;FOON&#65289;&#20316;&#20026;&#26426;&#22120;&#20154;&#30340;&#30693;&#35782;&#22270;&#34920;&#31034;&#24341;&#20837;&#12290;FOON&#21253;&#21547;&#23545;&#26426;&#22120;&#20154;&#20219;&#21153;&#29702;&#35299;&#21644;&#29289;&#20307;&#32423;&#35268;&#21010;&#30340;&#29615;&#22659;&#26377;&#29992;&#30340;&#31526;&#21495;&#27010;&#24565;&#12290;&#22312;&#26412;&#30740;&#31350;&#20043;&#21069;&#65292;&#24456;&#23569;&#26377;&#30740;&#31350;&#34920;&#26126;&#22914;&#20309;&#20174;FOON&#20013;&#33719;&#21462;&#30340;&#35745;&#21010;&#21487;&#20197;&#30001;&#26426;&#22120;&#20154;&#25191;&#34892;&#65292;&#22240;&#20026;FOON&#20013;&#30340;&#27010;&#24565;&#23545;&#20110;&#25191;&#34892;&#26469;&#35828;&#22826;&#25277;&#35937;&#20102;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#21033;&#29992;&#29289;&#20307;&#32423;&#21035;&#30693;&#35782;&#20316;&#20026;FOON&#36827;&#34892;&#20219;&#21153;&#35268;&#21010;&#21644;&#25191;&#34892;&#30340;&#24819;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#33258;&#21160;&#23558;FOON&#36716;&#25442;&#20026;PDDL&#65292;&#24182;&#21033;&#29992;&#29616;&#25104;&#30340;&#35268;&#21010;&#22120;&#12289;&#34892;&#20026;&#19978;&#19979;&#25991;&#21644;&#26426;&#22120;&#20154;&#25216;&#33021;&#22312;&#20998;&#23618;&#35268;&#21010;&#31649;&#36947;&#20013;&#29983;&#25104;&#21487;&#25191;&#34892;&#30340;&#20219;&#21153;&#35745;&#21010;&#12290;&#25105;&#20204;&#22312;CoppeliaSim&#20013;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#25972;&#20010;&#26041;&#27861;&#30340;&#38271;&#26399;&#20219;&#21153;&#65292;&#20197;&#21450;&#22914;&#20309;&#23558;&#23398;&#20064;&#21040;&#30340;&#34892;&#20026;&#19978;&#19979;&#25991;&#25193;&#23637;&#21040;&#21069;&#25152;&#26410;&#35265;&#30340;&#22330;&#26223;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Following work on joint object-action representations, functional object-oriented networks (FOON) were introduced as a knowledge graph representation for robots. A FOON contains symbolic concepts useful to a robot's understanding of tasks and its environment for object-level planning. Prior to this work, little has been done to show how plans acquired from FOON can be executed by a robot, as the concepts in a FOON are too abstract for execution. We thereby introduce the idea of exploiting object-level knowledge as a FOON for task planning and execution. Our approach automatically transforms FOON into PDDL and leverages off-the-shelf planners, action contexts, and robot skills in a hierarchical planning pipeline to generate executable task plans. We demonstrate our entire approach on long-horizon tasks in CoppeliaSim and show how learned action contexts can be extended to never-before-seen scenarios.
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#35270;&#35273;&#21464;&#21387;&#22120;&#20013;&#30340;&#31354;&#38388;&#31232;&#30095;&#24615;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#26631;&#35760;&#31232;&#30095;&#21270;&#26694;&#26550;&#65292;&#21487;&#21152;&#36895;&#21508;&#31181;&#32467;&#26500;&#30340;&#27169;&#22411;&#65292;&#34987;&#21098;&#26525;&#30340;&#20887;&#20313;&#26631;&#35760;&#30001;&#36731;&#37327;&#32423;&#39044;&#27979;&#27169;&#22359;&#36880;&#27493;&#21160;&#24577;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2207.01580</link><description>&lt;p&gt;
&#21160;&#24577;&#31354;&#38388;&#31232;&#30095;&#21270;&#65306;&#39640;&#25928;&#35270;&#35273;&#21464;&#21387;&#22120;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Dynamic Spatial Sparsification for Efficient Vision Transformers and Convolutional Neural Networks. (arXiv:2207.01580v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.01580
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#35270;&#35273;&#21464;&#21387;&#22120;&#20013;&#30340;&#31354;&#38388;&#31232;&#30095;&#24615;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#26631;&#35760;&#31232;&#30095;&#21270;&#26694;&#26550;&#65292;&#21487;&#21152;&#36895;&#21508;&#31181;&#32467;&#26500;&#30340;&#27169;&#22411;&#65292;&#34987;&#21098;&#26525;&#30340;&#20887;&#20313;&#26631;&#35760;&#30001;&#36731;&#37327;&#32423;&#39044;&#27979;&#27169;&#22359;&#36880;&#27493;&#21160;&#24577;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#35270;&#35273;&#25968;&#25454;&#20013;&#30340;&#31354;&#38388;&#31232;&#30095;&#24615;&#21152;&#36895;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#35270;&#35273;&#21464;&#21387;&#22120;&#20013;&#26368;&#32456;&#39044;&#27979;&#20165;&#22522;&#20110;&#19968;&#23567;&#37096;&#20998;&#20449;&#24687;&#26368;&#20016;&#23500;&#30340;&#26631;&#35760;&#65292;&#36825;&#23545;&#20110;&#20934;&#30830;&#30340;&#22270;&#20687;&#35782;&#21035;&#36275;&#22815;&#12290;&#22522;&#20110;&#36825;&#19968;&#35266;&#23519;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#26631;&#35760;&#31232;&#30095;&#21270;&#26694;&#26550;&#65292;&#26681;&#25454;&#36755;&#20837;&#36880;&#27493;&#21644;&#21160;&#24577;&#22320;&#21098;&#26525;&#20887;&#20313;&#26631;&#35760;&#20197;&#21152;&#36895;&#35270;&#35273;&#21464;&#21387;&#22120;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#36731;&#37327;&#32423;&#39044;&#27979;&#27169;&#22359;&#65292;&#20197;&#26681;&#25454;&#24403;&#21069;&#29305;&#24449;&#26469;&#20272;&#35745;&#27599;&#20010;&#26631;&#35760;&#30340;&#37325;&#35201;&#24615;&#24471;&#20998;&#12290;&#35813;&#27169;&#22359;&#34987;&#28155;&#21152;&#21040;&#19981;&#21516;&#23618;&#20013;&#20197;&#20998;&#23618;&#22320;&#21098;&#26525;&#20887;&#20313;&#26631;&#35760;&#12290;&#34429;&#28982;&#35813;&#26694;&#26550;&#26469;&#28304;&#20110;&#25105;&#20204;&#23545;&#35270;&#35273;&#21464;&#21387;&#22120;&#20013;&#31232;&#30095;&#27880;&#24847;&#21147;&#30340;&#35266;&#23519;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#33258;&#36866;&#24212;&#21644;&#38750;&#23545;&#31216;&#35745;&#31639;&#30340;&#24605;&#24819;&#21487;&#20197;&#25104;&#20026;&#21152;&#36895;&#21508;&#31181;&#32467;&#26500;&#30340;&#19968;&#33324;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#25193;&#23637;&#21040;&#21253;&#25324;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;&#20998;&#23618;&#35270;&#35273;&#21464;&#21387;&#22120;&#22312;&#20869;&#30340;&#20998;&#23618;&#27169;&#22411;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present a new approach for model acceleration by exploiting spatial sparsity in visual data. We observe that the final prediction in vision Transformers is only based on a subset of the most informative tokens, which is sufficient for accurate image recognition. Based on this observation, we propose a dynamic token sparsification framework to prune redundant tokens progressively and dynamically based on the input to accelerate vision Transformers. Specifically, we devise a lightweight prediction module to estimate the importance score of each token given the current features. The module is added to different layers to prune redundant tokens hierarchically. While the framework is inspired by our observation of the sparse attention in vision Transformers, we find the idea of adaptive and asymmetric computation can be a general solution for accelerating various architectures. We extend our method to hierarchical models including CNNs and hierarchical vision Transformers 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#36328;&#36793;&#30028;&#32852;&#37030;&#23398;&#20064;&#20013;&#20027;&#20307;&#32423;&#21035;&#30340;&#38544;&#31169;&#65292;&#24182;&#25552;&#20986;&#20004;&#31181;&#26032;&#39062;&#30340;&#40657;&#30418;&#25512;&#29702;&#25915;&#20987;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2206.03317</link><description>&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#20027;&#20307;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Subject Membership Inference Attacks in Federated Learning. (arXiv:2206.03317v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.03317
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#36328;&#36793;&#30028;&#32852;&#37030;&#23398;&#20064;&#20013;&#20027;&#20307;&#32423;&#21035;&#30340;&#38544;&#31169;&#65292;&#24182;&#25552;&#20986;&#20004;&#31181;&#26032;&#39062;&#30340;&#40657;&#30418;&#25512;&#29702;&#25915;&#20987;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#38544;&#31169;&#25915;&#20987;&#36890;&#24120;&#30528;&#37325;&#20110;&#25512;&#35770;&#35757;&#32451;&#25968;&#25454;&#20013;&#29305;&#23450;&#25968;&#25454;&#28857;&#30340;&#23384;&#22312;&#12290;&#28982;&#32780;&#65292;&#25915;&#20987;&#32773;&#30495;&#27491;&#24819;&#30693;&#36947;&#30340;&#26159;&#29305;&#23450;&#20010;&#20307;&#30340;&#65288;&#20027;&#20307;&#30340;&#65289;&#25968;&#25454;&#26159;&#21542;&#21253;&#21547;&#22312;&#35757;&#32451;&#20013;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#25915;&#20987;&#32773;&#26356;&#21487;&#33021;&#25317;&#26377;&#29305;&#23450;&#20027;&#20307;&#20998;&#24067;&#32780;&#38750;&#23454;&#38469;&#35760;&#24405;&#12290;&#26412;&#25991;&#30740;&#31350;&#36328;&#36793;&#30028;&#32852;&#37030;&#23398;&#20064;&#20013;&#20027;&#20307;&#32423;&#21035;&#30340;&#38544;&#31169;&#65292;&#25552;&#20986;&#20004;&#31181;&#26032;&#39062;&#30340;&#40657;&#30418;&#25512;&#29702;&#25915;&#20987;&#26041;&#27861;&#65292;&#24182;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Privacy attacks on Machine Learning (ML) models often focus on inferring the existence of particular data points in the training data. However, what the adversary really wants to know is if a particular individual's (subject's) data was included during training. In such scenarios, the adversary is more likely to have access to the distribution of a particular subject than actual records. Furthermore, in settings like cross-silo Federated Learning (FL), a subject's data can be embodied by multiple data records that are spread across multiple organizations. Nearly all of the existing private FL literature is dedicated to studying privacy at two granularities -- item-level (individual data records), and user-level (participating user in the federation), neither of which apply to data subjects in cross-silo FL. This insight motivates us to shift our attention from the privacy of data records to the privacy of data subjects, also known as subject-level privacy. We propose two novel black-bo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26080;&#26550;&#26500;&#20559;&#35265;&#30340;&#36974;&#34109;&#22270;&#20687;&#24314;&#27169;&#26694;&#26550;&#65288;A$^2$MIM&#65289;&#65292;&#26082;&#36866;&#29992;&#20110;Transformers&#20063;&#36866;&#29992;&#20110;CNNs&#65292;&#24182;&#36890;&#36807;&#23398;&#20064;&#20013;&#32423;&#20132;&#20114;&#20851;&#31995;&#26469;&#25552;&#39640;&#29305;&#24449;&#25552;&#21462;&#30340;&#19968;&#33324;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2205.13943</link><description>&lt;p&gt;
&#26080;&#26550;&#26500;&#20559;&#35265;&#30340;&#36974;&#34109;&#22270;&#20687;&#24314;&#27169;&#8212;&#8212;&#20174;ViT&#22238;&#21040;CNN
&lt;/p&gt;
&lt;p&gt;
Architecture-Agnostic Masked Image Modeling -- From ViT back to CNN. (arXiv:2205.13943v4 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.13943
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26080;&#26550;&#26500;&#20559;&#35265;&#30340;&#36974;&#34109;&#22270;&#20687;&#24314;&#27169;&#26694;&#26550;&#65288;A$^2$MIM&#65289;&#65292;&#26082;&#36866;&#29992;&#20110;Transformers&#20063;&#36866;&#29992;&#20110;CNNs&#65292;&#24182;&#36890;&#36807;&#23398;&#20064;&#20013;&#32423;&#20132;&#20114;&#20851;&#31995;&#26469;&#25552;&#39640;&#29305;&#24449;&#25552;&#21462;&#30340;&#19968;&#33324;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36974;&#34109;&#22270;&#20687;&#24314;&#27169;&#65288;MIM&#65289;&#26159;&#19968;&#31181;&#26032;&#20852;&#30340;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#22312;&#35768;&#22810;&#19979;&#28216;&#35270;&#35273;&#20219;&#21153;&#20013;&#65292;&#22914;Vision transformers&#65292;&#24050;&#32463;&#26174;&#31034;&#20986;&#24778;&#20154;&#30340;&#25104;&#21151;&#12290;&#20854;&#22522;&#26412;&#24605;&#24819;&#24456;&#31616;&#21333;&#65306;&#25226;&#36755;&#20837;&#22270;&#20687;&#30340;&#19968;&#37096;&#20998;&#36974;&#25377;&#20303;&#65292;&#28982;&#21518;&#36890;&#36807;&#39044;&#28909;&#20219;&#21153;&#36827;&#34892;&#37325;&#26500;&#12290;&#28982;&#32780;&#65292;MIM&#32972;&#21518;&#30340;&#24037;&#20316;&#21407;&#29702;&#24182;&#27809;&#26377;&#24471;&#21040;&#24456;&#22909;&#30340;&#35299;&#37322;&#65292;&#20197;&#21069;&#30340;&#30740;&#31350;&#22362;&#25345;&#35748;&#20026;MIM&#20027;&#35201;&#36866;&#29992;&#20110;Transformer&#23478;&#26063;&#65292;&#20294;&#19982;CNN&#19981;&#20860;&#23481;&#12290; &#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;MIM&#23454;&#36136;&#19978;&#26159;&#25945;&#27169;&#22411;&#23398;&#20064;&#26356;&#22909;&#30340;&#20013;&#32423;&#20132;&#20114;&#20851;&#31995;&#65292;&#20197;&#36827;&#34892;&#26356;&#24191;&#20041;&#30340;&#29305;&#24449;&#25552;&#21462;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#26550;&#26500;&#20559;&#35265;&#30340;&#36974;&#34109;&#22270;&#20687;&#24314;&#27169;&#26694;&#26550;&#65288;A$^2$MIM&#65289;&#65292;&#23427;&#20197;&#32479;&#19968;&#30340;&#26041;&#24335;&#20860;&#23481;Transformers&#21644;CNNs&#12290;&#23545;&#27969;&#34892;&#22522;&#20934;&#30340;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;A$^2$MIM&#23398;&#20064;&#26356;&#22909;&#30340;&#34920;&#31034;&#24418;&#24335;&#65292;&#26080;&#38656;&#26174;&#24335;&#35774;&#35745;&#65292;&#24182;&#36171;&#20104;&#39592;&#24178;&#27169;&#22411;&#26356;&#24378;&#30340;&#33021;&#21147;&#26469;&#36716;&#31227;&#21040;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Masked image modeling, an emerging self-supervised pre-training method, has shown impressive success across numerous downstream vision tasks with Vision transformers. Its underlying idea is simple: a portion of the input image is masked out and then reconstructed via a pre-text task. However, the working principle behind MIM is not well explained, and previous studies insist that MIM primarily works for the Transformer family but is incompatible with CNNs. In this work, we observe that MIM essentially teaches the model to learn better middle-order interactions among patches for more generalized feature extraction. We then propose an Architecture-Agnostic Masked Image Modeling framework (A$^2$MIM), which is compatible with both Transformers and CNNs in a unified way. Extensive experiments on popular benchmarks show that A$^2$MIM learns better representations without explicit design and endows the backbone model with the stronger capability to transfer to various downstream tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#30340;Object Preference Adaptation (OPA)&#26041;&#27861;&#36890;&#36807;&#23398;&#20064;&#20154;&#31867;&#38024;&#23545;&#29305;&#23450;&#29289;&#20307;&#30340;&#21453;&#39304;&#65292;&#22312;&#21333;&#27425;&#24178;&#39044;&#21518;&#36827;&#34892;&#26426;&#22120;&#20154;&#34892;&#20026;&#30340;&#33258;&#36866;&#24212;&#65292;&#25552;&#39640;&#20102;&#25968;&#25454;&#25928;&#29575;&#21644;&#36866;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2203.04951</link><description>&lt;p&gt;
&#20174;&#20154;&#30340;&#29289;&#29702;&#21453;&#39304;&#20013;&#23398;&#20064;&#65306;&#19968;&#31181;&#38754;&#21521;&#29289;&#20307;&#30340;&#21333;&#27425;&#33258;&#36866;&#24212;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning from Physical Human Feedback: An Object-Centric One-Shot Adaptation Method. (arXiv:2203.04951v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.04951
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#30340;Object Preference Adaptation (OPA)&#26041;&#27861;&#36890;&#36807;&#23398;&#20064;&#20154;&#31867;&#38024;&#23545;&#29305;&#23450;&#29289;&#20307;&#30340;&#21453;&#39304;&#65292;&#22312;&#21333;&#27425;&#24178;&#39044;&#21518;&#36827;&#34892;&#26426;&#22120;&#20154;&#34892;&#20026;&#30340;&#33258;&#36866;&#24212;&#65292;&#25552;&#39640;&#20102;&#25968;&#25454;&#25928;&#29575;&#21644;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#35753;&#26426;&#22120;&#20154;&#33021;&#22815;&#22312;&#26032;&#29615;&#22659;&#21644;&#20219;&#21153;&#20013;&#26377;&#25928;&#22320;&#37096;&#32626;&#65292;&#23427;&#20204;&#24517;&#39035;&#33021;&#22815;&#29702;&#35299;&#20154;&#31867;&#22312;&#24178;&#39044;&#36807;&#31243;&#20013;&#34920;&#36798;&#30340;&#21453;&#39304;&#12290;&#36825;&#21487;&#20197;&#32416;&#27491;&#19981;&#33391;&#34892;&#20026;&#25110;&#25351;&#20986;&#20854;&#20182;&#20559;&#22909;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#35201;&#20040;&#38656;&#35201;&#37325;&#22797;&#30340;&#20132;&#20114;&#22238;&#21512;&#65292;&#35201;&#20040;&#20551;&#23450;&#20808;&#21069;&#24050;&#30693;&#30340;&#22870;&#21169;&#29305;&#24449;&#65292;&#36825;&#26159;&#25968;&#25454;&#25928;&#29575;&#20302;&#19979;&#24182;&#19988;&#24456;&#38590;&#36716;&#31227;&#21040;&#26032;&#20219;&#21153;&#30340;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#26415;&#35821;&#19978;&#23558;&#20154;&#31867;&#20219;&#21153;&#25551;&#36848;&#20026;&#20197;&#29289;&#20307;&#20026;&#20013;&#24515;&#30340;&#23376;&#20219;&#21153;&#65292;&#24182;&#23558;&#29289;&#29702;&#24178;&#39044;&#35299;&#37322;&#20026;&#19982;&#29305;&#23450;&#29289;&#20307;&#30340;&#20851;&#31995;&#26469;&#25918;&#26494;&#36825;&#20123;&#20551;&#35774;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;Object Preference Adaptation (OPA)&#65292;&#30001;&#20004;&#20010;&#20851;&#38190;&#38454;&#27573;&#32452;&#25104;&#65306;1&#65289;&#39044;&#35757;&#32451;&#22522;&#30784;&#31574;&#30053;&#20197;&#20135;&#29983;&#21508;&#31181;&#34892;&#20026;&#65292;&#20197;&#21450;2&#65289;&#26681;&#25454;&#20154;&#31867;&#21453;&#39304;&#22312;&#32447;&#26356;&#26032;&#12290;&#25105;&#20204;&#24555;&#36895;&#32780;&#31616;&#21333;&#30340;&#33258;&#36866;&#24212;&#30340;&#20851;&#38190;&#22312;&#20110;&#22266;&#23450;&#20102;&#20195;&#29702;&#21644;&#29289;&#20307;&#20043;&#38388;&#30340;&#19968;&#33324;&#20132;&#20114;&#21160;&#24577;&#65292;&#21482;&#26356;&#26032;&#29305;&#23450;&#20110;&#29289;&#20307;&#30340;&#20559;&#22909;&#12290;&#25105;&#20204;&#30340;&#33258;&#36866;&#24212;&#22312;&#32447;&#36827;&#34892;&#65292;&#21482;&#38656;&#35201;&#19968;&#20010;&#20154;&#31867;&#24178;&#39044;&#12290;
&lt;/p&gt;
&lt;p&gt;
For robots to be effectively deployed in novel environments and tasks, they must be able to understand the feedback expressed by humans during intervention. This can either correct undesirable behavior or indicate additional preferences. Existing methods either require repeated episodes of interactions or assume prior known reward features, which is data-inefficient and can hardly transfer to new tasks. We relax these assumptions by describing human tasks in terms of object-centric sub-tasks and interpreting physical interventions in relation to specific objects. Our method, Object Preference Adaptation (OPA), is composed of two key stages: 1) pre-training a base policy to produce a wide variety of behaviors, and 2) online-updating according to human feedback. The key to our fast, yet simple adaptation is that general interaction dynamics between agents and objects are fixed, and only object-specific preferences are updated. Our adaptation occurs online, requires only one human interve
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20984;&#24615;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65292;&#21457;&#29616;&#26080;&#27861;&#20351;&#29992;&#38745;&#24577;&#22870;&#21169;&#20989;&#25968;&#34920;&#36798;&#30446;&#26631;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#20803;&#31639;&#27861;&#35299;&#20915;&#27492;&#38382;&#39064;&#65292;&#24182;&#32479;&#19968;&#20102;&#25991;&#29486;&#20013;&#30340;&#29616;&#26377;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2106.00661</link><description>&lt;p&gt;
&#22870;&#21169;&#36275;&#20197;&#22788;&#29702;&#20984;&#24615;MDP&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Reward is enough for convex MDPs. (arXiv:2106.00661v4 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.00661
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20984;&#24615;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65292;&#21457;&#29616;&#26080;&#27861;&#20351;&#29992;&#38745;&#24577;&#22870;&#21169;&#20989;&#25968;&#34920;&#36798;&#30446;&#26631;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#20803;&#31639;&#27861;&#35299;&#20915;&#27492;&#38382;&#39064;&#65292;&#24182;&#32479;&#19968;&#20102;&#25991;&#29486;&#20013;&#30340;&#29616;&#26377;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243; (MDP) &#20013;&#65292;&#26368;&#22823;&#21270;&#19968;&#20010;&#39532;&#23572;&#21487;&#22827;&#21644;&#24179;&#31283;&#30340;&#32047;&#31215;&#22870;&#21169;&#20989;&#25968;&#21487;&#20197;&#25429;&#25417;&#21040;&#35768;&#22810;&#30446;&#26631;&#12290;&#28982;&#32780;&#65292;&#24182;&#38750;&#25152;&#26377;&#30446;&#26631;&#37117;&#33021;&#20197;&#27492;&#26041;&#24335;&#25429;&#33719;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20984;&#24615;MDPs&#65292;&#20854;&#20013;&#30446;&#26631;&#26159;&#20316;&#20026;&#38745;&#24577;&#20998;&#24067;&#30340;&#20984;&#20989;&#25968;&#34920;&#36798;&#30340;&#65292;&#32467;&#26524;&#34920;&#26126;&#26080;&#27861;&#20351;&#29992;&#38745;&#24577;&#22870;&#21169;&#20989;&#25968;&#26469;&#34920;&#36798;&#30446;&#26631;&#12290;&#25105;&#20204;&#23558;&#20984;&#24615;MDP&#38382;&#39064;&#37325;&#26032;&#34920;&#36848;&#20026;&#25919;&#31574;&#21644;&#20195;&#20215;(&#36127;&#22870;&#21169;)&#8220;&#29609;&#23478;&#8221;&#30340;&#26368;&#23567;&#26368;&#22823;&#21338;&#24328;&#65292;&#21033;&#29992; Fenchel &#23545;&#20598;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#35299;&#20915;&#27492;&#38382;&#39064;&#30340;&#20803;&#31639;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#23427;&#32479;&#19968;&#20102;&#25991;&#29486;&#20013;&#35768;&#22810;&#29616;&#26377;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Maximising a cumulative reward function that is Markov and stationary, i.e., defined over state-action pairs and independent of time, is sufficient to capture many kinds of goals in a Markov decision process (MDP). However, not all goals can be captured in this manner. In this paper we study convex MDPs in which goals are expressed as convex functions of the stationary distribution and show that they cannot be formulated using stationary reward functions. Convex MDPs generalize the standard reinforcement learning (RL) problem formulation to a larger framework that includes many supervised and unsupervised RL problems, such as apprenticeship learning, constrained MDPs, and so-called `pure exploration'. Our approach is to reformulate the convex MDP problem as a min-max game involving policy and cost (negative reward) `players', using Fenchel duality. We propose a meta-algorithm for solving this problem and show that it unifies many existing algorithms in the literature.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#22522;&#20110;&#25512;&#36827;&#27169;&#22411;&#30340;&#26080;&#23494;&#24230;&#31163;&#32447;&#31639;&#27861; GAC&#65292;&#20351;&#29992;&#26368;&#22823;&#22343;&#20540;&#24046;&#29109;&#27491;&#21017;&#21270;&#22120;&#24179;&#34913;&#25506;&#32034;&#21644;&#24320;&#21457;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#20351;&#29992;&#33258;&#36866;&#24212;&#26426;&#21046;&#25552;&#39640;&#31639;&#27861;&#30340;&#31283;&#23450;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25512;&#36827;&#31574;&#30053;&#33021;&#22815;&#25552;&#39640;&#31639;&#27861;&#30340;&#25506;&#32034;&#25928;&#29575;&#21644;&#28176;&#36817;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2105.03733</link><description>&lt;p&gt;
&#29983;&#25104;&#24335;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;: &#20351;&#29992;&#25512;&#36827;&#27169;&#22411;&#30340;&#31163;&#32447;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Generative Actor-Critic: An Off-policy Algorithm Using the Push-forward Model. (arXiv:2105.03733v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2105.03733
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#22522;&#20110;&#25512;&#36827;&#27169;&#22411;&#30340;&#26080;&#23494;&#24230;&#31163;&#32447;&#31639;&#27861; GAC&#65292;&#20351;&#29992;&#26368;&#22823;&#22343;&#20540;&#24046;&#29109;&#27491;&#21017;&#21270;&#22120;&#24179;&#34913;&#25506;&#32034;&#21644;&#24320;&#21457;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#20351;&#29992;&#33258;&#36866;&#24212;&#26426;&#21046;&#25552;&#39640;&#31639;&#27861;&#30340;&#31283;&#23450;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25512;&#36827;&#31574;&#30053;&#33021;&#22815;&#25552;&#39640;&#31639;&#27861;&#30340;&#25506;&#32034;&#25928;&#29575;&#21644;&#28176;&#36817;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#27169;&#22411;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22312;&#35270;&#39057;&#28216;&#25103;&#12289;&#25512;&#33616;&#31995;&#32479;&#21644;&#26426;&#22120;&#20154;&#25511;&#21046;&#20219;&#21153;&#31561;&#35768;&#22810;&#39046;&#22495;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#22312;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#20013;&#65292;&#24191;&#27867;&#20351;&#29992;&#30340;&#20855;&#26377;&#39640;&#26031;&#20998;&#24067;&#30340;&#31574;&#30053;&#23548;&#33268;&#29615;&#22659;&#30340;&#25506;&#32034;&#20302;&#25928;&#65292;&#24182;&#19988;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#31639;&#27861;&#30340;&#24615;&#33021;&#21463;&#21040;&#38480;&#21046;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#23494;&#24230;&#30340;&#31163;&#32447;&#31639;&#27861;&#65292;&#31216;&#20026;&#29983;&#25104;&#24335;&#28436;&#21592;-&#35780;&#35770;&#23478;&#65288;Generative Actor-Critic&#65292;GAC&#65289;&#65292;&#20351;&#29992;&#25512;&#36827;&#27169;&#22411;&#26469;&#22686;&#21152;&#31574;&#30053;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;&#21516;&#26102;&#36824;&#21253;&#25324;&#19968;&#31181;&#29109;&#31867;&#25216;&#26415;&#65292;&#31216;&#20026;&#26368;&#22823;&#22343;&#20540;&#24046;&#65288;Maximum Mean Discrepancy&#65292;MMD&#65289;&#29109;&#27491;&#21017;&#21270;&#22120;&#65292;&#20197;&#24179;&#34913;&#25506;&#32034;&#21644;&#24320;&#21457;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#26426;&#21046;&#26469;&#33258;&#21160;&#32553;&#25918;&#36825;&#20010;&#27491;&#21017;&#21270;&#22120;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;GAC&#30340;&#31283;&#23450;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25512;&#36827;&#31574;&#30053;&#20855;&#26377;&#29702;&#24819;&#30340;&#29305;&#24449;&#65292;&#20363;&#22914;&#22810;&#27169;&#24335;&#24615;&#65292;&#21487;&#20197;&#26126;&#26174;&#25552;&#39640;&#31639;&#27861;&#30340;&#25506;&#32034;&#25928;&#29575;&#21644;&#28176;&#36817;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model-free deep reinforcement learning has achieved great success in many domains, such as video games, recommendation systems and robotic control tasks. In continuous control tasks, widely used policies with Gaussian distributions results in ineffective exploration of environments and limited performance of algorithms in many cases. In this paper, we propose a density-free off-policy algorithm, Generative Actor-Critic(GAC), using the push-forward model to increase the expressiveness of policies, which also includes an entropy-like technique, MMD-entropy regularizer, to balance the exploration and exploitation. Additionnally, we devise an adaptive mechanism to automatically scale this regularizer, which further improves the stability and robustness of GAC. The experiment results show that push-forward policies possess desirable features, such as multi-modality, which can improve the efficiency of exploration and asymptotic performance of algorithms obviously.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#30340;QCBA&#26041;&#27861;&#21487;&#20197;&#29992;&#20110;&#25913;&#36827;&#22522;&#20110;&#25968;&#20540;&#31867;&#22411;&#25968;&#25454;&#23398;&#20064;&#30340;&#35268;&#21017;&#20998;&#31867;&#22120;&#65292;&#24674;&#22797;&#39044;&#31163;&#25955;&#21270;&#36807;&#31243;&#20013;&#20002;&#22833;&#30340;&#20449;&#24687;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#21098;&#26525;&#25216;&#26415;&#12290;&#22312;22&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;FOIL2+QCBA&#30456;&#23545;&#20854;&#20182;&#22522;&#32447;&#26041;&#27861;&#32780;&#35328;&#65292;&#20855;&#26377;&#26356;&#39640;&#30340;&#39044;&#27979;&#24615;&#33021;&#21644;&#26356;&#23567;&#30340;&#27169;&#22411;&#22823;&#23567;&#12290;</title><link>http://arxiv.org/abs/1711.10166</link><description>&lt;p&gt;
QCBA: &#36890;&#36807;&#24674;&#22797;&#31163;&#25955;&#21270;&#36807;&#31243;&#20013;&#30340;&#20449;&#24687;&#25913;&#36827;&#22522;&#20110;&#25968;&#20540;&#25968;&#25454;&#23398;&#20064;&#30340;&#35268;&#21017;&#20998;&#31867;&#22120;
&lt;/p&gt;
&lt;p&gt;
QCBA: Improving Rule Classifiers Learned from Quantitative Data by Recovering Information Lost by Discretisation. (arXiv:1711.10166v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1711.10166
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#30340;QCBA&#26041;&#27861;&#21487;&#20197;&#29992;&#20110;&#25913;&#36827;&#22522;&#20110;&#25968;&#20540;&#31867;&#22411;&#25968;&#25454;&#23398;&#20064;&#30340;&#35268;&#21017;&#20998;&#31867;&#22120;&#65292;&#24674;&#22797;&#39044;&#31163;&#25955;&#21270;&#36807;&#31243;&#20013;&#20002;&#22833;&#30340;&#20449;&#24687;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#21098;&#26525;&#25216;&#26415;&#12290;&#22312;22&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;FOIL2+QCBA&#30456;&#23545;&#20854;&#20182;&#22522;&#32447;&#26041;&#27861;&#32780;&#35328;&#65292;&#20855;&#26377;&#26356;&#39640;&#30340;&#39044;&#27979;&#24615;&#33021;&#21644;&#26356;&#23567;&#30340;&#27169;&#22411;&#22823;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#20540;&#23646;&#24615;&#30340;&#39044;&#31163;&#25955;&#21270;&#26159;&#26576;&#20123;&#35268;&#21017;&#23398;&#20064;&#31639;&#27861;&#30340;&#24517;&#35201;&#27493;&#39588;&#65292;&#20294;&#26159;&#20250;&#23548;&#33268;&#19968;&#20123;&#25928;&#29575;&#20302;&#19979;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#26032;&#30340;&#35268;&#21017;&#35843;&#25972;&#27493;&#39588;&#20197;&#24674;&#22797;&#31163;&#25955;&#21270;&#20013;&#20002;&#22833;&#30340;&#20449;&#24687;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#21098;&#26525;&#25216;&#26415;&#65292;&#21487;&#20197;&#36827;&#19968;&#27493;&#20943;&#23567;&#35268;&#21017;&#27169;&#22411;&#30340;&#22823;&#23567;&#21644;&#25552;&#39640;&#20854;&#20934;&#30830;&#24615;&#12290;&#25552;&#20986;&#30340;QCBA&#26041;&#27861;&#26368;&#21021;&#26159;&#20026;&#20102;&#23545;&#22522;&#20110;&#20851;&#32852;&#24615;&#20998;&#31867;&#65288;CBA&#65289;&#31639;&#27861;&#29983;&#25104;&#30340;&#27169;&#22411;&#20013;&#30340;&#23450;&#37327;&#23646;&#24615;&#36827;&#34892;&#21518;&#22788;&#29702;&#65292;&#20294;&#20063;&#21487;&#24212;&#29992;&#20110;&#20854;&#20182;&#35268;&#21017;&#23398;&#20064;&#26041;&#27861;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#23545;&#20116;&#20010;&#20851;&#32852;&#35268;&#21017;&#20998;&#31867;&#31639;&#27861;&#65288;CBA&#12289;CMAR&#12289;CPAR&#12289;IDS&#12289;SBRL&#65289;&#21644;&#20004;&#20010;&#19968;&#38454;&#36923;&#36753;&#35268;&#21017;&#23398;&#20064;&#22120;&#65288;FOIL2&#21644;PRM&#65289;&#29983;&#25104;&#27169;&#22411;&#30340;&#21518;&#22788;&#29702;&#25928;&#26524;&#65292;&#20351;&#29992;UCI&#20179;&#24211;&#30340;22&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#65292;&#32467;&#26524;&#34920;&#26126;FOIL2+QCBA&#30456;&#27604;&#20110;&#25152;&#26377;&#19971;&#20010;&#22522;&#32447;&#30340;&#22823;&#23567;&#26356;&#23567;&#65292;&#24182;&#19988;&#20855;&#26377;&#26368;&#20339;&#30340;&#25972;&#20307;&#39044;&#27979;&#24615;&#33021;&#12290;&#21518;&#20248;&#21270;&#30340;CBA&#27169;&#22411;&#20855;&#26377;&#26356;&#22909;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
A prediscretisation of numerical attributes which is required by some rule learning algorithms is a source of inefficiencies. This paper describes new rule tuning steps that aim to recover lost information in the discretisation and new pruning techniques that may further reduce the size of rule models and improve their accuracy. The proposed QCBA method was initially developed to postprocess quantitative attributes in models generated by the Classification based on associations (CBA) algorithm, but it can also be applied to the results of other rule learning approaches. We demonstrate the effectiveness on the postprocessing of models generated by five association rule classification algorithms (CBA, CMAR, CPAR, IDS, SBRL) and two first-order logic rule learners (FOIL2 and PRM). Benchmarks on 22 datasets from the UCI repository show smaller size and the overall best predictive performance for FOIL2+QCBA compared to all seven baselines. Postoptimised CBA models have a better predictive p
&lt;/p&gt;</description></item></channel></rss>