<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27927;&#29260;&#33258;&#22238;&#24402;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#36816;&#21160;&#25554;&#20540;&#38382;&#39064;&#65292;&#21487;&#21033;&#29992;&#20219;&#24847;&#39034;&#24207;&#29983;&#25104;&#30340;&#33258;&#22238;&#24402;&#27169;&#22411;&#21644;&#24314;&#27169;&#20026;&#26377;&#21521;&#26080;&#29615;&#22270;&#30340;&#24103;&#20869;&#20381;&#36182;&#24615;&#65292;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.06367</link><description>&lt;p&gt;
&#27927;&#29260;&#33258;&#22238;&#24402;&#29992;&#20110;&#36816;&#21160;&#25554;&#20540;
&lt;/p&gt;
&lt;p&gt;
Shuffled Autoregression For Motion Interpolation. (arXiv:2306.06367v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06367
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27927;&#29260;&#33258;&#22238;&#24402;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#36816;&#21160;&#25554;&#20540;&#38382;&#39064;&#65292;&#21487;&#21033;&#29992;&#20219;&#24847;&#39034;&#24207;&#29983;&#25104;&#30340;&#33258;&#22238;&#24402;&#27169;&#22411;&#21644;&#24314;&#27169;&#20026;&#26377;&#21521;&#26080;&#29615;&#22270;&#30340;&#24103;&#20869;&#20381;&#36182;&#24615;&#65292;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#24037;&#20316;&#26088;&#22312;&#20026;&#36816;&#21160;&#25554;&#20540;&#20219;&#21153;&#25552;&#20379;&#28145;&#24230;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#20351;&#29992;&#20960;&#20309;&#26435;&#37325;&#20989;&#25968;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#20854;&#20182;&#19968;&#20123;&#24037;&#20316;&#21017;&#25552;&#20986;&#20102;&#31070;&#32463;&#32593;&#32476;&#26469;&#22788;&#29702;&#19981;&#21516;&#30340;&#38382;&#39064;&#35774;&#32622;&#65292;&#24182;&#20197;&#36830;&#32493;&#30340;&#23039;&#21183;&#24207;&#21015;&#20316;&#20026;&#36755;&#20837;&#12290;&#20294;&#26159;&#36816;&#21160;&#25554;&#20540;&#26159;&#19968;&#20010;&#26356;&#22797;&#26434;&#30340;&#38382;&#39064;&#65292;&#23427;&#23558;&#23396;&#31435;&#30340;&#23039;&#21183;&#65288;&#20363;&#22914;&#21482;&#20351;&#29992;&#19968;&#20010;&#36215;&#22987;&#23039;&#21183;&#21644;&#19968;&#20010;&#32467;&#26463;&#23039;&#21183;&#65289;&#20316;&#20026;&#36755;&#20837;&#12290;&#24403;&#24212;&#29992;&#20110;&#36816;&#21160;&#25554;&#20540;&#26102;&#65292;&#36825;&#20123;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#24615;&#33021;&#26377;&#38480;&#65292;&#22240;&#20026;&#23427;&#20204;&#26080;&#27861;&#21033;&#29992;&#21407;&#22987;&#20960;&#20309;&#20844;&#24335;&#20013;&#30340;&#28789;&#27963;&#20381;&#36182;&#20851;&#31995;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#31181;&#25554;&#20540;&#29305;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#31216;&#20026;&#8220;&#27927;&#29260;&#33258;&#22238;&#24402;&#8221;&#65292;&#23427;&#23558;&#33258;&#22238;&#24402;&#25193;&#23637;&#21040;&#20219;&#24847;&#65288;&#27927;&#29260;&#65289;&#39034;&#24207;&#29983;&#25104;&#65292;&#24182;&#23558;&#20219;&#20309;&#24103;&#38388;&#20381;&#36182;&#20851;&#31995;&#24314;&#27169;&#20026;&#26377;&#21521;&#26080;&#29615;&#22270;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#26500;&#24314;&#29305;&#23450;&#31867;&#22411;&#20381;&#36182;&#24615;&#22270;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#19977;&#20010;&#38454;&#27573;&#32452;&#35013;&#25104;&#19968;&#20010;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work aims to provide a deep-learning solution for the motion interpolation task. Previous studies solve it with geometric weight functions. Some other works propose neural networks for different problem settings with consecutive pose sequences as input. However, motion interpolation is a more complex problem that takes isolated poses (e.g., only one start pose and one end pose) as input. When applied to motion interpolation, these deep learning methods have limited performance since they do not leverage the flexible dependencies between interpolation frames as the original geometric formulas do. To realize this interpolation characteristic, we propose a novel framework, referred to as \emph{Shuffled AutoRegression}, which expands the autoregression to generate in arbitrary (shuffled) order and models any inter-frame dependencies as a directed acyclic graph. We further propose an approach to constructing a particular kind of dependency graph, with three stages assembled into an end
&lt;/p&gt;</description></item><item><title>Aria&#25968;&#23383;&#23402;&#29983;&#26159;&#19968;&#20010;&#33258;&#25105;&#20013;&#24515;&#25968;&#25454;&#38598;&#65292;&#20855;&#26377;&#20854;&#23427;&#20219;&#20309;&#25968;&#25454;&#38598;&#37117;&#27809;&#26377;&#30340;&#39640;&#31934;&#24230;&#12289;&#29031;&#29255;&#36924;&#30495;&#21644;&#35814;&#23613;&#30340;&#30495;&#23454;&#20449;&#24687;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#23558;&#25104;&#20026;&#33258;&#25105;&#20013;&#24515;&#26426;&#22120;&#24863;&#30693;&#35780;&#20272;&#30340;&#26032;&#26631;&#20934;&#12290;</title><link>http://arxiv.org/abs/2306.06362</link><description>&lt;p&gt;
Aria&#25968;&#23383;&#23402;&#29983;&#65306;&#19968;&#31181;&#26032;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#29992;&#20110;&#33258;&#25105;&#20013;&#24515;&#30340;3D&#26426;&#22120;&#24863;&#30693;&#12290;
&lt;/p&gt;
&lt;p&gt;
Aria Digital Twin: A New Benchmark Dataset for Egocentric 3D Machine Perception. (arXiv:2306.06362v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06362
&lt;/p&gt;
&lt;p&gt;
Aria&#25968;&#23383;&#23402;&#29983;&#26159;&#19968;&#20010;&#33258;&#25105;&#20013;&#24515;&#25968;&#25454;&#38598;&#65292;&#20855;&#26377;&#20854;&#23427;&#20219;&#20309;&#25968;&#25454;&#38598;&#37117;&#27809;&#26377;&#30340;&#39640;&#31934;&#24230;&#12289;&#29031;&#29255;&#36924;&#30495;&#21644;&#35814;&#23613;&#30340;&#30495;&#23454;&#20449;&#24687;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#23558;&#25104;&#20026;&#33258;&#25105;&#20013;&#24515;&#26426;&#22120;&#24863;&#30693;&#35780;&#20272;&#30340;&#26032;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25512;&#20986;&#20102;Aria&#25968;&#23383;&#23402;&#29983;&#65288;ADT&#65289;-&#19968;&#20010;&#20351;&#29992;Aria&#30524;&#38236;&#25429;&#33719;&#30340;&#33258;&#25105;&#20013;&#24515;&#25968;&#25454;&#38598;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#23545;&#35937;&#65292;&#29615;&#22659;&#21644;&#20154;&#31867;&#32423;&#21035;&#30340;&#30495;&#23454;&#25968;&#25454;&#12290;&#35813;ADT&#25968;&#25454;&#38598;&#21253;&#25324;200&#20010;&#30001;&#31359;&#25140;Aria&#35774;&#22791;&#30340;&#20154;&#22312;&#20004;&#20010;&#23460;&#20869;&#30495;&#23454;&#22330;&#26223;&#20013;&#25191;&#34892;&#30340;&#30495;&#23454;&#19990;&#30028;&#27963;&#21160;&#24207;&#21015;&#65292;&#21253;&#21547;398&#20010;&#23545;&#35937;&#23454;&#20363;&#65288;324&#20010;&#38745;&#24577;&#21644;74&#20010;&#21160;&#24577;&#65289;&#12290;&#27599;&#20010;&#24207;&#21015;&#21253;&#25324;&#65306;a&#65289;&#20004;&#20010;&#21333;&#33394;&#30456;&#26426;&#27969;&#65292;&#19968;&#20010;RGB&#30456;&#26426;&#27969;&#65292;&#20004;&#20010;IMU&#27969;&#30340;&#21407;&#22987;&#25968;&#25454;&#65307;b&#65289;&#23436;&#25972;&#30340;&#20256;&#24863;&#22120;&#26657;&#20934;&#65307;c&#65289;&#30495;&#23454;&#25968;&#25454;&#65292;&#21253;&#25324;Aria&#35774;&#22791;&#30340;&#36830;&#32493;6&#33258;&#30001;&#24230;&#65288;6DoF&#65289;&#23039;&#24577;&#65292;&#23545;&#35937;6DoF&#23039;&#24577;&#65292;3D&#27880;&#35270;&#30690;&#37327;&#65292;3D&#20154;&#20307;&#23039;&#24577;&#65292;2D&#22270;&#20687;&#20998;&#21106;&#65292;&#22270;&#20687;&#28145;&#24230;&#22270;&#65307;d&#65289;&#29031;&#29255;&#33324;&#30495;&#23454;&#30340;&#21512;&#25104;&#28210;&#26579;&#22270;&#20687;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#30446;&#21069;&#27809;&#26377;&#29616;&#26377;&#33258;&#25105;&#20013;&#24515;&#25968;&#25454;&#38598;&#33021;&#22815;&#19982;ADT&#30340;&#20934;&#30830;&#24615;&#12289;&#36924;&#30495;&#24230;&#21644;&#20840;&#38754;&#24615;&#30456;&#23218;&#32654;&#12290;&#36890;&#36807;&#21521;&#30740;&#31350;&#31038;&#21306;&#36129;&#29486;ADT&#65292;&#25105;&#20204;&#30340;&#20351;&#21629;&#26159;&#20026;&#33258;&#25105;&#20013;&#24515;&#26426;&#22120;&#24863;&#30693;&#30340;&#35780;&#20272;&#35774;&#31435;&#26032;&#30340;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce the Aria Digital Twin (ADT) - an egocentric dataset captured using Aria glasses with extensive object, environment, and human level ground truth. This ADT release contains 200 sequences of real-world activities conducted by Aria wearers in two real indoor scenes with 398 object instances (324 stationary and 74 dynamic). Each sequence consists of: a) raw data of two monochrome camera streams, one RGB camera stream, two IMU streams; b) complete sensor calibration; c) ground truth data including continuous 6-degree-of-freedom (6DoF) poses of the Aria devices, object 6DoF poses, 3D eye gaze vectors, 3D human poses, 2D image segmentations, image depth maps; and d) photo-realistic synthetic renderings. To the best of our knowledge, there is no existing egocentric dataset with a level of accuracy, photo-realism and comprehensiveness comparable to ADT. By contributing ADT to the research community, our mission is to set a new standard for evaluation in the egocentric machine perce
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#36816;&#21160;&#30340;&#32467;&#26500;&#30340;&#19977;&#32500;&#37325;&#24314;&#25216;&#26415;&#65292;&#21033;&#29992;&#19968;&#23545;HDR&#29031;&#30456;&#26426;&#21644;&#23460;&#20869;&#31227;&#21160;&#22320;&#38754;&#26426;&#22120;&#20154;&#36827;&#34892;&#25429;&#25417;&#21644;&#31639;&#27861;&#25512;&#31639;&#65292;&#23454;&#29616;&#23460;&#20869;&#31354;&#38388;&#30340;&#28145;&#24230;&#22270;&#21487;&#35270;&#21270;&#12290;</title><link>http://arxiv.org/abs/2306.06360</link><description>&lt;p&gt;
&#22522;&#20110;&#36816;&#21160;&#30340;&#32467;&#26500;&#30340;&#19977;&#32500;&#37325;&#24314;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
3D reconstruction using Structure for Motion. (arXiv:2306.06360v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06360
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#36816;&#21160;&#30340;&#32467;&#26500;&#30340;&#19977;&#32500;&#37325;&#24314;&#25216;&#26415;&#65292;&#21033;&#29992;&#19968;&#23545;HDR&#29031;&#30456;&#26426;&#21644;&#23460;&#20869;&#31227;&#21160;&#22320;&#38754;&#26426;&#22120;&#20154;&#36827;&#34892;&#25429;&#25417;&#21644;&#31639;&#27861;&#25512;&#31639;&#65292;&#23454;&#29616;&#23460;&#20869;&#31354;&#38388;&#30340;&#28145;&#24230;&#22270;&#21487;&#35270;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#21033;&#29992;&#19968;&#23545;HDR&#29031;&#30456;&#26426;&#36827;&#34892;&#23460;&#20869;&#31354;&#38388;&#30340;&#19977;&#32500;&#37325;&#24314;&#65292;&#35813;&#29031;&#30456;&#26426;&#20197;&#31435;&#20307;&#35270;&#35273;&#37197;&#32622;&#23433;&#35013;&#22312;&#23460;&#20869;&#31227;&#21160;&#22320;&#38754;&#26426;&#22120;&#20154;&#19978;&#65292;&#25429;&#25417;&#21508;&#31181;&#32441;&#29702;&#21644;&#31354;&#38388;&#29305;&#24449;&#24182;&#23558;&#20854;&#20316;&#20026;2D&#22270;&#20687;&#20256;&#32473;&#25105;&#20204;&#30340;&#31639;&#27861;&#65292;&#36827;&#32780;&#23454;&#29616;&#28145;&#24230;&#22270;&#30340;&#21487;&#35270;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
We are working towards 3D reconstruction of indoor spaces using a pair of HDR cameras in a stereo vision configuration mounted on an indoor mobile floor robot that captures various textures and spatial features as 2D images and this data is simultaneously utilized as a feed to our algorithm which will allow us to visualize the depth map.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20197;&#21463;&#21040;&#35821;&#35328;&#25351;&#23548;&#30340;&#22330;&#26223;&#32423;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#30495;&#23454;&#19988;&#21487;&#25511;&#30340;&#20132;&#36890;&#65292;&#24182;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23558;&#29992;&#25143;&#30340;&#26597;&#35810;&#36716;&#21270;&#20026;&#25439;&#22833;&#20989;&#25968;&#65292;&#25351;&#23548;&#27169;&#22411;&#29983;&#25104;&#31526;&#21512;&#26597;&#35810;&#26465;&#20214;&#30340;&#20223;&#30495;&#20132;&#36890;&#12290;</title><link>http://arxiv.org/abs/2306.06344</link><description>&lt;p&gt;
&#35821;&#35328;&#25351;&#23548;&#19979;&#30340;&#22330;&#26223;&#32423;&#20132;&#36890;&#20223;&#30495;&#27169;&#25311;
&lt;/p&gt;
&lt;p&gt;
Language-Guided Traffic Simulation via Scene-Level Diffusion. (arXiv:2306.06344v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06344
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20197;&#21463;&#21040;&#35821;&#35328;&#25351;&#23548;&#30340;&#22330;&#26223;&#32423;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#30495;&#23454;&#19988;&#21487;&#25511;&#30340;&#20132;&#36890;&#65292;&#24182;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23558;&#29992;&#25143;&#30340;&#26597;&#35810;&#36716;&#21270;&#20026;&#25439;&#22833;&#20989;&#25968;&#65292;&#25351;&#23548;&#27169;&#22411;&#29983;&#25104;&#31526;&#21512;&#26597;&#35810;&#26465;&#20214;&#30340;&#20223;&#30495;&#20132;&#36890;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#29616;&#30495;&#23454;&#21644;&#21487;&#25511;&#30340;&#20132;&#36890;&#20223;&#30495;&#26159;&#21152;&#36895;&#33258;&#20027;&#39550;&#39542;&#27773;&#36710;&#65288;AV&#65289;&#21457;&#23637;&#30340;&#26680;&#24515;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#29992;&#20110;&#25511;&#21046;&#22522;&#20110;&#23398;&#20064;&#30340;&#20132;&#36890;&#27169;&#22411;&#30340;&#26041;&#27861;&#38656;&#35201;&#22823;&#37327;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#65292;&#23545;&#20110;&#20174;&#19994;&#32773;&#26469;&#35828;&#24456;&#38590;&#20351;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CTG++&#65292;&#19968;&#31181;&#21487;&#20197;&#21463;&#21040;&#35821;&#35328;&#25351;&#23548;&#30340;&#22330;&#26223;&#32423;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#12290;&#20026;&#20102;&#36798;&#21040;&#36825;&#20010;&#30446;&#30340;&#65292;&#25105;&#20204;&#38656;&#35201;&#35299;&#20915;&#20004;&#20010;&#38382;&#39064;&#65306;&#38656;&#35201;&#19968;&#20010;&#30495;&#23454;&#21644;&#21487;&#25511;&#30340;&#20132;&#36890;&#27169;&#22411;&#39592;&#24178;&#32467;&#26500;&#65292;&#24182;&#19988;&#35201;&#26377;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#20351;&#29992;&#35821;&#35328;&#19982;&#20132;&#36890;&#27169;&#22411;&#36827;&#34892;&#20132;&#20114;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#20010;&#24102;&#26377;&#26102;&#31354;&#36716;&#25442;&#22120;&#39592;&#24178;&#32467;&#26500;&#30340;&#22330;&#26223;&#32423;&#25193;&#25955;&#27169;&#22411;&#65292;&#23427;&#29983;&#25104;&#20102;&#30495;&#23454;&#21644;&#21487;&#25511;&#30340;&#20132;&#36890;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23558;&#29992;&#25143;&#30340;&#26597;&#35810;&#36716;&#25442;&#20026;&#25439;&#22833;&#20989;&#25968;&#65292;&#25351;&#23548;&#25193;&#25955;&#27169;&#22411;&#26397;&#30528;&#26597;&#35810;&#21512;&#35268;&#30340;&#29983;&#25104;&#26041;&#21521;&#21069;&#36827;&#12290;&#36890;&#36807;&#20840;&#38754;&#30340;&#35780;&#20272;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Realistic and controllable traffic simulation is a core capability that is necessary to accelerate autonomous vehicle (AV) development. However, current approaches for controlling learning-based traffic models require significant domain expertise and are difficult for practitioners to use. To remedy this, we present CTG++, a scene-level conditional diffusion model that can be guided by language instructions. Developing this requires tackling two challenges: the need for a realistic and controllable traffic model backbone, and an effective method to interface with a traffic model using language. To address these challenges, we first propose a scene-level diffusion model equipped with a spatio-temporal transformer backbone, which generates realistic and controllable traffic. We then harness a large language model (LLM) to convert a user's query into a loss function, guiding the diffusion model towards query-compliant generation. Through comprehensive evaluation, we demonstrate the effect
&lt;/p&gt;</description></item><item><title>DocumentCLIP&#26159;&#19968;&#31181;&#26174;&#33879;&#24615;&#24863;&#30693;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#29702;&#35299;&#25991;&#26723;&#20869;&#38271;&#25991;&#26412;&#21644;&#22270;&#20687;&#20043;&#38388;&#30340;&#20132;&#20114;&#20316;&#29992;&#12290;&#25105;&#20204;&#26159;&#31532;&#19968;&#20010;&#22312;&#22810;&#27169;&#24577;&#25991;&#26723;&#20869;&#37096;&#38142;&#25509;&#26041;&#38754;&#36827;&#34892;&#23545;&#27604;&#23398;&#20064;&#30340;&#20154;&#12290;</title><link>http://arxiv.org/abs/2306.06306</link><description>&lt;p&gt;
DocumentCLIP&#65306;&#38142;&#25509;&#25442;&#34892;&#25991;&#20214;&#20013;&#30340;&#22270;&#20687;&#21644;&#27491;&#25991;&#25991;&#26412;
&lt;/p&gt;
&lt;p&gt;
DocumentCLIP: Linking Figures and Main Body Text in Reflowed Documents. (arXiv:2306.06306v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06306
&lt;/p&gt;
&lt;p&gt;
DocumentCLIP&#26159;&#19968;&#31181;&#26174;&#33879;&#24615;&#24863;&#30693;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#29702;&#35299;&#25991;&#26723;&#20869;&#38271;&#25991;&#26412;&#21644;&#22270;&#20687;&#20043;&#38388;&#30340;&#20132;&#20114;&#20316;&#29992;&#12290;&#25105;&#20204;&#26159;&#31532;&#19968;&#20010;&#22312;&#22810;&#27169;&#24577;&#25991;&#26723;&#20869;&#37096;&#38142;&#25509;&#26041;&#38754;&#36827;&#34892;&#23545;&#27604;&#23398;&#20064;&#30340;&#20154;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;&#36890;&#36807;&#29702;&#35299;&#22270;&#20687;&#21644;&#25991;&#26412;&#20043;&#38388;&#30340;&#23545;&#40784;&#32780;&#22312;&#25903;&#25345;&#22810;&#23186;&#20307;&#24212;&#29992;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;&#20027;&#35201;&#38598;&#20013;&#22312;&#29702;&#35299;&#21333;&#20010;&#22270;&#20687;&#19982;&#19968;&#27573;&#25991;&#26412;&#30456;&#20851;&#32852;&#65292;&#32780;&#24448;&#24448;&#24573;&#30053;&#20102;&#22810;&#21477;&#35805;&#19982;&#22810;&#20010;&#22270;&#20687;&#32452;&#25104;&#30340;&#25991;&#26723;&#20869;&#37096;&#30340;&#23545;&#40784;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;DocumentCLIP&#30340;&#26174;&#33879;&#24615;&#24863;&#30693;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#20197;&#24378;&#21046;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;&#29702;&#35299;&#25991;&#26723;&#20869;&#38271;&#25991;&#26412;&#21644;&#22270;&#20687;&#20043;&#38388;&#30340;&#20132;&#20114;&#20316;&#29992;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#26377;&#21161;&#20110;&#23545;&#26032;&#38395;&#25991;&#31456;&#12289;&#26434;&#24535;&#12289;&#20135;&#21697;&#25551;&#36848;&#31561;&#20855;&#26377;&#26356;&#20016;&#23500;&#35821;&#35328;&#21644;&#35270;&#35273;&#20869;&#23481;&#30340;&#30495;&#23454;&#19990;&#30028;&#22810;&#27169;&#24577;&#25991;&#26723;&#29702;&#35299;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#25105;&#20204;&#26159;&#31532;&#19968;&#20010;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#25506;&#32034;&#22810;&#27169;&#24577;&#25991;&#26723;&#20869;&#37096;&#38142;&#25509;&#30340;&#20154;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;&#19968;&#20010;&#22823;&#22411;&#30340;Wikipedia&#25968;&#25454;&#38598;&#36827;&#34892;&#39044;&#22521;&#35757;&#65292;&#20026;&#35757;&#32451;&#25552;&#20379;&#20102;&#22810;&#26679;&#21270;&#30340;&#26469;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision-language pretraining models have achieved great success in supporting multimedia applications by understanding the alignments between images and text. While existing vision-language pretraining models primarily focus on understanding single image associated with a single piece of text, they often ignore the alignment at the intra-document level, consisting of multiple sentences with multiple images. In this work, we propose DocumentCLIP, a salience-aware contrastive learning framework to enforce vision-language pretraining models to comprehend the interaction between images and longer text within documents. Our model is beneficial for the real-world multimodal document understanding like news article, magazines, product descriptions, which contain linguistically and visually richer content. To the best of our knowledge, we are the first to explore multimodal intra-document links by contrastive learning. In addition, we collect a large Wikipedia dataset for pretraining, which pro
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;NeRFBK&#30340;&#26032;&#30340;&#30495;&#23454;&#21644;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#19987;&#38376;&#29992;&#20110;&#27979;&#35797;&#21644;&#27604;&#36739;&#22522;&#20110;NeRF&#30340;3D&#37325;&#24314;&#31639;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#25910;&#38598;&#20855;&#26377;&#31934;&#30830;&#22320;&#38754;&#23454;&#20917;&#30340;&#22810;&#26679;&#21270;&#25968;&#25454;&#30340;&#25361;&#25112;&#38382;&#39064;&#12290;&#35813;&#25968;&#25454;&#38598;&#26377;&#26395;&#25512;&#21160;3D&#37325;&#24314;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;</title><link>http://arxiv.org/abs/2306.06300</link><description>&lt;p&gt;
NERFBK:&#38024;&#23545;&#22522;&#20110;NeRF&#30340;3D&#37325;&#24314;&#30340;&#39640;&#36136;&#37327;&#22522;&#20934;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
NERFBK: A High-Quality Benchmark for NERF-Based 3D Reconstruction. (arXiv:2306.06300v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06300
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;NeRFBK&#30340;&#26032;&#30340;&#30495;&#23454;&#21644;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#19987;&#38376;&#29992;&#20110;&#27979;&#35797;&#21644;&#27604;&#36739;&#22522;&#20110;NeRF&#30340;3D&#37325;&#24314;&#31639;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#25910;&#38598;&#20855;&#26377;&#31934;&#30830;&#22320;&#38754;&#23454;&#20917;&#30340;&#22810;&#26679;&#21270;&#25968;&#25454;&#30340;&#25361;&#25112;&#38382;&#39064;&#12290;&#35813;&#25968;&#25454;&#38598;&#26377;&#26395;&#25512;&#21160;3D&#37325;&#24314;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#19968;&#20010;&#21517;&#20026;NeRFBK&#30340;&#26032;&#30340;&#30495;&#23454;&#21644;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#19987;&#38376;&#29992;&#20110;&#27979;&#35797;&#21644;&#27604;&#36739;&#22522;&#20110;NeRF&#30340;3D&#37325;&#24314;&#31639;&#27861;&#12290;&#39640;&#36136;&#37327;&#30340;3D&#37325;&#24314;&#22312;&#21508;&#20010;&#39046;&#22495;&#37117;&#26377;&#37325;&#35201;&#30340;&#28508;&#21147;&#65292;&#22522;&#20110;&#22270;&#20687;&#30340;&#31639;&#27861;&#30340;&#25913;&#36827;&#20351;&#24471;&#35780;&#20272;&#26032;&#30340;&#20808;&#36827;&#25216;&#26415;&#21464;&#24471;&#24517;&#19981;&#21487;&#23569;&#12290;&#28982;&#32780;&#65292;&#25910;&#38598;&#20855;&#26377;&#31934;&#30830;&#22320;&#38754;&#23454;&#20917;&#30340;&#22810;&#26679;&#21270;&#25968;&#25454;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#19988;&#21487;&#33021;&#26080;&#27861;&#28085;&#30422;&#25152;&#26377;&#30456;&#20851;&#30340;&#24212;&#29992;&#12290;NeRFBK&#25968;&#25454;&#38598;&#36890;&#36807;&#25552;&#20379;&#22810;&#23610;&#24230;&#12289;&#23460;&#20869;&#12289;&#23460;&#22806;&#25968;&#25454;&#38598;&#21450;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#12289;&#35270;&#39057;&#21644;&#30456;&#26426;&#21442;&#25968;&#31561;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20197;&#27979;&#35797;&#21644;&#27604;&#36739;&#22522;&#20110;NeRF&#30340;&#31639;&#27861;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;NeRFBK&#22522;&#20934;&#30340;&#35774;&#35745;&#21644;&#21019;&#24314;&#12289;&#21508;&#31181;&#20363;&#23376;&#21644;&#24212;&#29992;&#22330;&#26223;&#65292;&#24182;&#24378;&#35843;&#20102;&#23427;&#22312;&#25512;&#21160;3D&#37325;&#24314;&#39046;&#22495;&#21457;&#23637;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a new real and synthetic dataset called NeRFBK specifically designed for testing and comparing NeRF-based 3D reconstruction algorithms. High-quality 3D reconstruction has significant potential in various fields, and advancements in image-based algorithms make it essential to evaluate new advanced techniques. However, gathering diverse data with precise ground truth is challenging and may not encompass all relevant applications. The NeRFBK dataset addresses this issue by providing multi-scale, indoor and outdoor datasets with high-resolution images and videos and camera parameters for testing and comparing NeRF-based algorithms. This paper presents the design and creation of the NeRFBK benchmark, various examples and application scenarios, and highlights its potential for advancing the field of 3D reconstruction.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#20004;&#20010;&#21327;&#35758;&#65292;&#26088;&#22312;&#20445;&#25252;LLM&#25552;&#31034;&#30340;&#30693;&#35782;&#20135;&#26435;&#65292;&#25552;&#20379;&#24320;&#25918;&#24066;&#22330;&#19978;&#20132;&#26131;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.06297</link><description>&lt;p&gt;
LLM&#24212;&#29992;&#20013;&#30340;IP&#20445;&#25252;&#21327;&#35758;
&lt;/p&gt;
&lt;p&gt;
Protect Your Prompts: Protocols for IP Protection in LLM Applications. (arXiv:2306.06297v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06297
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#20004;&#20010;&#21327;&#35758;&#65292;&#26088;&#22312;&#20445;&#25252;LLM&#25552;&#31034;&#30340;&#30693;&#35782;&#20135;&#26435;&#65292;&#25552;&#20379;&#24320;&#25918;&#24066;&#22330;&#19978;&#20132;&#26131;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24418;&#24335;&#30340;AI&#30340;&#24555;&#36895;&#37319;&#29992;&#65292;&#31934;&#24515;&#35774;&#35745;&#30340;&#25552;&#31034;&#30340;&#28508;&#22312;&#20215;&#20540;&#21464;&#24471;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#23454;&#29616;&#36825;&#20010;&#28508;&#21147;&#65292;&#25552;&#31034;&#24212;&#35813;&#22312;&#19968;&#20010;&#20844;&#24320;&#24066;&#22330;&#19978;&#20132;&#26131;&#12290;&#30001;&#20110;&#25552;&#31034;&#30446;&#21069;&#36890;&#24120;&#26159;&#32463;&#27982;&#19978;&#19981;&#21487;&#25490;&#38500;&#30340;&#65292;&#22240;&#20026;&#23427;&#20204;&#26159;&#25991;&#26412;&#30340;&#24615;&#36136;&#65292;&#22240;&#27492;&#23578;&#26410;&#24314;&#31435;&#19968;&#33324;&#30340;&#31454;&#20105;&#24066;&#22330;&#12290;&#26412;&#25991;&#35752;&#35770;&#20102;&#20004;&#20010;&#26088;&#22312;&#25552;&#20379;&#25552;&#31034;&#20445;&#25252;&#30340;&#21327;&#35758;&#65292;&#25552;&#39640;&#20854;&#20316;&#20026;&#30693;&#35782;&#20135;&#26435;&#30340;&#22320;&#20301;&#65292;&#20174;&#32780;&#30830;&#35748;&#25552;&#31034;&#24037;&#31243;&#24072;&#30340;&#30693;&#35782;&#20135;&#26435;&#65292;&#24182;&#21487;&#33021;&#25903;&#25345;LLM&#25552;&#31034;&#30340;&#24320;&#25918;&#24066;&#22330;&#30340;&#32321;&#33635;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the rapid adoption of AI in the form of large language models (LLMs), the potential value of carefully engineered prompts has become significant. However, to realize this potential, prompts should be tradable on an open market. Since prompts are, at present, generally economically non-excludable, by virtue of their nature as text, no general competitive market has yet been established. This note discusses two protocols intended to provide protection of prompts, elevating their status as intellectual property, thus confirming the intellectual property rights of prompt engineers, and potentially supporting the flourishing of an open market for LLM prompts.
&lt;/p&gt;</description></item><item><title>CausalSAT&#20351;&#29992;&#22240;&#26524;&#25512;&#29702;&#25581;&#31034;&#29616;&#20195;SAT&#27714;&#35299;&#22120;&#30340;&#20869;&#37096;&#36816;&#20316;&#26426;&#21046;&#12290;</title><link>http://arxiv.org/abs/2306.06294</link><description>&lt;p&gt;
&#20351;&#29992;&#22240;&#26524;&#25512;&#29702;&#35299;&#37322;SAT&#27714;&#35299;
&lt;/p&gt;
&lt;p&gt;
Explaining SAT Solving Using Causal Reasoning. (arXiv:2306.06294v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06294
&lt;/p&gt;
&lt;p&gt;
CausalSAT&#20351;&#29992;&#22240;&#26524;&#25512;&#29702;&#25581;&#31034;&#29616;&#20195;SAT&#27714;&#35299;&#22120;&#30340;&#20869;&#37096;&#36816;&#20316;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#19977;&#21313;&#24180;&#26469;&#65292;&#35774;&#35745;&#39640;&#25928;&#30340;SAT&#27714;&#35299;&#22120;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#29616;&#20195;&#27714;&#35299;&#22120;&#33021;&#22815;&#22312;&#20960;&#31186;&#38047;&#20869;&#35299;&#20915;&#21253;&#21547;&#25968;&#30334;&#19975;&#20010;&#21464;&#37327;&#30340;&#24037;&#19994;&#22522;&#20934;&#65292;&#20854;&#25104;&#21151;&#24402;&#21151;&#20110;&#24191;&#27867;&#20351;&#29992;&#30340;CDCL&#31639;&#27861;&#65292;&#20294;&#32570;&#20047;&#20840;&#38754;&#30340;&#29702;&#35770;&#30740;&#31350;&#12290;&#27492;&#22806;&#65292;&#24050;&#32463;&#35266;&#23519;&#21040;CDCL&#27714;&#35299;&#22120;&#20173;&#28982;&#38590;&#20197;&#22788;&#29702;&#29305;&#23450;&#31867;&#21035;&#30340;&#22522;&#20934;&#65292;&#20854;&#20013;&#21482;&#21253;&#21547;&#25968;&#30334;&#20010;&#21464;&#37327;&#65292;&#36825;&#19982;&#23427;&#20204;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#24191;&#27867;&#20351;&#29992;&#24418;&#25104;&#20102;&#21453;&#24046;&#12290;&#22240;&#27492;&#65292;&#26377;&#36843;&#20999;&#38656;&#35201;&#25581;&#31034;&#36825;&#20123;&#30475;&#20284;&#34180;&#24369;&#20294;&#24378;&#22823;&#30340;&#40657;&#21283;&#23376;&#30340;&#20869;&#37096;&#36816;&#20316;&#26426;&#21046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#31216;&#20026;CausalSAT&#30340;&#26041;&#27861;&#65292;&#37319;&#29992;&#22240;&#26524;&#25512;&#29702;&#26469;&#28145;&#20837;&#20102;&#35299;&#29616;&#20195;SAT&#27714;&#35299;&#22120;&#30340;&#21151;&#33021;&#65292;&#36825;&#26159;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#30340;&#31532;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
The past three decades have witnessed notable success in designing efficient SAT solvers, with modern solvers capable of solving industrial benchmarks containing millions of variables in just a few seconds. The success of modern SAT solvers owes to the widely-used CDCL algorithm, which lacks comprehensive theoretical investigation. Furthermore, it has been observed that CDCL solvers still struggle to deal with specific classes of benchmarks comprising only hundreds of variables, which contrasts with their widespread use in real-world applications. Consequently, there is an urgent need to uncover the inner workings of these seemingly weak yet powerful black boxes.  In this paper, we present a first step towards this goal by introducing an approach called CausalSAT, which employs causal reasoning to gain insights into the functioning of modern SAT solvers. CausalSAT initially generates observational data from the execution of SAT solvers and learns a structured graph representing the cau
&lt;/p&gt;</description></item><item><title>&#26412;&#39033;&#30446;&#25552;&#20986;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#20351;&#24471;&#21363;&#20351;&#26159;&#19994;&#20313;&#29233;&#22909;&#32773;&#20063;&#33021;&#22815;&#21019;&#20316;&#33258;&#24049;&#30340;&#38899;&#20048;&#20316;&#21697;&#65292;&#20854;&#21019;&#26032;&#22312;&#20110;&#36755;&#20837;&#33410;&#25293;&#29983;&#25104;&#21333;&#22768;&#37096;&#30340;&#26059;&#24459;&#65292;&#25552;&#20986;&#20102;&#19977;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#20998;&#21035;&#20026;&#20840;&#23616;&#27880;&#24847;&#21147;&#30340;LSTM&#12289;&#23616;&#37096;&#27880;&#24847;&#21147;&#30340;LSTM&#20197;&#21450;&#30456;&#23545;&#20301;&#32622;&#34920;&#31034;&#30340;Transformer&#65292;&#29983;&#25104;&#30340;&#38899;&#20048;&#20855;&#26377;&#20016;&#23500;&#30340;&#21464;&#21270;&#12289;&#21644;&#35856;&#21644;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2306.06284</link><description>&lt;p&gt;
&#20247;&#21019;&#26354;&#65306;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#29983;&#25104;&#38899;&#20048;&#33410;&#25293;
&lt;/p&gt;
&lt;p&gt;
Everybody Compose: Deep Beats To Music. (arXiv:2306.06284v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06284
&lt;/p&gt;
&lt;p&gt;
&#26412;&#39033;&#30446;&#25552;&#20986;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#20351;&#24471;&#21363;&#20351;&#26159;&#19994;&#20313;&#29233;&#22909;&#32773;&#20063;&#33021;&#22815;&#21019;&#20316;&#33258;&#24049;&#30340;&#38899;&#20048;&#20316;&#21697;&#65292;&#20854;&#21019;&#26032;&#22312;&#20110;&#36755;&#20837;&#33410;&#25293;&#29983;&#25104;&#21333;&#22768;&#37096;&#30340;&#26059;&#24459;&#65292;&#25552;&#20986;&#20102;&#19977;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#20998;&#21035;&#20026;&#20840;&#23616;&#27880;&#24847;&#21147;&#30340;LSTM&#12289;&#23616;&#37096;&#27880;&#24847;&#21147;&#30340;LSTM&#20197;&#21450;&#30456;&#23545;&#20301;&#32622;&#34920;&#31034;&#30340;Transformer&#65292;&#29983;&#25104;&#30340;&#38899;&#20048;&#20855;&#26377;&#20016;&#23500;&#30340;&#21464;&#21270;&#12289;&#21644;&#35856;&#21644;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#39033;&#30446;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36755;&#20837;&#33410;&#25293;&#29983;&#25104;&#21333;&#22768;&#37096;&#30340;&#26059;&#24459;&#65292;&#20351;&#24471;&#21363;&#20351;&#26159;&#19994;&#20313;&#29233;&#22909;&#32773;&#20063;&#33021;&#22815;&#21019;&#20316;&#33258;&#24049;&#30340;&#38899;&#20048;&#20316;&#21697;&#12290;&#35813;&#39033;&#30446;&#25552;&#20986;&#20102;&#19977;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#8212;&#8212;&#24102;&#26377;&#20840;&#23616;&#27880;&#24847;&#21147;&#30340;LSTM&#12289;&#24102;&#26377;&#23616;&#37096;&#27880;&#24847;&#21147;&#30340;LSTM&#21644;&#24102;&#26377;&#30456;&#23545;&#20301;&#32622;&#34920;&#31034;&#30340;Transformer&#65292;&#20026;&#29983;&#25104;&#30340;&#38899;&#20048;&#25552;&#20379;&#20102;&#20016;&#23500;&#30340;&#21464;&#21270;&#12289;&#21644;&#35856;&#21644;&#32467;&#26500;&#12290;&#35813;&#39033;&#30446;&#20801;&#35768;&#20219;&#20309;&#20154;&#36890;&#36807;&#25970;&#20987;&#38190;&#30424;&#25110;&#8220;&#37325;&#26032;&#35843;&#25972;&#8221;&#29616;&#26377;&#20316;&#21697;&#30340;&#33410;&#25293;&#24207;&#21015;&#26469;&#21019;&#20316;&#33258;&#24049;&#30340;&#38899;&#20048;&#12290;
&lt;/p&gt;
&lt;p&gt;
This project presents a deep learning approach to generate monophonic melodies based on input beats, allowing even amateurs to create their own music compositions. Three effective methods - LSTM with Full Attention, LSTM with Local Attention, and Transformer with Relative Position Representation - are proposed for this novel task, providing great variation, harmony, and structure in the generated music. This project allows anyone to compose their own music by tapping their keyboards or ``recoloring'' beat sequences from existing works.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20010;&#36965;&#24863;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#65292;&#21517;&#20026;DeepLCZChange&#65292;&#32467;&#21512;&#20102;&#31354;&#20013;LiDAR&#25968;&#25454;&#21644;Landsat 8&#21355;&#26143;&#30340;&#22320;&#34920;&#28201;&#24230;&#20135;&#21697;&#65292;&#29992;&#20110;&#30740;&#31350;&#22478;&#24066;&#22303;&#22320;&#21033;&#29992;&#19982;&#24403;&#22320;&#27668;&#20505;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#22312;&#32445;&#32422;&#24066;&#30340;&#24212;&#29992;&#20013;&#39564;&#35777;&#20102;&#22478;&#24066;&#26862;&#26519;&#30340;&#38477;&#28201;&#25928;&#24212;&#12290;</title><link>http://arxiv.org/abs/2306.06269</link><description>&lt;p&gt;
DeepLCZChange&#65306;&#19968;&#31181;&#29992;&#20110;&#22478;&#24066;&#27668;&#20505;&#38887;&#24615;&#30340;&#36965;&#24863;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
DeepLCZChange: A Remote Sensing Deep Learning Model Architecture for Urban Climate Resilience. (arXiv:2306.06269v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06269
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20010;&#36965;&#24863;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#65292;&#21517;&#20026;DeepLCZChange&#65292;&#32467;&#21512;&#20102;&#31354;&#20013;LiDAR&#25968;&#25454;&#21644;Landsat 8&#21355;&#26143;&#30340;&#22320;&#34920;&#28201;&#24230;&#20135;&#21697;&#65292;&#29992;&#20110;&#30740;&#31350;&#22478;&#24066;&#22303;&#22320;&#21033;&#29992;&#19982;&#24403;&#22320;&#27668;&#20505;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#22312;&#32445;&#32422;&#24066;&#30340;&#24212;&#29992;&#20013;&#39564;&#35777;&#20102;&#22478;&#24066;&#26862;&#26519;&#30340;&#38477;&#28201;&#25928;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22478;&#24066;&#22303;&#22320;&#21033;&#29992;&#32467;&#26500;&#20250;&#24433;&#21709;&#37117;&#24066;&#21306;&#30340;&#23616;&#37096;&#27668;&#20505;&#26465;&#20214;&#12290;&#20026;&#20102;&#25581;&#31034;&#22478;&#24066;&#22303;&#22320;&#21033;&#29992;&#19982;&#24403;&#22320;&#27668;&#20505;&#20043;&#38388;&#30340;&#26426;&#21046;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25968;&#25454;&#39537;&#21160;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#21644;&#27969;&#31243;&#65292;DeepLCZChange&#65292;&#20197;&#30456;&#20851;&#31354;&#20013;LiDAR&#25968;&#25454;&#32479;&#35745;&#25968;&#25454;&#19982;Landsat 8&#21355;&#26143;&#30340;&#22320;&#34920;&#28201;&#24230;&#20135;&#21697;&#30456;&#20851;&#32852;&#12290;&#19968;&#20010;&#27010;&#24565;&#39564;&#35777;&#25968;&#20540;&#23454;&#39564;&#21033;&#29992;&#32445;&#32422;&#24066;&#30340;&#30456;&#24212;&#36965;&#24863;&#25968;&#25454;&#39564;&#35777;&#20102;&#22478;&#24066;&#26862;&#26519;&#30340;&#38477;&#28201;&#25928;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
Urban land use structures impact local climate conditions of metropolitan areas. To shed light on the mechanism of local climate wrt. urban land use, we present a novel, data-driven deep learning architecture and pipeline, DeepLCZChange, to correlate airborne LiDAR data statistics with the Landsat 8 satellite's surface temperature product. A proof-of-concept numerical experiment utilizes corresponding remote sensing data for the city of New York to verify the cooling effect of urban forests.
&lt;/p&gt;</description></item><item><title>&#20915;&#31574;&#22534;&#21472;&#26159;&#19968;&#31181;&#28789;&#27963;&#30340;&#29983;&#25104;&#26694;&#26550;&#65292;&#23558;&#30446;&#26631;&#26465;&#20214;&#21270;&#31574;&#30053;&#20195;&#29702;&#20998;&#35299;&#20026;3&#20010;&#29983;&#25104;&#27169;&#22359;&#65292;&#24182;&#36890;&#36807;&#25945;&#24072;&#24378;&#21046;&#24182;&#34892;&#23398;&#20064;&#65292;&#21487;&#20197;&#29992;&#20110;&#35774;&#35745;&#21333;&#20010;&#27169;&#22359;&#20197;&#32771;&#34385;&#26550;&#26500;&#20559;&#24046;&#12289;&#20248;&#21270;&#30446;&#26631;&#21644;&#21160;&#24577;&#12289;&#36328;&#39046;&#22495;&#30340;&#21487;&#36716;&#31227;&#24615;&#21644;&#25512;&#29702;&#36895;&#24230;&#12290;&#22312;&#36830;&#32493;&#25511;&#21046;&#22522;&#20934;&#27979;&#35797;&#20013;&#20855;&#26377;&#28789;&#27963;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.06253</link><description>&lt;p&gt;
&#36890;&#36807;&#27169;&#22359;&#21270;&#29983;&#25104;&#27169;&#22411;&#23454;&#29616;&#28789;&#27963;&#30340;&#24378;&#21270;&#23398;&#20064;&#20915;&#31574;&#22534;&#21472;
&lt;/p&gt;
&lt;p&gt;
Decision Stacks: Flexible Reinforcement Learning via Modular Generative Models. (arXiv:2306.06253v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06253
&lt;/p&gt;
&lt;p&gt;
&#20915;&#31574;&#22534;&#21472;&#26159;&#19968;&#31181;&#28789;&#27963;&#30340;&#29983;&#25104;&#26694;&#26550;&#65292;&#23558;&#30446;&#26631;&#26465;&#20214;&#21270;&#31574;&#30053;&#20195;&#29702;&#20998;&#35299;&#20026;3&#20010;&#29983;&#25104;&#27169;&#22359;&#65292;&#24182;&#36890;&#36807;&#25945;&#24072;&#24378;&#21046;&#24182;&#34892;&#23398;&#20064;&#65292;&#21487;&#20197;&#29992;&#20110;&#35774;&#35745;&#21333;&#20010;&#27169;&#22359;&#20197;&#32771;&#34385;&#26550;&#26500;&#20559;&#24046;&#12289;&#20248;&#21270;&#30446;&#26631;&#21644;&#21160;&#24577;&#12289;&#36328;&#39046;&#22495;&#30340;&#21487;&#36716;&#31227;&#24615;&#21644;&#25512;&#29702;&#36895;&#24230;&#12290;&#22312;&#36830;&#32493;&#25511;&#21046;&#22522;&#20934;&#27979;&#35797;&#20013;&#20855;&#26377;&#28789;&#27963;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#26159;&#19968;&#31181;&#26377;&#21560;&#24341;&#21147;&#30340;&#27169;&#22411;&#65292;&#21487;&#20197;&#25512;&#29702;&#39034;&#24207;&#20915;&#31574;&#21046;&#23450;&#30340;&#20960;&#20010;&#19981;&#21516;&#26041;&#38754;&#65292;&#22914;&#25351;&#23450;&#22797;&#26434;&#30446;&#26631;&#12289;&#35268;&#21010;&#26410;&#26469;&#35266;&#23519;&#21644;&#34892;&#21160;&#65292;&#20197;&#21450;&#25209;&#35780;&#20854;&#23454;&#29992;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#33021;&#21147;&#30340;&#32508;&#21512;&#38598;&#25104;&#22312;&#20445;&#25345;&#26368;&#22823;&#34920;&#36798;&#33021;&#21147;&#30340;&#21516;&#26102;&#20801;&#35768;&#36827;&#34892;&#27169;&#22411;&#36873;&#25321;&#20197;&#23454;&#29616;&#39640;&#25928;&#30340;&#23398;&#20064;&#21644;&#25512;&#29702;&#65292;&#36825;&#26500;&#25104;&#20102;&#31454;&#20105;&#24615;&#30340;&#31639;&#27861;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20915;&#31574;&#22534;&#21472;&#65288;Decision Stacks&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#29983;&#25104;&#26694;&#26550;&#65292;&#23558;&#30446;&#26631;&#26465;&#20214;&#21270;&#31574;&#30053;&#20195;&#29702;&#20998;&#35299;&#20026;3&#20010;&#29983;&#25104;&#27169;&#22359;&#12290;&#36825;&#20123;&#27169;&#22359;&#36890;&#36807;&#29420;&#31435;&#30340;&#29983;&#25104;&#27169;&#22411;&#27169;&#25311;&#20102;&#35266;&#27979;&#12289;&#22870;&#21169;&#21644;&#34892;&#21160;&#30340;&#26102;&#38388;&#28436;&#21464;&#65292;&#21487;&#20197;&#36890;&#36807;&#25945;&#24072;&#24378;&#21046;&#24182;&#34892;&#23398;&#20064;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#20445;&#35777;&#20102;&#34920;&#36798;&#33021;&#21147;&#21644;&#28789;&#27963;&#24615;&#65292;&#22312;&#35774;&#35745;&#21333;&#20010;&#27169;&#22359;&#20197;&#32771;&#34385;&#20851;&#38190;&#22240;&#32032;&#65288;&#22914;&#26550;&#26500;&#20559;&#24046;&#12289;&#20248;&#21270;&#30446;&#26631;&#21644;&#21160;&#24577;&#12289;&#36328;&#39046;&#22495;&#30340;&#21487;&#36716;&#31227;&#24615;&#21644;&#25512;&#29702;&#36895;&#24230;&#65289;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;&#25105;&#20204;&#23545;&#19968;&#31995;&#21015;&#36830;&#32493;&#25511;&#21046;&#22522;&#20934;&#36827;&#34892;&#30340;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#20915;&#31574;&#22534;&#21472;&#25552;&#20379;&#20102;&#19968;&#31181;&#28789;&#27963;&#19988;&#21487;&#25193;&#23637;&#30340;&#26367;&#20195;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#27169;&#22411;&#21644;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning presents an attractive paradigm to reason about several distinct aspects of sequential decision making, such as specifying complex goals, planning future observations and actions, and critiquing their utilities. However, the combined integration of these capabilities poses competing algorithmic challenges in retaining maximal expressivity while allowing for flexibility in modeling choices for efficient learning and inference. We present Decision Stacks, a generative framework that decomposes goal-conditioned policy agents into 3 generative modules. These modules simulate the temporal evolution of observations, rewards, and actions via independent generative models that can be learned in parallel via teacher forcing. Our framework guarantees both expressivity and flexibility in designing individual modules to account for key factors such as architectural bias, optimization objective and dynamics, transferrability across domains, and inference speed. Our empirical 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#21487;&#25345;&#32493;&#21644;&#21487;&#25193;&#23637;&#30340;AI&#38598;&#25104;&#36890;&#20449;&#31995;&#32479;&#30340;&#35774;&#35745;&#21407;&#21017;&#65292;&#20391;&#37325;&#20110;&#21019;&#24314;&#20855;&#22791;&#36890;&#29992;&#24615;&#30340;AI&#31639;&#27861;&#65292;&#36890;&#36807;&#23569;&#37327;&#30340;AI&#39537;&#21160;&#30340;RAN&#20989;&#25968;&#26469;&#35299;&#20915;&#26356;&#22823;&#30340;&#38382;&#39064;&#65292;&#25552;&#39640;&#31995;&#32479;&#24615;&#33021;&#65292;&#24182;&#31616;&#21270;&#29983;&#21629;&#21608;&#26399;&#31649;&#29702;&#12290;</title><link>http://arxiv.org/abs/2306.06251</link><description>&lt;p&gt;
&#36890;&#20449;&#31995;&#32479;&#20013;AI&#36890;&#29992;&#24615;&#19982;&#21487;&#25193;&#23637;&#24615;&#30340;&#35774;&#35745;&#21407;&#21017;
&lt;/p&gt;
&lt;p&gt;
Design Principles for Generalization and Scalability of AI in Communication Systems. (arXiv:2306.06251v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06251
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#21487;&#25345;&#32493;&#21644;&#21487;&#25193;&#23637;&#30340;AI&#38598;&#25104;&#36890;&#20449;&#31995;&#32479;&#30340;&#35774;&#35745;&#21407;&#21017;&#65292;&#20391;&#37325;&#20110;&#21019;&#24314;&#20855;&#22791;&#36890;&#29992;&#24615;&#30340;AI&#31639;&#27861;&#65292;&#36890;&#36807;&#23569;&#37327;&#30340;AI&#39537;&#21160;&#30340;RAN&#20989;&#25968;&#26469;&#35299;&#20915;&#26356;&#22823;&#30340;&#38382;&#39064;&#65292;&#25552;&#39640;&#31995;&#32479;&#24615;&#33021;&#65292;&#24182;&#31616;&#21270;&#29983;&#21629;&#21608;&#26399;&#31649;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#24050;&#32463;&#25104;&#20026;&#36890;&#20449;&#31995;&#32479;&#20013;&#35299;&#20915;&#22797;&#26434;&#21644;&#21160;&#24577;&#20219;&#21153;&#30340;&#24378;&#22823;&#24037;&#20855;&#65292;&#20256;&#32479;&#30340;&#22522;&#20110;&#35268;&#21017;&#30340;&#31639;&#27861;&#24448;&#24448;&#26080;&#27861;&#32988;&#20219;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#32593;&#32476;&#20219;&#21153;&#30340;AI&#24212;&#29992;&#37117;&#26159;&#38024;&#23545;&#29305;&#23450;&#21644;&#26377;&#38480;&#30340;&#26465;&#20214;&#35774;&#35745;&#21644;&#35757;&#32451;&#30340;&#65292;&#20351;&#24471;&#31639;&#27861;&#26080;&#27861;&#36866;&#24212;&#20110;&#24120;&#35265;&#30340;&#32593;&#32476;&#29615;&#22659;&#12289;&#20219;&#21153;&#21644;&#25511;&#21046;&#20219;&#21153;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#21487;&#25345;&#32493;&#21644;&#21487;&#25193;&#23637;&#30340;AI&#38598;&#25104;&#36890;&#20449;&#31995;&#32479;&#30340;&#35774;&#35745;&#21407;&#21017;&#65292;&#20391;&#37325;&#20110;&#21019;&#24314;&#21487;&#20197;&#22312;&#32593;&#32476;&#29615;&#22659;&#12289;&#24847;&#22270;&#21644;&#25511;&#21046;&#20219;&#21153;&#19978;&#20855;&#22791;&#36890;&#29992;&#24615;&#30340;AI&#31639;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#20351;&#23569;&#37327;&#30340;AI&#39537;&#21160;&#30340;RAN&#20989;&#25968;&#26469;&#35299;&#20915;&#26356;&#22823;&#30340;&#38382;&#39064;&#65292;&#25552;&#39640;&#31995;&#32479;&#24615;&#33021;&#65292;&#24182;&#31616;&#21270;&#29983;&#21629;&#21608;&#26399;&#31649;&#29702;&#12290;&#20026;&#20102;&#23454;&#29616;&#21487;&#25345;&#32493;&#24615;&#21644;&#33258;&#21160;&#21270;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#23398;&#20064;&#26550;&#26500;&#65292;&#35813;&#26550;&#26500;&#25903;&#25345;&#31995;&#32479;&#20013;&#37096;&#32626;&#30340;&#25152;&#26377;AI&#24212;&#29992;&#31243;&#24207;&#12290;&#35813;&#26550;&#26500;&#23558;&#20013;&#22830;&#21270;&#23398;&#20064;&#21151;&#33021;&#19982;&#20998;&#24067;&#24335;&#23398;&#20064;&#21644;&#25968;&#25454;&#37319;&#38598;&#21151;&#33021;&#20998;&#31163;&#65292;&#30830;&#20445;&#20102;&#31995;&#32479;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial intelligence (AI) has emerged as a powerful tool for addressing complex and dynamic tasks in communication systems, where traditional rule-based algorithms often struggle. However, most AI applications to networking tasks are designed and trained for specific, limited conditions, hindering the algorithms from learning and adapting to generic situations, such as those met across radio access networks (RAN). This paper proposes design principles for sustainable and scalable AI integration in communication systems, focusing on creating AI algorithms that can generalize across network environments, intents, and control tasks. This approach enables a limited number of AI-driven RAN functions to tackle larger problems, improve system performance, and simplify lifecycle management. To achieve sustainability and automation, we introduce a scalable learning architecture that supports all deployed AI applications in the system. This architecture separates centralized learning function
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#31070;&#32463;&#32593;&#32476;&#21387;&#32553;&#20013;&#22914;&#20309;&#20445;&#25345;&#19982;&#21407;&#22987;&#32593;&#32476;&#30340;&#35821;&#20041;&#30456;&#21516;&#65292;&#22312;&#38271;&#23614;&#29616;&#35937;&#20013;&#25506;&#35752;&#20102;&#21387;&#32553;&#25552;&#39640;&#25512;&#24191;&#24615;&#33021;&#30340;&#35760;&#24518;&#35201;&#32032;&#12290;</title><link>http://arxiv.org/abs/2306.06238</link><description>&lt;p&gt;
&#29702;&#35299;&#38271;&#23614;&#25928;&#24212;&#23545;&#31070;&#32463;&#32593;&#32476;&#21387;&#32553;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Understanding the Effect of the Long Tail on Neural Network Compression. (arXiv:2306.06238v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06238
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#31070;&#32463;&#32593;&#32476;&#21387;&#32553;&#20013;&#22914;&#20309;&#20445;&#25345;&#19982;&#21407;&#22987;&#32593;&#32476;&#30340;&#35821;&#20041;&#30456;&#21516;&#65292;&#22312;&#38271;&#23614;&#29616;&#35937;&#20013;&#25506;&#35752;&#20102;&#21387;&#32553;&#25552;&#39640;&#25512;&#24191;&#24615;&#33021;&#30340;&#35760;&#24518;&#35201;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32593;&#32476;&#21387;&#32553;&#29616;&#22312;&#26159;&#31070;&#32463;&#32593;&#32476;&#30740;&#31350;&#30340;&#19968;&#20010;&#25104;&#29087;&#30340;&#23376;&#39046;&#22495;&#65292;&#36807;&#21435;&#30340;&#21313;&#24180;&#20013;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#20197;&#20943;&#23567;&#27169;&#22411;&#23610;&#23544;&#21644;&#21152;&#36895;&#25512;&#26029;&#20026;&#30446;&#26631;&#65292;&#21516;&#26102;&#20445;&#25345;&#20998;&#31867;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#30740;&#31350;&#35266;&#23519;&#21040;&#65292;&#20165;&#20851;&#27880;&#24635;&#20307;&#20934;&#30830;&#24615;&#21487;&#33021;&#26159;&#35823;&#23548;&#30340;&#12290;&#20363;&#22914;&#65292;&#24050;&#32463;&#35777;&#26126;&#20840;&#27169;&#22411;&#21644;&#21387;&#32553;&#27169;&#22411;&#20043;&#38388;&#30340;&#24046;&#24322;&#21487;&#33021;&#20250;&#20559;&#21521;&#20110;&#22312;&#25968;&#25454;&#38598;&#20013;&#20302;&#39057;&#30340;&#31867;&#12290;&#36825;&#24341;&#20986;&#20102;&#19968;&#20010;&#37325;&#35201;&#30340;&#30740;&#31350;&#38382;&#39064;&#65292;&#21363;&#8220;&#25105;&#20204;&#33021;&#21542;&#22312;&#20445;&#25345;&#19982;&#21407;&#22987;&#32593;&#32476;&#35821;&#20041;&#31561;&#21516;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#32593;&#32476;&#21387;&#32553;&#65311;&#8221;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#37325;&#28857;&#20851;&#27880;&#35745;&#31639;&#26426;&#35270;&#35273;&#25968;&#25454;&#38598;&#20013;Feldman&#31561;&#20154;&#35266;&#23519;&#21040;&#30340;&#8220;&#38271;&#23614;&#8221;&#29616;&#35937;&#12290;&#20182;&#20204;&#35748;&#20026;&#65292;&#26576;&#20123;&#36755;&#20837;&#65288;&#36866;&#24403;&#23450;&#20041;&#65289;&#30340;&#35760;&#24518;&#23545;&#20110;&#23454;&#29616;&#33391;&#22909;&#30340;&#27867;&#21270;&#26159;&#24517;&#35201;&#30340;&#12290;&#30001;&#20110;&#21387;&#32553;&#38480;&#21046;&#20102;&#32593;&#32476;&#30340;&#23481;&#37327;&#65288;&#22240;&#27492;&#20063;&#38480;&#21046;&#20102;&#20854;&#35760;&#24518;&#33021;&#21147;&#65289;&#65292;&#25152;&#20197;&#25105;&#20204;&#30740;&#31350;&#20102;&#36825;&#20010;&#38382;&#39064;&#65306;
&lt;/p&gt;
&lt;p&gt;
Network compression is now a mature sub-field of neural network research: over the last decade, significant progress has been made towards reducing the size of models and speeding up inference, while maintaining the classification accuracy. However, many works have observed that focusing on just the overall accuracy can be misguided. E.g., it has been shown that mismatches between the full and compressed models can be biased towards under-represented classes. This raises the important research question, \emph{can we achieve network compression while maintaining ``semantic equivalence'' with the original network?} In this work, we study this question in the context of the ``long tail'' phenomenon in computer vision datasets observed by Feldman, et al. They argue that \emph{memorization} of certain inputs (appropriately defined) is essential to achieving good generalization. As compression limits the capacity of a network (and hence also its ability to memorize), we study the question: a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#22522;&#30784;&#27169;&#22411;&#22312;&#26497;&#23569;&#30417;&#30563;&#19979;&#26816;&#27979;&#25919;&#31574;&#36829;&#35268;&#65292;&#21019;&#26032;&#24615;&#22320;&#23558;&#24605;&#32500;&#38142;&#25552;&#31034;&#24341;&#20837;&#25919;&#31574;&#36829;&#35268;&#20219;&#21153;&#65292;&#21516;&#26102;&#23558;&#30828;&#25552;&#31034;&#19982;&#36719;&#25552;&#31034;&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#39640;&#20934;&#30830;&#24230;&#22320;&#29983;&#25104;&#21512;&#29702;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2306.06234</link><description>&lt;p&gt;
&#20351;&#29992;&#22522;&#30784;&#27169;&#22411;&#22312;&#26497;&#23569;&#30417;&#30563;&#19979;&#26816;&#27979;&#25919;&#31574;&#36829;&#35268;
&lt;/p&gt;
&lt;p&gt;
Using Foundation Models to Detect Policy Violations with Minimal Supervision. (arXiv:2306.06234v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06234
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#22522;&#30784;&#27169;&#22411;&#22312;&#26497;&#23569;&#30417;&#30563;&#19979;&#26816;&#27979;&#25919;&#31574;&#36829;&#35268;&#65292;&#21019;&#26032;&#24615;&#22320;&#23558;&#24605;&#32500;&#38142;&#25552;&#31034;&#24341;&#20837;&#25919;&#31574;&#36829;&#35268;&#20219;&#21153;&#65292;&#21516;&#26102;&#23558;&#30828;&#25552;&#31034;&#19982;&#36719;&#25552;&#31034;&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#39640;&#20934;&#30830;&#24230;&#22320;&#29983;&#25104;&#21512;&#29702;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#30784;&#27169;&#22411;&#65292;&#21363;&#39044;&#35757;&#32451;&#20110;&#22823;&#22411;&#25991;&#26412;&#35821;&#26009;&#24211;&#30340;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#65292;&#24050;&#32463;&#24443;&#24213;&#25913;&#21464;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12290;&#23427;&#20204;&#21487;&#20197;&#30452;&#25509;&#25351;&#23548;&#65292;&#20363;&#22914;&#30828;&#25552;&#31034;(&#20363;&#22914;(arXiv:2005.14165))&#65292;&#20063;&#21487;&#20197;&#20351;&#29992;&#26497;&#23569;&#30340;&#25968;&#25454;&#36827;&#34892;&#35843;&#25972;&#65292;&#36825;&#31181;&#25216;&#26415;&#34987;&#31216;&#20026;&#36719;&#25552;&#31034;&#65292;&#25105;&#20204;&#35797;&#22270;&#21033;&#29992;&#23427;&#20204;&#30340;&#33021;&#21147;&#26469;&#26816;&#27979;&#25919;&#31574;&#36829;&#35268;&#12290;&#25105;&#20204;&#30340;&#21019;&#26032;&#28857;&#26159;&#65306;&#25105;&#20204;&#30830;&#23450;&#20102;&#19968;&#31181;&#30828;&#25552;&#31034;&#65292;&#23558;&#24605;&#32500;&#38142;&#25552;&#31034;&#36866;&#24212;&#20110;&#25919;&#31574;&#36829;&#35268;&#20219;&#21153;&#12290;&#36825;&#20010;&#25552;&#31034;&#29983;&#25104;&#25919;&#31574;&#36829;&#35268;&#20998;&#31867;&#20197;&#21450;&#25552;&#21462;&#24335;&#35299;&#37322;&#26469;&#35777;&#26126;&#20998;&#31867;&#30340;&#21512;&#29702;&#24615;&#12290;&#25105;&#20204;&#23558;&#30828;&#25552;&#31034;&#19982;&#36719;&#25552;&#31034;&#35843;&#25972;&#30456;&#32467;&#21512;&#65292;&#20351;&#29992;&#38750;&#24120;&#23569;&#30340;&#30417;&#30563;&#26469;&#29983;&#25104;&#19968;&#20010;&#20998;&#31867;&#22120;&#65292;&#21516;&#26102;&#35813;&#20998;&#31867;&#22120;&#36824;&#21487;&#20197;&#20135;&#29983;&#35299;&#37322;&#12290;&#34429;&#28982;&#30417;&#30563;&#21482;&#20316;&#29992;&#20110;&#20998;&#31867;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#20462;&#25913;&#21518;&#30340;&#35299;&#37322;&#19982;(&#35843;&#25972;&#21518;&#30340;)&#27169;&#22411;&#21709;&#24212;&#20445;&#25345;&#19968;&#33268;&#12290;&#22312;&#27492;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#19968;&#20123;&#20196;&#20154;&#36153;&#35299;&#30340;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
Foundation models, i.e. large neural networks pre-trained on large text corpora, have revolutionized NLP. They can be instructed directly (e.g. (arXiv:2005.14165)) - this is called hard prompting - and they can be tuned using very little data (e.g. (arXiv:2104.08691)) - this technique is called soft prompting. We seek to leverage their capabilities to detect policy violations. Our contributions are: We identify a hard prompt that adapts chain-of-thought prompting to policy violation tasks. This prompt produces policy violation classifications, along with extractive explanations that justify the classification. We compose the hard-prompts with soft prompt tuning to produce a classifier that attains high accuracy with very little supervision; the same classifier also produces explanations. Though the supervision only acts on the classifications, we find that the modified explanations remain consistent with the (tuned) model's response. Along the way, we identify several unintuitive aspec
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;UI-Diffuser&#30340;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#29983;&#25104;&#31227;&#21160;UI&#65292;&#36890;&#36807;&#31616;&#21333;&#30340;&#25991;&#26412;&#25551;&#36848;&#21644;UI&#32452;&#20214;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;GUI&#21407;&#22411;&#35774;&#35745;&#30340;&#36895;&#24230;&#21644;&#25928;&#29575;&#65292;&#20943;&#23569;&#21407;&#22411;&#35774;&#35745;&#30340;&#24037;&#20316;&#37327;&#12290;</title><link>http://arxiv.org/abs/2306.06233</link><description>&lt;p&gt;
&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;GUI&#21407;&#22411;&#35774;&#35745;&#21152;&#36895;
&lt;/p&gt;
&lt;p&gt;
Boosting GUI Prototyping with Diffusion Models. (arXiv:2306.06233v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06233
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;UI-Diffuser&#30340;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#29983;&#25104;&#31227;&#21160;UI&#65292;&#36890;&#36807;&#31616;&#21333;&#30340;&#25991;&#26412;&#25551;&#36848;&#21644;UI&#32452;&#20214;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;GUI&#21407;&#22411;&#35774;&#35745;&#30340;&#36895;&#24230;&#21644;&#25928;&#29575;&#65292;&#20943;&#23569;&#21407;&#22411;&#35774;&#35745;&#30340;&#24037;&#20316;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
GUI&#65288;&#22270;&#24418;&#29992;&#25143;&#30028;&#38754;&#65289;&#21407;&#22411;&#35774;&#35745;&#26159;&#38656;&#27714;&#24037;&#31243;&#20013;&#24191;&#27867;&#20351;&#29992;&#30340;&#25216;&#26415;&#65292;&#29992;&#20110;&#25910;&#38598;&#21644;&#23436;&#21892;&#38656;&#27714;&#65292;&#38477;&#20302;&#24320;&#21457;&#39118;&#38505;&#24182;&#22686;&#21152;&#24178;&#31995;&#20154;&#30340;&#21442;&#19982;&#24230;&#12290;&#28982;&#32780;&#65292;GUI&#21407;&#22411;&#35774;&#35745;&#21487;&#33021;&#26159;&#19968;&#39033;&#32791;&#26102;&#19988;&#26114;&#36149;&#30340;&#36807;&#31243;&#12290;&#36817;&#24180;&#26469;&#65292;&#35832;&#22914;Stable Diffusion&#31561;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#24050;&#25104;&#20026;&#19968;&#31181;&#24378;&#22823;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#24037;&#20855;&#65292;&#24182;&#33021;&#22815;&#26681;&#25454;&#25991;&#26412;&#25552;&#31034;&#29983;&#25104;&#35814;&#32454;&#30340;&#22270;&#20687;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;UI-Diffuser&#65292;&#19968;&#31181;&#21033;&#29992;Stable Diffusion&#29983;&#25104;&#31227;&#21160;UI&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#31616;&#21333;&#30340;&#25991;&#26412;&#25551;&#36848;&#21644;UI&#32452;&#20214;&#12290;&#21021;&#27493;&#32467;&#26524;&#34920;&#26126;&#65292;UI-Diffuser&#25552;&#20379;&#20102;&#19968;&#31181;&#39640;&#25928;&#19988;&#20855;&#26377;&#25104;&#26412;&#25928;&#30410;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20943;&#23569;&#23545;&#24191;&#27867;&#21407;&#22411;&#35774;&#35745;&#24037;&#20316;&#30340;&#38656;&#27714;&#65292;&#20174;&#32780;&#29983;&#25104;&#31227;&#21160;GUI&#35774;&#35745;&#12290;&#36825;&#31181;&#26041;&#27861;&#26377;&#26395;&#26174;&#33879;&#25552;&#39640;GUI&#21407;&#22411;&#35774;&#35745;&#22312;&#38656;&#27714;&#24037;&#31243;&#20013;&#30340;&#36895;&#24230;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
GUI (graphical user interface) prototyping is a widely-used technique in requirements engineering for gathering and refining requirements, reducing development risks and increasing stakeholder engagement. However, GUI prototyping can be a time-consuming and costly process. In recent years, deep learning models such as Stable Diffusion have emerged as a powerful text-to-image tool capable of generating detailed images based on text prompts. In this paper, we propose UI-Diffuser, an approach that leverages Stable Diffusion to generate mobile UIs through simple textual descriptions and UI components. Preliminary results show that UI-Diffuser provides an efficient and cost-effective way to generate mobile GUI designs while reducing the need for extensive prototyping efforts. This approach has the potential to significantly improve the speed and efficiency of GUI prototyping in requirements engineering.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#31070;&#32463;&#25104;&#20687;&#39046;&#22495;&#30340;&#22270;&#26426;&#22120;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;NeuroGraph&#65292;&#24182;&#25506;&#35752;&#20102;&#25968;&#25454;&#38598;&#29983;&#25104;&#30340;&#25628;&#32034;&#31354;&#38388;&#12290;</title><link>http://arxiv.org/abs/2306.06202</link><description>&lt;p&gt;
NeuroGraph:&#38754;&#21521;&#33041;&#36830;&#25509;&#32452;&#23398;&#30340;&#22270;&#26426;&#22120;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
NeuroGraph: Benchmarks for Graph Machine Learning in Brain Connectomics. (arXiv:2306.06202v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06202
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#31070;&#32463;&#25104;&#20687;&#39046;&#22495;&#30340;&#22270;&#26426;&#22120;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;NeuroGraph&#65292;&#24182;&#25506;&#35752;&#20102;&#25968;&#25454;&#38598;&#29983;&#25104;&#30340;&#25628;&#32034;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#20026;&#20998;&#26512;&#39640;&#32500;&#21151;&#33021;&#24615;&#31070;&#32463;&#25104;&#20687;&#25968;&#25454;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#24037;&#20855;&#65292;&#24050;&#34987;&#35777;&#26126;&#23545;&#39044;&#27979;&#21508;&#31181;&#31070;&#32463;&#30142;&#30149;&#12289;&#31934;&#31070;&#38556;&#30861;&#21644;&#35748;&#30693;&#27169;&#24335;&#26377;&#25928;&#12290;&#22312;&#21151;&#33021;&#30913;&#20849;&#25391;&#25104;&#20687;&#30740;&#31350;&#20013;&#65292;&#22823;&#33041;&#21306;&#22495;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#36890;&#24120;&#20351;&#29992;&#22522;&#20110;&#22270;&#30340;&#34920;&#31034;&#36827;&#34892;&#24314;&#27169;&#12290;&#22270;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#24050;&#22312;&#22810;&#20010;&#39046;&#22495;&#24471;&#21040;&#35777;&#23454;&#65292;&#26631;&#24535;&#30528;&#25968;&#25454;&#35299;&#37322;&#21644;&#39044;&#27979;&#24314;&#27169;&#20013;&#30340;&#19968;&#20010;&#36716;&#21464;&#27493;&#39588;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#26377;&#21069;&#26223;&#65292;&#20294;&#30001;&#20110;&#22270;&#24418;&#25968;&#25454;&#38598;&#26500;&#24314;&#30340;&#24191;&#27867;&#39044;&#22788;&#29702;&#27969;&#27700;&#32447;&#21644;&#22823;&#21442;&#25968;&#25628;&#32034;&#31354;&#38388;&#65292;&#22312;&#31070;&#32463;&#25104;&#20687;&#39046;&#22495;&#20013;&#24212;&#29992;&#36825;&#20123;&#25216;&#26415;&#30340;&#36716;&#25442;&#20173;&#28982;&#21463;&#21040;&#24847;&#22806;&#30340;&#38480;&#21046;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;NeuroGraph(&#19968;&#20010;&#22522;&#20110;&#22270;&#30340;&#31070;&#32463;&#25104;&#20687;&#25968;&#25454;&#38598;)&#65292;&#23427;&#28085;&#30422;&#20102;&#22810;&#20010;&#34892;&#20026;&#21644;&#35748;&#30693;&#29305;&#24449;&#31867;&#21035;&#12290;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;&#25968;&#25454;&#38598;&#29983;&#25104;&#25628;&#32034;&#31354;&#38388;
&lt;/p&gt;
&lt;p&gt;
Machine learning provides a valuable tool for analyzing high-dimensional functional neuroimaging data, and is proving effective in predicting various neurological conditions, psychiatric disorders, and cognitive patterns. In functional Magnetic Resonance Imaging (MRI) research, interactions between brain regions are commonly modeled using graph-based representations. The potency of graph machine learning methods has been established across myriad domains, marking a transformative step in data interpretation and predictive modeling. Yet, despite their promise, the transposition of these techniques to the neuroimaging domain remains surprisingly under-explored due to the expansive preprocessing pipeline and large parameter search space for graph-based datasets construction. In this paper, we introduce NeuroGraph, a collection of graph-based neuroimaging datasets that span multiple categories of behavioral and cognitive traits. We delve deeply into the dataset generation search space by c
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#23567;&#32780;&#39640;&#25928;&#27169;&#22411;&#65292;&#29992;&#20110;&#30830;&#23450;&#20004;&#20010;&#24515;&#30005;&#22270;(ECG)&#26159;&#21542;&#26469;&#33258;&#21516;&#19968;&#24739;&#32773;&#12290;&#21516;&#26102;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21033;&#29992;&#35813;&#27169;&#22411;&#26816;&#27979;&#35760;&#24405;-&#20998;&#37197;&#38169;&#35823;&#65292;&#23454;&#29616;&#20102;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#22312;&#26032;&#30340;ECG&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2306.06196</link><description>&lt;p&gt;
ElectroCardioGuard&#65306;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#38450;&#27490;&#24515;&#30005;&#22270;&#25968;&#25454;&#24211;&#20013;&#24739;&#32773;&#35823;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
ElectroCardioGuard: Preventing Patient Misidentification in Electrocardiogram Databases through Neural Networks. (arXiv:2306.06196v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06196
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#23567;&#32780;&#39640;&#25928;&#27169;&#22411;&#65292;&#29992;&#20110;&#30830;&#23450;&#20004;&#20010;&#24515;&#30005;&#22270;(ECG)&#26159;&#21542;&#26469;&#33258;&#21516;&#19968;&#24739;&#32773;&#12290;&#21516;&#26102;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21033;&#29992;&#35813;&#27169;&#22411;&#26816;&#27979;&#35760;&#24405;-&#20998;&#37197;&#38169;&#35823;&#65292;&#23454;&#29616;&#20102;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#22312;&#26032;&#30340;ECG&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24515;&#30005;&#22270;(ECG)&#36890;&#24120;&#34987;&#24515;&#33039;&#30149;&#19987;&#23478;&#29992;&#20110;&#26816;&#27979;&#19982;&#24515;&#33039;&#30456;&#20851;&#30340;&#30149;&#29702;&#24773;&#20917;&#65292;&#32780;&#21487;&#38752;&#30340;ECG&#38598;&#21512;&#23545;&#20110;&#30830;&#35786;&#38750;&#24120;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#22312;&#20020;&#24202;&#23454;&#36341;&#20013;&#65292;&#23558;&#35760;&#24405;&#30340;ECG&#20998;&#37197;&#32473;&#38169;&#35823;&#30340;&#24739;&#32773;&#21487;&#33021;&#20250;&#19981;&#32463;&#24847;&#22320;&#21457;&#29983;&#12290;&#26412;&#25991;&#19982;&#19968;&#23478;&#20020;&#24202;&#21644;&#30740;&#31350;&#26426;&#26500;&#21512;&#20316;&#65292;&#35813;&#26426;&#26500;&#35748;&#35782;&#21040;&#36825;&#19968;&#25361;&#25112;&#24182;&#32852;&#31995;&#25105;&#20204;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#39033;&#30740;&#31350;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23567;&#24039;&#39640;&#25928;&#30340;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#30830;&#23450;&#20004;&#20010;ECG&#26159;&#21542;&#26469;&#33258;&#21516;&#19968;&#24739;&#32773;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#23637;&#29616;&#20102;&#24456;&#24378;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#22312;&#21033;&#29992;760&#20493;&#26356;&#23569;&#30340;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#65292;&#22312;PTB-XL&#19978;&#23454;&#29616;&#20102;&#26368;&#26032;&#30340;&#30011;&#24266;&#25506;&#38024;&#24739;&#32773;&#35782;&#21035;&#34920;&#29616;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25216;&#26415;&#65292;&#21033;&#29992;&#25105;&#20204;&#30340;&#27169;&#22411;&#26469;&#26816;&#27979;&#35760;&#24405;-&#20998;&#37197;&#38169;&#35823;&#65292;&#23637;&#31034;&#20102;&#23427;&#22312;&#19968;&#20010;&#29616;&#23454;&#22330;&#26223;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23545;&#26032;&#25910;&#38598;&#30340;ECG&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Electrocardiograms (ECGs) are commonly used by cardiologists to detect heart-related pathological conditions. Reliable collections of ECGs are crucial for precise diagnosis. However, in clinical practice, the assignment of captured ECG recordings to incorrect patients can occur inadvertently. In collaboration with a clinical and research facility which recognized this challenge and reached out to us, we present a study that addresses this issue. In this work, we propose a small and efficient neural-network based model for determining whether two ECGs originate from the same patient. Our model demonstrates great generalization capabilities and achieves state-of-the-art performance in gallery-probe patient identification on PTB-XL while utilizing 760x fewer parameters. Furthermore, we present a technique leveraging our model for detection of recording-assignment mistakes, showcasing its applicability in a realistic scenario. Finally, we evaluate our model on a newly collected ECG dataset
&lt;/p&gt;</description></item><item><title>Ada-NAV&#26159;&#19968;&#31181;&#33258;&#36866;&#24212;&#36712;&#36857;&#20248;&#21270;&#31574;&#30053;&#23398;&#20064;&#26041;&#27861;&#65292;&#37319;&#29992;&#38477;&#20302;&#31574;&#30053;&#38543;&#26426;&#24615;&#30340;&#26041;&#27861;&#24179;&#34913;&#25506;&#32034;&#19982;&#21033;&#29992;&#65292;&#25552;&#39640;&#26426;&#22120;&#20154;&#23548;&#33322;&#20219;&#21153;&#30340;&#37319;&#26679;&#25928;&#29575;&#12290;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#27979;&#35797;&#20013;&#34920;&#29616;&#20248;&#24322;&#65292;&#21487;&#20197;&#22312;&#26356;&#30701;&#30340;&#37319;&#26679;&#26102;&#38388;&#20869;&#21462;&#24471;&#26356;&#39640;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.06192</link><description>&lt;p&gt;
Ada-NAV&#65306;&#29992;&#20110;&#26426;&#22120;&#20154;&#23548;&#33322;&#30340;&#33258;&#36866;&#24212;&#36712;&#36857;&#20248;&#21270;&#31574;&#30053;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Ada-NAV: Adaptive Trajectory-Based Sample Efficient Policy Learning for Robotic Navigation. (arXiv:2306.06192v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06192
&lt;/p&gt;
&lt;p&gt;
Ada-NAV&#26159;&#19968;&#31181;&#33258;&#36866;&#24212;&#36712;&#36857;&#20248;&#21270;&#31574;&#30053;&#23398;&#20064;&#26041;&#27861;&#65292;&#37319;&#29992;&#38477;&#20302;&#31574;&#30053;&#38543;&#26426;&#24615;&#30340;&#26041;&#27861;&#24179;&#34913;&#25506;&#32034;&#19982;&#21033;&#29992;&#65292;&#25552;&#39640;&#26426;&#22120;&#20154;&#23548;&#33322;&#20219;&#21153;&#30340;&#37319;&#26679;&#25928;&#29575;&#12290;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#27979;&#35797;&#20013;&#34920;&#29616;&#20248;&#24322;&#65292;&#21487;&#20197;&#22312;&#26356;&#30701;&#30340;&#37319;&#26679;&#26102;&#38388;&#20869;&#21462;&#24471;&#26356;&#39640;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#22312;&#23398;&#20064;&#26426;&#22120;&#20154;&#23548;&#33322;&#31574;&#30053;&#26041;&#38754;&#21313;&#20998;&#26377;&#25928;&#65292;&#20294;&#20854;&#37319;&#26679;&#25928;&#29575;&#20302;&#30340;&#38382;&#39064;&#20063;&#21313;&#20998;&#26126;&#26174;&#12290;&#22312;&#31574;&#30053;&#20248;&#21270;&#20013;&#65292;&#36825;&#31181;&#25928;&#29575;&#20302;&#19979;&#37096;&#20998;&#26469;&#33258;&#20110;&#26410;&#33021;&#36866;&#24403;&#22320;&#24179;&#34913;&#25506;&#32034;&#19982;&#21033;&#29992;&#30340;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#22312;&#38754;&#23545;&#38750;&#38745;&#24577;&#26102;&#12290;&#20026;&#20102;&#21152;&#20837;&#25506;&#32034;&#19982;&#21033;&#29992;&#30340;&#24179;&#34913;&#20197;&#25552;&#39640;&#37319;&#26679;&#25928;&#29575;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Ada-NAV&#65292;&#19968;&#31181;&#33258;&#36866;&#24212;&#36712;&#36857;&#38271;&#24230;&#26041;&#26696;&#65292;&#20854;&#20013;&#38271;&#24230;&#38543;&#30528;&#31574;&#30053;&#30340;&#38543;&#26426;&#24615;&#65288;&#29992;&#20854;Shannon&#25110;&#24046;&#20998;&#29109;&#34920;&#31034;&#65289;&#30340;&#20943;&#23567;&#32780;&#22686;&#21152;&#12290;&#25105;&#20204;&#30340;&#33258;&#36866;&#24212;&#36712;&#36857;&#38271;&#24230;&#26041;&#26696;&#30001;&#20110;&#26356;&#39057;&#32321;&#30340;&#26799;&#24230;&#26356;&#26032;&#24378;&#35843;&#20102;&#35757;&#32451;&#24320;&#22987;&#26102;&#30340;&#25506;&#32034;&#65292;&#21518;&#26469;&#21017;&#26356;&#24378;&#35843;&#21033;&#29992;&#12290;&#22312;&#32593;&#26684;&#19990;&#30028;&#65292;&#20223;&#30495;&#26426;&#22120;&#20154;&#29615;&#22659;&#21644;&#30495;&#23454;&#19990;&#30028;&#26426;&#22120;&#20154;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#20248;&#28857;&#65292;&#34920;&#29616;&#22312;&#24615;&#33021;&#21644;&#37319;&#26679;&#25928;&#29575;&#19978;&#22343;&#20248;&#20110;&#24120;&#25968;&#21644;&#38543;&#26426;&#37319;&#26679;&#30340;&#36712;&#36857;&#38271;&#24230;&#12290;&#22312;&#22266;&#23450;&#30340;&#26679;&#26412;&#39044;&#31639;&#19979;&#65292;&#30456;&#23545;&#20110;&#29616;&#26377;&#30340;&#22522;&#20934;&#26041;&#27861;&#65292;Ada-NAV&#30340;&#24615;&#33021;&#25552;&#39640;&#20102;&#39640;&#36798;46&#65285;&#65292;&#37319;&#26679;&#25968;&#37327;&#20943;&#23569;&#20102;&#39640;&#36798;80&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning methods, while effective for learning robotic navigation strategies, are known to be highly sample inefficient. This sample inefficiency comes in part from not suitably balancing the explore-exploit dilemma, especially in the presence of non-stationarity, during policy optimization. To incorporate a balance of exploration-exploitation for sample efficiency, we propose Ada-NAV, an adaptive trajectory length scheme where the length grows as a policy's randomness, represented by its Shannon or differential entropy, decreases. Our adaptive trajectory length scheme emphasizes exploration at the beginning of training due to more frequent gradient updates and emphasizes exploitation later on with longer trajectories. In gridworld, simulated robotic environments, and real-world robotic experiments, we demonstrate the merits of the approach over constant and randomly sampled trajectory lengths in terms of performance and sample efficiency. For a fixed sample budget, Ada-N
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#22411;&#28151;&#21512;CNN-ViT&#31070;&#32463;&#32593;&#32476;FasterViT&#65292;&#24341;&#20837;&#20102;&#20855;&#26377;&#20998;&#23618;&#27880;&#24847;&#21147;&#30340;&#26041;&#27861;HAT&#65292;&#23558;&#20840;&#23616;&#33258;&#25105;&#27880;&#24847;&#21147;&#20998;&#35299;&#20026;&#22810;&#32423;&#27880;&#24847;&#21147;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#36328;&#31383;&#21475;&#36890;&#20449;&#12290; FasterViT&#22312;&#31934;&#24230;&#21644;&#22270;&#20687;&#21534;&#21520;&#37327;&#26041;&#38754;&#36798;&#21040;&#20102;SOTA&#21069;&#27839;&#27700;&#24179;&#65292;&#24182;&#24050;&#22312;&#20998;&#31867;&#65292;&#29289;&#20307;&#26816;&#27979;&#21644;&#20998;&#21106;&#31561;CV&#20219;&#21153;&#20013;&#24471;&#21040;&#24191;&#27867;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2306.06189</link><description>&lt;p&gt;
FasterViT&#65306;&#20855;&#26377;&#20998;&#23618;&#27880;&#24847;&#21147;&#30340;&#24555;&#36895;&#35270;&#35273;&#21464;&#25442;&#22120;
&lt;/p&gt;
&lt;p&gt;
FasterViT: Fast Vision Transformers with Hierarchical Attention. (arXiv:2306.06189v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06189
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#22411;&#28151;&#21512;CNN-ViT&#31070;&#32463;&#32593;&#32476;FasterViT&#65292;&#24341;&#20837;&#20102;&#20855;&#26377;&#20998;&#23618;&#27880;&#24847;&#21147;&#30340;&#26041;&#27861;HAT&#65292;&#23558;&#20840;&#23616;&#33258;&#25105;&#27880;&#24847;&#21147;&#20998;&#35299;&#20026;&#22810;&#32423;&#27880;&#24847;&#21147;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#36328;&#31383;&#21475;&#36890;&#20449;&#12290; FasterViT&#22312;&#31934;&#24230;&#21644;&#22270;&#20687;&#21534;&#21520;&#37327;&#26041;&#38754;&#36798;&#21040;&#20102;SOTA&#21069;&#27839;&#27700;&#24179;&#65292;&#24182;&#24050;&#22312;&#20998;&#31867;&#65292;&#29289;&#20307;&#26816;&#27979;&#21644;&#20998;&#21106;&#31561;CV&#20219;&#21153;&#20013;&#24471;&#21040;&#24191;&#27867;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#22411;&#28151;&#21512;CNN-ViT&#31070;&#32463;&#32593;&#32476;&#65292;&#21629;&#21517;&#20026;FasterViT&#65292;&#19987;&#27880;&#20110;&#35745;&#31639;&#26426;&#35270;&#35273;&#24212;&#29992;&#20013;&#30340;&#39640;&#22270;&#20687;&#21534;&#21520;&#37327;&#12290;FasterViT&#23558;CNN&#20013;&#24555;&#36895;&#26412;&#22320;&#34920;&#31034;&#23398;&#20064;&#30340;&#20248;&#28857;&#19982;ViT&#20013;&#30340;&#20840;&#23616;&#24314;&#27169;&#29305;&#24615;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#20998;&#23618;&#27880;&#24847;&#21147;&#65288;HAT&#65289;&#26041;&#27861;&#65292;&#23558;&#20855;&#26377;&#20108;&#27425;&#22797;&#26434;&#24230;&#30340;&#20840;&#23616;&#33258;&#25105;&#27880;&#24847;&#21147;&#20998;&#35299;&#20026;&#20855;&#26377;&#36739;&#23567;&#35745;&#31639;&#25104;&#26412;&#30340;&#22810;&#32423;&#27880;&#24847;&#21147;&#12290;&#25105;&#20204;&#21463;&#30410;&#20110;&#39640;&#25928;&#30340;&#22522;&#20110;&#31383;&#21475;&#30340;&#33258;&#25105;&#27880;&#24847;&#21147;&#65292;&#27599;&#20010;&#31383;&#21475;&#37117;&#21487;&#20197;&#35775;&#38382;&#19987;&#38376;&#29992;&#20110;&#26412;&#22320;&#21644;&#20840;&#23616;&#34920;&#31034;&#23398;&#20064;&#30340;&#19987;&#29992;&#36733;&#20307;&#20196;&#29260;&#12290;&#22312;&#39640;&#23618;&#27425;&#19978;&#65292;&#20840;&#23616;&#33258;&#25105;&#27880;&#24847;&#21147;&#23454;&#29616;&#20102;&#36739;&#20302;&#25104;&#26412;&#30340;&#36328;&#31383;&#21475;&#36890;&#20449;&#65292;FasterViT&#22312;&#31934;&#24230;&#19982;&#22270;&#20687;&#21534;&#21520;&#37327;&#26041;&#38754;&#23454;&#29616;&#20102;SOTA&#21069;&#27839;&#27700;&#24179;&#65292;&#24182;&#24050;&#22312;&#21508;&#31181;CV&#20219;&#21153;&#20013;&#36827;&#34892;&#20102;&#24191;&#27867;&#39564;&#35777;&#65292;&#21253;&#25324;&#20998;&#31867;&#65292;&#29289;&#20307;&#26816;&#27979;&#21644;&#20998;&#21106;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;HAT&#21487;&#20197;&#29992;&#20316;&#29616;&#26377;CNN&#26550;&#26500;&#30340;&#21363;&#25554;&#21363;&#29992;&#27169;&#22359;&#65292;&#20197;&#25913;&#21892;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We design a new family of hybrid CNN-ViT neural networks, named FasterViT, with a focus on high image throughput for computer vision (CV) applications. FasterViT combines the benefits of fast local representation learning in CNNs and global modeling properties in ViT. Our newly introduced Hierarchical Attention (HAT) approach decomposes global self-attention with quadratic complexity into a multi-level attention with reduced computational costs. We benefit from efficient window-based self-attention. Each window has access to dedicated carrier tokens that participate in local and global representation learning. At a high level, global self-attentions enable the efficient cross-window communication at lower costs. FasterViT achieves a SOTA Pareto-front in terms of accuracy \vs image throughput. We have extensively validated its effectiveness on various CV tasks including classification, object detection and segmentation. We also show that HAT can be used as a plug-and-play module for exi
&lt;/p&gt;</description></item><item><title>HypLL&#26159;&#19968;&#20010;&#20351;&#29992;&#24076;&#20122;&#31354;&#38388;&#30340;&#28145;&#24230;&#23398;&#20064;&#24211;&#65292;&#22522;&#20110;PyTorch&#65292;&#26088;&#22312;&#20351;&#20854;&#26131;&#20110;&#20351;&#29992;&#65292;&#25645;&#24314;&#24076;&#20122;&#32593;&#32476;&#27169;&#22359;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#22788;&#29702;&#23618;&#27425;&#21270;&#25968;&#25454;&#21644;&#20351;&#29992;&#23569;&#37327;&#23884;&#20837;&#32500;&#24230;&#65292;&#26159;&#19968;&#31181;&#26032;&#30340;&#12289;&#24320;&#25918;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2306.06154</link><description>&lt;p&gt;
HypLL: &#24076;&#20122;&#31354;&#38388;&#28145;&#24230;&#23398;&#20064;&#24211;
&lt;/p&gt;
&lt;p&gt;
HypLL: The Hyperbolic Learning Library. (arXiv:2306.06154v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06154
&lt;/p&gt;
&lt;p&gt;
HypLL&#26159;&#19968;&#20010;&#20351;&#29992;&#24076;&#20122;&#31354;&#38388;&#30340;&#28145;&#24230;&#23398;&#20064;&#24211;&#65292;&#22522;&#20110;PyTorch&#65292;&#26088;&#22312;&#20351;&#20854;&#26131;&#20110;&#20351;&#29992;&#65292;&#25645;&#24314;&#24076;&#20122;&#32593;&#32476;&#27169;&#22359;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#22788;&#29702;&#23618;&#27425;&#21270;&#25968;&#25454;&#21644;&#20351;&#29992;&#23569;&#37327;&#23884;&#20837;&#32500;&#24230;&#65292;&#26159;&#19968;&#31181;&#26032;&#30340;&#12289;&#24320;&#25918;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#23398;&#20064;&#12289;&#22810;&#23186;&#20307;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#31561;&#39046;&#22495;&#65292;&#24076;&#20122;&#31354;&#38388;&#28145;&#24230;&#23398;&#20064;&#27491;&#36805;&#36895;&#24341;&#36215;&#20851;&#27880;&#12290;&#28145;&#24230;&#32593;&#32476;&#36890;&#24120;&#22312;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#20013;&#36816;&#34892;&#65292;&#38544;&#21547;&#22320;&#20551;&#35774;&#25968;&#25454;&#22312;&#35268;&#21017;&#32593;&#26684;&#19978;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#24403;&#22788;&#29702;&#23618;&#27425;&#21270;&#25968;&#25454;&#21644;&#20351;&#29992;&#23569;&#37327;&#23884;&#20837;&#32500;&#24230;&#26102;&#65292;&#24076;&#20122;&#20960;&#20309;&#25552;&#20379;&#20102;&#19968;&#20010;&#21487;&#34892;&#30340;&#28145;&#24230;&#23398;&#20064;&#22522;&#30784;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#27809;&#26377;&#21487;&#35775;&#38382;&#30340;&#24320;&#28304;&#24211;&#29992;&#20110;&#26500;&#24314;&#31867;&#20284;&#20110;&#20247;&#25152;&#21608;&#30693;&#30340;&#28145;&#24230;&#23398;&#20064;&#24211;&#30340;&#24076;&#20122;&#32593;&#32476;&#27169;&#22359;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;HypLL, &#21363;&#24076;&#20122;&#31354;&#38388;&#28145;&#24230;&#23398;&#20064;&#24211;&#65292;&#20197;&#23558;&#24076;&#20122;&#28145;&#24230;&#23398;&#20064;&#30340;&#36827;&#23637;&#32858;&#38598;&#22312;&#19968;&#36215;&#12290;HypLL&#24314;&#31435;&#22312;PyTorch&#20043;&#19978;&#65292;&#29305;&#21035;&#24378;&#35843;&#20854;&#26131;&#29992;&#24615;&#35774;&#35745;&#65292;&#20197;&#21560;&#24341;&#24191;&#27867;&#30340;&#21463;&#20247;&#20851;&#27880;&#36825;&#20010;&#26032;&#30340;&#21644;&#24320;&#25918;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#20195;&#30721;&#21487;&#22312;&#20197;&#19979;&#32593;&#22336;&#25214;&#21040;&#65306;https://github.com/maxvanspengler/hyperbolic_learning_library&#12290;&#21387;&#32553;&#25991;&#20214;&#21487;&#22312;&#20197;&#19979;&#32593;&#22336;&#25214;&#21040;&#65306;https://d
&lt;/p&gt;
&lt;p&gt;
Deep learning in hyperbolic space is quickly gaining traction in the fields of machine learning, multimedia, and computer vision. Deep networks commonly operate in Euclidean space, implicitly assuming that data lies on regular grids. Recent advances have shown that hyperbolic geometry provides a viable alternative foundation for deep learning, especially when data is hierarchical in nature and when working with few embedding dimensions. Currently however, no accessible open-source library exists to build hyperbolic network modules akin to well-known deep learning libraries. We present HypLL, the Hyperbolic Learning Library to bring the progress on hyperbolic deep learning together. HypLL is built on top of PyTorch, with an emphasis in its design for easy-of-use, in order to attract a broad audience towards this new and open-ended research direction. The code is available at: https://github.com/maxvanspengler/hyperbolic_learning_library. The compressed archive is available at: https://d
&lt;/p&gt;</description></item><item><title>EfficientBioAI&#26159;&#19968;&#20010;&#21363;&#25554;&#21363;&#29992;&#30340;&#24037;&#20855;&#31665;&#65292;&#21487;&#20197;&#21387;&#32553;&#29983;&#29289;&#25104;&#20687;AI&#27169;&#22411;&#65292;&#20351;&#23427;&#20204;&#22312;CPU&#21644;GPU&#19978;&#36816;&#34892;&#26102;&#33021;&#22815;&#26174;&#33879;&#20943;&#23569;&#33021;&#28304;&#25104;&#26412;&#21644;&#25512;&#29702;&#26102;&#38388;&#65292;&#32780;&#19981;&#20250;&#24433;&#21709;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.06152</link><description>&lt;p&gt;
EfficientBioAI&#65306;&#20351;&#29983;&#29289;&#25104;&#20687;AI&#27169;&#22411;&#22312;&#33021;&#37327;&#12289;&#24310;&#36831;&#21644;&#34920;&#31034;&#26041;&#38754;&#26356;&#39640;&#25928;
&lt;/p&gt;
&lt;p&gt;
EfficientBioAI: Making Bioimaging AI Models Efficient in Energy, Latency and Representation. (arXiv:2306.06152v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06152
&lt;/p&gt;
&lt;p&gt;
EfficientBioAI&#26159;&#19968;&#20010;&#21363;&#25554;&#21363;&#29992;&#30340;&#24037;&#20855;&#31665;&#65292;&#21487;&#20197;&#21387;&#32553;&#29983;&#29289;&#25104;&#20687;AI&#27169;&#22411;&#65292;&#20351;&#23427;&#20204;&#22312;CPU&#21644;GPU&#19978;&#36816;&#34892;&#26102;&#33021;&#22815;&#26174;&#33879;&#20943;&#23569;&#33021;&#28304;&#25104;&#26412;&#21644;&#25512;&#29702;&#26102;&#38388;&#65292;&#32780;&#19981;&#20250;&#24433;&#21709;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20170;&#65292;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#22312;&#29983;&#29289;&#25104;&#20687;&#20998;&#26512;&#20013;&#34987;&#24191;&#27867;&#20351;&#29992;&#65292;&#20294;&#30001;&#20110;&#27169;&#22411;&#35268;&#27169;&#21644;&#22797;&#26434;&#24615;&#30340;&#22686;&#38271;&#20197;&#21450;&#29616;&#20195;&#29983;&#29289;&#21307;&#23398;&#30740;&#31350;&#20013;&#38656;&#35201;&#24555;&#36895;&#22686;&#38271;&#30340;&#20998;&#26512;&#38656;&#27714;&#65292;AI&#27169;&#22411;&#30340;&#25928;&#29575;&#65292;&#22914;&#33021;&#37327;&#28040;&#32791;&#21644;&#24310;&#36831;&#65292;&#26159;&#19981;&#21487;&#24573;&#35270;&#30340;&#12290;&#23601;&#20687;&#25105;&#20204;&#21487;&#20197;&#21387;&#32553;&#22823;&#22411;&#22270;&#20687;&#20197;&#23454;&#29616;&#39640;&#25928;&#23384;&#20648;&#21644;&#20849;&#20139;&#19968;&#26679;&#65292;&#25105;&#20204;&#20063;&#21487;&#20197;&#21387;&#32553;AI&#27169;&#22411;&#20197;&#23454;&#29616;&#39640;&#25928;&#24212;&#29992;&#21644;&#37096;&#32626;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;EfficientBioAI&#65292;&#36825;&#26159;&#19968;&#20010;&#21363;&#25554;&#21363;&#29992;&#30340;&#24037;&#20855;&#31665;&#65292;&#21487;&#20197;&#21387;&#32553;&#32473;&#23450;&#30340;&#29983;&#29289;&#25104;&#20687;AI&#27169;&#22411;&#65292;&#20351;&#23427;&#20204;&#22312;CPU&#21644;GPU&#19978;&#36816;&#34892;&#26102;&#33021;&#22815;&#26174;&#33879;&#20943;&#23569;&#33021;&#28304;&#25104;&#26412;&#21644;&#25512;&#29702;&#26102;&#38388;&#65292;&#32780;&#19981;&#20250;&#24433;&#21709;&#20934;&#30830;&#24615;&#12290;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#21387;&#32553;&#21518;&#30340;&#39044;&#27979;&#20934;&#30830;&#29575;&#29978;&#33267;&#21487;&#33021;&#20250;&#22686;&#21152;&#65292;&#22240;&#20026;&#21387;&#32553;&#36807;&#31243;&#21487;&#20197;&#21435;&#38500;&#27169;&#22411;&#34920;&#31034;&#20013;&#30340;&#20887;&#20313;&#20449;&#24687;&#65292;&#20174;&#32780;&#20943;&#23569;&#36807;&#24230;&#25311;&#21512;&#12290;&#20174;&#22235;&#31181;&#19981;&#21516;&#30340;&#29983;&#29289;&#22270;&#20687;&#20998;&#26512;&#24212;&#29992;&#20013;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#25512;&#29702;&#36807;&#31243;&#20013;&#30340;&#36895;&#24230;&#25552;&#39640;&#20102;&#32422;2-5&#20493;&#65292;&#22312;GPU&#19978;&#25552;&#39640;&#20102;&#32422;3-10&#20493;&#65292;&#24182;&#19988;&#20934;&#30830;&#24615;&#22987;&#32456;&#24471;&#21040;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial intelligence (AI) has been widely used in bioimage image analysis nowadays, but the efficiency of AI models, like the energy consumption and latency is not ignorable due to the growing model size and complexity, as well as the fast-growing analysis needs in modern biomedical studies. Like we can compress large images for efficient storage and sharing, we can also compress the AI models for efficient applications and deployment. In this work, we present EfficientBioAI, a plug-and-play toolbox that can compress given bioimaging AI models for them to run with significantly reduced energy cost and inference time on both CPU and GPU, without compromise on accuracy. In some cases, the prediction accuracy could even increase after compression, since the compression procedure could remove redundant information in the model representation and therefore reduce over-fitting. From four different bioimage analysis applications, we observed around 2-5 times speed-up during inference and 3
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22270;&#20687;-&#26631;&#39064;&#23545;&#36827;&#34892;&#24369;&#30417;&#30563;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#23454;&#29616;&#23450;&#20301;&#21644;&#26631;&#27880;&#22270;&#20687;&#20013;&#30340;&#23545;&#35937;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23454;&#29616;&#20102;47.51%&#30340;&#30701;&#35821;&#25509;&#22320;&#20197;&#21450;21.1 &#21644; 10.5&#30340;&#26032;&#30340;&#26368;&#39640;mAP&#24471;&#20998;&#12290;</title><link>http://arxiv.org/abs/2306.06149</link><description>&lt;p&gt;
&#20174;&#22270;&#20687;-&#26631;&#39064;&#23545;&#26631;&#27880;&#20013;&#33719;&#21462;&#36793;&#30028;&#26694;
&lt;/p&gt;
&lt;p&gt;
Read, look and detect: Bounding box annotation from image-caption pairs. (arXiv:2306.06149v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06149
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22270;&#20687;-&#26631;&#39064;&#23545;&#36827;&#34892;&#24369;&#30417;&#30563;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#23454;&#29616;&#23450;&#20301;&#21644;&#26631;&#27880;&#22270;&#20687;&#20013;&#30340;&#23545;&#35937;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23454;&#29616;&#20102;47.51%&#30340;&#30701;&#35821;&#25509;&#22320;&#20197;&#21450;21.1 &#21644; 10.5&#30340;&#26032;&#30340;&#26368;&#39640;mAP&#24471;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22270;&#20687;-&#26631;&#39064;&#23545;&#36827;&#34892;&#23545;&#35937;&#23450;&#20301;&#21644;&#26631;&#27880;&#30340;&#26041;&#27861;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#21033;&#29992;&#20102;&#36739;&#24369;&#30340;&#30417;&#30563;&#65292;&#24182;&#36890;&#36807;&#32467;&#21512;&#26368;&#36817;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#21644;&#33258;&#30417;&#30563;&#35270;&#35273;&#36716;&#25442;&#22120;&#26469;&#23454;&#29616;&#30701;&#35821;&#25509;&#22320;&#21644;&#30446;&#26631;&#26816;&#27979;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Various methods have been proposed to detect objects while reducing the cost of data annotation. For instance, weakly supervised object detection (WSOD) methods rely only on image-level annotations during training. Unfortunately, data annotation remains expensive since annotators must provide the categories describing the content of each image and labeling is restricted to a fixed set of categories. In this paper, we propose a method to locate and label objects in an image by using a form of weaker supervision: image-caption pairs. By leveraging recent advances in vision-language (VL) models and self-supervised vision transformers (ViTs), our method is able to perform phrase grounding and object detection in a weakly supervised manner. Our experiments demonstrate the effectiveness of our approach by achieving a 47.51% recall@1 score in phrase grounding on Flickr30k Entities and establishing a new state-of-the-art in object detection by achieving 21.1 mAP 50 and 10.5 mAP 50:95 on MS COC
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#36752;&#23556;&#38450;&#25252;&#26041;&#27861;&#65292;&#25506;&#35752;&#20102;&#20154;&#24037;&#26234;&#33021;&#22312;&#36752;&#23556;&#38450;&#25252;&#20013;&#30340;&#28508;&#22312;&#20248;&#21183;&#21644;&#38556;&#30861;&#65292;&#24182;&#25552;&#20986;&#20102;&#20419;&#36827;&#31185;&#23398;&#25216;&#26415;&#25104;&#26524;&#30340;&#21512;&#20316;&#26041;&#24335;&#12290;</title><link>http://arxiv.org/abs/2306.06148</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#19982;&#36752;&#23556;&#38450;&#25252;&#65306;&#38761;&#21629;&#24615;&#36824;&#26159;&#26356;&#26032;&#25442;&#20195;&#65311;(arXiv:2306.06148v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
Artificial intelligence and radiation protection. A game changer or an update?. (arXiv:2306.06148v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06148
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#36752;&#23556;&#38450;&#25252;&#26041;&#27861;&#65292;&#25506;&#35752;&#20102;&#20154;&#24037;&#26234;&#33021;&#22312;&#36752;&#23556;&#38450;&#25252;&#20013;&#30340;&#28508;&#22312;&#20248;&#21183;&#21644;&#38556;&#30861;&#65292;&#24182;&#25552;&#20986;&#20102;&#20419;&#36827;&#31185;&#23398;&#25216;&#26415;&#25104;&#26524;&#30340;&#21512;&#20316;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#34987;&#35748;&#20026;&#26159;&#26412;&#19990;&#32426;&#26368;&#39072;&#35206;&#24615;&#30340;&#25216;&#26415;&#20043;&#19968;&#65292;&#20855;&#26377;&#26080;&#25968;&#30340;&#24212;&#29992;&#12290;&#37027;&#20040;&#23427;&#23545;&#36752;&#23556;&#38450;&#25252;&#24847;&#21619;&#30528;&#20160;&#20040;&#65311;&#26412;&#25991;&#20171;&#32461;&#20102;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#30340;&#26041;&#27861;&#30340;&#22522;&#26412;&#21407;&#29702;&#65292;&#24182;&#20171;&#32461;&#20102;&#22312;&#19981;&#21516;&#39046;&#22495;&#30340;&#36752;&#23556;&#38450;&#25252;&#20013;&#30340;&#39318;&#27425;&#24212;&#29992;&#12290;&#39044;&#35745;&#20154;&#24037;&#26234;&#33021;&#22312;&#36752;&#23556;&#38450;&#25252;&#20013;&#30340;&#20351;&#29992;&#23558;&#36234;&#26469;&#36234;&#24191;&#27867;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25506;&#35752;&#20102;&#19968;&#20123;&#20248;&#28857;&#21644;&#28508;&#22312;&#30340;&#38556;&#30861;&#21644;&#38382;&#39064;&#65292;&#21253;&#25324;&#20262;&#29702;&#38382;&#39064;&#12290;&#20316;&#32773;&#24314;&#35758;&#36752;&#23556;&#38450;&#25252;&#19987;&#19994;&#20154;&#21592;&#19982;&#25968;&#25454;&#31185;&#23398;&#23478;&#19987;&#23478;&#21512;&#20316;&#65292;&#21152;&#36895;&#24182;&#25351;&#23548;&#31639;&#27861;&#30340;&#24320;&#21457;&#65292;&#20197;&#33719;&#24471;&#26377;&#25928;&#30340;&#31185;&#23398;&#25216;&#26415;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial intelligence (AI) is regarded as one of the most disruptive technology of the century and with countless applications. What does it mean for radiation protection? This article describes the fundamentals of machine learning (ML) based methods and presents the inaugural applications in different fields of radiation protection. It is foreseen that the usage of AI will increase in radiation protection. Consequently, this article explores some of the benefits and also the potential barriers and questions, including ethical ones, that can come out. The article proposes that collaboration between radiation protection professionals and data scientist experts can accelerate and guide the development of the algorithms for effective scientific and technological outcomes.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;SentiGOLD&#65292;&#19968;&#20010;&#23391;&#21152;&#25289;&#35821;&#22810;&#39046;&#22495;&#24773;&#24863;&#20998;&#26512;&#25968;&#25454;&#38598;&#12290;&#35813;&#25968;&#25454;&#38598;&#30001;70,000&#20010;&#26469;&#33258;&#19981;&#21516;&#26469;&#28304;&#30340;&#26679;&#26412;&#32452;&#25104;&#65292;&#36981;&#23432;&#20102;&#25919;&#24220;&#21644;&#35821;&#35328;&#23398;&#22996;&#21592;&#20250;&#21830;&#23450;&#30340;&#35821;&#35328;&#32422;&#23450;&#65292;&#21253;&#25324;&#20102;30&#20010;&#39046;&#22495;&#21644;5&#20010;&#24773;&#24863;&#31867;&#21035;&#12290;&#22312;&#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#27880;&#37322;&#26041;&#26696;&#19979;&#65292;&#35813;&#25968;&#25454;&#38598;&#30340;&#20114;&#35780;&#19968;&#33268;&#24615;&#34920;&#29616;&#20986;&#33394;&#65292;&#21487;&#29992;&#20110;&#24314;&#31435;&#23391;&#21152;&#25289;&#35821;&#24773;&#24863;&#20998;&#26512;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2306.06147</link><description>&lt;p&gt;
SentiGOLD&#65306;&#19968;&#20010;&#22823;&#22411;&#23391;&#21152;&#25289;&#35821;&#22810;&#39046;&#22495;&#24773;&#24863;&#20998;&#26512;&#40644;&#37329;&#26631;&#20934;&#25968;&#25454;&#38598;&#21450;&#20854;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
SentiGOLD: A Large Bangla Gold Standard Multi-Domain Sentiment Analysis Dataset and its Evaluation. (arXiv:2306.06147v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06147
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;SentiGOLD&#65292;&#19968;&#20010;&#23391;&#21152;&#25289;&#35821;&#22810;&#39046;&#22495;&#24773;&#24863;&#20998;&#26512;&#25968;&#25454;&#38598;&#12290;&#35813;&#25968;&#25454;&#38598;&#30001;70,000&#20010;&#26469;&#33258;&#19981;&#21516;&#26469;&#28304;&#30340;&#26679;&#26412;&#32452;&#25104;&#65292;&#36981;&#23432;&#20102;&#25919;&#24220;&#21644;&#35821;&#35328;&#23398;&#22996;&#21592;&#20250;&#21830;&#23450;&#30340;&#35821;&#35328;&#32422;&#23450;&#65292;&#21253;&#25324;&#20102;30&#20010;&#39046;&#22495;&#21644;5&#20010;&#24773;&#24863;&#31867;&#21035;&#12290;&#22312;&#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#27880;&#37322;&#26041;&#26696;&#19979;&#65292;&#35813;&#25968;&#25454;&#38598;&#30340;&#20114;&#35780;&#19968;&#33268;&#24615;&#34920;&#29616;&#20986;&#33394;&#65292;&#21487;&#29992;&#20110;&#24314;&#31435;&#23391;&#21152;&#25289;&#35821;&#24773;&#24863;&#20998;&#26512;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;SentiGOLD&#65292;&#19968;&#20010;&#23391;&#21152;&#25289;&#35821;&#22810;&#39046;&#22495;&#24773;&#24863;&#20998;&#26512;&#25968;&#25454;&#38598;&#12290;&#23427;&#30001;&#26469;&#33258;&#19981;&#21516;&#26469;&#28304;&#30340;70,000&#20010;&#26679;&#26412;&#32452;&#25104;&#65292;&#24182;&#30001;&#19968;&#32452;&#24615;&#21035;&#24179;&#34913;&#30340;&#35821;&#35328;&#23398;&#23478;&#36827;&#34892;&#27880;&#37322;&#12290;SentiGOLD&#36981;&#23432;&#23391;&#21152;&#25289;&#22269;&#25919;&#24220;&#21644;&#23391;&#21152;&#25289;&#35821;&#35328;&#23398;&#22996;&#21592;&#20250;&#21830;&#23450;&#30340;&#26082;&#23450;&#35821;&#35328;&#32422;&#23450;&#12290;&#19982;&#33521;&#35821;&#21644;&#20854;&#20182;&#35821;&#35328;&#19981;&#21516;&#65292;&#30001;&#20110;&#32570;&#20047;&#22269;&#23478;&#35821;&#35328;&#23398;&#26694;&#26550;&#65292;&#23391;&#21152;&#25289;&#35821;&#32570;&#20047;&#26631;&#20934;&#30340;&#24773;&#24863;&#20998;&#26512;&#25968;&#25454;&#38598;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#25324;&#22312;&#32447;&#35270;&#39057;&#35780;&#35770;&#12289;&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#12289;&#21338;&#23458;&#12289;&#26032;&#38395;&#21644;&#20854;&#20182;&#26469;&#28304;&#30340;&#25968;&#25454;&#65292;&#24182;&#20005;&#26684;&#32500;&#25252;&#39046;&#22495;&#21644;&#31867;&#21035;&#20998;&#24067;&#12290;&#23427;&#28085;&#30422;&#20102;30&#20010;&#39046;&#22495;&#65288;&#22914;&#25919;&#27835;&#12289;&#23089;&#20048;&#12289;&#20307;&#32946;&#65289;&#65292;&#21253;&#25324;5&#20010;&#24773;&#24863;&#31867;&#21035;&#65288;&#24378;&#28872;&#30340;&#36127;&#38754;&#12289;&#24369;&#30340;&#36127;&#38754;&#12289;&#20013;&#24615;&#21644;&#24378;&#28872;&#30340;&#27491;&#38754;&#65289;&#12290;&#30001;&#22269;&#23478;&#35821;&#35328;&#23398;&#22996;&#21592;&#20250;&#25209;&#20934;&#30340;&#27880;&#37322;&#26041;&#26696;&#30830;&#20445;&#20855;&#26377;&#40065;&#26834;&#30340;&#20114;&#35780;&#19968;&#33268;&#24615;&#65288;IAA&#65289;&#65292;&#33778;&#21033;&#26031;kappa&#24471;&#20998;&#20026;0.88&#12290;&#20869;&#37096;&#21644;&#36328;&#25968;&#25454;&#38598;&#30340;&#35780;&#20272;&#34920;&#26126;...
&lt;/p&gt;
&lt;p&gt;
This study introduces SentiGOLD, a Bangla multi-domain sentiment analysis dataset. Comprising 70,000 samples, it was created from diverse sources and annotated by a gender-balanced team of linguists. SentiGOLD adheres to established linguistic conventions agreed upon by the Government of Bangladesh and a Bangla linguistics committee. Unlike English and other languages, Bangla lacks standard sentiment analysis datasets due to the absence of a national linguistics framework. The dataset incorporates data from online video comments, social media posts, blogs, news, and other sources while maintaining domain and class distribution rigorously. It spans 30 domains (e.g., politics, entertainment, sports) and includes 5 sentiment classes (strongly negative, weakly negative, neutral, and strongly positive). The annotation scheme, approved by the national linguistics committee, ensures a robust Inter Annotator Agreement (IAA) with a Fleiss' kappa score of 0.88. Intra- and cross-dataset evaluatio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20013;&#65292;&#30740;&#31350;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22521;&#35757;&#26041;&#27861;&#24433;&#21709;&#28145;&#23618;&#32593;&#32476;&#20998;&#31867;&#22120;&#24615;&#33021;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#22312;&#25968;&#25454;&#38544;&#34255;&#34920;&#31034;&#20013;&#36798;&#21040;&#26356;&#39640;&#30340;&#32447;&#24615;&#21487;&#20998;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.06146</link><description>&lt;p&gt;
&#38544;&#34255;&#20998;&#31867;&#23618;&#65306;&#20851;&#20110;&#25968;&#25454;&#38544;&#34255;&#34920;&#31034;&#20013;&#26356;&#39640;&#32447;&#24615;&#21487;&#20998;&#24615;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Hidden Classification Layers: a study on Data Hidden Representations with a Higher Degree of Linear Separability between the Classes. (arXiv:2306.06146v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06146
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20013;&#65292;&#30740;&#31350;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22521;&#35757;&#26041;&#27861;&#24433;&#21709;&#28145;&#23618;&#32593;&#32476;&#20998;&#31867;&#22120;&#24615;&#33021;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#22312;&#25968;&#25454;&#38544;&#34255;&#34920;&#31034;&#20013;&#36798;&#21040;&#26356;&#39640;&#30340;&#32447;&#24615;&#21487;&#20998;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20998;&#31867;&#38382;&#39064;&#30340;&#32972;&#26223;&#19979;&#65292;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#26041;&#27861;&#20195;&#34920;&#20102;&#26368;&#20808;&#36827;&#30340;&#25216;&#26415;&#12290;&#35768;&#22810;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#37117;&#22522;&#20110;&#26631;&#20934;&#30340;&#22810;&#23618;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#30340;&#21464;&#31181;&#12290;&#36825;&#20123;&#20063;&#34987;&#31216;&#20026;&#28145;&#24230;&#32593;&#32476;&#12290;&#22522;&#26412;&#24605;&#24819;&#26159;&#27599;&#20010;&#38544;&#34255;&#31070;&#32463;&#23618;&#23436;&#25104;&#19968;&#31181;&#25968;&#25454;&#36716;&#25442;&#65292;&#39044;&#26399;&#20351;&#25968;&#25454;&#34920;&#31034;&#8220;&#27604;&#20043;&#21069;&#26356;&#32447;&#24615;&#21487;&#20998;&#8221;&#65292;&#20197;&#33719;&#24471;&#23613;&#21487;&#33021;&#32447;&#24615;&#21487;&#20998;&#30340;&#26368;&#32456;&#25968;&#25454;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#30830;&#23450;&#21487;&#20197;&#25191;&#34892;&#36825;&#20123;&#36716;&#25442;&#30340;&#36866;&#24403;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#26159;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#22521;&#35757;&#26041;&#27861;&#23545;&#28145;&#23618;&#32593;&#32476;&#20998;&#31867;&#22120;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#36825;&#31181;&#26041;&#27861;&#20542;&#21521;&#20110;&#20351;&#29992;&#26631;&#20934;&#26041;&#27861;&#30456;&#27604;&#65292;&#38544;&#34255;&#23618;&#30340;&#25968;&#25454;&#34920;&#31034;&#20855;&#26377;&#26356;&#39640;&#30340;&#31867;&#20043;&#38388;&#32447;&#24615;&#21487;&#20998;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#35813;&#26550;&#26500;&#24341;&#20837;&#20102;&#19968;&#20010;&#28041;&#21450;&#35823;&#24046;&#20989;&#25968;&#30340;&#26032;&#39062;&#22521;&#35757;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the context of classification problems, Deep Learning (DL) approaches represent state of art. Many DL approaches are based on variations of standard multi-layer feed-forward neural networks. These are also referred to as deep networks. The basic idea is that each hidden neural layer accomplishes a data transformation which is expected to make the data representation "somewhat more linearly separable" than the previous one to obtain a final data representation which is as linearly separable as possible. However, determining the appropriate neural network parameters that can perform these transformations is a critical problem. In this paper, we investigate the impact on deep network classifier performances of a training approach favouring solutions where data representations at the hidden layers have a higher degree of linear separability between the classes with respect to standard methods. To this aim, we propose a neural network architecture which induces an error function involvin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#27169;&#24335;&#26041;&#27861;&#36827;&#34892;&#21152;&#26435;&#24322;&#24120;&#20540;&#26816;&#27979;&#30340;&#22823;&#25968;&#25454;&#25366;&#25496;&#26041;&#27861;&#65292;&#21487;&#25581;&#31034;&#31995;&#32479;&#25925;&#38556;&#12289;&#27450;&#35784;&#27963;&#21160;&#21644;&#25968;&#25454;&#27169;&#24335;&#30340;&#37325;&#35201;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2306.06139</link><description>&lt;p&gt;
WePaMaDM-Outlier Detection: &#20351;&#29992;&#27169;&#24335;&#26041;&#27861;&#36827;&#34892;&#21152;&#26435;&#24322;&#24120;&#20540;&#26816;&#27979;&#30340;&#22823;&#25968;&#25454;&#25366;&#25496;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
WePaMaDM-Outlier Detection: Weighted Outlier Detection using Pattern Approaches for Mass Data Mining. (arXiv:2306.06139v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06139
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#27169;&#24335;&#26041;&#27861;&#36827;&#34892;&#21152;&#26435;&#24322;&#24120;&#20540;&#26816;&#27979;&#30340;&#22823;&#25968;&#25454;&#25366;&#25496;&#26041;&#27861;&#65292;&#21487;&#25581;&#31034;&#31995;&#32479;&#25925;&#38556;&#12289;&#27450;&#35784;&#27963;&#21160;&#21644;&#25968;&#25454;&#27169;&#24335;&#30340;&#37325;&#35201;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21152;&#26435;&#24322;&#24120;&#20540;&#26816;&#27979;&#26159;&#19968;&#31181;&#35782;&#21035;&#25968;&#25454;&#38598;&#20013;&#24322;&#24120;&#25110;&#24322;&#24120;&#25968;&#25454;&#28857;&#30340;&#26041;&#27861;&#65292;&#21487;&#33021;&#30001;&#21508;&#31181;&#22240;&#32032;&#22914;&#20154;&#20026;&#38169;&#35823;&#12289;&#27450;&#35784;&#25110;&#35774;&#22791;&#25925;&#38556;&#24341;&#36215;&#12290;&#26816;&#27979;&#24322;&#24120;&#20540;&#21487;&#20197;&#25581;&#31034;&#26377;&#20851;&#31995;&#32479;&#25925;&#38556;&#12289;&#27450;&#35784;&#27963;&#21160;&#21644;&#25968;&#25454;&#27169;&#24335;&#30340;&#37325;&#35201;&#20449;&#24687;&#65292;&#24110;&#21161;&#19987;&#23478;&#35299;&#20915;&#36825;&#20123;&#24322;&#24120;&#30340;&#26681;&#26412;&#21407;&#22240;&#12290;&#28982;&#32780;&#65292;&#21019;&#24314;&#27491;&#24120;&#25968;&#25454;&#27169;&#24335;&#30340;&#27169;&#22411;&#20197;&#35782;&#21035;&#24322;&#24120;&#20540;&#21487;&#33021;&#30001;&#20110;&#36755;&#20837;&#25968;&#25454;&#30340;&#24615;&#36136;&#12289;&#26631;&#35760;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#20197;&#21450;&#38382;&#39064;&#30340;&#29305;&#23450;&#35201;&#27714;&#32780;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#38024;&#23545;&#29305;&#23450;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#25366;&#25496;&#39046;&#22495;&#25552;&#20986;&#20102;WePaMaDM-Outlier Detection&#26041;&#27861;&#65292;&#35777;&#26126;&#36825;&#26679;&#30340;&#25216;&#26415;&#26159;&#20381;&#36182;&#20110;&#29305;&#23450;&#39046;&#22495;&#24182;&#36890;&#24120;&#38024;&#23545;&#29305;&#23450;&#38382;&#39064;&#21046;&#23450;&#30340;&#12290;&#28982;&#32780;&#65292;&#31867;&#20284;&#30340;&#39046;&#22495;&#21487;&#20197;&#36827;&#34892;&#20462;&#25913;&#20197;&#36866;&#24212;&#35299;&#20915;&#26041;&#26696;&#12290;&#26412;&#30740;&#31350;&#36824;&#30740;&#31350;&#20102;&#25968;&#25454;&#24314;&#27169;&#22312;&#30417;&#25511;&#12289;&#25925;&#38556;&#26816;&#27979;&#21644;&#36235;&#21183;&#20998;&#26512;&#20013;&#30340;&#24322;&#24120;&#28857;&#26816;&#27979;&#25216;&#26415;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Weighted Outlier Detection is a method for identifying unusual or anomalous data points in a dataset, which can be caused by various factors like human error, fraud, or equipment malfunctions. Detecting outliers can reveal vital information about system faults, fraudulent activities, and patterns in the data, assisting experts in addressing the root causes of these anomalies. However,creating a model of normal data patterns to identify outliers can be challenging due to the nature of input data, labeled data availability, and specific requirements of the problem. This article proposed the WePaMaDM-Outlier Detection with distinct mass data mining domain, demonstrating that such techniques are domain-dependent and usually developed for specific problem formulations. Nevertheless, similar domains can adapt solutions with modifications. This work also investigates the significance of data modeling in outlier detection techniques in surveillance, fault detection, and trend analysis, also re
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#40065;&#26834;&#24615;&#27979;&#35797;&#26694;&#26550;RTCA&#65292;&#37319;&#29992;&#22522;&#20110;&#24046;&#20998;&#36827;&#21270;&#30340;&#20851;&#38190;&#26234;&#33021;&#20307;&#25915;&#20987;&#26041;&#27861;&#65292;&#20851;&#38190;&#26234;&#33021;&#20307;&#36890;&#36807;&#22242;&#38431;&#21512;&#20316;&#25919;&#31574;&#35780;&#20272;&#26041;&#27861;&#34987;&#36873;&#21462;&#20026;&#21463;&#23475;&#32773;&#12290;&#35813;&#26694;&#26550;&#22312;&#40065;&#26834;&#24615;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.06136</link><description>&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#40065;&#26834;&#24615;&#27979;&#35797;&#65306;&#23545;&#20851;&#38190;&#26234;&#33021;&#20307;&#36827;&#34892;&#29366;&#24577;&#25200;&#21160;
&lt;/p&gt;
&lt;p&gt;
Robustness Testing for Multi-Agent Reinforcement Learning: State Perturbations on Critical Agents. (arXiv:2306.06136v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06136
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#40065;&#26834;&#24615;&#27979;&#35797;&#26694;&#26550;RTCA&#65292;&#37319;&#29992;&#22522;&#20110;&#24046;&#20998;&#36827;&#21270;&#30340;&#20851;&#38190;&#26234;&#33021;&#20307;&#25915;&#20987;&#26041;&#27861;&#65292;&#20851;&#38190;&#26234;&#33021;&#20307;&#36890;&#36807;&#22242;&#38431;&#21512;&#20316;&#25919;&#31574;&#35780;&#20272;&#26041;&#27861;&#34987;&#36873;&#21462;&#20026;&#21463;&#23475;&#32773;&#12290;&#35813;&#26694;&#26550;&#22312;&#40065;&#26834;&#24615;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#24050;&#32463;&#24191;&#27867;&#24212;&#29992;&#20110;&#26234;&#33021;&#20132;&#36890;&#21644;&#26080;&#20154;&#26426;&#31561;&#35768;&#22810;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;MARL&#31639;&#27861;&#23481;&#26131;&#21463;&#21040;&#26234;&#33021;&#20307;&#29366;&#24577;&#30340;&#23545;&#25239;&#24615;&#25200;&#21160;&#12290;&#23545;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#40065;&#26834;&#24615;&#27979;&#35797;&#26159;&#30830;&#35748;&#27169;&#22411;&#22312;&#38754;&#23545;&#24847;&#22806;&#25200;&#21160;&#26102;&#21487;&#20449;&#36182;&#30340;&#22522;&#26412;&#27493;&#39588;&#12290;&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#38024;&#23545;MARL&#30340;&#26032;&#22411;&#40065;&#26834;&#24615;&#27979;&#35797;&#26694;&#26550;&#65292;&#20197;&#25915;&#20987;&#20851;&#38190;&#26234;&#33021;&#20307;&#30340;&#29366;&#24577;&#65288;RTCA&#65289;&#12290;RTCA&#26377;&#20004;&#20010;&#21019;&#26032;&#28857;&#65306;1&#65289;&#22522;&#20110;&#24046;&#20998;&#36827;&#21270;&#65288;DE&#65289;&#30340;&#26041;&#27861;&#36873;&#23450;&#20851;&#38190;&#30340;&#26234;&#33021;&#20307;&#20316;&#20026;&#21463;&#23475;&#32773;&#65292;&#24182;&#20026;&#23427;&#20204;&#25552;&#20379;&#26368;&#22351;&#24773;&#20917;&#30340;&#32852;&#21512;&#21160;&#20316;&#24314;&#35758;&#65307;2&#65289;&#37319;&#29992;&#22242;&#38431;&#21512;&#20316;&#25919;&#31574;&#35780;&#20272;&#26041;&#27861;&#20316;&#20026;DE&#20248;&#21270;&#30340;&#30446;&#26631;&#20989;&#25968;&#12290;&#28982;&#21518;&#65292;&#22522;&#20110;&#26368;&#22351;&#24773;&#20917;&#30340;&#32852;&#21512;&#21160;&#20316;&#29983;&#25104;&#20851;&#38190;&#26234;&#33021;&#20307;&#30340;&#23545;&#25239;&#24615;&#29366;&#24577;&#25200;&#21160;&#12290;&#36825;&#26159;&#31532;&#19968;&#20010;&#20855;&#26377;&#19981;&#21516;&#21463;&#23475;&#32773;&#26234;&#33021;&#20307;&#30340;&#40065;&#26834;&#24615;&#27979;&#35797;&#26694;&#26550;&#12290;RTCA&#22312;&#40065;&#26834;&#24615;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-Agent Reinforcement Learning (MARL) has been widely applied in many fields such as smart traffic and unmanned aerial vehicles. However, most MARL algorithms are vulnerable to adversarial perturbations on agent states. Robustness testing for a trained model is an essential step for confirming the trustworthiness of the model against unexpected perturbations. This work proposes a novel Robustness Testing framework for MARL that attacks states of Critical Agents (RTCA). The RTCA has two innovations: 1) a Differential Evolution (DE) based method to select critical agents as victims and to advise the worst-case joint actions on them; and 2) a team cooperation policy evaluation method employed as the objective function for the optimization of DE. Then, adversarial state perturbations of the critical agents are generated based on the worst-case joint actions. This is the first robustness testing framework with varying victim agents. RTCA demonstrates outstanding performance in terms of 
&lt;/p&gt;</description></item><item><title>&#29983;&#25104;&#27169;&#22411;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#27169;&#20223;&#26368;&#31967;&#31957;&#30340;&#20869;&#23481;&#65292;&#36890;&#36807;&#23433;&#20840;&#36755;&#20837;&#21644;&#36755;&#20986;&#36807;&#28388;&#22120;&#23454;&#29616;&#36127;&#36131;&#20219;&#37096;&#32626;&#65307;&#36890;&#36807;&#23433;&#20840;&#12289;&#20844;&#24179;&#21644;&#25351;&#26631;&#20844;&#24179;&#30340;&#23450;&#20041;&#65292;&#21015;&#20030;&#20102;&#27599;&#20010;&#39046;&#22495;&#21487;&#33021;&#36935;&#21040;&#30340;&#20363;&#23376;&#25439;&#23475;&#65292;&#24182;&#25552;&#20379;&#20102;&#25439;&#23475;&#37327;&#21270;&#30340;&#28436;&#31034;&#12290;</title><link>http://arxiv.org/abs/2306.06135</link><description>&lt;p&gt;
&#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#20869;&#23481;&#23457;&#26680;&#23433;&#20840;&#19982;&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;
Safety and Fairness for Content Moderation in Generative Models. (arXiv:2306.06135v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06135
&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#27169;&#22411;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#27169;&#20223;&#26368;&#31967;&#31957;&#30340;&#20869;&#23481;&#65292;&#36890;&#36807;&#23433;&#20840;&#36755;&#20837;&#21644;&#36755;&#20986;&#36807;&#28388;&#22120;&#23454;&#29616;&#36127;&#36131;&#20219;&#37096;&#32626;&#65307;&#36890;&#36807;&#23433;&#20840;&#12289;&#20844;&#24179;&#21644;&#25351;&#26631;&#20844;&#24179;&#30340;&#23450;&#20041;&#65292;&#21015;&#20030;&#20102;&#27599;&#20010;&#39046;&#22495;&#21487;&#33021;&#36935;&#21040;&#30340;&#20363;&#23376;&#25439;&#23475;&#65292;&#24182;&#25552;&#20379;&#20102;&#25439;&#23475;&#37327;&#21270;&#30340;&#28436;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#30340;&#26174;&#33879;&#36827;&#27493;&#65292;&#26032;&#25216;&#26415;&#27491;&#36805;&#36895;&#37096;&#32626;&#21040;&#29983;&#25104;&#32452;&#20214;&#20013;&#12290;&#29983;&#25104;&#27169;&#22411;&#36890;&#24120;&#26159;&#22312;&#22823;&#22411;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#23548;&#33268;&#20854;&#34892;&#20026;&#21487;&#33021;&#27169;&#20223;&#35757;&#32451;&#25968;&#25454;&#20013;&#26368;&#31967;&#31957;&#30340;&#20869;&#23481;&#12290;&#36127;&#36131;&#20219;&#22320;&#37096;&#32626;&#29983;&#25104;&#25216;&#26415;&#38656;&#35201;&#20869;&#23481;&#23457;&#26680;&#31574;&#30053;&#65292;&#20363;&#22914;&#23433;&#20840;&#36755;&#20837;&#21644;&#36755;&#20986;&#36807;&#28388;&#22120;&#12290;&#22312;&#27492;&#65292;&#25105;&#20204;&#25552;&#20379;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#65292;&#29992;&#20110;&#27010;&#24565;&#21270;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#25216;&#26415;&#30340;&#36127;&#36131;&#20219;&#20869;&#23481;&#23457;&#26680;&#65292;&#21253;&#25324;&#22914;&#20309;&#23454;&#35777;&#22320;&#34913;&#37327;&#25105;&#20204;&#21015;&#20030;&#30340;&#26500;&#36896;&#12290;&#25105;&#20204;&#23450;&#20041;&#21644;&#21306;&#20998;&#20102;&#23433;&#20840;&#12289;&#20844;&#24179;&#21644;&#25351;&#26631;&#20844;&#24179;&#30340;&#27010;&#24565;&#65292;&#24182;&#21015;&#20030;&#20102;&#27599;&#20010;&#39046;&#22495;&#21487;&#33021;&#20986;&#29616;&#30340;&#20363;&#23376;&#25439;&#23475;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#22914;&#20309;&#37327;&#21270;&#23450;&#20041;&#30340;&#25439;&#23475;&#30340;&#28436;&#31034;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#24635;&#32467;&#20102;&#25105;&#20204;&#28436;&#31034;&#30340;&#25439;&#23475;&#37327;&#21270;&#39118;&#26684;&#22914;&#20309;&#23454;&#29616;&#25968;&#25454;&#39537;&#21160;&#30340;&#20869;&#23481;&#23457;&#26680;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;
With significant advances in generative AI, new technologies are rapidly being deployed with generative components. Generative models are typically trained on large datasets, resulting in model behaviors that can mimic the worst of the content in the training data. Responsible deployment of generative technologies requires content moderation strategies, such as safety input and output filters. Here, we provide a theoretical framework for conceptualizing responsible content moderation of text-to-image generative technologies, including a demonstration of how to empirically measure the constructs we enumerate. We define and distinguish the concepts of safety, fairness, and metric equity, and enumerate example harms that can come in each domain. We then provide a demonstration of how the defined harms can be quantified. We conclude with a summary of how the style of harms quantification we demonstrate enables data-driven content moderation decisions.
&lt;/p&gt;</description></item><item><title>&#26412;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#22768;&#38899;&#35299;&#37322;&#30340;&#27010;&#24565;&#65292;&#20197;&#25552;&#20379;&#36275;&#22815;&#30340;&#20449;&#24687;&#26469;&#22240;&#26524;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#36827;&#34892;&#30340;&#39044;&#27979;&#65292;&#21453;&#23545;&#36890;&#36807;&#23558;&#20998;&#25968;&#24402;&#22240;&#20110;&#36755;&#20837;&#32452;&#20214;&#26469;&#35299;&#37322;&#40657;&#30418;&#27169;&#22411;&#30340;&#24815;&#20363;&#65292;&#24182;&#19988;&#25552;&#20986;&#24212;&#29992;&#29305;&#24449;&#36873;&#25321;&#20316;&#20026;&#30284;&#30151;&#39044;&#27979;&#27169;&#22411;&#30340;&#22768;&#38899;&#35299;&#37322;&#65292;&#20197;&#24314;&#31435;&#20020;&#24202;&#21307;&#29983;&#20043;&#38388;&#30340;&#20449;&#20219;&#12290;</title><link>http://arxiv.org/abs/2306.06134</link><description>&lt;p&gt;
&#21487;&#20449;&#26426;&#22120;&#23398;&#20064;&#30340;&#22768;&#38899;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Sound Explanation for Trustworthy Machine Learning. (arXiv:2306.06134v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06134
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#22768;&#38899;&#35299;&#37322;&#30340;&#27010;&#24565;&#65292;&#20197;&#25552;&#20379;&#36275;&#22815;&#30340;&#20449;&#24687;&#26469;&#22240;&#26524;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#36827;&#34892;&#30340;&#39044;&#27979;&#65292;&#21453;&#23545;&#36890;&#36807;&#23558;&#20998;&#25968;&#24402;&#22240;&#20110;&#36755;&#20837;&#32452;&#20214;&#26469;&#35299;&#37322;&#40657;&#30418;&#27169;&#22411;&#30340;&#24815;&#20363;&#65292;&#24182;&#19988;&#25552;&#20986;&#24212;&#29992;&#29305;&#24449;&#36873;&#25321;&#20316;&#20026;&#30284;&#30151;&#39044;&#27979;&#27169;&#22411;&#30340;&#22768;&#38899;&#35299;&#37322;&#65292;&#20197;&#24314;&#31435;&#20020;&#24202;&#21307;&#29983;&#20043;&#38388;&#30340;&#20449;&#20219;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#37319;&#29992;&#27491;&#24335;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#30340;&#21487;&#35299;&#37322;&#24615;&#38382;&#39064;&#12290;&#25105;&#20204;&#21453;&#23545;&#36890;&#36807;&#23558;&#20998;&#25968;&#24402;&#22240;&#20110;&#36755;&#20837;&#32452;&#20214;&#26469;&#35299;&#37322;&#40657;&#30418;&#27169;&#22411;&#30340;&#24815;&#20363;&#65292;&#22240;&#20026;&#36825;&#31181;&#35299;&#37322;&#26041;&#27861;&#30340;&#30446;&#26631;&#23384;&#22312;&#20869;&#22312;&#30340;&#20914;&#31361;&#12290;&#25105;&#20204;&#35777;&#26126;&#27809;&#26377;&#20219;&#20309;&#24402;&#22240;&#31639;&#27861;&#33021;&#22815;&#28385;&#36275;&#29305;&#24322;&#24615;&#12289;&#21487;&#21152;&#24615;&#12289;&#23436;&#25972;&#24615;&#21644;&#22522;&#32447;&#19981;&#21464;&#24615;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#27491;&#24335;&#23450;&#20041;&#20102;&#22312;&#20197;&#21069;&#30340;&#24037;&#20316;&#20013;&#19981;&#27491;&#24335;&#37319;&#29992;&#30340;&#27010;&#24565;&#8212;&#8212;&#22768;&#38899;&#35299;&#37322;&#12290;&#19968;&#20010;&#22768;&#38899;&#30340;&#35299;&#37322;&#38656;&#35201;&#25552;&#20379;&#36275;&#22815;&#30340;&#20449;&#24687;&#26469;&#22240;&#26524;&#35299;&#37322;&#31995;&#32479;&#25152;&#36827;&#34892;&#30340;&#39044;&#27979;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#24212;&#29992;&#29305;&#24449;&#36873;&#25321;&#20316;&#20026;&#30284;&#30151;&#39044;&#27979;&#27169;&#22411;&#30340;&#22768;&#38899;&#35299;&#37322;&#65292;&#20197;&#24314;&#31435;&#20020;&#24202;&#21307;&#29983;&#20043;&#38388;&#30340;&#20449;&#20219;&#12290;
&lt;/p&gt;
&lt;p&gt;
We take a formal approach to the explainability problem of machine learning systems. We argue against the practice of interpreting black-box models via attributing scores to input components due to inherently conflicting goals of attribution-based interpretation. We prove that no attribution algorithm satisfies specificity, additivity, completeness, and baseline invariance. We then formalize the concept, sound explanation, that has been informally adopted in prior work. A sound explanation entails providing sufficient information to causally explain the predictions made by a system. Finally, we present the application of feature selection as a sound explanation for cancer prediction models to cultivate trust among clinicians.
&lt;/p&gt;</description></item><item><title>&#25506;&#31350;&#29983;&#25104;AI&#21644;&#20114;&#32852;&#32593;&#20043;&#38388;&#20114;&#21160;&#30340;&#24433;&#21709;&#65292;&#29983;&#25104;AI&#21487;&#33021;&#25104;&#20026;&#25968;&#25454;&#20179;&#24211;&#36129;&#29486;&#32773;&#24182;&#24433;&#21709;&#21518;&#32493;&#35757;&#32451;&#65292;&#25552;&#20986;&#26410;&#26469;&#29256;&#26412;&#29983;&#25104;AI&#24037;&#20855;&#22312;&#20351;&#29992;&#28151;&#21512;&#25968;&#25454;&#35757;&#32451;&#26102;&#20250;&#20986;&#29616;&#30340;&#38382;&#39064;&#21644;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2306.06130</link><description>&lt;p&gt;
&#25506;&#31350;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#19982;&#20114;&#32852;&#32593;&#30340;&#30456;&#20114;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Towards Understanding the Interplay of Generative Artificial Intelligence and the Internet. (arXiv:2306.06130v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06130
&lt;/p&gt;
&lt;p&gt;
&#25506;&#31350;&#29983;&#25104;AI&#21644;&#20114;&#32852;&#32593;&#20043;&#38388;&#20114;&#21160;&#30340;&#24433;&#21709;&#65292;&#29983;&#25104;AI&#21487;&#33021;&#25104;&#20026;&#25968;&#25454;&#20179;&#24211;&#36129;&#29486;&#32773;&#24182;&#24433;&#21709;&#21518;&#32493;&#35757;&#32451;&#65292;&#25552;&#20986;&#26410;&#26469;&#29256;&#26412;&#29983;&#25104;AI&#24037;&#20855;&#22312;&#20351;&#29992;&#28151;&#21512;&#25968;&#25454;&#35757;&#32451;&#26102;&#20250;&#20986;&#29616;&#30340;&#38382;&#39064;&#21644;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26469;&#65292;&#22914;DALL-E&#12289;MidJourney&#25110;ChatGPT&#31561;&#33021;&#22815;&#29983;&#25104;&#36924;&#30495;&#22270;&#20687;&#25110;&#25991;&#26412;&#30340;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#24037;&#20855;&#30340;&#24191;&#27867;&#24212;&#29992;&#65292;&#24341;&#21457;&#20102;&#36825;&#20123;&#25216;&#26415;&#30340;&#31038;&#20250;&#24433;&#21709;&#25104;&#20026;&#20844;&#20849;&#20105;&#35770;&#30340;&#26680;&#24515;&#12290;&#36825;&#20123;&#24037;&#20855;&#26159;&#21487;&#33021;&#30340;&#65292;&#26159;&#22240;&#20026;&#20114;&#32852;&#32593;&#19978;&#20844;&#24320;&#21487;&#29992;&#30340;&#22823;&#37327;&#25968;&#25454;&#65288;&#25991;&#26412;&#21644;&#22270;&#20687;&#65289;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#36825;&#20123;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#25104;&#20026;&#20869;&#23481;&#21019;&#20316;&#32773;&#65292;&#24050;&#32463;&#20026;&#26410;&#26469;&#27169;&#22411;&#35757;&#32451;&#25152;&#38656;&#30340;&#25968;&#25454;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;&#22240;&#27492;&#65292;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#30340;&#26410;&#26469;&#29256;&#26412;&#23558;&#36890;&#36807;&#20154;&#24037;&#21019;&#24314;&#21644;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#30340;&#20869;&#23481;&#28151;&#21512;&#35757;&#32451;&#65292;&#24341;&#21457;&#28508;&#22312;&#30340;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#21644;&#20844;&#20849;&#25968;&#25454;&#20179;&#24211;&#20043;&#38388;&#30340;&#21453;&#39304;&#24490;&#29615;&#12290;&#36825;&#31181;&#20114;&#21160;&#24341;&#21457;&#20102;&#35768;&#22810;&#38382;&#39064;&#65306;&#26410;&#26469;&#29256;&#26412;&#30340;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#22312;&#28151;&#21512;&#30340;&#30495;&#23454;&#21644;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#30340;&#25968;&#25454;&#19978;&#35757;&#32451;&#26102;&#23558;&#22914;&#20309;&#34920;&#29616;&#65311;&#23427;&#20204;&#26159;&#21542;&#20250;&#38543;&#30528;&#26032;&#30340;&#25968;&#25454;&#38598;&#36827;&#21270;&#21644;&#25913;&#36827;&#36824;&#26159;&#30456;&#21453;&#21464;&#24471;&#26356;&#24046;&#65311;&#36827;&#21270;&#26159;&#21542;&#20250;&#24341;&#20837;&#20559;&#35823;&#25110;&#25910;&#32553;&#35270;&#37326;&#65311;
&lt;/p&gt;
&lt;p&gt;
The rapid adoption of generative Artificial Intelligence (AI) tools that can generate realistic images or text, such as DALL-E, MidJourney, or ChatGPT, have put the societal impacts of these technologies at the center of public debate. These tools are possible due to the massive amount of data (text and images) that is publicly available through the Internet. At the same time, these generative AI tools become content creators that are already contributing to the data that is available to train future models. Therefore, future versions of generative AI tools will be trained with a mix of human-created and AI-generated content, causing a potential feedback loop between generative AI and public data repositories. This interaction raises many questions: how will future versions of generative AI tools behave when trained on a mixture of real and AI generated data? Will they evolve and improve with the new data sets or on the contrary will they degrade? Will evolution introduce biases or red
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21367;&#31215;&#33258;&#32534;&#30721;&#22120;&#21644;K-means&#32858;&#31867;&#30340;&#26080;&#30417;&#30563;&#20998;&#31867;&#26041;&#27861;&#65292;&#21487;&#23558;&#30005;&#21147;&#31995;&#32479;&#20013;&#30340;&#24178;&#25200;&#27874;&#24418;&#32858;&#31867;&#20026;&#28041;&#21450;&#30005;&#21387;&#19979;&#38477;&#12289;&#20013;&#26029;&#12289;&#30636;&#24577;&#12289;&#27491;&#24120;&#21644;&#35856;&#27874;&#30072;&#21464;&#31561;&#31867;&#21035;&#65292;&#20174;&#32780;&#23454;&#29616;&#36807;&#28388;&#24322;&#24120;&#27874;&#24418;&#21644;&#24120;&#35268;&#27874;&#24418;&#12290;</title><link>http://arxiv.org/abs/2306.06124</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#21367;&#31215;&#33258;&#32534;&#30721;&#22120;&#30340;&#30005;&#21147;&#31995;&#32479;&#24178;&#25200;&#26080;&#30417;&#30563;&#32858;&#31867;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Unsupervised clustering of disturbances in power systems via deep convolutional autoencoders. (arXiv:2306.06124v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06124
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21367;&#31215;&#33258;&#32534;&#30721;&#22120;&#21644;K-means&#32858;&#31867;&#30340;&#26080;&#30417;&#30563;&#20998;&#31867;&#26041;&#27861;&#65292;&#21487;&#23558;&#30005;&#21147;&#31995;&#32479;&#20013;&#30340;&#24178;&#25200;&#27874;&#24418;&#32858;&#31867;&#20026;&#28041;&#21450;&#30005;&#21387;&#19979;&#38477;&#12289;&#20013;&#26029;&#12289;&#30636;&#24577;&#12289;&#27491;&#24120;&#21644;&#35856;&#27874;&#30072;&#21464;&#31561;&#31867;&#21035;&#65292;&#20174;&#32780;&#23454;&#29616;&#36807;&#28388;&#24322;&#24120;&#27874;&#24418;&#21644;&#24120;&#35268;&#27874;&#24418;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#30005;&#32593;&#26816;&#27979;&#21040;&#24322;&#24120;&#20107;&#20214;&#26102;&#65292;&#30005;&#21147;&#36136;&#37327;(PQ)&#20202;&#22120;&#20250;&#35760;&#24405;PQ&#20107;&#20214;&#12290;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#30340;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#24110;&#21161;&#20934;&#30830;&#20998;&#31867;&#35760;&#24405;&#30340;&#27874;&#24418;&#24182;&#24110;&#21161;&#30005;&#21147;&#31995;&#32479;&#24037;&#31243;&#24072;&#35786;&#26029;&#21644;&#32416;&#27491;&#38382;&#39064;&#30340;&#26681;&#26412;&#21407;&#22240;&#12290;&#28982;&#32780;&#65292;&#22312;&#30005;&#21147;&#31995;&#32479;&#24178;&#25200;&#26399;&#38388;&#25429;&#33719;&#30340;&#35768;&#22810;&#27874;&#24418;&#38656;&#35201;&#36827;&#34892;&#26631;&#35760;&#20197;&#36827;&#34892;&#30417;&#30563;&#23398;&#20064;&#65292;&#20351;&#24037;&#31243;&#24072;&#38656;&#35201;&#25163;&#21160;&#22788;&#29702;&#25110;&#26410;&#30475;&#21040;&#22823;&#37327;&#25968;&#25454;&#35760;&#24405;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#32534;&#30721;&#22120;&#21644;K-means&#32858;&#31867;&#30340;&#26080;&#30417;&#30563;&#25216;&#26415;&#65292;&#21487;&#23558;PQ&#20107;&#20214;&#32858;&#31867;&#20026;&#28041;&#21450;&#30005;&#21387;&#19979;&#38477;&#12289;&#20013;&#26029;&#12289;&#30636;&#24577;&#12289;&#27491;&#24120;&#21644;&#35856;&#27874;&#30072;&#21464;&#31561;&#31867;&#21035;&#65292;&#20174;&#32780;&#23454;&#29616;&#36807;&#28388;&#24322;&#24120;&#27874;&#24418;&#21644;&#24120;&#35268;&#27874;&#24418;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#22312;&#20998;&#24067;&#32593;&#26684;&#20013;&#35760;&#24405;&#30340;&#19977;&#30456;&#22330;&#33719;&#24471;&#30340;&#30005;&#21387;&#27874;&#24418;&#36827;&#34892;&#28436;&#31034;&#12290;&#39318;&#20808;&#65292;&#21367;&#31215;&#33258;&#32534;&#30721;&#22120;&#23558;&#36755;&#20837;&#20449;&#21495;&#21387;&#32553;&#20026;&#19968;&#32452;&#36739;&#20302;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Power quality (PQ) events are recorded by PQ meters whenever anomalous events are detected on the power grid. Using neural networks with machine learning can aid in accurately classifying the recorded waveforms and help power system engineers diagnose and rectify the root causes of problems. However, many of the waveforms captured during a disturbance in the power system need to be labeled for supervised learning, leaving a large number of data recordings for engineers to process manually or go unseen. This paper presents an autoencoder and K-means clustering-based unsupervised technique that can be used to cluster PQ events into categories like sag, interruption, transients, normal, and harmonic distortion to enable filtering of anomalous waveforms from recurring or normal waveforms. The method is demonstrated using three-phase, field-obtained voltage waveforms recorded in a distribution grid. First, a convolutional autoencoder compresses the input signals into a set of lower feature 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24635;&#32467;&#20102;&#23545;&#25239;&#24615;&#25915;&#20987;&#21644;&#38450;&#24481;&#22312;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#30740;&#31350;&#12290;&#21015;&#20986;&#20102;&#29616;&#26377;&#30340;&#19981;&#23433;&#20840;&#22240;&#32032;&#65292;&#24182;&#34920;&#26126;&#20102;&#26412;&#39046;&#22495;&#30340;&#26032;&#20852;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2306.06123</link><description>&lt;p&gt;
&#12298;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#21644;&#38450;&#24481;&#65306;&#35843;&#26597;&#25253;&#21578;&#12299;
&lt;/p&gt;
&lt;p&gt;
Adversarial Attacks and Defenses in Explainable Artificial Intelligence: A Survey. (arXiv:2306.06123v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06123
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24635;&#32467;&#20102;&#23545;&#25239;&#24615;&#25915;&#20987;&#21644;&#38450;&#24481;&#22312;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#30740;&#31350;&#12290;&#21015;&#20986;&#20102;&#29616;&#26377;&#30340;&#19981;&#23433;&#20840;&#22240;&#32032;&#65292;&#24182;&#34920;&#26126;&#20102;&#26412;&#39046;&#22495;&#30340;&#26032;&#20852;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#26041;&#27861;&#34987;&#25551;&#32472;&#20026;&#35843;&#35797;&#21644;&#20449;&#20219;&#32479;&#35745;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#27835;&#30103;&#26041;&#24335;&#65292;&#20197;&#21450;&#35299;&#37322;&#23427;&#20204;&#30340;&#39044;&#27979;&#12290;&#28982;&#32780;&#65292;&#23545;&#25239;&#26426;&#22120;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#31361;&#20986;&#20102;&#26368;&#26032;&#35299;&#37322;&#30340;&#23616;&#38480;&#24615;&#21644;&#28431;&#27934;&#65292;&#36825;&#20123;&#36827;&#23637;&#20196;&#20154;&#23545;&#20854;&#23433;&#20840;&#24615;&#21644;&#21487;&#20449;&#24230;&#20135;&#29983;&#36136;&#30097;&#12290;&#25805;&#32437;&#12289;&#27450;&#39575;&#25110;&#27927;&#30333;&#27169;&#22411;&#25512;&#29702;&#35777;&#25454;&#30340;&#21487;&#33021;&#24615;&#22312;&#39640;&#39118;&#38505;&#20915;&#31574;&#21644;&#30693;&#35782;&#21457;&#29616;&#20013;&#20135;&#29983;&#19981;&#21033;&#21518;&#26524;&#12290;&#26412;&#25991;&#24635;&#32467;&#20102;50&#22810;&#31687;&#35770;&#25991;&#30340;&#30740;&#31350;&#65292;&#27010;&#36848;&#20102;&#38024;&#23545;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35299;&#37322;&#30340;&#23545;&#25239;&#25915;&#20987;&#20197;&#21450;&#20844;&#24179;&#24230;&#37327;&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#22914;&#20309;&#38450;&#24481;&#25915;&#20987;&#24182;&#35774;&#35745;&#40065;&#26834;&#30340;&#35299;&#37322;&#26041;&#27861;&#12290;&#25105;&#20204;&#21015;&#20986;XAI&#20013;&#29616;&#26377;&#30340;&#19981;&#23433;&#20840;&#22240;&#32032;&#65292;&#24182;&#27010;&#36848;&#20102;&#23545;&#25239;&#24615;XAI&#65288;AdvXAI&#65289;&#30340;&#26032;&#20852;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Explainable artificial intelligence (XAI) methods are portrayed as a remedy for debugging and trusting statistical and deep learning models, as well as interpreting their predictions. However, recent advances in adversarial machine learning highlight the limitations and vulnerabilities of state-of-the-art explanations, putting their security and trustworthiness into question. The possibility of manipulating, fooling or fairwashing evidence of the model's reasoning has detrimental consequences when applied in high-stakes decision-making and knowledge discovery. This concise survey of over 50 papers summarizes research concerning adversarial attacks on explanations of machine learning models, as well as fairness metrics. We discuss how to defend against attacks and design robust interpretation methods. We contribute a list of existing insecurities in XAI and outline the emerging research directions in adversarial XAI (AdvXAI).
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;3D&#23039;&#21183;&#20272;&#35745;&#21644;&#24815;&#24615;&#36816;&#21160;&#25429;&#25417;&#31995;&#32479;&#22312;&#29305;&#23450;&#36816;&#21160;&#38203;&#28860;&#20013;&#30340;&#31934;&#24230;&#65292;&#20026;&#20020;&#24202;&#24212;&#29992;&#25552;&#20379;&#20102;&#26377;&#29992;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2306.06117</link><description>&lt;p&gt;
&#36816;&#21160;&#27835;&#30103;&#20013;3D&#23039;&#21183;&#20272;&#35745;&#21644;&#24815;&#24615;&#36816;&#21160;&#25429;&#25417;&#31995;&#32479;&#30340;&#20248;&#21183;&#21644;&#21155;&#21183;
&lt;/p&gt;
&lt;p&gt;
Strengths and Weaknesses of 3D Pose Estimation and Inertial Motion Capture System for Movement Therapy. (arXiv:2306.06117v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06117
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;3D&#23039;&#21183;&#20272;&#35745;&#21644;&#24815;&#24615;&#36816;&#21160;&#25429;&#25417;&#31995;&#32479;&#22312;&#29305;&#23450;&#36816;&#21160;&#38203;&#28860;&#20013;&#30340;&#31934;&#24230;&#65292;&#20026;&#20020;&#24202;&#24212;&#29992;&#25552;&#20379;&#20102;&#26377;&#29992;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
3D&#23039;&#21183;&#20272;&#35745;&#20026;&#24555;&#36895;&#12289;&#38750;&#20405;&#20837;&#24615;&#19988;&#20934;&#30830;&#30340;&#36816;&#21160;&#20998;&#26512;&#25552;&#20379;&#20102;&#26426;&#20250;&#12290;&#36825;&#23545;&#20110;&#20020;&#24202;&#24212;&#29992;&#20063;&#20855;&#26377;&#29305;&#27530;&#30340;&#20852;&#36259;&#12290;&#30446;&#21069;&#65292;&#36816;&#21160;&#25429;&#25417;&#31995;&#32479;&#34987;&#24212;&#29992;&#65292;&#22240;&#20026;&#23427;&#20204;&#25552;&#20379;&#20102;&#24378;&#22823;&#32780;&#31934;&#30830;&#30340;&#25968;&#25454;&#33719;&#21462;&#65292;&#22312;&#20020;&#24202;&#24212;&#29992;&#20013;&#26159;&#24517;&#19981;&#21487;&#23569;&#30340;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#26368;&#20808;&#36827;&#30340;3D&#20301;&#32622;&#20272;&#35745;&#26041;&#27861;MeTrabs&#19982;&#24050;&#24314;&#31435;&#30340;&#24815;&#24615;&#20256;&#24863;&#22120;&#31995;&#32479;MTw Awinda&#22312;&#29305;&#23450;&#36816;&#21160;&#38203;&#28860;&#20013;&#30340;&#31934;&#24230;&#12290;&#35813;&#30740;&#31350;&#20351;&#29992;&#24182;&#25552;&#20379;&#20102;10&#21517;&#21463;&#35797;&#32773;&#22312;&#21508;&#31181;&#36816;&#21160;&#27835;&#30103;&#38203;&#28860;&#26399;&#38388;&#30340;&#24179;&#34892;&#35760;&#24405;&#30340;&#35780;&#20272;&#25968;&#25454;&#38598;&#12290;Awinda&#31995;&#32479;&#30340;&#20449;&#24687;&#21644;&#21333;&#30524;&#23039;&#24577;&#20272;&#35745;&#30340;&#24103;&#34987;&#21516;&#27493;&#12290;&#20026;&#20102;&#36827;&#34892;&#27604;&#36739;&#65292;&#23545;&#36381;&#20851;&#33410;&#12289;&#33181;&#20851;&#33410;&#12289;&#32972;&#37096;&#21644;&#32920;&#37096;&#30340;&#20851;&#33410;&#35282;&#24230;&#36827;&#34892;&#20102;&#20020;&#24202;&#30456;&#20851;&#21442;&#25968;&#30340;&#20272;&#35745;&#21644;&#35780;&#20272;&#65292;&#20351;&#29992;&#24179;&#22343;&#20540;&#12289;&#20013;&#20301;&#25968;&#21644;&#35745;&#31639;&#20986;&#30340;&#19981;&#21516;&#38203;&#28860;&#20013;&#20851;&#33410;&#35282;&#24230;&#20043;&#38388;&#30340;&#26368;&#22823;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
3D pose estimation offers the opportunity for fast, non-invasive, and accurate motion analysis. This is of special interest also for clinical use. Currently, motion capture systems are used, as they offer robust and precise data acquisition, which is essential in the case of clinical applications. In this study, we investigate the accuracy of the state-of-the-art 3D position estimation approach MeTrabs, compared to the established inertial sensor system MTw Awinda for specific motion exercises. The study uses and provides an evaluation dataset of parallel recordings from 10 subjects during various movement therapy exercises. The information from the Awinda system and the frames for monocular pose estimation are synchronized. For the comparison, clinically relevant parameters for joint angles of ankle, knee, back, and elbow flexion-extension were estimated and evaluated using mean, median, and maximum deviation between the calculated joint angles for the different exercises, camera posi
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28431;&#27934;&#21305;&#37197;&#26041;&#27861;&#65292;&#21033;&#29992;&#23398;&#20064;&#30340;&#28431;&#27934;&#20195;&#30721;&#24211;&#20013;&#30340;&#28431;&#27934;&#27169;&#24335;&#23450;&#20301;&#26131;&#21463;&#25915;&#20987;&#35821;&#21477;&#65292;&#20197;&#35299;&#20915;&#20256;&#32479;&#28431;&#27934;&#26816;&#27979;&#26041;&#27861;&#20013;&#23384;&#22312;&#30340;&#35782;&#21035;&#28431;&#27934;&#33539;&#22260;&#19981;&#20934;&#30830;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.06109</link><description>&lt;p&gt;
&#23398;&#20064;&#37327;&#21270;&#28431;&#27934;&#27169;&#24335;&#24182;&#21305;&#37197;&#20197;&#23450;&#20301;&#35821;&#21477;&#32423;&#28431;&#27934;
&lt;/p&gt;
&lt;p&gt;
Learning to Quantize Vulnerability Patterns and Match to Locate Statement-Level Vulnerabilities. (arXiv:2306.06109v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06109
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28431;&#27934;&#21305;&#37197;&#26041;&#27861;&#65292;&#21033;&#29992;&#23398;&#20064;&#30340;&#28431;&#27934;&#20195;&#30721;&#24211;&#20013;&#30340;&#28431;&#27934;&#27169;&#24335;&#23450;&#20301;&#26131;&#21463;&#25915;&#20987;&#35821;&#21477;&#65292;&#20197;&#35299;&#20915;&#20256;&#32479;&#28431;&#27934;&#26816;&#27979;&#26041;&#27861;&#20013;&#23384;&#22312;&#30340;&#35782;&#21035;&#28431;&#27934;&#33539;&#22260;&#19981;&#20934;&#30830;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#35782;&#21035;&#36719;&#20214;&#28431;&#27934;&#20013;&#36234;&#26469;&#36234;&#27969;&#34892;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#19981;&#21516;&#26131;&#21463;&#25915;&#20987;&#30340;&#31243;&#24207;&#20013;&#30340;&#28431;&#27934;&#21487;&#33021;&#34920;&#29616;&#20986;&#31867;&#20284;&#30340;&#26131;&#21463;&#25915;&#20987;&#33539;&#22260;&#65292;&#36825;&#20123;&#33539;&#22260;&#24418;&#25104;&#21487;&#36890;&#36807;&#30417;&#30563;&#35757;&#32451;&#20026;DL&#27169;&#22411;&#25152;&#23398;&#30340;&#21487;&#36776;&#35748;&#30340;&#28431;&#27934;&#27169;&#24335;&#12290;&#28982;&#32780;&#65292;&#28431;&#27934;&#30340;&#26131;&#25915;&#20987;&#33539;&#22260;&#20173;&#20197;&#19981;&#21516;&#30340;&#31354;&#38388;&#20301;&#32622;&#21644;&#26684;&#24335;&#22312;&#31243;&#24207;&#20013;&#34920;&#29616;&#65292;&#36825;&#23548;&#33268;&#27169;&#22411;&#20934;&#30830;&#35782;&#21035;&#26131;&#21463;&#25915;&#20987;&#35821;&#21477;&#38754;&#20020;&#25361;&#25112;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#26368;&#20808;&#36827;&#30340;&#28431;&#27934;&#26816;&#27979;&#26041;&#27861;&#20173;&#26410;&#21033;&#29992;&#26131;&#21463;&#25915;&#20987;&#31243;&#24207;&#20013;&#20986;&#29616;&#30340;&#28431;&#27934;&#27169;&#24335;&#12290;&#20026;&#20805;&#20998;&#21033;&#29992;&#26131;&#21463;&#25915;&#20987;&#27169;&#24335;&#24182;&#37322;&#25918;DL&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28431;&#27934;&#21305;&#37197;&#26041;&#27861;&#65292;&#21463;&#31243;&#24207;&#20998;&#26512;&#24037;&#20855;&#30340;&#21551;&#21457;&#65292;&#26681;&#25454;&#39044;&#23450;&#20041;&#30340;&#27169;&#24335;&#26469;&#23450;&#20301;&#28431;&#27934;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23398;&#20064;&#20102;&#19968;&#20221;&#28431;&#27934;&#20195;&#30721;&#24211;&#65292;&#20854;&#20013;&#21253;&#25324;&#37327;&#21270;&#28431;&#27934;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning (DL) models have become increasingly popular in identifying software vulnerabilities. Prior studies found that vulnerabilities across different vulnerable programs may exhibit similar vulnerable scopes, implicitly forming discernible vulnerability patterns that can be learned by DL models through supervised training. However, vulnerable scopes still manifest in various spatial locations and formats within a program, posing challenges for models to accurately identify vulnerable statements. Despite this challenge, state-of-the-art vulnerability detection approaches fail to exploit the vulnerability patterns that arise in vulnerable programs. To take full advantage of vulnerability patterns and unleash the ability of DL models, we propose a novel vulnerability-matching approach in this paper, drawing inspiration from program analysis tools that locate vulnerabilities based on pre-defined patterns. Specifically, a vulnerability codebook is learned, which consists of quantize
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26631;&#20934;&#26041;&#27861;&#26469;&#35780;&#20272;&#20219;&#20309;&#27169;&#24577;&#30340;&#29983;&#25104;AI&#31995;&#32479;&#30340;&#31038;&#20250;&#24433;&#21709;&#65292;&#20998;&#20026;&#22522;&#30784;&#31995;&#32479;&#21644;&#31038;&#20250;&#26041;&#38754;&#30340;&#35780;&#20272;&#65292;&#28085;&#30422;7&#20010;&#31038;&#20250;&#24433;&#21709;&#31867;&#21035;&#65292;&#21253;&#25324;&#20559;&#35265;&#12289;&#38544;&#31169;&#20445;&#25252;&#12289;&#29615;&#22659;&#25104;&#26412;&#31561;&#12290;</title><link>http://arxiv.org/abs/2306.05949</link><description>&lt;p&gt;
&#35780;&#20272;&#29983;&#25104;AI&#31995;&#32479;&#22312;&#31995;&#32479;&#21644;&#31038;&#20250;&#20013;&#30340;&#31038;&#20250;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Evaluating the Social Impact of Generative AI Systems in Systems and Society. (arXiv:2306.05949v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05949
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26631;&#20934;&#26041;&#27861;&#26469;&#35780;&#20272;&#20219;&#20309;&#27169;&#24577;&#30340;&#29983;&#25104;AI&#31995;&#32479;&#30340;&#31038;&#20250;&#24433;&#21709;&#65292;&#20998;&#20026;&#22522;&#30784;&#31995;&#32479;&#21644;&#31038;&#20250;&#26041;&#38754;&#30340;&#35780;&#20272;&#65292;&#28085;&#30422;7&#20010;&#31038;&#20250;&#24433;&#21709;&#31867;&#21035;&#65292;&#21253;&#25324;&#20559;&#35265;&#12289;&#38544;&#31169;&#20445;&#25252;&#12289;&#29615;&#22659;&#25104;&#26412;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;AI&#31995;&#32479;&#36328;&#36234;&#25991;&#26412;&#12289;&#22270;&#20687;&#12289;&#38899;&#39057;&#12289;&#35270;&#39057;&#31561;&#22810;&#31181;&#27169;&#24577;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#31038;&#20250;&#24433;&#21709;&#65292;&#20294;&#30446;&#21069;&#19981;&#23384;&#22312;&#23448;&#26041;&#26631;&#20934;&#26469;&#35780;&#20272;&#36825;&#20123;&#24433;&#21709;&#21644;&#24212;&#35813;&#35780;&#20272;&#21738;&#20123;&#24433;&#21709;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26631;&#20934;&#26041;&#27861;&#26469;&#35780;&#20272;&#20219;&#20309;&#27169;&#24577;&#30340;&#29983;&#25104;AI&#31995;&#32479;&#65292;&#20998;&#20026;&#20004;&#22823;&#31867;&#21035;&#65306;&#23545;&#20110;&#27809;&#26377;&#39044;&#23450;&#24212;&#29992;&#30340;&#22522;&#30784;&#31995;&#32479;&#21487;&#20197;&#35780;&#20272;&#20160;&#20040;&#65292;&#20197;&#21450;&#21487;&#20197;&#22312;&#31038;&#20250;&#20013;&#35780;&#20272;&#20160;&#20040;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#20855;&#20307;&#30340;&#31038;&#20250;&#24433;&#21709;&#31867;&#21035;&#20197;&#21450;&#22914;&#20309;&#35780;&#20272;&#22522;&#30784;&#25216;&#26415;&#31995;&#32479;&#12289;&#20154;&#27665;&#21644;&#31038;&#20250;&#12290;&#25105;&#20204;&#30340;&#22522;&#30784;&#31995;&#32479;&#26694;&#26550;&#23450;&#20041;&#20102;&#19971;&#20010;&#31038;&#20250;&#24433;&#21709;&#31867;&#21035;&#65306;&#20559;&#35265;&#12289;&#21051;&#26495;&#21360;&#35937;&#21644;&#34920;&#29616;&#24615;&#20260;&#23475;&#65307;&#25991;&#21270;&#20215;&#20540;&#21644;&#25935;&#24863;&#20869;&#23481;&#65307;&#19981;&#23545;&#31561;&#30340;&#24615;&#33021;&#65307;&#38544;&#31169;&#21644;&#25968;&#25454;&#20445;&#25252;&#65307;&#36130;&#21153;&#25104;&#26412;&#65307;&#29615;&#22659;&#25104;&#26412;&#65307;&#20197;&#21450;&#25968;&#25454;&#21644;&#20869;&#23481;&#30417;&#31649;&#21171;&#21160;&#25104;&#26412;&#12290;&#24314;&#35758;&#30340;&#35780;&#20272;&#26041;&#27861;&#36866;&#29992;&#20110;&#25152;&#26377;&#27169;&#24577;&#21644;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative AI systems across modalities, ranging from text, image, audio, and video, have broad social impacts, but there exists no official standard for means of evaluating those impacts and which impacts should be evaluated. We move toward a standard approach in evaluating a generative AI system for any modality, in two overarching categories: what is able to be evaluated in a base system that has no predetermined application and what is able to be evaluated in society. We describe specific social impact categories and how to approach and conduct evaluations in the base technical system, then in people and society. Our framework for a base system defines seven categories of social impact: bias, stereotypes, and representational harms; cultural values and sensitive content; disparate performance; privacy and data protection; financial costs; environmental costs; and data and content moderation labor costs. Suggested methods for evaluation apply to all modalities and analyses of the li
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;INR&#30340;&#26102;&#38388;&#24207;&#21015;&#36830;&#32493;&#24314;&#27169;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#22788;&#29702;&#32570;&#22833;&#25968;&#25454;&#12289;&#19981;&#35268;&#21017;&#37319;&#26679;&#21644;&#22810;&#20256;&#24863;&#22120;&#19981;&#23545;&#20934;&#35266;&#27979;&#31561;&#37325;&#22797;&#24314;&#27169;&#38382;&#39064;&#65292;&#24182;&#22312;&#39044;&#27979;&#21644;&#25554;&#20540;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#20855;&#26377;&#24456;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.05880</link><description>&lt;p&gt;
&#22522;&#20110;Implicit Neural Representations&#30340;&#26102;&#38388;&#24207;&#21015;&#36830;&#32493;&#24314;&#27169;&#29992;&#20110;&#25554;&#20540;&#21644;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Time Series Continuous Modeling for Imputation and Forecasting with Implicit Neural Representations. (arXiv:2306.05880v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05880
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;INR&#30340;&#26102;&#38388;&#24207;&#21015;&#36830;&#32493;&#24314;&#27169;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#22788;&#29702;&#32570;&#22833;&#25968;&#25454;&#12289;&#19981;&#35268;&#21017;&#37319;&#26679;&#21644;&#22810;&#20256;&#24863;&#22120;&#19981;&#23545;&#20934;&#35266;&#27979;&#31561;&#37325;&#22797;&#24314;&#27169;&#38382;&#39064;&#65292;&#24182;&#22312;&#39044;&#27979;&#21644;&#25554;&#20540;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#20855;&#26377;&#24456;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;&#24050;&#34987;&#24191;&#27867;&#25506;&#32034;&#65292;&#20294;&#22312;&#38754;&#23545;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#26102;&#20173;&#38754;&#20020;&#37325;&#22823;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24314;&#27169;&#26041;&#27861;&#65292;&#21033;&#29992;Implicit Neural Representations (INR)&#12290;&#35813;&#26041;&#27861;&#20351;&#25105;&#20204;&#33021;&#22815;&#26377;&#25928;&#22320;&#25429;&#25417;&#26102;&#38388;&#24207;&#21015;&#30340;&#36830;&#32493;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#33258;&#28982;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#22788;&#29702;&#32570;&#22833;&#25968;&#25454;&#12289;&#22788;&#29702;&#19981;&#35268;&#21017;&#37319;&#26679;&#25110;&#26469;&#33258;&#22810;&#20010;&#20256;&#24863;&#22120;&#30340;&#19981;&#23545;&#20934;&#35266;&#27979;&#31561;&#37325;&#22797;&#24314;&#27169;&#38382;&#39064;&#12290;&#36890;&#36807;&#24341;&#20837;&#26465;&#20214;&#35843;&#21046;INR&#21442;&#25968;&#24182;&#21033;&#29992;&#20803;&#23398;&#20064;&#25216;&#26415;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#27169;&#22411;&#27867;&#21270;&#21040;&#26410;&#35265;&#26679;&#26412;&#21644;&#26102;&#38388;&#31383;&#21475;&#31227;&#20301;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#23637;&#31034;&#20102;&#22312;&#39044;&#27979;&#21644;&#25554;&#20540;&#20219;&#21153;&#20013;&#39046;&#20808;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#22312;&#22788;&#29702;&#35768;&#22810;&#31454;&#20105;&#27169;&#22411;&#26080;&#27861;&#22788;&#29702;&#30340;&#21508;&#31181;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22330;&#26223;&#26041;&#38754;&#23637;&#29616;&#20102;&#28789;&#27963;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although widely explored, time series modeling continues to encounter significant challenges when confronted with real-world data. We propose a novel modeling approach leveraging Implicit Neural Representations (INR). This approach enables us to effectively capture the continuous aspect of time series and provides a natural solution to recurring modeling issues such as handling missing data, dealing with irregular sampling, or unaligned observations from multiple sensors. By introducing conditional modulation of INR parameters and leveraging meta-learning techniques, we address the issue of generalization to both unseen samples and time window shifts. Through extensive experimentation, our model demonstrates state-of-the-art performance in forecasting and imputation tasks, while exhibiting flexibility in handling a wide range of challenging scenarios that competing models cannot.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24212;&#29992;&#20110;&#25512;&#33616;&#31995;&#32479;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35843;&#26597;&#30740;&#31350;&#65292;&#20174;&#20004;&#20010;&#35282;&#24230;&#24635;&#32467;&#20102;&#29616;&#26377;&#30340;&#30740;&#31350;&#24037;&#20316;&#65306;&#22914;&#20309;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#35843;&#25972;LLM&#21644;&#35843;&#25972;LLM&#26102;&#22312;&#21738;&#37324;&#35843;&#25972;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20123;&#28508;&#22312;&#30340;&#30740;&#31350;&#26041;&#21521;&#21644;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2306.05817</link><description>&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#22914;&#20309;&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#21463;&#30410;&#65306;&#19968;&#39033;&#35843;&#26597;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
How Can Recommender Systems Benefit from Large Language Models: A Survey. (arXiv:2306.05817v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05817
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24212;&#29992;&#20110;&#25512;&#33616;&#31995;&#32479;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35843;&#26597;&#30740;&#31350;&#65292;&#20174;&#20004;&#20010;&#35282;&#24230;&#24635;&#32467;&#20102;&#29616;&#26377;&#30340;&#30740;&#31350;&#24037;&#20316;&#65306;&#22914;&#20309;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#35843;&#25972;LLM&#21644;&#35843;&#25972;LLM&#26102;&#22312;&#21738;&#37324;&#35843;&#25972;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20123;&#28508;&#22312;&#30340;&#30740;&#31350;&#26041;&#21521;&#21644;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#22312;&#21305;&#37197;&#20114;&#32852;&#32593;&#24212;&#29992;&#31243;&#24207;&#29992;&#25143;&#30340;&#20449;&#24687;&#38656;&#27714;&#26041;&#38754;&#21457;&#25381;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#20013;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#23637;&#29616;&#20986;&#20102;&#24778;&#20154;&#30340;&#26032;&#20852;&#33021;&#21147;&#65288;&#20363;&#22914;&#25351;&#20196;&#36319;&#36394;&#12289;&#25512;&#29702;&#65289;&#65292;&#20174;&#32780;&#20026;&#23558;LLM&#35843;&#25972;&#21040;&#25512;&#33616;&#31995;&#32479;&#20013;&#20197;&#25552;&#39640;&#24615;&#33021;&#21644;&#25913;&#21892;&#29992;&#25143;&#20307;&#39564;&#30340;&#30740;&#31350;&#26041;&#21521;&#24102;&#26469;&#20102;&#24076;&#26395;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20174;&#24212;&#29992;&#23548;&#21521;&#30340;&#35282;&#24230;&#23545;&#27492;&#30740;&#31350;&#26041;&#21521;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35843;&#26597;&#12290;&#25105;&#20204;&#39318;&#20808;&#20174;&#20004;&#20010;&#27491;&#20132;&#30340;&#35282;&#24230;&#24635;&#32467;&#20102;&#29616;&#26377;&#30340;&#30740;&#31350;&#24037;&#20316;&#65306;&#22914;&#20309;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#35843;&#25972;LLM&#21644;&#35843;&#25972;LLM&#26102;&#22312;&#21738;&#37324;&#35843;&#25972;&#12290;&#23545;&#20110;&#8220;&#22312;&#21738;&#37324;&#8221;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;LLM&#22312;&#25512;&#33616;&#27969;&#31243;&#30340;&#19981;&#21516;&#38454;&#27573;&#20013;&#21487;&#33021;&#21457;&#25381;&#30340;&#20316;&#29992;&#65292;&#21363;&#29305;&#24449;&#24037;&#31243;&#12289;&#29305;&#24449;&#32534;&#30721;&#22120;&#12289;&#35780;&#20998;/&#25490;&#21517;&#20989;&#25968;&#21644;&#27969;&#31243;&#25511;&#21046;&#22120;&#12290;&#23545;&#20110;&#8220;&#22914;&#20309;&#8221;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#35757;&#32451;&#21644;&#25512;&#29702;&#31574;&#30053;&#65292;&#20174;&#32780;&#24471;&#20986;&#20004;&#20010;&#32454;&#31890;&#24230;&#30340;&#20998;&#31867;&#26631;&#20934;&#65292;&#21363;&#26159;&#21542;&#35843;&#25972;LLM&#21644;&#26159;&#21542;&#23558;LLM&#20316;&#20026;&#29420;&#31435;&#27169;&#22411;&#25110;&#28151;&#21512;&#27169;&#22411;&#32452;&#20214;&#20351;&#29992;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22312;&#23558;LLM&#35843;&#25972;&#21040;RS&#20013;&#30340;&#19968;&#20123;&#25361;&#25112;&#21644;&#28508;&#22312;&#26041;&#21521;&#65292;&#21253;&#25324;&#19982;&#29616;&#26377;&#31995;&#32479;&#30340;&#38598;&#25104;&#12289;&#29992;&#25143;&#21453;&#39304;&#12289;&#35780;&#20272;&#24230;&#37327;&#21644;&#30693;&#35782;&#33976;&#39311;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recommender systems (RS) play important roles to match users' information needs for Internet applications. In natural language processing (NLP) domains, large language model (LLM) has shown astonishing emergent abilities (e.g., instruction following, reasoning), thus giving rise to the promising research direction of adapting LLM to RS for performance enhancements and user experience improvements. In this paper, we conduct a comprehensive survey on this research direction from an application-oriented view. We first summarize existing research works from two orthogonal perspectives: where and how to adapt LLM to RS. For the "WHERE" question, we discuss the roles that LLM could play in different stages of the recommendation pipeline, i.e., feature engineering, feature encoder, scoring/ranking function, and pipeline controller. For the "HOW" question, we investigate the training and inference strategies, resulting in two fine-grained taxonomy criteria, i.e., whether to tune LLMs or not, a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#23545;&#22810;&#23380;&#26448;&#26009;&#20013;&#30340;&#36870;&#27969;&#33258;&#21457;&#28183;&#36879;&#36807;&#31243;&#36827;&#34892;&#20102;&#26089;&#26399;&#21644;&#26202;&#26399;&#30340;&#27169;&#25311;&#21644;&#39044;&#27979;&#65292;&#24182;&#20351;&#29992;&#25913;&#21464;&#21464;&#37327;&#25216;&#26415;&#26469;&#25913;&#36827;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.05554</link><description>&lt;p&gt;
&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#22312;&#36870;&#27969;&#33258;&#21457;&#28183;&#36879;&#20013;&#30340;&#24212;&#29992;&#21644;&#39044;&#27979;&#65306;&#26089;&#26399;&#21644;&#26202;&#26399;&#30340;&#27169;&#25311;
&lt;/p&gt;
&lt;p&gt;
Simulation and Prediction of Countercurrent Spontaneous Imbibition at Early and Late Times Using Physics-Informed Neural Networks. (arXiv:2306.05554v1 [physics.comp-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05554
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#23545;&#22810;&#23380;&#26448;&#26009;&#20013;&#30340;&#36870;&#27969;&#33258;&#21457;&#28183;&#36879;&#36807;&#31243;&#36827;&#34892;&#20102;&#26089;&#26399;&#21644;&#26202;&#26399;&#30340;&#27169;&#25311;&#21644;&#39044;&#27979;&#65292;&#24182;&#20351;&#29992;&#25913;&#21464;&#21464;&#37327;&#25216;&#26415;&#26469;&#25913;&#36827;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36870;&#27969;&#33258;&#21457;&#28183;&#36879;&#65288;COUCSI&#65289;&#26159;&#19968;&#31181;&#22810;&#23380;&#26448;&#26009;&#20013;&#30340;&#36807;&#31243;&#65292;&#20854;&#20013;&#28070;&#28287;&#30456;&#21462;&#20195;&#20102;&#38750;&#28070;&#28287;&#30456;&#30340;&#20301;&#32622;&#12290;&#26412;&#25991;&#39318;&#27425;&#25506;&#35752;&#20102;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#22312;&#35299;&#20915;&#26089;&#26399;&#65288;ET&#65289;&#21644;&#26202;&#26399;&#65288;LT&#65289;COUCSI&#38382;&#39064;&#20013;&#30340;&#24212;&#29992;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#25913;&#21464;&#21464;&#37327;&#25216;&#26415;&#20197;&#25913;&#36827;PINNs&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#36890;&#36807;&#25913;&#21464;&#33258;&#21464;&#37327;&#23558;COUCSI&#38382;&#39064;&#20998;&#21035;&#29992;XT-&#65292;XY-&#21644;Z-&#19977;&#31181;&#31561;&#25928;&#24418;&#24335;&#36827;&#34892;&#25551;&#36848;&#65306;&#31532;&#19968;&#20010;&#25551;&#36848;&#20102;&#39281;&#21644;&#24230;&#20316;&#20026;&#35268;&#33539;&#21270;&#20301;&#32622;X&#21644;&#26102;&#38388;T&#30340;&#20989;&#25968;;&#31532;&#20108;&#20010;&#25551;&#36848;&#20102;X&#21644;Y=T^0.5&#20316;&#20026;&#20989;&#25968;&#30340;&#39281;&#21644;&#24230;;&#31532;&#19977;&#20010;&#20316;&#20026;Z=X/T^0.5&#30340;&#21807;&#19968;&#20989;&#25968;&#65288;&#20165;&#22312;ET&#19979;&#26377;&#25928;&#65289;&#12290;&#35813;PINN&#27169;&#22411;&#20351;&#29992;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#29983;&#25104;&#65292;&#24182;&#22522;&#20110;&#26368;&#23567;&#21270;&#21152;&#26435;&#25439;&#22833;&#20989;&#25968;&#36827;&#34892;&#35757;&#32451;&#65292;&#21253;&#25324;&#29289;&#29702;&#20449;&#24687;&#20002;&#22833;&#39033;&#21644;&#19982;&#21021;&#22987;&#36793;&#30028;&#26465;&#20214;&#30456;&#23545;&#24212;&#30340;&#39033;&#12290;&#27809;&#26377;&#21512;&#25104;&#25110;&#23454;&#39564;&#25968;&#25454;&#34987;&#35843;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Countercurrent spontaneous imbibition (COUCSI) is a process in porous materials in which a wetting phase displaces non-wetting phase. In this work, we investigate for the first time the application of Physics-Informed Neural Networks (PINNs) in solving the 1D COUCSI problem in both early (ET) and late (LT) times. Also novel, we examine the Change-of-Variables technique for improving the performance of PINNs. We formulated the COUCSI problem in three equivalent forms by changing the independent variables: XT-, XY-, and Z-formulations. The first describes saturation as function of normalized position X and time T; the second as function of X and Y=T^0.5; and the third as a sole function of Z=X/T^0.5 (valid only at ET). The PINN model was generated using a feed-forward neural network and trained based on minimizing a weighted loss function, including the physics-informed loss term and terms corresponding to the initial and boundary conditions. No synthetical or experimental data were invo
&lt;/p&gt;</description></item><item><title>CoCo&#26159;&#19968;&#31181;&#32806;&#21512;&#23545;&#27604;&#22270;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#20854;&#20013;&#21253;&#21547;&#19968;&#20010;&#22270;&#21367;&#31215;&#32593;&#32476;&#21644;&#19968;&#20010;&#20998;&#23618;&#22270;&#20869;&#26680;&#32593;&#32476;&#65292;&#36890;&#36807;&#32806;&#21512;&#23545;&#27604;&#23398;&#20064;&#20943;&#23569;&#39046;&#22495;&#24046;&#24322;&#65292;&#29992;&#20110;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#22270;&#20998;&#31867;&#12290;</title><link>http://arxiv.org/abs/2306.04979</link><description>&lt;p&gt;
CoCo: &#19968;&#31181;&#29992;&#20110;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#22270;&#20998;&#31867;&#30340;&#32806;&#21512;&#23545;&#27604;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
CoCo: A Coupled Contrastive Framework for Unsupervised Domain Adaptive Graph Classification. (arXiv:2306.04979v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04979
&lt;/p&gt;
&lt;p&gt;
CoCo&#26159;&#19968;&#31181;&#32806;&#21512;&#23545;&#27604;&#22270;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#20854;&#20013;&#21253;&#21547;&#19968;&#20010;&#22270;&#21367;&#31215;&#32593;&#32476;&#21644;&#19968;&#20010;&#20998;&#23618;&#22270;&#20869;&#26680;&#32593;&#32476;&#65292;&#36890;&#36807;&#32806;&#21512;&#23545;&#27604;&#23398;&#20064;&#20943;&#23569;&#39046;&#22495;&#24046;&#24322;&#65292;&#29992;&#20110;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#22270;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#22270;&#20998;&#31867;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#26524;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#29305;&#23450;&#20219;&#21153;&#30340;&#26631;&#31614;&#65292;&#36825;&#21487;&#33021;&#38656;&#35201;&#26497;&#22823;&#30340;&#20195;&#20215;&#26469;&#33719;&#24471;&#12290;&#19968;&#31181;&#21487;&#38752;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#25506;&#32034;&#20854;&#20182;&#26631;&#27880;&#22270;&#20197;&#22686;&#24378;&#30446;&#26631;&#22495;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#65292;&#20294;&#22914;&#20309;&#23558;&#22270;&#31070;&#32463;&#32593;&#32476;&#24212;&#29992;&#21040;&#39046;&#22495;&#36866;&#24212;&#20013;&#20173;&#26410;&#35299;&#20915;&#65292;&#22240;&#20026;&#23545;&#22270;&#25299;&#25169;&#30340;&#19981;&#20805;&#20998;&#25506;&#32034;&#20197;&#21450;&#30456;&#24403;&#22823;&#30340;&#39046;&#22495;&#20559;&#24046;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;CoCo&#65288;Coupled Contrastive Graph Representation Learning&#65289;&#26041;&#26696;&#65292;&#35813;&#26041;&#26696;&#20174;&#32806;&#21512;&#23398;&#20064;&#20998;&#25903;&#20013;&#25552;&#21462;&#25299;&#25169;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#32806;&#21512;&#23545;&#27604;&#23398;&#20064;&#20943;&#23569;&#39046;&#22495;&#24046;&#24322;&#12290;CoCo&#21253;&#21547;&#19968;&#20010;&#22270;&#21367;&#31215;&#32593;&#32476;&#20998;&#25903;&#21644;&#20998;&#23618;&#22270;&#20869;&#26680;&#32593;&#32476;&#20998;&#25903;&#65292;&#20998;&#21035;&#29992;&#38544;&#24335;&#21644;&#26174;&#24335;&#26041;&#24335;&#25506;&#32034;&#22270;&#25299;&#25169;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#32806;&#21512;&#20998;&#25903;&#32467;&#21512;&#21040;&#19968;&#20010;&#20840;&#38754;&#30340;&#22810;&#35270;&#35282;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#20013;&#65292;
&lt;/p&gt;
&lt;p&gt;
Although graph neural networks (GNNs) have achieved impressive achievements in graph classification, they often need abundant task-specific labels, which could be extensively costly to acquire. A credible solution is to explore additional labeled graphs to enhance unsupervised learning on the target domain. However, how to apply GNNs to domain adaptation remains unsolved owing to the insufficient exploration of graph topology and the significant domain discrepancy. In this paper, we propose \underline{Co}upled \underline{Co}ntrastive Graph Representation Learning (\method{}), which extracts the topological information from coupled learning branches and reduces the domain discrepancy with coupled contrastive learning. \method{} contains a graph convolutional network branch and a hierarchical graph kernel network branch, which explore graph topology in implicit and explicit manners. Besides, we incorporate coupled branches into a holistic multi-view contrastive learning framework, which 
&lt;/p&gt;</description></item><item><title>INSTRUCTEVAL&#26159;&#19968;&#20010;&#19987;&#27880;&#20110;&#25351;&#23548;&#35843;&#25972;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#30340;&#32508;&#21512;&#22871;&#20214;&#65292;&#23427;&#37319;&#21462;&#20102;&#20840;&#38754;&#30340;&#26041;&#27861;&#26469;&#35780;&#20272;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;&#35299;&#20915;&#38382;&#39064;&#12289;&#20889;&#20316;&#33021;&#21147;&#21644;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#30340;&#19968;&#33268;&#24615;&#31561;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2306.04757</link><description>&lt;p&gt;
INSTRUCTEVAL&#65306;&#38754;&#21521;&#25351;&#23548;&#35843;&#25972;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25972;&#20307;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
INSTRUCTEVAL: Towards Holistic Evaluation of Instruction-Tuned Large Language Models. (arXiv:2306.04757v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04757
&lt;/p&gt;
&lt;p&gt;
INSTRUCTEVAL&#26159;&#19968;&#20010;&#19987;&#27880;&#20110;&#25351;&#23548;&#35843;&#25972;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#30340;&#32508;&#21512;&#22871;&#20214;&#65292;&#23427;&#37319;&#21462;&#20102;&#20840;&#38754;&#30340;&#26041;&#27861;&#26469;&#35780;&#20272;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;&#35299;&#20915;&#38382;&#39064;&#12289;&#20889;&#20316;&#33021;&#21147;&#21644;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#30340;&#19968;&#33268;&#24615;&#31561;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25351;&#23548;&#35843;&#25972;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#20174;&#26681;&#26412;&#19978;&#25913;&#21464;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65292;&#24050;&#32463;&#22312;&#35832;&#22914;&#23545;&#35805;&#20195;&#29702;&#31561;&#24212;&#29992;&#20013;&#26174;&#31034;&#20986;&#20102;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;&#36825;&#20123;&#27169;&#22411;&#65292;&#22914;GPT-4&#65292;&#19981;&#20165;&#33021;&#22815;&#25484;&#25569;&#35821;&#35328;&#65292;&#32780;&#19988;&#21487;&#20197;&#35299;&#20915;&#25968;&#23398;&#12289;&#32534;&#30721;&#12289;&#21307;&#23398;&#21644;&#27861;&#24459;&#31561;&#39046;&#22495;&#30340;&#22797;&#26434;&#20219;&#21153;&#12290;&#23613;&#31649;&#23427;&#20204;&#20855;&#26377;&#21331;&#36234;&#30340;&#33021;&#21147;&#65292;&#20294;&#30001;&#20110;&#35768;&#22810;&#27169;&#22411;&#30340;&#40657;&#30418;&#24615;&#36136;&#21644;&#32570;&#20047;&#20840;&#38754;&#30340;&#35780;&#20272;&#30740;&#31350;&#65292;&#23545;&#23427;&#20204;&#30340;&#20840;&#37096;&#28508;&#21147;&#20173;&#28982;&#32570;&#20047;&#20840;&#38754;&#30340;&#29702;&#35299;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;INSTRUCTEVAL&#65292;&#19968;&#20010;&#26356;&#20840;&#38754;&#30340;&#35780;&#20272;&#22871;&#20214;&#65292;&#19987;&#38376;&#38024;&#23545;&#25351;&#23548;&#35843;&#25972;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#19982;&#20197;&#24448;&#30340;&#20316;&#21697;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#35780;&#20272;&#21253;&#25324;&#23545;&#27169;&#22411;&#22522;&#20110;&#35299;&#20915;&#38382;&#39064;&#12289;&#20889;&#20316;&#33021;&#21147;&#21644;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#30340;&#19968;&#33268;&#24615;&#30340;&#20005;&#26684;&#35780;&#20272;&#12290;&#25105;&#20204;&#37319;&#21462;&#20102;&#20840;&#38754;&#30340;&#26041;&#27861;&#26469;&#20998;&#26512;&#24433;&#21709;&#27169;&#22411;&#24615;&#33021;&#30340;&#21508;&#31181;&#22240;&#32032;&#65292;&#21253;&#25324;&#39044;&#35757;&#32451;&#22522;&#30784;&#12289;&#25351;&#23548;&#35843;&#25972;&#25968;&#25454;&#21644;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Instruction-tuned large language models have revolutionized natural language processing and have shown great potential in applications such as conversational agents. These models, such as GPT-4, can not only master language but also solve complex tasks in areas like mathematics, coding, medicine, and law. Despite their impressive capabilities, there is still a lack of comprehensive understanding regarding their full potential, primarily due to the black-box nature of many models and the absence of holistic evaluation studies. To address these challenges, we present INSTRUCTEVAL, a more comprehensive evaluation suite designed specifically for instruction-tuned large language models. Unlike previous works, our evaluation involves a rigorous assessment of models based on problem-solving, writing ability, and alignment to human values. We take a holistic approach to analyze various factors affecting model performance, including the pretraining foundation, instruction-tuning data, and train
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;AI&#29983;&#25104;&#22270;&#20687;&#36136;&#37327;&#35780;&#20272;&#30340;&#24320;&#25918;&#25968;&#25454;&#24211;AGIQA-3K&#65292;&#24182;&#22312;&#20854;&#20013;&#36827;&#34892;&#20102;&#22522;&#20934;&#23454;&#39564;&#65292;&#25552;&#20986;&#20102;StairReward&#20197;&#25552;&#39640;&#20027;&#35266;&#25991;&#26412;&#21040;&#22270;&#20687;&#23545;&#40784;&#35780;&#20272;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.04717</link><description>&lt;p&gt;
AGIQA-3K&#65306;&#19968;&#20221;&#29992;&#20110;AI&#29983;&#25104;&#22270;&#20687;&#36136;&#37327;&#35780;&#20272;&#30340;&#24320;&#25918;&#25968;&#25454;&#24211;
&lt;/p&gt;
&lt;p&gt;
AGIQA-3K: An Open Database for AI-Generated Image Quality Assessment. (arXiv:2306.04717v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04717
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;AI&#29983;&#25104;&#22270;&#20687;&#36136;&#37327;&#35780;&#20272;&#30340;&#24320;&#25918;&#25968;&#25454;&#24211;AGIQA-3K&#65292;&#24182;&#22312;&#20854;&#20013;&#36827;&#34892;&#20102;&#22522;&#20934;&#23454;&#39564;&#65292;&#25552;&#20986;&#20102;StairReward&#20197;&#25552;&#39640;&#20027;&#35266;&#25991;&#26412;&#21040;&#22270;&#20687;&#23545;&#40784;&#35780;&#20272;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#30340;&#22270;&#20687;&#65288;AGIs&#65289;&#24050;&#24191;&#27867;&#24212;&#29992;&#20110;&#23089;&#20048;&#12289;&#25945;&#32946;&#12289;&#31038;&#20132;&#23186;&#20307;&#31561;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#32771;&#34385;&#21040;&#19981;&#21516;AGIs&#20043;&#38388;&#30340;&#22823;&#37327;&#36136;&#37327;&#24046;&#24322;&#65292;&#36843;&#20999;&#38656;&#35201;&#19982;&#20154;&#31867;&#20027;&#35266;&#35780;&#20998;&#19968;&#33268;&#30340;&#36136;&#37327;&#27169;&#22411;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24191;&#27867;&#32771;&#34385;&#20102;&#21508;&#31181;&#27969;&#34892;&#30340;AGI&#27169;&#22411;&#12289;&#19981;&#21516;&#25552;&#31034;&#21644;&#27169;&#22411;&#21442;&#25968;&#29983;&#25104;&#30340;AGI&#65292;&#24182;&#25910;&#38598;&#20102;&#24863;&#30693;&#36136;&#37327;&#21644;&#25991;&#26412;&#21040;&#22270;&#20687;&#23545;&#40784;&#26041;&#38754;&#30340;&#20027;&#35266;&#35780;&#20998;&#65292;&#20174;&#32780;&#24314;&#31435;&#20102;&#36804;&#20170;&#20026;&#27490;&#26368;&#20026;&#20840;&#38754;&#30340;AGI&#20027;&#35266;&#36136;&#37327;&#25968;&#25454;&#24211;AGIQA-3K&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;&#36825;&#20010;&#25968;&#25454;&#24211;&#19978;&#36827;&#34892;&#20102;&#22522;&#20934;&#23454;&#39564;&#65292;&#35780;&#20272;&#20102;&#24403;&#21069;&#22270;&#20687;&#36136;&#37327;&#35780;&#20272;&#65288;IQA&#65289;&#27169;&#22411;&#19982;&#20154;&#31867;&#24863;&#30693;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;StairReward&#65292;&#22823;&#22823;&#25552;&#39640;&#20102;&#20027;&#35266;&#25991;&#26412;&#21040;&#22270;&#20687;&#23545;&#40784;&#35780;&#20272;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30456;&#20449;&#65292;AGIQA-3K&#20013;&#30340;&#32454;&#31890;&#24230;&#20027;&#35266;&#35780;&#20998;&#23558;&#26377;&#21161;&#20110;&#25512;&#21160;AI&#29983;&#25104;&#22270;&#20687;&#36136;&#37327;&#35780;&#20272;&#25216;&#26415;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the rapid advancements of the text-to-image generative model, AI-generated images (AGIs) have been widely applied to entertainment, education, social media, etc. However, considering the large quality variance among different AGIs, there is an urgent need for quality models that are consistent with human subjective ratings. To address this issue, we extensively consider various popular AGI models, generated AGI through different prompts and model parameters, and collected subjective scores at the perceptual quality and text-to-image alignment, thus building the most comprehensive AGI subjective quality database AGIQA-3K so far. Furthermore, we conduct a benchmark experiment on this database to evaluate the consistency between the current Image Quality Assessment (IQA) model and human perception, while proposing StairReward that significantly improves the assessment performance of subjective text-to-image alignment. We believe that the fine-grained subjective scores in AGIQA-3K wil
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28151;&#21512;Actor-Critic&#24378;&#21270;&#23398;&#20064;&#30340;&#33258;&#36866;&#24212;&#39057;&#29575;&#32511;&#28783;&#26368;&#20339;&#36895;&#24230;&#24314;&#35758;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#31163;&#25955;&#21644;&#36830;&#32493;&#28436;&#21592;&#32593;&#32476;&#26469;&#20248;&#21270;&#32511;&#28783;&#26368;&#20339;&#36895;&#24230;&#24314;&#35758;&#31995;&#32479;&#30340;&#39057;&#29575;&#21644;&#21152;&#36895;&#24230;&#26354;&#32447;&#65292;&#35774;&#35745;&#20102;&#26032;&#30340;&#22870;&#21169;&#20989;&#25968;&#26469;&#24179;&#34913;&#20943;&#23569;&#20572;&#36710;&#21644;&#26368;&#23567;&#21270;&#36895;&#24230;&#21464;&#21270;&#30340;&#26435;&#34913;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#26377;&#25928;&#22320;&#20943;&#23569;&#26053;&#34892;&#26102;&#38388;&#21644;&#29123;&#26009;&#28040;&#32791;&#65292;&#24182;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#30340;GLOSA&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2306.04660</link><description>&lt;p&gt;
&#22522;&#20110;&#28151;&#21512;Actor-Critic&#24378;&#21270;&#23398;&#20064;&#30340;&#33258;&#36866;&#24212;&#39057;&#29575;&#32511;&#28783;&#26368;&#20339;&#36895;&#24230;&#24314;&#35758;
&lt;/p&gt;
&lt;p&gt;
Adaptive Frequency Green Light Optimal Speed Advisory based on Hybrid Actor-Critic Reinforcement Learning. (arXiv:2306.04660v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04660
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28151;&#21512;Actor-Critic&#24378;&#21270;&#23398;&#20064;&#30340;&#33258;&#36866;&#24212;&#39057;&#29575;&#32511;&#28783;&#26368;&#20339;&#36895;&#24230;&#24314;&#35758;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#31163;&#25955;&#21644;&#36830;&#32493;&#28436;&#21592;&#32593;&#32476;&#26469;&#20248;&#21270;&#32511;&#28783;&#26368;&#20339;&#36895;&#24230;&#24314;&#35758;&#31995;&#32479;&#30340;&#39057;&#29575;&#21644;&#21152;&#36895;&#24230;&#26354;&#32447;&#65292;&#35774;&#35745;&#20102;&#26032;&#30340;&#22870;&#21169;&#20989;&#25968;&#26469;&#24179;&#34913;&#20943;&#23569;&#20572;&#36710;&#21644;&#26368;&#23567;&#21270;&#36895;&#24230;&#21464;&#21270;&#30340;&#26435;&#34913;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#26377;&#25928;&#22320;&#20943;&#23569;&#26053;&#34892;&#26102;&#38388;&#21644;&#29123;&#26009;&#28040;&#32791;&#65292;&#24182;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#30340;GLOSA&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32511;&#28783;&#26368;&#20339;&#36895;&#24230;&#24314;&#35758;&#65288;GLOSA&#65289;&#31995;&#32479;&#24314;&#35758;&#36710;&#36742;&#36895;&#24230;&#65292;&#20197;&#24110;&#21161;&#23427;&#20204;&#22312;&#32511;&#33394;&#26102;&#38388;&#36890;&#36807;&#36335;&#21475;&#65292;&#20174;&#32780;&#36890;&#36807;&#26368;&#23567;&#21270;&#22312;&#36335;&#21475;&#20572;&#36710;&#21644;&#24608;&#36895;&#26102;&#38388;&#26469;&#20943;&#23569;&#20132;&#36890;&#25317;&#22581;&#21644;&#29123;&#26009;&#28040;&#32791;&#12290;&#20294;&#26159;&#65292;&#20197;&#21069;&#30340;&#30740;&#31350;&#38598;&#20013;&#20110;&#20248;&#21270;GLOSA&#31639;&#27861;&#65292;&#24573;&#30053;&#20102;GLOSA&#31995;&#32479;&#30340;&#36895;&#24230;&#24314;&#35758;&#39057;&#29575;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#19968;&#20123;&#30740;&#31350;&#22312;&#27599;&#20010;&#20915;&#31574;&#27493;&#39588;&#25552;&#20379;&#36895;&#24230;&#24314;&#35758;&#65292;&#23548;&#33268;&#20887;&#20313;&#24314;&#35758;&#65292;&#32780;&#20854;&#20182;&#20154;&#20165;&#20026;&#36710;&#36742;&#35745;&#31639;&#26368;&#20339;&#36895;&#24230;&#65292;&#26080;&#27861;&#36866;&#24212;&#21160;&#24577;&#20132;&#36890;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28151;&#21512;PPO&#65288;H-PPO&#65289;&#30340;&#33258;&#36866;&#24212;&#39057;&#29575;GLOSA&#65288;AF-GLOSA&#65289;&#27169;&#22411;&#65292;&#20854;&#37319;&#29992;&#20102;&#19968;&#20010;actor-critic&#26550;&#26500;&#21644;&#19968;&#20010;&#28151;&#21512;actor&#32593;&#32476;&#12290;&#28151;&#21512;&#28436;&#21592;&#32593;&#32476;&#21253;&#25324;&#19968;&#20010;&#31163;&#25955;&#28436;&#21592;&#65292;&#36755;&#20986;&#21672;&#35810;&#39057;&#29575;&#21644;&#19968;&#20010;&#36830;&#32493;&#28436;&#21592;&#65292;&#36755;&#20986;&#21152;&#36895;&#24230;&#26354;&#32447;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#22870;&#21169;&#20989;&#25968;&#26469;&#24179;&#34913;&#20943;&#23569;&#20572;&#36710;&#21644;&#26368;&#23567;&#21270;&#36895;&#24230;&#21464;&#21270;&#30340;&#26435;&#34913;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;AF-GLOSA&#27169;&#22411;&#21487;&#20197;&#26377;&#25928;&#22320;&#20943;&#23569;&#26053;&#34892;&#26102;&#38388;&#21644;&#29123;&#26009;&#28040;&#32791;&#65292;&#24182;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#30340;GLOSA&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
Green Light Optimal Speed Advisory (GLOSA) system suggests speeds to vehicles to assist them in passing through intersections during green intervals, thus reducing traffic congestion and fuel consumption by minimizing the number of stops and idle times at intersections. However, previous research has focused on optimizing the GLOSA algorithm, neglecting the frequency of speed advisory by the GLOSA system. Specifically, some studies provide speed advisory profile at each decision step, resulting in redundant advisory, while others calculate the optimal speed for the vehicle only once, which cannot adapt to dynamic traffic. In this paper, we propose an Adaptive Frequency GLOSA (AF-GLOSA) model based on Hybrid Proximal Policy Optimization (H-PPO), which employs an actor-critic architecture with a hybrid actor network. The hybrid actor network consists of a discrete actor that outputs advisory frequency and a continuous actor that outputs acceleration profiles. Additionally, we design a no
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#36873;&#39033;&#25913;&#36827;&#27169;&#20223;&#23398;&#20064;&#23545;&#25239;&#24615;&#31034;&#33539;&#30340;&#24615;&#33021;&#34920;&#29616;&#30340;&#26032;&#25216;&#26415;&#65292;&#21487;&#20197;&#35782;&#21035;&#26410;&#34987;&#23545;&#25163;&#26174;&#30528;&#20462;&#25913;&#30340;&#28436;&#31034;&#36712;&#36857;&#24182;&#20165;&#20174;&#20013;&#36827;&#34892;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2306.04581</link><description>&lt;p&gt;
&#21033;&#29992;&#36873;&#39033;&#25913;&#36827;&#27169;&#20223;&#23398;&#20064;&#23545;&#25239;&#24615;&#31034;&#33539;&#30340;&#24615;&#33021;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;
Divide and Repair: Using Options to Improve Performance of Imitation Learning Against Adversarial Demonstrations. (arXiv:2306.04581v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04581
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#36873;&#39033;&#25913;&#36827;&#27169;&#20223;&#23398;&#20064;&#23545;&#25239;&#24615;&#31034;&#33539;&#30340;&#24615;&#33021;&#34920;&#29616;&#30340;&#26032;&#25216;&#26415;&#65292;&#21487;&#20197;&#35782;&#21035;&#26410;&#34987;&#23545;&#25163;&#26174;&#30528;&#20462;&#25913;&#30340;&#28436;&#31034;&#36712;&#36857;&#24182;&#20165;&#20174;&#20013;&#36827;&#34892;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#20174;&#25945;&#24072;&#25110;&#19987;&#23478;&#30340;&#28436;&#31034;&#20013;&#23398;&#20064;&#25191;&#34892;&#20219;&#21153;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25216;&#26415;&#65292;&#21487;&#20197;&#35782;&#21035;&#26410;&#34987;&#23545;&#25163;&#26174;&#30528;&#20462;&#25913;&#30340;&#28436;&#31034;&#36712;&#36857;&#30340;&#37096;&#20998;&#65292;&#24182;&#20351;&#29992;&#26102;&#38388;&#19978;&#25193;&#23637;&#30340;&#31574;&#30053;&#25110;&#36873;&#39033;&#36827;&#34892;&#23398;&#20064;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#31181;&#22522;&#20110;&#28436;&#31034;&#36712;&#36857;&#30340;&#31354;&#38388;&#21644;&#26102;&#38388;&#29305;&#24449;&#30340;&#36712;&#36857;&#20998;&#27495;&#24230;&#37327;&#65292;&#20197;&#26816;&#27979;&#21644;&#20002;&#24323;&#24050;&#34987;&#23545;&#25163;&#26174;&#30528;&#20462;&#25913;&#30340;&#36712;&#36857;&#37096;&#20998;&#65292;&#24182;&#21487;&#33021;&#38477;&#20302;&#23398;&#20064;&#32773;&#30340;&#24615;&#33021;&#65292;&#22914;&#26524;&#29992;&#20110;&#23398;&#20064;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;&#36873;&#39033;&#30340;&#31639;&#27861;&#26469;&#20998;&#21106;&#36712;&#36857;&#65292;&#24182;&#21482;&#20174;&#24050;&#30830;&#23450;&#20026;&#21487;&#25509;&#21463;&#30340;&#36712;&#36857;&#37096;&#20998;&#20013;&#36827;&#34892;&#23398;&#20064;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#25105;&#20204;&#25216;&#26415;&#30340;&#29702;&#35770;&#32467;&#26524;&#65292;&#20197;&#34920;&#26126;&#20462;&#22797;&#37096;&#20998;&#36712;&#36857;&#21487;&#20197;&#25913;&#21892;&#23398;&#20064;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of learning to perform a task from demonstrations given by teachers or experts, when some of the experts' demonstrations might be adversarial and demonstrate an incorrect way to perform the task. We propose a novel technique that can identify parts of demonstrated trajectories that have not been significantly modified by the adversary and utilize them for learning, using temporally extended policies or options. We first define a trajectory divergence measure based on the spatial and temporal features of demonstrated trajectories to detect and discard parts of the trajectories that have been significantly modified by an adversarial expert, and, could degrade the learner's performance, if used for learning, We then use an options-based algorithm that partitions trajectories and learns only from the parts of trajectories that have been determined as admissible. We provide theoretical results of our technique to show that repairing partial trajectories improves the 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25506;&#31350;&#20102;&#20351;&#29992;&#31934;&#31616;&#31574;&#30053;&#32593;&#32476;&#20316;&#20026;&#33258;&#25105;&#27169;&#22411;&#30340;&#20248;&#32570;&#28857;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#32463;&#36807;&#31934;&#31616;&#30340;&#31574;&#30053;&#32593;&#32476;&#20316;&#20026;&#33258;&#25105;&#27169;&#22411;&#21487;&#20197;&#31283;&#23450;&#35757;&#32451;&#65292;&#24182;&#19988;&#26377;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2306.04440</link><description>&lt;p&gt;
&#20197;&#21452;&#31574;&#30053;&#20026;&#33258;&#27169;&#22411;&#30340;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Dual policy as self-model for planning. (arXiv:2306.04440v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04440
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25506;&#31350;&#20102;&#20351;&#29992;&#31934;&#31616;&#31574;&#30053;&#32593;&#32476;&#20316;&#20026;&#33258;&#25105;&#27169;&#22411;&#30340;&#20248;&#32570;&#28857;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#32463;&#36807;&#31934;&#31616;&#30340;&#31574;&#30053;&#32593;&#32476;&#20316;&#20026;&#33258;&#25105;&#27169;&#22411;&#21487;&#20197;&#31283;&#23450;&#35757;&#32451;&#65292;&#24182;&#19988;&#26377;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35268;&#21010;&#26159;&#19968;&#31181;&#25968;&#25454;&#26377;&#25928;&#30340;&#20915;&#31574;&#31574;&#30053;&#65292;&#20195;&#29702;&#36890;&#36807;&#25506;&#32034;&#21487;&#33021;&#30340;&#26410;&#26469;&#29366;&#24577;&#26469;&#36873;&#25321;&#20505;&#36873;&#21160;&#20316;&#12290;&#24403;&#23384;&#22312;&#39640;&#32500;&#34892;&#21160;&#31354;&#38388;&#26102;&#65292;&#20026;&#20102;&#27169;&#25311;&#26410;&#26469;&#29366;&#24577;&#65292;&#24517;&#39035;&#20351;&#29992;&#33258;&#24049;&#30340;&#20915;&#31574;&#31574;&#30053;&#26469;&#38480;&#21046;&#25152;&#38656;&#25506;&#32034;&#30340;&#21160;&#20316;&#25968;&#37327;&#12290;&#25105;&#20204;&#23558;&#29992;&#20110;&#27169;&#25311;&#33258;&#24049;&#20915;&#31574;&#30340;&#27169;&#22411;&#31216;&#20026;&#20195;&#29702;&#30340;&#33258;&#25105;&#27169;&#22411;&#12290;&#23613;&#31649;&#22312;&#35268;&#21010;&#34892;&#21160;&#26102;&#65292;&#19990;&#30028;&#27169;&#22411;&#36890;&#24120;&#19982;&#33258;&#25105;&#27169;&#22411;&#19968;&#36215;&#38544;&#21547;&#22320;&#20351;&#29992;&#65292;&#20294;&#22914;&#20309;&#35774;&#35745;&#33258;&#25105;&#27169;&#22411;&#20173;&#19981;&#28165;&#26970;&#12290;&#21463;&#24403;&#21069;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#21644;&#31070;&#32463;&#31185;&#23398;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#20351;&#29992;&#31934;&#31616;&#31574;&#30053;&#32593;&#32476;&#20316;&#20026;&#33258;&#25105;&#27169;&#22411;&#30340;&#20248;&#32570;&#28857;&#12290;&#22312;&#36825;&#26679;&#30340;&#21452;&#31574;&#30053;&#20195;&#29702;&#20013;&#65292;&#19968;&#20010;&#26080;&#27169;&#22411;&#31574;&#30053;&#21644;&#19968;&#20010;&#32463;&#36807;&#31934;&#31616;&#30340;&#31574;&#30053;&#20998;&#21035;&#29992;&#20110;&#26080;&#27169;&#22411;&#21160;&#20316;&#21644;&#35745;&#21010;&#21160;&#20316;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#29983;&#24577;&#30456;&#20851;&#30340;&#21442;&#25968;&#29615;&#22659;&#19978;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#32463;&#36807;&#31934;&#31616;&#30340;&#31574;&#30053;&#32593;&#32476;&#20316;&#20026;&#33258;&#25105;&#27169;&#22411;&#21487;&#20197;&#31283;&#23450;&#35757;&#32451;&#65292;&#24182;&#19988;&#26377;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Planning is a data efficient decision-making strategy where an agent selects candidate actions by exploring possible future states. To simulate future states when there is a high-dimensional action space, the knowledge of one's decision making strategy must be used to limit the number of actions to be explored. We refer to the model used to simulate one's decisions as the agent's self-model. While self-models are implicitly used widely in conjunction with world models to plan actions, it remains unclear how self-models should be designed. Inspired by current reinforcement learning approaches and neuroscience, we explore the benefits and limitations of using a distilled policy network as the self-model. In such dual-policy agents, a model-free policy and a distilled policy are used for model-free actions and planned actions, respectively. Our results on a ecologically relevant, parametric environment indicate that distilled policy network for self-model stabilizes training, has faster i
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#19982;&#20154;&#31867;&#24418;&#24577;&#26426;&#22120;&#20154;&#30340;&#28216;&#25103;&#21270;&#20250;&#35805;&#65292;&#21487;&#20197;&#25552;&#39640;&#20799;&#31461;&#21742;&#21912;&#24739;&#32773;&#30340;&#27835;&#30103;&#20381;&#20174;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.04422</link><description>&lt;p&gt;
&#31038;&#20132;&#26426;&#22120;&#20154;&#22312;&#20799;&#31461;&#21742;&#21912;&#27835;&#30103;&#20381;&#20174;&#24615;&#26041;&#38754;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Social robots to improve therapeutic adherence in pediatric asthma. (arXiv:2306.04422v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04422
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#19982;&#20154;&#31867;&#24418;&#24577;&#26426;&#22120;&#20154;&#30340;&#28216;&#25103;&#21270;&#20250;&#35805;&#65292;&#21487;&#20197;&#25552;&#39640;&#20799;&#31461;&#21742;&#21912;&#24739;&#32773;&#30340;&#27835;&#30103;&#20381;&#20174;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24930;&#24615;&#30142;&#30149;&#20013;&#65292;&#27491;&#30830;&#35786;&#26029;&#21644;&#25552;&#20379;&#26368;&#36866;&#24403;&#30340;&#27835;&#30103;&#36890;&#24120;&#24182;&#19981;&#36275;&#20197;&#20445;&#35777;&#24739;&#32773;&#20020;&#24202;&#29366;&#20917;&#30340;&#25913;&#21892;&#12290;&#20302;&#27835;&#30103;&#20381;&#20174;&#24615;&#26159;&#38459;&#30861;&#36798;&#21040;&#27835;&#30103;&#30446;&#26631;&#30340;&#20027;&#35201;&#21407;&#22240;&#20043;&#19968;&#65292;&#23588;&#20854;&#26159;&#22312;&#26576;&#20123;&#30142;&#30149;&#21644;&#29305;&#23450;&#30446;&#26631;&#24739;&#32773;&#65288;&#22914;&#20799;&#31461;&#65289;&#20013;&#23588;&#20026;&#24120;&#35265;&#12290;&#19968;&#31181;&#26377;&#36259;&#30340;&#25216;&#26415;&#21487;&#20197;&#29992;&#20110;&#25903;&#25345;&#20020;&#24202;&#23454;&#36341;&#65292;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#20581;&#24247;&#32467;&#26524;&#12290;&#25105;&#20204;&#20551;&#35774;&#19982;&#20256;&#32479;&#30340;&#27835;&#30103;&#25945;&#32946;&#26041;&#27861;&#30456;&#27604;&#65292;&#19982;&#20154;&#31867;&#24418;&#24577;&#26426;&#22120;&#20154;&#30340;&#28216;&#25103;&#21270;&#20250;&#35805;&#21487;&#20197;&#26356;&#21152;&#26377;&#25928;&#22320;&#25945;&#25480;&#20799;&#31461;&#21742;&#21912;&#24739;&#32773;&#27491;&#30830;&#30340;&#21560;&#20837;&#26041;&#27861;&#12290;&#22312;&#36825;&#20010;&#35282;&#24230;&#19978;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#22312;Pepper&#26426;&#22120;&#20154;&#24179;&#21488;&#19978;&#23454;&#29616;&#30340;&#20132;&#20114;&#24335;&#27169;&#22359;&#20197;&#21450;&#35745;&#21010;&#20110;2020&#24180;&#22312;Palermo&#30340;CNR&#32954;&#37096;&#36807;&#25935;&#20818;&#31461;&#35786;&#25152;&#24320;&#23637;&#30340;&#19968;&#39033;&#30740;&#31350;&#12290;&#30001;&#20110;COVID-19&#32039;&#24613;&#24773;&#20917;&#30340;&#21407;&#22240;&#65292;&#36825;&#39033;&#30740;&#31350;&#34987;&#21462;&#28040;&#65292;&#20294;&#25105;&#20204;&#25552;&#20379;&#21021;&#27493;&#32467;&#26524;&#65292;&#20197;&#35780;&#20272;&#26041;&#27861;&#30340;&#21487;&#34892;&#24615;&#21644;&#21516;&#40836;&#20154;&#30340;&#20852;&#36259;&#12290;&#36890;&#36807;&#20998;&#26512;&#22521;&#35757;&#21069;&#21518;&#30340;&#21560;&#20837;&#20064;&#24815;&#30340;&#27604;&#36739;&#21644;&#32473;&#21442;&#19982;&#32773;&#30340;&#38382;&#21367;&#35843;&#26597;&#32467;&#26524;&#26469;&#35780;&#20272;&#25945;&#32946;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In chronic diseases, obtaining a correct diagnosis and providing the most appropriate treatments often is not enough to guarantee an improvement of the clinical condition of a patient. Poor adherence to medical prescriptions constitutes one of the main causes preventing achievement of therapeutic goals. This is generally true especially for certain diseases and specific target patients, such as children. An engaging and entertaining technology can be exploited in support of clinical practices to achieve better health outcomes. Our assumption is that a gamified session with a humanoid robot, compared to the usual methodologies for therapeutic education, can be more incisive in learning the correct inhalation procedure in children affected by asthma. In this perspective, we describe an interactive module implemented on the Pepper robotic platform and the setting of a study that was planned in 2020 to be held at the Pneumoallergology Pediatric clinic of CNR in Palermo. The study was cance
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#23545;&#35805;&#21709;&#24212;&#36873;&#25321;&#30340;&#21518;&#35757;&#32451;&#25216;&#26415;Dial-MAE&#65292;&#21033;&#29992;&#29983;&#25104;&#26041;&#27861;&#26356;&#22909;&#22320;&#21387;&#32553;&#23545;&#35805;&#35821;&#20041;&#33267;&#23494;&#38598;&#21521;&#37327;&#65292;&#24182;&#25552;&#39640;&#23545;&#35805;&#21709;&#24212;&#36873;&#25321;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.04357</link><description>&lt;p&gt;
&#29992;&#20110;&#22522;&#20110;&#26816;&#32034;&#30340;&#23545;&#35805;&#31995;&#32479;&#30340;&#19978;&#19979;&#25991;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
ConTextual Masked Auto-Encoder for Retrieval-based Dialogue Systems. (arXiv:2306.04357v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04357
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#23545;&#35805;&#21709;&#24212;&#36873;&#25321;&#30340;&#21518;&#35757;&#32451;&#25216;&#26415;Dial-MAE&#65292;&#21033;&#29992;&#29983;&#25104;&#26041;&#27861;&#26356;&#22909;&#22320;&#21387;&#32553;&#23545;&#35805;&#35821;&#20041;&#33267;&#23494;&#38598;&#21521;&#37327;&#65292;&#24182;&#25552;&#39640;&#23545;&#35805;&#21709;&#24212;&#36873;&#25321;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#21709;&#24212;&#36873;&#25321;&#26088;&#22312;&#26681;&#25454;&#32473;&#23450;&#30340;&#29992;&#25143;&#21644;&#31995;&#32479;&#35805;&#35821;&#21382;&#21490;&#35760;&#24405;&#20174;&#20960;&#20010;&#20505;&#36873;&#21709;&#24212;&#20013;&#36873;&#25321;&#36866;&#24403;&#30340;&#21709;&#24212;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#36890;&#36807;&#21518;&#35757;&#32451;&#22823;&#22810;&#20381;&#36182;&#20110;&#21333;&#32431;&#30340;&#25513;&#30721;&#35821;&#35328;&#24314;&#27169;&#26041;&#27861;&#26469;&#25552;&#39640;&#23545;&#35805;&#21709;&#24212;&#36873;&#25321;&#30340;&#20934;&#30830;&#24615;&#12290;&#20294;&#26159;&#65292;&#26368;&#36817;&#24320;&#21457;&#30340;&#29983;&#25104;&#26041;&#27861;&#22312;IR&#31038;&#21306;&#23637;&#31034;&#20102;&#26377;&#24076;&#26395;&#30340;&#25991;&#26412;&#34920;&#31034;&#33021;&#21147;&#65292;&#36825;&#21487;&#33021;&#20250;&#23548;&#33268;&#26356;&#22909;&#30340;&#23545;&#35805;&#35821;&#20041;&#24314;&#27169;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986; Dial-MAE&#65288;&#23545;&#35805;&#19978;&#19979;&#25991;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#38024;&#23545;&#23545;&#35805;&#21709;&#24212;&#36873;&#25321;&#30340;&#21518;&#35757;&#32451;&#25216;&#26415;&#12290; Dial-MAE&#20351;&#29992;&#19968;&#20010;&#19981;&#23545;&#31216;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#65292;&#23398;&#20064;&#23558;&#23545;&#35805;&#30340;&#35821;&#20041;&#26356;&#22909;&#22320;&#21387;&#32553;&#21040;&#23494;&#38598;&#21521;&#37327;&#20013;&#12290; Dial-MAE&#30340;&#36807;&#31243;&#21253;&#25324;&#30001;&#28145;&#24230;&#32534;&#30721;&#22120;&#21019;&#24314;&#24102;&#26377;&#25513;&#30721;&#23545;&#35805;&#19978;&#19979;&#25991;&#30340;&#23545;&#35805;&#23884;&#20837;&#65292;&#28982;&#21518;&#26159;&#27973;&#35299;&#30721;&#22120;&#65292;&#35813;&#35299;&#30721;&#22120;&#20351;&#29992;&#27492;&#23884;&#20837;&#20197;&#21450;&#19978;&#19979;&#25991;&#21521;&#37327;&#26469;&#29983;&#25104;&#21709;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dialogue response selection aims to select an appropriate response from several candidates based on a given user and system utterance history. Recent studies have been improving the accuracy of dialogue response selection through post-training, mostly relying on naive masked language modeling methods. However, the recently developed generative methods have shown promising text representation capabilities in IR community, which could potentially lead to better dialogue semantics modeling. Thus, in this paper, we propose Dial-MAE (Dialogue Contextual Masking Auto-encoder), a straightforward yet effective post-training technique tailored for dialogue response selection. Dial-MAE uses an asymmetric encoder-decoder architecture that learns to better compress the semantics of the dialogue into dialogue-dense vectors. The process of Dial-MAE involves a deep encoder creating a dialogue embedding with the masked dialogue context, followed by a shallow decoder that uses this embedding along with
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20013;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;PLAYBEST&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#20174;&#31726;&#29699;&#27604;&#36187;&#21382;&#21490;&#25968;&#25454;&#20013;&#23398;&#20064;&#31574;&#30053;&#65292;&#29983;&#25104;&#26356;&#21152;&#30495;&#23454;&#21644;&#22810;&#26679;&#21270;&#30340;&#29699;&#21592;&#34892;&#20026;&#65292;&#20174;&#32780;&#25552;&#39640;&#29699;&#21592;&#30340;&#20915;&#31574;&#21046;&#23450;&#25928;&#26524;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2306.04090</link><description>&lt;p&gt;
&#36890;&#36807;&#25193;&#25955;&#35268;&#21010;&#36827;&#34892;&#32844;&#19994;&#31726;&#29699;&#36816;&#21160;&#21592;&#34892;&#20026;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
Professional Basketball Player Behavior Synthesis via Planning with Diffusion. (arXiv:2306.04090v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04090
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20013;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;PLAYBEST&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#20174;&#31726;&#29699;&#27604;&#36187;&#21382;&#21490;&#25968;&#25454;&#20013;&#23398;&#20064;&#31574;&#30053;&#65292;&#29983;&#25104;&#26356;&#21152;&#30495;&#23454;&#21644;&#22810;&#26679;&#21270;&#30340;&#29699;&#21592;&#34892;&#20026;&#65292;&#20174;&#32780;&#25552;&#39640;&#29699;&#21592;&#30340;&#20915;&#31574;&#21046;&#23450;&#25928;&#26524;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20013;&#21160;&#24577;&#35268;&#21010;&#24050;&#32463;&#34987;&#24212;&#29992;&#20110;&#25913;&#21892;&#21508;&#31181;&#39046;&#22495;&#30340;&#20915;&#31574;&#21046;&#23450;&#12290;&#32844;&#19994;&#31726;&#29699;&#20316;&#20026;&#19968;&#20010;&#21253;&#21547;&#38544;&#34109;&#24615;&#25112;&#30053;&#31574;&#30053;&#21644;&#20915;&#31574;&#21046;&#23450;&#30340;&#21160;&#24577;&#26102;&#31354;&#21338;&#24328;&#30340;&#24341;&#20154;&#27880;&#30446;&#30340;&#20363;&#23376;&#12290;&#28982;&#32780;&#65292;&#22788;&#29702;&#22810;&#26679;&#30340;&#22330;&#19978;&#20449;&#21495;&#21644;&#23548;&#33322;&#28508;&#22312;&#21160;&#20316;&#21644;&#32467;&#26524;&#30340;&#24191;&#38420;&#31354;&#38388;&#20351;&#24471;&#29616;&#26377;&#26041;&#27861;&#24456;&#38590;&#36805;&#36895;&#35782;&#21035;&#21709;&#24212;&#19981;&#26029;&#21464;&#21270;&#30340;&#24773;&#20917;&#19979;&#30340;&#26368;&#20339;&#31574;&#30053;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#23558;&#24207;&#21015;&#20915;&#31574;&#21046;&#23450;&#36807;&#31243;&#23450;&#20041;&#20026;&#26465;&#20214;&#36712;&#36857;&#29983;&#25104;&#36807;&#31243;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#24341;&#20837;PLAYBEST&#65288;PLAYER BEhavior SynThesis&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#25552;&#39640;&#29699;&#21592;&#20915;&#31574;&#21046;&#23450;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#25193;&#23637;&#20102;&#26368;&#20808;&#36827;&#30340;&#29983;&#25104;&#27169;&#22411;&#8212;&#8212;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65292;&#20197;&#20174;&#21382;&#21490;&#30340;&#32654;&#22269;&#32844;&#19994;&#31726;&#29699;&#32852;&#36187;(NBA)&#29699;&#21592;&#36816;&#21160;&#36319;&#36394;&#25968;&#25454;&#20013;&#23398;&#20064;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22810;&#26234;&#33021;&#20307;&#29615;&#22659;&#21160;&#24577;&#12290;&#20026;&#20102;&#34701;&#21512;&#25968;&#25454;&#39537;&#21160;&#30340;&#31574;&#30053;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#36741;&#21161;&#21464;&#37327;&#21040;PLAYBEST&#20013;&#65292;&#20197;&#36866;&#24212;&#22806;&#37096;&#36755;&#20837;&#24182;&#29983;&#25104;&#30495;&#23454;&#21644;&#22810;&#26679;&#21270;&#30340;&#29699;&#21592;&#36712;&#36857;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;PLAYBEST&#21487;&#20197;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#29699;&#21592;&#34892;&#20026;&#65292;&#24182;&#22312;&#21508;&#31181;&#35780;&#20272;&#22330;&#26223;&#20013;&#20248;&#20110;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dynamically planning in multi-agent systems has been explored to improve decision-making in various domains. Professional basketball serves as a compelling example of a dynamic spatio-temporal game, encompassing both concealed strategic policies and decision-making. However, processing the diverse on-court signals and navigating the vast space of potential actions and outcomes makes it difficult for existing approaches to swiftly identify optimal strategies in response to evolving circumstances. In this study, we first formulate the sequential decision-making process as a conditional trajectory generation process. We further introduce PLAYBEST (PLAYer BEhavior SynThesis), a method for enhancing player decision-making. We extend the state-of-the-art generative model, diffusion probabilistic model, to learn challenging multi-agent environmental dynamics from historical National Basketball Association (NBA) player motion tracking data. To incorporate data-driven strategies, an auxiliary v
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#24378;&#21270;&#23398;&#20064;&#30340;&#20013;&#20171;&#27169;&#22411;&#65292;&#21487;&#23454;&#29616;&#20195;&#29702;&#19982;LLM&#20043;&#38388;&#39640;&#25928;&#32463;&#27982;&#26377;&#25928;&#30340;&#20114;&#21160;&#65292;&#25552;&#39640;&#25928;&#29575;&#21644;&#25104;&#26412;&#25928;&#30410;&#12290;</title><link>http://arxiv.org/abs/2306.03604</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#20419;&#36827;&#31639;&#27861;&#20195;&#29702;&#19982;LLM&#20043;&#38388;&#30340;&#39640;&#25928;&#20114;&#21160;
&lt;/p&gt;
&lt;p&gt;
Enabling Efficient Interaction between an Algorithm Agent and an LLM: A Reinforcement Learning Approach. (arXiv:2306.03604v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03604
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#24378;&#21270;&#23398;&#20064;&#30340;&#20013;&#20171;&#27169;&#22411;&#65292;&#21487;&#23454;&#29616;&#20195;&#29702;&#19982;LLM&#20043;&#38388;&#39640;&#25928;&#32463;&#27982;&#26377;&#25928;&#30340;&#20114;&#21160;&#65292;&#25552;&#39640;&#25928;&#29575;&#21644;&#25104;&#26412;&#25928;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#21253;&#21547;&#20174;&#28023;&#37327;&#25991;&#26412;&#25968;&#25454;&#38598;&#20013;&#33719;&#21462;&#30340;&#22823;&#37327;&#19990;&#30028;&#30693;&#35782;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;LLMs&#21487;&#20197;&#36890;&#36807;&#25552;&#20379;&#39640;&#23618;&#25351;&#20196;&#26469;&#21327;&#21161;&#31639;&#27861;&#20195;&#29702;&#35299;&#20915;&#20855;&#26377;&#22797;&#26434;&#39034;&#24207;&#20915;&#31574;&#30340;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#19982;LLMs&#36827;&#34892;&#20132;&#20114;&#21487;&#33021;&#32791;&#26102;&#36739;&#38271;&#65292;&#22240;&#20026;&#22312;&#35768;&#22810;&#23454;&#38469;&#24773;&#20917;&#19979;&#65292;&#23427;&#20204;&#38656;&#35201;&#22823;&#37327;&#23384;&#20648;&#31354;&#38388;&#65292;&#21482;&#33021;&#37096;&#32626;&#22312;&#36828;&#31243;&#20113;&#26381;&#21153;&#22120;&#33410;&#28857;&#19978;&#12290;&#27492;&#22806;&#65292;&#20351;&#29992;&#21830;&#19994;LLMs&#21487;&#33021;&#25104;&#26412;&#24456;&#39640;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#33021;&#26681;&#25454;&#20351;&#29992;&#39057;&#29575;&#25910;&#36153;&#12290;&#26412;&#25991;&#25506;&#35752;&#22914;&#20309;&#23454;&#29616;&#20195;&#29702;&#19982;LLM&#20043;&#38388;&#30340;&#39640;&#25928;&#21644;&#32463;&#27982;&#26377;&#25928;&#30340;&#20114;&#21160;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#20013;&#20171;&#27169;&#22411;&#65292;&#20197;&#30830;&#23450;&#20309;&#26102;&#38656;&#35201;&#26597;&#35810;LLMs&#20197;&#23436;&#25104;&#30446;&#26631;&#20219;&#21153;&#30340;&#39640;&#32423;&#25351;&#20196;&#12290;&#22312;&#28041;&#21450;&#35268;&#21010;&#23376;&#30446;&#26631;&#30340;4&#20010;MiniGrid&#29615;&#22659;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#23398;&#20064;&#35299;&#20915;&#30446;&#26631;&#20219;&#21153;&#65292;&#24182;&#25552;&#21319;&#20102;&#25928;&#29575;&#21644;&#25104;&#26412;&#25928;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) encode a vast amount of world knowledge acquired from massive text datasets. Recent studies have demonstrated that LLMs can assist an algorithm agent in solving complex sequential decision making tasks in embodied environments by providing high-level instructions. However, interacting with LLMs can be time-consuming, as in many practical scenarios, they require a significant amount of storage space that can only be deployed on remote cloud server nodes. Additionally, using commercial LLMs can be costly since they may charge based on usage frequency. In this paper, we explore how to enable efficient and cost-effective interactions between the agent and an LLM. We propose a reinforcement learning based mediator model that determines when it is necessary to consult LLMs for high-level instructions to accomplish a target task. Experiments on 4 MiniGrid environments that entail planning sub-goals demonstrate that our method can learn to solve target tasks with o
&lt;/p&gt;</description></item><item><title>SDR-GAIN&#26159;&#19968;&#31181;&#29992;&#20110;&#35299;&#20915;&#34892;&#20154;&#23039;&#24577;&#20013;&#37096;&#20998;&#36974;&#25377;&#38382;&#39064;&#30340;&#20851;&#38190;&#28857;&#34917;&#20840;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#23545;&#19981;&#23436;&#25972;&#30340;&#20851;&#38190;&#28857;&#36827;&#34892;&#38477;&#32500;&#65292;&#32479;&#19968;&#29305;&#24449;&#20998;&#24067;&#65292;&#24182;&#20351;&#29992;GAN&#26694;&#26550;&#30340;&#20004;&#31181;&#29983;&#25104;&#27169;&#22411;&#26469;&#23436;&#25104;&#23039;&#24577;&#30340;&#34917;&#20840;&#12290;&#35813;&#26041;&#27861;&#30340;&#23454;&#39564;&#34920;&#26126;&#24615;&#33021;&#20248;&#20110;&#22522;&#26412;&#30340;GAIN&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2306.03538</link><description>&lt;p&gt;
SDR-GAIN&#65306;&#19968;&#31181;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#30340;&#39640;&#23454;&#26102;&#36974;&#25377;&#34892;&#20154;&#23039;&#24577;&#23436;&#25104;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
SDR-GAIN: A High Real-Time Occluded Pedestrian Pose Completion Method for Autonomous Driving. (arXiv:2306.03538v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03538
&lt;/p&gt;
&lt;p&gt;
SDR-GAIN&#26159;&#19968;&#31181;&#29992;&#20110;&#35299;&#20915;&#34892;&#20154;&#23039;&#24577;&#20013;&#37096;&#20998;&#36974;&#25377;&#38382;&#39064;&#30340;&#20851;&#38190;&#28857;&#34917;&#20840;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#23545;&#19981;&#23436;&#25972;&#30340;&#20851;&#38190;&#28857;&#36827;&#34892;&#38477;&#32500;&#65292;&#32479;&#19968;&#29305;&#24449;&#20998;&#24067;&#65292;&#24182;&#20351;&#29992;GAN&#26694;&#26550;&#30340;&#20004;&#31181;&#29983;&#25104;&#27169;&#22411;&#26469;&#23436;&#25104;&#23039;&#24577;&#30340;&#34917;&#20840;&#12290;&#35813;&#26041;&#27861;&#30340;&#23454;&#39564;&#34920;&#26126;&#24615;&#33021;&#20248;&#20110;&#22522;&#26412;&#30340;GAIN&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#32531;&#35299;&#22522;&#20110;&#20154;&#20307;&#23039;&#24577;&#20851;&#38190;&#28857;&#30340;&#34892;&#20154;&#26816;&#27979;&#31639;&#27861;&#20013;&#37096;&#20998;&#36974;&#25377;&#24102;&#26469;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#20998;&#31163;&#21644;&#38477;&#32500;&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#24615;&#34917;&#20840;&#32593;&#32476;(SDR-GAIN)&#30340;&#26032;&#22411;&#34892;&#20154;&#23039;&#21183;&#20851;&#38190;&#28857;&#34917;&#20840;&#26041;&#27861;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#21033;&#29992;OpenPose&#22312;&#22270;&#20687;&#20013;&#20272;&#35745;&#34892;&#20154;&#30340;&#23039;&#24577;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23545;&#30001;&#20110;&#36974;&#25377;&#25110;&#20854;&#20182;&#22240;&#32032;&#32780;&#19981;&#23436;&#25972;&#30340;&#34892;&#20154;&#22836;&#37096;&#21644;&#36527;&#24178;&#20851;&#38190;&#28857;&#36827;&#34892;&#32500;&#24230;&#32553;&#20943;&#65292;&#20197;&#22686;&#24378;&#29305;&#24449;&#24182;&#36827;&#19968;&#27493;&#32479;&#19968;&#29305;&#24449;&#20998;&#24067;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;(GAN)&#26694;&#26550;&#30340;&#20004;&#31181;&#29983;&#25104;&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#34701;&#21512;&#20102;Huber&#25439;&#22833;&#12289;&#27531;&#24046;&#32467;&#26500;&#21644;L1&#27491;&#21017;&#21270;&#26469;&#29983;&#25104;&#37096;&#20998;&#36974;&#25377;&#34892;&#20154;&#19981;&#23436;&#25972;&#22836;&#37096;&#21644;&#36527;&#24178;&#23039;&#24577;&#20851;&#38190;&#28857;&#30340;&#32570;&#22833;&#37096;&#20998;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#23039;&#24577;&#34917;&#20840;&#12290;&#25105;&#20204;&#22312;MS COCO&#21644;JAAD&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;SDR-GAIN&#30340;&#24615;&#33021;&#20248;&#20110;&#22522;&#26412;&#30340;GAIN&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
To mitigate the challenges arising from partial occlusion in human pose keypoint based pedestrian detection methods , we present a novel pedestrian pose keypoint completion method called the separation and dimensionality reduction-based generative adversarial imputation networks (SDR-GAIN) . Firstly, we utilize OpenPose to estimate pedestrian poses in images. Then, we isolate the head and torso keypoints of pedestrians with incomplete keypoints due to occlusion or other factors and perform dimensionality reduction to enhance features and further unify feature distribution. Finally, we introduce two generative models based on the generative adversarial networks (GAN) framework, which incorporate Huber loss, residual structure, and L1 regularization to generate missing parts of the incomplete head and torso pose keypoints of partially occluded pedestrians, resulting in pose completion. Our experiments on MS COCO and JAAD datasets demonstrate that SDR-GAIN outperforms basic GAIN framework
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#22914;&#20309;&#20026;AI&#29983;&#25104;&#30340;&#20869;&#23481;&#21046;&#23450;&#23433;&#20840;&#20445;&#38556;&#65292;&#20998;&#26512;LLMs&#30340;&#20869;&#23481;&#29983;&#25104;&#26426;&#21046;&#65292;&#30830;&#23450;&#20102;&#22235;&#20010;&#20851;&#38190;&#39046;&#22495;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#20998;&#21457;&#21644;&#38144;&#21806;LLM&#29983;&#25104;&#20869;&#23481;&#30340;&#20225;&#19994;&#30340;&#26631;&#20934;&#12290;</title><link>http://arxiv.org/abs/2306.03503</link><description>&lt;p&gt;
&#24212;&#29992;&#26631;&#20934;&#20419;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#19979;&#28216;&#20262;&#29702;
&lt;/p&gt;
&lt;p&gt;
Applying Standards to Advance Upstream &amp; Downstream Ethics in Large Language Models. (arXiv:2306.03503v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03503
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#22914;&#20309;&#20026;AI&#29983;&#25104;&#30340;&#20869;&#23481;&#21046;&#23450;&#23433;&#20840;&#20445;&#38556;&#65292;&#20998;&#26512;LLMs&#30340;&#20869;&#23481;&#29983;&#25104;&#26426;&#21046;&#65292;&#30830;&#23450;&#20102;&#22235;&#20010;&#20851;&#38190;&#39046;&#22495;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#20998;&#21457;&#21644;&#38144;&#21806;LLM&#29983;&#25104;&#20869;&#23481;&#30340;&#20225;&#19994;&#30340;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;AI&#25152;&#26377;&#32773;&#22914;&#20309;&#20511;&#37492;&#20854;&#20182;&#20869;&#23481;&#21019;&#20316;&#34892;&#19994;&#30340;&#34892;&#20026;&#20934;&#21017;&#21644;&#20262;&#29702;&#26631;&#20934;&#65292;&#20026;AI&#29983;&#25104;&#30340;&#20869;&#23481;&#21046;&#23450;&#23433;&#20840;&#20445;&#38556;&#12290;&#23427;&#28145;&#20837;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20262;&#29702;&#24847;&#35782;&#29616;&#29366;&#12290;&#36890;&#36807;&#20998;&#26512;LLMs&#30340;&#20869;&#23481;&#29983;&#25104;&#26426;&#21046;&#65292;&#30830;&#23450;&#20102;&#22235;&#20010;&#20851;&#38190;&#39046;&#22495;&#65288;&#19978;&#19979;&#28216;&#21644;&#29992;&#25143;&#25552;&#31034;/&#22238;&#31572;&#65289;&#65292;&#22312;&#36825;&#20123;&#39046;&#22495;&#21487;&#20197;&#26377;&#25928;&#22320;&#24212;&#29992;&#20445;&#38556;&#25514;&#26045;&#12290;&#38543;&#21518;&#65292;&#23545;&#36825;&#22235;&#20010;&#39046;&#22495;&#36827;&#34892;&#20102;&#27604;&#36739;&#20998;&#26512;&#65292;&#21253;&#25324;&#22312;&#25104;&#26412;&#12289;&#26377;&#25928;&#24615;&#21644;&#19982;&#34892;&#19994;&#24815;&#20363;&#30340;&#19968;&#33268;&#24615;&#26041;&#38754;&#35780;&#20272;&#29616;&#26377;&#30340;&#20262;&#29702;&#20445;&#38556;&#25514;&#26045;&#12290;&#26412;&#25991;&#30340;&#20027;&#35201;&#35266;&#28857;&#26159;&#65292;&#29616;&#26377;&#30340;&#19982;IT&#30456;&#20851;&#30340;&#20262;&#29702;&#20934;&#21017;&#34429;&#28982;&#36866;&#29992;&#20110;&#20256;&#32479;&#30340;IT&#24037;&#31243;&#39046;&#22495;&#65292;&#20294;&#19981;&#36275;&#20197;&#24212;&#23545;&#22522;&#20110;LLMs&#20869;&#23481;&#29983;&#25104;&#25152;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#20511;&#37492;&#26032;&#38395;&#19994;&#20869;&#24050;&#26377;&#30340;&#23454;&#36341;&#65292;&#20026;&#20998;&#21457;&#21644;&#38144;&#21806;LLM&#29983;&#25104;&#20869;&#23481;&#30340;&#20225;&#19994;&#25552;&#20986;&#20102;&#28508;&#22312;&#30340;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper explores how AI-owners can develop safeguards for AI-generated content by drawing from established codes of conduct and ethical standards in other content-creation industries. It delves into the current state of ethical awareness on Large Language Models (LLMs). By dissecting the mechanism of content generation by LLMs, four key areas (upstream/downstream and at user prompt/answer), where safeguards could be effectively applied, are identified. A comparative analysis of these four areas follows and includes an evaluation of the existing ethical safeguards in terms of cost, effectiveness, and alignment with established industry practices. The paper's key argument is that existing IT-related ethical codes, while adequate for traditional IT engineering, are inadequate for the challenges posed by LLM-based content generation. Drawing from established practices within journalism, we propose potential standards for businesses involved in distributing and selling LLM-generated cont
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24320;&#21019;&#24615;&#30740;&#31350;&#35843;&#26597;&#20102;&#23398;&#29983;&#23545;&#8220;AI-giarism&#8221;&#30340;&#35748;&#30693;&#65292;&#25552;&#20986;&#20102;&#21021;&#22987;&#27010;&#24565;&#21270;&#30340;AI-giarism&#24037;&#20855;&#65292;&#26377;&#21161;&#20110;&#24212;&#23545;&#19981;&#26029;&#21457;&#23637;&#30340;AI&#25216;&#26415;&#24102;&#26469;&#30340;&#23398;&#26415;&#19981;&#31471;&#34892;&#20026;&#65292;&#21516;&#26102;&#36824;&#25361;&#25112;&#20102;&#20256;&#32479;&#30340;&#23398;&#26415;&#19981;&#31471;&#34892;&#20026;&#23450;&#20041;&#12290;</title><link>http://arxiv.org/abs/2306.03358</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#25913;&#21464;&#20102;&#23398;&#26415;&#19981;&#31471;&#34892;&#20026;&#30340;&#35268;&#21017;&#21527;&#65311;&#28145;&#20837;&#25506;&#31350;&#23398;&#29983;&#23545;&#8220;AI-giarism&#8221;&#30340;&#30475;&#27861;
&lt;/p&gt;
&lt;p&gt;
Is AI Changing the Rules of Academic Misconduct? An In-depth Look at Students' Perceptions of 'AI-giarism'. (arXiv:2306.03358v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03358
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24320;&#21019;&#24615;&#30740;&#31350;&#35843;&#26597;&#20102;&#23398;&#29983;&#23545;&#8220;AI-giarism&#8221;&#30340;&#35748;&#30693;&#65292;&#25552;&#20986;&#20102;&#21021;&#22987;&#27010;&#24565;&#21270;&#30340;AI-giarism&#24037;&#20855;&#65292;&#26377;&#21161;&#20110;&#24212;&#23545;&#19981;&#26029;&#21457;&#23637;&#30340;AI&#25216;&#26415;&#24102;&#26469;&#30340;&#23398;&#26415;&#19981;&#31471;&#34892;&#20026;&#65292;&#21516;&#26102;&#36824;&#25361;&#25112;&#20102;&#20256;&#32479;&#30340;&#23398;&#26415;&#19981;&#31471;&#34892;&#20026;&#23450;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24320;&#21019;&#24615;&#30740;&#31350;&#25506;&#35752;&#20102;&#39640;&#31561;&#25945;&#32946;&#32972;&#26223;&#19979;&#65292;AI&#21644;&#25220;&#34989;&#21512;&#20307;&#25152;&#28041;&#21450;&#30340;&#26032;&#20852;&#23398;&#26415;&#19981;&#31471;&#34892;&#20026;&#8220;AI-giarism&#8221;&#30340;&#23398;&#29983;&#35748;&#30693;&#12290;&#20849;&#26377;&#26469;&#33258;&#19981;&#21516;&#23398;&#31185;&#30340;393&#21517;&#26412;&#31185;&#21644;&#30740;&#31350;&#29983;&#21442;&#19982;&#20102;&#35843;&#26597;&#65292;&#21463;&#35775;&#32773;&#38024;&#23545;&#22810;&#31181;AI-giarism&#24773;&#22659;&#34920;&#36798;&#20102;&#19981;&#21516;&#30340;&#30475;&#27861;&#12290;&#30740;&#31350;&#32467;&#26524;&#25581;&#31034;&#20102;&#19968;&#20010;&#22797;&#26434;&#30340;&#35748;&#30693;&#26684;&#23616;&#65292;&#26126;&#30830;&#21453;&#23545;&#30452;&#25509;&#29983;&#25104;AI&#20869;&#23481;&#65292;&#20294;&#23545;&#20110;&#26356;&#24494;&#22937;&#30340;AI&#20351;&#29992;&#26041;&#24335;&#21017;&#25345;&#26356;&#20026;&#26279;&#26151;&#24577;&#24230;&#12290;&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#22411;&#24037;&#20855;&#8212;&#8212;&#20316;&#20026;AI-giarism&#30340;&#21021;&#22987;&#27010;&#24565;&#21270;&#8212;&#8212;&#20026;&#25945;&#32946;&#24037;&#20316;&#32773;&#21644;&#25919;&#31574;&#21046;&#23450;&#32773;&#25552;&#20379;&#20102;&#37325;&#35201;&#25903;&#25345;&#12290;&#35813;&#37327;&#20855;&#26377;&#21161;&#20110;&#29702;&#35299;&#21644;&#25506;&#35752;&#19982;AI&#26377;&#20851;&#30340;&#23398;&#26415;&#19981;&#31471;&#34892;&#20026;&#65292;&#26377;&#21161;&#20110;&#22312;AI&#38598;&#25104;&#26102;&#36827;&#34892;&#25945;&#23398;&#35774;&#35745;&#21644;&#35780;&#20272;&#12290;&#27492;&#22806;&#65292;&#23427;&#36824;&#25361;&#25112;&#20102;&#20256;&#32479;&#30340;&#23398;&#26415;&#19981;&#31471;&#34892;&#20026;&#23450;&#20041;&#65292;&#24378;&#35843;&#20102;&#26681;&#25454;&#19981;&#26029;&#21457;&#23637;&#30340;AI&#25216;&#26415;&#36827;&#34892;&#36866;&#24212;&#30340;&#24517;&#35201;&#24615;&#12290;&#23613;&#31649;&#23384;&#22312;&#38480;&#21046;&#65292;&#20363;&#22914;&#20855;&#26377;&#26679;&#26412;&#20559;&#24046;&#31561;&#38382;&#39064;&#65292;&#20294;&#26159;&#35813;&#30740;&#31350;&#20173;&#28982;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
This pioneering study explores students' perceptions of AI-giarism, an emergent form of academic dishonesty involving AI and plagiarism, within the higher education context. A survey, undertaken by 393 undergraduate and postgraduate students from a variety of disciplines, investigated their perceptions of diverse AI-giarism scenarios. The findings portray a complex landscape of understanding, with clear disapproval for direct AI content generation, yet more ambivalent attitudes towards subtler uses of AI. The study introduces a novel instrument, as an initial conceptualization of AI-giarism, offering a significant tool for educators and policy-makers. This scale facilitates understanding and discussions around AI-related academic misconduct, aiding in pedagogical design and assessment in an era of AI integration. Moreover, it challenges traditional definitions of academic misconduct, emphasizing the need to adapt in response to evolving AI technology. Despite limitations, such as the r
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;LLM-Blender&#65292;&#23427;&#26159;&#19968;&#20010;&#38598;&#25104;&#26694;&#26550;&#65292;&#26088;&#22312;&#21033;&#29992;&#19981;&#21516;&#30340;&#24320;&#28304;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20248;&#31168;&#29305;&#24615;&#65292;&#23454;&#29616;&#22987;&#32456;&#22914;&#19968;&#30340;&#21331;&#36234;&#24615;&#33021;&#12290;PairRanker&#21644;GenFuser&#26159;&#35813;&#26694;&#26550;&#30340;&#20004;&#20010;&#27169;&#22359;&#65292;PairRanker&#20351;&#29992;&#25104;&#23545;&#27604;&#36739;&#26041;&#27861;&#26469;&#21306;&#20998;&#20505;&#36873;&#36755;&#20986;&#65292;&#24182;&#19988;GenFuser&#26088;&#22312;&#21512;&#24182;&#25490;&#21517;&#26368;&#39640;&#30340;&#20505;&#36873;&#32773;&#65292;&#20197;&#29983;&#25104;&#25913;&#36827;&#30340;&#36755;&#20986;&#12290;</title><link>http://arxiv.org/abs/2306.02561</link><description>&lt;p&gt;
LLM-Blender: &#21033;&#29992;&#25104;&#23545;&#25490;&#21517;&#21644;&#29983;&#25104;&#34701;&#21512;&#38598;&#25104;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
LLM-Blender: Ensembling Large Language Models with Pairwise Ranking and Generative Fusion. (arXiv:2306.02561v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02561
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;LLM-Blender&#65292;&#23427;&#26159;&#19968;&#20010;&#38598;&#25104;&#26694;&#26550;&#65292;&#26088;&#22312;&#21033;&#29992;&#19981;&#21516;&#30340;&#24320;&#28304;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20248;&#31168;&#29305;&#24615;&#65292;&#23454;&#29616;&#22987;&#32456;&#22914;&#19968;&#30340;&#21331;&#36234;&#24615;&#33021;&#12290;PairRanker&#21644;GenFuser&#26159;&#35813;&#26694;&#26550;&#30340;&#20004;&#20010;&#27169;&#22359;&#65292;PairRanker&#20351;&#29992;&#25104;&#23545;&#27604;&#36739;&#26041;&#27861;&#26469;&#21306;&#20998;&#20505;&#36873;&#36755;&#20986;&#65292;&#24182;&#19988;GenFuser&#26088;&#22312;&#21512;&#24182;&#25490;&#21517;&#26368;&#39640;&#30340;&#20505;&#36873;&#32773;&#65292;&#20197;&#29983;&#25104;&#25913;&#36827;&#30340;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;LLM-Blender&#65292;&#19968;&#20010;&#38598;&#25104;&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#21033;&#29992;&#22810;&#20010;&#24320;&#28304;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#19981;&#21516;&#20248;&#21183;&#26469;&#36798;&#21040;&#22987;&#32456;&#22914;&#19968;&#30340;&#21331;&#36234;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#30001;&#20004;&#20010;&#27169;&#22359;&#32452;&#25104;&#65306;PairRanker&#21644;GenFuser&#65292;&#20197;&#24212;&#23545;&#19981;&#21516;&#31034;&#20363;&#30340;&#26368;&#20248;LLMs&#21487;&#20197;&#26174;&#30528;&#21464;&#21270;&#30340;&#35266;&#23519;&#12290;PairRanker&#20351;&#29992;&#19987;&#38376;&#30340;&#25104;&#23545;&#27604;&#36739;&#26041;&#27861;&#26469;&#21306;&#20998;&#20505;&#36873;&#36755;&#20986;&#20043;&#38388;&#30340;&#24494;&#23567;&#24046;&#24322;&#12290;&#23427;&#32852;&#21512;&#32534;&#30721;&#36755;&#20837;&#25991;&#26412;&#21644;&#19968;&#23545;&#20505;&#36873;&#32773;&#65292;&#20351;&#29992;&#20132;&#21449;&#27880;&#24847;&#32534;&#30721;&#22120;&#26469;&#30830;&#23450;&#20248;&#36234;&#32773;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;PairRanker&#19982;ChatGPT&#30340;&#25490;&#21517;&#30456;&#20851;&#24615;&#26368;&#39640;&#12290;&#28982;&#21518;&#65292;GenFuser&#26088;&#22312;&#21512;&#24182;&#25490;&#21517;&#26368;&#39640;&#30340;&#20505;&#36873;&#32773;&#65292;&#36890;&#36807;&#21033;&#29992;&#23427;&#20204;&#30340;&#20248;&#21183;&#21644;&#20943;&#23569;&#23427;&#20204;&#30340;&#24369;&#28857;&#26469;&#29983;&#25104;&#25913;&#36827;&#30340;&#36755;&#20986;&#12290;&#20026;&#20102;&#20419;&#36827;&#22823;&#35268;&#27169;&#35780;&#20272;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;MixInstruct&#65292;&#23427;&#26159;&#22810;&#20010;&#25351;&#20196;&#25968;&#25454;&#38598;&#30340;&#28151;&#21512;&#65292;&#20855;&#26377;oracle p&#12290;
&lt;/p&gt;
&lt;p&gt;
We present LLM-Blender, an ensembling framework designed to attain consistently superior performance by leveraging the diverse strengths of multiple open-source large language models (LLMs). Our framework consists of two modules: PairRanker and GenFuser, addressing the observation that optimal LLMs for different examples can significantly vary. PairRanker employs a specialized pairwise comparison method to distinguish subtle differences between candidate outputs. It jointly encodes the input text and a pair of candidates, using cross-attention encoders to determine the superior one. Our results demonstrate that PairRanker exhibits the highest correlation with ChatGPT-based ranking. Then, GenFuser aims to merge the top-ranked candidates, generating an improved output by capitalizing on their strengths and mitigating their weaknesses. To facilitate large-scale evaluation, we introduce a benchmark dataset, MixInstruct, which is a mixture of multiple instruction datasets featuring oracle p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#25968;&#25454;&#23494;&#38598;&#22411;&#38382;&#39064;&#21644;&#22810;&#30446;&#26631;&#20248;&#21270;&#35774;&#32622;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#26694;&#26550;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#20195;&#29702;&#24314;&#27169;&#21644;&#21487;&#25193;&#23637;&#12289;&#20855;&#26377;&#19981;&#30830;&#23450;&#24615;&#30340;&#25910;&#36141;&#31574;&#30053;&#65292;&#33021;&#22815;&#22312;&#26368;&#23569;&#36845;&#20195;&#27425;&#25968;&#30340;&#24773;&#20917;&#19979;&#39640;&#25928;&#22320;&#36827;&#34892;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2306.01095</link><description>&lt;p&gt;
&#22823;&#25209;&#37327;&#31070;&#32463;&#22810;&#30446;&#26631;&#36125;&#21494;&#26031;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Large-Batch, Neural Multi-Objective Bayesian Optimization. (arXiv:2306.01095v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01095
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#25968;&#25454;&#23494;&#38598;&#22411;&#38382;&#39064;&#21644;&#22810;&#30446;&#26631;&#20248;&#21270;&#35774;&#32622;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#26694;&#26550;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#20195;&#29702;&#24314;&#27169;&#21644;&#21487;&#25193;&#23637;&#12289;&#20855;&#26377;&#19981;&#30830;&#23450;&#24615;&#30340;&#25910;&#36141;&#31574;&#30053;&#65292;&#33021;&#22815;&#22312;&#26368;&#23569;&#36845;&#20195;&#27425;&#25968;&#30340;&#24773;&#20917;&#19979;&#39640;&#25928;&#22320;&#36827;&#34892;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#20248;&#21270;&#22312;&#20840;&#23616;&#20248;&#21270;&#40657;&#30418;&#39640;&#25104;&#26412;&#20989;&#25968;&#26041;&#38754;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#26694;&#26550;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#40664;&#35748;&#39640;&#26031;&#36807;&#31243;&#20195;&#29702;&#30340;&#21487;&#25193;&#23637;&#24615;&#24046;&#65292;&#23427;&#22312;&#22788;&#29702;&#25968;&#25454;&#23494;&#38598;&#22411;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#22312;&#22810;&#30446;&#26631;&#35774;&#32622;&#20013;&#30340;&#33021;&#21147;&#26377;&#38480;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#26694;&#26550;&#65292;&#19987;&#20026;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#32780;&#35774;&#35745;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#36827;&#34892;&#20195;&#29702;&#24314;&#27169;&#12290;&#36825;&#20351;&#24471;&#23427;&#33021;&#22815;&#26377;&#25928;&#22320;&#22788;&#29702;&#22823;&#25209;&#37327;&#25968;&#25454;&#65292;&#24314;&#27169;&#22797;&#26434;&#38382;&#39064;&#20197;&#21450;&#20135;&#29983;&#39044;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#32467;&#21512;&#20102;&#19968;&#31181;&#22522;&#20110;&#20247;&#25152;&#21608;&#30693;&#19988;&#26131;&#20110;&#37096;&#32626;&#30340;NSGA-II&#30340;&#21487;&#25193;&#23637;&#30340;&#12289;&#20855;&#26377;&#19981;&#30830;&#23450;&#24615;&#30340;&#25910;&#36141;&#31574;&#30053;&#12290;&#36825;&#31181;&#23436;&#20840;&#21487;&#24182;&#34892;&#21270;&#30340;&#31574;&#30053;&#20419;&#36827;&#20102;&#26410;&#21208;&#25506;&#21306;&#22495;&#30340;&#26377;&#25928;&#25506;&#32034;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#20801;&#35768;&#22312;&#26368;&#23569;&#36845;&#20195;&#27425;&#25968;&#30340;&#24773;&#20917;&#19979;&#22312;&#25968;&#25454;&#23494;&#38598;&#29615;&#22659;&#20013;&#36827;&#34892;&#26377;&#25928;&#30340;&#20248;&#21270;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian optimization provides a powerful framework for global optimization of black-box, expensive-to-evaluate functions. However, it has a limited capacity in handling data-intensive problems, especially in multi-objective settings, due to the poor scalability of default Gaussian Process surrogates. We present a novel Bayesian optimization framework specifically tailored to address these limitations. Our method leverages a Bayesian neural networks approach for surrogate modeling. This enables efficient handling of large batches of data, modeling complex problems, and generating the uncertainty of the predictions. In addition, our method incorporates a scalable, uncertainty-aware acquisition strategy based on the well-known, easy-to-deploy NSGA-II. This fully parallelizable strategy promotes efficient exploration of uncharted regions. Our framework allows for effective optimization in data-intensive environments with a minimum number of iterations. We demonstrate the superiority of ou
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#24433;&#21709;&#29992;&#25143;&#31572;&#26696;&#36136;&#37327;&#30340;&#20004;&#20010;&#20851;&#38190;&#22240;&#32032;&#26159;&#21453;&#24212;&#26102;&#38388;&#21644;&#23436;&#25104;&#26102;&#38388;&#65292;&#24182;&#25506;&#31350;&#36825;&#20123;&#22240;&#32032;&#19982;&#22806;&#37096;&#21644;&#20869;&#37096;&#21407;&#22240;&#30340;&#30456;&#20851;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.00627</link><description>&lt;p&gt;
&#24433;&#21709;&#26234;&#33021;&#25163;&#26426;&#29992;&#25143;&#31572;&#26696;&#36136;&#37327;&#30340;&#22240;&#32032;
&lt;/p&gt;
&lt;p&gt;
Factors Impacting the Quality of User Answers on Smartphones. (arXiv:2306.00627v2 [cs.HC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00627
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#24433;&#21709;&#29992;&#25143;&#31572;&#26696;&#36136;&#37327;&#30340;&#20004;&#20010;&#20851;&#38190;&#22240;&#32032;&#26159;&#21453;&#24212;&#26102;&#38388;&#21644;&#23436;&#25104;&#26102;&#38388;&#65292;&#24182;&#25506;&#31350;&#36825;&#20123;&#22240;&#32032;&#19982;&#22806;&#37096;&#21644;&#20869;&#37096;&#21407;&#22240;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#65292;&#22823;&#22810;&#25968;&#30740;&#31350;&#25506;&#31350;&#20154;&#31867;&#34892;&#20026;&#30340;&#21487;&#39044;&#27979;&#24615;&#65292;&#22914;&#31227;&#21160;&#21644;&#31038;&#20132;&#20114;&#21160;&#65292;&#20027;&#35201;&#38598;&#20013;&#22312;&#20256;&#24863;&#22120;&#25968;&#25454;&#30340;&#21033;&#29992;&#19978;&#12290;&#28982;&#32780;&#65292;&#20256;&#24863;&#22120;&#25968;&#25454;&#24456;&#38590;&#25429;&#25417;&#21040;&#20010;&#20154;&#34892;&#20026;&#32972;&#21518;&#30340;&#20027;&#35266;&#21160;&#26426;&#12290;&#29702;&#35299;&#20010;&#20154;&#30340;&#19978;&#19979;&#25991;&#65288;&#20363;&#22914;&#65292;&#20182;&#20204;&#25152;&#22788;&#30340;&#22320;&#26041;&#21644;&#27491;&#22312;&#20570;&#20160;&#20040;&#65289;&#21487;&#20197;&#22823;&#22823;&#25552;&#39640;&#21487;&#39044;&#27979;&#24615;&#12290;&#20027;&#35201;&#38480;&#21046;&#26159;&#20154;&#31867;&#36755;&#20837;&#32463;&#24120;&#32570;&#22833;&#25110;&#19981;&#20934;&#30830;&#12290;&#26412;&#25991;&#30340;&#30446;&#26631;&#26159;&#30830;&#23450;&#24433;&#21709;&#29992;&#25143;&#34987;&#38382;&#21450;&#20854;&#24403;&#21069;&#19978;&#19979;&#25991;&#26102;&#21709;&#24212;&#36136;&#37327;&#30340;&#22240;&#32032;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20004;&#20010;&#20851;&#38190;&#22240;&#32032;&#24433;&#21709;&#30528;&#21709;&#24212;&#36136;&#37327;&#65306;&#29992;&#25143;&#30340;&#21453;&#24212;&#26102;&#38388;&#21644;&#23436;&#25104;&#26102;&#38388;&#12290;&#36825;&#20123;&#22240;&#32032;&#19982;&#21508;&#31181;&#22806;&#37096;&#21407;&#22240;&#65288;&#20363;&#22914;&#65292;&#24773;&#22659;&#32972;&#26223;&#65292;&#26102;&#38388;&#65289;&#21644;&#20869;&#37096;&#21407;&#22240;&#65288;&#20363;&#22914;&#65292;&#25302;&#24310;&#24577;&#24230;&#65292;&#24515;&#24773;&#65289;&#30456;&#20851;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#30740;&#31350;&#36825;&#20004;&#20010;&#22240;&#32032;&#22914;&#20309;&#24433;&#21709;&#21709;&#24212;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
So far, most research investigating the predictability of human behavior, such as mobility and social interactions, has focused mainly on the exploitation of sensor data. However, sensor data can be difficult to capture the subjective motivations behind the individuals' behavior. Understanding personal context (e.g., where one is and what they are doing) can greatly increase predictability. The main limitation is that human input is often missing or inaccurate. The goal of this paper is to identify factors that influence the quality of responses when users are asked about their current context. We find that two key factors influence the quality of responses: user reaction time and completion time. These factors correlate with various exogenous causes (e.g., situational context, time of day) and endogenous causes (e.g., procrastination attitude, mood). In turn, we study how these two factors impact the quality of responses.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#29305;&#23450;&#19978;&#19979;&#25991;&#29420;&#31435;&#20851;&#31995;&#23545;&#22240;&#26524;&#27169;&#20223;&#23398;&#20064;&#30340;&#24433;&#21709;&#65292;&#35777;&#26126;&#20102;&#36825;&#31181;&#24773;&#20917;&#19979;&#30340;&#27169;&#20223;&#21487;&#34892;&#24615;&#20915;&#31574;&#38382;&#39064;&#26159;NP&#38590;&#30340;&#65292;&#24182;&#25552;&#20379;&#20102;&#24517;&#35201;&#30340;&#22270;&#24418;&#26631;&#20934;&#20197;&#21450;&#19968;&#20010;&#26377;&#22768;&#30340;&#31639;&#27861;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.00585</link><description>&lt;p&gt;
&#29305;&#23450;&#19978;&#19979;&#25991;&#29420;&#31435;&#20851;&#31995;&#19979;&#30340;&#22240;&#26524;&#21487;&#27169;&#20223;&#24615;
&lt;/p&gt;
&lt;p&gt;
Causal Imitability Under Context-Specific Independence Relations. (arXiv:2306.00585v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00585
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#29305;&#23450;&#19978;&#19979;&#25991;&#29420;&#31435;&#20851;&#31995;&#23545;&#22240;&#26524;&#27169;&#20223;&#23398;&#20064;&#30340;&#24433;&#21709;&#65292;&#35777;&#26126;&#20102;&#36825;&#31181;&#24773;&#20917;&#19979;&#30340;&#27169;&#20223;&#21487;&#34892;&#24615;&#20915;&#31574;&#38382;&#39064;&#26159;NP&#38590;&#30340;&#65292;&#24182;&#25552;&#20379;&#20102;&#24517;&#35201;&#30340;&#22270;&#24418;&#26631;&#20934;&#20197;&#21450;&#19968;&#20010;&#26377;&#22768;&#30340;&#31639;&#27861;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#35748;&#35782;&#21040;&#65292;&#22312;&#25191;&#34892;&#27169;&#20223;&#23398;&#20064;&#26102;&#24573;&#30053;&#22240;&#26524;&#26426;&#21046;&#30340;&#32570;&#28857;&#12290;&#25991;&#29486;&#20013;&#25552;&#20986;&#20102;&#20960;&#31181;&#26041;&#27861;&#26469;&#35780;&#20272;&#27169;&#20223;&#30340;&#21487;&#34892;&#24615;&#20197;&#21450;&#35268;&#36991;&#22240;&#26524;&#28151;&#28102;&#21644;&#22240;&#26524;&#20559;&#24046;&#65292;&#20294;&#26159;&#23545;&#20110;&#20851;&#20110;&#28508;&#22312;&#22240;&#26524;&#32467;&#26500;&#30340;&#20854;&#20182;&#20449;&#24687;&#30340;&#28508;&#22312;&#22909;&#22788;&#21364;&#26410;&#34987;&#25506;&#32034;&#12290;&#36825;&#20123;&#24573;&#30053;&#30340;&#20449;&#24687;&#20043;&#19968;&#26159;&#29305;&#23450;&#19978;&#19979;&#25991;&#29420;&#31435;&#24615;&#65288;CSI&#65289;&#65292;&#21363;&#20165;&#22312;&#26576;&#20123;&#19978;&#19979;&#25991;&#20013;&#20445;&#25345;&#29420;&#31435;&#24615;&#12290;&#24403;&#24050;&#30693;CSI&#20851;&#31995;&#26102;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#22240;&#26524;&#27169;&#20223;&#23398;&#20064;&#38382;&#39064;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#20851;&#20110;&#27169;&#20223;&#21487;&#34892;&#24615;&#30340;&#20915;&#31574;&#38382;&#39064;&#26159;NP&#38590;&#30340;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#22312;CSI&#19979;&#30340;&#27169;&#20223;&#23398;&#20064;&#24517;&#35201;&#30340;&#22270;&#24418;&#26631;&#20934;&#65292;&#24182;&#34920;&#26126;&#22312;&#19968;&#31181;&#32467;&#26500;&#20551;&#35774;&#19979;&#65292;&#36825;&#19968;&#26631;&#20934;&#20063;&#26159;&#20805;&#20998;&#30340;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26377;&#22768;&#30340;&#31639;&#27861;&#26041;&#27861;&#29992;&#20110;&#22522;&#20110;&#22240;&#26524;&#30340;&#27169;&#20223;&#23398;&#20064;&#65292;&#35813;&#31639;&#27861;&#32771;&#34385;&#20102;CSI&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Drawbacks of ignoring the causal mechanisms when performing imitation learning have recently been acknowledged. Several approaches both to assess the feasibility of imitation and to circumvent causal confounding and causal misspecifications have been proposed in the literature. However, the potential benefits of the incorporation of additional information about the underlying causal structure are left unexplored. An example of such overlooked information is context-specific independence (CSI), i.e., independence that holds only in certain contexts. We consider the problem of causal imitation learning when CSI relations are known. We prove that the decision problem pertaining to the feasibility of imitation in this setting is NP-hard. Further, we provide a necessary graphical criterion for imitation learning under CSI and show that under a structural assumption, this criterion is also sufficient. Finally, we propose a sound algorithmic approach for causal imitation learning which takes 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23581;&#35797;&#23454;&#29616;&#22312;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#36816;&#29992;&#21487;&#36870;&#27169;&#22411;&#23454;&#29616;&#39640;&#25928;&#30340;&#24494;&#35843;&#65292;&#24182;&#21457;&#29616;&#22312;&#21021;&#22987;&#21270;&#24494;&#35843;&#26102;&#20445;&#30041;PLM&#30340;&#36215;&#28857;&#38750;&#24120;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2306.00477</link><description>&lt;p&gt;
&#20351;&#39044;&#35757;&#32451;&#27169;&#22411;&#20855;&#26377;&#21487;&#36870;&#24615;&#65306;&#20174;&#21442;&#25968;&#21040;&#20869;&#23384;&#39640;&#25928;&#30340;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Make Your Pre-trained Model Reversible: From Parameter to Memory Efficient Fine-Tuning. (arXiv:2306.00477v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00477
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23581;&#35797;&#23454;&#29616;&#22312;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#36816;&#29992;&#21487;&#36870;&#27169;&#22411;&#23454;&#29616;&#39640;&#25928;&#30340;&#24494;&#35843;&#65292;&#24182;&#21457;&#29616;&#22312;&#21021;&#22987;&#21270;&#24494;&#35843;&#26102;&#20445;&#30041;PLM&#30340;&#36215;&#28857;&#38750;&#24120;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#38750;&#24120;&#25104;&#21151;&#30340;&#26041;&#27861;&#65292;&#21482;&#38656;&#35757;&#32451;&#23569;&#37327;&#21442;&#25968;&#32780;&#19981;&#20250;&#38477;&#20302;&#24615;&#33021;&#65292;&#24182;&#38543;&#30528;PLM&#36234;&#26469;&#36234;&#22823;&#32780;&#25104;&#20026;&#20107;&#23454;&#19978;&#30340;&#23398;&#20064;&#33539;&#24335;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;PEFT&#26041;&#27861;&#19981;&#20855;&#22791;&#20869;&#23384;&#25928;&#29575;&#65292;&#22240;&#20026;&#23427;&#20204;&#20173;&#38656;&#35201;&#23384;&#20648;&#22823;&#37096;&#20998;&#20013;&#38388;&#28608;&#27963;&#20540;&#20197;&#20415;&#35745;&#31639;&#26799;&#24230;&#65292;&#31867;&#20284;&#20110;&#24494;&#35843;&#12290;&#19968;&#20010;&#20943;&#23569;&#28608;&#27963;&#20869;&#23384;&#30340;&#26377;&#25928;&#26041;&#27861;&#26159;&#24212;&#29992;&#21487;&#36870;&#27169;&#22411;&#65292;&#36825;&#26679;&#20013;&#38388;&#28608;&#27963;&#20540;&#23601;&#26080;&#38656;&#32531;&#23384;&#65292;&#21487;&#20197;&#37325;&#26032;&#35745;&#31639;&#12290;&#28982;&#32780;&#65292;&#23558;PLM&#20462;&#25913;&#20026;&#23427;&#30340;&#21487;&#36870;&#21464;&#20307;&#24182;&#36827;&#34892;PEFT&#24182;&#19981;&#26159;&#19968;&#20214;&#23481;&#26131;&#30340;&#20107;&#65292;&#22240;&#20026;&#21487;&#36870;&#27169;&#22411;&#20855;&#26377;&#19982;&#24403;&#21069;&#21457;&#24067;&#30340;PLM&#19981;&#21516;&#30340;&#20307;&#31995;&#32467;&#26500;&#12290;&#26412;&#25991;&#39318;&#20808;&#35843;&#26597;&#29616;&#26377;PEFT&#26041;&#27861;&#25104;&#21151;&#30340;&#20851;&#38190;&#22240;&#32032;&#65292;&#35748;&#35782;&#21040;&#22312;&#21021;&#22987;&#21270;PEFT&#26102;&#20445;&#30041;PLM&#30340;&#36215;&#28857;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Parameter-efficient fine-tuning (PEFT) of pre-trained language models (PLMs) has emerged as a highly successful approach, with training only a small number of parameters without sacrificing performance and becoming the de-facto learning paradigm with the increasing size of PLMs. However, existing PEFT methods are not memory-efficient, because they still require caching most of the intermediate activations for the gradient calculation, akin to fine-tuning. One effective way to reduce the activation memory is to apply a reversible model, so the intermediate activations are not necessary to be cached and can be recomputed. Nevertheless, modifying a PLM to its reversible variant with PEFT is not straightforward, since the reversible model has a distinct architecture from the currently released PLMs. In this paper, we first investigate what is a key factor for the success of existing PEFT methods, and realize that it's essential to preserve the PLM's starting point when initializing a PEFT 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#26041;&#27861;&#26469;&#35299;&#20915;&#22312;&#20351;&#29992;AI&#31995;&#32479;&#36827;&#34892;&#20915;&#31574;&#26102;&#30340;&#21487;&#20449;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#24341;&#20837;&#20102;&#19968;&#20010;&#21253;&#25324;&#34920;&#36798;&#12289;&#30495;&#23454;&#21644;&#22522;&#26412;&#27700;&#24179;&#30340;&#19981;&#21516;&#20449;&#20219;&#32423;&#21035;&#30340;TAI&#20998;&#31867;&#31995;&#32479;&#25110;&#26694;&#26550;&#65292;&#20351;&#29992;&#21313;&#20010;&#32500;&#24230;&#26469;&#34913;&#37327;&#20449;&#20219;&#65292;&#24182;&#25552;&#20379;&#20102;&#29616;&#26377;TAI&#30740;&#31350;&#30340;&#35843;&#26597;&#21644;&#20803;&#20998;&#26512;&#65292;&#36824;&#30830;&#23450;&#20102;TAI&#30740;&#31350;&#30340;&#26410;&#26469;&#26041;&#21521;&#21644;&#28508;&#22312;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2306.00380</link><description>&lt;p&gt;
&#21487;&#20449;&#20154;&#24037;&#26234;&#33021;&#30340;&#35843;&#26597;&#12289;&#20998;&#31867;&#21450;&#26410;&#26469;&#26041;&#21521;&#65306;&#20803;&#20915;&#31574;&#30340;&#25112;&#30053;&#20915;&#31574;&#20803;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
The Survey, Taxonomy, and Future Directions of Trustworthy AI: A Meta Decision of Strategic Decisions. (arXiv:2306.00380v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00380
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#26041;&#27861;&#26469;&#35299;&#20915;&#22312;&#20351;&#29992;AI&#31995;&#32479;&#36827;&#34892;&#20915;&#31574;&#26102;&#30340;&#21487;&#20449;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#24341;&#20837;&#20102;&#19968;&#20010;&#21253;&#25324;&#34920;&#36798;&#12289;&#30495;&#23454;&#21644;&#22522;&#26412;&#27700;&#24179;&#30340;&#19981;&#21516;&#20449;&#20219;&#32423;&#21035;&#30340;TAI&#20998;&#31867;&#31995;&#32479;&#25110;&#26694;&#26550;&#65292;&#20351;&#29992;&#21313;&#20010;&#32500;&#24230;&#26469;&#34913;&#37327;&#20449;&#20219;&#65292;&#24182;&#25552;&#20379;&#20102;&#29616;&#26377;TAI&#30740;&#31350;&#30340;&#35843;&#26597;&#21644;&#20803;&#20998;&#26512;&#65292;&#36824;&#30830;&#23450;&#20102;TAI&#30740;&#31350;&#30340;&#26410;&#26469;&#26041;&#21521;&#21644;&#28508;&#22312;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21046;&#23450;&#25112;&#30053;&#20915;&#31574;&#26102;&#65292;&#25105;&#20204;&#24120;&#24120;&#38754;&#20020;&#30528;&#22823;&#37327;&#38656;&#35201;&#22788;&#29702;&#30340;&#20449;&#24687;&#12290;&#24403;&#19968;&#20123;&#35777;&#25454;&#30456;&#20114;&#30683;&#30462;&#25110;&#33258;&#30456;&#30683;&#30462;&#26102;&#65292;&#24773;&#20917;&#21487;&#33021;&#20250;&#26356;&#21152;&#22797;&#26434;&#12290;&#27492;&#26102;&#65292;&#38382;&#39064;&#22312;&#20110;&#22914;&#20309;&#30830;&#23450;&#21738;&#20123;&#20449;&#24687;&#26159;&#26377;&#29992;&#30340;&#65292;&#21738;&#20123;&#24212;&#35813;&#34987;&#25490;&#38500;&#12290;&#36825;&#20010;&#36807;&#31243;&#34987;&#31216;&#20026;&#20803;&#20915;&#31574;&#12290;&#21516;&#26679;&#65292;&#22312;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#31995;&#32479;&#36827;&#34892;&#25112;&#30053;&#20915;&#31574;&#26102;&#65292;&#23545;AI&#26412;&#36523;&#30340;&#20449;&#20219;&#23601;&#25104;&#20026;&#20102;&#19968;&#20010;&#20803;&#20915;&#31574;&#65292;&#22240;&#20026;&#35768;&#22810;AI&#31995;&#32479;&#34987;&#35270;&#20026;&#22788;&#29702;&#22823;&#37327;&#25968;&#25454;&#30340;&#19981;&#36879;&#26126;&#8220;&#40657;&#21283;&#23376;&#8221;&#12290;&#20449;&#20219;&#19968;&#20010;&#19981;&#36879;&#26126;&#30340;&#31995;&#32479;&#28041;&#21450;&#20915;&#23450;&#20160;&#20040;&#26679;&#30340;&#21487;&#20449;&#20154;&#24037;&#26234;&#33021;&#65288;TAI&#65289;&#27700;&#24179;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#21363;&#24341;&#20837;&#19968;&#20010;&#26032;&#39062;&#30340;TAI&#20998;&#31867;&#31995;&#32479;&#25110;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21253;&#25324;&#19977;&#20010;&#20851;&#38190;&#39046;&#22495;&#65306;&#34920;&#36798;&#12289;&#30495;&#23454;&#21644;&#22522;&#26412;&#27700;&#24179;&#30340;&#19981;&#21516;&#20449;&#20219;&#32423;&#21035;&#12290;&#20026;&#20102;&#25903;&#25745;&#36825;&#20123;&#39046;&#22495;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#21313;&#20010;&#32500;&#24230;&#26469;&#34913;&#37327;&#20449;&#20219;&#65306;&#21487;&#35299;&#37322;&#24615;/&#36879;&#26126;&#24615;&#12289;&#26080;&#20559;&#24615;&#12289;&#38382;&#36131;&#21046;&#12289;&#20934;&#30830;&#24615;&#12289;&#21487;&#38752;&#24615;&#12289;&#38544;&#31169;&#12289;&#23433;&#20840;&#12289;&#20844;&#24179;&#24615;&#21644;&#20154;&#31867;&#25511;&#21046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#29616;&#26377;TAI&#30740;&#31350;&#30340;&#35843;&#26597;&#21644;&#20803;&#20998;&#26512;&#65292;&#20197;&#20840;&#38754;&#20102;&#35299;&#36825;&#19968;&#26032;&#20852;&#39046;&#22495;&#30340;&#30740;&#31350;&#29616;&#29366;&#12290;&#25105;&#20204;&#36824;&#30830;&#23450;&#20102;TAI&#30740;&#31350;&#30340;&#26410;&#26469;&#26041;&#21521;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20123;&#28508;&#22312;&#24212;&#29992;&#65292;&#36825;&#20123;&#24212;&#29992;&#21487;&#33021;&#20174;TAI&#30340;&#20351;&#29992;&#20013;&#21463;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
When making strategic decisions, we are often confronted with overwhelming information to process. The situation can be further complicated when some pieces of evidence are contradicted each other or paradoxical. The challenge then becomes how to determine which information is useful and which ones should be eliminated. This process is known as meta-decision. Likewise, when it comes to using Artificial Intelligence (AI) systems for strategic decision-making, placing trust in the AI itself becomes a meta-decision, given that many AI systems are viewed as opaque "black boxes" that process large amounts of data. Trusting an opaque system involves deciding on the level of Trustworthy AI (TAI). We propose a new approach to address this issue by introducing a novel taxonomy or framework of TAI, which encompasses three crucial domains: articulate, authentic, and basic for different levels of trust. To underpin these domains, we create ten dimensions to measure trust: explainability/transparen
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#28304;&#20195;&#30721;&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35843;&#26597;&#21644;&#32508;&#36848;&#65292;&#20171;&#32461;&#20102;&#23427;&#20204;&#30340;&#20998;&#31867;&#27861;&#12289;&#20248;&#21270;&#31574;&#30053;&#21644;&#24615;&#33021;&#32467;&#26524;&#65292;&#24182;&#35752;&#35770;&#20102;&#26410;&#26469;&#26041;&#21521;&#21644;&#30740;&#31350;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2305.19915</link><description>&lt;p&gt;
&#28304;&#20195;&#30721;&#27169;&#22411;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65306;&#19968;&#20221;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Data Augmentation Approaches for Source Code Models: A Survey. (arXiv:2305.19915v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19915
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#28304;&#20195;&#30721;&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35843;&#26597;&#21644;&#32508;&#36848;&#65292;&#20171;&#32461;&#20102;&#23427;&#20204;&#30340;&#20998;&#31867;&#27861;&#12289;&#20248;&#21270;&#31574;&#30053;&#21644;&#24615;&#33021;&#32467;&#26524;&#65292;&#24182;&#35752;&#35770;&#20102;&#26410;&#26469;&#26041;&#21521;&#21644;&#30740;&#31350;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28304;&#20195;&#30721;&#22312;&#35768;&#22810;&#20851;&#38190;&#20219;&#21153;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#20419;&#36827;&#20102;&#25968;&#25454;&#22686;&#24378;&#65288;DA&#65289;&#25216;&#26415;&#30340;&#21457;&#23637;&#65292;&#20197;&#22686;&#24378;&#35757;&#32451;&#25968;&#25454;&#24182;&#25552;&#39640;&#36825;&#20123;&#27169;&#22411;&#30340;&#21508;&#31181;&#33021;&#21147;&#65288;&#20363;&#22914;&#20581;&#22766;&#24615;&#21644;&#21487;&#27867;&#21270;&#24615;&#65289;&#12290;&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#24182;&#38024;&#23545;&#28304;&#20195;&#30721;&#27169;&#22411;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;DA&#26041;&#27861;&#30340;&#35843;&#25972;&#65292;&#20294;&#32570;&#20047;&#32508;&#21512;&#24615;&#30340;&#35843;&#26597;&#21644;&#23457;&#26597;&#20197;&#29702;&#35299;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#21644;&#21547;&#20041;&#12290;&#26412;&#25991;&#36890;&#36807;&#23545;&#28304;&#20195;&#30721;&#30340;&#25968;&#25454;&#22686;&#24378;&#36827;&#34892;&#20840;&#38754;&#32780;&#32508;&#21512;&#30340;&#35843;&#26597;&#65292;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#25972;&#29702;&#21644;&#27010;&#36848;&#29616;&#26377;&#25991;&#29486;&#65292;&#20197;&#25552;&#20379;&#35813;&#39046;&#22495;&#30340;&#20840;&#38754;&#27010;&#36848;&#12290;&#25105;&#20204;&#39318;&#20808;&#26500;&#24314;&#20102;&#36866;&#29992;&#20110;&#28304;&#20195;&#30721;&#27169;&#22411;&#30340;&#25968;&#25454;&#22686;&#24378;&#30340;&#20998;&#31867;&#27861;&#65292;&#28982;&#21518;&#35752;&#35770;&#20102;&#33879;&#21517;&#30340;&#12289;&#26041;&#27861;&#19978;&#20855;&#26377;&#35828;&#26126;&#24615;&#30340;&#26041;&#27861;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;&#20248;&#21270;DA&#36136;&#37327;&#30340;&#19968;&#33324;&#31574;&#30053;&#21644;&#25216;&#26415;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;&#22312;&#34987;&#24191;&#27867;&#25509;&#21463;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#21457;&#25381;&#20316;&#29992;&#30340;&#25216;&#26415;&#65292;&#24182;&#21576;&#29616;&#20102;&#23427;&#20204;&#30340;&#24615;&#33021;&#32467;&#26524;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;DA&#29992;&#20110;&#28304;&#20195;&#30721;&#27169;&#22411;&#30340;&#28508;&#22312;&#26410;&#26469;&#26041;&#21521;&#21644;&#24320;&#25918;&#30740;&#31350;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
The increasingly popular adoption of source code in many critical tasks motivates the development of data augmentation (DA) techniques to enhance training data and improve various capabilities (e.g., robustness and generalizability) of these models. Although a series of DA methods have been proposed and tailored for source code models, there lacks a comprehensive survey and examination to understand their effectiveness and implications. This paper fills this gap by conducting a comprehensive and integrative survey of data augmentation for source code, wherein we systematically compile and encapsulate existing literature to provide a comprehensive overview of the field. We start by constructing a taxonomy of DA for source code models model approaches, followed by a discussion on prominent, methodologically illustrative approaches. Next, we highlight the general strategies and techniques to optimize the DA quality. Subsequently, we underscore techniques that find utility in widely-accept
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SuperNorm&#30340;&#19987;&#29992;&#24402;&#19968;&#21270;&#26041;&#26696;&#65292;&#36890;&#36807;&#23884;&#20837;&#23376;&#22270;&#29305;&#23450;&#22240;&#23376;&#21644;&#32435;&#20837;&#22270;&#23454;&#20363;&#29305;&#23450;&#32479;&#35745;&#25968;&#25454;&#26469;&#21152;&#24378;GNN&#30340;&#20195;&#34920;&#24615;&#33021;&#21147;&#65292;&#23454;&#29616;&#23545;&#33410;&#28857;&#24863;&#24212;&#23376;&#22270;&#20013;&#20869;&#37096;&#36830;&#25509;&#20449;&#24687;&#30340;&#26126;&#30830;&#32771;&#34385;&#65292;&#20174;&#32780;&#25913;&#21892;GNN&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.19903</link><description>&lt;p&gt;
&#20351;&#29992;&#23376;&#22270;&#29305;&#23450;&#22240;&#23376;&#23884;&#20837;&#24402;&#19968;&#21270;&#25913;&#21892;GNN&#30340;&#34920;&#36798;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Improving Expressivity of GNNs with Subgraph-specific Factor Embedded Normalization. (arXiv:2305.19903v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19903
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SuperNorm&#30340;&#19987;&#29992;&#24402;&#19968;&#21270;&#26041;&#26696;&#65292;&#36890;&#36807;&#23884;&#20837;&#23376;&#22270;&#29305;&#23450;&#22240;&#23376;&#21644;&#32435;&#20837;&#22270;&#23454;&#20363;&#29305;&#23450;&#32479;&#35745;&#25968;&#25454;&#26469;&#21152;&#24378;GNN&#30340;&#20195;&#34920;&#24615;&#33021;&#21147;&#65292;&#23454;&#29616;&#23545;&#33410;&#28857;&#24863;&#24212;&#23376;&#22270;&#20013;&#20869;&#37096;&#36830;&#25509;&#20449;&#24687;&#30340;&#26126;&#30830;&#32771;&#34385;&#65292;&#20174;&#32780;&#25913;&#21892;GNN&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#24050;&#32463;&#25104;&#20026;&#19968;&#31867;&#22788;&#29702;&#22270;&#32467;&#26500;&#25968;&#25454;&#30340;&#24378;&#22823;&#23398;&#20064;&#26550;&#26500;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;GNN&#36890;&#24120;&#24573;&#30053;&#20102;&#33410;&#28857;&#24863;&#24212;&#23376;&#22270;&#20013;&#30340;&#37325;&#35201;&#32467;&#26500;&#29305;&#24449;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#35774;&#35745;&#19968;&#31181;&#19987;&#29992;&#30340;&#21363;&#25554;&#21363;&#29992;&#24402;&#19968;&#21270;&#26041;&#26696;&#8212;&#8212;SUbgraph-sPEcific FactoR Embedded Normalization&#65288;SuperNorm&#65289;&#26469;&#21152;&#24378;GNN&#30340;&#20195;&#34920;&#24615;&#33021;&#21147;&#65292;&#35813;&#26041;&#26696;&#26126;&#30830;&#32771;&#34385;&#20102;&#27599;&#20010;&#33410;&#28857;&#24863;&#24212;&#23376;&#22270;&#20869;&#37096;&#36830;&#25509;&#30340;&#20449;&#24687;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#22312;&#26631;&#20934;&#30340;BatchNorm&#24320;&#22987;&#21644;&#32467;&#26463;&#26102;&#23884;&#20837;&#20102;&#23376;&#22270;&#29305;&#23450;&#22240;&#23376;&#65292;&#24182;&#32435;&#20837;&#22270;&#23454;&#20363;&#29305;&#23450;&#32479;&#35745;&#25968;&#25454;&#20197;&#25552;&#39640;&#21306;&#20998;&#33021;&#21147;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#29702;&#35770;&#20998;&#26512;&#25903;&#25345;&#65292;&#25351;&#20986;&#36890;&#36807;&#25913;&#21892;&#30340;SuperNorm&#65292;&#20219;&#24847;GNN&#33267;&#23569;&#19982;1-WL&#27979;&#35797;&#19968;&#26679;&#33021;&#22815;&#21306;&#20998;&#38750;&#21516;&#26500;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks~(GNNs) have emerged as a powerful category of learning architecture for handling graph-structured data. However, existing GNNs typically ignore crucial structural characteristics in node-induced subgraphs, which thus limits their expressiveness for various downstream tasks. In this paper, we strive to strengthen the representative capabilities of GNNs by devising a dedicated plug-and-play normalization scheme, termed as SUbgraph-sPEcific FactoR Embedded Normalization (SuperNorm), that explicitly considers the intra-connection information within each node-induced subgraph. To this end, we embed the subgraph-specific factor at the beginning and the end of the standard BatchNorm, as well as incorporate graph instance-specific statistics for improved distinguishable capabilities. In the meantime, we provide theoretical analysis to support that, with the elaborated SuperNorm, an arbitrary GNN is at least as powerful as the 1-WL test in distinguishing non-isomorphism gr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#24335;&#35821;&#35328;&#27169;&#22411;&#30340;&#32454;&#31890;&#24230;&#25991;&#26412;&#39118;&#26684;&#36716;&#25442;&#26041;&#27861;&#65292;&#22312;&#19981;&#20381;&#36182;&#22806;&#37096;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#21462;&#24471;&#20102;&#27604;&#20043;&#21069;&#21033;&#29992;&#39044;&#35757;&#32451;&#26435;&#37325;&#12289;&#23884;&#20837;&#21644;&#22806;&#37096;&#35821;&#27861;&#20998;&#26512;&#22120;&#26356;&#22909;&#30340;&#25928;&#26524;&#65292;&#34920;&#26126;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#22312;&#25991;&#26412;&#29983;&#25104;&#39046;&#22495;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;</title><link>http://arxiv.org/abs/2305.19512</link><description>&lt;p&gt;
&#22522;&#20110;&#25193;&#25955;&#24335;&#35821;&#35328;&#27169;&#22411;&#30340;&#32454;&#31890;&#24230;&#25991;&#26412;&#39118;&#26684;&#36716;&#25442;
&lt;/p&gt;
&lt;p&gt;
Fine-grained Text Style Transfer with Diffusion-Based Language Models. (arXiv:2305.19512v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19512
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#24335;&#35821;&#35328;&#27169;&#22411;&#30340;&#32454;&#31890;&#24230;&#25991;&#26412;&#39118;&#26684;&#36716;&#25442;&#26041;&#27861;&#65292;&#22312;&#19981;&#20381;&#36182;&#22806;&#37096;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#21462;&#24471;&#20102;&#27604;&#20043;&#21069;&#21033;&#29992;&#39044;&#35757;&#32451;&#26435;&#37325;&#12289;&#23884;&#20837;&#21644;&#22806;&#37096;&#35821;&#27861;&#20998;&#26512;&#22120;&#26356;&#22909;&#30340;&#25928;&#26524;&#65292;&#34920;&#26126;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#22312;&#25991;&#26412;&#29983;&#25104;&#39046;&#22495;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#24335;&#27010;&#29575;&#27169;&#22411;&#24050;&#32463;&#22312;&#21487;&#25511;&#21046;&#22320;&#29983;&#25104;&#39640;&#36136;&#37327;&#22270;&#20687;&#19978;&#26174;&#31034;&#20986;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#65292;&#30740;&#31350;&#20154;&#21592;&#24050;&#32463;&#35797;&#22270;&#23558;&#36825;&#31181;&#21487;&#25511;&#24615;&#36816;&#29992;&#21040;&#25991;&#26412;&#29983;&#25104;&#39046;&#22495;&#12290;&#20197;&#21069;&#30340;&#25193;&#25955;&#24335;&#35821;&#35328;&#27169;&#22411;&#30740;&#31350;&#34920;&#26126;&#65292;&#23427;&#20204;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#22806;&#37096;&#30693;&#35782;&#65288;&#22914;&#39044;&#35757;&#32451;&#26435;&#37325;&#65289;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#19988;&#20173;&#28982;&#21487;&#20197;&#23454;&#29616;&#31283;&#23450;&#30340;&#24615;&#33021;&#21644;&#21487;&#25511;&#24615;&#12290; &#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#22312;StylePTB&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#20102;&#19968;&#20010;&#25193;&#25955;&#24335;&#27169;&#22411;&#65292;&#36825;&#26159;&#32454;&#31890;&#24230;&#25991;&#26412;&#39118;&#26684;&#36716;&#25442;&#30340;&#26631;&#20934;&#22522;&#20934;&#12290;&#19982;&#20197;&#21069;&#30340;&#24037;&#20316;&#35780;&#20272;&#20219;&#21153;&#30456;&#27604;&#65292;StylePTB&#20013;&#30340;&#20219;&#21153;&#38656;&#35201;&#23545;&#36755;&#20986;&#25991;&#26412;&#36827;&#34892;&#26356;&#21152;&#31934;&#32454;&#30340;&#25511;&#21046;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#33021;&#22815;&#22312;StylePTB&#19978;&#23454;&#29616;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;&#20010;&#21035;&#21644;&#32452;&#21512;&#36716;&#25442;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#27809;&#26377;&#22806;&#37096;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#20351;&#29992;StylePTB&#30340;&#26377;&#38480;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#20854;&#34920;&#29616;&#20248;&#20110;&#20197;&#21069;&#21033;&#29992;&#39044;&#35757;&#32451;&#26435;&#37325;&#12289;&#23884;&#20837;&#21644;&#22806;&#37096;&#35821;&#27861;&#20998;&#26512;&#22120;&#30340;&#24037;&#20316;&#65292;&#36825;&#21487;&#33021;&#34920;&#26126;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#22312;&#25991;&#26412;&#29983;&#25104;&#39046;&#22495;&#20855;&#26377;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion probabilistic models have shown great success in generating high-quality images controllably, and researchers have tried to utilize this controllability into text generation domain. Previous works on diffusion-based language models have shown that they can be trained without external knowledge (such as pre-trained weights) and still achieve stable performance and controllability. In this paper, we trained a diffusion-based model on StylePTB dataset, the standard benchmark for fine-grained text style transfers. The tasks in StylePTB requires much more refined control over the output text compared to tasks evaluated in previous works, and our model was able to achieve state-of-the-art performance on StylePTB on both individual and compositional transfers. Moreover, our model, trained on limited data from StylePTB without external knowledge, outperforms previous works that utilized pretrained weights, embeddings, and external grammar parsers, and this may indicate that diffusion
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#20013;&#30340;&#19977;&#31181;&#26631;&#31614;&#20559;&#24046;&#25552;&#20986;&#20998;&#31867;&#27861;&#65292;&#24182;&#25552;&#20986;&#19968;&#31181;&#31616;&#21333;&#30340;&#20559;&#24046;&#26657;&#20934;&#26041;&#27861;&#65292;&#20351;&#29992;&#38543;&#26426;&#30340;&#39046;&#22495;&#35789;&#20272;&#31639;&#35821;&#35328;&#27169;&#22411;&#30340;&#26631;&#31614;&#20559;&#24046;&#12290;</title><link>http://arxiv.org/abs/2305.19148</link><description>&lt;p&gt;
&#32531;&#35299;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#26631;&#31614;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
Mitigating Label Biases for In-context Learning. (arXiv:2305.19148v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19148
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#20013;&#30340;&#19977;&#31181;&#26631;&#31614;&#20559;&#24046;&#25552;&#20986;&#20998;&#31867;&#27861;&#65292;&#24182;&#25552;&#20986;&#19968;&#31181;&#31616;&#21333;&#30340;&#20559;&#24046;&#26657;&#20934;&#26041;&#27861;&#65292;&#20351;&#29992;&#38543;&#26426;&#30340;&#39046;&#22495;&#35789;&#20272;&#31639;&#35821;&#35328;&#27169;&#22411;&#30340;&#26631;&#31614;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#30340;&#21508;&#31181;&#35774;&#35745;&#35774;&#32622;&#65292;&#22914;&#36873;&#25321;&#21644;&#39034;&#24207;&#30340;&#19978;&#19979;&#25991;&#31034;&#20363;&#65292;&#21487;&#33021;&#20351;&#27169;&#22411;&#23545;&#26576;&#31181;&#29305;&#23450;&#39044;&#27979;&#20559;&#35265;&#65292;&#32780;&#36825;&#31181;&#39044;&#27979;&#24182;&#19981;&#21453;&#26144;&#23545;&#20219;&#21153;&#30340;&#29702;&#35299;&#12290;&#34429;&#28982;&#35768;&#22810;&#30740;&#31350;&#35752;&#35770;&#20102;&#36825;&#20123;&#35774;&#35745;&#36873;&#25321;&#65292;&#20294;&#23545;&#23427;&#20204;&#36827;&#34892;&#20998;&#31867;&#21644;&#20943;&#32531;&#20854;&#24433;&#21709;&#30340;&#31995;&#32479;&#35843;&#26597;&#24456;&#23569;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20026;&#25991;&#26412;&#20998;&#31867;&#20013;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#20013;&#30340;&#19977;&#31181;&#26631;&#31614;&#20559;&#24046;&#23450;&#20041;&#20102;&#19968;&#20010;&#20998;&#31867;&#27861;&#65306;&#39321;&#33609;&#26631;&#31614;&#20559;&#24046;&#12289;&#19978;&#19979;&#25991;&#26631;&#31614;&#20559;&#24046;&#21644;&#39046;&#22495;&#26631;&#31614;&#20559;&#24046;&#65288;&#25105;&#20204;&#39318;&#27425;&#27010;&#24565;&#21270;&#21644;&#26816;&#27979;&#21040;&#65289;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#20808;&#21069;&#30340;&#26631;&#31614;&#20559;&#24046;&#26657;&#20934;&#26041;&#27861;&#19981;&#33021;&#35299;&#20915;&#25152;&#26377;&#19977;&#31181;&#20559;&#24046;&#12290;&#29305;&#21035;&#26159;&#65292;&#39046;&#22495;&#26631;&#31614;&#20559;&#24046;&#20351;LLM&#22312;&#35768;&#22810;&#20219;&#21153;&#19978;&#21482;&#33021;&#23454;&#29616;&#38543;&#26426;&#32423;&#21035;&#30340;&#24615;&#33021;&#65292;&#32780;&#19981;&#31649;&#19978;&#19979;&#25991;&#31034;&#20363;&#30340;&#36873;&#25321;&#22914;&#20309;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20123;&#20559;&#24046;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#20010;&#31616;&#21333;&#30340;&#20559;&#24046;&#26657;&#20934;&#26041;&#27861;&#65292;&#20351;&#29992;&#38543;&#26426;&#30340;&#39046;&#22495;&#35789;&#20272;&#31639;&#35821;&#35328;&#27169;&#22411;&#30340;&#26631;&#31614;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Various design settings for in-context learning (ICL), such as the choice and order of the in-context examples, can bias a model toward a particular prediction without being reflective of an understanding of the task. While many studies discuss these design choices, there have been few systematic investigations into categorizing them and mitigating their impact. In this work, we define a typology for three types of label biases in ICL for text classification: vanilla-label bias, context-label bias, and domain-label bias (which we conceptualize and detect for the first time).  Our analysis demonstrates that prior label bias calibration methods fall short of addressing all three types of biases. Specifically, domain-label bias restricts LLMs to random-level performance on many tasks regardless of the choice of in-context examples. To mitigate the effect of these biases, we propose a simple bias calibration method that estimates a language model's label bias using random in-domain words f
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;PROPETL&#26041;&#27861;&#65292;&#36890;&#36807;&#21407;&#22411;&#32593;&#32476;&#21644;&#20108;&#36827;&#21046;&#25513;&#30721;&#23454;&#29616;&#20102;&#26356;&#39640;&#25928;&#30340;&#21442;&#25968;&#20849;&#29992;&#36801;&#31227;&#23398;&#20064;&#65292;&#35299;&#20915;&#20102;&#22810;&#20219;&#21153;&#24494;&#35843;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#23384;&#20648;&#31354;&#38388;&#21344;&#29992;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.17682</link><description>&lt;p&gt;
&#19968;&#32593;&#32476;&#65292;&#22810;&#20219;&#21153;&#65306;&#26356;&#39640;&#25928;&#30340;&#21442;&#25968;&#20849;&#29992;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
One Network, Many Masks: Towards More Parameter-Efficient Transfer Learning. (arXiv:2305.17682v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17682
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;PROPETL&#26041;&#27861;&#65292;&#36890;&#36807;&#21407;&#22411;&#32593;&#32476;&#21644;&#20108;&#36827;&#21046;&#25513;&#30721;&#23454;&#29616;&#20102;&#26356;&#39640;&#25928;&#30340;&#21442;&#25968;&#20849;&#29992;&#36801;&#31227;&#23398;&#20064;&#65292;&#35299;&#20915;&#20102;&#22810;&#20219;&#21153;&#24494;&#35843;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#23384;&#20648;&#31354;&#38388;&#21344;&#29992;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#22810;&#20010;&#20219;&#21153;&#26469;&#35828;&#65292;&#24494;&#35843;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#24448;&#24448;&#20250;&#21344;&#29992;&#22823;&#37327;&#23384;&#20648;&#31354;&#38388;&#65292;&#21442;&#25968;&#20849;&#29992;&#36801;&#31227;&#23398;&#20064;&#65288;PETL&#65289;&#26041;&#27861;&#21487;&#20197;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#22312;&#24212;&#29992;&#20110;&#26356;&#24191;&#27867;&#30340;&#20219;&#21153;&#33539;&#22260;&#26102;&#20173;&#38656;&#35201;&#22823;&#37327;&#30340;&#21442;&#25968;&#21644;&#23384;&#20648;&#31354;&#38388;&#12290;&#20026;&#20102;&#23454;&#29616;&#26356;&#22823;&#30340;&#23384;&#20648;&#31354;&#38388;&#20943;&#23569;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PROPETL&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#21516;&#23618;&#21644;&#20219;&#21153;&#20043;&#38388;&#20351;&#29992;&#21333;&#20010;PETL&#27169;&#22359;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#21407;&#22411;&#32593;&#32476;&#65288;&#20363;&#22914;&#36866;&#37197;&#22120;&#12289;LoRA&#21644;&#21069;&#32512;&#35843;&#25972;&#65289;&#12290;&#25105;&#20204;&#28982;&#21518;&#23398;&#20064;&#20108;&#36827;&#21046;&#25513;&#30721;&#20197;&#20174;&#20849;&#20139;&#30340;&#21407;&#22411;&#32593;&#32476;&#20013;&#36873;&#25321;&#19981;&#21516;&#30340;&#23376;&#32593;&#32476;&#65292;&#24182;&#23558;&#23427;&#20204;&#20316;&#20026;PETL&#27169;&#22359;&#24212;&#29992;&#20110;&#19981;&#21516;&#30340;&#23618;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20108;&#36827;&#21046;&#25513;&#30721;&#21487;&#20197;&#30830;&#23450;&#32593;&#32476;&#20013;&#20851;&#38190;&#30340;&#20449;&#24687;&#65292;&#36825;&#22312;&#20197;&#21069;&#30340;&#30740;&#31350;&#20013;&#32463;&#24120;&#34987;&#24573;&#30053;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#20063;&#21487;&#20197;&#30475;&#20316;&#26159;&#19968;&#31181;&#20462;&#21098;&#26041;&#27861;&#65292;&#25105;&#20204;&#21457;&#29616;&#21363;&#20351;&#22312;&#30475;&#20284;&#24456;&#23567;&#30340;PETL&#27169;&#22359;&#20013;&#20063;&#23384;&#22312;&#36807;&#24230;&#21442;&#25968;&#21270;&#12290;&#25105;&#20204;&#23545;PROPETL&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fine-tuning pre-trained language models for multiple tasks tends to be expensive in terms of storage. To mitigate this, parameter-efficient transfer learning (PETL) methods have been proposed to address this issue, but they still require a significant number of parameters and storage when being applied to broader ranges of tasks. To achieve even greater storage reduction, we propose PROPETL, a novel method that enables efficient sharing of a single PETL module which we call prototype network (e.g., adapter, LoRA, and prefix-tuning) across layers and tasks. We then learn binary masks to select different sub-networks from the shared prototype network and apply them as PETL modules into different layers. We find that the binary masks can determine crucial information from the network, which is often ignored in previous studies. Our work can also be seen as a type of pruning method, where we find that overparameterization also exists in the seemingly small PETL modules. We evaluate PROPETL
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22330;&#26223;&#22270;&#35760;&#24518;&#29366;&#24577;&#34920;&#31034;&#65292;&#32467;&#21512;&#33410;&#28857;&#36793;&#32536;&#39044;&#27979;&#22120;&#65288;NEP&#65289;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#33021;&#22815;&#24110;&#21161;&#20855;&#26377;&#34892;&#21160;&#33021;&#21147;&#30340;AI&#20195;&#29702;&#22312;&#37096;&#20998;&#21487;&#35266;&#23519;&#21160;&#24577;&#22330;&#26223;&#20013;&#39640;&#25928;&#25628;&#32034;&#12290;</title><link>http://arxiv.org/abs/2305.17537</link><description>&lt;p&gt;
&#20351;&#29992;&#22330;&#26223;&#22270;&#35760;&#24518;&#24314;&#27169;&#21160;&#24577;&#29615;&#22659;
&lt;/p&gt;
&lt;p&gt;
Modeling Dynamic Environments with Scene Graph Memory. (arXiv:2305.17537v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17537
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22330;&#26223;&#22270;&#35760;&#24518;&#29366;&#24577;&#34920;&#31034;&#65292;&#32467;&#21512;&#33410;&#28857;&#36793;&#32536;&#39044;&#27979;&#22120;&#65288;NEP&#65289;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#33021;&#22815;&#24110;&#21161;&#20855;&#26377;&#34892;&#21160;&#33021;&#21147;&#30340;AI&#20195;&#29702;&#22312;&#37096;&#20998;&#21487;&#35266;&#23519;&#21160;&#24577;&#22330;&#26223;&#20013;&#39640;&#25928;&#25628;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#22411;&#29615;&#22659;&#20013;&#65292;&#22914;&#23621;&#23460;&#31561;&#65292;&#23547;&#25214;&#29289;&#21697;&#30340;&#20855;&#26377;&#34892;&#21160;&#33021;&#21147;&#30340;AI&#20195;&#29702;&#38656;&#35201;&#22522;&#20110;&#37096;&#20998;&#20449;&#24687;&#39044;&#27979;&#29289;&#21697;&#20301;&#32622;&#26469;&#20570;&#20986;&#26377;&#25928;&#20915;&#31574;&#12290;&#25105;&#20204;&#23558;&#20854;&#24418;&#24335;&#21270;&#20026;&#19968;&#31181;&#26032;&#31867;&#22411;&#30340;&#38142;&#36335;&#39044;&#27979;&#38382;&#39064;&#65306;&#37096;&#20998;&#21487;&#35266;&#23519;&#21160;&#24577;&#22270;&#19978;&#30340;&#38142;&#36335;&#39044;&#27979;&#12290;&#25105;&#20204;&#30340;&#22270;&#34920;&#36798;&#20102;&#19968;&#20010;&#22330;&#26223;&#65292;&#20854;&#20013;&#25151;&#38388;&#21644;&#29289;&#21697;&#26159;&#33410;&#28857;&#65292;&#22312;&#36793;&#32536;&#20013;&#32534;&#30721;&#23427;&#20204;&#20043;&#38388;&#30340;&#20851;&#31995;&#65307;&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;&#39588;&#19978;&#65292;&#20195;&#29702;&#20154;&#20165;&#30693;&#36947;&#26356;&#25913;&#22270;&#30340;&#37096;&#20998;&#12290;&#36825;&#31181;&#37096;&#20998;&#21487;&#35266;&#27979;&#24615;&#23545;&#20110;&#29616;&#26377;&#30340;&#38142;&#36335;&#39044;&#27979;&#26041;&#27861;&#26500;&#25104;&#20102;&#25361;&#25112;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#35299;&#20915;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29366;&#24577;&#34920;&#31034; - &#22330;&#26223;&#22270;&#35760;&#24518;&#65288;SGM&#65289; - &#20854;&#20013;&#21253;&#25324;&#20195;&#29702;&#20154;&#30340;&#32047;&#31215;&#35266;&#23519;&#38598;&#21512;&#65292;&#20197;&#21450;&#19968;&#31181;&#21517;&#20026;&#33410;&#28857;&#36793;&#32536;&#39044;&#27979;&#22120;&#65288;NEP&#65289;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#35813;&#26550;&#26500;&#20174;SGM&#20013;&#25552;&#21462;&#20449;&#24687;&#20197;&#36827;&#34892;&#39640;&#25928;&#25628;&#32034;&#12290;&#25105;&#20204;&#22312;&#21160;&#24577;&#25151;&#23627;&#27169;&#25311;&#22120;&#20013;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#65292;&#23427;&#25353;&#29031;&#35821;&#20041;&#27169;&#24335;&#21019;&#24314;&#19981;&#21516;&#30340;&#21160;&#24577;&#22270;&#24418;&#12290;
&lt;/p&gt;
&lt;p&gt;
Embodied AI agents that search for objects in large environments such as households often need to make efficient decisions by predicting object locations based on partial information. We pose this as a new type of link prediction problem: link prediction on partially observable dynamic graphs. Our graph is a representation of a scene in which rooms and objects are nodes, and their relationships are encoded in the edges; only parts of the changing graph are known to the agent at each timestep. This partial observability poses a challenge to existing link prediction approaches, which we address. We propose a novel state representation -- Scene Graph Memory (SGM) -- with captures the agent's accumulated set of observations, as well as a neural net architecture called a Node Edge Predictor (NEP) that extracts information from the SGM to search efficiently. We evaluate our method in the Dynamic House Simulator, a new benchmark that creates diverse dynamic graphs following the semantic patte
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#20195;&#30721;&#25628;&#32034;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#38382;&#39064;&#65292;&#25351;&#20986;&#25915;&#20987;&#32773;&#21487;&#20197;&#27880;&#20837;&#21518;&#38376;&#26469;&#36820;&#22238;&#20855;&#26377;&#23433;&#20840;/&#38544;&#31169;&#38382;&#39064;&#30340;&#20195;&#30721;&#65292;&#25552;&#20986;&#20102;&#20960;&#31181;&#38450;&#24481;&#26426;&#21046;&#26469;&#32531;&#35299;&#36825;&#31181;&#23041;&#32961;&#65292;&#35813;&#24037;&#20316;&#31361;&#26174;&#20102;&#30740;&#31350;AI&#31995;&#32479;&#23433;&#20840;&#26041;&#38754;&#30340;&#37325;&#35201;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#37096;&#32626;&#20110;&#23433;&#20840;&#20851;&#38190;&#39046;&#22495;&#26102;&#12290;</title><link>http://arxiv.org/abs/2305.17506</link><description>&lt;p&gt;
&#31070;&#32463;&#20195;&#30721;&#25628;&#32034;&#20013;&#30340;&#21518;&#38376;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Backdooring Neural Code Search. (arXiv:2305.17506v2 [cs.SE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17506
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#20195;&#30721;&#25628;&#32034;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#38382;&#39064;&#65292;&#25351;&#20986;&#25915;&#20987;&#32773;&#21487;&#20197;&#27880;&#20837;&#21518;&#38376;&#26469;&#36820;&#22238;&#20855;&#26377;&#23433;&#20840;/&#38544;&#31169;&#38382;&#39064;&#30340;&#20195;&#30721;&#65292;&#25552;&#20986;&#20102;&#20960;&#31181;&#38450;&#24481;&#26426;&#21046;&#26469;&#32531;&#35299;&#36825;&#31181;&#23041;&#32961;&#65292;&#35813;&#24037;&#20316;&#31361;&#26174;&#20102;&#30740;&#31350;AI&#31995;&#32479;&#23433;&#20840;&#26041;&#38754;&#30340;&#37325;&#35201;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#37096;&#32626;&#20110;&#23433;&#20840;&#20851;&#38190;&#39046;&#22495;&#26102;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#22312;&#32447;&#20195;&#30721;&#24211;&#20013;&#37325;&#22797;&#20351;&#29992;&#29616;&#25104;&#20195;&#30721;&#26159;&#24120;&#35265;&#30340;&#20570;&#27861;&#65292;&#23427;&#26497;&#22823;&#22320;&#25552;&#39640;&#20102;&#36719;&#20214;&#24320;&#21457;&#20154;&#21592;&#30340;&#29983;&#20135;&#21147;&#12290;&#35201;&#26597;&#25214;&#25152;&#38656;&#30340;&#20195;&#30721;&#29255;&#27573;&#65292;&#24320;&#21457;&#20154;&#21592;&#21017;&#35201;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#26597;&#35810;&#20351;&#29992;&#20195;&#30721;&#25628;&#32034;&#24341;&#25806;&#12290;&#22240;&#27492;&#65292;&#22312;&#35768;&#22810;&#36825;&#26679;&#30340;&#25628;&#32034;&#24341;&#25806;&#21518;&#38754;&#65292;&#37117;&#26159;&#31070;&#32463;&#20195;&#30721;&#25628;&#32034;&#27169;&#22411;&#12290;&#36825;&#20123;&#27169;&#22411;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#65292;&#22240;&#20854;&#20986;&#33394;&#30340;&#24615;&#33021;&#32780;&#22791;&#21463;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#24456;&#23569;&#34987;&#30740;&#31350;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25915;&#20987;&#32773;&#21487;&#33021;&#20250;&#22312;&#31070;&#32463;&#20195;&#30721;&#25628;&#32034;&#27169;&#22411;&#20013;&#27880;&#20837;&#21518;&#38376;&#65292;&#20174;&#32780;&#36820;&#22238;&#20855;&#26377;&#23433;&#20840;/&#38544;&#31169;&#38382;&#39064;&#30340;&#38169;&#35823;&#25110;&#29978;&#33267;&#26131;&#21463;&#25915;&#20987;&#30340;&#20195;&#30721;&#12290;&#36825;&#21487;&#33021;&#20250;&#24433;&#21709;&#19979;&#28216;&#36719;&#20214;&#65288;&#20363;&#22914;&#32929;&#31080;&#20132;&#26131;&#31995;&#32479;&#21644;&#33258;&#21160;&#39550;&#39542;&#65289;&#65292;&#24182;&#36896;&#25104;&#36130;&#21153;&#25439;&#22833;&#21644;/&#25110;&#21361;&#21450;&#29983;&#21629;&#30340;&#20107;&#20214;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#26679;&#30340;&#25915;&#20987;&#26159;&#21487;&#34892;&#30340;&#65292;&#24182;&#19988;&#21487;&#33021;&#38750;&#24120;&#38544;&#34109;&#12290;&#36890;&#36807;&#31616;&#21333;&#22320;&#20462;&#25913;&#19968;&#20010;&#21464;&#37327;/&#20989;&#25968;&#21517;&#31216;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#20351;&#20986;&#29616;&#38169;&#35823;/&#26131;&#21463;&#25915;&#20987;&#30340;&#20195;&#30721;&#25490;&#21517;&#21069;11&#65285;&#12290;&#25105;&#20204;&#30340;&#25915;&#20987;&#21033;&#29992;&#20102;&#31070;&#32463;&#32593;&#32476;&#30340;&#21487;&#20877;&#35757;&#32451;&#24615;&#21644;&#20195;&#30721;&#30456;&#20284;&#24230;&#24230;&#37327;&#30340;&#23485;&#26494;&#24615;&#65292;&#20197;&#27880;&#20837;&#21518;&#38376;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#20960;&#31181;&#38450;&#24481;&#26426;&#21046;&#26469;&#32531;&#35299;&#36825;&#31181;&#23041;&#32961;&#65292;&#20363;&#22914;&#28155;&#21152;&#23545;&#25239;&#24615;&#35757;&#32451;&#25968;&#25454;&#21644;&#29305;&#24449;&#21387;&#32553;&#12290;&#25105;&#20204;&#30456;&#20449;&#25105;&#20204;&#30340;&#24037;&#20316;&#31361;&#26174;&#20102;&#30740;&#31350;AI&#31995;&#32479;&#23433;&#20840;&#26041;&#38754;&#30340;&#37325;&#35201;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#37096;&#32626;&#20110;&#23433;&#20840;&#20851;&#38190;&#39046;&#22495;&#26102;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reusing off-the-shelf code snippets from online repositories is a common practice, which significantly enhances the productivity of software developers. To find desired code snippets, developers resort to code search engines through natural language queries. Neural code search models are hence behind many such engines. These models are based on deep learning and gain substantial attention due to their impressive performance. However, the security aspect of these models is rarely studied. Particularly, an adversary can inject a backdoor in neural code search models, which return buggy or even vulnerable code with security/privacy issues. This may impact the downstream software (e.g., stock trading systems and autonomous driving) and cause financial loss and/or life-threatening incidents. In this paper, we demonstrate such attacks are feasible and can be quite stealthy. By simply modifying one variable/function name, the attacker can make buggy/vulnerable code rank in the top 11%. Our at
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#36890;&#36807;&#27169;&#31946;&#27979;&#35797;&#33719;&#21462;&#20195;&#34920;&#24615;&#36755;&#20837;&#26469;&#24110;&#21161;&#35821;&#20041;&#29702;&#35299;&#31243;&#24207;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.13592</link><description>&lt;p&gt;
&#21033;&#29992;&#65288;&#27169;&#31946;&#27979;&#35797;&#65289;&#27979;&#35797;&#29992;&#20363;&#26469;&#29702;&#35299;&#31243;&#24207;
&lt;/p&gt;
&lt;p&gt;
Understanding Programs by Exploiting (Fuzzing) Test Cases. (arXiv:2305.13592v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13592
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#36890;&#36807;&#27169;&#31946;&#27979;&#35797;&#33719;&#21462;&#20195;&#34920;&#24615;&#36755;&#20837;&#26469;&#24110;&#21161;&#35821;&#20041;&#29702;&#35299;&#31243;&#24207;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31243;&#24207;&#30340;&#35821;&#20041;&#29702;&#35299;&#24341;&#36215;&#20102;&#31038;&#21306;&#30340;&#26497;&#22823;&#20851;&#27880;&#12290;&#21463;&#21040;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20013;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#26368;&#36817;&#25104;&#21151;&#21551;&#21457;&#65292;&#36890;&#36807;&#23558;&#32534;&#31243;&#35821;&#35328;&#35270;&#20026;&#21478;&#19968;&#31181;&#33258;&#28982;&#35821;&#35328;&#65292;&#24182;&#22312;&#31243;&#24207;&#20195;&#30721;&#35821;&#26009;&#24211;&#19978;&#35757;&#32451;LLM&#65292;&#21462;&#24471;&#20102;&#24040;&#22823;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#31243;&#24207;&#27605;&#31455;&#19982;&#25991;&#26412;&#26377;&#26412;&#36136;&#30340;&#21306;&#21035;&#65292;&#22240;&#20026;&#23427;&#20204;&#36890;&#24120;&#20855;&#26377;&#20005;&#26684;&#30340;&#32467;&#26500;&#21644;&#35821;&#27861;&#12290;&#29305;&#21035;&#26159;&#65292;&#31243;&#24207;&#21450;&#20854;&#22522;&#26412;&#21333;&#20803;&#65288;&#21363;&#20989;&#25968;&#21644;&#23376;&#31243;&#24207;&#65289;&#26088;&#22312;&#23637;&#31034;&#21508;&#31181;&#34892;&#20026;&#21644;/&#25110;&#25552;&#20379;&#21487;&#33021;&#30340;&#36755;&#20986;&#65292;&#32473;&#23450;&#19981;&#21516;&#30340;&#36755;&#20837;&#12290;&#36755;&#20837;&#21644;&#21487;&#33021;&#30340;&#36755;&#20986;/&#34892;&#20026;&#20043;&#38388;&#30340;&#20851;&#31995;&#34920;&#31034;&#20989;&#25968;/&#23376;&#31243;&#24207;&#65292;&#24182;&#27010;&#36848;&#20102;&#25972;&#20010;&#31243;&#24207;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;&#36825;&#31181;&#20851;&#31995;&#32435;&#20837;&#23398;&#20064;&#20013;&#65292;&#20197;&#23454;&#29616;&#23545;&#31243;&#24207;&#30340;&#26356;&#28145;&#20837;&#35821;&#20041;&#29702;&#35299;&#12290;&#20026;&#20102;&#33719;&#24471;&#36275;&#22815;&#20195;&#34920;&#24615;&#30340;&#36755;&#20837;&#20197;&#35302;&#21457;&#22823;&#37327;&#25191;&#34892;&#65292;&#21487;&#20197;&#20351;&#29992;&#27169;&#31946;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Semantic understanding of programs has attracted great attention in the community. Inspired by recent successes of large language models (LLMs) in natural language understanding, tremendous progress has been made by treating programming language as another sort of natural language and training LLMs on corpora of program code. However, programs are essentially different from texts after all, in a sense that they are normally heavily structured and syntax-strict. In particular, programs and their basic units (i.e., functions and subroutines) are designed to demonstrate a variety of behaviors and/or provide possible outputs, given different inputs. The relationship between inputs and possible outputs/behaviors represents the functions/subroutines and profiles the program as a whole. Therefore, we propose to incorporate such a relationship into learning, for achieving a deeper semantic understanding of programs. To obtain inputs that are representative enough to trigger the execution of mo
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#37319;&#29992;&#22522;&#20110;&#25552;&#31034;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#22312;&#23398;&#26415;&#30693;&#35782;&#22270;&#35889;&#23545;&#35937;&#39044;&#27979;&#39046;&#22495;&#36827;&#34892;&#20102;&#22823;&#35268;&#27169;transformers&#27169;&#22411;&#30340;&#35780;&#20272;&#21644;&#27979;&#35797;&#65292;&#21457;&#29616;&#25552;&#31034;&#30340;&#20351;&#29992;&#21487;&#20197;&#25913;&#36827;pre-trained transformers&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.12900</link><description>&lt;p&gt;
&#22522;&#20110;&#25552;&#31034;&#30340;&#38382;&#39064;&#22238;&#31572;&#24212;&#29992;&#20110;&#24320;&#25918;&#30740;&#31350;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#23545;&#35937;&#39044;&#27979;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Evaluating Prompt-based Question Answering for Object Prediction in the Open Research Knowledge Graph. (arXiv:2305.12900v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12900
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#37319;&#29992;&#22522;&#20110;&#25552;&#31034;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#22312;&#23398;&#26415;&#30693;&#35782;&#22270;&#35889;&#23545;&#35937;&#39044;&#27979;&#39046;&#22495;&#36827;&#34892;&#20102;&#22823;&#35268;&#27169;transformers&#27169;&#22411;&#30340;&#35780;&#20272;&#21644;&#27979;&#35797;&#65292;&#21457;&#29616;&#25552;&#31034;&#30340;&#20351;&#29992;&#21487;&#20197;&#25913;&#36827;pre-trained transformers&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#23545;&#20110;&#22522;&#20110;&#25552;&#31034;&#30340;&#35757;&#32451;&#26041;&#27861;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#19979;&#23545;&#36716;&#25442;&#22120;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#26032;&#25991;&#26412;&#20307;&#35009;&#35757;&#32451;&#30340;&#35843;&#26597;&#26377;&#24456;&#22810;&#12290;&#21457;&#29616;&#22522;&#20110;&#25552;&#31034;&#30340;&#35757;&#32451;&#26041;&#27861;&#23545;&#20110;&#36890;&#29992;&#39044;&#35757;&#32451;&#25110;&#24494;&#35843;&#27169;&#22411;&#20197;&#36866;&#24212;&#36164;&#28304;&#32570;&#20047;&#30340;&#29615;&#22659;&#26377;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;&#26412;&#30740;&#31350;&#39318;&#27425;&#25253;&#36947;&#20102;&#37319;&#29992;&#22522;&#20110;&#25552;&#31034;&#35757;&#32451;transformers&#36827;&#34892;&#8220;&#23398;&#26415;&#30693;&#35782;&#22270;&#35889;&#23545;&#35937;&#39044;&#27979;&#8221;&#30340;&#32467;&#26524;&#12290;&#35813;&#30740;&#31350;&#20855;&#26377;&#20197;&#19979;&#20004;&#20010;&#20027;&#35201;&#29305;&#28857;&#12290;1&#65289;&#23427;&#20559;&#31163;&#20102;&#20854;&#20182;&#25552;&#20986;&#29992;&#20110;&#39044;&#27979;&#23398;&#26415;&#30693;&#35782;&#22270;&#35889;&#23545;&#35937;&#30340;&#23454;&#20307;&#21644;&#20851;&#31995;&#25552;&#21462;&#27969;&#31243;&#30340;&#30740;&#31350;&#12290;2&#65289;&#22312;&#20854;&#20182;&#30740;&#31350;&#20013;&#27979;&#35797;&#20102;&#35813;&#26041;&#27861;&#23545;&#20110;&#19982;&#36890;&#29992;&#30693;&#35782;&#39046;&#22495;&#30456;&#23545;&#25509;&#36817;&#30340;&#25991;&#26412;&#20307;&#35009;&#65292;&#32780;&#25105;&#20204;&#27979;&#35797;&#20102;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#26174;&#33879;&#19981;&#21516;&#30340;&#23398;&#26415;&#30693;&#35782;&#39046;&#22495;&#65292;&#20174;&#32780;&#27979;&#35797;&#36825;&#20123;&#22823;&#35268;&#27169;transformers&#27169;&#22411;&#30340;&#35821;&#35328;&#65292;&#27010;&#29575;&#21644;&#20107;&#23454;&#30340;&#26222;&#36866;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#65288;i&#65289;&#31526;&#21512;&#39044;&#26399;&#65292;&#20351;&#29992;&#25552;&#31034;&#36827;&#34892;&#24494;&#35843;&#30340;transformers&#20248;&#20110;&#22522;&#32447;&#65307;&#65288;ii&#65289;&#19982;&#20808;&#21069;&#30740;&#31350;&#20013;&#30475;&#21040;&#30340;&#27169;&#24335;&#19981;&#21516;&#65292;&#39044;&#20808;&#35757;&#32451;&#30340;transformers&#24182;&#19981;&#33021;&#22987;&#32456;&#36275;&#20197;&#32988;&#20219;&#23398;&#26415;&#23545;&#35937;&#39044;&#27979;&#30340;&#20219;&#21153;&#65292;&#32467;&#26524;&#34920;&#26126;&#25552;&#31034;&#30830;&#23454;&#26377;&#21161;&#20110;&#25913;&#36827;&#25277;&#21462;&#39044;&#35757;&#32451;&#27169;&#22411;&#25152;&#33719;&#24471;&#30340;&#35821;&#20041;&#20449;&#24687;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
There have been many recent investigations into prompt-based training of transformer language models for new text genres in low-resource settings. The prompt-based training approach has been found to be effective in generalizing pre-trained or fine-tuned models for transfer to resource-scarce settings. This work, for the first time, reports results on adopting prompt-based training of transformers for \textit{scholarly knowledge graph object prediction}. The work is unique in the following two main aspects. 1) It deviates from the other works proposing entity and relation extraction pipelines for predicting objects of a scholarly knowledge graph. 2) While other works have tested the method on text genera relatively close to the general knowledge domain, we test the method for a significantly different domain, i.e. scholarly knowledge, in turn testing the linguistic, probabilistic, and factual generalizability of these large-scale transformer models. We find that (i) per expectations, t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#34701;&#21512;&#39033;&#30446;&#30456;&#20851;&#24615;&#30340;&#26032;&#22411;&#35757;&#32451;&#25439;&#22833;&#20989;&#25968;&#65292;&#29992;&#20110;&#25552;&#39640;&#24207;&#21015;&#25512;&#33616;&#31995;&#32479;&#23545;&#22122;&#22768;&#30340;&#40065;&#26834;&#24615;&#21644;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.10824</link><description>&lt;p&gt;
&#34701;&#21512;&#39033;&#30446;&#30456;&#20851;&#24615;&#30340;&#24207;&#21015;&#25512;&#33616;&#31995;&#32479;&#35757;&#32451;&#25439;&#22833;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
Integrating Item Relevance in Training Loss for Sequential Recommender Systems. (arXiv:2305.10824v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10824
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#34701;&#21512;&#39033;&#30446;&#30456;&#20851;&#24615;&#30340;&#26032;&#22411;&#35757;&#32451;&#25439;&#22833;&#20989;&#25968;&#65292;&#29992;&#20110;&#25552;&#39640;&#24207;&#21015;&#25512;&#33616;&#31995;&#32479;&#23545;&#22122;&#22768;&#30340;&#40065;&#26834;&#24615;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24207;&#21015;&#25512;&#33616;&#31995;&#32479;&#26159;&#19968;&#31181;&#21463;&#27426;&#36814;&#30340;&#25512;&#33616;&#31995;&#32479;&#65292;&#23427;&#36890;&#36807;&#23398;&#20064;&#29992;&#25143;&#30340;&#21382;&#21490;&#25968;&#25454;&#26469;&#39044;&#27979;&#29992;&#25143;&#19979;&#19968;&#20010;&#21487;&#33021;&#19982;&#20043;&#20132;&#20114;&#30340;&#39033;&#30446;&#12290;&#28982;&#32780;&#65292;&#29992;&#25143;&#30340;&#20132;&#20114;&#21487;&#33021;&#20250;&#21463;&#21040;&#26469;&#33258;&#24080;&#25143;&#20849;&#20139;&#12289;&#19981;&#19968;&#33268;&#30340;&#20559;&#22909;&#25110;&#24847;&#22806;&#28857;&#20987;&#31561;&#22122;&#22768;&#30340;&#24433;&#21709;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#65288;i&#65289;&#25552;&#20986;&#20102;&#19968;&#20010;&#32771;&#34385;&#22810;&#20010;&#26410;&#26469;&#39033;&#30446;&#30340;&#26032;&#30340;&#35780;&#20272;&#21327;&#35758;&#65292;&#65288;ii&#65289;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#20851;&#27880;&#30456;&#20851;&#24615;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#29992;&#20110;&#35757;&#32451;&#20855;&#26377;&#22810;&#20010;&#26410;&#26469;&#39033;&#30446;&#30340;&#24207;&#21015;&#25512;&#33616;&#31995;&#32479;&#65292;&#20197;&#20351;&#20854;&#23545;&#22122;&#22768;&#26356;&#21152;&#40065;&#26834;&#12290;&#25105;&#20204;&#30340;&#20851;&#27880;&#30456;&#20851;&#24615;&#27169;&#22411;&#22312;&#20256;&#32479;&#35780;&#20272;&#21327;&#35758;&#20013;&#25552;&#39640;&#20102;NDCG@10&#32422;1.2%&#21644;HR&#32422;0.88%&#65292;&#32780;&#22312;&#26032;&#35780;&#20272;&#21327;&#35758;&#20013;&#65292;&#25913;&#36827;&#30340;NDCG@10&#32422;1.63%&#21644;HR&#32422;1.5%&#12290;
&lt;/p&gt;
&lt;p&gt;
Sequential Recommender Systems (SRSs) are a popular type of recommender system that learns from a user's history to predict the next item they are likely to interact with. However, user interactions can be affected by noise stemming from account sharing, inconsistent preferences, or accidental clicks. To address this issue, we (i) propose a new evaluation protocol that takes multiple future items into account and (ii) introduce a novel relevance-aware loss function to train a SRS with multiple future items to make it more robust to noise. Our relevance-aware models obtain an improvement of ~1.2% of NDCG@10 and 0.88% in the traditional evaluation protocol, while in the new evaluation protocol, the improvement is ~1.63% of NDCG@10 and ~1.5% of HR w.r.t the best performing models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#30005;&#23376;&#30149;&#21382;&#20013;&#27745;&#21517;&#21270;&#35821;&#35328;&#23545;&#22522;&#20110;Transformer&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21644;&#21487;&#35299;&#37322;AI(XAI)&#25216;&#26415;&#36827;&#34892;&#27515;&#20129;&#39044;&#27979;&#30340;&#24433;&#21709;&#12290;&#21457;&#29616;&#20020;&#24202;&#21307;&#29983;&#25152;&#20889;&#30340;SL&#20250;&#23545;AI&#24615;&#33021;&#34920;&#29616;&#19981;&#21033;&#65292;&#23588;&#20854;&#26159;&#22312;&#40657;&#20154;&#24739;&#32773;&#20013;&#34920;&#29616;&#26356;&#20026;&#26126;&#26174;&#65292;&#24378;&#35843;&#20102;&#29702;&#35299;&#20559;&#35265;&#23545;&#19979;&#28216;AI&#24615;&#33021;&#30340;&#24433;&#21709;&#30340;&#37325;&#35201;&#24615;&#65292;&#20197;&#24320;&#21457;&#26356;&#20855;&#20844;&#24179;&#21644;&#27491;&#20041;&#30340;&#21307;&#30103;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2305.10201</link><description>&lt;p&gt;
&#20154;&#20204;&#20132;&#35848;&#65292;AI&#20542;&#21548;&#65306;&#30005;&#23376;&#30149;&#21382;&#20013;&#27745;&#21517;&#21270;&#35821;&#35328;&#23545;AI&#21028;&#26029;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
People Talking and AI Listening: How Stigmatizing Language in EHR Notes Affect AI Performance. (arXiv:2305.10201v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10201
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#30005;&#23376;&#30149;&#21382;&#20013;&#27745;&#21517;&#21270;&#35821;&#35328;&#23545;&#22522;&#20110;Transformer&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21644;&#21487;&#35299;&#37322;AI(XAI)&#25216;&#26415;&#36827;&#34892;&#27515;&#20129;&#39044;&#27979;&#30340;&#24433;&#21709;&#12290;&#21457;&#29616;&#20020;&#24202;&#21307;&#29983;&#25152;&#20889;&#30340;SL&#20250;&#23545;AI&#24615;&#33021;&#34920;&#29616;&#19981;&#21033;&#65292;&#23588;&#20854;&#26159;&#22312;&#40657;&#20154;&#24739;&#32773;&#20013;&#34920;&#29616;&#26356;&#20026;&#26126;&#26174;&#65292;&#24378;&#35843;&#20102;&#29702;&#35299;&#20559;&#35265;&#23545;&#19979;&#28216;AI&#24615;&#33021;&#30340;&#24433;&#21709;&#30340;&#37325;&#35201;&#24615;&#65292;&#20197;&#24320;&#21457;&#26356;&#20855;&#20844;&#24179;&#21644;&#27491;&#20041;&#30340;&#21307;&#30103;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#23376;&#30149;&#21382;(EHRs)&#26159;&#26399;&#26395;&#20013;&#30340;&#20154;&#24037;&#26234;&#33021;(AI)-&#39537;&#21160;&#30340;&#21307;&#30103;&#36716;&#22411;&#30340;&#37325;&#35201;&#25968;&#25454;&#26469;&#28304;&#12290;&#28982;&#32780;&#65292;&#21453;&#26144;&#22312;EHR&#31508;&#35760;&#20013;&#30340;&#20020;&#24202;&#21307;&#24072;&#20559;&#35265;&#21487;&#33021;&#20250;&#23548;&#33268;AI&#27169;&#22411;&#32487;&#25215;&#24182;&#25918;&#22823;&#36825;&#20123;&#20559;&#35265;&#65292;&#20174;&#32780;&#19981;&#26029;&#21152;&#21095;&#20581;&#24247;&#19978;&#30340;&#19981;&#24179;&#31561;&#12290;&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;EHR&#31508;&#35760;&#20013;&#27745;&#21517;&#21270;&#35821;&#35328;(SL)&#23545;&#22522;&#20110;Transformer&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21644;&#21487;&#35299;&#37322;AI(XAI)&#25216;&#26415;&#36827;&#34892;&#27515;&#20129;&#39044;&#27979;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#20020;&#24202;&#21307;&#29983;&#25152;&#20889;&#30340;SL&#19981;&#21033;&#20110;AI&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#23588;&#20854;&#26159;&#22312;&#40657;&#20154;&#24739;&#32773;&#20013;&#34920;&#29616;&#26356;&#20026;&#26126;&#26174;&#65292;&#31361;&#20986;&#20102;SL&#20316;&#20026;AI&#27169;&#22411;&#21457;&#23637;&#20013;&#31181;&#26063;&#24046;&#24322;&#30340;&#19968;&#31181;&#26469;&#28304;&#12290;&#20026;&#25506;&#32034;&#19968;&#31181;&#25805;&#20316;&#19978;&#26377;&#25928;&#30340;&#32531;&#35299;SL&#24433;&#21709;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20020;&#24202;&#21307;&#29983;&#21327;&#20316;&#32593;&#32476;&#20013;SL&#29983;&#25104;&#30340;&#27169;&#24335;&#65292;&#21457;&#29616;&#20013;&#22830;&#21307;&#29983;&#23545;AI&#27169;&#22411;&#20013;&#30340;&#31181;&#26063;&#24046;&#24322;&#20855;&#26377;&#26356;&#24378;&#30340;&#24433;&#21709;&#21147;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#21024;&#38500;&#20013;&#22830;&#20020;&#24202;&#21307;&#29983;&#25776;&#20889;&#30340;SL&#26159;&#30456;&#23545;&#20110;&#38543;&#26426;&#36873;&#25321;&#20020;&#24202;&#21307;&#29983;&#32780;&#35328;&#65292;&#32531;&#35299;SL&#23545;AI&#24615;&#33021;&#24433;&#21709;&#30340;&#26356;&#20026;&#26377;&#25928;&#30340;&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#24378;&#35843;&#20102;&#29702;&#35299;&#21453;&#26144;&#22312;EHR&#31508;&#35760;&#20013;&#30340;&#20020;&#24202;&#21307;&#24072;&#20559;&#35265;&#23545;&#19979;&#28216;AI&#24615;&#33021;&#30340;&#24433;&#21709;&#30340;&#37325;&#35201;&#24615;&#65292;&#20197;&#24320;&#21457;&#26356;&#20855;&#20844;&#24179;&#21644;&#27491;&#20041;&#30340;&#21307;&#30103;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
Electronic health records (EHRs) serve as an essential data source for the envisioned artificial intelligence (AI)-driven transformation in healthcare. However, clinician biases reflected in EHR notes can lead to AI models inheriting and amplifying these biases, perpetuating health disparities. This study investigates the impact of stigmatizing language (SL) in EHR notes on mortality prediction using a Transformer-based deep learning model and explainable AI (XAI) techniques. Our findings demonstrate that SL written by clinicians adversely affects AI performance, particularly so for black patients, highlighting SL as a source of racial disparity in AI model development. To explore an operationally efficient way to mitigate SL's impact, we investigate patterns in the generation of SL through a clinicians' collaborative network, identifying central clinicians as having a stronger impact on racial disparity in the AI model. We find that removing SL written by central clinicians is a more 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#21487;&#25193;&#23637;Walsh-Hadamard&#27491;&#21017;&#21270;&#22120;&#65292;&#21487;&#36991;&#20813;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#20302;&#38454;&#39057;&#29575;&#20197;&#21450;&#35823;&#35782;&#21035;&#20302;&#38454;&#39057;&#29575;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.09779</link><description>&lt;p&gt;
&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;Walsh-Hadamard&#27491;&#21017;&#21270;&#22120;&#65292;&#20197;&#20811;&#26381;&#31070;&#32463;&#32593;&#32476;&#30340;&#20302;&#38454;&#35889;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
A Scalable Walsh-Hadamard Regularizer to Overcome the Low-degree Spectral Bias of Neural Networks. (arXiv:2305.09779v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09779
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#21487;&#25193;&#23637;Walsh-Hadamard&#27491;&#21017;&#21270;&#22120;&#65292;&#21487;&#36991;&#20813;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#20302;&#38454;&#39057;&#29575;&#20197;&#21450;&#35823;&#35782;&#21035;&#20302;&#38454;&#39057;&#29575;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#31070;&#32463;&#32593;&#32476;&#20855;&#26377;&#23398;&#20064;&#20219;&#24847;&#20989;&#25968;&#30340;&#33021;&#21147;&#65292;&#20294;&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;&#30340;&#27169;&#22411;&#24120;&#24120;&#34920;&#29616;&#20986;&#23545;&#8220;&#26356;&#31616;&#21333;&#8221;&#20989;&#25968;&#30340;&#20559;&#22909;&#12290;&#26412;&#25991;&#36890;&#36807;&#20613;&#37324;&#21494;&#65288;Walsh-Hadamard&#65289;&#21464;&#25442;&#65292;&#20174;&#31163;&#25955;&#65288;&#38646;&#19968;&#65289;&#36755;&#20837;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#35282;&#24230;&#25506;&#35752;&#20102;&#31616;&#21333;&#24615;&#30340;&#27010;&#24565;&#65292;&#20854;&#20013;&#21487;&#20197;&#36890;&#36807;&#20613;&#37324;&#21494;&#31995;&#25968;&#30340;&#8220;&#38454;&#8221;&#26469;&#25429;&#25417;&#31616;&#21333;&#24615;&#27010;&#24565;&#12290;&#25105;&#20204;&#23454;&#35777;&#34920;&#26126;&#31070;&#32463;&#32593;&#32476;&#26377;&#23398;&#20064;&#36739;&#20302;&#38454;&#39057;&#29575;&#30340;&#36235;&#21183;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#35889;&#20559;&#24046;&#21521;&#36739;&#31616;&#21333;&#29305;&#24449;&#30340;&#36235;&#21183;&#23454;&#38469;&#19978;&#20250;&#25439;&#23475;&#31070;&#32463;&#32593;&#32476;&#22312;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#25193;&#23637;&#30340;&#21151;&#33021;&#27491;&#21017;&#21270;&#26041;&#26696;&#65292;&#20197;&#24110;&#21161;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#26356;&#39640;&#30340;&#38454;&#39057;&#29575;&#12290;&#25105;&#20204;&#30340;&#27491;&#21017;&#21270;&#22120;&#36824;&#26377;&#21161;&#20110;&#36991;&#20813;&#23545;&#20302;&#38454;&#39057;&#29575;&#30340;&#38169;&#35823;&#35782;&#21035;&#65292;&#20174;&#32780;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35821;&#38899;&#35782;&#21035;&#20013;&#24212;&#29992;&#21508;&#31181;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#36827;&#34892;&#20998;&#31867;&#20219;&#21153;&#30340;&#24191;&#27867;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27491;&#21017;&#21270;&#22120;&#22312;&#20302;&#25968;&#25454;&#37327;&#29615;&#22659;&#19979;&#26174;&#33879;&#25552;&#39640;&#20102;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the capacity of neural nets to learn arbitrary functions, models trained through gradient descent often exhibit a bias towards ``simpler'' functions. Various notions of simplicity have been introduced to characterize this behavior. Here, we focus on the case of neural networks with discrete (zero-one) inputs through the lens of their Fourier (Walsh-Hadamard) transforms, where the notion of simplicity can be captured through the \emph{degree} of the Fourier coefficients. We empirically show that neural networks have a tendency to learn lower-degree frequencies. We show how this spectral bias towards simpler features can in fact \emph{hurt} the neural network's generalization on real-world datasets. To remedy this we propose a new scalable functional regularization scheme that aids the neural network to learn higher degree frequencies. Our regularizer also helps avoid erroneous identification of low-degree frequencies, which further improves generalization. We extensively evaluat
&lt;/p&gt;</description></item><item><title>AI&#35299;&#37322;&#21482;&#26377;&#22312;&#20801;&#35768;&#20154;&#31867;&#20915;&#31574;&#32773;&#39564;&#35777;AI&#39044;&#27979;&#30340;&#27491;&#30830;&#24615;&#26102;&#25165;&#26377;&#29992;&#65292;&#32780;&#22823;&#22810;&#25968;&#20915;&#31574;&#29615;&#22659;&#26080;&#27861;&#36827;&#34892;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2305.07722</link><description>&lt;p&gt;
&#23547;&#27714;&#21487;&#39564;&#35777;&#24615;: &#35299;&#37322;&#24456;&#23569;&#33021;&#22815;&#22312;AI&#36741;&#21161;&#20915;&#31574;&#20013;&#25552;&#39640;&#20915;&#31574;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
In Search of Verifiability: Explanations Rarely Enable Complementary Performance in AI-Advised Decision Making. (arXiv:2305.07722v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07722
&lt;/p&gt;
&lt;p&gt;
AI&#35299;&#37322;&#21482;&#26377;&#22312;&#20801;&#35768;&#20154;&#31867;&#20915;&#31574;&#32773;&#39564;&#35777;AI&#39044;&#27979;&#30340;&#27491;&#30830;&#24615;&#26102;&#25165;&#26377;&#29992;&#65292;&#32780;&#22823;&#22810;&#25968;&#20915;&#31574;&#29615;&#22659;&#26080;&#27861;&#36827;&#34892;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#20851;&#20110;AI&#36741;&#21161;&#20915;&#31574;&#30340;&#25991;&#29486;&#65292;&#28041;&#21450;&#21487;&#35299;&#37322;&#30340;AI&#31995;&#32479;&#20026;&#20154;&#31867;&#20915;&#31574;&#32773;&#25552;&#20379;&#24314;&#35758;&#65292;&#24182;&#21576;&#29616;&#20986;&#19968;&#31995;&#21015;&#19981;&#30830;&#23450;&#21644;&#20196;&#20154;&#22256;&#24785;&#30340;&#32467;&#26524;&#12290;&#20026;&#20102;&#32508;&#21512;&#36825;&#20123;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#29702;&#35770;&#65292;&#38416;&#26126;&#20102;AI&#35299;&#37322;&#32463;&#24120;&#26080;&#27861;&#20419;&#20351;&#36866;&#24403;&#30340;&#20381;&#36182;&#21644;&#20114;&#34917;&#20915;&#31574;&#34920;&#29616;&#30340;&#22833;&#36133;&#12290;&#25105;&#20204;&#35748;&#20026;&#35299;&#37322;&#21482;&#26377;&#22312;&#20801;&#35768;&#20154;&#31867;&#20915;&#31574;&#32773;&#39564;&#35777;AI&#39044;&#27979;&#30340;&#27491;&#30830;&#24615;&#26102;&#25165;&#26377;&#29992;&#65292;&#32780;&#19981;&#26159;&#20854;&#20182;&#26399;&#26395;&#65292;&#20363;&#22914;&#21487;&#35299;&#37322;&#24615;&#25110;&#28165;&#26224;&#38416;&#36848;AI&#30340;&#25512;&#29702;&#36807;&#31243;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#35768;&#22810;&#20915;&#31574;&#29615;&#22659;&#20013;&#65292;AI&#35299;&#37322;&#24182;&#26410;&#20419;&#36827;&#36825;&#31181;&#39564;&#35777;&#12290;&#27492;&#22806;&#65292;&#26080;&#35770;&#35299;&#37322;&#26041;&#27861;&#22914;&#20309;&#65292;&#22823;&#22810;&#25968;&#29615;&#22659;&#22522;&#26412;&#19978;&#37117;&#26080;&#27861;&#36827;&#34892;&#39564;&#35777;&#12290;&#25105;&#20204;&#26368;&#21518;&#35752;&#35770;&#20102;&#26356;&#26377;&#25928;&#30340;&#21487;&#35299;&#37322;AI&#36741;&#21161;&#20915;&#31574;&#21644;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;&#30340;&#28508;&#22312;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The current literature on AI-advised decision making -- involving explainable AI systems advising human decision makers -- presents a series of inconclusive and confounding results. To synthesize these findings, we propose a simple theory that elucidates the frequent failure of AI explanations to engender appropriate reliance and complementary decision making performance. We argue explanations are only useful to the extent that they allow a human decision maker to verify the correctness of an AI's prediction, in contrast to other desiderata, e.g., interpretability or spelling out the AI's reasoning process. Prior studies find in many decision making contexts AI explanations do not facilitate such verification. Moreover, most contexts fundamentally do not allow verification, regardless of explanation method. We conclude with a discussion of potential approaches for more effective explainable AI-advised decision making and human-AI collaboration.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21457;&#29616;&#65292;&#27169;&#22411;&#25193;&#25955;&#20013;&#30340;&#26080;&#26631;&#27880;&#25991;&#26412;&#23454;&#38469;&#19978;&#26159;&#19968;&#20010;&#33021;&#22815;&#29983;&#25104;&#21345;&#36890;&#39118;&#26684;&#22270;&#29255;&#30340;&#24037;&#20855;&#12290;&#36890;&#36807;&#31616;&#21333;&#22320;&#25200;&#21160;&#26080;&#26631;&#27880;&#25991;&#26412;&#25351;&#23548;&#65292;&#36825;&#19968;&#21151;&#33021;&#24471;&#20197;&#23454;&#29616;&#12290;&#22238;&#28378;&#25200;&#21160;&#33021;&#22815;&#23558;&#29983;&#25104;&#30340;&#22270;&#20687;&#26377;&#25928;&#36716;&#25442;&#25104;&#21345;&#36890;&#22270;&#20687;&#65292;&#32780;&#22270;&#20687;&#25200;&#21160;&#21017;&#33021;&#22815;&#20135;&#29983;&#39640;&#20445;&#30495;&#24230;&#12289;&#22810;&#26679;&#24615;&#30340;&#21345;&#36890;&#22270;&#20687;&#12290;</title><link>http://arxiv.org/abs/2305.06710</link><description>&lt;p&gt;
&#27169;&#22411;&#25193;&#25955;&#20013;&#30340;&#26080;&#26631;&#27880;&#25991;&#26412;&#23454;&#38469;&#19978;&#26159;&#21345;&#36890;&#39118;&#26684;&#29983;&#25104;&#22120;
&lt;/p&gt;
&lt;p&gt;
Null-text Guidance in Diffusion Models is Secretly a Cartoon-style Creator. (arXiv:2305.06710v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06710
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21457;&#29616;&#65292;&#27169;&#22411;&#25193;&#25955;&#20013;&#30340;&#26080;&#26631;&#27880;&#25991;&#26412;&#23454;&#38469;&#19978;&#26159;&#19968;&#20010;&#33021;&#22815;&#29983;&#25104;&#21345;&#36890;&#39118;&#26684;&#22270;&#29255;&#30340;&#24037;&#20855;&#12290;&#36890;&#36807;&#31616;&#21333;&#22320;&#25200;&#21160;&#26080;&#26631;&#27880;&#25991;&#26412;&#25351;&#23548;&#65292;&#36825;&#19968;&#21151;&#33021;&#24471;&#20197;&#23454;&#29616;&#12290;&#22238;&#28378;&#25200;&#21160;&#33021;&#22815;&#23558;&#29983;&#25104;&#30340;&#22270;&#20687;&#26377;&#25928;&#36716;&#25442;&#25104;&#21345;&#36890;&#22270;&#20687;&#65292;&#32780;&#22270;&#20687;&#25200;&#21160;&#21017;&#33021;&#22815;&#20135;&#29983;&#39640;&#20445;&#30495;&#24230;&#12289;&#22810;&#26679;&#24615;&#30340;&#21345;&#36890;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#20998;&#31867;&#22120;&#25351;&#23548;&#26159;&#25193;&#25955;&#27169;&#22411;&#20013;&#34987;&#24191;&#27867;&#37319;&#29992;&#30340;&#19968;&#31181;&#26377;&#25928;&#30340;&#37319;&#26679;&#25216;&#26415;&#12290;&#20854;&#20027;&#35201;&#24605;&#24819;&#26159;&#22312;&#25991;&#26412;&#25351;&#23548;&#26041;&#21521;&#19978;&#23545;&#27169;&#22411;&#36827;&#34892;&#22806;&#25512;&#65292;&#24182;&#36828;&#31163;&#26080;&#26631;&#27880;&#25991;&#26412;&#25351;&#23548;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#27169;&#22411;&#25193;&#25955;&#20013;&#30340;&#26080;&#26631;&#27880;&#25991;&#26412;&#23454;&#38469;&#19978;&#26159;&#19968;&#20010;&#21345;&#36890;&#39118;&#26684;&#29983;&#25104;&#22120;&#65292;&#21363;&#36890;&#36807;&#31616;&#21333;&#22320;&#25200;&#21160;&#26080;&#26631;&#27880;&#25991;&#26412;&#25351;&#23548;&#23601;&#21487;&#20197;&#26377;&#25928;&#22320;&#23558;&#29983;&#25104;&#30340;&#22270;&#20687;&#36716;&#25442;&#25104;&#21345;&#36890;&#22270;&#20687;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#25200;&#21160;&#26041;&#27861;&#65306;&#22238;&#28378;&#25200;&#21160;&#65288;Back-D&#65289;&#21644;&#22270;&#20687;&#25200;&#21160;&#65288;Image-D&#65289;&#65292;&#29992;&#20110;&#26500;&#36896;&#22312;&#37319;&#26679;&#36807;&#31243;&#20013;&#29992;&#20110;&#39044;&#27979;&#26080;&#26631;&#27880;&#25991;&#26412;&#25351;&#23548;&#21644;&#25991;&#26412;&#25351;&#23548;&#30340;&#22024;&#26434;&#22270;&#20687;&#20043;&#38388;&#30340;&#38169;&#20301;&#12290;Back-D&#36890;&#36807;&#36890;&#36807;&#23558;$x_t$&#26367;&#25442;&#20026;$x_{t+\Delta t}$&#26469;&#25913;&#21464;&#26080;&#26631;&#27880;&#22024;&#26434;&#22270;&#20687;&#30340;&#22122;&#22768;&#27700;&#24179;&#20174;&#32780;&#23454;&#29616;&#21345;&#36890;&#21270;&#12290;Image-D&#21017;&#36890;&#36807;&#20135;&#29983;&#39640;&#20445;&#30495;&#24230;&#12289;&#22810;&#26679;&#24615;&#30340;&#21345;&#36890;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Classifier-free guidance is an effective sampling technique in diffusion models that has been widely adopted. The main idea is to extrapolate the model in the direction of text guidance and away from null-text guidance. In this paper, we demonstrate that null-text guidance in diffusion models is secretly a cartoon-style creator, i.e., the generated images can be efficiently transformed into cartoons by simply perturbing the null-text guidance. Specifically, we proposed two disturbance methods, i.e., Rollback disturbance (Back-D) and Image disturbance (Image-D), to construct misalignment between the noisy images used for predicting null-text guidance and text guidance (subsequently referred to as \textbf{null-text noisy image} and \textbf{text noisy image} respectively) in the sampling process. Back-D achieves cartoonization by altering the noise level of null-text noisy image via replacing $x_t$ with $x_{t+\Delta t}$. Image-D, alternatively, produces high-fidelity, diverse cartoons by 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#21033;&#29992;&#22240;&#26524;&#26426;&#21046;&#22312;&#19981;&#21516;&#29615;&#22659;&#19979;&#20445;&#25345;&#19981;&#21464;&#30340;&#30452;&#35273;&#26469;&#20027;&#21160;&#20934;&#22791;&#30340;&#20195;&#29702;&#29305;&#24449;&#36873;&#25321;&#21644;&#24037;&#31243;&#25216;&#26415;&#65292;&#29992;&#20197;&#24212;&#23545;&#32479;&#35745;&#39044;&#27979;&#27169;&#22411;&#22312;&#20998;&#24067;&#36716;&#31227;&#24773;&#20917;&#19979;&#30340;&#31283;&#23450;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.05832</link><description>&lt;p&gt;
&#22240;&#26524;&#20449;&#24687;&#20998;&#31163;&#65306;&#20026;&#25239;&#20998;&#24067;&#36716;&#31227;&#35774;&#35745;&#20195;&#29702;&#29305;&#24449;
&lt;/p&gt;
&lt;p&gt;
Causal Information Splitting: Engineering Proxy Features for Robustness to Distribution Shifts. (arXiv:2305.05832v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05832
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#21033;&#29992;&#22240;&#26524;&#26426;&#21046;&#22312;&#19981;&#21516;&#29615;&#22659;&#19979;&#20445;&#25345;&#19981;&#21464;&#30340;&#30452;&#35273;&#26469;&#20027;&#21160;&#20934;&#22791;&#30340;&#20195;&#29702;&#29305;&#24449;&#36873;&#25321;&#21644;&#24037;&#31243;&#25216;&#26415;&#65292;&#29992;&#20197;&#24212;&#23545;&#32479;&#35745;&#39044;&#27979;&#27169;&#22411;&#22312;&#20998;&#24067;&#36716;&#31227;&#24773;&#20917;&#19979;&#30340;&#31283;&#23450;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32479;&#35745;&#39044;&#27979;&#27169;&#22411;&#36890;&#24120;&#26159;&#22312;&#19982;&#26368;&#32456;&#20351;&#29992;&#24773;&#20917;&#19981;&#21516;&#30340;&#27010;&#29575;&#20998;&#24067;&#20013;&#36827;&#34892;&#35757;&#32451;&#30340;&#12290;&#20026;&#20102;&#39044;&#27979;&#20998;&#24067;&#36716;&#31227;&#65292;&#26377;&#19968;&#31181;&#26041;&#27861;&#26159;&#21033;&#29992;&#22240;&#26524;&#26426;&#21046;&#22312;&#19981;&#21516;&#29615;&#22659;&#19979;&#20445;&#25345;&#19981;&#21464;&#30340;&#30452;&#35273;&#26469;&#20027;&#21160;&#20934;&#22791;&#12290;&#26412;&#25991;&#38024;&#23545;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22330;&#26223;&#65292;&#20854;&#20013;&#30446;&#26631;&#30340;&#22240;&#26524;&#21644;&#21453;&#22240;&#26524;&#21464;&#37327;&#37117;&#26159;&#26410;&#34987;&#35266;&#23519;&#21040;&#30340;&#12290;&#21033;&#29992;&#20449;&#24687;&#35770;&#65292;&#25105;&#20204;&#20026;&#19979;&#28216;&#35266;&#27979;&#21464;&#37327;&#24320;&#21457;&#20102;&#29305;&#24449;&#36873;&#25321;&#21644;&#24037;&#31243;&#25216;&#26415;&#65292;&#36825;&#20123;&#21464;&#37327;&#20805;&#24403;&#20195;&#29702;&#12290;&#25105;&#20204;&#36873;&#25321;&#26377;&#21161;&#20110;&#24314;&#31435;&#31283;&#23450;&#27169;&#22411;&#30340;&#20195;&#29702;&#65292;&#24182;&#20351;&#29992;&#36741;&#21161;&#35757;&#32451;&#20219;&#21153;&#20174;&#20195;&#29702;&#20013;&#25552;&#21462;&#22686;&#24378;&#31283;&#23450;&#24615;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#22312;&#21512;&#25104;&#25968;&#25454;&#21644;&#30495;&#23454;&#25968;&#25454;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#25216;&#26415;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Statistical prediction models are often trained on data that is drawn from different probability distributions than their eventual use cases. One approach to proactively prepare for these shifts harnesses the intuition that causal mechanisms should remain invariant between environments. Here we focus on a challenging setting in which the causal and anticausal variables of the target are unobserved. Leaning on information theory, we develop feature selection and engineering techniques for the observed downstream variables that act as proxies. We identify proxies that help to build stable models and moreover utilize auxiliary training tasks to extract stability-enhancing information from proxies. We demonstrate the effectiveness of our techniques on synthetic and real data.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#32034;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#25945;&#24072;&#19982;&#23398;&#29983;&#30340;&#20114;&#21160;&#65292;&#25552;&#39640;&#25945;&#23398;&#36136;&#37327;&#30340;&#28508;&#21147;&#21644;&#25361;&#25112;&#12290;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#26469;&#22788;&#29702;&#22810;&#26679;&#21270;&#30340;&#25945;&#32946;&#25968;&#25454;&#38598;&#65292;&#22788;&#29702;&#38271;&#26102;&#38388;&#30340;&#23545;&#35805;&#20197;&#21450;&#21387;&#32553;&#20449;&#24687;&#20197;&#26356;&#22909;&#22320;&#23436;&#25104;&#26356;&#22810;&#30340;&#19979;&#28216;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2305.03433</link><description>&lt;p&gt;
&#25506;&#32034;&#24212;&#29992;&#24378;&#22823;&#30340;&#22823;&#22411;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#22312;&#35838;&#22530;&#25945;&#23398;&#20013;&#65306;&#26426;&#36935;&#12289;&#25361;&#25112;&#21644;&#21069;&#26223;
&lt;/p&gt;
&lt;p&gt;
Towards Applying Powerful Large AI Models in Classroom Teaching: Opportunities, Challenges and Prospects. (arXiv:2305.03433v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03433
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#32034;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#25945;&#24072;&#19982;&#23398;&#29983;&#30340;&#20114;&#21160;&#65292;&#25552;&#39640;&#25945;&#23398;&#36136;&#37327;&#30340;&#28508;&#21147;&#21644;&#25361;&#25112;&#12290;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#26469;&#22788;&#29702;&#22810;&#26679;&#21270;&#30340;&#25945;&#32946;&#25968;&#25454;&#38598;&#65292;&#22788;&#29702;&#38271;&#26102;&#38388;&#30340;&#23545;&#35805;&#20197;&#21450;&#21387;&#32553;&#20449;&#24687;&#20197;&#26356;&#22909;&#22320;&#23436;&#25104;&#26356;&#22810;&#30340;&#19979;&#28216;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#20114;&#21160;&#22330;&#26223;&#65292;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#22686;&#24378;&#35838;&#22530;&#25945;&#23398;&#65292;&#22914;&#23545;&#35805;&#33258;&#21160;&#23436;&#25104;&#12289;&#30693;&#35782;&#21644;&#39118;&#26684;&#36716;&#31227;&#20197;&#21450;&#35780;&#20272;AI&#29983;&#25104;&#20869;&#23481;&#12290;&#36890;&#36807;&#21033;&#29992;&#36817;&#26399;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#21457;&#23637;&#65292;&#26412;&#25991;&#25506;&#32034;&#20102;AI&#22686;&#24378;&#21644;&#20016;&#23500;&#24072;&#29983;&#23545;&#35805;&#12289;&#25552;&#39640;&#25945;&#23398;&#36136;&#37327;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#22312;&#25945;&#24072;&#19982;&#23398;&#29983;&#20043;&#38388;&#20135;&#29983;&#21019;&#26032;&#21644;&#26377;&#24847;&#20041;&#30340;&#23545;&#35805;&#65292;&#21046;&#23450;&#35780;&#20272;&#26631;&#20934;&#65292;&#24182;&#25552;&#39640;AI&#25945;&#32946;&#35745;&#21010;&#30340;&#25928;&#21147;&#12290;&#22312;&#31532;&#19977;&#33410;&#20013;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#21033;&#29992;&#29616;&#26377;LLM&#26377;&#25928;&#23436;&#25104;&#25945;&#32946;&#20219;&#21153;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#26469;&#22788;&#29702;&#22810;&#26679;&#21270;&#30340;&#25945;&#32946;&#25968;&#25454;&#38598;&#65292;&#22788;&#29702;&#38271;&#26102;&#38388;&#30340;&#23545;&#35805;&#20197;&#21450;&#21387;&#32553;&#20449;&#24687;&#20197;&#26356;&#22909;&#22320;&#23436;&#25104;&#26356;&#22810;&#30340;&#19979;&#28216;&#20219;&#21153;&#12290;&#22312;&#31532;&#22235;&#33410;&#20013;&#65292;&#25105;&#20204;&#24635;&#32467;&#20102;&#20851;&#38190;&#20219;&#21153;&#65292;&#21253;&#25324;&#25945;&#24072;-&#23398;&#29983;&#23545;&#35805;&#33258;&#21160;&#23436;&#25104;&#12289;&#31034;&#33539;&#31572;&#26696;&#21644;AI&#25903;&#25345;&#30340;&#20889;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
This perspective paper proposes a series of interactive scenarios that utilize Artificial Intelligence (AI) to enhance classroom teaching, such as dialogue auto-completion, knowledge and style transfer, and assessment of AI-generated content. By leveraging recent developments in Large Language Models (LLMs), we explore the potential of AI to augment and enrich teacher-student dialogues and improve the quality of teaching. Our goal is to produce innovative and meaningful conversations between teachers and students, create standards for evaluation, and improve the efficacy of AI-for-Education initiatives. In Section 3, we discuss the challenges of utilizing existing LLMs to effectively complete the educated tasks and present a unified framework for addressing diverse education dataset, processing lengthy conversations, and condensing information to better accomplish more downstream tasks. In Section 4, we summarize the pivoting tasks including Teacher-Student Dialogue Auto-Completion, Ex
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#19981;&#21516;&#35821;&#35328;&#25552;&#31034;&#19979;&#30340;&#22870;&#21169;&#26102;&#38388;&#20559;&#22909;&#65292;&#24182;&#21457;&#29616;GPT&#22312;&#20855;&#26377;&#36739;&#24369;&#26410;&#26469;&#26102;&#24577;&#30340;&#35821;&#35328;&#19979;&#34920;&#29616;&#20986;&#26356;&#22823;&#30340;&#32784;&#24515;&#65292;&#36825;&#19982;&#20351;&#29992;&#35813;&#35821;&#35328;&#30340;&#20154;&#31867;&#30340;&#20559;&#22909;&#30456;&#20284;&#12290;</title><link>http://arxiv.org/abs/2305.02531</link><description>&lt;p&gt;
&#35821;&#35328;&#12289;&#26102;&#38388;&#20559;&#22909;&#21644;&#28040;&#36153;&#34892;&#20026;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35777;&#25454;
&lt;/p&gt;
&lt;p&gt;
Language, Time Preferences, and Consumer Behavior: Evidence from Large Language Models. (arXiv:2305.02531v1 [econ.GN])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02531
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#19981;&#21516;&#35821;&#35328;&#25552;&#31034;&#19979;&#30340;&#22870;&#21169;&#26102;&#38388;&#20559;&#22909;&#65292;&#24182;&#21457;&#29616;GPT&#22312;&#20855;&#26377;&#36739;&#24369;&#26410;&#26469;&#26102;&#24577;&#30340;&#35821;&#35328;&#19979;&#34920;&#29616;&#20986;&#26356;&#22823;&#30340;&#32784;&#24515;&#65292;&#36825;&#19982;&#20351;&#29992;&#35813;&#35821;&#35328;&#30340;&#20154;&#31867;&#30340;&#20559;&#22909;&#30456;&#20284;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#23545;&#25105;&#20204;&#23545;&#26102;&#38388;&#21644;&#22870;&#21169;&#30340;&#24863;&#30693;&#26377;&#24456;&#22823;&#30340;&#24433;&#21709;&#12290;&#36825;&#24341;&#21457;&#20102;&#19968;&#20010;&#38382;&#39064;&#65292;&#21363;&#24403;&#20197;&#19981;&#21516;&#30340;&#35821;&#35328;&#35810;&#38382;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26102;&#65292;&#23427;&#20204;&#26159;&#21542;&#26174;&#31034;&#20986;&#19981;&#21516;&#30340;&#22870;&#21169;&#26102;&#38388;&#20559;&#22909;&#65292;&#24182;&#19988;&#23427;&#20204;&#30340;&#36873;&#25321;&#26159;&#21542;&#31867;&#20284;&#20110;&#20154;&#31867;&#30340;&#36873;&#25321;&#12290;&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;GPT-3.5&#65288;&#20197;&#19979;&#31616;&#31216;GPT&#65289;&#22312;&#22810;&#31181;&#35821;&#35328;&#25552;&#31034;&#19979;&#30340;&#21709;&#24212;&#65292;&#25506;&#32034;&#20102;&#36739;&#23567;&#12289;&#36739;&#26089;&#30340;&#22870;&#21169;&#21644;&#36739;&#22823;&#12289;&#36739;&#26202;&#30340;&#22870;&#21169;&#20043;&#38388;&#30340;&#20559;&#22909;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#24403;&#20197;&#35821;&#20041;&#21547;&#20041;&#36739;&#24369;&#30340;&#26410;&#26469;&#26102;&#24577;&#21442;&#32771;&#65288;FTR&#65289;&#65292;&#22914;&#24503;&#35821;&#21644;&#27721;&#35821;&#65292;&#20026;&#25552;&#31034;&#35821;&#26102;&#65292;GPT&#34920;&#29616;&#20986;&#26356;&#22823;&#30340;&#32784;&#24515;&#65292;&#30456;&#27604;&#33521;&#35821;&#21644;&#27861;&#35821;&#31561;&#20855;&#26377;&#24378;&#22823;FTR&#30340;&#35821;&#35328;&#12290;&#36825;&#20123;&#21457;&#29616;&#19982;&#29616;&#26377;&#25991;&#29486;&#19968;&#33268;&#65292;&#24182;&#34920;&#26126;&#20102;GPT&#30340;&#36873;&#25321;&#19982;&#36825;&#20123;&#35821;&#35328;&#30340;&#20351;&#29992;&#32773;&#30340;&#20559;&#22909;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;&#28982;&#32780;&#65292;&#36827;&#19968;&#27493;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#36739;&#26089;&#25110;&#36739;&#26202;&#22870;&#21169;&#30340;&#20559;&#22909;&#24182;&#27809;&#26377;&#38543;&#30528;&#22870;&#21169;&#24046;&#24322;&#31995;&#32479;&#22320;&#25913;&#21464;&#65292;&#36825;&#34920;&#26126;&#20102;&#19968;&#31181;&#35789;&#20856;&#24207;&#20248;&#20808;&#30340;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language has a strong influence on our perceptions of time and rewards. This raises the question of whether large language models, when asked in different languages, show different preferences for rewards over time and if their choices are similar to those of humans. In this study, we analyze the responses of GPT-3.5 (hereafter referred to as GPT) to prompts in multiple languages, exploring preferences between smaller, sooner rewards and larger, later rewards. Our results show that GPT displays greater patience when prompted in languages with weak future tense references (FTR), such as German and Mandarin, compared to languages with strong FTR, like English and French. These findings are consistent with existing literature and suggest a correlation between GPT's choices and the preferences of speakers of these languages. However, further analysis reveals that the preference for earlier or later rewards does not systematically change with reward gaps, indicating a lexicographic preferen
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22240;&#26524;&#24863;&#30693;&#30340;&#30693;&#35782;&#24341;&#23548;&#25552;&#31034;&#26041;&#27861;&#65292;&#23558;&#20854;&#20316;&#20026;&#24178;&#39044;&#22120;&#35013;&#22791;&#21040;&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#21477;&#23376;&#25552;&#21462;&#22120;&#20013;&#65292;&#20197;&#32531;&#35299;&#27010;&#24565;&#20559;&#24046;&#12290;&#22312;&#20195;&#34920;&#24615;&#30340;&#22810;&#35821;&#35328;KG&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24191;&#27867;&#23454;&#39564;&#65292;&#33719;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.01876</link><description>&lt;p&gt;
&#22522;&#20110;&#22240;&#26524;&#24863;&#30693;&#30340;&#30693;&#35782;&#24341;&#23548;&#21477;&#23376;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Causality-aware Concept Extraction based on Knowledge-guided Prompting. (arXiv:2305.01876v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01876
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22240;&#26524;&#24863;&#30693;&#30340;&#30693;&#35782;&#24341;&#23548;&#25552;&#31034;&#26041;&#27861;&#65292;&#23558;&#20854;&#20316;&#20026;&#24178;&#39044;&#22120;&#35013;&#22791;&#21040;&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#21477;&#23376;&#25552;&#21462;&#22120;&#20013;&#65292;&#20197;&#32531;&#35299;&#27010;&#24565;&#20559;&#24046;&#12290;&#22312;&#20195;&#34920;&#24615;&#30340;&#22810;&#35821;&#35328;KG&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24191;&#27867;&#23454;&#39564;&#65292;&#33719;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27010;&#24565;&#26377;&#21161;&#20110;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#65292;&#20294;&#29616;&#26377;&#30340;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#20013;&#36828;&#26410;&#23436;&#21892;&#12290;&#26368;&#36817;&#65292;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#24050;&#34987;&#24191;&#27867;&#29992;&#20110;&#22522;&#20110;&#25991;&#26412;&#30340;&#27010;&#24565;&#25552;&#21462;&#65288;CE&#65289;&#12290;&#28982;&#32780;&#65292;PLM&#24448;&#24448;&#20174;&#22823;&#37327;&#35821;&#26009;&#24211;&#30340;&#20849;&#29616;&#20851;&#32852;&#20013;&#36827;&#34892;&#39044;&#35757;&#32451;&#30693;&#35782;&#25366;&#25496;&#65292;&#32780;&#38750;Token&#20043;&#38388;&#30340;&#30495;&#23454;&#22240;&#26524;&#20851;&#31995;&#12290;&#22240;&#27492;&#65292;&#39044;&#35757;&#32451;&#30693;&#35782;&#28151;&#28102;&#20102;PLM&#65292;&#23548;&#33268;&#25552;&#21462;&#22522;&#20110;&#34394;&#20551;&#20849;&#29616;&#30456;&#20851;&#24615;&#30340;&#26377;&#20559;&#27010;&#24565;&#65292;&#19981;&#21487;&#36991;&#20813;&#22320;&#23548;&#33268;&#20302;&#31934;&#24230;&#12290;&#26412;&#25991;&#36890;&#36807;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#65288;SCM&#65289;&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#24341;&#23548;&#25552;&#31034;&#26041;&#27861;&#65292;&#23558;&#20854;&#20316;&#20026;&#24178;&#39044;&#22120;&#35013;&#22791;&#21040;&#22522;&#20110;PLM&#30340;&#25552;&#21462;&#22120;&#20013;&#65292;&#20197;&#20943;&#36731;&#27010;&#24565;&#20559;&#24046;&#12290;&#25552;&#31034;&#37319;&#29992;&#29616;&#26377;KG&#20013;&#30340;&#32473;&#23450;&#23454;&#20307;&#20027;&#39064;&#26469;&#32531;&#35299;&#23454;&#20307;&#21644;&#26377;&#20559;&#27010;&#24565;&#20043;&#38388;&#30340;&#34394;&#20551;&#20849;&#29616;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#22312;&#20195;&#34920;&#24615;&#30340;&#22810;&#35821;&#35328;KG&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#25552;&#31034;&#26174;&#33879;&#25913;&#36827;&#20102;&#25552;&#21462;&#24615;&#33021;&#65292;&#24182;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Concepts benefit natural language understanding but are far from complete in existing knowledge graphs (KGs). Recently, pre-trained language models (PLMs) have been widely used in text-based concept extraction (CE). However, PLMs tend to mine the co-occurrence associations from massive corpus as pre-trained knowledge rather than the real causal effect between tokens.As a result, the pre-trained knowledge confounds PLMs to extract biased concepts based on spurious co-occurrence correlations, inevitably resulting in low precision. In this paper, through the lens of a Structural Causal Model (SCM), we propose equipping the PLM-based extractor with a knowledge-guided prompt as an intervention to alleviate concept bias. The prompt adopts the topic of the given entity from the existing knowledge in KGs to mitigate the spurious co-occurrence correlations between entities and biased concepts. Our extensive experiments on representative multilingual KG datasets justify that our proposed prompt 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;HE&#21451;&#22909;&#27169;&#22411;&#35757;&#32451;&#26041;&#27861;&#65292;&#25104;&#21151;&#28436;&#31034;&#20102;&#22312;ResNet&#21644;ConvNeXt&#31561;&#32463;&#20856;&#21644;&#29616;&#20195;CNN&#19978;&#36816;&#34892;&#21152;&#23494;&#26679;&#26412;&#65292;&#24182;&#20197;&#21069;&#25152;&#26410;&#26377;&#30340;&#26041;&#24335;&#28436;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;CLIP GPT-2&#27169;&#22411;&#36827;&#34892;&#38646;&#30693;&#35782;&#23433;&#20840;&#39044;&#27979;&#65292;&#26159;&#19968;&#31181;&#21487;&#34892;&#30340;&#38544;&#31169;&#20445;&#25252;&#26426;&#22120;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2304.14836</link><description>&lt;p&gt;
&#20351;&#29992;&#21516;&#24577;&#21152;&#23494;&#23545;&#22823;&#35268;&#27169;CNN&#36827;&#34892;&#25935;&#24863;&#35843;&#25972;&#20197;&#36827;&#34892;&#31471;&#21040;&#31471;&#23433;&#20840;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Sensitive Tuning of Large Scale CNNs for E2E Secure Prediction using Homomorphic Encryption. (arXiv:2304.14836v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14836
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;HE&#21451;&#22909;&#27169;&#22411;&#35757;&#32451;&#26041;&#27861;&#65292;&#25104;&#21151;&#28436;&#31034;&#20102;&#22312;ResNet&#21644;ConvNeXt&#31561;&#32463;&#20856;&#21644;&#29616;&#20195;CNN&#19978;&#36816;&#34892;&#21152;&#23494;&#26679;&#26412;&#65292;&#24182;&#20197;&#21069;&#25152;&#26410;&#26377;&#30340;&#26041;&#24335;&#28436;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;CLIP GPT-2&#27169;&#22411;&#36827;&#34892;&#38646;&#30693;&#35782;&#23433;&#20840;&#39044;&#27979;&#65292;&#26159;&#19968;&#31181;&#21487;&#34892;&#30340;&#38544;&#31169;&#20445;&#25252;&#26426;&#22120;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38544;&#31169;&#20445;&#25252;&#30340;&#26426;&#22120;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#36817;&#26469;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#20854;&#20013;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#30740;&#31350;&#36235;&#21183;&#26159;&#20351;&#29992;&#21516;&#24577;&#21152;&#23494;&#65288;HE&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#22312;&#21152;&#23494;&#25968;&#25454;&#19978;&#25191;&#34892;&#35745;&#31639;&#30340;&#26041;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#30340;&#19968;&#20010;&#20027;&#35201;&#25361;&#25112;&#26159;&#35757;&#32451;&#36866;&#29992;&#20110;HE&#30340;&#21152;&#23494;&#25110;&#26410;&#21152;&#23494;&#30340;&#28145;&#23618;CNN&#65292;&#20197;&#23454;&#29616;&#33391;&#22909;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;HE&#21451;&#22909;&#27169;&#22411;&#35757;&#32451;&#26041;&#27861;&#65292;&#24182;&#22312;&#22522;&#26412;&#21644;&#29616;&#20195;CNN&#19978;&#36827;&#34892;&#20102;&#28436;&#31034;&#65292;&#20363;&#22914;ResNet&#21644;ConvNeXt&#12290;&#35757;&#32451;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;HELayers SDK&#36816;&#34892;&#21152;&#23494;&#26679;&#26412;&#26469;&#35780;&#20272;&#25105;&#20204;&#30340;&#27169;&#22411;&#65292;&#24182;&#35777;&#26126;&#23427;&#20204;&#20135;&#29983;&#20102;&#25152;&#38656;&#30340;&#32467;&#26524;&#12290;&#22312;ImageNet&#25968;&#25454;&#38598;&#19978;&#36816;&#34892;&#26102;&#65292;&#25105;&#20204;&#30340;ResNet-18/50/101&#23454;&#29616;&#20165;&#38656;&#35201;7&#12289;31&#21644;57&#20998;&#38047;&#65292;&#36825;&#34920;&#26126;&#36825;&#20010;&#35299;&#20915;&#26041;&#26696;&#26159;&#23454;&#29992;&#30340;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20123;&#20851;&#20110;&#22312;HE&#19979;&#22788;&#29702;&#28608;&#27963;&#20989;&#25968;&#21644;&#36339;&#36291;&#36830;&#25509;&#30340;&#35265;&#35299;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20197;&#21069;&#25152;&#26410;&#26377;&#30340;&#26041;&#24335;&#28436;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;CLIP GPT-2&#27169;&#22411;&#36827;&#34892;&#38646;&#30693;&#35782;&#23433;&#20840;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Privacy-preserving machine learning solutions have recently gained significant attention. One promising research trend is using Homomorphic Encryption (HE), a method for performing computation over encrypted data. One major challenge in this approach is training HE-friendly, encrypted or unencrypted, deep CNNs with decent accuracy. We propose a novel training method for HE-friendly models, and demonstrate it on fundamental and modern CNNs, such as ResNet and ConvNeXt. After training, we evaluate our models by running encrypted samples using HELayers SDK and proving that they yield the desired results. When running on a GPU over the ImageNet dataset, our ResNet-18/50/101 implementations take only 7, 31 and 57 minutes, respectively, which shows that this solution is practical. Furthermore, we present several insights on handling the activation functions and skip-connections under HE. Finally, we demonstrate in an unprecedented way how to perform secure zero-shot prediction using a CLIP m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#21644;&#36229;&#32593;&#32476;&#36827;&#34892;&#34920;&#31034;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#19981;&#21464;&#24615;&#34920;&#31034;&#23398;&#20064;&#65292;&#21487;&#20197;&#22312;&#22797;&#26434;&#22270;&#20687;&#19978;&#23398;&#21040;&#20998;&#31163;&#30340;&#35821;&#20041;&#34920;&#31034;&#65292;&#24182;&#19988;&#21644;SCAN&#24456;&#22909;&#22320;&#21327;&#21516;&#24037;&#20316;&#65292;&#20174;&#32780;&#33719;&#24471;&#26368;&#20808;&#36827;&#30340;&#26080;&#30417;&#30563;&#32858;&#31867;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.13995</link><description>&lt;p&gt;
&#20351;&#29992;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#23398;&#20064;&#26059;&#36716;&#21644;&#24179;&#31227;&#19981;&#21464;&#24615;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Rotation and Translation Invariant Representation Learning with Implicit Neural Representations. (arXiv:2304.13995v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13995
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#21644;&#36229;&#32593;&#32476;&#36827;&#34892;&#34920;&#31034;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#19981;&#21464;&#24615;&#34920;&#31034;&#23398;&#20064;&#65292;&#21487;&#20197;&#22312;&#22797;&#26434;&#22270;&#20687;&#19978;&#23398;&#21040;&#20998;&#31163;&#30340;&#35821;&#20041;&#34920;&#31034;&#65292;&#24182;&#19988;&#21644;SCAN&#24456;&#22909;&#22320;&#21327;&#21516;&#24037;&#20316;&#65292;&#20174;&#32780;&#33719;&#24471;&#26368;&#20808;&#36827;&#30340;&#26080;&#30417;&#30563;&#32858;&#31867;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#35745;&#31639;&#26426;&#35270;&#35273;&#24212;&#29992;&#20013;&#65292;&#22270;&#20687;&#26159;&#20197;&#20219;&#24847;&#25110;&#38543;&#26426;&#26059;&#36716;&#21644;&#24179;&#31227;&#30340;&#26041;&#24335;&#33719;&#21462;&#30340;&#12290;&#22312;&#36825;&#26679;&#30340;&#35774;&#32622;&#20013;&#65292;&#33719;&#21462;&#19982;&#22270;&#20687;&#26041;&#21521;&#26080;&#20851;&#30340;&#35821;&#20041;&#34920;&#31034;&#26159;&#21487;&#21462;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#65288;INR&#65289;&#21644;&#36229;&#32593;&#32476;&#36827;&#34892;&#34920;&#31034;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#19981;&#21464;&#24615;&#34920;&#31034;&#23398;&#20064;&#65288;IRL-INR&#65289;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;IRL-INR&#21487;&#20197;&#22312;&#27604;&#20197;&#21069;&#30340;&#24037;&#20316;&#20013;&#26356;&#22797;&#26434;&#30340;&#22270;&#20687;&#19978;&#26377;&#25928;&#22320;&#23398;&#20064;&#20998;&#31163;&#30340;&#35821;&#20041;&#34920;&#31034;&#65292;&#24182;&#19988;&#34920;&#26126;&#36825;&#20123;&#35821;&#20041;&#34920;&#31034;&#21487;&#20197;&#19982;SCAN&#24456;&#22909;&#22320;&#21327;&#21516;&#24037;&#20316;&#65292;&#20174;&#32780;&#20135;&#29983;&#26368;&#20808;&#36827;&#30340;&#26080;&#30417;&#30563;&#32858;&#31867;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In many computer vision applications, images are acquired with arbitrary or random rotations and translations, and in such setups, it is desirable to obtain semantic representations disentangled from the image orientation. Examples of such applications include semiconductor wafer defect inspection, plankton microscope images, and inference on single-particle cryo-electron microscopy (cryo-EM) micro-graphs. In this work, we propose Invariant Representation Learning with Implicit Neural Representation (IRL-INR), which uses an implicit neural representation (INR) with a hypernetwork to obtain semantic representations disentangled from the orientation of the image. We show that IRL-INR can effectively learn disentangled semantic representations on more complex images compared to those considered in prior works and show that these semantic representations synergize well with SCAN to produce state-of-the-art unsupervised clustering results.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992; Evol-Instruct &#26041;&#27861;&#21019;&#24314;&#20102;&#22823;&#37327;&#19981;&#21516;&#22797;&#26434;&#24230;&#30340;&#25351;&#20196;&#25968;&#25454;&#29992;&#20110;&#24494;&#35843; LLaMA &#27169;&#22411;&#65292;&#24471;&#21040;&#20102;&#26032;&#27169;&#22411; WizardLM&#12290;&#20154;&#31867;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126; Evol-Instruct &#29983;&#25104;&#30340;&#25351;&#20196;&#20248;&#20110;&#20154;&#24037;&#21019;&#24314;&#30340;&#65292;&#32780; WizardLM &#36755;&#20986;&#30340;&#32467;&#26524;&#20063;&#27604; OpenAI ChatGPT &#26356;&#21463;&#27426;&#36814;&#12290;</title><link>http://arxiv.org/abs/2304.12244</link><description>&lt;p&gt;
WizardLM: &#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36981;&#24490;&#22797;&#26434;&#25351;&#20196;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
WizardLM: Empowering Large Language Models to Follow Complex Instructions. (arXiv:2304.12244v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12244
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992; Evol-Instruct &#26041;&#27861;&#21019;&#24314;&#20102;&#22823;&#37327;&#19981;&#21516;&#22797;&#26434;&#24230;&#30340;&#25351;&#20196;&#25968;&#25454;&#29992;&#20110;&#24494;&#35843; LLaMA &#27169;&#22411;&#65292;&#24471;&#21040;&#20102;&#26032;&#27169;&#22411; WizardLM&#12290;&#20154;&#31867;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126; Evol-Instruct &#29983;&#25104;&#30340;&#25351;&#20196;&#20248;&#20110;&#20154;&#24037;&#21019;&#24314;&#30340;&#65292;&#32780; WizardLM &#36755;&#20986;&#30340;&#32467;&#26524;&#20063;&#27604; OpenAI ChatGPT &#26356;&#21463;&#27426;&#36814;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#24320;&#25918;&#22495;&#25351;&#20196;&#36861;&#36394;&#25968;&#25454;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#24102;&#26469;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#25163;&#21160;&#21019;&#24314;&#36825;&#26679;&#30340;&#25351;&#20196;&#25968;&#25454;&#38750;&#24120;&#32791;&#26102;&#21644;&#21171;&#21160;&#23494;&#38598;&#65292;&#19988;&#20154;&#31867;&#21487;&#33021;&#38590;&#20197;&#29983;&#25104;&#39640;&#22797;&#26434;&#24230;&#25351;&#20196;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20351;&#29992;LLM&#32780;&#19981;&#26159;&#20154;&#31867;&#21019;&#24314;&#22823;&#37327;&#19981;&#21516;&#22797;&#26434;&#24230;&#25351;&#20196;&#25968;&#25454;&#30340;&#36884;&#24452;&#12290;&#25105;&#20204;&#20174;&#19968;&#32452;&#21021;&#22987;&#25351;&#20196;&#24320;&#22987;&#65292;&#20351;&#29992;&#25105;&#20204;&#25552;&#20986;&#30340;Evol-Instruct&#36880;&#27493;&#23558;&#20854;&#37325;&#26032;&#32534;&#20889;&#20026;&#26356;&#22797;&#26434;&#30340;&#25351;&#20196;&#12290;&#28982;&#21518;&#65292;&#23558;&#25152;&#26377;&#29983;&#25104;&#30340;&#25351;&#20196;&#25968;&#25454;&#28151;&#21512;&#20197;&#24494;&#35843;LLaMA&#12290;&#25105;&#20204;&#31216;&#32467;&#26524;&#27169;&#22411;&#20026;WizardLM&#12290;&#38024;&#23545;&#19968;&#20010;&#22797;&#26434;&#24230;&#24179;&#34913;&#30340;&#27979;&#35797;&#38598;&#21644;Vicuna&#30340;&#27979;&#35797;&#38598;&#36827;&#34892;&#30340;&#20154;&#31867;&#35780;&#20272;&#34920;&#26126;&#65292;Evol-Instruct&#29983;&#25104;&#30340;&#25351;&#20196;&#20248;&#20110;&#20154;&#24037;&#21019;&#24314;&#30340;&#25351;&#20196;&#12290;&#36890;&#36807;&#20998;&#26512;&#39640;&#22797;&#26434;&#24615;&#37096;&#20998;&#30340;&#20154;&#31867;&#35780;&#20272;&#32467;&#26524;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20174;&#25105;&#20204;&#30340;WizardLM&#29983;&#25104;&#30340;&#36755;&#20986;&#27604;&#20174;OpenAI ChatGPT&#29983;&#25104;&#30340;&#36755;&#20986;&#26356;&#21463;&#27426;&#36814;&#12290;&#22312;GPT-4&#33258;&#21160;&#35780;&#20272;&#20013;&#65292;WizardLM&#20135;&#29983;&#20102;&#26368;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training large language models (LLMs) with open-domain instruction following data brings colossal success. However, manually creating such instruction data is very time-consuming and labor-intensive. Moreover, humans may struggle to produce high-complexity instructions. In this paper, we show an avenue for creating large amounts of instruction data with varying levels of complexity using LLM instead of humans. Starting with an initial set of instructions, we use our proposed Evol-Instruct to rewrite them step by step into more complex instructions. Then, we mix all generated instruction data to fine-tune LLaMA. We call the resulting model WizardLM. Human evaluations on a complexity-balanced test bed and Vicuna's testset show that instructions from Evol-Instruct are superior to human-created ones. By analyzing the human evaluation results of the high complexity part, we demonstrate that outputs from our WizardLM are preferred to outputs from OpenAI ChatGPT. In GPT-4 automatic evaluation
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#20004;&#31181;&#32534;&#30721;&#65292;&#23558;&#26368;&#20248;&#24067;&#23616;&#32508;&#21512;&#20316;&#20026;&#32463;&#20856;&#35268;&#21010;&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#26368;&#20248;&#30340;&#32463;&#20856;&#35268;&#21010;&#22120;&#26469;&#32508;&#21512;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#30340;&#26368;&#20248;&#24067;&#23616;&#65292;&#35299;&#20915;&#20102;&#23545;&#20110;&#22823;&#35268;&#27169;&#37327;&#23376;&#27604;&#29305;&#20248;&#21270;&#24067;&#23616;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.12014</link><description>&lt;p&gt;
&#20316;&#20026;&#32463;&#20856;&#35745;&#21010;&#30340;&#37327;&#23376;&#30005;&#36335;&#26368;&#20248;&#24067;&#23616;&#32508;&#21512;
&lt;/p&gt;
&lt;p&gt;
Optimal Layout Synthesis for Quantum Circuits as Classical Planning. (arXiv:2304.12014v2 [quant-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12014
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#20004;&#31181;&#32534;&#30721;&#65292;&#23558;&#26368;&#20248;&#24067;&#23616;&#32508;&#21512;&#20316;&#20026;&#32463;&#20856;&#35268;&#21010;&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#26368;&#20248;&#30340;&#32463;&#20856;&#35268;&#21010;&#22120;&#26469;&#32508;&#21512;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#30340;&#26368;&#20248;&#24067;&#23616;&#65292;&#35299;&#20915;&#20102;&#23545;&#20110;&#22823;&#35268;&#27169;&#37327;&#23376;&#27604;&#29305;&#20248;&#21270;&#24067;&#23616;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24067;&#23616;&#32508;&#21512;&#20013;&#65292;&#23558;&#37327;&#23376;&#30005;&#36335;&#30340;&#36923;&#36753;&#37327;&#23376;&#27604;&#29305;&#26144;&#23556;&#21040;&#32473;&#23450;&#37327;&#23376;&#30828;&#20214;&#24179;&#21488;&#30340;&#29289;&#29702;&#37327;&#23376;&#27604;&#29305;&#65292;&#32771;&#34385;&#29289;&#29702;&#37327;&#23376;&#27604;&#29305;&#30340;&#36830;&#25509;&#12290;&#36825;&#28041;&#21450;&#22312;&#24212;&#29992;&#20110;&#36828;&#36317;&#31163;&#37327;&#23376;&#27604;&#29305;&#30340;&#25805;&#20316;&#20043;&#21069;&#25554;&#20837;SWAP&#38376;&#12290;&#26368;&#20248;&#24067;&#23616;&#32508;&#21512;&#23545;&#20110;&#24403;&#21069;&#35823;&#24046;&#29575;&#36739;&#39640;&#30340;&#30828;&#20214;&#19978;&#23454;&#29992;&#30340;&#37327;&#23376;&#35745;&#31639;&#38750;&#24120;&#37325;&#35201;&#65306;&#26368;&#23567;&#21270;SWAP&#38376;&#25968;&#37327;&#30452;&#25509;&#20943;&#36731;&#20102;&#36816;&#34892;&#37327;&#23376;&#30005;&#36335;&#26102;&#30340;&#38169;&#35823;&#29575;&#12290;&#36817;&#24180;&#26469;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#20960;&#31181;&#26041;&#27861;&#26469;&#26368;&#23567;&#21270;&#25152;&#38656;&#30340;SWAP&#25554;&#20837;&#27425;&#25968;&#12290;&#25152;&#25552;&#20986;&#30340;&#31934;&#30830;&#26041;&#27861;&#21482;&#33021;&#25193;&#23637;&#21040;&#23569;&#37327;&#30340;&#37327;&#23376;&#27604;&#29305;&#12290;&#35777;&#26126;&#25152;&#38656;&#30340;&#20132;&#25442;&#25554;&#20837;&#27425;&#25968;&#26159;&#26368;&#20248;&#30340;&#35201;&#27604;&#29983;&#25104;&#36817;&#20284;&#26368;&#20248;&#30340;&#26144;&#23556;&#22256;&#38590;&#24471;&#22810;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20004;&#31181;&#32534;&#30721;&#65292;&#23558;&#26368;&#20248;&#24067;&#23616;&#32508;&#21512;&#20316;&#20026;&#32463;&#20856;&#35268;&#21010;&#38382;&#39064;&#12290;&#25105;&#20204;&#20351;&#29992;&#26368;&#20248;&#30340;&#32463;&#20856;&#35268;&#21010;&#22120;&#26469;&#32508;&#21512;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#30340;&#26368;&#20248;&#24067;&#23616;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In Layout Synthesis, the logical qubits of a quantum circuit are mapped to the physical qubits of a given quantum hardware platform, taking into account the connectivity of physical qubits. This involves inserting SWAP gates before an operation is applied on distant qubits. Optimal Layout Synthesis is crucial for practical Quantum Computing on current error-prone hardware: Minimizing the number of SWAP gates directly mitigates the error rates when running quantum circuits.  In recent years, several approaches have been proposed for minimizing the required SWAP insertions. The proposed exact approaches can only scale to a small number of qubits. Proving that a number of swap insertions is optimal is much harder than producing near optimal mappings.  In this paper, we provide two encodings for Optimal Layout Synthesis as a classical planning problem. We use optimal classical planners to synthesize the optimal layout for a standard set of benchmarks. Our results show the scalability of ou
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#39537;&#21160;&#30340;&#30896;&#25758;&#26816;&#27979;&#26041;&#27861;&#65292;&#29992;&#20110;&#39640;&#25928;&#27169;&#25311;&#38750;&#20984;&#29289;&#20307;&#65292;&#19981;&#38656;&#35201;&#22312;&#35745;&#31639;&#36895;&#24230;&#21644;&#20934;&#30830;&#24615;&#20043;&#38388;&#36827;&#34892;&#26435;&#34913;&#65292;&#20855;&#26377;&#36739;&#23567;&#30340;&#22312;&#32447;&#35745;&#31639;&#26102;&#38388;&#21644;&#26356;&#22909;&#30340;GPU&#21033;&#29992;&#29575;&#12290;</title><link>http://arxiv.org/abs/2304.09439</link><description>&lt;p&gt;
&#29992;&#20110;GPU&#27169;&#25311;&#38750;&#20984;&#29289;&#20307;&#30340;&#26412;&#22320;&#29289;&#20307;&#35009;&#21098;&#30896;&#25758;&#32593;&#32476;&#30340;&#39640;&#25928;&#30896;&#25758;&#26816;&#27979;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Local object crop collision network for efficient simulation of non-convex objects in GPU-based simulators. (arXiv:2304.09439v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09439
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#39537;&#21160;&#30340;&#30896;&#25758;&#26816;&#27979;&#26041;&#27861;&#65292;&#29992;&#20110;&#39640;&#25928;&#27169;&#25311;&#38750;&#20984;&#29289;&#20307;&#65292;&#19981;&#38656;&#35201;&#22312;&#35745;&#31639;&#36895;&#24230;&#21644;&#20934;&#30830;&#24615;&#20043;&#38388;&#36827;&#34892;&#26435;&#34913;&#65292;&#20855;&#26377;&#36739;&#23567;&#30340;&#22312;&#32447;&#35745;&#31639;&#26102;&#38388;&#21644;&#26356;&#22909;&#30340;GPU&#21033;&#29992;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#24320;&#21457;&#19968;&#31181;&#29992;&#20110;GPU&#27169;&#25311;&#38750;&#20984;&#29289;&#20307;&#30340;&#39640;&#25928;&#30896;&#25758;&#26816;&#27979;&#31639;&#27861;&#12290;&#30446;&#21069;GPU&#27169;&#25311;&#22120;&#22312;&#27169;&#25311;&#38750;&#20984;&#29289;&#20307;&#26102;&#38656;&#35201;&#22312;&#36895;&#24230;&#12289;&#36890;&#29992;&#24615;&#21644;&#31934;&#24230;&#20043;&#38388;&#20570;&#20986;&#26435;&#34913;&#65292;&#20027;&#35201;&#38382;&#39064;&#22312;&#20110;&#29616;&#26377;&#30340;&#30896;&#25758;&#26816;&#27979;&#31639;&#27861;&#38656;&#35201;&#22312;&#35745;&#31639;&#36895;&#24230;&#21644;&#20934;&#30830;&#24615;&#20043;&#38388;&#36827;&#34892;&#26435;&#34913;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#39537;&#21160;&#30340;&#30896;&#25758;&#26816;&#27979;&#26041;&#27861;&#65292;&#20854;&#20013;&#31934;&#24230;&#20165;&#21462;&#20915;&#20110;&#31163;&#32447;&#25968;&#25454;&#38598;&#30340;&#36136;&#37327;&#21644;&#25968;&#37327;&#65292;&#32780;&#19981;&#26159;&#22312;&#32447;&#35745;&#31639;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
Our goal is to develop an efficient contact detection algorithm for large-scale GPU-based simulation of non-convex objects. Current GPU-based simulators such as IsaacGym and Brax must trade-off speed with fidelity, generality, or both when simulating non-convex objects. Their main issue lies in contact detection (CD): existing CD algorithms, such as Gilbert-Johnson-Keerthi (GJK), must trade off their computational speed with accuracy which becomes expensive as the number of collisions among non-convex objects increases. We propose a data-driven approach for CD, whose accuracy depends only on the quality and quantity of offline dataset rather than online computation time. Unlike GJK, our method inherently has a uniform computational flow, which facilitates efficient GPU usage based on advanced compilers such as XLA (Accelerated Linear Algebra). Further, we offer a data-efficient solution by learning the patterns of colliding local crop object shapes, rather than global object shapes whi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#33258;&#25105;&#20013;&#24515;&#35760;&#24518;&#21644;&#25511;&#21046;&#30340;&#26694;&#26550;LLM-Brain&#65292;&#20351;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#26426;&#22120;&#20154;&#22823;&#33041;&#36827;&#34892;&#38646;-shot&#23398;&#20064;&#12290;&#35813;&#26694;&#26550;&#21253;&#25324;&#23553;&#38381;&#24335;&#22810;&#36718;&#23545;&#35805;&#65292;&#35206;&#30422;&#20102;&#24863;&#30693;&#12289;&#35268;&#21010;&#12289;&#25511;&#21046;&#21644;&#35760;&#24518;&#65292;&#20855;&#26377;&#24456;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#36866;&#29992;&#20110;&#22810;&#20010;&#26426;&#22120;&#20154;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2304.09349</link><description>&lt;p&gt;
LLM&#20316;&#20026;&#26426;&#22120;&#20154;&#30340;&#22823;&#33041;&#65306;&#32479;&#19968;&#33258;&#25105;&#20013;&#24515;&#35760;&#24518;&#19982;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
LLM as A Robotic Brain: Unifying Egocentric Memory and Control. (arXiv:2304.09349v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09349
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#33258;&#25105;&#20013;&#24515;&#35760;&#24518;&#21644;&#25511;&#21046;&#30340;&#26694;&#26550;LLM-Brain&#65292;&#20351;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#26426;&#22120;&#20154;&#22823;&#33041;&#36827;&#34892;&#38646;-shot&#23398;&#20064;&#12290;&#35813;&#26694;&#26550;&#21253;&#25324;&#23553;&#38381;&#24335;&#22810;&#36718;&#23545;&#35805;&#65292;&#35206;&#30422;&#20102;&#24863;&#30693;&#12289;&#35268;&#21010;&#12289;&#25511;&#21046;&#21644;&#35760;&#24518;&#65292;&#20855;&#26377;&#24456;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#36866;&#29992;&#20110;&#22810;&#20010;&#26426;&#22120;&#20154;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20307;&#24863;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#21644;&#24320;&#21457;&#20855;&#22791;&#29289;&#29702;&#25110;&#34394;&#25311;&#23454;&#20307;&#65288;&#21363;&#26426;&#22120;&#20154;&#65289;&#24182;&#33021;&#22815;&#19982;&#29615;&#22659;&#21160;&#24577;&#20132;&#20114;&#30340;&#26234;&#33021;&#31995;&#32479;&#12290;&#35760;&#24518;&#21644;&#25511;&#21046;&#26159;&#20307;&#24863;&#31995;&#32479;&#30340;&#20004;&#20010;&#22522;&#26412;&#37096;&#20998;&#65292;&#36890;&#24120;&#38656;&#35201;&#20998;&#21035;&#20351;&#29992;&#26694;&#26550;&#36827;&#34892;&#24314;&#27169;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#12289;&#21487;&#25512;&#24191;&#30340;&#26694;&#26550;&#65292;&#31216;&#20026;LLM-Brain&#65306;&#20351;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#26426;&#22120;&#20154;&#22823;&#33041;&#65292;&#32479;&#19968;&#33258;&#25105;&#20013;&#24515;&#35760;&#24518;&#21644;&#25511;&#21046;&#12290;LLM-Brain&#26694;&#26550;&#38598;&#25104;&#20102;&#22810;&#20010;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#26426;&#22120;&#20154;&#20219;&#21153;&#65292;&#21033;&#29992;&#38646;-shot&#23398;&#20064;&#26041;&#27861;&#12290;LLM-Brain&#20013;&#30340;&#25152;&#26377;&#32452;&#20214;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#36827;&#34892;&#23553;&#38381;&#24335;&#22810;&#36718;&#23545;&#35805;&#65292;&#21253;&#25324;&#24863;&#30693;&#12289;&#35268;&#21010;&#12289;&#25511;&#21046;&#21644;&#35760;&#24518;&#12290;&#31995;&#32479;&#30340;&#26680;&#24515;&#26159;&#19968;&#20010;&#20855;&#22791;&#33258;&#25105;&#20013;&#24515;&#35760;&#24518;&#21644;&#25511;&#21046;&#26426;&#22120;&#20154;&#30340;&#23454;&#20307;LLM&#12290;&#25105;&#20204;&#36890;&#36807;&#30740;&#31350;&#20004;&#20010;&#19979;&#28216;&#20219;&#21153;&#65306;&#20027;&#21160;&#25506;&#32034;&#21644;&#23454;&#20307;&#38382;&#31572;&#26469;&#28436;&#31034;LLM-Brain&#12290;
&lt;/p&gt;
&lt;p&gt;
Embodied AI focuses on the study and development of intelligent systems that possess a physical or virtual embodiment (i.e. robots) and are able to dynamically interact with their environment. Memory and control are the two essential parts of an embodied system and usually require separate frameworks to model each of them. In this paper, we propose a novel and generalizable framework called LLM-Brain: using Large-scale Language Model as a robotic brain to unify egocentric memory and control. The LLM-Brain framework integrates multiple multimodal language models for robotic tasks, utilizing a zero-shot learning approach. All components within LLM-Brain communicate using natural language in closed-loop multi-round dialogues that encompass perception, planning, control, and memory. The core of the system is an embodied LLM to maintain egocentric memory and control the robot. We demonstrate LLM-Brain by examining two downstream tasks: active exploration and embodied question answering. The
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20043;&#20026;instructRL&#30340;&#26032;&#30340;&#26694;&#26550;&#65292;&#23427;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#26469;&#25351;&#23450;&#23545;&#20154;&#24037;&#26234;&#33021;&#25645;&#26723;&#30340;&#39044;&#26399;&#31574;&#30053;&#65292;&#35299;&#20915;&#22312;&#32570;&#20047;&#39640;&#36136;&#37327;&#20154;&#31867;&#34892;&#20026;&#25968;&#25454;&#30340;&#39046;&#22495;&#20013;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#25910;&#25947;&#20110;&#20154;&#31867;&#19981;&#20559;&#29233;&#30340;&#31574;&#30053;&#30340;&#38382;&#39064;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.07297</link><description>&lt;p&gt;
&#35821;&#35328;&#25351;&#23548;&#19979;&#30340;&#24378;&#21270;&#23398;&#20064;&#20197;&#23454;&#29616;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;
&lt;/p&gt;
&lt;p&gt;
Language Instructed Reinforcement Learning for Human-AI Coordination. (arXiv:2304.07297v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07297
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20043;&#20026;instructRL&#30340;&#26032;&#30340;&#26694;&#26550;&#65292;&#23427;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#26469;&#25351;&#23450;&#23545;&#20154;&#24037;&#26234;&#33021;&#25645;&#26723;&#30340;&#39044;&#26399;&#31574;&#30053;&#65292;&#35299;&#20915;&#22312;&#32570;&#20047;&#39640;&#36136;&#37327;&#20154;&#31867;&#34892;&#20026;&#25968;&#25454;&#30340;&#39046;&#22495;&#20013;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#25910;&#25947;&#20110;&#20154;&#31867;&#19981;&#20559;&#29233;&#30340;&#31574;&#30053;&#30340;&#38382;&#39064;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#30340;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#26159;&#22914;&#20309;&#35753;&#26234;&#33021;&#20307;&#33021;&#22815;&#21644;&#20154;&#31867;&#26377;&#25928;&#22320;&#21327;&#20316;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20043;&#20026;instructRL&#30340;&#26032;&#30340;&#26694;&#26550;&#65292;&#35753;&#20154;&#20204;&#21487;&#20197;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#26469;&#25351;&#23450;&#23545;&#20154;&#24037;&#26234;&#33021;&#25645;&#26723;&#30340;&#39044;&#26399;&#31574;&#30053;&#65292;&#20197;&#27492;&#35299;&#20915;&#22312;&#32570;&#20047;&#36739;&#39640;&#36136;&#37327;&#30340;&#20154;&#31867;&#34892;&#20026;&#25968;&#25454;&#30340;&#39046;&#22495;&#20013;&#65292;&#30001;&#20110;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#24120;&#24120;&#20250;&#25910;&#25947;&#21040;&#20154;&#31867;&#24182;&#19981;&#20559;&#29233;&#30340;&#31574;&#30053;&#30340;&#19981;&#36275;&#12290;&#25105;&#20204;&#20351;&#29992;&#39044;&#20808;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#29983;&#25104;&#19968;&#20010;&#22312;&#20154;&#31867;&#25351;&#20196;&#19979;&#30340;&#20808;&#39564;&#31574;&#30053;&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;&#32422;&#26463;&#24378;&#21270;&#23398;&#20064;&#30446;&#26631;&#12290;&#36825;&#23548;&#33268;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#25910;&#25947;&#21040;&#19982;&#20154;&#31867;&#21916;&#22909;&#19968;&#33268;&#30340;&#22343;&#34913;&#28857;&#12290;&#36890;&#36807;&#27010;&#24565;&#35777;&#26126;&#29615;&#22659;&#21644;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;Hanabi&#22522;&#20934;&#65292;&#35777;&#26126;&#20102;instructRL&#25910;&#25947;&#20110;&#28385;&#36275;&#32473;&#23450;&#25351;&#20196;&#30340;&#31867;&#20284;&#20154;&#31867;&#26234;&#33021;&#20307;&#30340;&#31574;&#30053;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#30693;&#36947;&#35821;&#35328;&#25351;&#20196;&#26174;&#33879;&#25552;&#39640;&#20102;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the fundamental quests of AI is to produce agents that coordinate well with humans. This problem is challenging, especially in domains that lack high quality human behavioral data, because multi-agent reinforcement learning (RL) often converges to different equilibria from the ones that humans prefer. We propose a novel framework, instructRL, that enables humans to specify what kind of strategies they expect from their AI partners through natural language instructions. We use pretrained large language models to generate a prior policy conditioned on the human instruction and use the prior to regularize the RL objective. This leads to the RL agent converging to equilibria that are aligned with human preferences. We show that instructRL converges to human-like policies that satisfy the given instructions in a proof-of-concept environment as well as the challenging Hanabi benchmark. Finally, we show that knowing the language instruction significantly boosts human-AI coordination pe
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;FITCARL&#30340;TKGC&#26041;&#27861;&#65292;&#23427;&#23558;&#23569;&#26679;&#26412;&#23398;&#20064;&#19982;&#24378;&#21270;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#20197;&#23454;&#29616;&#22312;&#26102;&#38388;&#30693;&#35782;&#22270;&#19978;&#36827;&#34892;&#23569;&#26679;&#26412;&#24402;&#32435;&#23398;&#20064;&#21644;&#39044;&#27979;&#12290;&#22312;FITCARL&#20013;&#65292;&#19968;&#20010;&#26234;&#33021;&#20307;&#36890;&#36807;&#31574;&#30053;&#32593;&#32476;&#26469;&#25351;&#23548;&#25628;&#32034;&#36807;&#31243;&#65292;&#36890;&#36807;&#24341;&#20837;&#21512;&#25104;&#26679;&#26412;&#30340;&#27169;&#22359;&#26469;&#35299;&#20915;&#25968;&#25454;&#31232;&#32570;&#24615;&#65292;&#22312;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#19978;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.00613</link><description>&lt;p&gt;
&#20351;&#29992;&#32622;&#20449;&#24230;&#22686;&#24378;&#24378;&#21270;&#23398;&#20064;&#22312;&#26102;&#38388;&#30693;&#35782;&#22270;&#19978;&#25913;&#36827;&#23569;&#26679;&#26412;&#24402;&#32435;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Improving Few-Shot Inductive Learning on Temporal Knowledge Graphs using Confidence-Augmented Reinforcement Learning. (arXiv:2304.00613v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00613
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;FITCARL&#30340;TKGC&#26041;&#27861;&#65292;&#23427;&#23558;&#23569;&#26679;&#26412;&#23398;&#20064;&#19982;&#24378;&#21270;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#20197;&#23454;&#29616;&#22312;&#26102;&#38388;&#30693;&#35782;&#22270;&#19978;&#36827;&#34892;&#23569;&#26679;&#26412;&#24402;&#32435;&#23398;&#20064;&#21644;&#39044;&#27979;&#12290;&#22312;FITCARL&#20013;&#65292;&#19968;&#20010;&#26234;&#33021;&#20307;&#36890;&#36807;&#31574;&#30053;&#32593;&#32476;&#26469;&#25351;&#23548;&#25628;&#32034;&#36807;&#31243;&#65292;&#36890;&#36807;&#24341;&#20837;&#21512;&#25104;&#26679;&#26412;&#30340;&#27169;&#22359;&#26469;&#35299;&#20915;&#25968;&#25454;&#31232;&#32570;&#24615;&#65292;&#22312;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#19978;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#30693;&#35782;&#22270;&#34917;&#20840;&#65288;TKGC&#65289;&#26088;&#22312;&#39044;&#27979;&#26102;&#38388;&#30693;&#35782;&#22270;&#65288;TKG&#65289;&#20013;&#23454;&#20307;&#20043;&#38388;&#32570;&#22833;&#30340;&#32852;&#31995;&#12290;&#22823;&#22810;&#25968;&#20808;&#21069;&#30340;TKGC&#26041;&#27861;&#20165;&#32771;&#34385;&#22312;&#35757;&#32451;&#38598;&#20013;&#30475;&#21040;&#30340;&#23454;&#20307;&#20043;&#38388;&#39044;&#27979;&#32570;&#22833;&#30340;&#32852;&#31995;&#65292;&#32780;&#26080;&#27861;&#22312;&#20851;&#20110;&#26032;&#20986;&#29616;&#30340;&#26410;&#35265;&#23454;&#20307;&#30340;&#38142;&#25509;&#39044;&#27979;&#26041;&#38754;&#36798;&#21040;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;&#26368;&#36817;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20219;&#21153;&#65292;&#21363;TKG&#23569;&#26679;&#26412;&#22330;&#26223;&#22806;&#38142;&#25509;&#39044;&#27979;&#65292;&#20854;&#20013;TKGC&#27169;&#22411;&#38656;&#35201;&#22312;&#20851;&#20110;&#20165;&#26377;&#23569;&#37327;&#35266;&#23519;&#26679;&#26412;&#30340;&#26032;&#20986;&#29616;&#23454;&#20307;&#26041;&#38754;&#23454;&#29616;&#33391;&#22909;&#30340;&#38142;&#25509;&#39044;&#27979;&#24615;&#33021;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FITCARL&#30340;TKGC&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#23558;&#23569;&#26679;&#26412;&#23398;&#20064;&#19982;&#24378;&#21270;&#23398;&#20064;&#30456;&#32467;&#21512;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#22312;FITCARL&#20013;&#65292;&#19968;&#20010;&#26234;&#33021;&#20307;&#36328;&#36234;&#25972;&#20010;TKG&#23547;&#25214;&#39044;&#27979;&#31572;&#26696;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#31574;&#30053;&#32593;&#32476;&#65292;&#26681;&#25454;&#36941;&#21382;&#30340;&#36335;&#24452;&#25351;&#23548;&#25628;&#32034;&#36807;&#31243;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#35299;&#20915;&#23569;&#26679;&#26412;&#35774;&#32622;&#20013;&#30340;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#27169;&#22359;&#65292;&#29983;&#25104;&#21512;&#25104;&#26679;&#26412;&#26469;&#22686;&#21152;&#35757;&#32451;&#38598;&#12290;&#36890;&#36807;&#22312;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24191;&#27867;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;FITCARL&#22312;&#23569;&#26679;&#26412;&#22330;&#26223;&#22806;&#38142;&#25509;&#39044;&#27979;&#21644;&#20256;&#32479;TKGC&#20219;&#21153;&#19978;&#22343;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Temporal knowledge graph completion (TKGC) aims to predict the missing links among the entities in a temporal knwoledge graph (TKG). Most previous TKGC methods only consider predicting the missing links among the entities seen in the training set, while they are unable to achieve great performance in link prediction concerning newly-emerged unseen entities. Recently, a new task, i.e., TKG few-shot out-of-graph (OOG) link prediction, is proposed, where TKGC models are required to achieve great link prediction performance concerning newly-emerged entities that only have few-shot observed examples. In this work, we propose a TKGC method FITCARL that combines few-shot learning with reinforcement learning to solve this task. In FITCARL, an agent traverses through the whole TKG to search for the prediction answer. A policy network is designed to guide the search process based on the traversed path. To better address the data scarcity problem in the few-shot setting, we introduce a module tha
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#32508;&#36848;&#20102;&#32593;&#32476;&#39537;&#21160;&#30340;&#23041;&#32961;&#21644;&#38382;&#39064;&#30340;&#31995;&#32479;&#20998;&#31867;&#65292;&#23457;&#26597;&#20102;&#32593;&#32476;&#31995;&#32479;&#20013;&#30340;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#22312;&#32593;&#32476;&#23433;&#20840;&#20013;&#30340;&#26368;&#26032;&#25216;&#26415;&#65292;&#24182;&#21246;&#30011;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#26377;&#21069;&#36884;&#30340;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2303.12942</link><description>&lt;p&gt;
&#35770;&#32593;&#32476;&#32593;&#32476;&#23433;&#20840;&#30340;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey on Explainable Artificial Intelligence for Network Cybersecurity. (arXiv:2303.12942v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12942
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#32508;&#36848;&#20102;&#32593;&#32476;&#39537;&#21160;&#30340;&#23041;&#32961;&#21644;&#38382;&#39064;&#30340;&#31995;&#32479;&#20998;&#31867;&#65292;&#23457;&#26597;&#20102;&#32593;&#32476;&#31995;&#32479;&#20013;&#30340;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#22312;&#32593;&#32476;&#23433;&#20840;&#20013;&#30340;&#26368;&#26032;&#25216;&#26415;&#65292;&#24182;&#21246;&#30011;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#26377;&#21069;&#36884;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#30340;&#40657;&#21283;&#23376;&#29305;&#24615;&#19968;&#30452;&#26159;&#29992;&#20110;&#20851;&#38190;&#24212;&#29992;&#31243;&#24207;&#30340;&#35768;&#22810;&#20851;&#27880;&#28857;&#30340;&#26469;&#28304;&#12290;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#26159;&#19968;&#20010;&#24555;&#36895;&#21457;&#23637;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#26088;&#22312;&#21019;&#24314;&#33021;&#22815;&#20026;&#20854;&#20915;&#31574;&#21644;&#34892;&#21160;&#25552;&#20379;&#28165;&#26224;&#21487;&#35299;&#37322;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#22312;&#32593;&#32476;&#32593;&#32476;&#23433;&#20840;&#39046;&#22495;&#65292;XAI&#20855;&#26377;&#36890;&#36807;&#20351;&#25105;&#20204;&#33021;&#22815;&#26356;&#22909;&#22320;&#20102;&#35299;&#32593;&#32476;&#23041;&#32961;&#30340;&#34892;&#20026;&#24182;&#35774;&#35745;&#26356;&#26377;&#25928;&#30340;&#38450;&#24481;&#26469;&#24443;&#24213;&#25913;&#21464;&#25105;&#20204;&#22788;&#29702;&#32593;&#32476;&#23433;&#20840;&#30340;&#26041;&#24335;&#30340;&#28508;&#21147;&#12290;&#22312;&#26412;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#23545;&#32593;&#32476;&#39537;&#21160;&#30340;&#23041;&#32961;&#21644;&#38382;&#39064;&#36827;&#34892;&#31995;&#32479;&#20998;&#31867;&#65292;&#23457;&#26597;&#20102;&#32593;&#32476;&#31995;&#32479;&#20013;&#30340;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#22312;&#32593;&#32476;&#32593;&#32476;&#23433;&#20840;&#20013;&#30340;&#26368;&#26032;&#25216;&#26415;&#65292;&#25506;&#35752;&#20102;&#21508;&#31181;&#35299;&#20915;&#36825;&#20010;&#37325;&#35201;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#24403;&#21069;XAI&#26041;&#27861;&#22312;&#32593;&#32476;&#23433;&#20840;&#32972;&#26223;&#19979;&#30340;&#25361;&#25112;&#21644;&#23616;&#38480;&#24615;&#65292;&#24182;&#21246;&#30011;&#26410;&#26469;&#30740;&#31350;&#30340;&#26377;&#21069;&#36884;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
The black-box nature of artificial intelligence (AI) models has been the source of many concerns in their use for critical applications. Explainable Artificial Intelligence (XAI) is a rapidly growing research field that aims to create machine learning models that can provide clear and interpretable explanations for their decisions and actions. In the field of network cybersecurity, XAI has the potential to revolutionize the way we approach network security by enabling us to better understand the behavior of cyber threats and to design more effective defenses. In this survey, we review the state of the art in XAI for cybersecurity in network systems and explore the various approaches that have been proposed to address this important problem. The review follows a systematic classification of network-driven cybersecurity threats and issues. We discuss the challenges and limitations of current XAI methods in the context of cybersecurity and outline promising directions for future research.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986; Reflexion &#26041;&#27861;&#65292;&#32473;&#26234;&#33021;&#20307;&#36171;&#20104;&#20102;&#21160;&#24577;&#35760;&#24518;&#21644;&#33258;&#25105;&#21453;&#24605;&#33021;&#21147;&#65292;&#20197;&#22686;&#24378;&#20854;&#20219;&#21153;&#29305;&#23450;&#30340;&#34892;&#21160;&#36873;&#25321;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.11366</link><description>&lt;p&gt;
Reflexion&#65306;&#20855;&#26377;&#21160;&#24577;&#35760;&#24518;&#21644;&#33258;&#25105;&#21453;&#24605;&#30340;&#33258;&#20027;&#26234;&#33021;&#20307;
&lt;/p&gt;
&lt;p&gt;
Reflexion: an autonomous agent with dynamic memory and self-reflection. (arXiv:2303.11366v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11366
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986; Reflexion &#26041;&#27861;&#65292;&#32473;&#26234;&#33021;&#20307;&#36171;&#20104;&#20102;&#21160;&#24577;&#35760;&#24518;&#21644;&#33258;&#25105;&#21453;&#24605;&#33021;&#21147;&#65292;&#20197;&#22686;&#24378;&#20854;&#20219;&#21153;&#29305;&#23450;&#30340;&#34892;&#21160;&#36873;&#25321;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20915;&#31574;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20195;&#29702;&#30340;&#21457;&#23637;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#36890;&#24120;&#38656;&#35201;&#20869;&#37096;&#27169;&#22411;&#24494;&#35843;&#12289;&#22806;&#37096;&#27169;&#22411;&#24494;&#35843;&#25110;&#22312;&#23450;&#20041;&#30340;&#29366;&#24577;&#31354;&#38388;&#19978;&#36827;&#34892;&#31574;&#30053;&#20248;&#21270;&#12290;&#30001;&#20110;&#39640;&#36136;&#37327;&#35757;&#32451;&#25968;&#25454;&#30340;&#31232;&#32570;&#24615;&#25110;&#32570;&#20047;&#33391;&#22909;&#23450;&#20041;&#30340;&#29366;&#24577;&#31354;&#38388;&#65292;&#23454;&#29616;&#36825;&#20123;&#26041;&#27861;&#21487;&#33021;&#20250;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#20195;&#29702;&#27809;&#26377;&#20154;&#31867;&#20915;&#31574;&#36807;&#31243;&#22266;&#26377;&#30340;&#26576;&#20123;&#21697;&#36136;&#65292;&#29305;&#21035;&#26159;&#20174;&#38169;&#35823;&#20013;&#23398;&#20064;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#21453;&#24605;&#65292;&#20154;&#31867;&#21487;&#20197;&#36890;&#36807;&#35797;&#38169;&#36807;&#31243;&#39640;&#25928;&#22320;&#35299;&#20915;&#26032;&#30340;&#38382;&#39064;&#12290;&#22312;&#26368;&#36817;&#30340;&#30740;&#31350;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986; Reflexion&#65292;&#19968;&#31181;&#23558;&#21160;&#24577;&#35760;&#24518;&#21644;&#33258;&#25105;&#21453;&#24605;&#33021;&#21147;&#36171;&#20104;&#26234;&#33021;&#20307;&#30340;&#26041;&#27861;&#65292;&#20197;&#22686;&#24378;&#20854;&#29616;&#26377;&#30340;&#25512;&#29702;&#36712;&#36857;&#21644;&#20219;&#21153;&#29305;&#23450;&#30340;&#34892;&#21160;&#36873;&#25321;&#33021;&#21147;&#12290;&#20026;&#20102;&#23454;&#29616;&#23436;&#20840;&#33258;&#21160;&#21270;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in decision-making large language model (LLM) agents have demonstrated impressive performance across various benchmarks. However, these state-of-the-art approaches typically necessitate internal model fine-tuning, external model fine-tuning, or policy optimization over a defined state space. Implementing these methods can prove challenging due to the scarcity of high-quality training data or the lack of well-defined state space. Moreover, these agents do not possess certain qualities inherent to human decision-making processes, specifically the ability to learn from mistakes. Self-reflection allows humans to efficiently solve novel problems through a process of trial and error. Building on recent research, we propose Reflexion, an approach that endows an agent with dynamic memory and self-reflection capabilities to enhance its existing reasoning trace and task-specific action choice abilities. To achieve full automation, we introduce a straightforward yet effective 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#25968;&#25454;&#20013;&#24515;&#20154;&#24037;&#26234;&#33021;&#30340;&#24517;&#35201;&#24615;&#24182;&#20174;&#19977;&#20010;&#19968;&#33324;&#24615;&#25968;&#25454;&#20013;&#24515;&#30446;&#26631;&#21644;&#20195;&#34920;&#24615;&#26041;&#27861;&#30340;&#20840;&#38754;&#35270;&#35282;&#36827;&#34892;&#20102;&#20171;&#32461;&#12290;&#35813;&#32508;&#36848;&#20174;&#33258;&#21160;&#21270;&#21644;&#21327;&#20316;&#30340;&#35282;&#24230;&#32452;&#32455;&#20102;&#29616;&#26377;&#25991;&#29486;&#24182;&#35752;&#35770;&#20102;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2303.10158</link><description>&lt;p&gt;
&#25968;&#25454;&#20013;&#24515;&#20154;&#24037;&#26234;&#33021;&#32508;&#36848;&#65306;&#19968;&#20221;&#35843;&#26597;&#25253;&#21578;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data-centric Artificial Intelligence: A Survey. (arXiv:2303.10158v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10158
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#25968;&#25454;&#20013;&#24515;&#20154;&#24037;&#26234;&#33021;&#30340;&#24517;&#35201;&#24615;&#24182;&#20174;&#19977;&#20010;&#19968;&#33324;&#24615;&#25968;&#25454;&#20013;&#24515;&#30446;&#26631;&#21644;&#20195;&#34920;&#24615;&#26041;&#27861;&#30340;&#20840;&#38754;&#35270;&#35282;&#36827;&#34892;&#20102;&#20171;&#32461;&#12290;&#35813;&#32508;&#36848;&#20174;&#33258;&#21160;&#21270;&#21644;&#21327;&#20316;&#30340;&#35282;&#24230;&#32452;&#32455;&#20102;&#29616;&#26377;&#25991;&#29486;&#24182;&#35752;&#35770;&#20102;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#27491;&#22312;&#20960;&#20046;&#25152;&#26377;&#39046;&#22495;&#20135;&#29983;&#28145;&#36828;&#30340;&#24433;&#21709;&#65292;&#20854;&#25104;&#21151;&#30340;&#20851;&#38190;&#20043;&#19968;&#26159;&#21487;&#29992;&#20110;&#26500;&#24314;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20016;&#23500;&#39640;&#36136;&#37327;&#25968;&#25454;&#12290;&#26368;&#36817;&#65292;&#25968;&#25454;&#22312;AI&#20013;&#30340;&#20316;&#29992;&#24471;&#21040;&#20102;&#26174;&#33879;&#25918;&#22823;&#65292;&#24341;&#21457;&#20102;&#25968;&#25454;&#20013;&#24515;AI&#36825;&#19968;&#26032;&#20852;&#27010;&#24565;&#30340;&#20986;&#29616;&#12290;&#30740;&#31350;&#20154;&#21592;&#21644;&#20174;&#19994;&#32773;&#30340;&#27880;&#24847;&#21147;&#36880;&#28176;&#20174;&#25512;&#36827;&#27169;&#22411;&#35774;&#35745;&#36716;&#21521;&#25552;&#39640;&#25968;&#25454;&#36136;&#37327;&#21644;&#25968;&#37327;&#12290;&#22312;&#26412;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#25968;&#25454;&#20013;&#24515;AI&#30340;&#24517;&#35201;&#24615;&#65292;&#38543;&#21518;&#20174;&#35757;&#32451;&#25968;&#25454;&#24320;&#21457;&#12289;&#25512;&#29702;&#25968;&#25454;&#24320;&#21457;&#21644;&#25968;&#25454;&#32500;&#25252;&#19977;&#20010;&#19968;&#33324;&#24615;&#25968;&#25454;&#20013;&#24515;&#30446;&#26631;&#20197;&#21450;&#20195;&#34920;&#24615;&#26041;&#27861;&#30340;&#20840;&#38754;&#35270;&#35282;&#36827;&#34892;&#20102;&#20171;&#32461;&#12290;&#25105;&#20204;&#36824;&#20174;&#33258;&#21160;&#21270;&#21644;&#21327;&#20316;&#30340;&#35282;&#24230;&#32452;&#32455;&#20102;&#29616;&#26377;&#25991;&#29486;&#65292;&#35752;&#35770;&#20102;&#25361;&#25112;&#65292;&#24182;&#21015;&#20986;&#20102;&#21508;&#31181;&#20219;&#21153;&#30340;&#27979;&#35797;&#22522;&#20934;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#36825;&#26159;&#31532;&#19968;&#20221;&#25552;&#20379;&#36328;&#36234;&#21508;&#20010;&#38454;&#27573;&#19968;&#31995;&#21015;&#20219;&#21153;&#30340;&#20840;&#29699;&#35270;&#35282;&#30340;&#32508;&#21512;&#24615;&#35843;&#26597;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial Intelligence (AI) is making a profound impact in almost every domain. A vital enabler of its great success is the availability of abundant and high-quality data for building machine learning models. Recently, the role of data in AI has been significantly magnified, giving rise to the emerging concept of data-centric AI. The attention of researchers and practitioners has gradually shifted from advancing model design to enhancing the quality and quantity of the data. In this survey, we discuss the necessity of data-centric AI, followed by a holistic view of three general data-centric goals (training data development, inference data development, and data maintenance) and the representative methods. We also organize the existing literature from automation and collaboration perspectives, discuss the challenges, and tabulate the benchmarks for various tasks. We believe this is the first comprehensive survey that provides a global view of a spectrum of tasks across various stages o
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FlexGen&#30340;&#25216;&#26415;&#65292;&#20351;&#29992;&#21333;&#20010;GPU&#23454;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#21534;&#21520;&#25512;&#29702;&#12290;FlexGen&#36890;&#36807;&#32858;&#21512;&#26469;&#33258;GPU&#12289;CPU&#21644;&#30913;&#30424;&#30340;&#20869;&#23384;&#21644;&#35745;&#31639;&#65292;&#25628;&#32034;&#26377;&#25928;&#30340;&#24352;&#37327;&#23384;&#20648;&#21644;&#35775;&#38382;&#27169;&#24335;&#65292;&#24182;&#23558;&#26435;&#37325;&#21644;&#27880;&#24847;&#21147;&#32531;&#23384;&#21387;&#32553;&#20026;4&#20010;&#20301;&#12290;&#36825;&#20123;&#25216;&#26415;&#20351;FlexGen&#20855;&#26377;&#26356;&#22823;&#30340;&#25209;&#37327;&#36873;&#25321;&#31354;&#38388;&#65292;&#20174;&#32780;&#26174;&#33879;&#22686;&#21152;&#20102;&#26368;&#22823;&#21534;&#21520;&#37327;&#12290;</title><link>http://arxiv.org/abs/2303.06865</link><description>&lt;p&gt;
&#21333;&#20010;GPU&#30340;&#39640;&#21534;&#21520;&#22823;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#25512;&#29702;&#25216;&#26415;&#8212;&#8212;FlexGen
&lt;/p&gt;
&lt;p&gt;
FlexGen: High-Throughput Generative Inference of Large Language Models with a Single GPU. (arXiv:2303.06865v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06865
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FlexGen&#30340;&#25216;&#26415;&#65292;&#20351;&#29992;&#21333;&#20010;GPU&#23454;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#21534;&#21520;&#25512;&#29702;&#12290;FlexGen&#36890;&#36807;&#32858;&#21512;&#26469;&#33258;GPU&#12289;CPU&#21644;&#30913;&#30424;&#30340;&#20869;&#23384;&#21644;&#35745;&#31639;&#65292;&#25628;&#32034;&#26377;&#25928;&#30340;&#24352;&#37327;&#23384;&#20648;&#21644;&#35775;&#38382;&#27169;&#24335;&#65292;&#24182;&#23558;&#26435;&#37325;&#21644;&#27880;&#24847;&#21147;&#32531;&#23384;&#21387;&#32553;&#20026;4&#20010;&#20301;&#12290;&#36825;&#20123;&#25216;&#26415;&#20351;FlexGen&#20855;&#26377;&#26356;&#22823;&#30340;&#25209;&#37327;&#36873;&#25321;&#31354;&#38388;&#65292;&#20174;&#32780;&#26174;&#33879;&#22686;&#21152;&#20102;&#26368;&#22823;&#21534;&#21520;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;(LLM)&#30340;&#39640;&#35745;&#31639;&#21644;&#20869;&#23384;&#38656;&#27714;&#20351;&#20854;&#21482;&#33021;&#22312;&#22810;&#20010;&#39640;&#31471;&#21152;&#36895;&#22120;&#19978;&#23454;&#29616;&#12290;&#26412;&#25991;&#30528;&#30524;&#20110;&#23545;&#25209;&#22788;&#29702;&#19981;&#25935;&#24863;&#30340;&#20219;&#21153;&#65292;&#24182;&#20351;&#29992;&#26377;&#38480;&#36164;&#28304;&#65288;&#22914;&#21333;&#20010;&#26222;&#36890;GPU&#65289;&#36827;&#34892;&#39640;&#21534;&#21520;&#37327;LLM&#25512;&#29702;&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;FlexGen&#65292;&#19968;&#31181;&#29992;&#20110;&#36816;&#34892;&#20855;&#26377;GPU&#20869;&#23384;&#38480;&#21046;&#30340;LLMs&#30340;&#39640;&#21534;&#21520;&#37327;&#29983;&#25104;&#24341;&#25806;&#12290;&#36890;&#36807;&#32858;&#21512;&#26469;&#33258;GPU&#12289;CPU&#21644;&#30913;&#30424;&#30340;&#20869;&#23384;&#21644;&#35745;&#31639;&#65292;&#21487;&#20197;&#22312;&#21508;&#31181;&#30828;&#20214;&#36164;&#28304;&#32422;&#26463;&#19979;&#28789;&#27963;&#37197;&#32622;FlexGen&#12290;&#36890;&#36807;&#35299;&#20915;&#32447;&#24615;&#35268;&#21010;&#38382;&#39064;&#65292;&#23427;&#21487;&#20197;&#25628;&#32034;&#26377;&#25928;&#30340;&#24352;&#37327;&#23384;&#20648;&#21644;&#35775;&#38382;&#27169;&#24335;&#12290;FlexGen&#36824;&#23558;&#26435;&#37325;&#21644;&#27880;&#24847;&#21147;&#32531;&#23384;&#21387;&#32553;&#20026;4&#20010;&#20301;&#65292;&#20960;&#20046;&#27809;&#26377;&#31934;&#24230;&#25439;&#22833;&#12290;&#36825;&#20123;&#25216;&#26415;&#20351;FlexGen&#20855;&#26377;&#26356;&#22823;&#30340;&#25209;&#37327;&#36873;&#25321;&#31354;&#38388;&#65292;&#20174;&#32780;&#26174;&#33879;&#22686;&#21152;&#20102;&#26368;&#22823;&#21534;&#21520;&#37327;&#12290;&#32467;&#26524;&#65292;&#24403;&#22312;&#21333;&#20010;16GB GPU&#19978;&#36816;&#34892;OPT-175B&#26102;&#65292;FlexGen&#30340;&#21534;&#21520;&#37327;&#20026;&#27599;&#31186;90&#20010;&#24207;&#21015;&#65292;&#27604;Megatron-LM&#24555;4.5&#20493;&#65292;&#27604;&#20351;&#29992;&#21333;&#20010;&#26222;&#36890;GPU&#30340;&#31454;&#20105;&#32773;&#24555;7.5-19&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
The high computational and memory requirements of large language model (LLM) inference make it feasible only with multiple high-end accelerators. Motivated by the emerging demand for latency-insensitive tasks with batched processing, this paper initiates the study of high-throughput LLM inference using limited resources, such as a single commodity GPU. We present FlexGen, a high-throughput generation engine for running LLMs with limited GPU memory. FlexGen can be flexibly configured under various hardware resource constraints by aggregating memory and computation from the GPU, CPU, and disk. By solving a linear programming problem, it searches for efficient patterns to store and access tensors. FlexGen further compresses the weights and the attention cache to 4 bits with negligible accuracy loss. These techniques enable FlexGen to have a larger space of batch size choices and thus significantly increase maximum throughput. As a result, when running OPT-175B on a single 16GB GPU, FlexGe
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#21644;&#33945;&#29305;&#21345;&#32599;&#26641;&#25628;&#32034;&#30340;&#31526;&#21495;&#22238;&#24402;&#35268;&#21010;&#31574;&#30053;TPSR&#65292;&#21487;&#20197;&#23558;&#38750;&#21487;&#24494;&#30340;&#21453;&#39304;&#20316;&#20026;&#30693;&#35782;&#30340;&#22806;&#37096;&#26469;&#28304;&#34701;&#20837;&#21040;&#26041;&#31243;&#29983;&#25104;&#36807;&#31243;&#20013;&#65292;&#25552;&#39640;&#26041;&#31243;&#29983;&#25104;&#30340;&#20934;&#30830;&#24615;&#21644;&#22797;&#26434;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.06833</link><description>&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#31526;&#21495;&#22238;&#24402;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Transformer-based Planning for Symbolic Regression. (arXiv:2303.06833v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06833
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#21644;&#33945;&#29305;&#21345;&#32599;&#26641;&#25628;&#32034;&#30340;&#31526;&#21495;&#22238;&#24402;&#35268;&#21010;&#31574;&#30053;TPSR&#65292;&#21487;&#20197;&#23558;&#38750;&#21487;&#24494;&#30340;&#21453;&#39304;&#20316;&#20026;&#30693;&#35782;&#30340;&#22806;&#37096;&#26469;&#28304;&#34701;&#20837;&#21040;&#26041;&#31243;&#29983;&#25104;&#36807;&#31243;&#20013;&#65292;&#25552;&#39640;&#26041;&#31243;&#29983;&#25104;&#30340;&#20934;&#30830;&#24615;&#21644;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31526;&#21495;&#22238;&#24402;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#23427;&#28041;&#21450;&#22522;&#20110;&#20989;&#25968;&#20540;&#26597;&#25214;&#20854;&#25968;&#23398;&#34920;&#36798;&#24335;&#12290;&#26368;&#36817;&#65292;&#31526;&#21495;&#22238;&#24402;&#30340;&#19968;&#20123;&#36827;&#23637;&#34920;&#26126;&#65292;&#39044;&#35757;&#32451;&#30340;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#23545;&#20110;&#29983;&#25104;&#26041;&#31243;&#24207;&#21015;&#26159;&#26377;&#25928;&#30340;&#65292;&#36825;&#20123;&#27169;&#22411;&#20174;&#21512;&#25104;&#25968;&#25454;&#38598;&#30340;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#20013;&#33719;&#30410;&#65292;&#24182;&#22312;&#25512;&#29702;&#26102;&#38388;&#26041;&#38754;&#27604;&#22522;&#20110;GP&#30340;&#26041;&#27861;&#20855;&#26377;&#26174;&#33879;&#20248;&#21183;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#20851;&#27880;&#30340;&#26159;&#20511;&#37492;&#25991;&#26412;&#29983;&#25104;&#30340;&#30417;&#30563;&#39044;&#35757;&#32451;&#30446;&#26631;&#65292;&#32780;&#24573;&#30053;&#20102;&#26041;&#31243;&#30340;&#29305;&#23450;&#30446;&#26631;&#65292;&#22914;&#20934;&#30830;&#24615;&#21644;&#22797;&#26434;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TPSR&#65292;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#31526;&#21495;&#22238;&#24402;&#35268;&#21010;&#31574;&#30053;&#65292;&#23558;&#33945;&#29305;&#21345;&#32599;&#26641;&#25628;&#32034;&#34701;&#20837;&#21040;Transformer&#35299;&#30721;&#36807;&#31243;&#20013;&#12290;&#19982;&#20256;&#32479;&#30340;&#35299;&#30721;&#31574;&#30053;&#19981;&#21516;&#65292;TPSR&#20801;&#35768;&#23558;&#38750;&#21487;&#24494;&#30340;&#21453;&#39304;&#65288;&#22914;&#25311;&#21512;&#20934;&#30830;&#24615;&#21644;&#22797;&#26434;&#24615;&#65289;&#20316;&#20026;&#30693;&#35782;&#30340;&#22806;&#37096;&#26469;&#28304;&#34701;&#20837;&#21040;&#26041;&#31243;&#29983;&#25104;&#36807;&#31243;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Symbolic regression (SR) is a challenging task in machine learning that involves finding a mathematical expression for a function based on its values. Recent advancements in SR have demonstrated the efficacy of pretrained transformer-based models for generating equations as sequences, which benefit from large-scale pretraining on synthetic datasets and offer considerable advantages over GP-based methods in terms of inference time. However, these models focus on supervised pretraining goals borrowed from text generation and ignore equation-specific objectives like accuracy and complexity. To address this, we propose TPSR, a Transformer-based Planning strategy for Symbolic Regression that incorporates Monte Carlo Tree Search into the transformer decoding process. TPSR, as opposed to conventional decoding strategies, allows for the integration of non-differentiable feedback, such as fitting accuracy and complexity, as external sources of knowledge into the equation generation process. Ext
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#30340;&#36335;&#20391;&#22810;&#31867;&#22411;&#22810;&#32452;&#20256;&#24863;&#22120;&#26816;&#27979;&#31995;&#32479;RMMDet&#65292;&#37319;&#29992;ROS&#34394;&#25311;&#29615;&#22659;&#27169;&#25311;&#30495;&#23454;&#36947;&#36335;&#26465;&#20214;&#65292;&#33021;&#22815;&#23454;&#29616;&#22810;&#31867;&#22411;&#20256;&#24863;&#22120;&#26816;&#27979;&#21644;&#22810;&#32452;&#20256;&#24863;&#22120;&#34701;&#21512;&#65292;&#22312;&#21508;&#31181;&#23454;&#39564;&#20013;&#24471;&#21040;&#37096;&#20998;&#25104;&#21151;&#12290;</title><link>http://arxiv.org/abs/2303.05203</link><description>&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#36335;&#20391;&#22810;&#31867;&#22411;&#22810;&#32452;&#20256;&#24863;&#22120;&#26816;&#27979;&#31995;&#32479;RMMDet(&#26356;&#26032;&#29256;)
&lt;/p&gt;
&lt;p&gt;
RMMDet: Road-Side Multitype and Multigroup Sensor Detection System for Autonomous Driving. (arXiv:2303.05203v3 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.05203
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#30340;&#36335;&#20391;&#22810;&#31867;&#22411;&#22810;&#32452;&#20256;&#24863;&#22120;&#26816;&#27979;&#31995;&#32479;RMMDet&#65292;&#37319;&#29992;ROS&#34394;&#25311;&#29615;&#22659;&#27169;&#25311;&#30495;&#23454;&#36947;&#36335;&#26465;&#20214;&#65292;&#33021;&#22815;&#23454;&#29616;&#22810;&#31867;&#22411;&#20256;&#24863;&#22120;&#26816;&#27979;&#21644;&#22810;&#32452;&#20256;&#24863;&#22120;&#34701;&#21512;&#65292;&#22312;&#21508;&#31181;&#23454;&#39564;&#20013;&#24471;&#21040;&#37096;&#20998;&#25104;&#21151;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#25216;&#26415;&#22312;&#20154;&#24037;&#26234;&#33021;&#30340;&#24110;&#21161;&#19979;&#21462;&#24471;&#20102;&#38271;&#36275;&#30340;&#36827;&#27493;&#65292;&#35768;&#22810;&#20808;&#36827;&#30340;&#26041;&#27861;&#24050;&#34987;&#25552;&#20986;&#29992;&#20110;&#36710;&#36742;&#31471;&#30446;&#26631;&#26816;&#27979;&#65292;&#21253;&#25324;&#21333;&#20256;&#24863;&#22120;&#25110;&#22810;&#20256;&#24863;&#22120;&#26816;&#27979;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#22797;&#26434;&#22810;&#21464;&#30340;&#23454;&#38469;&#20132;&#36890;&#24773;&#20917;&#38656;&#35201;&#25506;&#31350;&#22914;&#20309;&#23558;&#36825;&#20123;&#26041;&#27861;&#24212;&#29992;&#20110;&#23454;&#38469;&#36947;&#36335;&#26465;&#20214;&#20013;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;RMMDet&#65292;&#19968;&#31181;&#36866;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#30340;&#36335;&#20391;&#22810;&#31867;&#22411;&#22810;&#32452;&#20256;&#24863;&#22120;&#26816;&#27979;&#31995;&#32479;&#12290;&#25105;&#20204;&#22312;&#22522;&#20110;ROS&#30340;&#34394;&#25311;&#29615;&#22659;&#20013;&#27169;&#25311;&#30495;&#23454;&#19990;&#30028;&#26465;&#20214;&#65292;&#29305;&#21035;&#26159;&#20256;&#24863;&#22120;&#30340;&#29289;&#29702;&#21644;&#21151;&#33021;&#26500;&#36896;&#12290;&#28982;&#21518;&#65292;&#22312;&#27492;&#29615;&#22659;&#20013;&#23454;&#29616;&#20102;&#22810;&#31867;&#22411;&#20256;&#24863;&#22120;&#26816;&#27979;&#21644;&#22810;&#32452;&#20256;&#24863;&#22120;&#34701;&#21512;&#65292;&#21253;&#25324;&#22522;&#20110;&#32467;&#26524;&#32423;&#34701;&#21512;&#30340;&#25668;&#20687;&#22836;-&#38647;&#36798;&#21644;&#25668;&#20687;&#22836;-Lidar&#26816;&#27979;&#12290;&#25105;&#20204;&#36824;&#21046;&#20316;&#20102;&#26412;&#22320;&#25968;&#25454;&#38598;&#21644;&#30495;&#23454;&#27801;&#30424;&#22330;&#22320;&#65292;&#24182;&#36827;&#34892;&#20102;&#21508;&#31181;&#23454;&#39564;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23558;&#22810;&#26234;&#33021;&#20307;&#21327;&#21516;&#35843;&#24230;&#31995;&#32479;&#19982;&#34701;&#21512;&#26816;&#27979;&#31995;&#32479;&#30456;&#38142;&#25509;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autonomous driving has now made great strides thanks to artificial intelligence, and numerous advanced methods have been proposed for vehicle end target detection, including single sensor or multi sensor detection methods. However, the complexity and diversity of real traffic situations necessitate an examination of how to use these methods in real road conditions. In this paper, we propose RMMDet, a road-side multitype and multigroup sensor detection system for autonomous driving. We use a ROS-based virtual environment to simulate real-world conditions, in particular the physical and functional construction of the sensors. Then we implement muti-type sensor detection and multi-group sensors fusion in this environment, including camera-radar and camera-lidar detection based on result-level fusion. We produce local datasets and real sand table field, and conduct various experiments. Furthermore, we link a multi-agent collaborative scheduling system to the fusion detection system. Hence,
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#8220;Wasserstein Belief Updater&#8221;&#31639;&#27861;&#26469;&#23398;&#20064;&#37096;&#20998;&#21487;&#35266;&#27979;&#29615;&#22659;&#19979;&#30340;&#20449;&#24565;&#26356;&#26032;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#23398;&#20064;POMDP&#8203;&#8203;&#30340;&#28508;&#22312;&#27169;&#22411;&#21644;&#32622;&#20449;&#26356;&#26032;&#30340;&#36817;&#20284;&#20540;&#65292;&#23454;&#29616;&#20102;&#23545;&#21382;&#21490;&#35266;&#23519;&#21644;&#34892;&#21160;&#30340;&#26377;&#25928;&#21387;&#32553;&#65292;&#25552;&#21319;&#20102;&#31639;&#27861;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2303.03284</link><description>&lt;p&gt;
"Wasserstein Believer:&#36890;&#36807;&#21487;&#38752;&#30340;&#28508;&#22312;&#31354;&#38388;&#27169;&#22411;&#23398;&#20064;&#37096;&#20998;&#21487;&#35266;&#27979;&#29615;&#22659;&#19979;&#30340;&#20449;&#24565;&#26356;&#26032;"
&lt;/p&gt;
&lt;p&gt;
The Wasserstein Believer: Learning Belief Updates for Partially Observable Environments through Reliable Latent Space Models. (arXiv:2303.03284v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.03284
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#8220;Wasserstein Belief Updater&#8221;&#31639;&#27861;&#26469;&#23398;&#20064;&#37096;&#20998;&#21487;&#35266;&#27979;&#29615;&#22659;&#19979;&#30340;&#20449;&#24565;&#26356;&#26032;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#23398;&#20064;POMDP&#8203;&#8203;&#30340;&#28508;&#22312;&#27169;&#22411;&#21644;&#32622;&#20449;&#26356;&#26032;&#30340;&#36817;&#20284;&#20540;&#65292;&#23454;&#29616;&#20102;&#23545;&#21382;&#21490;&#35266;&#23519;&#21644;&#34892;&#21160;&#30340;&#26377;&#25928;&#21387;&#32553;&#65292;&#25552;&#21319;&#20102;&#31639;&#27861;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37096;&#20998;&#21487;&#35266;&#27979;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;POMDP&#65289;&#26159;&#24314;&#27169;&#20195;&#29702;&#26080;&#27861;&#24863;&#30693;&#21040;&#23436;&#25972;&#29366;&#24577;&#30340;&#29615;&#22659;&#30340;&#26377;&#29992;&#24037;&#20855;&#12290;&#22240;&#27492;&#65292;&#20195;&#29702;&#38656;&#35201;&#32771;&#34385;&#36807;&#21435;&#30340;&#35266;&#23519;&#21644;&#34892;&#21160;&#36827;&#34892;&#25512;&#29702;&#12290;&#20294;&#26159;&#65292;&#30001;&#20110;&#21382;&#21490;&#31354;&#38388;&#25351;&#25968;&#32423;&#22686;&#38271;&#65292;&#20165;&#20165;&#35760;&#20303;&#23436;&#25972;&#21382;&#21490;&#36890;&#24120;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#20445;&#25345;&#27169;&#25311;&#30495;&#23454;&#29366;&#24577;&#30340;&#32622;&#20449;&#27010;&#29575;&#20998;&#24067;&#21487;&#20197;&#20316;&#20026;&#21382;&#21490;&#30340;&#20805;&#20998;&#32479;&#35745;&#37327;&#65292;&#20294;&#20854;&#35745;&#31639;&#38656;&#35201;&#35775;&#38382;&#29615;&#22659;&#30340;&#27169;&#22411;&#65292;&#22240;&#27492;&#20063;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#26368;&#20808;&#36827;&#30340;&#31639;&#27861;&#20351;&#29992;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#26469;&#21387;&#32553;&#35266;&#23519;-&#34892;&#21160;&#21382;&#21490;&#20197;&#23398;&#20064;&#20805;&#20998;&#30340;&#32479;&#35745;&#37327;&#65292;&#20294;&#23427;&#20204;&#32570;&#20047;&#25104;&#21151;&#30340;&#20445;&#35777;&#24182;&#21487;&#33021;&#23548;&#33268;&#27425;&#20248;&#31574;&#30053;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Wasserstein Belief Updater &#65292;&#36825;&#26159;&#19968;&#31181;RL&#31639;&#27861;&#65292;&#23427;&#23398;&#20064;POMDP&#8203;&#8203;&#30340;&#28508;&#22312;&#27169;&#22411;&#21644;&#32622;&#20449;&#26356;&#26032;&#30340;&#36817;&#20284;&#20540;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#29702;&#35770;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Partially Observable Markov Decision Processes (POMDPs) are useful tools to model environments where the full state cannot be perceived by an agent. As such the agent needs to reason taking into account the past observations and actions. However, simply remembering the full history is generally intractable due to the exponential growth in the history space. Keeping a probability distribution that models the belief over what the true state is can be used as a sufficient statistic of the history, but its computation requires access to the model of the environment and is also intractable. State-of-the-art algorithms use Recurrent Neural Networks to compress the observation-action history aiming to learn a sufficient statistic, but they lack guarantees of success and can lead to sub-optimal policies. To overcome this, we propose the Wasserstein Belief Updater, an RL algorithm that learns a latent model of the POMDP and an approximation of the belief update. Our approach comes with theoreti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20013;&#25552;&#20986;&#20102;DART&#31574;&#30053;&#65292;&#20854;&#20013;&#21033;&#29992;&#22810;&#26679;&#21270;&#30340;&#22686;&#24378;&#26041;&#27861;&#35757;&#32451;&#19981;&#21516;&#30340;&#27169;&#22411;&#65292;&#28982;&#21518;&#36890;&#36807;&#32858;&#21512;&#36825;&#20123;&#27169;&#22411;&#30340;&#26435;&#37325;&#26469;&#32467;&#21512;&#20854;&#19987;&#19994;&#30693;&#35782;&#65292;&#24182;&#37325;&#22797;&#32858;&#21512;&#27493;&#39588;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2302.14685</link><description>&lt;p&gt;
DART: &#22810;&#26679;&#21270;&#32858;&#21512;&#37325;&#22797;&#35757;&#32451;&#25913;&#36827;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
DART: Diversify-Aggregate-Repeat Training Improves Generalization of Neural Networks. (arXiv:2302.14685v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.14685
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20013;&#25552;&#20986;&#20102;DART&#31574;&#30053;&#65292;&#20854;&#20013;&#21033;&#29992;&#22810;&#26679;&#21270;&#30340;&#22686;&#24378;&#26041;&#27861;&#35757;&#32451;&#19981;&#21516;&#30340;&#27169;&#22411;&#65292;&#28982;&#21518;&#36890;&#36807;&#32858;&#21512;&#36825;&#20123;&#27169;&#22411;&#30340;&#26435;&#37325;&#26469;&#32467;&#21512;&#20854;&#19987;&#19994;&#30693;&#35782;&#65292;&#24182;&#37325;&#22797;&#32858;&#21512;&#27493;&#39588;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#33021;&#21147;&#23545;&#20110;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#23433;&#20840;&#37096;&#32626;&#23427;&#20204;&#33267;&#20851;&#37325;&#35201;&#12290;&#25913;&#36827;&#27867;&#21270;&#30340;&#24120;&#35265;&#35757;&#32451;&#31574;&#30053;&#21253;&#25324;&#20351;&#29992;&#25968;&#25454;&#22686;&#24378;&#12289;&#38598;&#25104;&#21644;&#27169;&#22411;&#24179;&#22343;&#21270;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#24314;&#31435;&#20102;&#19968;&#20010;&#24778;&#20154;&#31616;&#21333;&#20294;&#24378;&#26377;&#21147;&#30340;&#27867;&#21270;&#22522;&#20934;&#65292;&#23427;&#21033;&#29992;&#20102;&#35757;&#32451;&#23567;&#25209;&#37327;&#20013;&#30340;&#22810;&#31181;&#19981;&#21516;&#22686;&#24378;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#36825;&#21487;&#20197;&#23398;&#20064;&#21040;&#19968;&#20010;&#26356;&#24179;&#34913;&#30340;&#29305;&#24449;&#20998;&#24067;&#12290;&#36827;&#19968;&#27493;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Diversify-Aggregate-Repeat Training (DART)&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#39318;&#20808;&#20351;&#29992;&#19981;&#21516;&#30340;&#22686;&#24378;&#26041;&#27861;(&#25110;&#39046;&#22495;)&#35757;&#32451;&#19981;&#21516;&#30340;&#27169;&#22411;&#65292;&#20197;&#25506;&#32034;&#25439;&#22833;&#30406;&#22320;&#65292;&#24182;&#36827;&#19968;&#27493;&#32858;&#21512;&#23427;&#20204;&#30340;&#26435;&#37325;&#65292;&#32467;&#21512;&#23427;&#20204;&#30340;&#19987;&#19994;&#30693;&#35782;&#24182;&#33719;&#24471;&#25913;&#36827;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#25972;&#20010;&#35757;&#32451;&#36807;&#31243;&#20013;&#37325;&#22797;&#32858;&#21512;&#27493;&#39588;&#21487;&#20197;&#25552;&#39640;&#25972;&#20307;&#20248;&#21270;&#36712;&#36857;&#65292;&#24182;&#30830;&#20445;&#21333;&#20010;&#27169;&#22411;&#20855;&#26377;&#36275;&#22815;&#20302;&#30340;&#25439;&#22833;&#38556;&#30861;&#65292;&#22312;&#23558;&#23427;&#20204;&#32452;&#21512;&#26102;&#21487;&#20197;&#33719;&#24471;&#25913;&#36827;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#35299;&#37322;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#19968;&#31181;&#27491;&#21017;&#21270;&#24418;&#24335;&#65292;&#23427;&#36843;&#20351;&#27169;&#22411;&#25506;&#32034;&#25439;&#22833;&#26223;&#35266;&#30340;&#22810;&#31181;&#27169;&#24335;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;DART&#22312;CIFAR-10&#21644;CIFAR-100&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;ImageNet&#21644;&#40065;&#26834;&#24615;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26377;&#31454;&#20105;&#21147;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generalization of neural networks is crucial for deploying them safely in the real world. Common training strategies to improve generalization involve the use of data augmentations, ensembling and model averaging. In this work, we first establish a surprisingly simple but strong benchmark for generalization which utilizes diverse augmentations within a training minibatch, and show that this can learn a more balanced distribution of features. Further, we propose Diversify-Aggregate-Repeat Training (DART) strategy that first trains diverse models using different augmentations (or domains) to explore the loss basin, and further Aggregates their weights to combine their expertise and obtain improved generalization. We find that Repeating the step of Aggregation throughout training improves the overall optimization trajectory and also ensures that the individual models have a sufficiently low loss barrier to obtain improved generalization on combining them. We shed light on our approach by 
&lt;/p&gt;</description></item><item><title>CrossSpeech&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#21892;&#36328;&#35821;&#38899;TTS&#36136;&#37327;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#35821;&#38899;&#34920;&#31034;&#20998;&#35299;&#20026;&#35821;&#38899;&#26080;&#20851;&#29983;&#25104;&#22120;&#21644;&#35828;&#35805;&#20154;&#30456;&#20851;&#30340;&#29983;&#25104;&#22120;&#24182;&#20998;&#21035;&#22788;&#29702;&#65292;&#23454;&#29616;&#20102;&#20998;&#31163;&#30340;&#35828;&#35805;&#20154;&#21644;&#35821;&#35328;&#34920;&#31034;&#65292;&#21516;&#26102;&#22312;Blizzard Challenge 2019&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2302.14370</link><description>&lt;p&gt;
&#36328;&#35821;&#38899;: &#38754;&#21521;&#36328;&#35821;&#38899;&#21512;&#25104;&#30340;&#35821;&#38899;&#26080;&#20851;&#29305;&#24449;&#34920;&#31034;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
CrossSpeech: Speaker-independent Acoustic Representation for Cross-lingual Speech Synthesis. (arXiv:2302.14370v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.14370
&lt;/p&gt;
&lt;p&gt;
CrossSpeech&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#21892;&#36328;&#35821;&#38899;TTS&#36136;&#37327;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#35821;&#38899;&#34920;&#31034;&#20998;&#35299;&#20026;&#35821;&#38899;&#26080;&#20851;&#29983;&#25104;&#22120;&#21644;&#35828;&#35805;&#20154;&#30456;&#20851;&#30340;&#29983;&#25104;&#22120;&#24182;&#20998;&#21035;&#22788;&#29702;&#65292;&#23454;&#29616;&#20102;&#20998;&#31163;&#30340;&#35828;&#35805;&#20154;&#21644;&#35821;&#35328;&#34920;&#31034;&#65292;&#21516;&#26102;&#22312;Blizzard Challenge 2019&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#26368;&#36817;&#30340;&#25991;&#26412;&#21040;&#35821;&#38899;&#21512;&#25104;&#31995;&#32479;&#24050;&#32463;&#22312;&#23454;&#29616;&#20154;&#31867;&#27700;&#24179;&#36136;&#37327;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#36328;&#35821;&#38899;&#21512;&#25104;&#30340;&#24615;&#33021;&#20173;&#28982;&#33853;&#21518;&#20110;&#21516;&#31181;&#35821;&#35328;&#30340;&#35821;&#38899;&#21512;&#25104;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CrossSpeech&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#22768;&#23398;&#29305;&#24449;&#31354;&#38388;&#30340;&#32423;&#21035;&#19978;&#26377;&#25928;&#22320;&#20998;&#31163;&#35828;&#35805;&#20154;&#21644;&#35821;&#35328;&#20449;&#24687;&#65292;&#25913;&#21892;&#36328;&#35821;&#38899;&#35821;&#38899;&#30340;&#36136;&#37327;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;CrossSpeech&#23558;&#35821;&#38899;&#29983;&#25104;&#27969;&#31243;&#20998;&#35299;&#20026;&#35821;&#38899;&#26080;&#20851;&#29983;&#25104;&#22120;&#65288;SIG&#65289;&#21644;&#35828;&#35805;&#20154;&#30456;&#20851;&#30340;&#29983;&#25104;&#22120;&#65288;SDG&#65289;&#12290; SIG&#29983;&#25104;&#19982;&#29305;&#23450;&#35828;&#35805;&#20154;&#20998;&#24067;&#26080;&#20851;&#30340;&#35821;&#38899;&#26080;&#20851;&#22768;&#23398;&#34920;&#31034;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;SDG&#27169;&#22411;&#35828;&#35805;&#20154;&#30456;&#20851;&#30340;&#35821;&#38899;&#21464;&#21270;&#65292;&#34920;&#24449;&#35828;&#35805;&#20154;&#23646;&#24615;&#12290;&#36890;&#36807;&#23558;&#27599;&#20010;&#20449;&#24687;&#20998;&#24320;&#22788;&#29702;&#65292;CrossSpeech&#21487;&#20197;&#33719;&#24471;&#20998;&#31163;&#30340;&#35828;&#35805;&#20154;&#21644;&#35821;&#35328;&#34920;&#31034;&#12290;&#20174;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;CrossSpeech&#22312;&#33258;&#28982;&#24230;&#21644;&#35828;&#35805;&#20154;&#30456;&#20284;&#24615;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#36328;&#35821;&#38899;TTS&#27169;&#22411;&#65292;&#24182;&#22312;Blizzard Challenge 2019&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
While recent text-to-speech (TTS) systems have made remarkable strides toward human-level quality, the performance of cross-lingual TTS lags behind that of intra-lingual TTS. This gap is mainly rooted from the speaker-language entanglement problem in cross-lingual TTS. In this paper, we propose CrossSpeech which improves the quality of cross-lingual speech by effectively disentangling speaker and language information in the level of acoustic feature space. Specifically, CrossSpeech decomposes the speech generation pipeline into the speaker-independent generator (SIG) and speaker-dependent generator (SDG). The SIG produces the speaker-independent acoustic representation which is not biased to specific speaker distributions. On the other hand, the SDG models speaker-dependent speech variation that characterizes speaker attributes. By handling each information separately, CrossSpeech can obtain disentangled speaker and language representations. From the experiments, we verify that CrossSp
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#33258;&#20027;&#39550;&#39542;&#22330;&#26223;&#29983;&#25104;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#31163;&#32447;&#23454;&#38469;&#25968;&#25454;&#21644;&#22312;&#32447;&#27169;&#25311;&#25968;&#25454;&#65292;&#23558;&#21453;&#21521; L_1 &#27491;&#21017;&#21270;&#29992;&#20110;&#20851;&#38190;&#29305;&#24449;&#25552;&#21462;&#65292;&#24182;&#26377;&#25928;&#22320;&#26597;&#35810;&#26356;&#39640;&#32500;&#24230;&#25628;&#32034;&#31354;&#38388;&#20013;&#30340;&#22330;&#26223;&#65292;&#23454;&#39564;&#35777;&#26126;&#20102;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.13726</link><description>&lt;p&gt;
(Re)$^2$H2O: &#33258;&#20027;&#39550;&#39542;&#22330;&#26223;&#29983;&#25104;&#30340;&#21453;&#21521;&#27491;&#21017;&#21270;&#28151;&#21512;&#31163;&#32447;&#21644;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
(Re)$^2$H2O: Autonomous Driving Scenario Generation via Reversely Regularized Hybrid Offline-and-Online Reinforcement Learning. (arXiv:2302.13726v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.13726
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#33258;&#20027;&#39550;&#39542;&#22330;&#26223;&#29983;&#25104;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#31163;&#32447;&#23454;&#38469;&#25968;&#25454;&#21644;&#22312;&#32447;&#27169;&#25311;&#25968;&#25454;&#65292;&#23558;&#21453;&#21521; L_1 &#27491;&#21017;&#21270;&#29992;&#20110;&#20851;&#38190;&#29305;&#24449;&#25552;&#21462;&#65292;&#24182;&#26377;&#25928;&#22320;&#26597;&#35810;&#26356;&#39640;&#32500;&#24230;&#25628;&#32034;&#31354;&#38388;&#20013;&#30340;&#22330;&#26223;&#65292;&#23454;&#39564;&#35777;&#26126;&#20102;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20027;&#39550;&#39542;&#21450;&#20854;&#24191;&#27867;&#37319;&#29992;&#19968;&#30452;&#34987;&#23492;&#20104;&#21402;&#26395;&#12290;&#28982;&#32780;&#65292;&#22312;&#27809;&#26377;&#21487;&#20449;&#30340;&#20840;&#38754;&#27979;&#35797;&#31243;&#24207;&#30340;&#24773;&#20917;&#19979;&#65292;&#19981;&#20165;&#34892;&#19994;&#38590;&#20197;&#22823;&#35268;&#27169;&#29983;&#20135;&#33258;&#20027;&#39550;&#39542;&#36710;&#36742;(AV)&#65292;&#32780;&#19988;&#20844;&#20247;&#21644;&#20915;&#31574;&#32773;&#20063;&#27809;&#26377;&#35828;&#26381;&#25509;&#21463;&#21019;&#26032;&#12290;&#29983;&#25104;&#23545;AV&#20855;&#26377;&#37325;&#35201;&#25361;&#25112;&#30340;&#23433;&#20840;&#20851;&#38190;&#22330;&#26223;&#26159;&#27979;&#35797;&#30340;&#37325;&#35201;&#31532;&#19968;&#27493;&#12290;&#29616;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#21253;&#25324;&#33258;&#28982;&#20294;&#36807;&#20110;&#23433;&#20840;&#30340;&#39550;&#39542;&#34892;&#20026;&#65292;&#32780;&#27169;&#25311;&#21017;&#20801;&#35768;&#26080;&#38480;&#21046;&#22320;&#25506;&#32034;&#22810;&#26679;&#21270;&#21644;&#28608;&#36827;&#30340;&#20132;&#36890;&#22330;&#26223;&#12290;&#30456;&#21453;&#65292;&#27169;&#25311;&#20013;&#30340;&#26356;&#39640;&#32500;&#24230;&#25628;&#32034;&#31354;&#38388;&#20351;&#24471;&#27809;&#26377;&#23454;&#38469;&#25968;&#25454;&#20998;&#24067;&#20316;&#20026;&#38544;&#24335;&#32422;&#26463;&#30340;&#24773;&#20917;&#19979;&#25928;&#29575;&#20302;&#19979;&#12290;&#20026;&#20102;&#23558;&#20004;&#32773;&#30340;&#20248;&#28857;&#32467;&#21512;&#36215;&#26469;&#65292;&#20174;&#31163;&#32447;&#23454;&#38469;&#25968;&#25454;&#21644;&#22312;&#32447;&#27169;&#25311;&#25968;&#25454;&#21516;&#26102;&#23398;&#20064;&#29983;&#25104;&#22330;&#26223;&#20284;&#20046;&#26159;&#21487;&#34892;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#37327;&#36523;&#25171;&#36896;&#20102;&#19968;&#20010;&#31216;&#20026; (Re)$^2$H2O &#30340;&#21453;&#21521;&#27491;&#21017;&#21270;&#28151;&#21512;&#31163;&#32447;&#21644;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#26469;&#23454;&#29616;&#22330;&#26223;&#29983;&#25104;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#31163;&#32447;&#37096;&#20998;&#20351;&#29992;&#21453;&#21521; L_1 &#27491;&#21017;&#21270;&#26469;&#25552;&#21462;&#20851;&#38190;&#29305;&#24449;&#65292;&#24182;&#25351;&#23548;&#20256;&#36755;&#21040;&#22312;&#32447;&#23398;&#20064;&#20013;&#65292;&#20197;&#28385;&#36275;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#20998;&#24067;&#12290;&#22312;&#32447;&#37096;&#20998;&#21017;&#36890;&#36807;&#20174;&#31163;&#32447;&#25968;&#25454;&#23398;&#21040;&#30340;&#31574;&#30053;&#26356;&#26377;&#25928;&#22320;&#26597;&#35810;&#26356;&#39640;&#32500;&#24230;&#25628;&#32034;&#31354;&#38388;&#20013;&#30340;&#22330;&#26223;&#12290;&#22312; CARLA &#27169;&#25311;&#22120;&#21644;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autonomous driving and its widespread adoption have long held tremendous promise. Nevertheless, without a trustworthy and thorough testing procedure, not only does the industry struggle to mass-produce autonomous vehicles (AV), but neither the general public nor policymakers are convinced to accept the innovations. Generating safety-critical scenarios that present significant challenges to AV is an essential first step in testing. Real-world datasets include naturalistic but overly safe driving behaviors, whereas simulation would allow for unrestricted exploration of diverse and aggressive traffic scenarios. Conversely, higher-dimensional searching space in simulation disables efficient scenario generation without real-world data distribution as implicit constraints. In order to marry the benefits of both, it seems appealing to learn to generate scenarios from both offline real-world and online simulation data simultaneously. Therefore, we tailor a Reversely Regularized Hybrid Offline-
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26816;&#39564;&#20102; ChatGPT &#22312; 25 &#20010;&#19981;&#21516;&#30340; NLP &#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#65292;&#23427;&#26159;&#19968;&#20010;&#19975;&#33021;&#30340; AI &#27169;&#22411;&#65292;&#20294;&#26080;&#20851;&#32039;&#35201;&#30340;&#34920;&#29616;&#21487;&#33021;&#20250;&#23545;&#26576;&#20123;&#20219;&#21153;&#30340;&#34920;&#29616;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2302.10724</link><description>&lt;p&gt;
ChatGPT&#65306;&#24212;&#20184;&#21315;&#20107;&#30340;&#19975;&#33021;&#22411; AI&#65292;&#20294;&#26080;&#25152;&#19987;&#31934;
&lt;/p&gt;
&lt;p&gt;
ChatGPT: Jack of all trades, master of none. (arXiv:2302.10724v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.10724
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26816;&#39564;&#20102; ChatGPT &#22312; 25 &#20010;&#19981;&#21516;&#30340; NLP &#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#65292;&#23427;&#26159;&#19968;&#20010;&#19975;&#33021;&#30340; AI &#27169;&#22411;&#65292;&#20294;&#26080;&#20851;&#32039;&#35201;&#30340;&#34920;&#29616;&#21487;&#33021;&#20250;&#23545;&#26576;&#20123;&#20219;&#21153;&#30340;&#34920;&#29616;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
OpenAI &#25512;&#20986;&#20102;&#32842;&#22825;&#29983;&#25104;&#39044;&#35757;&#32451; Transformer&#65288;ChatGPT&#65289;&#65292;&#38761;&#26032;&#20102;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#20114;&#21160;&#30340;&#26041;&#27861;&#12290;&#35768;&#22810;&#30740;&#31350;&#36890;&#36807;&#27979;&#35797; ChatGPT &#22312;&#20247;&#25152;&#21608;&#30693;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#30340;&#25928;&#26524;&#65292;&#26469;&#35780;&#20272;&#35813;&#27169;&#22411;&#30340;&#25928;&#33021;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#30740;&#31350;&#22823;&#22810;&#38750;&#33258;&#21160;&#21270;&#65292;&#24182;&#19988;&#35268;&#27169;&#38750;&#24120;&#26377;&#38480;&#12290;&#26412;&#30740;&#31350;&#22312; 25 &#20010;&#19981;&#21516;&#30340; NLP &#20219;&#21153;&#19978;&#26816;&#39564;&#20102; ChatGPT &#30340;&#24615;&#33021;&#65292;&#20854;&#20013;&#22823;&#22810;&#25968;&#20219;&#21153;&#29978;&#33267;&#23545;&#20154;&#31867;&#32780;&#35328;&#37117;&#26159;&#20027;&#35266;&#30340;&#65292;&#20363;&#22914;&#24773;&#24863;&#20998;&#26512;&#12289;&#24773;&#32490;&#35782;&#21035;&#12289;&#25915;&#20987;&#24615;&#21644;&#31435;&#22330;&#26816;&#27979;&#12290;&#21478;&#19968;&#20123;&#20219;&#21153;&#21017;&#38656;&#35201;&#26356;&#23458;&#35266;&#30340;&#25512;&#29702;&#65292;&#22914;&#35789;&#20041;&#28040;&#27495;&#12289;&#35821;&#35328;&#21487;&#25509;&#21463;&#24615;&#21644;&#38382;&#31572;&#12290;&#25105;&#20204;&#36824;&#23545; GPT-4 &#27169;&#22411;&#22312;&#20116;&#20010;&#36873;&#23450;&#30340; NLP &#20219;&#21153;&#23376;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#25105;&#20204;&#33258;&#21160;&#21270;&#20102; ChatGPT &#21644; GPT-4 &#30340;&#24341;&#23548;&#36807;&#31243;&#65292;&#24182;&#20998;&#26512;&#20102;&#36229;&#36807; 49k &#20010;&#21709;&#24212;&#12290;&#19982;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#35299;&#20915;&#26041;&#26696;&#65288;SOTA&#65289;&#36827;&#34892;&#27604;&#36739;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#19968;&#20123;&#20219;&#21153;&#19978; ChatGPT &#30340;&#24615;&#33021;&#23384;&#22312;&#19968;&#23450;&#30340;&#32570;&#38519;&#12290;
&lt;/p&gt;
&lt;p&gt;
OpenAI has released the Chat Generative Pre-trained Transformer (ChatGPT) and revolutionized the approach in artificial intelligence to human-model interaction. Several publications on ChatGPT evaluation test its effectiveness on well-known natural language processing (NLP) tasks. However, the existing studies are mostly non-automated and tested on a very limited scale. In this work, we examined ChatGPT's capabilities on 25 diverse analytical NLP tasks, most of them subjective even to humans, such as sentiment analysis, emotion recognition, offensiveness, and stance detection. In contrast, the other tasks require more objective reasoning like word sense disambiguation, linguistic acceptability, and question answering. We also evaluated GPT-4 model on five selected subsets of NLP tasks. We automated ChatGPT and GPT-4 prompting process and analyzed more than 49k responses. Our comparison of its results with available State-of-the-Art (SOTA) solutions showed that the average loss in quali
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#26469;&#25913;&#36827;&#20154;&#21592;&#20877;&#35782;&#21035;&#27169;&#22411;&#24615;&#33021;&#30340;&#26368;&#26032;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2302.09119</link><description>&lt;p&gt;
&#19968;&#39033;&#20851;&#20110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#22312;&#20154;&#21592;&#20877;&#35782;&#21035;&#31995;&#32479;&#25968;&#25454;&#22686;&#24378;&#20013;&#30340;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Review on Generative Adversarial Networks for Data Augmentation in Person Re-Identification Systems. (arXiv:2302.09119v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.09119
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#26469;&#25913;&#36827;&#20154;&#21592;&#20877;&#35782;&#21035;&#27169;&#22411;&#24615;&#33021;&#30340;&#26368;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#33258;&#21160;&#20154;&#21592;&#20877;&#35782;&#21035;&#31995;&#32479;&#30340;&#20852;&#36259;&#26174;&#30528;&#22686;&#21152;&#65292;&#20027;&#35201;&#26159;&#20026;&#20102;&#24320;&#21457;&#30417;&#25511;&#21644;&#26234;&#33021;&#24215;&#38138;&#36719;&#20214;&#12290;&#30001;&#20110;&#20154;&#21592;&#23039;&#21183;&#30340;&#21464;&#21270;&#12289;&#19981;&#21516;&#30340;&#29031;&#26126;&#26465;&#20214;&#21644;&#36974;&#25377;&#24773;&#20917;&#65292;&#20197;&#21450;&#19981;&#21516;&#25668;&#20687;&#22836;&#33719;&#24471;&#30340;&#22270;&#20687;&#36136;&#37327;&#36739;&#24046;&#65292;&#36825;&#26159;&#30446;&#21069;&#23578;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#22312;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#24212;&#29992;&#20013;&#65292;&#36890;&#36807;&#25193;&#23637;&#27169;&#22411;&#35757;&#32451;&#25152;&#38656;&#30340;&#22270;&#20687;&#25110;&#35270;&#39057;&#38598;&#21512;&#26469;&#25913;&#36827;&#20877;&#35782;&#21035;&#31995;&#32479;&#30340;&#24615;&#33021;&#26159;&#19968;&#31181;&#21487;&#33021;&#24615;&#12290;&#30446;&#21069;&#65292;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#26159;&#29983;&#25104;&#21512;&#25104;&#20449;&#24687;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#30340;&#26368;&#24378;&#22823;&#26041;&#24335;&#20043;&#19968;&#65292;&#26080;&#35770;&#26159;&#35270;&#39057;&#12289;&#22270;&#20687;&#36824;&#26159;&#25991;&#26412;&#12290;&#26412;&#25991;&#32508;&#36848;&#20102;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#26469;&#25913;&#36827;&#20154;&#21592;&#20877;&#35782;&#21035;&#27169;&#22411;&#24615;&#33021;&#30340;&#26368;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#20851;&#27880;&#19977;&#20010;&#20998;&#31867;&#65306;(1) &#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;(2) &#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30340;&#26102;&#31354;&#25968;&#25454;&#25193;&#23637;&#26041;&#27861;&#65292;(3) &#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30340;&#36328;&#25668;&#20687;&#22836;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Interest in automatic people re-identification systems has significantly grown in recent years, mainly for developing surveillance and smart shops software. Due to the variability in person posture, different lighting conditions, and occluded scenarios, together with the poor quality of the images obtained by different cameras, it is currently an unsolved problem. In machine learning-based computer vision applications with reduced data sets, one possibility to improve the performance of re-identification system is through the augmentation of the set of images or videos available for training the neural models. Currently, one of the most robust ways to generate synthetic information for data augmentation, whether it is video, images or text, are the generative adversarial networks. This article reviews the most relevant recent approaches to improve the performance of person re-identification models through data augmentation, using generative adversarial networks. We focus on three categ
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25351;&#20986;&#27010;&#29575;&#30005;&#36335;&#65288;PC&#65289;&#23545;&#36229;&#20986;&#20998;&#24067;&#65288;OOD&#65289;&#25968;&#25454;&#19981;&#20855;&#22791;&#40065;&#26834;&#24615;&#65307;&#36890;&#36807;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21487;&#22788;&#29702;&#30340;&#38543;&#26426;&#22833;&#27963;&#25512;&#26029;&#65288;TDI&#65289;&#26469;&#20811;&#26381;&#36825;&#19968;&#25361;&#25112;&#65292;&#24182;&#19988;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#22312;&#21333;&#20010;&#27491;&#21521;&#20256;&#36882;&#20013;&#25552;&#20379;&#21487;&#22788;&#29702;&#30340;&#26080;&#37319;&#26679;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65292;&#20174;&#32780;&#25913;&#21892;&#20102;PC&#23545;&#20998;&#24067;&#28418;&#31227;&#21644;OOD&#25968;&#25454;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.06544</link><description>&lt;p&gt;
&#30693;&#36947;&#33258;&#24049;&#19981;&#30693;&#36947;&#30340;&#27010;&#29575;&#30005;&#36335;
&lt;/p&gt;
&lt;p&gt;
Probabilistic Circuits That Know What They Don't Know. (arXiv:2302.06544v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.06544
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25351;&#20986;&#27010;&#29575;&#30005;&#36335;&#65288;PC&#65289;&#23545;&#36229;&#20986;&#20998;&#24067;&#65288;OOD&#65289;&#25968;&#25454;&#19981;&#20855;&#22791;&#40065;&#26834;&#24615;&#65307;&#36890;&#36807;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21487;&#22788;&#29702;&#30340;&#38543;&#26426;&#22833;&#27963;&#25512;&#26029;&#65288;TDI&#65289;&#26469;&#20811;&#26381;&#36825;&#19968;&#25361;&#25112;&#65292;&#24182;&#19988;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#22312;&#21333;&#20010;&#27491;&#21521;&#20256;&#36882;&#20013;&#25552;&#20379;&#21487;&#22788;&#29702;&#30340;&#26080;&#37319;&#26679;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65292;&#20174;&#32780;&#25913;&#21892;&#20102;PC&#23545;&#20998;&#24067;&#28418;&#31227;&#21644;OOD&#25968;&#25454;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27010;&#29575;&#30005;&#36335;&#65288;PC&#65289;&#26159;&#19968;&#31181;&#20801;&#35768;&#20934;&#30830;&#21644;&#21487;&#22788;&#29702;&#30340;&#27010;&#29575;&#25512;&#26029;&#30340;&#27169;&#22411;&#12290;&#19982;&#31070;&#32463;&#32593;&#32476;&#30456;&#27604;&#65292;&#23427;&#20204;&#36890;&#24120;&#34987;&#35748;&#20026;&#26159;&#33391;&#22909;&#26657;&#20934;&#30340;&#65292;&#24182;&#19988;&#23545;&#20110;&#36229;&#20986;&#20998;&#24067;&#65288;OOD&#65289;&#25968;&#25454;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#26412;&#25991;&#34920;&#26126; PC &#23454;&#38469;&#19978;&#19981;&#20855;&#26377;&#23545;OOD&#25968;&#25454;&#30340;&#40065;&#26834;&#24615;&#65292;&#36827;&#32780;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26469;&#20811;&#26381;&#36825;&#19968;&#25361;&#25112;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21487;&#22788;&#29702;&#30340;&#38543;&#26426;&#22833;&#27963;&#25512;&#26029;&#65288;TDI&#65289;&#8212;&#8212;&#19968;&#31181;&#25512;&#26029;&#31243;&#24207;&#65292;&#36890;&#36807;&#26041;&#24046;&#20256;&#25773;&#23548;&#20986;&#33945;&#29305;&#21345;&#27931;&#22833;&#27963;&#65288;MCD&#65289;&#30340;&#35299;&#26512;&#35299;&#26469;&#20272;&#35745;&#19981;&#30830;&#23450;&#24615;&#12290;&#19982;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340; MCD &#19981;&#21516;&#65292;TDI&#19981;&#38656;&#35201;&#36827;&#34892;&#22810;&#27425;&#32593;&#32476;&#35780;&#20272;&#23601;&#21487;&#20197;&#25552;&#20379;&#21487;&#22788;&#29702;&#30340;&#26080;&#37319;&#26679;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#23454;&#39564;&#35780;&#20272;&#22312;&#30495;&#23454;&#25968;&#25454;&#19978;&#30340;&#20998;&#31867;&#32622;&#20449;&#24230;&#21644;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65292;TDI&#25913;&#21892;&#20102;PC&#23545;&#20998;&#24067;&#28418;&#31227;&#21644;OOD&#25968;&#25454;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Probabilistic circuits (PCs) are models that allow exact and tractable probabilistic inference. In contrast to neural networks, they are often assumed to be well-calibrated and robust to out-of-distribution (OOD) data. In this paper, we show that PCs are in fact not robust to OOD data, i.e., they don't know what they don't know. We then show how this challenge can be overcome by model uncertainty quantification. To this end, we propose tractable dropout inference (TDI), an inference procedure to estimate uncertainty by deriving an analytical solution to Monte Carlo dropout (MCD) through variance propagation. Unlike MCD in neural networks, which comes at the cost of multiple network evaluations, TDI provides tractable sampling-free uncertainty estimates in a single forward pass. TDI improves the robustness of PCs to distribution shift and OOD data, demonstrated through a series of experiments evaluating the classification confidence and uncertainty estimates on real-world data.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;&#20174;&#22810;&#22495;&#25968;&#25454;&#20013;&#29983;&#25104;&#30340;&#22495;&#32034;&#24341;&#65292;&#25552;&#20379;&#39069;&#22806;&#30340;&#27934;&#23519;&#35270;&#35282;&#65292;&#24182;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#26368;&#20339;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2302.02561</link><description>&lt;p&gt;
&#22495;&#32034;&#24341;&#21464;&#20998;&#36125;&#21494;&#26031;&#65306;&#21487;&#35299;&#37322;&#30340;&#22495;&#32034;&#24341;&#29992;&#20110;&#22495;&#33258;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Domain-Indexing Variational Bayes: Interpretable Domain Index for Domain Adaptation. (arXiv:2302.02561v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.02561
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;&#20174;&#22810;&#22495;&#25968;&#25454;&#20013;&#29983;&#25104;&#30340;&#22495;&#32034;&#24341;&#65292;&#25552;&#20379;&#39069;&#22806;&#30340;&#27934;&#23519;&#35270;&#35282;&#65292;&#24182;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#26368;&#20339;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#21033;&#29992;&#22495;&#32034;&#24341;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22495;&#33258;&#36866;&#24212;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#24182;&#38750;&#24635;&#26159;&#26377;&#36825;&#26679;&#30340;&#22495;&#32034;&#24341;&#21487;&#29992;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#39318;&#20808;&#20174;&#27010;&#29575;&#35282;&#24230;&#25552;&#20379;&#20102;&#22495;&#32034;&#24341;&#30340;&#27491;&#24335;&#23450;&#20041;&#65292;&#28982;&#21518;&#25552;&#20986;&#20102;&#19968;&#20010;&#23545;&#25239;&#24615;&#21464;&#20998;&#36125;&#21494;&#26031;&#26694;&#26550;&#65292;&#20174;&#22810;&#22495;&#25968;&#25454;&#20013;&#25512;&#26029;&#20986;&#22495;&#32034;&#24341;&#65292;&#20174;&#32780;&#25552;&#20379;&#39069;&#22806;&#30340;&#22495;&#20851;&#31995;&#27934;&#23519;&#65292;&#24182;&#25552;&#39640;&#22495;&#33258;&#36866;&#24212;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#23545;&#25239;&#24615;&#21464;&#20998;&#36125;&#21494;&#26031;&#26694;&#26550;&#22312;&#24179;&#34913;&#28857;&#22788;&#25214;&#21040;&#20102;&#26368;&#20248;&#30340;&#22495;&#32034;&#24341;&#12290;&#23545;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#30340;&#23454;&#35777;&#32467;&#26524;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#20135;&#29983;&#21487;&#35299;&#37322;&#30340;&#22495;&#32034;&#24341;&#65292;&#20351;&#25105;&#20204;&#21487;&#20197;&#23454;&#29616;&#20248;&#20110;&#29616;&#26377;&#22495;&#36866;&#24212;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;&#20195;&#30721;&#21487;&#22312;https://github.com/Wang-ML-Lab/VDI&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Previous studies have shown that leveraging domain index can significantly boost domain adaptation performance (arXiv:2007.01807, arXiv:2202.03628). However, such domain indices are not always available. To address this challenge, we first provide a formal definition of domain index from the probabilistic perspective, and then propose an adversarial variational Bayesian framework that infers domain indices from multi-domain data, thereby providing additional insight on domain relations and improving domain adaptation performance. Our theoretical analysis shows that our adversarial variational Bayesian framework finds the optimal domain index at equilibrium. Empirical results on both synthetic and real data verify that our model can produce interpretable domain indices which enable us to achieve superior performance compared to state-of-the-art domain adaptation methods. Code is available at https://github.com/Wang-ML-Lab/VDI.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20026;&#25345;&#32493;&#23398;&#20064;&#30340;&#20840;&#38754;&#32508;&#36848;&#65292;&#24635;&#32467;&#20102;&#25345;&#32493;&#23398;&#20064;&#30340;&#19968;&#33324;&#30446;&#26631;&#65292;&#22238;&#39038;&#20102;&#21508;&#31181;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#24378;&#35843;&#20102;&#19968;&#20123;&#26377;&#21069;&#36884;&#30340;&#24212;&#29992;&#39046;&#22495;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2302.00487</link><description>&lt;p&gt;
&#25345;&#32493;&#23398;&#20064;&#32508;&#36848;&#65306;&#29702;&#35770;&#12289;&#26041;&#27861;&#19982;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Survey of Continual Learning: Theory, Method and Application. (arXiv:2302.00487v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.00487
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20026;&#25345;&#32493;&#23398;&#20064;&#30340;&#20840;&#38754;&#32508;&#36848;&#65292;&#24635;&#32467;&#20102;&#25345;&#32493;&#23398;&#20064;&#30340;&#19968;&#33324;&#30446;&#26631;&#65292;&#22238;&#39038;&#20102;&#21508;&#31181;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#24378;&#35843;&#20102;&#19968;&#20123;&#26377;&#21069;&#36884;&#30340;&#24212;&#29992;&#39046;&#22495;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26234;&#33021;&#31995;&#32479;&#38656;&#35201;&#22312;&#20854;&#29983;&#21629;&#21608;&#26399;&#20869;&#19981;&#26029;&#33719;&#21462;&#12289;&#26356;&#26032;&#12289;&#31215;&#32047;&#21644;&#21033;&#29992;&#30693;&#35782;&#20197;&#24212;&#23545;&#29616;&#23454;&#19990;&#30028;&#30340;&#21160;&#24577;&#21464;&#21270;&#12290;&#36825;&#31181;&#33021;&#21147;&#31216;&#20026;&#25345;&#32493;&#23398;&#20064;&#65292;&#20026;AI&#31995;&#32479;&#33258;&#36866;&#24212;&#21457;&#23637;&#25552;&#20379;&#20102;&#22522;&#30784;&#12290;&#28982;&#32780;&#65292;&#25345;&#32493;&#23398;&#20064;&#30340;&#19968;&#20010;&#26174;&#33879;&#38480;&#21046;&#26159;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#21363;&#23398;&#20064;&#19968;&#20010;&#26032;&#20219;&#21153;&#36890;&#24120;&#20250;&#23548;&#33268;&#26087;&#20219;&#21153;&#30340;&#24615;&#33021;&#26174;&#33879;&#38477;&#20302;&#12290;&#36817;&#24180;&#26469;&#65292;&#19981;&#26029;&#28044;&#29616;&#30340;&#21508;&#31181;&#36827;&#23637;&#22823;&#22823;&#25193;&#23637;&#20102;&#25345;&#32493;&#23398;&#20064;&#30340;&#29702;&#35299;&#21644;&#24212;&#29992;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#25345;&#32493;&#23398;&#20064;&#32508;&#36848;&#65292;&#26088;&#22312;&#36830;&#25509;&#22522;&#26412;&#35774;&#32622;&#12289;&#29702;&#35770;&#22522;&#30784;&#12289;&#20195;&#34920;&#24615;&#26041;&#27861;&#21644;&#23454;&#38469;&#24212;&#29992;&#12290;&#25105;&#20204;&#24635;&#32467;&#20102;&#29616;&#26377;&#29702;&#35770;&#21644;&#23454;&#35777;&#32467;&#26524;&#65292;&#27010;&#25324;&#20102;&#25345;&#32493;&#23398;&#20064;&#30340;&#19968;&#33324;&#30446;&#26631;&#65306;&#20943;&#23569;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#23454;&#29616;&#39640;&#25928;&#30340;&#21644;&#32456;&#36523;&#30340;&#23398;&#20064;&#65292;&#20197;&#21450;&#23454;&#29616;&#26356;&#28145;&#23618;&#27425;&#30340;&#34920;&#24449;&#23398;&#20064;&#12290;&#25105;&#20204;&#36824;&#22238;&#39038;&#20102;&#21508;&#31181;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#21253;&#25324;&#22522;&#20110;&#27491;&#21017;&#21270;&#12289;&#22522;&#20110;&#22238;&#25918;&#21644;&#22522;&#20110;&#29983;&#25104;&#30340;&#26041;&#27861;&#65292;&#24182;&#35752;&#35770;&#20102;&#23427;&#20204;&#30340;&#20248;&#32570;&#28857;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;&#19968;&#20123;&#26377;&#21069;&#36884;&#30340;&#24212;&#29992;&#39046;&#22495;&#65292;&#22914;&#26426;&#22120;&#20154;&#12289;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#65292;&#24182;&#30830;&#23450;&#20102;&#19968;&#20123;&#24320;&#25918;&#25361;&#25112;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
To cope with real-world dynamics, an intelligent agent needs to incrementally acquire, update, accumulate, and exploit knowledge throughout its lifetime. This ability, known as continual learning, provides a foundation for AI systems to develop themselves adaptively. In a general sense, continual learning is explicitly limited by catastrophic forgetting, where learning a new task usually results in a dramatic performance degradation of the old tasks. Beyond this, increasingly numerous advances have emerged in recent years that largely extend the understanding and application of continual learning. The growing and widespread interest in this direction demonstrates its realistic significance as well as complexity. In this work, we present a comprehensive survey of continual learning, seeking to bridge the basic settings, theoretical foundations, representative methods, and practical applications. Based on existing theoretical and empirical results, we summarize the general objectives of 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20004;&#31181;&#26032;&#20844;&#24179;&#24230;&#37327;&#26041;&#27861;&#65306;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;&#26354;&#32447;&#20043;&#38388;&#21306;&#22495;&#65288;ABPC&#65289;&#21644;&#32047;&#31215;&#23494;&#24230;&#20989;&#25968;&#26354;&#32447;&#20043;&#38388;&#21306;&#22495;&#65288;ABCC&#65289;&#65292;&#23454;&#29616;&#20154;&#21475;&#32479;&#35745;&#29305;&#24449;&#30340;&#20844;&#24179;&#26426;&#22120;&#23398;&#20064;</title><link>http://arxiv.org/abs/2301.13443</link><description>&lt;p&gt;
&#26032;&#30340;&#20998;&#24067;&#27700;&#24179;&#24230;&#37327;&#26041;&#27861;&#65306;&#24323;&#29992;$\Delta$DP&#65292;&#23454;&#29616;&#20154;&#21475;&#32479;&#35745;&#29305;&#24449;&#30340;&#20844;&#24179;&#26426;&#22120;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Retiring $\Delta$DP: New Distribution-Level Metrics for Demographic Parity. (arXiv:2301.13443v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.13443
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20004;&#31181;&#26032;&#20844;&#24179;&#24230;&#37327;&#26041;&#27861;&#65306;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;&#26354;&#32447;&#20043;&#38388;&#21306;&#22495;&#65288;ABPC&#65289;&#21644;&#32047;&#31215;&#23494;&#24230;&#20989;&#25968;&#26354;&#32447;&#20043;&#38388;&#21306;&#22495;&#65288;ABCC&#65289;&#65292;&#23454;&#29616;&#20154;&#21475;&#32479;&#35745;&#29305;&#24449;&#30340;&#20844;&#24179;&#26426;&#22120;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#21475;&#32479;&#35745;&#29305;&#24449;&#30340;&#24179;&#31561;&#23545;&#24453;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#26368;&#24191;&#27867;&#35748;&#21487;&#30340;&#20844;&#24179;&#24230;&#37327;&#26631;&#20934;&#12290;&#20026;&#23454;&#29616;&#20154;&#21475;&#32479;&#35745;&#24179;&#31561;&#65292;&#35768;&#22810;&#30740;&#31350;&#33268;&#21147;&#20110;&#36861;&#27714;&#24120;&#29992;&#24230;&#37327;&#26041;&#27861;$\Delta DP$&#12290;&#28982;&#32780;&#65292;&#26412;&#25991;&#25581;&#31034;&#20102;&#20844;&#24179;&#24230;&#37327;&#26041;&#27861;$\Delta DP$&#23384;&#22312;&#22266;&#26377;&#32570;&#38519;&#65306;i) &#38646;&#20540;$\Delta DP$&#19981;&#20445;&#35777;&#27665;&#26063;&#32479;&#35745;&#24179;&#31561;&#30340;&#38646;&#36829;&#35268;&#65292;ii) $\Delta DP$&#20540;&#38543;&#19981;&#21516;&#20998;&#31867;&#38408;&#20540;&#21464;&#21270;&#12290;&#20026;&#27492;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#20844;&#24179;&#24230;&#37327;&#26041;&#27861;&#8212;&#8212;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;&#26354;&#32447;&#20043;&#38388;&#21306;&#22495;&#65288;ABPC&#65289;&#21644;&#32047;&#31215;&#23494;&#24230;&#20989;&#25968;&#26354;&#32447;&#20043;&#38388;&#21306;&#22495;&#65288;ABCC&#65289;&#65292;&#20197;&#31934;&#30830;&#27979;&#37327;&#19981;&#21516;&#27665;&#26063;&#32479;&#35745;&#32676;&#20307;&#39044;&#27979;&#27010;&#29575;&#20998;&#24067;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Demographic parity is the most widely recognized measure of group fairness in machine learning, which ensures equal treatment of different demographic groups. Numerous works aim to achieve demographic parity by pursuing the commonly used metric $\Delta DP$. Unfortunately, in this paper, we reveal that the fairness metric $\Delta DP$ can not precisely measure the violation of demographic parity, because it inherently has the following drawbacks: i) zero-value $\Delta DP$ does not guarantee zero violation of demographic parity, ii) $\Delta DP$ values can vary with different classification thresholds. To this end, we propose two new fairness metrics, Area Between Probability density function Curves (ABPC) and Area Between Cumulative density function Curves (ABCC), to precisely measure the violation of demographic parity at the distribution level. The new fairness metrics directly measure the difference between the distributions of the prediction probability for different demographic group
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20165;&#27169;&#25311;&#29615;&#22659;&#20013;&#30456;&#20851;&#26041;&#38754;&#30340;&#8220;&#26368;&#23567;&#20215;&#20540;&#31561;&#20215;&#37096;&#20998;&#27169;&#22411;&#8221;&#65292;&#24182;&#35777;&#26126;&#20102;&#36825;&#20123;&#27169;&#22411;&#29992;&#20110;&#35268;&#21010;&#22312;&#29983;&#28079;&#24378;&#21270;&#23398;&#20064;&#22330;&#26223;&#20013;&#20855;&#26377;&#21487;&#25193;&#23637;&#24615;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2301.10119</link><description>&lt;p&gt;
&#22522;&#20110;&#26368;&#23567;&#20215;&#20540;&#31561;&#20215;&#37096;&#20998;&#27169;&#22411;&#30340;&#29983;&#28079;&#24378;&#21270;&#23398;&#20064;&#30340;&#21487;&#25193;&#23637;&#21644;&#40065;&#26834;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Minimal Value-Equivalent Partial Models for Scalable and Robust Planning in Lifelong Reinforcement Learning. (arXiv:2301.10119v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.10119
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20165;&#27169;&#25311;&#29615;&#22659;&#20013;&#30456;&#20851;&#26041;&#38754;&#30340;&#8220;&#26368;&#23567;&#20215;&#20540;&#31561;&#20215;&#37096;&#20998;&#27169;&#22411;&#8221;&#65292;&#24182;&#35777;&#26126;&#20102;&#36825;&#20123;&#27169;&#22411;&#29992;&#20110;&#35268;&#21010;&#22312;&#29983;&#28079;&#24378;&#21270;&#23398;&#20064;&#22330;&#26223;&#20013;&#20855;&#26377;&#21487;&#25193;&#23637;&#24615;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#32431;&#20132;&#20114;&#20013;&#23398;&#20064;&#29615;&#22659;&#27169;&#22411;&#36890;&#24120;&#34987;&#35748;&#20026;&#26159;&#26500;&#24314;&#29983;&#28079;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#30340;&#33267;&#20851;&#32452;&#25104;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#22312;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#36890;&#24120;&#30340;&#20570;&#27861;&#26159;&#23398;&#20064;&#27169;&#22411;&#26469;&#23545;&#26234;&#33021;&#20307;&#30340;&#29615;&#22659;&#30340;&#27599;&#20010;&#26041;&#38754;&#36827;&#34892;&#24314;&#27169;&#65292;&#26080;&#35770;&#36825;&#20123;&#26041;&#38754;&#26159;&#21542;&#22312;&#25552;&#20986;&#26368;&#20248;&#20915;&#31574;&#26041;&#38754;&#37325;&#35201;&#12290;&#26412;&#25991;&#35748;&#20026;&#36825;&#31181;&#27169;&#22411;&#24182;&#19981;&#36866;&#21512;&#22312;&#29983;&#28079;&#24378;&#21270;&#23398;&#20064;&#22330;&#26223;&#20013;&#25191;&#34892;&#21487;&#25193;&#23637;&#21644;&#40065;&#26834;&#30340;&#35268;&#21010;&#65292;&#22240;&#27492;&#25552;&#20986;&#20102;&#21482;&#27169;&#25311;&#29615;&#22659;&#20013;&#30456;&#20851;&#26041;&#38754;&#30340;&#26032;&#22411;&#27169;&#22411;&#65292;&#31216;&#20026;&#8220;&#26368;&#23567;&#20215;&#20540;&#31561;&#20215;&#37096;&#20998;&#27169;&#22411;&#8221;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#36825;&#20123;&#27169;&#22411;&#30340;&#27491;&#24335;&#23450;&#20041;&#65292;&#24182;&#25552;&#20379;&#20102;&#29702;&#35770;&#32467;&#26524;&#26469;&#35777;&#26126;&#20351;&#29992;&#36825;&#20123;&#27169;&#22411;&#36827;&#34892;&#35268;&#21010;&#30340;&#21487;&#25193;&#23637;&#24615;&#20248;&#21183;&#65292;&#28982;&#21518;&#36827;&#34892;&#23454;&#39564;&#20197;&#20174;&#23454;&#35777;&#35282;&#24230;&#35828;&#26126;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#65292;&#26368;&#21518;&#25552;&#20986;&#19968;&#20123;&#26377;&#29992;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#26469;&#23398;&#20064;&#36825;&#20123;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning models of the environment from pure interaction is often considered an essential component of building lifelong reinforcement learning agents. However, the common practice in model-based reinforcement learning is to learn models that model every aspect of the agent's environment, regardless of whether they are important in coming up with optimal decisions or not. In this paper, we argue that such models are not particularly well-suited for performing scalable and robust planning in lifelong reinforcement learning scenarios and we propose new kinds of models that only model the relevant aspects of the environment, which we call "minimal value-equivalent partial models". After providing a formal definition for these models, we provide theoretical results demonstrating the scalability advantages of performing planning with such models and then perform experiments to empirically illustrate our theoretical results. Then, we provide some useful heuristics on how to learn these kinds
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#37038;&#20214;&#26426;&#21046;&#65288;TEM&#65289;&#65292;&#35299;&#20915;&#20102;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#65292;&#23427;&#37319;&#29992;&#26412;&#22320;&#36890;&#20449;&#21644;&#28040;&#24687;&#38142;&#36716;&#21457;&#30340;&#26041;&#24335;&#36827;&#34892;&#36890;&#20449;&#65292;&#32780;&#19981;&#38656;&#35201;&#27169;&#25311;&#25152;&#26377;&#26234;&#33021;&#20307;&#12290;</title><link>http://arxiv.org/abs/2301.01919</link><description>&lt;p&gt;
&#22522;&#20110;Transformer&#37038;&#20214;&#26426;&#21046;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#21487;&#25193;&#23637;&#27807;&#36890;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Scalable Communication for Multi-Agent Reinforcement Learning via Transformer-Based Email Mechanism. (arXiv:2301.01919v2 [cs.MA] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.01919
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#37038;&#20214;&#26426;&#21046;&#65288;TEM&#65289;&#65292;&#35299;&#20915;&#20102;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#65292;&#23427;&#37319;&#29992;&#26412;&#22320;&#36890;&#20449;&#21644;&#28040;&#24687;&#38142;&#36716;&#21457;&#30340;&#26041;&#24335;&#36827;&#34892;&#36890;&#20449;&#65292;&#32780;&#19981;&#38656;&#35201;&#27169;&#25311;&#25152;&#26377;&#26234;&#33021;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#36890;&#35759;&#21487;&#20197;&#26497;&#22823;&#22320;&#25913;&#21892;&#21512;&#20316;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#37096;&#20998;&#21487;&#35266;&#27979;&#30340;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#35201;&#20040;&#24191;&#25773;&#20449;&#24687;&#23548;&#33268;&#20449;&#24687;&#20887;&#20313;&#65292;&#35201;&#20040;&#36890;&#36807;&#23558;&#25152;&#26377;&#20854;&#20182;&#26234;&#33021;&#20307;&#27169;&#25311;&#25104;&#30446;&#26631;&#26469;&#23398;&#20064;&#26377;&#38024;&#23545;&#24615;&#30340;&#36890;&#20449;&#65292;&#20294;&#26159;&#24403;&#26234;&#33021;&#20307;&#25968;&#37327;&#21464;&#21270;&#26102;&#65292;&#36825;&#31181;&#26041;&#27861;&#26080;&#27861;&#25193;&#23637;&#12290;&#20026;&#20102;&#35299;&#20915;&#37096;&#20998;&#21487;&#35266;&#23519;&#20219;&#21153;&#30340;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;Transformer-based Email Mechanism&#65288;TEM&#65289;&#12290;&#26234;&#33021;&#20307;&#37319;&#29992;&#26412;&#22320;&#36890;&#20449;&#65292;&#20165;&#21521;&#21487;&#20197;&#35266;&#23519;&#21040;&#30340;&#26234;&#33021;&#20307;&#21457;&#36865;&#28040;&#24687;&#65292;&#26080;&#38656;&#27169;&#25311;&#25152;&#26377;&#26234;&#33021;&#20307;&#12290;&#21463;&#21040;&#20154;&#31867;&#30005;&#23376;&#37038;&#20214;&#36716;&#21457;&#21512;&#20316;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#28040;&#24687;&#38142;&#20197;&#23558;&#20449;&#24687;&#36716;&#21457;&#32473;&#35266;&#23519;&#33539;&#22260;&#20043;&#22806;&#30340;&#26234;&#33021;&#20307;&#12290;&#25105;&#20204;&#24341;&#20837;Transformer&#26469;&#32534;&#30721;&#21644;&#35299;&#30721;&#28040;&#24687;&#38142;&#65292;&#20197;&#36873;&#25321;&#19979;&#19968;&#20010;&#25509;&#25910;&#32773;&#12290;&#22312;&#22810;&#20010;&#21512;&#20316;&#30340;MARL&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#23454;&#39564;&#35777;&#26126;TEM&#20248;&#20110;&#22522;&#32447;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Communication can impressively improve cooperation in multi-agent reinforcement learning (MARL), especially for partially-observed tasks. However, existing works either broadcast the messages leading to information redundancy, or learn targeted communication by modeling all the other agents as targets, which is not scalable when the number of agents varies. In this work, to tackle the scalability problem of MARL communication for partially-observed tasks, we propose a novel framework Transformer-based Email Mechanism (TEM). The agents adopt local communication to send messages only to the ones that can be observed without modeling all the agents. Inspired by human cooperation with email forwarding, we design message chains to forward information to cooperate with the agents outside the observation range. We introduce Transformer to encode and decode the message chain to choose the next receiver selectively. Empirically, TEM outperforms the baselines on multiple cooperative MARL benchma
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#37325;&#26032;&#23450;&#20041;&#20102;&#20449;&#24687;&#20256;&#36755;&#36895;&#29575;&#65288;ITR&#65289;&#65292;&#36890;&#36807;&#23558;&#23492;&#23487;&#20110;&#35270;&#32593;&#33180;&#31070;&#32463;&#20132;&#21449;&#22788;&#30340;&#20849;&#29983;&#36890;&#20449;&#23186;&#20171;&#24314;&#27169;&#20026;&#31163;&#25955;&#26080;&#35760;&#24518;&#20449;&#36947;&#65292;&#24182;&#20351;&#29992;&#20462;&#25913;&#21518;&#30340;&#23481;&#37327;&#34920;&#36798;&#24335;&#36827;&#34892;&#35745;&#31639;&#12290;&#36825;&#23558;&#26377;&#21161;&#20110;&#26410;&#26469;BCI&#35774;&#35745;&#30340;&#31471;&#21040;&#31471;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2301.00488</link><description>&lt;p&gt;
BCI&#20013;&#30340;&#20449;&#24687;&#20256;&#36755;&#36895;&#29575;&#65306;&#36208;&#21521;&#32039;&#23494;&#38598;&#25104;&#30340;&#20849;&#29983;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
Information Transfer Rate in BCIs: Towards Tightly Integrated Symbiosis. (arXiv:2301.00488v3 [cs.HC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.00488
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#37325;&#26032;&#23450;&#20041;&#20102;&#20449;&#24687;&#20256;&#36755;&#36895;&#29575;&#65288;ITR&#65289;&#65292;&#36890;&#36807;&#23558;&#23492;&#23487;&#20110;&#35270;&#32593;&#33180;&#31070;&#32463;&#20132;&#21449;&#22788;&#30340;&#20849;&#29983;&#36890;&#20449;&#23186;&#20171;&#24314;&#27169;&#20026;&#31163;&#25955;&#26080;&#35760;&#24518;&#20449;&#36947;&#65292;&#24182;&#20351;&#29992;&#20462;&#25913;&#21518;&#30340;&#23481;&#37327;&#34920;&#36798;&#24335;&#36827;&#34892;&#35745;&#31639;&#12290;&#36825;&#23558;&#26377;&#21161;&#20110;&#26410;&#26469;BCI&#35774;&#35745;&#30340;&#31471;&#21040;&#31471;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20449;&#24687;&#20256;&#36755;&#36895;&#29575;&#65288;ITR&#65289;&#25110;&#26377;&#25928;&#27604;&#29305;&#29575;&#26159;&#19968;&#31181;&#27969;&#34892;&#32780;&#24191;&#27867;&#20351;&#29992;&#30340;&#20449;&#24687;&#24230;&#37327;&#26631;&#20934;&#65292;&#29305;&#21035;&#26159;&#22312;&#22522;&#20110;SSVEP&#30340;&#33041;&#26426;&#25509;&#21475;&#65288;BCI&#65289;&#20013;&#27969;&#34892;&#12290;&#36890;&#36807;&#23558;&#36895;&#24230;&#21644;&#20934;&#30830;&#24615;&#32467;&#21512;&#20026;&#21333;&#19968;&#20540;&#21442;&#25968;&#65292;&#35813;&#25351;&#26631;&#26377;&#21161;&#20110;&#35780;&#20272;&#21644;&#27604;&#36739;&#19981;&#21516;BCI&#31038;&#21306;&#20013;&#30340;&#21508;&#31181;&#30446;&#26631;&#35782;&#21035;&#31639;&#27861;&#12290;&#20026;&#20102;&#35745;&#31639;ITR&#65292;&#36890;&#24120;&#20551;&#23450;&#36755;&#20837;&#20998;&#24067;&#26159;&#19968;&#33268;&#30340;&#65292;&#24182;&#19988;&#20449;&#36947;&#27169;&#22411;&#36807;&#20110;&#31616;&#21333;&#65292;&#26159;&#26080;&#35760;&#24518;&#30340;&#12289;&#24179;&#31283;&#30340;&#12289;&#23545;&#31216;&#30340;&#65292;&#24182;&#20855;&#26377;&#31163;&#25955;&#30340;&#23383;&#27597;&#34920;&#22823;&#23567;&#12290;&#20026;&#20102;&#20934;&#30830;&#22320;&#25551;&#36848;&#24615;&#33021;&#24182;&#28608;&#21457;&#38754;&#21521;&#26410;&#26469;&#30340;BCI&#35774;&#35745;&#30340;&#31471;&#21040;&#31471;&#35774;&#35745;&#65292;&#22240;&#27492;&#38656;&#35201;&#26356;&#20840;&#38754;&#22320;&#23457;&#26597;&#21644;&#23450;&#20041;ITR&#12290;&#25105;&#20204;&#23558;&#23492;&#23487;&#20110;&#35270;&#32593;&#33180;&#31070;&#32463;&#20132;&#21449;&#22788;&#30340;&#20849;&#29983;&#36890;&#20449;&#23186;&#20171;&#24314;&#27169;&#20026;&#31163;&#25955;&#26080;&#35760;&#24518;&#20449;&#36947;&#65292;&#24182;&#20351;&#29992;&#20462;&#25913;&#21518;&#30340;&#23481;&#37327;&#34920;&#36798;&#24335;&#37325;&#26032;&#23450;&#20041;ITR&#12290;&#25105;&#20204;&#21033;&#29992;&#26377;&#21521;&#22270;&#30340;&#32467;&#26524;&#26469;&#34920;&#24449;&#36890;&#36947;&#30340;&#21487;&#36798;&#24615;&#21644;&#36890;&#20449;&#23481;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
The information transmission rate (ITR), or effective bit rate, is a popular and widely used information measurement metric, particularly popularized for SSVEP-based Brain-Computer (BCI) interfaces. By combining speed and accuracy into a single-valued parameter, this metric aids in the evaluation and comparison of various target identification algorithms across different BCI communities. In order to calculate ITR, it is customary to assume a uniform input distribution and an oversimplified channel model that is memoryless, stationary, and symmetrical in nature with discrete alphabet sizes. To accurately depict performance and inspire an end-to-end design for futuristic BCI designs, a more thorough examination and definition of ITR is therefore required. We model the symbiotic communication medium, hosted by the retinogeniculate visual pathway, as a discrete memoryless channel and use the modified capacity expressions to redefine the ITR. We leverage a result for directed graphs to char
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;Twitter&#19978;&#19982;MEV&#30456;&#20851;&#30340;&#35805;&#39064;&#65292;&#32467;&#26524;&#34920;&#26126;&#25512;&#25991;&#35752;&#35770;&#20102;&#28145;&#21051;&#30340;&#20262;&#29702;&#38382;&#39064;&#65292;&#21253;&#25324;&#23433;&#20840;&#12289;&#20844;&#24179;&#12289;&#24773;&#24863;&#24773;&#32490;&#21644;&#23545;MEV&#35299;&#20915;&#26041;&#26696;&#30340;&#28212;&#26395;&#12290;</title><link>http://arxiv.org/abs/2212.06951</link><description>&lt;p&gt;
&#21306;&#22359;&#38142;&#19978;&#30340;AI&#20262;&#29702;: &#22522;&#20110;Twitter&#25968;&#25454;&#30340;&#21306;&#22359;&#38142;&#23433;&#20840;&#20027;&#39064;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
AI Ethics on Blockchain: Topic Analysis on Twitter Data for Blockchain Security. (arXiv:2212.06951v3 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.06951
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;Twitter&#19978;&#19982;MEV&#30456;&#20851;&#30340;&#35805;&#39064;&#65292;&#32467;&#26524;&#34920;&#26126;&#25512;&#25991;&#35752;&#35770;&#20102;&#28145;&#21051;&#30340;&#20262;&#29702;&#38382;&#39064;&#65292;&#21253;&#25324;&#23433;&#20840;&#12289;&#20844;&#24179;&#12289;&#24773;&#24863;&#24773;&#32490;&#21644;&#23545;MEV&#35299;&#20915;&#26041;&#26696;&#30340;&#28212;&#26395;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21306;&#22359;&#38142;&#20351;&#29992;&#20998;&#24067;&#24335;&#32593;&#32476;&#35753;&#35745;&#31639;&#26426;&#31995;&#32479;&#26356;&#23433;&#20840;&#65292;&#20294;&#24403;&#21069;&#30340;&#21306;&#22359;&#38142;&#35774;&#35745;&#22312;&#20132;&#26131;&#39034;&#24207;&#26041;&#38754;&#23384;&#22312;&#20844;&#24179;&#24615;&#38382;&#39064;&#12290;&#30719;&#24037;&#21487;&#20197;&#37325;&#26032;&#25490;&#24207;&#20132;&#26131;&#20197;&#29983;&#25104;&#21033;&#28070;&#65292;&#36825;&#34987;&#31216;&#20026;&#30719;&#24037;&#21487;&#25552;&#21462;&#20215;&#20540;&#65288;MEV&#65289;&#38382;&#39064;&#12290;&#29616;&#26377;&#30740;&#31350;&#35748;&#20026;MEV&#26159;&#19968;&#20010;&#20005;&#37325;&#30340;&#23433;&#20840;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#28508;&#22312;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21253;&#25324;&#30693;&#21517;&#30340;Flashbots&#12290;&#28982;&#32780;&#65292;&#20197;&#24448;&#30340;&#30740;&#31350;&#22823;&#22810;&#20998;&#26512;&#20102;&#21306;&#22359;&#38142;&#25968;&#25454;&#65292;&#36825;&#21487;&#33021;&#26080;&#27861;&#25429;&#25417;&#21040;MEV&#22312;&#26356;&#24191;&#27867;&#30340;AI&#31038;&#20250;&#20013;&#30340;&#24433;&#21709;&#12290;&#22240;&#27492;&#65292;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24212;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#26041;&#27861;&#20840;&#38754;&#20998;&#26512;&#20102;MEV&#25512;&#25991;&#20013;&#30340;&#35805;&#39064;&#12290;&#25105;&#20204;&#25910;&#38598;&#20102;&#36229;&#36807;20,000&#20010;MEV&#21644;Flashbots&#26631;&#31614;&#30340;&#25512;&#25991;&#24182;&#20998;&#26512;&#20102;&#23427;&#20204;&#30340;&#35805;&#39064;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#36825;&#20123;&#25512;&#25991;&#35752;&#35770;&#20102;&#28145;&#21051;&#30340;&#20262;&#29702;&#38382;&#39064;&#65292;&#21253;&#25324;&#23433;&#20840;&#12289;&#20844;&#24179;&#12289;&#24773;&#24863;&#24773;&#32490;&#21644;&#23545;MEV&#35299;&#20915;&#26041;&#26696;&#30340;&#28212;&#26395;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#20102;&#21306;&#22359;&#38142;&#19978;MEV&#27963;&#21160;&#30340;&#20849;&#21516;&#36816;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
Blockchain has empowered computer systems to be more secure using a distributed network. However, the current blockchain design suffers from fairness issues in transaction ordering. Miners are able to reorder transactions to generate profits, the so-called miner extractable value (MEV). Existing research recognizes MEV as a severe security issue and proposes potential solutions, including prominent Flashbots. However, previous studies have mostly analyzed blockchain data, which might not capture the impacts of MEV in a much broader AI society. Thus, in this research, we applied natural language processing (NLP) methods to comprehensively analyze topics in tweets on MEV. We collected more than 20000 tweets with MEV and Flashbots hashtags and analyzed their topics. Our results show that the tweets discussed profound topics of ethical concern, including security, equity, emotional sentiments, and the desire for solutions to MEV. We also identify the co-movements of MEV activities on block
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#39118;&#26684;&#36716;&#31227;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#21069;&#38376;&#35843;&#25972;&#26469;&#35299;&#20915;&#22240;&#26524;&#25512;&#26029;&#20013;&#23384;&#22312;&#30340;&#28151;&#28102;&#22240;&#32032;&#65292;&#23454;&#29616;&#39046;&#22495;&#22806;&#27867;&#21270;&#12290;</title><link>http://arxiv.org/abs/2212.03063</link><description>&lt;p&gt;
&#39118;&#26684;&#36716;&#31227;&#29992;&#20110;&#22240;&#26524;&#25512;&#26029;&#65292;&#23454;&#29616;&#39046;&#22495;&#22806;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Causal Inference via Style Transfer for Out-of-distribution Generalisation. (arXiv:2212.03063v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.03063
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#39118;&#26684;&#36716;&#31227;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#21069;&#38376;&#35843;&#25972;&#26469;&#35299;&#20915;&#22240;&#26524;&#25512;&#26029;&#20013;&#23384;&#22312;&#30340;&#28151;&#28102;&#22240;&#32032;&#65292;&#23454;&#29616;&#39046;&#22495;&#22806;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39046;&#22495;&#22806;&#27867;&#21270;&#26088;&#22312;&#26500;&#24314;&#19968;&#20010;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#20351;&#29992;&#22810;&#20010;&#28304;&#39046;&#22495;&#30340;&#30693;&#35782;&#65292;&#22312;&#26410;&#35265;&#30446;&#26631;&#39046;&#22495;&#19978;&#23454;&#29616;&#33391;&#22909;&#30340;&#27867;&#21270;&#12290;&#20026;&#27492;&#65292;&#27169;&#22411;&#24212;&#23547;&#27714;&#36755;&#20837;&#21644;&#26631;&#31614;&#20043;&#38388;&#22240;&#26524;&#20381;&#36182;&#24615;&#65292;&#35813;&#20381;&#36182;&#24615;&#21487;&#33021;&#30001;&#36755;&#20837;&#30340;&#35821;&#20041;&#20915;&#23450;&#65292;&#24182;&#22312;&#22810;&#20010;&#39046;&#22495;&#20013;&#20445;&#25345;&#19981;&#21464;&#12290;&#28982;&#32780;&#65292;&#32479;&#35745;&#25110;&#38750;&#22240;&#26524;&#26041;&#27861;&#36890;&#24120;&#26080;&#27861;&#25429;&#25417;&#21040;&#36825;&#31181;&#20381;&#36182;&#24615;&#65292;&#24182;&#22240;&#26410;&#32771;&#34385;&#30001;&#26410;&#35266;&#27979;&#21040;&#30340;&#28151;&#28102;&#22240;&#32032;&#23398;&#20064;&#30340;&#34920;&#35266;&#30456;&#20851;&#24615;&#32780;&#34920;&#29616;&#19981;&#20339;&#12290;&#29616;&#26377;&#30340;&#20247;&#25152;&#21608;&#30693;&#30340;&#22240;&#26524;&#25512;&#26029;&#26041;&#27861;&#65292;&#22914;&#21518;&#38376;&#35843;&#25972;&#65292;&#26080;&#27861;&#24212;&#29992;&#20110;&#28040;&#38500;&#34920;&#35266;&#30456;&#20851;&#24615;&#65292;&#22240;&#20026;&#23427;&#38656;&#35201;&#35266;&#23519;&#28151;&#28102;&#22240;&#32032;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#25104;&#21151;&#23454;&#29616;&#21069;&#38376;&#35843;&#25972;&#65288;FA&#65289;&#26377;&#25928;&#22320;&#22788;&#29702;&#38544;&#34255;&#30340;&#28151;&#28102;&#22240;&#32032;&#12290;FA&#38656;&#35201;&#36873;&#25321;&#19968;&#20010;&#20013;&#20171;&#20154;&#65292;&#25105;&#20204;&#35748;&#20026;&#36825;&#20010;&#20013;&#20171;&#20154;&#26159;&#22270;&#20687;&#30340;&#35821;&#20041;&#20449;&#24687;&#65292;&#23427;&#26377;&#21161;&#20110;&#19981;&#38656;&#35201;&#35266;&#23519;&#28151;&#28102;&#22240;&#32032;&#65292;&#23601;&#33021;&#35775;&#38382;&#22240;&#26524;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Out-of-distribution (OOD) generalisation aims to build a model that can generalise well on an unseen target domain using knowledge from multiple source domains. To this end, the model should seek the causal dependence between inputs and labels, which may be determined by the semantics of inputs and remain invariant across domains. However, statistical or non-causal methods often cannot capture this dependence and perform poorly due to not considering spurious correlations learnt from model training via unobserved confounders. A well-known existing causal inference method like back-door adjustment cannot be applied to remove spurious correlations as it requires the observation of confounders. In this paper, we propose a novel method that effectively deals with hidden confounders by successfully implementing front-door adjustment (FA). FA requires the choice of a mediator, which we regard as the semantic information of images that helps access the causal mechanism without the need for ob
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22343;&#20540;&#28418;&#31227;&#25513;&#27169;&#21464;&#25442;&#22120;&#65292;&#29992;&#20110;&#32852;&#21512;&#35757;&#32451;&#21644;&#25512;&#26029;&#29305;&#24449;&#25552;&#21462;&#22120;&#21644;&#32858;&#31867;&#22120;&#65292;&#21487;&#29992;&#20110;&#26410;&#30693;&#29289;&#20307;&#23454;&#20363;&#20998;&#21106;&#65292;&#22312;COCO&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#31454;&#20105;&#24615;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#32597;&#35265;&#21644;&#26410;&#30693;&#29289;&#20307;&#31867;&#21035;&#19978;&#20855;&#26377;&#26174;&#30528;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2211.11679</link><description>&lt;p&gt;
&#29992;&#20110;&#26410;&#30693;&#29289;&#20307;&#23454;&#20363;&#20998;&#21106;&#30340;&#22343;&#20540;&#28418;&#31227;&#25513;&#27169;&#21464;&#25442;&#22120;
&lt;/p&gt;
&lt;p&gt;
Mean Shift Mask Transformer for Unseen Object Instance Segmentation. (arXiv:2211.11679v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.11679
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22343;&#20540;&#28418;&#31227;&#25513;&#27169;&#21464;&#25442;&#22120;&#65292;&#29992;&#20110;&#32852;&#21512;&#35757;&#32451;&#21644;&#25512;&#26029;&#29305;&#24449;&#25552;&#21462;&#22120;&#21644;&#32858;&#31867;&#22120;&#65292;&#21487;&#29992;&#20110;&#26410;&#30693;&#29289;&#20307;&#23454;&#20363;&#20998;&#21106;&#65292;&#22312;COCO&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#31454;&#20105;&#24615;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#32597;&#35265;&#21644;&#26410;&#30693;&#29289;&#20307;&#31867;&#21035;&#19978;&#20855;&#26377;&#26174;&#30528;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#20307;&#23454;&#20363;&#30340;&#20998;&#21106;&#26159;&#26426;&#22120;&#20154;&#38656;&#35201;&#25484;&#25569;&#30340;&#20851;&#38190;&#24863;&#30693;&#25216;&#33021;&#20043;&#19968;&#65292;&#23427;&#26377;&#21161;&#20110;&#26426;&#22120;&#20154;&#25235;&#21462;&#21644;&#25805;&#20316;&#26410;&#30693;&#29289;&#20307;&#12290;&#22343;&#20540;&#28418;&#31227;&#32858;&#31867;&#26159;&#19968;&#31181;&#24191;&#27867;&#29992;&#20110;&#22270;&#20687;&#20998;&#21106;&#20219;&#21153;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#22343;&#20540;&#28418;&#31227;&#32858;&#31867;&#31639;&#27861;&#19981;&#21487;&#24494;&#20998;&#65292;&#20351;&#20854;&#38590;&#20197;&#38598;&#25104;&#21040;&#31471;&#21040;&#31471;&#30340;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#26694;&#26550;&#20013;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22343;&#20540;&#28418;&#31227;&#25513;&#27169;&#21464;&#25442;&#22120;&#65288;MSMFormer&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#30340;&#21464;&#25442;&#22120;&#20307;&#31995;&#32467;&#26500;&#65292;&#27169;&#25311; von Mises-Fisher&#65288;vMF&#65289;&#22343;&#20540;&#28418;&#31227;&#32858;&#31867;&#31639;&#27861;&#65292;&#20801;&#35768;&#32852;&#21512;&#35757;&#32451;&#21644;&#25512;&#26029;&#29305;&#24449;&#25552;&#21462;&#22120;&#21644;&#32858;&#31867;&#22120;&#12290;&#20854;&#26680;&#24515;&#32452;&#20214;&#26159;&#36229;&#29699;&#38754;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#21487;&#22312;&#36229;&#29699;&#38754;&#19978;&#26356;&#26032;&#29289;&#20307;&#26597;&#35810;&#12290;&#20026;&#20102;&#35828;&#26126;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#23558;MSMFormer&#24212;&#29992;&#20110;&#26410;&#30693;&#29289;&#20307;&#23454;&#20363;&#20998;&#21106;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;MSMFormer&#22312;COCO&#25968;&#25454;&#38598;&#19978;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#22312;&#32597;&#35265;&#21644;&#26410;&#30693;&#29289;&#20307;&#31867;&#21035;&#19978;&#20855;&#26377;&#26174;&#30528;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Segmenting unseen objects from images is a critical perception skill that a robot needs to acquire. In robot manipulation, it can facilitate a robot to grasp and manipulate unseen objects. Mean shift clustering is a widely used method for image segmentation tasks. However, the traditional mean shift clustering algorithm is not differentiable, making it difficult to integrate it into an end-to-end neural network training framework. In this work, we propose the Mean Shift Mask Transformer (MSMFormer), a new transformer architecture that simulates the von Mises-Fisher (vMF) mean shift clustering algorithm, allowing for the joint training and inference of both the feature extractor and the clustering. Its central component is a hypersphere attention mechanism, which updates object queries on a hypersphere. To illustrate the effectiveness of our method, we apply MSMFormer to unseen object instance segmentation. Our experiments show that MSMFormer achieves competitive performance compared to
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20449;&#24687;&#22686;&#30410;&#20027;&#23548;&#30340;&#20027;&#21160;&#25506;&#32034;&#26041;&#27861;&#65292;&#36890;&#36807;&#31890;&#23376;&#28388;&#27874;&#22120;&#23454;&#29616;&#39034;&#24207;&#36125;&#21494;&#26031;&#25512;&#29702;&#21644;&#30446;&#26631;&#30830;&#23450;&#65292;&#33021;&#22815;&#24110;&#21161;&#33258;&#20027;&#26426;&#22120;&#20154;&#22312;&#24418;&#25104;&#21644;&#23398;&#20064;&#31354;&#38388;&#27010;&#24565;&#30340;&#36807;&#31243;&#20013;&#39640;&#25928;&#22320;&#25506;&#32034;&#29615;&#22659;&#21644;&#19982;&#29992;&#25143;&#20132;&#20114;&#12290;</title><link>http://arxiv.org/abs/2211.10934</link><description>&lt;p&gt;
&#22522;&#20110;&#31890;&#23376;&#28388;&#27874;&#22120;&#30340;&#20449;&#24687;&#22686;&#30410;&#20027;&#23548;&#30340;&#20027;&#21160;&#25506;&#32034;&#29992;&#20110;&#39640;&#25928;&#30340;&#31354;&#38388;&#27010;&#24565;&#24418;&#25104;
&lt;/p&gt;
&lt;p&gt;
Active Exploration based on Information Gain by Particle Filter for Efficient Spatial Concept Formation. (arXiv:2211.10934v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.10934
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20449;&#24687;&#22686;&#30410;&#20027;&#23548;&#30340;&#20027;&#21160;&#25506;&#32034;&#26041;&#27861;&#65292;&#36890;&#36807;&#31890;&#23376;&#28388;&#27874;&#22120;&#23454;&#29616;&#39034;&#24207;&#36125;&#21494;&#26031;&#25512;&#29702;&#21644;&#30446;&#26631;&#30830;&#23450;&#65292;&#33021;&#22815;&#24110;&#21161;&#33258;&#20027;&#26426;&#22120;&#20154;&#22312;&#24418;&#25104;&#21644;&#23398;&#20064;&#31354;&#38388;&#27010;&#24565;&#30340;&#36807;&#31243;&#20013;&#39640;&#25928;&#22320;&#25506;&#32034;&#29615;&#22659;&#21644;&#19982;&#29992;&#25143;&#20132;&#20114;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20027;&#26426;&#22120;&#20154;&#38656;&#35201;&#36890;&#36807;&#25506;&#32034;&#29615;&#22659;&#24182;&#19982;&#29992;&#25143;&#20132;&#20114;&#26469;&#23398;&#20064;&#21508;&#31181;&#22320;&#26041;&#30340;&#31867;&#21035;&#12290;&#28982;&#32780;&#65292;&#20174;&#29992;&#25143;&#37027;&#37324;&#20934;&#22791;&#35821;&#35328;&#25351;&#23548;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#26159;&#32791;&#26102;&#19988;&#21171;&#21160;&#23494;&#38598;&#30340;&#12290;&#27492;&#22806;&#65292;&#26377;&#25928;&#30340;&#25506;&#32034;&#23545;&#20110;&#36866;&#24403;&#30340;&#27010;&#24565;&#24418;&#25104;&#21644;&#24555;&#36895;&#29615;&#22659;&#35206;&#30422;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20027;&#21160;&#25512;&#29702;&#26041;&#27861;&#65292;&#31216;&#20026;&#22522;&#20110;&#20449;&#24687;&#22686;&#30410;&#30340;&#20027;&#21160;&#25506;&#32034;&#31354;&#38388;&#27010;&#24565;&#24418;&#25104;&#65288;SpCoAE&#65289;&#65292;&#23558;&#31890;&#23376;&#28388;&#27874;&#22120;&#30340;&#39034;&#24207;&#36125;&#21494;&#26031;&#25512;&#29702;&#21644;&#22522;&#20110;&#20449;&#24687;&#22686;&#30410;&#30340;&#30446;&#26631;&#30830;&#23450;&#19982;&#27010;&#29575;&#29983;&#25104;&#27169;&#22411;&#30456;&#32467;&#21512;&#12290;&#35813;&#30740;&#31350;&#23558;&#26426;&#22120;&#20154;&#30340;&#21160;&#20316;&#35299;&#37322;&#20026;&#22312;&#20027;&#21160;&#25512;&#29702;&#30340;&#29615;&#22659;&#19979;&#36873;&#25321;&#35201;&#38382;&#29992;&#25143;&#8220;&#36825;&#26159;&#20160;&#20040;&#22320;&#26041;&#65311;&#8221;&#30340;&#30446;&#30340;&#22320;&#12290;&#35813;&#30740;&#31350;&#25552;&#20379;&#20102;&#26377;&#20851;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#25216;&#26415;&#26041;&#38754;&#65292;&#21253;&#25324;&#26426;&#22120;&#20154;&#30340;&#31215;&#26497;&#24863;&#30693;&#21644;&#25506;&#32034;&#65292;&#20197;&#21450;&#35813;&#26041;&#27861;&#22914;&#20309;&#20351;&#31227;&#21160;&#26426;&#22120;&#20154;&#33021;&#22815;&#39640;&#25928;&#22320;&#24418;&#25104;&#21644;&#23398;&#20064;&#31354;&#38388;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autonomous robots need to learn the categories of various places by exploring their environments and interacting with users. However, preparing training datasets with linguistic instructions from users is time-consuming and labor-intensive. Moreover, effective exploration is essential for appropriate concept formation and rapid environmental coverage. To address this issue, we propose an active inference method, referred to as spatial concept formation with information gain-based active exploration (SpCoAE) that combines sequential Bayesian inference using particle filters and information gain-based destination determination in a probabilistic generative model. This study interprets the robot's action as a selection of destinations to ask the user, `What kind of place is this?' in the context of active inference. This study provides insights into the technical aspects of the proposed method, including active perception and exploration by the robot, and how the method can enable mobile 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22270;&#27744;&#21270;&#31639;&#23376;&#23545;&#20110;Manifold-Mixup&#26041;&#27861;&#22312;&#22270;&#24418;&#23398;&#20064;&#20013;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28151;&#21512;&#27744;&#21270;&#26550;&#26500;&#65292;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#22312;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#19978;&#34920;&#29616;&#26174;&#33879;&#20248;&#36234;&#24182;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#24615;&#33021;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2210.03123</link><description>&lt;p&gt;
&#28151;&#21512;&#27744;&#21270;&#22312;&#22522;&#20110;Mixup&#30340;&#22270;&#24418;&#23398;&#20064;&#20013;&#30340;&#26377;&#25928;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Effectiveness of Hybrid Pooling in Mixup-Based Graph Learning for Language Processing. (arXiv:2210.03123v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.03123
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22270;&#27744;&#21270;&#31639;&#23376;&#23545;&#20110;Manifold-Mixup&#26041;&#27861;&#22312;&#22270;&#24418;&#23398;&#20064;&#20013;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28151;&#21512;&#27744;&#21270;&#26550;&#26500;&#65292;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#22312;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#19978;&#34920;&#29616;&#26174;&#33879;&#20248;&#36234;&#24182;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#24615;&#33021;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#22270;&#24418;&#23398;&#20064;&#22312;&#33258;&#28982;&#35821;&#35328;&#21644;&#32534;&#31243;&#35821;&#35328;&#22788;&#29702;&#26041;&#38754;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#29305;&#21035;&#26159;&#22312;&#25991;&#26412;&#21644;&#28304;&#20195;&#30721;&#20998;&#31867;&#26041;&#38754;&#12290;&#36890;&#24120;&#65292;GNN&#26159;&#30001;&#20132;&#26367;&#22270;&#23618;&#21644;&#22270;&#27744;&#21270;&#23618;&#26500;&#25104;&#30340;&#65292;&#20132;&#26367;&#22270;&#23618;&#21487;&#20197;&#23398;&#20064;&#22270;&#33410;&#28857;&#29305;&#24449;&#30340;&#36716;&#25442;&#65292;&#32780;&#22270;&#27744;&#21270;&#23618;&#21017;&#20351;&#29992;&#22270;&#27744;&#21270;&#31639;&#23376;&#65288;&#20363;&#22914;Max&#27744;&#21270;&#65289;&#26377;&#25928;&#22320;&#20943;&#23569;&#33410;&#28857;&#25968;&#37327;&#65292;&#21516;&#26102;&#20445;&#30041;&#22270;&#30340;&#35821;&#20041;&#20449;&#24687;&#12290;&#26368;&#36817;&#65292;&#20026;&#20102;&#22686;&#24378;GNN&#22312;&#22270;&#24418;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#65292;&#20154;&#20204;&#24191;&#27867;&#37319;&#29992;&#20102;Manifold-Mixup&#36825;&#31181;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#35813;&#25216;&#26415;&#36890;&#36807;&#32447;&#24615;&#28151;&#21512;&#19968;&#23545;&#22270;&#25968;&#25454;&#21644;&#23427;&#20204;&#30340;&#26631;&#31614;&#26469;&#29983;&#25104;&#21512;&#25104;&#22270;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;Manifold-Mixup&#30340;&#24615;&#33021;&#24456;&#22823;&#31243;&#24230;&#19978;&#21463;&#21040;&#22270;&#27744;&#21270;&#31639;&#23376;&#30340;&#24433;&#21709;&#65292;&#32780;&#19988;&#24182;&#27809;&#26377;&#36827;&#34892;&#24456;&#22810;&#20851;&#20110;&#36825;&#31181;&#24433;&#21709;&#30340;&#30740;&#31350;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#26089;&#26399;&#25506;&#32034;&#20102;&#22270;&#27744;&#21270;&#31639;&#23376;&#22914;&#20309;&#24433;&#21709;&#22522;&#20110;Mixup&#30340;&#22270;&#24418;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28151;&#21512;&#27744;&#21270;&#26550;&#26500;&#65292;&#32467;&#21512;&#20102;Max-pooling&#21644;Attention-pooling&#65292;&#20197;&#26356;&#22909;&#22320;&#25429;&#25417;&#26412;&#22320;&#21644;&#20840;&#23616;&#30340;&#22270;&#32467;&#26500;&#20449;&#24687;&#12290;&#25105;&#20204;&#22312;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#28151;&#21512;&#27744;&#21270;&#32467;&#26500;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;&#27744;&#21270;&#26041;&#27861;&#65292;&#24182;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural network (GNN)-based graph learning has been popular in natural language and programming language processing, particularly in text and source code classification. Typically, GNNs are constructed by incorporating alternating layers which learn transformations of graph node features, along with graph pooling layers that use graph pooling operators (e.g., Max-pooling) to effectively reduce the number of nodes while preserving the semantic information of the graph. Recently, to enhance GNNs in graph learning tasks, Manifold-Mixup, a data augmentation technique that produces synthetic graph data by linearly mixing a pair of graph data and their labels, has been widely adopted. However, the performance of Manifold-Mixup can be highly affected by graph pooling operators, and there have not been many studies that are dedicated to uncovering such affection. To bridge this gap, we take an early step to explore how graph pooling operators affect the performance of Mixup-based graph le
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#65292;&#36890;&#36807;&#20803;&#23398;&#20064;&#20808;&#39564;&#20174;&#31163;&#32447;&#25968;&#25454;&#20013;&#23454;&#29616;&#23433;&#20840;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#65292;&#21516;&#26102;&#24320;&#21457;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#20197;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#24335;&#36873;&#25321;&#31526;&#21512;&#23433;&#20840;&#35201;&#27714;&#30340;&#20808;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#30456;&#27604;&#20110;&#22522;&#20934;&#26041;&#27861;&#65292;&#20803;&#23398;&#20064;&#20808;&#39564;&#21152;&#24555;&#20102;&#23433;&#20840;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#25910;&#25947;&#36895;&#24230;&#24182;&#25913;&#36827;&#20102;&#25972;&#20307;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2210.00762</link><description>&lt;p&gt;
&#23433;&#20840;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#20803;&#23398;&#20064;&#20808;&#39564;
&lt;/p&gt;
&lt;p&gt;
Meta-Learning Priors for Safe Bayesian Optimization. (arXiv:2210.00762v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.00762
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#65292;&#36890;&#36807;&#20803;&#23398;&#20064;&#20808;&#39564;&#20174;&#31163;&#32447;&#25968;&#25454;&#20013;&#23454;&#29616;&#23433;&#20840;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#65292;&#21516;&#26102;&#24320;&#21457;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#20197;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#24335;&#36873;&#25321;&#31526;&#21512;&#23433;&#20840;&#35201;&#27714;&#30340;&#20808;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#30456;&#27604;&#20110;&#22522;&#20934;&#26041;&#27861;&#65292;&#20803;&#23398;&#20064;&#20808;&#39564;&#21152;&#24555;&#20102;&#23433;&#20840;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#25910;&#25947;&#36895;&#24230;&#24182;&#25913;&#36827;&#20102;&#25972;&#20307;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#20154;&#23398;&#20013;&#65292;&#20248;&#21270;&#25511;&#21046;&#22120;&#21442;&#25968;&#24182;&#28385;&#36275;&#23433;&#20840;&#32422;&#26463;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#25361;&#25112;&#12290;&#23433;&#20840;&#36125;&#21494;&#26031;&#20248;&#21270;&#36890;&#36807;&#37327;&#21270;&#30446;&#26631;&#21644;&#32422;&#26463;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#26469;&#23433;&#20840;&#22320;&#25351;&#23548;&#25506;&#32034;&#12290;&#28982;&#32780;&#65292;&#22312;&#23384;&#22312;&#26410;&#30693;&#23433;&#20840;&#32422;&#26463;&#30340;&#24773;&#20917;&#19979;&#65292;&#36873;&#25321;&#21487;&#38752;&#30340;&#27169;&#22411;&#36229;&#21442;&#25968;&#20197;&#36991;&#20813;&#23433;&#20840;&#36829;&#35268;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#20154;&#24037;&#35774;&#35745;&#36866;&#21512;&#30340;&#27010;&#29575;&#27169;&#22411;&#21487;&#33021;&#24456;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20803;&#23398;&#20064;&#20808;&#39564;&#20174;&#31163;&#32447;&#25968;&#25454;&#20013;&#23454;&#29616;&#23433;&#20840;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#12290;&#25105;&#20204;&#20511;&#21161;&#20803;&#23398;&#20064;&#31639;&#27861; F-PACOH&#65292;&#22312;&#25968;&#25454;&#31232;&#32570;&#24615;&#30340;&#24773;&#20917;&#19979;&#25552;&#20379;&#21487;&#38752;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#12290;&#21516;&#26102;&#65292;&#22312;&#22522;&#20934;&#20989;&#25968;&#21644;&#39640;&#31934;&#24230;&#36816;&#21160;&#31995;&#32479;&#19978;&#65292;&#25105;&#20204;&#36890;&#36807;&#32463;&#39564;&#19981;&#30830;&#23450;&#24230;&#24230;&#37327;&#21644;&#21069;&#27839;&#25628;&#32034;&#31639;&#27861;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#20197;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#24335;&#36873;&#25321;&#31526;&#21512;&#23433;&#20840;&#35201;&#27714;&#30340;&#20808;&#39564;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#30456;&#27604;&#20110;&#22522;&#20934;&#26041;&#27861;&#65292;&#25105;&#20204;&#30340;&#20803;&#23398;&#20064;&#20808;&#39564;&#21152;&#24555;&#20102;&#23433;&#20840;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#25910;&#25947;&#36895;&#24230;&#24182;&#25913;&#36827;&#20102;&#25972;&#20307;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In robotics, optimizing controller parameters under safety constraints is an important challenge. Safe Bayesian optimization (BO) quantifies uncertainty in the objective and constraints to safely guide exploration in such settings. Hand-designing a suitable probabilistic model can be challenging, however. In the presence of unknown safety constraints, it is crucial to choose reliable model hyper-parameters to avoid safety violations. Here, we propose a data-driven approach to this problem by meta-learning priors for safe BO from offline data. We build on a meta-learning algorithm, F-PACOH, capable of providing reliable uncertainty quantification in settings of data scarcity. As core contribution, we develop a novel framework for choosing safety-compliant priors in a data-riven manner via empirical uncertainty metrics and a frontier search algorithm. On benchmark functions and a high-precision motion system, we demonstrate that our meta-learned priors accelerate the convergence of safe 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026;Safe DeepReach&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#20026;DeepReach&#26041;&#27861;&#29983;&#25104;&#27491;&#24335;&#23433;&#20840;&#20445;&#38556;&#12290;&#35813;&#26694;&#26550;&#23558;&#26032;&#39062;&#30340;Lipschitz&#36830;&#32493;&#24615;&#20998;&#26512;&#19982;&#21306;&#38388;&#36793;&#30028;&#20256;&#25773;&#26041;&#27861;&#30456;&#32467;&#21512;&#65292;&#20197;&#26377;&#25928;&#21644;&#21487;&#20280;&#32553;&#30340;&#26041;&#24335;&#20445;&#35777;&#35299;&#20915;&#26041;&#26696;&#30340;&#23433;&#20840;&#24615;&#12290;</title><link>http://arxiv.org/abs/2209.12336</link><description>&lt;p&gt;
&#38754;&#21521;&#39640;&#32500;&#21487;&#36798;&#24615;&#38382;&#39064;&#30340;&#24418;&#24335;&#21270;&#23433;&#20840;&#20445;&#38556;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Generating Formal Safety Assurances for High-Dimensional Reachability. (arXiv:2209.12336v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.12336
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026;Safe DeepReach&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#20026;DeepReach&#26041;&#27861;&#29983;&#25104;&#27491;&#24335;&#23433;&#20840;&#20445;&#38556;&#12290;&#35813;&#26694;&#26550;&#23558;&#26032;&#39062;&#30340;Lipschitz&#36830;&#32493;&#24615;&#20998;&#26512;&#19982;&#21306;&#38388;&#36793;&#30028;&#20256;&#25773;&#26041;&#27861;&#30456;&#32467;&#21512;&#65292;&#20197;&#26377;&#25928;&#21644;&#21487;&#20280;&#32553;&#30340;&#26041;&#24335;&#20445;&#35777;&#35299;&#20915;&#26041;&#26696;&#30340;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#33258;&#20027;&#31995;&#32479;&#25552;&#20379;&#27491;&#24335;&#30340;&#23433;&#20840;&#21644;&#24615;&#33021;&#20445;&#35777;&#21464;&#24471;&#26085;&#30410;&#37325;&#35201;&#12290;&#21704;&#23494;&#39039;-&#38597;&#31185;&#27604;&#65288;HJ&#65289;&#21487;&#36798;&#24615;&#20998;&#26512;&#26159;&#19968;&#31181;&#27969;&#34892;&#30340;&#24418;&#24335;&#39564;&#35777;&#24037;&#20855;&#65292;&#29992;&#20110;&#25552;&#20379;&#36825;&#20123;&#20445;&#35777;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#22788;&#29702;&#19968;&#33324;&#38750;&#32447;&#24615;&#31995;&#32479;&#21160;&#24577;&#12289;&#26377;&#30028;&#23545;&#25239;&#31995;&#32479;&#24178;&#25200;&#20197;&#21450;&#29366;&#24577;&#21644;&#36755;&#20837;&#32422;&#26463;&#12290;&#20294;&#26159;&#65292;&#23427;&#28041;&#21450;&#21040;&#27714;&#35299;PDE&#65292;&#20854;&#35745;&#31639;&#21644;&#20869;&#23384;&#22797;&#26434;&#24230;&#38543;&#30528;&#29366;&#24577;&#32500;&#24230;&#30340;&#22686;&#21152;&#21576;&#25351;&#25968;&#32423;&#22686;&#38271;&#65292;&#20351;&#20854;&#22312;&#22823;&#22411;&#31995;&#32479;&#19978;&#30340;&#30452;&#25509;&#20351;&#29992;&#21464;&#24471;&#19981;&#21487;&#34892;&#12290;&#26368;&#36817;&#25552;&#20986;&#30340;DeepReach&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;&#27491;&#24358;&#31070;&#32463;PDE&#27714;&#35299;&#22120;&#26469;&#20811;&#26381;&#20102;&#36825;&#19968;&#25361;&#25112;&#65292;&#29992;&#20110;&#35299;&#20915;&#39640;&#32500;&#21487;&#36798;&#24615;&#38382;&#39064;&#65292;&#20854;&#35745;&#31639;&#35201;&#27714;&#38543;&#21487;&#36798;&#31649;&#22797;&#26434;&#24615;&#32780;&#19981;&#26159;&#29366;&#24577;&#31354;&#38388;&#32500;&#24230;&#32780;&#21464;&#21270;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#31070;&#32463;&#32593;&#32476;&#21487;&#33021;&#20250;&#20986;&#29616;&#38169;&#35823;&#65292;&#22240;&#27492;&#35745;&#31639;&#20986;&#30340;&#35299;&#20915;&#26041;&#26696;&#21487;&#33021;&#19981;&#23433;&#20840;&#65292;&#36825;&#27809;&#26377;&#36798;&#21040;&#25105;&#20204;&#25552;&#20379;&#27491;&#24335;&#23433;&#20840;&#20445;&#38556;&#30340;&#24635;&#20307;&#30446;&#26631;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026;Safe DeepReach&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#20026;DeepReach&#26041;&#27861;&#29983;&#25104;&#27491;&#24335;&#23433;&#20840;&#20445;&#38556;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#23558;&#26032;&#39062;&#30340;Lipschitz&#36830;&#32493;&#24615;&#20998;&#26512;&#19982;&#21306;&#38388;&#36793;&#30028;&#20256;&#25773;&#26041;&#27861;&#30456;&#32467;&#21512;&#65292;&#20197;&#26377;&#25928;&#21644;&#21487;&#20280;&#32553;&#30340;&#26041;&#24335;&#20445;&#35777;&#35299;&#20915;&#26041;&#26696;&#30340;&#23433;&#20840;&#24615;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#22522;&#20934;&#31034;&#20363;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#21253;&#25324;&#22522;&#20110;&#24863;&#30693;&#30340;&#39640;&#32500;&#36710;&#36947;&#20445;&#25345;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
Providing formal safety and performance guarantees for autonomous systems is becoming increasingly important. Hamilton-Jacobi (HJ) reachability analysis is a popular formal verification tool for providing these guarantees, since it can handle general nonlinear system dynamics, bounded adversarial system disturbances, and state and input constraints. However, it involves solving a PDE, whose computational and memory complexity scales exponentially with respect to the state dimensionality, making its direct use on large-scale systems intractable. A recently proposed method called DeepReach overcomes this challenge by leveraging a sinusoidal neural PDE solver for high-dimensional reachability problems, whose computational requirements scale with the complexity of the underlying reachable tube rather than the state space dimension. Unfortunately, neural networks can make errors and thus the computed solution may not be safe, which falls short of achieving our overarching goal to provide fo
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#35299;&#20915;&#40065;&#26834;&#24615;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#19968;&#38454;&#26041;&#27861;&#65292;&#31216;&#20026;&#40065;&#26834;&#31574;&#30053;&#38236;&#20687;&#19979;&#38477;&#31639;&#27861;&#65288;RPMD&#65289;&#12290;&#36890;&#36807;&#20351;&#29992;&#32447;&#24615;&#36882;&#22686;&#27493;&#36827;&#65292;&#31639;&#27861;&#20855;&#26377;&#36739;&#20302;&#30340;&#22797;&#26434;&#24230;&#65292;&#24182;&#19988;&#33021;&#22815;&#22312;&#19981;&#30830;&#23450;&#24773;&#20917;&#19979;&#25214;&#21040;&#26368;&#20248;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2209.10579</link><description>&lt;p&gt;
&#20445;&#35777;&#40065;&#26834;&#24615;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#19968;&#38454;&#31574;&#30053;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
First-order Policy Optimization for Robust Markov Decision Process. (arXiv:2209.10579v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.10579
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#35299;&#20915;&#40065;&#26834;&#24615;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#19968;&#38454;&#26041;&#27861;&#65292;&#31216;&#20026;&#40065;&#26834;&#31574;&#30053;&#38236;&#20687;&#19979;&#38477;&#31639;&#27861;&#65288;RPMD&#65289;&#12290;&#36890;&#36807;&#20351;&#29992;&#32447;&#24615;&#36882;&#22686;&#27493;&#36827;&#65292;&#31639;&#27861;&#20855;&#26377;&#36739;&#20302;&#30340;&#22797;&#26434;&#24230;&#65292;&#24182;&#19988;&#33021;&#22815;&#22312;&#19981;&#30830;&#23450;&#24773;&#20917;&#19979;&#25214;&#21040;&#26368;&#20248;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#35299;&#20915;&#40065;&#26834;&#24615;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#38382;&#39064;&#65292;&#21253;&#25324;&#19968;&#32452;&#20855;&#26377;&#19981;&#30830;&#23450;&#36716;&#31227;&#26680;&#30340;&#25240;&#25187;&#12289;&#26377;&#38480;&#29366;&#24577;&#12289;&#26377;&#38480;&#21160;&#20316;&#31354;&#38388;&#30340;MDP&#12290;&#35268;&#21010;&#30340;&#30446;&#26631;&#26159;&#25214;&#21040;&#19968;&#20010;&#40065;&#26834;&#31574;&#30053;&#26469;&#20248;&#21270;&#23545;&#36716;&#31227;&#19981;&#30830;&#23450;&#24615;&#30340;&#26368;&#22351;&#24773;&#20917;&#20540;&#65292;&#22240;&#27492;&#21253;&#25324;&#26631;&#20934;MDP&#35268;&#21010;&#20316;&#20026;&#19968;&#31181;&#29305;&#27530;&#24773;&#20917;&#12290;&#23545;&#20110;$(\mathbf{s},\mathbf{a})$-&#30697;&#24418;&#19981;&#30830;&#23450;&#38598;&#65292;&#26412;&#25991;&#24314;&#31435;&#20102;&#40065;&#26834;&#30446;&#26631;&#30340;&#20960;&#20010;&#32467;&#26500;&#24615;&#35266;&#23519;&#65292;&#20174;&#32780;&#20415;&#20110;&#24320;&#21457;&#22522;&#20110;&#31574;&#30053;&#30340;&#19968;&#38454;&#26041;&#27861;&#65292;&#21363;&#40065;&#26834;&#31574;&#30053;&#38236;&#20687;&#19979;&#38477;&#31639;&#27861;&#65288;RPMD&#65289;&#12290;&#20351;&#29992;&#32447;&#24615;&#36882;&#22686;&#30340;&#27493;&#38271;&#65292;&#24314;&#31435;&#20102;&#25214;&#21040;$\epsilon$-&#26368;&#20248;&#31574;&#30053;&#30340;$\mathcal{O}(\log(1/\epsilon))$&#36845;&#20195;&#22797;&#26434;&#24230;&#12290;&#24403;&#19968;&#38454;&#20449;&#24687;&#20165;&#36890;&#36807;&#19982;&#21517;&#20041;&#29615;&#22659;&#30340;&#22312;&#32447;&#20132;&#20114;&#33719;&#24471;&#26102;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#24320;&#21457;&#20102;&#40065;&#26834;&#31574;&#30053;&#38236;&#20687;&#19979;&#38477;&#26041;&#27861;&#30340;&#38543;&#26426;&#21464;&#20307;&#65292;&#21363;SRPMD&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#26368;&#20248;&#31574;&#30053;&#30340;&#21462;&#24471;&#26041;&#24335;&#19982;&#22522;&#20110;&#19968;&#38454;&#20449;&#24687;&#30340;&#26041;&#27861;&#26159;&#31561;&#20215;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of solving robust Markov decision process (MDP), which involves a set of discounted, finite state, finite action space MDPs with uncertain transition kernels. The goal of planning is to find a robust policy that optimizes the worst-case values against the transition uncertainties, and thus encompasses the standard MDP planning as a special case. For $(\mathbf{s},\mathbf{a})$-rectangular uncertainty sets, we establish several structural observations on the robust objective, which facilitates the development of a policy-based first-order method, namely the robust policy mirror descent (RPMD). An $\mathcal{O}(\log(1/\epsilon))$ iteration complexity for finding an $\epsilon$-optimal policy is established with linearly increasing stepsizes. We further develop a stochastic variant of the robust policy mirror descent method, named SRPMD, when the first-order information is only available through online interactions with the nominal environment. We show that the optimal
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#32467;&#26500;&#65292;&#21253;&#21547;&#31163;&#25955;&#29942;&#39048;&#65292;&#21487;&#20197;&#26377;&#25928;&#22788;&#29702;&#22312;&#22810;&#20010;&#20219;&#21153;&#20043;&#38388;&#36827;&#34892;&#36830;&#32493;&#23398;&#20064;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2207.11240</link><description>&lt;p&gt;
&#31163;&#25955;&#38190;&#20540;&#29942;&#39048;
&lt;/p&gt;
&lt;p&gt;
Discrete Key-Value Bottleneck. (arXiv:2207.11240v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.11240
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#32467;&#26500;&#65292;&#21253;&#21547;&#31163;&#25955;&#29942;&#39048;&#65292;&#21487;&#20197;&#26377;&#25928;&#22788;&#29702;&#22312;&#22810;&#20010;&#20219;&#21153;&#20043;&#38388;&#36827;&#34892;&#36830;&#32493;&#23398;&#20064;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;i.i.d.&#25968;&#25454;&#27969;&#21644;&#26631;&#27880;&#25968;&#25454;&#20016;&#23500;&#30340;&#20998;&#31867;&#20219;&#21153;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#23545;&#20110;&#36830;&#32493;&#23398;&#20064;&#31561;&#38750;&#24179;&#31283;&#35757;&#32451;&#25968;&#25454;&#27969;&#20250;&#20986;&#29616;&#25361;&#25112;&#12290;&#30446;&#21069;&#24050;&#26377;&#30340;&#19968;&#20010;&#26377;&#25928;&#26041;&#27861;&#26159;&#22312;&#22823;&#37327;&#21487;&#29992;&#25968;&#25454;&#19978;&#23545;&#32534;&#30721;&#22120;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#28982;&#21518;&#36827;&#34892;&#29305;&#23450;&#20219;&#21153;&#30340;&#24494;&#35843;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#26032;&#20219;&#21153;&#65292;&#26356;&#26032;&#36825;&#20123;&#32534;&#30721;&#22120;&#30340;&#26435;&#37325;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#38656;&#35201;&#24494;&#35843;&#22823;&#37327;&#30340;&#26435;&#37325;&#65292;&#24182;&#19988;&#20250;&#24536;&#35760;&#20808;&#21069;&#20219;&#21153;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22411;&#26550;&#26500;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#24314;&#31435;&#22312;&#21253;&#21547;&#25104;&#23545;&#20998;&#31163;&#21487;&#23398;&#20064;&#38190;&#20540;&#20195;&#30721;&#30340;&#31163;&#25955;&#29942;&#39048;&#30340;&#22522;&#30784;&#19978;&#12290;&#25105;&#20204;&#30340;&#33539;&#24335;&#26159;&#36827;&#34892;&#32534;&#30721;&#12289;&#36890;&#36807;&#31163;&#25955;&#29942;&#39048;&#36827;&#34892;&#34920;&#31034;&#22788;&#29702;&#12289;&#35299;&#30721;&#12290;&#22312;&#36825;&#37324;&#65292;&#36755;&#20837;&#34987;&#39304;&#36865;&#21040;&#39044;&#35757;&#32451;&#32534;&#30721;&#22120;&#65292;&#32534;&#30721;&#22120;&#30340;&#36755;&#20986;&#29992;&#20110;&#36873;&#25321;&#26368;&#36817;&#30340;&#38190;&#65292;&#24182;&#23558;&#30456;&#24212;&#30340;&#20540;&#39304;&#36865;&#21040;&#27491;&#22312;&#25191;&#34892;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks perform well on classification tasks where data streams are i.i.d. and labeled data is abundant. Challenges emerge with non-stationary training data streams such as continual learning. One powerful approach that has addressed this challenge involves pre-training of large encoders on volumes of readily available data, followed by task-specific tuning. Given a new task, however, updating the weights of these encoders is challenging as a large number of weights needs to be fine-tuned, and as a result, they forget information about the previous tasks. In the present work, we propose a model architecture to address this issue, building upon a discrete bottleneck containing pairs of separate and learnable key-value codes. Our paradigm will be to encode; process the representation via a discrete bottleneck; and decode. Here, the input is fed to the pre-trained encoder, the output of the encoder is used to select the nearest keys, and the corresponding values are fed to th
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#31163;&#32447;&#25968;&#25454;&#39044;&#35757;&#32451;&#40657;&#30418;&#20248;&#21270;&#22120;&#30340;&#29983;&#25104;&#26694;&#26550;BONET&#65292;&#20351;&#29992;&#33258;&#22238;&#24402;&#27169;&#22411;&#21644;&#26679;&#26412;&#31574;&#30053;&#21512;&#25104;&#36712;&#36857;&#20197;&#24110;&#21161;&#22312;&#39640;&#32500;&#31354;&#38388;&#20013;&#20248;&#21270;&#26114;&#36149;&#30340;&#40657;&#30418;&#20989;&#25968;&#12290;</title><link>http://arxiv.org/abs/2206.10786</link><description>&lt;p&gt;
&#29992;&#20110;&#40657;&#30418;&#20248;&#21270;&#30340;&#29983;&#25104;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Generative Pretraining for Black-Box Optimization. (arXiv:2206.10786v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.10786
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#31163;&#32447;&#25968;&#25454;&#39044;&#35757;&#32451;&#40657;&#30418;&#20248;&#21270;&#22120;&#30340;&#29983;&#25104;&#26694;&#26550;BONET&#65292;&#20351;&#29992;&#33258;&#22238;&#24402;&#27169;&#22411;&#21644;&#26679;&#26412;&#31574;&#30053;&#21512;&#25104;&#36712;&#36857;&#20197;&#24110;&#21161;&#22312;&#39640;&#32500;&#31354;&#38388;&#20013;&#20248;&#21270;&#26114;&#36149;&#30340;&#40657;&#30418;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31185;&#23398;&#21644;&#24037;&#31243;&#20013;&#30340;&#35768;&#22810;&#38382;&#39064;&#28041;&#21450;&#22312;&#39640;&#32500;&#31354;&#38388;&#20013;&#20248;&#21270;&#26114;&#36149;&#30340;&#40657;&#30418;&#20989;&#25968;&#12290;&#23545;&#20110;&#36825;&#26679;&#30340;&#40657;&#30418;&#20248;&#21270; (BBO) &#38382;&#39064;&#65292;&#25105;&#20204;&#36890;&#24120;&#20551;&#35774;&#22312;&#32447;&#20989;&#25968;&#35780;&#20272;&#30340;&#39044;&#31639;&#24456;&#23567;&#65292;&#20294;&#24448;&#24448;&#21487;&#20197;&#35775;&#38382;&#29992;&#20110;&#39044;&#35757;&#32451;&#30340;&#22266;&#23450;&#31163;&#32447;&#25968;&#25454;&#38598;&#12290;&#20043;&#21069;&#30340;&#26041;&#27861;&#35797;&#22270;&#21033;&#29992;&#31163;&#32447;&#25968;&#25454;&#26469;&#36924;&#36817;&#20989;&#25968;&#25110;&#20854;&#21453;&#20989;&#25968;&#65292;&#20294;&#22312;&#31163;&#25968;&#25454;&#20998;&#24067;&#36739;&#36828;&#26102;&#19981;&#22815;&#31934;&#30830;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;BONET&#65292;&#36825;&#26159;&#19968;&#20010;&#21033;&#29992;&#31163;&#32447;&#25968;&#25454;&#38598;&#39044;&#35757;&#32451;&#40657;&#30418;&#20248;&#21270;&#22120;&#30340;&#29983;&#25104;&#26694;&#26550;&#12290;&#22312;BONET&#20013;&#65292;&#25105;&#20204;&#23545;&#26469;&#33258;&#31163;&#32447;&#25968;&#25454;&#38598;&#30340;&#23450;&#38271;&#36712;&#36857;&#35757;&#32451;&#19968;&#20010;&#33258;&#22238;&#24402;&#27169;&#22411;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#37319;&#26679;&#31574;&#30053;&#65292;&#20351;&#29992;&#20174;&#20302;&#20445;&#30495;&#24230;&#26679;&#26412;&#21040;&#39640;&#20445;&#30495;&#24230;&#26679;&#26412;&#30340;&#21333;&#35843;&#36716;&#25442;&#30340;&#31616;&#21333;&#21551;&#21457;&#24335;&#26469;&#21512;&#25104;&#26469;&#33258;&#31163;&#32447;&#25968;&#25454;&#30340;&#36712;&#36857;&#12290;&#22312;Design-Bench&#19978;&#20351;&#29992;&#34987;&#22240;&#26524;&#25513;&#34109;&#30340;Transformer&#23454;&#20363;&#21270;BONET&#65292;&#24182;&#36827;&#34892;&#35780;&#20272;&#65292;&#25105;&#20204;&#22312;&#24179;&#22343;&#25490;&#21517;&#19978;&#25490;&#21517;&#31532;&#19968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many problems in science and engineering involve optimizing an expensive black-box function over a high-dimensional space. For such black-box optimization (BBO) problems, we typically assume a small budget for online function evaluations, but also often have access to a fixed, offline dataset for pretraining. Prior approaches seek to utilize the offline data to approximate the function or its inverse but are not sufficiently accurate far from the data distribution. We propose BONET, a generative framework for pretraining a novel black-box optimizer using offline datasets. In BONET, we train an autoregressive model on fixed-length trajectories derived from an offline dataset. We design a sampling strategy to synthesize trajectories from offline data using a simple heuristic of rolling out monotonic transitions from low-fidelity to high-fidelity samples. Empirically, we instantiate BONET using a causally masked Transformer and evaluate it on Design-Bench, where we rank the best on averag
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;Beyond the Imitation Game&#22522;&#20934;&#27979;&#35797;&#65288;BIG-bench&#65289;&#65292;&#35813;&#27979;&#35797;&#38598;&#21253;&#21547;&#20102;204&#20010;&#21508;&#39046;&#22495;&#30340;&#38590;&#39064;&#65292;&#26088;&#22312;&#35780;&#20272;&#24403;&#21069;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#24182;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#25552;&#20379;&#20449;&#24687;&#21644;&#20934;&#22791;&#12290;</title><link>http://arxiv.org/abs/2206.04615</link><description>&lt;p&gt;
&#36229;&#36234;&#27169;&#20223;&#28216;&#25103;&#65306;&#37327;&#21270;&#21644;&#25299;&#23637;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models. (arXiv:2206.04615v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.04615
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;Beyond the Imitation Game&#22522;&#20934;&#27979;&#35797;&#65288;BIG-bench&#65289;&#65292;&#35813;&#27979;&#35797;&#38598;&#21253;&#21547;&#20102;204&#20010;&#21508;&#39046;&#22495;&#30340;&#38590;&#39064;&#65292;&#26088;&#22312;&#35780;&#20272;&#24403;&#21069;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#24182;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#25552;&#20379;&#20449;&#24687;&#21644;&#20934;&#22791;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#35268;&#27169;&#30340;&#22686;&#22823;&#65292;&#35821;&#35328;&#27169;&#22411;&#23637;&#31034;&#20102;&#25968;&#37327;&#19978;&#30340;&#25552;&#21319;&#21644;&#26032;&#30340;&#23450;&#24615;&#33021;&#21147;&#12290;&#23613;&#31649;&#20855;&#26377;&#28508;&#22312;&#30340;&#36716;&#21464;&#24615;&#24433;&#21709;&#65292;&#20294;&#36825;&#20123;&#26032;&#30340;&#33021;&#21147;&#30446;&#21069;&#23578;&#26410;&#34987;&#20805;&#20998;&#25551;&#36848;&#12290;&#20026;&#20102;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#25552;&#20379;&#20449;&#24687;&#65292;&#20026;&#21095;&#21464;&#30340;&#26032;&#22411;&#27169;&#22411;&#33021;&#21147;&#20570;&#20934;&#22791;&#65292;&#24182;&#32531;&#35299;&#31038;&#20250;&#26377;&#23475;&#24433;&#21709;&#65292;&#25105;&#20204;&#24517;&#39035;&#20102;&#35299;&#35821;&#35328;&#27169;&#22411;&#30340;&#29616;&#26377;&#21644;&#36817;&#26399;&#33021;&#21147;&#21644;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Beyond the Imitation Game&#22522;&#20934;&#27979;&#35797;&#65288;BIG-bench&#65289;&#12290;BIG-bench&#30446;&#21069;&#21253;&#25324;204&#20010;&#20219;&#21153;&#65292;&#30001;132&#20010;&#26426;&#26500;&#30340;450&#21517;&#20316;&#32773;&#36129;&#29486;&#12290;&#20219;&#21153;&#20027;&#39064;&#22810;&#26679;&#65292;&#28085;&#30422;&#20102;&#35821;&#35328;&#23398;&#12289;&#20799;&#31461;&#21457;&#23637;&#12289;&#25968;&#23398;&#12289;&#24120;&#35782;&#25512;&#29702;&#12289;&#29983;&#29289;&#23398;&#12289;&#29289;&#29702;&#23398;&#12289;&#31038;&#20250;&#20559;&#35265;&#12289;&#36719;&#20214;&#24320;&#21457;&#31561;&#31561;&#12290;BIG-bench&#19987;&#27880;&#20110;&#37027;&#20123;&#34987;&#35748;&#20026;&#36229;&#20986;&#20102;&#24403;&#21069;&#35821;&#35328;&#27169;&#22411;&#33021;&#21147;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;OpenAI&#30340;GPT&#27169;&#22411;&#21644;&#35895;&#27468;&#20869;&#37096;&#30340;&#23494;&#38598;&#36716;&#25442;&#27169;&#22411;&#30340;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models demonstrate both quantitative improvement and new qualitative capabilities with increasing scale. Despite their potentially transformative impact, these new capabilities are as yet poorly characterized. In order to inform future research, prepare for disruptive new model capabilities, and ameliorate socially harmful effects, it is vital that we understand the present and near-future capabilities and limitations of language models. To address this challenge, we introduce the Beyond the Imitation Game benchmark (BIG-bench). BIG-bench currently consists of 204 tasks, contributed by 450 authors across 132 institutions. Task topics are diverse, drawing problems from linguistics, childhood development, math, common-sense reasoning, biology, physics, social bias, software development, and beyond. BIG-bench focuses on tasks that are believed to be beyond the capabilities of current language models. We evaluate the behavior of OpenAI's GPT models, Google-internal dense transform
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#36328;&#35270;&#22270;&#35821;&#35328;&#24314;&#27169;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#23558;&#36328;&#35821;&#35328;&#21644;&#36328;&#27169;&#24577;&#39044;&#35757;&#32451;&#32479;&#19968;&#22312;&#20849;&#20139;&#30340;&#26550;&#26500;&#21644;&#30446;&#26631;&#19979;&#36827;&#34892;&#65292;&#36890;&#36807;&#26377;&#26465;&#20214;&#30340;&#25513;&#30721;&#35821;&#35328;&#24314;&#27169;&#21644;&#23545;&#27604;&#23398;&#20064;&#26469;&#26368;&#22823;&#21270;&#19981;&#21516;&#35270;&#22270;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#20197;&#23454;&#29616;&#20004;&#20010;&#35270;&#22270;&#30340;&#23545;&#40784;&#12290;</title><link>http://arxiv.org/abs/2206.00621</link><description>&lt;p&gt;
&#36328;&#35270;&#22270;&#35821;&#35328;&#24314;&#27169;&#65306;&#36808;&#21521;&#32479;&#19968;&#30340;&#36328;&#35821;&#35328;&#36328;&#27169;&#24577;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Cross-View Language Modeling: Towards Unified Cross-Lingual Cross-Modal Pre-training. (arXiv:2206.00621v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.00621
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#36328;&#35270;&#22270;&#35821;&#35328;&#24314;&#27169;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#23558;&#36328;&#35821;&#35328;&#21644;&#36328;&#27169;&#24577;&#39044;&#35757;&#32451;&#32479;&#19968;&#22312;&#20849;&#20139;&#30340;&#26550;&#26500;&#21644;&#30446;&#26631;&#19979;&#36827;&#34892;&#65292;&#36890;&#36807;&#26377;&#26465;&#20214;&#30340;&#25513;&#30721;&#35821;&#35328;&#24314;&#27169;&#21644;&#23545;&#27604;&#23398;&#20064;&#26469;&#26368;&#22823;&#21270;&#19981;&#21516;&#35270;&#22270;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#20197;&#23454;&#29616;&#20004;&#20010;&#35270;&#22270;&#30340;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#36328;&#35270;&#22270;&#35821;&#35328;&#24314;&#27169;&#65292;&#36825;&#26159;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#39044;&#35757;&#32451;&#26694;&#26550;&#65292;&#23558;&#36328;&#35821;&#35328;&#21644;&#36328;&#27169;&#24577;&#39044;&#35757;&#32451;&#19982;&#20849;&#20139;&#26550;&#26500;&#21644;&#30446;&#26631;&#32479;&#19968;&#36215;&#26469;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21463;&#21040;&#19968;&#20010;&#20851;&#38190;&#35266;&#23519;&#30340;&#21551;&#21457;&#65292;&#21363;&#36328;&#35821;&#35328;&#21644;&#36328;&#27169;&#24577;&#39044;&#35757;&#32451;&#20855;&#26377;&#23558;&#21516;&#19968;&#23545;&#35937;&#30340;&#20004;&#20010;&#19981;&#21516;&#35270;&#22270;&#23545;&#40784;&#21040;&#19968;&#20010;&#20849;&#21516;&#35821;&#20041;&#31354;&#38388;&#30340;&#30456;&#21516;&#30446;&#26631;&#12290;&#20026;&#27492;&#65292;&#36328;&#35270;&#22270;&#35821;&#35328;&#24314;&#27169;&#26694;&#26550;&#23558;&#22810;&#27169;&#24577;&#25968;&#25454;&#65288;&#21363;&#22270;&#20687;-&#26631;&#39064;&#23545;&#65289;&#21644;&#22810;&#35821;&#35328;&#25968;&#25454;&#65288;&#21363;&#24179;&#34892;&#21477;&#23545;&#65289;&#35270;&#20026;&#21516;&#19968;&#23545;&#35937;&#30340;&#20004;&#20010;&#19981;&#21516;&#35270;&#22270;&#65292;&#24182;&#36890;&#36807;&#26377;&#26465;&#20214;&#30340;&#25513;&#30721;&#35821;&#35328;&#24314;&#27169;&#21644;&#23545;&#27604;&#23398;&#20064;&#26469;&#26368;&#22823;&#21270;&#23427;&#20204;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#26469;&#35757;&#32451;&#27169;&#22411;&#65292;&#20197;&#23545;&#40784;&#20004;&#20010;&#35270;&#22270;&#12290;&#25105;&#20204;&#20351;&#29992;&#36328;&#35270;&#22270;&#35821;&#35328;&#24314;&#27169;&#26694;&#26550;&#23545;&#36328;&#35821;&#35328;&#36328;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;CCLM&#36827;&#34892;&#39044;&#35757;&#32451;&#12290;&#22312;&#22810;&#35821;&#35328;&#22810;&#27169;&#24577;&#22522;&#20934;IGLUE&#21644;&#20004;&#20010;&#22810;&#35821;&#35328;&#22270;&#20687;-&#25991;&#26412;&#26816;&#32034;&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#23454;&#35777;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce Cross-View Language Modeling, a simple and effective pre-training framework that unifies cross-lingual and cross-modal pre-training with shared architectures and objectives. Our approach is motivated by a key observation that cross-lingual and cross-modal pre-training share the same goal of aligning two different views of the same object into a common semantic space. To this end, the cross-view language modeling framework considers both multi-modal data (i.e., image-caption pairs) and multi-lingual data (i.e., parallel sentence pairs) as two different views of the same object, and trains the model to align the two views by maximizing the mutual information between them with conditional masked language modeling and contrastive learning. We pre-train CCLM, a Cross-lingual Cross-modal Language Model, with the cross-view language modeling framework. Empirical results on IGLUE, a multi-lingual multi-modal benchmark, and two multi-lingual image-text retrieval data
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;QUIC-FL&#26041;&#27861;&#65292;&#21363;&#38754;&#21521;&#32852;&#37030;&#23398;&#20064;&#30340;&#24555;&#36895;&#26080;&#20559;&#21387;&#32553;&#65292;&#36890;&#36807;&#25913;&#36827;DME&#25216;&#26415;&#23454;&#29616;&#20102;&#26368;&#20248;&#24402;&#19968;&#21270;&#22343;&#26041;&#35823;&#24046;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2205.13341</link><description>&lt;p&gt;
QUIC-FL&#65306;&#38754;&#21521;&#32852;&#37030;&#23398;&#20064;&#30340;&#24555;&#36895;&#26080;&#20559;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
QUIC-FL: Quick Unbiased Compression for Federated Learning. (arXiv:2205.13341v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.13341
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;QUIC-FL&#26041;&#27861;&#65292;&#21363;&#38754;&#21521;&#32852;&#37030;&#23398;&#20064;&#30340;&#24555;&#36895;&#26080;&#20559;&#21387;&#32553;&#65292;&#36890;&#36807;&#25913;&#36827;DME&#25216;&#26415;&#23454;&#29616;&#20102;&#26368;&#20248;&#24402;&#19968;&#21270;&#22343;&#26041;&#35823;&#24046;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#24067;&#24335;&#22343;&#20540;&#20272;&#35745;&#65288;DME&#65289;&#26159;&#36890;&#20449;&#39640;&#25928;&#30340;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#22522;&#26412;&#26500;&#24314;&#22359;&#65292;&#22312;&#20854;&#20013;$n$&#20010;&#23458;&#25143;&#31471;&#21521;&#21442;&#25968;&#26381;&#21153;&#22120;&#36890;&#20449;&#21521;&#37327;&#65292;&#21442;&#25968;&#26381;&#21153;&#22120;&#20272;&#31639;&#20854;&#24179;&#22343;&#20540;&#12290;&#26412;&#25991;&#25913;&#36827;&#20102;&#20808;&#21069;&#30340;DME&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#26368;&#20248;$O(1/n)$&#30340;&#24402;&#19968;&#21270;&#22343;&#26041;&#35823;&#24046;&#65288;NMSE&#65289;&#20445;&#35777;&#65292;&#36890;&#36807;&#28176;&#36827;&#25913;&#36827;&#32534;&#30721;&#25110;&#35299;&#30721;&#65288;&#25110;&#20004;&#32773;&#65289;&#30340;&#22797;&#26434;&#24230;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#20197;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#24335;&#24418;&#24335;&#21270;&#20102;&#38382;&#39064;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#20351;&#29992;&#29616;&#25104;&#30340;&#25968;&#23398;&#27714;&#35299;&#22120;&#26469;&#35774;&#35745;&#37327;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Distributed Mean Estimation (DME), in which $n$ clients communicate vectors to a parameter server that estimates their average, is a fundamental building block in communication-efficient federated learning. In this paper, we improve on previous DME techniques that achieve the optimal $O(1/n)$ Normalized Mean Squared Error (NMSE) guarantee by asymptotically improving the complexity for either encoding or decoding (or both). To achieve this, we formalize the problem in a novel way that allows us to use off-the-shelf mathematical solvers to design the quantization.
&lt;/p&gt;</description></item><item><title>SE-MoE&#25552;&#20986;&#20102;&#19968;&#31181;&#24377;&#24615;&#30340;&#35757;&#32451;&#26041;&#24335;&#21644;&#19981;&#21516;&#20248;&#21270;&#25514;&#26045;&#65292;&#20197;&#25552;&#39640;&#28151;&#21512;&#19987;&#23478;&#27169;&#22411;&#30340;&#20998;&#24067;&#24335;&#35757;&#32451;&#21644;&#25512;&#29702;&#25928;&#29575;&#65292;&#35299;&#20915;&#20102;&#36127;&#36733;&#24179;&#34913;&#12289;&#36890;&#20449;/&#35745;&#31639;&#25928;&#29575;&#21644;&#20869;&#23384;&#38480;&#21046;&#31561;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2205.10034</link><description>&lt;p&gt;
SE-MoE: &#19968;&#31181;&#21487;&#25193;&#23637;&#39640;&#25928;&#30340;&#20998;&#24067;&#24335;&#35757;&#32451;&#21644;&#25512;&#29702;&#28151;&#21512;&#19987;&#23478;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
SE-MoE: A Scalable and Efficient Mixture-of-Experts Distributed Training and Inference System. (arXiv:2205.10034v2 [cs.DC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.10034
&lt;/p&gt;
&lt;p&gt;
SE-MoE&#25552;&#20986;&#20102;&#19968;&#31181;&#24377;&#24615;&#30340;&#35757;&#32451;&#26041;&#24335;&#21644;&#19981;&#21516;&#20248;&#21270;&#25514;&#26045;&#65292;&#20197;&#25552;&#39640;&#28151;&#21512;&#19987;&#23478;&#27169;&#22411;&#30340;&#20998;&#24067;&#24335;&#35757;&#32451;&#21644;&#25512;&#29702;&#25928;&#29575;&#65292;&#35299;&#20915;&#20102;&#36127;&#36733;&#24179;&#34913;&#12289;&#36890;&#20449;/&#35745;&#31639;&#25928;&#29575;&#21644;&#20869;&#23384;&#38480;&#21046;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#26550;&#26500;&#30340;&#22810;&#26679;&#24615;&#22686;&#21152;&#65292;&#23558;&#27169;&#22411;&#20998;&#24067;&#24335;&#35757;&#32451;&#22312;&#24322;&#26500;&#35745;&#31639;&#31995;&#32479;&#19978;&#20197;&#26041;&#20415;&#29983;&#25104;&#22823;&#27169;&#22411;&#25104;&#20026;&#20102;&#19968;&#31181;&#36235;&#21183;&#12290;&#28151;&#21512;&#19987;&#23478;&#27169;&#22411;&#21033;&#29992;&#20998;&#27835;&#31574;&#30053;&#36890;&#36807;&#38376;&#25511;&#21644;&#24182;&#34892;&#22788;&#29702;&#26041;&#24335;&#26469;&#38477;&#20302;&#35757;&#32451;&#25104;&#26412;&#24182;&#25511;&#21046;&#24635;&#20307;&#27169;&#22411;/&#25968;&#25454;&#22823;&#23567;&#12290;&#34429;&#28982; DeepSpeed &#22312;&#36827;&#34892;&#22823;&#35268;&#27169; MoE &#35757;&#32451;&#19978;&#36827;&#34892;&#20102;&#23581;&#35797;&#65292;&#20294;&#35757;&#32451;&#21644;&#25512;&#29702;&#30340;&#25928;&#29575;&#20173;&#26377;&#25552;&#21319;&#30340;&#31354;&#38388;&#65292;&#21253;&#25324;&#36127;&#36733;&#24179;&#34913;&#12289;&#36890;&#20449;/&#35745;&#31639;&#25928;&#29575;&#21644;&#20869;&#23384;&#38480;&#21046;&#31561;&#26041;&#38754;&#12290;&#26412;&#24037;&#20316;&#25552;&#20986;&#20102; SE-MoE&#65292;&#20351;&#29992;&#24377;&#24615; MoE &#35757;&#32451;&#12289;&#22522;&#20110;&#23618;&#27425;&#23384;&#20648;&#30340; 2D &#39044;&#21462;&#21644;&#34701;&#21512;&#36890;&#20449;&#31561;&#26041;&#27861;&#65292;&#20197;&#20415;&#22312;&#19981;&#21516;&#30340;&#31867;&#22411;&#20013;&#33719;&#24471;&#39640;&#25928;&#30340;&#24182;&#34892;&#22788;&#29702;&#12290;&#38024;&#23545;&#21333;&#33410;&#28857;&#30340;&#21487;&#25193;&#23637;&#25512;&#29702;&#65292;&#29305;&#21035;&#26159;&#24403;&#27169;&#22411;&#22823;&#23567;&#22823;&#20110; GPU &#20869;&#23384;&#26102;&#65292;SE-MoE &#23558; CPU-GPU &#23384;&#20648;&#32852;&#21512;&#24418;&#25104;&#19968;&#20010;&#26356;&#22823;&#30340;&#23384;&#20648;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the increasing diversity of ML infrastructures nowadays, distributed training over heterogeneous computing systems is desired to facilitate the production of big models. Mixture-of-Experts (MoE) models have been proposed to lower the cost of training subject to the overall size of models/data through gating and parallelism in a divide-and-conquer fashion. While DeepSpeed has made efforts in carrying out large-scale MoE training over heterogeneous infrastructures, the efficiency of training and inference could be further improved from several system aspects, including load balancing, communication/computation efficiency, and memory footprint limits. In this work, we present SE-MoE that proposes Elastic MoE training with 2D prefetch and Fusion communication over Hierarchical storage, so as to enjoy efficient parallelisms in various types. For scalable inference in a single node, especially when the model size is larger than GPU memory, SE-MoE forms the CPU-GPU memory jointly into a 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#35821;&#20041;&#36890;&#20449;&#31995;&#32479;SINFONY&#65292;&#23427;&#36890;&#36807;&#23545;&#28040;&#24687;&#36827;&#34892;&#25968;&#25454;&#20943;&#23569;&#21644;&#21487;&#38752;&#20256;&#36755;&#26469;&#26368;&#22909;&#22320;&#20445;&#30041;&#35821;&#20041;&#65292;&#20174;&#32780;&#23454;&#29616;&#26080;&#32447;&#32593;&#32476;&#20013;&#30340;&#35821;&#20041;&#20449;&#24687;&#24674;&#22797;&#12290;</title><link>http://arxiv.org/abs/2204.13366</link><description>&lt;p&gt;
&#26080;&#32447;&#32593;&#32476;&#20013;&#30340;&#35821;&#20041;&#20449;&#24687;&#24674;&#22797;
&lt;/p&gt;
&lt;p&gt;
Semantic Information Recovery in Wireless Networks. (arXiv:2204.13366v4 [cs.IT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.13366
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#35821;&#20041;&#36890;&#20449;&#31995;&#32479;SINFONY&#65292;&#23427;&#36890;&#36807;&#23545;&#28040;&#24687;&#36827;&#34892;&#25968;&#25454;&#20943;&#23569;&#21644;&#21487;&#38752;&#20256;&#36755;&#26469;&#26368;&#22909;&#22320;&#20445;&#30041;&#35821;&#20041;&#65292;&#20174;&#32780;&#23454;&#29616;&#26080;&#32447;&#32593;&#32476;&#20013;&#30340;&#35821;&#20041;&#20449;&#24687;&#24674;&#22797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#26426;&#22120;&#23398;&#20064;&#24037;&#20855;&#22312;&#26080;&#32447;&#36890;&#20449;&#20013;&#30340;&#25104;&#21151;&#21551;&#21457;&#65292;1949&#24180;Weaver&#25552;&#20986;&#30340;&#35821;&#20041;&#36890;&#20449;&#24605;&#24819;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#20851;&#27880;&#12290; &#23427;&#25171;&#30772;&#20102;Shannon&#30340;&#32463;&#20856;&#35774;&#35745;&#33539;&#20363;&#65292;&#26088;&#22312;&#20256;&#36882;&#28040;&#24687;&#30340;&#21547;&#20041;&#65292;&#21363;&#35821;&#20041;&#65292;&#32780;&#19981;&#26159;&#20854;&#30830;&#20999;&#29256;&#26412;&#65292;&#20174;&#32780;&#20801;&#35768;&#33410;&#30465;&#20449;&#24687;&#36895;&#29575;&#12290; &#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23558;Basu&#31561;&#20154;&#30340;&#24314;&#27169;&#35821;&#20041;&#30340;&#22522;&#26412;&#26041;&#27861;&#25193;&#23637;&#21040;&#23436;&#25972;&#36890;&#20449;&#39532;&#23572;&#21487;&#22827;&#38142;&#12290; &#22240;&#27492;&#65292;&#25105;&#20204;&#36890;&#36807;&#38544;&#21547;&#30340;&#38543;&#26426;&#21464;&#37327;&#26469;&#24314;&#27169;&#35821;&#20041;&#65292;&#24182;&#23558;&#35821;&#20041;&#36890;&#20449;&#20219;&#21153;&#23450;&#20041;&#20026;&#36890;&#36807;&#36890;&#20449;&#20449;&#36947;&#23545;&#28040;&#24687;&#36827;&#34892;&#25968;&#25454;&#20943;&#23569;&#21644;&#21487;&#38752;&#20256;&#36755;&#65292;&#20174;&#32780;&#26368;&#22909;&#22320;&#20445;&#30041;&#35821;&#20041;&#12290; &#25105;&#20204;&#23558;&#27492;&#20219;&#21153;&#20316;&#20026;&#31471;&#21040;&#31471;&#20449;&#24687;&#29942;&#39048;&#38382;&#39064;&#36827;&#34892;&#24314;&#27169;&#65292;&#20801;&#35768;&#22312;&#20445;&#30041;&#30456;&#20851;&#20449;&#24687;&#30340;&#21516;&#26102;&#36827;&#34892;&#21387;&#32553;&#12290; &#20316;&#20026;&#35299;&#20915;&#26041;&#26696;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;ML&#30340;&#35821;&#20041;&#36890;&#20449;&#31995;&#32479;SINFONY&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;&#20998;&#24067;&#24335;&#22810;&#28857;&#22330;&#26223;&#65306;SIN&#12290;
&lt;/p&gt;
&lt;p&gt;
Motivated by the recent success of Machine Learning (ML) tools in wireless communications, the idea of semantic communication by Weaver from 1949 has gained attention. It breaks with Shannon's classic design paradigm by aiming to transmit the meaning of a message, i.e., semantics, rather than its exact version and thus allows for savings in information rate. In this work, we extend the fundamental approach from Basu et al. for modeling semantics to the complete communications Markov chain. Thus, we model semantics by means of hidden random variables and define the semantic communication task as the data-reduced and reliable transmission of messages over a communication channel such that semantics is best preserved. We cast this task as an end-to-end Information Bottleneck problem, allowing for compression while preserving relevant information most. As a solution approach, we propose the ML-based semantic communication system SINFONY and use it for a distributed multipoint scenario: SIN
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#26368;&#36817;&#31070;&#32463;&#25991;&#26412;&#29983;&#25104;&#39046;&#22495;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#21253;&#25324;&#25968;&#25454;&#26500;&#24314;&#12289;&#31070;&#32463;&#26694;&#26550;&#12289;&#35757;&#32451;&#21644;&#25512;&#29702;&#31574;&#30053;&#21644;&#35780;&#20272;&#25351;&#26631;&#31561;&#22235;&#20010;&#26041;&#38754;&#65292;&#24182;&#25506;&#35752;&#20102;&#31070;&#32463;&#31649;&#36947;&#21644;&#32972;&#26223;&#30693;&#35782;&#30340;&#21033;&#29992;&#31561;&#26410;&#26469;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2203.03047</link><description>&lt;p&gt;
&#31070;&#32463;&#25991;&#26412;&#29983;&#25104;&#30340;&#26368;&#26032;&#36827;&#23637;&#65306;&#19968;&#39033;&#20219;&#21153;&#26080;&#20851;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Recent Advances in Neural Text Generation: A Task-Agnostic Survey. (arXiv:2203.03047v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.03047
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#26368;&#36817;&#31070;&#32463;&#25991;&#26412;&#29983;&#25104;&#39046;&#22495;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#21253;&#25324;&#25968;&#25454;&#26500;&#24314;&#12289;&#31070;&#32463;&#26694;&#26550;&#12289;&#35757;&#32451;&#21644;&#25512;&#29702;&#31574;&#30053;&#21644;&#35780;&#20272;&#25351;&#26631;&#31561;&#22235;&#20010;&#26041;&#38754;&#65292;&#24182;&#25506;&#35752;&#20102;&#31070;&#32463;&#31649;&#36947;&#21644;&#32972;&#26223;&#30693;&#35782;&#30340;&#21033;&#29992;&#31561;&#26410;&#26469;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#30456;&#24403;&#22810;&#30340;&#30740;&#31350;&#33268;&#21147;&#20110;&#22312;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#65288;NLG&#65289;&#39046;&#22495;&#20013;&#24212;&#29992;&#31070;&#32463;&#27169;&#22411;&#12290;&#20027;&#35201;&#30446;&#26631;&#26159;&#29983;&#25104;&#26082;&#20855;&#26377;&#35821;&#35328;&#33258;&#28982;&#24615;&#21448;&#20855;&#26377;&#20154;&#31867;&#21270;&#23646;&#24615;&#30340;&#25991;&#26412;&#65292;&#24182;&#21516;&#26102;&#23545;&#29983;&#25104;&#36807;&#31243;&#36827;&#34892;&#25511;&#21046;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20221;&#20840;&#38754;&#30340;&#65292;&#20219;&#21153;&#26080;&#20851;&#30340;&#31070;&#32463;&#25991;&#26412;&#29983;&#25104;&#26368;&#26032;&#36827;&#23637;&#35843;&#26597;&#12290;&#36825;&#20123;&#36827;&#23637;&#36890;&#36807;&#22810;&#31181;&#21457;&#23637;&#24471;&#20197;&#23454;&#29616;&#65292;&#25105;&#20204;&#23558;&#20854;&#20998;&#20026;&#22235;&#20010;&#20027;&#35201;&#26041;&#38754;&#65306;&#25968;&#25454;&#26500;&#24314;&#65292;&#31070;&#32463;&#26694;&#26550;&#65292;&#35757;&#32451;&#21644;&#25512;&#29702;&#31574;&#30053;&#21644;&#35780;&#20272;&#25351;&#26631;&#12290;&#36890;&#36807;&#30740;&#31350;&#36825;&#20123;&#19981;&#21516;&#26041;&#38754;&#65292;&#25105;&#20204;&#26088;&#22312;&#25552;&#20379;&#23545;&#35813;&#39046;&#22495;&#30340;&#36827;&#23637;&#30340;&#20840;&#38754;&#27010;&#36848;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#31070;&#32463;&#25991;&#26412;&#29983;&#25104;&#30340;&#26410;&#26469;&#26041;&#21521;&#65292;&#36825;&#20123;&#26041;&#21521;&#21253;&#25324;&#21033;&#29992;&#31070;&#32463;&#31649;&#36947;&#21644;&#34701;&#21512;&#32972;&#26223;&#30693;&#35782;&#65292;&#36825;&#20123;&#36884;&#24452;&#20026;&#36827;&#19968;&#27493;&#22686;&#24378;&#31070;&#32463;&#25991;&#26412;&#29983;&#25104;&#31995;&#32479;&#30340;&#33021;&#21147;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, considerable research has been dedicated to the application of neural models in the field of natural language generation (NLG). The primary objective is to generate text that is both linguistically natural and human-like, while also exerting control over the generation process. This paper offers a comprehensive and task-agnostic survey of the recent advancements in neural text generation. These advancements have been facilitated through a multitude of developments, which we categorize into four key areas: data construction, neural frameworks, training and inference strategies, and evaluation metrics. By examining these different aspects, we aim to provide a holistic overview of the progress made in the field. Furthermore, we explore the future directions for the advancement of neural text generation, which encompass the utilization of neural pipelines and the incorporation of background knowledge. These avenues present promising opportunities to further enhance the cap
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#21322;&#38543;&#26426;&#27169;&#22411;&#20013;&#65292;&#32852;&#30431;&#22823;&#23567;&#20219;&#24847;&#19988;&#21487;&#21464;&#24773;&#20917;&#19979;&#65292;&#36827;&#34892;&#25805;&#32437;&#12289;&#32988;&#21033;&#36793;&#32536;&#21644;&#36873;&#31080;&#25511;&#21046;&#31561;&#24433;&#21709;&#22823;&#36873;&#30340;&#21487;&#33021;&#24615;&#12290;&#20027;&#35201;&#23450;&#29702;&#25552;&#20379;&#20102;&#36873;&#20030;&#32467;&#26524;&#21463; $s$ &#20010;&#32852;&#30431;&#24433;&#21709;&#30340;&#19978;&#19979;&#30028;&#12290;</title><link>http://arxiv.org/abs/2202.06411</link><description>&lt;p&gt;
&#32852;&#30431;&#30340;&#24433;&#21709;&#65306;&#35780;&#20272;&#22823;&#36873;&#20013;&#36873;&#27665;&#24433;&#21709;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Impact of a Coalition: Assessing the Likelihood of Voter Influence in Large Elections. (arXiv:2202.06411v4 [econ.TH] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.06411
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#21322;&#38543;&#26426;&#27169;&#22411;&#20013;&#65292;&#32852;&#30431;&#22823;&#23567;&#20219;&#24847;&#19988;&#21487;&#21464;&#24773;&#20917;&#19979;&#65292;&#36827;&#34892;&#25805;&#32437;&#12289;&#32988;&#21033;&#36793;&#32536;&#21644;&#36873;&#31080;&#25511;&#21046;&#31561;&#24433;&#21709;&#22823;&#36873;&#30340;&#21487;&#33021;&#24615;&#12290;&#20027;&#35201;&#23450;&#29702;&#25552;&#20379;&#20102;&#36873;&#20030;&#32467;&#26524;&#21463; $s$ &#20010;&#32852;&#30431;&#24433;&#21709;&#30340;&#19978;&#19979;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20960;&#20010;&#19990;&#32426;&#20197;&#26469;&#65292;&#20154;&#20204;&#26222;&#36941;&#35748;&#20026;&#23567;&#32852;&#30431;&#36873;&#27665;&#30340;&#24433;&#21709;&#22312;&#22823;&#36873;&#20013;&#24494;&#19981;&#36275;&#36947;&#12290;&#22240;&#27492;&#65292;&#26377;&#22823;&#37327;&#25991;&#29486;&#25551;&#36848;&#20102;&#29305;&#23450;&#20998;&#24067;&#19979;&#36873;&#20030;&#21487;&#33021;&#20250;&#34987;&#24433;&#21709;&#30340;&#21487;&#33021;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312; i.i.d. &#22343;&#21248;&#20998;&#24067;&#19979;&#19968;&#20010;&#36873;&#27665;&#36827;&#34892;&#25805;&#32437;&#30340;&#21487;&#33021;&#24615;&#65292;&#31216;&#20026;&#20844;&#27491;&#25991;&#21270;&#65288;IC&#65289;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20174;&#19977;&#20010;&#26041;&#38754;&#25193;&#23637;&#20102;&#20808;&#21069;&#30340;&#30740;&#31350;&#65306;&#65288;1&#65289;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#19968;&#33324;&#30340;&#21322;&#38543;&#26426;&#27169;&#22411;&#65292;&#20854;&#20013;&#20998;&#24067;&#23545;&#25163;&#36873;&#25321;&#26368;&#22351;&#24773;&#20917;&#20998;&#24067;&#65292;&#28982;&#21518;&#27745;&#26579;&#23545;&#25163;&#20462;&#25913;&#39640;&#36798; $ \psi $ &#25968;&#25454;&#30340;&#19968;&#37096;&#20998;&#65292;&#65288;2&#65289;&#25105;&#20204;&#32771;&#34385;&#20102;&#35768;&#22810;&#32852;&#30431;&#24433;&#21709;&#38382;&#39064;&#65292;&#21253;&#25324;&#32852;&#30431;&#25805;&#32437;&#65292;&#32988;&#21033;&#36793;&#32536;&#21644;&#21508;&#31181;&#36873;&#31080;&#25511;&#21046;&#21644;&#36159;&#36162;&#65292;&#65288;3&#65289;&#25105;&#20204;&#32771;&#34385;&#20219;&#24847;&#21644;&#21487;&#21464;&#32852;&#30431;&#35268;&#27169; $B$&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#23450;&#29702;&#25552;&#20379;&#20102;&#21322;&#38543;&#26426;&#21487;&#33021;&#24615;&#23384;&#22312;&#19968;&#32452;&#22823;&#23567;&#20026; $s$ &#30340;&#32852;&#30431;&#23545;&#36873;&#20030;&#32467;&#26524;&#26045;&#21152;&#24433;&#21709;&#30340;&#32039;&#23494;&#19978;&#19979;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
For centuries, it has been widely believed that the influence of a small coalition of voters is negligible in a large election. Consequently, there is a large body of literature on characterizing the likelihood for an election to be influenced when the votes follow certain distributions, especially the likelihood of being manipulable by a single voter under the i.i.d. uniform distribution, known as the Impartial Culture (IC).  In this paper, we extend previous studies in three aspects: (1) we propose a more general semi-random model, where a distribution adversary chooses a worst-case distribution and then a contamination adversary modifies up to $\psi$ portion of the data, (2) we consider many coalitional influence problems, including coalitional manipulation, margin of victory, and various vote controls and bribery, and (3) we consider arbitrary and variable coalition size $B$. Our main theorem provides asymptotically tight bounds on the semi-random likelihood of the existence of a s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;AI&#23433;&#20840;&#39046;&#22495;&#20013;&#30340;&#37325;&#35201;&#24615;&#27010;&#24565;&#8212;&#8212;&#20851;&#38190;&#25805;&#20316;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#21152;&#39640;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20351;&#25805;&#20316;&#21592;&#21487;&#20197;&#21442;&#19982;&#20854;&#20182;&#27963;&#21160;&#32780;&#19981;&#24573;&#30053;&#30417;&#25511;&#20219;&#21153;&#12290;AI&#20195;&#29702;&#20165;&#35831;&#27714;&#20851;&#38190;&#25805;&#20316;&#30340;&#35768;&#21487;&#65292;&#24182;&#29992;&#25805;&#20316;&#21592;&#30340;&#21453;&#39304;&#20351;&#20195;&#29702;&#26356;&#21152;&#26234;&#33021;&#21270;&#12290;</title><link>http://arxiv.org/abs/2201.04632</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#23433;&#20840;&#20013;&#30340;&#37325;&#35201;&#24615;&#27010;&#24565;
&lt;/p&gt;
&lt;p&gt;
The Concept of Criticality in AI Safety. (arXiv:2201.04632v2 [cs.HC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.04632
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;AI&#23433;&#20840;&#39046;&#22495;&#20013;&#30340;&#37325;&#35201;&#24615;&#27010;&#24565;&#8212;&#8212;&#20851;&#38190;&#25805;&#20316;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#21152;&#39640;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20351;&#25805;&#20316;&#21592;&#21487;&#20197;&#21442;&#19982;&#20854;&#20182;&#27963;&#21160;&#32780;&#19981;&#24573;&#30053;&#30417;&#25511;&#20219;&#21153;&#12290;AI&#20195;&#29702;&#20165;&#35831;&#27714;&#20851;&#38190;&#25805;&#20316;&#30340;&#35768;&#21487;&#65292;&#24182;&#29992;&#25805;&#20316;&#21592;&#30340;&#21453;&#39304;&#20351;&#20195;&#29702;&#26356;&#21152;&#26234;&#33021;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;AI&#20195;&#29702;&#27809;&#26377;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#20445;&#25345;&#19968;&#33268;&#26102;&#65292;&#20182;&#20204;&#21487;&#33021;&#20250;&#36896;&#25104;&#20005;&#37325;&#30340;&#20260;&#23475;&#12290;&#35299;&#20915;&#20215;&#20540;&#35266;&#23545;&#40784;&#38382;&#39064;&#30340;&#19968;&#31181;&#26041;&#27861;&#26159;&#21253;&#25324;&#19968;&#20010;&#20154;&#31867;&#25805;&#20316;&#21592;&#30417;&#25511;&#25152;&#26377;&#20195;&#29702;&#30340;&#27963;&#21160;&#12290;&#23613;&#31649;&#36825;&#31181;&#35299;&#20915;&#26041;&#26696;&#20445;&#35777;&#20102;&#26368;&#22823;&#30340;&#23433;&#20840;&#24615;&#65292;&#20294;&#23427;&#38750;&#24120;&#20302;&#25928;&#65292;&#22240;&#20026;&#23427;&#35201;&#27714;&#20154;&#31867;&#25805;&#20316;&#21592;&#23558;&#25152;&#26377;&#27880;&#24847;&#21147;&#37117;&#19987;&#27880;&#20110;&#20195;&#29702;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26356;&#21152;&#39640;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20351;&#24471;&#25805;&#20316;&#21592;&#21487;&#20197;&#21442;&#19982;&#20854;&#20182;&#27963;&#21160;&#32780;&#19981;&#24573;&#30053;&#30417;&#25511;&#20219;&#21153;&#12290;&#22312;&#25105;&#20204;&#30340;&#26041;&#27861;&#20013;&#65292;AI&#20195;&#29702;&#20165;&#35831;&#27714;&#20851;&#38190;&#25805;&#20316;&#30340;&#35768;&#21487;&#65292;&#20063;&#23601;&#26159;&#28508;&#22312;&#30340;&#26377;&#23475;&#25805;&#20316;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19982;AI&#23433;&#20840;&#30456;&#20851;&#30340;&#20851;&#38190;&#25805;&#20316;&#27010;&#24565;&#65292;&#24182;&#35752;&#35770;&#22914;&#20309;&#26500;&#24314;&#19968;&#20010;&#24230;&#37327;&#34892;&#21160;&#20851;&#38190;&#24615;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#22914;&#20309;&#21033;&#29992;&#25805;&#20316;&#21592;&#30340;&#21453;&#39304;&#20351;&#20195;&#29702;&#26356;&#21152;&#26234;&#33021;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
When AI agents don't align their actions with human values they may cause serious harm. One way to solve the value alignment problem is by including a human operator who monitors all of the agent's actions. Despite the fact, that this solution guarantees maximal safety, it is very inefficient, since it requires the human operator to dedicate all of his attention to the agent. In this paper, we propose a much more efficient solution that allows an operator to be engaged in other activities without neglecting his monitoring task. In our approach the AI agent requests permission from the operator only for critical actions, that is, potentially harmful actions. We introduce the concept of critical actions with respect to AI safety and discuss how to build a model that measures action criticality. We also discuss how the operator's feedback could be used to make the agent smarter.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23433;&#20840;RL&#31639;&#27861;&#65292;&#32467;&#21512;&#20102;&#22522;&#20110;&#23631;&#38556;&#21147;&#30340;&#25511;&#21046;&#31574;&#30053;&#32467;&#26500;&#19982;&#22810;&#27493;&#31574;&#30053;&#35780;&#20272;&#26426;&#21046;&#65292;&#22312;&#20445;&#35777;&#25511;&#21046;&#23433;&#20840;&#30340;&#21516;&#26102;&#65292;&#33021;&#22815;&#24212;&#23545;&#26102;&#38388;&#21464;&#21270;&#30340;&#23433;&#20840;&#32422;&#26463;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#31283;&#23450;&#24615;&#12289;&#40065;&#26834;&#24615;&#21644;&#25910;&#25947;&#24615;&#65292;&#20248;&#20110;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;RL&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2112.11217</link><description>&lt;p&gt;
&#27169;&#22411;&#20026;&#22522;&#30784;&#30340;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#22312;&#26102;&#38388;&#21464;&#21270;&#29366;&#24577;&#21644;&#25511;&#21046;&#32422;&#26463;&#19979;&#30340;&#24212;&#29992;&#65306;&#26234;&#33021;&#36710;&#36742;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Model-Based Safe Reinforcement Learning with Time-Varying State and Control Constraints: An Application to Intelligent Vehicles. (arXiv:2112.11217v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.11217
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23433;&#20840;RL&#31639;&#27861;&#65292;&#32467;&#21512;&#20102;&#22522;&#20110;&#23631;&#38556;&#21147;&#30340;&#25511;&#21046;&#31574;&#30053;&#32467;&#26500;&#19982;&#22810;&#27493;&#31574;&#30053;&#35780;&#20272;&#26426;&#21046;&#65292;&#22312;&#20445;&#35777;&#25511;&#21046;&#23433;&#20840;&#30340;&#21516;&#26102;&#65292;&#33021;&#22815;&#24212;&#23545;&#26102;&#38388;&#21464;&#21270;&#30340;&#23433;&#20840;&#32422;&#26463;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#31283;&#23450;&#24615;&#12289;&#40065;&#26834;&#24615;&#21644;&#25910;&#25947;&#24615;&#65292;&#20248;&#20110;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;RL&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;&#28436;&#21592;-&#35780;&#35770;&#23478;&#32467;&#26500;&#30340;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#22312;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#20013;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#23398;&#20064;&#19968;&#20010;&#20855;&#26377;&#23433;&#20840;&#24615;&#21644;&#25910;&#25947;&#24615;&#20445;&#35777;&#30340;&#36817;&#20284;&#26368;&#20248;&#25511;&#21046;&#31574;&#30053;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#21516;&#26102;&#65292;&#24456;&#23569;&#26377;&#20316;&#21697;&#35752;&#35770;&#20102;&#22312;&#26102;&#38388;&#21464;&#21270;&#30340;&#23433;&#20840;&#24615;&#32422;&#26463;&#19979;&#35774;&#35745;&#23433;&#20840;RL&#31639;&#27861;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23433;&#20840;RL&#31639;&#27861;&#65292;&#29992;&#20110;&#20855;&#26377;&#26102;&#38388;&#21464;&#21270;&#29366;&#24577;&#21644;&#25511;&#21046;&#32422;&#26463;&#30340;&#38750;&#32447;&#24615;&#31995;&#32479;&#30340;&#26368;&#20248;&#25511;&#21046;&#12290;&#22312;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20013;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#23631;&#38556;&#21147;&#30340;&#25511;&#21046;&#31574;&#30053;&#32467;&#26500;&#65292;&#20197;&#20445;&#35777;&#25511;&#21046;&#23433;&#20840;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27493;&#31574;&#30053;&#35780;&#20272;&#26426;&#21046;&#65292;&#29992;&#20110;&#39044;&#27979;&#31574;&#30053;&#22312;&#26102;&#38388;&#21464;&#21270;&#30340;&#23433;&#20840;&#32422;&#26463;&#19979;&#30340;&#23433;&#20840;&#39118;&#38505;&#65292;&#24182;&#25351;&#23548;&#31574;&#30053;&#23433;&#20840;&#26356;&#26032;&#12290;&#24050;&#35777;&#26126;&#31283;&#23450;&#24615;&#21644;&#40065;&#26834;&#24615;&#30340;&#29702;&#35770;&#32467;&#26524;&#12290;&#21516;&#26102;&#65292;&#20998;&#26512;&#20102;&#28436;&#21592;&#35780;&#35770;&#23478;&#23454;&#29616;&#30340;&#25910;&#25947;&#24615;&#12290;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#30340;&#24615;&#33021;&#22312;&#27169;&#25311;&#30340;Sa&#20013;&#20248;&#20110;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;RL&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Recently, safe reinforcement learning (RL) with the actor-critic structure for continuous control tasks has received increasing attention. It is still challenging to learn a near-optimal control policy with safety and convergence guarantees. Also, few works have addressed the safe RL algorithm design under time-varying safety constraints. This paper proposes a safe RL algorithm for optimal control of nonlinear systems with time-varying state and control constraints. In the proposed approach, we construct a novel barrier force-based control policy structure to guarantee control safety. A multi-step policy evaluation mechanism is proposed to predict the policy's safety risk under time-varying safety constraints and guide the policy to update safely. Theoretical results on stability and robustness are proven. Also, the convergence of the actor-critic implementation is analyzed. The performance of the proposed algorithm outperforms several state-of-the-art RL algorithms in the simulated Sa
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MITQA&#30340;&#22522;&#20110;Transformer&#30340;TextTableQA&#31995;&#32479;&#65292;&#36890;&#36807;&#22810;&#23454;&#20363;&#23398;&#20064;&#20197;&#21450;&#36828;&#31243;&#30417;&#30563;&#26041;&#27861;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#34920;&#26684;&#21152;&#25991;&#26412;&#38382;&#39064;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2112.07337</link><description>&lt;p&gt;
&#22522;&#20110;&#22810;&#34892;&#22810;&#36328;&#24230;&#36828;&#31243;&#30417;&#30563;&#30340;&#34920;&#26684;&#21152;&#25991;&#26412;&#38382;&#39064;&#22810;&#23454;&#20363;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Multi-Row, Multi-Span Distant Supervision For Table+Text Question. (arXiv:2112.07337v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.07337
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MITQA&#30340;&#22522;&#20110;Transformer&#30340;TextTableQA&#31995;&#32479;&#65292;&#36890;&#36807;&#22810;&#23454;&#20363;&#23398;&#20064;&#20197;&#21450;&#36828;&#31243;&#30417;&#30563;&#26041;&#27861;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#34920;&#26684;&#21152;&#25991;&#26412;&#38382;&#39064;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20851;&#20110;&#34920;&#26684;&#21644;&#38142;&#25509;&#25991;&#26412;&#30340;&#38382;&#31572;&#65288;TextTableQA&#65289;&#24050;&#32463;&#21462;&#24471;&#20102;&#37325;&#35201;&#30340;&#30740;&#31350;&#25104;&#26524;&#65292;&#22240;&#20026;&#34920;&#26684;&#36890;&#24120;&#19982;&#30456;&#20851;&#30340;&#25991;&#26412;&#23884;&#20837;&#22312;&#25991;&#26723;&#20013;&#12290;HybridQA&#21644;OTT-QA&#26159;&#20004;&#20010;&#26368;&#30693;&#21517;&#30340;TextTableQA&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#30340;&#38382;&#39064;&#26368;&#22909;&#36890;&#36807;&#21516;&#26102;&#20174;&#34920;&#26684;&#21333;&#20803;&#21644;&#38142;&#25509;&#25991;&#26412;&#27573;&#33853;&#20013;&#33719;&#21462;&#20449;&#24687;&#26469;&#22238;&#31572;&#12290;&#36825;&#20004;&#20010;&#25968;&#25454;&#38598;&#30340;&#20849;&#21516;&#25361;&#25112;&#26159;&#65292;&#35757;&#32451;&#23454;&#20363;&#22914;&#38382;&#39064;&#21644;&#31572;&#26696;&#65292;&#20854;&#20013;&#40644;&#37329;&#31572;&#26696;&#21487;&#33021;&#19981;&#20165;&#21305;&#37197;&#36328;&#36234;&#34920;&#26684;&#34892;&#30340;&#22810;&#20010;&#34920;&#26684;&#21333;&#20803;&#65292;&#32780;&#19988;&#36824;&#21253;&#25324;&#34920;&#26684;&#34892;&#21450;&#20854;&#30456;&#20851;&#25991;&#26412;&#33539;&#22260;&#20869;&#30340;&#22810;&#20010;&#25991;&#26412;&#36328;&#24230;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;MITQA&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;Transformer&#30340;TextTableQA&#31995;&#32479;&#65292;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#36890;&#36807;&#22810;&#23454;&#20363;&#25439;&#22833;&#30446;&#26631;&#21644;&#35880;&#24910;&#30340;&#35838;&#31243;&#35774;&#35745;&#26469;&#24212;&#23545;&#36825;&#20004;&#20010;&#26041;&#38754;&#30340;&#36828;&#31243;&#30417;&#30563;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;MRMS-DS&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;MITQA&#22312;HybridQA&#21644;OTT-QA&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22788;&#29702;TextTableQA&#25361;&#25112;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Question answering (QA) over tables and linked text, also called TextTableQA, has witnessed significant research in recent years, as tables are often found embedded in documents along with related text. HybridQA and OTT-QA are the two best-known TextTableQA datasets, with questions that are best answered by combining information from both table cells and linked text passages. A common challenge in both datasets, and TextTableQA in general, is that the training instances include just the question and answer, where the gold answer may match not only multiple table cells across table rows but also multiple text spans within the scope of a table row and its associated text. This leads to a noisy multi instance training regime. We present MITQA, a transformer-based TextTableQA system that is explicitly designed to cope with distant supervision along both these axes, through a multi-instance loss objective, together with careful curriculum design. Our experiments show that the proposed multi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32463;&#39564;&#24341;&#23548;&#30340;&#33945;&#29305;&#21345;&#32599;&#26641;&#25628;&#32034;&#31639;&#27861;&#29992;&#20110;&#21453;&#21512;&#25104;&#35268;&#21010;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#25628;&#32034;&#25928;&#29575;&#21644;&#26377;&#25928;&#24615;&#65292;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2112.06028</link><description>&lt;p&gt;
&#32463;&#39564;&#24341;&#23548;&#30340;&#33945;&#29305;&#21345;&#32599;&#26641;&#25628;&#32034;&#22312;&#21453;&#21512;&#25104;&#35268;&#21010;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Retrosynthetic Planning with Experience-Guided Monte Carlo Tree Search. (arXiv:2112.06028v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.06028
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32463;&#39564;&#24341;&#23548;&#30340;&#33945;&#29305;&#21345;&#32599;&#26641;&#25628;&#32034;&#31639;&#27861;&#29992;&#20110;&#21453;&#21512;&#25104;&#35268;&#21010;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#25628;&#32034;&#25928;&#29575;&#21644;&#26377;&#25928;&#24615;&#65292;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21453;&#21512;&#25104;&#35268;&#21010;&#20013;&#65292;&#20351;&#29992;&#31616;&#21333;&#30340;&#22522;&#30784;&#20998;&#23376;&#21512;&#25104;&#22797;&#26434;&#20998;&#23376;&#30340;&#21487;&#33021;&#36335;&#24452;&#25968;&#37327;&#24040;&#22823;&#65292;&#23548;&#33268;&#21487;&#33021;&#24615;&#21576;&#25351;&#25968;&#32423;&#22686;&#38271;&#12290;&#21363;&#20351;&#26159;&#26377;&#32463;&#39564;&#30340;&#21270;&#23398;&#23478;&#20063;&#24448;&#24448;&#38590;&#20197;&#36873;&#25321;&#26368;&#26377;&#21069;&#36884;&#30340;&#36716;&#21270;&#36335;&#24452;&#12290;&#29616;&#26377;&#26041;&#27861;&#20381;&#36182;&#20110;&#20154;&#24037;&#23450;&#20041;&#25110;&#26426;&#22120;&#35757;&#32451;&#30340;&#35780;&#20998;&#20989;&#25968;&#65292;&#36825;&#20123;&#20989;&#25968;&#30340;&#21270;&#23398;&#30693;&#35782;&#26377;&#38480;&#65292;&#25110;&#20351;&#29992;&#26114;&#36149;&#30340;&#20272;&#31639;&#26041;&#27861;&#26469;&#36827;&#34892;&#25351;&#23548;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#32463;&#39564;&#24341;&#23548;&#30340;&#33945;&#29305;&#21345;&#32599;&#26641;&#25628;&#32034;&#31639;&#27861;&#65288;EG-MCTS&#65289;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#22312;&#25628;&#32034;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#32463;&#39564;&#24341;&#23548;&#32593;&#32476;&#65292;&#20174;&#21512;&#25104;&#32463;&#39564;&#20013;&#23398;&#20064;&#30693;&#35782;&#12290;&#22312;&#22522;&#20934; USPTO &#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;EG-MCTS &#22312;&#25928;&#29575;&#21644;&#26377;&#25928;&#24615;&#26041;&#38754;&#37117;&#27604;&#29616;&#26377;&#30340;&#26041;&#27861;&#26377;&#26174;&#33879;&#25913;&#36827;&#12290;&#22312;&#19982;&#25991;&#29486;&#30340;&#27604;&#36739;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#35745;&#31639;&#26426;&#29983;&#25104;&#30340;&#36335;&#24452;&#22823;&#22810;&#19982;&#25253;&#36947;&#30340;&#36335;&#24452;&#21305;&#37197;&#12290;&#35774;&#35745;&#29992;&#20110;&#30495;&#23454;&#33647;&#29289;&#21270;&#21512;&#29289;&#30340;&#36335;&#24452;&#23637;&#29616;&#20102;&#35813;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In retrosynthetic planning, the huge number of possible routes to synthesize a complex molecule using simple building blocks leads to a combinatorial explosion of possibilities. Even experienced chemists often have difficulty to select the most promising transformations. The current approaches rely on human-defined or machine-trained score functions which have limited chemical knowledge or use expensive estimation methods for guiding. Here we an propose experience-guided Monte Carlo tree search (EG-MCTS) to deal with this problem. Instead of rollout, we build an experience guidance network to learn knowledge from synthetic experiences during the search. Experiments on benchmark USPTO datasets show that, EG-MCTS gains significant improvement over state-of-the-art approaches both in efficiency and effectiveness. In a comparative experiment with the literature, our computer-generated routes mostly matched the reported routes. Routes designed for real drug compounds exhibit the effectivene
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;TND-NAS&#26694;&#26550;&#65292;&#21033;&#29992;&#21487;&#24494;&#26550;&#26500;&#25628;&#32034;&#26694;&#26550;&#21644;&#22810;&#30446;&#26631;NAS&#30340;&#20860;&#23481;&#24615;&#65292;&#36890;&#36807;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#23454;&#29616;&#39640;&#25928;&#22810;&#30446;&#26631;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2111.03892</link><description>&lt;p&gt;
&#36890;&#36807;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#30340;&#39640;&#25928;&#22810;&#30446;&#26631;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Efficient Multi-objective Neural Architecture Search Framework via Policy Gradient Algorithm. (arXiv:2111.03892v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.03892
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;TND-NAS&#26694;&#26550;&#65292;&#21033;&#29992;&#21487;&#24494;&#26550;&#26500;&#25628;&#32034;&#26694;&#26550;&#21644;&#22810;&#30446;&#26631;NAS&#30340;&#20860;&#23481;&#24615;&#65292;&#36890;&#36807;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#23454;&#29616;&#39640;&#25928;&#22810;&#30446;&#26631;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#24494;&#26550;&#26500;&#25628;&#32034;&#24050;&#25104;&#20026;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#39046;&#22495;&#30340;&#20027;&#27969;&#30740;&#31350;&#35838;&#39064;&#65292;&#30456;&#23545;&#20110;&#26089;&#26399;&#30340;EA-based&#21644;RL-based&#26041;&#27861;&#65292;&#20854;&#39640;&#25928;&#29575;&#22791;&#21463;&#38738;&#30544;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#24050;&#19981;&#20877;&#33021;&#22815;&#33258;&#28982;&#22320;&#24212;&#23545;&#19981;&#21487;&#24494;&#21442;&#25968;&#65292;&#22914;&#33021;&#28304;&#21644;&#36164;&#28304;&#21463;&#38480;&#25928;&#29575;&#31561;&#12290;&#38024;&#23545;&#22810;&#30446;&#26631;NAS&#39046;&#22495;&#30340;&#30740;&#31350;&#26088;&#22312;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#30001;&#20110;&#23545;&#27599;&#20010;&#20505;&#36873;&#26550;&#26500;&#36827;&#34892;&#21807;&#19968;&#30340;&#20248;&#21270;&#65292;&#22240;&#27492;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#36164;&#28304;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TND-NAS&#65292;&#23427;&#20855;&#26377;&#19981;&#21487;&#24494;&#21442;&#25968;&#30340;&#20860;&#23481;&#24615;&#21644;&#19981;&#21516;iable NAS&#26694;&#26550;&#20013;&#30340;&#39640;&#25928;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;TND-NAS&#22312;&#25628;&#32034;&#25928;&#29575;&#21644;&#35299;&#20915;&#26041;&#26696;&#36136;&#37327;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Differentiable architecture search has gradually become the mainstream research topic in the field of Neural Architecture Search (NAS) for its high efficiency compared with the early NAS (EA-based, RL-based) methods. Recent differentiable NAS also aims at further improving the search performance and reducing the GPU-memory consumption. However, these methods are no longer naturally capable of tackling the non-differentiable objectives, e.g., energy, resource-constrained efficiency, and other metrics, let alone the multi-objective search demands. Researches in the multi-objective NAS field target this but requires vast computational resources cause of the sole optimization of each candidate architecture. In light of this discrepancy, we propose the TND-NAS, which is with the merits of the high efficiency in differentiable NAS framework and the compatibility among non-differentiable metrics in Multi-objective NAS. Under the differentiable NAS framework, with the continuous relaxation of 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#26412;&#22320;&#21644;&#20998;&#24067;&#24335;&#25191;&#34892;&#32423;&#21035;&#19978;&#23454;&#29616;&#20195;&#29702;&#32452;&#21512;&#24615;&#30340;&#35774;&#35745;&#36873;&#25321;&#65292;&#36890;&#36807;&#29420;&#31435;&#30340;&#21487;&#37325;&#29992;&#32452;&#20214;&#20801;&#35768;&#22312;&#19981;&#21516;&#23610;&#24230;&#19978;&#23450;&#20041;RL&#20195;&#29702;&#65292;&#24182;&#25104;&#21151;&#35299;&#20915;&#20102;&#26032;&#39062;&#21644;&#22797;&#26434;&#30340;&#29615;&#22659;&#65292;&#20855;&#26377;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2007.02622</link><description>&lt;p&gt;
&#39640;&#24230;&#27169;&#22359;&#21270;&#24378;&#21270;&#23398;&#20064;&#24211;&#20013;&#20998;&#24067;&#24335;&#26550;&#26500;&#30340;&#38598;&#25104;
&lt;/p&gt;
&lt;p&gt;
Integrating Distributed Architectures in Highly Modular RL Libraries. (arXiv:2007.02622v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2007.02622
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#26412;&#22320;&#21644;&#20998;&#24067;&#24335;&#25191;&#34892;&#32423;&#21035;&#19978;&#23454;&#29616;&#20195;&#29702;&#32452;&#21512;&#24615;&#30340;&#35774;&#35745;&#36873;&#25321;&#65292;&#36890;&#36807;&#29420;&#31435;&#30340;&#21487;&#37325;&#29992;&#32452;&#20214;&#20801;&#35768;&#22312;&#19981;&#21516;&#23610;&#24230;&#19978;&#23450;&#20041;RL&#20195;&#29702;&#65292;&#24182;&#25104;&#21151;&#35299;&#20915;&#20102;&#26032;&#39062;&#21644;&#22797;&#26434;&#30340;&#29615;&#22659;&#65292;&#20855;&#26377;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#36827;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#38656;&#35201;&#20855;&#26377;&#36275;&#22815;&#28789;&#27963;&#24615;&#30340;&#24037;&#20855;&#65292;&#20197;&#20415;&#36731;&#26494;&#22320;&#21407;&#22411;&#21270;&#26032;&#26041;&#27861;&#65292;&#21516;&#26102;&#36991;&#20813;&#19981;&#20999;&#23454;&#38469;&#30340;&#23454;&#39564;&#21608;&#36716;&#26102;&#38388;&#12290;&#20026;&#20102;&#21305;&#37197;&#31532;&#19968;&#20010;&#35201;&#27714;&#65292;&#26368;&#27969;&#34892;&#30340;RL&#24211;&#20513;&#23548;&#39640;&#24230;&#27169;&#22359;&#21270;&#30340;&#20195;&#29702;&#32452;&#21512;&#24615;&#65292;&#36825;&#26377;&#21161;&#20110;&#23454;&#39564;&#21644;&#24320;&#21457;&#12290;&#20026;&#20102;&#22312;&#21512;&#29702;&#30340;&#26102;&#38388;&#33539;&#22260;&#20869;&#35299;&#20915;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#29615;&#22659;&#65292;&#23558;RL&#25193;&#23637;&#21040;&#22823;&#35268;&#27169;&#37319;&#26679;&#21644;&#35745;&#31639;&#36164;&#28304;&#24050;&#34987;&#35777;&#26126;&#26159;&#19968;&#31181;&#25104;&#21151;&#30340;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#33021;&#21147;&#36804;&#20170;&#20026;&#27490;&#24456;&#38590;&#19982;&#27169;&#22359;&#21270;&#30456;&#32467;&#21512;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#35774;&#35745;&#36873;&#25321;&#65292;&#20197;&#20801;&#35768;&#22312;&#26412;&#22320;&#21644;&#20998;&#24067;&#24335;&#25191;&#34892;&#32423;&#21035;&#19978;&#23454;&#29616;&#20195;&#29702;&#32452;&#21512;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#26041;&#27861;&#65292;&#36890;&#36807;&#29420;&#31435;&#30340;&#21487;&#37325;&#29992;&#32452;&#20214;&#20801;&#35768;&#22312;&#19981;&#21516;&#23610;&#24230;&#19978;&#23450;&#20041;RL&#20195;&#29702;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#35774;&#35745;&#36873;&#25321;&#20351;&#25105;&#20204;&#33021;&#22815;&#22797;&#21046;&#32463;&#20856;&#22522;&#20934;&#27979;&#35797;&#65292;&#25506;&#32034;&#22810;&#20010;&#20998;&#24067;&#24335;&#26550;&#26500;&#65292;&#24182;&#35299;&#20915;&#26032;&#39062;&#21644;&#22797;&#26434;&#30340;&#29615;&#22659;&#65292;&#20855;&#26377;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Advancing reinforcement learning (RL) requires tools that are flexible enough to easily prototype new methods while avoiding impractically slow experimental turnaround times. To match the first requirement, the most popular RL libraries advocate for highly modular agent composability, which facilitates experimentation and development. To solve challenging environments within reasonable time frames, scaling RL to large sampling and computing resources has proved a successful strategy. However, this capability has been so far difficult to combine with modularity. In this work, we explore design choices to allow agent composability both at a local and distributed level of execution. We propose a versatile approach that allows the definition of RL agents at different scales through independent reusable components. We demonstrate experimentally that our design choices allow us to reproduce classical benchmarks, explore multiple distributed architectures, and solve novel and complex environm
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35821;&#20041;&#32034;&#24341;&#26041;&#27861;&#65292;&#36890;&#36807;&#27010;&#24565;&#23618;&#27425;&#32467;&#26500;&#34920;&#31034;&#30693;&#35782;&#24182;&#23558;&#38190;&#20998;&#37197;&#32473;&#33410;&#28857;&#65292;&#20351;&#24471;&#27010;&#24565;&#19982;&#25152;&#26377;&#26356;&#20855;&#20307;&#30340;&#27010;&#24565;&#37096;&#20998;&#21487;&#32479;&#19968;&#65292;&#24182;&#19988;&#21482;&#20801;&#35768;&#28155;&#21152;&#35821;&#20041;&#27491;&#30830;&#30340;&#27010;&#24565;&#12290;&#27492;&#26041;&#27861;&#21487;&#20197;&#20351;&#29992;&#22522;&#20110;&#26696;&#20363;&#30340;&#25512;&#29702;&#21644;&#36890;&#29992;&#38382;&#39064;&#35299;&#20915;&#26041;&#27861;&#36827;&#34892;&#25512;&#29702;&#12290;</title><link>http://arxiv.org/abs/1910.01539</link><description>&lt;p&gt;
&#27010;&#24565;&#23618;&#27425;&#32467;&#26500;&#30340;&#35821;&#20041;&#32034;&#24341;&#26041;&#27861;&#65292;&#32479;&#19968;&#34920;&#31034;&#65292;&#20851;&#31995;&#25968;&#25454;&#24211;&#31995;&#32479;&#21644;&#36890;&#29992;&#19982;&#22522;&#20110;&#26696;&#20363;&#30340;&#25512;&#29702;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Method for the semantic indexing of concept hierarchies, uniform representation, use of relational database systems and generic and case-based reasoning. (arXiv:1910.01539v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1910.01539
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35821;&#20041;&#32034;&#24341;&#26041;&#27861;&#65292;&#36890;&#36807;&#27010;&#24565;&#23618;&#27425;&#32467;&#26500;&#34920;&#31034;&#30693;&#35782;&#24182;&#23558;&#38190;&#20998;&#37197;&#32473;&#33410;&#28857;&#65292;&#20351;&#24471;&#27010;&#24565;&#19982;&#25152;&#26377;&#26356;&#20855;&#20307;&#30340;&#27010;&#24565;&#37096;&#20998;&#21487;&#32479;&#19968;&#65292;&#24182;&#19988;&#21482;&#20801;&#35768;&#28155;&#21152;&#35821;&#20041;&#27491;&#30830;&#30340;&#27010;&#24565;&#12290;&#27492;&#26041;&#27861;&#21487;&#20197;&#20351;&#29992;&#22522;&#20110;&#26696;&#20363;&#30340;&#25512;&#29702;&#21644;&#36890;&#29992;&#38382;&#39064;&#35299;&#20915;&#26041;&#27861;&#36827;&#34892;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35821;&#20041;&#32034;&#24341;&#26041;&#27861;&#65292;&#24182;&#25551;&#36848;&#20102;&#20854;&#22312;&#30693;&#35782;&#34920;&#31034;&#39046;&#22495;&#30340;&#24212;&#29992;&#12290;&#35821;&#20041;&#32034;&#24341;&#30340;&#36215;&#28857;&#26159;&#36890;&#36807;&#27010;&#24565;&#23618;&#27425;&#32467;&#26500;&#34920;&#31034;&#30340;&#30693;&#35782;&#12290;&#20854;&#30446;&#26631;&#26159;&#23558;&#38190;&#20998;&#37197;&#32473;&#33410;&#28857;&#65288;&#27010;&#24565;&#65289;&#65292;&#36825;&#20123;&#33410;&#28857;&#25353;&#23618;&#27425;&#39034;&#24207;&#25490;&#21015;&#19988;&#35821;&#27861;&#21644;&#35821;&#20041;&#27491;&#30830;&#12290;&#20351;&#29992;&#32034;&#24341;&#31639;&#27861;&#65292;&#35745;&#31639;&#38190;&#65292;&#20351;&#24471;&#27010;&#24565;&#19982;&#25152;&#26377;&#26356;&#20855;&#20307;&#30340;&#27010;&#24565;&#37096;&#20998;&#21487;&#32479;&#19968;&#65292;&#24182;&#19988;&#21482;&#20801;&#35768;&#28155;&#21152;&#35821;&#20041;&#27491;&#30830;&#30340;&#27010;&#24565;&#12290;&#38190;&#34920;&#31034;&#26415;&#35821;&#20851;&#31995;&#12290;&#25152;&#36848;&#32034;&#24341;&#31639;&#27861;&#30340;&#27491;&#30830;&#24615;&#21644;&#23436;&#22791;&#24615;&#24050;&#34987;&#35777;&#26126;&#12290;&#25551;&#36848;&#20102;&#23558;&#32463;&#20856;&#20851;&#31995;&#25968;&#25454;&#24211;&#29992;&#20110;&#23454;&#20363;&#23384;&#20648;&#30340;&#26041;&#27861;&#12290;&#30001;&#20110;&#32479;&#19968;&#34920;&#31034;&#65292;&#21487;&#20197;&#20351;&#29992;&#22522;&#20110;&#26696;&#20363;&#30340;&#25512;&#29702;&#21644;&#36890;&#29992;&#38382;&#39064;&#35299;&#20915;&#26041;&#27861;&#36827;&#34892;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a method for semantic indexing and describes its application in the field of knowledge representation. Starting point of the semantic indexing is the knowledge represented by concept hierarchies. The goal is to assign keys to nodes (concepts) that are hierarchically ordered and syntactically and semantically correct. With the indexing algorithm, keys are computed such that concepts are partially unifiable with all more specific concepts and only semantically correct concepts are allowed to be added. The keys represent terminological relationships. Correctness and completeness of the underlying indexing algorithm are proven. The use of classical relational databases for the storage of instances is described. Because of the uniform representation, inference can be done using case-based reasoning and generic problem solving methods.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#36890;&#36807;&#21033;&#29992;&#25968;&#25454;&#25366;&#25496;&#25216;&#26415;&#65292;&#25913;&#36827;&#20102;&#19968;&#31181;&#22522;&#20110;GRASP&#30340;&#26368;&#23567;&#24310;&#36831;&#38382;&#39064;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#25928;&#26524;&#65292;&#21305;&#37197;&#25110;&#20248;&#20110;&#35299;&#30340;&#36136;&#37327;&#65292;&#22312;&#22823;&#22823;&#32553;&#30701;&#35745;&#31639;&#26102;&#38388;&#30340;&#21516;&#26102;&#65292;&#36824;&#25104;&#21151;&#22320;&#24341;&#20837;&#20102;88&#20010;&#26032;&#30340;&#35299;&#25104;&#26412;&#20540;&#12290;</title><link>http://arxiv.org/abs/1908.10705</link><description>&lt;p&gt;
&#36816;&#29992;&#25968;&#25454;&#25366;&#25496;&#25913;&#36827;&#26368;&#23567;&#24310;&#36831;&#38382;&#39064;&#30340;&#21551;&#21457;&#24335;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Improving a State-of-the-Art Heuristic for the Minimum Latency Problem with Data Mining. (arXiv:1908.10705v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1908.10705
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#36890;&#36807;&#21033;&#29992;&#25968;&#25454;&#25366;&#25496;&#25216;&#26415;&#65292;&#25913;&#36827;&#20102;&#19968;&#31181;&#22522;&#20110;GRASP&#30340;&#26368;&#23567;&#24310;&#36831;&#38382;&#39064;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#25928;&#26524;&#65292;&#21305;&#37197;&#25110;&#20248;&#20110;&#35299;&#30340;&#36136;&#37327;&#65292;&#22312;&#22823;&#22823;&#32553;&#30701;&#35745;&#31639;&#26102;&#38388;&#30340;&#21516;&#26102;&#65292;&#36824;&#25104;&#21151;&#22320;&#24341;&#20837;&#20102;88&#20010;&#26032;&#30340;&#35299;&#25104;&#26412;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28151;&#21512;&#20803;&#21551;&#21457;&#24335;&#31639;&#27861;&#22312;&#36816;&#31609;&#23398;&#20013;&#36234;&#26469;&#36234;&#27969;&#34892;&#12290;&#20854;&#20013;&#19968;&#31181;&#25104;&#21151;&#30340;&#26041;&#27861;&#26159;&#23558;&#36138;&#24515;&#38543;&#26426;&#33258;&#36866;&#24212;&#25628;&#32034;&#31243;&#24207;&#65288;GRASP&#65289;&#19982;&#25968;&#25454;&#25366;&#25496;&#25216;&#26415;&#30456;&#32467;&#21512;&#65292;&#21033;&#29992;&#39640;&#36136;&#37327;&#35299;&#20013;&#21457;&#29616;&#30340;&#39057;&#32321;&#27169;&#24335;&#65292;&#22312;&#20445;&#35777;&#25628;&#32034;&#33539;&#22260;&#30340;&#21516;&#26102;&#26174;&#33879;&#20943;&#23569;&#35745;&#31639;&#26102;&#38388;&#12290;&#26412;&#25991;&#21033;&#29992;&#25968;&#25454;&#25366;&#25496;&#25216;&#26415;&#25913;&#36827;&#20102;&#19968;&#20010;&#22522;&#20110;GRASP&#30340;&#26368;&#23567;&#24310;&#36831;&#38382;&#39064;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#36866;&#29992;&#20110;&#20004;&#20010;&#38382;&#39064;&#21464;&#20307;&#12290;&#35745;&#31639;&#23454;&#39564;&#35777;&#26126;&#65292;&#25968;&#25454;&#25366;&#25496;&#26041;&#27861;&#33021;&#22815;&#22312;&#36739;&#22823;&#25968;&#37327;&#30340;&#23454;&#20363;&#19978;&#21305;&#37197;&#25110;&#25913;&#21892;&#35299;&#30340;&#36136;&#37327;&#65292;&#21516;&#26102;&#22823;&#22823;&#20943;&#23569;&#36816;&#34892;&#26102;&#38388;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#36824;&#24341;&#20837;&#20102;88&#20010;&#26032;&#30340;&#35299;&#25104;&#26412;&#20540;&#12290;&#20026;&#20102;&#25903;&#25345;&#25105;&#20204;&#30340;&#32467;&#26524;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#32479;&#35745;&#26174;&#33879;&#24615;&#26816;&#39564;&#12289;&#25366;&#25496;&#27169;&#24335;&#30340;&#24433;&#21709;&#12289;&#31561;&#26102;&#38388;&#27604;&#36739;&#21644;&#26102;&#38388;&#21040;&#30446;&#26631;&#26354;&#32447;&#30340;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, hybrid metaheuristics have become a trend in operations research. A successful example combines the Greedy Randomized Adaptive Search Procedures (GRASP) and data mining techniques, where frequent patterns found in high-quality solutions can lead to an efficient exploration of the search space, along with a significant reduction of computational time. In this work, a GRASP-based state-of-the-art heuristic for the Minimum Latency Problem (MLP) is improved by means of data mining techniques for two MLP variants. Computational experiments showed that the approaches with data mining were able to match or improve the solution quality for a large number of instances, together with a substantial reduction of running time. In addition, 88 new cost values of solutions are introduced into the literature. To support our results, tests of statistical significance, impact of using mined patterns, equal time comparisons and time-to-target plots are provided.
&lt;/p&gt;</description></item></channel></rss>