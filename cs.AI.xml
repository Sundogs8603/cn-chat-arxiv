<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#36890;&#36807;&#35299;&#32806;&#20132;&#20114;&#35821;&#20041;&#21644;&#21160;&#24577;&#65292;&#26412;&#25991;&#23637;&#31034;&#20102;&#22312;&#27809;&#26377;&#30452;&#25509;&#35757;&#32451;&#25991;&#26412;-&#20132;&#20114;&#23545;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#29983;&#25104;&#20154;&#29289;-&#29289;&#20307;&#20132;&#20114;&#30340;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.19652</link><description>&lt;p&gt;
InterDreamer&#65306;&#38646;&#26679;&#26412;&#25991;&#26412;&#21040;&#19977;&#32500;&#21160;&#24577;&#20154;&#29289;-&#29289;&#20307;&#20132;&#20114;
&lt;/p&gt;
&lt;p&gt;
InterDreamer: Zero-Shot Text to 3D Dynamic Human-Object Interaction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19652
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35299;&#32806;&#20132;&#20114;&#35821;&#20041;&#21644;&#21160;&#24577;&#65292;&#26412;&#25991;&#23637;&#31034;&#20102;&#22312;&#27809;&#26377;&#30452;&#25509;&#35757;&#32451;&#25991;&#26412;-&#20132;&#20114;&#23545;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#29983;&#25104;&#20154;&#29289;-&#29289;&#20307;&#20132;&#20114;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19652v1 &#23459;&#24067;&#31867;&#22411;&#65306;&#36328;&#39046;&#22495; &#25688;&#35201;&#65306;&#22312;&#24191;&#27867;&#30340;&#21160;&#20316;&#25429;&#25417;&#25968;&#25454;&#21644;&#30456;&#24212;&#30340;&#25991;&#26412;&#27880;&#37322;&#19978;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#24050;&#32463;&#26174;&#33879;&#25512;&#21160;&#20102;&#25991;&#26412;&#26465;&#20214;&#30340;&#20154;&#20307;&#36816;&#21160;&#29983;&#25104;&#12290;&#28982;&#32780;&#65292;&#23558;&#36825;&#31181;&#25104;&#21151;&#24310;&#20280;&#21040;&#19977;&#32500;&#21160;&#24577;&#20154;&#29289;-&#29289;&#20307;&#20132;&#20114;&#65288;HOI&#65289;&#29983;&#25104;&#38754;&#20020;&#30528;&#26174;&#33879;&#25361;&#25112;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#32570;&#20047;&#22823;&#35268;&#27169;&#20132;&#20114;&#25968;&#25454;&#21644;&#19982;&#36825;&#20123;&#20132;&#20114;&#19968;&#33268;&#30340;&#20840;&#38754;&#25551;&#36848;&#12290;&#26412;&#25991;&#37319;&#21462;&#20102;&#34892;&#21160;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#27809;&#26377;&#30452;&#25509;&#35757;&#32451;&#25991;&#26412;-&#20132;&#20114;&#23545;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#29983;&#25104;&#20154;&#29289;-&#29289;&#20307;&#20132;&#20114;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#22312;&#23454;&#29616;&#36825;&#19968;&#28857;&#30340;&#20851;&#38190;&#35265;&#35299;&#26159;&#20132;&#20114;&#35821;&#20041;&#21644;&#21160;&#24577;&#21487;&#20197;&#35299;&#32806;&#12290;&#26080;&#27861;&#36890;&#36807;&#30417;&#30563;&#35757;&#32451;&#23398;&#20064;&#20132;&#20114;&#35821;&#20041;&#65292;&#25105;&#20204;&#36716;&#32780;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#27169;&#22411;&#65292;&#23558;&#26469;&#33258;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#25991;&#26412;&#21040;&#36816;&#21160;&#27169;&#22411;&#30340;&#30693;&#35782;&#30456;&#36741;&#30456;&#25104;&#12290;&#23613;&#31649;&#36825;&#26679;&#30340;&#30693;&#35782;&#25552;&#20379;&#20102;&#23545;&#20132;&#20114;&#35821;&#20041;&#30340;&#39640;&#32423;&#25511;&#21046;&#65292;&#20294;&#19981;&#33021;&#25552;&#20379;&#21040;&#19981;&#25104;&#23545;&#20132;&#20114;&#25991;&#26412;&#30340;&#30452;&#25509;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19652v1 Announce Type: cross  Abstract: Text-conditioned human motion generation has experienced significant advancements with diffusion models trained on extensive motion capture data and corresponding textual annotations. However, extending such success to 3D dynamic human-object interaction (HOI) generation faces notable challenges, primarily due to the lack of large-scale interaction data and comprehensive descriptions that align with these interactions. This paper takes the initiative and showcases the potential of generating human-object interactions without direct training on text-interaction pair data. Our key insight in achieving this is that interaction semantics and dynamics can be decoupled. Being unable to learn interaction semantics through supervised training, we instead leverage pre-trained large models, synergizing knowledge from a large language model and a text-to-motion model. While such knowledge offers high-level control over interaction semantics, it c
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;MagicLens&#65292;&#19968;&#31995;&#21015;&#25903;&#25345;&#24320;&#25918;&#24335;&#25351;&#20196;&#30340;&#33258;&#30417;&#30563;&#22270;&#20687;&#26816;&#32034;&#27169;&#22411;&#65292;&#26680;&#24515;&#21019;&#26032;&#22312;&#20110;&#21033;&#29992;&#25991;&#26412;&#25351;&#20196;&#20351;&#24471;&#22270;&#20687;&#26816;&#32034;&#21487;&#20197;&#26816;&#32034;&#21040;&#27604;&#35270;&#35273;&#30456;&#20284;&#24615;&#26356;&#20016;&#23500;&#20851;&#31995;&#30340;&#22270;&#20687;&#12290;</title><link>https://arxiv.org/abs/2403.19651</link><description>&lt;p&gt;
MagicLens&#65306;&#33258;&#30417;&#30563;&#22270;&#20687;&#26816;&#32034;&#19982;&#24320;&#25918;&#24335;&#25351;&#20196;
&lt;/p&gt;
&lt;p&gt;
MagicLens: Self-Supervised Image Retrieval with Open-Ended Instructions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19651
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;MagicLens&#65292;&#19968;&#31995;&#21015;&#25903;&#25345;&#24320;&#25918;&#24335;&#25351;&#20196;&#30340;&#33258;&#30417;&#30563;&#22270;&#20687;&#26816;&#32034;&#27169;&#22411;&#65292;&#26680;&#24515;&#21019;&#26032;&#22312;&#20110;&#21033;&#29992;&#25991;&#26412;&#25351;&#20196;&#20351;&#24471;&#22270;&#20687;&#26816;&#32034;&#21487;&#20197;&#26816;&#32034;&#21040;&#27604;&#35270;&#35273;&#30456;&#20284;&#24615;&#26356;&#20016;&#23500;&#20851;&#31995;&#30340;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#26816;&#32034;&#65292;&#21363;&#26681;&#25454;&#21442;&#32771;&#22270;&#20687;&#26597;&#25214;&#25152;&#38656;&#22270;&#20687;&#65292;&#22266;&#26377;&#22320;&#21253;&#21547;&#38590;&#20197;&#20165;&#20351;&#29992;&#22522;&#20110;&#22270;&#20687;&#30340;&#24230;&#37327;&#25429;&#25417;&#21040;&#30340;&#20016;&#23500;&#12289;&#22810;&#26041;&#38754;&#30340;&#25628;&#32034;&#24847;&#22270;&#12290;&#26368;&#36817;&#30340;&#24037;&#20316;&#21033;&#29992;&#25991;&#26412;&#25351;&#20196;&#20801;&#35768;&#29992;&#25143;&#26356;&#33258;&#30001;&#22320;&#34920;&#36798;&#20182;&#20204;&#30340;&#25628;&#32034;&#24847;&#22270;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#37027;&#20123;&#35270;&#35273;&#19978;&#30456;&#20284;&#21644;/&#25110;&#21487;&#20197;&#29992;&#19968;&#23567;&#32452;&#39044;&#23450;&#20041;&#20851;&#31995;&#26469;&#34920;&#24449;&#30340;&#22270;&#20687;&#23545;&#19978;&#12290;&#26412;&#25991;&#30340;&#26680;&#24515;&#35770;&#28857;&#26159;&#25991;&#26412;&#25351;&#20196;&#21487;&#20197;&#20351;&#22270;&#20687;&#26816;&#32034;&#33021;&#22815;&#26816;&#32034;&#21040;&#27604;&#35270;&#35273;&#30456;&#20284;&#24615;&#26356;&#20016;&#23500;&#20851;&#31995;&#30340;&#22270;&#20687;&#12290;&#20026;&#20102;&#35777;&#26126;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;MagicLens&#65292;&#19968;&#31995;&#21015;&#25903;&#25345;&#24320;&#25918;&#24335;&#25351;&#20196;&#30340;&#33258;&#30417;&#30563;&#22270;&#20687;&#26816;&#32034;&#27169;&#22411;&#12290;MagicLens&#24314;&#31435;&#22312;&#19968;&#20010;&#37325;&#35201;&#30340;&#26032;&#39062;&#35265;&#35299;&#19978;&#65306;&#33258;&#28982;&#21457;&#29983;&#22312;&#21516;&#19968;&#32593;&#39029;&#19978;&#30340;&#22270;&#20687;&#23545;&#21253;&#21547;&#30528;&#22823;&#37327;&#38544;&#24335;&#20851;&#31995;&#65288;&#20363;&#22914;&#65292;&#20869;&#37096;&#35270;&#22270;&#65289;&#65292;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#32508;&#21512;&#25351;&#20196;&#23558;&#36825;&#20123;&#38544;&#24335;&#20851;&#31995;&#21464;&#20026;&#26174;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19651v1 Announce Type: cross  Abstract: Image retrieval, i.e., finding desired images given a reference image, inherently encompasses rich, multi-faceted search intents that are difficult to capture solely using image-based measures. Recent work leverages text instructions to allow users to more freely express their search intents. However, existing work primarily focuses on image pairs that are visually similar and/or can be characterized by a small set of pre-defined relations. The core thesis of this paper is that text instructions can enable retrieving images with richer relations beyond visual similarity. To show this, we introduce MagicLens, a series of self-supervised image retrieval models that support open-ended instructions. MagicLens is built on a key novel insight: image pairs that naturally occur on the same web pages contain a wide range of implicit relations (e.g., inside view of), and we can bring those implicit relations explicit by synthesizing instructions
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Human-Regularized PPO (HR-PPO)&#31639;&#27861;&#65292;&#36890;&#36807;&#33258;&#25105;&#21338;&#24328;&#35757;&#32451;&#20195;&#29702;&#65292;&#23454;&#29616;&#22312;&#23553;&#38381;&#29615;&#22659;&#20013;&#36924;&#30495;&#19988;&#26377;&#25928;&#30340;&#39550;&#39542;&#20249;&#20276;</title><link>https://arxiv.org/abs/2403.19648</link><description>&lt;p&gt;
&#36890;&#36807;&#25968;&#25454;&#27491;&#21017;&#21270;&#30340;&#33258;&#25105;&#21338;&#24328;&#24378;&#21270;&#23398;&#20064;&#23454;&#29616;&#19982;&#20154;&#31867;&#20860;&#23481;&#30340;&#39550;&#39542;&#20249;&#20276;
&lt;/p&gt;
&lt;p&gt;
Human-compatible driving partners through data-regularized self-play reinforcement learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19648
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Human-Regularized PPO (HR-PPO)&#31639;&#27861;&#65292;&#36890;&#36807;&#33258;&#25105;&#21338;&#24328;&#35757;&#32451;&#20195;&#29702;&#65292;&#23454;&#29616;&#22312;&#23553;&#38381;&#29615;&#22659;&#20013;&#36924;&#30495;&#19988;&#26377;&#25928;&#30340;&#39550;&#39542;&#20249;&#20276;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20027;&#39550;&#39542;&#27773;&#36710;&#38754;&#20020;&#30340;&#19968;&#20010;&#26680;&#24515;&#25361;&#25112;&#26159;&#19982;&#20154;&#31867;&#36827;&#34892;&#21327;&#35843;&#12290;&#22240;&#27492;&#65292;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#65292;&#23558;&#36924;&#30495;&#30340;&#20154;&#31867;&#20195;&#29702;&#32435;&#20837;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#30340;&#21487;&#25193;&#23637;&#35757;&#32451;&#21644;&#35780;&#20272;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Human-Regularized PPO (HR-PPO)&#30340;&#22810;&#26234;&#33021;&#20307;&#31639;&#27861;&#65292;&#20854;&#20013;&#20195;&#29702;&#36890;&#36807;&#33258;&#25105;&#21338;&#24328;&#36827;&#34892;&#35757;&#32451;&#65292;&#23545;&#20559;&#31163;&#20154;&#31867;&#21442;&#32771;&#31574;&#30053;&#30340;&#34892;&#20026;&#36827;&#34892;&#23567;&#24133;&#24809;&#32602;&#65292;&#20197;&#26500;&#24314;&#22312;&#23553;&#38381;&#29615;&#22659;&#20013;&#26082;&#36924;&#30495;&#21448;&#26377;&#25928;&#30340;&#20195;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19648v1 Announce Type: cross  Abstract: A central challenge for autonomous vehicles is coordinating with humans. Therefore, incorporating realistic human agents is essential for scalable training and evaluation of autonomous driving systems in simulation. Simulation agents are typically developed by imitating large-scale, high-quality datasets of human driving. However, pure imitation learning agents empirically have high collision rates when executed in a multi-agent closed-loop setting. To build agents that are realistic and effective in closed-loop settings, we propose Human-Regularized PPO (HR-PPO), a multi-agent algorithm where agents are trained through self-play with a small penalty for deviating from a human reference policy. In contrast to prior work, our approach is RL-first and only uses 30 minutes of imperfect human demonstrations. We evaluate agents in a large set of multi-agent traffic scenes. Results show our HR-PPO agents are highly effective in achieving goa
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21363;&#31232;&#30095;&#29305;&#24449;&#30005;&#36335;&#65292;&#21487;&#20197;&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#21457;&#29616;&#21644;&#32534;&#36753;&#21487;&#35299;&#37322;&#30340;&#22240;&#26524;&#22270;&#65292;&#20026;&#25105;&#20204;&#25552;&#20379;&#20102;&#23545;&#26410;&#39044;&#26009;&#26426;&#21046;&#30340;&#35814;&#32454;&#29702;&#35299;&#21644;&#21253;&#21547;&#20102;&#29992;&#20110;&#25552;&#39640;&#20998;&#31867;&#22120;&#27867;&#21270;&#33021;&#21147;&#30340;SHIFT&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.19647</link><description>&lt;p&gt;
&#31232;&#30095;&#29305;&#24449;&#30005;&#36335;&#65306;&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#21457;&#29616;&#21644;&#32534;&#36753;&#21487;&#35299;&#37322;&#30340;&#22240;&#26524;&#22270;
&lt;/p&gt;
&lt;p&gt;
Sparse Feature Circuits: Discovering and Editing Interpretable Causal Graphs in Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19647
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21363;&#31232;&#30095;&#29305;&#24449;&#30005;&#36335;&#65292;&#21487;&#20197;&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#21457;&#29616;&#21644;&#32534;&#36753;&#21487;&#35299;&#37322;&#30340;&#22240;&#26524;&#22270;&#65292;&#20026;&#25105;&#20204;&#25552;&#20379;&#20102;&#23545;&#26410;&#39044;&#26009;&#26426;&#21046;&#30340;&#35814;&#32454;&#29702;&#35299;&#21644;&#21253;&#21547;&#20102;&#29992;&#20110;&#25552;&#39640;&#20998;&#31867;&#22120;&#27867;&#21270;&#33021;&#21147;&#30340;SHIFT&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#29992;&#20110;&#21457;&#29616;&#21644;&#24212;&#29992;&#31232;&#30095;&#29305;&#24449;&#30005;&#36335;&#30340;&#26041;&#27861;&#12290;&#36825;&#20123;&#30005;&#36335;&#26159;&#20154;&#31867;&#21487;&#35299;&#37322;&#29305;&#24449;&#30340;&#22240;&#26524;&#30456;&#20851;&#23376;&#32593;&#32476;&#65292;&#29992;&#20110;&#35299;&#37322;&#35821;&#35328;&#27169;&#22411;&#34892;&#20026;&#12290; &#22312;&#20808;&#21069;&#30340;&#24037;&#20316;&#20013;&#30830;&#23450;&#30340;&#30005;&#36335;&#30001;&#22810;&#20041;&#19988;&#38590;&#20197;&#35299;&#37322;&#30340;&#21333;&#20803;&#32452;&#25104;&#65292;&#20363;&#22914;&#27880;&#24847;&#21147;&#22836;&#25110;&#31070;&#32463;&#20803;&#65292;&#20351;&#23427;&#20204;&#19981;&#36866;&#29992;&#20110;&#35768;&#22810;&#19979;&#28216;&#24212;&#29992;&#12290; &#30456;&#27604;&#20043;&#19979;&#65292;&#31232;&#30095;&#29305;&#24449;&#30005;&#36335;&#23454;&#29616;&#20102;&#23545;&#26410;&#39044;&#26009;&#26426;&#21046;&#30340;&#35814;&#32454;&#29702;&#35299;&#12290; &#30001;&#20110;&#23427;&#20204;&#22522;&#20110;&#32454;&#31890;&#24230;&#21333;&#20803;&#65292;&#31232;&#30095;&#29305;&#24449;&#30005;&#36335;&#23545;&#19979;&#28216;&#20219;&#21153;&#38750;&#24120;&#26377;&#29992;&#65306;&#25105;&#20204; introduc&#20102;SHIFT&#65292;&#36890;&#36807;&#20999;&#38500;&#20154;&#31867;&#21028;&#26029;&#20026;&#20219;&#21153;&#19981;&#30456;&#20851;&#30340;&#29305;&#24449;&#65292;&#20174;&#32780;&#25552;&#39640;&#20998;&#31867;&#22120;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290; &#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#21457;&#29616;&#25104;&#21315;&#19978;&#19975;&#20010;&#31232;&#30095;&#29305;&#24449;&#30005;&#36335;&#26469;&#23637;&#31034;&#19968;&#20010;&#23436;&#20840;&#26080;&#30417;&#30563;&#19988;&#21487;&#25193;&#23637;&#30340;&#21487;&#35299;&#37322;&#24615;&#31649;&#32447;&#65292;&#29992;&#20110;&#33258;&#21160;&#21457;&#29616;&#30340;&#27169;&#22411;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19647v1 Announce Type: cross  Abstract: We introduce methods for discovering and applying sparse feature circuits. These are causally implicated subnetworks of human-interpretable features for explaining language model behaviors. Circuits identified in prior work consist of polysemantic and difficult-to-interpret units like attention heads or neurons, rendering them unsuitable for many downstream applications. In contrast, sparse feature circuits enable detailed understanding of unanticipated mechanisms. Because they are based on fine-grained units, sparse feature circuits are useful for downstream tasks: We introduce SHIFT, where we improve the generalization of a classifier by ablating features that a human judges to be task-irrelevant. Finally, we demonstrate an entirely unsupervised and scalable interpretability pipeline by discovering thousands of sparse feature circuits for automatically discovered model behaviors.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#29992;&#20110;&#22810;&#36339;&#38382;&#39064;&#22238;&#31572;&#30340;&#26816;&#32034;&#22686;&#24378;&#27169;&#22411;&#32534;&#36753;&#65288;RAE&#65289;&#26694;&#26550;&#65292;&#21033;&#29992;&#20114;&#20449;&#24687;&#26368;&#22823;&#21270;&#30340;&#26816;&#32034;&#26041;&#27861;&#21644;&#20462;&#21098;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#23545;&#35821;&#35328;&#27169;&#22411;&#30340;&#26377;&#25928;&#20248;&#21270;&#12290;</title><link>https://arxiv.org/abs/2403.19631</link><description>&lt;p&gt;
&#22810;&#36339;&#38382;&#39064;&#22238;&#31572;&#20013;&#30340;&#26816;&#32034;&#22686;&#24378;&#30693;&#35782;&#32534;&#36753;&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Retrieval-Enhanced Knowledge Editing for Multi-Hop Question Answering in Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19631
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#29992;&#20110;&#22810;&#36339;&#38382;&#39064;&#22238;&#31572;&#30340;&#26816;&#32034;&#22686;&#24378;&#27169;&#22411;&#32534;&#36753;&#65288;RAE&#65289;&#26694;&#26550;&#65292;&#21033;&#29992;&#20114;&#20449;&#24687;&#26368;&#22823;&#21270;&#30340;&#26816;&#32034;&#26041;&#27861;&#21644;&#20462;&#21098;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#23545;&#35821;&#35328;&#27169;&#22411;&#30340;&#26377;&#25928;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#38382;&#39064;&#22238;&#31572;&#20219;&#21153;&#20013;&#26174;&#31034;&#20986;&#39640;&#25928;&#33021;&#65292;&#20294;&#24448;&#24448;&#38590;&#20197;&#25972;&#21512;&#23454;&#26102;&#30693;&#35782;&#26356;&#26032;&#65292;&#23548;&#33268;&#21487;&#33021;&#36807;&#26102;&#25110;&#19981;&#20934;&#30830;&#30340;&#21709;&#24212;&#12290;&#24403;&#22788;&#29702;&#22810;&#36339;&#38382;&#39064;&#26102;&#65292;&#36825;&#20010;&#38382;&#39064;&#21464;&#24471;&#26356;&#20855;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#23427;&#20204;&#35201;&#27714;LLMs&#26356;&#26032;&#21644;&#25972;&#21512;&#19982;&#38382;&#39064;&#30456;&#20851;&#30340;&#22810;&#20010;&#30693;&#35782;&#29255;&#27573;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#38024;&#23545;&#22810;&#36339;&#38382;&#39064;&#22238;&#31572;&#23450;&#21046;&#30340;&#26816;&#32034;&#22686;&#24378;&#27169;&#22411;&#32534;&#36753;&#65288;RAE&#65289;&#26694;&#26550;&#12290;RAE&#39318;&#20808;&#26816;&#32034;&#32534;&#36753;&#21518;&#30340;&#20107;&#23454;&#65292;&#28982;&#21518;&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#26469;&#23436;&#21892;&#35821;&#35328;&#27169;&#22411;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#26816;&#32034;&#26041;&#27861;&#22522;&#20110;&#20114;&#20449;&#24687;&#26368;&#22823;&#21270;&#65292;&#21033;&#29992;LLMs&#30340;&#25512;&#29702;&#33021;&#21147;&#26469;&#35782;&#21035;&#38142;&#24335;&#20107;&#23454;&#65292;&#32780;&#22825;&#30495;&#30340;&#22522;&#20110;&#30456;&#20284;&#24615;&#30340;&#25628;&#32034;&#21487;&#33021;&#20250;&#24573;&#30053;&#36825;&#20123;&#20107;&#23454;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#36824;&#37319;&#29992;&#20102;&#20462;&#21098;&#31574;&#30053;&#65292;&#20174;&#26816;&#32034;&#21040;&#30340;&#20107;&#23454;&#20013;&#28040;&#38500;&#20887;&#20313;&#20449;&#24687;&#65292;&#36825;&#22686;&#24378;&#20102;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19631v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have shown proficiency in question-answering tasks but often struggle to integrate real-time knowledge updates, leading to potentially outdated or inaccurate responses. This problem becomes even more challenging when dealing with multi-hop questions since they require LLMs to update and integrate multiple knowledge pieces relevant to the questions. To tackle the problem, we propose the Retrieval-Augmented model Editing (RAE) framework tailored for multi-hop question answering. RAE first retrieves edited facts and then refines the language model through in-context learning. Specifically, our retrieval approach, based on mutual information maximization, leverages the reasoning abilities of LLMs to identify chain facts that na\"ive similarity-based searches might miss. Additionally, our framework incorporates a pruning strategy to eliminate redundant information from the retrieved facts, which enhances the edi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#37319;&#29992;&#21019;&#24847;&#23545;&#25239;&#32593;&#32476;&#21644;&#28436;&#21270;&#26041;&#27861;&#22312;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#36827;&#34892;&#21327;&#20316;&#20114;&#21160;&#28436;&#21270;&#33402;&#26415;&#65292;&#36890;&#36807;&#33258;&#21160;&#32654;&#23398;&#21644;&#21327;&#20316;&#20132;&#20114;&#24335;&#20154;&#31867;&#35780;&#20272;&#25351;&#26631;&#35780;&#20272;&#29983;&#25104;&#30340;&#22270;&#20687;&#36136;&#37327;&#12290;</title><link>https://arxiv.org/abs/2403.19620</link><description>&lt;p&gt;
&#22312;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#21327;&#20316;&#20114;&#21160;&#28436;&#21270;&#33402;&#26415;
&lt;/p&gt;
&lt;p&gt;
Collaborative Interactive Evolution of Art in the Latent Space of Deep Generative Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19620
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#37319;&#29992;&#21019;&#24847;&#23545;&#25239;&#32593;&#32476;&#21644;&#28436;&#21270;&#26041;&#27861;&#22312;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#36827;&#34892;&#21327;&#20316;&#20114;&#21160;&#28436;&#21270;&#33402;&#26415;&#65292;&#36890;&#36807;&#33258;&#21160;&#32654;&#23398;&#21644;&#21327;&#20316;&#20132;&#20114;&#24335;&#20154;&#31867;&#35780;&#20272;&#25351;&#26631;&#35780;&#20272;&#29983;&#25104;&#30340;&#22270;&#20687;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#22312;&#29983;&#25104;&#39640;&#36136;&#37327;&#22270;&#20687;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#22240;&#27492;&#34987;&#29992;&#20316;&#29983;&#25104;&#33402;&#26415;&#22270;&#20687;&#30340;&#20027;&#35201;&#26041;&#27861;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#36890;&#24120;&#22270;&#20687;&#29983;&#25104;&#36807;&#31243;&#28041;&#21450;&#20174;&#23398;&#20064;&#30340;&#33402;&#26415;&#34920;&#24449;&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#36827;&#34892;&#37319;&#26679;&#65292;&#23545;&#36755;&#20986;&#30340;&#25511;&#21046;&#24456;&#23569;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;&#24050;&#30693;&#20026;&#21019;&#24847;&#23545;&#25239;&#32593;&#32476;&#65288;CANs&#65289;&#30340;&#26550;&#26500;&#35757;&#32451;GANs&#29983;&#25104;&#21019;&#24847;&#22270;&#20687;&#65292;&#28982;&#21518;&#65292;&#25105;&#20204;&#37319;&#29992;&#19968;&#20010;&#28436;&#21270;&#26041;&#27861;&#22312;&#27169;&#22411;&#30340;&#28508;&#22312;&#31354;&#38388;&#20869;&#23548;&#33322;&#20197;&#21457;&#29616;&#22270;&#20687;&#12290;&#25105;&#20204;&#20351;&#29992;&#33258;&#21160;&#32654;&#23398;&#21644;&#21327;&#20316;&#20132;&#20114;&#24335;&#20154;&#31867;&#35780;&#20272;&#25351;&#26631;&#26469;&#35780;&#20272;&#29983;&#25104;&#30340;&#22270;&#20687;&#12290;&#22312;&#20154;&#31867;&#20114;&#21160;&#35780;&#20272;&#26696;&#20363;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#22810;&#20301;&#21442;&#19982;&#32773;&#35780;&#20272;&#30340;&#21327;&#20316;&#35780;&#20272;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23581;&#35797;&#20102;&#19968;&#31181;&#26088;&#22312;&#25552;&#39640;&#22270;&#20687;&#36136;&#37327;&#30340;&#26234;&#33021;&#31361;&#21464;&#36816;&#31639;&#31526;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19620v1 Announce Type: cross  Abstract: Generative Adversarial Networks (GANs) have shown great success in generating high quality images and are thus used as one of the main approaches to generate art images. However, usually the image generation process involves sampling from the latent space of the learned art representations, allowing little control over the output. In this work, we first employ GANs that are trained to produce creative images using an architecture known as Creative Adversarial Networks (CANs), then, we employ an evolutionary approach to navigate within the latent space of the models to discover images. We use automatic aesthetic and collaborative interactive human evaluation metrics to assess the generated images. In the human interactive evaluation case, we propose a collaborative evaluation based on the assessments of several participants. Furthermore, we also experiment with an intelligent mutation operator that aims to improve the quality of the ima
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#35821;&#20041;&#22320;&#22270;&#20316;&#20026;&#35270;&#35273;&#36755;&#20837;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23548;&#33322;&#35828;&#26126;&#29983;&#25104;&#26041;&#27861;&#65292;&#23558;&#38382;&#39064;&#26500;&#24314;&#20026;&#22270;&#20687;&#23383;&#24149;&#20219;&#21153;&#65292;&#26377;&#26395;&#38477;&#20302;&#29983;&#25104;&#25351;&#20196;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.19603</link><description>&lt;p&gt;
&#22522;&#20110;&#35821;&#20041;&#22320;&#22270;&#30340;&#23548;&#33322;&#35828;&#26126;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Semantic Map-based Generation of Navigation Instructions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19603
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#35821;&#20041;&#22320;&#22270;&#20316;&#20026;&#35270;&#35273;&#36755;&#20837;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23548;&#33322;&#35828;&#26126;&#29983;&#25104;&#26041;&#27861;&#65292;&#23558;&#38382;&#39064;&#26500;&#24314;&#20026;&#22270;&#20687;&#23383;&#24149;&#20219;&#21153;&#65292;&#26377;&#26395;&#38477;&#20302;&#29983;&#25104;&#25351;&#20196;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23545;&#23548;&#33322;&#35828;&#26126;&#30340;&#29983;&#25104;&#24456;&#24863;&#20852;&#36259;&#65292;&#26080;&#35770;&#26159;&#20316;&#20026;&#33258;&#36523;&#23384;&#22312;&#30340;&#25991;&#26412;&#36824;&#26159;&#20316;&#20026;&#26426;&#22120;&#20154;&#23548;&#33322;&#20219;&#21153;&#30340;&#35757;&#32451;&#26448;&#26009;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23548;&#33322;&#35828;&#26126;&#29983;&#25104;&#26041;&#27861;&#65292;&#23558;&#38382;&#39064;&#26500;&#24314;&#20026;&#20351;&#29992;&#35821;&#20041;&#22320;&#22270;&#20316;&#20026;&#35270;&#35273;&#36755;&#20837;&#30340;&#22270;&#20687;&#23383;&#24149;&#20219;&#21153;&#12290;&#20256;&#32479;&#26041;&#27861;&#37319;&#29992;&#19968;&#31995;&#21015;&#20840;&#26223;&#22270;&#20687;&#26469;&#29983;&#25104;&#23548;&#33322;&#35828;&#26126;&#12290;&#35821;&#20041;&#22320;&#22270;&#23558;&#35270;&#35273;&#32454;&#33410;&#25277;&#35937;&#20986;&#26469;&#65292;&#23558;&#22810;&#20010;&#20840;&#26223;&#22270;&#20687;&#20013;&#30340;&#20449;&#24687;&#34701;&#21512;&#21040;&#21333;&#20010;&#33258;&#19978;&#32780;&#19979;&#30340;&#34920;&#31034;&#20013;&#65292;&#20174;&#32780;&#38477;&#20302;&#20102;&#22788;&#29702;&#36755;&#20837;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#20351;&#29992;&#35821;&#20041;&#22320;&#22270;&#29983;&#25104;&#35828;&#26126;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#21021;&#22987;&#27169;&#22411;&#65292;&#24182;&#35831;&#20154;&#24037;&#20027;&#35266;&#35780;&#20272;&#29983;&#25104;&#35828;&#26126;&#30340;&#36136;&#37327;&#12290;&#25105;&#20204;&#30340;&#21021;&#27493;&#35843;&#26597;&#26174;&#31034;&#65292;&#20351;&#29992;&#35821;&#20041;&#22320;&#22270;&#29983;&#25104;&#35828;&#26126;&#32780;&#19981;&#26159;&#19968;&#31995;&#21015;&#20840;&#26223;&#22270;&#20687;&#20855;&#26377;&#28508;&#21147;&#65292;&#20294;&#30740;&#31350;&#33539;&#22260;&#20173;&#28982;&#24191;&#38420;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19603v1 Announce Type: cross  Abstract: We are interested in the generation of navigation instructions, either in their own right or as training material for robotic navigation task. In this paper, we propose a new approach to navigation instruction generation by framing the problem as an image captioning task using semantic maps as visual input. Conventional approaches employ a sequence of panorama images to generate navigation instructions. Semantic maps abstract away from visual details and fuse the information in multiple panorama images into a single top-down representation, thereby reducing computational complexity to process the input. We present a benchmark dataset for instruction generation using semantic maps, propose an initial model and ask human subjects to manually assess the quality of generated instructions. Our initial investigations show promise in using semantic maps for instruction generation instead of a sequence of panorama images, but there is vast sco
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24773;&#22659;&#24863;&#30693;&#30340;&#39550;&#39542;&#39118;&#26684;&#27169;&#22411;&#65292;&#36890;&#36807;&#19981;&#21516;&#35270;&#35273;&#29305;&#24449;&#32534;&#30721;&#22120;&#21644;&#39550;&#39542;&#34892;&#20026;&#39044;&#27979;&#22120;&#30340;&#32467;&#21512;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#36866;&#24212;&#39550;&#39542;&#24773;&#22659;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#20854;&#20248;&#20110;&#38745;&#24577;&#39550;&#39542;&#39118;&#26684;&#12290;</title><link>https://arxiv.org/abs/2403.19595</link><description>&lt;p&gt;
&#38754;&#21521;&#39550;&#39542;&#21592;&#20013;&#24515;&#39550;&#39542;&#39118;&#26684;&#33258;&#36866;&#24212;&#30340;&#24773;&#22659;&#24863;&#30693;
&lt;/p&gt;
&lt;p&gt;
Situation Awareness for Driver-Centric Driving Style Adaptation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19595
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24773;&#22659;&#24863;&#30693;&#30340;&#39550;&#39542;&#39118;&#26684;&#27169;&#22411;&#65292;&#36890;&#36807;&#19981;&#21516;&#35270;&#35273;&#29305;&#24449;&#32534;&#30721;&#22120;&#21644;&#39550;&#39542;&#34892;&#20026;&#39044;&#27979;&#22120;&#30340;&#32467;&#21512;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#36866;&#24212;&#39550;&#39542;&#24773;&#22659;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#20854;&#20248;&#20110;&#38745;&#24577;&#39550;&#39542;&#39118;&#26684;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#35777;&#25454;&#34920;&#26126;&#65292;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#30340;&#39550;&#39542;&#39118;&#26684;&#23545;&#20110;&#25552;&#39640;&#20056;&#23458;&#30340;&#25509;&#21463;&#31243;&#24230;&#21644;&#20449;&#20219;&#24230;&#33267;&#20851;&#37325;&#35201;&#12290;&#39550;&#39542;&#24773;&#22659;&#24050;&#34987;&#21457;&#29616;&#23545;&#20154;&#31867;&#39550;&#39542;&#34892;&#20026;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#39550;&#39542;&#39118;&#26684;&#27169;&#22411;&#20165;&#37096;&#20998;&#22320;&#23558;&#39550;&#39542;&#29615;&#22659;&#20449;&#24687;&#32435;&#20837;&#32771;&#34385;&#65292;&#38480;&#21046;&#20102;&#20195;&#29702;&#19982;&#32473;&#23450;&#24773;&#22659;&#20043;&#38388;&#30340;&#34900;&#25509;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39044;&#20808;&#35757;&#32451;&#30340;&#19981;&#21516;&#35270;&#35273;&#29305;&#24449;&#32534;&#30721;&#22120;&#21644;&#36866;&#24212;&#20110;&#29305;&#23450;&#39550;&#39542;&#21592;&#39550;&#39542;&#39118;&#26684;&#30340;&#39550;&#39542;&#34892;&#20026;&#39044;&#27979;&#22120;&#30340;&#24773;&#22659;&#24863;&#30693;&#39550;&#39542;&#39118;&#26684;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#26126;&#26174;&#20248;&#20110;&#38745;&#24577;&#39550;&#39542;&#39118;&#26684;&#65292;&#24182;&#24418;&#25104;&#20102;&#21512;&#29702;&#30340;&#24773;&#22659;&#32676;&#32858;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#19978;&#39044;&#20808;&#35757;&#32451;&#30340;&#29305;&#24449;&#32534;&#30721;&#22120;&#33021;&#22815;&#24102;&#26469;&#26356;&#31934;&#30830;&#30340;&#39550;&#39542;&#34892;&#20026;&#24314;&#27169;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#22312;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#19979;&#39044;&#20808;&#35757;&#32451;&#30340;&#29305;&#24449;&#32534;&#30721;&#22120;&#23548;&#33268;&#27169;&#22411;&#24615;&#33021;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19595v1 Announce Type: cross  Abstract: There is evidence that the driving style of an autonomous vehicle is important to increase the acceptance and trust of the passengers. The driving situation has been found to have a significant influence on human driving behavior. However, current driving style models only partially incorporate driving environment information, limiting the alignment between an agent and the given situation. Therefore, we propose a situation-aware driving style model based on different visual feature encoders pretrained on fleet data, as well as driving behavior predictors, which are adapted to the driving style of a specific driver. Our experiments show that the proposed method outperforms static driving styles significantly and forms plausible situation clusters. Furthermore, we found that feature encoders pretrained on our dataset lead to more precise driving behavior modeling. In contrast, feature encoders pretrained supervised and unsupervised on d
&lt;/p&gt;</description></item><item><title>Img2Loc&#36890;&#36807;&#22810;&#27169;&#24577;&#22522;&#30784;&#27169;&#22411;&#21644;&#22522;&#20110;&#22270;&#20687;&#26816;&#32034;&#22686;&#24378;&#30340;&#29983;&#25104;&#37325;&#26032;&#23450;&#20041;&#22270;&#20687;&#22320;&#29702;&#23450;&#20301;&#20026;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#12290;</title><link>https://arxiv.org/abs/2403.19584</link><description>&lt;p&gt;
Img2Loc: &#36890;&#36807;&#22810;&#27169;&#24577;&#22522;&#30784;&#27169;&#22411;&#21644;&#22522;&#20110;&#22270;&#20687;&#26816;&#32034;&#22686;&#24378;&#30340;&#29983;&#25104;&#37325;&#26032;&#23457;&#35270;&#22270;&#20687;&#22320;&#29702;&#23450;&#20301;
&lt;/p&gt;
&lt;p&gt;
Img2Loc: Revisiting Image Geolocalization using Multi-modality Foundation Models and Image-based Retrieval-Augmented Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19584
&lt;/p&gt;
&lt;p&gt;
Img2Loc&#36890;&#36807;&#22810;&#27169;&#24577;&#22522;&#30784;&#27169;&#22411;&#21644;&#22522;&#20110;&#22270;&#20687;&#26816;&#32034;&#22686;&#24378;&#30340;&#29983;&#25104;&#37325;&#26032;&#23450;&#20041;&#22270;&#20687;&#22320;&#29702;&#23450;&#20301;&#20026;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#22270;&#20687;&#20013;&#23450;&#20301;&#31934;&#30830;&#20301;&#32622;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#20449;&#24687;&#26816;&#32034;&#20013;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#20256;&#32479;&#26041;&#27861;&#36890;&#24120;&#37319;&#29992;&#20998;&#31867;&#25110;&#26816;&#32034;&#65292;&#20998;&#31867;&#26041;&#27861;&#23558;&#22320;&#29699;&#34920;&#38754;&#21010;&#20998;&#20026;&#32593;&#26684;&#21333;&#20803;&#24182;&#30456;&#24212;&#22320;&#23545;&#22270;&#20687;&#36827;&#34892;&#20998;&#31867;&#65292;&#32780;&#26816;&#32034;&#26041;&#27861;&#36890;&#36807;&#23558;&#22270;&#20687;&#19982;&#22270;&#20687;-&#20301;&#32622;&#23545;&#25968;&#25454;&#24211;&#21305;&#37197;&#26469;&#35782;&#21035;&#20301;&#32622;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;&#20998;&#31867;&#30340;&#26041;&#27861;&#21463;&#21040;&#21333;&#20803;&#26684;&#22823;&#23567;&#30340;&#38480;&#21046;&#65292;&#19981;&#33021;&#20135;&#29983;&#31934;&#30830;&#30340;&#39044;&#27979;&#65292;&#32780;&#22522;&#20110;&#26816;&#32034;&#30340;&#31995;&#32479;&#36890;&#24120;&#25628;&#32034;&#36136;&#37327;&#36739;&#24046;&#65292;&#23545;&#20840;&#29699;&#26223;&#35266;&#22312;&#19981;&#21516;&#23610;&#24230;&#21644;&#32858;&#21512;&#32423;&#21035;&#30340;&#35206;&#30422;&#19981;&#36275;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#32570;&#28857;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Img2Loc&#65292;&#36825;&#26159;&#19968;&#20010;&#36890;&#36807;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#37325;&#26032;&#23450;&#20041;&#22270;&#20687;&#22320;&#29702;&#23450;&#20301;&#30340;&#26032;&#31995;&#32479;&#12290;&#36825;&#26159;&#36890;&#36807;&#20351;&#29992;&#20687;GPT4V&#25110;LLaVA&#36825;&#26679;&#30340;&#23574;&#31471;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#19982;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#26469;&#23454;&#29616;&#30340;&#12290;Img2Loc&#39318;&#20808;&#20351;&#29992;&#22522;&#20110;CLIP&#30340;&#34920;&#31034;&#26469;&#29983;&#25104;&#19968;&#24352;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19584v1 Announce Type: cross  Abstract: Geolocating precise locations from images presents a challenging problem in computer vision and information retrieval.Traditional methods typically employ either classification, which dividing the Earth surface into grid cells and classifying images accordingly, or retrieval, which identifying locations by matching images with a database of image-location pairs. However, classification-based approaches are limited by the cell size and cannot yield precise predictions, while retrieval-based systems usually suffer from poor search quality and inadequate coverage of the global landscape at varied scale and aggregation levels. To overcome these drawbacks, we present Img2Loc, a novel system that redefines image geolocalization as a text generation task. This is achieved using cutting-edge large multi-modality models like GPT4V or LLaVA with retrieval augmented generation. Img2Loc first employs CLIP-based representations to generate an image
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#25105;&#25913;&#36827;&#23398;&#20064;(SIL)&#26041;&#27861;&#65292;&#23454;&#29616;&#31070;&#32463;&#32452;&#21512;&#20248;&#21270;&#30340;&#26356;&#22909;&#21487;&#25193;&#23637;&#24615;&#65292;&#36890;&#36807;&#33258;&#36523;&#29983;&#25104;&#35299;&#20915;&#26041;&#26696;&#20316;&#20026;&#20266;&#26631;&#31614;&#65292;&#35774;&#35745;&#32447;&#24615;&#22797;&#26434;&#24230;&#30340;&#27880;&#24847;&#26426;&#21046;&#26469;&#22788;&#29702;&#22823;&#35268;&#27169;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#23454;&#20363;&#12290;</title><link>https://arxiv.org/abs/2403.19561</link><description>&lt;p&gt;
&#33258;&#25105;&#25913;&#36827;&#23398;&#20064;&#29992;&#20110;&#21487;&#25193;&#23637;&#31070;&#32463;&#32452;&#21512;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Self-Improved Learning for Scalable Neural Combinatorial Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19561
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#25105;&#25913;&#36827;&#23398;&#20064;(SIL)&#26041;&#27861;&#65292;&#23454;&#29616;&#31070;&#32463;&#32452;&#21512;&#20248;&#21270;&#30340;&#26356;&#22909;&#21487;&#25193;&#23637;&#24615;&#65292;&#36890;&#36807;&#33258;&#36523;&#29983;&#25104;&#35299;&#20915;&#26041;&#26696;&#20316;&#20026;&#20266;&#26631;&#31614;&#65292;&#35774;&#35745;&#32447;&#24615;&#22797;&#26434;&#24230;&#30340;&#27880;&#24847;&#26426;&#21046;&#26469;&#22788;&#29702;&#22823;&#35268;&#27169;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#23454;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
end-to-end&#31070;&#32463;&#32452;&#21512;&#20248;&#21270;(NCO)&#26041;&#27861;&#22312;&#35299;&#20915;&#22797;&#26434;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#26041;&#38754;&#34920;&#29616;&#20986;&#26377;&#24076;&#26395;&#30340;&#24615;&#33021;&#65292;&#32780;&#19981;&#38656;&#35201;&#19987;&#23478;&#35774;&#35745;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#22312;&#22788;&#29702;&#22823;&#35268;&#27169;&#38382;&#39064;&#26102;&#23384;&#22312;&#22256;&#38590;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#23454;&#38469;&#36866;&#29992;&#24615;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#38480;&#21046;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#25105;&#25913;&#36827;&#23398;&#20064;(SIL)&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#31070;&#32463;&#32452;&#21512;&#20248;&#21270;&#30340;&#26356;&#22909;&#21487;&#25193;&#23637;&#24615;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#33258;&#25105;&#25913;&#36827;&#26426;&#21046;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#22312;&#27809;&#26377;&#26631;&#35760;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#30452;&#25509;&#22312;&#22823;&#35268;&#27169;&#38382;&#39064;&#23454;&#20363;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#36890;&#36807;&#19968;&#31181;&#21019;&#26032;&#30340;&#23616;&#37096;&#37325;&#26500;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#36890;&#36807;&#33258;&#36523;&#36845;&#20195;&#29983;&#25104;&#26356;&#22909;&#30340;&#35299;&#20915;&#26041;&#26696;&#20316;&#20026;&#20266;&#26631;&#31614;&#65292;&#20197;&#25351;&#23548;&#26377;&#25928;&#30340;&#27169;&#22411;&#35757;&#32451;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#32447;&#24615;&#22797;&#26434;&#24230;&#30340;&#27880;&#24847;&#26426;&#21046;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#26377;&#25928;&#22788;&#29702;&#20302;&#35745;&#31639;&#24320;&#38144;&#30340;&#22823;&#35268;&#27169;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#23454;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19561v1 Announce Type: cross  Abstract: The end-to-end neural combinatorial optimization (NCO) method shows promising performance in solving complex combinatorial optimization problems without the need for expert design. However, existing methods struggle with large-scale problems, hindering their practical applicability. To overcome this limitation, this work proposes a novel Self-Improved Learning (SIL) method for better scalability of neural combinatorial optimization. Specifically, we develop an efficient self-improved mechanism that enables direct model training on large-scale problem instances without any labeled data. Powered by an innovative local reconstruction approach, this method can iteratively generate better solutions by itself as pseudo-labels to guide efficient model training. In addition, we design a linear complexity attention mechanism for the model to efficiently handle large-scale combinatorial problem instances with low computation overhead. Comprehens
&lt;/p&gt;</description></item><item><title>Croissant&#26159;&#19968;&#31181;&#38754;&#21521;&#26426;&#22120;&#23398;&#20064;&#25968;&#25454;&#38598;&#30340;&#20803;&#25968;&#25454;&#26684;&#24335;&#65292;&#20351;&#25968;&#25454;&#38598;&#26356;&#26131;&#21457;&#29616;&#12289;&#21487;&#31227;&#26893;&#21644;&#20114;&#25805;&#20316;&#65292;&#26377;&#21161;&#20110;&#35299;&#20915;ML&#25968;&#25454;&#31649;&#29702;&#21644;&#36127;&#36131;&#20219;AI&#20013;&#30340;&#37325;&#35201;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.19546</link><description>&lt;p&gt;
Croissant&#65306;&#19968;&#31181;&#38754;&#21521;&#26426;&#22120;&#23398;&#20064;&#25968;&#25454;&#38598;&#30340;&#20803;&#25968;&#25454;&#26684;&#24335;
&lt;/p&gt;
&lt;p&gt;
Croissant: A Metadata Format for ML-Ready Datasets
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19546
&lt;/p&gt;
&lt;p&gt;
Croissant&#26159;&#19968;&#31181;&#38754;&#21521;&#26426;&#22120;&#23398;&#20064;&#25968;&#25454;&#38598;&#30340;&#20803;&#25968;&#25454;&#26684;&#24335;&#65292;&#20351;&#25968;&#25454;&#38598;&#26356;&#26131;&#21457;&#29616;&#12289;&#21487;&#31227;&#26893;&#21644;&#20114;&#25805;&#20316;&#65292;&#26377;&#21161;&#20110;&#35299;&#20915;ML&#25968;&#25454;&#31649;&#29702;&#21644;&#36127;&#36131;&#20219;AI&#20013;&#30340;&#37325;&#35201;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#26159;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#30340;&#20851;&#38190;&#36164;&#28304;&#65292;&#20294;&#22788;&#29702;&#25968;&#25454;&#20173;&#28982;&#26159;&#19968;&#20010;&#20027;&#35201;&#30340;&#25705;&#25830;&#28857;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;Croissant&#65292;&#19968;&#31181;&#29992;&#20110;&#25968;&#25454;&#38598;&#30340;&#20803;&#25968;&#25454;&#26684;&#24335;&#65292;&#31616;&#21270;&#20102;&#25968;&#25454;&#34987;ML&#24037;&#20855;&#21644;&#26694;&#26550;&#20351;&#29992;&#30340;&#26041;&#24335;&#12290;Croissant&#20351;&#25968;&#25454;&#38598;&#26356;&#26131;&#21457;&#29616;&#12289;&#21487;&#31227;&#26893;&#21644;&#20114;&#25805;&#20316;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;ML&#25968;&#25454;&#31649;&#29702;&#21644;&#36127;&#36131;&#20219;AI&#20013;&#30340;&#37325;&#35201;&#25361;&#25112;&#12290;Croissant&#24050;&#24471;&#21040;&#20960;&#20010;&#27969;&#34892;&#25968;&#25454;&#38598;&#24211;&#30340;&#25903;&#25345;&#65292;&#28085;&#30422;&#25968;&#21313;&#19975;&#20010;&#25968;&#25454;&#38598;&#65292;&#21487;&#20197;&#21152;&#36733;&#21040;&#26368;&#27969;&#34892;&#30340;ML&#26694;&#26550;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19546v1 Announce Type: cross  Abstract: Data is a critical resource for Machine Learning (ML), yet working with data remains a key friction point. This paper introduces Croissant, a metadata format for datasets that simplifies how data is used by ML tools and frameworks. Croissant makes datasets more discoverable, portable and interoperable, thereby addressing significant challenges in ML data management and responsible AI. Croissant is already supported by several popular dataset repositories, spanning hundreds of thousands of datasets, ready to be loaded into the most popular ML frameworks.
&lt;/p&gt;</description></item><item><title>&#25289;&#39532;&#20811;&#36951;&#20256;&#21407;&#29702;&#22312;&#21160;&#24577;&#29615;&#22659;&#20013;&#20248;&#21270;&#26426;&#22120;&#20154;&#36827;&#21270;&#65292;&#34920;&#29616;&#20986;&#27604;&#20256;&#32479;&#36798;&#23572;&#25991;&#27169;&#22411;&#26356;&#39640;&#30340;&#36866;&#24212;&#24615;&#21644;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.19545</link><description>&lt;p&gt;
&#25289;&#39532;&#20811;&#36951;&#20256;&#25913;&#21892;&#26426;&#22120;&#20154;&#22312;&#21160;&#24577;&#29615;&#22659;&#20013;&#30340;&#36827;&#21270;
&lt;/p&gt;
&lt;p&gt;
Lamarckian Inheritance Improves Robot Evolution in Dynamic Environments
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19545
&lt;/p&gt;
&lt;p&gt;
&#25289;&#39532;&#20811;&#36951;&#20256;&#21407;&#29702;&#22312;&#21160;&#24577;&#29615;&#22659;&#20013;&#20248;&#21270;&#26426;&#22120;&#20154;&#36827;&#21270;&#65292;&#34920;&#29616;&#20986;&#27604;&#20256;&#32479;&#36798;&#23572;&#25991;&#27169;&#22411;&#26356;&#39640;&#30340;&#36866;&#24212;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#23558;&#25289;&#39532;&#20811;&#31995;&#32479;&#25972;&#21512;&#21040;&#36827;&#21270;&#26426;&#22120;&#20154;&#23398;&#65288;ER&#65289;&#20013;&#65292;&#36890;&#36807;&#27604;&#36739;&#20854;&#19982;&#20256;&#32479;&#36798;&#23572;&#25991;&#27169;&#22411;&#22312;&#19981;&#21516;&#29615;&#22659;&#19979;&#30340;&#34920;&#29616;&#12290;&#36890;&#36807;&#37319;&#29992;&#25289;&#39532;&#20811;&#21407;&#21017;&#65292;&#21363;&#26426;&#22120;&#20154;&#32487;&#25215;&#23398;&#20064;&#21040;&#30340;&#29305;&#24449;&#65292;&#21516;&#26102;&#36827;&#34892;&#36798;&#23572;&#25991;&#23398;&#20064;&#32780;&#19981;&#32487;&#25215;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#21160;&#24577;&#35774;&#32622;&#20013;&#30340;&#36866;&#24212;&#24615;&#12290;&#25105;&#20204;&#22312;&#20845;&#20010;&#19981;&#21516;&#30340;&#29615;&#22659;&#35774;&#32622;&#20013;&#36827;&#34892;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#25289;&#39532;&#20811;&#31995;&#32479;&#22312;&#36866;&#24212;&#24615;&#21644;&#25928;&#29575;&#26041;&#38754;&#30340;&#34920;&#29616;&#20248;&#20110;&#36798;&#23572;&#25991;&#31995;&#32479;&#65292;&#29305;&#21035;&#26159;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#26465;&#20214;&#19979;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#31361;&#20986;&#20102;&#25511;&#21046;&#22120;&#21644;&#24418;&#24577;&#36827;&#21270;&#20197;&#21450;&#29615;&#22659;&#36866;&#24212;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#30340;&#20851;&#38190;&#20316;&#29992;&#65292;&#29238;&#20195;&#21644;&#23376;&#20195;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65292;&#20197;&#21450;&#23398;&#20064;&#21069;&#21518;&#30340;&#26032;&#29983;&#21644;&#24184;&#23384;&#32773;&#25552;&#20379;&#20102;&#26377;&#20851;&#29305;&#24449;&#32487;&#25215;&#26377;&#25928;&#24615;&#30340;&#35265;&#35299;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#25289;&#39532;&#20811;&#21407;&#21017;&#21487;&#33021;&#26174;&#33879;&#25512;&#21160;&#33258;&#20027;&#31995;&#32479;&#35774;&#35745;&#30340;&#36827;&#27493;&#65292;&#24378;&#35843;&#20102;&#26356;&#20016;&#23500;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19545v1 Announce Type: cross  Abstract: This study explores the integration of Lamarckian system into evolutionary robotics (ER), comparing it with the traditional Darwinian model across various environments. By adopting Lamarckian principles, where robots inherit learned traits, alongside Darwinian learning without inheritance, we investigate adaptation in dynamic settings. Our research, conducted in six distinct environmental setups, demonstrates that Lamarckian systems outperform Darwinian ones in adaptability and efficiency, particularly in challenging conditions. Our analysis highlights the critical role of the interplay between controller \&amp; morphological evolution and environment adaptation, with parent-offspring similarities and newborn \&amp;survivors before and after learning providing insights into the effectiveness of trait inheritance. Our findings suggest Lamarckian principles could significantly advance autonomous system design, highlighting the potential for more
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#28145;&#20837;&#30740;&#31350;Transformer-based&#35821;&#35328;&#27169;&#22411;&#22312;&#20107;&#23454;&#22238;&#24518;&#20219;&#21153;&#20013;&#30340;&#26426;&#21046;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#38646;/&#23569;&#27425;&#26679;&#26412;&#24773;&#20917;&#19979;&#30340;&#29305;&#23450;&#20219;&#21153;&#22836;&#12289;MLP&#23618;&#21644;&#27531;&#24046;&#27969;&#30340;&#21151;&#33021;&#65292;&#20197;&#21450;&#25239;&#36807;&#24230;&#33258;&#20449;&#26426;&#21046;&#12290;</title><link>https://arxiv.org/abs/2403.19521</link><description>&lt;p&gt;
&#35299;&#37322;&#22522;&#20110;Transformer&#27169;&#22411;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#20107;&#23454;&#22238;&#24518;&#20013;&#30340;&#20851;&#38190;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
Interpreting Key Mechanisms of Factual Recall in Transformer-Based Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19521
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#28145;&#20837;&#30740;&#31350;Transformer-based&#35821;&#35328;&#27169;&#22411;&#22312;&#20107;&#23454;&#22238;&#24518;&#20219;&#21153;&#20013;&#30340;&#26426;&#21046;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#38646;/&#23569;&#27425;&#26679;&#26412;&#24773;&#20917;&#19979;&#30340;&#29305;&#23450;&#20219;&#21153;&#22836;&#12289;MLP&#23618;&#21644;&#27531;&#24046;&#27969;&#30340;&#21151;&#33021;&#65292;&#20197;&#21450;&#25239;&#36807;&#24230;&#33258;&#20449;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#28145;&#20837;&#25506;&#35752;&#20102;Transformer-based&#35821;&#35328;&#27169;&#22411;&#22312;&#20107;&#23454;&#22238;&#24518;&#20219;&#21153;&#20013;&#25152;&#37319;&#29992;&#30340;&#26426;&#21046;&#12290;&#22312;&#38646;&#27425;&#26679;&#26412;&#24773;&#20917;&#19979;&#65292;&#32473;&#23450;&#31867;&#20284;&#8220;&#27861;&#22269;&#30340;&#39318;&#37117;&#26159;&#8221;&#30340;&#25552;&#31034;&#65292;&#29305;&#23450;&#20219;&#21153;&#30340;&#27880;&#24847;&#21147;&#22836;&#20250;&#20174;&#19978;&#19979;&#25991;&#20013;&#25552;&#21462;&#20027;&#39064;&#23454;&#20307;&#65292;&#22914;&#8220;&#27861;&#22269;&#8221;&#65292;&#24182;&#23558;&#20854;&#20256;&#36882;&#32473;&#21518;&#32493;&#30340;MLP&#20197;&#22238;&#24518;&#25152;&#38656;&#30340;&#31572;&#26696;&#65292;&#22914;&#8220;&#24052;&#40654;&#8221;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#26512;&#26041;&#27861;&#65292;&#26088;&#22312;&#23558;MLP&#30340;&#36755;&#20986;&#20998;&#35299;&#20026;&#20154;&#31867;&#21487;&#29702;&#35299;&#30340;&#32452;&#20214;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#37327;&#21270;&#20102;&#36319;&#38543;&#36825;&#20123;&#29305;&#23450;&#20219;&#21153;&#22836;&#30340;MLP&#23618;&#30340;&#21151;&#33021;&#12290;&#22312;&#27531;&#24046;&#27969;&#20013;&#65292;&#23427;&#20250;&#25830;&#38500;&#25110;&#25918;&#22823;&#26469;&#33258;&#21508;&#20010;&#22836;&#30340;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#23427;&#20250;&#29983;&#25104;&#19968;&#20010;&#32452;&#20214;&#65292;&#23558;&#27531;&#24046;&#27969;&#37325;&#26032;&#23450;&#21521;&#21040;&#39044;&#26399;&#31572;&#26696;&#30340;&#26041;&#21521;&#12290;&#36825;&#20123;&#38646;&#27425;&#26426;&#21046;&#20063;&#36866;&#29992;&#20110;&#23569;&#27425;&#26679;&#26412;&#24773;&#20917;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#19968;&#31181;&#24191;&#27867;&#23384;&#22312;&#30340;&#25239;&#36807;&#24230;&#33258;&#20449;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19521v1 Announce Type: cross  Abstract: In this paper, we deeply explore the mechanisms employed by Transformer-based language models in factual recall tasks. In zero-shot scenarios, given a prompt like "The capital of France is," task-specific attention heads extract the topic entity, such as "France," from the context and pass it to subsequent MLPs to recall the required answer such as "Paris." We introduce a novel analysis method aimed at decomposing the outputs of the MLP into components understandable by humans. Through this method, we quantify the function of the MLP layer following these task-specific heads. In the residual stream, it either erases or amplifies the information originating from individual heads. Moreover, it generates a component that redirects the residual stream towards the direction of its expected answer. These zero-shot mechanisms are also employed in few-shot scenarios. Additionally, we observed a widely existent anti-overconfidence mechanism in 
&lt;/p&gt;</description></item><item><title>RiEMann&#25552;&#20986;&#20102;&#19968;&#20010;&#36817;&#23454;&#26102;SE(3)&#31561;&#21464;&#26426;&#22120;&#20154;&#25805;&#20316;&#27169;&#20223;&#23398;&#20064;&#26694;&#26550;&#65292;&#26080;&#38656;&#28857;&#20113;&#20998;&#21106;&#65292;&#21487;&#20197;&#20174;&#38646;&#24320;&#22987;&#23398;&#20064;&#25805;&#20316;&#20219;&#21153;&#65292;&#27867;&#21270;&#21040;&#30475;&#19981;&#35265;&#30340;&#36716;&#25442;&#21644;&#30446;&#26631;&#23545;&#35937;&#23454;&#20363;&#65292;&#23545;&#25239;&#35270;&#35273;&#24178;&#25200;&#65292;&#23454;&#26102;&#36319;&#36394;&#30446;&#26631;&#23545;&#35937;&#30340;&#23039;&#21183;&#21464;&#21270;&#65292;&#21516;&#26102;&#20855;&#26377;&#21487;&#25193;&#23637;&#30340;&#21160;&#20316;&#31354;&#38388;&#20351;&#24471;&#20851;&#33410;&#23545;&#35937;&#25805;&#20316;&#25104;&#20026;&#21487;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.19460</link><description>&lt;p&gt;
RiEMann: &#19981;&#38656;&#35201;&#28857;&#20113;&#20998;&#21106;&#30340;&#36817;&#23454;&#26102; SE(3)&#31561;&#21464;&#26426;&#22120;&#20154;&#25805;&#20316;
&lt;/p&gt;
&lt;p&gt;
RiEMann: Near Real-Time SE(3)-Equivariant Robot Manipulation without Point Cloud Segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19460
&lt;/p&gt;
&lt;p&gt;
RiEMann&#25552;&#20986;&#20102;&#19968;&#20010;&#36817;&#23454;&#26102;SE(3)&#31561;&#21464;&#26426;&#22120;&#20154;&#25805;&#20316;&#27169;&#20223;&#23398;&#20064;&#26694;&#26550;&#65292;&#26080;&#38656;&#28857;&#20113;&#20998;&#21106;&#65292;&#21487;&#20197;&#20174;&#38646;&#24320;&#22987;&#23398;&#20064;&#25805;&#20316;&#20219;&#21153;&#65292;&#27867;&#21270;&#21040;&#30475;&#19981;&#35265;&#30340;&#36716;&#25442;&#21644;&#30446;&#26631;&#23545;&#35937;&#23454;&#20363;&#65292;&#23545;&#25239;&#35270;&#35273;&#24178;&#25200;&#65292;&#23454;&#26102;&#36319;&#36394;&#30446;&#26631;&#23545;&#35937;&#30340;&#23039;&#21183;&#21464;&#21270;&#65292;&#21516;&#26102;&#20855;&#26377;&#21487;&#25193;&#23637;&#30340;&#21160;&#20316;&#31354;&#38388;&#20351;&#24471;&#20851;&#33410;&#23545;&#35937;&#25805;&#20316;&#25104;&#20026;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;RiEMann&#65292;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#36817;&#23454;&#26102; SE(3)&#31561;&#21464;&#26426;&#22120;&#20154;&#25805;&#20316;&#27169;&#20223;&#23398;&#20064;&#26694;&#26550;&#65292;&#20174;&#22330;&#26223;&#28857;&#20113;&#36755;&#20837;&#20013;&#23398;&#20064;&#12290;&#19982;&#20808;&#21069;&#20381;&#36182;&#25551;&#36848;&#31526;&#21305;&#37197;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;RiEMann&#30452;&#25509;&#39044;&#27979;&#23545;&#35937;&#30340;&#30446;&#26631;&#23039;&#21183;&#36827;&#34892;&#25805;&#20316;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#20219;&#20309;&#23545;&#35937;&#20998;&#21106;&#12290;RiEMann&#21487;&#20197;&#20174;&#38646;&#24320;&#22987;&#23398;&#20064;&#19968;&#20010;&#25805;&#20316;&#20219;&#21153;&#65292;&#21482;&#38656;5&#21040;10&#20010;&#28436;&#31034;&#65292;&#21487;&#20197;&#27867;&#21270;&#21040;&#30475;&#19981;&#35265;&#30340;SE(3)&#36716;&#25442;&#21644;&#30446;&#26631;&#23545;&#35937;&#30340;&#23454;&#20363;&#65292;&#25269;&#25239;&#24178;&#25200;&#23545;&#35937;&#30340;&#35270;&#35273;&#24178;&#25200;&#65292;&#24182;&#36981;&#24490;&#30446;&#26631;&#23545;&#35937;&#30340;&#36817;&#23454;&#26102;&#23039;&#21183;&#21464;&#21270;&#12290;RiEMann&#30340;&#21487;&#20280;&#32553;&#21160;&#20316;&#31354;&#38388;&#26377;&#21161;&#20110;&#28155;&#21152;&#33258;&#23450;&#20041;&#31561;&#21464;&#21160;&#20316;&#65292;&#20363;&#22914;&#26059;&#36716;&#27700;&#40857;&#22836;&#30340;&#26041;&#21521;&#65292;&#36825;&#20351;&#24471;RiEMann&#21487;&#20197;&#36827;&#34892;&#20851;&#33410;&#23545;&#35937;&#25805;&#20316;&#12290;&#22312;&#27169;&#25311;&#21644;&#29616;&#23454;&#19990;&#30028;&#30340;6&#33258;&#30001;&#24230;&#26426;&#22120;&#20154;&#25805;&#20316;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#27979;&#35797;&#20102;RiEMann &#23545; 5&#31867;&#25805;&#32437;&#20219;&#21153;&#30340;25&#31181;&#21464;&#20307;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19460v1 Announce Type: cross  Abstract: We present RiEMann, an end-to-end near Real-time SE(3)-Equivariant Robot Manipulation imitation learning framework from scene point cloud input. Compared to previous methods that rely on descriptor field matching, RiEMann directly predicts the target poses of objects for manipulation without any object segmentation. RiEMann learns a manipulation task from scratch with 5 to 10 demonstrations, generalizes to unseen SE(3) transformations and instances of target objects, resists visual interference of distracting objects, and follows the near real-time pose change of the target object. The scalable action space of RiEMann facilitates the addition of custom equivariant actions such as the direction of turning the faucet, which makes articulated object manipulation possible for RiEMann. In simulation and real-world 6-DOF robot manipulation experiments, we test RiEMann on 5 categories of manipulation tasks with a total of 25 variants and show
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#32447;&#24615;&#36951;&#20256;&#35268;&#21010;&#20316;&#20026;&#34920;&#31034;&#65292;NeuroLGP-SM &#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20195;&#29702;&#27169;&#22411;&#36741;&#21161;&#31070;&#32463;&#36827;&#21270;&#30340;&#26041;&#27861;&#65292;&#20197;&#24212;&#23545;&#31070;&#32463;&#36827;&#21270;&#30340;&#35745;&#31639;&#25361;&#25112;&#65292;&#21516;&#26102;&#20445;&#25345;&#33391;&#22909;&#30340;DNN&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.19459</link><description>&lt;p&gt;
NeuroLGP-SM&#65306;&#20351;&#29992;&#32447;&#24615;&#36951;&#20256;&#35268;&#21010;&#30340;&#36741;&#21161;&#31070;&#32463;&#36827;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
NeuroLGP-SM: A Surrogate-assisted Neuroevolution Approach using Linear Genetic Programming
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19459
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#32447;&#24615;&#36951;&#20256;&#35268;&#21010;&#20316;&#20026;&#34920;&#31034;&#65292;NeuroLGP-SM &#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20195;&#29702;&#27169;&#22411;&#36741;&#21161;&#31070;&#32463;&#36827;&#21270;&#30340;&#26041;&#27861;&#65292;&#20197;&#24212;&#23545;&#31070;&#32463;&#36827;&#21270;&#30340;&#35745;&#31639;&#25361;&#25112;&#65292;&#21516;&#26102;&#20445;&#25345;&#33391;&#22909;&#30340;DNN&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36827;&#21270;&#31639;&#27861;&#36234;&#26469;&#36234;&#34987;&#35748;&#20026;&#26159;&#19968;&#31181;&#21487;&#34892;&#30340;&#35745;&#31639;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#33258;&#21160;&#20248;&#21270;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#12290;&#36825;&#31181;&#26041;&#27861;&#24310;&#20280;&#21040;DNNs&#30340;&#35757;&#32451;&#65292;&#21363;&#25152;&#35859;&#30340;&#31070;&#32463;&#36827;&#21270;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#31070;&#32463;&#36827;&#21270;&#26159;&#19968;&#20010;&#22266;&#26377;&#30340;&#36164;&#28304;&#23494;&#38598;&#22411;&#36807;&#31243;&#65292;&#26576;&#20123;&#30740;&#31350;&#25253;&#21578;&#31216;&#20026;&#25913;&#36827;&#21644;&#35757;&#32451;&#21333;&#20010;DNN&#32593;&#32476;&#28040;&#32791;&#20102;&#25968;&#21315;&#20010;GPU&#22825;&#12290;&#20026;&#20102;&#35299;&#20915;&#19982;&#31070;&#32463;&#36827;&#21270;&#30456;&#20851;&#30340;&#35745;&#31639;&#25361;&#25112;&#65292;&#21516;&#26102;&#20173;&#28982;&#33719;&#24471;&#33391;&#22909;&#30340;DNN&#20934;&#30830;&#24615;&#65292;&#20195;&#29702;&#27169;&#22411;&#20986;&#29616;&#20316;&#20026;&#19968;&#20010;&#20999;&#23454;&#21487;&#34892;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#23613;&#31649;&#20195;&#29702;&#27169;&#22411;&#26377;&#28508;&#21147;&#65292;&#20294;&#26159;&#23558;&#20195;&#29702;&#27169;&#22411;&#25972;&#21512;&#21040;&#31070;&#32463;&#36827;&#21270;&#20013;&#20173;&#22788;&#20110;&#36215;&#27493;&#38454;&#27573;&#65292;&#21463;&#21040;&#35832;&#22914;&#39640;&#32500;&#25968;&#25454;&#30340;&#26377;&#25928;&#21033;&#29992;&#21644;&#31070;&#32463;&#36827;&#21270;&#20013;&#37319;&#29992;&#30340;&#34920;&#31034;&#31561;&#22240;&#32032;&#30340;&#38459;&#30861;&#12290;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#32447;&#24615;&#36951;&#20256;&#35268;&#21010;&#30340;&#36866;&#24403;&#34920;&#31034;&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19459v1 Announce Type: cross  Abstract: Evolutionary algorithms are increasingly recognised as a viable computational approach for the automated optimisation of deep neural networks (DNNs) within artificial intelligence. This method extends to the training of DNNs, an approach known as neuroevolution. However, neuroevolution is an inherently resource-intensive process, with certain studies reporting the consumption of thousands of GPU days for refining and training a single DNN network. To address the computational challenges associated with neuroevolution while still attaining good DNN accuracy, surrogate models emerge as a pragmatic solution. Despite their potential, the integration of surrogate models into neuroevolution is still in its early stages, hindered by factors such as the effective use of high-dimensional data and the representation employed in neuroevolution. In this context, we address these challenges by employing a suitable representation based on Linear Gen
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#27861;&#26816;&#27979;&#24182;&#32416;&#27491;&#22269;&#23478;&#26292;&#21147;&#27515;&#20129;&#25253;&#21578;&#31995;&#32479;&#20013;&#30340;&#27880;&#37322;&#19981;&#19968;&#33268;&#24615;&#65292;&#25552;&#39640;&#20102;&#33258;&#26432;&#21361;&#26426;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.19432</link><description>&lt;p&gt;
&#36890;&#36807;&#27515;&#22240;&#35843;&#26597;&#31508;&#35760;&#20013;&#30340;&#27880;&#37322;&#19981;&#19968;&#33268;&#24615;&#26816;&#27979;&#25581;&#31034;&#33258;&#26432;&#21407;&#22240;&#30340;&#35823;&#24402;&#22240;
&lt;/p&gt;
&lt;p&gt;
Uncovering Misattributed Suicide Causes through Annotation Inconsistency Detection in Death Investigation Notes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19432
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#27861;&#26816;&#27979;&#24182;&#32416;&#27491;&#22269;&#23478;&#26292;&#21147;&#27515;&#20129;&#25253;&#21578;&#31995;&#32479;&#20013;&#30340;&#27880;&#37322;&#19981;&#19968;&#33268;&#24615;&#65292;&#25552;&#39640;&#20102;&#33258;&#26432;&#21361;&#26426;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#20934;&#30830;&#24615;&#23545;&#31185;&#23398;&#30740;&#31350;&#21644;&#25919;&#31574;&#21046;&#23450;&#33267;&#20851;&#37325;&#35201;&#12290;&#22269;&#23478;&#26292;&#21147;&#27515;&#20129;&#25253;&#21578;&#31995;&#32479;&#65288;NVDRS&#65289;&#25968;&#25454;&#34987;&#24191;&#27867;&#29992;&#20110;&#21457;&#29616;&#27515;&#20129;&#30340;&#27169;&#24335;&#21644;&#21407;&#22240;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;NVDRS&#20869;&#23384;&#22312;&#27880;&#37322;&#19981;&#19968;&#33268;&#24615;&#65292;&#24182;&#21487;&#33021;&#24433;&#21709;&#38169;&#35823;&#30340;&#33258;&#26432;&#21407;&#22240;&#24402;&#22240;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32463;&#39564;&#24615;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#26041;&#27861;&#26469;&#26816;&#27979;&#27880;&#37322;&#19981;&#19968;&#33268;&#24615;&#65292;&#24182;&#37319;&#29992;&#31867;&#20284;&#20132;&#21449;&#39564;&#35777;&#30340;&#33539;&#24335;&#26469;&#35782;&#21035;&#26377;&#38382;&#39064;&#30340;&#23454;&#20363;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;2003&#24180;&#33267;2020&#24180;&#38388;&#20174;NVDRS&#20013;&#30340;267,804&#36215;&#33258;&#26432;&#27515;&#20129;&#26696;&#20363;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#23558;&#30446;&#26631;&#24030;&#30340;&#25968;&#25454;&#32435;&#20837;&#35757;&#32451;&#33258;&#26432;&#21361;&#26426;&#20998;&#31867;&#22120;&#65292;&#20351;&#24471;&#22312;&#30446;&#26631;&#24030;&#27979;&#35797;&#38598;&#19978;&#30340;F-1&#20998;&#25968;&#22686;&#21152;&#20102;5.4&#65285;&#65292;&#22312;&#20854;&#20182;&#24030;&#27979;&#35797;&#38598;&#19978;&#38477;&#20302;&#20102;1.1&#65285;&#12290;&#24635;&#20043;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;NVDRS&#27515;&#22240;&#35843;&#26597;&#31508;&#35760;&#20013;&#30340;&#27880;&#37322;&#19981;&#19968;&#33268;&#24615;&#65292;&#24182;&#30830;&#23450;&#20102;&#38382;&#39064;&#30340;&#23454;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19432v1 Announce Type: cross  Abstract: Data accuracy is essential for scientific research and policy development. The National Violent Death Reporting System (NVDRS) data is widely used for discovering the patterns and causes of death. Recent studies suggested the annotation inconsistencies within the NVDRS and the potential impact on erroneous suicide-cause attributions. We present an empirical Natural Language Processing (NLP) approach to detect annotation inconsistencies and adopt a cross-validation-like paradigm to identify problematic instances. We analyzed 267,804 suicide death incidents between 2003 and 2020 from the NVDRS. Our results showed that incorporating the target state's data into training the suicide-crisis classifier brought an increase of 5.4% to the F-1 score on the target state's test set and a decrease of 1.1% on other states' test set. To conclude, we demonstrated the annotation inconsistencies in NVDRS's death investigation notes, identified problema
&lt;/p&gt;</description></item><item><title>&#27492;&#30740;&#31350;&#20174;&#35821;&#35328;&#23398;&#35282;&#24230;&#30740;&#31350;&#20102;&#19981;&#21516;&#26041;&#27861;&#20043;&#38388;&#30340;&#35299;&#37322;&#19981;&#19968;&#33268;&#65292;&#21457;&#29616;&#22312;&#21477;&#27861;&#36328;&#24230;&#27700;&#24179;&#19978;&#27604;&#36739;&#26041;&#27861;&#21487;&#20197;&#24179;&#28369;&#25481;&#26631;&#35760;&#32423;&#21035;&#30340;&#24046;&#24322;&#65292;&#25552;&#20986;&#20102;&#21160;&#24577;&#20272;&#35745;&#26368;&#37325;&#35201;&#36328;&#24230;&#30340;&#26041;&#27861;&#65292;&#25913;&#36827;&#20102;&#36873;&#25321;&#37325;&#35201;&#26631;&#35760;&#30340;&#37197;&#32622;&#12290;</title><link>https://arxiv.org/abs/2403.19424</link><description>&lt;p&gt;
&#35821;&#27861;&#36328;&#24230;&#20559;&#22909;&#22312;&#20107;&#21518;&#35299;&#37322;&#19981;&#19968;&#33268;&#20013;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
The Role of Syntactic Span Preferences in Post-Hoc Explanation Disagreement
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19424
&lt;/p&gt;
&lt;p&gt;
&#27492;&#30740;&#31350;&#20174;&#35821;&#35328;&#23398;&#35282;&#24230;&#30740;&#31350;&#20102;&#19981;&#21516;&#26041;&#27861;&#20043;&#38388;&#30340;&#35299;&#37322;&#19981;&#19968;&#33268;&#65292;&#21457;&#29616;&#22312;&#21477;&#27861;&#36328;&#24230;&#27700;&#24179;&#19978;&#27604;&#36739;&#26041;&#27861;&#21487;&#20197;&#24179;&#28369;&#25481;&#26631;&#35760;&#32423;&#21035;&#30340;&#24046;&#24322;&#65292;&#25552;&#20986;&#20102;&#21160;&#24577;&#20272;&#35745;&#26368;&#37325;&#35201;&#36328;&#24230;&#30340;&#26041;&#27861;&#65292;&#25913;&#36827;&#20102;&#36873;&#25321;&#37325;&#35201;&#26631;&#35760;&#30340;&#37197;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20107;&#21518;&#35299;&#37322;&#26041;&#27861;&#26159;&#22686;&#21152;&#27169;&#22411;&#36879;&#26126;&#24230;&#23545;&#29992;&#25143;&#26469;&#35828;&#26159;&#19968;&#20010;&#37325;&#35201;&#24037;&#20855;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#30446;&#21069;&#29992;&#20110;&#24402;&#22240;&#26631;&#35760;&#37325;&#35201;&#24615;&#30340;&#26041;&#27861;&#32463;&#24120;&#20135;&#29983;&#19981;&#21516;&#30340;&#27169;&#24335;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20174;&#35821;&#35328;&#23398;&#35282;&#24230;&#30740;&#31350;&#20102;&#26041;&#27861;&#20043;&#38388;&#19981;&#19968;&#33268;&#30340;&#28508;&#22312;&#26469;&#28304;&#12290;&#25105;&#20204;&#21457;&#29616;&#19981;&#21516;&#30340;&#26041;&#27861;&#31995;&#32479;&#22320;&#36873;&#25321;&#19981;&#21516;&#31867;&#21035;&#30340;&#35789;&#65292;&#24182;&#19988;&#37027;&#20123;&#19982;&#20854;&#20182;&#26041;&#27861;&#21644;&#20154;&#31867;&#36798;&#25104;&#26368;&#39640;&#19968;&#33268;&#24615;&#30340;&#26041;&#27861;&#23637;&#29616;&#20986;&#31867;&#20284;&#30340;&#35821;&#35328;&#20559;&#22909;&#12290;&#22914;&#26524;&#25105;&#20204;&#22312;&#21477;&#27861;&#36328;&#24230;&#27700;&#24179;&#19978;&#27604;&#36739;&#26041;&#27861;&#65292;&#37027;&#20040;&#26041;&#27861;&#20043;&#38388;&#30340;&#26631;&#35760;&#32423;&#24046;&#24322;&#23601;&#20250;&#34987;&#24179;&#28369;&#25481;&#12290;&#36890;&#36807;&#21160;&#24577;&#20272;&#35745;&#26368;&#37325;&#35201;&#30340;&#36328;&#24230;&#32780;&#19981;&#26159;&#20381;&#36182;&#20110;&#22266;&#23450;&#22823;&#23567;&#20026;$k$&#30340;&#23376;&#38598;&#65292;&#25105;&#20204;&#21457;&#29616;&#26041;&#27861;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#26356;&#39640;&#12290;&#25105;&#20204;&#31995;&#32479;&#22320;&#35843;&#26597;&#20102;$k$&#21644;&#36328;&#24230;&#20043;&#38388;&#30340;&#20132;&#20114;&#20316;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#36873;&#25321;&#37325;&#35201;&#26631;&#35760;&#30340;&#25913;&#36827;&#37197;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19424v1 Announce Type: cross  Abstract: Post-hoc explanation methods are an important tool for increasing model transparency for users. Unfortunately, the currently used methods for attributing token importance often yield diverging patterns. In this work, we study potential sources of disagreement across methods from a linguistic perspective. We find that different methods systematically select different classes of words and that methods that agree most with other methods and with humans display similar linguistic preferences. Token-level differences between methods are smoothed out if we compare them on the syntactic span level. We also find higher agreement across methods by estimating the most important spans dynamically instead of relying on a fixed subset of size $k$. We systematically investigate the interaction between $k$ and spans and propose an improved configuration for selecting important tokens.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35780;&#20272;&#20102;&#19981;&#21516;&#30340;&#24182;&#34892;&#21270;&#25216;&#26415;&#65292;&#20197;&#32553;&#30701;&#23725;&#22238;&#24402;&#33041;&#32534;&#30721;&#27169;&#22411;&#35757;&#32451;&#26102;&#38388;&#65292;&#24182;&#22312;&#22823;&#35268;&#27169;&#28145;&#24230;fMRI&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;</title><link>https://arxiv.org/abs/2403.19421</link><description>&lt;p&gt;
&#22312;&#22823;&#35268;&#27169;&#20010;&#20307;fMRI&#25968;&#25454;&#38598;&#20013;&#25193;&#23637;&#23725;&#22238;&#24402;&#36827;&#34892;&#33041;&#32534;&#30721;
&lt;/p&gt;
&lt;p&gt;
Scaling up ridge regression for brain encoding in a massive individual fMRI dataset
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19421
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35780;&#20272;&#20102;&#19981;&#21516;&#30340;&#24182;&#34892;&#21270;&#25216;&#26415;&#65292;&#20197;&#32553;&#30701;&#23725;&#22238;&#24402;&#33041;&#32534;&#30721;&#27169;&#22411;&#35757;&#32451;&#26102;&#38388;&#65292;&#24182;&#22312;&#22823;&#35268;&#27169;&#28145;&#24230;fMRI&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#31070;&#32463;&#24433;&#20687;&#25968;&#25454;&#36827;&#34892;&#22823;&#33041;&#32534;&#30721;&#26159;&#19968;&#31181;&#26088;&#22312;&#30452;&#25509;&#20174;&#22797;&#26434;&#21050;&#28608;&#29305;&#24449;&#65288;&#22914;&#30005;&#24433;&#24103;&#65289;&#39044;&#27979;&#20154;&#31867;&#22823;&#33041;&#27963;&#21160;&#30340;&#20998;&#26512;&#26041;&#27861;&#12290;&#23725;&#22238;&#24402;&#26159;&#19968;&#31181;&#27969;&#34892;&#30340;&#33041;&#32534;&#30721;&#39044;&#27979;&#27169;&#22411;&#65292;&#22240;&#20026;&#23427;&#20855;&#26377;&#33391;&#22909;&#30340;&#26679;&#26412;&#22806;&#27867;&#21270;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22312;&#22788;&#29702;&#21253;&#21547;&#35768;&#22810;&#22823;&#35268;&#27169;&#28145;&#24230;&#21151;&#33021;&#30913;&#20849;&#25391;&#25104;&#20687;&#65288;fMRI&#65289;&#25968;&#25454;&#38598;&#26102;&#65292;&#35757;&#32451;&#23725;&#22238;&#24402;&#27169;&#22411;&#21487;&#33021;&#38750;&#24120;&#32791;&#26102;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#21253;&#25324;&#35768;&#22810;&#33041;&#27963;&#21160;&#30340;&#31354;&#38388;-&#26102;&#38388;&#26679;&#26412;&#12290;&#26412;&#25991;&#35780;&#20272;&#20102;&#19981;&#21516;&#30340;&#24182;&#34892;&#21270;&#25216;&#26415;&#65292;&#20197;&#20943;&#23569;&#22312;CNeuroMod Friends&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;&#23725;&#22238;&#24402;&#36827;&#34892;&#33041;&#32534;&#30721;&#30340;&#35757;&#32451;&#26102;&#38388;&#65292;&#35813;&#25968;&#25454;&#38598;&#26159;&#30446;&#21069;&#21487;&#29992;&#30340;&#26368;&#22823;&#30340;&#28145;&#24230;fMRI&#36164;&#28304;&#20043;&#19968;&#12290;&#36890;&#36807;&#22810;&#32447;&#31243;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;Intel Math Kernel&#24211;&#65288;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19421v1 Announce Type: cross  Abstract: Brain encoding with neuroimaging data is an established analysis aimed at predicting human brain activity directly from complex stimuli features such as movie frames. Typically, these features are the latent space representation from an artificial neural network, and the stimuli are image, audio, or text inputs. Ridge regression is a popular prediction model for brain encoding due to its good out-of-sample generalization performance. However, training a ridge regression model can be highly time-consuming when dealing with large-scale deep functional magnetic resonance imaging (fMRI) datasets that include many space-time samples of brain activity. This paper evaluates different parallelization techniques to reduce the training time of brain encoding with ridge regression on the CNeuroMod Friends dataset, one of the largest deep fMRI resource currently available. With multi-threading, our results show that the Intel Math Kernel Library (
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#25490;&#21517;&#21518;&#22788;&#29702;&#30340;&#38543;&#26426;&#21270;&#26041;&#27861;&#65292;&#26080;&#38656;&#21463;&#20445;&#25252;&#23646;&#24615;&#65292;&#36890;&#36807;&#25968;&#20540;&#30740;&#31350;&#26174;&#31034;&#20102;&#26041;&#27861;&#30456;&#23545;&#20110;&#22522;&#32447;&#25490;&#21517;&#30340;P-&#20844;&#24179;&#24615;&#21644;&#30456;&#23545;&#20110;&#24402;&#19968;&#21270;&#25240;&#25187;&#32047;&#35745;&#22686;&#30410;(NDCG)&#30340;&#26377;&#25928;&#24615;&#30340;&#31283;&#20581;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.19419</link><description>&lt;p&gt;
&#25490;&#21517;&#20013;&#30340;&#20844;&#24179;&#24615;&#65306;&#36890;&#36807;&#38543;&#26426;&#21270;&#23454;&#29616;&#25239;&#24178;&#25200;&#32780;&#26080;&#38656;&#21463;&#20445;&#25252;&#23646;&#24615;
&lt;/p&gt;
&lt;p&gt;
Fairness in Ranking: Robustness through Randomization without the Protected Attribute
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19419
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#25490;&#21517;&#21518;&#22788;&#29702;&#30340;&#38543;&#26426;&#21270;&#26041;&#27861;&#65292;&#26080;&#38656;&#21463;&#20445;&#25252;&#23646;&#24615;&#65292;&#36890;&#36807;&#25968;&#20540;&#30740;&#31350;&#26174;&#31034;&#20102;&#26041;&#27861;&#30456;&#23545;&#20110;&#22522;&#32447;&#25490;&#21517;&#30340;P-&#20844;&#24179;&#24615;&#21644;&#30456;&#23545;&#20110;&#24402;&#19968;&#21270;&#25240;&#25187;&#32047;&#35745;&#22686;&#30410;(NDCG)&#30340;&#26377;&#25928;&#24615;&#30340;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#20110;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#20844;&#24179;&#24615;&#65292;&#23588;&#20854;&#26159;&#19982;&#20998;&#31867;&#38382;&#39064;&#26377;&#20851;&#30340;&#20844;&#24179;&#24615;&#65292;&#24341;&#36215;&#20102;&#26497;&#22823;&#20852;&#36259;&#12290;&#22312;&#28041;&#21450;&#25490;&#21517;&#30340;&#38382;&#39064;&#20013;&#65292;&#22914;&#22312;&#32447;&#24191;&#21578;&#12289;&#25512;&#33616;&#31995;&#32479;&#21644;&#20154;&#21147;&#36164;&#28304;&#33258;&#21160;&#21270;&#20013;&#65292;&#20173;&#28982;&#38656;&#35201;&#36827;&#34892;&#22823;&#37327;&#20844;&#24179;&#24615;&#26041;&#38754;&#30340;&#24037;&#20316;&#12290;&#20004;&#20010;&#22797;&#26434;&#20043;&#22788;&#22312;&#20110;&#65306;&#39318;&#20808;&#65292;&#22312;&#35768;&#22810;&#24212;&#29992;&#31243;&#24207;&#20013;&#21487;&#33021;&#26080;&#27861;&#33719;&#21462;&#21463;&#20445;&#25252;&#23646;&#24615;&#12290;&#20854;&#27425;&#65292;&#25490;&#21517;&#30340;&#20844;&#24179;&#24615;&#23384;&#22312;&#22810;&#20010;&#34913;&#37327;&#26631;&#20934;&#65292;&#22522;&#20110;&#21333;&#20010;&#34913;&#37327;&#26631;&#20934;&#30340;&#20248;&#21270;&#26041;&#27861;&#21487;&#33021;&#20250;&#20135;&#29983;&#30456;&#23545;&#20854;&#20182;&#34913;&#37327;&#26631;&#20934;&#19981;&#20844;&#24179;&#30340;&#25490;&#21517;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#25490;&#21517;&#21518;&#22788;&#29702;&#30340;&#38543;&#26426;&#21270;&#26041;&#27861;&#65292;&#19981;&#38656;&#35201;&#21463;&#20445;&#25252;&#23646;&#24615;&#30340;&#21487;&#29992;&#24615;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#25968;&#20540;&#30740;&#31350;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30456;&#23545;&#20110;&#22522;&#32447;&#25490;&#21517;&#30340;P-&#20844;&#24179;&#24615;&#21644;&#30456;&#23545;&#20110;&#24402;&#19968;&#21270;&#25240;&#25187;&#32047;&#35745;&#22686;&#30410;(NDCG)&#30340;&#25928;&#26524;&#30340;&#31283;&#20581;&#24615;&#65292;&#25913;&#36827;&#20102;&#20808;&#21069;&#25552;&#20986;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19419v1 Announce Type: cross  Abstract: There has been great interest in fairness in machine learning, especially in relation to classification problems. In ranking-related problems, such as in online advertising, recommender systems, and HR automation, much work on fairness remains to be done. Two complications arise: first, the protected attribute may not be available in many applications. Second, there are multiple measures of fairness of rankings, and optimization-based methods utilizing a single measure of fairness of rankings may produce rankings that are unfair with respect to other measures. In this work, we propose a randomized method for post-processing rankings, which do not require the availability of the protected attribute. In an extensive numerical study, we show the robustness of our methods with respect to P-Fairness and effectiveness with respect to Normalized Discounted Cumulative Gain (NDCG) from the baseline ranking, improving on previously proposed meth
&lt;/p&gt;</description></item><item><title>&#25361;&#25112;&#24120;&#29992;&#30340;&#24207;&#25968;&#32534;&#30721;&#65292;&#25552;&#20986;&#22522;&#20110;&#23383;&#31526;&#20018;&#30456;&#20284;&#24615;&#32534;&#30721;&#30340;&#34920;&#26684;&#23398;&#20064;&#26041;&#27861;&#65292;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#20998;&#31867;&#25928;&#26524;&#21644;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2403.19405</link><description>&lt;p&gt;
&#34920;&#26684;&#23398;&#20064;&#65306;&#23454;&#20307;&#21644;&#19978;&#19979;&#25991;&#23884;&#20837;&#30340;&#32534;&#30721;
&lt;/p&gt;
&lt;p&gt;
Tabular Learning: Encoding for Entity and Context Embeddings
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19405
&lt;/p&gt;
&lt;p&gt;
&#25361;&#25112;&#24120;&#29992;&#30340;&#24207;&#25968;&#32534;&#30721;&#65292;&#25552;&#20986;&#22522;&#20110;&#23383;&#31526;&#20018;&#30456;&#20284;&#24615;&#32534;&#30721;&#30340;&#34920;&#26684;&#23398;&#20064;&#26041;&#27861;&#65292;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#20998;&#31867;&#25928;&#26524;&#21644;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19981;&#21516;&#32534;&#30721;&#25216;&#26415;&#23545;&#23454;&#20307;&#21644;&#19978;&#19979;&#25991;&#23884;&#20837;&#30340;&#24433;&#21709;&#65292;&#26088;&#22312;&#25361;&#25112;&#24120;&#29992;&#30340;&#24207;&#25968;&#32534;&#30721;&#22312;&#34920;&#26684;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#24212;&#29992;&#19981;&#21516;&#30340;&#39044;&#22788;&#29702;&#26041;&#27861;&#21644;&#32593;&#32476;&#26550;&#26500;&#65292;&#23545;&#32534;&#30721;&#22120;&#22914;&#20309;&#24433;&#21709;&#32593;&#32476;&#23398;&#20064;&#32467;&#26524;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#36890;&#36807;&#20445;&#25345;&#27979;&#35797;&#12289;&#39564;&#35777;&#21644;&#35757;&#32451;&#25968;&#25454;&#30340;&#19968;&#33268;&#24615;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#23545;&#20110;&#20998;&#31867;&#25968;&#25454;&#65292;&#24207;&#25968;&#32534;&#30721;&#24182;&#19981;&#26159;&#26368;&#21512;&#36866;&#30340;&#32534;&#30721;&#22120;&#65292;&#26080;&#27861;&#27491;&#30830;&#39044;&#22788;&#29702;&#25968;&#25454;&#24182;&#20998;&#31867;&#30446;&#26631;&#21464;&#37327;&#12290;&#36890;&#36807;&#22522;&#20110;&#23383;&#31526;&#20018;&#30456;&#20284;&#24615;&#23545;&#29305;&#24449;&#36827;&#34892;&#32534;&#30721;&#65292;&#35745;&#31639;&#30456;&#20284;&#24615;&#30697;&#38453;&#20316;&#20026;&#32593;&#32476;&#36755;&#20837;&#65292;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;&#36825;&#36866;&#29992;&#20110;&#23454;&#20307;&#21644;&#19978;&#19979;&#25991;&#23884;&#20837;&#65292;&#22312;&#22810;&#26631;&#31614;&#20998;&#31867;&#20013;&#65292;&#21464;&#25442;&#22120;&#26550;&#26500;&#22312;&#24207;&#25968;&#32534;&#30721;&#21644;&#30456;&#20284;&#24615;&#32534;&#30721;&#26041;&#38754;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19405v1 Announce Type: cross  Abstract: Examining the effect of different encoding techniques on entity and context embeddings, the goal of this work is to challenge commonly used Ordinal encoding for tabular learning. Applying different preprocessing methods and network architectures over several datasets resulted in a benchmark on how the encoders influence the learning outcome of the networks. By keeping the test, validation and training data consistent, results have shown that ordinal encoding is not the most suited encoder for categorical data in terms of preprocessing the data and thereafter, classifying the target variable correctly. A better outcome was achieved, encoding the features based on string similarities by computing a similarity matrix as input for the network. This is the case for both, entity and context embeddings, where the transformer architecture showed improved performance for Ordinal and Similarity encoding with regard to multi-label classification 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#23454;&#20363;&#32423;&#26816;&#32034;&#20219;&#21153;&#65306;PointCloud-Text&#21305;&#37197;&#65288;PTM&#65289;&#65292;&#24182;&#26500;&#24314;&#20102;&#19977;&#20010;&#26032;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#20197;&#35299;&#20915;&#25968;&#25454;&#31232;&#30095;&#12289;&#25991;&#26412;&#27169;&#31946;&#31561;&#25361;&#25112;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;RoMa&#26041;&#27861;&#20316;&#20026;PTM&#30340;&#22522;&#32447;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2403.19386</link><description>&lt;p&gt;
PointCloud-Text&#21305;&#37197;&#65306;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#19968;&#20010;&#22522;&#32447;
&lt;/p&gt;
&lt;p&gt;
PointCloud-Text Matching: Benchmark Datasets and a Baseline
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19386
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#23454;&#20363;&#32423;&#26816;&#32034;&#20219;&#21153;&#65306;PointCloud-Text&#21305;&#37197;&#65288;PTM&#65289;&#65292;&#24182;&#26500;&#24314;&#20102;&#19977;&#20010;&#26032;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#20197;&#35299;&#20915;&#25968;&#25454;&#31232;&#30095;&#12289;&#25991;&#26412;&#27169;&#31946;&#31561;&#25361;&#25112;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;RoMa&#26041;&#27861;&#20316;&#20026;PTM&#30340;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#21644;&#30740;&#31350;&#20102;&#19968;&#20010;&#26032;&#30340;&#23454;&#20363;&#32423;&#26816;&#32034;&#20219;&#21153;&#65306;PointCloud-Text Matching&#65288;PTM&#65289;&#65292;&#26088;&#22312;&#25214;&#21040;&#19982;&#32473;&#23450;&#30340;&#28857;&#20113;&#26597;&#35810;&#25110;&#25991;&#26412;&#26597;&#35810;&#21305;&#37197;&#30340;&#30830;&#20999;&#36328;&#27169;&#24577;&#23454;&#20363;&#12290;PTM&#21487;&#24212;&#29992;&#20110;&#21508;&#31181;&#22330;&#26223;&#65292;&#22914;&#23460;&#20869;/&#22478;&#24066;&#23777;&#35895;&#23450;&#20301;&#21644;&#22330;&#26223;&#26816;&#32034;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#23578;&#26080;&#36866;&#29992;&#30340;&#12289;&#26377;&#38024;&#23545;&#24615;&#30340;PTM&#25968;&#25454;&#38598;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19977;&#20010;&#26032;&#30340;PTM&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#20998;&#21035;&#20026;3D2T-SR&#12289;3D2T-NR&#21644;3D2T-QA&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#25968;&#25454;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#30001;&#20110;&#28857;&#20113;&#30340;&#31232;&#30095;&#12289;&#22122;&#22768;&#25110;&#26080;&#24207;&#65292;&#20197;&#21450;&#25991;&#26412;&#30340;&#27169;&#31946;&#12289;&#21547;&#31946;&#25110;&#19981;&#23436;&#25972;&#65292;&#23548;&#33268;&#23384;&#22312;&#22024;&#26434;&#30340;&#23545;&#24212;&#20851;&#31995;&#65292;&#20351;&#24471;&#29616;&#26377;&#30340;&#36328;&#27169;&#24577;&#21305;&#37197;&#26041;&#27861;&#23545;PTM&#26080;&#25928;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;PTM&#22522;&#32447;&#65292;&#21629;&#21517;&#20026;Robust PointCloud-Text Matching&#26041;&#27861;&#65288;RoMa&#65289;&#12290;RoMa&#21253;&#21547;&#20004;&#20010;&#27169;&#22359;&#65306;&#21452;&#37325;&#27880;&#24847;&#24863;&#30693;&#27169;&#22359;&#65288;DAP&#65289;&#21644;&#40065;&#26834;&#36127;&#23545;&#27604;&#27169;&#22359;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19386v1 Announce Type: cross  Abstract: In this paper, we present and study a new instance-level retrieval task: PointCloud-Text Matching~(PTM), which aims to find the exact cross-modal instance that matches a given point-cloud query or text query. PTM could be applied to various scenarios, such as indoor/urban-canyon localization and scene retrieval. However, there exists no suitable and targeted dataset for PTM in practice. Therefore, we construct three new PTM benchmark datasets, namely 3D2T-SR, 3D2T-NR, and 3D2T-QA. We observe that the data is challenging and with noisy correspondence due to the sparsity, noise, or disorder of point clouds and the ambiguity, vagueness, or incompleteness of texts, which make existing cross-modal matching methods ineffective for PTM. To tackle these challenges, we propose a PTM baseline, named Robust PointCloud-Text Matching method (RoMa). RoMa consists of two modules: a Dual Attention Perception module (DAP) and a Robust Negative Contrast
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#39318;&#27425;&#20351;&#29992;&#26469;&#33258;&#21363;&#25554;&#21363;&#29992;&#30340;&#38388;&#25509;&#39134;&#34892;&#26102;&#38388;&#20256;&#24863;&#22120;&#30340;&#25968;&#25454;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#33021;&#22815;&#23558;&#20809;&#32447;&#21453;&#23556;&#21457;&#29983;&#30340;&#34920;&#38754;&#37325;&#26032;&#26500;&#24314;&#20026;&#34394;&#25311;&#38236;&#23376;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#33719;&#21462;&#38544;&#34255;&#22330;&#26223;&#28145;&#24230;&#20449;&#24687;&#30340;&#21487;&#34892;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.19376</link><description>&lt;p&gt;
NIGHT -- &#38388;&#25509;&#39134;&#34892;&#26102;&#38388;&#25968;&#25454;&#30340;&#38750;&#35270;&#36317;&#25104;&#20687;
&lt;/p&gt;
&lt;p&gt;
NIGHT -- Non-Line-of-Sight Imaging from Indirect Time of Flight Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19376
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#20351;&#29992;&#26469;&#33258;&#21363;&#25554;&#21363;&#29992;&#30340;&#38388;&#25509;&#39134;&#34892;&#26102;&#38388;&#20256;&#24863;&#22120;&#30340;&#25968;&#25454;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#33021;&#22815;&#23558;&#20809;&#32447;&#21453;&#23556;&#21457;&#29983;&#30340;&#34920;&#38754;&#37325;&#26032;&#26500;&#24314;&#20026;&#34394;&#25311;&#38236;&#23376;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#33719;&#21462;&#38544;&#34255;&#22330;&#26223;&#28145;&#24230;&#20449;&#24687;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#38750;&#35270;&#35282;&#30456;&#26426;&#22806;&#37096;&#33719;&#21462;&#29289;&#20307;&#26159;&#19968;&#20010;&#38750;&#24120;&#24341;&#20154;&#27880;&#30446;&#20294;&#20063;&#26497;&#20855;&#25361;&#25112;&#24615;&#30340;&#30740;&#31350;&#35838;&#39064;&#12290;&#26368;&#36817;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#21033;&#29992;&#23450;&#21046;&#30340;&#30452;&#25509;&#39134;&#34892;&#26102;&#38388;&#20256;&#24863;&#22120;&#20135;&#29983;&#30340;&#30636;&#26102;&#25104;&#20687;&#25968;&#25454;&#65292;&#36825;&#20010;&#24819;&#27861;&#26159;&#21487;&#34892;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#20351;&#29992;&#26469;&#33258;&#21363;&#25554;&#21363;&#29992;&#30340;&#38388;&#25509;&#39134;&#34892;&#26102;&#38388;&#20256;&#24863;&#22120;&#30340;&#25968;&#25454;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#32780;&#19981;&#38656;&#35201;&#20219;&#20309;&#39069;&#22806;&#30340;&#30828;&#20214;&#35201;&#27714;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#33021;&#22815;&#23558;&#20809;&#32447;&#21453;&#23556;&#21457;&#29983;&#30340;&#34920;&#38754;&#37325;&#26032;&#26500;&#24314;&#20026;&#34394;&#25311;&#38236;&#23376;&#12290;&#36825;&#31181;&#24314;&#27169;&#20351;&#24471;&#20219;&#21153;&#26356;&#23481;&#26131;&#22788;&#29702;&#65292;&#20063;&#26377;&#21161;&#20110;&#26500;&#24314;&#24102;&#26377;&#27880;&#37322;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#20174;&#33719;&#24471;&#30340;&#25968;&#25454;&#20013;&#65292;&#21487;&#20197;&#24674;&#22797;&#38544;&#34255;&#22330;&#26223;&#30340;&#28145;&#24230;&#20449;&#24687;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#39318;&#21019;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#29992;&#20110;&#36825;&#20010;&#20219;&#21153;&#65292;&#24182;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;&#24819;&#27861;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19376v1 Announce Type: cross  Abstract: The acquisition of objects outside the Line-of-Sight of cameras is a very intriguing but also extremely challenging research topic. Recent works showed the feasibility of this idea exploiting transient imaging data produced by custom direct Time of Flight sensors. In this paper, for the first time, we tackle this problem using only data from an off-the-shelf indirect Time of Flight sensor without any further hardware requirement. We introduced a Deep Learning model able to re-frame the surfaces where light bounces happen as a virtual mirror. This modeling makes the task easier to handle and also facilitates the construction of annotated training data. From the obtained data it is possible to retrieve the depth information of the hidden scene. We also provide a first-in-its-kind synthetic dataset for the task and demonstrate the feasibility of the proposed idea over it.
&lt;/p&gt;</description></item><item><title>BAHE&#25552;&#20986;&#20102;&#34892;&#20026;&#32858;&#21512;&#20998;&#23618;&#32534;&#30721;&#65288;BAHE&#65289;&#26469;&#22686;&#24378;LLM-based CTR&#24314;&#27169;&#30340;&#25928;&#29575;&#65292;&#36890;&#36807;&#35299;&#32806;&#29992;&#25143;&#34892;&#20026;&#30340;&#32534;&#30721;&#19982;&#34892;&#20026;&#20043;&#38388;&#30340;&#20132;&#20114;&#12290;</title><link>https://arxiv.org/abs/2403.19347</link><description>&lt;p&gt;
&#25171;&#30772;&#38271;&#24230;&#38480;&#21046;&#65306;LLM&#22686;&#24378;&#38271;&#25991;&#26412;&#29992;&#25143;&#34892;&#20026;&#20013;&#30340;CTR&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Breaking the Length Barrier: LLM-Enhanced CTR Prediction in Long Textual User Behaviors
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19347
&lt;/p&gt;
&lt;p&gt;
BAHE&#25552;&#20986;&#20102;&#34892;&#20026;&#32858;&#21512;&#20998;&#23618;&#32534;&#30721;&#65288;BAHE&#65289;&#26469;&#22686;&#24378;LLM-based CTR&#24314;&#27169;&#30340;&#25928;&#29575;&#65292;&#36890;&#36807;&#35299;&#32806;&#29992;&#25143;&#34892;&#20026;&#30340;&#32534;&#30721;&#19982;&#34892;&#20026;&#20043;&#38388;&#30340;&#20132;&#20114;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20852;&#36215;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#21033;&#29992;LLMs&#25552;&#39640;&#20102;&#28857;&#20987;&#29575;&#65288;CTR&#65289;&#39044;&#27979;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35748;&#20026;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#37096;&#32626;LLMs&#20173;&#28982;&#23384;&#22312;&#19968;&#20010;&#20851;&#38190;&#38556;&#30861;&#65306;LLMs&#22312;&#22788;&#29702;&#38271;&#25991;&#26412;&#29992;&#25143;&#34892;&#20026;&#26102;&#30340;&#25928;&#29575;&#12290;&#38543;&#30528;&#29992;&#25143;&#24207;&#21015;&#21464;&#24471;&#26356;&#38271;&#65292;&#24403;&#21069;&#30340;LLMs&#25928;&#29575;&#19981;&#36275;&#20197;&#22312;&#25968;&#21313;&#20159;&#29992;&#25143;&#21644;&#39033;&#30446;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#20026;&#20102;&#31361;&#30772;LLMs&#30340;&#25928;&#29575;&#38556;&#30861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#34892;&#20026;&#32858;&#21512;&#20998;&#23618;&#32534;&#30721;&#65288;BAHE&#65289;&#26469;&#22686;&#24378;&#22522;&#20110;LLM&#30340;CTR&#24314;&#27169;&#30340;&#25928;&#29575;&#12290;&#20855;&#20307;&#22320;&#65292;BAHE&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#23618;&#26550;&#26500;&#65292;&#23558;&#29992;&#25143;&#34892;&#20026;&#30340;&#32534;&#30721;&#19982;&#34892;&#20026;&#20043;&#38388;&#30340;&#20132;&#20114;&#35299;&#32806;&#12290;&#39318;&#20808;&#65292;&#20026;&#20102;&#38450;&#27490;&#30001;&#20110;&#37325;&#22797;&#32534;&#30721;&#30456;&#21516;&#29992;&#25143;&#34892;&#20026;&#32780;&#20135;&#29983;&#30340;&#35745;&#31639;&#20887;&#20313;&#65292;BAHE&#21033;&#29992;LLM&#30340;&#39044;&#35757;&#32451;&#27973;&#23618;&#26469;&#25552;&#21462;&#26368;&#31890;&#24230;&#30340;&#21407;&#23376;&#29992;&#25143;&#34892;&#20026;&#30340;&#23884;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19347v1 Announce Type: cross  Abstract: With the rise of large language models (LLMs), recent works have leveraged LLMs to improve the performance of click-through rate (CTR) prediction. However, we argue that a critical obstacle remains in deploying LLMs for practical use: the efficiency of LLMs when processing long textual user behaviors. As user sequences grow longer, the current efficiency of LLMs is inadequate for training on billions of users and items. To break through the efficiency barrier of LLMs, we propose Behavior Aggregated Hierarchical Encoding (BAHE) to enhance the efficiency of LLM-based CTR modeling. Specifically, BAHE proposes a novel hierarchical architecture that decouples the encoding of user behaviors from inter-behavior interactions. Firstly, to prevent computational redundancy from repeated encoding of identical user behaviors, BAHE employs the LLM's pre-trained shallow layers to extract embeddings of the most granular, atomic user behaviors from ext
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#27604;&#36739;&#20102;&#20256;&#32479;&#30005;&#23376;&#21830;&#21153;&#21830;&#21697;&#20998;&#31867;&#31995;&#32479;&#21644;&#20010;&#24615;&#21270;&#25512;&#33616;&#31995;&#32479;&#30340;&#36816;&#20316;&#26426;&#21046;&#65292;&#38416;&#36848;&#20102;&#20010;&#24615;&#21270;&#25512;&#33616;&#31995;&#32479;&#22312;&#30005;&#23376;&#21830;&#21153;&#31561;&#39046;&#22495;&#20013;&#30340;&#37325;&#35201;&#24615;&#21644;&#24212;&#29992;&#65292;&#21516;&#26102;&#28145;&#20837;&#25506;&#35752;&#20102;&#30456;&#20851;&#31995;&#32479;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.19345</link><description>&lt;p&gt;
&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26234;&#33021;&#30005;&#23376;&#21830;&#21153;&#20135;&#21697;&#20998;&#31867;&#19982;&#20010;&#24615;&#21270;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Intelligent Classification and Personalized Recommendation of E-commerce Products Based on Machine Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19345
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#27604;&#36739;&#20102;&#20256;&#32479;&#30005;&#23376;&#21830;&#21153;&#21830;&#21697;&#20998;&#31867;&#31995;&#32479;&#21644;&#20010;&#24615;&#21270;&#25512;&#33616;&#31995;&#32479;&#30340;&#36816;&#20316;&#26426;&#21046;&#65292;&#38416;&#36848;&#20102;&#20010;&#24615;&#21270;&#25512;&#33616;&#31995;&#32479;&#22312;&#30005;&#23376;&#21830;&#21153;&#31561;&#39046;&#22495;&#20013;&#30340;&#37325;&#35201;&#24615;&#21644;&#24212;&#29992;&#65292;&#21516;&#26102;&#28145;&#20837;&#25506;&#35752;&#20102;&#30456;&#20851;&#31995;&#32479;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20114;&#32852;&#32593;&#30340;&#36805;&#36895;&#21457;&#23637;&#21644;&#20449;&#24687;&#30340;&#25351;&#25968;&#32423;&#22686;&#38271;&#65292;&#29992;&#25143;&#36935;&#21040;&#20102;&#20449;&#24687;&#36807;&#36733;&#21644;&#36873;&#25321;&#22256;&#22659;&#12290;&#20010;&#24615;&#21270;&#25512;&#33616;&#31995;&#32479;&#36890;&#36807;&#24110;&#21161;&#29992;&#25143;&#31579;&#36873;&#21644;&#36873;&#25321;&#31526;&#21512;&#20854;&#20559;&#22909;&#21644;&#38656;&#27714;&#30340;&#20449;&#24687;&#65292;&#20174;&#32780;&#22312;&#32531;&#35299;&#36825;&#19968;&#36127;&#25285;&#26041;&#38754;&#21457;&#25381;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#36825;&#20123;&#31995;&#32479;&#19981;&#20165;&#25552;&#21319;&#20102;&#29992;&#25143;&#20307;&#39564;&#21644;&#28385;&#24847;&#24230;&#65292;&#36824;&#20026;&#20225;&#19994;&#21644;&#24179;&#21488;&#25552;&#20379;&#20102;&#22686;&#21152;&#29992;&#25143;&#21442;&#19982;&#24230;&#12289;&#38144;&#21806;&#39069;&#21644;&#24191;&#21578;&#25928;&#26524;&#30340;&#26426;&#20250;&#12290;&#26412;&#25991;&#23545;&#20256;&#32479;&#30005;&#23376;&#21830;&#21153;&#21830;&#21697;&#20998;&#31867;&#31995;&#32479;&#21644;&#20010;&#24615;&#21270;&#25512;&#33616;&#31995;&#32479;&#30340;&#36816;&#20316;&#26426;&#21046;&#36827;&#34892;&#20102;&#27604;&#36739;&#20998;&#26512;&#65292;&#38416;&#36848;&#20102;&#20010;&#24615;&#21270;&#25512;&#33616;&#31995;&#32479;&#22312;&#30005;&#23376;&#21830;&#21153;&#12289;&#20869;&#23481;&#20449;&#24687;&#21644;&#23186;&#20307;&#39046;&#22495;&#30340;&#37325;&#35201;&#24615;&#21644;&#24212;&#29992;&#12290;&#27492;&#22806;&#65292;&#36824;&#28145;&#20837;&#25506;&#35752;&#20102;&#30005;&#23376;&#21830;&#21153;&#20013;&#20010;&#24615;&#21270;&#25512;&#33616;&#31995;&#32479;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19345v1 Announce Type: cross  Abstract: With the rapid evolution of the Internet and the exponential proliferation of information, users encounter information overload and the conundrum of choice. Personalized recommendation systems play a pivotal role in alleviating this burden by aiding users in filtering and selecting information tailored to their preferences and requirements. Such systems not only enhance user experience and satisfaction but also furnish opportunities for businesses and platforms to augment user engagement, sales, and advertising efficacy.This paper undertakes a comparative analysis between the operational mechanisms of traditional e-commerce commodity classification systems and personalized recommendation systems. It delineates the significance and application of personalized recommendation systems across e-commerce, content information, and media domains. Furthermore, it delves into the challenges confronting personalized recommendation systems in e-co
&lt;/p&gt;</description></item><item><title>Dataverse&#26159;&#19968;&#20010;&#38754;&#21521;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24320;&#28304;ETL&#31649;&#36947;&#65292;&#25552;&#20379;&#20102;&#29992;&#25143;&#21451;&#22909;&#30340;&#35774;&#35745;&#21644;&#26131;&#20110;&#23450;&#21046;&#30340;&#22788;&#29702;&#22120;&#28155;&#21152;&#21151;&#33021;&#65292;&#26088;&#22312;&#25104;&#20026;LLM&#24320;&#21457;&#30340;&#37325;&#35201;&#24037;&#20855;&#65292;&#24182;&#24320;&#28304;&#25972;&#20010;&#24211;&#20197;&#20419;&#36827;&#31038;&#21306;&#36129;&#29486;&#12290;</title><link>https://arxiv.org/abs/2403.19340</link><description>&lt;p&gt;
Dataverse&#65306;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24320;&#28304;ETL&#65288;&#25277;&#21462;&#12289;&#36716;&#25442;&#12289;&#21152;&#36733;&#65289;&#31649;&#36947;
&lt;/p&gt;
&lt;p&gt;
Dataverse: Open-Source ETL (Extract, Transform, Load) Pipeline for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19340
&lt;/p&gt;
&lt;p&gt;
Dataverse&#26159;&#19968;&#20010;&#38754;&#21521;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24320;&#28304;ETL&#31649;&#36947;&#65292;&#25552;&#20379;&#20102;&#29992;&#25143;&#21451;&#22909;&#30340;&#35774;&#35745;&#21644;&#26131;&#20110;&#23450;&#21046;&#30340;&#22788;&#29702;&#22120;&#28155;&#21152;&#21151;&#33021;&#65292;&#26088;&#22312;&#25104;&#20026;LLM&#24320;&#21457;&#30340;&#37325;&#35201;&#24037;&#20855;&#65292;&#24182;&#24320;&#28304;&#25972;&#20010;&#24211;&#20197;&#20419;&#36827;&#31038;&#21306;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#35299;&#20915;&#35268;&#27169;&#21270;&#25968;&#25454;&#22788;&#29702;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Dataverse&#65292;&#19968;&#20010;&#32479;&#19968;&#30340;&#38754;&#21521;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24320;&#28304;&#25277;&#21462;-&#36716;&#25442;-&#21152;&#36733;&#65288;ETL&#65289;&#31649;&#36947;&#65292;&#20854;&#26680;&#24515;&#20855;&#26377;&#29992;&#25143;&#21451;&#22909;&#30340;&#35774;&#35745;&#12290;&#22312;Dataverse&#20013;&#65292;&#36890;&#36807;&#22522;&#20110;&#22359;&#30340;&#30028;&#38754;&#36731;&#26494;&#28155;&#21152;&#33258;&#23450;&#20041;&#22788;&#29702;&#22120;&#65292;&#20351;&#29992;&#25143;&#21487;&#20197;&#26041;&#20415;&#39640;&#25928;&#22320;&#20351;&#29992;Dataverse&#26500;&#24314;&#33258;&#24049;&#30340;ETL&#31649;&#36947;&#12290;&#25105;&#20204;&#24076;&#26395;Dataverse&#23558;&#25104;&#20026;LLM&#24320;&#21457;&#30340;&#37325;&#35201;&#24037;&#20855;&#65292;&#24182;&#24320;&#25918;&#25972;&#20010;&#24211;&#20197;&#27426;&#36814;&#31038;&#21306;&#36129;&#29486;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#31616;&#27905;&#30340;&#12289;&#20004;&#20998;&#38047;&#30340;&#31995;&#32479;&#28436;&#31034;&#35270;&#39057;&#65292;&#23637;&#31034;&#20854;&#21151;&#33021;&#21644;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19340v1 Announce Type: cross  Abstract: To address the challenges associated with data processing at scale, we propose Dataverse, a unified open-source Extract-Transform-Load (ETL) pipeline for large language models (LLMs) with a user-friendly design at its core. Easy addition of custom processors with block-based interface in Dataverse allows users to readily and efficiently use Dataverse to build their own ETL pipeline. We hope that Dataverse will serve as a vital tool for LLM development and open source the entire library to welcome community contribution. Additionally, we provide a concise, two-minute video demonstration of our system, illustrating its capabilities and implementation.
&lt;/p&gt;</description></item><item><title>IVLMap&#20026;&#26426;&#22120;&#20154;&#23548;&#33322;&#25552;&#20379;&#20102;&#23454;&#20363;&#32423;&#21644;&#23646;&#24615;&#32423;&#35821;&#20041;&#26144;&#23556;&#33021;&#21147;&#65292;&#36890;&#36807;&#23558;RGBD&#35270;&#39057;&#25968;&#25454;&#19982;&#29305;&#23450;&#35774;&#35745;&#30340;&#33258;&#28982;&#35821;&#35328;&#22320;&#22270;&#32034;&#24341;&#30456;&#34701;&#21512;&#32780;&#23454;&#29616;&#12290;</title><link>https://arxiv.org/abs/2403.19336</link><description>&lt;p&gt;
IVLMap&#65306;&#38024;&#23545;&#28040;&#36153;&#32423;&#26426;&#22120;&#20154;&#23548;&#33322;&#30340;&#23454;&#20363;&#24863;&#30693;&#35270;&#35273;&#35821;&#35328;&#22522;&#30784;
&lt;/p&gt;
&lt;p&gt;
IVLMap: Instance-Aware Visual Language Grounding for Consumer Robot Navigation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19336
&lt;/p&gt;
&lt;p&gt;
IVLMap&#20026;&#26426;&#22120;&#20154;&#23548;&#33322;&#25552;&#20379;&#20102;&#23454;&#20363;&#32423;&#21644;&#23646;&#24615;&#32423;&#35821;&#20041;&#26144;&#23556;&#33021;&#21147;&#65292;&#36890;&#36807;&#23558;RGBD&#35270;&#39057;&#25968;&#25454;&#19982;&#29305;&#23450;&#35774;&#35745;&#30340;&#33258;&#28982;&#35821;&#35328;&#22320;&#22270;&#32034;&#24341;&#30456;&#34701;&#21512;&#32780;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19336v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#36328;&#30028;&#25688;&#35201;&#65306;&#35270;&#35273;&#19982;&#35821;&#35328;&#23548;&#33322;&#65288;VLN&#65289;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#38656;&#35201;&#26426;&#22120;&#20154;&#22312;&#30495;&#23454;&#29615;&#22659;&#20013;&#20351;&#29992;&#20154;&#31867;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#36827;&#34892;&#23548;&#33322;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#26500;&#24314;&#29615;&#22659;&#30340;&#35821;&#20041;&#31354;&#38388;&#22320;&#22270;&#34920;&#31034;&#65292;&#28982;&#21518;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#29702;&#26041;&#38754;&#30340;&#24378;&#22823;&#33021;&#21147;&#26469;&#25512;&#24191;&#29992;&#20110;&#24341;&#23548;&#26426;&#22120;&#20154;&#23548;&#33322;&#30340;&#20195;&#30721;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#23454;&#20363;&#32423;&#21644;&#23646;&#24615;&#32423;&#23548;&#33322;&#20219;&#21153;&#20013;&#38754;&#20020;&#38480;&#21046;&#65292;&#22240;&#20026;&#23427;&#20204;&#26080;&#27861;&#21306;&#20998;&#21516;&#19968;&#23545;&#35937;&#30340;&#19981;&#21516;&#23454;&#20363;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21363;&#23454;&#20363;&#24863;&#30693;&#35270;&#35273;&#35821;&#35328;&#22320;&#22270;&#65288;IVLMap&#65289;&#65292;&#20197;&#36171;&#20104;&#26426;&#22120;&#20154;&#23454;&#20363;&#32423;&#21644;&#23646;&#24615;&#32423;&#35821;&#20041;&#26144;&#23556;&#65292;&#20854;&#20013;&#36890;&#36807;&#23558;&#26426;&#22120;&#20154;&#20195;&#29702;&#25910;&#38598;&#30340;RGBD&#35270;&#39057;&#25968;&#25454;&#19982;&#40479;&#30640;&#35270;&#35282;&#20013;&#29305;&#21035;&#35774;&#35745;&#30340;&#33258;&#28982;&#35821;&#35328;&#22320;&#22270;&#32034;&#24341;&#34701;&#21512;&#26469;&#33258;&#21160;&#26500;&#24314;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19336v1 Announce Type: cross  Abstract: Vision-and-Language Navigation (VLN) is a challenging task that requires a robot to navigate in photo-realistic environments with human natural language promptings. Recent studies aim to handle this task by constructing the semantic spatial map representation of the environment, and then leveraging the strong ability of reasoning in large language models for generalizing code for guiding the robot navigation. However, these methods face limitations in instance-level and attribute-level navigation tasks as they cannot distinguish different instances of the same object. To address this challenge, we propose a new method, namely, Instance-aware Visual Language Map (IVLMap), to empower the robot with instance-level and attribute-level semantic mapping, where it is autonomously constructed by fusing the RGBD video data collected from the robot agent with special-designed natural language map indexing in the bird's-in-eye view. Such indexing
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;HyperMV&#30340;&#22810;&#35270;&#35282;&#20107;&#20214;&#21160;&#20316;&#35782;&#21035;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#23558;&#31163;&#25955;&#20107;&#20214;&#25968;&#25454;&#36716;&#25442;&#25104;&#24103;&#29366;&#34920;&#31034;&#65292;&#24182;&#21033;&#29992;&#20849;&#20139;&#21367;&#31215;&#32593;&#32476;&#25552;&#21462;&#35270;&#35282;&#30456;&#20851;&#29305;&#24449;&#12290;</title><link>https://arxiv.org/abs/2403.19316</link><description>&lt;p&gt;
&#22522;&#20110;&#36229;&#22270;&#30340;&#22810;&#35270;&#35282;&#20107;&#20214;&#30456;&#26426;&#21160;&#20316;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Hypergraph-based Multi-View Action Recognition using Event Cameras
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19316
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;HyperMV&#30340;&#22810;&#35270;&#35282;&#20107;&#20214;&#21160;&#20316;&#35782;&#21035;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#23558;&#31163;&#25955;&#20107;&#20214;&#25968;&#25454;&#36716;&#25442;&#25104;&#24103;&#29366;&#34920;&#31034;&#65292;&#24182;&#21033;&#29992;&#20849;&#20139;&#21367;&#31215;&#32593;&#32476;&#25552;&#21462;&#35270;&#35282;&#30456;&#20851;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#39057;&#25968;&#25454;&#30340;&#21160;&#20316;&#35782;&#21035;&#26159;&#20855;&#26377;&#24191;&#27867;&#24212;&#29992;&#30340;&#22522;&#30707;&#12290;&#21333;&#35270;&#35282;&#21160;&#20316;&#35782;&#21035;&#30001;&#20110;&#20381;&#36182;&#21333;&#19968;&#35270;&#35282;&#32780;&#38754;&#20020;&#38480;&#21046;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#22810;&#35270;&#35282;&#26041;&#27861;&#20174;&#19981;&#21516;&#35270;&#35282;&#25429;&#33719;&#20114;&#34917;&#20449;&#24687;&#20197;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;&#26368;&#36817;&#65292;&#20107;&#20214;&#30456;&#26426;&#20316;&#20026;&#21019;&#26032;&#30340;&#20223;&#29983;&#20256;&#24863;&#22120;&#23853;&#38706;&#22836;&#35282;&#65292;&#20026;&#22522;&#20110;&#20107;&#20214;&#30340;&#21160;&#20316;&#35782;&#21035;&#24102;&#26469;&#20102;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#20316;&#21697;&#20027;&#35201;&#20851;&#27880;&#21333;&#19968;&#35270;&#35282;&#22330;&#26223;&#65292;&#22312;&#22810;&#35270;&#35282;&#20107;&#20214;&#25968;&#25454;&#21033;&#29992;&#26041;&#38754;&#23384;&#22312;&#31354;&#30333;&#65292;&#29305;&#21035;&#26159;&#22312;&#20449;&#24687;&#19981;&#36275;&#21644;&#35821;&#20041;&#38169;&#37197;&#31561;&#25361;&#25112;&#26041;&#38754;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;HyperMV&#65292;&#36825;&#26159;&#19968;&#20010;&#22810;&#35270;&#35282;&#22522;&#20110;&#20107;&#20214;&#30340;&#21160;&#20316;&#35782;&#21035;&#26694;&#26550;&#12290;HyperMV&#23558;&#31163;&#25955;&#20107;&#20214;&#25968;&#25454;&#36716;&#25442;&#25104;&#31867;&#20284;&#24103;&#30340;&#34920;&#31034;&#65292;&#24182;&#20351;&#29992;&#20849;&#20139;&#30340;&#21367;&#31215;&#32593;&#32476;&#25552;&#21462;&#19982;&#35270;&#35282;&#30456;&#20851;&#30340;&#29305;&#24449;&#12290;&#36890;&#36807;&#23558;&#27573;&#35270;&#20026;&#39030;&#28857;&#24182;&#20351;&#29992;&#22522;&#20110;&#35268;&#21017;&#30340;&#26041;&#27861;&#26500;&#24314;&#36229;&#36793;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19316v1 Announce Type: cross  Abstract: Action recognition from video data forms a cornerstone with wide-ranging applications. Single-view action recognition faces limitations due to its reliance on a single viewpoint. In contrast, multi-view approaches capture complementary information from various viewpoints for improved accuracy. Recently, event cameras have emerged as innovative bio-inspired sensors, leading to advancements in event-based action recognition. However, existing works predominantly focus on single-view scenarios, leaving a gap in multi-view event data exploitation, particularly in challenges like information deficit and semantic misalignment. To bridge this gap, we introduce HyperMV, a multi-view event-based action recognition framework. HyperMV converts discrete event data into frame-like representations and extracts view-related features using a shared convolutional network. By treating segments as vertices and constructing hyperedges using rule-based and
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;MATEval&#26694;&#26550;&#65292;&#21033;&#29992;&#22810;&#20010;&#31867;GPT-4&#30340;LLMs&#20316;&#20026;&#35780;&#20272;Agent&#65292;&#27169;&#25311;&#20154;&#31867;&#21512;&#20316;&#35752;&#35770;&#26041;&#27861;&#65292;&#20197;&#35780;&#20272;&#24320;&#25918;&#24615;&#25991;&#26412;&#65292;&#32467;&#21512;&#33258;&#25105;&#21453;&#24605;&#21644;&#24605;&#32500;&#38142;&#31574;&#30053;&#65292;&#24182;&#21152;&#20837;&#21453;&#39304;&#26426;&#21046;&#65292;&#25552;&#21319;&#35780;&#20272;&#28145;&#24230;&#21644;&#24191;&#24230;&#12290;</title><link>https://arxiv.org/abs/2403.19305</link><description>&lt;p&gt;
MATEval&#65306;&#29992;&#20110;&#25512;&#36827;&#24320;&#25918;&#24615;&#25991;&#26412;&#35780;&#20272;&#30340;&#22810;Agent&#35752;&#35770;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
MATEval: A Multi-Agent Discussion Framework for Advancing Open-Ended Text Evaluation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19305
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;MATEval&#26694;&#26550;&#65292;&#21033;&#29992;&#22810;&#20010;&#31867;GPT-4&#30340;LLMs&#20316;&#20026;&#35780;&#20272;Agent&#65292;&#27169;&#25311;&#20154;&#31867;&#21512;&#20316;&#35752;&#35770;&#26041;&#27861;&#65292;&#20197;&#35780;&#20272;&#24320;&#25918;&#24615;&#25991;&#26412;&#65292;&#32467;&#21512;&#33258;&#25105;&#21453;&#24605;&#21644;&#24605;&#32500;&#38142;&#31574;&#30053;&#65292;&#24182;&#21152;&#20837;&#21453;&#39304;&#26426;&#21046;&#65292;&#25552;&#21319;&#35780;&#20272;&#28145;&#24230;&#21644;&#24191;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#29983;&#25104;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36827;&#23637;&#20196;&#20154;&#30633;&#30446;&#65292;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#29983;&#25104;&#30340;&#25991;&#26412;&#36136;&#37327;&#32463;&#24120;&#26292;&#38706;&#20986;&#25345;&#32493;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;&#35780;&#20272;&#36825;&#20123;&#27169;&#22411;&#29983;&#25104;&#30340;&#25991;&#26412;&#36136;&#37327;&#65292;&#29305;&#21035;&#26159;&#22312;&#24320;&#25918;&#24615;&#25991;&#26412;&#20013;&#65292;&#19968;&#30452;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;LLMs&#20316;&#20026;&#35780;&#20272;&#32773;&#30340;&#21487;&#33021;&#24615;&#12290;&#34429;&#28982;&#20351;&#29992;&#21333;&#20010;LLM&#20316;&#20026;&#35780;&#20272;Agent&#34920;&#29616;&#20986;&#28508;&#21147;&#65292;&#20294;&#21364;&#23384;&#22312;&#26174;&#33879;&#30340;&#19981;&#30830;&#23450;&#24615;&#21644;&#19981;&#31283;&#23450;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; MATEval&#65306;&#19968;&#31181;&#8220;&#22810;Agent&#25991;&#26412;&#35780;&#20272;&#26694;&#26550;&#8221;&#65292;&#20854;&#20013;&#25152;&#26377;Agent&#37117;&#30001;&#20687;GPT-4&#30340;LLMs&#25198;&#28436;&#12290;MATEval&#26694;&#26550;&#27169;&#25311;&#20154;&#31867;&#21327;&#20316;&#35752;&#35770;&#26041;&#27861;&#65292;&#25972;&#21512;&#22810;&#20010;Agent&#30340;&#20114;&#21160;&#26469;&#35780;&#20272;&#24320;&#25918;&#24615;&#25991;&#26412;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#32467;&#21512;&#20102;&#33258;&#25105;&#21453;&#24605;&#21644;&#8220;&#24605;&#32500;&#38142;&#8221;&#31574;&#30053;&#65292;&#20197;&#21450;&#21453;&#39304;&#26426;&#21046;&#65292;&#22686;&#24378;&#20102;&#35780;&#20272;&#30340;&#28145;&#24230;&#21644;&#24191;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19305v1 Announce Type: cross  Abstract: Recent advancements in generative Large Language Models(LLMs) have been remarkable, however, the quality of the text generated by these models often reveals persistent issues. Evaluating the quality of text generated by these models, especially in open-ended text, has consistently presented a significant challenge. Addressing this, recent work has explored the possibility of using LLMs as evaluators. While using a single LLM as an evaluation agent shows potential, it is filled with significant uncertainty and instability. To address these issues, we propose the MATEval: A "Multi-Agent Text Evaluation framework" where all agents are played by LLMs like GPT-4. The MATEval framework emulates human collaborative discussion methods, integrating multiple agents' interactions to evaluate open-ended text. Our framework incorporates self-reflection and Chain-of-Thought (CoT) strategies, along with feedback mechanisms, enhancing the depth and br
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22270;&#31070;&#32463;&#32593;&#32476;&#26469;&#20943;&#23569;&#27835;&#30103;&#25928;&#26524;&#39044;&#27979;&#25152;&#38656;&#30340;&#35757;&#32451;&#38598;&#22823;&#23567;&#65292;&#26377;&#25928;&#21033;&#29992;&#30005;&#23376;&#21830;&#21153;&#25968;&#25454;&#30340;&#22270;&#32467;&#26500;&#65292;&#20026;&#27835;&#30103;&#25928;&#26524;&#39044;&#27979;&#24102;&#26469;&#26032;&#30340;&#21487;&#33021;&#24615;</title><link>https://arxiv.org/abs/2403.19289</link><description>&lt;p&gt;
&#29992;&#20110;&#27835;&#30103;&#25928;&#26524;&#39044;&#27979;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks for Treatment Effect Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19289
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22270;&#31070;&#32463;&#32593;&#32476;&#26469;&#20943;&#23569;&#27835;&#30103;&#25928;&#26524;&#39044;&#27979;&#25152;&#38656;&#30340;&#35757;&#32451;&#38598;&#22823;&#23567;&#65292;&#26377;&#25928;&#21033;&#29992;&#30005;&#23376;&#21830;&#21153;&#25968;&#25454;&#30340;&#22270;&#32467;&#26500;&#65292;&#20026;&#27835;&#30103;&#25928;&#26524;&#39044;&#27979;&#24102;&#26469;&#26032;&#30340;&#21487;&#33021;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30005;&#23376;&#21830;&#21153;&#20013;&#20272;&#35745;&#22240;&#26524;&#25928;&#24212;&#24448;&#24448;&#28041;&#21450;&#26114;&#36149;&#30340;&#27835;&#30103;&#20998;&#37197;&#65292;&#36825;&#22312;&#22823;&#35268;&#27169;&#35774;&#32622;&#20013;&#21487;&#33021;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#12290;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#26469;&#39044;&#27979;&#36825;&#31181;&#27835;&#30103;&#25928;&#26524;&#32780;&#26080;&#38656;&#23454;&#38469;&#24178;&#39044;&#26159;&#20943;&#23569;&#39118;&#38505;&#30340;&#19968;&#31181;&#26631;&#20934;&#20570;&#27861;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#27835;&#30103;&#25928;&#26524;&#39044;&#27979;&#26041;&#27861;&#24448;&#24448;&#20381;&#36182;&#20110;&#22823;&#35268;&#27169;&#23454;&#39564;&#26500;&#24314;&#30340;&#35757;&#32451;&#38598;&#65292;&#22240;&#27492;&#20174;&#26681;&#26412;&#19978;&#23384;&#22312;&#39118;&#38505;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#20197;&#20943;&#23569;&#25152;&#38656;&#30340;&#35757;&#32451;&#38598;&#22823;&#23567;&#65292;&#20381;&#36182;&#20110;&#30005;&#23376;&#21830;&#21153;&#25968;&#25454;&#20013;&#24120;&#35265;&#30340;&#22270;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#23558;&#38382;&#39064;&#35270;&#20026;&#20855;&#26377;&#26377;&#38480;&#25968;&#37327;&#26631;&#35760;&#23454;&#20363;&#30340;&#33410;&#28857;&#22238;&#24402;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#31867;&#20284;&#20110;&#20808;&#21069;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;&#22120;&#30340;&#21452;&#27169;&#22411;&#31070;&#32463;&#26550;&#26500;&#65292;&#24182;&#27979;&#35797;&#20102;&#19981;&#21516;&#30340;&#28040;&#24687;&#20256;&#36882;&#23618;&#36827;&#34892;&#32534;&#30721;&#12290;&#27492;&#22806;&#65292;&#20316;&#20026;&#39069;&#22806;&#27493;&#39588;&#65292;&#25105;&#20204;&#23558;&#27169;&#22411;&#19982;&#33719;&#21462;&#20989;&#25968;&#30456;&#32467;&#21512;&#65292;&#20197;&#24341;&#23548;&#20449;&#24687;&#20256;&#36882;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19289v1 Announce Type: cross  Abstract: Estimating causal effects in e-commerce tends to involve costly treatment assignments which can be impractical in large-scale settings. Leveraging machine learning to predict such treatment effects without actual intervention is a standard practice to diminish the risk. However, existing methods for treatment effect prediction tend to rely on training sets of substantial size, which are built from real experiments and are thus inherently risky to create. In this work we propose a graph neural network to diminish the required training set size, relying on graphs that are common in e-commerce data. Specifically, we view the problem as node regression with a restricted number of labeled instances, develop a two-model neural architecture akin to previous causal effect estimators, and test varying message-passing layers for encoding. Furthermore, as an extra step, we combine the model with an acquisition function to guide the creation of th
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#22312;&#31574;&#30053;&#19978;&#30340;&#22870;&#21169;&#23398;&#20064;&#26694;&#26550;&#65292;&#20351;&#29992;&#31574;&#30053;&#26679;&#26412;&#20248;&#21270;&#22870;&#21169;&#27169;&#22411;&#20197;&#20445;&#25345;&#20854;&#20998;&#24067;&#19978;&#30340;&#19968;&#33268;&#24615;</title><link>https://arxiv.org/abs/2403.19279</link><description>&lt;p&gt;
&#20351;&#29992;&#22870;&#21169;&#23398;&#20064;&#22312;&#31574;&#30053;&#19978;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Fine-Tuning Language Models with Reward Learning on Policy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19279
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#22312;&#31574;&#30053;&#19978;&#30340;&#22870;&#21169;&#23398;&#20064;&#26694;&#26550;&#65292;&#20351;&#29992;&#31574;&#30053;&#26679;&#26412;&#20248;&#21270;&#22870;&#21169;&#27169;&#22411;&#20197;&#20445;&#25345;&#20854;&#20998;&#24067;&#19978;&#30340;&#19968;&#33268;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#65288;RLHF&#65289;&#20316;&#20026;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#20986;&#29616;&#65292;&#29992;&#20110;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#20154;&#31867;&#20559;&#22909;&#20445;&#25345;&#19968;&#33268;&#12290;RLHF&#21253;&#21547;&#19977;&#20010;&#27493;&#39588;&#65292;&#21363;&#25910;&#38598;&#20154;&#31867;&#20559;&#22909;&#12289;&#22870;&#21169;&#23398;&#20064;&#21644;&#31574;&#30053;&#20248;&#21270;&#65292;&#36890;&#24120;&#26159;&#20018;&#34892;&#25191;&#34892;&#30340;&#12290;&#28982;&#32780;&#65292;&#65288;&#22266;&#23450;&#30340;&#65289;&#22870;&#21169;&#27169;&#22411;&#21487;&#33021;&#20250;&#22240;&#20026;&#31574;&#30053;&#20248;&#21270;&#19981;&#26029;&#25913;&#21464;LLMs&#30340;&#25968;&#25454;&#20998;&#24067;&#32780;&#36973;&#21463;&#19981;&#20934;&#30830;&#30340;&#31163;&#20998;&#24067;&#24773;&#20917;&#12290;&#20174;&#26368;&#26032;&#30340;LLMs&#37325;&#22797;&#25910;&#38598;&#26032;&#30340;&#20559;&#22909;&#25968;&#25454;&#21487;&#33021;&#20250;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#19981;&#24184;&#30340;&#26159;&#65292;&#36825;&#20250;&#20351;&#24471;&#32467;&#26524;&#31995;&#32479;&#26356;&#21152;&#22797;&#26434;&#21644;&#38590;&#20197;&#20248;&#21270;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22312;&#31574;&#30053;&#19978;&#30340;&#22870;&#21169;&#23398;&#20064;&#65288;RLP&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#26080;&#30417;&#30563;&#30340;&#26694;&#26550;&#65292;&#20351;&#29992;&#31574;&#30053;&#26679;&#26412;&#26469;&#20248;&#21270;&#22870;&#21169;&#27169;&#22411;&#20197;&#20445;&#25345;&#20854;&#20998;&#24067;&#19978;&#30340;&#19968;&#33268;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#22810;&#35270;&#22270;&#23398;&#20064;&#26041;&#27861;&#26469;&#23398;&#20064;&#31574;&#30053;&#26679;&#26412;&#30340;&#31283;&#20581;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19279v1 Announce Type: cross  Abstract: Reinforcement learning from human feedback (RLHF) has emerged as an effective approach to aligning large language models (LLMs) to human preferences. RLHF contains three steps, i.e., human preference collecting, reward learning, and policy optimization, which are usually performed serially. Despite its popularity, however, (fixed) reward models may suffer from inaccurate off-distribution, since policy optimization continuously shifts LLMs' data distribution. Repeatedly collecting new preference data from the latest LLMs may alleviate this issue, which unfortunately makes the resulting system more complicated and difficult to optimize. In this paper, we propose reward learning on policy (RLP), an unsupervised framework that refines a reward model using policy samples to keep it on-distribution. Specifically, an unsupervised multi-view learning method is introduced to learn robust representations of policy samples. Meanwhile, a synthetic
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20010;&#24615;&#21270;&#30693;&#35782;&#21644;&#21160;&#24577;&#35282;&#33394;&#20449;&#24687;&#26500;&#24314;&#31038;&#20132;&#23186;&#20307;&#20195;&#29702;&#20197;&#35299;&#20915;&#20195;&#29702;&#25317;&#26377;&#19981;&#23646;&#20110;&#20854;&#35282;&#33394;&#30340;&#30693;&#35782;&#21644;&#26080;&#27861;&#28040;&#38500;&#22810;&#26679;&#21270;&#35282;&#33394;&#20449;&#24687;&#24178;&#25200;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.19275</link><description>&lt;p&gt;
&#30693;&#35782;&#36793;&#30028;&#19982;&#35282;&#33394;&#21160;&#24577;&#22609;&#36896;&#26356;&#22909;&#30340;&#31038;&#20132;&#23186;&#20307;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
Knowledge Boundary and Persona Dynamic Shape A Better Social Media Agent
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19275
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20010;&#24615;&#21270;&#30693;&#35782;&#21644;&#21160;&#24577;&#35282;&#33394;&#20449;&#24687;&#26500;&#24314;&#31038;&#20132;&#23186;&#20307;&#20195;&#29702;&#20197;&#35299;&#20915;&#20195;&#29702;&#25317;&#26377;&#19981;&#23646;&#20110;&#20854;&#35282;&#33394;&#30340;&#30693;&#35782;&#21644;&#26080;&#27861;&#28040;&#38500;&#22810;&#26679;&#21270;&#35282;&#33394;&#20449;&#24687;&#24178;&#25200;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26500;&#24314;&#20010;&#24615;&#21270;&#21644;&#25311;&#20154;&#21270;&#20195;&#29702;&#22312;&#31038;&#20132;&#32593;&#32476;&#27169;&#25311;&#20013;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#20316;&#21697;&#20013;&#20173;&#23384;&#22312;&#20004;&#20010;&#20851;&#38190;&#38382;&#39064;&#65306;&#20195;&#29702;&#25317;&#26377;&#19981;&#23646;&#20110;&#20854;&#35282;&#33394;&#30340;&#19990;&#30028;&#30693;&#35782;&#65292;&#19981;&#33021;&#28040;&#38500;&#22810;&#26679;&#21270;&#35282;&#33394;&#20449;&#24687;&#23545;&#24403;&#21069;&#34892;&#20026;&#30340;&#24178;&#25200;&#65292;&#20174;&#32780;&#38477;&#20302;&#20102;&#20195;&#29702;&#30340;&#20010;&#24615;&#21270;&#21644;&#25311;&#20154;&#21270;&#12290;&#20026;&#20102;&#35299;&#20915;&#20197;&#19978;&#38382;&#39064;&#65292;&#25105;&#20204;&#22522;&#20110;&#20010;&#24615;&#21270;&#30693;&#35782;&#21644;&#21160;&#24577;&#35282;&#33394;&#20449;&#24687;&#26500;&#24314;&#31038;&#20132;&#23186;&#20307;&#20195;&#29702;&#12290;&#23545;&#20110;&#20010;&#24615;&#21270;&#30693;&#35782;&#65292;&#25105;&#20204;&#28155;&#21152;&#22806;&#37096;&#30693;&#35782;&#28304;&#24182;&#23558;&#20854;&#19982;&#20195;&#29702;&#30340;&#35282;&#33394;&#20449;&#24687;&#21305;&#37197;&#65292;&#20174;&#32780;&#36171;&#20104;&#20195;&#29702;&#20010;&#24615;&#21270;&#30340;&#19990;&#30028;&#30693;&#35782;&#12290;&#23545;&#20110;&#21160;&#24577;&#35282;&#33394;&#20449;&#24687;&#65292;&#25105;&#20204;&#20351;&#29992;&#24403;&#21069;&#34892;&#20026;&#20449;&#24687;&#20869;&#37096;&#26816;&#32034;&#20195;&#29702;&#30340;&#35282;&#33394;&#20449;&#24687;&#65292;&#20174;&#32780;&#20943;&#23569;&#22810;&#26679;&#21270;&#35282;&#33394;&#20449;&#24687;&#23545;&#24403;&#21069;&#34892;&#20026;&#30340;&#24178;&#25200;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19275v1 Announce Type: cross  Abstract: Constructing personalized and anthropomorphic agents holds significant importance in the simulation of social networks. However, there are still two key problems in existing works: the agent possesses world knowledge that does not belong to its personas, and it cannot eliminate the interference of diverse persona information on current actions, which reduces the personalization and anthropomorphism of the agent. To solve the above problems, we construct the social media agent based on personalized knowledge and dynamic persona information. For personalized knowledge, we add external knowledge sources and match them with the persona information of agents, thereby giving the agent personalized world knowledge. For dynamic persona information, we use current action information to internally retrieve the persona information of the agent, thereby reducing the interference of diverse persona information on the current action. To make the age
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25972;&#21512;&#22303;&#22756;&#33829;&#20859;&#21644;&#27668;&#35937;&#22240;&#32032;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#20316;&#29289;&#20135;&#37327;&#21644;&#30149;&#23475;&#39044;&#27979;&#65292;&#33268;&#21147;&#20110;&#35299;&#20915;&#23391;&#21152;&#25289;&#22269;&#20892;&#19994;&#20013;&#20316;&#29289;&#36873;&#25321;&#21644;&#30149;&#23475;&#39044;&#27979;&#20013;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.19273</link><description>&lt;p&gt;
&#19968;&#31181;&#25972;&#21512;&#22303;&#22756;&#33829;&#20859;&#21644;&#27668;&#35937;&#22240;&#32032;&#30340;&#20316;&#29289;&#20135;&#37327;&#21644;&#30149;&#23475;&#39044;&#27979;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Machine Learning Approach for Crop Yield and Disease Prediction Integrating Soil Nutrition and Weather Factors
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19273
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25972;&#21512;&#22303;&#22756;&#33829;&#20859;&#21644;&#27668;&#35937;&#22240;&#32032;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#20316;&#29289;&#20135;&#37327;&#21644;&#30149;&#23475;&#39044;&#27979;&#65292;&#33268;&#21147;&#20110;&#35299;&#20915;&#23391;&#21152;&#25289;&#22269;&#20892;&#19994;&#20013;&#20316;&#29289;&#36873;&#25321;&#21644;&#30149;&#23475;&#39044;&#27979;&#20013;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#24320;&#21457;&#19968;&#20010;&#26234;&#33021;&#30340;&#20892;&#19994;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#65292;&#29992;&#20110;&#23391;&#21152;&#25289;&#22269;&#30340;&#20316;&#29289;&#36873;&#25321;&#21644;&#30149;&#23475;&#39044;&#27979;&#12290;&#35813;&#22269;&#23478;&#30340;&#32463;&#27982;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#20892;&#19994;&#12290;&#28982;&#32780;&#65292;&#36873;&#25321;&#29983;&#20135;&#29575;&#26356;&#39640;&#30340;&#20316;&#29289;&#20197;&#21450;&#26377;&#25928;&#25511;&#21046;&#20316;&#29289;&#30149;&#23475;&#26159;&#20892;&#27665;&#24517;&#39035;&#38754;&#23545;&#30340;&#38556;&#30861;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#21644;&#23454;&#38469;&#25968;&#25454;&#38598;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#25512;&#33616;&#30340;&#26041;&#27861;&#20351;&#29992;&#20102;&#22810;&#31181;&#25968;&#25454;&#38598;&#65292;&#28085;&#30422;&#20316;&#29289;&#29983;&#20135;&#12289;&#22303;&#22756;&#26465;&#20214;&#12289;&#20892;&#19994;&#27668;&#35937;&#21306;&#22495;&#12289;&#20316;&#29289;&#30149;&#23475;&#21644;&#27668;&#35937;&#22240;&#32032;&#12290;&#36825;&#20123;&#25968;&#25454;&#38598;&#25552;&#20379;&#20102;&#20851;&#20110;&#30149;&#23475;&#36235;&#21183;&#12289;&#20316;&#29289;&#23545;&#22303;&#22756;&#33829;&#20859;&#38656;&#27714;&#20197;&#21450;&#20892;&#19994;&#29983;&#20135;&#21382;&#21490;&#30340;&#26377;&#35265;&#22320;&#20449;&#24687;&#12290;&#36890;&#36807;&#25972;&#21512;&#36825;&#20123;&#30693;&#35782;&#65292;&#35813;&#27169;&#22411;&#39318;&#20808;&#26681;&#25454;&#29305;&#23450;&#29992;&#25143;&#20301;&#32622;&#30340;&#22303;&#22756;&#33829;&#20859;&#25512;&#33616;&#20027;&#35201;&#36873;&#23450;&#20316;&#29289;&#30340;&#21015;&#34920;&#12290;&#28982;&#21518;&#65292;&#36890;&#36807;&#23558;&#26469;&#33258;&#19981;&#21516;&#25968;&#25454;&#28304;&#30340;&#20449;&#24687;&#36827;&#34892;&#25972;&#21512;&#65292;&#27169;&#22411;&#21487;&#20197;&#39044;&#27979;&#19981;&#21516;&#20316;&#29289;&#30340;&#20135;&#37327;&#21644;&#30149;&#23475;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19273v1 Announce Type: cross  Abstract: The development of an intelligent agricultural decision-supporting system for crop selection and disease forecasting in Bangladesh is the main objective of this work. The economy of the nation depends heavily on agriculture. However, choosing crops with better production rates and efficiently controlling crop disease are obstacles that farmers have to face. These issues are addressed in this research by utilizing machine learning methods and real-world datasets. The recommended approach uses a variety of datasets on the production of crops, soil conditions, agro-meteorological regions, crop disease, and meteorological factors. These datasets offer insightful information on disease trends, soil nutrition demand of crops, and agricultural production history. By incorporating this knowledge, the model first recommends the list of primarily selected crops based on the soil nutrition of a particular user location. Then the predictions of me
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#22522;&#20110;&#27010;&#29575;&#37319;&#26679;&#25552;&#20986;&#20102;DeepSample&#65292;&#36890;&#36807;&#23567;&#25968;&#25454;&#38598;&#22823;&#23567;&#12289;&#21487;&#20449;&#36182;&#30340;&#20272;&#35745;&#21644;&#35823;&#21028;&#26333;&#20809;&#19977;&#26041;&#38754;&#30340;&#25216;&#26415;&#30446;&#26631;&#65292;&#23454;&#29616;&#20102;&#25104;&#26412;&#25928;&#30410;&#30340;DNN&#20934;&#30830;&#24230;&#35780;&#20272;&#12290;</title><link>https://arxiv.org/abs/2403.19271</link><description>&lt;p&gt;
DeepSample&#65306;&#22522;&#20110;DNN&#37319;&#26679;&#30340;&#27979;&#35797;&#29992;&#20110;&#25805;&#20316;&#20934;&#30830;&#24230;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
DeepSample: DNN sampling-based testing for operational accuracy assessment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19271
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#22522;&#20110;&#27010;&#29575;&#37319;&#26679;&#25552;&#20986;&#20102;DeepSample&#65292;&#36890;&#36807;&#23567;&#25968;&#25454;&#38598;&#22823;&#23567;&#12289;&#21487;&#20449;&#36182;&#30340;&#20272;&#35745;&#21644;&#35823;&#21028;&#26333;&#20809;&#19977;&#26041;&#38754;&#30340;&#25216;&#26415;&#30446;&#26631;&#65292;&#23454;&#29616;&#20102;&#25104;&#26412;&#25928;&#30410;&#30340;DNN&#20934;&#30830;&#24230;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Deep Neural Networks&#65288;DNN&#65289;&#26159;&#35768;&#22810;&#36719;&#20214;&#31995;&#32479;&#20998;&#31867;&#21644;&#22238;&#24402;&#20219;&#21153;&#30340;&#26680;&#24515;&#32452;&#20214;&#12290;&#20225;&#19994;&#20026;&#20351;&#29992;&#20195;&#34920;&#25805;&#20316;&#20013;&#39044;&#26399;&#36755;&#20837;&#30340;&#25968;&#25454;&#38598;&#27979;&#35797;DNN&#32780;&#20135;&#29983;&#39640;&#25104;&#26412;&#65292;&#22240;&#20026;&#36825;&#20123;&#38656;&#35201;&#25163;&#21160;&#26631;&#35760;&#12290;&#25361;&#25112;&#22312;&#20110;&#23613;&#21487;&#33021;&#36873;&#25321;&#19968;&#32452;&#20195;&#34920;&#24615;&#30340;&#27979;&#35797;&#36755;&#20837;&#38598;&#65292;&#20197;&#20943;&#23569;&#26631;&#35760;&#25104;&#26412;&#65292;&#21516;&#26102;&#36275;&#20197;&#20135;&#29983;&#26080;&#20559;&#39640;&#32622;&#20449;&#24230;&#30340;&#39044;&#26399;DNN&#20934;&#30830;&#24615;&#20272;&#35745;&#12290;&#21516;&#26102;&#65292;&#27979;&#35797;&#20154;&#21592;&#24076;&#26395;&#23613;&#21487;&#33021;&#26292;&#38706;&#20986;&#23613;&#21487;&#33021;&#22810;&#30340;DNN&#35823;&#21028;&#65292;&#20197;&#25913;&#36827;DNN&#65292;&#22240;&#27492;&#38656;&#35201;&#36861;&#27714;&#19977;&#37325;&#30446;&#26631;&#30340;&#25216;&#26415;&#65306;&#23567;&#25968;&#25454;&#38598;&#22823;&#23567;&#12289;&#21487;&#20449;&#36182;&#30340;&#20272;&#35745;&#21644;&#35823;&#21028;&#26333;&#20809;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;DeepSample&#65292;&#19968;&#31181;&#22522;&#20110;&#27010;&#29575;&#37319;&#26679;&#30340;&#29992;&#20110;&#25104;&#26412;&#25928;&#30410;&#30340;&#20934;&#30830;&#24230;&#35780;&#20272;&#30340;DNN&#27979;&#35797;&#25216;&#26415;&#31995;&#21015;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#27010;&#29575;&#37319;&#26679;&#22312;&#20309;&#31181;&#31243;&#24230;&#21644;&#22312;&#21738;&#20123;&#26465;&#20214;&#19979;&#26377;&#21161;&#20110;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19271v1 Announce Type: cross  Abstract: Deep Neural Networks (DNN) are core components for classification and regression tasks of many software systems. Companies incur in high costs for testing DNN with datasets representative of the inputs expected in operation, as these need to be manually labelled. The challenge is to select a representative set of test inputs as small as possible to reduce the labelling cost, while sufficing to yield unbiased high-confidence estimates of the expected DNN accuracy. At the same time, testers are interested in exposing as many DNN mispredictions as possible to improve the DNN, ending up in the need for techniques pursuing a threefold aim: small dataset size, trustworthy estimates, mispredictions exposure. This study presents DeepSample, a family of DNN testing techniques for cost-effective accuracy assessment based on probabilistic sampling. We investigate whether, to what extent, and under which conditions probabilistic sampling can help 
&lt;/p&gt;</description></item><item><title>sDPO&#26159;&#23545;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#26041;&#27861;&#30340;&#25193;&#23637;&#65292;&#36890;&#36807;&#20998;&#27493;&#21033;&#29992;&#20559;&#22909;&#25968;&#25454;&#38598;&#32780;&#38750;&#19968;&#27425;&#24615;&#20351;&#29992;&#65292;&#20419;&#36827;&#26356;&#31934;&#30830;&#23545;&#40784;&#21442;&#32771;&#27169;&#22411;&#30340;&#20351;&#29992;&#65292;&#24182;&#35757;&#32451;&#20986;&#24615;&#33021;&#26356;&#20248;&#30340;&#26368;&#32456;&#27169;&#22411;&#65292;&#29978;&#33267;&#32988;&#36807;&#20854;&#20182;&#20855;&#26377;&#26356;&#22810;&#21442;&#25968;&#30340;&#27969;&#34892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2403.19270</link><description>&lt;p&gt;
sDPO&#65306;&#19981;&#35201;&#19968;&#27425;&#24615;&#20351;&#29992;&#24744;&#30340;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
sDPO: Don't Use Your Data All at Once
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19270
&lt;/p&gt;
&lt;p&gt;
sDPO&#26159;&#23545;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#26041;&#27861;&#30340;&#25193;&#23637;&#65292;&#36890;&#36807;&#20998;&#27493;&#21033;&#29992;&#20559;&#22909;&#25968;&#25454;&#38598;&#32780;&#38750;&#19968;&#27425;&#24615;&#20351;&#29992;&#65292;&#20419;&#36827;&#26356;&#31934;&#30830;&#23545;&#40784;&#21442;&#32771;&#27169;&#22411;&#30340;&#20351;&#29992;&#65292;&#24182;&#35757;&#32451;&#20986;&#24615;&#33021;&#26356;&#20248;&#30340;&#26368;&#32456;&#27169;&#22411;&#65292;&#29978;&#33267;&#32988;&#36807;&#20854;&#20182;&#20855;&#26377;&#26356;&#22810;&#21442;&#25968;&#30340;&#27969;&#34892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#21457;&#23637;&#65292;&#23558;&#23427;&#20204;&#19982;&#20154;&#31867;&#20559;&#22909;&#30456;&#19968;&#33268;&#21464;&#24471;&#26085;&#30410;&#37325;&#35201;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20998;&#27493;DPO&#65288;sDPO&#65289;&#65292;&#36825;&#26159;&#23545;&#26368;&#36817;&#27969;&#34892;&#30340;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#65288;DPO&#65289;&#36827;&#34892;&#35843;&#25972;&#30340;&#19968;&#20010;&#25193;&#23637;&#12290;&#36825;&#31181;&#26041;&#27861;&#28041;&#21450;&#23558;&#21487;&#29992;&#30340;&#20559;&#22909;&#25968;&#25454;&#38598;&#20998;&#21106;&#65292;&#24182;&#20197;&#20998;&#27493;&#26041;&#24335;&#21033;&#29992;&#23427;&#20204;&#65292;&#32780;&#19981;&#26159;&#19968;&#27425;&#24615;&#20351;&#29992;&#12290;&#25105;&#20204;&#28436;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#20419;&#36827;&#20102;&#26356;&#31934;&#30830;&#23545;&#40784;&#21442;&#32771;&#27169;&#22411;&#22312;DPO&#35757;&#32451;&#26694;&#26550;&#20869;&#30340;&#20351;&#29992;&#12290;&#27492;&#22806;&#65292;sDPO&#35757;&#32451;&#26368;&#32456;&#27169;&#22411;&#30340;&#24615;&#33021;&#26356;&#22909;&#65292;&#29978;&#33267;&#32988;&#36807;&#25317;&#26377;&#26356;&#22810;&#21442;&#25968;&#30340;&#20854;&#20182;&#27969;&#34892;LLM&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19270v1 Announce Type: cross  Abstract: As development of large language models (LLM) progresses, aligning them with human preferences has become increasingly important. We propose stepwise DPO (sDPO), an extension of the recently popularized direct preference optimization (DPO) for alignment tuning. This approach involves dividing the available preference datasets and utilizing them in a stepwise manner, rather than employing it all at once. We demonstrate that this method facilitates the use of more precisely aligned reference models within the DPO training framework. Furthermore, sDPO trains the final model to be more performant, even outperforming other popular LLMs with more parameters.
&lt;/p&gt;</description></item><item><title>MineLand&#27169;&#25311;&#22120;&#24341;&#20837;&#26377;&#38480;&#22810;&#27169;&#24863;&#30693;&#21644;&#29983;&#29702;&#38656;&#27714;&#65292;&#25903;&#25345;&#22810;&#26234;&#33021;&#20307;&#22312;&#21327;&#20316;&#20013;&#22635;&#34917;&#20102;&#20449;&#24687;&#21644;&#21151;&#33021;&#38480;&#21046;&#30340;&#31354;&#30333;&#65292;&#20174;&#32780;&#20419;&#36827;&#26356;&#20855;&#21160;&#24577;&#21644;&#26377;&#25928;&#24615;&#30340;&#22810;&#26234;&#33021;&#20307;&#20132;&#20114;&#12290;</title><link>https://arxiv.org/abs/2403.19267</link><description>&lt;p&gt;
MineLand&#65306;&#27169;&#25311;&#20855;&#26377;&#26377;&#38480;&#22810;&#27169;&#24863;&#30693;&#21644;&#29983;&#29702;&#38656;&#27714;&#30340;&#22823;&#35268;&#27169;&#22810;&#26234;&#33021;&#20307;&#20132;&#20114;
&lt;/p&gt;
&lt;p&gt;
MineLand: Simulating Large-Scale Multi-Agent Interactions with Limited Multimodal Senses and Physical Needs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19267
&lt;/p&gt;
&lt;p&gt;
MineLand&#27169;&#25311;&#22120;&#24341;&#20837;&#26377;&#38480;&#22810;&#27169;&#24863;&#30693;&#21644;&#29983;&#29702;&#38656;&#27714;&#65292;&#25903;&#25345;&#22810;&#26234;&#33021;&#20307;&#22312;&#21327;&#20316;&#20013;&#22635;&#34917;&#20102;&#20449;&#24687;&#21644;&#21151;&#33021;&#38480;&#21046;&#30340;&#31354;&#30333;&#65292;&#20174;&#32780;&#20419;&#36827;&#26356;&#20855;&#21160;&#24577;&#21644;&#26377;&#25928;&#24615;&#30340;&#22810;&#26234;&#33021;&#20307;&#20132;&#20114;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#22810;&#26234;&#33021;&#20307;&#20223;&#30495;&#22120;&#36890;&#24120;&#20551;&#35774;&#25317;&#26377;&#23436;&#32654;&#20449;&#24687;&#21644;&#26080;&#38480;&#21151;&#33021;&#65292;&#36825;&#38480;&#21046;&#20102;&#31038;&#20250;&#20114;&#21160;&#30340;&#29983;&#24577;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#26234;&#33021;&#20307;Minecraft&#27169;&#25311;&#22120;MineLand&#65292;&#36890;&#36807;&#24341;&#20837;&#26377;&#38480;&#30340;&#22810;&#27169;&#24863;&#30693;&#21644;&#29983;&#29702;&#38656;&#27714;&#26469;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#12290;&#25105;&#20204;&#30340;&#20223;&#30495;&#22120;&#25903;&#25345;&#26368;&#22810;48&#20010;&#20855;&#26377;&#26377;&#38480;&#35270;&#35273;&#12289;&#21548;&#35273;&#21644;&#29615;&#22659;&#24847;&#35782;&#30340;&#26234;&#33021;&#20307;&#65292;&#36843;&#20351;&#23427;&#20204;&#31215;&#26497;&#27807;&#36890;&#21644;&#21327;&#20316;&#20197;&#28385;&#36275;&#39135;&#29289;&#21644;&#36164;&#28304;&#31561;&#29983;&#29702;&#38656;&#27714;&#12290;&#36825;&#20419;&#36827;&#20102;&#21160;&#24577;&#21644;&#26377;&#25928;&#30340;&#22810;&#26234;&#33021;&#20307;&#20132;&#20114;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#20171;&#32461;&#20102;&#19968;&#20010;&#28789;&#24863;&#26469;&#33258;&#22810;&#20219;&#21153;&#22788;&#29702;&#29702;&#35770;&#30340;AI&#26234;&#33021;&#20307;&#26694;&#26550;Alex&#65292;&#20351;&#26234;&#33021;&#20307;&#33021;&#22815;&#22788;&#29702;&#22797;&#26434;&#30340;&#21327;&#35843;&#21644;&#35843;&#24230;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#27169;&#25311;&#22120;&#12289;&#30456;&#24212;&#30340;&#22522;&#20934;&#27979;&#35797;&#21644;AI&#26234;&#33021;&#20307;&#26694;&#26550;&#26377;&#21161;&#20110;&#26356;&#20855;&#29983;&#24577;&#21644;&#32454;&#33268;&#30340;&#38598;&#20307;&#34892;&#20026;&#12290;MineLand&#21644;Alex&#30340;&#28304;&#20195;&#30721;&#21487;&#20197;&#22312;https://github.com/c&#20013;&#20844;&#24320;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19267v1 Announce Type: cross  Abstract: Conventional multi-agent simulators often assume perfect information and limitless capabilities, hindering the ecological validity of social interactions. We propose a multi-agent Minecraft simulator, MineLand, that bridges this gap by introducing limited multimodal senses and physical needs. Our simulator supports up to 48 agents with limited visual, auditory, and environmental awareness, forcing them to actively communicate and collaborate to fulfill physical needs like food and resources. This fosters dynamic and valid multi-agent interactions. We further introduce an AI agent framework, Alex, inspired by multitasking theory, enabling agents to handle intricate coordination and scheduling. Our experiments demonstrate that the simulator, the corresponding benchmark, and the AI agent framework contribute to more ecological and nuanced collective behavior. The source code of MineLand and Alex is openly available at https://github.com/c
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26597;&#25214;&#34920;&#36827;&#34892;&#39640;&#25928;&#36793;&#32536;&#22270;&#20687;&#25512;&#26029;&#30340;ICELUT&#31639;&#27861;&#65292;&#26080;&#38656;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#22312;&#38477;&#20302;&#30828;&#20214;&#25512;&#26029;&#26102;&#38388;&#21644;&#21151;&#32791;&#30340;&#21516;&#26102;&#23454;&#29616;&#36817;&#20046;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.19238</link><description>&lt;p&gt;
&#39640;&#25928;&#22270;&#20687;&#20462;&#39280;&#30340;&#26597;&#25214;&#34920;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Taming Lookup Tables for Efficient Image Retouching
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19238
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26597;&#25214;&#34920;&#36827;&#34892;&#39640;&#25928;&#36793;&#32536;&#22270;&#20687;&#25512;&#26029;&#30340;ICELUT&#31639;&#27861;&#65292;&#26080;&#38656;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#22312;&#38477;&#20302;&#30828;&#20214;&#25512;&#26029;&#26102;&#38388;&#21644;&#21151;&#32791;&#30340;&#21516;&#26102;&#23454;&#29616;&#36817;&#20046;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#28165;&#23631;&#24149;&#22312;&#31471;&#35774;&#22791;(&#22914;&#32456;&#31471;&#29992;&#25143;&#30456;&#26426;&#12289;&#26234;&#33021;&#25163;&#26426;&#21644;&#30005;&#35270;)&#19978;&#30340;&#24191;&#27867;&#24212;&#29992;&#25512;&#21160;&#20102;&#22270;&#20687;&#22686;&#24378;&#38656;&#27714;&#30340;&#26174;&#30528;&#22686;&#38271;&#12290;&#29616;&#26377;&#30340;&#22686;&#24378;&#27169;&#22411;&#36890;&#24120;&#22312;&#20248;&#21270;&#39640;&#24615;&#33021;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#20943;&#23569;&#30828;&#20214;&#25512;&#26029;&#26102;&#38388;&#21644;&#21151;&#32791;&#26041;&#38754;&#23384;&#22312;&#19981;&#36275;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#35745;&#31639;&#21644;&#23384;&#20648;&#36164;&#28304;&#21463;&#38480;&#30340;&#31471;&#35774;&#22791;&#32780;&#35328;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22270;&#20687;&#39068;&#33394;&#22686;&#24378;&#26597;&#25214;&#34920;(ICELUT)&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#37319;&#29992;&#26597;&#25214;&#34920;&#36827;&#34892;&#26497;&#20854;&#39640;&#25928;&#30340;&#36793;&#32536;&#25512;&#26029;&#65292;&#32780;&#26080;&#38656;&#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(CNN)&#12290;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#36880;&#28857;(1x1)&#21367;&#31215;&#26469;&#25552;&#21462;&#39068;&#33394;&#20449;&#24687;&#65292;&#21516;&#26102;&#20351;&#29992;&#20998;&#21106;&#20840;&#36830;&#25509;&#23618;&#26469;&#34701;&#20837;&#20840;&#23616;&#20449;&#24687;&#12290;&#28982;&#21518;&#65292;&#36825;&#20004;&#20010;&#32452;&#20214;&#37117;&#26080;&#32541;&#36716;&#25442;&#20026;&#26597;&#25214;&#34920;&#65292;&#20197;&#20415;&#36827;&#34892;&#30828;&#20214;&#26080;&#20851;&#30340;&#37096;&#32626;&#12290;ICELUT&#23454;&#29616;&#20102;&#25509;&#36817;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#21151;&#32791;&#26497;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19238v1 Announce Type: cross  Abstract: The widespread use of high-definition screens in edge devices, such as end-user cameras, smartphones, and televisions, is spurring a significant demand for image enhancement. Existing enhancement models often optimize for high performance while falling short of reducing hardware inference time and power consumption, especially on edge devices with constrained computing and storage resources. To this end, we propose Image Color Enhancement Lookup Table (ICELUT) that adopts LUTs for extremely efficient edge inference, without any convolutional neural network (CNN). During training, we leverage pointwise (1x1) convolution to extract color information, alongside a split fully connected layer to incorporate global information. Both components are then seamlessly converted into LUTs for hardware-agnostic deployment. ICELUT achieves near-state-of-the-art performance and remarkably low power consumption. We observe that the pointwise network s
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377; Missing-Resistant &#26694;&#26550;&#30340;&#22810;&#27169;&#24577;&#35270;&#39057;&#27573;&#33853;&#23383;&#24149;&#29983;&#25104;&#27169;&#22411;&#65292;&#33021;&#22815;&#26377;&#25928;&#25972;&#21512;&#21508;&#31181;&#21487;&#29992;&#30340;&#36741;&#21161;&#36755;&#20837;&#65292;&#22312;&#32570;&#22833;&#26576;&#20123;&#27169;&#24577;&#30340;&#24773;&#20917;&#19979;&#20173;&#33021;&#20445;&#25345;&#38887;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;&#38543;&#26426;&#30465;&#30053;&#36741;&#21161;&#36755;&#20837;&#30340;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#20197;&#21450;&#29992;&#20110;&#25552;&#28860;&#30693;&#35782;&#30340;&#27491;&#21017;&#21270;&#30446;&#26631;&#12290;</title><link>https://arxiv.org/abs/2403.19221</link><description>&lt;p&gt;
&#26397;&#30528;&#23545;&#32570;&#22833;&#27169;&#24577;&#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#22810;&#27169;&#24577;&#35270;&#39057;&#27573;&#33853;&#23383;&#24149;&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Towards Multimodal Video Paragraph Captioning Models Robust to Missing Modality
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19221
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377; Missing-Resistant &#26694;&#26550;&#30340;&#22810;&#27169;&#24577;&#35270;&#39057;&#27573;&#33853;&#23383;&#24149;&#29983;&#25104;&#27169;&#22411;&#65292;&#33021;&#22815;&#26377;&#25928;&#25972;&#21512;&#21508;&#31181;&#21487;&#29992;&#30340;&#36741;&#21161;&#36755;&#20837;&#65292;&#22312;&#32570;&#22833;&#26576;&#20123;&#27169;&#24577;&#30340;&#24773;&#20917;&#19979;&#20173;&#33021;&#20445;&#25345;&#38887;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;&#38543;&#26426;&#30465;&#30053;&#36741;&#21161;&#36755;&#20837;&#30340;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#20197;&#21450;&#29992;&#20110;&#25552;&#28860;&#30693;&#35782;&#30340;&#27491;&#21017;&#21270;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#39057;&#27573;&#33853;&#23383;&#24149;&#29983;&#25104;&#65288;VPC&#65289;&#28041;&#21450;&#20026;&#38271;&#35270;&#39057;&#29983;&#25104;&#35814;&#32454;&#30340;&#21465;&#36848;&#65292;&#21033;&#29992;&#25903;&#25345;&#24615;&#27169;&#24577;&#65292;&#22914;&#35821;&#38899;&#21644;&#20107;&#20214;&#36793;&#30028;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#27169;&#22411;&#21463;&#21046;&#20110;&#19968;&#20010;&#20551;&#35774;&#65292;&#21363;&#21333;&#19968;&#36741;&#21161;&#27169;&#24577;&#30340;&#24658;&#23450;&#21487;&#29992;&#24615;&#65292;&#36825;&#22312;&#30495;&#23454;&#22330;&#26223;&#30340;&#22810;&#26679;&#24615;&#21644;&#19981;&#21487;&#39044;&#27979;&#24615;&#19979;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20855;&#26377; Missing-Resistant &#26694;&#26550; MR-VPC&#65292;&#35813;&#26694;&#26550;&#26377;&#25928;&#22320;&#21033;&#29992;&#25152;&#26377;&#21487;&#29992;&#30340;&#36741;&#21161;&#36755;&#20837;&#65292;&#24182;&#19988;&#21363;&#20351;&#26576;&#20123;&#27169;&#24577;&#32570;&#22833;&#20063;&#33021;&#20445;&#25345;&#38887;&#24615;&#12290;&#22312;&#35813;&#26694;&#26550;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#34701;&#21512;&#35270;&#39057;&#12289;&#35821;&#38899;&#21644;&#20107;&#20214;&#36793;&#30028;&#36755;&#20837;&#30340;&#22810;&#27169;&#24577; VPC&#65288;MVPC&#65289;&#26550;&#26500;&#65292;&#20197;&#32479;&#19968;&#26041;&#24335;&#22788;&#29702;&#21508;&#31181;&#36741;&#21161;&#36755;&#20837;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#21152;&#24378;&#27169;&#22411;&#23545;&#19981;&#23436;&#25972;&#25968;&#25454;&#30340;&#40065;&#26834;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102; DropAM&#65292;&#19968;&#31181;&#38543;&#26426;&#30465;&#30053;&#36741;&#21161;&#36755;&#20837;&#30340;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#65292;&#32467;&#21512; DistillAM&#65292;&#19968;&#31181;&#29992;&#20110;&#25552;&#28860;&#30693;&#35782;&#30340;&#27491;&#21017;&#21270;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19221v1 Announce Type: cross  Abstract: Video paragraph captioning (VPC) involves generating detailed narratives for long videos, utilizing supportive modalities such as speech and event boundaries. However, the existing models are constrained by the assumption of constant availability of a single auxiliary modality, which is impractical given the diversity and unpredictable nature of real-world scenarios. To this end, we propose a Missing-Resistant framework MR-VPC that effectively harnesses all available auxiliary inputs and maintains resilience even in the absence of certain modalities. Under this framework, we propose the Multimodal VPC (MVPC) architecture integrating video, speech, and event boundary inputs in a unified manner to process various auxiliary inputs. Moreover, to fortify the model against incomplete data, we introduce DropAM, a data augmentation strategy that randomly omits auxiliary inputs, paired with DistillAM, a regularization target that distills knowl
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35774;&#32622;&#65292;&#31216;&#20026;&#27979;&#35797;&#26102;&#38388;&#20010;&#24615;&#21270;&#65292;&#19981;&#20165;&#20851;&#27880;&#30446;&#26631;&#26412;&#22320;&#20219;&#21153;&#65292;&#36824;&#24310;&#20280;&#21040;&#20854;&#20182;&#23637;&#31034;&#27979;&#35797;&#26102;&#38388;&#20010;&#24615;&#21270;&#30340;&#20219;&#21153;</title><link>https://arxiv.org/abs/2403.19211</link><description>&lt;p&gt;
&#20026;&#32852;&#37030;&#22522;&#37329;&#20250;&#27169;&#22411;&#25552;&#20379;&#21452;&#37325;&#20010;&#24615;&#21270;&#36866;&#37197;&#22120;
&lt;/p&gt;
&lt;p&gt;
Dual-Personalizing Adapter for Federated Foundation Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19211
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35774;&#32622;&#65292;&#31216;&#20026;&#27979;&#35797;&#26102;&#38388;&#20010;&#24615;&#21270;&#65292;&#19981;&#20165;&#20851;&#27880;&#30446;&#26631;&#26412;&#22320;&#20219;&#21153;&#65292;&#36824;&#24310;&#20280;&#21040;&#20854;&#20182;&#23637;&#31034;&#27979;&#35797;&#26102;&#38388;&#20010;&#24615;&#21270;&#30340;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22522;&#30784;&#27169;&#22411;&#65292;&#23588;&#20854;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#36890;&#36807;&#24494;&#35843;&#22823;&#37327;&#30340;&#25351;&#20196;&#25968;&#25454;&#65292;&#23637;&#29616;&#20986;&#20102;&#36866;&#24212;&#21508;&#31181;&#20219;&#21153;&#30340;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#32852;&#37030;&#22522;&#37329;&#20250;&#27169;&#22411;&#20316;&#20026;&#19968;&#31181;&#38544;&#31169;&#20445;&#25252;&#26041;&#27861;&#65292;&#22312;&#20998;&#24067;&#24335;&#23398;&#20064;&#65288;FL&#65289;&#29615;&#22659;&#19979;&#36890;&#36807;&#21033;&#29992;&#35768;&#22810;&#20998;&#24067;&#24335;&#25968;&#25454;&#38598;&#36827;&#34892;&#21327;&#20316;&#24494;&#35843;&#27169;&#22411;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#20855;&#26377;&#38750;IID&#25968;&#25454;&#12290;&#20026;&#20102;&#20943;&#36731;&#36890;&#20449;&#21644;&#35745;&#31639;&#24320;&#38144;&#65292;&#24341;&#20837;&#20102;&#21442;&#25968;&#39640;&#25928;&#26041;&#27861;&#20197;&#25552;&#39640;&#25928;&#29575;&#65292;&#24182;&#19988;&#19968;&#20123;&#30740;&#31350;&#23558;&#20010;&#24615;&#21270;&#26041;&#27861;&#35843;&#25972;&#20026;&#32852;&#37030;&#22522;&#37329;&#20250;&#27169;&#22411;&#65292;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#29992;&#25143;&#20559;&#22909;&#23545;&#40784;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30740;&#31350;&#20013;&#23384;&#22312;&#30340;&#19968;&#20010;&#20851;&#38190;&#32570;&#21475;&#26159;&#22312;&#30495;&#23454;&#24212;&#29992;&#20013;&#24573;&#30053;&#20102;&#27979;&#35797;&#26102;&#38388;&#20998;&#24067;&#36716;&#31227;&#12290;&#22240;&#27492;&#65292;&#20026;&#20102;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35774;&#32622;&#65292;&#31216;&#20026;&#27979;&#35797;&#26102;&#38388;&#20010;&#24615;&#21270;&#65292;&#23427;&#19981;&#20165;&#19987;&#27880;&#20110;&#30446;&#26631;&#26412;&#22320;&#20219;&#21153;&#65292;&#36824;&#24310;&#20280;&#21040;&#20854;&#20182;&#23637;&#31034;&#27979;&#35797;&#26102;&#38388;&#20010;&#24615;&#21270;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19211v1 Announce Type: cross  Abstract: Recently, foundation models, particularly large language models (LLMs), have demonstrated an impressive ability to adapt to various tasks by fine-tuning large amounts of instruction data. Notably, federated foundation models emerge as a privacy preservation method to fine-tune models collaboratively under federated learning (FL) settings by leveraging many distributed datasets with non-IID data. To alleviate communication and computation overhead, parameter-efficient methods are introduced for efficiency, and some research adapted personalization methods to federated foundation models for better user preferences alignment. However, a critical gap in existing research is the neglect of test-time distribution shifts in real-world applications. Therefore, to bridge this gap, we propose a new setting, termed test-time personalization, which not only concentrates on the targeted local task but also extends to other tasks that exhibit test-t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#28145;&#20837;&#30740;&#31350;&#20102;&#22522;&#20110;&#21306;&#22359;&#38142;&#30340;&#32852;&#37030;&#23398;&#20064;&#65288;BCFL&#65289;&#65292;&#31361;&#20986;&#20102;&#21306;&#22359;&#38142;&#30340;&#23433;&#20840;&#21151;&#33021;&#19982;&#32852;&#37030;&#23398;&#20064;&#30340;&#38544;&#31169;&#20445;&#25252;&#27169;&#24335;&#20043;&#38388;&#30340;&#21327;&#21516;&#20316;&#29992;&#12290;</title><link>https://arxiv.org/abs/2403.19178</link><description>&lt;p&gt;
&#22312;&#20998;&#24067;&#24335;&#32593;&#32476;&#20013;&#22686;&#24378;&#20449;&#20219;&#21644;&#38544;&#31169;&#65306;&#22522;&#20110;&#21306;&#22359;&#38142;&#30340;&#32852;&#37030;&#23398;&#20064;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Enhancing Trust and Privacy in Distributed Networks: A Comprehensive Survey on Blockchain-based Federated Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19178
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#28145;&#20837;&#30740;&#31350;&#20102;&#22522;&#20110;&#21306;&#22359;&#38142;&#30340;&#32852;&#37030;&#23398;&#20064;&#65288;BCFL&#65289;&#65292;&#31361;&#20986;&#20102;&#21306;&#22359;&#38142;&#30340;&#23433;&#20840;&#21151;&#33021;&#19982;&#32852;&#37030;&#23398;&#20064;&#30340;&#38544;&#31169;&#20445;&#25252;&#27169;&#24335;&#20043;&#38388;&#30340;&#21327;&#21516;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#38598;&#20013;&#24335;&#26381;&#21153;&#22120;&#23384;&#22312;&#21333;&#28857;&#25925;&#38556;&#39118;&#38505;&#26102;&#65292;&#20687;&#21306;&#22359;&#38142;&#36825;&#26679;&#30340;&#21435;&#20013;&#24515;&#21270;&#26041;&#27861;&#36890;&#36807;&#22312;&#22810;&#20010;&#23454;&#20307;&#20043;&#38388;&#23454;&#26045;&#20849;&#35782;&#26426;&#21046;&#25552;&#20379;&#20102;&#19968;&#20010;&#24341;&#20154;&#27880;&#30446;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#23558;&#20998;&#24067;&#24335;&#35745;&#31639;&#19982;&#21152;&#23494;&#25216;&#26415;&#30456;&#32467;&#21512;&#65292;&#21435;&#20013;&#24515;&#21270;&#25216;&#26415;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35745;&#31639;&#33539;&#24335;&#12290;&#21306;&#22359;&#38142;&#36890;&#36807;&#22312;&#32593;&#32476;&#33410;&#28857;&#20043;&#38388;&#32463;&#36807;&#20849;&#35782;&#39564;&#35777;&#21644;&#35760;&#24405;&#20132;&#26131;&#26469;&#30830;&#20445;&#23433;&#20840;&#12289;&#36879;&#26126;&#21644;&#38450;&#31713;&#25913;&#30340;&#25968;&#25454;&#31649;&#29702;&#12290;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20316;&#20026;&#19968;&#31181;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#20351;&#21442;&#19982;&#32773;&#33021;&#22815;&#22312;&#21327;&#21516;&#35757;&#32451;&#27169;&#22411;&#30340;&#21516;&#26102;&#36890;&#36807;&#36991;&#20813;&#30452;&#25509;&#21407;&#22987;&#25968;&#25454;&#20132;&#25442;&#26469;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#12290;&#23613;&#31649;&#23545;&#21435;&#20013;&#24515;&#21270;&#26041;&#27861;&#36234;&#26469;&#36234;&#24863;&#20852;&#36259;&#65292;&#20294;&#23427;&#20204;&#22312;FL&#20013;&#30340;&#24212;&#29992;&#20173;&#28982;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#35752;&#12290;&#26412;&#25991;&#28145;&#20837;&#35843;&#26597;&#20102;&#22522;&#20110;&#21306;&#22359;&#38142;&#30340;FL&#65288;BCFL&#65289;&#65292;&#37325;&#28857;&#20851;&#27880;&#21306;&#22359;&#38142;&#30340;&#23433;&#20840;&#29305;&#24615;&#19982;FL&#30340;&#38544;&#31169;&#20445;&#25252;&#27169;&#24335;&#20043;&#38388;&#30340;&#21327;&#21516;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19178v1 Announce Type: cross  Abstract: While centralized servers pose a risk of being a single point of failure, decentralized approaches like blockchain offer a compelling solution by implementing a consensus mechanism among multiple entities. Merging distributed computing with cryptographic techniques, decentralized technologies introduce a novel computing paradigm. Blockchain ensures secure, transparent, and tamper-proof data management by validating and recording transactions via consensus across network nodes. Federated Learning (FL), as a distributed machine learning framework, enables participants to collaboratively train models while safeguarding data privacy by avoiding direct raw data exchange. Despite the growing interest in decentralized methods, their application in FL remains underexplored. This paper presents a thorough investigation into Blockchain-based FL (BCFL), spotlighting the synergy between blockchain's security features and FL's privacy-preserving mo
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;Stagger&#32593;&#32476;&#65288;SNet&#65289;&#65292;&#36890;&#36807;&#34701;&#21512;&#32467;&#26500;&#32531;&#35299;CNN&#21644;ViTs&#20043;&#38388;&#30340;&#28508;&#22312;&#29305;&#24449;&#20998;&#24067;&#24046;&#24322;&#65292;&#20943;&#23569;&#20449;&#24687;&#20002;&#22833;&#12290;</title><link>https://arxiv.org/abs/2403.19177</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#21508;&#31181;&#22823;&#23567;&#30446;&#26631;&#30340;&#20449;&#24687;&#20002;&#22833;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Rethinking Information Loss in Medical Image Segmentation with Various-sized Targets
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19177
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;Stagger&#32593;&#32476;&#65288;SNet&#65289;&#65292;&#36890;&#36807;&#34701;&#21512;&#32467;&#26500;&#32531;&#35299;CNN&#21644;ViTs&#20043;&#38388;&#30340;&#28508;&#22312;&#29305;&#24449;&#20998;&#24067;&#24046;&#24322;&#65292;&#20943;&#23569;&#20449;&#24687;&#20002;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#38754;&#20020;&#30528;&#20197;&#19981;&#21516;&#22823;&#23567;&#30446;&#26631;&#20026;&#29305;&#24449;&#30340;&#25361;&#25112;&#65292;&#35201;&#27714;&#27169;&#22411;&#26377;&#25928;&#25429;&#25417;&#23616;&#37096;&#21644;&#20840;&#23616;&#20449;&#24687;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;Stagger&#32593;&#32476;&#65288;SNet&#65289;&#65292;&#25552;&#20986;&#36890;&#36807;&#31934;&#24515;&#35774;&#35745;&#30340;&#34701;&#21512;&#32467;&#26500;&#26469;&#32531;&#35299;CNN&#21644;ViTs&#20043;&#38388;&#30340;&#28508;&#22312;&#29305;&#24449;&#20998;&#24067;&#24046;&#24322;&#65292;&#20174;&#32780;&#20943;&#23569;&#20449;&#24687;&#20002;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19177v1 Announce Type: cross  Abstract: Medical image segmentation presents the challenge of segmenting various-size targets, demanding the model to effectively capture both local and global information. Despite recent efforts using CNNs and ViTs to predict annotations of different scales, these approaches often struggle to effectively balance the detection of targets across varying sizes. Simply utilizing local information from CNNs and global relationships from ViTs without considering potential significant divergence in latent feature distributions may result in substantial information loss. To address this issue, in this paper, we will introduce a novel Stagger Network (SNet) and argues that a well-designed fusion structure can mitigate the divergence in latent feature distributions between CNNs and ViTs, thereby reducing information loss. Specifically, to emphasize both global dependencies and local focus, we design a Parallel Module to bridge the semantic gap. Meanwhil
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#36873;&#25321;&#24615;&#36807;&#28388;&#25512;&#29702;&#22120;&#65288;SelF-Reasoner&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#35780;&#20272;&#38382;&#39064;&#19982;&#20505;&#36873;&#25512;&#29702;&#38142;&#20043;&#38388;&#30340;&#34164;&#28085;&#20851;&#31995;&#65292;&#20197;&#20943;&#36731;&#20855;&#26377;&#35823;&#23548;&#24615;&#30340;&#24605;&#32500;&#38142;&#25512;&#29702;&#36807;&#31243;&#12290;</title><link>https://arxiv.org/abs/2403.19167</link><description>&lt;p&gt;
&#29992;&#36873;&#25321;&#24615;&#36807;&#28388;&#20943;&#36731;&#20855;&#26377;&#35823;&#23548;&#24615;&#30340;&#24605;&#32500;&#38142;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Mitigating Misleading Chain-of-Thought Reasoning with Selective Filtering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19167
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#36873;&#25321;&#24615;&#36807;&#28388;&#25512;&#29702;&#22120;&#65288;SelF-Reasoner&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#35780;&#20272;&#38382;&#39064;&#19982;&#20505;&#36873;&#25512;&#29702;&#38142;&#20043;&#38388;&#30340;&#34164;&#28085;&#20851;&#31995;&#65292;&#20197;&#20943;&#36731;&#20855;&#26377;&#35823;&#23548;&#24615;&#30340;&#24605;&#32500;&#38142;&#25512;&#29702;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#21033;&#29992;&#24605;&#32500;&#38142;&#25512;&#29702;&#25216;&#26415;&#35299;&#20915;&#22797;&#26434;&#38382;&#39064;&#65292;&#23637;&#29616;&#20102;&#21331;&#36234;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#25512;&#29702;&#30340;&#25928;&#21147;&#21462;&#20915;&#20110;&#24605;&#32500;&#38142;&#30340;&#36136;&#37327;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#36873;&#25321;&#24615;&#36807;&#28388;&#25512;&#29702;&#22120;&#65288;SelF-Reasoner&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#35780;&#20272;&#38382;&#39064;&#19982;&#20505;&#36873;&#25512;&#29702;&#38142;&#20043;&#38388;&#30340;&#34164;&#28085;&#20851;&#31995;&#12290;&#24403;&#25512;&#29702;&#38142;&#23637;&#31034;&#20986;&#33258;&#20449;&#26102;&#65292;&#25105;&#20204;&#32487;&#32493;&#36827;&#34892;&#24605;&#32500;&#38142;&#25512;&#29702;&#65307;&#21542;&#21017;&#65292;&#25105;&#20204;&#36873;&#25321;&#30452;&#25509;&#39044;&#27979;&#31572;&#26696;&#12290;SelF-Reasoner&#22312;ScienceQA&#12289;ECQA&#21644;LastLetter&#20219;&#21153;&#19978;&#25345;&#32493;&#25913;&#21892;&#20102;&#32463;&#36807;&#24494;&#35843;&#30340;T5&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19167v1 Announce Type: cross  Abstract: Large language models have manifested remarkable capabilities by leveraging chain-of-thought (CoT) reasoning techniques to solve intricate questions through step-by-step reasoning chains. Despite its success, the efficacy of such reasoning is inherently contingent upon the quality of CoT. However, flawless CoT reasoning cannot be guaranteed due to the presence of indecomposable questions and the potential for erroneous reasoning chains, particularly in the case of small-scale language models. To tackle this challenge, we propose a novel approach called the selective filtering reasoner (SelF-Reasoner) that assesses the entailment relationship between the question and the candidate reasoning chain. Then, we proceed with CoT reasoning when the reasoning chain demonstrates confidence; otherwise, we opt to predict the answer directly. SelF-Reasoner improves the fine-tuned T5 baseline consistently over the ScienceQA, ECQA, and LastLetter tas
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22870;&#21169;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#26377;&#29992;&#38382;&#39064;&#26469;&#33258;&#25105;&#25913;&#36827;&#30340;&#26041;&#27861;&#65292;&#25552;&#38382;&#32773;&#36890;&#36807;&#35810;&#38382;&#35282;&#33394;&#25198;&#28436;&#32773;&#26469;&#24341;&#20986;&#20559;&#22909;&#65292;&#20174;&#32780;&#36845;&#20195;&#24494;&#35843;&#20197;&#22686;&#21152;&#20219;&#21153;&#39640;&#36136;&#37327;&#21709;&#24212;&#30340;&#27010;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.19154</link><description>&lt;p&gt;
STaR-GATE: &#25945;&#25480;&#35821;&#35328;&#27169;&#22411;&#35810;&#38382;&#28548;&#28165;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
STaR-GATE: Teaching Language Models to Ask Clarifying Questions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19154
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22870;&#21169;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#26377;&#29992;&#38382;&#39064;&#26469;&#33258;&#25105;&#25913;&#36827;&#30340;&#26041;&#27861;&#65292;&#25552;&#38382;&#32773;&#36890;&#36807;&#35810;&#38382;&#35282;&#33394;&#25198;&#28436;&#32773;&#26469;&#24341;&#20986;&#20559;&#22909;&#65292;&#20174;&#32780;&#36845;&#20195;&#24494;&#35843;&#20197;&#22686;&#21152;&#20219;&#21153;&#39640;&#36136;&#37327;&#21709;&#24212;&#30340;&#27010;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#25552;&#31034;&#35821;&#35328;&#27169;&#22411;&#23436;&#25104;&#20219;&#21153;&#26102;&#65292;&#29992;&#25143;&#36890;&#24120;&#20250;&#36951;&#28431;&#37325;&#35201;&#30340;&#32454;&#33410;&#12290;&#34429;&#28982;&#25552;&#38382;&#21487;&#20197;&#35299;&#20915;&#36825;&#31181;&#27495;&#20041;&#65292;&#20294;&#27169;&#22411;&#24448;&#24448;&#24456;&#38590;&#25552;&#20986;&#22909;&#38382;&#39064;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#22870;&#21169;&#27169;&#22411;&#29983;&#25104;&#26377;&#29992;&#38382;&#39064;&#26469;&#33258;&#25105;&#25913;&#36827;&#30340;&#33021;&#21147;&#65292;&#36825;&#26159;&#19968;&#31181;&#31616;&#21333;&#26041;&#27861;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;STaR-GATE&#12290;&#25105;&#20204;&#29983;&#25104;&#20102;&#19968;&#20010;&#21253;&#21547;25,500&#20010;&#29420;&#29305;&#20154;&#29289;-&#20219;&#21153;&#25552;&#31034;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#20197;&#27169;&#25311;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;--&#25552;&#38382;&#32773;--&#19982;&#19968;&#20010;&#20854;&#20559;&#22909;&#26410;&#30693;&#30340;&#35282;&#33394;&#25198;&#28436;&#32773;&#20043;&#38388;&#30340;&#23545;&#35805;&#12290;&#36890;&#36807;&#25552;&#38382;&#65292;&#25552;&#38382;&#32773;&#20174;&#35282;&#33394;&#25198;&#28436;&#32773;&#37027;&#37324;&#24341;&#20986;&#20559;&#22909;&#12290;&#25552;&#38382;&#32773;&#22312;&#37027;&#20123;&#22686;&#21152;&#39640;&#36136;&#37327;&#21709;&#24212;&#27010;&#29575;&#30340;&#38382;&#39064;&#19978;&#36827;&#34892;&#36845;&#20195;&#24494;&#35843;&#65292;&#36825;&#20123;&#38382;&#39064;&#26159;&#30001;&#20855;&#26377;&#23545;&#35282;&#33394;&#25198;&#28436;&#32773;&#35775;&#38382;&#26435;&#38480;&#30340;&#39044;&#35328;&#32773;&#29983;&#25104;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19154v1 Announce Type: cross  Abstract: When prompting language models to complete a task, users often leave important aspects unsaid. While asking questions could resolve this ambiguity \citep[GATE;][]{li2023eliciting}, models often struggle to ask good questions. We explore a language model's ability to self-improve \citep[STaR;][]{zelikman2022star} by rewarding the model for generating useful questions -- a simple method we dub STaR-GATE. We generate a synthetic dataset of 25,500 unique persona-task prompts to simulate conversations between a pretrained language model -- the \texttt{Questioner} -- and a \texttt{Roleplayer} whose preferences are unknown to the \texttt{Questioner}. By asking questions, the \texttt{Questioner} elicits preferences from the \texttt{Roleplayer}. The \texttt{Questioner} is iteratively finetuned on questions that increase the probability of high-quality responses to the task, which are generated by an \texttt{Oracle} with access to the \texttt{Ro
&lt;/p&gt;</description></item><item><title>&#22312;&#28151;&#21512;&#23545;&#25239;&#35757;&#32451;&#20013;&#65292;&#20998;&#31163;&#20223;&#23556;&#21442;&#25968;&#27604;&#20998;&#31163;&#32479;&#35745;&#25968;&#25454;&#22312;&#27169;&#22411;&#35757;&#32451;&#20013;&#21457;&#25381;&#26356;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;</title><link>https://arxiv.org/abs/2403.19150</link><description>&lt;p&gt;
&#25506;&#32034;&#28151;&#21512;&#23545;&#25239;&#35757;&#32451;&#20013;&#30340;&#21452;&#37325;&#25209;&#37327;&#24402;&#19968;&#21270;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Towards Understanding Dual BN In Hybrid Adversarial Training
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19150
&lt;/p&gt;
&lt;p&gt;
&#22312;&#28151;&#21512;&#23545;&#25239;&#35757;&#32451;&#20013;&#65292;&#20998;&#31163;&#20223;&#23556;&#21442;&#25968;&#27604;&#20998;&#31163;&#32479;&#35745;&#25968;&#25454;&#22312;&#27169;&#22411;&#35757;&#32451;&#20013;&#21457;&#25381;&#26356;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#20851;&#22312;&#23545;&#25239;&#35757;&#32451;&#65288;AT&#65289;&#20013;&#24212;&#29992;&#25209;&#37327;&#24402;&#19968;&#21270;&#65288;BN&#65289;&#30340;&#20851;&#27880;&#26085;&#30410;&#22686;&#38271;&#65292;&#23588;&#20854;&#26159;&#24403;&#27169;&#22411;&#21516;&#26102;&#22312;&#23545;&#25239;&#26679;&#26412;&#21644;&#24178;&#20928;&#26679;&#26412;&#19978;&#36827;&#34892;&#35757;&#32451;&#65288;&#31216;&#20026;&#28151;&#21512;-AT&#65289;&#26102;&#12290;&#19968;&#20010;&#20808;&#21069;&#30740;&#31350;&#24120;&#35265;&#30340;&#20570;&#27861;&#26159;&#20551;&#35774;&#23545;&#25239;&#26679;&#26412;&#21644;&#24178;&#20928;&#26679;&#26412;&#26469;&#33258;&#20004;&#20010;&#19981;&#21516;&#30340;&#39046;&#22495;&#65292;&#37319;&#29992;&#21452;&#37325;BN&#65292;&#20998;&#21035;&#29992;&#20110;&#23545;&#25239;&#20998;&#25903;&#21644;&#24178;&#20928;&#20998;&#25903;&#12290;&#28608;&#21169;&#21452;&#37325;BN&#30340;&#19968;&#31181;&#27969;&#34892;&#35266;&#24565;&#26159;&#65292;&#20272;&#35745;&#36825;&#31181;&#28151;&#21512;&#20998;&#24067;&#30340;&#35268;&#33539;&#21270;&#32479;&#35745;&#25968;&#25454;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#27492;&#20026;&#35268;&#33539;&#21270;&#32780;&#23558;&#20854;&#20998;&#24320;&#21487;&#20197;&#23454;&#29616;&#26356;&#24378;&#30340;&#40065;&#26834;&#24615;&#12290;&#19982;&#36825;&#19968;&#35266;&#24565;&#30456;&#21453;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#27169;&#22411;&#35757;&#32451;&#20013;&#65292;&#20998;&#31163;&#32479;&#35745;&#25968;&#25454;&#30340;&#20316;&#29992;&#27604;&#20998;&#31163;&#20223;&#23556;&#21442;&#25968;&#30340;&#20316;&#29992;&#36739;&#23567;&#12290;&#36825;&#19968;&#21457;&#29616;&#19982;&#20808;&#21069;&#30340;&#30740;&#31350;&#65288;Rebuffi&#31561;&#20154;&#65292;2023&#65289;&#19968;&#33268;&#65292;&#25105;&#20204;&#22312;&#20854;&#30740;&#31350;&#30340;&#22522;&#30784;&#19978;&#36827;&#34892;&#36827;&#19968;&#27493;&#30340;&#25506;&#35752;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19150v1 Announce Type: cross  Abstract: There is a growing concern about applying batch normalization (BN) in adversarial training (AT), especially when the model is trained on both adversarial samples and clean samples (termed Hybrid-AT). With the assumption that adversarial and clean samples are from two different domains, a common practice in prior works is to adopt Dual BN, where BN and BN are used for adversarial and clean branches, respectively. A popular belief for motivating Dual BN is that estimating normalization statistics of this mixture distribution is challenging and thus disentangling it for normalization achieves stronger robustness. In contrast to this belief, we reveal that disentangling statistics plays a less role than disentangling affine parameters in model training. This finding aligns with prior work (Rebuffi et al., 2023), and we build upon their research for further investigations. We demonstrate that the domain gap between adversarial and clean sam
&lt;/p&gt;</description></item><item><title>GenAI&#26816;&#27979;&#24037;&#20855;&#22312;&#38754;&#23545;&#36890;&#36807;&#23545;&#25239;&#25216;&#26415;&#20462;&#25913;&#30340;&#20869;&#23481;&#26102;&#20934;&#30830;&#24615;&#26174;&#33879;&#19979;&#38477;&#65292;&#19981;&#33021;&#25512;&#33616;&#29992;&#20110;&#30830;&#23450;&#23398;&#26415;&#35802;&#20449;&#36829;&#35268;&#65292;&#20294;&#22312;&#25903;&#25345;&#23398;&#29983;&#23398;&#20064;&#21644;&#32500;&#25252;&#23398;&#26415;&#35802;&#20449;&#26041;&#38754;&#21487;&#33021;&#26377;&#24110;&#21161;</title><link>https://arxiv.org/abs/2403.19148</link><description>&lt;p&gt;
GenAI&#26816;&#27979;&#24037;&#20855;&#65292;&#23545;&#25239;&#25216;&#26415;&#21450;&#20854;&#23545;&#39640;&#31561;&#25945;&#32946;&#21253;&#23481;&#24615;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
GenAI Detection Tools, Adversarial Techniques and Implications for Inclusivity in Higher Education
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19148
&lt;/p&gt;
&lt;p&gt;
GenAI&#26816;&#27979;&#24037;&#20855;&#22312;&#38754;&#23545;&#36890;&#36807;&#23545;&#25239;&#25216;&#26415;&#20462;&#25913;&#30340;&#20869;&#23481;&#26102;&#20934;&#30830;&#24615;&#26174;&#33879;&#19979;&#38477;&#65292;&#19981;&#33021;&#25512;&#33616;&#29992;&#20110;&#30830;&#23450;&#23398;&#26415;&#35802;&#20449;&#36829;&#35268;&#65292;&#20294;&#22312;&#25903;&#25345;&#23398;&#29983;&#23398;&#20064;&#21644;&#32500;&#25252;&#23398;&#26415;&#35802;&#20449;&#26041;&#38754;&#21487;&#33021;&#26377;&#24110;&#21161;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#20845;&#31181;&#20027;&#35201;&#29983;&#25104;&#24335;AI&#65288;GenAI&#65289;&#25991;&#26412;&#26816;&#27979;&#22120;&#22312;&#38754;&#23545;&#29992;&#20110;&#35268;&#36991;&#36825;&#20123;&#24037;&#20855;&#26816;&#27979;&#30340;&#25216;&#26415;&#20462;&#25913;&#30340;&#26426;&#22120;&#29983;&#25104;&#20869;&#23481;&#26102;&#30340;&#26377;&#25928;&#24615;&#65288;n = 805&#65289;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#26816;&#27979;&#22120;&#30340;&#20934;&#30830;&#29575;&#24050;&#32463;&#24456;&#20302;&#65288;39.5&#65285;&#65289;&#65292;&#24403;&#38754;&#23545;&#34987;&#25805;&#32437;&#30340;&#20869;&#23481;&#26102;&#65292;&#20934;&#30830;&#29575;&#26174;&#30528;&#38477;&#20302;&#65288;17.4&#65285;&#65289;&#65292;&#24182;&#19988;&#19968;&#20123;&#25216;&#26415;&#22312;&#35268;&#36991;&#26816;&#27979;&#26041;&#38754;&#27604;&#20854;&#20182;&#25216;&#26415;&#26356;&#26377;&#25928;&#12290; &#20934;&#30830;&#24615;&#38480;&#21046;&#21644;&#21487;&#33021;&#23548;&#33268;&#38169;&#35823;&#25351;&#25511;&#30340;&#24773;&#20917;&#34920;&#26126;&#65292;&#36825;&#20123;&#24037;&#20855;&#30446;&#21069;&#26080;&#27861;&#25512;&#33616;&#29992;&#20110;&#30830;&#23450;&#26159;&#21542;&#21457;&#29983;&#20102;&#23398;&#26415;&#35802;&#20449;&#30340;&#36829;&#35268;&#34892;&#20026;&#65292;&#31361;&#26174;&#20102;&#25945;&#32946;&#24037;&#20316;&#32773;&#22312;&#32500;&#25252;&#21253;&#23481;&#21644;&#20844;&#24179;&#35780;&#20272;&#23454;&#36341;&#26041;&#38754;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290; &#20294;&#26159;&#65292;&#24403;&#20197;&#38750;&#24809;&#32602;&#24615;&#26041;&#24335;&#20351;&#29992;&#26102;&#65292;&#23427;&#20204;&#21487;&#33021;&#22312;&#25903;&#25345;&#23398;&#29983;&#23398;&#20064;&#21644;&#32500;&#25252;&#23398;&#26415;&#35802;&#20449;&#26041;&#38754;&#21457;&#25381;&#20316;&#29992;&#12290; &#36825;&#20123;&#32467;&#26524;&#24378;&#35843;&#20102;&#38656;&#35201;&#32508;&#21512;&#26041;&#27861;&#30340;&#24517;&#35201;&#24615;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19148v1 Announce Type: cross  Abstract: This study investigates the efficacy of six major Generative AI (GenAI) text detectors when confronted with machine-generated content that has been modified using techniques designed to evade detection by these tools (n=805). The results demonstrate that the detectors' already low accuracy rates (39.5%) show major reductions in accuracy (17.4%) when faced with manipulated content, with some techniques proving more effective than others in evading detection.   The accuracy limitations and the potential for false accusations demonstrate that these tools cannot currently be recommended for determining whether violations of academic integrity have occurred, underscoring the challenges educators face in maintaining inclusive and fair assessment practices. However, they may have a role in supporting student learning and maintaining academic integrity when used in a non-punitive manner.   These results underscore the need for a combined appro
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#37327;&#21270;&#22122;&#22768;&#26657;&#27491;&#26041;&#26696;&#65288;QNCD&#65289;&#65292;&#26088;&#22312;&#20943;&#23567;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#37327;&#21270;&#22122;&#22768;&#65292;&#35299;&#20915;&#20102;&#21518;&#35757;&#32451;&#37327;&#21270;&#23545;&#37319;&#26679;&#21152;&#36895;&#30340;&#24433;&#21709;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.19140</link><description>&lt;p&gt;
QNCD&#65306;&#25193;&#25955;&#27169;&#22411;&#30340;&#37327;&#21270;&#22122;&#22768;&#26657;&#27491;
&lt;/p&gt;
&lt;p&gt;
QNCD: Quantization Noise Correction for Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19140
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#37327;&#21270;&#22122;&#22768;&#26657;&#27491;&#26041;&#26696;&#65288;QNCD&#65289;&#65292;&#26088;&#22312;&#20943;&#23567;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#37327;&#21270;&#22122;&#22768;&#65292;&#35299;&#20915;&#20102;&#21518;&#35757;&#32451;&#37327;&#21270;&#23545;&#37319;&#26679;&#21152;&#36895;&#30340;&#24433;&#21709;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#22312;&#22270;&#20687;&#21512;&#25104;&#39046;&#22495;&#24341;&#36215;&#20102;&#38761;&#21629;&#65292;&#24314;&#31435;&#20102;&#36136;&#37327;&#21644;&#21019;&#36896;&#21147;&#30340;&#26032;&#22522;&#20934;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#36845;&#20195;&#21435;&#22122;&#36807;&#31243;&#20013;&#38656;&#35201;&#30340;&#23494;&#38598;&#35745;&#31639;&#38459;&#30861;&#20102;&#23427;&#20204;&#30340;&#24191;&#27867;&#37319;&#29992;&#12290;&#21518;&#35757;&#32451;&#37327;&#21270;&#65288;PTQ&#65289;&#25552;&#20379;&#20102;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#20197;&#21152;&#36895;&#37319;&#26679;&#65292;&#23613;&#31649;&#20197;&#20302;&#27604;&#29305;&#35774;&#32622;&#26497;&#22823;&#38477;&#20302;&#20102;&#26679;&#26412;&#36136;&#37327;&#12290;&#38024;&#23545;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#37327;&#21270;&#22122;&#22768;&#26657;&#27491;&#26041;&#26696;&#65288;QNCD&#65289;&#65292;&#26088;&#22312;&#20943;&#23567;&#25972;&#20010;&#37319;&#26679;&#36807;&#31243;&#20013;&#30340;&#37327;&#21270;&#22122;&#22768;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#20004;&#20010;&#20027;&#35201;&#30340;&#37327;&#21270;&#25361;&#25112;&#65306;&#20869;&#37096;&#21644;&#22806;&#37096;&#37327;&#21270;&#22122;&#22768;&#12290;&#20869;&#37096;&#37327;&#21270;&#22122;&#22768;&#20027;&#35201;&#30001;&#20110;&#23884;&#20837;&#22312;resblock&#27169;&#22359;&#20013;&#32780;&#21152;&#21095;&#65292;&#25193;&#23637;&#20102;&#28608;&#27963;&#37327;&#21270;&#33539;&#22260;&#65292;&#22312;&#27599;&#20010;&#21333;&#29420;&#30340;&#21435;&#22122;&#27493;&#39588;&#20013;&#22686;&#21152;&#20102;&#24178;&#25200;&#12290;&#27492;&#22806;&#65292;&#22806;&#37096;&#37327;&#21270;&#22122;&#22768;&#28304;&#33258;&#25972;&#20010;&#21435;&#22122;&#36807;&#31243;&#20013;&#30340;&#32047;&#31215;&#37327;&#21270;&#20559;&#24046;&#65292;&#25913;&#21464;&#20102;&#25968;&#25454;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19140v1 Announce Type: cross  Abstract: Diffusion models have revolutionized image synthesis, setting new benchmarks in quality and creativity. However, their widespread adoption is hindered by the intensive computation required during the iterative denoising process. Post-training quantization (PTQ) presents a solution to accelerate sampling, aibeit at the expense of sample quality, extremely in low-bit settings. Addressing this, our study introduces a unified Quantization Noise Correction Scheme (QNCD), aimed at minishing quantization noise throughout the sampling process. We identify two primary quantization challenges: intra and inter quantization noise. Intra quantization noise, mainly exacerbated by embeddings in the resblock module, extends activation quantization ranges, increasing disturbances in each single denosing step. Besides, inter quantization noise stems from cumulative quantization deviations across the entire denoising process, altering data distributions 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#35266;&#23519;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#19981;&#21516;&#23618;&#23545;&#38544;&#34255;&#29366;&#24577;&#30340;&#24433;&#21709;&#31243;&#24230;&#65292;&#25552;&#20986;&#20102;LLM-Streamline&#26041;&#27861;&#65292;&#21253;&#25324;&#23618;&#21098;&#26525;&#21644;&#23618;&#26367;&#25442;&#65292;&#29992;&#20110;&#21387;&#32553;&#27169;&#22411;&#24182;&#20445;&#25345;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.19135</link><description>&lt;p&gt;
&#36890;&#36807;&#31616;&#21270;&#19981;&#37325;&#35201;&#30340;&#23618;&#21387;&#32553;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Compressing Large Language Models by Streamlining the Unimportant Layer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19135
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35266;&#23519;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#19981;&#21516;&#23618;&#23545;&#38544;&#34255;&#29366;&#24577;&#30340;&#24433;&#21709;&#31243;&#24230;&#65292;&#25552;&#20986;&#20102;LLM-Streamline&#26041;&#27861;&#65292;&#21253;&#25324;&#23618;&#21098;&#26525;&#21644;&#23618;&#26367;&#25442;&#65292;&#29992;&#20110;&#21387;&#32553;&#27169;&#22411;&#24182;&#20445;&#25345;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#24050;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#21644;&#39046;&#22495;&#65292;&#20294;&#20854;&#36866;&#29992;&#24615;&#21463;&#21040;&#27169;&#22411;&#21442;&#25968;&#30340;&#38480;&#21046;&#12290;&#22240;&#27492;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#20154;&#20851;&#27880;&#34920;&#29616;&#20986;&#39640;&#24615;&#33021;&#30340;&#32039;&#20945;&#27169;&#22411;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;LLM&#30340;&#19981;&#21516;&#23618;&#23545;&#38544;&#34255;&#29366;&#24577;&#26377;&#19981;&#21516;&#31243;&#24230;&#30340;&#25200;&#21160;&#65292;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#35782;&#21035;&#20986;&#19981;&#37027;&#20040;&#37325;&#35201;&#30340;&#23618;&#12290;&#22522;&#20110;&#36825;&#19968;&#29616;&#35937;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LLM-Streamline&#65292;&#21253;&#25324;&#20004;&#37096;&#20998;&#65306;&#23618;&#21098;&#26525;&#65292;&#26681;&#25454;&#30446;&#26631;&#31232;&#30095;&#24230;&#31227;&#38500;&#27169;&#22411;&#20013;&#19968;&#32452;&#36830;&#32493;&#30340;&#26368;&#19981;&#37325;&#35201;&#30340;&#23618;&#65307;&#23618;&#26367;&#25442;&#65292;&#35757;&#32451;&#19968;&#20010;&#36731;&#37327;&#32423;&#27169;&#22411;&#26469;&#26367;&#25442;&#34987;&#21098;&#26525;&#30340;&#23618;&#65292;&#20174;&#32780;&#32531;&#35299;&#30001;&#21098;&#26525;&#36896;&#25104;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#22810;&#23618;&#24863;&#30693;&#22120;(MLP)&#21644;&#19968;&#20010;transformer&#23618;&#31561;&#32467;&#26500;&#20316;&#20026;&#36731;&#37327;&#32423;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19135v1 Announce Type: cross  Abstract: Large language models (LLM) have been extensively applied in various natural language tasks and domains, but their applicability is constrained by the large number of parameters of the models. Consequently, there is an increasing emphasis on compact models that exhibit high performance. In this study, we observe that different layers in LLM have varying degrees of perturbation on the hidden states, which allows us to identify less important layers. Based on this phenomenon, we propose LLM-Streamline, which consists of two parts: layer pruning, where we remove a set of consecutive layers with the lowest importance in the model according to the target sparsity; and layer replacement, where we train a lightweight model to substitute the pruned layers, thereby mitigating the performance degradation caused by pruning. In our experiments, we utilize structures such as a multi-layer perceptron (MLP) and a transformer layer as lightweight mode
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;MFORT-QA&#26041;&#27861;&#65292;&#36890;&#36807;Few-Shot Learning&#21644;&#22823;&#35821;&#35328;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#22312;&#34920;&#26684;&#25968;&#25454;&#20013;&#36827;&#34892;&#22810;&#36339;&#23569;&#26679;&#26412;&#30340;&#24320;&#25918;&#24335;&#20016;&#23500;&#38382;&#31572;&#12290;</title><link>https://arxiv.org/abs/2403.19116</link><description>&lt;p&gt;
MFORT-QA: &#22810;&#36339;&#23569;&#26679;&#26412;&#24320;&#25918;&#24335;&#20016;&#23500;&#34920;&#26684;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
MFORT-QA: Multi-hop Few-shot Open Rich Table Question Answering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19116
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;MFORT-QA&#26041;&#27861;&#65292;&#36890;&#36807;Few-Shot Learning&#21644;&#22823;&#35821;&#35328;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#22312;&#34920;&#26684;&#25968;&#25454;&#20013;&#36827;&#34892;&#22810;&#36339;&#23569;&#26679;&#26412;&#30340;&#24320;&#25918;&#24335;&#20016;&#23500;&#38382;&#31572;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24403;&#20170;&#24555;&#33410;&#22863;&#30340;&#34892;&#19994;&#20013;&#65292;&#19987;&#19994;&#20154;&#22763;&#27599;&#22825;&#38754;&#20020;&#30528;&#24635;&#32467;&#22823;&#37327;&#25991;&#26723;&#24182;&#20174;&#20013;&#25552;&#21462;&#20851;&#38190;&#20449;&#24687;&#30340;&#25361;&#25112;&#12290;&#36825;&#20123;&#24230;&#37327;&#32463;&#24120;&#38544;&#34255;&#22312;&#34920;&#26684;&#21644;/&#25110;&#20854;&#23884;&#22871;&#30340;&#36229;&#38142;&#25509;&#20013;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#24320;&#21457;&#20102;&#34920;&#26684;&#38382;&#31572;&#65288;QA&#65289;&#26041;&#27861;&#26469;&#25552;&#21462;&#30456;&#20851;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#34920;&#26684;QA&#35757;&#32451;&#20219;&#21153;&#24182;&#19981;&#24635;&#26159;&#33021;&#30830;&#20445;&#25552;&#21462;&#20934;&#30830;&#31572;&#26696;&#65292;&#36825;&#20123;&#20219;&#21153;&#20250;&#21521;&#38382;&#39064;&#25552;&#20379;&#19968;&#20010;&#34920;&#26684;&#21644;&#19968;&#20010;&#25110;&#22810;&#20010;&#26469;&#33258;&#40644;&#37329;&#21333;&#20803;&#26684;&#22352;&#26631;&#30340;&#31572;&#26696;&#12290;&#36817;&#26399;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#36827;&#23637;&#20026;&#20351;&#29992;&#25552;&#31034;&#20174;&#34920;&#26684;&#25968;&#25454;&#20013;&#25552;&#21462;&#20449;&#24687;&#24320;&#36767;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;Multi-hop Few-shot Open Rich Table QA&#65288;MFORT-QA&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21253;&#25324;&#20004;&#20010;&#20027;&#35201;&#27493;&#39588;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19116v1 Announce Type: cross  Abstract: In today's fast-paced industry, professionals face the challenge of summarizing a large number of documents and extracting vital information from them on a daily basis. These metrics are frequently hidden away in tables and/or their nested hyperlinks. To address this challenge, the approach of Table Question Answering (QA) has been developed to extract the relevant information. However, traditional Table QA training tasks that provide a table and an answer(s) from a gold cell coordinate(s) for a question may not always ensure extracting the accurate answer(s). Recent advancements in Large Language Models (LLMs) have opened up new possibilities for extracting information from tabular data using prompts. In this paper, we introduce the Multi-hop Few-shot Open Rich Table QA (MFORT-QA) approach, which consists of two major steps. The first step involves Few-Shot Learning (FSL), where relevant tables and associated contexts of hyperlinks ar
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#34920;&#26126;&#20256;&#32479;&#30340;&#25991;&#26412;&#34164;&#28085;&#26041;&#27861;&#26080;&#27861;&#26377;&#25928;&#22320;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#20869;&#23481;&#20013;&#23384;&#22312;&#30340;&#24187;&#35273;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.19113</link><description>&lt;p&gt;
FACTOID: &#29992;&#20110;&#24187;&#35273;&#26816;&#27979;&#30340;&#20107;&#23454;&#25512;&#29702;&#65288;FACTOID: FACtual enTailment fOr hallucInation Detection&#65289;
&lt;/p&gt;
&lt;p&gt;
FACTOID: FACtual enTailment fOr hallucInation Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19113
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#34920;&#26126;&#20256;&#32479;&#30340;&#25991;&#26412;&#34164;&#28085;&#26041;&#27861;&#26080;&#27861;&#26377;&#25928;&#22320;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#20869;&#23481;&#20013;&#23384;&#22312;&#30340;&#24187;&#35273;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24191;&#27867;&#24212;&#29992;&#24102;&#26469;&#20102;&#35768;&#22810;&#22909;&#22788;&#65292;&#20294;&#24187;&#35273;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#38382;&#39064;&#12290;&#20026;&#24212;&#23545;&#36825;&#19968;&#38382;&#39064;&#65292;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#20316;&#20026;&#19968;&#31181;&#39640;&#24230;&#26377;&#21069;&#36884;&#30340;&#33539;&#24335;&#20986;&#29616;&#65292;&#36890;&#36807;&#22312;&#20107;&#23454;&#20449;&#24687;&#20013;&#25509;&#22320;&#26469;&#25552;&#21319;LLMs&#30340;&#36755;&#20986;&#12290;RAG&#20381;&#36182;&#20110;&#25991;&#26412;&#34164;&#28085;&#65288;TE&#65289;&#25110;&#31867;&#20284;&#26041;&#27861;&#65292;&#26816;&#26597;LLMs&#29983;&#25104;&#30340;&#25991;&#26412;&#26159;&#21542;&#34987;&#26816;&#32034;&#25991;&#26723;&#25903;&#25345;&#25110;&#21453;&#39539;&#12290;&#26412;&#25991;&#35748;&#20026;&#20256;&#32479;&#30340;TE&#26041;&#27861;&#19981;&#36866;&#29992;&#20110;&#21457;&#29616;LLMs&#29983;&#25104;&#20869;&#23481;&#20013;&#30340;&#24187;&#35273;&#12290;&#20363;&#22914;&#65292;&#32771;&#34385;&#19968;&#20010;&#20851;&#20110;&#8220;&#32654;&#22269;&#23545;&#20044;&#20811;&#20848;&#25112;&#20105;&#31435;&#22330;&#8221;&#30340;&#25552;&#31034;&#12290;AI&#29983;&#25104;&#30340;&#25991;&#26412;&#34920;&#31034;&#65292;&#8220;&#32654;&#22269;&#24635;&#32479;&#24052;&#25289;&#20811;&#183;&#22885;&#24052;&#39532;&#35828;&#32654;&#22269;&#19981;&#20250;&#27966;&#20853;&#36827;&#20837;&#20044;&#20811;&#20848;&#8221;&#65292;&#28982;&#32780;&#22312;&#25112;&#20105;&#26399;&#38388;&#65292;&#32654;&#22269;&#24635;&#32479;&#26159;&#20052;&#183;&#25308;&#30331;&#65292;&#36825;&#19982;&#20107;&#23454;&#19981;&#31526;&#12290;&#27492;&#22806;&#65292;&#24403;&#21069;&#30340;TE&#31995;&#32479;&#26080;&#27861;&#20934;&#30830;&#27880;&#37322;&#32473;&#23450;&#30340;&#25991;&#26412;&#21644;&#35782;&#21035;&#24187;&#35273;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19113v1 Announce Type: cross  Abstract: The widespread adoption of Large Language Models (LLMs) has facilitated numerous benefits. However, hallucination is a significant concern. In response, Retrieval Augmented Generation (RAG) has emerged as a highly promising paradigm to improve LLM outputs by grounding them in factual information. RAG relies on textual entailment (TE) or similar methods to check if the text produced by LLMs is supported or contradicted, compared to retrieved documents. This paper argues that conventional TE methods are inadequate for spotting hallucinations in content generated by LLMs. For instance, consider a prompt about the 'USA's stance on the Ukraine war''. The AI-generated text states, ...U.S. President Barack Obama says the U.S. will not put troops in Ukraine...'' However, during the war the U.S. president is Joe Biden which contradicts factual reality. Moreover, current TE systems are unable to accurately annotate the given text and identify th
&lt;/p&gt;</description></item><item><title>PRISM&#26159;&#19968;&#31181;&#31639;&#27861;&#65292;&#21487;&#20197;&#33258;&#21160;&#35782;&#21035;&#20154;&#31867;&#21487;&#35299;&#37322;&#19988;&#26131;&#20256;&#36882;&#30340;&#25552;&#31034;&#65292;&#20174;&#32780;&#26377;&#25928;&#29983;&#25104;&#25152;&#38656;&#27010;&#24565;&#65292;&#20165;&#20351;&#29992;&#40657;&#30418;&#35775;&#38382;T2I&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2403.19103</link><description>&lt;p&gt;
&#29992;&#20110;&#20010;&#24615;&#21270;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#30340;&#33258;&#21160;&#21270;&#40657;&#30418;&#25552;&#31034;&#24037;&#31243;
&lt;/p&gt;
&lt;p&gt;
Automated Black-box Prompt Engineering for Personalized Text-to-Image Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19103
&lt;/p&gt;
&lt;p&gt;
PRISM&#26159;&#19968;&#31181;&#31639;&#27861;&#65292;&#21487;&#20197;&#33258;&#21160;&#35782;&#21035;&#20154;&#31867;&#21487;&#35299;&#37322;&#19988;&#26131;&#20256;&#36882;&#30340;&#25552;&#31034;&#65292;&#20174;&#32780;&#26377;&#25928;&#29983;&#25104;&#25152;&#38656;&#27010;&#24565;&#65292;&#20165;&#20351;&#29992;&#40657;&#30418;&#35775;&#38382;T2I&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#31034;&#24037;&#31243;&#23545;&#20110;&#25511;&#21046;&#25991;&#26412;&#21040;&#22270;&#20687;&#65288;T2I&#65289;&#29983;&#25104;&#27169;&#22411;&#30340;&#36755;&#20986;&#26159;&#26377;&#25928;&#30340;&#65292;&#20294;&#30001;&#20110;&#38656;&#35201;&#25163;&#21160;&#21046;&#20316;&#25552;&#31034;&#32780;&#23548;&#33268;&#24037;&#20316;&#32321;&#37325;&#12290;&#36825;&#19968;&#25361;&#25112;&#20419;&#20351;&#20102;&#33258;&#21160;&#25552;&#31034;&#29983;&#25104;&#31639;&#27861;&#30340;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#22312;T2I&#27169;&#22411;&#20043;&#38388;&#30340;&#21487;&#20256;&#36882;&#24615;&#26041;&#38754;&#36935;&#21040;&#22256;&#38590;&#65292;&#38656;&#35201;&#23545;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#30333;&#30418;&#35775;&#38382;&#65292;&#24182;&#20135;&#29983;&#38750;&#30452;&#35266;&#30340;&#25552;&#31034;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;PRISM&#65292;&#36825;&#26159;&#19968;&#31181;&#31639;&#27861;&#65292;&#21487;&#20197;&#20165;&#20351;&#29992;&#40657;&#30418;&#35775;&#38382;T2I&#27169;&#22411;&#23601;&#33258;&#21160;&#35782;&#21035;&#20154;&#31867;&#21487;&#35299;&#37322;&#19988;&#26131;&#20256;&#36882;&#30340;&#25552;&#31034;&#65292;&#20174;&#32780;&#26377;&#25928;&#29983;&#25104;&#25152;&#38656;&#27010;&#24565;&#12290;&#21463;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36234;&#29425;&#30340;&#21551;&#21457;&#65292;PRISM&#21033;&#29992;LLM&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#26469;&#36845;&#20195;&#22320;&#25913;&#36827;&#32473;&#23450;&#21442;&#32771;&#22270;&#20687;&#30340;&#20505;&#36873;&#25552;&#31034;&#20998;&#24067;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#23637;&#31034;&#20102;PRISM&#22312;&#20026;&#23545;&#35937;&#12289;&#26679;&#24335;&#31561;&#29983;&#25104;&#20934;&#30830;&#25552;&#31034;&#26041;&#38754;&#30340;&#22810;&#26679;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19103v1 Announce Type: cross  Abstract: Prompt engineering is effective for controlling the output of text-to-image (T2I) generative models, but it is also laborious due to the need for manually crafted prompts. This challenge has spurred the development of algorithms for automated prompt generation. However, these methods often struggle with transferability across T2I models, require white-box access to the underlying model, and produce non-intuitive prompts. In this work, we introduce PRISM, an algorithm that automatically identifies human-interpretable and transferable prompts that can effectively generate desired concepts given only black-box access to T2I models. Inspired by large language model (LLM) jailbreaking, PRISM leverages the in-context learning ability of LLMs to iteratively refine the candidate prompts distribution for given reference images. Our experiments demonstrate the versatility and effectiveness of PRISM in generating accurate prompts for objects, sty
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#24320;&#21457;&#19968;&#20010;&#27169;&#22411;&#65292;&#36890;&#36807;&#25552;&#31034;&#21644;&#24230;&#37327;&#36716;&#25442;&#22120;&#26469;&#35780;&#20272;&#36890;&#29992;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#22270;&#20687;&#30340;&#36136;&#37327;&#65292;&#22635;&#34917;&#24403;&#21069;&#35780;&#20272;&#26041;&#27861;&#19982;&#26368;&#20808;&#36827;&#26631;&#20934;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;</title><link>https://arxiv.org/abs/2403.19101</link><description>&lt;p&gt;
AAPMT: &#36890;&#36807;&#25552;&#31034;&#21644;&#24230;&#37327;&#36716;&#25442;&#22120;&#35780;&#20272;&#36890;&#29992;&#20154;&#24037;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
AAPMT: AGI Assessment Through Prompt and Metric Transformer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19101
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#24320;&#21457;&#19968;&#20010;&#27169;&#22411;&#65292;&#36890;&#36807;&#25552;&#31034;&#21644;&#24230;&#37327;&#36716;&#25442;&#22120;&#26469;&#35780;&#20272;&#36890;&#29992;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#22270;&#20687;&#30340;&#36136;&#37327;&#65292;&#22635;&#34917;&#24403;&#21069;&#35780;&#20272;&#26041;&#27861;&#19982;&#26368;&#20808;&#36827;&#26631;&#20934;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#30340;&#20986;&#29616;&#26631;&#24535;&#30528;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#22270;&#20687;&#65288;AGIs&#65289;&#21457;&#23637;&#21382;&#31243;&#20013;&#30340;&#37325;&#35201;&#37324;&#31243;&#30865;&#65292;&#25193;&#22823;&#20102;&#23427;&#20204;&#22312;&#35774;&#35745;&#12289;&#23089;&#20048;&#31561;&#21508;&#20010;&#39046;&#22495;&#30340;&#24212;&#29992;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#36825;&#20123;&#31361;&#30772;&#65292;&#20294;AGIs&#30340;&#36136;&#37327;&#24448;&#24448;&#20173;&#28982;&#19981;&#29702;&#24819;&#65292;&#31361;&#26174;&#20102;&#26377;&#25928;&#35780;&#20272;&#26041;&#27861;&#30340;&#24517;&#35201;&#24615;&#12290;&#36825;&#20123;&#26041;&#27861;&#23545;&#20110;&#35780;&#20272;&#22270;&#20687;&#19982;&#20854;&#25991;&#26412;&#25551;&#36848;&#20043;&#38388;&#30340;&#36136;&#37327;&#33267;&#20851;&#37325;&#35201;&#65292;&#24182;&#19988;&#24517;&#39035;&#20934;&#30830;&#21453;&#26144;&#20154;&#31867;&#24863;&#30693;&#12290;&#22312;&#36825;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#23454;&#36136;&#24615;&#36827;&#23637;&#65292;&#21019;&#26032;&#25216;&#26415;&#22914;BLIP&#21644;DBCNN&#20570;&#20986;&#20102;&#37325;&#22823;&#36129;&#29486;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#65292;&#21253;&#25324;AGIQA-3K&#65292;&#25581;&#31034;&#20102;&#24403;&#21069;&#26041;&#27861;&#19982;&#26368;&#20808;&#36827;&#26631;&#20934;&#20043;&#38388;&#30340;&#26174;&#33879;&#24046;&#24322;&#12290;&#36825;&#19968;&#24046;&#36317;&#24378;&#35843;&#20102;&#26356;&#22797;&#26434;&#21644;&#20934;&#30830;&#30340;&#35780;&#20272;&#24230;&#37327;&#30340;&#24517;&#35201;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#24320;&#21457;&#19968;&#20010;&#33021;&#22815;&#20026;&#24230;&#37327;&#25552;&#20379;&#35780;&#20998;&#30340;&#27169;&#22411;&#65292;&#37325;&#28857;&#20851;&#27880;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19101v1 Announce Type: cross  Abstract: The emergence of text-to-image models marks a significant milestone in the evolution of AI-generated images (AGIs), expanding their use in diverse domains like design, entertainment, and more. Despite these breakthroughs, the quality of AGIs often remains suboptimal, highlighting the need for effective evaluation methods. These methods are crucial for assessing the quality of images relative to their textual descriptions, and they must accurately mirror human perception. Substantial progress has been achieved in this domain, with innovative techniques such as BLIP and DBCNN contributing significantly. However, recent studies, including AGIQA-3K, reveal a notable discrepancy between current methods and state-of-the-art (SOTA) standards. This gap emphasizes the necessity for a more sophisticated and precise evaluation metric. In response, our objective is to develop a model that could give ratings for metrics, which focuses on parameters
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#21487;&#24494;&#20998;&#30340;&#22522;&#20110;&#20219;&#21153;&#21551;&#21457;&#30340;&#25509;&#35302;&#24863;&#30693;&#26426;&#22120;&#20154;&#35774;&#35745;&#26694;&#26550;Task2Morph&#65292;&#36890;&#36807;&#23558;&#20219;&#21153;&#29305;&#24449;&#25277;&#35937;&#21040;&#20219;&#21153;&#21040;&#24418;&#24577;&#30340;&#26144;&#23556;&#20013;&#65292;&#23454;&#29616;&#25972;&#20307;&#20248;&#21270;&#21644;&#26144;&#23556;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2403.19093</link><description>&lt;p&gt;
Task2Morph&#65306;&#21487;&#24494;&#20998;&#30340;&#22522;&#20110;&#20219;&#21153;&#21551;&#21457;&#30340;&#25509;&#35302;&#24863;&#30693;&#26426;&#22120;&#20154;&#35774;&#35745;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Task2Morph: Differentiable Task-inspired Framework for Contact-Aware Robot Design
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19093
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#21487;&#24494;&#20998;&#30340;&#22522;&#20110;&#20219;&#21153;&#21551;&#21457;&#30340;&#25509;&#35302;&#24863;&#30693;&#26426;&#22120;&#20154;&#35774;&#35745;&#26694;&#26550;Task2Morph&#65292;&#36890;&#36807;&#23558;&#20219;&#21153;&#29305;&#24449;&#25277;&#35937;&#21040;&#20219;&#21153;&#21040;&#24418;&#24577;&#30340;&#26144;&#23556;&#20013;&#65292;&#23454;&#29616;&#25972;&#20307;&#20248;&#21270;&#21644;&#26144;&#23556;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20248;&#21270;&#36866;&#24212;&#21508;&#31181;&#20219;&#21153;&#30340;&#26426;&#22120;&#20154;&#24418;&#24577;&#21644;&#25511;&#21046;&#22120;&#26159;&#26426;&#22120;&#20154;&#35774;&#35745;&#39046;&#22495;&#20013;&#30340;&#20851;&#38190;&#38382;&#39064;&#65292;&#21448;&#31216;&#20026;&#20855;&#36523;&#26234;&#33021;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#19988;&#36890;&#29992;&#30340;&#21487;&#24494;&#20998;&#20219;&#21153;&#21551;&#21457;&#24335;&#25509;&#35302;&#24863;&#30693;&#26426;&#22120;&#20154;&#35774;&#35745;&#26694;&#26550;Task2Morph&#12290;&#25105;&#20204;&#23558;&#19982;&#20219;&#21153;&#24615;&#33021;&#39640;&#24230;&#30456;&#20851;&#30340;&#20219;&#21153;&#29305;&#24449;&#25277;&#35937;&#20986;&#26469;&#65292;&#29992;&#20110;&#26500;&#24314;&#20219;&#21153;&#21040;&#24418;&#24577;&#30340;&#26144;&#23556;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#26144;&#23556;&#23884;&#20837;&#21040;&#19968;&#20010;&#21487;&#24494;&#20998;&#30340;&#26426;&#22120;&#20154;&#35774;&#35745;&#36807;&#31243;&#20013;&#65292;&#20854;&#20013;&#26799;&#24230;&#20449;&#24687;&#34987;&#29992;&#20110;&#26144;&#23556;&#23398;&#20064;&#20197;&#21450;&#25972;&#20307;&#20248;&#21270;&#12290;&#23454;&#39564;&#22312;&#26576;&#19968;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19093v1 Announce Type: cross  Abstract: Optimizing the morphologies and the controllers that adapt to various tasks is a critical issue in the field of robot design, aka. embodied intelligence. Previous works typically model it as a joint optimization problem and use search-based methods to find the optimal solution in the morphology space. However, they ignore the implicit knowledge of task-to-morphology mapping which can directly inspire robot design. For example, flipping heavier boxes tends to require more muscular robot arms. This paper proposes a novel and general differentiable task-inspired framework for contact-aware robot design called Task2Morph. We abstract task features highly related to task performance and use them to build a task-to-morphology mapping. Further, we embed the mapping into a differentiable robot design process, where the gradient information is leveraged for both the mapping learning and the whole optimization. The experiments are conducted on t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#36125;&#21494;&#26031;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#32467;&#21512;&#28145;&#24230;&#23398;&#20064;&#21644;&#36125;&#21494;&#26031;&#32593;&#32476;&#30340;&#20248;&#21183;&#65292;&#26368;&#23567;&#21270;&#21508;&#33258;&#30340;&#21155;&#21183;&#65292;&#24182;&#20998;&#26512;&#20102;&#20854;&#22312;&#20581;&#24247;&#20135;&#19994;&#20013;&#22270;&#20687;&#20998;&#31867;&#26041;&#38754;&#30340;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2403.19083</link><description>&lt;p&gt;
&#20351;&#29992;&#36125;&#21494;&#26031;&#32593;&#32476;&#21644;&#28145;&#24230;&#23398;&#20064;&#25913;&#36827;&#30284;&#30151;&#25104;&#20687;&#35786;&#26029;&#65306;&#19968;&#31181;&#36125;&#21494;&#26031;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Improving Cancer Imaging Diagnosis with Bayesian Networks and Deep Learning: A Bayesian Deep Learning Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19083
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36125;&#21494;&#26031;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#32467;&#21512;&#28145;&#24230;&#23398;&#20064;&#21644;&#36125;&#21494;&#26031;&#32593;&#32476;&#30340;&#20248;&#21183;&#65292;&#26368;&#23567;&#21270;&#21508;&#33258;&#30340;&#21155;&#21183;&#65292;&#24182;&#20998;&#26512;&#20102;&#20854;&#22312;&#20581;&#24247;&#20135;&#19994;&#20013;&#22270;&#20687;&#20998;&#31867;&#26041;&#38754;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#29702;&#35770;&#21644;&#31639;&#27861;&#21487;&#20197;&#21019;&#24314;&#35768;&#22810;&#31934;&#30830;&#30340;&#27169;&#22411;&#26469;&#35757;&#32451;&#21644;&#39044;&#27979;&#32473;&#23450;&#30340;&#25968;&#25454;&#38598;&#12290;&#26412;&#25991;&#26088;&#22312;&#30740;&#31350;&#28145;&#24230;&#23398;&#20064;&#21644;&#36125;&#21494;&#26031;&#32593;&#32476;&#39044;&#27979;&#27169;&#22411;&#32972;&#21518;&#30340;&#29702;&#35770;&#65292;&#26681;&#25454;&#27599;&#20010;&#27169;&#22411;&#30340;&#20248;&#21183;&#21644;&#21155;&#21183;&#65292;&#37319;&#29992;&#19981;&#21516;&#30340;&#26041;&#27861;&#26500;&#24314;&#36125;&#21494;&#26031;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#32467;&#21512;&#20248;&#21183;&#21516;&#26102;&#26368;&#23567;&#21270;&#21155;&#21183;&#12290;&#26368;&#32456;&#65292;&#23558;&#20998;&#26512;&#32467;&#26524;&#36125;&#21494;&#26031;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#20581;&#24247;&#20135;&#19994;&#20013;&#20998;&#31867;&#22270;&#20687;&#30340;&#24212;&#29992;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19083v1 Announce Type: cross  Abstract: With recent advancements in the development of artificial intelligence applications using theories and algorithms in machine learning, many accurate models can be created to train and predict on given datasets. With the realization of the importance of imaging interpretation in cancer diagnosis, this article aims to investigate the theory behind Deep Learning and Bayesian Network prediction models. Based on the advantages and drawbacks of each model, different approaches will be used to construct a Bayesian Deep Learning Model, combining the strengths while minimizing the weaknesses. Finally, the applications and accuracy of the resulting Bayesian Deep Learning approach in the health industry in classifying images will be analyzed.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#21033;&#29992;E-&#26816;&#39564;&#32479;&#35745;&#24341;&#20837;BB-predictor&#65292;&#22686;&#24378;&#31526;&#21512;&#24615;&#39044;&#27979;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.19082</link><description>&lt;p&gt;
&#21033;&#29992;E-&#26816;&#39564;&#32479;&#35745;&#22686;&#24378;&#31526;&#21512;&#24615;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Enhancing Conformal Prediction Using E-Test Statistics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19082
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#21033;&#29992;E-&#26816;&#39564;&#32479;&#35745;&#24341;&#20837;BB-predictor&#65292;&#22686;&#24378;&#31526;&#21512;&#24615;&#39044;&#27979;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31526;&#21512;&#24615;&#39044;&#27979;&#65288;CP&#65289;&#20316;&#20026;&#19968;&#31181;&#31283;&#20581;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#37327;&#21270;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#25152;&#20570;&#39044;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#19982;&#20256;&#32479;&#30340;&#28857;&#39044;&#27979;&#22120;&#19981;&#21516;&#65292;CP&#22522;&#20110;&#25968;&#25454;&#21487;&#20132;&#25442;&#24615;&#30340;&#20551;&#35774;&#29983;&#25104;&#32479;&#35745;&#19978;&#26377;&#25928;&#30340;&#39044;&#27979;&#21306;&#22495;&#65292;&#20063;&#31216;&#20026;&#39044;&#27979;&#21306;&#38388;&#12290;&#36890;&#24120;&#65292;&#26500;&#24314;&#31526;&#21512;&#24615;&#39044;&#27979;&#20381;&#36182;&#20110;p&#20540;&#12290;&#28982;&#32780;&#65292;&#26412;&#25991;&#36208;&#19978;&#20102;&#21478;&#19968;&#26465;&#36335;&#24452;&#65292;&#21033;&#29992;E-&#26816;&#39564;&#32479;&#35745;&#30340;&#21147;&#37327;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#19979;&#30028;&#39044;&#27979;&#22120;&#65288;BB-predictor&#65289;&#26469;&#22686;&#24378;&#31526;&#21512;&#24615;&#39044;&#27979;&#30340;&#25928;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19082v1 Announce Type: cross  Abstract: Conformal Prediction (CP) serves as a robust framework that quantifies uncertainty in predictions made by Machine Learning (ML) models. Unlike traditional point predictors, CP generates statistically valid prediction regions, also known as prediction intervals, based on the assumption of data exchangeability. Typically, the construction of conformal predictions hinges on p-values. This paper, however, ventures down an alternative path, harnessing the power of e-test statistics to augment the efficacy of conformal predictions by introducing a BB-predictor (bounded from the below predictor).
&lt;/p&gt;</description></item><item><title>&#35813;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MVEB&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#28040;&#38500;&#22270;&#20687;&#35270;&#22270;&#20043;&#38388;&#19981;&#20849;&#20139;&#30340;&#22810;&#20313;&#20449;&#24687;&#65292;&#23454;&#29616;&#20102;&#26368;&#23567;&#36275;&#22815;&#34920;&#31034;&#30340;&#23398;&#20064;&#65292;&#24182;&#35299;&#20915;&#20102;&#20114;&#20449;&#24687;&#35745;&#31639;&#22256;&#38590;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.19078</link><description>&lt;p&gt;
MVEB&#65306;&#20855;&#26377;&#22810;&#35270;&#22270;&#29109;&#29942;&#39048;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
MVEB: Self-Supervised Learning with Multi-View Entropy Bottleneck
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19078
&lt;/p&gt;
&lt;p&gt;
&#35813;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MVEB&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#28040;&#38500;&#22270;&#20687;&#35270;&#22270;&#20043;&#38388;&#19981;&#20849;&#20139;&#30340;&#22810;&#20313;&#20449;&#24687;&#65292;&#23454;&#29616;&#20102;&#26368;&#23567;&#36275;&#22815;&#34920;&#31034;&#30340;&#23398;&#20064;&#65292;&#24182;&#35299;&#20915;&#20102;&#20114;&#20449;&#24687;&#35745;&#31639;&#22256;&#38590;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#26088;&#22312;&#23398;&#20064;&#33021;&#22815;&#26377;&#25928;&#25512;&#24191;&#21040;&#19979;&#28216;&#20219;&#21153;&#30340;&#34920;&#31034;&#12290;&#35768;&#22810;&#33258;&#30417;&#30563;&#26041;&#27861;&#23558;&#22270;&#20687;&#30340;&#20004;&#20010;&#35270;&#22270;&#35270;&#20026;&#36755;&#20837;&#21644;&#33258;&#30417;&#30563;&#20449;&#21495;&#65292;&#20551;&#23450;&#20219;&#19968;&#35270;&#22270;&#37117;&#21253;&#21547;&#30456;&#21516;&#30340;&#19982;&#20219;&#21153;&#30456;&#20851;&#20449;&#24687;&#65292;&#24182;&#19988;&#20849;&#20139;&#20449;&#24687;&#65288;&#22823;&#33268;&#65289;&#36275;&#20197;&#39044;&#27979;&#19979;&#28216;&#20219;&#21153;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#33293;&#24323;&#35270;&#22270;&#20043;&#38388;&#19981;&#20849;&#20139;&#30340;&#22810;&#20313;&#20449;&#24687;&#21487;&#20197;&#25913;&#21892;&#27867;&#21270;&#33021;&#21147;&#12290;&#22240;&#27492;&#65292;&#29702;&#24819;&#30340;&#34920;&#31034;&#23545;&#19979;&#28216;&#20219;&#21153;&#26159;&#36275;&#22815;&#30340;&#65292;&#24182;&#19988;&#21253;&#21547;&#26368;&#23569;&#30340;&#22810;&#20313;&#20449;&#24687;&#65292;&#31216;&#20026;&#26368;&#23567;&#36275;&#22815;&#34920;&#31034;&#12290;&#19968;&#20010;&#21487;&#20197;&#36890;&#36807;&#26368;&#22823;&#21270;&#34920;&#31034;&#21644;&#30417;&#30563;&#35270;&#22270;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#24182;&#28040;&#38500;&#22810;&#20313;&#20449;&#24687;&#26469;&#23398;&#20064;&#36825;&#31181;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#20114;&#20449;&#24687;&#30340;&#35745;&#31639;&#22240;&#20026;&#22256;&#38590;&#32780;&#33261;&#21517;&#26157;&#30528;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#31216;&#20026;&#22810;&#35270;&#22270;&#29109;&#29942;&#39048;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19078v1 Announce Type: cross  Abstract: Self-supervised learning aims to learn representation that can be effectively generalized to downstream tasks. Many self-supervised approaches regard two views of an image as both the input and the self-supervised signals, assuming that either view contains the same task-relevant information and the shared information is (approximately) sufficient for predicting downstream tasks. Recent studies show that discarding superfluous information not shared between the views can improve generalization. Hence, the ideal representation is sufficient for downstream tasks and contains minimal superfluous information, termed minimal sufficient representation. One can learn this representation by maximizing the mutual information between the representation and the supervised view while eliminating superfluous information. Nevertheless, the computation of mutual information is notoriously intractable. In this work, we propose an objective termed mult
&lt;/p&gt;</description></item><item><title>TinyML&#26159;&#19968;&#31181;&#23558;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21387;&#32553;&#21040;&#29289;&#32852;&#32593;&#35774;&#22791;&#21644;&#24494;&#25511;&#21046;&#22120;&#20013;&#23454;&#29616;&#26080;&#22788;&#19981;&#22312;&#26234;&#33021;&#30340;&#26032;&#26041;&#27861;&#65292;&#38656;&#35201;&#20849;&#21516;&#35774;&#35745;&#31639;&#27861;&#21644;&#31995;&#32479;&#22534;&#26632;&#20197;&#20811;&#26381;&#30828;&#20214;&#38480;&#21046;&#12290;</title><link>https://arxiv.org/abs/2403.19076</link><description>&lt;p&gt;
&#23567;&#22411;&#26426;&#22120;&#23398;&#20064;&#65306;&#36827;&#23637;&#19982;&#26410;&#26469;&#23637;&#26395;
&lt;/p&gt;
&lt;p&gt;
Tiny Machine Learning: Progress and Futures
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19076
&lt;/p&gt;
&lt;p&gt;
TinyML&#26159;&#19968;&#31181;&#23558;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21387;&#32553;&#21040;&#29289;&#32852;&#32593;&#35774;&#22791;&#21644;&#24494;&#25511;&#21046;&#22120;&#20013;&#23454;&#29616;&#26080;&#22788;&#19981;&#22312;&#26234;&#33021;&#30340;&#26032;&#26041;&#27861;&#65292;&#38656;&#35201;&#20849;&#21516;&#35774;&#35745;&#31639;&#27861;&#21644;&#31995;&#32479;&#22534;&#26632;&#20197;&#20811;&#26381;&#30828;&#20214;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Tiny Machine Learning&#65288;TinyML&#65289;&#26159;&#26426;&#22120;&#23398;&#20064;&#30340;&#19968;&#20010;&#26032;&#39046;&#22495;&#12290;&#36890;&#36807;&#23558;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21387;&#32553;&#21040;&#25968;&#21313;&#20159;&#20010;&#29289;&#32852;&#32593;&#35774;&#22791;&#21644;&#24494;&#25511;&#21046;&#22120;&#65288;MCUs&#65289;&#20013;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#30340;&#33539;&#22260;&#65292;&#23454;&#29616;&#20102;&#26080;&#22788;&#19981;&#22312;&#30340;&#26234;&#33021;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#30828;&#20214;&#38480;&#21046;&#65292;TinyML&#20855;&#26377;&#25361;&#25112;&#24615;&#65306;&#26377;&#38480;&#30340;&#20869;&#23384;&#36164;&#28304;&#20351;&#24471;&#38590;&#20197;&#23481;&#32435;&#20026;&#20113;&#21644;&#31227;&#21160;&#24179;&#21488;&#35774;&#35745;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;&#23545;&#20110;&#35064;&#26426;&#35774;&#22791;&#65292;&#32534;&#35793;&#22120;&#21644;&#25512;&#26029;&#24341;&#25806;&#25903;&#25345;&#20063;&#26377;&#38480;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#38656;&#35201;&#20849;&#21516;&#35774;&#35745;&#31639;&#27861;&#21644;&#31995;&#32479;&#22534;&#26632;&#20197;&#23454;&#29616;TinyML&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19076v1 Announce Type: cross  Abstract: Tiny Machine Learning (TinyML) is a new frontier of machine learning. By squeezing deep learning models into billions of IoT devices and microcontrollers (MCUs), we expand the scope of AI applications and enable ubiquitous intelligence. However, TinyML is challenging due to hardware constraints: the tiny memory resource makes it difficult to hold deep learning models designed for cloud and mobile platforms. There is also limited compiler and inference engine support for bare-metal devices. Therefore, we need to co-design the algorithm and system stack to enable TinyML. In this review, we will first discuss the definition, challenges, and applications of TinyML. We then survey the recent progress in TinyML and deep learning on MCUs. Next, we will introduce MCUNet, showing how we can achieve ImageNet-scale AI applications on IoT devices with system-algorithm co-design. We will further extend the solution from inference to training and in
&lt;/p&gt;</description></item><item><title>&#35774;&#35745;&#20102;&#19968;&#31181;&#38754;&#21521;&#25968;&#25454;&#27969;&#30340;PIM&#21551;&#29992;&#30340;&#22810;&#26680;&#20307;&#31995;&#32467;&#26500;&#20197;&#21152;&#36895;&#28145;&#24230;&#23398;&#20064;&#24037;&#20316;&#36127;&#36733;</title><link>https://arxiv.org/abs/2403.19073</link><description>&lt;p&gt;
&#38754;&#21521;&#25968;&#25454;&#27969;&#30340;&#29992;&#20110;&#28145;&#24230;&#23398;&#20064;&#24037;&#20316;&#36127;&#36733;&#30340;PIM&#21551;&#29992;&#30340;&#22810;&#26680;&#20307;&#31995;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
Dataflow-Aware PIM-Enabled Manycore Architecture for Deep Learning Workloads
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19073
&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#20102;&#19968;&#31181;&#38754;&#21521;&#25968;&#25454;&#27969;&#30340;PIM&#21551;&#29992;&#30340;&#22810;&#26680;&#20307;&#31995;&#32467;&#26500;&#20197;&#21152;&#36895;&#28145;&#24230;&#23398;&#20064;&#24037;&#20316;&#36127;&#36733;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Processing-in-memory (PIM)&#24050;&#32463;&#25104;&#20026;&#28145;&#24230;&#23398;&#20064;(DL)&#24037;&#20316;&#36127;&#36733;&#30340;&#33021;&#25928;&#39640;&#24615;&#33021;&#21152;&#36895;&#30340;&#25512;&#21160;&#22240;&#32032;&#12290;&#38459;&#21464;&#38543;&#26426;&#23384;&#21462;&#23384;&#20648;&#22120;(ReRAM)&#26159;&#23454;&#29616;PIM&#30340;&#26368;&#26377;&#21069;&#36884;&#30340;&#25216;&#26415;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(DNNs)&#30340;&#22797;&#26434;&#24230;&#22686;&#21152;&#65292;&#25105;&#20204;&#38656;&#35201;&#35774;&#35745;&#19968;&#20010;&#22810;&#26680;&#20307;&#31995;&#32467;&#26500;&#65292;&#22312;&#21333;&#29255;&#19978;&#26377;&#22810;&#20010;&#22522;&#20110;ReRAM&#30340;&#22788;&#29702;&#20803;&#32032;(PEs)&#12290;&#29616;&#26377;&#30340;&#22522;&#20110;PIM&#30340;&#20307;&#31995;&#32467;&#26500;&#20027;&#35201;&#20851;&#27880;&#35745;&#31639;&#65292;&#32780;&#24573;&#35270;&#36890;&#20449;&#30340;&#20316;&#29992;&#12290;ReRAM&#20026;&#22522;&#30784;&#30340;&#20999;&#29255;&#24335;&#22810;&#26680;&#20307;&#31995;&#32467;&#26500;&#36890;&#24120;&#28041;&#21450;&#35768;&#22810;&#22788;&#29702;&#20803;&#32032;(PEs)&#65292;&#36825;&#20123;&#20803;&#32032;&#38656;&#35201;&#36890;&#36807;&#39640;&#25928;&#30340;&#29255;&#19978;&#36890;&#20449;&#22522;&#30784;&#35774;&#26045;&#30456;&#20114;&#36830;&#25509;&#12290;&#22914;&#26524;&#36890;&#20449;&#22522;&#30784;&#35774;&#26045;&#26080;&#27861;&#36319;&#24471;&#19978;&#65292;&#31616;&#21333;&#22320;&#20998;&#37197;&#26356;&#22810;&#36164;&#28304;(ReRAMs)&#26469;&#21152;&#36895;&#35745;&#31639;&#26159;&#26080;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19073v1 Announce Type: cross  Abstract: Processing-in-memory (PIM) has emerged as an enabler for the energy-efficient and high-performance acceleration of deep learning (DL) workloads. Resistive random-access memory (ReRAM) is one of the most promising technologies to implement PIM. However, as the complexity of Deep convolutional neural networks (DNNs) grows, we need to design a manycore architecture with multiple ReRAM-based processing elements (PEs) on a single chip. Existing PIM-based architectures mostly focus on computation while ignoring the role of communication. ReRAM-based tiled manycore architectures often involve many Processing Elements (PEs), which need to be interconnected via an efficient on-chip communication infrastructure. Simply allocating more resources (ReRAMs) to speed up only computation is ineffective if the communication infrastructure cannot keep up with it. In this paper, we highlight the design principles of a dataflow-aware PIM-enabled manycore 
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#26680;&#24515;&#21019;&#26032;&#26159;&#20351;&#29992;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#26694;&#26550;&#19979;&#30340;&#26333;&#20809;&#21512;&#25104;&#27169;&#22411;&#65292;&#20801;&#35768;&#20174;&#21333;&#20010;&#35266;&#27979;&#20013;&#29983;&#25104;&#19968;&#31995;&#21015;&#26333;&#20809;&#65292;&#20197;&#30830;&#20445;&#19968;&#33268;&#30340;&#26333;&#20809;&#22312;&#20108;&#36827;&#21046;&#22270;&#20687;&#20013;&#36827;&#34892;&#30528;&#33394;&#65292;&#20174;&#32780;&#26174;&#33879;&#22686;&#24378;&#20102;&#30528;&#33394;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.19066</link><description>&lt;p&gt;
&#29983;&#25104;&#37327;&#23376;&#24425;&#33394;&#25104;&#20687;
&lt;/p&gt;
&lt;p&gt;
Generative Quanta Color Imaging
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19066
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#26680;&#24515;&#21019;&#26032;&#26159;&#20351;&#29992;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#26694;&#26550;&#19979;&#30340;&#26333;&#20809;&#21512;&#25104;&#27169;&#22411;&#65292;&#20801;&#35768;&#20174;&#21333;&#20010;&#35266;&#27979;&#20013;&#29983;&#25104;&#19968;&#31995;&#21015;&#26333;&#20809;&#65292;&#20197;&#30830;&#20445;&#19968;&#33268;&#30340;&#26333;&#20809;&#22312;&#20108;&#36827;&#21046;&#22270;&#20687;&#20013;&#36827;&#34892;&#30528;&#33394;&#65292;&#20174;&#32780;&#26174;&#33879;&#22686;&#24378;&#20102;&#30528;&#33394;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19066v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#20132;&#21449; &#25688;&#35201;&#65306;&#21333;&#20809;&#23376;&#30456;&#26426;&#30340;&#24778;&#20154;&#21457;&#23637;&#20026;&#31185;&#23398;&#21644;&#24037;&#19994;&#25104;&#20687;&#21019;&#36896;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#26426;&#20250;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;1&#20301;&#20256;&#24863;&#22120;&#20135;&#29983;&#30340;&#39640;&#25968;&#25454;&#21534;&#21520;&#37327;&#23545;&#20302;&#21151;&#32791;&#24212;&#29992;&#26469;&#35828;&#26500;&#25104;&#20102;&#37325;&#22823;&#29942;&#39048;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#20174;&#21333;&#20809;&#23376;&#30456;&#26426;&#30340;&#21333;&#20010;&#20108;&#36827;&#21046;&#24103;&#29983;&#25104;&#24425;&#33394;&#22270;&#20687;&#30340;&#21487;&#33021;&#24615;&#12290;&#25105;&#20204;&#26126;&#26174;&#21457;&#29616;&#65292;&#30001;&#20110;&#26333;&#20809;&#21464;&#21270;&#30340;&#26174;&#30528;&#31243;&#24230;&#65292;&#36825;&#20010;&#38382;&#39064;&#23545;&#26631;&#20934;&#30528;&#33394;&#26041;&#27861;&#29305;&#21035;&#22256;&#38590;&#12290;&#25105;&#20204;&#35770;&#25991;&#30340;&#26680;&#24515;&#21019;&#26032;&#26159;&#19968;&#20010;&#22312;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;&#31070;&#32463;ODE&#65289;&#19979;&#23450;&#20041;&#30340;&#26333;&#20809;&#21512;&#25104;&#27169;&#22411;&#65292;&#23427;&#20801;&#35768;&#25105;&#20204;&#20174;&#21333;&#20010;&#35266;&#27979;&#20013;&#29983;&#25104;&#19968;&#31995;&#21015;&#26333;&#20809;&#12290;&#36825;&#19968;&#21019;&#26032;&#30830;&#20445;&#20102;&#22312;&#20108;&#36827;&#21046;&#22270;&#20687;&#20013;&#39068;&#33394;&#21270;&#22120;&#37319;&#29992;&#30340;&#19968;&#33268;&#26333;&#20809;&#65292;&#23548;&#33268;&#26174;&#33879;&#22686;&#24378;&#30340;&#30528;&#33394;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#22312;&#21333;&#22270;&#20687;&#21644;&#31361;&#21457;&#30528;&#33394;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19066v1 Announce Type: cross  Abstract: The astonishing development of single-photon cameras has created an unprecedented opportunity for scientific and industrial imaging. However, the high data throughput generated by these 1-bit sensors creates a significant bottleneck for low-power applications. In this paper, we explore the possibility of generating a color image from a single binary frame of a single-photon camera. We evidently find this problem being particularly difficult to standard colorization approaches due to the substantial degree of exposure variation. The core innovation of our paper is an exposure synthesis model framed under a neural ordinary differential equation (Neural ODE) that allows us to generate a continuum of exposures from a single observation. This innovation ensures consistent exposure in binary images that colorizers take on, resulting in notably enhanced colorization. We demonstrate applications of the method in single-image and burst coloriza
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20154;&#31867;&#20013;&#24515;&#30340;&#24314;&#31569;&#26426;&#22120;&#20154;&#26041;&#27861;&#65292;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#39537;&#21160;&#30340;&#21161;&#25163;&#26426;&#22120;&#20154;&#20026;&#26408;&#24037;&#21171;&#21160;&#32773;&#25552;&#20379;&#29615;&#22659;&#19978;&#19979;&#25991;&#21327;&#21161;&#65292;&#25512;&#36827;&#20102;&#26426;&#22120;&#20154;&#22312;&#24314;&#31569;&#20013;&#30340;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2403.19060</link><description>&lt;p&gt;
&#20154;&#31867;&#20013;&#24515;&#26045;&#24037;&#26426;&#22120;&#20154;&#65306;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#21161;&#25163;&#26426;&#22120;&#20154;&#20026;&#26408;&#24037;&#21171;&#21160;&#32773;&#25552;&#20379;&#29615;&#22659;&#19978;&#19979;&#25991;&#21327;&#21161;
&lt;/p&gt;
&lt;p&gt;
Towards Human-Centered Construction Robotics: An RL-Driven Companion Robot For Contextually Assisting Carpentry Workers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19060
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20154;&#31867;&#20013;&#24515;&#30340;&#24314;&#31569;&#26426;&#22120;&#20154;&#26041;&#27861;&#65292;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#39537;&#21160;&#30340;&#21161;&#25163;&#26426;&#22120;&#20154;&#20026;&#26408;&#24037;&#21171;&#21160;&#32773;&#25552;&#20379;&#29615;&#22659;&#19978;&#19979;&#25991;&#21327;&#21161;&#65292;&#25512;&#36827;&#20102;&#26426;&#22120;&#20154;&#22312;&#24314;&#31569;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#20010;&#20805;&#28385;&#27963;&#21147;&#30340;&#24314;&#31569;&#34892;&#19994;&#20013;&#65292;&#20256;&#32479;&#30340;&#26426;&#22120;&#20154;&#38598;&#25104;&#20027;&#35201;&#38598;&#20013;&#22312;&#33258;&#21160;&#21270;&#29305;&#23450;&#20219;&#21153;&#65292;&#36890;&#24120;&#24573;&#30053;&#20102;&#24314;&#31569;&#24037;&#20316;&#27969;&#31243;&#20013;&#20154;&#31867;&#22240;&#32032;&#30340;&#22797;&#26434;&#24615;&#21644;&#21464;&#21270;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#20154;&#20026;&#26412;&#30340;&#26041;&#27861;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#8220;&#24037;&#20316;&#20276;&#20387;&#28459;&#28216;&#22120;&#8221;&#65292;&#26088;&#22312;&#21327;&#21161;&#24314;&#31569;&#24037;&#20154;&#23436;&#25104;&#20854;&#29616;&#26377;&#23454;&#36341;&#65292;&#26088;&#22312;&#22686;&#24378;&#23433;&#20840;&#24615;&#21644;&#24037;&#20316;&#27969;&#31243;&#30340;&#27969;&#30021;&#24615;&#65292;&#21516;&#26102;&#23562;&#37325;&#24314;&#31569;&#21171;&#21160;&#30340;&#25216;&#26415;&#24615;&#36136;&#12290;&#25105;&#20204;&#23545;&#22312;&#26408;&#24037;&#27169;&#26495;&#24037;&#31243;&#20013;&#37096;&#32626;&#26426;&#22120;&#20154;&#31995;&#32479;&#36827;&#34892;&#20102;&#28145;&#20837;&#30740;&#31350;&#65292;&#23637;&#31034;&#20102;&#19968;&#20010;&#21407;&#22411;&#65292;&#36890;&#36807;&#29615;&#22659;&#30456;&#20851;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#39537;&#21160;&#27169;&#22359;&#21270;&#26694;&#26550;&#65292;&#37325;&#28857;&#24378;&#35843;&#20102;&#22312;&#21160;&#24577;&#29615;&#22659;&#20013;&#30340;&#26426;&#21160;&#24615;&#12289;&#23433;&#20840;&#24615;&#21644;&#33298;&#36866;&#30340;&#24037;&#20154;-&#26426;&#22120;&#20154;&#21327;&#20316;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25512;&#36827;&#20102;&#26426;&#22120;&#20154;&#22312;&#24314;&#31569;&#20013;&#30340;&#24212;&#29992;&#65292;&#20513;&#23548;&#21327;&#20316;&#27169;&#22411;&#65292;&#20854;&#20013;&#33258;&#36866;&#24212;&#26426;&#22120;&#20154;&#25903;&#25345;&#32780;&#19981;&#26159;&#21462;&#20195;&#20154;&#31867;&#65292;&#24378;&#35843;&#20102;&#20132;&#20114;&#24335;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19060v1 Announce Type: cross  Abstract: In the dynamic construction industry, traditional robotic integration has primarily focused on automating specific tasks, often overlooking the complexity and variability of human aspects in construction workflows. This paper introduces a human-centered approach with a ``work companion rover" designed to assist construction workers within their existing practices, aiming to enhance safety and workflow fluency while respecting construction labor's skilled nature. We conduct an in-depth study on deploying a robotic system in carpentry formwork, showcasing a prototype that emphasizes mobility, safety, and comfortable worker-robot collaboration in dynamic environments through a contextual Reinforcement Learning (RL)-driven modular framework. Our research advances robotic applications in construction, advocating for collaborative models where adaptive robots support rather than replace humans, underscoring the potential for an interactive a
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#21033;&#29992;&#36807;&#25311;&#21512;&#30340;&#36974;&#34109;&#33258;&#32534;&#30721;&#22120;(MAE)&#26469;&#26816;&#27979;&#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#29983;&#25104;&#24615;&#27169;&#20223;&#65292;&#24314;&#31435;&#20102;&#22522;&#20110;&#35757;&#32451;&#25968;&#25454;&#38598;&#25439;&#22833;&#30340;&#26816;&#27979;&#38408;&#20540;&#65292;&#20026;&#20102;&#30830;&#20445;&#29983;&#25104;&#27169;&#22411;&#30340;&#21512;&#27861;&#20351;&#29992;&#21644;&#25552;&#21319;&#20854;&#27861;&#24459;&#21512;&#35268;&#24615;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.19050</link><description>&lt;p&gt;
&#36890;&#36807;&#36807;&#25311;&#21512;&#30340;&#36974;&#34109;&#33258;&#32534;&#30721;&#22120;&#26816;&#27979;&#29983;&#25104;&#24615;&#27169;&#20223;
&lt;/p&gt;
&lt;p&gt;
Detecting Generative Parroting through Overfitting Masked Autoencoders
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19050
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#21033;&#29992;&#36807;&#25311;&#21512;&#30340;&#36974;&#34109;&#33258;&#32534;&#30721;&#22120;(MAE)&#26469;&#26816;&#27979;&#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#29983;&#25104;&#24615;&#27169;&#20223;&#65292;&#24314;&#31435;&#20102;&#22522;&#20110;&#35757;&#32451;&#25968;&#25454;&#38598;&#25439;&#22833;&#30340;&#26816;&#27979;&#38408;&#20540;&#65292;&#20026;&#20102;&#30830;&#20445;&#29983;&#25104;&#27169;&#22411;&#30340;&#21512;&#27861;&#20351;&#29992;&#21644;&#25552;&#21319;&#20854;&#27861;&#24459;&#21512;&#35268;&#24615;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#30340;&#20986;&#29616;&#24443;&#24213;&#25913;&#21464;&#20102;&#25968;&#23383;&#20869;&#23481;&#21019;&#24314;&#30340;&#26041;&#24335;&#65292;&#28982;&#32780;&#30001;&#20110;&#29983;&#25104;&#24615;&#27169;&#20223;&#38382;&#39064;&#65292;&#27169;&#22411;&#36807;&#20110;&#27169;&#20223;&#20854;&#35757;&#32451;&#25968;&#25454;&#32780;&#32473;&#29256;&#26435;&#23436;&#25972;&#24615;&#24102;&#26469;&#25361;&#25112;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#21363;&#21033;&#29992;&#19968;&#20010;&#36807;&#25311;&#21512;&#30340;&#36974;&#34109;&#33258;&#32534;&#30721;&#22120;(MAE)&#26469;&#26377;&#25928;&#22320;&#26816;&#27979;&#36825;&#31181;&#27169;&#20223;&#26679;&#26412;&#12290;&#25105;&#20204;&#22522;&#20110;&#35757;&#32451;&#25968;&#25454;&#38598;&#19978;&#30340;&#24179;&#22343;&#25439;&#22833;&#24314;&#31435;&#19968;&#20010;&#26816;&#27979;&#38408;&#20540;&#65292;&#20174;&#32780;&#31934;&#30830;&#23450;&#20301;&#20462;&#25913;&#21518;&#25968;&#25454;&#38598;&#20013;&#30340;&#27169;&#20223;&#20869;&#23481;&#12290;&#21021;&#27493;&#35780;&#20272;&#34920;&#26126;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#26174;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30830;&#20445;&#29983;&#25104;&#27169;&#22411;&#30340;&#21512;&#27861;&#20351;&#29992;&#24182;&#21152;&#24378;&#27861;&#24459;&#21512;&#35268;&#24615;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19050v1 Announce Type: cross  Abstract: The advent of generative AI models has revolutionized digital content creation, yet it introduces challenges in maintaining copyright integrity due to generative parroting, where models mimic their training data too closely. Our research presents a novel approach to tackle this issue by employing an overfitted Masked Autoencoder (MAE) to detect such parroted samples effectively. We establish a detection threshold based on the mean loss across the training dataset, allowing for the precise identification of parroted content in modified datasets. Preliminary evaluations demonstrate promising results, suggesting our method's potential to ensure ethical use and enhance the legal compliance of generative models.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LITA&#30340;&#35821;&#35328;&#25351;&#23548;&#30340;&#26102;&#38388;&#23450;&#20301;&#21161;&#25163;&#65292;&#36890;&#36807;&#24341;&#20837;&#26102;&#38388;&#20196;&#29260;&#12289;SlowFast&#20196;&#29260;&#21644;&#24378;&#35843;&#26102;&#38388;&#23450;&#20301;&#25968;&#25454;&#26469;&#25913;&#21892;&#35270;&#39057;&#20013;&#30340;&#26102;&#38388;&#23450;&#20301;&#33021;&#21147;</title><link>https://arxiv.org/abs/2403.19046</link><description>&lt;p&gt;
LITA: &#35821;&#35328;&#25351;&#23548;&#30340;&#26102;&#38388;&#23450;&#20301;&#21161;&#25163;
&lt;/p&gt;
&lt;p&gt;
LITA: Language Instructed Temporal-Localization Assistant
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19046
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LITA&#30340;&#35821;&#35328;&#25351;&#23548;&#30340;&#26102;&#38388;&#23450;&#20301;&#21161;&#25163;&#65292;&#36890;&#36807;&#24341;&#20837;&#26102;&#38388;&#20196;&#29260;&#12289;SlowFast&#20196;&#29260;&#21644;&#24378;&#35843;&#26102;&#38388;&#23450;&#20301;&#25968;&#25454;&#26469;&#25913;&#21892;&#35270;&#39057;&#20013;&#30340;&#26102;&#38388;&#23450;&#20301;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21462;&#24471;&#20102;&#24040;&#22823;&#36827;&#23637;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#23558;&#36825;&#20123;&#27169;&#22411;&#25193;&#23637;&#21040;&#35270;&#39057;&#36755;&#20837;&#65292;&#24182;&#20855;&#26377;&#26377;&#21069;&#36884;&#30340;&#25351;&#20196;&#36319;&#38543;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#19968;&#39033;&#37325;&#35201;&#32570;&#22833;&#30340;&#37096;&#20998;&#26159;&#26102;&#38388;&#23450;&#20301;&#12290;&#36825;&#20123;&#27169;&#22411;&#26080;&#27861;&#20934;&#30830;&#22238;&#31572;&#8220;&#20160;&#20040;&#26102;&#20505;&#65311;&#8221;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#19977;&#20010;&#38480;&#21046;&#23427;&#20204;&#26102;&#38388;&#23450;&#20301;&#33021;&#21147;&#30340;&#20851;&#38190;&#26041;&#38754;&#65306;&#65288;i&#65289;&#26102;&#38388;&#34920;&#31034;&#65292;&#65288;ii&#65289;&#26550;&#26500;&#21644;&#65288;iii&#65289;&#25968;&#25454;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#20855;&#26377;&#20197;&#19979;&#21151;&#33021;&#30340;&#35821;&#35328;&#25351;&#23548;&#30340;&#26102;&#38388;&#23450;&#20301;&#21161;&#25163;&#65288;LITA&#65289;&#26469;&#35299;&#20915;&#36825;&#20123;&#32570;&#28857;: (1) &#25105;&#20204;&#24341;&#20837;&#20102;&#26102;&#38388;&#20196;&#29260;&#65292;&#29992;&#20110;&#32534;&#30721;&#30456;&#23545;&#20110;&#35270;&#39057;&#38271;&#24230;&#30340;&#26102;&#38388;&#25139;&#65292;&#20197;&#26356;&#22909;&#22320;&#34920;&#31034;&#35270;&#39057;&#20013;&#30340;&#26102;&#38388;&#12290;(2) &#25105;&#20204;&#22312;&#26550;&#26500;&#20013;&#24341;&#20837;&#20102;SlowFast&#20196;&#29260;&#65292;&#20197;&#25429;&#33719;&#32454;&#31890;&#24230;&#30340;&#26102;&#38388;&#20449;&#24687;&#12290;(3) &#25105;&#20204;&#24378;&#35843;LITA&#30340;&#26102;&#38388;&#23450;&#20301;&#25968;&#25454;&#12290;&#38500;&#20102;&#21033;&#29992;&#20855;&#26377;&#26102;&#38388;&#25139;&#30340;&#29616;&#26377;&#35270;&#39057;&#25968;&#25454;&#38598;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#19968;&#20010;&#26032;&#30340;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19046v1 Announce Type: cross  Abstract: There has been tremendous progress in multimodal Large Language Models (LLMs). Recent works have extended these models to video input with promising instruction following capabilities. However, an important missing piece is temporal localization. These models cannot accurately answer the "When?" questions. We identify three key aspects that limit their temporal localization capabilities: (i) time representation, (ii) architecture, and (iii) data. We address these shortcomings by proposing Language Instructed Temporal-Localization Assistant (LITA) with the following features: (1) We introduce time tokens that encode timestamps relative to the video length to better represent time in videos. (2) We introduce SlowFast tokens in the architecture to capture temporal information at fine temporal resolution. (3) We emphasize temporal localization data for LITA. In addition to leveraging existing video datasets with timestamps, we propose a ne
&lt;/p&gt;</description></item><item><title>&#32508;&#21512;&#23454;&#39564;&#34920;&#26126;&#65292;&#21033;&#29992;LLMs&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#21487;&#20197;...</title><link>https://arxiv.org/abs/2403.19031</link><description>&lt;p&gt;
&#20351;&#29992;&#20844;&#20849;&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20581;&#24247;&#30456;&#20851;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;
Evaluating Large Language Models for Health-Related Text Classification Tasks with Public Social Media Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19031
&lt;/p&gt;
&lt;p&gt;
&#32508;&#21512;&#23454;&#39564;&#34920;&#26126;&#65292;&#21033;&#29992;LLMs&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#21487;&#20197;...
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;NLP&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#20960;&#20046;&#27809;&#26377;&#30740;&#31350;&#35797;&#22270;&#35780;&#20272;&#23427;&#20204;&#22312;&#22522;&#20110;&#31038;&#20132;&#23186;&#20307;&#30340;&#20581;&#24247;&#30456;&#20851;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#36825;&#20123;&#20219;&#21153;&#20256;&#32479;&#19978;&#24456;&#38590;&#33719;&#24471;&#39640;&#20998;&#12290;&#25105;&#20204;&#22312;6&#20010;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#19978;&#23545;&#19968;&#20010;&#22522;&#20110;&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;SVMs&#65289;&#30340;&#30417;&#30563;&#32463;&#20856;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#19977;&#20010;&#22522;&#20110;RoBERTa&#12289;BERTweet&#21644;SocBERT&#30340;&#30417;&#30563;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#65292;&#20197;&#21450;&#20004;&#20010;&#22522;&#20110;GPT3.5&#21644;GPT4&#30340;LLM&#20998;&#31867;&#22120;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19977;&#31181;&#21033;&#29992;LLMs&#36827;&#34892;&#25991;&#26412;&#20998;&#31867;&#30340;&#26041;&#27861;&#65306;&#23558;LLMs&#29992;&#20316;&#38646;&#27425;&#20998;&#31867;&#22120;&#65292;&#23558;LLMs&#29992;&#20316;&#27880;&#37322;&#22120;&#20026;&#30417;&#30563;&#20998;&#31867;&#22120;&#27880;&#37322;&#35757;&#32451;&#25968;&#25454;&#65292;&#20197;&#21450;&#21033;&#29992;LLMs&#36827;&#34892;&#23569;&#37327;&#31034;&#20363;&#26469;&#22686;&#21152;&#25163;&#21160;&#27880;&#37322;&#25968;&#25454;&#12290;&#25105;&#20204;&#20840;&#38754;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#21033;&#29992;LLMs&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#21487;&#20197;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19031v1 Announce Type: cross  Abstract: Large language models (LLMs) have demonstrated remarkable success in NLP tasks. However, there is a paucity of studies that attempt to evaluate their performances on social media-based health-related natural language processing tasks, which have traditionally been difficult to achieve high scores in. We benchmarked one supervised classic machine learning model based on Support Vector Machines (SVMs), three supervised pretrained language models (PLMs) based on RoBERTa, BERTweet, and SocBERT, and two LLM based classifiers (GPT3.5 and GPT4), across 6 text classification tasks. We developed three approaches for leveraging LLMs for text classification: employing LLMs as zero-shot classifiers, us-ing LLMs as annotators to annotate training data for supervised classifiers, and utilizing LLMs with few-shot examples for augmentation of manually annotated data. Our comprehensive experiments demonstrate that employ-ing data augmentation using LLM
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25193;&#23637;&#20102;&#24378;&#21270;&#23398;&#20064;&#21644;&#25511;&#21046;&#29702;&#35770;&#20013;&#23545;&#31216;&#25216;&#26415;&#30340;&#24212;&#29992;&#33539;&#22260;&#65292;&#36890;&#36807;&#21033;&#29992;&#21160;&#24577;&#23545;&#31216;&#24615;&#23398;&#20064;&#21160;&#21147;&#23398;&#27169;&#22411;&#65292;&#32780;&#19981;&#35201;&#27714;&#22870;&#21169;&#20855;&#26377;&#30456;&#21516;&#30340;&#23545;&#31216;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.19024</link><description>&lt;p&gt;
&#21033;&#29992;&#21160;&#24577;&#23545;&#31216;&#24615;&#36827;&#34892;&#22522;&#20110;&#27169;&#22411;&#30340;&#38750;&#23545;&#31216;&#22870;&#21169;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Exploiting Symmetry in Dynamics for Model-Based Reinforcement Learning with Asymmetric Rewards
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19024
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25193;&#23637;&#20102;&#24378;&#21270;&#23398;&#20064;&#21644;&#25511;&#21046;&#29702;&#35770;&#20013;&#23545;&#31216;&#25216;&#26415;&#30340;&#24212;&#29992;&#33539;&#22260;&#65292;&#36890;&#36807;&#21033;&#29992;&#21160;&#24577;&#23545;&#31216;&#24615;&#23398;&#20064;&#21160;&#21147;&#23398;&#27169;&#22411;&#65292;&#32780;&#19981;&#35201;&#27714;&#22870;&#21169;&#20855;&#26377;&#30456;&#21516;&#30340;&#23545;&#31216;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20013;&#26368;&#36817;&#30340;&#24037;&#20316;&#21033;&#29992;&#27169;&#22411;&#20013;&#30340;&#23545;&#31216;&#24615;&#26469;&#25552;&#39640;&#31574;&#30053;&#35757;&#32451;&#30340;&#37319;&#26679;&#25928;&#29575;&#12290;&#19968;&#20010;&#24120;&#29992;&#30340;&#31616;&#21270;&#20551;&#35774;&#26159;&#21160;&#21147;&#23398;&#21644;&#22870;&#21169;&#37117;&#34920;&#29616;&#20986;&#30456;&#21516;&#30340;&#23545;&#31216;&#24615;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#30495;&#23454;&#29615;&#22659;&#20013;&#65292;&#21160;&#21147;&#23398;&#27169;&#22411;&#34920;&#29616;&#20986;&#19982;&#22870;&#21169;&#27169;&#22411;&#29420;&#31435;&#30340;&#23545;&#31216;&#24615;&#65306;&#22870;&#21169;&#21487;&#33021;&#19981;&#28385;&#36275;&#19982;&#21160;&#21147;&#23398;&#30456;&#21516;&#30340;&#23545;&#31216;&#24615;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#21482;&#20551;&#23450;&#21160;&#21147;&#23398;&#34920;&#29616;&#20986;&#23545;&#31216;&#24615;&#30340;&#24773;&#20917;&#65292;&#25193;&#23637;&#20102;&#24378;&#21270;&#23398;&#20064;&#21644;&#25511;&#21046;&#29702;&#35770;&#23398;&#20064;&#20013;&#21487;&#24212;&#29992;&#23545;&#31216;&#25216;&#26415;&#30340;&#38382;&#39064;&#33539;&#22260;&#12290;&#25105;&#20204;&#21033;&#29992;&#21345;&#22612;&#24681;&#31227;&#21160;&#26694;&#26550;&#26041;&#27861;&#24341;&#20837;&#19968;&#31181;&#23398;&#20064;&#21160;&#21147;&#23398;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#26500;&#36896;&#65292;&#36825;&#31181;&#21160;&#21147;&#23398;&#34920;&#29616;&#20986;&#25351;&#23450;&#30340;&#23545;&#31216;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#23398;&#21040;&#20102;&#26356;&#20934;&#30830;&#30340;&#21160;&#24577;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19024v1 Announce Type: cross  Abstract: Recent work in reinforcement learning has leveraged symmetries in the model to improve sample efficiency in training a policy. A commonly used simplifying assumption is that the dynamics and reward both exhibit the same symmetry. However, in many real-world environments, the dynamical model exhibits symmetry independent of the reward model: the reward may not satisfy the same symmetries as the dynamics. In this paper, we investigate scenarios where only the dynamics are assumed to exhibit symmetry, extending the scope of problems in reinforcement learning and learning in control theory where symmetry techniques can be applied. We use Cartan's moving frame method to introduce a technique for learning dynamics which, by construction, exhibit specified symmetries. We demonstrate through numerical experiments that the proposed method learns a more accurate dynamical model.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25552;&#20986;IDGen&#65292;&#23558;&#27599;&#20010;&#25512;&#33616;&#39033;&#30446;&#34920;&#31034;&#20026;&#29420;&#29305;&#12289;&#31616;&#27905;&#12289;&#35821;&#20041;&#20016;&#23500;&#30340;&#25991;&#26412;ID&#65292;&#20174;&#32780;&#20351;&#24471;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#33616;&#26356;&#22909;&#22320;&#19982;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#23545;&#40784;&#12290;</title><link>https://arxiv.org/abs/2403.19021</link><description>&lt;p&gt;
&#26397;&#21521;LLM-RecSys&#23545;&#40784;&#19982;&#25991;&#26412;ID&#23398;&#20064;&#30340;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;
Towards LLM-RecSys Alignment with Textual ID Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19021
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25552;&#20986;IDGen&#65292;&#23558;&#27599;&#20010;&#25512;&#33616;&#39033;&#30446;&#34920;&#31034;&#20026;&#29420;&#29305;&#12289;&#31616;&#27905;&#12289;&#35821;&#20041;&#20016;&#23500;&#30340;&#25991;&#26412;ID&#65292;&#20174;&#32780;&#20351;&#24471;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#33616;&#26356;&#22909;&#22320;&#19982;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#29983;&#25104;&#24335;&#25512;&#33616;&#24050;&#32463;&#23558;&#20256;&#32479;&#30340;&#22522;&#20110;&#25490;&#21517;&#30340;&#25512;&#33616;&#26041;&#24335;&#36716;&#21464;&#20026;&#25991;&#26412;&#29983;&#25104;&#33539;&#20363;&#12290;&#28982;&#32780;&#65292;&#19982;&#22266;&#26377;&#25805;&#20316;&#20154;&#31867;&#35789;&#27719;&#30340;&#26631;&#20934;NLP&#20219;&#21153;&#30456;&#21453;&#65292;&#30446;&#21069;&#29983;&#25104;&#24335;&#25512;&#33616;&#39046;&#22495;&#30340;&#30740;&#31350;&#22312;&#22914;&#20309;&#22312;&#25991;&#26412;&#29983;&#25104;&#33539;&#24335;&#20013;&#20197;&#31616;&#27905;&#32780;&#26377;&#24847;&#20041;&#30340;ID&#34920;&#31034;&#26377;&#25928;&#32534;&#30721;&#25512;&#33616;&#39033;&#30446;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#23545;&#40784;LLMs&#19982;&#25512;&#33616;&#38656;&#27714;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;IDGen&#65292;&#20351;&#29992;&#20154;&#31867;&#35821;&#35328;&#26631;&#35760;&#23558;&#27599;&#20010;&#39033;&#30446;&#34920;&#31034;&#20026;&#29420;&#29305;&#12289;&#31616;&#27905;&#12289;&#35821;&#20041;&#20016;&#23500;&#12289;&#19982;&#24179;&#21488;&#26080;&#20851;&#30340;&#25991;&#26412;ID&#12290;&#36825;&#36890;&#36807;&#22312;&#22522;&#20110;LLM&#30340;&#25512;&#33616;&#31995;&#32479;&#26049;&#35757;&#32451;&#25991;&#26412;ID&#29983;&#25104;&#22120;&#26469;&#23454;&#29616;&#65292;&#20351;&#20010;&#24615;&#21270;&#25512;&#33616;&#33021;&#22815;&#26080;&#32541;&#38598;&#25104;&#21040;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#20013;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#30001;&#20110;&#29992;&#25143;&#21382;&#21490;&#35760;&#24405;&#20197;&#33258;&#28982;&#35821;&#35328;&#34920;&#36798;&#24182;&#19982;&#21407;&#22987;&#25968;&#25454;&#38598;&#35299;&#32806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#20986;&#20102;&#28508;&#22312;&#30340;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19021v1 Announce Type: cross  Abstract: Generative recommendation based on Large Language Models (LLMs) have transformed the traditional ranking-based recommendation style into a text-to-text generation paradigm. However, in contrast to standard NLP tasks that inherently operate on human vocabulary, current research in generative recommendations struggles to effectively encode recommendation items within the text-to-text framework using concise yet meaningful ID representations. To better align LLMs with recommendation needs, we propose IDGen, representing each item as a unique, concise, semantically rich, platform-agnostic textual ID using human language tokens. This is achieved by training a textual ID generator alongside the LLM-based recommender, enabling seamless integration of personalized recommendations into natural language generation. Notably, as user history is expressed in natural language and decoupled from the original dataset, our approach suggests the potenti
&lt;/p&gt;</description></item><item><title>ReflectSumm&#26159;&#19968;&#20010;&#26088;&#22312;&#24635;&#32467;&#23398;&#29983;&#21453;&#24605;&#24615;&#20889;&#20316;&#30340;&#25968;&#25454;&#38598;&#65292;&#21487;&#20197;&#24110;&#21161;&#24320;&#21457;&#21644;&#35780;&#20272;&#38024;&#23545;&#29616;&#23454;&#22330;&#26223;&#30340;&#26032;&#22411;&#25688;&#35201;&#25216;&#26415;&#65292;&#20026;&#36827;&#19968;&#27493;&#30740;&#31350;&#25552;&#20379;&#20102;&#22522;&#20934;&#12290;</title><link>https://arxiv.org/abs/2403.19012</link><description>&lt;p&gt;
ReflectSumm: &#19968;&#20010;&#29992;&#20110;&#35838;&#31243;&#21453;&#24605;&#25688;&#35201;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
ReflectSumm: A Benchmark for Course Reflection Summarization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19012
&lt;/p&gt;
&lt;p&gt;
ReflectSumm&#26159;&#19968;&#20010;&#26088;&#22312;&#24635;&#32467;&#23398;&#29983;&#21453;&#24605;&#24615;&#20889;&#20316;&#30340;&#25968;&#25454;&#38598;&#65292;&#21487;&#20197;&#24110;&#21161;&#24320;&#21457;&#21644;&#35780;&#20272;&#38024;&#23545;&#29616;&#23454;&#22330;&#26223;&#30340;&#26032;&#22411;&#25688;&#35201;&#25216;&#26415;&#65292;&#20026;&#36827;&#19968;&#27493;&#30740;&#31350;&#25552;&#20379;&#20102;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;ReflectSumm&#65292;&#19968;&#20010;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#24635;&#32467;&#23398;&#29983;&#21453;&#24605;&#24615;&#20889;&#20316;&#30340;&#26032;&#22411;&#25688;&#35201;&#25968;&#25454;&#38598;&#12290;ReflectSumm&#30340;&#30446;&#26631;&#26159;&#20419;&#36827;&#24320;&#21457;&#21644;&#35780;&#20272;&#38024;&#23545;&#29616;&#23454;&#22330;&#26223;&#30340;&#26032;&#22411;&#25688;&#35201;&#25216;&#26415;&#65292;&#36825;&#20123;&#22330;&#26223;&#20855;&#26377;&#23569;&#37327;&#35757;&#32451;&#25968;&#25454;&#65292;%&#20855;&#26377;&#28508;&#22312;&#22312;&#24847;&#35265;&#24635;&#32467;&#39046;&#22495;&#21644;&#29305;&#21035;&#26159;&#25945;&#32946;&#39046;&#22495;&#20013;&#30340;&#24433;&#21709;&#12290;&#35813;&#25968;&#25454;&#38598;&#28085;&#30422;&#20102;&#21508;&#31181;&#25688;&#35201;&#20219;&#21153;&#65292;&#24182;&#21253;&#25324;&#20840;&#38754;&#30340;&#20803;&#25968;&#25454;&#65292;&#21487;&#20197;&#25506;&#32034;&#21508;&#31181;&#30740;&#31350;&#38382;&#39064;&#24182;&#25903;&#25345;&#19981;&#21516;&#30340;&#24212;&#29992;&#12290;&#20026;&#23637;&#31034;&#20854;&#25928;&#29992;&#65292;&#25105;&#20204;&#20351;&#29992;&#22810;&#20010;&#26368;&#20808;&#36827;&#30340;&#22522;&#20934;&#36827;&#34892;&#20102;&#24191;&#27867;&#35780;&#20272;&#12290;&#32467;&#26524;&#20026;&#20419;&#36827;&#36825;&#19968;&#39046;&#22495;&#30340;&#36827;&#19968;&#27493;&#30740;&#31350;&#25552;&#20379;&#20102;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19012v1 Announce Type: cross  Abstract: This paper introduces ReflectSumm, a novel summarization dataset specifically designed for summarizing students' reflective writing. The goal of ReflectSumm is to facilitate developing and evaluating novel summarization techniques tailored to real-world scenarios with little training data, %practical tasks with potential implications in the opinion summarization domain in general and the educational domain in particular. The dataset encompasses a diverse range of summarization tasks and includes comprehensive metadata, enabling the exploration of various research questions and supporting different applications. To showcase its utility, we conducted extensive evaluations using multiple state-of-the-art baselines. The results provide benchmarks for facilitating further research in this area.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#26032;&#39062;&#30340;&#26694;&#26550;SFFormer&#65292;&#32467;&#21512;&#20102;&#22810;&#22836;&#20132;&#21449;&#27880;&#24847;&#21147;&#29305;&#24449;&#34701;&#21512;&#27169;&#22359;&#65292;&#22522;&#20110;dMRI&#32420;&#32500;&#26463;&#36861;&#36394;&#65292;&#39044;&#27979;&#20102;&#20027;&#35266;&#35821;&#35328;&#34920;&#29616;&#65292;&#25299;&#23637;&#20102;&#33041;&#32467;&#26500;&#19982;&#20154;&#31867;&#35748;&#30693;&#21151;&#33021;&#30340;&#20851;&#32852;&#30740;&#31350;&#12290;</title><link>https://arxiv.org/abs/2403.19001</link><description>&lt;p&gt;
&#36328;&#39046;&#22495;&#30340;&#32420;&#32500;&#31751;&#24418;&#29366;&#20998;&#26512;&#29992;&#20110;&#35821;&#35328;&#34920;&#29616;&#35748;&#30693;&#20998;&#25968;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Cross--domain Fiber Cluster Shape Analysis for Language Performance Cognitive Score Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19001
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#26032;&#39062;&#30340;&#26694;&#26550;SFFormer&#65292;&#32467;&#21512;&#20102;&#22810;&#22836;&#20132;&#21449;&#27880;&#24847;&#21147;&#29305;&#24449;&#34701;&#21512;&#27169;&#22359;&#65292;&#22522;&#20110;dMRI&#32420;&#32500;&#26463;&#36861;&#36394;&#65292;&#39044;&#27979;&#20102;&#20027;&#35266;&#35821;&#35328;&#34920;&#29616;&#65292;&#25299;&#23637;&#20102;&#33041;&#32467;&#26500;&#19982;&#20154;&#31867;&#35748;&#30693;&#21151;&#33021;&#30340;&#20851;&#32852;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24418;&#29366;&#22312;&#35745;&#31639;&#26426;&#22270;&#24418;&#23398;&#20013;&#25198;&#28436;&#37325;&#35201;&#35282;&#33394;&#65292;&#25552;&#20379;&#20102;&#26377;&#20851;&#23545;&#35937;&#24418;&#24577;&#21644;&#21151;&#33021;&#30340;&#20449;&#24687;&#29305;&#24449;&#12290;&#33041;&#25104;&#20687;&#20013;&#30340;&#24418;&#29366;&#20998;&#26512;&#21487;&#24110;&#21161;&#35299;&#37322;&#20154;&#33041;&#32467;&#26500;&#21644;&#21151;&#33021;&#30340;&#30456;&#20851;&#24615;&#12290;&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#22823;&#33041;&#30340;3D&#30333;&#36136;&#36830;&#25509;&#30340;&#24418;&#29366;&#21450;&#20854;&#19982;&#20154;&#31867;&#35748;&#30693;&#21151;&#33021;&#30340;&#28508;&#22312;&#39044;&#27979;&#20851;&#31995;&#12290;&#25105;&#20204;&#20351;&#29992;&#25193;&#25955;&#30913;&#20849;&#25391;&#25104;&#20687;&#65288;dMRI&#65289;&#32420;&#32500;&#26463;&#36861;&#36394;&#23558;&#22823;&#33041;&#36830;&#25509;&#37325;&#24314;&#20026;3D&#28857;&#24207;&#21015;&#12290;&#20026;&#20102;&#25551;&#36848;&#27599;&#20010;&#36830;&#25509;&#65292;&#25105;&#20204;&#25552;&#21462;&#20102;12&#20010;&#24418;&#29366;&#25551;&#36848;&#31526;&#20197;&#21450;&#20256;&#32479;&#30340;dMRI&#36830;&#25509;&#21644;&#32452;&#32455;&#24494;&#32467;&#26500;&#29305;&#24449;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#24418;&#29366;&#34701;&#21512;&#32420;&#32500;&#31751;&#21464;&#25442;&#22120;&#65288;SFFormer&#65289;&#65292;&#21033;&#29992;&#22810;&#22836;&#20132;&#21449;&#27880;&#24847;&#21147;&#29305;&#24449;&#34701;&#21512;&#27169;&#22359;&#22522;&#20110;dMRI&#32420;&#32500;&#26463;&#36861;&#36394;&#26469;&#39044;&#27979;&#29305;&#23450;&#20010;&#20307;&#30340;&#35821;&#35328;&#34920;&#29616;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#22823;&#22411;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#35813;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19001v1 Announce Type: cross  Abstract: Shape plays an important role in computer graphics, offering informative features to convey an object's morphology and functionality. Shape analysis in brain imaging can help interpret structural and functionality correlations of the human brain. In this work, we investigate the shape of the brain's 3D white matter connections and its potential predictive relationship to human cognitive function. We reconstruct brain connections as sequences of 3D points using diffusion magnetic resonance imaging (dMRI) tractography. To describe each connection, we extract 12 shape descriptors in addition to traditional dMRI connectivity and tissue microstructure features. We introduce a novel framework, Shape--fused Fiber Cluster Transformer (SFFormer), that leverages a multi-head cross-attention feature fusion module to predict subject-specific language performance based on dMRI tractography. We assess the performance of the method on a large dataset
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#38024;&#23545;&#24494;&#26381;&#21153;&#31995;&#32479;&#30340;&#23569;&#26679;&#26412;&#24322;&#24120;&#36319;&#36394;&#20998;&#31867;&#30340;&#26032;&#26694;&#26550;&#65292;&#21033;&#29992;&#22810;&#22836;&#27880;&#24847;&#21147;&#33258;&#32534;&#30721;&#22120;&#26500;&#24314;&#31995;&#32479;&#29305;&#23450;&#30340;&#36319;&#36394;&#34920;&#31034;&#65292;&#24182;&#24212;&#29992;&#22522;&#20110;Transformer&#32534;&#30721;&#22120;&#30340;&#27169;&#22411;&#26080;&#20851;&#20803;&#23398;&#20064;&#36827;&#34892;&#39640;&#25928;&#20998;&#31867;&#12290;</title><link>https://arxiv.org/abs/2403.18998</link><description>&lt;p&gt;
&#24494;&#26381;&#21153;&#31995;&#32479;&#30340;&#23569;&#26679;&#26412;&#36328;&#31995;&#32479;&#24322;&#24120;&#36319;&#36394;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Few-Shot Cross-System Anomaly Trace Classification for Microservice-based systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18998
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#38024;&#23545;&#24494;&#26381;&#21153;&#31995;&#32479;&#30340;&#23569;&#26679;&#26412;&#24322;&#24120;&#36319;&#36394;&#20998;&#31867;&#30340;&#26032;&#26694;&#26550;&#65292;&#21033;&#29992;&#22810;&#22836;&#27880;&#24847;&#21147;&#33258;&#32534;&#30721;&#22120;&#26500;&#24314;&#31995;&#32479;&#29305;&#23450;&#30340;&#36319;&#36394;&#34920;&#31034;&#65292;&#24182;&#24212;&#29992;&#22522;&#20110;Transformer&#32534;&#30721;&#22120;&#30340;&#27169;&#22411;&#26080;&#20851;&#20803;&#23398;&#20064;&#36827;&#34892;&#39640;&#25928;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24494;&#26381;&#21153;&#31995;&#32479;&#65288;MSS&#65289;&#30001;&#20110;&#20854;&#22797;&#26434;&#21644;&#21160;&#24577;&#30340;&#29305;&#24615;&#21487;&#33021;&#22312;&#21508;&#31181;&#25925;&#38556;&#31867;&#21035;&#20013;&#20986;&#29616;&#25925;&#38556;&#12290;&#20026;&#20102;&#26377;&#25928;&#22788;&#29702;&#25925;&#38556;&#65292;AIOps&#24037;&#20855;&#21033;&#29992;&#22522;&#20110;&#36319;&#36394;&#30340;&#24322;&#24120;&#26816;&#27979;&#21644;&#26681;&#26412;&#21407;&#22240;&#20998;&#26512;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#24494;&#26381;&#21153;&#31995;&#32479;&#30340;&#23569;&#26679;&#26412;&#24322;&#24120;&#36319;&#36394;&#20998;&#31867;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21253;&#25324;&#20004;&#20010;&#20027;&#35201;&#32452;&#25104;&#37096;&#20998;&#65306;&#65288;1&#65289;&#22810;&#22836;&#27880;&#24847;&#21147;&#33258;&#32534;&#30721;&#22120;&#29992;&#20110;&#26500;&#24314;&#31995;&#32479;&#29305;&#23450;&#30340;&#36319;&#36394;&#34920;&#31034;&#65292;&#20174;&#32780;&#23454;&#29616;&#65288;2&#65289;&#22522;&#20110;Transformer&#32534;&#30721;&#22120;&#30340;&#27169;&#22411;&#26080;&#20851;&#20803;&#23398;&#20064;&#65292;&#20197;&#36827;&#34892;&#26377;&#25928;&#21644;&#39640;&#25928;&#30340;&#23569;&#26679;&#26412;&#24322;&#24120;&#36319;&#36394;&#20998;&#31867;&#12290;&#35813;&#26694;&#26550;&#22312;&#20004;&#20010;&#20195;&#34920;&#24615;&#30340;MSS&#65292;Trainticket&#21644;OnlineBoutique&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#20351;&#29992;&#24320;&#25918;&#25968;&#25454;&#38598;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#33021;&#22815;&#35843;&#25972;&#23398;&#21040;&#30340;&#30693;&#35782;&#65292;&#20197;&#23545;&#26032;&#30340;&#12289;&#26410;&#35265;&#30340;&#26032;&#39062;&#25925;&#38556;&#31867;&#21035;&#30340;&#24322;&#24120;&#36319;&#36394;&#36827;&#34892;&#20998;&#31867;&#65292;&#26080;&#35770;&#26159;&#22312;&#26368;&#21021;&#35757;&#32451;&#30340;&#21516;&#19968;&#31995;&#32479;&#20869;&#65292;&#36824;&#26159;&#22312;&#20854;&#20182;&#31995;&#32479;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18998v1 Announce Type: cross  Abstract: Microservice-based systems (MSS) may experience failures in various fault categories due to their complex and dynamic nature. To effectively handle failures, AIOps tools utilize trace-based anomaly detection and root cause analysis. In this paper, we propose a novel framework for few-shot abnormal trace classification for MSS. Our framework comprises two main components: (1) Multi-Head Attention Autoencoder for constructing system-specific trace representations, which enables (2) Transformer Encoder-based Model-Agnostic Meta-Learning to perform effective and efficient few-shot learning for abnormal trace classification. The proposed framework is evaluated on two representative MSS, Trainticket and OnlineBoutique, with open datasets. The results show that our framework can adapt the learned knowledge to classify new, unseen abnormal traces of novel fault categories both within the same system it was initially trained on and even in the 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21512;&#25104;&#23569;&#25968;&#31867;&#36807;&#25277;&#26679;&#25216;&#26415;(SMOTE)&#30340;&#20108;&#20803;&#20998;&#31867;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;Bot-IoT&#25968;&#25454;&#38598;&#20013;&#30340;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.18989</link><description>&lt;p&gt;
&#22788;&#29702;Bot-IoT&#25968;&#25454;&#38598;&#20013;&#30340;&#19981;&#24179;&#34913;&#31867;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Dealing with Imbalanced Classes in Bot-IoT Dataset
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18989
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21512;&#25104;&#23569;&#25968;&#31867;&#36807;&#25277;&#26679;&#25216;&#26415;(SMOTE)&#30340;&#20108;&#20803;&#20998;&#31867;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;Bot-IoT&#25968;&#25454;&#38598;&#20013;&#30340;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#29289;&#32852;&#32593;(IoT)&#35774;&#22791;&#30340;&#24191;&#27867;&#20351;&#29992;&#65292;&#32593;&#32476;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;(NIDS)&#22312;&#26816;&#27979;&#21644;&#20445;&#25252;&#29289;&#32852;&#32593;&#32593;&#32476;&#20013;&#21508;&#31181;&#31867;&#22411;&#30340;&#25915;&#20987;&#20013;&#21457;&#25381;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#20026;&#20102;&#35780;&#20272;NIDS&#22312;&#29289;&#32852;&#32593;&#32593;&#32476;&#20013;&#30340;&#40065;&#26834;&#24615;&#65292;&#29616;&#26377;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#20010;&#29616;&#23454;&#30340;&#29289;&#32852;&#32593;&#32593;&#32476;&#20013;&#30340;&#20725;&#23608;&#32593;&#32476;&#25968;&#25454;&#38598;(Bot-IoT&#25968;&#25454;&#38598;)&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#24322;&#24120;&#26816;&#27979;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#19981;&#24179;&#34913;&#30340;&#27491;&#24120;&#25968;&#25454;&#21253;&#21644;&#25915;&#20987;&#25968;&#25454;&#21253;&#65292;&#22240;&#20026;&#27491;&#24120;&#25968;&#25454;&#21253;&#30340;&#25968;&#37327;&#36828;&#23567;&#20110;&#25915;&#20987;&#25968;&#25454;&#21253;&#30340;&#25968;&#37327;&#12290;&#19981;&#24179;&#34913;&#25968;&#25454;&#30340;&#24615;&#36136;&#21487;&#33021;&#20250;&#20351;&#36776;&#35782;&#23569;&#25968;&#31867;&#21035;&#21464;&#24471;&#22256;&#38590;&#12290;&#22312;&#26412;&#35770;&#25991;&#20013;&#65292;&#20026;&#20102;&#35299;&#20915;Bot-IoT&#25968;&#25454;&#38598;&#20013;&#30340;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21512;&#25104;&#23569;&#25968;&#31867;&#36807;&#25277;&#26679;&#25216;&#26415;(SMOTE)&#30340;&#20108;&#20803;&#20998;&#31867;&#26041;&#27861;&#12290;&#25152;&#25552;&#20986;&#30340;&#20998;&#31867;&#22120;&#26088;&#22312;&#26816;&#27979;&#25915;&#20987;&#25968;&#25454;&#21253;&#24182;&#20351;&#29992;SMOTE&#31639;&#27861;&#20811;&#26381;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18989v1 Announce Type: cross  Abstract: With the rapidly spreading usage of Internet of Things (IoT) devices, a network intrusion detection system (NIDS) plays an important role in detecting and protecting various types of attacks in the IoT network. To evaluate the robustness of the NIDS in the IoT network, the existing work proposed a realistic botnet dataset in the IoT network (Bot-IoT dataset) and applied it to machine learning-based anomaly detection. This dataset contains imbalanced normal and attack packets because the number of normal packets is much smaller than that of attack ones. The nature of imbalanced data may make it difficult to identify the minority class correctly. In this thesis, to address the class imbalance problem in the Bot-IoT dataset, we propose a binary classification method with synthetic minority over-sampling techniques (SMOTE). The proposed classifier aims to detect attack packets and overcome the class imbalance problem using the SMOTE algori
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#20174;&#24515;&#30005;&#22270;&#20449;&#21495;&#20998;&#26512;&#21040;&#22270;&#20687;&#21644;&#35270;&#39057;&#20998;&#31867;&#30340;&#19981;&#21516;&#27169;&#22411;&#31867;&#22411;&#30340;&#23545;&#25239;&#25915;&#20987;&#65292;&#36890;&#36807;&#35782;&#21035;&#25935;&#24863;&#21306;&#22495;&#24182;&#22312;&#26368;&#23567;&#31243;&#24230;&#25197;&#26354;&#19979;&#35825;&#23548;&#38169;&#35823;&#20998;&#31867;&#65292;&#29983;&#25104;&#20248;&#36234;&#30340;&#23450;&#20301;&#25513;&#27169;&#65292;&#24182;&#22312;&#40065;&#26834;&#24615;&#21644;&#36879;&#26126;&#24230;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;</title><link>https://arxiv.org/abs/2403.18985</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#29992;&#20110;&#40657;&#30418;&#22270;&#20687;&#12289;&#35270;&#39057;&#21644;&#24515;&#30005;&#22270;&#20449;&#21495;&#20998;&#31867;&#30340;&#40065;&#26834;&#24615;&#21644;&#35270;&#35273;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Robustness and Visual Explanation for Black Box Image, Video, and ECG Signal Classification with Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18985
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#20174;&#24515;&#30005;&#22270;&#20449;&#21495;&#20998;&#26512;&#21040;&#22270;&#20687;&#21644;&#35270;&#39057;&#20998;&#31867;&#30340;&#19981;&#21516;&#27169;&#22411;&#31867;&#22411;&#30340;&#23545;&#25239;&#25915;&#20987;&#65292;&#36890;&#36807;&#35782;&#21035;&#25935;&#24863;&#21306;&#22495;&#24182;&#22312;&#26368;&#23567;&#31243;&#24230;&#25197;&#26354;&#19979;&#35825;&#23548;&#38169;&#35823;&#20998;&#31867;&#65292;&#29983;&#25104;&#20248;&#36234;&#30340;&#23450;&#20301;&#25513;&#27169;&#65292;&#24182;&#22312;&#40065;&#26834;&#24615;&#21644;&#36879;&#26126;&#24230;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26694;&#26550;&#65292;&#38024;&#23545;&#20174;&#24515;&#30005;&#22270;&#20449;&#21495;&#20998;&#26512;&#65288;1D&#65289;&#12289;&#22270;&#20687;&#20998;&#31867;&#65288;2D&#65289;&#21040;&#35270;&#39057;&#20998;&#31867;&#65288;3D&#65289;&#31561;&#19981;&#21516;&#27169;&#22411;&#31867;&#22411;&#36827;&#34892;&#23545;&#25239;&#25915;&#20987;&#30340;&#20248;&#21270;&#12290;&#35813;&#26694;&#26550;&#19987;&#27880;&#20110;&#35782;&#21035;&#25935;&#24863;&#21306;&#22495;&#24182;&#22312;&#26368;&#23567;&#31243;&#24230;&#25197;&#26354;&#21644;&#21508;&#31181;&#25197;&#26354;&#31867;&#22411;&#19979;&#35825;&#23548;&#38169;&#35823;&#20998;&#31867;&#12290;&#26032;&#39062;&#30340;RL&#26041;&#27861;&#22312;&#25152;&#26377;&#19977;&#20010;&#24212;&#29992;&#20013;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#35777;&#26126;&#20102;&#20854;&#25928;&#29575;&#12290;&#25105;&#20204;&#30340;RL&#26041;&#27861;&#29983;&#25104;&#20102;&#20248;&#36234;&#30340;&#23450;&#20301;&#25513;&#27169;&#65292;&#22686;&#24378;&#20102;&#22270;&#20687;&#20998;&#31867;&#21644;&#24515;&#30005;&#22270;&#20998;&#26512;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#23545;&#20110;&#24515;&#30005;&#22270;&#20998;&#26512;&#31561;&#24212;&#29992;&#65292;&#25105;&#20204;&#30340;&#24179;&#21488;&#31361;&#20986;&#20102;&#20020;&#24202;&#21307;&#29983;&#20851;&#27880;&#30340;&#20851;&#38190;&#24515;&#30005;&#22270;&#29255;&#27573;&#65292;&#21516;&#26102;&#30830;&#20445;&#23545;&#27969;&#34892;&#25197;&#26354;&#30340;&#38887;&#24615;&#12290;&#36825;&#19968;&#20840;&#38754;&#30340;&#24037;&#20855;&#26088;&#22312;&#36890;&#36807;&#23545;&#25239;&#24615;&#35757;&#32451;&#21644;&#36879;&#26126;&#24230;&#25552;&#39640;&#21508;&#31181;&#24212;&#29992;&#21644;&#25968;&#25454;&#31867;&#22411;&#30340;&#38887;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18985v1 Announce Type: cross  Abstract: We present a generic Reinforcement Learning (RL) framework optimized for crafting adversarial attacks on different model types spanning from ECG signal analysis (1D), image classification (2D), and video classification (3D). The framework focuses on identifying sensitive regions and inducing misclassifications with minimal distortions and various distortion types. The novel RL method outperforms state-of-the-art methods for all three applications, proving its efficiency. Our RL approach produces superior localization masks, enhancing interpretability for image classification and ECG analysis models. For applications such as ECG analysis, our platform highlights critical ECG segments for clinicians while ensuring resilience against prevalent distortions. This comprehensive tool aims to bolster both resilience with adversarial training and transparency across varied applications and data types.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24494;&#35843;&#25991;&#26412;&#32534;&#30721;&#22120;&#26469;&#25913;&#36827;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.18978</link><description>&lt;p&gt;
TextCraftor&#65306;&#24744;&#30340;&#25991;&#26412;&#32534;&#30721;&#22120;&#21487;&#20197;&#25104;&#20026;&#22270;&#20687;&#36136;&#37327;&#25511;&#21046;&#22120;
&lt;/p&gt;
&lt;p&gt;
TextCraftor: Your Text Encoder Can be Image Quality Controller
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18978
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24494;&#35843;&#25991;&#26412;&#32534;&#30721;&#22120;&#26469;&#25913;&#36827;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25193;&#25955;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#65292;&#22914;&#31283;&#23450;&#25193;&#25955;&#65292;&#24050;&#32463;&#24443;&#24213;&#25913;&#21464;&#20102;&#20869;&#23481;&#29983;&#25104;&#39046;&#22495;&#65292;&#20351;&#24471;&#22312;&#35832;&#22914;&#22270;&#20687;&#32534;&#36753;&#21644;&#35270;&#39057;&#21512;&#25104;&#31561;&#39046;&#22495;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#23613;&#31649;&#36825;&#20123;&#27169;&#22411;&#20855;&#26377;&#24378;&#22823;&#30340;&#21151;&#33021;&#65292;&#20294;&#23427;&#20204;&#24182;&#38750;&#27809;&#26377;&#23616;&#38480;&#24615;&#12290;&#21512;&#25104;&#19982;&#36755;&#20837;&#25991;&#26412;&#30456;&#22865;&#21512;&#30340;&#22270;&#20687;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#24182;&#19988;&#38656;&#35201;&#22810;&#27425;&#36816;&#34892;&#20197;&#31934;&#24515;&#35774;&#35745;&#30340;&#25552;&#31034;&#25165;&#33021;&#23454;&#29616;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#12290;&#20026;&#20102;&#20943;&#36731;&#36825;&#20123;&#23616;&#38480;&#24615;&#65292;&#35768;&#22810;&#30740;&#31350;&#21162;&#21147;&#23545;&#39044;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#21363;UNet&#65292;&#36827;&#34892;&#24494;&#35843;&#65292;&#21033;&#29992;&#21508;&#31181;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#22312;&#36825;&#20123;&#21162;&#21147;&#20013;&#65292;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#65292;&#21363;&#23545;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#35757;&#32451;&#36827;&#34892;&#24494;&#35843;&#20197;&#25913;&#21892;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#24615;&#33021;&#26159;&#21542;&#21487;&#33021;&#21644;&#21487;&#34892;&#65292;&#20173;&#28982;&#22823;&#22810;&#26410;&#34987;&#25506;&#35752;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#19982;&#20854;&#26367;&#25442;CLIP&#25991;&#26412;&#32534;&#30721;&#22120;&#65292;&#26356;&#22909;&#30340;&#26041;&#27861;&#26159;&#24494;&#35843;&#25991;&#26412;&#32534;&#30721;&#22120;&#20197;&#25552;&#21319;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18978v1 Announce Type: cross  Abstract: Diffusion-based text-to-image generative models, e.g., Stable Diffusion, have revolutionized the field of content generation, enabling significant advancements in areas like image editing and video synthesis. Despite their formidable capabilities, these models are not without their limitations. It is still challenging to synthesize an image that aligns well with the input text, and multiple runs with carefully crafted prompts are required to achieve satisfactory results. To mitigate these limitations, numerous studies have endeavored to fine-tune the pre-trained diffusion models, i.e., UNet, utilizing various technologies. Yet, amidst these efforts, a pivotal question of text-to-image diffusion model training has remained largely unexplored: Is it possible and feasible to fine-tune the text encoder to improve the performance of text-to-image diffusion models? Our findings reveal that, instead of replacing the CLIP text encoder used in 
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#25552;&#31034;&#31574;&#30053;&#8220;Sorry, Come Again (SCA)&#8221;&#26469;&#36991;&#20813;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20135;&#29983;&#24187;&#35273;&#65292;&#36890;&#36807;&#36827;&#34892;&#26368;&#20339;&#30340;&#25913;&#20889;&#21644;&#27880;&#20837;[PAUSE]&#26631;&#35760;&#26469;&#22686;&#24378;&#29702;&#35299;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.18976</link><description>&lt;p&gt;
"&#23545;&#19981;&#36215;&#65292;&#20877;&#27425;&#26469;&#21527;&#65311;&#25552;&#31034;&#8212;&#8212;&#36890;&#36807;&#27880;&#20837;[PAUSE]&#20248;&#21270;&#25913;&#20889;&#26469;&#22686;&#24378;&#29702;&#35299;&#21147;&#21644;&#20943;&#23569;&#24187;&#35273;"
&lt;/p&gt;
&lt;p&gt;
"Sorry, Come Again?" Prompting -- Enhancing Comprehension and Diminishing Hallucination with [PAUSE]-injected Optimal Paraphrasing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18976
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#25552;&#31034;&#31574;&#30053;&#8220;Sorry, Come Again (SCA)&#8221;&#26469;&#36991;&#20813;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20135;&#29983;&#24187;&#35273;&#65292;&#36890;&#36807;&#36827;&#34892;&#26368;&#20339;&#30340;&#25913;&#20889;&#21644;&#27880;&#20837;[PAUSE]&#26631;&#35760;&#26469;&#22686;&#24378;&#29702;&#35299;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Hallucination&#24050;&#32463;&#25104;&#20026;&#24403;&#20195;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#26368;&#33030;&#24369;&#30340;&#26041;&#38754;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;Sorry, Come Again (SCA)&#25552;&#31034;&#65292;&#26088;&#22312;&#36890;&#36807;&#65306;(i) &#26368;&#20339;&#30340;&#25913;&#20889;&#21644;(ii) &#27880;&#20837;[PAUSE]&#26631;&#35760;&#26469;&#24310;&#36831;LLMs&#30340;&#29983;&#25104;&#65292;&#20197;&#36991;&#20813;LLM&#20135;&#29983;&#24187;&#35273;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23545;21&#20010;LLMs&#30340;&#25552;&#31034;&#30340;&#35821;&#35328;&#32454;&#24494;&#24046;&#21035;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#65306;&#27491;&#24335;&#24615;&#12289;&#21487;&#35835;&#24615;&#21644;&#20855;&#20307;&#24615;&#65292;&#24182;&#38416;&#26126;&#20102;&#36825;&#20123;&#24046;&#21035;&#26159;&#22914;&#20309;&#23548;&#33268;&#20135;&#29983;&#24187;&#35273;&#30340;&#12290;&#25552;&#31034;&#30340;&#21487;&#35835;&#24615;&#12289;&#27491;&#24335;&#24615;&#25110;&#20855;&#20307;&#24615;&#36739;&#20302;&#20250;&#32473;LLMs&#24102;&#26469;&#29702;&#35299;&#25361;&#25112;&#65292;&#31867;&#20284;&#20110;&#20154;&#31867;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;LLM&#20542;&#21521;&#20110;&#26681;&#25454;&#20854;&#24819;&#35937;&#21147;&#65288;&#32852;&#24819;&#35760;&#24518;&#65289;&#25512;&#27979;&#21644;&#29983;&#25104;&#20869;&#23481;&#26469;&#22635;&#34917;&#36825;&#20123;&#20449;&#24687;&#32570;&#22833;&#12290;&#23613;&#31649;&#36825;&#20123;&#29468;&#27979;&#20598;&#23572;&#21487;&#33021;&#19982;&#20107;&#23454;&#20449;&#24687;&#19968;&#33268;&#65292;&#20294;&#20854;&#20934;&#30830;&#24615;&#24182;&#19981;&#20445;&#35777;&#65292;&#32463;&#24120;&#23548;&#33268;&#24187;&#35273;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18976v1 Announce Type: cross  Abstract: Hallucination has emerged as the most vulnerable aspect of contemporary Large Language Models (LLMs). In this paper, we introduce the Sorry, Come Again (SCA) prompting, aimed to avoid LLM hallucinations by enhancing comprehension through: (i) optimal paraphrasing and (ii) injecting [PAUSE] tokens to delay LLM generation. First, we provide an in-depth analysis of linguistic nuances: formality, readability, and concreteness of prompts for 21 LLMs, and elucidate how these nuances contribute to hallucinated generation. Prompts with lower readability, formality, or concreteness pose comprehension challenges for LLMs, similar to those faced by humans. In such scenarios, an LLM tends to speculate and generate content based on its imagination (associative memory) to fill these information gaps. Although these speculations may occasionally align with factual information, their accuracy is not assured, often resulting in hallucination. Recent st
&lt;/p&gt;</description></item><item><title>Transformer&#27169;&#22411;&#22312;&#25913;&#38761;&#20256;&#32479;&#20219;&#21153;&#21644;&#25512;&#36827;&#36328;&#34892;&#19994;&#30740;&#31350;&#21644;&#24320;&#21457;&#20013;&#20135;&#29983;&#38761;&#21629;&#24615;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2403.18969</link><description>&lt;p&gt;
&#20174;&#27010;&#24565;&#21040;&#23454;&#29616;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey on Large Language Models from Concept to Implementation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18969
&lt;/p&gt;
&lt;p&gt;
Transformer&#27169;&#22411;&#22312;&#25913;&#38761;&#20256;&#32479;&#20219;&#21153;&#21644;&#25512;&#36827;&#36328;&#34892;&#19994;&#30740;&#31350;&#21644;&#24320;&#21457;&#20013;&#20135;&#29983;&#38761;&#21629;&#24615;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22522;&#20110;Transformer&#26550;&#26500;&#26500;&#24314;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#21457;&#23637;&#26497;&#22823;&#25299;&#23485;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#24212;&#29992;&#30340;&#33539;&#22260;&#65292;&#36229;&#36234;&#20102;&#26368;&#21021;&#22312;&#32842;&#22825;&#26426;&#22120;&#20154;&#25216;&#26415;&#20013;&#30340;&#24212;&#29992;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#36825;&#20123;&#27169;&#22411;&#30340;&#22810;&#26041;&#38754;&#24212;&#29992;&#65292;&#30528;&#37325;&#20171;&#32461;&#20102;GPT&#31995;&#21015;&#12290;&#36825;&#39033;&#25506;&#32034;&#32858;&#28966;&#20110;&#20154;&#24037;&#26234;&#33021;(AI)&#39537;&#21160;&#24037;&#20855;&#22312;&#25913;&#38761;&#20256;&#32479;&#32534;&#30721;&#21644;&#38382;&#39064;&#35299;&#20915;&#31561;&#20219;&#21153;&#19978;&#30340;&#38761;&#21629;&#24615;&#24433;&#21709;&#65292;&#21516;&#26102;&#22312;&#36328;&#36234;&#19981;&#21516;&#34892;&#19994;&#30340;&#30740;&#31350;&#21644;&#24320;&#21457;&#20013;&#24320;&#36767;&#26032;&#36335;&#24452;&#12290;&#20174;&#20195;&#30721;&#35299;&#37322;&#21644;&#22270;&#20687;&#25551;&#36848;&#21040;&#20419;&#36827;&#20132;&#20114;&#24335;&#31995;&#32479;&#30340;&#25645;&#24314;&#21644;&#25512;&#36827;&#35745;&#31639;&#39046;&#22495;&#65292;Transformer&#27169;&#22411;&#20307;&#29616;&#20102;&#28145;&#24230;&#23398;&#20064;&#12289;&#25968;&#25454;&#20998;&#26512;&#21644;&#31070;&#32463;&#32593;&#32476;&#35774;&#35745;&#30340;&#21327;&#21516;&#20316;&#29992;&#12290;&#26412;&#32508;&#36848;&#28145;&#20837;&#25506;&#35752;&#20102;Transformer&#27169;&#22411;&#30340;&#26368;&#26032;&#30740;&#31350;&#65292;&#31361;&#20986;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18969v1 Announce Type: cross  Abstract: Recent advancements in Large Language Models (LLMs), particularly those built on Transformer architectures, have significantly broadened the scope of natural language processing (NLP) applications, transcending their initial use in chatbot technology. This paper investigates the multifaceted applications of these models, with an emphasis on the GPT series. This exploration focuses on the transformative impact of artificial intelligence (AI) driven tools in revolutionizing traditional tasks like coding and problem-solving, while also paving new paths in research and development across diverse industries. From code interpretation and image captioning to facilitating the construction of interactive systems and advancing computational domains, Transformer models exemplify a synergy of deep learning, data analysis, and neural network design. This survey provides an in-depth look at the latest research in Transformer models, highlighting the
&lt;/p&gt;</description></item><item><title>LORD&#36890;&#36807;&#19981;&#26399;&#26395;&#30340;&#35821;&#35328;&#30446;&#26631;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#27169;&#22411;&#30340;&#30456;&#21453;&#22870;&#21169;&#35774;&#35745;&#65292;&#20197;&#20415;&#26377;&#25928;&#21033;&#29992;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#20316;&#20026;&#38646;-shot&#22870;&#21169;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2403.18965</link><description>&lt;p&gt;
LORD&#65306;&#22522;&#20110;&#22823;&#27169;&#22411;&#30340;&#30456;&#21453;&#22870;&#21169;&#35774;&#35745;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;
&lt;/p&gt;
&lt;p&gt;
LORD: Large Models based Opposite Reward Design for Autonomous Driving
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18965
&lt;/p&gt;
&lt;p&gt;
LORD&#36890;&#36807;&#19981;&#26399;&#26395;&#30340;&#35821;&#35328;&#30446;&#26631;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#27169;&#22411;&#30340;&#30456;&#21453;&#22870;&#21169;&#35774;&#35745;&#65292;&#20197;&#20415;&#26377;&#25928;&#21033;&#29992;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#20316;&#20026;&#38646;-shot&#22870;&#21169;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#39537;&#21160;&#30340;&#33258;&#21160;&#39550;&#39542;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#26367;&#20195;&#25968;&#25454;&#39537;&#21160;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#30340;&#36873;&#25321;&#12290;&#28982;&#32780;&#65292;&#20026;RL&#21046;&#23450;&#26377;&#25928;&#30340;&#22870;&#21169;&#20989;&#25968;&#38754;&#20020;&#25361;&#25112;&#65292;&#22240;&#20026;&#35201;&#22312;&#19981;&#21516;&#22330;&#26223;&#20013;&#23450;&#20041;&#21644;&#37327;&#21270;&#33391;&#22909;&#30340;&#39550;&#39542;&#34892;&#20026;&#30340;&#22797;&#26434;&#24615;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#20316;&#20026;&#38646;-shot&#22870;&#21169;&#27169;&#22411;&#65292;&#20026;&#25351;&#23450;&#20855;&#26377;&#26399;&#26395;&#35821;&#35328;&#30446;&#26631;&#30340;&#20219;&#21153;&#24341;&#36215;&#20102;&#37325;&#35201;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#33258;&#21160;&#39550;&#39542;&#30340;&#26399;&#26395;&#35821;&#35328;&#30446;&#26631;&#65292;&#22914;&#8220;&#23433;&#20840;&#39550;&#39542;&#8221;&#65292;&#23545;&#20110;&#39044;&#35757;&#32451;&#27169;&#22411;&#26469;&#35828;&#26159;&#27169;&#31946;&#19988;&#38590;&#20197;&#29702;&#35299;&#30340;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#19981;&#26399;&#26395;&#30340;&#35821;&#35328;&#30446;&#26631;&#65292;&#27604;&#22914;&#8220;&#30896;&#25758;&#8221;&#65292;&#26356;&#21152;&#20855;&#20307;&#19988;&#21487;&#36319;&#36394;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;LORD&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#22823;&#27169;&#22411;&#30340;&#30456;&#21453;&#22870;&#21169;&#35774;&#35745;&#65292;&#36890;&#36807;&#19981;&#26399;&#26395;&#30340;&#35821;&#35328;&#30446;&#26631;&#26469;&#23454;&#29616;&#23545;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#26377;&#25928;&#20351;&#29992;&#65292;&#20316;&#20026;&#38646;-shot&#22870;&#21169;&#27169;&#22411;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#26174;&#31034;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18965v1 Announce Type: cross  Abstract: Reinforcement learning (RL) based autonomous driving has emerged as a promising alternative to data-driven imitation learning approaches. However, crafting effective reward functions for RL poses challenges due to the complexity of defining and quantifying good driving behaviors across diverse scenarios. Recently, large pretrained models have gained significant attention as zero-shot reward models for tasks specified with desired linguistic goals. However, the desired linguistic goals for autonomous driving such as "drive safely" are ambiguous and incomprehensible by pretrained models. On the other hand, undesired linguistic goals like "collision" are more concrete and tractable. In this work, we introduce LORD, a novel large models based opposite reward design through undesired linguistic goals to enable the efficient use of large pretrained models as zero-shot reward models. Through extensive experiments, our proposed framework shows
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;Grover&#21644;Deutsch-Josza&#31561;&#22522;&#30784;&#37327;&#23376;&#31639;&#27861;&#65292;&#36890;&#36807;&#19968;&#32452;&#31934;&#24515;&#26500;&#24314;&#30340;&#26465;&#20214;&#65292;&#25512;&#26029;&#29983;&#29289;&#21644;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#22312;&#19968;&#27573;&#26102;&#38388;&#20869;&#26159;&#21542;&#20855;&#26377;&#32487;&#32493;&#32500;&#25345;&#21160;&#24577;&#27963;&#21160;&#30340;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.18963</link><description>&lt;p&gt;
&#20351;&#29992;&#37327;&#23376;&#35745;&#31639;&#25512;&#26029;&#29983;&#29289;&#21644;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#30340;&#21160;&#24577;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Using Quantum Computing to Infer Dynamic Behaviors of Biological and Artificial Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18963
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;Grover&#21644;Deutsch-Josza&#31561;&#22522;&#30784;&#37327;&#23376;&#31639;&#27861;&#65292;&#36890;&#36807;&#19968;&#32452;&#31934;&#24515;&#26500;&#24314;&#30340;&#26465;&#20214;&#65292;&#25512;&#26029;&#29983;&#29289;&#21644;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#22312;&#19968;&#27573;&#26102;&#38388;&#20869;&#26159;&#21542;&#20855;&#26377;&#32487;&#32493;&#32500;&#25345;&#21160;&#24577;&#27963;&#21160;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26032;&#38382;&#39064;&#31867;&#21035;&#30340;&#25506;&#32034;&#26159;&#37327;&#23376;&#35745;&#31639;&#30740;&#31350;&#30340;&#19968;&#20010;&#27963;&#36291;&#39046;&#22495;&#12290;&#19968;&#20010;&#22522;&#26412;&#19978;&#23436;&#20840;&#26410;&#34987;&#25506;&#35752;&#30340;&#20027;&#39064;&#26159;&#20351;&#29992;&#37327;&#23376;&#31639;&#27861;&#21644;&#35745;&#31639;&#26469;&#25506;&#32034;&#21644;&#35810;&#38382;&#31070;&#32463;&#32593;&#32476;&#30340;&#21151;&#33021;&#21160;&#24577;&#12290;&#36825;&#26159;&#23558;&#37327;&#23376;&#35745;&#31639;&#24212;&#29992;&#20110;&#29983;&#29289;&#21644;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#24314;&#27169;&#21644;&#20223;&#30495;&#30340;&#23578;&#26410;&#25104;&#29087;&#30340;&#20027;&#39064;&#30340;&#19968;&#20010;&#32452;&#25104;&#37096;&#20998;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#31934;&#24515;&#26500;&#24314;&#30340;&#19968;&#32452;&#26465;&#20214;&#26469;&#20351;&#29992;&#20004;&#20010;&#22522;&#30784;&#37327;&#23376;&#31639;&#27861;&#65292;Grover&#21644;Deutsch-Josza&#65292;&#20197;&#20351;&#36755;&#20986;&#27979;&#37327;&#20855;&#26377;&#19968;&#31181;&#35299;&#37322;&#65292;&#20445;&#35777;&#25105;&#20204;&#33021;&#22815;&#25512;&#26029;&#19968;&#20010;&#31616;&#21333;&#30340;&#31070;&#32463;&#32593;&#32476;&#34920;&#31034;&#65288;&#36866;&#29992;&#20110;&#29983;&#29289;&#21644;&#20154;&#24037;&#32593;&#32476;&#65289;&#22312;&#19968;&#27573;&#26102;&#38388;&#21518;&#26159;&#21542;&#26377;&#21487;&#33021;&#32487;&#32493;&#32500;&#25345;&#21160;&#24577;&#27963;&#21160;&#12290;&#25110;&#32773;&#36825;&#20123;&#21160;&#24577;&#20445;&#35777;&#20250;&#20572;&#27490;&#65292;&#35201;&#20040;&#26159;&#36890;&#36807;'&#30315;&#30187;'&#21160;&#24577;&#65292;&#35201;&#20040;&#26159;&#38745;&#27490;&#29366;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18963v1 Announce Type: cross  Abstract: The exploration of new problem classes for quantum computation is an active area of research. An essentially completely unexplored topic is the use of quantum algorithms and computing to explore and ask questions \textit{about} the functional dynamics of neural networks. This is a component of the still-nascent topic of applying quantum computing to the modeling and simulations of biological and artificial neural networks. In this work, we show how a carefully constructed set of conditions can use two foundational quantum algorithms, Grover and Deutsch-Josza, in such a way that the output measurements admit an interpretation that guarantees we can infer if a simple representation of a neural network (which applies to both biological and artificial networks) after some period of time has the potential to continue sustaining dynamic activity. Or whether the dynamics are guaranteed to stop either through 'epileptic' dynamics or quiescence
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20221;&#32508;&#21512;&#24615;&#26816;&#26597;&#28165;&#21333;&#65292;&#26088;&#22312;&#25351;&#23548;&#20174;&#19994;&#32773;&#35780;&#20272;&#29983;&#25104;AI&#36719;&#20214;&#20135;&#21697;&#30340;&#20851;&#38190;&#21457;&#24067;&#23601;&#32490;&#26041;&#38754;&#65292;&#20197;&#25552;&#39640;&#22312;&#30495;&#23454;&#29615;&#22659;&#20013;&#30340;&#21487;&#38752;&#24615;&#21644;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.18958</link><description>&lt;p&gt;
&#19968;&#20221;&#38024;&#23545;&#29983;&#25104;AI&#36719;&#20214;&#20135;&#21697;&#30340;&#26368;&#26032;&#21457;&#24067;&#23601;&#32490;&#26816;&#26597;&#28165;&#21333;&#23454;&#36341;
&lt;/p&gt;
&lt;p&gt;
A State-of-the-practice Release-readiness Checklist for Generative AI-based Software Products
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18958
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20221;&#32508;&#21512;&#24615;&#26816;&#26597;&#28165;&#21333;&#65292;&#26088;&#22312;&#25351;&#23548;&#20174;&#19994;&#32773;&#35780;&#20272;&#29983;&#25104;AI&#36719;&#20214;&#20135;&#21697;&#30340;&#20851;&#38190;&#21457;&#24067;&#23601;&#32490;&#26041;&#38754;&#65292;&#20197;&#25552;&#39640;&#22312;&#30495;&#23454;&#29615;&#22659;&#20013;&#30340;&#21487;&#38752;&#24615;&#21644;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#38598;&#25104;&#21040;&#36719;&#20214;&#20135;&#21697;&#20013;&#30340;&#22797;&#26434;&#24615;&#65292;&#37325;&#28857;&#20851;&#27880;&#30830;&#23450;&#20854;&#21457;&#24067;&#23601;&#32490;&#24615;&#25152;&#36935;&#21040;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#23545;&#28784;&#33394;&#25991;&#29486;&#30340;&#31995;&#32479;&#22238;&#39038;&#30830;&#23450;&#20102;&#37096;&#32626;LLMs&#26102;&#36935;&#21040;&#30340;&#24120;&#35265;&#25361;&#25112;&#65292;&#20174;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#21040;&#29992;&#25143;&#20307;&#39564;&#31561;&#26041;&#38754;&#12290;&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20221;&#20840;&#38754;&#30340;&#26816;&#26597;&#28165;&#21333;&#65292;&#26088;&#22312;&#25351;&#23548;&#20174;&#19994;&#32773;&#35780;&#20272;&#20851;&#38190;&#30340;&#21457;&#24067;&#23601;&#32490;&#26041;&#38754;&#65292;&#27604;&#22914;&#24615;&#33021;&#12289;&#30417;&#25511;&#21644;&#37096;&#32626;&#31574;&#30053;&#65292;&#26088;&#22312;&#25552;&#39640;LLM&#24212;&#29992;&#22312;&#29616;&#23454;&#29615;&#22659;&#20013;&#30340;&#21487;&#38752;&#24615;&#21644;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18958v1 Announce Type: cross  Abstract: This paper investigates the complexities of integrating Large Language Models (LLMs) into software products, with a focus on the challenges encountered for determining their readiness for release. Our systematic review of grey literature identifies common challenges in deploying LLMs, ranging from pre-training and fine-tuning to user experience considerations. The study introduces a comprehensive checklist designed to guide practitioners in evaluating key release readiness aspects such as performance, monitoring, and deployment strategies, aiming to enhance the reliability and effectiveness of LLM-based applications in real-world settings.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38543;&#26426;&#32858;&#21512;&#27874;&#26463;&#25104;&#24418;&#30340;&#26041;&#26696;&#65292;&#36890;&#36807;&#38543;&#26426;&#25277;&#26679;&#29983;&#25104;&#32858;&#21512;&#22120;&#27874;&#26463;&#24418;&#25104;&#30690;&#37327;&#65292;&#20197;&#35299;&#20915;&#22823;&#35268;&#27169;&#32593;&#32476;&#20013;&#30340;&#32858;&#21512;&#35823;&#24046;&#26368;&#23567;&#21270;&#21644;&#35774;&#22791;&#36873;&#25321;&#26368;&#22823;&#21270;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.18946</link><description>&lt;p&gt;
&#38754;&#21521;&#22823;&#35268;&#27169;&#32593;&#32476;&#30340;&#38543;&#26426;&#32858;&#21512;&#27874;&#26463;&#25104;&#24418;&#29992;&#20110;&#31354;&#20013;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Random Aggregate Beamforming for Over-the-Air Federated Learning in Large-Scale Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18946
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38543;&#26426;&#32858;&#21512;&#27874;&#26463;&#25104;&#24418;&#30340;&#26041;&#26696;&#65292;&#36890;&#36807;&#38543;&#26426;&#25277;&#26679;&#29983;&#25104;&#32858;&#21512;&#22120;&#27874;&#26463;&#24418;&#25104;&#30690;&#37327;&#65292;&#20197;&#35299;&#20915;&#22823;&#35268;&#27169;&#32593;&#32476;&#20013;&#30340;&#32858;&#21512;&#35823;&#24046;&#26368;&#23567;&#21270;&#21644;&#35774;&#22791;&#36873;&#25321;&#26368;&#22823;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#65292;&#37096;&#32626;&#26222;&#36941;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#24212;&#29992;&#20110;&#32593;&#32476;&#36793;&#32536;&#30340;&#36235;&#21183;&#26085;&#30410;&#26126;&#26174;&#12290;&#20316;&#20026;&#19968;&#31181;&#26377;&#26395;&#23454;&#29616;&#23433;&#20840;&#36793;&#32536;&#26234;&#33021;&#30340;&#26694;&#26550;&#65292;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#31354;&#20013;&#35745;&#31639;&#65288;AirComp&#65289;&#24050;&#32463;&#34987;&#38598;&#25104;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#36890;&#20449;&#25928;&#29575;&#12290;&#26412;&#25991;&#32771;&#34385;&#20102;&#32852;&#21512;&#35774;&#22791;&#36873;&#25321;&#21644;&#32858;&#21512;&#27874;&#26463;&#25104;&#24418;&#35774;&#35745;&#65292;&#20854;&#30446;&#26631;&#26159;&#26368;&#23567;&#21270;&#32858;&#21512;&#35823;&#24046;&#24182;&#26368;&#22823;&#21270;&#36873;&#23450;&#35774;&#22791;&#30340;&#25968;&#37327;&#12290;&#36825;&#23548;&#33268;&#20102;&#19968;&#20010;&#32452;&#21512;&#38382;&#39064;&#65292;&#22312;&#22823;&#35268;&#27169;&#32593;&#32476;&#20013;&#23588;&#20854;&#38590;&#20197;&#35299;&#20915;&#12290;&#20026;&#20102;&#20197;&#19968;&#31181;&#32463;&#27982;&#26377;&#25928;&#30340;&#26041;&#24335;&#35299;&#20915;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38543;&#26426;&#32858;&#21512;&#27874;&#26463;&#25104;&#24418;&#30340;&#26041;&#26696;&#65292;&#35813;&#26041;&#26696;&#36890;&#36807;&#38543;&#26426;&#25277;&#26679;&#29983;&#25104;&#32858;&#21512;&#27874;&#26463;&#24418;&#25104;&#30690;&#37327;&#65292;&#32780;&#38750;&#20248;&#21270;&#12290;&#35813;&#26041;&#26696;&#30340;&#23454;&#26045;&#19981;&#38656;&#35201;&#36827;&#34892;&#20449;&#36947;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18946v1 Announce Type: cross  Abstract: At present, there is a trend to deploy ubiquitous artificial intelligence (AI) applications at the edge of the network. As a promising framework that enables secure edge intelligence, federated learning (FL) has received widespread attention, and over-the-air computing (AirComp) has been integrated to further improve the communication efficiency. In this paper, we consider a joint device selection and aggregate beamforming design with the objectives of minimizing the aggregate error and maximizing the number of selected devices. This yields a combinatorial problem, which is difficult to solve especially in large-scale networks. To tackle the problems in a cost-effective manner, we propose a random aggregate beamforming-based scheme, which generates the aggregator beamforming vector via random sampling rather than optimization. The implementation of the proposed scheme does not require the channel estimation. We additionally use asympto
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#20174;&#33258;&#30001;&#25991;&#26412;&#25918;&#23556;&#23398;&#25253;&#21578;&#20013;&#25552;&#21462;&#20449;&#24687;&#30340;&#27969;&#31243;&#65292;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#21644;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#26469;&#22788;&#29702;&#33258;&#21160;&#30340;SR&#27880;&#20876;&#34920;&#22635;&#20889;&#12290;</title><link>https://arxiv.org/abs/2403.18938</link><description>&lt;p&gt;
&#23558;&#33258;&#30001;&#25991;&#26412;&#25918;&#23556;&#23398;&#31508;&#35760;&#36716;&#21464;&#20026;&#20855;&#26377;&#29983;&#25104;&#21464;&#25442;&#22120;&#30340;&#32467;&#26500;&#21270;&#25253;&#21578;
&lt;/p&gt;
&lt;p&gt;
Reshaping Free-Text Radiology Notes Into Structured Reports With Generative Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18938
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#20174;&#33258;&#30001;&#25991;&#26412;&#25918;&#23556;&#23398;&#25253;&#21578;&#20013;&#25552;&#21462;&#20449;&#24687;&#30340;&#27969;&#31243;&#65292;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#21644;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#26469;&#22788;&#29702;&#33258;&#21160;&#30340;SR&#27880;&#20876;&#34920;&#22635;&#20889;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32972;&#26223;&#65306;&#25918;&#23556;&#23398;&#25253;&#21578;&#36890;&#24120;&#20197;&#33258;&#30001;&#25991;&#26412;&#26684;&#24335;&#32534;&#20889;&#65292;&#20351;&#20020;&#24202;&#20449;&#24687;&#38590;&#20197;&#25552;&#21462;&#21644;&#20351;&#29992;&#12290; &#26368;&#36817;&#65292;&#21508;&#31181;&#21307;&#23398;&#23398;&#20250;&#25512;&#33616;&#37319;&#29992;&#32467;&#26500;&#21270;&#25253;&#21578;&#65288;SR&#65289;&#65292;&#30001;&#20110;&#20854;&#25552;&#20379;&#30340;&#20248;&#21183;&#65288;&#22914;&#26631;&#20934;&#21270;&#12289;&#23436;&#25972;&#24615;&#21644;&#20449;&#24687;&#26816;&#32034;&#65289;&#65292;&#32467;&#26500;&#21270;&#25253;&#21578;&#65288;SR&#65289;&#30340;&#37319;&#29992;&#24471;&#21040;&#20102;&#25512;&#33616;&#12290; &#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20174;&#33258;&#30001;&#25991;&#26412;&#25918;&#23556;&#23398;&#25253;&#21578;&#20013;&#25552;&#21462;&#20449;&#24687;&#30340;&#27969;&#31243;&#65292;&#19982;&#19968;&#20010;&#22269;&#23478;&#20171;&#20837;&#21644;&#21307;&#23398;&#25918;&#23556;&#23398;&#20250;&#25552;&#20986;&#30340;&#21442;&#32771;SR&#27880;&#20876;&#34920;&#39033;&#30446;&#30456;&#21305;&#37197;&#65292;&#37325;&#28857;&#25918;&#22312;&#23545;&#20855;&#26377;&#28107;&#24052;&#30244;&#30340;&#24739;&#32773;&#36827;&#34892;CT&#20998;&#26399;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18938v1 Announce Type: cross  Abstract: BACKGROUND: Radiology reports are typically written in a free-text format, making clinical information difficult to extract and use. Recently the adoption of structured reporting (SR) has been recommended by various medical societies thanks to the advantages it offers, e.g. standardization, completeness and information retrieval. We propose a pipeline to extract information from free-text radiology reports, that fits with the items of the reference SR registry proposed by a national society of interventional and medical radiology, focusing on CT staging of patients with lymphoma. METHODS: Our work aims to leverage the potential of Natural Language Processing (NLP) and Transformer-based models to deal with automatic SR registry filling. With the availability of 174 radiology reports, we investigate a rule-free generative Question Answering approach based on a domain-specific version of T5 (IT5). Two strategies (batch-truncation and ex-p
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#36890;&#36807;&#20998;&#26512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#25919;&#27835;&#35758;&#39064;&#20869;&#23481;&#21644;&#39118;&#26684;&#26469;&#27979;&#37327;&#20854;&#25919;&#27835;&#20559;&#35265;&#65292;&#20027;&#24352;&#24212;&#35813;&#26377;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#25919;&#27835;&#20559;&#35265;&#30340;&#32454;&#31890;&#24230;&#21644;&#21487;&#35299;&#37322;&#24615;&#34913;&#37327;&#12290;</title><link>https://arxiv.org/abs/2403.18932</link><description>&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#27979;&#37327;&#25919;&#27835;&#20559;&#35265;&#65306;&#35328;&#35770;&#20869;&#23481;&#21644;&#34920;&#36798;&#26041;&#24335;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Measuring Political Bias in Large Language Models: What Is Said and How It Is Said
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18932
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#36890;&#36807;&#20998;&#26512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#25919;&#27835;&#35758;&#39064;&#20869;&#23481;&#21644;&#39118;&#26684;&#26469;&#27979;&#37327;&#20854;&#25919;&#27835;&#20559;&#35265;&#65292;&#20027;&#24352;&#24212;&#35813;&#26377;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#25919;&#27835;&#20559;&#35265;&#30340;&#32454;&#31890;&#24230;&#21644;&#21487;&#35299;&#37322;&#24615;&#34913;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#20998;&#26512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#25919;&#27835;&#35758;&#39064;&#20869;&#23481;&#21644;&#39118;&#26684;&#26469;&#27979;&#37327;&#20854;&#25919;&#27835;&#20559;&#35265;&#12290;&#29616;&#26377;&#30340;&#22522;&#20934;&#21644;&#27979;&#37327;&#26041;&#27861;&#20851;&#27880;&#24615;&#21035;&#21644;&#31181;&#26063;&#20559;&#35265;&#65292;&#20294;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23384;&#22312;&#25919;&#27835;&#20559;&#35265;&#65292;&#21487;&#33021;&#23548;&#33268;&#19979;&#28216;&#24212;&#29992;&#20013;&#30340;&#26497;&#21270;&#21644;&#20854;&#20182;&#21361;&#23475;&#12290;&#20026;&#20102;&#21521;&#29992;&#25143;&#25552;&#20379;&#36879;&#26126;&#24230;&#65292;&#25105;&#20204;&#20027;&#24352;&#24212;&#35813;&#26377;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#25919;&#27835;&#20559;&#35265;&#30340;&#32454;&#31890;&#24230;&#21644;&#21487;&#35299;&#37322;&#24615;&#27979;&#37327;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#27979;&#37327;&#26041;&#27861;&#26082;&#32771;&#34385;&#20102;&#29983;&#27542;&#26435;&#21033;&#21644;&#27668;&#20505;&#21464;&#21270;&#31561;&#19981;&#21516;&#25919;&#27835;&#35758;&#39064;&#30340;&#20869;&#23481;&#65288;&#29983;&#25104;&#29289;&#30340;&#23454;&#36136;&#65289;&#65292;&#20063;&#32771;&#34385;&#20102;&#36825;&#31181;&#20559;&#35265;&#30340;&#39118;&#26684;&#65288;&#35789;&#27719;&#30340;&#26497;&#24615;&#65289;&#12290;&#25105;&#20204;&#27979;&#37327;&#20102;&#21313;&#19968;&#20010;&#24320;&#28304;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25919;&#27835;&#20559;&#35265;&#65292;&#24182;&#23637;&#31034;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#22312;&#25193;&#23637;&#21040;&#20854;&#20182;&#20027;&#39064;&#26102;&#26082;&#26131;&#20110;&#25193;&#23637;&#21448;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18932v1 Announce Type: cross  Abstract: We propose to measure political bias in LLMs by analyzing both the content and style of their generated content regarding political issues. Existing benchmarks and measures focus on gender and racial biases. However, political bias exists in LLMs and can lead to polarization and other harms in downstream applications. In order to provide transparency to users, we advocate that there should be fine-grained and explainable measures of political biases generated by LLMs. Our proposed measure looks at different political issues such as reproductive rights and climate change, at both the content (the substance of the generation) and the style (the lexical polarity) of such bias. We measured the political bias in eleven open-sourced LLMs and showed that our proposed framework is easily scalable to other topics and is explainable.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#28982;&#24341;&#23548;&#30340;&#35748;&#30693;&#36827;&#21270;&#31574;&#30053;&#65292;&#36890;&#36807;&#22810;&#23618;&#34701;&#21512;&#33258;&#36866;&#24212;&#23398;&#20064;&#21644;&#33258;&#28982;&#36807;&#31243;&#65292;&#26377;&#25928;&#39044;&#27979;&#21271;&#28201;&#24102;&#28246;&#27850;&#20013;&#30340;&#28342;&#35299;&#27687;&#27987;&#24230;</title><link>https://arxiv.org/abs/2403.18923</link><description>&lt;p&gt;
&#33258;&#28982;&#24341;&#23548;&#30340;&#35748;&#30693;&#36827;&#21270;&#29992;&#20110;&#39044;&#27979;&#21271;&#28201;&#24102;&#28246;&#27850;&#20013;&#30340;&#28342;&#35299;&#27687;&#27987;&#24230;
&lt;/p&gt;
&lt;p&gt;
Nature-Guided Cognitive Evolution for Predicting Dissolved Oxygen Concentrations in North Temperate Lakes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18923
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#28982;&#24341;&#23548;&#30340;&#35748;&#30693;&#36827;&#21270;&#31574;&#30053;&#65292;&#36890;&#36807;&#22810;&#23618;&#34701;&#21512;&#33258;&#36866;&#24212;&#23398;&#20064;&#21644;&#33258;&#28982;&#36807;&#31243;&#65292;&#26377;&#25928;&#39044;&#27979;&#21271;&#28201;&#24102;&#28246;&#27850;&#20013;&#30340;&#28342;&#35299;&#27687;&#27987;&#24230;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#21271;&#28201;&#24102;&#28246;&#27850;&#20013;&#30340;&#28342;&#35299;&#27687;&#65288;DO&#65289;&#27987;&#24230;&#38656;&#35201;&#23545;&#19981;&#21516;&#29983;&#24577;&#31995;&#32479;&#20013;&#30340;&#29289;&#20505;&#27169;&#24335;&#36827;&#34892;&#20840;&#38754;&#30740;&#31350;&#65292;&#36825;&#20984;&#26174;&#20102;&#36873;&#25321;&#29289;&#20505;&#29305;&#24449;&#21644;&#29305;&#24449;&#20132;&#20114;&#30340;&#37325;&#35201;&#24615;&#12290;&#22522;&#20110;&#36807;&#31243;&#30340;&#27169;&#22411;&#21463;&#37096;&#20998;&#36807;&#31243;&#30693;&#35782;&#38480;&#21046;&#25110;&#29305;&#24449;&#34920;&#31034;&#36807;&#20110;&#31616;&#21270;&#65292;&#32780;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#26377;&#25928;&#36873;&#25321;&#19981;&#21516;&#28246;&#27850;&#31867;&#22411;&#21644;&#20219;&#21153;&#30340;&#30456;&#20851;&#29305;&#24449;&#20132;&#20114;&#26041;&#38754;&#38754;&#20020;&#25361;&#25112;&#65292;&#23588;&#20854;&#26159;&#22312;DO&#25968;&#25454;&#25910;&#38598;&#19981;&#39057;&#32321;&#30340;&#24773;&#20917;&#19979;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#28982;&#24341;&#23548;&#30340;&#35748;&#30693;&#36827;&#21270;&#65288;NGCE&#65289;&#31574;&#30053;&#65292;&#36825;&#20195;&#34920;&#20102;&#33258;&#36866;&#24212;&#23398;&#20064;&#19982;&#33258;&#28982;&#36807;&#31243;&#22810;&#23618;&#34701;&#21512;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#21033;&#29992;&#20195;&#35874;&#36807;&#31243;&#20026;&#22522;&#30784;&#30340;&#27169;&#22411;&#29983;&#25104;&#27169;&#25311;DO&#26631;&#31614;&#12290;&#21033;&#29992;&#36825;&#20123;&#27169;&#25311;&#26631;&#31614;&#65292;&#25105;&#20204;&#23454;&#26045;&#20102;&#19968;&#20010;&#22810;&#31181;&#32676;&#35748;&#30693;&#36827;&#21270;&#25628;&#32034;&#65292;&#27169;&#22411;&#21453;&#26144;&#33258;&#28982;&#26377;&#26426;&#20307;&#65292;&#36866;&#24212;&#24615;&#22320;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18923v1 Announce Type: cross  Abstract: Predicting dissolved oxygen (DO) concentrations in north temperate lakes requires a comprehensive study of phenological patterns across various ecosystems, which highlights the significance of selecting phenological features and feature interactions. Process-based models are limited by partial process knowledge or oversimplified feature representations, while machine learning models face challenges in efficiently selecting relevant feature interactions for different lake types and tasks, especially under the infrequent nature of DO data collection. In this paper, we propose a Nature-Guided Cognitive Evolution (NGCE) strategy, which represents a multi-level fusion of adaptive learning with natural processes. Specifically, we utilize metabolic process-based models to generate simulated DO labels. Using these simulated labels, we implement a multi-population cognitive evolutionary search, where models, mirroring natural organisms, adaptiv
&lt;/p&gt;</description></item><item><title>CPR&#26159;&#19968;&#31181;&#26032;&#30340;&#29992;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;RAG&#26041;&#27861;&#65292;&#20855;&#26377;&#24378;&#22823;&#30340;&#29256;&#26435;&#20445;&#25252;&#20445;&#38556;&#65292;&#21487;&#20197;&#22312;&#28151;&#21512;&#31169;&#20154;&#35774;&#32622;&#20013;&#26465;&#20214;&#29983;&#25104;&#36755;&#20986;&#65292;&#21516;&#26102;&#19981;&#26292;&#38706;&#20851;&#20110;&#26816;&#32034;&#22270;&#29255;&#30340;&#29420;&#29305;&#21487;&#35782;&#21035;&#20449;&#24687;&#12290;</title><link>https://arxiv.org/abs/2403.18920</link><description>&lt;p&gt;
CPR&#65306;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#29992;&#20110;&#29256;&#26435;&#20445;&#25252;
&lt;/p&gt;
&lt;p&gt;
CPR: Retrieval Augmented Generation for Copyright Protection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18920
&lt;/p&gt;
&lt;p&gt;
CPR&#26159;&#19968;&#31181;&#26032;&#30340;&#29992;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;RAG&#26041;&#27861;&#65292;&#20855;&#26377;&#24378;&#22823;&#30340;&#29256;&#26435;&#20445;&#25252;&#20445;&#38556;&#65292;&#21487;&#20197;&#22312;&#28151;&#21512;&#31169;&#20154;&#35774;&#32622;&#20013;&#26465;&#20214;&#29983;&#25104;&#36755;&#20986;&#65292;&#21516;&#26102;&#19981;&#26292;&#38706;&#20851;&#20110;&#26816;&#32034;&#22270;&#29255;&#30340;&#29420;&#29305;&#21487;&#35782;&#21035;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#27491;&#26085;&#30410;&#25104;&#20026;&#19968;&#31181;&#28789;&#27963;&#19988;&#24378;&#22823;&#30340;&#25216;&#26415;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#36866;&#24212;&#31169;&#20154;&#29992;&#25143;&#25968;&#25454;&#32780;&#26080;&#38656;&#35757;&#32451;&#65292;&#22788;&#29702;&#20449;&#29992;&#24402;&#22240;&#65292;&#24182;&#20801;&#35768;&#35268;&#27169;&#21270;&#30340;&#39640;&#25928;&#26426;&#22120;&#36951;&#24536;&#12290;&#28982;&#32780;&#65292;&#22270;&#20687;&#29983;&#25104;&#30340;RAG&#25216;&#26415;&#21487;&#33021;&#23548;&#33268;&#27169;&#22411;&#36755;&#20986;&#20013;&#30340;&#37096;&#20998;&#26816;&#32034;&#26679;&#26412;&#34987;&#22797;&#21046;&#12290;&#20026;&#20102;&#20943;&#23569;&#27844;&#28431;&#26816;&#32034;&#38598;&#20013;&#21253;&#21547;&#30340;&#31169;&#20154;&#20449;&#24687;&#30340;&#39118;&#38505;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#24102;&#26377;&#26816;&#32034;&#30340;&#21463;&#29256;&#26435;&#20445;&#25252;&#29983;&#25104;&#65288;CPR&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#30340;&#29992;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;RAG&#26041;&#27861;&#65292;&#22312;&#28151;&#21512;&#31169;&#20154;&#35774;&#32622;&#20013;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#29256;&#26435;&#20445;&#25252;&#20445;&#35777;&#12290;CPR&#20801;&#35768;&#23558;&#25193;&#25955;&#27169;&#22411;&#30340;&#36755;&#20986;&#26465;&#20214;&#35774;&#32622;&#20026;&#19968;&#32452;&#26816;&#32034;&#21040;&#30340;&#22270;&#20687;&#65292;&#21516;&#26102;&#36824;&#20445;&#35777;&#19981;&#20250;&#22312;&#29983;&#25104;&#30340;&#36755;&#20986;&#20013;&#26292;&#38706;&#20851;&#20110;&#36825;&#20123;&#31034;&#20363;&#30340;&#29420;&#29305;&#21487;&#35782;&#21035;&#20449;&#24687;&#12290;&#29305;&#21035;&#26159;&#36890;&#36807;&#20174;&#21512;&#24182;&#23427;&#20204;&#30340;&#25193;&#25955;&#27010;&#29575;&#20998;&#24067;&#36827;&#34892;&#25277;&#26679;&#26469;&#23454;&#29616;&#36825;&#19968;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18920v1 Announce Type: cross  Abstract: Retrieval Augmented Generation (RAG) is emerging as a flexible and robust technique to adapt models to private users data without training, to handle credit attribution, and to allow efficient machine unlearning at scale. However, RAG techniques for image generation may lead to parts of the retrieved samples being copied in the model's output. To reduce risks of leaking private information contained in the retrieved set, we introduce Copy-Protected generation with Retrieval (CPR), a new method for RAG with strong copyright protection guarantees in a mixed-private setting for diffusion models.CPR allows to condition the output of diffusion models on a set of retrieved images, while also guaranteeing that unique identifiable information about those example is not exposed in the generated outputs. In particular, it does so by sampling from a mixture of public (safe) distribution and private (user) distribution by merging their diffusion s
&lt;/p&gt;</description></item><item><title>&#39640;&#20284;&#28982;&#21306;&#22495;&#23558;&#19981;&#20250;&#34987;&#29983;&#25104;&#22914;&#26524;&#23427;&#20204;&#21253;&#21547;&#26368;&#23567;&#27010;&#29575;&#36136;&#37327;&#65292;&#22522;&#20110;&#27492;&#35266;&#23519;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26412;&#22320;&#22266;&#26377;&#32500;&#24230;&#20272;&#35745;&#36827;&#34892;&#31163;&#32676;&#26816;&#27979;&#30340;&#26041;&#27861;</title><link>https://arxiv.org/abs/2403.18910</link><description>&lt;p&gt;
&#23545;&#31163;&#32676;&#25968;&#25454;&#26816;&#27979;&#24726;&#35770;&#30340;&#20284;&#28982;&#20960;&#20309;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
A Geometric Explanation of the Likelihood OOD Detection Paradox
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18910
&lt;/p&gt;
&lt;p&gt;
&#39640;&#20284;&#28982;&#21306;&#22495;&#23558;&#19981;&#20250;&#34987;&#29983;&#25104;&#22914;&#26524;&#23427;&#20204;&#21253;&#21547;&#26368;&#23567;&#27010;&#29575;&#36136;&#37327;&#65292;&#22522;&#20110;&#27492;&#35266;&#23519;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26412;&#22320;&#22266;&#26377;&#32500;&#24230;&#20272;&#35745;&#36827;&#34892;&#31163;&#32676;&#26816;&#27979;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#20284;&#28982;&#30340;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;(DGMs)&#36890;&#24120;&#34920;&#29616;&#20986;&#20196;&#20154;&#22256;&#24785;&#30340;&#34892;&#20026;&#65306;&#24403;&#22312;&#30456;&#23545;&#22797;&#26434;&#30340;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#26102;&#65292;&#23427;&#20204;&#20250;&#32473;&#26469;&#33258;&#26356;&#31616;&#21333;&#26469;&#28304;&#30340;&#31163;&#32676;&#25968;&#25454;&#36171;&#20104;&#26356;&#39640;&#30340;&#20284;&#28982;&#20540;&#12290;&#26356;&#20351;&#20154;&#24863;&#21040;&#31070;&#31192;&#30340;&#26159;&#65292;&#23613;&#31649;&#20855;&#26377;&#26356;&#39640;&#30340;&#20284;&#28982;&#20540;&#65292;&#20294;&#36825;&#20123;DGMs&#20174;&#26410;&#29983;&#25104;&#36807;&#31163;&#32676;&#26679;&#26412;&#12290;&#36825;&#20010;&#21452;&#31649;&#40784;&#19979;&#30340;&#24726;&#35770;&#23578;&#26410;&#24471;&#21040;&#26368;&#32456;&#35299;&#37322;&#65292;&#20351;&#24471;&#22522;&#20110;&#20284;&#28982;&#30340;&#31163;&#32676;&#26816;&#27979;&#19981;&#21487;&#38752;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#35266;&#23519;&#26159;&#65292;&#22914;&#26524;&#39640;&#20284;&#28982;&#21306;&#22495;&#20013;&#21253;&#21547;&#20102;&#26368;&#23567;&#27010;&#29575;&#36136;&#37327;&#65292;&#37027;&#20040;&#36825;&#20123;&#21306;&#22495;&#23558;&#19981;&#20250;&#34987;&#29983;&#25104;&#12290;&#25105;&#20204;&#28436;&#31034;&#20102;&#22312;&#22260;&#32469;&#20302;&#32500;&#27969;&#24418;&#25968;&#25454;&#30340;&#22320;&#26041;&#21487;&#33021;&#20986;&#29616;&#22823;&#23494;&#24230;&#20294;&#20302;&#27010;&#29575;&#36136;&#37327;&#30340;&#30475;&#20284;&#30683;&#30462;&#24773;&#20917;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#36890;&#36807;&#26412;&#22320;&#22266;&#26377;&#32500;&#24230;(LID)&#20272;&#35745;&#21487;&#20197;&#35782;&#21035;&#36825;&#31181;&#22330;&#26223;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#39044;&#35757;&#32451;&#30340;DGM&#33719;&#24471;&#30340;&#20284;&#28982;&#21644;LID&#20272;&#35745;&#30456;&#37197;&#23545;&#30340;&#31163;&#32676;&#26816;&#27979;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18910v1 Announce Type: cross  Abstract: Likelihood-based deep generative models (DGMs) commonly exhibit a puzzling behaviour: when trained on a relatively complex dataset, they assign higher likelihood values to out-of-distribution (OOD) data from simpler sources. Adding to the mystery, OOD samples are never generated by these DGMs despite having higher likelihoods. This two-pronged paradox has yet to be conclusively explained, making likelihood-based OOD detection unreliable. Our primary observation is that high-likelihood regions will not be generated if they contain minimal probability mass. We demonstrate how this seeming contradiction of large densities yet low probability mass can occur around data confined to low-dimensional manifolds. We also show that this scenario can be identified through local intrinsic dimension (LID) estimation, and propose a method for OOD detection which pairs the likelihoods and LID estimates obtained from a pre-trained DGM. Our method can b
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;DeepView&#26041;&#27861;&#24212;&#29992;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#65292;&#20197;&#20943;&#23569;&#32534;&#30721;&#22120;&#27169;&#22411;&#23384;&#22312;&#30340;&#39118;&#38505;&#24182;&#35299;&#37322;&#27169;&#22411;&#20915;&#31574;&#36807;&#31243;&#12290;</title><link>https://arxiv.org/abs/2403.18872</link><description>&lt;p&gt;
&#32534;&#30721;&#22120;LLMs&#39592;&#24178;&#30340;&#23450;&#21521;&#21487;&#35270;&#21270;
&lt;/p&gt;
&lt;p&gt;
Targeted Visualization of the Backbone of Encoder LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18872
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;DeepView&#26041;&#27861;&#24212;&#29992;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#65292;&#20197;&#20943;&#23569;&#32534;&#30721;&#22120;&#27169;&#22411;&#23384;&#22312;&#30340;&#39118;&#38505;&#24182;&#35299;&#37322;&#27169;&#22411;&#20915;&#31574;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#20013;&#30340;&#26368;&#26032;&#25216;&#26415;&#12290;&#20854;&#20013;&#26368;&#24120;&#35265;&#30340;&#20004;&#31181;&#26550;&#26500;&#26159;&#32534;&#30721;&#22120;&#65292;&#22914;BERT&#65292;&#21644;&#35299;&#30721;&#22120;&#65292;&#22914;GPT&#27169;&#22411;&#12290;&#23613;&#31649;&#32534;&#30721;&#22120;&#27169;&#22411;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#20063;&#23384;&#22312;&#19968;&#20123;&#39118;&#38505;&#65292;&#21253;&#25324;&#20559;&#35265;&#38382;&#39064;&#25110;&#26131;&#21463;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#36825;&#34920;&#26126;&#20102;&#38656;&#35201;&#21487;&#35299;&#37322;&#30340;AI&#26469;&#26816;&#27979;&#36825;&#20123;&#38382;&#39064;&#12290;&#34429;&#28982;&#30446;&#21069;&#23384;&#22312;&#21508;&#31181;&#20851;&#27880;&#39044;&#27979;&#21333;&#20010;&#36755;&#20837;&#30340;&#23616;&#37096;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#65292;&#20294;&#22522;&#20110;&#38477;&#32500;&#30340;&#29992;&#20110;&#20998;&#31867;&#26816;&#26597;&#30340;&#20840;&#23616;&#26041;&#27861;&#65292;&#24182;&#19988;&#22312;&#20854;&#20182;&#39046;&#22495;&#20986;&#29616;&#24182;&#36229;&#36234;&#20165;&#22312;&#23884;&#20837;&#31354;&#38388;&#20013;&#20351;&#29992;t-SNE&#30340;&#26041;&#27861;&#65292;&#22312;NLP&#20013;&#24182;&#19981;&#21313;&#20998;&#24191;&#27867;&#20256;&#25773;&#12290;&#20026;&#20102;&#32553;&#23567;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;DeepView&#26041;&#27861;&#22312;NLP&#39046;&#22495;&#30340;&#24212;&#29992;&#65292;&#35813;&#26041;&#27861;&#29992;&#20110;&#22312;&#20108;&#32500;&#20013;&#21487;&#35270;&#21270;&#20915;&#31574;&#20989;&#25968;&#30340;&#19968;&#37096;&#20998;&#20197;&#21450;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18872v1 Announce Type: cross  Abstract: Attention based Large Language Models (LLMs) are the state-of-the-art in natural language processing (NLP). The two most common architectures are encoders such as BERT, and decoders like the GPT models. Despite the success of encoder models, on which we focus in this work, they also bear several risks, including issues with bias or their susceptibility for adversarial attacks, signifying the necessity for explainable AI to detect such issues. While there does exist various local explainability methods focusing on the prediction of single inputs, global methods based on dimensionality reduction for classification inspection, which have emerged in other domains and that go further than just using t-SNE in the embedding space, are not widely spread in NLP.   To reduce this gap, we investigate the application of DeepView, a method for visualizing a part of the decision function together with a data set in two dimensions, to the NLP domain.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#26495;&#24341;&#23548;&#26041;&#27861;&#65292;&#23558;&#27668;&#33016;&#30340;&#20020;&#24202;&#30693;&#35782;&#34701;&#20837;XAI&#26041;&#27861;&#65292;&#20197;&#36807;&#28388;&#25481;&#33853;&#22312;&#27169;&#26495;&#20043;&#22806;&#30340;&#19981;&#30456;&#20851;&#35299;&#37322;&#12290;</title><link>https://arxiv.org/abs/2403.18871</link><description>&lt;p&gt;
&#36890;&#36807;&#20020;&#24202;&#39046;&#22495;&#30693;&#35782;&#34893;&#29983;&#30340;&#27169;&#26495;&#25552;&#39640;&#20102;&#21518;&#32493;AI&#35299;&#37322;&#22312;&#27668;&#33016;&#20998;&#31867;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Clinical Domain Knowledge-Derived Template Improves Post Hoc AI Explanations in Pneumothorax Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18871
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#26495;&#24341;&#23548;&#26041;&#27861;&#65292;&#23558;&#27668;&#33016;&#30340;&#20020;&#24202;&#30693;&#35782;&#34701;&#20837;XAI&#26041;&#27861;&#65292;&#20197;&#36807;&#28388;&#25481;&#33853;&#22312;&#27169;&#26495;&#20043;&#22806;&#30340;&#19981;&#30456;&#20851;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27668;&#33016;&#26159;&#19968;&#31181;&#24613;&#24615;&#33016;&#37096;&#30142;&#30149;&#65292;&#30001;&#20110;&#32954;&#37096;&#21644;&#33016;&#22721;&#20043;&#38388;&#24322;&#24120;&#31215;&#32858;&#27668;&#20307;&#24341;&#36215;&#12290;&#20026;&#20102;&#35299;&#20915;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#24120;&#35265;&#30340;&#19981;&#36879;&#26126;&#38382;&#39064;&#65292;&#24341;&#20837;&#20102;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#26041;&#27861;&#26469;&#21246;&#30011;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#27668;&#33016;&#35786;&#26029;&#30456;&#20851;&#21306;&#22495;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#35299;&#37322;&#26377;&#26102;&#20250;&#20559;&#31163;&#23454;&#38469;&#30149;&#21464;&#21306;&#22495;&#65292;&#31361;&#26174;&#20102;&#36827;&#19968;&#27493;&#25913;&#36827;&#30340;&#24517;&#35201;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#26495;&#24341;&#23548;&#26041;&#27861;&#65292;&#23558;&#27668;&#33016;&#30340;&#20020;&#24202;&#30693;&#35782;&#34701;&#20837;XAI&#26041;&#27861;&#29983;&#25104;&#30340;&#27169;&#22411;&#35299;&#37322;&#20013;&#65292;&#20174;&#32780;&#25552;&#39640;&#36825;&#20123;&#35299;&#37322;&#30340;&#36136;&#37327;&#12290;&#21033;&#29992;&#25918;&#23556;&#31185;&#21307;&#29983;&#21019;&#24314;&#30340;&#19968;&#31181;&#30149;&#21464;&#21010;&#20998;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#39318;&#20808;&#29983;&#25104;&#20195;&#34920;&#21487;&#33021;&#21457;&#29983;&#27668;&#33016;&#21306;&#22495;&#30340;&#27169;&#26495;&#12290;&#28982;&#21518;&#23558;&#27492;&#27169;&#26495;&#21472;&#21152;&#21040;&#27169;&#22411;&#35299;&#37322;&#19978;&#65292;&#20197;&#36807;&#28388;&#25481;&#33853;&#22312;&#27169;&#26495;&#20043;&#22806;&#30340;&#19981;&#30456;&#20851;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18871v1 Announce Type: cross  Abstract: Background: Pneumothorax is an acute thoracic disease caused by abnormal air collection between the lungs and chest wall. To address the opaqueness often associated with deep learning (DL) models, explainable artificial intelligence (XAI) methods have been introduced to outline regions related to pneumothorax diagnoses made by DL models. However, these explanations sometimes diverge from actual lesion areas, highlighting the need for further improvement. Method: We propose a template-guided approach to incorporate the clinical knowledge of pneumothorax into model explanations generated by XAI methods, thereby enhancing the quality of these explanations. Utilizing one lesion delineation created by radiologists, our approach first generates a template that represents potential areas of pneumothorax occurrence. This template is then superimposed on model explanations to filter out extraneous explanations that fall outside the template's b
&lt;/p&gt;</description></item><item><title>&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#23545;&#20110;&#22686;&#24378;&#22825;&#27668;&#21644;&#27668;&#20505;&#24314;&#27169;&#30340;&#21487;&#20449;&#24230;&#21644;&#23454;&#29992;&#24615;&#33267;&#20851;&#37325;&#35201;&#65292;&#21253;&#25324;&#21518;&#39564;&#21487;&#35299;&#37322;&#24615;&#25216;&#26415;&#21644;&#20174;&#22836;&#35774;&#35745;&#30340;&#22266;&#26377;&#21487;&#35299;&#37322;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2403.18864</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#29992;&#20110;&#22825;&#27668;&#21644;&#27668;&#20505;&#39044;&#27979;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Interpretable Machine Learning for Weather and Climate Prediction: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18864
&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#23545;&#20110;&#22686;&#24378;&#22825;&#27668;&#21644;&#27668;&#20505;&#24314;&#27169;&#30340;&#21487;&#20449;&#24230;&#21644;&#23454;&#29992;&#24615;&#33267;&#20851;&#37325;&#35201;&#65292;&#21253;&#25324;&#21518;&#39564;&#21487;&#35299;&#37322;&#24615;&#25216;&#26415;&#21644;&#20174;&#22836;&#35774;&#35745;&#30340;&#22266;&#26377;&#21487;&#35299;&#37322;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#22825;&#27668;&#21644;&#27668;&#20505;&#39044;&#27979;&#26041;&#38754;&#21462;&#24471;&#20102;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#22797;&#26434;&#27169;&#22411;&#36890;&#24120;&#32570;&#20047;&#22266;&#26377;&#30340;&#36879;&#26126;&#24230;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#34920;&#29616;&#20026;&#8220;&#40657;&#21283;&#23376;&#8221;&#65292;&#38459;&#30861;&#20102;&#29992;&#25143;&#20449;&#20219;&#65292;&#20063;&#38480;&#21046;&#20102;&#36827;&#19968;&#27493;&#30340;&#27169;&#22411;&#25913;&#36827;&#12290;&#22240;&#27492;&#65292;&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#22312;&#22686;&#24378;&#22825;&#27668;&#21644;&#27668;&#20505;&#24314;&#27169;&#30340;&#21487;&#20449;&#24230;&#21644;&#23454;&#29992;&#24615;&#26041;&#38754;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#26412;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#23457;&#26597;&#20102;&#24212;&#29992;&#20110;&#27668;&#35937;&#39044;&#27979;&#30340;&#24403;&#21069;&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#12290;&#25105;&#20204;&#23558;&#26041;&#27861;&#20998;&#31867;&#20026;&#20004;&#20010;&#20027;&#35201;&#33539;&#20363;&#65306;1&#65289;&#21518;&#39564;&#21487;&#35299;&#37322;&#24615;&#25216;&#26415;&#65292;&#35299;&#37322;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#22914;&#22522;&#20110;&#25200;&#21160;&#12289;&#22522;&#20110;&#21338;&#24328;&#35770;&#21644;&#22522;&#20110;&#26799;&#24230;&#30340;&#24402;&#22240;&#26041;&#27861;&#12290;2&#65289;&#20174;&#22836;&#24320;&#22987;&#35774;&#35745;&#22266;&#26377;&#21487;&#35299;&#37322;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#26641;&#38598;&#25104;&#21644;&#21487;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#31561;&#26550;&#26500;&#12290;&#25105;&#20204;&#24635;&#32467;&#20102;&#27599;&#31181;&#25216;&#26415;&#22914;&#20309;&#25552;&#20379;&#23545;&#39044;&#27979;&#27169;&#22411;&#20869;&#37096;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18864v1 Announce Type: cross  Abstract: Advanced machine learning models have recently achieved high predictive accuracy for weather and climate prediction. However, these complex models often lack inherent transparency and interpretability, acting as "black boxes" that impede user trust and hinder further model improvements. As such, interpretable machine learning techniques have become crucial in enhancing the credibility and utility of weather and climate modeling. In this survey, we review current interpretable machine learning approaches applied to meteorological predictions. We categorize methods into two major paradigms: 1) Post-hoc interpretability techniques that explain pre-trained models, such as perturbation-based, game theory based, and gradient-based attribution methods. 2) Designing inherently interpretable models from scratch using architectures like tree ensembles and explainable neural networks. We summarize how each technique provides insights into the pre
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25581;&#31034;&#20102;&#37327;&#23376;&#27979;&#37327;&#36807;&#31243;&#20013;&#33539;&#30068;&#24863;&#30693;&#29616;&#35937;&#30340;&#26426;&#21046;&#65292;&#35748;&#20026;&#39068;&#33394;&#22312;&#20154;&#31867;&#35270;&#35273;&#20013;&#21487;&#20197;&#34987;&#35270;&#20026;&#20809;&#23376;&#65292;&#20026;&#35270;&#35273;&#24863;&#30693;&#25552;&#20379;&#20102;&#26032;&#30340;&#37327;&#23376;&#35748;&#30693;&#35270;&#35282;&#12290;</title><link>https://arxiv.org/abs/2403.18850</link><description>&lt;p&gt;
&#20154;&#31867;&#35270;&#35273;&#20013;&#39068;&#33394;&#26159;&#21542;&#26159;&#20809;&#23376;&#65311;&#35270;&#35273;&#30693;&#35273;&#30340;&#37327;&#23376;&#35748;&#30693;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Are Colors Quanta of Light for Human Vision? A Quantum Cognition Study of Visual Perception
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18850
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25581;&#31034;&#20102;&#37327;&#23376;&#27979;&#37327;&#36807;&#31243;&#20013;&#33539;&#30068;&#24863;&#30693;&#29616;&#35937;&#30340;&#26426;&#21046;&#65292;&#35748;&#20026;&#39068;&#33394;&#22312;&#20154;&#31867;&#35270;&#35273;&#20013;&#21487;&#20197;&#34987;&#35270;&#20026;&#20809;&#23376;&#65292;&#20026;&#35270;&#35273;&#24863;&#30693;&#25552;&#20379;&#20102;&#26032;&#30340;&#37327;&#23376;&#35748;&#30693;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#37327;&#23376;&#27979;&#37327;&#36807;&#31243;&#20013;&#33539;&#30068;&#24863;&#30693;&#30340;&#29616;&#35937;&#12290;&#35813;&#29616;&#35937;&#30340;&#26426;&#21046;&#22312;&#20110;&#34987;&#24863;&#30693;&#30340;&#25193;&#25955;&#21050;&#28608;&#34987;&#35748;&#20026;&#23646;&#20110;&#19981;&#21516;&#31867;&#21035;&#65292;&#32780;&#34987;&#24863;&#30693;&#30340;&#25910;&#32553;&#21050;&#28608;&#34987;&#35748;&#20026;&#23646;&#20110;&#30456;&#21516;&#31867;&#21035;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#65292;&#30001;&#20110;&#32431;&#24577;&#20043;&#38388;&#30340;&#36317;&#31163;&#19982;&#23494;&#24230;&#24577;&#20043;&#38388;&#30340;&#36317;&#31163;&#30340;&#30830;&#23450;&#26041;&#24335;&#33258;&#28982;&#19981;&#21516;&#65292;&#22240;&#27492;&#33539;&#30068;&#24863;&#30693;&#29616;&#35937;&#26681;&#26893;&#20110;&#37327;&#23376;&#27979;&#37327;&#36807;&#31243;&#30340;&#32467;&#26500;&#20013;&#12290;&#25105;&#20204;&#23558;&#30740;&#31350;&#32467;&#26524;&#24212;&#29992;&#20110;&#39068;&#33394;&#30340;&#35270;&#35273;&#24863;&#30693;&#24773;&#20917;&#65292;&#24182;&#35748;&#20026;&#21487;&#20197;&#23558;&#39068;&#33394;&#35270;&#20026;&#20154;&#31867;&#35270;&#35273;&#24863;&#30693;&#20013;&#30340;&#20809;&#23376;&#65292;&#31867;&#20284;&#20110;&#20809;&#39057;&#29289;&#29702;&#27979;&#37327;&#20013;&#30340;&#20809;&#23376;&#12290;&#22312;&#25105;&#20204;&#30340;&#26041;&#27861;&#20013;&#65292;&#25105;&#20204;&#23558;&#30693;&#35273;&#30475;&#20316;&#26159;&#29616;&#26377;&#29289;&#29702;&#29616;&#23454;&#12289;&#21050;&#28608;&#20197;&#21450;&#34987;&#24863;&#30693;&#32773;&#25152;&#26399;&#26395;&#30340;&#29616;&#23454;&#20043;&#38388;&#30340;&#22797;&#26434;&#30456;&#36935;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18850v1 Announce Type: cross  Abstract: We study the phenomenon of categorical perception within the quantum measurement process. The mechanism underlying this phenomenon consists in dilating stimuli being perceived to belong to different categories and contracting stimuli being perceived to belong to the same category. We show that, due to the naturally different way in determining the distance between pure states compared to the distance between density states, the phenomenon of categorical perception is rooted in the structure of the quantum measurement process itself. We apply our findings to the situation of visual perception of colors and argue that it is possible to consider colors as light quanta for human visual perception in a similar way as photons are light quanta for physical measurements of light frequencies. In our approach we see perception as a complex encounter between the existing physical reality, the stimuli, and the reality expected by the perciever, re
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30450;&#24402;&#19968;&#21270;&#26031;&#22374;&#21464;&#20998;&#26799;&#24230;&#19979;&#38477;&#30340;&#26816;&#27979;&#22120;&#65292;&#29992;&#20110;&#35299;&#20915;&#26234;&#33021;&#22823;&#35268;&#27169;&#38543;&#26426;&#25509;&#20837;&#20013;&#30340;&#21069;&#23548;&#30896;&#25758;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#24320;&#21457;&#25913;&#36827;Hadamard&#21464;&#25442;&#21644;&#35774;&#35745;&#22359;MHT&#23618;&#26469;&#25552;&#39640;&#26816;&#27979;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.18846</link><description>&lt;p&gt;
&#22522;&#20110;&#30450;&#24402;&#19968;&#21270;&#26031;&#22374;&#21464;&#20998;&#26799;&#24230;&#19979;&#38477;&#30340;&#26234;&#33021;&#22823;&#35268;&#27169;&#38543;&#26426;&#25509;&#20837;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
The Blind Normalized Stein Variational Gradient Descent-Based Detection for Intelligent Massive Random Access
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18846
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30450;&#24402;&#19968;&#21270;&#26031;&#22374;&#21464;&#20998;&#26799;&#24230;&#19979;&#38477;&#30340;&#26816;&#27979;&#22120;&#65292;&#29992;&#20110;&#35299;&#20915;&#26234;&#33021;&#22823;&#35268;&#27169;&#38543;&#26426;&#25509;&#20837;&#20013;&#30340;&#21069;&#23548;&#30896;&#25758;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#24320;&#21457;&#25913;&#36827;Hadamard&#21464;&#25442;&#21644;&#35774;&#35745;&#22359;MHT&#23618;&#26469;&#25552;&#39640;&#26816;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32570;&#20047;&#39640;&#25928;&#30340;&#21069;&#23548;&#26816;&#27979;&#31639;&#27861;&#20173;&#28982;&#26159;&#35299;&#20915;&#23454;&#38469;&#36890;&#20449;&#22330;&#26223;&#20013;&#26234;&#33021;&#22823;&#35268;&#27169;&#38543;&#26426;&#25509;&#20837;(RA)&#20013;&#21069;&#23548;&#30896;&#25758;&#38382;&#39064;&#30340;&#25361;&#25112;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;(MLE)&#27169;&#22411;&#30340;&#26089;&#26399;&#21069;&#23548;&#26816;&#27979;&#26041;&#26696;&#65292;&#22312;&#25480;&#20104;&#24335;RA&#27969;&#31243;&#30340;&#31532;&#19968;&#27493;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#30450;&#24402;&#19968;&#21270;&#26031;&#22374;&#21464;&#20998;&#26799;&#24230;&#19979;&#38477;(SVGD)&#30340;&#26816;&#27979;&#22120;&#65292;&#20197;&#33719;&#24471;MLE&#27169;&#22411;&#30340;&#36817;&#20284;&#35299;&#12290;&#39318;&#20808;&#65292;&#36890;&#36807;&#25506;&#32034;Hadamard&#21464;&#25442;&#21644;&#23567;&#27874;&#21464;&#25442;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#25913;&#36827;Hadamard&#21464;&#25442;(MHT)&#65292;&#20351;&#29992;&#20108;&#38454;&#23548;&#25968;&#28388;&#27874;&#22120;&#23558;&#39640;&#39057;&#20998;&#31163;&#20986;&#37325;&#35201;&#37096;&#20998;&#12290;&#25509;&#19979;&#26469;&#65292;&#20026;&#20102;&#28040;&#38500;SVGD&#26816;&#27979;&#22120;&#20013;&#30340;&#22122;&#22768;&#24182;&#20943;&#36731;&#26799;&#24230;&#28040;&#22833;&#38382;&#39064;&#65292;&#35774;&#35745;&#20102;&#22522;&#20110;MHT&#30340;&#22359;MHT&#23618;&#65292;&#35813;&#23618;&#22522;&#20110;MHT&#12289;&#32553;&#25918;&#23618;&#12289;&#36719;&#38408;&#20540;&#23618;&#26500;&#24314;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18846v1 Announce Type: cross  Abstract: The lack of an efficient preamble detection algorithm remains a challenge for solving preamble collision problems in intelligent massive random access (RA) in practical communication scenarios. To solve this problem, we present a novel early preamble detection scheme based on a maximum likelihood estimation (MLE) model at the first step of the grant-based RA procedure. A novel blind normalized Stein variational gradient descent (SVGD)-based detector is proposed to obtain an approximate solution to the MLE model. First, by exploring the relationship between the Hadamard transform and wavelet transform, a new modified Hadamard transform (MHT) is developed to separate high-frequencies from important components using the second-order derivative filter. Next, to eliminate noise and mitigate the vanishing gradients problem in the SVGD-based detectors, the block MHT layer is designed based on the MHT, scaling layer, soft-thresholding layer, i
&lt;/p&gt;</description></item><item><title>&#20998;&#26512;&#20154;&#24037;&#26234;&#33021;&#19982;&#31185;&#23398;&#35745;&#37327;&#23398;&#12289;&#32593;&#32476;&#35745;&#37327;&#23398;&#21644;&#25991;&#29486;&#35745;&#37327;&#23398;&#30340;&#21327;&#21516;&#20316;&#29992;&#65292;&#25581;&#31034;&#21644;&#24378;&#35843;&#20154;&#24037;&#26234;&#33021;&#31639;&#27861;&#22312;&#36825;&#20123;&#39046;&#22495;&#20013;&#30340;&#24212;&#29992;&#28508;&#21147;&#21644;&#22909;&#22788;</title><link>https://arxiv.org/abs/2403.18838</link><description>&lt;p&gt;
&#21457;&#25381;&#20154;&#24037;&#26234;&#33021;&#30340;&#21147;&#37327;&#12290;&#20154;&#24037;&#26234;&#33021;&#22686;&#24378;&#30340;&#31185;&#23398;&#35745;&#37327;&#23398;&#12289;&#32593;&#32476;&#35745;&#37327;&#23398;&#21644;&#25991;&#29486;&#35745;&#37327;&#23398;&#30340;&#21069;&#27839;&#25216;&#26415;&#31995;&#32479;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Unleashing the Power of AI. A Systematic Review of Cutting-Edge Techniques in AI-Enhanced Scientometrics, Webometrics, and Bibliometrics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18838
&lt;/p&gt;
&lt;p&gt;
&#20998;&#26512;&#20154;&#24037;&#26234;&#33021;&#19982;&#31185;&#23398;&#35745;&#37327;&#23398;&#12289;&#32593;&#32476;&#35745;&#37327;&#23398;&#21644;&#25991;&#29486;&#35745;&#37327;&#23398;&#30340;&#21327;&#21516;&#20316;&#29992;&#65292;&#25581;&#31034;&#21644;&#24378;&#35843;&#20154;&#24037;&#26234;&#33021;&#31639;&#27861;&#22312;&#36825;&#20123;&#39046;&#22495;&#20013;&#30340;&#24212;&#29992;&#28508;&#21147;&#21644;&#22909;&#22788;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18838v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#20132;&#21449;&#25688;&#35201;&#65306;&#30446;&#30340;&#65306;&#26412;&#30740;&#31350;&#26088;&#22312;&#20998;&#26512;&#20154;&#24037;&#26234;&#33021;&#19982;&#31185;&#23398;&#35745;&#37327;&#23398;&#12289;&#32593;&#32476;&#35745;&#37327;&#23398;&#21644;&#25991;&#29486;&#35745;&#37327;&#23398;&#30340;&#21327;&#21516;&#20316;&#29992;&#65292;&#20197;&#25581;&#31034;&#21644;&#24378;&#35843;&#20154;&#24037;&#26234;&#33021;&#31639;&#27861;&#22312;&#36825;&#20123;&#39046;&#22495;&#20013;&#30340;&#24212;&#29992;&#28508;&#21147;&#21644;&#22909;&#22788;&#12290;&#26041;&#27861;/&#35774;&#35745;&#65306;&#36890;&#36807;&#36827;&#34892;&#31995;&#32479;&#25991;&#29486;&#32508;&#36848;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#25506;&#32034;&#20154;&#24037;&#26234;&#33021;&#22312;&#38761;&#26032;&#34913;&#37327;&#21644;&#20998;&#26512;&#23398;&#26415;&#20132;&#27969;&#26041;&#27861;&#12289;&#35782;&#21035;&#26032;&#20852;&#30740;&#31350;&#36235;&#21183;&#20197;&#21450;&#35780;&#20272;&#31185;&#23398;&#20986;&#29256;&#29289;&#24433;&#21709;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#22312;&#35832;&#22914;ProQuest&#12289;IEEE Explore&#12289;EBSCO&#12289;Web of Science&#21644;Scopus&#31561;&#30693;&#21517;&#25968;&#25454;&#24211;&#19978;&#23454;&#26045;&#20102;&#32508;&#21512;&#25628;&#32034;&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#25628;&#32034;&#33539;&#22260;&#28085;&#30422;&#20102;2000&#24180;1&#26376;1&#26085;&#33267;2022&#24180;9&#26376;&#38388;&#21457;&#34920;&#30340;&#25991;&#31456;&#65292;&#24635;&#20849;&#23457;&#38405;&#20102;61&#31687;&#30456;&#20851;&#25991;&#31456;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18838v1 Announce Type: cross  Abstract: Purpose: The study aims to analyze the synergy of Artificial Intelligence (AI), with scientometrics, webometrics, and bibliometrics to unlock and to emphasize the potential of the applications and benefits of AI algorithms in these fields.   Design/methodology/approach: By conducting a systematic literature review, our aim is to explore the potential of AI in revolutionizing the methods used to measure and analyze scholarly communication, identify emerging research trends, and evaluate the impact of scientific publications. To achieve this, we implemented a comprehensive search strategy across reputable databases such as ProQuest, IEEE Explore, EBSCO, Web of Science, and Scopus. Our search encompassed articles published from January 1, 2000, to September 2022, resulting in a thorough review of 61 relevant articles.   Findings: (i) Regarding scientometrics, the application of AI yields various distinct advantages, such as conducting ana
&lt;/p&gt;</description></item><item><title>DeepTraderX&#26159;&#19968;&#20010;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#30340;&#20132;&#26131;&#31995;&#32479;&#65292;&#22312;&#22810;&#32447;&#31243;&#24066;&#22330;&#27169;&#25311;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#19988;&#21487;&#20197;&#19982;&#25991;&#29486;&#20013;&#26368;&#20339;&#31574;&#30053;&#21305;&#25932;&#12290;</title><link>https://arxiv.org/abs/2403.18831</link><description>&lt;p&gt;
DeepTraderX: &#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#25361;&#25112;&#20256;&#32479;&#20132;&#26131;&#31574;&#30053;&#22312;&#22810;&#32447;&#31243;&#24066;&#22330;&#27169;&#25311;&#20013;&#30340;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;
DeepTraderX: Challenging Conventional Trading Strategies with Deep Learning in Multi-Threaded Market Simulations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18831
&lt;/p&gt;
&lt;p&gt;
DeepTraderX&#26159;&#19968;&#20010;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#30340;&#20132;&#26131;&#31995;&#32479;&#65292;&#22312;&#22810;&#32447;&#31243;&#24066;&#22330;&#27169;&#25311;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#19988;&#21487;&#20197;&#19982;&#25991;&#29486;&#20013;&#26368;&#20339;&#31574;&#30053;&#21305;&#25932;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;DeepTraderX&#65288;DTX&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#31616;&#21333;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#20132;&#26131;&#31995;&#32479;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#22312;&#22810;&#32447;&#31243;&#24066;&#22330;&#27169;&#25311;&#20013;&#30340;&#34920;&#29616;&#12290;&#22312;&#22823;&#32422;500&#20010;&#27169;&#25311;&#24066;&#22330;&#20132;&#26131;&#26085;&#20013;&#65292;DTX&#20165;&#36890;&#36807;&#35266;&#23519;&#20854;&#20182;&#31574;&#30053;&#20135;&#29983;&#30340;&#20215;&#26684;&#36827;&#34892;&#23398;&#20064;&#12290;&#36890;&#36807;&#36825;&#26679;&#20570;&#65292;&#23427;&#25104;&#21151;&#22320;&#21019;&#24314;&#20102;&#19968;&#31181;&#20174;&#24066;&#22330;&#25968;&#25454;&#21040;&#35201;&#20026;&#26576;&#20010;&#36164;&#20135;&#19979;&#36798;&#30340;&#25253;&#20215;&#65288;&#20080;&#20837;&#25110;&#21334;&#20986;&#35746;&#21333;&#65289;&#30340;&#26144;&#23556;&#20851;&#31995;&#12290;DTX&#22312;&#21382;&#21490;Level-2&#24066;&#22330;&#25968;&#25454;&#65288;&#21363;&#29305;&#23450;&#21487;&#20132;&#26131;&#36164;&#20135;&#30340;&#38480;&#20215;&#35746;&#21333;&#31807;&#65289;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#22788;&#29702;&#27599;&#20010;&#26102;&#38388;&#27493;&#39588;$T$&#30340;&#24066;&#22330;&#29366;&#24577;$S$&#65292;&#20197;&#30830;&#23450;&#24066;&#22330;&#35746;&#21333;&#30340;&#20215;&#26684;$P$&#12290;&#35757;&#32451;&#21644;&#27979;&#35797;&#20013;&#20351;&#29992;&#30340;&#24066;&#22330;&#25968;&#25454;&#26159;&#22522;&#20110;&#30495;&#23454;&#21382;&#21490;&#32929;&#31080;&#24066;&#22330;&#25968;&#25454;&#29983;&#25104;&#30340;&#29420;&#29305;&#24066;&#22330;&#26102;&#38388;&#34920;&#12290;DTX&#32463;&#36807;&#20102;&#19982;&#25991;&#29486;&#20013;&#26368;&#20339;&#31574;&#30053;&#30340;&#22823;&#37327;&#27979;&#35797;&#65292;&#20854;&#32467;&#26524;&#32463;&#36807;&#20102;&#32479;&#35745;&#20998;&#26512;&#39564;&#35777;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#24378;&#35843;&#20102;DTX&#19982;&#26368;&#20339;&#31574;&#30053;&#21305;&#25932;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18831v1 Announce Type: cross  Abstract: In this paper, we introduce DeepTraderX (DTX), a simple Deep Learning-based trader, and present results that demonstrate its performance in a multi-threaded market simulation. In a total of about 500 simulated market days, DTX has learned solely by watching the prices that other strategies produce. By doing this, it has successfully created a mapping from market data to quotes, either bid or ask orders, to place for an asset. Trained on historical Level-2 market data, i.e., the Limit Order Book (LOB) for specific tradable assets, DTX processes the market state $S$ at each timestep $T$ to determine a price $P$ for market orders. The market data used in both training and testing was generated from unique market schedules based on real historic stock market data. DTX was tested extensively against the best strategies in the literature, with its results validated by statistical analysis. Our findings underscore DTX's capability to rival, a
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23558;&#27169;&#22359;&#37325;&#26500;&#20026;&#24433;&#23376;&#29983;&#25104;&#31995;&#32479;&#65292;&#23454;&#29616;&#20102;&#35748;&#30693;&#26550;&#26500;&#19982;&#29983;&#25104;&#31070;&#32463;&#32593;&#32476;&#30340;&#26080;&#32541;&#36830;&#25509;</title><link>https://arxiv.org/abs/2403.18827</link><description>&lt;p&gt;
&#23558;&#29983;&#25104;&#32593;&#32476;&#19982;&#35748;&#30693;&#20849;&#21516;&#27169;&#22411;&#30456;&#32467;&#21512;
&lt;/p&gt;
&lt;p&gt;
Bridging Generative Networks with the Common Model of Cognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18827
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23558;&#27169;&#22359;&#37325;&#26500;&#20026;&#24433;&#23376;&#29983;&#25104;&#31995;&#32479;&#65292;&#23454;&#29616;&#20102;&#35748;&#30693;&#26550;&#26500;&#19982;&#29983;&#25104;&#31070;&#32463;&#32593;&#32476;&#30340;&#26080;&#32541;&#36830;&#25509;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#65292;&#23558;&#35748;&#30693;&#20849;&#21516;&#27169;&#22411;&#35843;&#25972;&#20026;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#20013;&#30340;&#22823;&#22411;&#29983;&#25104;&#32593;&#32476;&#27169;&#22411;&#12290;&#36890;&#36807;&#23558;&#20849;&#21516;&#27169;&#22411;&#20013;&#30340;&#27169;&#22359;&#37325;&#26500;&#20026;&#21608;&#36793;&#30340;&#24433;&#23376;&#29983;&#25104;&#31995;&#32479;&#65292;&#36825;&#20123;&#24433;&#23376;&#29983;&#25104;&#31995;&#32479;&#36741;&#21161;&#22788;&#29702;&#39640;&#23618;&#25512;&#29702;&#65292;&#20174;&#32780;&#23454;&#29616;&#35748;&#30693;&#26550;&#26500;&#19982;&#29983;&#25104;&#31070;&#32463;&#32593;&#32476;&#30340;&#26080;&#32541;&#36830;&#25509;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18827v1 Announce Type: new  Abstract: This article presents a theoretical framework for adapting the Common Model of Cognition to large generative network models within the field of artificial intelligence. This can be accomplished by restructuring modules within the Common Model into shadow production systems that are peripheral to a central production system, which handles higher-level reasoning based on the shadow productions' output. Implementing this novel structure within the Common Model allows for a seamless connection between cognitive architectures and generative neural networks.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;ViT&#27169;&#22411;&#29983;&#25104;&#30340;&#20840;&#23616;&#22270;&#20687;&#20808;&#39564;&#65292;&#20026;&#21333;&#22270;&#28145;&#24230;&#20272;&#35745;&#27169;&#22411;&#25552;&#20379;&#26356;&#35814;&#32454;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20351;&#29992;&#25193;&#25955;&#39592;&#24178;&#19988;&#21463;ViT&#23884;&#20837;&#26465;&#20214;&#32422;&#26463;&#30340;&#28145;&#24230;&#20272;&#35745;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2403.18807</link><description>&lt;p&gt;
ECoDepth: &#26377;&#25928;&#35843;&#25972;&#25193;&#25955;&#27169;&#22411;&#20197;&#29992;&#20110;&#21333;&#30446;&#28145;&#24230;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
ECoDepth: Effective Conditioning of Diffusion Models for Monocular Depth Estimation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18807
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;ViT&#27169;&#22411;&#29983;&#25104;&#30340;&#20840;&#23616;&#22270;&#20687;&#20808;&#39564;&#65292;&#20026;&#21333;&#22270;&#28145;&#24230;&#20272;&#35745;&#27169;&#22411;&#25552;&#20379;&#26356;&#35814;&#32454;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20351;&#29992;&#25193;&#25955;&#39592;&#24178;&#19988;&#21463;ViT&#23884;&#20837;&#26465;&#20214;&#32422;&#26463;&#30340;&#28145;&#24230;&#20272;&#35745;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32570;&#20047;&#35270;&#24046;&#32447;&#32034;&#30340;&#24773;&#20917;&#19979;&#65292;&#22522;&#20110;&#23398;&#20064;&#30340;&#21333;&#22270;&#28145;&#24230;&#20272;&#35745;&#65288;SIDE&#65289;&#27169;&#22411;&#20005;&#37325;&#20381;&#36182;&#22270;&#20687;&#20013;&#30340;&#38452;&#24433;&#21644;&#19978;&#19979;&#25991;&#32447;&#32034;&#12290;&#25105;&#20204;&#20174;&#24050;&#26377;&#30740;&#31350;&#30340;&#21551;&#21457;&#20013;&#25506;&#35752;&#20351;&#29992;&#20174;&#39044;&#35757;&#32451;&#30340;ViT&#27169;&#22411;&#29983;&#25104;&#30340;&#20840;&#23616;&#22270;&#20687;&#20808;&#39564;&#65292;&#20197;&#25552;&#20379;&#26356;&#35814;&#32454;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;&#22522;&#20110;&#36825;&#19968;&#24819;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20351;&#29992;&#25193;&#25955;&#39592;&#24178;&#30340;SIDE&#27169;&#22411;&#65292;&#20854;&#21463;&#21040;ViT&#23884;&#20837;&#30340;&#26465;&#20214;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18807v1 Announce Type: cross  Abstract: In the absence of parallax cues, a learning-based single image depth estimation (SIDE) model relies heavily on shading and contextual cues in the image. While this simplicity is attractive, it is necessary to train such models on large and varied datasets, which are difficult to capture. It has been shown that using embeddings from pre-trained foundational models, such as CLIP, improves zero shot transfer in several applications. Taking inspiration from this, in our paper we explore the use of global image priors generated from a pre-trained ViT model to provide more detailed contextual information. We argue that the embedding vector from a ViT model, pre-trained on a large dataset, captures greater relevant information for SIDE than the usual route of generating pseudo image captions, followed by CLIP based text embeddings. Based on this idea, we propose a new SIDE model using a diffusion backbone which is conditioned on ViT embedding
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#22810;&#30446;&#26631;&#36827;&#21270;&#31639;&#27861;&#65292;&#26412;&#30740;&#31350;&#22312;&#26368;&#22823;&#21270;&#24433;&#21709;&#21644;&#26368;&#23567;&#21270;&#31181;&#23376;&#38598;&#22823;&#23567;&#30340;&#22522;&#30784;&#19978;&#65292;&#20248;&#21270;&#20102;&#22810;&#20010;IM&#29305;&#23450;&#30446;&#26631;&#20989;&#25968;&#65292;&#21253;&#25324;&#39044;&#31639;&#12289;&#20844;&#24179;&#24615;&#12289;&#31038;&#21306;&#21644;&#26102;&#38388;&#12290;</title><link>https://arxiv.org/abs/2403.18755</link><description>&lt;p&gt;
&#22810;&#30446;&#26631;&#36827;&#21270;&#24433;&#21709;&#26368;&#22823;&#21270;&#65306;&#24179;&#34913;&#20256;&#25773;&#12289;&#39044;&#31639;&#12289;&#20844;&#24179;&#24615;&#21644;&#26102;&#38388;
&lt;/p&gt;
&lt;p&gt;
Many-Objective Evolutionary Influence Maximization: Balancing Spread, Budget, Fairness, and Time
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18755
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#22810;&#30446;&#26631;&#36827;&#21270;&#31639;&#27861;&#65292;&#26412;&#30740;&#31350;&#22312;&#26368;&#22823;&#21270;&#24433;&#21709;&#21644;&#26368;&#23567;&#21270;&#31181;&#23376;&#38598;&#22823;&#23567;&#30340;&#22522;&#30784;&#19978;&#65292;&#20248;&#21270;&#20102;&#22810;&#20010;IM&#29305;&#23450;&#30446;&#26631;&#20989;&#25968;&#65292;&#21253;&#25324;&#39044;&#31639;&#12289;&#20844;&#24179;&#24615;&#12289;&#31038;&#21306;&#21644;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24433;&#21709;&#26368;&#22823;&#21270;&#65288;IM&#65289;&#38382;&#39064;&#26088;&#22312;&#21457;&#29616;&#22270;&#20013;&#21487;&#20197;&#26368;&#22823;&#31243;&#24230;&#20256;&#25773;&#20449;&#24687;&#20256;&#25773;&#30340;&#33410;&#28857;&#38598;&#12290;&#36825;&#20010;&#38382;&#39064;&#34987;&#31216;&#20026;NP&#38590;&#39064;&#65292;&#36890;&#24120;&#36890;&#36807;&#26368;&#22823;&#21270;&#24433;&#21709;&#65288;&#20256;&#25773;&#65289;&#20197;&#21450;&#36873;&#25321;&#24615;&#20248;&#21270;&#31532;&#20108;&#20010;&#30446;&#26631;&#65288;&#20363;&#22914;&#26368;&#23567;&#21270;&#31181;&#23376;&#38598;&#22823;&#23567;&#25110;&#26368;&#22823;&#21270;&#24433;&#21709;&#20844;&#24179;&#24615;&#65289;&#26469;&#30740;&#31350;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#23454;&#38469;&#22330;&#26223;&#20013;&#65292;IM&#38382;&#39064;&#30340;&#22810;&#20010;&#26041;&#38754;&#24517;&#39035;&#21516;&#26102;&#36827;&#34892;&#20248;&#21270;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31532;&#19968;&#20010;&#26696;&#20363;&#30740;&#31350;&#65292;&#22312;&#27492;&#22522;&#30784;&#19978;&#20248;&#21270;&#20102;&#20960;&#20010;IM&#29305;&#23450;&#30446;&#26631;&#20989;&#25968;&#65292;&#21363;&#39044;&#31639;&#12289;&#20844;&#24179;&#24615;&#12289;&#31038;&#21306;&#21644;&#26102;&#38388;&#65292;&#21516;&#26102;&#26368;&#22823;&#21270;&#20256;&#25773;&#24433;&#21709;&#24182;&#26368;&#23567;&#21270;&#31181;&#23376;&#38598;&#22823;&#23567;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;MOEIM&#65288;&#29992;&#20110;&#24433;&#21709;&#26368;&#22823;&#21270;&#30340;&#22810;&#30446;&#26631;&#36827;&#21270;&#31639;&#27861;&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;NSGA-II&#30340;&#22810;&#30446;&#26631;&#36827;&#21270;&#31639;&#27861;&#65288;MOEA&#65289;&#65292;&#32467;&#21512;&#20102;&#20855;&#26377;&#22270;&#24863;&#30693;&#24615;&#30340;&#31639;&#23376;&#21644;&#26234;&#33021;&#21021;&#22987;&#21270;&#12290;&#25105;&#20204;&#36890;&#36807;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18755v1 Announce Type: cross  Abstract: The Influence Maximization (IM) problem seeks to discover the set of nodes in a graph that can spread the information propagation at most. This problem is known to be NP-hard, and it is usually studied by maximizing the influence (spread) and, optionally, optimizing a second objective, such as minimizing the seed set size or maximizing the influence fairness. However, in many practical scenarios multiple aspects of the IM problem must be optimized at the same time. In this work, we propose a first case study where several IM-specific objective functions, namely budget, fairness, communities, and time, are optimized on top of the maximization of influence and minimization of the seed set size. To this aim, we introduce MOEIM (Many-Objective Evolutionary Algorithm for Influence Maximization) a Multi-Objective Evolutionary Algorithm (MOEA) based on NSGA-II incorporating graph-aware operators and a smart initialization. We compare MOEIM in
&lt;/p&gt;</description></item><item><title>&#24635;&#20307;&#32780;&#35328;&#65292;&#36825;&#31687;&#35770;&#25991;&#35752;&#35770;&#20102;&#22312;&#20013;&#25991;&#20013;&#26816;&#27979; offensive &#35821;&#35328;&#30340;&#25361;&#25112;&#65292;&#24182;&#24378;&#35843;&#20102;&#24320;&#21457;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#30340;&#29305;&#23450;&#27169;&#22411;&#21644;&#24037;&#20855;&#12290;</title><link>https://arxiv.org/abs/2403.18314</link><description>&lt;p&gt;
&#20013;&#22269; offensive &#35821;&#35328;&#26816;&#27979;&#65306;&#29616;&#29366;&#19982;&#26410;&#26469;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;
Chinese Offensive Language Detection:Current Status and Future Directions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18314
&lt;/p&gt;
&lt;p&gt;
&#24635;&#20307;&#32780;&#35328;&#65292;&#36825;&#31687;&#35770;&#25991;&#35752;&#35770;&#20102;&#22312;&#20013;&#25991;&#20013;&#26816;&#27979; offensive &#35821;&#35328;&#30340;&#25361;&#25112;&#65292;&#24182;&#24378;&#35843;&#20102;&#24320;&#21457;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#30340;&#29305;&#23450;&#27169;&#22411;&#21644;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#27491;&#22312;&#20570;&#20986;&#30456;&#24403;&#22823;&#30340;&#21162;&#21147;&#30417;&#27979;&#21644;&#35268;&#33539;&#29992;&#25143;&#29983;&#25104;&#20869;&#23481;&#65292;&#20294;&#22312;&#25968;&#23383;&#31354;&#38388;&#20013;&#65292;&#24694;&#24847;&#35821;&#35328;&#65288;&#22914;&#20167;&#24680;&#35328;&#35770;&#25110;&#32593;&#32476;&#27450;&#20940;&#65289;&#30340;&#26222;&#36941;&#23384;&#22312;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#12290;&#37492;&#20110;&#32500;&#25252;&#25991;&#26126;&#21644;&#23562;&#37325;&#30340;&#22312;&#32447;&#29615;&#22659;&#30340;&#37325;&#35201;&#24615;&#65292;&#36843;&#20999;&#38656;&#35201;&#33021;&#22815;&#23454;&#26102;&#26816;&#27979;&#24694;&#24847;&#35328;&#35770;&#30340;&#33258;&#21160;&#31995;&#32479;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#24320;&#21457;&#22788;&#29702;&#27721;&#35821;&#31561;&#35821;&#35328;&#30340;&#26377;&#25928;&#31995;&#32479;&#65292;&#38754;&#20020;&#30528;&#37325;&#22823;&#25361;&#25112;&#65292;&#22240;&#20026;&#36825;&#20123;&#35821;&#35328;&#30340;&#22797;&#26434;&#21644;&#24494;&#22937;&#24615;&#20351;&#24471;&#33258;&#21160;&#22788;&#29702;&#21464;&#24471;&#22256;&#38590;&#12290;&#26412;&#25991;&#20840;&#38754;&#24635;&#32467;&#20102;&#20013;&#22269; offensive &#35821;&#35328;&#26816;&#27979;&#24773;&#20917;&#65292;&#23457;&#26597;&#20102;&#24403;&#21069;&#30340;&#22522;&#20934;&#21644;&#26041;&#27861;&#65292;&#24182;&#37325;&#28857;&#20171;&#32461;&#20102;&#29992;&#20110;&#35299;&#20915;&#22312;&#36825;&#31181;&#22797;&#26434;&#35821;&#35328;&#20013;&#26816;&#27979;&#24694;&#24847;&#35821;&#35328;&#30340;&#29420;&#29305;&#25361;&#25112;&#30340;&#29305;&#23450;&#27169;&#22411;&#21644;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18314v1 Announce Type: cross  Abstract: Despite the considerable efforts being made to monitor and regulate user-generated content on social media platforms, the pervasiveness of offensive language, such as hate speech or cyberbullying, in the digital space remains a significant challenge. Given the importance of maintaining a civilized and respectful online environment, there is an urgent and growing need for automatic systems capable of detecting offensive speech in real time. However, developing effective systems for processing languages such as Chinese presents a significant challenge, owing to the language's complex and nuanced nature, which makes it difficult to process automatically. This paper provides a comprehensive overview of offensive language detection in Chinese, examining current benchmarks and approaches and highlighting specific models and tools for addressing the unique challenges of detecting offensive language in this complex language. The primary object
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20449;&#21495;&#20256;&#25773;&#20998;&#26512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#37327;&#21270;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;ov-freeze&#31283;&#23450;KD-QAT&#36807;&#31243;&#30340;&#27934;&#23519;&#12290;</title><link>https://arxiv.org/abs/2403.18159</link><description>&lt;p&gt;
&#22114;&#65281;&#25105;&#20204;&#20919;&#20923;&#65306;&#36890;&#36807;&#20449;&#21495;&#20256;&#25773;&#20998;&#26512;&#25913;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#37327;&#21270;&#30693;&#35782;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Oh! We Freeze: Improving Quantized Knowledge Distillation via Signal Propagation Analysis for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18159
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20449;&#21495;&#20256;&#25773;&#20998;&#26512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#37327;&#21270;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;ov-freeze&#31283;&#23450;KD-QAT&#36807;&#31243;&#30340;&#27934;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#29983;&#25104;&#27169;&#22411;&#65292;&#22914;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21644;&#25193;&#25955;&#27169;&#22411;&#20998;&#21035;&#22312;NLP&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#24341;&#36215;&#20102;&#38761;&#21629;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#25512;&#29702;&#36895;&#24230;&#24930;&#65292;&#35745;&#31639;&#21644;&#20869;&#23384;&#38656;&#27714;&#39640;&#65292;&#36825;&#20351;&#24471;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#37096;&#32626;&#23427;&#20204;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#37327;&#21270;&#24863;&#30693;&#24494;&#35843;&#25216;&#26415;&#65292;&#20351;&#29992;&#30693;&#35782;&#33976;&#39311;&#65288;KD-QAT&#65289;&#26469;&#25913;&#21892;&#20351;&#29992;&#24120;&#29992;&#25968;&#25454;&#38598;&#25913;&#36827;4&#20301;&#37325;&#37327;&#37327;&#21270;&#30340;LLMs&#30340;&#24615;&#33021;&#65292;&#20197;&#23454;&#29616;&#27969;&#34892;&#30340;&#35821;&#35328;&#20351;&#29992;&#26696;&#20363;&#65292;&#22312;&#35774;&#22791;&#32842;&#22825;&#24212;&#29992;&#20013;&#12290;&#20026;&#20102;&#25913;&#36827;&#36825;&#31181;&#24494;&#35843;&#33539;&#24335;&#65292;&#20316;&#20026;&#20027;&#35201;&#36129;&#29486;&#65292;&#25105;&#20204;&#36890;&#36807;&#32463;&#39564;&#30740;&#31350;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#26799;&#24230;&#20256;&#25773;&#65292;&#25552;&#20379;&#23545;KD-QAT&#31283;&#23450;&#24615;&#30340;&#27934;&#23519;&#65292;&#20197;&#26356;&#22909;&#22320;&#29702;&#35299;&#22522;&#20110;KD-QAT&#30340;&#26041;&#27861;&#23545;&#20302;&#20301;&#37327;&#21270;&#35823;&#24046;&#30340;&#33030;&#24369;&#24615;&#12290;&#26681;&#25454;&#25105;&#20204;&#30340;&#35265;&#35299;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ov-freeze&#65292;&#19968;&#31181;&#31283;&#23450;KD-QAT&#36807;&#31243;&#30340;&#31616;&#21333;&#25216;&#26415;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#23454;&#39564;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18159v1 Announce Type: cross  Abstract: Large generative models, such as large language models (LLMs) and diffusion models have as revolutionized the fields of NLP and computer vision respectively. However, their slow inference, high computation and memory requirement makes it challenging to deploy them on edge devices. In this study, we propose a light-weight quantization aware fine tuning technique using knowledge distillation (KD-QAT) to improve the performance of 4-bit weight quantized LLMs using commonly available datasets to realize a popular language use case, on device chat applications. To improve this paradigm of finetuning, as main contributions, we provide insights into stability of KD-QAT by empirically studying the gradient propagation during training to better understand the vulnerabilities of KD-QAT based approaches to low-bit quantization errors. Based on our insights, we propose ov-freeze, a simple technique to stabilize the KD-QAT process. Finally, we expe
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#38382;&#39064;&#65292;&#21363;&#37319;&#29992;&#21355;&#26143;&#22270;&#20687;&#21644;&#20854;&#20182;&#29289;&#31181;&#20986;&#29616;&#20449;&#24687;&#26469;&#39044;&#27979;&#29289;&#31181;&#20986;&#29616;&#27169;&#24335;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#27169;&#22411;R-Tran&#65292;&#21487;&#20197;&#21033;&#29992;&#37096;&#20998;&#35266;&#27979;&#25968;&#25454;&#36827;&#34892;&#39044;&#27979;&#65292;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.18028</link><description>&lt;p&gt;
&#20174;&#37096;&#20998;&#35266;&#27979;&#39044;&#27979;&#29289;&#31181;&#20986;&#29616;&#27169;&#24335;
&lt;/p&gt;
&lt;p&gt;
Predicting species occurrence patterns from partial observations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18028
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#38382;&#39064;&#65292;&#21363;&#37319;&#29992;&#21355;&#26143;&#22270;&#20687;&#21644;&#20854;&#20182;&#29289;&#31181;&#20986;&#29616;&#20449;&#24687;&#26469;&#39044;&#27979;&#29289;&#31181;&#20986;&#29616;&#27169;&#24335;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#27169;&#22411;R-Tran&#65292;&#21487;&#20197;&#21033;&#29992;&#37096;&#20998;&#35266;&#27979;&#25968;&#25454;&#36827;&#34892;&#39044;&#27979;&#65292;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#24212;&#23545;&#29983;&#29289;&#22810;&#26679;&#24615;&#21644;&#27668;&#20505;&#21361;&#26426;&#65292;&#25105;&#20204;&#38656;&#35201;&#20102;&#35299;&#29289;&#31181;&#20998;&#24067;&#30340;&#20301;&#32622;&#20197;&#21450;&#36825;&#20123;&#27169;&#24335;&#22914;&#20309;&#21464;&#21270;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29289;&#31181;&#30340;&#35266;&#27979;&#25968;&#25454;&#20173;&#28982;&#38750;&#24120;&#26377;&#38480;&#65292;&#21487;&#29992;&#25968;&#25454;&#30340;&#37327;&#22312;&#19981;&#21516;&#20998;&#31867;&#32676;&#20043;&#38388;&#24046;&#24322;&#24456;&#22823;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#38382;&#39064;&#65292;&#21363;&#22312;&#32473;&#23450;&#21355;&#26143;&#22270;&#20687;&#21644;&#20854;&#20182;&#29289;&#31181;&#20986;&#29616;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#39044;&#27979;&#29289;&#31181;&#20986;&#29616;&#27169;&#24335;&#12290;&#20026;&#20102;&#22312;&#27492;&#20219;&#21153;&#19978;&#35780;&#20272;&#31639;&#27861;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;SatButterfly&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#34676;&#34678;&#30340;&#21355;&#26143;&#22270;&#20687;&#12289;&#29615;&#22659;&#25968;&#25454;&#21644;&#35266;&#27979;&#25968;&#25454;&#65292;&#26088;&#22312;&#19982;&#29616;&#26377;&#30340;&#40479;&#31867;&#35266;&#27979;&#25968;&#25454;&#38598;SatBird&#37197;&#23545;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#20219;&#21153;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#27169;&#22411;R-Tran&#65292;&#29992;&#20110;&#39044;&#27979;&#29289;&#31181;&#20986;&#29616;&#27169;&#24335;&#65292;&#21487;&#20197;&#22312;&#20219;&#20309;&#22320;&#26041;&#20351;&#29992;&#37096;&#20998;&#35266;&#27979;&#25968;&#25454;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;R-Tran&#22312;&#39044;&#27979;&#29289;&#31181;&#36973;&#36935;&#29575;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18028v1 Announce Type: cross  Abstract: To address the interlinked biodiversity and climate crises, we need an understanding of where species occur and how these patterns are changing. However, observational data on most species remains very limited, and the amount of data available varies greatly between taxonomic groups. We introduce the problem of predicting species occurrence patterns given (a) satellite imagery, and (b) known information on the occurrence of other species. To evaluate algorithms on this task, we introduce SatButterfly, a dataset of satellite images, environmental data and observational data for butterflies, which is designed to pair with the existing SatBird dataset of bird observational data. To address this task, we propose a general model, R-Tran, for predicting species occurrence patterns that enables the use of partial observational data wherever found. We find that R-Tran outperforms other methods in predicting species encounter rates with partial
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Mask Specific Language Modeling&#65288;MSLM&#65289;&#26041;&#27861;&#26469;&#25913;&#21892;LM&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#23545;&#30446;&#26631;&#39046;&#22495;&#30693;&#35782;&#30340;&#25935;&#24863;&#24615;&#65292;&#36890;&#36807;&#21152;&#26435;&#39046;&#22495;&#29305;&#23450;&#26415;&#35821;&#30340;&#37325;&#35201;&#24615;&#36827;&#34892;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2403.18025</link><description>&lt;p&gt;
&#36890;&#36807;&#29305;&#23450;&#25513;&#30721;&#25439;&#22833;&#25913;&#21892;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#25935;&#24863;&#24615;&#65306;&#20197;&#29983;&#29289;&#21307;&#23398;&#23454;&#20307;&#35782;&#21035;&#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
Improving Pre-trained Language Model Sensitivity via Mask Specific losses: A case study on Biomedical NER
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18025
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Mask Specific Language Modeling&#65288;MSLM&#65289;&#26041;&#27861;&#26469;&#25913;&#21892;LM&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#23545;&#30446;&#26631;&#39046;&#22495;&#30693;&#35782;&#30340;&#25935;&#24863;&#24615;&#65292;&#36890;&#36807;&#21152;&#26435;&#39046;&#22495;&#29305;&#23450;&#26415;&#35821;&#30340;&#37325;&#35201;&#24615;&#36827;&#34892;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#35843;&#25972;&#21040;&#26032;&#39046;&#22495;&#36890;&#24120;&#36890;&#36807;&#22312;&#29305;&#23450;&#39046;&#22495;&#25968;&#25454;&#19978;&#24494;&#35843;&#39044;&#35757;&#32451;LM&#65288;PLM&#65289;&#26469;&#23454;&#29616;&#12290;&#24494;&#35843;&#23558;&#26032;&#30693;&#35782;&#24341;&#20837;LM&#65292;&#20351;&#23427;&#33021;&#22815;&#29702;&#35299;&#21644;&#26377;&#25928;&#25191;&#34892;&#30446;&#26631;&#22495;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#24494;&#35843;&#21487;&#33021;&#20250;&#26080;&#24847;&#20013;&#21464;&#24471;&#19981;&#22815;&#25935;&#24863;&#65292;&#22914;&#26524;&#23427;&#24573;&#35270;&#20102;&#28304;&#22495;&#21644;&#30446;&#26631;&#22495;&#20043;&#38388;&#30340;&#24191;&#27867;&#24046;&#24322;&#65288;&#20363;&#22914;&#22312;&#35789;&#20041;&#19978;&#65289;&#12290;&#20026;&#20102;&#35299;&#20915;&#24494;&#35843;&#19981;&#25935;&#24863;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Mask Specific Language Modeling&#65288;MSLM&#65289;&#65292;&#19968;&#31181;&#36890;&#36807;&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#36866;&#24403;&#21152;&#26435;&#39046;&#22495;&#29305;&#23450;&#26415;&#35821;&#65288;DS-terms&#65289;&#30340;&#37325;&#35201;&#24615;&#26469;&#26377;&#25928;&#33719;&#21462;&#30446;&#26631;&#39046;&#22495;&#30693;&#35782;&#30340;&#26041;&#27861;&#12290;MSLM&#21516;&#26102;&#23631;&#34109;DS&#26415;&#35821;&#21644;&#36890;&#29992;&#35789;&#65292;&#28982;&#21518;&#36890;&#36807;&#30830;&#20445;LM&#21463;&#21040;&#26356;&#22823;&#24809;&#32602;&#26469;&#23398;&#20064;&#29305;&#23450;&#20110;&#25513;&#30721;&#30340;&#25439;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18025v1 Announce Type: cross  Abstract: Adapting language models (LMs) to novel domains is often achieved through fine-tuning a pre-trained LM (PLM) on domain-specific data. Fine-tuning introduces new knowledge into an LM, enabling it to comprehend and efficiently perform a target domain task. Fine-tuning can however be inadvertently insensitive if it ignores the wide array of disparities (e.g in word meaning) between source and target domains. For instance, words such as chronic and pressure may be treated lightly in social conversations, however, clinically, these words are usually an expression of concern. To address insensitive fine-tuning, we propose Mask Specific Language Modeling (MSLM), an approach that efficiently acquires target domain knowledge by appropriately weighting the importance of domain-specific terms (DS-terms) during fine-tuning. MSLM jointly masks DS-terms and generic words, then learns mask-specific losses by ensuring LMs incur larger penalties for in
&lt;/p&gt;</description></item><item><title>&#36880;&#23618;&#37325;&#35201;&#24615;&#37319;&#26679;&#30340;&#26032;&#26041;&#27861;LISA&#22312;&#24494;&#35843;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#35760;&#24518;&#25104;&#26412;&#20302;&#19988;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.17919</link><description>&lt;p&gt;
LISA&#65306;&#29992;&#20110;&#39640;&#25928;&#20869;&#23384;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#30340;&#36880;&#23618;&#37325;&#35201;&#24615;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17919
&lt;/p&gt;
&lt;p&gt;
&#36880;&#23618;&#37325;&#35201;&#24615;&#37319;&#26679;&#30340;&#26032;&#26041;&#27861;LISA&#22312;&#24494;&#35843;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#35760;&#24518;&#25104;&#26412;&#20302;&#19988;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#33258;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#39318;&#27425;&#20986;&#29616;&#20197;&#26469;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#36827;&#23637;&#65292;&#28982;&#32780;&#23427;&#20204;&#24040;&#22823;&#30340;&#20869;&#23384;&#28040;&#32791;&#24050;&#25104;&#20026;&#22823;&#35268;&#27169;&#35757;&#32451;&#30340;&#20027;&#35201;&#38556;&#30861;&#12290;&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#20102;&#35832;&#22914;&#20302;&#31209;&#35843;&#25972;&#65288;LoRA&#65289;&#20043;&#31867;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#25216;&#26415;&#26469;&#32531;&#35299;&#36825;&#19968;&#38382;&#39064;&#65292;&#20294;&#22312;&#22823;&#22810;&#25968;&#22823;&#35268;&#27169;&#24494;&#35843;&#35774;&#32622;&#20013;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#20173;&#26080;&#27861;&#19982;&#23436;&#25972;&#21442;&#25968;&#35757;&#32451;&#30456;&#21305;&#37197;&#12290;&#20026;&#24357;&#34917;&#36825;&#19968;&#19981;&#36275;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;LoRA&#22312;&#24494;&#35843;&#20219;&#21153;&#20013;&#30340;&#36880;&#23618;&#29305;&#24615;&#65292;&#24182;&#35266;&#23519;&#21040;&#19981;&#21516;&#23618;&#20043;&#38388;&#26435;&#37325;&#33539;&#25968;&#30340;&#24322;&#24120;&#20559;&#26012;&#12290;&#21033;&#29992;&#36825;&#19968;&#20851;&#38190;&#35266;&#23519;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#19968;&#20010;&#20196;&#20154;&#24778;&#35766;&#31616;&#21333;&#30340;&#35757;&#32451;&#31574;&#30053;&#65292;&#22312;&#35760;&#24518;&#25104;&#26412;&#20302;&#20110;LoRA&#30340;&#24773;&#20917;&#19979;&#65292;&#22312;&#24191;&#27867;&#30340;&#35774;&#32622;&#20013;&#20248;&#20110;LoRA&#21644;&#23436;&#25972;&#21442;&#25968;&#35757;&#32451;&#12290;&#25105;&#20204;&#23558;&#20854;&#21629;&#21517;&#20026;Layerwise Importance Sampled AdamW&#65288;LISA&#65289;&#65292;&#36825;&#26159;LoRA&#30340;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#24212;&#29992;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17919v1 Announce Type: cross  Abstract: The machine learning community has witnessed impressive advancements since the first appearance of large language models (LLMs), yet their huge memory consumption has become a major roadblock to large-scale training. Parameter Efficient Fine-Tuning techniques such as Low-Rank Adaptation (LoRA) have been proposed to alleviate this problem, but their performance still fails to match full parameter training in most large-scale fine-tuning settings. Attempting to complement this deficiency, we investigate layerwise properties of LoRA on fine-tuning tasks and observe an uncommon skewness of weight norms across different layers. Utilizing this key observation, a surprisingly simple training strategy is discovered, which outperforms both LoRA and full parameter training in a wide range of settings with memory costs as low as LoRA. We name it Layerwise Importance Sampled AdamW (LISA), a promising alternative for LoRA, which applies the idea of
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#24322;&#36136;&#20132;&#20114;&#35780;&#20998;&#32593;&#32476;&#65288;HIRE&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#24322;&#36136;&#20132;&#20114;&#27169;&#22359;&#65288;HIM&#65289;&#26469;&#20849;&#21516;&#24314;&#27169;&#24322;&#36136;&#20132;&#20114;&#24182;&#30452;&#25509;&#25512;&#26029;&#37325;&#35201;&#29305;&#24449;</title><link>https://arxiv.org/abs/2403.17740</link><description>&lt;p&gt;
&#19968;&#20307;&#21270;&#65306;&#24322;&#36136;&#20132;&#20114;&#24314;&#27169;&#29992;&#20110;&#20919;&#21551;&#21160;&#35780;&#20998;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
All-in-One: Heterogeneous Interaction Modeling for Cold-Start Rating Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17740
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#24322;&#36136;&#20132;&#20114;&#35780;&#20998;&#32593;&#32476;&#65288;HIRE&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#24322;&#36136;&#20132;&#20114;&#27169;&#22359;&#65288;HIM&#65289;&#26469;&#20849;&#21516;&#24314;&#27169;&#24322;&#36136;&#20132;&#20114;&#24182;&#30452;&#25509;&#25512;&#26029;&#37325;&#35201;&#29305;&#24449;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20919;&#21551;&#21160;&#35780;&#20998;&#39044;&#27979;&#26159;&#25512;&#33616;&#31995;&#32479;&#20013;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#65292;&#24050;&#24471;&#21040;&#24191;&#27867;&#30740;&#31350;&#12290;&#35768;&#22810;&#26041;&#27861;&#24050;&#32463;&#34987;&#25552;&#20986;&#65292;&#21033;&#29992;&#29616;&#26377;&#25968;&#25454;&#20043;&#38388;&#30340;&#26174;&#24335;&#20851;&#31995;&#65292;&#20363;&#22914;&#21327;&#21516;&#36807;&#28388;&#12289;&#31038;&#20132;&#25512;&#33616;&#21644;&#24322;&#26500;&#20449;&#24687;&#32593;&#32476;&#65292;&#20197;&#32531;&#35299;&#20919;&#21551;&#21160;&#29992;&#25143;&#21644;&#29289;&#21697;&#30340;&#25968;&#25454;&#19981;&#36275;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;&#19981;&#21516;&#35282;&#33394;&#20043;&#38388;&#30340;&#25968;&#25454;&#26500;&#24314;&#30340;&#26174;&#24335;&#20851;&#31995;&#21487;&#33021;&#19981;&#21487;&#38752;&#19988;&#26080;&#20851;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#29305;&#23450;&#25512;&#33616;&#20219;&#21153;&#30340;&#24615;&#33021;&#19978;&#38480;&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#28789;&#27963;&#30340;&#26694;&#26550;&#65292;&#21517;&#20026;&#24322;&#36136;&#20132;&#20114;&#35780;&#20998;&#32593;&#32476;&#65288;HIRE&#65289;&#12290;HIRE&#19981;&#20165;&#20165;&#20381;&#36182;&#20110;&#39044;&#20808;&#23450;&#20041;&#30340;&#20132;&#20114;&#27169;&#24335;&#25110;&#25163;&#21160;&#26500;&#24314;&#30340;&#24322;&#26500;&#20449;&#24687;&#32593;&#32476;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#24322;&#36136;&#20132;&#20114;&#27169;&#22359;&#65288;HIM&#65289;&#65292;&#26469;&#20849;&#21516;&#24314;&#27169;&#24322;&#36136;&#20132;&#20114;&#24182;&#30452;&#25509;&#25512;&#26029;&#37325;&#35201;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17740v1 Announce Type: cross  Abstract: Cold-start rating prediction is a fundamental problem in recommender systems that has been extensively studied. Many methods have been proposed that exploit explicit relations among existing data, such as collaborative filtering, social recommendations and heterogeneous information network, to alleviate the data insufficiency issue for cold-start users and items. However, the explicit relations constructed based on data between different roles may be unreliable and irrelevant, which limits the performance ceiling of the specific recommendation task. Motivated by this, in this paper, we propose a flexible framework dubbed heterogeneous interaction rating network (HIRE). HIRE dose not solely rely on the pre-defined interaction pattern or the manually constructed heterogeneous information network. Instead, we devise a Heterogeneous Interaction Module (HIM) to jointly model the heterogeneous interactions and directly infer the important in
&lt;/p&gt;</description></item><item><title>UADA3D&#26159;&#19968;&#31181;&#26080;&#30417;&#30563;&#23545;&#25239;&#39046;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;3D&#29289;&#20307;&#26816;&#27979;&#20013;&#22788;&#29702;&#31232;&#30095;LiDAR&#25968;&#25454;&#21644;&#22823;&#39046;&#22495;&#24046;&#36317;&#65292;&#24182;&#22312;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#21644;&#31227;&#21160;&#26426;&#22120;&#20154;&#39046;&#22495;&#20013;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2403.17633</link><description>&lt;p&gt;
UADA3D&#65306;&#38754;&#21521;&#31232;&#30095;LiDAR&#21644;&#22823;&#39046;&#22495;&#24046;&#36317;&#30340;&#26080;&#30417;&#30563;&#23545;&#25239;&#39046;&#22495;&#33258;&#36866;&#24212;&#22312;3D&#29289;&#20307;&#26816;&#27979;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
UADA3D: Unsupervised Adversarial Domain Adaptation for 3D Object Detection with Sparse LiDAR and Large Domain Gaps
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17633
&lt;/p&gt;
&lt;p&gt;
UADA3D&#26159;&#19968;&#31181;&#26080;&#30417;&#30563;&#23545;&#25239;&#39046;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;3D&#29289;&#20307;&#26816;&#27979;&#20013;&#22788;&#29702;&#31232;&#30095;LiDAR&#25968;&#25454;&#21644;&#22823;&#39046;&#22495;&#24046;&#36317;&#65292;&#24182;&#22312;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#21644;&#31227;&#21160;&#26426;&#22120;&#20154;&#39046;&#22495;&#20013;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#29616;&#26377;&#26080;&#30417;&#30563;&#39046;&#22495;&#36866;&#24212;&#26041;&#27861;&#22312;&#22522;&#20110;LiDAR&#30340;3D&#29289;&#20307;&#26816;&#27979;&#20013;&#30340;&#19968;&#20010;&#38382;&#39064;&#65292;&#36825;&#20123;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#36866;&#24212;&#24050;&#24314;&#31435;&#30340;&#39640;&#23494;&#24230;&#33258;&#21160;&#39550;&#39542;&#25968;&#25454;&#38598;&#20043;&#38388;&#30340;&#36716;&#21464;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#26356;&#31232;&#30095;&#30340;&#28857;&#20113;&#65292;&#25429;&#25417;&#26469;&#33258;&#19981;&#21516;&#35270;&#35282;&#30340;&#22330;&#26223;&#65306;&#19981;&#20165;&#26469;&#33258;&#36947;&#36335;&#19978;&#30340;&#36710;&#36742;&#65292;&#36824;&#26469;&#33258;&#20154;&#34892;&#36947;&#19978;&#30340;&#31227;&#21160;&#26426;&#22120;&#20154;&#65292;&#36973;&#36935;&#30528;&#26126;&#26174;&#19981;&#21516;&#30340;&#29615;&#22659;&#26465;&#20214;&#21644;&#20256;&#24863;&#22120;&#37197;&#32622;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#26080;&#30417;&#30563;&#23545;&#25239;&#39046;&#22495;&#33258;&#36866;&#24212;3D&#29289;&#20307;&#26816;&#27979;&#65288;UADA3D&#65289;&#12290;UADA3D&#19981;&#20381;&#36182;&#20110;&#39044;&#35757;&#32451;&#30340;&#28304;&#27169;&#22411;&#25110;&#24072;&#29983;&#26550;&#26500;&#12290;&#30456;&#21453;&#65292;&#23427;&#20351;&#29992;&#23545;&#25239;&#26041;&#27861;&#30452;&#25509;&#23398;&#20064;&#22495;&#19981;&#21464;&#29305;&#24449;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#23427;&#22312;&#21508;&#31181;&#36866;&#24212;&#22330;&#26223;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#22312;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#21644;&#31227;&#21160;&#26426;&#22120;&#20154;&#39046;&#22495;&#22343;&#26174;&#31034;&#20986;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#26159;&#24320;&#28304;&#30340;&#65292;&#24456;&#24555;&#23558;&#20250;&#25552;&#20379;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17633v1 Announce Type: cross  Abstract: In this study, we address a gap in existing unsupervised domain adaptation approaches on LiDAR-based 3D object detection, which have predominantly concentrated on adapting between established, high-density autonomous driving datasets. We focus on sparser point clouds, capturing scenarios from different perspectives: not just from vehicles on the road but also from mobile robots on sidewalks, which encounter significantly different environmental conditions and sensor configurations. We introduce Unsupervised Adversarial Domain Adaptation for 3D Object Detection (UADA3D). UADA3D does not depend on pre-trained source models or teacher-student architectures. Instead, it uses an adversarial approach to directly learn domain-invariant features. We demonstrate its efficacy in various adaptation scenarios, showing significant improvements in both self-driving car and mobile robot domains. Our code is open-source and will be available soon.
&lt;/p&gt;</description></item><item><title>&#35768;&#22810;AI&#29983;&#25104;&#22270;&#20687;&#26816;&#27979;&#25968;&#25454;&#38598;&#23384;&#22312;&#19982;JPEG&#21387;&#32553;&#21644;&#22270;&#20687;&#22823;&#23567;&#30456;&#20851;&#30340;&#20559;&#35265;&#65292;&#21435;&#38500;&#36825;&#20123;&#20559;&#35265;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#23545;JPEG&#21387;&#32553;&#30340;&#31283;&#20581;&#24615;&#24182;&#26174;&#33879;&#25913;&#21464;&#26816;&#27979;&#22120;&#30340;&#36328;&#29983;&#25104;&#22120;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.17608</link><description>&lt;p&gt;
&#20266;&#36896;&#36824;&#26159;JPEG&#65311;&#25581;&#31034;&#29983;&#25104;&#22270;&#20687;&#26816;&#27979;&#25968;&#25454;&#38598;&#20013;&#30340;&#24120;&#35265;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Fake or JPEG? Revealing Common Biases in Generated Image Detection Datasets
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17608
&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;AI&#29983;&#25104;&#22270;&#20687;&#26816;&#27979;&#25968;&#25454;&#38598;&#23384;&#22312;&#19982;JPEG&#21387;&#32553;&#21644;&#22270;&#20687;&#22823;&#23567;&#30456;&#20851;&#30340;&#20559;&#35265;&#65292;&#21435;&#38500;&#36825;&#20123;&#20559;&#35265;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#23545;JPEG&#21387;&#32553;&#30340;&#31283;&#20581;&#24615;&#24182;&#26174;&#33879;&#25913;&#21464;&#26816;&#27979;&#22120;&#30340;&#36328;&#29983;&#25104;&#22120;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#22270;&#20687;&#27169;&#22411;&#30340;&#24191;&#27867;&#24212;&#29992;&#20984;&#26174;&#20102;&#26816;&#27979;&#20154;&#36896;&#20869;&#23481;&#30340;&#36843;&#20999;&#38656;&#27714;&#65292;&#36825;&#26159;&#25171;&#20987;&#24191;&#27867;&#25805;&#32437;&#21644;&#35823;&#23548;&#30340;&#20851;&#38190;&#19968;&#27493;&#12290;&#22240;&#27492;&#65292;&#35768;&#22810;&#26816;&#27979;&#22120;&#21644;&#30456;&#20851;&#25968;&#25454;&#38598;&#24050;&#32463;&#20986;&#29616;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#36825;&#20123;&#25968;&#25454;&#38598;&#19981;&#32463;&#24847;&#22320;&#24341;&#20837;&#20102;&#19981;&#33391;&#20559;&#35265;&#65292;&#20174;&#32780;&#24433;&#21709;&#20102;&#26816;&#27979;&#22120;&#30340;&#25928;&#26524;&#21644;&#35780;&#20272;&#12290;&#26412;&#25991;&#24378;&#35843;&#20102;&#35768;&#22810;&#29992;&#20110;AI&#29983;&#25104;&#22270;&#20687;&#26816;&#27979;&#30340;&#25968;&#25454;&#38598;&#21253;&#21547;&#19982;JPEG&#21387;&#32553;&#21644;&#22270;&#20687;&#22823;&#23567;&#26377;&#20851;&#30340;&#20559;&#35265;&#12290;&#20351;&#29992;GenImage&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#35777;&#26126;&#26816;&#27979;&#22120;&#30830;&#23454;&#20174;&#36825;&#20123;&#19981;&#21463;&#27426;&#36814;&#30340;&#22240;&#32032;&#20013;&#23398;&#20064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#21435;&#38500;&#36825;&#20123;&#21629;&#21517;&#20559;&#35265;&#20250;&#26174;&#33879;&#22686;&#21152;&#38024;&#23545;JPEG&#21387;&#32553;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#26174;&#33879;&#25913;&#21464;&#35780;&#20272;&#26816;&#27979;&#22120;&#30340;&#36328;&#29983;&#25104;&#22120;&#24615;&#33021;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#23545;&#20110;ResNet50&#21644;S
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17608v1 Announce Type: cross  Abstract: The widespread adoption of generative image models has highlighted the urgent need to detect artificial content, which is a crucial step in combating widespread manipulation and misinformation. Consequently, numerous detectors and associated datasets have emerged. However, many of these datasets inadvertently introduce undesirable biases, thereby impacting the effectiveness and evaluation of detectors. In this paper, we emphasize that many datasets for AI-generated image detection contain biases related to JPEG compression and image size. Using the GenImage dataset, we demonstrate that detectors indeed learn from these undesired factors. Furthermore, we show that removing the named biases substantially increases robustness to JPEG compression and significantly alters the cross-generator performance of evaluated detectors. Specifically, it leads to more than 11 percentage points increase in cross-generator performance for ResNet50 and S
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;CADGL&#26694;&#26550;&#65292;&#21033;&#29992;&#19978;&#19979;&#25991;&#24863;&#30693;&#28145;&#24230;&#22270;&#23398;&#20064;&#26469;&#39044;&#27979;&#33647;&#29289;-&#33647;&#29289;&#30456;&#20114;&#20316;&#29992;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;DDI&#39044;&#27979;&#27169;&#22411;&#22312;&#27867;&#21270;&#12289;&#29305;&#24449;&#25552;&#21462;&#21644;&#29616;&#23454;&#24212;&#29992;&#26041;&#38754;&#30340;&#25361;&#25112;</title><link>https://arxiv.org/abs/2403.17210</link><description>&lt;p&gt;
CADGL: &#19978;&#19979;&#25991;&#24863;&#30693;&#28145;&#24230;&#22270;&#23398;&#20064;&#29992;&#20110;&#39044;&#27979;&#33647;&#29289;-&#33647;&#29289;&#30456;&#20114;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
CADGL: Context-Aware Deep Graph Learning for Predicting Drug-Drug Interactions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17210
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;CADGL&#26694;&#26550;&#65292;&#21033;&#29992;&#19978;&#19979;&#25991;&#24863;&#30693;&#28145;&#24230;&#22270;&#23398;&#20064;&#26469;&#39044;&#27979;&#33647;&#29289;-&#33647;&#29289;&#30456;&#20114;&#20316;&#29992;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;DDI&#39044;&#27979;&#27169;&#22411;&#22312;&#27867;&#21270;&#12289;&#29305;&#24449;&#25552;&#21462;&#21644;&#29616;&#23454;&#24212;&#29992;&#26041;&#38754;&#30340;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33647;&#29289;-&#33647;&#29289;&#30456;&#20114;&#20316;&#29992;&#65288;DDIs&#65289;&#30340;&#30740;&#31350;&#26159;&#33647;&#29289;&#24320;&#21457;&#36807;&#31243;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#20803;&#32032;&#12290;DDIs&#21457;&#29983;&#22312;&#19968;&#20010;&#33647;&#29289;&#30340;&#24615;&#36136;&#21463;&#20854;&#20182;&#33647;&#29289;&#21253;&#21547;&#30340;&#24433;&#21709;&#26102;&#12290;&#26816;&#27979;&#26377;&#21033;&#30340;DDIs&#26377;&#21487;&#33021;&#20026;&#22312;&#23454;&#38469;&#35774;&#32622;&#20013;&#24212;&#29992;&#30340;&#21019;&#26032;&#33647;&#29289;&#30340;&#21019;&#36896;&#21644;&#25512;&#36827;&#38138;&#24179;&#36947;&#36335;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;DDI&#39044;&#27979;&#27169;&#22411;&#22312;&#26497;&#31471;&#24773;&#20917;&#19979;&#30340;&#27867;&#21270;&#12289;&#31283;&#20581;&#29305;&#24449;&#25552;&#21462;&#20197;&#21450;&#29616;&#23454;&#24212;&#29992;&#21487;&#33021;&#24615;&#26041;&#38754;&#25345;&#32493;&#38754;&#20020;&#25361;&#25112;&#12290;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#21033;&#29992;&#19978;&#19979;&#25991;&#24863;&#30693;&#28145;&#24230;&#22270;&#23398;&#20064;&#30340;&#26377;&#25928;&#24615;&#65292;&#24341;&#20837;&#19968;&#31181;&#21517;&#20026;CADGL&#30340;&#26032;&#39062;&#26694;&#26550;&#26469;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#12290;&#22522;&#20110;&#23450;&#21046;&#30340;&#21464;&#20998;&#22270;&#33258;&#32534;&#30721;&#22120;&#65288;VGAE&#65289;&#65292;&#25105;&#20204;&#21033;&#29992;&#20004;&#20010;&#19978;&#19979;&#25991;&#39044;&#22788;&#29702;&#22120;&#20174;&#20004;&#20010;&#19981;&#21516;&#35270;&#35282;&#65306;&#23616;&#37096;&#37051;&#22495;&#21644;&#20998;&#23376;&#19978;&#19979;&#25991;&#65292;&#22312;&#24322;&#36136;&#22270;&#32467;&#26500;&#20013;&#25552;&#21462;&#29305;&#24449;&#65292;&#25429;&#33719;&#20851;&#38190;&#30340;&#32467;&#26500;&#21644;&#29983;&#29702;&#21270;&#23398;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17210v1 Announce Type: cross  Abstract: Examining Drug-Drug Interactions (DDIs) is a pivotal element in the process of drug development. DDIs occur when one drug's properties are affected by the inclusion of other drugs. Detecting favorable DDIs has the potential to pave the way for creating and advancing innovative medications applicable in practical settings. However, existing DDI prediction models continue to face challenges related to generalization in extreme cases, robust feature extraction, and real-life application possibilities. We aim to address these challenges by leveraging the effectiveness of context-aware deep graph learning by introducing a novel framework named CADGL. Based on a customized variational graph autoencoder (VGAE), we capture critical structural and physio-chemical information using two context preprocessors for feature extraction from two different perspectives: local neighborhood and molecular context, in a heterogeneous graphical structure. Ou
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25506;&#35752;&#20102;&#26412;&#22320;&#24046;&#20998;&#38544;&#31169;&#12289;&#36125;&#21494;&#26031;&#38544;&#31169;&#21450;&#20854;&#20043;&#38388;&#30340;&#30456;&#20114;&#20851;&#31995;&#65292;&#25581;&#31034;&#20102;&#20851;&#20110;&#25928;&#29992;-&#38544;&#31169;&#26435;&#34913;&#30340;&#26032;&#35265;&#35299;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#31361;&#20986;&#25915;&#20987;&#21644;&#38450;&#24481;&#31574;&#30053;&#30340;&#30456;&#20114;&#20316;&#29992;&#21644;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.16591</link><description>&lt;p&gt;
&#25581;&#31034;&#26412;&#22320;&#24046;&#20998;&#38544;&#31169;&#12289;&#24179;&#22343;&#36125;&#21494;&#26031;&#38544;&#31169;&#21644;&#26368;&#22823;&#36125;&#21494;&#26031;&#38544;&#31169;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Deciphering the Interplay between Local Differential Privacy, Average Bayesian Privacy, and Maximum Bayesian Privacy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16591
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25506;&#35752;&#20102;&#26412;&#22320;&#24046;&#20998;&#38544;&#31169;&#12289;&#36125;&#21494;&#26031;&#38544;&#31169;&#21450;&#20854;&#20043;&#38388;&#30340;&#30456;&#20114;&#20851;&#31995;&#65292;&#25581;&#31034;&#20102;&#20851;&#20110;&#25928;&#29992;-&#38544;&#31169;&#26435;&#34913;&#30340;&#26032;&#35265;&#35299;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#31361;&#20986;&#25915;&#20987;&#21644;&#38450;&#24481;&#31574;&#30053;&#30340;&#30456;&#20114;&#20316;&#29992;&#21644;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#30340;&#36805;&#36895;&#21457;&#23637;&#23548;&#33268;&#20102;&#38544;&#31169;&#23450;&#20041;&#30340;&#22810;&#26679;&#21270;&#65292;&#30001;&#20110;&#23545;&#38544;&#31169;&#26500;&#25104;&#30340;&#23041;&#32961;&#65292;&#21253;&#25324;&#26412;&#22320;&#24046;&#20998;&#38544;&#31169;&#65288;LDP&#65289;&#30340;&#27010;&#24565;&#12290;&#34429;&#28982;&#34987;&#24191;&#27867;&#25509;&#21463;&#24182;&#22312;&#35768;&#22810;&#39046;&#22495;&#20013;&#34987;&#21033;&#29992;&#65292;&#20294;&#36825;&#31181;&#20256;&#32479;&#30340;&#38544;&#31169;&#27979;&#37327;&#26041;&#27861;&#20173;&#28982;&#23384;&#22312;&#19968;&#23450;&#38480;&#21046;&#65292;&#20174;&#26080;&#27861;&#38450;&#27490;&#25512;&#26029;&#25259;&#38706;&#21040;&#32570;&#20047;&#23545;&#23545;&#25163;&#32972;&#26223;&#30693;&#35782;&#30340;&#32771;&#34385;&#12290;&#22312;&#36825;&#39033;&#20840;&#38754;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#36125;&#21494;&#26031;&#38544;&#31169;&#24182;&#28145;&#20837;&#25506;&#35752;&#26412;&#22320;&#24046;&#20998;&#38544;&#31169;&#21644;&#20854;&#36125;&#21494;&#26031;&#23545;&#24212;&#29289;&#20043;&#38388;&#38169;&#32508;&#22797;&#26434;&#30340;&#20851;&#31995;&#65292;&#25581;&#31034;&#20102;&#20851;&#20110;&#25928;&#29992;-&#38544;&#31169;&#26435;&#34913;&#30340;&#26032;&#35265;&#35299;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#27010;&#25324;&#20102;&#25915;&#20987;&#21644;&#38450;&#24481;&#31574;&#30053;&#65292;&#31361;&#20986;&#23427;&#20204;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#21644;&#25928;&#26524;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#36129;&#29486;&#22522;&#20110;&#24179;&#22343;&#36125;&#21494;&#26031;&#38544;&#31169;&#65288;ABP&#65289;&#21644;&#26368;&#22823;&#36125;&#21494;&#26031;&#38544;&#31169;&#20043;&#38388;&#30340;&#20005;&#26684;&#23450;&#20041;&#21644;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16591v1 Announce Type: cross  Abstract: The swift evolution of machine learning has led to emergence of various definitions of privacy due to the threats it poses to privacy, including the concept of local differential privacy (LDP). Although widely embraced and utilized across numerous domains, this conventional approach to measure privacy still exhibits certain limitations, spanning from failure to prevent inferential disclosure to lack of consideration for the adversary's background knowledge. In this comprehensive study, we introduce Bayesian privacy and delve into the intricate relationship between local differential privacy and its Bayesian counterparts, unveiling novel insights into utility-privacy trade-offs. We introduce a framework that encapsulates both attack and defense strategies, highlighting their interplay and effectiveness. Our theoretical contributions are anchored in the rigorous definitions and relationships between Average Bayesian Privacy (ABP) and Max
&lt;/p&gt;</description></item><item><title>DeepMachining&#26159;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;AI&#31995;&#32479;&#65292;&#21487;&#20197;&#22312;&#32447;&#39044;&#27979;&#36710;&#24202;&#26426;&#24202;&#21152;&#24037;&#25805;&#20316;&#30340;&#35823;&#24046;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#39640;&#20934;&#30830;&#24615;&#39044;&#27979;&#65292;&#26159;&#39318;&#25209;&#20351;&#29992;&#39044;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#36710;&#24202;&#26426;&#24202;&#21152;&#24037;&#35823;&#24046;&#30340;&#24037;&#21378;&#23454;&#39564;&#20043;&#19968;&#12290;</title><link>https://arxiv.org/abs/2403.16451</link><description>&lt;p&gt;
DeepMachining: &#38115;&#24202;&#26426;&#24202;&#21152;&#24037;&#35823;&#24046;&#22312;&#32447;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
DeepMachining: Online Prediction of Machining Errors of Lathe Machines
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16451
&lt;/p&gt;
&lt;p&gt;
DeepMachining&#26159;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;AI&#31995;&#32479;&#65292;&#21487;&#20197;&#22312;&#32447;&#39044;&#27979;&#36710;&#24202;&#26426;&#24202;&#21152;&#24037;&#25805;&#20316;&#30340;&#35823;&#24046;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#39640;&#20934;&#30830;&#24615;&#39044;&#27979;&#65292;&#26159;&#39318;&#25209;&#20351;&#29992;&#39044;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#36710;&#24202;&#26426;&#24202;&#21152;&#24037;&#35823;&#24046;&#30340;&#24037;&#21378;&#23454;&#39564;&#20043;&#19968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25551;&#36848;&#20102;DeepMachining&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#65292;&#29992;&#20110;&#22312;&#32447;&#39044;&#27979;&#36710;&#24202;&#21152;&#24037;&#25805;&#20316;&#30340;&#21152;&#24037;&#35823;&#24046;&#12290;&#25105;&#20204;&#22522;&#20110;&#24037;&#21378;&#30340;&#21046;&#36896;&#25968;&#25454;&#26500;&#24314;&#24182;&#35780;&#20272;&#20102;DeepMachining&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#39318;&#20808;&#23545;&#29305;&#23450;&#36710;&#24202;&#26426;&#24202;&#25805;&#20316;&#39044;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#20197;&#23398;&#20064;&#21152;&#24037;&#29366;&#24577;&#30340;&#26174;&#33879;&#29305;&#24449;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24494;&#35843;&#39044;&#35757;&#32451;&#27169;&#22411;&#20197;&#36866;&#24212;&#29305;&#23450;&#21152;&#24037;&#20219;&#21153;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;DeepMachining&#22312;&#28041;&#21450;&#19981;&#21516;&#24037;&#20214;&#21644;&#20992;&#20855;&#30340;&#22810;&#20010;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#39033;&#24037;&#20316;&#26159;&#20351;&#29992;&#39044;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#36710;&#24202;&#26426;&#24202;&#21152;&#24037;&#35823;&#24046;&#30340;&#39318;&#25209;&#24037;&#21378;&#23454;&#39564;&#20043;&#19968;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16451v1 Announce Type: cross  Abstract: We describe DeepMachining, a deep learning-based AI system for online prediction of machining errors of lathe machine operations. We have built and evaluated DeepMachining based on manufacturing data from factories. Specifically, we first pretrain a deep learning model for a given lathe machine's operations to learn the salient features of machining states. Then, we fine-tune the pretrained model to adapt to specific machining tasks. We demonstrate that DeepMachining achieves high prediction accuracy for multiple tasks that involve different workpieces and cutting tools. To the best of our knowledge, this work is one of the first factory experiments using pre-trained deep-learning models to predict machining errors of lathe machines.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20219;&#21153;&#65292;&#21363;&#36923;&#36753;&#20195;&#30721;&#27169;&#25311;&#65292;&#36843;&#20351;LLMs&#22312;&#39044;&#27979;&#36923;&#36753;&#31243;&#24207;&#30340;&#32467;&#26524;&#26102;&#27169;&#25311;&#36923;&#36753;&#27714;&#35299;&#22120;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19977;&#20010;&#30740;&#31350;&#38382;&#39064;&#20197;&#28145;&#20837;&#35843;&#26597;&#36825;&#19968;&#20219;&#21153;&#23545;LLMs&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2403.16097</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#27169;&#25311;&#27714;&#35299;&#22120;&#21527;&#65311;LLMs&#30340;&#36923;&#36753;&#20195;&#30721;&#27169;&#25311;
&lt;/p&gt;
&lt;p&gt;
Can Language Models Pretend Solvers? Logic Code Simulation with LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16097
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20219;&#21153;&#65292;&#21363;&#36923;&#36753;&#20195;&#30721;&#27169;&#25311;&#65292;&#36843;&#20351;LLMs&#22312;&#39044;&#27979;&#36923;&#36753;&#31243;&#24207;&#30340;&#32467;&#26524;&#26102;&#27169;&#25311;&#36923;&#36753;&#27714;&#35299;&#22120;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19977;&#20010;&#30740;&#31350;&#38382;&#39064;&#20197;&#28145;&#20837;&#35843;&#26597;&#36825;&#19968;&#20219;&#21153;&#23545;LLMs&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#35299;&#20915;&#36923;&#36753;&#38382;&#39064;&#26041;&#38754;&#23637;&#31034;&#20102;&#37325;&#35201;&#28508;&#21147;&#12290;&#21033;&#29992;LLMs&#22312;&#20195;&#30721;&#30456;&#20851;&#27963;&#21160;&#20013;&#30340;&#21331;&#36234;&#33021;&#21147;&#65292;&#26368;&#36817;&#25552;&#20986;&#20102;&#20960;&#31181;&#21033;&#29992;&#36923;&#36753;&#27714;&#35299;&#22120;&#36827;&#34892;&#36923;&#36753;&#25512;&#29702;&#30340;&#26694;&#26550;&#12290;&#34429;&#28982;&#29616;&#26377;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#23558;LLMs&#35270;&#20026;&#33258;&#28982;&#35821;&#35328;&#36923;&#36753;&#27714;&#35299;&#22120;&#25110;&#32763;&#35793;&#22120;&#65292;&#20294;&#23427;&#20204;&#20316;&#20026;&#36923;&#36753;&#20195;&#30721;&#35299;&#37322;&#22120;&#21644;&#25191;&#34892;&#22120;&#30340;&#35282;&#33394;&#21463;&#21040;&#20102;&#36739;&#23569;&#20851;&#27880;&#12290;&#26412;&#30740;&#31350;&#28145;&#20837;&#25506;&#35752;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26041;&#38754;&#65292;&#21363;&#36923;&#36753;&#20195;&#30721;&#27169;&#25311;&#65292;&#23427;&#36843;&#20351;LLMs&#22312;&#39044;&#27979;&#36923;&#36753;&#31243;&#24207;&#30340;&#32467;&#26524;&#26102;&#27169;&#25311;&#36923;&#36753;&#27714;&#35299;&#22120;&#12290;&#20026;&#36827;&#19968;&#27493;&#30740;&#31350;&#36825;&#19968;&#26032;&#39062;&#20219;&#21153;&#65292;&#25105;&#20204;&#21046;&#23450;&#20102;&#19977;&#20010;&#30740;&#31350;&#38382;&#39064;&#65306;LLMs&#33021;&#21542;&#26377;&#25928;&#22320;&#27169;&#25311;&#36923;&#36753;&#20195;&#30721;&#30340;&#36755;&#20986;&#65311;&#36923;&#36753;&#20195;&#30721;&#27169;&#25311;&#20276;&#38543;&#30528;&#21738;&#20123;&#20248;&#21183;&#65311;&#20197;&#21450;&#23384;&#22312;&#21738;&#20123;&#32570;&#38519;&#65311;&#20026;&#20102;&#22238;&#31572;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25972;&#29702;&#20102;&#19977;&#20010;&#38024;&#23545;&#36923;&#36753;&#20195;&#30721;&#27169;&#25311;&#30340;&#26032;&#39062;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16097v1 Announce Type: new  Abstract: Transformer-based large language models (LLMs) have demonstrated significant potential in addressing logic problems. capitalizing on the great capabilities of LLMs for code-related activities, several frameworks leveraging logical solvers for logic reasoning have been proposed recently. While existing research predominantly focuses on viewing LLMs as natural language logic solvers or translators, their roles as logic code interpreters and executors have received limited attention. This study delves into a novel aspect, namely logic code simulation, which forces LLMs to emulate logical solvers in predicting the results of logical programs. To further investigate this novel task, we formulate our three research questions: Can LLMs efficiently simulate the outputs of logic codes? What strength arises along with logic code simulation? And what pitfalls? To address these inquiries, we curate three novel datasets tailored for the logic code si
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36879;&#26126;&#27700;&#21360;&#26816;&#27979;&#30340;&#40657;&#30418;&#26041;&#27861;WMD&#65292;&#22312;&#26080;&#27880;&#37322;&#35774;&#32622;&#19979;&#65292;&#21033;&#29992;&#24178;&#20928;&#26080;&#27700;&#21360;&#25968;&#25454;&#38598;&#26816;&#27979;&#20219;&#24847;&#27700;&#21360;&#65292;&#25928;&#26524;&#26174;&#33879;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;</title><link>https://arxiv.org/abs/2403.15955</link><description>&lt;p&gt;
&#22312;&#24178;&#33609;&#22534;&#20013;&#23547;&#25214;&#38024;: &#19968;&#31181;&#36879;&#26126;&#27700;&#21360;&#26816;&#27979;&#30340;&#40657;&#30418;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Finding needles in a haystack: A Black-Box Approach to Invisible Watermark Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15955
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36879;&#26126;&#27700;&#21360;&#26816;&#27979;&#30340;&#40657;&#30418;&#26041;&#27861;WMD&#65292;&#22312;&#26080;&#27880;&#37322;&#35774;&#32622;&#19979;&#65292;&#21033;&#29992;&#24178;&#20928;&#26080;&#27700;&#21360;&#25968;&#25454;&#38598;&#26816;&#27979;&#20219;&#24847;&#27700;&#21360;&#65292;&#25928;&#26524;&#26174;&#33879;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;WaterMark Detection&#65288;WMD&#65289;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#22312;&#40657;&#30418;&#21644;&#26080;&#27880;&#37322;&#35774;&#32622;&#19979;&#36827;&#34892;&#36879;&#26126;&#27700;&#21360;&#26816;&#27979;&#30340;&#26041;&#27861;&#12290;WMD&#33021;&#22815;&#21033;&#29992;&#19968;&#20010;&#24178;&#20928;&#30340;&#26080;&#27700;&#21360;&#25968;&#25454;&#38598;&#20316;&#20026;&#21442;&#32771;&#65292;&#22312;&#19981;&#20381;&#36182;&#29305;&#23450;&#35299;&#30721;&#26041;&#27861;&#25110;&#23545;&#27700;&#21360;&#25216;&#26415;&#30340;&#20107;&#20808;&#20102;&#35299;&#30340;&#24773;&#20917;&#19979;&#65292;&#26816;&#27979;&#32473;&#23450;&#21442;&#32771;&#25968;&#25454;&#38598;&#20013;&#30340;&#20219;&#24847;&#27700;&#21360;&#12290;&#25105;&#20204;&#20351;&#29992;&#20559;&#31227;&#23398;&#20064;&#30340;&#22522;&#30784;&#24320;&#21457;&#20102;WMD&#65292;&#24178;&#20928;&#30340;&#26080;&#27700;&#21360;&#25968;&#25454;&#38598;&#20351;&#25105;&#20204;&#33021;&#22815;&#20165;&#20998;&#31163;&#20986;&#21442;&#32771;&#25968;&#25454;&#38598;&#20013;&#24102;&#27700;&#21360;&#26679;&#26412;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35780;&#20272;&#65292;&#35777;&#26126;&#20102;WMD&#30340;&#26377;&#25928;&#24615;&#65292;&#26126;&#26174;&#20248;&#20110;&#20165;&#20135;&#29983;&#32422;0.5&#30340;AUC&#24471;&#20998;&#30340;&#31616;&#21333;&#26816;&#27979;&#26041;&#27861;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;WMD&#22312;&#22823;&#22810;&#25968;&#21333;&#27700;&#21360;&#25968;&#25454;&#38598;&#20013;&#25345;&#32493;&#33719;&#24471;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#26816;&#27979;AUC&#24471;&#20998;&#65292;&#36229;&#36807;0.9&#65292;&#24182;&#22312;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#22810;&#27700;&#21360;&#22330;&#26223;&#20013;&#30340;&#21508;&#31181;&#25968;&#25454;&#38598;&#21644;&#27700;&#21360;&#26041;&#27861;&#20013;&#36229;&#36807;0.7&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15955v1 Announce Type: cross  Abstract: In this paper, we propose WaterMark Detection (WMD), the first invisible watermark detection method under a black-box and annotation-free setting. WMD is capable of detecting arbitrary watermarks within a given reference dataset using a clean non-watermarked dataset as a reference, without relying on specific decoding methods or prior knowledge of the watermarking techniques. We develop WMD using foundations of offset learning, where a clean non-watermarked dataset enables us to isolate the influence of only watermarked samples in the reference dataset. Our comprehensive evaluations demonstrate the effectiveness of WMD, significantly outperforming naive detection methods, which only yield AUC scores around 0.5. In contrast, WMD consistently achieves impressive detection AUC scores, surpassing 0.9 in most single-watermark datasets and exceeding 0.7 in more challenging multi-watermark scenarios across diverse datasets and watermarking me
&lt;/p&gt;</description></item><item><title>&#36825;&#37324;&#26159;&#20013;&#25991;&#24635;&#32467;&#20986;&#30340;&#19968;&#21477;&#35805;&#35201;&#28857;: &#35813;&#35770;&#25991;&#25552;&#20986;&#20102;X-Portrait&#65292;&#19968;&#31181;&#29992;&#20110;&#29983;&#25104;&#20855;&#26377;&#34920;&#29616;&#21147;&#21644;&#26102;&#38388;&#36830;&#36143;&#24615;&#30340;&#32918;&#20687;&#21160;&#30011;&#30340;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#65292;&#21033;&#29992;&#25511;&#21046;&#20449;&#21495;&#23454;&#29616;&#20102;&#32454;&#31890;&#24230;&#22836;&#37096;&#23039;&#21183;&#21644;&#34920;&#24773;&#25511;&#21046;&#65292;&#20197;&#25552;&#39640;&#36816;&#21160;&#31934;&#24230;&#12290;</title><link>https://arxiv.org/abs/2403.15931</link><description>&lt;p&gt;
X-Portrait: &#20855;&#26377;&#20998;&#23618;&#21160;&#20316;&#27880;&#24847;&#21147;&#30340;&#34920;&#29616;&#24615;&#32918;&#20687;&#21160;&#30011;
&lt;/p&gt;
&lt;p&gt;
X-Portrait: Expressive Portrait Animation with Hierarchical Motion Attention
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15931
&lt;/p&gt;
&lt;p&gt;
&#36825;&#37324;&#26159;&#20013;&#25991;&#24635;&#32467;&#20986;&#30340;&#19968;&#21477;&#35805;&#35201;&#28857;: &#35813;&#35770;&#25991;&#25552;&#20986;&#20102;X-Portrait&#65292;&#19968;&#31181;&#29992;&#20110;&#29983;&#25104;&#20855;&#26377;&#34920;&#29616;&#21147;&#21644;&#26102;&#38388;&#36830;&#36143;&#24615;&#30340;&#32918;&#20687;&#21160;&#30011;&#30340;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#65292;&#21033;&#29992;&#25511;&#21046;&#20449;&#21495;&#23454;&#29616;&#20102;&#32454;&#31890;&#24230;&#22836;&#37096;&#23039;&#21183;&#21644;&#34920;&#24773;&#25511;&#21046;&#65292;&#20197;&#25552;&#39640;&#36816;&#21160;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;X-Portrait&#65292;&#36825;&#26159;&#19968;&#31181;&#21019;&#26032;&#30340;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#65292;&#19987;&#38376;&#29992;&#20110;&#29983;&#25104;&#20855;&#26377;&#34920;&#29616;&#21147;&#21644;&#26102;&#38388;&#36830;&#36143;&#24615;&#30340;&#32918;&#20687;&#21160;&#30011;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#26088;&#22312;&#22522;&#20110;&#21333;&#20010;&#32918;&#20687;&#20316;&#20026;&#22806;&#35266;&#21442;&#32771;&#65292;&#24182;&#21033;&#29992;&#26469;&#33258;&#39537;&#21160;&#35270;&#39057;&#30340;&#36816;&#21160;&#26469;&#20026;&#20854;&#28155;&#21152;&#21160;&#30011;&#65292;&#25429;&#25417;&#20855;&#26377;&#39640;&#24230;&#21160;&#24577;&#24615;&#21644;&#24494;&#22937;&#38754;&#37096;&#34920;&#24773;&#20197;&#21450;&#24191;&#27867;&#33539;&#22260;&#22836;&#37096;&#36816;&#21160;&#12290;&#22312;&#20854;&#26680;&#24515;&#37096;&#20998;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#39044;&#20808;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#30340;&#29983;&#25104;&#20808;&#39564;&#20316;&#20026;&#28210;&#26579;&#39592;&#26550;&#65292;&#21516;&#26102;&#22312;ControlNet&#26694;&#26550;&#20869;&#36890;&#36807;&#26032;&#39062;&#30340;&#25511;&#21046;&#20449;&#21495;&#23454;&#29616;&#20102;&#32454;&#31890;&#24230;&#22836;&#37096;&#23039;&#21183;&#21644;&#34920;&#24773;&#25511;&#21046;&#12290;&#19982;&#20256;&#32479;&#30340;&#31895;&#31961;&#26174;&#24335;&#25511;&#21046;&#65288;&#22914;&#38754;&#37096;&#26631;&#24535;&#28857;&#65289;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#36816;&#21160;&#25511;&#21046;&#27169;&#22359;&#23398;&#20250;&#30452;&#25509;&#20174;&#21407;&#22987;&#39537;&#21160;RGB&#36755;&#20837;&#20013;&#35299;&#35835;&#21160;&#24577;&#12290;&#36890;&#36807;&#26377;&#25928;&#22686;&#24378;&#23545;&#30524;&#31070;&#31561;&#23567;&#23610;&#24230;&#32454;&#24494;&#24046;&#24322;&#30340;&#36816;&#21160;&#20851;&#27880;&#30340;&#22522;&#20110;&#34917;&#19969;&#30340;&#23616;&#37096;&#25511;&#21046;&#27169;&#22359;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#36816;&#21160;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15931v1 Announce Type: cross  Abstract: We propose X-Portrait, an innovative conditional diffusion model tailored for generating expressive and temporally coherent portrait animation. Specifically, given a single portrait as appearance reference, we aim to animate it with motion derived from a driving video, capturing both highly dynamic and subtle facial expressions along with wide-range head movements. As its core, we leverage the generative prior of a pre-trained diffusion model as the rendering backbone, while achieve fine-grained head pose and expression control with novel controlling signals within the framework of ControlNet. In contrast to conventional coarse explicit controls such as facial landmarks, our motion control module is learned to interpret the dynamics directly from the original driving RGB inputs. The motion accuracy is further enhanced with a patch-based local control module that effectively enhance the motion attention to small-scale nuances like eyeba
&lt;/p&gt;</description></item><item><title>WoLF&#26694;&#26550;&#25552;&#20986;&#20102;&#23545;&#20110;CXR&#30340;&#20840;&#38754;&#29702;&#35299;&#30340;&#25913;&#36827;&#65292;&#21253;&#25324;&#20351;&#29992;&#39069;&#22806;&#30340;&#20581;&#24247;&#30456;&#20851;&#25968;&#25454;&#12289;&#37325;&#26500;&#25253;&#21578;&#20197;&#25552;&#20379;&#26356;&#26377;&#32452;&#32455;&#30340;&#20449;&#24687;&#12289;&#20197;&#21450;&#25913;&#36827;&#29983;&#25104;&#31572;&#26696;&#30340;&#32454;&#33268;&#35780;&#20272;&#12290;</title><link>https://arxiv.org/abs/2403.15456</link><description>&lt;p&gt;
WoLF: &#29992;&#20110;&#33016;&#37096;X&#32447;&#22270;&#29702;&#35299;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
WoLF: Large Language Model Framework for CXR Understanding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15456
&lt;/p&gt;
&lt;p&gt;
WoLF&#26694;&#26550;&#25552;&#20986;&#20102;&#23545;&#20110;CXR&#30340;&#20840;&#38754;&#29702;&#35299;&#30340;&#25913;&#36827;&#65292;&#21253;&#25324;&#20351;&#29992;&#39069;&#22806;&#30340;&#20581;&#24247;&#30456;&#20851;&#25968;&#25454;&#12289;&#37325;&#26500;&#25253;&#21578;&#20197;&#25552;&#20379;&#26356;&#26377;&#32452;&#32455;&#30340;&#20449;&#24687;&#12289;&#20197;&#21450;&#25913;&#36827;&#29983;&#25104;&#31572;&#26696;&#30340;&#32454;&#33268;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#29616;&#20195;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;(VLMs)&#21462;&#24471;&#20102;&#23545;&#33016;&#37096;X&#32447;&#22270;(CXR)&#29702;&#35299;&#26041;&#38754;&#30340;&#26174;&#30528;&#26041;&#27861;&#36827;&#23637;&#65292;&#23637;&#31034;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#35270;&#35273;&#38382;&#31572;(VQA)&#21644;CXR&#25253;&#21578;&#29983;&#25104;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;CXR&#29702;&#35299;&#26694;&#26550;&#20173;&#23384;&#22312;&#20960;&#20010;&#31243;&#24207;&#19978;&#30340;&#32570;&#38519;&#12290;(1)&#20197;&#24448;&#30340;&#26041;&#27861;&#20165;&#20351;&#29992;CXR&#25253;&#21578;&#65292;&#36825;&#23545;&#20110;&#20840;&#38754;&#30340;&#35270;&#35273;&#38382;&#31572;(VQA)&#26469;&#35828;&#26159;&#19981;&#22815;&#30340;&#65292;&#29305;&#21035;&#26159;&#24403;&#38656;&#35201;&#39069;&#22806;&#30340;&#20581;&#24247;&#30456;&#20851;&#25968;&#25454;&#22914;&#29992;&#33647;&#21382;&#21490;&#21644;&#20808;&#21069;&#30340;&#35786;&#26029;&#26102;&#12290;(2)&#20197;&#24448;&#30340;&#26041;&#27861;&#20351;&#29992;&#26410;&#32463;&#22788;&#29702;&#30340;CXR&#25253;&#21578;&#65292;&#36825;&#20123;&#25253;&#21578;&#24448;&#24448;&#32467;&#26500;&#38543;&#24847;&#12290;&#34429;&#28982;&#29616;&#20195;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#29702;&#35299;&#21508;&#31181;&#25991;&#26412;&#26684;&#24335;&#65292;&#20294;&#20026;&#20102;&#25552;&#20379;&#26356;&#28165;&#26224;&#12289;&#26377;&#32452;&#32455;&#30340;&#22522;&#20110;&#35299;&#21078;&#23398;&#30340;&#20449;&#24687;&#65292;&#37325;&#26500;&#25253;&#21578;&#21487;&#33021;&#20250;&#22686;&#24378;&#23427;&#20204;&#30340;&#23454;&#29992;&#24615;&#12290;(3)&#30446;&#21069;&#29992;&#20110;CXR-VQA&#30340;&#35780;&#20272;&#26041;&#27861;&#20027;&#35201;&#24378;&#35843;&#35821;&#35328;&#27491;&#30830;&#24615;&#65292;&#32570;&#20047;&#23545;&#29983;&#25104;&#31572;&#26696;&#30340;&#24494;&#22937;&#35780;&#20272;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15456v1 Announce Type: new  Abstract: Significant methodological strides have been made toward Chest X-ray (CXR) understanding via modern vision-language models (VLMs), demonstrating impressive Visual Question Answering (VQA) and CXR report generation abilities. However, existing CXR understanding frameworks still possess several procedural caveats. (1) Previous methods solely use CXR reports, which are insufficient for comprehensive Visual Question Answering (VQA), especially when additional health-related data like medication history and prior diagnoses are needed. (2) Previous methods use raw CXR reports, which are often arbitrarily structured. While modern language models can understand various text formats, restructuring reports for clearer, organized anatomy-based information could enhance their usefulness. (3) Current evaluation methods for CXR-VQA primarily emphasize linguistic correctness, lacking the capability to offer nuanced assessments of the generated answers.
&lt;/p&gt;</description></item><item><title>&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#20449;&#20219;&#26159;&#25511;&#21046;&#20854;&#20256;&#25773;&#31243;&#24230;&#30340;&#35843;&#33410;&#22120;&#65292;&#36890;&#36807;&#22686;&#21152;&#20449;&#20219;&#21644;&#20943;&#23569;&#19981;&#20449;&#20219;&#65292;&#21487;&#20197;&#26174;&#33879;&#24433;&#21709;&#20154;&#24037;&#26234;&#33021;&#30340;&#37319;&#29992;&#36895;&#24230;&#12290;</title><link>https://arxiv.org/abs/2403.14680</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#20449;&#20219;: &#36827;&#23637;&#12289;&#25361;&#25112;&#21644;&#26410;&#26469;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;
Trust in AI: Progress, Challenges, and Future Directions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14680
&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#20449;&#20219;&#26159;&#25511;&#21046;&#20854;&#20256;&#25773;&#31243;&#24230;&#30340;&#35843;&#33410;&#22120;&#65292;&#36890;&#36807;&#22686;&#21152;&#20449;&#20219;&#21644;&#20943;&#23569;&#19981;&#20449;&#20219;&#65292;&#21487;&#20197;&#26174;&#33879;&#24433;&#21709;&#20154;&#24037;&#26234;&#33021;&#30340;&#37319;&#29992;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#22312;&#25105;&#20204;&#26085;&#24120;&#29983;&#27963;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#36890;&#36807;&#21508;&#31181;&#24212;&#29992;&#12289;&#26381;&#21153;&#21644;&#20135;&#21697;&#65292;&#35828;&#26126;&#20102;&#26469;&#33258;&#29992;&#25143;&#35282;&#24230;&#23545;&#20154;&#24037;&#26234;&#33021;&#30340;&#20449;&#20219;/&#19981;&#20449;&#20219;&#30340;&#37325;&#35201;&#24615;&#12290;&#19982;&#20854;&#20182;&#25216;&#26415;&#30456;&#27604;&#65292;&#30001;&#20154;&#24037;&#26234;&#33021;&#39537;&#21160;&#30340;&#31995;&#32479;&#19981;&#20165;&#20316;&#20026;&#19968;&#20123;&#26377;&#30410;&#24037;&#20855;&#24191;&#27867;&#28183;&#36879;&#21040;&#25105;&#20204;&#30340;&#29983;&#27963;&#20013;&#65292;&#32780;&#19988;&#36824;&#20250;&#25104;&#20026;&#20195;&#34920;&#25105;&#20204;&#30340;&#26367;&#20195;&#24615;&#20195;&#29702;&#20154;&#65292;&#25110;&#32773;&#20250;&#24433;&#21709;&#20154;&#31867;&#24605;&#32500;&#12289;&#20915;&#31574;&#21644;&#34892;&#21160;&#30340;&#25805;&#32437;&#24615;&#24515;&#26234;&#12290;&#36817;&#26469;&#65292;&#21508;&#31181;&#30740;&#31350;&#24050;&#32463;&#20851;&#27880;&#20102;&#20154;&#24037;&#26234;&#33021;&#20013;&#20449;&#20219;/&#19981;&#20449;&#20219;&#30340;&#19981;&#21516;&#32500;&#24230;&#21450;&#20854;&#30456;&#20851;&#32771;&#34385;&#22240;&#32032;&#12290;&#22312;&#36825;&#31687;&#31995;&#32479;&#24615;&#25991;&#29486;&#32508;&#36848;&#20013;&#65292;&#22312;&#23545;&#24403;&#21069;&#20154;&#24037;&#26234;&#33021;&#25991;&#29486;&#20013;&#23545;&#20449;&#20219;&#30340;&#27010;&#24565;&#21270;&#20043;&#21518;&#65292;&#25105;&#20204;&#23558;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14680v1 Announce Type: cross  Abstract: The increasing use of artificial intelligence (AI) systems in our daily life through various applications, services, and products explains the significance of trust/distrust in AI from a user perspective. AI-driven systems (as opposed to other technologies) have ubiquitously diffused in our life not only as some beneficial tools to be used by human agents but also are going to be substitutive agents on our behalf, or manipulative minds that would influence human thought, decision, and agency. Trust/distrust in AI plays the role of a regulator and could significantly control the level of this diffusion, as trust can increase, and distrust may reduce the rate of adoption of AI. Recently, varieties of studies have paid attention to the variant dimension of trust/distrust in AI, and its relevant considerations. In this systematic literature review, after conceptualization of trust in the current AI literature review, we will investigate tr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#30693;&#35782;&#32534;&#36753;&#25216;&#26415;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#21435;&#27602;&#21270;&#65292;&#22312;&#26500;&#24314;&#20102;SafeEdit&#22522;&#20934;&#30340;&#22522;&#30784;&#19978;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861; DINM&#65292;&#21487;&#20197;&#36890;&#36807;&#23569;&#37327;&#35843;&#25972;&#27493;&#39588;&#20943;&#23569;&#27169;&#22411;&#30340;&#27602;&#24615;&#65292;&#21516;&#26102;&#23545;&#21508;&#31181;&#21435;&#27602;&#26041;&#27861;&#30340;&#20869;&#37096;&#26426;&#21046;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#12290;</title><link>https://arxiv.org/abs/2403.14472</link><description>&lt;p&gt;
&#36890;&#36807;&#30693;&#35782;&#32534;&#36753;&#23454;&#29616;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21435;&#27602;&#21270;
&lt;/p&gt;
&lt;p&gt;
Detoxifying Large Language Models via Knowledge Editing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14472
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#30693;&#35782;&#32534;&#36753;&#25216;&#26415;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#21435;&#27602;&#21270;&#65292;&#22312;&#26500;&#24314;&#20102;SafeEdit&#22522;&#20934;&#30340;&#22522;&#30784;&#19978;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861; DINM&#65292;&#21487;&#20197;&#36890;&#36807;&#23569;&#37327;&#35843;&#25972;&#27493;&#39588;&#20943;&#23569;&#27169;&#22411;&#30340;&#27602;&#24615;&#65292;&#21516;&#26102;&#23545;&#21508;&#31181;&#21435;&#27602;&#26041;&#27861;&#30340;&#20869;&#37096;&#26426;&#21046;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#30693;&#35782;&#32534;&#36753;&#25216;&#26415;&#26469;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#21435;&#27602;&#21270;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#21517;&#20026;SafeEdit&#30340;&#22522;&#20934;&#65292;&#28085;&#30422;&#20102;&#20061;&#31181;&#19981;&#23433;&#20840;&#31867;&#21035;&#65292;&#20855;&#26377;&#21508;&#31181;&#24378;&#22823;&#30340;&#25915;&#20987;&#25552;&#31034;&#65292;&#24182;&#37197;&#22791;&#20102;&#20840;&#38754;&#30340;&#24230;&#37327;&#26631;&#20934;&#36827;&#34892;&#31995;&#32479;&#35780;&#20272;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#27604;&#36739;&#20102;&#30693;&#35782;&#32534;&#36753;&#26041;&#27861;&#19982;&#20043;&#21069;&#30340;&#22522;&#20934;&#32447;&#65292;&#32467;&#26524;&#34920;&#26126;&#30693;&#35782;&#32534;&#36753;&#26377;&#28508;&#21147;&#22312;&#23545;LLMs&#36827;&#34892;&#21435;&#27602;&#21270;&#26102;&#65292;&#22312;&#23545;&#19968;&#33324;&#24615;&#33021;&#30340;&#24433;&#21709;&#30456;&#23545;&#26377;&#38480;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#22522;&#20934;&#32447;&#65292;&#31216;&#20026;&#36890;&#36807;&#26415;&#20013;&#31070;&#32463;&#30417;&#27979;&#21435;&#27602;&#21270;&#65288;DINM&#65289;&#65292;&#36890;&#36807;&#20165;&#19968;&#27425;&#23454;&#20363;&#30340;&#23569;&#37327;&#35843;&#25972;&#27493;&#39588;&#20943;&#23569;LLMs&#30340;&#27602;&#24615;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23545;&#21508;&#31181;&#21435;&#27602;&#26041;&#27861;&#30340;&#20869;&#37096;&#26426;&#21046;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#65292;&#34920;&#26126;&#20808;&#21069;&#30340;&#26041;&#27861;&#22914;SFT&#21644;DPO&#21487;&#33021;&#20165;&#25233;&#21046;&#26377;&#27602;&#21442;&#25968;&#30340;&#28608;&#27963;&#65292;&#32780;DINM&#21017;&#20943;&#36731;&#26377;&#27602;&#21442;&#25968;&#30340;&#27602;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14472v1 Announce Type: cross  Abstract: This paper investigates using knowledge editing techniques to detoxify Large Language Models (LLMs). We construct a benchmark, SafeEdit, which covers nine unsafe categories with various powerful attack prompts and equips comprehensive metrics for systematic evaluation. We conduct experiments to compare knowledge editing approaches with previous baselines, indicating that knowledge editing has the potential to efficiently detoxify LLMs with limited impact on general performance. Then, we propose a simple yet effective baseline, dubbed Detoxifying with Intraoperative Neural Monitoring (DINM), to diminish the toxicity of LLMs within a few tuning steps via only one instance. We further provide an in-depth analysis of the internal mechanism for various detoxify approaches, demonstrating that previous methods like SFT and DPO may merely suppress the activations of toxic parameters, while DINM mitigates the toxicity of the toxic parameters to
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26032;&#39062;&#30340;&#33258;&#36866;&#24212;QA&#26694;&#26550;&#65292;&#26681;&#25454;&#26597;&#35810;&#30340;&#22797;&#26434;&#24230;&#21160;&#24577;&#36873;&#25321;&#36866;&#21512;&#30340;&#26816;&#32034;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#31574;&#30053;&#65292;&#25552;&#39640;&#20102;&#22238;&#31572;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.14403</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#26816;&#32034;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65306;&#36890;&#36807;&#38382;&#39064;&#22797;&#26434;&#24230;&#23398;&#20064;&#35843;&#36866;
&lt;/p&gt;
&lt;p&gt;
Adaptive-RAG: Learning to Adapt Retrieval-Augmented Large Language Models through Question Complexity
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14403
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26032;&#39062;&#30340;&#33258;&#36866;&#24212;QA&#26694;&#26550;&#65292;&#26681;&#25454;&#26597;&#35810;&#30340;&#22797;&#26434;&#24230;&#21160;&#24577;&#36873;&#25321;&#36866;&#21512;&#30340;&#26816;&#32034;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#31574;&#30053;&#65292;&#25552;&#39640;&#20102;&#22238;&#31572;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#32034;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23558;&#22806;&#37096;&#30693;&#35782;&#24211;&#20013;&#30340;&#38750;&#21442;&#25968;&#30693;&#35782;&#32435;&#20837;LLMs&#65292;&#24050;&#25104;&#20026;&#25552;&#39640;&#22810;&#31181;&#20219;&#21153;&#20013;&#22238;&#31572;&#20934;&#30830;&#24615;&#30340;&#26377;&#24076;&#26395;&#26041;&#27861;&#65292;&#22914;&#38382;&#31572;&#65288;QA&#65289;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#26377;&#21508;&#31181;&#26041;&#27861;&#22788;&#29702;&#19981;&#21516;&#22797;&#26434;&#24230;&#30340;&#26597;&#35810;&#65292;&#20294;&#23427;&#20204;&#35201;&#20040;&#22788;&#29702;&#31616;&#21333;&#26597;&#35810;&#26102;&#20135;&#29983;&#19981;&#24517;&#35201;&#30340;&#35745;&#31639;&#24320;&#38144;&#65292;&#35201;&#20040;&#26410;&#33021;&#20805;&#20998;&#35299;&#20915;&#22797;&#26434;&#30340;&#22810;&#27493;&#26597;&#35810;&#65307;&#28982;&#32780;&#65292;&#24182;&#38750;&#25152;&#26377;&#29992;&#25143;&#35831;&#27714;&#37117;&#21482;&#33021;&#21010;&#20998;&#20026;&#31616;&#21333;&#25110;&#22797;&#26434;&#31867;&#21035;&#20013;&#30340;&#19968;&#31181;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#36866;&#24212;QA&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#21160;&#24577;&#36873;&#25321;&#20174;&#26368;&#31616;&#21333;&#21040;&#26368;&#22797;&#26434;&#30340;&#65288;&#26816;&#32034;&#22686;&#24378;&#65289;LLMs&#31574;&#30053;&#65292;&#36825;&#21462;&#20915;&#20110;&#26597;&#35810;&#30340;&#22797;&#26434;&#24230;&#12290;&#27492;&#22806;&#65292;&#36825;&#20010;&#36873;&#25321;&#36807;&#31243;&#26159;&#36890;&#36807;&#19968;&#20010;&#20998;&#31867;&#22120;&#23454;&#29616;&#30340;&#65292;&#35813;&#20998;&#31867;&#22120;&#26159;&#19968;&#20010;&#36739;&#23567;&#30340;LM&#65292;&#35757;&#32451;&#20197;&#39044;&#27979;&#20256;&#20837;&#26597;&#35810;&#30340;&#22797;&#26434;&#24230;&#32423;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14403v1 Announce Type: cross  Abstract: Retrieval-Augmented Large Language Models (LLMs), which incorporate the non-parametric knowledge from external knowledge bases into LLMs, have emerged as a promising approach to enhancing response accuracy in several tasks, such as Question-Answering (QA). However, even though there are various approaches dealing with queries of different complexities, they either handle simple queries with unnecessary computational overhead or fail to adequately address complex multi-step queries; yet, not all user requests fall into only one of the simple or complex categories. In this work, we propose a novel adaptive QA framework, that can dynamically select the most suitable strategy for (retrieval-augmented) LLMs from the simplest to the most sophisticated ones based on the query complexity. Also, this selection process is operationalized with a classifier, which is a smaller LM trained to predict the complexity level of incoming queries with aut
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;ROUTERBENCH&#65292;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;LLM&#36335;&#30001;&#31995;&#32479;&#24615;&#33021;&#30340;&#22522;&#20934;&#27979;&#35797;&#26694;&#26550;&#65292;&#21253;&#25324;&#36229;&#36807;405k&#25512;&#29702;&#32467;&#26524;&#30340;&#25968;&#25454;&#38598;&#65292;&#20197;&#25903;&#25345;&#36335;&#30001;&#31574;&#30053;&#30340;&#24320;&#21457;&#12290;</title><link>https://arxiv.org/abs/2403.12031</link><description>&lt;p&gt;
ROUTERBENCH&#65306;&#29992;&#20110;&#22810;LLM&#36335;&#30001;&#31995;&#32479;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
ROUTERBENCH: A Benchmark for Multi-LLM Routing System
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12031
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;ROUTERBENCH&#65292;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;LLM&#36335;&#30001;&#31995;&#32479;&#24615;&#33021;&#30340;&#22522;&#20934;&#27979;&#35797;&#26694;&#26550;&#65292;&#21253;&#25324;&#36229;&#36807;405k&#25512;&#29702;&#32467;&#26524;&#30340;&#25968;&#25454;&#38598;&#65292;&#20197;&#25903;&#25345;&#36335;&#30001;&#31574;&#30053;&#30340;&#24320;&#21457;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24212;&#29992;&#33539;&#22260;&#19981;&#26029;&#25193;&#22823;&#65292;&#23545;&#26377;&#25928;&#30340;&#26381;&#21153;&#35299;&#20915;&#26041;&#26696;&#30340;&#38656;&#27714;&#21464;&#24471;&#26085;&#30410;&#20851;&#38190;&#12290;&#23613;&#31649;LLMs&#20855;&#26377;&#22810;&#26679;&#24615;&#65292;&#20294;&#27809;&#26377;&#21333;&#19968;&#27169;&#22411;&#21487;&#20197;&#26368;&#20248;&#22320;&#35299;&#20915;&#25152;&#26377;&#20219;&#21153;&#21644;&#24212;&#29992;&#65292;&#29305;&#21035;&#26159;&#22312;&#24179;&#34913;&#24615;&#33021;&#21644;&#25104;&#26412;&#20043;&#38388;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#38480;&#21046;&#65292;&#21457;&#23637;&#20102;LLM&#36335;&#30001;&#31995;&#32479;&#65292;&#36825;&#20123;&#31995;&#32479;&#32467;&#21512;&#20102;&#21508;&#31181;&#27169;&#22411;&#30340;&#20248;&#21183;&#65292;&#20197;&#20811;&#26381;&#21333;&#20010;LLMs&#30340;&#32422;&#26463;&#12290;&#28982;&#32780;&#65292;&#32570;&#20047;&#29992;&#20110;&#35780;&#20272;LLM&#36335;&#30001;&#22120;&#24615;&#33021;&#30340;&#26631;&#20934;&#21270;&#22522;&#20934;&#27979;&#35797;&#65292;&#38459;&#30861;&#20102;&#36825;&#19968;&#39046;&#22495;&#30340;&#36827;&#23637;&#12290;&#20026;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ROUTERBENCH&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#26088;&#22312;&#31995;&#32479;&#35780;&#20272;LLM&#36335;&#30001;&#31995;&#32479;&#30340;&#21151;&#25928;&#65292;&#20197;&#21450;&#19968;&#20010;&#21253;&#25324;&#26469;&#33258;&#20195;&#34920;&#24615;LLMs&#30340;&#36229;&#36807;405k&#25512;&#29702;&#32467;&#26524;&#30340;&#20840;&#38754;&#25968;&#25454;&#38598;&#65292;&#20197;&#25903;&#25345;&#36335;&#30001;&#31574;&#30053;&#30340;&#24320;&#21457;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#20010;LLM&#36335;&#30001;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#20197;&#21450;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12031v1 Announce Type: cross  Abstract: As the range of applications for Large Language Models (LLMs) continues to grow, the demand for effective serving solutions becomes increasingly critical. Despite the versatility of LLMs, no single model can optimally address all tasks and applications, particularly when balancing performance with cost. This limitation has led to the development of LLM routing systems, which combine the strengths of various models to overcome the constraints of individual LLMs. Yet, the absence of a standardized benchmark for evaluating the performance of LLM routers hinders progress in this area. To bridge this gap, we present ROUTERBENCH, a novel evaluation framework designed to systematically assess the efficacy of LLM routing systems, along with a comprehensive dataset comprising over 405k inference outcomes from representative LLMs to support the development of routing strategies. We further propose a theoretical framework for LLM routing, and del
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#24314;&#31435;&#19968;&#20010;&#32479;&#19968;&#30340;&#22810;&#27169;&#24577;&#20010;&#24615;&#21270;&#31995;&#32479;(UniMP)&#65292;&#26377;&#25928;&#21033;&#29992;&#22810;&#27169;&#24577;&#25968;&#25454;&#21516;&#26102;&#28040;&#38500;&#19982;&#20219;&#21153;&#21644;&#27169;&#24577;&#29305;&#23450;&#23450;&#21046;&#30456;&#20851;&#30340;&#22797;&#26434;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.10667</link><description>&lt;p&gt;
&#36890;&#21521;&#32479;&#19968;&#22810;&#27169;&#24335;&#20010;&#24615;&#21270;&#65306;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#29983;&#25104;&#25512;&#33616;&#21644;&#26356;&#22810;&#39046;&#22495;
&lt;/p&gt;
&lt;p&gt;
Towards Unified Multi-Modal Personalization: Large Vision-Language Models for Generative Recommendation and Beyond
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10667
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#24314;&#31435;&#19968;&#20010;&#32479;&#19968;&#30340;&#22810;&#27169;&#24577;&#20010;&#24615;&#21270;&#31995;&#32479;(UniMP)&#65292;&#26377;&#25928;&#21033;&#29992;&#22810;&#27169;&#24577;&#25968;&#25454;&#21516;&#26102;&#28040;&#38500;&#19982;&#20219;&#21153;&#21644;&#27169;&#24577;&#29305;&#23450;&#23450;&#21046;&#30456;&#20851;&#30340;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#19968;&#20010;&#33021;&#22815;&#26377;&#25928;&#21033;&#29992;&#24322;&#26500;&#36164;&#28304;&#24182;&#28385;&#36275;&#21508;&#31181;&#20010;&#24615;&#21270;&#38656;&#27714;&#30340;&#36890;&#29992;&#27169;&#22411;&#19968;&#30452;&#26159;&#31038;&#21306;&#28212;&#26395;&#30340;&#30446;&#26631;&#12290;&#25105;&#20204;&#26085;&#24120;&#30340;&#36873;&#25321;&#65292;&#23588;&#20854;&#26159;&#22312;&#26102;&#23578;&#21644;&#38646;&#21806;&#31561;&#39046;&#22495;&#65292;&#24456;&#22823;&#31243;&#24230;&#19978;&#21463;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#24433;&#21709;&#65292;&#27604;&#22914;&#22270;&#29255;&#21644;&#25991;&#26412;&#25551;&#36848;&#12290;&#36825;&#20123;&#27169;&#24577;&#19981;&#20165;&#25552;&#20379;&#30452;&#35266;&#30340;&#25351;&#23548;&#65292;&#36824;&#36814;&#21512;&#20010;&#24615;&#21270;&#29992;&#25143;&#20559;&#22909;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#20027;&#27969;&#30340;&#20010;&#24615;&#21270;&#26041;&#27861;&#20027;&#35201;&#32858;&#28966;&#20110;&#22522;&#20110;ID&#25110;&#25991;&#26412;&#30340;&#25512;&#33616;&#38382;&#39064;&#65292;&#26410;&#33021;&#29702;&#35299;&#28085;&#30422;&#21508;&#31181;&#20219;&#21153;&#25110;&#27169;&#24577;&#30340;&#20449;&#24687;&#12290;&#26412;&#25991;&#30340;&#30446;&#26631;&#26159;&#24314;&#31435;&#19968;&#20010;&#32479;&#19968;&#30340;&#22810;&#27169;&#24577;&#20010;&#24615;&#21270;&#31995;&#32479;(UniMP)&#65292;&#33021;&#22815;&#26377;&#25928;&#21033;&#29992;&#22810;&#27169;&#24577;&#25968;&#25454;&#65292;&#21516;&#26102;&#28040;&#38500;&#19982;&#20219;&#21153;&#21644;&#27169;&#24577;&#29305;&#23450;&#23450;&#21046;&#30456;&#20851;&#30340;&#22797;&#26434;&#24615;&#12290;&#25105;&#20204;&#35748;&#20026;&#22522;&#30784;&#29983;&#25104;&#24314;&#27169;&#30340;&#36827;&#23637;&#25552;&#20379;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10667v1 Announce Type: cross  Abstract: Developing a universal model that can effectively harness heterogeneous resources and respond to a wide range of personalized needs has been a longstanding community aspiration. Our daily choices, especially in domains like fashion and retail, are substantially shaped by multi-modal data, such as pictures and textual descriptions. These modalities not only offer intuitive guidance but also cater to personalized user preferences. However, the predominant personalization approaches mainly focus on the ID or text-based recommendation problem, failing to comprehend the information spanning various tasks or modalities. In this paper, our goal is to establish a Unified paradigm for Multi-modal Personalization systems (UniMP), which effectively leverages multi-modal data while eliminating the complexities associated with task- and modality-specific customization. We argue that the advancements in foundational generative modeling have provided
&lt;/p&gt;</description></item><item><title>&#24320;&#25918;&#22270;&#35889;&#26159;&#38024;&#23545;&#22823;&#35268;&#27169;&#23460;&#22806;&#29615;&#22659;&#35774;&#35745;&#30340;&#24320;&#25918;&#35789;&#27719;&#20998;&#23618;&#22270;&#32467;&#26500;&#65292;&#26088;&#22312;&#35299;&#20915;&#29616;&#26377;&#22320;&#22270;&#21463;&#38480;&#20110;&#23460;&#20869;&#22330;&#26223;&#21644;VLM&#29305;&#24449;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#35270;&#35273;&#22270;&#20687;&#25552;&#21462;&#23454;&#20363;&#21644;&#26631;&#39064;&#65292;&#24182;&#21152;&#24378;&#25991;&#23383;&#25512;&#29702;&#12290;</title><link>https://arxiv.org/abs/2403.09412</link><description>&lt;p&gt;
&#24320;&#25918;&#22270;&#35889;&#65306;&#22823;&#35268;&#27169;&#23460;&#22806;&#29615;&#22659;&#20013;&#30340;&#24320;&#25918;&#35789;&#27719;&#20998;&#23618;3D&#22270;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
OpenGraph: Open-Vocabulary Hierarchical 3D Graph Representation in Large-Scale Outdoor Environments
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09412
&lt;/p&gt;
&lt;p&gt;
&#24320;&#25918;&#22270;&#35889;&#26159;&#38024;&#23545;&#22823;&#35268;&#27169;&#23460;&#22806;&#29615;&#22659;&#35774;&#35745;&#30340;&#24320;&#25918;&#35789;&#27719;&#20998;&#23618;&#22270;&#32467;&#26500;&#65292;&#26088;&#22312;&#35299;&#20915;&#29616;&#26377;&#22320;&#22270;&#21463;&#38480;&#20110;&#23460;&#20869;&#22330;&#26223;&#21644;VLM&#29305;&#24449;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#35270;&#35273;&#22270;&#20687;&#25552;&#21462;&#23454;&#20363;&#21644;&#26631;&#39064;&#65292;&#24182;&#21152;&#24378;&#25991;&#23383;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20855;&#26377;&#22797;&#26434;&#35821;&#20041;&#30340;&#29615;&#22659;&#22320;&#22270;&#23545;&#20110;&#20419;&#36827;&#26426;&#22120;&#20154;&#21644;&#20154;&#31867;&#20043;&#38388;&#30340;&#26080;&#32541;&#20132;&#20114;&#33267;&#20851;&#37325;&#35201;&#65292;&#20351;&#23427;&#20204;&#33021;&#22815;&#26377;&#25928;&#22320;&#25191;&#34892;&#21508;&#31181;&#20219;&#21153;&#12290;&#24320;&#25918;&#35789;&#27719;&#22320;&#22270;&#65292;&#30001;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;VLM&#65289;&#39537;&#21160;&#65292;&#20855;&#26377;&#22266;&#26377;&#20248;&#21183;&#65292;&#21253;&#25324;&#22810;&#27169;&#24577;&#26816;&#32034;&#21644;&#24320;&#25918;&#31867;&#21035;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#24320;&#25918;&#35789;&#27719;&#22320;&#22270;&#21463;&#38480;&#20110;&#23553;&#38381;&#30340;&#23460;&#20869;&#22330;&#26223;&#21644;VLM&#29305;&#24449;&#65292;&#20174;&#32780;&#38477;&#20302;&#20102;&#23427;&#20204;&#30340;&#21487;&#29992;&#24615;&#21644;&#25512;&#29702;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#32570;&#20047;&#25299;&#25169;&#20851;&#31995;&#36827;&#19968;&#27493;&#20351;&#24471;&#23545;&#29305;&#23450;&#23454;&#20363;&#30340;&#20934;&#30830;&#26597;&#35810;&#21464;&#24471;&#22797;&#26434;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;OpenGraph&#65292;&#36825;&#26159;&#19968;&#31181;&#20026;&#22823;&#35268;&#27169;&#23460;&#22806;&#29615;&#22659;&#35774;&#35745;&#30340;&#24320;&#25918;&#35789;&#27719;&#20998;&#23618;&#22270;&#32467;&#26500;&#34920;&#31034;&#12290;OpenGraph&#39318;&#20808;&#21033;&#29992;2D&#22522;&#30784;&#27169;&#22411;&#20174;&#35270;&#35273;&#22270;&#20687;&#20013;&#25552;&#21462;&#23454;&#20363;&#21450;&#20854;&#26631;&#39064;&#65292;&#24182;&#23545;&#26631;&#39064;&#36827;&#34892;&#29305;&#24449;&#32534;&#30721;&#20197;&#22686;&#24378;&#25991;&#23383;&#25512;&#29702;&#12290;&#38543;&#21518;&#65292;3D&#36827;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09412v1 Announce Type: cross  Abstract: Environment maps endowed with sophisticated semantics are pivotal for facilitating seamless interaction between robots and humans, enabling them to effectively carry out various tasks. Open-vocabulary maps, powered by Visual-Language models (VLMs), possess inherent advantages, including multimodal retrieval and open-set classes. However, existing open-vocabulary maps are constrained to closed indoor scenarios and VLM features, thereby diminishing their usability and inference capabilities. Moreover, the absence of topological relationships further complicates the accurate querying of specific instances. In this work, we propose OpenGraph, a representation of open-vocabulary hierarchical graph structure designed for large-scale outdoor environments. OpenGraph initially extracts instances and their captions from visual images using 2D foundation models, encoding the captions with features to enhance textual reasoning. Subsequently, 3D in
&lt;/p&gt;</description></item><item><title>FluoroSAM&#26159;&#29992;&#20110;X&#20809;&#22270;&#20687;&#30340;&#20998;&#21106;&#30340;&#35821;&#35328;&#23545;&#40784;&#22522;&#30784;&#27169;&#22411;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#22312;X&#20809;&#25104;&#20687;&#39046;&#22495;&#20855;&#26377;&#24191;&#27867;&#36866;&#29992;&#24615;&#30340;&#33258;&#21160;&#22270;&#20687;&#20998;&#26512;&#24037;&#20855;&#12290;</title><link>https://arxiv.org/abs/2403.08059</link><description>&lt;p&gt;
FluoroSAM: &#29992;&#20110;X&#20809;&#22270;&#20687;&#20998;&#21106;&#30340;&#35821;&#35328;&#23545;&#40784;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
FluoroSAM: A Language-aligned Foundation Model for X-ray Image Segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08059
&lt;/p&gt;
&lt;p&gt;
FluoroSAM&#26159;&#29992;&#20110;X&#20809;&#22270;&#20687;&#30340;&#20998;&#21106;&#30340;&#35821;&#35328;&#23545;&#40784;&#22522;&#30784;&#27169;&#22411;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#22312;X&#20809;&#25104;&#20687;&#39046;&#22495;&#20855;&#26377;&#24191;&#27867;&#36866;&#29992;&#24615;&#30340;&#33258;&#21160;&#22270;&#20687;&#20998;&#26512;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;X&#20809;&#22270;&#20687;&#20998;&#21106;&#23558;&#21152;&#36895;&#35786;&#26029;&#21644;&#20171;&#20837;&#31934;&#20934;&#21307;&#23398;&#39046;&#22495;&#30340;&#30740;&#31350;&#21644;&#21457;&#23637;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#35299;&#20915;&#29305;&#23450;&#22270;&#20687;&#20998;&#26512;&#38382;&#39064;&#30340;&#29305;&#23450;&#20219;&#21153;&#27169;&#22411;&#65292;&#20294;&#36825;&#20123;&#27169;&#22411;&#30340;&#25928;&#29992;&#21463;&#38480;&#20110;&#29305;&#23450;&#20219;&#21153;&#39046;&#22495;&#65292;&#35201;&#25299;&#23637;&#21040;&#26356;&#24191;&#27867;&#30340;&#24212;&#29992;&#21017;&#38656;&#35201;&#39069;&#22806;&#30340;&#25968;&#25454;&#12289;&#26631;&#31614;&#21644;&#37325;&#26032;&#35757;&#32451;&#24037;&#20316;&#12290;&#26368;&#36817;&#65292;&#22522;&#30784;&#27169;&#22411;&#65288;FMs&#65289; - &#35757;&#32451;&#22312;&#22823;&#37327;&#39640;&#24230;&#21464;&#21270;&#25968;&#25454;&#19978;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22240;&#27492;&#20351;&#24471;&#24191;&#27867;&#36866;&#29992;&#24615;&#25104;&#20026;&#21487;&#33021; - &#24050;&#32463;&#25104;&#20026;&#33258;&#21160;&#22270;&#20687;&#20998;&#26512;&#30340;&#26377;&#24076;&#26395;&#30340;&#24037;&#20855;&#12290;&#29616;&#26377;&#30340;&#29992;&#20110;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#30340;FMs&#32858;&#28966;&#20110;&#23545;&#35937;&#34987;&#26126;&#26174;&#21487;&#35265;&#36793;&#30028;&#28165;&#26224;&#23450;&#20041;&#30340;&#22330;&#26223;&#21644;&#27169;&#24335;&#65292;&#22914;&#20869;&#31397;&#38236;&#25163;&#26415;&#24037;&#20855;&#20998;&#21106;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;X&#20809;&#25104;&#20687;&#36890;&#24120;&#27809;&#26377;&#25552;&#20379;&#36825;&#31181;&#28165;&#26224;&#30340;&#36793;&#30028;&#25110;&#32467;&#26500;&#20808;&#39564;&#12290;&#22312;X&#20809;&#22270;&#20687;&#24418;&#25104;&#26399;&#38388;&#65292;&#22797;&#26434;&#30340;&#19977;&#32500;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08059v1 Announce Type: cross  Abstract: Automated X-ray image segmentation would accelerate research and development in diagnostic and interventional precision medicine. Prior efforts have contributed task-specific models capable of solving specific image analysis problems, but the utility of these models is restricted to their particular task domain, and expanding to broader use requires additional data, labels, and retraining efforts. Recently, foundation models (FMs) -- machine learning models trained on large amounts of highly variable data thus enabling broad applicability -- have emerged as promising tools for automated image analysis. Existing FMs for medical image analysis focus on scenarios and modalities where objects are clearly defined by visually apparent boundaries, such as surgical tool segmentation in endoscopy. X-ray imaging, by contrast, does not generally offer such clearly delineated boundaries or structure priors. During X-ray image formation, complex 3D
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;GRU&#21644;LSTM&#28151;&#21512;&#26041;&#27861;&#36827;&#34892;3D&#28857;&#20113;&#20013;&#29289;&#20307;&#20998;&#31867;&#30340;&#28145;&#24230;&#23398;&#20064;&#31574;&#30053;&#65292;&#21462;&#24471;&#20102;&#39640;&#20934;&#30830;&#29575;&#65292;&#24182;&#22312;&#23545;&#22810;&#20010;&#31867;&#21035;&#30340;&#25968;&#25454;&#38598;&#20013;&#21462;&#24471;&#20248;&#24322;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2403.05950</link><description>&lt;p&gt;
&#20351;&#29992;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#22312;3D&#28857;&#20113;&#20013;&#23545;&#29289;&#20307;&#36827;&#34892;&#20998;&#31867;&#65306;&#19968;&#31181;GRU LSTM&#28151;&#21512;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Classifying Objects in 3D Point Clouds Using Recurrent Neural Network: A GRU LSTM Hybrid Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05950
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;GRU&#21644;LSTM&#28151;&#21512;&#26041;&#27861;&#36827;&#34892;3D&#28857;&#20113;&#20013;&#29289;&#20307;&#20998;&#31867;&#30340;&#28145;&#24230;&#23398;&#20064;&#31574;&#30053;&#65292;&#21462;&#24471;&#20102;&#39640;&#20934;&#30830;&#29575;&#65292;&#24182;&#22312;&#23545;&#22810;&#20010;&#31867;&#21035;&#30340;&#25968;&#25454;&#38598;&#20013;&#21462;&#24471;&#20248;&#24322;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#20027;&#23548;&#33322;&#21644;&#22686;&#24378;/&#34394;&#25311;&#29616;&#23454;&#22330;&#26223;&#31561;&#22810;&#20010;&#24212;&#29992;&#20013;&#65292;&#20934;&#30830;&#23545;3D&#28857;&#20113;&#20013;&#30340;&#29289;&#20307;&#36827;&#34892;&#20998;&#31867;&#26159;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#65292;&#24050;&#25104;&#20026;&#30740;&#31350;&#28909;&#28857;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22686;&#24378;&#29616;&#23454;&#20013;&#30340;3D&#29289;&#20307;&#20998;&#31867;&#30340;&#28145;&#24230;&#23398;&#20064;&#31574;&#30053;&#12290;&#35813;&#26041;&#27861;&#26159;GRU&#21644;LSTM&#30340;&#32452;&#21512;&#12290;LSTM&#32593;&#32476;&#33021;&#22815;&#24456;&#22909;&#22320;&#23398;&#20064;&#38271;&#26399;&#20381;&#36182;&#20851;&#31995;&#65292;&#20294;&#30001;&#20110;&#38376;&#25968;&#37327;&#36739;&#22810;&#65292;&#35757;&#32451;&#26102;&#38388;&#36739;&#38271;&#65307;&#21478;&#19968;&#26041;&#38754;&#65292;GRU&#32593;&#32476;&#24615;&#33021;&#36739;&#24369;&#20110;LSTM&#65292;&#20294;&#20854;&#35757;&#32451;&#36895;&#24230;&#36828;&#39640;&#20110;LSTM&#65292;&#36825;&#26159;&#30001;&#20110;&#20854;&#38376;&#25968;&#37327;&#36739;&#23569;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;&#36825;&#20004;&#31181;&#32593;&#32476;&#30340;&#36895;&#24230;&#21644;&#20934;&#30830;&#24615;&#30340;&#32452;&#21512;&#12290;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#21253;&#21547;&#20843;&#20010;&#31867;&#21035;&#65288;&#26410;&#26631;&#35760;&#12289;&#20154;&#36896;&#22320;&#24418;&#12289;&#33258;&#28982;&#22320;&#24418;&#12289;&#39640;&#26893;&#34987;&#12289;&#20302;&#26893;&#34987;&#12289;&#24314;&#31569;&#12289;&#30828;&#26223;&#35266;&#65289;&#30340;4,499,0641&#20010;&#28857;&#25968;&#25454;&#38598;&#20013;&#23454;&#29616;&#20102;0.99&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05950v1 Announce Type: cross  Abstract: Accurate classification of objects in 3D point clouds is a significant problem in several applications, such as autonomous navigation and augmented/virtual reality scenarios, which has become a research hot spot. In this paper, we presented a deep learning strategy for 3D object classification in augmented reality. The proposed approach is a combination of the GRU and LSTM. LSTM networks learn longer dependencies well, but due to the number of gates, it takes longer to train; on the other hand, GRU networks have a weaker performance than LSTM, but their training speed is much higher than GRU, which is The speed is due to its fewer gates. The proposed approach used the combination of speed and accuracy of these two networks. The proposed approach achieved an accuracy of 0.99 in the 4,499,0641 points dataset, which includes eight classes (unlabeled, man-made terrain, natural terrain, high vegetation, low vegetation, buildings, hardscape,
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#26088;&#22312;&#36890;&#36807;&#24320;&#21457;&#19968;&#20010;&#36890;&#29992;&#22270;&#22522;&#30784;&#27169;&#22411;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#27867;&#21270;&#21040;&#19982;&#35757;&#32451;&#25968;&#25454;&#26174;&#33879;&#19981;&#21516;&#30340;&#26410;&#35265;&#22270;&#25968;&#25454;&#26102;&#36935;&#21040;&#30340;&#22256;&#38590;&#12290;</title><link>https://arxiv.org/abs/2403.01121</link><description>&lt;p&gt;
OpenGraph: &#36808;&#21521;&#24320;&#25918;&#22270;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
OpenGraph: Towards Open Graph Foundation Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01121
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#26088;&#22312;&#36890;&#36807;&#24320;&#21457;&#19968;&#20010;&#36890;&#29992;&#22270;&#22522;&#30784;&#27169;&#22411;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#27867;&#21270;&#21040;&#19982;&#35757;&#32451;&#25968;&#25454;&#26174;&#33879;&#19981;&#21516;&#30340;&#26410;&#35265;&#22270;&#25968;&#25454;&#26102;&#36935;&#21040;&#30340;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01121v1 &#20844;&#21578;&#31867;&#22411;: &#36328;&#20132;&#20114;   &#25688;&#35201;: &#22270;&#23398;&#20064;&#24050;&#25104;&#20026;&#35299;&#37322;&#21644;&#21033;&#29992;&#21508;&#39046;&#22495;&#30340;&#20851;&#31995;&#25968;&#25454;&#30340;&#19981;&#21487;&#25110;&#32570;&#37096;&#20998;&#65292;&#20174;&#25512;&#33616;&#31995;&#32479;&#21040;&#31038;&#20132;&#32593;&#32476;&#20998;&#26512;&#12290;&#22312;&#36825;&#31181;&#32972;&#26223;&#19979;&#65292;&#21508;&#31181;GNN&#24050;&#32463;&#25104;&#20026;&#32534;&#30721;&#22270;&#30340;&#32467;&#26500;&#20449;&#24687;&#30340;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#35770;&#65292;&#36890;&#36807;&#26377;&#25928;&#22320;&#25429;&#25417;&#22270;&#30340;&#28508;&#22312;&#32467;&#26500;&#65292;&#36825;&#20123;GNN&#24050;&#32463;&#23637;&#31034;&#20986;&#22312;&#22686;&#24378;&#22270;&#23398;&#20064;&#20219;&#21153;&#24615;&#33021;&#26041;&#38754;&#30340;&#24040;&#22823;&#28508;&#21147;&#65292;&#20363;&#22914;&#38142;&#25509;&#39044;&#27979;&#21644;&#33410;&#28857;&#20998;&#31867;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#19968;&#20010;&#26174;&#33879;&#30340;&#25361;&#25112;&#20173;&#28982;&#23384;&#22312;: &#36825;&#20123;&#20808;&#36827;&#26041;&#27861;&#36890;&#24120;&#22312;&#23558;&#26174;&#33879;&#19981;&#21516;&#20110;&#35757;&#32451;&#23454;&#20363;&#30340;&#26410;&#35265;&#22270;&#25968;&#25454;&#27867;&#21270;&#26102;&#36935;&#21040;&#22256;&#38590;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#36890;&#36807;&#24320;&#21457;&#19968;&#20010;&#36890;&#29992;&#22270;&#22522;&#30784;&#27169;&#22411;&#26469;&#25512;&#36827;&#22270;&#23398;&#20064;&#33539;&#24335;&#12290;&#35813;&#27169;&#22411;&#26088;&#22312;&#29702;&#35299;&#22810;&#26679;&#22270;&#25968;&#25454;&#20013;&#23384;&#22312;&#30340;&#22797;&#26434;&#25299;&#25169;&#27169;&#24335;&#65292;&#20351;&#20854;&#22312;&#38646;-shot&#24773;&#20917;&#19979;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01121v1 Announce Type: cross  Abstract: Graph learning has become indispensable for interpreting and harnessing relational data in diverse fields, ranging from recommendation systems to social network analysis. In this context, a variety of GNNs have emerged as promising methodologies for encoding the structural information of graphs. By effectively capturing the graph's underlying structure, these GNNs have shown great potential in enhancing performance in graph learning tasks, such as link prediction and node classification. However, despite their successes, a significant challenge persists: these advanced methods often face difficulties in generalizing to unseen graph data that significantly differs from the training instances. In this work, our aim is to advance the graph learning paradigm by developing a general graph foundation model. This model is designed to understand the complex topological patterns present in diverse graph data, enabling it to excel in zero-shot g
&lt;/p&gt;</description></item><item><title>MemoNav&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22270;&#20687;&#30446;&#26631;&#23548;&#33322;&#30340;&#26032;&#22411;&#35760;&#24518;&#27169;&#22411;&#65292;&#36890;&#36807;&#19977;&#31181;&#23548;&#33322;&#35760;&#24518;&#31867;&#22411;&#21644;&#36951;&#24536;&#27169;&#22359;&#25552;&#39640;&#20102;&#23548;&#33322;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.19161</link><description>&lt;p&gt;
MemoNav&#65306;&#35270;&#35273;&#23548;&#33322;&#30340;&#24037;&#20316;&#35760;&#24518;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
MemoNav: Working Memory Model for Visual Navigation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19161
&lt;/p&gt;
&lt;p&gt;
MemoNav&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22270;&#20687;&#30446;&#26631;&#23548;&#33322;&#30340;&#26032;&#22411;&#35760;&#24518;&#27169;&#22411;&#65292;&#36890;&#36807;&#19977;&#31181;&#23548;&#33322;&#35760;&#24518;&#31867;&#22411;&#21644;&#36951;&#24536;&#27169;&#22359;&#25552;&#39640;&#20102;&#23548;&#33322;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#30446;&#26631;&#23548;&#33322;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#38656;&#35201;&#19968;&#20010;agent&#22312;&#38476;&#29983;&#29615;&#22659;&#20013;&#23548;&#33322;&#21040;&#30001;&#22270;&#20687;&#25351;&#31034;&#30340;&#30446;&#26631;&#12290;&#29616;&#26377;&#26041;&#27861;&#21033;&#29992;&#19981;&#21516;&#30340;&#22330;&#26223;&#35760;&#24518;&#23384;&#22312;&#30528;&#25928;&#29575;&#20302;&#19979;&#30340;&#25506;&#32034;&#38382;&#39064;&#65292;&#22240;&#20026;&#23427;&#20204;&#21033;&#29992;&#20102;&#25152;&#26377;&#21382;&#21490;&#35266;&#23519;&#32467;&#26524;&#36827;&#34892;&#20915;&#31574;&#65292;&#32780;&#27809;&#26377;&#32771;&#34385;&#19982;&#30446;&#26631;&#30456;&#20851;&#30340;&#37096;&#20998;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MemoNav&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#29992;&#20110;&#22270;&#20687;&#30446;&#26631;&#23548;&#33322;&#30340;&#35760;&#24518;&#27169;&#22411;&#65292;&#23427;&#21033;&#29992;&#20102;&#31867;&#20284;&#24037;&#20316;&#35760;&#24518;&#30340;&#27969;&#31243;&#26469;&#25552;&#39640;&#23548;&#33322;&#24615;&#33021;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19977;&#31181;&#23548;&#33322;&#35760;&#24518;&#31867;&#22411;&#12290;&#22320;&#22270;&#19978;&#30340;&#33410;&#28857;&#29305;&#24449;&#23384;&#20648;&#22312;&#30701;&#26399;&#35760;&#24518;&#65288;STM&#65289;&#20013;&#65292;&#22240;&#20026;&#36825;&#20123;&#29305;&#24449;&#26159;&#21160;&#24577;&#26356;&#26032;&#30340;&#12290;&#28982;&#21518;&#65292;&#19968;&#20010;&#36951;&#24536;&#27169;&#22359;&#20445;&#30041;&#20449;&#24687;&#37327;&#22823;&#30340;STM&#37096;&#20998;&#20197;&#25552;&#39640;&#25928;&#29575;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#38271;&#26399;&#35760;&#24518;&#65288;LTM&#65289;&#26469;&#23398;&#20064;&#20840;&#23616;&#22330;&#26223;&#34920;&#31034;&#65292;&#36880;&#28176;&#32858;&#21512;STM&#29305;&#24449;&#12290;&#38543;&#21518;&#65292;&#19968;&#20010;&#22270;&#27880;&#24847;&#21147;&#27169;&#22359;&#23545;&#37325;&#26032;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19161v1 Announce Type: cross  Abstract: Image-goal navigation is a challenging task that requires an agent to navigate to a goal indicated by an image in unfamiliar environments. Existing methods utilizing diverse scene memories suffer from inefficient exploration since they use all historical observations for decision-making without considering the goal-relevant fraction. To address this limitation, we present MemoNav, a novel memory model for image-goal navigation, which utilizes a working memory-inspired pipeline to improve navigation performance. Specifically, we employ three types of navigation memory. The node features on a map are stored in the short-term memory (STM), as these features are dynamically updated. A forgetting module then retains the informative STM fraction to increase efficiency. We also introduce long-term memory (LTM) to learn global scene representations by progressively aggregating STM features. Subsequently, a graph attention module encodes the re
&lt;/p&gt;</description></item><item><title>&#23454;&#29616;&#20102;&#39318;&#20010;&#22522;&#20110;GPU&#30340;LTL&#23398;&#20064;&#22120;&#65292;&#20351;&#29992;&#26032;&#39062;&#30340;&#26522;&#20030;&#24335;&#31243;&#24207;&#21512;&#25104;&#65292;&#24615;&#33021;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#23398;&#20064;&#22120;&#65292;&#22788;&#29702;&#36319;&#36394;&#33267;&#23569;&#22810;2048&#20493;&#65292;&#36895;&#24230;&#24179;&#22343;&#24555;46&#20493;&#65292;&#24182;&#19988;&#24341;&#20837;&#20102;&#20855;&#26377;$O(\log n)$&#26102;&#38388;&#22797;&#26434;&#24230;&#30340;&#26080;&#20998;&#25903;LTL semantics&#12290;</title><link>https://arxiv.org/abs/2402.12373</link><description>&lt;p&gt;
&#22522;&#20110;GPU&#30340;LTL&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
LTL learning on GPUs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12373
&lt;/p&gt;
&lt;p&gt;
&#23454;&#29616;&#20102;&#39318;&#20010;&#22522;&#20110;GPU&#30340;LTL&#23398;&#20064;&#22120;&#65292;&#20351;&#29992;&#26032;&#39062;&#30340;&#26522;&#20030;&#24335;&#31243;&#24207;&#21512;&#25104;&#65292;&#24615;&#33021;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#23398;&#20064;&#22120;&#65292;&#22788;&#29702;&#36319;&#36394;&#33267;&#23569;&#22810;2048&#20493;&#65292;&#36895;&#24230;&#24179;&#22343;&#24555;46&#20493;&#65292;&#24182;&#19988;&#24341;&#20837;&#20102;&#20855;&#26377;$O(\log n)$&#26102;&#38388;&#22797;&#26434;&#24230;&#30340;&#26080;&#20998;&#25903;LTL semantics&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32447;&#24615;&#26102;&#24207;&#36923;&#36753;&#65288;LTL&#65289;&#22312;&#24037;&#19994;&#39564;&#35777;&#20013;&#34987;&#24191;&#27867;&#20351;&#29992;&#12290;LTL&#20844;&#24335;&#21487;&#20197;&#20174;&#36319;&#36394;&#20013;&#23398;&#20064;&#12290;&#25193;&#23637;LTL&#20844;&#24335;&#23398;&#20064;&#26159;&#19968;&#20010;&#24453;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#23454;&#29616;&#20102;&#31532;&#19968;&#31181;&#22522;&#20110;GPU&#30340;LTL&#23398;&#20064;&#22120;&#65292;&#20351;&#29992;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26522;&#20030;&#24335;&#31243;&#24207;&#21512;&#25104;&#12290;&#35813;&#23398;&#20064;&#22120;&#26159;&#23436;&#22791;&#21644;&#27491;&#30830;&#30340;&#12290;&#25105;&#20204;&#30340;&#22522;&#20934;&#27979;&#35797;&#34920;&#26126;&#65292;&#23427;&#22788;&#29702;&#30340;&#36319;&#36394;&#33267;&#23569;&#27604;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#23398;&#20064;&#22120;&#22810;2048&#20493;&#65292;&#24179;&#22343;&#33267;&#23569;&#24555;46&#20493;&#12290;&#36825;&#26159;&#36890;&#36807;&#35832;&#22810;&#26041;&#27861;&#23454;&#29616;&#30340;&#65292;&#21253;&#25324;&#20855;&#26377;$O(\log n)$&#26102;&#38388;&#22797;&#26434;&#24230;&#30340;&#26032;&#22411;&#26080;&#20998;&#25903;LTL&#35821;&#20041;&#65292;&#20854;&#20013;$n$&#26159;&#36319;&#36394;&#38271;&#24230;&#65292;&#32780;&#20197;&#21069;&#30340;&#23454;&#29616;&#26159;$O(n^2)$&#25110;&#26356;&#31967;&#65288;&#20551;&#35774;&#25353;&#20301;&#24067;&#23572;&#36816;&#31639;&#21644;&#25353;2&#30340;&#24130;&#31227;&#20301;&#20855;&#26377;&#21333;&#20301;&#25104;&#26412;&#8212;&#8212;&#36825;&#26159;&#23545;&#29616;&#20195;&#22788;&#29702;&#22120;&#30340;&#29616;&#23454;&#20551;&#35774;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12373v1 Announce Type: cross  Abstract: Linear temporal logic (LTL) is widely used in industrial verification. LTL formulae can be learned from traces. Scaling LTL formula learning is an open problem. We implement the first GPU-based LTL learner using a novel form of enumerative program synthesis. The learner is sound and complete. Our benchmarks indicate that it handles traces at least 2048 times more numerous, and on average at least 46 times faster than existing state-of-the-art learners. This is achieved with, among others, novel branch-free LTL semantics that has $O(\log n)$ time complexity, where $n$ is trace length, while previous implementations are $O(n^2)$ or worse (assuming bitwise boolean operations and shifts by powers of 2 have unit costs -- a realistic assumption on modern processors).
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#21333;&#19968;&#27169;&#22411;&#65292;&#29992;&#36739;&#23569;&#30340;&#21442;&#25968;&#23454;&#29616;&#19982;&#22522;&#32447;&#30456;&#24403;&#30340;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#24615;&#33021;</title><link>https://arxiv.org/abs/2402.11815</link><description>&lt;p&gt;
HU&#22312;SemEval-2024&#20219;&#21153;8A&#20013;&#30340;&#34920;&#29616;&#65306;&#23545;&#27604;&#23398;&#20064;&#33021;&#21542;&#23398;&#20064;&#23884;&#20837;&#20197;&#26816;&#27979;&#26426;&#22120;&#29983;&#25104;&#30340;&#25991;&#26412;&#65311;
&lt;/p&gt;
&lt;p&gt;
HU at SemEval-2024 Task 8A: Can Contrastive Learning Learn Embeddings to Detect Machine-Generated Text?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11815
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#21333;&#19968;&#27169;&#22411;&#65292;&#29992;&#36739;&#23569;&#30340;&#21442;&#25968;&#23454;&#29616;&#19982;&#22522;&#32447;&#30456;&#24403;&#30340;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25551;&#36848;&#20102;&#25105;&#20204;&#20026;SemEval-2024&#20219;&#21153;8&#8220;&#22810;&#29983;&#25104;&#22120;&#12289;&#22810;&#39046;&#22495;&#21644;&#22810;&#35821;&#35328;&#40657;&#21283;&#23376;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#8221;&#24320;&#21457;&#30340;&#31995;&#32479;&#12290;&#30001;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#34394;&#20551;&#25991;&#26412;&#29983;&#25104;&#12289;&#32593;&#32476;&#38035;&#40060;&#12289;&#32771;&#35797;&#20316;&#24330;&#29978;&#33267;&#25220;&#34989;&#29256;&#26435;&#26448;&#26009;&#20013;&#30340;&#20351;&#29992;&#65292;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#19968;&#30452;&#26159;&#20027;&#35201;&#20851;&#27880;&#30340;&#38382;&#39064;&#20043;&#19968;&#12290;&#35768;&#22810;&#31995;&#32479;&#24050;&#32463;&#34987;&#24320;&#21457;&#29992;&#20110;&#26816;&#27979;&#26426;&#22120;&#29983;&#25104;&#30340;&#25991;&#26412;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#31995;&#32479;&#20013;&#30340;&#22823;&#37096;&#20998;&#20381;&#36182;&#20110;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;&#65292;&#36825;&#26159;&#19968;&#20010;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#19981;&#20999;&#23454;&#38469;&#30340;&#38480;&#21046;&#65292;&#22240;&#20026;&#36890;&#24120;&#19981;&#21487;&#33021;&#30693;&#36947;&#29992;&#25143;&#29992;&#20110;&#25991;&#26412;&#29983;&#25104;&#30340;&#20855;&#20307;&#27169;&#22411;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#21333;&#19968;&#27169;&#22411;&#65292;&#20854;&#20351;&#29992;&#22522;&#32447;&#21442;&#25968;&#30340;&#22823;&#32422;40%&#65288;149M&#27604;355M&#65289;&#65292;&#20294;&#22312;&#27979;&#35797;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#20102;&#21487;&#27604;&#30340;&#24615;&#33021;&#65288;&#22312;137&#20010;&#21442;&#19982;&#32773;&#20013;&#25490;&#21517;&#31532;21&#65289;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#21457;&#29616;&#26159;&#65292;&#21363;&#20351;&#27809;&#26377;&#22810;&#20010;&#27169;&#22411;&#30340;&#38598;&#25104;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11815v1 Announce Type: cross  Abstract: This paper describes our system developed for SemEval-2024 Task 8, "Multigenerator, Multidomain, and Multilingual Black-Box Machine-Generated Text Detection." Machine-generated texts have been one of the main concerns due to the use of large language models (LLM) in fake text generation, phishing, cheating in exams, or even plagiarizing copyright materials. A lot of systems have been developed to detect machine-generated text. Nonetheless, the majority of these systems rely on the text-generating model, a limitation that is impractical in real-world scenarios, as it's often impossible to know which specific model the user has used for text generation. In this work, we propose a single model based on contrastive learning, which uses ~40% of the baseline's parameters (149M vs. 355M) but shows a comparable performance on the test dataset (21st out of 137 participants). Our key finding is that even without an ensemble of multiple models, a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#33521;&#35821;&#21644;&#24503;&#35821;&#21477;&#27861;&#35821;&#35328;&#21464;&#21270;&#36235;&#21183;&#65292;&#20351;&#29992;&#35758;&#20250;&#36777;&#35770;&#35821;&#26009;&#24211;&#65292;&#25506;&#35752;&#20102;&#21477;&#27861;&#20381;&#23384;&#36317;&#31163;&#26368;&#23567;&#21270;&#21450;&#22522;&#20110;&#26641;&#22270;&#23646;&#24615;&#30340;15&#20010;&#24230;&#37327;&#26631;&#20934;&#65292;&#25581;&#31034;&#20102;&#29616;&#20195;&#35299;&#26512;&#22120;&#22312;&#36825;&#31181;&#21464;&#21270;&#20013;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2402.11549</link><description>&lt;p&gt;
&#33521;&#35821;&#21644;&#24503;&#35821;&#30340;&#21477;&#27861;&#35821;&#35328;&#21464;&#21270;&#65306;&#24230;&#37327;&#12289;&#35299;&#26512;&#22120;&#21644;&#36235;&#21516;
&lt;/p&gt;
&lt;p&gt;
Syntactic Language Change in English and German: Metrics, Parsers, and Convergences
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11549
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#33521;&#35821;&#21644;&#24503;&#35821;&#21477;&#27861;&#35821;&#35328;&#21464;&#21270;&#36235;&#21183;&#65292;&#20351;&#29992;&#35758;&#20250;&#36777;&#35770;&#35821;&#26009;&#24211;&#65292;&#25506;&#35752;&#20102;&#21477;&#27861;&#20381;&#23384;&#36317;&#31163;&#26368;&#23567;&#21270;&#21450;&#22522;&#20110;&#26641;&#22270;&#23646;&#24615;&#30340;15&#20010;&#24230;&#37327;&#26631;&#20934;&#65292;&#25581;&#31034;&#20102;&#29616;&#20195;&#35299;&#26512;&#22120;&#22312;&#36825;&#31181;&#21464;&#21270;&#20013;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#30740;&#31350;&#34920;&#26126;&#65292;&#20154;&#31867;&#35821;&#35328;&#24448;&#24448;&#20250;&#20248;&#21270;&#35821;&#35328;&#32467;&#26500;&#20197;&#38477;&#20302;&#22797;&#26434;&#24615;&#65292;&#22686;&#21152;&#20132;&#27969;&#25928;&#29575;&#12290;&#21477;&#27861;&#20381;&#23384;&#36317;&#31163;&#34913;&#37327;&#20102;&#30456;&#20851;&#35789;&#27719;&#20043;&#38388;&#30340;&#32447;&#24615;&#36317;&#31163;&#65292;&#36890;&#24120;&#34987;&#35748;&#20026;&#26159;&#35821;&#35328;&#22788;&#29702;&#22256;&#38590;&#21644;&#24037;&#20316;&#35760;&#24518;&#36127;&#33655;&#30340;&#20851;&#38190;&#25351;&#26631;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#33521;&#35821;&#21644;&#24503;&#35821;&#21477;&#27861;&#35821;&#35328;&#21464;&#21270;&#30340;&#21382;&#26102;&#36235;&#21183;&#65292;&#20351;&#29992;&#20102;&#36807;&#21435;&#22823;&#32422;160&#24180;&#38388;&#30340;&#35758;&#20250;&#36777;&#35770;&#35821;&#26009;&#24211;&#12290;&#25105;&#20204;&#22522;&#20110;5&#20010;&#20381;&#23384;&#21477;&#27861;&#35299;&#26512;&#22120;&#30340;&#35266;&#23519;&#32467;&#26524;&#65292;&#21253;&#25324;&#24191;&#27867;&#20351;&#29992;&#30340;Stanford CoreNLP&#20197;&#21450;&#20854;&#20182;4&#20010;&#26356;&#26032;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;&#25105;&#20204;&#30340;&#21477;&#27861;&#35821;&#35328;&#21464;&#21270;&#20998;&#26512;&#36229;&#36234;&#20102;&#32447;&#24615;&#20381;&#23384;&#36317;&#31163;&#65292;&#25506;&#35752;&#20102;&#19982;&#20381;&#23384;&#36317;&#31163;&#26368;&#23567;&#21270;&#65288;DDM&#65289;&#30456;&#20851;&#30340;15&#20010;&#24230;&#37327;&#26631;&#20934;&#65292;&#25110;&#32773;&#22522;&#20110;&#26641;&#22270;&#23646;&#24615;&#65292;&#27604;&#22914;&#26641;&#39640;&#21644;&#24230;&#21464;&#24322;&#12290;&#23613;&#31649;&#25105;&#20204;&#26377;&#35777;&#25454;&#34920;&#26126;&#65292;&#26368;&#36817;&#22522;&#20110;&#29616;&#20195;&#26641;&#24211;&#35757;&#32451;&#30340;&#35299;&#26512;&#22120;&#24182;&#26410;&#21463;&#21040;&#37325;&#22823;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11549v1 Announce Type: cross  Abstract: Many studies have shown that human languages tend to optimize for lower complexity and increased communication efficiency. Syntactic dependency distance, which measures the linear distance between dependent words, is often considered a key indicator of language processing difficulty and working memory load. The current paper looks at diachronic trends in syntactic language change in both English and German, using corpora of parliamentary debates from the last c. 160 years. We base our observations on five dependency parsers, including the widely used Stanford CoreNLP as well as 4 newer alternatives. Our analysis of syntactic language change goes beyond linear dependency distance and explores 15 metrics relevant to dependency distance minimization (DDM) and/or based on tree graph properties, such as the tree height and degree variance. Even though we have evidence that recent parsers trained on modern treebanks are not heavily affected 
&lt;/p&gt;</description></item><item><title>Brant-2&#26159;&#33041;&#20449;&#21495;&#39046;&#22495;&#26368;&#22823;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#30456;&#27604;&#20110;Brant&#65292;&#23427;&#19981;&#20165;&#23545;&#25968;&#25454;&#21464;&#21270;&#21644;&#24314;&#27169;&#23610;&#24230;&#20855;&#26377;&#31283;&#20581;&#24615;&#65292;&#36824;&#33021;&#36866;&#29992;&#20110;&#26356;&#24191;&#27867;&#33539;&#22260;&#30340;&#33041;&#31070;&#32463;&#25968;&#25454;&#12290;</title><link>https://arxiv.org/abs/2402.10251</link><description>&lt;p&gt;
Brant-2&#65306;&#33041;&#20449;&#21495;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Brant-2: Foundation Model for Brain Signals
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10251
&lt;/p&gt;
&lt;p&gt;
Brant-2&#26159;&#33041;&#20449;&#21495;&#39046;&#22495;&#26368;&#22823;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#30456;&#27604;&#20110;Brant&#65292;&#23427;&#19981;&#20165;&#23545;&#25968;&#25454;&#21464;&#21270;&#21644;&#24314;&#27169;&#23610;&#24230;&#20855;&#26377;&#31283;&#20581;&#24615;&#65292;&#36824;&#33021;&#36866;&#29992;&#20110;&#26356;&#24191;&#27867;&#33539;&#22260;&#30340;&#33041;&#31070;&#32463;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#30784;&#27169;&#22411;&#21463;&#30410;&#20110;&#22312;&#22823;&#37327;&#26410;&#26631;&#35760;&#25968;&#25454;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#24182;&#19988;&#22312;&#23569;&#37327;&#26631;&#35760;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#33021;&#22815;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#36825;&#31181;&#27169;&#22411;&#22312;&#20998;&#26512;&#33041;&#20449;&#21495;&#26041;&#38754;&#29305;&#21035;&#26377;&#25928;&#65292;&#22240;&#20026;&#36825;&#19968;&#39046;&#22495;&#28085;&#30422;&#20102;&#20247;&#22810;&#24212;&#29992;&#22330;&#26223;&#65292;&#24182;&#19988;&#36827;&#34892;&#22823;&#35268;&#27169;&#27880;&#37322;&#26159;&#25104;&#26412;&#39640;&#26114;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#33041;&#20449;&#21495;&#39046;&#22495;&#26368;&#22823;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;Brant-2&#12290;&#19982;&#29992;&#20110;&#39045;&#20869;&#31070;&#32463;&#20449;&#21495;&#30340;&#22522;&#30784;&#27169;&#22411;Brant&#30456;&#27604;&#65292;Brant-2&#19981;&#20165;&#23545;&#25968;&#25454;&#21464;&#21270;&#21644;&#24314;&#27169;&#23610;&#24230;&#34920;&#29616;&#20986;&#31283;&#20581;&#24615;&#65292;&#32780;&#19988;&#21487;&#20197;&#24212;&#29992;&#20110;&#26356;&#24191;&#27867;&#33539;&#22260;&#30340;&#33041;&#31070;&#32463;&#25968;&#25454;&#12290;&#36890;&#36807;&#22312;&#22823;&#37327;&#20219;&#21153;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;Brant-2&#23545;&#33041;&#20449;&#21495;&#20013;&#21508;&#31181;&#24212;&#29992;&#22330;&#26223;&#30340;&#36866;&#24212;&#24615;&#12290;&#36827;&#19968;&#27493;&#20998;&#26512;&#25581;&#31034;&#20102;Brant-2&#30340;&#21487;&#25193;&#23637;&#24615;&#65292;&#39564;&#35777;&#20102;&#27599;&#20010;&#32452;&#20214;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#25105;&#20204;&#27169;&#22411;&#20445;&#25345;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10251v1 Announce Type: cross  Abstract: Foundational models benefit from pre-training on large amounts of unlabeled data and enable strong performance in a wide variety of applications with a small amount of labeled data. Such models can be particularly effective in analyzing brain signals, as this field encompasses numerous application scenarios, and it is costly to perform large-scale annotation. In this work, we present the largest foundation model in brain signals, Brant-2. Compared to Brant, a foundation model designed for intracranial neural signals, Brant-2 not only exhibits robustness towards data variations and modeling scales but also can be applied to a broader range of brain neural data. By experimenting on an extensive range of tasks, we demonstrate that Brant-2 is adaptive to various application scenarios in brain signals. Further analyses reveal the scalability of the Brant-2, validate each component's effectiveness, and showcase our model's ability to maintai
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;PRDP&#26041;&#27861;&#65292;&#36890;&#36807;&#36817;&#31471;&#22870;&#21169;&#24046;&#24322;&#39044;&#27979;&#23454;&#29616;&#20102;&#31283;&#23450;&#30340;&#40657;&#30418;&#22870;&#21169;&#24494;&#35843;&#25193;&#25955;&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;&#22823;&#35268;&#27169;&#25552;&#31034;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#19988;&#20855;&#26377;&#26356;&#22909;&#30340;&#35757;&#32451;&#31283;&#23450;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.08714</link><description>&lt;p&gt;
PRDP&#65306;&#22823;&#35268;&#27169;&#25193;&#25955;&#27169;&#22411;&#30340;&#36817;&#31471;&#22870;&#21169;&#24046;&#24322;&#39044;&#27979;&#29992;&#20110;&#22870;&#21169;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
PRDP: Proximal Reward Difference Prediction for Large-Scale Reward Finetuning of Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08714
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;PRDP&#26041;&#27861;&#65292;&#36890;&#36807;&#36817;&#31471;&#22870;&#21169;&#24046;&#24322;&#39044;&#27979;&#23454;&#29616;&#20102;&#31283;&#23450;&#30340;&#40657;&#30418;&#22870;&#21169;&#24494;&#35843;&#25193;&#25955;&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;&#22823;&#35268;&#27169;&#25552;&#31034;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#19988;&#20855;&#26377;&#26356;&#22909;&#30340;&#35757;&#32451;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22870;&#21169;&#24494;&#35843;&#24050;&#25104;&#20026;&#23558;&#22522;&#30784;&#27169;&#22411;&#19982;&#19979;&#28216;&#30446;&#26631;&#23545;&#40784;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#12290;&#22312;&#35821;&#35328;&#39046;&#22495;&#65292;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26469;&#26368;&#22823;&#21270;&#21453;&#26144;&#20154;&#31867;&#20559;&#22909;&#30340;&#22870;&#21169;&#24050;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#22312;&#35270;&#35273;&#39046;&#22495;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;RL&#30340;&#22870;&#21169;&#24494;&#35843;&#26041;&#27861;&#22312;&#22823;&#35268;&#27169;&#35757;&#32451;&#20013;&#23384;&#22312;&#19981;&#31283;&#23450;&#24615;&#65292;&#20351;&#23427;&#20204;&#26080;&#27861;&#25512;&#24191;&#21040;&#22797;&#26434;&#30340;&#12289;&#26410;&#30693;&#30340;&#25552;&#31034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36817;&#31471;&#22870;&#21169;&#24046;&#24322;&#39044;&#27979;&#65288;PRDP&#65289;&#65292;&#39318;&#27425;&#22312;&#36229;&#36807;100K&#20010;&#25552;&#31034;&#30340;&#22823;&#35268;&#27169;&#25552;&#31034;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#31283;&#23450;&#30340;&#40657;&#30418;&#22870;&#21169;&#24494;&#35843;&#25193;&#25955;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#21019;&#26032;&#26159;&#22870;&#21169;&#24046;&#24322;&#39044;&#27979;&#65288;RDP&#65289;&#30446;&#26631;&#65292;&#35813;&#30446;&#26631;&#19982;RL&#30446;&#26631;&#20855;&#26377;&#30456;&#21516;&#30340;&#26368;&#20248;&#35299;&#65292;&#21516;&#26102;&#20139;&#21463;&#26356;&#22909;&#30340;&#35757;&#32451;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08714v1 Announce Type: cross Abstract: Reward finetuning has emerged as a promising approach to aligning foundation models with downstream objectives. Remarkable success has been achieved in the language domain by using reinforcement learning (RL) to maximize rewards that reflect human preference. However, in the vision domain, existing RL-based reward finetuning methods are limited by their instability in large-scale training, rendering them incapable of generalizing to complex, unseen prompts. In this paper, we propose Proximal Reward Difference Prediction (PRDP), enabling stable black-box reward finetuning for diffusion models for the first time on large-scale prompt datasets with over 100K prompts. Our key innovation is the Reward Difference Prediction (RDP) objective that has the same optimal solution as the RL objective while enjoying better training stability. Specifically, the RDP objective is a supervised regression objective that tasks the diffusion model with pred
&lt;/p&gt;</description></item><item><title>&#37325;&#26032;&#26500;&#24819;&#30340;&#35770;&#25991;&#25552;&#20986;&#20102;&#26410;&#26469;&#25351;&#25381;&#19982;&#25511;&#21046;&#65288;C2&#65289;&#20915;&#31574;&#38656;&#35201;&#38754;&#23545;&#26356;&#22797;&#26434;&#21644;&#25361;&#25112;&#24615;&#30340;&#29615;&#22659;&#65292;&#22240;&#27492;&#25552;&#20986;&#20102;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#19982;&#20154;&#31867;&#24378;&#26377;&#21147;&#20249;&#20276;&#20851;&#31995;&#30340;&#26410;&#26469;C2&#30340;&#24895;&#26223;&#12290;&#36825;&#20010;&#24895;&#26223;&#30340;&#26680;&#24515;&#26159;&#20248;&#21270;C2&#25805;&#20316;&#27969;&#31243;&#65292;&#20445;&#25345;&#21327;&#21516;&#21162;&#21147;&#65292;&#21457;&#23637;&#33258;&#36866;&#24212;&#30340;&#38598;&#20307;&#30693;&#35782;&#31995;&#32479;&#12290;</title><link>https://arxiv.org/abs/2402.07946</link><description>&lt;p&gt;
&#37325;&#26032;&#26500;&#24819;&#25351;&#25381;&#19982;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Re-Envisioning Command and Control
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07946
&lt;/p&gt;
&lt;p&gt;
&#37325;&#26032;&#26500;&#24819;&#30340;&#35770;&#25991;&#25552;&#20986;&#20102;&#26410;&#26469;&#25351;&#25381;&#19982;&#25511;&#21046;&#65288;C2&#65289;&#20915;&#31574;&#38656;&#35201;&#38754;&#23545;&#26356;&#22797;&#26434;&#21644;&#25361;&#25112;&#24615;&#30340;&#29615;&#22659;&#65292;&#22240;&#27492;&#25552;&#20986;&#20102;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#19982;&#20154;&#31867;&#24378;&#26377;&#21147;&#20249;&#20276;&#20851;&#31995;&#30340;&#26410;&#26469;C2&#30340;&#24895;&#26223;&#12290;&#36825;&#20010;&#24895;&#26223;&#30340;&#26680;&#24515;&#26159;&#20248;&#21270;C2&#25805;&#20316;&#27969;&#31243;&#65292;&#20445;&#25345;&#21327;&#21516;&#21162;&#21147;&#65292;&#21457;&#23637;&#33258;&#36866;&#24212;&#30340;&#38598;&#20307;&#30693;&#35782;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26410;&#26469;&#30340;&#25112;&#20105;&#23558;&#35201;&#27714;&#22312;&#26356;&#22797;&#26434;&#12289;&#24555;&#33410;&#22863;&#12289;&#19981;&#32467;&#26500;&#21270;&#21644;&#26497;&#20855;&#25361;&#25112;&#24615;&#30340;&#29615;&#22659;&#20013;&#36827;&#34892;&#25351;&#25381;&#19982;&#25511;&#21046;&#65288;C2&#65289;&#20915;&#31574;&#12290;C2&#23558;&#22240;&#34987;&#25298;&#32477;&#12289;&#36864;&#21270;&#12289;&#38388;&#27463;&#21644;&#26377;&#38480;&#30340;&#36890;&#20449;&#20197;&#21450;&#38656;&#35201;&#32771;&#34385;&#21040;&#22810;&#20010;&#20316;&#25112;&#39046;&#22495;&#20013;&#30340;&#35768;&#22810;&#25968;&#25454;&#27969;&#32780;&#21464;&#24471;&#26356;&#21152;&#22797;&#26434;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;C2&#23454;&#36341;&#8212;&#8212;&#28304;&#33258;&#24037;&#19994;&#26102;&#20195;&#32780;&#38750;&#26032;&#20852;&#30340;&#26234;&#33021;&#26102;&#20195;&#8212;&#8212;&#26159;&#32447;&#24615;&#30340;&#19988;&#32791;&#26102;&#12290;&#32780;&#19988;&#65292;&#36825;&#20123;&#26041;&#27861;&#21487;&#33021;&#26080;&#27861;&#22312;&#26410;&#26469;&#25112;&#22330;&#19978;&#19982;&#23545;&#25163;&#20445;&#25345;&#20248;&#21183;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#31995;&#32479;&#19982;&#20154;&#31867;&#20043;&#38388;&#24378;&#26377;&#21147;&#20249;&#20276;&#20851;&#31995;&#30340;&#26410;&#26469;C2&#24895;&#26223;&#12290;&#36825;&#20010;&#26410;&#26469;&#24895;&#26223;&#20307;&#29616;&#22312;&#19977;&#20010;&#36816;&#33829;&#24433;&#21709;&#19978;&#65306;&#20248;&#21270;C2&#25805;&#20316;&#27969;&#31243;&#65292;&#20445;&#25345;&#21327;&#21516;&#21162;&#21147;&#65292;&#20197;&#21450;&#21457;&#23637;&#33258;&#36866;&#24212;&#30340;&#38598;&#20307;&#30693;&#35782;&#31995;&#32479;&#12290;&#26412;&#25991;&#38416;&#36848;&#20102;&#25152;&#35774;&#24819;&#30340;&#26410;&#26469;&#25351;&#25381;&#19982;&#25511;&#21046;&#30340;&#24895;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
Future warfare will require Command and Control (C2) decision-making to occur in more complex, fast-paced, ill-structured, and demanding conditions. C2 will be further complicated by operational challenges such as Denied, Degraded, Intermittent, and Limited (DDIL) communications and the need to account for many data streams, potentially across multiple domains of operation. Yet, current C2 practices -- which stem from the industrial era rather than the emerging intelligence era -- are linear and time-consuming. Critically, these approaches may fail to maintain overmatch against adversaries on the future battlefield. To address these challenges, we propose a vision for future C2 based on robust partnerships between humans and artificial intelligence (AI) systems. This future vision is encapsulated in three operational impacts: streamlining the C2 operations process, maintaining unity of effort, and developing adaptive collective knowledge systems. This paper illustrates the envisaged fu
&lt;/p&gt;</description></item><item><title>&#26410;&#26469;&#25112;&#20105;&#23558;&#38656;&#35201;&#25351;&#25381;&#19982;&#25511;&#21046;&#65288;C2&#65289;&#20154;&#21592;&#22312;&#22797;&#26434;&#19988;&#28508;&#22312;&#27169;&#31946;&#30340;&#24773;&#20917;&#19979;&#20197;&#26356;&#30701;&#30340;&#26102;&#38388;&#20869;&#20570;&#20986;&#20915;&#31574;&#12290;&#26412;&#35770;&#25991;&#36890;&#36807;&#21033;&#29992;&#20114;&#21160;&#24335;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#32467;&#21512;&#20154;&#24037;&#26234;&#33021;&#21644;&#20154;&#31867;&#26234;&#33021;&#65292;&#20197;&#25552;&#39640;C2&#36816;&#20316;&#30340;&#36866;&#24212;&#24615;&#21644;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.06501</link><description>&lt;p&gt;
&#21487;&#25193;&#23637;&#20114;&#21160;&#24335;&#26426;&#22120;&#23398;&#20064;&#29992;&#20110;&#26410;&#26469;&#25351;&#25381;&#19982;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Scalable Interactive Machine Learning for Future Command and Control
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06501
&lt;/p&gt;
&lt;p&gt;
&#26410;&#26469;&#25112;&#20105;&#23558;&#38656;&#35201;&#25351;&#25381;&#19982;&#25511;&#21046;&#65288;C2&#65289;&#20154;&#21592;&#22312;&#22797;&#26434;&#19988;&#28508;&#22312;&#27169;&#31946;&#30340;&#24773;&#20917;&#19979;&#20197;&#26356;&#30701;&#30340;&#26102;&#38388;&#20869;&#20570;&#20986;&#20915;&#31574;&#12290;&#26412;&#35770;&#25991;&#36890;&#36807;&#21033;&#29992;&#20114;&#21160;&#24335;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#32467;&#21512;&#20154;&#24037;&#26234;&#33021;&#21644;&#20154;&#31867;&#26234;&#33021;&#65292;&#20197;&#25552;&#39640;C2&#36816;&#20316;&#30340;&#36866;&#24212;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26410;&#26469;&#25112;&#20105;&#23558;&#38656;&#35201;&#25351;&#25381;&#19982;&#25511;&#21046;&#65288;C2&#65289;&#20154;&#21592;&#22312;&#22797;&#26434;&#19988;&#28508;&#22312;&#27169;&#31946;&#30340;&#24773;&#20917;&#19979;&#20197;&#26356;&#30701;&#30340;&#26102;&#38388;&#20869;&#20570;&#20986;&#20915;&#31574;&#12290;&#37492;&#20110;&#38656;&#35201;&#24378;&#22823;&#30340;&#20915;&#31574;&#36807;&#31243;&#21644;&#20915;&#31574;&#25903;&#25345;&#24037;&#20855;&#65292;&#20154;&#24037;&#26234;&#33021;&#21644;&#20154;&#31867;&#26234;&#33021;&#30340;&#38598;&#25104;&#20855;&#26377;&#38761;&#21629;&#24615;&#22320;&#25913;&#21464;C2&#36816;&#20316;&#27969;&#31243;&#30340;&#28508;&#21147;&#65292;&#20197;&#30830;&#20445;&#22312;&#24555;&#36895;&#21464;&#21270;&#30340;&#25805;&#20316;&#29615;&#22659;&#20013;&#30340;&#36866;&#24212;&#24615;&#21644;&#25928;&#29575;&#12290;&#25105;&#20204;&#25552;&#35758;&#21033;&#29992;&#26368;&#36817;&#22312;&#20114;&#21160;&#24335;&#26426;&#22120;&#23398;&#20064;&#26041;&#38754;&#21462;&#24471;&#30340;&#31361;&#30772;&#65292;&#20154;&#31867;&#21487;&#20197;&#19982;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#21512;&#20316;&#20197;&#25351;&#23548;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#34892;&#20026;&#12290;&#26412;&#25991;&#30830;&#23450;&#20102;&#30446;&#21069;&#31185;&#25216;&#21457;&#23637;&#20013;&#23384;&#22312;&#30340;&#20960;&#20010;&#24046;&#36317;&#65292;&#26410;&#26469;&#30340;&#24037;&#20316;&#24212;&#35813;&#35299;&#20915;&#36825;&#20123;&#24046;&#36317;&#65292;&#20197;&#25193;&#23637;&#36825;&#20123;&#26041;&#27861;&#22312;&#22797;&#26434;&#30340;C2&#29615;&#22659;&#20013;&#21457;&#25381;&#20316;&#29992;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#19977;&#20010;&#30740;&#31350;&#37325;&#28857;&#39046;&#22495;&#65292;&#20849;&#21516;&#26088;&#22312;&#23454;&#29616;&#21487;&#25193;&#23637;&#30340;&#20114;&#21160;&#24335;&#26426;&#22120;&#23398;&#20064;&#65288;SIML&#65289;&#65306;1&#65289;&#24320;&#21457;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#20132;&#20114;&#31639;&#27861;&#20197;&#23454;&#29616;&#21327;&#21516;&#35268;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;
Future warfare will require Command and Control (C2) personnel to make decisions at shrinking timescales in complex and potentially ill-defined situations. Given the need for robust decision-making processes and decision-support tools, integration of artificial and human intelligence holds the potential to revolutionize the C2 operations process to ensure adaptability and efficiency in rapidly changing operational environments. We propose to leverage recent promising breakthroughs in interactive machine learning, in which humans can cooperate with machine learning algorithms to guide machine learning algorithm behavior. This paper identifies several gaps in state-of-the-art science and technology that future work should address to extend these approaches to function in complex C2 contexts. In particular, we describe three research focus areas that together, aim to enable scalable interactive machine learning (SIML): 1) developing human-AI interaction algorithms to enable planning in co
&lt;/p&gt;</description></item><item><title>COA-GPT&#26159;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24555;&#36895;&#39640;&#25928;&#29983;&#25104;&#26377;&#25928;&#34892;&#21160;&#26041;&#26696;&#30340;&#31639;&#27861;&#65292;&#23427;&#34701;&#21512;&#20102;&#20891;&#20107;&#23398;&#35828;&#21644;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#65292;&#24182;&#22312;&#20891;&#20107;&#28216;&#25103;&#20013;&#30340;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#20854;&#24555;&#36895;&#29983;&#25104;&#25112;&#30053;&#21512;&#29702;COAs&#30340;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2402.01786</link><description>&lt;p&gt;
COA-GPT&#65306;&#29992;&#20110;&#20891;&#20107;&#34892;&#21160;&#20013;&#21152;&#36895;&#34892;&#21160;&#26041;&#26696;&#24320;&#21457;&#30340;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#21464;&#21387;&#22120;
&lt;/p&gt;
&lt;p&gt;
COA-GPT: Generative Pre-trained Transformers for Accelerated Course of Action Development in Military Operations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01786
&lt;/p&gt;
&lt;p&gt;
COA-GPT&#26159;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24555;&#36895;&#39640;&#25928;&#29983;&#25104;&#26377;&#25928;&#34892;&#21160;&#26041;&#26696;&#30340;&#31639;&#27861;&#65292;&#23427;&#34701;&#21512;&#20102;&#20891;&#20107;&#23398;&#35828;&#21644;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#65292;&#24182;&#22312;&#20891;&#20107;&#28216;&#25103;&#20013;&#30340;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#20854;&#24555;&#36895;&#29983;&#25104;&#25112;&#30053;&#21512;&#29702;COAs&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20891;&#20107;&#34892;&#21160;&#20013;&#34892;&#21160;&#26041;&#26696;&#65288;COAs&#65289;&#30340;&#24320;&#21457;&#20256;&#32479;&#19978;&#26159;&#19968;&#20010;&#32791;&#26102;&#19988;&#22797;&#26434;&#30340;&#36807;&#31243;&#12290;&#38024;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;COA-GPT&#65292;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24555;&#36895;&#39640;&#25928;&#29983;&#25104;&#26377;&#25928;COAs&#30340;&#26032;&#31639;&#27861;&#12290;COA-GPT&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#23558;&#20891;&#20107;&#23398;&#35828;&#21644;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#34701;&#20837;&#21040;LLMs&#20013;&#65292;&#20801;&#35768;&#25351;&#25381;&#23448;&#36755;&#20837;&#20219;&#21153;&#20449;&#24687;&#65288;&#21253;&#25324;&#25991;&#26412;&#21644;&#22270;&#20687;&#26684;&#24335;&#65289;&#65292;&#24182;&#33719;&#24471;&#19982;&#25112;&#30053;&#23545;&#40784;&#30340;COAs&#20197;&#20379;&#23457;&#26597;&#21644;&#25209;&#20934;&#12290;&#29420;&#29305;&#30340;&#26159;&#65292;COA-GPT&#19981;&#20165;&#21152;&#36895;&#20102;COA&#30340;&#24320;&#21457;&#65292;&#22312;&#20960;&#31186;&#38047;&#20869;&#29983;&#25104;&#21021;&#22987;COAs&#65292;&#36824;&#33021;&#26681;&#25454;&#25351;&#25381;&#23448;&#30340;&#21453;&#39304;&#23454;&#26102;&#31934;&#32454;&#21270;&#25913;&#36827;&#12290;&#26412;&#30740;&#31350;&#22312;&#12298;&#26143;&#38469;&#20105;&#38712;II&#12299;&#28216;&#25103;&#30340;&#20891;&#20107;&#30456;&#20851;&#22330;&#26223;&#20013;&#35780;&#20272;&#20102;COA-GPT&#65292;&#23558;&#20854;&#24615;&#33021;&#19982;&#26368;&#20808;&#36827;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;COA-GPT&#22312;&#26356;&#24555;&#29983;&#25104;&#25112;&#30053;&#21512;&#29702;&#30340;COAs&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
The development of Courses of Action (COAs) in military operations is traditionally a time-consuming and intricate process. Addressing this challenge, this study introduces COA-GPT, a novel algorithm employing Large Language Models (LLMs) for rapid and efficient generation of valid COAs. COA-GPT incorporates military doctrine and domain expertise to LLMs through in-context learning, allowing commanders to input mission information - in both text and image formats - and receive strategically aligned COAs for review and approval. Uniquely, COA-GPT not only accelerates COA development, producing initial COAs within seconds, but also facilitates real-time refinement based on commander feedback. This work evaluates COA-GPT in a military-relevant scenario within a militarized version of the StarCraft II game, comparing its performance against state-of-the-art reinforcement learning algorithms. Our results demonstrate COA-GPT's superiority in generating strategically sound COAs more swiftly, 
&lt;/p&gt;</description></item><item><title>HQ-VAE&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#26694;&#26550;&#65292;&#21033;&#29992;&#21464;&#20998;&#36125;&#21494;&#26031;&#22312;&#20998;&#23618;&#32467;&#26500;&#20013;&#38543;&#26426;&#23398;&#20064;&#31163;&#25955;&#34920;&#31034;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;VQ-VAE&#20013;&#30340;&#30721;&#20070;/&#23618;&#22349;&#22604;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2401.00365</link><description>&lt;p&gt;
HQ-VAE&#65306;&#20855;&#26377;&#21464;&#20998;&#36125;&#21494;&#26031;&#30340;&#20998;&#23618;&#31163;&#25955;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
HQ-VAE: Hierarchical Discrete Representation Learning with Variational Bayes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.00365
&lt;/p&gt;
&lt;p&gt;
HQ-VAE&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#26694;&#26550;&#65292;&#21033;&#29992;&#21464;&#20998;&#36125;&#21494;&#26031;&#22312;&#20998;&#23618;&#32467;&#26500;&#20013;&#38543;&#26426;&#23398;&#20064;&#31163;&#25955;&#34920;&#31034;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;VQ-VAE&#20013;&#30340;&#30721;&#20070;/&#23618;&#22349;&#22604;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21521;&#37327;&#37327;&#21270;&#65288;VQ&#65289;&#26159;&#19968;&#31181;&#30830;&#23450;&#24615;&#23398;&#20064;&#20855;&#26377;&#31163;&#25955;&#30721;&#20070;&#34920;&#31034;&#30340;&#29305;&#24449;&#30340;&#25216;&#26415;&#12290;&#36890;&#24120;&#36890;&#36807;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#27169;&#22411; VQ-VAE &#26469;&#25191;&#34892;&#65292;&#21487;&#20197;&#36827;&#19968;&#27493;&#25193;&#23637;&#21040;&#20998;&#23618;&#32467;&#26500;&#20197;&#36827;&#34892;&#39640;&#20445;&#30495;&#37325;&#24314;&#12290;&#28982;&#32780;&#65292;VQ-VAE &#30340;&#36825;&#31181;&#20998;&#23618;&#25193;&#23637;&#32463;&#24120;&#21463;&#21040;&#30721;&#20070;/&#23618;&#22349;&#22604;&#38382;&#39064;&#30340;&#22256;&#25200;&#65292;&#20854;&#20013;&#30721;&#20070;&#26410;&#34987;&#26377;&#25928;&#22320;&#29992;&#26469;&#34920;&#36798;&#25968;&#25454;&#65292;&#20174;&#32780;&#38477;&#20302;&#37325;&#24314;&#31934;&#24230;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#22312;&#21464;&#20998;&#36125;&#21494;&#26031;&#26694;&#26550;&#30340;&#22522;&#30784;&#19978;&#38543;&#26426;&#23398;&#20064;&#20998;&#23618;&#31163;&#25955;&#34920;&#31034;&#65292;&#31216;&#20026;&#20998;&#23618;&#37327;&#21270;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;HQ-VAE&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.00365v2 Announce Type: replace-cross  Abstract: Vector quantization (VQ) is a technique to deterministically learn features with discrete codebook representations. It is commonly performed with a variational autoencoding model, VQ-VAE, which can be further extended to hierarchical structures for making high-fidelity reconstructions. However, such hierarchical extensions of VQ-VAE often suffer from the codebook/layer collapse issue, where the codebook is not efficiently used to express the data, and hence degrades reconstruction accuracy. To mitigate this problem, we propose a novel unified framework to stochastically learn hierarchical discrete representation on the basis of the variational Bayes framework, called hierarchically quantized variational autoencoder (HQ-VAE). HQ-VAE naturally generalizes the hierarchical variants of VQ-VAE, such as VQ-VAE-2 and residual-quantized VAE (RQ-VAE), and provides them with a Bayesian training scheme. Our comprehensive experiments on im
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25361;&#25112;&#20102;&#23454;&#20307;&#30456;&#20284;&#24615;&#22312;&#22270;&#20013;&#22312;&#23884;&#20837;&#31354;&#38388;&#20013;&#33258;&#28982;&#21453;&#26144;&#30340;&#20027;&#27969;&#20551;&#35774;&#65292;&#36890;&#36807;&#36827;&#34892;&#24191;&#27867;&#30340;&#23454;&#39564;&#26469;&#34913;&#37327;&#36825;&#31181;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2312.10370</link><description>&lt;p&gt;
&#30456;&#20284;&#30340;&#23454;&#20307;&#26159;&#21542;&#20855;&#26377;&#30456;&#20284;&#30340;&#23884;&#20837;&#65311;
&lt;/p&gt;
&lt;p&gt;
Do Similar Entities have Similar Embeddings?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.10370
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25361;&#25112;&#20102;&#23454;&#20307;&#30456;&#20284;&#24615;&#22312;&#22270;&#20013;&#22312;&#23884;&#20837;&#31354;&#38388;&#20013;&#33258;&#28982;&#21453;&#26144;&#30340;&#20027;&#27969;&#20551;&#35774;&#65292;&#36890;&#36807;&#36827;&#34892;&#24191;&#27867;&#30340;&#23454;&#39564;&#26469;&#34913;&#37327;&#36825;&#31181;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#36827;&#34892;&#38142;&#25509;&#39044;&#27979;&#32780;&#24320;&#21457;&#30340;&#30693;&#35782;&#22270;&#23884;&#20837;&#27169;&#22411;&#65288;KGEMs&#65289;&#23398;&#20064;&#30693;&#35782;&#22270;&#20013;&#23454;&#20307;&#30340;&#21521;&#37327;&#34920;&#31034;&#65292;&#21363;&#23884;&#20837;&#12290;&#19968;&#20010;&#26222;&#36941;&#30340;&#40664;&#35748;&#20551;&#35774;&#26159;KGE&#23454;&#20307;&#30456;&#20284;&#24615;&#20551;&#35774;&#65292;&#21363;&#36825;&#20123;KGEMs&#22312;&#23427;&#20204;&#30340;&#23884;&#20837;&#31354;&#38388;&#20013;&#20445;&#30041;&#22270;&#30340;&#32467;&#26500;&#65292;&#21363;&#23558;&#30456;&#20284;&#30340;&#23454;&#20307;&#25918;&#22312;&#22270;&#20013;&#24444;&#27492;&#38752;&#36817;&#12290;&#36825;&#31181;&#29702;&#24819;&#30340;&#24615;&#36136;&#20351;&#24471;KGEMs&#22312;&#25512;&#33616;&#31995;&#32479;&#25110;&#33647;&#29289;&#20877;&#21033;&#29992;&#31561;&#19979;&#28216;&#20219;&#21153;&#20013;&#34987;&#24191;&#27867;&#20351;&#29992;&#12290;&#28982;&#32780;&#65292;&#23454;&#20307;&#30340;&#30456;&#20284;&#24615;&#19982;&#23884;&#20837;&#31354;&#38388;&#20013;&#30340;&#30456;&#20284;&#24615;&#20043;&#38388;&#30340;&#20851;&#31995;&#24456;&#23569;&#34987;&#27491;&#24335;&#35780;&#20272;&#12290;&#36890;&#24120;&#65292;KGEMs&#26159;&#22522;&#20110;&#20854;&#21807;&#19968;&#30340;&#38142;&#25509;&#39044;&#27979;&#33021;&#21147;&#36827;&#34892;&#35780;&#20272;&#30340;&#65292;&#20351;&#29992;&#31867;&#20284;Hits@K&#25110;Mean Rank&#30340;&#25490;&#21517;&#25351;&#26631;&#12290;&#26412;&#25991;&#36136;&#30097;&#20102;&#22270;&#20013;&#30340;&#23454;&#20307;&#30456;&#20284;&#24615;&#22312;&#23884;&#20837;&#31354;&#38388;&#20013;&#22825;&#28982;&#21453;&#26144;&#36825;&#19968;&#27969;&#34892;&#20551;&#35774;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#26469;&#34913;&#37327;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.10370v2 Announce Type: replace  Abstract: Knowledge graph embedding models (KGEMs) developed for link prediction learn vector representations for entities in a knowledge graph, known as embeddings. A common tacit assumption is the KGE entity similarity assumption, which states that these KGEMs retain the graph's structure within their embedding space, \textit{i.e.}, position similar entities within the graph close to one another. This desirable property make KGEMs widely used in downstream tasks such as recommender systems or drug repurposing. Yet, the relation of entity similarity and similarity in the embedding space has rarely been formally evaluated. Typically, KGEMs are assessed based on their sole link prediction capabilities, using ranked-based metrics such as Hits@K or Mean Rank. This paper challenges the prevailing assumption that entity similarity in the graph is inherently mirrored in the embedding space. Therefore, we conduct extensive experiments to measure the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38754;&#21521;&#24320;&#25918;&#25968;&#25454;&#38598;&#21644;&#36127;&#36131;&#20219;&#20154;&#24037;&#26234;&#33021;&#35780;&#20272;&#30340;&#26426;&#22120;&#21487;&#35835;&#25991;&#26723;&#26694;&#26550;&#65292;&#26088;&#22312;&#25552;&#39640;&#25968;&#25454;&#38598;&#30340;&#21487;&#29702;&#35299;&#24615;&#21644;&#21487;&#29992;&#24615;&#65292;&#31616;&#21270;&#25968;&#25454;&#38598;&#35780;&#20272;&#36807;&#31243;&#65292;&#20419;&#36827;&#26356;&#21487;&#38752;&#30340;&#25968;&#25454;&#24212;&#29992;&#65292;&#20174;&#32780;&#22521;&#32946;&#26356;&#36127;&#36131;&#20219;&#21644;&#21487;&#20449;&#36182;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#12290;</title><link>https://arxiv.org/abs/2312.06153</link><description>&lt;p&gt;
&#24320;&#25918;&#25968;&#25454;&#34920;&#65306;&#38754;&#21521;&#24320;&#25918;&#25968;&#25454;&#38598;&#21644;&#36127;&#36131;&#20219;&#20154;&#24037;&#26234;&#33021;&#35780;&#20272;&#30340;&#26426;&#22120;&#21487;&#35835;&#25991;&#26723;
&lt;/p&gt;
&lt;p&gt;
Open Datasheets: Machine-readable Documentation for Open Datasets and Responsible AI Assessments
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.06153
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38754;&#21521;&#24320;&#25918;&#25968;&#25454;&#38598;&#21644;&#36127;&#36131;&#20219;&#20154;&#24037;&#26234;&#33021;&#35780;&#20272;&#30340;&#26426;&#22120;&#21487;&#35835;&#25991;&#26723;&#26694;&#26550;&#65292;&#26088;&#22312;&#25552;&#39640;&#25968;&#25454;&#38598;&#30340;&#21487;&#29702;&#35299;&#24615;&#21644;&#21487;&#29992;&#24615;&#65292;&#31616;&#21270;&#25968;&#25454;&#38598;&#35780;&#20272;&#36807;&#31243;&#65292;&#20419;&#36827;&#26356;&#21487;&#38752;&#30340;&#25968;&#25454;&#24212;&#29992;&#65292;&#20174;&#32780;&#22521;&#32946;&#26356;&#36127;&#36131;&#20219;&#21644;&#21487;&#20449;&#36182;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26080;&#20195;&#30721;&#30340;&#12289;&#38754;&#21521;&#24320;&#25918;&#25968;&#25454;&#38598;&#30340;&#26426;&#22120;&#21487;&#35835;&#25991;&#26723;&#26694;&#26550;&#65292;&#37325;&#28857;&#20851;&#27880;&#36127;&#36131;&#20219;&#20154;&#24037;&#26234;&#33021;&#65288;RAI&#65289;&#32771;&#34385;&#22240;&#32032;&#12290;&#35813;&#26694;&#26550;&#26088;&#22312;&#25552;&#39640;&#24320;&#25918;&#25968;&#25454;&#38598;&#30340;&#21487;&#29702;&#35299;&#24615;&#21644;&#21487;&#29992;&#24615;&#65292;&#20419;&#36827;&#26356;&#23481;&#26131;&#22320;&#21457;&#29616;&#21644;&#20351;&#29992;&#25968;&#25454;&#38598;&#65292;&#26356;&#22909;&#22320;&#29702;&#35299;&#20869;&#23481;&#21644;&#19978;&#19979;&#25991;&#65292;&#20197;&#21450;&#35780;&#20272;&#25968;&#25454;&#38598;&#30340;&#36136;&#37327;&#21644;&#20934;&#30830;&#24615;&#12290;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#26088;&#22312;&#31616;&#21270;&#25968;&#25454;&#38598;&#30340;&#35780;&#20272;&#65292;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#12289;&#25968;&#25454;&#31185;&#23398;&#23478;&#21644;&#20854;&#20182;&#24320;&#25918;&#25968;&#25454;&#29992;&#25143;&#24555;&#36895;&#35782;&#21035;&#31526;&#21512;&#20854;&#38656;&#27714;&#21644;&#32452;&#32455;&#25919;&#31574;&#25110;&#27861;&#35268;&#30340;&#25968;&#25454;&#38598;&#12290;&#35770;&#25991;&#36824;&#35752;&#35770;&#20102;&#26694;&#26550;&#30340;&#23454;&#26045;&#65292;&#24182;&#25552;&#20986;&#20102;&#26368;&#22823;&#21270;&#20854;&#28508;&#21147;&#30340;&#24314;&#35758;&#12290;&#39044;&#26399;&#35813;&#26694;&#26550;&#23558;&#22686;&#24378;&#22312;&#30740;&#31350;&#21644;&#20915;&#31574;&#20013;&#20351;&#29992;&#30340;&#25968;&#25454;&#30340;&#36136;&#37327;&#21644;&#21487;&#38752;&#24615;&#65292;&#20419;&#36827;&#26356;&#36127;&#36131;&#20219;&#12289;&#21487;&#20449;&#36182;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.06153v2 Announce Type: replace-cross  Abstract: This paper introduces a no-code, machine-readable documentation framework for open datasets, with a focus on responsible AI (RAI) considerations. The framework aims to improve comprehensibility, and usability of open datasets, facilitating easier discovery and use, better understanding of content and context, and evaluation of dataset quality and accuracy. The proposed framework is designed to streamline the evaluation of datasets, helping researchers, data scientists, and other open data users quickly identify datasets that meet their needs and organizational policies or regulations. The paper also discusses the implementation of the framework and provides recommendations to maximize its potential. The framework is expected to enhance the quality and reliability of data used in research and decision-making, fostering the development of more responsible and trustworthy AI systems.
&lt;/p&gt;</description></item><item><title>MMM &#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36974;&#34109;&#36816;&#21160;&#27169;&#22411;&#30340;&#26032;&#39062;&#36816;&#21160;&#29983;&#25104;&#33539;&#24335;&#65292;&#36890;&#36807;&#36816;&#21160;&#26631;&#35760;&#22120;&#21644;&#26465;&#20214;&#36974;&#34109;&#36816;&#21160;&#21464;&#25442;&#22120;&#65292;&#22312;&#23454;&#26102;&#24615;&#33021;&#12289;&#39640;&#20445;&#30495;&#24230;&#21644;&#36816;&#21160;&#21487;&#32534;&#36753;&#24615;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#12290;</title><link>https://arxiv.org/abs/2312.03596</link><description>&lt;p&gt;
MMM&#65306;&#29983;&#25104;&#24335;&#36974;&#34109;&#36816;&#21160;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
MMM: Generative Masked Motion Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.03596
&lt;/p&gt;
&lt;p&gt;
MMM &#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36974;&#34109;&#36816;&#21160;&#27169;&#22411;&#30340;&#26032;&#39062;&#36816;&#21160;&#29983;&#25104;&#33539;&#24335;&#65292;&#36890;&#36807;&#36816;&#21160;&#26631;&#35760;&#22120;&#21644;&#26465;&#20214;&#36974;&#34109;&#36816;&#21160;&#21464;&#25442;&#22120;&#65292;&#22312;&#23454;&#26102;&#24615;&#33021;&#12289;&#39640;&#20445;&#30495;&#24230;&#21644;&#36816;&#21160;&#21487;&#32534;&#36753;&#24615;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#20351;&#29992;&#25193;&#25955;&#21644;&#33258;&#22238;&#24402;&#27169;&#22411;&#36827;&#34892;&#25991;&#26412;&#21040;&#36816;&#21160;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#19968;&#20123;&#36827;&#23637;&#65292;&#26174;&#31034;&#20986;&#20102;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#24448;&#24448;&#22312;&#23454;&#26102;&#24615;&#33021;&#12289;&#39640;&#20445;&#30495;&#24230;&#21644;&#36816;&#21160;&#21487;&#32534;&#36753;&#24615;&#20043;&#38388;&#23384;&#22312;&#26435;&#34913;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;MMM&#65292;&#19968;&#31181;&#22522;&#20110;&#36974;&#34109;&#36816;&#21160;&#27169;&#22411;&#30340;&#26032;&#39062;&#32780;&#31616;&#21333;&#30340;&#36816;&#21160;&#29983;&#25104;&#33539;&#24335;&#12290;MMM&#30001;&#20004;&#20010;&#20851;&#38190;&#32452;&#20214;&#32452;&#25104;&#65306;&#65288;1&#65289;&#36816;&#21160;&#26631;&#35760;&#22120;&#65292;&#23558;3D&#20154;&#20307;&#36816;&#21160;&#36716;&#21270;&#20026;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#19968;&#31995;&#21015;&#31163;&#25955;&#26631;&#35760;&#65292;&#20197;&#21450;&#65288;2&#65289;&#26465;&#20214;&#36974;&#34109;&#36816;&#21160;&#21464;&#25442;&#22120;&#65292;&#23398;&#20064;&#39044;&#27979;&#22312;&#39044;&#20808;&#35745;&#31639;&#30340;&#25991;&#26412;&#26631;&#35760;&#30340;&#26465;&#20214;&#19979;&#38543;&#26426;&#36974;&#34109;&#30340;&#36816;&#21160;&#26631;&#35760;&#12290;&#36890;&#36807;&#22312;&#25152;&#26377;&#26041;&#21521;&#19978;&#20851;&#27880;&#36816;&#21160;&#21644;&#25991;&#26412;&#26631;&#35760;&#65292;MMM&#26126;&#30830;&#22320;&#25429;&#33719;&#20102;&#36816;&#21160;&#26631;&#35760;&#20043;&#38388;&#30340;&#22266;&#26377;&#20381;&#36182;&#20851;&#31995;&#20197;&#21450;&#36816;&#21160;&#21644;&#25991;&#26412;&#26631;&#35760;&#20043;&#38388;&#30340;&#35821;&#20041;&#26144;&#23556;&#12290;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#65292;&#36825;&#20801;&#35768;&#23545;&#19982;fi&#39640;&#24230;&#19968;&#33268;&#30340;&#22810;&#20010;&#36816;&#21160;&#26631;&#35760;&#36827;&#34892;&#24182;&#34892;&#21644;&#36845;&#20195;&#35299;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.03596v2 Announce Type: replace-cross  Abstract: Recent advances in text-to-motion generation using diffusion and autoregressive models have shown promising results. However, these models often suffer from a trade-off between real-time performance, high fidelity, and motion editability. To address this gap, we introduce MMM, a novel yet simple motion generation paradigm based on Masked Motion Model. MMM consists of two key components: (1) a motion tokenizer that transforms 3D human motion into a sequence of discrete tokens in latent space, and (2) a conditional masked motion transformer that learns to predict randomly masked motion tokens, conditioned on the pre-computed text tokens. By attending to motion and text tokens in all directions, MMM explicitly captures inherent dependency among motion tokens and semantic mapping between motion and text tokens. During inference, this allows parallel and iterative decoding of multiple motion tokens that are highly consistent with fi
&lt;/p&gt;</description></item><item><title>TimeChat&#26159;&#19968;&#31181;&#26102;&#38388;&#25935;&#24863;&#30340;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#21253;&#21547;&#26102;&#38388;&#25139;&#24863;&#30693;&#24103;&#32534;&#30721;&#22120;&#21644;&#28369;&#21160;&#35270;&#39057;Q-Former&#65292;&#20197;&#23454;&#29616;&#23545;&#38271;&#35270;&#39057;&#36827;&#34892;&#24378;&#22823;&#30340;&#38646;-shot&#26102;&#38388;&#26412;&#22320;&#21270;&#21644;&#25512;&#29702;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#21508;&#31181;&#35270;&#39057;&#29702;&#35299;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>https://arxiv.org/abs/2312.02051</link><description>&lt;p&gt;
TimeChat&#65306;&#19968;&#31181;&#38754;&#21521;&#38271;&#35270;&#39057;&#29702;&#35299;&#30340;&#26102;&#38388;&#25935;&#24863;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
TimeChat: A Time-sensitive Multimodal Large Language Model for Long Video Understanding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.02051
&lt;/p&gt;
&lt;p&gt;
TimeChat&#26159;&#19968;&#31181;&#26102;&#38388;&#25935;&#24863;&#30340;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#21253;&#21547;&#26102;&#38388;&#25139;&#24863;&#30693;&#24103;&#32534;&#30721;&#22120;&#21644;&#28369;&#21160;&#35270;&#39057;Q-Former&#65292;&#20197;&#23454;&#29616;&#23545;&#38271;&#35270;&#39057;&#36827;&#34892;&#24378;&#22823;&#30340;&#38646;-shot&#26102;&#38388;&#26412;&#22320;&#21270;&#21644;&#25512;&#29702;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#21508;&#31181;&#35270;&#39057;&#29702;&#35299;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;TimeChat&#65292;&#19968;&#31181;&#19987;&#38376;&#20026;&#38271;&#35270;&#39057;&#29702;&#35299;&#35774;&#35745;&#30340;&#26102;&#38388;&#25935;&#24863;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290; &#25105;&#20204;&#30340;&#27169;&#22411;&#21253;&#21547;&#20004;&#20010;&#20851;&#38190;&#30340;&#26550;&#26500;&#36129;&#29486;&#65306;(1)&#19968;&#20010;&#26102;&#38388;&#25139;&#24863;&#30693;&#24103;&#32534;&#30721;&#22120;&#65292;&#23558;&#35270;&#35273;&#20869;&#23481;&#19982;&#27599;&#24103;&#30340;&#26102;&#38388;&#25139;&#32465;&#23450;&#22312;&#19968;&#36215;&#65307;(2)&#19968;&#20010;&#28369;&#21160;&#35270;&#39057;Q-Former&#65292;&#29983;&#25104;&#21508;&#31181;&#38271;&#24230;&#30340;&#35270;&#39057;&#20196;&#29260;&#24207;&#21015;&#65292;&#20197;&#36866;&#24212;&#19981;&#21516;&#25345;&#32493;&#26102;&#38388;&#30340;&#35270;&#39057;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#25351;&#20196;&#35843;&#20248;&#25968;&#25454;&#38598;&#65292;&#28085;&#30422;6&#20010;&#20219;&#21153;&#21644;&#24635;&#35745;125K&#20010;&#23454;&#20363;&#65292;&#20197;&#36827;&#19968;&#27493;&#25552;&#21319;TimeChat&#22312;&#36981;&#24490;&#25351;&#20196;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;&#22312;&#21508;&#31181;&#35270;&#39057;&#29702;&#35299;&#20219;&#21153;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#65292;&#22914;&#23494;&#38598;&#23383;&#24149;&#29983;&#25104;&#12289;&#26102;&#38388;&#23450;&#20301;&#21644;&#31934;&#24425;&#29255;&#27573;&#26816;&#27979;&#65292;&#23637;&#31034;&#20102;TimeChat&#24378;&#22823;&#30340;&#38646;-shot&#26102;&#38388;&#26412;&#22320;&#21270;&#21644;&#25512;&#29702;&#33021;&#21147;&#12290;&#20363;&#22914;&#65292;&#23427;&#22312;YouCook2&#19978;&#23454;&#29616;&#20102;+9.2&#30340;F1&#20998;&#25968;&#21644;+2.8&#30340;CIDEr&#65292;&#22312;QVHighlights&#19978;&#23454;&#29616;&#20102;+5.8&#30340;HIT@1&#65292;&#22312;Cha&#19978;&#23454;&#29616;&#20102;+27.5&#30340;R@1&#65288;IoU=0.5&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.02051v2 Announce Type: replace-cross  Abstract: This work proposes TimeChat, a time-sensitive multimodal large language model specifically designed for long video understanding. Our model incorporates two key architectural contributions: (1) a timestamp-aware frame encoder that binds visual content with the timestamp of each frame, and (2) a sliding video Q-Former that produces a video token sequence of varying lengths to accommodate videos of various durations. Additionally, we construct an instruction-tuning dataset, encompassing 6 tasks and a total of 125K instances, to further enhance TimeChat's instruction-following performance. Experiment results across various video understanding tasks, such as dense captioning, temporal grounding, and highlight detection, demonstrate TimeChat's strong zero-shot temporal localization and reasoning capabilities. For example, it achieves +9.2 F1 score and +2.8 CIDEr on YouCook2, +5.8 HIT@1 on QVHighlights, and +27.5 R@1 (IoU=0.5) on Cha
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#23610;&#24230;&#29305;&#24449;&#25200;&#21160;&#25216;&#26415;(MRFP)&#65292;&#36890;&#36807;&#38543;&#26426;&#21270;&#39046;&#22495;&#29305;&#23450;&#30340;&#32454;&#31890;&#24230;&#29305;&#24449;&#21644;&#25200;&#21160;&#31895;&#31890;&#24230;&#29305;&#24449;&#30340;&#39118;&#26684;&#26469;&#35299;&#20915;&#20174;&#20223;&#30495;&#21040;&#30495;&#23454;&#22330;&#26223;&#35821;&#20041;&#20998;&#21106;&#30340;&#27867;&#21270;&#25361;&#25112;</title><link>https://arxiv.org/abs/2311.18331</link><description>&lt;p&gt;
&#20174; Sim-2-Real &#23398;&#20064;&#21487;&#25512;&#24191;&#30340;&#22810;&#23610;&#24230;&#29305;&#24449;&#25200;&#21160;&#35821;&#20041;&#20998;&#21106;&#65288;MRFP&#65289;
&lt;/p&gt;
&lt;p&gt;
MRFP: Learning Generalizable Semantic Segmentation from Sim-2-Real with Multi-Resolution Feature Perturbation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.18331
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#23610;&#24230;&#29305;&#24449;&#25200;&#21160;&#25216;&#26415;(MRFP)&#65292;&#36890;&#36807;&#38543;&#26426;&#21270;&#39046;&#22495;&#29305;&#23450;&#30340;&#32454;&#31890;&#24230;&#29305;&#24449;&#21644;&#25200;&#21160;&#31895;&#31890;&#24230;&#29305;&#24449;&#30340;&#39118;&#26684;&#26469;&#35299;&#20915;&#20174;&#20223;&#30495;&#21040;&#30495;&#23454;&#22330;&#26223;&#35821;&#20041;&#20998;&#21106;&#30340;&#27867;&#21270;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#28304;&#39046;&#22495;&#19978;&#23637;&#29616;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#20294;&#30001;&#20110;&#35757;&#32451;&#36807;&#31243;&#20013;&#32570;&#20047;&#39118;&#26684;&#22810;&#26679;&#24615;&#65292;&#20165;&#20351;&#29992;&#21333;&#19968;&#28304;&#39046;&#22495;&#25968;&#25454;&#26469;&#25552;&#39640;&#22312;&#26410;&#30693;&#30446;&#26631;&#39046;&#22495;&#19978;&#30340;&#24615;&#33021;&#20173;&#28982;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#22312;&#29983;&#25104;&#20223;&#30495;&#25968;&#25454;&#25104;&#20026;&#33719;&#21462;&#22823;&#35268;&#27169;&#39118;&#26684;&#22810;&#26679;&#30340;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#30340;&#21487;&#34892;&#26367;&#20195;&#26041;&#26696;&#65292;&#22240;&#20026;&#36825;&#26159;&#19968;&#20010;&#36153;&#21147;&#21644;&#36164;&#37329;&#23494;&#38598;&#22411;&#30340;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#20223;&#30495;&#25968;&#25454;&#19982;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#20043;&#38388;&#30340;&#22823;&#37327;&#39046;&#22495;&#29305;&#23450;&#19981;&#19968;&#33268;&#24615;&#23545;&#35821;&#20041;&#20998;&#21106;&#36896;&#25104;&#20102;&#26174;&#33879;&#30340;&#27867;&#21270;&#25361;&#25112;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#23610;&#24230;&#29305;&#24449;&#25200;&#21160;&#65288;MRFP&#65289;&#25216;&#26415;&#65292;&#29992;&#20110;&#20351;&#39046;&#22495;&#29305;&#23450;&#30340;&#32454;&#31890;&#24230;&#29305;&#24449;&#38543;&#26426;&#21270;&#65292;&#24182;&#25200;&#21160;&#31895;&#31961;&#29305;&#24449;&#30340;&#39118;&#26684;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#37117;&#24066;&#22330;&#26223;&#20998;&#21106;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#28165;&#26970;&#22320;&#34920;&#26126;&#65292;&#38543;&#30528;&#26679;&#24335;&#20449;&#24687;&#30340;&#25200;&#21160;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.18331v2 Announce Type: replace-cross  Abstract: Deep neural networks have shown exemplary performance on semantic scene understanding tasks on source domains, but due to the absence of style diversity during training, enhancing performance on unseen target domains using only single source domain data remains a challenging task. Generation of simulated data is a feasible alternative to retrieving large style-diverse real-world datasets as it is a cumbersome and budget-intensive process. However, the large domain-specfic inconsistencies between simulated and real-world data pose a significant generalization challenge in semantic segmentation. In this work, to alleviate this problem, we propose a novel MultiResolution Feature Perturbation (MRFP) technique to randomize domain-specific fine-grained features and perturb style of coarse features. Our experimental results on various urban-scene segmentation datasets clearly indicate that, along with the perturbation of style-informa
&lt;/p&gt;</description></item><item><title>&#25552;&#31034;&#39118;&#38505;&#25511;&#21046;&#26159;&#19968;&#20010;&#36731;&#37327;&#32423;&#26694;&#26550;&#65292;&#36890;&#36807;&#20005;&#26684;&#30340;&#20449;&#24687;&#39118;&#38505;&#24230;&#37327;&#26063;&#30340;&#19978;&#38480;&#36873;&#21462;&#25552;&#31034;&#65292;&#24110;&#21161;&#20943;&#36731;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36127;&#36131;&#37096;&#32626;&#36807;&#31243;&#20013;&#20135;&#29983;&#24847;&#22806;&#31967;&#31957;&#21709;&#24212;&#30340;&#39118;&#38505;&#12290;</title><link>https://arxiv.org/abs/2311.13628</link><description>&lt;p&gt;
&#25552;&#31034;&#39118;&#38505;&#25511;&#21046;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36127;&#36131;&#37096;&#32626;&#30340;&#20005;&#26684;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Prompt Risk Control: A Rigorous Framework for Responsible Deployment of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.13628
&lt;/p&gt;
&lt;p&gt;
&#25552;&#31034;&#39118;&#38505;&#25511;&#21046;&#26159;&#19968;&#20010;&#36731;&#37327;&#32423;&#26694;&#26550;&#65292;&#36890;&#36807;&#20005;&#26684;&#30340;&#20449;&#24687;&#39118;&#38505;&#24230;&#37327;&#26063;&#30340;&#19978;&#38480;&#36873;&#21462;&#25552;&#31034;&#65292;&#24110;&#21161;&#20943;&#36731;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36127;&#36131;&#37096;&#32626;&#36807;&#31243;&#20013;&#20135;&#29983;&#24847;&#22806;&#31967;&#31957;&#21709;&#24212;&#30340;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#21147;&#30340;&#29190;&#28856;&#24335;&#22686;&#38271;&#24341;&#21457;&#20102;&#23545;&#22914;&#20309;&#26368;&#22909;&#22320;&#25552;&#31034;&#27169;&#22411;&#25191;&#34892;&#29305;&#23450;&#20219;&#21153;&#30340;&#20852;&#36259;&#28010;&#28526;&#12290;&#36873;&#25321;&#19968;&#20010;&#22522;&#20110;&#39564;&#35777;&#38598;&#19978;&#24179;&#22343;&#24615;&#33021;&#30340;&#25552;&#31034;&#21487;&#33021;&#24456;&#35825;&#20154;&#65292;&#20294;&#36825;&#21487;&#33021;&#23548;&#33268;&#29983;&#25104;&#20986;&#20046;&#24847;&#26009;&#30340;&#31967;&#31957;&#21709;&#24212;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#22788;&#22659;&#26368;&#22256;&#38590;&#30340;&#29992;&#25143;&#12290;&#20026;&#20102;&#20943;&#36731;&#36825;&#19968;&#21487;&#33021;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#25552;&#31034;&#39118;&#38505;&#25511;&#21046;&#65292;&#36825;&#26159;&#19968;&#20010;&#36731;&#37327;&#32423;&#26694;&#26550;&#65292;&#26681;&#25454;&#20449;&#24687;&#39118;&#38505;&#24230;&#37327;&#26063;&#30340;&#20005;&#26684;&#19978;&#38480;&#36873;&#25321;&#25552;&#31034;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#29992;&#20110;&#20135;&#29983;&#22810;&#31181;&#24230;&#37327;&#19978;&#38480;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#34913;&#37327;&#26368;&#22351;&#24773;&#20917;&#21709;&#24212;&#21644;&#29992;&#25143;&#32676;&#20307;&#29983;&#25104;&#36136;&#37327;&#19981;&#22343;&#34913;&#30340;&#37327;&#65292;&#27492;&#22806;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;&#22522;&#30784;&#32479;&#35745;&#30028;&#23450;&#25216;&#26415;&#65292;&#20197;&#36866;&#24212;&#37096;&#32626;&#20013;&#20998;&#24067;&#21464;&#21270;&#21487;&#33021;&#24615;&#30340;&#24773;&#20917;&#12290;&#22312;&#24320;&#25918;&#24335;&#32842;&#22825;&#12289;&#21307;&#23398;&#38382;&#39064;&#31561;&#24212;&#29992;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.13628v2 Announce Type: replace-cross  Abstract: The recent explosion in the capabilities of large language models has led to a wave of interest in how best to prompt a model to perform a given task. While it may be tempting to simply choose a prompt based on average performance on a validation set, this can lead to a deployment where unexpectedly poor responses are generated, especially for the worst-off users. To mitigate this prospect, we propose Prompt Risk Control, a lightweight framework for selecting a prompt based on rigorous upper bounds on families of informative risk measures. We offer methods for producing bounds on a diverse set of metrics, including quantities that measure worst-case responses and disparities in generation quality across the population of users. In addition, we extend the underlying statistical bounding techniques to accommodate the possibility of distribution shifts in deployment. Experiments on applications such as open-ended chat, medical que
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#23637;&#31034;&#20102;&#29289;&#29702;&#23398;&#27169;&#25311;&#19982; NeRF &#32467;&#21512;&#65292;&#26080;&#38656;&#20013;&#38388;&#24418;&#24577;&#20195;&#29702;&#65292;&#36890;&#36807; Q-GMLS &#25429;&#25417;&#38750;&#32447;&#24615;&#21160;&#21147;&#23398;&#21644;&#22823;&#24418;&#21464;&#65292;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#24377;&#24615;&#21160;&#21147;&#23398;&#29983;&#25104;&#65292;&#24182;&#36866;&#24212; NeRF &#23494;&#24230;&#22330;&#35843;&#25972;&#26368;&#23567;&#20108;&#20056;&#26680;&#65292;&#20174;&#32780;&#39640;&#25928;&#21512;&#25104;&#21508;&#31181;&#39640;&#24377;&#24615;&#26448;&#26009;&#30340;&#29289;&#29702;&#36924;&#30495;&#21160;&#30011;&#12290;</title><link>https://arxiv.org/abs/2311.13099</link><description>&lt;p&gt;
PIE-NeRF: &#20351;&#29992; NeRF &#36827;&#34892;&#22522;&#20110;&#29289;&#29702;&#30340;&#20132;&#20114;&#24377;&#24615;&#21160;&#21147;&#23398;
&lt;/p&gt;
&lt;p&gt;
PIE-NeRF: Physics-based Interactive Elastodynamics with NeRF
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.13099
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#23637;&#31034;&#20102;&#29289;&#29702;&#23398;&#27169;&#25311;&#19982; NeRF &#32467;&#21512;&#65292;&#26080;&#38656;&#20013;&#38388;&#24418;&#24577;&#20195;&#29702;&#65292;&#36890;&#36807; Q-GMLS &#25429;&#25417;&#38750;&#32447;&#24615;&#21160;&#21147;&#23398;&#21644;&#22823;&#24418;&#21464;&#65292;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#24377;&#24615;&#21160;&#21147;&#23398;&#29983;&#25104;&#65292;&#24182;&#36866;&#24212; NeRF &#23494;&#24230;&#22330;&#35843;&#25972;&#26368;&#23567;&#20108;&#20056;&#26680;&#65292;&#20174;&#32780;&#39640;&#25928;&#21512;&#25104;&#21508;&#31181;&#39640;&#24377;&#24615;&#26448;&#26009;&#30340;&#29289;&#29702;&#36924;&#30495;&#21160;&#30011;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23637;&#31034;&#20102;&#29289;&#29702;&#23398;&#27169;&#25311;&#19982; NeRF &#21487;&#20197;&#26080;&#32541;&#38598;&#25104;&#65292;&#29992;&#20110;&#29983;&#25104;&#30495;&#23454;&#29289;&#20307;&#30340;&#39640;&#36136;&#37327;&#24377;&#24615;&#21160;&#21147;&#23398;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#20197;&#26080;&#32593;&#26684;&#30340;&#26041;&#24335;&#31163;&#25955;&#21270;&#38750;&#32447;&#24615;&#36229;&#24377;&#24615;&#65292;&#36991;&#20813;&#20102;&#20013;&#38388;&#36741;&#21161;&#24418;&#24577;&#20195;&#29702;&#29289;&#22914;&#22235;&#38754;&#20307;&#32593;&#26684;&#25110;&#20307;&#32032;&#32593;&#26684;&#30340;&#24517;&#35201;&#24615;&#12290;&#37319;&#29992;&#20108;&#27425;&#24191;&#20041;&#31227;&#21160;&#26368;&#23567;&#20108;&#20056;&#65288;Q-GMLS&#65289;&#26469;&#25429;&#25417;&#38544;&#24335;&#27169;&#22411;&#19978;&#30340;&#38750;&#32447;&#24615;&#21160;&#24577;&#21644;&#22823;&#24418;&#21464;&#12290;&#36825;&#31181;&#26080;&#32593;&#26684;&#38598;&#25104;&#20351;&#22797;&#26434;&#21644;&#20849;&#32500;&#24230;&#24418;&#29366;&#30340;&#22810;&#21151;&#33021;&#27169;&#25311;&#25104;&#20026;&#21487;&#33021;&#12290;&#25105;&#20204;&#26681;&#25454; NeRF &#23494;&#24230;&#22330;&#33258;&#36866;&#24212;&#22320;&#25918;&#32622;&#26368;&#23567;&#20108;&#20056;&#26680;&#65292;&#26174;&#33879;&#38477;&#20302;&#38750;&#32447;&#24615;&#27169;&#25311;&#30340;&#22797;&#26434;&#24615;&#12290;&#22240;&#27492;&#65292;&#21487;&#20197;&#26041;&#20415;&#22320;&#20351;&#29992;&#25105;&#20204;&#30340;&#26041;&#27861;&#20197;&#20132;&#20114;&#36895;&#29575;&#21512;&#25104;&#21508;&#31181;&#39640;&#24377;&#24615;&#26448;&#26009;&#30340;&#29289;&#29702;&#36924;&#30495;&#21160;&#30011;&#12290;&#26377;&#20851;&#26356;&#22810;&#20449;&#24687;&#65292;&#35831;&#35775;&#38382;&#25105;&#20204;&#30340;&#39033;&#30446;&#39029;&#38754;https://fytalo
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.13099v2 Announce Type: replace-cross  Abstract: We show that physics-based simulations can be seamlessly integrated with NeRF to generate high-quality elastodynamics of real-world objects. Unlike existing methods, we discretize nonlinear hyperelasticity in a meshless way, obviating the necessity for intermediate auxiliary shape proxies like a tetrahedral mesh or voxel grid. A quadratic generalized moving least square (Q-GMLS) is employed to capture nonlinear dynamics and large deformation on the implicit model. Such meshless integration enables versatile simulations of complex and codimensional shapes. We adaptively place the least-square kernels according to the NeRF density field to significantly reduce the complexity of the nonlinear simulation. As a result, physically realistic animations can be conveniently synthesized using our method for a wide range of hyperelastic materials at an interactive rate. For more information, please visit our project page at https://fytalo
&lt;/p&gt;</description></item><item><title>&#36830;&#32493;&#23398;&#20064;&#26159;&#26426;&#22120;&#23398;&#20064;&#30340;&#23376;&#39046;&#22495;&#65292;&#33268;&#21147;&#20110;&#35753;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#26032;&#25968;&#25454;&#19978;&#19981;&#26029;&#23398;&#20064;&#65292;&#32780;&#19981;&#24536;&#35760;&#36807;&#21435;&#23398;&#21040;&#30340;&#30693;&#35782;&#12290;&#30740;&#31350;&#25581;&#31034;&#20102;&#20869;&#23384;&#38480;&#21046;&#22330;&#26223;&#30340;&#20027;&#23548;&#22320;&#20301;&#65292;&#24182;&#35752;&#35770;&#20102;&#36830;&#32493;&#23398;&#20064;&#22312;&#35299;&#20915;&#27169;&#22411;&#32534;&#36753;&#12289;&#20010;&#24615;&#21270;&#12289;&#19987;&#19994;&#21270;&#12289;&#35774;&#22791;&#31471;&#23398;&#20064;&#12289;&#24555;&#36895;&#65288;&#37325;&#26032;&#65289;&#35757;&#32451;&#21644;&#24378;&#21270;&#23398;&#20064;&#31561;&#38382;&#39064;&#20013;&#30340;&#20316;&#29992;&#12290;</title><link>https://arxiv.org/abs/2311.11908</link><description>&lt;p&gt;
&#36830;&#32493;&#23398;&#20064;&#65306;&#24212;&#29992;&#19982;&#26410;&#26469;&#36335;&#24452;
&lt;/p&gt;
&lt;p&gt;
Continual Learning: Applications and the Road Forward
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.11908
&lt;/p&gt;
&lt;p&gt;
&#36830;&#32493;&#23398;&#20064;&#26159;&#26426;&#22120;&#23398;&#20064;&#30340;&#23376;&#39046;&#22495;&#65292;&#33268;&#21147;&#20110;&#35753;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#26032;&#25968;&#25454;&#19978;&#19981;&#26029;&#23398;&#20064;&#65292;&#32780;&#19981;&#24536;&#35760;&#36807;&#21435;&#23398;&#21040;&#30340;&#30693;&#35782;&#12290;&#30740;&#31350;&#25581;&#31034;&#20102;&#20869;&#23384;&#38480;&#21046;&#22330;&#26223;&#30340;&#20027;&#23548;&#22320;&#20301;&#65292;&#24182;&#35752;&#35770;&#20102;&#36830;&#32493;&#23398;&#20064;&#22312;&#35299;&#20915;&#27169;&#22411;&#32534;&#36753;&#12289;&#20010;&#24615;&#21270;&#12289;&#19987;&#19994;&#21270;&#12289;&#35774;&#22791;&#31471;&#23398;&#20064;&#12289;&#24555;&#36895;&#65288;&#37325;&#26032;&#65289;&#35757;&#32451;&#21644;&#24378;&#21270;&#23398;&#20064;&#31561;&#38382;&#39064;&#20013;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36830;&#32493;&#23398;&#20064;&#26159;&#26426;&#22120;&#23398;&#20064;&#30340;&#19968;&#20010;&#23376;&#39046;&#22495;&#65292;&#26088;&#22312;&#20351;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#33021;&#22815;&#22312;&#26032;&#25968;&#25454;&#19978;&#19981;&#26029;&#23398;&#20064;&#65292;&#36890;&#36807;&#31215;&#32047;&#30693;&#35782;&#32780;&#19981;&#36951;&#24536;&#36807;&#21435;&#25152;&#23398;&#12290;&#26412;&#30740;&#31350;&#36864;&#19968;&#27493;&#24605;&#32771;&#65292;&#24182;&#25552;&#20986;&#38382;&#39064;&#65306;&#8220;&#20026;&#20160;&#20040;&#39318;&#20808;&#35201;&#20851;&#27880;&#36830;&#32493;&#23398;&#20064;&#65311;&#8221;&#12290;&#25105;&#20204;&#36890;&#36807;&#23457;&#35270;&#36817;&#26399;&#22312;&#22235;&#20010;&#20027;&#35201;&#26426;&#22120;&#23398;&#20064;&#20250;&#35758;&#19978;&#21457;&#34920;&#30340;&#36830;&#32493;&#23398;&#20064;&#35770;&#25991;&#26469;&#38138;&#22443;&#65292;&#23637;&#31034;&#20102;&#21463;&#20869;&#23384;&#38480;&#21046;&#30340;&#22330;&#26223;&#20027;&#23548;&#20102;&#35813;&#39046;&#22495;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#20116;&#20010;&#26410;&#35299;&#38382;&#39064;&#65292;&#23613;&#31649;&#20045;&#30475;&#36215;&#26469;&#21487;&#33021;&#19982;&#36830;&#32493;&#23398;&#20064;&#26080;&#20851;&#65292;&#20294;&#25105;&#20204;&#23637;&#31034;&#20102;&#36830;&#32493;&#23398;&#20064;&#23558;&#24517;&#28982;&#25104;&#20026;&#23427;&#20204;&#35299;&#20915;&#26041;&#26696;&#30340;&#19968;&#37096;&#20998;&#12290;&#36825;&#20123;&#38382;&#39064;&#21253;&#25324;&#27169;&#22411;&#32534;&#36753;&#12289;&#20010;&#24615;&#21270;&#21644;&#19987;&#19994;&#21270;&#12289;&#35774;&#22791;&#31471;&#23398;&#20064;&#12289;&#26356;&#24555;&#30340;&#65288;&#37325;&#26032;&#65289;&#35757;&#32451;&#21644;&#24378;&#21270;&#23398;&#20064;&#12290;&#26368;&#21518;&#65292;&#36890;&#36807;&#27604;&#36739;&#36825;&#20123;&#26410;&#35299;&#38382;&#39064;&#30340;&#26399;&#26395;&#21644;&#24403;&#21069;&#30340;&#20551;&#35774;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.11908v3 Announce Type: replace-cross  Abstract: Continual learning is a subfield of machine learning, which aims to allow machine learning models to continuously learn on new data, by accumulating knowledge without forgetting what was learned in the past. In this work, we take a step back, and ask: "Why should one care about continual learning in the first place?". We set the stage by examining recent continual learning papers published at four major machine learning conferences, and show that memory-constrained settings dominate the field. Then, we discuss five open problems in machine learning, and even though they might seem unrelated to continual learning at first sight, we show that continual learning will inevitably be part of their solution. These problems are model editing, personalization and specialization, on-device learning, faster (re-)training and reinforcement learning. Finally, by comparing the desiderata from these unsolved problems and the current assumptio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#32467;&#21512;&#20840;&#23616;&#35821;&#20041;&#34701;&#21512;&#21644;&#33258;&#30456;&#20284;&#29305;&#24449;&#22686;&#24378;&#27169;&#22359;&#65292;&#20197;&#24341;&#23548;&#24067;&#23616;&#21040;&#22270;&#20687;&#21512;&#25104;&#20013;&#30340;&#23545;&#35937;&#36830;&#36143;&#24615;&#12290;</title><link>https://arxiv.org/abs/2311.10522</link><description>&lt;p&gt;
&#25552;&#21319;&#24067;&#23616;&#21040;&#22270;&#20687;&#21512;&#25104;&#20013;&#30340;&#23545;&#35937;&#36830;&#36143;&#24615;
&lt;/p&gt;
&lt;p&gt;
Enhancing Object Coherence in Layout-to-Image Synthesis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.10522
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#32467;&#21512;&#20840;&#23616;&#35821;&#20041;&#34701;&#21512;&#21644;&#33258;&#30456;&#20284;&#29305;&#24449;&#22686;&#24378;&#27169;&#22359;&#65292;&#20197;&#24341;&#23548;&#24067;&#23616;&#21040;&#22270;&#20687;&#21512;&#25104;&#20013;&#30340;&#23545;&#35937;&#36830;&#36143;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24067;&#23616;&#21040;&#22270;&#20687;&#21512;&#25104;&#26159;&#19968;&#31181;&#26032;&#20852;&#30340;&#26465;&#20214;&#22270;&#20687;&#29983;&#25104;&#25216;&#26415;&#12290;&#23427;&#26088;&#22312;&#29983;&#25104;&#22797;&#26434;&#22330;&#26223;&#65292;&#29992;&#25143;&#21487;&#20197;&#23545;&#22330;&#26223;&#20013;&#30340;&#23545;&#35937;&#24067;&#23616;&#36827;&#34892;&#31934;&#32454;&#25511;&#21046;&#12290;&#28982;&#32780;&#65292;&#25511;&#21046;&#23545;&#35937;&#36830;&#36143;&#24615;&#65292;&#21253;&#25324;&#35821;&#20041;&#36830;&#36143;&#24615;&#65288;&#20363;&#22914;&#65292;&#29483;&#26159;&#21542;&#30475;&#21521;&#33457;&#26421;&#65289;&#21644;&#29289;&#29702;&#36830;&#36143;&#24615;&#65288;&#20363;&#22914;&#65292;&#25163;&#21644;&#29699;&#25293;&#19981;&#24212;&#38169;&#20301;&#65289;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#37197;&#22791;&#20102;&#26377;&#25928;&#30340;&#20840;&#23616;&#35821;&#20041;&#34701;&#21512;&#65288;GSF&#65289;&#21644;&#33258;&#30456;&#20284;&#29305;&#24449;&#22686;&#24378;&#27169;&#22359;&#65292;&#20197;&#24341;&#23548;&#35813;&#20219;&#21153;&#30340;&#23545;&#35937;&#36830;&#36143;&#24615;&#12290;&#23545;&#20110;&#35821;&#20041;&#36830;&#36143;&#24615;&#65292;&#25105;&#20204;&#35748;&#20026;&#22270;&#20687;&#26631;&#39064;&#21253;&#21547;&#20016;&#23500;&#20449;&#24687;&#65292;&#21487;&#20197;&#23450;&#20041;&#22270;&#20687;&#20013;&#23545;&#35937;&#20043;&#38388;&#30340;&#35821;&#20041;&#20851;&#31995;&#12290;&#19982;&#20854;&#31616;&#21333;&#22320;&#20351;&#29992;&#26631;&#39064;&#21644;&#29983;&#25104;&#22270;&#20687;&#20043;&#38388;&#30340;&#36328;&#27880;&#24847;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#21516;&#26102;&#22788;&#29702;&#39640;&#24230;&#30456;&#20851;&#24067;&#23616;&#38480;&#21046;&#21644;&#35821;&#20041;&#36830;&#36143;&#24615;&#30340;&#26041;&#27861;&#65292;&#20174;&#32780;&#20351;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.10522v4 Announce Type: replace-cross  Abstract: Layout-to-image synthesis is an emerging technique in conditional image generation. It aims to generate complex scenes, where users require fine control over the layout of the objects in a scene. However, it remains challenging to control the object coherence, including semantic coherence (e.g., the cat looks at the flowers or not) and physical coherence (e.g., the hand and the racket should not be misaligned). In this paper, we propose a novel diffusion model with effective global semantic fusion (GSF) and self-similarity feature enhancement modules to guide the object coherence for this task. For semantic coherence, we argue that the image caption contains rich information for defining the semantic relationship within the objects in the images. Instead of simply employing cross-attention between captions and generated images, which addresses the highly relevant layout restriction and semantic coherence separately and thus lea
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21019;&#24314;MACGYVER&#25968;&#25454;&#38598;&#24182;&#19982;&#20154;&#31867;&#27604;&#36739;&#65292;&#30740;&#31350;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21019;&#24847;&#38382;&#39064;&#35299;&#20915;&#26041;&#38754;&#29420;&#20855;&#25361;&#25112;&#24615;&#65292;&#22312;&#30693;&#35782;&#24191;&#24230;&#21644;&#21487;&#34892;&#24615;&#26041;&#38754;&#19982;&#20154;&#31867;&#23384;&#22312;&#29420;&#29305;&#24046;&#24322;&#65292;&#21516;&#26102;&#36824;&#23637;&#31034;&#20102;&#36890;&#36807;&#26032;&#30340;&#25552;&#31034;&#25216;&#26415;&#25552;&#21319;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2311.09682</link><description>&lt;p&gt;
MacGyver&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#26159;&#21019;&#24847;&#38382;&#39064;&#35299;&#20915;&#32773;&#65311;
&lt;/p&gt;
&lt;p&gt;
MacGyver: Are Large Language Models Creative Problem Solvers?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.09682
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21019;&#24314;MACGYVER&#25968;&#25454;&#38598;&#24182;&#19982;&#20154;&#31867;&#27604;&#36739;&#65292;&#30740;&#31350;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21019;&#24847;&#38382;&#39064;&#35299;&#20915;&#26041;&#38754;&#29420;&#20855;&#25361;&#25112;&#24615;&#65292;&#22312;&#30693;&#35782;&#24191;&#24230;&#21644;&#21487;&#34892;&#24615;&#26041;&#38754;&#19982;&#20154;&#31867;&#23384;&#22312;&#29420;&#29305;&#24046;&#24322;&#65292;&#21516;&#26102;&#36824;&#23637;&#31034;&#20102;&#36890;&#36807;&#26032;&#30340;&#25552;&#31034;&#25216;&#26415;&#25552;&#21319;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22312;&#19968;&#20010;&#20840;&#26032;&#30340;&#32422;&#26463;&#35774;&#32622;&#20013;&#25506;&#31350;&#20102;&#29616;&#20195;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21019;&#24847;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;MACGYVER&#65292;&#36825;&#26159;&#19968;&#20010;&#33258;&#21160;&#29983;&#25104;&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#36229;&#36807;1600&#20010;&#29305;&#24847;&#35774;&#35745;&#30340;&#29616;&#23454;&#19990;&#30028;&#38382;&#39064;&#65292;&#26088;&#22312;&#24341;&#21457;&#29289;&#20307;&#30340;&#21019;&#26032;&#20351;&#29992;&#65292;&#24182;&#38656;&#35201;&#36229;&#36234;&#24120;&#35268;&#24605;&#32500;&#12290;&#25105;&#20204;&#38543;&#21518;&#21521;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#20154;&#31867;&#23637;&#31034;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#65292;&#20197;&#27604;&#36739;&#21644;&#23545;&#27604;&#23427;&#20204;&#30340;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#12290;MACGYVER&#23545;&#36825;&#20004;&#20010;&#32676;&#20307;&#37117;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#20294;&#20197;&#29420;&#29305;&#21644;&#20114;&#34917;&#30340;&#26041;&#24335;&#21576;&#29616;&#12290;&#20363;&#22914;&#65292;&#20154;&#31867;&#25797;&#38271;&#29087;&#24713;&#30340;&#20219;&#21153;&#65292;&#20294;&#22312;&#29305;&#23450;&#39046;&#22495;&#30693;&#35782;&#19978;&#26377;&#22256;&#38590;&#65292;&#23548;&#33268;&#26356;&#39640;&#30340;&#24046;&#24322;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26292;&#38706;&#20110;&#21508;&#31181;&#19987;&#19994;&#30693;&#35782;&#65292;&#23581;&#35797;&#26356;&#24191;&#27867;&#30340;&#38382;&#39064;&#65292;&#20294;&#22312;&#25552;&#20986;&#29289;&#29702;&#19978;&#19981;&#21487;&#34892;&#30340;&#34892;&#21160;&#26102;&#22833;&#36133;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#35814;&#32454;&#30340;&#38169;&#35823;&#20998;&#26512;&#65292;&#24182;&#23637;&#31034;&#20102;&#36890;&#36807;&#26032;&#30340;&#25552;&#31034;&#25216;&#26415;&#25552;&#39640;&#23427;&#20204;&#30340;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.09682v2 Announce Type: replace-cross  Abstract: We explore the creative problem-solving capabilities of modern LLMs in a novel constrained setting. To this end, we create MACGYVER, an automatically generated dataset consisting of over 1,600 real-world problems deliberately designed to trigger innovative usage of objects and necessitate out-of-the-box thinking. We then present our collection to both LLMs and humans to compare and contrast their problem-solving abilities. MACGYVER is challenging for both groups, but in unique and complementary ways. For instance, humans excel in tasks they are familiar with but struggle with domain-specific knowledge, leading to a higher variance. In contrast, LLMs, exposed to a variety of specialized knowledge, attempt broader problems but fail by proposing physically-infeasible actions. Finally, we provide a detailed error analysis of LLMs, and demonstrate the potential of enhancing their problem-solving ability with novel prompting techniqu
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26465;&#20214;&#21270;&#24402;&#19968;&#21270;&#27969;&#30340;&#27010;&#29575;&#22825;&#27668;&#39044;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20854;&#33021;&#22815;&#25429;&#25417;&#21644;&#33391;&#22909;&#22806;&#25512;&#31354;&#38388;-&#26102;&#38388;&#30456;&#20851;&#24615;&#12290;</title><link>https://arxiv.org/abs/2311.06958</link><description>&lt;p&gt;
&#22522;&#20110;&#26465;&#20214;&#21270;&#31354;&#38388;-&#26102;&#38388;&#24402;&#19968;&#21270;&#27969;&#30340;&#27010;&#29575;&#22825;&#27668;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Towards probabilistic Weather Forecasting with Conditioned Spatio-Temporal Normalizing Flows
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.06958
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26465;&#20214;&#21270;&#24402;&#19968;&#21270;&#27969;&#30340;&#27010;&#29575;&#22825;&#27668;&#39044;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20854;&#33021;&#22815;&#25429;&#25417;&#21644;&#33391;&#22909;&#22806;&#25512;&#31354;&#38388;-&#26102;&#38388;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#24402;&#19968;&#21270;&#27969;&#33021;&#22815;&#24314;&#27169;&#22810;&#27169;&#24577;&#31354;&#38388;&#20998;&#24067;&#65292;&#24050;&#32463;&#25104;&#21151;&#22320;&#27169;&#25311;&#20102;&#26102;&#38388;&#30456;&#20851;&#24615;&#12290;&#30001;&#20110;&#20854;&#35757;&#32451;&#31283;&#23450;&#24615;&#12289;&#21487;&#36870;&#24615;&#20197;&#21450;&#22312;&#37319;&#26679;&#21644;&#25512;&#26029;&#26041;&#38754;&#30340;&#39640;&#25928;&#24615;&#65292;&#36825;&#20123;&#27169;&#22411;&#27604;&#20854;&#20182;&#31867;&#22411;&#30340;&#29983;&#25104;&#27169;&#22411;&#25552;&#20379;&#20102;&#20960;&#39033;&#22909;&#22788;&#12290;&#36825;&#20351;&#23427;&#20204;&#25104;&#20026;&#38543;&#26426;&#31354;&#38388;-&#26102;&#38388;&#39044;&#27979;&#38382;&#39064;&#30340;&#21512;&#36866;&#20505;&#36873;&#32773;&#65292;&#22312;&#35768;&#22810;&#31185;&#23398;&#39046;&#22495;&#20013;&#37117;&#26222;&#36941;&#23384;&#22312;&#65292;&#22914;&#22320;&#29699;&#31185;&#23398;&#12289;&#22825;&#20307;&#29289;&#29702;&#23398;&#25110;&#20998;&#23376;&#31185;&#23398;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#29992;&#20110;&#38543;&#26426;&#31354;&#38388;-&#26102;&#38388;&#24314;&#27169;&#30340;&#26465;&#20214;&#21270;&#24402;&#19968;&#21270;&#27969;&#12290;&#35813;&#26041;&#27861;&#22312;&#20174;ERA5&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#26085;&#28201;&#24230;&#21644;&#23567;&#26102;&#31561;&#21387;&#22270;&#39044;&#27979;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#25429;&#25417;&#31354;&#38388;-&#26102;&#38388;&#30456;&#20851;&#24615;&#65292;&#24182;&#33021;&#22815;&#22312;&#35757;&#32451;&#26399;&#38388;&#20351;&#29992;&#30340;&#26102;&#38388;&#33539;&#22260;&#20043;&#22806;&#36827;&#34892;&#33391;&#22909;&#30340;&#22806;&#25512;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.06958v2 Announce Type: replace-cross  Abstract: Generative normalizing flows are able to model multimodal spatial distributions, and they have been shown to model temporal correlations successfully as well. These models provide several benefits over other types of generative models due to their training stability, invertibility and efficiency in sampling and inference. This makes them a suitable candidate for stochastic spatio-temporal prediction problems, which are omnipresent in many fields of sciences, such as earth sciences, astrophysics or molecular sciences. In this paper, we present conditional normalizing flows for stochastic spatio-temporal modelling. The method is evaluated on the task of daily temperature and hourly geopotential map prediction from ERA5 datasets. Experiments show that our method is able to capture spatio-temporal correlations and extrapolates well beyond the time horizon used during training.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;Multi-teacher Cross-modality Alignment Distillation&#65288;MCAD&#65289;&#25216;&#26415;&#65292;&#36890;&#36807;&#23558;&#34701;&#21512;&#30340;&#21333;&#27969;&#29305;&#24449;&#21512;&#24182;&#21040;&#21452;&#27969;&#27169;&#22411;&#30340;&#22270;&#20687;&#21644;&#25991;&#26412;&#29305;&#24449;&#20013;&#65292;&#20197;&#25972;&#21512;&#21333;&#27969;&#21644;&#21452;&#27969;&#27169;&#22411;&#30340;&#20248;&#28857;&#12290;</title><link>https://arxiv.org/abs/2310.19654</link><description>&lt;p&gt;
MCAD: &#22810;&#25945;&#24072;&#36328;&#27169;&#24577;&#23545;&#40784;&#33976;&#39311;&#29992;&#20110;&#39640;&#25928;&#22270;&#20687;-&#25991;&#26412;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
MCAD: Multi-teacher Cross-modal Alignment Distillation for efficient image-text retrieval
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.19654
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;Multi-teacher Cross-modality Alignment Distillation&#65288;MCAD&#65289;&#25216;&#26415;&#65292;&#36890;&#36807;&#23558;&#34701;&#21512;&#30340;&#21333;&#27969;&#29305;&#24449;&#21512;&#24182;&#21040;&#21452;&#27969;&#27169;&#22411;&#30340;&#22270;&#20687;&#21644;&#25991;&#26412;&#29305;&#24449;&#20013;&#65292;&#20197;&#25972;&#21512;&#21333;&#27969;&#21644;&#21452;&#27969;&#27169;&#22411;&#30340;&#20248;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#22823;&#35268;&#27169;&#35270;&#35273;-&#35821;&#35328;&#39044;&#35757;&#32451;&#65288;VLP&#65289;&#27169;&#22411;&#30340;&#25104;&#21151;&#20197;&#21450;&#22270;&#20687;-&#25991;&#26412;&#26816;&#32034;&#22312;&#24037;&#19994;&#39046;&#22495;&#30340;&#24191;&#27867;&#24212;&#29992;&#65292;&#29616;&#22312;&#36843;&#20999;&#38656;&#35201;&#20943;&#23567;&#27169;&#22411;&#22823;&#23567;&#24182;&#31616;&#21270;&#23427;&#20204;&#22312;&#31227;&#21160;&#35774;&#22791;&#19978;&#30340;&#37096;&#32626;&#12290; &#22270;&#20687;-&#25991;&#26412;&#26816;&#32034;&#20013;&#36890;&#24120;&#20351;&#29992;&#21333;&#27969;&#21644;&#21452;&#27969;&#27169;&#22411;&#32467;&#26500;&#65292;&#30446;&#30340;&#26159;&#32553;&#23567;&#25991;&#26412;&#21644;&#35270;&#35273;&#27169;&#24577;&#20043;&#38388;&#30340;&#35821;&#20041;&#24046;&#36317;&#12290; &#34429;&#28982;&#21333;&#27969;&#27169;&#22411;&#20351;&#29992;&#28145;&#24230;&#29305;&#24449;&#34701;&#21512;&#23454;&#29616;&#26356;&#20934;&#30830;&#30340;&#36328;&#27169;&#24577;&#23545;&#40784;&#65292;&#20294;&#21452;&#27969;&#27169;&#22411;&#26356;&#36866;&#29992;&#20110;&#31163;&#32447;&#32034;&#24341;&#21644;&#24555;&#36895;&#25512;&#29702;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#25945;&#24072;&#36328;&#27169;&#24577;&#23545;&#40784;&#33976;&#39311;&#65288;MCAD&#65289;&#25216;&#26415;&#65292;&#20197;&#25972;&#21512;&#21333;&#27969;&#21644;&#21452;&#27969;&#27169;&#22411;&#30340;&#20248;&#28857;&#12290; &#36890;&#36807;&#23558;&#34701;&#21512;&#30340;&#21333;&#27969;&#29305;&#24449;&#21512;&#24182;&#21040;&#21452;&#27969;&#27169;&#22411;&#30340;&#22270;&#20687;&#21644;&#25991;&#26412;&#29305;&#24449;&#20013;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#26032;&#30340;&#20462;&#25913;&#21518;&#30340;&#25945;&#24072;&#30456;&#20284;&#24615;&#20998;&#24067;&#21644;&#29305;&#24449;&#12290; &#28982;&#21518;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#20998;&#24067;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.19654v2 Announce Type: replace-cross  Abstract: Due to the success of large-scale visual-language pretraining (VLP) models and the widespread use of image-text retrieval in industry areas, it is now critically necessary to reduce the model size and streamline their mobile-device deployment. Single- and dual-stream model structures are commonly used in image-text retrieval with the goal of closing the semantic gap between textual and visual modalities. While single-stream models use deep feature fusion to achieve more accurate cross-model alignment, dual-stream models are better at offline indexing and fast inference.We propose a Multi-teacher Cross-modality Alignment Distillation (MCAD) technique to integrate the advantages of single- and dual-stream models. By incorporating the fused single-stream features into the image and text features of the dual-stream model, we formulate new modified teacher similarity distributions and features. Then, we conduct both distribution and
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#30456;&#20114;&#39564;&#35777;&#30340;&#38646;-shot&#26597;&#35810;&#25193;&#23637;&#26694;&#26550;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#26597;&#35810;&#25193;&#23637;&#20013;&#24050;&#26377;&#26041;&#27861;&#30340;&#38480;&#21046;&#21644;&#32570;&#38519;&#12290;</title><link>https://arxiv.org/abs/2310.19056</link><description>&lt;p&gt;
MILL&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#38646;-shot&#26597;&#35810;&#25193;&#23637;&#30340;&#30456;&#20114;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
MILL: Mutual Verification with Large Language Models for Zero-Shot Query Expansion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.19056
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#30456;&#20114;&#39564;&#35777;&#30340;&#38646;-shot&#26597;&#35810;&#25193;&#23637;&#26694;&#26550;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#26597;&#35810;&#25193;&#23637;&#20013;&#24050;&#26377;&#26041;&#27861;&#30340;&#38480;&#21046;&#21644;&#32570;&#38519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38646;shot&#26597;&#35810;&#25193;&#23637;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#30456;&#20114;&#39564;&#35777;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#39318;&#20808;&#35774;&#35745;&#20102;&#19968;&#31181;&#26597;&#35810;-&#26597;&#35810;-&#25991;&#26723;&#29983;&#25104;&#26041;&#27861;&#65292;&#21033;&#29992;LLMs&#30340;&#38646;-shot&#25512;&#29702;&#33021;&#21147;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#23376;&#26597;&#35810;&#21644;&#30456;&#24212;&#30340;&#25991;&#26723;&#12290;&#28982;&#21518;&#65292;&#19968;&#20010;&#30456;&#20114;&#39564;&#35777;&#36807;&#31243;&#21327;&#21516;&#29983;&#25104;&#21644;&#26816;&#32034;&#30340;&#25991;&#26723;&#20197;&#23454;&#29616;&#26368;&#20339;&#25193;&#23637;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#23436;&#20840;&#26159;&#38646;-shot&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.19056v3 Announce Type: replace-cross  Abstract: Query expansion, pivotal in search engines, enhances the representation of user information needs with additional terms. While existing methods expand queries using retrieved or generated contextual documents, each approach has notable limitations. Retrieval-based methods often fail to accurately capture search intent, particularly with brief or ambiguous queries. Generation-based methods, utilizing large language models (LLMs), generally lack corpus-specific knowledge and entail high fine-tuning costs. To address these gaps, we propose a novel zero-shot query expansion framework utilizing LLMs for mutual verification. Specifically, we first design a query-query-document generation method, leveraging LLMs' zero-shot reasoning ability to produce diverse sub-queries and corresponding documents. Then, a mutual verification process synergizes generated and retrieved documents for optimal expansion. Our proposed method is fully zero
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#36825;&#39033;&#30740;&#31350;&#35299;&#20915;&#20102;&#31038;&#20132;&#26426;&#22120;&#20154;&#22312;&#29983;&#24577;&#22330;&#26223;&#20013;&#29702;&#35299;&#35828;&#35805;&#32773;&#23545;&#35937;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2308.10757</link><description>&lt;p&gt;
&#35841;&#22312;&#19982;&#20320;&#20132;&#35848;&#65311;&#19968;&#31181;&#36171;&#20104;&#31038;&#20132;&#26426;&#22120;&#20154;&#23450;&#20301;&#33021;&#21147;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
To Whom are You Talking? A Deep Learning Model to Endow Social Robots with Addressee Estimation Skills
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2308.10757
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#36825;&#39033;&#30740;&#31350;&#35299;&#20915;&#20102;&#31038;&#20132;&#26426;&#22120;&#20154;&#22312;&#29983;&#24577;&#22330;&#26223;&#20013;&#29702;&#35299;&#35828;&#35805;&#32773;&#23545;&#35937;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#27969;&#22609;&#36896;&#20102;&#25105;&#20204;&#30340;&#31038;&#20132;&#19990;&#30028;&#12290;&#20026;&#20102;&#35753;&#26426;&#22120;&#20154;&#34987;&#35270;&#20026;&#31038;&#20132;&#30340;&#65292;&#24182;&#34987;&#32435;&#20837;&#25105;&#20204;&#30340;&#31038;&#20132;&#29615;&#22659;&#20013;&#65292;&#29702;&#35299;&#39537;&#20351;&#20154;&#38469;&#20132;&#27969;&#30340;&#19968;&#20123;&#21160;&#24577;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#23450;&#20301;&#23545;&#35937;&#30340;&#38382;&#39064;&#65292;&#21363;&#29702;&#35299;&#35805;&#35821;&#30340;&#23545;&#35937;&#65292;&#36890;&#36807;&#35299;&#37322;&#21644;&#21033;&#29992;&#35828;&#35805;&#32773;&#30340;&#38750;&#35821;&#35328;&#36523;&#20307;&#20449;&#21495;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#29616;&#19968;&#20010;&#28151;&#21512;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26469;&#20570;&#21040;&#36825;&#19968;&#28857;&#65292;&#35813;&#27169;&#22411;&#30001;&#21367;&#31215;&#23618;&#21644;LSTM&#21333;&#20803;&#32452;&#25104;&#65292;&#20197;&#25551;&#32472;&#35828;&#35805;&#32773;&#33080;&#37096;&#30340;&#22270;&#20687;&#21644;&#35828;&#35805;&#32773;&#36523;&#20307;&#23039;&#21183;&#30340;2D&#21521;&#37327;&#20316;&#20026;&#36755;&#20837;&#12290;&#25105;&#20204;&#30340;&#23454;&#29616;&#36873;&#25321;&#26159;&#20026;&#20102;&#24320;&#21457;&#19968;&#20010;&#22312;&#31038;&#20132;&#26426;&#22120;&#20154;&#19978;&#21487;&#20197;&#37096;&#32626;&#24182;&#33021;&#22312;&#29983;&#24577;&#22330;&#26223;&#20013;&#39640;&#25928;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#33021;&#22815;&#20174;&#26426;&#22120;&#20154;&#30340;&#33258;&#25105;&#20013;&#24515;&#35266;&#28857;&#35299;&#20915;&#23450;&#20301;&#23545;&#35937;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2308.10757v2 Announce Type: replace-cross  Abstract: Communicating shapes our social word. For a robot to be considered social and being consequently integrated in our social environment it is fundamental to understand some of the dynamics that rule human-human communication. In this work, we tackle the problem of Addressee Estimation, the ability to understand an utterance's addressee, by interpreting and exploiting non-verbal bodily cues from the speaker. We do so by implementing an hybrid deep learning model composed of convolutional layers and LSTM cells taking as input images portraying the face of the speaker and 2D vectors of the speaker's body posture. Our implementation choices were guided by the aim to develop a model that could be deployed on social robots and be efficient in ecological scenarios. We demonstrate that our model is able to solve the Addressee Estimation problem in terms of addressee localisation in space, from a robot ego-centric point of view.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#39537;&#21160;&#30340;&#32452;&#21512;&#26102;&#38047;&#25293;&#21334;&#65292;&#36890;&#36807;&#20165;&#20351;&#29992;&#38656;&#27714;&#26597;&#35810;&#32780;&#19981;&#26159;&#20215;&#20540;&#26597;&#35810;&#26469;&#33719;&#21462;&#25237;&#26631;&#20154;&#30340;&#20559;&#22909;&#20449;&#24687;&#12290;</title><link>https://arxiv.org/abs/2308.10226</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#39537;&#21160;&#30340;&#32452;&#21512;&#26102;&#38047;&#25293;&#21334;
&lt;/p&gt;
&lt;p&gt;
Machine Learning-Powered Combinatorial Clock Auction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2308.10226
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#39537;&#21160;&#30340;&#32452;&#21512;&#26102;&#38047;&#25293;&#21334;&#65292;&#36890;&#36807;&#20165;&#20351;&#29992;&#38656;&#27714;&#26597;&#35810;&#32780;&#19981;&#26159;&#20215;&#20540;&#26597;&#35810;&#26469;&#33719;&#21462;&#25237;&#26631;&#20154;&#30340;&#20559;&#22909;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#36845;&#20195;&#32452;&#21512;&#25293;&#21334;&#65288;ICA&#65289;&#30340;&#35774;&#35745;&#12290;&#22312;&#36825;&#20010;&#39046;&#22495;&#20013;&#30340;&#20027;&#35201;&#25361;&#25112;&#22312;&#20110;&#26463;&#31354;&#38388;&#38543;&#30528;&#29289;&#21697;&#25968;&#37327;&#21576;&#25351;&#25968;&#22686;&#38271;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26368;&#36817;&#26377;&#20960;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#30340;&#20559;&#22909;&#35843;&#26597;&#31639;&#27861;&#65292;&#26088;&#22312;&#20165;&#20174;&#25237;&#26631;&#20154;&#37027;&#37324;&#33719;&#21462;&#26368;&#37325;&#35201;&#30340;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#20174;&#23454;&#38469;&#35282;&#24230;&#30475;&#65292;&#36825;&#20123;&#20808;&#21069;&#24037;&#20316;&#30340;&#20027;&#35201;&#32570;&#28857;&#26159;&#36890;&#36807;&#20215;&#20540;&#26597;&#35810;&#65288;&#21363;&#65292;&#8220;&#23545;&#20110;&#25414;&#32465;&#21253;$\{A,B\}$&#65292;&#24744;&#30340;&#20215;&#20540;&#26159;&#22810;&#23569;&#65311;&#8221;&#65289;&#24341;&#20986;&#25237;&#26631;&#20154;&#30340;&#20559;&#22909;&#12290;&#22312;&#22823;&#22810;&#25968;&#23454;&#38469;ICA&#39046;&#22495;&#20013;&#65292;&#20215;&#20540;&#26597;&#35810;&#34987;&#35748;&#20026;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#65292;&#22240;&#20026;&#23427;&#20204;&#32473;&#25237;&#26631;&#20154;&#24102;&#26469;&#20102;&#19981;&#20999;&#23454;&#38469;&#30340;&#39640;&#35748;&#30693;&#36127;&#25285;&#65292;&#36825;&#23601;&#26159;&#20026;&#20160;&#20040;&#23427;&#20204;&#22312;&#23454;&#36341;&#20013;&#19981;&#34987;&#20351;&#29992;&#30340;&#21407;&#22240;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#35774;&#35745;&#19968;&#20010;&#26426;&#22120;&#23398;&#20064;&#39537;&#21160;&#30340;&#32452;&#21512;&#26102;&#38047;&#25293;&#21334;&#26469;&#35299;&#20915;&#36825;&#19968;&#32570;&#28857;&#65292;&#35813;&#25293;&#21334;&#21482;&#36890;&#36807;&#38656;&#27714;&#26597;&#35810;&#65288;&#21363;&#65292;&#8220;&#22312;&#20215;&#26684;$p$&#19979;&#65292;&#24744;&#23545;&#25414;&#32465;&#21253;$\{A,B\}$&#30340;&#38656;&#27714;&#26377;&#22810;&#23569;&#65311;&#8221;&#65289;&#20174;&#25237;&#26631;&#20154;&#37027;&#37324;&#24341;&#36215;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2308.10226v2 Announce Type: replace-cross  Abstract: We study the design of iterative combinatorial auctions (ICAs). The main challenge in this domain is that the bundle space grows exponentially in the number of items. To address this, several papers have recently proposed machine learning (ML)-based preference elicitation algorithms that aim to elicit only the most important information from bidders. However, from a practical point of view, the main shortcoming of this prior work is that those designs elicit bidders' preferences via value queries (i.e., ``What is your value for the bundle $\{A,B\}$?''). In most real-world ICA domains, value queries are considered impractical, since they impose an unrealistically high cognitive burden on bidders, which is why they are not used in practice. In this paper, we address this shortcoming by designing an ML-powered combinatorial clock auction that elicits information from the bidders only via demand queries (i.e., ``At prices $p$, what
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;$do_{code}$&#30340;&#21518;&#39564;&#35299;&#37322;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#37322;&#31070;&#32463;&#20195;&#30721;&#27169;&#22411;&#30340;&#39044;&#27979;&#65292;&#22522;&#20110;&#22240;&#26524;&#25512;&#26029;&#65292;&#26088;&#22312;&#23454;&#29616;&#38754;&#21521;&#32534;&#31243;&#35821;&#35328;&#30340;&#35299;&#37322;&#12290;</title><link>https://arxiv.org/abs/2302.03788</link><description>&lt;p&gt;
&#38754;&#21521;&#35299;&#37322;&#31070;&#32463;&#20195;&#30721;&#27169;&#22411;&#30340;&#22240;&#26524;&#35770;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
Toward a Theory of Causation for Interpreting Neural Code Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2302.03788
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;$do_{code}$&#30340;&#21518;&#39564;&#35299;&#37322;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#37322;&#31070;&#32463;&#20195;&#30721;&#27169;&#22411;&#30340;&#39044;&#27979;&#65292;&#22522;&#20110;&#22240;&#26524;&#25512;&#26029;&#65292;&#26088;&#22312;&#23454;&#29616;&#38754;&#21521;&#32534;&#31243;&#35821;&#35328;&#30340;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Neural Language Models of Code&#65292;&#25110;&#32773;&#31216;&#20026;&#31070;&#32463;&#20195;&#30721;&#27169;&#22411;&#65288;NCMs&#65289;&#65292;&#27491;&#22312;&#36805;&#36895;&#20174;&#30740;&#31350;&#21407;&#22411;&#21457;&#23637;&#20026;&#21830;&#19994;&#24320;&#21457;&#32773;&#24037;&#20855;&#12290;&#22240;&#27492;&#65292;&#29702;&#35299;&#36825;&#20123;&#27169;&#22411;&#30340;&#33021;&#21147;&#21644;&#23616;&#38480;&#24615;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#33021;&#21147;&#36890;&#24120;&#26159;&#20351;&#29992;&#33258;&#21160;&#21270;&#25351;&#26631;&#26469;&#34913;&#37327;&#30340;&#65292;&#36825;&#20123;&#25351;&#26631;&#36890;&#24120;&#21482;&#33021;&#25581;&#31034;&#23427;&#20204;&#30495;&#23454;&#24615;&#33021;&#30340;&#19968;&#37096;&#20998;&#12290;&#19968;&#33324;&#26469;&#35828;&#65292;NCMs&#30340;&#24615;&#33021;&#20284;&#20046;&#24456;&#26377;&#21069;&#36884;&#65292;&#20294;&#30446;&#21069;&#20851;&#20110;&#36825;&#20123;&#27169;&#22411;&#22914;&#20309;&#20570;&#20986;&#20915;&#31574;&#20173;&#26377;&#24456;&#22810;&#26410;&#30693;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;$do_{code}$&#30340;&#21518;&#39564;&#35299;&#37322;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#19987;&#38376;&#38024;&#23545;NCMs&#65292;&#33021;&#22815;&#35299;&#37322;&#27169;&#22411;&#30340;&#39044;&#27979;&#12290;$do_{code}$&#22522;&#20110;&#22240;&#26524;&#25512;&#26029;&#65292;&#20197;&#23454;&#29616;&#38754;&#21521;&#32534;&#31243;&#35821;&#35328;&#30340;&#35299;&#37322;&#12290;&#34429;&#28982;$do_{code}$&#30340;&#29702;&#35770;&#22522;&#30784;&#21487;&#25193;&#23637;&#21040;&#25506;&#32034;&#19981;&#21516;&#30340;&#27169;&#22411;&#23646;&#24615;&#65292;&#20294;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#20855;&#20307;&#30340;&#23454;&#20363;&#65292;&#26088;&#22312;&#20943;&#23569;&#24433;&#21709;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2302.03788v2 Announce Type: replace-cross  Abstract: Neural Language Models of Code, or Neural Code Models (NCMs), are rapidly progressing from research prototypes to commercial developer tools. As such, understanding the capabilities and limitations of such models is becoming critical. However, the abilities of these models are typically measured using automated metrics that often only reveal a portion of their real-world performance. While, in general, the performance of NCMs appears promising, currently much is unknown about how such models arrive at decisions. To this end, this paper introduces $do_{code}$, a post hoc interpretability method specific to NCMs that is capable of explaining model predictions. $do_{code}$ is based upon causal inference to enable programming language-oriented explanations. While the theoretical underpinnings of $do_{code}$ are extensible to exploring different model properties, we provide a concrete instantiation that aims to mitigate the impact o
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#22522;&#20110;&#26368;&#20248;&#36755;&#36816;&#25200;&#21160;&#30340;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#26500;&#24314;&#26368;&#22351;&#24773;&#20917;&#30340;&#34394;&#25311;&#29366;&#24577;&#36716;&#25442;&#20197;&#25552;&#21319;&#40065;&#26834;&#24615;&#33021;&#21644;&#23433;&#20840;&#24615;&#12290;</title><link>https://arxiv.org/abs/2301.13375</link><description>&lt;p&gt;
&#20351;&#29992;&#24102;&#26377;&#40065;&#26834;&#24615;&#20445;&#35777;&#30340;&#26368;&#20248;&#36755;&#36816;&#25200;&#21160;&#36827;&#34892;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Optimal Transport Perturbations for Safe Reinforcement Learning with Robustness Guarantees
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2301.13375
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#22522;&#20110;&#26368;&#20248;&#36755;&#36816;&#25200;&#21160;&#30340;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#26500;&#24314;&#26368;&#22351;&#24773;&#20917;&#30340;&#34394;&#25311;&#29366;&#24577;&#36716;&#25442;&#20197;&#25552;&#21319;&#40065;&#26834;&#24615;&#33021;&#21644;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#26368;&#20248;&#36755;&#36816;&#25104;&#26412;&#19981;&#30830;&#23450;&#24615;&#38598;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#24212;&#29992;&#26368;&#20248;&#36755;&#36816;&#25200;&#21160;&#26469;&#26500;&#24314;&#26368;&#22351;&#24773;&#20917;&#30340;&#34394;&#25311;&#29366;&#24577;&#36716;&#25442;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#23454;&#29616;&#26041;&#27861;&#12290;&#22312;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23637;&#31034;&#20102;&#40065;&#26834;&#24615;&#33021;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#20102;&#37096;&#32626;&#26102;&#30340;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2301.13375v2 Announce Type: replace-cross  Abstract: Robustness and safety are critical for the trustworthy deployment of deep reinforcement learning. Real-world decision making applications require algorithms that can guarantee robust performance and safety in the presence of general environment disturbances, while making limited assumptions on the data collection process during training. In order to accomplish this goal, we introduce a safe reinforcement learning framework that incorporates robustness through the use of an optimal transport cost uncertainty set. We provide an efficient implementation based on applying Optimal Transport Perturbations to construct worst-case virtual state transitions, which does not impact data collection during training and does not require detailed simulator access. In experiments on continuous control tasks with safety constraints, our approach demonstrates robust performance while significantly improving safety at deployment time compared to 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#20302;&#36164;&#28304;&#35821;&#35328;&#8212;&#8212;&#20711;&#20285;&#32599;&#35821;&#25915;&#20987;&#24615;&#35821;&#35328;&#35782;&#21035;&#25968;&#25454;&#38598;(SOLD)&#65292;&#22635;&#34917;&#20102;&#30446;&#21069;&#25915;&#20987;&#24615;&#35821;&#35328;&#35782;&#21035;&#30740;&#31350;&#23616;&#38480;&#20110;&#39640;&#36164;&#28304;&#35821;&#35328;&#30340;&#31354;&#30333;&#12290;</title><link>https://arxiv.org/abs/2212.00851</link><description>&lt;p&gt;
SOLD&#65306;&#20711;&#20285;&#32599;&#35821;&#25915;&#20987;&#24615;&#35821;&#35328;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
SOLD: Sinhala Offensive Language Dataset
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2212.00851
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#20302;&#36164;&#28304;&#35821;&#35328;&#8212;&#8212;&#20711;&#20285;&#32599;&#35821;&#25915;&#20987;&#24615;&#35821;&#35328;&#35782;&#21035;&#25968;&#25454;&#38598;(SOLD)&#65292;&#22635;&#34917;&#20102;&#30446;&#21069;&#25915;&#20987;&#24615;&#35821;&#35328;&#35782;&#21035;&#30740;&#31350;&#23616;&#38480;&#20110;&#39640;&#36164;&#28304;&#35821;&#35328;&#30340;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#25915;&#20987;&#24615;&#20869;&#23481;&#30340;&#26222;&#36941;&#23384;&#22312;&#65292;&#27604;&#22914;&#20167;&#24680;&#35328;&#35770;&#21644;&#32593;&#32476;&#27450;&#20940;&#65292;&#24050;&#25104;&#20026;&#20840;&#29699;&#24615;&#29616;&#35937;&#12290;&#36825;&#24341;&#36215;&#20102;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#31038;&#21306;&#30340;&#20852;&#36259;&#65292;&#20419;&#20351;&#24320;&#21457;&#21508;&#31181;&#31995;&#32479;&#65292;&#33021;&#22815;&#33258;&#21160;&#26816;&#27979;&#28508;&#22312;&#26377;&#23475;&#20869;&#23481;&#12290;&#28982;&#32780;&#65292;&#38500;&#20102;&#23569;&#25968;&#20960;&#20010;&#20363;&#22806;&#24773;&#20917;&#22806;&#65292;&#22823;&#22810;&#25968;&#20851;&#20110;&#36825;&#19968;&#20027;&#39064;&#30340;&#25968;&#25454;&#38598;&#37117;&#22788;&#29702;&#33521;&#35821;&#21644;&#23569;&#25968;&#20854;&#20182;&#39640;&#36164;&#28304;&#35821;&#35328;&#12290;&#22240;&#27492;&#65292;&#25915;&#20987;&#24615;&#35821;&#35328;&#35782;&#21035;&#30740;&#31350;&#19968;&#30452;&#23616;&#38480;&#20110;&#36825;&#20123;&#35821;&#35328;&#12290;&#26412;&#25991;&#36890;&#36807;&#22788;&#29702;&#20711;&#20285;&#32599;&#35821;&#25915;&#20987;&#24615;&#35821;&#35328;&#35782;&#21035;&#26469;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#20711;&#20285;&#32599;&#35821;&#26159;&#26031;&#37324;&#20848;&#21345;&#26377;&#36229;&#36807;1700&#19975;&#20154;&#21475;&#20351;&#29992;&#30340;&#20302;&#36164;&#28304;&#21360;&#27431;&#35821;&#35328;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#20711;&#20285;&#32599;&#35821;&#25915;&#20987;&#24615;&#35821;&#35328;&#25968;&#25454;&#38598;&#65288;SOLD&#65289;&#65292;&#24182;&#22312;&#35813;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#22810;&#20010;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2212.00851v2 Announce Type: replace-cross  Abstract: The widespread of offensive content online, such as hate speech and cyber-bullying, is a global phenomenon. This has sparked interest in the artificial intelligence (AI) and natural language processing (NLP) communities, motivating the development of various systems trained to detect potentially harmful content automatically. These systems require annotated datasets to train the machine learning (ML) models. However, with a few notable exceptions, most datasets on this topic have dealt with English and a few other high-resource languages. As a result, the research in offensive language identification has been limited to these languages. This paper addresses this gap by tackling offensive language identification in Sinhala, a low-resource Indo-Aryan language spoken by over 17 million people in Sri Lanka. We introduce the Sinhala Offensive Language Dataset (SOLD) and present multiple experiments on this dataset. SOLD is a manuall
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#30340;&#31471;&#21040;&#31471;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#22312;&#26234;&#33021;&#25163;&#26426;&#19978;&#39640;&#25928;&#20272;&#35745;&#29983;&#21629;&#20307;&#24449;&#65292;&#28040;&#38500;&#20102;&#32321;&#29712;&#30340;&#39044;&#22788;&#29702;&#27493;&#39588;&#12290;</title><link>https://arxiv.org/abs/2204.08989</link><description>&lt;p&gt;
&#22312;&#26234;&#33021;&#25163;&#26426;&#19978;&#39640;&#25928;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#29983;&#21629;&#20307;&#24449;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Efficient Deep Learning-based Estimation of the Vital Signs on Smartphones
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2204.08989
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#30340;&#31471;&#21040;&#31471;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#22312;&#26234;&#33021;&#25163;&#26426;&#19978;&#39640;&#25928;&#20272;&#35745;&#29983;&#21629;&#20307;&#24449;&#65292;&#28040;&#38500;&#20102;&#32321;&#29712;&#30340;&#39044;&#22788;&#29702;&#27493;&#39588;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26234;&#33021;&#25163;&#26426;&#22312;&#25105;&#20204;&#30340;&#26085;&#24120;&#29983;&#27963;&#20013;&#30340;&#26085;&#30410;&#26222;&#21450;&#65292;&#36825;&#20123;&#35774;&#22791;&#24050;&#32463;&#33021;&#22815;&#25191;&#34892;&#35768;&#22810;&#22797;&#26434;&#30340;&#20219;&#21153;&#12290;&#38024;&#23545;&#23545;&#29983;&#21629;&#20307;&#24449;&#30340;&#25345;&#32493;&#30417;&#27979;&#38656;&#27714;&#65292;&#29305;&#21035;&#26159;&#38024;&#23545;&#32769;&#24180;&#20154;&#25110;&#24739;&#26377;&#26576;&#20123;&#31867;&#22411;&#30142;&#30149;&#30340;&#20154;&#32676;&#65292;&#24320;&#21457;&#33021;&#22815;&#20351;&#29992;&#26234;&#33021;&#25163;&#26426;&#20272;&#35745;&#29983;&#21629;&#20307;&#24449;&#30340;&#31639;&#27861;&#24341;&#36215;&#20102;&#20840;&#29699;&#30740;&#31350;&#20154;&#21592;&#30340;&#20851;&#27880;&#12290;&#30740;&#31350;&#20154;&#21592;&#24050;&#32463;&#22312;&#25506;&#32034;&#20351;&#29992;&#21487;&#20197;&#22312;&#26234;&#33021;&#25163;&#26426;&#19978;&#36816;&#34892;&#30340;&#31639;&#27861;&#26469;&#20272;&#35745;&#29983;&#21629;&#20307;&#24449;&#65292;&#20363;&#22914;&#24515;&#29575;&#12289;&#34880;&#27687;&#39281;&#21644;&#24230;&#27700;&#24179;&#21644;&#21628;&#21560;&#29575;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#36825;&#20123;&#31639;&#27861;&#38656;&#35201;&#22810;&#20010;&#39044;&#22788;&#29702;&#27493;&#39588;&#65292;&#36825;&#21487;&#33021;&#20250;&#24341;&#20837;&#19968;&#20123;&#23454;&#29616;&#24320;&#38144;&#25110;&#38656;&#35201;&#35774;&#35745;&#20960;&#20010;&#25163;&#24037;&#38454;&#27573;&#25165;&#33021;&#33719;&#24471;&#26368;&#20339;&#32467;&#26524;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#31227;&#21160;&#35774;&#22791;&#29983;&#21629;&#20307;&#24449;&#20272;&#35745;&#30340;&#31471;&#21040;&#31471;&#35299;&#20915;&#26041;&#26696;&#65292;&#28040;&#38500;&#20102;&#23545;&#39044;&#22788;&#29702;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2204.08989v3 Announce Type: replace-cross  Abstract: With the increasing use of smartphones in our daily lives, these devices have become capable of performing many complex tasks. Concerning the need for continuous monitoring of vital signs, especially for the elderly or those with certain types of diseases, the development of algorithms that can estimate vital signs using smartphones has attracted researchers worldwide. In particular, researchers have been exploring ways to estimate vital signs, such as heart rate, oxygen saturation levels, and respiratory rate, using algorithms that can be run on smartphones. However, many of these algorithms require multiple pre-processing steps that might introduce some implementation overheads or require the design of a couple of hand-crafted stages to obtain an optimal result. To address this issue, this research proposes a novel end-to-end solution to mobile-based vital sign estimation using deep learning that eliminates the need for pre-p
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#26368;&#22823;&#25928;&#29992;&#30340;&#33218;&#36873;&#25321;&#31574;&#30053;&#65292;&#20197;&#20943;&#23569;&#22312;&#24207;&#21015;&#26597;&#35810;&#25512;&#33616;&#20013;&#30340;&#32047;&#31215;&#36951;&#25022;&#12290;</title><link>https://arxiv.org/abs/2108.13810</link><description>&lt;p&gt;
&#22522;&#20110;&#26368;&#22823;&#25928;&#29992;&#30340;&#24207;&#21015;&#26597;&#35810;&#25512;&#33616;&#20013;&#30340;&#33218;&#36873;&#25321;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Max-Utility Based Arm Selection Strategy For Sequential Query Recommendations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2108.13810
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#26368;&#22823;&#25928;&#29992;&#30340;&#33218;&#36873;&#25321;&#31574;&#30053;&#65292;&#20197;&#20943;&#23569;&#22312;&#24207;&#21015;&#26597;&#35810;&#25512;&#33616;&#20013;&#30340;&#32047;&#31215;&#36951;&#25022;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#22312;&#38381;&#29615;&#20132;&#20114;&#24335;&#23398;&#20064;&#35774;&#32622;&#20013;&#30340;&#26597;&#35810;&#25512;&#33616;&#38382;&#39064;&#65292;&#20363;&#22914;&#22312;&#32447;&#20449;&#24687;&#25910;&#38598;&#21644;&#25506;&#32034;&#20998;&#26512;&#12290;&#35813;&#38382;&#39064;&#21487;&#20197;&#33258;&#28982;&#22320;&#20351;&#29992;&#22810;&#33218;&#32769;&#34382;&#26426;&#65288;MAB&#65289;&#26694;&#26550;&#26469;&#24314;&#27169;&#65292;&#20854;&#20013;&#26377;&#21487;&#25968;&#20010;&#33218;&#12290;&#26631;&#20934;&#30340;&#21487;&#25968;&#33218;MAB&#31639;&#27861;&#20174;&#36873;&#25321;&#19968;&#20010;&#38543;&#26426;&#30340;&#20505;&#36873;&#33218;&#38598;&#24320;&#22987;&#65292;&#28982;&#21518;&#22312;&#36825;&#20010;&#20505;&#36873;&#38598;&#21512;&#19978;&#24212;&#29992;&#26631;&#20934;&#30340;MAB&#31639;&#27861;&#65292;&#20363;&#22914;UCB&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#26679;&#30340;&#36873;&#25321;&#31574;&#30053;&#36890;&#24120;&#20250;&#23548;&#33268;&#26356;&#39640;&#30340;&#32047;&#31215;&#36951;&#25022;&#65292;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33218;&#30340;&#26368;&#22823;&#25928;&#29992;&#30340;&#36873;&#25321;&#31574;&#30053;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#35832;&#22914;&#22312;&#32447;&#20449;&#24687;&#25910;&#38598;&#36825;&#26679;&#30340;&#20219;&#21153;&#20013;&#65292;&#20854;&#20013;&#37319;&#29992;&#20102;&#24207;&#21015;&#26597;&#35810;&#25512;&#33616;&#65292;&#26597;&#35810;&#24207;&#21015;&#26159;&#30456;&#20851;&#30340;&#65292;&#24182;&#19988;&#36890;&#36807;&#36873;&#25321;&#30456;&#23545;&#20110;&#24403;&#21069;&#25191;&#34892;&#26597;&#35810;&#20855;&#26377;&#26368;&#22823;&#25928;&#29992;&#30340;&#26597;&#35810;&#65292;&#21487;&#20197;&#23558;&#28508;&#22312;&#26368;&#20339;&#26597;&#35810;&#30340;&#25968;&#37327;&#20943;&#23569;&#21040;&#19968;&#20010;&#21487;&#31649;&#29702;&#30340;&#22823;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2108.13810v1 Announce Type: cross  Abstract: We consider the query recommendation problem in closed loop interactive learning settings like online information gathering and exploratory analytics. The problem can be naturally modelled using the Multi-Armed Bandits (MAB) framework with countably many arms. The standard MAB algorithms for countably many arms begin with selecting a random set of candidate arms and then applying standard MAB algorithms, e.g., UCB, on this candidate set downstream. We show that such a selection strategy often results in higher cumulative regret and to this end, we propose a selection strategy based on the maximum utility of the arms. We show that in tasks like online information gathering, where sequential query recommendations are employed, the sequences of queries are correlated and the number of potentially optimal queries can be reduced to a manageable size by selecting queries with maximum utility with respect to the currently executing query. Our
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#31995;&#32479;&#35780;&#20272;&#20102;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#21644;&#27431;&#20960;&#37324;&#24471;&#23545;&#40784;&#23545;&#33041;&#30005;&#35299;&#30721;&#30340;&#24433;&#21709;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#27431;&#20960;&#37324;&#24471;&#23545;&#40784;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#35299;&#30721;&#29575;&#65292;&#24182;&#19988;&#20943;&#23569;&#20102;&#25910;&#25947;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2401.10746</link><description>&lt;p&gt;
&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#23545;&#33041;&#30005;&#35299;&#30721;&#20013;&#30340;&#27431;&#20960;&#37324;&#24471;&#23545;&#40784;&#36827;&#34892;&#31995;&#32479;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
A Systematic Evaluation of Euclidean Alignment with Deep Learning for EEG Decoding. (arXiv:2401.10746v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10746
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#31995;&#32479;&#35780;&#20272;&#20102;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#21644;&#27431;&#20960;&#37324;&#24471;&#23545;&#40784;&#23545;&#33041;&#30005;&#35299;&#30721;&#30340;&#24433;&#21709;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#27431;&#20960;&#37324;&#24471;&#23545;&#40784;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#35299;&#30721;&#29575;&#65292;&#24182;&#19988;&#20943;&#23569;&#20102;&#25910;&#25947;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#20449;&#21495;&#32463;&#24120;&#29992;&#20110;&#21508;&#31181;&#33041;&#26426;&#25509;&#21475;&#65288;BCI&#65289;&#20219;&#21153;&#12290;&#23613;&#31649;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#25216;&#26415;&#26174;&#31034;&#20986;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#20294;&#23427;&#20204;&#21463;&#21040;&#22823;&#37327;&#25968;&#25454;&#35201;&#27714;&#30340;&#38480;&#21046;&#12290;&#36890;&#36807;&#21033;&#29992;&#26469;&#33258;&#22810;&#20010;&#21463;&#35797;&#32773;&#30340;&#25968;&#25454;&#65292;&#36801;&#31227;&#23398;&#20064;&#33021;&#22815;&#26356;&#26377;&#25928;&#22320;&#35757;&#32451;DL&#27169;&#22411;&#12290;&#19968;&#31181;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#30340;&#25216;&#26415;&#26159;&#27431;&#20960;&#37324;&#24471;&#23545;&#40784;&#65288;EA&#65289;&#65292;&#22240;&#20026;&#23427;&#26131;&#20110;&#20351;&#29992;&#12289;&#35745;&#31639;&#22797;&#26434;&#24230;&#20302;&#24182;&#19988;&#19982;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20860;&#23481;&#12290;&#28982;&#32780;&#65292;&#24456;&#23569;&#26377;&#30740;&#31350;&#35780;&#20272;&#20854;&#23545;&#20849;&#20139;&#21644;&#20010;&#20307;DL&#27169;&#22411;&#30340;&#35757;&#32451;&#25928;&#26524;&#30340;&#24433;&#21709;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#35780;&#20272;&#20102;EA&#19982;DL&#30456;&#32467;&#21512;&#22312;&#35299;&#30721;BCI&#20449;&#21495;&#20013;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#20351;&#29992;EA&#26469;&#35757;&#32451;&#26469;&#33258;&#22810;&#20010;&#21463;&#35797;&#32773;&#30340;&#20849;&#20139;&#27169;&#22411;&#65292;&#24182;&#35780;&#20272;&#20854;&#23545;&#26032;&#21463;&#35797;&#32773;&#30340;&#21487;&#36801;&#31227;&#24615;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#23427;&#23558;&#30446;&#26631;&#21463;&#35797;&#32773;&#30340;&#35299;&#30721;&#29575;&#25552;&#39640;&#20102;4.33&#65285;&#65292;&#24182;&#19988;&#25910;&#25947;&#26102;&#38388;&#32553;&#30701;&#20102;&#36229;&#36807;70&#65285;&#12290;&#25105;&#20204;&#36824;&#20026;&#20010;&#20307;&#27169;&#22411;&#36827;&#34892;&#20102;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Electroencephalography (EEG) signals are frequently used for various Brain-Computer Interface (BCI) tasks. While Deep Learning (DL) techniques have shown promising results, they are hindered by the substantial data requirements. By leveraging data from multiple subjects, transfer learning enables more effective training of DL models. A technique that is gaining popularity is Euclidean Alignment (EA) due to its ease of use, low computational complexity, and compatibility with Deep Learning models. However, few studies evaluate its impact on the training performance of shared and individual DL models. In this work, we systematically evaluate the effect of EA combined with DL for decoding BCI signals. We used EA to train shared models with data from multiple subjects and evaluated its transferability to new subjects. Our experimental results show that it improves decoding in the target subject by 4.33% and decreases convergence time by more than 70%. We also trained individual models for 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20840;&#38754;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#32534;&#36753;&#65292;&#26088;&#22312;&#26377;&#25928;&#20462;&#25913;&#27169;&#22411;&#30340;&#34892;&#20026;&#65292;&#21516;&#26102;&#20445;&#25345;&#25972;&#20307;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.01286</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#32534;&#36753;&#20840;&#38754;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Study of Knowledge Editing for Large Language Models. (arXiv:2401.01286v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01286
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20840;&#38754;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#32534;&#36753;&#65292;&#26088;&#22312;&#26377;&#25928;&#20462;&#25913;&#27169;&#22411;&#30340;&#34892;&#20026;&#65292;&#21516;&#26102;&#20445;&#25345;&#25972;&#20307;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#22312;&#29702;&#35299;&#21644;&#29983;&#25104;&#19982;&#20154;&#31867;&#20132;&#27969;&#32039;&#23494;&#30456;&#20284;&#30340;&#25991;&#26412;&#26041;&#38754;&#23637;&#29616;&#20986;&#20102;&#38750;&#20961;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#20854;&#20027;&#35201;&#38480;&#21046;&#22312;&#20110;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#26174;&#33879;&#35745;&#31639;&#38656;&#27714;&#65292;&#36825;&#26159;&#30001;&#20110;&#20854;&#24191;&#27867;&#30340;&#21442;&#25968;&#21270;&#36896;&#25104;&#30340;&#12290;&#36825;&#19968;&#25361;&#25112;&#22312;&#20110;&#19990;&#30028;&#30340;&#21160;&#24577;&#24615;&#65292;&#38656;&#35201;&#39057;&#32321;&#26356;&#26032;LLM&#20197;&#20462;&#27491;&#36807;&#26102;&#30340;&#20449;&#24687;&#25110;&#38598;&#25104;&#26032;&#30693;&#35782;&#65292;&#20174;&#32780;&#30830;&#20445;&#20854;&#25345;&#32493;&#30340;&#30456;&#20851;&#24615;&#12290;&#35768;&#22810;&#24212;&#29992;&#38656;&#35201;&#22312;&#35757;&#32451;&#21518;&#36827;&#34892;&#25345;&#32493;&#30340;&#27169;&#22411;&#35843;&#25972;&#65292;&#20197;&#35299;&#20915;&#32570;&#38519;&#25110;&#19981;&#33391;&#34892;&#20026;&#12290;&#36817;&#24180;&#26469;&#65292;&#23545;&#20110;LLM&#30340;&#30693;&#35782;&#32534;&#36753;&#25216;&#26415;&#30340;&#20852;&#36259;&#36234;&#26469;&#36234;&#39640;&#65292;&#22312;&#29305;&#23450;&#39046;&#22495;&#20869;&#26377;&#25928;&#22320;&#20462;&#25913;LLM&#30340;&#34892;&#20026;&#65292;&#21516;&#26102;&#20445;&#25345;&#25972;&#20307;&#24615;&#33021;&#22312;&#21508;&#31181;&#36755;&#20837;&#20013;&#30340;&#34920;&#29616;&#12290;&#26412;&#25991;&#39318;&#20808;&#23450;&#20041;&#20102;&#30693;&#35782;&#32534;&#36753;&#30340;&#30446;&#26631;&#21644;&#25361;&#25112;&#65292;&#28982;&#21518;&#32508;&#36848;&#20102;&#29616;&#26377;&#30340;&#30693;&#35782;&#32534;&#36753;&#26041;&#27861;&#21644;&#25216;&#26415;&#65292;&#24182;&#35752;&#35770;&#20102;&#20854;&#24212;&#29992;&#21644;&#26410;&#26469;&#21457;&#23637;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have shown extraordinary capabilities in understanding and generating text that closely mirrors human communication. However, a primary limitation lies in the significant computational demands during training, arising from their extensive parameterization. This challenge is further intensified by the dynamic nature of the world, necessitating frequent updates to LLMs to correct outdated information or integrate new knowledge, thereby ensuring their continued relevance. Note that many applications demand continual model adjustments post-training to address deficiencies or undesirable behaviors. There is an increasing interest in efficient, lightweight methods for on-the-fly model modifications. To this end, recent years have seen a burgeoning in the techniques of knowledge editing for LLMs, which aim to efficiently modify LLMs' behaviors within specific domains while preserving overall performance across various inputs. In this paper, we first define the kno
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20851;&#27880;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#65292;&#36890;&#36807;&#23450;&#21046;&#25552;&#31034;&#26469;&#35843;&#25972;&#38745;&#24577;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#65292;&#30740;&#31350;&#20102;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#25512;&#29702;&#20219;&#21153;&#20013;&#24615;&#33021;&#21644;&#26657;&#20934;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;&#30740;&#31350;&#21457;&#29616;&#38543;&#30528;ICL&#31034;&#20363;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;&#27169;&#22411;&#30340;&#26657;&#20934;&#20250;&#20808;&#22686;&#21152;&#32780;&#21518;&#24471;&#21040;&#25913;&#21892;&#65292;&#32780;&#26657;&#20934;&#35823;&#24046;&#20027;&#35201;&#20986;&#29616;&#22312;&#20302;&#26679;&#26412;&#22330;&#26223;&#19979;&#12290;&#27492;&#22806;&#65292;&#24494;&#35843;&#21644;CoT&#25552;&#31034;&#31561;&#26041;&#27861;&#21487;&#33021;&#23548;&#33268;&#26657;&#20934;&#35823;&#24046;&#21644;&#19981;&#21487;&#38752;&#30340;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#65292;&#25552;&#31034;&#38656;&#35201;&#38024;&#23545;&#21487;&#38752;&#24615;&#22330;&#26223;&#24320;&#21457;&#26032;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2312.04021</link><description>&lt;p&gt;
&#20851;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#26657;&#20934;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Study on the Calibration of In-context Learning. (arXiv:2312.04021v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.04021
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20851;&#27880;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#65292;&#36890;&#36807;&#23450;&#21046;&#25552;&#31034;&#26469;&#35843;&#25972;&#38745;&#24577;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#65292;&#30740;&#31350;&#20102;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#25512;&#29702;&#20219;&#21153;&#20013;&#24615;&#33021;&#21644;&#26657;&#20934;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;&#30740;&#31350;&#21457;&#29616;&#38543;&#30528;ICL&#31034;&#20363;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;&#27169;&#22411;&#30340;&#26657;&#20934;&#20250;&#20808;&#22686;&#21152;&#32780;&#21518;&#24471;&#21040;&#25913;&#21892;&#65292;&#32780;&#26657;&#20934;&#35823;&#24046;&#20027;&#35201;&#20986;&#29616;&#22312;&#20302;&#26679;&#26412;&#22330;&#26223;&#19979;&#12290;&#27492;&#22806;&#65292;&#24494;&#35843;&#21644;CoT&#25552;&#31034;&#31561;&#26041;&#27861;&#21487;&#33021;&#23548;&#33268;&#26657;&#20934;&#35823;&#24046;&#21644;&#19981;&#21487;&#38752;&#30340;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#65292;&#25552;&#31034;&#38656;&#35201;&#38024;&#23545;&#21487;&#38752;&#24615;&#22330;&#26223;&#24320;&#21457;&#26032;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#23545;&#20110;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#30340;&#23433;&#20840;&#37096;&#32626;&#33267;&#20851;&#37325;&#35201;&#65292;&#20197;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#35777;&#26126;&#20102;&#29616;&#20195;LMs&#26657;&#20934;&#24615;&#30340;&#25913;&#36827;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#37325;&#28857;&#26159;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#65292;&#19968;&#31181;&#36890;&#36807;&#23450;&#21046;&#25552;&#31034;&#26469;&#35843;&#25972;&#38745;&#24577;LMs&#30340;&#24120;&#35265;&#26041;&#27861;&#65292;&#24182;&#30740;&#31350;&#22312;&#24191;&#27867;&#30340;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#25512;&#29702;&#20219;&#21153;&#20013;&#24615;&#33021;&#21644;&#26657;&#20934;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;&#36890;&#36807;&#20840;&#38754;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#38543;&#30528;ICL&#31034;&#20363;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;&#27169;&#22411;&#26368;&#21021;&#20250;&#20986;&#29616;&#22686;&#21152;&#30340;&#26657;&#20934;&#35823;&#24046;&#65292;&#28982;&#21518;&#25165;&#33021;&#23454;&#29616;&#26356;&#22909;&#30340;&#26657;&#20934;&#65292;&#32780;&#26657;&#20934;&#35823;&#24046;&#24448;&#24448;&#22312;&#20302;&#26679;&#26412;&#22330;&#26223;&#19979;&#20986;&#29616;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#20197;&#25552;&#39640;&#21487;&#29992;&#24615;&#20026;&#30446;&#26631;&#30340;&#26041;&#27861;&#65292;&#22914;&#24494;&#35843;&#21644;CoT&#25552;&#31034;&#65292;&#21487;&#33021;&#23548;&#33268;&#26657;&#20934;&#35823;&#24046;&#21644;&#19981;&#21487;&#38752;&#30340;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#65292;&#36825;&#34920;&#26126;&#22312;&#26399;&#26395;&#27169;&#22411;&#21487;&#38752;&#24615;&#30340;&#22330;&#26223;&#20013;&#21487;&#33021;&#38656;&#35201;&#26032;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate uncertainty quantification is crucial for the safe deployment of language models (LMs), and prior research has demonstrated improvements in the calibration of modern LMs. Our study focuses on in-context learning (ICL), a prevalent method for adapting static LMs through tailored prompts, and examines the balance between performance and calibration across a broad spectrum of natural language understanding and reasoning tasks. Through comprehensive experiments, we observe that, with an increasing number of ICL examples, models initially exhibit increased miscalibration before achieving better calibration and miscalibration tends to arise in low-shot settings. Moreover, we find that methods aimed at improving usability, such as fine-tuning and chain-of-thought (CoT) prompting, can lead to miscalibration and unreliable natural language explanations, suggesting that new methods may be required for scenarios where models are expected to be reliable.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#23398;&#20064;&#25913;&#36827;&#20256;&#24863;&#22120;&#24067;&#25918;&#31574;&#30053;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#19982;&#20854;&#20182;&#26041;&#27861;&#30340;&#23545;&#27604;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#20135;&#29983;&#39640;&#36136;&#37327;&#35299;&#20915;&#26041;&#26696;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#21644;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.12387</link><description>&lt;p&gt;
&#20351;&#29992;Transformer&#23398;&#20064;&#35299;&#20915;&#27668;&#20505;&#20256;&#24863;&#22120;&#24067;&#25918;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Learning to Solve Climate Sensor Placement Problems with a Transformer. (arXiv:2310.12387v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12387
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#23398;&#20064;&#25913;&#36827;&#20256;&#24863;&#22120;&#24067;&#25918;&#31574;&#30053;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#19982;&#20854;&#20182;&#26041;&#27861;&#30340;&#23545;&#27604;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#20135;&#29983;&#39640;&#36136;&#37327;&#35299;&#20915;&#26041;&#26696;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#21644;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20854;NP&#38590;&#24615;&#36136;&#65292;&#29615;&#22659;&#30417;&#27979;&#21644;&#28798;&#23475;&#31649;&#29702;&#20013;&#30340;&#20256;&#24863;&#22120;&#24067;&#25918;&#20248;&#21270;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#20256;&#32479;&#30340;&#20256;&#24863;&#22120;&#24067;&#25918;&#26041;&#27861;&#21253;&#25324;&#31934;&#30830;&#12289;&#36817;&#20284;&#25110;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#20854;&#20013;&#21551;&#21457;&#24335;&#26041;&#27861;&#26159;&#26368;&#24120;&#29992;&#30340;&#12290;&#28982;&#32780;&#65292;&#21551;&#21457;&#24335;&#26041;&#27861;&#21463;&#21040;&#19987;&#23478;&#30452;&#35273;&#21644;&#32463;&#39564;&#30340;&#38480;&#21046;&#12290;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#24050;&#32463;&#25104;&#20026;&#33258;&#21160;&#29983;&#25104;&#21551;&#21457;&#24335;&#31639;&#27861;&#30340;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20256;&#24863;&#22120;&#24067;&#25918;&#26041;&#27861;&#65292;&#37325;&#28857;&#26159;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26041;&#27861;&#23398;&#20064;&#25913;&#36827;&#30340;&#21551;&#21457;&#24335;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;&#19968;&#20010;&#24378;&#21270;&#23398;&#20064;&#20844;&#24335;&#26469;&#23398;&#20064;&#25913;&#36827;&#30340;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#36890;&#36807;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#35757;&#32451;&#31574;&#30053;&#32593;&#32476;&#12290;&#25105;&#20204;&#36890;&#36807;&#36827;&#34892;&#20840;&#38754;&#30340;&#23454;&#39564;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#20135;&#29983;&#39640;&#36136;&#37327;&#35299;&#20915;&#26041;&#26696;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#21644;&#20248;&#36234;&#24615;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#27668;&#20505;&#20256;&#24863;&#22120;&#24067;&#25918;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The optimal placement of sensors for environmental monitoring and disaster management is a challenging problem due to its NP-hard nature. Traditional methods for sensor placement involve exact, approximation, or heuristic approaches, with the latter being the most widely used. However, heuristic methods are limited by expert intuition and experience. Deep learning (DL) has emerged as a promising approach for generating heuristic algorithms automatically. In this paper, we introduce a novel sensor placement approach focused on learning improvement heuristics using deep reinforcement learning (RL) methods. Our approach leverages an RL formulation for learning improvement heuristics, driven by an actor-critic algorithm for training the policy network. We compare our method with several state-of-the-art approaches by conducting comprehensive experiments, demonstrating the effectiveness and superiority of our proposed approach in producing high-quality solutions. Our work presents a promisi
&lt;/p&gt;</description></item><item><title>&#12298;Rayleigh Quotient Graph Neural Networks&#29992;&#20110;&#22270;&#32423;&#24322;&#24120;&#26816;&#27979;&#30340;&#30740;&#31350;&#12299;&#25552;&#20986;&#20351;&#29992;Rayleigh Quotient&#20316;&#20026;&#39537;&#21160;&#22240;&#32032;&#65292;&#36890;&#36807;&#25506;&#32034;&#22270;&#30340;&#22266;&#26377;&#20809;&#35889;&#29305;&#24449;&#26469;&#23454;&#29616;&#22270;&#32423;&#24322;&#24120;&#26816;&#27979;&#12290;</title><link>http://arxiv.org/abs/2310.02861</link><description>&lt;p&gt;
Rayleigh Quotient Graph Neural Networks&#29992;&#20110;&#22270;&#32423;&#24322;&#24120;&#26816;&#27979;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Rayleigh Quotient Graph Neural Networks for Graph-level Anomaly Detection. (arXiv:2310.02861v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02861
&lt;/p&gt;
&lt;p&gt;
&#12298;Rayleigh Quotient Graph Neural Networks&#29992;&#20110;&#22270;&#32423;&#24322;&#24120;&#26816;&#27979;&#30340;&#30740;&#31350;&#12299;&#25552;&#20986;&#20351;&#29992;Rayleigh Quotient&#20316;&#20026;&#39537;&#21160;&#22240;&#32032;&#65292;&#36890;&#36807;&#25506;&#32034;&#22270;&#30340;&#22266;&#26377;&#20809;&#35889;&#29305;&#24449;&#26469;&#23454;&#29616;&#22270;&#32423;&#24322;&#24120;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#32423;&#24322;&#24120;&#26816;&#27979;&#22312;&#30284;&#30151;&#35786;&#26029;&#21644;&#37238;&#39044;&#27979;&#31561;&#39046;&#22495;&#20013;&#24191;&#27867;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#26080;&#27861;&#25429;&#25417;&#21040;&#22270;&#24322;&#24120;&#30340;&#28508;&#22312;&#23646;&#24615;&#65292;&#23548;&#33268;&#26694;&#26550;&#35774;&#35745;&#19981;&#21487;&#35299;&#37322;&#21644;&#24615;&#33021;&#19981;&#20196;&#20154;&#28385;&#24847;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36864;&#19968;&#27493;&#37325;&#26032;&#30740;&#31350;&#20102;&#24322;&#24120;&#21644;&#27491;&#24120;&#22270;&#20043;&#38388;&#30340;&#20809;&#35889;&#24046;&#24322;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#35266;&#23519;&#34920;&#26126;&#65292;&#36825;&#20004;&#20010;&#31867;&#20043;&#38388;&#30340;&#32047;&#35745;&#20809;&#35889;&#33021;&#37327;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22270;&#20449;&#21495;&#30340;&#32047;&#35745;&#20809;&#35889;&#33021;&#37327;&#21487;&#20197;&#29992;&#20854;&#29790;&#21033;&#21830;&#34920;&#31034;&#65292;&#36825;&#34920;&#26126;&#29790;&#21033;&#21830;&#26159;&#22270;&#24322;&#24120;&#23646;&#24615;&#30340;&#19968;&#20010;&#39537;&#21160;&#22240;&#32032;&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Rayleigh Quotient Graph Neural Network&#65288;RQGNN&#65289;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#22270;&#32423;&#24322;&#24120;&#26816;&#27979;&#30340;&#20809;&#35889;GNN&#65292;&#20026;&#25506;&#32034;&#24322;&#24120;&#22270;&#30340;&#22266;&#26377;&#20809;&#35889;&#29305;&#24449;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph-level anomaly detection has gained significant attention as it finds many applications in various domains, such as cancer diagnosis and enzyme prediction. However, existing methods fail to capture the underlying properties of graph anomalies, resulting in unexplainable framework design and unsatisfying performance. In this paper, we take a step back and re-investigate the spectral differences between anomalous and normal graphs. Our main observation shows a significant disparity in the accumulated spectral energy between these two classes. Moreover, we prove that the accumulated spectral energy of the graph signal can be represented by its Rayleigh Quotient, indicating that the Rayleigh Quotient is a driving factor behind the anomalous properties of graphs. Motivated by this, we propose Rayleigh Quotient Graph Neural Network (RQGNN), the first spectral GNN for graph-level anomaly detection, providing a new perspective on exploring the inherent spectral features of anomalous graph
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#21152;&#26435;&#26377;&#21521;&#22270;&#30340;&#26368;&#20005;&#26684;&#21487;&#25509;&#21463;&#30340;&#26368;&#30701;&#36335;&#24452;&#38382;&#39064;&#65292;&#21033;&#29992;&#36793;&#26435;&#19981;&#30830;&#23450;&#24615;&#36827;&#34892;&#35745;&#31639;&#25104;&#26412;&#20132;&#25442;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#23436;&#25972;&#30340;&#31639;&#27861;&#26469;&#35299;&#20915;&#27492;&#38382;&#39064;&#65292;&#24182;&#20445;&#35777;&#35299;&#30340;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2308.08453</link><description>&lt;p&gt;
&#26368;&#20005;&#26684;&#21487;&#25509;&#21463;&#30340;&#26368;&#30701;&#36335;&#24452;
&lt;/p&gt;
&lt;p&gt;
Tightest Admissible Shortest Path. (arXiv:2308.08453v1 [cs.DS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08453
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#21152;&#26435;&#26377;&#21521;&#22270;&#30340;&#26368;&#20005;&#26684;&#21487;&#25509;&#21463;&#30340;&#26368;&#30701;&#36335;&#24452;&#38382;&#39064;&#65292;&#21033;&#29992;&#36793;&#26435;&#19981;&#30830;&#23450;&#24615;&#36827;&#34892;&#35745;&#31639;&#25104;&#26412;&#20132;&#25442;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#23436;&#25972;&#30340;&#31639;&#27861;&#26469;&#35299;&#20915;&#27492;&#38382;&#39064;&#65292;&#24182;&#20445;&#35777;&#35299;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20013;&#30340;&#26368;&#30701;&#36335;&#24452;&#38382;&#39064;&#23545;&#20110;&#20154;&#24037;&#26234;&#33021;&#26469;&#35828;&#26159;&#22522;&#30784;&#24615;&#30340;&#12290;&#20960;&#20046;&#25152;&#26377;&#38382;&#39064;&#30340;&#21464;&#31181;&#21644;&#30456;&#20851;&#31639;&#27861;&#37117;&#24573;&#30053;&#20102;&#36793;&#26435;&#35745;&#31639;&#26102;&#38388;&#21450;&#20854;&#19982;&#26435;&#37325;&#19981;&#30830;&#23450;&#24615;&#30340;&#24120;&#35265;&#20851;&#31995;&#12290;&#36825;&#24847;&#21619;&#30528;&#32771;&#34385;&#36825;&#20123;&#22240;&#32032;&#21487;&#33021;&#20250;&#22312;&#30456;&#20851;&#24212;&#29992;&#20013;&#24102;&#26469;&#24615;&#33021;&#25552;&#21319;&#12290;&#26368;&#36817;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#21152;&#26435;&#26377;&#21521;&#22270;&#30340;&#25512;&#24191;&#26694;&#26550;&#65292;&#21487;&#20197;&#22810;&#27425;&#35745;&#31639;&#65288;&#20272;&#35745;&#65289;&#36793;&#26435;&#65292;&#38543;&#30528;&#31934;&#24230;&#30340;&#22686;&#21152;&#21644;&#36816;&#34892;&#26102;&#38388;&#30340;&#22686;&#21152;&#12290;&#25105;&#20204;&#22312;&#27492;&#26694;&#26550;&#19978;&#24341;&#20837;&#20102;&#23547;&#25214;&#26368;&#20005;&#26684;&#21487;&#25509;&#21463;&#30340;&#26368;&#30701;&#36335;&#24452;&#65288;TASP&#65289;&#30340;&#38382;&#39064;&#65307;&#36825;&#26159;&#23558;&#26368;&#30701;&#36335;&#24452;&#38382;&#39064;&#25512;&#24191;&#21040;&#26377;&#30028;&#19981;&#30830;&#23450;&#24615;&#30340;&#24773;&#20917;&#65292;&#20854;&#20013;&#21487;&#20197;&#36890;&#36807;&#35745;&#31639;&#25104;&#26412;&#26469;&#20132;&#25442;&#36793;&#26435;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23436;&#25972;&#30340;&#31639;&#27861;&#26469;&#35299;&#20915;TASP&#65292;&#24182;&#20445;&#35777;&#20102;&#35299;&#30340;&#36136;&#37327;&#12290;&#23454;&#35777;&#35780;&#20272;&#25903;&#25345;&#36825;&#31181;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The shortest path problem in graphs is fundamental to AI. Nearly all variants of the problem and relevant algorithms that solve them ignore edge-weight computation time and its common relation to weight uncertainty. This implies that taking these factors into consideration can potentially lead to a performance boost in relevant applications. Recently, a generalized framework for weighted directed graphs was suggested, where edge-weight can be computed (estimated) multiple times, at increasing accuracy and run-time expense. We build on this framework to introduce the problem of finding the tightest admissible shortest path (TASP); a path with the tightest suboptimality bound on the optimal cost. This is a generalization of the shortest path problem to bounded uncertainty, where edge-weight uncertainty can be traded for computational cost. We present a complete algorithm for solving TASP, with guarantees on solution quality. Empirical evaluation supports the effectiveness of this approac
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#35270;&#39057;&#29255;&#27573;&#20013;&#25552;&#21462;&#30340;&#29289;&#20307;&#34892;&#20026;&#36827;&#34892;&#25512;&#29702;&#26469;&#35782;&#21035;&#21103;&#35789;&#31867;&#22411;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25928;&#26524;&#19978;&#36229;&#36234;&#20102;&#20043;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.04132</link><description>&lt;p&gt;
&#23545;&#35270;&#39057;&#29255;&#27573;&#20013;&#29289;&#20307;&#34892;&#20026;&#30340;&#25512;&#29702;&#29992;&#20110;&#21103;&#35789;&#31867;&#22411;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Reasoning over the Behaviour of Objects in Video-Clips for Adverb-Type Recognition. (arXiv:2307.04132v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04132
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#35270;&#39057;&#29255;&#27573;&#20013;&#25552;&#21462;&#30340;&#29289;&#20307;&#34892;&#20026;&#36827;&#34892;&#25512;&#29702;&#26469;&#35782;&#21035;&#21103;&#35789;&#31867;&#22411;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25928;&#26524;&#19978;&#36229;&#36234;&#20102;&#20043;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26681;&#25454;&#23545;&#25551;&#36848;&#22330;&#26223;&#24207;&#21015;&#30340;&#21103;&#35789;&#26368;&#20339;&#35782;&#21035;&#26041;&#27861;&#20026;&#22522;&#30784;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#20174;&#21407;&#22987;&#35270;&#39057;&#29255;&#27573;&#20013;&#25552;&#21462;&#30340;&#29289;&#20307;&#34892;&#20026;&#36827;&#34892;&#25512;&#29702;&#26469;&#35782;&#21035;&#29255;&#27573;&#23545;&#24212;&#30340;&#21103;&#35789;&#31867;&#22411;&#12290;&#19982;&#20043;&#21069;&#38024;&#23545;&#24120;&#35268;&#22330;&#26223;&#21103;&#35789;&#35782;&#21035;&#30340;&#26041;&#27861;&#19981;&#21516;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36866;&#29992;&#20110;&#35270;&#39057;&#29255;&#27573;&#30340;&#34892;&#21160;&#31867;&#22411;&#26410;&#30693;&#30340;&#26356;&#19968;&#33324;&#30340;&#38382;&#39064;&#35774;&#32622;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#27969;&#31243;&#65292;&#20174;&#21407;&#22987;&#35270;&#39057;&#29255;&#27573;&#20013;&#25552;&#21462;&#20102;&#21487;&#20197;&#20154;&#31867;&#29702;&#35299;&#30340;&#29289;&#20307;&#34892;&#20026;&#20107;&#23454;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;&#31526;&#21495;&#21644;&#36716;&#25442;&#22120;&#30340;&#25512;&#29702;&#26041;&#27861;&#65292;&#23545;&#36825;&#20123;&#25552;&#21462;&#20986;&#30340;&#20107;&#23454;&#36827;&#34892;&#25805;&#20316;&#20197;&#35782;&#21035;&#21103;&#35789;&#31867;&#22411;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;&#20043;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#25903;&#25345;&#31526;&#21495;&#35270;&#39057;&#22788;&#29702;&#30340;&#24037;&#20316;&#65292;&#25105;&#20204;&#36824;&#21457;&#24067;&#20102;&#35270;&#39057;&#39046;&#22495;&#30456;&#20851;&#30340;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, following the intuition that adverbs describing scene-sequences are best identified by reasoning over high-level concepts of object-behavior, we propose the design of a new framework that reasons over object-behaviours extracted from raw-video-clips to recognize the clip's corresponding adverb-types. Importantly, while previous works for general scene adverb-recognition assume knowledge of the clips underlying action-types, our method is directly applicable in the more general problem setting where the action-type of a video-clip is unknown. Specifically, we propose a novel pipeline that extracts human-interpretable object-behaviour-facts from raw video clips and propose novel symbolic and transformer based reasoning methods that operate over these extracted facts to identify adverb-types. Experiment results demonstrate that our proposed methods perform favourably against the previous state-of-the-art. Additionally, to support efforts in symbolic video-processing, we rele
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;FormAI&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;112,000&#20010;&#21487;&#32534;&#35793;&#30340;C&#31243;&#24207;&#65292;&#21033;&#29992;&#21160;&#24577;&#38646;-shot&#25552;&#31034;&#25216;&#26415;&#29983;&#25104;&#12290;&#36825;&#20123;&#31243;&#24207;&#32463;&#36807;&#24418;&#24335;&#39564;&#35777;&#65292;&#26631;&#35760;&#20102;&#28304;&#20195;&#30721;&#20013;&#30340;&#28431;&#27934;&#65292;&#24182;&#20351;&#29992;&#22810;&#31181;&#25216;&#26415;&#26469;&#25552;&#39640;&#31243;&#24207;&#30340;&#23433;&#20840;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.02192</link><description>&lt;p&gt;
FormAI&#25968;&#25454;&#38598;&#65306;&#20197;&#24418;&#24335;&#39564;&#35777;&#20026;&#35270;&#35282;&#30340;&#36719;&#20214;&#23433;&#20840;&#29983;&#25104;AI
&lt;/p&gt;
&lt;p&gt;
The FormAI Dataset: Generative AI in Software Security Through the Lens of Formal Verification. (arXiv:2307.02192v2 [cs.DB] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02192
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;FormAI&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;112,000&#20010;&#21487;&#32534;&#35793;&#30340;C&#31243;&#24207;&#65292;&#21033;&#29992;&#21160;&#24577;&#38646;-shot&#25552;&#31034;&#25216;&#26415;&#29983;&#25104;&#12290;&#36825;&#20123;&#31243;&#24207;&#32463;&#36807;&#24418;&#24335;&#39564;&#35777;&#65292;&#26631;&#35760;&#20102;&#28304;&#20195;&#30721;&#20013;&#30340;&#28431;&#27934;&#65292;&#24182;&#20351;&#29992;&#22810;&#31181;&#25216;&#26415;&#26469;&#25552;&#39640;&#31243;&#24207;&#30340;&#23433;&#20840;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;FormAI&#25968;&#25454;&#38598;&#65292;&#36825;&#26159;&#19968;&#20010;&#21253;&#21547;112,000&#20010;&#20855;&#26377;&#28431;&#27934;&#20998;&#31867;&#30340;AI&#29983;&#25104;&#30340;&#21487;&#32534;&#35793;&#21644;&#29420;&#31435;&#30340;C&#31243;&#24207;&#30340;&#22823;&#22411;&#38598;&#21512;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21160;&#24577;&#38646;-shot&#25552;&#31034;&#25216;&#26415;&#65292;&#29992;&#20110;&#29983;&#25104;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#22810;&#26679;&#21270;&#31243;&#24207;&#12290;&#35813;&#25968;&#25454;&#38598;&#30001;GPT-3.5-turbo&#29983;&#25104;&#65292;&#24182;&#21253;&#21547;&#20855;&#26377;&#19981;&#21516;&#22797;&#26434;&#24230;&#30340;&#31243;&#24207;&#12290;&#26377;&#20123;&#31243;&#24207;&#22788;&#29702;&#22797;&#26434;&#20219;&#21153;&#65292;&#22914;&#32593;&#32476;&#31649;&#29702;&#12289;&#26700;&#38754;&#28216;&#25103;&#25110;&#21152;&#23494;&#65292;&#32780;&#20854;&#20182;&#31243;&#24207;&#22788;&#29702;&#31616;&#21333;&#20219;&#21153;&#65292;&#22914;&#23383;&#31526;&#20018;&#25805;&#20316;&#12290;&#27599;&#20010;&#31243;&#24207;&#37117;&#29992;&#28304;&#20195;&#30721;&#20013;&#25214;&#21040;&#30340;&#28431;&#27934;&#36827;&#34892;&#26631;&#35760;&#65292;&#25351;&#31034;&#31867;&#22411;&#12289;&#34892;&#21495;&#21644;&#28431;&#27934;&#20989;&#25968;&#21517;&#12290;&#36825;&#26159;&#36890;&#36807;&#20351;&#29992;&#39640;&#25928;&#30340;&#22522;&#20110;SMT&#30340;&#26377;&#30028;&#27169;&#22411;&#26816;&#26597;&#22120;&#65288;ESBMC&#65289;&#30340;&#24418;&#24335;&#39564;&#35777;&#26041;&#27861;&#23454;&#29616;&#30340;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#27169;&#22411;&#26816;&#26597;&#12289;&#25277;&#35937;&#35299;&#37322;&#12289;&#32422;&#26463;&#32534;&#31243;&#21644;&#21487;&#28385;&#36275;&#24615;&#27169;&#29702;&#35770;&#26469;&#25512;&#29702;&#31243;&#24207;&#20013;&#30340;&#23433;&#20840;&#24615;/&#23433;&#20840;&#23646;&#24615;&#12290;&#36825;&#31181;&#26041;&#27861;&#30830;&#23450;&#20102;&#21644;&#39564;&#35777;&#20102;&#31243;&#24207;&#30340;&#23433;&#20840;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents the FormAI dataset, a large collection of 112, 000 AI-generated compilable and independent C programs with vulnerability classification. We introduce a dynamic zero-shot prompting technique constructed to spawn diverse programs utilizing Large Language Models (LLMs). The dataset is generated by GPT-3.5-turbo and comprises programs with varying levels of complexity. Some programs handle complicated tasks like network management, table games, or encryption, while others deal with simpler tasks like string manipulation. Every program is labeled with the vulnerabilities found within the source code, indicating the type, line number, and vulnerable function name. This is accomplished by employing a formal verification method using the Efficient SMT-based Bounded Model Checker (ESBMC), which uses model checking, abstract interpretation, constraint programming, and satisfiability modulo theories to reason over safety/security properties in programs. This approach definitiv
&lt;/p&gt;</description></item><item><title>RAPGen&#26159;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#38646;&#26679;&#26412;&#24773;&#20917;&#19979;&#20351;&#29992;Retrieval-Augmented Prompt Generation&#65288;RAPGen&#65289;&#26041;&#27861;&#65292;&#21363;&#20174;&#39044;&#20808;&#26500;&#24314;&#30340;&#24615;&#33021;Bug&#20462;&#22797;&#30693;&#35782;&#24211;&#20013;&#26816;&#32034;&#25552;&#31034;&#25351;&#20196;&#24182;&#29983;&#25104;&#25552;&#31034;&#65292;&#28982;&#21518;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#29983;&#25104;&#20462;&#22797;&#26041;&#26696;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#35299;&#20915;&#20195;&#30721;&#20302;&#25928;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#19987;&#23478;&#39564;&#35777;&#30340;&#25968;&#25454;&#38598;&#20013;&#65292;RAPGen&#22312;60%&#30340;&#24773;&#20917;&#19979;&#21487;&#20197;&#29983;&#25104;&#19982;&#24320;&#21457;&#32773;&#31561;&#25928;&#25110;&#26356;&#22909;&#30340;&#24615;&#33021;&#25913;&#36827;&#24314;&#35758;&#65292;&#20854;&#20013;&#32422;39%&#30340;&#24314;&#35758;&#23436;&#20840;&#30456;&#21516;&#12290;</title><link>http://arxiv.org/abs/2306.17077</link><description>&lt;p&gt;
RAPGen: &#19968;&#31181;&#35299;&#20915;&#38646;&#26679;&#26412;&#20195;&#30721;&#20302;&#25928;&#38382;&#39064;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
RAPGen: An Approach for Fixing Code Inefficiencies in Zero-Shot. (arXiv:2306.17077v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17077
&lt;/p&gt;
&lt;p&gt;
RAPGen&#26159;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#38646;&#26679;&#26412;&#24773;&#20917;&#19979;&#20351;&#29992;Retrieval-Augmented Prompt Generation&#65288;RAPGen&#65289;&#26041;&#27861;&#65292;&#21363;&#20174;&#39044;&#20808;&#26500;&#24314;&#30340;&#24615;&#33021;Bug&#20462;&#22797;&#30693;&#35782;&#24211;&#20013;&#26816;&#32034;&#25552;&#31034;&#25351;&#20196;&#24182;&#29983;&#25104;&#25552;&#31034;&#65292;&#28982;&#21518;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#29983;&#25104;&#20462;&#22797;&#26041;&#26696;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#35299;&#20915;&#20195;&#30721;&#20302;&#25928;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#19987;&#23478;&#39564;&#35777;&#30340;&#25968;&#25454;&#38598;&#20013;&#65292;RAPGen&#22312;60%&#30340;&#24773;&#20917;&#19979;&#21487;&#20197;&#29983;&#25104;&#19982;&#24320;&#21457;&#32773;&#31561;&#25928;&#25110;&#26356;&#22909;&#30340;&#24615;&#33021;&#25913;&#36827;&#24314;&#35758;&#65292;&#20854;&#20013;&#32422;39%&#30340;&#24314;&#35758;&#23436;&#20840;&#30456;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24615;&#33021;Bug&#26159;&#19968;&#31181;&#21363;&#20351;&#22312;&#32463;&#36807;&#20805;&#20998;&#27979;&#35797;&#30340;&#21830;&#19994;&#20135;&#21697;&#20013;&#20063;&#21487;&#33021;&#20986;&#29616;&#30340;&#38750;&#21151;&#33021;&#24615;&#38382;&#39064;&#12290;&#20462;&#22797;&#36825;&#20123;&#24615;&#33021;Bug&#26159;&#19968;&#20010;&#37325;&#35201;&#20294;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#36825;&#20010;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Retrieval-Augmented Prompt Generation&#65288;RAPGen&#65289;&#30340;&#26032;&#26041;&#27861;&#12290;&#32473;&#23450;&#19968;&#20010;&#23384;&#22312;&#24615;&#33021;&#38382;&#39064;&#30340;&#20195;&#30721;&#29255;&#27573;&#65292;RAPGen&#39318;&#20808;&#20174;&#39044;&#20808;&#26500;&#24314;&#30340;&#20043;&#21069;&#24615;&#33021;Bug&#20462;&#22797;&#30693;&#35782;&#24211;&#20013;&#26816;&#32034;&#19968;&#20010;&#25552;&#31034;&#25351;&#20196;&#65292;&#28982;&#21518;&#20351;&#29992;&#26816;&#32034;&#21040;&#30340;&#25351;&#20196;&#29983;&#25104;&#19968;&#20010;&#25552;&#31034;&#12290;&#28982;&#21518;&#65292;&#23427;&#22312;&#38646;&#26679;&#26412;&#24773;&#20917;&#19979;&#20351;&#29992;&#36825;&#20010;&#25552;&#31034;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;Codex&#65289;&#19978;&#29983;&#25104;&#19968;&#20010;&#20462;&#22797;&#26041;&#26696;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#21508;&#31181;&#25552;&#31034;&#21464;&#20307;&#21644;&#29616;&#26377;&#26041;&#27861;&#22312;&#24615;&#33021;Bug&#20462;&#22797;&#20219;&#21153;&#20013;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#32467;&#26524;&#26174;&#31034;&#65292;RAPGen&#22312;60%&#30340;&#24773;&#20917;&#19979;&#21487;&#20197;&#29983;&#25104;&#19982;&#24320;&#21457;&#32773;&#31561;&#25928;&#25110;&#26356;&#22909;&#30340;&#24615;&#33021;&#25913;&#36827;&#24314;&#35758;&#65292;&#22312;&#32463;&#36807;&#19987;&#23478;&#39564;&#35777;&#30340;&#36807;&#21435;C#&#24320;&#21457;&#32773;&#25152;&#20570;&#30340;&#24615;&#33021;&#26356;&#25913;&#25968;&#25454;&#38598;&#20013;&#26377;&#32422;39%&#30340;&#24314;&#35758;&#23436;&#20840;&#30456;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;
Performance bugs are non-functional bugs that can even manifest in well-tested commercial products. Fixing these performance bugs is an important yet challenging problem. In this work, we address this challenge and present a new approach called Retrieval-Augmented Prompt Generation (RAPGen). Given a code snippet with a performance issue, RAPGen first retrieves a prompt instruction from a pre-constructed knowledge-base of previous performance bug fixes and then generates a prompt using the retrieved instruction. It then uses this prompt on a Large Language Model (such as Codex) in zero-shot to generate a fix. We compare our approach with the various prompt variations and state of the art methods in the task of performance bug fixing. Our evaluation shows that RAPGen can generate performance improvement suggestions equivalent or better than a developer in ~60% of the cases, getting ~39% of them verbatim, in an expert-verified dataset of past performance changes made by C# developers.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22312;&#33258;&#32534;&#30721;&#22120;&#30340;&#25439;&#22833;&#20989;&#25968;&#20013;&#28155;&#21152;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;&#32422;&#26463;&#39033;&#65292;&#29992;&#20110;&#35299;&#20915;&#20256;&#32479;&#33258;&#32534;&#30721;&#22120;&#22312;&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#19981;&#36275;&#65292;&#24182;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2306.12627</link><description>&lt;p&gt;
&#38024;&#23545;&#24322;&#24120;&#26816;&#27979;&#30340;&#30446;&#26631;&#22604;&#32553;&#27491;&#21017;&#21270;&#33258;&#32534;&#30721;&#22120;&#65306;&#20013;&#24515;&#30340;&#40657;&#27934;
&lt;/p&gt;
&lt;p&gt;
Targeted collapse regularized autoencoder for anomaly detection: black hole at the center. (arXiv:2306.12627v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12627
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22312;&#33258;&#32534;&#30721;&#22120;&#30340;&#25439;&#22833;&#20989;&#25968;&#20013;&#28155;&#21152;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;&#32422;&#26463;&#39033;&#65292;&#29992;&#20110;&#35299;&#20915;&#20256;&#32479;&#33258;&#32534;&#30721;&#22120;&#22312;&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#19981;&#36275;&#65292;&#24182;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#32534;&#30721;&#22120;&#24050;&#24191;&#27867;&#29992;&#20110;&#26368;&#36817;&#30340;&#24322;&#24120;&#26816;&#27979;&#25216;&#26415;&#24320;&#21457;&#20013;&#12290;&#23427;&#20204;&#30340;&#24212;&#29992;&#21069;&#25552;&#26159;&#22312;&#27491;&#24120;&#35757;&#32451;&#25968;&#25454;&#19978;&#35757;&#32451;&#33258;&#32534;&#30721;&#22120;&#21518;&#65292;&#24322;&#24120;&#36755;&#20837;&#23558;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#37325;&#26500;&#35823;&#24046;&#12290;&#22240;&#27492;&#65292;&#36825;&#20351;&#24471;&#27491;&#24120;&#21644;&#24322;&#24120;&#26679;&#26412;&#20043;&#38388;&#26377;&#20102;&#26126;&#26174;&#30340;&#21306;&#21035;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#35266;&#23519;&#21040;&#65292;&#33258;&#32534;&#30721;&#22120;&#21487;&#20197;&#19968;&#23450;&#31243;&#24230;&#19978;&#27867;&#21270;&#21040;&#27491;&#24120;&#31867;&#20043;&#22806;&#65292;&#24182;&#22312;&#19968;&#20123;&#24322;&#24120;&#26679;&#26412;&#19978;&#23454;&#29616;&#36739;&#23567;&#30340;&#37325;&#26500;&#35823;&#24046;&#12290;&#20026;&#20102;&#25913;&#21892;&#24615;&#33021;&#65292;&#21508;&#31181;&#25216;&#26415;&#25552;&#20986;&#20102;&#20854;&#20182;&#32452;&#20214;&#21644;&#26356;&#22797;&#26434;&#30340;&#35757;&#32451;&#31243;&#24207;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#38750;&#24120;&#31616;&#21333;&#30340;&#26367;&#20195;&#26041;&#27861;&#65306;&#19981;&#26159;&#28155;&#21152;&#31070;&#32463;&#32593;&#32476;&#32452;&#20214;&#12289;&#28041;&#21450;&#35745;&#31639;&#21644;&#32321;&#29712;&#30340;&#35757;&#32451;&#65292;&#32780;&#26159;&#36890;&#36807;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#35843;&#33410;&#34920;&#31034;&#30340;&#33539;&#25968;&#65292;&#29992;&#19968;&#20010;&#35745;&#31639;&#31616;&#21333;&#30340;&#39033;&#26469;&#34917;&#20805;&#37325;&#26500;&#25439;&#22833;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#31616;&#21333;&#24615;&#26368;&#23567;&#21270;&#20102;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autoencoders have been extensively used in the development of recent anomaly detection techniques. The premise of their application is based on the notion that after training the autoencoder on normal training data, anomalous inputs will exhibit a significant reconstruction error. Consequently, this enables a clear differentiation between normal and anomalous samples. In practice, however, it is observed that autoencoders can generalize beyond the normal class and achieve a small reconstruction error on some of the anomalous samples. To improve the performance, various techniques propose additional components and more sophisticated training procedures. In this work, we propose a remarkably straightforward alternative: instead of adding neural network components, involved computations, and cumbersome training, we complement the reconstruction loss with a computationally light term that regulates the norm of representations in the latent space. The simplicity of our approach minimizes th
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;&#8220;&#36830;&#25509;&#35884;&#35823;&#8221;&#20316;&#20026;&#20844;&#24179;&#24615;&#38382;&#39064;&#22312;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#20013;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20123;&#38382;&#39064;&#65292;&#20197;&#24110;&#21161;AI&#30740;&#31350;&#20154;&#21592;&#21644;&#20174;&#19994;&#32773;&#36991;&#20813;&#31867;&#20284;&#24773;&#20917;&#22312;&#26410;&#26469;&#20013;&#22797;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.09535</link><description>&lt;p&gt;
&#12298;&#29747;&#36798;&#65292;&#20986;&#20102;&#20160;&#20040;&#38382;&#39064;&#65311;&#12299;&#8220;&#36830;&#25509;&#35884;&#35823;&#8221;&#20316;&#20026;&#20844;&#24179;&#24615;&#38382;&#39064;&#30340;&#25506;&#35752;
&lt;/p&gt;
&lt;p&gt;
What's the Problem, Linda? The Conjunction Fallacy as a Fairness Problem. (arXiv:2305.09535v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09535
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;&#8220;&#36830;&#25509;&#35884;&#35823;&#8221;&#20316;&#20026;&#20844;&#24179;&#24615;&#38382;&#39064;&#22312;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#20013;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20123;&#38382;&#39064;&#65292;&#20197;&#24110;&#21161;AI&#30740;&#31350;&#20154;&#21592;&#21644;&#20174;&#19994;&#32773;&#36991;&#20813;&#31867;&#20284;&#24773;&#20917;&#22312;&#26410;&#26469;&#20013;&#22797;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#27491;&#22312;&#19987;&#27880;&#20110;&#21019;&#24314;&#23613;&#21487;&#33021;&#25509;&#36817;&#20154;&#31867;&#26234;&#33021;&#30340;&#33258;&#21160;&#20915;&#31574;&#31995;&#32479;&#12290;&#36825;&#19968;&#21162;&#21147;&#25512;&#21160;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#20154;&#21592;&#25506;&#32034;&#24515;&#29702;&#23398;&#31561;&#35748;&#30693;&#39046;&#22495;&#12290; Daniel Kahneman&#21644;&#24050;&#25925;&#30340;Amos Tversky&#22312;&#26377;&#20559;&#35265;&#30340;&#20154;&#31867;&#20915;&#31574;&#21046;&#23450;&#26041;&#38754;&#30340;&#24037;&#20316;&#65292;&#21253;&#25324;&#23545;&#36830;&#25509;&#35884;&#35823;&#30340;&#30740;&#31350;&#65292;&#22240;&#27492;&#36827;&#34892;&#20102;&#31532;&#20108;&#27425;&#22797;&#20852;&#12290; &#22312;&#36830;&#25509;&#35884;&#35823;&#19979;&#65292;&#20915;&#31574;&#21046;&#23450;&#32773;&#20250;&#36829;&#21453;&#22522;&#26412;&#27010;&#29575;&#27861;&#21017;&#65292;&#35748;&#20026;&#36830;&#35789;&#27604;&#20854;&#20013;&#19968;&#20010;&#37096;&#20998;&#26356;&#26377;&#21487;&#33021;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#19982;&#29747;&#36798;&#38382;&#39064;&#26368;&#20026;&#33879;&#21517;&#30340;&#23454;&#39564;&#65292;&#23427;&#24050;&#34987;&#35777;&#26126;&#26159;&#32463;&#24471;&#36215;&#26102;&#38388;&#32771;&#39564;&#30340;&#12290;&#34429;&#28982;&#36825;&#31181;&#36328;&#23398;&#31185;&#30340;&#21162;&#21147;&#21463;&#21040;&#27426;&#36814;&#65292;&#20294;&#25105;&#20204;&#25285;&#24515;&#65292;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#20154;&#21592;&#24573;&#30053;&#20102;&#29747;&#36798;&#38382;&#39064;&#25152;&#25429;&#25417;&#21040;&#30340;&#39537;&#21160;&#21147;&#65306;&#29747;&#36798;&#24517;&#39035;&#34987;&#21051;&#26495;&#22320;&#25551;&#36848;&#20026;&#19968;&#20010;&#22899;&#24615;&#12290; &#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#29747;&#36798;&#38382;&#39064;&#65292;&#24182;&#23558;&#20854;&#24418;&#24335;&#21270;&#20026;AI&#20013;&#30340;&#20844;&#24179;&#24615;&#38382;&#39064;&#12290;&#25105;&#20204;&#35748;&#20026;&#36830;&#25509;&#35884;&#35823;&#26159;&#20559;&#35265;&#25968;&#25454;&#38598;&#22914;&#20309;&#23548;&#33268;&#20559;&#35265;&#32467;&#26524;&#30340;&#26126;&#26174;&#20363;&#23376;&#65292;&#20174;&#32780;&#24310;&#32493;&#21644;&#25918;&#22823;&#29616;&#26377;&#30340;&#31995;&#32479;&#24615;&#20559;&#35265;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#32452;&#38382;&#39064;&#20379;AI&#30740;&#31350;&#20154;&#21592;&#21644;&#20174;&#19994;&#20154;&#21592;&#20351;&#29992;&#65292;&#20197;&#36991;&#20813;&#31867;&#20284;&#24773;&#20917;&#22312;&#26410;&#26469;&#21457;&#29983;&#12290;
&lt;/p&gt;
&lt;p&gt;
The field of Artificial Intelligence (AI) is focusing on creating automated decision-making (ADM) systems that operate as close as possible to human-like intelligence. This effort has pushed AI researchers into exploring cognitive fields like psychology. The work of Daniel Kahneman and the late Amos Tversky on biased human decision-making, including the study of the conjunction fallacy, has experienced a second revival because of this. Under the conjunction fallacy a human decision-maker will go against basic probability laws and rank as more likely a conjunction over one of its parts. It has been proven overtime through a set of experiments with the Linda Problem being the most famous one. Although this interdisciplinary effort is welcomed, we fear that AI researchers ignore the driving force behind the conjunction fallacy as captured by the Linda Problem: the fact that Linda must be stereotypically described as a woman. In this paper we revisit the Linda Problem and formulate it as a
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#30417;&#30563;&#32858;&#31867;&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30830;&#23450;&#24182;&#35782;&#21035;&#23545;&#20110;TBI&#31561;&#24613;&#24615;&#30142;&#30149;&#27835;&#30103;&#38750;&#24120;&#37325;&#35201;&#30340;&#29983;&#29702;&#29366;&#24577;&#12290;&#30740;&#31350;&#36824;&#21033;&#29992;&#20020;&#24202;&#25968;&#25454;&#39564;&#35777;&#24182;&#35299;&#37322;&#25152;&#35782;&#21035;&#30340;&#29983;&#29702;&#29366;&#24577;&#12290;</title><link>http://arxiv.org/abs/2303.13024</link><description>&lt;p&gt;
&#29992;&#20110;&#35782;&#21035;TBI&#29983;&#29702;&#29366;&#24577;&#30340;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#33258;&#30417;&#30563;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Self-Supervised Clustering of Multivariate Time-Series Data for Identifying TBI Physiological States. (arXiv:2303.13024v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13024
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#30417;&#30563;&#32858;&#31867;&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30830;&#23450;&#24182;&#35782;&#21035;&#23545;&#20110;TBI&#31561;&#24613;&#24615;&#30142;&#30149;&#27835;&#30103;&#38750;&#24120;&#37325;&#35201;&#30340;&#29983;&#29702;&#29366;&#24577;&#12290;&#30740;&#31350;&#36824;&#21033;&#29992;&#20020;&#24202;&#25968;&#25454;&#39564;&#35777;&#24182;&#35299;&#37322;&#25152;&#35782;&#21035;&#30340;&#29983;&#29702;&#29366;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#20855;&#26377;&#32570;&#22833;&#20540;&#30340;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30830;&#23450;&#20020;&#24202;&#30456;&#20851;&#30340;&#29983;&#29702;&#29366;&#24577;&#38750;&#24120;&#37325;&#35201;&#65292;&#36825;&#23545;&#20110;&#25552;&#20379;&#24613;&#24615;&#30142;&#30149;&#65288;&#22914;&#39045;&#33041;&#25439;&#20260;&#12289;&#21628;&#21560;&#34928;&#31469;&#21644;&#24515;&#21147;&#34928;&#31469;&#65289;&#30340;&#36866;&#24403;&#27835;&#30103;&#33267;&#20851;&#37325;&#35201;&#12290;&#21033;&#29992;&#38750;&#26102;&#38388;&#24207;&#21015;&#32858;&#31867;&#25110;&#25968;&#25454;&#25554;&#20540;&#21644;&#32858;&#21512;&#25216;&#26415;&#21487;&#33021;&#23548;&#33268;&#26377;&#20215;&#20540;&#20449;&#24687;&#30340;&#20002;&#22833;&#21644;&#20559;&#35265;&#20998;&#26512;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24212;&#29992;&#20102;&#22522;&#20110;&#33258;&#30417;&#30563;&#30340;SLAC-Time&#31639;&#27861;&#65292;&#36991;&#20813;&#20102;&#25554;&#20540;&#25110;&#32858;&#21512;&#65292;&#20174;&#32780;&#26356;&#26377;&#25928;&#22320;&#34920;&#31034;&#24613;&#24615;&#24739;&#32773;&#29366;&#24577;&#12290;&#36890;&#36807;&#20351;&#29992;SLAC-Time&#26469;&#32858;&#31867;&#22823;&#22411;&#30740;&#31350;&#25968;&#25454;&#38598;&#20013;&#30340;&#25968;&#25454;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;TBI&#29983;&#29702;&#29366;&#24577;&#21450;&#20854;&#20855;&#20307;&#29305;&#24449;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#21508;&#31181;&#32858;&#31867;&#35780;&#20272;&#25351;&#26631;&#65292;&#24182;&#32467;&#21512;&#20020;&#24202;&#39046;&#22495;&#19987;&#23478;&#30340;&#24847;&#35265;&#26469;&#39564;&#35777;&#21644;&#35299;&#37322;&#25152;&#35782;&#21035;&#30340;&#29983;&#29702;&#29366;&#24577;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#29305;&#23450;&#20020;&#24202;&#20107;&#20214;&#21644;&#29983;&#29702;&#29366;&#24577;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Determining clinically relevant physiological states from multivariate time series data with missing values is essential for providing appropriate treatment for acute conditions such as Traumatic Brain Injury (TBI), respiratory failure, and heart failure. Utilizing non-temporal clustering or data imputation and aggregation techniques may lead to loss of valuable information and biased analyses. In our study, we apply the SLAC-Time algorithm, an innovative self-supervision-based approach that maintains data integrity by avoiding imputation or aggregation, offering a more useful representation of acute patient states. By using SLAC-Time to cluster data in a large research dataset, we identified three distinct TBI physiological states and their specific feature profiles. We employed various clustering evaluation metrics and incorporated input from a clinical domain expert to validate and interpret the identified physiological states. Further, we discovered how specific clinical events and
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#25105;&#25552;&#31034;&#26694;&#26550;&#65292;&#21487;&#20197;&#26377;&#25928;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21442;&#25968;&#20013;&#23384;&#20648;&#30340;&#30693;&#35782;&#21644;&#25351;&#20196;&#29702;&#35299;&#33021;&#21147;&#65292;&#20197;&#23454;&#29616;&#38646;&#26679;&#26412;&#24320;&#25918;&#22495;&#38382;&#31572;&#65292;&#24182;&#19988;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#19977;&#20010;&#24191;&#27867;&#20351;&#29992;&#30340;ODQA&#25968;&#25454;&#38598;&#20013;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2212.08635</link><description>&lt;p&gt;
&#33258;&#25105;&#25552;&#31034;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#38646;&#26679;&#26412;&#24320;&#25918;&#22495;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
Self-Prompting Large Language Models for Zero-Shot Open-Domain QA. (arXiv:2212.08635v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.08635
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#25105;&#25552;&#31034;&#26694;&#26550;&#65292;&#21487;&#20197;&#26377;&#25928;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21442;&#25968;&#20013;&#23384;&#20648;&#30340;&#30693;&#35782;&#21644;&#25351;&#20196;&#29702;&#35299;&#33021;&#21147;&#65292;&#20197;&#23454;&#29616;&#38646;&#26679;&#26412;&#24320;&#25918;&#22495;&#38382;&#31572;&#65292;&#24182;&#19988;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#19977;&#20010;&#24191;&#27867;&#20351;&#29992;&#30340;ODQA&#25968;&#25454;&#38598;&#20013;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#25918;&#22495;&#38382;&#31572;&#30446;&#26631;&#22312;&#20110;&#22238;&#31572;&#20851;&#20110;&#20107;&#23454;&#30340;&#38382;&#39064;&#65292;&#32780;&#26080;&#38656;&#25552;&#20379;&#29305;&#23450;&#30340;&#32972;&#26223;&#25991;&#26723;&#12290;&#22312;&#38646;&#26679;&#26412;&#35774;&#32622;&#19979;&#65292;&#30001;&#20110;&#27809;&#26377;&#25968;&#25454;&#26469;&#35757;&#32451;&#31867;&#20284;&#26816;&#32034;&#22120;-&#38405;&#35835;&#22120;&#30340;&#23450;&#21046;&#27169;&#22411;&#65292;&#22240;&#27492;&#27492;&#20219;&#21153;&#26356;&#21152;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26368;&#36817;&#65292;&#20687;GPT-3&#36825;&#26679;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#36890;&#36807;&#30452;&#25509;&#25552;&#31034;&#26041;&#27861;&#22312;&#38646;&#26679;&#26412;&#24320;&#25918;&#22495;&#38382;&#31572;&#20013;&#23637;&#31034;&#20102;&#20854;&#24378;&#22823;&#30340;&#33021;&#21147;&#65292;&#20294;&#26159;&#36825;&#20123;&#26041;&#27861;&#20173;&#28982;&#36828;&#36828;&#19981;&#33021;&#20805;&#20998;&#21457;&#25381;LLM&#30340;&#24378;&#22823;&#21151;&#33021;&#65292;&#32780;&#21482;&#26159;&#20197;&#38544;&#24335;&#26041;&#24335;&#35843;&#29992;&#23427;&#20204;&#32780;&#24050;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#25105;&#25552;&#31034;&#26694;&#26550;&#65292;&#20197;&#26126;&#30830;&#21033;&#29992;LLM&#21442;&#25968;&#20013;&#23384;&#20648;&#30340;&#22823;&#37327;&#30693;&#35782;&#21644;&#20854;&#24378;&#22823;&#30340;&#25351;&#20196;&#29702;&#35299;&#33021;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36880;&#27493;&#25552;&#31034;LLM&#29983;&#25104;&#22810;&#20010;&#20266;QA&#23545;&#65292;&#24182;&#20174;&#22836;&#24320;&#22987;&#29983;&#25104;&#32972;&#26223;&#27573;&#33853;&#21644;&#35299;&#37322;&#65292;&#28982;&#21518;&#20351;&#29992;&#29983;&#25104;&#30340;&#20803;&#32032;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#26696;&#22312;&#19977;&#20010;&#24191;&#27867;&#20351;&#29992;&#30340;ODQA&#25968;&#25454;&#38598;&#19978;&#26174;&#33879;&#36229;&#36807;&#20102;&#20808;&#21069;&#30340;SOTA&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Open-Domain Question Answering (ODQA) aims at answering factoid questions without explicitly providing specific background documents. In a zero-shot setting, this task is more challenging since no data is available to train customized models like Retriever-Readers. Recently, Large Language Models (LLMs) like GPT-3 have shown their power in zero-shot ODQA with direct prompting methods, but these methods are still far from releasing the full powerfulness of LLMs only in an implicitly invoking way. In this paper, we propose a Self-Prompting framework to explicitly utilize the massive knowledge stored in the parameters of LLMs and their strong instruction understanding abilities. Concretely, we prompt LLMs step by step to generate multiple pseudo QA pairs with background passages and explanations from scratch and then use those generated elements for in-context learning. Experimental results show our method surpasses previous SOTA methods significantly on three widely-used ODQA datasets, a
&lt;/p&gt;</description></item></channel></rss>