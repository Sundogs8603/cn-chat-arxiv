<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>H2O&#25512;&#20986;&#20102;&#24320;&#25918;&#29983;&#24577;&#31995;&#32479;&#65292;&#26088;&#22312;&#24320;&#21457;&#21644;&#27979;&#35797;&#26368;&#20808;&#36827;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#21253;&#25324;h2oGPT&#21644;H2O LLM Studio&#12290;&#36825;&#19968;&#24320;&#28304;&#39033;&#30446;&#25552;&#20379;&#20102;&#20840;&#38754;&#24320;&#25918;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#33021;&#22815;&#24110;&#21161;&#25512;&#21160;&#20154;&#24037;&#26234;&#33021;&#30340;&#21457;&#23637;&#65292;&#20351;&#20854;&#26356;&#21152;&#21487;&#20449;&#36182;&#21644;&#21487;&#35775;&#38382;&#12290;</title><link>http://arxiv.org/abs/2310.13012</link><description>&lt;p&gt;
H2O&#24320;&#25918;&#29983;&#24577;&#31995;&#32479;&#29992;&#20110;&#26368;&#20808;&#36827;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
H2O Open Ecosystem for State-of-the-art Large Language Models. (arXiv:2310.13012v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13012
&lt;/p&gt;
&lt;p&gt;
H2O&#25512;&#20986;&#20102;&#24320;&#25918;&#29983;&#24577;&#31995;&#32479;&#65292;&#26088;&#22312;&#24320;&#21457;&#21644;&#27979;&#35797;&#26368;&#20808;&#36827;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#21253;&#25324;h2oGPT&#21644;H2O LLM Studio&#12290;&#36825;&#19968;&#24320;&#28304;&#39033;&#30446;&#25552;&#20379;&#20102;&#20840;&#38754;&#24320;&#25918;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#33021;&#22815;&#24110;&#21161;&#25512;&#21160;&#20154;&#24037;&#26234;&#33021;&#30340;&#21457;&#23637;&#65292;&#20351;&#20854;&#26356;&#21152;&#21487;&#20449;&#36182;&#21644;&#21487;&#35775;&#38382;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20195;&#34920;&#20102;&#20154;&#24037;&#26234;&#33021;&#30340;&#19968;&#39033;&#38761;&#21629;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20063;&#24102;&#26469;&#20102;&#35768;&#22810;&#37325;&#22823;&#39118;&#38505;&#65292;&#20363;&#22914;&#23384;&#22312;&#20559;&#35265;&#12289;&#31169;&#26377;&#12289;&#21463;&#29256;&#26435;&#20445;&#25252;&#25110;&#26377;&#23475;&#30340;&#25991;&#26412;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#38656;&#35201;&#24320;&#25918;&#12289;&#36879;&#26126;&#21644;&#23433;&#20840;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#23436;&#25972;&#30340;&#24320;&#28304;&#29983;&#24577;&#31995;&#32479;&#65292;&#29992;&#20110;&#24320;&#21457;&#21644;&#27979;&#35797;LLMs&#12290;&#35813;&#39033;&#30446;&#30340;&#30446;&#26631;&#26159;&#25512;&#21160;&#23545;&#23553;&#38381;&#28304;&#26041;&#27861;&#30340;&#24320;&#25918;&#24335;&#26367;&#20195;&#26041;&#26696;&#12290;&#25105;&#20204;&#21457;&#24067;&#20102;h2oGPT&#65292;&#21363;&#20174;70&#20159;&#21040;700&#20159;&#21442;&#25968;&#30340;&#19968;&#31995;&#21015;&#31934;&#32454;&#35843;&#25972;&#30340;LLMs&#12290;&#25105;&#20204;&#36824;&#25512;&#20986;&#20102;H2O LLM Studio&#65292;&#36825;&#26159;&#19968;&#20010;&#26694;&#26550;&#21644;&#26080;&#20195;&#30721;GUI&#65292;&#19987;&#20026;&#20351;&#29992;&#26368;&#26032;&#30340;&#20808;&#36827;&#25216;&#26415;&#36827;&#34892;LLMs&#30340;&#39640;&#25928;&#31934;&#32454;&#35843;&#25972;&#12289;&#35780;&#20272;&#21644;&#37096;&#32626;&#32780;&#35774;&#35745;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21644;&#27169;&#22411;&#22312;&#23436;&#20840;&#33258;&#30001;&#30340;Apache 2.0&#35768;&#21487;&#35777;&#19979;&#25480;&#26435;&#20351;&#29992;&#12290;&#25105;&#20204;&#30456;&#20449;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#26377;&#21161;&#20110;&#25512;&#21160;&#20154;&#24037;&#26234;&#33021;&#30340;&#21457;&#23637;&#65292;&#24182;&#20351;&#20854;&#26356;&#21487;&#35775;&#38382;&#21644;&#21487;&#20449;&#36182;&#12290;&#28436;&#31034;&#32593;&#22336;&#20026;&#65306;https://gpt.h2o.ai/
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) represent a revolution in AI. However, they also pose many significant risks, such as the presence of biased, private, copyrighted or harmful text. For this reason we need open, transparent and safe solutions. We introduce a complete open-source ecosystem for developing and testing LLMs. The goal of this project is to boost open alternatives to closed-source approaches. We release h2oGPT, a family of fine-tuned LLMs from 7 to 70 Billion parameters. We also introduce H2O LLM Studio, a framework and no-code GUI designed for efficient fine-tuning, evaluation, and deployment of LLMs using the most recent state-of-the-art techniques. Our code and models are licensed under fully permissive Apache 2.0 licenses. We believe open-source language models help to boost AI development and make it more accessible and trustworthy. The demo is available at: https://gpt.h2o.ai/
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#24212;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#21327;&#21516;&#21019;&#20316;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#32467;&#26500;&#21270;&#29983;&#25104;&#35774;&#35745;&#31354;&#38388;&#24182;&#25552;&#20379;&#26080;&#32541;&#25506;&#32034;&#12289;&#35780;&#20272;&#21644;&#32508;&#21512;&#22810;&#31181;&#21709;&#24212;&#30340;&#21151;&#33021;&#65292;&#25299;&#23637;&#20102;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21019;&#24847;&#20219;&#21153;&#20013;&#30340;&#20132;&#20114;&#26041;&#24335;&#12290;</title><link>http://arxiv.org/abs/2310.12953</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#21327;&#21516;&#21019;&#20316;&#30340;&#35774;&#35745;&#31354;&#38388;&#32467;&#26500;&#21270;&#29983;&#25104;&#19982;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Structured Generation and Exploration of Design Space with Large Language Models for Human-AI Co-Creation. (arXiv:2310.12953v2 [cs.HC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12953
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#24212;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#21327;&#21516;&#21019;&#20316;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#32467;&#26500;&#21270;&#29983;&#25104;&#35774;&#35745;&#31354;&#38388;&#24182;&#25552;&#20379;&#26080;&#32541;&#25506;&#32034;&#12289;&#35780;&#20272;&#21644;&#32508;&#21512;&#22810;&#31181;&#21709;&#24212;&#30340;&#21151;&#33021;&#65292;&#25299;&#23637;&#20102;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21019;&#24847;&#20219;&#21153;&#20013;&#30340;&#20132;&#20114;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#20973;&#20511;&#20854;&#29983;&#25104;&#33021;&#21147;&#65292;&#24050;&#25104;&#20026;&#21019;&#24847;&#36807;&#31243;&#20013;&#30340;&#26080;&#20215;&#24037;&#20855;&#12290;&#36825;&#20123;&#27169;&#22411;&#33021;&#22815;&#20135;&#29983;&#25968;&#30334;&#29978;&#33267;&#25968;&#21315;&#20010;&#35270;&#35273;&#21644;&#25991;&#26412;&#36755;&#20986;&#65292;&#20026;&#21019;&#24847;&#21162;&#21147;&#25552;&#20379;&#20102;&#20016;&#23500;&#30340;&#28789;&#24863;&#12290;&#20294;&#25105;&#20204;&#26159;&#21542;&#20805;&#20998;&#21457;&#25381;&#20102;&#23427;&#20204;&#30340;&#28508;&#21147;&#65311;&#25105;&#20204;&#35748;&#20026;&#24403;&#21069;&#30340;&#20132;&#20114;&#33539;&#24335;&#23384;&#22312;&#19981;&#36275;&#20043;&#22788;&#65292;&#23558;&#29992;&#25143;&#24341;&#23548;&#21040;&#26377;&#38480;&#30340;&#19968;&#32452;&#24819;&#27861;&#19978;&#65292;&#32780;&#19981;&#26159;&#36171;&#20104;&#20182;&#20204;&#25506;&#32034;&#29983;&#25104;&#27169;&#22411;&#20013;&#24191;&#38420;&#30340;&#28508;&#22312;&#35774;&#35745;&#31354;&#38388;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#20419;&#36827;&#20102;&#35774;&#35745;&#31354;&#38388;&#30340;&#32467;&#26500;&#21270;&#29983;&#25104;&#65292;&#29992;&#25143;&#21487;&#20197;&#26080;&#32541;&#22320;&#25506;&#32034;&#12289;&#35780;&#20272;&#21644;&#32508;&#21512;&#22810;&#31181;&#21709;&#24212;&#12290;&#36890;&#36807;&#35774;&#35745;&#21644;&#24320;&#21457;&#19968;&#20010;&#20132;&#20114;&#31995;&#32479;Luminate&#65292;&#24182;&#19982;8&#21517;&#19987;&#19994;&#20316;&#23478;&#36827;&#34892;&#29992;&#25143;&#30740;&#31350;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#35813;&#26694;&#26550;&#30340;&#21487;&#34892;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#25512;&#36827;&#20102;&#25105;&#20204;&#19982;LLM&#22312;&#21019;&#24847;&#20219;&#21153;&#20013;&#30340;&#20132;&#20114;&#26041;&#24335;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#25506;&#32034;&#20854;&#28508;&#21147;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Thanks to their generative capabilities, large language models (LLMs) have become an invaluable tool for creative processes. These models have the capacity to produce hundreds and thousands of visual and textual outputs, offering abundant inspiration for creative endeavors. But are we harnessing their full potential? We argue that current interaction paradigms fall short, guiding users towards rapid convergence on a limited set of ideas, rather than empowering them to explore the vast latent design space in generative models. To address this limitation, we propose a framework that facilitates the structured generation of design space in which users can seamlessly explore, evaluate, and synthesize a multitude of responses. We demonstrate the feasibility and usefulness of this framework through the design and development of an interactive system, Luminate, and a user study with 8 professional writers. Our work advances how we interact with LLMs for creative tasks, introducing a way to ha
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;AgentTuning&#65292;&#19968;&#31181;&#31616;&#21333;&#32780;&#36890;&#29992;&#30340;&#26041;&#27861;&#65292;&#21487;&#25552;&#21319;LLMs&#30340;&#20195;&#29702;&#33021;&#21147;&#65292;&#21516;&#26102;&#20445;&#25345;&#20854;&#36890;&#29992;&#33021;&#21147;&#12290;&#36890;&#36807;&#26500;&#24314;AgentInstruct&#25968;&#25454;&#38598;&#65292;&#24182;&#37319;&#29992;&#19968;&#31181;&#28151;&#21512;&#35757;&#32451;&#26041;&#27861;&#65292;&#20316;&#32773;&#25104;&#21151;&#22320;&#23454;&#29616;&#20102;&#25552;&#39640;LLMs&#20195;&#29702;&#33021;&#21147;&#30340;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2310.12823</link><description>&lt;p&gt;
AgentTuning: &#20026;LLMs&#23454;&#29616;&#36890;&#29992;&#20195;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
AgentTuning: Enabling Generalized Agent Abilities for LLMs. (arXiv:2310.12823v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12823
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;AgentTuning&#65292;&#19968;&#31181;&#31616;&#21333;&#32780;&#36890;&#29992;&#30340;&#26041;&#27861;&#65292;&#21487;&#25552;&#21319;LLMs&#30340;&#20195;&#29702;&#33021;&#21147;&#65292;&#21516;&#26102;&#20445;&#25345;&#20854;&#36890;&#29992;&#33021;&#21147;&#12290;&#36890;&#36807;&#26500;&#24314;AgentInstruct&#25968;&#25454;&#38598;&#65292;&#24182;&#37319;&#29992;&#19968;&#31181;&#28151;&#21512;&#35757;&#32451;&#26041;&#27861;&#65292;&#20316;&#32773;&#25104;&#21151;&#22320;&#23454;&#29616;&#20102;&#25552;&#39640;LLMs&#20195;&#29702;&#33021;&#21147;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#25918;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#20855;&#26377;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#26497;&#22823;&#22320;&#25512;&#21160;&#20102;LLMs&#30340;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#24403;&#23427;&#20204;&#20316;&#20026;&#20195;&#29702;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#24212;&#23545;&#22797;&#26434;&#20219;&#21153;&#26102;&#65292;&#23427;&#20204;&#36828;&#19981;&#21450;ChatGPT&#21644;GPT-4&#31561;&#21830;&#19994;&#27169;&#22411;&#12290;&#36825;&#20123;&#20195;&#29702;&#20219;&#21153;&#23558;LLMs&#20316;&#20026;&#36127;&#36131;&#35268;&#21010;&#12289;&#35760;&#24518;&#21644;&#24037;&#20855;&#21033;&#29992;&#30340;&#20013;&#22830;&#25511;&#21046;&#22120;&#65292;&#38656;&#35201;&#32454;&#31890;&#24230;&#30340;&#25552;&#31034;&#26041;&#27861;&#21644;&#24378;&#22823;&#30340;LLMs&#25165;&#33021;&#36798;&#21040;&#20196;&#20154;&#28385;&#24847;&#30340;&#24615;&#33021;&#12290;&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#25552;&#31034;&#26041;&#27861;&#26469;&#23436;&#25104;&#29305;&#23450;&#30340;&#20195;&#29702;&#20219;&#21153;&#65292;&#20294;&#32570;&#20047;&#30740;&#31350;&#19987;&#27880;&#20110;&#25552;&#39640;LLMs&#33258;&#36523;&#30340;&#20195;&#29702;&#33021;&#21147;&#32780;&#19981;&#25439;&#23475;&#20854;&#36890;&#29992;&#33021;&#21147;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;AgentTuning&#65292;&#19968;&#31181;&#31616;&#21333;&#32780;&#36890;&#29992;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#25552;&#21319;LLMs&#30340;&#20195;&#29702;&#33021;&#21147;&#65292;&#21516;&#26102;&#20445;&#25345;&#20854;&#36890;&#29992;&#30340;LLM&#33021;&#21147;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;AgentInstruct&#65292;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;&#25351;&#20196;&#35843;&#25972;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#39640;&#36136;&#37327;&#30340;&#20132;&#20114;&#36712;&#36857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Open large language models (LLMs) with great performance in various tasks have significantly advanced the development of LLMs. However, they are far inferior to commercial models such as ChatGPT and GPT-4 when acting as agents to tackle complex tasks in the real world. These agent tasks employ LLMs as the central controller responsible for planning, memorization, and tool utilization, necessitating both fine-grained prompting methods and robust LLMs to achieve satisfactory performance. Though many prompting methods have been proposed to complete particular agent tasks, there is lack of research focusing on improving the agent capabilities of LLMs themselves without compromising their general abilities. In this work, we present AgentTuning, a simple and general method to enhance the agent abilities of LLMs while maintaining their general LLM capabilities. We construct AgentInstruct, a lightweight instruction-tuning dataset containing high-quality interaction trajectories. We employ a hy
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35821;&#35328;&#27169;&#22411;&#39044;&#35757;&#32451;&#26694;&#26550;&#65292;&#33021;&#22815;&#26126;&#30830;&#32771;&#34385;&#21040;&#25991;&#26412;&#23646;&#24615;&#24322;&#26500;&#22270;&#20013;&#30340;&#25299;&#25169;&#21644;&#24322;&#26500;&#20449;&#24687;&#12290;&#36890;&#36807;&#20248;&#21270;&#35821;&#35328;&#27169;&#22411;&#21644;&#36741;&#21161;&#30340;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#39044;&#27979;&#20102;&#25991;&#26412;&#23646;&#24615;&#24322;&#26500;&#22270;&#20013;&#30340;&#33410;&#28857;&#12290;&#21516;&#26102;&#65292;&#36824;&#35774;&#35745;&#20102;&#19968;&#20010;&#25991;&#26412;&#20016;&#23500;&#24615;&#21152;&#26435;&#30340;&#33410;&#28857;&#25277;&#26679;&#31574;&#30053;&#65292;&#20197;&#26356;&#22909;&#22320;&#21033;&#29992;&#25991;&#26412;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2310.12580</link><description>&lt;p&gt;
&#20351;&#29992;&#25991;&#26412;&#23646;&#24615;&#24322;&#26500;&#22270;&#36827;&#34892;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Pretraining Language Models with Text-Attributed Heterogeneous Graphs. (arXiv:2310.12580v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12580
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35821;&#35328;&#27169;&#22411;&#39044;&#35757;&#32451;&#26694;&#26550;&#65292;&#33021;&#22815;&#26126;&#30830;&#32771;&#34385;&#21040;&#25991;&#26412;&#23646;&#24615;&#24322;&#26500;&#22270;&#20013;&#30340;&#25299;&#25169;&#21644;&#24322;&#26500;&#20449;&#24687;&#12290;&#36890;&#36807;&#20248;&#21270;&#35821;&#35328;&#27169;&#22411;&#21644;&#36741;&#21161;&#30340;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#39044;&#27979;&#20102;&#25991;&#26412;&#23646;&#24615;&#24322;&#26500;&#22270;&#20013;&#30340;&#33410;&#28857;&#12290;&#21516;&#26102;&#65292;&#36824;&#35774;&#35745;&#20102;&#19968;&#20010;&#25991;&#26412;&#20016;&#23500;&#24615;&#21152;&#26435;&#30340;&#33410;&#28857;&#25277;&#26679;&#31574;&#30053;&#65292;&#20197;&#26356;&#22909;&#22320;&#21033;&#29992;&#25991;&#26412;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#23454;&#38469;&#22330;&#26223;&#20013;&#65288;&#22914;&#23398;&#26415;&#32593;&#32476;&#12289;&#31038;&#20132;&#24179;&#21488;&#65289;&#65292;&#19981;&#21516;&#31867;&#22411;&#30340;&#23454;&#20307;&#19981;&#20165;&#19982;&#25991;&#26412;&#30456;&#20851;&#65292;&#36824;&#36890;&#36807;&#21508;&#31181;&#20851;&#31995;&#30456;&#36830;&#65292;&#36825;&#21487;&#20197;&#34987;&#25277;&#35937;&#20026;&#25991;&#26412;&#23646;&#24615;&#24322;&#26500;&#22270;&#65288;Text-Attributed Heterogeneous Graphs&#65292;TAHGs&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
In many real-world scenarios (e.g., academic networks, social platforms), different types of entities are not only associated with texts but also connected by various relationships, which can be abstracted as Text-Attributed Heterogeneous Graphs (TAHGs). Current pretraining tasks for Language Models (LMs) primarily focus on separately learning the textual information of each entity and overlook the crucial aspect of capturing topological connections among entities in TAHGs. In this paper, we present a new pretraining framework for LMs that explicitly considers the topological and heterogeneous information in TAHGs. Firstly, we define a context graph as neighborhoods of a target node within specific orders and propose a topology-aware pretraining task to predict nodes involved in the context graph by jointly optimizing an LM and an auxiliary heterogeneous graph neural network. Secondly, based on the observation that some nodes are text-rich while others have little text, we devise a tex
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23545;&#29616;&#26377;GNN&#32858;&#21512;&#32534;&#31243;&#25277;&#35937;&#36827;&#34892;&#20998;&#31867;&#65292;&#24182;&#22312;&#26368;&#20808;&#36827;&#30340;GNN&#24211;&#19978;&#36827;&#34892;&#29305;&#24449;&#30740;&#31350;&#21644;&#24615;&#33021;&#27604;&#36739;&#65292;&#25552;&#20379;&#20102;&#26410;&#26469;GNN&#21152;&#36895;&#30340;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2310.12184</link><description>&lt;p&gt;
GNN&#32858;&#21512;&#32534;&#31243;&#25277;&#35937;&#30340;&#26550;&#26500;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Architectural Implications of GNN Aggregation Programming Abstractions. (arXiv:2310.12184v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12184
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;&#29616;&#26377;GNN&#32858;&#21512;&#32534;&#31243;&#25277;&#35937;&#36827;&#34892;&#20998;&#31867;&#65292;&#24182;&#22312;&#26368;&#20808;&#36827;&#30340;GNN&#24211;&#19978;&#36827;&#34892;&#29305;&#24449;&#30740;&#31350;&#21644;&#24615;&#33021;&#27604;&#36739;&#65292;&#25552;&#20379;&#20102;&#26410;&#26469;GNN&#21152;&#36895;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30001;&#20110;&#20174;&#22270;&#25968;&#25454;&#20013;&#25552;&#21462;&#26377;&#29992;&#34920;&#31034;&#30340;&#24378;&#22823;&#33021;&#21147;&#32780;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#12290;&#38543;&#30528;&#23545;&#39640;&#25928;GNN&#35745;&#31639;&#30340;&#38656;&#27714;&#22686;&#21152;&#65292;&#20026;&#20248;&#21270;GNN&#32858;&#21512;&#32780;&#35774;&#35745;&#30340;&#21508;&#31181;&#32534;&#31243;&#25277;&#35937;&#24212;&#36816;&#32780;&#29983;&#65292;&#20197;&#20419;&#36827;&#21152;&#36895;&#12290;&#28982;&#32780;&#65292;&#23545;&#29616;&#26377;&#25277;&#35937;&#27809;&#26377;&#20840;&#38754;&#30340;&#35780;&#20272;&#21644;&#20998;&#26512;&#65292;&#22240;&#27492;&#23545;&#21738;&#31181;&#26041;&#27861;&#26356;&#22909;&#27809;&#26377;&#26126;&#30830;&#30340;&#20849;&#35782;&#12290;&#22312;&#36825;&#23553;&#20449;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#25968;&#25454;&#32452;&#32455;&#21644;&#20256;&#25773;&#26041;&#27861;&#30340;&#32500;&#24230;&#23545;&#29616;&#26377;&#30340;GNN&#32858;&#21512;&#32534;&#31243;&#25277;&#35937;&#36827;&#34892;&#20998;&#31867;&#12290;&#36890;&#36807;&#22312;&#26368;&#20808;&#36827;&#30340;GNN&#24211;&#19978;&#26500;&#24314;&#36825;&#20123;&#25277;&#35937;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#24443;&#24213;&#21644;&#35814;&#32454;&#30340;&#29305;&#24449;&#30740;&#31350;&#65292;&#20197;&#27604;&#36739;&#23427;&#20204;&#30340;&#24615;&#33021;&#21644;&#25928;&#29575;&#65292;&#24182;&#26681;&#25454;&#25105;&#20204;&#30340;&#20998;&#26512;&#25552;&#20379;&#20102;&#19968;&#20123;&#20851;&#20110;&#26410;&#26469;GNN&#21152;&#36895;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks (GNNs) have gained significant popularity due to the powerful capability to extract useful representations from graph data. As the need for efficient GNN computation intensifies, a variety of programming abstractions designed for optimizing GNN Aggregation have emerged to facilitate acceleration. However, there is no comprehensive evaluation and analysis upon existing abstractions, thus no clear consensus on which approach is better. In this letter, we classify existing programming abstractions for GNN Aggregation by the dimension of data organization and propagation method. By constructing these abstractions on a state-of-the-art GNN library, we perform a thorough and detailed characterization study to compare their performance and efficiency, and provide several insights on future GNN acceleration based on our analysis.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#32852;&#37030;&#23398;&#20064;&#20013;&#23545;&#25239;&#24615;&#35757;&#32451;&#21644;&#21518;&#38376;&#25915;&#20987;&#30340;&#20132;&#21449;&#28857;&#65292;&#24341;&#20837;&#20102;Adversarial Robustness Unhardening&#65288;ARU&#65289;&#65292;&#36890;&#36807;&#26377;&#24847;&#20171;&#20837;&#20998;&#25955;&#24335;&#35757;&#32451;&#36807;&#31243;&#20013;&#30772;&#22351;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#20351;&#27169;&#22411;&#26356;&#23481;&#26131;&#21463;&#21040;&#26356;&#24191;&#27867;&#30340;&#36867;&#36991;&#25915;&#20987;&#12290;</title><link>http://arxiv.org/abs/2310.11594</link><description>&lt;p&gt;
Adversarial Robustness Unhardening via Backdoor Attacks in Federated Learning. (arXiv:2310.11594v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
Adversarial Robustness Unhardening via Backdoor Attacks in Federated Learning. (arXiv:2310.11594v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11594
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#32852;&#37030;&#23398;&#20064;&#20013;&#23545;&#25239;&#24615;&#35757;&#32451;&#21644;&#21518;&#38376;&#25915;&#20987;&#30340;&#20132;&#21449;&#28857;&#65292;&#24341;&#20837;&#20102;Adversarial Robustness Unhardening&#65288;ARU&#65289;&#65292;&#36890;&#36807;&#26377;&#24847;&#20171;&#20837;&#20998;&#25955;&#24335;&#35757;&#32451;&#36807;&#31243;&#20013;&#30772;&#22351;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#20351;&#27169;&#22411;&#26356;&#23481;&#26131;&#21463;&#21040;&#26356;&#24191;&#27867;&#30340;&#36867;&#36991;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24403;&#20170;&#30340;&#25968;&#25454;&#39537;&#21160;&#29615;&#22659;&#20013;&#65292;&#32500;&#25252;&#29992;&#25143;&#38544;&#31169;&#21644;&#37322;&#25918;&#25968;&#25454;&#28508;&#21147;&#20043;&#38388;&#24494;&#22937;&#30340;&#24179;&#34913;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#20851;&#27880;&#28857;&#12290;&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#20197;&#38544;&#31169;&#20026;&#20013;&#24515;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#23427;&#23454;&#29616;&#20102;&#21327;&#20316;&#27169;&#22411;&#35757;&#32451;&#32780;&#26080;&#38656;&#20849;&#20139;&#25968;&#25454;&#12290;&#36825;&#31181;&#20998;&#25955;&#24335;&#26041;&#27861;&#24102;&#26469;&#20102;&#23433;&#20840;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#24694;&#24847;&#23454;&#20307;&#27880;&#20837;&#25439;&#22351;&#25968;&#25454;&#30340;&#20013;&#27602;&#21644;&#21518;&#38376;&#25915;&#20987;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#26368;&#21021;&#21463;&#21040;&#27979;&#35797;&#26102;&#38388;&#36867;&#36991;&#25915;&#20987;&#30340;&#21551;&#21457;&#65292;&#25506;&#35752;&#20102;&#32852;&#37030;&#23398;&#20064;&#20013;&#23545;&#25239;&#24615;&#35757;&#32451;&#21644;&#21518;&#38376;&#25915;&#20987;&#30340;&#20132;&#21449;&#28857;&#65292;&#24341;&#20837;&#20102;Adversarial Robustness Unhardening&#65288;ARU&#65289;&#12290;ARU&#34987;&#19968;&#37096;&#20998;&#23545;&#25163;&#20351;&#29992;&#65292;&#20197;&#26377;&#24847;&#20171;&#20837;&#20998;&#25955;&#24335;&#35757;&#32451;&#36807;&#31243;&#20013;&#30772;&#22351;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#20351;&#27169;&#22411;&#26356;&#23481;&#26131;&#21463;&#21040;&#26356;&#24191;&#27867;&#30340;&#36867;&#36991;&#25915;&#20987;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#35777;&#23454;&#39564;&#65292;&#35780;&#20272;&#20102;ARU&#23545;&#23545;&#25239;&#24615;&#35757;&#32451;&#21644;&#29616;&#26377;&#30340;&#40065;&#26834;&#32858;&#21512;&#38450;&#24481;&#31574;&#30053;&#23545;&#20013;&#27602;&#21644;&#21518;&#38376;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
In today's data-driven landscape, the delicate equilibrium between safeguarding user privacy and unleashing data potential stands as a paramount concern. Federated learning, which enables collaborative model training without necessitating data sharing, has emerged as a privacy-centric solution. This decentralized approach brings forth security challenges, notably poisoning and backdoor attacks where malicious entities inject corrupted data. Our research, initially spurred by test-time evasion attacks, investigates the intersection of adversarial training and backdoor attacks within federated learning, introducing Adversarial Robustness Unhardening (ARU). ARU is employed by a subset of adversaries to intentionally undermine model robustness during decentralized training, rendering models susceptible to a broader range of evasion attacks. We present extensive empirical experiments evaluating ARU's impact on adversarial training and existing robust aggregation defenses against poisoning a
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22312;&#29305;&#26435;&#21319;&#32423;&#22330;&#26223;&#20013;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#28183;&#36879;&#27979;&#35797;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#21019;&#24314;&#19968;&#20010;&#33258;&#21160;&#21270;&#30340;Linux&#29305;&#26435;&#21319;&#32423;&#22522;&#20934;&#21644;&#19968;&#20010;LLM-guided&#29305;&#26435;&#21319;&#32423;&#24037;&#20855;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;LLMs&#30340;&#19981;&#21516;&#25552;&#31034;&#35774;&#35745;&#12289;&#19978;&#19979;&#25991;&#23398;&#20064;&#21644;&#39640;&#32423;&#25351;&#23548;&#23545;&#27979;&#35797;&#30340;&#24433;&#21709;&#65292;&#24182;&#35752;&#35770;&#20102;LLMs&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2310.11409</link><description>&lt;p&gt;
&#35780;&#20272;LLMs&#22312;&#29305;&#26435;&#21319;&#32423;&#22330;&#26223;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Evaluating LLMs for Privilege-Escalation Scenarios. (arXiv:2310.11409v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11409
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22312;&#29305;&#26435;&#21319;&#32423;&#22330;&#26223;&#20013;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#28183;&#36879;&#27979;&#35797;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#21019;&#24314;&#19968;&#20010;&#33258;&#21160;&#21270;&#30340;Linux&#29305;&#26435;&#21319;&#32423;&#22522;&#20934;&#21644;&#19968;&#20010;LLM-guided&#29305;&#26435;&#21319;&#32423;&#24037;&#20855;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;LLMs&#30340;&#19981;&#21516;&#25552;&#31034;&#35774;&#35745;&#12289;&#19978;&#19979;&#25991;&#23398;&#20064;&#21644;&#39640;&#32423;&#25351;&#23548;&#23545;&#27979;&#35797;&#30340;&#24433;&#21709;&#65292;&#24182;&#35752;&#35770;&#20102;LLMs&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28183;&#36879;&#27979;&#35797;&#26159;&#32593;&#32476;&#23433;&#20840;&#30340;&#19968;&#20010;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#23427;&#20801;&#35768;&#32452;&#32455;&#20027;&#21160;&#35782;&#21035;&#21644;&#20462;&#22797;&#31995;&#32479;&#20013;&#30340;&#28431;&#27934;&#65292;&#20174;&#32780;&#22686;&#24378;&#20854;&#23545;&#28508;&#22312;&#32593;&#32476;&#25915;&#20987;&#30340;&#38450;&#24481;&#26426;&#21046;&#12290;&#22312;&#28183;&#36879;&#27979;&#35797;&#39046;&#22495;&#65292;&#26368;&#36817;&#30340;&#19968;&#20010;&#36827;&#23637;&#26159;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#12290;&#25105;&#20204;&#25506;&#32034;LLMs&#19982;&#28183;&#36879;&#27979;&#35797;&#30340;&#20132;&#21449;&#39046;&#22495;&#65292;&#20197;&#20102;&#35299;&#23427;&#20204;&#22312;&#29305;&#26435;&#21319;&#32423;&#22330;&#26223;&#20013;&#30340;&#33021;&#21147;&#21644;&#25361;&#25112;&#12290;&#25105;&#20204;&#20351;&#29992;&#26412;&#22320;&#34394;&#25311;&#26426;&#21019;&#24314;&#20102;&#19968;&#20010;&#33258;&#21160;&#21270;&#30340;Linux&#29305;&#26435;&#21319;&#32423;&#22522;&#20934;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;LLMs&#30340;&#29305;&#26435;&#21319;&#32423;&#24037;&#20855;&#65292;&#29992;&#20110;&#35780;&#20272;&#19981;&#21516;&#30340;LLMs&#21644;&#25552;&#31034;&#31574;&#30053;&#22312;&#25105;&#20204;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#19981;&#21516;&#25552;&#31034;&#35774;&#35745;&#30340;&#24433;&#21709;&#65292;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#22909;&#22788;&#65292;&#20197;&#21450;&#21521;LLMs&#25552;&#20379;&#39640;&#32423;&#25351;&#23548;&#30340;&#20248;&#21183;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;LLMs&#38754;&#20020;&#30340;&#25361;&#25112;&#39046;&#22495;&#65292;&#21253;&#25324;&#22312;&#27979;&#35797;&#36807;&#31243;&#20013;&#20445;&#25345;&#19987;&#27880;&#12289;&#22788;&#29702;&#38169;&#35823;&#20197;&#21450;&#19982;&#20256;&#32479;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Penetration testing, an essential component of cybersecurity, allows organizations to proactively identify and remediate vulnerabilities in their systems, thus bolstering their defense mechanisms against potential cyberattacks. One recent advancement in the realm of penetration testing is the utilization of Language Models (LLMs). We explore the intersection of LLMs and penetration testing to gain insight into their capabilities and challenges in the context of privilige escalation. We create an automated Linux privilege-escalation benchmark utilizing local virtual machines. We introduce an LLM-guided privilege-escalation tool designed for evaluating different LLMs and prompt strategies against our benchmark. We analyze the impact of different prompt designs, the benefits of in-context learning, and the advantages of offering high-level guidance to LLMs. We discuss challenging areas for LLMs, including maintaining focus during testing, coping with errors, and finally comparing them wit
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;BiomedJourney&#65292;&#36890;&#36807;&#25351;&#23548;&#23398;&#20064;&#22810;&#27169;&#24577;&#24739;&#32773;&#26053;&#31243;&#65292;&#36827;&#34892;&#21453;&#20107;&#23454;&#29983;&#29289;&#21307;&#23398;&#22270;&#20687;&#29983;&#25104;&#12290;&#20351;&#29992;GPT-4&#22788;&#29702;&#22270;&#20687;&#25253;&#21578;&#29983;&#25104;&#30142;&#30149;&#36827;&#23637;&#30340;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#65292;&#24182;&#35757;&#32451;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2310.10765</link><description>&lt;p&gt;
BiomedJourney: &#25351;&#23548;&#23398;&#20064;&#22810;&#27169;&#24577;&#24739;&#32773;&#26053;&#31243;&#20013;&#30340;&#21453;&#20107;&#23454;&#29983;&#29289;&#21307;&#23398;&#22270;&#20687;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
BiomedJourney: Counterfactual Biomedical Image Generation by Instruction-Learning from Multimodal Patient Journeys. (arXiv:2310.10765v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10765
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;BiomedJourney&#65292;&#36890;&#36807;&#25351;&#23548;&#23398;&#20064;&#22810;&#27169;&#24577;&#24739;&#32773;&#26053;&#31243;&#65292;&#36827;&#34892;&#21453;&#20107;&#23454;&#29983;&#29289;&#21307;&#23398;&#22270;&#20687;&#29983;&#25104;&#12290;&#20351;&#29992;GPT-4&#22788;&#29702;&#22270;&#20687;&#25253;&#21578;&#29983;&#25104;&#30142;&#30149;&#36827;&#23637;&#30340;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#65292;&#24182;&#35757;&#32451;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#22270;&#20687;&#32534;&#36753;&#30340;&#25351;&#23548;&#23398;&#20064;&#21462;&#24471;&#20102;&#24555;&#36895;&#36827;&#23637;&#65292;&#22914;InstructPix2Pix&#65292;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#21487;&#20197;&#23558;&#36825;&#20123;&#26041;&#27861;&#24212;&#29992;&#20110;&#21453;&#20107;&#23454;&#22270;&#20687;&#29983;&#25104;&#65292;&#20174;&#32780;&#24110;&#21161;&#21306;&#20998;&#22240;&#26524;&#32467;&#26500;&#21644;&#20266;&#30456;&#20851;&#65292;&#24182;&#20419;&#36827;&#30142;&#30149;&#36827;&#23637;&#24314;&#27169;&#30340;&#31283;&#20581;&#22270;&#20687;&#35299;&#37322;&#12290;&#28982;&#32780;&#65292;&#36890;&#29992;&#30340;&#22270;&#20687;&#32534;&#36753;&#27169;&#22411;&#24182;&#19981;&#36866;&#29992;&#20110;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#65292;&#21453;&#20107;&#23454;&#29983;&#29289;&#21307;&#23398;&#22270;&#20687;&#29983;&#25104;&#30340;&#30740;&#31350;&#36824;&#36828;&#26410;&#28145;&#20837;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;BiomedJourney&#65292;&#36890;&#36807;&#25351;&#23548;&#23398;&#20064;&#22810;&#27169;&#24577;&#24739;&#32773;&#26053;&#31243;&#65292;&#36827;&#34892;&#21453;&#20107;&#23454;&#29983;&#29289;&#21307;&#23398;&#22270;&#20687;&#29983;&#25104;&#12290;&#32473;&#23450;&#19968;&#20010;&#25293;&#25668;&#20110;&#19981;&#21516;&#26102;&#38388;&#28857;&#30340;&#20004;&#20010;&#29983;&#29289;&#21307;&#23398;&#22270;&#20687;&#30340;&#24739;&#32773;&#65292;&#25105;&#20204;&#20351;&#29992;GPT-4&#22788;&#29702;&#30456;&#24212;&#30340;&#22270;&#20687;&#25253;&#21578;&#65292;&#24182;&#29983;&#25104;&#30142;&#30149;&#36827;&#23637;&#30340;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#12290;&#28982;&#21518;&#65292;&#20351;&#29992;&#29983;&#25104;&#30340;&#19977;&#20803;&#32452;&#65288;&#20808;&#21069;&#22270;&#20687;&#12289;&#36827;&#23637;&#25551;&#36848;&#12289;&#26032;&#22270;&#20687;&#65289;&#26469;&#35757;&#32451;&#19968;&#20010;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Rapid progress has been made in instruction-learning for image editing with natural-language instruction, as exemplified by InstructPix2Pix. In biomedicine, such methods can be applied to counterfactual image generation, which helps differentiate causal structure from spurious correlation and facilitate robust image interpretation for disease progression modeling. However, generic image-editing models are ill-suited for the biomedical domain, and counterfactual biomedical image generation is largely underexplored. In this paper, we present BiomedJourney, a novel method for counterfactual biomedical image generation by instruction-learning from multimodal patient journeys. Given a patient with two biomedical images taken at different time points, we use GPT-4 to process the corresponding imaging reports and generate a natural language description of disease progression. The resulting triples (prior image, progression description, new image) are then used to train a latent diffusion mode
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#22312;&#22810;&#26234;&#33021;&#20307;&#21512;&#20316;&#28216;&#25103;&#20013;&#35780;&#20272;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26234;&#33021;&#20307;&#65292;&#21457;&#29616;&#23427;&#20204;&#21487;&#20197;&#34920;&#29616;&#20986;&#21327;&#20316;&#34892;&#20026;&#21644;&#39640;&#32423;&#29702;&#35770;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#26126;&#30830;&#30340;&#20449;&#24565;&#29366;&#24577;&#34920;&#31034;&#26469;&#25552;&#39640;&#20219;&#21153;&#24615;&#33021;&#21644;&#29702;&#35770;&#25512;&#29702;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.10701</link><description>&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#21327;&#20316;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
Theory of Mind for Multi-Agent Collaboration via Large Language Models. (arXiv:2310.10701v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10701
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#22312;&#22810;&#26234;&#33021;&#20307;&#21512;&#20316;&#28216;&#25103;&#20013;&#35780;&#20272;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26234;&#33021;&#20307;&#65292;&#21457;&#29616;&#23427;&#20204;&#21487;&#20197;&#34920;&#29616;&#20986;&#21327;&#20316;&#34892;&#20026;&#21644;&#39640;&#32423;&#29702;&#35770;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#26126;&#30830;&#30340;&#20449;&#24565;&#29366;&#24577;&#34920;&#31034;&#26469;&#25552;&#39640;&#20219;&#21153;&#24615;&#33021;&#21644;&#29702;&#35770;&#25512;&#29702;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#29702;&#21644;&#35268;&#21010;&#26041;&#38754;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#25104;&#23601;&#65292;&#20294;&#23427;&#22312;&#22810;&#26234;&#33021;&#20307;&#21327;&#20316;&#26041;&#38754;&#30340;&#33021;&#21147;&#23578;&#26410;&#24471;&#21040;&#28145;&#20837;&#25506;&#32034;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#27604;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#21644;&#22522;&#20110;&#35268;&#21010;&#30340;&#22522;&#20934;&#26041;&#27861;&#65292;&#22312;&#22810;&#26234;&#33021;&#20307;&#21512;&#20316;&#25991;&#26412;&#28216;&#25103;&#20013;&#35780;&#20272;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26234;&#33021;&#20307;&#22312;&#29702;&#35770;&#25512;&#29702;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26234;&#33021;&#20307;&#20986;&#29616;&#20102;&#21327;&#20316;&#34892;&#20026;&#21644;&#39640;&#32423;&#29702;&#35770;&#25512;&#29702;&#33021;&#21147;&#30340;&#35777;&#25454;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#25581;&#31034;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26234;&#33021;&#20307;&#22312;&#38271;&#26399;&#35268;&#21010;&#19978;&#23384;&#22312;&#20248;&#21270;&#30340;&#23616;&#38480;&#24615;&#65292;&#20197;&#21450;&#23545;&#20219;&#21153;&#29366;&#24577;&#30340;&#38169;&#35823;&#35748;&#30693;&#12290;&#25105;&#20204;&#23581;&#35797;&#20351;&#29992;&#26126;&#30830;&#30340;&#20449;&#24565;&#29366;&#24577;&#34920;&#31034;&#26469;&#32531;&#35299;&#36825;&#20123;&#38382;&#39064;&#65292;&#24182;&#21457;&#29616;&#23427;&#21487;&#20197;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26234;&#33021;&#20307;&#30340;&#20219;&#21153;&#24615;&#33021;&#21644;&#29702;&#35770;&#25512;&#29702;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
While Large Language Models (LLMs) have demonstrated impressive accomplishments in both reasoning and planning, their abilities in multi-agent collaborations remains largely unexplored. This study evaluates LLM-based agents in a multi-agent cooperative text game with Theory of Mind (ToM) inference tasks, comparing their performance with Multi-Agent Reinforcement Learning (MARL) and planning-based baselines. We observed evidence of emergent collaborative behaviors and high-order Theory of Mind capabilities among LLM-based agents. Our results reveal limitations in LLM-based agents' planning optimization due to systematic failures in managing long-horizon contexts and hallucination about the task state. We explore the use of explicit belief state representations to mitigate these issues, finding that it enhances task performance and the accuracy of ToM inferences for LLM-based agents.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36229;&#36234;&#25991;&#26723;&#36793;&#30028;&#30340;&#19978;&#19979;&#25991;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#30456;&#20851;&#25991;&#26723;&#24207;&#21015;&#19978;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#40723;&#21169;&#27169;&#22411;&#36827;&#34892;&#36328;&#25991;&#26723;&#30340;&#38405;&#35835;&#21644;&#25512;&#29702;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#25913;&#21464;&#25991;&#26723;&#39034;&#24207;&#24182;&#24212;&#29992;&#29616;&#26377;&#30340;&#39044;&#35757;&#32451;&#31649;&#36947;&#26469;&#23454;&#29616;&#12290;</title><link>http://arxiv.org/abs/2310.10638</link><description>&lt;p&gt;
&#36229;&#36234;&#25991;&#26723;&#36793;&#30028;&#30340;&#19978;&#19979;&#25991;&#39044;&#35757;&#32451;&#65306;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
In-Context Pretraining: Language Modeling Beyond Document Boundaries. (arXiv:2310.10638v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10638
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36229;&#36234;&#25991;&#26723;&#36793;&#30028;&#30340;&#19978;&#19979;&#25991;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#30456;&#20851;&#25991;&#26723;&#24207;&#21015;&#19978;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#40723;&#21169;&#27169;&#22411;&#36827;&#34892;&#36328;&#25991;&#26723;&#30340;&#38405;&#35835;&#21644;&#25512;&#29702;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#25913;&#21464;&#25991;&#26723;&#39034;&#24207;&#24182;&#24212;&#29992;&#29616;&#26377;&#30340;&#39044;&#35757;&#32451;&#31649;&#36947;&#26469;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#36890;&#36807;&#39044;&#27979;&#32473;&#23450;&#25991;&#26723;&#21069;&#32512;&#30340;&#26631;&#35760;&#26469;&#36827;&#34892;&#35757;&#32451;&#65292;&#20174;&#32780;&#33021;&#22815;&#30452;&#25509;&#36827;&#34892;&#38271;&#31687;&#29983;&#25104;&#21644;&#25552;&#31034;&#24335;&#20219;&#21153;&#65292;&#36825;&#21487;&#20197;&#31616;&#21270;&#20026;&#25991;&#26723;&#23436;&#25104;&#12290;&#29616;&#26377;&#30340;&#39044;&#35757;&#32451;&#31649;&#36947;&#36890;&#36807;&#36830;&#25509;&#38543;&#26426;&#32452;&#21512;&#30340;&#30701;&#25991;&#26723;&#26469;&#35757;&#32451;LMs&#65292;&#20197;&#21019;&#24314;&#36755;&#20837;&#19978;&#19979;&#25991;&#65292;&#20294;&#21069;&#19968;&#20010;&#25991;&#26723;&#23545;&#20110;&#39044;&#27979;&#19979;&#19968;&#20010;&#25991;&#26723;&#27809;&#26377;&#25552;&#20379;&#20219;&#20309;&#20449;&#21495;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#8212;&#8212;&#19978;&#19979;&#25991;&#39044;&#35757;&#32451;&#65292;&#21363;&#22312;&#30456;&#20851;&#25991;&#26723;&#24207;&#21015;&#19978;&#39044;&#20808;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#20174;&#32780;&#26126;&#30830;&#40723;&#21169;&#23427;&#20204;&#36328;&#36234;&#25991;&#26723;&#36793;&#30028;&#36827;&#34892;&#38405;&#35835;&#21644;&#25512;&#29702;&#12290;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#25913;&#21464;&#25991;&#26723;&#39034;&#24207;&#65292;&#20351;&#27599;&#20010;&#19978;&#19979;&#25991;&#21253;&#21547;&#30456;&#20851;&#30340;&#25991;&#26723;&#65292;&#24182;&#30452;&#25509;&#24212;&#29992;&#29616;&#26377;&#30340;&#39044;&#35757;&#32451;&#31649;&#36947;&#26469;&#36827;&#34892;&#19978;&#19979;&#25991;&#39044;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#36825;&#20010;&#25991;&#26723;&#25490;&#24207;&#38382;&#39064;&#24456;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26377;&#25968;&#21313;&#20159;&#20010;&#25991;&#26723;&#65292;&#25105;&#20204;&#24076;&#26395;&#22312;&#27599;&#20010;&#25991;&#26723;&#20013;&#26368;&#22823;&#21270;&#19978;&#19979;&#25991;&#30456;&#20284;&#24615;&#32780;&#19981;&#37325;&#22797;&#20219;&#20309;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LMs) are currently trained to predict tokens given document prefixes, enabling them to directly perform long-form generation and prompting-style tasks which can be reduced to document completion. Existing pretraining pipelines train LMs by concatenating random sets of short documents to create input contexts but the prior documents provide no signal for predicting the next document. We instead present In-Context Pretraining, a new approach where language models are pretrained on a sequence of related documents, thereby explicitly encouraging them to read and reason across document boundaries. We can do In-Context Pretraining by simply changing the document ordering so that each context contains related documents, and directly applying existing pretraining pipelines. However, this document sorting problem is challenging. There are billions of documents and we would like the sort to maximize contextual similarity for every document without repeating any data. To do
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#20107;&#23454;&#30693;&#35782;&#30340;&#36328;&#35821;&#35328;&#19968;&#33268;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#20998;&#26512;&#27169;&#22411;&#22823;&#23567;&#12289;&#35821;&#35328;&#37197;&#23545;&#31561;&#22240;&#32032;&#21457;&#29616;&#20102;&#24433;&#21709;&#19968;&#33268;&#24615;&#30340;&#22240;&#32032;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22686;&#21152;&#27169;&#22411;&#22823;&#23567;&#21487;&#20197;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#20294;&#19981;&#20250;&#25913;&#21892;&#36328;&#35821;&#35328;&#19968;&#33268;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.10378</link><description>&lt;p&gt;
&#36328;&#35821;&#35328;&#22810;&#35821;&#35328;&#27169;&#22411;&#20013;&#20107;&#23454;&#30693;&#35782;&#30340;&#36328;&#35821;&#35328;&#19968;&#33268;&#24615;
&lt;/p&gt;
&lt;p&gt;
Cross-Lingual Consistency of Factual Knowledge in Multilingual Language Models. (arXiv:2310.10378v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10378
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#20107;&#23454;&#30693;&#35782;&#30340;&#36328;&#35821;&#35328;&#19968;&#33268;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#20998;&#26512;&#27169;&#22411;&#22823;&#23567;&#12289;&#35821;&#35328;&#37197;&#23545;&#31561;&#22240;&#32032;&#21457;&#29616;&#20102;&#24433;&#21709;&#19968;&#33268;&#24615;&#30340;&#22240;&#32032;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22686;&#21152;&#27169;&#22411;&#22823;&#23567;&#21487;&#20197;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#20294;&#19981;&#20250;&#25913;&#21892;&#36328;&#35821;&#35328;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#35821;&#35328;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#26174;&#31034;&#23384;&#20648;&#20102;&#22823;&#37327;&#30340;&#20107;&#23454;&#30693;&#35782;&#65292;&#20294;&#22312;&#19981;&#21516;&#35821;&#35328;&#20043;&#38388;&#23384;&#22312;&#36739;&#22823;&#30340;&#21464;&#21270;&#12290;&#20026;&#20102;&#30830;&#20445;&#19981;&#21516;&#35821;&#35328;&#32972;&#26223;&#30340;&#29992;&#25143;&#20174;&#21516;&#19968;&#20010;&#27169;&#22411;&#20013;&#33719;&#24471;&#19968;&#33268;&#30340;&#21453;&#39304;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#21508;&#31181;&#22810;&#35821;&#35328;PLM&#20013;&#20107;&#23454;&#30693;&#35782;&#30340;&#36328;&#35821;&#35328;&#19968;&#33268;&#24615;&#65288;CLC&#65289;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25490;&#24207;&#30340;&#19968;&#33268;&#24615;&#65288;RankC&#65289;&#24230;&#37327;&#65292;&#29992;&#20110;&#29420;&#31435;&#20110;&#20934;&#30830;&#24615;&#35780;&#20272;&#36328;&#35821;&#35328;&#38388;&#30340;&#30693;&#35782;&#19968;&#33268;&#24615;&#12290;&#21033;&#29992;&#36825;&#20010;&#24230;&#37327;&#26041;&#27861;&#65292;&#25105;&#20204;&#23545;&#20915;&#23450;CLC&#30340;&#22240;&#32032;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#65292;&#21253;&#25324;&#27169;&#22411;&#23618;&#38754;&#21644;&#35821;&#35328;&#23545;&#23618;&#38754;&#12290;&#22312;&#20854;&#20182;&#32467;&#26524;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#22686;&#21152;&#27169;&#22411;&#22823;&#23567;&#21487;&#20197;&#25552;&#39640;&#22823;&#22810;&#25968;&#35821;&#35328;&#20013;&#30340;&#20107;&#23454;&#25506;&#27979;&#20934;&#30830;&#24615;&#65292;&#20294;&#19981;&#33021;&#25913;&#21892;&#36328;&#35821;&#35328;&#19968;&#33268;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#27169;&#22411;&#32534;&#36753;&#22312;PLMs&#20013;&#25554;&#20837;&#26032;&#30340;&#20107;&#23454;&#20851;&#32852;&#36827;&#34892;&#20102;&#19968;&#20010;CLC&#30340;&#26696;&#20363;&#30740;&#31350;&#12290;&#23545;&#19968;&#23567;&#37096;&#20998;&#20107;&#23454;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multilingual large-scale Pretrained Language Models (PLMs) have been shown to store considerable amounts of factual knowledge, but large variations are observed across languages. With the ultimate goal of ensuring that users with different language backgrounds obtain consistent feedback from the same model, we study the cross-lingual consistency (CLC) of factual knowledge in various multilingual PLMs. To this end, we propose a Ranking-based Consistency (RankC) metric to evaluate knowledge consistency across languages independently from accuracy. Using this metric, we conduct an in-depth analysis of the determining factors for CLC, both at model level and at language-pair level. Among other results, we find that increasing model size leads to higher factual probing accuracy in most languages, but does not improve cross-lingual consistency. Finally, we conduct a case study on CLC when new factual associations are inserted in the PLMs via model editing. Results on a small sample of facts 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#35821;&#20041;&#20016;&#23500;&#30340;&#25340;&#25509;&#25968;&#25454;&#38598;GreatSplicing&#65292;&#36890;&#36807;&#21253;&#25324;&#22823;&#37327;&#19981;&#21516;&#35821;&#20041;&#31867;&#21035;&#30340;&#25340;&#25509;&#21306;&#22495;&#65292;&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;&#25340;&#25509;&#30165;&#36857;&#26816;&#27979;&#19978;&#34920;&#29616;&#20986;&#36739;&#20302;&#30340;&#35823;&#35782;&#29575;&#21644;&#26356;&#22909;&#30340;&#36328;&#25968;&#25454;&#38598;&#26816;&#27979;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.10070</link><description>&lt;p&gt;
GreatSplicing: &#19968;&#20010;&#35821;&#20041;&#20016;&#23500;&#30340;&#25340;&#25509;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
GreatSplicing: A Semantically Rich Splicing Dataset. (arXiv:2310.10070v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10070
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#35821;&#20041;&#20016;&#23500;&#30340;&#25340;&#25509;&#25968;&#25454;&#38598;GreatSplicing&#65292;&#36890;&#36807;&#21253;&#25324;&#22823;&#37327;&#19981;&#21516;&#35821;&#20041;&#31867;&#21035;&#30340;&#25340;&#25509;&#21306;&#22495;&#65292;&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;&#25340;&#25509;&#30165;&#36857;&#26816;&#27979;&#19978;&#34920;&#29616;&#20986;&#36739;&#20302;&#30340;&#35823;&#35782;&#29575;&#21644;&#26356;&#22909;&#30340;&#36328;&#25968;&#25454;&#38598;&#26816;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#26377;&#30340;&#25340;&#25509;&#20266;&#36896;&#25968;&#25454;&#38598;&#20013;&#65292;&#25340;&#25509;&#21306;&#22495;&#30340;&#35821;&#20041;&#21464;&#21270;&#19981;&#36275;&#23548;&#33268;&#35757;&#32451;&#30340;&#26816;&#27979;&#27169;&#22411;&#23545;&#35821;&#20041;&#29305;&#24449;&#30340;&#36807;&#25311;&#21512;&#12290;&#21516;&#26102;&#65292;&#30001;&#20110;&#32570;&#20047;&#21512;&#29702;&#30340;&#25968;&#25454;&#38598;&#65292;&#19981;&#21516;&#30340;&#26816;&#27979;&#26041;&#27861;&#22312;&#23454;&#39564;&#35774;&#32622;&#19978;&#26080;&#27861;&#36798;&#25104;&#19968;&#33268;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#32039;&#36843;&#30340;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;GreatSplicing&#65292;&#19968;&#20010;&#25163;&#21160;&#21019;&#24314;&#30340;&#20855;&#26377;&#22823;&#37327;&#21644;&#39640;&#36136;&#37327;&#30340;&#25340;&#25509;&#25968;&#25454;&#38598;&#12290;GreatSplicing&#21253;&#25324;5000&#24352;&#25340;&#25509;&#22270;&#20687;&#65292;&#24182;&#28085;&#30422;&#20102;335&#20010;&#19981;&#21516;&#30340;&#35821;&#20041;&#31867;&#21035;&#30340;&#25340;&#25509;&#21306;&#22495;&#65292;&#35753;&#31070;&#32463;&#32593;&#32476;&#26356;&#22909;&#22320;&#25235;&#20303;&#25340;&#25509;&#30165;&#36857;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#20351;&#29992;GreatSplicing&#35757;&#32451;&#30340;&#27169;&#22411;&#30456;&#36739;&#20110;&#29616;&#26377;&#25968;&#25454;&#38598;&#34920;&#29616;&#20986;&#36739;&#20302;&#30340;&#35823;&#35782;&#29575;&#21644;&#26356;&#22909;&#30340;&#36328;&#25968;&#25454;&#38598;&#26816;&#27979;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;GreatSplicing&#21487;&#20379;&#25152;&#26377;&#30740;&#31350;&#30446;&#30340;&#20351;&#29992;&#65292;&#24182;&#21487;&#20174;www.greatsplicing.net&#19979;&#36733;&#12290;
&lt;/p&gt;
&lt;p&gt;
In existing splicing forgery datasets, the insufficient semantic varieties of spliced regions cause a problem that trained detection models overfit semantic features rather than splicing traces. Meanwhile, because of the absence of a reasonable dataset, different detection methods proposed cannot reach a consensus on experimental settings. To address these urgent issues, GreatSplicing, a manually created splicing dataset with a considerable amount and high quality, is proposed in this paper. GreatSplicing comprises 5,000 spliced images and covers spliced regions with 335 distinct semantic categories, allowing neural networks to grasp splicing traces better. Extensive experiments demonstrate that models trained on GreatSplicing exhibit minimal misidentification rates and superior cross-dataset detection capabilities compared to existing datasets. Furthermore, GreatSplicing is available for all research purposes and can be downloaded from www.greatsplicing.net.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36845;&#20195;&#31034;&#33539;&#36873;&#25321;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#38646;&#26679;&#26412;&#38142;&#24335;&#24605;&#32500;&#25512;&#29702;&#26469;&#36873;&#25321;&#19982;&#27979;&#35797;&#26679;&#26412;&#19981;&#21516;&#20294;&#20173;&#19982;&#20043;&#24378;&#30456;&#20851;&#30340;&#31034;&#33539;&#20316;&#20026;&#23398;&#20064;&#30340;&#19978;&#19979;&#25991;&#12290;</title><link>http://arxiv.org/abs/2310.09881</link><description>&lt;p&gt;
&#22522;&#20110;&#36845;&#20195;&#31034;&#33539;&#36873;&#25321;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
In-Context Learning with Iterative Demonstration Selection. (arXiv:2310.09881v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09881
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36845;&#20195;&#31034;&#33539;&#36873;&#25321;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#38646;&#26679;&#26412;&#38142;&#24335;&#24605;&#32500;&#25512;&#29702;&#26469;&#36873;&#25321;&#19982;&#27979;&#35797;&#26679;&#26412;&#19981;&#21516;&#20294;&#20173;&#19982;&#20043;&#24378;&#30456;&#20851;&#30340;&#31034;&#33539;&#20316;&#20026;&#23398;&#20064;&#30340;&#19978;&#19979;&#25991;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#21040;&#35268;&#27169;&#30340;&#36827;&#23637;&#30340;&#25512;&#21160;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;(ICL)&#23637;&#31034;&#20102;&#24378;&#22823;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;ICL&#30340;&#24615;&#33021;&#24050;&#34987;&#35777;&#26126;&#23545;&#20110;&#31034;&#33539;&#30340;&#36873;&#25321;&#38750;&#24120;&#25935;&#24863;&#12290;&#36873;&#25321;&#26368;&#21512;&#36866;&#30340;&#31034;&#33539;&#20316;&#20026;&#19978;&#19979;&#25991;&#20173;&#28982;&#26159;&#19968;&#20010;&#25345;&#32493;&#25361;&#25112;&#21644;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#29616;&#26377;&#25991;&#29486;&#24050;&#32463;&#24378;&#35843;&#20102;&#36873;&#25321;&#37027;&#20123;&#19982;&#27979;&#35797;&#26679;&#26412;&#19981;&#21516;&#25110;&#35821;&#20041;&#30456;&#20284;&#24615;&#30340;&#31034;&#33539;&#30340;&#37325;&#35201;&#24615;&#65292;&#32780;&#24573;&#35270;&#20102;&#26368;&#20248;&#31034;&#33539;&#36873;&#25321;&#32500;&#24230;&#26159;&#20219;&#21153;&#29305;&#23450;&#30340;&#20107;&#23454;&#12290;&#20511;&#37492;&#20004;&#20010;&#32500;&#24230;&#30340;&#20248;&#28857;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36845;&#20195;&#31034;&#33539;&#36873;&#25321;(IDS)&#12290;&#20351;&#29992;&#38646;&#26679;&#26412;&#38142;&#24335;&#24605;&#32500;&#25512;&#29702;(Zero-shot-CoT)&#65292;IDS&#36845;&#20195;&#22320;&#36873;&#25321;&#37027;&#20123;&#19982;&#27979;&#35797;&#26679;&#26412;&#19981;&#21516;&#20294;&#20173;&#19982;&#20043;&#24378;&#30456;&#20851;&#30340;&#31034;&#33539;&#20316;&#20026;ICL&#30340;&#31034;&#33539;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;IDS&#22312;&#31034;&#33539;&#36873;&#25321;&#20043;&#21069;&#23558;Zero-shot-CoT&#24212;&#29992;&#20110;&#27979;&#35797;&#26679;&#26412;&#12290;&#36755;&#20986;&#30340;&#25512;&#29702;&#36335;&#24452;&#26159;...
&lt;/p&gt;
&lt;p&gt;
Spurred by advancements in scale, large language models (LLMs) have demonstrated strong few-shot learning ability via in-context learning (ICL). However, the performance of ICL has been shown to be highly sensitive to the selection of few-shot demonstrations. Selecting the most suitable examples as context remains an ongoing challenge and an open problem. Existing literature has highlighted the importance of selecting examples that are diverse or semantically similar to the test sample while ignoring the fact that the optimal selection dimension, i.e., diversity or similarity, is task-specific. Leveraging the merits of both dimensions, we propose Iterative Demonstration Selection (IDS). Using zero-shot chain-of-thought reasoning (Zero-shot-CoT), IDS iteratively selects examples that are diverse but still strongly correlated with the test sample as ICL demonstrations. Specifically, IDS applies Zero-shot-CoT to the test sample before demonstration selection. The output reasoning path is 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#35805;&#24605;&#36335;&#25552;&#28860;&#30340;&#30693;&#35782;&#25552;&#28860;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#25945;&#24072;&#65292;&#24182;&#36890;&#36807;&#23545;&#40784;&#36807;&#28388;&#22120;&#36873;&#25321;&#24615;&#22320;&#25552;&#28860;&#19968;&#33268;&#21644;&#26377;&#29992;&#30340;&#29702;&#30001;&#12290;&#21516;&#26102;&#65292;&#20171;&#32461;&#20102;&#19968;&#20010;&#21487;&#38752;&#30340;&#23545;&#35805;&#24605;&#36335;&#25512;&#29702;&#22120;&#65292;&#29992;&#20110;&#29983;&#25104;&#21709;&#24212;&#12290;</title><link>http://arxiv.org/abs/2310.09343</link><description>&lt;p&gt;
&#23545;&#24120;&#35782;&#24863;&#30693;&#23545;&#35805;&#20195;&#29702;&#30340;&#23545;&#35805;&#24605;&#36335;&#25552;&#28860;
&lt;/p&gt;
&lt;p&gt;
Dialogue Chain-of-Thought Distillation for Commonsense-aware Conversational Agents. (arXiv:2310.09343v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09343
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#35805;&#24605;&#36335;&#25552;&#28860;&#30340;&#30693;&#35782;&#25552;&#28860;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#25945;&#24072;&#65292;&#24182;&#36890;&#36807;&#23545;&#40784;&#36807;&#28388;&#22120;&#36873;&#25321;&#24615;&#22320;&#25552;&#28860;&#19968;&#33268;&#21644;&#26377;&#29992;&#30340;&#29702;&#30001;&#12290;&#21516;&#26102;&#65292;&#20171;&#32461;&#20102;&#19968;&#20010;&#21487;&#38752;&#30340;&#23545;&#35805;&#24605;&#36335;&#25512;&#29702;&#22120;&#65292;&#29992;&#20110;&#29983;&#25104;&#21709;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#29616;&#20154;&#31867;&#21270;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#38656;&#35201;&#20351;&#29992;&#24120;&#35782;&#25512;&#29702;&#26469;&#26377;&#25928;&#22320;&#29702;&#35299;&#21644;&#22238;&#24212;&#23545;&#35805;&#20013;&#30340;&#38544;&#21547;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#23454;&#29616;&#36825;&#26679;&#30340;&#36830;&#36143;&#24615;&#21644;&#20449;&#24687;&#21547;&#37327;&#26159;&#19968;&#20010;&#38750;&#24120;&#22256;&#38590;&#30340;&#20219;&#21153;&#12290;&#21363;&#20351;&#23545;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22312;&#19968;&#20010;&#21333;&#36339;&#20869;&#35782;&#21035;&#21644;&#32858;&#21512;&#20851;&#38190;&#35777;&#25454;&#30340;&#20219;&#21153;&#20063;&#26159;&#20855;&#26377;&#30456;&#24403;&#22823;&#25361;&#25112;&#24615;&#30340;&#12290;&#36825;&#20010;&#22797;&#26434;&#24615;&#30340;&#21407;&#22240;&#26159;&#36825;&#26679;&#30340;&#35777;&#25454;&#20998;&#25955;&#22312;&#23545;&#35805;&#30340;&#22810;&#20010;&#36718;&#27425;&#20013;&#65292;&#22240;&#27492;&#38656;&#35201;&#22312;&#22810;&#20010;&#36339;&#20013;&#36827;&#34892;&#25972;&#21512;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#28966;&#28857;&#26159;&#20419;&#36827;&#23545;&#35805;&#19978;&#30340;&#22810;&#36339;&#25512;&#29702;&#65292;&#21363;&#23545;&#35805;&#24605;&#36335;&#65288;CoT&#65289;&#25512;&#29702;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#30693;&#35782;&#25552;&#28860;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#40784;&#36807;&#28388;&#22120;&#36873;&#25321;&#24615;&#22320;&#25552;&#28860;&#19968;&#33268;&#21644;&#26377;&#29992;&#30340;&#29702;&#30001;&#65292;&#21033;&#29992;LLMs&#20316;&#20026;&#19981;&#21487;&#38752;&#30340;&#25945;&#24072;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;DOCTOR&#65292;&#19968;&#20010;&#25552;&#20379;&#21487;&#38752;&#30340;CoT&#29702;&#30001;&#20197;&#36827;&#34892;&#21709;&#24212;&#29983;&#25104;&#30340;&#23545;&#35805;&#24605;&#36335;&#25512;&#29702;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human-like chatbots necessitate the use of commonsense reasoning in order to effectively comprehend and respond to implicit information present within conversations. Achieving such coherence and informativeness in responses, however, is a non-trivial task. Even for large language models (LLMs), the task of identifying and aggregating key evidence within a single hop presents a substantial challenge. This complexity arises because such evidence is scattered across multiple turns in a conversation, thus necessitating integration over multiple hops. Hence, our focus is to facilitate such multi-hop reasoning over a dialogue context, namely dialogue chain-of-thought (CoT) reasoning. To this end, we propose a knowledge distillation framework that leverages LLMs as unreliable teachers and selectively distills consistent and helpful rationales via alignment filters. We further present DOCTOR, a DialOgue Chain-of-ThOught Reasoner that provides reliable CoT rationales for response generation. We
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;LoftQ&#65306;&#19968;&#31181;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;LoRA&#31934;&#35843;&#24863;&#30693;&#37327;&#21270;&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#21516;&#26102;&#23545;LLM&#36827;&#34892;&#37327;&#21270;&#65292;&#24182;&#20026;LoRA&#31934;&#35843;&#25214;&#21040;&#36866;&#24403;&#30340;&#20302;&#31209;&#21021;&#22987;&#21270;&#65292;&#20197;&#32531;&#35299;&#37327;&#21270;&#27169;&#22411;&#21644;&#20840;&#31934;&#24230;&#27169;&#22411;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#20102;&#19979;&#28216;&#20219;&#21153;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.08659</link><description>&lt;p&gt;
LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models
&lt;/p&gt;
&lt;p&gt;
LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models. (arXiv:2310.08659v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08659
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;LoftQ&#65306;&#19968;&#31181;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;LoRA&#31934;&#35843;&#24863;&#30693;&#37327;&#21270;&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#21516;&#26102;&#23545;LLM&#36827;&#34892;&#37327;&#21270;&#65292;&#24182;&#20026;LoRA&#31934;&#35843;&#25214;&#21040;&#36866;&#24403;&#30340;&#20302;&#31209;&#21021;&#22987;&#21270;&#65292;&#20197;&#32531;&#35299;&#37327;&#21270;&#27169;&#22411;&#21644;&#20840;&#31934;&#24230;&#27169;&#22411;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#20102;&#19979;&#28216;&#20219;&#21153;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#21270;&#26159;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#26381;&#21153;&#30340;&#19981;&#21487;&#25110;&#32570;&#30340;&#25216;&#26415;&#65292;&#24182;&#26368;&#36817;&#34987;&#24212;&#29992;&#20110;LoRA&#31934;&#35843;&#20013;&#12290;&#26412;&#25991;&#20851;&#27880;&#22312;&#39044;&#35757;&#32451;&#27169;&#22411;&#19978;&#21516;&#26102;&#24212;&#29992;&#37327;&#21270;&#21644;LoRA&#31934;&#35843;&#30340;&#22330;&#26223;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#24120;&#24120;&#35266;&#23519;&#21040;&#23436;&#25972;&#31934;&#35843;&#21644;&#37327;&#21270;&#21152;LoRA&#31934;&#35843;&#26041;&#27861;&#20043;&#38388;&#22312;&#19979;&#28216;&#20219;&#21153;&#34920;&#29616;&#19978;&#23384;&#22312;&#19968;&#33268;&#30340;&#24046;&#36317;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LoftQ&#65288;LoRA-Fine-Tuning-aware Quantization&#65289;&#8212;&#8212;&#19968;&#31181;&#26032;&#30340;&#37327;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#21516;&#26102;&#23545;LLM&#36827;&#34892;&#37327;&#21270;&#65292;&#24182;&#25214;&#21040;&#36866;&#24403;&#30340;&#20302;&#31209;&#21021;&#22987;&#21270;&#26469;&#36827;&#34892;LoRA&#31934;&#35843;&#12290;&#36825;&#31181;&#21021;&#22987;&#21270;&#20943;&#36731;&#20102;&#37327;&#21270;&#27169;&#22411;&#21644;&#20840;&#31934;&#24230;&#27169;&#22411;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#20102;&#19979;&#28216;&#20219;&#21153;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#12289;&#38382;&#31572;&#12289;&#25688;&#35201;&#21644;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#38750;&#24120;&#26377;&#25928;&#65292;&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantization is an indispensable technique for serving Large Language Models (LLMs) and has recently found its way into LoRA fine-tuning. In this work we focus on the scenario where quantization and LoRA fine-tuning are applied together on a pre-trained model. In such cases it is common to observe a consistent gap in the performance on downstream tasks between full fine-tuning and quantization plus LoRA fine-tuning approach. In response, we propose LoftQ (LoRA-Fine-Tuning-aware Quantization), a novel quantization framework that simultaneously quantizes an LLM and finds a proper low-rank initialization for LoRA fine-tuning. Such an initialization alleviates the discrepancy between the quantized and full-precision model and significantly improves the generalization in downstream tasks. We evaluate our method on natural language understanding, question answering, summarization, and natural language generation tasks. Experiments show that our method is highly effective and outperforms exis
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#20998;&#23618;&#25351;&#25968;&#26063;&#33021;&#37327;&#27169;&#22411;(HEE)&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#37319;&#26679;&#26799;&#24230;&#20174;&#32780;&#20272;&#35745;&#20998;&#21306;&#20989;&#25968;&#21644;&#36827;&#34892;&#25512;&#26029;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#33021;&#37327;&#27169;&#22411;&#20013;&#30340;&#36127;&#30456;&#20301;&#38382;&#39064;&#65292;&#20174;&#32780;&#20351;&#23398;&#20064;&#36807;&#31243;&#23616;&#37096;&#21270;&#19988;&#23481;&#26131;&#25910;&#25947;&#12290;</title><link>http://arxiv.org/abs/2310.08431</link><description>&lt;p&gt;
&#20998;&#23618;&#25351;&#25968;&#26063;&#33021;&#37327;&#27169;&#22411;&#20013;&#30340;&#31070;&#32463;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
Neural Sampling in Hierarchical Exponential-family Energy-based Models. (arXiv:2310.08431v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08431
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#20998;&#23618;&#25351;&#25968;&#26063;&#33021;&#37327;&#27169;&#22411;(HEE)&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#37319;&#26679;&#26799;&#24230;&#20174;&#32780;&#20272;&#35745;&#20998;&#21306;&#20989;&#25968;&#21644;&#36827;&#34892;&#25512;&#26029;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#33021;&#37327;&#27169;&#22411;&#20013;&#30340;&#36127;&#30456;&#20301;&#38382;&#39064;&#65292;&#20174;&#32780;&#20351;&#23398;&#20064;&#36807;&#31243;&#23616;&#37096;&#21270;&#19988;&#23481;&#26131;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#33041;&#29702;&#35770;&#35748;&#20026;&#22823;&#33041;&#20351;&#29992;&#29983;&#25104;&#27169;&#22411;&#26469;&#29702;&#35299;&#22806;&#37096;&#19990;&#30028;&#12290;&#22522;&#20110;&#37319;&#26679;&#30340;&#35266;&#28857;&#35748;&#20026;&#65292;&#22823;&#33041;&#36890;&#36807;&#38543;&#26426;&#31070;&#32463;&#21709;&#24212;&#26679;&#26412;&#25512;&#26029;&#21518;&#39564;&#20998;&#24067;&#12290;&#27492;&#22806;&#65292;&#22823;&#33041;&#36824;&#19981;&#26029;&#26356;&#26032;&#20854;&#29983;&#25104;&#27169;&#22411;&#20197;&#36924;&#36817;&#22806;&#37096;&#19990;&#30028;&#30340;&#30495;&#23454;&#20998;&#24067;&#12290;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20998;&#23618;&#25351;&#25968;&#26063;&#33021;&#37327;&#27169;&#22411;(HEE)&#65292;&#35813;&#27169;&#22411;&#25429;&#25417;&#20102;&#25512;&#26029;&#21644;&#23398;&#20064;&#30340;&#21160;&#24577;&#36807;&#31243;&#12290;&#22312;HEE&#27169;&#22411;&#20013;&#65292;&#25105;&#20204;&#23558;&#20998;&#21306;&#20989;&#25968;&#20998;&#35299;&#20026;&#22810;&#20010;&#23618;&#27425;&#65292;&#24182;&#21033;&#29992;&#19968;&#32452;&#20855;&#26377;&#36739;&#30701;&#26102;&#38388;&#24120;&#25968;&#30340;&#31070;&#32463;&#20803;&#26469;&#37319;&#26679;&#20998;&#35299;&#30340;&#24402;&#19968;&#21270;&#39033;&#30340;&#26799;&#24230;&#12290;&#36825;&#20351;&#24471;&#25105;&#20204;&#30340;&#27169;&#22411;&#33021;&#22815;&#21516;&#26102;&#20272;&#35745;&#20998;&#21306;&#20989;&#25968;&#24182;&#36827;&#34892;&#25512;&#26029;&#65292;&#20174;&#32780;&#36991;&#20813;&#20102;&#20256;&#32479;&#33021;&#37327;&#27169;&#22411;(EBMs)&#20013;&#36935;&#21040;&#30340;&#36127;&#30456;&#20301;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#23398;&#20064;&#36807;&#31243;&#22312;&#26102;&#38388;&#21644;&#31354;&#38388;&#19978;&#37117;&#26159;&#23616;&#37096;&#21270;&#30340;&#65292;&#27169;&#22411;&#25910;&#25947;&#23481;&#26131;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian brain theory suggests that the brain employs generative models to understand the external world. The sampling-based perspective posits that the brain infers the posterior distribution through samples of stochastic neuronal responses. Additionally, the brain continually updates its generative model to approach the true distribution of the external world. In this study, we introduce the Hierarchical Exponential-family Energy-based (HEE) model, which captures the dynamics of inference and learning. In the HEE model, we decompose the partition function into individual layers and leverage a group of neurons with shorter time constants to sample the gradient of the decomposed normalization term. This allows our model to estimate the partition function and perform inference simultaneously, circumventing the negative phase encountered in conventional energy-based models (EBMs). As a result, the learning process is localized both in time and space, and the model is easy to converge. To
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;SegLoc&#65292;&#19968;&#31181;&#29992;&#20110;&#23433;&#20840;&#26816;&#26597;X&#23556;&#32447;&#22270;&#20687;&#30340;&#23494;&#38598;&#39044;&#27979;&#20219;&#21153;&#30340;&#26032;&#39062;&#35270;&#35273;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#26696;&#12290;&#35813;&#26041;&#26696;&#32467;&#21512;&#20102;&#23545;&#27604;&#23398;&#20064;&#21644;&#29616;&#26377;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#65292;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#36229;&#36234;&#20102;&#20256;&#32479;&#30340;&#26377;&#30417;&#30563;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2310.08421</link><description>&lt;p&gt;
SegLoc: &#26032;&#39062;&#30340;&#35270;&#35273;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#26696;&#29992;&#20110;&#23433;&#20840;&#26816;&#26597;X&#23556;&#32447;&#22270;&#20687;&#30340;&#23494;&#38598;&#39044;&#27979;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
SegLoc: Novel Visual Self-supervised Learning Scheme for Dense Prediction Tasks of Security Inspection X-ray Images. (arXiv:2310.08421v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08421
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;SegLoc&#65292;&#19968;&#31181;&#29992;&#20110;&#23433;&#20840;&#26816;&#26597;X&#23556;&#32447;&#22270;&#20687;&#30340;&#23494;&#38598;&#39044;&#27979;&#20219;&#21153;&#30340;&#26032;&#39062;&#35270;&#35273;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#26696;&#12290;&#35813;&#26041;&#26696;&#32467;&#21512;&#20102;&#23545;&#27604;&#23398;&#20064;&#21644;&#29616;&#26377;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#65292;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#36229;&#36234;&#20102;&#20256;&#32479;&#30340;&#26377;&#30417;&#30563;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20154;&#24037;&#26234;&#33021;&#30340;&#26174;&#33879;&#36827;&#23637;&#24402;&#21151;&#20110;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#26696;&#30340;&#25972;&#21512;&#12290;&#23613;&#31649;&#22312;NLP&#39046;&#22495;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#25104;&#23601;&#65292;&#20294;&#19982;&#35745;&#31639;&#26426;&#35270;&#35273;&#30456;&#27604;&#65292;&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#36824;&#19981;&#33021;&#20445;&#25345;&#30456;&#24212;&#30340;&#21457;&#23637;&#12290;&#26368;&#36817;&#65292;&#23558;&#23545;&#27604;&#23398;&#20064;&#19982;&#29616;&#26377;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#32467;&#21512;&#36215;&#26469;&#65292;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#35270;&#35273;&#33258;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#34920;&#29616;&#20986;&#36229;&#36234;&#26377;&#30417;&#30563;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#36825;&#20123;&#25913;&#36827;&#37117;&#23616;&#38480;&#20110;&#20998;&#31867;&#20219;&#21153;&#65292;&#24182;&#19988;&#21482;&#26377;&#23569;&#25968;&#24037;&#20316;&#33268;&#21147;&#20110;&#35780;&#20272;&#35745;&#31639;&#26426;&#35270;&#35273;&#23454;&#38469;&#22330;&#26223;&#19979;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#65292;&#32780;&#22823;&#37096;&#20998;&#24037;&#20316;&#38598;&#20013;&#22312;&#21253;&#21547;&#31867;&#21035;&#20154;&#20687;&#22270;&#20687;&#30340;&#25968;&#25454;&#38598;&#20013;&#65292;&#23588;&#20854;&#26159;ImageNet&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#23433;&#20840;&#26816;&#26597;X&#23556;&#32447;&#22270;&#20687;&#20013;&#30340;&#35821;&#20041;&#20998;&#21106;&#30340;&#23494;&#38598;&#39044;&#27979;&#20219;&#21153;&#26469;&#35780;&#20272;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;SegLoc&#12290;
&lt;/p&gt;
&lt;p&gt;
Lately, remarkable advancements of artificial intelligence have been attributed to the integration of self-supervised learning scheme. Despite impressive achievements within NLP, yet SSL in computer vision has not been able to stay on track comparatively. Recently, integration of contrastive learning on top of existing SSL models has established considerable progress in computer vision through which visual SSL models have outperformed their supervised counterparts. Nevertheless, most of these improvements were limited to classification tasks, and also, few works have been dedicated to evaluation of SSL models in real-world scenarios of computer vision, while the majority of works are centered around datasets containing class-wise portrait images, most notably, ImageNet. Consequently, in this work, we have considered dense prediction task of semantic segmentation in security inspection x-ray images to evaluate our proposed model Segmentation Localization. Based upon the model Instance L
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24605;&#36335;&#38142;&#65288;CoT&#65289;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#23569;&#26679;&#26412;&#30693;&#35782;&#24211;&#38382;&#39064;&#29983;&#25104;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#23558;&#38382;&#39064;&#29983;&#25104;&#20219;&#21153;&#24418;&#24335;&#21270;&#20026;&#25512;&#29702;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#26816;&#32034;&#25903;&#25345;&#24615;&#36923;&#36753;&#24418;&#24335;&#21644;&#32534;&#20889;&#25552;&#31034;&#26469;&#23454;&#29616;&#29983;&#25104;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2310.08395</link><description>&lt;p&gt;
&#20351;&#29992;&#24605;&#36335;&#38142;&#36827;&#34892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23569;&#26679;&#26412;&#30693;&#35782;&#24211;&#38382;&#39064;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Prompting Large Language Models with Chain-of-Thought for Few-Shot Knowledge Base Question Generation. (arXiv:2310.08395v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08395
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24605;&#36335;&#38142;&#65288;CoT&#65289;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#23569;&#26679;&#26412;&#30693;&#35782;&#24211;&#38382;&#39064;&#29983;&#25104;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#23558;&#38382;&#39064;&#29983;&#25104;&#20219;&#21153;&#24418;&#24335;&#21270;&#20026;&#25512;&#29702;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#26816;&#32034;&#25903;&#25345;&#24615;&#36923;&#36753;&#24418;&#24335;&#21644;&#32534;&#20889;&#25552;&#31034;&#26469;&#23454;&#29616;&#29983;&#25104;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#24211;&#38382;&#31572;&#29983;&#25104;&#65288;KBQG&#65289;&#30340;&#20219;&#21153;&#26159;&#23558;&#36923;&#36753;&#24418;&#24335;&#36716;&#21270;&#20026;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#12290;&#30001;&#20110;&#22823;&#35268;&#27169;&#38382;&#39064;&#27880;&#37322;&#30340;&#26114;&#36149;&#25104;&#26412;&#65292;&#22312;&#20302;&#36164;&#28304;&#22330;&#26223;&#19979;&#24613;&#38656;&#24320;&#21457;KBQG&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#26041;&#27861;&#22312;&#23569;&#26679;&#26412;&#38382;&#39064;&#29983;&#25104;&#20013;&#36807;&#20110;&#20381;&#36182;&#27880;&#37322;&#25968;&#25454;&#30340;&#24494;&#35843;&#65292;&#36825;&#23545;&#20110;&#23569;&#26679;&#26412;&#38382;&#39064;&#29983;&#25104;&#24182;&#19981;&#21512;&#36866;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#20986;&#29616;&#23637;&#31034;&#20102;&#23427;&#20204;&#22312;&#23569;&#26679;&#26412;&#20219;&#21153;&#20013;&#30340;&#21360;&#35937;&#21147;&#27867;&#21270;&#33021;&#21147;&#12290;&#21463;&#21040;&#24605;&#36335;&#38142;&#65288;CoT&#65289;&#25552;&#31034;&#30340;&#21551;&#21457;&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;&#25512;&#29702;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#31574;&#30053;&#65292;&#25105;&#20204;&#23558;KBQG&#20219;&#21153;&#24418;&#24335;&#21270;&#20026;&#25512;&#29702;&#38382;&#39064;&#65292;&#20854;&#20013;&#19968;&#20010;&#23436;&#25972;&#38382;&#39064;&#30340;&#29983;&#25104;&#34987;&#20998;&#20026;&#19968;&#31995;&#21015;&#30340;&#23376;&#38382;&#39064;&#29983;&#25104;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#25552;&#31034;&#26041;&#27861;KQG-CoT&#39318;&#20808;&#20174;&#26410;&#26631;&#35760;&#25968;&#25454;&#27744;&#20013;&#26816;&#32034;&#25903;&#25345;&#24615;&#30340;&#36923;&#36753;&#24418;&#24335;&#65292;&#32771;&#34385;&#36923;&#36753;&#24418;&#24335;&#30340;&#29305;&#24449;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#32534;&#20889;&#19968;&#20010;&#25552;&#31034;&#26469;&#26126;&#30830;&#25512;&#29702;&#38142;&#30340;&#29983;&#25104;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
The task of Question Generation over Knowledge Bases (KBQG) aims to convert a logical form into a natural language question. For the sake of expensive cost of large-scale question annotation, the methods of KBQG under low-resource scenarios urgently need to be developed. However, current methods heavily rely on annotated data for fine-tuning, which is not well-suited for few-shot question generation. The emergence of Large Language Models (LLMs) has shown their impressive generalization ability in few-shot tasks. Inspired by Chain-of-Thought (CoT) prompting, which is an in-context learning strategy for reasoning, we formulate KBQG task as a reasoning problem, where the generation of a complete question is splitted into a series of sub-question generation. Our proposed prompting method KQG-CoT first retrieves supportive logical forms from the unlabeled data pool taking account of the characteristics of the logical form. Then, we write a prompt to explicit the reasoning chain of generati
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#25552;&#20986;&#33258;&#25105;&#25913;&#36827;&#30340;&#24320;&#28304;&#27169;&#22411;&#21644;&#19968;&#20010;&#26032;&#30340;&#25490;&#21517;&#25351;&#26631;&#65288;PeRFICS&#65289;&#26469;&#35299;&#20915;&#24615;&#33021;&#21644;&#25104;&#26412;&#20043;&#38388;&#30340;&#26435;&#34913;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#21508;&#31181;&#22823;&#23567;&#30340;&#24320;&#28304;&#27169;&#22411;&#20013;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#65292;&#24182;&#20026;&#24320;&#28304;LLMs&#30340;&#21457;&#23637;&#25552;&#20379;&#20102;&#19968;&#31181;&#25104;&#26412;&#20248;&#21270;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.07611</link><description>&lt;p&gt;
LLM&#30340;&#27665;&#20027;&#21270;&#65306;&#33258;&#25105;&#25913;&#36827;&#30340;&#24320;&#28304;&#27169;&#22411;&#20013;&#24615;&#33021;&#19982;&#25104;&#26412;&#30340;&#26435;&#34913;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Democratizing LLMs: An Exploration of Cost-Performance Trade-offs in Self-Refined Open-Source Models. (arXiv:2310.07611v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07611
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#25552;&#20986;&#33258;&#25105;&#25913;&#36827;&#30340;&#24320;&#28304;&#27169;&#22411;&#21644;&#19968;&#20010;&#26032;&#30340;&#25490;&#21517;&#25351;&#26631;&#65288;PeRFICS&#65289;&#26469;&#35299;&#20915;&#24615;&#33021;&#21644;&#25104;&#26412;&#20043;&#38388;&#30340;&#26435;&#34913;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#21508;&#31181;&#22823;&#23567;&#30340;&#24320;&#28304;&#27169;&#22411;&#20013;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#65292;&#24182;&#20026;&#24320;&#28304;LLMs&#30340;&#21457;&#23637;&#25552;&#20379;&#20102;&#19968;&#31181;&#25104;&#26412;&#20248;&#21270;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31169;&#26377;LLMs&#30340;&#20027;&#23548;&#22320;&#20301;&#23548;&#33268;&#20102;&#21463;&#38480;&#30340;&#35775;&#38382;&#21644;&#25552;&#39640;&#30340;&#20449;&#24687;&#38544;&#31169;&#38382;&#39064;&#12290;&#23545;&#20110;&#20449;&#24687;&#25935;&#24863;&#21644;&#39640;&#23481;&#37327;&#24212;&#29992;&#31243;&#24207;&#26469;&#35828;&#65292;&#39640;&#24615;&#33021;&#30340;&#24320;&#28304;&#26367;&#20195;&#26041;&#26696;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#36890;&#24120;&#22312;&#24615;&#33021;&#26041;&#38754;&#33853;&#21518;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;(1)&#19968;&#31181;&#26080;&#22806;&#37096;&#24433;&#21709;&#30340;&#38750;&#38024;&#23545;&#24615;&#36845;&#20195;&#33258;&#25105;&#25209;&#35780;&#21644;&#33258;&#25105;&#25913;&#36827;&#30340;&#21464;&#20307;&#12290;(2)&#19968;&#31181;&#26032;&#39062;&#30340;&#25490;&#21517;&#25351;&#26631; - &#24615;&#33021;&#12289;&#25913;&#36827;&#21644;&#25512;&#29702;&#25104;&#26412;&#24471;&#20998;(PeRFICS)&#65292;&#20197;&#32771;&#34385;&#25913;&#36827;&#21518;&#30340;&#24615;&#33021;&#21644;&#25104;&#26412;&#26469;&#25214;&#21040;&#32473;&#23450;&#20219;&#21153;&#30340;&#26368;&#20339;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#19981;&#21516;&#22823;&#23567;&#30340;&#26368;&#20808;&#36827;&#24320;&#28304;&#27169;&#22411;&#20174;7B&#21040;65B&#65292;&#24179;&#22343;&#25913;&#21892;&#20102;8.2%&#30340;&#22522;&#20934;&#24615;&#33021;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#21363;&#20351;&#26159;&#20869;&#23384;&#21344;&#29992;&#26497;&#23567;&#30340;&#27169;&#22411;&#65292;&#27604;&#22914;Vicuna-7B&#65292;&#22312;&#25972;&#20307;&#19978;&#20063;&#26377;11.74%&#30340;&#25913;&#21892;&#65292;&#24182;&#22312;Vicuna&#22522;&#20934;&#27979;&#35797;&#30340;&#39640;&#21019;&#36896;&#21147;&#21644;&#24320;&#25918;&#24615;&#20219;&#21153;&#20013;&#25552;&#39640;&#20102;25.39%&#12290;Vicuna-13B&#26356;&#36827;&#19968;&#27493;&#65292;&#22312;&#25913;&#36827;&#21518;&#36229;&#36234;&#20102;ChatGPT&#12290;&#36825;&#39033;&#24037;&#20316;&#20855;&#26377;&#37325;&#35201;&#30340;&#21019;&#26032;&#21644;&#36129;&#29486;&#65292;&#20026;&#24320;&#28304;LLMs&#30340;&#21457;&#23637;&#25552;&#20379;&#20102;&#19968;&#31181;&#25104;&#26412;&#20248;&#21270;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The dominance of proprietary LLMs has led to restricted access and raised information privacy concerns. High-performing open-source alternatives are crucial for information-sensitive and high-volume applications but often lag behind in performance. To address this gap, we propose (1) A untargeted variant of iterative self-critique and self-refinement devoid of external influence. (2) A novel ranking metric - Performance, Refinement, and Inference Cost Score (PeRFICS) - to find the optimal model for a given task considering refined performance and cost. Our experiments show that SoTA open source models of varying sizes from 7B - 65B, on average, improve 8.2% from their baseline performance. Strikingly, even models with extremely small memory footprints, such as Vicuna-7B, show a 11.74% improvement overall and up to a 25.39% improvement in high-creativity, open ended tasks on the Vicuna benchmark. Vicuna-13B takes it a step further and outperforms ChatGPT post-refinement. This work has p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Fed-GraB&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#33258;&#36866;&#24212;&#26799;&#24230;&#24179;&#34913;&#22120;&#26469;&#35299;&#20915;&#32852;&#37030;&#24335;&#38271;&#23614;&#23398;&#20064;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;&#38544;&#31169;&#32422;&#26463;&#19979;&#21051;&#30011;&#20840;&#23616;&#38271;&#23614;&#20998;&#24067;&#65292;&#24182;&#36890;&#36807;&#35843;&#25972;&#26412;&#22320;&#23398;&#20064;&#31574;&#30053;&#26469;&#35299;&#20915;&#22836;&#37096;-&#23614;&#37096;&#19981;&#24179;&#34913;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.07587</link><description>&lt;p&gt;
Fed-GraB&#65306;&#20855;&#26377;&#33258;&#36866;&#24212;&#26799;&#24230;&#24179;&#34913;&#22120;&#30340;&#32852;&#37030;&#24335;&#38271;&#23614;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Fed-GraB: Federated Long-tailed Learning with Self-Adjusting Gradient Balancer. (arXiv:2310.07587v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07587
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Fed-GraB&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#33258;&#36866;&#24212;&#26799;&#24230;&#24179;&#34913;&#22120;&#26469;&#35299;&#20915;&#32852;&#37030;&#24335;&#38271;&#23614;&#23398;&#20064;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;&#38544;&#31169;&#32422;&#26463;&#19979;&#21051;&#30011;&#20840;&#23616;&#38271;&#23614;&#20998;&#24067;&#65292;&#24182;&#36890;&#36807;&#35843;&#25972;&#26412;&#22320;&#23398;&#20064;&#31574;&#30053;&#26469;&#35299;&#20915;&#22836;&#37096;-&#23614;&#37096;&#19981;&#24179;&#34913;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#38544;&#31169;&#21644;&#38271;&#23614;&#20998;&#24067;&#22312;&#35768;&#22810;&#29616;&#23454;&#20219;&#21153;&#20013;&#26159;&#24120;&#24577;&#32780;&#38750;&#20363;&#22806;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#32852;&#37030;&#24335;&#38271;&#23614;&#23398;&#20064;&#65288;Fed-LT&#65289;&#20219;&#21153;&#65292;&#22312;&#35813;&#20219;&#21153;&#20013;&#65292;&#27599;&#20010;&#23458;&#25143;&#31471;&#25345;&#26377;&#19968;&#20010;&#26412;&#22320;&#24322;&#26500;&#25968;&#25454;&#38598;&#65307;&#22914;&#26524;&#21487;&#20197;&#20840;&#23616;&#32858;&#21512;&#25968;&#25454;&#38598;&#65292;&#21017;&#23427;&#20204;&#20849;&#21516;&#23637;&#29616;&#20986;&#38271;&#23614;&#20998;&#24067;&#12290;&#22312;&#36825;&#26679;&#30340;&#35774;&#32622;&#19979;&#65292;&#29616;&#26377;&#30340;&#32852;&#37030;&#20248;&#21270;&#21644;/&#25110;&#38598;&#20013;&#24335;&#38271;&#23614;&#23398;&#20064;&#26041;&#27861;&#24456;&#38590;&#24212;&#29992;&#65292;&#22240;&#20026;&#23384;&#22312;&#20197;&#19979;&#25361;&#25112;&#65306;&#65288;a&#65289;&#22312;&#38544;&#31169;&#32422;&#26463;&#19979;&#21051;&#30011;&#20840;&#23616;&#38271;&#23614;&#20998;&#24067;&#65292;&#20197;&#21450;&#65288;b&#65289;&#35843;&#25972;&#26412;&#22320;&#23398;&#20064;&#31574;&#30053;&#20197;&#24212;&#23545;&#22836;&#37096;-&#23614;&#37096;&#19981;&#24179;&#34913;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#31216;&#20026;$\texttt{Fed-GraB}$&#65292;&#23427;&#21253;&#25324;&#19968;&#20010;&#33258;&#36866;&#24212;&#26799;&#24230;&#24179;&#34913;&#22120;&#65288;SGB&#65289;&#27169;&#22359;&#65292;&#35813;&#27169;&#22359;&#20197;&#38381;&#29615;&#26041;&#24335;&#26681;&#25454;&#20840;&#23616;&#38271;&#23614;&#20998;&#24067;&#30340;&#21453;&#39304;&#23545;&#23458;&#25143;&#31471;&#30340;&#26799;&#24230;&#36827;&#34892;&#37325;&#26032;&#21152;&#26435;&#65292;&#35780;&#20272;&#26041;&#27861;&#20026;&#30452;&#25509;&#20808;&#39564;&#20998;&#26512;&#22120;&#65288;DPA&#65289;&#27169;&#22359;&#12290;&#20351;&#29992;$\texttt{Fed-GraB}$&#65292;&#23458;&#25143;&#31471;&#21487;&#20197;&#26377;&#25928;&#32531;&#35299;&#25968;&#25454;&#20998;&#24067;&#30340;&#19981;&#22343;&#34913;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data privacy and long-tailed distribution are the norms rather than the exception in many real-world tasks. This paper investigates a federated long-tailed learning (Fed-LT) task in which each client holds a locally heterogeneous dataset; if the datasets can be globally aggregated, they jointly exhibit a long-tailed distribution. Under such a setting, existing federated optimization and/or centralized long-tailed learning methods hardly apply due to challenges in (a) characterizing the global long-tailed distribution under privacy constraints and (b) adjusting the local learning strategy to cope with the head-tail imbalance. In response, we propose a method termed $\texttt{Fed-GraB}$, comprised of a Self-adjusting Gradient Balancer (SGB) module that re-weights clients' gradients in a closed-loop manner, based on the feedback of global long-tailed distribution evaluated by a Direct Prior Analyzer (DPA) module. Using $\texttt{Fed-GraB}$, clients can effectively alleviate the distribution
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21160;&#24577;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#23616;&#37096;&#20960;&#20309;&#32467;&#26500;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#28857;&#20113;&#30340;&#21435;&#22122;&#21644;&#24322;&#24120;&#26816;&#27979;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#20934;&#30830;&#24615;&#26041;&#38754;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.07376</link><description>&lt;p&gt;
&#20351;&#29992;&#21160;&#24577;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#23616;&#37096;&#20960;&#20309;&#32467;&#26500;&#23545;&#28857;&#20113;&#21435;&#22122;&#21644;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Point Cloud Denoising and Outlier Detection with Local Geometric Structure by Dynamic Graph CNN. (arXiv:2310.07376v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07376
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21160;&#24577;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#23616;&#37096;&#20960;&#20309;&#32467;&#26500;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#28857;&#20113;&#30340;&#21435;&#22122;&#21644;&#24322;&#24120;&#26816;&#27979;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#20934;&#30830;&#24615;&#26041;&#38754;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20250;&#30340;&#25968;&#23383;&#21270;&#24555;&#36895;&#21457;&#23637;&#27491;&#22312;&#26397;&#30528;&#23454;&#29616;&#25968;&#23383;&#23402;&#29983;&#21644;&#20803;&#23431;&#23449;&#30340;&#30446;&#26631;&#36808;&#36827;&#12290;&#29305;&#21035;&#26159;&#65292;&#28857;&#20113;&#20316;&#20026;3D&#31354;&#38388;&#30340;&#19968;&#31181;&#23186;&#20307;&#26684;&#24335;&#27491;&#22312;&#24341;&#36215;&#20851;&#27880;&#12290;&#30001;&#20110;&#27979;&#37327;&#35823;&#24046;&#65292;&#28857;&#20113;&#25968;&#25454;&#21463;&#21040;&#22122;&#22768;&#21644;&#24322;&#24120;&#20540;&#30340;&#27745;&#26579;&#12290;&#22240;&#27492;&#65292;&#23545;&#28857;&#20113;&#36827;&#34892;&#21435;&#22122;&#21644;&#24322;&#24120;&#26816;&#27979;&#26159;&#24517;&#35201;&#30340;&#12290;&#20854;&#20013;&#65292;PointCleanNet&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#28857;&#20113;&#21435;&#22122;&#21644;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#23427;&#27809;&#26377;&#32771;&#34385;&#21040;&#34917;&#19969;&#30340;&#23616;&#37096;&#20960;&#20309;&#32467;&#26500;&#12290;&#25105;&#20204;&#36890;&#36807;&#24212;&#29992;&#22522;&#20110;&#21160;&#24577;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#35774;&#35745;&#30340;&#20004;&#31181;&#31867;&#22411;&#30340;&#22270;&#21367;&#31215;&#23618;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#34913;&#37327;&#24322;&#24120;&#26816;&#27979;&#20934;&#30830;&#24615;&#30340;AUPR&#21644;&#34913;&#37327;&#21435;&#22122;&#20934;&#30830;&#24615;&#30340;Chamfer&#36317;&#31163;&#26041;&#38754;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The digitalization of society is rapidly developing toward the realization of the digital twin and metaverse. In particular, point clouds are attracting attention as a media format for 3D space. Point cloud data is contaminated with noise and outliers due to measurement errors. Therefore, denoising and outlier detection are necessary for point cloud processing. Among them, PointCleanNet is an effective method for point cloud denoising and outlier detection. However, it does not consider the local geometric structure of the patch. We solve this problem by applying two types of graph convolutional layer designed based on the Dynamic Graph CNN. Experimental results show that the proposed methods outperform the conventional method in AUPR, which indicates outlier detection accuracy, and Chamfer Distance, which indicates denoising accuracy.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#25105;&#25552;&#21319;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#36827;&#30693;&#35782;&#39537;&#21160;&#23545;&#35805;&#29983;&#25104;&#30340;&#20013;&#38388;&#27493;&#39588;&#30340;&#29983;&#25104;&#24615;&#33021;&#12290;&#36890;&#36807;&#24341;&#20837;&#24341;&#23548;&#25552;&#31034;&#21644;&#20462;&#25913;&#25439;&#22833;&#20989;&#25968;&#30340;&#33258;&#20030;&#31574;&#30053;&#65292;&#25552;&#39640;&#20102;&#29983;&#25104;&#33258;&#21160;&#29983;&#25104;&#22238;&#31572;&#30340;&#22810;&#26679;&#24615;&#65292;&#24182;&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.06404</link><description>&lt;p&gt;
Hexa: &#30693;&#35782;&#39537;&#21160;&#30340;&#23545;&#35805;&#31995;&#32479;&#30340;&#33258;&#25105;&#25552;&#21319;
&lt;/p&gt;
&lt;p&gt;
Hexa: Self-Improving for Knowledge-Grounded Dialogue System. (arXiv:2310.06404v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06404
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#25105;&#25552;&#21319;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#36827;&#30693;&#35782;&#39537;&#21160;&#23545;&#35805;&#29983;&#25104;&#30340;&#20013;&#38388;&#27493;&#39588;&#30340;&#29983;&#25104;&#24615;&#33021;&#12290;&#36890;&#36807;&#24341;&#20837;&#24341;&#23548;&#25552;&#31034;&#21644;&#20462;&#25913;&#25439;&#22833;&#20989;&#25968;&#30340;&#33258;&#20030;&#31574;&#30053;&#65292;&#25552;&#39640;&#20102;&#29983;&#25104;&#33258;&#21160;&#29983;&#25104;&#22238;&#31572;&#30340;&#22810;&#26679;&#24615;&#65292;&#24182;&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#39537;&#21160;&#30340;&#23545;&#35805;&#29983;&#25104;&#20013;&#19968;&#31181;&#24120;&#35265;&#30340;&#20570;&#27861;&#26159;&#20351;&#29992;&#27169;&#22359;&#21270;&#30340;&#26041;&#27861;&#26126;&#30830;&#22320;&#21033;&#29992;&#20013;&#38388;&#27493;&#39588;&#65288;&#22914;&#32593;&#32476;&#25628;&#32034;&#12289;&#35760;&#24518;&#26816;&#32034;&#65289;&#12290;&#28982;&#32780;&#65292;&#19982;&#23545;&#35805;&#21709;&#24212;&#30456;&#27604;&#65292;&#36825;&#20123;&#27493;&#39588;&#30340;&#25968;&#25454;&#24448;&#24448;&#38590;&#20197;&#33719;&#21462;&#65292;&#22240;&#20026;&#22312;&#26222;&#36890;&#23545;&#35805;&#20013;&#26080;&#27861;&#35266;&#23519;&#21040;&#23427;&#20204;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#20123;&#25968;&#25454;&#30340;&#32570;&#22833;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#33258;&#25105;&#25552;&#21319;&#26041;&#27861;&#65292;&#20197;&#25913;&#36827;&#20013;&#38388;&#27493;&#39588;&#30340;&#29983;&#25104;&#24615;&#33021;&#65292;&#32780;&#19981;&#38656;&#35201;&#22320;&#38754;&#30495;&#23454;&#25968;&#25454;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24341;&#23548;&#25552;&#31034;&#21644;&#20462;&#25913;&#30340;&#25439;&#22833;&#20989;&#25968;&#30340;&#24341;&#23548;&#33258;&#21160;&#29983;&#25104;&#22238;&#31572;&#22810;&#26679;&#24615;&#30340;&#33258;&#20030;&#26041;&#27861;&#12290;&#36890;&#36807;&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#25105;&#20204;&#32463;&#39564;&#35777;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#25104;&#21151;&#22320;&#21033;&#29992;&#20102;&#33258;&#25105;&#25552;&#21319;&#26426;&#21046;&#65292;&#22312;&#29983;&#25104;&#20013;&#38388;&#21644;&#26368;&#32456;&#22238;&#31572;&#26041;&#38754;&#25913;&#21892;&#20102;&#30693;&#35782;&#39537;&#21160;&#23545;&#35805;&#29983;&#25104;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
A common practice in knowledge-grounded dialogue generation is to explicitly utilize intermediate steps (e.g., web-search, memory retrieval) with modular approaches. However, data for such steps are often inaccessible compared to those of dialogue responses as they are unobservable in an ordinary dialogue. To fill in the absence of these data, we develop a self-improving method to improve the generative performances of intermediate steps without the ground truth data. In particular, we propose a novel bootstrapping scheme with a guided prompt and a modified loss function to enhance the diversity of appropriate self-generated responses. Through experiments on various benchmark datasets, we empirically demonstrate that our method successfully leverages a self-improving mechanism in generating intermediate and final responses and improves the performances on the task of knowledge-grounded dialogue generation.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36125;&#21494;&#26031;&#25512;&#26029;&#30340;&#26041;&#27861;&#26469;&#38477;&#20302;&#33258;&#21160;&#39550;&#39542;&#24863;&#30693;&#20013;&#30340;&#35823;&#25253;&#29575;&#12290;&#36890;&#36807;&#20351;&#29992;&#22810;&#24863;&#30693;&#21644;&#22810;&#27169;&#24577;&#26041;&#27861;&#65292;&#20197;&#21450;&#32771;&#34385;&#30446;&#26631;&#30340;&#20284;&#28982;&#20989;&#25968;&#21644;&#20808;&#39564;&#27010;&#29575;&#65292;&#35813;&#26041;&#27861;&#22312;&#20943;&#23569;&#35823;&#25253;&#29575;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;&#22312;KITTI&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.05951</link><description>&lt;p&gt;
&#20351;&#29992;&#36125;&#21494;&#26031;&#25512;&#26029;&#22312;&#33258;&#21160;&#39550;&#39542;&#24863;&#30693;&#20013;&#38477;&#20302;&#35823;&#25253;&#29575;
&lt;/p&gt;
&lt;p&gt;
Reducing the False Positive Rate Using Bayesian Inference in Autonomous Driving Perception. (arXiv:2310.05951v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05951
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36125;&#21494;&#26031;&#25512;&#26029;&#30340;&#26041;&#27861;&#26469;&#38477;&#20302;&#33258;&#21160;&#39550;&#39542;&#24863;&#30693;&#20013;&#30340;&#35823;&#25253;&#29575;&#12290;&#36890;&#36807;&#20351;&#29992;&#22810;&#24863;&#30693;&#21644;&#22810;&#27169;&#24577;&#26041;&#27861;&#65292;&#20197;&#21450;&#32771;&#34385;&#30446;&#26631;&#30340;&#20284;&#28982;&#20989;&#25968;&#21644;&#20808;&#39564;&#27010;&#29575;&#65292;&#35813;&#26041;&#27861;&#22312;&#20943;&#23569;&#35823;&#25253;&#29575;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;&#22312;KITTI&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#26631;&#35782;&#21035;&#26159;&#33258;&#21160;&#21644;&#26234;&#33021;&#36710;&#36742;&#24863;&#30693;&#31995;&#32479;&#20013;&#20851;&#38190;&#30340;&#19968;&#27493;&#65292;&#36825;&#24050;&#32463;&#24471;&#21040;&#20102;&#22823;&#37327;&#30340;&#30740;&#31350;&#24037;&#20316;&#30340;&#35777;&#26126;&#12290;&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#22810;&#24863;&#30693;&#21644;&#22810;&#27169;&#24577;&#26041;&#27861;&#26469;&#25506;&#32034;&#30446;&#26631;&#35782;&#21035;&#65292;&#26088;&#22312;&#38477;&#20302;&#35823;&#25253;&#29575;&#65288;FPR&#65289;&#12290;&#30001;&#20110;&#35823;&#25253;&#23545;&#35937;&#30340;&#38169;&#35823;&#20998;&#31867;&#21487;&#33021;&#23548;&#33268;&#20107;&#25925;&#65292;&#38477;&#20302;&#35823;&#25253;&#29575;&#22312;&#24863;&#30693;&#31995;&#32479;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36125;&#21494;&#26031;&#25512;&#26029;&#30340;&#31574;&#30053;&#26469;&#38477;&#20302;&#35823;&#25253;&#29575;&#65292;&#20854;&#20013;&#23558;&#20284;&#28982;&#20989;&#25968;&#35270;&#20026;&#39640;&#26031;&#26680;&#23494;&#24230;&#20272;&#35745;&#30340;&#32047;&#31215;&#20998;&#24067;&#20989;&#25968;&#65292;&#23558;&#20808;&#39564;&#27010;&#29575;&#35270;&#20026;&#24402;&#19968;&#21270;&#30452;&#26041;&#22270;&#30340;&#32047;&#31215;&#20989;&#25968;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;KITTI&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;&#28145;&#24230;&#32593;&#32476;&#65288;DenseNet&#12289;NasNet&#21644;EfficientNet&#65289;&#21644;&#26368;&#36817;&#30340;3D&#28857;&#20113;&#32593;&#32476;&#65288;PointNet&#21644;PointNet++&#65289;&#36827;&#34892;&#39564;&#35777;&#65292;&#32771;&#34385;&#20102;&#19977;&#20010;&#30446;&#26631;&#31867;&#21035;&#65288;&#27773;&#36710;&#12289;&#33258;&#34892;&#36710;&#12289;&#34892;&#20154;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Object recognition is a crucial step in perception systems for autonomous and intelligent vehicles, as evidenced by the numerous research works in the topic. In this paper, object recognition is explored by using multisensory and multimodality approaches, with the intention of reducing the false positive rate (FPR). The reduction of the FPR becomes increasingly important in perception systems since the misclassification of an object can potentially cause accidents. In particular, this work presents a strategy through Bayesian inference to reduce the FPR considering the likelihood function as a cumulative distribution function from Gaussian kernel density estimations, and the prior probabilities as cumulative functions of normalized histograms. The validation of the proposed methodology is performed on the KITTI dataset using deep networks (DenseNet, NasNet, and EfficientNet), and recent 3D point cloud networks (PointNet, and PintNet++), by considering three object-categories (cars, cyc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;Siamese&#32534;&#30721;&#22120;&#30340;&#23616;&#37096;&#24402;&#22240;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#38598;&#25104;&#26799;&#24230;&#21407;&#29702;&#25512;&#24191;&#21040;&#20855;&#26377;&#22810;&#20010;&#36755;&#20837;&#30340;&#27169;&#22411;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#35299;&#37322;&#21477;&#23376;&#36716;&#25442;&#22120;&#27169;&#22411;&#20013;&#37325;&#35201;&#30340;&#39044;&#27979;&#20196;&#29260;&#23545;&#65292;&#20027;&#35201;&#38598;&#20013;&#22312;&#21517;&#35789;&#21644;&#21160;&#35789;&#19978;&#12290;</title><link>http://arxiv.org/abs/2310.05703</link><description>&lt;p&gt;
Siamese&#32534;&#30721;&#22120;&#30340;&#24402;&#22240;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
An Attribution Method for Siamese Encoders. (arXiv:2310.05703v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05703
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;Siamese&#32534;&#30721;&#22120;&#30340;&#23616;&#37096;&#24402;&#22240;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#38598;&#25104;&#26799;&#24230;&#21407;&#29702;&#25512;&#24191;&#21040;&#20855;&#26377;&#22810;&#20010;&#36755;&#20837;&#30340;&#27169;&#22411;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#35299;&#37322;&#21477;&#23376;&#36716;&#25442;&#22120;&#27169;&#22411;&#20013;&#37325;&#35201;&#30340;&#39044;&#27979;&#20196;&#29260;&#23545;&#65292;&#20027;&#35201;&#38598;&#20013;&#22312;&#21517;&#35789;&#21644;&#21160;&#35789;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#21477;&#23376;&#36716;&#25442;&#22120;&#31561;Siamese&#32534;&#30721;&#22120;&#27169;&#22411;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#20154;&#20204;&#23545;&#23427;&#20204;&#20851;&#27880;&#30340;&#36755;&#20837;&#26041;&#38754;&#30693;&#20043;&#29978;&#23569;&#12290;&#19968;&#20010;&#38556;&#30861;&#26159;&#23427;&#20204;&#30340;&#39044;&#27979;&#19981;&#33021;&#24402;&#22240;&#20110;&#20010;&#21035;&#29305;&#24449;&#65292;&#22240;&#20026;&#23427;&#20204;&#27604;&#36739;&#30340;&#26159;&#20004;&#20010;&#36755;&#20837;&#32780;&#19981;&#26159;&#19968;&#20010;&#36755;&#20837;&#12290;&#26412;&#25991;&#36890;&#36807;&#23558;&#38598;&#25104;&#26799;&#24230;&#21407;&#29702;&#25512;&#24191;&#21040;&#20855;&#26377;&#22810;&#20010;&#36755;&#20837;&#30340;&#27169;&#22411;&#65292;&#25512;&#23548;&#20986;&#19968;&#31181;&#36866;&#29992;&#20110;Siamese&#32534;&#30721;&#22120;&#30340;&#23616;&#37096;&#24402;&#22240;&#26041;&#27861;&#12290;&#35813;&#35299;&#20915;&#26041;&#26696;&#37319;&#29992;&#29305;&#24449;&#23545;&#24402;&#22240;&#30340;&#24418;&#24335;&#65292;&#24182;&#21487;&#23558;&#20854;&#31616;&#21270;&#20026;&#21477;&#23376;&#36716;&#25442;&#22120;&#30340;&#20196;&#29260;-&#20196;&#29260;&#30697;&#38453;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#24341;&#20837;&#38598;&#25104;&#38597;&#21487;&#27604;&#30697;&#38453;&#65292;&#24182;&#32487;&#25215;&#20102;&#38598;&#25104;&#26799;&#24230;&#30340;&#20248;&#21183;&#24418;&#24335;&#29305;&#24615;&#65306;&#23427;&#32771;&#34385;&#20102;&#27169;&#22411;&#30340;&#23436;&#25972;&#35745;&#31639;&#22270;&#65292;&#24182;&#30830;&#20445;&#25910;&#25947;&#21040;&#23454;&#38469;&#39044;&#27979;&#32467;&#26524;&#12290;&#19968;&#39033;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#21477;&#23376;&#36716;&#25442;&#22120;&#20013;&#65292;&#24456;&#23569;&#30340;&#20196;&#29260;&#23545;&#24448;&#24448;&#21487;&#20197;&#35299;&#37322;&#22823;&#37096;&#20998;&#30340;&#39044;&#27979;&#65292;&#24182;&#19988;&#23427;&#20204;&#20027;&#35201;&#38598;&#20013;&#22312;&#21517;&#35789;&#21644;&#21160;&#35789;&#19978;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#33719;&#24471;&#20934;&#30830;&#30340;&#39044;&#27979;&#65292;&#23427;&#38656;&#35201;&#20851;&#27880;&#22823;&#22810;&#25968;&#30340;&#20196;&#29260;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the success of Siamese encoder models such as sentence transformers (ST), little is known about the aspects of inputs they pay attention to. A barrier is that their predictions cannot be attributed to individual features, as they compare two inputs rather than processing a single one. This paper derives a local attribution method for Siamese encoders by generalizing the principle of integrated gradients to models with multiple inputs. The solution takes the form of feature-pair attributions, and can be reduced to a token-token matrix for STs. Our method involves the introduction of integrated Jacobians and inherits the advantageous formal properties of integrated gradients: it accounts for the model's full computation graph and is guaranteed to converge to the actual prediction. A pilot study shows that in an ST few token-pairs can often explain large fractions of predictions, and it focuses on nouns and verbs. For accurate predictions, it however needs to attend to the majorit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;SAMA&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#38544;&#24335;&#24494;&#20998;&#31639;&#27861;&#21644;&#31995;&#32479;&#36827;&#23637;&#65292;&#20351;&#24471;&#20803;&#23398;&#20064;&#20855;&#26377;&#21487;&#25193;&#23637;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;SAMA&#22312;&#22522;&#20110;&#20803;&#23398;&#20064;&#30340;&#31243;&#24207;&#20013;&#28789;&#27963;&#25903;&#25345;&#21508;&#31181;&#33258;&#36866;&#24212;&#20248;&#21270;&#22120;&#65292;&#21516;&#26102;&#36890;&#36807;&#36991;&#20813;&#26174;&#24335;&#35745;&#31639;&#20108;&#38454;&#26799;&#24230;&#20449;&#24687;&#21644;&#21033;&#29992;&#39640;&#25928;&#30340;&#20998;&#24067;&#24335;&#35757;&#32451;&#25216;&#26415;&#38477;&#20302;&#35745;&#31639;&#36127;&#25285;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;SAMA&#22312;&#22810;&#20010;&#22823;&#35268;&#27169;&#20803;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#20013;&#23637;&#31034;&#20102;&#26174;&#33879;&#30340;&#21534;&#21520;&#37327;&#25552;&#21319;&#21644;&#20869;&#23384;&#28040;&#32791;&#20943;&#23569;&#12290;</title><link>http://arxiv.org/abs/2310.05674</link><description>&lt;p&gt;
&#23454;&#29616;&#21487;&#25193;&#23637;&#30340;&#20803;&#23398;&#20064;&#30340;&#21487;&#34892;&#24615;
&lt;/p&gt;
&lt;p&gt;
Making Scalable Meta Learning Practical. (arXiv:2310.05674v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05674
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;SAMA&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#38544;&#24335;&#24494;&#20998;&#31639;&#27861;&#21644;&#31995;&#32479;&#36827;&#23637;&#65292;&#20351;&#24471;&#20803;&#23398;&#20064;&#20855;&#26377;&#21487;&#25193;&#23637;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;SAMA&#22312;&#22522;&#20110;&#20803;&#23398;&#20064;&#30340;&#31243;&#24207;&#20013;&#28789;&#27963;&#25903;&#25345;&#21508;&#31181;&#33258;&#36866;&#24212;&#20248;&#21270;&#22120;&#65292;&#21516;&#26102;&#36890;&#36807;&#36991;&#20813;&#26174;&#24335;&#35745;&#31639;&#20108;&#38454;&#26799;&#24230;&#20449;&#24687;&#21644;&#21033;&#29992;&#39640;&#25928;&#30340;&#20998;&#24067;&#24335;&#35757;&#32451;&#25216;&#26415;&#38477;&#20302;&#35745;&#31639;&#36127;&#25285;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;SAMA&#22312;&#22810;&#20010;&#22823;&#35268;&#27169;&#20803;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#20013;&#23637;&#31034;&#20102;&#26174;&#33879;&#30340;&#21534;&#21520;&#37327;&#25552;&#21319;&#21644;&#20869;&#23384;&#28040;&#32791;&#20943;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20803;&#23398;&#20064;&#65288;&#21363;&#23398;&#20250;&#23398;&#20064;&#65289;&#22312;&#26426;&#22120;&#23398;&#20064;&#31243;&#24207;&#20013;&#23398;&#20064;&#22810;&#26679;&#30340;&#24402;&#32435;&#20559;&#32622;&#26041;&#38754;&#38750;&#24120;&#28789;&#27963;&#65292;&#20294;&#30001;&#20110;&#35745;&#31639;/&#20869;&#23384;&#24320;&#38144;&#24040;&#22823;&#12289;&#35757;&#32451;&#19981;&#31283;&#23450;&#21644;&#32570;&#20047;&#26377;&#25928;&#30340;&#20998;&#24067;&#24335;&#35757;&#32451;&#25903;&#25345;&#65292;&#23427;&#38271;&#26399;&#20197;&#26469;&#19968;&#30452;&#34987;&#35748;&#20026;&#19981;&#20855;&#26377;&#33391;&#22909;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#36890;&#36807;&#24341;&#20837;SAMA&#65292;&#23558;&#38544;&#24335;&#24494;&#20998;&#31639;&#27861;&#21644;&#31995;&#32479;&#30340;&#36827;&#23637;&#30456;&#32467;&#21512;&#65292;&#20174;&#32780;&#20351;&#21487;&#25193;&#23637;&#30340;&#20803;&#23398;&#20064;&#20855;&#26377;&#23454;&#29992;&#24615;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;SAMA&#26088;&#22312;&#28789;&#27963;&#25903;&#25345;&#20803;&#23398;&#20064;&#31243;&#24207;&#30340;&#22522;&#26412;&#32423;&#21035;&#20013;&#36866;&#24212;&#24615;&#20248;&#21270;&#22120;&#30340;&#24191;&#27867;&#33539;&#22260;&#65292;&#21516;&#26102;&#36890;&#36807;&#36991;&#20813;&#26174;&#24335;&#35745;&#31639;&#20108;&#38454;&#26799;&#24230;&#20449;&#24687;&#21644;&#21033;&#29992;&#20026;&#19968;&#38454;&#26799;&#24230;&#23454;&#29616;&#30340;&#39640;&#25928;&#20998;&#24067;&#24335;&#35757;&#32451;&#25216;&#26415;&#26469;&#38477;&#20302;&#35745;&#31639;&#36127;&#25285;&#12290;&#22312;&#22810;&#20010;&#22823;&#35268;&#27169;&#20803;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#20013;&#36827;&#34892;&#35780;&#20272;&#26102;&#65292;SAMA&#22312;&#21333;&#20010;/&#22810;&#20010;GPU&#19978;&#20998;&#21035;&#23637;&#31034;&#20102;&#39640;&#36798;1.7 / 4.8&#20493;&#30340;&#21534;&#21520;&#37327;&#22686;&#21152;&#21644;2.0 / 3.8&#20493;&#30340;&#20869;&#23384;&#28040;&#32791;&#20943;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite its flexibility to learn diverse inductive biases in machine learning programs, meta learning (i.e., learning to learn) has long been recognized to suffer from poor scalability due to its tremendous compute/memory costs, training instability, and a lack of efficient distributed training support. In this work, we focus on making scalable meta learning practical by introducing SAMA, which combines advances in both implicit differentiation algorithms and systems. Specifically, SAMA is designed to flexibly support a broad range of adaptive optimizers in the base level of meta learning programs, while reducing computational burden by avoiding explicit computation of second-order gradient information, and exploiting efficient distributed training techniques implemented for first-order gradients. Evaluated on multiple large-scale meta learning benchmarks, SAMA showcases up to 1.7/4.8x increase in throughput and 2.0/3.8x decrease in memory consumption respectively on single-/multi-GPU 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;InterroLang&#30340;&#23545;&#35805;&#24335;&#35299;&#37322;&#24037;&#20855;&#65292;&#36890;&#36807;&#23545;&#35805;&#30028;&#38754;&#24110;&#21161;&#29992;&#25143;&#20197;&#24773;&#22659;&#21270;&#30340;&#26041;&#24335;&#25506;&#32034;&#20855;&#26377;&#35299;&#37322;&#30340;NLP&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#65292;&#25903;&#25345;&#28548;&#28165;&#21644;&#21518;&#32493;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25509;&#21475;&#36827;&#34892;&#20132;&#20114;&#12290;&#30740;&#31350;&#20013;&#36824;&#25552;&#20986;&#20102;&#26032;&#30340;NLP&#25805;&#20316;&#65292;&#24182;&#22312;&#19977;&#20010;NLP&#20219;&#21153;&#19978;&#23637;&#31034;&#20102;&#20854;&#27867;&#21270;&#33021;&#21147;&#12290;&#36827;&#34892;&#20102;&#29992;&#25143;&#30740;&#31350;&#35780;&#20272;&#24037;&#20855;&#30340;&#27491;&#30830;&#24615;&#12289;&#26377;&#29992;&#24615;&#21644;&#21487;&#27169;&#25311;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.05592</link><description>&lt;p&gt;
InterroLang: &#36890;&#36807;&#23545;&#35805;&#24335;&#35299;&#37322;&#25506;&#32034;NLP&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
InterroLang: Exploring NLP Models and Datasets through Dialogue-based Explanations. (arXiv:2310.05592v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05592
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;InterroLang&#30340;&#23545;&#35805;&#24335;&#35299;&#37322;&#24037;&#20855;&#65292;&#36890;&#36807;&#23545;&#35805;&#30028;&#38754;&#24110;&#21161;&#29992;&#25143;&#20197;&#24773;&#22659;&#21270;&#30340;&#26041;&#24335;&#25506;&#32034;&#20855;&#26377;&#35299;&#37322;&#30340;NLP&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#65292;&#25903;&#25345;&#28548;&#28165;&#21644;&#21518;&#32493;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25509;&#21475;&#36827;&#34892;&#20132;&#20114;&#12290;&#30740;&#31350;&#20013;&#36824;&#25552;&#20986;&#20102;&#26032;&#30340;NLP&#25805;&#20316;&#65292;&#24182;&#22312;&#19977;&#20010;NLP&#20219;&#21153;&#19978;&#23637;&#31034;&#20102;&#20854;&#27867;&#21270;&#33021;&#21147;&#12290;&#36827;&#34892;&#20102;&#29992;&#25143;&#30740;&#31350;&#35780;&#20272;&#24037;&#20855;&#30340;&#27491;&#30830;&#24615;&#12289;&#26377;&#29992;&#24615;&#21644;&#21487;&#27169;&#25311;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#21457;&#23637;&#30340;NLP&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#20197;&#21508;&#31181;&#26041;&#24335;&#25171;&#24320;&#20102;&#40657;&#31665;&#65292;&#20294;&#26159;&#22312;&#36825;&#19968;&#21162;&#21147;&#20013;&#32570;&#23569;&#30340;&#26159;&#25552;&#20379;&#23545;&#35805;&#30028;&#38754;&#30340;&#20132;&#20114;&#24037;&#20855;&#12290;&#36825;&#26679;&#30340;&#23545;&#35805;&#31995;&#32479;&#21487;&#20197;&#24110;&#21161;&#29992;&#25143;&#20197;&#24773;&#22659;&#21270;&#30340;&#26041;&#24335;&#36890;&#36807;&#28548;&#28165;&#25110;&#21518;&#32493;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25509;&#21475;&#25506;&#32034;&#20855;&#26377;&#35299;&#37322;&#30340;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#12290;&#25105;&#20204;&#23558;&#23545;&#35805;&#24335;&#35299;&#37322;&#26694;&#26550;TalkToModel&#65288;Slack&#31561;&#65292;2022&#65289;&#25913;&#32534;&#20026;NLP&#39046;&#22495;&#65292;&#22686;&#21152;&#20102;&#33258;&#30001;&#25991;&#26412;&#35299;&#37322;&#31561;&#26032;&#30340;NLP&#29305;&#23450;&#25805;&#20316;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#22312;&#19977;&#20010;NLP&#20219;&#21153;&#65288;&#23545;&#35805;&#34892;&#20026;&#20998;&#31867;&#65292;&#38382;&#31572;&#65292;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#65289;&#19978;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#20026;&#20102;&#35782;&#21035;&#29992;&#25143;&#30340;&#35299;&#37322;&#26597;&#35810;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#31934;&#35843;&#21644;&#23569;&#26679;&#26412;&#25552;&#31034;&#27169;&#22411;&#65292;&#24182;&#23454;&#26045;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#36866;&#37197;&#22120;&#30340;&#26041;&#27861;&#12290;&#28982;&#21518;&#25105;&#20204;&#36827;&#34892;&#20102;&#20004;&#20010;&#29992;&#25143;&#30740;&#31350;&#65306;&#65288;1&#65289;&#35780;&#20272;&#23545;&#35805;&#30340;&#27491;&#30830;&#24615;&#21644;&#26377;&#29992;&#24615;&#65292;&#65288;2&#65289;&#21487;&#27169;&#25311;&#24615;&#65292;&#21363;&#22914;&#20309;&#23545;&#35937;&#21270;&#29992;&#25143;&#23545;&#35805;&#12290;
&lt;/p&gt;
&lt;p&gt;
While recently developed NLP explainability methods let us open the black box in various ways (Madsen et al., 2022), a missing ingredient in this endeavor is an interactive tool offering a conversational interface. Such a dialogue system can help users explore datasets and models with explanations in a contextualized manner, e.g. via clarification or follow-up questions, and through a natural language interface. We adapt the conversational explanation framework TalkToModel (Slack et al., 2022) to the NLP domain, add new NLP-specific operations such as free-text rationalization, and illustrate its generalizability on three NLP tasks (dialogue act classification, question answering, hate speech detection). To recognize user queries for explanations, we evaluate fine-tuned and few-shot prompting models and implement a novel Adapter-based approach. We then conduct two user studies on (1) the perceived correctness and helpfulness of the dialogues, and (2) the simulatability, i.e. how object
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20219;&#21153;&#33258;&#36866;&#24212;&#20998;&#35789;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#20998;&#35789;&#36807;&#31243;&#26469;&#22686;&#24378;&#22312;&#24515;&#29702;&#20581;&#24247;&#39046;&#22495;&#20013;&#30340;&#38271;&#25991;&#26412;&#29983;&#25104;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#20943;&#23569;&#26631;&#35760;&#25968;&#37327;&#30340;&#24773;&#20917;&#19979;&#26174;&#33879;&#25552;&#39640;&#20102;&#29983;&#25104;&#24615;&#33021;&#65292;&#24182;&#19988;&#21487;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32467;&#21512;&#20351;&#29992;&#12290;</title><link>http://arxiv.org/abs/2310.05317</link><description>&lt;p&gt;
&#22312;&#24515;&#29702;&#20581;&#24247;&#39046;&#22495;&#20013;&#36890;&#36807;&#20219;&#21153;&#33258;&#36866;&#24212;&#20998;&#35789;&#26469;&#22686;&#24378;&#38271;&#25991;&#26412;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Enhancing Long-form Text Generation in Mental Health with Task-adaptive Tokenization. (arXiv:2310.05317v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05317
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20219;&#21153;&#33258;&#36866;&#24212;&#20998;&#35789;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#20998;&#35789;&#36807;&#31243;&#26469;&#22686;&#24378;&#22312;&#24515;&#29702;&#20581;&#24247;&#39046;&#22495;&#20013;&#30340;&#38271;&#25991;&#26412;&#29983;&#25104;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#20943;&#23569;&#26631;&#35760;&#25968;&#37327;&#30340;&#24773;&#20917;&#19979;&#26174;&#33879;&#25552;&#39640;&#20102;&#29983;&#25104;&#24615;&#33021;&#65292;&#24182;&#19988;&#21487;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32467;&#21512;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#20219;&#21153;&#33258;&#36866;&#24212;&#20998;&#35789;&#20316;&#20026;&#19968;&#31181;&#26041;&#24335;&#65292;&#23558;&#29983;&#25104;&#27969;&#27700;&#32447;&#36866;&#24212;&#20110;&#19979;&#28216;&#20219;&#21153;&#30340;&#29305;&#23450;&#35201;&#27714;&#65292;&#24182;&#22686;&#24378;&#22312;&#24515;&#29702;&#20581;&#24247;&#39046;&#22495;&#30340;&#38271;&#25991;&#26412;&#29983;&#25104;&#12290;&#21463;&#35748;&#30693;&#31185;&#23398;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#30340;&#20219;&#21153;&#33258;&#36866;&#24212;&#20998;&#35789;&#22120;&#20174;&#22810;&#20010;&#32467;&#26524;&#20013;&#37319;&#26679;&#21487;&#21464;&#30340;&#20998;&#27573;&#65292;&#37319;&#26679;&#27010;&#29575;&#22522;&#20110;&#20219;&#21153;&#29305;&#23450;&#30340;&#25968;&#25454;&#36827;&#34892;&#20248;&#21270;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26500;&#24314;&#19987;&#29992;&#35789;&#27719;&#30340;&#31574;&#30053;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#31181;&#35789;&#27719;&#21512;&#24182;&#21327;&#35758;&#65292;&#21487;&#20197;&#23558;&#20219;&#21153;&#29305;&#23450;&#30340;&#26631;&#35760;&#25972;&#21512;&#21040;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#20998;&#35789;&#27493;&#39588;&#20013;&#12290;&#36890;&#36807;&#23545;&#20013;&#33521;&#25991;&#24515;&#29702;&#38382;&#31572;&#20219;&#21153;&#36827;&#34892;&#24191;&#27867;&#23454;&#39564;&#65292;&#25105;&#20204;&#21457;&#29616;&#25105;&#20204;&#30340;&#20219;&#21153;&#33258;&#36866;&#24212;&#20998;&#35789;&#26041;&#27861;&#22312;&#20351;&#29992;&#26356;&#23569;&#30340;&#26631;&#35760;&#30340;&#24773;&#20917;&#19979;&#24102;&#26469;&#20102;&#26174;&#33879;&#30340;&#29983;&#25104;&#24615;&#33021;&#25552;&#21319;&#65292;&#26368;&#39640;&#21487;&#36798;60%&#12290;&#21021;&#27493;&#23454;&#39564;&#34920;&#26126;&#65292;&#20351;&#29992;&#25105;&#20204;&#30340;&#20998;&#35789;&#26041;&#27861;&#19982;&#38750;&#24120;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#32467;&#21512;&#33021;&#22815;&#24471;&#21040;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose task-adaptive tokenization as a way to adapt the generation pipeline to the specifics of a downstream task and enhance long-form generation in mental health. Inspired by insights from cognitive science, our task-adaptive tokenizer samples variable segmentations from multiple outcomes, with sampling probabilities optimized based on task-specific data. We introduce a strategy for building a specialized vocabulary and introduce a vocabulary merging protocol that allows for the integration of task-specific tokens into the pre-trained model's tokenization step. Through extensive experiments on psychological question-answering tasks in both Chinese and English, we find that our task-adaptive tokenization approach brings a significant improvement in generation performance while using up to 60% fewer tokens. Preliminary experiments point to promising results when using our tokenization approach with very large language models.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#35780;&#20272;&#20102;&#23545;&#35805;&#31995;&#32479;&#20013;&#30340;&#20154;&#26684;&#20559;&#35265;&#23545;&#31038;&#20132;&#20559;&#35265;&#30340;&#24433;&#21709;&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#20010;&#32508;&#21512;&#35780;&#20272;&#26694;&#26550;&#26469;&#34913;&#37327;&#19981;&#21516;&#20154;&#26684;&#37319;&#29992;&#19979;&#30340;&#20559;&#35265;&#31243;&#24230;&#12290;</title><link>http://arxiv.org/abs/2310.05280</link><description>&lt;p&gt;
&#20010;&#24615;&#21270;&#38543;&#26426;&#40550;&#40521;&#26356;&#21361;&#38505;&#21527;&#65311;&#35780;&#20272;&#23545;&#35805;&#31995;&#32479;&#20013;&#30340;&#20154;&#26684;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Are Personalized Stochastic Parrots More Dangerous? Evaluating Persona Biases in Dialogue Systems. (arXiv:2310.05280v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05280
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#35780;&#20272;&#20102;&#23545;&#35805;&#31995;&#32479;&#20013;&#30340;&#20154;&#26684;&#20559;&#35265;&#23545;&#31038;&#20132;&#20559;&#35265;&#30340;&#24433;&#21709;&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#20010;&#32508;&#21512;&#35780;&#20272;&#26694;&#26550;&#26469;&#34913;&#37327;&#19981;&#21516;&#20154;&#26684;&#37319;&#29992;&#19979;&#30340;&#20559;&#35265;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21457;&#23637;&#20351;&#20854;&#33021;&#22815;&#25353;&#29031;&#33258;&#30001;&#24418;&#24335;&#30340;&#25351;&#20196;&#36827;&#34892;&#25805;&#20316;&#65292;&#21253;&#25324;&#22312;&#23545;&#35805;&#20013;&#27169;&#20223;&#36890;&#29992;&#25110;&#29305;&#23450;&#20154;&#21475;&#32676;&#20307;&#30340;&#20154;&#26684;&#12290;&#36890;&#29992;&#20154;&#26684;&#25351;&#30340;&#26159;&#26469;&#33258;&#26576;&#19968;&#20154;&#21475;&#32676;&#20307;&#30340;&#20010;&#20307;&#65288;&#20363;&#22914;&#20122;&#27954;&#20154;&#65289;&#65292;&#32780;&#29305;&#23450;&#20154;&#26684;&#21487;&#20197;&#26159;&#21382;&#21490;&#20154;&#29289;&#30340;&#23454;&#38469;&#22995;&#21517;&#12290;&#34429;&#28982;&#37319;&#29992;&#20154;&#26684;&#20351;&#23545;&#35805;&#31995;&#32479;&#26356;&#20855;&#21560;&#24341;&#21147;&#21644;&#20146;&#21644;&#21147;&#65292;&#20294;&#20063;&#23384;&#22312;&#28508;&#22312;&#39118;&#38505;&#65292;&#21487;&#33021;&#36890;&#36807;&#19982;&#29992;&#25143;&#30340;&#20132;&#20114;&#32780;&#21152;&#21095;&#31038;&#20250;&#20559;&#35265;&#65292;&#36827;&#19968;&#27493;&#36896;&#25104;&#31038;&#20250;&#20260;&#23475;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#30740;&#31350;&#8220;&#20154;&#26684;&#20559;&#35265;&#8221;&#65292;&#25105;&#20204;&#23558;&#20854;&#23450;&#20041;&#20026;&#26377;&#23475;&#23545;&#35805;&#27169;&#22411;&#34892;&#20026;&#23545;&#19981;&#21516;&#20154;&#26684;&#37319;&#29992;&#30340;&#25935;&#24863;&#24615;&#12290;&#25105;&#20204;&#23558;&#20154;&#26684;&#20559;&#35265;&#20998;&#20026;&#26377;&#23475;&#34920;&#36798;&#21644;&#26377;&#23475;&#35748;&#21516;&#20004;&#31867;&#65292;&#21516;&#26102;&#24314;&#31435;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#20197;&#34913;&#37327;&#20116;&#20010;&#26041;&#38754;&#30340;&#20154;&#26684;&#20559;&#35265;&#65306;&#20882;&#29359;&#24615;&#12289;&#26377;&#27602;&#24310;&#32493;&#12289;&#20851;&#24576;&#12289;&#21051;&#26495;&#21360;&#35937;&#30340;&#35748;&#21516;&#20197;&#21450;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in Large Language Models empower them to follow freeform instructions, including imitating generic or specific demographic personas in conversations. Generic personas refer to an individual from a demographic group (e.g. an Asian person), whereas specific personas can be actual names of historical figures. While the adoption of personas allows dialogue systems to be more engaging and approachable to users, it also carries the potential risk of exacerbating social biases in model responses, further causing societal harms through interactions with users. In this paper, we systematically study "persona biases", which we define to be the sensitivity of harmful dialogue model behaviors to different persona adoptions. We categorize persona biases into biases in harmful expression and harmful agreement, as well as establish a comprehensive evaluation framework to measure persona biases in five aspects: Offensiveness, Toxic Continuation, Regard, Stereotype Agreement, and To
&lt;/p&gt;</description></item><item><title>DialCoT&#26159;&#19968;&#31181;&#23545;&#35805;&#24341;&#23548;&#30340;&#38142;&#24335;&#24605;&#32500;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#36739;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#20998;&#35299;&#21644;&#25506;&#32034;&#25512;&#29702;&#36335;&#24452;&#12290;&#36890;&#36807;&#23558;&#22797;&#26434;&#38382;&#39064;&#20998;&#35299;&#20026;&#31616;&#21333;&#30340;&#23376;&#38382;&#39064;&#65292;&#23427;&#38477;&#20302;&#20102;&#20219;&#21153;&#38590;&#24230;&#65292;&#24182;&#20351;&#29992;PPO&#31639;&#27861;&#20248;&#21270;&#27169;&#22411;&#30340;&#25512;&#29702;&#36335;&#24452;&#36873;&#25321;&#12290;</title><link>http://arxiv.org/abs/2310.05074</link><description>&lt;p&gt;
DialCoT&#36935;&#21040;&#20102;PPO&#65306;&#22312;&#36739;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#20998;&#35299;&#21644;&#25506;&#32034;&#25512;&#29702;&#36335;&#24452;
&lt;/p&gt;
&lt;p&gt;
DialCoT Meets PPO: Decomposing and Exploring Reasoning Paths in Smaller Language Models. (arXiv:2310.05074v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05074
&lt;/p&gt;
&lt;p&gt;
DialCoT&#26159;&#19968;&#31181;&#23545;&#35805;&#24341;&#23548;&#30340;&#38142;&#24335;&#24605;&#32500;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#36739;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#20998;&#35299;&#21644;&#25506;&#32034;&#25512;&#29702;&#36335;&#24452;&#12290;&#36890;&#36807;&#23558;&#22797;&#26434;&#38382;&#39064;&#20998;&#35299;&#20026;&#31616;&#21333;&#30340;&#23376;&#38382;&#39064;&#65292;&#23427;&#38477;&#20302;&#20102;&#20219;&#21153;&#38590;&#24230;&#65292;&#24182;&#20351;&#29992;PPO&#31639;&#27861;&#20248;&#21270;&#27169;&#22411;&#30340;&#25512;&#29702;&#36335;&#24452;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38142;&#24335;&#24605;&#32500;&#65288;CoT&#65289;&#25552;&#31034;&#24050;&#32463;&#34987;&#35777;&#26126;&#26377;&#21161;&#20110;&#22686;&#24378;&#33267;&#23569;&#20855;&#26377;1000&#20159;&#21442;&#25968;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#24403;&#24212;&#29992;&#20110;&#20855;&#26377;&#19981;&#21040;100&#20159;&#21442;&#25968;&#30340;&#36739;&#23567;&#35821;&#35328;&#27169;&#22411;&#65288;SLMs&#65289;&#30340;&#25512;&#29702;&#20219;&#21153;&#26102;&#65292;&#23427;&#26159;&#26080;&#25928;&#29978;&#33267;&#26377;&#23475;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#23545;&#35805;&#24341;&#23548;&#30340;&#38142;&#24335;&#24605;&#32500;&#65288;DialCoT&#65289;&#65292;&#23427;&#37319;&#29992;&#23545;&#35805;&#26684;&#24335;&#29983;&#25104;&#20013;&#38388;&#25512;&#29702;&#27493;&#39588;&#65292;&#24341;&#23548;&#27169;&#22411;&#26397;&#30528;&#26368;&#32456;&#31572;&#26696;&#21069;&#36827;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;PPO&#31639;&#27861;&#20248;&#21270;&#27169;&#22411;&#30340;&#25512;&#29702;&#36335;&#24452;&#36873;&#25321;&#65292;&#36827;&#19968;&#27493;&#22686;&#24378;&#20854;&#25512;&#29702;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#20197;&#21069;&#30340;&#26041;&#27861;&#30456;&#27604;&#20855;&#26377;&#20960;&#20010;&#20248;&#28857;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;&#35299;&#20915;&#22797;&#26434;&#25512;&#29702;&#38382;&#39064;&#30340;&#36807;&#31243;&#20998;&#35299;&#25104;&#19968;&#31995;&#21015;&#26356;&#31616;&#21333;&#30340;&#23376;&#38382;&#39064;&#65292;&#26174;&#33879;&#38477;&#20302;&#20102;&#20219;&#21153;&#30340;&#38590;&#24230;&#65292;&#20351;&#20854;&#26356;&#36866;&#21512;&#20110;&#36739;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#20248;&#21270;&#20102;&#27169;&#22411;&#30340;&#25512;&#29702;&#36335;&#24452;&#36873;&#25321;&#65292;&#20351;&#20854;&#26356;&#20934;&#30830;&#21644;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
Chain-of-Thought (CoT) prompting has proven to be effective in enhancing the reasoning capabilities of Large Language Models (LLMs) with at least 100 billion parameters. However, it is ineffective or even detrimental when applied to reasoning tasks in Smaller Language Models (SLMs) with less than 10 billion parameters. To address this limitation, we introduce Dialogue-guided Chain-of-Thought (DialCoT) which employs a dialogue format to generate intermediate reasoning steps, guiding the model toward the final answer. Additionally, we optimize the model's reasoning path selection using the Proximal Policy Optimization (PPO) algorithm, further enhancing its reasoning capabilities. Our method offers several advantages compared to previous approaches. Firstly, we transform the process of solving complex reasoning questions by breaking them down into a series of simpler sub-questions, significantly reducing the task difficulty and making it more suitable for SLMs. Secondly, we optimize the m
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#24187;&#35273;&#30340;&#38382;&#39064;&#36827;&#34892;&#20102;&#32454;&#33268;&#30340;&#23450;&#20041;&#21644;&#37327;&#21270;&#65292;&#24182;&#25552;&#20986;&#20102;&#35299;&#20915;&#26041;&#27861;&#12290;&#35770;&#25991;&#23558;&#24187;&#35273;&#20998;&#20026;&#20004;&#31181;&#24635;&#20307;&#20542;&#21521;&#65292;&#24182;&#36827;&#19968;&#27493;&#32454;&#20998;&#20026;&#20869;&#22312;&#21644;&#22806;&#22312;&#20004;&#31181;&#12290;&#35770;&#25991;&#36824;&#23545;&#24187;&#35273;&#36827;&#34892;&#20102;&#19977;&#20010;&#31243;&#24230;&#30340;&#20005;&#37325;&#24615;&#20998;&#31867;&#12290;</title><link>http://arxiv.org/abs/2310.04988</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#24187;&#35273;&#30340;&#20196;&#20154;&#25285;&#24551;&#30340;&#20986;&#29616;&#8212;&#8212;&#24191;&#27867;&#23450;&#20041;&#12289;&#37327;&#21270;&#21644;&#35268;&#23450;&#30699;&#27491;&#25514;&#26045;
&lt;/p&gt;
&lt;p&gt;
The Troubling Emergence of Hallucination in Large Language Models -- An Extensive Definition, Quantification, and Prescriptive Remediations. (arXiv:2310.04988v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04988
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#24187;&#35273;&#30340;&#38382;&#39064;&#36827;&#34892;&#20102;&#32454;&#33268;&#30340;&#23450;&#20041;&#21644;&#37327;&#21270;&#65292;&#24182;&#25552;&#20986;&#20102;&#35299;&#20915;&#26041;&#27861;&#12290;&#35770;&#25991;&#23558;&#24187;&#35273;&#20998;&#20026;&#20004;&#31181;&#24635;&#20307;&#20542;&#21521;&#65292;&#24182;&#36827;&#19968;&#27493;&#32454;&#20998;&#20026;&#20869;&#22312;&#21644;&#22806;&#22312;&#20004;&#31181;&#12290;&#35770;&#25991;&#36824;&#23545;&#24187;&#35273;&#36827;&#34892;&#20102;&#19977;&#20010;&#31243;&#24230;&#30340;&#20005;&#37325;&#24615;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#21457;&#23637;&#24341;&#36215;&#20102;&#24191;&#27867;&#36190;&#35465;&#65292;&#22240;&#20026;&#23427;&#20204;&#20855;&#26377;&#21331;&#36234;&#30340;&#26032;&#20852;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#24187;&#35273;&#38382;&#39064;&#20063;&#21516;&#26102;&#20986;&#29616;&#65292;&#24182;&#24341;&#21457;&#20102;&#37325;&#22823;&#20851;&#20999;&#12290;&#34429;&#28982;&#26368;&#36817;&#26377;&#19968;&#20123;&#21162;&#21147;&#26469;&#35782;&#21035;&#21644;&#20943;&#36731;&#19981;&#21516;&#31867;&#22411;&#30340;&#24187;&#35273;&#65292;&#20294;&#23545;&#24187;&#35273;&#30340;&#32454;&#33268;&#20998;&#31867;&#21644;&#30456;&#20851;&#32531;&#35299;&#26041;&#27861;&#30340;&#37325;&#35270;&#26377;&#38480;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20221;&#20851;&#20110;&#24187;&#35273;&#26681;&#25454;&#20854;&#31243;&#24230;&#12289;&#20542;&#21521;&#21644;&#31867;&#21035;&#30340;&#31934;&#32454;&#21270;&#35752;&#35770;&#65292;&#24182;&#25552;&#20379;&#20102;&#32531;&#35299;&#31574;&#30053;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#20004;&#20010;&#24635;&#20307;&#20542;&#21521;&#30340;&#24187;&#35273;&#65306;&#65288;i&#65289;&#20107;&#23454;&#19978;&#30340;&#28023;&#24066;&#34563;&#27004;&#65288;FM&#65289;&#21644;&#65288;ii&#65289;&#38134;&#20809;&#65288;SL&#65289;&#12290;&#20026;&#20102;&#25552;&#20379;&#26356;&#20840;&#38754;&#30340;&#29702;&#35299;&#65292;&#36825;&#20004;&#20010;&#20542;&#21521;&#36827;&#19968;&#27493;&#32454;&#20998;&#20026;&#20869;&#22312;&#21644;&#22806;&#22312;&#20004;&#31181;&#65292;&#24182;&#19988;&#26377;&#19977;&#20010;&#20005;&#37325;&#31243;&#24230;&#65306;&#65288;i&#65289;&#36731;&#24494;&#65292;&#65288;ii&#65289;&#20013;&#24230;&#21644;&#65288;iii&#65289;&#20196;&#20154;&#25285;&#24551;&#12290;&#25105;&#20204;&#36824;&#23545;&#24187;&#35273;&#36827;&#34892;&#20102;&#32454;&#33268;&#30340;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent advancements in Large Language Models (LLMs) have garnered widespread acclaim for their remarkable emerging capabilities. However, the issue of hallucination has parallelly emerged as a by-product, posing significant concerns. While some recent endeavors have been made to identify and mitigate different types of hallucination, there has been a limited emphasis on the nuanced categorization of hallucination and associated mitigation methods. To address this gap, we offer a fine-grained discourse on profiling hallucination based on its degree, orientation, and category, along with offering strategies for alleviation. As such, we define two overarching orientations of hallucination: (i) factual mirage (FM) and (ii) silver lining (SL). To provide a more comprehensive understanding, both orientations are further sub-categorized into intrinsic and extrinsic, with three degrees of severity - (i) mild, (ii) moderate, and (iii) alarming. We also meticulously categorize hallucination 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LoFT&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#19982;&#26377;&#23475;&#26597;&#35810;&#22788;&#20110;&#35789;&#27719;-&#35821;&#20041;&#37051;&#22495;&#30340;&#30456;&#20284;&#26597;&#35810;&#19978;&#36827;&#34892;&#20195;&#29702;&#27169;&#22411;&#30340;&#24494;&#35843;&#65292;&#26469;&#25913;&#21892;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23545;&#25239;&#25915;&#20987;&#30340;&#21487;&#20256;&#36882;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.04445</link><description>&lt;p&gt;
LoFT: &#29992;&#20110;&#25913;&#36827;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23545;&#25239;&#25915;&#20987;&#21487;&#20256;&#36882;&#24615;&#30340;&#26412;&#22320;&#20195;&#29702;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
LoFT: Local Proxy Fine-tuning For Improving Transferability Of Adversarial Attacks Against Large Language Model. (arXiv:2310.04445v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04445
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LoFT&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#19982;&#26377;&#23475;&#26597;&#35810;&#22788;&#20110;&#35789;&#27719;-&#35821;&#20041;&#37051;&#22495;&#30340;&#30456;&#20284;&#26597;&#35810;&#19978;&#36827;&#34892;&#20195;&#29702;&#27169;&#22411;&#30340;&#24494;&#35843;&#65292;&#26469;&#25913;&#21892;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23545;&#25239;&#25915;&#20987;&#30340;&#21487;&#20256;&#36882;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24050;&#32463;&#21457;&#29616;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#23545;&#40784;&#21487;&#20197;&#36890;&#36807;&#38468;&#21152;&#29305;&#21046;&#30340;&#25915;&#20987;&#21518;&#32512;&#21644;&#26377;&#23475;&#26597;&#35810;&#26469;&#35268;&#36991;&#65292;&#20197;&#24341;&#21457;&#26377;&#23475;&#30340;&#21709;&#24212;&#12290;&#20026;&#20102;&#23545;&#26410;&#30693;&#29305;&#24449;&#30340;&#31169;&#26377;&#30446;&#26631;&#27169;&#22411;&#36827;&#34892;&#25915;&#20987;&#65292;&#21487;&#20197;&#20351;&#29992;&#20844;&#20849;&#27169;&#22411;&#20316;&#20026;&#20195;&#29702;&#26469;&#26500;&#24314;&#25915;&#20987;&#65292;&#24182;&#23558;&#25104;&#21151;&#30340;&#25915;&#20987;&#20174;&#20844;&#20849;&#20195;&#29702;&#20256;&#36882;&#21040;&#31169;&#26377;&#30446;&#26631;&#27169;&#22411;&#12290;&#25915;&#20987;&#30340;&#25104;&#21151;&#29575;&#21462;&#20915;&#20110;&#20195;&#29702;&#27169;&#22411;&#33021;&#22815;&#22810;&#22823;&#31243;&#24230;&#19978;&#36924;&#36817;&#31169;&#26377;&#27169;&#22411;&#12290;&#25105;&#20204;&#20551;&#35774;&#65292;&#23545;&#20110;&#25915;&#20987;&#21487;&#20256;&#36882;&#24615;&#26469;&#35828;&#65292;&#21482;&#35201;&#20195;&#29702;&#33021;&#22815;&#22312;&#26377;&#23475;&#26597;&#35810;&#30340;&#35789;&#27719;-&#35821;&#20041;&#37051;&#22495;&#20869;&#36924;&#36817;&#30446;&#26631;&#27169;&#22411;&#21363;&#21487;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;&#26412;&#22320;&#24494;&#35843;&#65288;LoFT&#65289;&#8221;&#65292;&#21363;&#22312;&#19982;&#26377;&#23475;&#26597;&#35810;&#22788;&#20110;&#35789;&#27719;-&#35821;&#20041;&#37051;&#22495;&#30340;&#30456;&#20284;&#26597;&#35810;&#19978;&#36827;&#34892;&#20195;&#29702;&#27169;&#22411;&#30340;&#24494;&#35843;&#65292;&#20197;&#20943;&#23567;&#20195;&#29702;&#21644;&#30446;&#26631;&#27169;&#22411;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#28436;&#31034;&#20102;&#19977;&#31181;&#20419;&#20351;&#31169;&#26377;&#30446;&#26631;&#27169;&#22411;&#21464;&#24471;&#26131;&#21463;&#25915;&#20987;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
It has been shown that Large Language Model (LLM) alignments can be circumvented by appending specially crafted attack suffixes with harmful queries to elicit harmful responses. To conduct attacks against private target models whose characterization is unknown, public models can be used as proxies to fashion the attack, with successful attacks being transferred from public proxies to private target models. The success rate of attack depends on how closely the proxy model approximates the private model. We hypothesize that for attacks to be transferrable, it is sufficient if the proxy can approximate the target model in the neighborhood of the harmful query. Therefore, in this paper, we propose \emph{Local Fine-Tuning (LoFT)}, \textit{i.e.}, fine-tuning proxy models on similar queries that lie in the lexico-semantic neighborhood of harmful queries to decrease the divergence between the proxy and target models. First, we demonstrate three approaches to prompt private target models to obt
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#32508;&#21512;&#35780;&#20272;&#19981;&#21516;&#25915;&#20987;&#22330;&#26223;&#19979;&#30340;&#24494;&#35843;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31283;&#23450;&#30340;&#21518;&#38376;&#20928;&#21270;&#26041;&#27861;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#20302;&#27745;&#26579;&#29575;&#30340;&#24773;&#20917;&#19979;&#65292;&#21518;&#38376;&#21644;&#24178;&#20928;&#29305;&#24449;&#20043;&#38388;&#30340;&#32416;&#32544;&#20250;&#21066;&#24369;&#35843;&#25972;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.01875</link><description>&lt;p&gt;
&#36890;&#36807;&#29305;&#24449;&#28418;&#31227;&#35843;&#25972;&#23454;&#29616;&#31283;&#23450;&#30340;&#21518;&#38376;&#20928;&#21270;
&lt;/p&gt;
&lt;p&gt;
Towards Stable Backdoor Purification through Feature Shift Tuning. (arXiv:2310.01875v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01875
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#32508;&#21512;&#35780;&#20272;&#19981;&#21516;&#25915;&#20987;&#22330;&#26223;&#19979;&#30340;&#24494;&#35843;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31283;&#23450;&#30340;&#21518;&#38376;&#20928;&#21270;&#26041;&#27861;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#20302;&#27745;&#26579;&#29575;&#30340;&#24773;&#20917;&#19979;&#65292;&#21518;&#38376;&#21644;&#24178;&#20928;&#29305;&#24449;&#20043;&#38388;&#30340;&#32416;&#32544;&#20250;&#21066;&#24369;&#35843;&#25972;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#23481;&#26131;&#21463;&#21040;&#21518;&#38376;&#25915;&#20987;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#36890;&#36807;&#31713;&#25913;&#19968;&#23567;&#32452;&#35757;&#32451;&#26679;&#26412;&#26469;&#24694;&#24847;&#25805;&#25511;&#27169;&#22411;&#34892;&#20026;&#12290;&#34429;&#28982;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#38450;&#24481;&#26041;&#27861;&#26469;&#20943;&#36731;&#36825;&#31181;&#23041;&#32961;&#65292;&#20294;&#23427;&#20204;&#35201;&#20040;&#38656;&#35201;&#23545;&#35757;&#32451;&#36807;&#31243;&#36827;&#34892;&#22797;&#26434;&#20462;&#25913;&#65292;&#35201;&#20040;&#20005;&#37325;&#20381;&#36182;&#29305;&#23450;&#30340;&#27169;&#22411;&#26550;&#26500;&#65292;&#20351;&#24471;&#23427;&#20204;&#38590;&#20197;&#24212;&#29992;&#20110;&#29616;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20174;&#24494;&#35843;&#24320;&#22987;&#65292;&#36890;&#36807;&#23545;&#21508;&#31181;&#25915;&#20987;&#22330;&#26223;&#30340;&#20840;&#38754;&#35780;&#20272;&#26469;&#25506;&#32034;&#26368;&#24120;&#35265;&#21644;&#26131;&#20110;&#37096;&#32626;&#30340;&#21518;&#38376;&#38450;&#24481;&#26041;&#27861;&#12290;&#36890;&#36807;&#21021;&#27493;&#23454;&#39564;&#35266;&#23519;&#21457;&#29616;&#65292;&#19982;&#39640;&#27745;&#26579;&#29575;&#30340;&#26377;&#24076;&#26395;&#30340;&#38450;&#24481;&#32467;&#26524;&#30456;&#27604;&#65292;&#26222;&#36890;&#30340;&#35843;&#25972;&#26041;&#27861;&#22312;&#20302;&#27745;&#26579;&#29575;&#22330;&#26223;&#19979;&#23436;&#20840;&#22833;&#25928;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#22312;&#20302;&#27745;&#26579;&#29575;&#19979;&#65292;&#21518;&#38376;&#21644;&#24178;&#20928;&#29305;&#24449;&#20043;&#38388;&#30340;&#32416;&#32544;&#30772;&#22351;&#20102;&#22522;&#20110;&#35843;&#25972;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
It has been widely observed that deep neural networks (DNN) are vulnerable to backdoor attacks where attackers could manipulate the model behavior maliciously by tampering with a small set of training samples. Although a line of defense methods is proposed to mitigate this threat, they either require complicated modifications to the training process or heavily rely on the specific model architecture, which makes them hard to deploy into real-world applications. Therefore, in this paper, we instead start with fine-tuning, one of the most common and easy-to-deploy backdoor defenses, through comprehensive evaluations against diverse attack scenarios. Observations made through initial experiments show that in contrast to the promising defensive results on high poisoning rates, vanilla tuning methods completely fail at low poisoning rate scenarios. Our analysis shows that with the low poisoning rate, the entanglement between backdoor and clean features undermines the effect of tuning-based 
&lt;/p&gt;</description></item><item><title>LanguageBind&#25552;&#20986;&#20102;&#23558;&#35821;&#35328;&#20316;&#20026;&#19981;&#21516;&#27169;&#24577;&#20043;&#38388;&#32445;&#24102;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20923;&#32467;&#35270;&#39057;-&#35821;&#35328;&#39044;&#35757;&#32451;&#33719;&#21462;&#30340;&#35821;&#35328;&#32534;&#30721;&#22120;&#65292;&#24182;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#35757;&#32451;&#20854;&#20182;&#27169;&#24577;&#30340;&#32534;&#30721;&#22120;&#65292;&#23454;&#29616;&#20102;&#22810;&#27169;&#24577;&#30340;&#35821;&#20041;&#23545;&#40784;&#12290;&#27492;&#22806;&#65292;&#20316;&#32773;&#36824;&#25552;&#20986;&#20102;VIDAL-10M&#25968;&#25454;&#38598;&#26469;&#25903;&#25345;&#35813;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.01852</link><description>&lt;p&gt;
LanguageBind:&#36890;&#36807;&#22522;&#20110;&#35821;&#20041;&#23545;&#40784;&#30340;&#35821;&#35328;&#23558;&#35270;&#39057;-&#35821;&#35328;&#39044;&#35757;&#32451;&#25193;&#23637;&#21040;N&#27169;&#24577;&#65288;arXiv:2310.01852v1[cs.CV]&#65289;
&lt;/p&gt;
&lt;p&gt;
LanguageBind: Extending Video-Language Pretraining to N-modality by Language-based Semantic Alignment. (arXiv:2310.01852v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01852
&lt;/p&gt;
&lt;p&gt;
LanguageBind&#25552;&#20986;&#20102;&#23558;&#35821;&#35328;&#20316;&#20026;&#19981;&#21516;&#27169;&#24577;&#20043;&#38388;&#32445;&#24102;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20923;&#32467;&#35270;&#39057;-&#35821;&#35328;&#39044;&#35757;&#32451;&#33719;&#21462;&#30340;&#35821;&#35328;&#32534;&#30721;&#22120;&#65292;&#24182;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#35757;&#32451;&#20854;&#20182;&#27169;&#24577;&#30340;&#32534;&#30721;&#22120;&#65292;&#23454;&#29616;&#20102;&#22810;&#27169;&#24577;&#30340;&#35821;&#20041;&#23545;&#40784;&#12290;&#27492;&#22806;&#65292;&#20316;&#32773;&#36824;&#25552;&#20986;&#20102;VIDAL-10M&#25968;&#25454;&#38598;&#26469;&#25903;&#25345;&#35813;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#39057;-&#35821;&#35328;&#65288;VL&#65289;&#39044;&#35757;&#32451;&#22312;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;VL&#39044;&#35757;&#32451;&#26694;&#26550;&#38590;&#20197;&#23558;&#20854;&#25193;&#23637;&#21040;&#38500;&#35270;&#35273;&#21644;&#35821;&#35328;&#20043;&#22806;&#30340;&#22810;&#27169;&#24577;&#65288;N&#27169;&#24577;&#65292;N&gt;=3&#65289;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LanguageBind&#65292;&#36890;&#36807;&#23558;&#35821;&#35328;&#20316;&#20026;&#19981;&#21516;&#27169;&#24577;&#20043;&#38388;&#30340;&#32445;&#24102;&#65292;&#22240;&#20026;&#35821;&#35328;&#27169;&#24577;&#24050;&#32463;&#24471;&#21040;&#20102;&#24456;&#22909;&#30340;&#25506;&#32034;&#65292;&#21253;&#21547;&#20016;&#23500;&#30340;&#35821;&#20041;&#20449;&#24687;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20351;&#29992;VL&#39044;&#35757;&#32451;&#33719;&#21462;&#30340;&#35821;&#35328;&#32534;&#30721;&#22120;&#65292;&#24182;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#35757;&#32451;&#20854;&#20182;&#27169;&#24577;&#30340;&#32534;&#30721;&#22120;&#12290;&#32467;&#26524;&#26159;&#65292;&#25152;&#26377;&#27169;&#24577;&#34987;&#26144;&#23556;&#21040;&#19968;&#20010;&#20849;&#20139;&#30340;&#29305;&#24449;&#31354;&#38388;&#20013;&#65292;&#23454;&#29616;&#20102;&#22810;&#27169;&#24577;&#30340;&#35821;&#20041;&#23545;&#40784;&#12290;&#34429;&#28982;LanguageBind&#21487;&#20197;&#25193;&#23637;VL&#27169;&#24577;&#21040;N&#27169;&#24577;&#65292;&#20294;&#25105;&#20204;&#36824;&#38656;&#35201;&#19968;&#20010;&#24102;&#26377;&#20197;&#35821;&#35328;&#20026;&#20013;&#24515;&#30340;&#23545;&#40784;&#25968;&#25454;&#23545;&#30340;&#39640;&#36136;&#37327;&#25968;&#25454;&#38598;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;VIDAL-10M&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#35270;&#39057;&#12289;&#32418;&#22806;&#12289;&#28145;&#24230;&#12289;&#38899;&#39057;&#21450;&#20854;&#30456;&#24212;&#30340;&#35821;&#35328;&#25968;&#25454;&#65292;&#21629;&#21517;&#20026;VIDAL-10M&#12290;
&lt;/p&gt;
&lt;p&gt;
The video-language (VL) pretraining has achieved remarkable improvement in multiple downstream tasks. However, the current VL pretraining framework is hard to extend to multiple modalities (N modalities, N&gt;=3) beyond vision and language. We thus propose LanguageBind, taking the language as the bind across different modalities because the language modality is well-explored and contains rich semantics. Specifically, we freeze the language encoder acquired by VL pretraining, then train encoders for other modalities with contrastive learning. As a result, all modalities are mapped to a shared feature space, implementing multi-modal semantic alignment. While LanguageBind ensures that we can extend VL modalities to N modalities, we also need a high-quality dataset with alignment data pairs centered on language. We thus propose VIDAL-10M with Video, Infrared, Depth, Audio and their corresponding Language, naming as VIDAL-10M. In our VIDAL-10M, all videos are from short video platforms with co
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#35748;&#20026;&#65292;&#23545;&#35805;&#31649;&#29702;&#22120;&#22312;&#36798;&#21040;&#26368;&#20339;&#24615;&#33021;&#26041;&#38754;&#30340;&#20027;&#35201;&#38382;&#39064;&#22312;&#20110;&#25968;&#25454;&#38598;&#30340;&#36136;&#37327;&#65292;&#32780;&#19981;&#26159;&#37319;&#29992;&#30340;&#27169;&#22411;&#12290;&#30740;&#31350;&#21457;&#29616;&#26368;&#24191;&#27867;&#20351;&#29992;&#30340;&#25968;&#25454;&#38598;&#20013;&#23384;&#22312;&#30340;&#38169;&#35823;&#26159;&#23548;&#33268;&#23545;&#35805;&#31649;&#29702;&#22833;&#36133;&#30340;&#20027;&#35201;&#21407;&#22240;&#12290;</title><link>http://arxiv.org/abs/2310.01339</link><description>&lt;p&gt;
&#25913;&#36827;&#23545;&#35805;&#31649;&#29702;&#65306;&#36136;&#37327;&#25968;&#25454;&#38598; vs &#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Improving Dialogue Management: Quality Datasets vs Models. (arXiv:2310.01339v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01339
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#35748;&#20026;&#65292;&#23545;&#35805;&#31649;&#29702;&#22120;&#22312;&#36798;&#21040;&#26368;&#20339;&#24615;&#33021;&#26041;&#38754;&#30340;&#20027;&#35201;&#38382;&#39064;&#22312;&#20110;&#25968;&#25454;&#38598;&#30340;&#36136;&#37327;&#65292;&#32780;&#19981;&#26159;&#37319;&#29992;&#30340;&#27169;&#22411;&#12290;&#30740;&#31350;&#21457;&#29616;&#26368;&#24191;&#27867;&#20351;&#29992;&#30340;&#25968;&#25454;&#38598;&#20013;&#23384;&#22312;&#30340;&#38169;&#35823;&#26159;&#23548;&#33268;&#23545;&#35805;&#31649;&#29702;&#22833;&#36133;&#30340;&#20027;&#35201;&#21407;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38754;&#21521;&#20219;&#21153;&#30340;&#23545;&#35805;&#31995;&#32479;(TODS)&#24050;&#32463;&#25104;&#20026;&#29992;&#25143;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#19982;&#26426;&#22120;&#21644;&#35745;&#31639;&#26426;&#20132;&#20114;&#30340;&#20851;&#38190;&#12290;&#20854;&#20013;&#19968;&#20010;&#20851;&#38190;&#32452;&#20214;&#26159;&#23545;&#35805;&#31649;&#29702;&#22120;&#65292;&#36890;&#36807;&#25552;&#20379;&#26368;&#20339;&#21709;&#24212;&#23558;&#23545;&#35805;&#24341;&#23548;&#21040;&#29992;&#25143;&#30340;&#30446;&#26631;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;&#35268;&#21017;&#30340;&#31995;&#32479;(RBS)&#12289;&#24378;&#21270;&#23398;&#20064;(RL)&#21644;&#30417;&#30563;&#23398;&#20064;(SL)&#20316;&#20026;&#27491;&#30830;&#23545;&#35805;&#31649;&#29702;&#30340;&#35299;&#20915;&#26041;&#26696;&#65307;&#25442;&#21477;&#35805;&#35828;&#65292;&#26681;&#25454;&#29992;&#25143;&#30340;&#36755;&#20837;&#36873;&#25321;&#26368;&#20339;&#21709;&#24212;&#12290;&#28982;&#32780;&#65292;&#26412;&#30740;&#31350;&#35748;&#20026;&#65292;&#23545;&#35805;&#31649;&#29702;&#22120;&#26410;&#33021;&#36798;&#21040;&#26368;&#20339;&#24615;&#33021;&#30340;&#20027;&#35201;&#21407;&#22240;&#22312;&#20110;&#25968;&#25454;&#38598;&#30340;&#36136;&#37327;&#65292;&#32780;&#19981;&#26159;&#36804;&#20170;&#20026;&#27490;&#37319;&#29992;&#30340;&#27169;&#22411;&#65307;&#36825;&#24847;&#21619;&#30528;&#25968;&#25454;&#38598;&#38169;&#35823;&#65292;&#22914;&#38169;&#35823;&#26631;&#35760;&#65292;&#23548;&#33268;&#23545;&#35805;&#31649;&#29702;&#30340;&#22823;&#37096;&#20998;&#22833;&#36133;&#12290;&#20026;&#20102;&#35777;&#26126;&#36825;&#19968;&#20551;&#35774;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#26368;&#24191;&#27867;&#20351;&#29992;&#30340;&#25968;&#25454;&#38598;Multiwoz 2.1&#21644;SGD&#20013;&#30340;&#20027;&#35201;&#38169;&#35823;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#21512;&#25104;&#23545;&#35805;&#29983;&#25104;&#22120;&#20197;&#23436;&#20840;&#25511;&#21046;&#23545;&#35805;&#30340;&#29983;&#25104;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Task-oriented dialogue systems (TODS) have become crucial for users to interact with machines and computers using natural language. One of its key components is the dialogue manager, which guides the conversation towards a good goal for the user by providing the best possible response. Previous works have proposed rule-based systems (RBS), reinforcement learning (RL), and supervised learning (SL) as solutions for the correct dialogue management; in other words, select the best response given input by the user. However, this work argues that the leading cause of DMs not achieving maximum performance resides in the quality of the datasets rather than the models employed thus far; this means that dataset errors, like mislabeling, originate a large percentage of failures in dialogue management. We studied the main errors in the most widely used datasets, Multiwoz 2.1 and SGD, to demonstrate this hypothesis. To do this, we have designed a synthetic dialogue generator to fully control the am
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#36890;&#36807;&#20351;&#29992;ACE&#20195;&#29702;&#35299;&#20915;&#30446;&#26631;&#38169;&#35823;&#27867;&#21270;&#20013;&#30340;CoinRun&#25361;&#25112;&#65292;&#24182;&#23637;&#31034;&#20102;&#33258;&#20027;&#20195;&#29702;&#22312;&#26032;&#29615;&#22659;&#19979;&#21487;&#20197;&#22312;&#19981;&#20351;&#29992;&#26032;&#22870;&#21169;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#65292;&#22312;&#20851;&#38190;&#24773;&#20917;&#19979;&#21463;&#20154;&#20449;&#20219;&#22320;&#34892;&#21160;&#12290;</title><link>http://arxiv.org/abs/2309.16166</link><description>&lt;p&gt;
CoinRun: &#35299;&#20915;&#30446;&#26631;&#38169;&#35823;&#27867;&#21270;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
CoinRun: Solving Goal Misgeneralisation. (arXiv:2309.16166v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16166
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#36890;&#36807;&#20351;&#29992;ACE&#20195;&#29702;&#35299;&#20915;&#30446;&#26631;&#38169;&#35823;&#27867;&#21270;&#20013;&#30340;CoinRun&#25361;&#25112;&#65292;&#24182;&#23637;&#31034;&#20102;&#33258;&#20027;&#20195;&#29702;&#22312;&#26032;&#29615;&#22659;&#19979;&#21487;&#20197;&#22312;&#19981;&#20351;&#29992;&#26032;&#22870;&#21169;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#65292;&#22312;&#20851;&#38190;&#24773;&#20917;&#19979;&#21463;&#20154;&#20449;&#20219;&#22320;&#34892;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#26631;&#38169;&#35823;&#27867;&#21270;&#26159;&#20154;&#24037;&#26234;&#33021;&#23545;&#40784;&#30340;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#65292;&#21363;&#20351;&#24378;&#22823;&#30340;&#20154;&#24037;&#26234;&#33021;&#33021;&#22815;&#23558;&#20854;&#30446;&#26631;&#19982;&#20154;&#31867;&#24847;&#22270;&#21644;&#36947;&#24503;&#23545;&#40784;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;ACE&#65288;&#27010;&#24565;&#25193;&#23637;&#31639;&#27861;&#65289;&#20195;&#29702;&#22914;&#20309;&#35299;&#20915;&#30446;&#26631;&#38169;&#35823;&#27867;&#21270;&#30340;&#19968;&#39033;&#20851;&#38190;&#26631;&#20934;&#25361;&#25112;&#65306;CoinRun&#25361;&#25112;&#12290;&#35813;&#20195;&#29702;&#22312;&#26032;&#29615;&#22659;&#20013;&#19981;&#20351;&#29992;&#20219;&#20309;&#26032;&#30340;&#22870;&#21169;&#20449;&#24687;&#12290;&#36825;&#34920;&#26126;&#33258;&#20027;&#20195;&#29702;&#21487;&#20197;&#22312;&#26032;&#39062;&#21644;&#20851;&#38190;&#30340;&#24773;&#20917;&#19979;&#21463;&#20154;&#20449;&#20219;&#22320;&#34892;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
Goal misgeneralisation is a key challenge in AI alignment -- the task of getting powerful Artificial Intelligences to align their goals with human intentions and human morality. In this paper, we show how the ACE (Algorithm for Concept Extrapolation) agent can solve one of the key standard challenges in goal misgeneralisation: the CoinRun challenge. It uses no new reward information in the new environment. This points to how autonomous agents could be trusted to act in human interests, even in novel and critical situations.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#20840;&#29699;&#30456;&#20851;&#30340;&#19977;&#32500;&#35299;&#32806;Transformer&#26550;&#26500;&#65292;&#29992;&#20110;&#20174;&#21333;&#30446;&#22270;&#20687;&#20013;&#37325;&#24314;&#20855;&#26377;&#34915;&#26381;&#30340;&#20154;&#29289;&#21270;&#36523;&#12290;&#36890;&#36807;&#20351;&#29992;Transformer&#27169;&#22411;&#25429;&#25417;&#20840;&#23616;&#30456;&#20851;&#30340;&#22270;&#20687;&#29305;&#24449;&#65292;&#24182;&#37319;&#29992;&#21019;&#26032;&#30340;3D&#35299;&#32806;&#35299;&#30721;&#22120;&#36827;&#34892;&#29305;&#24449;&#34701;&#21512;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#37325;&#24314;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.13524</link><description>&lt;p&gt;
&#20840;&#29699;&#30456;&#20851;&#30340;&#19977;&#32500;&#35299;&#32806;Transformer&#29992;&#20110;&#26381;&#35013;&#21270;&#36523;&#37325;&#24314;
&lt;/p&gt;
&lt;p&gt;
Global-correlated 3D-decoupling Transformer for Clothed Avatar Reconstruction. (arXiv:2309.13524v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13524
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#20840;&#29699;&#30456;&#20851;&#30340;&#19977;&#32500;&#35299;&#32806;Transformer&#26550;&#26500;&#65292;&#29992;&#20110;&#20174;&#21333;&#30446;&#22270;&#20687;&#20013;&#37325;&#24314;&#20855;&#26377;&#34915;&#26381;&#30340;&#20154;&#29289;&#21270;&#36523;&#12290;&#36890;&#36807;&#20351;&#29992;Transformer&#27169;&#22411;&#25429;&#25417;&#20840;&#23616;&#30456;&#20851;&#30340;&#22270;&#20687;&#29305;&#24449;&#65292;&#24182;&#37319;&#29992;&#21019;&#26032;&#30340;3D&#35299;&#32806;&#35299;&#30721;&#22120;&#36827;&#34892;&#29305;&#24449;&#34701;&#21512;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#37325;&#24314;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#21333;&#19968;&#22270;&#20687;&#20013;&#37325;&#24314;&#19977;&#32500;&#26381;&#35013;&#21270;&#36523;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#29305;&#21035;&#26159;&#22312;&#36935;&#21040;&#22797;&#26434;&#23039;&#21183;&#21644;&#23485;&#26494;&#34915;&#29289;&#26102;&#12290;&#29616;&#26377;&#26041;&#27861;&#22312;&#24615;&#33021;&#19978;&#23384;&#22312;&#23616;&#38480;&#24615;&#65292;&#36825;&#20027;&#35201;&#24402;&#22240;&#20110;&#23427;&#20204;&#23545;&#19981;&#36275;&#30340;&#20108;&#32500;&#22270;&#20687;&#29305;&#24449;&#21644;&#19981;&#19968;&#33268;&#30340;&#26597;&#35810;&#26041;&#27861;&#30340;&#20381;&#36182;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#29992;&#20110;&#26381;&#35013;&#21270;&#36523;&#37325;&#24314;&#30340;&#20840;&#29699;&#30456;&#20851;&#30340;&#19977;&#32500;&#35299;&#32806;Transformer&#65288;GTA&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#21019;&#26032;&#20307;&#31995;&#32467;&#26500;&#65292;&#21487;&#20197;&#20174;&#21333;&#30446;&#22270;&#20687;&#20013;&#37325;&#24314;&#20986;&#20855;&#26377;&#34915;&#26381;&#30340;&#20154;&#29289;&#21270;&#36523;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;Transformer&#20307;&#31995;&#32467;&#26500;&#65292;&#36890;&#36807;&#20351;&#29992;Vision Transformer&#27169;&#22411;&#20316;&#20026;&#32534;&#30721;&#22120;&#26469;&#25429;&#25417;&#20840;&#29699;&#30456;&#20851;&#30340;&#22270;&#20687;&#29305;&#24449;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#21019;&#26032;&#24615;&#22320;&#37319;&#29992;&#20132;&#21449;&#27880;&#24847;&#21147;&#26469;&#35299;&#32806;&#19977;&#20301;&#24179;&#38754;&#29305;&#24449;&#65292;&#24182;&#20351;&#29992;&#21487;&#23398;&#20064;&#30340;&#23884;&#20837;&#20316;&#20026;&#36328;&#24179;&#38754;&#29983;&#25104;&#30340;&#26597;&#35810;&#12290;&#20026;&#20102;&#26377;&#25928;&#22686;&#24378;&#19982;&#19977;&#32500;&#29305;&#24449;&#21644;&#20154;&#20307;&#20808;&#39564;&#30340;&#29305;&#24449;&#34701;&#21512;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#34701;&#21512;&#31354;&#38388;&#21644;p&#30340;&#28151;&#21512;&#20808;&#39564;&#34701;&#21512;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Reconstructing 3D clothed human avatars from single images is a challenging task, especially when encountering complex poses and loose clothing. Current methods exhibit limitations in performance, largely attributable to their dependence on insufficient 2D image features and inconsistent query methods. Owing to this, we present the Global-correlated 3D-decoupling Transformer for clothed Avatar reconstruction (GTA), a novel transformer-based architecture that reconstructs clothed human avatars from monocular images. Our approach leverages transformer architectures by utilizing a Vision Transformer model as an encoder for capturing global-correlated image features. Subsequently, our innovative 3D-decoupling decoder employs cross-attention to decouple tri-plane features, using learnable embeddings as queries for cross-plane generation. To effectively enhance feature fusion with the tri-plane 3D feature and human body prior, we propose a hybrid prior fusion strategy combining spatial and p
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#20805;&#20998;&#22240;&#32032;&#21644;&#24517;&#35201;&#22240;&#32032;&#30340;&#27010;&#29575;&#65288;PNS&#65289;&#26469;&#25913;&#21892;&#22312;&#26410;&#30693;&#27979;&#35797;&#20998;&#24067;&#19978;&#30340;&#27867;&#21270;&#38382;&#39064;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#20851;&#27880;&#22240;&#26524;&#24615;&#30340;&#19981;&#21464;&#24615;&#23646;&#24615;&#32780;&#24573;&#35270;&#20805;&#20998;&#24615;&#21644;&#24517;&#35201;&#24615;&#26465;&#20214;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.12559</link><description>&lt;p&gt;
&#36890;&#36807;&#20805;&#20998;&#22240;&#32032;&#21644;&#24517;&#35201;&#22240;&#32032;&#30340;&#27010;&#29575;&#36827;&#34892;&#19981;&#21464;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Invariant Learning via Probability of Sufficient and Necessary Causes. (arXiv:2309.12559v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12559
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#20805;&#20998;&#22240;&#32032;&#21644;&#24517;&#35201;&#22240;&#32032;&#30340;&#27010;&#29575;&#65288;PNS&#65289;&#26469;&#25913;&#21892;&#22312;&#26410;&#30693;&#27979;&#35797;&#20998;&#24067;&#19978;&#30340;&#27867;&#21270;&#38382;&#39064;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#20851;&#27880;&#22240;&#26524;&#24615;&#30340;&#19981;&#21464;&#24615;&#23646;&#24615;&#32780;&#24573;&#35270;&#20805;&#20998;&#24615;&#21644;&#24517;&#35201;&#24615;&#26465;&#20214;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#37326;&#22806;&#23398;&#20064;&#20013;&#65292;&#23545;&#20110;&#26410;&#30693;&#30340;&#12289;&#19982;&#35757;&#32451;&#20998;&#24067;&#19981;&#21516;&#30340;&#27979;&#35797;&#20998;&#24067;&#65292;&#22806;&#37096;&#20998;&#24067;&#65288;OOD&#65289;&#27867;&#21270;&#26159;&#19981;&#21487;&#25110;&#32570;&#30340;&#12290;&#26368;&#36817;&#20174;&#22240;&#26524;&#24615;&#24341;&#21457;&#30340;&#26041;&#27861;&#22312;&#23454;&#29616;OOD&#27867;&#21270;&#26041;&#38754;&#26174;&#31034;&#20986;&#20102;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#20851;&#27880;&#22240;&#26524;&#24615;&#30340;&#19981;&#21464;&#24615;&#23646;&#24615;&#65292;&#32780;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#24573;&#35270;&#20102;&#20805;&#20998;&#24615;&#21644;&#24517;&#35201;&#24615;&#26465;&#20214;&#30340;&#23646;&#24615;&#12290;&#25442;&#21477;&#35805;&#35828;&#65292;&#19968;&#20010;&#24517;&#35201;&#20294;&#19981;&#20805;&#20998;&#30340;&#21407;&#22240;&#65288;&#29305;&#24449;&#65289;&#23545;&#20110;&#20998;&#24067;&#36716;&#25442;&#26159;&#19981;&#21464;&#30340;&#65292;&#20294;&#21487;&#33021;&#27809;&#26377;&#25152;&#38656;&#30340;&#20934;&#30830;&#24230;&#12290;&#30456;&#21453;&#65292;&#19968;&#20010;&#20805;&#20998;&#20294;&#19981;&#24517;&#35201;&#30340;&#21407;&#22240;&#65288;&#29305;&#24449;&#65289;&#20542;&#21521;&#20110;&#24456;&#22909;&#22320;&#36866;&#24212;&#29305;&#23450;&#25968;&#25454;&#65292;&#20294;&#21487;&#33021;&#23384;&#22312;&#36866;&#24212;&#26032;&#39046;&#22495;&#30340;&#39118;&#38505;&#12290;&#20026;&#20102;&#25429;&#25417;&#20805;&#20998;&#21644;&#24517;&#35201;&#22240;&#32032;&#30340;&#20449;&#24687;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#32463;&#20856;&#27010;&#24565;&#8212;&#8212;&#20805;&#20998;&#21644;&#24517;&#35201;&#22240;&#32032;&#30340;&#27010;&#29575;&#65288;PNS&#65289;&#65292;&#23427;&#25351;&#31034;&#20102;&#19968;&#20010;&#22240;&#32032;&#26159;&#24517;&#35201;&#21644;&#20805;&#20998;&#21407;&#22240;&#30340;&#27010;&#29575;&#12290;&#20026;&#20102;&#23558;PNS&#19982;OOD&#27867;&#21270;&#32852;&#31995;&#36215;&#26469;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Out-of-distribution (OOD) generalization is indispensable for learning models in the wild, where testing distribution typically unknown and different from the training. Recent methods derived from causality have shown great potential in achieving OOD generalization. However, existing methods mainly focus on the invariance property of causes, while largely overlooking the property of \textit{sufficiency} and \textit{necessity} conditions. Namely, a necessary but insufficient cause (feature) is invariant to distribution shift, yet it may not have required accuracy. By contrast, a sufficient yet unnecessary cause (feature) tends to fit specific data well but may have a risk of adapting to a new domain. To capture the information of sufficient and necessary causes, we employ a classical concept, the probability of sufficiency and necessary causes (PNS), which indicates the probability of whether one is the necessary and sufficient cause. To associate PNS with OOD generalization, we propose
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;Benchmark&#27979;&#35797;&#65292;&#21457;&#29616;&#20351;&#29992;Prompt Tuning&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#26631;&#31614;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#20013;&#20855;&#26377;&#36739;&#22909;&#30340;&#24615;&#33021;&#21644;&#35745;&#31639;&#25928;&#29575;&#12290;&#21516;&#26102;&#65292;&#25552;&#20986;&#20102;&#20351;&#29992;Trie&#25628;&#32034;&#26469;&#35299;&#20915;&#29983;&#25104;&#26631;&#31614;&#21305;&#37197;&#38382;&#39064;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2309.12075</link><description>&lt;p&gt;
&#20351;&#29992;Prompt&#35843;&#20248;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21152;&#36895;&#20027;&#39064;&#25237;&#36164;
&lt;/p&gt;
&lt;p&gt;
Accelerating Thematic Investment with Prompt Tuned Pretrained Language Models. (arXiv:2309.12075v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12075
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;Benchmark&#27979;&#35797;&#65292;&#21457;&#29616;&#20351;&#29992;Prompt Tuning&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#26631;&#31614;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#20013;&#20855;&#26377;&#36739;&#22909;&#30340;&#24615;&#33021;&#21644;&#35745;&#31639;&#25928;&#29575;&#12290;&#21516;&#26102;&#65292;&#25552;&#20986;&#20102;&#20351;&#29992;Trie&#25628;&#32034;&#26469;&#35299;&#20915;&#29983;&#25104;&#26631;&#31614;&#21305;&#37197;&#38382;&#39064;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Prompt Tuning&#20316;&#20026;&#19968;&#31181;&#21487;&#25193;&#23637;&#19988;&#25104;&#26412;&#25928;&#30410;&#39640;&#30340;&#26041;&#27861;&#65292;&#27491;&#22312;&#25104;&#20026;&#32454;&#35843;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#30340;&#19968;&#31181;&#27969;&#34892;&#26041;&#27861;&#12290;&#26412;&#30740;&#31350;&#22522;&#20110;&#22810;&#26631;&#31614;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#23545;Prompt Tuning&#21644;&#22522;&#20934;&#26041;&#27861;&#30340;&#24615;&#33021;&#21644;&#35745;&#31639;&#25928;&#29575;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#12290;&#23558;&#20854;&#24212;&#29992;&#20110;&#23558;&#20844;&#21496;&#20998;&#31867;&#20026;&#25237;&#36164;&#20844;&#21496;&#19987;&#26377;&#30340;&#34892;&#19994;&#20998;&#31867;&#27861;&#65292;&#20197;&#25903;&#25345;&#20854;&#20027;&#39064;&#25237;&#36164;&#31574;&#30053;&#12290;&#22312;&#22810;&#26631;&#31614;&#20998;&#31867;&#38382;&#39064;&#20013;&#65292;&#20351;&#29992;PLMs&#36827;&#34892;&#25991;&#26412;&#21040;&#25991;&#26412;&#20998;&#31867;&#32463;&#24120;&#34987;&#25253;&#21578;&#20026;&#20248;&#20110;&#20351;&#29992;&#20998;&#31867;&#22836;&#36827;&#34892;&#20998;&#31867;&#65292;&#20294;&#22312;&#27599;&#20010;&#26631;&#31614;&#30001;&#22810;&#20010;&#20196;&#29260;&#32452;&#25104;&#30340;&#22810;&#26631;&#31614;&#20998;&#31867;&#38382;&#39064;&#20013;&#65292;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#65306;&#65288;a&#65289;&#29983;&#25104;&#30340;&#26631;&#31614;&#21487;&#33021;&#19981;&#21305;&#37197;&#34892;&#19994;&#20998;&#31867;&#27861;&#20013;&#30340;&#20219;&#20309;&#26631;&#31614;&#65307;&#65288;b&#65289;&#22312;&#32454;&#35843;&#38454;&#27573;&#65292;&#24517;&#39035;&#20197;&#20219;&#24847;&#39034;&#24207;&#25552;&#20379;&#22810;&#20010;&#26631;&#31614;&#65307;&#65288;c&#65289;&#27169;&#22411;&#20026;&#27599;&#20010;&#26631;&#31614;&#25552;&#20379;&#20108;&#36827;&#21046;&#20915;&#31574;&#65292;&#32780;&#19981;&#26159;&#36866;&#24403;&#30340;&#32622;&#20449;&#24230;&#20998;&#25968;&#12290;&#36890;&#36807;&#24212;&#29992;Trie&#25628;&#32034;&#26469;&#35299;&#20915;&#38480;&#21046;&#65288;a&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prompt Tuning is emerging as a scalable and cost-effective method to fine-tune Pretrained Language Models (PLMs). This study benchmarks the performance and computational efficiency of Prompt Tuning and baseline methods on a multi-label text classification task. This is applied to the use case of classifying companies into an investment firm's proprietary industry taxonomy, supporting their thematic investment strategy. Text-to-text classification with PLMs is frequently reported to outperform classification with a classification head, but has several limitations when applied to a multi-label classification problem where each label consists of multiple tokens: (a) Generated labels may not match any label in the industry taxonomy; (b) During fine-tuning, multiple labels must be provided in an arbitrary order; (c) The model provides a binary decision for each label, rather than an appropriate confidence score. Limitation (a) is addressed by applying constrained decoding using Trie Search,
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;Gold-YOLO&#27169;&#22411;&#65292;&#36890;&#36807;&#20808;&#36827;&#30340;&#25910;&#38598;&#21644;&#20998;&#21457;&#26426;&#21046;&#65288;GD&#65289;&#26426;&#21046;&#20197;&#21450;MAE&#39118;&#26684;&#30340;&#39044;&#35757;&#32451;&#65292;&#35299;&#20915;&#20102;YOLO&#31995;&#21015;&#27169;&#22411;&#20013;&#30340;&#20449;&#24687;&#34701;&#21512;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#30446;&#26631;&#26816;&#27979;&#21644;&#22810;&#23610;&#24230;&#29305;&#24449;&#34701;&#21512;&#12290;</title><link>http://arxiv.org/abs/2309.11331</link><description>&lt;p&gt;
Gold-YOLO: &#36890;&#36807;&#25910;&#38598;&#21644;&#20998;&#21457;&#26426;&#21046;&#23454;&#29616;&#39640;&#25928;&#30446;&#26631;&#26816;&#27979;&#22120;
&lt;/p&gt;
&lt;p&gt;
Gold-YOLO: Efficient Object Detector via Gather-and-Distribute Mechanism. (arXiv:2309.11331v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11331
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;Gold-YOLO&#27169;&#22411;&#65292;&#36890;&#36807;&#20808;&#36827;&#30340;&#25910;&#38598;&#21644;&#20998;&#21457;&#26426;&#21046;&#65288;GD&#65289;&#26426;&#21046;&#20197;&#21450;MAE&#39118;&#26684;&#30340;&#39044;&#35757;&#32451;&#65292;&#35299;&#20915;&#20102;YOLO&#31995;&#21015;&#27169;&#22411;&#20013;&#30340;&#20449;&#24687;&#34701;&#21512;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#30446;&#26631;&#26816;&#27979;&#21644;&#22810;&#23610;&#24230;&#29305;&#24449;&#34701;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#20960;&#24180;&#20013;&#65292;YOLO&#31995;&#21015;&#27169;&#22411;&#24050;&#25104;&#20026;&#23454;&#26102;&#30446;&#26631;&#26816;&#27979;&#39046;&#22495;&#30340;&#39046;&#20808;&#26041;&#27861;&#12290;&#35768;&#22810;&#30740;&#31350;&#36890;&#36807;&#20462;&#25913;&#26550;&#26500;&#12289;&#22686;&#21152;&#25968;&#25454;&#21644;&#35774;&#35745;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#23558;&#22522;&#32447;&#25552;&#21319;&#21040;&#20102;&#26356;&#39640;&#27700;&#24179;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#20043;&#21069;&#30340;&#27169;&#22411;&#20173;&#28982;&#23384;&#22312;&#20449;&#24687;&#34701;&#21512;&#38382;&#39064;&#65292;&#34429;&#28982;&#29305;&#24449;&#37329;&#23383;&#22612;&#32593;&#32476;&#65288;FPN&#65289;&#21644;&#36335;&#24452;&#32858;&#21512;&#32593;&#32476;&#65288;PANet&#65289;&#24050;&#32463;&#32531;&#35299;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20808;&#36827;&#30340;&#25910;&#38598;&#21644;&#20998;&#21457;&#26426;&#21046;&#65288;GD&#65289;&#26426;&#21046;&#65292;&#36890;&#36807;&#21367;&#31215;&#21644;&#33258;&#27880;&#24847;&#21147;&#25805;&#20316;&#23454;&#29616;&#12290;&#36825;&#20010;&#26032;&#35774;&#35745;&#30340;&#27169;&#22411;&#21517;&#20026;Gold-YOLO&#65292;&#25552;&#21319;&#20102;&#22810;&#23610;&#24230;&#29305;&#24449;&#34701;&#21512;&#33021;&#21147;&#65292;&#24182;&#22312;&#25152;&#26377;&#27169;&#22411;&#23610;&#24230;&#19978;&#23454;&#29616;&#20102;&#24310;&#36831;&#21644;&#20934;&#30830;&#24615;&#30340;&#29702;&#24819;&#24179;&#34913;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#39318;&#27425;&#22312;YOLO&#31995;&#21015;&#20013;&#23454;&#29616;&#20102;MAE&#39118;&#26684;&#30340;&#39044;&#35757;&#32451;&#65292;&#20351;&#24471;YOLO&#31995;&#21015;&#27169;&#22411;&#21487;&#20197;&#20174;&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#20013;&#21463;&#30410;&#12290;Gold-YOLO-N&#22312;COCO val2017&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#20102;&#20986;&#33394;&#30340;39.9%&#24179;&#22343;&#31934;&#24230;&#65288;AP&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the past years, YOLO-series models have emerged as the leading approaches in the area of real-time object detection. Many studies pushed up the baseline to a higher level by modifying the architecture, augmenting data and designing new losses. However, we find previous models still suffer from information fusion problem, although Feature Pyramid Network (FPN) and Path Aggregation Network (PANet) have alleviated this. Therefore, this study provides an advanced Gatherand-Distribute mechanism (GD) mechanism, which is realized with convolution and self-attention operations. This new designed model named as Gold-YOLO, which boosts the multi-scale feature fusion capabilities and achieves an ideal balance between latency and accuracy across all model scales. Additionally, we implement MAE-style pretraining in the YOLO-series for the first time, allowing YOLOseries models could be to benefit from unsupervised pretraining. Gold-YOLO-N attains an outstanding 39.9% AP on the COCO val2017 datas
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#36712;&#36857;&#36319;&#36394;&#25216;&#26415;&#65292;&#36890;&#36807;&#28369;&#27169;&#25511;&#21046;&#21644;&#28145;&#24230;&#23398;&#20064;&#65292;&#22312;&#25143;&#22806;&#29615;&#22659;&#20013;&#23454;&#29616;&#20102;&#21487;&#34892;&#30340;&#31227;&#21160;&#26426;&#22120;&#20154;&#36712;&#36857;&#36319;&#36394;&#21644;&#22312;&#32447;&#28369;&#21160;&#19982;&#25171;&#28369;&#34917;&#20607;&#12290;</title><link>http://arxiv.org/abs/2309.08863</link><description>&lt;p&gt;
&#20351;&#29992;&#28369;&#27169;&#25511;&#21046;&#21644;&#28145;&#24230;&#23398;&#20064;&#30340;&#24102;&#26377;&#28369;&#21160;&#19982;&#25171;&#28369;&#34917;&#20607;&#30340;&#23653;&#24102;&#24335;&#31227;&#21160;&#26426;&#22120;&#20154;&#36712;&#36857;&#36319;&#36394;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Trajectory Tracking Control of Skid-Steering Mobile Robots with Slip and Skid Compensation using Sliding-Mode Control and Deep Learning. (arXiv:2309.08863v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08863
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#36712;&#36857;&#36319;&#36394;&#25216;&#26415;&#65292;&#36890;&#36807;&#28369;&#27169;&#25511;&#21046;&#21644;&#28145;&#24230;&#23398;&#20064;&#65292;&#22312;&#25143;&#22806;&#29615;&#22659;&#20013;&#23454;&#29616;&#20102;&#21487;&#34892;&#30340;&#31227;&#21160;&#26426;&#22120;&#20154;&#36712;&#36857;&#36319;&#36394;&#21644;&#22312;&#32447;&#28369;&#21160;&#19982;&#25171;&#28369;&#34917;&#20607;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25143;&#22806;&#29615;&#22659;&#21644;&#19981;&#24179;&#22320;&#24418;&#20013;&#65292;&#28369;&#21160;&#21644;&#25171;&#28369;&#34917;&#20607;&#23545;&#31227;&#21160;&#26426;&#22120;&#20154;&#30340;&#23548;&#33322;&#33267;&#20851;&#37325;&#35201;&#12290;&#38500;&#20102;&#24120;&#35268;&#30340;&#25143;&#22806;&#29615;&#22659;&#20013;&#31227;&#21160;&#26426;&#22120;&#20154;&#30340;&#28369;&#21160;&#21644;&#25171;&#28369;&#21361;&#38505;&#22806;&#65292;&#28369;&#21160;&#21644;&#25171;&#28369;&#36824;&#20250;&#32473;&#36712;&#36857;&#36319;&#36394;&#31995;&#32479;&#24102;&#26469;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#19988;&#20351;&#31283;&#23450;&#24615;&#20998;&#26512;&#30340;&#26377;&#25928;&#24615;&#21463;&#21040;&#23041;&#32961;&#12290;&#23613;&#31649;&#22312;&#35813;&#39046;&#22495;&#26377;&#30740;&#31350;&#65292;&#20294;&#30001;&#20110;&#25143;&#22806;&#29615;&#22659;&#20013;&#36718;&#32974;-&#22320;&#38754;&#30456;&#20114;&#20316;&#29992;&#30340;&#22797;&#26434;&#24615;&#65292;&#23454;&#29616;&#21487;&#34892;&#30340;&#22312;&#32447;&#28369;&#21160;&#21644;&#25171;&#28369;&#34917;&#20607;&#20173;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36712;&#36857;&#36319;&#36394;&#25216;&#26415;&#65292;&#33021;&#22815;&#22312;&#25143;&#22806;&#29615;&#22659;&#20013;&#23454;&#29616;&#36710;&#36742;&#32423;&#30340;&#23454;&#26102;&#28369;&#21160;&#21644;&#25171;&#28369;&#34917;&#20607;&#12290;&#37319;&#29992;&#28369;&#27169;&#25511;&#21046;&#25216;&#26415;&#35774;&#35745;&#20102;&#40065;&#26834;&#30340;&#36712;&#36857;&#36319;&#36394;&#31995;&#32479;&#65292;&#21487;&#20197;&#32771;&#34385;&#21040;&#36825;&#31181;&#31867;&#22411;&#26426;&#22120;&#20154;&#30340;&#21442;&#25968;&#19981;&#30830;&#23450;&#24615;&#12290;&#22312;&#25511;&#21046;&#21453;&#39304;&#24490;&#29615;&#20013;&#65292;&#23558;&#20004;&#20010;&#20808;&#21069;&#24320;&#21457;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;[1]&#65292;[2]&#38598;&#25104;&#29992;&#20110;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Slip and skid compensation is crucial for mobile robots' navigation in outdoor environments and uneven terrains. In addition to the general slipping and skidding hazards for mobile robots in outdoor environments, slip and skid cause uncertainty for the trajectory tracking system and put the validity of stability analysis at risk. Despite research in this field, having a real-world feasible online slip and skid compensation is still challenging due to the complexity of wheel-terrain interaction in outdoor environments. This paper presents a novel trajectory tracking technique with real-world feasible online slip and skid compensation at the vehicle-level for skid-steering mobile robots in outdoor environments. The sliding mode control technique is utilized to design a robust trajectory tracking system to be able to consider the parameter uncertainty of this type of robot. Two previously developed deep learning models [1], [2] are integrated into the control feedback loop to estimate the
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#38656;&#27714;&#39537;&#21160;&#23548;&#33322;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#29992;&#25143;&#30340;&#38656;&#27714;&#19982;&#22330;&#26223;&#20013;&#30340;&#23545;&#35937;&#23646;&#24615;&#31354;&#38388;&#36827;&#34892;&#23548;&#33322;&#20915;&#31574;&#65292;&#24182;&#35299;&#20915;&#20102;&#22312;&#23454;&#38469;&#24773;&#20917;&#20013;&#29992;&#25143;&#26080;&#27861;&#30693;&#36947;&#23545;&#35937;&#21517;&#31216;&#25110;&#25351;&#23450;&#23545;&#35937;&#19981;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.08138</link><description>&lt;p&gt;
&#23547;&#25214;&#20320;&#24819;&#35201;&#30340;&#65306;&#23398;&#20064;&#23454;&#29616;&#38656;&#27714;&#39537;&#21160;&#23548;&#33322;&#30340;&#23545;&#35937;&#23646;&#24615;&#31354;&#38388;
&lt;/p&gt;
&lt;p&gt;
Find What You Want: Learning Demand-conditioned Object Attribute Space for Demand-driven Navigation. (arXiv:2309.08138v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08138
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#38656;&#27714;&#39537;&#21160;&#23548;&#33322;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#29992;&#25143;&#30340;&#38656;&#27714;&#19982;&#22330;&#26223;&#20013;&#30340;&#23545;&#35937;&#23646;&#24615;&#31354;&#38388;&#36827;&#34892;&#23548;&#33322;&#20915;&#31574;&#65292;&#24182;&#35299;&#20915;&#20102;&#22312;&#23454;&#38469;&#24773;&#20917;&#20013;&#29992;&#25143;&#26080;&#27861;&#30693;&#36947;&#23545;&#35937;&#21517;&#31216;&#25110;&#25351;&#23450;&#23545;&#35937;&#19981;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#23545;&#35937;&#23548;&#33322;&#65288;VON&#65289;&#30340;&#20219;&#21153;&#26159;&#20351;&#26234;&#33021;&#20307;&#33021;&#22815;&#22312;&#32473;&#23450;&#30340;&#22330;&#26223;&#20013;&#23450;&#20301;&#29305;&#23450;&#30340;&#23545;&#35937;&#12290;&#20026;&#20102;&#25104;&#21151;&#23436;&#25104;VON&#20219;&#21153;&#65292;&#24517;&#39035;&#28385;&#36275;&#20004;&#20010;&#22522;&#26412;&#26465;&#20214;&#65306;1&#65289;&#29992;&#25143;&#24517;&#39035;&#30693;&#36947;&#25152;&#38656;&#23545;&#35937;&#30340;&#21517;&#31216;&#65307;2&#65289;&#29992;&#25143;&#25351;&#23450;&#30340;&#23545;&#35937;&#24517;&#39035;&#30830;&#23454;&#23384;&#22312;&#20110;&#22330;&#26223;&#20013;&#12290;&#20026;&#20102;&#28385;&#36275;&#36825;&#20123;&#26465;&#20214;&#65292;&#27169;&#25311;&#22120;&#21487;&#20197;&#23558;&#39044;&#23450;&#20041;&#30340;&#23545;&#35937;&#21517;&#31216;&#21644;&#20301;&#32622;&#32435;&#20837;&#22330;&#26223;&#30340;&#20803;&#25968;&#25454;&#20013;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#24773;&#20917;&#19979;&#65292;&#30830;&#20445;&#22987;&#32456;&#28385;&#36275;&#36825;&#20123;&#26465;&#20214;&#24448;&#24448;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22312;&#38476;&#29983;&#30340;&#29615;&#22659;&#20013;&#65292;&#20154;&#20204;&#21487;&#33021;&#19981;&#30693;&#36947;&#22330;&#26223;&#20013;&#23384;&#22312;&#21738;&#20123;&#23545;&#35937;&#65292;&#25110;&#32773;&#20182;&#20204;&#21487;&#33021;&#38169;&#35823;&#22320;&#25351;&#23450;&#19968;&#20010;&#23454;&#38469;&#19978;&#19981;&#23384;&#22312;&#30340;&#23545;&#35937;&#12290;&#23613;&#31649;&#23384;&#22312;&#36825;&#20123;&#25361;&#25112;&#65292;&#20154;&#20204;&#20173;&#28982;&#21487;&#33021;&#23545;&#19968;&#20010;&#23545;&#35937;&#26377;&#38656;&#27714;&#65292;&#36825;&#20010;&#38656;&#27714;&#21487;&#33021;&#21487;&#20197;&#36890;&#36807;&#22330;&#26223;&#20013;&#23384;&#22312;&#30340;&#20854;&#20182;&#23545;&#35937;&#20197;&#31561;&#25928;&#30340;&#26041;&#24335;&#26469;&#28385;&#36275;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#38656;&#27714;&#39537;&#21160;&#23548;&#33322;&#65288;DDN&#65289;&#65292;&#23427;&#21033;&#29992;&#29992;&#25143;&#30340;&#38656;&#27714;&#19982;&#22330;&#26223;&#20013;&#30340;&#23545;&#35937;&#23646;&#24615;&#31354;&#38388;&#36827;&#34892;&#23548;&#33322;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;
The task of Visual Object Navigation (VON) involves an agent's ability to locate a particular object within a given scene. In order to successfully accomplish the VON task, two essential conditions must be fulfilled:1) the user must know the name of the desired object; and 2) the user-specified object must actually be present within the scene. To meet these conditions, a simulator can incorporate pre-defined object names and positions into the metadata of the scene. However, in real-world scenarios, it is often challenging to ensure that these conditions are always met. Human in an unfamiliar environment may not know which objects are present in the scene, or they may mistakenly specify an object that is not actually present. Nevertheless, despite these challenges, human may still have a demand for an object, which could potentially be fulfilled by other objects present within the scene in an equivalent manner. Hence, we propose Demand-driven Navigation (DDN), which leverages the user'
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#31163;&#32447;&#36870;&#21521;&#24378;&#21270;&#23398;&#20064;&#30340;&#25552;&#31034;&#35780;&#20272;&#19982;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#31163;&#32447;&#25968;&#25454;&#38598;&#21644;&#36870;&#21521;&#24378;&#21270;&#23398;&#20064;&#65292;&#39044;&#27979;&#25552;&#31034;&#24615;&#33021;&#12289;&#25552;&#39640;&#25104;&#26412;&#25928;&#30410;&#12289;&#29983;&#25104;&#26131;&#35835;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.06553</link><description>&lt;p&gt;
&#31163;&#32447;&#36870;&#21521;&#24378;&#21270;&#23398;&#20064;&#19979;&#30340;&#25552;&#31034;&#35780;&#20272;&#19982;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Offline Prompt Evaluation and Optimization with Inverse Reinforcement Learning. (arXiv:2309.06553v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06553
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#31163;&#32447;&#36870;&#21521;&#24378;&#21270;&#23398;&#20064;&#30340;&#25552;&#31034;&#35780;&#20272;&#19982;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#31163;&#32447;&#25968;&#25454;&#38598;&#21644;&#36870;&#21521;&#24378;&#21270;&#23398;&#20064;&#65292;&#39044;&#27979;&#25552;&#31034;&#24615;&#33021;&#12289;&#25552;&#39640;&#25104;&#26412;&#25928;&#30410;&#12289;&#29983;&#25104;&#26131;&#35835;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20687;ChatGPT&#36825;&#26679;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#21457;&#23637;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#21033;&#29992;&#20154;&#31867;&#19987;&#19994;&#30693;&#35782;&#12290;&#28982;&#32780;&#65292;&#20805;&#20998;&#25581;&#31034;LLMs&#22312;&#22797;&#26434;&#20219;&#21153;&#20013;&#30340;&#28508;&#21147;&#38656;&#35201;&#22312;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#30340;&#24191;&#38420;&#25628;&#32034;&#31354;&#38388;&#20013;&#36827;&#34892;&#23548;&#33322;&#12290;&#34429;&#28982;&#25552;&#31034;&#24037;&#31243;&#26174;&#31034;&#20986;&#28508;&#21147;&#65292;&#20294;&#35797;&#38169;&#23581;&#35797;&#20013;&#25152;&#38656;&#30340;&#20154;&#24037;&#35774;&#35745;&#25552;&#31034;&#21644;&#30456;&#20851;&#25104;&#26412;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#20851;&#38190;&#26159;&#65292;&#25552;&#31034;&#20248;&#21270;&#30340;&#25928;&#29575;&#21462;&#20915;&#20110;&#26114;&#36149;&#30340;&#25552;&#31034;&#35780;&#20272;&#36807;&#31243;&#12290;&#26412;&#24037;&#20316;&#20171;&#32461;&#20102;Prompt-OIRL&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#31163;&#32447;&#36870;&#21521;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#24357;&#21512;&#26377;&#25928;&#25552;&#31034;&#35780;&#20272;&#21644;&#21487;&#36127;&#25285;&#24615;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#19987;&#23478;&#35780;&#20272;&#30340;&#31163;&#32447;&#25968;&#25454;&#38598;&#65292;&#36816;&#29992;&#36870;&#21521;&#24378;&#21270;&#23398;&#20064;&#33719;&#24471;&#19968;&#20010;&#38024;&#23545;&#31163;&#32447;&#12289;&#26597;&#35810;&#20381;&#36182;&#22411;&#25552;&#31034;&#35780;&#20272;&#30340;&#22870;&#21169;&#27169;&#22411;&#12290;Prompt-OIRL&#30340;&#20248;&#28857;&#26159;&#22810;&#26041;&#38754;&#30340;&#65306;&#23427;&#39044;&#27979;&#25552;&#31034;&#30340;&#24615;&#33021;&#65292;&#25104;&#26412;&#39640;&#25928;&#65292;&#29983;&#25104;&#26131;&#35835;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent advances in the development of Large Language Models (LLMs) like ChatGPT have achieved remarkable performance by leveraging human expertise. Yet, fully eliciting LLMs' potential for complex tasks requires navigating the vast search space of natural language prompts. While prompt engineering has shown promise, the requisite human-crafted prompts in trial-and-error attempts and the associated costs pose significant challenges. Crucially, the efficiency of prompt optimization hinges on the costly procedure of prompt evaluation. This work introduces Prompt-OIRL, an approach rooted in offline inverse reinforcement learning that seeks to bridge the gap between effective prompt evaluation and affordability. Our method draws on offline datasets from expert evaluations, employing Inverse-RL to derive a reward model for offline, query-dependent prompt evaluations. The advantages of Prompt-OIRL are manifold: it predicts prompt performance, is cost-efficient, produces human-readable res
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;Transformer&#26550;&#26500;&#20013;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#65288;FFN&#65289;&#30340;&#20316;&#29992;&#65292;&#21457;&#29616;&#23613;&#31649;&#23427;&#21344;&#25454;&#20102;&#27169;&#22411;&#24456;&#22823;&#19968;&#37096;&#20998;&#30340;&#21442;&#25968;&#65292;&#20294;&#23427;&#26159;&#20887;&#20313;&#30340;&#12290;&#36890;&#36807;&#31227;&#38500;&#35299;&#30721;&#22120;&#23618;&#30340;FFN&#24182;&#22312;&#32534;&#30721;&#22120;&#19978;&#20849;&#20139;&#21333;&#20010;FFN&#65292;&#25105;&#20204;&#33021;&#22815;&#26174;&#33879;&#20943;&#23569;&#21442;&#25968;&#25968;&#37327;&#24182;&#23454;&#29616;&#20934;&#30830;&#24615;&#21644;&#24310;&#36831;&#19978;&#30340;&#26174;&#33879;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2309.01826</link><description>&lt;p&gt;
&#21482;&#38656;&#35201;&#19968;&#20010;&#23485;&#24230;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
One Wide Feedforward is All You Need. (arXiv:2309.01826v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.01826
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;Transformer&#26550;&#26500;&#20013;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#65288;FFN&#65289;&#30340;&#20316;&#29992;&#65292;&#21457;&#29616;&#23613;&#31649;&#23427;&#21344;&#25454;&#20102;&#27169;&#22411;&#24456;&#22823;&#19968;&#37096;&#20998;&#30340;&#21442;&#25968;&#65292;&#20294;&#23427;&#26159;&#20887;&#20313;&#30340;&#12290;&#36890;&#36807;&#31227;&#38500;&#35299;&#30721;&#22120;&#23618;&#30340;FFN&#24182;&#22312;&#32534;&#30721;&#22120;&#19978;&#20849;&#20139;&#21333;&#20010;FFN&#65292;&#25105;&#20204;&#33021;&#22815;&#26174;&#33879;&#20943;&#23569;&#21442;&#25968;&#25968;&#37327;&#24182;&#23454;&#29616;&#20934;&#30830;&#24615;&#21644;&#24310;&#36831;&#19978;&#30340;&#26174;&#33879;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#26550;&#26500;&#26377;&#20004;&#20010;&#20027;&#35201;&#30340;&#38750;&#23884;&#20837;&#32452;&#20214;&#65306;&#27880;&#24847;&#21147;&#21644;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#65288;FFN&#65289;&#12290;&#27880;&#24847;&#21147;&#25429;&#25417;&#21040;&#19981;&#32771;&#34385;&#20301;&#32622;&#30340;&#21333;&#35789;&#20043;&#38388;&#30340;&#30456;&#20114;&#20381;&#36182;&#20851;&#31995;&#65292;&#32780;FFN&#29420;&#31435;&#22320;&#23545;&#27599;&#20010;&#36755;&#20837;&#26631;&#35760;&#36827;&#34892;&#38750;&#32447;&#24615;&#36716;&#25442;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;FFN&#30340;&#20316;&#29992;&#65292;&#24182;&#21457;&#29616;&#23613;&#31649;&#23427;&#21344;&#25454;&#20102;&#27169;&#22411;&#21442;&#25968;&#30340;&#30456;&#24403;&#22823;&#27604;&#20363;&#65292;&#20294;&#23427;&#26159;&#39640;&#24230;&#20887;&#20313;&#30340;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#36890;&#36807;&#22312;&#35299;&#30721;&#22120;&#23618;&#31227;&#38500;FFN&#24182;&#22312;&#32534;&#30721;&#22120;&#19978;&#20849;&#20139;&#19968;&#20010;&#21333;&#19968;&#30340;FFN&#65292;&#25105;&#20204;&#33021;&#22815;&#26174;&#33879;&#20943;&#23569;&#21442;&#25968;&#30340;&#25968;&#37327;&#65292;&#21482;&#26377;&#36731;&#24494;&#30340;&#20934;&#30830;&#24615;&#19979;&#38477;&#12290;&#26368;&#21518;&#65292;&#36890;&#36807;&#22686;&#21152;&#20849;&#20139;FFN&#30340;&#38544;&#34255;&#32500;&#24230;&#65292;&#25105;&#20204;&#23558;&#27492;&#26550;&#26500;&#32553;&#23567;&#22238;&#21407;&#22987;&#22823;&#23567;&#65292;&#23454;&#29616;&#20102;&#20934;&#30830;&#24615;&#21644;&#24310;&#36831;&#26041;&#38754;&#19982;&#21407;&#22987;Transformer Big&#30456;&#27604;&#30340;&#26174;&#33879;&#22686;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Transformer architecture has two main non-embedding components: Attention and the Feed Forward Network (FFN). Attention captures interdependencies between words regardless of their position, while the FFN non-linearly transforms each input token independently. In this work we explore the role of the FFN, and find that despite taking up a significant fraction of the model's parameters, it is highly redundant. Concretely, we are able to substantially reduce the number of parameters with only a modest drop in accuracy by removing the FFN on the decoder layers and sharing a single FFN across the encoder. Finally we scale this architecture back to its original size by increasing the hidden dimension of the shared FFN, achieving substantial gains in both accuracy and latency with respect to the original Transformer Big.
&lt;/p&gt;</description></item><item><title>&#36807;&#24230;&#21387;&#32553;&#26159;&#22270;&#31070;&#32463;&#32593;&#32476;&#38754;&#20020;&#30340;&#20851;&#38190;&#25361;&#25112;&#65292;&#23427;&#38480;&#21046;&#20102;&#33410;&#28857;&#20043;&#38388;&#30340;&#38271;&#31243;&#20449;&#24687;&#20256;&#36882;&#65292;&#24433;&#21709;&#20102;&#22312;&#38656;&#35201;&#24191;&#27867;&#19978;&#19979;&#25991;&#27934;&#23519;&#21147;&#30340;&#24773;&#20917;&#19979;&#30340;&#20934;&#30830;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2308.15568</link><description>&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#36807;&#24230;&#21387;&#32553;&#38382;&#39064;&#65306;&#19968;&#39033;&#20840;&#38754;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Over-Squashing in Graph Neural Networks: A Comprehensive survey. (arXiv:2308.15568v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15568
&lt;/p&gt;
&lt;p&gt;
&#36807;&#24230;&#21387;&#32553;&#26159;&#22270;&#31070;&#32463;&#32593;&#32476;&#38754;&#20020;&#30340;&#20851;&#38190;&#25361;&#25112;&#65292;&#23427;&#38480;&#21046;&#20102;&#33410;&#28857;&#20043;&#38388;&#30340;&#38271;&#31243;&#20449;&#24687;&#20256;&#36882;&#65292;&#24433;&#21709;&#20102;&#22312;&#38656;&#35201;&#24191;&#27867;&#19978;&#19979;&#25991;&#27934;&#23519;&#21147;&#30340;&#24773;&#20917;&#19979;&#30340;&#20934;&#30830;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#24050;&#25104;&#20026;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#30340;&#19968;&#31181;&#38761;&#21629;&#24615;&#33539; Paradigm&#65292;&#20026;&#20998;&#26512;&#22270;&#32467;&#26500;&#25968;&#25454;&#20013;&#22266;&#26377;&#30340;&#22797;&#26434;&#20851;&#31995;&#25552;&#20379;&#20102;&#19968;&#31181;&#21464;&#38761;&#24615;&#26041;&#27861;&#12290;&#22823;&#22810;&#25968;GNN&#30340;&#22522;&#26412;&#26550;&#26500;&#28041;&#21450;&#36890;&#36807;&#28040;&#24687;&#32858;&#21512;&#21644;&#36716;&#25442;&#22312;&#30456;&#20114;&#36830;&#25509;&#30340;&#33410;&#28857;&#20043;&#38388;&#20256;&#25773;&#20449;&#24687;&#30340;&#26426;&#21046;&#65292;&#22312;&#21253;&#25324;&#33410;&#28857;&#20998;&#31867;&#12289;&#38142;&#25509;&#39044;&#27979;&#21644;&#25512;&#33616;&#31995;&#32479;&#30340;&#21508;&#31181;&#24212;&#29992;&#20013;&#24050;&#32463;&#23637;&#29616;&#20986;&#26174;&#33879;&#30340;&#26377;&#25928;&#24615;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#28508;&#22312;&#23454;&#21147;&#36935;&#21040;&#20102;&#22312;&#38656;&#35201;&#24191;&#27867;&#19978;&#19979;&#25991;&#27934;&#23519;&#21147;&#30340;&#24773;&#20917;&#19979;&#22266;&#26377;&#30340;&#38480;&#21046;&#12290;&#22312;&#26576;&#20123;&#24773;&#22659;&#20013;&#65292;&#20934;&#30830;&#30340;&#39044;&#27979;&#19981;&#20165;&#21462;&#20915;&#20110;&#33410;&#28857;&#30340;&#21363;&#26102;&#23616;&#37096;&#29615;&#22659;&#65292;&#36824;&#21462;&#20915;&#20110;&#36328;&#36234;&#24191;&#22495;&#30340;&#20132;&#20114;&#20316;&#29992;&#12290;&#36825;&#31181;&#22797;&#26434;&#30340;&#23545;&#38271;&#31243;&#20449;&#24687;&#20256;&#25773;&#30340;&#38656;&#27714;&#26292;&#38706;&#20102;&#19968;&#20010;&#34987;&#31216;&#20026;&#8220;&#36807;&#24230;&#21387;&#32553;&#8221;&#30340;&#20851;&#38190;&#25361;&#25112;&#65292;&#20854;&#20013;&#26469;&#33258;&#36828;&#31163;&#33410;&#28857;&#30340;&#20449;&#24687;&#27969;&#30340;&#21487;&#38752;&#24615;&#21463;&#21040;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) have emerged as a revolutionary paradigm in the realm of machine learning, offering a transformative approach to dissect intricate relationships inherent in graph-structured data. The foundational architecture of most GNNs involves the dissemination of information through message aggregation and transformation among interconnected nodes, a mechanism that has demonstrated remarkable efficacy across diverse applications encompassing node classification, link prediction, and recommendation systems. Nonetheless, their potential prowess encounters a restraint intrinsic to scenarios necessitating extensive contextual insights. In certain contexts, accurate predictions hinge not only upon a node's immediate local surroundings but also on interactions spanning far-reaching domains. This intricate demand for long-range information dissemination exposes a pivotal challenge recognized as "over-squashing," wherein the fidelity of information flow from distant nodes bec
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MISSRec&#30340;&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;&#21644;&#36716;&#31227;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#25506;&#32034;&#22810;&#27169;&#24577;&#20449;&#24687;&#30340;&#28508;&#21147;&#65292;&#35299;&#20915;&#20102;&#24207;&#21015;&#25512;&#33616;&#20013;&#30340;&#31232;&#30095;ID&#21644;&#20919;&#21551;&#21160;&#38382;&#39064;&#65292;&#24182;&#25552;&#21319;&#20102;&#25512;&#33616;&#27169;&#22411;&#30340;&#21487;&#36716;&#31227;&#24615;&#21644;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.11175</link><description>&lt;p&gt;
MISSRec: &#38754;&#21521;&#25512;&#33616;&#30340;&#39044;&#35757;&#32451;&#21644;&#36716;&#31227;&#22810;&#27169;&#24577;&#20852;&#36259;&#24863;&#30693;&#24207;&#21015;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
MISSRec: Pre-training and Transferring Multi-modal Interest-aware Sequence Representation for Recommendation. (arXiv:2308.11175v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11175
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MISSRec&#30340;&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;&#21644;&#36716;&#31227;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#25506;&#32034;&#22810;&#27169;&#24577;&#20449;&#24687;&#30340;&#28508;&#21147;&#65292;&#35299;&#20915;&#20102;&#24207;&#21015;&#25512;&#33616;&#20013;&#30340;&#31232;&#30095;ID&#21644;&#20919;&#21551;&#21160;&#38382;&#39064;&#65292;&#24182;&#25552;&#21319;&#20102;&#25512;&#33616;&#27169;&#22411;&#30340;&#21487;&#36716;&#31227;&#24615;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24207;&#21015;&#25512;&#33616;&#30340;&#30446;&#26631;&#26159;&#22522;&#20110;&#29992;&#25143;&#30340;&#21382;&#21490;&#20132;&#20114;&#24207;&#21015;&#39044;&#27979;&#20854;&#21487;&#33021;&#24863;&#20852;&#36259;&#30340;&#29289;&#21697;&#12290;&#22823;&#37096;&#20998;&#29616;&#26377;&#30340;&#24207;&#21015;&#25512;&#33616;&#22120;&#26159;&#22522;&#20110;ID&#29305;&#24449;&#24320;&#21457;&#30340;&#65292;&#28982;&#32780;&#22312;&#20351;&#29992;&#31232;&#30095;ID&#26102;&#24448;&#24448;&#34920;&#29616;&#19981;&#20339;&#65292;&#24182;&#19988;&#22312;&#20919;&#21551;&#21160;&#38382;&#39064;&#19978;&#36935;&#21040;&#22256;&#38590;&#12290;&#27492;&#22806;&#65292;&#19981;&#19968;&#33268;&#30340;ID&#26144;&#23556;&#38480;&#21046;&#20102;&#27169;&#22411;&#30340;&#21487;&#36716;&#31227;&#24615;&#65292;&#20351;&#24471;&#30456;&#20284;&#30340;&#25512;&#33616;&#39046;&#22495;&#26080;&#27861;&#36827;&#34892;&#20849;&#21516;&#20248;&#21270;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#25506;&#32034;&#22810;&#27169;&#24577;&#20449;&#24687;&#30340;&#28508;&#21147;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;MISSRec&#65292;&#19968;&#31181;&#38754;&#21521;SR&#30340;&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;&#21644;&#36716;&#31227;&#23398;&#20064;&#26694;&#26550;&#12290;&#22312;&#29992;&#25143;&#31471;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#22522;&#20110;Transformer&#30340;&#32534;&#30721;-&#35299;&#30721;&#27169;&#22411;&#65292;&#20854;&#20013;&#19978;&#19979;&#25991;&#32534;&#30721;&#22120;&#23398;&#20064;&#25429;&#25417;&#24207;&#21015;&#32423;&#30340;&#22810;&#27169;&#24577;&#21327;&#21516;&#20316;&#29992;&#65292;&#32780;&#26032;&#39062;&#30340;&#20852;&#36259;&#24863;&#30693;&#35299;&#30721;&#22120;&#21017;&#29992;&#20110;&#25226;&#25569;&#29289;&#21697;-&#27169;&#24577;-&#20852;&#36259;&#20851;&#31995;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#24207;&#21015;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
The goal of sequential recommendation (SR) is to predict a user's potential interested items based on her/his historical interaction sequences. Most existing sequential recommenders are developed based on ID features, which, despite their widespread use, often underperform with sparse IDs and struggle with the cold-start problem. Besides, inconsistent ID mappings hinder the model's transferability, isolating similar recommendation domains that could have been co-optimized. This paper aims to address these issues by exploring the potential of multi-modal information in learning robust and generalizable sequence representations. We propose MISSRec, a multi-modal pre-training and transfer learning framework for SR. On the user side, we design a Transformer-based encoder-decoder model, where the contextual encoder learns to capture the sequence-level multi-modal synergy while a novel interest-aware decoder is developed to grasp item-modality-interest relations for better sequence represent
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#36125;&#21494;&#26031;&#27969;&#32593;&#32476;&#65288;BFNs&#65289;&#65292;&#19968;&#31181;&#26032;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#23427;&#36890;&#36807;&#36125;&#21494;&#26031;&#25512;&#26029;&#20462;&#25913;&#20102;&#19968;&#32452;&#29420;&#31435;&#20998;&#24067;&#30340;&#21442;&#25968;&#65292;&#24182;&#23558;&#20854;&#20316;&#20026;&#36755;&#20837;&#20256;&#36882;&#32473;&#31070;&#32463;&#32593;&#32476;&#26469;&#29983;&#25104;&#21478;&#19968;&#20010;&#30456;&#20114;&#20381;&#36182;&#30340;&#20998;&#24067;&#12290;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#21069;&#21521;&#36807;&#31243;&#65292;&#36866;&#29992;&#20110;&#36830;&#32493;&#21644;&#31163;&#25955;&#25968;&#25454;&#65292;&#24182;&#20855;&#26377;&#20248;&#21270;&#25968;&#25454;&#21387;&#32553;&#30340;&#21151;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.07037</link><description>&lt;p&gt;
&#36125;&#21494;&#26031;&#27969;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Bayesian Flow Networks. (arXiv:2308.07037v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07037
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#36125;&#21494;&#26031;&#27969;&#32593;&#32476;&#65288;BFNs&#65289;&#65292;&#19968;&#31181;&#26032;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#23427;&#36890;&#36807;&#36125;&#21494;&#26031;&#25512;&#26029;&#20462;&#25913;&#20102;&#19968;&#32452;&#29420;&#31435;&#20998;&#24067;&#30340;&#21442;&#25968;&#65292;&#24182;&#23558;&#20854;&#20316;&#20026;&#36755;&#20837;&#20256;&#36882;&#32473;&#31070;&#32463;&#32593;&#32476;&#26469;&#29983;&#25104;&#21478;&#19968;&#20010;&#30456;&#20114;&#20381;&#36182;&#30340;&#20998;&#24067;&#12290;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#21069;&#21521;&#36807;&#31243;&#65292;&#36866;&#29992;&#20110;&#36830;&#32493;&#21644;&#31163;&#25955;&#25968;&#25454;&#65292;&#24182;&#20855;&#26377;&#20248;&#21270;&#25968;&#25454;&#21387;&#32553;&#30340;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#36125;&#21494;&#26031;&#27969;&#32593;&#32476;&#65288;BFNs&#65289;&#65292;&#19968;&#31181;&#26032;&#30340;&#29983;&#25104;&#27169;&#22411;&#12290;&#22312;BFNs&#20013;&#65292;&#29420;&#31435;&#20998;&#24067;&#30340;&#21442;&#25968;&#20250;&#22312;&#22024;&#26434;&#30340;&#25968;&#25454;&#26679;&#26412;&#30340;&#24433;&#21709;&#19979;&#36890;&#36807;&#36125;&#21494;&#26031;&#25512;&#26029;&#36827;&#34892;&#20462;&#25913;&#65292;&#28982;&#21518;&#20316;&#20026;&#36755;&#20837;&#20256;&#36882;&#32473;&#31070;&#32463;&#32593;&#32476;&#65292;&#35813;&#31070;&#32463;&#32593;&#32476;&#36755;&#20986;&#19968;&#20010;&#30456;&#20114;&#20381;&#36182;&#30340;&#20998;&#24067;&#12290;&#20174;&#31616;&#21333;&#30340;&#20808;&#39564;&#24320;&#22987;&#65292;&#36890;&#36807;&#36845;&#20195;&#26356;&#26032;&#36825;&#20004;&#20010;&#20998;&#24067;&#21487;&#20197;&#24471;&#21040;&#19968;&#20010;&#31867;&#20284;&#20110;&#25193;&#25955;&#27169;&#22411;&#21453;&#21521;&#36807;&#31243;&#30340;&#29983;&#25104;&#36807;&#31243;&#65307;&#19981;&#36807;&#65292;&#36825;&#20010;&#36807;&#31243;&#22312;&#27010;&#24565;&#19978;&#26356;&#31616;&#21333;&#65292;&#26080;&#38656;&#21069;&#21521;&#36807;&#31243;&#12290;&#23545;&#20110;&#36830;&#32493;&#12289;&#31163;&#25955;&#21270;&#21644;&#31163;&#25955;&#25968;&#25454;&#65292;&#25512;&#23548;&#20986;&#20102;&#31163;&#25955;&#21644;&#36830;&#32493;&#26102;&#38388;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#20197;&#21450;&#26679;&#26412;&#29983;&#25104;&#36807;&#31243;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#23545;&#20110;&#31163;&#25955;&#25968;&#25454;&#65292;&#32593;&#32476;&#30340;&#36755;&#20837;&#20301;&#20110;&#27010;&#29575;&#21333;&#32431;&#24418;&#19978;&#65292;&#22240;&#27492;&#26412;&#36136;&#19978;&#26159;&#21487;&#24494;&#20998;&#30340;&#65292;&#20026;&#22522;&#20110;&#26799;&#24230;&#30340;&#26679;&#26412;&#24341;&#23548;&#21644;&#22312;&#35821;&#35328;&#24314;&#27169;&#31561;&#31163;&#25955;&#39046;&#22495;&#36827;&#34892;&#23569;&#37327;&#27493;&#39588;&#29983;&#25104;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;&#25439;&#22833;&#20989;&#25968;&#30452;&#25509;&#20248;&#21270;&#20102;&#25968;&#25454;&#21387;&#32553;&#65292;&#24182;&#19988;&#19981;&#25918;&#32622;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces Bayesian Flow Networks (BFNs), a new class of generative model in which the parameters of a set of independent distributions are modified with Bayesian inference in the light of noisy data samples, then passed as input to a neural network that outputs a second, interdependent distribution. Starting from a simple prior and iteratively updating the two distributions yields a generative procedure similar to the reverse process of diffusion models; however it is conceptually simpler in that no forward process is required. Discrete and continuous-time loss functions are derived for continuous, discretised and discrete data, along with sample generation procedures. Notably, the network inputs for discrete data lie on the probability simplex, and are therefore natively differentiable, paving the way for gradient-based sample guidance and few-step generation in discrete domains such as language modelling. The loss function directly optimises data compression and places no
&lt;/p&gt;</description></item><item><title>PDE-Refiner &#26159;&#19968;&#31181;&#21033;&#29992;&#22810;&#27493;&#32454;&#21270;&#36807;&#31243;&#20934;&#30830;&#24314;&#27169;&#25152;&#26377;&#39057;&#29575;&#20998;&#37327;&#30340;&#31070;&#32463;PDE&#27714;&#35299;&#22120;&#65292;&#33021;&#22815;&#22312;&#38271;&#26102;&#38388;&#33539;&#22260;&#20869;&#25552;&#20379;&#31283;&#23450;&#12289;&#20934;&#30830;&#30340;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2308.05732</link><description>&lt;p&gt;
PDE-Refiner: &#21033;&#29992;&#31070;&#32463;PDE&#27714;&#35299;&#22120;&#23454;&#29616;&#20934;&#30830;&#30340;&#38271;&#26102;&#38388;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
PDE-Refiner: Achieving Accurate Long Rollouts with Neural PDE Solvers. (arXiv:2308.05732v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05732
&lt;/p&gt;
&lt;p&gt;
PDE-Refiner &#26159;&#19968;&#31181;&#21033;&#29992;&#22810;&#27493;&#32454;&#21270;&#36807;&#31243;&#20934;&#30830;&#24314;&#27169;&#25152;&#26377;&#39057;&#29575;&#20998;&#37327;&#30340;&#31070;&#32463;PDE&#27714;&#35299;&#22120;&#65292;&#33021;&#22815;&#22312;&#38271;&#26102;&#38388;&#33539;&#22260;&#20869;&#25552;&#20379;&#31283;&#23450;&#12289;&#20934;&#30830;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#30456;&#20851;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#22312;&#31185;&#23398;&#21644;&#24037;&#31243;&#20013;&#38750;&#24120;&#26222;&#36941;&#12290;&#26368;&#36817;&#65292;&#30001;&#20110;&#20256;&#32479;&#35299;&#27861;&#30340;&#39640;&#35745;&#31639;&#25104;&#26412;&#65292;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#26367;&#20195;&#26041;&#27861;&#24341;&#36215;&#20102;&#26497;&#22823;&#20851;&#27880;&#12290;&#36825;&#20123;&#31070;&#32463;PDE&#27714;&#35299;&#22120;&#30340;&#23454;&#29992;&#20215;&#20540;&#20381;&#36182;&#20110;&#23427;&#20204;&#33021;&#22815;&#22312;&#38271;&#26102;&#38388;&#33539;&#22260;&#20869;&#25552;&#20379;&#20934;&#30830;&#12289;&#31283;&#23450;&#30340;&#39044;&#27979;&#65292;&#36825;&#26159;&#19968;&#20010;&#30456;&#24403;&#22256;&#38590;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23545;&#24120;&#35265;&#30340;&#26102;&#38388;&#23637;&#24320;&#31574;&#30053;&#36827;&#34892;&#20102;&#22823;&#35268;&#27169;&#20998;&#26512;&#65292;&#21457;&#29616;&#24573;&#30053;&#38750;&#20027;&#23548;&#31354;&#38388;&#39057;&#29575;&#20449;&#24687;&#65288;&#36890;&#24120;&#19982;PDE&#35299;&#20013;&#30340;&#39640;&#39057;&#29575;&#30456;&#20851;&#65289;&#26159;&#38480;&#21046;&#31283;&#23450;&#12289;&#20934;&#30830;&#23637;&#24320;&#24615;&#33021;&#30340;&#20027;&#35201;&#38519;&#38449;&#12290;&#22522;&#20110;&#36825;&#20123;&#27934;&#23519;&#65292;&#25105;&#20204;&#20511;&#37492;&#20102;&#25193;&#25955;&#27169;&#22411;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#24341;&#20837;&#20102;PDE-Refiner&#65307;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22411;&#31867;&#21035;&#65292;&#36890;&#36807;&#22810;&#27493;&#32454;&#21270;&#36807;&#31243;&#23454;&#29616;&#23545;&#25152;&#26377;&#39057;&#29575;&#20998;&#37327;&#30340;&#26356;&#20934;&#30830;&#24314;&#27169;&#12290;&#25105;&#20204;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#39564;&#35777;&#20102;PDE-Refiner&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Time-dependent partial differential equations (PDEs) are ubiquitous in science and engineering. Recently, mostly due to the high computational cost of traditional solution techniques, deep neural network based surrogates have gained increased interest. The practical utility of such neural PDE solvers relies on their ability to provide accurate, stable predictions over long time horizons, which is a notoriously hard problem. In this work, we present a large-scale analysis of common temporal rollout strategies, identifying the neglect of non-dominant spatial frequency information, often associated with high frequencies in PDE solutions, as the primary pitfall limiting stable, accurate rollout performance. Based on these insights, we draw inspiration from recent advances in diffusion models to introduce PDE-Refiner; a novel model class that enables more accurate modeling of all frequency components via a multistep refinement process. We validate PDE-Refiner on challenging benchmarks of co
&lt;/p&gt;</description></item><item><title>SSLRec&#26159;&#19968;&#20010;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#25512;&#33616;&#31995;&#32479;&#24211;&#65292;&#20026;&#35780;&#20272;&#21508;&#31181;SSL&#22686;&#24378;&#25512;&#33616;&#31995;&#32479;&#25552;&#20379;&#20102;&#26631;&#20934;&#21270;&#12289;&#28789;&#27963;&#21644;&#32508;&#21512;&#30340;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2308.05697</link><description>&lt;p&gt;
SSLRec: &#19968;&#20010;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#25512;&#33616;&#31995;&#32479;&#24211;
&lt;/p&gt;
&lt;p&gt;
SSLRec: A Self-Supervised Learning Library for Recommendation. (arXiv:2308.05697v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05697
&lt;/p&gt;
&lt;p&gt;
SSLRec&#26159;&#19968;&#20010;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#25512;&#33616;&#31995;&#32479;&#24211;&#65292;&#20026;&#35780;&#20272;&#21508;&#31181;SSL&#22686;&#24378;&#25512;&#33616;&#31995;&#32479;&#25552;&#20379;&#20102;&#26631;&#20934;&#21270;&#12289;&#28789;&#27963;&#21644;&#32508;&#21512;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#20316;&#20026;&#35299;&#20915;&#25512;&#33616;&#31995;&#32479;&#20013;&#31232;&#30095;&#21644;&#22122;&#22768;&#25968;&#25454;&#25361;&#25112;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#22312;&#26368;&#36817;&#20960;&#24180;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#23613;&#31649;&#35774;&#35745;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;SSL&#31639;&#27861;&#26469;&#22312;&#19981;&#21516;&#39046;&#22495;&#20013;&#25552;&#20379;&#26368;&#20808;&#36827;&#30340;&#25512;&#33616;&#24615;&#33021;&#65288;&#20363;&#22914;&#22270;&#21327;&#21516;&#36807;&#28388;&#12289;&#39034;&#24207;&#25512;&#33616;&#12289;&#31038;&#20132;&#25512;&#33616;&#12289;&#30693;&#35782;&#22270;&#22686;&#24378;&#25512;&#33616;&#65289;&#65292;&#20294;&#30446;&#21069;&#20173;&#32570;&#20047;&#19968;&#20010;&#32479;&#19968;&#26694;&#26550;&#26469;&#25972;&#21512;&#19981;&#21516;&#39046;&#22495;&#30340;&#25512;&#33616;&#31639;&#27861;&#12290;&#36825;&#26679;&#30340;&#26694;&#26550;&#21487;&#20197;&#20316;&#20026;&#33258;&#30417;&#30563;&#25512;&#33616;&#31639;&#27861;&#30340;&#22522;&#30707;&#65292;&#32479;&#19968;&#29616;&#26377;&#26041;&#27861;&#30340;&#39564;&#35777;&#65292;&#24182;&#25512;&#21160;&#26032;&#26041;&#27861;&#30340;&#35774;&#35745;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;SSLRec&#65292;&#19968;&#20010;&#26032;&#39062;&#30340;&#22522;&#20934;&#24179;&#21488;&#65292;&#20026;&#35780;&#20272;&#21508;&#31181;SSL&#22686;&#24378;&#25512;&#33616;&#31995;&#32479;&#25552;&#20379;&#20102;&#26631;&#20934;&#21270;&#12289;&#28789;&#27963;&#21644;&#32508;&#21512;&#30340;&#26694;&#26550;&#12290;SSLRec&#24211;&#20855;&#26377;&#27169;&#22359;&#21270;&#26550;&#26500;&#65292;&#21487;&#20197;&#26041;&#20415;&#29992;&#25143;&#35780;&#20272;&#26368;&#20808;&#36827;&#30340;&#25512;&#33616;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning (SSL) has gained significant interest in recent years as a solution to address the challenges posed by sparse and noisy data in recommender systems. Despite the growing number of SSL algorithms designed to provide state-of-the-art performance in various recommendation scenarios (e.g., graph collaborative filtering, sequential recommendation, social recommendation, KG-enhanced recommendation), there is still a lack of unified frameworks that integrate recommendation algorithms across different domains. Such a framework could serve as the cornerstone for self-supervised recommendation algorithms, unifying the validation of existing methods and driving the design of new ones. To address this gap, we introduce SSLRec, a novel benchmark platform that provides a standardized, flexible, and comprehensive framework for evaluating various SSL-enhanced recommenders. The SSLRec library features a modular architecture that allows users to easily evaluate state-of-the-art m
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#22312;GNN&#27169;&#22411;&#20013;&#22686;&#24378;&#21487;&#35299;&#37322;&#24615;&#65292;&#36890;&#36807;&#24341;&#20837;&#35821;&#20041;&#20851;&#27880;&#21644;&#24314;&#31435;&#29305;&#24449;&#37325;&#35201;&#24615;&#26435;&#37325;&#19982;&#27169;&#22411;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#36825;&#23545;&#20110;&#22270;&#28145;&#24230;&#23398;&#20064;&#20219;&#21153;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2308.04220</link><description>&lt;p&gt;
&#23545;GNN&#27169;&#22411;&#22522;&#20110;&#22270;Attention&#30340;&#35299;&#37322;&#30340;&#35821;&#20041;&#35299;&#37322;&#21644;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
Semantic Interpretation and Validation of Graph Attention-based Explanations for GNN Models. (arXiv:2308.04220v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04220
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#22312;GNN&#27169;&#22411;&#20013;&#22686;&#24378;&#21487;&#35299;&#37322;&#24615;&#65292;&#36890;&#36807;&#24341;&#20837;&#35821;&#20041;&#20851;&#27880;&#21644;&#24314;&#31435;&#29305;&#24449;&#37325;&#35201;&#24615;&#26435;&#37325;&#19982;&#27169;&#22411;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#36825;&#23545;&#20110;&#22270;&#28145;&#24230;&#23398;&#20064;&#20219;&#21153;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#30740;&#31350;&#22312;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#27169;&#22411;&#20013;&#24212;&#29992;&#35821;&#20041;&#20851;&#27880;&#20197;&#22686;&#24378;&#21487;&#35299;&#37322;&#24615;&#65292;&#24341;&#20837;&#35821;&#20041;&#20449;&#24687;&#30340;&#25200;&#21160;&#65292;&#24182;&#24314;&#31435;&#39044;&#27979;&#29305;&#24449;&#37325;&#35201;&#24615;&#26435;&#37325;&#19982;&#27169;&#22411;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#22270;&#28145;&#24230;&#23398;&#20064;&#65288;GDL&#65289;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#24212;&#29992;&#20110;&#22330;&#26223;&#35299;&#37322;&#31561;&#20219;&#21153;&#30340;&#26377;&#21069;&#36884;&#30340;&#39046;&#22495;&#65292;&#21033;&#29992;&#28789;&#27963;&#30340;&#22270;&#32467;&#26500;&#26469;&#31616;&#27905;&#22320;&#25551;&#36848;&#22797;&#26434;&#30340;&#29305;&#24449;&#21644;&#20851;&#31995;&#12290;&#30001;&#20110;&#20256;&#32479;&#30340;&#35299;&#37322;&#24615;AI&#65288;XAI&#65289;&#20013;&#20351;&#29992;&#30340;&#35299;&#37322;&#26041;&#27861;&#19981;&#33021;&#30452;&#25509;&#24212;&#29992;&#20110;&#36825;&#31181;&#32467;&#26500;&#65292;&#22240;&#27492;&#24341;&#20837;&#20102;&#22270;&#29305;&#23450;&#30340;&#26041;&#27861;&#12290;&#27880;&#24847;&#21147;&#26426;&#21046;&#22312;&#20272;&#35745;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20013;&#36755;&#20837;&#29305;&#24449;&#30340;&#37325;&#35201;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#24456;&#22909;&#30340;&#25928;&#26524;&#65292;&#22240;&#27492;&#20808;&#21069;&#24050;&#32463;&#20351;&#29992;&#23427;&#20204;&#20026;GNN&#39044;&#27979;&#25552;&#20379;&#22522;&#20110;&#29305;&#24449;&#30340;&#35299;&#37322;&#12290;&#22522;&#20110;&#36825;&#20123;&#35265;&#35299;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;&#29616;&#26377;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#22270;&#35299;&#37322;&#26041;&#27861;&#65292;&#30740;&#31350;&#20102;&#20351;&#29992;&#35821;&#20041;&#20449;&#24687;&#30340;&#22270;Attention&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we propose a methodology for investigating the application of semantic attention to enhance the explainability of Graph Neural Network (GNN)-based models, introducing semantically-informed perturbations and establishing a correlation between predicted feature-importance weights and model accuracy. Graph Deep Learning (GDL) has emerged as a promising field for tasks like scene interpretation, leveraging flexible graph structures to concisely describe complex features and relationships. As traditional explainability methods used in eXplainable AI (XAI) cannot be directly applied to such structures, graph-specific approaches are introduced. Attention mechanisms have demonstrated their efficacy in estimating the importance of input features in deep learning models and thus have been previously employed to provide feature-based explanations for GNN predictions. Building upon these insights, we extend existing attention-based graph-explainability methods investigating the use o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#35821;&#20041;&#21644;&#23616;&#37096;&#20960;&#20309;&#20449;&#24687;&#25351;&#23548;&#21487;&#38752;&#30340;&#28857;&#20113;&#27880;&#20876;&#65292;&#23454;&#29616;&#31934;&#30830;&#30340;&#28608;&#20809;&#38647;&#36798;&#23039;&#24577;&#20272;&#35745;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#38745;&#24577;&#22270;&#32467;&#26500;&#21644;&#36328;&#22270;&#27880;&#24847;&#21147;&#65292;&#26377;&#25928;&#20943;&#23569;&#20102;&#28857;&#20113;&#27880;&#20876;&#30340;&#35745;&#31639;&#36127;&#25285;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#20869;&#30465;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2308.03718</link><description>&lt;p&gt;
SEM-GAT: &#20351;&#29992;&#23398;&#20064;&#21040;&#30340;&#22270;&#27880;&#24847;&#21147;&#36827;&#34892;&#21487;&#35299;&#37322;&#30340;&#35821;&#20041;&#23039;&#24577;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
SEM-GAT: Explainable Semantic Pose Estimation using Learned Graph Attention. (arXiv:2308.03718v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03718
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#35821;&#20041;&#21644;&#23616;&#37096;&#20960;&#20309;&#20449;&#24687;&#25351;&#23548;&#21487;&#38752;&#30340;&#28857;&#20113;&#27880;&#20876;&#65292;&#23454;&#29616;&#31934;&#30830;&#30340;&#28608;&#20809;&#38647;&#36798;&#23039;&#24577;&#20272;&#35745;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#38745;&#24577;&#22270;&#32467;&#26500;&#21644;&#36328;&#22270;&#27880;&#24847;&#21147;&#65292;&#26377;&#25928;&#20943;&#23569;&#20102;&#28857;&#20113;&#27880;&#20876;&#30340;&#35745;&#31639;&#36127;&#25285;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#20869;&#30465;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;(GNN)&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#35821;&#20041;&#21644;&#23616;&#37096;&#20960;&#20309;&#20449;&#24687;&#25351;&#23548;&#21487;&#38752;&#30340;&#28857;&#20113;&#27880;&#20876;&#20505;&#36873;&#28857;&#30340;&#30830;&#23450;&#12290;&#29615;&#22659;&#30340;&#35821;&#20041;&#21644;&#24418;&#24577;&#23398;&#29305;&#24449;&#20316;&#20026;&#27880;&#20876;&#30340;&#20851;&#38190;&#21442;&#32771;&#28857;&#65292;&#23454;&#29616;&#20102;&#31934;&#30830;&#30340;&#28608;&#20809;&#38647;&#36798;&#23039;&#24577;&#20272;&#35745;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#38745;&#24577;&#22270;&#32467;&#26500;&#65292;&#36890;&#36807;&#35782;&#21035;&#35821;&#20041;-&#23454;&#20363;&#20851;&#31995;&#65292;&#20026;&#27880;&#24847;&#21147;&#32858;&#21512;&#32593;&#32476;&#25552;&#20379;&#25351;&#23548;&#65292;&#26174;&#33879;&#20943;&#23569;&#20102;&#28857;&#20113;&#27880;&#20876;&#30340;&#35745;&#31639;&#36127;&#25285;&#12290;&#36890;&#36807;&#36830;&#25509;&#20505;&#36873;&#33410;&#28857;&#24182;&#21033;&#29992;&#36328;&#22270;&#27880;&#24847;&#21147;&#65292;&#25105;&#20204;&#35782;&#21035;&#20102;&#25152;&#26377;&#21487;&#33021;&#27880;&#20876;&#23545;&#24212;&#30340;&#32622;&#20449;&#24230;&#20998;&#25968;&#65292;&#24182;&#20272;&#35745;&#28857;&#20113;&#25195;&#25551;&#20043;&#38388;&#30340;&#20301;&#31227;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#36890;&#36807;&#23558;&#27169;&#22411;&#30340;&#34920;&#29616;&#19982;&#29615;&#22659;&#20013;&#23616;&#37096;&#32467;&#26500;&#30340;&#20010;&#20307;&#36129;&#29486;&#30456;&#20851;&#32852;&#65292;&#23454;&#29616;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#20869;&#30465;&#20998;&#26512;&#65292;&#20174;&#32780;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#27934;&#23519;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a Graph Neural Network(GNN)-based method for exploiting semantics and local geometry to guide the identification of reliable pointcloud registration candidates. Semantic and morphological features of the environment serve as key reference points for registration, enabling accurate lidar-based pose estimation. Our novel lightweight static graph structure informs our attention-based node aggregation network by identifying semantic-instance relationships, acting as an inductive bias to significantly reduce the computational burden of pointcloud registration. By connecting candidate nodes and exploiting cross-graph attention, we identify confidence scores for all potential registration correspondences and estimate the displacement between pointcloud scans. Our pipeline enables introspective analysis of the model's performance by correlating it with the individual contributions of local structures in the environment, providing valuable insights into the system's behaviou
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;AI&#20195;&#29702;&#29992;&#20110;&#20219;&#21153;&#35268;&#21010;&#21644;&#24037;&#20855;&#20351;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32467;&#26500;&#26694;&#26550;&#65292;&#35774;&#35745;&#20102;&#20004;&#31181;&#20195;&#29702;&#26469;&#25191;&#34892;&#25512;&#29702;&#36807;&#31243;&#65292;&#23454;&#20363;&#21270;&#20102;&#26694;&#26550;&#65292;&#24182;&#35780;&#20272;&#20102;&#23427;&#20204;&#30340;&#20219;&#21153;&#35268;&#21010;&#21644;&#24037;&#20855;&#20351;&#29992;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.03427</link><description>&lt;p&gt;
TPTU: &#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;AI&#20195;&#29702;&#29992;&#20110;&#20219;&#21153;&#35268;&#21010;&#21644;&#24037;&#20855;&#20351;&#29992;
&lt;/p&gt;
&lt;p&gt;
TPTU: Large Language Model-based AI Agents for Task Planning and Tool Usage. (arXiv:2308.03427v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03427
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;AI&#20195;&#29702;&#29992;&#20110;&#20219;&#21153;&#35268;&#21010;&#21644;&#24037;&#20855;&#20351;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32467;&#26500;&#26694;&#26550;&#65292;&#35774;&#35745;&#20102;&#20004;&#31181;&#20195;&#29702;&#26469;&#25191;&#34892;&#25512;&#29702;&#36807;&#31243;&#65292;&#23454;&#20363;&#21270;&#20102;&#26694;&#26550;&#65292;&#24182;&#35780;&#20272;&#20102;&#23427;&#20204;&#30340;&#20219;&#21153;&#35268;&#21010;&#21644;&#24037;&#20855;&#20351;&#29992;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#32463;&#25104;&#20026;&#21508;&#31181;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;&#23613;&#31649;&#23427;&#20204;&#38750;&#24120;&#24378;&#22823;&#65292;&#20294;&#26159;LLMs&#30340;&#20869;&#22312;&#29983;&#25104;&#33021;&#21147;&#21487;&#33021;&#19981;&#36275;&#20197;&#22788;&#29702;&#22797;&#26434;&#30340;&#20219;&#21153;&#65292;&#36825;&#20123;&#20219;&#21153;&#38656;&#35201;&#32467;&#21512;&#20219;&#21153;&#35268;&#21010;&#21644;&#22806;&#37096;&#24037;&#20855;&#30340;&#20351;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#20010;&#19987;&#38376;&#20026;LLM-based AI Agents&#37327;&#36523;&#23450;&#21046;&#30340;&#32467;&#26500;&#26694;&#26550;&#65292;&#24182;&#35752;&#35770;&#20102;&#22788;&#29702;&#22797;&#26434;&#38382;&#39064;&#25152;&#24517;&#38656;&#30340;&#20851;&#38190;&#33021;&#21147;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#20869;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#20004;&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#20195;&#29702;&#65288;&#21363;&#19968;&#27493;&#20195;&#29702;&#21644;&#36830;&#32493;&#20195;&#29702;&#65289;&#26469;&#25191;&#34892;&#25512;&#29702;&#36807;&#31243;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#21508;&#31181;LLMs&#23454;&#20363;&#21270;&#20102;&#36825;&#20010;&#26694;&#26550;&#65292;&#24182;&#35780;&#20272;&#20102;&#23427;&#20204;&#22312;&#20856;&#22411;&#20219;&#21153;&#20013;&#30340;&#20219;&#21153;&#35268;&#21010;&#21644;&#24037;&#20855;&#20351;&#29992;&#33021;&#21147;&#12290;&#36890;&#36807;&#24378;&#35843;&#20851;&#38190;&#21457;&#29616;&#21644;&#25361;&#25112;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#20026;&#30740;&#31350;&#20154;&#21592;&#21644;&#23454;&#36341;&#32773;&#25552;&#20379;&#19968;&#20010;&#26377;&#21161;&#20110;&#22312;&#20182;&#20204;&#30340;AI&#24212;&#29992;&#20013;&#21457;&#25381;LLMs&#33021;&#21147;&#30340;&#26377;&#29992;&#36164;&#28304;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#24378;&#35843;&#20102;
&lt;/p&gt;
&lt;p&gt;
With recent advancements in natural language processing, Large Language Models (LLMs) have emerged as powerful tools for various real-world applications. Despite their prowess, the intrinsic generative abilities of LLMs may prove insufficient for handling complex tasks which necessitate a combination of task planning and the usage of external tools. In this paper, we first propose a structured framework tailored for LLM-based AI Agents and discuss the crucial capabilities necessary for tackling intricate problems. Within this framework, we design two distinct types of agents (i.e., one-step agent and sequential agent) to execute the inference process. Subsequently, we instantiate the framework using various LLMs and evaluate their Task Planning and Tool Usage (TPTU) abilities on typical tasks. By highlighting key findings and challenges, our goal is to provide a helpful resource for researchers and practitioners to leverage the power of LLMs in their AI applications. Our study emphasiz
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#20302;&#24310;&#36831;&#35821;&#38899;&#32763;&#35793;&#30340;&#21508;&#20010;&#26041;&#38754;&#12290;&#36890;&#36807;&#35813;&#26694;&#26550;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#19981;&#21516;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#24182;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2308.03415</link><description>&lt;p&gt;
&#20302;&#24310;&#36831;&#21516;&#26102;&#35821;&#38899;&#32763;&#35793;&#30340;&#31471;&#21040;&#31471;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
End-to-End Evaluation for Low-Latency Simultaneous Speech Translation. (arXiv:2308.03415v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03415
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#20302;&#24310;&#36831;&#35821;&#38899;&#32763;&#35793;&#30340;&#21508;&#20010;&#26041;&#38754;&#12290;&#36890;&#36807;&#35813;&#26694;&#26550;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#19981;&#21516;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#24182;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20302;&#24310;&#36831;&#35821;&#38899;&#32763;&#35793;&#30340;&#25361;&#25112;&#24341;&#36215;&#20102;&#30740;&#31350;&#30028;&#30340;&#24191;&#27867;&#20851;&#27880;&#65292;&#35768;&#22810;&#20986;&#29256;&#29289;&#21644;&#20849;&#20139;&#20219;&#21153;&#20063;&#35777;&#26126;&#20102;&#36825;&#19968;&#28857;&#12290;&#22240;&#27492;&#65292;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#35780;&#20272;&#36825;&#20123;&#19981;&#21516;&#30340;&#26041;&#27861;&#38750;&#24120;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#21482;&#26377;&#31995;&#32479;&#30340;&#29305;&#23450;&#26041;&#38754;&#34987;&#35780;&#20272;&#65292;&#24182;&#19988;&#24448;&#24448;&#26080;&#27861;&#27604;&#36739;&#19981;&#21516;&#30340;&#26041;&#27861;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#22312;&#23454;&#38469;&#26465;&#20214;&#19979;&#25191;&#34892;&#21644;&#35780;&#20272;&#20302;&#24310;&#36831;&#35821;&#38899;&#32763;&#35793;&#21508;&#20010;&#26041;&#38754;&#30340;&#26694;&#26550;&#12290;&#35780;&#20272;&#26159;&#20197;&#31471;&#21040;&#31471;&#30340;&#26041;&#24335;&#36827;&#34892;&#30340;&#65292;&#21253;&#25324;&#38899;&#39057;&#30340;&#20998;&#27573;&#20197;&#21450;&#19981;&#21516;&#32452;&#25104;&#37096;&#20998;&#30340;&#36816;&#34892;&#26102;&#38388;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#20351;&#29992;&#35813;&#26694;&#26550;&#27604;&#36739;&#20102;&#19981;&#21516;&#30340;&#20302;&#24310;&#36831;&#35821;&#38899;&#32763;&#35793;&#26041;&#27861;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#20855;&#26377;&#20462;&#35746;&#36755;&#20986;&#36873;&#39033;&#30340;&#27169;&#22411;&#20197;&#21450;&#20855;&#26377;&#22266;&#23450;&#36755;&#20986;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30452;&#25509;&#27604;&#36739;&#20102;&#26368;&#20808;&#36827;&#30340;&#32423;&#32852;&#31995;&#32479;&#21644;&#31471;&#21040;&#31471;&#31995;&#32479;&#12290;&#26368;&#21518;&#65292;&#35813;&#26694;&#26550;&#22522;&#20110;&#19968;&#20010;&#32479;&#19968;&#30340;&#24230;&#37327;&#26469;&#35780;&#20272;&#20302;&#24310;&#36831;&#35821;&#38899;&#32763;&#35793;&#24615;&#33021;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#35780;&#20272;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The challenge of low-latency speech translation has recently draw significant interest in the research community as shown by several publications and shared tasks. Therefore, it is essential to evaluate these different approaches in realistic scenarios. However, currently only specific aspects of the systems are evaluated and often it is not possible to compare different approaches.  In this work, we propose the first framework to perform and evaluate the various aspects of low-latency speech translation under realistic conditions. The evaluation is carried out in an end-to-end fashion. This includes the segmentation of the audio as well as the run-time of the different components.  Secondly, we compare different approaches to low-latency speech translation using this framework. We evaluate models with the option to revise the output as well as methods with fixed output. Furthermore, we directly compare state-of-the-art cascaded as well as end-to-end systems. Finally, the framework all
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#21033;&#29992;&#20248;&#21270;&#30340;&#39134;&#34892;&#23545;&#25239;&#36148;&#29255;&#26469;&#32465;&#26550;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#22810;&#26059;&#32764;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#36825;&#20123;&#26041;&#27861;&#22312;&#23545;&#25239;&#36148;&#29255;&#25968;&#37327;&#22686;&#21152;&#26102;&#30340;&#33391;&#22909;&#25193;&#23637;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.00344</link><description>&lt;p&gt;
&#21033;&#29992;&#20248;&#21270;&#30340;&#39134;&#34892;&#23545;&#25239;&#36148;&#29255;&#32465;&#26550;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#22810;&#26059;&#32764;
&lt;/p&gt;
&lt;p&gt;
Kidnapping Deep Learning-based Multirotors using Optimized Flying Adversarial Patches. (arXiv:2308.00344v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00344
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#21033;&#29992;&#20248;&#21270;&#30340;&#39134;&#34892;&#23545;&#25239;&#36148;&#29255;&#26469;&#32465;&#26550;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#22810;&#26059;&#32764;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#36825;&#20123;&#26041;&#27861;&#22312;&#23545;&#25239;&#36148;&#29255;&#25968;&#37327;&#22686;&#21152;&#26102;&#30340;&#33391;&#22909;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20027;&#39134;&#34892;&#26426;&#22120;&#20154;&#65292;&#20363;&#22914;&#22810;&#26059;&#32764;&#65292;&#36890;&#24120;&#20381;&#36182;&#20110;&#22522;&#20110;&#30456;&#26426;&#22270;&#20687;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#39044;&#27979;&#65292;&#20363;&#22914;&#23039;&#24577;&#20272;&#35745;&#12290;&#22914;&#26524;&#23558;&#36825;&#20123;&#27169;&#22411;&#24212;&#29992;&#20110;&#35757;&#32451;&#22495;&#20043;&#22806;&#30340;&#36755;&#20837;&#22270;&#20687;&#65292;&#23427;&#20204;&#21487;&#33021;&#20250;&#20135;&#29983;&#24847;&#24819;&#19981;&#21040;&#30340;&#32467;&#26524;&#12290;&#23545;&#25239;&#24615;&#25915;&#20987;&#21487;&#20197;&#21033;&#29992;&#36825;&#20010;&#32570;&#28857;&#65292;&#20363;&#22914;&#36890;&#36807;&#35745;&#31639;&#23567;&#22270;&#29255;&#65292;&#21363;&#25152;&#35859;&#30340;&#23545;&#25239;&#36148;&#29255;&#65292;&#22312;&#29615;&#22659;&#20013;&#25918;&#32622;&#20197;&#25805;&#32437;&#31070;&#32463;&#32593;&#32476;&#30340;&#39044;&#27979;&#12290;&#25105;&#20204;&#24341;&#20837;&#39134;&#34892;&#23545;&#25239;&#36148;&#29255;&#65292;&#36890;&#36807;&#23558;&#22810;&#20010;&#22270;&#29255;&#23433;&#35013;&#22312;&#33267;&#23569;&#19968;&#20010;&#20854;&#20182;&#39134;&#34892;&#26426;&#22120;&#20154;&#19978;&#65292;&#22240;&#27492;&#21487;&#20197;&#25918;&#32622;&#22312;&#21463;&#23475;&#22810;&#26059;&#32764;&#30340;&#35270;&#37326;&#33539;&#22260;&#20869;&#30340;&#20219;&#20309;&#20301;&#32622;&#12290;&#36890;&#36807;&#24341;&#20837;&#25915;&#20987;&#32773;&#26426;&#22120;&#20154;&#65292;&#25105;&#20204;&#23558;&#31995;&#32479;&#25193;&#23637;&#20026;&#23545;&#25239;&#24615;&#22810;&#26426;&#22120;&#20154;&#31995;&#32479;&#12290;&#20026;&#20102;&#23454;&#26045;&#26377;&#25928;&#30340;&#25915;&#20987;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#19977;&#31181;&#21516;&#26102;&#20248;&#21270;&#22810;&#20010;&#23545;&#25239;&#36148;&#29255;&#21450;&#20854;&#22312;&#36755;&#20837;&#22270;&#20687;&#20013;&#20301;&#32622;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#35777;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#23545;&#25239;&#36148;&#29255;&#25968;&#37327;&#22686;&#21152;&#26102;&#20855;&#26377;&#33391;&#22909;&#30340;&#25193;&#23637;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;...
&lt;/p&gt;
&lt;p&gt;
Autonomous flying robots, such as multirotors, often rely on deep learning models that makes predictions based on a camera image, e.g. for pose estimation. These models can predict surprising results if applied to input images outside the training domain. This fault can be exploited by adversarial attacks, for example, by computing small images, so-called adversarial patches, that can be placed in the environment to manipulate the neural network's prediction. We introduce flying adversarial patches, where multiple images are mounted on at least one other flying robot and therefore can be placed anywhere in the field of view of a victim multirotor. By introducing the attacker robots, the system is extended to an adversarial multi-robot system. For an effective attack, we compare three methods that simultaneously optimize multiple adversarial patches and their position in the input image. We show that our methods scale well with the number of adversarial patches. Moreover, we demonstrate
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20803;&#29983;&#25104;&#27491;&#21017;&#21270;&#65288;MGR&#65289;&#30340;&#26032;&#22411;&#29983;&#25104;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#65292;&#36890;&#36807;&#23558;&#21512;&#25104;&#26679;&#26412;&#29992;&#20110;&#29305;&#24449;&#25552;&#21462;&#22120;&#30340;&#27491;&#21017;&#21270;&#39033;&#32780;&#19981;&#26159;&#25439;&#22833;&#20989;&#25968;&#65292;&#26368;&#23567;&#21270;&#39564;&#35777;&#25439;&#22833;&#65292;&#25552;&#39640;&#20102;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#29983;&#25104;&#25968;&#25454;&#22686;&#24378;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.13899</link><description>&lt;p&gt;
&#29992;&#20803;&#23398;&#20064;&#29983;&#25104;&#27169;&#22411;&#27491;&#21017;&#21270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Regularizing Neural Networks with Meta-Learning Generative Models. (arXiv:2307.13899v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13899
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20803;&#29983;&#25104;&#27491;&#21017;&#21270;&#65288;MGR&#65289;&#30340;&#26032;&#22411;&#29983;&#25104;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#65292;&#36890;&#36807;&#23558;&#21512;&#25104;&#26679;&#26412;&#29992;&#20110;&#29305;&#24449;&#25552;&#21462;&#22120;&#30340;&#27491;&#21017;&#21270;&#39033;&#32780;&#19981;&#26159;&#25439;&#22833;&#20989;&#25968;&#65292;&#26368;&#23567;&#21270;&#39564;&#35777;&#25439;&#22833;&#65292;&#25552;&#39640;&#20102;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#29983;&#25104;&#25968;&#25454;&#22686;&#24378;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#25913;&#36827;&#28145;&#24230;&#23398;&#20064;&#30340;&#29983;&#25104;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#12290;&#29983;&#25104;&#25968;&#25454;&#22686;&#24378;&#21033;&#29992;&#29983;&#25104;&#27169;&#22411;&#20135;&#29983;&#30340;&#21512;&#25104;&#26679;&#26412;&#20316;&#20026;&#23567;&#25968;&#25454;&#38598;&#20998;&#31867;&#30340;&#39069;&#22806;&#25968;&#25454;&#38598;&#12290;&#29983;&#25104;&#25968;&#25454;&#22686;&#24378;&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#26159;&#21512;&#25104;&#25968;&#25454;&#20013;&#21253;&#21547;&#38477;&#20302;&#20934;&#30830;&#24615;&#30340;&#26080;&#20449;&#24687;&#26679;&#26412;&#12290;&#36825;&#26159;&#22240;&#20026;&#21512;&#25104;&#26679;&#26412;&#19981;&#33021;&#23436;&#32654;&#22320;&#20195;&#34920;&#30495;&#23454;&#25968;&#25454;&#20013;&#30340;&#31867;&#21035;&#65292;&#22343;&#21248;&#25277;&#26679;&#20063;&#19981;&#19968;&#23450;&#20026;&#20219;&#21153;&#25552;&#20379;&#26377;&#29992;&#30340;&#26679;&#26412;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20803;&#29983;&#25104;&#27491;&#21017;&#21270;&#65288;MGR&#65289;&#30340;&#26032;&#22411;&#29983;&#25104;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#12290;&#20026;&#20102;&#36991;&#20813;&#29983;&#25104;&#25968;&#25454;&#22686;&#24378;&#30340;&#38477;&#32423;&#65292;MGR&#23558;&#21512;&#25104;&#26679;&#26412;&#29992;&#20110;&#29305;&#24449;&#25552;&#21462;&#22120;&#30340;&#27491;&#21017;&#21270;&#39033;&#32780;&#19981;&#26159;&#25439;&#22833;&#20989;&#25968;&#65292;&#22914;&#20132;&#21449;&#29109;&#12290;&#36825;&#20123;&#21512;&#25104;&#26679;&#26412;&#36890;&#36807;&#20803;&#23398;&#20064;&#21160;&#24577;&#30830;&#23450;&#65292;&#20197;&#26368;&#23567;&#21270;&#39564;&#35777;&#25439;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates methods for improving generative data augmentation for deep learning. Generative data augmentation leverages the synthetic samples produced by generative models as an additional dataset for classification with small dataset settings. A key challenge of generative data augmentation is that the synthetic data contain uninformative samples that degrade accuracy. This is because the synthetic samples do not perfectly represent class categories in real data and uniform sampling does not necessarily provide useful samples for tasks. In this paper, we present a novel strategy for generative data augmentation called meta generative regularization (MGR). To avoid the degradation of generative data augmentation, MGR utilizes synthetic samples in the regularization term for feature extractors instead of in the loss function, e.g., cross-entropy. These synthetic samples are dynamically determined to minimize the validation losses through meta-learning. We observed that MGR 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20248;&#21270;&#30340;&#32593;&#32476;&#26550;&#26500;&#65292;&#29992;&#20110;&#35757;&#32451;&#25317;&#26377;&#25968;&#21313;&#20159;&#21442;&#25968;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#36825;&#20010;&#26550;&#26500;&#26681;&#25454;&#35821;&#35328;&#27169;&#22411;&#30340;&#36890;&#20449;&#38656;&#27714;&#65292;&#23558;&#38598;&#32676;&#20998;&#21106;&#25104;&#19968;&#32452;&#36890;&#36807;&#38750;&#38459;&#22622;&#39640;&#24102;&#23485;&#20114;&#36830;&#30340;GPU&#38598;&#21512;&#65292;&#24182;&#36890;&#36807;&#36712;&#36947;&#36830;&#25509;&#20165;&#36830;&#25509;&#20855;&#26377;&#36890;&#20449;&#38656;&#27714;&#30340;GPU&#65292;&#20174;&#32780;&#38477;&#20302;&#32593;&#32476;&#25104;&#26412;&#39640;&#36798;75&#65285;&#65292;&#21516;&#26102;&#19981;&#24433;&#21709;&#35757;&#32451;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.12169</link><description>&lt;p&gt;
&#29992;&#20110;&#35757;&#32451;&#25317;&#26377;&#25968;&#21313;&#20159;&#21442;&#25968;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20248;&#21270;&#32593;&#32476;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
Optimized Network Architectures for Large Language Model Training with Billions of Parameters. (arXiv:2307.12169v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12169
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20248;&#21270;&#30340;&#32593;&#32476;&#26550;&#26500;&#65292;&#29992;&#20110;&#35757;&#32451;&#25317;&#26377;&#25968;&#21313;&#20159;&#21442;&#25968;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#36825;&#20010;&#26550;&#26500;&#26681;&#25454;&#35821;&#35328;&#27169;&#22411;&#30340;&#36890;&#20449;&#38656;&#27714;&#65292;&#23558;&#38598;&#32676;&#20998;&#21106;&#25104;&#19968;&#32452;&#36890;&#36807;&#38750;&#38459;&#22622;&#39640;&#24102;&#23485;&#20114;&#36830;&#30340;GPU&#38598;&#21512;&#65292;&#24182;&#36890;&#36807;&#36712;&#36947;&#36830;&#25509;&#20165;&#36830;&#25509;&#20855;&#26377;&#36890;&#20449;&#38656;&#27714;&#30340;GPU&#65292;&#20174;&#32780;&#38477;&#20302;&#32593;&#32476;&#25104;&#26412;&#39640;&#36798;75&#65285;&#65292;&#21516;&#26102;&#19981;&#24433;&#21709;&#35757;&#32451;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25361;&#25112;&#20102;&#20026;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26500;&#24314;&#20219;&#24847;&#21040;&#20219;&#24847;&#32593;&#32476;&#30340;&#20256;&#32479;&#33539;&#24335;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;LLMs&#21576;&#29616;&#20986;&#19968;&#31181;&#29420;&#29305;&#30340;&#36890;&#20449;&#27169;&#24335;&#65292;&#22312;&#20854;&#20013;&#65292;&#21482;&#26377;&#23567;&#32452;&#30340;GPU&#38656;&#35201;&#39640;&#24102;&#23485;&#30340;&#20219;&#24847;&#21040;&#20219;&#24847;&#36890;&#20449;&#65292;&#20197;&#23454;&#29616;&#25509;&#36817;&#26368;&#20248;&#30340;&#35757;&#32451;&#24615;&#33021;&#12290;&#22312;&#36825;&#20123;GPU&#23567;&#32452;&#20043;&#38388;&#65292;&#36890;&#20449;&#38750;&#24120;&#24494;&#19981;&#36275;&#36947;&#12289;&#31232;&#30095;&#19988;&#22343;&#21248;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#32593;&#32476;&#26550;&#26500;&#65292;&#32039;&#23494;&#21305;&#37197;LLMs&#30340;&#36890;&#20449;&#38656;&#27714;&#12290;&#25105;&#20204;&#30340;&#26550;&#26500;&#23558;&#38598;&#32676;&#20998;&#21106;&#20026;&#19968;&#32452;&#36890;&#36807;&#38750;&#38459;&#22622;&#20219;&#24847;&#21040;&#20219;&#24847;&#39640;&#24102;&#23485;&#20114;&#36830;&#30340;GPU&#38598;&#21512;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;HB&#22495;&#12290;&#22312;HB&#22495;&#20043;&#38388;&#65292;&#32593;&#32476;&#21482;&#36830;&#25509;&#20855;&#26377;&#36890;&#20449;&#38656;&#27714;&#30340;GPU&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#32593;&#32476;&#36830;&#25509;&#31216;&#20026;&#8220;&#20165;&#36712;&#36947;&#36830;&#25509;&#8221;&#65292;&#24182;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26550;&#26500;&#30456;&#23545;&#20110;&#26368;&#20808;&#36827;&#30340;&#20219;&#24847;&#21040;&#20219;&#24847;Clos&#32593;&#32476;&#21487;&#20197;&#23558;&#32593;&#32476;&#25104;&#26412;&#38477;&#20302;&#39640;&#36798;75&#65285;&#65292;&#21516;&#26102;&#19981;&#25439;&#23475;LLM&#35757;&#32451;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper challenges the well-established paradigm for building any-to-any networks for training Large Language Models (LLMs). We show that LLMs exhibit a unique communication pattern where only small groups of GPUs require high-bandwidth any-to-any communication within them, to achieve near-optimal training performance. Across these groups of GPUs, the communication is insignificant, sparse, and homogeneous. We propose a new network architecture that closely resembles the communication requirement of LLMs. Our architecture partitions the cluster into sets of GPUs interconnected with non-blocking any-to-any high-bandwidth interconnects that we call HB domains. Across the HB domains, the network only connects GPUs with communication demands. We call this network a "rail-only" connection, and show that our proposed architecture reduces the network cost by up to 75% compared to the state-of-the-art any-to-any Clos networks without compromising the performance of LLM training.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#31163;&#32447;&#25968;&#25454;&#30340;&#30446;&#26631;&#23548;&#21521;&#24378;&#21270;&#23398;&#20064;&#30340;&#20998;&#23618;&#31639;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#30446;&#26631;&#36798;&#25104;&#38382;&#39064;&#30340;&#32467;&#26500;&#65292;&#20351;&#29992;&#19968;&#20010;&#26080;&#21160;&#20316;&#30340;&#20215;&#20540;&#20989;&#25968;&#23398;&#20064;&#20102;&#20004;&#20010;&#31574;&#30053;&#65292;&#20174;&#32780;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#26356;&#26377;&#25928;&#22320;&#21033;&#29992;&#31163;&#32447;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2307.11949</link><description>&lt;p&gt;
HIQL: &#20197;&#28508;&#22312;&#29366;&#24577;&#20316;&#20026;&#21160;&#20316;&#30340;&#31163;&#32447;&#30446;&#26631;&#23548;&#21521;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
HIQL: Offline Goal-Conditioned RL with Latent States as Actions. (arXiv:2307.11949v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11949
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#31163;&#32447;&#25968;&#25454;&#30340;&#30446;&#26631;&#23548;&#21521;&#24378;&#21270;&#23398;&#20064;&#30340;&#20998;&#23618;&#31639;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#30446;&#26631;&#36798;&#25104;&#38382;&#39064;&#30340;&#32467;&#26500;&#65292;&#20351;&#29992;&#19968;&#20010;&#26080;&#21160;&#20316;&#30340;&#20215;&#20540;&#20989;&#25968;&#23398;&#20064;&#20102;&#20004;&#20010;&#31574;&#30053;&#65292;&#20174;&#32780;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#26356;&#26377;&#25928;&#22320;&#21033;&#29992;&#31163;&#32447;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#26368;&#36817;&#24050;&#25104;&#20026;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#22522;&#30707;&#12290;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#30446;&#26631;&#23548;&#21521;&#24378;&#21270;&#23398;&#20064;&#21487;&#20197;&#28508;&#22312;&#22320;&#21033;&#29992;&#22823;&#37327;&#26410;&#26631;&#35760;&#30340;&#65288;&#26080;&#22870;&#21169;&#65289;&#25968;&#25454;&#65292;&#25552;&#20379;&#31867;&#20284;&#20110;&#33258;&#25105;&#30417;&#30563;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#26500;&#24314;&#26377;&#25928;&#30340;&#30446;&#26631;&#23548;&#21521;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#24182;&#30452;&#25509;&#20174;&#22810;&#26679;&#21270;&#30340;&#31163;&#32447;&#25968;&#25454;&#20013;&#36827;&#34892;&#23398;&#20064;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#20934;&#30830;&#20272;&#35745;&#36828;&#26399;&#30446;&#26631;&#30340;&#20215;&#20540;&#20989;&#25968;&#24456;&#22256;&#38590;&#12290;&#28982;&#32780;&#65292;&#30446;&#26631;&#36798;&#25104;&#38382;&#39064;&#34920;&#29616;&#20986;&#19968;&#23450;&#30340;&#32467;&#26500;&#65292;&#21363;&#36798;&#21040;&#36828;&#26399;&#30446;&#26631;&#38656;&#35201;&#39318;&#20808;&#36890;&#36807;&#36739;&#36817;&#23376;&#30446;&#26631;&#12290;&#36825;&#31181;&#32467;&#26500;&#38750;&#24120;&#26377;&#29992;&#65292;&#22240;&#20026;&#35780;&#20272;&#37051;&#36817;&#30446;&#26631;&#30340;&#21160;&#20316;&#36136;&#37327;&#36890;&#24120;&#27604;&#26356;&#36828;&#30446;&#26631;&#23481;&#26131;&#12290;&#22522;&#20110;&#36825;&#19968;&#24605;&#24819;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#31163;&#32447;&#25968;&#25454;&#30340;&#30446;&#26631;&#23548;&#21521;&#24378;&#21270;&#23398;&#20064;&#30340;&#20998;&#23618;&#31639;&#27861;&#12290;&#21033;&#29992;&#19968;&#20010;&#27809;&#26377;&#21160;&#20316;&#30340;&#20215;&#20540;&#20989;&#25968;&#65292;&#25105;&#20204;&#23398;&#20064;&#20102;&#20004;&#20010;&#31574;&#30053;&#65292;&#20801;&#35768;&#25105;&#20204;&#21033;&#29992;&#36825;&#31181;&#32467;&#26500;&#65306;&#19968;&#20010;&#39640;&#23618;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Unsupervised pre-training has recently become the bedrock for computer vision and natural language processing. In reinforcement learning (RL), goal-conditioned RL can potentially provide an analogous self-supervised approach for making use of large quantities of unlabeled (reward-free) data. However, building effective algorithms for goal-conditioned RL that can learn directly from diverse offline data is challenging, because it is hard to accurately estimate the exact value function for faraway goals. Nonetheless, goal-reaching problems exhibit structure, such that reaching distant goals entails first passing through closer subgoals. This structure can be very useful, as assessing the quality of actions for nearby goals is typically easier than for more distant goals. Based on this idea, we propose a hierarchical algorithm for goal-conditioned RL from offline data. Using one action-free value function, we learn two policies that allow us to exploit this structure: a high-level policy 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31163;&#32447;&#25216;&#33021;&#21457;&#29616;&#31639;&#27861;&#65292;&#36890;&#36807;Fenchel&#23545;&#20598;&#26041;&#27861;&#23558;&#24378;&#21270;&#23398;&#20064;&#21644;&#26080;&#30417;&#30563;&#25216;&#33021;&#21457;&#29616;&#32467;&#21512;&#36215;&#26469;&#65292;&#23454;&#29616;&#23398;&#20064;&#19982;&#19987;&#23478;&#30456;&#19968;&#33268;&#30340;&#22810;&#26679;&#30340;&#25216;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.11373</link><description>&lt;p&gt;
&#36890;&#36807;Fenchel&#23545;&#20598;&#23454;&#29616;&#22810;&#26679;&#30340;&#31163;&#32447;&#27169;&#20223;
&lt;/p&gt;
&lt;p&gt;
Diverse Offline Imitation via Fenchel Duality. (arXiv:2307.11373v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11373
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31163;&#32447;&#25216;&#33021;&#21457;&#29616;&#31639;&#27861;&#65292;&#36890;&#36807;Fenchel&#23545;&#20598;&#26041;&#27861;&#23558;&#24378;&#21270;&#23398;&#20064;&#21644;&#26080;&#30417;&#30563;&#25216;&#33021;&#21457;&#29616;&#32467;&#21512;&#36215;&#26469;&#65292;&#23454;&#29616;&#23398;&#20064;&#19982;&#19987;&#23478;&#30456;&#19968;&#33268;&#30340;&#22810;&#26679;&#30340;&#25216;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26080;&#30417;&#30563;&#25216;&#33021;&#21457;&#29616;&#39046;&#22495;&#65292;&#26368;&#36817;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#21508;&#31181;&#24037;&#20316;&#25552;&#20986;&#20102;&#20197;&#20114;&#20449;&#24687;&#20026;&#22522;&#30784;&#30340;&#30446;&#26631;&#65292;&#20316;&#20026;&#20869;&#22312;&#39537;&#21160;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#35774;&#35745;&#38656;&#35201;&#22312;&#32447;&#29615;&#22659;&#35775;&#38382;&#30340;&#31639;&#27861;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;\textit{&#31163;&#32447;}&#25216;&#33021;&#21457;&#29616;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#38382;&#39064;&#24418;&#24335;&#21270;&#32771;&#34385;&#20102;&#22312;KL-&#25955;&#24230;&#32422;&#26463;&#19979;&#26368;&#22823;&#21270;&#20114;&#20449;&#24687;&#30446;&#26631;&#12290;&#26356;&#30830;&#20999;&#22320;&#35828;&#65292;&#32422;&#26463;&#30830;&#20445;&#27599;&#20010;&#25216;&#33021;&#30340;&#29366;&#24577;&#21344;&#29992;&#20445;&#25345;&#22312;&#19968;&#20010;&#20855;&#26377;&#33391;&#22909;&#29366;&#24577;&#25805;&#20316;&#35206;&#30422;&#29575;&#30340;&#31163;&#32447;&#25968;&#25454;&#38598;&#30340;&#25903;&#25345;&#33539;&#22260;&#20869;&#19982;&#19987;&#23478;&#30340;&#29366;&#24577;&#21344;&#29992;&#36924;&#36817;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#36830;&#25509;Fenchel&#23545;&#20598;&#12289;&#24378;&#21270;&#23398;&#20064;&#21644;&#26080;&#30417;&#30563;&#25216;&#33021;&#21457;&#29616;&#65292;&#24182;&#32473;&#20986;&#19968;&#20010;&#31616;&#21333;&#30340;&#31163;&#32447;&#31639;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#19982;&#19987;&#23478;&#30456;&#19968;&#33268;&#30340;&#22810;&#26679;&#30340;&#25216;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
There has been significant recent progress in the area of unsupervised skill discovery, with various works proposing mutual information based objectives, as a source of intrinsic motivation. Prior works predominantly focused on designing algorithms that require online access to the environment. In contrast, we develop an \textit{offline} skill discovery algorithm. Our problem formulation considers the maximization of a mutual information objective constrained by a KL-divergence. More precisely, the constraints ensure that the state occupancy of each skill remains close to the state occupancy of an expert, within the support of an offline dataset with good state-action coverage. Our main contribution is to connect Fenchel duality, reinforcement learning and unsupervised skill discovery, and to give a simple offline algorithm for learning diverse skills that are aligned with an expert.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#26641;&#22411;&#22402;&#30452;&#32852;&#21512;&#23398;&#20064;&#20013;&#30340;&#26631;&#31614;&#27844;&#38706;&#38382;&#39064;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26631;&#31614;&#25512;&#26029;&#25915;&#20987;&#26041;&#27861;ID2Graph&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;ID-LMID&#30340;&#38450;&#24481;&#26426;&#21046;&#65292;&#36890;&#36807;&#20851;&#27880;&#20114;&#20449;&#24687;&#27491;&#21017;&#21270;&#26469;&#38450;&#27490;&#26631;&#31614;&#27844;&#38706;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;ID2Graph&#25915;&#20987;&#23384;&#22312;&#26174;&#33879;&#30340;&#27844;&#38706;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.10318</link><description>&lt;p&gt;
&#28145;&#20837;&#30740;&#31350;&#28040;&#38500;&#26641;&#22411;&#22402;&#30452;&#32852;&#21512;&#23398;&#20064;&#20013;&#30340;&#26631;&#31614;&#27844;&#38706;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Eliminating Label Leakage in Tree-Based Vertical Federated Learning. (arXiv:2307.10318v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10318
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#26641;&#22411;&#22402;&#30452;&#32852;&#21512;&#23398;&#20064;&#20013;&#30340;&#26631;&#31614;&#27844;&#38706;&#38382;&#39064;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26631;&#31614;&#25512;&#26029;&#25915;&#20987;&#26041;&#27861;ID2Graph&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;ID-LMID&#30340;&#38450;&#24481;&#26426;&#21046;&#65292;&#36890;&#36807;&#20851;&#27880;&#20114;&#20449;&#24687;&#27491;&#21017;&#21270;&#26469;&#38450;&#27490;&#26631;&#31614;&#27844;&#38706;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;ID2Graph&#25915;&#20987;&#23384;&#22312;&#26174;&#33879;&#30340;&#27844;&#38706;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22402;&#30452;&#32852;&#21512;&#23398;&#20064;&#65288;VFL&#65289;&#20351;&#24471;&#20855;&#26377;&#20849;&#21516;&#29992;&#25143;&#38598;&#21512;&#30340;&#22810;&#20010;&#21442;&#19982;&#26041;&#33021;&#22815;&#22312;&#19981;&#20998;&#20139;&#31169;&#26377;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#30001;&#20110;&#20854;&#21487;&#35299;&#37322;&#24615;&#21644;&#25928;&#29575;&#65292;&#22522;&#20110;&#26641;&#32467;&#26500;&#30340;&#27169;&#22411;&#22312;VFL&#20013;&#21464;&#24471;&#27969;&#34892;&#36215;&#26469;&#12290;&#28982;&#32780;&#65292;&#26641;&#22411;VFL&#30340;&#33030;&#24369;&#24615;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#30340;&#30740;&#31350;&#12290;&#26412;&#30740;&#31350;&#39318;&#20808;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26631;&#31614;&#25512;&#26029;&#25915;&#20987;&#26041;&#27861;ID2Graph&#65292;&#35813;&#25915;&#20987;&#21033;&#29992;&#27599;&#20010;&#33410;&#28857;&#65288;&#21363;&#23454;&#20363;&#31354;&#38388;&#65289;&#20998;&#37197;&#30340;&#35760;&#24405;&#26631;&#35782;&#38598;&#21512;&#26469;&#25512;&#23548;&#31169;&#26377;&#35757;&#32451;&#26631;&#31614;&#12290;ID2Graph&#25915;&#20987;&#29983;&#25104;&#35757;&#32451;&#26679;&#26412;&#30340;&#22270;&#32467;&#26500;&#65292;&#20174;&#22270;&#20013;&#25552;&#21462;&#31038;&#21306;&#65292;&#24182;&#20351;&#29992;&#31038;&#21306;&#20449;&#24687;&#23545;&#23616;&#37096;&#25968;&#25454;&#38598;&#36827;&#34892;&#32858;&#31867;&#12290;&#20026;&#20102;&#25269;&#24481;&#23454;&#20363;&#31354;&#38388;&#20013;&#30340;&#26631;&#31614;&#27844;&#38706;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#38450;&#24481;&#26426;&#21046;ID-LMID&#65292;&#35813;&#26426;&#21046;&#36890;&#36807;&#20851;&#27880;&#20114;&#20449;&#24687;&#27491;&#21017;&#21270;&#26469;&#38450;&#27490;&#26631;&#31614;&#27844;&#38706;&#12290;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#32508;&#21512;&#23454;&#39564;&#34920;&#26126;&#65292;ID2Graph&#25915;&#20987;&#21576;&#29616;&#20986;&#26174;&#33879;&#30340;&#27844;&#38706;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vertical federated learning (VFL) enables multiple parties with disjoint features of a common user set to train a machine learning model without sharing their private data. Tree-based models have become prevalent in VFL due to their interpretability and efficiency. However, the vulnerability of tree-based VFL has not been sufficiently investigated. In this study, we first introduce a novel label inference attack, ID2Graph, which utilizes the sets of record-IDs assigned to each node (i.e., instance space) to deduce private training labels. The ID2Graph attack generates a graph structure from training samples, extracts communities from the graph, and clusters the local dataset using community information. To counteract label leakage from the instance space, we propose an effective defense mechanism, ID-LMID, which prevents label leakage by focusing on mutual information regularization. Comprehensive experiments conducted on various datasets reveal that the ID2Graph attack presents signif
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#27979;&#35797;&#26102;&#39046;&#22495;&#27867;&#21270;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#27979;&#35797;&#26102;&#20351;&#29992;&#27010;&#29575;&#20266;&#26631;&#31614;&#21644;&#21464;&#20998;&#37051;&#23621;&#26631;&#31614;&#26469;&#25512;&#24191;&#28304;&#22495;&#35757;&#32451;&#30340;&#27169;&#22411;&#21040;&#30446;&#26631;&#39046;&#22495;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.04033</link><description>&lt;p&gt;
&#23398;&#20064;&#29992;&#20110;&#27979;&#35797;&#26102;&#39046;&#22495;&#27867;&#21270;&#30340;&#21464;&#20998;&#37051;&#23621;&#26631;&#31614;
&lt;/p&gt;
&lt;p&gt;
Learning Variational Neighbor Labels for Test-Time Domain Generalization. (arXiv:2307.04033v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04033
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#27979;&#35797;&#26102;&#39046;&#22495;&#27867;&#21270;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#27979;&#35797;&#26102;&#20351;&#29992;&#27010;&#29575;&#20266;&#26631;&#31614;&#21644;&#21464;&#20998;&#37051;&#23621;&#26631;&#31614;&#26469;&#25512;&#24191;&#28304;&#22495;&#35757;&#32451;&#30340;&#27169;&#22411;&#21040;&#30446;&#26631;&#39046;&#22495;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#33268;&#21147;&#20110;&#39046;&#22495;&#27867;&#21270;&#65292;&#22312;&#26410;&#30693;&#30340;&#30446;&#26631;&#39046;&#22495;&#20013;&#21482;&#22312;&#28304;&#39046;&#22495;&#19978;&#36827;&#34892;&#35757;&#32451;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#28304;&#22495;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#28982;&#21518;&#22312;&#30446;&#26631;&#22495;&#19978;&#36827;&#34892;&#25512;&#29702;&#65292;&#21033;&#29992;&#26080;&#26631;&#31614;&#30446;&#26631;&#25968;&#25454;&#26412;&#36523;&#30340;&#20215;&#20540;&#12290;&#25105;&#20204;&#20570;&#20986;&#20102;&#19977;&#20010;&#36129;&#29486;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#30446;&#26631;&#26679;&#26412;&#30340;&#27010;&#29575;&#20266;&#26631;&#31614;&#65292;&#20197;&#22312;&#27979;&#35797;&#26102;&#23558;&#28304;&#39046;&#22495;&#35757;&#32451;&#30340;&#27169;&#22411;&#25512;&#24191;&#21040;&#30446;&#26631;&#39046;&#22495;&#12290;&#25105;&#20204;&#23558;&#27979;&#35797;&#26102;&#30340;&#25512;&#24191;&#24314;&#27169;&#20026;&#21464;&#20998;&#25512;&#29702;&#38382;&#39064;&#65292;&#36890;&#36807;&#23558;&#20266;&#26631;&#31614;&#24314;&#27169;&#20026;&#20998;&#24067;&#65292;&#32771;&#34385;&#27867;&#21270;&#36807;&#31243;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#20943;&#36731;&#20266;&#26631;&#31614;&#19981;&#20934;&#30830;&#24615;&#24102;&#26469;&#30340;&#35823;&#23548;&#20449;&#21495;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#23398;&#20064;&#20102;&#21464;&#20998;&#37051;&#23621;&#26631;&#31614;&#65292;&#23558;&#37051;&#36817;&#30446;&#26631;&#26679;&#26412;&#30340;&#20449;&#24687;&#32435;&#20837;&#21040;&#29983;&#25104;&#26356;&#24378;&#40065;&#26834;&#20266;&#26631;&#31614;&#30340;&#36807;&#31243;&#20013;&#12290;&#31532;&#19977;&#65292;&#20026;&#20102;&#23398;&#20064;&#23558;&#26356;&#20855;&#20195;&#34920;&#24615;&#30340;&#30446;&#26631;&#20449;&#24687;&#32435;&#20837;&#21040;&#29983;&#25104;&#26356;&#20934;&#30830;&#12289;&#26356;&#24378;&#40065;&#26834;&#30340;&#21464;&#20998;&#37051;&#23621;&#26631;&#31614;&#30340;&#33021;&#21147;&#20013;&#65292;&#25105;&#20204;
&lt;/p&gt;
&lt;p&gt;
This paper strives for domain generalization, where models are trained exclusively on source domains before being deployed at unseen target domains. We follow the strict separation of source training and target testing but exploit the value of the unlabeled target data itself during inference. We make three contributions. First, we propose probabilistic pseudo-labeling of target samples to generalize the source-trained model to the target domain at test time. We formulate the generalization at test time as a variational inference problem by modeling pseudo labels as distributions to consider the uncertainty during generalization and alleviate the misleading signal of inaccurate pseudo labels. Second, we learn variational neighbor labels that incorporate the information of neighboring target samples to generate more robust pseudo labels. Third, to learn the ability to incorporate more representative target information and generate more precise and robust variational neighbor labels, we 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22686;&#37327;&#30446;&#26631;&#20998;&#37197;CBS&#65288;ITA-CBS&#65289;&#31639;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#21333;&#20010;&#25628;&#32034;&#26641;&#24182;&#36991;&#20813;&#35745;&#31639;K&#26368;&#20339;&#20998;&#37197;&#26469;&#35299;&#20915;&#22810;&#26234;&#33021;&#20307;&#30446;&#26631;&#20998;&#37197;&#21644;&#36335;&#24452;&#35268;&#21010;&#38382;&#39064;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#35745;&#31639;&#25928;&#29575;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.00663</link><description>&lt;p&gt;
&#29992;&#21333;&#19968;&#32422;&#26463;&#26641;&#35299;&#20915;&#22810;&#26234;&#33021;&#20307;&#30446;&#26631;&#20998;&#37197;&#21644;&#36335;&#24452;&#35268;&#21010;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Solving Multi-Agent Target Assignment and Path Finding with a Single Constraint Tree. (arXiv:2307.00663v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00663
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22686;&#37327;&#30446;&#26631;&#20998;&#37197;CBS&#65288;ITA-CBS&#65289;&#31639;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#21333;&#20010;&#25628;&#32034;&#26641;&#24182;&#36991;&#20813;&#35745;&#31639;K&#26368;&#20339;&#20998;&#37197;&#26469;&#35299;&#20915;&#22810;&#26234;&#33021;&#20307;&#30446;&#26631;&#20998;&#37197;&#21644;&#36335;&#24452;&#35268;&#21010;&#38382;&#39064;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#35745;&#31639;&#25928;&#29575;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32452;&#21512;&#30446;&#26631;&#20998;&#37197;&#21644;&#36335;&#24452;&#35268;&#21010;&#38382;&#39064;&#65288;TAPF&#65289;&#35201;&#27714;&#21516;&#26102;&#20026;&#26234;&#33021;&#20307;&#20998;&#37197;&#30446;&#26631;&#24182;&#20026;&#20854;&#35268;&#21010;&#20174;&#36215;&#22987;&#20301;&#32622;&#21040;&#30446;&#26631;&#20301;&#32622;&#30340;&#26080;&#30896;&#25758;&#36335;&#24452;&#12290;&#20316;&#20026;&#35299;&#20915;TAPF&#38382;&#39064;&#30340;&#20027;&#35201;&#26041;&#27861;&#65292;&#22522;&#20110;&#20914;&#31361;&#25628;&#32034;&#19982;&#30446;&#26631;&#20998;&#37197;&#65288;CBS-TA&#65289;&#21033;&#29992;K&#26368;&#20339;&#30446;&#26631;&#20998;&#37197;&#21019;&#24314;&#22810;&#20010;&#25628;&#32034;&#26641;&#65292;&#24182;&#21033;&#29992;&#20914;&#31361;&#22522;&#30784;&#25628;&#32034;&#65288;CBS&#65289;&#35299;&#20915;&#27599;&#20010;&#25628;&#32034;&#26641;&#20013;&#30340;&#30896;&#25758;&#38382;&#39064;&#12290;&#23613;&#31649;&#33021;&#22815;&#25214;&#21040;&#26368;&#20248;&#35299;&#65292;&#20294;&#30001;&#20110;&#22312;&#22810;&#20010;&#26641;&#20013;&#37325;&#22797;&#35299;&#20915;&#30896;&#25758;&#38382;&#39064;&#21644;&#35745;&#31639;&#39640;&#26114;&#30340;K&#26368;&#20339;&#20998;&#37197;&#65292;CBS-TA&#22312;&#21487;&#25193;&#23637;&#24615;&#26041;&#38754;&#23384;&#22312;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#22686;&#37327;&#30446;&#26631;&#20998;&#37197;CBS&#65288;ITA-CBS&#65289;&#26469;&#35299;&#20915;&#36825;&#20004;&#20010;&#35745;&#31639;&#29942;&#39048;&#12290;ITA-CBS&#21482;&#29983;&#25104;&#21333;&#20010;&#25628;&#32034;&#26641;&#65292;&#24182;&#22312;&#25628;&#32034;&#36807;&#31243;&#20013;&#36890;&#36807;&#36880;&#27493;&#35745;&#31639;&#26032;&#30340;&#26368;&#20339;&#20998;&#37197;&#26469;&#36991;&#20813;&#35745;&#31639;K&#26368;&#20339;&#20998;&#37197;&#12290;&#29702;&#35770;&#19978;&#65292;&#25105;&#20204;&#35777;&#26126;ITA-CBS&#33021;&#22815;&#20445;&#35777;&#25214;&#21040;&#26368;&#20248;&#35299;&#65292;&#24182;&#22312;&#23454;&#36341;&#20013;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Combined Target-Assignment and Path-Finding problem (TAPF) requires simultaneously assigning targets to agents and planning collision-free paths for agents from their start locations to their assigned targets. As a leading approach to address TAPF, Conflict-Based Search with Target Assignment (CBS-TA) leverages both K-best target assignments to create multiple search trees and Conflict-Based Search (CBS) to resolve collisions in each search tree. While being able to find an optimal solution, CBS-TA suffers from scalability due to the duplicated collision resolution in multiple trees and the expensive computation of K-best assignments. We therefore develop Incremental Target Assignment CBS (ITA-CBS) to bypass these two computational bottlenecks. ITA-CBS generates only a single search tree and avoids computing K-best assignments by incrementally computing new 1-best assignments during the search. We show that, in theory, ITA-CBS is guaranteed to find an optimal solution and, in practice,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;NLP&#22788;&#29702;&#26041;&#27861;&#30340;&#26032;&#30340;&#35757;&#32451;&#27169;&#22411;&#65292;&#36890;&#36807;&#29983;&#25104;&#26032;&#26679;&#26412;&#26469;&#24179;&#34913;&#19981;&#22343;&#21248;&#30340;&#25968;&#25454;&#38598;&#20998;&#24067;&#30340;&#32570;&#38519;&#65292;&#24182;&#20351;&#29992;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#26469;&#36171;&#20104;&#27169;&#22411;&#23545;&#23567;&#25968;&#25454;&#38598;&#30340;&#33391;&#22909;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.17020</link><description>&lt;p&gt;
&#20351;&#29992;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#21028;&#20915;&#25991;&#20214;&#23545;&#29359;&#32618;&#31867;&#22411;&#36827;&#34892;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Classifying Crime Types using Judgment Documents from Social Media. (arXiv:2306.17020v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17020
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;NLP&#22788;&#29702;&#26041;&#27861;&#30340;&#26032;&#30340;&#35757;&#32451;&#27169;&#22411;&#65292;&#36890;&#36807;&#29983;&#25104;&#26032;&#26679;&#26412;&#26469;&#24179;&#34913;&#19981;&#22343;&#21248;&#30340;&#25968;&#25454;&#38598;&#20998;&#24067;&#30340;&#32570;&#38519;&#65292;&#24182;&#20351;&#29992;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#26469;&#36171;&#20104;&#27169;&#22411;&#23545;&#23567;&#25968;&#25454;&#38598;&#30340;&#33391;&#22909;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#29359;&#32618;&#34892;&#20026;&#20107;&#23454;&#26469;&#30830;&#23450;&#29359;&#32618;&#31867;&#22411;&#30340;&#20219;&#21153;&#22312;&#31038;&#20250;&#31185;&#23398;&#20013;&#21464;&#24471;&#38750;&#24120;&#37325;&#35201;&#21644;&#26377;&#24847;&#20041;&#12290;&#20294;&#35813;&#39046;&#22495;&#38754;&#20020;&#30340;&#38382;&#39064;&#26159;&#65292;&#30001;&#20110;&#29359;&#32618;&#26412;&#36523;&#30340;&#24615;&#36136;&#65292;&#25968;&#25454;&#26679;&#26412;&#26412;&#36523;&#20998;&#24067;&#19981;&#22343;&#21248;&#12290;&#21516;&#26102;&#65292;&#21496;&#27861;&#39046;&#22495;&#30340;&#25968;&#25454;&#38598;&#23569;&#26377;&#20844;&#24320;&#21487;&#29992;&#65292;&#26080;&#27861;&#20135;&#29983;&#29992;&#20110;&#30452;&#25509;&#35757;&#32451;&#30340;&#22823;&#22411;&#25968;&#25454;&#38598;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;NLP&#22788;&#29702;&#26041;&#27861;&#35299;&#20915;&#35813;&#38382;&#39064;&#30340;&#26032;&#30340;&#35757;&#32451;&#27169;&#22411;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#20010;&#29359;&#32618;&#20107;&#23454;&#25968;&#25454;&#39044;&#22788;&#29702;&#27169;&#22359;(CFDPM)&#65292;&#36890;&#36807;&#29983;&#25104;&#26032;&#26679;&#26412;&#26469;&#24179;&#34913;&#19981;&#22343;&#21248;&#30340;&#25968;&#25454;&#38598;&#20998;&#24067;&#30340;&#32570;&#38519;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#22823;&#22411;&#24320;&#28304;&#25968;&#25454;&#38598;(CAIL-big)&#20316;&#20026;&#25105;&#20204;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#20351;&#29992;&#25105;&#20204;&#33258;&#24049;&#25910;&#38598;&#30340;&#19968;&#20010;&#23567;&#25968;&#25454;&#38598;&#36827;&#34892;&#24494;&#35843;&#65292;&#36171;&#20104;&#27169;&#22411;&#23545;&#19981;&#29087;&#24713;&#30340;&#23567;&#25968;&#25454;&#38598;&#20855;&#26377;&#33391;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#20351;&#29992;&#25913;&#36827;&#30340;Bert&#27169;&#22411;&#21644;&#21160;&#24577;&#36974;&#34109;&#26469;&#25913;&#36827;&#27169;&#22411;&#12290;&#23454;&#39564;&#35777;&#26126;
&lt;/p&gt;
&lt;p&gt;
The task of determining crime types based on criminal behavior facts has become a very important and meaningful task in social science. But the problem facing the field now is that the data samples themselves are unevenly distributed, due to the nature of the crime itself. At the same time, data sets in the judicial field are less publicly available, and it is not practical to produce large data sets for direct training. This article proposes a new training model to solve this problem through NLP processing methods. We first propose a Crime Fact Data Preprocessing Module (CFDPM), which can balance the defects of uneven data set distribution by generating new samples. Then we use a large open source dataset (CAIL-big) as our pretraining dataset and a small dataset collected by ourselves for Fine-tuning, giving it good generalization ability to unfamiliar small datasets. At the same time, we use the improved Bert model with dynamic masking to improve the model. Experiments show that the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#35821;&#35328;&#39537;&#21160;&#30340;&#39640;&#25928;&#24207;&#25968;&#20998;&#31867;&#26041;&#27861;&#65292;&#21363;L2RCLIP&#65292;&#23427;&#36890;&#36807;&#35270;&#35273;-&#35821;&#35328;&#23545;&#40784;&#20219;&#21153;&#20805;&#20998;&#21033;&#29992;&#35821;&#35328;&#20013;&#30340;&#24207;&#25968;&#20808;&#39564;&#65292;&#21033;&#29992;&#34917;&#20805;&#25552;&#31034;&#35843;&#25972;&#25216;&#26415;RankFormer&#22686;&#24378;&#21407;&#22987;&#25490;&#24207;&#25552;&#31034;&#30340;&#25490;&#24207;&#20851;&#31995;&#65292;&#24182;&#20351;&#29992;&#36328;&#27169;&#24577;&#25490;&#24207;&#32422;&#26463;&#25439;&#22833;(CMOCL)&#36827;&#19968;&#27493;&#23558;&#35821;&#35328;&#20808;&#39564;&#34701;&#20837;&#27169;&#22411;&#20013;&#12290;&#22312;&#22810;&#20010;&#26631;&#20934;&#25968;&#25454;&#38598;&#20013;&#65292;L2RCLIP&#37117;&#27604;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2306.13856</link><description>&lt;p&gt;
&#23398;&#20064;&#25490;&#24207;&#36935;&#35265;&#35821;&#35328;&#65306;&#22686;&#24378;&#22522;&#20110;&#35821;&#35328;&#39537;&#21160;&#30340;&#25490;&#24207;&#23545;&#40784;&#20197;&#25903;&#25345;&#24207;&#25968;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Learning-to-Rank Meets Language: Boosting Language-Driven Ordering Alignment for Ordinal Classification. (arXiv:2306.13856v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13856
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#35821;&#35328;&#39537;&#21160;&#30340;&#39640;&#25928;&#24207;&#25968;&#20998;&#31867;&#26041;&#27861;&#65292;&#21363;L2RCLIP&#65292;&#23427;&#36890;&#36807;&#35270;&#35273;-&#35821;&#35328;&#23545;&#40784;&#20219;&#21153;&#20805;&#20998;&#21033;&#29992;&#35821;&#35328;&#20013;&#30340;&#24207;&#25968;&#20808;&#39564;&#65292;&#21033;&#29992;&#34917;&#20805;&#25552;&#31034;&#35843;&#25972;&#25216;&#26415;RankFormer&#22686;&#24378;&#21407;&#22987;&#25490;&#24207;&#25552;&#31034;&#30340;&#25490;&#24207;&#20851;&#31995;&#65292;&#24182;&#20351;&#29992;&#36328;&#27169;&#24577;&#25490;&#24207;&#32422;&#26463;&#25439;&#22833;(CMOCL)&#36827;&#19968;&#27493;&#23558;&#35821;&#35328;&#20808;&#39564;&#34701;&#20837;&#27169;&#22411;&#20013;&#12290;&#22312;&#22810;&#20010;&#26631;&#20934;&#25968;&#25454;&#38598;&#20013;&#65292;L2RCLIP&#37117;&#27604;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#35821;&#35328;&#39537;&#21160;&#30340;&#25490;&#24207;&#23545;&#20934;&#26041;&#27861;&#65292;&#29992;&#20110;&#24207;&#25968;&#20998;&#31867;&#12290;&#22312;&#24207;&#25968;&#20998;&#31867;&#20013;&#65292;&#26631;&#31614;&#21253;&#21547;&#39069;&#22806;&#30340;&#25490;&#24207;&#20851;&#31995;&#65292;&#22914;&#26524;&#20165;&#20381;&#36182;&#20110;&#35757;&#32451;&#25968;&#25454;&#65292;&#24456;&#23481;&#26131;&#20986;&#29616;&#36807;&#25311;&#21512;&#29616;&#35937;&#12290;&#26368;&#36817;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#21457;&#23637;&#21551;&#21457;&#25105;&#20204;&#36890;&#36807;&#23558;&#21407;&#22987;&#20219;&#21153;&#36716;&#21270;&#20026;&#35270;&#35273;-&#35821;&#35328;&#23545;&#40784;&#20219;&#21153;&#26469;&#21033;&#29992;&#20154;&#31867;&#35821;&#35328;&#20013;&#20016;&#23500;&#30340;&#24207;&#25968;&#20808;&#39564;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;L2RCLIP&#65292;&#23427;&#20174;&#20004;&#20010;&#26041;&#38754;&#20805;&#20998;&#21033;&#29992;&#20102;&#35821;&#35328;&#20808;&#39564;&#65306;&#39318;&#20808;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#34917;&#20805;&#25552;&#31034;&#35843;&#25972;&#25216;&#26415;RankFormer&#65292;&#26088;&#22312;&#22686;&#24378;&#21407;&#22987;&#25490;&#24207;&#25552;&#31034;&#30340;&#25490;&#24207;&#20851;&#31995;&#12290;&#23427;&#22312;&#21333;&#35789;&#23884;&#20837;&#31354;&#38388;&#20013;&#20351;&#29992;&#26631;&#35760;&#32423;&#21035;&#30340;&#27880;&#24847;&#21147;&#21644;&#27531;&#24046;&#39118;&#26684;&#25552;&#31034;&#28151;&#21512;&#12290;&#20854;&#27425;&#65292;&#20026;&#20102;&#36827;&#19968;&#27493;&#34701;&#20837;&#35821;&#35328;&#20808;&#39564;&#65292;&#25105;&#20204;&#37325;&#26032;&#32771;&#34385;&#20102;&#39321;&#33609;&#20132;&#21449;&#29109;&#25439;&#22833;&#30340;&#36817;&#20284;&#32465;&#23450;&#20248;&#21270;&#65292;&#24182;&#22312;&#36328;&#27169;&#24577;&#23884;&#20837;&#31354;&#38388;&#20869;&#36827;&#34892;&#20102;&#37325;&#26500;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36328;&#27169;&#24577;&#25490;&#24207;&#32422;&#26463;&#25439;&#22833;&#65288;CMOCL&#65289;&#65292;&#29992;&#20110;&#35268;&#33539;&#20174;&#35821;&#35328;&#20013;&#23548;&#20986;&#30340;&#24207;&#25968;&#32422;&#26463;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#22810;&#20010;&#27969;&#34892;&#30340;&#24207;&#25968;&#20998;&#31867;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#22343;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel language-driven ordering alignment method for ordinal classification. The labels in ordinal classification contain additional ordering relations, making them prone to overfitting when relying solely on training data. Recent developments in pre-trained vision-language models inspire us to leverage the rich ordinal priors in human language by converting the original task into a vision-language alignment task. Consequently, we propose L2RCLIP, which fully utilizes the language priors from two perspectives. First, we introduce a complementary prompt tuning technique called RankFormer, designed to enhance the ordering relation of original rank prompts. It employs token-level attention with residual-style prompt blending in the word embedding space. Second, to further incorporate language priors, we revisit the approximate bound optimization of vanilla cross-entropy loss and restructure it within the cross-modal embedding space. Consequently, we propose a cross-modal ordin
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;UUKG&#65292;&#19968;&#20010;&#29992;&#20110;&#22478;&#24066;&#26102;&#31354;&#39044;&#27979;&#30340;&#32479;&#19968;&#22478;&#24066;&#30693;&#35782;&#22270;&#35889;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#26500;&#24314;UrbanKG&#24182;&#20998;&#26512;&#20854;&#39640;&#38454;&#32467;&#26500;&#27169;&#24335;&#65292;&#35813;&#25968;&#25454;&#38598;&#21487;&#20197;&#25552;&#20379;&#20851;&#38190;&#30693;&#35782;&#65292;&#22686;&#24378;&#22478;&#24066;&#26102;&#31354;&#39044;&#27979;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.11443</link><description>&lt;p&gt;
UUKG&#65306;&#29992;&#20110;&#22478;&#24066;&#26102;&#31354;&#39044;&#27979;&#30340;&#32479;&#19968;&#22478;&#24066;&#30693;&#35782;&#22270;&#35889;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
UUKG: Unified Urban Knowledge Graph Dataset for Urban Spatiotemporal Prediction. (arXiv:2306.11443v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11443
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;UUKG&#65292;&#19968;&#20010;&#29992;&#20110;&#22478;&#24066;&#26102;&#31354;&#39044;&#27979;&#30340;&#32479;&#19968;&#22478;&#24066;&#30693;&#35782;&#22270;&#35889;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#26500;&#24314;UrbanKG&#24182;&#20998;&#26512;&#20854;&#39640;&#38454;&#32467;&#26500;&#27169;&#24335;&#65292;&#35813;&#25968;&#25454;&#38598;&#21487;&#20197;&#25552;&#20379;&#20851;&#38190;&#30693;&#35782;&#65292;&#22686;&#24378;&#22478;&#24066;&#26102;&#31354;&#39044;&#27979;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#30340;&#22478;&#24066;&#26102;&#31354;&#39044;&#27979;&#23545;&#26234;&#24935;&#22478;&#24066;&#30340;&#21457;&#23637;&#21644;&#36816;&#33829;&#33267;&#20851;&#37325;&#35201;&#12290;&#20316;&#20026;&#26032;&#20852;&#30340;&#26500;&#24314;&#27169;&#22359;&#65292;&#22810;&#28304;&#22478;&#24066;&#25968;&#25454;&#36890;&#24120;&#34987;&#25972;&#21512;&#20026;&#22478;&#24066;&#30693;&#35782;&#22270;&#35889;&#65288;UrbanKG&#65289;&#65292;&#20026;&#22478;&#24066;&#26102;&#31354;&#39044;&#27979;&#27169;&#22411;&#25552;&#20379;&#20851;&#38190;&#30693;&#35782;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;UrbanKG&#36890;&#24120;&#20026;&#29305;&#23450;&#30340;&#19979;&#28216;&#39044;&#27979;&#20219;&#21153;&#37327;&#36523;&#23450;&#21046;&#65292;&#19988;&#19981;&#20844;&#24320;&#21487;&#29992;&#65292;&#38480;&#21046;&#20102;&#28508;&#22312;&#30340;&#36827;&#23637;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;UUKG&#65292;&#19968;&#31181;&#32479;&#19968;&#30340;&#22478;&#24066;&#30693;&#35782;&#22270;&#35889;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#22686;&#24378;&#30693;&#35782;&#30340;&#22478;&#24066;&#26102;&#31354;&#39044;&#27979;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#26500;&#24314;&#20102;&#21253;&#21547;&#25968;&#30334;&#19975;&#20010;&#19977;&#20803;&#32452;&#30340;UrbanKG&#65292;&#36830;&#25509;&#20102;&#22478;&#24066;&#20013;&#30340;&#24322;&#26500;&#23454;&#20307;&#65292;&#22914;&#34892;&#25919;&#21306;&#12289;&#20852;&#36259;&#28857;&#21644;&#36947;&#36335;&#27573;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545;&#26500;&#24314;&#30340;UrbanKG&#36827;&#34892;&#20102;&#23450;&#24615;&#21644;&#23450;&#37327;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#21508;&#31181;&#39640;&#38454;&#32467;&#26500;&#27169;&#24335;&#65292;&#22914;&#23618;&#27425;&#32467;&#26500;&#21644;&#24490;&#29615;&#65292;&#21487;&#20197;&#29992;&#26469;&#25552;&#39640;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate Urban SpatioTemporal Prediction (USTP) is of great importance to the development and operation of the smart city. As an emerging building block, multi-sourced urban data are usually integrated as urban knowledge graphs (UrbanKGs) to provide critical knowledge for urban spatiotemporal prediction models. However, existing UrbanKGs are often tailored for specific downstream prediction tasks and are not publicly available, which limits the potential advancement. This paper presents UUKG, the unified urban knowledge graph dataset for knowledge-enhanced urban spatiotemporal predictions. Specifically, we first construct UrbanKGs consisting of millions of triplets for two metropolises by connecting heterogeneous urban entities such as administrative boroughs, POIs, and road segments. Moreover, we conduct qualitative and quantitative analysis on constructed UrbanKGs and uncover diverse high-order structural patterns, such as hierarchies and cycles, that can be leveraged to benefit down
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#31232;&#30095;&#21487;&#35266;&#27979;&#29615;&#22659;&#20013;&#30340;&#23545;&#25239;&#25628;&#32034;&#21644;&#36861;&#36394;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#21487;&#23398;&#20064;&#30340;&#36807;&#28388;&#27169;&#22411;&#26469;&#20272;&#35745;&#23545;&#25163;&#20301;&#32622;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#26816;&#27979;&#29575;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2306.11301</link><description>&lt;p&gt;
&#22312;&#31232;&#30095;&#21487;&#35266;&#27979;&#29615;&#22659;&#20013;&#65292;&#20351;&#29992;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#23545;&#25239;&#25628;&#32034;&#21644;&#36861;&#36394;
&lt;/p&gt;
&lt;p&gt;
Adversarial Search and Tracking with Multiagent Reinforcement Learning in Sparsely Observable Environment. (arXiv:2306.11301v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11301
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#31232;&#30095;&#21487;&#35266;&#27979;&#29615;&#22659;&#20013;&#30340;&#23545;&#25239;&#25628;&#32034;&#21644;&#36861;&#36394;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#21487;&#23398;&#20064;&#30340;&#36807;&#28388;&#27169;&#22411;&#26469;&#20272;&#35745;&#23545;&#25163;&#20301;&#32622;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#26816;&#27979;&#29575;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#20010;&#25628;&#32034;&#21644;&#36861;&#36394;&#38382;&#39064;&#65292;&#20854;&#20013;&#19968;&#20010;&#21160;&#24577;&#25628;&#32034;&#22242;&#38431;&#24517;&#39035;&#21512;&#20316;&#36861;&#36394;&#19968;&#20010;&#23545;&#25239;&#24615;&#30340;&#12289;&#38590;&#20197;&#25429;&#25417;&#30340;&#20195;&#29702;&#12290;&#24322;&#26500;&#30340;&#25628;&#32034;&#22242;&#38431;&#21487;&#33021;&#21482;&#33021;&#22312;&#19968;&#20010;&#22823;&#30340;&#25628;&#32034;&#31354;&#38388;&#20869;&#35775;&#38382;&#26377;&#38480;&#25968;&#37327;&#30340;&#36807;&#21435;&#23545;&#25163;&#36712;&#36857;&#12290;&#30001;&#20110;&#23545;&#25163;&#22312;&#22823;&#31354;&#38388;&#20869;&#34920;&#29616;&#20986;&#21453;&#24212;&#24615;&#21644;&#27450;&#39575;&#24615;&#30340;&#36867;&#36991;&#34892;&#20026;&#65292;&#23548;&#33268;&#25628;&#32034;&#20195;&#29702;&#30340;&#26816;&#27979;&#21464;&#24471;&#31232;&#30095;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#26694;&#26550;&#65292;&#21033;&#29992;&#25105;&#20204;&#21487;&#23398;&#20064;&#30340;&#36807;&#28388;&#27169;&#22411;&#23545;&#23545;&#25163;&#20301;&#32622;&#36827;&#34892;&#20272;&#35745;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;MARL&#26550;&#26500;&#21487;&#20197;&#36229;&#36234;&#25152;&#26377;&#22522;&#32447;&#26041;&#27861;&#65292;&#24182;&#23454;&#29616;&#20102;46%&#30340;&#26816;&#27979;&#29575;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study a search and tracking (S&amp;T) problem where a team of dynamic search agents must collaborate to track an adversarial, evasive agent. The heterogeneous search team may only have access to a limited number of past adversary trajectories within a large search space. This problem is challenging for both model-based searching and reinforcement learning (RL) methods since the adversary exhibits reactionary and deceptive evasive behaviors in a large space leading to sparse detections for the search agents. To address this challenge, we propose a novel Multi-Agent RL (MARL) framework that leverages the estimated adversary location from our learnable filtering model. We show that our MARL architecture can outperform all baselines and achieves a 46% increase in detection rate.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#21457;&#29616;&#29983;&#25104;&#24335; AI &#24037;&#20855; ChatGPT &#21487;&#20197;&#26356;&#26377;&#25928;&#22320;&#23637;&#31034;&#32929;&#31080;&#24066;&#22330;&#30456;&#20851;&#20449;&#24687;&#65292;&#25552;&#20986;&#20102;&#20449;&#24687;&#33192;&#32960;&#25351;&#26631;&#24182;&#35777;&#26126;&#20854;&#19982;&#36127;&#38754;&#30340;&#36164;&#26412;&#24066;&#22330;&#21518;&#26524;&#30456;&#20851;&#65292;&#21516;&#26102;&#23637;&#31034;&#20854;&#22312;&#26500;&#24314;&#38024;&#23545;&#24615;&#24635;&#32467;&#26041;&#38754;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.10224</link><description>&lt;p&gt;
&#33192;&#32960;&#30340;&#25259;&#38706;&#65306;ChatGPT&#26159;&#21542;&#33021;&#24110;&#21161;&#25237;&#36164;&#32773;&#22788;&#29702;&#36130;&#21153;&#20449;&#24687;&#65311;
&lt;/p&gt;
&lt;p&gt;
Bloated Disclosures: Can ChatGPT Help Investors Process Financial Information?. (arXiv:2306.10224v1 [econ.GN])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10224
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#21457;&#29616;&#29983;&#25104;&#24335; AI &#24037;&#20855; ChatGPT &#21487;&#20197;&#26356;&#26377;&#25928;&#22320;&#23637;&#31034;&#32929;&#31080;&#24066;&#22330;&#30456;&#20851;&#20449;&#24687;&#65292;&#25552;&#20986;&#20102;&#20449;&#24687;&#33192;&#32960;&#25351;&#26631;&#24182;&#35777;&#26126;&#20854;&#19982;&#36127;&#38754;&#30340;&#36164;&#26412;&#24066;&#22330;&#21518;&#26524;&#30456;&#20851;&#65292;&#21516;&#26102;&#23637;&#31034;&#20854;&#22312;&#26500;&#24314;&#38024;&#23545;&#24615;&#24635;&#32467;&#26041;&#38754;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335; AI &#24037;&#20855;&#65288;&#22914; ChatGPT&#65289;&#21487;&#20197;&#20174;&#26681;&#26412;&#19978;&#25913;&#21464;&#25237;&#36164;&#32773;&#22788;&#29702;&#20449;&#24687;&#30340;&#26041;&#24335;&#12290;&#25105;&#20204;&#20351;&#29992;&#32929;&#31080;&#24066;&#22330;&#20316;&#20026;&#23454;&#39564;&#23460;&#65292;&#25506;&#31350;&#36825;&#20123;&#24037;&#20855;&#22312;&#24635;&#32467;&#22797;&#26434;&#30340;&#20844;&#21496;&#25259;&#38706;&#20449;&#24687;&#26102;&#30340;&#32463;&#27982;&#25928;&#29992;&#12290;&#24635;&#32467;&#25688;&#35201;&#26126;&#26174;&#26356;&#30701;&#65292;&#36890;&#24120;&#27604;&#21407;&#22987;&#25991;&#26412;&#32553;&#30701;&#36229;&#36807; 70%&#65292;&#32780;&#20449;&#24687;&#20869;&#23481;&#24471;&#21040;&#22686;&#24378;&#12290;&#24403;&#19968;&#20221;&#25991;&#20214;&#20855;&#26377;&#31215;&#26497;&#65288;&#28040;&#26497;&#65289;&#24773;&#24863;&#26102;&#65292;&#20854;&#24635;&#32467;&#21464;&#24471;&#26356;&#31215;&#26497;&#65288;&#28040;&#26497;&#65289;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#24635;&#32467;&#23545;&#35299;&#37322;&#32929;&#24066;&#23545;&#25259;&#38706;&#20449;&#24687;&#30340;&#21453;&#24212;&#26356;&#26377;&#25928;&#12290;&#22522;&#20110;&#36825;&#20123;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20449;&#24687;&#8220;&#33192;&#32960;&#8221;&#25351;&#26631;&#12290;&#25105;&#20204;&#26174;&#31034;&#65292;&#33192;&#32960;&#30340;&#25259;&#38706;&#19982;&#36127;&#38754;&#30340;&#36164;&#26412;&#24066;&#22330;&#21518;&#26524;&#30456;&#20851;&#65292;&#20363;&#22914;&#26356;&#20302;&#30340;&#20215;&#26684;&#26377;&#25928;&#24615;&#21644;&#26356;&#39640;&#30340;&#20449;&#24687;&#19981;&#23545;&#31216;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20010;&#27169;&#22411;&#22312;&#26500;&#24314;&#38024;&#23545;&#24615;&#24635;&#32467;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#20197;&#30830;&#23450;&#20844;&#21496;&#30340;&#65288;&#38750;&#65289;&#36130;&#21153;&#34920;&#29616;&#21644;&#39118;&#38505;&#12290;&#24635;&#20043;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#20687; ChatGPT &#36825;&#26679;&#30340;&#29983;&#25104;&#24335; AI &#24037;&#20855;&#21487;&#20197;&#26377;&#25928;&#22320;&#24110;&#21161;&#25237;&#36164;&#32773;&#26356;&#39640;&#25928;&#22320;&#22788;&#29702;&#36130;&#21153;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative AI tools such as ChatGPT can fundamentally change the way investors process information. We probe the economic usefulness of these tools in summarizing complex corporate disclosures using the stock market as a laboratory. The unconstrained summaries are dramatically shorter, often by more than 70% compared to the originals, whereas their information content is amplified. When a document has a positive (negative) sentiment, its summary becomes more positive (negative). More importantly, the summaries are more effective at explaining stock market reactions to the disclosed information. Motivated by these findings, we propose a measure of information "bloat." We show that bloated disclosure is associated with adverse capital markets consequences, such as lower price efficiency and higher information asymmetry. Finally, we show that the model is effective at constructing targeted summaries that identify firms' (non-)financial performance and risks. Collectively, our results indi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24320;&#21019;&#24615;&#22320;&#30740;&#31350;&#20102;&#22522;&#20110; prompt &#23398;&#20064;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411; API &#30340;&#29305;&#27931;&#20234;&#26131;&#24863;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#40657;&#30418;&#26694;&#26550;&#8212;&#8212;TrojPrompt&#65292;&#29992;&#20110;&#29983;&#25104;&#36890;&#29992;&#21644;&#38544;&#34109;&#30340;&#35302;&#21457;&#22120;&#65292;&#24182;&#23558;&#29305;&#27931;&#20234;&#26408;&#39532;&#25554;&#20837;&#30828;&#25552;&#31034;&#12290;</title><link>http://arxiv.org/abs/2306.06815</link><description>&lt;p&gt;
TrojPrompt&#65306;&#22522;&#20110;&#40657;&#30418;&#26041;&#24335;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#26408;&#39532;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
TrojPrompt: A Black-box Trojan Attack on Pre-trained Language Models. (arXiv:2306.06815v1 [cs.CR] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06815
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24320;&#21019;&#24615;&#22320;&#30740;&#31350;&#20102;&#22522;&#20110; prompt &#23398;&#20064;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411; API &#30340;&#29305;&#27931;&#20234;&#26131;&#24863;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#40657;&#30418;&#26694;&#26550;&#8212;&#8212;TrojPrompt&#65292;&#29992;&#20110;&#29983;&#25104;&#36890;&#29992;&#21644;&#38544;&#34109;&#30340;&#35302;&#21457;&#22120;&#65292;&#24182;&#23558;&#29305;&#27931;&#20234;&#26408;&#39532;&#25554;&#20837;&#30828;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Prompt&#23398;&#20064;&#34987;&#35777;&#26126;&#22312;&#25552;&#39640;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#36866;&#24212;&#24615;&#26041;&#38754;&#38750;&#24120;&#26377;&#25928;&#65292;&#36229;&#36234;&#20102;&#20256;&#32479;&#30340;&#24494;&#35843;&#33539;&#24335;&#65292;&#24182;&#22312;&#19987;&#20026;&#23569;&#26679;&#26412;&#23398;&#20064;&#22330;&#26223;&#37327;&#36523;&#23450;&#21046;&#30340;&#24212;&#29992;&#31243;&#24207;&#21644;API&#20013;&#23637;&#29616;&#20102;&#26480;&#20986;&#30340;&#21069;&#26223;&#12290;&#20294;&#26159;&#65292;&#23613;&#31649;prompt&#23398;&#20064;&#30340;API&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#20294;&#23427;&#20204;&#30340;&#23433;&#20840;&#38382;&#39064;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#26412;&#25991;&#22312;prompt&#23398;&#20064;&#30340;PLM API&#30340;&#29305;&#27931;&#20234;&#26131;&#24863;&#24615;&#26041;&#38754;&#36827;&#34892;&#20102;&#24320;&#21019;&#24615;&#30740;&#31350;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#31163;&#25955;&#25552;&#31034;&#65292;&#23569;&#26679;&#26412;&#21644;&#40657;&#30418;&#35774;&#32622;&#26159;&#20960;&#20010;&#20851;&#38190;&#25361;&#25112;&#65292;&#38480;&#21046;&#20102;&#29616;&#26377;&#21518;&#38376;&#25915;&#20987;&#30340;&#36866;&#29992;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TrojPrompt&#65292;&#36825;&#26159;&#19968;&#31181;&#33258;&#21160;&#30340;&#40657;&#30418;&#26694;&#26550;&#65292;&#21487;&#26377;&#25928;&#29983;&#25104;&#36890;&#29992;&#30340;&#21644;&#38544;&#31192;&#30340;&#35302;&#21457;&#22120;&#65292;&#24182;&#23558;&#29305;&#27931;&#20234;&#26408;&#39532;&#25554;&#20837;&#30828;&#25552;&#31034;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;API&#39537;&#21160;&#30340;&#36890;&#29992;&#35302;&#21457;&#22120;&#21457;&#29616;&#31639;&#27861;&#65292;&#36890;&#36807;&#26597;&#35810;&#21463;&#23475;&#32773;PLM API&#65292;&#20026;&#21508;&#31181;&#36755;&#20837;&#29983;&#25104;&#36890;&#29992;&#35302;&#21457;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prompt learning has been proven to be highly effective in improving pre-trained language model (PLM) adaptability, surpassing conventional fine-tuning paradigms, and showing exceptional promise in an ever-growing landscape of applications and APIs tailored for few-shot learning scenarios. Despite the growing prominence of prompt learning-based APIs, their security concerns remain underexplored. In this paper, we undertake a pioneering study on the Trojan susceptibility of prompt-learning PLM APIs. We identified several key challenges, including discrete-prompt, few-shot, and black-box settings, which limit the applicability of existing backdoor attacks. To address these challenges, we propose TrojPrompt, an automatic and black-box framework to effectively generate universal and stealthy triggers and insert Trojans into hard prompts. Specifically, we propose a universal API-driven trigger discovery algorithm for generating universal triggers for various inputs by querying victim PLM API
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20449;&#21495;&#26102;&#24207;&#36923;&#36753;&#24341;&#23548;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#23558;&#20219;&#21153;&#35268;&#33539;&#21644;&#23433;&#20840;&#35268;&#33539;&#32467;&#21512;&#36215;&#26469;&#20316;&#20026;&#22870;&#21169;&#30340;&#29983;&#25104;&#26465;&#20214;&#12290;</title><link>http://arxiv.org/abs/2306.06808</link><description>&lt;p&gt;
&#30001;&#20449;&#21495;&#26102;&#24207;&#36923;&#36753;&#35268;&#33539;&#24341;&#23548;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Multi-Agent Reinforcement Learning Guided by Signal Temporal Logic Specifications. (arXiv:2306.06808v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06808
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20449;&#21495;&#26102;&#24207;&#36923;&#36753;&#24341;&#23548;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#23558;&#20219;&#21153;&#35268;&#33539;&#21644;&#23433;&#20840;&#35268;&#33539;&#32467;&#21512;&#36215;&#26469;&#20316;&#20026;&#22870;&#21169;&#30340;&#29983;&#25104;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22870;&#21169;&#35774;&#35745;&#26159;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65292;&#28982;&#32780;&#26576;&#20123;&#20219;&#21153;&#21644;&#35774;&#35745;&#32773;&#30340;&#30446;&#26631;&#21487;&#33021;&#19981;&#36866;&#21512;&#23450;&#20041;&#20026;&#26631;&#37327;&#25104;&#26412;&#20989;&#25968;&#12290;&#22312;&#21508;&#31181;&#25216;&#26415;&#20013;&#65292;&#19982;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#38598;&#25104;&#30340;&#24418;&#24335;&#21270;&#26041;&#27861;&#30001;&#20110;&#20854;&#34920;&#36798;&#33021;&#21147;&#21644;&#28789;&#27963;&#24615;&#32780;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#21487;&#20197;&#23450;&#20041;&#20195;&#29702;&#30340;&#22870;&#21169;&#21644;&#35201;&#27714;&#30340;&#19981;&#21516;&#29366;&#24577;&#21644;&#21160;&#20316;&#12290;&#28982;&#32780;&#65292;&#22914;&#20309;&#21033;&#29992;&#20449;&#21495;&#26102;&#24207;&#36923;&#36753;&#65288;STL&#65289;&#26469;&#24341;&#23548;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#22870;&#21169;&#35774;&#35745;&#20173;&#26410;&#34987;&#25506;&#32034;&#12290;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20013;&#30340;&#22797;&#26434;&#20132;&#20114;&#12289;&#24322;&#26500;&#30446;&#26631;&#21644;&#20851;&#38190;&#23433;&#20840;&#35201;&#27714;&#20351;&#24471;&#36825;&#20010;&#38382;&#39064;&#26356;&#21152;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;STL&#24341;&#23548;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#12290;STL&#35201;&#27714;&#34987;&#35774;&#35745;&#20026;&#21516;&#26102;&#21253;&#25324;&#27599;&#20010;&#26234;&#33021;&#20307;&#30340;&#20219;&#21153;&#35268;&#33539;&#21644;&#23433;&#20840;&#35268;&#33539;&#65292;&#24182;&#21033;&#29992;STL&#35268;&#33539;&#30340;&#40065;&#26834;&#24615;&#20540;&#29983;&#25104;&#22870;&#21169;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reward design is a key component of deep reinforcement learning, yet some tasks and designer's objectives may be unnatural to define as a scalar cost function. Among the various techniques, formal methods integrated with DRL have garnered considerable attention due to their expressiveness and flexibility to define the reward and requirements for different states and actions of the agent. However, how to leverage Signal Temporal Logic (STL) to guide multi-agent reinforcement learning reward design remains unexplored. Complex interactions, heterogeneous goals and critical safety requirements in multi-agent systems make this problem even more challenging. In this paper, we propose a novel STL-guided multi-agent reinforcement learning framework. The STL requirements are designed to include both task specifications according to the objective of each agent and safety specifications, and the robustness values of the STL specifications are leveraged to generate rewards. We validate the advanta
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20195;&#30721;&#30456;&#20114;&#36716;&#25442;&#26041;&#27861;&#65292;&#21033;&#29992;&#27880;&#24847;&#21147;&#26426;&#21046;&#12289;&#32534;&#35793;&#21644;&#31526;&#21495;&#25191;&#34892;&#27979;&#35797;&#29983;&#25104;&#36827;&#34892;&#31561;&#20215;&#27979;&#35797;&#12290;&#22312;&#24191;&#27867;&#30340;&#23454;&#39564;&#20013;&#65292;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#32534;&#35793;&#21644;&#36816;&#34892;&#26102;&#31561;&#20215;&#20934;&#30830;&#24615;&#31561;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#36716;&#25442;&#22120;&#21644;&#32763;&#35793;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2306.06755</link><description>&lt;p&gt;
&#27880;&#24847;&#21147;&#12289;&#32534;&#35793;&#21644;&#22522;&#20110;&#27714;&#35299;&#22120;&#30340;&#31526;&#21495;&#20998;&#26512;&#26159;&#24744;&#25152;&#38656;&#35201;&#30340;&#19968;&#20999;
&lt;/p&gt;
&lt;p&gt;
Attention, Compilation, and Solver-based Symbolic Analysis are All You Need. (arXiv:2306.06755v2 [cs.PL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06755
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20195;&#30721;&#30456;&#20114;&#36716;&#25442;&#26041;&#27861;&#65292;&#21033;&#29992;&#27880;&#24847;&#21147;&#26426;&#21046;&#12289;&#32534;&#35793;&#21644;&#31526;&#21495;&#25191;&#34892;&#27979;&#35797;&#29983;&#25104;&#36827;&#34892;&#31561;&#20215;&#27979;&#35797;&#12290;&#22312;&#24191;&#27867;&#30340;&#23454;&#39564;&#20013;&#65292;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#32534;&#35793;&#21644;&#36816;&#34892;&#26102;&#31561;&#20215;&#20934;&#30830;&#24615;&#31561;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#36716;&#25442;&#22120;&#21644;&#32763;&#35793;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;Java&#21040;Python&#65288;J2P&#65289;&#21644;Python&#21040;Java&#65288;P2J&#65289;&#20195;&#30721;&#30456;&#20114;&#36716;&#25442;&#26041;&#27861;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;CoTran&#30340;&#30456;&#20851;&#24037;&#20855;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#12289;&#32534;&#35793;&#21644;&#22522;&#20110;&#31526;&#21495;&#25191;&#34892;&#30340;&#27979;&#35797;&#29983;&#25104;&#65292;&#29992;&#20110;&#36755;&#20837;&#21644;&#36755;&#20986;&#31243;&#24207;&#20043;&#38388;&#30340;&#31561;&#20215;&#27979;&#35797;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20462;&#25913;&#20102;&#20856;&#22411;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#24490;&#29615;&#65292;&#21152;&#20837;&#20102;&#32534;&#35793;&#22120;&#21644;&#31526;&#21495;&#25191;&#34892;&#25439;&#22833;&#12290;&#36890;&#36807;&#22312;&#36229;&#36807;57,000&#20010;Java-Python&#31561;&#20215;&#23545;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#23558;CoTran&#19982;&#20854;&#20182;12&#20010;&#36716;&#25442;&#22120;&#21644;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32763;&#35793;&#24037;&#20855;&#36827;&#34892;&#24191;&#27867;&#30340;&#23454;&#39564;&#27604;&#36739;&#65292;&#25105;&#20204;&#21457;&#29616;CoTran&#22312;&#35832;&#22914;&#32534;&#35793;&#21644;&#36816;&#34892;&#26102;&#31561;&#20215;&#20934;&#30830;&#24615;&#31561;&#30456;&#20851;&#25351;&#26631;&#19978;&#34920;&#29616;&#20248;&#20110;&#23427;&#20204;&#12290;&#20363;&#22914;&#65292;&#25105;&#20204;&#30340;&#24037;&#20855;&#22312;J2P&#36716;&#25442;&#20013;&#33719;&#24471;97.43%&#30340;&#32534;&#35793;&#20934;&#30830;&#24615;&#21644;49.66%&#30340;&#36816;&#34892;&#26102;&#31561;&#20215;&#20934;&#30830;&#24615;&#65292;&#32780;&#26368;&#25509;&#36817;&#30340;&#31454;&#20105;&#24037;&#20855;&#20998;&#21035;&#21482;&#26377;92.84%&#21644;40.95%&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present a Java-to-Python (J2P) and Python-to-Java (P2J) back-to-back code translation method, and an associated tool called CoTran, based on large language models (LLMs). Our method leverages the attention mechanism of LLMs, compilation, and symbolic execution-based test generation for equivalence testing between the input and output programs. More precisely, we modify the typical LLM training loop to incorporate compiler and symbolic execution loss. Via extensive experiments comparing CoTran with 12 other transpilers and LLM-based translation tools over a benchmark of more than 57,000 Java-Python equivalent pairs, we show that CoTran outperforms them on relevant metrics such as compilation and runtime equivalence accuracy. For example, our tool gets 97.43% compilation accuracy and 49.66% runtime equivalence accuracy for J2P translation, whereas the nearest competing tool only gets 92.84% and 40.95% respectively.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#31867;&#27604;&#25512;&#29702;&#33021;&#21542;&#23454;&#29616;&#23545;&#21487;&#32452;&#21512;&#35270;&#35273;&#21050;&#28608;&#25104;&#20998;&#30340;&#19978;&#19979;&#25991;&#20869;&#32452;&#21512;&#65292;&#36890;&#36807;&#24341;&#20837;&#19977;&#20010;&#22522;&#20934;&#27979;&#35797;&#22871;&#20214;&#65292;&#25552;&#20379;&#20102;&#35774;&#35745;&#31867;&#27604;&#25512;&#29702;&#30340;&#20803;&#23398;&#20064;&#26694;&#26550; Im-Promptu&#12290;&#20351;&#29992; Im-Promptu &#21487;&#20197;&#35757;&#32451;&#22810;&#20010;&#20855;&#26377;&#19981;&#21516;&#32452;&#21512;&#27700;&#24179;&#30340;&#20195;&#29702;&#65292;&#21253;&#25324;&#30690;&#37327;&#34920;&#31034;&#12289;&#34917;&#19969;&#34920;&#31034;&#21644;&#29289;&#20307;&#27133;&#12290;</title><link>http://arxiv.org/abs/2305.17262</link><description>&lt;p&gt;
Im-Promptu: &#20174;&#22270;&#20687;&#25552;&#31034;&#36827;&#34892;&#19978;&#19979;&#25991;&#32452;&#21512;
&lt;/p&gt;
&lt;p&gt;
Im-Promptu: In-Context Composition from Image Prompts. (arXiv:2305.17262v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17262
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#31867;&#27604;&#25512;&#29702;&#33021;&#21542;&#23454;&#29616;&#23545;&#21487;&#32452;&#21512;&#35270;&#35273;&#21050;&#28608;&#25104;&#20998;&#30340;&#19978;&#19979;&#25991;&#20869;&#32452;&#21512;&#65292;&#36890;&#36807;&#24341;&#20837;&#19977;&#20010;&#22522;&#20934;&#27979;&#35797;&#22871;&#20214;&#65292;&#25552;&#20379;&#20102;&#35774;&#35745;&#31867;&#27604;&#25512;&#29702;&#30340;&#20803;&#23398;&#20064;&#26694;&#26550; Im-Promptu&#12290;&#20351;&#29992; Im-Promptu &#21487;&#20197;&#35757;&#32451;&#22810;&#20010;&#20855;&#26377;&#19981;&#21516;&#32452;&#21512;&#27700;&#24179;&#30340;&#20195;&#29702;&#65292;&#21253;&#25324;&#30690;&#37327;&#34920;&#31034;&#12289;&#34917;&#19969;&#34920;&#31034;&#21644;&#29289;&#20307;&#27133;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#26159;&#23569;&#26679;&#26412;&#23398;&#20064;&#32773;&#65292;&#21487;&#20197;&#20174;&#23569;&#37327;&#28436;&#31034;&#20013;&#35299;&#20915;&#21508;&#31181;&#20219;&#21153;&#12290;&#36825;&#31181;&#38544;&#21547;&#30340;&#20219;&#21153;&#29702;&#35299;&#34920;&#26126;&#65292;&#21333;&#35789;&#20196;&#29260;&#19978;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#21487;&#33021;&#22312;&#31867;&#27604;&#25512;&#29702;&#20013;&#21457;&#25381;&#20316;&#29992;&#12290;&#26412;&#25991;&#30740;&#31350;&#31867;&#27604;&#25512;&#29702;&#26159;&#21542;&#33021;&#23454;&#29616;&#23545;&#21487;&#32452;&#21512;&#35270;&#35273;&#21050;&#28608;&#25104;&#20998;&#30340;&#19978;&#19979;&#25991;&#20869;&#32452;&#21512;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19977;&#20010;&#22522;&#20934;&#27979;&#35797;&#22871;&#20214;&#65292;&#20197;&#27979;&#35797;&#35270;&#35273;&#19978;&#19979;&#25991;&#23398;&#20064;&#22120;&#30340;&#27867;&#21270;&#23646;&#24615;&#12290;&#25105;&#20204;&#35268;&#33539;&#21270;&#20102;&#22522;&#20110;&#31867;&#27604;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#22120;&#30340;&#27010;&#24565;&#65292;&#24182;&#29992;&#23427;&#26469;&#35774;&#35745;&#19968;&#20010;&#20803;&#23398;&#20064;&#26694;&#26550;&#31216;&#20026; Im-Promptu&#12290;&#34429;&#28982;&#35821;&#35328;&#30340;&#25152;&#38656;&#20196;&#29260;&#31890;&#24230;&#24050;&#32463;&#24471;&#21040;&#20102;&#20805;&#20998;&#35777;&#23454;&#65292;&#20294;&#29992;&#20110;&#23454;&#29616;&#35270;&#35273;&#21050;&#28608;&#20869;&#19978;&#19979;&#25991;&#27867;&#21270;&#30340;&#36866;&#24403;&#32452;&#21512;&#31890;&#24230;&#36890;&#24120;&#26410;&#32463;&#25351;&#23450;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20351;&#29992; Im-Promptu &#35757;&#32451;&#22810;&#20010;&#20855;&#26377;&#19981;&#21516;&#32452;&#21512;&#27700;&#24179;&#30340;&#20195;&#29702;&#65292;&#21253;&#25324;&#30690;&#37327;&#34920;&#31034;&#12289;&#34917;&#19969;&#34920;&#31034;&#21644;&#29289;&#20307;&#27133;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models are few-shot learners that can solve diverse tasks from a handful of demonstrations. This implicit understanding of tasks suggests that the attention mechanisms over word tokens may play a role in analogical reasoning. In this work, we investigate whether analogical reasoning can enable in-context composition over composable elements of visual stimuli. First, we introduce a suite of three benchmarks to test the generalization properties of a visual in-context learner. We formalize the notion of an analogy-based in-context learner and use it to design a meta-learning framework called Im-Promptu. Whereas the requisite token granularity for language is well established, the appropriate compositional granularity for enabling in-context generalization in visual stimuli is usually unspecified. To this end, we use Im-Promptu to train multiple agents with different levels of compositionality, including vector representations, patch representations, and object slots. Our e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#27573;&#24490;&#29615;Transformer&#65288;SRformer&#65289;&#26469;&#20943;&#23569;&#35745;&#31639;/&#20869;&#23384;&#25104;&#26412;&#65292;&#24182;&#20351;&#29992;RAF&#23618;&#22788;&#29702;&#36328;&#27573;&#30340;&#20449;&#24687;&#65292;&#20174;&#32780;&#25552;&#39640;&#24207;&#21015;&#22788;&#29702;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.16340</link><description>&lt;p&gt;
&#20998;&#27573;&#24490;&#29615;Transformer:&#19968;&#31181;&#39640;&#25928;&#30340;&#24207;&#21015;&#21040;&#24207;&#21015;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Segmented Recurrent Transformer: An Efficient Sequence-to-Sequence Model. (arXiv:2305.16340v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16340
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#27573;&#24490;&#29615;Transformer&#65288;SRformer&#65289;&#26469;&#20943;&#23569;&#35745;&#31639;/&#20869;&#23384;&#25104;&#26412;&#65292;&#24182;&#20351;&#29992;RAF&#23618;&#22788;&#29702;&#36328;&#27573;&#30340;&#20449;&#24687;&#65292;&#20174;&#32780;&#25552;&#39640;&#24207;&#21015;&#22788;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#22312;&#35768;&#22810;&#39046;&#22495;&#20013;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;&#35821;&#35328;&#21644;&#35270;&#35273;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#24207;&#21015;&#38271;&#24230;&#30340;&#22686;&#21152;&#65292;&#23427;&#20204;&#30340;&#35745;&#31639;&#25104;&#26412;&#21576;&#20108;&#27425;&#22686;&#38271;&#65292;&#20351;&#24471;&#23427;&#20204;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#24212;&#29992;&#20013;&#20351;&#29992;&#25104;&#20026;&#19981;&#21487;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#23558;&#25972;&#20010;&#24207;&#21015;&#21010;&#20998;&#25104;&#33509;&#24178;&#27573;&#12290;&#28982;&#21518;&#20351;&#29992;&#20855;&#26377;&#24490;&#29615;&#32467;&#26500;&#30340;&#31070;&#32463;&#20803;&#26469;&#32858;&#21512;&#36328;&#27573;&#30340;&#20449;&#24687;&#65292;&#20174;&#32780;&#23454;&#29616;&#20855;&#26377;&#36739;&#20302;&#35745;&#31639;/&#20869;&#23384;&#25104;&#26412;&#30340;&#24207;&#21015;&#22788;&#29702;&#33021;&#21147;&#27169;&#22411;&#12290;&#20026;&#20102;&#39564;&#35777;&#36825;&#20010;&#24819;&#27861;&#65292;&#25105;&#20204;&#39318;&#20808;&#30740;&#31350;&#20102;&#20351;&#29992;&#23616;&#37096;Attention&#26426;&#21046;&#23545;&#21333;&#20010;&#27573;&#30340;&#24433;&#21709;&#12290;&#28982;&#21518;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#27573;&#24490;&#29615;Transformer&#65288;SRformer&#65289;&#65292;&#23427;&#23558;&#20998;&#27573;Attention&#21644;&#24490;&#29615;Attention&#30456;&#32467;&#21512;&#12290;&#23427;&#20351;&#29992;&#24490;&#29615;accumulate and fire&#65288;RAF&#65289;&#23618;&#22312;&#30456;&#37051;&#27573;&#20043;&#38388;&#22788;&#29702;&#20449;&#24687;&#12290;&#36890;&#36807;&#26356;&#26032;key&#30340;&#20135;&#21697;&#26469;&#34917;&#20607;&#20943;&#23569;Attention&#31383;&#21475;&#38271;&#24230;&#20135;&#29983;&#30340;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformers have shown dominant performance across a range of domains including language and vision. However, their computational cost grows quadratically with the sequence length, making their usage prohibitive for resource-constrained applications. To counter this, our approach is to divide the whole sequence into segments. The information across segments can then be aggregated using neurons with recurrence leveraging their inherent memory. Such an approach leads to models with sequential processing capability at a lower computation/memory cost. To investigate this idea, first, we examine the effects of using local attention mechanism on the individual segments. Then we propose a segmented recurrent transformer (SRformer) that combines segmented attention with recurrent attention. It uses recurrent accumulate and fire (RAF) layers to process information between consecutive segments. The loss caused by reducing the attention window length is compensated by updating the product of key
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#30340;Sharpness-Aware Minimization&#31639;&#27861;&#22823;&#22823;&#25552;&#39640;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#32780;&#20854;&#20013;&#35268;&#33539;&#21270;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#36890;&#36807;&#31283;&#23450;&#31639;&#27861;&#21644;&#20351;&#20854;&#28418;&#31227;&#27839;&#30528;&#19968;&#31995;&#21015;&#26497;&#23567;&#20540;&#25552;&#21319;&#24615;&#33021;&#65292;&#24182;&#20351;&#31639;&#27861;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.15287</link><description>&lt;p&gt;
&#35268;&#33539;&#21270;&#22312;Sharpness-Aware Minimization&#20013;&#30340;&#20851;&#38190;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
The Crucial Role of Normalization in Sharpness-Aware Minimization. (arXiv:2305.15287v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15287
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#30340;Sharpness-Aware Minimization&#31639;&#27861;&#22823;&#22823;&#25552;&#39640;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#32780;&#20854;&#20013;&#35268;&#33539;&#21270;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#36890;&#36807;&#31283;&#23450;&#31639;&#27861;&#21644;&#20351;&#20854;&#28418;&#31227;&#27839;&#30528;&#19968;&#31995;&#21015;&#26497;&#23567;&#20540;&#25552;&#21319;&#24615;&#33021;&#65292;&#24182;&#20351;&#31639;&#27861;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Sharpness-Aware Minimization&#65288;SAM&#65289;&#26159;&#19968;&#31181;&#22522;&#20110;&#26799;&#24230;&#30340;&#20248;&#21270;&#22120;&#65292;&#26497;&#22823;&#22320;&#25552;&#39640;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;SAM&#26356;&#26032;&#20013;&#35268;&#33539;&#21270;&#36825;&#19968;&#20851;&#38190;&#32452;&#20214;&#30340;&#20316;&#29992;&#65292;&#20174;&#29702;&#35770;&#21644;&#23454;&#39564;&#20004;&#26041;&#38754;&#20998;&#26512;&#20102;&#35268;&#33539;&#21270;&#22312;SAM&#20013;&#23545;&#20984;&#20989;&#25968;&#21644;&#38750;&#20984;&#20989;&#25968;&#30340;&#24433;&#21709;&#65292;&#25581;&#31034;&#20102;&#35268;&#33539;&#21270;&#21457;&#25381;&#30340;&#20004;&#20010;&#20851;&#38190;&#20316;&#29992;&#65306;i&#65289;&#23427;&#26377;&#21161;&#20110;&#31283;&#23450;&#31639;&#27861;&#65307;ii&#65289;&#23427;&#20351;&#31639;&#27861;&#33021;&#22815;&#27839;&#30528;&#19968;&#31995;&#21015;&#26497;&#23567;&#20540;&#65288;&#27969;&#24418;&#65289;&#28418;&#31227;&#65292;&#36825;&#26159;&#26368;&#36817;&#19968;&#20123;&#29702;&#35770;&#24037;&#20316;&#30830;&#23450;&#30340;&#24615;&#33021;&#25552;&#21319;&#20851;&#38190;&#24615;&#36136;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35748;&#20026;&#65292;&#36825;&#20004;&#20010;&#27491;&#24120;&#21270;&#30340;&#23646;&#24615;&#20351;SAM&#23545;&#36229;&#21442;&#25968;&#30340;&#36873;&#25321;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#35777;&#23454;&#20102;SAM&#30340;&#23454;&#29992;&#24615;&#12290;&#21508;&#31181;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#32467;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sharpness-Aware Minimization (SAM) is a recently proposed gradient-based optimizer (Foret et al., ICLR 2021) that greatly improves the prediction performance of deep neural networks. Consequently, there has been a surge of interest in explaining its empirical success. We focus, in particular, on understanding the role played by normalization, a key component of the SAM updates. We theoretically and empirically study the effect of normalization in SAM for both convex and non-convex functions, revealing two key roles played by normalization: i) it helps in stabilizing the algorithm; and ii) it enables the algorithm to drift along a continuum (manifold) of minima -- a property identified by recent theoretical works that is the key to better performance. We further argue that these two properties of normalization make SAM robust against the choice of hyper-parameters, supporting the practicality of SAM. Our conclusions are backed by various experiments.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22411;&#65292;&#21487;&#20381;&#25454;&#29992;&#25143;&#25552;&#20379;&#30340;&#20219;&#24847;&#32423;&#21035;&#30340;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#65292;&#29983;&#25104;&#21512;&#29702;&#19988;&#35270;&#35273;&#19978;&#20196;&#20154;&#24841;&#24742;&#30340;&#24425;&#33394;&#21270;&#25928;&#26524;&#12290;&#36890;&#36807;&#21033;&#29992;&#36328;&#27169;&#24577;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#35821;&#35328;&#29702;&#35299;&#21644;&#39068;&#33394;&#20808;&#39564;&#30693;&#35782;&#65292;&#32467;&#21512;&#26032;&#22411;&#37319;&#26679;&#31574;&#30053;&#21644;&#27169;&#22359;&#35774;&#35745;&#65292;&#23454;&#29616;&#20102;&#23454;&#20363;&#24863;&#30693;&#30340;&#24425;&#33394;&#21270;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.15217</link><description>&lt;p&gt;
L-CAD: &#24102;&#26377;&#20219;&#24847;&#32423;&#21035;&#25551;&#36848;&#30340;&#35821;&#35328;&#24425;&#33394;&#21270;
&lt;/p&gt;
&lt;p&gt;
L-CAD: Language-based Colorization with Any-level Descriptions. (arXiv:2305.15217v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15217
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22411;&#65292;&#21487;&#20381;&#25454;&#29992;&#25143;&#25552;&#20379;&#30340;&#20219;&#24847;&#32423;&#21035;&#30340;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#65292;&#29983;&#25104;&#21512;&#29702;&#19988;&#35270;&#35273;&#19978;&#20196;&#20154;&#24841;&#24742;&#30340;&#24425;&#33394;&#21270;&#25928;&#26524;&#12290;&#36890;&#36807;&#21033;&#29992;&#36328;&#27169;&#24577;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#35821;&#35328;&#29702;&#35299;&#21644;&#39068;&#33394;&#20808;&#39564;&#30693;&#35782;&#65292;&#32467;&#21512;&#26032;&#22411;&#37319;&#26679;&#31574;&#30053;&#21644;&#27169;&#22359;&#35774;&#35745;&#65292;&#23454;&#29616;&#20102;&#23454;&#20363;&#24863;&#30693;&#30340;&#24425;&#33394;&#21270;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#24425;&#33394;&#21270;&#26159;&#22312;&#29992;&#25143;&#21451;&#22909;&#30340;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#25351;&#23548;&#19979;&#29983;&#25104;&#21512;&#29702;&#19988;&#35270;&#35273;&#19978;&#20196;&#20154;&#24841;&#24742;&#30340;&#39068;&#33394;&#12290;&#20197;&#21069;&#30340;&#26041;&#27861;&#38544;&#21547;&#22320;&#20551;&#35774;&#29992;&#25143;&#20026;&#22270;&#20687;&#20013;&#22823;&#22810;&#25968;&#23545;&#35937;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#39068;&#33394;&#25551;&#36848;&#65292;&#36825;&#20250;&#23548;&#33268;&#27425;&#20248;&#30340;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#27169;&#22411;&#65292;&#21487;&#25191;&#34892;&#20219;&#24847;&#32423;&#21035;&#25551;&#36848;&#30340;&#35821;&#35328;&#24425;&#33394;&#21270;&#12290;&#25105;&#20204;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#36328;&#27169;&#24335;&#29983;&#25104;&#27169;&#22411;&#65292;&#20197;&#22788;&#29702;&#20219;&#24847;&#32423;&#21035;&#30340;&#25551;&#36848;&#30340;&#20869;&#22312;&#27495;&#20041;&#65292;&#36890;&#36807;&#20016;&#23500;&#30340;&#39068;&#33394;&#20808;&#39564;&#30693;&#35782;&#36827;&#34892;&#35821;&#35328;&#29702;&#35299;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35774;&#35745;&#20102;&#27169;&#22359;&#26469;&#19982;&#36755;&#20837;&#26465;&#20214;&#23545;&#40784;&#65292;&#20197;&#20445;&#30041;&#23616;&#37096;&#31354;&#38388;&#32467;&#26500;&#24182;&#38450;&#27490;&#24189;&#28789;&#25928;&#24212;&#12290;&#36890;&#36807;&#25552;&#20986;&#30340;&#26032;&#22411;&#37319;&#26679;&#31574;&#30053;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#21508;&#31181;&#22797;&#26434;&#22330;&#26223;&#20013;&#23454;&#29616;&#20102;&#23454;&#20363;&#24863;&#30693;&#30340;&#24425;&#33394;&#21270;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#25105;&#20204;&#22312;&#26377;&#25928;&#22788;&#29702;&#20219;&#24847;&#32423;&#21035;&#25551;&#36848;&#26041;&#38754;&#30340;&#20248;&#21183;&#65292;&#19988;&#22312;&#35821;&#35328;&#24425;&#33394;&#21270;&#21644;&#33258;&#21160;&#24425;&#33394;&#21270;&#26041;&#38754;&#30340;&#34920;&#29616;&#37117;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language-based colorization produces plausible and visually pleasing colors under the guidance of user-friendly natural language descriptions. Previous methods implicitly assume that users provide comprehensive color descriptions for most of the objects in the image, which leads to suboptimal performance. In this paper, we propose a unified model to perform language-based colorization with any-level descriptions. We leverage the pretrained cross-modality generative model for its robust language understanding and rich color priors to handle the inherent ambiguity of any-level descriptions. We further design modules to align with input conditions to preserve local spatial structures and prevent the ghosting effect. With the proposed novel sampling strategy, our model achieves instance-aware colorization in diverse and complex scenarios. Extensive experimental results demonstrate our advantages of effectively handling any-level descriptions and outperforming both language-based and automa
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;JEEBench&#65292;&#19968;&#20010;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#12290;&#36890;&#36807;&#35780;&#20272;&#21508;&#31181;&#27169;&#22411;&#65292;&#32467;&#26524;&#26174;&#31034;&#30446;&#21069;&#26368;&#22909;&#30340;&#27169;&#22411;&#22312;&#35299;&#20915;&#38382;&#39064;&#26102;&#23384;&#22312;&#20195;&#25968;&#25805;&#20316;&#38169;&#35823;&#12289;&#25277;&#35937;&#27010;&#24565;&#36716;&#21270;&#19981;&#20934;&#30830;&#21644;&#38590;&#20197;&#26816;&#32034;&#30456;&#20851;&#27010;&#24565;&#31561;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.15074</link><description>&lt;p&gt;
LLM&#20204;&#36827;&#27493;&#21040;&#20102;&#20160;&#20040;&#31243;&#24230;&#65311;&#19968;&#20010;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#35299;&#20915;&#22522;&#20934;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Have LLMs Advanced Enough? A Challenging Problem Solving Benchmark For Large Language Models. (arXiv:2305.15074v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15074
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;JEEBench&#65292;&#19968;&#20010;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#12290;&#36890;&#36807;&#35780;&#20272;&#21508;&#31181;&#27169;&#22411;&#65292;&#32467;&#26524;&#26174;&#31034;&#30446;&#21069;&#26368;&#22909;&#30340;&#27169;&#22411;&#22312;&#35299;&#20915;&#38382;&#39064;&#26102;&#23384;&#22312;&#20195;&#25968;&#25805;&#20316;&#38169;&#35823;&#12289;&#25277;&#35937;&#27010;&#24565;&#36716;&#21270;&#19981;&#20934;&#30830;&#21644;&#38590;&#20197;&#26816;&#32034;&#30456;&#20851;&#27010;&#24565;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#20960;&#24180;&#37324;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#29616;&#26377;&#30340;&#25512;&#29702;&#22522;&#20934;&#19978;&#30340;&#24615;&#33021;&#26174;&#33879;&#25552;&#39640;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;JEEBench&#65292;&#19968;&#20010;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35780;&#20272;LLMs&#30340;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#12290;&#25105;&#20204;&#20174;&#39640;&#31454;&#20105;&#30340;&#21360;&#24230;&#29702;&#24037;&#23398;&#38498;&#65288;IIT&#65289;JEE-Advanced&#32771;&#35797;&#20013;&#31934;&#36873;&#20986;&#20102;515&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#39044;&#24037;&#31243;&#25968;&#23398;&#12289;&#29289;&#29702;&#21644;&#21270;&#23398;&#38382;&#39064;&#12290;&#22312;&#36825;&#20010;&#22522;&#20934;&#20013;&#65292;&#38271;&#26399;&#25512;&#29702;&#21644;&#28145;&#20837;&#39046;&#22495;&#30693;&#35782;&#30340;&#36816;&#29992;&#23545;&#38382;&#39064;&#30340;&#35299;&#20915;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#23545;&#21508;&#31181;&#24320;&#28304;&#21644;&#19987;&#26377;&#27169;&#22411;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#32467;&#26524;&#26174;&#31034;&#65292;&#21363;&#20351;&#20351;&#29992;&#20102;&#33258;&#19968;&#33268;&#24615;&#12289;&#33258;&#25105;&#23436;&#21892;&#21644;&#24605;&#32500;&#38142;&#25552;&#31034;&#31561;&#25216;&#26415;&#65292;&#26368;&#39640;&#24615;&#33021;&#20063;&#19981;&#21040;40\%&#12290;&#26368;&#22909;&#30340;&#27169;&#22411;GPT-4&#30340;&#20856;&#22411;&#22833;&#36133;&#27169;&#24335;&#21253;&#25324;&#20195;&#25968;&#25805;&#20316;&#38169;&#35823;&#12289;&#23558;&#25277;&#35937;&#27010;&#24565;&#20934;&#30830;&#22320;&#36716;&#21270;&#20026;&#25968;&#23398;&#26041;&#31243;&#20197;&#21450;&#26080;&#27861;&#26816;&#32034;&#30456;&#20851;&#30340;&#39046;&#22495;&#29305;&#23450;&#27010;&#24565;&#12290;&#25105;&#20204;&#36824;&#35266;&#23519;&#21040;&#65292;&#20165;&#20165;&#36890;&#36807;&#36755;&#20837;&#25552;&#31034;&#19981;&#33021;&#35753;&#27169;&#22411;&#25104;&#21151;&#35299;&#20915;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The performance of large language models (LLMs) on existing reasoning benchmarks has significantly improved over the past years. In response, we present JEEBench, a considerably more challenging benchmark dataset for evaluating the problem solving abilities of LLMs. We curate 515 challenging pre-engineering mathematics, physics and chemistry problems from the highly competitive IIT JEE-Advanced exam. Long-horizon reasoning on top of deep in-domain knowledge is essential for solving problems in this benchmark. Our evaluation on various open-source and proprietary models reveals that the highest performance, even after using techniques like self-consistency, self-refinement and chain-of-thought prompting, is less than 40\%. The typical failure modes of GPT-4, the best model, are errors in algebraic manipulation, difficulty in grounding abstract concepts into mathematical equations accurately and failure in retrieving relevant domain-specific concepts. We also observe that by mere prompti
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;ImageNetVC&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#23545;1000&#20010;ImageNet&#31867;&#21035;&#36827;&#34892;&#38646;&#27425;&#21644;&#23569;&#27425;&#36828;&#36317;&#31163;&#35270;&#35273;&#24120;&#35782;&#35780;&#20272;&#12290;&#36890;&#36807;&#35813;&#35780;&#20272;&#65292;&#20998;&#26512;&#20102;&#30446;&#21069;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#35270;&#35273;&#22686;&#24378;&#27169;&#22411;&#23545;&#35270;&#35273;&#24120;&#35782;&#30693;&#35782;&#30340;&#25484;&#25569;&#31243;&#24230;&#65292;&#20026;&#20016;&#23500;&#35821;&#35328;&#27169;&#22411;&#30340;&#35270;&#35273;&#24120;&#35782;&#30693;&#35782;&#25552;&#20379;&#20102;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2305.15028</link><description>&lt;p&gt;
ImageNetVC&#65306;&#22312;1000&#20010;ImageNet&#31867;&#21035;&#19978;&#36827;&#34892;&#38646;&#27425;&#21644;&#23569;&#27425;&#36828;&#36317;&#31163;&#35270;&#35273;&#24120;&#35782;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
ImageNetVC: Zero- and Few-Shot Visual Commonsense Evaluation on 1000 ImageNet Categories. (arXiv:2305.15028v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15028
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;ImageNetVC&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#23545;1000&#20010;ImageNet&#31867;&#21035;&#36827;&#34892;&#38646;&#27425;&#21644;&#23569;&#27425;&#36828;&#36317;&#31163;&#35270;&#35273;&#24120;&#35782;&#35780;&#20272;&#12290;&#36890;&#36807;&#35813;&#35780;&#20272;&#65292;&#20998;&#26512;&#20102;&#30446;&#21069;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#35270;&#35273;&#22686;&#24378;&#27169;&#22411;&#23545;&#35270;&#35273;&#24120;&#35782;&#30693;&#35782;&#30340;&#25484;&#25569;&#31243;&#24230;&#65292;&#20026;&#20016;&#23500;&#35821;&#35328;&#27169;&#22411;&#30340;&#35270;&#35273;&#24120;&#35782;&#30693;&#35782;&#25552;&#20379;&#20102;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#25104;&#20026;&#36890;&#29992;&#25509;&#21475;&#65292;&#23545;&#20840;&#38754;&#30340;&#35270;&#35273;&#30693;&#35782;&#25552;&#20986;&#20102;&#37325;&#35201;&#35201;&#27714;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;LLMs&#21450;&#20854;&#35270;&#35273;&#22686;&#24378;&#22411;&#27169;&#22411;&#65288;VaLMs&#65289;&#22312;&#25484;&#25569;&#35270;&#35273;&#24120;&#35782;&#30693;&#35782;&#26041;&#38754;&#30340;&#27700;&#24179;&#20173;&#19981;&#28165;&#26970;&#12290;&#20026;&#20102;&#35843;&#26597;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ImageNetVC&#65292;&#36825;&#26159;&#19968;&#20010;&#20154;&#24037;&#27880;&#37322;&#30340;&#25968;&#25454;&#38598;&#65292;&#19987;&#38376;&#29992;&#20110;&#23545;1000&#20010;ImageNet&#31867;&#21035;&#36827;&#34892;&#38646;&#27425;&#21644;&#23569;&#27425;&#36828;&#36317;&#31163;&#35270;&#35273;&#24120;&#35782;&#35780;&#20272;&#12290;&#21033;&#29992;ImageNetVC&#65292;&#25105;&#20204;&#23545;&#21333;&#27169;&#24577;LLMs&#21644;VaLMs&#30340;&#22522;&#26412;&#35270;&#35273;&#24120;&#35782;&#30693;&#35782;&#36827;&#34892;&#20102;&#22522;&#20934;&#35780;&#20272;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#24433;&#21709;&#22823;&#35268;&#27169;&#27169;&#22411;&#35270;&#35273;&#24120;&#35782;&#30693;&#35782;&#30340;&#22240;&#32032;&#65292;&#25552;&#20379;&#20102;&#20016;&#23500;&#20102;&#35270;&#35273;&#24120;&#35782;&#30693;&#35782;&#30340;&#35821;&#35328;&#27169;&#22411;&#21457;&#23637;&#30340;&#35265;&#35299;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21644;&#25968;&#25454;&#38598;&#21487;&#22312;https://github.com/hemingkx/ImageNetVC&#19978;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, Large Language Models (LLMs) have been serving as general-purpose interfaces, posing a significant demand for comprehensive visual knowledge. However, it remains unclear how well current LLMs and their visually augmented counterparts (VaLMs) can master visual commonsense knowledge. To investigate this, we propose ImageNetVC, a human-annotated dataset specifically designed for zero- and few-shot visual commonsense evaluation across 1,000 ImageNet categories. Utilizing ImageNetVC, we benchmark the fundamental visual commonsense knowledge of both unimodal LLMs and VaLMs. Furthermore, we analyze the factors affecting the visual commonsense knowledge of large-scale models, providing insights into the development of language models enriched with visual commonsense knowledge. Our code and dataset are available at https://github.com/hemingkx/ImageNetVC.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#24314;&#31435;&#20102;&#28145;&#24230;&#23398;&#20064;&#22312;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#20013;&#25152;&#20351;&#29992;&#30340;&#28145;&#24230;&#38598;&#25104;&#21644;&#65288;&#21464;&#20998;&#65289;&#36125;&#21494;&#26031;&#26041;&#27861;&#30340;&#32479;&#19968;&#29702;&#35770;&#65292;&#36890;&#36807;&#23558;&#38750;&#20984;&#20248;&#21270;&#38382;&#39064;&#36716;&#21270;&#20026;&#27010;&#29575;&#27979;&#24230;&#31354;&#38388;&#19978;&#30340;&#20984;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#19968;&#26063;&#20132;&#20114;&#24335;&#28145;&#24230;&#38598;&#25104;&#26041;&#26696;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#39564;&#35777;&#20102;&#29702;&#35770;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.15027</link><description>&lt;p&gt;
&#28145;&#24230;&#38598;&#25104;&#19982;&#65288;&#21464;&#20998;&#65289;&#36125;&#21494;&#26031;&#26041;&#27861;&#20043;&#38388;&#30340;&#20005;&#26684;&#32852;&#31995;
&lt;/p&gt;
&lt;p&gt;
A Rigorous Link between Deep Ensembles and (Variational) Bayesian Methods. (arXiv:2305.15027v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15027
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#24314;&#31435;&#20102;&#28145;&#24230;&#23398;&#20064;&#22312;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#20013;&#25152;&#20351;&#29992;&#30340;&#28145;&#24230;&#38598;&#25104;&#21644;&#65288;&#21464;&#20998;&#65289;&#36125;&#21494;&#26031;&#26041;&#27861;&#30340;&#32479;&#19968;&#29702;&#35770;&#65292;&#36890;&#36807;&#23558;&#38750;&#20984;&#20248;&#21270;&#38382;&#39064;&#36716;&#21270;&#20026;&#27010;&#29575;&#27979;&#24230;&#31354;&#38388;&#19978;&#30340;&#20984;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#19968;&#26063;&#20132;&#20114;&#24335;&#28145;&#24230;&#38598;&#25104;&#26041;&#26696;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#39564;&#35777;&#20102;&#29702;&#35770;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#39318;&#27425;&#22312;&#25968;&#23398;&#19978;&#24314;&#31435;&#20102;&#36125;&#21494;&#26031;&#12289;&#21464;&#20998;&#36125;&#21494;&#26031;&#21644;&#38598;&#25104;&#26041;&#27861;&#20043;&#38388;&#30340;&#20005;&#26684;&#32852;&#31995;&#12290;&#20854;&#20851;&#38190;&#27493;&#39588;&#26159;&#23558;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#36890;&#24120;&#36935;&#21040;&#30340;&#38750;&#20984;&#20248;&#21270;&#38382;&#39064;&#37325;&#26032;&#34920;&#36848;&#20026;&#27010;&#29575;&#27979;&#24230;&#31354;&#38388;&#20013;&#30340;&#20984;&#20248;&#21270;&#38382;&#39064;&#12290;&#22312;&#25216;&#26415;&#23618;&#38754;&#19978;&#65292;&#25105;&#20204;&#30340;&#36129;&#29486;&#26159;&#36890;&#36807;Wasserstein&#26799;&#24230;&#27969;&#30340;&#36879;&#38236;&#30740;&#31350;&#24191;&#20041;&#21464;&#20998;&#25512;&#26029;&#12290;&#32467;&#26524;&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#29702;&#35770;&#65292;&#28085;&#30422;&#22810;&#31181;&#30475;&#20284;&#26080;&#20851;&#30340;&#26041;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#29992;&#20110;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#65292;&#21253;&#25324;&#28145;&#24230;&#38598;&#25104;&#21644;&#65288;&#21464;&#20998;&#65289;&#36125;&#21494;&#26031;&#26041;&#27861;&#12290;&#36825;&#20026;&#28145;&#24230;&#38598;&#25104;&#32988;&#36807;&#22522;&#20110;&#21442;&#25968;&#21270;&#21464;&#20998;&#25512;&#26029;&#30340;&#31243;&#24207;&#32972;&#21518;&#30340;&#21407;&#22240;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#65292;&#24182;&#20801;&#35768;&#25512;&#23548;&#20855;&#26377;&#25910;&#25947;&#20445;&#35777;&#30340;&#26032;&#38598;&#25104;&#26041;&#26696;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#19968;&#26063;&#20855;&#26377;&#30452;&#25509;&#31867;&#27604;&#20110;&#29289;&#29702;&#23398;&#20013;&#31890;&#23376;&#31995;&#32479;&#20132;&#20114;&#30340;&#20132;&#20114;&#24335;&#28145;&#24230;&#38598;&#25104;&#26469;&#23637;&#31034;&#36825;&#19968;&#28857;&#65292;&#24182;&#25552;&#20379;&#19968;&#31995;&#21015;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We establish the first mathematically rigorous link between Bayesian, variational Bayesian, and ensemble methods. A key step towards this it to reformulate the non-convex optimisation problem typically encountered in deep learning as a convex optimisation in the space of probability measures. On a technical level, our contribution amounts to studying generalised variational inference through the lense of Wasserstein gradient flows. The result is a unified theory of various seemingly disconnected approaches that are commonly used for uncertainty quantification in deep learning -- including deep ensembles and (variational) Bayesian methods. This offers a fresh perspective on the reasons behind the success of deep ensembles over procedures based on parameterised variational inference, and allows the derivation of new ensembling schemes with convergence guarantees. We showcase this by proposing a family of interacting deep ensembles with direct parallels to the interactions of particle sys
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;Calc-X&#21644;Calcformers&#65292;&#23427;&#20204;&#36890;&#36807;&#19982;&#31526;&#21495;&#31995;&#32479;&#30340;&#20132;&#20114;&#20351;&#35821;&#35328;&#27169;&#22411;&#22312;&#31639;&#26415;&#25512;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#26356;&#20934;&#30830;&#65292;&#25552;&#39640;&#20102;&#29983;&#25104;&#27491;&#30830;&#32467;&#26524;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.15017</link><description>&lt;p&gt;
Calc-X&#21644;Calcformers&#65306;&#36890;&#36807;&#19982;&#31526;&#21495;&#31995;&#32479;&#30340;&#20132;&#20114;&#22686;&#24378;&#31639;&#26415;&#25512;&#29702;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Calc-X and Calcformers: Empowering Arithmetical Chain-of-Thought through Interaction with Symbolic Systems. (arXiv:2305.15017v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15017
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;Calc-X&#21644;Calcformers&#65292;&#23427;&#20204;&#36890;&#36807;&#19982;&#31526;&#21495;&#31995;&#32479;&#30340;&#20132;&#20114;&#20351;&#35821;&#35328;&#27169;&#22411;&#22312;&#31639;&#26415;&#25512;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#26356;&#20934;&#30830;&#65292;&#25552;&#39640;&#20102;&#29983;&#25104;&#27491;&#30830;&#32467;&#26524;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#35768;&#22810;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#35821;&#35328;&#27169;&#22411;&#22312;&#38656;&#35201;&#36827;&#34892;&#31639;&#26415;&#35745;&#31639;&#30340;&#20219;&#21153;&#20013;&#24448;&#24448;&#20250;&#20135;&#29983;&#20107;&#23454;&#38169;&#35823;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;Calc-X&#65292;&#36825;&#26159;&#19968;&#20010;&#28436;&#31034;&#22914;&#20309;&#22312;&#25512;&#29702;&#38142;&#20013;&#27491;&#30830;&#20351;&#29992;&#35745;&#31639;&#22120;&#30340;&#25968;&#25454;&#38598;&#21512;&#12290;Calc-X&#36866;&#29992;&#20110;&#25945;&#23548;&#35821;&#35328;&#27169;&#22411;&#23558;&#35745;&#31639;&#20219;&#21153;&#36716;&#31227;&#21040;&#31526;&#21495;&#31995;&#32479;&#20013;&#12290;&#25105;&#20204;&#35843;&#26597;&#24182;&#32479;&#19968;&#20102;&#20960;&#20010;&#24050;&#26377;&#30340;&#25512;&#29702;&#38142;&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26631;&#20934;&#26684;&#24335;&#65292;&#32467;&#26524;&#26159;&#19968;&#20010;&#21253;&#21547;&#36229;&#36807;30&#19975;&#20010;&#38656;&#35201;&#36827;&#34892;&#31639;&#26415;&#25512;&#29702;&#30340;&#26679;&#26412;&#30340;&#26631;&#20934;&#25968;&#25454;&#38598;&#21512;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#26032;&#30340;Calc-X&#38598;&#21512;&#26469;&#35757;&#32451;&#25105;&#20204;&#31216;&#20043;&#20026;Calcformers&#30340;&#24320;&#28304;&#35745;&#31639;&#22120;&#27169;&#22411;&#65292;&#24182;&#23637;&#31034;&#36825;&#20123;&#27169;&#22411;&#30456;&#23545;&#20110;&#26222;&#36890;&#35821;&#35328;&#27169;&#22411;&#22522;&#32447;&#29983;&#25104;&#27491;&#30830;&#32467;&#26524;&#30340;&#20934;&#30830;&#24615;&#36817;&#20046;&#32763;&#20493;&#12290;&#25105;&#20204;&#20844;&#24320;&#25552;&#20379;&#25152;&#26377;&#30340;Calc-X&#25968;&#25454;&#38598;&#12289;&#28304;&#20195;&#30721;&#21644;Calcformers&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite outstanding performance in many tasks, language models are notoriously inclined to make factual errors in tasks requiring arithmetic computation. We address this deficiency by creating Calc-X, a collection of datasets that demonstrates the appropriate use of a calculator in reasoning chains. Calc-X is suitable for teaching language models to offload computations to a symbolic system. We survey and unify several existing chain-of-thought datasets into a proposed format, resulting in a standard collection of over 300,000 samples requiring arithmetic reasoning. Finally, we use the new Calc-X collection to train open-source calculator-using models we call Calcformers and show that these models approximately double the accuracy of generating correct results compared to vanilla language model baselines. We make all Calc-X datasets, source code and Calcformers models publicly available.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#26694;&#26550;RAP&#65292;&#36890;&#36807;&#26500;&#24314;&#20869;&#37096;&#30340;&#19990;&#30028;&#27169;&#22411;&#24182;&#27169;&#25311;&#38271;&#26399;&#34892;&#21160;&#32467;&#26524;&#65292;&#20174;&#32780;&#20351;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#36827;&#34892;&#20687;&#20154;&#31867;&#22823;&#33041;&#19968;&#26679;&#30340;&#26377;&#24847;&#35782;&#35268;&#21010;&#12290;</title><link>http://arxiv.org/abs/2305.14992</link><description>&lt;p&gt;
&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25512;&#29702;&#23601;&#26159;&#20351;&#29992;&#19990;&#30028;&#27169;&#22411;&#36827;&#34892;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Reasoning with Language Model is Planning with World Model. (arXiv:2305.14992v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14992
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#26694;&#26550;RAP&#65292;&#36890;&#36807;&#26500;&#24314;&#20869;&#37096;&#30340;&#19990;&#30028;&#27169;&#22411;&#24182;&#27169;&#25311;&#38271;&#26399;&#34892;&#21160;&#32467;&#26524;&#65292;&#20174;&#32780;&#20351;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#36827;&#34892;&#20687;&#20154;&#31867;&#22823;&#33041;&#19968;&#26679;&#30340;&#26377;&#24847;&#35782;&#35268;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#29305;&#21035;&#26159;&#22312;&#25552;&#31034;&#29983;&#25104;&#20013;&#38388;&#25512;&#29702;&#27493;&#39588;&#26102;&#65288;&#20363;&#22914;&#24605;&#32500;&#38142;&#65289;&#12290;&#28982;&#32780;&#65292;LLMs&#22312;&#19968;&#20123;&#23545;&#20154;&#31867;&#26469;&#35828;&#23481;&#26131;&#30340;&#38382;&#39064;&#19978;&#20173;&#28982;&#23384;&#22312;&#22256;&#38590;&#65292;&#20363;&#22914;&#22312;&#32473;&#23450;&#29615;&#22659;&#20013;&#29983;&#25104;&#25191;&#34892;&#20219;&#21153;&#30340;&#34892;&#21160;&#35745;&#21010;&#65292;&#25110;&#36827;&#34892;&#22797;&#26434;&#30340;&#25968;&#23398;&#12289;&#36923;&#36753;&#21644;&#24120;&#35782;&#25512;&#29702;&#12290;&#36825;&#31181;&#19981;&#36275;&#28304;&#20110;LLMs&#32570;&#20047;&#19968;&#20010;&#20869;&#37096;&#30340;&#8220;&#19990;&#30028;&#27169;&#22411;&#8221;&#65292;&#29992;&#20110;&#39044;&#27979;&#19990;&#30028;&#30340;&#29366;&#24577;&#65288;&#20363;&#22914;&#29615;&#22659;&#29366;&#20917;&#12289;&#20013;&#38388;&#21464;&#37327;&#20540;&#65289;&#24182;&#27169;&#25311;&#34892;&#21160;&#30340;&#38271;&#26399;&#32467;&#26524;&#12290;&#36825;&#20351;&#24471;LLMs&#26080;&#27861;&#20687;&#20154;&#31867;&#22823;&#33041;&#37027;&#26679;&#36827;&#34892;&#26377;&#24847;&#35782;&#30340;&#35268;&#21010;&#65292;&#20854;&#20013;&#21253;&#25324;&#25506;&#32034;&#26367;&#20195;&#30340;&#25512;&#29702;&#36335;&#24452;&#12289;&#39044;&#27979;&#26410;&#26469;&#30340;&#29366;&#24577;&#21644;&#22238;&#25253;&#65292;&#24182;&#23545;&#29616;&#26377;&#30340;&#25512;&#29702;&#27493;&#39588;&#36827;&#34892;&#36845;&#20195;&#20248;&#21270;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;LLM&#25512;&#29702;&#26694;&#26550;&#65292;&#21363;RAP&#65288;&#36890;&#36807;&#35268;&#21010;&#36827;&#34892;&#25512;&#29702;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have shown remarkable reasoning capabilities, especially when prompted to generate intermediate reasoning steps (e.g., Chain-of-Thought, CoT). However, LLMs can still struggle with problems that are easy for humans, such as generating action plans for executing tasks in a given environment, or performing complex math, logical, and commonsense reasoning. The deficiency stems from the key fact that LLMs lack an internal $\textit{world model}$ to predict the world $\textit{state}$ (e.g., environment status, intermediate variable values) and simulate long-term outcomes of actions. This prevents LLMs from performing deliberate planning akin to human brains, which involves exploring alternative reasoning paths, anticipating future states and rewards, and iteratively refining existing reasoning steps. To overcome the limitations, we propose a new LLM reasoning framework, $\underline{R}$easoning vi$\underline{a}$ $\underline{P}$lanning $\textbf{(RAP)}$. RAP repurpo
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20171;&#32461;&#19968;&#31181;&#19987;&#38376;&#20026;&#38646;&#26679;&#26412;&#23398;&#20064;&#32780;&#35774;&#35745;&#30340;&#33258;&#21160;&#25552;&#31034;&#35774;&#35745;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38646;&#26679;&#26412;&#24615;&#33021;&#36739;&#24369;&#30340;&#38382;&#39064;&#12290;&#36825;&#31181;&#26041;&#27861;&#21482;&#38656;&#35201;&#23569;&#37327;&#26080;&#26631;&#31614;&#25968;&#25454;&#21644;&#19968;&#20010;&#25512;&#29702;&#27169;&#22411;&#65292;&#20855;&#26377;&#39640;&#24230;&#28789;&#27963;&#24615;&#21644;&#36890;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.14926</link><description>&lt;p&gt;
&#36890;&#29992;&#30340;&#33258;&#36866;&#24212;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
Universal Self-Adaptive Prompting. (arXiv:2305.14926v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14926
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20171;&#32461;&#19968;&#31181;&#19987;&#38376;&#20026;&#38646;&#26679;&#26412;&#23398;&#20064;&#32780;&#35774;&#35745;&#30340;&#33258;&#21160;&#25552;&#31034;&#35774;&#35745;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38646;&#26679;&#26412;&#24615;&#33021;&#36739;&#24369;&#30340;&#38382;&#39064;&#12290;&#36825;&#31181;&#26041;&#27861;&#21482;&#38656;&#35201;&#23569;&#37327;&#26080;&#26631;&#31614;&#25968;&#25454;&#21644;&#19968;&#20010;&#25512;&#29702;&#27169;&#22411;&#65292;&#20855;&#26377;&#39640;&#24230;&#28789;&#27963;&#24615;&#21644;&#36890;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#26631;&#24535;&#26159;&#23427;&#20204;&#20986;&#33394;&#30340;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#33021;&#21147;&#65292;&#36890;&#24120;&#36890;&#36807;&#25552;&#31034;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;(ICL)&#26469;&#23454;&#29616;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#39640;&#24230;&#20196;&#20154;&#22402;&#28046;&#24182;&#19988;&#26368;&#20026;&#36890;&#29992;&#65292;LLMs&#30340;&#38646;&#26679;&#26412;&#24615;&#33021;&#36890;&#24120;&#36739;&#24369;&#65292;&#22240;&#20026;&#32570;&#20047;&#24341;&#23548;&#24182;&#19988;&#38590;&#20197;&#22312;&#22522;&#20110;&#26222;&#36890;&#20219;&#21153;&#30340;&#24773;&#20917;&#19979;&#24212;&#29992;&#29616;&#26377;&#30340;&#33258;&#21160;&#25552;&#31034;&#35774;&#35745;&#26041;&#27861;&#65292;&#24403;&#22320;&#38754;&#30495;&#23454;&#26631;&#31614;&#19981;&#21487;&#29992;&#26102;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#36890;&#29992;&#33258;&#36866;&#24212;&#25552;&#31034;(USP)&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#36825;&#26159;&#19968;&#31181;&#19987;&#38376;&#38024;&#23545;&#38646;&#26679;&#26412;&#23398;&#20064;&#30340;&#33258;&#21160;&#25552;&#31034;&#35774;&#35745;&#26041;&#27861;(&#21516;&#26102;&#20860;&#23481;&#23569;&#26679;&#26412;&#23398;&#20064;)&#12290;USP&#21482;&#38656;&#35201;&#23569;&#37327;&#26080;&#26631;&#31614;&#25968;&#25454;&#21644;&#19968;&#20010;&#20165;&#36827;&#34892;&#25512;&#29702;&#30340;LLM&#65292;&#38750;&#24120;&#28789;&#27963;&#65306;&#20026;&#20102;&#23454;&#29616;&#36890;&#29992;&#25552;&#31034;&#65292;USP&#23558;&#21487;&#33021;&#30340;NLP&#20219;&#21153;&#24402;&#31867;&#20026;&#19977;&#31181;&#21487;&#33021;&#30340;&#20219;&#21153;&#31867;&#22411;&#20043;&#19968;&#65292;&#28982;&#21518;&#20351;&#29992;&#30456;&#24212;&#30340;&#36873;&#25321;&#22120;&#26469;&#36873;&#25321;&#26368;&#21512;&#36866;&#30340;&#26597;&#35810;&#21644;&#38646;&#26679;&#26412;&#27169;&#22411;&#29983;&#25104;&#30340;&#21709;&#24212;&#20316;&#20026;&#20266;&#28436;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
A hallmark of modern large language models (LLMs) is their impressive general zero-shot and few-shot abilities, often elicited through in-context learning (ICL) via prompting. However, while highly coveted and being the most general, zero-shot performances in LLMs are still typically weaker due to the lack of guidance and the difficulty of applying existing automatic prompt design methods in general tasks when ground-truth labels are unavailable. In this study, we address this by presenting Universal Self-Adaptive Prompting (USP), an automatic prompt design approach specifically tailored for zero-shot learning (while compatible with few-shot). Requiring only a small amount of unlabeled data and an inference-only LLM, USP is highly versatile: to achieve universal prompting, USP categorizes a possible NLP task into one of the three possible task types and then uses a corresponding selector to select the most suitable queries and zero-shot model-generated responses as pseudo-demonstration
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#27979;&#37327;&#29702;&#35770;&#30340;&#26694;&#26550;MetricEval&#65292;&#29992;&#20110;&#35780;&#20272;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#65288;NLG&#65289;&#30340;&#35780;&#20272;&#25351;&#26631;&#12290;&#36890;&#36807;&#35813;&#26694;&#26550;&#65292;&#21487;&#20197;&#37327;&#21270;&#25351;&#26631;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#35299;&#20915;&#20154;&#24037;&#35780;&#20272;&#30340;&#25928;&#24230;&#32467;&#26500;&#28151;&#28102;&#21644;&#22522;&#20110;LLM&#30340;&#25351;&#26631;&#30340;&#21487;&#38752;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.14889</link><description>&lt;p&gt;
&#35780;&#20272;&#35780;&#20272;&#25351;&#26631;&#65306;&#20351;&#29992;&#27979;&#37327;&#29702;&#35770;&#20998;&#26512;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#35780;&#20272;&#25351;&#26631;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Evaluating Evaluation Metrics: A Framework for Analyzing NLG Evaluation Metrics using Measurement Theory. (arXiv:2305.14889v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14889
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#27979;&#37327;&#29702;&#35770;&#30340;&#26694;&#26550;MetricEval&#65292;&#29992;&#20110;&#35780;&#20272;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#65288;NLG&#65289;&#30340;&#35780;&#20272;&#25351;&#26631;&#12290;&#36890;&#36807;&#35813;&#26694;&#26550;&#65292;&#21487;&#20197;&#37327;&#21270;&#25351;&#26631;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#35299;&#20915;&#20154;&#24037;&#35780;&#20272;&#30340;&#25928;&#24230;&#32467;&#26500;&#28151;&#28102;&#21644;&#22522;&#20110;LLM&#30340;&#25351;&#26631;&#30340;&#21487;&#38752;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#65288;NLG&#65289;&#27169;&#22411;&#35780;&#20272;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#25361;&#25112;--&#35780;&#20272;&#25351;&#26631;&#30340;&#35774;&#35745;&#21644;&#35780;&#20272;&#12290;&#25105;&#20204;&#24847;&#35782;&#21040;&#29616;&#26377;&#33258;&#21160;&#25351;&#26631;&#30340;&#23616;&#38480;&#24615;&#20197;&#21450;&#24403;&#21069;&#20154;&#24037;&#35780;&#20272;&#23384;&#22312;&#30340;&#35823;&#24046;&#65292;&#25552;&#20986;&#20102;MetricEval&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#22522;&#20110;&#27979;&#37327;&#29702;&#35770;&#65292;&#25945;&#32946;&#27979;&#35797;&#35774;&#35745;&#30340;&#22522;&#30784;&#65292;&#29992;&#20110;&#27010;&#24565;&#21270;&#21644;&#35780;&#20272;NLG&#35780;&#20272;&#25351;&#26631;&#30340;&#21487;&#38752;&#24615;&#21644;&#25928;&#24230;&#12290;&#35813;&#26694;&#26550;&#35268;&#33539;&#20102;&#27979;&#37327;&#35823;&#24046;&#30340;&#26469;&#28304;&#65292;&#24182;&#25552;&#20379;&#20102;&#22522;&#20110;&#23454;&#35777;&#25968;&#25454;&#35780;&#20272;&#35780;&#20272;&#25351;&#26631;&#30340;&#32479;&#35745;&#24037;&#20855;&#12290;&#20511;&#21161;&#25105;&#20204;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#37327;&#21270;&#25351;&#26631;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#20197;&#26356;&#22909;&#22320;&#35299;&#37322;&#32467;&#26524;&#12290;&#20026;&#20102;&#31034;&#33539;&#25105;&#20204;&#30340;&#26694;&#26550;&#22312;&#23454;&#36341;&#20013;&#30340;&#24212;&#29992;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#19968;&#32452;&#29992;&#20110;&#25688;&#35201;&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;&#24182;&#30830;&#23450;&#20102;&#20154;&#24037;&#35780;&#20272;&#20013;&#30340;&#25928;&#24230;&#32467;&#26500;&#28151;&#28102;&#21644;&#22522;&#20110;LLM&#30340;&#25351;&#26631;&#20013;&#30340;&#21487;&#38752;&#24615;&#30456;&#20851;&#38382;&#39064;&#12290;&#36890;&#36807;MetricEval&#65292;&#25105;&#20204;&#26088;&#22312;&#25512;&#21160;&#35780;&#20272;&#25351;&#26631;&#30340;&#35774;&#35745;&#12289;&#35780;&#20272;&#21644;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
We address a fundamental challenge in Natural Language Generation (NLG) model evaluation -- the design and evaluation of evaluation metrics. Recognizing the limitations of existing automatic metrics and noises from how current human evaluation was conducted, we propose MetricEval, a framework informed by measurement theory, the foundation of educational test design, for conceptualizing and evaluating the reliability and validity of NLG evaluation metrics. The framework formalizes the source of measurement error and offers statistical tools for evaluating evaluation metrics based on empirical data. With our framework, one can quantify the uncertainty of the metrics to better interpret the result. To exemplify the use of our framework in practice, we analyzed a set of evaluation metrics for summarization and identified issues related to conflated validity structure in human-eval and reliability in LLM-based metrics. Through MetricEval, we aim to promote the design, evaluation, and interp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#22522;&#20110;&#31181;&#23376;&#21305;&#37197;&#30340;&#20266;&#26631;&#31614;&#29983;&#25104;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#31616;&#21333;&#30340;&#21333;&#35789;&#21024;&#38500;&#26469;&#32531;&#35299;&#22240;&#35268;&#21017;&#27880;&#20837;&#30340;&#26631;&#31614;&#20559;&#35265;&#32780;&#24102;&#26469;&#30340;&#24433;&#21709;&#65292;&#25552;&#39640;&#35813;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#20854;&#24615;&#33021;&#36798;&#21040;&#29978;&#33267;&#36229;&#36807;&#26368;&#20808;&#36827;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2305.14794</link><description>&lt;p&gt;
&#30465;&#24515;&#23398;&#20064;&#21464;&#24471;&#39046;&#20808;&#65306;&#37325;&#26032;&#23457;&#35270;&#22522;&#20110;&#31616;&#21333;&#31181;&#23376;&#24369;&#30417;&#30563;&#25991;&#26412;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Debiasing Made State-of-the-art: Revisiting the Simple Seed-based Weak Supervision for Text Classification. (arXiv:2305.14794v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14794
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#22522;&#20110;&#31181;&#23376;&#21305;&#37197;&#30340;&#20266;&#26631;&#31614;&#29983;&#25104;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#31616;&#21333;&#30340;&#21333;&#35789;&#21024;&#38500;&#26469;&#32531;&#35299;&#22240;&#35268;&#21017;&#27880;&#20837;&#30340;&#26631;&#31614;&#20559;&#35265;&#32780;&#24102;&#26469;&#30340;&#24433;&#21709;&#65292;&#25552;&#39640;&#35813;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#20854;&#24615;&#33021;&#36798;&#21040;&#29978;&#33267;&#36229;&#36807;&#26368;&#20808;&#36827;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26469;&#65292;&#24369;&#30417;&#30563;&#25991;&#26412;&#20998;&#31867;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#35774;&#35745;&#22797;&#26434;&#30340;&#26041;&#27861;&#65292;&#23558;&#39640;&#23618;&#27425;&#30340;&#20154;&#31867;&#21551;&#21457;&#24335;&#26041;&#27861;&#36716;&#21270;&#20026;&#39640;&#36136;&#37327;&#30340;&#20266;&#26631;&#31614;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;&#22522;&#20110;&#31181;&#23376;&#21305;&#37197;&#30340;&#26041;&#27861;&#65292;&#23427;&#26159;&#29983;&#25104;&#20266;&#26631;&#31614;&#30340;&#26368;&#31616;&#21333;&#26041;&#27861;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23427;&#30340;&#24378;&#22823;&#24615;&#33021;&#12290;&#25105;&#20204;&#34920;&#26126;&#31181;&#23376;&#21305;&#37197;&#30340;&#26377;&#38480;&#24615;&#33021;&#24456;&#22823;&#31243;&#24230;&#19978;&#24402;&#22240;&#20110;&#31181;&#23376;&#21305;&#37197;&#35268;&#21017;&#27880;&#20837;&#30340;&#26631;&#31614;&#20559;&#24046;&#65292;&#36825;&#20250;&#38459;&#27490;&#20998;&#31867;&#22120;&#23398;&#20064;&#21487;&#38752;&#30340;&#32622;&#20449;&#24230;&#26469;&#36873;&#25321;&#39640;&#36136;&#37327;&#20266;&#26631;&#31614;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#31616;&#21333;&#22320;&#21024;&#38500;&#21305;&#37197;&#36755;&#20837;&#25991;&#26412;&#20013;&#30340;&#31181;&#23376;&#35789;&#21487;&#20197;&#32531;&#35299;&#26631;&#31614;&#20559;&#24046;&#24182;&#24110;&#21161;&#23398;&#20064;&#26356;&#22909;&#30340;&#32622;&#20449;&#24230;&#12290;&#38543;&#21518;&#65292;&#31181;&#23376;&#21305;&#37197;&#30340;&#24615;&#33021;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#65292;&#20351;&#23427;&#36798;&#21040;&#25110;&#29978;&#33267;&#36229;&#36807;&#26368;&#20808;&#36827;&#25216;&#26415;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#22788;&#29702;&#31181;&#23376;&#35789;&#19981;&#20026;&#20154;&#30693;&#30340;&#24773;&#20917;&#65292;&#25105;&#20204;&#24314;&#35758;&#31616;&#21333;&#22320;&#21024;&#38500;&#36755;&#20837;&#25991;&#26412;&#20013;&#30340;&#21333;&#35789;&#26631;&#35760;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in weakly supervised text classification mostly focus on designing sophisticated methods to turn high-level human heuristics into quality pseudo-labels. In this paper, we revisit the seed matching-based method, which is arguably the simplest way to generate pseudo-labels, and show that its power was greatly underestimated. We show that the limited performance of seed matching is largely due to the label bias injected by the simple seed-match rule, which prevents the classifier from learning reliable confidence for selecting high-quality pseudo-labels. Interestingly, simply deleting the seed words present in the matched input texts can mitigate the label bias and help learn better confidence. Subsequently, the performance achieved by seed matching can be improved significantly, making it on par with or even better than the state-of-the-art. Furthermore, to handle the case when the seed words are not made known, we propose to simply delete the word tokens in the input tex
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#25552;&#20986;&#20102;&#22312;&#25991;&#20307;&#25913;&#20889;&#30340;&#37325;&#20889;&#21644;&#35780;&#20272;&#38454;&#27573;&#25972;&#21512;&#25991;&#26412;&#19978;&#19979;&#25991;&#30340;&#24517;&#35201;&#24615;&#65292;&#24182;&#36890;&#36807;few-shot prompting&#27604;&#36739;&#38750;&#19978;&#19979;&#25991;&#25913;&#20889;&#21644;&#19978;&#19979;&#25991;&#25913;&#20889;&#30340;&#25928;&#26524;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#33258;&#21160;&#24230;&#37327;&#25351;&#26631;&#19981;&#19968;&#23450;&#33021;&#21453;&#26144;&#20986;&#20154;&#31867;&#30340;&#20559;&#22909;&#12290;</title><link>http://arxiv.org/abs/2305.14755</link><description>&lt;p&gt;
&#19978;&#19979;&#25991;&#27169;&#22411;&#21450;&#35780;&#20272;&#22312;&#25991;&#20307;&#25913;&#20889;&#20013;&#30340;&#24517;&#35201;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Don't Take This Out of Context! On the Need for Contextual Models and Evaluations for Stylistic Rewriting. (arXiv:2305.14755v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14755
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#25552;&#20986;&#20102;&#22312;&#25991;&#20307;&#25913;&#20889;&#30340;&#37325;&#20889;&#21644;&#35780;&#20272;&#38454;&#27573;&#25972;&#21512;&#25991;&#26412;&#19978;&#19979;&#25991;&#30340;&#24517;&#35201;&#24615;&#65292;&#24182;&#36890;&#36807;few-shot prompting&#27604;&#36739;&#38750;&#19978;&#19979;&#25991;&#25913;&#20889;&#21644;&#19978;&#19979;&#25991;&#25913;&#20889;&#30340;&#25928;&#26524;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#33258;&#21160;&#24230;&#37327;&#25351;&#26631;&#19981;&#19968;&#23450;&#33021;&#21453;&#26144;&#20986;&#20154;&#31867;&#30340;&#20559;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#25991;&#20307;&#25913;&#20889;&#26041;&#27861;&#22312;&#21477;&#23376;&#32423;&#21035;&#25805;&#20316;&#65292;&#20294;&#26159;&#24573;&#35270;&#25991;&#26412;&#26356;&#24191;&#27867;&#30340;&#19978;&#19979;&#25991;&#21487;&#20197;&#23548;&#33268;&#25913;&#20889;&#32467;&#26524;&#26159;&#19968;&#33324;&#21270;&#12289;&#27495;&#20041;&#21644;&#19981;&#36830;&#36143;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#25972;&#21512;&#25991;&#26412;&#19978;&#19979;&#25991;&#21040;&#25991;&#20307;&#25913;&#20889;&#30340;&#37325;&#20889;&#21644;&#35780;&#20272;&#38454;&#27573;&#65292;&#37325;&#28857;&#20851;&#27880;&#24418;&#24335;&#12289;&#27602;&#24615;&#21644;&#24773;&#24863;&#36716;&#31227;&#20219;&#21153;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545; GPT-3.5 &#21644; GPT NeoX &#30340; few-shot &#25552;&#38382;&#27604;&#36739;&#37325;&#20889;&#30340;&#26041;&#27861;&#65292;&#24182;&#27604;&#36739;&#38750;&#19978;&#19979;&#25991;&#25913;&#20889;&#21644;&#19978;&#19979;&#25991;&#25913;&#20889;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#20154;&#20204;&#36890;&#24120;&#26356;&#21916;&#27426;&#19978;&#19979;&#25991;&#25913;&#20889;&#65292;&#20294;&#33258;&#21160;&#24230;&#37327;&#25351;&#26631;&#65288;&#22914; BLEU&#65292;sBERT&#65289;&#19981;&#26159;&#36825;&#26679;&#30340;&#12290;&#20026;&#24357;&#21512;&#36825;&#31181;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#36890;&#29992;&#33258;&#21160;&#24230;&#37327;&#25351;&#26631;&#30340;&#19978;&#19979;&#25991;&#34701;&#21512;&#29256;&#26412;&#65292;&#24182;&#35777;&#26126;&#36825;&#20123;&#26356;&#33021;&#21453;&#26144;&#20154;&#31867;&#20559;&#22909;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#26412;&#25991;&#24378;&#35843;&#22312;&#25991;&#20307;&#25913;&#20889;&#30340;&#37325;&#20889;&#21644;&#35780;&#20272;&#38454;&#27573;&#25972;&#21512;&#25991;&#26412;&#19978;&#19979;&#25991;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most existing stylistic text rewriting methods operate on a sentence level, but ignoring the broader context of the text can lead to generic, ambiguous, and incoherent rewrites. In this paper, we propose the integration of preceding textual context into both the rewriting and evaluation stages of stylistic text rewriting, focusing on formality, toxicity, and sentiment transfer tasks. We conduct a comparative evaluation of rewriting through few-shot prompting of GPT-3.5 and GPT NeoX, comparing non-contextual rewrites to contextual rewrites. Our experiments show that humans often prefer contextual rewrites over non-contextual ones, but automatic metrics (e.g., BLEU, sBERT) do not. To bridge this gap, we propose context-infused versions of common automatic metrics, and show that these better reflect human preferences. Overall, our paper highlights the importance of integrating preceding textual context into both the rewriting and evaluation stages of stylistic text rewriting.
&lt;/p&gt;</description></item><item><title>ECHo&#26159;&#19968;&#20010;&#22522;&#20110;&#20154;&#31867;&#20013;&#24515;&#25512;&#29702;&#30340;&#20107;&#20214;&#22240;&#26524;&#25512;&#26029;&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#19982;CoT&#33539;&#24335;&#23545;&#40784;&#30340;&#32479;&#19968;&#26694;&#26550;&#26469;&#35780;&#20272;&#24403;&#21069;AI&#31995;&#32479;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.14740</link><description>&lt;p&gt;
ECHo: &#22522;&#20110;&#20154;&#31867;&#20013;&#24515;&#25512;&#29702;&#30340;&#20107;&#20214;&#22240;&#26524;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
ECHo: Event Causality Inference via Human-centric Reasoning. (arXiv:2305.14740v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14740
&lt;/p&gt;
&lt;p&gt;
ECHo&#26159;&#19968;&#20010;&#22522;&#20110;&#20154;&#31867;&#20013;&#24515;&#25512;&#29702;&#30340;&#20107;&#20214;&#22240;&#26524;&#25512;&#26029;&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#19982;CoT&#33539;&#24335;&#23545;&#40784;&#30340;&#32479;&#19968;&#26694;&#26550;&#26469;&#35780;&#20272;&#24403;&#21069;AI&#31995;&#32479;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102; ECHo&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#35270;&#35273;&#21644;&#35821;&#35328;&#31038;&#20132;&#24773;&#22659;&#30340;&#20107;&#20214;&#22240;&#26524;&#25512;&#26029;&#35786;&#26029;&#25968;&#25454;&#38598;&#12290; ECHo&#21033;&#29992;&#20174;&#29359;&#32618;&#21095;&#20013;&#25910;&#38598;&#30340;&#30495;&#23454;&#20154;&#31867;&#20013;&#24515;&#28436;&#32462;&#20449;&#24687;&#65292;&#36890;&#36807;&#28608;&#21457;&#20013;&#38388;&#24515;&#28789;&#29702;&#35770;&#65288;ToM&#65289;&#26469;&#24357;&#21512;&#22810;&#27169;&#24577;&#25512;&#29702;&#30340;&#40511;&#27807;&#65292;&#20174;&#32780;&#25552;&#39640;&#31038;&#20132;&#26234;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#19982;Chain-of-Thought&#65288;CoT&#65289;&#33539;&#24335;&#23545;&#40784;&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#20197;&#35780;&#20272;&#24403;&#21069;AI&#31995;&#32479;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#36825;&#20010;ToM&#22686;&#24378;&#30340;CoT&#31649;&#36947;&#21487;&#20197;&#22312; &#38646;-shot&#35270;&#35273;&#21644;&#35821;&#35328;&#29702;&#35299;&#20013;&#21253;&#23481;&#21644;&#25972;&#21512;&#21508;&#31181;&#22823;&#22411;&#22522;&#30784;&#27169;&#22411;&#12290;&#21033;&#29992;&#36825;&#20010;&#26694;&#26550;&#65292;&#25105;&#20204;&#36890;&#36807;&#19977;&#20010;&#20114;&#34917;&#30340;&#22522;&#20110;&#20154;&#31867;&#20013;&#24515;&#30340;ECHo&#20219;&#21153;&#26469;&#23457;&#26597;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#21644;&#22810;&#27169;&#24577;&#27169;&#22411;&#12290;&#36827;&#19968;&#27493;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;ECHo&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25968;&#25454;&#38598;&#65292;&#21487;&#20197;&#26292;&#38706;&#25512;&#29702;&#20013;&#30340;&#19981;&#23436;&#21892;&#21644;&#19981;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce ECHo, a diagnostic dataset of event causality inference grounded in visual-and-linguistic social scenarios. ECHo employs real-world human-centric deductive information collected from crime drama, bridging the gap in multimodal reasoning towards higher social intelligence through the elicitation of intermediate Theory-of-Mind (ToM). We propose a unified framework aligned with the Chain-of-Thought (CoT) paradigm to assess the reasoning capability of current AI systems. This ToM-enhanced CoT pipeline can accommodate and integrate various large foundation models in zero-shot visual-and-linguistic understanding. With this framework, we scrutinize the advanced large language and multimodal models via three complementary human-centric ECHo tasks. Further analysis demonstrates ECHo as a challenging dataset to expose imperfections and inconsistencies in reasoning.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24322;&#24120;&#20540;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35782;&#21035;&#22312;&#27602;&#24615;&#26816;&#27979;&#20013;&#21463;&#21040;&#20260;&#23475;&#30340;&#20154;&#32676;&#65292;&#21457;&#29616;&#23545;&#20110;&#36825;&#20123;&#24322;&#24120;&#20540;&#65292;&#27169;&#22411;&#24615;&#33021;&#36739;&#24046;&#65292;&#20182;&#20204;&#38754;&#20020;&#30340;&#27602;&#24615;&#26356;&#39640;&#12290;</title><link>http://arxiv.org/abs/2305.14735</link><description>&lt;p&gt;
&#36793;&#32536;&#32858;&#28966;&#65306;&#22522;&#20110;&#24322;&#24120;&#20540;&#30340;&#27602;&#24615;&#26816;&#27979;&#20013;&#21463;&#25439;&#20154;&#32676;&#30340;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Centering the Margins: Outlier-Based Identification of Harmed Populations in Toxicity Detection. (arXiv:2305.14735v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14735
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24322;&#24120;&#20540;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35782;&#21035;&#22312;&#27602;&#24615;&#26816;&#27979;&#20013;&#21463;&#21040;&#20260;&#23475;&#30340;&#20154;&#32676;&#65292;&#21457;&#29616;&#23545;&#20110;&#36825;&#20123;&#24322;&#24120;&#20540;&#65292;&#27169;&#22411;&#24615;&#33021;&#36739;&#24046;&#65292;&#20182;&#20204;&#38754;&#20020;&#30340;&#27602;&#24615;&#26356;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34913;&#37327;&#20154;&#24037;&#26234;&#33021;&#23545;&#36793;&#32536;&#31038;&#21306;&#24433;&#21709;&#30340;&#26631;&#20934;&#26041;&#27861;&#26159;&#30830;&#23450;&#29305;&#23450;&#20154;&#21475;&#32676;&#20307;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#24322;&#12290;&#36825;&#20123;&#26041;&#27861;&#26088;&#22312;&#35299;&#20915;&#38024;&#23545;&#24369;&#21183;&#32676;&#20307;&#30340;&#20260;&#23475;&#38382;&#39064;&#65292;&#20294;&#23427;&#20204;&#20250;&#25513;&#30422;&#30001;&#20132;&#21449;&#23376;&#32676;&#25110;&#36328;&#20154;&#21475;&#32676;&#20307;&#20849;&#20139;&#30340;&#20260;&#23475;&#27169;&#24335;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#23558;&#8220;&#36793;&#32536;&#8221;&#23450;&#20041;&#20026;&#20855;&#26377;&#36828;&#31163;&#8220;&#24120;&#24577;&#8221; &#30340;&#20154;&#21475;&#23646;&#24615;&#30340;&#25968;&#25454;&#28857;&#65292;&#24182;&#24230;&#37327;&#38024;&#23545;&#36825;&#20123;&#24322;&#24120;&#20540;&#30340;&#20260;&#23475;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32676;&#20307;&#30340;&#24615;&#33021;&#24046;&#24322;&#25351;&#25968;&#65288;GPDI&#65289;&#65292;&#20197;&#34913;&#37327;&#25968;&#25454;&#38598;&#32454;&#20998;&#20026;&#23376;&#32452;&#23545;&#38754;&#20020;&#22686;&#21152;&#30340;&#20260;&#23475;&#30340;&#35782;&#21035;&#31243;&#24230;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;&#26816;&#27979;&#27602;&#24615;&#26816;&#27979;&#20013;&#30340;&#24046;&#24322;&#65292;&#24182;&#21457;&#29616;&#38024;&#23545;&#24322;&#24120;&#20540;&#30340;&#25991;&#26412;&#22312;&#25152;&#26377;&#31867;&#22411;&#30340;&#27602;&#24615;&#26816;&#39564;&#20013;&#27602;&#24615;&#26356;&#39640;&#65292;&#39640;&#36798;28&#65285;&#33267;86&#65285;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;&#23545;&#20110;&#20154;&#21475;&#23398;&#24322;&#24120;&#20540;&#65292;&#27169;&#22411;&#24615;&#33021;&#22987;&#32456;&#36739;&#24046;&#65292;&#24322;&#24120;&#20540;&#21644;&#38750;&#24322;&#24120;&#20540;&#20043;&#38388;&#30340;&#38169;&#35823;&#24046;&#36317;&#39640;&#36798;10&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
A standard method for measuring the impacts of AI on marginalized communities is to determine performance discrepancies between specified demographic groups. These approaches aim to address harms toward vulnerable groups, but they obscure harm patterns faced by intersectional subgroups or shared across demographic groups. We instead operationalize "the margins" as data points that are statistical outliers due to having demographic attributes distant from the "norm" and measure harms toward these outliers. We propose a Group-Based Performance Disparity Index (GPDI) that measures the extent to which a subdivision of a dataset into subgroups identifies those facing increased harms. We apply our approach to detecting disparities in toxicity detection and find that text targeting outliers is 28% to 86% more toxic for all types of toxicity examined. We also discover that model performance is consistently worse for demographic outliers, with disparities in error between outliers and non-outli
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;COMET-M&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#25512;&#29702;&#22797;&#26434;&#21477;&#23376;&#20013;&#22810;&#20010;&#20107;&#20214;&#20043;&#38388;&#30340;&#20851;&#31995;&#20197;&#21450;&#29983;&#25104;&#24120;&#35782;&#25512;&#26029;&#65292;&#24182;&#22312;35K&#20010;&#20154;&#31867;&#32534;&#20889;&#30340;&#25512;&#26029;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#30456;&#23545;&#20110;&#20043;&#21069;&#30340;COMET&#27169;&#22411;&#22312;&#29983;&#25104;&#22810;&#20107;&#20214;&#25512;&#26029;&#26041;&#38754;&#26377;&#26174;&#30528;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2305.14617</link><description>&lt;p&gt;
COMET-M: &#22312;&#22797;&#26434;&#21477;&#23376;&#20013;&#25512;&#29702;&#22810;&#20010;&#20107;&#20214;
&lt;/p&gt;
&lt;p&gt;
COMET-M: Reasoning about Multiple Events in Complex Sentences. (arXiv:2305.14617v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14617
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;COMET-M&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#25512;&#29702;&#22797;&#26434;&#21477;&#23376;&#20013;&#22810;&#20010;&#20107;&#20214;&#20043;&#38388;&#30340;&#20851;&#31995;&#20197;&#21450;&#29983;&#25104;&#24120;&#35782;&#25512;&#26029;&#65292;&#24182;&#22312;35K&#20010;&#20154;&#31867;&#32534;&#20889;&#30340;&#25512;&#26029;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#30456;&#23545;&#20110;&#20043;&#21069;&#30340;COMET&#27169;&#22411;&#22312;&#29983;&#25104;&#22810;&#20107;&#20214;&#25512;&#26029;&#26041;&#38754;&#26377;&#26174;&#30528;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#35828;&#35805;&#32773;&#30340;&#24847;&#22270;&#36890;&#24120;&#28041;&#21450;&#32472;&#21046;&#24120;&#35782;&#25512;&#26029;&#65292;&#20197;&#25512;&#29702;&#26410;&#26126;&#30830;&#38472;&#36848;&#30340;&#20869;&#23481;&#12290;&#22312;&#22810;&#20107;&#20214;&#21477;&#23376;&#20013;&#65292;&#38656;&#35201;&#22522;&#20110;&#19978;&#19979;&#25991;&#30693;&#35782;&#29702;&#35299;&#20107;&#20214;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;COMET-M&#65288;Multi-Event&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#20197;&#20107;&#20214;&#20026;&#20013;&#24515;&#30340;&#24120;&#35782;&#27169;&#22411;&#65292;&#33021;&#22815;&#38024;&#23545;&#22797;&#26434;&#21477;&#23376;&#20869;&#30340;&#30446;&#26631;&#20107;&#20214;&#29983;&#25104;&#24120;&#35782;&#25512;&#26029;&#12290;COMET-M&#26159;&#22522;&#20110;COMET&#65288;Bosselut et al.&#65292;2019&#65289;&#21457;&#23637;&#32780;&#26469;&#30340;&#65292;&#21518;&#32773;&#25797;&#38271;&#20026;&#31616;&#21333;&#21477;&#23376;&#29983;&#25104;&#20197;&#20107;&#20214;&#20026;&#20013;&#24515;&#30340;&#25512;&#26029;&#65292;&#20294;&#22312;&#33258;&#28982;&#25991;&#26412;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#22810;&#20107;&#20214;&#21477;&#23376;&#30340;&#22797;&#26434;&#24615;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25972;&#29702;&#20102;&#19968;&#20010;&#21253;&#21547;35K&#20010;&#20154;&#31867;&#32534;&#20889;&#25512;&#26029;&#30340;&#22810;&#20107;&#20214;&#25512;&#26029;&#25968;&#25454;&#38598;&#12290; &#25105;&#20204;&#22312;&#20154;&#31867;&#32534;&#20889;&#30340;&#25512;&#26029;&#19978;&#35757;&#32451;&#20102;COMET-M&#65292;&#24182;&#21019;&#24314;&#20102;&#20351;&#29992;&#33258;&#21160;&#26631;&#35760;&#31034;&#20363;&#30340;&#22522;&#32447;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;COMET-M&#22312;&#29983;&#25104;&#22810;&#20107;&#20214;&#25512;&#26029;&#26041;&#38754;&#30456;&#23545;&#20110;COMET&#20855;&#26377;&#26174;&#30528;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;&#27492;&#22806;&#65292;COMET-M&#25104;&#21151;&#39044;&#27979;&#20102;&#27979;&#35797;&#38598;&#20013;60&#65285;&#30340;&#22797;&#26434;&#21477;&#23376;&#30446;&#26631;&#20107;&#20214;&#30340;&#24120;&#35782;&#25512;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding the speaker's intended meaning often involves drawing commonsense inferences to reason about what is not stated explicitly. In multi-event sentences, it requires understanding the relationships between events based on contextual knowledge. We propose COMET-M (Multi-Event), an event-centric commonsense model capable of generating commonsense inferences for a target event within a complex sentence. COMET-M builds upon COMET (Bosselut et al., 2019), which excels at generating event-centric inferences for simple sentences, but struggles with the complexity of multi-event sentences prevalent in natural text. To overcome this limitation, we curate a multi-event inference dataset of 35K human-written inferences. We trained COMET-M on the human-written inferences and also created baselines using automatically labeled examples. Experimental results demonstrate the significant performance improvement of COMET-M over COMET in generating multi-event inferences. Moreover, COMET-M succ
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#26469;&#33258;&#21160;&#36873;&#25321;&#24102;&#26377;&#25512;&#29702;&#30340;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#33258;&#28982;&#35821;&#35328;&#21644;&#32534;&#31243;&#35821;&#35328;&#30340;&#20248;&#21183;&#65292;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#36827;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.14333</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#36873;&#25321;&#24102;&#26377;&#25512;&#29702;&#30340;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Automatic Model Selection with Large Language Models for Reasoning. (arXiv:2305.14333v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14333
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#26469;&#33258;&#21160;&#36873;&#25321;&#24102;&#26377;&#25512;&#29702;&#30340;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#33258;&#28982;&#35821;&#35328;&#21644;&#32534;&#31243;&#35821;&#35328;&#30340;&#20248;&#21183;&#65292;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#36827;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Chain-of-Thought&#65288;CoT&#65289;&#21644;Program-Aided Language Models&#65288;PAL&#65289;&#20195;&#34920;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#25512;&#29702;&#26041;&#27861;&#65292;&#21508;&#33258;&#20855;&#26377;&#33258;&#24049;&#30340;&#20248;&#21183;&#12290;CoT&#37319;&#29992;&#33258;&#28982;&#35821;&#35328;&#65292;&#20855;&#26377;&#28789;&#27963;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#32780;PAL&#21033;&#29992;&#32534;&#31243;&#35821;&#35328;&#65292;&#20135;&#29983;&#26356;&#32467;&#26500;&#21270;&#21644;&#20005;&#23494;&#30340;&#36923;&#36753;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#27169;&#22411;&#36873;&#25321;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21160;&#24577;&#36873;&#25321;&#23427;&#20204;&#20043;&#38388;&#30340;&#26368;&#20339;&#26041;&#27861;&#26469;&#32467;&#21512;&#20004;&#32773;&#30340;&#20248;&#21183;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#24378;&#35843;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#21487;&#34892;&#24615;&#65292;&#32463;&#39564;&#32467;&#26524;&#36827;&#19968;&#27493;&#35777;&#23454;&#20102;&#36825;&#19968;&#28857;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#20843;&#20010;&#25512;&#29702;&#25968;&#25454;&#38598;&#19978;&#19982;Codex&#12289;ChatGPT&#21644;GPT-4&#23637;&#31034;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#33258;&#19968;&#33268;&#24615;&#30456;&#36741;&#30456;&#25104;&#65307;&#24403;&#25972;&#21512;&#22312;&#19968;&#36215;&#26102;&#65292;&#23427;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#24615;&#33021;&#65292;&#21516;&#26102;&#26174;&#33879;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;GSM8K&#21644;SVAMP&#19978;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#65292;&#20998;&#21035;&#36798;&#21040;96.8%&#21644;93.7%&#30340;&#20934;&#30830;&#29575;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#12289;&#25968;&#25454;&#21644;...
&lt;/p&gt;
&lt;p&gt;
Chain-of-Thought (CoT) and Program-Aided Language Models (PAL) represent two distinct reasoning methods, each with its own strengths. CoT employs natural language, offering flexibility and interpretability, while PAL utilizes programming language, yielding more structured and rigorous logic. We introduce a model selection method to combine the best of both worlds by employing a large language model (LLM) to dynamically select between them. Our theoretical analysis underscores the feasibility of this method, which is further corroborated by empirical results. Our proposed method demonstrates significant performance improvements across eight reasoning datasets with Codex, ChatGPT, and GPT-4. Additionally, our method is complementary to self-consistency; when integrated, it can further enhance performance while significantly reducing computation costs. Moreover, we achieve new state-of-the-art results on GSM8K and SVAMP, with respective accuracies of 96.8% and 93.7%. Our code, data and pr
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#25552;&#31034;&#26041;&#27861;&#26469;&#35299;&#20915;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#22797;&#26434;&#35266;&#23519;&#30340;&#20132;&#20114;&#20915;&#31574;&#20219;&#21153;&#20013;&#30340;&#22256;&#38590;&#12290;&#30740;&#31350;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#32593;&#32476;&#23548;&#33322;&#20013;&#30340;&#25928;&#26524;&#20248;&#20110;&#20808;&#21069;&#26368;&#20808;&#36827;&#30340;&#25552;&#31034;&#26426;&#21046;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.14257</link><description>&lt;p&gt;
&#20998;&#23618;&#25552;&#31034;&#25552;&#21319;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#32593;&#32476;&#23548;&#33322;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Prompting Assists Large Language Model on Web Navigation. (arXiv:2305.14257v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14257
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#25552;&#31034;&#26041;&#27861;&#26469;&#35299;&#20915;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#22797;&#26434;&#35266;&#23519;&#30340;&#20132;&#20114;&#20915;&#31574;&#20219;&#21153;&#20013;&#30340;&#22256;&#38590;&#12290;&#30740;&#31350;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#32593;&#32476;&#23548;&#33322;&#20013;&#30340;&#25928;&#26524;&#20248;&#20110;&#20808;&#21069;&#26368;&#20808;&#36827;&#30340;&#25552;&#31034;&#26426;&#21046;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22788;&#29702;&#20132;&#20114;&#20915;&#31574;&#20219;&#21153;&#20013;&#30340;&#22797;&#26434;&#35266;&#23519;&#26102;&#36935;&#21040;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#20998;&#23618;&#25552;&#31034;&#26041;&#27861;&#12290;&#19981;&#21516;&#20110;&#20197;&#24448;&#24635;&#26159;&#25226;\emph{&#23436;&#25972;}&#35266;&#23519;&#65288;&#20363;&#22914;&#32593;&#39029;&#65289;&#25918;&#21040;&#25552;&#31034;&#20013;&#30340;&#25552;&#31034;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#39318;&#20808;&#26500;&#24314;&#19968;&#20010;&#19982;&#21160;&#20316;&#30456;&#20851;&#30340;\emph{&#21387;&#32553;}&#21644;\emph{&#30456;&#20851;}&#30340;&#35266;&#23519;&#65292;&#24182;&#20351;&#29992;&#19987;&#38376;&#30340;\summ&#25552;&#31034;&#12290;&#28982;&#21518;&#65292;\actor&#25552;&#31034;&#26681;&#25454;&#24635;&#32467;&#30340;&#35266;&#23519;&#39044;&#27979;&#19979;&#19968;&#20010;&#21160;&#20316;&#12290;&#23613;&#31649;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#65292;&#20294;&#25105;&#20204;&#23588;&#20854;&#23637;&#31034;&#20102;&#23427;&#22312;&#22797;&#26434;&#30340;&#32593;&#32476;&#23548;&#33322;&#39046;&#22495;&#30340;&#26377;&#25928;&#24615;&#65292;&#20854;&#20013;&#23436;&#25972;&#30340;&#35266;&#23519;&#36890;&#24120;&#21253;&#21547;&#20887;&#20313;&#21644;&#26080;&#20851;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20219;&#21153;&#25104;&#21151;&#29575;&#19978;&#20248;&#20110;&#20808;&#21069;&#26368;&#20808;&#36827;&#30340;&#25552;&#31034;&#26426;&#21046;6.2\%&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#20855;&#26377;&#38271;&#26102;&#38388;&#35266;&#23519;&#36712;&#36857;&#30340;&#20132;&#20114;&#20915;&#31574;&#20219;&#21153;&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) struggle on processing complicated observations in interactive decision making tasks. To alleviate this issue, we propose a simple hierarchical prompting approach. Diverging from previous prompting approaches that always put the \emph{full} observation~(\eg a web page) to the prompt, we propose to first construct an action-aware observation which is more \emph{condensed} and \emph{relevant} with a dedicated \summ prompt. The \actor prompt then predicts the next action based on the summarized observation. While our method has broad applicability, we particularly demonstrate its efficacy in the complex domain of web navigation where a full observation often contains redundant and irrelevant information. Our approach outperforms the previous state-of-the-art prompting mechanis by 6.2\% on task success rate, demonstrating its potential on interactive decision making tasks with long observation traces.
&lt;/p&gt;</description></item><item><title>&#22810;&#35821;&#35328;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#34920;&#29616;&#20986;&#20102;&#24378;&#22823;&#30340;&#24615;&#33021;&#65292;&#20294;&#26159;&#22312;&#20195;&#30721;&#20999;&#25442;&#30340;&#35821;&#22659;&#20013;&#65292;&#23427;&#20204;&#20173;&#28982;&#34920;&#29616;&#19981;&#20339;&#12290;</title><link>http://arxiv.org/abs/2305.14235</link><description>&lt;p&gt;
&#22810;&#35821;&#35328;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23578;&#26080;&#27861;&#36827;&#34892;&#20195;&#30721;&#20999;&#25442;
&lt;/p&gt;
&lt;p&gt;
Multilingual Large Language Models Are Not (Yet) Code-Switchers. (arXiv:2305.14235v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14235
&lt;/p&gt;
&lt;p&gt;
&#22810;&#35821;&#35328;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#34920;&#29616;&#20986;&#20102;&#24378;&#22823;&#30340;&#24615;&#33021;&#65292;&#20294;&#26159;&#22312;&#20195;&#30721;&#20999;&#25442;&#30340;&#35821;&#22659;&#20013;&#65292;&#23427;&#20204;&#20173;&#28982;&#34920;&#29616;&#19981;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#35821;&#35328;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLMs) &#26368;&#36817;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23637;&#31034;&#20986;&#20102;&#24378;&#22823;&#30340;&#33021;&#21147;&#65292;&#36890;&#36807;&#38646;-shot&#25110;&#23569;&#37327;-shot&#30340;&#25552;&#31034;&#26041;&#27861;&#23637;&#29616;&#20986;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#34429;&#28982;&#24050;&#32463;&#26377;&#22823;&#37327;&#30740;&#31350;&#20851;&#20110;&#23427;&#20204;&#22312;&#21333;&#35821;&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;&#65292;&#20294;&#26159;&#22312;&#20195;&#30721;&#20999;&#25442; (CSW) &#30340;&#35821;&#22659;&#20013;&#65292;&#21363;&#22312;&#19968;&#20010;&#35805;&#35821;&#20013;&#20132;&#26367;&#20351;&#29992;&#22810;&#31181;&#35821;&#35328;&#65292;&#23427;&#20204;&#30340;&#28508;&#21147;&#30340;&#30740;&#31350;&#36824;&#30456;&#23545;&#36739;&#23569;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;&#22810;&#20010;&#22810;&#35821;&#35328;LLM&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#32463;&#39564;&#35777;&#23454;&#20998;&#26512;&#65292;&#23558;&#23427;&#20204;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#65292;&#21253;&#25324;&#24773;&#24863;&#20998;&#26512;&#12289;&#26426;&#22120;&#32763;&#35793;&#12289;&#25688;&#35201;&#21644;&#21333;&#35789;&#32423;&#35821;&#35328;&#35782;&#21035;&#31561;&#22235;&#20010;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#23613;&#31649;&#22810;&#35821;&#35328;LLMs&#22312;&#26576;&#20123;&#20219;&#21153;&#20013;&#36890;&#36807;&#38646;-shot&#25110;&#23569;&#37327;-shot&#30340;&#25552;&#31034;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#25928;&#26524;&#65292;&#20294;&#19982;&#35268;&#27169;&#23567;&#24471;&#22810;&#30340;&#31934;&#35843;&#27169;&#22411;&#30456;&#27604;&#65292;&#23427;&#20204;&#20173;&#28982;&#34920;&#29616;&#19981;&#20339;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#30446;&#21069;LLMs&#20013;&#30340;"&#22810;&#35821;&#35328;&#33021;&#21147;"&#24182;&#19981;&#24847;&#21619;&#30528;&#20855;&#22791;&#20195;&#30721;&#20999;&#25442;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multilingual Large Language Models (LLMs) have recently shown great capabilities in a wide range of tasks, exhibiting state-of-the-art performance through zero-shot or few-shot prompting methods. While there have been extensive studies on their abilities in monolingual tasks, the investigation of their potential in the context of code-switching (CSW), the practice of alternating languages within an utterance, remains relatively uncharted. In this paper, we provide a comprehensive empirical analysis of various multilingual LLMs, benchmarking their performance across four tasks: sentiment analysis, machine translation, summarization and word-level language identification. Our results indicate that despite multilingual LLMs exhibiting promising outcomes in certain tasks using zero or few-shot prompting, they still underperform in comparison to fine-tuned models of much smaller scales. We argue that current "multilingualism" in LLMs does not inherently imply proficiency with code-switching
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32467;&#21512;&#22810;&#31181;&#29305;&#24449;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#29992;&#20110;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#19978;&#19979;&#25991;&#31034;&#20363;&#36873;&#25321;&#12290;&#36890;&#36807;&#23398;&#20064;&#19968;&#20010;&#22238;&#24402;&#27169;&#22411;&#65292;CTQ Scorer&#33021;&#22815;&#36873;&#25321;&#26368;&#20248;&#31034;&#20363;&#20197;&#25552;&#39640;&#32763;&#35793;&#36136;&#37327;&#65292;&#24182;&#22312;&#22810;&#20010;&#35821;&#35328;&#23545;&#21644;&#35821;&#35328;&#27169;&#22411;&#19978;&#26174;&#33879;&#36229;&#36807;&#38543;&#26426;&#36873;&#25321;&#21644;&#21333;&#22240;&#32032;&#22522;&#32447;&#12290;</title><link>http://arxiv.org/abs/2305.14105</link><description>&lt;p&gt;
CTQScorer: &#32467;&#21512;&#22810;&#31181;&#29305;&#24449;&#36827;&#34892;&#19978;&#19979;&#25991;&#31034;&#20363;&#36873;&#25321;&#20197;&#29992;&#20110;&#26426;&#22120;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
CTQScorer: Combining Multiple Features for In-context Example Selection for Machine Translation. (arXiv:2305.14105v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14105
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32467;&#21512;&#22810;&#31181;&#29305;&#24449;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#29992;&#20110;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#19978;&#19979;&#25991;&#31034;&#20363;&#36873;&#25321;&#12290;&#36890;&#36807;&#23398;&#20064;&#19968;&#20010;&#22238;&#24402;&#27169;&#22411;&#65292;CTQ Scorer&#33021;&#22815;&#36873;&#25321;&#26368;&#20248;&#31034;&#20363;&#20197;&#25552;&#39640;&#32763;&#35793;&#36136;&#37327;&#65292;&#24182;&#22312;&#22810;&#20010;&#35821;&#35328;&#23545;&#21644;&#35821;&#35328;&#27169;&#22411;&#19978;&#26174;&#33879;&#36229;&#36807;&#38543;&#26426;&#36873;&#25321;&#21644;&#21333;&#22240;&#32032;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#36755;&#20837;&#25552;&#31034;&#20960;&#20010;&#31034;&#20363;&#26102;&#65288;&#19978;&#19979;&#25991;&#23398;&#20064;&#65289;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#23637;&#31034;&#20102;&#22312;&#26426;&#22120;&#32763;&#35793;&#19978;&#30340;&#33021;&#21147;&#12290;&#32763;&#35793;&#36136;&#37327;&#21462;&#20915;&#20110;&#25152;&#36873;&#31034;&#20363;&#30340;&#21508;&#31181;&#29305;&#24449;&#65292;&#22914;&#20854;&#36136;&#37327;&#21644;&#30456;&#20851;&#24615;&#65292;&#20294;&#20197;&#24448;&#30340;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#21333;&#20010;&#29420;&#31435;&#30340;&#29305;&#24449;&#19978;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#29992;&#20110;&#32467;&#21512;&#24433;&#21709;&#31034;&#20363;&#36873;&#25321;&#30340;&#19981;&#21516;&#29305;&#24449;&#12290;&#25105;&#20204;&#23398;&#20064;&#20102;&#19968;&#20010;&#22238;&#24402;&#27169;&#22411;CTQ Scorer&#65288;&#19978;&#19979;&#25991;&#32763;&#35793;&#36136;&#37327;&#65289;&#65292;&#23427;&#22522;&#20110;&#22810;&#31181;&#29305;&#24449;&#36873;&#25321;&#31034;&#20363;&#20197;&#26368;&#22823;&#21270;&#32763;&#35793;&#36136;&#37327;&#12290;&#22312;&#22810;&#31181;&#35821;&#35328;&#23545;&#21644;&#35821;&#35328;&#27169;&#22411;&#19978;&#65292;&#25105;&#20204;&#35777;&#26126;CTQ Scorer&#26174;&#33879;&#36229;&#36807;&#20102;&#38543;&#26426;&#36873;&#25321;&#20197;&#21450;&#25991;&#29486;&#20013;&#25253;&#21578;&#30340;&#24378;&#21333;&#22240;&#32032;&#22522;&#32447;&#12290;&#30456;&#23545;&#20110;&#24378;BM25&#22522;&#32447;&#65292;&#25105;&#20204;&#36824;&#30475;&#21040;&#24179;&#22343;&#25913;&#21892;&#20102;2.5&#20010;COMET&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models have demonstrated the capability to perform on machine translation when the input is prompted with a few examples (in-context learning). Translation quality depends on various features of the selected examples, such as their quality and relevance, but previous work has predominantly focused on individual features in isolation. In this paper, we propose a general framework for combining different features influencing example selection. We learn a regression model, CTQ Scorer (Contextual Translation Quality), that selects examples based on multiple features in order to maximize the translation quality. On multiple language pairs and language models, we show that CTQ Scorer helps significantly outperform random selection as well as strong single-factor baselines reported in the literature. We also see an improvement of over 2.5 COMET points on average with respect to a strong BM25 retrieval-based baseline.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24418;&#24335;&#35821;&#27861;&#32422;&#26463;&#20016;&#23500;&#35299;&#30721;&#27493;&#39588;&#30340;&#26041;&#27861;&#65292;&#26377;&#25928;&#29983;&#25104;&#31526;&#21512;&#29305;&#23450;&#35821;&#27861;&#30340;&#22797;&#26434;&#36755;&#20986;&#32467;&#26500;&#65292;&#21516;&#26102;&#20801;&#35768;&#20219;&#20309;&#19978;&#19979;&#25991;&#26080;&#20851;&#35821;&#27861;&#38598;&#25104;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#22235;&#20010;&#20449;&#24687;&#25552;&#21462;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.13971</link><description>&lt;p&gt;
&#22522;&#20110;&#35821;&#27861;&#32422;&#26463;&#30340;&#35821;&#35328;&#27169;&#22411;&#28789;&#27963;&#35299;&#30721;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Flexible Grammar-Based Constrained Decoding for Language Models. (arXiv:2305.13971v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13971
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24418;&#24335;&#35821;&#27861;&#32422;&#26463;&#20016;&#23500;&#35299;&#30721;&#27493;&#39588;&#30340;&#26041;&#27861;&#65292;&#26377;&#25928;&#29983;&#25104;&#31526;&#21512;&#29305;&#23450;&#35821;&#27861;&#30340;&#22797;&#26434;&#36755;&#20986;&#32467;&#26500;&#65292;&#21516;&#26102;&#20801;&#35768;&#20219;&#20309;&#19978;&#19979;&#25991;&#26080;&#20851;&#35821;&#27861;&#38598;&#25104;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#22235;&#20010;&#20449;&#24687;&#25552;&#21462;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
LLM&#22312;&#35768;&#22810;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#24778;&#20154;&#30340;&#23569;&#37327;&#26679;&#26412;&#34920;&#29616;&#65292;&#20294;&#22312;&#29983;&#25104;&#20449;&#24687;&#25552;&#21462;&#25152;&#38656;&#30340;&#22797;&#26434;&#36755;&#20986;&#32467;&#26500;&#26102;&#20173;&#23384;&#22312;&#22256;&#38590;&#12290;&#36825;&#20010;&#38480;&#21046;&#28304;&#20110;LLM&#22312;&#27809;&#26377;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#20542;&#21521;&#20110;&#29983;&#25104;&#33258;&#30001;&#25991;&#26412;&#32780;&#19981;&#26159;&#36981;&#24490;&#29305;&#23450;&#35821;&#27861;&#30340;&#31934;&#30830;&#32467;&#26500;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#22312;&#35299;&#30721;&#27493;&#39588;&#20013;&#20351;&#29992;&#24418;&#24335;&#35821;&#27861;&#32422;&#26463;&#26469;&#20016;&#23500;&#27169;&#22411;&#12290;&#22312;&#25628;&#32034;&#36807;&#31243;&#20013;&#65292;&#21482;&#26377;&#31526;&#21512;&#35821;&#27861;&#20135;&#29983;&#35268;&#21017;&#30340;&#26377;&#25928;&#20196;&#29260;&#33021;&#34987;&#32771;&#34385;&#21040;&#12290;&#36825;&#26679;&#23601;&#24378;&#21046;&#21482;&#20135;&#29983;&#26377;&#25928;&#30340;&#24207;&#21015;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#38750;&#24120;&#36890;&#29992;&#21644;&#28789;&#27963;&#65292;&#20801;&#35768;&#20219;&#20309;&#19978;&#19979;&#25991;&#26080;&#20851;&#35821;&#27861;(CFG)&#38598;&#25104;&#21040;&#25105;&#20204;&#30340;&#33258;&#23450;&#20041;&#32422;&#26463;beam&#25628;&#32034;&#23454;&#29616;&#20013;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#35768;&#22810;NLP&#20219;&#21153;&#30340;&#36755;&#20986;&#21487;&#20197;&#34987;&#34920;&#31034;&#20026;&#24418;&#24335;&#35821;&#35328;&#65292;&#20351;&#23427;&#20204;&#36866;&#21512;&#22312;&#25105;&#20204;&#30340;&#26694;&#26550;&#20013;&#30452;&#25509;&#20351;&#29992;&#12290;&#23545;&#20110;&#36755;&#20986;&#31354;&#38388;&#21462;&#20915;&#20110;&#36755;&#20837;&#30340;&#20219;&#21153;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#36755;&#20837;&#30340;CFG&#65292;&#26681;&#25454;&#29305;&#23450;&#20110;&#36755;&#20837;&#30340;&#29305;&#24449;&#26356;&#26032;&#20135;&#29983;&#35268;&#21017;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#29983;&#25104;&#22797;&#26434;&#36755;&#20986;&#32467;&#26500;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#22312;&#22235;&#20010;&#20449;&#24687;&#25552;&#21462;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
LLMs have shown impressive few-shot performance across many tasks. However, they still struggle when it comes to generating complex output structures, such as those required for Information Extraction. This limitation stems from the fact that LLMs, without finetuning, tend to generate free text rather than precise structures that follow a specific grammar. In this work, we propose to enrich the decoding step with formal grammar constraints. During beam search, only valid token continuations compliant with the grammar production rules are considered. This enforces the generation of valid sequences exclusively. Our framework is highly general and flexible, allowing any Context-Free Grammar (CFG) to be integrated into our custom constrained beam search implementation. We demonstrate that the outputs of many NLP tasks can be represented as formal languages, making them suitable for direct use in our framework. For task where the output space is dependent on the input, we propose input-depe
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21512;&#25104;&#21453;&#39304;&#23545;&#40784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#26694;&#26550;&#65292;&#20960;&#20046;&#19981;&#38656;&#35201;&#20154;&#21147;&#25104;&#26412;&#65292;&#20063;&#19981;&#20381;&#36182;&#20110;&#39044;&#20808;&#23545;&#40784;&#30340;LLMs&#12290;&#20854;&#20013;&#65292;&#36890;&#36807;&#23545;&#23610;&#23544;&#21644;&#25552;&#31034;&#31561;&#19981;&#21516;&#22240;&#32032;&#30340;&#26222;&#36890; LLMS&#30340;&#21709;&#24212;&#36827;&#34892;&#22870;&#21169;&#24314;&#27169;&#65292;&#26469;&#27169;&#25311;&#39640;&#36136;&#37327;&#30340;&#31034;&#33539;&#26469;&#35757;&#32451;&#30417;&#30563;&#31574;&#30053;&#65292;&#24182;&#36827;&#19968;&#27493;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#20248;&#21270;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.13735</link><description>&lt;p&gt;
&#36890;&#36807;&#21512;&#25104;&#21453;&#39304;&#23545;&#40784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Aligning Large Language Models through Synthetic Feedback. (arXiv:2305.13735v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13735
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21512;&#25104;&#21453;&#39304;&#23545;&#40784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#26694;&#26550;&#65292;&#20960;&#20046;&#19981;&#38656;&#35201;&#20154;&#21147;&#25104;&#26412;&#65292;&#20063;&#19981;&#20381;&#36182;&#20110;&#39044;&#20808;&#23545;&#40784;&#30340;LLMs&#12290;&#20854;&#20013;&#65292;&#36890;&#36807;&#23545;&#23610;&#23544;&#21644;&#25552;&#31034;&#31561;&#19981;&#21516;&#22240;&#32032;&#30340;&#26222;&#36890; LLMS&#30340;&#21709;&#24212;&#36827;&#34892;&#22870;&#21169;&#24314;&#27169;&#65292;&#26469;&#27169;&#25311;&#39640;&#36136;&#37327;&#30340;&#31034;&#33539;&#26469;&#35757;&#32451;&#30417;&#30563;&#31574;&#30053;&#65292;&#24182;&#36827;&#19968;&#27493;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#20248;&#21270;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#23545;&#40784;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#65292;&#22240;&#20026;&#23427;&#33021;&#22815;&#25552;&#20379;&#22797;&#26434;&#30340;LLMs&#25511;&#21046;&#65292;&#20363;&#22914;&#20351;&#23427;&#20204;&#25353;&#29031;&#29305;&#23450;&#30340;&#25351;&#20196;&#25805;&#20316;&#32780;&#19981;&#20250;&#20135;&#29983;&#26377;&#23475;&#21453;&#24212;&#12290;&#28982;&#32780;&#65292;&#36825;&#38656;&#35201;&#22823;&#37327;&#30340;&#20154;&#31867;&#31034;&#33539;&#21644;&#21453;&#39304;&#12290;&#26368;&#36817;&#65292;&#24320;&#28304;&#27169;&#22411;&#35797;&#22270;&#36890;&#36807;&#25552;&#28860;&#26469;&#33258;&#24050;&#23545;&#40784;&#30340;LLMs&#65288;&#22914;InstructGPT&#25110;ChatGPT&#65289;&#30340;&#25968;&#25454;&#26469;&#22797;&#21046;&#23545;&#40784;&#23398;&#20064;&#36807;&#31243;&#12290;&#34429;&#28982;&#36825;&#20010;&#36807;&#31243;&#20943;&#23569;&#20102;&#20154;&#21147;&#25104;&#26412;&#65292;&#20294;&#26159;&#26500;&#24314;&#36825;&#20123;&#25968;&#25454;&#38598;&#23545;&#25945;&#24072;&#27169;&#22411;&#30340;&#20381;&#36182;&#24615;&#24456;&#39640;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#23545;&#40784;&#23398;&#20064;&#26694;&#26550;&#65292;&#20960;&#20046;&#19981;&#38656;&#35201;&#20154;&#31867;&#21171;&#21160;&#65292;&#20063;&#19981;&#20381;&#36182;&#20110;&#39044;&#20808;&#23545;&#40784;&#30340;LLMs&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;&#22823;&#23567;&#21644;&#25552;&#31034;&#31561;&#19981;&#21516;&#22240;&#32032;&#30340;&#26222;&#36890;LLMs&#30340;&#21709;&#24212;&#36827;&#34892;&#21512;&#25104;&#21453;&#39304;&#30340;&#22870;&#21169;&#24314;&#27169;(RM)&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;RM&#27169;&#25311;&#39640;&#36136;&#37327;&#30340;&#31034;&#33539;&#26469;&#35757;&#32451;&#30417;&#30563;&#31574;&#30053;&#65292;&#24182;&#36827;&#19968;&#27493;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#20248;&#21270;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Aligning large language models (LLMs) to human values has become increasingly important as it enables sophisticated steering of LLMs, e.g., making them follow given instructions while keeping them less toxic. However, it requires a significant amount of human demonstrations and feedback. Recently, open-sourced models have attempted to replicate the alignment learning process by distilling data from already aligned LLMs like InstructGPT or ChatGPT. While this process reduces human efforts, constructing these datasets has a heavy dependency on the teacher models. In this work, we propose a novel framework for alignment learning with almost no human labor and no dependency on pre-aligned LLMs. First, we perform reward modeling (RM) with synthetic feedback by contrasting responses from vanilla LLMs with various sizes and prompts. Then, we use the RM for simulating high-quality demonstrations to train a supervised policy and for further optimizing the model with reinforcement learning. Our 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;&#37038;&#20214;&#39046;&#22495;&#30340;&#20107;&#20214;&#25277;&#21462;&#25968;&#25454;&#38598;\dataset&#65292;&#27604;&#36739;&#20102;&#24207;&#21015;&#26631;&#35760;&#21644;&#29983;&#25104;&#24335;&#31471;&#21040;&#31471;&#25277;&#21462;&#30340;&#26041;&#27861;&#65292;&#32467;&#26524;&#34920;&#26126;&#35813;&#20219;&#21153;&#23384;&#22312;&#38750;&#36830;&#32493;&#20849;&#20139;&#35302;&#21457;&#22120;&#36328;&#24230;&#12289;&#38750;&#21629;&#21517;&#23454;&#20307;&#21442;&#25968;&#21644;&#37038;&#20214;&#20250;&#35805;&#21382;&#21490;&#31561;&#38590;&#28857;&#65292;&#26410;&#26469;&#38656;&#35201;&#26356;&#22810;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2305.13469</link><description>&lt;p&gt;
MAILEX: &#37038;&#20214;&#20107;&#20214;&#19982;&#21442;&#25968;&#25277;&#21462;
&lt;/p&gt;
&lt;p&gt;
MAILEX: Email Event and Argument Extraction. (arXiv:2305.13469v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13469
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;&#37038;&#20214;&#39046;&#22495;&#30340;&#20107;&#20214;&#25277;&#21462;&#25968;&#25454;&#38598;\dataset&#65292;&#27604;&#36739;&#20102;&#24207;&#21015;&#26631;&#35760;&#21644;&#29983;&#25104;&#24335;&#31471;&#21040;&#31471;&#25277;&#21462;&#30340;&#26041;&#27861;&#65292;&#32467;&#26524;&#34920;&#26126;&#35813;&#20219;&#21153;&#23384;&#22312;&#38750;&#36830;&#32493;&#20849;&#20139;&#35302;&#21457;&#22120;&#36328;&#24230;&#12289;&#38750;&#21629;&#21517;&#23454;&#20307;&#21442;&#25968;&#21644;&#37038;&#20214;&#20250;&#35805;&#21382;&#21490;&#31561;&#38590;&#28857;&#65292;&#26410;&#26469;&#38656;&#35201;&#26356;&#22810;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#25968;&#25454;&#38598; \dataset&#65292;&#29992;&#20110;&#20174;&#37038;&#20214;&#20018;&#20013;&#25191;&#34892;&#20107;&#20214;&#25277;&#21462;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20998;&#31867;&#27861;&#65292;&#28085;&#30422;&#20102;&#37038;&#20214;&#39046;&#22495;&#20013;&#30340; 10 &#31181;&#20107;&#20214;&#31867;&#22411;&#21644; 76 &#20010;&#21442;&#25968;&#12290;&#25105;&#20204;&#30340;&#26368;&#32456;&#25968;&#25454;&#38598;&#21253;&#25324;&#32422; 4K &#23553;&#26631;&#35760;&#26377;&#32422; 9K &#20010;&#20107;&#20214;&#23454;&#20363;&#30340;&#37038;&#20214;&#12290;&#20026;&#20102;&#20102;&#35299;&#20219;&#21153;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#23454;&#39564;&#65292;&#27604;&#36739;&#20102;&#20004;&#31181;&#24120;&#35265;&#30340;&#20107;&#20214;&#25277;&#21462;&#26041;&#27861;&#65292;&#21363;&#24207;&#21015;&#26631;&#35760;&#21644;&#29983;&#25104;&#24335;&#31471;&#21040;&#31471;&#25277;&#21462;&#65288;&#21253;&#25324;&#20960;&#29575; GPT-3.5&#65289;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#37038;&#20214;&#20107;&#20214;&#25277;&#21462;&#20219;&#21153;&#36828;&#26410;&#24471;&#21040;&#35299;&#20915;&#65292;&#22240;&#20026;&#23384;&#22312;&#35832;&#22810;&#38590;&#28857;&#65292;&#20363;&#22914;&#25552;&#21462;&#38750;&#36830;&#32493;&#20849;&#20139;&#35302;&#21457;&#22120;&#36328;&#24230;&#12289;&#25552;&#21462;&#38750;&#21629;&#21517;&#23454;&#20307;&#21442;&#25968;&#21644;&#24314;&#27169;&#37038;&#20214;&#20250;&#35805;&#21382;&#21490;&#31561;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#25552;&#20986;&#20102;&#26410;&#26469;&#22312;&#36825;&#20010;&#29305;&#23450;&#39046;&#22495;&#30340;&#20107;&#20214;&#25277;&#21462;&#20219;&#21153;&#20013;&#38656;&#35201;&#36827;&#34892;&#26356;&#22810;&#30740;&#31350;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we present the first dataset, \dataset, for performing event extraction from conversational email threads. To this end, we first proposed a new taxonomy covering 10 event types and 76 arguments in the email domain. Our final dataset includes $\sim$4K emails annotated with $\sim$9K event instances. To understand the task challenges, we conducted a series of experiments comparing two commonly-seen lines of approaches for event extraction, i.e., sequence labeling and generative end-to-end extraction (including few-shot GPT-3.5). Our results showed that the task of email event extraction is far from being addressed, due to challenges lying in, e.g., extracting non-continuous, shared trigger spans, extracting non-named entity arguments, and modeling the email conversational history. Our work thus suggests more investigations in this domain-specific event extraction task in the future.\footnote{The source code and dataset can be obtained from \url{https://github.com/salokr/Emai
&lt;/p&gt;</description></item><item><title>&#20803;&#35821;&#35328;&#25552;&#31034;&#19982;&#30452;&#25509;&#27010;&#29575;&#27979;&#37327;&#30456;&#27604;&#65292;&#23545;&#20110;&#34913;&#37327;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35821;&#35328;&#30693;&#35782;&#26469;&#35828;&#65292;&#20803;&#35821;&#35328;&#21028;&#26029;&#25928;&#26524;&#36739;&#24046;&#65292;&#24182;&#19988;&#38543;&#30528;&#25552;&#31034;&#26597;&#35810;&#20559;&#31163;&#30452;&#25509;&#27979;&#37327;&#30340;&#27010;&#29575;&#65292;&#19968;&#33268;&#24615;&#21464;&#24046;&#12290;&#25552;&#31034;&#30340;&#36127;&#38754;&#32467;&#26524;&#19981;&#33021;&#20316;&#20026;&#32570;&#20047;&#29305;&#23450;&#35821;&#35328;&#27010;&#25324;&#30340;&#30830;&#20991;&#35777;&#25454;&#12290;&#20174;&#38381;&#28304;API&#36801;&#31227;&#20013;&#65292;&#25105;&#20204;&#20063;&#20250;&#22833;&#21435;&#19968;&#23450;&#30340;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2305.13264</link><description>&lt;p&gt;
&#25552;&#31034;&#19981;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#27010;&#29575;&#27979;&#37327;&#30340;&#26367;&#20195;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Prompting is not a substitute for probability measurements in large language models. (arXiv:2305.13264v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13264
&lt;/p&gt;
&lt;p&gt;
&#20803;&#35821;&#35328;&#25552;&#31034;&#19982;&#30452;&#25509;&#27010;&#29575;&#27979;&#37327;&#30456;&#27604;&#65292;&#23545;&#20110;&#34913;&#37327;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35821;&#35328;&#30693;&#35782;&#26469;&#35828;&#65292;&#20803;&#35821;&#35328;&#21028;&#26029;&#25928;&#26524;&#36739;&#24046;&#65292;&#24182;&#19988;&#38543;&#30528;&#25552;&#31034;&#26597;&#35810;&#20559;&#31163;&#30452;&#25509;&#27979;&#37327;&#30340;&#27010;&#29575;&#65292;&#19968;&#33268;&#24615;&#21464;&#24046;&#12290;&#25552;&#31034;&#30340;&#36127;&#38754;&#32467;&#26524;&#19981;&#33021;&#20316;&#20026;&#32570;&#20047;&#29305;&#23450;&#35821;&#35328;&#27010;&#25324;&#30340;&#30830;&#20991;&#35777;&#25454;&#12290;&#20174;&#38381;&#28304;API&#36801;&#31227;&#20013;&#65292;&#25105;&#20204;&#20063;&#20250;&#22833;&#21435;&#19968;&#23450;&#30340;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#31034;&#29616;&#22312;&#26159;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#35821;&#35328;&#30693;&#35782;&#30340;&#20027;&#35201;&#26041;&#27861;&#12290;&#32780;&#20854;&#20182;&#26041;&#27861;&#30452;&#25509;&#35835;&#21462;&#27169;&#22411;&#23545;&#23383;&#31526;&#20018;&#30340;&#27010;&#29575;&#20998;&#24067;&#65292;&#25552;&#31034;&#38656;&#35201;&#27169;&#22411;&#36890;&#36807;&#22788;&#29702;&#35821;&#35328;&#36755;&#20837;&#26469;&#35775;&#38382;&#36825;&#20123;&#20869;&#37096;&#20449;&#24687;&#65292;&#20174;&#32780;&#38544;&#21547;&#22320;&#27979;&#35797;&#19968;&#31181;&#26032;&#22411;&#30340;&#32039;&#24613;&#33021;&#21147;&#65306;&#20803;&#35821;&#35328;&#21028;&#26029;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#27604;&#36739;&#20803;&#35821;&#35328;&#25552;&#31034;&#21644;&#30452;&#25509;&#27010;&#29575;&#27979;&#37327;&#20316;&#20026;&#34913;&#37327;&#27169;&#22411;&#35821;&#35328;&#30693;&#35782;&#30340;&#26041;&#27861;&#12290;&#24635;&#20307;&#19978;&#65292;&#25105;&#20204;&#21457;&#29616;LLM&#30340;&#20803;&#35821;&#35328;&#21028;&#26029;&#19981;&#22914;&#30452;&#25509;&#20174;&#34920;&#31034;&#20013;&#27966;&#29983;&#30340;&#25968;&#37327;&#12290;&#27492;&#22806;&#65292;&#38543;&#30528;&#25552;&#31034;&#26597;&#35810;&#20559;&#31163;&#30452;&#25509;&#27979;&#37327;&#19979;&#19968;&#20010;&#21333;&#35789;&#27010;&#29575;&#65292;&#19968;&#33268;&#24615;&#21464;&#24046;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#20381;&#36182;&#20110;&#20803;&#35821;&#35328;&#25552;&#31034;&#30340;&#36127;&#38754;&#32467;&#26524;&#19981;&#33021;&#20316;&#20026;LLM&#32570;&#20047;&#29305;&#23450;&#35821;&#35328;&#27010;&#25324;&#30340;&#30830;&#20991;&#35777;&#25454;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#36824;&#31361;&#26174;&#20102;&#20174;&#38381;&#28304;API&#36801;&#31227;&#20013;&#25152;&#22833;&#21435;&#30340;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prompting is now a dominant method for evaluating the linguistic knowledge of large language models (LLMs). While other methods directly read out models' probability distributions over strings, prompting requires models to access this internal information by processing linguistic input, thereby implicitly testing a new type of emergent ability: metalinguistic judgment. In this study, we compare metalinguistic prompting and direct probability measurements as ways of measuring models' linguistic knowledge. Broadly, we find that LLMs' metalinguistic judgments are inferior to quantities directly derived from representations. Furthermore, consistency gets worse as the prompt query diverges from direct measurements of next-word probabilities. Our findings suggest that negative results relying on metalinguistic prompts cannot be taken as conclusive evidence that an LLM lacks a particular linguistic generalization. Our results also highlight the value that is lost with the move to closed APIs 
&lt;/p&gt;</description></item><item><title>SCITAB&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#35780;&#20272;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;1.2K&#20010;&#32463;&#39564;&#35777;&#30340;&#31185;&#23398;&#20107;&#23454;&#21644;&#30456;&#20851;&#30340;&#31185;&#23398;&#34920;&#26684;&#65292;&#35201;&#27714;&#36827;&#34892;&#32452;&#21512;&#25512;&#29702;&#21644;&#20107;&#23454;&#39564;&#35777;&#12290;&#23545;&#20110;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#26469;&#35828;&#65292;SCITAB&#25552;&#20986;&#20102;&#35768;&#22810;&#29420;&#29305;&#25361;&#25112;&#65292;&#21253;&#25324;&#34920;&#26684;&#23450;&#20301;&#12289;&#20107;&#23454;&#27495;&#20041;&#21644;&#32452;&#21512;&#25512;&#29702;&#12290;&#25152;&#26377;&#27169;&#22411;&#20013;&#65292;&#38500;&#20102;GPT-4&#20043;&#22806;&#65292;&#24615;&#33021;&#20165;&#30053;&#39640;&#20110;&#38543;&#26426;&#29468;&#27979;&#12290;&#25552;&#31034;&#25216;&#26415;&#22914;&#24605;&#32500;&#38142;&#23545;&#20110;&#22312;SCITAB&#19978;&#25552;&#21319;&#24615;&#33021;&#20960;&#20046;&#27809;&#26377;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2305.13186</link><description>&lt;p&gt;
SCITAB: &#19968;&#20010;&#23545;&#31185;&#23398;&#34920;&#26684;&#36827;&#34892;&#32452;&#21512;&#25512;&#29702;&#21644;&#20107;&#23454;&#39564;&#35777;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
SCITAB: A Challenging Benchmark for Compositional Reasoning and Claim Verification on Scientific Tables. (arXiv:2305.13186v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13186
&lt;/p&gt;
&lt;p&gt;
SCITAB&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#35780;&#20272;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;1.2K&#20010;&#32463;&#39564;&#35777;&#30340;&#31185;&#23398;&#20107;&#23454;&#21644;&#30456;&#20851;&#30340;&#31185;&#23398;&#34920;&#26684;&#65292;&#35201;&#27714;&#36827;&#34892;&#32452;&#21512;&#25512;&#29702;&#21644;&#20107;&#23454;&#39564;&#35777;&#12290;&#23545;&#20110;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#26469;&#35828;&#65292;SCITAB&#25552;&#20986;&#20102;&#35768;&#22810;&#29420;&#29305;&#25361;&#25112;&#65292;&#21253;&#25324;&#34920;&#26684;&#23450;&#20301;&#12289;&#20107;&#23454;&#27495;&#20041;&#21644;&#32452;&#21512;&#25512;&#29702;&#12290;&#25152;&#26377;&#27169;&#22411;&#20013;&#65292;&#38500;&#20102;GPT-4&#20043;&#22806;&#65292;&#24615;&#33021;&#20165;&#30053;&#39640;&#20110;&#38543;&#26426;&#29468;&#27979;&#12290;&#25552;&#31034;&#25216;&#26415;&#22914;&#24605;&#32500;&#38142;&#23545;&#20110;&#22312;SCITAB&#19978;&#25552;&#21319;&#24615;&#33021;&#20960;&#20046;&#27809;&#26377;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#31185;&#23398;&#20107;&#23454;&#26680;&#26597;&#22522;&#20934;&#27979;&#35797;&#23384;&#22312;&#19968;&#20123;&#19981;&#36275;&#65292;&#20363;&#22914;&#26469;&#33258;&#20247;&#21253;&#23457;&#26597;&#30340;&#20559;&#35265;&#21644;&#23545;&#22522;&#20110;&#25991;&#26412;&#30340;&#35777;&#25454;&#30340;&#36807;&#24230;&#20381;&#36182;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;SCITAB&#65292;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#35780;&#20272;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;1.2K&#20010;&#32463;&#39564;&#35777;&#30340;&#31185;&#23398;&#20107;&#23454;&#65292;&#36825;&#20123;&#20107;&#23454;1&#65289;&#26469;&#28304;&#20110;&#30495;&#23454;&#30340;&#31185;&#23398;&#20986;&#29256;&#29289;&#65292;2&#65289;&#38656;&#35201;&#32452;&#21512;&#25512;&#29702;&#36827;&#34892;&#39564;&#35777;&#12290;&#36825;&#20123;&#20107;&#23454;&#19982;&#21253;&#21547;&#35777;&#25454;&#30340;&#31185;&#23398;&#34920;&#26684;&#36827;&#34892;&#37197;&#23545;&#65292;&#24182;&#36827;&#34892;&#20102;&#26631;&#35760;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#35780;&#20272;&#65292;&#25105;&#20204;&#35777;&#26126;SCITAB&#23545;&#20110;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#65292;&#21253;&#25324;&#22522;&#20110;&#34920;&#26684;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#38500;&#20102;GPT-4&#22806;&#65292;&#25152;&#26377;&#27169;&#22411;&#30340;&#24615;&#33021;&#20165;&#30053;&#39640;&#20110;&#38543;&#26426;&#29468;&#27979;&#12290;&#27969;&#34892;&#30340;&#25552;&#31034;&#25216;&#26415;&#65292;&#22914;&#24605;&#32500;&#38142;&#65292;&#23545;SCITAB&#30340;&#24615;&#33021;&#25552;&#21319;&#19981;&#22823;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;SCITAB&#25552;&#20986;&#30340;&#20960;&#20010;&#29420;&#29305;&#25361;&#25112;&#65292;&#21253;&#25324;&#34920;&#26684;&#23450;&#20301;&#12289;&#20107;&#23454;&#30340;&#27495;&#20041;&#24615;&#21644;&#32452;&#21512;&#25512;&#29702;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21644;&#25968;&#25454;&#21487;&#20379;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current scientific fact-checking benchmarks exhibit several shortcomings, such as biases arising from crowd-sourced claims and an over-reliance on text-based evidence. We present SCITAB, a challenging evaluation dataset consisting of 1.2K expert-verified scientific claims that 1) originate from authentic scientific publications and 2) require compositional reasoning for verification. The claims are paired with evidence-containing scientific tables annotated with labels. Through extensive evaluations, we demonstrate that SCITAB poses a significant challenge to state-of-the-art models, including table-based pretraining models and large language models. All models except GPT-4 achieved performance barely above random guessing. Popular prompting techniques, such as Chain-of-Thought, do not achieve much performance gains on SCITAB. Our analysis uncovers several unique challenges posed by SCITAB, including table grounding, claim ambiguity, and compositional reasoning. Our codes and data are 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20391;&#37325;&#20110;&#35780;&#20272;&#24320;&#25918;&#24335;&#38382;&#31572;&#65288;Open-QA&#65289;&#20219;&#21153;&#30340;&#26041;&#27861;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#20219;&#21153;QA-Eval&#21644;&#25968;&#25454;&#38598;EVOUNA&#65292;&#36890;&#36807;&#20154;&#24037;&#35780;&#20272;&#26041;&#27861;&#26469;&#35780;&#20272;AI&#29983;&#25104;&#30340;&#31572;&#26696;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#35843;&#26597;&#20102;&#19982;&#20154;&#24037;&#35780;&#20272;&#30456;&#20851;&#30340;&#26041;&#27861;&#65292;&#24182;&#35752;&#35770;&#20102;&#24403;&#21069;&#26041;&#27861;&#30340;&#32570;&#38519;&#21644;&#25913;&#36827;&#26041;&#27861;&#12290;&#25105;&#20204;&#30456;&#20449;&#36825;&#23545;&#20110;&#26410;&#26469;&#30340;&#33258;&#21160;&#35780;&#20272;&#24037;&#20855;&#21457;&#23637;&#21644;&#30740;&#31350;&#20855;&#26377;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2305.12421</link><description>&lt;p&gt;
&#35780;&#20272;&#24320;&#25918;&#24335;&#38382;&#31572;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Evaluating Open-QA Evaluation. (arXiv:2305.12421v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12421
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20391;&#37325;&#20110;&#35780;&#20272;&#24320;&#25918;&#24335;&#38382;&#31572;&#65288;Open-QA&#65289;&#20219;&#21153;&#30340;&#26041;&#27861;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#20219;&#21153;QA-Eval&#21644;&#25968;&#25454;&#38598;EVOUNA&#65292;&#36890;&#36807;&#20154;&#24037;&#35780;&#20272;&#26041;&#27861;&#26469;&#35780;&#20272;AI&#29983;&#25104;&#30340;&#31572;&#26696;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#35843;&#26597;&#20102;&#19982;&#20154;&#24037;&#35780;&#20272;&#30456;&#20851;&#30340;&#26041;&#27861;&#65292;&#24182;&#35752;&#35770;&#20102;&#24403;&#21069;&#26041;&#27861;&#30340;&#32570;&#38519;&#21644;&#25913;&#36827;&#26041;&#27861;&#12290;&#25105;&#20204;&#30456;&#20449;&#36825;&#23545;&#20110;&#26410;&#26469;&#30340;&#33258;&#21160;&#35780;&#20272;&#24037;&#20855;&#21457;&#23637;&#21644;&#30740;&#31350;&#20855;&#26377;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20391;&#37325;&#20110;&#23545;&#24320;&#25918;&#24335;&#38382;&#31572;&#65288;Open-QA&#65289;&#20219;&#21153;&#30340;&#35780;&#20272;&#65292;&#35813;&#20219;&#21153;&#21487;&#20197;&#30452;&#25509;&#20272;&#35745;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20107;&#23454;&#24615;&#12290;&#30446;&#21069;&#30340;&#33258;&#21160;&#35780;&#20272;&#26041;&#27861;&#24050;&#26174;&#31034;&#20986;&#19968;&#23450;&#30340;&#23616;&#38480;&#24615;&#65292;&#34920;&#26126;&#20154;&#24037;&#35780;&#20272;&#20173;&#28982;&#26159;&#26368;&#21487;&#38752;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#20219;&#21153;&#65292;&#21363;&#35780;&#20272;QA&#35780;&#20272;&#65288;QA-Eval&#65289;&#20197;&#21450;&#30456;&#24212;&#30340;&#25968;&#25454;&#38598;EVOUNA&#65292;&#26088;&#22312;&#35780;&#20272;AI&#29983;&#25104;&#30340;&#31572;&#26696;&#19982;Open-QA&#20013;&#30340;&#26631;&#20934;&#31572;&#26696;&#20043;&#38388;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#21033;&#29992;&#20154;&#24037;&#26631;&#27880;&#30340;&#32467;&#26524;&#26469;&#35780;&#20272;&#36825;&#20123;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#37027;&#20123;&#19982;&#20154;&#24037;&#35780;&#20272;&#20855;&#26377;&#39640;&#24230;&#30456;&#20851;&#24615;&#30340;&#26041;&#27861;&#65292;&#35748;&#20026;&#23427;&#20204;&#26356;&#21487;&#38752;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#24403;&#21069;&#26041;&#27861;&#30340;&#32570;&#38519;&#20197;&#21450;&#25913;&#36827;&#22522;&#20110;LLM&#30340;&#35780;&#20272;&#22120;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30456;&#20449;&#65292;&#36825;&#20010;&#26032;&#30340;QA-Eval&#20219;&#21153;&#21644;&#30456;&#24212;&#30340;&#25968;&#25454;&#38598;EVOUNA&#23558;&#20419;&#36827;&#26356;&#26377;&#25928;&#30340;&#33258;&#21160;&#35780;&#20272;&#24037;&#20855;&#30340;&#24320;&#21457;&#65292;&#24182;&#23545;&#26410;&#26469;&#30340;&#30740;&#31350;&#20855;&#26377;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study focuses on the evaluation of the Open Question Answering (Open-QA) task, which can directly estimate the factuality of large language models (LLMs). Current automatic evaluation methods have shown limitations, indicating that human evaluation still remains the most reliable approach. We introduce a new task, Evaluating QA Evaluation (QA-Eval) and the corresponding dataset EVOUNA, designed to assess the accuracy of AI-generated answers in relation to standard answers within Open-QA. Our evaluation of these methods utilizes human-annotated results to measure their performance. Specifically, the work investigates methods that show high correlation with human evaluations, deeming them more reliable. We also discuss the pitfalls of current methods and methods to improve LLM-based evaluators. We believe this new QA-Eval task and corresponding dataset EVOUNA will facilitate the development of more effective automatic evaluation tools and prove valuable for future research in this a
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#24341;&#20837;&#20102;Clifford&#32676;&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;&#65292;&#23427;&#21487;&#20197;&#26500;&#24314;O(n)&#21644;E(n)&#31561;&#21464;&#27169;&#22411;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#35843;&#25972;Clifford&#32676;&#30340;&#23450;&#20041;&#20197;&#21450;&#20445;&#25345;&#21521;&#37327;&#31354;&#38388;&#21644;&#20056;&#27861;&#32467;&#26500;&#30340;&#20316;&#29992;&#26469;&#23454;&#29616;&#22810;&#20010;&#26377;&#21033;&#23646;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.11141</link><description>&lt;p&gt;
Clifford&#32676;&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Clifford Group Equivariant Neural Networks. (arXiv:2305.11141v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11141
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;Clifford&#32676;&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;&#65292;&#23427;&#21487;&#20197;&#26500;&#24314;O(n)&#21644;E(n)&#31561;&#21464;&#27169;&#22411;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#35843;&#25972;Clifford&#32676;&#30340;&#23450;&#20041;&#20197;&#21450;&#20445;&#25345;&#21521;&#37327;&#31354;&#38388;&#21644;&#20056;&#27861;&#32467;&#26500;&#30340;&#20316;&#29992;&#26469;&#23454;&#29616;&#22810;&#20010;&#26377;&#21033;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;Clifford&#32676;&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;&#65306;&#19968;&#31181;&#26500;&#24314;O(n)&#21644;E(n)&#31561;&#21464;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#30830;&#23450;&#24182;&#30740;&#31350;&#20102;Clifford&#32676;&#65292;&#23427;&#26159;Clifford&#20195;&#25968;&#20013;&#30340;&#19968;&#20010;&#23376;&#32676;&#65292;&#20854;&#23450;&#20041;&#32463;&#36807;&#35843;&#25972;&#20197;&#23454;&#29616;&#22810;&#20010;&#26377;&#21033;&#23646;&#24615;&#12290;&#20027;&#35201;&#22320;&#65292;&#35813;&#32676;&#30340;&#20316;&#29992;&#24418;&#25104;&#20102;&#19968;&#20010;&#27491;&#20132;&#33258;&#21516;&#26500;&#65292;&#25193;&#23637;&#21040;&#25972;&#20010;Clifford&#20195;&#25968;&#65292;&#21516;&#26102;&#23562;&#37325;&#22810;&#30690;&#20998;&#32423;&#12290;&#36825;&#23548;&#33268;&#20102;&#23545;&#24212;&#20110;&#22810;&#30690;&#20998;&#35299;&#30340;&#22810;&#20010;&#38750;&#31561;&#20215;&#23376;&#34920;&#31034;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#35813;&#20316;&#29992;&#19981;&#20165;&#23562;&#37325;Clifford&#20195;&#25968;&#30340;&#21521;&#37327;&#31354;&#38388;&#32467;&#26500;&#65292;&#36824;&#23562;&#37325;&#20854;&#20056;&#27861;&#32467;&#26500;&#65292;&#21363;&#20960;&#20309;&#20056;&#31215;&#12290;&#36825;&#20123;&#21457;&#29616;&#24847;&#21619;&#30528;&#25105;&#20204;&#21487;&#20197;&#24471;&#21040;&#22312;&#20219;&#24847;&#32500;&#30340;&#20869;&#31215;&#31354;&#38388;&#20013;&#20248;&#38597;&#22320;&#25512;&#24191;&#30340;&#34920;&#36798;&#23618;&#12290;&#25105;&#20204;&#29305;&#21035;&#23637;&#31034;&#20102;&#20174;&#19968;&#20010;sin
&lt;/p&gt;
&lt;p&gt;
We introduce Clifford Group Equivariant Neural Networks: a novel approach for constructing $\mathrm{O}(n)$- and $\mathrm{E}(n)$-equivariant models. We identify and study the $\textit{Clifford group}$, a subgroup inside the Clifford algebra whose definition we adjust to achieve several favorable properties. Primarily, the group's action forms an orthogonal automorphism that extends beyond the typical vector space to the entire Clifford algebra while respecting the multivector grading. This leads to several non-equivalent subrepresentations corresponding to the multivector decomposition. Furthermore, we prove that the action respects not just the vector space structure of the Clifford algebra but also its multiplicative structure, i.e., the geometric product. These findings imply that every polynomial in multivectors, An advantage worth mentioning is that we obtain expressive layers that can elegantly generalize to inner-product spaces of any dimension. We demonstrate, notably from a sin
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#21033;&#29992;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22914;CLIP&#26469;&#26816;&#27979;&#25991;&#26412;&#30340;&#35270;&#35273;&#24615;&#65292;&#24182;&#25552;&#20986;fine-tuning&#31574;&#30053;&#65292;&#23558;&#38750;&#35270;&#35273;&#25991;&#26412;&#26144;&#23556;&#20026;NULL&#22270;&#20687;&#65292;&#21305;&#37197;&#35270;&#35273;&#25991;&#26412;&#19982;&#23545;&#24212;&#22270;&#20687;&#65292;&#20197;&#35299;&#38145;&#22312;&#25991;&#26412;&#20013;&#23884;&#20837;&#30456;&#20851;&#22270;&#20687;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.10434</link><description>&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#23398;&#20064;&#25991;&#26412;&#30340;&#35270;&#35273;&#24615;
&lt;/p&gt;
&lt;p&gt;
Learning the Visualness of Text Using Large Vision-Language Models. (arXiv:2305.10434v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10434
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#21033;&#29992;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22914;CLIP&#26469;&#26816;&#27979;&#25991;&#26412;&#30340;&#35270;&#35273;&#24615;&#65292;&#24182;&#25552;&#20986;fine-tuning&#31574;&#30053;&#65292;&#23558;&#38750;&#35270;&#35273;&#25991;&#26412;&#26144;&#23556;&#20026;NULL&#22270;&#20687;&#65292;&#21305;&#37197;&#35270;&#35273;&#25991;&#26412;&#19982;&#23545;&#24212;&#22270;&#20687;&#65292;&#20197;&#35299;&#38145;&#22312;&#25991;&#26412;&#20013;&#23884;&#20837;&#30456;&#20851;&#22270;&#20687;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#25991;&#26412;&#20250;&#22312;&#20154;&#20204;&#30340;&#33041;&#28023;&#20013;&#21576;&#29616;&#22270;&#20687;&#65292;&#32780;&#38750;&#35270;&#35273;&#25991;&#26412;&#21017;&#26080;&#27861;&#36798;&#21040;&#27492;&#25928;&#26524;&#12290;&#33258;&#21160;&#26816;&#27979;&#25991;&#26412;&#30340;&#35270;&#35273;&#24615;&#23558;&#26377;&#21161;&#20110;&#22312;&#25991;&#26412;&#20013;&#23884;&#20837;&#30456;&#20851;&#22270;&#20687;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;3620&#20010;&#33521;&#35821;&#21477;&#23376;&#21450;&#20854;&#22810;&#20010;&#20154;&#31867;&#27880;&#37322;&#32773;&#25552;&#20379;&#30340;&#35270;&#35273;&#24615;&#24471;&#20998;&#65292;&#24182;&#20351;&#29992;&#21253;&#21547;&#25991;&#26412;&#21644;&#35270;&#35273;&#36164;&#20135;&#30340;&#25991;&#26723;&#26469;&#21019;&#24314;&#36828;&#31243;&#30417;&#30563;&#35821;&#26009;&#24211;&#65292;&#20197;&#35780;&#20272;&#25991;&#26412;&#30340;&#35270;&#35273;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visual text evokes an image in a person's mind, while non-visual text fails to do so. A method to automatically detect visualness in text will unlock the ability to augment text with relevant images, as neural text-to-image generation and retrieval models operate on the implicit assumption that the input text is visual in nature. We curate a dataset of 3,620 English sentences and their visualness scores provided by multiple human annotators. Additionally, we use documents that contain text and visual assets to create a distantly supervised corpus of document text and associated images. We also propose a fine-tuning strategy that adapts large vision-language models like CLIP that assume a one-to-one correspondence between text and image to the task of scoring text visualness from text input alone. Our strategy involves modifying the model's contrastive learning objective to map text identified as non-visual to a common NULL image while matching visual text to their corresponding images 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#22312;ChatGPT&#20013;&#38598;&#25104;&#21307;&#23398;&#39046;&#22495;&#30693;&#35782;&#21644;&#21551;&#29992;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#65292;&#22312;&#20013;&#22269;&#22269;&#23478;&#21307;&#23398;&#25191;&#19994;&#21307;&#24072;&#36164;&#26684;&#32771;&#35797;&#20013;&#21462;&#24471;&#25104;&#21151;&#65292;&#36825;&#20026;&#24314;&#31435;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#21644;&#21307;&#23398;&#39046;&#22495;&#30693;&#35782;&#30340;&#21019;&#26032;&#24212;&#29992;&#25552;&#20379;&#20102;&#21487;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.10163</link><description>&lt;p&gt;
&#22522;&#20110;&#30693;&#35782;&#22686;&#24378;&#30340;&#29983;&#25104;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#20013;&#22269;&#21307;&#23398;&#25191;&#19994;&#21307;&#24072;&#36164;&#26684;&#32771;&#35797;&#19978;&#30340;&#24212;&#29992;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Qualifying Chinese Medical Licensing Examination with Knowledge Enhanced Generative Pre-training Model. (arXiv:2305.10163v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10163
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#22312;ChatGPT&#20013;&#38598;&#25104;&#21307;&#23398;&#39046;&#22495;&#30693;&#35782;&#21644;&#21551;&#29992;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#65292;&#22312;&#20013;&#22269;&#22269;&#23478;&#21307;&#23398;&#25191;&#19994;&#21307;&#24072;&#36164;&#26684;&#32771;&#35797;&#20013;&#21462;&#24471;&#25104;&#21151;&#65292;&#36825;&#20026;&#24314;&#31435;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#21644;&#21307;&#23398;&#39046;&#22495;&#30693;&#35782;&#30340;&#21019;&#26032;&#24212;&#29992;&#25552;&#20379;&#20102;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#27169;&#22411;&#65288;GPT&#65289;&#65292;&#22914;ChatGPT&#65292;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;&#23613;&#31649;ChatGPT&#24050;&#34987;&#25972;&#21512;&#21040;&#21508;&#20010;&#39046;&#22495;&#30340;&#24037;&#20316;&#27969;&#20013;&#20197;&#25552;&#39640;&#25928;&#29575;&#65292;&#20294;&#20854;&#24494;&#35843;&#36807;&#31243;&#30340;&#28789;&#27963;&#24615;&#19981;&#36275;&#65292;&#38459;&#30861;&#20102;&#20854;&#22312;&#38656;&#35201;&#24191;&#27867;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#21644;&#35821;&#20041;&#30693;&#35782;&#30340;&#39046;&#22495;&#65292;&#22914;&#21307;&#30103;&#20445;&#20581;&#65292;&#30340;&#24212;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;ChatGPT&#22312;&#20013;&#22269;&#22269;&#23478;&#21307;&#23398;&#25191;&#19994;&#21307;&#24072;&#36164;&#26684;&#32771;&#35797;&#65288;CNMLE&#65289;&#20013;&#30340;&#34920;&#29616;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#25913;&#36827;ChatGPT&#65292;&#21363;&#20174;&#20004;&#20010;&#26041;&#38754;&#38598;&#25104;&#21307;&#23398;&#39046;&#22495;&#30693;&#35782;&#21644;&#21551;&#29992;&#23569;&#26679;&#26412;&#23398;&#20064;&#12290;&#36890;&#36807;&#20351;&#29992;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#26816;&#32034;&#26041;&#27861;&#65292;&#23558;&#21307;&#23398;&#32972;&#26223;&#30693;&#35782;&#25552;&#21462;&#20026;&#35821;&#20041;&#25351;&#20196;&#26469;&#25351;&#23548;ChatGPT&#30340;&#25512;&#26029;&#12290;&#31867;&#20284;&#22320;&#65292;&#30456;&#20851;&#30340;&#21307;&#30103;&#38382;&#39064;&#34987;&#35782;&#21035;&#24182;&#20316;&#20026;&#28436;&#31034;&#36755;&#20837;&#32473;ChatGPT&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#30452;&#25509;&#24212;&#29992;ChatGPT&#26080;&#27861;&#22312;CNMLE&#19978;&#33719;&#24471;&#21512;&#26684;&#20998;&#25968;&#65288;51&#20998;&#65289;&#65292;&#21482;&#26377;&#22522;&#20110;&#30693;&#35782;&#22686;&#24378;&#35757;&#32451;&#30340;&#27169;&#22411;&#25104;&#21151;&#36890;&#36807;&#32771;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative Pre-Training (GPT) models like ChatGPT have demonstrated exceptional performance in various Natural Language Processing (NLP) tasks. Although ChatGPT has been integrated into the overall workflow to boost efficiency in many domains, the lack of flexibility in the finetuning process hinders its applications in areas that demand extensive domain expertise and semantic knowledge, such as healthcare. In this paper, we evaluate ChatGPT on the China National Medical Licensing Examination (CNMLE) and propose a novel approach to improve ChatGPT from two perspectives: integrating medical domain knowledge and enabling few-shot learning. By using a simple but effective retrieval method, medical background knowledge is extracted as semantic instructions to guide the inference of ChatGPT. Similarly, relevant medical questions are identified and fed as demonstrations to ChatGPT. Experimental results show that directly applying ChatGPT fails to qualify the CNMLE at a score of 51 (i.e., onl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;$\mathrm{E}(n)$&#31561;&#21464;&#28040;&#24687;&#20256;&#36882;&#21333;&#32431;&#32593;&#32476;(EMPSNs)&#65292;&#19968;&#31181;&#21516;&#26102;&#23558;&#28040;&#24687;&#20256;&#36882;&#21333;&#32431;&#32593;&#32476;&#21644;$\mathrm{E}(n)$&#31561;&#21464;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#20248;&#21183;&#32467;&#21512;&#65292;&#22312;&#22788;&#29702;&#39640;&#32500;&#25968;&#25454;&#26102;&#21033;&#29992;&#20960;&#20309;&#20449;&#24687;&#38450;&#27490;&#36807;&#24230;&#24179;&#28369;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.07100</link><description>&lt;p&gt;
$\mathrm{E}(n)$&#31561;&#21464;&#28040;&#24687;&#20256;&#36882;&#21333;&#32431;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
$\mathrm{E}(n)$ Equivariant Message Passing Simplicial Networks. (arXiv:2305.07100v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07100
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;$\mathrm{E}(n)$&#31561;&#21464;&#28040;&#24687;&#20256;&#36882;&#21333;&#32431;&#32593;&#32476;(EMPSNs)&#65292;&#19968;&#31181;&#21516;&#26102;&#23558;&#28040;&#24687;&#20256;&#36882;&#21333;&#32431;&#32593;&#32476;&#21644;$\mathrm{E}(n)$&#31561;&#21464;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#20248;&#21183;&#32467;&#21512;&#65292;&#22312;&#22788;&#29702;&#39640;&#32500;&#25968;&#25454;&#26102;&#21033;&#29992;&#20960;&#20309;&#20449;&#24687;&#38450;&#27490;&#36807;&#24230;&#24179;&#28369;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;$\mathrm{E}(n)$&#31561;&#21464;&#28040;&#24687;&#20256;&#36882;&#21333;&#32431;&#32593;&#32476;(EMPSNs)&#65292;&#36825;&#26159;&#19968;&#31181;&#23398;&#20064;&#22312;&#20960;&#20309;&#22270;&#24418;&#21644;&#28857;&#20113;&#19978;&#30340;&#26041;&#27861;&#65292;&#20854;&#31561;&#21464;&#20110;&#26059;&#36716;&#12289;&#24179;&#31227;&#21644;&#21453;&#23556;&#12290;EMPSNs&#21487;&#20197;&#23398;&#20064;&#22312;&#22270;&#24418;&#20013;&#30340;&#39640;&#32500;&#21333;&#32431;&#38754;&#65288;&#22914;&#19977;&#35282;&#24418;&#65289;&#65292;&#24182;&#20197;$\mathrm{E}(n)$&#31561;&#21464;&#26041;&#24335;&#21033;&#29992;&#26356;&#39640;&#32500;&#21333;&#32431;&#20307;&#30340;&#20960;&#20309;&#20449;&#24687;&#12290;EMPSNs&#21516;&#26102;&#23558;$\mathrm{E}(n)$&#31561;&#21464;&#22270;&#31070;&#32463;&#32593;&#32476;&#25512;&#24191;&#21040;&#26356;&#21152;&#22797;&#26434;&#30340;&#25299;&#25169;&#32467;&#26500;&#39046;&#22495;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#22312;&#28040;&#24687;&#20256;&#36882;&#21333;&#32431;&#32593;&#32476;&#20013;&#21253;&#21547;&#20960;&#20309;&#20449;&#24687;&#30340;&#26041;&#27861;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;EMPSNs&#21487;&#20197;&#21033;&#29992;&#20004;&#31181;&#26041;&#27861;&#30340;&#20248;&#21183;&#65292;&#30456;&#36739;&#20110;&#21333;&#29420;&#20351;&#29992;&#20854;&#20013;&#19968;&#31181;&#26041;&#27861;&#65292;&#24615;&#33021;&#26377;&#20102;&#26222;&#36941;&#25552;&#39640;&#12290;&#27492;&#22806;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#39640;&#32500;&#25805;&#20316;&#20013;&#65292;&#21253;&#21547;&#20960;&#20309;&#20449;&#24687;&#26159;&#38450;&#27490;&#28040;&#24687;&#20256;&#36882;&#32593;&#32476;&#36807;&#24230;&#24179;&#28369;&#30340;&#26377;&#25928;&#25514;&#26045;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents $\mathrm{E}(n)$ Equivariant Message Passing Simplicial Networks (EMPSNs), a novel approach to learning on geometric graphs and point clouds that is equivariant to rotations, translations, and reflections. EMPSNs can learn high-dimensional simplex features in graphs (e.g. triangles), and use the increase of geometric information of higher-dimensional simplices in an $\mathrm{E}(n)$ equivariant fashion. EMPSNs simultaneously generalize $\mathrm{E}(n)$ Equivariant Graph Neural Networks to a topologically more elaborate counterpart and provide an approach for including geometric information in Message Passing Simplicial Networks. The results indicate that EMPSNs can leverage the benefits of both approaches, leading to a general increase in performance when compared to either method. Furthermore, the results suggest that incorporating geometric information serves as an effective measure against over-smoothing in message passing networks, especially when operating on high
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#26426;&#22120;&#36951;&#24536;&#30340;&#29616;&#29366;&#21644;&#25216;&#26415;&#24212;&#29992;&#65292;&#21253;&#25324;&#25968;&#25454;&#21024;&#38500;&#12289;&#25200;&#21160;&#21644;&#27169;&#22411;&#26356;&#26032;&#65292;&#35752;&#35770;&#20102;MU&#22312;&#38544;&#31169;&#12289;&#23433;&#20840;&#21644;&#20844;&#27491;&#24615;&#31561;&#39046;&#22495;&#30340;&#28508;&#22312;&#30410;&#22788;&#65292;&#20197;&#21450;&#23427;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2305.06360</link><description>&lt;p&gt;
&#25506;&#32034;&#26426;&#22120;&#36951;&#24536;&#30340;&#39046;&#22495;&#65306;&#19968;&#31687;&#32508;&#36848;&#19982;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Exploring the Landscape of Machine Unlearning: A Survey and Taxonomy. (arXiv:2305.06360v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06360
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#26426;&#22120;&#36951;&#24536;&#30340;&#29616;&#29366;&#21644;&#25216;&#26415;&#24212;&#29992;&#65292;&#21253;&#25324;&#25968;&#25454;&#21024;&#38500;&#12289;&#25200;&#21160;&#21644;&#27169;&#22411;&#26356;&#26032;&#65292;&#35752;&#35770;&#20102;MU&#22312;&#38544;&#31169;&#12289;&#23433;&#20840;&#21644;&#20844;&#27491;&#24615;&#31561;&#39046;&#22495;&#30340;&#28508;&#22312;&#30410;&#22788;&#65292;&#20197;&#21450;&#23427;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#36951;&#24536;&#26159;&#19968;&#20010;&#36234;&#26469;&#36234;&#21463;&#20851;&#27880;&#30340;&#39046;&#22495;&#65292;&#22240;&#20026;&#38656;&#35201;&#21024;&#38500;&#25110;&#20462;&#25913;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25152;&#20570;&#20986;&#30340;&#39044;&#27979;&#12290;&#34429;&#28982;&#35757;&#32451;&#27169;&#22411;&#21464;&#24471;&#26356;&#21152;&#26377;&#25928;&#21644;&#20934;&#30830;&#65292;&#20294;&#22312;&#26576;&#20123;&#39046;&#22495;&#65288;&#22914;&#38544;&#31169;&#12289;&#23433;&#20840;&#21644;&#20844;&#27491;&#24615;&#65289;&#65292;&#36951;&#24536;&#20808;&#21069;&#23398;&#21040;&#30340;&#20449;&#24687;&#30340;&#37325;&#35201;&#24615;&#21464;&#24471;&#36234;&#26469;&#36234;&#26174;&#33879;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#26426;&#22120;&#36951;&#24536;&#30340;&#32508;&#36848;&#65292;&#28085;&#30422;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#25216;&#26415;&#21644;&#26041;&#27861;&#65292;&#21253;&#25324;&#25968;&#25454;&#21024;&#38500;&#12289;&#25200;&#21160;&#21644;&#27169;&#22411;&#26356;&#26032;&#12290;&#27492;&#22806;&#65292;&#25991;&#20013;&#36824;&#20171;&#32461;&#20102;&#24120;&#29992;&#30340;&#24230;&#37327;&#26631;&#20934;&#21644;&#25968;&#25454;&#38598;&#12290;&#25991;&#31456;&#36824;&#24378;&#35843;&#20102;&#38656;&#35201;&#35299;&#20915;&#30340;&#25361;&#25112;&#65292;&#21253;&#25324;&#25915;&#20987;&#22797;&#26434;&#24615;&#12289;&#26631;&#20934;&#21270;&#12289;&#21487;&#36716;&#31227;&#24615;&#12289;&#21487;&#35299;&#37322;&#24615;&#12289;&#35757;&#32451;&#25968;&#25454;&#21644;&#36164;&#28304;&#38480;&#21046;&#12290;&#26412;&#25991;&#30340;&#36129;&#29486;&#21253;&#25324;&#35752;&#35770;MU&#30340;&#28508;&#22312;&#30410;&#22788;&#20197;&#21450;&#23427;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#26410;&#26469;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine unlearning (MU) is a field that is gaining increasing attention due to the need to remove or modify predictions made by machine learning (ML) models. While training models have become more efficient and accurate, the importance of unlearning previously learned information has become increasingly significant in fields such as privacy, security, and fairness. This paper presents a comprehensive survey of MU, covering current state-of-the-art techniques and approaches, including data deletion, perturbation, and model updates. In addition, commonly used metrics and datasets are also presented. The paper also highlights the challenges that need to be addressed, including attack sophistication, standardization, transferability, interpretability, training data, and resource constraints. The contributions of this paper include discussions about the potential benefits of MU and its future directions in Natural Language Processing, Computer vision, and Recommender Systems. Additionally, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#26631;&#27880;&#25551;&#36848;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#22312;&#38646;&#26679;&#26412;&#20998;&#31867;&#20013;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#20934;&#30830;&#29575;&#65292;&#24182;&#33021;&#26356;&#40065;&#26834;&#22320;&#22788;&#29702;&#20998;&#31867;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2305.02239</link><description>&lt;p&gt;
&#26631;&#27880;&#25551;&#36848;&#35757;&#32451;&#22312;&#38646;&#26679;&#26412;&#25991;&#26412;&#20998;&#31867;&#20013;&#30340;&#22909;&#22788;
&lt;/p&gt;
&lt;p&gt;
The Benefits of Label-Description Training for Zero-Shot Text Classification. (arXiv:2305.02239v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02239
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#26631;&#27880;&#25551;&#36848;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#22312;&#38646;&#26679;&#26412;&#20998;&#31867;&#20013;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#20934;&#30830;&#29575;&#65292;&#24182;&#33021;&#26356;&#40065;&#26834;&#22320;&#22788;&#29702;&#20998;&#31867;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#20801;&#35768;&#20174;&#35757;&#32451;&#25968;&#25454;&#20013;&#36716;&#31227;&#35821;&#20041;&#30693;&#35782;&#65292;&#25552;&#39640;&#20102;&#38646;&#26679;&#26412;&#25991;&#26412;&#20998;&#31867;&#30340;&#24615;&#33021;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#38646;&#26679;&#26412;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#31574;&#21010;&#20102;&#19968;&#20010;&#23567;&#30340;&#24494;&#35843;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#25551;&#36848;&#20219;&#21153;&#26631;&#31614;&#12290;&#19982;&#36890;&#24120;&#26377;&#25991;&#26412;&#26631;&#27880;&#26631;&#31614;&#30340;&#24494;&#35843;&#25968;&#25454;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#25968;&#25454;&#21482;&#26159;&#29992;&#35821;&#35328;&#25551;&#36848;&#26631;&#31614;&#65292;&#20363;&#22914;&#20351;&#29992;&#19968;&#20123;&#30456;&#20851;&#26415;&#35821;&#12289;&#35789;&#20856;/&#30334;&#31185;&#20840;&#20070;&#26465;&#30446;&#21644;&#30701;&#27169;&#26495;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21508;&#31181;&#20027;&#39064;&#21644;&#24773;&#24863;&#25968;&#25454;&#38598;&#19978;&#30340;&#20934;&#30830;&#24615;&#27604;&#38646;&#26679;&#26412;&#39640;15-17&#65285;&#32477;&#23545;&#20540;&#12290;&#23427;&#36824;&#26356;&#20855;&#26377;&#38646;&#26679;&#26412;&#20998;&#31867;&#25152;&#38656;&#36873;&#25321;&#30340;&#40065;&#26834;&#24615;&#65292;&#20363;&#22914;&#25552;&#31034;&#27169;&#22411;&#36827;&#34892;&#20998;&#31867;&#30340;&#27169;&#24335;&#20197;&#21450;&#20174;&#26631;&#31614;&#26144;&#23556;&#21040;&#27169;&#22411;&#35789;&#27719;&#34920;&#20013;&#30340;&#20196;&#29260;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#25105;&#20204;&#30340;&#25968;&#25454;&#20165;&#25551;&#36848;&#26631;&#31614;&#20294;&#19981;&#20351;&#29992;&#36755;&#20837;&#25991;&#26412;&#65292;&#22240;&#27492;&#22312;&#20854;&#19978;&#24494;&#35843;&#30340;&#27169;&#22411;&#21487;&#20197;&#23558;&#20998;&#31867;&#30340;&#37325;&#28857;&#26356;&#19987;&#27880;&#20110;&#26631;&#31614;&#32780;&#19981;&#26159;&#25991;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models have improved zero-shot text classification by allowing the transfer of semantic knowledge from the training data in order to classify among specific label sets in downstream tasks. We propose a simple way to further improve zero-shot accuracies with minimal effort. We curate small finetuning datasets intended to describe the labels for a task. Unlike typical finetuning data, which has texts annotated with labels, our data simply describes the labels in language, e.g., using a few related terms, dictionary/encyclopedia entries, and short templates. Across a range of topic and sentiment datasets, our method is more accurate than zero-shot by 15-17% absolute. It is also more robust to choices required for zero-shot classification, such as patterns for prompting the model to classify and mappings from labels to tokens in the model's vocabulary. Furthermore, since our data merely describes the labels but does not use input texts, finetuning on it yields a model that p
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;PeerSum&#29992;&#20110;&#29983;&#25104;&#31185;&#23398;&#35770;&#25991;&#30340;&#20803;&#35780;&#35770;&#65292;&#28304;&#25991;&#26723;&#20855;&#26377;&#26174;&#24335;&#23618;&#27425;&#32467;&#26500;&#30340;&#20016;&#23500;&#25991;&#26723;&#38388;&#20851;&#31995;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20803;&#35780;&#35770;&#29983;&#25104;&#30340;&#20851;&#31995;&#24863;&#30693;&#22810;&#20219;&#21153;&#27169;&#22411;Rammer&#12290;</title><link>http://arxiv.org/abs/2305.01498</link><description>&lt;p&gt;
&#26088;&#22312;&#24635;&#32467;&#24102;&#26377;&#23618;&#27425;&#20851;&#31995;&#30340;&#22810;&#31687;&#25991;&#26723;
&lt;/p&gt;
&lt;p&gt;
Towards Summarizing Multiple Documents with Hierarchical Relationships. (arXiv:2305.01498v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01498
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;PeerSum&#29992;&#20110;&#29983;&#25104;&#31185;&#23398;&#35770;&#25991;&#30340;&#20803;&#35780;&#35770;&#65292;&#28304;&#25991;&#26723;&#20855;&#26377;&#26174;&#24335;&#23618;&#27425;&#32467;&#26500;&#30340;&#20016;&#23500;&#25991;&#26723;&#38388;&#20851;&#31995;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20803;&#35780;&#35770;&#29983;&#25104;&#30340;&#20851;&#31995;&#24863;&#30693;&#22810;&#20219;&#21153;&#27169;&#22411;Rammer&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#25968;&#29616;&#23384;&#30340;&#22810;&#25991;&#26723;&#25688;&#35201;(MDS)&#25968;&#25454;&#38598;&#32570;&#23569;&#20154;&#24037;&#29983;&#25104;&#30340;&#12289;&#30495;&#23454;&#30340;(&#21363;&#38750;&#21512;&#25104;&#30340;)&#25688;&#35201;&#25110;&#32773;&#24102;&#26377;&#26174;&#24335;&#25991;&#26723;&#38388;&#20851;&#31995;&#30340;&#28304;&#25991;&#26723;&#12290;&#20026;&#20102;&#22686;&#24378;MDS&#31995;&#32479;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;PeerSum&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#29983;&#25104;&#31185;&#23398;&#35770;&#25991;&#30340;&#20803;&#35780;&#35770;&#65292;&#20854;&#20013;&#20803;&#35780;&#35770;&#26159;&#23545;&#35780;&#35770;&#21644;&#30456;&#24212;&#35752;&#35770;&#30340;&#39640;&#24230;&#27010;&#25324;&#19988;&#30495;&#23454;&#30340;&#25688;&#35201;&#12290;&#36825;&#20123;&#28304;&#25991;&#26723;&#20855;&#26377;&#26174;&#24335;&#23618;&#27425;&#32467;&#26500;&#30340;&#20016;&#23500;&#25991;&#26723;&#38388;&#20851;&#31995;&#65292;&#21253;&#25324;&#20132;&#21449;&#24341;&#29992;&#21644;&#32463;&#24120;&#20986;&#29616;&#30340;&#20914;&#31361;&#12290;&#37492;&#20110;&#24456;&#23569;&#26377;&#30740;&#31350;&#37319;&#29992;&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#27880;&#24847;&#21147;&#25805;&#32437;&#26469;&#23558;&#23618;&#27425;&#20851;&#31995;&#32435;&#20837;MDS&#31995;&#32479;&#20013;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;Rammer(&#20851;&#31995;&#24863;&#30693;&#22810;&#20219;&#21153;&#20803;&#35780;&#35770;&#29983;&#25104;&#22120;)&#65292;&#36825;&#26159;&#19968;&#31181;&#20803;&#35780;&#35770;&#29983;&#25104;&#27169;&#22411;&#65292;&#20351;&#29992;&#22522;&#20110;&#23618;&#27425;&#20851;&#31995;&#30340;&#31232;&#30095;&#27880;&#24847;&#21147;&#21644;&#22810;&#20219;&#21153;&#30446;&#26631;&#65292;&#21487;&#20197;&#39044;&#27979;&#22810;&#20010;&#24230;&#37327;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most existing multi-document summarization (MDS) datasets lack human-generated and genuine (i.e., not synthetic) summaries or source documents with explicit inter-document relationships that a summary must capture. To enhance the capabilities of MDS systems we present PeerSum, a novel dataset for generating meta-reviews of scientific papers, where the meta-reviews are highly abstractive and genuine summaries of reviews and corresponding discussions. These source documents have rich inter-document relationships of an explicit hierarchical structure with cross-references and often feature conflicts. As there is a scarcity of research that incorporates hierarchical relationships into MDS systems through attention manipulation on pre-trained language models, we additionally present Rammer (Relationship-aware Multi-task Meta-review Generator), a meta-review generation model that uses sparse attention based on the hierarchical relationships and a multi-task objective that predicts several me
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#26377;&#25928;&#30340;&#8220;ProAttack&#8221;&#26041;&#27861;&#26469;&#25191;&#34892;&#24178;&#20928;&#26631;&#31614;&#30340;&#21518;&#38376;&#25915;&#20987;&#65292;&#20351;&#29992;&#30340;&#26159;&#25552;&#31034;&#26412;&#36523;&#20316;&#20026;&#35302;&#21457;&#22120;&#12290;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#22806;&#37096;&#35302;&#21457;&#22120;&#65292;&#24182;&#30830;&#20445;&#27602;&#30244;&#25968;&#25454;&#30340;&#26631;&#27880;&#27491;&#30830;&#65292;&#25552;&#39640;&#20102;&#21518;&#38376;&#25915;&#20987;&#30340;&#38544;&#34109;&#24615;&#65292;&#30456;&#27604;&#20110;&#29616;&#26377;&#30340;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;&#26377;&#26174;&#33879;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2305.01219</link><description>&lt;p&gt;
&#35302;&#21457;&#35789;&#20316;&#20026;&#21518;&#38376;&#25915;&#20987;&#30340;&#35302;&#21457;&#22120;&#65306;&#26816;&#26597;&#35821;&#35328;&#27169;&#22411;&#30340;&#33030;&#24369;&#24615;
&lt;/p&gt;
&lt;p&gt;
Prompt as Triggers for Backdoor Attack: Examining the Vulnerability in Language Models. (arXiv:2305.01219v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01219
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#26377;&#25928;&#30340;&#8220;ProAttack&#8221;&#26041;&#27861;&#26469;&#25191;&#34892;&#24178;&#20928;&#26631;&#31614;&#30340;&#21518;&#38376;&#25915;&#20987;&#65292;&#20351;&#29992;&#30340;&#26159;&#25552;&#31034;&#26412;&#36523;&#20316;&#20026;&#35302;&#21457;&#22120;&#12290;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#22806;&#37096;&#35302;&#21457;&#22120;&#65292;&#24182;&#30830;&#20445;&#27602;&#30244;&#25968;&#25454;&#30340;&#26631;&#27880;&#27491;&#30830;&#65292;&#25552;&#39640;&#20102;&#21518;&#38376;&#25915;&#20987;&#30340;&#38544;&#34109;&#24615;&#65292;&#30456;&#27604;&#20110;&#29616;&#26377;&#30340;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;&#26377;&#26174;&#33879;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25552;&#31034;&#30340;&#23398;&#20064;&#33539;&#20363;&#24357;&#21512;&#20102;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#22312;&#20960;&#20010;NLP&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#26159;&#22312;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#12290;&#23613;&#31649;&#24212;&#29992;&#24191;&#27867;&#65292;&#20294;&#22522;&#20110;&#25552;&#31034;&#30340;&#23398;&#20064;&#23481;&#26131;&#21463;&#21040;&#21518;&#38376;&#25915;&#20987;&#12290;&#25991;&#26412;&#21518;&#38376;&#25915;&#20987;&#26088;&#22312;&#36890;&#36807;&#27880;&#20837;&#35302;&#21457;&#22120;&#24182;&#20462;&#25913;&#26631;&#31614;&#26469;&#22312;&#27169;&#22411;&#20013;&#24341;&#20837;&#26377;&#38024;&#23545;&#24615;&#30340;&#28431;&#27934;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#35302;&#21457;&#22120;&#30340;&#23384;&#22312;&#21644;&#27602;&#30244;&#25968;&#25454;&#26631;&#27880;&#19981;&#27491;&#30830;&#31561;&#32570;&#38519;&#65292;&#36825;&#31181;&#25915;&#20987;&#23384;&#22312;&#24322;&#24120;&#30340;&#33258;&#28982;&#35821;&#35328;&#34920;&#36798;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#26377;&#25928;&#30340;&#8220;ProAttack&#8221;&#26041;&#27861;&#65292;&#22522;&#20110;&#25552;&#31034;&#26469;&#25191;&#34892;&#24178;&#20928;&#26631;&#31614;&#30340;&#21518;&#38376;&#25915;&#20987;&#65292;&#20351;&#29992;&#30340;&#26159;&#25552;&#31034;&#26412;&#36523;&#20316;&#20026;&#35302;&#21457;&#22120;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#38656;&#35201;&#22806;&#37096;&#35302;&#21457;&#22120;&#65292;&#24182;&#30830;&#20445;&#27602;&#30244;&#25968;&#25454;&#30340;&#26631;&#27880;&#27491;&#30830;&#65292;&#25552;&#39640;&#20102;&#21518;&#38376;&#25915;&#20987;&#30340;&#38544;&#34109;&#24615;&#12290;&#36890;&#36807;&#22312;&#20016;&#23500;&#30340;&#36164;&#28304;&#21644;&#23569;&#26679;&#26412;&#25991;&#26412;&#35821;&#26009;&#24211;&#19978;&#30340;&#24191;&#27867;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;ProAttack&#26041;&#27861;&#22312;&#20445;&#25345;&#24178;&#20928;&#25968;&#25454;&#19968;&#33268;&#24615;&#30340;&#21516;&#26102;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;&#21518;&#38376;&#25915;&#20987;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
The prompt-based learning paradigm, which bridges the gap between pre-training and fine-tuning, achieves state-of-the-art performance on several NLP tasks, particularly in few-shot settings. Despite being widely applied, prompt-based learning is vulnerable to backdoor attacks. Textual backdoor attacks are designed to introduce targeted vulnerabilities into models by poisoning a subset of training samples through trigger injection and label modification. However, they suffer from flaws such as abnormal natural language expressions resulting from the trigger and incorrect labeling of poisoned samples. In this study, we propose {\bf ProAttack}, a novel and efficient method for performing clean-label backdoor attacks based on the prompt, which uses the prompt itself as a trigger. Our method does not require external triggers and ensures correct labeling of poisoned samples, improving the stealthy nature of the backdoor attack. With extensive experiments on rich-resource and few-shot text c
&lt;/p&gt;</description></item><item><title>LEA&#26159;&#19968;&#31181;&#36866;&#24212;&#24615;&#24378;&#19988;&#33021;&#22815;&#26377;&#25928;&#21033;&#29992;&#30446;&#26631;&#20219;&#21153;&#20302;&#20445;&#30495;&#24230;&#20449;&#24687;&#30340;&#23398;&#20064;&#36827;&#21270;&#31639;&#27861;&#65292;&#20174;&#32780;&#27604;&#20256;&#32479;&#36827;&#21270;&#31639;&#27861;&#22312;&#26356;&#23569;&#30340;&#35745;&#31639;&#25104;&#26412;&#19979;&#33719;&#24471;&#26356;&#22909;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2304.09599</link><description>&lt;p&gt;
LEA: &#23398;&#20064;&#20248;&#21270;&#31574;&#30053;&#30340;&#36229;&#36234;&#36827;&#21270;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
LEA: Beyond Evolutionary Algorithms via Learned Optimization Strategy. (arXiv:2304.09599v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09599
&lt;/p&gt;
&lt;p&gt;
LEA&#26159;&#19968;&#31181;&#36866;&#24212;&#24615;&#24378;&#19988;&#33021;&#22815;&#26377;&#25928;&#21033;&#29992;&#30446;&#26631;&#20219;&#21153;&#20302;&#20445;&#30495;&#24230;&#20449;&#24687;&#30340;&#23398;&#20064;&#36827;&#21270;&#31639;&#27861;&#65292;&#20174;&#32780;&#27604;&#20256;&#32479;&#36827;&#21270;&#31639;&#27861;&#22312;&#26356;&#23569;&#30340;&#35745;&#31639;&#25104;&#26412;&#19979;&#33719;&#24471;&#26356;&#22909;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36827;&#21270;&#31639;&#27861;&#24050;&#25104;&#20026;&#26114;&#36149;&#40657;&#30418;&#20248;&#21270;&#30340;&#24378;&#22823;&#26694;&#26550;&#12290;&#22312;&#26356;&#23569;&#30340;&#35745;&#31639;&#25104;&#26412;&#19979;&#33719;&#24471;&#26356;&#22909;&#30340;&#35299;&#20915;&#26041;&#26696;&#23545;&#20110;&#40657;&#30418;&#20248;&#21270;&#33267;&#20851;&#37325;&#35201;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26368;&#20851;&#38190;&#30340;&#38556;&#30861;&#26159;&#25214;&#20986;&#22914;&#20309;&#26377;&#25928;&#21033;&#29992;&#30446;&#26631;&#20219;&#21153;&#20449;&#24687;&#26469;&#24418;&#25104;&#39640;&#25928;&#30340;&#20248;&#21270;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#26041;&#27861;&#30001;&#20110;&#20248;&#21270;&#31574;&#30053;&#30340;&#34920;&#24449;&#19981;&#36275;&#20197;&#21450;&#20248;&#21270;&#31574;&#30053;&#19982;&#30446;&#26631;&#20219;&#21153;&#20043;&#38388;&#30340;&#20302;&#25928;&#20132;&#20114;&#32780;&#26174;&#24471;&#34180;&#24369;&#12290;&#20026;&#20102;&#20811;&#26381;&#19978;&#36848;&#38480;&#21046;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#23398;&#20064;&#36827;&#21270;&#31639;&#27861;&#65288;LEA&#65289;&#65292;&#20197;&#23454;&#29616;&#20174;&#25163;&#21160;&#35774;&#35745;&#30340;&#20248;&#21270;&#31574;&#30053;&#21040;&#23398;&#20064;&#20248;&#21270;&#31574;&#30053;&#30340;&#36716;&#25442;&#65292;&#20854;&#20013;&#21253;&#25324;&#36229;&#21442;&#25968;&#21644;&#26356;&#26032;&#35268;&#21017;&#12290;&#19982;&#20256;&#32479;&#36827;&#21270;&#31639;&#27861;&#19981;&#21516;&#65292;LEA&#23545;&#30446;&#26631;&#20219;&#21153;&#20855;&#26377;&#39640;&#36866;&#24212;&#24615;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#26356;&#23569;&#30340;&#35745;&#31639;&#25104;&#26412;&#19979;&#33719;&#24471;&#26356;&#22909;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;LEA&#36824;&#33021;&#22815;&#26377;&#25928;&#22320;&#21033;&#29992;&#30446;&#26631;&#20219;&#21153;&#30340;&#20302;&#20445;&#30495;&#24230;&#20449;&#24687;&#26469;&#24418;&#25104;&#39640;&#25928;&#30340;&#20248;&#21270;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Evolutionary algorithms (EAs) have emerged as a powerful framework for expensive black-box optimization. Obtaining better solutions with less computational cost is essential and challenging for black-box optimization. The most critical obstacle is figuring out how to effectively use the target task information to form an efficient optimization strategy. However, current methods are weak due to the poor representation of the optimization strategy and the inefficient interaction between the optimization strategy and the target task. To overcome the above limitations, we design a learned EA (LEA) to realize the move from hand-designed optimization strategies to learned optimization strategies, including not only hyperparameters but also update rules. Unlike traditional EAs, LEA has high adaptability to the target task and can obtain better solutions with less computational cost. LEA is also able to effectively utilize the low-fidelity information of the target task to form an efficient op
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#30340;&#21452;&#37325;&#20351;&#29992;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20221;&#23450;&#21046;&#30340;&#21452;&#37325;&#20351;&#29992;&#23450;&#20041;&#65292;&#24182;&#35752;&#35770;&#20102;&#24403;&#21069;&#30340;&#29366;&#20917;&#21644;&#21487;&#33021;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2304.08315</link><description>&lt;p&gt;
&#33606;&#26840;&#29611;&#29808;&#65306;&#25506;&#31350;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#21452;&#37325;&#20351;&#29992;&#22256;&#22659;
&lt;/p&gt;
&lt;p&gt;
Thorny Roses: Investigating the Dual Use Dilemma in Natural Language Processing. (arXiv:2304.08315v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08315
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#30340;&#21452;&#37325;&#20351;&#29992;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20221;&#23450;&#21046;&#30340;&#21452;&#37325;&#20351;&#29992;&#23450;&#20041;&#65292;&#24182;&#35752;&#35770;&#20102;&#24403;&#21069;&#30340;&#29366;&#20917;&#21644;&#21487;&#33021;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21452;&#37325;&#20351;&#29992;&#26159;&#25351;&#26377;&#24847;&#23558;&#25216;&#26415;&#21644;&#31185;&#23398;&#25104;&#26524;&#29992;&#20110;&#26377;&#23475;&#30446;&#30340;&#30340;&#38382;&#39064;&#65292;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#23578;&#26410;&#26126;&#30830;&#23450;&#20041;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;NLP&#25216;&#26415;&#30340;&#19981;&#26029;&#21457;&#23637;&#21644;&#22312;&#31038;&#20250;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#65292;&#20854;&#20869;&#37096;&#36816;&#34892;&#26041;&#24335;&#21464;&#24471;&#36234;&#26469;&#36234;&#19981;&#36879;&#26126;&#12290;&#22240;&#27492;&#65292;&#29702;&#35299;&#21452;&#37325;&#20351;&#29992;&#30340;&#38382;&#39064;&#20197;&#21450;&#38480;&#21046;&#21452;&#37325;&#20351;&#29992;&#30340;&#28508;&#22312;&#26041;&#27861;&#23545;&#20110;&#20943;&#23569;&#30740;&#31350;&#21644;&#24320;&#21457;&#30340;&#28508;&#22312;&#21361;&#23475;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;NLP&#30740;&#31350;&#20154;&#21592;&#21644;&#20174;&#19994;&#32773;&#36827;&#34892;&#20102;&#35843;&#26597;&#65292;&#20197;&#20102;&#35299;&#20182;&#20204;&#23545;&#35813;&#38382;&#39064;&#30340;&#28145;&#24230;&#29702;&#35299;&#21644;&#35266;&#28857;&#65292;&#24182;&#35780;&#20272;&#29616;&#26377;&#30340;&#25903;&#25345;&#24773;&#20917;&#12290;&#26681;&#25454;&#35843;&#26597;&#32467;&#26524;&#65292;&#25105;&#20204;&#20026;NLP&#31038;&#21306;&#25552;&#20379;&#20102;&#19968;&#20221;&#23450;&#21046;&#30340;&#21452;&#37325;&#20351;&#29992;&#23450;&#20041;&#12290;&#35843;&#26597;&#32467;&#26524;&#26174;&#31034;&#65292;&#22823;&#22810;&#25968;&#30740;&#31350;&#20154;&#21592;&#23545;&#20182;&#20204;&#30340;&#30740;&#31350;&#30340;&#28508;&#22312;&#21452;&#37325;&#20351;&#29992;&#38382;&#39064;&#34920;&#31034;&#20851;&#20999;&#65292;&#20294;&#21482;&#37319;&#21462;&#26377;&#38480;&#30340;&#34892;&#21160;&#12290;&#22522;&#20110;&#35843;&#26597;&#32467;&#26524;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#24403;&#21069;&#30340;&#29366;&#20917;&#21644;&#21487;&#33021;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dual use, the intentional, harmful reuse of technology and scientific artefacts, is a problem yet to be well-defined within the context of Natural Language Processing (NLP). However, as NLP technologies continue to advance and become increasingly widespread in society, their inner workings have become increasingly opaque. Therefore, understanding dual use concerns and potential ways of limiting them is critical to minimising the potential harms of research and development. In this paper, we conduct a survey of NLP researchers and practitioners to understand the depth and their perspective of the problem as well as to assess existing available support. Based on the results of our survey, we offer a definition of dual use that is tailored to the needs of the NLP community. The survey revealed that a majority of researchers are concerned about the potential dual use of their research but only take limited action toward it. In light of the survey results, we discuss the current state and p
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#36807;&#31243;&#27169;&#22411;&#65292;&#21363;&#22810;&#27169;&#24577;&#31070;&#32463;&#36807;&#31243;&#65292;&#29992;&#20110;&#23545;&#22810;&#27169;&#24577;&#25968;&#25454;&#36827;&#34892;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65292;&#35813;&#27169;&#22411;&#20855;&#26377;&#21160;&#24577;&#19978;&#19979;&#25991;&#35760;&#24518;&#12289;&#22810;&#27169;&#24577;&#36125;&#21494;&#26031;&#32858;&#21512;&#21644;&#26657;&#20934;&#39044;&#27979;&#30340;&#27880;&#24847;&#26426;&#21046;&#65292;&#32463;&#23454;&#39564;&#34920;&#26126;&#22312;&#22810;&#27169;&#24577;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#38754;&#24615;&#33021;&#26368;&#20808;&#36827;&#65292;&#23545;&#20110;&#22122;&#22768;&#26679;&#26412;&#20855;&#26377;&#33391;&#22909;&#25269;&#25239;&#33021;&#21147;&#65292;&#24182;&#19988;&#23545;&#20110;&#39046;&#22495;&#20043;&#22806;&#30340;&#26816;&#27979;&#26159;&#21487;&#38752;&#30340;&#12290;</title><link>http://arxiv.org/abs/2304.01518</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#31070;&#32463;&#36807;&#31243;&#29992;&#20110;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Multimodal Neural Processes for Uncertainty Estimation. (arXiv:2304.01518v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01518
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#36807;&#31243;&#27169;&#22411;&#65292;&#21363;&#22810;&#27169;&#24577;&#31070;&#32463;&#36807;&#31243;&#65292;&#29992;&#20110;&#23545;&#22810;&#27169;&#24577;&#25968;&#25454;&#36827;&#34892;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65292;&#35813;&#27169;&#22411;&#20855;&#26377;&#21160;&#24577;&#19978;&#19979;&#25991;&#35760;&#24518;&#12289;&#22810;&#27169;&#24577;&#36125;&#21494;&#26031;&#32858;&#21512;&#21644;&#26657;&#20934;&#39044;&#27979;&#30340;&#27880;&#24847;&#26426;&#21046;&#65292;&#32463;&#23454;&#39564;&#34920;&#26126;&#22312;&#22810;&#27169;&#24577;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#38754;&#24615;&#33021;&#26368;&#20808;&#36827;&#65292;&#23545;&#20110;&#22122;&#22768;&#26679;&#26412;&#20855;&#26377;&#33391;&#22909;&#25269;&#25239;&#33021;&#21147;&#65292;&#24182;&#19988;&#23545;&#20110;&#39046;&#22495;&#20043;&#22806;&#30340;&#26816;&#27979;&#26159;&#21487;&#38752;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#36807;&#31243;( Neural Processes, NPs)&#23558;&#21442;&#25968;&#21270;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#31034;&#33021;&#21147;&#21644;&#38750;&#21442;&#25968;&#39640;&#26031;&#36807;&#31243;&#21487;&#38752;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#32467;&#21512;&#22312;&#20102;&#19968;&#36215;&#12290;&#34429;&#28982;&#26368;&#36817;NPs&#30340;&#21457;&#23637;&#24050;&#32463;&#22312;&#22238;&#24402;&#21644;&#20998;&#31867;&#26041;&#38754;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#26159;&#22914;&#20309;&#23558;NPs&#36866;&#24212;&#22810;&#27169;&#24577;&#25968;&#25454;&#23578;&#26410;&#21463;&#21040;&#20180;&#32454;&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#39318;&#27425;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;NP&#23478;&#26063;&#27169;&#22411;&#65292;&#29992;&#20110;&#22810;&#27169;&#24577;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65292;&#21363;&#22810;&#27169;&#24577;&#31070;&#32463;&#36807;&#31243;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#31181;&#25972;&#20307;&#30340;&#12289;&#22522;&#20110;&#21407;&#21017;&#30340;&#26041;&#27861;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#30001;&#20998;&#31867;&#35823;&#24046;&#26356;&#26032;&#30340;&#21160;&#24577;&#19978;&#19979;&#25991;&#35760;&#24518;&#65292;&#19968;&#20010;&#32858;&#21512;&#22810;&#27169;&#24577;&#34920;&#31034;&#30340;&#22810;&#27169;&#24577;&#36125;&#21494;&#26031;&#32858;&#21512;&#26426;&#21046;&#65292;&#20197;&#21450;&#19968;&#20010;&#29992;&#20110;&#26657;&#20934;&#39044;&#27979;&#30340;&#26032;&#30340;&#27880;&#24847;&#26426;&#21046;&#12290;&#22312;&#24191;&#27867;&#30340;&#32463;&#39564;&#35780;&#20272;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#22810;&#27169;&#24577;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#24615;&#33021;&#65292;&#23637;&#31034;&#20102;&#23427;&#30340;&#21560;&#24341;&#21147;&#65292;&#21363;&#33021;&#22815;&#25269;&#25239;&#22122;&#22768;&#26679;&#26412;&#30340;&#24178;&#25200;&#65292;&#24182;&#21487;&#38752;&#22320;&#22312;&#39046;&#22495;&#20043;&#22806;&#36827;&#34892;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural processes (NPs) have brought the representation power of parametric deep neural networks and the reliable uncertainty estimation of non-parametric Gaussian processes together. Although recent development of NPs has shown success in both regression and classification, how to adapt NPs to multimodal data has not be carefully studied. For the first time, we propose a new model of NP family for multimodal uncertainty estimation, namely Multimodal Neural Processes. In a holistic and principled way, we develop a dynamic context memory updated by the classification error, a multimodal Bayesian aggregation mechanism to aggregate multimodal representations, and a new attention mechanism for calibrated predictions. In extensive empirical evaluation, our method achieves the state-of-the-art multimodal uncertainty estimation performance, showing its appealing ability of being robust against noisy samples and reliable in out-of-domain detection.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38543;&#26426;&#25237;&#24433;&#26862;&#26519;&#65288;rpForest&#65289;&#30340;&#22270;&#26500;&#36896;&#21644;&#21021;&#22987;&#21270;&#26041;&#24335;&#65292;&#30456;&#27604;&#20110;&#20256;&#32479;&#26041;&#27861;&#65292;&#20351;&#29992;rpForest&#21021;&#22987;&#21270;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCN&#65289;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2302.12001</link><description>&lt;p&gt;
&#38543;&#26426;&#25237;&#24433;&#26862;&#26519;&#21021;&#22987;&#21270;&#22270;&#21367;&#31215;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Random Projection Forest Initialization for Graph Convolutional Networks. (arXiv:2302.12001v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.12001
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38543;&#26426;&#25237;&#24433;&#26862;&#26519;&#65288;rpForest&#65289;&#30340;&#22270;&#26500;&#36896;&#21644;&#21021;&#22987;&#21270;&#26041;&#24335;&#65292;&#30456;&#27604;&#20110;&#20256;&#32479;&#26041;&#27861;&#65292;&#20351;&#29992;rpForest&#21021;&#22987;&#21270;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCN&#65289;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCNs&#65289;&#26159;&#23558;&#28145;&#24230;&#23398;&#20064;&#25193;&#23637;&#21040;&#26080;&#32467;&#26500;&#25968;&#25454;&#65288;&#22914;&#22270;&#65289;&#30340;&#19968;&#22823;&#27493;&#12290;&#20294;GCNs&#20173;&#38656;&#35201;&#26500;&#36896;&#22270;&#26469;&#36827;&#34892;&#24037;&#20316;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#36890;&#24120;&#20351;&#29992;&#32463;&#20856;&#22270;&#65288;&#22914;k&#36817;&#37051;&#22270;&#65289;&#26469;&#21021;&#22987;&#21270;GCN&#12290;&#23613;&#31649;&#26500;&#36896;k&#36817;&#37051;&#22270;&#30340;&#35745;&#31639;&#25928;&#29575;&#24456;&#39640;&#65292;&#20294;&#26500;&#36896;&#30340;&#22270;&#23545;&#20110;&#23398;&#20064;&#21487;&#33021;&#27809;&#26377;&#22826;&#22823;&#30340;&#29992;&#22788;&#12290;&#22312;k&#36817;&#37051;&#22270;&#20013;&#65292;&#28857;&#34987;&#38480;&#21046;&#20026;&#20855;&#26377;&#22266;&#23450;&#25968;&#37327;&#30340;&#36793;&#65292;&#22270;&#20013;&#30340;&#25152;&#26377;&#36793;&#37117;&#20855;&#26377;&#30456;&#31561;&#30340;&#26435;&#37325;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#24335;&#26469;&#26500;&#24314;&#22270;&#24182;&#21021;&#22987;&#21270;GCN&#12290;&#23427;&#22522;&#20110;&#38543;&#26426;&#25237;&#24433;&#26862;&#26519;&#65288;rpForest&#65289;&#12290;rpForest&#20351;&#25105;&#20204;&#33021;&#22815;&#36171;&#20104;&#36793;&#19981;&#21516;&#30340;&#26435;&#37325;&#65292;&#34920;&#31034;&#19981;&#21516;&#30340;&#37325;&#35201;&#24615;&#65292;&#20174;&#32780;&#22686;&#24378;&#23398;&#20064;&#33021;&#21147;&#12290;&#26641;&#30340;&#25968;&#37327;&#26159;rpForest&#20013;&#30340;&#36229;&#21442;&#25968;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#35889;&#20998;&#26512;&#26469;&#24110;&#21161;&#25105;&#20204;&#35774;&#32622;&#27491;&#30830;&#33539;&#22260;&#30340;&#21442;&#25968;&#12290;&#22312;&#23454;&#39564;&#35777;&#26126;&#65292;&#20351;&#29992;rpForest&#21021;&#22987;&#21270;GCN&#30456;&#27604;&#20351;&#29992;&#20256;&#32479;&#26041;&#27861;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph convolutional networks (GCNs) were a great step towards extending deep learning to unstructured data such as graphs. But GCNs still need a constructed graph to work with. To solve this problem, classical graphs such as $k$-nearest neighbor are usually used to initialize the GCN. Although it is computationally efficient to construct $k$-nn graphs, the constructed graph might not be very useful for learning. In a $k$-nn graph, points are restricted to have a fixed number of edges, and all edges in the graph have equal weights. We present a new way to construct the graph and initialize the GCN. It is based on random projection forest (rpForest). rpForest enables us to assign varying weights on edges indicating varying importance, which enhanced the learning. The number of trees is a hyperparameter in rpForest. We performed spectral analysis to help us setting this parameter in the right range. In the experiments, initializing the GCN using rpForest provides better results compared t
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22823;&#22411;&#23454;&#39564;&#30340;&#32467;&#26524;&#65292;&#35777;&#26126;&#20102;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#21487;&#20197;&#25913;&#21892;&#20851;&#20110;&#20998;&#35010;&#24615;&#35805;&#39064;&#30340;&#22312;&#32447;&#23545;&#35805;&#12290;&#20182;&#20204;&#36890;&#36807;&#20351;&#29992;&#19968;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#26102;&#25552;&#20379;&#22522;&#20110;&#35777;&#25454;&#30340;&#24314;&#35758;&#65292;&#24110;&#21161;&#20154;&#20204;&#22312;&#23545;&#35805;&#20013;&#24863;&#21463;&#21040;&#29702;&#35299;&#30340;&#24863;&#35273;&#12290;</title><link>http://arxiv.org/abs/2302.07268</link><description>&lt;p&gt;
AI&#32842;&#22825;&#21161;&#25163;&#21487;&#25913;&#21892;&#20851;&#20110;&#20998;&#35010;&#24615;&#35805;&#39064;&#30340;&#23545;&#35805;
&lt;/p&gt;
&lt;p&gt;
AI Chat Assistants can Improve Conversations about Divisive Topics. (arXiv:2302.07268v4 [cs.HC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.07268
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22823;&#22411;&#23454;&#39564;&#30340;&#32467;&#26524;&#65292;&#35777;&#26126;&#20102;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#21487;&#20197;&#25913;&#21892;&#20851;&#20110;&#20998;&#35010;&#24615;&#35805;&#39064;&#30340;&#22312;&#32447;&#23545;&#35805;&#12290;&#20182;&#20204;&#36890;&#36807;&#20351;&#29992;&#19968;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#26102;&#25552;&#20379;&#22522;&#20110;&#35777;&#25454;&#30340;&#24314;&#35758;&#65292;&#24110;&#21161;&#20154;&#20204;&#22312;&#23545;&#35805;&#20013;&#24863;&#21463;&#21040;&#29702;&#35299;&#30340;&#24863;&#35273;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#22312;&#32447;&#20132;&#27969;&#25968;&#37327;&#27491;&#22312;&#36805;&#36895;&#22686;&#38271;&#12290;&#20294;&#26159;&#65292;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#12289;&#28040;&#24687;&#24212;&#29992;&#31243;&#24207;&#21644;&#20854;&#20182;&#25968;&#23383;&#35770;&#22363;&#19978;&#30340;&#22522;&#20110;&#25991;&#26412;&#30340;&#20114;&#21160;&#21487;&#33021;&#20250;&#20135;&#29983;&#20998;&#35010;&#21644;&#20914;&#31361;&#12290;&#36825;&#31181;&#26377;&#27602;&#24615;&#22686;&#21152;&#20102;&#26497;&#21270;&#30340;&#31243;&#24230;&#65292;&#24182;&#19988;&#37325;&#35201;&#30340;&#26159;&#65292;&#20405;&#34432;&#20102;&#22810;&#20803;&#21270;&#31038;&#20250;&#21457;&#23637;&#35299;&#20915;&#24433;&#21709;&#25152;&#26377;&#20154;&#30340;&#22797;&#26434;&#31038;&#20250;&#38382;&#39064;&#30340;&#33021;&#21147;&#12290;&#23398;&#32773;&#21644;&#27665;&#38388;&#31038;&#20250;&#32452;&#32455;&#25512;&#21160;&#24178;&#39044;&#25514;&#26045;&#65292;&#20351;&#38754;&#23545;&#38754;&#30340;&#23545;&#35805;&#19981;&#37027;&#20040;&#20855;&#26377;&#20998;&#35010;&#24615;&#25110;&#26356;&#20855;&#29983;&#20135;&#21147;&#65292;&#20294;&#23558;&#36825;&#20123;&#21162;&#21147;&#25193;&#23637;&#33267;&#22312;&#32447;&#21457;&#29983;&#30340;&#35768;&#22810;&#35805;&#35821;&#26159;&#26497;&#20855;&#25361;&#25112;&#24615;&#30340;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#23454;&#39564;&#30340;&#32467;&#26524;&#65292;&#35813;&#23454;&#39564;&#35777;&#26126;&#20102;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#22914;&#20309;&#25913;&#21892;&#20851;&#20110;&#20998;&#35010;&#24615;&#35805;&#39064;&#30340;&#22312;&#32447;&#23545;&#35805;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#37319;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#26102;&#25552;&#20379;&#22522;&#20110;&#35777;&#25454;&#30340;&#24314;&#35758;&#65292;&#20197;&#25913;&#21892;&#21442;&#19982;&#32773;&#22312;&#23545;&#35805;&#20013;&#24863;&#21463;&#21040;&#29702;&#35299;&#30340;&#24863;&#35273;&#12290;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#24314;&#35758;&#30830;&#23454;&#21487;&#20197;&#24110;&#21161;&#20154;&#20204;&#25913;&#21892;&#23545;&#35805;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
A rapidly increasing amount of human conversation occurs online. But divisiveness and conflict can fester in text-based interactions on social media platforms, in messaging apps, and on other digital forums. Such toxicity increases polarization and, importantly, corrodes the capacity of diverse societies to develop efficient solutions to complex social problems that impact everyone. Scholars and civil society groups promote interventions that can make interpersonal conversations less divisive or more productive in offline settings, but scaling these efforts to the amount of discourse that occurs online is extremely challenging. We present results of a large-scale experiment that demonstrates how online conversations about divisive topics can be improved with artificial intelligence tools. Specifically, we employ a large language model to make real-time, evidence-based recommendations intended to improve participants' perception of feeling understood in conversations. We find that these
&lt;/p&gt;</description></item><item><title>ConceptFusion&#26159;&#19968;&#31181;&#24320;&#25918;&#38598;&#21512;&#30340;&#22810;&#27169;&#24577;&#19977;&#32500;&#24314;&#22270;&#26041;&#27861;&#65292;&#33021;&#22815;&#36229;&#36234;&#23553;&#38381;&#38598;&#21512;&#30340;&#27010;&#24565;&#65292;&#21516;&#26102;&#25903;&#25345;&#20174;&#35821;&#35328;&#12289;&#22270;&#20687;&#12289;&#38899;&#39057;&#21644;&#19977;&#32500;&#20960;&#20309;&#31561;&#22810;&#31181;&#26041;&#24335;&#26597;&#35810;&#19977;&#32500;&#22320;&#22270;&#12290;</title><link>http://arxiv.org/abs/2302.07241</link><description>&lt;p&gt;
ConceptFusion&#65306;&#24320;&#25918;&#38598;&#21512;&#22810;&#27169;&#24577;&#19977;&#32500;&#24314;&#22270;
&lt;/p&gt;
&lt;p&gt;
ConceptFusion: Open-set Multimodal 3D Mapping. (arXiv:2302.07241v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.07241
&lt;/p&gt;
&lt;p&gt;
ConceptFusion&#26159;&#19968;&#31181;&#24320;&#25918;&#38598;&#21512;&#30340;&#22810;&#27169;&#24577;&#19977;&#32500;&#24314;&#22270;&#26041;&#27861;&#65292;&#33021;&#22815;&#36229;&#36234;&#23553;&#38381;&#38598;&#21512;&#30340;&#27010;&#24565;&#65292;&#21516;&#26102;&#25903;&#25345;&#20174;&#35821;&#35328;&#12289;&#22270;&#20687;&#12289;&#38899;&#39057;&#21644;&#19977;&#32500;&#20960;&#20309;&#31561;&#22810;&#31181;&#26041;&#24335;&#26597;&#35810;&#19977;&#32500;&#22320;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26500;&#24314;&#29615;&#22659;&#30340;&#19977;&#32500;&#22320;&#22270;&#23545;&#26426;&#22120;&#20154;&#23548;&#33322;&#12289;&#35268;&#21010;&#21644;&#19982;&#22330;&#26223;&#20013;&#30340;&#29289;&#20307;&#20132;&#20114;&#33267;&#20851;&#37325;&#35201;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#23558;&#35821;&#20041;&#27010;&#24565;&#19982;&#19977;&#32500;&#22320;&#22270;&#38598;&#25104;&#30340;&#26041;&#27861;&#20027;&#35201;&#23616;&#38480;&#20110;&#23553;&#38381;&#38598;&#21512;&#30340;&#35774;&#23450;&#65306;&#23427;&#20204;&#21482;&#33021;&#25512;&#29702;&#20986;&#19968;&#20010;&#22312;&#35757;&#32451;&#26102;&#39044;&#23450;&#20041;&#30340;&#26377;&#38480;&#27010;&#24565;&#38598;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#22320;&#22270;&#21482;&#33021;&#20351;&#29992;&#31867;&#21035;&#26631;&#31614;&#25110;&#26368;&#36817;&#30340;&#24037;&#20316;&#20013;&#65292;&#20351;&#29992;&#25991;&#26412;&#25552;&#31034;&#26469;&#26597;&#35810;&#12290;&#25105;&#20204;&#36890;&#36807;ConceptFusion&#35299;&#20915;&#20102;&#36825;&#20004;&#20010;&#38382;&#39064;&#65292;&#36825;&#26159;&#19968;&#31181;&#22330;&#26223;&#34920;&#31034;&#65292;&#23427;&#26082;&#26159;&#22522;&#26412;&#30340;&#24320;&#25918;&#38598;&#21512;&#65292;&#21487;&#20197;&#22312;&#23553;&#38381;&#30340;&#27010;&#24565;&#38598;&#20043;&#22806;&#36827;&#34892;&#25512;&#29702;&#65292;&#21448;&#26159;&#22266;&#26377;&#30340;&#22810;&#27169;&#24577;&#65292;&#21487;&#20197;&#22312;&#35821;&#35328;&#12289;&#22270;&#20687;&#12289;&#38899;&#39057;&#21644;&#19977;&#32500;&#20960;&#20309;&#31561;&#22810;&#31181;&#26597;&#35810;&#26041;&#24335;&#19979;&#20351;&#29992;&#19977;&#32500;&#22320;&#22270;&#12290;ConceptFusion&#21033;&#29992;&#20102;&#24403;&#20170;&#22312;&#20114;&#32852;&#32593;&#35268;&#27169;&#25968;&#25454;&#19978;&#39044;&#35757;&#32451;&#30340;&#22522;&#30784;&#27169;&#22411;&#30340;&#24320;&#25918;&#38598;&#21512;&#33021;&#21147;&#65292;&#21487;&#20197;&#25512;&#29702;&#21508;&#31181;&#36328;&#27169;&#24577;&#30340;&#27010;&#24565;&#65292;&#22914;&#33258;&#28982;&#35821;&#35328;&#12289;&#22270;&#20687;&#21644;&#38899;&#39057;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#20687;&#32032;&#23545;&#40784;&#30340;&#24320;&#25918;&#38598;&#21512;&#29305;&#24449;&#30340;&#33021;&#21147;&#65292;&#24182;&#22312;&#20960;&#20010;&#23454;&#38469;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;ConceptFusion&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Building 3D maps of the environment is central to robot navigation, planning, and interaction with objects in a scene. Most existing approaches that integrate semantic concepts with 3D maps largely remain confined to the closed-set setting: they can only reason about a finite set of concepts, pre-defined at training time. Further, these maps can only be queried using class labels, or in recent work, using text prompts.  We address both these issues with ConceptFusion, a scene representation that is (1) fundamentally open-set, enabling reasoning beyond a closed set of concepts and (ii) inherently multimodal, enabling a diverse range of possible queries to the 3D map, from language, to images, to audio, to 3D geometry, all working in concert. ConceptFusion leverages the open-set capabilities of today's foundation models pre-trained on internet-scale data to reason about concepts across modalities such as natural language, images, and audio. We demonstrate that pixel-aligned open-set feat
&lt;/p&gt;</description></item><item><title>Re-ViLM&#26159;&#19968;&#31181;&#26816;&#32034;&#22686;&#24378;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#20174;&#22806;&#37096;&#25968;&#25454;&#24211;&#20013;&#26816;&#32034;&#30456;&#20851;&#30693;&#35782;&#65292;&#20943;&#23569;&#20102;&#27169;&#22411;&#21442;&#25968;&#30340;&#25968;&#37327;&#65292;&#24182;&#19988;&#21487;&#20197;&#36731;&#26494;&#36866;&#24212;&#26032;&#25968;&#25454;&#65292;&#29992;&#20110;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2302.04858</link><description>&lt;p&gt;
Re-ViLM: &#29992;&#20110;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#22270;&#20687;&#23383;&#24149;&#30340;&#26816;&#32034;&#22686;&#24378;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Re-ViLM: Retrieval-Augmented Visual Language Model for Zero and Few-Shot Image Captioning. (arXiv:2302.04858v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.04858
&lt;/p&gt;
&lt;p&gt;
Re-ViLM&#26159;&#19968;&#31181;&#26816;&#32034;&#22686;&#24378;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#20174;&#22806;&#37096;&#25968;&#25454;&#24211;&#20013;&#26816;&#32034;&#30456;&#20851;&#30693;&#35782;&#65292;&#20943;&#23569;&#20102;&#27169;&#22411;&#21442;&#25968;&#30340;&#25968;&#37327;&#65292;&#24182;&#19988;&#21487;&#20197;&#36731;&#26494;&#36866;&#24212;&#26032;&#25968;&#25454;&#65292;&#29992;&#20110;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22270;&#20687;&#21040;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#20013;&#65292;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#21644;&#35270;&#35273;&#32534;&#30721;&#22120;&#65288;&#22914;Flamingo&#65289;&#30456;&#32467;&#21512;&#24050;&#32463;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#23558;&#25152;&#26377;&#30693;&#35782;&#23384;&#20648;&#22312;&#20854;&#21442;&#25968;&#20013;&#65292;&#22240;&#27492;&#36890;&#24120;&#38656;&#35201;&#24040;&#22823;&#30340;&#27169;&#22411;&#21442;&#25968;&#26469;&#24314;&#27169;&#20016;&#23500;&#30340;&#35270;&#35273;&#27010;&#24565;&#21644;&#20016;&#23500;&#30340;&#25991;&#26412;&#25551;&#36848;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#22312;&#34701;&#21512;&#26032;&#25968;&#25454;&#26041;&#38754;&#25928;&#29575;&#20302;&#19979;&#65292;&#38656;&#35201;&#32791;&#26102;&#30340;&#24494;&#35843;&#36807;&#31243;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26816;&#32034;&#22686;&#24378;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;Re-ViLM&#65292;&#22522;&#20110;Flamingo&#26500;&#24314;&#65292;&#25903;&#25345;&#22312;&#38646;&#26679;&#26412;&#21644;&#19978;&#19979;&#25991;&#20869;&#23569;&#26679;&#26412;&#22270;&#20687;&#21040;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#20013;&#20174;&#22806;&#37096;&#25968;&#25454;&#24211;&#20013;&#26816;&#32034;&#30456;&#20851;&#30693;&#35782;&#12290;&#36890;&#36807;&#23558;&#26576;&#20123;&#30693;&#35782;&#26126;&#30830;&#23384;&#20648;&#22312;&#22806;&#37096;&#25968;&#25454;&#24211;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20943;&#23569;&#20102;&#27169;&#22411;&#21442;&#25968;&#30340;&#25968;&#37327;&#65292;&#24182;&#19988;&#21487;&#20197;&#36890;&#36807;&#31616;&#21333;&#26356;&#26032;&#25968;&#25454;&#24211;&#26469;&#36731;&#26494;&#36866;&#24212;&#35780;&#20272;&#36807;&#31243;&#20013;&#30340;&#26032;&#25968;&#25454;&#12290;&#25105;&#20204;&#36824;&#26500;&#24314;&#20102;&#19968;&#31181;&#20132;&#38169;&#30340;&#22270;&#20687;&#21644;&#25991;&#26412;&#25968;&#25454;&#65292;&#20197;&#20419;&#36827;&#19978;&#19979;&#25991;&#20869;&#23569;&#26679;&#26412;&#29983;&#25104;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Augmenting pretrained language models (LMs) with a vision encoder (e.g., Flamingo) has obtained the state-of-the-art results in image-to-text generation. However, these models store all the knowledge within their parameters, thus often requiring enormous model parameters to model the abundant visual concepts and very rich textual descriptions. Additionally, they are inefficient in incorporating new data, requiring a computational-expensive fine-tuning process. In this work, we introduce a Retrieval-augmented Visual Language Model, Re-ViLM, built upon the Flamingo, that supports retrieving the relevant knowledge from the external database for zero and in-context few-shot image-to-text generations. By storing certain knowledge explicitly in the external database, our approach reduces the number of model parameters and can easily accommodate new data during evaluation by simply updating the database. We also construct an interleaved image and text data that facilitates in-context few-shot
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#31995;&#32479;&#30740;&#31350;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#23433;&#20840;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#35780;&#20272;&#21644;&#21457;&#29616;&#40657;&#30418;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#23433;&#20840;&#28431;&#27934;&#12290;</title><link>http://arxiv.org/abs/2302.04012</link><description>&lt;p&gt;
CodeLMSec&#22522;&#20934;&#65306;&#31995;&#32479;&#35780;&#20272;&#21644;&#21457;&#29616;&#40657;&#30418;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#23433;&#20840;&#28431;&#27934;
&lt;/p&gt;
&lt;p&gt;
CodeLMSec Benchmark: Systematically Evaluating and Finding Security Vulnerabilities in Black-Box Code Language Models. (arXiv:2302.04012v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.04012
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#31995;&#32479;&#30740;&#31350;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#23433;&#20840;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#35780;&#20272;&#21644;&#21457;&#29616;&#40657;&#30418;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#23433;&#20840;&#28431;&#27934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#29992;&#20110;&#33258;&#21160;&#20195;&#30721;&#29983;&#25104;&#22312;&#20960;&#20010;&#32534;&#31243;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#31361;&#30772;&#12290;&#23427;&#20204;&#22312;&#31454;&#36187;&#32423;&#32534;&#31243;&#38382;&#39064;&#19978;&#30340;&#36827;&#23637;&#20351;&#23427;&#20204;&#25104;&#20026;AI&#36741;&#21161;&#23545;&#32534;&#31243;&#30340;&#37325;&#35201;&#25903;&#26609;&#65292;&#24037;&#20855;&#22914;GitHub Copilot&#24050;&#32463;&#25104;&#20026;&#25968;&#30334;&#19975;&#24320;&#21457;&#20154;&#21592;&#26085;&#24120;&#32534;&#31243;&#24037;&#20316;&#27969;&#31243;&#30340;&#19968;&#37096;&#20998;&#12290;&#36825;&#20123;&#27169;&#22411;&#30340;&#35757;&#32451;&#25968;&#25454;&#36890;&#24120;&#26469;&#33258;&#20110;&#20114;&#32852;&#32593;&#65288;&#20363;&#22914;&#24320;&#28304;&#23384;&#20648;&#24211;&#65289;&#24182;&#19988;&#21487;&#33021;&#21547;&#26377;&#32570;&#38519;&#21644;&#23433;&#20840;&#28431;&#27934;&#12290;&#36825;&#20123;&#26410;&#32463;&#28040;&#27602;&#30340;&#35757;&#32451;&#25968;&#25454;&#21487;&#33021;&#23548;&#33268;&#35821;&#35328;&#27169;&#22411;&#23398;&#20064;&#36825;&#20123;&#28431;&#27934;&#24182;&#22312;&#20195;&#30721;&#29983;&#25104;&#36807;&#31243;&#20013;&#20256;&#25773;&#23427;&#20204;&#12290;&#23613;&#31649;&#36825;&#20123;&#27169;&#22411;&#24050;&#32463;&#24191;&#27867;&#35780;&#20272;&#20102;&#23427;&#20204;&#29983;&#25104;&#21151;&#33021;&#19978;&#27491;&#30830;&#31243;&#24207;&#30340;&#33021;&#21147;&#65292;&#20294;&#23545;&#36825;&#20123;&#27169;&#22411;&#30340;&#23433;&#20840;&#38382;&#39064;&#20173;&#28982;&#32570;&#20047;&#20840;&#38754;&#30340;&#35843;&#26597;&#21644;&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) for automatic code generation have achieved breakthroughs in several programming tasks. Their advances in competition-level programming problems have made them an essential pillar of AI-assisted pair programming, and tools such as GitHub Copilot have emerged as part of the daily programming workflow used by millions of developers. The training data for these models is usually collected from the Internet (e.g., from open-source repositories) and is likely to contain faults and security vulnerabilities. This unsanitized training data can cause the language models to learn these vulnerabilities and propagate them during the code generation procedure. While these models have been extensively assessed for their ability to produce functionally correct programs, there remains a lack of comprehensive investigations and benchmarks addressing the security aspects of these models.  In this work, we propose a method to systematically study the security issues of code l
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21452;&#23618;&#30693;&#35782;&#22270;&#35889;&#30340;&#26041;&#27861;&#26469;&#23398;&#20064;&#23884;&#20837;&#65292;&#23558;&#19977;&#20803;&#32452;&#20043;&#38388;&#30340;&#20851;&#31995;&#32771;&#34385;&#36827;&#21435;&#65292;&#24182;&#20351;&#29992;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#26469;&#22686;&#21152;&#21512;&#29702;&#30340;&#19977;&#20803;&#32452;&#12290;</title><link>http://arxiv.org/abs/2302.02601</link><description>&lt;p&gt;
&#23398;&#20064;&#21452;&#23618;&#30693;&#35782;&#22270;&#35889;&#30340;&#34920;&#31034;&#20197;&#36827;&#34892;&#36229;&#36234;&#38142;&#25509;&#39044;&#27979;&#30340;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Learning Representations of Bi-level Knowledge Graphs for Reasoning beyond Link Prediction. (arXiv:2302.02601v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.02601
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21452;&#23618;&#30693;&#35782;&#22270;&#35889;&#30340;&#26041;&#27861;&#26469;&#23398;&#20064;&#23884;&#20837;&#65292;&#23558;&#19977;&#20803;&#32452;&#20043;&#38388;&#30340;&#20851;&#31995;&#32771;&#34385;&#36827;&#21435;&#65292;&#24182;&#20351;&#29992;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#26469;&#22686;&#21152;&#21512;&#29702;&#30340;&#19977;&#20803;&#32452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#20351;&#29992;&#19977;&#20803;&#32452;&#26469;&#34920;&#31034;&#24050;&#30693;&#20107;&#23454;&#12290;&#29616;&#26377;&#30340;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#26041;&#27861;&#20165;&#32771;&#34385;&#23454;&#20307;&#20043;&#38388;&#30340;&#36830;&#25509;&#65292;&#32780;&#26412;&#25991;&#25552;&#20986;&#32771;&#34385;&#19977;&#20803;&#32452;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#26412;&#25991;&#23450;&#20041;&#20102;&#19968;&#20010;&#26356;&#39640;&#32423;&#30340;&#19977;&#20803;&#32452;&#26469;&#34920;&#31034;&#19977;&#20803;&#32452;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#20363;&#22914;&#65292;$\langle T_1$, PrerequisiteFor, $T_2\rangle$&#65292;&#20854;&#20013;PrerequisiteFor&#26159;&#26356;&#39640;&#32423;&#21035;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#23450;&#20041;&#19968;&#20010;&#30001;&#22522;&#26412;&#32423;&#21035;&#21644;&#26356;&#39640;&#32423;&#21035;&#30340;&#19977;&#20803;&#32452;&#32452;&#25104;&#30340;&#21452;&#23618;&#30693;&#35782;&#22270;&#35889;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21452;&#23618;&#30693;&#35782;&#22270;&#35889;&#19978;&#30340;&#38543;&#26426;&#28216;&#36208;&#30340;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#26469;&#22686;&#21152;&#21512;&#29702;&#30340;&#19977;&#20803;&#32452;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;BiVE&#36890;&#36807;&#32771;&#34385;&#22522;&#26412;&#32423;&#21035;&#21644;&#26356;&#39640;&#32423;&#21035;&#19977;&#20803;&#32452;&#30340;&#32467;&#26500;&#26469;&#23398;&#20064;&#23884;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge graphs represent known facts using triplets. While existing knowledge graph embedding methods only consider the connections between entities, we propose considering the relationships between triplets. For example, let us consider two triplets $T_1$ and $T_2$ where $T_1$ is (Academy_Awards, Nominates, Avatar) and $T_2$ is (Avatar, Wins, Academy_Awards). Given these two base-level triplets, we see that $T_1$ is a prerequisite for $T_2$. In this paper, we define a higher-level triplet to represent a relationship between triplets, e.g., $\langle T_1$, PrerequisiteFor, $T_2\rangle$ where PrerequisiteFor is a higher-level relation. We define a bi-level knowledge graph that consists of the base-level and the higher-level triplets. We also propose a data augmentation strategy based on the random walks on the bi-level knowledge graph to augment plausible triplets. Our model called BiVE learns embeddings by taking into account the structures of the base-level and the higher-level tripl
&lt;/p&gt;</description></item><item><title>ResMem&#26159;&#19968;&#31181;&#36890;&#36807;&#26174;&#24335;&#35760;&#24518;&#26469;&#25913;&#21892;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#25311;&#21512;&#27169;&#22411;&#30340;&#27531;&#24046;&#26469;&#23454;&#29616;&#12290;&#22312;&#21508;&#31181;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;ResMem&#19968;&#33268;&#22320;&#25913;&#21892;&#20102;&#21407;&#22987;&#39044;&#27979;&#27169;&#22411;&#30340;&#27979;&#35797;&#38598;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2302.01576</link><description>&lt;p&gt;
ResMem&#65306;&#23398;&#20064;&#21487;&#20197;&#30340;&#65292;&#35760;&#20303;&#21097;&#19979;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
ResMem: Learn what you can and memorize the rest. (arXiv:2302.01576v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.01576
&lt;/p&gt;
&lt;p&gt;
ResMem&#26159;&#19968;&#31181;&#36890;&#36807;&#26174;&#24335;&#35760;&#24518;&#26469;&#25913;&#21892;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#25311;&#21512;&#27169;&#22411;&#30340;&#27531;&#24046;&#26469;&#23454;&#29616;&#12290;&#22312;&#21508;&#31181;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;ResMem&#19968;&#33268;&#22320;&#25913;&#21892;&#20102;&#21407;&#22987;&#39044;&#27979;&#27169;&#22411;&#30340;&#27979;&#35797;&#38598;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#31070;&#32463;&#32593;&#32476;&#25152;&#23637;&#29616;&#20986;&#30340;&#20196;&#20154;&#30633;&#30446;&#30340;&#27867;&#21270;&#24615;&#33021;&#37096;&#20998;&#24402;&#21151;&#20110;&#20854;&#38544;&#24335;&#35760;&#24518;&#22797;&#26434;&#30340;&#35757;&#32451;&#27169;&#24335;&#30340;&#33021;&#21147;&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#19968;&#31181;&#25913;&#36827;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#30340;&#26032;&#26426;&#21046;&#65292;&#36890;&#36807;&#26174;&#24335;&#35760;&#24518;&#26469;&#23454;&#29616;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#27531;&#24046;&#35760;&#24518;&#65288;ResMem&#65289;&#31639;&#27861;&#65292;&#36825;&#26159;&#19968;&#31181;&#36890;&#36807;&#29992;&#22522;&#20110;$k$&#26368;&#36817;&#37051;&#30340;&#22238;&#24402;&#22120;&#25311;&#21512;&#27169;&#22411;&#30340;&#27531;&#24046;&#26469;&#22686;&#21152;&#29616;&#26377;&#39044;&#27979;&#27169;&#22411;&#65288;&#20363;&#22914;&#31070;&#32463;&#32593;&#32476;&#65289;&#30340;&#26041;&#27861;&#12290;&#26368;&#32456;&#39044;&#27979;&#26159;&#21407;&#22987;&#27169;&#22411;&#21644;&#25311;&#21512;&#30340;&#27531;&#24046;&#22238;&#24402;&#22120;&#30340;&#21644;&#12290;&#36890;&#36807;&#26500;&#36896;&#65292;ResMem&#21487;&#20197;&#26174;&#24335;&#22320;&#35760;&#20303;&#35757;&#32451;&#26631;&#31614;&#12290;&#23454;&#35777;&#19978;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;ResMem&#22312;&#21508;&#31181;&#26631;&#20934;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#22522;&#20934;&#27979;&#35797;&#20013;&#19968;&#33268;&#22320;&#25913;&#21892;&#20102;&#21407;&#22987;&#39044;&#27979;&#27169;&#22411;&#30340;&#27979;&#35797;&#38598;&#27867;&#21270;&#33021;&#21147;&#12290;&#29702;&#35770;&#19978;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#31616;&#21270;&#30340;&#32447;&#24615;&#22238;&#24402;&#38382;&#39064;&#65292;&#24182;&#20005;&#26684;&#35777;&#26126;&#20102;ResMem&#30456;&#23545;&#20110;&#22522;&#26412;&#39044;&#27979;&#27169;&#22411;&#20855;&#26377;&#26356;&#26377;&#21033;&#30340;&#27979;&#35797;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;
The impressive generalization performance of modern neural networks is attributed in part to their ability to implicitly memorize complex training patterns. Inspired by this, we explore a novel mechanism to improve model generalization via explicit memorization. Specifically, we propose the residual-memorization (ResMem) algorithm, a new method that augments an existing prediction model (e.g. a neural network) by fitting the model's residuals with a $k$-nearest neighbor based regressor. The final prediction is then the sum of the original model and the fitted residual regressor. By construction, ResMem can explicitly memorize the training labels. Empirically, we show that ResMem consistently improves the test set generalization of the original prediction model across various standard vision and natural language processing benchmarks. Theoretically, we formulate a stylized linear regression problem and rigorously show that ResMem results in a more favorable test risk over the base predi
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#26032;&#30340;&#38750;&#38543;&#26426;&#32852;&#37030;&#22810;&#33218;&#36172;&#21338;&#38382;&#39064;&#65292;&#32771;&#34385;&#20102;&#20855;&#26377;&#21452;&#37325;&#23545;&#25239;&#24615;&#30340;&#35774;&#32622;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20219;&#20309;&#32852;&#37030;&#36172;&#21338;&#31639;&#27861;&#30340;&#36951;&#25022;&#19979;&#30028;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#25509;&#36817;&#26368;&#20248;&#30340;&#31639;&#27861;FEDEXP3&#12290;&#35813;&#31639;&#27861;&#35299;&#20915;&#20102;&#20043;&#21069;&#30340;&#24320;&#25918;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2301.09223</link><description>&lt;p&gt;
&#21452;&#37325;&#23545;&#25239;&#24615;&#32852;&#37030;&#22810;&#33218;&#36172;&#21338;&#38382;&#39064;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Doubly Adversarial Federated Bandits. (arXiv:2301.09223v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.09223
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#26032;&#30340;&#38750;&#38543;&#26426;&#32852;&#37030;&#22810;&#33218;&#36172;&#21338;&#38382;&#39064;&#65292;&#32771;&#34385;&#20102;&#20855;&#26377;&#21452;&#37325;&#23545;&#25239;&#24615;&#30340;&#35774;&#32622;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20219;&#20309;&#32852;&#37030;&#36172;&#21338;&#31639;&#27861;&#30340;&#36951;&#25022;&#19979;&#30028;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#25509;&#36817;&#26368;&#20248;&#30340;&#31639;&#27861;FEDEXP3&#12290;&#35813;&#31639;&#27861;&#35299;&#20915;&#20102;&#20043;&#21069;&#30340;&#24320;&#25918;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#26032;&#30340;&#38750;&#38543;&#26426;&#32852;&#37030;&#22810;&#33218;&#36172;&#21338;&#38382;&#39064;&#65292;&#22810;&#20010;&#20195;&#29702;&#36890;&#36807;&#36890;&#20449;&#32593;&#32476;&#36827;&#34892;&#21327;&#20316;&#12290;&#33218;&#30340;&#25439;&#22833;&#30001;&#19968;&#20010;&#26080;&#24847;&#35782;&#30340;&#23545;&#25163;&#20998;&#37197;&#65292;&#35813;&#23545;&#25163;&#19981;&#20165;&#25351;&#23450;&#27599;&#20010;&#26102;&#38388;&#27493;&#21644;&#27599;&#20010;&#20195;&#29702;&#30340;&#27599;&#20010;&#33218;&#30340;&#25439;&#22833;&#65292;&#36824;&#20855;&#26377;&#8220;&#21452;&#37325;&#23545;&#25239;&#24615;&#8221;&#12290;&#22312;&#36825;&#31181;&#35774;&#32622;&#19979;&#65292;&#19981;&#21516;&#30340;&#20195;&#29702;&#21487;&#33021;&#22312;&#21516;&#19968;&#26102;&#38388;&#27493;&#36873;&#25321;&#30456;&#21516;&#30340;&#33218;&#65292;&#20294;&#35266;&#23519;&#21040;&#19981;&#21516;&#30340;&#21453;&#39304;&#12290;&#27599;&#20010;&#20195;&#29702;&#30340;&#30446;&#26631;&#26159;&#25214;&#21040;&#19968;&#20010;&#20840;&#23616;&#26368;&#22909;&#30340;&#33218;&#65292;&#20351;&#24471;&#22312;&#25152;&#26377;&#20195;&#29702;&#19978;&#24179;&#22343;&#32047;&#31215;&#25439;&#22833;&#26368;&#20302;&#65292;&#36825;&#38656;&#35201;&#20195;&#29702;&#20043;&#38388;&#30340;&#36890;&#20449;&#12290;&#25105;&#20204;&#38024;&#23545;&#19981;&#21516;&#35774;&#32622;&#25552;&#20379;&#20102;&#20219;&#20309;&#32852;&#37030;&#36172;&#21338;&#31639;&#27861;&#30340;&#36951;&#25022;&#19979;&#30028;&#65292;&#24403;&#20195;&#29702;&#26377;&#23436;&#20840;&#20449;&#24687;&#21453;&#39304;&#25110;&#36172;&#21338;&#21453;&#39304;&#26102;&#12290;&#23545;&#20110;&#36172;&#21338;&#21453;&#39304;&#35774;&#32622;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25509;&#36817;&#26368;&#20248;&#30340;&#32852;&#37030;&#36172;&#21338;&#31639;&#27861;&#31216;&#20026;FEDEXP3&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#23545;Cesa-Bianchi&#31561;&#20154;&#65288;2016&#65289;&#25552;&#20986;&#30340;&#19968;&#20010;&#24320;&#25918;&#24615;&#38382;&#39064;&#32473;&#20986;&#20102;&#27491;&#38754;&#31572;&#26696;&#65306;FEDEXP3&#21487;&#20197;&#20445;&#35777;...
&lt;/p&gt;
&lt;p&gt;
We study a new non-stochastic federated multi-armed bandit problem with multiple agents collaborating via a communication network. The losses of the arms are assigned by an oblivious adversary that specifies the loss of each arm not only for each time step but also for each agent, which we call ``doubly adversarial". In this setting, different agents may choose the same arm in the same time step but observe different feedback. The goal of each agent is to find a globally best arm in hindsight that has the lowest cumulative loss averaged over all agents, which necessities the communication among agents. We provide regret lower bounds for any federated bandit algorithm under different settings, when agents have access to full-information feedback, or the bandit feedback. For the bandit feedback setting, we propose a near-optimal federated bandit algorithm called FEDEXP3. Our algorithm gives a positive answer to an open question proposed in Cesa-Bianchi et al. (2016): FEDEXP3 can guarante
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#23545;&#26368;&#36817;&#22312;NLP&#39046;&#22495;&#20013;&#30340;&#24046;&#20998;&#38544;&#31169;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#26368;&#26032;&#36827;&#23637;&#36827;&#34892;&#20102;&#31995;&#32479;&#32508;&#36848;&#65292;&#35752;&#35770;&#20102;&#19982;&#26631;&#20934;&#24046;&#20998;&#38544;&#31169;&#28145;&#24230;&#23398;&#20064;&#30340;&#19981;&#21516;&#20043;&#22788;&#21644;&#39069;&#22806;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2301.09112</link><description>&lt;p&gt;
&#19981;&#21516;ially&#31169;&#23494;&#30340;&#33258;&#28982;&#35821;&#35328;&#27169;&#22411;: &#26368;&#26032;&#36827;&#23637;&#21644;&#26410;&#26469;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Differentially Private Natural Language Models: Recent Advances and Future Directions. (arXiv:2301.09112v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.09112
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#23545;&#26368;&#36817;&#22312;NLP&#39046;&#22495;&#20013;&#30340;&#24046;&#20998;&#38544;&#31169;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#26368;&#26032;&#36827;&#23637;&#36827;&#34892;&#20102;&#31995;&#32479;&#32508;&#36848;&#65292;&#35752;&#35770;&#20102;&#19982;&#26631;&#20934;&#24046;&#20998;&#38544;&#31169;&#28145;&#24230;&#23398;&#20064;&#30340;&#19981;&#21516;&#20043;&#22788;&#21644;&#39069;&#22806;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#30340;&#26368;&#26032;&#21457;&#23637;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#24212;&#29992;&#21487;&#33021;&#28041;&#21450;&#21253;&#21547;&#25935;&#24863;&#20449;&#24687;&#30340;&#25968;&#25454;&#12290;&#22240;&#27492;&#65292;&#22312;&#20445;&#25252;&#25935;&#24863;&#25968;&#25454;&#38544;&#31169;&#30340;&#21516;&#26102;&#22914;&#20309;&#23454;&#29616;&#33391;&#22909;&#30340;&#24615;&#33021;&#26159;NLP&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#12290;&#20026;&#20102;&#20445;&#25252;&#38544;&#31169;&#65292;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#25104;&#20026;&#20102;&#38544;&#31169;&#25968;&#25454;&#20998;&#26512;&#20013;&#38450;&#27490;&#37325;&#24314;&#25915;&#20987;&#21644;&#38450;&#25252;&#28508;&#22312;&#36793;&#32536;&#30693;&#35782;&#30340;&#20107;&#23454;&#26631;&#20934;&#25216;&#26415;&#12290;&#36817;&#24180;&#26469;&#65292;DP&#22312;NLP&#27169;&#22411;&#65288;DP-NLP&#65289;&#26041;&#38754;&#24050;&#32463;&#20174;&#19981;&#21516;&#30340;&#35282;&#24230;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#20540;&#24471;&#36827;&#34892;&#20840;&#38754;&#30340;&#22238;&#39038;&#12290;&#26412;&#25991;&#39318;&#27425;&#23545;DP&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;NLP&#20013;&#30340;&#26368;&#26032;&#36827;&#23637;&#36827;&#34892;&#20102;&#31995;&#32479;&#24615;&#30340;&#32508;&#36848;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#35752;&#35770;&#20102;&#19982;&#26631;&#20934;DP&#28145;&#24230;&#23398;&#20064;&#30456;&#27604;&#65292;DP-NLP&#30340;&#19968;&#20123;&#24046;&#24322;&#21644;&#39069;&#22806;&#25361;&#25112;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#19968;&#20123;&#29616;&#26377;&#30340;DP-NLP&#24037;&#20316;&#65292;&#24182;&#20174;&#19977;&#20010;&#26041;&#38754;&#20171;&#32461;&#20102;&#26368;&#26032;&#30340;&#21457;&#23637;&#65306;&#26799;&#24230;pe
&lt;/p&gt;
&lt;p&gt;
Recent developments in deep learning have led to great success in various natural language processing (NLP) tasks. However, these applications may involve data that contain sensitive information. Therefore, how to achieve good performance while also protecting the privacy of sensitive data is a crucial challenge in NLP. To preserve privacy, Differential Privacy (DP), which can prevent reconstruction attacks and protect against potential side knowledge, is becoming a de facto technique for private data analysis. In recent years, NLP in DP models (DP-NLP) has been studied from different perspectives, which deserves a comprehensive review. In this paper, we provide the first systematic review of recent advances in DP deep learning models in NLP. In particular, we first discuss some differences and additional challenges of DP-NLP compared with the standard DP deep learning. Then, we investigate some existing work on DP-NLP and present its recent developments from three aspects: gradient pe
&lt;/p&gt;</description></item><item><title>AtMan&#26159;&#19968;&#31181;&#36890;&#36807;&#22312;&#29983;&#25104;&#24335;Transformer&#27169;&#22411;&#20013;&#25805;&#32437;&#27880;&#24847;&#21147;&#26426;&#21046;&#26469;&#35299;&#37322;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#30456;&#36739;&#20110;&#20256;&#32479;&#26041;&#27861;&#20960;&#20046;&#19981;&#21344;&#29992;&#39069;&#22806;&#20869;&#23384;&#65292;&#21487;&#22312;&#29983;&#20135;&#29615;&#22659;&#20013;&#20351;&#29992;&#12290;</title><link>http://arxiv.org/abs/2301.08110</link><description>&lt;p&gt;
AtMan:&#36890;&#36807;&#33410;&#32422;&#20869;&#23384;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#29702;&#35299;Transformer&#30340;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
AtMan: Understanding Transformer Predictions Through Memory Efficient Attention Manipulation. (arXiv:2301.08110v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.08110
&lt;/p&gt;
&lt;p&gt;
AtMan&#26159;&#19968;&#31181;&#36890;&#36807;&#22312;&#29983;&#25104;&#24335;Transformer&#27169;&#22411;&#20013;&#25805;&#32437;&#27880;&#24847;&#21147;&#26426;&#21046;&#26469;&#35299;&#37322;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#30456;&#36739;&#20110;&#20256;&#32479;&#26041;&#27861;&#20960;&#20046;&#19981;&#21344;&#29992;&#39069;&#22806;&#20869;&#23384;&#65292;&#21487;&#22312;&#29983;&#20135;&#29615;&#22659;&#20013;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#30340;Transformer&#27169;&#22411;&#36234;&#26469;&#36234;&#22797;&#26434;&#65292;&#21442;&#25968;&#25968;&#37327;&#22823;&#19988;&#20855;&#22791;&#22788;&#29702;&#22810;&#36755;&#20837;&#27169;&#24577;&#30340;&#33021;&#21147;&#12290;&#30446;&#21069;&#35299;&#37322;&#23427;&#20204;&#30340;&#39044;&#27979;&#30340;&#26041;&#27861;&#36164;&#28304;&#23494;&#38598;&#12290;&#26368;&#37325;&#35201;&#30340;&#26159;&#65292;&#23427;&#20204;&#38656;&#35201;&#36807;&#22810;&#30340;&#39069;&#22806;&#20869;&#23384;&#65292;&#22240;&#20026;&#23427;&#20204;&#20381;&#36182;&#21453;&#21521;&#20256;&#25773;&#65292;&#32780;&#21453;&#21521;&#20256;&#25773;&#20250;&#20998;&#37197;&#30340;GPU&#20869;&#23384;&#20960;&#20046;&#26159;&#21069;&#21521;&#20256;&#25773;&#30340;&#20004;&#20493;&#12290;&#36825;&#20351;&#24471;&#22312;&#29983;&#20135;&#29615;&#22659;&#20013;&#20351;&#29992;&#23427;&#20204;&#38750;&#24120;&#22256;&#38590;&#65292;&#29978;&#33267;&#19981;&#21487;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;AtMan&#65292;&#23427;&#20960;&#20046;&#19981;&#20250;&#20135;&#29983;&#39069;&#22806;&#30340;&#25104;&#26412;&#65292;&#29992;&#20110;&#35299;&#37322;&#29983;&#25104;&#24335;Transformer&#27169;&#22411;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;AtMan&#26159;&#19968;&#31181;&#27169;&#24577;&#26080;&#20851;&#30340;&#25200;&#21160;&#26041;&#27861;&#65292;&#36890;&#36807;&#25805;&#32437;Transformer&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#29983;&#25104;&#19982;&#36755;&#20986;&#39044;&#27979;&#30456;&#20851;&#24615;&#30340;&#37325;&#35201;&#24615;&#22270;&#12290;AtMan&#19981;&#20351;&#29992;&#21453;&#21521;&#20256;&#25773;&#65292;&#32780;&#26159;&#22312;&#23884;&#20837;&#31354;&#38388;&#20013;&#24212;&#29992;&#19968;&#31181;&#22522;&#20110;&#20313;&#24358;&#30456;&#20284;&#24230;&#37051;&#36817;&#24615;&#30340;&#21487;&#24182;&#34892;&#21270;&#22522;&#20110;&#35760;&#21495;&#30340;&#25628;&#32034;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;&#25991;&#26412;&#21644;&#22270;&#20687;-&#25991;&#26412;&#22522;&#20934;&#27979;&#35797;&#20013;&#36827;&#34892;&#20102;&#35814;&#23613;&#30340;&#23454;&#39564;
&lt;/p&gt;
&lt;p&gt;
Generative transformer models have become increasingly complex, with large numbers of parameters and the ability to process multiple input modalities. Current methods for explaining their predictions are resource-intensive. Most crucially, they require prohibitively large amounts of extra memory, since they rely on backpropagation which allocates almost twice as much GPU memory as the forward pass. This makes it difficult, if not impossible, to use them in production. We present AtMan that provides explanations of generative transformer models at almost no extra cost. Specifically, AtMan is a modality-agnostic perturbation method that manipulates the attention mechanisms of transformers to produce relevance maps for the input with respect to the output prediction. Instead of using backpropagation, AtMan applies a parallelizable token-based search method based on cosine similarity neighborhood in the embedding space. Our exhaustive experiments on text and image-text benchmarks demonstra
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26465;&#20214;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#26469;&#35299;&#37322;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#39044;&#27979;&#30340;&#26032;&#26041;&#27861;&#12290;&#36890;&#36807;&#35757;&#32451;GAN&#20351;&#29992;CNN&#22788;&#29702;&#22270;&#20687;&#26102;&#30340;&#20449;&#24687;&#65292;&#25105;&#20204;&#25104;&#21151;&#29983;&#25104;&#20102;&#35270;&#35273;&#35299;&#37322;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#35299;&#20915;&#22914;&#20309;&#34920;&#31034;&#21644;&#36755;&#20837;&#36825;&#20123;&#20449;&#24687;&#30340;&#25361;&#25112;&#30340;&#26041;&#27861;&#65292;&#24182;&#36827;&#34892;&#20102;&#23450;&#24615;&#21644;&#23450;&#37327;&#35780;&#20272;&#21644;&#27604;&#36739;&#65292;&#21457;&#29616;&#21021;&#22987;&#23618;&#26159;&#35299;&#37322;CNN&#39044;&#27979;&#30340;&#20851;&#38190;&#12290;</title><link>http://arxiv.org/abs/2301.08067</link><description>&lt;p&gt;
&#20351;&#29992;&#26465;&#20214;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#35299;&#37322;CNN&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Interpreting CNN Predictions using Conditional Generative Adversarial Networks. (arXiv:2301.08067v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.08067
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26465;&#20214;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#26469;&#35299;&#37322;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#39044;&#27979;&#30340;&#26032;&#26041;&#27861;&#12290;&#36890;&#36807;&#35757;&#32451;GAN&#20351;&#29992;CNN&#22788;&#29702;&#22270;&#20687;&#26102;&#30340;&#20449;&#24687;&#65292;&#25105;&#20204;&#25104;&#21151;&#29983;&#25104;&#20102;&#35270;&#35273;&#35299;&#37322;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#35299;&#20915;&#22914;&#20309;&#34920;&#31034;&#21644;&#36755;&#20837;&#36825;&#20123;&#20449;&#24687;&#30340;&#25361;&#25112;&#30340;&#26041;&#27861;&#65292;&#24182;&#36827;&#34892;&#20102;&#23450;&#24615;&#21644;&#23450;&#37327;&#35780;&#20272;&#21644;&#27604;&#36739;&#65292;&#21457;&#29616;&#21021;&#22987;&#23618;&#26159;&#35299;&#37322;CNN&#39044;&#27979;&#30340;&#20851;&#38190;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#20351;&#29992;&#26465;&#20214;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#35757;&#32451;&#26469;&#29983;&#25104;&#23545;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#36827;&#34892;&#35270;&#35273;&#35299;&#37322;&#12290;&#20026;&#20102;&#29702;&#35299;CNN&#65292;&#25105;&#20204;&#29992;CNN&#22788;&#29702;&#22270;&#20687;&#36827;&#34892;&#39044;&#27979;&#30340;&#20449;&#24687;&#26469;&#35757;&#32451;GAN&#12290;&#25552;&#20379;&#36825;&#20123;&#20449;&#24687;&#26377;&#20004;&#20010;&#20027;&#35201;&#25361;&#25112;&#65306;&#22914;&#20309;&#23558;&#36825;&#20123;&#20449;&#24687;&#34920;&#31034;&#20026;&#21487;&#36755;&#20837;GAN&#30340;&#24418;&#24335;&#65292;&#20197;&#21450;&#22914;&#20309;&#26377;&#25928;&#22320;&#23558;&#34920;&#31034;&#36755;&#20837;GAN&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#36890;&#36807;&#32047;&#35745;&#24179;&#22343;&#20013;&#38388;&#35299;&#37322;&#26144;&#23556;&#26469;&#24320;&#21457;&#20102;&#36866;&#21512;&#30340;CNN&#26550;&#26500;&#34920;&#31034;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#20004;&#31181;&#36873;&#25321;GAN&#36755;&#20837;&#34920;&#31034;&#21644;&#36873;&#25321;&#26377;&#25928;&#35757;&#32451;&#31574;&#30053;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23398;&#20064;&#20102;CNN&#30340;&#36890;&#29992;&#29305;&#24449;&#65292;&#24182;&#19982;&#29616;&#26377;&#25216;&#26415;&#36827;&#34892;&#20102;&#36136;&#37327;&#21644;&#25968;&#37327;&#35780;&#20272;&#27604;&#36739;&#12290;&#25105;&#20204;&#21457;&#29616;&#21021;&#22987;&#23618;&#26159;&#35299;&#37322;CNN&#39044;&#27979;&#30340;&#20851;&#38190;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a novel method that trains a conditional Generative Adversarial Network (GAN) to generate visual interpretations of a Convolutional Neural Network (CNN). To comprehend a CNN, the GAN is trained with information on how the CNN processes an image when making predictions. Supplying that information has two main challenges: how to represent this information in a form that is feedable to the GANs and how to effectively feed the representation to the GAN. To address these issues, we developed a suitable representation of CNN architectures by cumulatively averaging intermediate interpretation maps. We also propose two alternative approaches to feed the representations to the GAN and to choose an effective training strategy. Our approach learned the general aspects of CNNs and was agnostic to datasets and CNN architectures. The study includes both qualitative and quantitative evaluations and compares the proposed GANs with state-of-the-art approaches. We found that the initial layer
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26816;&#32034;&#21644;&#38405;&#35835;&#26694;&#26550;&#26469;&#35299;&#20915;&#29616;&#26377;&#30693;&#35782;&#22270;&#35889;&#38142;&#25509;&#39044;&#27979;&#31995;&#32479;&#30340;&#23616;&#38480;&#24615;&#12290;&#36890;&#36807;&#39318;&#20808;&#26816;&#32034;&#30456;&#20851;&#23376;&#22270;&#19978;&#19979;&#25991;&#65292;&#28982;&#21518;&#20351;&#29992;&#39640;&#23481;&#37327;&#38405;&#35835;&#22120;&#32852;&#21512;&#25512;&#29702;&#19978;&#19979;&#25991;&#21644;&#26597;&#35810;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#25552;&#20379;&#26356;&#26377;&#29992;&#30340;&#20449;&#24687;&#21644;&#26356;&#24378;&#22823;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2212.09724</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#30693;&#35782;&#22270;&#35889;&#38142;&#25509;&#39044;&#27979;&#30340;&#26816;&#32034;&#21644;&#38405;&#35835;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Retrieve-and-Read Framework for Knowledge Graph Link Prediction. (arXiv:2212.09724v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09724
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26816;&#32034;&#21644;&#38405;&#35835;&#26694;&#26550;&#26469;&#35299;&#20915;&#29616;&#26377;&#30693;&#35782;&#22270;&#35889;&#38142;&#25509;&#39044;&#27979;&#31995;&#32479;&#30340;&#23616;&#38480;&#24615;&#12290;&#36890;&#36807;&#39318;&#20808;&#26816;&#32034;&#30456;&#20851;&#23376;&#22270;&#19978;&#19979;&#25991;&#65292;&#28982;&#21518;&#20351;&#29992;&#39640;&#23481;&#37327;&#38405;&#35835;&#22120;&#32852;&#21512;&#25512;&#29702;&#19978;&#19979;&#25991;&#21644;&#26597;&#35810;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#25552;&#20379;&#26356;&#26377;&#29992;&#30340;&#20449;&#24687;&#21644;&#26356;&#24378;&#22823;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#38142;&#25509;&#39044;&#27979;&#26088;&#22312;&#26681;&#25454;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#29616;&#26377;&#20107;&#23454;&#25512;&#26029;&#20986;&#26032;&#30340;&#20107;&#23454;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#20351;&#29992;&#33410;&#28857;&#30340;&#22270;&#37051;&#22495;&#25552;&#20379;&#20102;&#27604;&#20165;&#20351;&#29992;&#26597;&#35810;&#20449;&#24687;&#26356;&#26377;&#29992;&#30340;&#20449;&#24687;&#12290;&#20256;&#32479;&#30340;KG&#38142;&#25509;&#39044;&#27979;&#30340;GNNs&#36981;&#24490;&#25972;&#20010;KG&#19978;&#30340;&#26631;&#20934;&#28040;&#24687;&#20256;&#36882;&#33539;&#24335;&#65292;&#36825;&#23548;&#33268;&#20102;&#20887;&#20313;&#35745;&#31639;&#12289;&#33410;&#28857;&#34920;&#31034;&#30340;&#36807;&#24230;&#24179;&#28369;&#20197;&#21450;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;&#22312;&#22823;&#35268;&#27169;&#19978;&#65292;&#20174;&#25972;&#20010;KG&#20013;&#32858;&#21512;&#26377;&#29992;&#30340;&#20449;&#24687;&#36827;&#34892;&#25512;&#29702;&#21464;&#24471;&#35745;&#31639;&#19978;&#26114;&#36149;&#12290;&#20026;&#20102;&#35299;&#20915;&#29616;&#26377;KG&#38142;&#25509;&#39044;&#27979;&#26694;&#26550;&#30340;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26816;&#32034;&#21644;&#38405;&#35835;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#39318;&#20808;&#26816;&#32034;&#19982;&#26597;&#35810;&#30456;&#20851;&#30340;&#23376;&#22270;&#19978;&#19979;&#25991;&#65292;&#28982;&#21518;&#36890;&#36807;&#39640;&#23481;&#37327;&#38405;&#35835;&#22120;&#32852;&#21512;&#25512;&#29702;&#19978;&#19979;&#25991;&#21644;&#26597;&#35810;&#12290;&#20316;&#20026;&#25105;&#20204;&#26032;&#26694;&#26550;&#30340;&#23454;&#20363;&#21270;&#30340;&#19968;&#37096;&#20998;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;GNN&#20316;&#20026;r&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge graph (KG) link prediction aims to infer new facts based on existing facts in the KG. Recent studies have shown that using the graph neighborhood of a node via graph neural networks (GNNs) provides more useful information compared to just using the query information. Conventional GNNs for KG link prediction follow the standard message-passing paradigm on the entire KG, which leads to superfluous computation, over-smoothing of node representations, and also limits their expressive power. On a large scale, it becomes computationally expensive to aggregate useful information from the entire KG for inference. To address the limitations of existing KG link prediction frameworks, we propose a novel retrieve-and-read framework, which first retrieves a relevant subgraph context for the query and then jointly reasons over the context and the query with a high-capacity reader. As part of our exemplar instantiation for the new framework, we propose a novel Transformer-based GNN as the r
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#38382;&#39064;&#9472;&#9472;&#20107;&#20214;&#20010;&#20307;&#21270;&#23545;&#20110;&#27169;&#26495;&#22635;&#20805;&#20219;&#21153;&#26159;&#21542;&#36866;&#29992;&#65292;&#36890;&#36807;&#27880;&#37322;&#30740;&#31350;&#21644;&#35823;&#24046;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#24341;&#21457;&#20102;&#23545;&#27169;&#26495;&#22635;&#20805;&#24230;&#37327;&#30340;&#26377;&#25928;&#24615;&#12289;&#20219;&#21153;&#25968;&#25454;&#38598;&#30340;&#36136;&#37327;&#20197;&#21450;&#27169;&#22411;&#23398;&#20064;&#33021;&#21147;&#30340;&#25285;&#24551;&#12290;</title><link>http://arxiv.org/abs/2212.09702</link><description>&lt;p&gt;
&#35770;&#25991;&#20449;&#24687;&#25552;&#21462;&#20013;&#30340;&#20107;&#20214;&#20010;&#20307;&#21270;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
On Event Individuation for Document-Level Information Extraction. (arXiv:2212.09702v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09702
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#38382;&#39064;&#9472;&#9472;&#20107;&#20214;&#20010;&#20307;&#21270;&#23545;&#20110;&#27169;&#26495;&#22635;&#20805;&#20219;&#21153;&#26159;&#21542;&#36866;&#29992;&#65292;&#36890;&#36807;&#27880;&#37322;&#30740;&#31350;&#21644;&#35823;&#24046;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#24341;&#21457;&#20102;&#23545;&#27169;&#26495;&#22635;&#20805;&#24230;&#37327;&#30340;&#26377;&#25928;&#24615;&#12289;&#20219;&#21153;&#25968;&#25454;&#38598;&#30340;&#36136;&#37327;&#20197;&#21450;&#27169;&#22411;&#23398;&#20064;&#33021;&#21147;&#30340;&#25285;&#24551;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20449;&#24687;&#25552;&#21462;&#31995;&#32479;&#22312;&#22788;&#29702;&#25972;&#20010;&#25991;&#20214;&#26041;&#38754;&#36234;&#26469;&#36234;&#29087;&#32451;&#65292;&#20256;&#32479;&#30340;&#27169;&#26495;&#22635;&#20805;&#20219;&#21153;&#20316;&#20026;&#25991;&#20214;&#32423;&#20449;&#24687;&#25552;&#21462;&#30340;&#22522;&#20934;&#20219;&#21153;&#20877;&#27425;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#20851;&#27880;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36136;&#30097;&#20102;&#27169;&#26495;&#22635;&#20805;&#20219;&#21153;&#22312;&#36825;&#26041;&#38754;&#30340;&#36866;&#29992;&#24615;&#12290;&#25105;&#20204;&#35748;&#20026;&#35813;&#20219;&#21153;&#35201;&#27714;&#23545;&#20107;&#20214;&#20010;&#20307;&#21270;&#38382;&#39064;&#25552;&#20379;&#26126;&#30830;&#30340;&#31572;&#26696;&#8212;&#8212;&#21363;&#21306;&#20998;&#19981;&#21516;&#30340;&#20107;&#20214;&#8212;&#8212;&#32780;&#21363;&#20351;&#26159;&#20154;&#31867;&#19987;&#23478;&#22312;&#36825;&#20010;&#38382;&#39064;&#19978;&#20063;&#23384;&#22312;&#20998;&#27495;&#12290;&#36890;&#36807;&#27880;&#37322;&#30740;&#31350;&#21644;&#35823;&#24046;&#20998;&#26512;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#24341;&#21457;&#20102;&#23545;&#27169;&#26495;&#22635;&#20805;&#24230;&#37327;&#30340;&#26377;&#25928;&#24615;&#12289;&#20219;&#21153;&#25968;&#25454;&#38598;&#30340;&#36136;&#37327;&#20197;&#21450;&#27169;&#22411;&#23398;&#20064;&#33021;&#21147;&#30340;&#25285;&#24551;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#21487;&#33021;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
As information extraction (IE) systems have grown more adept at processing whole documents, the classic task of template filling has seen renewed interest as benchmark for document-level IE. In this position paper, we call into question the suitability of template filling for this purpose. We argue that the task demands definitive answers to thorny questions of event individuation -- the problem of distinguishing distinct events -- about which even human experts disagree. Through an annotation study and error analysis, we show that this raises concerns about the usefulness of template filling metrics, the quality of datasets for the task, and the ability of models to learn it. Finally, we consider possible solutions.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#24191;&#20041;&#26799;&#24230;&#27969;&#31639;&#27861;&#65292;&#23427;&#33021;&#22815;&#22312;&#22266;&#23450;&#26102;&#38388;&#20869;&#25910;&#25947;&#21040;&#38750;&#20984;&#20989;&#25968;&#30340;&#26368;&#20248;&#35299;&#65292;&#24182;&#19988;&#33021;&#22815;&#24555;&#36895;&#36867;&#36920;&#38750;&#36864;&#21270;&#38797;&#28857;&#12290;</title><link>http://arxiv.org/abs/2212.03765</link><description>&lt;p&gt;
&#21487;&#35777;&#26126;&#22266;&#23450;&#26102;&#38388;&#25910;&#25947;&#21644;&#24555;&#36895;&#36867;&#36920;&#38750;&#36864;&#21270;&#38797;&#28857;&#30340;&#24191;&#20041;&#26799;&#24230;&#27969;
&lt;/p&gt;
&lt;p&gt;
Generalized Gradient Flows with Provable Fixed-Time Convergence and Fast Evasion of Non-Degenerate Saddle Points. (arXiv:2212.03765v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.03765
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#24191;&#20041;&#26799;&#24230;&#27969;&#31639;&#27861;&#65292;&#23427;&#33021;&#22815;&#22312;&#22266;&#23450;&#26102;&#38388;&#20869;&#25910;&#25947;&#21040;&#38750;&#20984;&#20989;&#25968;&#30340;&#26368;&#20248;&#35299;&#65292;&#24182;&#19988;&#33021;&#22815;&#24555;&#36895;&#36867;&#36920;&#38750;&#36864;&#21270;&#38797;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#26799;&#24230;&#30340;&#19968;&#38454;&#20984;&#20248;&#21270;&#31639;&#27861;&#22312;&#21508;&#31181;&#39046;&#22495;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#12290;&#21463;&#36830;&#32493;&#26102;&#38388;&#21160;&#21147;&#31995;&#32479;&#22266;&#23450;&#26102;&#38388;&#31283;&#23450;&#24615;&#29702;&#35770;&#30340;&#26368;&#26032;&#36827;&#23637;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#24191;&#20041;&#26694;&#26550;&#65292;&#29992;&#20110;&#35774;&#35745;&#20855;&#26377;&#26368;&#24378;&#25910;&#25947;&#24615;&#20445;&#35777;&#30340;&#21152;&#36895;&#20248;&#21270;&#31639;&#27861;&#65292;&#36825;&#20123;&#31639;&#27861;&#36827;&#19968;&#27493;&#36866;&#29992;&#20110;&#38750;&#20984;&#20989;&#25968;&#30340;&#23376;&#31867;&#12290;&#25105;&#20204;&#29305;&#21035;&#20171;&#32461;&#20102;GenFlow&#31639;&#27861;&#21450;&#20854;&#21160;&#37327;&#21464;&#20307;&#65292;&#23427;&#20204;&#21487;&#35777;&#26126;&#22312;&#22266;&#23450;&#26102;&#38388;&#20869;&#25910;&#25947;&#21040;&#28385;&#36275;Polyak-Lojasiewicz (PL)&#19981;&#31561;&#24335;&#30340;&#30446;&#26631;&#20989;&#25968;&#30340;&#26368;&#20248;&#35299;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;&#20855;&#26377;&#38750;&#36864;&#21270;&#38797;&#28857;&#30340;&#20989;&#25968;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#23545;&#20110;&#25552;&#20986;&#30340;GenFlow&#31639;&#27861;&#65292;&#36530;&#36991;&#36825;&#20123;&#38797;&#28857;&#25152;&#38656;&#30340;&#26102;&#38388;&#22312;&#25152;&#26377;&#21021;&#22987;&#26465;&#20214;&#19979;&#37117;&#26377;&#19968;&#33268;&#30340;&#19978;&#30028;&#12290;&#26368;&#21518;&#65292;&#23545;&#20110;&#26368;&#20248;&#35299;&#20026;&#38797;&#28857;&#30340;&#24378;&#20984;-&#24378;&#20985;&#26497;&#23567;&#26497;&#22823;&#38382;&#39064;&#65292;&#31867;&#20284;&#30340;&#26041;&#26696;&#34987;&#35777;&#26126;&#21487;&#20197;&#36798;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Gradient-based first-order convex optimization algorithms find widespread applicability in a variety of domains, including machine learning tasks. Motivated by the recent advances in fixed-time stability theory of continuous-time dynamical systems, we introduce a generalized framework for designing accelerated optimization algorithms with strongest convergence guarantees that further extend to a subclass of non-convex functions. In particular, we introduce the GenFlow algorithm and its momentum variant that provably converge to the optimal solution of objective functions satisfying the Polyak-{\L}ojasiewicz (PL) inequality in a fixed time. Moreover, for functions that admit non-degenerate saddle-points, we show that for the proposed GenFlow algorithm, the time required to evade these saddle-points is uniformly bounded for all initial conditions. Finally, for strongly convex-strongly concave minimax problems whose optimal solution is a saddle point, a similar scheme is shown to arrive a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;GNN&#27169;&#25311;&#39030;&#28857;&#38388;&#30456;&#20114;&#20316;&#29992;&#30340;&#33021;&#21147;&#65292;&#36890;&#36807;&#19968;&#20010;&#34987;&#31216;&#20026;&#20998;&#31163;&#31209;&#30340;&#24230;&#37327;&#26631;&#20934;&#26469;&#37327;&#21270;&#36825;&#31181;&#33021;&#21147;&#65292;&#32467;&#26524;&#34920;&#26126;&#27169;&#25311;&#30456;&#20114;&#20316;&#29992;&#30340;&#33021;&#21147;&#20027;&#35201;&#21462;&#20915;&#20110;&#20998;&#21306;&#30340;&#34892;&#36208;&#25351;&#25968;&#65292;&#21363;&#20174;&#20998;&#30028;&#32447;&#24320;&#22987;&#30340;&#34892;&#36208;&#25968;&#37327;&#65292;&#21516;&#26102;&#35774;&#35745;&#20102;&#19968;&#31181;&#21517;&#20026;WISA&#30340;&#36793;&#31232;&#30095;&#21270;&#31639;&#27861;&#20197;&#25552;&#39640;GNNs&#30340;&#22788;&#29702;&#25928;&#29575;&#21644;&#34920;&#36798;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2211.16494</link><description>&lt;p&gt;
&#20851;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#25311;&#39030;&#28857;&#38388;&#30456;&#20114;&#20316;&#29992;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Ability of Graph Neural Networks to Model Interactions Between Vertices. (arXiv:2211.16494v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.16494
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;GNN&#27169;&#25311;&#39030;&#28857;&#38388;&#30456;&#20114;&#20316;&#29992;&#30340;&#33021;&#21147;&#65292;&#36890;&#36807;&#19968;&#20010;&#34987;&#31216;&#20026;&#20998;&#31163;&#31209;&#30340;&#24230;&#37327;&#26631;&#20934;&#26469;&#37327;&#21270;&#36825;&#31181;&#33021;&#21147;&#65292;&#32467;&#26524;&#34920;&#26126;&#27169;&#25311;&#30456;&#20114;&#20316;&#29992;&#30340;&#33021;&#21147;&#20027;&#35201;&#21462;&#20915;&#20110;&#20998;&#21306;&#30340;&#34892;&#36208;&#25351;&#25968;&#65292;&#21363;&#20174;&#20998;&#30028;&#32447;&#24320;&#22987;&#30340;&#34892;&#36208;&#25968;&#37327;&#65292;&#21516;&#26102;&#35774;&#35745;&#20102;&#19968;&#31181;&#21517;&#20026;WISA&#30340;&#36793;&#31232;&#30095;&#21270;&#31639;&#27861;&#20197;&#25552;&#39640;GNNs&#30340;&#22788;&#29702;&#25928;&#29575;&#21644;&#34920;&#36798;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;(GNNs)&#34987;&#24191;&#27867;&#29992;&#20110;&#24314;&#27169;&#30001;&#22270;&#20013;&#39030;&#28857;&#34920;&#31034;&#30340;&#23454;&#20307;&#20043;&#38388;&#30340;&#22797;&#26434;&#20114;&#21160;&#12290;&#23613;&#31649;&#26368;&#36817;&#26377;&#19968;&#20123;&#29702;&#35770;&#20998;&#26512;GNNs&#34920;&#36798;&#33021;&#21147;&#30340;&#21162;&#21147;&#65292;&#20294;&#23545;&#20854;&#27169;&#25311;&#30456;&#20114;&#20316;&#29992;&#30340;&#33021;&#21147;&#32570;&#20047;&#19968;&#20010;&#27491;&#24335;&#30340;&#25551;&#36848;&#12290;&#26412;&#25991;&#26088;&#22312;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#36890;&#36807;&#19968;&#20010;&#24050;&#30693;&#30340;&#24230;&#37327;&#26631;&#20934;&#8212;&#8212;&#20998;&#31163;&#31209;(separation rank)&#26469;&#35268;&#33539;&#21270;&#30456;&#20114;&#20316;&#29992;&#30340;&#24378;&#24230;&#65292;&#25105;&#20204;&#37327;&#21270;&#20102;&#26576;&#20123;GNNs&#27169;&#25311;&#32473;&#23450;&#39030;&#28857;&#23376;&#38598;&#21450;&#20854;&#34917;&#38598;&#20043;&#38388;&#20132;&#20114;&#30340;&#33021;&#21147;&#65292;&#21363;&#36755;&#20837;&#39030;&#28857;&#32452;&#25104;&#30340;&#32473;&#23450;&#20998;&#21306;&#30340;&#20004;&#20391;&#20043;&#38388;&#30340;&#20114;&#21160;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#27169;&#25311;&#30456;&#20114;&#20316;&#29992;&#30340;&#33021;&#21147;&#20027;&#35201;&#21462;&#20915;&#20110;&#20998;&#21306;&#30340;&#34892;&#36208;&#25351;&#25968;(walk index)&#8212;&#8212;&#19968;&#20010;&#30001;&#20998;&#30028;&#32447;&#24320;&#22987;&#30340;&#34892;&#36208;&#25968;&#37327;&#23450;&#20041;&#30340;&#22270;&#24418;&#29305;&#24449;&#12290;&#24120;&#35265;GNN&#26550;&#26500;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#19968;&#21457;&#29616;&#12290;&#20316;&#20026;&#25105;&#20204;&#29702;&#35770;&#30340;&#23454;&#38469;&#24212;&#29992;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#21517;&#20026;Walk Indexed Sparsification Algorithm (WISA)&#30340;&#36793;&#31232;&#30095;&#21270;&#31639;&#27861;&#65292;&#21033;&#29992;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#25552;&#39640;&#22788;&#29702;&#22823;&#35268;&#27169;&#22270;&#24418;&#30340;GNNs&#25928;&#29575;&#21516;&#26102;&#20445;&#25345;&#23427;&#20204;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks (GNNs) are widely used for modeling complex interactions between entities represented as vertices of a graph. Despite recent efforts to theoretically analyze the expressive power of GNNs, a formal characterization of their ability to model interactions is lacking. The current paper aims to address this gap. Formalizing strength of interactions through an established measure known as separation rank, we quantify the ability of certain GNNs to model interaction between a given subset of vertices and its complement, i.e. between the sides of a given partition of input vertices. Our results reveal that the ability to model interaction is primarily determined by the partition's walk index -- a graph-theoretical characteristic defined by the number of walks originating from the boundary of the partition. Experiments with common GNN architectures corroborate this finding. As a practical application of our theory, we design an edge sparsification algorithm named Walk Inde
&lt;/p&gt;</description></item><item><title>SciRepEval&#26159;&#31532;&#19968;&#20010;&#32508;&#21512;&#35780;&#20272;&#31185;&#23398;&#25991;&#29486;&#34920;&#31034;&#30340;&#20840;&#38754;&#22522;&#20934;&#65292;&#20854;&#20013;&#21253;&#25324;&#22235;&#31181;&#26684;&#24335;&#30340; 25 &#20010;&#20219;&#21153;&#12290;&#36890;&#36807;&#20351;&#29992;&#26684;&#24335;&#29305;&#23450;&#30340;&#25511;&#21046;&#20195;&#30721;&#21644;&#36866;&#37197;&#22120;&#65292;&#21487;&#20197;&#25913;&#36827;&#31185;&#23398;&#25991;&#29486;&#34920;&#31034;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2211.13308</link><description>&lt;p&gt;
SciRepEval&#65306;&#19968;&#20010;&#29992;&#20110;&#31185;&#23398;&#25991;&#29486;&#34920;&#31034;&#30340;&#22810;&#26684;&#24335;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
SciRepEval: A Multi-Format Benchmark for Scientific Document Representations. (arXiv:2211.13308v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.13308
&lt;/p&gt;
&lt;p&gt;
SciRepEval&#26159;&#31532;&#19968;&#20010;&#32508;&#21512;&#35780;&#20272;&#31185;&#23398;&#25991;&#29486;&#34920;&#31034;&#30340;&#20840;&#38754;&#22522;&#20934;&#65292;&#20854;&#20013;&#21253;&#25324;&#22235;&#31181;&#26684;&#24335;&#30340; 25 &#20010;&#20219;&#21153;&#12290;&#36890;&#36807;&#20351;&#29992;&#26684;&#24335;&#29305;&#23450;&#30340;&#25511;&#21046;&#20195;&#30721;&#21644;&#36866;&#37197;&#22120;&#65292;&#21487;&#20197;&#25913;&#36827;&#31185;&#23398;&#25991;&#29486;&#34920;&#31034;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#30340;&#31185;&#23398;&#25991;&#29486;&#34920;&#31034;&#21487;&#20197;&#20316;&#20026;&#19979;&#28216;&#20219;&#21153;&#30340;&#26377;&#20215;&#20540;&#36755;&#20837;&#29305;&#24449;&#65292;&#26080;&#38656;&#36827;&#19968;&#27493;&#24494;&#35843;&#12290;&#28982;&#32780;&#65292;&#29992;&#20110;&#35780;&#20272;&#36825;&#20123;&#34920;&#31034;&#30340;&#29616;&#26377;&#22522;&#20934;&#26410;&#33021;&#25429;&#25417;&#21040;&#30456;&#20851;&#20219;&#21153;&#30340;&#22810;&#26679;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102; SciRepEval&#65292;&#31532;&#19968;&#20010;&#29992;&#20110;&#35757;&#32451;&#21644;&#35780;&#20272;&#31185;&#23398;&#25991;&#29486;&#34920;&#31034;&#30340;&#20840;&#38754;&#22522;&#20934;&#12290;&#23427;&#21253;&#25324;&#22235;&#31181;&#26684;&#24335;&#30340; 25 &#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#21644;&#29616;&#23454;&#24615;&#30340;&#20219;&#21153;&#65292;&#20854;&#20013; 11 &#20010;&#26159;&#26032;&#20219;&#21153;&#65306;&#20998;&#31867;&#12289;&#22238;&#24402;&#12289;&#25490;&#21517;&#21644;&#25628;&#32034;&#12290;&#25105;&#20204;&#20351;&#29992;&#35813;&#22522;&#20934;&#26469;&#30740;&#31350;&#21644;&#25913;&#36827;&#31185;&#23398;&#25991;&#26723;&#34920;&#31034;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#22914;&#20309;&#22312;&#20219;&#21153;&#26684;&#24335;&#26041;&#38754;&#32570;&#20047;&#27867;&#21270;&#24615;&#33021;&#65292;&#31616;&#21333;&#30340;&#22810;&#20219;&#21153;&#35757;&#32451;&#20063;&#19981;&#33021;&#25913;&#36827;&#23427;&#20204;&#12290;&#28982;&#32780;&#65292;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#23398;&#20064;&#27599;&#20010;&#25991;&#26723;&#30340;&#22810;&#20010;&#23884;&#20837;&#65292;&#27599;&#20010;&#23884;&#20837;&#19987;&#38376;&#38024;&#23545;&#19981;&#21516;&#30340;&#26684;&#24335;&#65292;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;&#25105;&#20204;&#23581;&#35797;&#20351;&#29992;&#20219;&#21153;&#26684;&#24335;&#29305;&#23450;&#30340;&#25511;&#21046;&#20195;&#30721;&#21644;&#36866;&#37197;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learned representations of scientific documents can serve as valuable input features for downstream tasks, without the need for further fine-tuning. However, existing benchmarks for evaluating these representations fail to capture the diversity of relevant tasks. In response, we introduce SciRepEval, the first comprehensive benchmark for training and evaluating scientific document representations. It includes 25 challenging and realistic tasks, 11 of which are new, across four formats: classification, regression, ranking and search. We then use the benchmark to study and improve the generalization ability of scientific document representation models. We show how state-of-the-art models struggle to generalize across task formats, and that simple multi-task training fails to improve them. However, a new approach that learns multiple embeddings per document, each tailored to a different format, can improve performance. We experiment with task-format-specific control codes and adapters in 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21363;"&#24605;&#32500;&#31243;&#24207;"&#65288;PoT&#65289;&#65292;&#36890;&#36807;&#23558;&#25512;&#29702;&#36807;&#31243;&#34920;&#36798;&#20026;&#19968;&#20010;&#31243;&#24207;&#65292;&#23558;&#35745;&#31639;&#19982;&#25512;&#29702;&#36807;&#31243;&#20998;&#31163;&#24320;&#26469;&#65292;&#20197;&#25552;&#21319;&#35299;&#20915;&#25968;&#20540;&#25512;&#29702;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#35780;&#20272;&#20013;&#65292;PoT&#30456;&#27604;&#26368;&#26032;&#26041;&#27861;CoT&#22312;&#24615;&#33021;&#19978;&#24179;&#22343;&#25552;&#39640;&#20102;&#32422;12&#65285;&#12290;</title><link>http://arxiv.org/abs/2211.12588</link><description>&lt;p&gt;
&#24605;&#32500;&#20419;&#36827;&#31243;&#24207;&#65306;&#20026;&#35299;&#20915;&#25968;&#20540;&#25512;&#29702;&#20219;&#21153;&#23558;&#35745;&#31639;&#19982;&#25512;&#29702;&#20998;&#31163;&#24320;&#26469;
&lt;/p&gt;
&lt;p&gt;
Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks. (arXiv:2211.12588v4 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.12588
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21363;"&#24605;&#32500;&#31243;&#24207;"&#65288;PoT&#65289;&#65292;&#36890;&#36807;&#23558;&#25512;&#29702;&#36807;&#31243;&#34920;&#36798;&#20026;&#19968;&#20010;&#31243;&#24207;&#65292;&#23558;&#35745;&#31639;&#19982;&#25512;&#29702;&#36807;&#31243;&#20998;&#31163;&#24320;&#26469;&#65292;&#20197;&#25552;&#21319;&#35299;&#20915;&#25968;&#20540;&#25512;&#29702;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#35780;&#20272;&#20013;&#65292;PoT&#30456;&#27604;&#26368;&#26032;&#26041;&#27861;CoT&#22312;&#24615;&#33021;&#19978;&#24179;&#22343;&#25552;&#39640;&#20102;&#32422;12&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22312;&#25945;&#25480;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22797;&#26434;&#25968;&#20540;&#25512;&#29702;&#20219;&#21153;&#30340;&#36880;&#27493;&#25512;&#29702;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#24605;&#32500;&#38142;&#25552;&#31034;&#65288;CoT&#65289;&#26159;&#30446;&#21069;&#36825;&#20123;&#20219;&#21153;&#30340;&#26368;&#26032;&#26041;&#27861;&#12290;CoT&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#26469;&#25191;&#34892;&#22810;&#27493;&#39588;&#30340;&#25512;&#29702;&#21644;&#35745;&#31639;&#36807;&#31243;&#12290;&#20026;&#20102;&#23558;&#35745;&#31639;&#19982;&#25512;&#29702;&#20998;&#31163;&#24320;&#26469;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;"&#24605;&#32500;&#31243;&#24207;"&#65288;PoT&#65289;&#65292;&#23427;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#65288;&#20027;&#35201;&#26159;Codex&#65289;&#23558;&#25512;&#29702;&#36807;&#31243;&#34920;&#36798;&#20026;&#19968;&#20010;&#31243;&#24207;&#12290;&#35745;&#31639;&#36807;&#31243;&#34987;&#22996;&#25176;&#32473;&#22806;&#37096;&#35745;&#31639;&#26426;&#25191;&#34892;&#29983;&#25104;&#30340;&#31243;&#24207;&#20197;&#24471;&#20986;&#31572;&#26696;&#12290;&#25105;&#20204;&#22312;&#20116;&#20010;&#25968;&#23398;&#38382;&#39064;&#25968;&#25454;&#38598;&#65288;GSM&#65292;AQuA&#65292;SVAMP&#65292;TabMWP&#65292;MultiArith&#65289;&#21644;&#19977;&#20010;&#37329;&#34701;&#38382;&#31572;&#25968;&#25454;&#38598;&#65288;FinQA&#65292;ConvFinQA&#65292;TATQA&#65289;&#19978;&#35780;&#20272;&#20102;PoT&#30340;&#34920;&#29616;&#65292;&#21253;&#25324;&#23569;&#26679;&#26412;&#21644;&#38646;&#26679;&#26412;&#35774;&#32622;&#12290;&#22312;&#23569;&#26679;&#26412;&#21644;&#38646;&#26679;&#26412;&#35774;&#32622;&#19979;&#65292;PoT&#30340;&#24179;&#22343;&#24615;&#33021;&#25552;&#21319;&#32422;&#20026;12&#65285;&#65292;&#22312;&#25152;&#26377;&#35780;&#20272;&#30340;&#25968;&#25454;&#38598;&#19978;&#12290;&#36890;&#36807;&#23558;PoT&#19982;&#33258;&#19968;&#33268;&#24615;&#32452;&#21512;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#22312;&#25968;&#20540;&#25512;&#29702;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, there has been significant progress in teaching language models to perform step-by-step reasoning to solve complex numerical reasoning tasks. Chain-of-thoughts prompting (CoT) is by far the state-of-art method for these tasks. CoT uses language models to perform both reasoning and computation in the multi-step `thought' process. To disentangle computation from reasoning, we propose `Program of Thoughts' (PoT), which uses language models (mainly Codex) to express the reasoning process as a program. The computation is relegated to an external computer, which executes the generated programs to derive the answer. We evaluate PoT on five math word problem datasets (GSM, AQuA, SVAMP, TabMWP, MultiArith) and three financial-QA datasets (FinQA, ConvFinQA, TATQA) for both few-shot and zero-shot setups. Under both few-shot and zero-shot settings, PoT can show an average performance gain over CoT by around 12\% across all the evaluated datasets. By combining PoT with self-consistency de
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;[RE]VER&#30340;&#31995;&#32479;&#65292;&#20351;&#29992;&#22522;&#20110;transformer&#30340;&#27169;&#22411;&#26469;&#23398;&#20064;&#23454;&#20307;&#21644;&#20851;&#31995;&#30340;&#33258;&#28982;&#35821;&#35328;&#34920;&#31034;&#65292;&#33021;&#22815;&#29983;&#25104;&#19968;&#20010;&#33021;&#22815;&#34920;&#31034;&#23454;&#20307;&#19982;&#20854;&#20182;&#23454;&#20307;&#20851;&#31995;&#30340;&#21477;&#23376;&#65292;&#30456;&#27604;&#20110;&#20043;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#26377;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2211.11093</link><description>&lt;p&gt;
[RE]VER&#65306;&#23398;&#20064;&#33258;&#28982;&#35821;&#35328;&#34920;&#31034;&#20197;&#38416;&#36848;&#23454;&#20307;&#21644;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
[RE]VER: Learning Natural Language Representations for Verbalizing Entities and Relations. (arXiv:2211.11093v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.11093
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;[RE]VER&#30340;&#31995;&#32479;&#65292;&#20351;&#29992;&#22522;&#20110;transformer&#30340;&#27169;&#22411;&#26469;&#23398;&#20064;&#23454;&#20307;&#21644;&#20851;&#31995;&#30340;&#33258;&#28982;&#35821;&#35328;&#34920;&#31034;&#65292;&#33021;&#22815;&#29983;&#25104;&#19968;&#20010;&#33021;&#22815;&#34920;&#31034;&#23454;&#20307;&#19982;&#20854;&#20182;&#23454;&#20307;&#20851;&#31995;&#30340;&#21477;&#23376;&#65292;&#30456;&#27604;&#20110;&#20043;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#26377;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#20307;&#21450;&#23454;&#20307;&#20043;&#38388;&#30340;&#20851;&#31995;&#26159;&#29616;&#23454;&#19990;&#30028;&#20013;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#20154;&#20204;&#36890;&#36807;&#29702;&#35299;&#23454;&#20307;&#21644;&#20851;&#31995;&#26469;&#20102;&#35299;&#19990;&#30028;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;[RE]VER&#30340;&#31995;&#32479;&#65292;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;transformer&#30340;&#27169;&#22411;&#26469;&#23398;&#20064;&#23454;&#20307;&#21644;&#20851;&#31995;&#30340;&#33258;&#28982;&#35821;&#35328;&#34920;&#31034;&#65292;&#24182;&#29983;&#25104;&#19968;&#20010;&#33021;&#22815;&#34920;&#31034;&#23454;&#20307;&#19982;&#20854;&#20182;&#23454;&#20307;&#20851;&#31995;&#30340;&#21477;&#23376;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#65292;&#24182;&#35777;&#26126;&#20854;&#30456;&#27604;&#20110;&#20043;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#26377;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Entities and relationships between entities are vital in the real world. Essentially, we understand the world by understanding entities and relations. For instance, to understand a field, e.g., computer science, we need to understand the relevant concepts, e.g., machine learning, and the relationships between concepts, e.g., machine learning and artificial intelligence. To understand a person, we should first know who he/she is and how he/she is related to others. To understand entities and relations, humans may refer to natural language descriptions. For instance, when learning a new scientific term, people usually start by reading its definition in dictionaries or encyclopedias. To know the relationship between two entities, humans tend to create a sentence to connect them. In this paper, we propose [RE]VER: A Unified Model for Verbalizing Entities and Relations. Specifically, we attempt to build a system that takes any entity or entity set as input and generates a sentence to repres
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#24658;&#23450;&#20869;&#23384;&#38656;&#27714;&#25193;&#23637;&#25968;&#25454;&#38598;&#31934;&#31616;&#30340;&#26041;&#27861;&#65292;&#21487;&#23558;Matching Training Trajectories&#65288;MTT&#65289;&#24212;&#29992;&#20110;ImageNet-1K&#25968;&#25454;&#38598;&#65292;&#36798;&#21040;6&#20493;&#30340;&#20869;&#23384;&#38477;&#20302;&#65292;&#21516;&#26102;&#22686;&#21152;&#20102;&#32422;2%&#30340;&#36816;&#34892;&#26102;&#24320;&#38144;&#12290;&#21516;&#26102;&#20063;&#21457;&#29616;&#65292;&#20026;&#21512;&#25104;&#22270;&#20687;&#20998;&#37197;&#36719;&#26631;&#31614;&#23545;&#20110;&#23454;&#29616;&#33391;&#22909;&#30340;&#24615;&#33021;&#38750;&#24120;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2211.10586</link><description>&lt;p&gt;
&#21033;&#29992;&#24658;&#23450;&#20869;&#23384;&#23558;&#25968;&#25454;&#38598;&#31934;&#31616;&#25193;&#23637;&#21040;ImageNet-1K
&lt;/p&gt;
&lt;p&gt;
Scaling Up Dataset Distillation to ImageNet-1K with Constant Memory. (arXiv:2211.10586v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.10586
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#24658;&#23450;&#20869;&#23384;&#38656;&#27714;&#25193;&#23637;&#25968;&#25454;&#38598;&#31934;&#31616;&#30340;&#26041;&#27861;&#65292;&#21487;&#23558;Matching Training Trajectories&#65288;MTT&#65289;&#24212;&#29992;&#20110;ImageNet-1K&#25968;&#25454;&#38598;&#65292;&#36798;&#21040;6&#20493;&#30340;&#20869;&#23384;&#38477;&#20302;&#65292;&#21516;&#26102;&#22686;&#21152;&#20102;&#32422;2%&#30340;&#36816;&#34892;&#26102;&#24320;&#38144;&#12290;&#21516;&#26102;&#20063;&#21457;&#29616;&#65292;&#20026;&#21512;&#25104;&#22270;&#20687;&#20998;&#37197;&#36719;&#26631;&#31614;&#23545;&#20110;&#23454;&#29616;&#33391;&#22909;&#30340;&#24615;&#33021;&#38750;&#24120;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#38598;&#31934;&#31616;&#26041;&#27861;&#26088;&#22312;&#23558;&#22823;&#22411;&#25968;&#25454;&#38598;&#21387;&#32553;&#25104;&#19968;&#23567;&#32452;&#21512;&#25104;&#26679;&#26412;&#65292;&#20351;&#24471;&#22312;&#35757;&#32451;&#26102;&#65292;&#19982;&#22312;&#25972;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24120;&#35268;&#35757;&#32451;&#30456;&#27604;&#65292;&#21487;&#20197;&#33719;&#24471;&#31454;&#20105;&#24615;&#30340;&#24615;&#33021;&#12290;&#22312;&#26368;&#36817;&#25552;&#20986;&#30340;&#26041;&#27861;&#20013;&#65292;&#21305;&#37197;&#35757;&#32451;&#36712;&#36857;&#65288;MTT&#65289;&#22312;CIFAR-10/100&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#20294;&#30001;&#20110;&#22312;&#21453;&#21521;&#20256;&#25773;&#36807;&#31243;&#20013;&#25191;&#34892;&#23637;&#24320;&#26799;&#24230;&#35745;&#31639;&#26102;&#38656;&#35201;&#22823;&#37327;&#20869;&#23384;&#65292;&#22240;&#27492;&#24456;&#38590;&#25193;&#23637;&#21040;ImageNet-1k&#25968;&#25454;&#38598;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#23384;&#22312;&#19968;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#20351;&#29992;&#24658;&#23450;&#30340;GPU&#20869;&#23384;&#38656;&#27714;&#65288;&#19982;&#23637;&#24320;&#27493;&#39588;&#30340;&#25968;&#37327;&#26080;&#20851;&#65289;&#31934;&#30830;&#35745;&#31639;&#36712;&#36857;&#21305;&#37197;&#25439;&#22833;&#20989;&#25968;&#30340;&#26799;&#24230;&#12290;&#26377;&#20102;&#36825;&#19968;&#21457;&#29616;&#65292;&#25152;&#25552;&#20986;&#30340;&#20869;&#23384;&#39640;&#25928;&#30340;&#36712;&#36857;&#21305;&#37197;&#26041;&#27861;&#21482;&#38656;&#35201;&#27604;&#21407;&#22987;MTT&#22810;&#32422;2&#65285;&#30340;&#36816;&#34892;&#26102;&#24320;&#38144;&#65292;&#21363;&#21487;&#36731;&#26494;&#25193;&#23637;&#21040;&#20855;&#26377;6&#20493;&#20869;&#23384;&#32553;&#20943;&#30340;ImageNet-1K&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#20026;&#21512;&#25104;&#22270;&#20687;&#20998;&#37197;&#36719;&#26631;&#31614;&#23545;&#20110;&#23454;&#29616;&#33391;&#22909;&#30340;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dataset distillation methods aim to compress a large dataset into a small set of synthetic samples, such that when being trained on, competitive performances can be achieved compared to regular training on the entire dataset. Among recently proposed methods, Matching Training Trajectories (MTT) achieves state-of-the-art performance on CIFAR-10/100, while having difficulty scaling to ImageNet-1k dataset due to the large memory requirement when performing unrolled gradient computation through back-propagation. Surprisingly, we show that there exists a procedure to exactly calculate the gradient of the trajectory matching loss with constant GPU memory requirement (irrelevant to the number of unrolled steps). With this finding, the proposed memory-efficient trajectory matching method can easily scale to ImageNet-1K with 6x memory reduction while introducing only around 2% runtime overhead than original MTT. Further, we find that assigning soft labels for synthetic images is crucial for the
&lt;/p&gt;</description></item><item><title>CAPE&#26159;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20174;&#21069;&#32622;&#38169;&#35823;&#20013;&#32416;&#27491;&#34892;&#21160;&#30340;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#29983;&#25104;&#35745;&#21010;&#30340;&#36136;&#37327;&#65292;&#20351;&#20855;&#36523;&#20195;&#29702;&#33021;&#22815;&#25191;&#34892;&#26356;&#22810;&#20219;&#21153;&#65292;&#24182;&#25913;&#21892;&#20102;&#35745;&#21010;&#30340;&#27491;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2211.09935</link><description>&lt;p&gt;
CAPE: &#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20174;&#21069;&#32622;&#38169;&#35823;&#20013;&#32416;&#27491;&#34892;&#21160;
&lt;/p&gt;
&lt;p&gt;
CAPE: Corrective Actions from Precondition Errors using Large Language Models. (arXiv:2211.09935v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.09935
&lt;/p&gt;
&lt;p&gt;
CAPE&#26159;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20174;&#21069;&#32622;&#38169;&#35823;&#20013;&#32416;&#27491;&#34892;&#21160;&#30340;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#29983;&#25104;&#35745;&#21010;&#30340;&#36136;&#37327;&#65292;&#20351;&#20855;&#36523;&#20195;&#29702;&#33021;&#22815;&#25191;&#34892;&#26356;&#22810;&#20219;&#21153;&#65292;&#24182;&#25913;&#21892;&#20102;&#35745;&#21010;&#30340;&#27491;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#21462;&#24120;&#35782;&#30693;&#35782;&#20026;&#35774;&#35745;&#26234;&#33021;&#26426;&#22120;&#20154;&#25552;&#20379;&#20102;&#19968;&#31181;&#36884;&#24452;&#12290;&#29616;&#26377;&#30340;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35268;&#21010;&#30340;&#26041;&#27861;&#22312;&#34892;&#21160;&#22833;&#36133;&#26102;&#26080;&#27861;&#24674;&#22797;&#65292;&#24182;&#19988;&#36890;&#24120;&#21482;&#33021;&#23581;&#35797;&#37325;&#26032;&#25191;&#34892;&#22833;&#36133;&#30340;&#34892;&#21160;&#65292;&#32780;&#26080;&#27861;&#35299;&#20915;&#38169;&#35823;&#30340;&#26681;&#26412;&#21407;&#22240;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65288;CAPE&#65289;&#65292;&#35797;&#22270;&#22312;&#35268;&#21010;&#36807;&#31243;&#20013;&#25552;&#20986;&#32416;&#27491;&#21069;&#32622;&#26465;&#20214;&#38169;&#35823;&#30340;&#34892;&#21160;&#12290;CAPE&#36890;&#36807;&#21033;&#29992;&#23569;&#26679;&#26412;&#25512;&#29702;&#20174;&#34892;&#21160;&#21069;&#32622;&#26465;&#20214;&#20013;&#25552;&#39640;&#20102;&#29983;&#25104;&#35745;&#21010;&#30340;&#36136;&#37327;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#24471;&#20855;&#36523;&#20195;&#29702;&#33021;&#22815;&#25191;&#34892;&#27604;&#22522;&#32447;&#26041;&#27861;&#26356;&#22810;&#30340;&#20219;&#21153;&#65292;&#21516;&#26102;&#30830;&#20445;&#35821;&#20041;&#27491;&#30830;&#24615;&#21644;&#26368;&#23567;&#21270;&#37325;&#26032;&#25552;&#31034;&#12290;&#22312;VirtualHome&#20013;&#65292;CAPE&#29983;&#25104;&#21487;&#25191;&#34892;&#30340;&#35745;&#21010;&#65292;&#24182;&#19988;&#30456;&#27604;SayCan&#65292;&#23558;&#20154;&#24037;&#26631;&#27880;&#30340;&#35745;&#21010;&#27491;&#30830;&#24230;&#25351;&#26631;&#20174;28.89%&#25552;&#39640;&#21040;49.63%&#12290;&#25105;&#20204;&#30340;&#25913;&#36827;&#20063;&#36866;&#29992;&#20110;&#19968;&#21488;&#37197;&#32622;&#20102;&#19968;&#32452;&#20197;&#35821;&#35328;&#20026;&#25351;&#23450;&#30340;&#25216;&#33021;&#21644;&#30456;&#20851;&#21069;&#32622;&#26465;&#20214;&#30340;&#27874;&#22763;&#39039;&#21160;&#21147;&#20844;&#21496;&#30340;Spot&#26426;&#22120;&#20154;&#65292;&#20854;&#20013;CAPE&#25552;&#39640;&#20102;&#27491;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Extracting commonsense knowledge from a large language model (LLM) offers a path to designing intelligent robots. Existing approaches that leverage LLMs for planning are unable to recover when an action fails and often resort to retrying failed actions, without resolving the error's underlying cause.  We propose a novel approach (CAPE) that attempts to propose corrective actions to resolve precondition errors during planning. CAPE improves the quality of generated plans by leveraging few-shot reasoning from action preconditions. Our approach enables embodied agents to execute more tasks than baseline methods while ensuring semantic correctness and minimizing re-prompting. In VirtualHome, CAPE generates executable plans while improving a human-annotated plan correctness metric from 28.89% to 49.63% over SayCan. Our improvements transfer to a Boston Dynamics Spot robot initialized with a set of skills (specified in language) and associated preconditions, where CAPE improves the correctne
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#23545;&#27604;&#23398;&#20064;&#21644;&#25968;&#23383;&#35777;&#25454;&#30340;&#26041;&#27861;&#25913;&#36827;&#28151;&#28102;&#30340;&#27861;&#24459;&#21028;&#20915;&#39044;&#27979;&#12290;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#21644;&#21033;&#29992;&#25968;&#23383;&#35777;&#25454;&#39044;&#27979;&#22788;&#32602;&#26399;&#38480;&#65292;&#25104;&#21151;&#22320;&#35299;&#20915;&#20102;&#21306;&#20998;&#20998;&#31867;&#38169;&#35823;&#21644;&#21033;&#29992;&#25968;&#23383;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2211.08238</link><description>&lt;p&gt;
&#21033;&#29992;&#23545;&#27604;&#23398;&#20064;&#21644;&#25968;&#23383;&#35777;&#25454;&#25913;&#36827;&#28151;&#28102;&#30340;&#27861;&#24459;&#21028;&#20915;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Exploiting Contrastive Learning and Numerical Evidence for Improving Confusing Legal Judgment Prediction. (arXiv:2211.08238v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.08238
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#23545;&#27604;&#23398;&#20064;&#21644;&#25968;&#23383;&#35777;&#25454;&#30340;&#26041;&#27861;&#25913;&#36827;&#28151;&#28102;&#30340;&#27861;&#24459;&#21028;&#20915;&#39044;&#27979;&#12290;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#21644;&#21033;&#29992;&#25968;&#23383;&#35777;&#25454;&#39044;&#27979;&#22788;&#32602;&#26399;&#38480;&#65292;&#25104;&#21151;&#22320;&#35299;&#20915;&#20102;&#21306;&#20998;&#20998;&#31867;&#38169;&#35823;&#21644;&#21033;&#29992;&#25968;&#23383;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#19968;&#20010;&#27861;&#24459;&#26696;&#20363;&#30340;&#20107;&#23454;&#25551;&#36848;&#25991;&#26412;&#65292;&#27861;&#24459;&#21028;&#20915;&#39044;&#27979;&#65288;LJP&#65289;&#26088;&#22312;&#39044;&#27979;&#26696;&#20363;&#30340;&#32618;&#21517;&#12289;&#27861;&#24459;&#26465;&#27454;&#21644;&#22788;&#32602;&#26399;&#38480;&#12290;LJP&#30340;&#19968;&#20010;&#26680;&#24515;&#38382;&#39064;&#26159;&#22914;&#20309;&#21306;&#20998;&#28151;&#28102;&#30340;&#27861;&#24459;&#26696;&#20363;&#65292;&#20854;&#20013;&#21482;&#23384;&#22312;&#24494;&#22937;&#30340;&#25991;&#26412;&#24046;&#24322;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#22312;&#20351;&#29992;&#26631;&#20934;&#30340;&#20132;&#21449;&#29109;&#20998;&#31867;&#25439;&#22833;&#26080;&#27861;&#21306;&#20998;&#19981;&#21516;&#30340;&#20998;&#31867;&#38169;&#35823;&#65292;&#24182;&#24573;&#30053;&#20102;&#20107;&#23454;&#25551;&#36848;&#20013;&#30340;&#25968;&#23383;&#65292;&#29992;&#20110;&#39044;&#27979;&#22788;&#32602;&#26399;&#38480;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;moco&#30340;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#65292;&#20197;&#23398;&#20064;&#21487;&#21306;&#20998;&#30340;&#34920;&#31034;&#65292;&#24182;&#25506;&#32034;&#26500;&#24314;&#27491;&#20363;&#23545;&#30340;&#26368;&#20339;&#31574;&#30053;&#65292;&#20174;&#32780;&#21516;&#26102;&#26377;&#21033;&#20110;LJP&#30340;&#19977;&#20010;&#23376;&#20219;&#21153;&#12290;&#20854;&#27425;&#65292;&#20026;&#20102;&#21033;&#29992;&#27861;&#24459;&#26696;&#20363;&#20013;&#30340;&#25968;&#23383;&#26469;&#39044;&#27979;&#26576;&#20123;&#26696;&#20363;&#30340;&#22788;&#32602;&#26399;&#38480;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#22686;&#24378;&#20102;&#30001;&#39044;&#35757;&#32451;&#25968;&#20540;&#27169;&#22411;&#32534;&#30721;&#30340;&#25552;&#21462;&#30340;&#29359;&#32618;&#37329;&#39069;&#23545;&#20107;&#23454;&#25551;&#36848;&#30340;&#34920;&#31034;&#12290;&#23545;&#20844;&#24320;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given the fact description text of a legal case, legal judgment prediction (LJP) aims to predict the case's charge, law article and penalty term. A core problem of LJP is how to distinguish confusing legal cases, where only subtle text differences exist. Previous studies fail to distinguish different classification errors with a standard cross-entropy classification loss, and ignore the numbers in the fact description for predicting the term of penalty. To tackle these issues, in this work, first, we propose a moco-based supervised contrastive learning to learn distinguishable representations, and explore the best strategy to construct positive example pairs to benefit all three subtasks of LJP simultaneously. Second, in order to exploit the numbers in legal cases for predicting the penalty terms of certain cases, we further enhance the representation of the fact description with extracted crime amounts which are encoded by a pre-trained numeracy model. Extensive experiments on public 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#34701;&#21512;&#22270;&#26469;&#35774;&#35745;&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#32452;&#20214;&#65292;&#20026;&#35299;&#20915;&#28041;&#21450;&#20840;&#23616;&#31354;&#38388;&#21644;&#32622;&#25442;&#23545;&#31216;&#24615;&#30340;&#23398;&#20064;&#20219;&#21153;&#25552;&#20379;&#20102;&#19968;&#31181;&#22270;&#24418;&#21270;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2211.07482</link><description>&lt;p&gt;
&#29992;&#24352;&#37327;&#32593;&#32476;&#24418;&#24335;&#32479;&#19968;O(3)&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Unifying O(3) Equivariant Neural Networks Design with Tensor-Network Formalism. (arXiv:2211.07482v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.07482
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#34701;&#21512;&#22270;&#26469;&#35774;&#35745;&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#32452;&#20214;&#65292;&#20026;&#35299;&#20915;&#28041;&#21450;&#20840;&#23616;&#31354;&#38388;&#21644;&#32622;&#25442;&#23545;&#31216;&#24615;&#30340;&#23398;&#20064;&#20219;&#21153;&#25552;&#20379;&#20102;&#19968;&#31181;&#22270;&#24418;&#21270;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#23398;&#20064;&#20219;&#21153;&#65292;&#21253;&#25324;&#20174;&#20174;&#31532;&#19968;&#21407;&#29702;&#35745;&#31639;&#20013;&#23398;&#20064;&#21183;&#33021;&#38754;&#65292;&#28041;&#21450;&#21040;&#20840;&#23616;&#31354;&#38388;&#23545;&#31216;&#24615;&#21644;&#21407;&#23376;&#25110;&#19968;&#33324;&#31890;&#23376;&#20043;&#38388;&#30340;&#32622;&#25442;&#23545;&#31216;&#24615;&#12290;&#31561;&#21464;&#22270;&#31070;&#32463;&#32593;&#32476;&#26159;&#35299;&#20915;&#36825;&#31867;&#38382;&#39064;&#30340;&#26631;&#20934;&#26041;&#27861;&#20043;&#19968;&#65292;&#20854;&#20013;&#26368;&#25104;&#21151;&#30340;&#26041;&#27861;&#20043;&#19968;&#26159;&#20351;&#29992;&#22312;&#31354;&#38388;&#32676;&#19979;&#21464;&#25442;&#30340;&#21508;&#31181;&#24352;&#37327;&#20043;&#38388;&#30340;&#24352;&#37327;&#31215;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#19981;&#21516;&#24352;&#37327;&#30340;&#25968;&#37327;&#21644;&#23427;&#20204;&#20043;&#38388;&#20851;&#31995;&#30340;&#22797;&#26434;&#24615;&#22686;&#21152;&#65292;&#20445;&#25345;&#31616;&#27905;&#21644;&#31561;&#21464;&#24615;&#21464;&#24471;&#36234;&#26469;&#36234;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#34701;&#21512;&#22270;&#65292;&#19968;&#31181;&#24191;&#27867;&#29992;&#20110;&#27169;&#25311;SU(2)&#23545;&#31216;&#37327;&#23376;&#22810;&#20307;&#38382;&#39064;&#30340;&#25216;&#26415;&#65292;&#26469;&#20026;&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;&#35774;&#35745;&#26032;&#30340;&#31561;&#21464;&#32452;&#20214;&#12290;&#36825;&#23548;&#33268;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#30340;&#26041;&#27861;&#26469;&#26500;&#24314;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#12290;&#24403;&#24212;&#29992;&#20110;&#32473;&#23450;&#23616;&#37096;&#37051;&#22495;&#20013;&#30340;&#31890;&#23376;&#26102;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#8220;&#34701;&#21512;&#22359;&#8221;&#30340;&#32467;&#26524;&#32452;&#20214;&#36215;&#21040;&#20102;
&lt;/p&gt;
&lt;p&gt;
Many learning tasks, including learning potential energy surfaces from ab initio calculations, involve global spatial symmetries and permutational symmetry between atoms or general particles. Equivariant graph neural networks are a standard approach to such problems, with one of the most successful methods employing tensor products between various tensors that transform under the spatial group. However, as the number of different tensors and the complexity of relationships between them increase, maintaining parsimony and equivariance becomes increasingly challenging. In this paper, we propose using fusion diagrams, a technique widely employed in simulating SU($2$)-symmetric quantum many-body problems, to design new equivariant components for equivariant neural networks. This results in a diagrammatic approach to constructing novel neural network architectures. When applied to particles within a given local neighborhood, the resulting components, which we term "fusion blocks," serve as 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#35780;&#20272;&#27969;&#31243;&#26469;&#27979;&#35797;&#27169;&#22411;&#23545;&#35270;&#35273;&#22330;&#26223;&#12289;&#25991;&#26412;&#21644;&#30456;&#20851;&#30693;&#35782;&#30340;&#29702;&#35299;&#33021;&#21147;&#65292;&#24182;&#34920;&#26126;&#20351;&#29992;&#35813;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#22312;&#26631;&#20934;&#35780;&#20272;&#20013;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2211.05895</link><description>&lt;p&gt;
&#29702;&#35299;ME&#65311;&#29992;&#20110;&#32454;&#31890;&#24230;&#35270;&#35273;&#24120;&#35782;&#30340;&#22810;&#27169;&#24577;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Understanding ME? Multimodal Evaluation for Fine-grained Visual Commonsense. (arXiv:2211.05895v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.05895
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#35780;&#20272;&#27969;&#31243;&#26469;&#27979;&#35797;&#27169;&#22411;&#23545;&#35270;&#35273;&#22330;&#26223;&#12289;&#25991;&#26412;&#21644;&#30456;&#20851;&#30693;&#35782;&#30340;&#29702;&#35299;&#33021;&#21147;&#65292;&#24182;&#34920;&#26126;&#20351;&#29992;&#35813;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#22312;&#26631;&#20934;&#35780;&#20272;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#24120;&#35782;&#29702;&#35299;&#35201;&#27714;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#19981;&#20165;&#35201;&#29702;&#35299;&#22270;&#20687;&#21644;&#25991;&#26412;&#65292;&#36824;&#35201;&#22312;&#20004;&#32773;&#20043;&#38388;&#36827;&#34892;&#20114;&#30456;&#21442;&#32771;&#65292;&#20197;&#23436;&#20840;&#34701;&#21512;&#21644;&#29702;&#35299;&#25152;&#25551;&#36848;&#30340;&#35270;&#35273;&#22330;&#26223;&#12290;&#26368;&#36817;&#65292;&#24050;&#32463;&#24320;&#21457;&#20102;&#21508;&#31181;&#26041;&#27861;&#65292;&#24182;&#22312;&#35270;&#35273;&#24120;&#35782;&#22522;&#20934;&#19978;&#21462;&#24471;&#20102;&#39640;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#26377;&#38480;&#30340;&#35780;&#20272;&#25968;&#25454;&#36164;&#28304;&#65292;&#25105;&#20204;&#26080;&#27861;&#30830;&#23450;&#27169;&#22411;&#26159;&#21542;&#30495;&#27491;&#29702;&#35299;&#35270;&#35273;&#22330;&#26223;&#21644;&#28508;&#22312;&#30340;&#24120;&#35782;&#30693;&#35782;&#12290;&#20026;&#20102;&#25552;&#20379;&#28145;&#20837;&#30340;&#20998;&#26512;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#35780;&#20272;&#65288;ME&#65289;&#27969;&#31243;&#65292;&#33258;&#21160;&#29983;&#25104;&#38382;&#39064;-&#31572;&#26696;&#23545;&#26469;&#27979;&#35797;&#27169;&#22411;&#23545;&#35270;&#35273;&#22330;&#26223;&#12289;&#25991;&#26412;&#21644;&#30456;&#20851;&#30693;&#35782;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;&#25105;&#20204;&#36824;&#36827;&#19968;&#27493;&#23637;&#31034;&#65292;&#20351;&#29992;ME&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#22312;&#26631;&#20934;VCR&#35780;&#20272;&#20013;&#30340;&#24615;&#33021;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#30340;&#28145;&#20837;&#20998;&#26512;&#21644;&#27604;&#36739;&#25581;&#31034;&#20102;&#19968;&#20123;&#26377;&#36259;&#30340;&#21457;&#29616;&#65306;&#65288;1&#65289;&#35821;&#20041;&#20302;&#32423;&#20449;&#24687;&#21487;&#20197;&#24110;&#21161;&#23398;&#20064;&#39640;&#32423;&#20449;&#24687;&#65292;&#20294;&#19981;&#33021;&#23436;&#20840;&#35299;&#20915;&#35821;&#20041;&#29702;&#35299;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visual commonsense understanding requires Vision Language (VL) models to not only understand image and text but also cross-reference in-between to fully integrate and achieve comprehension of the visual scene described. Recently, various approaches have been developed and have achieved high performance on visual commonsense benchmarks. However, it is unclear whether the models really understand the visual scene and underlying commonsense knowledge due to limited evaluation data resources. To provide an in-depth analysis, we present a Multimodal Evaluation (ME) pipeline to automatically generate question-answer pairs to test models' understanding of the visual scene, text, and related knowledge. We then take a step further to show that training with the ME data boosts the model's performance in standard VCR evaluation. Lastly, our in-depth analysis and comparison reveal interesting findings: (1) semantically low-level information can assist the learning of high-level information but not
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#20020;&#24202;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#65292;&#23558;&#20174;&#19968;&#33324;&#39046;&#22495;&#25110;&#30456;&#20851;&#39046;&#22495;&#25968;&#25454;&#39044;&#35757;&#32451;&#30340;&#36801;&#31227;&#23398;&#20064;&#27169;&#22411;&#24494;&#35843;&#21040;&#29305;&#23450;&#20219;&#21153;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#24494;&#35843;&#30340;&#35821;&#35328;&#27169;&#22411;&#30456;&#23545;&#20110;&#20174;&#22836;&#24320;&#22987;&#23398;&#20064;&#30340;&#27169;&#22411;&#22312;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2210.12770</link><description>&lt;p&gt;
&#20851;&#20110;&#20020;&#24202;&#25991;&#26412;&#25366;&#25496;&#30340;&#36328;&#39046;&#22495;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65306;&#22312;&#25968;&#25454;&#21463;&#38480;&#24494;&#35843;&#20013;&#23427;&#20204;&#34920;&#29616;&#22914;&#20309;&#65311;
&lt;/p&gt;
&lt;p&gt;
On Cross-Domain Pre-Trained Language Models for Clinical Text Mining: How Do They Perform on Data-Constrained Fine-Tuning?. (arXiv:2210.12770v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.12770
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#20020;&#24202;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#65292;&#23558;&#20174;&#19968;&#33324;&#39046;&#22495;&#25110;&#30456;&#20851;&#39046;&#22495;&#25968;&#25454;&#39044;&#35757;&#32451;&#30340;&#36801;&#31227;&#23398;&#20064;&#27169;&#22411;&#24494;&#35843;&#21040;&#29305;&#23450;&#20219;&#21153;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#24494;&#35843;&#30340;&#35821;&#35328;&#27169;&#22411;&#30456;&#23545;&#20110;&#20174;&#22836;&#24320;&#22987;&#23398;&#20064;&#30340;&#27169;&#22411;&#22312;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#65292;&#20351;&#29992;&#20174;&#19968;&#33324;&#25110;&#30456;&#20851;&#39046;&#22495;&#25968;&#25454;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#23558;&#20854;&#24494;&#35843;&#21040;&#29305;&#23450;&#39046;&#22495;&#21644;&#20219;&#21153;&#19978;&#65292;&#24182;&#20351;&#29992;&#26032;&#20219;&#21153;&#20013;&#21487;&#29992;&#30340;&#26377;&#38480;&#36164;&#28304;&#36827;&#34892;&#24494;&#35843;&#65292;&#19968;&#30452;&#20197;&#26469;&#37117;&#26159;&#19968;&#20010;&#27969;&#34892;&#30340;&#23454;&#36341;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#37325;&#26032;&#32771;&#34385;&#20102;&#36825;&#31181;&#20551;&#35774;&#65292;&#24182;&#22312;&#20020;&#24202;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#20855;&#20307;&#26159;&#22312;&#33647;&#29289;&#21450;&#20854;&#30456;&#20851;&#23646;&#24615;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20219;&#21153;&#19978;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#20174;&#22836;&#24320;&#22987;&#23398;&#20064;&#30340;Transformer&#27169;&#22411;&#21644;&#36890;&#36807;&#24494;&#35843;BERT-based LLMs&#65288;&#21253;&#25324;BERT-base&#12289;BioBERT&#21644;ClinicalBERT&#65289;&#36827;&#34892;&#24494;&#35843;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#36824;&#23545;&#36825;&#20123;&#27169;&#22411;&#21450;&#20854;&#25193;&#23637;&#27169;&#22411;&#19982;&#24102;&#26377;CRF&#23618;&#30340;&#36830;&#32493;&#23398;&#20064;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#20351;&#29992;n2c2-2018&#20849;&#20139;&#20219;&#21153;&#25968;&#25454;&#36827;&#34892;&#27169;&#22411;&#24320;&#21457;&#21644;&#35780;&#20272;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65306;1&#65289;CRF&#23618;&#23545;&#25152;&#26377;&#31070;&#32463;&#27169;&#22411;&#37117;&#36215;&#21040;&#20102;&#31215;&#26497;&#30340;&#24433;&#21709;&#65307;2&#65289;&#22312;&#20351;&#29992;&#23439;&#24179;&#22343;F1&#23545;BIO-strict&#36328;&#24230;&#32423;&#21035;&#36827;&#34892;&#35780;&#20272;&#26102;&#65292;&#24494;&#35843;&#30340;LLMs&#33719;&#24471;&#20102;0.83+&#30340;&#24471;&#20998;&#65292;&#32780;&#20174;&#22836;&#24320;&#22987;&#23398;&#20064;&#30340;TransformerCRF&#27169;&#22411;&#24471;&#20998;&#20026;0.78+&#65292;&#35777;&#26126;&#20102;&#24494;&#35843;&#27169;&#22411;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fine-tuning Large Language Models (LLMs) pre-trained from general or related domain data to a specific domain and task using a limited amount of resources available in the new task has been a popular practice in NLP fields. In this work, we re-visit this assumption, and carry out investigation in clinical NLP, specifically named-entity recognition on Drugs and their related Attributes. We compare Transformer models that are learned from scratch to fine-tuning BERT-based LLMs including BERT-base, BioBERT, and ClinicalBERT. We also investigate the comparison of such models and their extended models with a CRF layer for continuous learning. We use n2c2-2018 shared task data for model development and evaluations. The experimental outcomes show that 1) the CRF layer makes a difference for all neural models; 2) on BIO-strict span level evaluation using macro-average F1, while the fine-tuned LLMs achieved scores 0.83+, the TransformerCRF model learned from scratch achieved 0.78+ demonstrating
&lt;/p&gt;</description></item><item><title>ConSpec&#26159;&#19968;&#20010;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#31163;&#32447;&#23545;&#27604;&#23398;&#20064;&#26469;&#30830;&#23450;&#20219;&#21153;&#20013;&#30340;&#20851;&#38190;&#27493;&#39588;&#65292;&#23454;&#29616;&#24555;&#36895;&#23398;&#20064;&#21644;&#27867;&#21270;&#12290;&#35813;&#31639;&#27861;&#36890;&#36807;&#23398;&#20064;&#20851;&#38190;&#27493;&#39588;&#30340;&#21407;&#22411;&#65292;&#24182;&#22312;&#24403;&#21069;&#29366;&#24577;&#21305;&#37197;&#26102;&#25552;&#20379;&#20869;&#22312;&#22870;&#21169;&#65292;&#20855;&#26377;&#24555;&#36895;&#35782;&#21035;&#20851;&#38190;&#27493;&#39588;&#21644;&#21487;&#35299;&#37322;&#30340;&#20449;&#29992;&#20998;&#37197;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2210.05845</link><description>&lt;p&gt;
ConSpec: &#31361;&#20986;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20851;&#38190;&#27493;&#39588;&#65292;&#23454;&#29616;&#24555;&#36895;&#23398;&#20064;&#21644;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
ConSpec: honing in on critical steps for rapid learning and generalization in RL. (arXiv:2210.05845v5 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.05845
&lt;/p&gt;
&lt;p&gt;
ConSpec&#26159;&#19968;&#20010;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#31163;&#32447;&#23545;&#27604;&#23398;&#20064;&#26469;&#30830;&#23450;&#20219;&#21153;&#20013;&#30340;&#20851;&#38190;&#27493;&#39588;&#65292;&#23454;&#29616;&#24555;&#36895;&#23398;&#20064;&#21644;&#27867;&#21270;&#12290;&#35813;&#31639;&#27861;&#36890;&#36807;&#23398;&#20064;&#20851;&#38190;&#27493;&#39588;&#30340;&#21407;&#22411;&#65292;&#24182;&#22312;&#24403;&#21069;&#29366;&#24577;&#21305;&#37197;&#26102;&#25552;&#20379;&#20869;&#22312;&#22870;&#21169;&#65292;&#20855;&#26377;&#24555;&#36895;&#35782;&#21035;&#20851;&#38190;&#27493;&#39588;&#21644;&#21487;&#35299;&#37322;&#30340;&#20449;&#29992;&#20998;&#37197;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#29983;&#27963;&#20013;&#65292;&#25104;&#21151;&#24448;&#24448;&#21462;&#20915;&#20110;&#22810;&#20010;&#20851;&#38190;&#27493;&#39588;&#65292;&#36825;&#20123;&#27493;&#39588;&#22312;&#26102;&#38388;&#19978;&#30456;&#36317;&#36739;&#36828;&#65292;&#19982;&#26368;&#32456;&#22870;&#21169;&#20063;&#30456;&#36317;&#29978;&#36828;&#12290;&#20256;&#32479;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#22312;&#20449;&#29992;&#20998;&#37197;&#26041;&#38754;&#20381;&#36182;Bellman&#26041;&#31243;&#65292;&#24456;&#38590;&#35782;&#21035;&#36825;&#20123;&#20851;&#38190;&#27493;&#39588;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#20351;&#29992;&#31163;&#32447;&#23545;&#27604;&#23398;&#20064;&#26469;&#30830;&#23450;&#20851;&#38190;&#27493;&#39588;&#12290;&#36825;&#20010;&#31639;&#27861;&#34987;&#31216;&#20026;&#23545;&#27604;&#20869;&#30465;&#65288;ConSpec&#65289;&#65292;&#21487;&#20197;&#28155;&#21152;&#21040;&#20219;&#20309;&#29616;&#26377;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20013;&#12290;ConSpec&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#27604;&#25439;&#22833;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#20851;&#38190;&#27493;&#39588;&#30340;&#21407;&#22411;&#65292;&#24182;&#22312;&#24403;&#21069;&#29366;&#24577;&#19982;&#36825;&#20123;&#21407;&#22411;&#20043;&#19968;&#21305;&#37197;&#26102;&#25552;&#20379;&#20869;&#22312;&#22870;&#21169;&#12290;ConSpec&#20013;&#30340;&#21407;&#22411;&#22312;&#20449;&#29992;&#20998;&#37197;&#26041;&#38754;&#20855;&#26377;&#20004;&#20010;&#20851;&#38190;&#20248;&#21183;&#65306;&#65288;1&#65289;&#23427;&#20204;&#20351;&#24471;&#33021;&#22815;&#36805;&#36895;&#35782;&#21035;&#25152;&#26377;&#20851;&#38190;&#27493;&#39588;&#65307;&#65288;2&#65289;&#23427;&#20204;&#20197;&#23481;&#26131;&#35299;&#37322;&#30340;&#26041;&#24335;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#20351;&#24471;&#22312;&#24863;&#35273;&#29305;&#24449;&#25913;&#21464;&#26102;&#21487;&#20197;&#36827;&#34892;&#36229;&#20986;&#20998;&#24067;&#30340;&#27867;&#21270;&#12290;&#19982;&#20854;&#20182;&#24403;&#20195;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#19981;&#21516;&#65292;
&lt;/p&gt;
&lt;p&gt;
In real life, success is often contingent upon multiple critical steps that are distant in time from each other and from the final reward. These critical steps are challenging to identify with traditional reinforcement learning (RL) methods that rely on the Bellman equation for credit assignment. Here, we present a new RL algorithm that uses offline contrastive learning to hone in on critical steps. This algorithm, which we call contrastive introspection (ConSpec), can be added to any existing RL algorithm. ConSpec learns a set of prototypes for the critical steps in a task by a novel contrastive loss and delivers an intrinsic reward when the current state matches one of these prototypes. The prototypes in ConSpec provide two key benefits for credit assignment: (1) They enable rapid identification of all the critical steps. (2) They do so in a readily interpretable manner, enabling out-of-distribution generalization when sensory features are altered. Distinct from other contemporary RL
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MixEncoder&#30340;&#26032;&#33539;&#24335;&#65292;&#29992;&#20110;&#39640;&#25928;&#30340;&#21477;&#23545;&#24314;&#27169;&#12290;&#35813;&#33539;&#24335;&#36890;&#36807;&#36731;&#37327;&#32423;&#30340;&#20132;&#21449;&#27880;&#24847;&#21147;&#26426;&#21046;&#23454;&#29616;&#20102;&#36229;&#36807;113&#20493;&#30340;&#21477;&#23545;&#21305;&#37197;&#21152;&#36895;&#65292;&#19982;&#26356;&#26114;&#36149;&#30340;&#20132;&#21449;&#27880;&#24847;&#21147;&#27169;&#22411;&#30456;&#27604;&#65292;&#24615;&#33021;&#30456;&#24403;&#12290;</title><link>http://arxiv.org/abs/2210.05261</link><description>&lt;p&gt;
&#19968;&#27425;&#23601;&#22815;&#20102;&#65306;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#29992;&#20110;&#24555;&#36895;&#21477;&#23545;&#24314;&#27169;&#30340;&#20132;&#21449;&#27880;&#24847;&#21147;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Once is Enough: A Light-Weight Cross-Attention for Fast Sentence Pair Modeling. (arXiv:2210.05261v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.05261
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MixEncoder&#30340;&#26032;&#33539;&#24335;&#65292;&#29992;&#20110;&#39640;&#25928;&#30340;&#21477;&#23545;&#24314;&#27169;&#12290;&#35813;&#33539;&#24335;&#36890;&#36807;&#36731;&#37327;&#32423;&#30340;&#20132;&#21449;&#27880;&#24847;&#21147;&#26426;&#21046;&#23454;&#29616;&#20102;&#36229;&#36807;113&#20493;&#30340;&#21477;&#23545;&#21305;&#37197;&#21152;&#36895;&#65292;&#19982;&#26356;&#26114;&#36149;&#30340;&#20132;&#21449;&#27880;&#24847;&#21147;&#27169;&#22411;&#30456;&#27604;&#65292;&#24615;&#33021;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#22312;&#21477;&#23545;&#24314;&#27169;&#20219;&#21153;&#65288;&#22914;&#31572;&#26696;&#36873;&#25321;&#21644;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65289;&#19978;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#22312;&#36755;&#20837;&#21477;&#23545;&#19978;&#25191;&#34892;&#20132;&#21449;&#27880;&#24847;&#21147;&#65292;&#23548;&#33268;&#35745;&#31639;&#25104;&#26412;&#36807;&#39640;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#21452;&#32534;&#30721;&#22120;&#21644;&#21518;&#26399;&#20132;&#20114;&#30340;&#26550;&#26500;&#20197;&#23454;&#29616;&#26356;&#24555;&#30340;&#35745;&#31639;&#12290;&#28982;&#32780;&#65292;&#20132;&#21449;&#27880;&#24847;&#21147;&#30340;&#34920;&#36798;&#24615;&#21644;&#35745;&#31639;&#36895;&#24230;&#20043;&#38388;&#30340;&#24179;&#34913;&#20173;&#38656;&#26356;&#22909;&#30340;&#21327;&#35843;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#33539;&#24335;MixEncoder&#29992;&#20110;&#39640;&#25928;&#30340;&#21477;&#23545;&#24314;&#27169;&#12290;MixEncoder&#21253;&#21547;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#20132;&#21449;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#22312;&#32534;&#30721;&#26597;&#35810;&#26102;&#21482;&#36827;&#34892;&#19968;&#27425;&#65292;&#21516;&#26102;&#24182;&#34892;&#24314;&#27169;&#26597;&#35810;-&#20505;&#36873;&#20132;&#20114;&#12290;&#22312;&#22235;&#20010;&#20219;&#21153;&#19978;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;MixEncoder&#21487;&#20197;&#23558;&#21477;&#23545;&#21305;&#37197;&#21152;&#36895;&#36229;&#36807;113&#20493;&#65292;&#21516;&#26102;&#23454;&#29616;&#19982;&#26356;&#26114;&#36149;&#30340;&#20132;&#21449;&#27880;&#24847;&#21147;&#27169;&#22411;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer-based models have achieved great success on sentence pair modeling tasks, such as answer selection and natural language inference (NLI). These models generally perform cross-attention over input pairs, leading to prohibitive computational costs. Recent studies propose dual-encoder and late interaction architectures for faster computation. However, the balance between the expressive of cross-attention and computation speedup still needs better coordinated. To this end, this paper introduces a novel paradigm MixEncoder for efficient sentence pair modeling. MixEncoder involves a light-weight cross-attention mechanism. It conducts query encoding only once while modeling the query-candidate interaction in parallel. Extensive experiments conducted on four tasks demonstrate that our MixEncoder can speed up sentence pairing by over 113x while achieving comparable performance as the more expensive cross-attention models.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;&#37319;&#26679;&#26694;&#26550;&#65292;&#36890;&#36807;&#20165;&#20351;&#29992;&#26377;&#38480;&#25968;&#37327;&#30340;&#19987;&#23478;&#21453;&#39304;&#65292;&#23398;&#20064;&#20102;&#30001;&#19981;&#21516;&#20998;&#24067;&#29983;&#25104;&#30340;&#25968;&#25454;&#30340;&#36328;&#32676;&#20307;&#30456;&#20284;&#24615;&#20989;&#25968;&#12290;</title><link>http://arxiv.org/abs/2208.12731</link><description>&lt;p&gt;
&#27604;&#36739;&#33529;&#26524;&#21644;&#27225;&#23376;&#65306;&#23398;&#20064;&#19981;&#21516;&#20998;&#24067;&#29983;&#25104;&#30340;&#25968;&#25454;&#30340;&#30456;&#20284;&#24615;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
Comparing Apples to Oranges: Learning Similarity Functions for Data Produced by Different Distributions. (arXiv:2208.12731v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.12731
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;&#37319;&#26679;&#26694;&#26550;&#65292;&#36890;&#36807;&#20165;&#20351;&#29992;&#26377;&#38480;&#25968;&#37327;&#30340;&#19987;&#23478;&#21453;&#39304;&#65292;&#23398;&#20064;&#20102;&#30001;&#19981;&#21516;&#20998;&#24067;&#29983;&#25104;&#30340;&#25968;&#25454;&#30340;&#36328;&#32676;&#20307;&#30456;&#20284;&#24615;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30456;&#20284;&#24615;&#20989;&#25968;&#34913;&#37327;&#20102;&#21487;&#27604;&#36739;&#30340;&#20803;&#32032;&#23545;&#30340;&#30456;&#20284;&#31243;&#24230;&#65292;&#24182;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#20363;&#22914;&#36981;&#24490;Dwork&#31561;&#20154;&#30340;&#24320;&#21019;&#24615;&#33539;&#24335;&#30340;&#20010;&#20307;&#20844;&#24179;&#24615;&#27010;&#24565;&#20197;&#21450;&#32858;&#31867;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#24182;&#19981;&#33021;&#24635;&#26159;&#20445;&#35777;&#21487;&#20197;&#33719;&#24471;&#20934;&#30830;&#30340;&#30456;&#20284;&#24615;&#20989;&#25968;&#65292;&#29978;&#33267;Dwork&#31561;&#20154;&#20063;&#25552;&#20986;&#20102;&#36825;&#19968;&#28857;&#12290;&#20363;&#22914;&#65292;&#21512;&#29702;&#22320;&#20551;&#35774;&#65292;&#24403;&#35201;&#27604;&#36739;&#30340;&#20803;&#32032;&#30001;&#19981;&#21516;&#30340;&#20998;&#24067;&#29983;&#25104;&#65292;&#25110;&#32773;&#25442;&#21477;&#35805;&#35828;&#23646;&#20110;&#19981;&#21516;&#30340;&#8220;&#20154;&#21475;&#8221;&#32676;&#20307;&#26102;&#65292;&#33719;&#24471;&#23427;&#20204;&#30340;&#30495;&#23454;&#30456;&#20284;&#24615;&#21487;&#33021;&#38750;&#24120;&#22256;&#38590;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#37319;&#26679;&#26694;&#26550;&#65292;&#36890;&#36807;&#20165;&#20351;&#29992;&#26377;&#38480;&#25968;&#37327;&#30340;&#19987;&#23478;&#21453;&#39304;&#26469;&#23398;&#20064;&#36825;&#20123;&#36328;&#32676;&#20307;&#30340;&#30456;&#20284;&#24615;&#20989;&#25968;&#12290;&#25105;&#20204;&#36890;&#36807;&#20005;&#26684;&#30340;&#29702;&#35770;&#30028;&#38480;&#23637;&#31034;&#20102;&#20998;&#26512;&#32467;&#26524;&#65292;&#24182;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Similarity functions measure how comparable pairs of elements are, and play a key role in a wide variety of applications, e.g., notions of Individual Fairness abiding by the seminal paradigm of Dwork et al., as well as Clustering problems. However, access to an accurate similarity function should not always be considered guaranteed, and this point was even raised by Dwork et al. For instance, it is reasonable to assume that when the elements to be compared are produced by different distributions, or in other words belong to different ``demographic'' groups, knowledge of their true similarity might be very difficult to obtain. In this work, we present an efficient sampling framework that learns these across-groups similarity functions, using only a limited amount of experts' feedback. We show analytical results with rigorous theoretical bounds, and empirically validate our algorithms via a large suite of experiments.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24314;&#31435;&#20102;&#19968;&#20010;&#35745;&#31639;&#25509;&#21475;&#65292;&#21487;&#20197;&#23558;&#38750;&#32467;&#26500;&#21270;&#35821;&#35328;&#31574;&#30053;&#32763;&#35793;&#20026;&#21487;&#25191;&#34892;&#30340;&#30446;&#26631;&#21644;&#32422;&#26463;&#65292;&#24182;&#35777;&#26126;&#35813;&#27169;&#22411;&#22312;&#25512;&#26029;&#25112;&#30053;&#24847;&#22270;&#26041;&#38754;&#20248;&#20110;&#20154;&#31867;&#35299;&#37322;&#21592;&#12290;</title><link>http://arxiv.org/abs/2208.08374</link><description>&lt;p&gt;
&#20174;&#38750;&#32467;&#26500;&#21270;&#35821;&#35328;&#20013;&#32763;&#35793;&#25112;&#30053;&#24847;&#22270;&#30340;&#35745;&#31639;&#25509;&#21475;
&lt;/p&gt;
&lt;p&gt;
A Computational Interface to Translate Strategic Intent from Unstructured Language in a Low-Data Setting. (arXiv:2208.08374v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.08374
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24314;&#31435;&#20102;&#19968;&#20010;&#35745;&#31639;&#25509;&#21475;&#65292;&#21487;&#20197;&#23558;&#38750;&#32467;&#26500;&#21270;&#35821;&#35328;&#31574;&#30053;&#32763;&#35793;&#20026;&#21487;&#25191;&#34892;&#30340;&#30446;&#26631;&#21644;&#32422;&#26463;&#65292;&#24182;&#35777;&#26126;&#35813;&#27169;&#22411;&#22312;&#25512;&#26029;&#25112;&#30053;&#24847;&#22270;&#26041;&#38754;&#20248;&#20110;&#20154;&#31867;&#35299;&#37322;&#21592;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#30340;&#20219;&#21153;&#28041;&#21450;&#21040;&#20154;&#31867;&#21644;AI&#31995;&#32479;&#21327;&#21516;&#23436;&#25104;&#20219;&#21153;&#30340;&#28151;&#21512;&#21457;&#36215;&#35774;&#23450;&#12290;&#23613;&#31649;&#22312;&#36890;&#36807;&#35821;&#35328;&#31934;&#30830;&#25351;&#23450;&#20195;&#29702;&#23436;&#25104;&#20219;&#21153;&#30340;&#20302;&#32423;&#35268;&#33539;&#26041;&#38754;&#36827;&#34892;&#20102;&#22823;&#37327;&#24037;&#20316;&#65292;&#20294;&#20043;&#21069;&#30340;&#24037;&#20316;&#22312;&#35299;&#37322;&#20154;&#31867;&#25351;&#25381;&#23448;&#30340;&#39640;&#32423;&#25112;&#30053;&#24847;&#22270;&#26041;&#38754;&#32570;&#20047;&#12290;&#20174;&#35821;&#35328;&#20013;&#35299;&#26512;&#25112;&#30053;&#24847;&#22270;&#23558;&#20351;&#33258;&#20027;&#31995;&#32479;&#33021;&#22815;&#26681;&#25454;&#29992;&#25143;&#30340;&#35745;&#21010;&#29420;&#31435;&#36816;&#34892;&#65292;&#32780;&#26080;&#38656;&#39057;&#32321;&#30340;&#25351;&#23548;&#25110;&#25351;&#20196;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#33021;&#22815;&#23558;&#38750;&#32467;&#26500;&#21270;&#35821;&#35328;&#31574;&#30053;&#36716;&#21270;&#20026;&#21487;&#34892;&#30340;&#30446;&#26631;&#21644;&#32422;&#26463;&#24418;&#24335;&#30340;&#35745;&#31639;&#25509;&#21475;&#12290;&#21033;&#29992;&#19968;&#20010;&#28216;&#25103;&#29615;&#22659;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;1000&#22810;&#20010;&#31034;&#20363;&#30340;&#25968;&#25454;&#38598;&#65292;&#23558;&#35821;&#35328;&#31574;&#30053;&#26144;&#23556;&#21040;&#30456;&#24212;&#30340;&#30446;&#26631;&#21644;&#32422;&#26463;&#65292;&#24182;&#35777;&#26126;&#25105;&#20204;&#22312;&#36825;&#20010;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;&#25512;&#26029;&#25112;&#30053;&#24847;&#22270;&#26041;&#38754;&#26174;&#33879;&#20248;&#20110;&#20154;&#31867;&#35299;&#37322;&#21592;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many real-world tasks involve a mixed-initiative setup, wherein humans and AI systems collaboratively perform a task. While significant work has been conducted towards enabling humans to specify, through language, exactly how an agent should complete a task (i.e., low-level specification), prior work lacks on interpreting the high-level strategic intent of the human commanders. Parsing strategic intent from language will allow autonomous systems to independently operate according to the user's plan without frequent guidance or instruction. In this paper, we build a computational interface capable of translating unstructured language strategies into actionable intent in the form of goals and constraints. Leveraging a game environment, we collect a dataset of over 1000 examples, mapping language strategies to the corresponding goals and constraints, and show that our model, trained on this dataset, significantly outperforms human interpreters in inferring strategic intent (i.e., goals an
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#23545;&#26410;&#30693;&#25968;&#37327;&#30340;&#21333;&#36890;&#36947;&#27700;&#22768;&#20449;&#21495;&#36827;&#34892;&#28304;&#20998;&#31163;&#12290;&#36890;&#36807;&#22266;&#23450;&#36755;&#20986;&#36890;&#36947;&#25968;&#37327;&#21644;&#26032;&#30340;&#24615;&#33021;&#35780;&#20272;&#26041;&#27861;&#65292;&#36991;&#20813;&#20102;&#25490;&#21015;&#38382;&#39064;&#24341;&#36215;&#30340;&#32500;&#24230;&#28798;&#38590;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#19982;&#24050;&#30693;&#20449;&#21495;&#25968;&#37327;&#30456;&#20284;&#30340;&#20998;&#31163;&#24615;&#33021;&#12290;&#35813;&#31639;&#27861;&#20855;&#26377;&#31454;&#20105;&#24615;&#33021;&#12289;&#21487;&#35299;&#37322;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#65292;&#22312;&#35813;&#26694;&#26550;&#19979;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2207.11749</link><description>&lt;p&gt;
&#22522;&#20110;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#26410;&#30693;&#25968;&#37327;&#21333;&#36890;&#36947;&#27700;&#22768;&#20449;&#21495;&#28304;&#20998;&#31163;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Source Separation of Unknown Numbers of Single-Channel Underwater Acoustic Signals Based on Autoencoders. (arXiv:2207.11749v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.11749
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#23545;&#26410;&#30693;&#25968;&#37327;&#30340;&#21333;&#36890;&#36947;&#27700;&#22768;&#20449;&#21495;&#36827;&#34892;&#28304;&#20998;&#31163;&#12290;&#36890;&#36807;&#22266;&#23450;&#36755;&#20986;&#36890;&#36947;&#25968;&#37327;&#21644;&#26032;&#30340;&#24615;&#33021;&#35780;&#20272;&#26041;&#27861;&#65292;&#36991;&#20813;&#20102;&#25490;&#21015;&#38382;&#39064;&#24341;&#36215;&#30340;&#32500;&#24230;&#28798;&#38590;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#19982;&#24050;&#30693;&#20449;&#21495;&#25968;&#37327;&#30456;&#20284;&#30340;&#20998;&#31163;&#24615;&#33021;&#12290;&#35813;&#31639;&#27861;&#20855;&#26377;&#31454;&#20105;&#24615;&#33021;&#12289;&#21487;&#35299;&#37322;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#65292;&#22312;&#35813;&#26694;&#26550;&#19979;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#24456;&#23569;&#26377;&#30740;&#31350;&#20851;&#27880;&#26410;&#30693;&#25968;&#37327;&#20449;&#21495;&#30340;&#28304;&#20998;&#31163;&#38382;&#39064;&#65292;&#20197;&#21450;&#22914;&#20309;&#35780;&#20272;&#31995;&#32479;&#30340;&#24615;&#33021;&#23578;&#19981;&#28165;&#26970;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20004;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20855;&#26377;&#22266;&#23450;&#36755;&#20986;&#36890;&#36947;&#25968;&#37327;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36991;&#20813;&#20102;&#30001;&#20110;&#36755;&#20986;&#19982;&#30446;&#26631;&#23545;&#40784;&#24341;&#36215;&#30340;&#25490;&#21015;&#38382;&#39064;&#23548;&#33268;&#30340;&#32500;&#24230;&#28798;&#38590;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#20004;&#27493;&#31639;&#27861;&#65292;&#24182;&#38024;&#23545;&#26377;&#38745;&#38899;&#36890;&#36947;&#30340;&#24773;&#20917;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24615;&#33021;&#35780;&#20272;&#26041;&#27861;&#12290;&#36890;&#36807;&#22312;&#27169;&#25311;&#28151;&#21512;&#30340;&#36752;&#23556;&#33337;&#22122;&#22768;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#35299;&#20915;&#26041;&#26696;&#21487;&#20197;&#36798;&#21040;&#19982;&#24050;&#30693;&#20449;&#21495;&#25968;&#37327;&#30456;&#20284;&#30340;&#20998;&#31163;&#24615;&#33021;&#12290;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#22312;&#24050;&#30693;&#20449;&#21495;&#25968;&#37327;&#30340;&#24773;&#20917;&#19979;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#33021;&#65292;&#20855;&#26377;&#39640;&#24230;&#21487;&#35299;&#37322;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#65292;&#24182;&#22312;&#35813;&#26694;&#26550;&#19979;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Few existing studies focus on the source separation problem with unknown numbers of signals, and how to evaluate the performances of the systems is not yet clear. We propose a solution with a fixed number of output channels to address these two problems, enabling it to avoid the dimensional disaster caused by the permutation problem induced by the alignment of outputs to targets. Specifically, we propose a two-step algorithm based on autoencoders and a new performance evaluation method for situations with mute channels. Experiments conducted on simulated mixtures of radiated ship noise show that the proposed solution can achieve similar separation performance to that attained with a known number of signals. The proposed algorithm achieved competitive performance as two algorithms developed for known numbers of signals, which is highly explainable and extensible and get the state of the art under this framework.
&lt;/p&gt;</description></item><item><title>ApHMM &#26159;&#19968;&#20010;&#28789;&#27963;&#30340;&#21152;&#36895;&#26694;&#26550;&#65292;&#26088;&#22312;&#26174;&#33879;&#20943;&#23569;Profile Hidden Markov Models&#20013;Baum-Welch&#31639;&#27861;&#30340;&#35745;&#31639;&#21644;&#33021;&#37327;&#24320;&#38144;&#12290;</title><link>http://arxiv.org/abs/2207.09765</link><description>&lt;p&gt;
ApHMM: &#24555;&#36895;&#21644;&#33410;&#33021;&#30340;&#22522;&#22240;&#32452;&#20998;&#26512;&#20013;&#21152;&#36895;Profile Hidden Markov Models
&lt;/p&gt;
&lt;p&gt;
ApHMM: Accelerating Profile Hidden Markov Models for Fast and Energy-Efficient Genome Analysis. (arXiv:2207.09765v2 [cs.AR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.09765
&lt;/p&gt;
&lt;p&gt;
ApHMM &#26159;&#19968;&#20010;&#28789;&#27963;&#30340;&#21152;&#36895;&#26694;&#26550;&#65292;&#26088;&#22312;&#26174;&#33879;&#20943;&#23569;Profile Hidden Markov Models&#20013;Baum-Welch&#31639;&#27861;&#30340;&#35745;&#31639;&#21644;&#33021;&#37327;&#24320;&#38144;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Profile Hidden Markov Models (pHMMs)&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#31181;&#29983;&#29289;&#20449;&#24687;&#23398;&#24212;&#29992;&#20013;&#65292;&#29992;&#20110;&#35782;&#21035;&#29983;&#29289;&#24207;&#21015;&#65288;&#22914;DNA&#25110;&#34507;&#30333;&#36136;&#24207;&#21015;&#65289;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#12290;&#22312;pHMMs&#20013;&#65292;&#24207;&#21015;&#34987;&#34920;&#31034;&#20026;&#22270;&#24418;&#32467;&#26500;&#12290;&#36825;&#20123;&#27010;&#29575;&#38543;&#21518;&#34987;&#29992;&#20110;&#35745;&#31639;&#24207;&#21015;&#19982;pHMM&#22270;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#24471;&#20998;&#12290;Baum-Welch&#31639;&#27861;&#26159;&#19968;&#31181;&#24120;&#29992;&#19988;&#39640;&#24230;&#20934;&#30830;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#36825;&#20123;&#27010;&#29575;&#26469;&#20248;&#21270;&#21644;&#35745;&#31639;&#30456;&#20284;&#24615;&#24471;&#20998;&#12290;&#28982;&#32780;&#65292;Baum-Welch&#31639;&#27861;&#35745;&#31639;&#23494;&#38598;&#65292;&#29616;&#26377;&#30340;&#35299;&#20915;&#26041;&#26696;&#35201;&#20040;&#21482;&#25552;&#20379;&#36719;&#20214;&#26041;&#27861;&#65292;&#35201;&#20040;&#21482;&#25552;&#20379;&#30828;&#20214;&#26041;&#27861;&#65292;&#24182;&#19988;&#20855;&#26377;&#22266;&#23450;&#30340;pHMM&#35774;&#35745;&#12290;&#25105;&#20204;&#35748;&#20026;&#26377;&#24517;&#35201;&#35774;&#35745;&#19968;&#31181;&#28789;&#27963;&#12289;&#39640;&#24615;&#33021;&#21644;&#33410;&#33021;&#30340;&#30828;&#20214;/&#36719;&#20214;&#21327;&#21516;&#35774;&#35745;&#26041;&#26696;&#65292;&#20197;&#35299;&#20915;pHMM&#20013;Baum-Welch&#31639;&#27861;&#30340;&#20027;&#35201;&#20302;&#25928;&#38382;&#39064;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;ApHMM&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#28789;&#27963;&#30340;&#21152;&#36895;&#26694;&#26550;&#65292;&#26088;&#22312;&#26174;&#33879;&#20943;&#23569;&#19982;Baum-Welch&#31639;&#27861;&#30456;&#20851;&#30340;&#35745;&#31639;&#21644;&#33021;&#37327;&#24320;&#38144;&#12290;
&lt;/p&gt;
&lt;p&gt;
Profile hidden Markov models (pHMMs) are widely employed in various bioinformatics applications to identify similarities between biological sequences, such as DNA or protein sequences. In pHMMs, sequences are represented as graph structures. These probabilities are subsequently used to compute the similarity score between a sequence and a pHMM graph. The Baum-Welch algorithm, a prevalent and highly accurate method, utilizes these probabilities to optimize and compute similarity scores. However, the Baum-Welch algorithm is computationally intensive, and existing solutions offer either software-only or hardware-only approaches with fixed pHMM designs. We identify an urgent need for a flexible, high-performance, and energy-efficient HW/SW co-design to address the major inefficiencies in the Baum-Welch algorithm for pHMMs.  We introduce ApHMM, the first flexible acceleration framework designed to significantly reduce both computational and energy overheads associated with the Baum-Welch al
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;APEL&#26694;&#26550;&#65292;&#38750;&#31243;&#24207;&#21592;&#21487;&#20197;&#36890;&#36807;&#26816;&#26597;&#36755;&#20837;-&#36755;&#20986;&#31034;&#20363;&#38388;&#25509;&#36873;&#25321;&#22797;&#26434;&#31243;&#24207;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#33258;&#28982;&#35821;&#35328;&#34920;&#36798;&#30340;&#27880;&#37322;&#65292;&#24182;&#19988;&#22312;&#37325;&#26032;&#27880;&#37322;&#25991;&#26412;&#21040;SQL&#25968;&#25454;&#38598;&#26102;&#36798;&#21040;&#20102;&#19982;&#19987;&#23478;&#30456;&#21516;&#30340;&#20934;&#30830;&#24230;&#65292;&#21516;&#26102;&#25581;&#31034;&#20102;&#21407;&#22987;&#27880;&#37322;&#20013;&#30340;&#32454;&#24494;&#38169;&#35823;&#12290;</title><link>http://arxiv.org/abs/2205.12422</link><description>&lt;p&gt;
&#38388;&#25509;&#36890;&#36807;&#20027;&#21160;&#31034;&#20363;&#20026;&#38750;&#31243;&#24207;&#21592;&#28155;&#21152;&#26631;&#31614;&#31243;&#24207;&#65306;&#20197;Text-to-SQL&#20026;&#20363;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Labeling Programs with Non-Programmers Indirectly via Active Examples: A Case Study with Text-to-SQL. (arXiv:2205.12422v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.12422
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;APEL&#26694;&#26550;&#65292;&#38750;&#31243;&#24207;&#21592;&#21487;&#20197;&#36890;&#36807;&#26816;&#26597;&#36755;&#20837;-&#36755;&#20986;&#31034;&#20363;&#38388;&#25509;&#36873;&#25321;&#22797;&#26434;&#31243;&#24207;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#33258;&#28982;&#35821;&#35328;&#34920;&#36798;&#30340;&#27880;&#37322;&#65292;&#24182;&#19988;&#22312;&#37325;&#26032;&#27880;&#37322;&#25991;&#26412;&#21040;SQL&#25968;&#25454;&#38598;&#26102;&#36798;&#21040;&#20102;&#19982;&#19987;&#23478;&#30456;&#21516;&#30340;&#20934;&#30830;&#24230;&#65292;&#21516;&#26102;&#25581;&#31034;&#20102;&#21407;&#22987;&#27880;&#37322;&#20013;&#30340;&#32454;&#24494;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38750;&#31243;&#24207;&#21592;&#33021;&#21542;&#20351;&#29992;&#22797;&#26434;&#31243;&#24207;&#23545;&#33258;&#28982;&#35821;&#35328;&#34920;&#36798;&#36827;&#34892;&#27880;&#37322;&#20197;&#34920;&#31034;&#20854;&#21547;&#20041;&#65311;&#25105;&#20204;&#20171;&#32461;&#20102;APEL&#65292;&#36825;&#26159;&#19968;&#20010;&#26694;&#26550;&#65292;&#22312;&#36825;&#20010;&#26694;&#26550;&#20013;&#65292;&#38750;&#31243;&#24207;&#21592;&#20174;&#30001;&#31181;&#23376;&#35821;&#20041;&#35299;&#26512;&#22120;&#65288;&#20363;&#22914;Codex&#65289;&#29983;&#25104;&#30340;&#20505;&#36873;&#31243;&#24207;&#20013;&#36873;&#25321;&#12290;&#30001;&#20110;&#20182;&#20204;&#26080;&#27861;&#29702;&#35299;&#20505;&#36873;&#31243;&#24207;&#65292;&#25105;&#20204;&#35201;&#27714;&#20182;&#20204;&#36890;&#36807;&#26816;&#26597;&#31243;&#24207;&#30340;&#36755;&#20837;-&#36755;&#20986;&#31034;&#20363;&#26469;&#38388;&#25509;&#36873;&#25321;&#12290;&#23545;&#20110;&#27599;&#20010;&#34920;&#36798;&#24335;&#65292;APEL&#20027;&#21160;&#25628;&#32034;&#19968;&#20010;&#31616;&#21333;&#30340;&#36755;&#20837;&#65292;&#20505;&#36873;&#31243;&#24207;&#22312;&#36825;&#20010;&#36755;&#20837;&#19978;&#26356;&#20542;&#21521;&#20110;&#20135;&#29983;&#19981;&#21516;&#30340;&#36755;&#20986;&#12290;&#28982;&#21518;&#65292;&#23427;&#21482;&#35201;&#27714;&#38750;&#31243;&#24207;&#21592;&#36873;&#25321;&#36866;&#24403;&#30340;&#36755;&#20986;&#65292;&#20174;&#32780;&#20801;&#35768;&#25105;&#20204;&#25512;&#26029;&#20986;&#21738;&#20010;&#31243;&#24207;&#26159;&#27491;&#30830;&#30340;&#65292;&#21487;&#20197;&#29992;&#20110;&#35843;&#20248;&#35299;&#26512;&#22120;&#12290;&#20316;&#20026;&#31532;&#19968;&#20010;&#26696;&#20363;&#30740;&#31350;&#65292;&#25105;&#20204;&#25307;&#21215;&#20102;&#20154;&#31867;&#38750;&#31243;&#24207;&#21592;&#20351;&#29992;APEL&#37325;&#26032;&#27880;&#37322;SPIDER&#65292;&#19968;&#20010;&#25991;&#26412;&#21040;SQL&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23454;&#29616;&#20102;&#19982;&#21407;&#22987;&#19987;&#23478;&#27880;&#37322;&#32773;&#30456;&#21516;&#30340;&#27880;&#37322;&#20934;&#30830;&#24230;&#65288;75%&#65289;&#65292;&#24182;&#25581;&#31034;&#20102;&#21407;&#22987;&#27880;&#37322;&#20013;&#35768;&#22810;&#24494;&#23567;&#30340;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;
Can non-programmers annotate natural language utterances with complex programs that represent their meaning? We introduce APEL, a framework in which non-programmers select among candidate programs generated by a seed semantic parser (e.g., Codex). Since they cannot understand the candidate programs, we ask them to select indirectly by examining the programs' input-ouput examples. For each utterance, APEL actively searches for a simple input on which the candidate programs tend to produce different outputs. It then asks the non-programmers only to choose the appropriate output, thus allowing us to infer which program is correct and could be used to fine-tune the parser. As a first case study, we recruited human non-programmers to use APEL to re-annotate SPIDER, a text-to-SQL dataset. Our approach achieved the same annotation accuracy as the original expert annotators (75%) and exposed many subtle errors in the original annotations.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26368;&#36817;&#37051;&#23545;&#27604;&#23398;&#20064;&#30340;&#35821;&#38899;&#24207;&#21015;&#23884;&#20837;&#30340;&#31616;&#21333;&#31070;&#32463;&#32534;&#30721;&#22120;&#26550;&#26500;&#65292;&#36890;&#36807;&#22312;&#33258;&#30417;&#30563;&#38899;&#39057;&#34920;&#31034;&#20043;&#19978;&#36827;&#34892;&#36845;&#20195;&#24212;&#29992;&#65292;&#23454;&#29616;&#20102;&#22312;&#26597;&#35810;&#31034;&#20363;&#21644;&#21475;&#36848;&#26415;&#35821;&#21457;&#29616;&#31561;&#20219;&#21153;&#19978;&#30340;&#26377;&#31454;&#20105;&#21147;&#30340;&#32467;&#26524;&#65292;&#24182;&#22312;5&#31181;&#19981;&#21516;&#30340;&#35821;&#35328;&#20013;&#23558;&#20808;&#36827;&#25216;&#26415;&#25512;&#21521;&#20102;&#19968;&#20010;&#26174;&#33879;&#30340;&#36793;&#30028;&#12290;</title><link>http://arxiv.org/abs/2204.05148</link><description>&lt;p&gt;
&#20351;&#29992;&#26368;&#36817;&#37051;&#23545;&#27604;&#23398;&#20064;&#30340;&#35821;&#38899;&#24207;&#21015;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Speech Sequence Embeddings using Nearest Neighbors Contrastive Learning. (arXiv:2204.05148v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.05148
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26368;&#36817;&#37051;&#23545;&#27604;&#23398;&#20064;&#30340;&#35821;&#38899;&#24207;&#21015;&#23884;&#20837;&#30340;&#31616;&#21333;&#31070;&#32463;&#32534;&#30721;&#22120;&#26550;&#26500;&#65292;&#36890;&#36807;&#22312;&#33258;&#30417;&#30563;&#38899;&#39057;&#34920;&#31034;&#20043;&#19978;&#36827;&#34892;&#36845;&#20195;&#24212;&#29992;&#65292;&#23454;&#29616;&#20102;&#22312;&#26597;&#35810;&#31034;&#20363;&#21644;&#21475;&#36848;&#26415;&#35821;&#21457;&#29616;&#31561;&#20219;&#21153;&#19978;&#30340;&#26377;&#31454;&#20105;&#21147;&#30340;&#32467;&#26524;&#65292;&#24182;&#22312;5&#31181;&#19981;&#21516;&#30340;&#35821;&#35328;&#20013;&#23558;&#20808;&#36827;&#25216;&#26415;&#25512;&#21521;&#20102;&#19968;&#20010;&#26174;&#33879;&#30340;&#36793;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#31070;&#32463;&#32534;&#30721;&#22120;&#26550;&#26500;&#65292;&#21487;&#20197;&#20351;&#29992;&#26080;&#30417;&#30563;&#30340;&#23545;&#27604;&#23398;&#20064;&#30446;&#26631;&#36827;&#34892;&#35757;&#32451;&#65292;&#35813;&#30446;&#26631;&#20174;&#25968;&#25454;&#22686;&#24378;&#30340;k&#26368;&#36817;&#37051;&#25628;&#32034;&#20013;&#33719;&#21462;&#20854;&#27491;&#26679;&#26412;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#24403;&#24314;&#31435;&#22312;&#26368;&#36817;&#30340;&#33258;&#30417;&#30563;&#38899;&#39057;&#34920;&#31034;&#20043;&#19978;&#26102;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#36845;&#20195;&#24212;&#29992;&#24182;&#22312;&#20004;&#20010;&#20219;&#21153;&#19978;&#20135;&#29983;&#26377;&#31454;&#20105;&#21147;&#30340;SSE&#35780;&#20272;&#32467;&#26524;&#65306;&#38543;&#26426;&#35821;&#38899;&#24207;&#21015;&#30340;&#26597;&#35810;&#31034;&#20363;&#21644;&#21475;&#36848;&#26415;&#35821;&#21457;&#29616;&#12290;&#22312;&#36825;&#20004;&#20010;&#20219;&#21153;&#19978;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;5&#31181;&#19981;&#21516;&#30340;&#35821;&#35328;&#20013;&#23558;&#20808;&#36827;&#25216;&#26415;&#25512;&#21521;&#20102;&#19968;&#20010;&#26174;&#33879;&#30340;&#36793;&#30028;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#22312;LibriSpeech&#25968;&#25454;&#38598;&#19978;&#24314;&#31435;&#20102;&#19968;&#20010;&#26597;&#35810;&#31034;&#20363;&#20219;&#21153;&#30340;&#22522;&#20934;&#65292;&#20197;&#30417;&#27979;&#26410;&#26469;&#22312;&#35813;&#39046;&#22495;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a simple neural encoder architecture that can be trained using an unsupervised contrastive learning objective which gets its positive samples from data-augmented k-Nearest Neighbors search. We show that when built on top of recent self-supervised audio representations, this method can be applied iteratively and yield competitive SSE as evaluated on two tasks: query-by-example of random sequences of speech, and spoken term discovery. On both tasks our method pushes the state-of-the-art by a significant margin across 5 different languages. Finally, we establish a benchmark on a query-by-example task on the LibriSpeech dataset to monitor future improvements in the field.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#35299;&#32806;Mixup&#65288;DM&#65289;&#30340;&#39640;&#25928;mixup&#30446;&#26631;&#20989;&#25968;&#65292;&#36890;&#36807;&#21033;&#29992;&#22256;&#38590;&#28151;&#21512;&#26679;&#26412;&#26469;&#25366;&#25496;&#20855;&#26377;&#21028;&#21035;&#29305;&#24449;&#30340;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2203.10761</link><description>&lt;p&gt;
&#21033;&#29992;&#35299;&#32806;&#27491;&#21017;&#21270;&#26041;&#27861;&#26469;&#24212;&#29992;&#20110;&#22256;&#38590;&#28151;&#21512;&#26679;&#26412;
&lt;/p&gt;
&lt;p&gt;
Harnessing Hard Mixed Samples with Decoupled Regularizer. (arXiv:2203.10761v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.10761
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#35299;&#32806;Mixup&#65288;DM&#65289;&#30340;&#39640;&#25928;mixup&#30446;&#26631;&#20989;&#25968;&#65292;&#36890;&#36807;&#21033;&#29992;&#22256;&#38590;&#28151;&#21512;&#26679;&#26412;&#26469;&#25366;&#25496;&#20855;&#26377;&#21028;&#21035;&#29305;&#24449;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Mixup&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#36890;&#36807;&#28151;&#21512;&#25968;&#25454;&#24179;&#28369;&#20915;&#31574;&#36793;&#30028;&#65292;&#25552;&#39640;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#26368;&#36817;&#65292;&#21160;&#24577;mixup&#26041;&#27861;&#36890;&#36807;&#26368;&#22823;&#21270;&#28151;&#21512;&#26679;&#26412;&#20013;&#19982;&#30446;&#26631;&#30456;&#20851;&#30340;&#26174;&#33879;&#21306;&#22495;&#65292;&#26377;&#25928;&#25913;&#36827;&#20102;&#20808;&#21069;&#30340;&#38745;&#24577;&#31574;&#30053;&#65288;&#22914;&#32447;&#24615;&#25554;&#20540;&#65289;&#65292;&#20294;&#39069;&#22806;&#30340;&#26102;&#38388;&#25104;&#26412;&#26159;&#19981;&#21487;&#25509;&#21463;&#30340;&#12290;&#36825;&#20123;&#39069;&#22806;&#30340;&#35745;&#31639;&#24320;&#38144;&#20027;&#35201;&#26469;&#33258;&#26681;&#25454;&#28151;&#21512;&#26631;&#31614;&#20248;&#21270;&#28151;&#21512;&#26679;&#26412;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#39069;&#22806;&#30340;&#20248;&#21270;&#27493;&#39588;&#21487;&#33021;&#26159;&#22810;&#20313;&#30340;&#65292;&#22240;&#20026;&#26631;&#31614;&#19981;&#21305;&#37197;&#30340;&#28151;&#21512;&#26679;&#26412;&#23545;&#20110;&#28145;&#24230;&#27169;&#22411;&#26469;&#23450;&#20301;&#26377;&#24046;&#24322;&#24615;&#29305;&#24449;&#26159;&#26377;&#20449;&#24687;&#37327;&#30340;&#22256;&#38590;&#28151;&#21512;&#26679;&#26412;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#35299;&#32806;Mixup&#65288;DM&#65289;&#30340;&#39640;&#25928;mixup&#30446;&#26631;&#20989;&#25968;&#65292;&#32780;&#19981;&#26159;&#25552;&#20986;&#26356;&#22797;&#26434;&#30340;&#21160;&#24577;mixup&#31574;&#30053;&#12290;&#20854;&#20027;&#35201;&#25928;&#26524;&#26159;DM&#33021;&#22815;&#33258;&#36866;&#24212;&#22320;&#21033;&#29992;&#36825;&#20123;&#22256;&#38590;&#28151;&#21512;&#26679;&#26412;&#26469;&#25366;&#25496;&#20855;&#26377;&#21028;&#21035;&#29305;&#24449;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mixup is an efficient data augmentation approach that improves the generalization of neural networks by smoothing the decision boundary with mixed data. Recently, dynamic mixup methods have improved previous static policies effectively (e.g., linear interpolation) by maximizing target-related salient regions in mixed samples, but excessive additional time costs are not acceptable. These additional computational overheads mainly come from optimizing the mixed samples according to the mixed labels. However, we found that the extra optimizing step may be redundant because label-mismatched mixed samples are informative hard mixed samples for deep models to localize discriminative features. In this paper, we thus are not trying to propose a more complicated dynamic mixup policy but rather an efficient mixup objective function with a decoupled regularizer named Decoupled Mixup (DM). The primary effect is that DM can adaptively utilize those hard mixed samples to mine discriminative features 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#24320;&#25918;&#39046;&#22495;&#20013;&#25991;&#23545;&#35805;&#31995;&#32479;&#30340;&#20851;&#38190;&#22240;&#32032;&#65292;&#21253;&#25324;&#25968;&#25454;&#36136;&#37327;&#25511;&#21046;&#12289;&#27169;&#22411;&#26550;&#26500;&#35774;&#35745;&#12289;&#35757;&#32451;&#26041;&#27861;&#21644;&#35299;&#30721;&#31574;&#30053;&#65292;&#24182;&#25552;&#20986;&#20102;&#20855;&#26377;28&#20159;&#21442;&#25968;&#30340;EVA2.0&#27169;&#22411;&#65292;&#20854;&#22312;&#33258;&#21160;&#21644;&#20154;&#24037;&#35780;&#20272;&#20013;&#34920;&#29616;&#20248;&#31168;&#65292;&#36824;&#35752;&#35770;&#20102;&#35813;&#30740;&#31350;&#30340;&#23616;&#38480;&#24615;&#21644;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2203.09313</link><description>&lt;p&gt;
EVA2.0&#65306;&#20351;&#29992;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#30740;&#31350;&#24320;&#25918;&#39046;&#22495;&#30340;&#20013;&#25991;&#23545;&#35805;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
EVA2.0: Investigating Open-Domain Chinese Dialogue Systems with Large-Scale Pre-Training. (arXiv:2203.09313v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.09313
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#24320;&#25918;&#39046;&#22495;&#20013;&#25991;&#23545;&#35805;&#31995;&#32479;&#30340;&#20851;&#38190;&#22240;&#32032;&#65292;&#21253;&#25324;&#25968;&#25454;&#36136;&#37327;&#25511;&#21046;&#12289;&#27169;&#22411;&#26550;&#26500;&#35774;&#35745;&#12289;&#35757;&#32451;&#26041;&#27861;&#21644;&#35299;&#30721;&#31574;&#30053;&#65292;&#24182;&#25552;&#20986;&#20102;&#20855;&#26377;28&#20159;&#21442;&#25968;&#30340;EVA2.0&#27169;&#22411;&#65292;&#20854;&#22312;&#33258;&#21160;&#21644;&#20154;&#24037;&#35780;&#20272;&#20013;&#34920;&#29616;&#20248;&#31168;&#65292;&#36824;&#35752;&#35770;&#20102;&#35813;&#30740;&#31350;&#30340;&#23616;&#38480;&#24615;&#21644;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#22312;&#26500;&#24314;&#24320;&#25918;&#39046;&#22495;&#30340;&#23545;&#35805;&#31995;&#32479;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#20043;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#21457;&#24067;&#30340;&#23545;&#35805;&#27169;&#22411;&#30340;&#23545;&#35805;&#24615;&#33021;&#23637;&#31034;&#21644;&#35780;&#20272;&#65292;&#24573;&#35270;&#20102;&#19968;&#20123;&#20851;&#38190;&#22240;&#32032;&#23545;&#20110;&#24378;&#22823;&#30340;&#31867;&#20154;&#32842;&#22825;&#26426;&#22120;&#20154;&#29305;&#21035;&#22312;&#20013;&#25991;&#22330;&#26223;&#19979;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#36825;&#20123;&#26410;&#26366;&#25506;&#32034;&#30340;&#22240;&#32032;&#65292;&#21253;&#25324;&#25968;&#25454;&#36136;&#37327;&#25511;&#21046;&#12289;&#27169;&#22411;&#26550;&#26500;&#35774;&#35745;&#12289;&#35757;&#32451;&#26041;&#27861;&#21644;&#35299;&#30721;&#31574;&#30053;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;EVA2.0&#65292;&#19968;&#20010;&#20855;&#26377;28&#20159;&#21442;&#25968;&#30340;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#24320;&#25918;&#39046;&#22495;&#20013;&#25991;&#23545;&#35805;&#27169;&#22411;&#65292;&#24182;&#23558;&#25105;&#20204;&#30340;&#27169;&#22411;&#21644;&#20195;&#30721;&#20844;&#24320;&#21487;&#29992;&#12290;&#33258;&#21160;&#21644;&#20154;&#24037;&#35780;&#20272;&#34920;&#26126;&#65292;EVA2.0&#26126;&#26174;&#20248;&#20110;&#20854;&#20182;&#24320;&#28304;&#23545;&#24212;&#27169;&#22411;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#23637;&#31034;&#19968;&#20123;&#22833;&#36133;&#26696;&#20363;&#35752;&#35770;&#20102;&#35813;&#30740;&#31350;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#22823;&#35268;&#27169;&#20013;&#25991;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#31995;&#32479;&#30340;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large-scale pre-training has shown remarkable performance in building open-domain dialogue systems. However, previous works mainly focus on showing and evaluating the conversational performance of the released dialogue model, ignoring the discussion of some key factors towards a powerful human-like chatbot, especially in Chinese scenarios. In this paper, we conduct extensive experiments to investigate these under-explored factors, including data quality control, model architecture designs, training approaches, and decoding strategies. We propose EVA2.0, a large-scale pre-trained open-domain Chinese dialogue model with 2.8 billion parameters, and will make our models and codes publicly available. Automatic and human evaluations show that EVA2.0 significantly outperforms other open-source counterparts. We also discuss the limitations of this work by presenting some failure cases and pose some future research directions on large-scale Chinese open-domain dialogue systems.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#32467;&#21512;&#26368;&#20248;&#36335;&#24452;&#25628;&#32034;&#21644;&#20219;&#21153;&#30456;&#20851;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#25104;&#26412;&#20540;&#36716;&#21270;&#20026;&#31070;&#32463;&#32593;&#32476;&#30340;&#26435;&#37325;&#26469;&#23454;&#29616;&#22312;&#32447;&#26435;&#37325;&#36866;&#24212;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#19982;&#32463;&#20856;&#31639;&#27861;Bellman-Ford&#20855;&#26377;&#30456;&#21516;&#30340;&#35299;&#65292;&#24182;&#19988;&#32593;&#32476;&#23398;&#20064;&#26426;&#21046;&#21487;&#20197;&#36827;&#19968;&#27493;&#22686;&#24378;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2201.11104</link><description>&lt;p&gt;
&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#32467;&#21512;&#26368;&#20248;&#36335;&#24452;&#25628;&#32034;&#21644;&#20219;&#21153;&#30456;&#20851;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Combining optimal path search with task-dependent learning in a neural network. (arXiv:2201.11104v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.11104
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#32467;&#21512;&#26368;&#20248;&#36335;&#24452;&#25628;&#32034;&#21644;&#20219;&#21153;&#30456;&#20851;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#25104;&#26412;&#20540;&#36716;&#21270;&#20026;&#31070;&#32463;&#32593;&#32476;&#30340;&#26435;&#37325;&#26469;&#23454;&#29616;&#22312;&#32447;&#26435;&#37325;&#36866;&#24212;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#19982;&#32463;&#20856;&#31639;&#27861;Bellman-Ford&#20855;&#26377;&#30456;&#21516;&#30340;&#35299;&#65292;&#24182;&#19988;&#32593;&#32476;&#23398;&#20064;&#26426;&#21046;&#21487;&#20197;&#36827;&#19968;&#27493;&#22686;&#24378;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36830;&#25509;&#22270;&#20013;&#25214;&#21040;&#26368;&#20248;&#36335;&#24452;&#38656;&#35201;&#30830;&#23450;&#27839;&#30528;&#22270;&#30340;&#36793;&#32536;&#34892;&#36827;&#30340;&#26368;&#23567;&#24635;&#25104;&#26412;&#12290;&#36825;&#20010;&#38382;&#39064;&#21487;&#20197;&#36890;&#36807;&#20960;&#31181;&#32463;&#20856;&#31639;&#27861;&#26469;&#35299;&#20915;&#65292;&#36890;&#24120;&#25152;&#26377;&#36793;&#32536;&#30340;&#25104;&#26412;&#37117;&#26159;&#39044;&#20808;&#23450;&#20041;&#22909;&#30340;&#12290;&#22240;&#27492;&#65292;&#22312;&#24819;&#35201;&#26681;&#25454;&#26576;&#20010;&#20219;&#21153;&#30340;&#35201;&#27714;&#20197;&#33258;&#36866;&#24212;&#30340;&#26041;&#24335;&#25913;&#21464;&#25104;&#26412;&#26102;&#65292;&#36890;&#24120;&#26080;&#27861;&#20351;&#29992;&#20256;&#32479;&#35268;&#21010;&#26041;&#27861;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#21487;&#20197;&#36890;&#36807;&#23558;&#25104;&#26412;&#20540;&#36716;&#21270;&#20026;&#31361;&#35302;&#26435;&#37325;&#26469;&#23450;&#20041;&#36335;&#24452;&#25628;&#32034;&#38382;&#39064;&#30340;&#31070;&#32463;&#32593;&#32476;&#34920;&#31034;&#65292;&#36825;&#20801;&#35768;&#20351;&#29992;&#32593;&#32476;&#23398;&#20064;&#26426;&#21046;&#36827;&#34892;&#22312;&#32447;&#26435;&#37325;&#36866;&#24212;&#12290;&#24403;&#20174;&#19968;&#20010;&#21021;&#22987;&#27963;&#36291;&#24230;&#20540;&#20026;1&#24320;&#22987;&#26102;&#65292;&#22312;&#36825;&#20010;&#32593;&#32476;&#20013;&#30340;&#27963;&#21160;&#20256;&#25773;&#23558;&#23548;&#33268;&#19982;Bellman-Ford&#31639;&#27861;&#25214;&#21040;&#30340;&#35299;&#30456;&#21516;&#30340;&#35299;&#12290;&#31070;&#32463;&#32593;&#32476;&#20855;&#26377;&#19982;Bellman-Ford&#30456;&#21516;&#30340;&#31639;&#27861;&#22797;&#26434;&#24230;&#65292;&#24182;&#19988;&#27492;&#22806;&#65292;&#25105;&#20204;&#21487;&#20197;&#35777;&#26126;&#32593;&#32476;&#23398;&#20064;&#26426;&#21046;&#65288;&#22914;&#36203;&#24067;&#23398;&#20064;&#65289;&#21487;&#20197;&#35843;&#25972;&#32593;&#32476;&#20013;&#30340;&#26435;&#37325;&#26469;&#22686;&#24378;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Finding optimal paths in connected graphs requires determining the smallest total cost for traveling along the graph's edges. This problem can be solved by several classical algorithms where, usually, costs are predefined for all edges. Conventional planning methods can, thus, normally not be used when wanting to change costs in an adaptive way following the requirements of some task. Here we show that one can define a neural network representation of path finding problems by transforming cost values into synaptic weights, which allows for online weight adaptation using network learning mechanisms. When starting with an initial activity value of one, activity propagation in this network will lead to solutions, which are identical to those found by the Bellman-Ford algorithm. The neural network has the same algorithmic complexity as Bellman-Ford and, in addition, we can show that network learning mechanisms (such as Hebbian learning) can adapt the weights in the network augmenting the r
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25506;&#32034;&#20102;&#22522;&#20110;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#35823;&#24046;&#26377;&#30028;&#31185;&#23398;&#25968;&#25454;&#21387;&#32553;&#65292;&#24182;&#25552;&#20986;&#20102;&#19977;&#20010;&#20851;&#38190;&#36129;&#29486;&#65306;&#65288;1&#65289;&#28145;&#20837;&#30740;&#31350;&#20102;&#21508;&#31181;&#33258;&#21160;&#32534;&#30721;&#22120;&#27169;&#22411;&#30340;&#29305;&#24615;&#65292;&#24182;&#24320;&#21457;&#20102;&#22522;&#20110;SZ&#27169;&#22411;&#30340;&#35823;&#24046;&#26377;&#30028;&#33258;&#21160;&#32534;&#30721;&#22120;&#26694;&#26550;&#65307;&#65288;2&#65289;&#20248;&#21270;&#20102;&#35774;&#35745;&#30340;&#22522;&#20110;AE&#30340;&#35823;&#24046;&#26377;&#30028;&#21387;&#32553;&#26694;&#26550;&#20013;&#30340;&#20027;&#35201;&#38454;&#27573;&#30340;&#21387;&#32553;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2105.11730</link><description>&lt;p&gt;
&#25506;&#32034;&#22522;&#20110;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#35823;&#24046;&#26377;&#30028;&#31185;&#23398;&#25968;&#25454;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
Exploring Autoencoder-based Error-bounded Compression for Scientific Data. (arXiv:2105.11730v7 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2105.11730
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25506;&#32034;&#20102;&#22522;&#20110;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#35823;&#24046;&#26377;&#30028;&#31185;&#23398;&#25968;&#25454;&#21387;&#32553;&#65292;&#24182;&#25552;&#20986;&#20102;&#19977;&#20010;&#20851;&#38190;&#36129;&#29486;&#65306;&#65288;1&#65289;&#28145;&#20837;&#30740;&#31350;&#20102;&#21508;&#31181;&#33258;&#21160;&#32534;&#30721;&#22120;&#27169;&#22411;&#30340;&#29305;&#24615;&#65292;&#24182;&#24320;&#21457;&#20102;&#22522;&#20110;SZ&#27169;&#22411;&#30340;&#35823;&#24046;&#26377;&#30028;&#33258;&#21160;&#32534;&#30721;&#22120;&#26694;&#26550;&#65307;&#65288;2&#65289;&#20248;&#21270;&#20102;&#35774;&#35745;&#30340;&#22522;&#20110;AE&#30340;&#35823;&#24046;&#26377;&#30028;&#21387;&#32553;&#26694;&#26550;&#20013;&#30340;&#20027;&#35201;&#38454;&#27573;&#30340;&#21387;&#32553;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35823;&#24046;&#26377;&#30028;&#30340;&#26377;&#25439;&#21387;&#32553;&#23545;&#20110;&#24403;&#20170;&#31185;&#23398;&#39033;&#30446;&#30340;&#25104;&#21151;&#38750;&#24120;&#37325;&#35201;&#65292;&#22240;&#20026;&#22312;&#27169;&#25311;&#25110;&#20202;&#22120;&#25968;&#25454;&#37319;&#38598;&#36807;&#31243;&#20013;&#20135;&#29983;&#30340;&#25968;&#25454;&#37327;&#24040;&#22823;&#12290;&#23427;&#19981;&#20165;&#21487;&#20197;&#26174;&#33879;&#20943;&#23567;&#25968;&#25454;&#22823;&#23567;&#65292;&#36824;&#21487;&#20197;&#26681;&#25454;&#29992;&#25143;&#25351;&#23450;&#30340;&#35823;&#24046;&#30028;&#38480;&#26469;&#25511;&#21046;&#21387;&#32553;&#35823;&#24046;&#12290;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;AE&#65289;&#27169;&#22411;&#22312;&#22270;&#20687;&#21387;&#32553;&#20013;&#34987;&#24191;&#27867;&#20351;&#29992;&#65292;&#20294;&#24456;&#23569;&#26377;&#22522;&#20110;AE&#30340;&#21387;&#32553;&#26041;&#27861;&#25903;&#25345;&#35823;&#24046;&#30028;&#38480;&#29305;&#24615;&#65292;&#32780;&#36825;&#22312;&#31185;&#23398;&#24212;&#29992;&#20013;&#26159;&#38750;&#24120;&#38656;&#35201;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#20351;&#29992;&#21367;&#31215;&#33258;&#21160;&#32534;&#30721;&#22120;&#25913;&#36827;&#35823;&#24046;&#26377;&#30028;&#31185;&#23398;&#25968;&#25454;&#21387;&#32553;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20197;&#19979;&#19977;&#20010;&#20851;&#38190;&#36129;&#29486;&#65306;&#65288;1&#65289;&#25105;&#20204;&#28145;&#20837;&#30740;&#31350;&#20102;&#21508;&#31181;&#33258;&#21160;&#32534;&#30721;&#22120;&#27169;&#22411;&#30340;&#29305;&#24615;&#65292;&#24182;&#22312;SZ&#27169;&#22411;&#30340;&#22522;&#30784;&#19978;&#24320;&#21457;&#20102;&#19968;&#20010;&#35823;&#24046;&#26377;&#30028;&#30340;&#33258;&#21160;&#32534;&#30721;&#22120;&#26694;&#26550;&#12290;&#65288;2&#65289;&#25105;&#20204;&#20248;&#21270;&#20102;&#25105;&#20204;&#35774;&#35745;&#30340;&#22522;&#20110;AE&#30340;&#35823;&#24046;&#26377;&#30028;&#21387;&#32553;&#26694;&#26550;&#20013;&#30340;&#20027;&#35201;&#38454;&#27573;&#30340;&#21387;&#32553;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Error-bounded lossy compression is becoming an indispensable technique for the success of today's scientific projects with vast volumes of data produced during simulations or instrument data acquisitions. Not only can it significantly reduce data size, but it also can control the compression errors based on user-specified error bounds. Autoencoder (AE) models have been widely used in image compression, but few AE-based compression approaches support error-bounding features, which are highly required by scientific applications. To address this issue, we explore using convolutional autoencoders to improve error-bounded lossy compression for scientific data, with the following three key contributions. (1) We provide an in-depth investigation of the characteristics of various autoencoder models and develop an error-bounded autoencoder-based framework in terms of the SZ model. (2) We optimize the compression quality for the main stages in our designed AE-based error-bounded compression fram
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;&#25512;&#33616;&#31995;&#32479;&#21644;&#19979;&#38477;&#29702;&#35770;&#20013;&#30340;&#38468;&#21152;&#35889;&#65292;&#24182;&#35299;&#37322;&#20102;&#23427;&#20204;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;&#36890;&#36807;&#23545;&#20351;&#29992;&#30697;&#38453;&#36827;&#34892;&#27010;&#24565;&#20998;&#26512;&#65292;&#25512;&#33616;&#31995;&#32479;&#26500;&#24314;&#29992;&#25143;&#37197;&#32622;&#25991;&#20214;&#65292;&#24182;&#24418;&#25104;&#20285;&#32599;&#21326;&#36830;&#25509;&#12290;&#19979;&#38477;&#26159;&#19968;&#31181;&#29992;&#20110;&#20195;&#25968;&#20960;&#20309;&#21644;&#25299;&#25169;&#30340;&#35889;&#20998;&#35299;&#26041;&#27861;&#65292;&#20063;&#23548;&#33268;&#20102;&#24191;&#20041;&#20285;&#32599;&#21326;&#36830;&#25509;&#12290;&#36825;&#31687;&#35770;&#25991;&#23545;&#25968;&#25454;&#20998;&#26512;&#38382;&#39064;&#25552;&#20986;&#20102;&#26032;&#39062;&#30340;&#33539;&#30068;&#35770;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2004.07353</link><description>&lt;p&gt;
Nucleus I: &#25512;&#33616;&#31995;&#32479;&#21644;&#19979;&#38477;&#20013;&#30340;&#38468;&#21152;&#35889; (arXiv:2004.07353v4 [math.CT] &#26356;&#26032;)
&lt;/p&gt;
&lt;p&gt;
Nucleus I: Adjunction spectra in recommender systems and descent. (arXiv:2004.07353v4 [math.CT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2004.07353
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;&#25512;&#33616;&#31995;&#32479;&#21644;&#19979;&#38477;&#29702;&#35770;&#20013;&#30340;&#38468;&#21152;&#35889;&#65292;&#24182;&#35299;&#37322;&#20102;&#23427;&#20204;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;&#36890;&#36807;&#23545;&#20351;&#29992;&#30697;&#38453;&#36827;&#34892;&#27010;&#24565;&#20998;&#26512;&#65292;&#25512;&#33616;&#31995;&#32479;&#26500;&#24314;&#29992;&#25143;&#37197;&#32622;&#25991;&#20214;&#65292;&#24182;&#24418;&#25104;&#20285;&#32599;&#21326;&#36830;&#25509;&#12290;&#19979;&#38477;&#26159;&#19968;&#31181;&#29992;&#20110;&#20195;&#25968;&#20960;&#20309;&#21644;&#25299;&#25169;&#30340;&#35889;&#20998;&#35299;&#26041;&#27861;&#65292;&#20063;&#23548;&#33268;&#20102;&#24191;&#20041;&#20285;&#32599;&#21326;&#36830;&#25509;&#12290;&#36825;&#31687;&#35770;&#25991;&#23545;&#25968;&#25454;&#20998;&#26512;&#38382;&#39064;&#25552;&#20986;&#20102;&#26032;&#39062;&#30340;&#33539;&#30068;&#35770;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#36890;&#36807;&#23545;&#20351;&#29992;&#30697;&#38453;&#36827;&#34892;&#27010;&#24565;&#20998;&#26512;&#26469;&#26500;&#24314;&#29992;&#25143;&#37197;&#32622;&#25991;&#20214;&#12290;&#36825;&#20123;&#27010;&#24565;&#34987;&#35270;&#20026;&#35889;&#24182;&#24418;&#25104;&#20285;&#32599;&#21326;&#36830;&#25509;&#12290;&#19979;&#38477;&#26159;&#20195;&#25968;&#20960;&#20309;&#21644;&#25299;&#25169;&#20013;&#35889;&#20998;&#35299;&#30340;&#19968;&#31181;&#36890;&#29992;&#26041;&#27861;&#65292;&#20063;&#23548;&#33268;&#20102;&#24191;&#20041;&#20285;&#32599;&#21326;&#36830;&#25509;&#12290;&#25512;&#33616;&#31995;&#32479;&#21644;&#19979;&#38477;&#29702;&#35770;&#37117;&#26159;&#24191;&#27867;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#30001;&#20110;&#25216;&#26415;&#24046;&#36317;&#36807;&#22823;&#65292;&#35797;&#22270;&#24314;&#31435;&#32852;&#31995;&#20284;&#20046;&#26159;&#24858;&#34850;&#30340;&#12290;&#28982;&#32780;&#65292;&#19968;&#20010;&#24418;&#24335;&#38142;&#25509;&#33258;&#24049;&#24418;&#25104;&#65292;&#33258;&#24213;&#21521;&#19978;&#65292;&#22312;&#20316;&#32773;&#30340;&#24847;&#22270;&#21644;&#26368;&#20339;&#21028;&#26029;&#20043;&#22806;&#12290;&#29087;&#24713;&#30340;&#25968;&#25454;&#20998;&#26512;&#38382;&#39064;&#23548;&#33268;&#20102;&#33539;&#30068;&#35770;&#20013;&#30340;&#19968;&#31181;&#26032;&#39062;&#35299;&#20915;&#26041;&#26696;&#12290;&#26412;&#25991;&#26159;&#19968;&#31995;&#21015;&#26089;&#26399;&#21162;&#21147;&#30340;&#32467;&#26524;&#65292;&#26088;&#22312;&#25552;&#20379;&#23545;&#36825;&#20123;&#21457;&#23637;&#30340;&#33258;&#19978;&#32780;&#19979;&#30340;&#35828;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recommender systems build user profiles using concept analysis of usage matrices. The concepts are mined as spectra and form Galois connections. Descent is a general method for spectral decomposition in algebraic geometry and topology which also leads to generalized Galois connections. Both recommender systems and descent theory are vast research areas, separated by a technical gap so large that trying to establish a link would seem foolish. Yet a formal link emerged, all on its own, bottom-up, against authors' intentions and better judgment. Familiar problems of data analysis led to a novel solution in category theory. The present paper arose from a series of earlier efforts to provide a top-down account of these developments.
&lt;/p&gt;</description></item></channel></rss>