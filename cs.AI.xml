<rss version="2.0"><channel><title>Chinese Chat Arxiv AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;QD&#31639;&#27861;&#8212;&#8212;MAP-Elites-Multi-ES&#65288;MEMES&#65289;&#65292;&#35813;&#31639;&#27861;&#22522;&#20110;&#36827;&#21270;&#31574;&#30053;&#65288;ES&#65289;&#21644;&#24555;&#36895;&#24182;&#34892;&#35780;&#20272;&#65292;&#36890;&#36807;&#32500;&#25252;&#20855;&#26377;&#22823;&#35268;&#27169;&#24182;&#34892;&#24615;&#30340;&#22810;&#20010;&#29420;&#31435;ES&#32447;&#31243;&#26469;&#25193;&#23637;&#29616;&#26377;&#31639;&#27861;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#31639;&#27861;&#22312;&#20195;&#25968;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#65292;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#31639;&#27861;&#21019;&#26032;&#12290;</title><link>http://arxiv.org/abs/2303.06137</link><description>&lt;p&gt;
&#22810;&#25163;&#36731;&#26494;&#23436;&#25104;&#65306;&#20351;&#29992;&#22810;&#20010;&#24182;&#34892;&#36827;&#21270;&#31574;&#30053;&#30340;MAP-Elites&#22686;&#24378;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;
&lt;/p&gt;
&lt;p&gt;
Multiple Hands Make Light Work: Enhancing Quality and Diversity using MAP-Elites with Multiple Parallel Evolution Strategies. (arXiv:2303.06137v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
[http://arxiv.org/abs/2303.06137](http://arxiv.org/abs/2303.06137)
&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#30828;&#20214;&#21152;&#36895;&#22120;&#21450;&#20854;&#30456;&#24212;&#24037;&#20855;&#30340;&#21457;&#23637;&#65292;&#22312;&#26576;&#20123;&#24212;&#29992;&#20013;&#36890;&#36807;&#24555;&#36895;&#21644;&#22823;&#35268;&#27169;&#30340;&#35780;&#20272;&#65292;&#35780;&#20272;&#24050;&#21464;&#24471;&#26356;&#21152;&#32463;&#27982;&#23454;&#24800;&#12290;&#36825;&#31181;&#36827;&#23637;&#26497;&#22823;&#22320;&#21152;&#24555;&#20102;&#21551;&#21457;&#24335;&#31639;&#27861;&#65288;&#22914;&#36136;&#37327;-&#22810;&#26679;&#24615;&#20248;&#21270;&#65289;&#30340;&#36816;&#34892;&#26102;&#65292;&#24182;&#21019;&#36896;&#20102;&#36890;&#36807;&#35268;&#27169;&#36827;&#34892;&#31639;&#27861;&#21019;&#26032;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MAP-Elites-Multi-ES&#65288;MEMES&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#36827;&#21270;&#31574;&#30053;&#65288;ES&#65289;&#30340;&#26032;&#22411;QD&#31639;&#27861;&#65292;&#19987;&#20026;&#24555;&#36895;&#24182;&#34892;&#35780;&#20272;&#32780;&#35774;&#35745;&#12290; ME-Multi-ES&#22312;&#29616;&#26377;&#30340;MAP-Elites-ES&#31639;&#27861;&#22522;&#30784;&#19978;&#36827;&#34892;&#20102;&#25913;&#36827;&#65292;&#36890;&#36807;&#32500;&#25252;&#20855;&#26377;&#22823;&#35268;&#27169;&#24182;&#34892;&#24615;&#30340;&#22810;&#20010;&#29420;&#31435;ES&#32447;&#31243;&#26469;&#25193;&#23637;&#23427;&#12290;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#21160;&#24577;&#37325;&#32622;&#31243;&#24207;&#65292;&#29992;&#20110;&#33258;&#20027;&#26368;&#22823;&#21270;QD&#32676;&#20307;&#30340;&#25913;&#36827;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#19982;&#22522;&#20110;&#26799;&#24230;&#21644;&#23458;&#35266;&#19981;&#21487;&#30693;&#30340;QD&#31639;&#27861;&#30456;&#27604;&#65292;MEMES&#22312;&#20195;&#25968;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the development of hardware accelerators and their corresponding tools, evaluations have become more affordable through fast and massively parallel evaluations in some applications. This advancement has drastically sped up the runtime of evolution-inspired algorithms such as Quality-Diversity optimization, creating tremendous potential for algorithmic innovation through scale. In this work, we propose MAP-Elites-Multi-ES (MEMES), a novel QD algorithm based on Evolution Strategies (ES) designed for fast parallel evaluations. ME-Multi-ES builds on top of the existing MAP-Elites-ES algorithm, scaling it by maintaining multiple independent ES threads with massive parallelization. We also introduce a new dynamic reset procedure for the lifespan of the independent ES to autonomously maximize the improvement of the QD population. We show experimentally that MEMES outperforms existing gradient-based and objective-agnostic QD algorithms when compared in terms of generations. We perform thi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22870;&#21169;&#31995;&#32479;&#26469;&#35757;&#32451;&#20248;&#31168;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#21033;&#29992;&#29992;&#25143;&#21453;&#39304;&#25968;&#25454;&#21435;&#31579;&#36873;&#36755;&#20986;&#26469;&#25552;&#39640;&#20445;&#30041;&#29575;&#65292;A/B&#27979;&#35797;&#34920;&#26126;&#35813;&#26041;&#27861;&#33021;&#25552;&#39640;68%&#30340;&#20445;&#30041;&#29575;&#12290;</title><link>http://arxiv.org/abs/2303.06135</link><description>&lt;p&gt;
&#29992;&#20110;&#30334;&#19975;&#29992;&#25143;&#30495;&#23454;&#19990;&#30028;&#20114;&#21160;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#22870;&#21169;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Rewarding Chatbots for Real-World Engagement with Millions of Users. (arXiv:2303.06135v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
[http://arxiv.org/abs/2303.06135](http://arxiv.org/abs/2303.06135)
&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20986;&#29616;&#23548;&#33268;&#20102;&#19968;&#31995;&#21015;&#30340;&#31038;&#20132;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#37096;&#32626;&#65292;&#29992;&#20110;&#38386;&#32842;&#12290;&#34429;&#28982;&#36825;&#20123;&#32842;&#22825;&#26426;&#22120;&#20154;&#23637;&#31034;&#20102;&#35821;&#35328;&#33021;&#21147;&#21644;&#27969;&#21033;&#24230;&#65292;&#20294;&#23427;&#20204;&#24182;&#19981;&#19968;&#23450;&#24341;&#20154;&#20837;&#32988;&#65292;&#26377;&#26102;&#20505;&#24456;&#38590;&#21560;&#24341;&#29992;&#25143;&#12290;&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#24320;&#21457;&#27880;&#37325;&#29992;&#25143;&#21442;&#19982;&#24230;&#30340;&#31038;&#20132;&#32842;&#22825;&#26426;&#22120;&#20154;&#20197;&#22686;&#24378;&#20445;&#30041;&#29575;&#65292;&#29305;&#21035;&#26159;&#32771;&#23519;&#20102;&#20351;&#29992;&#20154;&#31867;&#21453;&#39304;&#26469;&#39640;&#25928;&#24320;&#21457;&#26497;&#20855;&#21560;&#24341;&#21147;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21033;&#29992;&#20174;&#29992;&#25143;&#20132;&#20114;&#20013;&#25910;&#38598;&#30340;&#33258;&#21160;&#20266;&#26631;&#31614;&#26469;&#35757;&#32451;&#19968;&#20010;&#22870;&#21169;&#27169;&#22411;&#65292;&#22312;&#25512;&#29702;&#26102;&#21487;&#20197;&#29992;&#26469;&#25298;&#32477;&#32842;&#22825;&#26426;&#22120;&#20154;&#27169;&#22411;&#20135;&#29983;&#30340;&#20302;&#20998;&#26679;&#26412;&#21709;&#24212;&#12290;&#24341;&#20837;&#20102;&#30452;&#35266;&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;&#22914;&#24179;&#22343;&#23545;&#35805;&#38271;&#24230; (MCL)&#65292;&#20316;&#20026;&#34913;&#37327;&#37096;&#32626;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#21442;&#19982;&#27700;&#24179;&#30340;&#20195;&#29702;&#12290;&#22312;Chai Research&#24179;&#21488;&#19978;&#65292;&#23545;&#27599;&#26085;&#26032;&#30340;10,000&#20010;&#32842;&#22825;&#26426;&#22120;&#20154;&#29992;&#25143;&#32452;&#36827;&#34892;&#30340;A/B&#27979;&#35797;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#23558;MCL&#25552;&#39640;&#20102;&#26368;&#22810;70&#65285;&#65292;&#36825;&#30456;&#24403;&#20110;&#23558;&#20445;&#30041;&#29575;&#20174;40&#65285;&#22686;&#21152;&#21040;68&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
The emergence of pretrained large language models has led to the deployment of a range of social chatbots for chitchat. Although these chatbots demonstrate language ability and fluency, they are not guaranteed to be engaging and can struggle to retain users. This work investigates the development of social chatbots that prioritize user engagement to enhance retention, specifically examining the use of human feedback to efficiently develop highly engaging chatbots. The proposed approach uses automatic pseudo-labels collected from user interactions to train a reward model that can be used to reject low-scoring sample responses generated by the chatbot model at inference time. Intuitive evaluation metrics, such as mean conversation length (MCL), are introduced as proxies to measure the level of engagement of deployed chatbots. A/B testing on groups of 10,000 new daily chatbot users on the Chai Research platform shows that this approach increases the MCL by up to 70%, which translates to a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#35757;&#32451;&#26679;&#26412;&#38590;&#24230;&#24179;&#34913;&#30340;&#23616;&#37096;&#25551;&#36848;&#31526;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#27491;&#36127;&#26679;&#26412;&#19981;&#24179;&#34913;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#24179;&#34913;&#25439;&#22833;&#23545;&#36127;&#26679;&#26412;&#30340;&#36136;&#37327;&#36827;&#34892;&#26377;&#25928;&#30340;&#37492;&#21035;&#65292;&#24182;&#20351;&#29992;&#21160;&#24577;&#26799;&#24230;&#35843;&#33410;&#26469;&#23454;&#29616;&#26679;&#26412;&#38590;&#24230;&#30340;&#24179;&#34913;&#12290;</title><link>http://arxiv.org/abs/2303.06124</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#35757;&#32451;&#26679;&#26412;&#38590;&#24230;&#24179;&#34913;&#30340;&#23616;&#37096;&#25551;&#36848;&#31526;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Self-supervised Training Sample Difficulty Balancing for Local Descriptor Learning. (arXiv:2303.06124v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
[http://arxiv.org/abs/2303.06124](http://arxiv.org/abs/2303.06124)
&lt;/p&gt;
&lt;p&gt;
&#22312;&#27491;&#36127;&#26679;&#26412;&#19981;&#24179;&#34913;&#30340;&#24773;&#20917;&#19979;&#65292;&#30828;&#36127;&#26679;&#26412;&#25366;&#25496;&#31574;&#30053;&#24050;&#34987;&#35777;&#26126;&#26377;&#21161;&#20110;&#27169;&#22411;&#23398;&#20064;&#27491;&#36127;&#26679;&#26412;&#20043;&#38388;&#26356;&#24494;&#22937;&#30340;&#24046;&#24322;&#65292;&#20174;&#32780;&#25552;&#39640;&#35782;&#21035;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22914;&#26524;&#22312;&#25968;&#25454;&#38598;&#20013;&#25512;&#24191;&#36807;&#20110;&#20005;&#26684;&#30340;&#25366;&#25496;&#31574;&#30053;&#65292;&#21487;&#33021;&#23384;&#22312;&#24341;&#20837;&#20551;&#36127;&#26679;&#26412;&#30340;&#39118;&#38505;&#12290;&#21516;&#26102;&#65292;&#25366;&#25496;&#31574;&#30053;&#30340;&#23454;&#26045;&#30772;&#22351;&#20102;&#30495;&#23454;&#25968;&#25454;&#38598;&#20013;&#26679;&#26412;&#30340;&#38590;&#24230;&#20998;&#24067;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#27169;&#22411;&#36807;&#24230;&#25311;&#21512;&#36825;&#20123;&#38590;&#30340;&#26679;&#26412;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#26435;&#34913;&#25366;&#25496;&#26679;&#26412;&#30340;&#38590;&#24230;&#65292;&#20197;&#33719;&#24471;&#21644;&#21033;&#29992;&#39640;&#36136;&#37327;&#30340;&#36127;&#26679;&#26412;&#65292;&#24182;&#35797;&#22270;&#20174;&#25439;&#22833;&#20989;&#25968;&#21644;&#35757;&#32451;&#31574;&#30053;&#20004;&#26041;&#38754;&#35299;&#20915;&#38382;&#39064;&#12290;&#25152;&#25552;&#20986;&#30340;&#24179;&#34913;&#25439;&#22833;&#36890;&#36807;&#23558;&#33258;&#30417;&#30563;&#26041;&#27861;&#19982;&#25439;&#22833;&#20989;&#25968;&#30456;&#32467;&#21512;&#65292;&#25552;&#20379;&#20102;&#36127;&#26679;&#26412;&#36136;&#37327;&#30340;&#26377;&#25928;&#37492;&#21035;&#65292;&#24182;&#20351;&#29992;&#21160;&#24577;&#26799;&#24230;&#35843;&#33410;&#26469;&#23454;&#29616;&#26679;&#26412;&#38590;&#24230;&#30340;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the case of an imbalance between positive and negative samples, hard negative mining strategies have been shown to help models learn more subtle differences between positive and negative samples, thus improving recognition performance. However, if too strict mining strategies are promoted in the dataset, there may be a risk of introducing false negative samples. Meanwhile, the implementation of the mining strategy disrupts the difficulty distribution of samples in the real dataset, which may cause the model to over-fit these difficult samples. Therefore, in this paper, we investigate how to trade off the difficulty of the mined samples in order to obtain and exploit high-quality negative samples, and try to solve the problem in terms of both the loss function and the training strategy. The proposed balance loss provides an effective discriminant for the quality of negative samples by combining a self-supervised approach to the loss function, and uses a dynamic gradient modulation st
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20449;&#24687;&#38376;&#25511;&#30340;&#27010;&#24565;&#65292;&#21363;&#23398;&#20064;&#29992;&#20110;&#35299;&#20915;&#29305;&#23450;&#20219;&#21153;&#30340;&#26368;&#23567;&#20449;&#24687;&#25513;&#30721;&#65292;&#20197;&#23454;&#29616;&#26356;&#31616;&#27905;&#30340;&#34920;&#31034;&#12290;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#21508;&#31181;&#30446;&#26631;&#65292;&#24182;&#19988;&#21487;&#20197;&#25913;&#21892;&#31639;&#27861;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.06121</link><description>&lt;p&gt;
&#26080;&#30693;&#21363;&#31119;&#65306;&#36890;&#36807;&#20449;&#24687;&#38376;&#25511;&#23454;&#29616;&#40065;&#26834;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Ignorance is Bliss: Robust Control via Information Gating. (arXiv:2303.06121v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
[http://arxiv.org/abs/2303.06121](http://arxiv.org/abs/2303.06121)
&lt;/p&gt;
&lt;p&gt;
&#20449;&#24687;&#31616;&#27905;&#24615;&#8212;&#8212;&#21363;&#20351;&#29992;&#20219;&#21153;&#25152;&#38656;&#30340;&#26368;&#23567;&#20449;&#24687;&#8212;&#8212;&#20026;&#23398;&#20064;&#34920;&#31034;&#25552;&#20379;&#20102;&#26377;&#29992;&#30340;&#24402;&#32435;&#20559;&#32622;&#65292;&#20174;&#32780;&#36890;&#36807;&#23545;&#22122;&#22768;&#21644;&#20266;&#30456;&#20851;&#24615;&#20855;&#26377;&#40065;&#26834;&#24615;&#32780;&#23454;&#29616;&#26356;&#22909;&#30340;&#27867;&#21270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20687;&#32032;&#31354;&#38388;&#20013;&#30340;&#20449;&#24687;&#38376;&#25511;&#20316;&#20026;&#23398;&#20064;&#26356;&#31616;&#27905;&#34920;&#31034;&#30340;&#19968;&#31181;&#26041;&#27861;&#12290;&#20449;&#24687;&#38376;&#25511;&#30340;&#20316;&#29992;&#26159;&#23398;&#20064;&#25429;&#25417;&#35299;&#20915;&#32473;&#23450;&#20219;&#21153;&#25152;&#38656;&#30340;&#26368;&#23567;&#20449;&#24687;&#30340;&#25513;&#30721;&#12290;&#30452;&#35266;&#22320;&#35828;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#23398;&#20064;&#35782;&#21035;&#21738;&#20123;&#35270;&#35273;&#32447;&#32034;&#23545;&#20110;&#32473;&#23450;&#20219;&#21153;&#23454;&#38469;&#19978;&#24456;&#37325;&#35201;&#12290;&#25105;&#20204;&#20351;&#29992;&#20449;&#22122;&#27604;&#30340;&#21487;&#24494;&#21442;&#25968;&#21270;&#23545;&#20449;&#21495;&#36827;&#34892;&#38376;&#25511;&#65292;&#35813;&#20449;&#22122;&#27604;&#21487;&#24212;&#29992;&#20110;&#32593;&#32476;&#20013;&#30340;&#20219;&#24847;&#20540;&#65292;&#20363;&#22914;&#23631;&#34109;&#36755;&#20837;&#23618;&#30340;&#20687;&#32032;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#31216;&#20026; InfoGating&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#21508;&#31181;&#30446;&#26631;&#65292;&#20363;&#22914;&#65306;&#22810;&#27493;&#21069;&#21521;&#21644;&#36870;&#21521;&#21160;&#21147;&#23398;&#12289;Q&#23398;&#20064;&#12289;&#34892;&#20026;&#20811;&#38534;&#21644;&#26631;&#20934;&#33258;&#30417;&#30563;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#23398;&#20064;&#35782;&#21035;&#21644;&#20351;&#29992;&#26368;&#23567;&#20449;&#24687;&#26377;&#21161;&#20110;&#25913;&#21892;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Informational parsimony -- i.e., using the minimal information required for a task, -- provides a useful inductive bias for learning representations that achieve better generalization by being robust to noise and spurious correlations. We propose information gating in the pixel space as a way to learn more parsimonious representations. Information gating works by learning masks that capture only the minimal information required to solve a given task. Intuitively, our models learn to identify which visual cues actually matter for a given task. We gate information using a differentiable parameterization of the signal-to-noise ratio, which can be applied to arbitrary values in a network, e.g.~masking out pixels at the input layer. We apply our approach, which we call InfoGating, to various objectives such as: multi-step forward and inverse dynamics, Q-learning, behavior cloning, and standard self-supervised tasks. Our experiments show that learning to identify and use minimal information 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32771;&#34385;&#20102;&#32852;&#37030;&#20915;&#31574;&#20013;&#20449;&#24687;&#32858;&#21512;&#30340;&#38382;&#39064;&#65292;&#24314;&#31435;&#20102;&#27719;&#38598;&#31574;&#30053;&#37117;&#23548;&#33268;&#31995;&#32479;&#30340;&#28176;&#36817;&#27491;&#24577;&#24615;&#25551;&#36848;&#65292;&#29992;&#20110;&#35745;&#31639;&#38169;&#35823;&#27010;&#29575;&#65292;&#24182;&#36890;&#36807;&#27169;&#25311;&#39564;&#35777;&#20102;&#29702;&#35770;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.06109</link><description>&lt;p&gt;
&#20851;&#20110;&#32852;&#37030;&#20915;&#31574;&#20013;&#34701;&#21512;&#31574;&#30053;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Fusion Strategies for Federated Decision Making. (arXiv:2303.06109v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
[http://arxiv.org/abs/2303.06109](http://arxiv.org/abs/2303.06109)
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#32852;&#37030;&#20915;&#31574;&#20013;&#20449;&#24687;&#32858;&#21512;&#30340;&#38382;&#39064;&#65292;&#35813;&#38382;&#39064;&#28041;&#21450;&#19968;&#32452;&#20195;&#29702;&#21327;&#20316;&#20197;&#25512;&#26029;&#33258;&#28982;&#29366;&#24577;&#65292;&#32780;&#19981;&#19982;&#20013;&#22830;&#22788;&#29702;&#22120;&#25110;&#24444;&#27492;&#20849;&#20139;&#31169;&#20154;&#25968;&#25454;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#38750;&#36125;&#21494;&#26031;&#31038;&#20132;&#23398;&#20064;&#31574;&#30053;&#65292;&#22312;&#35813;&#31574;&#30053;&#20013;&#65292;&#20195;&#29702;&#23558;&#20854;&#20010;&#20154;&#35266;&#23519;&#32467;&#26524;&#19982;&#36125;&#21494;&#26031;&#35268;&#21017;&#30456;&#32467;&#21512;&#65292;&#24418;&#25104;&#35266;&#28857;&#65288;&#21363;&#36719;&#20915;&#31574;&#65289;&#65292;&#24182;&#19988;&#20013;&#22830;&#22788;&#29702;&#22120;&#36890;&#36807;&#31639;&#26415;&#25110;&#20960;&#20309;&#24179;&#22343;&#32858;&#21512;&#36825;&#20123;&#35266;&#28857;&#12290;&#22522;&#20110;&#25105;&#20204;&#20043;&#21069;&#30340;&#24037;&#20316;&#65292;&#25105;&#20204;&#30830;&#23450;&#20004;&#31181;&#27719;&#38598;&#31574;&#30053;&#37117;&#23548;&#33268;&#31995;&#32479;&#30340;&#28176;&#36817;&#27491;&#24577;&#24615;&#25551;&#36848;&#65292;&#20363;&#22914;&#65292;&#21487;&#20197;&#21033;&#29992;&#36825;&#31181;&#25551;&#36848;&#36817;&#20284;&#35745;&#31639;&#38169;&#35823;&#27010;&#29575;&#30340;&#34920;&#36798;&#24335;&#12290;&#25105;&#20204;&#36890;&#36807;&#27169;&#25311;&#39564;&#35777;&#20102;&#29702;&#35770;&#32467;&#26524;&#24182;&#27604;&#36739;&#20102;&#20004;&#31181;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of information aggregation in federated decision making, where a group of agents collaborate to infer the underlying state of nature without sharing their private data with the central processor or each other. We analyze the non-Bayesian social learning strategy in which agents incorporate their individual observations into their opinions (i.e., soft-decisions) with Bayes rule, and the central processor aggregates these opinions by arithmetic or geometric averaging. Building on our previous work, we establish that both pooling strategies result in asymptotic normality characterization of the system, which, for instance, can be utilized in order to give approximate expressions for the error probability. We verify the theoretical findings with simulations and compare both strategies.
&lt;/p&gt;</description></item><item><title>HiNet&#26159;&#19968;&#31181;&#29992;&#20110;&#22810;&#22330;&#26223;&#21644;&#22810;&#20219;&#21153;&#25512;&#33616;&#30340;&#20998;&#23618;&#20449;&#24687;&#25552;&#21462;&#32593;&#32476;&#65292;&#33021;&#22815;&#22686;&#24378;&#36328;&#22330;&#26223;&#20256;&#36882;&#26377;&#20215;&#20540;&#20449;&#24687;&#30340;&#33021;&#21147;&#65292;&#21516;&#26102;&#20445;&#30041;&#22330;&#26223;&#21644;&#20219;&#21153;&#30340;&#29305;&#23450;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2303.06095</link><description>&lt;p&gt;
HiNet:&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#22330;&#26223;&#21644;&#22810;&#20219;&#21153;&#23398;&#20064;&#26041;&#27861;&#65292;&#20855;&#26377;&#20998;&#23618;&#20449;&#24687;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
HiNet: A Novel Multi-Scenario &amp; Multi-Task Learning Approach with Hierarchical Information Extraction. (arXiv:2303.06095v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
[http://arxiv.org/abs/2303.06095](http://arxiv.org/abs/2303.06095)
&lt;/p&gt;
&lt;p&gt;
&#22810;&#22330;&#26223;&#21644;&#22810;&#20219;&#21153;&#23398;&#20064;&#24050;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#24037;&#19994;&#24212;&#29992;&#30340;&#35768;&#22810;&#25512;&#33616;&#31995;&#32479;&#20013;&#65292;&#22312;Mixture-of-Expert&#65288;MoE&#65289;&#20307;&#31995;&#32467;&#26500;&#30340;&#22522;&#30784;&#19978;&#36827;&#34892;&#22810;&#22330;&#26223;&#36801;&#31227;&#23398;&#20064;&#26159;&#19968;&#31181;&#26377;&#25928;&#21644;&#23454;&#29992;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#20197;&#23558;&#25152;&#26377;&#20449;&#24687;&#25237;&#24433;&#21040;&#21516;&#19968;&#29305;&#24449;&#31354;&#38388;&#20026;&#30446;&#26631;&#30340;MoE&#26041;&#27861;&#26080;&#27861;&#26377;&#25928;&#22320;&#22788;&#29702;&#21508;&#31181;&#22330;&#26223;&#21644;&#20219;&#21153;&#20043;&#38388;&#22266;&#26377;&#30340;&#22797;&#26434;&#20851;&#31995;&#65292;&#23548;&#33268;&#34920;&#29616;&#19981;&#20339;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#22810;&#22330;&#26223;&#21644;&#22810;&#20219;&#21153;&#25512;&#33616;&#30340;&#20998;&#23618;&#20449;&#24687;&#25552;&#21462;&#32593;&#32476;&#65288;HiNet&#65289;&#65292;&#23427;&#36890;&#36807;&#31895;&#21040;&#32454;&#30340;&#30693;&#35782;&#20256;&#36882;&#26041;&#26696;&#23454;&#29616;&#20998;&#23618;&#25552;&#21462;&#12290;&#20998;&#23618;&#32593;&#32476;&#30340;&#22810;&#20010;&#25552;&#21462;&#23618;&#20351;&#27169;&#22411;&#33021;&#22815;&#22686;&#24378;&#36328;&#22330;&#26223;&#20256;&#36882;&#26377;&#20215;&#20540;&#20449;&#24687;&#30340;&#33021;&#21147;&#65292;&#21516;&#26102;&#20445;&#30041;&#22330;&#26223;&#21644;&#20219;&#21153;&#30340;&#29305;&#23450;&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#22330;&#26223;&#24863;&#30693;&#27880;&#24847;&#21147;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Multi-scenario &amp; multi-task learning has been widely applied to many recommendation systems in industrial applications, wherein an effective and practical approach is to carry out multi-scenario transfer learning on the basis of the Mixture-of-Expert (MoE) architecture. However, the MoE-based method, which aims to project all information in the same feature space, cannot effectively deal with the complex relationships inherent among various scenarios and tasks, resulting in unsatisfactory performance. To tackle the problem, we propose a Hierarchical information extraction Network (HiNet) for multi-scenario and multi-task recommendation, which achieves hierarchical extraction based on coarse-to-fine knowledge transfer scheme. The multiple extraction layers of the hierarchical network enable the model to enhance the capability of transferring valuable information across scenarios while preserving specific features of scenarios and tasks. Furthermore, a novel scenario-aware attentive netw
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#30340;&#22270;&#20687;&#21040;&#35821;&#38899;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#22312;&#20302;&#36164;&#28304;&#35774;&#22791;&#19978;&#21576;&#29616;&#23567;&#37096;&#20998;&#26174;&#31034;&#20869;&#23481;&#30340;&#38899;&#39057;&#28210;&#26579;&#65292;&#35299;&#20915;&#20102;&#30828;&#20214;&#23618;&#38754;&#19978;&#35270;&#38556;&#25110;&#35270;&#35273;&#20998;&#25955;&#29992;&#25143;&#30340;&#21487;&#35775;&#38382;&#24615;&#38382;&#39064;&#65292;&#24182;&#19988;&#27604;&#38750;E2E&#26041;&#27861;&#26356;&#39640;&#25928;&#65292;&#20351;&#29992;&#30340;&#21442;&#25968;&#26356;&#23569;&#65292;&#38477;&#20302;&#20102;&#30005;&#35805;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2303.06078</link><description>&lt;p&gt;
&#19968;&#31181;&#31471;&#21040;&#31471;&#30340;&#22270;&#20687;&#21040;&#38899;&#39057;&#36716;&#25442;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
An End-to-End Neural Network for Image-to-Audio Transformation. (arXiv:2303.06078v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
[http://arxiv.org/abs/2303.06078](http://arxiv.org/abs/2303.06078)
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#65288;E2E&#65289;&#31070;&#32463;&#32467;&#26500;&#65292;&#29992;&#20110;&#22312;&#20302;&#36164;&#28304;&#20010;&#20154;&#35745;&#31639;&#35774;&#22791;&#19978;&#21576;&#29616;&#23567;&#37096;&#20998;&#26174;&#31034;&#20869;&#23481;&#30340;&#38899;&#39057;&#28210;&#26579;&#12290;&#26088;&#22312;&#35299;&#20915;&#30828;&#20214;&#23618;&#38754;&#19978;&#35270;&#38556;&#25110;&#35270;&#35273;&#20998;&#25955;&#29992;&#25143;&#30340;&#21487;&#35775;&#38382;&#24615;&#38382;&#39064;&#12290;&#22238;&#39038;&#20102;&#31070;&#32463;&#22270;&#20687;&#21040;&#25991;&#26412;&#65288;ITT&#65289;&#21644;&#25991;&#26412;&#21040;&#35821;&#38899;&#65288;TTS&#65289;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#25216;&#26415;&#65292;&#20197;&#26377;&#25928;&#22320;&#23558;&#23427;&#20204;&#38598;&#25104;&#22312;&#19968;&#36215;&#65292;&#26082;&#39640;&#25928;&#21448;&#21487;&#21453;&#21521;&#20256;&#25773;&#65292;&#20174;&#32780;&#23548;&#33268;&#19968;&#31181;&#38750;&#33258;&#22238;&#24402;&#30340;E2E&#22270;&#20687;&#21040;&#35821;&#38899;&#65288;ITS&#65289;&#31070;&#32463;&#32593;&#32476;&#65292;&#36825;&#31181;&#31070;&#32463;&#32593;&#32476;&#20855;&#26377;&#39640;&#25928;&#21644;&#21487;&#35757;&#32451;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#38750;E2E&#26041;&#27861;&#30456;&#27604;&#65292;&#25152;&#25552;&#20986;&#30340;E2E&#31995;&#32479;&#36895;&#24230;&#25552;&#39640;&#20102;29&#65285;&#65292;&#21442;&#25968;&#20943;&#23569;&#20102;19&#65285;&#65292;&#30005;&#35805;&#20934;&#30830;&#29575;&#38477;&#20302;&#20102;2&#65285;&#12290;&#25552;&#20986;&#20102;&#19968;&#20010;&#26410;&#26469;&#26041;&#21521;&#26469;&#35299;&#20915;&#20934;&#30830;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper describes an end-to-end (E2E) neural architecture for the audio rendering of small portions of display content on low resource personal computing devices. It is intended to address the problem of accessibility for vision-impaired or vision-distracted users at the hardware level. Neural image-to-text (ITT) and text-to-speech (TTS) approaches are reviewed and a new technique is introduced to efficiently integrate them in a way that is both efficient and back-propagate-able, leading to a non-autoregressive E2E image-to-speech (ITS) neural network that is efficient and trainable. Experimental results are presented showing that, compared with the non-E2E approach, the proposed E2E system is 29% faster and uses 19% fewer parameters with a 2% reduction in phone accuracy. A future direction to address accuracy is presented.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#20351;&#29992;&#27010;&#29575;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#33041;&#30005;&#21512;&#25104;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#20174;&#24102;&#26377;&#24773;&#24863;&#26631;&#31614;&#30340;EEG&#35760;&#24405;&#30340;&#30005;&#26497;-&#39057;&#29575;&#20998;&#24067;&#22270;&#20013;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2303.06068</link><description>&lt;p&gt;
&#20351;&#29992;&#27010;&#29575;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#33041;&#30005;&#21512;&#25104;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
EEG Synthetic Data Generation Using Probabilistic Diffusion Models. (arXiv:2303.06068v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
[http://arxiv.org/abs/2303.06068](http://arxiv.org/abs/2303.06068)
&lt;/p&gt;
&lt;p&gt;
&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#30001;&#20110;&#20854;&#26080;&#21019;&#12289;&#20302;&#25104;&#26412;&#21644;&#26131;&#20110;&#20351;&#29992;&#30340;&#29305;&#28857;&#22312;&#22823;&#33041;&#35745;&#31639;&#26426;&#30028;&#39047;&#20855;&#24433;&#21709;&#21147;&#65292;&#36825;&#20351;&#20854;&#25104;&#20026;&#20844;&#20247;&#24191;&#27867;&#37319;&#29992;&#30340;&#39640;&#24230;&#29702;&#24819;&#30340;&#36873;&#39033;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20808;&#36827;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65306;&#20351;&#29992;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#29983;&#25104;&#21512;&#25104;EEG&#25968;&#25454;&#12290;&#21512;&#25104;&#25968;&#25454;&#26159;&#20174;&#24102;&#26377;&#24773;&#24863;&#26631;&#31614;&#30340;EEG&#35760;&#24405;&#30340;&#30005;&#26497;-&#39057;&#29575;&#20998;&#24067;&#22270;&#65288;EFDM&#65289;&#20013;&#29983;&#25104;&#30340;&#12290;&#20026;&#20102;&#35780;&#20272;&#25152;&#29983;&#25104;&#30340;&#21512;&#25104;&#25968;&#25454;&#30340;&#26377;&#25928;&#24615;&#65292;&#25104;&#21151;&#22320;&#36827;&#34892;&#20102;&#23450;&#24615;&#21644;&#23450;&#37327;&#19982;&#30495;&#23454;EEG&#25968;&#25454;&#30340;&#27604;&#36739;&#12290; 
&lt;/p&gt;
&lt;p&gt;
Electroencephalography (EEG) plays a significant role in the Brain Computer Interface (BCI) domain, due to its non-invasive nature, low cost, and ease of use, making it a highly desirable option for widespread adoption by the general public. This technology is commonly used in conjunction with deep learning techniques, the success of which is largely dependent on the quality and quantity of data used for training. To address the challenge of obtaining sufficient EEG data from individual participants while minimizing user effort and maintaining accuracy, this study proposes an advanced methodology for data augmentation: generating synthetic EEG data using denoising diffusion probabilistic models. The synthetic data are generated from electrode-frequency distribution maps (EFDMs) of emotionally labeled EEG recordings. To assess the validity of the synthetic data generated, both a qualitative and a quantitative comparison with real EEG data were successfully conducted. This study opens up
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#26032;&#39062;&#30340;&#21160;&#24577;&#19979;&#21322;&#36523;&#36127;&#21387;&#27169;&#22411;&#27169;&#25311;&#25345;&#32493;&#24615;&#22833;&#34880;&#27700;&#24179;&#65292;&#24182;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#26694;&#26550;&#20998;&#31867;&#37492;&#21035;&#19981;&#21516;&#31243;&#24230;&#30340;&#22833;&#34880;&#27700;&#24179;&#65292;&#20026;&#24613;&#35786;&#20998;&#32423;&#25552;&#20379;&#26032;&#24605;&#36335;&#12290;</title><link>http://arxiv.org/abs/2303.06064</link><description>&lt;p&gt;
&#38750;&#20405;&#20837;&#24335;&#27874;&#24418;&#20998;&#26512;&#29992;&#20110;&#36890;&#36807;&#27169;&#25311;&#22833;&#34880;&#36827;&#34892;&#24613;&#35786;&#20998;&#32423;&#65306;&#19968;&#39033;&#20351;&#29992;&#26032;&#39062;&#30340;&#21160;&#24577;&#19979;&#21322;&#36523;&#36127;&#21387;&#27169;&#22411;&#30340;&#23454;&#39564;&#30740;&#31350;&#12290;(arXiv:2303.06064v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
Non-invasive Waveform Analysis for Emergency Triage via Simulated Hemorrhage: An Experimental Study using Novel Dynamic Lower Body Negative Pressure Model. (arXiv:2303.06064v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
[http://arxiv.org/abs/2303.06064](http://arxiv.org/abs/2303.06064)
&lt;/p&gt;
&lt;p&gt;
&#25506;&#32034;&#38750;&#20405;&#20837;&#24615;&#29983;&#29702;&#20449;&#21495;&#30340;&#20808;&#36827;&#27874;&#24418;&#20998;&#26512;&#33021;&#22815;&#35786;&#26029;&#22833;&#34880;&#31243;&#24230;&#30340;&#31243;&#24230;&#20173;&#19981;&#36275;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#26694;&#26550;&#20998;&#31867;&#36827;&#34892;&#19979;&#21322;&#36523;&#36127;&#21387;&#65288;LBNP&#65289;&#27169;&#25311;&#30340;&#25345;&#32493;&#24615;&#22833;&#34880;&#27700;&#24179;&#30340;&#37492;&#21035;&#33021;&#21147;&#12290;&#25105;&#20204;&#20351;&#29992;&#21160;&#24577;LBNP&#21327;&#35758;&#65292;&#32780;&#19981;&#26159;&#20256;&#32479;&#30340;&#27169;&#22411;&#65292;&#22312;&#35813;&#21327;&#35758;&#20013;&#65292;LBNP&#20197;&#21487;&#39044;&#27979;&#30340;&#36880;&#27493;&#19979;&#38477;&#30340;&#26041;&#24335;&#26045;&#21152;&#12290;&#35813;&#21160;&#24577;LBNP&#29256;&#26412;&#26377;&#21161;&#20110;&#35299;&#20915;&#26102;&#38388;&#20381;&#36182;&#24615;&#38382;&#39064;&#65292;&#22240;&#20026;&#22312;&#29616;&#23454;&#29983;&#27963;&#20013;&#30340;&#38498;&#21069;&#35774;&#32622;&#20013;&#65292;&#30001;&#20110;&#20307;&#31215;&#22797;&#33487;&#65292;&#34880;&#31649;&#20869;&#30340;&#34880;&#23481;&#37327;&#21487;&#33021;&#20250;&#27874;&#21160;&#12290;&#36890;&#36807;&#20998;&#21106;&#24213;&#23618;&#38750;&#20405;&#20837;&#24335;&#20449;&#21495;&#24182;&#20351;&#29992;&#30456;&#24212;&#30340;LBNP&#30446;&#26631;&#32423;&#21035;&#23545;&#29255;&#27573;&#36827;&#34892;&#26631;&#35760;&#65292;&#23454;&#29616;&#20102;&#29992;&#20110;&#19977;&#20803;&#20998;&#31867;&#30340;&#30417;&#30563;&#24335;DL&#26694;&#26550;&#12290;&#20855;&#26377;&#20004;&#20010;&#36755;&#20837;&#30340;DL&#27169;&#22411;&#32463;&#36807;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
The extent to which advanced waveform analysis of non-invasive physiological signals can diagnose levels of hypovolemia remains insufficiently explored. The present study explores the discriminative ability of a deep learning (DL) framework to classify levels of ongoing hypovolemia, simulated via novel dynamic lower body negative pressure (LBNP) model among healthy volunteers. We used a dynamic LBNP protocol as opposed to the traditional model, where LBNP is applied in a predictable step-wise, progressively descending manner. This dynamic LBNP version assists in circumventing the problem posed in terms of time dependency, as in real-life pre-hospital settings, intravascular blood volume may fluctuate due to volume resuscitation. A supervised DL-based framework for ternary classification was realized by segmenting the underlying noninvasive signal and labeling segments with corresponding LBNP target levels. The proposed DL model with two inputs was trained with respective time-frequency
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;MLP&#26550;&#26500;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#27861;TSMixer&#65292;&#36890;&#36807;&#28151;&#21512;&#25805;&#20316;&#65292;&#21033;&#29992;&#26102;&#38388;&#21644;&#29305;&#24449;&#32500;&#24230;&#39640;&#25928;&#25552;&#21462;&#20449;&#24687;&#65292;&#22312;M5&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20248;&#20110;&#26368;&#26032;&#30340;&#26367;&#20195;&#26041;&#26696;</title><link>http://arxiv.org/abs/2303.06053</link><description>&lt;p&gt;
TSMixer&#65306;&#19968;&#31181;&#20840;MLP&#26550;&#26500;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
TSMixer: An all-MLP Architecture for Time Series Forecasting. (arXiv:2303.06053v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
[http://arxiv.org/abs/2303.06053](http://arxiv.org/abs/2303.06053)
&lt;/p&gt;
&lt;p&gt;
&#23454;&#38469;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#36890;&#24120;&#26159;&#22810;&#21464;&#37327;&#19988;&#20855;&#26377;&#22797;&#26434;&#30340;&#21160;&#24577;&#29305;&#24615;&#12290;&#20256;&#32479;&#30340;&#39640;&#23481;&#37327;&#20307;&#31995;&#32467;&#26500;&#65292;&#22914;&#22522;&#20110;&#36882;&#24402;&#25110;&#27880;&#24847;&#21147;&#30340;&#39034;&#24207;&#27169;&#22411;&#24050;&#32463;&#21464;&#24471;&#27969;&#34892;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#31616;&#21333;&#30340;&#21333;&#21464;&#37327;&#32447;&#24615;&#27169;&#22411;&#21487;&#20197;&#32988;&#36807;&#37027;&#20123;&#28145;&#24230;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#32447;&#24615;&#27169;&#22411;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#26102;&#38388;&#24207;&#21015;&#28151;&#21512;&#22120;&#65288;TSMixer&#65289;&#65292;&#19968;&#31181;&#36890;&#36807;&#22534;&#21472;&#22810;&#23618;&#24863;&#30693;&#26426;&#65288;MLP&#65289;&#35774;&#35745;&#30340;&#20307;&#31995;&#32467;&#26500;&#12290;TSMixer&#22522;&#20110;&#27839;&#26102;&#38388;&#21644;&#29305;&#24449;&#32500;&#24230;&#36827;&#34892;&#28151;&#21512;&#25805;&#20316;&#65292;&#20197;&#26377;&#25928;&#25552;&#21462;&#20449;&#24687;&#12290;&#22312;&#27969;&#34892;&#30340;&#23398;&#26415;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#26131;&#20110;&#23454;&#29616;&#30340;TSMixer&#19982;&#21033;&#29992;&#29305;&#23450;&#22522;&#20934;&#30340;&#24402;&#32435;&#20559;&#24046;&#30340;&#19987;&#19994;&#26368;&#26032;&#27169;&#22411;&#30456;&#24403;&#12290;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#21644;&#22823;&#35268;&#27169;&#30340;M5&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#21363;&#23454;&#38469;&#30340;&#38646;&#21806;&#25968;&#25454;&#38598;&#19978;&#65292;TSMixer&#34920;&#29616;&#20248;&#20110;&#26368;&#26032;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#24378;&#35843;&#20102;&#39640;&#25928;&#25552;&#21462;&#20449;&#24687;&#30340;&#37325;&#35201;&#24615;
&lt;/p&gt;
&lt;p&gt;
Real-world time-series datasets are often multivariate with complex dynamics. Commonly-used high capacity architectures like recurrent- or attention-based sequential models have become popular. However, recent work demonstrates that simple univariate linear models can outperform those deep alternatives. In this paper, we investigate the capabilities of linear models for time-series forecasting and present Time-Series Mixer (TSMixer), an architecture designed by stacking multi-layer perceptrons (MLPs). TSMixer is based on mixing operations along time and feature dimensions to extract information efficiently. On popular academic benchmarks, the simple-to-implement TSMixer is comparable to specialized state-of-the-art models that leverage the inductive biases of specific benchmarks. On the challenging and large scale M5 benchmark, a real-world retail dataset, TSMixer demonstrates superior performance compared to the state-of-the-art alternatives. Our results underline the importance of ef
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#20102;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#21644;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#26469;&#39044;&#27979;&#33258;&#26432;&#39118;&#38505;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20915;&#31574;&#26641;&#65288;DT&#65289;&#12289;&#38543;&#26426;&#26862;&#26519;&#65288;RF&#65289;&#21644;&#26497;&#38480;&#26799;&#24230;&#25552;&#21319;&#65288;XGBoost&#65289;&#27169;&#22411;&#21462;&#24471;&#20102;&#26368;&#20339;&#32467;&#26524;&#12290;&#24868;&#24594;&#38382;&#39064;&#12289;&#25233;&#37057;&#30151;&#21644;&#31038;&#20132;&#38548;&#31163;&#26159;&#39044;&#27979;&#33258;&#26432;&#39118;&#38505;&#30340;&#20027;&#35201;&#21464;&#37327;&#65292;&#32780;&#25910;&#20837;&#22909;&#12289;&#32844;&#19994;&#21463;&#20154;&#23562;&#25964;&#21644;&#25509;&#21463;&#22823;&#23398;&#25945;&#32946;&#30340;&#24739;&#32773;&#39118;&#38505;&#26368;&#23567;&#12290;&#36825;&#20123;&#32467;&#26524;&#35777;&#26126;&#20102;&#26426;&#22120;&#23398;&#20064;&#21644;XAI&#26694;&#26550;&#22312;&#33258;&#26432;&#39118;&#38505;&#39044;&#27979;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#20197;&#21450;&#23427;&#20204;&#22312;&#21307;&#30103;&#20445;&#20581;&#21644;&#20844;&#20849;&#21355;&#29983;&#39046;&#22495;&#30340;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2303.06052</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#22312;&#33258;&#26432;&#39118;&#38505;&#35780;&#20272;&#20013;&#30340;&#20998;&#26512;&#19982;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Analysis and Evaluation of Explainable Artificial Intelligence on Suicide Risk Assessment. (arXiv:2303.06052v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
[http://arxiv.org/abs/2303.06052](http://arxiv.org/abs/2303.06052)
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#25216;&#26415;&#22312;&#39044;&#27979;&#33258;&#26432;&#39118;&#38505;&#21644;&#30830;&#23450;&#20027;&#35201;&#21407;&#22240;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#21033;&#29992;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#21644;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26469;&#39044;&#27979;&#30456;&#20851;&#39118;&#38505;&#12290;&#27492;&#22806;&#65292;&#20351;&#29992;SHapley Additive exPlanations&#65288;SHAP&#65289;&#21644;&#30456;&#20851;&#20998;&#26512;&#26469;&#25490;&#21517;&#39044;&#27979;&#20013;&#21464;&#37327;&#30340;&#37325;&#35201;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20915;&#31574;&#26641;&#65288;DT&#65289;&#12289;&#38543;&#26426;&#26862;&#26519;&#65288;RF&#65289;&#21644;&#26497;&#38480;&#26799;&#24230;&#25552;&#21319;&#65288;XGBoost&#65289;&#27169;&#22411;&#21462;&#24471;&#20102;&#26368;&#20339;&#32467;&#26524;&#65292;&#20854;&#20013;DT&#20855;&#26377;&#26368;&#20339;&#34920;&#29616;&#65292;&#20934;&#30830;&#29575;&#20026;95.23%&#65292;&#26354;&#32447;&#19979;&#38754;&#31215;&#65288;AUC&#65289;&#20026;0.95&#12290;&#26681;&#25454;SHAP&#30340;&#32467;&#26524;&#65292;&#24868;&#24594;&#38382;&#39064;&#12289;&#25233;&#37057;&#30151;&#21644;&#31038;&#20132;&#38548;&#31163;&#26159;&#39044;&#27979;&#33258;&#26432;&#39118;&#38505;&#30340;&#20027;&#35201;&#21464;&#37327;&#65292;&#32780;&#25910;&#20837;&#22909;&#12289;&#32844;&#19994;&#21463;&#20154;&#23562;&#25964;&#21644;&#25509;&#21463;&#22823;&#23398;&#25945;&#32946;&#30340;&#24739;&#32773;&#39118;&#38505;&#26368;&#23567;&#12290;&#32467;&#26524;&#35777;&#26126;&#20102;&#26426;&#22120;&#23398;&#20064;&#21644;XAI&#26694;&#26550;&#22312;&#33258;&#26432;&#39118;&#38505;&#39044;&#27979;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#20197;&#21450;&#23427;&#20204;&#22312;&#21307;&#30103;&#20445;&#20581;&#21644;&#20844;&#20849;&#21355;&#29983;&#39046;&#22495;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study investigates the effectiveness of Explainable Artificial Intelligence (XAI) techniques in predicting suicide risks and identifying the dominant causes for such behaviours. Data augmentation techniques and ML models are utilized to predict the associated risk. Furthermore, SHapley Additive exPlanations (SHAP) and correlation analysis are used to rank the importance of variables in predictions. Experimental results indicate that Decision Tree (DT), Random Forest (RF) and eXtreme Gradient Boosting (XGBoost) models achieve the best results while DT has the best performance with an accuracy of 95:23% and an Area Under Curve (AUC) of 0.95. As per SHAP results, anger problems, depression, and social isolation are the leading variables in predicting the risk of suicide, and patients with good incomes, respected occupations, and university education have the least risk. Results demonstrate the effectiveness of machine learning and XAI framework for suicide risk prediction, and they c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#26368;&#20248;&#35269;&#39135;&#31574;&#30053;&#21487;&#20197;&#34987;&#29983;&#29289;&#23398;&#20064;&#65292;&#23454;&#39564;&#34920;&#26126;&#23398;&#20064;&#21040;&#30340;&#31574;&#30053;&#20248;&#20110;&#24050;&#30693;&#31574;&#30053;&#22914;&#21033;&#32500;&#27493;&#34892;&#12290;</title><link>http://arxiv.org/abs/2303.06050</link><description>&lt;p&gt;
&#26368;&#20248;&#35269;&#39135;&#31574;&#30053;&#21487;&#20197;&#34987;&#23398;&#20064;&#24182;&#36229;&#36807;&#21033;&#32500;&#27493;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
Optimal foraging strategies can be learned and outperform L\'evy walks. (arXiv:2303.06050v1 [cond-mat.stat-mech])
&lt;/p&gt;
&lt;p&gt;
[http://arxiv.org/abs/2303.06050](http://arxiv.org/abs/2303.06050)
&lt;/p&gt;
&lt;p&gt;
&#21033;&#32500;&#27493;&#34892;&#21644;&#20854;&#20182;&#29702;&#35770;&#27169;&#22411;&#24050;&#32463;&#25104;&#21151;&#22320;&#29992;&#20110;&#25551;&#36848;&#29616;&#23454;&#24773;&#20917;&#65292;&#22312;&#32463;&#27982;&#23398;&#12289;&#29289;&#29702;&#23398;&#12289;&#29983;&#24577;&#23398;&#21644;&#36827;&#21270;&#29983;&#29289;&#23398;&#31561;&#22810;&#20010;&#39046;&#22495;&#24341;&#36215;&#20102;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#65292;&#26368;&#22823;&#21270;&#35269;&#39135;&#25928;&#29575;&#30340;&#31574;&#30053;&#20197;&#21450;&#36825;&#20123;&#31574;&#30053;&#26159;&#21542;&#21487;&#20197;&#34987;&#29983;&#29289;&#23398;&#20064;&#36824;&#19981;&#28165;&#26970;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#23558;&#35269;&#39135;&#32773;&#24314;&#27169;&#20026;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#12290;&#25105;&#20204;&#39318;&#20808;&#20174;&#29702;&#35770;&#19978;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#20013;&#26368;&#22823;&#21270;&#22870;&#21169;&#31561;&#20215;&#20110;&#20248;&#21270;&#35269;&#39135;&#25928;&#29575;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#25968;&#23383;&#23454;&#39564;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#20195;&#29702;&#23398;&#20064;&#20102;&#20248;&#20110;&#24050;&#30693;&#31574;&#30053;&#65288;&#22914;&#21033;&#32500;&#27493;&#34892;&#65289;&#25928;&#29575;&#30340;&#35269;&#39135;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
L\'evy walks and other theoretical models of optimal foraging have been successfully used to describe real-world scenarios, attracting attention in several fields such as economy, physics, ecology, and evolutionary biology. However, it remains unclear in most cases which strategies maximize foraging efficiency and whether such strategies can be learned by living organisms. To address these questions, we model foragers as reinforcement learning agents. We first prove theoretically that maximizing rewards in our reinforcement learning model is equivalent to optimizing foraging efficiency. We then show with numerical experiments that our agents learn foraging strategies which outperform the efficiency of known strategies such as L\'evy walks.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#39044;&#27979;&#20892;&#22330;&#24494;&#27668;&#20505;&#26465;&#20214;&#30340;AI&#25216;&#26415;&#65292;&#35813;&#25216;&#26415;&#23558;&#20892;&#19994;&#23454;&#36341;&#19982;&#25968;&#25454;&#20998;&#26512;&#30456;&#32467;&#21512;&#65292;&#26377;&#21161;&#20110;&#22686;&#24378;&#20892;&#27665;&#23545;&#22303;&#22320;&#30340;&#28145;&#20837;&#20102;&#35299;&#65292;&#20351;&#20892;&#19994;&#29983;&#20135;&#26356;&#21487;&#25345;&#32493;&#21644;&#26377;&#21033;&#21487;&#22270;</title><link>http://arxiv.org/abs/2303.06049</link><description>&lt;p&gt;
&#24265;&#20215;&#20154;&#24037;&#26234;&#33021;&#8212;&#8212;&#36890;&#36807;AI&#22686;&#24378;&#20892;&#27665;&#30340;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
Affordable Artificial Intelligence -- Augmenting Farmer Knowledge with AI. (arXiv:2303.06049v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
[http://arxiv.org/abs/2303.06049](http://arxiv.org/abs/2303.06049)
&lt;/p&gt;
&lt;p&gt;
&#20892;&#22330;&#27599;&#22825;&#20250;&#20135;&#29983;&#25968;&#21313;&#19975;&#20010;&#25968;&#25454;&#28857;&#65292;&#23558;&#31181;&#26893;&#25216;&#26415;&#19982;&#36825;&#20123;&#25968;&#25454;&#28857;&#20013;&#21457;&#29616;&#30340;&#35265;&#35299;&#32467;&#21512;&#36215;&#26469;&#30340;&#20892;&#19994;&#25216;&#26415;&#31216;&#20026;&#31934;&#20934;&#20892;&#19994;&#12290;&#31934;&#20934;&#20892;&#19994;&#25216;&#26415;&#22686;&#24378;&#21644;&#25193;&#23637;&#20102;&#20892;&#27665;&#23545;&#22303;&#22320;&#30340;&#28145;&#20837;&#20102;&#35299;&#65292;&#20351;&#29983;&#20135;&#26356;&#21152;&#21487;&#25345;&#32493;&#21644;&#26377;&#21033;&#21487;&#22270;&#12290;&#26412;&#25991;&#20316;&#20026;&#24494;&#36719;&#36171;&#33021;&#20892;&#19994;&#21171;&#21160;&#21147;&#26356;&#20855;&#29983;&#20135;&#21147;&#21644;&#21487;&#25345;&#32493;&#24615;&#30340;&#26356;&#22823;&#21162;&#21147;&#30340;&#19968;&#37096;&#20998;&#65292;&#20171;&#32461;&#20102;&#39044;&#27979;&#20892;&#22330;&#24494;&#27668;&#20505;&#26465;&#20214;&#30340;AI&#25216;&#26415;&#12290;&#26412;&#25991;&#26159;&#32852;&#21512;&#22269;&#31918;&#39135;&#21644;&#20892;&#19994;&#32452;&#32455;&#20197;&#21450;&#22269;&#38469;&#30005;&#20449;&#32852;&#30431;2021&#24180;&#26364;&#35895;&#20986;&#29256;&#30340;&#19968;&#31456;&#12290;&#36825;&#26412;&#20851;&#20110;&#20892;&#19994;&#20154;&#24037;&#26234;&#33021;&#30340;&#20986;&#29256;&#29289;&#26159;&#8220;&#30005;&#23376;&#20892;&#19994;&#34892;&#21160;&#31995;&#21015;&#8221;&#20013;&#30340;&#31532;&#20116;&#26412;&#65292;&#35813;&#31995;&#21015;&#20110;2016&#24180;&#25512;&#20986;&#65292;&#30001;&#32852;&#21512;&#22269;&#31918;&#39135;&#21644;&#20892;&#19994;&#32452;&#32455;&#21644;&#22269;&#38469;&#30005;&#20449;&#32852;&#30431;&#20849;&#21516;&#21046;&#20316;&#12290;&#23427;&#26088;&#22312;&#25552;&#39640;&#20154;&#20204;&#23545;&#20892;&#19994;&#20013;&#29616;&#26377;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#30340;&#35748;&#35782;&#65292;&#24182;&#28608;&#21169;&#21033;&#30410;&#30456;&#20851;&#32773;
&lt;/p&gt;
&lt;p&gt;
Farms produce hundreds of thousands of data points on the ground daily. Farming technique which combines farming practices with the insights uncovered in these data points using AI technology is called precision farming. Precision farming technology augments and extends farmers' deep knowledge about their land, making production more sustainable and profitable. As part of the larger effort at Microsoft for empowering agricultural labor force to be more productive and sustainable, this paper presents the AI technology for predicting micro-climate conditions on the farm.  This article is a chapter in publication by Food and Agriculture Organization of the United Nations and International Telecommunication Union Bangkok, 2021. This publication on artificial intelligence (AI) for agriculture is the fifth in the E-agriculture in Action series, launched in 2016 and jointly produced by FAO and ITU. It aims to raise awareness about existing AI applications in agriculture and to inspire stakeho
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22522;&#20110;&#35270;&#35273;&#30340;&#35302;&#35273;&#20256;&#24863;&#22120;&#36827;&#34892;&#22810;&#23545;&#35937;&#32452;&#35013;&#30340;&#20132;&#20114;&#24335;&#24863;&#30693;&#26041;&#27861;&#65292;&#20854;&#20013;&#26426;&#22120;&#20154;&#20351;&#29992;&#35302;&#35273;&#20256;&#24863;&#22120;&#21644;&#20351;&#29992;&#31890;&#23376;&#28388;&#27874;&#22120;&#30340;&#21453;&#39304;&#26426;&#21046;&#36880;&#27493;&#25913;&#36827;&#20854;&#23545;&#36866;&#21512;&#32452;&#35013;&#30340;&#23545;&#35937;&#30340;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2303.06034</link><description>&lt;p&gt;
Tactile-Filter: &#29992;&#20110;&#37197;&#20214;&#32452;&#35013;&#30340;&#20132;&#20114;&#24335;&#35302;&#35273;&#24863;&#30693;
&lt;/p&gt;
&lt;p&gt;
Tactile-Filter: Interactive Tactile Perception for Part Mating. (arXiv:2303.06034v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
[http://arxiv.org/abs/2303.06034](http://arxiv.org/abs/2303.06034)
&lt;/p&gt;
&lt;p&gt;
&#20154;&#20204;&#20381;&#36182;&#35302;&#35273;&#21644;&#35302;&#35273;&#20256;&#24863;&#22120;&#23436;&#25104;&#35768;&#22810;&#29087;&#32451;&#30340;&#25805;&#20316;&#20219;&#21153;&#12290; &#35302;&#35273;&#20256;&#24863;&#22120;&#20026;&#25105;&#20204;&#25552;&#20379;&#20102;&#35768;&#22810;&#20851;&#20110;&#25509;&#35302;&#24418;&#24577;&#20197;&#21450;&#22312;&#20219;&#20309;&#20132;&#20114;&#26399;&#38388;&#20851;&#20110;&#23545;&#35937;&#30340;&#20960;&#20309;&#20449;&#24687;&#12290; &#20973;&#20511;&#36825;&#31181;&#21160;&#21147;&#65292;&#22522;&#20110;&#35270;&#35273;&#30340;&#35302;&#35273;&#20256;&#24863;&#22120;&#34987;&#24191;&#27867;&#29992;&#20110;&#21508;&#31181;&#26426;&#22120;&#20154;&#24863;&#30693;&#21644;&#25511;&#21046;&#20219;&#21153;&#20013;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22522;&#20110;&#35270;&#35273;&#30340;&#35302;&#35273;&#20256;&#24863;&#22120;&#36827;&#34892;&#22810;&#23545;&#35937;&#32452;&#35013;&#30340;&#20132;&#20114;&#24335;&#24863;&#30693;&#26041;&#27861;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#23545;&#38646;&#20214;&#32452;&#35013;&#36807;&#31243;&#20013;&#30340;&#35302;&#35273;&#24863;&#30693;&#24863;&#20852;&#36259;&#65292;&#20854;&#20013;&#26426;&#22120;&#20154;&#21487;&#20197;&#20351;&#29992;&#35302;&#35273;&#20256;&#24863;&#22120;&#21644;&#20351;&#29992;&#31890;&#23376;&#28388;&#27874;&#22120;&#30340;&#21453;&#39304;&#26426;&#21046;&#36880;&#27493;&#25913;&#36827;&#20854;&#23545;&#36866;&#21512;&#32452;&#35013;&#30340;&#23545;&#35937;&#30340;&#20272;&#35745;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#35757;&#32451;&#20102;&#19968;&#20010;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#21033;&#29992;&#35302;&#35273;&#22270;&#20687;&#26469;&#39044;&#27979;&#24444;&#27492;&#22865;&#21512;&#30340;&#20219;&#24847;&#24418;&#29366;&#23545;&#35937;&#20043;&#38388;&#30340;&#27010;&#29575;&#23545;&#24212;&#20851;&#31995;&#12290;&#35757;&#32451;&#22909;&#30340;&#27169;&#22411;&#29992;&#20110;&#35774;&#35745;&#31890;&#23376;&#28388;&#27874;&#22120;&#65292;&#20854;&#20855;&#26377;&#20004;&#20010;&#29992;&#36884;&#12290; &#39318;&#20808;&#65292;&#32473;&#23450;
&lt;/p&gt;
&lt;p&gt;
Humans rely on touch and tactile sensing for a lot of dexterous manipulation tasks. Our tactile sensing provides us with a lot of information regarding contact formations as well as geometric information about objects during any interaction. With this motivation, vision-based tactile sensors are being widely used for various robotic perception and control tasks. In this paper, we present a method for interactive perception using vision-based tactile sensors for multi-object assembly. In particular, we are interested in tactile perception during part mating, where a robot can use tactile sensors and a feedback mechanism using particle filter to incrementally improve its estimate of objects that fit together for assembly. To do this, we first train a deep neural network that makes use of tactile images to predict the probabilistic correspondence between arbitrarily shaped objects that fit together. The trained model is used to design a particle filter which is used twofold. First, given 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24515;&#24459;&#22833;&#24120;&#20998;&#31867;&#26041;&#27861;&#65292;&#31216;&#20026;HARDC&#65292;&#36890;&#36807;&#21033;&#29992;&#25193;&#24352;CNN&#21644;&#21452;&#21521;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#21333;&#20803;&#26469;&#29983;&#25104;&#34701;&#21512;&#29305;&#24449;&#65292;&#21152;&#19978;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.06020</link><description>&lt;p&gt;
HARDC&#65306;&#19968;&#31181;&#22522;&#20110;&#24515;&#30005;&#22270;&#30340;&#24515;&#36339;&#20998;&#31867;&#26041;&#27861;&#65292;&#20351;&#29992;&#20998;&#23618;&#27880;&#24847;&#21147;&#21452;&#32467;&#26500;RNN&#21644;&#25193;&#24352;CNN(arXiv&#65306;2303.06020v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
HARDC : A novel ECG-based heartbeat classification method to detect arrhythmia using hierarchical attention based dual structured RNN with dilated CNN. (arXiv:2303.06020v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
[http://arxiv.org/abs/2303.06020](http://arxiv.org/abs/2303.06020)
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28151;&#21512;&#20998;&#23618;&#27880;&#24847;&#21147;&#21452;&#21521;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#19982;&#25193;&#24352;CNN&#65288;HARDC&#65289;&#26041;&#27861;&#29992;&#20110;&#24515;&#24459;&#22833;&#24120;&#20998;&#31867;&#12290;&#36825;&#35299;&#20915;&#20102;&#20256;&#32479;&#25193;&#24352;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(CNN)&#27169;&#22411;&#24573;&#30053;&#19978;&#19979;&#25991;&#20043;&#38388;&#30340;&#20851;&#32852;&#21644;&#26799;&#24230;&#20998;&#25955;&#30340;&#38382;&#39064;&#12290;&#25152;&#25552;&#20986;&#30340;HARDC&#20805;&#20998;&#21033;&#29992;&#20102;&#25193;&#24352;CNN&#21644;&#21452;&#21521;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#21333;&#20803;&#65288;BiGRU-BiLSTM&#65289;&#20307;&#31995;&#32467;&#26500;&#26469;&#29983;&#25104;&#34701;&#21512;&#29305;&#24449;&#12290;&#36890;&#36807;&#32467;&#21512;&#26412;&#22320;&#21644;&#20840;&#23616;&#29305;&#24449;&#20449;&#24687;&#21644;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#35813;&#27169;&#22411;&#30340;&#39044;&#27979;&#24615;&#33021;&#24471;&#21040;&#20102;&#25552;&#39640;&#12290;&#36890;&#36807;&#23558;&#34701;&#21512;&#29305;&#24449;&#19982;&#25193;&#24352;CNN&#21644;&#20998;&#23618;&#27880;&#24847;&#21147;&#26426;&#21046;&#30456;&#32467;&#21512;&#65292;&#35757;&#32451;&#21518;&#30340;HARDC&#27169;&#22411;&#22312;PhysioNet 2017&#25361;&#25112;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#20102;&#26126;&#26174;&#30340;&#25913;&#36827;&#20998;&#31867;&#32467;&#26524;&#21644;&#29305;&#24449;&#25552;&#21462;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#20351;&#29992;&#36830;&#32493;Z-Score&#24402;&#19968;&#21270;&#12289;&#28388;&#27874;&#12289;&#21435;&#22122;&#21644;&#20998;&#27573;&#26469;&#20934;&#22791;&#21407;&#22987;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper have developed a novel hybrid hierarchical attention-based bidirectional recurrent neural network with dilated CNN (HARDC) method for arrhythmia classification. This solves problems that arise when traditional dilated convolutional neural network (CNN) models disregard the correlation between contexts and gradient dispersion. The proposed HARDC fully exploits the dilated CNN and bidirectional recurrent neural network unit (BiGRU-BiLSTM) architecture to generate fusion features. As a result of incorporating both local and global feature information and an attention mechanism, the model's performance for prediction is improved.By combining the fusion features with a dilated CNN and a hierarchical attention mechanism, the trained HARDC model showed significantly improved classification results and interpretability of feature extraction on the PhysioNet 2017 challenge dataset. Sequential Z-Score normalization, filtering, denoising, and segmentation are used to prepare the raw
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#32452;&#21512;&#31243;&#24207;&#26469;&#21512;&#25104;&#31243;&#24207;&#30340;&#21487;&#25193;&#23637;&#31243;&#24207;&#21512;&#25104;&#26694;&#26550;&#65292;&#36825;&#33021;&#22815;&#36991;&#20813;&#20174;&#22836;&#21512;&#25104;&#38271;&#25110;&#26356;&#22797;&#26434;&#30340;&#31243;&#24207;&#12290;</title><link>http://arxiv.org/abs/2303.06018</link><description>&lt;p&gt;
&#20998;&#23618;&#31070;&#32463;&#31243;&#24207;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Neural Program Synthesis. (arXiv:2303.06018v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
[http://arxiv.org/abs/2303.06018](http://arxiv.org/abs/2303.06018)
&lt;/p&gt;
&lt;p&gt;
&#31243;&#24207;&#21512;&#25104;&#26088;&#22312;&#33258;&#21160;&#26500;&#24314;&#28385;&#36275;&#32473;&#23450;&#20219;&#21153;&#35268;&#33539;&#65288;&#20363;&#22914;&#36755;&#20837;/&#36755;&#20986;&#23545;&#25110;&#28436;&#31034;&#65289;&#30340;&#20154;&#31867;&#21487;&#35835;&#31243;&#24207;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#22312;&#21508;&#31181;&#39046;&#22495;&#21462;&#24471;&#20102;&#20196;&#20154;&#40723;&#33310;&#30340;&#25104;&#26524;&#65292;&#22914;&#23383;&#31526;&#20018;&#36716;&#25442;&#12289;&#24352;&#37327;&#25805;&#20316;&#21644;&#25551;&#36848;&#20855;&#20307;&#21270;&#36523;&#20195;&#29702;&#30340;&#34892;&#20026;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#31243;&#24207;&#21512;&#25104;&#26041;&#27861;&#37117;&#26159;&#20174;&#22836;&#24320;&#22987;&#21512;&#25104;&#31243;&#24207;&#65292;&#36880;&#20010;&#20196;&#29260;&#12289;&#36880;&#34892;&#29983;&#25104;&#31243;&#24207;&#12290;&#36825;&#20174;&#26681;&#26412;&#19978;&#38459;&#27490;&#20102;&#36825;&#20123;&#26041;&#27861;&#25193;&#23637;&#21040;&#21512;&#25104;&#26356;&#38271;&#25110;&#26356;&#22797;&#26434;&#30340;&#31243;&#24207;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#31243;&#24207;&#21512;&#25104;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#20998;&#23618;&#32452;&#21512;&#31243;&#24207;&#26469;&#21512;&#25104;&#31243;&#24207;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#23398;&#20064;&#20219;&#21153;&#23884;&#20837;&#31354;&#38388;&#21644;&#31243;&#24207;&#35299;&#30721;&#22120;&#65292;&#35813;&#35299;&#30721;&#22120;&#21487;&#20197;&#23558;&#20219;&#21153;&#23884;&#20837;&#35299;&#30721;&#20026;&#31243;&#24207;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35757;&#32451;&#19968;&#20010;&#39640;&#23618;&#27169;&#22359;&#65292;&#20197;&#29702;&#35299;&#26469;&#33258;&#38271;&#31243;&#24207;&#30340;&#20219;&#21153;&#35268;&#33539;&#65288;&#20363;&#22914;&#36755;&#20837;/&#36755;&#20986;&#23545;&#25110;&#28436;&#31034;&#65289;&#24182;&#29983;&#25104;&#31243;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;
Program synthesis aims to automatically construct human-readable programs that satisfy given task specifications, such as input/output pairs or demonstrations. Recent works have demonstrated encouraging results in a variety of domains, such as string transformation, tensor manipulation, and describing behaviors of embodied agents. Most existing program synthesis methods are designed to synthesize programs from scratch, generating a program token by token, line by line. This fundamentally prevents these methods from scaling up to synthesize programs that are longer or more complex. In this work, we present a scalable program synthesis framework that instead synthesizes a program by hierarchically composing programs. Specifically, we first learn a task embedding space and a program decoder that can decode a task embedding into a program. Then, we train a high-level module to comprehend the task specification (e.g., input/output pairs or demonstrations) from long programs and produce a se
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;Pobe&#27169;&#22411;&#65292;&#20197;&#23637;&#26395;&#29702;&#35770;&#20998;&#26512;&#29992;&#25143;&#30340;&#38750;&#29702;&#24615;&#20915;&#31574;&#65292;&#24182;&#35745;&#31639;&#29992;&#25143;&#30340;&#25237;&#24433;&#20559;&#35823;&#65292;&#20174;&#32780;&#21487;&#20197;&#20934;&#30830;&#39044;&#27979;&#29992;&#25143;&#30340;&#36873;&#25321;&#12290;</title><link>http://arxiv.org/abs/2303.06016</link><description>&lt;p&gt;
&#24314;&#31435;&#22522;&#20110;&#23637;&#26395;&#29702;&#35770;&#30340;&#25237;&#24433;&#20559;&#35823;&#30340;&#26102;&#38388;&#36873;&#25321;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Modelling Projection Bias in Intertemporal Choices: A Prospect Theory Based Approach. (arXiv:2303.06016v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
[http://arxiv.org/abs/2303.06016](http://arxiv.org/abs/2303.06016)
&lt;/p&gt;
&lt;p&gt;
&#22312;&#36141;&#20080;&#26102;&#65292;&#29992;&#25143;&#32463;&#24120;&#38754;&#20020;&#25414;&#32465;&#20419;&#38144;&#65292;&#38656;&#35201;&#22312;&#20004;&#20010;&#36873;&#39033;&#20043;&#38388;&#36827;&#34892;&#36873;&#25321;&#65306;&#20197;&#20840;&#20215;&#36141;&#20080;&#21333;&#20010;&#29289;&#21697;&#65292;&#25110;&#20197;&#25240;&#25187;&#20215;&#36141;&#20080;&#25414;&#32465;&#21253;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#29992;&#25143;&#30340;&#20559;&#22909;&#36890;&#24120;&#21463;&#21040;&#25237;&#24433;&#20559;&#35823;&#30340;&#24433;&#21709;&#65292;&#21363;&#29992;&#25143;&#32463;&#24120;&#35748;&#20026;&#20182;&#20204;&#26410;&#26469;&#30340;&#20559;&#22909;&#19982;&#20182;&#20204;&#24403;&#21069;&#30340;&#20559;&#22909;&#30456;&#20284;&#65292;&#23548;&#33268;&#20182;&#20204;&#20570;&#20986;&#38750;&#29702;&#24615;&#21644;&#30701;&#35270;&#30340;&#20915;&#31574;&#12290;&#20998;&#26512;&#25237;&#24433;&#20559;&#35823;&#23545;&#29992;&#25143;&#20559;&#22909;&#30340;&#24433;&#21709;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#65292;&#26412;&#30740;&#31350;&#21487;&#33021;&#26377;&#21161;&#20110;&#29702;&#35299;&#29992;&#25143;&#30340;&#20915;&#31574;&#36807;&#31243;&#65292;&#24182;&#20026;&#21334;&#23478;&#25552;&#20379;&#25414;&#32465;&#21644;&#23450;&#20215;&#31574;&#30053;&#12290;&#20197;&#24448;&#30340;&#24037;&#20316;&#36890;&#24120;&#20351;&#29992;&#32447;&#24615;&#20559;&#35823;&#27169;&#22411;&#36827;&#34892;&#23450;&#24615;&#20998;&#26512;&#65292;&#26080;&#27861;&#23450;&#37327;&#35745;&#31639;&#29992;&#25143;&#30340;&#38750;&#32447;&#24615;&#21644;&#20010;&#24615;&#21270;&#20559;&#35823;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Pobe&#65292;&#19968;&#31181;&#25237;&#24433;&#20559;&#35823;&#23884;&#20837;&#20559;&#22909;&#27169;&#22411;&#65292;&#20197;&#20934;&#30830;&#39044;&#27979;&#29992;&#25143;&#30340;&#36873;&#25321;&#12290;&#25152;&#25552;&#20986;&#30340;Pobe&#24341;&#20837;&#20102;&#23637;&#26395;&#29702;&#35770;&#26469;&#20998;&#26512;&#29992;&#25143;&#30340;&#38750;&#29702;&#24615;&#20915;&#31574;&#65292;&#24182;&#21033;&#29992;
&lt;/p&gt;
&lt;p&gt;
Users often face bundle promotions when purchasing, where they have to select between two options: buy the single item at full price, or buy the bundle at a discount. In this scenario, users' preferences are usually influenced by the projection bias, that is, users often believe that their future preferences are similar to their current preferences, causing them to make irrational and short-sighted decisions. It is of great significance to analyze the effect of the projection bias on users' preferences, and this study may help understand users' decision-making process and provide bundling and pricing strategies for sellers. Prior works typically use a linear bias model for qualitative analysis, and they cannot quantitatively calculate users' nonlinear and personalized bias. In this work, we propose Pobe, a projection bias-embedded preference model to accurately predict users' choices. The proposed Pobe introduces the prospect theory to analyze users' irrational decisions, and utilizes 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#23450;&#20041;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#25991;&#26412;&#30340;&#35270;&#35273;&#37325;&#29616;&#20219;&#21153;&#65292;&#24182;&#26500;&#24314;&#20102;&#26032;&#30340;&#21512;&#25104;CLEVR-NOT&#25968;&#25454;&#38598;&#21644;&#25163;&#21160;&#32472;&#21046;&#30340;Fruit-NOT&#25968;&#25454;&#38598;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#23545;&#31105;&#27490;&#30340;&#25351;&#20196;&#36827;&#34892;&#36127;&#36131;&#24182;&#35828;&#26126;&#29702;&#30001;&#12290;</title><link>http://arxiv.org/abs/2303.05983</link><description>&lt;p&gt;
&#36131;&#20219;&#21270;&#25991;&#26412;&#35270;&#35273;&#37325;&#29616;&#30340;&#26032;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
New Benchmarks for Accountable Text-based Visual Re-creation. (arXiv:2303.05983v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
[http://arxiv.org/abs/2303.05983](http://arxiv.org/abs/2303.05983)
&lt;/p&gt;
&lt;p&gt;
&#32473;&#23450;&#25351;&#20196;&#21518;&#65292;&#20154;&#31867;&#21487;&#20197;&#30452;&#25509;&#25191;&#34892;&#21160;&#20316;&#25110;&#36873;&#25321;&#25298;&#32477;&#65292;&#21516;&#26102;&#21512;&#29702;&#22320;&#21453;&#39304;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#26041;&#27861;&#30340;&#34892;&#20026;&#19981;&#21487;&#25511;&#19988;&#19981;&#36127;&#36131;&#20219;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#22823;&#37327;&#23454;&#39564;&#26469;&#39564;&#35777;&#23427;&#20204;&#33021;&#21542;&#23545;&#36825;&#20123;&#34987;&#31105;&#27490;&#30340;&#25351;&#20196;&#36127;&#36131;&#24182;&#35828;&#26126;&#29702;&#30001;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#25991;&#26412;&#30340;&#35270;&#35273;&#37325;&#29616;&#20219;&#21153;&#65292;&#24182;&#26500;&#24314;&#20102;&#26032;&#30340;&#21512;&#25104;CLEVR-NOT&#25968;&#25454;&#38598;&#65288;620K&#65289;&#21644;&#25163;&#21160;&#32472;&#21046;&#30340;Fruit-NOT&#25968;&#25454;&#38598;&#65288;50K&#65289;&#12290;&#22312;&#25105;&#20204;&#30340;&#26041;&#27861;&#20013;&#65292;&#23558;&#19968;&#20010;&#25991;&#26412;-&#22270;&#20687;&#23545;&#20316;&#20026;&#26597;&#35810;&#39304;&#20837;&#26426;&#22120;&#65292;&#27169;&#22411;&#22312;&#35270;&#35273;&#21644;&#25991;&#26412;&#25512;&#29702;&#20043;&#21518;&#32473;&#20986;&#19968;&#20010;&#26159;&#25110;&#21542;&#30340;&#31572;&#26696;&#12290;&#22914;&#26524;&#31572;&#26696;&#26159;&#32943;&#23450;&#30340;&#65292;&#37027;&#20040;&#22270;&#20687;&#33258;&#21160;&#32534;&#30721;&#22120;&#21644;&#33258;&#22238;&#24402;&#21464;&#25442;&#22120;&#24517;&#39035;&#22312;&#30830;&#20445;&#22270;&#20687;&#36136;&#37327;&#30340;&#21069;&#25552;&#19979;&#23436;&#25104;&#35270;&#35273;&#37325;&#29616;&#65292;&#21542;&#21017;&#31995;&#32479;&#38656;&#35201;&#35299;&#37322;&#20026;&#20160;&#20040;&#21629;&#20196;&#26080;&#27861;&#23436;&#25104;&#25110;&#34987;&#31105;&#27490;&#12290;&#25105;&#20204;&#23545;&#23454;&#39564;&#36827;&#34892;&#20102;&#35814;&#32454;&#30340;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given a command, humans can directly execute the action after thinking or choose to reject it, with reasonable feedback at the same time. However, the behavior of existing text-to-image generation methods are uncontrollable and irresponsible. In this paper, we construct extensive experiments to verify whether they can be accountable (say no and explain why) for those prohibited instructions. To this end, we define a novel text-based visual re-creation task and construct new synthetic CLEVR-NOT dataset (620K) and manually pictured Fruit-NOT dataset (50K). In our method, one text-image pair as the query is fed into the machine, and the model gives a yes or no answer after visual and textual reasoning. If the answer is yes, the image auto-encoder and auto-regressive transformer must complete the visual re-creation under the premise of ensuring image quality, otherwise the system needs to explain why the commands cannot be completed or prohibited. We provide a detailed analysis of experime
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#37319;&#29992;&#21160;&#24577;&#36125;&#21494;&#26031;&#32593;&#32476;&#21644;&#31070;&#32463;&#32593;&#32476;&#65292;&#23545;COVID-19&#24739;&#32773;&#30340;&#30149;&#24773;&#36827;&#34892;&#39044;&#27979;&#20998;&#31867;&#65292;&#35813;&#27169;&#22411;&#20855;&#26377;&#36739;&#39640;&#30340;&#20998;&#31867;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.05972</link><description>&lt;p&gt;
&#37319;&#29992;&#21160;&#24577;&#36125;&#21494;&#26031;&#32593;&#32476;&#21644;&#31070;&#32463;&#32593;&#32476;&#23545;COVID-19&#24739;&#32773;&#30149;&#24773;&#28436;&#21464;&#36827;&#34892;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Classifying the evolution of COVID-19 severity on patients with combined dynamic Bayesian networks and neural networks. (arXiv:2303.05972v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
[http://arxiv.org/abs/2303.05972](http://arxiv.org/abs/2303.05972)
&lt;/p&gt;
&lt;p&gt;
&#24403;&#25105;&#20204;&#38754;&#23545;&#21040;&#36798;&#21307;&#38498;&#30340;&#24739;&#32773;&#24739;&#30149;&#30340;&#24433;&#21709;&#26102;&#65292;&#25105;&#20204;&#20250;&#38754;&#20020;&#19968;&#20010;&#20027;&#35201;&#38382;&#39064;&#65292;&#21363;&#35780;&#20272;&#36825;&#20123;&#24739;&#32773;&#26159;&#21542;&#22312;&#19981;&#20037;&#30340;&#23558;&#26469;&#38656;&#35201;&#37325;&#30151;&#30417;&#25252;&#12290;&#36825;&#31181;&#37325;&#30151;&#30417;&#25252;&#38656;&#35201;&#20998;&#37197;&#23453;&#36149;&#32780;&#31232;&#32570;&#30340;&#36164;&#28304;&#65292;&#39044;&#20808;&#30693;&#36947;&#24739;&#32773;&#30142;&#30149;&#30340;&#20005;&#37325;&#31243;&#24230;&#21487;&#20197;&#25913;&#21892;&#20854;&#27835;&#30103;&#21644;&#36164;&#28304;&#32452;&#32455;&#12290;&#25105;&#20204;&#22312;&#30001;&#35199;&#29677;&#29273;COVID-19&#24739;&#32773;&#32452;&#25104;&#30340;&#25968;&#25454;&#38598;&#20013;&#35828;&#26126;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#20854;&#20013;&#25105;&#20204;&#23558;&#24739;&#32773;&#26631;&#35760;&#20026;&#37325;&#30151;&#24739;&#32773;&#65292;&#24403;&#20182;&#20204;&#19981;&#24471;&#19981;&#36827;&#20837;&#37325;&#30151;&#30417;&#25252;&#23460;&#25110;&#21435;&#19990;&#26102;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#32467;&#21512;&#20351;&#29992;&#21160;&#24577;&#36125;&#21494;&#26031;&#32593;&#32476;&#65292;&#23545;&#26410;&#26469;40&#23567;&#26102;&#20869;&#24739;&#32773;&#30340;&#29983;&#21629;&#20307;&#24449;&#21644;&#34880;&#28082;&#20998;&#26512;&#32467;&#26524;&#36827;&#34892;&#39044;&#27979;&#65292;&#24182;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#35780;&#20272;&#24739;&#32773;&#22312;&#35813;&#26102;&#38388;&#38388;&#38548;&#20869;&#30340;&#30142;&#30149;&#20005;&#37325;&#31243;&#24230;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;DBN&#23558;&#24739;&#32773;&#24403;&#21069;&#29366;&#24577;&#36716;&#31227;&#21040;&#26410;&#26469;&#20540;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#20855;&#26377;&#24456;&#39640;&#30340;&#20998;&#31867;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
When we face patients arriving to a hospital suffering from the effects of some illness, one of the main problems we can encounter is evaluating whether or not said patients are going to require intensive care in the near future. This intensive care requires allotting valuable and scarce resources, and knowing beforehand the severity of a patients illness can improve both its treatment and the organization of resources. We illustrate this issue in a dataset consistent of Spanish COVID-19 patients from the sixth epidemic wave where we label patients as critical when they either had to enter the intensive care unit or passed away. We then combine the use of dynamic Bayesian networks, to forecast the vital signs and the blood analysis results of patients over the next 40 hours, and neural networks, to evaluate the severity of a patients disease in that interval of time. Our empirical results show that the transposition of the current state of a patient to future values with the DBN for it
&lt;/p&gt;</description></item><item><title>&#23545;&#27604;&#25439;&#22833;&#20351;&#27169;&#24577;&#21305;&#37197;&#65292;&#20294;&#23436;&#32654;&#30340;&#27169;&#24577;&#23545;&#40784;&#19981;&#21033;&#20110;&#19979;&#28216;&#39044;&#27979;&#20219;&#21153;&#12290; &#26412;&#25991;&#25552;&#20986;&#26377;&#24847;&#20041;&#30340;&#28508;&#22312;&#27169;&#24577;&#32467;&#26500;&#26159;&#26356;&#22909;&#30340;&#34920;&#29616;&#20851;&#38190;&#12290; &#20316;&#32773;&#35774;&#35745;&#20102;&#19977;&#31181;&#36890;&#29992;&#26041;&#27861;&#65292;&#20998;&#21035;&#26159;&#29992;&#20110;&#27169;&#20869;&#27491;&#21017;&#21270;&#30340;&#28145;&#24230;&#29305;&#24449;&#20998;&#31163;&#25439;&#22833;&#65292;&#29992;&#20110;&#27169;&#38388;&#27491;&#21017;&#21270;&#30340;&#24067;&#26391;&#36816;&#21160;&#26725;&#25439;&#22833;&#20197;&#21450;&#29992;&#20110;&#27169;&#20869;&#21644;&#27169;&#38388;&#27491;&#21017;&#21270;&#30340;&#20960;&#20309;&#19968;&#33268;&#24615;&#25439;&#22833;&#12290; &#32463;&#36807;&#24191;&#27867;&#23454;&#39564;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2303.05952</link><description>&lt;p&gt;
&#29702;&#35299;&#21644;&#26500;&#24314;&#22810;&#27169;&#24577;&#34920;&#31034;&#23398;&#20064;&#20013;&#30340;&#28508;&#22312;&#27169;&#24577;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
Understanding and Constructing Latent Modality Structures in Multi-modal Representation Learning. (arXiv:2303.05952v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
[http://arxiv.org/abs/2303.05952](http://arxiv.org/abs/2303.05952)
&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#25439;&#22833;&#22312;&#20174;&#22810;&#20010;&#27169;&#24577;&#23398;&#20064;&#34920;&#31034;&#20013;&#30340;&#24212;&#29992;&#36234;&#26469;&#36234;&#24191;&#27867;&#12290; &#22312;&#26497;&#38480;&#24773;&#20917;&#19979;&#65292;&#23545;&#27604;&#25439;&#22833;&#30340;&#26412;&#36136;&#20419;&#20351;&#27169;&#24577;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#23436;&#20840;&#21305;&#37197;&#12290; &#28982;&#32780;&#65292;&#27169;&#24577;&#23545;&#40784;&#22914;&#20309;&#24433;&#21709;&#19979;&#28216;&#20219;&#21153;&#24615;&#33021;&#20173;&#28982;&#26159;&#19968;&#20010;&#24320;&#25918;&#30340;&#38382;&#39064;&#12290; &#22522;&#20110;&#20449;&#24687;&#29702;&#35770;&#30340;&#35770;&#25454;&#65292;&#26412;&#25991;&#39318;&#20808;&#35777;&#26126;&#19968;&#33324;&#24773;&#20917;&#19979;&#31934;&#30830;&#30340;&#27169;&#24577;&#23545;&#40784;&#26159;&#27425;&#20248;&#30340;&#19979;&#28216;&#39044;&#27979;&#20219;&#21153;&#12290; &#22240;&#27492;&#65292;&#25105;&#20204;&#20027;&#24352;&#26356;&#22909;&#30340;&#34920;&#29616;&#20851;&#38190;&#22312;&#20110;&#26377;&#24847;&#20041;&#30340;&#28508;&#22312;&#27169;&#24577;&#32467;&#26500;&#65292;&#32780;&#19981;&#26159;&#23436;&#32654;&#30340;&#27169;&#24577;&#23545;&#40784;&#12290; &#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#31181;&#26500;&#24314;&#28508;&#22312;&#27169;&#24577;&#32467;&#26500;&#30340;&#36890;&#29992;&#26041;&#27861;&#12290; &#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;1&#65289;&#29992;&#20110;&#27169;&#20869;&#27491;&#21017;&#21270;&#30340;&#28145;&#24230;&#29305;&#24449;&#20998;&#31163;&#25439;&#22833;&#65307; 2&#65289;&#29992;&#20110;&#27169;&#38388;&#27491;&#21017;&#21270;&#30340;&#24067;&#26391;&#36816;&#21160;&#26725;&#25439;&#22833;&#65307; 3&#65289;&#29992;&#20110;&#27169;&#20869;&#21644;&#27169;&#38388;&#27491;&#21017;&#21270;&#30340;&#20960;&#20309;&#19968;&#33268;&#24615;&#25439;&#22833;&#12290; &#22312;&#20004;&#20010;&#27969;&#34892;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contrastive loss has been increasingly used in learning representations from multiple modalities. In the limit, the nature of the contrastive loss encourages modalities to exactly match each other in the latent space. Yet it remains an open question how the modality alignment affects the downstream task performance. In this paper, based on an information-theoretic argument, we first prove that exact modality alignment is sub-optimal in general for downstream prediction tasks. Hence we advocate that the key of better performance lies in meaningful latent modality structures instead of perfect modality alignment. To this end, we propose three general approaches to construct latent modality structures. Specifically, we design 1) a deep feature separation loss for intra-modality regularization; 2) a Brownian-bridge loss for inter-modality regularization; and 3) a geometric consistency loss for both intra- and inter-modality regularization. Extensive experiments are conducted on two popular
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#29305;&#24449;&#20018;&#32852;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#21069;&#19968;&#24103;&#30340;&#29305;&#24449;&#26144;&#23556;&#24182;&#20837;&#24403;&#21069;&#24103;&#26469;&#26816;&#27979;&#24687;&#32905;&#65292;&#20197;&#27492;&#25552;&#39640;&#35270;&#39057;&#20013;&#33258;&#21160;&#24687;&#32905;&#26816;&#27979;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.05871</link><description>&lt;p&gt;
&#35270;&#39057;&#20013;&#20934;&#30830;&#30340;&#23454;&#26102;&#24687;&#32905;&#26816;&#27979;&#65306;&#26469;&#33258;&#36830;&#32493;&#24103;&#25552;&#21462;&#30340;&#28508;&#22312;&#29305;&#24449;&#30340;&#20018;&#32852;
&lt;/p&gt;
&lt;p&gt;
Accurate Real-time Polyp Detection in Videos from Concatenation of Latent Features Extracted from Consecutive Frames. (arXiv:2303.05871v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
[http://arxiv.org/abs/2303.05871](http://arxiv.org/abs/2303.05871)
&lt;/p&gt;
&lt;p&gt;
&#23454;&#26102;&#30340;&#24687;&#32905;&#26816;&#27979;&#23545;&#20110;&#20943;&#23569;&#31579;&#26597;&#36807;&#31243;&#20013;&#30340;&#28431;&#35786;&#29575;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#30740;&#31350;&#23581;&#35797;&#36890;&#36807;&#22312;&#30456;&#37051;&#24103;&#20043;&#38388;&#25972;&#21512;&#26102;&#38388;&#20449;&#24687;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#29305;&#24449;&#20018;&#32852;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#21069;&#19968;&#24103;&#30340;&#29305;&#24449;&#26144;&#23556;&#24182;&#20837;&#24403;&#21069;&#24103;&#26469;&#26816;&#27979;&#24687;&#32905;&#65292;&#20197;&#27492;&#19981;&#22686;&#21152;&#27169;&#22411;&#30340;&#22797;&#26434;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#29305;&#24449;&#20018;&#32852;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#35270;&#39057;&#20013;&#33258;&#21160;&#24687;&#32905;&#26816;&#27979;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
An efficient deep learning model that can be implemented in real-time for polyp detection is crucial to reducing polyp miss-rate during screening procedures. Convolutional neural networks (CNNs) are vulnerable to small changes in the input image. A CNN-based model may miss the same polyp appearing in a series of consecutive frames and produce unsubtle detection output due to changes in camera pose, lighting condition, light reflection, etc. In this study, we attempt to tackle this problem by integrating temporal information among neighboring frames. We propose an efficient feature concatenation method for a CNN-based encoder-decoder model without adding complexity to the model. The proposed method incorporates extracted feature maps of previous frames to detect polyps in the current frame. The experimental results demonstrate that the proposed method of feature concatenation improves the overall performance of automatic polyp detection in videos. The following results are obtained on a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#35268;&#21017;&#30340;&#23450;&#29702;&#35777;&#26126;&#22120;&#65292;&#20854;&#20351;&#29992;&#20960;&#20309;&#25512;&#29702;&#25968;&#25454;&#24211;&#26041;&#27861;&#23454;&#29616;&#65292;&#21487;&#20197;&#24110;&#21161;&#20013;&#23398;&#29983;&#23398;&#20064;&#35777;&#26126;&#65292;&#23545;&#23398;&#29983;&#36827;&#34892;&#35777;&#26126;&#25945;&#23398;&#26159;&#26377;&#30410;&#30340;&#12290;</title><link>http://arxiv.org/abs/2303.05863</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#35268;&#21017;&#30340;&#23450;&#29702;&#35777;&#26126;&#22120;&#65306;&#20013;&#23398;&#35777;&#26126;&#25945;&#23398;&#20171;&#32461;
&lt;/p&gt;
&lt;p&gt;
A Rule Based Theorem Prover: an Introduction to Proofs in Secondary Schools. (arXiv:2303.05863v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
[http://arxiv.org/abs/2303.05863](http://arxiv.org/abs/2303.05863)
&lt;/p&gt;
&lt;p&gt;
&#22312;&#20013;&#23398;&#24341;&#20837;&#33258;&#21160;&#25512;&#29702;&#31995;&#32479;&#38754;&#20020;&#20960;&#20010;&#29942;&#39048;&#12290;&#38500;&#20102;&#19982;&#35838;&#31243;&#21644;&#25945;&#24072;&#30456;&#20851;&#30340;&#38382;&#39064;&#22806;&#65292;&#20960;&#20309;&#33258;&#21160;&#23450;&#29702;&#35777;&#26126;&#22120;&#30340;&#32467;&#26524;&#19982;&#23398;&#26657;&#29468;&#27979;&#21644;&#35777;&#26126;&#30340;&#27491;&#24120;&#23454;&#36341;&#20043;&#38388;&#30340;&#19981;&#21644;&#35856;&#26159;&#22312;&#25945;&#32946;&#29615;&#22659;&#20013;&#24191;&#27867;&#20351;&#29992;&#27492;&#31867;&#24037;&#20855;&#30340;&#20027;&#35201;&#38556;&#30861;&#12290;&#33258;&#20960;&#20309;&#33258;&#21160;&#23450;&#29702;&#35777;&#26126;&#22120;&#30340;&#26089;&#26399;&#23454;&#29616;&#20197;&#26469;&#65292;&#22522;&#20110;&#25512;&#29702;&#35268;&#21017;&#30340;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#12289;&#32508;&#21512;&#35777;&#26126;&#22120;&#21644;&#20351;&#29992;&#21069;&#21521;&#38142;&#25509;&#25512;&#29702;&#30340;&#26041;&#27861;&#34987;&#35748;&#20026;&#26356;&#36866;&#21512;&#25945;&#32946;&#30446;&#30340;&#12290;&#36873;&#25321;&#36866;&#24403;&#30340;&#35268;&#21017;&#38598;&#21644;&#33021;&#22815;&#20351;&#29992;&#36825;&#20123;&#35268;&#21017;&#30340;&#33258;&#21160;&#21270;&#26041;&#27861;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#36825;&#26679;&#19968;&#31181;&#35268;&#21017;&#38598;&#21450;&#20854;&#20351;&#29992;&#20960;&#20309;&#25512;&#29702;&#25968;&#25454;&#24211;&#26041;&#27861;&#65288;GDDM&#65289;&#30340;&#23454;&#29616;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#20102;&#19968;&#20123;&#36873;&#25321;&#30340;&#20960;&#20309;&#29468;&#24819;&#36827;&#34892;&#27979;&#35797;&#65292;&#36825;&#20123;&#29468;&#24819;&#21487;&#33021;&#26159;&#31532;7&#24180;&#32423;&#65288;&#32422;12&#23681;&#30340;&#23398;&#29983;&#65289;&#29677;&#32423;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
The introduction of automated deduction systems in secondary schools face several bottlenecks. Beyond the problems related with the curricula and the teachers, the dissonance between the outcomes of the geometry automated theorem provers and the normal practice of conjecturing and proving in schools is a major barrier to a wider use of such tools in an educational environment. Since the early implementations of geometry automated theorem provers, applications of artificial intelligence methods, synthetic provers based on inference rules and using forward chaining reasoning are considered to be more suited for education proposes. Choosing an appropriate set of rules and an automated method that can use those rules is a major challenge. We discuss one such rule set and its implementation using the geometry deductive databases method (GDDM). The approach is tested using some chosen geometric conjectures that could be the goal of a 7th year class (approx. 12-year-old students). A lesson pl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#36229;&#36234;&#20256;&#32479;&#27010;&#29575;&#29702;&#35299;&#30340;&#19981;&#30830;&#23450;&#24615;&#20915;&#31574;&#65292;&#20171;&#32461;&#20102;&#35299;&#20915;&#37096;&#20998;&#21487;&#35266;&#27979;&#24615;&#21644;&#23545;&#25239;&#34892;&#20026;&#30340;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#21450;&#20854;&#25193;&#23637;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#36824;&#23637;&#31034;&#20102;&#22810;&#31181;&#35299;&#20915;&#31163;&#25955;&#21644;&#36830;&#32493;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.05848</link><description>&lt;p&gt;
&#19981;&#30830;&#23450;&#24615;&#20915;&#31574;&#65306;&#36229;&#36234;&#27010;&#29575;
&lt;/p&gt;
&lt;p&gt;
Decision-Making Under Uncertainty: Beyond Probabilities. (arXiv:2303.05848v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
[http://arxiv.org/abs/2303.05848](http://arxiv.org/abs/2303.05848)
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#23545;&#19981;&#30830;&#23450;&#24615;&#20915;&#31574;&#30340;&#26368;&#26032;&#21457;&#23637;&#36827;&#34892;&#20102;&#22238;&#39038;&#12290;&#20256;&#32479;&#30340;&#20551;&#35774;&#26159;&#27010;&#29575;&#21487;&#20197;&#20805;&#20998;&#25429;&#25417;&#31995;&#32479;&#20013;&#30340;&#25152;&#26377;&#19981;&#30830;&#23450;&#24615;&#12290;&#26412;&#25991;&#20851;&#27880;&#36229;&#36234;&#36825;&#31181;&#20256;&#32479;&#35299;&#37322;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#29305;&#21035;&#26159;&#36890;&#36807;&#28165;&#26224;&#21306;&#20998;&#27169;&#31946;&#19981;&#30830;&#23450;&#24615;&#21644;&#35748;&#30693;&#19981;&#30830;&#23450;&#24615;&#12290;&#25991;&#31456;&#27010;&#36848;&#20102;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#21450;&#20854;&#25193;&#23637;&#65292;&#20197;&#35299;&#20915;&#37096;&#20998;&#21487;&#35266;&#27979;&#24615;&#21644;&#23545;&#25239;&#34892;&#20026;&#30340;&#38382;&#39064;&#12290;&#36825;&#20123;&#27169;&#22411;&#21487;&#20197;&#24456;&#22909;&#22320;&#25429;&#25417;&#27169;&#31946;&#19981;&#30830;&#23450;&#24615;&#65292;&#20294;&#26410;&#33021;&#20805;&#20998;&#32771;&#34385;&#35748;&#30693;&#19981;&#30830;&#23450;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#23545;&#25152;&#35859;&#30340;&#8220;&#19981;&#30830;&#23450;&#24615;&#27169;&#22411;&#8221;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#27010;&#36848;&#65292;&#24182;&#23637;&#31034;&#20102;&#19968;&#20123;&#31163;&#25955;&#21644;&#36830;&#32493;&#27169;&#22411;&#30340;&#35299;&#20915;&#26041;&#27861;&#65292;&#20174;&#24418;&#24335;&#21270;&#39564;&#35777;&#12289;&#22522;&#20110;&#25511;&#21046;&#30340;&#25277;&#35937;&#21040;&#24378;&#21270;&#23398;&#20064;&#12290;&#20316;&#20026;&#26412;&#25991;&#30340;&#19968;&#20010;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#25105;&#20204;&#21015;&#20986;&#24182;&#35752;&#35770;&#20102;&#33509;&#24178;&#20010;&#20363;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;
This position paper reflects on the state-of-the-art in decision-making under uncertainty. A classical assumption is that probabilities can sufficiently capture all uncertainty in a system. In this paper, the focus is on the uncertainty that goes beyond this classical interpretation, particularly by employing a clear distinction between aleatoric and epistemic uncertainty. The paper features an overview of Markov decision processes (MDPs) and extensions to account for partial observability and adversarial behavior. These models sufficiently capture aleatoric uncertainty but fail to account for epistemic uncertainty robustly. Consequently, we present a thorough overview of so-called uncertainty models that exhibit uncertainty in a more robust interpretation. We show several solution techniques for both discrete and continuous models, ranging from formal verification, over control-based abstractions, to reinforcement learning. As an integral part of this paper, we list and discuss severa
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#34920;&#26126;&#65292;&#23545;&#27604;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#27169;&#22411;&#26080;&#38656;&#20869;&#37096;&#20998;&#24067;&#24494;&#35843;&#21363;&#21487;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#26080;&#30417;&#30563;&#36234;&#30028;&#26816;&#27979;&#34920;&#29616;&#65292;&#24182;&#25552;&#20986;&#26159;&#21542;&#38656;&#35201;&#26032;&#30340;&#35270;&#35273;&#24322;&#24120;&#26816;&#27979;&#22522;&#20934;</title><link>http://arxiv.org/abs/2303.05828</link><description>&lt;p&gt;
&#23545;&#27604;&#35821;&#35328;&#22270;&#20687;&#39044;&#35757;&#32451;&#65288;CLIP&#65289;&#27169;&#22411;&#26159;&#24378;&#22823;&#30340;&#36234;&#30028;&#26816;&#27979;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contrastive Language-Image Pretrained (CLIP) Models are Powerful Out-of-Distribution Detectors. (arXiv:2303.05828v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
[http://arxiv.org/abs/2303.05828](http://arxiv.org/abs/2303.05828)
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23545;&#29992;&#20110;&#35270;&#35273;&#36234;&#30028;&#26816;&#27979;&#30340;&#39044;&#35757;&#32451;&#29305;&#24449;&#25552;&#21462;&#22120;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23454;&#39564;&#30740;&#31350;&#12290; &#25105;&#20204;&#26816;&#26597;&#20102;&#20960;&#20010;&#35774;&#32622;&#65292;&#22522;&#20110;&#26631;&#31614;&#25110;&#22270;&#20687;&#26631;&#39064;&#30340;&#21487;&#29992;&#24615;&#65292;&#20351;&#29992;&#19981;&#21516;&#30340;&#20869;&#37096;&#21644;&#22806;&#37096;&#20998;&#24067;&#30340;&#32452;&#21512;&#12290; &#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#65288;i&#65289;&#23545;&#27604;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#27169;&#22411;&#20351;&#29992;&#26368;&#36817;&#37051;&#29305;&#24449;&#30456;&#20284;&#24615;&#20316;&#20026;&#36234;&#30028;&#26816;&#27979;&#20998;&#25968;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#26080;&#30417;&#30563;&#36234;&#30028;&#26816;&#27979;&#34920;&#29616;&#65292;&#65288;ii&#65289;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#20869;&#37096;&#20998;&#24067;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#33719;&#24471;&#30417;&#30563;&#29366;&#24577;&#19979;&#30340;&#26368;&#20808;&#36827;&#36234;&#30028;&#26816;&#27979;&#24615;&#33021;&#65292;&#65288;iii&#65289;&#21363;&#20351;&#26159;&#38024;&#23545;&#33258;&#28982;&#35821;&#35328;&#30417;&#30563;&#36827;&#34892;&#35757;&#32451;&#30340;&#24615;&#33021;&#26368;&#20339;&#30340;&#21313;&#20159;&#32423;&#35270;&#35273;&#21464;&#25442;&#22120;&#20063;&#26080;&#27861;&#26816;&#27979;&#21040;&#32463;&#36807;&#25932;&#23545;&#25805;&#32437;&#30340;&#36234;&#30028;&#22270;&#20687;&#12290; &#26368;&#21518;&#65292;&#25105;&#20204;&#26681;&#25454;&#23454;&#39564;&#32467;&#26524;&#35752;&#35770;&#20102;&#26159;&#21542;&#38656;&#35201;&#26032;&#30340;&#22522;&#20110;&#35270;&#35273;&#24322;&#24120;&#26816;&#27979;&#30340;&#22522;&#20934;&#12290; &#20351;&#29992;&#26368;&#22823;&#30340;&#20844;&#24320;&#21487;&#29992;&#35270;&#35273;&#21464;&#25442;&#22120;&#65292;&#22312;&#25152;&#26377;18&#20010;&#25253;&#21578;&#30340;&#36234;&#30028;&#22522;&#20934;&#27979;&#35797;&#20013;&#37117;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
We present a comprehensive experimental study on pretrained feature extractors for visual out-of-distribution (OOD) detection. We examine several setups, based on the availability of labels or image captions and using different combinations of in- and out-distributions. Intriguingly, we find that (i) contrastive language-image pretrained models achieve state-of-the-art unsupervised out-of-distribution performance using nearest neighbors feature similarity as the OOD detection score, (ii) supervised state-of-the-art OOD detection performance can be obtained without in-distribution fine-tuning, (iii) even top-performing billion-scale vision transformers trained with natural language supervision fail at detecting adversarially manipulated OOD images. Finally, we argue whether new benchmarks for visual anomaly detection are needed based on our experiments. Using the largest publicly available vision transformer, we achieve state-of-the-art performance across all $18$ reported OOD benchmark
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#22836;&#29305;&#24449;&#33258;&#36866;&#24212;&#27169;&#22359;&#65292;&#29992;&#20110;&#20174;&#28304;&#39046;&#22495;&#21040;&#30446;&#26631;&#39046;&#22495;&#30340;&#30693;&#35782;&#36801;&#31227;&#12290;WSI&#20998;&#31867;&#26159;&#35813;&#26041;&#27861;&#30340;&#29702;&#24819;&#27979;&#35797;&#24179;&#21488;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20855;&#26377;&#30693;&#35782;&#36801;&#31227;&#30340;&#27169;&#22411;&#20248;&#20110;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#30340;&#27169;&#22411;&#65292;&#26080;&#35770;&#25968;&#25454;&#38598;&#20013;&#30340;WSI&#25968;&#37327;&#22914;&#20309;&#12290;</title><link>http://arxiv.org/abs/2303.05780</link><description>&lt;p&gt;
&#22810;&#22836;&#29305;&#24449;&#33258;&#36866;&#24212;&#23398;&#20064;&#20013;&#30340;&#30693;&#35782;&#36801;&#31227;&#22312;&#20840;&#24187;&#28783;&#29255;&#22270;&#20687;&#20998;&#31867;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Knowledge Transfer via Multi-Head Feature Adaptation for Whole Slide Image Classification. (arXiv:2303.05780v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
[http://arxiv.org/abs/2303.05780](http://arxiv.org/abs/2303.05780)
&lt;/p&gt;
&lt;p&gt;
&#20174;&#28304;&#39046;&#22495;&#20256;&#36882;&#20808;&#21069;&#30340;&#30693;&#35782;&#21040;&#30456;&#21516;&#25110;&#30456;&#20284;&#30340;&#30446;&#26631;&#39046;&#22495;&#21487;&#20197;&#26497;&#22823;&#22320;&#25552;&#39640;&#27169;&#22411;&#22312;&#30446;&#26631;&#39046;&#22495;&#19978;&#30340;&#24615;&#33021;&#12290;&#20294;&#26159;&#65292;&#30001;&#20110;&#20219;&#21153;&#19981;&#19968;&#33268;&#21644;&#39046;&#22495;&#36716;&#31227;&#65292;&#30452;&#25509;&#21033;&#29992;&#28304;&#39046;&#22495;&#20013;&#30340;&#30693;&#35782;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#20026;&#20102;&#24357;&#21512;&#19981;&#21516;&#20219;&#21153;&#21644;&#39046;&#22495;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#22836;&#29305;&#24449;&#33258;&#36866;&#24212;&#27169;&#22359;&#65292;&#23427;&#23558;&#28304;&#29305;&#24449;&#31354;&#38388;&#20013;&#30340;&#29305;&#24449;&#25237;&#24433;&#21040;&#19968;&#20010;&#26356;&#31867;&#20284;&#20110;&#30446;&#26631;&#31354;&#38388;&#30340;&#26032;&#31354;&#38388;&#20013;&#12290;&#22312;&#25972;&#20010;&#24187;&#28783;&#29255;&#22270;&#20687;&#65288;WSI&#65289;&#20998;&#31867;&#20013;&#65292;&#30693;&#35782;&#36716;&#31227;&#23588;&#20854;&#37325;&#35201;&#65292;&#22240;&#20026;&#19968;&#20010;&#25968;&#25454;&#38598;&#20013;&#30340;WSI&#25968;&#37327;&#21487;&#33021;&#22826;&#23569;&#32780;&#26080;&#27861;&#36798;&#21040;&#20196;&#20154;&#28385;&#24847;&#30340;&#24615;&#33021;&#12290;&#22240;&#27492;&#65292;WSI&#20998;&#31867;&#26159;&#25105;&#20204;&#26041;&#27861;&#30340;&#29702;&#24819;&#27979;&#35797;&#24179;&#21488;&#65292;&#24182;&#19988;&#25105;&#20204;&#38024;&#23545;WSI&#20998;&#31867;&#35843;&#25972;&#20102;&#22810;&#31181;&#30693;&#35782;&#36801;&#31227;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20855;&#26377;&#30693;&#35782;&#36716;&#31227;&#30340;&#27169;&#22411;&#26080;&#35770;&#25968;&#25454;&#38598;&#20013;&#30340;WSI&#25968;&#37327;&#22914;&#20309;&#65292;&#20854;&#24615;&#33021;&#37117;&#22823;&#22823;&#20248;&#20110;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transferring prior knowledge from a source domain to the same or similar target domain can greatly enhance the performance of models on the target domain. However, it is challenging to directly leverage the knowledge from the source domain due to task discrepancy and domain shift. To bridge the gaps between different tasks and domains, we propose a Multi-Head Feature Adaptation module, which projects features in the source feature space to a new space that is more similar to the target space. Knowledge transfer is particularly important in Whole Slide Image (WSI) classification since the number of WSIs in one dataset might be too small to achieve satisfactory performance. Therefore, WSI classification is an ideal testbed for our method, and we adapt multiple knowledge transfer methods for WSI classification. The experimental results show that models with knowledge transfer outperform models that are trained from scratch by a large margin regardless of the number of WSIs in the datasets
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25551;&#36848;&#20102;&#19968;&#31181;&#26234;&#33021;&#25163;&#26426;&#24212;&#29992;&#31243;&#24207;&#30340;&#24320;&#21457;&#65292;&#20801;&#35768;&#29992;&#25143;&#36890;&#36807;&#8220;&#25381;&#21160;&#8221;&#25163;&#26426;&#23558;&#32440;&#36136;&#25910;&#25454;&#25968;&#23383;&#21270;&#65292;&#24182;&#33258;&#21160;&#26816;&#27979;&#21644;&#26657;&#27491;&#20197;&#36827;&#34892;&#21518;&#32493;&#30340;&#25991;&#26412;&#35782;&#21035;&#12290; &#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#23558;&#27599;&#20010;&#22235;&#20010;&#35282;&#20316;&#20026;&#19968;&#20010;&#29420;&#29305;&#30340;&#8220;&#23545;&#35937;&#8221;&#65292;&#24182;&#35757;&#32451;&#21333;&#27425;&#26816;&#27979;MobileNet&#23545;&#35937;&#26816;&#27979;&#27169;&#22411;&#26469;&#22788;&#29702;&#25910;&#25454;&#35282;&#30340;&#26816;&#27979;&#65292;&#20197;&#35299;&#20915;&#20256;&#32479;&#26041;&#27861;&#22312;&#26816;&#27979;&#21644;&#32416;&#27491;&#25910;&#25454;&#26041;&#38754;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.05763</link><description>&lt;p&gt;
&#26234;&#33021;&#25163;&#26426;&#19978;&#32440;&#36136;&#25910;&#25454;&#30340;&#33258;&#21160;&#26816;&#27979;&#21644;&#30699;&#27491;
&lt;/p&gt;
&lt;p&gt;
Automatic Detection and Rectification of Paper Receipts on Smartphones. (arXiv:2303.05763v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
[http://arxiv.org/abs/2303.05763](http://arxiv.org/abs/2303.05763)
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#31181;&#23454;&#26102;&#26234;&#33021;&#25163;&#26426;&#24212;&#29992;&#30340;&#24320;&#21457;&#65292;&#20801;&#35768;&#29992;&#25143;&#20197;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#24335;&#36890;&#36807;&#8220;&#25381;&#21160;&#8221;&#25163;&#26426;&#23558;&#32440;&#36136;&#25910;&#25454;&#25968;&#23383;&#21270;&#65292;&#35753;&#24212;&#29992;&#31243;&#24207;&#33258;&#21160;&#26816;&#27979;&#21644;&#26657;&#27491;&#25910;&#25454;&#20197;&#36827;&#34892;&#21518;&#32493;&#30340;&#25991;&#26412;&#35782;&#21035;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#20256;&#32479;&#30340;&#36793;&#32536;&#21644;&#35282;&#28857;&#26816;&#27979;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#31639;&#27861;&#19981;&#33021;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#29615;&#22659;&#19979;&#24378;&#21046;&#26816;&#27979;&#20856;&#22411;&#32440;&#36136;&#25910;&#25454;&#30340;&#38750;&#32447;&#24615;&#21644;&#19981;&#36830;&#32493;&#30340;&#36793;&#32536;&#21644;&#35282;&#28857;&#12290;&#29305;&#21035;&#26159;&#22312;&#25910;&#25454;&#21644;&#32972;&#26223;&#39068;&#33394;&#30456;&#20284;&#25110;&#20854;&#20182;&#24178;&#25200;&#30697;&#24418;&#29289;&#20307;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#12290;&#24403;&#20351;&#29992;&#20223;&#23556;&#25237;&#24433;&#21464;&#25442;&#26469;&#30699;&#27491;&#36879;&#35270;&#26102;&#65292;&#25910;&#25454;&#35282;&#30340;&#20301;&#32622;&#30340;&#19981;&#20934;&#30830;&#26816;&#27979;&#20250;&#23548;&#33268;&#22270;&#20687;&#21464;&#24418;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#23558;&#27599;&#20010;&#22235;&#20010;&#35282;&#20316;&#20026;&#19968;&#20010;&#29420;&#29305;&#30340;&#8220;&#23545;&#35937;&#8221;&#65292;&#24182;&#35757;&#32451;&#21333;&#27425;&#26816;&#27979;MobileNet&#23545;&#35937;&#26816;&#27979;&#27169;&#22411;&#26469;&#22788;&#29702;&#25910;&#25454;&#35282;&#30340;&#26816;&#27979;&#12290;&#25105;&#20204;&#20351;&#29992;&#23569;&#37327;&#30340;&#37325;&#26032;&#35757;&#32451;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#20174;&#32780;&#22312;&#21508;&#31181;&#19981;&#21516;&#30340;&#24773;&#20917;&#19979;&#37117;&#33021;&#20934;&#30830;&#26816;&#27979;&#25910;&#25454;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#26816;&#27979;&#21644;&#32416;&#27491;&#25910;&#25454;&#26041;&#38754;&#27604;&#20256;&#32479;&#26041;&#27861;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We describe the development of a real-time smartphone app that allows the user to digitize paper receipts in a novel way by "waving" their phone over the receipts and letting the app automatically detect and rectify the receipts for subsequent text recognition.  We show that traditional computer vision algorithms for edge and corner detection do not robustly detect the non-linear and discontinuous edges and corners of a typical paper receipt in real-world settings. This is particularly the case when the colors of the receipt and background are similar, or where other interfering rectangular objects are present. Inaccurate detection of a receipt's corner positions then results in distorted images when using an affine projective transformation to rectify the perspective.  We propose an innovative solution to receipt corner detection by treating each of the four corners as a unique "object", and training a Single Shot Detection MobileNet object detection model. We use a small amount of re
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#25193;&#25955;&#37319;&#26679;&#31574;&#30053;&#65292;&#37319;&#29992;&#20960;&#20309;&#20998;&#35299;&#30340;&#26041;&#27861;&#35299;&#20915;&#36870;&#38382;&#39064;&#20013;&#26465;&#20214;&#37319;&#26679;&#30340;&#38480;&#21046;&#38382;&#39064;&#12290;&#30740;&#31350;&#32773;&#20204;&#21457;&#29616;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#30340;&#26679;&#26412;&#21487;&#20197;&#20998;&#35299;&#20026;&#21435;&#22122;&#21644;&#22122;&#22768;&#20004;&#20010;&#37096;&#20998;&#65292;&#36825;&#20351;&#24471;&#26045;&#21152;&#26465;&#20214;&#30340;&#20849;&#36717;&#26799;&#24230;&#26356;&#26032;&#23646;&#20110;&#24178;&#20928;&#27969;&#24418;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#37319;&#26679;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2303.05754</link><description>&lt;p&gt;
&#36890;&#36807;&#20960;&#20309;&#20998;&#35299;&#23454;&#29616;&#36870;&#38382;&#39064;&#30340;&#24555;&#36895;&#25193;&#25955;&#37319;&#26679;&#22120;
&lt;/p&gt;
&lt;p&gt;
Fast Diffusion Sampler for Inverse Problems by Geometric Decomposition. (arXiv:2303.05754v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
[http://arxiv.org/abs/2303.05754](http://arxiv.org/abs/2303.05754)
&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#22312;&#35299;&#20915;&#36870;&#38382;&#39064;&#26041;&#38754;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#20294;&#20854;&#25512;&#29702;&#26102;&#38388;&#36739;&#24930;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#39640;&#25928;&#30340;&#25193;&#25955;&#37319;&#26679;&#31574;&#30053;&#65292;&#37319;&#29992;&#25193;&#25955;&#37319;&#26679;&#30340;&#20960;&#20309;&#20998;&#35299;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#21457;&#29616;&#20174;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#30340;&#26679;&#26412;&#21487;&#20197;&#20998;&#35299;&#20026;&#20004;&#20010;&#27491;&#20132;&#20998;&#37327;&#65306;&#36890;&#36807;&#23558;&#26679;&#26412;&#25237;&#24433;&#21040;&#24178;&#20928;&#25968;&#25454;&#27969;&#24418;&#19978;&#33719;&#24471;&#30340;&#8220;&#21435;&#22122;&#8221;&#20998;&#37327;&#20197;&#21450;&#36890;&#36807;&#28155;&#21152;&#38543;&#26426;&#22122;&#22768;&#23558;&#20854;&#36716;&#25442;&#20026;&#19979;&#19968;&#20010;&#36739;&#20302;&#32423;&#21035;&#30340;&#22024;&#26434;&#27969;&#24418;&#30340;&#8220;&#22122;&#22768;&#8221;&#20998;&#37327;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#19968;&#20123;&#23545;&#20110;&#24178;&#20928;&#25968;&#25454;&#27969;&#24418;&#30340;&#26465;&#20214;&#19979;&#65292;&#36890;&#36807;&#20174;&#21435;&#22122;&#20449;&#21495;&#20013;&#26045;&#21152;&#26465;&#20214;&#30340;&#20849;&#36717;&#26799;&#24230;&#26356;&#26032;&#23646;&#20110;&#24178;&#20928;&#27969;&#24418;&#65292;&#20174;&#32780;&#23548;&#33268;&#19968;&#20010;mu
&lt;/p&gt;
&lt;p&gt;
Diffusion models have shown exceptional performance in solving inverse problems. However, one major limitation is the slow inference time. While faster diffusion samplers have been developed for unconditional sampling, there has been limited research on conditional sampling in the context of inverse problems. In this study, we propose a novel and efficient diffusion sampling strategy that employs the geometric decomposition of diffusion sampling. Specifically, we discover that the samples generated from diffusion models can be decomposed into two orthogonal components: a ``denoised" component obtained by projecting the sample onto the clean data manifold, and a ``noise" component that induces a transition to the next lower-level noisy manifold with the addition of stochastic noise. Furthermore, we prove that, under some conditions on the clean data manifold, the conjugate gradient update for imposing conditioning from the denoised signal belongs to the clean manifold, resulting in a mu
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37327;&#21270;&#35823;&#24046;&#24863;&#30693;&#30340;&#21464;&#36895;&#29575;&#26694;&#26550;&#65292;&#21033;&#29992;&#21333;&#21464;&#37327;&#37327;&#21270;&#35843;&#33410;&#22120;&#23454;&#29616;&#21333;&#27169;&#22411;&#20869;&#24191;&#27867;&#30340;&#21487;&#21464;&#27604;&#29305;&#29575;&#65292;&#21516;&#26102;&#23454;&#29616;&#20102;&#31163;&#25955;&#21487;&#21464;&#27604;&#29305;&#29575;&#12290;&#22312;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#23454;&#29616;&#24191;&#27867;&#30340;&#36830;&#32493;&#21487;&#21464;&#27604;&#29305;&#29575;&#32780;&#19981;&#20250;&#20986;&#29616;&#26174;&#33879;&#30340;&#24615;&#33021;&#38477;&#32423;&#12290;</title><link>http://arxiv.org/abs/2303.05744</link><description>&lt;p&gt;
QVRF&#65306;&#19968;&#31181;&#37327;&#21270;&#35823;&#24046;&#24863;&#30693;&#30340;&#21464;&#36895;&#29575;&#23398;&#20064;&#22270;&#20687;&#21387;&#32553;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
QVRF: A Quantization-error-aware Variable Rate Framework for Learned Image Compression. (arXiv:2303.05744v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
[http://arxiv.org/abs/2303.05744](http://arxiv.org/abs/2303.05744)
&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#22270;&#20687;&#21387;&#32553;&#34920;&#29616;&#20986;&#20102;&#24456;&#22909;&#30340;&#21387;&#32553;&#24615;&#33021;&#65292;&#20294;&#22312;&#24191;&#27867;&#30340;&#27604;&#29305;&#29575;&#33539;&#22260;&#20869;&#23454;&#29616;&#21487;&#21464;&#27604;&#29305;&#29575;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#26368;&#20808;&#36827;&#30340;&#21487;&#21464;&#36895;&#29575;&#26041;&#27861;&#22312;&#27169;&#22411;&#24615;&#33021;&#25439;&#22833;&#26041;&#38754;&#23384;&#22312;&#22949;&#21327;&#65292;&#24182;&#38656;&#35201;&#22823;&#37327;&#30340;&#39069;&#22806;&#21442;&#25968;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37327;&#21270;&#35823;&#24046;&#24863;&#30693;&#30340;&#21464;&#36895;&#29575;&#26694;&#26550;&#65288;QVRF&#65289;&#65292;&#35813;&#26694;&#26550;&#21033;&#29992;&#21333;&#21464;&#37327;&#37327;&#21270;&#35843;&#33410;&#22120;&#23454;&#29616;&#21333;&#27169;&#22411;&#20869;&#24191;&#27867;&#30340;&#21487;&#21464;&#27604;&#29305;&#29575;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;QVRF&#23450;&#20041;&#20102;&#19968;&#20010;&#37327;&#21270;&#35843;&#33410;&#22120;&#21521;&#37327;&#65292;&#32467;&#21512;&#39044;&#23450;&#20041;&#30340;Lagrange&#20056;&#25968;&#65292;&#25511;&#21046;&#20102;&#25152;&#26377;&#28508;&#22312;&#34920;&#31034;&#30340;&#37327;&#21270;&#35823;&#24046;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#31163;&#25955;&#21487;&#21464;&#27604;&#29305;&#29575;&#12290;&#27492;&#22806;&#65292;&#37325;&#26032;&#21442;&#25968;&#21270;&#26041;&#27861;&#20351;&#24471;QVRF&#19982;&#22278;&#25972;&#37327;&#21270;&#22120;&#20860;&#23481;&#12290;&#35814;&#23613;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#29616;&#26377;&#30340;&#37197;&#22791;QVRF&#30340;&#22266;&#23450;&#27604;&#29305;&#29575;&#22522;&#20110;VAE&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#21333;&#20010;&#27169;&#22411;&#20869;&#23454;&#29616;&#24191;&#27867;&#30340;&#36830;&#32493;&#21487;&#21464;&#27604;&#29305;&#29575;&#32780;&#19981;&#20250;&#20986;&#29616;&#26174;&#33879;&#30340;&#24615;&#33021;&#38477;&#32423;&#12290;&#27492;&#22806;&#65292;QVRF&#20248;&#20110;&#24403;&#20195;&#21487;&#21464;&#36895;&#29575;&#21387;&#32553;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learned image compression has exhibited promising compression performance, but variable bitrates over a wide range remain a challenge. State-of-the-art variable rate methods compromise the loss of model performance and require numerous additional parameters. In this paper, we present a Quantization-error-aware Variable Rate Framework (QVRF) that utilizes a univariate quantization regulator a to achieve wide-range variable rates within a single model. Specifically, QVRF defines a quantization regulator vector coupled with predefined Lagrange multipliers to control quantization error of all latent representation for discrete variable rates. Additionally, the reparameterization method makes QVRF compatible with a round quantizer. Exhaustive experiments demonstrate that existing fixed-rate VAE-based methods equipped with QVRF can achieve wide-range continuous variable rates within a single model without significant performance degradation. Furthermore, QVRF outperforms contemporary variabl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#25216;&#26415;3D Cinemagraphy&#65292;&#32473;&#23450;&#19968;&#24352;&#21333;&#29031;&#29255;&#20316;&#20026;&#36755;&#20837;&#65292;&#21487;&#20197;&#29983;&#25104;&#21253;&#21547;&#35270;&#35273;&#20869;&#23481;&#21160;&#30011;&#21644;&#30456;&#26426;&#21160;&#20316;&#30340;&#35270;&#39057;&#12290;&#36890;&#36807;&#23558;&#22330;&#26223;&#22312;3D&#31354;&#38388;&#20013;&#34920;&#31034;&#21644;&#21160;&#30011;&#21270;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;2D&#22270;&#20687;&#21160;&#30011;&#21644;3D&#25668;&#24433;&#26041;&#27861;&#32452;&#21512;&#24102;&#26469;&#30340;&#20266;&#24433;&#25110;&#19981;&#19968;&#33268;&#30340;&#21160;&#30011;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.05724</link><description>&lt;p&gt;
&#19968;&#24352;&#21333;&#29031;&#29255;&#29983;&#25104;3D Cinemagraphy
&lt;/p&gt;
&lt;p&gt;
3D Cinemagraphy from a Single Image. (arXiv:2303.05724v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
[http://arxiv.org/abs/2303.05724](http://arxiv.org/abs/2303.05724)
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#25216;&#26415;3D Cinemagraphy&#65292;&#23558;2D&#22270;&#20687;&#21160;&#30011;&#21644;3D&#25668;&#24433;&#34701;&#21512;&#36215;&#26469;&#12290;&#32473;&#23450;&#19968;&#24352;&#21333;&#29031;&#29255;&#20316;&#20026;&#36755;&#20837;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#29983;&#25104;&#21253;&#21547;&#35270;&#35273;&#20869;&#23481;&#21160;&#30011;&#21644;&#30456;&#26426;&#21160;&#20316;&#30340;&#35270;&#39057;&#12290;&#36890;&#36807;&#23454;&#39564;&#25105;&#20204;&#21457;&#29616;&#65292;&#31616;&#21333;&#30340;&#32452;&#21512;&#29616;&#26377;&#30340;2D&#22270;&#20687;&#21160;&#30011;&#21644;3D&#25668;&#24433;&#26041;&#27861;&#20250;&#23548;&#33268;&#26126;&#26174;&#30340;&#20266;&#24433;&#25110;&#19981;&#19968;&#33268;&#30340;&#21160;&#30011;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#27934;&#23519;&#21147;&#26159;&#22312;3D&#31354;&#38388;&#20013;&#34920;&#31034;&#21644;&#21160;&#30011;&#21270;&#22330;&#26223;&#20026;&#36825;&#20010;&#20219;&#21153;&#25552;&#20379;&#20102;&#19968;&#20010;&#33258;&#28982;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;&#39044;&#27979;&#30340;&#28145;&#24230;&#20540;&#23558;&#36755;&#20837;&#22270;&#20687;&#36716;&#25442;&#20026;&#22522;&#20110;&#29305;&#24449;&#30340;&#20998;&#23618;&#28145;&#24230;&#22270;&#20687;&#65292;&#28982;&#21518;&#23558;&#23427;&#20204;&#23637;&#24320;&#21040;&#19968;&#20010;&#29305;&#24449;&#28857;&#20113;&#20013;&#12290;&#20026;&#20102;&#21160;&#30011;&#21270;&#22330;&#26223;&#65292;&#25105;&#20204;&#25191;&#34892;&#36816;&#21160;&#20272;&#35745;&#24182;&#23558;2D&#21160;&#20316;&#25552;&#21319;&#21040;3D&#22330;&#26223;&#27969;&#20013;&#12290;&#26368;&#21518;&#65292;&#20026;&#20102;&#35299;&#20915;&#28857;&#21521;&#21069;&#31227;&#21160;&#26102;&#20986;&#29616;&#30340;&#27934;&#38544;&#38382;&#39064;&#65292;&#25105;&#20204;&#24314;&#35758;&#25353;&#22330;&#26223;&#27969;&#21452;&#21521;&#20301;&#31227;&#28857;&#20113;&#65292;&#24182;&#36890;&#36807;&#23558;&#20854;&#20998;&#21035;&#25237;&#24433;&#21040;&#30446;&#26631;&#22270;&#20687;&#24179;&#38754;&#20013;&#21512;&#25104;&#26032;&#35270;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present 3D Cinemagraphy, a new technique that marries 2D image animation with 3D photography. Given a single still image as input, our goal is to generate a video that contains both visual content animation and camera motion. We empirically find that naively combining existing 2D image animation and 3D photography methods leads to obvious artifacts or inconsistent animation. Our key insight is that representing and animating the scene in 3D space offers a natural solution to this task. To this end, we first convert the input image into feature-based layered depth images using predicted depth values, followed by unprojecting them to a feature point cloud. To animate the scene, we perform motion estimation and lift the 2D motion into the 3D scene flow. Finally, to resolve the problem of hole emergence as points move forward, we propose to bidirectionally displace the point cloud as per the scene flow and synthesize novel views by separately projecting them into target image planes and
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#21033;&#29992;&#31070;&#32463;&#22349;&#22604;&#29616;&#35937;&#26469;&#20943;&#23569;&#38169;&#35823;&#20005;&#37325;&#31243;&#24230;&#12290;&#19982;&#20256;&#32479;&#30340;ETF&#19981;&#21516;&#65292;&#26412;&#25991;&#37319;&#29992;&#23618;&#27425;&#24863;&#30693;&#26694;&#26550;(HAFrame)&#26469;&#22266;&#23450;&#32447;&#24615;&#20998;&#31867;&#22120;&#65292;&#24182;&#20351;&#29992;&#20313;&#24358;&#30456;&#20284;&#24230;&#36741;&#21161;&#25439;&#22833;&#26469;&#35757;&#32451;&#23618;&#27425;&#24863;&#30693;&#20498;&#25968;&#31532;&#20108;&#29305;&#24449;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#26377;&#25928;&#38477;&#20302;&#20102;&#27169;&#22411;&#39044;&#27979;&#30340;&#38169;&#35823;&#20005;&#37325;&#31243;&#24230;&#12290;</title><link>http://arxiv.org/abs/2303.05689</link><description>&lt;p&gt;
&#23558;&#31070;&#32463;&#22349;&#22604;&#35825;&#23548;&#21040;&#22266;&#23450;&#30340;&#23618;&#27425;&#24863;&#30693;&#26694;&#26550;&#65292;&#20197;&#20943;&#23569;&#38169;&#35823;&#20005;&#37325;&#31243;&#24230;
&lt;/p&gt;
&lt;p&gt;
Inducing Neural Collapse to a Fixed Hierarchy-Aware Frame for Reducing Mistake Severity. (arXiv:2303.05689v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
[http://arxiv.org/abs/2303.05689](http://arxiv.org/abs/2303.05689)
&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#21457;&#29616;&#20102;&#19968;&#20010;&#26377;&#36259;&#30340;&#29616;&#35937;&#31216;&#20026;&#31070;&#32463;&#22349;&#22604;&#65306;&#22312;&#35757;&#32451;&#29992;&#20110;&#20998;&#31867;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#26411;&#26399;&#38454;&#27573;&#65292;&#25152;&#26377;&#24179;&#22374;&#31867;&#21035;&#30340;&#20498;&#25968;&#31532;&#20108;&#29305;&#24449;&#24179;&#22343;&#20540;&#21644;&#30456;&#20851;&#30340;&#20998;&#31867;&#22120;&#21521;&#37327;&#22349;&#22604;&#21040;&#19968;&#20010;&#31616;&#21333;&#30340;&#31561;&#35282;&#32039;&#26694;&#26550;(ETF)&#30340;&#39030;&#28857;&#12290;&#26368;&#36817;&#30340;&#24037;&#20316;&#23581;&#35797;&#21033;&#29992;&#36825;&#31181;&#29616;&#35937;&#65292;&#36890;&#36807;&#23558;&#30456;&#20851;&#30340;&#20998;&#31867;&#22120;&#26435;&#37325;&#22266;&#23450;&#20026;&#39044;&#35745;&#31639;&#30340;ETF&#26469;&#35825;&#23548;&#31070;&#32463;&#22349;&#22604;&#65292;&#24182;&#22312;&#19981;&#24179;&#34913;&#25968;&#25454;&#35757;&#32451;&#26102;&#26368;&#22823;&#21270;&#23398;&#20064;&#29305;&#24449;&#20043;&#38388;&#30340;&#20998;&#31163;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#32447;&#24615;&#20998;&#31867;&#22120;&#22266;&#23450;&#21040;&#19968;&#20010;&#23618;&#27425;&#24863;&#30693;&#26694;&#26550;(HAFrame)&#19978;&#65292;&#32780;&#19981;&#26159;ETF&#65292;&#24182;&#20351;&#29992;&#22522;&#20110;&#20313;&#24358;&#30456;&#20284;&#24230;&#30340;&#36741;&#21161;&#25439;&#22833;&#26469;&#23398;&#20064;&#23849;&#28291;&#21040;HAFrame&#30340;&#23618;&#27425;&#24863;&#30693;&#20498;&#25968;&#31532;&#20108;&#29305;&#24449;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20445;&#25345;&#22312;&#20855;&#26377;&#23618;&#27425;&#32467;&#26500;&#30340;&#19981;&#21516;&#35268;&#27169;&#30340;&#25968;&#25454;&#38598;&#19978;&#30340;top-1&#20934;&#30830;&#29575;&#30340;&#21516;&#26102;&#20943;&#23569;&#20102;&#27169;&#22411;&#39044;&#27979;&#30340;&#38169;&#35823;&#20005;&#37325;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
There is a recently discovered and intriguing phenomenon called Neural Collapse: at the terminal phase of training a deep neural network for classification, the within-class penultimate feature means and the associated classifier vectors of all flat classes collapse to the vertices of a simplex Equiangular Tight Frame (ETF). Recent work has tried to exploit this phenomenon by fixing the related classifier weights to a pre-computed ETF to induce neural collapse and maximize the separation of the learned features when training with imbalanced data. In this work, we propose to fix the linear classifier of a deep neural network to a Hierarchy-Aware Frame (HAFrame), instead of an ETF, and use a cosine similarity-based auxiliary loss to learn hierarchy-aware penultimate features that collapse to the HAFrame. We demonstrate that our approach reduces the mistake severity of the model's predictions while maintaining its top-1 accuracy on several datasets of varying scales with hierarchies of he
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23460;&#20869;&#23478;&#20855;&#30340;&#37325;&#32452;&#26694;&#26550;&#65292;&#26088;&#22312;&#20026;&#26426;&#22120;&#20154;&#27963;&#21160;&#25552;&#20379;&#36275;&#22815;&#30340;&#21487;&#35775;&#38382;&#31354;&#38388;&#65292;&#21516;&#26102;&#19981;&#24433;&#21709;&#26085;&#24120;&#20154;&#31867;&#27963;&#21160;&#12290;&#23454;&#39564;&#26174;&#31034;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#22686;&#21152;14%&#30340;&#21487;&#35775;&#38382;&#31354;&#38388;&#21644;30%&#30340;&#21487;&#20132;&#20114;&#23545;&#35937;&#12290;</title><link>http://arxiv.org/abs/2303.05676</link><description>&lt;p&gt;
&#23460;&#20869;&#22330;&#26223;&#30340;&#20154;&#26426;&#21512;&#20316;&#20248;&#21270;&#19982;&#37325;&#32452;
&lt;/p&gt;
&lt;p&gt;
Rearrange Indoor Scenes for Human-Robot Co-Activity. (arXiv:2303.05676v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
[http://arxiv.org/abs/2303.05676](http://arxiv.org/abs/2303.05676)
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20248;&#21270;&#30340;&#26694;&#26550;&#65292;&#20197;&#26356;&#22909;&#22320;&#37197;&#21512;&#20154;&#26426;&#21327;&#20316;&#26469;&#37325;&#26032;&#24067;&#32622;&#23460;&#20869;&#23478;&#20855;&#12290;&#37325;&#32452;&#26088;&#22312;&#20026;&#26426;&#22120;&#20154;&#27963;&#21160;&#25552;&#20379;&#36275;&#22815;&#30340;&#21487;&#35775;&#38382;&#31354;&#38388;&#65292;&#21516;&#26102;&#19981;&#24433;&#21709;&#26085;&#24120;&#20154;&#31867;&#27963;&#21160;&#12290;&#20026;&#20445;&#30041;&#20154;&#31867;&#27963;&#21160;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#36890;&#36807;&#25972;&#21512;&#20174;SUNCG&#21644;ConceptNet&#20013;&#25552;&#21462;&#30340;&#31354;&#38388;&#21644;&#35821;&#20041;&#20849;&#29616;&#26469;&#20445;&#30041;&#23478;&#20855;&#20043;&#38388;&#30340;&#21151;&#33021;&#20851;&#31995;&#12290;&#36890;&#36807;&#23450;&#20041;&#26426;&#22120;&#20154;&#21487;&#35775;&#38382;&#31354;&#38388;&#30340;&#24320;&#25918;&#31354;&#38388;&#37327;&#21644;&#33021;&#22815;&#21040;&#36798;&#30340;&#29289;&#20307;&#25968;&#37327;&#65292;&#25105;&#20204;&#23558;&#20154;&#26426;&#21327;&#20316;&#30340;&#37325;&#32452;&#24418;&#24335;&#21270;&#20026;&#19968;&#20010;&#20248;&#21270;&#38382;&#39064;&#65292;&#30001;&#33258;&#36866;&#24212;&#27169;&#25311;&#36864;&#28779;(ASA)&#21644;&#21327;&#26041;&#24046;&#30697;&#38453;&#36866;&#24212;&#36827;&#21270;&#31574;&#30053;(CMA-ES)&#27714;&#35299;&#12290;&#25105;&#20204;&#22312;SUNCG&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#23450;&#37327;&#26174;&#31034;&#65292;&#37325;&#26032;&#25490;&#21015;&#30340;&#22330;&#26223;&#25552;&#20379;&#24179;&#22343;14%&#30340;&#21487;&#35775;&#38382;&#31354;&#38388;&#21644;30%&#30340;&#21487;&#20132;&#20114;&#23545;&#35937;&#12290;&#37325;&#32452;&#22330;&#26223;&#30340;&#36136;&#37327;&#22312;&#23450;&#24615;&#19978;&#24471;&#21040;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present an optimization-based framework for rearranging indoor furniture to accommodate human-robot co-activities better. The rearrangement aims to afford sufficient accessible space for robot activities without compromising everyday human activities. To retain human activities, our algorithm preserves the functional relations among furniture by integrating spatial and semantic co-occurrence extracted from SUNCG and ConceptNet, respectively. By defining the robot's accessible space by the amount of open space it can traverse and the number of objects it can reach, we formulate the rearrangement for human-robot co-activity as an optimization problem, solved by adaptive simulated annealing (ASA) and covariance matrix adaptation evolution strategy (CMA-ES). Our experiments on the SUNCG dataset quantitatively show that rearranged scenes provide an average of 14% more accessible space and 30% more objects to interact with. The quality of the rearranged scenes is qualitatively validated b
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#39044;&#35757;&#32451;&#30340;&#21477;&#23376;&#32534;&#30721;&#22120;&#20013;&#23384;&#22312;&#30340;&#24102;&#26377;&#38472;&#35268;&#38475;&#20064;&#30340;&#21051;&#26495;&#21360;&#35937;&#65292;&#24182;&#27604;&#36739;&#20102;&#19982;&#20043;&#23545;&#24212;&#30340;&#25991;&#26412;&#34164;&#28085;&#27169;&#22411;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#37319;&#29992;&#25991;&#26412;&#34164;&#28085;&#30340;&#26126;&#30830;&#36923;&#36753;&#23398;&#20064;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#20559;&#24046;&#65292;&#24182;&#25552;&#39640;&#31038;&#20132;&#31038;&#21306;&#30340;&#35782;&#21035;&#33021;&#21147;&#65292;&#32780;&#26080;&#38656;&#26126;&#30830;&#21435;&#20559;&#35265;&#30340;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2303.05670</link><description>&lt;p&gt;
&#36923;&#36753;&#25171;&#30772;&#20559;&#35265;&#65306;&#25991;&#26412;&#34164;&#28085;&#32531;&#35299;&#20102;&#24102;&#26377;&#38472;&#35268;&#38475;&#20064;&#30340;&#21477;&#23376;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Logic Against Bias: Textual Entailment Mitigates Stereotypical Sentence Reasoning. (arXiv:2303.05670v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
[http://arxiv.org/abs/2303.05670](http://arxiv.org/abs/2303.05670)
&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#39044;&#35757;&#32451;&#21477;&#23376;&#32534;&#30721;&#22120;&#30340;&#30456;&#20284;&#24230;&#23398;&#20064;&#30446;&#26631;&#65292;&#23427;&#20204;&#32463;&#24120;&#20869;&#21270;&#21453;&#26144;&#20854;&#35757;&#32451;&#35821;&#26009;&#24211;&#20013;&#23384;&#22312;&#30340;&#31038;&#20250;&#20559;&#35265;&#30340;&#38472;&#35268;&#38475;&#20064;&#12290;&#26412;&#25991;&#25551;&#36848;&#20102;&#23384;&#22312;&#20110;&#27969;&#34892;&#21477;&#23376;&#34920;&#31034;&#27169;&#22411;&#20013;&#30340;&#20960;&#31181;&#20851;&#20110;&#19981;&#21516;&#31038;&#21306;&#30340;&#21051;&#26495;&#21360;&#35937;&#12290;&#25105;&#20204;&#23558;&#36825;&#26679;&#30340;&#27169;&#22411;&#19982;&#23398;&#20064;&#21508;&#31181;&#19979;&#28216;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#30340;&#25991;&#26412;&#34164;&#28085;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#12290;&#36890;&#36807;&#27604;&#36739;&#22522;&#20110;&#25991;&#26412;&#30456;&#20284;&#24230;&#30340;&#24378;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#25991;&#26412;&#34164;&#28085;&#23398;&#20064;&#65292;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65306;&#25991;&#26412;&#34164;&#28085;&#30340;&#26126;&#30830;&#36923;&#36753;&#23398;&#20064;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#20559;&#24046;&#65292;&#24182;&#25552;&#39640;&#31038;&#20132;&#31038;&#21306;&#30340;&#35782;&#21035;&#33021;&#21147;&#65292;&#32780;&#26080;&#38656;&#26126;&#30830;&#21435;&#20559;&#35265;&#30340;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to their similarity-based learning objectives, pretrained sentence encoders often internalize stereotypical assumptions that reflect the social biases that exist within their training corpora. In this paper, we describe several kinds of stereotypes concerning different communities that are present in popular sentence representation models, including pretrained next sentence prediction and contrastive sentence representation models. We compare such models to textual entailment models that learn language logic for a variety of downstream language understanding tasks. By comparing strong pretrained models based on text similarity with textual entailment learning, we conclude that the explicit logic learning with textual entailment can significantly reduce bias and improve the recognition of social communities, without an explicit de-biasing process
&lt;/p&gt;</description></item><item><title>UnFuSeD&#26159;&#19968;&#31181;&#20351;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#26080;&#30417;&#30563;&#24494;&#35843;&#30340;&#26041;&#27861;&#65292;&#23558;&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#26080;&#30417;&#30563;&#24494;&#35843;&#30456;&#32467;&#21512;&#65292;&#20351;&#29992;&#20266;&#26631;&#31614;&#36827;&#34892;&#24494;&#35843;&#65292;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#33073;&#31163;&#25991;&#29486;&#20013;&#36890;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#33539;&#20363;&#30340;&#31995;&#32479;</title><link>http://arxiv.org/abs/2303.05668</link><description>&lt;p&gt;
UNFUSED: &#20351;&#29992;&#33258;&#30417;&#30563;&#33976;&#39311;&#30340;&#26080;&#30417;&#30563;&#24494;&#35843;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
UNFUSED: UNsupervised Finetuning Using SElf supervised Distillation. (arXiv:2303.05668v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
[http://arxiv.org/abs/2303.05668](http://arxiv.org/abs/2303.05668)
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;UnFuSeD&#65292;&#21033;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#24182;&#20943;&#23569;&#38899;&#39057;&#20998;&#31867;&#25152;&#38656;&#30340;&#26631;&#35760;&#25968;&#25454;&#37327;&#12290;&#30456;&#27604;&#20043;&#21069;&#30340;&#24037;&#20316;&#65292;&#30452;&#25509;&#22312;&#30446;&#26631;&#25968;&#25454;&#38598;&#19978;&#24494;&#35843;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#30340;&#32534;&#30721;&#22120;&#65292;&#25105;&#20204;&#20351;&#29992;&#32534;&#30721;&#22120;&#36890;&#36807;&#32858;&#31867;&#25552;&#21462;&#30340;&#34920;&#24449;&#29983;&#25104;&#20266;&#26631;&#31614;&#36827;&#34892;&#26080;&#30417;&#30563;&#24494;&#35843;&#12290;&#36825;&#20123;&#20266;&#26631;&#31614;&#28982;&#21518;&#29992;&#20110;&#25351;&#23548;&#38543;&#26426;&#21021;&#22987;&#21270;&#27169;&#22411;&#19978;&#30340;&#33258;&#33976;&#39311;&#12290;&#26368;&#21518;&#65292;&#23545;&#24471;&#21040;&#30340;&#32534;&#30721;&#22120;&#22312;&#30446;&#26631;&#20219;&#21153;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#12290;&#36890;&#36807;UnFuSeD&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#33073;&#31163;&#25991;&#29486;&#20013;&#36890;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#33539;&#20363;&#30340;&#31995;&#32479;&#65292;&#35813;&#33539;&#20363;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#30456;&#21516;&#30340;&#32534;&#30721;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce UnFuSeD, a novel approach to leverage self-supervised learning and reduce the need for large amounts of labeled data for audio classification. Unlike prior works, which directly fine-tune a self-supervised pre-trained encoder on a target dataset, we use the encoder to generate pseudo-labels for unsupervised fine-tuning before the actual fine-tuning step. We first train an encoder using a novel self-supervised learning algorithm (SSL) on an unlabeled audio dataset. Then, we use that encoder to generate pseudo-labels on our target task dataset via clustering the extracted representations. These pseudo-labels are then used to guide self-distillation on a randomly initialized model, which we call unsupervised fine-tuning. Finally, the resultant encoder is then fine-tuned on our target task dataset. Through UnFuSeD, we propose the first system that moves away from generic SSL paradigms in literature, which pre-train and fine-tune the same encoder, and present a n
&lt;/p&gt;</description></item><item><title>GATOR&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#25429;&#25417;&#20174;&#39592;&#26550;&#21040;&#32593;&#26684;&#30340;&#22810;&#20010;&#20851;&#31995;&#65292;&#21253;&#25324;&#20851;&#33410;-&#20851;&#33410;&#65292;&#20851;&#33410;-&#39030;&#28857;&#21644;&#39030;&#28857;-&#39030;&#28857;&#20851;&#31995;&#65292;&#24182;&#21033;&#29992;&#36816;&#21160;&#35299;&#32806;&#22238;&#24402;&#26469;&#25506;&#32034;&#20851;&#33410;&#21644;&#39030;&#28857;&#20851;&#31995;&#12290;GATOR&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.05652</link><description>&lt;p&gt;
&#20351;&#29992;&#22270;&#24418;&#24863;&#30693;&#21464;&#21387;&#22120;&#21644;&#36816;&#21160;&#35299;&#32806;&#22238;&#24402;&#36827;&#34892;&#20154;&#20307;&#32593;&#26684;&#20174;2D&#23039;&#21183;&#20013;&#24674;&#22797;
&lt;/p&gt;
&lt;p&gt;
GATOR: Graph-Aware Transformer with Motion-Disentangled Regression for Human Mesh Recovery from a 2D Pose. (arXiv:2303.05652v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
[http://arxiv.org/abs/2303.05652](http://arxiv.org/abs/2303.05652)
&lt;/p&gt;
&lt;p&gt;
&#20174;2D&#23039;&#21183;&#20013;&#24674;&#22797;3D&#20154;&#20307;&#32593;&#26684;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290; &#20294;&#26159;&#65292;&#29616;&#26377;&#26041;&#27861;&#24456;&#38590;&#21516;&#26102;&#25429;&#33719;&#20174;&#39592;&#26550;&#21040;&#32593;&#26684;&#30340;&#22810;&#20010;&#20851;&#31995;&#65292;&#21253;&#25324;&#20851;&#33410;-&#20851;&#33410;&#65292;&#20851;&#33410;-&#39030;&#28857;&#21644;&#39030;&#28857;-&#39030;&#28857;&#20851;&#31995;&#65292;&#36825;&#32463;&#24120;&#23548;&#33268;&#19981;&#20999;&#23454;&#38469;&#30340;&#32467;&#26524;&#12290; &#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#31216;&#20026;GATOR&#65292;&#20854;&#20013;&#21253;&#21547;&#19968;&#20010;&#22270;&#24418;&#24863;&#30693;&#21464;&#21387;&#22120;&#65288;GAT&#65289;&#30340;&#32534;&#30721;&#22120;&#21644;&#20855;&#26377;&#36816;&#21160;&#35299;&#32806;&#22238;&#24402;&#65288;MDR&#65289;&#30340;&#35299;&#30721;&#22120;&#65292;&#20197;&#25506;&#32034;&#36825;&#20123;&#22810;&#20010;&#20851;&#31995;&#12290; &#20855;&#20307;&#32780;&#35328;&#65292;GAT&#24182;&#34892;&#32452;&#21512;&#20102;GCN&#21644;&#22270;&#24418;&#24863;&#30693;&#30340;&#33258;&#25105;&#27880;&#24847;&#21147;&#65292;&#20197;&#25429;&#33719;&#29289;&#29702;&#21644;&#38544;&#34255;&#30340;&#20851;&#33410;-&#20851;&#33410;&#20851;&#31995;&#12290; &#27492;&#22806;&#65292;MDR&#36890;&#36807;&#27169;&#25311;&#20851;&#33410;&#21644;&#39030;&#28857;&#20851;&#31995;&#26469;&#25506;&#32034;&#20851;&#33410;-&#39030;&#28857;&#21644;&#39030;&#28857;-&#39030;&#28857;&#20132;&#20114;&#20316;&#29992;&#12290; &#22522;&#20110;&#39030;&#28857;&#20559;&#31227;&#22330;&#30340;&#32858;&#31867;&#29305;&#24449;&#65292;MDR&#36890;&#36807;&#32452;&#21512;&#39044;&#27979;&#30340;&#22522;&#26412;&#36816;&#21160;&#26469;&#22238;&#24402;&#39030;&#28857;&#12290; &#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;GATOR&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
3D human mesh recovery from a 2D pose plays an important role in various applications. However, it is hard for existing methods to simultaneously capture the multiple relations during the evolution from skeleton to mesh, including joint-joint, joint-vertex and vertex-vertex relations, which often leads to implausible results. To address this issue, we propose a novel solution, called GATOR, that contains an encoder of Graph-Aware Transformer (GAT) and a decoder with Motion-Disentangled Regression (MDR) to explore these multiple relations. Specifically, GAT combines a GCN and a graph-aware self-attention in parallel to capture physical and hidden joint-joint relations. Furthermore, MDR models joint-vertex and vertex-vertex interactions to explore joint and vertex relations. Based on the clustering characteristics of vertex offset fields, MDR regresses the vertices by composing the predicted base motions. Extensive experiments show that GATOR achieves state-of-the-art performance on two 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#19978;&#19979;&#25991;&#20381;&#36182;&#20559;&#22909;&#27169;&#22411;Pacos&#65292;&#38024;&#23545;&#20559;&#22909;&#21453;&#36716;&#38382;&#39064;&#21516;&#26102;&#35299;&#20915;&#29992;&#25143;&#33258;&#36866;&#24212;&#26435;&#37325;&#12289;&#39033;&#30446;&#38388;&#27604;&#36739;&#21644;&#26174;&#31034;&#20301;&#32622;&#19977;&#20010;&#22240;&#32032;&#65292;&#35813;&#27169;&#22411;&#21253;&#25324;&#20855;&#26377;&#39640;&#35299;&#37322;&#24615;&#30340;&#21152;&#27861;&#26041;&#27861;&#21644;&#20855;&#26377;&#39640;&#20934;&#30830;&#24615;&#30340;&#22522;&#20110;ANN&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.05648</link><description>&lt;p&gt;
Pacos&#65306;&#22312;&#20559;&#22909;&#21453;&#36716;&#20013;&#24314;&#27169;&#29992;&#25143;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#19978;&#19979;&#25991;&#20381;&#36182;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Pacos: Modeling Users' Interpretable and Context-Dependent Choices in Preference Reversals. (arXiv:2303.05648v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
[http://arxiv.org/abs/2303.05648](http://arxiv.org/abs/2303.05648)
&lt;/p&gt;
&lt;p&gt;
&#36873;&#25321;&#38382;&#39064;&#26159;&#20174;&#22810;&#20010;&#39033;&#30446;&#20013;&#36873;&#25321;&#26368;&#20339;&#36873;&#25321;&#65292;&#23398;&#20064;&#29992;&#25143;&#22312;&#36873;&#25321;&#38382;&#39064;&#20013;&#30340;&#20559;&#22909;&#23545;&#20110;&#20102;&#35299;&#20915;&#31574;&#21046;&#23450;&#26426;&#21046;&#21644;&#25552;&#20379;&#20010;&#24615;&#21270;&#26381;&#21153;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#29616;&#26377;&#30340;&#24037;&#20316;&#36890;&#24120;&#20551;&#23450;&#20154;&#20204;&#29420;&#31435;&#35780;&#20272;&#39033;&#30446;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#29992;&#25143;&#30340;&#20559;&#22909;&#21462;&#20915;&#20110;&#29289;&#21697;&#25152;&#22312;&#30340;&#24066;&#22330;&#65292;&#36825;&#34987;&#31216;&#20026;&#19978;&#19979;&#25991;&#25928;&#24212;&#65307;&#29992;&#25143;&#23545;&#20004;&#20010;&#39033;&#30446;&#30340;&#20559;&#22909;&#39034;&#24207;&#29978;&#33267;&#21487;&#33021;&#34987;&#39072;&#20498;&#65292;&#36825;&#34987;&#31216;&#20026;&#20559;&#22909;&#21453;&#36716;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#19977;&#20010;&#23548;&#33268;&#19978;&#19979;&#25991;&#25928;&#24212;&#30340;&#22240;&#32032;&#65306;&#29992;&#25143;&#30340;&#33258;&#36866;&#24212;&#26435;&#37325;&#65292;&#39033;&#30446;&#38388;&#27604;&#36739;&#21644;&#26174;&#31034;&#20301;&#32622;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Pacos&#30340;&#19978;&#19979;&#25991;&#20381;&#36182;&#20559;&#22909;&#27169;&#22411;&#65292;&#20316;&#20026;&#21516;&#26102;&#35299;&#20915;&#19977;&#20010;&#22240;&#32032;&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#24182;&#32771;&#34385;&#20102;&#20004;&#31181;&#35774;&#35745;&#26041;&#27861;&#65292;&#21253;&#25324;&#20855;&#26377;&#39640;&#35299;&#37322;&#24615;&#30340;&#21152;&#27861;&#26041;&#27861;&#21644;&#20855;&#26377;&#39640;&#20934;&#30830;&#24615;&#30340;&#22522;&#20110;ANN&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#20559;&#22909;&#21453;&#36716;&#30340;&#26465;&#20214;
&lt;/p&gt;
&lt;p&gt;
Choice problems refer to selecting the best choices from several items, and learning users' preferences in choice problems is of great significance in understanding the decision making mechanisms and providing personalized services. Existing works typically assume that people evaluate items independently. In practice, however, users' preferences depend on the market in which items are placed, which is known as context effects; and the order of users' preferences for two items may even be reversed, which is referred to preference reversals. In this work, we identify three factors contributing to context effects: users' adaptive weights, the inter-item comparison, and display positions. We propose a context-dependent preference model named Pacos as a unified framework for addressing three factors simultaneously, and consider two design methods including an additive method with high interpretability and an ANN-based method with high accuracy. We study the conditions for preference reversa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36882;&#24402;&#32593;&#32476;&#30340;&#35757;&#32451;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#21487;&#29992;&#20110;&#23454;&#29616;&#39640;&#25928;&#23454;&#26102;&#36882;&#24402;&#23398;&#20064;&#12290;&#35813;&#31639;&#27861;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#35745;&#31639;&#25104;&#26412;&#65292;&#24182;&#22312;&#20302;&#36164;&#28304;&#23454;&#26102;&#31995;&#32479;&#19978;&#23454;&#29616;&#12290;</title><link>http://arxiv.org/abs/2303.05641</link><description>&lt;p&gt;
&#36890;&#36807;&#27963;&#21160;&#21644;&#21442;&#25968;&#31232;&#30095;&#30456;&#32467;&#21512;&#23454;&#29616;&#39640;&#25928;&#30340;&#23454;&#26102;&#36882;&#24402;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Efficient Real Time Recurrent Learning through combined activity and parameter sparsity. (arXiv:2303.05641v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
[http://arxiv.org/abs/2303.05641](http://arxiv.org/abs/2303.05641)
&lt;/p&gt;
&lt;p&gt;
BPTT&#26159;&#35757;&#32451;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;(RNNs)&#30340;&#26631;&#20934;&#31639;&#27861;&#65292;&#38656;&#35201;&#21069;&#21521;&#21644;&#21518;&#21521;&#27169;&#25311;&#38454;&#27573;&#20998;&#21035;&#36827;&#34892;&#25512;&#29702;&#21644;&#23398;&#20064;&#65292;BPTT&#38656;&#35201;&#23384;&#20648;&#32593;&#32476;&#29366;&#24577;&#30340;&#23436;&#25972;&#21382;&#21490;&#35760;&#24405;&#65292;&#23384;&#20648;&#37327;&#19982;&#36755;&#20837;&#24207;&#21015;&#38271;&#24230;&#25104;&#27604;&#20363;&#12290;&#36825;&#20351;&#24471;BPTT&#19981;&#36866;&#21512;&#22312;&#32447;&#23398;&#20064;&#65292;&#24182;&#22312;&#20302;&#36164;&#28304;&#23454;&#26102;&#31995;&#32479;&#19978;&#23454;&#29616;&#26159;&#19968;&#39033;&#25361;&#25112;&#12290;&#23454;&#26102;&#36882;&#24402;&#23398;&#20064;(RTRL)&#20801;&#35768;&#22312;&#32447;&#23398;&#20064;&#65292;&#25152;&#38656;&#20869;&#23384;&#30340;&#22686;&#38271;&#19982;&#24207;&#21015;&#38271;&#24230;&#26080;&#20851;&#12290;&#28982;&#32780;&#65292;RTRL&#30340;&#35745;&#31639;&#25104;&#26412;&#38750;&#24120;&#39640;&#65292;&#19982;&#29366;&#24577;&#22823;&#23567;&#30340;&#22235;&#27425;&#26041;&#25104;&#27604;&#20363;&#65292;&#20351;RTRL&#22312;&#25152;&#26377;&#32593;&#32476;&#35268;&#27169;&#19978;&#30340;&#35745;&#31639;&#37117;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#34920;&#29616;&#20986;&#39640;&#27963;&#21160;&#31232;&#30095;&#24615;&#30340;&#36882;&#24402;&#32593;&#32476;&#21487;&#20197;&#38477;&#20302;RTRL&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#27492;&#22806;&#65292;&#32452;&#21512;&#27963;&#21160;&#21644;&#21442;&#25968;&#31232;&#30095;&#24230;&#30340;&#26041;&#27861;&#21487;&#36827;&#19968;&#27493;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#65292;&#23454;&#29616;&#39640;&#25928;&#23454;&#26102;&#36882;&#24402;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Backpropagation through time (BPTT) is the standard algorithm for training recurrent neural networks (RNNs), which requires separate simulation phases for the forward and backward passes for inference and learning, respectively. Moreover, BPTT requires storing the complete history of network states between phases, with memory consumption growing proportional to the input sequence length. This makes BPTT unsuited for online learning and presents a challenge for implementation on low-resource real-time systems. Real-Time Recurrent Learning (RTRL) allows online learning, and the growth of required memory is independent of sequence length. However, RTRL suffers from exceptionally high computational costs that grow proportional to the fourth power of the state size, making RTRL computationally intractable for all but the smallest of networks. In this work, we show that recurrent networks exhibiting high activity sparsity can reduce the computational cost of RTRL. Moreover, combining activit
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#21644;&#35780;&#20272;&#20102;&#19968;&#31181;&#20351;&#29992;&#35777;&#25454;&#26435;&#37325;&#26694;&#26550;&#30340;&#21487;&#35299;&#37322;&#30446;&#26631;&#35782;&#21035;&#27169;&#22411;&#65292;&#21487;&#20197;&#25552;&#20379;&#20154;&#31867;&#20013;&#24515;&#30340;&#35299;&#37322;&#24182;&#19988;&#22312;&#20843;&#20010;&#19981;&#21516;&#30340;&#39046;&#22495;&#34920;&#29616;&#33391;&#22909;&#12290;&#32463;&#36807;&#20154;&#31867;&#34892;&#20026;&#30740;&#31350;&#39564;&#35777;&#35813;&#27169;&#22411;&#21487;&#20197;&#25104;&#21151;&#29983;&#25104;&#31867;&#20284;&#20154;&#31867;&#30340;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2303.05622</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#30446;&#26631;&#35782;&#21035;: &#22522;&#20110;&#35777;&#25454;&#26435;&#37325;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Explainable Goal Recognition: A Framework Based on Weight of Evidence. (arXiv:2303.05622v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
[http://arxiv.org/abs/2303.05622](http://arxiv.org/abs/2303.05622)
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#24182;&#35780;&#20272;&#20102;&#19968;&#31181;&#20351;&#29992;&#35777;&#25454;&#26435;&#37325;(WoE)&#26694;&#26550;&#26469;&#35299;&#37322;&#30446;&#26631;&#35782;&#21035;&#38382;&#39064;&#30340;&#21487;&#35299;&#37322;&#30340;&#30446;&#26631;&#35782;&#21035;(XGR)&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#25552;&#20379;&#20102;&#20154;&#31867;&#20013;&#24515;&#30340;&#35299;&#37322;&#65292;&#22238;&#31572;&#20102;&#20026;&#20160;&#20040;&#65311;&#21644;&#20026;&#20160;&#20040;&#19981;&#65311;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#22312;&#20843;&#20010;&#19981;&#21516;&#30340;&#39046;&#22495;&#35745;&#31639;&#22320;&#35780;&#20272;&#20102;&#25105;&#20204;&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;&#20351;&#29992;&#20154;&#31867;&#34892;&#20026;&#30740;&#31350;&#26469;&#20174;&#20154;&#31867;&#27880;&#37322;&#20013;&#33719;&#24471;&#22522;&#30784;&#20107;&#23454;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;XGR&#27169;&#22411;&#21487;&#20197;&#25104;&#21151;&#29983;&#25104;&#31867;&#20284;&#20154;&#31867;&#30340;&#35299;&#37322;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25253;&#21578;&#20102;&#19968;&#39033;&#30740;&#31350;&#65292;&#20854;&#20013;60&#21517;&#21442;&#19982;&#32773;&#35266;&#23519;&#20195;&#29702;&#31243;&#24207;&#29609;Sokoban&#28216;&#25103;&#65292;&#28982;&#21518;&#25509;&#25910;&#30446;&#26631;&#35782;&#21035;&#36755;&#20986;&#30340;&#35299;&#37322;&#12290;&#25105;&#20204;&#36890;&#36807;&#20219;&#21153;&#39044;&#27979;&#12289;&#35299;&#37322;&#28385;&#24847;&#24230;&#21644;&#20449;&#20219;&#35843;&#26597;&#21442;&#19982;&#32773;&#23545;&#35299;&#37322;&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce and evaluate an eXplainable Goal Recognition (XGR) model that uses the Weight of Evidence (WoE) framework to explain goal recognition problems. Our model provides human-centered explanations that answer why? and why not? questions. We computationally evaluate the performance of our system over eight different domains. Using a human behavioral study to obtain the ground truth from human annotators, we further show that the XGR model can successfully generate human-like explanations. We then report on a study with 60 participants who observe agents playing Sokoban game and then receive explanations of the goal recognition output. We investigate participants' understanding obtained by explanations through task prediction, explanation satisfaction, and trust.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20851;&#38190;&#28857;&#30340;RGB-D&#36755;&#20837;&#19979;&#30340;&#26032;&#22411;&#20845;&#33258;&#30001;&#24230;&#25235;&#21462;&#23039;&#24577;&#21512;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#35774;&#35745;&#26032;&#30340;&#25235;&#21462;&#29983;&#25104;&#32593;&#32476;&#65292;&#20943;&#23569;&#20102;&#23545;&#31934;&#30830;&#20851;&#38190;&#28857;&#20272;&#35745;&#30340;&#20381;&#36182;&#24615;&#65292;&#35813;&#26041;&#27861;&#34920;&#29616;&#26356;&#22909;&#65292;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.05617</link><description>&lt;p&gt;
KGNv2:&#22522;&#20110;&#20851;&#38190;&#28857;&#30340;RGB-D&#36755;&#20837;&#19979;&#30340;&#22522;&#20110;&#20845;&#33258;&#30001;&#24230;&#30340;&#25235;&#21462;&#23039;&#24577;&#21512;&#25104;&#20013;&#30340;&#23610;&#24230;&#21644;&#23039;&#24577;&#39044;&#27979;&#30340;&#20998;&#31163;
&lt;/p&gt;
&lt;p&gt;
KGNv2: Separating Scale and Pose Prediction for Keypoint-based 6-DoF Grasp Pose Synthesis on RGB-D input. (arXiv:2303.05617v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
[http://arxiv.org/abs/2303.05617](http://arxiv.org/abs/2303.05617)
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20851;&#38190;&#28857;&#30340;&#20108;&#32500;/&#20108;&#28857;&#20116;&#32500;&#36755;&#20837;&#30340;&#26032;&#22411;&#20845;&#33258;&#30001;&#24230;&#25235;&#21462;&#23039;&#24577;&#21512;&#25104;&#26041;&#27861;&#12290;&#20174;&#22270;&#20687;&#36755;&#20837;&#30340;&#22522;&#20110;&#20851;&#38190;&#28857;&#30340;&#25235;&#21462;&#25506;&#27979;&#22120;&#22312;&#20197;&#21069;&#30340;&#30740;&#31350;&#20013;&#34920;&#29616;&#20986;&#26377;&#21069;&#36884;&#30340;&#32467;&#26524;&#65292;&#20854;&#20013;&#39068;&#33394;&#22270;&#20687;&#25552;&#20379;&#30340;&#38468;&#21152;&#35270;&#35273;&#20449;&#24687;&#34917;&#20607;&#20102;&#22122;&#22768;&#28145;&#24230;&#24863;&#30693;&#12290;&#28982;&#32780;&#65292;&#23427;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#20934;&#30830;&#22320;&#39044;&#27979;&#22270;&#20687;&#31354;&#38388;&#20013;&#30340;&#20851;&#38190;&#28857;&#20301;&#32622;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#26032;&#30340;&#25235;&#21462;&#29983;&#25104;&#32593;&#32476;&#65292;&#23427;&#20943;&#23569;&#20102;&#23545;&#31934;&#30830;&#20851;&#38190;&#28857;&#20272;&#35745;&#30340;&#20381;&#36182;&#24615;&#12290;&#32473;&#23450;RGB-D&#36755;&#20837;&#65292;&#25105;&#20204;&#30340;&#32593;&#32476;&#26082;&#21487;&#20197;&#20174;&#20851;&#38190;&#28857;&#26816;&#27979;&#20013;&#20272;&#35745;&#25235;&#21462;&#23039;&#24577;&#65292;&#20063;&#21487;&#20197;&#20272;&#35745;&#25509;&#36817;&#30456;&#26426;&#30340;&#23610;&#24230;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#37325;&#26032;&#35774;&#35745;&#20851;&#38190;&#28857;&#36755;&#20986;&#31354;&#38388;&#65292;&#20197;&#20943;&#36731;&#20851;&#38190;&#28857;&#39044;&#27979;&#22122;&#22768;&#23545;&#36879;&#35270;N&#28857;&#65288;PnP&#65289;&#31639;&#27861;&#30340;&#36127;&#38754;&#24433;&#21709;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#27604;&#22522;&#32447;&#26041;&#27861;&#34920;&#29616;&#26356;&#22909;&#65292;&#39564;&#35777;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#26368;&#21518;&#65292;&#23613;&#31649;&#26159;&#22312;&#31616;&#21333;&#30340;&#21512;&#25104;&#29289;&#20307;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21364;&#34920;&#29616;&#20986;&#20102;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a new 6-DoF grasp pose synthesis approach from 2D/2.5D input based on keypoints. Keypoint-based grasp detector from image input has demonstrated promising results in the previous study, where the additional visual information provided by color images compensates for the noisy depth perception. However, it relies heavily on accurately predicting the location of keypoints in the image space. In this paper, we devise a new grasp generation network that reduces the dependency on precise keypoint estimation. Given an RGB-D input, our network estimates both the grasp pose from keypoint detection as well as scale towards the camera. We further re-design the keypoint output space in order to mitigate the negative impact of keypoint prediction noise to Perspective-n-Point (PnP) algorithm. Experiments show that the proposed method outperforms the baseline by a large margin, validating the efficacy of our approach. Finally, despite trained on simple synthetic objects, our method demons
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#30693;&#35782;&#33976;&#39311;&#30340;&#26410;&#26631;&#35760;&#25968;&#25454;&#23884;&#20837;&#26408;&#39532;&#65292;&#26368;&#32456;&#25104;&#21151;&#22320;&#35774;&#35745;&#20986;&#21487;&#20197;&#38477;&#20302;&#23398;&#29983;&#27169;&#22411;&#20934;&#30830;&#24230;&#32780;&#19981;&#24433;&#21709;&#25945;&#24072;&#27169;&#22411;&#30340;Trojan&#25915;&#20987;</title><link>http://arxiv.org/abs/2303.05593</link><description>&lt;p&gt;
&#23398;&#20064;&#38169;&#35823;&#30340;&#25945;&#35757;&#65306;&#22312;&#30693;&#35782;&#33976;&#39311;&#36807;&#31243;&#20013;&#25554;&#20837;&#26408;&#39532;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning the Wrong Lessons: Inserting Trojans During Knowledge Distillation. (arXiv:2303.05593v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
[http://arxiv.org/abs/2303.05593](http://arxiv.org/abs/2303.05593)
&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#30693;&#35782;&#33976;&#39311;&#24050;&#25104;&#20026;&#26377;&#25928;&#37096;&#32626;&#26426;&#22120;&#23398;&#20064;&#30340;&#22522;&#30707;&#65292;&#35768;&#22810;&#23454;&#39564;&#23460;&#21644;&#20135;&#19994;&#37117;&#20351;&#29992;&#30693;&#35782;&#33976;&#39311;&#26469;&#35757;&#32451;&#32463;&#27982;&#12289;&#36164;&#28304;&#20248;&#21270;&#30340;&#27169;&#22411;&#12290;&#21516;&#26102;&#65292;Trojan&#25915;&#20987;&#20063;&#24341;&#36215;&#20102;&#37325;&#35201;&#30340;&#20851;&#27880;&#65292;&#25581;&#31034;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#22522;&#26412;&#28431;&#27934;&#12290;&#37492;&#20110;&#30693;&#35782;&#33976;&#39311;&#30340;&#24191;&#27867;&#24212;&#29992;&#65292;&#25105;&#20204;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#35797;&#22270;&#21033;&#29992;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#30693;&#35782;&#33976;&#39311;&#36807;&#31243;&#22312;&#23398;&#29983;&#27169;&#22411;&#20013;&#23884;&#20837;&#26408;&#39532;&#32780;&#19981;&#24341;&#20837;&#25945;&#24072;&#20013;&#26174;&#30524;&#30340;&#34892;&#20026;&#12290;&#25105;&#20204;&#26368;&#32456;&#35774;&#35745;&#20102;&#19968;&#31181;Trojan&#25915;&#20987;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#38477;&#20302;&#23398;&#29983;&#30340;&#20934;&#30830;&#24615;&#65292;&#19981;&#20250;&#25913;&#21464;&#25945;&#24072;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#22312;&#23454;&#36341;&#20013;&#26131;&#20110;&#26500;&#24314;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, knowledge distillation has become a cornerstone of efficiently deployed machine learning, with labs and industries using knowledge distillation to train models that are inexpensive and resource-optimized. Trojan attacks have contemporaneously gained significant prominence, revealing fundamental vulnerabilities in deep learning models. Given the widespread use of knowledge distillation, in this work we seek to exploit the unlabelled data knowledge distillation process to embed Trojans in a student model without introducing conspicuous behavior in the teacher. We ultimately devise a Trojan attack that effectively reduces student accuracy, does not alter teacher performance, and is efficiently constructible in practice.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;SocialGym 2&#30340;&#22810;&#26234;&#33021;&#20307;&#23548;&#33322;&#27169;&#25311;&#22120;&#65292;&#29992;&#20110;&#31038;&#20132;&#26426;&#22120;&#20154;&#30740;&#31350;&#12290;&#23427;&#37319;&#29992;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#26469;&#24320;&#21457;&#22810;&#20010;&#20855;&#26377;&#19981;&#21516;&#12289;&#21160;&#24577;&#32422;&#26463;&#30340;&#26426;&#22120;&#20154;&#30340;&#26368;&#20339;&#23548;&#33322;&#31574;&#30053;&#65292;&#25552;&#20379;&#26131;&#20110;&#35775;&#38382;&#30340;python&#25509;&#21475;&#65292;&#24182;&#25552;&#20379;&#20102;&#20132;&#25442;&#21644;&#35780;&#20272;&#19981;&#21516;MARL&#31639;&#27861;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.05584</link><description>&lt;p&gt;
SOCIALGYM 2.0&#65306;&#20849;&#20139;&#20154;&#31867;&#31354;&#38388;&#20013;&#22810;&#26234;&#33021;&#20307;&#31038;&#20132;&#26426;&#22120;&#20154;&#23548;&#33322;&#27169;&#25311;&#22120;
&lt;/p&gt;
&lt;p&gt;
SOCIALGYM 2.0: Simulator for Multi-Agent Social Robot Navigation in Shared Human Spaces. (arXiv:2303.05584v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
[http://arxiv.org/abs/2303.05584](http://arxiv.org/abs/2303.05584)
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;SocialGym 2&#65292;&#19968;&#20010;&#29992;&#20110;&#31038;&#20132;&#26426;&#22120;&#20154;&#30740;&#31350;&#30340;&#22810;&#26234;&#33021;&#20307;&#23548;&#33322;&#27169;&#25311;&#22120;&#12290;&#25105;&#20204;&#30340;&#27169;&#25311;&#22120;&#27169;&#25311;&#20102;&#22810;&#20010;&#33258;&#20027;&#20195;&#29702;&#65292;&#22797;&#21046;&#20102;&#22797;&#26434;&#29615;&#22659;&#20013;&#30340;&#23454;&#38469;&#21160;&#24577;&#65292;&#21253;&#25324;&#38376;&#24266;&#12289;&#36208;&#24266;&#12289;&#21313;&#23383;&#36335;&#21475;&#21644;&#29615;&#24418;&#20132;&#21449;&#21475;&#12290;&#19982;&#20256;&#32479;&#30340;&#20197;&#24320;&#25918;&#31354;&#38388;&#20013;&#20855;&#26377;&#22522;&#26412;&#36816;&#21160;&#32422;&#26463;&#30340;&#21333;&#20010;&#26426;&#22120;&#20154;&#20026;&#37325;&#28857;&#30340;&#27169;&#25311;&#22120;&#19981;&#21516;&#65292;SocialGym 2&#37319;&#29992;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#26469;&#24320;&#21457;&#22810;&#20010;&#20855;&#26377;&#19981;&#21516;&#12289;&#21160;&#24577;&#32422;&#26463;&#30340;&#26426;&#22120;&#20154;&#30340;&#26368;&#20339;&#23548;&#33322;&#31574;&#30053;&#12290;&#22522;&#20110;PettingZoo MARL&#24211;&#21644;Stable Baselines3 API&#65292;SocialGym 2&#25552;&#20379;&#20102;&#19968;&#20010;&#26131;&#20110;&#35775;&#38382;&#30340;python&#25509;&#21475;&#65292;&#36890;&#36807;ROS&#28040;&#24687;&#19982;&#23548;&#33322;&#22534;&#26632;&#38598;&#25104;&#12290;SocialGym 2&#21487;&#20197;&#36731;&#26494;&#23433;&#35013;&#65292;&#24182;&#25171;&#21253;&#22312;&#19968;&#20010;docker&#23481;&#22120;&#20013;&#65292;&#23427;&#25552;&#20379;&#20102;&#20132;&#25442;&#21644;&#35780;&#20272;&#19981;&#21516;MARL&#31639;&#27861;&#30340;&#33021;&#21147;&#65292;&#20197;&#21450;&#33258;&#23450;&#20041;&#35266;&#23519;&#21644;&#22870;&#21169;&#20989;&#25968;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#33050;&#26412;&#65292;&#20351;&#29992;&#25143;&#21487;&#20197;&#21019;&#24314;&#33258;&#24049;&#30340;&#29615;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present SocialGym 2, a multi-agent navigation simulator for social robot research. Our simulator models multiple autonomous agents, replicating real-world dynamics in complex environments, including doorways, hallways, intersections, and roundabouts. Unlike traditional simulators that concentrate on single robots with basic kinematic constraints in open spaces, SocialGym 2 employs multi-agent reinforcement learning (MARL) to develop optimal navigation policies for multiple robots with diverse, dynamic constraints in complex environments. Built on the PettingZoo MARL library and Stable Baselines3 API, SocialGym 2 offers an accessible python interface that integrates with a navigation stack through ROS messaging. SocialGym 2 can be easily installed and is packaged in a docker container, and it provides the capability to swap and evaluate different MARL algorithms, as well as customize observation and reward functions. We also provide scripts to allow users to create their own environm
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#36866;&#24212;&#36127;&#26679;&#26412;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#22312;&#22521;&#35757;&#38454;&#27573;&#29983;&#25104;&#26377;&#25928;&#30340;&#21512;&#25104;&#24320;&#25918;&#31867;&#21035;&#26679;&#26412;&#65292;&#29992;&#20110;&#35299;&#20915;&#24320;&#25918;&#19990;&#30028;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#25361;&#25112;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#36741;&#21161;&#30340;&#19968;&#23545;&#22810;&#20108;&#36827;&#21046;&#20998;&#31867;&#22120;&#20855;&#26377;&#26174;&#30528;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2303.05581</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#36127;&#26679;&#26412;&#30340;&#24320;&#25918;&#19990;&#30028;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Open World Classification with Adaptive Negative Samples. (arXiv:2303.05581v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
[http://arxiv.org/abs/2303.05581](http://arxiv.org/abs/2303.05581)
&lt;/p&gt;
&lt;p&gt;
&#24320;&#25918;&#19990;&#30028;&#20998;&#31867;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#20855;&#26377;&#20851;&#38190;&#23454;&#38469;&#24847;&#20041;&#21644;&#24433;&#21709;&#30340;&#20219;&#21153;&#12290;&#30001;&#20110;&#24320;&#25918;&#25110;&#26410;&#30693;&#31867;&#21035;&#25968;&#25454;&#20165;&#22312;&#25512;&#26029;&#38454;&#27573;&#26174;&#31034;&#65292;&#22240;&#27492;&#23547;&#25214;&#20855;&#26377;&#36866;&#24403;&#20915;&#31574;&#36793;&#30028;&#20197;&#23481;&#32435;&#24050;&#30693;&#31867;&#21035;&#30340;&#35782;&#21035;&#21644;&#24320;&#25918;&#31867;&#21035;&#30340;&#21306;&#20998;&#30340;&#27169;&#22411;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#29616;&#26377;&#27169;&#22411;&#30340;&#24615;&#33021;&#21463;&#21040;&#35757;&#32451;&#38454;&#27573;&#32570;&#20047;&#26377;&#25928;&#30340;&#24320;&#25918;&#31867;&#21035;&#25968;&#25454;&#25110;&#32570;&#20047;&#23398;&#20064;&#36866;&#24403;&#20915;&#31574;&#36793;&#30028;&#30340;&#33391;&#22909;&#26426;&#21046;&#30340;&#38480;&#21046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#36866;&#24212;&#36127;&#26679;&#26412;&#65288;ANS&#65289;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#22312;&#22521;&#35757;&#38454;&#27573;&#29983;&#25104;&#26377;&#25928;&#30340;&#21512;&#25104;&#24320;&#25918;&#31867;&#21035;&#26679;&#26412;&#65292;&#32780;&#19981;&#38656;&#35201;&#20219;&#20309;&#20808;&#21069;&#30340;&#30693;&#35782;&#25110;&#22806;&#37096;&#25968;&#25454;&#38598;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#36741;&#21161;&#30340;&#19968;&#23545;&#22810;&#20108;&#36827;&#21046;&#20998;&#31867;&#22120;&#20855;&#26377;&#26174;&#30528;&#20248;&#21183;&#65292;&#36825;&#20123;&#20998;&#31867;&#22120;&#26377;&#25928;&#22320;&#21033;&#29992;&#20102;&#29983;&#25104;&#30340;&#36127;&#26679;&#26412;&#65292;&#24182;&#36991;&#20813;&#20102;&#22797;&#26434;&#30340;&#38408;&#20540;&#25628;&#32034;&#38454;&#27573;&#12290;
&lt;/p&gt;
&lt;p&gt;
Open world classification is a task in natural language processing with key practical relevance and impact. Since the open or {\em unknown} category data only manifests in the inference phase, finding a model with a suitable decision boundary accommodating for the identification of known classes and discrimination of the open category is challenging. The performance of existing models is limited by the lack of effective open category data during the training stage or the lack of a good mechanism to learn appropriate decision boundaries. We propose an approach based on \underline{a}daptive \underline{n}egative \underline{s}amples (ANS) designed to generate effective synthetic open category samples in the training stage and without requiring any prior knowledge or external datasets. Empirically, we find a significant advantage in using auxiliary one-versus-rest binary classifiers, which effectively utilize the generated negative samples and avoid the complex threshold-seeking stage in pr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23545;&#25239;&#24615;&#35780;&#20272;&#26041;&#26696;&#65292;&#21253;&#25324;&#20004;&#20010;&#31867;&#21035;&#30340;&#22235;&#20010;&#26041;&#26696;&#65292;&#24182;&#33258;&#21160;&#29983;&#25104;&#23545;&#25239;&#24615;&#31034;&#20363;&#26469;&#35780;&#20272;&#20250;&#35805;&#24335;&#25512;&#33616;&#31995;&#32479;&#22312;&#38754;&#23545;&#19981;&#21516;&#36755;&#20837;&#25968;&#25454;&#26102;&#30340;&#40065;&#26834;&#24615;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#29616;&#26377;&#30340;&#31995;&#32479;&#37117;&#19981;&#22815;&#40065;&#26834;&#21644;&#21487;&#38752;&#20197;&#25269;&#24481;&#23545;&#25239;&#24615;&#31034;&#20363;&#12290;</title><link>http://arxiv.org/abs/2303.05575</link><description>&lt;p&gt;
&#36890;&#36807;&#23545;&#25239;&#26679;&#26412;&#35780;&#20272;&#20250;&#35805;&#24335;&#25512;&#33616;&#31995;&#32479;&#30340;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Evaluating the Robustness of Conversational Recommender Systems by Adversarial Examples. (arXiv:2303.05575v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
[http://arxiv.org/abs/2303.05575](http://arxiv.org/abs/2303.05575)
&lt;/p&gt;
&lt;p&gt;
&#20250;&#35805;&#24335;&#25512;&#33616;&#31995;&#32479;&#65288;CRS&#65289;&#26681;&#25454;&#26631;&#20934;&#30340;&#25512;&#33616;&#20934;&#30830;&#24615;&#25351;&#26631;&#36805;&#36895;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#30830;&#20445;&#36825;&#20123;&#31995;&#32479;&#19982;&#29992;&#25143;&#20132;&#20114;&#30340;&#40065;&#26834;&#24615;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#21253;&#25324;&#27491;&#24120;&#29992;&#25143;&#21644;&#24694;&#24847;&#29992;&#25143;&#12290;&#20182;&#20204;&#24076;&#26395;&#36890;&#36807;&#25552;&#20379;&#20462;&#25913;&#36807;&#30340;&#36755;&#20837;&#25968;&#25454;&#26469;&#25915;&#20987;&#31995;&#32479;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23545;&#25239;&#24615;&#35780;&#20272;&#26041;&#26696;&#65292;&#21253;&#25324;&#20004;&#20010;&#31867;&#21035;&#30340;&#22235;&#20010;&#26041;&#26696;&#65292;&#24182;&#33258;&#21160;&#29983;&#25104;&#23545;&#25239;&#24615;&#31034;&#20363;&#26469;&#35780;&#20272;&#31995;&#32479;&#22312;&#38754;&#23545;&#19981;&#21516;&#36755;&#20837;&#25968;&#25454;&#26102;&#30340;&#40065;&#26834;&#24615;&#12290;&#36890;&#36807;&#25191;&#34892;&#36825;&#20123;&#23545;&#25239;&#24615;&#31034;&#20363;&#65292;&#25105;&#20204;&#21487;&#20197;&#27604;&#36739;&#19981;&#21516;&#23545;&#35805;&#24335;&#25512;&#33616;&#31995;&#32479;&#28385;&#36275;&#29992;&#25143;&#20559;&#22909;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#30340;&#23545;&#25239;&#24615;&#31034;&#20363;&#23545;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#19977;&#20010;CRS&#36827;&#34892;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20123;&#31995;&#32479;&#37117;&#19981;&#22815;&#40065;&#26834;&#21644;&#21487;&#38752;&#20197;&#25269;&#24481;&#23545;&#25239;&#24615;&#31034;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conversational recommender systems (CRSs) are improving rapidly, according to the standard recommendation accuracy metrics. However, it is essential to make sure that these systems are robust in interacting with users including regular and malicious users who want to attack the system by feeding the system modified input data. In this paper, we propose an adversarial evaluation scheme including four scenarios in two categories and automatically generate adversarial examples to evaluate the robustness of these systems in the face of different input data. By executing these adversarial examples we can compare the ability of different conversational recommender systems to satisfy the user's preferences. We evaluate three CRSs by the proposed adversarial examples on two datasets. Our results show that none of these systems are robust and reliable to the adversarial examples.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20165;&#20351;&#29992;&#22270;&#20687;&#32423;&#20132;&#20114;&#26631;&#31614;&#12289;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;VLM&#65289;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26469;&#22788;&#29702;HOI&#26816;&#27979;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#21033;&#29992;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#23450;&#20301;&#33021;&#21147;&#26469;&#20462;&#21098;&#38750;&#20132;&#20114;&#20154;&#21644;&#29289;&#20307;&#30340;&#24314;&#35758;&#65292;&#20174;&#32780;&#25552;&#39640;&#34955;&#20869;&#27491;&#26679;&#26412;&#23545;&#30340;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2303.05546</link><description>&lt;p&gt;
&#20174;&#20132;&#20114;&#26631;&#31614;&#21644;&#35821;&#35328;/&#35270;&#35273;-&#35821;&#35328;&#20808;&#39564;&#20013;&#20165;&#20351;&#29992;&#24369;&#30417;&#30563;&#36827;&#34892;HOI&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Weakly-Supervised HOI Detection from Interaction Labels Only and Language/Vision-Language Priors. (arXiv:2303.05546v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
[http://arxiv.org/abs/2303.05546](http://arxiv.org/abs/2303.05546)
&lt;/p&gt;
&lt;p&gt;
&#20154;-&#29289;&#20132;&#20114;&#65288;HOI&#65289;&#26816;&#27979;&#26088;&#22312;&#20174;&#32473;&#23450;&#30340;&#33258;&#28982;&#22270;&#20687;&#20013;&#25552;&#21462;&#30456;&#20114;&#20316;&#29992;&#30340;&#20154;-&#29289;&#23545;&#21450;&#20854;&#20132;&#20114;&#31867;&#21035;&#12290;&#23613;&#31649;&#26500;&#24314;HOI&#26816;&#27979;&#25968;&#25454;&#38598;&#25152;&#38656;&#30340;&#26631;&#35760;&#24037;&#20316;&#26412;&#36136;&#19978;&#27604;&#35768;&#22810;&#20854;&#20182;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#26356;&#21152;&#24191;&#27867;&#65292;&#20294;&#30001;&#20110;&#30456;&#20114;&#20316;&#29992;&#22312;&#23545;&#35937;&#21644;&#35859;&#35789;&#31354;&#38388;&#19978;&#30340;&#32452;&#21512;&#24615;&#36136;&#65292;&#23398;&#20064;&#24369;&#30417;&#30563;&#19979;&#30340;&#20154;-&#29289;&#20132;&#20114;&#20173;&#28982;&#24456;&#22256;&#38590;&#12290;&#26412;&#25991;&#20351;&#29992;&#20165;&#26377;&#30340;&#22270;&#20687;&#32423;&#20132;&#20114;&#26631;&#31614;&#21644;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;VLM&#65289;&#20197;&#21450;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26469;&#22788;&#29702;HOI&#26816;&#27979;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21033;&#29992;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#23450;&#20301;&#33021;&#21147;&#26469;&#20462;&#21098;&#38750;&#20132;&#20114;&#20154;&#21644;&#29289;&#20307;&#30340;&#24314;&#35758;&#65292;&#20174;&#32780;&#25552;&#39640;&#34955;&#20869;&#27491;&#26679;&#26412;&#23545;&#30340;&#36136;&#37327;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;L...
&lt;/p&gt;
&lt;p&gt;
Human-object interaction (HOI) detection aims to extract interacting human-object pairs and their interaction categories from a given natural image. Even though the labeling effort required for building HOI detection datasets is inherently more extensive than for many other computer vision tasks, weakly-supervised directions in this area have not been sufficiently explored due to the difficulty of learning human-object interactions with weak supervision, rooted in the combinatorial nature of interactions over the object and predicate space. In this paper, we tackle HOI detection with the weakest supervision setting in the literature, using only image-level interaction labels, with the help of a pretrained vision-language model (VLM) and a large language model (LLM). We first propose an approach to prune non-interacting human and object proposals to increase the quality of positive pairs within the bag, exploiting the grounding capability of the vision-language model. Second, we use a l
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#24378;&#21270;&#23398;&#20064;&#20013;&#36890;&#29992;&#30446;&#26631;&#30340;PAC&#21487;&#23398;&#20064;&#24615;&#65292;&#24182;&#35777;&#26126;&#20102;&#21482;&#35201;&#35813;&#30446;&#26631;&#26159;&#19968;&#33268;&#36830;&#32493;&#30340;&#25110;&#21487;&#35745;&#31639;&#36830;&#32493;&#30340;&#65292;&#21017;&#23427;&#23601;&#26159;PAC&#21487;&#23398;&#20064;&#30340;</title><link>http://arxiv.org/abs/2303.05518</link><description>&lt;p&gt;
&#21487;&#35745;&#31639;&#36830;&#32493;&#24378;&#21270;&#23398;&#20064;&#30446;&#26631;&#26159;PAC&#21487;&#23398;&#20064;&#30340;
&lt;/p&gt;
&lt;p&gt;
Computably Continuous Reinforcement-Learning Objectives are PAC-learnable. (arXiv:2303.05518v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
[http://arxiv.org/abs/2303.05518](http://arxiv.org/abs/2303.05518)
&lt;/p&gt;
&lt;p&gt;
&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#26368;&#22823;&#21270;&#25240;&#25187;&#21644;&#26377;&#38480;&#26102;&#38388;&#32047;&#31215;&#22238;&#25253;&#30340;&#32463;&#20856;&#30446;&#26631;&#26159;&#21487;&#20197;&#34987;PAC&#23398;&#20064;&#30340;&#65306;&#26377;&#31639;&#27861;&#21487;&#20197;&#20351;&#29992;&#26377;&#38480;&#30340;&#26679;&#26412;&#21644;&#35745;&#31639;&#39640;&#27010;&#29575;&#22320;&#23398;&#20064;&#25509;&#36817;&#26368;&#20248;&#31574;&#30053;&#12290;&#36817;&#24180;&#26469;&#65292;&#30740;&#31350;&#20154;&#21592;&#20171;&#32461;&#20102;&#36229;&#36234;&#32463;&#20856;&#32047;&#31215;&#22238;&#25253;&#30340;&#30446;&#26631;&#21644;&#30456;&#24212;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#22914;&#25351;&#23450;&#20026;&#32447;&#24615;&#26102;&#24577;&#36923;&#36753;&#20844;&#24335;&#30340;&#30446;&#26631;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26032;&#30446;&#26631;&#30340;PAC&#21487;&#23398;&#20064;&#24615;&#38382;&#39064;&#20173;&#28982;&#27809;&#26377;&#24471;&#21040;&#35299;&#20915;&#12290;&#36825;&#39033;&#24037;&#20316;&#36890;&#36807;&#20004;&#31181;&#20998;&#26512;&#35774;&#32622;&#30340;PAC&#21487;&#23398;&#20064;&#24615;&#30340;&#20805;&#20998;&#26465;&#20214;&#65292;&#35777;&#26126;&#20102;&#36890;&#29992;&#24378;&#21270;&#23398;&#20064;&#30446;&#26631;&#30340;PAC&#21487;&#23398;&#20064;&#24615;&#12290;&#29305;&#21035;&#26159;&#23545;&#20110;&#21482;&#32771;&#34385;&#26679;&#26412;&#22797;&#26434;&#24230;&#30340;&#20998;&#26512;&#65292;&#25105;&#20204;&#35777;&#26126;&#65292;&#22914;&#26524;&#20316;&#20026;&#19968;&#20010;oracle&#32473;&#20986;&#30340;&#30446;&#26631;&#26159;&#19968;&#33268;&#36830;&#32493;&#30340;&#65292;&#21017;&#23427;&#26159;PAC&#21487;&#23398;&#20064;&#30340;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;&#32771;&#34385;&#35745;&#31639;&#22797;&#26434;&#24230;&#30340;&#20998;&#26512;&#65292;&#25105;&#20204;&#35777;&#26126;&#22914;&#26524;&#19968;&#20010;&#30446;&#26631;&#26159;&#21487;&#35745;&#31639;&#36830;&#32493;&#30340;&#21017;&#23427;&#26159;PAC&#21487;&#23398;&#20064;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
In reinforcement learning, the classic objectives of maximizing discounted and finite-horizon cumulative rewards are PAC-learnable: There are algorithms that learn a near-optimal policy with high probability using a finite amount of samples and computation. In recent years, researchers have introduced objectives and corresponding reinforcement-learning algorithms beyond the classic cumulative rewards, such as objectives specified as linear temporal logic formulas. However, questions about the PAC-learnability of these new objectives have remained open.  This work demonstrates the PAC-learnability of general reinforcement-learning objectives through sufficient conditions for PAC-learnability in two analysis settings. In particular, for the analysis that considers only sample complexity, we prove that if an objective given as an oracle is uniformly continuous, then it is PAC-learnable. Further, for the analysis that considers computational complexity, we prove that if an objective is com
&lt;/p&gt;</description></item></channel></rss>