<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>FRIGATE&#26159;&#19968;&#31181;&#20351;&#29992;&#26102;&#31354;GNN&#36827;&#34892;&#20844;&#36335;&#32593;&#32476;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#20381;&#36182;&#27599;&#20010;&#33410;&#28857;&#30340;&#24863;&#30693;&#12289;&#23436;&#25972;&#30340;&#24863;&#30693;&#21382;&#21490;&#35760;&#24405;&#25110;&#38745;&#24577;&#32593;&#32476;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#24037;&#20316;&#65292;&#22312;&#30495;&#23454;&#20132;&#36890;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#26356;&#39640;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.08277</link><description>&lt;p&gt;
FRIGATE:&#20844;&#36335;&#32593;&#32476;&#30340;&#33410;&#32422;&#26102;&#31354;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
FRIGATE: Frugal Spatio-temporal Forecasting on Road Networks. (arXiv:2306.08277v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08277
&lt;/p&gt;
&lt;p&gt;
FRIGATE&#26159;&#19968;&#31181;&#20351;&#29992;&#26102;&#31354;GNN&#36827;&#34892;&#20844;&#36335;&#32593;&#32476;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#20381;&#36182;&#27599;&#20010;&#33410;&#28857;&#30340;&#24863;&#30693;&#12289;&#23436;&#25972;&#30340;&#24863;&#30693;&#21382;&#21490;&#35760;&#24405;&#25110;&#38745;&#24577;&#32593;&#32476;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#24037;&#20316;&#65292;&#22312;&#30495;&#23454;&#20132;&#36890;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#26356;&#39640;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24314;&#27169;&#20844;&#36335;&#32593;&#32476;&#19978;&#30340;&#26102;&#31354;&#36807;&#31243;&#26159;&#19968;&#20010;&#36234;&#26469;&#36234;&#37325;&#35201;&#30340;&#20219;&#21153;&#12290;&#34429;&#28982;&#22312;&#21457;&#23637;&#26102;&#31354;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#65292;&#20294;&#29616;&#26377;&#30340;&#24037;&#20316;&#24314;&#31435;&#22312;&#19977;&#20010;&#19981;&#36866;&#29992;&#20110;&#23454;&#38469;&#20844;&#36335;&#32593;&#32476;&#30340;&#20551;&#35774;&#22522;&#30784;&#19978;&#12290;&#39318;&#20808;&#65292;&#23427;&#20204;&#20551;&#23450;&#27599;&#20010;&#33410;&#28857;&#37117;&#37197;&#22791;&#20256;&#24863;&#22120;&#36827;&#34892;&#24863;&#30693;&#12290;&#28982;&#32780;&#22312;&#29616;&#23454;&#20013;&#65292;&#30001;&#20110;&#39044;&#31639;&#38480;&#21046;&#25110;&#20256;&#24863;&#22120;&#25925;&#38556;&#65292;&#25152;&#26377;&#20301;&#32622;&#65288;&#33410;&#28857;&#65289;&#21487;&#33021;&#27809;&#26377;&#37197;&#22791;&#20256;&#24863;&#22120;&#12290;&#20854;&#27425;&#65292;&#23427;&#20204;&#20551;&#23450;&#25152;&#26377;&#23433;&#35013;&#30340;&#20256;&#24863;&#22120;&#37117;&#20855;&#26377;&#24863;&#30693;&#21382;&#21490;&#35760;&#24405;&#12290;&#28982;&#32780;&#30001;&#20110;&#20256;&#24863;&#22120;&#25925;&#38556;&#12289;&#36890;&#20449;&#20013;&#30340;&#25968;&#25454;&#20002;&#22833;&#31561;&#21407;&#22240;&#65292;&#36825;&#20063;&#26159;&#19981;&#29616;&#23454;&#30340;&#12290;&#26368;&#21518;&#65292;&#20551;&#35774;&#38745;&#24577;&#30340;&#36947;&#36335;&#32593;&#32476;&#12290;&#32593;&#32476;&#20869;&#36830;&#36890;&#24615;&#20250;&#30001;&#20110;&#36947;&#36335;&#23553;&#38381;&#12289;&#26032;&#36335;&#20462;&#24314;&#31561;&#21407;&#22240;&#32780;&#21457;&#29983;&#21464;&#21270;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;FRIGATE&#26469;&#35299;&#20915;&#25152;&#26377;&#36825;&#20123;&#32570;&#28857;&#12290;FRIGATE&#30001;&#26102;&#31354;GNN&#39537;&#21160;&#65292;&#23558;&#20301;&#32622;&#12289;&#25299;&#25169;&#21644;&#26102;&#38388;&#20449;&#24687;&#38598;&#25104;&#21040;&#33410;&#28857;&#34920;&#31034;&#20013;&#65292;&#29992;&#20110;&#39044;&#27979;&#12290;&#36890;&#36807;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#29616;&#21147;&#65292;FRIGATE&#21487;&#20197;&#23398;&#20064;&#39044;&#27979;&#20844;&#36335;&#32593;&#32476;&#19978;&#26102;&#31354;&#36807;&#31243;&#30340;&#28436;&#21464;&#65292;&#32780;&#26080;&#38656;&#27599;&#20010;&#33410;&#28857;&#30340;&#24863;&#30693;&#12289;&#23436;&#25972;&#30340;&#24863;&#30693;&#21382;&#21490;&#35760;&#24405;&#25110;&#38745;&#24577;&#32593;&#32476;&#30340;&#20551;&#35774;&#12290;&#25105;&#20204;&#22312;&#30495;&#23454;&#20132;&#36890;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;FRIGATE&#65292;&#24182;&#35777;&#26126;&#20854;&#30456;&#23545;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#20855;&#26377;&#26356;&#20248;&#24322;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modelling spatio-temporal processes on road networks is a task of growing importance. While significant progress has been made on developing spatio-temporal graph neural networks (Gnns), existing works are built upon three assumptions that are not practical on real-world road networks. First, they assume sensing on every node of a road network. In reality, due to budget-constraints or sensor failures, all locations (nodes) may not be equipped with sensors. Second, they assume that sensing history is available at all installed sensors. This is unrealistic as well due to sensor failures, loss of packets during communication, etc. Finally, there is an assumption of static road networks. Connectivity within networks change due to road closures, constructions of new roads, etc. In this work, we develop FRIGATE to address all these shortcomings. FRIGATE is powered by a spatio-temporal Gnn that integrates positional, topological, and temporal information into rich inductive node representatio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#25216;&#26415;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;GBSD&#65292;&#24182;&#20351;&#29992;&#38454;&#27573;&#25193;&#25955;&#25216;&#26415;&#21512;&#25104;&#20855;&#26377;&#34394;&#21270;&#39118;&#26684;&#30340;&#29031;&#29255;&#12290;</title><link>http://arxiv.org/abs/2306.08251</link><description>&lt;p&gt;
GBSD: &#24102;&#26377;&#38454;&#27573;&#25193;&#25955;&#30340;&#29983;&#25104;&#34394;&#21270;&#25928;&#26524;
&lt;/p&gt;
&lt;p&gt;
GBSD: Generative Bokeh with Stage Diffusion. (arXiv:2306.08251v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08251
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#25216;&#26415;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;GBSD&#65292;&#24182;&#20351;&#29992;&#38454;&#27573;&#25193;&#25955;&#25216;&#26415;&#21512;&#25104;&#20855;&#26377;&#34394;&#21270;&#39118;&#26684;&#30340;&#29031;&#29255;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Bokeh&#25928;&#26524;&#26159;&#19968;&#31181;&#33402;&#26415;&#25216;&#24039;&#65292;&#21487;&#20197;&#20351;&#29031;&#29255;&#20013;&#30340;&#22833;&#28966;&#21306;&#22495;&#27169;&#31946;&#65292;&#36817;&#26399;&#30001;&#20110;&#25991;&#26412;&#21040;&#22270;&#20687;&#21512;&#25104;&#25216;&#26415;&#30340;&#21457;&#23637;&#20197;&#21450;&#26234;&#33021;&#25163;&#26426;&#30340;&#26222;&#21450;&#21644;&#29031;&#29255;&#20998;&#20139;&#24212;&#29992;&#31243;&#24207;&#30340;&#24191;&#27867;&#20351;&#29992;&#65292;&#36825;&#31181;&#25928;&#26524;&#22791;&#21463;&#20851;&#27880;&#12290;&#20197;&#24448;&#30340;&#34394;&#21270;&#25928;&#26524;&#28210;&#26579;&#24037;&#20316;&#37117;&#26159;&#38024;&#23545;&#24050;&#26377;&#29031;&#29255;&#36827;&#34892;&#20107;&#21518;&#22270;&#20687;&#22788;&#29702;&#65292;&#20351;&#29992;&#32463;&#20856;&#35745;&#31639;&#26426;&#22270;&#24418;&#23398;&#25110;&#31070;&#32463;&#28210;&#26579;&#25216;&#26415;&#20135;&#29983;&#31867;&#20284;&#30340;&#27169;&#31946;&#25928;&#26524;&#65292;&#20294;&#26159;&#24448;&#24448;&#23384;&#22312;&#28145;&#24230;&#19981;&#36830;&#32493;&#30340;&#22270;&#20687;&#20266;&#24433;&#65292;&#32780;&#19988;&#38480;&#21046;&#20110;&#32451;&#20064;&#25968;&#25454;&#27979;&#35797;&#30340;&#34394;&#21270;&#25928;&#26524;&#12290;&#36817;&#26399;&#30340;&#25193;&#25955;&#27169;&#22411;&#21487;&#20197;&#21512;&#25104;&#20855;&#26377;&#33402;&#26415;&#39118;&#26684;&#30340;&#22270;&#20687;&#65292;&#20294;&#26159;&#35201;&#27714;&#29983;&#25104;&#39640;&#32500;&#24230;&#25513;&#27169;&#25110;&#32773;&#36827;&#34892;&#26114;&#36149;&#30340;&#35843;&#25972;&#65292;&#21487;&#33021;&#24433;&#21709;&#25972;&#20307;&#22270;&#20687;&#29305;&#24449;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;GBSD&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#25216;&#26415;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#65292;&#20855;&#26377;&#34394;&#21270;&#39118;&#26684;&#30340;&#29031;&#29255;&#12290;&#21463;&#25193;&#25955;&#27169;&#22411;&#20013;&#36880;&#27493;&#21512;&#25104;&#22270;&#20687;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#20102;&#38454;&#27573;&#25193;&#25955;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
The bokeh effect is an artistic technique that blurs out-of-focus areas in a photograph and has gained interest due to recent developments in text-to-image synthesis and the ubiquity of smart-phone cameras and photo-sharing apps. Prior work on rendering bokeh effects have focused on post hoc image manipulation to produce similar blurring effects in existing photographs using classical computer graphics or neural rendering techniques, but have either depth discontinuity artifacts or are restricted to reproducing bokeh effects that are present in the training data. More recent diffusion based models can synthesize images with an artistic style, but either require the generation of high-dimensional masks, expensive fine-tuning, or affect global image characteristics. In this paper, we present GBSD, the first generative text-to-image model that synthesizes photorealistic images with a bokeh style. Motivated by how image synthesis occurs progressively in diffusion models, our approach combi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21322;&#30417;&#30563;&#30340;&#32454;&#32990;&#35782;&#21035;&#26041;&#27861;&#65292;&#20351;&#29992;&#31471;&#21040;&#31471;PCR&#27169;&#22411;&#21644;&#29983;&#25104;&#30340;&#20266;&#26631;&#31614;&#26469;&#26356;&#22909;&#22320;&#21033;&#29992;&#26410;&#26631;&#27880;&#30340;&#22270;&#20687;&#65292;&#24320;&#21457;&#20102;&#36866;&#29992;&#20110;SSPCR&#30340;&#26694;&#26550;&#65292;&#24182;&#24341;&#20837;&#20102;&#20004;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#21327;&#35843;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#26679;&#26412;&#12290;</title><link>http://arxiv.org/abs/2306.08240</link><description>&lt;p&gt;
&#28857;&#32423;&#30417;&#30563;&#19979;&#30340;&#21322;&#30417;&#30563;&#32454;&#32990;&#35782;&#21035;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Semi-supervised Cell Recognition under Point Supervision. (arXiv:2306.08240v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08240
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21322;&#30417;&#30563;&#30340;&#32454;&#32990;&#35782;&#21035;&#26041;&#27861;&#65292;&#20351;&#29992;&#31471;&#21040;&#31471;PCR&#27169;&#22411;&#21644;&#29983;&#25104;&#30340;&#20266;&#26631;&#31614;&#26469;&#26356;&#22909;&#22320;&#21033;&#29992;&#26410;&#26631;&#27880;&#30340;&#22270;&#20687;&#65292;&#24320;&#21457;&#20102;&#36866;&#29992;&#20110;SSPCR&#30340;&#26694;&#26550;&#65292;&#24182;&#24341;&#20837;&#20102;&#20004;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#21327;&#35843;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32454;&#32990;&#35782;&#21035;&#26159;&#25968;&#23383;&#32452;&#32455;&#30149;&#29702;&#23398;&#22270;&#20687;&#20998;&#26512;&#20013;&#30340;&#22522;&#26412;&#20219;&#21153;&#12290;&#22522;&#20110;&#28857;&#30340;&#32454;&#32990;&#35782;&#21035;&#26041;&#27861;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#30340;&#27880;&#37322;&#65292;&#32791;&#26102;&#36153;&#21147;&#12290;&#21322;&#30417;&#30563;&#23398;&#20064;&#21487;&#20197;&#25552;&#20379;&#19968;&#31181;&#25463;&#24452;&#65292;&#20805;&#20998;&#21033;&#29992;&#21315;&#20159;&#20687;&#32032;&#30340;&#25972;&#24352;&#20999;&#29255;&#22270;&#20687;&#20013;&#30340;&#32454;&#32990;&#20449;&#24687;&#32780;&#26080;&#38656;&#35814;&#23613;&#26631;&#35760;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#21322;&#30417;&#30563;&#28857;&#32423;&#32454;&#32990;&#35782;&#21035;&#65288;SSPCR&#65289;&#30340;&#30740;&#31350;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#37325;&#35270;&#12290;&#20808;&#21069;&#30340;SSPCR&#24037;&#20316;&#37117;&#26159;&#22522;&#20110;&#23494;&#24230;&#22270;&#30340;PCR&#27169;&#22411;&#65292;&#20854;&#31934;&#24230;&#19981;&#23613;&#22914;&#20154;&#24847;&#65292;&#25512;&#29702;&#36895;&#24230;&#32531;&#24930;&#65292;&#23545;&#36229;&#21442;&#25968;&#30340;&#25935;&#24863;&#24230;&#39640;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26368;&#36817;&#25552;&#20986;&#20102;&#31471;&#21040;&#31471;PCR&#27169;&#22411;&#12290;&#26412;&#25991;&#39318;&#27425;&#20026;&#31471;&#21040;&#31471;PCR&#27169;&#22411;&#24320;&#21457;&#20102;&#36866;&#29992;&#30340;SSPCR&#26694;&#26550;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20351;&#29992;&#24403;&#21069;&#27169;&#22411;&#20026;&#26410;&#26631;&#35760;&#30340;&#22270;&#20687;&#29983;&#25104;&#20266;&#26631;&#31614;&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;&#30417;&#30563;&#27169;&#22411;&#30340;&#35757;&#32451;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#21487;&#20197;&#21327;&#35843;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#26679;&#26412;&#12290;&#20026;&#20102;&#39564;&#35777;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#22312;&#20004;&#20010;&#20844;&#24320;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20197;&#26174;&#33879;&#26356;&#23569;&#30340;&#27880;&#37322;&#25104;&#26412;&#21644;&#26102;&#38388;&#33719;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cell recognition is a fundamental task in digital histopathology image analysis. Point-based cell recognition (PCR) methods normally require a vast number of annotations, which is extremely costly, time-consuming and labor-intensive. Semi-supervised learning (SSL) can provide a shortcut to make full use of cell information in gigapixel whole slide images without exhaustive labeling. However, research into semi-supervised point-based cell recognition (SSPCR) remains largely overlooked. Previous SSPCR works are all built on density map-based PCR models, which suffer from unsatisfactory accuracy, slow inference speed and high sensitivity to hyper-parameters. To address these issues, end-to-end PCR models are proposed recently. In this paper, we develop a SSPCR framework suitable for the end-to-end PCR models for the first time. Overall, we use the current models to generate pseudo labels for unlabeled images, which are in turn utilized to supervise the models training. Besides, we introdu
&lt;/p&gt;</description></item><item><title>Maestro&#26159;&#19968;&#20010;&#29992;&#20110;&#25945;&#25480;&#20154;&#24037;&#26234;&#33021;&#23481;&#38169;&#24615;&#30340;&#28216;&#25103;&#21270;&#24179;&#21488;&#65292;&#23427;&#20026;&#22823;&#23398;&#29983;&#20204;&#25552;&#20379;&#20102;&#20855;&#26377;&#29983;&#27963;&#21551;&#21457;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#23398;&#29983;&#20204;&#22312;&#23454;&#39564;&#23545;&#25239;&#25915;&#20987;&#21644;&#38450;&#24481;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2306.08238</link><description>&lt;p&gt;
Maestro: &#19968;&#20010;&#29992;&#20110;&#25945;&#25480;&#20154;&#24037;&#26234;&#33021;&#23481;&#38169;&#24615;&#30340;&#28216;&#25103;&#21270;&#24179;&#21488;
&lt;/p&gt;
&lt;p&gt;
Maestro: A Gamified Platform for Teaching AI Robustness. (arXiv:2306.08238v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08238
&lt;/p&gt;
&lt;p&gt;
Maestro&#26159;&#19968;&#20010;&#29992;&#20110;&#25945;&#25480;&#20154;&#24037;&#26234;&#33021;&#23481;&#38169;&#24615;&#30340;&#28216;&#25103;&#21270;&#24179;&#21488;&#65292;&#23427;&#20026;&#22823;&#23398;&#29983;&#20204;&#25552;&#20379;&#20102;&#20855;&#26377;&#29983;&#27963;&#21551;&#21457;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#23398;&#29983;&#20204;&#22312;&#23454;&#39564;&#23545;&#25239;&#25915;&#20987;&#21644;&#38450;&#24481;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#38450;&#27490;&#20154;&#24037;&#26234;&#33021;&#28431;&#27934;&#23545;&#20110;&#32500;&#25252;&#29992;&#25143;&#21644;&#20225;&#19994;&#30340;&#23433;&#20840;&#21644;&#38544;&#31169;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#20840;&#29699;&#33539;&#22260;&#20869;&#23545;&#20110;&#23481;&#38169;&#22411;&#20154;&#24037;&#26234;&#33021;&#30340;&#25945;&#32946;&#24037;&#20855;&#20173;&#28982;&#19981;&#22815;&#25104;&#29087;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;Maestro&#30340;&#35774;&#35745;&#12289;&#23454;&#26045;&#21644;&#35780;&#20272;&#12290;Maestro&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;&#12289;&#26377;&#25928;&#30340;&#22522;&#20110;&#28216;&#25103;&#30340;&#24179;&#21488;&#65292;&#26377;&#21161;&#20110;&#25512;&#21160;&#23481;&#38169;&#24615;&#20154;&#24037;&#26234;&#33021;&#25945;&#32946;&#30340;&#21457;&#23637;&#12290;Maestro&#25552;&#20379;&#30446;&#26631;&#20026;&#23548;&#21521;&#30340;&#22330;&#26223;&#65292;&#20351;&#22823;&#23398;&#29983;&#20204;&#22312;&#31454;&#20105;&#24615;&#32534;&#31243;&#29615;&#22659;&#20013;&#25509;&#35302;&#21040;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20197;&#29983;&#27963;&#20026;&#28789;&#24863;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;Maestro&#23545;&#22823;&#23398;&#29983;&#22312;&#23398;&#20064;&#20154;&#24037;&#26234;&#33021;&#23481;&#38169;&#24615;&#26041;&#38754;&#30340;&#21442;&#19982;&#24230;&#12289;&#21160;&#26426;&#21644;&#23398;&#20064;&#25104;&#21151;&#30340;&#24433;&#21709;&#12290;&#35813;&#24037;&#20316;&#36824;&#25552;&#20379;&#20102;&#20851;&#20110;&#20419;&#36827;&#23481;&#38169;&#24615;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#20013;&#31215;&#26497;&#23398;&#20064;&#26426;&#20250;&#30340;&#22312;&#32447;&#23398;&#20064;&#24037;&#20855;&#35774;&#35745;&#29305;&#28857;&#30340;&#35265;&#35299;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#22312;&#20004;&#20010;&#22823;&#23398;AI&#35838;&#31243;&#20013;&#20351;&#29992;Maestro&#30340;147&#21517;&#26412;&#31185;&#29983;&#30340;&#21453;&#24605;&#21709;&#24212;&#65288;&#29992;Likert&#37327;&#34920;&#27979;&#37327;&#65289;&#12290;&#26681;&#25454;&#32467;&#26524;&#65292;&#34920;&#31034;&#33719;&#24471;&#26032;&#25216;&#33021;&#21644;&#30693;&#35782;&#12289;&#33258;&#25105;&#25928;&#21147;&#21644;&#28385;&#24847;&#24230;&#30340;&#23398;&#29983;&#65292;&#22312;&#23454;&#39564;&#23545;&#25239;&#25915;&#20987;&#21644;&#38450;&#24481;&#26102;&#34920;&#29616;&#20986;&#26356;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although the prevention of AI vulnerabilities is critical to preserve the safety and privacy of users and businesses, educational tools for robust AI are still underdeveloped worldwide. We present the design, implementation, and assessment of Maestro. Maestro is an effective open-source game-based platform that contributes to the advancement of robust AI education. Maestro provides goal-based scenarios where college students are exposed to challenging life-inspired assignments in a competitive programming environment. We assessed Maestro's influence on students' engagement, motivation, and learning success in robust AI. This work also provides insights into the design features of online learning tools that promote active learning opportunities in the robust AI domain. We analyzed the reflection responses (measured with Likert scales) of 147 undergraduate students using Maestro in two quarterly college courses in AI. According to the results, students who felt the acquisition of new ski
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#36870;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#21363;&#22522;&#20110;&#35838;&#31243;&#23376;&#30446;&#26631;&#30340;&#36870;&#24378;&#21270;&#23398;&#20064; (CSIRL)&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#30340;&#26159;&#65292;&#35813;&#26694;&#26550;&#23558;&#19968;&#20010;&#20219;&#21153;&#20998;&#35299;&#20026;&#22810;&#20010;&#23616;&#37096;&#23376;&#30446;&#26631;&#65292;&#20197;&#24341;&#23548;&#26234;&#33021;&#20307;&#36827;&#34892;&#27169;&#20223;&#65292;&#36825;&#26377;&#21161;&#20110;&#35299;&#20915;&#20840;&#23616;&#35774;&#35745;&#20013;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.08232</link><description>&lt;p&gt;
&#36870;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#35838;&#31243;&#23376;&#30446;&#26631;
&lt;/p&gt;
&lt;p&gt;
Curricular Subgoals for Inverse Reinforcement Learning. (arXiv:2306.08232v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08232
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#36870;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#21363;&#22522;&#20110;&#35838;&#31243;&#23376;&#30446;&#26631;&#30340;&#36870;&#24378;&#21270;&#23398;&#20064; (CSIRL)&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#30340;&#26159;&#65292;&#35813;&#26694;&#26550;&#23558;&#19968;&#20010;&#20219;&#21153;&#20998;&#35299;&#20026;&#22810;&#20010;&#23616;&#37096;&#23376;&#30446;&#26631;&#65292;&#20197;&#24341;&#23548;&#26234;&#33021;&#20307;&#36827;&#34892;&#27169;&#20223;&#65292;&#36825;&#26377;&#21161;&#20110;&#35299;&#20915;&#20840;&#23616;&#35774;&#35745;&#20013;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36870;&#24378;&#21270;&#23398;&#20064;&#65288;IRL&#65289;&#26088;&#22312;&#20174;&#19987;&#23478;&#28436;&#31034;&#20013;&#37325;&#26500;&#22870;&#21169;&#20989;&#25968;&#20197;&#20419;&#36827;&#31574;&#30053;&#23398;&#20064;&#65292;&#24182;&#24050;&#22312;&#27169;&#20223;&#23398;&#20064;&#20013;&#23637;&#31034;&#20986;&#21331;&#36234;&#30340;&#25104;&#21151;&#12290;&#29616;&#26377;&#30340;IRL&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#20110;&#23398;&#20064;&#20840;&#23616;&#22870;&#21169;&#20989;&#25968;&#20197;&#26368;&#23567;&#21270;&#27169;&#20223;&#32773;&#21644;&#19987;&#23478;&#20043;&#38388;&#30340;&#36712;&#36857;&#24046;&#24322;&#65292;&#20294;&#36825;&#20123;&#20840;&#23616;&#35774;&#35745;&#20173;&#28982;&#23384;&#22312;&#20887;&#20313;&#22122;&#22768;&#21644;&#35823;&#24046;&#20256;&#25773;&#38382;&#39064;&#65292;&#23548;&#33268;&#19981;&#36866;&#24403;&#30340;&#22870;&#21169;&#20998;&#37197;&#65292;&#20174;&#32780;&#38477;&#20302;&#20195;&#29702;&#22312;&#22797;&#26434;&#30340;&#22810;&#38454;&#27573;&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inverse Reinforcement Learning (IRL) aims to reconstruct the reward function from expert demonstrations to facilitate policy learning, and has demonstrated its remarkable success in imitation learning. To promote expert-like behavior, existing IRL methods mainly focus on learning global reward functions to minimize the trajectory difference between the imitator and the expert. However, these global designs are still limited by the redundant noise and error propagation problems, leading to the unsuitable reward assignment and thus downgrading the agent capability in complex multi-stage tasks. In this paper, we propose a novel Curricular Subgoal-based Inverse Reinforcement Learning (CSIRL) framework, that explicitly disentangles one task with several local subgoals to guide agent imitation. Specifically, CSIRL firstly introduces decision uncertainty of the trained agent over expert trajectories to dynamically select subgoals, which directly determines the exploration boundary of differen
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21028;&#21035;&#33539;&#24335;&#30340;&#32467;&#26500;&#27169;&#24577;&#32422;&#26463;&#30340;&#26080;&#30417;&#30563;&#36328;&#39046;&#22495;&#32958;&#33039;&#20998;&#21106;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#36793;&#32536;&#32467;&#26500;&#26469;&#21306;&#20998;&#22495;&#19981;&#21464;&#30340;&#32467;&#26500;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#32467;&#26500;&#32422;&#26463;&#30340;&#33258;&#25105;&#23398;&#20064;&#21644;&#28176;&#36827;&#24335;ROI&#23436;&#25104;&#32958;&#33039;&#20998;&#21106;&#65292;&#26080;&#38656;&#22312;&#30446;&#26631;&#39046;&#22495;&#19978;&#36827;&#34892;&#27880;&#37322;&#12290;</title><link>http://arxiv.org/abs/2306.08213</link><description>&lt;p&gt;
SMC-UDA&#65306;&#32467;&#26500;&#27169;&#24577;&#32422;&#26463;&#30340;&#26080;&#30417;&#30563;&#36328;&#39046;&#22495;&#32958;&#33039;&#20998;&#21106;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
SMC-UDA: Structure-Modal Constraint for Unsupervised Cross-Domain Renal Segmentation. (arXiv:2306.08213v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08213
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21028;&#21035;&#33539;&#24335;&#30340;&#32467;&#26500;&#27169;&#24577;&#32422;&#26463;&#30340;&#26080;&#30417;&#30563;&#36328;&#39046;&#22495;&#32958;&#33039;&#20998;&#21106;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#36793;&#32536;&#32467;&#26500;&#26469;&#21306;&#20998;&#22495;&#19981;&#21464;&#30340;&#32467;&#26500;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#32467;&#26500;&#32422;&#26463;&#30340;&#33258;&#25105;&#23398;&#20064;&#21644;&#28176;&#36827;&#24335;ROI&#23436;&#25104;&#32958;&#33039;&#20998;&#21106;&#65292;&#26080;&#38656;&#22312;&#30446;&#26631;&#39046;&#22495;&#19978;&#36827;&#34892;&#27880;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#21307;&#23398;&#24433;&#20687;&#20998;&#21106;&#32463;&#24120;&#22312;&#37096;&#32626;&#21040;&#19981;&#21516;&#39046;&#22495;&#30340;&#22270;&#20687;&#26102;&#22833;&#36133;&#12290;&#39046;&#22495;&#36866;&#24212;&#26041;&#27861;&#26088;&#22312;&#35299;&#20915;&#39046;&#22495;&#36716;&#31227;&#25361;&#25112;&#65292;&#20294;&#20173;&#38754;&#20020;&#19968;&#20123;&#38382;&#39064;&#12290;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#38656;&#35201;&#22312;&#30446;&#26631;&#39046;&#22495;&#19978;&#36827;&#34892;&#27880;&#37322;&#65292;&#29983;&#25104;&#24335;&#26080;&#30417;&#30563;&#39046;&#22495;&#36866;&#24212;&#65288;UDA&#65289;&#27169;&#22411;&#24573;&#30053;&#20102;&#29305;&#23450;&#20110;&#22495;&#30340;&#34920;&#31034;&#65292;&#20854;&#29983;&#25104;&#30340;&#36136;&#37327;&#20005;&#37325;&#38480;&#21046;&#20102;&#20998;&#21106;&#24615;&#33021;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#21028;&#21035;&#33539;&#24335;&#30340;&#32467;&#26500;&#27169;&#24577;&#32422;&#26463;&#65288;SMC&#65289;UDA&#26694;&#26550;&#65292;&#24182;&#24341;&#20837;&#20102;&#36793;&#32536;&#32467;&#26500;&#20316;&#20026;&#36328;&#22495;&#30340;&#26725;&#26753;&#12290;&#25152;&#25552;&#20986;&#30340;&#22810;&#27169;&#24577;&#23398;&#20064;&#39592;&#24178;&#20174;&#22270;&#20687;&#32441;&#29702;&#20013;&#25552;&#21462;&#32467;&#26500;&#20449;&#24687;&#20197;&#21306;&#20998;&#22495;&#19981;&#21464;&#30340;&#36793;&#32536;&#32467;&#26500;&#12290;&#36890;&#36807;&#32467;&#26500;&#32422;&#26463;&#30340;&#33258;&#25105;&#23398;&#20064;&#21644;&#28176;&#36827;&#24335;ROI&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#23450;&#20301;&#36793;&#32536;&#30340;&#19977;&#32500;&#31354;&#38388;&#32467;&#26500;&#26469;&#20998;&#21106;&#32958;&#33039;&#12290;&#25105;&#20204;&#22312;&#20844;&#20849;&#32958;&#33039;&#20998;&#21106;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;SMC-UDA&#65292;&#20174;&#26377;&#26631;&#31614;&#30340;S&#25968;&#25454;&#32452;&#36866;&#24212;&#21040;&#19981;&#21516;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Medical image segmentation based on deep learning often fails when deployed on images from a different domain. The domain adaptation methods aim to solve domain-shift challenges, but still face some problems. The transfer learning methods require annotation on the target domain, and the generative unsupervised domain adaptation (UDA) models ignore domain-specific representations, whose generated quality highly restricts segmentation performance. In this study, we propose a novel Structure-Modal Constrained (SMC) UDA framework based on a discriminative paradigm and introduce edge structure as a bridge between domains. The proposed multi-modal learning backbone distills structure information from image texture to distinguish domain-invariant edge structure. With the structure-constrained self-learning and progressive ROI, our methods segment the kidney by locating the 3D spatial structure of the edge. We evaluated SMC-UDA on public renal segmentation datasets, adapting from the labeled s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#38598;&#21512;&#21464;&#25442;&#22120;&#21644;&#20998;&#23618;&#21452;&#21521;LSTM&#20174;&#22810;&#26234;&#33021;&#20307;&#36816;&#21160;&#29615;&#22659;&#20013;&#25512;&#26029;&#29699;&#30340;&#36712;&#36857;&#30340;&#25512;&#26029;&#26694;&#26550;&#65292;&#33021;&#22815;&#25104;&#21151;&#22320;&#39044;&#27979;&#29699;&#30340;&#36712;&#36857;&#65292;&#21363;&#20351;&#22312;&#22797;&#26434;&#30340;&#24773;&#20917;&#19979;&#20063;&#26159;&#22914;&#27492;&#12290;</title><link>http://arxiv.org/abs/2306.08206</link><description>&lt;p&gt;
&#20351;&#29992;&#38598;&#21512;&#21464;&#25442;&#22120;&#21644;&#20998;&#23618;&#21452;&#21521;LSTM&#20174;&#22810;&#26234;&#33021;&#20307;&#36816;&#21160;&#29615;&#22659;&#20013;&#25512;&#26029;&#29699;&#30340;&#36712;&#36857;
&lt;/p&gt;
&lt;p&gt;
Ball Trajectory Inference from Multi-Agent Sports Contexts Using Set Transformer and Hierarchical Bi-LSTM. (arXiv:2306.08206v1 [cs.MA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08206
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#38598;&#21512;&#21464;&#25442;&#22120;&#21644;&#20998;&#23618;&#21452;&#21521;LSTM&#20174;&#22810;&#26234;&#33021;&#20307;&#36816;&#21160;&#29615;&#22659;&#20013;&#25512;&#26029;&#29699;&#30340;&#36712;&#36857;&#30340;&#25512;&#26029;&#26694;&#26550;&#65292;&#33021;&#22815;&#25104;&#21151;&#22320;&#39044;&#27979;&#29699;&#30340;&#36712;&#36857;&#65292;&#21363;&#20351;&#22312;&#22797;&#26434;&#30340;&#24773;&#20917;&#19979;&#20063;&#26159;&#22914;&#27492;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#21040;&#20102;&#35768;&#22810;&#39046;&#22495;&#65292;&#23558;AI&#24212;&#29992;&#20110;&#20307;&#32946;&#20998;&#26512;&#20063;&#21463;&#21040;&#20102;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#33258;&#21160;&#33719;&#21462;&#36816;&#21160;&#27604;&#36187;&#20013;&#36830;&#32493;&#36816;&#21160;&#25968;&#25454;&#30340;&#38590;&#24230;&#26159;&#20027;&#35201;&#25361;&#25112;&#20043;&#19968;&#12290;&#29305;&#21035;&#26159;&#65292;&#22312;&#19968;&#20010;&#26377;&#38556;&#30861;&#29289;&#22914;&#36974;&#25377;&#21644;&#24178;&#25200;&#30340;&#23485;&#24191;&#36275;&#29699;&#22330;&#19978;&#21487;&#38752;&#22320;&#36319;&#36394;&#19968;&#20010;&#23567;&#29699;&#26159;&#38590;&#20197;&#35299;&#20915;&#30340;&#38590;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#29699;&#21592;&#36712;&#36857;&#25512;&#26029;&#29699;&#30340;&#36712;&#36857;&#30340;&#25512;&#26029;&#26694;&#26550;&#65292;&#20316;&#20026;&#29699;&#36861;&#36394;&#30340;&#19968;&#31181;&#32463;&#27982;&#23454;&#29992;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#25105;&#20204;&#32467;&#21512;&#20102;&#38598;&#21512;&#21464;&#25442;&#22120;&#26469;&#33719;&#24471;&#22810;&#26234;&#33021;&#20307;&#29615;&#22659;&#30340;&#32622;&#25442;&#19981;&#21464;&#21644;&#31561;&#21464;&#34920;&#31034;&#65292;&#24182;&#37319;&#29992;&#20998;&#23618;&#26550;&#26500;&#65292;&#20013;&#38388;&#39044;&#27979;&#29699;&#21592;&#25317;&#26377;&#26435;&#20197;&#25903;&#25345;&#26368;&#32456;&#30340;&#36712;&#36857;&#25512;&#26029;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#29616;&#23454;&#25439;&#22833;&#39033;&#21644;&#21518;&#22788;&#29702;&#26469;&#30830;&#20445;&#20272;&#35745;&#36712;&#36857;&#30340;&#29289;&#29702;&#21512;&#29702;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#25552;&#20379;&#20102;&#33258;&#28982;&#21644;&#20934;&#30830;&#30340;&#22810;&#26234;&#33021;&#20307;&#36816;&#21160;&#29615;&#22659;&#32763;&#35793;&#65292;&#24182;&#20351;&#24471;&#21363;&#20351;&#22312;&#22797;&#26434;&#30340;&#24773;&#20917;&#19979;&#20063;&#33021;&#25104;&#21151;&#39044;&#27979;&#29699;&#30340;&#36712;&#36857;&#12290;
&lt;/p&gt;
&lt;p&gt;
As artificial intelligence spreads out to numerous fields, the application of AI to sports analytics is also in the spotlight. However, one of the major challenges is the difficulty of automated acquisition of continuous movement data during sports matches. In particular, it is a conundrum to reliably track a tiny ball on a wide soccer pitch with obstacles such as occlusion and imitations. Tackling the problem, this paper proposes an inference framework of ball trajectory from player trajectories as a cost-efficient alternative to ball tracking. We combine Set Transformers to get permutation-invariant and equivariant representations of the multi-agent contexts with a hierarchical architecture that intermediately predicts the player ball possession to support the final trajectory inference. Also, we introduce the reality loss term and postprocessing to secure the estimated trajectories to be physically realistic. The experimental results show that our model provides natural and accurate
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20351;&#29992;&#21452;&#37325;&#31574;&#30053;&#35299;&#20915;ARC&#20219;&#21153;&#65292;&#36890;&#36807;&#20915;&#31574;Transformer&#27169;&#20223;&#20154;&#31867;&#35299;&#20915;&#26041;&#26696;&#21644;&#24341;&#20837;&#29289;&#20307;&#26816;&#27979;&#31639;&#27861;&#26469;&#25552;&#39640;AI&#30340;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#65292;&#21516;&#26102;&#25581;&#31034;&#20102;&#25968;&#25454;&#21644;&#27169;&#22411;&#32467;&#26500;&#31561;&#26041;&#38754;&#30340;&#38656;&#27714;&#65292;&#20026;&#26410;&#26469;AGI&#30740;&#31350;&#25552;&#20379;&#20102;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2306.08204</link><description>&lt;p&gt;
&#35299;&#35868;ARC&#65306;&#29992;&#29289;&#20307;&#20026;&#20013;&#24515;&#30340;&#20915;&#31574;Transformer&#27169;&#20223;&#20154;&#31867;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Unraveling the ARC Puzzle: Mimicking Human Solutions with Object-Centric Decision Transformer. (arXiv:2306.08204v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08204
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20351;&#29992;&#21452;&#37325;&#31574;&#30053;&#35299;&#20915;ARC&#20219;&#21153;&#65292;&#36890;&#36807;&#20915;&#31574;Transformer&#27169;&#20223;&#20154;&#31867;&#35299;&#20915;&#26041;&#26696;&#21644;&#24341;&#20837;&#29289;&#20307;&#26816;&#27979;&#31639;&#27861;&#26469;&#25552;&#39640;AI&#30340;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#65292;&#21516;&#26102;&#25581;&#31034;&#20102;&#25968;&#25454;&#21644;&#27169;&#22411;&#32467;&#26500;&#31561;&#26041;&#38754;&#30340;&#38656;&#27714;&#65292;&#20026;&#26410;&#26469;AGI&#30740;&#31350;&#25552;&#20379;&#20102;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36861;&#27714;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#65288;AGI&#65289;&#30340;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21452;&#37325;&#26041;&#27861;&#26469;&#35299;&#20915;&#25277;&#35937;&#25512;&#29702;&#25991;&#26412;&#38598;&#65288;ARC&#65289;&#20219;&#21153;&#12290;&#25105;&#20204;&#20351;&#29992;&#20915;&#31574;Transformer&#22312;&#27169;&#20223;&#23398;&#20064;&#33539;&#24335;&#19979;&#24314;&#27169;&#20154;&#31867;&#38382;&#39064;&#35299;&#20915;&#65292;&#24182;&#24341;&#20837;&#20102;&#29289;&#20307;&#26816;&#27979;&#31639;&#27861;Push and Pull&#32858;&#31867;&#26041;&#27861;&#12290;&#36825;&#31181;&#21452;&#37325;&#31574;&#30053;&#22686;&#24378;&#20102;AI&#30340;ARC&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#65292;&#24182;&#20026;AGI&#36827;&#23637;&#25552;&#20379;&#20102;&#35265;&#35299;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#25581;&#31034;&#20102;&#39640;&#32423;&#25968;&#25454;&#25910;&#38598;&#24037;&#20855;&#12289;&#24378;&#22823;&#30340;&#22521;&#35757;&#25968;&#25454;&#38598;&#21644;&#31934;&#32454;&#30340;&#27169;&#22411;&#32467;&#26500;&#38656;&#27714;&#12290;&#36825;&#39033;&#30740;&#31350;&#31361;&#26174;&#20102;&#20915;&#31574;Transformer&#30340;&#28508;&#22312;&#25913;&#36827;&#65292;&#24182;&#25512;&#21160;&#26410;&#26469;AGI&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the pursuit of artificial general intelligence (AGI), we tackle Abstraction and Reasoning Corpus (ARC) tasks using a novel two-pronged approach. We employ the Decision Transformer in an imitation learning paradigm to model human problem-solving, and introduce an object detection algorithm, the Push and Pull clustering method. This dual strategy enhances AI's ARC problem-solving skills and provides insights for AGI progression. Yet, our work reveals the need for advanced data collection tools, robust training datasets, and refined model structures. This study highlights potential improvements for Decision Transformers and propels future AGI research.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861; CGNN&#65292;&#23427;&#21033;&#29992;&#19968;&#33268;&#24615;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#22522;&#20110;&#21516;&#36136;&#24615;&#20551;&#35774;&#30340;&#26679;&#26412;&#36873;&#25321;&#25216;&#26415;&#65292;&#22312;&#26631;&#31614;&#22122;&#22768;&#30340;&#24773;&#20917;&#19979;&#23545;&#33410;&#28857;&#20998;&#31867;&#36827;&#34892;&#24314;&#27169;&#65292;&#23454;&#29616;&#36807;&#28388;&#20986;&#22122;&#22768;&#33410;&#28857;&#21644;&#22686;&#24378;&#33410;&#28857;&#34920;&#31034;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.08194</link><description>&lt;p&gt;
&#22312;&#26631;&#31614;&#22122;&#22768;&#19979;&#30340;&#22270;&#19978;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Learning on Graphs under Label Noise. (arXiv:2306.08194v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08194
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861; CGNN&#65292;&#23427;&#21033;&#29992;&#19968;&#33268;&#24615;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#22522;&#20110;&#21516;&#36136;&#24615;&#20551;&#35774;&#30340;&#26679;&#26412;&#36873;&#25321;&#25216;&#26415;&#65292;&#22312;&#26631;&#31614;&#22122;&#22768;&#30340;&#24773;&#20917;&#19979;&#23545;&#33410;&#28857;&#20998;&#31867;&#36827;&#34892;&#24314;&#27169;&#65292;&#23454;&#29616;&#36807;&#28388;&#20986;&#22122;&#22768;&#33410;&#28857;&#21644;&#22686;&#24378;&#33410;&#28857;&#34920;&#31034;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#19978;&#30340;&#33410;&#28857;&#20998;&#31867;&#26159;&#19968;&#39033;&#37325;&#35201;&#30340;&#20219;&#21153;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#31038;&#20132;&#20998;&#26512;&#21644;&#24322;&#24120;&#26816;&#27979;&#12290;&#34429;&#28982;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#22312;&#36825;&#39033;&#20219;&#21153;&#19978;&#24050;&#32463;&#21462;&#24471;&#20102;&#19968;&#20123;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#20294;&#30446;&#21069;&#30340;&#25216;&#26415;&#36890;&#24120;&#20551;&#35774;&#33410;&#28857;&#30340;&#26631;&#31614;&#20449;&#24687;&#26159;&#20934;&#30830;&#30340;&#65292;&#36825;&#22312;&#29616;&#23454;&#24212;&#29992;&#20013;&#21487;&#33021;&#24182;&#19981;&#25104;&#31435;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22914;&#20309;&#22312;&#26631;&#31614;&#22122;&#22768;&#19979;&#30340;&#22270;&#19978;&#23398;&#20064;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#21517;&#20026;&#19968;&#33268;&#24615;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;CGNN&#65289;&#30340;&#26032;&#26041;&#27861;&#26469;&#35299;&#20915;&#23427;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#22270;&#23545;&#27604;&#23398;&#20064;&#20316;&#20026;&#27491;&#21017;&#21270;&#39033;&#65292;&#20419;&#36827;&#22686;&#24378;&#33410;&#28857;&#30340;&#20004;&#20010;&#35270;&#35282;&#20855;&#26377;&#19968;&#33268;&#30340;&#34920;&#31034;&#12290;&#30001;&#20110;&#36825;&#20010;&#27491;&#21017;&#21270;&#39033;&#19981;&#33021;&#21033;&#29992;&#26631;&#31614;&#20449;&#24687;&#65292;&#23427;&#21487;&#20197;&#22686;&#24378;&#33410;&#28857;&#34920;&#31034;&#23545;&#26631;&#31614;&#22122;&#22768;&#30340;&#40065;&#26834;&#24615;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#22312;&#22270;&#19978;&#26816;&#27979;&#22122;&#22768;&#26631;&#31614;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21516;&#36136;&#24615;&#20551;&#35774;&#30340;&#26679;&#26412;&#36873;&#25321;&#25216;&#26415;&#65292;&#36890;&#36807;&#27979;&#37327;&#23884;&#20837;&#21644;&#23427;&#20204;&#30340;&#37051;&#23621;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#26469;&#35782;&#21035;&#22122;&#22768;&#33410;&#28857;&#12290;&#21508;&#31181;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;CGNN&#21487;&#20197;&#26377;&#25928;&#22320;&#32531;&#35299;&#26631;&#31614;&#22122;&#22768;&#23545;&#33410;&#28857;&#20998;&#31867;&#30340;&#36127;&#38754;&#24433;&#21709;&#65292;&#24182;&#22312;&#23545;&#31216;&#21644;&#38750;&#23545;&#31216;&#26631;&#31614;&#22122;&#22768;&#27169;&#22411;&#19979;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
Node classification on graphs is a significant task with a wide range of applications, including social analysis and anomaly detection. Even though graph neural networks (GNNs) have produced promising results on this task, current techniques often presume that label information of nodes is accurate, which may not be the case in real-world applications. To tackle this issue, we investigate the problem of learning on graphs with label noise and develop a novel approach dubbed Consistent Graph Neural Network (CGNN) to solve it. Specifically, we employ graph contrastive learning as a regularization term, which promotes two views of augmented nodes to have consistent representations. Since this regularization term cannot utilize label information, it can enhance the robustness of node representations to label noise. Moreover, to detect noisy labels on the graph, we present a sample selection technique based on the homophily assumption, which identifies noisy nodes by measuring the consisten
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#25506;&#27979;&#20998;&#31867;&#22120;&#26469;&#35780;&#20272;&#32452;&#20214;&#26159;&#21542;&#34920;&#31034;&#23646;&#24615;&#65292;&#22635;&#34917;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#20851;&#20110;&#8220;&#34920;&#31034;&#8221;&#30340;&#21746;&#23398;&#31354;&#30333;&#12290;</title><link>http://arxiv.org/abs/2306.08193</link><description>&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#34920;&#31034;&#23454;&#36341;
&lt;/p&gt;
&lt;p&gt;
Operationalising Representation in Natural Language Processing. (arXiv:2306.08193v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08193
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#25506;&#27979;&#20998;&#31867;&#22120;&#26469;&#35780;&#20272;&#32452;&#20214;&#26159;&#21542;&#34920;&#31034;&#23646;&#24615;&#65292;&#22635;&#34917;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#20851;&#20110;&#8220;&#34920;&#31034;&#8221;&#30340;&#21746;&#23398;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#8220;&#34920;&#31034;&#8221;&#22312;&#35748;&#30693;&#31185;&#23398;&#21746;&#23398;&#20013;&#20855;&#26377;&#26680;&#24515;&#22320;&#20301;&#65292;&#20294;&#22312;&#24403;&#20195;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#23454;&#36341;&#20013;&#65292;&#20960;&#20046;&#27809;&#26377;&#21746;&#23398;&#39046;&#22495;&#30340;&#20808;&#21069;&#30740;&#31350;&#19982;&#20043;&#28041;&#21450;&#12290;&#26412;&#25991;&#26088;&#22312;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65306;&#32467;&#21512;&#35748;&#30693;&#31185;&#23398;&#30340;&#24605;&#24819;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#35780;&#20272;&#31070;&#32463;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#32452;&#20214;&#25152;&#20316;&#20986;&#30340;&#34920;&#31034;&#24615;&#22768;&#26126;&#65292;&#24182;&#25552;&#20986;&#19977;&#20010;&#35780;&#20272;&#32452;&#20214;&#26159;&#21542;&#34920;&#31034;&#23646;&#24615;&#30340;&#26631;&#20934;&#65292;&#24182;&#20351;&#29992;&#25506;&#27979;&#20998;&#31867;&#22120;&#26469;&#23454;&#29616;&#36825;&#20123;&#26631;&#20934;&#30340;&#25805;&#20316;&#21270;&#65292;&#25506;&#27979;&#20998;&#31867;&#22120;&#26159;NLP&#65288;&#21644;&#26356;&#24191;&#27867;&#30340;&#28145;&#24230;&#23398;&#20064;&#65289;&#20013;&#27969;&#34892;&#30340;&#20998;&#26512;&#25216;&#26415;&#12290;&#25805;&#20316;&#21270;&#19968;&#20010;&#22312;&#21746;&#23398;&#19978;&#21463;&#21040;&#21551;&#21457;&#30340;&#8220;&#34920;&#31034;&#8221;&#27010;&#24565;&#30340;&#39033;&#30446;&#24212;&#35813;&#24341;&#36215;&#31185;&#23398;&#21746;&#23398;&#23478;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#23454;&#36341;&#32773;&#30340;&#20852;&#36259;&#12290;&#23545;&#20110;&#21746;&#23398;&#23478;&#26469;&#35828;&#65292;&#36825;&#25552;&#20379;&#20102;&#19968;&#20010;&#27979;&#35797;&#26377;&#20851;&#34920;&#31034;&#30340;&#26412;&#36136;&#30340;&#35770;&#25454;&#30340;&#26032;&#39062;&#22330;&#22320;&#65292;&#24182;&#24110;&#21161;NLPers&#32452;&#32455;&#26377;&#20851;&#25506;&#27979;&#23454;&#39564;&#30340;&#22823;&#37327;&#25991;&#29486;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#32463;&#39564;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite its centrality in the philosophy of cognitive science, there has been little prior philosophical work engaging with the notion of representation in contemporary NLP practice. This paper attempts to fill that lacuna: drawing on ideas from cognitive science, I introduce a framework for evaluating the representational claims made about components of neural NLP models, proposing three criteria with which to evaluate whether a component of a model represents a property and operationalising these criteria using probing classifiers, a popular analysis technique in NLP (and deep learning more broadly).  The project of operationalising a philosophically-informed notion of representation should be of interest to both philosophers of science and NLP practitioners. It affords philosophers a novel testing-ground for claims about the nature of representation, and helps NLPers organise the large literature on probing experiments, suggesting novel avenues for empirical research.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Conformer&#30340;&#26032;&#22411;&#21160;&#24577;&#19978;&#19979;&#25991;&#20256;&#36882;&#26426;&#21046;DCTX-Conformer&#65292;&#35299;&#20915;&#20102;&#27969;&#24335;&#35782;&#21035;&#24615;&#33021;&#24046;&#36317;&#30340;&#38382;&#39064;&#65292;&#30456;&#27604;&#20110;&#29616;&#26377;&#26368;&#20248;&#35299;&#65292;&#35782;&#21035;&#32467;&#26524;&#30340;&#35823;&#24046;&#29575;&#25552;&#39640;&#20102;25%&#65292;&#20294;&#23545;&#24310;&#36831;&#24433;&#21709;&#21487;&#20197;&#24573;&#30053;&#19981;&#35745;&#12290;</title><link>http://arxiv.org/abs/2306.08175</link><description>&lt;p&gt;
DCTX-Conformer&#65306;&#38024;&#23545;&#20302;&#24310;&#36831;&#32479;&#19968;&#27969;&#24335;&#21644;&#38750;&#27969;&#24335;Conformer&#30340;&#21160;&#24577;&#19978;&#19979;&#25991;&#20256;&#36882;
&lt;/p&gt;
&lt;p&gt;
DCTX-Conformer: Dynamic context carry-over for low latency unified streaming and non-streaming Conformer. (arXiv:2306.08175v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08175
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Conformer&#30340;&#26032;&#22411;&#21160;&#24577;&#19978;&#19979;&#25991;&#20256;&#36882;&#26426;&#21046;DCTX-Conformer&#65292;&#35299;&#20915;&#20102;&#27969;&#24335;&#35782;&#21035;&#24615;&#33021;&#24046;&#36317;&#30340;&#38382;&#39064;&#65292;&#30456;&#27604;&#20110;&#29616;&#26377;&#26368;&#20248;&#35299;&#65292;&#35782;&#21035;&#32467;&#26524;&#30340;&#35823;&#24046;&#29575;&#25552;&#39640;&#20102;25%&#65292;&#20294;&#23545;&#24310;&#36831;&#24433;&#21709;&#21487;&#20197;&#24573;&#30053;&#19981;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Conformer&#30340;&#31471;&#21040;&#31471;&#27169;&#22411;&#29616;&#22312;&#24050;&#32463;&#21464;&#24471;&#26222;&#36941;&#65292;&#34987;&#24191;&#27867;&#29992;&#20110;&#27969;&#24335;&#21644;&#38750;&#27969;&#24335;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#20013;&#12290;&#35832;&#22914;&#21452;&#27169;&#24335;&#21644;&#21160;&#24577;&#20998;&#22359;&#35757;&#32451;&#31561;&#25216;&#26415;&#26377;&#21161;&#20110;&#32479;&#19968;&#27969;&#24335;&#21644;&#38750;&#27969;&#24335;&#31995;&#32479;&#12290;&#28982;&#32780;&#65292;&#22312;&#23436;&#25972;&#21644;&#26377;&#38480;&#30340;&#36807;&#21435;&#19978;&#19979;&#25991;&#30340;&#24773;&#20917;&#19979;&#65292;&#27969;&#24335;&#35782;&#21035;&#20043;&#38388;&#20173;&#28982;&#23384;&#22312;&#30528;&#24615;&#33021;&#24046;&#36317;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21160;&#24577;&#19978;&#19979;&#25991;&#20256;&#36882;&#26426;&#21046;&#65292;&#23558;&#20854;&#19982;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#30340;&#32479;&#19968;ASR&#31995;&#32479;&#36827;&#34892;&#38598;&#25104;&#12290;&#25105;&#20204;&#30340;&#25552;&#35758;&#8212;&#8212;&#21160;&#24577;&#19978;&#19979;&#25991;Conformer&#65288;DCTX-Conformer&#65289;&#21033;&#29992;&#20102;&#19968;&#20010;&#38750;&#37325;&#21472;&#30340;&#19978;&#19979;&#25991;&#20256;&#36882;&#26426;&#21046;&#65292;&#21516;&#26102;&#32771;&#34385;&#20102;&#19968;&#22359;&#30340;&#24038;&#19978;&#19979;&#25991;&#21644;&#19968;&#20010;&#25110;&#22810;&#20010;&#20808;&#21069;&#30340;&#19978;&#19979;&#25991;&#23884;&#20837;&#12290;&#30001;&#20110;&#39069;&#22806;&#30340;&#19978;&#19979;&#25991;&#23884;&#20837;&#65292;&#25105;&#20204;&#30456;&#23545;&#20110;&#30446;&#21069;&#26368;&#20248;&#35299;&#25552;&#21319;&#20102;25.0%&#30340;&#35789;&#38169;&#35823;&#29575;&#65292;&#32780;&#24310;&#36831;&#24433;&#21709;&#21487;&#20197;&#24573;&#30053;&#19981;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conformer-based end-to-end models have become ubiquitous these days and are commonly used in both streaming and non-streaming automatic speech recognition (ASR). Techniques like dual-mode and dynamic chunk training helped unify streaming and non-streaming systems. However, there remains a performance gap between streaming with a full and limited past context. To address this issue, we propose the integration of a novel dynamic contextual carry-over mechanism in a state-of-the-art (SOTA) unified ASR system. Our proposed dynamic context Conformer (DCTX-Conformer) utilizes a non-overlapping contextual carry-over mechanism that takes into account both the left context of a chunk and one or more preceding context embeddings. We outperform the SOTA by a relative 25.0% word error rate, with a negligible latency impact due to the additional context embeddings.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24378;&#21270;&#23398;&#20064;&#39537;&#21160;&#30340;&#38142;&#25509;&#29289;&#35774;&#35745;&#26041;&#27861;&#65292;&#25104;&#21151;&#22320;&#29983;&#25104;&#28385;&#36275;2D&#21644;3D&#35201;&#27714;&#30340;&#36830;&#25509;&#21058;&#65292;&#23454;&#29616;&#20102;&#26032;&#22411;&#36830;&#25509;&#21058;&#30340;&#21046;&#22791;&#12290;</title><link>http://arxiv.org/abs/2306.08166</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#39537;&#21160;&#30340;&#24555;&#36895;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#28857;&#20113;&#23545;&#40784;&#30340;&#36830;&#25509;&#29289;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning-Driven Linker Design via Fast Attention-based Point Cloud Alignment. (arXiv:2306.08166v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08166
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24378;&#21270;&#23398;&#20064;&#39537;&#21160;&#30340;&#38142;&#25509;&#29289;&#35774;&#35745;&#26041;&#27861;&#65292;&#25104;&#21151;&#22320;&#29983;&#25104;&#28385;&#36275;2D&#21644;3D&#35201;&#27714;&#30340;&#36830;&#25509;&#21058;&#65292;&#23454;&#29616;&#20102;&#26032;&#22411;&#36830;&#25509;&#21058;&#30340;&#21046;&#22791;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Proteolysis-Targeting Chimeras(PROTACs)&#26159;&#19968;&#31867;&#26032;&#22411;&#23567;&#20998;&#23376;&#65292;&#35774;&#35745;&#25104;&#26725;&#26753;&#65292;&#23558;E3&#36830;&#25509;&#37238;&#21644;&#19982;&#30142;&#30149;&#30456;&#20851;&#30340;&#34507;&#30333;&#36136;&#36830;&#25509;&#36215;&#26469;&#65292;&#20174;&#32780;&#20419;&#36827;&#20854;&#21518;&#32493;&#38477;&#35299;&#12290; PROTACs&#30001;&#20004;&#20010;&#34507;&#30333;&#36136;&#32467;&#21512;&#30340;&#8220;&#27963;&#24615;&#8221;&#21306;&#22495;&#32452;&#25104;&#65292;&#30001;&#8220;&#36830;&#25509;&#8221;&#21306;&#22495;&#36830;&#25509;&#12290; &#36830;&#25509;&#21306;&#22495;&#30340;&#35774;&#35745;&#30001;&#20854;&#30456;&#20114;&#20316;&#29992;&#32473;&#20986;&#20960;&#20309;&#21644;&#21270;&#23398;&#32422;&#26463;&#26465;&#20214;&#65292;&#24182;&#38656;&#35201;&#26368;&#22823;&#21270;&#33647;&#29289;&#26679;&#26412;&#12290; &#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;ShapeLinker&#65292;&#19968;&#31181;&#29992;&#20110;&#20840;&#26032;&#35774;&#35745;&#38142;&#25509;&#29289;&#30340;&#26041;&#27861;&#12290; &#23427;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#22312;&#33258;&#22238;&#24402;SMILES&#29983;&#25104;&#22120;&#19978;&#36827;&#34892;&#29255;&#27573;&#36830;&#25509;&#12290; &#35813;&#26041;&#27861;&#36890;&#36807;&#32467;&#21512;&#30456;&#20851;&#30340;&#29289;&#29702;&#21270;&#23398;&#24615;&#36136;&#21644;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#28857;&#20113;&#23545;&#40784;&#24471;&#20998;&#26469;&#23545;&#20854;&#36827;&#34892;&#20248;&#21270;&#12290; &#36825;&#31181;&#26032;&#26041;&#27861;&#25104;&#21151;&#22320;&#29983;&#25104;&#28385;&#36275;&#30456;&#20851;2D&#21644;3D&#35201;&#27714;&#30340;&#36830;&#25509;&#21058;&#65292;&#24182;&#22312;&#29983;&#25104;&#20551;&#23450;&#30446;&#26631;&#36830;&#25509;&#21058;&#26500;&#36896;&#30340;&#26032;&#22411;&#36830;&#25509;&#21058;&#26041;&#38754;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Proteolysis-Targeting Chimeras (PROTACs) represent a novel class of small molecules which are designed to act as a bridge between an E3 ligase and a disease-relevant protein, thereby promoting its subsequent degradation. PROTACs are composed of two protein binding "active" domains, linked by a "linker" domain. The design of the linker domain is challenging due to geometric and chemical constraints given by its interactions, and the need to maximize drug-likeness. To tackle these challenges, we introduce ShapeLinker, a method for de novo design of linkers. It performs fragment-linking using reinforcement learning on an autoregressive SMILES generator. The method optimizes for a composite score combining relevant physicochemical properties and a novel, attention-based point cloud alignment score. This new method successfully generates linkers that satisfy both relevant 2D and 3D requirements, and achieves state-of-the-art results in producing novel linkers assuming a target linker confor
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20302;&#31209;&#33258;&#36866;&#24212;&#32416;&#38169;&#30340;&#26041;&#27861;&#65292;&#20174;&#32780;&#21487;&#20197;&#26174;&#33879;&#22320;&#20943;&#23569;&#31934;&#32454;&#35843;&#25972;VRAM&#38656;&#27714;&#65292;&#24182;&#32416;&#27491;&#37327;&#21270;&#22823;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#37327;&#21270;&#35823;&#24046;&#65292;&#20351;&#28040;&#36153;&#32773;&#31508;&#35760;&#26412;&#30005;&#33041;&#21487;&#20197;&#23545;70&#20159;&#20010;&#21442;&#25968;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#31934;&#32454;&#35843;&#25972;&#65292;&#29983;&#25104;&#36830;&#36143;&#30340;&#33521;&#25991;&#25991;&#26412;&#12290;</title><link>http://arxiv.org/abs/2306.08162</link><description>&lt;p&gt;
INT2.1&#65306;&#36890;&#36807;&#20302;&#31209;&#33258;&#36866;&#24212;&#32416;&#38169;&#23454;&#29616;&#31934;&#32454;&#21487;&#35843;&#30340;&#37327;&#21270;&#22823;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
INT2.1: Towards Fine-Tunable Quantized Large Language Models with Error Correction through Low-Rank Adaptation. (arXiv:2306.08162v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08162
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20302;&#31209;&#33258;&#36866;&#24212;&#32416;&#38169;&#30340;&#26041;&#27861;&#65292;&#20174;&#32780;&#21487;&#20197;&#26174;&#33879;&#22320;&#20943;&#23569;&#31934;&#32454;&#35843;&#25972;VRAM&#38656;&#27714;&#65292;&#24182;&#32416;&#27491;&#37327;&#21270;&#22823;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#37327;&#21270;&#35823;&#24046;&#65292;&#20351;&#28040;&#36153;&#32773;&#31508;&#35760;&#26412;&#30005;&#33041;&#21487;&#20197;&#23545;70&#20159;&#20010;&#21442;&#25968;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#31934;&#32454;&#35843;&#25972;&#65292;&#29983;&#25104;&#36830;&#36143;&#30340;&#33521;&#25991;&#25991;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#26174;&#33879;&#22320;&#20943;&#23569;&#31934;&#32454;&#35843;&#25972;VRAM&#38656;&#27714;&#65292;&#24182;&#32416;&#27491;&#37327;&#21270;&#22823;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#37327;&#21270;&#35823;&#24046;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;&#20302;&#31209;&#33258;&#36866;&#24212;&#65288;LoRA&#65289;&#24320;&#21457;&#20102;&#19968;&#31181;&#26497;&#20854;&#20869;&#23384;&#39640;&#25928;&#30340;&#37327;&#21270;&#27169;&#22411;&#31934;&#32454;&#35843;&#25972;&#26041;&#27861;&#65288;EMEF&#65289;&#65292;&#24182;&#26681;&#25454;&#23427;&#26500;&#24314;&#20102;&#19968;&#20010;&#38169;&#35823;&#20462;&#27491;&#31639;&#27861;&#65292;&#26088;&#22312;&#26368;&#23567;&#21270;&#37327;&#21270;&#36807;&#31243;&#20013;&#24341;&#36215;&#30340;&#35823;&#24046;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#23558;&#20869;&#23384;&#35201;&#27714;&#38477;&#20302;&#22810;&#36798;5.6&#20493;&#65292;&#20174;&#32780;&#20351;&#28040;&#36153;&#32773;&#31508;&#35760;&#26412;&#30005;&#33041;&#21487;&#20197;&#23545;70&#20159;&#20010;&#21442;&#25968;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#31934;&#32454;&#35843;&#25972;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20302;&#31209;&#32416;&#38169;&#65288;LREC&#65289;&#26041;&#27861;&#65292;&#21033;&#29992;&#22686;&#21152;&#30340;LoRA&#23618;&#26469;&#25913;&#21892;&#37327;&#21270;&#27169;&#22411;&#19982;&#20854;&#28014;&#28857;&#25968;&#23545;&#24212;&#29289;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#25105;&#20204;&#30340;&#32416;&#38169;&#26694;&#26550;&#21487;&#20197;&#29983;&#25104;&#36830;&#36143;&#30340;&#33521;&#25991;&#25991;&#26412;&#65292;&#23454;&#29616;&#20102;&#23436;&#20840;&#21151;&#33021;&#30340;INT2&#37327;&#21270;&#22823;&#35821;&#35328;&#27169;&#22411;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#33021;&#22815;&#36798;&#21040;&#36825;&#31181;&#24615;&#33021;&#30340;INT2&#22823;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a method that dramatically reduces fine-tuning VRAM requirements and rectifies quantization errors in quantized Large Language Models. First, we develop an extremely memory-efficient fine-tuning (EMEF) method for quantized models using Low-Rank Adaptation (LoRA), and drawing upon it, we construct an error-correcting algorithm designed to minimize errors induced by the quantization process. Our method reduces the memory requirements by up to 5.6 times, which enables fine-tuning a 7 billion parameter Large Language Model (LLM) on consumer laptops. At the same time, we propose a Low-Rank Error Correction (LREC) method that exploits the added LoRA layers to ameliorate the gap between the quantized model and its float point counterpart. Our error correction framework leads to a fully functional INT2 quantized LLM with the capacity to generate coherent English text. To the best of our knowledge, this is the first INT2 Large Language Model that has been able to reach such a perfo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;h2oGPT&#65292;&#36825;&#26159;&#19968;&#22871;&#24320;&#28304;&#20195;&#30721;&#24211;&#65292;&#29992;&#20110;&#21019;&#24314;&#21644;&#20351;&#29992;&#22522;&#20110;GPTs&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#21253;&#25324;100&#65285;&#31169;&#26377;&#25991;&#26723;&#25628;&#32034;&#12290;&#30446;&#26631;&#26159;&#21019;&#24314;&#30495;&#27491;&#24320;&#28304;&#30340;&#26367;&#20195;&#23553;&#38381;&#28304;GPTs&#65292;&#25552;&#39640;&#20154;&#24037;&#26234;&#33021;&#30340;&#24320;&#21457;&#21644;&#21487;&#38752;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.08161</link><description>&lt;p&gt;
h2oGPT&#65306;&#27665;&#20027;&#21270;&#22823;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
h2oGPT: Democratizing Large Language Models. (arXiv:2306.08161v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08161
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;h2oGPT&#65292;&#36825;&#26159;&#19968;&#22871;&#24320;&#28304;&#20195;&#30721;&#24211;&#65292;&#29992;&#20110;&#21019;&#24314;&#21644;&#20351;&#29992;&#22522;&#20110;GPTs&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#21253;&#25324;100&#65285;&#31169;&#26377;&#25991;&#26723;&#25628;&#32034;&#12290;&#30446;&#26631;&#26159;&#21019;&#24314;&#30495;&#27491;&#24320;&#28304;&#30340;&#26367;&#20195;&#23553;&#38381;&#28304;GPTs&#65292;&#25552;&#39640;&#20154;&#24037;&#26234;&#33021;&#30340;&#24320;&#21457;&#21644;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#29983;&#25104;&#39044;&#35757;&#32451;&#21464;&#21387;&#22120;&#65288;GPTs&#65289;&#65292;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;GPT-4&#22240;&#20854;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#38754;&#30340;&#29616;&#23454;&#24212;&#29992;&#32780;&#25104;&#20026;&#20154;&#24037;&#26234;&#33021;&#38761;&#21629;&#30340;&#19968;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20063;&#24102;&#26469;&#20102;&#35768;&#22810;&#37325;&#22823;&#30340;&#39118;&#38505;&#65292;&#22914;&#23384;&#22312;&#26377;&#20559;&#35265;&#12289;&#31169;&#20154;&#25110;&#26377;&#23475;&#25991;&#26412;&#21644;&#26410;&#32463;&#25480;&#26435;&#30340;&#29256;&#26435;&#26448;&#26009;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;h2oGPT&#65292;&#36825;&#26159;&#19968;&#22871;&#24320;&#28304;&#20195;&#30721;&#24211;&#65292;&#29992;&#20110;&#21019;&#24314;&#21644;&#20351;&#29992;&#22522;&#20110;GPTs&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#12290;&#35813;&#39033;&#30446;&#30340;&#30446;&#26631;&#26159;&#21019;&#24314;&#19990;&#30028;&#19978;&#26368;&#22909;&#30340;&#30495;&#27491;&#24320;&#28304;&#30340;&#26367;&#20195;&#23553;&#38381;&#28304;GPTs&#12290;&#19982;&#24320;&#28304;&#31038;&#21306;&#21512;&#20316;&#65292;&#20316;&#20026;&#20854;&#19968;&#37096;&#20998;&#65292;&#25105;&#20204;&#24320;&#28304;&#20102;&#20960;&#20010;LLM&#65292;&#20854;&#21442;&#25968;&#20174;7&#20159;&#21040;400&#20159;&#65292;&#21487;&#22312;&#23436;&#20840;&#33258;&#30001;&#30340;Apache 2.0&#35768;&#21487;&#19979;&#21830;&#29992;&#12290;&#25105;&#20204;&#30340;&#21457;&#24067;&#21253;&#25324;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#30340;100&#65285;&#31169;&#26377;&#25991;&#26723;&#25628;&#32034;&#12290;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#26377;&#21161;&#20110;&#20419;&#36827;&#20154;&#24037;&#26234;&#33021;&#30340;&#21457;&#23637;&#24182;&#20351;&#20854;&#26356;&#21152;&#21487;&#38752;&#12290;
&lt;/p&gt;
&lt;p&gt;
Foundation Large Language Models (LLMs) such as GPT-4 represent a revolution in AI due to their real-world applications though natural language processing. However, they also pose many significant risks such as the presence of biased, private, or harmful text, and the unauthorized inclusion of copyrighted material.  We introduce h2oGPT, a suite of open-source code repositories for the creation and use of Large Language Models (LLMs) based on Generative Pretrained Transformers (GPTs). The goal of this project is to create the world's best truly open-source alternative to closed-source GPTs. In collaboration with and as part of the incredible and unstoppable open-source community, we open-source several fine-tuned h2oGPT models from 7 to 40 Billion parameters, ready for commercial use under fully permissive Apache 2.0 licenses. Included in our release is 100% private document search using natural language.  Open-source language models help boost AI development and make it more accessible
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;209&#31687;&#20851;&#20110;NLP&#27169;&#22411;&#20559;&#35265;&#30340;&#35770;&#25991;&#65292;&#20854;&#20013;&#22823;&#37096;&#20998;&#28041;&#21450;&#31038;&#20250;&#20154;&#21475;&#32479;&#35745;&#20559;&#35265;&#12290;&#30740;&#31350;&#32773;&#25552;&#20986;&#20102;&#31038;&#20250;&#20154;&#21475;&#32479;&#35745;&#20559;&#35265;&#30340;&#23450;&#20041;&#65292;&#24182;&#30830;&#23450;&#20102;NLP&#20559;&#35265;&#30740;&#31350;&#30340;&#19977;&#20010;&#20027;&#35201;&#31867;&#21035;&#12290;&#24403;&#21069;&#21435;&#20559;&#35265;&#25216;&#26415;&#21482;&#26159;&#38544;&#34255;&#20102;&#20559;&#35265;&#32780;&#19981;&#26159;&#30495;&#27491;&#21435;&#38500;&#23427;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2306.08158</link><description>&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#31038;&#20250;&#20154;&#21475;&#32479;&#35745;&#20559;&#35265;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Survey on Sociodemographic Bias in Natural Language Processing. (arXiv:2306.08158v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08158
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;209&#31687;&#20851;&#20110;NLP&#27169;&#22411;&#20559;&#35265;&#30340;&#35770;&#25991;&#65292;&#20854;&#20013;&#22823;&#37096;&#20998;&#28041;&#21450;&#31038;&#20250;&#20154;&#21475;&#32479;&#35745;&#20559;&#35265;&#12290;&#30740;&#31350;&#32773;&#25552;&#20986;&#20102;&#31038;&#20250;&#20154;&#21475;&#32479;&#35745;&#20559;&#35265;&#30340;&#23450;&#20041;&#65292;&#24182;&#30830;&#23450;&#20102;NLP&#20559;&#35265;&#30740;&#31350;&#30340;&#19977;&#20010;&#20027;&#35201;&#31867;&#21035;&#12290;&#24403;&#21069;&#21435;&#20559;&#35265;&#25216;&#26415;&#21482;&#26159;&#38544;&#34255;&#20102;&#20559;&#35265;&#32780;&#19981;&#26159;&#30495;&#27491;&#21435;&#38500;&#23427;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#24448;&#24448;&#20250;&#23398;&#20064;&#21040;&#38750;&#39044;&#26399;&#30340;&#20559;&#35265;&#65292;&#36825;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#21487;&#33021;&#20250;&#20135;&#29983;&#26377;&#23475;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#23545;209&#31687;&#20851;&#20110;NLP&#27169;&#22411;&#20013;&#20559;&#35265;&#30340;&#35770;&#25991;&#36827;&#34892;&#20102;&#35843;&#26597;&#65292;&#20854;&#20013;&#22823;&#37096;&#20998;&#35770;&#25991;&#28041;&#21450;&#31038;&#20250;&#20154;&#21475;&#32479;&#35745;&#20559;&#35265;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#29702;&#35299;&#20559;&#35265;&#19982;&#30495;&#23454;&#19990;&#30028;&#30340;&#21361;&#23475;&#20043;&#38388;&#30340;&#21306;&#21035;&#65292;&#25105;&#20204;&#20511;&#37492;&#24515;&#29702;&#23398;&#21644;&#34892;&#20026;&#32463;&#27982;&#23398;&#30340;&#24605;&#24819;&#65292;&#25552;&#20986;&#20102;&#31038;&#20250;&#20154;&#21475;&#32479;&#35745;&#20559;&#35265;&#30340;&#23450;&#20041;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;NLP&#20559;&#35265;&#30740;&#31350;&#30340;&#19977;&#20010;&#20027;&#35201;&#31867;&#21035;&#65306;&#20559;&#35265;&#31867;&#22411;&#12289;&#37327;&#21270;&#20559;&#35265;&#21644;&#21435;&#20559;&#35265;&#12290;&#25105;&#20204;&#35748;&#20026;&#24403;&#21069;&#23545;&#20110;&#37327;&#21270;&#20559;&#35265;&#30340;&#26041;&#27861;&#23384;&#22312;&#21487;&#38752;&#24615;&#38382;&#39064;&#65292;&#35768;&#22810;&#20559;&#35265;&#24230;&#37327;&#24182;&#19981;&#28041;&#21450;&#30495;&#23454;&#19990;&#30028;&#20013;&#30340;&#20559;&#35265;&#65292;&#24403;&#21069;&#30340;&#21435;&#20559;&#35265;&#25216;&#26415;&#26159;&#34920;&#38754;&#30340;&#65292;&#21482;&#26159;&#38544;&#34255;&#20102;&#20559;&#35265;&#65292;&#32780;&#19981;&#26159;&#30495;&#27491;&#21435;&#38500;&#23427;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#26410;&#26469;&#24037;&#20316;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks often learn unintended biases during training, which might have harmful effects when deployed in real-world settings. This paper surveys 209 papers on bias in NLP models, most of which address sociodemographic bias. To better understand the distinction between bias and real-world harm, we turn to ideas from psychology and behavioral economics to propose a definition for sociodemographic bias. We identify three main categories of NLP bias research: types of bias, quantifying bias, and debiasing. We conclude that current approaches on quantifying bias face reliability issues, that many of the bias metrics do not relate to real-world biases, and that current debiasing techniques are superficial and hide bias rather than removing it. Finally, we provide recommendations for future work.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21160;&#24577;&#36125;&#21494;&#26031;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#26469;&#39044;&#27979;&#21152;&#23494;&#36135;&#24065;&#20215;&#26684;&#26041;&#21521;&#65292;&#20197;&#24110;&#21161;&#25237;&#36164;&#32773;&#20570;&#20986;&#26126;&#26234;&#30340;&#25237;&#36164;&#20915;&#31574;&#12290;</title><link>http://arxiv.org/abs/2306.08157</link><description>&lt;p&gt;
&#20351;&#29992;&#21160;&#24577;&#36125;&#21494;&#26031;&#32593;&#32476;&#36827;&#34892;&#21152;&#23494;&#36135;&#24065;&#20215;&#26684;&#26041;&#21521;&#22240;&#26524;&#29305;&#24449;&#24037;&#31243;
&lt;/p&gt;
&lt;p&gt;
Causal Feature Engineering of Price Directions of Cryptocurrencies using Dynamic Bayesian Networks. (arXiv:2306.08157v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08157
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21160;&#24577;&#36125;&#21494;&#26031;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#26469;&#39044;&#27979;&#21152;&#23494;&#36135;&#24065;&#20215;&#26684;&#26041;&#21521;&#65292;&#20197;&#24110;&#21161;&#25237;&#36164;&#32773;&#20570;&#20986;&#26126;&#26234;&#30340;&#25237;&#36164;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21152;&#23494;&#36135;&#24065;&#22312;&#21508;&#20010;&#39046;&#22495;&#65292;&#29305;&#21035;&#26159;&#37329;&#34701;&#21644;&#25237;&#36164;&#39046;&#22495;&#20013;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#12290;&#20854;&#29420;&#29305;&#30340;&#21306;&#22359;&#38142;&#30456;&#20851;&#29305;&#24615;&#65292;&#22914;&#38544;&#31169;&#12289;&#21435;&#20013;&#24515;&#21270;&#21644;&#19981;&#21487;&#36861;&#36394;&#24615;&#65292;&#37096;&#20998;&#21407;&#22240;&#26159;&#20854;&#21463;&#27426;&#36814;&#30340;&#21407;&#22240;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#21152;&#23494;&#36135;&#24065;&#20215;&#26684;&#30340;&#27874;&#21160;&#24615;&#21644;&#19981;&#30830;&#23450;&#24615;&#65292;&#21152;&#23494;&#36135;&#24065;&#20173;&#28982;&#26159;&#19968;&#31181;&#39640;&#39118;&#38505;&#25237;&#36164;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21160;&#24577;&#36125;&#21494;&#26031;&#32593;&#32476;&#65288;DBN&#65289;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#22810;&#20803;&#35774;&#32622;&#19979;&#27169;&#25311;&#22797;&#26434;&#31995;&#32479;&#65292;&#20197;&#39044;&#27979;&#20116;&#31181;&#27969;&#34892;&#21152;&#23494;&#36135;&#24065;&#30340;&#20215;&#26684;&#36816;&#21160;&#26041;&#21521;&#65292;&#20197;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cryptocurrencies have gained popularity across various sectors, especially in finance and investment. The popularity is partly due to their unique specifications originating from blockchain-related characteristics such as privacy, decentralisation, and untraceability. Despite their growing popularity, cryptocurrencies remain a high-risk investment due to their price volatility and uncertainty. The inherent volatility in cryptocurrency prices, coupled with internal cryptocurrency-related factors and external influential global economic factors makes predicting their prices and price movement directions challenging. Nevertheless, the knowledge obtained from predicting the direction of cryptocurrency prices can provide valuable guidance for investors in making informed investment decisions. To address this issue, this paper proposes a dynamic Bayesian network (DBN) approach, which can model complex systems in multivariate settings, to predict the price movement direction of five popular a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#31070;&#32463;&#28151;&#21512;&#25928;&#24212;&#65288;NME&#65289;&#27169;&#22411;&#65292;&#29992;&#20110;&#20010;&#24615;&#21270;&#39044;&#27979;&#65292;&#24182;&#36890;&#36807;&#32467;&#21512;&#20010;&#20154;&#36890;&#29992;&#21644;&#20010;&#20154;&#29305;&#23450;&#21442;&#25968;&#26469;&#32771;&#34385;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#36235;&#21183;&#12290;</title><link>http://arxiv.org/abs/2306.08149</link><description>&lt;p&gt;
&#38750;&#32447;&#24615;&#20010;&#24615;&#21270;&#39044;&#27979;&#30340;&#31070;&#32463;&#28151;&#21512;&#25928;&#24212;
&lt;/p&gt;
&lt;p&gt;
Neural Mixed Effects for Nonlinear Personalized Predictions. (arXiv:2306.08149v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08149
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#31070;&#32463;&#28151;&#21512;&#25928;&#24212;&#65288;NME&#65289;&#27169;&#22411;&#65292;&#29992;&#20110;&#20010;&#24615;&#21270;&#39044;&#27979;&#65292;&#24182;&#36890;&#36807;&#32467;&#21512;&#20010;&#20154;&#36890;&#29992;&#21644;&#20010;&#20154;&#29305;&#23450;&#21442;&#25968;&#26469;&#32771;&#34385;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#36235;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20010;&#24615;&#21270;&#39044;&#27979;&#26159;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#26681;&#25454;&#36807;&#21435;&#26631;&#35760;&#35266;&#27979;&#39044;&#27979;&#19968;&#20010;&#20154;&#26410;&#26469;&#30340;&#35266;&#27979;&#20540;&#65292;&#36890;&#24120;&#29992;&#20110;&#36830;&#32493;&#20219;&#21153;&#65292;&#20363;&#22914;&#39044;&#27979;&#26085;&#24120;&#24773;&#32490;&#35780;&#20998;&#12290;&#22312;&#36827;&#34892;&#20010;&#24615;&#21270;&#39044;&#27979;&#26102;&#65292;&#27169;&#22411;&#21487;&#20197;&#32467;&#21512;&#20004;&#31181;&#36235;&#21183;&#65306;&#65288;a&#65289;&#36328;&#20154;&#20849;&#20139;&#30340;&#36235;&#21183;&#65292;&#21363;&#20010;&#20154;&#36890;&#29992;&#36235;&#21183;&#65292;&#20363;&#22914;&#21608;&#26411;&#26356;&#24320;&#24515;&#65292;&#21644;&#65288;b&#65289;&#27599;&#20010;&#20154;&#29420;&#29305;&#30340;&#36235;&#21183;&#65292;&#21363;&#20010;&#20154;&#29305;&#23450;&#30340;&#36235;&#21183;&#65292;&#20363;&#22914;&#27599;&#21608;&#26377;&#19968;&#27425;&#21387;&#21147;&#22823;&#30340;&#20250;&#35758;&#12290;&#28151;&#21512;&#25928;&#24212;&#27169;&#22411;&#26159;&#19968;&#31181;&#27969;&#34892;&#30340;&#32479;&#35745;&#27169;&#22411;&#65292;&#29992;&#20110;&#36890;&#36807;&#32452;&#21512;&#20010;&#20154;&#36890;&#29992;&#21644;&#20010;&#20154;&#29305;&#23450;&#21442;&#25968;&#26469;&#30740;&#31350;&#36825;&#20004;&#31181;&#36235;&#21183;&#12290;&#23613;&#31649;&#29616;&#22312;&#32447;&#24615;&#28151;&#21512;&#25928;&#24212;&#27169;&#22411;&#36890;&#36807;&#23558;&#20854;&#19982;&#31070;&#32463;&#32593;&#32476;&#25972;&#21512;&#32780;&#21464;&#24471;&#36234;&#26469;&#36234;&#27969;&#34892;&#65292;&#20294;&#36825;&#31181;&#25972;&#21512;&#30446;&#21069;&#20165;&#38480;&#20110;&#32447;&#24615;&#20010;&#20154;&#29305;&#23450;&#21442;&#25968;&#65306;&#25490;&#38500;&#38750;&#32447;&#24615;&#20010;&#20154;&#29305;&#23450;&#36235;&#21183;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31070;&#32463;&#28151;&#21512;&#25928;&#24212;&#65288;NME&#65289;&#27169;&#22411;&#65292;&#20197;&#20248;&#21270;&#38750;&#32447;&#24615;&#20010;&#20154;&#29305;&#23450;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Personalized prediction is a machine learning approach that predicts a person's future observations based on their past labeled observations and is typically used for sequential tasks, e.g., to predict daily mood ratings. When making personalized predictions, a model can combine two types of trends: (a) trends shared across people, i.e., person-generic trends, such as being happier on weekends, and (b) unique trends for each person, i.e., person-specific trends, such as a stressful weekly meeting. Mixed effect models are popular statistical models to study both trends by combining person-generic and person-specific parameters. Though linear mixed effect models are gaining popularity in machine learning by integrating them with neural networks, these integrations are currently limited to linear person-specific parameters: ruling out nonlinear person-specific trends. In this paper, we propose Neural Mixed Effect (NME) models to optimize nonlinear person-specific parameters anywhere in a 
&lt;/p&gt;</description></item><item><title>&#20026;&#30740;&#31350;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#20132;&#20114;&#65292;&#30740;&#31350;&#32773;&#21019;&#24314;&#20102;ArtWhisperer&#25968;&#25454;&#38598;&#65292;&#36825;&#26159;&#19968;&#20010;&#22312;&#32447;&#28216;&#25103;&#65292;&#20154;&#20204;&#36890;&#36807;&#21453;&#22797;&#23581;&#35797;&#19981;&#21516;&#30340;&#25552;&#31034;&#35789;&#65292;&#26469;&#29983;&#25104;&#21644;&#30446;&#26631;&#22270;&#20687;&#31867;&#20284;&#30340;&#22270;&#20687;&#65292;&#24182;&#35760;&#24405;&#20102;50,000&#22810;&#20010;&#20132;&#20114;&#35760;&#24405;&#12290;&#22312;&#21021;&#27493;&#20998;&#26512;&#20013;&#65292;&#30740;&#31350;&#32773;&#21457;&#29616;&#20154;&#20204;&#25552;&#20132;&#20102;&#21508;&#31181;&#21508;&#26679;&#30340;&#25552;&#31034;&#35789;&#65292;&#24182;&#33021;&#22815;&#21457;&#29616;&#29983;&#25104;&#21508;&#31181;&#25991;&#26412;&#25551;&#36848;&#30340;&#22270;&#20687;&#12290;</title><link>http://arxiv.org/abs/2306.08141</link><description>&lt;p&gt;
ArtWhisperer&#65306;&#19968;&#20010;&#29992;&#20110;&#25551;&#36848;&#33402;&#26415;&#21019;&#20316;&#20013;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#20132;&#20114;&#30340;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
ArtWhisperer: A Dataset for Characterizing Human-AI Interactions in Artistic Creations. (arXiv:2306.08141v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08141
&lt;/p&gt;
&lt;p&gt;
&#20026;&#30740;&#31350;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#20132;&#20114;&#65292;&#30740;&#31350;&#32773;&#21019;&#24314;&#20102;ArtWhisperer&#25968;&#25454;&#38598;&#65292;&#36825;&#26159;&#19968;&#20010;&#22312;&#32447;&#28216;&#25103;&#65292;&#20154;&#20204;&#36890;&#36807;&#21453;&#22797;&#23581;&#35797;&#19981;&#21516;&#30340;&#25552;&#31034;&#35789;&#65292;&#26469;&#29983;&#25104;&#21644;&#30446;&#26631;&#22270;&#20687;&#31867;&#20284;&#30340;&#22270;&#20687;&#65292;&#24182;&#35760;&#24405;&#20102;50,000&#22810;&#20010;&#20132;&#20114;&#35760;&#24405;&#12290;&#22312;&#21021;&#27493;&#20998;&#26512;&#20013;&#65292;&#30740;&#31350;&#32773;&#21457;&#29616;&#20154;&#20204;&#25552;&#20132;&#20102;&#21508;&#31181;&#21508;&#26679;&#30340;&#25552;&#31034;&#35789;&#65292;&#24182;&#33021;&#22815;&#21457;&#29616;&#29983;&#25104;&#21508;&#31181;&#25991;&#26412;&#25551;&#36848;&#30340;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#36234;&#26469;&#36234;&#26222;&#21450;&#65292;&#30740;&#31350;&#20154;&#31867;&#29992;&#25143;&#22914;&#20309;&#19982;&#36825;&#20123;&#27169;&#22411;&#20132;&#20114;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20154;&#20204;&#22914;&#20309;&#20351;&#29992;&#25991;&#26412;&#21040;&#22270;&#20687;&#30340;&#27169;&#22411;&#29983;&#25104;&#25152;&#38656;&#30340;&#30446;&#26631;&#22270;&#20687;&#12290;&#20026;&#20102;&#30740;&#31350;&#36825;&#31181;&#20132;&#20114;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;ArtWhisperer&#65292;&#36825;&#26159;&#19968;&#20010;&#22312;&#32447;&#28216;&#25103;&#65292;&#29992;&#25143;&#20250;&#24471;&#21040;&#19968;&#20010;&#30446;&#26631;&#22270;&#20687;&#65292;&#24182;&#38656;&#35201;&#21453;&#22797;&#23581;&#35797;&#19981;&#21516;&#30340;&#25552;&#31034;&#35789;&#65292;&#20197;&#20415;&#29983;&#25104;&#31867;&#20284;&#30446;&#26631;&#22270;&#20687;&#30340;&#22270;&#20687;&#12290;&#36890;&#36807;&#36825;&#20010;&#28216;&#25103;&#65292;&#25105;&#20204;&#35760;&#24405;&#20102;50,000&#22810;&#20010;&#20154;&#24037;&#26234;&#33021;-&#20154;&#31867;&#20132;&#20114;&#30340;&#35760;&#24405;&#65307;&#27599;&#20010;&#20132;&#20114;&#37117;&#23545;&#24212;&#30528;&#29992;&#25143;&#21019;&#24314;&#30340;&#19968;&#20010;&#25552;&#31034;&#35789;&#21644;&#30456;&#24212;&#29983;&#25104;&#30340;&#22270;&#20687;&#12290;&#22823;&#22810;&#25968;&#35760;&#24405;&#37117;&#26159;&#37325;&#22797;&#30340;&#20132;&#20114;&#65292;&#29992;&#25143;&#36890;&#36807;&#21453;&#22797;&#23581;&#35797;&#25214;&#21040;&#26368;&#20339;&#30340;&#25552;&#31034;&#35789;&#20197;&#29983;&#25104;&#30446;&#26631;&#22270;&#20687;&#65292;&#36825;&#20351;&#24471;&#36825;&#20010;&#25968;&#25454;&#38598;&#25104;&#20026;&#30740;&#31350;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#21327;&#20316;&#30340;&#29420;&#29305;&#36830;&#32493;&#25968;&#25454;&#38598;&#12290;&#22312;&#23545;&#36825;&#20010;&#25968;&#25454;&#38598;&#30340;&#21021;&#27493;&#20998;&#26512;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#19968;&#20123;&#25552;&#31034;&#35789;&#20132;&#20114;&#21644;&#29992;&#25143;&#31574;&#30053;&#30340;&#29305;&#24449;&#12290;&#20154;&#20204;&#25552;&#20132;&#20102;&#21508;&#31181;&#21508;&#26679;&#30340;&#25552;&#31034;&#35789;&#65292;&#24182;&#33021;&#22815;&#21457;&#29616;&#29983;&#25104;&#21508;&#31181;&#25991;&#26412;&#25551;&#36848;&#30340;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;
As generative AI becomes more prevalent, it is important to study how human users interact with such models. In this work, we investigate how people use text-to-image models to generate desired target images. To study this interaction, we created ArtWhisperer, an online game where users are given a target image and are tasked with iteratively finding a prompt that creates a similar-looking image as the target. Through this game, we recorded over 50,000 human-AI interactions; each interaction corresponds to one text prompt created by a user and the corresponding generated image. The majority of these are repeated interactions where a user iterates to find the best prompt for their target image, making this a unique sequential dataset for studying human-AI collaborations. In an initial analysis of this dataset, we identify several characteristics of prompt interactions and user strategies. People submit diverse prompts and are able to discover a variety of text descriptions that generate
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#20027;&#20449;&#24687;&#26816;&#32034;&#35270;&#35273;&#38382;&#31572;&#26694;&#26550;AVIS&#65292;&#21487;&#20197;&#35299;&#20915;&#35270;&#35273;&#38382;&#39064;&#25152;&#38656;&#30340;&#22806;&#37096;&#30693;&#35782;&#33719;&#21462;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.08129</link><description>&lt;p&gt;
AVIS:&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#20027;&#35270;&#35273;&#20449;&#24687;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
AVIS: Autonomous Visual Information Seeking with Large Language Models. (arXiv:2306.08129v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08129
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#20027;&#20449;&#24687;&#26816;&#32034;&#35270;&#35273;&#38382;&#31572;&#26694;&#26550;AVIS&#65292;&#21487;&#20197;&#35299;&#20915;&#35270;&#35273;&#38382;&#39064;&#25152;&#38656;&#30340;&#22806;&#37096;&#30693;&#35782;&#33719;&#21462;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23454;&#29616;&#33258;&#20027;&#20449;&#24687;&#26816;&#32034;&#30340;&#35270;&#35273;&#38382;&#31572;&#26694;&#26550;AVIS&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;LLM&#21160;&#24577;&#22320;&#21046;&#23450;&#21033;&#29992;&#22806;&#37096;&#24037;&#20855;&#30340;&#31574;&#30053;&#65292;&#24182;&#35843;&#26597;&#23427;&#20204;&#30340;&#36755;&#20986;&#65292;&#20174;&#32780;&#33719;&#21462;&#25552;&#20379;&#25152;&#25552;&#20986;&#38382;&#39064;&#25152;&#38656;&#30340;&#19981;&#21487;&#25110;&#32570;&#30340;&#30693;&#35782;&#12290;&#22238;&#31572;&#38656;&#35201;&#22806;&#37096;&#30693;&#35782;&#30340;&#35270;&#35273;&#38382;&#39064;&#65292;&#22914;&#8220;&#36825;&#24133;&#22270;&#20687;&#25152;&#25551;&#32472;&#30340;&#24314;&#31569;&#29289;&#26159;&#20026;&#20102;&#32426;&#24565;&#21738;&#20010;&#20107;&#20214;&#65311;&#8221;&#65292;&#26159;&#19968;&#39033;&#22797;&#26434;&#30340;&#20219;&#21153;&#12290;&#36825;&#20010;&#20219;&#21153;&#21576;&#29616;&#20986;&#19968;&#20010;&#32452;&#21512;&#25628;&#32034;&#31354;&#38388;&#65292;&#38656;&#35201;&#19968;&#31995;&#21015;&#34892;&#21160;&#65292;&#21253;&#25324;&#35843;&#29992;API&#12289;&#20998;&#26512;&#23427;&#20204;&#30340;&#21709;&#24212;&#24182;&#20570;&#20986;&#26126;&#26234;&#30340;&#20915;&#31574;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#20010;&#29992;&#25143;&#30740;&#31350;&#65292;&#25910;&#38598;&#20102;&#20154;&#31867;&#38754;&#23545;&#36825;&#20010;&#20219;&#21153;&#26102;&#21508;&#31181;&#21508;&#26679;&#30340;&#20915;&#31574;&#23454;&#20363;&#12290;&#28982;&#21518;&#21033;&#29992;&#36825;&#20123;&#25968;&#25454;&#35774;&#35745;&#20102;&#19968;&#20010;&#30001;&#19977;&#20010;&#32452;&#20214;&#32452;&#25104;&#30340;&#31995;&#32479;&#65306;&#19968;&#20010;&#30001;LLM&#39537;&#21160;&#30340;&#35268;&#21010;&#22120;&#65292;&#21160;&#24577;&#30830;&#23450;&#19979;&#19968;&#20010;&#35201;&#20351;&#29992;&#30340;&#24037;&#20855;&#65307;&#19968;&#20010;&#30001;LLM&#39537;&#21160;&#30340;&#25512;&#29702;&#22120;&#65292;&#20998;&#26512;&#24182;&#25552;&#21462;&#20851;&#38190;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose an autonomous information seeking visual question answering framework, AVIS. Our method leverages a Large Language Model (LLM) to dynamically strategize the utilization of external tools and to investigate their outputs, thereby acquiring the indispensable knowledge needed to provide answers to the posed questions. Responding to visual questions that necessitate external knowledge, such as "What event is commemorated by the building depicted in this image?", is a complex task. This task presents a combinatorial search space that demands a sequence of actions, including invoking APIs, analyzing their responses, and making informed decisions. We conduct a user study to collect a variety of instances of human decision-making when faced with this task. This data is then used to design a system comprised of three components: an LLM-powered planner that dynamically determines which tool to use next, an LLM-powered reasoner that analyzes and extracts key information 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;PersonaPKT&#65292;&#19968;&#31181;&#36731;&#37327;&#32423;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#27809;&#26377;&#26126;&#30830;&#20010;&#24615;&#25551;&#36848;&#30340;&#24773;&#20917;&#19979;&#26500;&#24314;&#31526;&#21512;&#35282;&#33394;&#30340;&#23545;&#35805;&#27169;&#22411;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#23558;&#27599;&#20010;&#20010;&#24615;&#34920;&#31034;&#20026;&#19968;&#20010;&#36830;&#32493;&#21521;&#37327;&#65292;&#30452;&#25509;&#20174;&#21516;&#19968;&#35282;&#33394;&#20135;&#29983;&#30340;&#23569;&#37327;&#23545;&#35805;&#26679;&#26412;&#20013;&#23398;&#20064;&#38544;&#21547;&#30340;&#20010;&#24615;&#29305;&#23450;&#29305;&#24449;&#65292;&#24182;&#33021;&#22815;&#39640;&#25928;&#26500;&#24314;&#20010;&#24615;&#21270;&#23545;&#35805;&#20195;&#29702;&#24182;&#22686;&#24378;&#38544;&#31169;&#20445;&#25252;&#12290;</title><link>http://arxiv.org/abs/2306.08126</link><description>&lt;p&gt;
PersonaPKT&#65306;&#36890;&#36807;&#21442;&#25968;&#39640;&#25928;&#30340;&#30693;&#35782;&#36801;&#31227;&#26500;&#24314;&#20010;&#24615;&#21270;&#23545;&#35805;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
PersonaPKT: Building Personalized Dialogue Agents via Parameter-efficient Knowledge Transfer. (arXiv:2306.08126v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08126
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;PersonaPKT&#65292;&#19968;&#31181;&#36731;&#37327;&#32423;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#27809;&#26377;&#26126;&#30830;&#20010;&#24615;&#25551;&#36848;&#30340;&#24773;&#20917;&#19979;&#26500;&#24314;&#31526;&#21512;&#35282;&#33394;&#30340;&#23545;&#35805;&#27169;&#22411;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#23558;&#27599;&#20010;&#20010;&#24615;&#34920;&#31034;&#20026;&#19968;&#20010;&#36830;&#32493;&#21521;&#37327;&#65292;&#30452;&#25509;&#20174;&#21516;&#19968;&#35282;&#33394;&#20135;&#29983;&#30340;&#23569;&#37327;&#23545;&#35805;&#26679;&#26412;&#20013;&#23398;&#20064;&#38544;&#21547;&#30340;&#20010;&#24615;&#29305;&#23450;&#29305;&#24449;&#65292;&#24182;&#33021;&#22815;&#39640;&#25928;&#26500;&#24314;&#20010;&#24615;&#21270;&#23545;&#35805;&#20195;&#29702;&#24182;&#22686;&#24378;&#38544;&#31169;&#20445;&#25252;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#39537;&#21160;&#30340;&#20010;&#24615;&#21270;&#23545;&#35805;&#20195;&#29702;&#65288;DA&#65289;&#36890;&#24120;&#20381;&#36182;&#20110;&#26126;&#30830;&#30340;&#35282;&#33394;&#25551;&#36848;&#26469;&#20445;&#25345;&#20010;&#24615;&#30340;&#19968;&#33268;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25551;&#36848;&#21487;&#33021;&#24182;&#19981;&#24635;&#26159;&#21487;&#29992;&#25110;&#21487;&#33021;&#23384;&#22312;&#38544;&#31169;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;PersonaPKT&#65292;&#36825;&#26159;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#27809;&#26377;&#26126;&#30830;&#20010;&#24615;&#25551;&#36848;&#30340;&#24773;&#20917;&#19979;&#26500;&#24314;&#31526;&#21512;&#35282;&#33394;&#30340;&#23545;&#35805;&#27169;&#22411;&#12290;&#36890;&#36807;&#23558;&#27599;&#20010;&#20010;&#24615;&#34920;&#31034;&#20026;&#19968;&#20010;&#36830;&#32493;&#21521;&#37327;&#65292;PersonaPKT&#30452;&#25509;&#20174;&#21516;&#19968;&#35282;&#33394;&#20135;&#29983;&#30340;&#23569;&#37327;&#23545;&#35805;&#26679;&#26412;&#20013;&#23398;&#20064;&#38544;&#21547;&#30340;&#20010;&#24615;&#29305;&#23450;&#29305;&#24449;&#65292;&#24182;&#22312;PLM&#39592;&#24178;&#19978;&#22686;&#21152;&#23569;&#20110;0.1&#65285;&#30340;&#21487;&#35757;&#32451;&#21442;&#25968;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;PersonaPKT&#21487;&#20197;&#39640;&#25928;&#22320;&#26500;&#24314;&#20010;&#24615;&#21270;DA&#65292;&#24182;&#22312;&#20445;&#25345;&#33391;&#22909;&#30340;&#21709;&#24212;&#29983;&#25104;&#36136;&#37327;&#30340;&#21516;&#26102;&#65292;&#22312;&#35282;&#33394;&#30340;&#19968;&#33268;&#24615;&#26041;&#38754;&#20248;&#20110;&#21508;&#31181;&#22522;&#32447;&#12290;&#27492;&#22806;&#65292;&#23427;&#36890;&#36807;&#36991;&#20813;&#26126;&#30830;&#30340;&#20010;&#24615;&#25551;&#36848;&#26469;&#22686;&#24378;&#38544;&#31169;&#20445;&#25252;&#12290;
&lt;/p&gt;
&lt;p&gt;
Personalized dialogue agents (DAs) powered by large pre-trained language models (PLMs) often rely on explicit persona descriptions to maintain personality consistency. However, such descriptions may not always be available or may pose privacy concerns. To tackle this bottleneck, we introduce PersonaPKT, a lightweight transfer learning approach that can build persona-consistent dialogue models without explicit persona descriptions. By representing each persona as a continuous vector, PersonaPKT learns implicit persona-specific features directly from a small number of dialogue samples produced by the same persona, adding less than 0.1% trainable parameters for each persona on top of the PLM backbone. Empirical results demonstrate that PersonaPKT effectively builds personalized DAs with high storage efficiency, outperforming various baselines in terms of persona consistency while maintaining good response generation quality. In addition, it enhances privacy protection by avoiding explicit
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#27861;&#65292;&#21487;&#23545;&#23398;&#26415;&#20889;&#20316;&#20013;&#30340;&#25220;&#34989;&#34892;&#20026;&#36827;&#34892;&#26377;&#25928;&#26816;&#27979;&#65292;&#19981;&#20165;&#22312;&#21477;&#23376;&#32423;&#21035;&#19978;&#36827;&#34892;&#35780;&#20272;&#65292;&#36824;&#22312;&#25991;&#26723;&#32423;&#21035;&#19978;&#25552;&#20379;&#21487;&#37327;&#21270;&#25351;&#26631;&#12290;&#35813;&#26041;&#27861;&#30340;&#20934;&#30830;&#29575;&#39640;&#36798;94&#65285;&#65292;&#20855;&#26377;&#36739;&#24378;&#30340;&#36866;&#24212;&#24615;&#21644;&#21487;&#38752;&#24615;&#65292;&#33021;&#22815;&#19981;&#26029;&#38543;&#30528;LLM&#25216;&#26415;&#30340;&#21457;&#23637;&#32780;&#19981;&#26029;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2306.08122</link><description>&lt;p&gt;
&#20174;&#21477;&#23376;&#21040;&#25991;&#26723;&#23618;&#38754;&#30340;AI&#29983;&#25104;&#25220;&#34989;&#26816;&#27979;&#65306;&#36229;&#36234;&#40657;&#21283;&#23376;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Beyond Black Box AI-Generated Plagiarism Detection: From Sentence to Document Level. (arXiv:2306.08122v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08122
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#27861;&#65292;&#21487;&#23545;&#23398;&#26415;&#20889;&#20316;&#20013;&#30340;&#25220;&#34989;&#34892;&#20026;&#36827;&#34892;&#26377;&#25928;&#26816;&#27979;&#65292;&#19981;&#20165;&#22312;&#21477;&#23376;&#32423;&#21035;&#19978;&#36827;&#34892;&#35780;&#20272;&#65292;&#36824;&#22312;&#25991;&#26723;&#32423;&#21035;&#19978;&#25552;&#20379;&#21487;&#37327;&#21270;&#25351;&#26631;&#12290;&#35813;&#26041;&#27861;&#30340;&#20934;&#30830;&#29575;&#39640;&#36798;94&#65285;&#65292;&#20855;&#26377;&#36739;&#24378;&#30340;&#36866;&#24212;&#24615;&#21644;&#21487;&#38752;&#24615;&#65292;&#33021;&#22815;&#19981;&#26029;&#38543;&#30528;LLM&#25216;&#26415;&#30340;&#21457;&#23637;&#32780;&#19981;&#26029;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#26415;&#20889;&#20316;&#20013;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36234;&#26469;&#36234;&#20381;&#36182;&#23548;&#33268;&#20102;&#25220;&#34989;&#29616;&#35937;&#30340;&#22686;&#21152;&#12290;&#29616;&#26377;&#30340;AI&#29983;&#25104;&#25991;&#26412;&#20998;&#31867;&#22120;&#20934;&#30830;&#24615;&#26377;&#38480;&#65292;&#24448;&#24448;&#20135;&#29983;&#35823;&#25253;&#12290;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#25216;&#26415;&#65292;&#25552;&#20379;&#21477;&#23376;&#21644;&#25991;&#26723;&#32423;&#21035;&#30340;&#21487;&#37327;&#21270;&#25351;&#26631;&#65292;&#26041;&#20415;&#20154;&#31867;&#35780;&#20272;&#32773;&#36827;&#34892;&#35299;&#37322;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#37319;&#29992;&#22810;&#26041;&#38754;&#30340;&#26041;&#27861;&#65292;&#29983;&#25104;&#32473;&#23450;&#38382;&#39064;&#30340;&#22810;&#20010;&#37322;&#20041;&#29256;&#26412;&#65292;&#23558;&#23427;&#20204;&#36755;&#20837;LLM&#20197;&#29983;&#25104;&#31572;&#26696;&#12290;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#20313;&#24358;&#30456;&#20284;&#24230;&#30340;&#23545;&#27604;&#25439;&#22833;&#20989;&#25968;&#65292;&#23558;&#29983;&#25104;&#30340;&#21477;&#23376;&#19982;&#23398;&#29983;&#22238;&#31572;&#20013;&#30340;&#21477;&#23376;&#21305;&#37197;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20998;&#31867;&#20154;&#31867;&#21644;AI&#25991;&#26412;&#26041;&#38754;&#30340;&#20934;&#30830;&#24615;&#36798;&#21040;&#20102;94&#65285;&#65292;&#20026;&#23398;&#26415;&#29615;&#22659;&#20013;&#30340;&#25220;&#34989;&#26816;&#27979;&#25552;&#20379;&#20102;&#24378;&#22823;&#32780;&#36866;&#24212;&#24615;&#24378;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#35813;&#26041;&#27861;&#38543;&#30528;LLM&#25216;&#26415;&#30340;&#21457;&#23637;&#32780;&#19981;&#26029;&#25913;&#36827;&#65292;&#20943;&#23569;&#20102;&#26032;&#27169;&#22411;&#35757;&#32451;&#25110;&#37325;&#26032;&#37197;&#32622;&#30340;&#38656;&#27714;&#65292;&#24182;&#25552;&#20379;&#20102;&#27604;&#20256;&#32479;&#40657;&#21283;&#23376;&#26041;&#27861;&#26356;&#36879;&#26126;&#30340;&#35780;&#20272;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
The increasing reliance on large language models (LLMs) in academic writing has led to a rise in plagiarism. Existing AI-generated text classifiers have limited accuracy and often produce false positives. We propose a novel approach using natural language processing (NLP) techniques, offering quantifiable metrics at both sentence and document levels for easier interpretation by human evaluators. Our method employs a multi-faceted approach, generating multiple paraphrased versions of a given question and inputting them into the LLM to generate answers. By using a contrastive loss function based on cosine similarity, we match generated sentences with those from the student's response. Our approach achieves up to 94% accuracy in classifying human and AI text, providing a robust and adaptable solution for plagiarism detection in academic settings. This method improves with LLM advancements, reducing the need for new model training or reconfiguration, and offers a more transparent way of ev
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#25506;&#35752;&#20351;&#29992; ChatGPT &#35299;&#20915;&#28151;&#21512;&#20132;&#36890;&#27969;&#25511;&#21046;&#38382;&#39064;&#65292;&#36890;&#36807;&#22823;&#35268;&#27169;&#29992;&#25143;&#30740;&#31350;&#21457;&#29616; ChatGPT &#22312;&#26576;&#20123;&#29615;&#22659;&#19979;&#33021;&#22815;&#25552;&#39640;&#25104;&#21151;&#31574;&#30053;&#25968;&#37327;</title><link>http://arxiv.org/abs/2306.08094</link><description>&lt;p&gt;
&#33021;&#21542;&#36890;&#36807; ChatGPT &#23454;&#29616;&#26234;&#33021;&#20132;&#36890;&#31995;&#32479;&#65311;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#23454;&#29616;&#28151;&#21512;&#20132;&#36890;&#27969;&#25511;&#21046;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Can ChatGPT Enable ITS? The Case of Mixed Traffic Control via Reinforcement Learning. (arXiv:2306.08094v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08094
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#25506;&#35752;&#20351;&#29992; ChatGPT &#35299;&#20915;&#28151;&#21512;&#20132;&#36890;&#27969;&#25511;&#21046;&#38382;&#39064;&#65292;&#36890;&#36807;&#22823;&#35268;&#27169;&#29992;&#25143;&#30740;&#31350;&#21457;&#29616; ChatGPT &#22312;&#26576;&#20123;&#29615;&#22659;&#19979;&#33021;&#22815;&#25552;&#39640;&#25104;&#21151;&#31574;&#30053;&#25968;&#37327;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#22312;&#26234;&#33021;&#20132;&#36890;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;&#19981;&#26029;&#22686;&#22810;&#65292;&#21516;&#26102;&#20063;&#20984;&#26174;&#20102;&#19968;&#20123;&#20851;&#38190;&#38382;&#39064;&#12290;&#26412;&#25991;&#30740;&#31350;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; ChatGPT &#30740;&#31350;&#26159;&#21542;&#21487;&#20197;&#24110;&#21161;&#35299;&#20915;&#22797;&#26434;&#30340;&#28151;&#21512;&#20132;&#36890;&#27969;&#25511;&#21046;&#38382;&#39064;&#65292;&#36890;&#36807;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#29992;&#25143;&#30740;&#31350;&#65292;&#21457;&#29616; ChatGPT &#22312;&#26576;&#20123;&#29615;&#22659;&#19979;&#33021;&#22815;&#22686;&#21152;&#25104;&#21151;&#31574;&#30053;&#25968;&#37327;
&lt;/p&gt;
&lt;p&gt;
The surge in Reinforcement Learning (RL) applications in Intelligent Transportation Systems (ITS) has contributed to its growth as well as highlighted key challenges. However, defining objectives of RL agents in traffic control and management tasks, as well as aligning policies with these goals through an effective formulation of Markov Decision Process (MDP), can be challenging and often require domain experts in both RL and ITS. Recent advancements in Large Language Models (LLMs) such as GPT-4 highlight their broad general knowledge, reasoning capabilities, and commonsense priors across various domains. In this work, we conduct a large-scale user study involving 70 participants to investigate whether novices can leverage ChatGPT to solve complex mixed traffic control problems. Three environments are tested, including ring road, bottleneck, and intersection. We find ChatGPT has mixed results. For intersection and bottleneck, ChatGPT increases number of successful policies by 150% and 
&lt;/p&gt;</description></item><item><title>DORSal&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#29289;&#20307;&#20013;&#24515;&#22330;&#26223;&#34920;&#31034;&#26041;&#27861;&#65292;&#21487;&#20197;&#21576;&#29616;&#39640;&#20445;&#30495;&#26032;&#35270;&#22270;&#65292;&#24182;&#22312;&#36739;&#22823;&#31243;&#24230;&#19978;&#20445;&#30041;&#20102;&#35832;&#22914;&#22522;&#20110;&#29289;&#20307;&#30340;&#22330;&#26223;&#32534;&#36753;&#20043;&#31867;&#30340;&#20248;&#28857;&#12290;</title><link>http://arxiv.org/abs/2306.08068</link><description>&lt;p&gt;
DORSal: &#22522;&#20110;&#25193;&#25955;&#30340;&#29289;&#20307;&#20013;&#24515;&#22330;&#26223;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
DORSal: Diffusion for Object-centric Representations of Scenes $\textit{et al.}$. (arXiv:2306.08068v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08068
&lt;/p&gt;
&lt;p&gt;
DORSal&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#29289;&#20307;&#20013;&#24515;&#22330;&#26223;&#34920;&#31034;&#26041;&#27861;&#65292;&#21487;&#20197;&#21576;&#29616;&#39640;&#20445;&#30495;&#26032;&#35270;&#22270;&#65292;&#24182;&#22312;&#36739;&#22823;&#31243;&#24230;&#19978;&#20445;&#30041;&#20102;&#35832;&#22914;&#22522;&#20110;&#29289;&#20307;&#30340;&#22330;&#26223;&#32534;&#36753;&#20043;&#31867;&#30340;&#20248;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#19977;&#32500;&#22330;&#26223;&#29702;&#35299;&#26041;&#38754;&#21462;&#24471;&#30340;&#36827;&#23637;&#20351;&#36328;&#22823;&#37327;&#19981;&#21516;&#22330;&#26223;&#30340;&#25968;&#25454;&#38598;&#30340;&#21487;&#25193;&#23637;&#34920;&#31034;&#23398;&#20064;&#25104;&#20026;&#21487;&#33021;&#12290;&#22240;&#27492;&#65292;&#23545;&#20110;&#26410;&#35265;&#36807;&#30340;&#22330;&#26223;&#21644;&#29289;&#20307;&#30340;&#27867;&#21270;&#65292;&#20165;&#36890;&#36807;&#21333;&#20010;&#25110;&#23569;&#25968;&#22270;&#20687;&#28210;&#26579;&#26032;&#35270;&#22270;&#65292;&#20197;&#21450;&#25903;&#25345;&#32534;&#36753;&#30340;&#21487;&#25511;&#22330;&#26223;&#29983;&#25104;&#29616;&#22312;&#25104;&#20026;&#21487;&#33021;&#12290;&#28982;&#32780;&#65292;&#32852;&#21512;&#35757;&#32451;&#22823;&#37327;&#22330;&#26223;&#36890;&#24120;&#20250;&#22312;&#28210;&#26579;&#36136;&#37327;&#19978;&#22949;&#21327;&#65292;&#32780;&#19982;&#21333;&#20010;&#22330;&#26223;&#20248;&#21270;&#27169;&#22411;&#65288;&#22914;NeRF&#65289;&#30456;&#27604;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#26368;&#36817;&#25193;&#25955;&#27169;&#22411;&#30340;&#36827;&#23637;&#65292;&#20351;&#19977;&#32500;&#22330;&#26223;&#34920;&#31034;&#23398;&#20064;&#27169;&#22411;&#20855;&#22791;&#21576;&#29616;&#39640;&#20445;&#30495;&#26032;&#35270;&#22270;&#30340;&#33021;&#21147;&#65292;&#21516;&#26102;&#22312;&#36739;&#22823;&#31243;&#24230;&#19978;&#20445;&#30041;&#20102;&#35832;&#22914;&#22522;&#20110;&#29289;&#20307;&#30340;&#22330;&#26223;&#32534;&#36753;&#20043;&#31867;&#30340;&#20248;&#28857;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DORSal&#65292;&#23427;&#22522;&#20110;&#25193;&#25955;&#35270;&#39057;&#26550;&#26500;&#65292;&#20026;&#22522;&#20110;&#29289;&#20307;&#20013;&#24515;&#30340;&#22330;&#26223;&#25554;&#27133;&#34920;&#31034;&#30340;&#19977;&#32500;&#22330;&#26223;&#29983;&#25104;&#25552;&#20379;&#36866;&#24212;&#24615;&#12290;&#25105;&#20204;&#22312;&#22797;&#26434;&#30340;&#21512;&#25104;&#22810;&#29289;&#20307;&#22330;&#26223;&#21644;&#29616;&#23454;&#19990;&#30028;&#22823;&#35268;&#27169;&#34903;&#26223;&#25968;&#25454;&#38598;&#19978;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#22330;&#26223;&#26032;&#35270;&#22270;&#65292;&#21516;&#26102;&#25903;&#25345;&#29289;&#20307;&#32423;&#21035;&#30340;&#32534;&#36753;&#65292;&#24182;&#20445;&#30041;&#32454;&#31890;&#24230;&#30340;&#32441;&#29702;&#21644;&#21453;&#23556;&#31561;&#32454;&#33410;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent progress in 3D scene understanding enables scalable learning of representations across large datasets of diverse scenes. As a consequence, generalization to unseen scenes and objects, rendering novel views from just a single or a handful of input images, and controllable scene generation that supports editing, is now possible. However, training jointly on a large number of scenes typically compromises rendering quality when compared to single-scene optimized models such as NeRFs. In this paper, we leverage recent progress in diffusion models to equip 3D scene representation learning models with the ability to render high-fidelity novel views, while retaining benefits such as object-level scene editing to a large degree. In particular, we propose DORSal, which adapts a video diffusion architecture for 3D scene generation conditioned on object-centric slot-based representations of scenes. On both complex synthetic multi-object scenes and on the real-world large-scale Street View d
&lt;/p&gt;</description></item><item><title>CVGP&#26159;&#19968;&#31181;&#36890;&#36807;&#25511;&#21046;&#21464;&#37327;&#23454;&#39564;&#35774;&#35745;&#21152;&#24555;&#31526;&#21495;&#34920;&#36798;&#24335;&#21457;&#29616;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#23398;&#20064;&#22797;&#26434;&#31526;&#21495;&#34920;&#36798;&#24335;&#65292;&#25193;&#23637;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.08057</link><description>&lt;p&gt;
&#36890;&#36807;&#25511;&#21046;&#21464;&#37327;&#22522;&#22240;&#34920;&#36798;&#24335;&#32534;&#31243;&#36827;&#34892;&#31526;&#21495;&#22238;&#24402;
&lt;/p&gt;
&lt;p&gt;
Symbolic Regression via Control Variable Genetic Programming. (arXiv:2306.08057v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08057
&lt;/p&gt;
&lt;p&gt;
CVGP&#26159;&#19968;&#31181;&#36890;&#36807;&#25511;&#21046;&#21464;&#37327;&#23454;&#39564;&#35774;&#35745;&#21152;&#24555;&#31526;&#21495;&#34920;&#36798;&#24335;&#21457;&#29616;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#23398;&#20064;&#22797;&#26434;&#31526;&#21495;&#34920;&#36798;&#24335;&#65292;&#25193;&#23637;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30452;&#25509;&#20174;&#23454;&#39564;&#25968;&#25454;&#20013;&#23398;&#20064;&#31526;&#21495;&#34920;&#36798;&#24335;&#26159;&#20154;&#24037;&#26234;&#33021;&#39537;&#21160;&#30340;&#31185;&#23398;&#21457;&#29616;&#30340;&#37325;&#35201;&#27493;&#39588;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#20165;&#38480;&#20110;&#23398;&#20064;&#31616;&#21333;&#30340;&#34920;&#36798;&#24335;&#12290;&#22238;&#24402;&#28041;&#21450;&#35768;&#22810;&#33258;&#21464;&#37327;&#30340;&#34920;&#36798;&#24335;&#20173;&#28982;&#38590;&#20197;&#23454;&#29616;&#12290;&#21463;&#31185;&#23398;&#30028;&#24191;&#27867;&#20351;&#29992;&#30340;&#25511;&#21046;&#21464;&#37327;&#23454;&#39564;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25511;&#21046;&#21464;&#37327;&#30340;&#22522;&#22240;&#34920;&#36798;&#24335;&#32534;&#31243;&#65288;CVGP&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#22810;&#20010;&#33258;&#21464;&#37327;&#30340;&#31526;&#21495;&#22238;&#24402;&#12290;CVGP&#36890;&#36807;&#23450;&#21046;&#23454;&#39564;&#35774;&#35745;&#65292;&#32780;&#19981;&#26159;&#20174;&#39044;&#20808;&#25910;&#38598;&#30340;&#22266;&#23450;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#65292;&#21152;&#24555;&#20102;&#31526;&#21495;&#34920;&#36798;&#24335;&#30340;&#21457;&#29616;&#36807;&#31243;&#12290;&#23427;&#39318;&#20808;&#20351;&#29992;&#22522;&#22240;&#34920;&#36798;&#24335;&#32534;&#31243;&#25311;&#21512;&#28041;&#21450;&#23569;&#37327;&#33258;&#21464;&#37327;&#30340;&#31616;&#21333;&#34920;&#36798;&#24335;&#65292;&#22312;&#25511;&#21046;&#21464;&#37327;&#23454;&#39564;&#20013;&#65292;&#20854;&#20013;&#20854;&#20182;&#21464;&#37327;&#34987;&#20445;&#25345;&#20026;&#24120;&#37327;&#12290;&#28982;&#21518;&#36890;&#36807;&#22686;&#21152;&#26032;&#30340;&#33258;&#21464;&#37327;&#25193;&#23637;&#20197;&#21069;&#23398;&#20064;&#21040;&#30340;&#34920;&#36798;&#24335;&#65292;&#20351;&#29992;&#26032;&#30340;&#25511;&#21046;&#21464;&#37327;&#23454;&#39564;&#65292;&#22312;&#36825;&#20123;&#23454;&#39564;&#20013;&#65292;&#36825;&#20123;&#21464;&#37327;&#34987;&#20801;&#35768;&#21464;&#21270;&#12290;&#29702;&#35770;&#19978;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;CVGP&#21487;&#20197;&#26377;&#25928;&#22320;&#25506;&#32034;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#30340;&#31354;&#38388;&#65292;&#24182;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;CVGP&#33021;&#22815;&#23398;&#20064;&#28041;&#21450;&#35768;&#22810;&#33258;&#21464;&#37327;&#30340;&#22797;&#26434;&#31526;&#21495;&#34920;&#36798;&#24335;&#65292;&#32780;&#29616;&#26377;&#30340;&#26041;&#27861;&#26080;&#27861;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning symbolic expressions directly from experiment data is a vital step in AI-driven scientific discovery. Nevertheless, state-of-the-art approaches are limited to learning simple expressions. Regressing expressions involving many independent variables still remain out of reach. Motivated by the control variable experiments widely utilized in science, we propose Control Variable Genetic Programming (CVGP) for symbolic regression over many independent variables. CVGP expedites symbolic expression discovery via customized experiment design, rather than learning from a fixed dataset collected a priori. CVGP starts by fitting simple expressions involving a small set of independent variables using genetic programming, under controlled experiments where other variables are held as constants. It then extends expressions learned in previous generations by adding new independent variables, using new control variable experiments in which these variables are allowed to vary. Theoretically, we
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#22810;&#20010;&#23398;&#31185;&#30340;&#35282;&#24230;&#23545;&#20998;&#24067;&#24335;&#20449;&#20219;&#27010;&#24565;&#36827;&#34892;&#27010;&#36848;&#65292;&#24182;&#25506;&#35752;&#30001;&#20998;&#24067;&#24335;&#20449;&#20219;&#25216;&#26415;&#23454;&#29616;&#30340;&#20449;&#20219;&#37325;&#20998;&#37197;/&#36716;&#31227;&#21450;&#30456;&#20851;&#30340;&#31995;&#32479;&#19982;&#24212;&#29992;&#30340;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2306.08056</link><description>&lt;p&gt;
&#22522;&#20110;&#36719;&#20214;&#26550;&#26500;&#35270;&#35282;&#30340;&#20998;&#24067;&#24335;&#20449;&#20219;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Distributed Trust Through the Lens of Software Architecture. (arXiv:2306.08056v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08056
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#22810;&#20010;&#23398;&#31185;&#30340;&#35282;&#24230;&#23545;&#20998;&#24067;&#24335;&#20449;&#20219;&#27010;&#24565;&#36827;&#34892;&#27010;&#36848;&#65292;&#24182;&#25506;&#35752;&#30001;&#20998;&#24067;&#24335;&#20449;&#20219;&#25216;&#26415;&#23454;&#29616;&#30340;&#20449;&#20219;&#37325;&#20998;&#37197;/&#36716;&#31227;&#21450;&#30456;&#20851;&#30340;&#31995;&#32479;&#19982;&#24212;&#29992;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#24067;&#24335;&#20449;&#20219;&#26159;&#19968;&#20010;&#36817;&#24180;&#26469;&#20174;&#19981;&#21516;&#35282;&#24230;&#28436;&#21270;&#20986;&#30340;&#27169;&#31946;&#27010;&#24565;&#12290;&#34429;&#28982;&#30446;&#21069;&#20154;&#20204;&#26222;&#36941;&#23558;&#20854;&#24402;&#21151;&#20110;&#21306;&#22359;&#38142;&#21644;&#21152;&#23494;&#36135;&#24065;&#65292;&#20294;&#20998;&#24067;&#24335;&#20449;&#20219;&#30340;&#27010;&#24565;&#27491;&#22312;&#29983;&#24577;&#31995;&#32479;&#20013;&#25512;&#21160;&#32852;&#37030;&#23398;&#20064;&#12289;&#21487;&#20449;&#21644;&#36127;&#36131;&#20219;&#20154;&#24037;&#26234;&#33021;&#12289;&#25968;&#25454;&#20849;&#20139;&#12289;&#36328;&#32452;&#32455;&#36793;&#30028;&#30340;&#38544;&#31169;&#38382;&#39064;&#21644;&#38646;&#20449;&#20219;&#32593;&#32476;&#23433;&#20840;&#31561;&#39046;&#22495;&#30340;&#36827;&#23637;&#12290;&#26412;&#25991;&#23558;&#20174;&#22810;&#20010;&#23398;&#31185;&#30340;&#35282;&#24230;&#23545;&#20998;&#24067;&#24335;&#20449;&#20219;&#30340;&#27010;&#24565;&#36827;&#34892;&#27010;&#36848;&#65292;&#24182;&#20174;&#31995;&#32479;/&#36719;&#20214;&#26550;&#26500;&#30340;&#35282;&#24230;&#25506;&#35752;&#30001;&#20998;&#24067;&#24335;&#20449;&#20219;&#25216;&#26415;&#23454;&#29616;&#30340;&#20449;&#20219;&#37325;&#20998;&#37197;/&#36716;&#31227;&#21450;&#30456;&#20851;&#30340;&#31995;&#32479;&#19982;&#24212;&#29992;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
Distributed trust is a nebulous concept that has evolved from different perspectives in recent years. While one can attribute its current prominence to blockchain and cryptocurrency, the distributed trust concept has been cultivating progress in federated learning, trustworthy and responsible AI in an ecosystem setting, data sharing, privacy issues across organizational boundaries, and zero trust cybersecurity. This paper will survey the concept of distributed trust in multiple disciplines. It will take a system/software architecture point of view to look at trust redistribution/shift and the associated tradeoffs in systems and applications enabled by distributed trust technologies.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;CARBS&#8221;&#30340;&#31639;&#27861;&#65292;&#23427;&#21033;&#29992;&#36125;&#21494;&#26031;&#20248;&#21270;&#31639;&#27861;&#22312;&#24615;&#33021;-&#35745;&#31639; Pareto &#21069;&#27839;&#38468;&#36817;&#25191;&#34892;&#23616;&#37096;&#25628;&#32034;&#26469;&#35299;&#20915;&#22823;&#22411;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36229;&#21442;&#25968;&#35843;&#25972;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#20855;&#26377;&#35768;&#22810;&#36229;&#21442;&#25968;&#30340;&#26080;&#30028;&#25628;&#32034;&#31354;&#38388;&#65292;&#23398;&#20064;&#32553;&#25918;&#20851;&#31995;&#65292;&#24182;&#33258;&#21160;&#21270;&#20102;&#35768;&#22810;&#35843;&#25972;&#20013;&#30340;&#8220;&#40657;&#39764;&#27861;&#8221;&#12290;&#27492;&#22806;&#65292;&#35813;&#26041;&#27861;&#23545;&#35745;&#31639;&#21463;&#38480;&#21046;&#30340;&#24773;&#20917;&#23588;&#20854;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2306.08055</link><description>&lt;p&gt;
&#22312;&#20280;&#32553;&#24615;&#35757;&#32451;&#20013;&#20248;&#21270;&#36229;&#21442;&#25968;&#65306;&#35745;&#31639;&#25928;&#29575;&#35757;&#32451;&#30340;&#36229;&#21442;&#25968;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Tune As You Scale: Hyperparameter Optimization For Compute Efficient Training. (arXiv:2306.08055v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08055
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;CARBS&#8221;&#30340;&#31639;&#27861;&#65292;&#23427;&#21033;&#29992;&#36125;&#21494;&#26031;&#20248;&#21270;&#31639;&#27861;&#22312;&#24615;&#33021;-&#35745;&#31639; Pareto &#21069;&#27839;&#38468;&#36817;&#25191;&#34892;&#23616;&#37096;&#25628;&#32034;&#26469;&#35299;&#20915;&#22823;&#22411;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36229;&#21442;&#25968;&#35843;&#25972;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#20855;&#26377;&#35768;&#22810;&#36229;&#21442;&#25968;&#30340;&#26080;&#30028;&#25628;&#32034;&#31354;&#38388;&#65292;&#23398;&#20064;&#32553;&#25918;&#20851;&#31995;&#65292;&#24182;&#33258;&#21160;&#21270;&#20102;&#35768;&#22810;&#35843;&#25972;&#20013;&#30340;&#8220;&#40657;&#39764;&#27861;&#8221;&#12290;&#27492;&#22806;&#65292;&#35813;&#26041;&#27861;&#23545;&#35745;&#31639;&#21463;&#38480;&#21046;&#30340;&#24773;&#20917;&#23588;&#20854;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#36229;&#21442;&#25968;&#35843;&#25972;&#21487;&#20197;&#20351;&#30456;&#21516;&#30340;&#35745;&#31639;&#37327;&#33719;&#24471;&#25968;&#37327;&#32423;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#31995;&#32479;&#35843;&#25972;&#36824;&#19981;&#26222;&#36941;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#22823;&#22411;&#27169;&#22411;&#26356;&#26159;&#22914;&#27492;&#65292;&#36825;&#20123;&#27169;&#22411;&#35780;&#20272;&#26114;&#36149;&#65292;&#36229;&#21442;&#25968;&#36739;&#22810;&#65292;&#38656;&#35201;&#36827;&#34892;&#38590;&#20197;&#25226;&#25569;&#30340;&#25240;&#20013;&#12289;&#39044;&#31639;&#21644;&#25628;&#32034;&#36793;&#30028;&#20915;&#31574;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#24182;&#25552;&#20986;&#19968;&#31181;&#23454;&#29992;&#30340;&#26041;&#27861;&#26469;&#31283;&#20581;&#22320;&#35843;&#25972;&#22823;&#22411;&#27169;&#22411;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25104;&#26412;&#24863;&#30693; Pareto &#21306;&#22495;&#36125;&#21494;&#26031;&#25628;&#32034;&#65288;CARBS&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#36125;&#21494;&#26031;&#20248;&#21270;&#31639;&#27861;&#65292;&#23427;&#22312;&#24615;&#33021;-&#35745;&#31639; Pareto &#21069;&#27839;&#38468;&#36817;&#25191;&#34892;&#23616;&#37096;&#25628;&#32034;&#12290;CARBS &#22312;&#20855;&#26377;&#35768;&#22810;&#36229;&#21442;&#25968;&#30340;&#26080;&#30028;&#25628;&#32034;&#31354;&#38388;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#23398;&#20064;&#32553;&#25918;&#20851;&#31995;&#65292;&#22240;&#27492;&#21363;&#20351;&#22312;&#27169;&#22411;&#32553;&#25918;&#30340;&#21516;&#26102;&#20063;&#21487;&#20197;&#35843;&#25972;&#27169;&#22411;&#65292;&#24182;&#33258;&#21160;&#21270;&#20102;&#35768;&#22810;&#8220;&#40657;&#39764;&#27861;&#8221;&#35843;&#25972;&#12290;&#22312;&#25105;&#20204;&#30340;&#32467;&#26524;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#35843;&#25972;&#31616;&#21333;&#30340;&#22522;&#32447;&#65288;ProcGen &#35770;&#25991;&#20013;&#25552;&#20379;&#30340; PPO &#26041;&#27861;&#65289;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#25972;&#20010; ProcGen &#22522;&#20934;&#27979;&#35797;&#12290;&#25105;&#20204;&#36824;&#22312;&#20351;&#29992;&#26356;&#23569;&#30340;&#35780;&#20272;&#26102;&#22797;&#21046;&#20102; Bertinetto &#31561;&#20154;&#30340;&#27169;&#22411;&#36873;&#25321;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#24120;&#36866;&#29992;&#65292;&#20294;&#29305;&#21035;&#36866;&#21512;&#35745;&#31639;&#21463;&#38480;&#30340;&#24773;&#20917;&#65292;&#20854;&#20013;&#35774;&#35745;&#24072;&#21487;&#20197;&#36731;&#26494;&#35780;&#20272;&#25110;&#38480;&#21046;&#22521;&#35757;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hyperparameter tuning of deep learning models can lead to order-of-magnitude performance gains for the same amount of compute. Despite this, systematic tuning is uncommon, particularly for large models, which are expensive to evaluate and tend to have many hyperparameters, necessitating difficult judgment calls about tradeoffs, budgets, and search bounds. To address these issues and propose a practical method for robustly tuning large models, we present Cost-Aware Pareto Region Bayesian Search (CARBS), a Bayesian optimization algorithm that performs local search around the performance-cost Pareto frontier. CARBS does well even in unbounded search spaces with many hyperparameters, learns scaling relationships so that it can tune models even as they are scaled up, and automates much of the "black magic" of tuning. Among our results, we effectively solve the entire ProcGen benchmark just by tuning a simple baseline (PPO, as provided in the original ProcGen paper). We also reproduce the mo
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#28145;&#24230;Q&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21098;&#26525;&#21160;&#20316;&#38598;&#26469;&#23454;&#29616;&#23558;&#20013;&#38388;&#29983;&#29289;&#26631;&#24535;&#29289;&#20449;&#21495;&#25972;&#21512;&#21040;&#22870;&#21169;&#35268;&#33539;&#20013;&#65292;&#25552;&#39640;&#20102;&#37325;&#30151;&#25252;&#29702;&#31574;&#30053;&#30340;&#21487;&#38752;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.08044</link><description>&lt;p&gt;
&#21098;&#26525;&#26041;&#24335;&#25552;&#39640;&#21487;&#38752;&#31574;&#30053;&#65306;&#19968;&#31181;&#22810;&#30446;&#26631;&#28145;&#24230;Q&#23398;&#20064;&#26041;&#27861;&#24212;&#29992;&#20110;&#37325;&#30151;&#25252;&#29702;
&lt;/p&gt;
&lt;p&gt;
Pruning the Way to Reliable Policies: A Multi-Objective Deep Q-Learning Approach to Critical Care. (arXiv:2306.08044v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08044
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#28145;&#24230;Q&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21098;&#26525;&#21160;&#20316;&#38598;&#26469;&#23454;&#29616;&#23558;&#20013;&#38388;&#29983;&#29289;&#26631;&#24535;&#29289;&#20449;&#21495;&#25972;&#21512;&#21040;&#22870;&#21169;&#35268;&#33539;&#20013;&#65292;&#25552;&#39640;&#20102;&#37325;&#30151;&#25252;&#29702;&#31574;&#30053;&#30340;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#21307;&#30103;&#20915;&#31574;&#20855;&#26377;&#36830;&#32493;&#24615;&#65292;&#22240;&#27492;&#65292;&#24378;&#21270;&#23398;&#20064;&#21487;&#33021;&#26377;&#26395;&#21046;&#23450;&#31934;&#30830;&#30340;&#25968;&#25454;&#39537;&#21160;&#27835;&#30103;&#35745;&#21010;&#12290;&#28982;&#32780;&#65292;&#35813;&#39046;&#22495;&#30340;&#20027;&#35201;&#25361;&#25112;&#20043;&#19968;&#26159;&#20027;&#35201;&#22522;&#20110;&#27515;&#20129;&#29575;&#30340;&#22870;&#21169;&#20989;&#25968;&#30340;&#31232;&#30095;&#24615;&#65292;&#23548;&#33268;&#31163;&#32447;&#20272;&#35745;&#30340;&#31283;&#23450;&#24615;&#38477;&#20302;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#28145;&#24230;Q&#23398;&#20064;&#26041;&#27861;&#65292;&#33021;&#22815;&#33719;&#24471;&#26356;&#21487;&#38752;&#30340;&#37325;&#30151;&#25252;&#29702;&#31574;&#30053;&#12290;&#35813;&#26041;&#27861;&#23558;&#30456;&#20851;&#20294;&#22024;&#26434;&#30340;&#20013;&#38388;&#29983;&#29289;&#26631;&#24535;&#29289;&#20449;&#21495;&#25972;&#21512;&#21040;&#22870;&#21169;&#35268;&#33539;&#20013;&#65292;&#21516;&#26102;&#19981;&#20250;&#25439;&#23475;&#24863;&#20852;&#36259;&#30340;&#20027;&#35201;&#32467;&#26524;&#65288;&#20363;&#22914;&#24739;&#32773;&#29983;&#23384;&#29575;&#65289;&#30340;&#20248;&#21270;&#12290;&#36890;&#36807;&#26681;&#25454;&#25152;&#26377;&#21487;&#29992;&#22870;&#21169;&#23545;&#21160;&#20316;&#38598;&#36827;&#34892;&#21098;&#26525;&#65292;&#28982;&#21518;&#22522;&#20110;&#31232;&#30095;&#20027;&#35201;&#22870;&#21169;&#65292;&#20351;&#29992;&#21463;&#38480;&#21160;&#20316;&#38598;&#36827;&#34892;&#26368;&#32456;&#27169;&#22411;&#35757;&#32451;&#65292;&#36890;&#36807;&#35299;&#31163;&#20934;&#30830;&#21644;&#36817;&#20284;&#22870;&#21169;&#26469;&#26368;&#23567;&#21270;&#20027;&#35201;&#30446;&#26631;&#30340;&#28508;&#22312;&#25197;&#26354;&#65292;&#23454;&#29616;&#20102;&#19978;&#36848;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most medical treatment decisions are sequential in nature. Hence, there is substantial hope that reinforcement learning may make it possible to formulate precise data-driven treatment plans. However, a key challenge for most applications in this field is the sparse nature of primarily mortality-based reward functions, leading to decreased stability of offline estimates. In this work, we introduce a deep Q-learning approach able to obtain more reliable critical care policies. This method integrates relevant but noisy intermediate biomarker signals into the reward specification, without compromising the optimization of the main outcome of interest (e.g. patient survival). We achieve this by first pruning the action set based on all available rewards, and second training a final model based on the sparse main reward but with a restricted action set. By disentangling accurate and approximated rewards through action pruning, potential distortions of the main objective are minimized, all whi
&lt;/p&gt;</description></item><item><title>FLamE&#26159;&#19968;&#20010;&#21033;&#29992;GPT-3&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#29992;&#20110;RoBERTa&#24494;&#35843;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#26694;&#26550;&#65292;&#22312;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#20219;&#21153;&#19978;&#23637;&#29616;&#20986;&#36739;&#22909;&#30340;&#24615;&#33021;&#65292;&#20294;&#26159;&#20154;&#29983;&#25104;&#30340;&#35299;&#37322;&#22823;&#22810;&#19981;&#33021;&#20805;&#20998;&#22320;&#35777;&#26126;&#20998;&#31867;&#20915;&#31574;&#65292;&#26631;&#31614;&#29305;&#23450;&#32447;&#32034;&#22312;&#35299;&#37322;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2306.08042</link><description>&lt;p&gt;
FLamE: &#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#19979;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
FLamE: Few-shot Learning from Natural Language Explanations. (arXiv:2306.08042v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08042
&lt;/p&gt;
&lt;p&gt;
FLamE&#26159;&#19968;&#20010;&#21033;&#29992;GPT-3&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#29992;&#20110;RoBERTa&#24494;&#35843;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#26694;&#26550;&#65292;&#22312;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#20219;&#21153;&#19978;&#23637;&#29616;&#20986;&#36739;&#22909;&#30340;&#24615;&#33021;&#65292;&#20294;&#26159;&#20154;&#29983;&#25104;&#30340;&#35299;&#37322;&#22823;&#22810;&#19981;&#33021;&#20805;&#20998;&#22320;&#35777;&#26126;&#20998;&#31867;&#20915;&#31574;&#65292;&#26631;&#31614;&#29305;&#23450;&#32447;&#32034;&#22312;&#35299;&#37322;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#22312;&#29702;&#35770;&#19978;&#20855;&#26377;&#20026;&#27169;&#22411;&#25512;&#29702;&#25552;&#20379;&#20016;&#23500;&#20449;&#24687;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;Lampinen&#31561;&#20154;&#30340;&#26368;&#26032;&#30740;&#31350;&#34920;&#26126;&#65292;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#22312;&#25552;&#39640;&#20998;&#31867;&#26041;&#38754;&#30340;&#25928;&#29992;&#26377;&#38480;&#12290;&#20026;&#20102;&#26377;&#25928;&#22320;&#20174;&#35299;&#37322;&#20013;&#23398;&#20064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FLamE&#65292;&#36825;&#26159;&#19968;&#20010;&#20004;&#38454;&#27573;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#26694;&#26550;&#65292;&#39318;&#20808;&#20351;&#29992;GPT-3&#29983;&#25104;&#35299;&#37322;&#65292;&#28982;&#21518;&#20351;&#29992;&#29983;&#25104;&#30340;&#35299;&#37322;&#23545;&#36739;&#23567;&#30340;&#27169;&#22411;&#65288;&#20363;&#22914;RoBERTa&#65289;&#36827;&#34892;&#24494;&#35843;&#12290;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#23454;&#39564;&#34920;&#26126;&#65292;&#19982;&#24378;&#22522;&#32447;&#30456;&#27604;&#65292;FLamE&#20855;&#26377;&#24456;&#22909;&#30340;&#25928;&#26524;&#65292;&#22312;e-SNLI&#25968;&#25454;&#38598;&#19978;&#30340;&#20934;&#30830;&#24615;&#27604;GPT-3 Babbage&#25552;&#39640;&#20102;17.6&#65285;&#65292;&#27604;GPT-3 Davinci&#25552;&#39640;&#20102;5.7&#65285;&#12290;&#23613;&#31649;&#25552;&#39640;&#20102;&#20998;&#31867;&#24615;&#33021;&#65292;&#20294;&#26159;&#20154;&#31867;&#35780;&#20272;&#20986;&#20154;&#29983;&#25104;&#30340;&#22823;&#37096;&#20998;&#35299;&#37322;&#37117;&#19981;&#33021;&#20805;&#20998;&#22320;&#35777;&#26126;&#20998;&#31867;&#20915;&#31574;&#12290;&#39069;&#22806;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#26631;&#31614;&#29305;&#23450;&#32447;&#32034;&#65288;&#22914;&#20013;&#31435;&#26631;&#31614;&#30340;&#8220;&#19981;&#30693;&#36947;&#8221;&#65289;&#22312;&#29983;&#25104;&#35299;&#37322;&#20013;&#21457;&#25381;&#20102;&#37325;&#35201;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Natural language explanations have the potential to provide rich information that in principle guides model reasoning. Yet, recent work by Lampinen et al. (2022) has shown limited utility of natural language explanations in improving classification. To effectively learn from explanations, we present FLamE, a two-stage few-shot learning framework that first generates explanations using GPT-3, and then finetunes a smaller model (e.g., RoBERTa) with generated explanations. Our experiments on natural language inference demonstrate effectiveness over strong baselines, increasing accuracy by 17.6% over GPT-3 Babbage and 5.7% over GPT-3 Davinci in e-SNLI. Despite improving classification performance, human evaluation surprisingly reveals that the majority of generated explanations does not adequately justify classification decisions. Additional analyses point to the important role of label-specific cues (e.g., "not know" for the neutral label) in generated explanations.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25968;&#25454;&#27745;&#26579;&#25915;&#20987;&#65292;&#25552;&#20986;&#20102;&#21807;&#19968;&#32435;&#20160;&#38598;&#30340;&#27010;&#24565;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#32447;&#24615;&#35268;&#21010;&#26041;&#26696;&#26469;&#35745;&#31639;&#26368;&#20248;&#27745;&#26579;&#25915;&#20987;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2306.08041</link><description>&lt;p&gt;
&#20851;&#20110;&#20266;&#36896;&#32435;&#20160;&#22343;&#34913;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On Faking a Nash Equilibrium. (arXiv:2306.08041v1 [cs.MA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08041
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25968;&#25454;&#27745;&#26579;&#25915;&#20987;&#65292;&#25552;&#20986;&#20102;&#21807;&#19968;&#32435;&#20160;&#38598;&#30340;&#27010;&#24565;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#32447;&#24615;&#35268;&#21010;&#26041;&#26696;&#26469;&#35745;&#31639;&#26368;&#20248;&#27745;&#26579;&#25915;&#20987;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25968;&#25454;&#27745;&#26579;&#25915;&#20987;&#65292;&#25915;&#20987;&#32773;&#35797;&#22270;&#26356;&#25913;&#25968;&#25454;&#38598;&#20197;&#23433;&#35013;&#65288;&#28508;&#22312;&#34394;&#20551;&#30340;&#65289;&#21807;&#19968;&#39532;&#23572;&#21487;&#22827;&#23436;&#32654;&#32435;&#20160;&#22343;&#34913;&#28857;(Nash equilibrium)&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#21807;&#19968;&#32435;&#20160;&#38598;&#30340;&#27010;&#24565;&#65292;&#21363;&#30001;&#20854;Q&#20989;&#25968;&#35268;&#23450;&#30340;&#28216;&#25103;&#30340;&#38598;&#21512;&#65292;&#20854;&#20855;&#26377;&#21807;&#19968;&#30340;&#32852;&#21512;&#31574;&#30053;&#20316;&#20026;&#21807;&#19968;&#30340;&#32435;&#20160;&#22343;&#34913;&#28857;&#12290;&#21807;&#19968;&#32435;&#20160;&#38598;&#23545;&#20110;&#27745;&#26579;&#25915;&#20987;&#38750;&#24120;&#37325;&#35201;&#65292;&#22240;&#20026;&#21482;&#26377;&#24403;&#25968;&#25454;&#27745;&#26579;&#20351;&#25152;&#26377;&#21512;&#29702;&#30340;&#28216;&#25103;&#37117;&#22312;&#20854;&#20013;&#26102;&#65292;&#25915;&#20987;&#25165;&#25104;&#21151;&#12290;&#21807;&#19968;&#32435;&#20160;&#38598;&#23558;&#24120;&#29992;&#20110;&#36870;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#22870;&#21169;&#22810;&#38754;&#20307;&#25512;&#24191;&#21040;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#12290;&#23545;&#20110;&#38646;&#21644;&#39532;&#23572;&#31185;&#22827;&#21338;&#24328;&#65292;&#36870;&#32435;&#20160;&#38598;&#20197;&#21450;&#30001;&#25968;&#25454;&#24341;&#36215;&#30340;&#21512;&#29702;&#28216;&#25103;&#38598;&#37117;&#26159;Q&#20989;&#25968;&#31354;&#38388;&#20013;&#30340;&#22810;&#38754;&#20307;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32447;&#24615;&#35268;&#21010;&#26041;&#26696;&#20197;&#26377;&#25928;&#22320;&#35745;&#31639;&#26368;&#20248;&#30340;&#27745;&#26579;&#25915;&#20987;&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#20026;&#35774;&#35745;&#26356;&#21152;&#40065;&#26834;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20043;&#21069;&#24517;&#35201;&#30340;&#27493;&#39588;&#25581;&#31034;&#20102;&#31163;&#32447;MARL&#25968;&#25454;&#27745;&#26579;&#25915;&#20987;&#32467;&#26500;&#30340;&#19968;&#20123;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
We characterize offline data poisoning attacks on Multi-Agent Reinforcement Learning (MARL), where an attacker may change a data set in an attempt to install a (potentially fictitious) unique Markov-perfect Nash equilibrium. We propose the unique Nash set, namely the set of games, specified by their Q functions, with a specific joint policy being the unique Nash equilibrium. The unique Nash set is central to poisoning attacks because the attack is successful if and only if data poisoning pushes all plausible games inside it. The unique Nash set generalizes the reward polytope commonly used in inverse reinforcement learning to MARL. For zero-sum Markov games, both the inverse Nash set and the set of plausible games induced by data are polytopes in the Q function space. We exhibit a linear program to efficiently compute the optimal poisoning attack. Our work sheds light on the structure of data poisoning attacks on offline MARL, a necessary step before one can design more robust MARL alg
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#24494;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#26041;&#27861;&#65292;&#20351;&#29992;&#21160;&#24577;&#36890;&#36947;&#20998;&#37197;&#31639;&#27861;&#23454;&#29616;&#36890;&#36947;&#32500;&#24230;&#30340;&#28789;&#27963;&#25628;&#32034;&#31354;&#38388;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#25214;&#21040;&#31561;&#21516;&#20110;&#20197;&#21069;&#26041;&#27861;&#22312;CIFAR-10&#25968;&#25454;&#38598;&#19978;&#20219;&#21153;&#20934;&#30830;&#24615;&#21644;&#25512;&#29702;&#24310;&#36831;&#26041;&#38754;&#30340;DNN&#26550;&#26500;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#25163;&#21160;&#35774;&#35745;&#25110;&#26550;&#26500;&#24037;&#31243;&#19987;&#19994;&#30693;&#35782;&#12290;</title><link>http://arxiv.org/abs/2306.08021</link><description>&lt;p&gt;
&#21487;&#21464;&#36890;&#36947;&#32500;&#24230;&#30340;&#21487;&#24494;&#26550;&#26500;&#25628;&#32034;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Flexible Channel Dimensions for Differentiable Architecture Search. (arXiv:2306.08021v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08021
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#24494;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#26041;&#27861;&#65292;&#20351;&#29992;&#21160;&#24577;&#36890;&#36947;&#20998;&#37197;&#31639;&#27861;&#23454;&#29616;&#36890;&#36947;&#32500;&#24230;&#30340;&#28789;&#27963;&#25628;&#32034;&#31354;&#38388;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#25214;&#21040;&#31561;&#21516;&#20110;&#20197;&#21069;&#26041;&#27861;&#22312;CIFAR-10&#25968;&#25454;&#38598;&#19978;&#20219;&#21153;&#20934;&#30830;&#24615;&#21644;&#25512;&#29702;&#24310;&#36831;&#26041;&#38754;&#30340;DNN&#26550;&#26500;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#25163;&#21160;&#35774;&#35745;&#25110;&#26550;&#26500;&#24037;&#31243;&#19987;&#19994;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35745;&#31639;&#36164;&#28304;&#26377;&#38480;&#30340;&#26465;&#20214;&#19979;&#35774;&#35745;&#34920;&#29616;&#33391;&#22909;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#25214;&#21040;&#26368;&#20248;&#30340;&#36890;&#36947;&#32500;&#24230;&#65288;&#21363;DNN&#23618;&#20013;&#30340;&#36807;&#28388;&#22120;&#25968;&#37327;&#65289;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#36817;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#24037;&#20316;&#26088;&#22312;&#33258;&#21160;&#21270;DNN&#27169;&#22411;&#23454;&#29616;&#30340;&#20248;&#21270;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#36890;&#36947;&#32500;&#24230;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#26041;&#27861;&#20381;&#36182;&#20110;&#22266;&#23450;&#30340;&#25628;&#32034;&#31354;&#38388;&#65292;&#36825;&#38459;&#30861;&#20102;&#23454;&#29616;&#39640;&#25928;&#19988;&#23436;&#20840;&#33258;&#21160;&#21270;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#24494;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#26041;&#27861;&#65292;&#37197;&#22791;&#26377;&#25928;&#30340;&#21160;&#24577;&#36890;&#36947;&#20998;&#37197;&#31639;&#27861;&#65292;&#20197;&#23454;&#29616;&#36890;&#36947;&#32500;&#24230;&#30340;&#28789;&#27963;&#25628;&#32034;&#31354;&#38388;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#33021;&#22815;&#25214;&#21040;&#31561;&#21516;&#20110;&#20197;&#21069;&#26041;&#27861;&#22312;CIFAR-10&#25968;&#25454;&#38598;&#19978;&#20219;&#21153;&#20934;&#30830;&#24615;&#21644;&#25512;&#29702;&#24310;&#36831;&#26041;&#38754;&#30340;DNN&#26550;&#26500;&#65292;architecture search&#38454;&#27573;GPU-hours&#25552;&#39640;&#20102;1.3-1.7&#20493;&#65292;&#20869;&#23384;&#35201;&#27714;&#25552;&#39640;&#20102;1.5-1.7&#20493;&#12290;&#27492;&#22806;&#65292;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#19981;&#38656;&#35201;&#20219;&#20309;&#25163;&#21160;&#35774;&#35745;&#25110;&#26550;&#26500;&#24037;&#31243;&#19987;&#19994;&#30693;&#35782;&#65292;&#24182;&#19988;&#21487;&#20197;&#36731;&#26494;&#25193;&#23637;&#21040;&#21508;&#31181;DNN&#26550;&#26500;&#21644;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Finding optimal channel dimensions (i.e., the number of filters in DNN layers) is essential to design DNNs that perform well under computational resource constraints. Recent work in neural architecture search aims at automating the optimization of the DNN model implementation. However, existing neural architecture search methods for channel dimensions rely on fixed search spaces, which prevents achieving an efficient and fully automated solution. In this work, we propose a novel differentiable neural architecture search method with an efficient dynamic channel allocation algorithm to enable a flexible search space for channel dimensions. We show that the proposed framework is able to find DNN architectures that are equivalent to previous methods in task accuracy and inference latency for the CIFAR-10 dataset with an improvement of $1.3-1.7\times$ in GPU-hours and $1.5-1.7\times$ in the memory requirements during the architecture search stage. Moreover, the proposed frameworks do not re
&lt;/p&gt;</description></item><item><title>Curatr&#26159;&#19968;&#20010;&#22312;&#32447;&#24179;&#21488;&#65292;&#21487;&#20197;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#25903;&#25345;&#30340;&#35821;&#20041;&#25628;&#32034;&#26469;&#36827;&#34892;&#21382;&#21490;&#25991;&#23398;&#25991;&#26412;&#30340;&#31579;&#36873;&#65292;&#25552;&#20379;&#20027;&#39064;&#35789;&#20856;&#30340;&#29983;&#25104;&#65292;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#24555;&#36895;&#20174;&#22823;&#37327;&#30340;&#25968;&#23383;&#21270;&#25991;&#26412;&#20013;&#25361;&#36873;&#20986;&#30456;&#20851;&#30340;&#23376;&#35821;&#26009;&#24211;&#12290;</title><link>http://arxiv.org/abs/2306.08020</link><description>&lt;p&gt;
Curatr&#65306;&#21382;&#21490;&#25991;&#23398;&#25991;&#26412;&#35821;&#20041;&#20998;&#26512;&#19982;&#31579;&#36873;&#24179;&#21488;&#12290;
&lt;/p&gt;
&lt;p&gt;
Curatr: A Platform for Semantic Analysis and Curation of Historical Literary Texts. (arXiv:2306.08020v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08020
&lt;/p&gt;
&lt;p&gt;
Curatr&#26159;&#19968;&#20010;&#22312;&#32447;&#24179;&#21488;&#65292;&#21487;&#20197;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#25903;&#25345;&#30340;&#35821;&#20041;&#25628;&#32034;&#26469;&#36827;&#34892;&#21382;&#21490;&#25991;&#23398;&#25991;&#26412;&#30340;&#31579;&#36873;&#65292;&#25552;&#20379;&#20027;&#39064;&#35789;&#20856;&#30340;&#29983;&#25104;&#65292;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#24555;&#36895;&#20174;&#22823;&#37327;&#30340;&#25968;&#23383;&#21270;&#25991;&#26412;&#20013;&#25361;&#36873;&#20986;&#30456;&#20851;&#30340;&#23376;&#35821;&#26009;&#24211;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23383;&#21270;&#21382;&#21490;&#21644;&#29616;&#20195;&#25991;&#23398;&#25991;&#26412;&#30340;&#25345;&#32493;&#22686;&#21152;&#25552;&#20379;&#20102;&#20154;&#25991;&#23398;&#31185;&#26032;&#30740;&#31350;&#30340;&#20016;&#23500;&#21487;&#33021;&#24615;&#65292;&#28982;&#32780;&#36825;&#20123;&#38598;&#21512;&#30340;&#35268;&#27169;&#21644;&#22810;&#26679;&#24615;&#65292;&#20063;&#38754;&#20020;&#30528;&#29305;&#27530;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;Curatr&#65292;&#19968;&#20010;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#25903;&#25345;&#30340;&#35821;&#20041;&#25628;&#32034;&#22312;&#32447;&#24179;&#21488;&#65292;&#26088;&#22312;&#20026;&#25968;&#23383;&#20154;&#25991;&#23398;&#31185;&#23398;&#32773;&#25552;&#20379;&#25991;&#23398;&#25991;&#26412;&#30340;&#25506;&#32034;&#21644;&#31579;&#36873;&#12290;&#35813;&#24179;&#21488;&#25552;&#20379;&#20102;&#19968;&#20010;&#25991;&#26412;&#25366;&#25496;&#24037;&#20316;&#27969;&#65292;&#36890;&#36807;&#23558;&#31070;&#32463;&#35789;&#23884;&#20837;&#19982;&#19987;&#19994;&#39046;&#22495;&#30693;&#35782;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#20027;&#39064;&#35789;&#20856;&#30340;&#29983;&#25104;&#65292;&#21487;&#20197;&#35753;&#30740;&#31350;&#20154;&#21592;&#20174;&#22823;&#37327;&#30340;18&#19990;&#32426;&#21644;19&#19990;&#32426;&#25968;&#23383;&#21270;&#25991;&#26412;&#20013;&#31579;&#36873;&#30456;&#20851;&#23376;&#35821;&#26009;&#24211;&#12290;
&lt;/p&gt;
&lt;p&gt;
The increasing availability of digital collections of historical and contemporary literature presents a wealth of possibilities for new research in the humanities. The scale and diversity of such collections however, presents particular challenges in identifying and extracting relevant content. This paper presents Curatr, an online platform for the exploration and curation of literature with machine learning-supported semantic search, designed within the context of digital humanities scholarship. The platform provides a text mining workflow that combines neural word embeddings with expert domain knowledge to enable the generation of thematic lexicons, allowing researches to curate relevant sub-corpora from a large corpus of 18th and 19th century digitised texts.
&lt;/p&gt;</description></item><item><title>Mol-Instructions&#26159;&#19968;&#20010;&#19987;&#38376;&#20026;&#29983;&#29289;&#20998;&#23376;&#39046;&#22495;&#35774;&#35745;&#30340;&#32508;&#21512;&#25351;&#20196;&#25968;&#25454;&#38598;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#29289;&#39046;&#22495;&#20013;&#30340;&#36866;&#24212;&#33021;&#21147;&#21644;&#35748;&#30693;&#25935;&#38160;&#24230;&#12290;</title><link>http://arxiv.org/abs/2306.08018</link><description>&lt;p&gt;
Mol-Instructions: &#19968;&#20010;&#22823;&#35268;&#27169;&#29983;&#29289;&#20998;&#23376;&#25351;&#20196;&#25968;&#25454;&#38598;&#65292;&#20026;&#22823;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#25903;&#25345;
&lt;/p&gt;
&lt;p&gt;
Mol-Instructions: A Large-Scale Biomolecular Instruction Dataset for Large Language Models. (arXiv:2306.08018v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08018
&lt;/p&gt;
&lt;p&gt;
Mol-Instructions&#26159;&#19968;&#20010;&#19987;&#38376;&#20026;&#29983;&#29289;&#20998;&#23376;&#39046;&#22495;&#35774;&#35745;&#30340;&#32508;&#21512;&#25351;&#20196;&#25968;&#25454;&#38598;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#29289;&#39046;&#22495;&#20013;&#30340;&#36866;&#24212;&#33021;&#21147;&#21644;&#35748;&#30693;&#25935;&#38160;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20197;&#20854;&#21331;&#36234;&#30340;&#20219;&#21153;&#22788;&#29702;&#33021;&#21147;&#21644;&#21019;&#26032;&#30340;&#36755;&#20986;&#65292;&#22312;&#35768;&#22810;&#39046;&#22495;&#25512;&#21160;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#29983;&#29289;&#20998;&#23376;&#30740;&#31350;&#31561;&#19987;&#19994;&#39046;&#22495;&#30340;&#29087;&#32451;&#24212;&#29992;&#36824;&#21463;&#21040;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Mol-Instructions&#65292;&#36825;&#26159;&#19968;&#20010;&#32463;&#36807;&#31934;&#24515;&#31574;&#21010;&#12289;&#19987;&#38376;&#38024;&#23545;&#29983;&#29289;&#20998;&#23376;&#39046;&#22495;&#35774;&#35745;&#30340;&#32508;&#21512;&#25351;&#20196;&#25968;&#25454;&#38598;&#12290;Mol-Instructions&#30001;&#19977;&#20010;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#32452;&#25104;&#65306;&#20998;&#23376;&#23548;&#21521;&#25351;&#20196;&#12289;&#34507;&#30333;&#36136;&#23548;&#21521;&#25351;&#20196;&#21644;&#29983;&#29289;&#20998;&#23376;&#25991;&#26412;&#25351;&#20196;&#65292;&#27599;&#20010;&#37096;&#20998;&#37117;&#34987;&#31574;&#21010;&#29992;&#20110;&#22686;&#24378;LLM&#23545;&#29983;&#29289;&#20998;&#23376;&#29305;&#24615;&#21644;&#34892;&#20026;&#30340;&#29702;&#35299;&#21644;&#39044;&#27979;&#33021;&#21147;&#12290;&#36890;&#36807;&#23545;&#20195;&#34920;&#24615;LLM&#30340;&#24191;&#27867;&#25351;&#20196;&#35843;&#25972;&#23454;&#39564;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;Mol-Instructions&#22312;&#22686;&#24378;&#22823;&#27169;&#22411;&#22312;&#29983;&#29289;&#20998;&#23376;&#30740;&#31350;&#22797;&#26434;&#39046;&#22495;&#20869;&#30340;&#36866;&#24212;&#33021;&#21147;&#21644;&#35748;&#30693;&#25935;&#38160;&#24230;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#20174;&#32780;&#20419;&#36827;&#29983;&#29289;&#20998;&#23376;&#39046;&#22495;&#30340;&#36827;&#19968;&#27493;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs), with their remarkable task-handling capabilities and innovative outputs, have catalyzed significant advancements across a spectrum of fields. However, their proficiency within specialized domains such as biomolecular studies remains limited. To address this challenge, we introduce Mol-Instructions, a meticulously curated, comprehensive instruction dataset expressly designed for the biomolecular realm. Mol-Instructions is composed of three pivotal components: molecule-oriented instructions, protein-oriented instructions, and biomolecular text instructions, each curated to enhance the understanding and prediction capabilities of LLMs concerning biomolecular features and behaviors. Through extensive instruction tuning experiments on the representative LLM, we underscore the potency of Mol-Instructions to enhance the adaptability and cognitive acuity of large models within the complex sphere of biomolecular studies, thereby promoting advancements in the biomol
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#33258;&#30001;&#33021;&#21407;&#29702;&#21644;&#20027;&#21160;&#25512;&#29702;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#24182;&#25512;&#23548;&#20102;&#36866;&#29992;&#20110;&#20219;&#24847;&#22270;&#24418;&#27169;&#22411;&#30340;&#20027;&#21160;&#25512;&#29702;&#29256;&#26412;&#12290;&#21516;&#26102;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#24418;&#35828;&#26126;&#35821;&#35328;&#65288;GSL&#65289;&#26469;&#26126;&#30830;&#35268;&#23450;&#31995;&#32479;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2306.08014</link><description>&lt;p&gt;
&#23454;&#29616;&#21512;&#25104;&#20027;&#21160;&#25512;&#29702;&#20195;&#29702;&#65292;&#31532;&#19968;&#37096;&#20998;&#65306;&#35748;&#35782;&#30446;&#26631;&#21644;&#22270;&#24418;&#35828;&#26126;&#35821;&#35328;
&lt;/p&gt;
&lt;p&gt;
Realising Synthetic Active Inference Agents, Part I: Epistemic Objectives and Graphical Specification Language. (arXiv:2306.08014v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08014
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#33258;&#30001;&#33021;&#21407;&#29702;&#21644;&#20027;&#21160;&#25512;&#29702;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#24182;&#25512;&#23548;&#20102;&#36866;&#29992;&#20110;&#20219;&#24847;&#22270;&#24418;&#27169;&#22411;&#30340;&#20027;&#21160;&#25512;&#29702;&#29256;&#26412;&#12290;&#21516;&#26102;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#24418;&#35828;&#26126;&#35821;&#35328;&#65288;GSL&#65289;&#26469;&#26126;&#30830;&#35268;&#23450;&#31995;&#32479;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30001;&#33021;&#21407;&#29702;&#65288;FEP&#65289;&#26159;&#19968;&#31181;&#25551;&#36848;&#31995;&#32479;&#22914;&#20309;&#36890;&#36807;&#26368;&#23567;&#21270;&#33258;&#30001;&#33021;&#27867;&#20989;&#32780;&#33258;&#32452;&#32455;&#25104;&#20855;&#26377;&#36830;&#36143;&#24615;&#12289;&#31283;&#23450;&#32467;&#26500;&#65288;&#26234;&#33021;&#65289;&#30340;&#29702;&#35770;&#26694;&#26550;&#12290;&#20027;&#21160;&#25512;&#29702;&#65288;AIF&#65289;&#26159;FEP&#30340;&#19968;&#20010;&#25512;&#35770;&#65292;&#23427;&#26126;&#30830;&#20102;&#33021;&#22815;&#20026;&#26410;&#26469;&#36827;&#34892;&#35268;&#21010;&#65288;&#20195;&#29702;&#65289;&#30340;&#31995;&#32479;&#26159;&#22914;&#20309;&#36890;&#36807;&#26368;&#23567;&#21270;&#21253;&#21547;&#20449;&#24687;&#23547;&#27714;&#32452;&#20214;&#30340;&#29305;&#23450;&#33258;&#30001;&#33021;&#27867;&#20989;&#26469;&#36816;&#20316;&#30340;&#12290;&#26412;&#25991;&#26159;&#19968;&#20010;&#31995;&#21015;&#20013;&#30340;&#31532;&#19968;&#31687;&#65292;&#25105;&#20204;&#22312;&#33258;&#30001;&#24418;&#24335;&#22240;&#23376;&#22270;&#19978;&#25512;&#23548;&#20102;AIF&#30340;&#21512;&#25104;&#29256;&#26412;&#12290;&#26412;&#25991;&#37325;&#28857;&#25512;&#23548;&#20102;AIF&#25152;&#20351;&#29992;&#30340;&#33258;&#30001;&#33021;&#27867;&#20989;&#30340;&#23616;&#37096;&#29256;&#26412;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#26500;&#36896;&#19968;&#20010;&#36866;&#29992;&#20110;&#20219;&#24847;&#22270;&#24418;&#27169;&#22411;&#24182;&#19982;&#26377;&#20851;&#28040;&#24687;&#20256;&#36882;&#31639;&#27861;&#30340;&#20808;&#21069;&#24037;&#20316;&#25509;&#21475;&#30340;AIF&#29256;&#26412;&#12290;&#32467;&#26524;&#28040;&#24687;&#26159;&#22312;&#25105;&#20204;&#30340;&#20276;&#20387;&#35770;&#25991;&#20013;&#24471;&#20986;&#30340;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#22240;&#23376;&#22270;&#24418;&#24335;&#20013;&#23384;&#22312;&#19968;&#20010;&#32570;&#21475;&#12290;&#34429;&#28982;&#22240;&#23376;&#22270;&#34920;&#36798;&#20102;&#29983;&#25104;&#27169;&#22411;&#65292;&#20294;&#22312;&#25351;&#23450;&#31995;&#32479;&#30446;&#26631;&#26041;&#38754;&#32570;&#20047;&#19968;&#20010;&#22270;&#24418;&#21270;&#35821;&#35328;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#22240;&#23376;&#22270;&#25551;&#36848;&#27861;&#30340;&#26032;&#25193;&#23637;&#65292;&#31216;&#20026;&#22270;&#24418;&#35828;&#26126;&#35821;&#35328;&#65288;GSL&#65289;&#65292;&#23427;&#20351;&#31995;&#32479;&#30446;&#26631;&#24471;&#21040;&#26126;&#30830;&#35268;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Free Energy Principle (FEP) is a theoretical framework for describing how (intelligent) systems self-organise into coherent, stable structures by minimising a free energy functional. Active Inference (AIF) is a corollary of the FEP that specifically details how systems that are able to plan for the future (agents) function by minimising particular free energy functionals that incorporate information seeking components. This paper is the first in a series of two where we derive a synthetic version of AIF on free form factor graphs. The present paper focuses on deriving a local version of the free energy functionals used for AIF. This enables us to construct a version of AIF which applies to arbitrary graphical models and interfaces with prior work on message passing algorithms. The resulting messages are derived in our companion paper. We also identify a gap in the graphical notation used for factor graphs. While factor graphs are great at expressing a generative model, they have so
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#21487;&#38752;&#30340;&#29983;&#25104;&#27169;&#22411;&#35780;&#20272;&#25351;&#26631;TopP\&amp;R&#65292;&#36890;&#36807;&#24341;&#20837;&#25299;&#25169;&#21644;&#32479;&#35745;&#22788;&#29702;&#36827;&#34892;&#20005;&#26684;&#30340;&#25903;&#25345;&#20272;&#35745;&#12290;TopP\&amp;R&#20165;&#20445;&#30041;&#20855;&#26377;&#19968;&#23450;&#32622;&#20449;&#27700;&#24179;&#30340;&#20855;&#26377;&#25299;&#25169;&#21644;&#32479;&#35745;&#19978;&#37325;&#35201;&#24615;&#30340;&#29305;&#24449;&#65292;&#23545;&#20110;&#22122;&#22768;&#29305;&#24449;&#20855;&#26377;&#24378;&#22823;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#32479;&#35745;&#19968;&#33268;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.08013</link><description>&lt;p&gt;
TopP\&amp;R: &#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#25903;&#25345;&#20272;&#35745;&#26041;&#27861;&#65292;&#29992;&#20110;&#35780;&#20272;&#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#20445;&#30495;&#24230;&#21644;&#22810;&#26679;&#24615;
&lt;/p&gt;
&lt;p&gt;
TopP\&amp;R: Robust Support Estimation Approach for Evaluating Fidelity and Diversity in Generative Models. (arXiv:2306.08013v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08013
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#21487;&#38752;&#30340;&#29983;&#25104;&#27169;&#22411;&#35780;&#20272;&#25351;&#26631;TopP\&amp;R&#65292;&#36890;&#36807;&#24341;&#20837;&#25299;&#25169;&#21644;&#32479;&#35745;&#22788;&#29702;&#36827;&#34892;&#20005;&#26684;&#30340;&#25903;&#25345;&#20272;&#35745;&#12290;TopP\&amp;R&#20165;&#20445;&#30041;&#20855;&#26377;&#19968;&#23450;&#32622;&#20449;&#27700;&#24179;&#30340;&#20855;&#26377;&#25299;&#25169;&#21644;&#32479;&#35745;&#19978;&#37325;&#35201;&#24615;&#30340;&#29305;&#24449;&#65292;&#23545;&#20110;&#22122;&#22768;&#29305;&#24449;&#20855;&#26377;&#24378;&#22823;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#32479;&#35745;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#21487;&#38752;&#30340;&#29983;&#25104;&#27169;&#22411;&#35780;&#20272;&#25351;&#26631;&#65292;&#36890;&#36807;&#24341;&#20837;&#25299;&#25169;&#21644;&#32479;&#35745;&#22788;&#29702;&#36827;&#34892;&#20005;&#26684;&#30340;&#25903;&#25345;&#20272;&#35745;&#12290;&#29616;&#26377;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#22914;Inception Score&#65288;IS&#65289;&#65292;Fr\'echet Inception Distance&#65288;FID&#65289;&#20197;&#21450;Precision and Recall&#65288;P\&amp;R&#65289;&#30340;&#21464;&#20307;&#65292;&#20005;&#37325;&#20381;&#36182;&#20110;&#20174;&#26679;&#26412;&#29305;&#24449;&#20272;&#35745;&#30340;&#25903;&#25345;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#35780;&#20272;&#30340;&#36136;&#37327;&#23436;&#20840;&#21462;&#20915;&#20110;&#20854;&#21487;&#38752;&#24615;&#65292;&#20294;&#20854;&#20272;&#35745;&#30340;&#21487;&#38752;&#24615;&#24182;&#27809;&#26377;&#24471;&#21040;&#20005;&#32899;&#30340;&#35752;&#35770;&#65288;&#24182;&#34987;&#24573;&#35270;&#65289;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#25299;&#25169;&#31934;&#24230;&#21644;&#21484;&#22238;&#29575;&#65288;TopP\&amp;R&#65292;&#21457;&#38899;&#20026;&#8220;topper&#8221;&#65289;&#65292;&#23427;&#25552;&#20379;&#20102;&#19968;&#31181;&#31995;&#32479;&#30340;&#26041;&#27861;&#26469;&#20272;&#35745;&#25903;&#25345;&#65292;&#20165;&#20445;&#30041;&#20855;&#26377;&#19968;&#23450;&#32622;&#20449;&#27700;&#24179;&#30340;&#20855;&#26377;&#25299;&#25169;&#21644;&#32479;&#35745;&#19978;&#37325;&#35201;&#24615;&#30340;&#29305;&#24449;&#12290;&#36825;&#19981;&#20165;&#20351;TopP\&amp;R&#23545;&#20110;&#22122;&#22768;&#29305;&#24449;&#20855;&#26377;&#24378;&#22823;&#30340;&#40065;&#26834;&#24615;&#65292;&#32780;&#19988;&#36824;&#25552;&#20379;&#20102;&#32479;&#35745;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#21644;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;TopP\&amp;R&#23545;&#20110;&#31163;&#32676;&#20540;&#21644;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a robust and reliable evaluation metric for generative models by introducing topological and statistical treatments for rigorous support estimation. Existing metrics, such as Inception Score (IS), Fr\'echet Inception Distance (FID), and the variants of Precision and Recall (P\&amp;R), heavily rely on supports that are estimated from sample features. However, the reliability of their estimation has not been seriously discussed (and overlooked) even though the quality of the evaluation entirely depends on it. In this paper, we propose Topological Precision and Recall (TopP\&amp;R, pronounced 'topper'), which provides a systematic approach to estimating supports, retaining only topologically and statistically important features with a certain level of confidence. This not only makes TopP\&amp;R strong for noisy features, but also provides statistical consistency. Our theoretical and experimental results show that TopP\&amp;R is robust to outliers and non-independent and identically distributed
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#22330;&#26223;&#19979;&#32852;&#37030;&#23398;&#20064;&#30340;&#38544;&#34109;&#21518;&#38376;&#25915;&#20987;&#26041;&#26696;&#65292;&#20854;&#20013;&#36890;&#36807;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#25552;&#20379;&#34917;&#20805;&#25968;&#25454;&#38598;&#65292;&#26377;&#25928;&#24212;&#23545;&#25968;&#25454;&#24322;&#26500;&#38382;&#39064;&#21644;&#38544;&#31169;&#25512;&#26029;&#25915;&#20987;&#39118;&#38505;&#12290;</title><link>http://arxiv.org/abs/2306.08011</link><description>&lt;p&gt;
&#22312;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#22330;&#26223;&#19979;&#22522;&#20110;&#38544;&#34109;&#21518;&#38376;&#30340;&#32852;&#37030;&#23398;&#20064;&#38544;&#31169;&#25512;&#26029;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Privacy Inference-Empowered Stealthy Backdoor Attack on Federated Learning under Non-IID Scenarios. (arXiv:2306.08011v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08011
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#22330;&#26223;&#19979;&#32852;&#37030;&#23398;&#20064;&#30340;&#38544;&#34109;&#21518;&#38376;&#25915;&#20987;&#26041;&#26696;&#65292;&#20854;&#20013;&#36890;&#36807;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#25552;&#20379;&#34917;&#20805;&#25968;&#25454;&#38598;&#65292;&#26377;&#25928;&#24212;&#23545;&#25968;&#25454;&#24322;&#26500;&#38382;&#39064;&#21644;&#38544;&#31169;&#25512;&#26029;&#25915;&#20987;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#22312;&#29616;&#23454;&#24773;&#20917;&#19979;&#38754;&#20020;&#25968;&#25454;&#24322;&#26500;&#38382;&#39064;&#65292;&#20294;&#26159;&#36825;&#20010;&#38382;&#39064;&#24448;&#24448;&#34987;&#30740;&#31350;FL&#23433;&#20840;&#21644;&#38544;&#31169;&#30340;&#30740;&#31350;&#25152;&#24573;&#35270;&#12290;&#26412;&#25991;&#38024;&#23545;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#22330;&#26223;&#19979;&#30340;FL&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#22411;&#30340;&#31169;&#23494;&#25512;&#29702;&#21152;&#24378;&#30340;&#38544;&#34109;&#21518;&#38376;&#25915;&#20987;&#65288;PI-SBA&#65289;&#26041;&#26696;&#12290;&#35813;&#26041;&#26696;&#21253;&#25324;&#36890;&#36807;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#25552;&#20379;&#34917;&#20805;&#25968;&#25454;&#38598;&#30340;&#26426;&#21046;&#21644;&#22522;&#20110;&#28304;&#29305;&#23450;&#30340;&#21518;&#38376;&#23398;&#20064;&#31574;&#30053;&#12290;&#27492;&#22806;&#65292;&#35813;&#26041;&#26696;&#36824;&#33021;&#26377;&#25928;&#24212;&#23545;&#24694;&#24847;&#23458;&#25143;&#36890;&#36807;&#38544;&#31169;&#25512;&#26029;&#25915;&#20987;&#31363;&#21462;&#31169;&#26377;&#25968;&#25454;&#30340;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) naturally faces the problem of data heterogeneity in real-world scenarios, but this is often overlooked by studies on FL security and privacy. On the one hand, the effectiveness of backdoor attacks on FL may drop significantly under non-IID scenarios. On the other hand, malicious clients may steal private data through privacy inference attacks. Therefore, it is necessary to have a comprehensive perspective of data heterogeneity, backdoor, and privacy inference. In this paper, we propose a novel privacy inference-empowered stealthy backdoor attack (PI-SBA) scheme for FL under non-IID scenarios. Firstly, a diverse data reconstruction mechanism based on generative adversarial networks (GANs) is proposed to produce a supplementary dataset, which can improve the attacker's local data distribution and support more sophisticated strategies for backdoor attacks. Based on this, we design a source-specified backdoor learning (SSBL) strategy as a demonstration, allowing th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23558;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#21487;&#25511;&#38376;&#25511;&#36866;&#37197;&#22120;ConGater&#24212;&#29992;&#20110;&#22768;&#26223;&#20998;&#31867;&#20219;&#21153;&#65292;&#36739;&#29616;&#26377;&#39046;&#22495;&#27867;&#21270;&#26041;&#27861;&#26356;&#22909;&#22320;&#25552;&#39640;&#27169;&#22411;&#22312;&#26032;&#39046;&#22495;&#20013;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.08010</link><description>&lt;p&gt;
&#25484;&#25511;&#25512;&#29702;&#38454;&#27573;&#30340;&#39046;&#22495;&#20449;&#24687;&#25511;&#21046;&#65292;&#29992;&#20110;&#22768;&#26223;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Domain Information Control at Inference Time for Acoustic Scene Classification. (arXiv:2306.08010v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08010
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#21487;&#25511;&#38376;&#25511;&#36866;&#37197;&#22120;ConGater&#24212;&#29992;&#20110;&#22768;&#26223;&#20998;&#31867;&#20219;&#21153;&#65292;&#36739;&#29616;&#26377;&#39046;&#22495;&#27867;&#21270;&#26041;&#27861;&#26356;&#22909;&#22320;&#25552;&#39640;&#27169;&#22411;&#22312;&#26032;&#39046;&#22495;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39046;&#22495;&#36716;&#31227;&#34987;&#35748;&#20026;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#25361;&#25112;&#65292;&#22240;&#20026;&#23427;&#20250;&#23548;&#33268;&#27169;&#22411;&#24615;&#33021;&#30340;&#26174;&#33879;&#38477;&#20302;&#12290;&#22312;&#22768;&#26223;&#20998;&#31867;&#20219;&#21153;&#20013;&#65292;&#39046;&#22495;&#36716;&#31227;&#20027;&#35201;&#30001;&#19981;&#21516;&#30340;&#24405;&#38899;&#35774;&#22791;&#24341;&#36215;&#12290;&#24050;&#32463;&#26377;&#19968;&#20123;&#30740;&#31350;&#38024;&#23545;&#39046;&#22495;&#27867;&#21270;&#36827;&#34892;&#25913;&#36827;&#65292;&#20197;&#25552;&#39640;&#22312;&#26032;&#39046;&#22495;&#20013;&#65288;&#22914;&#26032;&#35774;&#22791;&#65289;ASC&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#26368;&#36817;&#65292;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#25552;&#20986;&#20102;&#21487;&#25511;&#38376;&#25511;&#36866;&#37197;&#22120;ConGater&#65292;&#20197;&#35299;&#20915;&#20559;&#35265;&#35757;&#32451;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;ConGater&#20801;&#35768;&#22312;&#25512;&#29702;&#26102;&#25511;&#21046;&#21435;&#20559;&#30340;&#36807;&#31243;&#12290;ConGater&#30340;&#20027;&#35201;&#20248;&#28857;&#26159;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#23545;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#36830;&#32493;&#21644;&#36873;&#25321;&#24615;&#21435;&#20559;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;ConGater&#36866;&#24212;&#20110;&#38899;&#39057;&#39057;&#35889;&#21464;&#25442;&#22120;&#65292;&#29992;&#20110;&#22768;&#26223;&#20998;&#31867;&#20219;&#21153;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;ConGater&#21487;&#20197;&#29992;&#20110;&#26377;&#36873;&#25321;&#24615;&#22320;&#20351;&#23398;&#20064;&#34920;&#31034;&#19982;&#35760;&#24405;&#35774;&#22791;&#31561;&#39046;&#22495;&#36716;&#31227;&#19981;&#21464;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;ConGater&#21487;&#20197;&#36880;&#27493;&#25552;&#39640;ASC&#27169;&#22411;&#22312;&#30475;&#19981;&#35265;&#30340;&#39046;&#22495;&#19978;&#30340;&#24615;&#33021;&#65292;&#36229;&#36807;&#29616;&#26377;&#30340;&#39046;&#22495;&#27867;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Domain shift is considered a challenge in machine learning as it causes significant degradation of model performance. In the Acoustic Scene Classification task (ASC), domain shift is mainly caused by different recording devices. Several studies have already targeted domain generalization to improve the performance of ASC models on unseen domains, such as new devices. Recently, the Controllable Gate Adapter ConGater has been proposed in Natural Language Processing to address the biased training data problem. ConGater allows controlling the debiasing process at inference time. ConGater's main advantage is the continuous and selective debiasing of a trained model, during inference. In this work, we adapt ConGater to the audio spectrogram transformer for an acoustic scene classification task. We show that ConGater can be used to selectively adapt the learned representations to be invariant to device domain shifts such as recording devices. Our analysis shows that ConGater can progressively
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#26080;&#20851;&#30340;&#20840;&#38754;&#21518;&#38376;&#25830;&#38500;&#65288;DHBE&#65289;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#23558;&#21518;&#38376;&#25830;&#38500;&#20219;&#21153;&#35270;&#20026;&#19968;&#20010;&#32479;&#19968;&#30340;&#23545;&#25239;&#36807;&#31243;&#65292;&#22312;&#36716;&#31227;&#24178;&#20928;&#25968;&#25454;&#30340;&#30693;&#35782;&#30340;&#21516;&#26102;&#20445;&#35777;&#25830;&#38500;&#21518;&#38376;&#12290;&#36890;&#36807;&#23545;&#25239;&#33976;&#39311;&#21644;&#21518;&#38376;&#27491;&#21017;&#21270;&#20004;&#20010;&#19981;&#21516;&#30340;&#31454;&#20105;&#36807;&#31243;&#65292;DHBE&#23454;&#29616;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#12289;&#26080;&#38656;&#25968;&#25454;&#65292;&#24182;&#19988;&#26377;&#25928;&#30340;&#21518;&#38376;&#25830;&#38500;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2306.08009</link><description>&lt;p&gt;
DHBE: &#36890;&#36807;&#21463;&#38480;&#23545;&#25239;&#33976;&#39311;&#30340;&#25968;&#25454;&#26080;&#20851;&#20840;&#38754;&#21518;&#38376;&#25830;&#38500;&#26469;&#20445;&#25252;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
DHBE: Data-free Holistic Backdoor Erasing in Deep Neural Networks via Restricted Adversarial Distillation. (arXiv:2306.08009v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08009
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#26080;&#20851;&#30340;&#20840;&#38754;&#21518;&#38376;&#25830;&#38500;&#65288;DHBE&#65289;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#23558;&#21518;&#38376;&#25830;&#38500;&#20219;&#21153;&#35270;&#20026;&#19968;&#20010;&#32479;&#19968;&#30340;&#23545;&#25239;&#36807;&#31243;&#65292;&#22312;&#36716;&#31227;&#24178;&#20928;&#25968;&#25454;&#30340;&#30693;&#35782;&#30340;&#21516;&#26102;&#20445;&#35777;&#25830;&#38500;&#21518;&#38376;&#12290;&#36890;&#36807;&#23545;&#25239;&#33976;&#39311;&#21644;&#21518;&#38376;&#27491;&#21017;&#21270;&#20004;&#20010;&#19981;&#21516;&#30340;&#31454;&#20105;&#36807;&#31243;&#65292;DHBE&#23454;&#29616;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#12289;&#26080;&#38656;&#25968;&#25454;&#65292;&#24182;&#19988;&#26377;&#25928;&#30340;&#21518;&#38376;&#25830;&#38500;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21518;&#38376;&#25915;&#20987;&#24050;&#25104;&#20026;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#30340;&#32039;&#24613;&#23041;&#32961;&#12290;&#20026;&#38450;&#24481;&#21518;&#38376;&#25915;&#20987;&#65292;&#35768;&#22810;&#24037;&#20316;&#37117;&#24314;&#31435;&#20102;&#19968;&#20010;&#20998;&#38454;&#27573;&#30340;&#27969;&#31243;&#26469;&#20174;&#21463;&#23475;DNN&#20013;&#31227;&#38500;&#21518;&#38376;&#65306;&#26816;&#26597;&#12289;&#23450;&#20301;&#21644;&#25830;&#38500;&#12290;&#28982;&#32780;&#65292;&#22312;&#21482;&#26377;&#23569;&#37327;&#24178;&#20928;&#25968;&#25454;&#21487;&#29992;&#30340;&#24773;&#20917;&#19979;&#65292;&#36825;&#31181;&#27969;&#31243;&#26159;&#33030;&#24369;&#30340;&#65292;&#32780;&#19988;&#19981;&#33021;&#22312;&#19981;&#29306;&#29298;&#27169;&#22411;&#20934;&#30830;&#24615;&#30340;&#24773;&#20917;&#19979;&#23436;&#20840;&#25830;&#38500;&#21518;&#38376;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#26032;&#30340;&#25968;&#25454;&#26080;&#20851;&#20840;&#38754;&#21518;&#38376;&#25830;&#38500;&#65288;DHBE&#65289;&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#23558;&#21518;&#38376;&#25830;&#38500;&#20219;&#21153;&#35270;&#20026;&#19968;&#20010;&#32479;&#19968;&#30340;&#23545;&#25239;&#36807;&#31243;&#65292;&#22312;&#36716;&#31227;&#24178;&#20928;&#25968;&#25454;&#30340;&#30693;&#35782;&#30340;&#21516;&#26102;&#20445;&#35777;&#25830;&#38500;&#21518;&#38376;&#12290;&#36890;&#36807;&#23545;&#25239;&#33976;&#39311;&#21644;&#21518;&#38376;&#27491;&#21017;&#21270;&#20004;&#20010;&#19981;&#21516;&#30340;&#31454;&#20105;&#36807;&#31243;&#65292;DHBE&#23454;&#29616;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#12289;&#26080;&#38656;&#25968;&#25454;&#65292;&#24182;&#19988;&#26377;&#25928;&#30340;&#21518;&#38376;&#25830;&#38500;&#26694;&#26550;&#12290;&#22823;&#37327;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;DHBE&#21487;&#20197;&#39640;&#25104;&#21151;&#29575;&#22320;&#25830;&#38500;&#21518;&#38376;&#65292;&#21516;&#26102;&#20445;&#25345;&#27169;&#22411;&#20934;&#30830;&#24615;&#65292;&#32988;&#36807;&#29616;&#26377;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Backdoor attacks have emerged as an urgent threat to Deep Neural Networks (DNNs), where victim DNNs are furtively implanted with malicious neurons that could be triggered by the adversary. To defend against backdoor attacks, many works establish a staged pipeline to remove backdoors from victim DNNs: inspecting, locating, and erasing. However, in a scenario where a few clean data can be accessible, such pipeline is fragile and cannot erase backdoors completely without sacrificing model accuracy. To address this issue, in this paper, we propose a novel data-free holistic backdoor erasing (DHBE) framework. Instead of the staged pipeline, the DHBE treats the backdoor erasing task as a unified adversarial procedure, which seeks equilibrium between two different competing processes: distillation and backdoor regularization. In distillation, the backdoored DNN is distilled into a proxy model, transferring its knowledge about clean data, yet backdoors are simultaneously transferred. In backdo
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38543;&#26426;&#26862;&#26519;&#31639;&#27861;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#26816;&#27979;&#32454;&#24494;&#30340;&#20809;&#20239;&#25925;&#38556;&#24182;&#36827;&#34892;&#20998;&#31867;&#65292;&#25552;&#39640;&#20102;&#25925;&#38556;&#20998;&#31867;&#30340;&#35745;&#31639;&#26102;&#38388;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#39640;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2306.08004</link><description>&lt;p&gt;
&#26816;&#27979;&#19982;&#20998;&#31867;&#26088;&#22312;&#39044;&#38450;&#20809;&#20239;&#31995;&#32479;&#25925;&#38556;&#30340;&#35770;&#25991;&#12290;
&lt;/p&gt;
&lt;p&gt;
Detection and classification of faults aimed at preventive maintenance of PV systems. (arXiv:2306.08004v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08004
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38543;&#26426;&#26862;&#26519;&#31639;&#27861;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#26816;&#27979;&#32454;&#24494;&#30340;&#20809;&#20239;&#25925;&#38556;&#24182;&#36827;&#34892;&#20998;&#31867;&#65292;&#25552;&#39640;&#20102;&#25925;&#38556;&#20998;&#31867;&#30340;&#35745;&#31639;&#26102;&#38388;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#39640;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20809;&#20239;&#31995;&#32479;&#30340;&#35786;&#26029;&#26088;&#22312;&#26816;&#27979;&#12289;&#23450;&#20301;&#21644;&#35782;&#21035;&#25925;&#38556;&#12290;&#35786;&#26029;&#36825;&#20123;&#25925;&#38556;&#23545;&#20110;&#20445;&#35777;&#33021;&#28304;&#29983;&#20135;&#21644;&#24310;&#38271;&#20809;&#20239;&#21457;&#30005;&#21378;&#30340;&#20351;&#29992;&#23551;&#21629;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#25991;&#29486;&#20013;&#65292;&#24050;&#32463;&#20026;&#27492;&#25552;&#20986;&#20102;&#22810;&#31181;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#30740;&#31350;&#20013;&#24456;&#23569;&#26377;&#20851;&#27880;&#32454;&#24494;&#25925;&#38556;&#30340;&#26816;&#27979;&#21644;&#19987;&#38376;&#30340;&#29305;&#24449;&#25552;&#21462;&#21644;&#20998;&#31867;&#36873;&#25321;&#36807;&#31243;&#12290;&#32454;&#24494;&#25925;&#38556;&#26159;&#19968;&#31181;&#20854;&#29305;&#24449;&#31614;&#21517;&#19982;&#20581;&#24247;&#38754;&#26495;&#30340;&#29305;&#24449;&#31614;&#21517;&#38590;&#20197;&#21306;&#21035;&#30340;&#25925;&#38556;&#12290;&#20316;&#20026;&#26816;&#27979;&#32454;&#24494;&#25925;&#38556;&#30340;&#36129;&#29486;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38543;&#26426;&#26862;&#26519;&#65288;RF&#65289;&#31639;&#27861;&#30340;&#21019;&#26032;&#26041;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#20351;&#29992;&#22797;&#26434;&#30340;&#29305;&#24449;&#25552;&#21462;&#21644;&#36873;&#25321;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#25925;&#38556;&#20998;&#31867;&#30340;&#35745;&#31639;&#26102;&#38388;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#39640;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diagnosis in PV systems aims to detect, locate and identify faults. Diagnosing these faults is vital to guarantee energy production and extend the useful life of PV power plants. In the literature, multiple machine learning approaches have been proposed for this purpose. However, few of these works have paid special attention to the detection of fine faults and the specialized process of extraction and selection of features for their classification. A fine fault is one whose characteristic signature is difficult to distinguish to that of a healthy panel. As a contribution to the detection of fine faults (especially of the snail trail type), this article proposes an innovative approach based on the Random Forest (RF) algorithm. This approach uses a complex feature extraction and selection method that improves the computational time of fault classification while maintaining high accuracy.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#32858;&#31867;&#25216;&#26415;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#22823;&#35268;&#27169;&#20809;&#20239;&#30005;&#21378;&#20013;&#26816;&#27979;&#21644;&#35782;&#21035;&#25925;&#38556;&#31867;&#22411;&#65292;&#37319;&#29992;&#21160;&#24577;&#26102;&#38388;&#35268;&#25972;&#65288;DTW&#65289;&#36317;&#31163;&#24230;&#37327;&#65292;&#33021;&#22815;&#25552;&#39640;&#32858;&#31867;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.08003</link><description>&lt;p&gt;
DTW k-means&#32858;&#31867;&#29992;&#20110;&#20809;&#20239;&#27169;&#22359;&#25925;&#38556;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
DTW k-means clustering for fault detection in photovoltaic modules. (arXiv:2306.08003v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08003
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#32858;&#31867;&#25216;&#26415;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#22823;&#35268;&#27169;&#20809;&#20239;&#30005;&#21378;&#20013;&#26816;&#27979;&#21644;&#35782;&#21035;&#25925;&#38556;&#31867;&#22411;&#65292;&#37319;&#29992;&#21160;&#24577;&#26102;&#38388;&#35268;&#25972;&#65288;DTW&#65289;&#36317;&#31163;&#24230;&#37327;&#65292;&#33021;&#22815;&#25552;&#39640;&#32858;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20809;&#20239;&#33021;&#28304;&#22312;&#20840;&#29699;&#30340;&#24212;&#29992;&#22686;&#21152;&#34920;&#26126;&#65292;&#20809;&#20239;&#30005;&#21378;&#30340;&#23551;&#21629;&#21644;&#32500;&#25252;&#30452;&#25509;&#20381;&#36182;&#20110;&#24555;&#36895;&#26816;&#27979;&#20809;&#20239;&#30005;&#21378;&#30340;&#20005;&#37325;&#25925;&#38556;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#26816;&#27979;&#38382;&#39064;&#65292;&#25991;&#29486;&#20013;&#25552;&#20986;&#20102;&#22522;&#20110;&#25968;&#25454;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#20043;&#21069;&#30340;&#35299;&#20915;&#26041;&#26696;&#21482;&#32771;&#34385;&#20102;&#19968;&#20010;&#25110;&#23569;&#25968;&#20960;&#20010;&#25925;&#38556;&#30340;&#20855;&#20307;&#34892;&#20026;&#12290;&#20854;&#20013;&#22823;&#22810;&#25968;&#26041;&#27861;&#21487;&#20197;&#34987;&#24402;&#20026;&#30417;&#30563;&#23398;&#20064;&#65292;&#38656;&#35201;&#22823;&#37327;&#26631;&#35760;&#21162;&#21147;&#65288;&#27599;&#31181;&#25216;&#26415;&#20013;&#26126;&#30830;&#35782;&#21035;&#30340;&#25925;&#38556;&#31867;&#22411;&#65289;&#12290;&#27492;&#22806;&#65292;&#22823;&#22810;&#25968;&#26041;&#27861;&#22312;PV&#30005;&#27744;&#25110;&#19968;&#20010;PV&#27169;&#22359;&#20013;&#39564;&#35777;&#12290;&#36825;&#22312;&#32771;&#34385;&#23427;&#20204;&#30340;&#22797;&#26434;&#24615;&#30340;&#22823;&#35268;&#27169;PV&#30005;&#21378;&#20013;&#24456;&#38590;&#24212;&#29992;&#12290;&#30456;&#21453;&#65292;&#19968;&#20123;&#22522;&#20110;&#25968;&#25454;&#30340;&#26080;&#30417;&#30563;&#30693;&#21517;&#26041;&#27861;&#23581;&#35797;&#26816;&#27979;&#24322;&#24120;&#65292;&#20294;&#19981;&#33021;&#31934;&#30830;&#22320;&#35782;&#21035;&#25925;&#38556;&#31867;&#22411;&#12290;&#20854;&#20013;&#34920;&#29616;&#26368;&#22909;&#30340;&#26041;&#27861;&#25104;&#21151;&#22320;&#23558;&#20581;&#24247;&#38754;&#26495;&#26377;&#25928;&#22320;&#20998;&#32452;&#24182;&#23558;&#20854;&#19982;&#25925;&#38556;&#38754;&#26495;&#20998;&#24320;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26080;&#30417;&#30563;&#32858;&#31867;&#25216;&#26415;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22823;&#35268;&#27169;PV&#30005;&#21378;&#30340;&#19968;&#33324;&#25925;&#38556;&#26816;&#27979;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#23545;PV&#30005;&#21378;&#20013;&#27599;&#20010;&#38754;&#26495;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#24212;&#29992;&#20102;k-means&#32858;&#31867;&#31639;&#27861;&#12290;&#20026;&#20102;&#25552;&#39640;&#32858;&#31867;&#24615;&#33021;&#65292;&#37319;&#29992;&#20102;&#19968;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#30456;&#20284;&#24615;&#24230;&#37327;&#65292;&#21363;&#21160;&#24577;&#26102;&#38388;&#35268;&#25972;&#65288;DTW&#65289;&#36317;&#31163;&#65292;&#20197;&#20811;&#26381;PV&#38754;&#26495;&#23545;&#19981;&#21516;&#29615;&#22659;&#21644;&#25805;&#20316;&#26465;&#20214;&#30340;&#21709;&#24212;&#30340;&#21464;&#24322;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#25104;&#21151;&#26816;&#27979;PV&#30005;&#21378;&#20013;&#30340;&#25925;&#38556;&#24182;&#35782;&#21035;&#20854;&#31867;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
The increase in the use of photovoltaic (PV) energy in the world has shown that the useful life and maintenance of a PV plant directly depend on theability to quickly detect severe faults on a PV plant. To solve this problem of detection, data based approaches have been proposed in the literature.However, these previous solutions consider only specific behavior of one or few faults. Most of these approaches can be qualified as supervised, requiring an enormous labelling effort (fault types clearly identified in each technology). In addition, most of them are validated in PV cells or one PV module. That is hardly applicable in large-scale PV plants considering their complexity. Alternatively, some unsupervised well-known approaches based on data try to detect anomalies but are not able to identify precisely the type of fault. The most performant of these methods do manage to efficiently group healthy panels and separate them from faulty panels. In that way, this article presents an unsu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#39532;&#23572;&#21487;&#22827;&#31995;&#32479;&#30340;&#24418;&#24335;&#21270;&#20027;&#20041;&#23545;&#20027;&#21160;&#23398;&#20064;&#36827;&#34892;&#27010;&#36848;&#65292;&#23558;&#20027;&#21160;&#23398;&#20064;&#36807;&#31243;&#20316;&#20026;&#19968;&#20010;&#37096;&#20998;&#21487;&#35266;&#23519;&#30340;&#39532;&#23572;&#21487;&#22827;&#31995;&#32479;&#30340;&#25972;&#20307;&#22788;&#29702;&#65292;&#24182;&#19988;&#38416;&#36848;&#22914;&#20309;&#23558;&#26597;&#35810;&#12289;&#25968;&#25454;&#38598;&#22686;&#24378;&#12289;&#22870;&#21169;&#26356;&#26032;&#31561;&#36807;&#31243;&#35270;&#20026;&#39532;&#23572;&#21487;&#22827;&#31995;&#32479;&#20013;&#20803;&#29366;&#24577;&#20043;&#38388;&#30340;&#36716;&#31227;&#12290;</title><link>http://arxiv.org/abs/2306.08001</link><description>&lt;p&gt;
&#20027;&#21160;&#26597;&#35810;&#30340;&#39532;&#23572;&#21487;&#22827;&#24418;&#24335;&#20027;&#20041;
&lt;/p&gt;
&lt;p&gt;
A Markovian Formalism for Active Querying. (arXiv:2306.08001v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08001
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#39532;&#23572;&#21487;&#22827;&#31995;&#32479;&#30340;&#24418;&#24335;&#21270;&#20027;&#20041;&#23545;&#20027;&#21160;&#23398;&#20064;&#36827;&#34892;&#27010;&#36848;&#65292;&#23558;&#20027;&#21160;&#23398;&#20064;&#36807;&#31243;&#20316;&#20026;&#19968;&#20010;&#37096;&#20998;&#21487;&#35266;&#23519;&#30340;&#39532;&#23572;&#21487;&#22827;&#31995;&#32479;&#30340;&#25972;&#20307;&#22788;&#29702;&#65292;&#24182;&#19988;&#38416;&#36848;&#22914;&#20309;&#23558;&#26597;&#35810;&#12289;&#25968;&#25454;&#38598;&#22686;&#24378;&#12289;&#22870;&#21169;&#26356;&#26032;&#31561;&#36807;&#31243;&#35270;&#20026;&#39532;&#23572;&#21487;&#22827;&#31995;&#32479;&#20013;&#20803;&#29366;&#24577;&#20043;&#38388;&#30340;&#36716;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#21160;&#23398;&#20064;&#31639;&#27861;&#26159;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#20013;&#26368;&#36817;&#36827;&#23637;&#30340;&#37325;&#35201;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#35813;&#39046;&#22495;&#30340;&#30740;&#31350;&#21464;&#21270;&#24456;&#22823;&#65292;&#32570;&#20047;&#25972;&#20307;&#24615;&#30340;&#32452;&#32455;&#32467;&#26500;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#39532;&#23572;&#21487;&#22827;&#31995;&#32479;&#30340;&#24418;&#24335;&#21270;&#20027;&#20041;&#65292;&#29992;&#20110;&#23545;&#20027;&#21160;&#23398;&#20064;&#39046;&#22495;&#36827;&#34892;&#27010;&#36848;&#65292;&#24182;&#36890;&#36807;&#25991;&#29486;&#32508;&#36848;&#26469;&#23637;&#31034;&#25105;&#20204;&#25152;&#25552;&#20986;&#30340;&#24418;&#24335;&#21270;&#20027;&#20041;&#30340;&#32452;&#32455;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#24418;&#24335;&#21270;&#20027;&#20041;&#23558;&#20027;&#21160;&#23398;&#20064;&#36807;&#31243;&#20316;&#20026;&#19968;&#20010;&#37096;&#20998;&#21487;&#35266;&#23519;&#30340;&#39532;&#23572;&#21487;&#22827;&#31995;&#32479;&#30340;&#25972;&#20307;&#26469;&#22788;&#29702;&#12290;&#25105;&#20204;&#20855;&#20307;&#27010;&#36848;&#20102;&#26597;&#35810;&#12289;&#25968;&#25454;&#38598;&#22686;&#24378;&#12289;&#22870;&#21169;&#26356;&#26032;&#20197;&#21450;&#20854;&#20182;&#20027;&#21160;&#23398;&#20064;&#26041;&#38754;&#22914;&#20309;&#35270;&#20026;&#39532;&#23572;&#21487;&#22827;&#31995;&#32479;&#20013;&#20803;&#29366;&#24577;&#20043;&#38388;&#30340;&#36716;&#31227;&#65292;&#24182;&#25351;&#23548;&#20854;&#20182;&#20027;&#21160;&#23398;&#20064;&#26041;&#38754;&#22914;&#20309;&#36866;&#24212;&#25105;&#20204;&#30340;&#24418;&#24335;&#21270;&#20027;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
Active learning algorithms have been an integral part of recent advances in artificial intelligence. However, the research in the field is widely varying and lacks an overall organizing leans. We outline a Markovian formalism for the field of active learning and survey the literature to demonstrate the organizing capability of our proposed formalism. Our formalism takes a partially observable Markovian system approach to the active learning process as a whole. We specifically outline how querying, dataset augmentation, reward updates, and other aspects of active learning can be viewed as a transition between meta-states in a Markovian system, and give direction into how other aspects of active learning can fit into our formalism.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36827;&#34892;&#20102;&#25968;&#23383;&#30149;&#29702;&#22270;&#20687;&#20013;&#24212;&#29992;&#20154;&#24037;&#26234;&#33021;&#30340;&#25152;&#26377;&#30149;&#29702;&#23398;&#39046;&#22495;&#30340;&#35786;&#26029;&#20934;&#30830;&#24230;&#30340;&#31995;&#32479;&#32508;&#36848;&#21644;Meta&#20998;&#26512;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20154;&#24037;&#26234;&#33021;&#22312;&#25968;&#23383;&#30149;&#29702;&#23398;&#20013;&#21462;&#24471;&#20102;&#39640;&#24230;&#30340;&#20934;&#30830;&#24230;&#65292;&#26159;&#21487;&#34892;&#30340;&#36741;&#21161;&#35786;&#26029;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2306.07999</link><description>&lt;p&gt;
&#25968;&#23383;&#30149;&#29702;&#23398;&#20013;&#20154;&#24037;&#26234;&#33021;&#30340;&#35786;&#26029;&#27979;&#35797;&#20934;&#30830;&#24230;&#65306;&#31995;&#32479;&#32508;&#36848;&#12289;Meta&#20998;&#26512;&#21644;&#36136;&#37327;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Diagnostic test accuracy (DTA) of artificial intelligence in digital pathology: a systematic review, meta-analysis and quality assessment. (arXiv:2306.07999v1 [physics.med-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07999
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36827;&#34892;&#20102;&#25968;&#23383;&#30149;&#29702;&#22270;&#20687;&#20013;&#24212;&#29992;&#20154;&#24037;&#26234;&#33021;&#30340;&#25152;&#26377;&#30149;&#29702;&#23398;&#39046;&#22495;&#30340;&#35786;&#26029;&#20934;&#30830;&#24230;&#30340;&#31995;&#32479;&#32508;&#36848;&#21644;Meta&#20998;&#26512;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20154;&#24037;&#26234;&#33021;&#22312;&#25968;&#23383;&#30149;&#29702;&#23398;&#20013;&#21462;&#24471;&#20102;&#39640;&#24230;&#30340;&#20934;&#30830;&#24230;&#65292;&#26159;&#21487;&#34892;&#30340;&#36741;&#21161;&#35786;&#26029;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30830;&#20445;&#20020;&#24202;&#20351;&#29992;&#20043;&#21069;AI&#27169;&#22411;&#30340;&#35786;&#26029;&#34920;&#29616;&#26159;&#20851;&#38190;&#65292;&#20197;&#30830;&#20445;&#36825;&#20123;&#25216;&#26415;&#30340;&#23433;&#20840;&#21644;&#25104;&#21151;&#30340;&#37319;&#29992;&#12290;&#36817;&#24180;&#26469;&#65292;&#25253;&#36947;&#24212;&#29992;&#20110;&#25968;&#23383;&#30149;&#29702;&#23398;&#22270;&#20687;&#36827;&#34892;&#35786;&#26029;&#30446;&#30340;&#30340;AI&#30740;&#31350;&#25968;&#37327;&#36805;&#36895;&#22686;&#21152;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#25552;&#20379;&#25968;&#23383;&#30149;&#29702;&#23398;&#20013;AI&#30340;&#35786;&#26029;&#20934;&#30830;&#24230;&#30340;&#27010;&#36848;&#65292;&#28085;&#30422;&#20102;&#25152;&#26377;&#30149;&#29702;&#23398;&#39046;&#22495;&#12290;&#36825;&#39033;&#31995;&#32479;&#24615;&#32508;&#36848;&#21644;Meta&#20998;&#26512;&#21253;&#25324;&#20351;&#29992;&#20219;&#20309;&#31867;&#22411;&#30340;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#20110;&#20219;&#20309;&#30142;&#30149;&#31867;&#22411;&#30340;WSI&#22270;&#20687;&#30340;&#35786;&#26029;&#20934;&#30830;&#24615;&#30740;&#31350;&#12290;&#21442;&#32771;&#26631;&#20934;&#26159;&#36890;&#36807;&#32452;&#32455;&#30149;&#29702;&#23398;&#35780;&#20272;&#21644;/&#25110;&#20813;&#30123;&#32452;&#21270;&#35786;&#26029;&#12290;&#25628;&#32034;&#22312;2022&#24180;6&#26376;&#22312;PubMed&#12289;EMBASE&#21644;CENTRAL&#20013;&#36827;&#34892;&#12290;&#22312;2976&#39033;&#30740;&#31350;&#20013;&#65292;&#26377;100&#39033;&#32435;&#20837;&#32508;&#36848;&#65292;48&#39033;&#32435;&#20837;&#23436;&#25972;&#30340;Meta&#20998;&#26512;&#12290;&#20351;&#29992;QUADAS-2&#24037;&#20855;&#35780;&#20272;&#20102;&#20559;&#20506;&#39118;&#38505;&#21644;&#36866;&#29992;&#24615;&#30340;&#20851;&#27880;&#28857;&#12290;&#25968;&#25454;&#25552;&#21462;&#30001;&#20004;&#20010;&#35843;&#26597;&#21592;&#36827;&#34892;&#65292;&#24182;&#36827;&#34892;&#20102;Meta&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ensuring diagnostic performance of AI models before clinical use is key to the safe and successful adoption of these technologies. Studies reporting AI applied to digital pathology images for diagnostic purposes have rapidly increased in number in recent years. The aim of this work is to provide an overview of the diagnostic accuracy of AI in digital pathology images from all areas of pathology. This systematic review and meta-analysis included diagnostic accuracy studies using any type of artificial intelligence applied to whole slide images (WSIs) in any disease type. The reference standard was diagnosis through histopathological assessment and / or immunohistochemistry. Searches were conducted in PubMed, EMBASE and CENTRAL in June 2022. We identified 2976 studies, of which 100 were included in the review and 48 in the full meta-analysis. Risk of bias and concerns of applicability were assessed using the QUADAS-2 tool. Data extraction was conducted by two investigators and meta-analy
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#27604;&#27880;&#24847;&#21147;&#24230;&#37327;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#37492;&#23450;&#26089;&#26399;&#21360;&#21047;&#21697;&#20013;&#30340;&#26410;&#30693;&#21360;&#21047;&#21830;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23545;&#23383;&#24418;&#24494;&#23567;&#24046;&#24322;&#25935;&#24863;&#65292;&#23545;&#21508;&#31181;&#28151;&#28102;&#22122;&#22768;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#38543;&#26426;&#25968;&#25454;&#21512;&#25104;&#36807;&#31243;&#65292;&#25104;&#21151;&#25552;&#39640;&#20102;&#35813;&#26102;&#26399;&#21360;&#21047;&#21697;&#20013;&#30340;&#25439;&#22351;&#31867;&#22411;&#21360;&#35760;&#21305;&#37197;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.07998</link><description>&lt;p&gt;
&#23545;&#27604;&#27880;&#24847;&#21147;&#32593;&#32476;&#29992;&#20110;&#26089;&#26399;&#21360;&#21047;&#21697;&#30340;&#29305;&#24449;&#37492;&#23450;
&lt;/p&gt;
&lt;p&gt;
Contrastive Attention Networks for Attribution of Early Modern Print. (arXiv:2306.07998v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07998
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#27604;&#27880;&#24847;&#21147;&#24230;&#37327;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#37492;&#23450;&#26089;&#26399;&#21360;&#21047;&#21697;&#20013;&#30340;&#26410;&#30693;&#21360;&#21047;&#21830;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23545;&#23383;&#24418;&#24494;&#23567;&#24046;&#24322;&#25935;&#24863;&#65292;&#23545;&#21508;&#31181;&#28151;&#28102;&#22122;&#22768;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#38543;&#26426;&#25968;&#25454;&#21512;&#25104;&#36807;&#31243;&#65292;&#25104;&#21151;&#25552;&#39640;&#20102;&#35813;&#26102;&#26399;&#21360;&#21047;&#21697;&#20013;&#30340;&#25439;&#22351;&#31867;&#22411;&#21360;&#35760;&#21305;&#37197;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#35782;&#21035;&#26089;&#26399;&#65288;&#22823;&#32422;1500-1800&#24180;&#65289;&#33521;&#22269;&#21360;&#21047;&#20070;&#31821;&#20013;&#30340;&#26410;&#30693;&#21360;&#21047;&#21830;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#21305;&#37197;&#21311;&#21517;&#21360;&#21047;&#20070;&#31821;&#20013;&#30340;&#21807;&#19968;&#25439;&#22351;&#23383;&#31526;&#31867;&#22411;&#21360;&#35760;&#19982;&#24050;&#30693;&#21360;&#21047;&#21830;&#20316;&#21697;&#65292;&#20197;&#25552;&#20379;&#20854;&#26469;&#28304;&#30340;&#35777;&#25454;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#27880;&#24847;&#21147;&#24230;&#37327;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#20197;&#35782;&#21035;&#21516;&#19968;&#23383;&#31526;&#22270;&#20687;&#23545;&#20013;&#30340;&#30456;&#20284;&#25439;&#20260;&#65292;&#23545;&#23383;&#24418;&#30340;&#24494;&#23567;&#24046;&#24322;&#25935;&#24863;&#65292;&#20294;&#23545;&#19982;&#25968;&#23383;&#21270;&#21382;&#21490;&#20070;&#31821;&#30456;&#20851;&#30340;&#21508;&#31181;&#28151;&#28102;&#22122;&#22768;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#20026;&#20102;&#20811;&#26381;&#21463;&#30417;&#30563;&#25968;&#25454;&#21294;&#20047;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#38543;&#26426;&#25968;&#25454;&#21512;&#25104;&#36807;&#31243;&#65292;&#26088;&#22312;&#27169;&#25311;&#26089;&#26399;&#21360;&#21047;&#36807;&#31243;&#24341;&#36215;&#30340;&#24367;&#26354;&#12289;&#26029;&#35010;&#21644;&#27833;&#22696;&#21464;&#21270;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25104;&#21151;&#25552;&#39640;&#20102;&#35813;&#26102;&#26399;&#21360;&#21047;&#21697;&#20013;&#30340;&#25439;&#22351;&#31867;&#22411;&#21360;&#35760;&#21305;&#37197;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we develop machine learning techniques to identify unknown printers in early modern (c.~1500--1800) English printed books. Specifically, we focus on matching uniquely damaged character type-imprints in anonymously printed books to works with known printers in order to provide evidence of their origins. Until now, this work has been limited to manual investigations by analytical bibliographers. We present a Contrastive Attention-based Metric Learning approach to identify similar damage across character image pairs, which is sensitive to very subtle differences in glyph shapes, yet robust to various confounding sources of noise associated with digitized historical books. To overcome the scarce amount of supervised data, we design a random data synthesis procedure that aims to simulate bends, fractures, and inking variations induced by the early printing process. Our method successfully improves downstream damaged type-imprint matching among printed works from this period, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#20462;&#22797;&#31070;&#32463;&#32593;&#32476;&#38169;&#35823;&#30340;&#26041;&#27861;&#65292;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#23618;&#30340;&#21487;&#25191;&#34892;&#35821;&#20041;&#65292;&#24182;&#19987;&#27880;&#20110;&#23454;&#36341;&#20013;&#24120;&#35265;&#30340;&#22235;&#31181;&#38169;&#35823;&#12290;</title><link>http://arxiv.org/abs/2306.07995</link><description>&lt;p&gt;
&#22522;&#20110;&#35821;&#20041;&#30340;&#31070;&#32463;&#32593;&#32476;&#20462;&#22797;
&lt;/p&gt;
&lt;p&gt;
Semantic-Based Neural Network Repair. (arXiv:2306.07995v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07995
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#20462;&#22797;&#31070;&#32463;&#32593;&#32476;&#38169;&#35823;&#30340;&#26041;&#27861;&#65292;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#23618;&#30340;&#21487;&#25191;&#34892;&#35821;&#20041;&#65292;&#24182;&#19987;&#27880;&#20110;&#23454;&#36341;&#20013;&#24120;&#35265;&#30340;&#22235;&#31181;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#31070;&#32463;&#32593;&#32476;&#24050;&#32463;&#24191;&#27867;&#24212;&#29992;&#20110;&#22810;&#20010;&#39046;&#22495;&#65292;&#20854;&#20013;&#21253;&#25324;&#35768;&#22810;&#23433;&#20840;&#20851;&#38190;&#31995;&#32479;&#12290;&#31070;&#32463;&#32593;&#32476;&#26159;&#36890;&#36807;&#22312; TensorFlow &#21644; PyTorch &#31561;&#26694;&#26550;&#20013;&#36827;&#34892;&#32534;&#31243;&#26500;&#24314;&#65288;&#21644;&#35757;&#32451;&#65289;&#30340;&#12290;&#24320;&#21457;&#20154;&#21592;&#21487;&#20197;&#24212;&#29992;&#20016;&#23500;&#30340;&#39044;&#23450;&#20041;&#23618;&#25163;&#21160;&#32534;&#20889;&#31070;&#32463;&#32593;&#32476;&#25110;&#36890;&#36807; AutoML &#33258;&#21160;&#29983;&#25104;&#32593;&#32476;&#12290;&#30001;&#20110;&#24517;&#39035;&#28385;&#36275;&#20351;&#29992;&#36825;&#20123;&#23618;&#30340;&#38750;&#24179;&#20961;&#32422;&#26463;&#26465;&#20214;&#65292;&#25152;&#20197;&#20351;&#29992;&#19981;&#21516;&#23618;&#26469;&#32452;&#21512;&#31070;&#32463;&#32593;&#32476;&#23481;&#26131;&#20986;&#29616;&#38169;&#35823;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#20462;&#22797;&#38169;&#35823;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#12290;&#25361;&#25112;&#22312;&#20110;&#35782;&#21035;&#20986;&#23545;&#32593;&#32476;&#36827;&#34892;&#26368;&#23567;&#20462;&#25913;&#20197;&#20351;&#20854;&#21464;&#20026;&#26377;&#25928;&#30340;&#20462;&#25913;&#12290;&#20462;&#25913;&#19968;&#23618;&#21487;&#33021;&#20250;&#23545;&#38543;&#21518;&#30340;&#23618;&#20135;&#29983;&#32423;&#32852;&#25928;&#24212;&#65292;&#22240;&#27492;&#25105;&#20204;&#30340;&#26041;&#27861;&#24517;&#39035;&#36882;&#24402;&#22320;&#25628;&#32034;&#20197;&#35782;&#21035;&#8220;&#20840;&#23616;&#8221;&#26368;&#23567;&#20462;&#25913;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#23618;&#30340;&#21487;&#25191;&#34892;&#35821;&#20041;&#65292;&#24182;&#19987;&#27880;&#20110;&#23454;&#36341;&#20013;&#24120;&#35265;&#30340;&#22235;&#31181;&#38169;&#35823;&#12290;&#25105;&#20204;&#23545;&#25105;&#20204;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, neural networks have spread into numerous fields including many safety-critical systems. Neural networks are built (and trained) by programming in frameworks such as TensorFlow and PyTorch. Developers apply a rich set of pre-defined layers to manually program neural networks or to automatically generate them (e.g., through AutoML). Composing neural networks with different layers is error-prone due to the non-trivial constraints that must be satisfied in order to use those layers. In this work, we propose an approach to automatically repair erroneous neural networks. The challenge is in identifying a minimal modification to the network so that it becomes valid. Modifying a layer might have cascading effects on subsequent layers and thus our approach must search recursively to identify a "globally" minimal modification. Our approach is based on an executable semantics of deep learning layers and focuses on four kinds of errors which are common in practice. We evaluate our appro
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20026;&#27599;&#20010;&#35789;&#27719;&#20998;&#37197;&#21333;&#29420;&#30340;&#39118;&#26684;&#21521;&#37327;&#26469;&#23545;&#25991;&#26412;&#36827;&#34892;&#39118;&#26684;&#36716;&#25442;&#65292;&#24182;&#24341;&#20837;&#22522;&#20110;&#25945;&#24072;-&#23398;&#29983;&#23398;&#20064;&#30340;&#23545;&#25239;&#24615;&#22521;&#35757;&#26694;&#26550;&#65292;&#20197;&#25552;&#39640;&#22521;&#35757;&#31283;&#23450;&#24615;&#65292;&#23454;&#29616;&#20102;&#21452;&#39118;&#26684;&#36716;&#25442;&#21644;&#22810;&#39118;&#26684;&#36716;&#25442;&#20004;&#31181;&#24773;&#20917;&#19979;&#30340;&#26126;&#26174;&#25552;&#39640;&#30340;&#39118;&#26684;&#36716;&#25442;&#20934;&#30830;&#24615;&#21644;&#20869;&#23481;&#20445;&#30041;&#12290;</title><link>http://arxiv.org/abs/2306.07994</link><description>&lt;p&gt;
MSSRNet: &#26080;&#30417;&#30563;&#25991;&#26412;&#39118;&#26684;&#36716;&#25442;&#20013;&#30340;&#39034;&#24207;&#39118;&#26684;&#34920;&#31034;&#25805;&#20316;
&lt;/p&gt;
&lt;p&gt;
MSSRNet: Manipulating Sequential Style Representation for Unsupervised Text Style Transfer. (arXiv:2306.07994v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07994
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20026;&#27599;&#20010;&#35789;&#27719;&#20998;&#37197;&#21333;&#29420;&#30340;&#39118;&#26684;&#21521;&#37327;&#26469;&#23545;&#25991;&#26412;&#36827;&#34892;&#39118;&#26684;&#36716;&#25442;&#65292;&#24182;&#24341;&#20837;&#22522;&#20110;&#25945;&#24072;-&#23398;&#29983;&#23398;&#20064;&#30340;&#23545;&#25239;&#24615;&#22521;&#35757;&#26694;&#26550;&#65292;&#20197;&#25552;&#39640;&#22521;&#35757;&#31283;&#23450;&#24615;&#65292;&#23454;&#29616;&#20102;&#21452;&#39118;&#26684;&#36716;&#25442;&#21644;&#22810;&#39118;&#26684;&#36716;&#25442;&#20004;&#31181;&#24773;&#20917;&#19979;&#30340;&#26126;&#26174;&#25552;&#39640;&#30340;&#39118;&#26684;&#36716;&#25442;&#20934;&#30830;&#24615;&#21644;&#20869;&#23481;&#20445;&#30041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#25991;&#26412;&#39118;&#26684;&#36716;&#31227;&#20219;&#21153;&#26088;&#22312;&#23558;&#25991;&#26412;&#37325;&#20889;&#20026;&#30446;&#26631;&#39118;&#26684;&#65292;&#21516;&#26102;&#20445;&#30041;&#20854;&#20027;&#35201;&#20869;&#23481;&#12290;&#20256;&#32479;&#26041;&#27861;&#20381;&#36182;&#20110;&#20351;&#29992;&#22266;&#23450;&#22823;&#23567;&#30340;&#21521;&#37327;&#26469;&#35843;&#33410;&#25991;&#26412;&#39118;&#26684;&#65292;&#36825;&#24456;&#38590;&#20934;&#30830;&#20256;&#36798;&#27599;&#20010;&#21333;&#29420;&#20196;&#29260;&#30340;&#39118;&#26684;&#24378;&#24230;&#12290;&#20107;&#23454;&#19978;&#65292;&#25991;&#26412;&#30340;&#27599;&#20010;&#20196;&#29260;&#37117;&#21253;&#21547;&#19981;&#21516;&#30340;&#39118;&#26684;&#24378;&#24230;&#65292;&#24182;&#23545;&#25972;&#20307;&#39118;&#26684;&#20135;&#29983;&#19981;&#21516;&#30340;&#36129;&#29486;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#36890;&#36807;&#20026;&#25991;&#26412;&#20013;&#30340;&#27599;&#20010;&#20196;&#29260;&#20998;&#37197;&#21333;&#29420;&#30340;&#39118;&#26684;&#21521;&#37327;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20801;&#35768;&#23545;&#39118;&#26684;&#24378;&#24230;&#36827;&#34892;&#32454;&#31890;&#24230;&#30340;&#25511;&#21046;&#21644;&#25805;&#20316;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#22522;&#20110;&#25945;&#24072;-&#23398;&#29983;&#23398;&#20064;&#30340;&#23545;&#25239;&#24615;&#22521;&#35757;&#26694;&#26550;&#65292;&#20197;&#22686;&#24378;&#22521;&#35757;&#31283;&#23450;&#24615;&#24182;&#20943;&#36731;&#39640;&#32500;&#20248;&#21270;&#30340;&#22797;&#26434;&#24615;&#12290;&#25105;&#20204;&#23454;&#39564;&#30340;&#32467;&#26524;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21452;&#39118;&#26684;&#36716;&#25442;&#21644;&#22810;&#39118;&#26684;&#36716;&#25442;&#35774;&#32622;&#20013;&#65292;&#20855;&#26377;&#26126;&#26174;&#25552;&#39640;&#30340;&#39118;&#26684;&#36716;&#25442;&#20934;&#30830;&#24615;&#21644;&#20869;&#23481;&#20445;&#30041;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unsupervised text style transfer task aims to rewrite a text into target style while preserving its main content. Traditional methods rely on the use of a fixed-sized vector to regulate text style, which is difficult to accurately convey the style strength for each individual token. In fact, each token of a text contains different style intensity and makes different contribution to the overall style. Our proposed method addresses this issue by assigning individual style vector to each token in a text, allowing for fine-grained control and manipulation of the style strength. Additionally, an adversarial training framework integrated with teacher-student learning is introduced to enhance training stability and reduce the complexity of high-dimensional optimization. The results of our experiments demonstrate the efficacy of our method in terms of clearly improved style transfer accuracy and content preservation in both two-style transfer and multi-style transfer settings.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#25239;&#22270;&#20687;&#37325;&#26500;&#21450;&#26816;&#27979;&#26694;&#26550;&#26469;&#20445;&#25252;&#35270;&#35273;&#24863;&#30693;&#25512;&#33616;&#31995;&#32479;&#65292;&#33021;&#22815;&#38450;&#24481;&#23616;&#37096;&#25200;&#21160;&#20026;&#29305;&#24449;&#30340;&#23545;&#25239;&#25915;&#20987;&#24182;&#19988;&#33021;&#22815;&#22312;&#24178;&#20928;&#21644;&#23545;&#25239;&#24615;&#22270;&#20687;&#19978;&#36827;&#34892;&#35757;&#32451;&#26469;&#26816;&#27979;&#23545;&#25239;&#24615;&#22270;&#20687;&#12290;</title><link>http://arxiv.org/abs/2306.07992</link><description>&lt;p&gt;
&#23433;&#20840;&#30340;&#35270;&#35273;&#24863;&#30693;&#25512;&#33616;&#31995;&#32479;&#65306;&#19968;&#31181;&#23545;&#25239;&#22270;&#20687;&#37325;&#26500;&#21450;&#26816;&#27979;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Securing Visually-Aware Recommender Systems: An Adversarial Image Reconstruction and Detection Framework. (arXiv:2306.07992v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07992
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#25239;&#22270;&#20687;&#37325;&#26500;&#21450;&#26816;&#27979;&#26694;&#26550;&#26469;&#20445;&#25252;&#35270;&#35273;&#24863;&#30693;&#25512;&#33616;&#31995;&#32479;&#65292;&#33021;&#22815;&#38450;&#24481;&#23616;&#37096;&#25200;&#21160;&#20026;&#29305;&#24449;&#30340;&#23545;&#25239;&#25915;&#20987;&#24182;&#19988;&#33021;&#22815;&#22312;&#24178;&#20928;&#21644;&#23545;&#25239;&#24615;&#22270;&#20687;&#19978;&#36827;&#34892;&#35757;&#32451;&#26469;&#26816;&#27979;&#23545;&#25239;&#24615;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#23500;&#21547;&#22270;&#29255;&#31561;&#35270;&#35273;&#25968;&#25454;&#19982;&#29289;&#21697;&#20851;&#32852;&#24230;&#22686;&#21152;&#65292;&#35270;&#35273;&#24863;&#30693;&#25512;&#33616;&#31995;&#32479;&#65288;VARS&#65289;&#24050;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#19981;&#21516;&#24212;&#29992;&#39046;&#22495;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;VARS&#26131;&#21463;&#21040;&#29289;&#21697;-&#22270;&#20687;&#23545;&#25239;&#25915;&#20987;&#30340;&#25915;&#20987;&#65292;&#36825;&#20123;&#25915;&#20987;&#21521;&#19982;&#36825;&#20123;&#29289;&#21697;&#20851;&#32852;&#30340;&#24178;&#20928;&#22270;&#20687;&#28155;&#21152;&#20154;&#31867;&#26080;&#27861;&#24863;&#30693;&#30340;&#25200;&#21160;&#12290;&#23545;VARS&#30340;&#25915;&#20987;&#20026;&#24191;&#27867;&#20351;&#29992;VARS&#30340;&#35768;&#22810;&#24212;&#29992;&#65288;&#22914;&#30005;&#23376;&#21830;&#21153;&#21644;&#31038;&#20132;&#32593;&#32476;&#65289;&#24102;&#26469;&#26032;&#30340;&#23433;&#20840;&#25361;&#25112;&#12290;&#22914;&#20309;&#20445;&#25252;VARS&#20813;&#21463;&#27492;&#31867;&#23545;&#25239;&#25915;&#20987;&#25104;&#20026;&#19968;&#20010;&#20851;&#38190;&#30340;&#38382;&#39064;&#12290;&#30446;&#21069;&#65292;&#23578;&#32570;&#20047;&#31995;&#32479;&#22320;&#30740;&#31350;&#22914;&#20309;&#35774;&#35745;&#38024;&#23545;VARS&#35270;&#35273;&#25915;&#20987;&#30340;&#23433;&#20840;&#38450;&#24481;&#31574;&#30053;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#25239;&#22270;&#20687;&#37325;&#26500;&#21450;&#26816;&#27979;&#26694;&#26550;&#26469;&#20445;&#25252;VARS&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#21516;&#26102;(1)&#36890;&#36807;&#22522;&#20110;&#20840;&#23616;&#35270;&#35273;&#20256;&#36755;&#30340;&#22270;&#20687;&#37325;&#26500;&#26469;&#38450;&#24481;&#20197;&#23616;&#37096;&#25200;&#21160;&#20026;&#29305;&#24449;&#30340;&#23545;&#25239;&#25915;&#20987;&#65292;(2)&#20351;&#29992;&#22312;&#23569;&#37327;&#24178;&#20928;&#21644;&#23545;&#25239;&#24615;&#22270;&#20687;&#19978;&#35757;&#32451;&#30340;&#26816;&#27979;&#27169;&#22411;&#26469;&#26816;&#27979;&#23545;&#25239;&#24615;&#22270;&#20687;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#33021;&#22815;&#26377;&#25928;&#22320;&#38450;&#24481;&#21508;&#31181;&#29289;&#21697;-&#22270;&#20687;&#23545;&#25239;&#25915;&#20987;&#23545;VARS&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
With rich visual data, such as images, becoming readily associated with items, visually-aware recommendation systems (VARS) have been widely used in different applications. Recent studies have shown that VARS are vulnerable to item-image adversarial attacks, which add human-imperceptible perturbations to the clean images associated with those items. Attacks on VARS pose new security challenges to a wide range of applications such as e-Commerce and social networks where VARS are widely used. How to secure VARS from such adversarial attacks becomes a critical problem. Currently, there is still a lack of systematic study on how to design secure defense strategies against visual attacks on VARS. In this paper, we attempt to fill this gap by proposing an adversarial image reconstruction and detection framework to secure VARS. Our proposed method can simultaneously (1) secure VARS from adversarial attacks characterized by local perturbations by image reconstruction based on global vision tra
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#28151;&#21512;&#27169;&#22411;&#30340;&#35686;&#25253;&#31995;&#32479;&#65292;&#22312;&#35299;&#20915;&#21487;&#25193;&#23637;&#24615;&#12289;&#25968;&#25454;&#24322;&#26500;&#24615;&#12289;&#21487;&#27867;&#21270;&#24615;&#21644;&#21487;&#32500;&#25252;&#24615;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#12290;&#30333;&#21517;&#21333;&#26426;&#21046;&#26377;&#25928;&#20943;&#23569;&#20102;&#35823;&#25253;&#35686;&#25253;&#65292;&#26410;&#26469;&#30340;&#24037;&#20316;&#23558;&#21253;&#25324;&#26356;&#22810;&#30340;&#29305;&#24449;&#24037;&#31243;&#21644;&#36755;&#20837;&#25968;&#25454;&#65292;&#20197;&#21450;&#21253;&#25324;&#20154;&#31867;&#21453;&#39304;&#22312;&#20869;&#30340;&#27169;&#22411;&#24320;&#21457;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2306.07983</link><description>&lt;p&gt;
&#26234;&#33021;&#35686;&#25253;&#29983;&#25104;&#30340;&#28151;&#21512;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Hybrid Approach for Smart Alert Generation. (arXiv:2306.07983v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07983
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#28151;&#21512;&#27169;&#22411;&#30340;&#35686;&#25253;&#31995;&#32479;&#65292;&#22312;&#35299;&#20915;&#21487;&#25193;&#23637;&#24615;&#12289;&#25968;&#25454;&#24322;&#26500;&#24615;&#12289;&#21487;&#27867;&#21270;&#24615;&#21644;&#21487;&#32500;&#25252;&#24615;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#12290;&#30333;&#21517;&#21333;&#26426;&#21046;&#26377;&#25928;&#20943;&#23569;&#20102;&#35823;&#25253;&#35686;&#25253;&#65292;&#26410;&#26469;&#30340;&#24037;&#20316;&#23558;&#21253;&#25324;&#26356;&#22810;&#30340;&#29305;&#24449;&#24037;&#31243;&#21644;&#36755;&#20837;&#25968;&#25454;&#65292;&#20197;&#21450;&#21253;&#25324;&#20154;&#31867;&#21453;&#39304;&#22312;&#20869;&#30340;&#27169;&#22411;&#24320;&#21457;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#24120;&#26816;&#27979;&#26159;&#32593;&#32476;&#31649;&#29702;&#20013;&#30340;&#37325;&#35201;&#20219;&#21153;&#65292;&#20294;&#24403;&#32771;&#34385;&#21487;&#25193;&#23637;&#24615;&#12289;&#25968;&#25454;&#24322;&#26500;&#24615;&#20197;&#21450;&#21487;&#27867;&#21270;&#24615;&#21644;&#21487;&#32500;&#25252;&#24615;&#26102;&#65292;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#22823;&#35268;&#27169;&#32593;&#32476;&#31995;&#32479;&#20013;&#37096;&#32626;&#26234;&#33021;&#35686;&#25253;&#31995;&#32479;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#27169;&#22411;&#30340;&#35686;&#25253;&#31995;&#32479;&#65292;&#23558;&#32479;&#35745;&#27169;&#22411;&#19982;&#30333;&#21517;&#21333;&#26426;&#21046;&#32467;&#21512;&#36215;&#26469;&#65292;&#20197;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#24182;&#20943;&#23569;&#35823;&#25253;&#35686;&#25253;&#12290; &#32479;&#35745;&#27169;&#22411;&#21033;&#29992;&#22823;&#22411;&#25968;&#25454;&#24211;&#26816;&#27979;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#24322;&#24120;&#65292;&#32780;&#30333;&#21517;&#21333;&#21017;&#36807;&#28388;&#20986;&#25345;&#32493;&#35686;&#25253;&#30340;&#33410;&#28857;&#20197;&#20943;&#23569;&#35823;&#25253;&#12290;&#25105;&#20204;&#20351;&#29992;&#23458;&#25143;&#25903;&#25345;&#26696;&#20363;&#30340;&#23450;&#24615;&#25968;&#25454;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#12290;&#26410;&#26469;&#30340;&#24037;&#20316;&#21253;&#25324;&#26356;&#22810;&#30340;&#29305;&#24449;&#24037;&#31243;&#21644;&#36755;&#20837;&#25968;&#25454;&#65292;&#20197;&#21450;&#22312;&#27169;&#22411;&#24320;&#21457;&#36807;&#31243;&#20013;&#21253;&#25324;&#20154;&#31867;&#21453;&#39304;&#12290;
&lt;/p&gt;
&lt;p&gt;
Anomaly detection is an important task in network management. However, deploying intelligent alert systems in real-world large-scale networking systems is challenging when we take into account (i) scalability, (ii) data heterogeneity, and (iii) generalizability and maintainability. In this paper, we propose a hybrid model for an alert system that combines statistical models with a whitelist mechanism to tackle these challenges and reduce false positive alerts. The statistical models take advantage of a large database to detect anomalies in time-series data, while the whitelist filters out persistently alerted nodes to further reduce false positives. Our model is validated using qualitative data from customer support cases. Future work includes more feature engineering and input data, as well as including human feedback in the model development process.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#39640;&#32423;&#25345;&#20037;&#24615;&#23041;&#32961;&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;&#38450;&#24481;&#30340;&#23569;&#26679;&#26412;&#12289;&#22810;&#39046;&#22495;&#30693;&#35782;&#37325;&#35013; (FMKR) &#26041;&#26696;&#65292;&#21487;&#20197;&#22312;&#19981;&#21516;&#30340;&#32593;&#32476;&#22495;&#29983;&#25104;&#22810;&#20010;&#23567;&#20219;&#21153;&#65292;&#23436;&#25104;&#22810;&#39046;&#22495;&#30340;&#30693;&#35782;&#37325;&#26032;&#35013;&#22791;&#65292;&#25552;&#39640;&#38450;&#24481;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.07685</link><description>&lt;p&gt;
&#38754;&#21521;&#39640;&#32423;&#25345;&#20037;&#24615;&#23041;&#32961;&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;&#38450;&#24481;&#30340;&#23569;&#26679;&#26412;&#22810;&#39046;&#22495;&#30693;&#35782;&#37325;&#35013;
&lt;/p&gt;
&lt;p&gt;
Few-shot Multi-domain Knowledge Rearming for Context-aware Defence against Advanced Persistent Threats. (arXiv:2306.07685v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07685
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#39640;&#32423;&#25345;&#20037;&#24615;&#23041;&#32961;&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;&#38450;&#24481;&#30340;&#23569;&#26679;&#26412;&#12289;&#22810;&#39046;&#22495;&#30693;&#35782;&#37325;&#35013; (FMKR) &#26041;&#26696;&#65292;&#21487;&#20197;&#22312;&#19981;&#21516;&#30340;&#32593;&#32476;&#22495;&#29983;&#25104;&#22810;&#20010;&#23567;&#20219;&#21153;&#65292;&#23436;&#25104;&#22810;&#39046;&#22495;&#30340;&#30693;&#35782;&#37325;&#26032;&#35013;&#22791;&#65292;&#25552;&#39640;&#38450;&#24481;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#32423;&#25345;&#20037;&#24615;&#23041;&#32961;(APTs)&#20855;&#26377;&#22810;&#38454;&#27573;&#28183;&#36879;&#12289;&#39640;&#24230;&#23450;&#21046;&#21270;&#24847;&#22270;&#21644;&#35268;&#36991;&#31574;&#30053;&#31561;&#26032;&#39062;&#29305;&#24449;&#12290;APTs&#30340;&#38450;&#24481;&#38656;&#35201;&#34701;&#21512;&#22810;&#32500;&#32593;&#32476;&#23041;&#32961;&#24773;&#25253;&#25968;&#25454;&#26469;&#35782;&#21035;&#25915;&#20987;&#24847;&#22270;&#65292;&#24182;&#36890;&#36807;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#26426;&#22120;&#23398;&#20064;&#36827;&#34892;&#26377;&#25928;&#30340;&#30693;&#35782;&#21457;&#29616;&#31574;&#30053;&#20197;&#35782;&#21035;&#23454;&#20307;&#20851;&#31995;&#12290;&#28982;&#32780;&#65292;&#25968;&#25454;&#39537;&#21160;&#30340;&#26426;&#22120;&#23398;&#20064;&#32570;&#20047;&#23545;&#26032;&#30340;&#25110;&#26410;&#30693;&#26679;&#26412;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#38477;&#20302;&#20102;&#38450;&#24481;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;&#27492;&#22806;&#65292;&#23558;&#36825;&#20123;APTs&#38450;&#24481;&#27169;&#22411;&#31169;&#26377;&#37096;&#32626;&#22312;&#24322;&#26500;&#29615;&#22659;&#20013;&#21644;&#21508;&#31181;&#32593;&#32476;&#35774;&#22791;&#19978;&#65292;&#38656;&#35201;&#22312;&#19978;&#19979;&#25991;&#24863;&#30693;&#26041;&#38754;&#36827;&#34892;&#37325;&#22823;&#25237;&#20837;&#65288;&#20363;&#22914;&#24050;&#30693;&#25915;&#20987;&#23454;&#20307;&#12289;&#36830;&#32493;&#30340;&#32593;&#32476;&#29366;&#24577;&#21644;&#24403;&#21069;&#30340;&#23433;&#20840;&#31574;&#30053;&#65289;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#39640;&#32423;&#25345;&#20037;&#24615;&#23041;&#32961;&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;&#38450;&#24481;&#30340;&#23569;&#26679;&#26412;&#22810;&#39046;&#22495;&#30693;&#35782;&#37325;&#35013;(FMKR)&#26041;&#26696;&#12290;&#36890;&#36807;&#22312;&#19981;&#21516;&#32593;&#32476;&#22495;&#29983;&#25104;&#22810;&#20010;&#23567;&#20219;&#21153;&#65292;&#23436;&#25104;&#22810;&#39046;&#22495;&#30693;&#35782;&#37325;&#26032;&#35013;&#22791;&#65292;&#20174;&#32780;&#25552;&#21319;&#38450;&#24481;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Advanced persistent threats (APTs) have novel features such as multi-stage penetration, highly-tailored intention, and evasive tactics. APTs defense requires fusing multi-dimensional Cyber threat intelligence data to identify attack intentions and conducts efficient knowledge discovery strategies by data-driven machine learning to recognize entity relationships. However, data-driven machine learning lacks generalization ability on fresh or unknown samples, reducing the accuracy and practicality of the defense model. Besides, the private deployment of these APT defense models on heterogeneous environments and various network devices requires significant investment in context awareness (such as known attack entities, continuous network states, and current security strategies). In this paper, we propose a few-shot multi-domain knowledge rearming (FMKR) scheme for context-aware defense against APTs. By completing multiple small tasks that are generated from different network domains with m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#21452;&#26354;&#22270;&#25193;&#25955;&#27169;&#22411;&#30340;&#20998;&#23376;&#29983;&#25104;&#26041;&#27861;&#65292;&#21487;&#20197;&#26356;&#20840;&#38754;&#22320;&#25429;&#25417;&#20998;&#23376;&#30340;&#20869;&#37096;&#38750;&#27431;&#20960;&#37324;&#24503;&#32467;&#26500;&#65292;&#23454;&#29616;&#25968;&#25454;&#29983;&#25104;&#65292;&#24182;&#25552;&#21462;&#22797;&#26434;&#20960;&#20309;&#29305;&#24449;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.07618</link><description>&lt;p&gt;
&#22522;&#20110;&#21452;&#26354;&#22270;&#25193;&#25955;&#27169;&#22411;&#30340;&#20998;&#23376;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Hyperbolic Graph Diffusion Model for Molecule Generation. (arXiv:2306.07618v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07618
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#21452;&#26354;&#22270;&#25193;&#25955;&#27169;&#22411;&#30340;&#20998;&#23376;&#29983;&#25104;&#26041;&#27861;&#65292;&#21487;&#20197;&#26356;&#20840;&#38754;&#22320;&#25429;&#25417;&#20998;&#23376;&#30340;&#20869;&#37096;&#38750;&#27431;&#20960;&#37324;&#24503;&#32467;&#26500;&#65292;&#23454;&#29616;&#25968;&#25454;&#29983;&#25104;&#65292;&#24182;&#25552;&#21462;&#22797;&#26434;&#20960;&#20309;&#29305;&#24449;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#25193;&#25955;&#27169;&#22411;&#22312;&#25968;&#25454;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#26524;&#65292;&#20363;&#22914;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#22270;&#20687;&#12290;&#28982;&#32780;&#65292;&#21270;&#23398;&#20998;&#23376;&#36890;&#24120;&#20855;&#26377;&#22797;&#26434;&#30340;&#38750;&#27431;&#20960;&#37324;&#24503;&#31354;&#38388;&#32467;&#26500;&#65292;&#20854;&#34892;&#20026;&#21160;&#24577;&#21464;&#21270;&#19988;&#38590;&#20197;&#39044;&#27979;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#25193;&#25955;&#27169;&#22411;&#39640;&#24230;&#20381;&#36182;&#20110;&#35745;&#31639;&#27431;&#20960;&#37324;&#24503;&#31354;&#38388;&#20013;&#30340;&#27010;&#29575;&#20998;&#24067;&#65292;&#21363;&#39640;&#26031;&#20998;&#24067;&#65292;&#19981;&#33021;&#25429;&#25417;&#20998;&#23376;&#30340;&#20869;&#37096;&#38750;&#27431;&#20960;&#37324;&#24503;&#32467;&#26500;&#65292;&#29305;&#21035;&#26159;&#20998;&#23376;&#25152;&#34920;&#31034;&#30340;&#38544;&#24335;&#27969;&#24418;&#34920;&#38754;&#30340;&#20998;&#23618;&#32467;&#26500;&#12290;&#35266;&#23519;&#21040;&#65292;&#21452;&#26354;&#23884;&#20837;&#31354;&#38388;&#20013;&#30340;&#22797;&#26434;&#20998;&#23618;&#32467;&#26500;&#21464;&#24471;&#26356;&#21152;&#26126;&#26174;&#19988;&#26356;&#23481;&#26131;&#34987;&#25429;&#25417;&#12290;&#20026;&#20102;&#20805;&#20998;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#30340;&#25968;&#25454;&#29983;&#25104;&#33021;&#21147;&#21644;&#25552;&#21462;&#22797;&#26434;&#20960;&#20309;&#29305;&#24449;&#30340;&#21452;&#26354;&#23884;&#20837;&#30340;&#24378;&#22823;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;&#25193;&#25955;&#27169;&#22411;&#25193;&#23637;&#21040;&#21452;&#26354;&#27969;&#24418;&#19978;&#36827;&#34892;&#20998;&#23376;&#29983;&#25104;&#65292;&#21363;&#22522;&#20110;&#21452;&#26354;&#22270;&#25193;&#25955;&#27169;&#22411;&#30340;&#20998;&#23376;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, diffusion models have achieved remarkable performance in data generation, e.g., generating high-quality images. Nevertheless, chemistry molecules often have complex non-Euclidean spatial structures, with the behavior changing dynamically and unpredictably. Most existing diffusion models highly rely on computing the probability distribution, i.e., Gaussian distribution, in Euclidean space, which cannot capture internal non-Euclidean structures of molecules, especially the hierarchical structures of the implicit manifold surface represented by molecules. It has been observed that the complex hierarchical structures in hyperbolic embedding space become more prominent and easier to be captured. In order to leverage both the data generation power of diffusion models and the strong capability to extract complex geometric features of hyperbolic embedding, we propose to extend the diffusion model to hyperbolic manifolds for molecule generation, namely, Hyperbolic Graph Diffusion Mode
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#21644;&#26426;&#22120;&#20154;&#24037;&#20855;&#30340;&#20114;&#21160;&#30740;&#31350;&#65292;&#35777;&#26126;&#20102;&#21487;&#34892;&#24615;&#26159;&#22235;&#36275;&#21160;&#29289;&#27493;&#24577;&#36716;&#25442;&#30340;&#37325;&#35201;&#26631;&#20934;&#12290;&#20854;&#20013;&#65292;&#27493;-&#23567;&#36305;&#27493;&#24577;&#36716;&#25442;&#33021;&#22815;&#22312;&#24179;&#22374;&#22320;&#24418;&#19978;&#21516;&#26102;&#25552;&#39640;&#21487;&#34892;&#24615;&#21644;&#33410;&#33021;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.07419</link><description>&lt;p&gt;
DeepTransition&#65306;&#21487;&#34892;&#24615;&#23548;&#33268;&#27493;&#24577;&#36716;&#25442;&#30340;&#20986;&#29616;
&lt;/p&gt;
&lt;p&gt;
DeepTransition: Viability Leads to the Emergence of Gait Transitions in Learning Anticipatory Quadrupedal Locomotion Skills. (arXiv:2306.07419v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07419
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#21644;&#26426;&#22120;&#20154;&#24037;&#20855;&#30340;&#20114;&#21160;&#30740;&#31350;&#65292;&#35777;&#26126;&#20102;&#21487;&#34892;&#24615;&#26159;&#22235;&#36275;&#21160;&#29289;&#27493;&#24577;&#36716;&#25442;&#30340;&#37325;&#35201;&#26631;&#20934;&#12290;&#20854;&#20013;&#65292;&#27493;-&#23567;&#36305;&#27493;&#24577;&#36716;&#25442;&#33021;&#22815;&#22312;&#24179;&#22374;&#22320;&#24418;&#19978;&#21516;&#26102;&#25552;&#39640;&#21487;&#34892;&#24615;&#21644;&#33410;&#33021;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22235;&#36275;&#21160;&#29289;&#22312;&#25913;&#21464;&#36816;&#21160;&#36895;&#24230;&#26102;&#33021;&#22815;&#26080;&#32541;&#22320;&#36716;&#25442;&#27493;&#24577;&#12290;&#26412;&#25991;&#25552;&#20986;&#21487;&#34892;&#24615;&#65288;&#21363;&#36991;&#20813;&#36300;&#20498;&#65289;&#20195;&#34920;&#27493;&#24577;&#36716;&#25442;&#30340;&#19968;&#20010;&#37325;&#35201;&#26631;&#20934;&#12290;&#36890;&#36807;&#21033;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#21644;&#26426;&#22120;&#20154;&#24037;&#20855;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#27493;&#24577;&#36716;&#25442;&#30340;&#20986;&#29616;&#12290;&#19968;&#33268;&#20110;&#22235;&#36275;&#21160;&#29289;&#25968;&#25454;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#24179;&#22374;&#22320;&#24418;&#19978;&#65292;&#22235;&#36275;&#26426;&#22120;&#20154;&#30340;&#27493;-&#23567;&#36305;&#27493;&#24577;&#36716;&#25442;&#33021;&#21516;&#26102;&#25552;&#39640;&#21487;&#34892;&#24615;&#21644;&#33410;&#33021;&#25928;&#26524;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#31163;&#25955;&#22320;&#24418;&#65288;&#21363;&#31359;&#36234;&#36830;&#32493;&#38388;&#38548;&#65289;&#23545;&#24378;&#21046;&#27493;&#24577;&#36716;&#25442;&#30340;&#24433;&#21709;&#65292;&#24182;&#25214;&#21040;&#36275;-&#36454;&#27493;&#24577;&#30340;&#20986;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quadruped animals seamlessly transition between gaits as they change locomotion speeds. While the most widely accepted explanation for gait transitions is energy efficiency, there is no clear consensus on the determining factor, nor on the potential effects from terrain properties. In this article, we propose that viability, i.e. the avoidance of falls, represents an important criterion for gait transitions. We investigate the emergence of gait transitions through the interaction between supraspinal drive (brain), the central pattern generator in the spinal cord, the body, and exteroceptive sensing by leveraging deep reinforcement learning and robotics tools. Consistent with quadruped animal data, we show that the walk-trot gait transition for quadruped robots on flat terrain improves both viability and energy efficiency. Furthermore, we investigate the effects of discrete terrain (i.e. crossing successive gaps) on imposing gait transitions, and find the emergence of trot-pronk transit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;Strokes2Surface&#65292;&#23427;&#21487;&#20174;&#24314;&#31569;&#24072;&#30340;&#31508;&#30011;&#20013;&#24674;&#22797;&#20986;&#26354;&#32447;&#32593;&#32476;&#65292;&#23545;&#20110;&#24314;&#31569;&#35774;&#35745;&#20013;&#30340;&#27010;&#24565;&#35774;&#35745;&#21644;&#25968;&#23383;&#24314;&#27169;&#20043;&#38388;&#30340;&#26725;&#26753;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2306.07220</link><description>&lt;p&gt;
Strokes2Surface&#65306;&#20174;&#22235;&#32500;&#24314;&#31569;&#35774;&#35745;&#32032;&#25551;&#20013;&#24674;&#22797;&#26354;&#32447;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Strokes2Surface: Recovering Curve Networks From 4D Architectural Design Sketches. (arXiv:2306.07220v2 [cs.GR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07220
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;Strokes2Surface&#65292;&#23427;&#21487;&#20174;&#24314;&#31569;&#24072;&#30340;&#31508;&#30011;&#20013;&#24674;&#22797;&#20986;&#26354;&#32447;&#32593;&#32476;&#65292;&#23545;&#20110;&#24314;&#31569;&#35774;&#35745;&#20013;&#30340;&#27010;&#24565;&#35774;&#35745;&#21644;&#25968;&#23383;&#24314;&#27169;&#20043;&#38388;&#30340;&#26725;&#26753;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#31163;&#32447;&#20960;&#20309;&#37325;&#24314;&#31649;&#36947;Strokes2Surface&#65292;&#23427;&#26159;&#22522;&#20110;4D Sketching Interface&#65292;MR.Sketch&#30340;&#30446;&#26631;&#26159;&#38754;&#21521;&#24314;&#31569;&#35774;&#35745;&#30340;&#12290;&#35813;&#31649;&#36947;&#20174;&#35774;&#35745;&#24072;&#32472;&#21046;&#30340;&#31508;&#30011;&#20013;&#24674;&#22797;&#26354;&#32447;&#32593;&#32476;&#65292;&#22240;&#27492;&#22312;&#24314;&#31569;&#35774;&#35745;&#30340;&#27010;&#24565;&#35774;&#35745;&#21644;&#25968;&#23383;&#24314;&#27169;&#38454;&#27573;&#20043;&#38388;&#24314;&#31435;&#20102;&#26725;&#26753;&#12290;&#25105;&#20204;&#30340;&#31649;&#36947;&#30340;&#36755;&#20837;&#21253;&#25324;3D&#31508;&#30011;&#30340;&#25240;&#32447;&#39030;&#28857;&#21450;&#20854;&#30456;&#24212;&#30340;&#26102;&#38388;&#25139;&#65288;&#20316;&#20026;&#31532;&#22235;&#20010;&#32500;&#24230;&#65289;&#65292;&#20197;&#21450;&#39069;&#22806;&#30340;&#20960;&#20309;&#21644;&#31508;&#35302;&#30456;&#20851;&#30340;&#35760;&#24405;&#23646;&#24615;&#12290;&#22522;&#20110;&#32032;&#25551;&#21512;&#24182;&#21644;&#22522;&#20110;&#32032;&#25551;&#24314;&#27169;&#26041;&#27861;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#30340;&#31649;&#36947;&#21033;&#29992;&#36825;&#20123;&#25968;&#25454;&#24182;&#32452;&#21512;&#19977;&#20010;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#65307;&#19968;&#20010;&#20998;&#31867;&#22120;&#21644;&#20004;&#20010;&#32858;&#31867;&#27169;&#22411;&#12290;&#29305;&#21035;&#26159;&#65292;&#26681;&#25454;&#24314;&#31569;&#35774;&#35745;&#32032;&#25551;&#20013;&#35774;&#35745;&#24072;&#36890;&#24120;&#37319;&#29992;&#30340;&#23454;&#36341;&#35266;&#23519;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#19968;&#20010;&#20108;&#20803;&#20998;&#31867;&#38382;&#39064;&#65292;&#20197;&#35782;&#21035;&#19968;&#31508;&#30011;&#26159;&#25551;&#32472;&#36793;&#30028;&#21644;&#36793;&#32536;&#36824;&#26159;&#29992;&#20110;&#22635;&#20805;&#25152;&#38656;&#24314;&#31569;&#29289;&#30340;&#23553;&#38381;&#21306;&#22495;&#21644;&#34920;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Strokes2Surface, an offline geometry-reconstruction pipeline built upon a 4D Sketching Interface, MR.Sketch, targeted at architectural design. The pipeline recovers a curve network from designer-drawn strokes, thus bridging between concept design and digital modeling stages in architectural design. The input to our pipeline consists of 3D strokes' polyline vertices and their corresponding timestamps (as of the fourth dimension), along with additional geometric and stylus-related recorded properties. Inspired by sketch consolidation and sketch-based modeling methods, our pipeline leverages such data and combines three Machine Learning (ML) models; a classifier and two clustering models. In particular, based on observations of practices designers typically employ in architectural design sketches, we solve a binary classification problem to recognize whether a stroke depicts a boundary and edge or is used to fill in the enclosing areas and faces of the intended architectural ob
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#20154;&#24037;&#26234;&#33021;&#21487;&#33021;&#24102;&#26469;&#30340;&#31038;&#20250;&#35268;&#27169;&#39118;&#38505;&#30340;&#20998;&#31867;&#21644;&#20998;&#26512;&#65292;&#20854;&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38382;&#36131;&#21046;&#30340;&#20998;&#31867;&#27861;&#12290;&#38024;&#23545;&#21487;&#33021;&#20986;&#29616;&#30340;&#39118;&#38505;&#31867;&#22411;&#25552;&#20379;&#20102;&#23454;&#38469;&#30340;&#35777;&#26126;&#65292;&#24182;&#25351;&#20986;&#38656;&#35201;&#32852;&#21512;&#25216;&#26415;&#21644;&#25919;&#31574;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2306.06924</link><description>&lt;p&gt;
TASRA: &#19968;&#20221;&#20851;&#20110;&#20154;&#24037;&#26234;&#33021;&#21487;&#33021;&#24102;&#26469;&#30340;&#31038;&#20250;&#35268;&#27169;&#39118;&#38505;&#30340;&#20998;&#31867;&#19982;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
TASRA: a Taxonomy and Analysis of Societal-Scale Risks from AI. (arXiv:2306.06924v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06924
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#20154;&#24037;&#26234;&#33021;&#21487;&#33021;&#24102;&#26469;&#30340;&#31038;&#20250;&#35268;&#27169;&#39118;&#38505;&#30340;&#20998;&#31867;&#21644;&#20998;&#26512;&#65292;&#20854;&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38382;&#36131;&#21046;&#30340;&#20998;&#31867;&#27861;&#12290;&#38024;&#23545;&#21487;&#33021;&#20986;&#29616;&#30340;&#39118;&#38505;&#31867;&#22411;&#25552;&#20379;&#20102;&#23454;&#38469;&#30340;&#35777;&#26126;&#65292;&#24182;&#25351;&#20986;&#38656;&#35201;&#32852;&#21512;&#25216;&#26415;&#21644;&#25919;&#31574;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#26368;&#36817;&#19968;&#20123;&#20316;&#21697;&#24050;&#32463;&#30830;&#35748;&#20102;&#20154;&#24037;&#26234;&#33021;&#21487;&#33021;&#32473;&#20154;&#31867;&#24102;&#26469;&#30340;&#31038;&#20250;&#35268;&#27169;&#21644;&#28781;&#32477;&#32423;&#30340;&#39118;&#38505;&#65292;&#20294;&#20960;&#20046;&#27809;&#26377;&#20154;&#23581;&#35797;&#36807;&#36827;&#34892;&#20840;&#38754;&#24402;&#32435;&#36825;&#20123;&#39118;&#38505;&#12290;&#35768;&#22810;&#20840;&#38754;&#24615;&#30340;&#20998;&#31867;&#27861;&#37117;&#26159;&#21487;&#33021;&#30340;&#65292;&#24182;&#19988;&#26377;&#20123;&#26159;&#26377;&#29992;&#30340;&#8212;&#8212;&#23588;&#20854;&#26159;&#22914;&#26524;&#23427;&#20204;&#25581;&#31034;&#20102;&#26032;&#30340;&#39118;&#38505;&#25110;&#23433;&#20840;&#30340;&#23454;&#38469;&#26041;&#27861;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#19968;&#31181;&#22522;&#20110;&#38382;&#36131;&#21046;&#30340;&#20998;&#31867;&#27861;&#65306;&#21738;&#20123;&#34892;&#20026;&#23548;&#33268;&#20102;&#39118;&#38505;&#65292;&#34892;&#21160;&#32773;&#26159;&#21542;&#32479;&#19968;&#65292;&#20182;&#20204;&#26159;&#21542;&#26159;&#33988;&#24847;&#30340;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#25925;&#20107;&#26469;&#35828;&#26126;&#21508;&#31181;&#39118;&#38505;&#31867;&#22411;&#22914;&#20309;&#21457;&#25381;&#20316;&#29992;&#65292;&#21253;&#25324;&#26469;&#33258;&#35768;&#22810;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#24847;&#22806;&#30456;&#20114;&#20316;&#29992;&#30340;&#39118;&#38505;&#65292;&#20197;&#21450;&#26469;&#33258;&#33988;&#24847;&#28389;&#29992;&#30340;&#39118;&#38505;&#65292;&#38656;&#35201;&#32852;&#21512;&#25216;&#26415;&#21644;&#25919;&#31574;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
While several recent works have identified societal-scale and extinction-level risks to humanity arising from artificial intelligence, few have attempted an {\em exhaustive taxonomy} of such risks. Many exhaustive taxonomies are possible, and some are useful -- particularly if they reveal new risks or practical approaches to safety. This paper explores a taxonomy based on accountability: whose actions lead to the risk, are the actors unified, and are they deliberate? We also provide stories to illustrate how the various risk types could each play out, including risks arising from unanticipated interactions of many AI systems, as well as risks from deliberate misuse, for which combined technical and policy solutions are indicated.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24320;&#21019;&#24615;&#22320;&#30740;&#31350;&#20102;&#22522;&#20110; prompt &#23398;&#20064;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411; API &#30340;&#29305;&#27931;&#20234;&#26131;&#24863;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#40657;&#30418;&#26694;&#26550;&#8212;&#8212;TrojPrompt&#65292;&#29992;&#20110;&#29983;&#25104;&#36890;&#29992;&#21644;&#38544;&#34109;&#30340;&#35302;&#21457;&#22120;&#65292;&#24182;&#23558;&#29305;&#27931;&#20234;&#26408;&#39532;&#25554;&#20837;&#30828;&#25552;&#31034;&#12290;</title><link>http://arxiv.org/abs/2306.06815</link><description>&lt;p&gt;
TrojPrompt&#65306;&#22522;&#20110;&#40657;&#30418;&#26041;&#24335;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#26408;&#39532;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
TrojPrompt: A Black-box Trojan Attack on Pre-trained Language Models. (arXiv:2306.06815v1 [cs.CR] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06815
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24320;&#21019;&#24615;&#22320;&#30740;&#31350;&#20102;&#22522;&#20110; prompt &#23398;&#20064;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411; API &#30340;&#29305;&#27931;&#20234;&#26131;&#24863;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#40657;&#30418;&#26694;&#26550;&#8212;&#8212;TrojPrompt&#65292;&#29992;&#20110;&#29983;&#25104;&#36890;&#29992;&#21644;&#38544;&#34109;&#30340;&#35302;&#21457;&#22120;&#65292;&#24182;&#23558;&#29305;&#27931;&#20234;&#26408;&#39532;&#25554;&#20837;&#30828;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Prompt&#23398;&#20064;&#34987;&#35777;&#26126;&#22312;&#25552;&#39640;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#36866;&#24212;&#24615;&#26041;&#38754;&#38750;&#24120;&#26377;&#25928;&#65292;&#36229;&#36234;&#20102;&#20256;&#32479;&#30340;&#24494;&#35843;&#33539;&#24335;&#65292;&#24182;&#22312;&#19987;&#20026;&#23569;&#26679;&#26412;&#23398;&#20064;&#22330;&#26223;&#37327;&#36523;&#23450;&#21046;&#30340;&#24212;&#29992;&#31243;&#24207;&#21644;API&#20013;&#23637;&#29616;&#20102;&#26480;&#20986;&#30340;&#21069;&#26223;&#12290;&#20294;&#26159;&#65292;&#23613;&#31649;prompt&#23398;&#20064;&#30340;API&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#20294;&#23427;&#20204;&#30340;&#23433;&#20840;&#38382;&#39064;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#26412;&#25991;&#22312;prompt&#23398;&#20064;&#30340;PLM API&#30340;&#29305;&#27931;&#20234;&#26131;&#24863;&#24615;&#26041;&#38754;&#36827;&#34892;&#20102;&#24320;&#21019;&#24615;&#30740;&#31350;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#31163;&#25955;&#25552;&#31034;&#65292;&#23569;&#26679;&#26412;&#21644;&#40657;&#30418;&#35774;&#32622;&#26159;&#20960;&#20010;&#20851;&#38190;&#25361;&#25112;&#65292;&#38480;&#21046;&#20102;&#29616;&#26377;&#21518;&#38376;&#25915;&#20987;&#30340;&#36866;&#29992;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TrojPrompt&#65292;&#36825;&#26159;&#19968;&#31181;&#33258;&#21160;&#30340;&#40657;&#30418;&#26694;&#26550;&#65292;&#21487;&#26377;&#25928;&#29983;&#25104;&#36890;&#29992;&#30340;&#21644;&#38544;&#31192;&#30340;&#35302;&#21457;&#22120;&#65292;&#24182;&#23558;&#29305;&#27931;&#20234;&#26408;&#39532;&#25554;&#20837;&#30828;&#25552;&#31034;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;API&#39537;&#21160;&#30340;&#36890;&#29992;&#35302;&#21457;&#22120;&#21457;&#29616;&#31639;&#27861;&#65292;&#36890;&#36807;&#26597;&#35810;&#21463;&#23475;&#32773;PLM API&#65292;&#20026;&#21508;&#31181;&#36755;&#20837;&#29983;&#25104;&#36890;&#29992;&#35302;&#21457;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prompt learning has been proven to be highly effective in improving pre-trained language model (PLM) adaptability, surpassing conventional fine-tuning paradigms, and showing exceptional promise in an ever-growing landscape of applications and APIs tailored for few-shot learning scenarios. Despite the growing prominence of prompt learning-based APIs, their security concerns remain underexplored. In this paper, we undertake a pioneering study on the Trojan susceptibility of prompt-learning PLM APIs. We identified several key challenges, including discrete-prompt, few-shot, and black-box settings, which limit the applicability of existing backdoor attacks. To address these challenges, we propose TrojPrompt, an automatic and black-box framework to effectively generate universal and stealthy triggers and insert Trojans into hard prompts. Specifically, we propose a universal API-driven trigger discovery algorithm for generating universal triggers for various inputs by querying victim PLM API
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35748;&#30693;&#20195;&#29702;&#26041;&#27861;&#65292;&#36890;&#36807;&#22686;&#21152; LLMs &#30340;&#21709;&#24212;&#31354;&#38388;&#65292;&#24182;&#37096;&#32626;&#36890;&#29992;&#31574;&#30053;&#65292;&#23884;&#20837;&#33258;&#20027;&#26426;&#22120;&#20154;&#20869;&#37096;&#65292;&#20174;&#32780;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#33719;&#21462;&#19982;&#20854;&#35821;&#35328;&#33021;&#21147;&#12289;&#20307;&#29616;&#33021;&#21147;&#12289;&#29615;&#22659;&#21644;&#29992;&#25143;&#20559;&#22909;&#30456;&#21305;&#37197;&#30340;&#26032;&#30340;&#20219;&#21153;&#30693;&#35782;&#65292;&#23454;&#29616;&#19968;&#27425;&#23398;&#20064;&#21363;&#21487;&#23436;&#25104;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2306.06770</link><description>&lt;p&gt;
&#36890;&#36807;&#20195;&#29702;&#20998;&#26512;&#25552;&#39640; LLM &#23545;&#26426;&#22120;&#20154;&#20219;&#21153;&#23398;&#20064;&#30340;&#30693;&#35782;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Improving Knowledge Extraction from LLMs for Robotic Task Learning through Agent Analysis. (arXiv:2306.06770v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06770
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35748;&#30693;&#20195;&#29702;&#26041;&#27861;&#65292;&#36890;&#36807;&#22686;&#21152; LLMs &#30340;&#21709;&#24212;&#31354;&#38388;&#65292;&#24182;&#37096;&#32626;&#36890;&#29992;&#31574;&#30053;&#65292;&#23884;&#20837;&#33258;&#20027;&#26426;&#22120;&#20154;&#20869;&#37096;&#65292;&#20174;&#32780;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#33719;&#21462;&#19982;&#20854;&#35821;&#35328;&#33021;&#21147;&#12289;&#20307;&#29616;&#33021;&#21147;&#12289;&#29615;&#22659;&#21644;&#29992;&#25143;&#20559;&#22909;&#30456;&#21305;&#37197;&#30340;&#26032;&#30340;&#20219;&#21153;&#30693;&#35782;&#65292;&#23454;&#29616;&#19968;&#27425;&#23398;&#20064;&#21363;&#21487;&#23436;&#25104;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#34987;&#35270;&#20026;&#26426;&#22120;&#20154;&#20219;&#21153;&#23398;&#20064;&#30340;&#30693;&#35782;&#26469;&#28304;&#65292;&#20294;&#26159;&#21333;&#29420;&#30340;&#25552;&#31034;&#24037;&#31243;&#24182;&#19981;&#33021;&#20026;&#26426;&#22120;&#20154;&#33719;&#21462;&#19982;&#20854;&#35821;&#35328;&#33021;&#21147;&#12289;&#20307;&#29616;&#33021;&#21147;&#12289;&#29615;&#22659;&#21644;&#29992;&#25143;&#20559;&#22909;&#30456;&#21305;&#37197;&#30340;&#24773;&#22659;&#30456;&#20851;&#30693;&#35782;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35748;&#30693;&#20195;&#29702;&#26041;&#27861;&#65292;&#25193;&#23637;&#21644;&#34917;&#20805;&#25552;&#31034;&#24037;&#31243;&#65292;&#32531;&#35299;&#20854;&#23616;&#38480;&#24615;&#65292;&#20197;&#27492;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#33719;&#21462;&#26032;&#30340;&#20219;&#21153;&#30693;&#35782;&#12290;&#35813;&#26041;&#27861;&#26159;&#36890;&#36807;&#22686;&#21152; LLMs &#30340;&#21709;&#24212;&#31354;&#38388;&#65292;&#24182;&#37096;&#32626;&#36890;&#29992;&#31574;&#30053;&#65292;&#23884;&#20837;&#33258;&#20027;&#26426;&#22120;&#20154;&#20869;&#37096;&#65292;&#23545; LLMs &#20135;&#29983;&#30340;&#20505;&#36873;&#21709;&#24212;&#36827;&#34892;&#35780;&#20272;&#12289;&#20462;&#22797;&#21644;&#36873;&#25321;&#65292;&#23454;&#29616;&#19968;&#27425;&#23398;&#20064;&#21363;&#21487;&#23436;&#25104;&#20219;&#21153;&#12290;&#36890;&#36807;&#23454;&#39564;&#34920;&#26126;&#65292;&#26426;&#22120;&#20154;&#20174; LLM &#20013;&#26816;&#32034;&#21644;&#35780;&#20272;&#19968;&#31995;&#21015;&#19981;&#21516;&#21709;&#24212;&#21518;&#21487;&#20197;&#36798;&#21040;&gt;75% &#30340;&#20219;&#21153;&#23436;&#25104;&#29575;&#65292;&#26080;&#38656;&#29992;&#25143;&#30417;&#30563;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) offer significant promise as a knowledge source for robotic task learning. Prompt engineering has been shown to be effective for eliciting knowledge from an LLM but alone is insufficient for acquiring relevant, situationally grounded knowledge for an embodied robotic agent learning novel tasks. We describe a cognitive-agent approach that extends and complements prompt engineering, mitigating its limitations, and thus enabling a robot to acquire new task knowledge matched to its native language capabilities, embodiment, environment, and user preferences. The approach is to increase the response space of LLMs and deploy general strategies, embedded within the autonomous robot, to evaluate, repair, and select among candidate responses produced by the LLM. We describe the approach and experiments that show how a robot, by retrieving and evaluating a breadth of responses from the LLM, can achieve &gt;75% task completion in one-shot learning without user oversight. 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;NeRFBK&#30340;&#26032;&#30340;&#30495;&#23454;&#21644;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#19987;&#38376;&#29992;&#20110;&#27979;&#35797;&#21644;&#27604;&#36739;&#22522;&#20110;NeRF&#30340;3D&#37325;&#24314;&#31639;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#25910;&#38598;&#20855;&#26377;&#31934;&#30830;&#22320;&#38754;&#23454;&#20917;&#30340;&#22810;&#26679;&#21270;&#25968;&#25454;&#30340;&#25361;&#25112;&#38382;&#39064;&#12290;&#35813;&#25968;&#25454;&#38598;&#26377;&#26395;&#25512;&#21160;3D&#37325;&#24314;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;</title><link>http://arxiv.org/abs/2306.06300</link><description>&lt;p&gt;
NERFBK:&#38024;&#23545;&#22522;&#20110;NeRF&#30340;3D&#37325;&#24314;&#30340;&#39640;&#36136;&#37327;&#22522;&#20934;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
NERFBK: A High-Quality Benchmark for NERF-Based 3D Reconstruction. (arXiv:2306.06300v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06300
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;NeRFBK&#30340;&#26032;&#30340;&#30495;&#23454;&#21644;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#19987;&#38376;&#29992;&#20110;&#27979;&#35797;&#21644;&#27604;&#36739;&#22522;&#20110;NeRF&#30340;3D&#37325;&#24314;&#31639;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#25910;&#38598;&#20855;&#26377;&#31934;&#30830;&#22320;&#38754;&#23454;&#20917;&#30340;&#22810;&#26679;&#21270;&#25968;&#25454;&#30340;&#25361;&#25112;&#38382;&#39064;&#12290;&#35813;&#25968;&#25454;&#38598;&#26377;&#26395;&#25512;&#21160;3D&#37325;&#24314;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#19968;&#20010;&#21517;&#20026;NeRFBK&#30340;&#26032;&#30340;&#30495;&#23454;&#21644;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#19987;&#38376;&#29992;&#20110;&#27979;&#35797;&#21644;&#27604;&#36739;&#22522;&#20110;NeRF&#30340;3D&#37325;&#24314;&#31639;&#27861;&#12290;&#39640;&#36136;&#37327;&#30340;3D&#37325;&#24314;&#22312;&#21508;&#20010;&#39046;&#22495;&#37117;&#26377;&#37325;&#35201;&#30340;&#28508;&#21147;&#65292;&#22522;&#20110;&#22270;&#20687;&#30340;&#31639;&#27861;&#30340;&#25913;&#36827;&#20351;&#24471;&#35780;&#20272;&#26032;&#30340;&#20808;&#36827;&#25216;&#26415;&#21464;&#24471;&#24517;&#19981;&#21487;&#23569;&#12290;&#28982;&#32780;&#65292;&#25910;&#38598;&#20855;&#26377;&#31934;&#30830;&#22320;&#38754;&#23454;&#20917;&#30340;&#22810;&#26679;&#21270;&#25968;&#25454;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#19988;&#21487;&#33021;&#26080;&#27861;&#28085;&#30422;&#25152;&#26377;&#30456;&#20851;&#30340;&#24212;&#29992;&#12290;NeRFBK&#25968;&#25454;&#38598;&#36890;&#36807;&#25552;&#20379;&#22810;&#23610;&#24230;&#12289;&#23460;&#20869;&#12289;&#23460;&#22806;&#25968;&#25454;&#38598;&#21450;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#12289;&#35270;&#39057;&#21644;&#30456;&#26426;&#21442;&#25968;&#31561;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20197;&#27979;&#35797;&#21644;&#27604;&#36739;&#22522;&#20110;NeRF&#30340;&#31639;&#27861;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;NeRFBK&#22522;&#20934;&#30340;&#35774;&#35745;&#21644;&#21019;&#24314;&#12289;&#21508;&#31181;&#20363;&#23376;&#21644;&#24212;&#29992;&#22330;&#26223;&#65292;&#24182;&#24378;&#35843;&#20102;&#23427;&#22312;&#25512;&#21160;3D&#37325;&#24314;&#39046;&#22495;&#21457;&#23637;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a new real and synthetic dataset called NeRFBK specifically designed for testing and comparing NeRF-based 3D reconstruction algorithms. High-quality 3D reconstruction has significant potential in various fields, and advancements in image-based algorithms make it essential to evaluate new advanced techniques. However, gathering diverse data with precise ground truth is challenging and may not encompass all relevant applications. The NeRFBK dataset addresses this issue by providing multi-scale, indoor and outdoor datasets with high-resolution images and videos and camera parameters for testing and comparing NeRF-based algorithms. This paper presents the design and creation of the NeRFBK benchmark, various examples and application scenarios, and highlights its potential for advancing the field of 3D reconstruction.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#20687;&#20998;&#31867;&#30340;&#23545;&#25239;&#24615;&#38450;&#24481;&#26426;&#21046;CARSO&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#27604;&#26368;&#20808;&#36827;&#30340;&#23545;&#25239;&#24615;&#35757;&#32451;&#26356;&#22909;&#22320;&#20445;&#25252;&#20998;&#31867;&#22120;&#65292;&#36890;&#36807;&#21033;&#29992;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#23545;&#25239;&#20928;&#21270;&#26469;&#36827;&#34892;&#26368;&#32456;&#20998;&#31867;&#65292;&#24182;&#25104;&#21151;&#22320;&#20445;&#25252;&#33258;&#24049;&#20813;&#21463;&#26410;&#39044;&#35265;&#30340;&#23041;&#32961;&#21644;&#26368;&#32456;&#25915;&#20987;&#12290;</title><link>http://arxiv.org/abs/2306.06081</link><description>&lt;p&gt;
CARSO: &#23545;&#25239;&#24615;&#21512;&#25104;&#35266;&#27979;&#30340;&#21453;&#23545;&#25239;&#24615;&#21484;&#22238;
&lt;/p&gt;
&lt;p&gt;
CARSO: Counter-Adversarial Recall of Synthetic Observations. (arXiv:2306.06081v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06081
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#20687;&#20998;&#31867;&#30340;&#23545;&#25239;&#24615;&#38450;&#24481;&#26426;&#21046;CARSO&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#27604;&#26368;&#20808;&#36827;&#30340;&#23545;&#25239;&#24615;&#35757;&#32451;&#26356;&#22909;&#22320;&#20445;&#25252;&#20998;&#31867;&#22120;&#65292;&#36890;&#36807;&#21033;&#29992;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#23545;&#25239;&#20928;&#21270;&#26469;&#36827;&#34892;&#26368;&#32456;&#20998;&#31867;&#65292;&#24182;&#25104;&#21151;&#22320;&#20445;&#25252;&#33258;&#24049;&#20813;&#21463;&#26410;&#39044;&#35265;&#30340;&#23041;&#32961;&#21644;&#26368;&#32456;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#25239;&#24615;&#38450;&#24481;&#26426;&#21046;CARSO&#65292;&#29992;&#20110;&#22270;&#20687;&#20998;&#31867;&#65292;&#28789;&#24863;&#26469;&#33258;&#35748;&#30693;&#31070;&#32463;&#31185;&#23398;&#30340;&#32447;&#32034;&#12290;&#35813;&#26041;&#27861;&#19982;&#23545;&#25239;&#35757;&#32451;&#20855;&#26377;&#21327;&#21516;&#20114;&#34917;&#24615;&#65292;&#24182;&#20381;&#36182;&#20110;&#34987;&#25915;&#20987;&#20998;&#31867;&#22120;&#30340;&#20869;&#37096;&#34920;&#31034;&#30340;&#30693;&#35782;&#12290;&#36890;&#36807;&#21033;&#29992;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#23545;&#25239;&#20928;&#21270;&#65292;&#35813;&#26041;&#27861;&#37319;&#26679;&#36755;&#20837;&#30340;&#37325;&#26500;&#26469;&#36827;&#34892;&#26368;&#32456;&#20998;&#31867;&#12290;&#22312;&#21508;&#31181;&#22270;&#20687;&#25968;&#25454;&#38598;&#21644;&#20998;&#31867;&#22120;&#20307;&#31995;&#32467;&#26500;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#35780;&#20272;&#34920;&#26126;&#65292;CARSO&#33021;&#22815;&#27604;&#26368;&#20808;&#36827;&#30340;&#23545;&#25239;&#24615;&#35757;&#32451;&#26356;&#22909;&#22320;&#20445;&#25252;&#20998;&#31867;&#22120;&#8212;&#8212;&#21516;&#26102;&#20855;&#26377;&#21487;&#25509;&#21463;&#30340;&#28165;&#27905;&#20934;&#30830;&#24230;&#25439;&#22833;&#12290;&#27492;&#22806;&#65292;&#38450;&#24481;&#20307;&#31995;&#32467;&#26500;&#25104;&#21151;&#22320;&#20445;&#25252;&#33258;&#24049;&#20813;&#21463;&#26410;&#39044;&#35265;&#30340;&#23041;&#32961;&#21644;&#26368;&#32456;&#25915;&#20987;&#12290;&#20195;&#30721;&#21644;&#39044;&#35757;&#32451;&#27169;&#22411;&#21487;&#22312;https://github.com/&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose a novel adversarial defence mechanism for image classification -- CARSO -- inspired by cues from cognitive neuroscience. The method is synergistically complementary to adversarial training and relies on knowledge of the internal representation of the attacked classifier. Exploiting a generative model for adversarial purification, conditioned on such representation, it samples reconstructions of inputs to be finally classified. Experimental evaluation by a well-established benchmark of varied, strong adaptive attacks, across diverse image datasets and classifier architectures, shows that CARSO is able to defend the classifier significantly better than state-of-the-art adversarial training alone -- with a tolerable clean accuracy toll. Furthermore, the defensive architecture succeeds in effectively shielding itself from unforeseen threats, and end-to-end attacks adapted to fool stochastic defences. Code and pre-trained models are available at https://github.com/
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21551;&#21457;&#24335;&#36138;&#24515;&#23545;&#25239;&#25915;&#20987;&#65292;&#38024;&#23545;&#22522;&#20110;&#25552;&#31034;&#30340;&#27169;&#26495;&#22312;PLMs&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#28431;&#27934;&#65292;&#36890;&#36807;&#23383;&#31526;&#32423;&#21644;&#21333;&#35789;&#32423;&#30340;&#30772;&#22351;&#26041;&#27861;&#36827;&#34892;&#25915;&#20987;&#65292;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;&#12290;</title><link>http://arxiv.org/abs/2306.05659</link><description>&lt;p&gt;
COVER&#65306;&#19968;&#31181;&#21551;&#21457;&#24335;&#36138;&#24515;&#23545;&#25239;&#25915;&#20987;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#22522;&#20110;&#25552;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
COVER: A Heuristic Greedy Adversarial Attack on Prompt-based Learning in Language Models. (arXiv:2306.05659v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05659
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21551;&#21457;&#24335;&#36138;&#24515;&#23545;&#25239;&#25915;&#20987;&#65292;&#38024;&#23545;&#22522;&#20110;&#25552;&#31034;&#30340;&#27169;&#26495;&#22312;PLMs&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#28431;&#27934;&#65292;&#36890;&#36807;&#23383;&#31526;&#32423;&#21644;&#21333;&#35789;&#32423;&#30340;&#30772;&#22351;&#26041;&#27861;&#36827;&#34892;&#25915;&#20987;&#65292;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25552;&#31034;&#30340;&#23398;&#20064;&#24050;&#34987;&#35777;&#26126;&#26159;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#20013;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#24335;&#65292;&#29305;&#21035;&#26159;&#22312;&#20687;&#23569;&#37327;&#26679;&#26412;&#22330;&#26223;&#36825;&#26679;&#30340;&#20302;&#36164;&#28304;&#24773;&#20917;&#19979;&#12290;&#28982;&#32780;&#65292;PLMs&#30340;&#21487;&#20449;&#24230;&#33267;&#20851;&#37325;&#35201;&#65292;&#24182;&#19988;&#22312;&#22522;&#20110;&#27169;&#26495;&#30340;&#25552;&#31034;&#20013;&#24050;&#32463;&#26174;&#31034;&#20986;&#20102;&#28508;&#22312;&#30340;&#28431;&#27934;&#65292;&#21487;&#33021;&#20250;&#35823;&#23548;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#27979;&#65292;&#24341;&#36215;&#20005;&#37325;&#30340;&#23433;&#20840;&#38382;&#39064;&#12290;&#26412;&#25991;&#36890;&#36807;&#22312;&#40657;&#30418;&#22330;&#26223;&#20013;&#25552;&#20986;&#22522;&#20110;&#25552;&#31034;&#30340;&#23545;&#25239;&#25915;&#20987;&#25163;&#27573;&#65292;&#25581;&#31034;&#20102;PLMs&#30340;&#19968;&#20123;&#28431;&#27934;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#23383;&#31526;&#32423;&#21035;&#21644;&#21333;&#35789;&#32423;&#21035;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#26469;&#30772;&#22351;&#25163;&#21160;&#27169;&#26495;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#22522;&#20110;&#19978;&#36848;&#21551;&#21457;&#24335;&#30772;&#22351;&#26041;&#27861;&#25552;&#20986;&#20102;&#19968;&#31181;&#36138;&#24515;&#31639;&#27861;&#36827;&#34892;&#25915;&#20987;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;BERT&#31995;&#21015;&#27169;&#22411;&#30340;&#19977;&#20010;&#21464;&#31181;&#21644;&#20843;&#20010;&#25968;&#25454;&#38598;&#30340;&#20998;&#31867;&#20219;&#21153;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25915;&#20987;&#25104;&#21151;&#29575;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prompt-based learning has been proved to be an effective way in pre-trained language models (PLMs), especially in low-resource scenarios like few-shot settings. However, the trustworthiness of PLMs is of paramount significance and potential vulnerabilities have been shown in prompt-based templates that could mislead the predictions of language models, causing serious security concerns. In this paper, we will shed light on some vulnerabilities of PLMs, by proposing a prompt-based adversarial attack on manual templates in black box scenarios. First of all, we design character-level and word-level heuristic approaches to break manual templates separately. Then we present a greedy algorithm for the attack based on the above heuristic destructive approaches. Finally, we evaluate our approach with the classification tasks on three variants of BERT series models and eight datasets. And comprehensive experimental results justify the effectiveness of our approach in terms of attack success rate
&lt;/p&gt;</description></item><item><title>INSTRUCTEVAL&#26159;&#19968;&#20010;&#19987;&#27880;&#20110;&#25351;&#23548;&#35843;&#25972;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#30340;&#32508;&#21512;&#22871;&#20214;&#65292;&#23427;&#37319;&#21462;&#20102;&#20840;&#38754;&#30340;&#26041;&#27861;&#26469;&#35780;&#20272;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;&#35299;&#20915;&#38382;&#39064;&#12289;&#20889;&#20316;&#33021;&#21147;&#21644;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#30340;&#19968;&#33268;&#24615;&#31561;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2306.04757</link><description>&lt;p&gt;
INSTRUCTEVAL&#65306;&#38754;&#21521;&#25351;&#23548;&#35843;&#25972;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25972;&#20307;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
INSTRUCTEVAL: Towards Holistic Evaluation of Instruction-Tuned Large Language Models. (arXiv:2306.04757v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04757
&lt;/p&gt;
&lt;p&gt;
INSTRUCTEVAL&#26159;&#19968;&#20010;&#19987;&#27880;&#20110;&#25351;&#23548;&#35843;&#25972;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#30340;&#32508;&#21512;&#22871;&#20214;&#65292;&#23427;&#37319;&#21462;&#20102;&#20840;&#38754;&#30340;&#26041;&#27861;&#26469;&#35780;&#20272;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;&#35299;&#20915;&#38382;&#39064;&#12289;&#20889;&#20316;&#33021;&#21147;&#21644;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#30340;&#19968;&#33268;&#24615;&#31561;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25351;&#23548;&#35843;&#25972;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#20174;&#26681;&#26412;&#19978;&#25913;&#21464;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65292;&#24050;&#32463;&#22312;&#35832;&#22914;&#23545;&#35805;&#20195;&#29702;&#31561;&#24212;&#29992;&#20013;&#26174;&#31034;&#20986;&#20102;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;&#36825;&#20123;&#27169;&#22411;&#65292;&#22914;GPT-4&#65292;&#19981;&#20165;&#33021;&#22815;&#25484;&#25569;&#35821;&#35328;&#65292;&#32780;&#19988;&#21487;&#20197;&#35299;&#20915;&#25968;&#23398;&#12289;&#32534;&#30721;&#12289;&#21307;&#23398;&#21644;&#27861;&#24459;&#31561;&#39046;&#22495;&#30340;&#22797;&#26434;&#20219;&#21153;&#12290;&#23613;&#31649;&#23427;&#20204;&#20855;&#26377;&#21331;&#36234;&#30340;&#33021;&#21147;&#65292;&#20294;&#30001;&#20110;&#35768;&#22810;&#27169;&#22411;&#30340;&#40657;&#30418;&#24615;&#36136;&#21644;&#32570;&#20047;&#20840;&#38754;&#30340;&#35780;&#20272;&#30740;&#31350;&#65292;&#23545;&#23427;&#20204;&#30340;&#20840;&#37096;&#28508;&#21147;&#20173;&#28982;&#32570;&#20047;&#20840;&#38754;&#30340;&#29702;&#35299;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;INSTRUCTEVAL&#65292;&#19968;&#20010;&#26356;&#20840;&#38754;&#30340;&#35780;&#20272;&#22871;&#20214;&#65292;&#19987;&#38376;&#38024;&#23545;&#25351;&#23548;&#35843;&#25972;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#19982;&#20197;&#24448;&#30340;&#20316;&#21697;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#35780;&#20272;&#21253;&#25324;&#23545;&#27169;&#22411;&#22522;&#20110;&#35299;&#20915;&#38382;&#39064;&#12289;&#20889;&#20316;&#33021;&#21147;&#21644;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#30340;&#19968;&#33268;&#24615;&#30340;&#20005;&#26684;&#35780;&#20272;&#12290;&#25105;&#20204;&#37319;&#21462;&#20102;&#20840;&#38754;&#30340;&#26041;&#27861;&#26469;&#20998;&#26512;&#24433;&#21709;&#27169;&#22411;&#24615;&#33021;&#30340;&#21508;&#31181;&#22240;&#32032;&#65292;&#21253;&#25324;&#39044;&#35757;&#32451;&#22522;&#30784;&#12289;&#25351;&#23548;&#35843;&#25972;&#25968;&#25454;&#21644;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Instruction-tuned large language models have revolutionized natural language processing and have shown great potential in applications such as conversational agents. These models, such as GPT-4, can not only master language but also solve complex tasks in areas like mathematics, coding, medicine, and law. Despite their impressive capabilities, there is still a lack of comprehensive understanding regarding their full potential, primarily due to the black-box nature of many models and the absence of holistic evaluation studies. To address these challenges, we present INSTRUCTEVAL, a more comprehensive evaluation suite designed specifically for instruction-tuned large language models. Unlike previous works, our evaluation involves a rigorous assessment of models based on problem-solving, writing ability, and alignment to human values. We take a holistic approach to analyze various factors affecting model performance, including the pretraining foundation, instruction-tuning data, and train
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#23545;&#35805;&#21709;&#24212;&#36873;&#25321;&#30340;&#21518;&#35757;&#32451;&#25216;&#26415;Dial-MAE&#65292;&#21033;&#29992;&#29983;&#25104;&#26041;&#27861;&#26356;&#22909;&#22320;&#21387;&#32553;&#23545;&#35805;&#35821;&#20041;&#33267;&#23494;&#38598;&#21521;&#37327;&#65292;&#24182;&#25552;&#39640;&#23545;&#35805;&#21709;&#24212;&#36873;&#25321;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.04357</link><description>&lt;p&gt;
&#29992;&#20110;&#22522;&#20110;&#26816;&#32034;&#30340;&#23545;&#35805;&#31995;&#32479;&#30340;&#19978;&#19979;&#25991;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
ConTextual Masked Auto-Encoder for Retrieval-based Dialogue Systems. (arXiv:2306.04357v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04357
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#23545;&#35805;&#21709;&#24212;&#36873;&#25321;&#30340;&#21518;&#35757;&#32451;&#25216;&#26415;Dial-MAE&#65292;&#21033;&#29992;&#29983;&#25104;&#26041;&#27861;&#26356;&#22909;&#22320;&#21387;&#32553;&#23545;&#35805;&#35821;&#20041;&#33267;&#23494;&#38598;&#21521;&#37327;&#65292;&#24182;&#25552;&#39640;&#23545;&#35805;&#21709;&#24212;&#36873;&#25321;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#21709;&#24212;&#36873;&#25321;&#26088;&#22312;&#26681;&#25454;&#32473;&#23450;&#30340;&#29992;&#25143;&#21644;&#31995;&#32479;&#35805;&#35821;&#21382;&#21490;&#35760;&#24405;&#20174;&#20960;&#20010;&#20505;&#36873;&#21709;&#24212;&#20013;&#36873;&#25321;&#36866;&#24403;&#30340;&#21709;&#24212;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#36890;&#36807;&#21518;&#35757;&#32451;&#22823;&#22810;&#20381;&#36182;&#20110;&#21333;&#32431;&#30340;&#25513;&#30721;&#35821;&#35328;&#24314;&#27169;&#26041;&#27861;&#26469;&#25552;&#39640;&#23545;&#35805;&#21709;&#24212;&#36873;&#25321;&#30340;&#20934;&#30830;&#24615;&#12290;&#20294;&#26159;&#65292;&#26368;&#36817;&#24320;&#21457;&#30340;&#29983;&#25104;&#26041;&#27861;&#22312;IR&#31038;&#21306;&#23637;&#31034;&#20102;&#26377;&#24076;&#26395;&#30340;&#25991;&#26412;&#34920;&#31034;&#33021;&#21147;&#65292;&#36825;&#21487;&#33021;&#20250;&#23548;&#33268;&#26356;&#22909;&#30340;&#23545;&#35805;&#35821;&#20041;&#24314;&#27169;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986; Dial-MAE&#65288;&#23545;&#35805;&#19978;&#19979;&#25991;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#38024;&#23545;&#23545;&#35805;&#21709;&#24212;&#36873;&#25321;&#30340;&#21518;&#35757;&#32451;&#25216;&#26415;&#12290; Dial-MAE&#20351;&#29992;&#19968;&#20010;&#19981;&#23545;&#31216;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#65292;&#23398;&#20064;&#23558;&#23545;&#35805;&#30340;&#35821;&#20041;&#26356;&#22909;&#22320;&#21387;&#32553;&#21040;&#23494;&#38598;&#21521;&#37327;&#20013;&#12290; Dial-MAE&#30340;&#36807;&#31243;&#21253;&#25324;&#30001;&#28145;&#24230;&#32534;&#30721;&#22120;&#21019;&#24314;&#24102;&#26377;&#25513;&#30721;&#23545;&#35805;&#19978;&#19979;&#25991;&#30340;&#23545;&#35805;&#23884;&#20837;&#65292;&#28982;&#21518;&#26159;&#27973;&#35299;&#30721;&#22120;&#65292;&#35813;&#35299;&#30721;&#22120;&#20351;&#29992;&#27492;&#23884;&#20837;&#20197;&#21450;&#19978;&#19979;&#25991;&#21521;&#37327;&#26469;&#29983;&#25104;&#21709;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dialogue response selection aims to select an appropriate response from several candidates based on a given user and system utterance history. Recent studies have been improving the accuracy of dialogue response selection through post-training, mostly relying on naive masked language modeling methods. However, the recently developed generative methods have shown promising text representation capabilities in IR community, which could potentially lead to better dialogue semantics modeling. Thus, in this paper, we propose Dial-MAE (Dialogue Contextual Masking Auto-encoder), a straightforward yet effective post-training technique tailored for dialogue response selection. Dial-MAE uses an asymmetric encoder-decoder architecture that learns to better compress the semantics of the dialogue into dialogue-dense vectors. The process of Dial-MAE involves a deep encoder creating a dialogue embedding with the masked dialogue context, followed by a shallow decoder that uses this embedding along with
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#30340;&#25298;&#32477;&#34892;&#20026;&#65292;&#24182;&#21457;&#29616;&#36825;&#31181;&#34892;&#20026;&#19981;&#26159;&#23436;&#20840;&#20108;&#20803;&#30340;&#12290;&#20316;&#32773;&#23545;ChatGPT&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#30340;&#20559;&#35265;&#26469;&#33258;&#20110;&#20010;&#21035;&#24037;&#31243;&#24072;&#21644;&#20844;&#21496;&#25919;&#31574;&#65292;&#24182;&#24433;&#21709;&#27169;&#22411;&#36873;&#25321;&#25298;&#32477;&#21738;&#20123;&#25552;&#31034;&#12290;</title><link>http://arxiv.org/abs/2306.03423</link><description>&lt;p&gt;
&#25105;&#23475;&#24597;&#25105;&#20570;&#19981;&#21040;&#65306;&#39044;&#27979;&#40657;&#21283;&#23376;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25552;&#31034;&#25298;&#32477;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
I'm Afraid I Can't Do That: Predicting Prompt Refusal in Black-Box Generative Language Models. (arXiv:2306.03423v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03423
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#30340;&#25298;&#32477;&#34892;&#20026;&#65292;&#24182;&#21457;&#29616;&#36825;&#31181;&#34892;&#20026;&#19981;&#26159;&#23436;&#20840;&#20108;&#20803;&#30340;&#12290;&#20316;&#32773;&#23545;ChatGPT&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#30340;&#20559;&#35265;&#26469;&#33258;&#20110;&#20010;&#21035;&#24037;&#31243;&#24072;&#21644;&#20844;&#21496;&#25919;&#31574;&#65292;&#24182;&#24433;&#21709;&#27169;&#22411;&#36873;&#25321;&#25298;&#32477;&#21738;&#20123;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20174;OpenAI&#30340;ChatGPT&#21457;&#24067;&#20197;&#26469;&#65292;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#22686;&#21152;&#30340;&#20351;&#29992;&#37327;&#20984;&#26174;&#20102;&#29983;&#25104;&#27169;&#22411;&#30340;&#24191;&#27867;&#23454;&#29992;&#24615;&#65292;&#20294;&#21516;&#26102;&#20063;&#25581;&#31034;&#20102;&#19968;&#20123;&#23884;&#20837;&#24335;&#20559;&#35265;&#12290;&#20854;&#20013;&#19968;&#20123;&#26159;&#30001;&#39044;&#35757;&#32451;&#35821;&#26009;&#24211;&#24341;&#36215;&#30340;&#65307;&#20294;&#26159;&#65292;&#38024;&#23545;&#29983;&#25104;&#27169;&#22411;&#30340;&#39069;&#22806;&#20559;&#35265;&#26469;&#33258;&#20110;&#20027;&#35266;&#24494;&#35843;&#20197;&#36991;&#20813;&#29983;&#25104;&#26377;&#23475;&#20869;&#23481;&#12290;&#24494;&#35843;&#20559;&#35265;&#21487;&#33021;&#26469;&#33258;&#20010;&#21035;&#24037;&#31243;&#24072;&#21644;&#20844;&#21496;&#25919;&#31574;&#65292;&#24182;&#24433;&#21709;&#27169;&#22411;&#36873;&#25321;&#25298;&#32477;&#21738;&#20123;&#25552;&#31034;&#12290;&#26412;&#23454;&#39564;&#20351;&#29992;&#40657;&#30418;&#25915;&#20987;&#65292;&#23545;ChatGPT&#30340;&#25298;&#32477;&#34892;&#20026;&#36827;&#34892;&#20102;&#34920;&#24449;&#12290;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;&#21508;&#31181;&#25915;&#20987;&#24615;&#21644;&#33391;&#24615;&#25552;&#31034;&#65288;n = 1,730&#65289;&#26597;&#35810;ChatGPT&#65292;&#28982;&#21518;&#25163;&#21160;&#26631;&#35760;&#27599;&#20010;&#21709;&#24212;&#26159;&#21542;&#23653;&#34892;&#25110;&#25298;&#32477;&#12290;&#21709;&#24212;&#30340;&#25163;&#21160;&#26816;&#26597;&#34920;&#26126;&#65292;&#25298;&#32477;&#19981;&#26159;&#23436;&#20840;&#20108;&#20803;&#30340;&#65292;&#24182;&#19988;&#22312;&#36830;&#32493;&#30340;&#33539;&#22260;&#20869;&#65307;&#22240;&#27492;&#65292;&#25105;&#20204;&#23558;&#20960;&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#21709;&#24212;&#26144;&#23556;&#21040;&#23653;&#34892;&#25110;&#25298;&#32477;&#30340;&#20108;&#20803;&#20540;&#20013;&#12290;&#20351;&#29992;&#20102;&#23567;&#22411;&#25163;&#21160;&#26631;&#35760;&#30340;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Since the release of OpenAI's ChatGPT, generative language models have attracted extensive public attention. The increased usage has highlighted generative models' broad utility, but also revealed several forms of embedded bias. Some is induced by the pre-training corpus; but additional bias specific to generative models arises from the use of subjective fine-tuning to avoid generating harmful content. Fine-tuning bias may come from individual engineers and company policies, and affects which prompts the model chooses to refuse. In this experiment, we characterize ChatGPT's refusal behavior using a black-box attack. We first query ChatGPT with a variety of offensive and benign prompts (n=1,730), then manually label each response as compliance or refusal. Manual examination of responses reveals that refusal is not cleanly binary, and lies on a continuum; as such, we map several different kinds of responses to a binary of compliance or refusal. The small manually-labeled dataset is used 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102; DreamSparse &#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#21033;&#29992;&#20808;&#21069;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#30340; 2D &#20808;&#39564;&#30693;&#35782;&#65292;&#36890;&#36807;&#20960;&#20309;&#27169;&#22359;&#21644;&#31354;&#38388;&#24341;&#23548;&#27169;&#22411;&#26469;&#35299;&#20915; 2D &#27169;&#22411;&#32570;&#20047; 3D &#24863;&#30693;&#33021;&#21147;&#30340;&#38382;&#39064;&#65292;&#36827;&#19968;&#27493;&#23454;&#29616;&#20102;&#20174;&#23569;&#35270;&#35282;&#24773;&#20917;&#19979;&#21512;&#25104;&#39640;&#36136;&#37327;&#30340;&#26032;&#35270;&#35282;&#22270;&#20687;&#12290;</title><link>http://arxiv.org/abs/2306.03414</link><description>&lt;p&gt;
DreamSparse: &#21033;&#29992; 2D &#25193;&#25955;&#27169;&#22411;&#20174;&#31232;&#30095;&#35270;&#35282;&#20013;&#21512;&#25104;&#22270;&#20687;
&lt;/p&gt;
&lt;p&gt;
DreamSparse: Escaping from Plato's Cave with 2D Diffusion Model Given Sparse Views. (arXiv:2306.03414v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03414
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102; DreamSparse &#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#21033;&#29992;&#20808;&#21069;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#30340; 2D &#20808;&#39564;&#30693;&#35782;&#65292;&#36890;&#36807;&#20960;&#20309;&#27169;&#22359;&#21644;&#31354;&#38388;&#24341;&#23548;&#27169;&#22411;&#26469;&#35299;&#20915; 2D &#27169;&#22411;&#32570;&#20047; 3D &#24863;&#30693;&#33021;&#21147;&#30340;&#38382;&#39064;&#65292;&#36827;&#19968;&#27493;&#23454;&#29616;&#20102;&#20174;&#23569;&#35270;&#35282;&#24773;&#20917;&#19979;&#21512;&#25104;&#39640;&#36136;&#37327;&#30340;&#26032;&#35270;&#35282;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#23569;&#37327;&#35270;&#35282;&#20013;&#21512;&#25104;&#26032;&#30340;&#22270;&#20687;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#20294;&#23454;&#38469;&#30340;&#38382;&#39064;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#38590;&#20197;&#20135;&#29983;&#39640;&#36136;&#37327;&#30340;&#32467;&#26524;&#25110;&#22312;&#27492;&#31867;&#23569;&#35270;&#35282;&#35774;&#32622;&#20013;&#38656;&#35201;&#36880;&#20010;&#23545;&#35937;&#20248;&#21270;&#65292;&#22240;&#20026;&#25552;&#20379;&#30340;&#20449;&#24687;&#19981;&#36275;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#21033;&#29992;&#39044;&#20808;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#30340;&#24378;&#22823;&#30340; 2D &#20808;&#39564;&#30693;&#35782;&#65292;&#26469;&#21512;&#25104;&#26032;&#39062;&#30340;&#35270;&#35282;&#22270;&#20687;&#12290;&#28982;&#32780;&#65292;2D &#25193;&#25955;&#27169;&#22411;&#32570;&#20047; 3D &#24863;&#30693;&#33021;&#21147;&#65292;&#23548;&#33268;&#22270;&#20687;&#21512;&#25104;&#22833;&#30495;&#65292;&#24433;&#21709;&#20102;&#22270;&#20687;&#30340;&#35782;&#21035;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; DreamSparse&#65292;&#19968;&#20010;&#21487;&#20197;&#29983;&#25104;&#20960;&#20309;&#21644;&#35782;&#21035;&#32852;&#21512;&#19968;&#33268;&#30340;&#26032;&#35270;&#35282;&#22270;&#20687;&#30340;&#26694;&#26550;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;DreamSparse &#21253;&#25324;&#19968;&#20010;&#20960;&#20309;&#27169;&#22359;&#65292;&#29992;&#20110;&#20174;&#31232;&#30095;&#35270;&#35282;&#33719;&#21462; 3D &#29305;&#24449;&#20316;&#20026; 3D &#20808;&#39564;&#65292;&#38543;&#21518;&#24341;&#20837;&#19968;&#20010;&#31354;&#38388;&#24341;&#23548;&#27169;&#22411;&#23558;&#36825;&#20123; 3D &#29305;&#24449;&#22270;&#36716;&#25442;&#20026;&#29983;&#25104;&#36807;&#31243;&#30340;&#31354;&#38388;&#20449;&#24687;&#12290;&#36825;&#20123;&#20449;&#24687;&#28982;&#21518;&#29992;&#20110;&#36890;&#36807;&#23545;&#25239;&#25439;&#22833;&#25351;&#23548;&#39044;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;&#21512;&#25104;&#39640;&#36136;&#37327;&#30340;&#26032;&#35270;&#35282;&#22270;&#20687;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;DreamSparse &#22312;&#23569;&#35270;&#35282;&#22270;&#20687;&#21512;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#24182;&#19988;&#21487;&#20197;&#29983;&#25104;&#20934;&#30830;&#21644;&#31283;&#20581;&#30340;&#29289;&#20307;&#20960;&#20309;&#21644;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
Synthesizing novel view images from a few views is a challenging but practical problem. Existing methods often struggle with producing high-quality results or necessitate per-object optimization in such few-view settings due to the insufficient information provided. In this work, we explore leveraging the strong 2D priors in pre-trained diffusion models for synthesizing novel view images. 2D diffusion models, nevertheless, lack 3D awareness, leading to distorted image synthesis and compromising the identity. To address these problems, we propose DreamSparse, a framework that enables the frozen pre-trained diffusion model to generate geometry and identity-consistent novel view image. Specifically, DreamSparse incorporates a geometry module designed to capture 3D features from sparse views as a 3D prior. Subsequently, a spatial guidance model is introduced to convert these 3D feature maps into spatial information for the generative process. This information is then used to guide the pre-
&lt;/p&gt;</description></item><item><title>&#26412;&#31687;&#35770;&#25991;&#23545;350&#22810;&#20010;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#36827;&#34892;&#20102;&#20840;&#38754;&#35843;&#26597;&#65292;&#24635;&#32467;&#20102;&#19981;&#21516;&#21333;&#27169;&#21644;&#22810;&#27169;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#24212;&#29992;&#12290;&#35813;&#35843;&#26597;&#20026;&#30740;&#31350;&#20154;&#21592;&#21644;&#20174;&#19994;&#32773;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#36164;&#28304;&#65292;&#24110;&#21161;&#20182;&#20204;&#26356;&#22909;&#22320;&#20102;&#35299;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30446;&#21069;&#30340;&#26368;&#20808;&#36827;&#25216;&#26415;&#65292;&#24182;&#20419;&#36827;&#35813;&#39046;&#22495;&#30340;&#36827;&#19968;&#27493;&#21019;&#26032;&#12290;</title><link>http://arxiv.org/abs/2306.02781</link><description>&lt;p&gt;
&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
A survey of Generative AI Applications. (arXiv:2306.02781v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02781
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#35770;&#25991;&#23545;350&#22810;&#20010;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#36827;&#34892;&#20102;&#20840;&#38754;&#35843;&#26597;&#65292;&#24635;&#32467;&#20102;&#19981;&#21516;&#21333;&#27169;&#21644;&#22810;&#27169;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#24212;&#29992;&#12290;&#35813;&#35843;&#26597;&#20026;&#30740;&#31350;&#20154;&#21592;&#21644;&#20174;&#19994;&#32773;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#36164;&#28304;&#65292;&#24110;&#21161;&#20182;&#20204;&#26356;&#22909;&#22320;&#20102;&#35299;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30446;&#21069;&#30340;&#26368;&#20808;&#36827;&#25216;&#26415;&#65292;&#24182;&#20419;&#36827;&#35813;&#39046;&#22495;&#30340;&#36827;&#19968;&#27493;&#21019;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#26377;&#20102;&#26174;&#33879;&#22686;&#38271;&#65292;&#24182;&#22312;&#21508;&#20010;&#39046;&#22495;&#23637;&#31034;&#20102;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;&#26412;&#25991;&#23545;350&#22810;&#20010;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#36827;&#34892;&#20102;&#20840;&#38754;&#35843;&#26597;&#65292;&#25552;&#20379;&#20102;&#20998;&#31867;&#32467;&#26500;&#21644;&#23545;&#19981;&#21516;&#21333;&#27169;&#21644;&#22810;&#27169;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#31616;&#27905;&#25551;&#36848;&#12290;&#35813;&#35843;&#26597;&#20998;&#25104;&#22810;&#20010;&#37096;&#20998;&#65292;&#35206;&#30422;&#20102;&#25991;&#26412;&#12289;&#22270;&#20687;&#12289;&#35270;&#39057;&#12289;&#28216;&#25103;&#21644;&#33041;&#20449;&#24687;&#31561;&#21333;&#27169;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#24191;&#27867;&#24212;&#29992;&#12290;&#25105;&#20204;&#30340;&#35843;&#30740;&#26088;&#22312;&#20026;&#30740;&#31350;&#20154;&#21592;&#21644;&#20174;&#19994;&#32773;&#25552;&#20379;&#23453;&#36149;&#30340;&#36164;&#28304;&#65292;&#24110;&#21161;&#20182;&#20204;&#26356;&#22909;&#22320;&#20102;&#35299;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30446;&#21069;&#30340;&#26368;&#20808;&#36827;&#25216;&#26415;&#65292;&#24182;&#20419;&#36827;&#35813;&#39046;&#22495;&#30340;&#36827;&#19968;&#27493;&#21019;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative AI has experienced remarkable growth in recent years, leading to a wide array of applications across diverse domains. In this paper, we present a comprehensive survey of more than 350 generative AI applications, providing a structured taxonomy and concise descriptions of various unimodal and even multimodal generative AIs. The survey is organized into sections, covering a wide range of unimodal generative AI applications such as text, images, video, gaming and brain information. Our survey aims to serve as a valuable resource for researchers and practitioners to navigate the rapidly expanding landscape of generative AI, facilitating a better understanding of the current state-of-the-art and fostering further innovation in the field.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;PassGPT&#36827;&#34892;&#23494;&#30721;&#24314;&#27169;&#21644;&#29983;&#25104;&#65292;&#35813;&#27169;&#22411;&#27604;&#22522;&#20110;GAN&#30340;&#29616;&#26377;&#26041;&#27861;&#26356;&#20934;&#30830;&#65292;&#33021;&#29983;&#25104;&#31526;&#21512;&#20219;&#24847;&#38480;&#21046;&#30340;&#23494;&#30721;&#65292;&#20026;&#25552;&#39640;&#23494;&#30721;&#24378;&#24230;&#20272;&#35745;&#22120;&#25552;&#20379;&#20102;&#28508;&#22312;&#30340;&#24110;&#21161;&#12290;</title><link>http://arxiv.org/abs/2306.01545</link><description>&lt;p&gt;
PassGPT: &#22823;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#23494;&#30721;&#24314;&#27169;&#21644;&#65288;&#24341;&#23548;&#24335;&#65289;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
PassGPT: Password Modeling and (Guided) Generation with Large Language Models. (arXiv:2306.01545v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01545
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;PassGPT&#36827;&#34892;&#23494;&#30721;&#24314;&#27169;&#21644;&#29983;&#25104;&#65292;&#35813;&#27169;&#22411;&#27604;&#22522;&#20110;GAN&#30340;&#29616;&#26377;&#26041;&#27861;&#26356;&#20934;&#30830;&#65292;&#33021;&#29983;&#25104;&#31526;&#21512;&#20219;&#24847;&#38480;&#21046;&#30340;&#23494;&#30721;&#65292;&#20026;&#25552;&#39640;&#23494;&#30721;&#24378;&#24230;&#20272;&#35745;&#22120;&#25552;&#20379;&#20102;&#28508;&#22312;&#30340;&#24110;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21487;&#20197;&#25104;&#21151;&#22320;&#27169;&#25311;&#33258;&#28982;&#35821;&#35328;&#65292;&#26080;&#38656;&#26126;&#30830;&#30340;&#30417;&#30563;&#65292;&#20165;&#36890;&#36807;&#22823;&#37327;&#30340;&#25991;&#26412;&#36827;&#34892;&#35757;&#32451;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;LLMs&#22312;&#24314;&#27169;&#23494;&#30721;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;PassGPT&#65292;&#23427;&#26159;&#19968;&#20010;&#22312;&#23494;&#30721;&#27844;&#38706;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#30340;LLM&#65292;&#29992;&#20110;&#29983;&#25104;&#23494;&#30721;&#12290;PassGPT&#36890;&#36807;&#29468;&#27979;&#20004;&#20493;&#20110;&#22522;&#20110;&#29983;&#25104;&#24615;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#30340;&#29616;&#26377;&#26041;&#27861;&#20013;&#30340;&#20197;&#21069;&#26410;&#35265;&#36807;&#30340;&#23494;&#30721;&#32780;&#32988;&#36807;&#20854;&#23427;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#24341;&#23548;&#24335;&#23494;&#30721;&#29983;&#25104;&#30340;&#27010;&#24565;&#65292;&#21033;&#29992;PassGPT&#30340;&#25277;&#26679;&#36807;&#31243;&#29983;&#25104;&#31526;&#21512;&#20219;&#24847;&#38480;&#21046;&#30340;&#23494;&#30721;&#65292;&#36825;&#22312;&#24403;&#21069;&#22522;&#20110;GAN&#30340;&#31574;&#30053;&#20013;&#26159;&#32570;&#20047;&#30340;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23545;PassGPT&#23545;&#23494;&#30721;&#23450;&#20041;&#30340;&#29109;&#21644;&#27010;&#29575;&#20998;&#24067;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#65292;&#24182;&#35752;&#35770;&#20102;&#20854;&#22312;&#22686;&#24378;&#29616;&#26377;&#23494;&#30721;&#24378;&#24230;&#20272;&#35745;&#22120;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) successfully model natural language from vast amounts of text without the need for explicit supervision. In this paper, we investigate the efficacy of LLMs in modeling passwords. We present PassGPT, a LLM trained on password leaks for password generation. PassGPT outperforms existing methods based on generative adversarial networks (GAN) by guessing twice as many previously unseen passwords. Furthermore, we introduce the concept of guided password generation, where we leverage PassGPT sampling procedure to generate passwords matching arbitrary constraints, a feat lacking in current GAN-based strategies. Lastly, we conduct an in-depth analysis of the entropy and probability distribution that PassGPT defines over passwords and discuss their use in enhancing existing password strength estimators.
&lt;/p&gt;</description></item><item><title>&#24247;&#31185;&#36842;&#20122;&#26694;&#26550;&#26159;&#19968;&#20010;&#25903;&#25345;&#21508;&#31181;&#27010;&#29575;&#29702;&#35770;&#65292;&#20811;&#26381;&#20102;&#29616;&#26377;&#25216;&#26415;&#30340;&#38480;&#21046;&#30340;&#24182;&#34892;&#30340;&#31070;&#32463;&#31526;&#21495;&#32467;&#26500;&#65292;&#25104;&#21151;&#24212;&#29992;&#20110;&#38598;&#21512;&#27963;&#21160;&#26816;&#27979;&#12289;&#23454;&#20307;&#38142;&#25509;&#21644;&#25512;&#33616;&#20219;&#21153;&#65292;&#25552;&#39640;&#20102;&#26368;&#26032;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.00480</link><description>&lt;p&gt;
&#19982;&#24247;&#31185;&#36842;&#20122;&#24182;&#34892;&#30340;&#31070;&#32463;&#31526;&#21495;&#19968;&#20307;&#21270;
&lt;/p&gt;
&lt;p&gt;
Parallel Neurosymbolic Integration with Concordia. (arXiv:2306.00480v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00480
&lt;/p&gt;
&lt;p&gt;
&#24247;&#31185;&#36842;&#20122;&#26694;&#26550;&#26159;&#19968;&#20010;&#25903;&#25345;&#21508;&#31181;&#27010;&#29575;&#29702;&#35770;&#65292;&#20811;&#26381;&#20102;&#29616;&#26377;&#25216;&#26415;&#30340;&#38480;&#21046;&#30340;&#24182;&#34892;&#30340;&#31070;&#32463;&#31526;&#21495;&#32467;&#26500;&#65292;&#25104;&#21151;&#24212;&#29992;&#20110;&#38598;&#21512;&#27963;&#21160;&#26816;&#27979;&#12289;&#23454;&#20307;&#38142;&#25509;&#21644;&#25512;&#33616;&#20219;&#21153;&#65292;&#25552;&#39640;&#20102;&#26368;&#26032;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24182;&#34892;&#30340;&#31070;&#32463;&#31526;&#21495;&#32467;&#26500;&#36890;&#36807;&#23558;&#36923;&#36753;&#29702;&#35770;&#30340;&#30693;&#35782;&#25552;&#21462;&#21040;&#28145;&#24230;&#27169;&#22411;&#20013;&#65292;&#22312;NLP&#20013;&#24471;&#21040;&#26377;&#25928;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#25216;&#26415;&#20173;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#65292;&#21253;&#25324;&#25903;&#25345;&#21463;&#38480;&#24418;&#24335;&#30340;&#36923;&#36753;&#29702;&#35770;&#65292;&#24182;&#20381;&#36182;&#20110;&#36923;&#36753;&#21644;&#28145;&#24230;&#32593;&#32476;&#20043;&#38388;&#30340;&#29420;&#31435;&#24615;&#20551;&#35774;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#24247;&#31185;&#36842;&#20122;&#26694;&#26550;&#65292;&#20811;&#26381;&#20102;&#29616;&#26377;&#25216;&#26415;&#30340;&#38480;&#21046;&#12290;&#24247;&#31185;&#36842;&#20122;&#26694;&#26550;&#26082;&#19981;&#20851;&#27880;&#28145;&#24230;&#32593;&#32476;&#65292;&#20063;&#19981;&#20851;&#27880;&#36923;&#36753;&#29702;&#35770;&#65292;&#25903;&#25345;&#21508;&#31181;&#27010;&#29575;&#29702;&#35770;&#12290;&#26694;&#26550;&#21487;&#20197;&#25903;&#25345;&#20004;&#20010;&#32452;&#20214;&#30340;&#30417;&#30563;&#35757;&#32451;&#21644;&#31070;&#32463;&#32452;&#20214;&#30340;&#26080;&#30417;&#30563;&#35757;&#32451;&#12290;&#24247;&#31185;&#36842;&#20122;&#24050;&#25104;&#21151;&#24212;&#29992;&#20110;NLP&#21644;&#25968;&#25454;&#20998;&#31867;&#20197;&#22806;&#30340;&#20219;&#21153;&#65292;&#25552;&#39640;&#20102;&#38598;&#21512;&#27963;&#21160;&#26816;&#27979;&#12289;&#23454;&#20307;&#38142;&#25509;&#21644;&#25512;&#33616;&#20219;&#21153;&#30340;&#26368;&#26032;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Parallel neurosymbolic architectures have been applied effectively in NLP by distilling knowledge from a logic theory into a deep model.However, prior art faces several limitations including supporting restricted forms of logic theories and relying on the assumption of independence between the logic and the deep network. We present Concordia, a framework overcoming the limitations of prior art. Concordia is agnostic both to the deep network and the logic theory offering support for a wide range of probabilistic theories. Our framework can support supervised training of both components and unsupervised training of the neural component. Concordia has been successfully applied to tasks beyond NLP and data classification, improving the accuracy of state-of-the-art on collective activity detection, entity linking and recommendation tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#25968;&#25454;&#21464;&#25442;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#29616;&#33021;&#21147;&#65292;&#35777;&#26126;&#20102;&#21333;&#35843;&#26368;&#22823;&#21644;&#30340;GNN&#19982;Datalog&#20043;&#38388;&#30340;&#20851;&#31995;&#32039;&#23494;&#65292;&#21363;&#20219;&#20309;&#21333;&#35843;&#26368;&#22823;&#21644;&#30340;GNN&#37117;&#21487;&#20197;&#34920;&#31034;&#20026;&#19968;&#20010;Datalog&#31243;&#24207;&#65292;&#21453;&#20043;&#20134;&#28982;&#12290;</title><link>http://arxiv.org/abs/2305.18015</link><description>&lt;p&gt;
&#22522;&#20110;&#21333;&#35843;&#26368;&#22823;&#21644;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#19982;Datalog&#30340;&#23545;&#24212;&#20851;&#31995;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Correspondence Between Monotonic Max-Sum GNNs and Datalog. (arXiv:2305.18015v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18015
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#25968;&#25454;&#21464;&#25442;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#29616;&#33021;&#21147;&#65292;&#35777;&#26126;&#20102;&#21333;&#35843;&#26368;&#22823;&#21644;&#30340;GNN&#19982;Datalog&#20043;&#38388;&#30340;&#20851;&#31995;&#32039;&#23494;&#65292;&#21363;&#20219;&#20309;&#21333;&#35843;&#26368;&#22823;&#21644;&#30340;GNN&#37117;&#21487;&#20197;&#34920;&#31034;&#20026;&#19968;&#20010;Datalog&#31243;&#24207;&#65292;&#21453;&#20043;&#20134;&#28982;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#24212;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#21040;&#32467;&#26500;&#21270;&#25968;&#25454;&#19978;&#24341;&#36215;&#20102;&#24191;&#27867;&#30340;&#20851;&#27880;&#65292;&#20294;&#36825;&#20123;&#25216;&#26415;&#30340;&#34920;&#36798;&#33021;&#21147;&#65288;&#21363;&#21487;&#20197;&#23398;&#20064;&#21040;&#20160;&#20040;&#65289;&#20173;&#28982;&#19981;&#22826;&#28165;&#26970;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#25968;&#25454;&#21464;&#25442;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#27880;&#24847;&#21040;&#22914;&#20309;&#23558;&#25968;&#25454;&#38598;&#32534;&#30721;&#25104;GNN&#21487;&#22788;&#29702;&#30340;&#25968;&#23383;&#24418;&#24335;&#21487;&#20197;&#27169;&#31946;&#27169;&#22411;&#34920;&#36798;&#33021;&#21147;&#30340;&#25551;&#36848;&#65292;&#25105;&#20204;&#35748;&#20026;&#19968;&#31181;&#35268;&#33539;&#32534;&#30721;&#25552;&#20379;&#20102;&#36866;&#24403;&#30340;&#22522;&#30784;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#21333;&#35843;&#26368;&#22823;&#21644;&#30340;GNN&#30340;&#34920;&#29616;&#33021;&#21147;&#65292;&#23427;&#20204;&#28085;&#30422;&#20102;&#20855;&#26377;max&#21644;sum&#32858;&#21512;&#20989;&#25968;&#30340;GNN&#30340;&#19968;&#20010;&#23376;&#31867;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#23545;&#20110;&#27599;&#20010;&#36825;&#26679;&#30340;GNN&#65292;&#21487;&#20197;&#35745;&#31639;&#20986;&#19968;&#20010;Datalog&#31243;&#24207;&#65292;&#20351;&#24471;&#23558;GNN&#24212;&#29992;&#20110;&#20219;&#20309;&#25968;&#25454;&#38598;&#37117;&#20250;&#20135;&#29983;&#19982;&#23558;&#31243;&#24207;&#35268;&#21017;&#24212;&#29992;&#20110;&#25968;&#25454;&#38598;&#30340;&#21333;&#20010;&#24490;&#29615;&#30456;&#21516;&#30340;&#20107;&#23454;&#12290;&#21333;&#35843;&#26368;&#22823;&#21644;&#30340;GNN&#33021;&#22815;&#23545;&#26080;&#38480;&#25968;&#37327;&#30340;&#29305;&#24449;&#21521;&#37327;&#27714;&#21644;&#65292;&#36825;&#21487;&#33021;&#20250;&#23548;&#33268;&#29305;&#24449;&#20540;&#20219;&#24847;&#22686;&#22823;&#65292;&#32780;&#22522;&#20110;&#35268;&#21017;&#30340;&#31995;&#32479;&#65288;&#20363;&#22914;Datalog&#65289;&#20855;&#26377;&#20445;&#35777;&#31995;&#32479;&#22987;&#32456;&#20250;&#36235;&#20110;&#31283;&#23450;&#29366;&#24577;&#30340;&#21333;&#35843;&#24615;&#36136;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#21333;&#35843;&#26368;&#22823;&#21644;&#30340;GNN&#19982;Datalog&#20043;&#38388;&#30340;&#36825;&#31181;&#20851;&#31995;&#26159;&#32039;&#23494;&#30340;&#65292;&#21363;&#20219;&#20309;&#21333;&#35843;&#26368;&#22823;&#21644;&#30340;GNN&#37117;&#21487;&#20197;&#34920;&#31034;&#20026;&#19968;&#20010;Datalog&#31243;&#24207;&#65292;&#21453;&#20043;&#20134;&#28982;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although there has been significant interest in applying machine learning techniques to structured data, the expressivity (i.e., a description of what can be learned) of such techniques is still poorly understood. In this paper, we study data transformations based on graph neural networks (GNNs). First, we note that the choice of how a dataset is encoded into a numeric form processable by a GNN can obscure the characterisation of a model's expressivity, and we argue that a canonical encoding provides an appropriate basis. Second, we study the expressivity of monotonic max-sum GNNs, which cover a subclass of GNNs with max and sum aggregation functions. We show that, for each such GNN, one can compute a Datalog program such that applying the GNN to any dataset produces the same facts as a single round of application of the program's rules to the dataset. Monotonic max-sum GNNs can sum an unbounded number of feature vectors which can result in arbitrarily large feature values, whereas rul
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20154;&#21592;&#30740;&#31350;&#20102;&#22312;&#25968;&#25454;&#21463;&#38480;&#21046;&#30340;&#24773;&#20917;&#19979;&#32553;&#25918;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#35745;&#31639;&#26368;&#20248;&#24615;&#30340;&#32553;&#25918;&#23450;&#24459;&#65292;&#32771;&#34385;&#21040;&#37325;&#22797;&#20196;&#29260;&#21644;&#36807;&#37327;&#21442;&#25968;&#30340;&#20215;&#20540;&#36882;&#20943;&#12290;</title><link>http://arxiv.org/abs/2305.16264</link><description>&lt;p&gt;
&#32553;&#25918;&#25968;&#25454;&#21463;&#38480;&#30340;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Scaling Data-Constrained Language Models. (arXiv:2305.16264v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16264
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20154;&#21592;&#30740;&#31350;&#20102;&#22312;&#25968;&#25454;&#21463;&#38480;&#21046;&#30340;&#24773;&#20917;&#19979;&#32553;&#25918;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#35745;&#31639;&#26368;&#20248;&#24615;&#30340;&#32553;&#25918;&#23450;&#24459;&#65292;&#32771;&#34385;&#21040;&#37325;&#22797;&#20196;&#29260;&#21644;&#36807;&#37327;&#21442;&#25968;&#30340;&#20215;&#20540;&#36882;&#20943;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#22312;&#25193;&#23637;&#35821;&#35328;&#27169;&#22411;&#30340;&#36235;&#21183;&#28041;&#21450;&#22686;&#21152;&#21442;&#25968;&#35745;&#25968;&#21644;&#35757;&#32451;&#25968;&#25454;&#38598;&#22823;&#23567;&#12290;&#25512;&#26029;&#36825;&#20010;&#36235;&#21183;&#34920;&#26126;&#65292;&#35757;&#32451;&#25968;&#25454;&#38598;&#22823;&#23567;&#21487;&#33021;&#24456;&#24555;&#23601;&#20250;&#21463;&#21040;&#20114;&#32852;&#32593;&#19978;&#21487;&#29992;&#25991;&#26412;&#25968;&#25454;&#30340;&#38480;&#21046;&#12290;&#20986;&#20110;&#27492;&#38480;&#21046;&#30340;&#21160;&#26426;&#65292;&#25105;&#20204;&#30740;&#31350;&#22312;&#25968;&#25454;&#21463;&#38480;&#21046;&#30340;&#24773;&#20917;&#19979;&#32553;&#25918;&#35821;&#35328;&#27169;&#22411;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36816;&#34892;&#20102;&#22823;&#37327;&#30340;&#23454;&#39564;&#65292;&#21464;&#21270;&#25968;&#25454;&#37325;&#22797;&#31243;&#24230;&#21644;&#35745;&#31639;&#39044;&#31639;&#65292;&#33539;&#22260;&#36798;&#21040;&#20102;9000&#20159;&#20010;&#35757;&#32451;&#20196;&#29260;&#21644;9&#20159;&#21442;&#25968;&#27169;&#22411;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#26377;&#38480;&#30340;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;&#39640;&#36798;4&#27425;&#37325;&#22797;&#25968;&#25454;&#30340;&#35757;&#32451;&#19982;&#20351;&#29992;&#21807;&#19968;&#25968;&#25454;&#30456;&#27604;&#23545;&#25439;&#22833;&#30340;&#36129;&#29486;&#24494;&#19981;&#36275;&#36947;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#26356;&#22810;&#30340;&#37325;&#22797;&#25968;&#25454;&#65292;&#28155;&#21152;&#35745;&#31639;&#30340;&#20215;&#20540;&#26368;&#32456;&#20250;&#34928;&#20943;&#20026;&#38646;&#12290;&#25105;&#20204;&#25552;&#20986;&#24182;&#32463;&#39564;&#35777;&#20102;&#19968;&#20010;&#35745;&#31639;&#26368;&#20248;&#24615;&#30340;&#32553;&#25918;&#23450;&#24459;&#65292;&#32771;&#34385;&#21040;&#37325;&#22797;&#20196;&#29260;&#21644;&#36807;&#37327;&#21442;&#25968;&#30340;&#20215;&#20540;&#36882;&#20943;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23581;&#35797;&#20102;&#32531;&#35299;&#25968;&#25454;&#31232;&#32570;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The current trend of scaling language models involves increasing both parameter count and training dataset size. Extrapolating this trend suggests that training dataset size may soon be limited by the amount of text data available on the internet. Motivated by this limit, we investigate scaling language models in data-constrained regimes. Specifically, we run a large set of experiments varying the extent of data repetition and compute budget, ranging up to 900 billion training tokens and 9 billion parameter models. We find that with constrained data for a fixed compute budget, training with up to 4 epochs of repeated data yields negligible changes to loss compared to having unique data. However, with more repetition, the value of adding compute eventually decays to zero. We propose and empirically validate a scaling law for compute optimality that accounts for the decreasing value of repeated tokens and excess parameters. Finally, we experiment with approaches mitigating data scarcity,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#24320;&#25918;&#35789;&#27719;&#21344;&#25454;&#65288;OVO&#65289;&#26041;&#27861;&#65292;&#21487;&#20197;&#20801;&#35768;&#20219;&#24847;&#31867;&#21035;&#30340;&#35821;&#20041;&#21344;&#25454;&#39044;&#27979;&#65292;&#26080;&#38656;&#19977;&#32500;&#27880;&#37322;&#65292;&#20854;&#20851;&#38190;&#25216;&#26415;&#21253;&#25324;&#39044;&#35757;&#32451;&#20108;&#32500;&#24320;&#25918;&#35789;&#27719;&#20998;&#21106;&#27169;&#22411;&#21040;&#19977;&#32500;&#21344;&#25454;&#32593;&#32476;&#30340;&#30693;&#35782;&#33976;&#39311;&#21644;&#20687;&#32032;-&#20307;&#32032;&#36807;&#28388;&#20197;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2305.16133</link><description>&lt;p&gt;
OVO: &#24320;&#25918;&#35789;&#27719;&#21344;&#25454;
&lt;/p&gt;
&lt;p&gt;
OVO: Open-Vocabulary Occupancy. (arXiv:2305.16133v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16133
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#24320;&#25918;&#35789;&#27719;&#21344;&#25454;&#65288;OVO&#65289;&#26041;&#27861;&#65292;&#21487;&#20197;&#20801;&#35768;&#20219;&#24847;&#31867;&#21035;&#30340;&#35821;&#20041;&#21344;&#25454;&#39044;&#27979;&#65292;&#26080;&#38656;&#19977;&#32500;&#27880;&#37322;&#65292;&#20854;&#20851;&#38190;&#25216;&#26415;&#21253;&#25324;&#39044;&#35757;&#32451;&#20108;&#32500;&#24320;&#25918;&#35789;&#27719;&#20998;&#21106;&#27169;&#22411;&#21040;&#19977;&#32500;&#21344;&#25454;&#32593;&#32476;&#30340;&#30693;&#35782;&#33976;&#39311;&#21644;&#20687;&#32032;-&#20307;&#32032;&#36807;&#28388;&#20197;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#20041;&#21344;&#25454;&#39044;&#27979;&#26088;&#22312;&#25512;&#26029;&#20986;&#33258;&#20027;&#20195;&#29702;&#22312;&#19977;&#32500;&#29615;&#22659;&#20013;&#25805;&#20316;&#30340;&#23494;&#38598;&#20960;&#20309;&#21644;&#35821;&#20041;&#12290;&#29616;&#26377;&#30340;&#21344;&#25454;&#39044;&#27979;&#26041;&#27861;&#20960;&#20046;&#23436;&#20840;&#26159;&#22522;&#20110;&#20154;&#24037;&#27880;&#37322;&#30340;&#20307;&#31215;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#30340;&#12290;&#34429;&#28982;&#36825;&#20123;&#25968;&#25454;&#26377;&#39640;&#36136;&#37327;&#65292;&#20294;&#26159;&#29983;&#25104;&#36825;&#20123;&#19977;&#32500;&#27880;&#37322;&#26159;&#36153;&#21147;&#19988;&#25104;&#26412;&#39640;&#26114;&#30340;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#20165;&#21487;&#20197;&#35757;&#32451;&#23569;&#37327;&#29305;&#23450;&#29289;&#20307;&#31867;&#21035;&#30340;&#25968;&#25454;&#38598;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38480;&#21046;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65306;&#24320;&#25918;&#35789;&#27719;&#21344;&#25454;&#65288;OVO&#65289;&#65292;&#20801;&#35768;&#20219;&#24847;&#31867;&#21035;&#30340;&#35821;&#20041;&#21344;&#25454;&#39044;&#27979;&#65292;&#20294;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#26080;&#38656;&#19977;&#32500;&#27880;&#37322;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#20851;&#38190;&#26159;&#65306;&#65288;1&#65289;&#20174;&#39044;&#35757;&#32451;&#30340;&#20108;&#32500;&#24320;&#25918;&#35789;&#27719;&#20998;&#21106;&#27169;&#22411;&#21040;&#19977;&#32500;&#21344;&#25454;&#32593;&#32476;&#30340;&#30693;&#35782;&#33976;&#39311;&#65292;&#65288;2&#65289;&#20687;&#32032; - &#20307;&#32032;&#36807;&#28388;&#20197;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#35813;&#26694;&#26550;&#31616;&#21333;&#12289;&#32039;&#20945;&#19988;&#20860;&#23481;&#22823;&#22810;&#25968;&#26368;&#20808;&#36827;&#30340;&#35821;&#20041;&#21344;&#25454;&#39044;&#27979;&#27169;&#22411;&#12290;&#22312; NYUv2 &#21644; SemanticKIT &#19978;&#30340;&#23454;&#39564;&#26174;&#31034;&#20986;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Semantic occupancy prediction aims to infer dense geometry and semantics of surroundings for an autonomous agent to operate safely in the 3D environment. Existing occupancy prediction methods are almost entirely trained on human-annotated volumetric data. Although of high quality, the generation of such 3D annotations is laborious and costly, restricting them to a few specific object categories in the training dataset. To address this limitation, this paper proposes Open Vocabulary Occupancy (OVO), a novel approach that allows semantic occupancy prediction of arbitrary classes but without the need for 3D annotations during training. Keys to our approach are (1) knowledge distillation from a pre-trained 2D open-vocabulary segmentation model to the 3D occupancy network, and (2) pixel-voxel filtering for high-quality training data generation. The resulting framework is simple, compact, and compatible with most state-of-the-art semantic occupancy prediction models. On NYUv2 and SemanticKIT
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#25628;&#32034;&#26041;&#27861;&#8212;&#8212;&#31070;&#32463;&#32467;&#26500;&#32534;&#30721;&#65288;NAC&#65289;&#65292;&#23427;&#36890;&#36807;&#31232;&#30095;&#32534;&#30721;&#23547;&#25214;&#26368;&#20248;&#32467;&#26500;&#21442;&#25968;&#65292;&#26080;&#38656;&#35757;&#32451;&#23601;&#33021;&#21457;&#25381;&#34920;&#29616;&#21147;&#65292;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#24615;&#33021;&#65292;&#24182;&#19988;&#36816;&#31639;&#36895;&#24230;&#27604;&#24378;&#22522;&#32447;&#26041;&#27861;&#24555;&#20102;200&#20493;&#65292;&#31934;&#24230;&#25552;&#39640;&#20102;18.8&#65285;&#12290;</title><link>http://arxiv.org/abs/2305.14065</link><description>&lt;p&gt;
&#19981;&#35201;&#35757;&#32451;&#23427;&#65306;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#32447;&#24615;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Do Not Train It: A Linear Neural Architecture Search of Graph Neural Networks. (arXiv:2305.14065v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14065
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#25628;&#32034;&#26041;&#27861;&#8212;&#8212;&#31070;&#32463;&#32467;&#26500;&#32534;&#30721;&#65288;NAC&#65289;&#65292;&#23427;&#36890;&#36807;&#31232;&#30095;&#32534;&#30721;&#23547;&#25214;&#26368;&#20248;&#32467;&#26500;&#21442;&#25968;&#65292;&#26080;&#38656;&#35757;&#32451;&#23601;&#33021;&#21457;&#25381;&#34920;&#29616;&#21147;&#65292;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#24615;&#33021;&#65292;&#24182;&#19988;&#36816;&#31639;&#36895;&#24230;&#27604;&#24378;&#22522;&#32447;&#26041;&#27861;&#24555;&#20102;200&#20493;&#65292;&#31934;&#24230;&#25552;&#39640;&#20102;18.8&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65288;NAS-GNN&#65289;&#24050;&#32463;&#26174;&#33879;&#22320;&#25552;&#39640;&#20102;&#25163;&#21160;&#35774;&#35745;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#32487;&#25215;&#20102;&#20256;&#32479;NAS&#26041;&#27861;&#30340;&#38382;&#39064;&#65292;&#22914;&#39640;&#35745;&#31639;&#25104;&#26412;&#21644;&#20248;&#21270;&#38590;&#24230;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#20197;&#21069;&#30340;NAS&#26041;&#27861;&#24573;&#35270;&#20102;GNN&#30340;&#29420;&#29305;&#24615;&#65292;&#21363;GNN&#20855;&#26377;&#26080;&#38656;&#35757;&#32451;&#23601;&#20855;&#26377;&#34920;&#29616;&#21147;&#30340;&#29305;&#28857;&#12290;&#37319;&#29992;&#38543;&#26426;&#21021;&#22987;&#21270;&#30340;&#26435;&#37325;&#65292;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#31232;&#30095;&#32534;&#30721;&#30446;&#26631;&#23547;&#25214;&#26368;&#20248;&#30340;&#26550;&#26500;&#21442;&#25968;&#65292;&#24182;&#24471;&#20986;&#19968;&#31181;&#26032;&#30340;NAS-GNN&#26041;&#27861;&#65292;&#21363;&#31070;&#32463;&#32467;&#26500;&#32534;&#30721;&#65288;NAC&#65289;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;NAC&#22312;GNN&#19978;&#23454;&#29616;&#20102;&#26080;&#26356;&#26032;&#26041;&#26696;&#65292;&#21487;&#20197;&#22312;&#32447;&#24615;&#26102;&#38388;&#20869;&#39640;&#25928;&#35745;&#31639;&#12290;&#22312;&#22810;&#20010;GNN&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#35777;&#35780;&#20272;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23548;&#33268;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#27604;&#24378;&#22522;&#32447;&#26041;&#27861;&#24555;200&#20493;&#65292;&#31934;&#24230;&#25552;&#39640;&#20102;18.8&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural architecture search (NAS) for Graph neural networks (GNNs), called NAS-GNNs, has achieved significant performance over manually designed GNN architectures. However, these methods inherit issues from the conventional NAS methods, such as high computational cost and optimization difficulty. More importantly, previous NAS methods have ignored the uniqueness of GNNs, where GNNs possess expressive power without training. With the randomly-initialized weights, we can then seek the optimal architecture parameters via the sparse coding objective and derive a novel NAS-GNNs method, namely neural architecture coding (NAC). Consequently, our NAC holds a no-update scheme on GNNs and can efficiently compute in linear time. Empirical evaluations on multiple GNN benchmark datasets demonstrate that our approach leads to state-of-the-art performance, which is up to $200\times$ faster and $18.8\%$ more accurate than the strong baselines.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20803;&#23398;&#20064;&#21644;&#20449;&#36947;&#29366;&#24577;&#20449;&#24687;&#30340;&#23460;&#20869;&#23450;&#20301;&#27169;&#22411;&#65292;&#20197;&#35299;&#20915;&#28145;&#24230;&#23398;&#20064;&#23450;&#20301;&#27169;&#22411;&#20013;&#25345;&#32493;&#23384;&#22312;&#30340;&#36890;&#29992;&#24615;&#32570;&#22833;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.13453</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#20803;&#23398;&#20064;&#21644;&#20449;&#36947;&#29366;&#24577;&#20449;&#24687;&#30340;&#21487;&#25512;&#24191;&#23460;&#20869;&#23450;&#20301;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Meta-learning based Generalizable Indoor Localization Model using Channel State Information. (arXiv:2305.13453v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13453
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20803;&#23398;&#20064;&#21644;&#20449;&#36947;&#29366;&#24577;&#20449;&#24687;&#30340;&#23460;&#20869;&#23450;&#20301;&#27169;&#22411;&#65292;&#20197;&#35299;&#20915;&#28145;&#24230;&#23398;&#20064;&#23450;&#20301;&#27169;&#22411;&#20013;&#25345;&#32493;&#23384;&#22312;&#30340;&#36890;&#29992;&#24615;&#32570;&#22833;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#23460;&#20869;&#23450;&#20301;&#22240;&#20854;&#22312;&#26234;&#33021;&#23478;&#23621;&#12289;&#24037;&#19994;&#33258;&#21160;&#21270;&#21644;&#21307;&#30103;&#20445;&#20581;&#31561;&#39046;&#22495;&#30340;&#24191;&#27867;&#24212;&#29992;&#32780;&#21463;&#21040;&#20102;&#37325;&#35270;&#65292;&#29305;&#21035;&#26159;&#38543;&#30528;&#36234;&#26469;&#36234;&#22810;&#30340;&#20154;&#20381;&#36182;&#20854;&#26080;&#32447;&#35774;&#22791;&#36827;&#34892;&#22522;&#20110;&#20301;&#32622;&#30340;&#26381;&#21153;&#12290;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#35299;&#20915;&#26041;&#26696;&#21033;&#29992;&#26080;&#32447;&#21442;&#25968;&#65288;&#22914;&#20449;&#36947;&#29366;&#24577;&#20449;&#24687;&#65288;CSI&#65289;&#21644;&#25509;&#25910;&#20449;&#21495;&#24378;&#24230;&#25351;&#31034;&#22120;&#65288;RSSI&#65289;&#65289;&#22312;&#23460;&#20869;&#29615;&#22659;&#20013;&#20934;&#30830;&#20272;&#35745;&#26080;&#32447;&#35774;&#22791;&#30340;&#20301;&#32622;&#24050;&#32463;&#21462;&#24471;&#20102;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#23454;&#29616;&#39640;&#20934;&#30830;&#24230;&#30340;&#23450;&#20301;&#26041;&#38754;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#36825;&#20123;&#27169;&#22411;&#32570;&#20047;&#36890;&#29992;&#24615;&#65292;&#22312;&#19981;&#37325;&#26032;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#26080;&#27861;&#36731;&#26494;&#37096;&#32626;&#21040;&#26032;&#29615;&#22659;&#25110;&#22312;&#21160;&#24577;&#29615;&#22659;&#20013;&#36816;&#34892;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#20803;&#23398;&#20064;&#30340;&#23450;&#20301;&#27169;&#22411;&#26469;&#35299;&#20915;&#20256;&#32479;&#28145;&#24230;&#23398;&#20064;&#23450;&#20301;&#27169;&#22411;&#20013;&#25345;&#32493;&#23384;&#22312;&#30340;&#36890;&#29992;&#24615;&#32570;&#22833;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#20803;&#23398;&#20064;&#31639;&#27861;&#38656;&#35201;&#22810;&#20803;&#21270;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Indoor localization has gained significant attention in recent years due to its various applications in smart homes, industrial automation, and healthcare, especially since more people rely on their wireless devices for location-based services. Deep learning-based solutions have shown promising results in accurately estimating the position of wireless devices in indoor environments using wireless parameters such as Channel State Information (CSI) and Received Signal Strength Indicator (RSSI). However, despite the success of deep learning-based approaches in achieving high localization accuracy, these models suffer from a lack of generalizability and can not be readily-deployed to new environments or operate in dynamic environments without retraining. In this paper, we propose meta-learning-based localization models to address the lack of generalizability that persists in conventionally trained DL-based localization models. Furthermore, since meta-learning algorithms require diverse dat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#24067;&#23616;&#36139;&#27665;&#31391;&#36947;&#36335;&#12290;&#36890;&#36807;&#25513;&#30721;&#31574;&#30053;&#20248;&#21270;&#65292;&#21487;&#20351;&#21487;&#36798;&#24615;&#25552;&#39640;14.3&#65285;&#65292;&#23545;&#29616;&#26377;&#22522;&#32447;&#26041;&#27861;&#20855;&#26377;&#26126;&#26174;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2305.13060</link><description>&lt;p&gt;
&#20511;&#21161;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#36139;&#27665;&#31391;&#36947;&#36335;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Road Planning for Slums via Deep Reinforcement Learning. (arXiv:2305.13060v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13060
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#24067;&#23616;&#36139;&#27665;&#31391;&#36947;&#36335;&#12290;&#36890;&#36807;&#25513;&#30721;&#31574;&#30053;&#20248;&#21270;&#65292;&#21487;&#20351;&#21487;&#36798;&#24615;&#25552;&#39640;14.3&#65285;&#65292;&#23545;&#29616;&#26377;&#22522;&#32447;&#26041;&#27861;&#20855;&#26377;&#26126;&#26174;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#30334;&#19975;&#36139;&#27665;&#31391;&#23621;&#27665;&#30001;&#20110;&#36139;&#27665;&#31391;&#20869;&#19981;&#36275;&#30340;&#36947;&#36335;&#22522;&#30784;&#35774;&#26045;&#32780;&#36973;&#21463;&#22478;&#24066;&#26381;&#21153;&#26080;&#27861;&#35775;&#38382;&#30340;&#22256;&#22659;&#65292;&#32780;&#36139;&#27665;&#31391;&#36947;&#36335;&#35268;&#21010;&#23545;&#22478;&#24066;&#30340;&#21487;&#25345;&#32493;&#21457;&#23637;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#26377;&#30340;&#37325;&#32452;&#25110;&#21551;&#21457;&#24335;&#26041;&#27861;&#35201;&#20040;&#32791;&#26102;&#65292;&#19981;&#33021;&#25512;&#24191;&#21040;&#19981;&#21516;&#30340;&#36139;&#27665;&#31391;&#65292;&#35201;&#20040;&#22312;&#21487;&#36798;&#24615;&#21644;&#24314;&#35774;&#25104;&#26412;&#26041;&#38754;&#20135;&#29983;&#27425;&#20248;&#30340;&#36947;&#36335;&#35268;&#21010;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#24067;&#23616;&#36139;&#27665;&#31391;&#36947;&#36335;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#22270;&#27169;&#22411;&#65292;&#29992;&#20110;&#25429;&#33719;&#36139;&#27665;&#31391;&#30340;&#25299;&#25169;&#32467;&#26500;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#36873;&#25321;&#35745;&#21010;&#36947;&#36335;&#30340;&#20301;&#32622;&#12290;&#36890;&#36807;&#25513;&#30721;&#31574;&#30053;&#20248;&#21270;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#29983;&#25104;&#36830;&#25509;&#36139;&#27665;&#31391;&#22320;&#28857;&#30340;&#36947;&#36335;&#35268;&#21010;&#65292;&#20197;&#26368;&#23567;&#30340;&#24314;&#35774;&#25104;&#26412;&#12290;&#23545;&#19981;&#21516;&#22269;&#23478;&#30340;&#30495;&#23454;&#36139;&#27665;&#31391;&#36827;&#34892;&#22823;&#37327;&#23454;&#39564;&#39564;&#35777;&#20102;&#25105;&#20204;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#65292;&#21487;&#20351;&#21487;&#36798;&#24615;&#25552;&#39640;14.3&#65285;&#65292;&#23545;&#29616;&#26377;&#22522;&#32447;&#26041;&#27861;&#20855;&#26377;&#26126;&#26174;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Millions of slum dwellers suffer from poor accessibility to urban services due to inadequate road infrastructure within slums, and road planning for slums is critical to the sustainable development of cities. Existing re-blocking or heuristic methods are either time-consuming which cannot generalize to different slums, or yield sub-optimal road plans in terms of accessibility and construction costs. In this paper, we present a deep reinforcement learning based approach to automatically layout roads for slums. We propose a generic graph model to capture the topological structure of a slum, and devise a novel graph neural network to select locations for the planned roads. Through masked policy optimization, our model can generate road plans that connect places in a slum at minimal construction costs. Extensive experiments on real-world slums in different countries verify the effectiveness of our model, which can significantly improve accessibility by 14.3% against existing baseline metho
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21464;&#25442;&#22120;&#26550;&#26500; GPTrans&#65292;&#20197;&#22270;&#20256;&#25773;&#27880;&#24847;&#21147;&#20026;&#22522;&#30784;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#23398;&#20064;&#22270;&#24418;&#27169;&#22411;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#38598;&#19978;&#36229;&#36807;&#20102;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#21464;&#25442;&#22120;&#30340;&#22270;&#24418;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.11424</link><description>&lt;p&gt;
&#22270;&#20256;&#25773;&#21464;&#25442;&#22120;&#29992;&#20110;&#22270;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Graph Propagation Transformer for Graph Representation Learning. (arXiv:2305.11424v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11424
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21464;&#25442;&#22120;&#26550;&#26500; GPTrans&#65292;&#20197;&#22270;&#20256;&#25773;&#27880;&#24847;&#21147;&#20026;&#22522;&#30784;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#23398;&#20064;&#22270;&#24418;&#27169;&#22411;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#38598;&#19978;&#36229;&#36807;&#20102;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#21464;&#25442;&#22120;&#30340;&#22270;&#24418;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22270;&#34920;&#31034;&#23398;&#20064;&#30340;&#26032;&#22411;&#21464;&#25442;&#22120;&#26550;&#26500;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26680;&#24515;&#35265;&#35299;&#26159;&#22312;&#26500;&#24314;&#21464;&#25442;&#22120;&#22359;&#20013;&#30340;&#27880;&#24847;&#21147;&#27169;&#22359;&#26102;&#65292;&#20805;&#20998;&#32771;&#34385;&#22270;&#20013;&#33410;&#28857;&#21644;&#36793;&#20043;&#38388;&#30340;&#20449;&#24687;&#20256;&#25773;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#31216;&#20026;&#22270;&#20256;&#25773;&#27880;&#24847;&#21147;&#65288;GPA&#65289;&#65292;&#23427;&#23558;&#20449;&#24687;&#22312;&#33410;&#28857;&#21644;&#36793;&#20043;&#38388;&#20197;&#19977;&#31181;&#26041;&#24335;&#26126;&#30830;&#20256;&#36882;&#65292;&#21363;&#20174;&#33410;&#28857;&#21040;&#33410;&#28857;&#65292;&#20174;&#33410;&#28857;&#21040;&#36793;&#21644;&#20174;&#36793;&#21040;&#33410;&#28857;&#65292;&#36825;&#23545;&#20110;&#23398;&#20064;&#22270;&#32467;&#26500;&#25968;&#25454;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#21517;&#20026;&#22270;&#20256;&#25773;&#21464;&#25442;&#22120;&#65288;GPTrans&#65289;&#30340;&#26377;&#25928;&#21464;&#25442;&#22120;&#26550;&#26500;&#65292;&#36827;&#19968;&#27493;&#24110;&#21161;&#23398;&#20064;&#22270;&#25968;&#25454;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#24191;&#27867;&#22270;&#23398;&#20064;&#23454;&#39564;&#20013;&#39564;&#35777;&#20102;GPTrans&#30340;&#24615;&#33021;&#12290;&#36825;&#20123;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20197;&#26356;&#22909;&#30340;&#24615;&#33021;&#36229;&#36807;&#20102;&#35768;&#22810;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#21464;&#25442;&#22120;&#30340;&#22270;&#24418;&#27169;&#22411;&#12290;&#20195;&#30721;&#23558;&#22312;https://github.com/czczup/GPTrans&#19978;&#21457;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a novel transformer architecture for graph representation learning. The core insight of our method is to fully consider the information propagation among nodes and edges in a graph when building the attention module in the transformer blocks. Specifically, we propose a new attention mechanism called Graph Propagation Attention (GPA). It explicitly passes the information among nodes and edges in three ways, i.e. node-to-node, node-to-edge, and edge-to-node, which is essential for learning graph-structured data. On this basis, we design an effective transformer architecture named Graph Propagation Transformer (GPTrans) to further help learn graph data. We verify the performance of GPTrans in a wide range of graph learning experiments on several benchmark datasets. These results show that our method outperforms many state-of-the-art transformer-based graph models with better performance. The code will be released at https://github.com/czczup/GPTrans.
&lt;/p&gt;</description></item><item><title>LoViT&#26159;&#19968;&#31181;&#29992;&#20110;&#25163;&#26415;&#38454;&#27573;&#35782;&#21035;&#30340;&#38271;&#35270;&#39057;Transformer&#65292;&#23427;&#36890;&#36807;&#32467;&#21512;&#26102;&#38388;&#20016;&#23500;&#30340;&#31354;&#38388;&#29305;&#24449;&#25552;&#21462;&#22120;&#21644;&#22810;&#23610;&#24230;&#26102;&#38388;&#32858;&#21512;&#22120;&#26469;&#23545;&#38271;&#35270;&#39057;&#36827;&#34892;&#20998;&#26512;&#65292;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.08989</link><description>&lt;p&gt;
LoViT: &#29992;&#20110;&#25163;&#26415;&#38454;&#27573;&#35782;&#21035;&#30340;&#38271;&#35270;&#39057;Transformer
&lt;/p&gt;
&lt;p&gt;
LoViT: Long Video Transformer for Surgical Phase Recognition. (arXiv:2305.08989v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08989
&lt;/p&gt;
&lt;p&gt;
LoViT&#26159;&#19968;&#31181;&#29992;&#20110;&#25163;&#26415;&#38454;&#27573;&#35782;&#21035;&#30340;&#38271;&#35270;&#39057;Transformer&#65292;&#23427;&#36890;&#36807;&#32467;&#21512;&#26102;&#38388;&#20016;&#23500;&#30340;&#31354;&#38388;&#29305;&#24449;&#25552;&#21462;&#22120;&#21644;&#22810;&#23610;&#24230;&#26102;&#38388;&#32858;&#21512;&#22120;&#26469;&#23545;&#38271;&#35270;&#39057;&#36827;&#34892;&#20998;&#26512;&#65292;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26500;&#24314;&#33021;&#22815;&#37327;&#21270;&#34920;&#29616;&#24182;&#30417;&#30563;&#25163;&#26415;&#27969;&#31243;&#25191;&#34892;&#30340;&#19978;&#19979;&#25991;&#24037;&#20855;&#26041;&#38754;&#65292;&#22312;&#32447;&#25163;&#26415;&#38454;&#27573;&#35782;&#21035;&#21457;&#25381;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#30446;&#21069;&#30340;&#26041;&#27861;&#26377;&#38480;&#65292;&#22240;&#20026;&#23427;&#20204;&#20351;&#29992;&#22522;&#20110;&#24103;&#32423;&#30417;&#30563;&#30340;&#31354;&#38388;&#29305;&#24449;&#25552;&#21462;&#22120;&#36827;&#34892;&#35757;&#32451;&#65292;&#36825;&#21487;&#33021;&#20250;&#23548;&#33268;&#30001;&#20110;&#30456;&#20284;&#24103;&#22312;&#19981;&#21516;&#38454;&#27573;&#20986;&#29616;&#32780;&#23548;&#33268;&#30340;&#38169;&#35823;&#39044;&#27979;&#65292;&#24182;&#19988;&#30001;&#20110;&#35745;&#31639;&#38480;&#21046;&#32780;&#26410;&#33021;&#24456;&#22909;&#22320;&#34701;&#21512;&#26412;&#22320;&#21644;&#20840;&#23616;&#29305;&#24449;&#65292;&#36825;&#21487;&#33021;&#24433;&#21709;&#25163;&#26415;&#24178;&#39044;&#20013;&#36890;&#24120;&#36935;&#21040;&#30340;&#38271;&#35270;&#39057;&#30340;&#20998;&#26512;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LoViT&#30340;&#20004;&#38454;&#27573;&#26041;&#27861;&#65292;&#29992;&#20110;&#34701;&#21512;&#30701;&#26399;&#21644;&#38271;&#26399;&#26102;&#38388;&#20449;&#24687;&#65292;&#23427;&#32467;&#21512;&#20102;&#19968;&#20010;&#26102;&#38388;&#20016;&#23500;&#30340;&#31354;&#38388;&#29305;&#24449;&#25552;&#21462;&#22120;&#21644;&#19968;&#20010;&#22810;&#23610;&#24230;&#26102;&#38388;&#32858;&#21512;&#22120;&#65292;&#21518;&#32773;&#30001;&#22522;&#20110;&#33258;&#27880;&#24847;&#21147;&#30340;&#20004;&#20010;&#32423;&#32852;L-Trans&#27169;&#22359;&#21644;&#19968;&#20010;&#22522;&#20110;ProbSparse&#33258;&#27880;&#24847;&#21147;&#30340;G-Informer&#27169;&#22359;&#32452;&#25104;&#65292;&#29992;&#20110;&#22788;&#29702;&#20840;&#23616;&#26102;&#38388;&#20449;&#24687;&#12290;&#28982;&#21518;&#65292;&#22810;&#23610;&#24230;&#26102;&#38388;&#22836;&#32467;&#21512;&#26412;&#22320;&#21644;&#20840;&#23616;&#29305;&#24449;&#26469;&#35782;&#21035;&#38271;&#35270;&#39057;&#20013;&#30340;&#25163;&#26415;&#38454;&#27573;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;LoViT&#22312;&#20004;&#20010;&#20844;&#20849;&#25163;&#26415;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Online surgical phase recognition plays a significant role towards building contextual tools that could quantify performance and oversee the execution of surgical workflows. Current approaches are limited since they train spatial feature extractors using frame-level supervision that could lead to incorrect predictions due to similar frames appearing at different phases, and poorly fuse local and global features due to computational constraints which can affect the analysis of long videos commonly encountered in surgical interventions. In this paper, we present a two-stage method, called Long Video Transformer (LoViT) for fusing short- and long-term temporal information that combines a temporally-rich spatial feature extractor and a multi-scale temporal aggregator consisting of two cascaded L-Trans modules based on self-attention, followed by a G-Informer module based on ProbSparse self-attention for processing global temporal information. The multi-scale temporal head then combines loc
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#35889;&#26102;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#24182;&#25581;&#31034;&#20102;&#20854;&#20855;&#26377;&#32447;&#24615;&#35889;&#26102;GNN&#26159;&#26222;&#36866;&#30340;&#12289;&#34920;&#29616;&#21147;&#21463;&#21040;&#31163;&#25955;&#26102;&#38388;&#21160;&#24577;&#22270;&#25193;&#23637;&#30340;&#31532;&#19968;&#38454;Weisfeiler-Leman&#31639;&#27861;&#30340;&#38480;&#21046;&#12290;&#21516;&#26102;&#65292;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#23454;&#20363;TGC&#65292;&#20854;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#38754;&#20855;&#26377;&#26174;&#33879;&#30340;&#24615;&#33021;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2305.06587</link><description>&lt;p&gt;
&#38754;&#21521;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#35889;&#26102;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
How Expressive are Spectral-Temporal Graph Neural Networks for Time Series Forecasting?. (arXiv:2305.06587v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06587
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#35889;&#26102;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#24182;&#25581;&#31034;&#20102;&#20854;&#20855;&#26377;&#32447;&#24615;&#35889;&#26102;GNN&#26159;&#26222;&#36866;&#30340;&#12289;&#34920;&#29616;&#21147;&#21463;&#21040;&#31163;&#25955;&#26102;&#38388;&#21160;&#24577;&#22270;&#25193;&#23637;&#30340;&#31532;&#19968;&#38454;Weisfeiler-Leman&#31639;&#27861;&#30340;&#38480;&#21046;&#12290;&#21516;&#26102;&#65292;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#23454;&#20363;TGC&#65292;&#20854;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#38754;&#20855;&#26377;&#26174;&#33879;&#30340;&#24615;&#33021;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35889;&#26102;&#22270;&#31070;&#32463;&#32593;&#32476;&#26159;&#22823;&#22810;&#25968;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;(GNN)&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#27169;&#22411;&#30340;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#25277;&#35937;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#38656;&#35201;&#26356;&#22810;&#20851;&#20110;&#36825;&#31181;&#26041;&#27861;&#30340;&#22522;&#30784;&#30693;&#35782;&#12290;&#26412;&#25991;&#24314;&#31435;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#65292;&#25581;&#31034;&#20102;&#35889;&#26102;GNN&#30340;&#34920;&#29616;&#21147;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#20855;&#26377;&#32447;&#24615;&#35889;&#26102;GNN&#26159;&#26222;&#36866;&#30340;&#65292;&#22312;&#28201;&#21644;&#30340;&#20551;&#35774;&#19979;&#65292;&#23427;&#20204;&#30340;&#34920;&#29616;&#21147;&#21463;&#21040;&#25105;&#20204;&#30340;&#31163;&#25955;&#26102;&#38388;&#21160;&#24577;&#22270;&#25193;&#23637;&#30340;&#31532;&#19968;&#38454;Weisfeiler-Leman&#31639;&#27861;&#30340;&#38480;&#21046;&#12290;&#20026;&#20102;&#20351;&#25105;&#20204;&#30340;&#21457;&#29616;&#22312;&#23454;&#36341;&#20013;&#26377;&#29992;&#65292;&#25105;&#20204;&#35814;&#32454;&#35752;&#35770;&#20102;&#30456;&#20851;&#38480;&#21046;&#65292;&#24182;&#27010;&#36848;&#20102;&#22312;&#35889;&#22495;&#20013;&#35774;&#35745;&#31354;&#38388;&#21644;&#26102;&#38388;&#27169;&#22359;&#30340;&#29702;&#35770;&#34013;&#22270;&#12290;&#22522;&#20110;&#36825;&#20123;&#35265;&#35299;&#65292;&#24182;&#20026;&#20102;&#23637;&#31034;&#22522;&#20110;&#25105;&#20204;&#30340;&#26694;&#26550;&#65292;&#35889;&#26102;GNN&#26377;&#22810;&#20040;&#24378;&#22823;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026; Temporal Graph GegenConv (TGC) &#30340;&#31616;&#21333;&#23454;&#20363;&#65292;&#26174;&#33879;&#20248;&#20110;&#22823;&#22810;&#25968;&#24050;&#26377;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spectral-temporal graph neural network is a promising abstraction underlying most time series forecasting models that are based on graph neural networks (GNNs). However, more is needed to know about the underpinnings of this branch of methods. In this paper, we establish a theoretical framework that unravels the expressive power of spectral-temporal GNNs. Our results show that linear spectral-temporal GNNs are universal under mild assumptions, and their expressive power is bounded by our extended first-order Weisfeiler-Leman algorithm on discrete-time dynamic graphs. To make our findings useful in practice on valid instantiations, we discuss related constraints in detail and outline a theoretical blueprint for designing spatial and temporal modules in spectral domains. Building on these insights and to demonstrate how powerful spectral-temporal GNNs are based on our framework, we propose a simple instantiation named Temporal Graph GegenConv (TGC), which significantly outperforms most e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22522;&#20110;&#22270;&#28789;&#27979;&#35797;&#65292;&#22238;&#36991;&#20102;&#26426;&#22120;&#26159;&#21542;&#26234;&#33021;&#30340;&#38382;&#39064;&#65292;&#25506;&#35752;&#22312;&#26085;&#24120;&#29983;&#27963;&#20013;&#22914;&#20309;&#30830;&#23450;&#19968;&#20010;&#20132;&#20114;&#23545;&#35937;&#26159;&#20154;&#36824;&#26159;&#26426;&#22120;&#30340;&#25361;&#25112;&#65292;&#24182;&#24605;&#32771;&#20102;&#20854;&#24212;&#29992;&#21644;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.04312</link><description>&lt;p&gt;
&#20154;&#36824;&#26159;&#26426;&#22120;&#65306;&#22522;&#20110;&#22270;&#28789;&#27979;&#35797;&#30340;&#26085;&#24120;&#21453;&#24605;
&lt;/p&gt;
&lt;p&gt;
Human or Machine: Reflections on Turing-Inspired Testing for the Everyday. (arXiv:2305.04312v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04312
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22522;&#20110;&#22270;&#28789;&#27979;&#35797;&#65292;&#22238;&#36991;&#20102;&#26426;&#22120;&#26159;&#21542;&#26234;&#33021;&#30340;&#38382;&#39064;&#65292;&#25506;&#35752;&#22312;&#26085;&#24120;&#29983;&#27963;&#20013;&#22914;&#20309;&#30830;&#23450;&#19968;&#20010;&#20132;&#20114;&#23545;&#35937;&#26159;&#20154;&#36824;&#26159;&#26426;&#22120;&#30340;&#25361;&#25112;&#65292;&#24182;&#24605;&#32771;&#20102;&#20854;&#24212;&#29992;&#21644;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20182;&#30340;&#24320;&#21019;&#24615;&#35770;&#25991;&#12298;&#35745;&#31639;&#26426;&#22120;&#26800;&#19982;&#26234;&#33021;&#12299;&#20013;&#65292;&#33406;&#20262;&#183;&#22270;&#28789;&#24341;&#20837;&#20102;&#8220;&#27169;&#20223;&#28216;&#25103;&#8221;&#65292;&#25506;&#35752;&#20102;&#26426;&#22120;&#26234;&#33021;&#30340;&#27010;&#24565;&#12290;&#22270;&#28789;&#27979;&#35797;&#33258;&#37027;&#26102;&#20197;&#26469;&#19968;&#30452;&#26159;&#24191;&#27867;&#35752;&#35770;&#12289;&#23436;&#21892;&#21644;&#25193;&#23637;&#30340;&#20027;&#39064;&#12290;&#26412;&#25991;&#22238;&#36991;&#20102;&#20851;&#20110;&#26576;&#20010;&#29305;&#23450;&#26426;&#22120;&#26159;&#21542;&#33021;&#34987;&#26631;&#35760;&#20026;&#26234;&#33021;&#25110;&#33021;&#21542;&#22312;&#32473;&#23450;&#29615;&#22659;&#20013;&#21305;&#37197;&#20154;&#31867;&#33021;&#21147;&#30340;&#38382;&#39064;&#12290;&#19982;&#27492;&#30456;&#21453;&#65292;&#20294;&#21463;&#22270;&#28789;&#21551;&#21457;&#65292;&#25105;&#20204;&#20851;&#27880;&#22312;&#26085;&#24120;&#29983;&#27963;&#20013;&#30830;&#23450;&#26159;&#21542;&#27491;&#22312;&#19982;&#19968;&#20010;&#20154;&#25110;&#19968;&#20010;&#26426;&#22120;&#36827;&#34892;&#20132;&#20114;&#36825;&#20010;&#30475;&#20284;&#31616;&#21333;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#23545;&#36825;&#20010;&#20154;&#36824;&#26159;&#26426;&#22120;&#38382;&#39064;&#21450;&#20854;&#21487;&#38752;&#31572;&#26696;&#30340;&#24212;&#29992;&#24863;&#21040;&#24863;&#20852;&#36259;&#65292;&#24182;&#24076;&#26395;&#21453;&#24605;&#20854;&#37325;&#35201;&#24615;&#12290;&#34429;&#28982;&#22270;&#28789;&#30340;&#21407;&#22987;&#27979;&#35797;&#34987;&#24191;&#27867;&#35748;&#20026;&#26159;&#19968;&#31181;&#24605;&#32500;&#23454;&#39564;&#65292;&#20294;&#26412;&#25991;&#35752;&#35770;&#30340;&#20154;&#36824;&#26159;&#26426;&#22120;&#38382;&#39064;&#20855;&#26377;&#26126;&#26174;&#30340;&#23454;&#38469;&#24847;&#20041;&#12290;&#34429;&#28982;&#20154;&#31867;&#26159;&#21542;&#33021;&#22815;&#21019;&#36896;&#20986;&#33021;&#22815;&#32988;&#20219;&#25152;&#26377;&#30340;&#20154;&#31867;&#24037;&#20316;&#30340;&#26426;&#22120;&#20063;&#26410;&#21487;&#30693;&#12290;
&lt;/p&gt;
&lt;p&gt;
In his seminal paper "Computing Machinery and Intelligence", Alan Turing introduced the "imitation game" as part of exploring the concept of machine intelligence. The Turing Test has since been the subject of much analysis, debate, refinement and extension. Here we sidestep the question of whether a particular machine can be labeled intelligent, or can be said to match human capabilities in a given context. Instead, but inspired by Turing, we draw attention to the seemingly simpler challenge of determining whether one is interacting with a human or with a machine, in the context of everyday life. We are interested in reflecting upon the importance of this Human-or-Machine question and the use one may make of a reliable answer thereto. Whereas Turing's original test is widely considered to be more of a thought experiment, the Human-or-Machine question as discussed here has obvious practical significance. And while the jury is still not in regarding the possibility of machines that can m
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#19981;&#21516;&#35821;&#35328;&#25552;&#31034;&#19979;&#30340;&#22870;&#21169;&#26102;&#38388;&#20559;&#22909;&#65292;&#24182;&#21457;&#29616;GPT&#22312;&#20855;&#26377;&#36739;&#24369;&#26410;&#26469;&#26102;&#24577;&#30340;&#35821;&#35328;&#19979;&#34920;&#29616;&#20986;&#26356;&#22823;&#30340;&#32784;&#24515;&#65292;&#36825;&#19982;&#20351;&#29992;&#35813;&#35821;&#35328;&#30340;&#20154;&#31867;&#30340;&#20559;&#22909;&#30456;&#20284;&#12290;</title><link>http://arxiv.org/abs/2305.02531</link><description>&lt;p&gt;
&#35821;&#35328;&#12289;&#26102;&#38388;&#20559;&#22909;&#21644;&#28040;&#36153;&#34892;&#20026;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35777;&#25454;
&lt;/p&gt;
&lt;p&gt;
Language, Time Preferences, and Consumer Behavior: Evidence from Large Language Models. (arXiv:2305.02531v1 [econ.GN])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02531
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#19981;&#21516;&#35821;&#35328;&#25552;&#31034;&#19979;&#30340;&#22870;&#21169;&#26102;&#38388;&#20559;&#22909;&#65292;&#24182;&#21457;&#29616;GPT&#22312;&#20855;&#26377;&#36739;&#24369;&#26410;&#26469;&#26102;&#24577;&#30340;&#35821;&#35328;&#19979;&#34920;&#29616;&#20986;&#26356;&#22823;&#30340;&#32784;&#24515;&#65292;&#36825;&#19982;&#20351;&#29992;&#35813;&#35821;&#35328;&#30340;&#20154;&#31867;&#30340;&#20559;&#22909;&#30456;&#20284;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#23545;&#25105;&#20204;&#23545;&#26102;&#38388;&#21644;&#22870;&#21169;&#30340;&#24863;&#30693;&#26377;&#24456;&#22823;&#30340;&#24433;&#21709;&#12290;&#36825;&#24341;&#21457;&#20102;&#19968;&#20010;&#38382;&#39064;&#65292;&#21363;&#24403;&#20197;&#19981;&#21516;&#30340;&#35821;&#35328;&#35810;&#38382;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26102;&#65292;&#23427;&#20204;&#26159;&#21542;&#26174;&#31034;&#20986;&#19981;&#21516;&#30340;&#22870;&#21169;&#26102;&#38388;&#20559;&#22909;&#65292;&#24182;&#19988;&#23427;&#20204;&#30340;&#36873;&#25321;&#26159;&#21542;&#31867;&#20284;&#20110;&#20154;&#31867;&#30340;&#36873;&#25321;&#12290;&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;GPT-3.5&#65288;&#20197;&#19979;&#31616;&#31216;GPT&#65289;&#22312;&#22810;&#31181;&#35821;&#35328;&#25552;&#31034;&#19979;&#30340;&#21709;&#24212;&#65292;&#25506;&#32034;&#20102;&#36739;&#23567;&#12289;&#36739;&#26089;&#30340;&#22870;&#21169;&#21644;&#36739;&#22823;&#12289;&#36739;&#26202;&#30340;&#22870;&#21169;&#20043;&#38388;&#30340;&#20559;&#22909;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#24403;&#20197;&#35821;&#20041;&#21547;&#20041;&#36739;&#24369;&#30340;&#26410;&#26469;&#26102;&#24577;&#21442;&#32771;&#65288;FTR&#65289;&#65292;&#22914;&#24503;&#35821;&#21644;&#27721;&#35821;&#65292;&#20026;&#25552;&#31034;&#35821;&#26102;&#65292;GPT&#34920;&#29616;&#20986;&#26356;&#22823;&#30340;&#32784;&#24515;&#65292;&#30456;&#27604;&#33521;&#35821;&#21644;&#27861;&#35821;&#31561;&#20855;&#26377;&#24378;&#22823;FTR&#30340;&#35821;&#35328;&#12290;&#36825;&#20123;&#21457;&#29616;&#19982;&#29616;&#26377;&#25991;&#29486;&#19968;&#33268;&#65292;&#24182;&#34920;&#26126;&#20102;GPT&#30340;&#36873;&#25321;&#19982;&#36825;&#20123;&#35821;&#35328;&#30340;&#20351;&#29992;&#32773;&#30340;&#20559;&#22909;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;&#28982;&#32780;&#65292;&#36827;&#19968;&#27493;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#36739;&#26089;&#25110;&#36739;&#26202;&#22870;&#21169;&#30340;&#20559;&#22909;&#24182;&#27809;&#26377;&#38543;&#30528;&#22870;&#21169;&#24046;&#24322;&#31995;&#32479;&#22320;&#25913;&#21464;&#65292;&#36825;&#34920;&#26126;&#20102;&#19968;&#31181;&#35789;&#20856;&#24207;&#20248;&#20808;&#30340;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language has a strong influence on our perceptions of time and rewards. This raises the question of whether large language models, when asked in different languages, show different preferences for rewards over time and if their choices are similar to those of humans. In this study, we analyze the responses of GPT-3.5 (hereafter referred to as GPT) to prompts in multiple languages, exploring preferences between smaller, sooner rewards and larger, later rewards. Our results show that GPT displays greater patience when prompted in languages with weak future tense references (FTR), such as German and Mandarin, compared to languages with strong FTR, like English and French. These findings are consistent with existing literature and suggest a correlation between GPT's choices and the preferences of speakers of these languages. However, further analysis reveals that the preference for earlier or later rewards does not systematically change with reward gaps, indicating a lexicographic preferen
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;GPTutor&#30340;ChatGPT&#21160;&#21147;&#32534;&#31243;&#24037;&#20855;&#65292;&#23427;&#26159;&#19968;&#20010;&#20351;&#29992;ChatGPT API&#30340;Visual Studio Code&#25193;&#23637;&#65292;&#36890;&#36807;&#35774;&#35745;&#25552;&#31034;&#35789;&#65292;&#21487;&#20197;&#23545;&#25152;&#36873;&#20195;&#30721;&#36827;&#34892;&#31934;&#31616;&#12289;&#20934;&#30830;&#30340;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2305.01863</link><description>&lt;p&gt;
GPTutor: &#19968;&#31181;&#30001;ChatGPT&#39537;&#21160;&#30340;&#32534;&#31243;&#24037;&#20855;&#65292;&#29992;&#20110;&#31243;&#24207;&#20195;&#30721;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
GPTutor: a ChatGPT-powered programming tool for code explanation. (arXiv:2305.01863v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01863
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;GPTutor&#30340;ChatGPT&#21160;&#21147;&#32534;&#31243;&#24037;&#20855;&#65292;&#23427;&#26159;&#19968;&#20010;&#20351;&#29992;ChatGPT API&#30340;Visual Studio Code&#25193;&#23637;&#65292;&#36890;&#36807;&#35774;&#35745;&#25552;&#31034;&#35789;&#65292;&#21487;&#20197;&#23545;&#25152;&#36873;&#20195;&#30721;&#36827;&#34892;&#31934;&#31616;&#12289;&#20934;&#30830;&#30340;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#26032;&#30340;&#32534;&#31243;&#25216;&#33021;&#38656;&#35201;&#20010;&#24615;&#21270;&#25351;&#23548;&#12290;&#38543;&#30528;ChatGPT API&#31561;&#39640;&#32423;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#27169;&#22411;&#30340;&#20986;&#29616;&#65292;&#29616;&#22312;&#26377;&#21487;&#33021;&#21019;&#24314;&#19968;&#20010;&#26041;&#20415;&#30340;&#12289;&#20010;&#24615;&#21270;&#30340;AI&#32534;&#31243;&#25945;&#32946;&#36741;&#23548;&#31995;&#32479;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;GPTutor&#30340;ChatGPT&#21160;&#21147;&#32534;&#31243;&#24037;&#20855;&#65292;&#23427;&#26159;&#19968;&#20010;&#20351;&#29992;ChatGPT API&#30340;Visual Studio Code&#25193;&#23637;&#65292;&#29992;&#20110;&#25552;&#20379;&#32534;&#31243;&#20195;&#30721;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning new programming skills requires tailored guidance. With the emergence of advanced Natural Language Generation models like the ChatGPT API, there is now a possibility of creating a convenient and personalized tutoring system with AI for computer science education. This paper presents GPTutor, a ChatGPT-powered programming tool, which is a Visual Studio Code extension using the ChatGPT API to provide programming code explanations. By integrating Visual Studio Code API, GPTutor can comprehensively analyze the provided code by referencing the relevant source codes. As a result, GPTutor can use designed prompts to explain the selected code with a pop-up message. GPTutor is now published at the Visual Studio Code Extension Marketplace, and its source code is openly accessible on GitHub. Preliminary evaluation indicates that GPTutor delivers the most concise and accurate explanations compared to vanilla ChatGPT and GitHub Copilot. Moreover, the feedback from students and teachers ind
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#36890;&#36807;&#24179;&#34913;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#30340;&#27169;&#24335;&#26469;&#22686;&#21152;&#27169;&#22411;&#36755;&#20986;&#22810;&#26679;&#24615;&#30340;&#22810;&#26679;&#24615;&#26435;&#37325;&#35757;&#32451;&#26041;&#26696;&#65292;&#20197;&#26356;&#22909;&#22320;&#36866;&#24212;&#38656;&#35201;&#22810;&#26679;&#21270;&#36755;&#20986;&#30340;&#21019;&#24847;&#24212;&#29992;&#65292;&#24182;&#22312;&#21463;&#25511;&#29615;&#22659;&#20013;&#36827;&#34892;&#30340;&#21021;&#27493;&#23454;&#39564;&#23637;&#31034;&#20102;&#20854;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2304.11961</link><description>&lt;p&gt;
&#36890;&#36807;&#22810;&#26679;&#24615;&#26435;&#37325;&#23454;&#29616;&#29983;&#25104;&#27169;&#22411;&#30340;&#27169;&#24335;&#24179;&#34913;
&lt;/p&gt;
&lt;p&gt;
Towards Mode Balancing of Generative Models via Diversity Weights. (arXiv:2304.11961v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11961
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#36890;&#36807;&#24179;&#34913;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#30340;&#27169;&#24335;&#26469;&#22686;&#21152;&#27169;&#22411;&#36755;&#20986;&#22810;&#26679;&#24615;&#30340;&#22810;&#26679;&#24615;&#26435;&#37325;&#35757;&#32451;&#26041;&#26696;&#65292;&#20197;&#26356;&#22909;&#22320;&#36866;&#24212;&#38656;&#35201;&#22810;&#26679;&#21270;&#36755;&#20986;&#30340;&#21019;&#24847;&#24212;&#29992;&#65292;&#24182;&#22312;&#21463;&#25511;&#29615;&#22659;&#20013;&#36827;&#34892;&#30340;&#21021;&#27493;&#23454;&#39564;&#23637;&#31034;&#20102;&#20854;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#25968;&#25454;&#39537;&#21160;&#30340;&#22270;&#20687;&#27169;&#22411;&#34987;&#24191;&#27867;&#29992;&#20110;&#25903;&#25345;&#21019;&#24847;&#21644;&#33402;&#26415;&#20316;&#21697;&#12290;&#22312;&#24403;&#21069;&#20027;&#23548;&#30340;&#20998;&#24067;&#25311;&#21512;&#33539;&#24335;&#19979;&#65292;&#25968;&#25454;&#38598;&#34987;&#35270;&#20026;&#35201;&#23613;&#21487;&#33021;&#25509;&#36817;&#30340;&#30495;&#23454;&#20540;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#21019;&#24847;&#24212;&#29992;&#38656;&#35201;&#22810;&#26679;&#21270;&#30340;&#36755;&#20986;&#65292;&#21019;&#20316;&#32773;&#32463;&#24120;&#21162;&#21147;&#20174;&#32473;&#23450;&#30340;&#25968;&#25454;&#20998;&#24067;&#20013;&#31215;&#26497;&#20998;&#31163;&#20986;&#26469;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#20174;&#32431;&#27169;&#24335;&#35206;&#30422;&#36716;&#21521;&#27169;&#24335;&#24179;&#34913;&#30340;&#24314;&#27169;&#30446;&#26631;&#35843;&#25972;&#26159;&#24517;&#35201;&#30340;&#65292;&#20197;&#36866;&#24212;&#26356;&#39640;&#30340;&#36755;&#20986;&#22810;&#26679;&#24615;&#30446;&#26631;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22810;&#26679;&#24615;&#26435;&#37325;&#65292;&#36825;&#26159;&#19968;&#31181;&#36890;&#36807;&#24179;&#34913;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#30340;&#27169;&#24335;&#26469;&#22686;&#21152;&#27169;&#22411;&#36755;&#20986;&#22810;&#26679;&#24615;&#30340;&#35757;&#32451;&#26041;&#26696;&#12290;&#22312;&#21463;&#25511;&#29615;&#22659;&#20013;&#36827;&#34892;&#30340;&#21021;&#27493;&#23454;&#39564;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#25105;&#20204;&#26041;&#27861;&#19982;&#22810;&#26679;&#24615;&#12289;&#20844;&#24179;&#21644;&#21253;&#23481;&#22312;&#29983;&#25104;&#24335;&#26426;&#22120;&#23398;&#20064;&#20197;&#21450;&#35745;&#31639;&#26426;&#21019;&#24847;&#20013;&#30340;&#32852;&#31995;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#23454;&#29616;&#21487;&#20197;&#22312;https://github.com/&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large data-driven image models are extensively used to support creative and artistic work. Under the currently predominant distribution-fitting paradigm, a dataset is treated as ground truth to be approximated as closely as possible. Yet, many creative applications demand a diverse range of output, and creators often strive to actively diverge from a given data distribution. We argue that an adjustment of modelling objectives, from pure mode coverage towards mode balancing, is necessary to accommodate the goal of higher output diversity. We present diversity weights, a training scheme that increases a model's output diversity by balancing the modes in the training dataset. First experiments in a controlled setting demonstrate the potential of our method. We discuss connections of our approach to diversity, equity, and inclusion in generative machine learning more generally, and computational creativity specifically. An implementation of our algorithm is available at https://github.com/
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#24037;&#20855;&#23398;&#20064;&#32467;&#21512;&#20102;&#19987;&#29992;&#24037;&#20855;&#21644;&#22522;&#30784;&#27169;&#22411;&#30340;&#20248;&#21183;&#65292;&#23454;&#29616;&#20102;&#38382;&#39064;&#35299;&#20915;&#30340;&#22686;&#24378;&#31934;&#24230;&#12289;&#25928;&#29575;&#21644;&#33258;&#21160;&#21270;&#12290;&#26412;&#25991;&#23545;&#24037;&#20855;&#23398;&#20064;&#36827;&#34892;&#20102;&#31995;&#32479;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#28085;&#30422;&#20004;&#31181;&#31867;&#22411;&#23398;&#20064;&#30340;&#36890;&#29992;&#24037;&#20855;&#23398;&#20064;&#26694;&#26550;&#65292;&#24182;&#20998;&#26512;&#20102;&#23427;&#20204;&#30340;&#29420;&#29305;&#25361;&#25112;&#12289;&#26426;&#20250;&#21644;&#26410;&#26469;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2304.08354</link><description>&lt;p&gt;
&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#24037;&#20855;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Tool Learning with Foundation Models. (arXiv:2304.08354v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08354
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#24037;&#20855;&#23398;&#20064;&#32467;&#21512;&#20102;&#19987;&#29992;&#24037;&#20855;&#21644;&#22522;&#30784;&#27169;&#22411;&#30340;&#20248;&#21183;&#65292;&#23454;&#29616;&#20102;&#38382;&#39064;&#35299;&#20915;&#30340;&#22686;&#24378;&#31934;&#24230;&#12289;&#25928;&#29575;&#21644;&#33258;&#21160;&#21270;&#12290;&#26412;&#25991;&#23545;&#24037;&#20855;&#23398;&#20064;&#36827;&#34892;&#20102;&#31995;&#32479;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#28085;&#30422;&#20004;&#31181;&#31867;&#22411;&#23398;&#20064;&#30340;&#36890;&#29992;&#24037;&#20855;&#23398;&#20064;&#26694;&#26550;&#65292;&#24182;&#20998;&#26512;&#20102;&#23427;&#20204;&#30340;&#29420;&#29305;&#25361;&#25112;&#12289;&#26426;&#20250;&#21644;&#26410;&#26469;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#25317;&#26377;&#38750;&#20961;&#30340;&#21019;&#36896;&#21644;&#21033;&#29992;&#24037;&#20855;&#30340;&#33021;&#21147;&#65292;&#20351;&#24471;&#20182;&#20204;&#33021;&#22815;&#20811;&#26381;&#29289;&#29702;&#38480;&#21046;&#24182;&#25506;&#32034;&#26032;&#30340;&#39046;&#22495;&#12290;&#38543;&#30528;&#22522;&#30784;&#27169;&#22411;&#30340;&#20986;&#29616;&#65292;AI&#31995;&#32479;&#26377;&#26395;&#20687;&#20154;&#31867;&#19968;&#26679;&#29087;&#32451;&#22320;&#20351;&#29992;&#24037;&#20855;&#12290;&#36825;&#31181;&#33539;&#24335;&#21363;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#24037;&#20855;&#23398;&#20064;&#65292;&#32467;&#21512;&#20102;&#19987;&#29992;&#24037;&#20855;&#21644;&#22522;&#30784;&#27169;&#22411;&#30340;&#20248;&#21183;&#65292;&#23454;&#29616;&#20102;&#38382;&#39064;&#35299;&#20915;&#30340;&#22686;&#24378;&#31934;&#24230;&#12289;&#25928;&#29575;&#21644;&#33258;&#21160;&#21270;&#12290;&#23613;&#31649;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#65292;&#20294;&#35813;&#39046;&#22495;&#20173;&#32570;&#20047;&#23545;&#20851;&#38190;&#25361;&#25112;&#12289;&#26426;&#20250;&#21644;&#26410;&#26469;&#21457;&#23637;&#30340;&#20840;&#38754;&#29702;&#35299;&#12290;&#38024;&#23545;&#36825;&#19968;&#38382;&#39064;&#65292;&#26412;&#25991;&#23545;&#24037;&#20855;&#23398;&#20064;&#36827;&#34892;&#20102;&#31995;&#32479;&#30740;&#31350;&#12290;&#39318;&#20808;&#20171;&#32461;&#20102;&#24037;&#20855;&#23398;&#20064;&#30340;&#32972;&#26223;&#65292;&#21253;&#25324;&#20854;&#35748;&#30693;&#36215;&#28304;&#12289;&#22522;&#30784;&#27169;&#22411;&#30340;&#33539;&#24335;&#36716;&#25442;&#21644;&#24037;&#20855;&#21644;&#27169;&#22411;&#30340;&#20114;&#34917;&#20316;&#29992;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#22238;&#39038;&#20102;&#29616;&#26377;&#30340;&#24037;&#20855;&#23398;&#20064;&#30740;&#31350;&#65292;&#21253;&#25324;&#22522;&#20110;&#24037;&#20855;&#21644;&#38754;&#21521;&#24037;&#20855;&#30340;&#23398;&#20064;&#12290;&#25105;&#20204;&#21046;&#23450;&#20102;&#19968;&#20010;&#28085;&#30422;&#20004;&#31181;&#31867;&#22411;&#23398;&#20064;&#30340;&#36890;&#29992;&#24037;&#20855;&#23398;&#20064;&#26694;&#26550;&#65292;&#24182;&#20998;&#26512;&#20102;&#23427;&#20204;&#30340;&#29420;&#29305;&#25361;&#25112;&#12289;&#26426;&#20250;&#21644;&#26410;&#26469;&#26041;&#21521;&#12290;&#25105;&#20204;&#39044;&#35745;&#36825;&#31181;&#31995;&#32479;&#30340;&#25506;&#32034;&#23558;&#20026;&#26410;&#26469;&#24320;&#21457;&#20855;&#26377;&#22797;&#26434;&#24037;&#20855;&#23398;&#20064;&#33021;&#21147;&#30340;AI&#31995;&#32479;&#25552;&#20379;&#19968;&#20010;&#36339;&#26495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Humans possess an extraordinary ability to create and utilize tools, allowing them to overcome physical limitations and explore new frontiers. With the advent of foundation models, AI systems have the potential to be equally adept in tool use as humans. This paradigm, i.e., tool learning with foundation models, combines the strengths of specialized tools and foundation models to achieve enhanced accuracy, efficiency, and automation in problem-solving. Despite its immense potential, there is still a lack of a comprehensive understanding of key challenges, opportunities, and future endeavors in this field. To this end, we present a systematic investigation of tool learning in this paper. We first introduce the background of tool learning, including its cognitive origins, the paradigm shift of foundation models, and the complementary roles of tools and models. Then we recapitulate existing tool learning research into tool-augmented and tool-oriented learning. We formulate a general tool l
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;OpenAGI&#24179;&#21488;&#36890;&#36807;&#25972;&#21512;&#39046;&#22495;&#19987;&#23478;&#27169;&#22411;&#21644;&#33258;&#28982;&#35821;&#35328;&#38382;&#31572;&#24418;&#24335;&#65292;&#23454;&#29616;&#22797;&#26434;&#20219;&#21153;&#35299;&#20915;&#12290;</title><link>http://arxiv.org/abs/2304.04370</link><description>&lt;p&gt;
OpenAGI&#65306;&#24403;LLM&#36935;&#21040;&#39046;&#22495;&#19987;&#23478;
&lt;/p&gt;
&lt;p&gt;
OpenAGI: When LLM Meets Domain Experts. (arXiv:2304.04370v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04370
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;OpenAGI&#24179;&#21488;&#36890;&#36807;&#25972;&#21512;&#39046;&#22495;&#19987;&#23478;&#27169;&#22411;&#21644;&#33258;&#28982;&#35821;&#35328;&#38382;&#31572;&#24418;&#24335;&#65292;&#23454;&#29616;&#22797;&#26434;&#20219;&#21153;&#35299;&#20915;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#20855;&#26377;&#23558;&#22522;&#26412;&#25216;&#33021;&#32452;&#21512;&#25104;&#22797;&#26434;&#25216;&#33021;&#20197;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#30340;&#26174;&#33879;&#33021;&#21147;&#12290;&#36825;&#31181;&#33021;&#21147;&#23545;&#20110;&#20154;&#24037;&#26234;&#33021;&#21516;&#26679;&#37325;&#35201;&#65292;&#22240;&#27492;&#65292;&#25105;&#20204;&#26029;&#35328;&#65292;&#38500;&#20102;&#24320;&#21457;&#22823;&#22411;&#32508;&#21512;&#26234;&#33021;&#27169;&#22411;&#22806;&#65292;&#23558;&#19981;&#21516;&#39046;&#22495;&#19987;&#23478;&#27169;&#22411;&#24212;&#29992;&#20110;&#22797;&#26434;&#20219;&#21153;&#35299;&#20915;&#33021;&#21147;&#21516;&#26679;&#20851;&#38190;&#65292;&#20197;&#22312;&#20154;&#24037;&#26234;&#33021;&#36890;&#29992;&#26234;&#33021;&#30340;&#36861;&#27714;&#20013;&#20351;&#20854;&#20855;&#22791;&#36825;&#31181;&#33021;&#21147;&#12290;&#26368;&#36817;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#21457;&#23637;&#35777;&#26126;&#20854;&#20855;&#26377;&#20986;&#33394;&#30340;&#23398;&#20064;&#21644;&#25512;&#29702;&#33021;&#21147;&#65292;&#20351;&#23427;&#20204;&#25104;&#20026;&#36873;&#25321;&#12289;&#32508;&#21512;&#21644;&#25191;&#34892;&#22806;&#37096;&#27169;&#22411;&#20197;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#30340;&#25511;&#21046;&#22120;&#30340;&#26377;&#21069;&#36884;&#30340;&#36873;&#25321;&#12290;&#22312;&#36825;&#20010;&#39033;&#30446;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#21517;&#20026;OpenAGI&#30340;&#24320;&#28304;AGI&#30740;&#31350;&#24179;&#21488;&#65292;&#19987;&#38376;&#35774;&#35745;&#20026;&#25552;&#20379;&#22797;&#26434;&#30340;&#22810;&#27493;&#39588;&#20219;&#21153;&#65292;&#24182;&#37197;&#26377;&#20219;&#21153;&#29305;&#23450;&#30340;&#25968;&#25454;&#38598;&#12289;&#35780;&#20272;&#25351;&#26631;&#21644;&#21508;&#31181;&#21487;&#25193;&#23637;&#27169;&#22411;&#12290;OpenAGI&#23558;&#22797;&#26434;&#20219;&#21153;&#38416;&#37322;&#20026;&#33258;&#28982;&#35821;&#35328;&#38382;&#31572;&#65292;&#26088;&#22312;&#20419;&#36827;&#39046;&#22495;&#19987;&#23478;&#21644;&#35821;&#35328;&#27169;&#22411;&#20043;&#38388;&#30340;&#21327;&#21516;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human intelligence has the remarkable ability to assemble basic skills into complex ones so as to solve complex tasks. This ability is equally important for Artificial Intelligence (AI), and thus, we assert that in addition to the development of large, comprehensive intelligent models, it is equally crucial to equip such models with the capability to harness various domain-specific expert models for complex task-solving in the pursuit of Artificial General Intelligence (AGI). Recent developments in Large Language Models (LLMs) have demonstrated remarkable learning and reasoning abilities, making them promising as a controller to select, synthesize, and execute external models to solve complex tasks. In this project, we develop OpenAGI, an open-source AGI research platform, specifically designed to offer complex, multi-step tasks and accompanied by task-specific datasets, evaluation metrics, and a diverse range of extensible models. OpenAGI formulates complex tasks as natural language q
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26680;&#20984;&#21253;&#26426;&#30340;&#26041;&#27861;&#26469;&#30830;&#20445;&#25968;&#25454;&#38544;&#31169;&#20445;&#25252;&#65292;&#21516;&#26102;&#20445;&#30041;&#25968;&#25454;&#32467;&#26500;&#65292;&#29992;&#20110;&#25968;&#25454;&#34920;&#31034;&#23398;&#20064;&#30340;&#20998;&#31867;&#24212;&#29992;&#20013;&#12290;&#20026;&#20102;&#30830;&#20445;&#38544;&#31169;&#20445;&#25252;&#23398;&#20064;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29983;&#25104;&#34394;&#20551;&#25968;&#25454;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.01300</link><description>&lt;p&gt;
&#22522;&#20110;&#26680;&#20984;&#21253;&#26426;&#30340;&#24046;&#20998;&#38544;&#31169;&#23398;&#20064;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Kernel Affine Hull Machines for Differentially Private Learning. (arXiv:2304.01300v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01300
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26680;&#20984;&#21253;&#26426;&#30340;&#26041;&#27861;&#26469;&#30830;&#20445;&#25968;&#25454;&#38544;&#31169;&#20445;&#25252;&#65292;&#21516;&#26102;&#20445;&#30041;&#25968;&#25454;&#32467;&#26500;&#65292;&#29992;&#20110;&#25968;&#25454;&#34920;&#31034;&#23398;&#20064;&#30340;&#20998;&#31867;&#24212;&#29992;&#20013;&#12290;&#20026;&#20102;&#30830;&#20445;&#38544;&#31169;&#20445;&#25252;&#23398;&#20064;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29983;&#25104;&#34394;&#20551;&#25968;&#25454;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#36890;&#36807;&#23398;&#20064;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20013;&#30340;&#28857;&#30340;&#20984;&#21253;&#26469;&#34920;&#31034;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#23558;&#25968;&#25454;&#31354;&#38388;&#21010;&#20998;&#20026;&#20960;&#20309;&#20307;&#65292;&#20174;&#32780;&#38544;&#34255;&#26377;&#20851;&#21333;&#20010;&#25968;&#25454;&#28857;&#30340;&#38544;&#31169;&#20449;&#24687;&#65292;&#21516;&#26102;&#20445;&#30041;&#21407;&#22987;&#23398;&#20064;&#38382;&#39064;&#30340;&#32467;&#26500;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#26680;&#20984;&#21253;&#26426;&#65288;KAHM&#65289;&#65292;&#23427;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#35745;&#31639;&#20174;&#32467;&#26524;&#26377;&#30028;&#20960;&#20309;&#20307;&#20013;&#30340;&#36317;&#31163;&#24230;&#37327;&#12290;KAHM&#26159;&#24191;&#27867;&#21644;&#28145;&#20837;&#30340;&#33258;&#32534;&#30721;&#22120;&#30340;&#20851;&#38190;&#26500;&#24314;&#22359;&#65292;&#23427;&#20204;&#20351;&#25968;&#25454;&#34920;&#31034;&#23398;&#20064;&#29992;&#20110;&#20998;&#31867;&#24212;&#29992;&#12290;&#20026;&#20102;&#30830;&#20445;&#38544;&#31169;&#20445;&#25252;&#23398;&#20064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29983;&#25104;&#34394;&#20551;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#28041;&#21450;&#23558;&#24046;&#20998;&#38544;&#31169;&#25968;&#25454;&#26679;&#26412;&#36890;&#36807;&#36716;&#25442;&#36807;&#31243;&#36827;&#34892;&#24179;&#28369;&#22788;&#29702;&#12290;&#29983;&#25104;&#30340;&#34394;&#20551;&#25968;&#25454;&#19981;&#20165;&#20445;&#35777;&#24046;&#20998;&#38544;&#31169;&#65292;&#32780;&#19988;&#30830;&#20445;KAHM&#24314;&#27169;&#35823;&#24046;&#19981;&#22823;&#20110;&#21407;&#22987;&#25968;&#25454;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper explores the use of affine hulls of points as a means of representing data via learning in Reproducing Kernel Hilbert Spaces (RKHS), with the goal of partitioning the data space into geometric bodies that conceal privacy-sensitive information about individual data points, while preserving the structure of the original learning problem. To this end, we introduce the Kernel Affine Hull Machine (KAHM), which provides an effective way of computing a distance measure from the resulting bounded geometric body. KAHM is a critical building block in wide and deep autoencoders, which enable data representation learning for classification applications. To ensure privacy-preserving learning, we propose a novel method for generating fabricated data, which involves smoothing differentially private data samples through a transformation process. The resulting fabricated data guarantees not only differential privacy but also ensures that the KAHM modeling error is not larger than that of the
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#24037;&#20855;&#65292;&#21487;&#20026; Lean &#35777;&#26126;&#21161;&#25163;&#24314;&#35758;&#19982;&#29992;&#25143;&#27491;&#22312;&#35777;&#26126;&#30340;&#23450;&#29702;&#30456;&#20851;&#30340;&#21069;&#25552;&#26465;&#20214;&#12290;</title><link>http://arxiv.org/abs/2304.00994</link><description>&lt;p&gt;
Lean &#30340;&#26426;&#22120;&#23398;&#20064;&#21069;&#25552;&#36873;&#25321;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
Machine-Learned Premise Selection for Lean. (arXiv:2304.00994v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00994
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#24037;&#20855;&#65292;&#21487;&#20026; Lean &#35777;&#26126;&#21161;&#25163;&#24314;&#35758;&#19982;&#29992;&#25143;&#27491;&#22312;&#35777;&#26126;&#30340;&#23450;&#29702;&#30456;&#20851;&#30340;&#21069;&#25552;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#24037;&#20855;&#65292;&#21487;&#20026; Lean &#35777;&#26126;&#21161;&#25163;&#24314;&#35758;&#19982;&#29992;&#25143;&#27491;&#22312;&#35777;&#26126;&#30340;&#23450;&#29702;&#30456;&#20851;&#30340;&#21069;&#25552;&#26465;&#20214;&#12290;&#35813;&#24037;&#20855;&#30340;&#35774;&#35745;&#21407;&#21017;&#20026;&#65306;&#65288;1&#65289;&#19982;&#35777;&#26126;&#21161;&#25163;&#32039;&#23494;&#38598;&#25104;&#65292;&#65288;2&#65289;&#26131;&#20110;&#20351;&#29992;&#21644;&#23433;&#35013;&#65292;&#65288;3&#65289;&#37319;&#29992;&#36731;&#37327;&#32423;&#19988;&#24555;&#36895;&#30340;&#26041;&#27861;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22312;&#32447;&#23398;&#20064;&#30340;&#33258;&#23450;&#20041;&#38543;&#26426;&#26862;&#26519;&#27169;&#22411;&#65292;&#30452;&#25509;&#22312; Lean &#20013;&#23454;&#29616;&#65292;&#36825;&#24471;&#30410;&#20110; Lean 4 &#20016;&#23500;&#32780;&#39640;&#25928;&#30340;&#20803;&#32534;&#31243;&#21151;&#33021;&#12290;&#38543;&#26426;&#26862;&#26519;&#30340;&#35757;&#32451;&#25968;&#25454;&#26469;&#33258;&#20110; mathlib -- Lean &#30340;&#25968;&#23398;&#24211;&#12290;&#25105;&#20204;&#23581;&#35797;&#20102;&#21508;&#31181;&#36873;&#39033;&#26469;&#20135;&#29983;&#35757;&#32451;&#29305;&#24449;&#21644;&#26631;&#31614;&#12290;&#32463;&#36807;&#35757;&#32451;&#30340;&#27169;&#22411;&#30340;&#24314;&#35758;&#21487;&#20197;&#36890;&#36807;&#8220;suggest_premises&#31574;&#30053;&#8221;&#20256;&#36798;&#32473;&#29992;&#25143;&#65292;&#22312;&#20132;&#20114;&#24335;&#26500;&#24314;&#35777;&#26126;&#36807;&#31243;&#20013;&#21487;&#20197;&#22312;&#32534;&#36753;&#22120;&#20013;&#35843;&#29992;&#35813;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a machine-learning-based tool for the Lean proof assistant that suggests relevant premises for theorems being proved by a user. The design principles for the tool are (1) tight integration with the proof assistant, (2) ease of use and installation, (3) a lightweight and fast approach. For this purpose, we designed a custom version of the random forest model, trained in an online fashion. It is implemented directly in Lean, which was possible thanks to the rich and efficient metaprogramming features of Lean 4. The random forest is trained on data extracted from mathlib -- Lean's mathematics library. We experiment with various options for producing training features and labels. The advice from a trained model is accessible to the user via the suggest_premises tactic which can be called in an editor while constructing a proof interactively.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#24102;&#26377;&#27010;&#29575;&#35302;&#21457;&#33218;&#30340;&#24773;&#22659;&#32452;&#21512;&#36172;&#21338;&#26426;&#65292;&#22312;&#19981;&#21516;&#26465;&#20214;&#19979;&#35774;&#35745;&#20102;C$^2$-UCB-T&#31639;&#27861;&#21644;VAC$^2$-UCB&#31639;&#27861;&#65292;&#24182;&#20998;&#21035;&#23548;&#20986;&#20102;&#23545;&#24212;&#30340;&#36951;&#25022;&#20540;&#19978;&#38480;&#65292;&#20026;&#30456;&#20851;&#24212;&#29992;&#25552;&#20379;&#20102;&#29702;&#35770;&#25903;&#25345;&#12290;</title><link>http://arxiv.org/abs/2303.17110</link><description>&lt;p&gt;
&#24102;&#26377;&#27010;&#29575;&#35302;&#21457;&#33218;&#30340;&#24773;&#22659;&#32452;&#21512;&#36172;&#21338;&#26426;
&lt;/p&gt;
&lt;p&gt;
Contextual Combinatorial Bandits with Probabilistically Triggered Arms. (arXiv:2303.17110v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17110
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24102;&#26377;&#27010;&#29575;&#35302;&#21457;&#33218;&#30340;&#24773;&#22659;&#32452;&#21512;&#36172;&#21338;&#26426;&#65292;&#22312;&#19981;&#21516;&#26465;&#20214;&#19979;&#35774;&#35745;&#20102;C$^2$-UCB-T&#31639;&#27861;&#21644;VAC$^2$-UCB&#31639;&#27861;&#65292;&#24182;&#20998;&#21035;&#23548;&#20986;&#20102;&#23545;&#24212;&#30340;&#36951;&#25022;&#20540;&#19978;&#38480;&#65292;&#20026;&#30456;&#20851;&#24212;&#29992;&#25552;&#20379;&#20102;&#29702;&#35770;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#25429;&#25417;&#24191;&#27867;&#24212;&#29992;&#33539;&#22260;&#30340;&#19968;&#31995;&#21015;&#24179;&#28369;&#26465;&#20214;&#19979;&#30340;&#24102;&#26377;&#27010;&#29575;&#35302;&#21457;&#33218;&#30340;&#24773;&#22659;&#32452;&#21512;&#36172;&#21338;&#26426;(C$^2$MAB-T)&#65292;&#20363;&#22914;&#24773;&#22659;&#32423;&#32852;&#36172;&#21338;&#26426;&#21644;&#24773;&#22659;&#26368;&#22823;&#21270;&#36172;&#21338;&#26426;&#12290;&#22312;&#27169;&#25311;&#35302;&#21457;&#27010;&#29575;(TPM)&#30340;&#26465;&#20214;&#19979;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;C$^2$-UCB-T&#31639;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#26512;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#19968;&#20010;$\tilde{O}(d\sqrt{KT})$&#30340;&#36951;&#25022;&#20540;&#19978;&#38480;&#65292;&#28040;&#38500;&#20102;&#19968;&#20010;&#21487;&#33021;&#25351;&#25968;&#32423;&#22686;&#38271;&#30340;&#22240;&#23376;$O(1/p_{\min})$&#65292;&#20854;&#20013;$d$&#26159;&#24773;&#22659;&#30340;&#32500;&#25968;&#65292;$p_{\min}$&#26159;&#33021;&#34987;&#35302;&#21457;&#30340;&#20219;&#20309;&#33218;&#30340;&#26368;&#23567;&#27491;&#27010;&#29575;&#65292;&#25209;&#22823;&#23567;$K$&#26159;&#27599;&#36718;&#33021;&#34987;&#35302;&#21457;&#30340;&#33218;&#30340;&#26368;&#22823;&#25968;&#37327;&#12290;&#22312;&#26041;&#24046;&#35843;&#21046;(VM)&#25110;&#35302;&#21457;&#27010;&#29575;&#21644;&#26041;&#24046;&#35843;&#21046;(TPVM)&#26465;&#20214;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#24046;&#33258;&#36866;&#24212;&#31639;&#27861;VAC$^2$-UCB&#65292;&#24182;&#23548;&#20986;&#20102;&#19968;&#20010;$\tilde{O}(d\sqrt{T})$&#30340;&#36951;&#25022;&#20540;&#19978;&#38480;&#65292;&#35813;&#19978;&#38480;&#19982;&#25209;&#22823;&#23567;$K$&#26080;&#20851;&#12290;&#20316;&#20026;&#19968;&#20010;&#26377;&#20215;&#20540;&#30340;&#21103;&#20135;&#21697;&#65292;&#25105;&#20204;&#21457;&#29616;&#25105;&#20204;&#30340;&#19968;&#20010;...
&lt;/p&gt;
&lt;p&gt;
We study contextual combinatorial bandits with probabilistically triggered arms (C$^2$MAB-T) under a variety of smoothness conditions that capture a wide range of applications, such as contextual cascading bandits and contextual influence maximization bandits. Under the triggering probability modulated (TPM) condition, we devise the C$^2$-UCB-T algorithm and propose a novel analysis that achieves an $\tilde{O}(d\sqrt{KT})$ regret bound, removing a potentially exponentially large factor $O(1/p_{\min})$, where $d$ is the dimension of contexts, $p_{\min}$ is the minimum positive probability that any arm can be triggered, and batch-size $K$ is the maximum number of arms that can be triggered per round. Under the variance modulated (VM) or triggering probability and variance modulated (TPVM) conditions, we propose a new variance-adaptive algorithm VAC$^2$-UCB and derive a regret bound $\tilde{O}(d\sqrt{T})$, which is independent of the batch-size $K$. As a valuable by-product, we find our a
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36136;&#37327;&#22810;&#26679;&#24615;&#31639;&#27861;&#21644; Transformer &#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20004;&#31181;&#26426;&#21046;&#20197;&#23454;&#29616;&#36136;&#37327;&#19968;&#33268;&#30340;&#29983;&#25104;&#34892;&#20026;&#26465;&#20214;&#19979;&#30340;&#36712;&#36857;&#12290;</title><link>http://arxiv.org/abs/2303.16207</link><description>&lt;p&gt;
&#36136;&#37327;&#22810;&#26679;&#24615;&#21464;&#24418;&#22120;&#65306;&#22522;&#20110;&#20915;&#31574;Transformer&#29983;&#25104;&#34892;&#20026;&#26465;&#20214;&#19979;&#30340;&#36712;&#36857;
&lt;/p&gt;
&lt;p&gt;
The Quality-Diversity Transformer: Generating Behavior-Conditioned Trajectories with Decision Transformers. (arXiv:2303.16207v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16207
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36136;&#37327;&#22810;&#26679;&#24615;&#31639;&#27861;&#21644; Transformer &#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20004;&#31181;&#26426;&#21046;&#20197;&#23454;&#29616;&#36136;&#37327;&#19968;&#33268;&#30340;&#29983;&#25104;&#34892;&#20026;&#26465;&#20214;&#19979;&#30340;&#36712;&#36857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31070;&#32463;&#36827;&#21270;&#35745;&#31639;&#30340;&#32972;&#26223;&#19979;&#65292;&#36136;&#37327;&#22810;&#26679;&#24615;&#31639;&#27861;&#36890;&#36807;&#20381;&#36182;&#34892;&#20026;&#31354;&#38388;&#30340;&#23450;&#20041;&#26469;&#29983;&#25104;&#21508;&#31181;&#19981;&#21516;&#21644;&#39640;&#25928;&#30340;&#31574;&#30053;&#38598;&#21512;&#65292;&#21462;&#24471;&#20102;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;&#28982;&#32780;&#65292;&#22312;&#19981;&#30830;&#23450;&#30340;&#29615;&#22659;&#20013;&#65292;&#20250;&#26377;&#20004;&#20010;&#38382;&#39064;&#20986;&#29616;&#12290;&#31532;&#19968;&#65292;&#31574;&#30053;&#21487;&#33021;&#32570;&#20047;&#40065;&#26834;&#24615;&#21644;&#21487;&#37325;&#22797;&#24615;&#65292;&#21363;&#22312;&#30053;&#24494;&#19981;&#21516;&#30340;&#24773;&#20917;&#19979;&#65292;&#22810;&#20010; episodes &#24448;&#24448;&#20250;&#23548;&#33268;&#38750;&#24120;&#19981;&#21516;&#30340;&#34892;&#20026;&#32467;&#26524;&#12290;&#31532;&#20108;&#65292;&#30001;&#20110;&#31574;&#30053;&#38598;&#30340;&#31163;&#25955;&#24615;&#65292;&#35299;&#20915;&#26041;&#26696;&#30340;&#21464;&#21270;&#26159;&#19981;&#36830;&#32493;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#23454;&#29616;&#22522;&#20110;&#34892;&#20026;&#26465;&#20214;&#19979;&#30340;&#36712;&#36857;&#29983;&#25104;&#65292;&#20854;&#22522;&#20110;&#20004;&#20010;&#26426;&#21046;&#65306;&#39318;&#20808;&#26159; MAP-Elites Low-Spread (ME-LS)&#65292;&#23427;&#38480;&#21046;&#20102;&#36873;&#25321;&#37027;&#20123;&#22312;&#34892;&#20026;&#31354;&#38388;&#19978;&#26368;&#19968;&#33268;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#20854;&#27425;&#26159;&#36136;&#37327;&#22810;&#26679;&#24615;&#21464;&#24418;&#22120; (QDT)&#65292;&#23427;&#26159;&#22522;&#20110; Transformer &#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the context of neuroevolution, Quality-Diversity algorithms have proven effective in generating repertoires of diverse and efficient policies by relying on the definition of a behavior space. A natural goal induced by the creation of such a repertoire is trying to achieve behaviors on demand, which can be done by running the corresponding policy from the repertoire. However, in uncertain environments, two problems arise. First, policies can lack robustness and repeatability, meaning that multiple episodes under slightly different conditions often result in very different behaviors. Second, due to the discrete nature of the repertoire, solutions vary discontinuously. Here we present a new approach to achieve behavior-conditioned trajectory generation based on two mechanisms: First, MAP-Elites Low-Spread (ME-LS), which constrains the selection of solutions to those that are the most consistent in the behavior space. Second, the Quality-Diversity Transformer (QDT), a Transformer-based 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36866;&#24212;&#25552;&#31034;&#21644;&#38646;&#21021;&#22987;&#21270;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#36731;&#37327;&#32423;&#35821;&#35328;&#27169;&#22411;&#35843;&#25972;&#26041;&#27861;&#65292;&#21487;&#39640;&#25928;&#24494;&#35843;LLaMA&#20026;&#25351;&#20196;&#36319;&#38543;&#27169;&#22411;&#65292;&#20855;&#26377;&#27604;Alpaca&#26356;&#30701;&#30340;&#24494;&#35843;&#26102;&#38388;&#24182;&#20855;&#26377;&#36817;&#20284;&#30340;&#21709;&#24212;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2303.16199</link><description>&lt;p&gt;
LLaMA-Adapter: &#38646;&#21021;&#22987;&#21270;&#27880;&#24847;&#21147;&#19979;&#30340;&#35821;&#35328;&#27169;&#22411;&#31934;&#32454;&#35843;&#25972;&#30340;&#39640;&#25928;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention. (arXiv:2303.16199v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16199
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36866;&#24212;&#25552;&#31034;&#21644;&#38646;&#21021;&#22987;&#21270;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#36731;&#37327;&#32423;&#35821;&#35328;&#27169;&#22411;&#35843;&#25972;&#26041;&#27861;&#65292;&#21487;&#39640;&#25928;&#24494;&#35843;LLaMA&#20026;&#25351;&#20196;&#36319;&#38543;&#27169;&#22411;&#65292;&#20855;&#26377;&#27604;Alpaca&#26356;&#30701;&#30340;&#24494;&#35843;&#26102;&#38388;&#24182;&#20855;&#26377;&#36817;&#20284;&#30340;&#21709;&#24212;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;LLaMA-Adapter&#36825;&#19968;&#36731;&#37327;&#32423;&#36866;&#24212;&#26041;&#27861;&#65292;&#29992;&#20110;&#23558;LLaMA&#39640;&#25928;&#22320;&#24494;&#35843;&#20026;&#19968;&#20010;&#25351;&#20196;&#36319;&#38543;&#27169;&#22411;&#12290;&#21033;&#29992;52K&#20010;&#33258;&#25105;&#25351;&#23548;&#31034;&#33539;&#65292;LLaMA-Adapter&#20165;&#22312;&#20923;&#32467;&#30340;LLaMA 7B&#27169;&#22411;&#19978;&#24341;&#20837;&#20102;1.2M&#20010;&#21487;&#23398;&#20064;&#21442;&#25968;&#65292;&#24182;&#19988;&#22312;8&#20010;A100 GPU&#19978;&#20165;&#32791;&#26102;&#19981;&#21040;&#19968;&#20010;&#23567;&#26102;&#36827;&#34892;&#24494;&#35843;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#37319;&#29992;&#19968;&#32452;&#21487;&#23398;&#20064;&#30340;&#36866;&#24212;&#25552;&#31034;&#65292;&#24182;&#22312;&#36739;&#39640;&#30340;&#21464;&#21387;&#22120;&#23618;&#20013;&#23558;&#23427;&#20204;&#39044;&#32622;&#20110;&#36755;&#20837;&#25991;&#26412;&#20196;&#29260;&#20043;&#21069;&#12290;&#28982;&#21518;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#38646;&#21021;&#22987;&#21270;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#38646;&#38376;&#25511;&#26426;&#21046;&#65292;&#35813;&#26426;&#21046;&#21487;&#20197;&#33258;&#36866;&#24212;&#22320;&#23558;&#26032;&#30340;&#25351;&#20196;&#25552;&#31034;&#27880;&#20837;LLaMA&#65292;&#24182;&#26377;&#25928;&#22320;&#20445;&#30041;&#20102;&#20854;&#39044;&#20808;&#35757;&#32451;&#30340;&#30693;&#35782;&#12290;&#36890;&#36807;&#39640;&#25928;&#35757;&#32451;&#65292;LLaMA-Adapter&#33021;&#22815;&#20135;&#29983;&#39640;&#36136;&#37327;&#30340;&#21709;&#24212;&#65292;&#19982;&#23436;&#20840;&#24494;&#35843;&#30340;7B&#21442;&#25968;&#30340;Alpaca&#30456;&#20284;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36824;&#21487;&#20197;&#31616;&#21333;&#22320;&#25193;&#23637;&#21040;&#22810;&#27169;&#24577;&#36755;&#20837;&#65292;&#20363;&#22914;&#22270;&#20687;&#65292;&#29992;&#20110;&#22270;&#20687;&#30456;&#20851;&#30340;LLaMA&#65292;&#22312;ScienceQA&#19978;&#23454;&#29616;&#20102;&#26356;&#24378;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;https://github.com/ZrrSkywalker/LLaMA-Adapt&#21457;&#24067;&#20102;&#25105;&#20204;&#30340;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present LLaMA-Adapter, a lightweight adaption method to efficiently fine-tune LLaMA into an instruction-following model. Using 52K self-instruct demonstrations, LLaMA-Adapter only introduces 1.2M learnable parameters upon the frozen LLaMA 7B model, and costs less than one hour for fine-tuning on 8 A100 GPUs. Specifically, we adopt a set of learnable adaption prompts, and prepend them to the input text tokens at higher transformer layers. Then, a zero-init attention mechanism with zero gating is proposed, which adaptively injects the new instructional cues into LLaMA, while effectively preserves its pre-trained knowledge. With efficient training, LLaMA-Adapter generates high-quality responses, comparable to Alpaca with fully fine-tuned 7B parameters. Furthermore, our approach can be simply extended to multi-modal input, e.g., images, for image-conditioned LLaMA, which achieves superior reasoning capacity on ScienceQA. We release our code at https://github.com/ZrrSkywalker/LLaMA-Adapt
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23581;&#35797;&#22312;&#36827;&#21270;&#35745;&#31639;&#19982;&#24378;&#21270;&#23398;&#20064;&#20013;&#30456;&#32467;&#21512;&#35299;&#20915;&#26426;&#22120;&#20154;&#25511;&#21046;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#25913;&#36827;ME&#31639;&#27861;&#25552;&#39640;&#20102;&#25928;&#29575;&#21644;&#22810;&#26679;&#24615;&#65292;&#20294;&#20063;&#20986;&#29616;&#20102;&#19968;&#20123;&#24120;&#35265;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2303.12803</link><description>&lt;p&gt;
&#22522;&#20110;MAP-Elites&#30340;&#22810;&#26679;&#21270;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#32676;&#20307;&#36827;&#21270;
&lt;/p&gt;
&lt;p&gt;
Evolving Populations of Diverse RL Agents with MAP-Elites. (arXiv:2303.12803v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12803
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23581;&#35797;&#22312;&#36827;&#21270;&#35745;&#31639;&#19982;&#24378;&#21270;&#23398;&#20064;&#20013;&#30456;&#32467;&#21512;&#35299;&#20915;&#26426;&#22120;&#20154;&#25511;&#21046;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#25913;&#36827;ME&#31639;&#27861;&#25552;&#39640;&#20102;&#25928;&#29575;&#21644;&#22810;&#26679;&#24615;&#65292;&#20294;&#20063;&#20986;&#29616;&#20102;&#19968;&#20123;&#24120;&#35265;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21697;&#36136;&#22810;&#26679;&#24615;(QD)&#24050;&#25104;&#20026;&#19968;&#31181;&#24378;&#22823;&#30340;&#26367;&#20195;&#20248;&#21270;&#27169;&#24335;&#65292;&#26088;&#22312;&#29983;&#25104;&#22823;&#37327;&#21644;&#22810;&#26679;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20854;&#20195;&#34920;&#31639;&#27861;MAP-Elites(ME)&#36890;&#36807;&#21464;&#24322;&#21644;&#20132;&#21449;&#36827;&#21270;&#35299;&#20915;&#26041;&#26696;&#12290;&#34429;&#28982;ME&#23545;&#20110;&#26576;&#20123;&#38750;&#32467;&#26500;&#21270;&#38382;&#39064;&#38750;&#24120;&#26377;&#25928;&#65292;&#20294;&#26089;&#26399;&#30340;ME&#23454;&#29616;&#20165;&#20381;&#36182;&#20110;&#38543;&#26426;&#25628;&#32034;&#26469;&#36827;&#21270;&#35299;&#20915;&#26041;&#26696;&#30340;&#31181;&#32676;&#65292;&#22240;&#27492;&#22312;&#39640;&#32500;&#38382;&#39064;&#20013;&#22914;&#36827;&#21270;&#31070;&#32463;&#32593;&#32476;&#26102;&#24120;&#24120;&#26080;&#27861;&#26377;&#25928;&#22320;&#36827;&#34892;&#65292;&#25928;&#29575;&#26497;&#20302;&#12290;&#21518;&#32493;&#30340;&#30740;&#31350;&#32771;&#34385;&#21033;&#29992;&#26799;&#24230;&#20449;&#24687;&#36890;&#36807;&#40657;&#30418;&#20248;&#21270;&#65288;BBO&#65289;&#25110;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20013;&#30340;&#25216;&#26415;&#26469;&#24341;&#23548;&#25628;&#32034;&#65292;&#20197;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#23558;RL&#25216;&#24039;&#19982;ME&#32467;&#21512;&#22312;&#26426;&#22120;&#20154;&#25511;&#21046;&#38382;&#39064;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#20294;&#26159;&#20063;&#22312;ME&#21464;&#20307;&#20013;&#24341;&#20837;&#20102;&#36825;&#20123;&#31639;&#27861;&#30340;&#20849;&#21516;&#38480;&#21046;&#65292;&#20363;&#22914;&#23545;&#25506;&#32034;&#38656;&#27714;&#30340;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quality Diversity (QD) has emerged as a powerful alternative optimization paradigm that aims at generating large and diverse collections of solutions, notably with its flagship algorithm MAP-ELITES (ME) which evolves solutions through mutations and crossovers. While very effective for some unstructured problems, early ME implementations relied exclusively on random search to evolve the population of solutions, rendering them notoriously sample-inefficient for high-dimensional problems, such as when evolving neural networks. Follow-up works considered exploiting gradient information to guide the search in order to address these shortcomings through techniques borrowed from either Black-Box Optimization (BBO) or Reinforcement Learning (RL). While mixing RL techniques with ME unlocked state-of-the-art performance for robotics control problems that require a good amount of exploration, it also plagued these ME variants with limitations common among RL algorithms that ME was free of, such a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#20855;&#26377;&#21035;&#21517;&#35266;&#27979;&#30340;&#28508;&#22312;&#22270;&#19978;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#26368;&#22823;&#21270;&#25506;&#32034;&#25928;&#29575;&#30340;&#25919;&#31574;&#31639;&#27861; eFeX&#65292;&#30456;&#27604;&#20110;&#38543;&#26426;&#31574;&#30053;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#26356;&#24555;&#22320;&#24674;&#22797;&#21508;&#31181;&#25299;&#25169;&#32467;&#26500;&#19979;&#30340;&#22270;&#34920;&#12290;</title><link>http://arxiv.org/abs/2303.07397</link><description>&lt;p&gt;
&#20855;&#26377;&#21035;&#21517;&#35266;&#27979;&#30340;&#28508;&#22312;&#22270;&#30340;&#24555;&#36895;&#25506;&#32034;&#19982;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Fast exploration and learning of latent graphs with aliased observations. (arXiv:2303.07397v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07397
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#20855;&#26377;&#21035;&#21517;&#35266;&#27979;&#30340;&#28508;&#22312;&#22270;&#19978;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#26368;&#22823;&#21270;&#25506;&#32034;&#25928;&#29575;&#30340;&#25919;&#31574;&#31639;&#27861; eFeX&#65292;&#30456;&#27604;&#20110;&#38543;&#26426;&#31574;&#30053;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#26356;&#24555;&#22320;&#24674;&#22797;&#21508;&#31181;&#25299;&#25169;&#32467;&#26500;&#19979;&#30340;&#22270;&#34920;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32771;&#34385;&#36825;&#31181;&#22330;&#26223;&#65306;&#19968;&#20010;&#26234;&#33021;&#20307;&#36890;&#36807;&#25191;&#34892;&#25805;&#20316;&#20174;&#19968;&#20010;&#33410;&#28857;&#21040;&#21478;&#19968;&#20010;&#33410;&#28857;&#26469;&#23548;&#33322;&#28508;&#22312;&#22270;&#12290;&#25152;&#36873;&#25805;&#20316;&#30830;&#23450;&#20102;&#19979;&#19968;&#20010;&#35775;&#38382;&#33410;&#28857;&#19978;&#30340;&#27010;&#29575;&#20998;&#24067;&#12290;&#22312;&#27599;&#20010;&#33410;&#28857;&#22788;&#65292;&#26234;&#33021;&#20307;&#25910;&#21040;&#19968;&#20010;&#35266;&#27979;&#65292;&#20294;&#35813;&#35266;&#27979;&#19981;&#26159;&#21807;&#19968;&#30340;&#65292;&#22240;&#27492;&#23427;&#19981;&#33021;&#21807;&#19968;&#22320;&#26631;&#35782;&#33410;&#28857;&#65292;&#36825;&#20351;&#24471;&#38382;&#39064;&#21035;&#21517;&#21270;&#12290;&#26412;&#25991;&#26088;&#22312;&#25552;&#20379;&#19968;&#20010;&#25919;&#31574;&#65292;&#35813;&#25919;&#31574;&#32422;&#31561;&#20110;&#26368;&#22823;&#21270;&#25506;&#32034;&#25928;&#29575;&#65288;&#21363;&#22312;&#32473;&#23450;&#30340;&#25506;&#32034;&#39044;&#31639;&#19979;&#22914;&#20309;&#24674;&#22797;&#22270;&#34920;&#65289;&#12290;&#22312;&#38750;&#21035;&#21517;&#21270;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#30456;&#23545;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#24378;&#21270;&#23398;&#20064;&#22522;&#32447;&#30340;&#25913;&#36827;&#24615;&#33021;&#12290;&#23545;&#20110;&#21035;&#21517;&#21270;&#30340;&#24773;&#20917;&#65292;&#25105;&#20204;&#19981;&#30693;&#36947;&#36866;&#29992;&#30340;&#22522;&#32447;&#65292;&#32780;&#26159;&#23637;&#31034;&#20102;&#22312;&#21508;&#31181;&#25299;&#25169;&#32467;&#26500;&#19979;&#30456;&#23545;&#20110;&#38543;&#26426;&#31574;&#30053;&#26356;&#24555;&#30340;&#24674;&#22797;&#36895;&#24230;&#65292;&#24182;&#19988;&#23545;&#20110;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25299;&#25169;&#32467;&#26500;&#65292;&#24674;&#22797;&#36895;&#24230;&#27604;&#38543;&#26426;&#31574;&#30053;&#24555;&#25351;&#25968;&#20493;&#12290;&#25105;&#20204;&#23558;&#35813;&#31639;&#27861;&#31216;&#20026; eFeX&#65288;&#26469;&#33258;&#20110; efficient exploration &#30340;&#32553;&#20889;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Consider this scenario: an agent navigates a latent graph by performing actions that take it from one node to another. The chosen action determines the probability distribution over the next visited node. At each node, the agent receives an observation, but this observation is not unique, so it does not identify the node, making the problem aliased. The purpose of this work is to provide a policy that approximately maximizes exploration efficiency (i.e., how well the graph is recovered for a given exploration budget). In the unaliased case, we show improved performance w.r.t. state-of-the-art reinforcement learning baselines. For the aliased case we are not aware of suitable baselines and instead show faster recovery w.r.t. a random policy for a wide variety of topologies, and exponentially faster recovery than a random policy for challenging topologies. We dub the algorithm eFeX (from eFficient eXploration).
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#26500;&#24314;&#20102;&#20004;&#20010;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#26469;&#39564;&#35777;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#25991;&#26412;-&#35270;&#35273;&#23545;&#35805;&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;&#65292;&#24182;&#24320;&#21457;&#29305;&#23450;&#35268;&#21017;&#30340;&#30417;&#30563;&#20449;&#21495;&#26469;&#35753;&#31995;&#32479;&#23637;&#31034;&#21487;&#36861;&#28335;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.05983</link><description>&lt;p&gt;
&#22522;&#20110;&#20154;&#31867;&#25351;&#20196;&#25298;&#32477;&#22270;&#20687;&#20877;&#21019;&#20316;&#30340;&#25991;&#26412;-&#35270;&#35273;&#23545;&#35805;&#21487;&#36861;&#28335;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Accountable Textual-Visual Chat Learns to Reject Human Instructions in Image Re-creation. (arXiv:2303.05983v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.05983
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#26500;&#24314;&#20102;&#20004;&#20010;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#26469;&#39564;&#35777;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#25991;&#26412;-&#35270;&#35273;&#23545;&#35805;&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;&#65292;&#24182;&#24320;&#21457;&#29305;&#23450;&#35268;&#21017;&#30340;&#30417;&#30563;&#20449;&#21495;&#26469;&#35753;&#31995;&#32479;&#23637;&#31034;&#21487;&#36861;&#28335;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#21644;GPT-4&#30340;&#25104;&#21151;&#24341;&#36215;&#20102;&#23545;&#22810;&#27169;&#24577;&#23545;&#35805;&#31995;&#32479;&#30340;&#24191;&#27867;&#20851;&#27880;&#65292;&#20294;&#32570;&#20047;&#21487;&#20197;&#39564;&#35777;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#25991;&#26412;-&#35270;&#35273;&#23545;&#35805;&#20219;&#21153;&#20013;&#22810;&#27169;&#24577;&#29983;&#25104;&#33021;&#21147;&#30340;&#25968;&#25454;&#38598;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#26500;&#24314;&#20102;&#20004;&#20010;&#26032;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#65306;&#21512;&#25104;CLEVR-ATVC&#25968;&#25454;&#38598;&#65288;620K&#65289;&#21644;&#25163;&#21160;&#32472;&#21046;&#30340;Fruit-ATVC&#25968;&#25454;&#38598;&#65288;50K&#65289;&#65292;&#22343;&#20855;&#26377;&#22522;&#20110;&#35270;&#35273;&#21644;&#25991;&#26412;&#30340;&#36755;&#20837;&#21644;&#36755;&#20986;&#12290;&#20026;&#20102;&#35753;&#22810;&#27169;&#24577;&#31995;&#32479;&#33021;&#22815;&#25298;&#32477;&#20154;&#31867;&#35831;&#27714;&#65288;&#21363;&#23637;&#31034;&#21487;&#36861;&#28335;&#24615;&#65289;&#65292;&#25105;&#20204;&#22312;&#25968;&#25454;&#38598;&#20013;&#24320;&#21457;&#21644;&#24182;&#20837;&#20102;&#29305;&#23450;&#35268;&#21017;&#20316;&#20026;&#30417;&#30563;&#20449;&#21495;&#12290;&#36825;&#20801;&#35768;&#35757;&#32451;&#21518;&#30340;VLM&#22312;&#35270;&#35273;&#21644;&#25991;&#26412;&#25512;&#29702;&#21518;&#25552;&#20379;yes&#25110;no&#30340;&#31572;&#26696;&#65292;&#24182;&#38468;&#24102;&#35828;&#26126;&#35821;&#35328;&#20026;&#20160;&#20040;&#26080;&#27861;&#25191;&#34892;&#20154;&#31867;&#25351;&#20196;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#35757;&#32451;&#31243;&#24207;&#26469;&#35757;&#32451;&#22270;&#20687;&#33258;&#32534;&#30721;&#22120;&#21644;&#33258;&#22238;&#24402;&#31070;&#32463;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent success of ChatGPT and GPT-4 has drawn widespread attention to multimodal dialogue systems. However, the academia community lacks a dataset that can validate the multimodal generation capabilities of Visual Language Models (VLMs) in textual-visual chat tasks. In this paper, we construct two new multimodal datasets: the synthetic CLEVR-ATVC dataset (620K) and the manually pictured Fruit-ATVC dataset (50K), both featuring visual and text-based inputs and outputs. Additionally, to enable the multimodal system to reject human requests (i.e., demonstrate accountability), as in language-based ChatGPT conversations, we develop and incorporate specific rules into the datasets as supervisory signals. This allows the trained VLM to provide a yes or no answer after visual and textual reasoning, accompanied by a language explanation as to why the human instruction cannot be excuted. In our method, we propose a two-state training procedure to train the image auto-encoder and auto-regress
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#30740;&#31350;&#20102;&#20154;&#31867;&#34892;&#20026;&#21463;&#32422;&#26463;&#35268;&#33539;&#32422;&#26463;&#30340;&#24433;&#21709;&#65292;&#21516;&#26102;&#38024;&#23545;&#20110;&#26500;&#24314;&#36981;&#23432;&#36825;&#20123;&#32422;&#26463;&#30340;&#36890;&#29992;&#20195;&#29702;&#25552;&#20986;&#20102;&#22522;&#20110;&#36125;&#21494;&#26031;&#20915;&#31574;&#29702;&#35770;&#30340;&#35745;&#31639;&#27700;&#24179;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2303.04352</link><description>&lt;p&gt;
&#35745;&#31639;&#27700;&#24179;&#20998;&#26512;&#36890;&#29992;&#26234;&#33021;&#30340;&#32422;&#26463;&#36981;&#20174;&#24615;
&lt;/p&gt;
&lt;p&gt;
Computational-level Analysis of Constraint Compliance for General Intelligence. (arXiv:2303.04352v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.04352
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#30740;&#31350;&#20102;&#20154;&#31867;&#34892;&#20026;&#21463;&#32422;&#26463;&#35268;&#33539;&#32422;&#26463;&#30340;&#24433;&#21709;&#65292;&#21516;&#26102;&#38024;&#23545;&#20110;&#26500;&#24314;&#36981;&#23432;&#36825;&#20123;&#32422;&#26463;&#30340;&#36890;&#29992;&#20195;&#29702;&#25552;&#20986;&#20102;&#22522;&#20110;&#36125;&#21494;&#26031;&#20915;&#31574;&#29702;&#35770;&#30340;&#35745;&#31639;&#27700;&#24179;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#34892;&#20026;&#21463;&#21040;&#38480;&#21046;&#34892;&#20026;&#30340;&#35268;&#33539;&#21644;&#35268;&#24459;&#30340;&#21046;&#32422;&#12290;&#35268;&#21017;&#12289;&#31036;&#20202;&#12289;&#27861;&#24459;&#21644;&#36947;&#24503;&#20041;&#21153;&#31561;&#32422;&#26463;&#34892;&#20026;&#30340;&#24418;&#24335;&#37117;&#26159;&#23545;&#20154;&#31867;&#34892;&#20026;&#36827;&#34892;&#35268;&#33539;&#30340;&#31867;&#21035;&#12290;&#36825;&#20123;&#32422;&#26463;&#31995;&#32479;&#26159;&#8220;&#22797;&#26434;&#30340;&#8221;&#65306;&#20010;&#20307;&#32422;&#26463;&#36890;&#24120;&#23450;&#20041;&#19981;&#26126;&#30830;&#65292;&#29305;&#23450;&#24773;&#20917;&#19979;&#21738;&#20123;&#32422;&#26463;&#26159;&#30456;&#20851;&#30340;&#21487;&#33021;&#26410;&#30693;&#25110;&#23384;&#22312;&#27495;&#20041;&#65292;&#32422;&#26463;&#20043;&#38388;&#30456;&#20114;&#20316;&#29992;&#21644;&#20914;&#31361;&#65292;&#30830;&#23450;&#22914;&#20309;&#22312;&#30456;&#20851;&#32422;&#26463;&#30340;&#33539;&#22260;&#20869;&#34892;&#20107;&#21487;&#33021;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#65292;&#23588;&#20854;&#26159;&#22312;&#38656;&#35201;&#24555;&#36895;&#20915;&#31574;&#30340;&#24773;&#20917;&#19979;&#12290;&#23613;&#31649;&#23384;&#22312;&#36825;&#26679;&#30340;&#28151;&#20081;&#65292;&#20154;&#31867;&#20173;&#28982;&#33021;&#22815;&#31283;&#20581;&#12289;&#24555;&#36895;&#22320;&#23558;&#32422;&#26463;&#34701;&#20837;&#20854;&#20915;&#31574;&#20013;&#12290;&#36890;&#29992;&#30340;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#20063;&#24517;&#39035;&#33021;&#22815;&#22312;&#29616;&#23454;&#32422;&#26463;&#31995;&#32479;&#30340;&#28151;&#20081;&#20013;&#36827;&#34892;&#23548;&#33322;&#65292;&#20197;&#20415;&#34892;&#20026;&#20855;&#26377;&#21487;&#39044;&#27979;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;&#36890;&#29992;&#20195;&#29702;&#32422;&#26463;&#22788;&#29702;&#30340;&#22797;&#26434;&#24615;&#26469;&#28304;&#36827;&#34892;&#20102;&#34920;&#24449;&#65292;&#24182;&#22522;&#20110;&#36125;&#21494;&#26031;&#20915;&#31574;&#29702;&#35770;&#30340;&#24605;&#24819;&#25551;&#36848;&#20102;&#19968;&#31181;&#38024;&#23545;&#36825;&#31181;&#32422;&#26463;&#35745;&#31639;&#27700;&#24179;&#20998;&#26512;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#21015;&#20030;&#20102;&#20960;&#20010;&#24314;&#35774;&#36890;&#29992;&#30340;&#12289;&#36981;&#23432;&#32422;&#26463;&#30340;&#20195;&#29702;&#30340;&#38556;&#30861;&#65292;&#24182;&#25551;&#36848;&#20102;&#20195;&#29702;&#22914;&#20309;&#21487;&#20197;&#20197;&#26368;&#20248;&#26041;&#24335;&#32771;&#34385;&#32422;&#26463;&#30340;&#36125;&#21494;&#26031;&#35745;&#31639;&#27700;&#24179;&#27169;&#22411;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#23545;&#36890;&#29992;&#26234;&#33021;&#30740;&#31350;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human behavior is conditioned by codes and norms that constrain action. Rules, ``manners,'' laws, and moral imperatives are examples of classes of constraints that govern human behavior. These systems of constraints are ``messy:'' individual constraints are often poorly defined, what constraints are relevant in a particular situation may be unknown or ambiguous, constraints interact and conflict with one another, and determining how to act within the bounds of the relevant constraints may be a significant challenge, especially when rapid decisions are needed. Despite such messiness, humans incorporate constraints in their decisions robustly and rapidly. General, artificially-intelligent agents must also be able to navigate the messiness of systems of real-world constraints in order to behave predictability and reliably. In this paper, we characterize sources of complexity in constraint processing for general agents and describe a computational-level analysis for such \textit{constraint
&lt;/p&gt;</description></item><item><title>iSAGE&#26159;&#19968;&#31181;&#22522;&#20110;&#22686;&#37327;&#23398;&#20064;&#30340;SAGE&#22312;&#32447;&#35299;&#37322;&#26041;&#27861;&#65292;&#20855;&#22791;&#24555;&#36895;&#12289;&#20869;&#23384;&#39640;&#25928;&#30340;&#29305;&#28857;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#23545;&#27169;&#22411;&#21464;&#21270;&#20197;&#21450;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#20013;&#30340;&#28418;&#31227;&#36827;&#34892;&#21453;&#24212;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#26377;&#25928;&#30340;&#29305;&#24449;&#31227;&#38500;&#26041;&#27861;&#65292;&#20855;&#26377;&#21644;SAGE&#31867;&#20284;&#30340;&#29702;&#35770;&#24615;&#36136;&#12290;</title><link>http://arxiv.org/abs/2303.01181</link><description>&lt;p&gt;
iSAGE&#65306;&#19968;&#31181;&#22522;&#20110;&#22686;&#37327;&#23398;&#20064;&#30340;SAGE&#22312;&#32447;&#35299;&#37322;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
iSAGE: An Incremental Version of SAGE for Online Explanation on Data Streams. (arXiv:2303.01181v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.01181
&lt;/p&gt;
&lt;p&gt;
iSAGE&#26159;&#19968;&#31181;&#22522;&#20110;&#22686;&#37327;&#23398;&#20064;&#30340;SAGE&#22312;&#32447;&#35299;&#37322;&#26041;&#27861;&#65292;&#20855;&#22791;&#24555;&#36895;&#12289;&#20869;&#23384;&#39640;&#25928;&#30340;&#29305;&#28857;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#23545;&#27169;&#22411;&#21464;&#21270;&#20197;&#21450;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#20013;&#30340;&#28418;&#31227;&#36827;&#34892;&#21453;&#24212;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#26377;&#25928;&#30340;&#29305;&#24449;&#31227;&#38500;&#26041;&#27861;&#65292;&#20855;&#26377;&#21644;SAGE&#31867;&#20284;&#30340;&#29702;&#35770;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#26041;&#27861;&#65292;&#21253;&#25324;SAGE&#31561;&#27969;&#34892;&#30340;&#29305;&#24449;&#37325;&#35201;&#24615;&#27979;&#37327;&#65292;&#22823;&#22810;&#38480;&#20110;&#25209;&#37327;&#23398;&#20064;&#22330;&#26223;&#12290;&#28982;&#32780;&#65292;&#26426;&#22120;&#23398;&#20064;&#36890;&#24120;&#24212;&#29992;&#20110;&#21160;&#24577;&#29615;&#22659;&#20013;&#65292;&#25968;&#25454;&#25345;&#32493;&#21040;&#36798;&#65292;&#24517;&#39035;&#20197;&#22312;&#32447;&#26041;&#24335;&#36827;&#34892;&#23398;&#20064;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;iSAGE&#65292;&#19968;&#31181;&#24555;&#36895;&#12289;&#20869;&#23384;&#39640;&#25928;&#30340;SAGE&#22686;&#37327;&#26041;&#27861;&#65292;&#23427;&#33021;&#22815;&#23545;&#27169;&#22411;&#30340;&#21464;&#21270;&#20197;&#21450;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#20013;&#30340;&#28418;&#31227;&#36827;&#34892;&#21453;&#24212;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#26377;&#25928;&#30340;&#29305;&#24449;&#31227;&#38500;&#26041;&#27861;&#65292;&#30772;&#22351;&#65288;&#24178;&#39044;&#65289;&#21644;&#20445;&#30041;&#65288;&#35266;&#27979;&#65289;&#29305;&#24449;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#27491;&#24335;&#20998;&#26512;&#20102;&#25105;&#20204;&#30340;&#35299;&#37322;&#26041;&#27861;&#65292;&#23637;&#31034;&#20102;iSAGE&#19982;SAGE&#20855;&#26377;&#31867;&#20284;&#30340;&#29702;&#35770;&#24615;&#36136;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#22522;&#20110;&#24191;&#27867;&#20351;&#29992;&#30340;&#25968;&#25454;&#38598;&#21644;&#20855;&#26377;&#27010;&#24565;&#28418;&#31227;&#30340;&#25968;&#25454;&#27969;&#23545;&#25105;&#20204;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#24443;&#24213;&#30340;&#23454;&#39564;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing methods for explainable artificial intelligence (XAI), including popular feature importance measures such as SAGE, are mostly restricted to the batch learning scenario. However, machine learning is often applied in dynamic environments, where data arrives continuously and learning must be done in an online manner. Therefore, we propose iSAGE, a time- and memory-efficient incrementalization of SAGE, which is able to react to changes in the model as well as to drift in the data-generating process. We further provide efficient feature removal methods that break (interventional) and retain (observational) feature dependencies. Moreover, we formally analyze our explanation method to show that iSAGE adheres to similar theoretical properties as SAGE. Finally, we evaluate our approach in a thorough experimental analysis based on well-established data sets and data streams with concept drift.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#32034;&#20102;&#22810;&#31181;Gumbel-Softmax&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;MADDPG&#20013;&#65292;&#20197;&#35299;&#20915;&#31163;&#25955;&#21160;&#20316;&#31354;&#38388;&#19979;&#30340;&#24615;&#33021;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2302.11793</link><description>&lt;p&gt;
&#37325;&#35775;MADDPG&#20013;&#30340;Gumbel-Softmax
&lt;/p&gt;
&lt;p&gt;
Revisiting the Gumbel-Softmax in MADDPG. (arXiv:2302.11793v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.11793
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#32034;&#20102;&#22810;&#31181;Gumbel-Softmax&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;MADDPG&#20013;&#65292;&#20197;&#35299;&#20915;&#31163;&#25955;&#21160;&#20316;&#31354;&#38388;&#19979;&#30340;&#24615;&#33021;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
MADDPG&#26159;&#19968;&#31181;&#36866;&#29992;&#20110;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#31639;&#27861;&#65292;&#23427;&#23558;&#21333;&#26234;&#33021;&#20307;&#26041;&#27861;DDPG&#25512;&#24191;&#21040;&#22810;&#26234;&#33021;&#20307;&#22330;&#26223;&#20013;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;DDPG&#26159;&#19968;&#31181;&#38024;&#23545;&#36830;&#32493;&#21160;&#20316;&#31354;&#38388;&#35774;&#35745;&#30340;&#31639;&#27861;&#65292;&#22312;&#20854;&#20013;&#29366;&#24577;-&#21160;&#20316;&#20215;&#20540;&#20989;&#25968;&#30340;&#26799;&#24230;&#23384;&#22312;&#12290;&#20026;&#20102;&#20351;&#35813;&#31639;&#27861;&#36866;&#29992;&#20110;&#31163;&#25955;&#21160;&#20316;&#31354;&#38388;&#65292;&#24517;&#39035;&#36827;&#34892;&#31163;&#25955;&#30340;&#26799;&#24230;&#20272;&#35745;&#12290;&#23545;&#20110;MADDPG&#31639;&#27861;&#65292;&#20351;&#29992;&#20102;Gumbel-Softmax&#65288;GS&#65289;&#20272;&#31639;&#22120;--&#19968;&#31181;&#23558;&#31163;&#25955;&#20998;&#24067;&#26494;&#24347;&#21040;&#31867;&#20284;&#36830;&#32493;&#20998;&#24067;&#30340;&#20877;&#21442;&#25968;&#21270;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#32479;&#35745;&#20559;&#24046;&#65292;&#26368;&#36817;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22522;&#20934;&#27979;&#35797;&#35770;&#25991;&#34920;&#26126;&#65292;&#36825;&#31181;&#20559;&#24046;&#20351;&#24471;MADDPG&#22312;&#26684;&#23376;&#19990;&#30028;&#31561;&#31163;&#25955;&#21160;&#20316;&#31354;&#38388;&#19979;&#34920;&#29616;&#19981;&#20339;&#12290;&#24184;&#36816;&#30340;&#26159;&#65292;GS&#30340;&#35768;&#22810;&#26367;&#20195;&#26041;&#27861;&#23384;&#22312;&#65292;&#20855;&#26377;&#21508;&#31181;&#21508;&#26679;&#30340;&#24615;&#33021;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#20854;&#20013;&#20960;&#31181;&#26367;&#20195;&#26041;&#27861;&#65292;&#24182;&#23558;&#23427;&#20204;&#25972;&#21512;&#21040;&#31163;&#25955;&#26684;&#23376;&#19990;&#30028;&#22330;&#26223;&#20013;&#30340;MADDPG&#20013;&#12290;&#28982;&#21518;&#23545;&#21508;&#31181;&#24615;&#33021;&#25351;&#26631;&#30340;&#30456;&#24212;&#24433;&#21709;&#36827;&#34892;&#20102;&#27979;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
MADDPG is an algorithm in multi-agent reinforcement learning (MARL) that extends the popular single-agent method, DDPG, to multi-agent scenarios. Importantly, DDPG is an algorithm designed for continuous action spaces, where the gradient of the state-action value function exists. For this algorithm to work in discrete action spaces, discrete gradient estimation must be performed. For MADDPG, the Gumbel-Softmax (GS) estimator is used -- a reparameterisation which relaxes a discrete distribution into a similar continuous one. This method, however, is statistically biased, and a recent MARL benchmarking paper suggests that this bias makes MADDPG perform poorly in grid-world situations, where the action space is discrete. Fortunately, many alternatives to the GS exist, boasting a wide range of properties. This paper explores several of these alternatives and integrates them into MADDPG for discrete grid-world scenarios. The corresponding impact on various performance metrics is then measur
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35777;&#26126;&#20840;&#23616;&#26368;&#20248;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;(CVAE)&#21487;&#20197;&#23398;&#20064;&#27491;&#30830;&#30340;&#27969;&#24418;&#32500;&#24230;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#21487;&#20197;&#20849;&#21516;&#23398;&#20064;&#27969;&#24418;&#32500;&#24230;&#21644;&#26465;&#20214;&#20998;&#24067;&#65292;&#20197;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#26356;&#22909;&#30340;&#29305;&#24449;&#20998;&#31163;&#21644;&#26679;&#26412;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2302.11756</link><description>&lt;p&gt;
&#26465;&#20214;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#23398;&#20064;&#27969;&#24418;&#32500;&#24230;
&lt;/p&gt;
&lt;p&gt;
Learning Manifold Dimensions with Conditional Variational Autoencoders. (arXiv:2302.11756v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.11756
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35777;&#26126;&#20840;&#23616;&#26368;&#20248;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;(CVAE)&#21487;&#20197;&#23398;&#20064;&#27491;&#30830;&#30340;&#27969;&#24418;&#32500;&#24230;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#21487;&#20197;&#20849;&#21516;&#23398;&#20064;&#27969;&#24418;&#32500;&#24230;&#21644;&#26465;&#20214;&#20998;&#24067;&#65292;&#20197;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#26356;&#22909;&#30340;&#29305;&#24449;&#20998;&#31163;&#21644;&#26679;&#26412;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#21450;&#20854;&#26465;&#20214;&#25193;&#23637;&#65288;CVAE&#65289;&#22312;&#22810;&#20010;&#39046;&#22495;&#20013;&#33021;&#22815;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#20294;&#23427;&#20204;&#30340;&#31934;&#30830;&#34892;&#20026;&#20173;&#26410;&#23436;&#20840;&#29702;&#35299;&#65292;&#29305;&#21035;&#26159;&#22312;&#25968;&#25454;&#65288;&#22914;&#22270;&#20687;&#65289;&#22312;&#25110;&#25509;&#36817;&#20302;&#32500;&#27969;&#24418;&#19978;&#30340;&#24773;&#20917;&#19979;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;VAE&#20840;&#23616;&#26368;&#23567;&#20540;&#30830;&#23454;&#33021;&#22815;&#24674;&#22797;&#27491;&#30830;&#30340;&#27969;&#24418;&#32500;&#24230;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#26032;&#26041;&#27861;&#26469;&#20849;&#21516;&#23398;&#20064;&#27969;&#24418;&#32500;&#24230;&#21644;&#26465;&#20214;&#20998;&#24067;&#65292;&#36827;&#19968;&#27493;&#25193;&#23637;&#20102;&#36825;&#19968;&#32467;&#26524;&#21040;&#26356;&#19968;&#33324;&#30340;CVAEs&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#21253;&#25324;MNIST&#21644;CelebA&#65292;&#23454;&#29616;&#20102;&#34920;&#29616;&#26368;&#22909;&#30340;&#35270;&#35273;&#36136;&#37327;&#21644;&#29305;&#24449;&#20998;&#31163;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although the variational autoencoder (VAE) and its conditional extension (CVAE) are capable of state-of-the-art results across multiple domains, their precise behavior is still not fully understood, particularly in the context of data (like images) that lie on or near a low-dimensional manifold. For example, while prior work has suggested that the globally optimal VAE solution can learn the correct manifold dimension, a necessary (but not sufficient) condition for producing samples from the true data distribution, this has never been rigorously proven. Moreover, it remains unclear how such considerations would change when various types of conditioning variables are introduced, or when the data support is extended to a union of manifolds (e.g., as is likely the case for MNIST digits and related). In this work, we address these points by first proving that VAE global minima are indeed capable of recovering the correct manifold dimension. We then extend this result to more general CVAEs, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#21644;&#20027;&#21160;&#23398;&#20064;&#30456;&#32467;&#21512;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;Android&#24694;&#24847;&#36719;&#20214;&#20998;&#31867;&#22120;&#20013;&#30340;&#27010;&#24565;&#28418;&#31227;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2302.04332</link><description>&lt;p&gt;
Android&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#30340;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Continuous Learning for Android Malware Detection. (arXiv:2302.04332v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.04332
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#21644;&#20027;&#21160;&#23398;&#20064;&#30456;&#32467;&#21512;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;Android&#24694;&#24847;&#36719;&#20214;&#20998;&#31867;&#22120;&#20013;&#30340;&#27010;&#24565;&#28418;&#31227;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#21487;&#20197;&#20197;&#38750;&#24120;&#39640;&#30340;&#31934;&#24230;&#26816;&#27979;Android&#24694;&#24847;&#36719;&#20214;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#20998;&#31867;&#22120;&#26377;&#19968;&#20010;&#24369;&#28857;&#65292;&#27010;&#24565;&#28418;&#31227;&#65306;&#38543;&#30528;&#24694;&#24847;&#36719;&#20214;&#24212;&#29992;&#21644;&#33391;&#24615;&#24212;&#29992;&#30340;&#19981;&#26029;&#21457;&#23637;&#65292;&#23427;&#20204;&#24456;&#24555;&#20250;&#36807;&#26102;&#24182;&#22833;&#25928;&#12290;&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#20351;&#29992;&#19968;&#24180;&#30340;&#25968;&#25454;&#35757;&#32451;Android&#24694;&#24847;&#36719;&#20214;&#20998;&#31867;&#22120;&#21518;&#65292;F1&#24471;&#20998;&#22312;&#26032;&#30340;&#27979;&#35797;&#26679;&#26412;&#19978;&#33853;&#21518;&#20110;6&#20010;&#26376;&#21518;&#30340;0.99&#33267;0.76&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26032;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;Android&#24694;&#24847;&#36719;&#20214;&#20998;&#31867;&#22120;&#20013;&#30340;&#27010;&#24565;&#28418;&#31227;&#38382;&#39064;&#12290;&#20351;&#29992;&#20027;&#21160;&#23398;&#20064;&#65292;&#25105;&#20204;&#19981;&#26029;&#22320;&#36873;&#25321;&#26032;&#30340;&#26679;&#26412;&#32473;&#20998;&#26512;&#21592;&#26631;&#27880;&#65292;&#24182;&#23558;&#26631;&#27880;&#30340;&#26679;&#26412;&#28155;&#21152;&#21040;&#35757;&#32451;&#38598;&#20013;&#37325;&#26032;&#35757;&#32451;&#20998;&#31867;&#22120;&#26469;&#25345;&#32493;&#26356;&#26032;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#24605;&#24819;&#26159;&#65292;&#22522;&#20110;&#30456;&#20284;&#24230;&#30340;&#19981;&#30830;&#23450;&#24615;&#26356;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#23558;&#23545;&#27604;&#23398;&#20064;&#19982;&#20027;&#21160;&#23398;&#20064;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#23618;&#23545;&#27604;&#23398;&#20064;&#26041;&#26696;&#21644;&#19968;&#31181;&#26032;&#30340;&#26679;&#26412;&#36873;&#25321;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning methods can detect Android malware with very high accuracy. However, these classifiers have an Achilles heel, concept drift: they rapidly become out of date and ineffective, due to the evolution of malware apps and benign apps. Our research finds that, after training an Android malware classifier on one year's worth of data, the F1 score quickly dropped from 0.99 to 0.76 after 6 months of deployment on new test samples.  In this paper, we propose new methods to combat the concept drift problem of Android malware classifiers. Since machine learning technique needs to be continuously deployed, we use active learning: we select new samples for analysts to label, and then add the labeled samples to the training set to retrain the classifier. Our key idea is, similarity-based uncertainty is more robust against concept drift. Therefore, we combine contrastive learning with active learning. We propose a new hierarchical contrastive learning scheme, and a new sample selection 
&lt;/p&gt;</description></item><item><title>Mnemosyne&#20248;&#21270;&#22120;&#20351;&#29992;Performers&#26041;&#27861;&#26469;&#23398;&#20064;&#35757;&#32451;&#25972;&#20010;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#21253;&#25324;&#20854;&#20182;Transformers&#65292;&#32780;&#26080;&#38656;&#38024;&#23545;&#29305;&#23450;&#20219;&#21153;&#36827;&#34892;&#20248;&#21270;&#22120;&#35843;&#33410;&#65292;&#24182;&#25104;&#21151;&#35757;&#32451;ViTs&#21644;&#24212;&#29992;&#20110;&#26426;&#22120;&#20154;&#39046;&#22495;&#20013;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#19982;&#24555;&#36895;&#25910;&#25947;&#12290;</title><link>http://arxiv.org/abs/2302.01128</link><description>&lt;p&gt;
Mnemosyne: &#20351;&#29992;Transformers&#26469;&#35757;&#32451;Transformers
&lt;/p&gt;
&lt;p&gt;
Mnemosyne: Learning to Train Transformers with Transformers. (arXiv:2302.01128v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.01128
&lt;/p&gt;
&lt;p&gt;
Mnemosyne&#20248;&#21270;&#22120;&#20351;&#29992;Performers&#26041;&#27861;&#26469;&#23398;&#20064;&#35757;&#32451;&#25972;&#20010;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#21253;&#25324;&#20854;&#20182;Transformers&#65292;&#32780;&#26080;&#38656;&#38024;&#23545;&#29305;&#23450;&#20219;&#21153;&#36827;&#34892;&#20248;&#21270;&#22120;&#35843;&#33410;&#65292;&#24182;&#25104;&#21151;&#35757;&#32451;ViTs&#21644;&#24212;&#29992;&#20110;&#26426;&#22120;&#20154;&#39046;&#22495;&#20013;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#19982;&#24555;&#36895;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#22797;&#26434;&#30340;&#26426;&#22120;&#23398;&#20064;(ML)&#26550;&#26500;&#38656;&#35201;&#32791;&#36153;&#22823;&#37327;&#35745;&#31639;&#21644;&#26102;&#38388;&#26469;&#36873;&#25321;&#21512;&#36866;&#30340;&#20248;&#21270;&#22120;&#24182;&#35843;&#33410;&#20854;&#36229;&#21442;&#25968;&#12290;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#20248;&#21270;&#22120;&#30340;&#26032;&#23398;&#20064;&#33539;&#24335;&#24050;&#32463;&#25104;&#20026;&#25163;&#21160;&#35774;&#35745;ML&#20248;&#21270;&#22120;&#30340;&#26356;&#22909;&#36873;&#25321;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Mnemosyne&#20248;&#21270;&#22120;&#65292;&#23427;&#20351;&#29992;Performers: &#38544;&#24335;&#20302;&#31209;attention Transformers&#12290;&#23427;&#21487;&#20197;&#23398;&#20064;&#35757;&#32451;&#25972;&#20010;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#21253;&#25324;&#20854;&#20182;Transformers&#65292;&#32780;&#26080;&#38656;&#38024;&#23545;&#29305;&#23450;&#20219;&#21153;&#36827;&#34892;&#20248;&#21270;&#22120;&#35843;&#33410;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;Mnemosyne&#65306;(a)&#27604;&#27969;&#34892;&#30340;LSTM&#20248;&#21270;&#22120;&#20855;&#26377;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#65307;(b)&#29305;&#21035;&#22320;&#65292;&#21487;&#20197;&#22312;&#26631;&#20934;MLPs&#19978;&#36827;&#34892;&#20803;&#35757;&#32451;&#21518;&#25104;&#21151;&#22320;&#35757;&#32451;Vision Transformers(ViTs) (c)&#21487;&#20197;&#21021;&#22987;&#21270;&#20248;&#21270;&#22120;&#20197;&#23454;&#29616;&#26426;&#22120;&#20154;&#24212;&#29992;&#20013;&#26356;&#24555;&#30340;&#25910;&#25947;&#12290;&#25105;&#20204;&#30456;&#20449;&#36825;&#20123;&#32467;&#26524;&#24320;&#21551;&#20102;&#20351;&#29992;Transformers&#26500;&#24314;&#22522;&#30784;&#20248;&#21270;&#27169;&#22411;&#30340;&#21487;&#33021;&#24615;&#65292;&#21487;&#20197;&#24212;&#23545;&#24120;&#35268;&#30340;Transformer&#35757;&#32451;&#25361;&#25112;&#12290;&#25105;&#20204;&#36890;&#36807;&#24191;&#27867;&#30340;&#29702;&#35770;&#20998;&#26512;&#26469;&#34917;&#20805;&#25105;&#20204;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training complex machine learning (ML) architectures requires a compute and time consuming process of selecting the right optimizer and tuning its hyper-parameters. A new paradigm of learning optimizers from data has emerged as a better alternative to hand-designed ML optimizers. We propose Mnemosyne optimizer, that uses Performers: implicit low-rank attention Transformers. It can learn to train entire neural network architectures including other Transformers without any task-specific optimizer tuning. We show that Mnemosyne: (a) generalizes better than popular LSTM optimizer, (b) in particular can successfully train Vision Transformers (ViTs) while meta--trained on standard MLPs and (c) can initialize optimizers for faster convergence in Robotics applications. We believe that these results open the possibility of using Transformers to build foundational optimization models that can address the challenges of regular Transformer training. We complement our results with an extensive theo
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#23558;&#20165;&#22788;&#29702;&#25991;&#26412;&#30340;&#35821;&#35328;&#27169;&#22411;&#19982;&#22270;&#20687;&#36827;&#34892;&#32852;&#31995;&#65292;&#20351;&#20854;&#33021;&#22815;&#22788;&#29702;&#20219;&#24847;&#20132;&#38169;&#30340;&#22270;&#20687;&#21644;&#25991;&#26412;&#25968;&#25454;&#65292;&#24182;&#29983;&#25104;&#19982;&#26816;&#32034;&#22270;&#20687;&#20132;&#38169;&#30340;&#33258;&#30001;&#24418;&#24335;&#25991;&#26412;&#12290;&#35813;&#26041;&#27861;&#22312;&#29615;&#22659;&#30456;&#20851;&#30340;&#22270;&#20687;&#26816;&#32034;&#21644;&#22810;&#27169;&#24577;&#23545;&#35805;&#31561;&#20219;&#21153;&#20013;&#34920;&#29616;&#21313;&#20998;&#20248;&#24322;&#65292;&#26159;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#35299;&#20915;&#35270;&#35273;&#22330;&#26223;&#19979;&#20132;&#20114;&#38382;&#39064;&#30340;&#26377;&#25928;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2301.13823</link><description>&lt;p&gt;
&#23558;&#35821;&#35328;&#27169;&#22411;&#19982;&#22270;&#20687;&#36827;&#34892;&#32852;&#31995;&#20197;&#22788;&#29702;&#22810;&#27169;&#24577;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
Grounding Language Models to Images for Multimodal Inputs and Outputs. (arXiv:2301.13823v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.13823
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#23558;&#20165;&#22788;&#29702;&#25991;&#26412;&#30340;&#35821;&#35328;&#27169;&#22411;&#19982;&#22270;&#20687;&#36827;&#34892;&#32852;&#31995;&#65292;&#20351;&#20854;&#33021;&#22815;&#22788;&#29702;&#20219;&#24847;&#20132;&#38169;&#30340;&#22270;&#20687;&#21644;&#25991;&#26412;&#25968;&#25454;&#65292;&#24182;&#29983;&#25104;&#19982;&#26816;&#32034;&#22270;&#20687;&#20132;&#38169;&#30340;&#33258;&#30001;&#24418;&#24335;&#25991;&#26412;&#12290;&#35813;&#26041;&#27861;&#22312;&#29615;&#22659;&#30456;&#20851;&#30340;&#22270;&#20687;&#26816;&#32034;&#21644;&#22810;&#27169;&#24577;&#23545;&#35805;&#31561;&#20219;&#21153;&#20013;&#34920;&#29616;&#21313;&#20998;&#20248;&#24322;&#65292;&#26159;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#35299;&#20915;&#35270;&#35273;&#22330;&#26223;&#19979;&#20132;&#20114;&#38382;&#39064;&#30340;&#26377;&#25928;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#23558;&#39044;&#35757;&#32451;&#30340;&#20165;&#25991;&#26412;&#35821;&#35328;&#27169;&#22411;&#19982;&#35270;&#35273;&#39046;&#22495;&#32852;&#31995;&#36215;&#26469;&#65292;&#20351;&#20854;&#33021;&#22815;&#22788;&#29702;&#20219;&#24847;&#20132;&#38169;&#30340;&#22270;&#20687;&#21644;&#25991;&#26412;&#25968;&#25454;&#65292;&#24182;&#29983;&#25104;&#19982;&#26816;&#32034;&#22270;&#20687;&#20132;&#38169;&#30340;&#25991;&#26412;&#12290;&#25105;&#20204;&#21033;&#29992;&#20174;&#22823;&#35268;&#27169;&#25991;&#26412;&#39044;&#35757;&#32451;&#20013;&#23398;&#21040;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#20363;&#22914;&#19978;&#19979;&#25991;&#23398;&#20064;&#21644;&#33258;&#30001;&#24418;&#24335;&#25991;&#26412;&#29983;&#25104;&#12290;&#25105;&#20204;&#20445;&#25345;&#35821;&#35328;&#27169;&#22411;&#20923;&#32467;&#65292;&#24182;&#24494;&#35843;&#36755;&#20837;&#21644;&#36755;&#20986;&#32447;&#24615;&#23618;&#20197;&#23454;&#29616;&#36328;&#27169;&#24577;&#20132;&#20114;&#12290;&#36825;&#20351;&#24471;&#25105;&#20204;&#30340;&#27169;&#22411;&#33021;&#22815;&#22788;&#29702;&#20219;&#24847;&#20132;&#38169;&#30340;&#22270;&#20687;&#21644;&#25991;&#26412;&#36755;&#20837;&#65292;&#24182;&#29983;&#25104;&#19982;&#26816;&#32034;&#22270;&#20687;&#20132;&#38169;&#30340;&#33258;&#30001;&#24418;&#24335;&#25991;&#26412;&#12290;&#25105;&#20204;&#22312;&#29615;&#22659;&#30456;&#20851;&#30340;&#22270;&#20687;&#26816;&#32034;&#21644;&#22810;&#27169;&#24577;&#23545;&#35805;&#31561;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#24378;&#22823;&#30340;&#38646;-shot&#34920;&#29616;&#65292;&#24182;&#23637;&#31034;&#20102;&#24341;&#20154;&#20837;&#32988;&#30340;&#20132;&#20114;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36866;&#29992;&#20110;&#20219;&#20309;&#29616;&#25104;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#20026;&#22312;&#35270;&#35273;&#22330;&#26223;&#19979;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#25928;&#19988;&#36890;&#29992;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose an efficient method to ground pretrained text-only language models to the visual domain, enabling them to process arbitrarily interleaved image-and-text data, and generate text interleaved with retrieved images. Our method leverages the abilities of language models learnt from large scale text-only pretraining, such as in-context learning and free-form text generation. We keep the language model frozen, and finetune input and output linear layers to enable cross-modality interactions. This allows our model to process arbitrarily interleaved image-and-text inputs, and generate free-form text interleaved with retrieved images. We achieve strong zero-shot performance on grounded tasks such as contextual image retrieval and multimodal dialogue, and showcase compelling interactive abilities. Our approach works with any off-the-shelf language model and paves the way towards an effective, general solution for leveraging pretrained language models in visually grounded settings.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;(VLMs)&#26469;&#30417;&#30563;&#20855;&#20307;&#20195;&#29702;&#65292;&#23558;&#20114;&#32852;&#32593;&#35268;&#27169;&#30340;VLMs&#30340;&#36890;&#29992;&#35821;&#35328;&#25509;&#22320;&#25216;&#33021;&#37325;&#26032;&#21033;&#29992;&#21040;&#20855;&#20307;&#20195;&#29702;&#30340;&#34892;&#21160;&#20013;&#12290;</title><link>http://arxiv.org/abs/2301.12507</link><description>&lt;p&gt;
&#23558;&#20114;&#32852;&#32593;&#35268;&#27169;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#36716;&#21270;&#20026;&#20855;&#20307;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
Distilling Internet-Scale Vision-Language Models into Embodied Agents. (arXiv:2301.12507v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12507
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;(VLMs)&#26469;&#30417;&#30563;&#20855;&#20307;&#20195;&#29702;&#65292;&#23558;&#20114;&#32852;&#32593;&#35268;&#27169;&#30340;VLMs&#30340;&#36890;&#29992;&#35821;&#35328;&#25509;&#22320;&#25216;&#33021;&#37325;&#26032;&#21033;&#29992;&#21040;&#20855;&#20307;&#20195;&#29702;&#30340;&#34892;&#21160;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25351;&#20196;&#36319;&#36394;&#20195;&#29702;&#24517;&#39035;&#23558;&#35821;&#35328;&#25509;&#22320;&#21040;&#20854;&#35266;&#23519;&#21644;&#25805;&#20316;&#31354;&#38388;&#20013;&#12290;&#23398;&#20064;&#25509;&#22320;&#35821;&#35328;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#36890;&#24120;&#38656;&#35201;&#29305;&#23450;&#39046;&#22495;&#30340;&#24037;&#31243;&#25110;&#22823;&#37327;&#20154;&#31867;&#20132;&#20114;&#25968;&#25454;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24314;&#35758;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411; (VLMs) &#26469;&#30417;&#30563;&#20855;&#20307;&#20195;&#29702;&#12290;&#25105;&#20204;&#32467;&#21512;&#20102;&#27169;&#22411;&#33976;&#39311;&#21644;&#20107;&#21518;&#32463;&#39564;&#22238;&#25918; (HER) &#30340;&#24819;&#27861;&#65292;&#20351;&#29992; VLM &#26469;&#36861;&#28335;&#24615;&#22320;&#29983;&#25104;&#25551;&#36848;&#20195;&#29702;&#34892;&#20026;&#30340;&#35821;&#35328;&#12290;&#31616;&#21333;&#30340;&#25552;&#31034;&#20801;&#35768;&#25105;&#20204;&#25511;&#21046;&#30417;&#30563;&#20449;&#21495;&#65292;&#25945;&#20195;&#29702;&#26681;&#25454;&#20854;&#21517;&#31216; (&#20363;&#22914;&#65292;&#39134;&#26426;) &#25110;&#20854;&#29305;&#24449; (&#20363;&#22914;&#65292;&#39068;&#33394;) &#19982;&#26032;&#22411;&#29289;&#20307;&#20132;&#20114;&#22312;3D&#28210;&#26579;&#29615;&#22659;&#20013;&#12290;Fewshot&#25552;&#31034;&#20351;&#25105;&#20204;&#21487;&#20197;&#25945;&#25480;&#25277;&#35937;&#30340;&#31867;&#21035;&#25104;&#21592;&#36164;&#26684;&#65292;&#21253;&#25324;&#29616;&#26377;&#31867;&#21035;&#65288;&#39135;&#21697; vs &#29609;&#20855;&#65289;&#21644;&#33258;&#21457;&#31867;&#21035;&#65288;&#23545;&#23545;&#35937;&#30340;&#20219;&#24847;&#20559;&#22909;&#65289;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#27010;&#36848;&#20102;&#19968;&#31181;&#26032;&#30340;&#26377;&#25928;&#26041;&#27861;&#65292;&#20351;&#29992;&#20114;&#32852;&#32593;&#35268;&#27169;&#30340;VLMs&#65292;&#37325;&#26032;&#21033;&#29992;&#23427;&#20204;&#20174;&#22823;&#37327;&#25991;&#26412;&#25968;&#25454;&#20013;&#23398;&#20064;&#30340;&#36890;&#29992;&#35821;&#35328;&#25509;&#22320;&#25216;&#33021;&#65292;&#23558;&#35821;&#35328;&#25509;&#22320;&#21040;&#20855;&#20307;&#20195;&#29702;&#30340;&#34892;&#21160;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Instruction-following agents must ground language into their observation and action spaces. Learning to ground language is challenging, typically requiring domain-specific engineering or large quantities of human interaction data. To address this challenge, we propose using pretrained vision-language models (VLMs) to supervise embodied agents. We combine ideas from model distillation and hindsight experience replay (HER), using a VLM to retroactively generate language describing the agent's behavior. Simple prompting allows us to control the supervision signal, teaching an agent to interact with novel objects based on their names (e.g., planes) or their features (e.g., colors) in a 3D rendered environment. Fewshot prompting lets us teach abstract category membership, including pre-existing categories (food vs toys) and ad-hoc ones (arbitrary preferences over objects). Our work outlines a new and effective way to use internet-scale VLMs, repurposing the generic language grounding acquir
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20154;&#31867;&#30452;&#35273;&#23545;&#20154;&#24037;&#26234;&#33021;&#20915;&#31574;&#20013;&#30340;&#20381;&#36182;&#20316;&#29992;&#21644;&#35299;&#37322;&#65292;&#22312;&#37319;&#29992;&#29305;&#24449;&#21644;&#22522;&#20110;&#31034;&#20363;&#30340;&#20004;&#31181;&#35299;&#37322;&#31867;&#22411;&#36827;&#34892;&#20004;&#20010;&#39044;&#27979;&#20219;&#21153;&#30340;&#24605;&#32771;&#23454;&#39564;&#20013;&#30830;&#35748;&#20102;&#19977;&#31181;&#30452;&#35273;&#31867;&#22411;&#65306;&#20851;&#20110;AI&#27491;&#30830;&#24615;&#30340;&#30452;&#35273;&#12289;&#22522;&#20110;&#23545;&#20219;&#21153;&#21644;AI&#27169;&#22411;&#30340;&#39044;&#26399;&#30340;&#30452;&#35273;&#20197;&#21450;&#22522;&#20110;&#20010;&#20154;&#20215;&#20540;&#35266;&#21644;&#30446;&#26631;&#30340;&#30452;&#35273;&#12290;</title><link>http://arxiv.org/abs/2301.07255</link><description>&lt;p&gt;
&#29702;&#35299;&#20154;&#31867;&#30452;&#35273;&#23545;&#20154;&#24037;&#26234;&#33021;&#20915;&#31574;&#20013;&#20381;&#36182;&#30340;&#20316;&#29992;&#19982;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Understanding the Role of Human Intuition on Reliance in Human-AI Decision-Making with Explanations. (arXiv:2301.07255v3 [cs.HC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.07255
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20154;&#31867;&#30452;&#35273;&#23545;&#20154;&#24037;&#26234;&#33021;&#20915;&#31574;&#20013;&#30340;&#20381;&#36182;&#20316;&#29992;&#21644;&#35299;&#37322;&#65292;&#22312;&#37319;&#29992;&#29305;&#24449;&#21644;&#22522;&#20110;&#31034;&#20363;&#30340;&#20004;&#31181;&#35299;&#37322;&#31867;&#22411;&#36827;&#34892;&#20004;&#20010;&#39044;&#27979;&#20219;&#21153;&#30340;&#24605;&#32771;&#23454;&#39564;&#20013;&#30830;&#35748;&#20102;&#19977;&#31181;&#30452;&#35273;&#31867;&#22411;&#65306;&#20851;&#20110;AI&#27491;&#30830;&#24615;&#30340;&#30452;&#35273;&#12289;&#22522;&#20110;&#23545;&#20219;&#21153;&#21644;AI&#27169;&#22411;&#30340;&#39044;&#26399;&#30340;&#30452;&#35273;&#20197;&#21450;&#22522;&#20110;&#20010;&#20154;&#20215;&#20540;&#35266;&#21644;&#30446;&#26631;&#30340;&#30452;&#35273;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
AI&#35299;&#37322;&#32463;&#24120;&#34987;&#25552;&#21450;&#20316;&#20026;&#25552;&#39640;&#20154;&#24037;&#26234;&#33021;&#20915;&#31574;&#30340;&#26041;&#24335;&#65292;&#20294;&#23454;&#35777;&#30740;&#31350;&#27809;&#26377;&#25214;&#21040;&#35299;&#37322;&#30340;&#26377;&#25928;&#24615;&#19968;&#33268;&#30340;&#35777;&#25454;&#65292;&#21453;&#32780;&#34920;&#26126;&#23427;&#20204;&#20250;&#22312;AI&#31995;&#32479;&#20986;&#29616;&#38169;&#35823;&#26102;&#22686;&#21152;&#36807;&#24230;&#20381;&#36182;&#12290;&#34429;&#28982;&#35768;&#22810;&#22240;&#32032;&#21487;&#33021;&#24433;&#21709;&#23545;AI&#25903;&#25345;&#30340;&#20381;&#36182;&#65292;&#20294;&#19968;&#20010;&#37325;&#35201;&#22240;&#32032;&#26159;&#20915;&#31574;&#32773;&#22914;&#20309;&#21327;&#35843;&#33258;&#24049;&#30340;&#30452;&#35273;&#8212;&#8212;&#22522;&#20110;&#20808;&#21069;&#30693;&#35782;&#12289;&#32463;&#39564;&#25110;&#27169;&#24335;&#35782;&#21035;&#30340;&#20449;&#24565;&#25110;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#29992;&#20110;&#36827;&#34892;&#21028;&#26029;&#8212;&#8212;&#19982;AI&#31995;&#32479;&#25552;&#20379;&#30340;&#20449;&#24687;&#30456;&#32467;&#21512;&#65292;&#20197;&#30830;&#23450;&#20309;&#26102;&#35206;&#30422;AI&#39044;&#27979;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#28151;&#21512;&#26041;&#27861;&#30340;&#24605;&#32771;&#23454;&#39564;&#65292;&#37319;&#29992;&#20004;&#31181;&#35299;&#37322;&#31867;&#22411;&#65288;&#29305;&#24449;&#21644;&#22522;&#20110;&#31034;&#20363;&#30340;&#65289;&#36827;&#34892;&#20004;&#20010;&#39044;&#27979;&#20219;&#21153;&#65292;&#20197;&#25506;&#32034;&#20915;&#31574;&#32773;&#30340;&#30452;&#35273;&#22914;&#20309;&#24433;&#21709;&#20182;&#20204;&#23545;AI&#39044;&#27979;&#21644;&#35299;&#37322;&#30340;&#20351;&#29992;&#65292;&#20197;&#21450;&#26368;&#32456;&#20182;&#20204;&#36873;&#25321;&#20309;&#26102;&#20381;&#36182;AI&#30340;&#20915;&#23450;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#30830;&#23450;&#20102;&#19977;&#31181;&#28041;&#21450;&#25512;&#29702;AI&#39044;&#27979;&#21644;&#35299;&#37322;&#30340;&#30452;&#35273;&#31867;&#22411;&#65306;&#20851;&#20110;AI&#27491;&#30830;&#24615;&#30340;&#30452;&#35273;&#12289;&#22522;&#20110;&#23545;&#20219;&#21153;&#21644;AI&#27169;&#22411;&#30340;&#39044;&#26399;&#30340;&#30452;&#35273;&#20197;&#21450;&#22522;&#20110;&#20010;&#20154;&#20215;&#20540;&#35266;&#21644;&#30446;&#26631;&#30340;&#30452;&#35273;&#12290;
&lt;/p&gt;
&lt;p&gt;
AI explanations are often mentioned as a way to improve human-AI decision-making, but empirical studies have not found consistent evidence of explanations' effectiveness and, on the contrary, suggest that they can increase overreliance when the AI system is wrong. While many factors may affect reliance on AI support, one important factor is how decision-makers reconcile their own intuition -- beliefs or heuristics, based on prior knowledge, experience, or pattern recognition, used to make judgments -- with the information provided by the AI system to determine when to override AI predictions. We conduct a think-aloud, mixed-methods study with two explanation types (feature- and example-based) for two prediction tasks to explore how decision-makers' intuition affects their use of AI predictions and explanations, and ultimately their choice of when to rely on AI. Our results identify three types of intuition involved in reasoning about AI predictions and explanations: intuition about the
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25163;&#26415;&#32858;&#21512;&#30340;&#21327;&#21516;&#23398;&#20064;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21487;&#29992;&#20110;&#21327;&#35843;&#21644;&#32858;&#21512;&#20998;&#24067;&#24335;&#21307;&#23398;&#24433;&#20687;&#25968;&#25454;&#38598;&#30340;&#30693;&#35782;&#65292;&#24182;&#24102;&#26377;&#37096;&#20998;&#30142;&#30149;&#27880;&#37322;&#65292;&#20174;&#32780;&#21487;&#35757;&#32451;&#20855;&#26377;&#23436;&#25972;&#33016;&#37096;&#20869;&#21487;&#33021;&#20986;&#29616;&#30340;&#25152;&#26377;&#24322;&#24120;&#30340;&#20020;&#24202;&#23454;&#29992;&#12289;&#24378;&#22823;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2301.06683</link><description>&lt;p&gt;
&#25163;&#26415;&#32858;&#21512;&#65306;&#19968;&#31181;&#29992;&#20110;&#21327;&#21516;&#23398;&#20064;&#30340;&#20998;&#24067;&#24335;&#21307;&#23398;&#24433;&#20687;&#25968;&#25454;&#21644;&#22810;&#26679;&#20219;&#21153;&#21327;&#35843;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Surgical Aggregation: A Collaborative Learning Framework for Harmonizing Distributed Medical Imaging Datasets with Diverse Tasks. (arXiv:2301.06683v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.06683
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25163;&#26415;&#32858;&#21512;&#30340;&#21327;&#21516;&#23398;&#20064;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21487;&#29992;&#20110;&#21327;&#35843;&#21644;&#32858;&#21512;&#20998;&#24067;&#24335;&#21307;&#23398;&#24433;&#20687;&#25968;&#25454;&#38598;&#30340;&#30693;&#35782;&#65292;&#24182;&#24102;&#26377;&#37096;&#20998;&#30142;&#30149;&#27880;&#37322;&#65292;&#20174;&#32780;&#21487;&#35757;&#32451;&#20855;&#26377;&#23436;&#25972;&#33016;&#37096;&#20869;&#21487;&#33021;&#20986;&#29616;&#30340;&#25152;&#26377;&#24322;&#24120;&#30340;&#20020;&#24202;&#23454;&#29992;&#12289;&#24378;&#22823;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#30340;&#33016;&#37096;X&#20809;&#25968;&#25454;&#38598;&#24050;&#32463;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#24322;&#24120;&#26816;&#27979;&#65292;&#24182;&#26377;&#28508;&#21147;&#20026;&#35768;&#22810;&#20020;&#24202;&#24212;&#29992;&#25552;&#20379;&#24040;&#22823;&#30340;&#30410;&#22788;&#12290;&#28982;&#32780;&#65292;&#27599;&#20010;&#25968;&#25454;&#38598;&#20165;&#19987;&#27880;&#20110;&#26816;&#27979;&#24739;&#32773;&#21487;&#33021;&#21516;&#26102;&#20986;&#29616;&#30340;&#19968;&#37096;&#20998;&#21457;&#29616;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#20854;&#20020;&#24202;&#25928;&#29992;&#12290;&#22240;&#27492;&#65292;&#25968;&#25454;&#21327;&#35843;&#23545;&#20110;&#32858;&#21512;&#36825;&#20123;&#25968;&#25454;&#38598;&#26469;&#35757;&#32451;&#20855;&#26377;&#23436;&#25972;&#33016;&#37096;&#20869;&#21487;&#33021;&#20986;&#29616;&#30340;&#25152;&#26377;&#24322;&#24120;&#30340;&#20020;&#24202;&#23454;&#29992;&#12289;&#24378;&#22823;&#27169;&#22411;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25163;&#26415;&#32858;&#21512;&#65292;&#19968;&#31181;&#21327;&#21516;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#21327;&#35843;&#21644;&#32858;&#21512;&#20998;&#24067;&#24335;&#24322;&#26500;&#25968;&#25454;&#38598;&#30340;&#30693;&#35782;&#65292;&#24182;&#24102;&#26377;&#37096;&#20998;&#30142;&#30149;&#27880;&#37322;&#12290;&#25105;&#20204;&#22312;&#21512;&#25104;&#30340;iid&#25968;&#25454;&#38598;&#21644;&#20855;&#26377;&#37096;&#20998;&#27880;&#37322;&#30340;&#30495;&#23454;&#22823;&#35268;&#27169;&#38750;iid&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25163;&#26415;&#32858;&#21512;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25163;&#26415;&#32858;&#21512;&#26174;&#33879;&#20248;&#20110;&#24403;&#21069;&#30340;&#31574;&#30053;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#36890;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large-scale chest x-ray datasets have been curated for the detection of abnormalities using deep learning, with the potential to provide substantial benefits across many clinical applications. However, each dataset focuses only on detecting a subset of findings that can be simultaneously present in a patient, thereby limiting its clinical utility. Therefore, data harmonization is crucial to leverage these datasets in aggregate to train clinically-useful, robust models with a complete representation of all abnormalities that may occur within the thorax. To that end, we propose surgical aggregation, a collaborative learning framework for harmonizing and aggregating knowledge from distributed heterogeneous datasets with partial disease annotations. We evaluate surgical aggregation across synthetic iid datasets and real-world large-scale non-iid datasets with partial annotations. Our results indicate that surgical aggregation significantly outperforms current strategies, has better general
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35299;&#37322;&#23450;&#20041;&#21644;&#29702;&#35770;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;AI&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#12290;&#36825;&#20010;&#26694;&#26550;&#21487;&#24110;&#21161;&#30830;&#23450;&#27169;&#22411;&#20915;&#31574;&#20013;&#26368;&#20855;&#24433;&#21709;&#21147;&#30340;&#36755;&#20837;&#29305;&#24449;&#65292;&#25903;&#25345;&#23545;&#27169;&#22411;&#34892;&#20026;&#30340;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2212.14447</link><description>&lt;p&gt;
&#19968;&#31181;AI&#27169;&#22411;&#35299;&#37322;&#24615;&#30340;&#29702;&#35770;&#26694;&#26550;&#21450;&#20854;&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
A Theoretical Framework for AI Models Explainability with Application in Biomedicine. (arXiv:2212.14447v4 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.14447
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35299;&#37322;&#23450;&#20041;&#21644;&#29702;&#35770;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;AI&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#12290;&#36825;&#20010;&#26694;&#26550;&#21487;&#24110;&#21161;&#30830;&#23450;&#27169;&#22411;&#20915;&#31574;&#20013;&#26368;&#20855;&#24433;&#21709;&#21147;&#30340;&#36755;&#20837;&#29305;&#24449;&#65292;&#25903;&#25345;&#23545;&#27169;&#22411;&#34892;&#20026;&#30340;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#24615;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#26159;&#20154;&#24037;&#26234;&#33021;&#31038;&#21306;&#20013;&#19968;&#20010;&#20805;&#28385;&#27963;&#21147;&#30340;&#30740;&#31350;&#35838;&#39064;&#65292;&#21463;&#21040;&#19981;&#21516;&#26041;&#27861;&#21644;&#39046;&#22495;&#30340;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#34429;&#28982;&#26377;&#24456;&#22810;&#20851;&#20110;&#35813;&#20027;&#39064;&#30340;&#25991;&#31456;&#65292;&#20294;XAI&#20173;&#32570;&#20047;&#20849;&#20139;&#30340;&#26415;&#35821;&#21644;&#26694;&#26550;&#65292;&#26080;&#27861;&#20026;&#35299;&#37322;&#25552;&#20379;&#32467;&#26500;&#19978;&#30340;&#23436;&#25972;&#24615;&#12290;&#22312;&#25105;&#20204;&#30340;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#19968;&#20010;&#26032;&#30340;&#35299;&#37322;&#23450;&#20041;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#35813;&#23450;&#20041;&#26159;&#25991;&#29486;&#20013;&#21487;&#20197;&#25214;&#21040;&#30340;&#32508;&#21512;&#20307;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#35299;&#37322;&#19981;&#26159;&#21407;&#23376;&#24615;&#30340;&#65292;&#32780;&#26159;&#26469;&#33258;&#20110;&#27169;&#22411;&#21644;&#20854;&#36755;&#20837;&#36755;&#20986;&#26144;&#23556;&#30340;&#35777;&#25454;&#65292;&#20197;&#21450;&#20154;&#31867;&#23545;&#36825;&#20123;&#35777;&#25454;&#30340;&#35299;&#37322;&#32452;&#21512;&#32780;&#25104;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#35299;&#37322;&#32435;&#20837;&#21040;&#30495;&#23454;&#24615;&#65288;&#21363;&#35299;&#37322;&#26159;&#21542;&#26159;&#27169;&#22411;&#20869;&#37096;&#24037;&#20316;&#21644;&#20915;&#31574;&#36807;&#31243;&#30340;&#30495;&#23454;&#25551;&#36848;&#65289;&#21644;&#21487;&#20449;&#24230;&#65288;&#21363;&#35299;&#37322;&#23545;&#29992;&#25143;&#30340;&#35828;&#26381;&#21147;&#65289;&#30340;&#29305;&#24615;&#20013;&#12290;&#20351;&#29992;&#25105;&#20204;&#25552;&#20986;&#30340;&#29702;&#35770;&#26694;&#26550;&#31616;&#21270;&#20102;&#36825;&#20123;&#29305;&#24615;&#30340;&#25805;&#20316;&#65292;&#24182;&#25552;&#20379;&#20102;&#35780;&#20272;AI&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#30340;&#32508;&#21512;&#26041;&#27861;&#12290;&#25105;&#20204;&#23558;&#35813;&#26694;&#26550;&#24212;&#29992;&#20110;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#65292;&#24182;&#23637;&#31034;&#20854;&#20351;&#24471;&#25105;&#20204;&#33021;&#22815;&#30830;&#23450;&#27169;&#22411;&#20915;&#31574;&#20013;&#26368;&#20855;&#24433;&#21709;&#21147;&#30340;&#36755;&#20837;&#29305;&#24449;&#65292;&#25903;&#25345;&#23545;&#27169;&#22411;&#34892;&#20026;&#30340;&#35299;&#37322;&#12290;&#25105;&#20204;&#30456;&#20449;&#65292;&#25105;&#20204;&#30340;&#36129;&#29486;&#23558;&#20419;&#36827;&#21487;&#35299;&#37322;&#24615;&#24037;&#20855;&#21644;&#26041;&#27861;&#30340;&#21457;&#23637;&#65292;&#20174;&#32780;&#25512;&#36827;AI&#31639;&#27861;&#22312;&#20851;&#38190;&#39046;&#22495;&#30340;&#21487;&#20449;&#37096;&#32626;&#12290;
&lt;/p&gt;
&lt;p&gt;
EXplainable Artificial Intelligence (XAI) is a vibrant research topic in the artificial intelligence community, with growing interest across methods and domains. Much has been written about the subject, yet XAI still lacks shared terminology and a framework capable of providing structural soundness to explanations. In our work, we address these issues by proposing a novel definition of explanation that is a synthesis of what can be found in the literature. We recognize that explanations are not atomic but the combination of evidence stemming from the model and its input-output mapping, and the human interpretation of this evidence. Furthermore, we fit explanations into the properties of faithfulness (i.e., the explanation being a true description of the model's inner workings and decision-making process) and plausibility (i.e., how much the explanation looks convincing to the user). Using our proposed theoretical framework simplifies how these properties are operationalized and it prov
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026; Detailed Outline Control(DOC) &#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#35814;&#32454;&#22823;&#32434;&#21644;&#35814;&#32454;&#25511;&#21046;&#22120;&#26469;&#25552;&#39640;&#29983;&#25104;&#38271;&#31687;&#25925;&#20107;&#26102;&#30340;&#24773;&#33410;&#36830;&#36143;&#24615;&#21644;&#22823;&#32434;&#30456;&#20851;&#24615;&#65292;&#20154;&#31867;&#35780;&#20272;&#35777;&#23454;&#35813;&#26041;&#27861;&#22312;&#36825;&#20123;&#26041;&#38754;&#26174;&#33879;&#20248;&#20110;&#22522;&#32447;&#26041;&#27861;&#65292;&#24182;&#19988;&#26356;&#36866;&#29992;&#20110;&#20132;&#20114;&#29983;&#25104;&#35774;&#32622;&#12290;</title><link>http://arxiv.org/abs/2212.10077</link><description>&lt;p&gt;
&#36890;&#36807;&#35814;&#32454;&#30340;&#22823;&#32434;&#25511;&#21046;&#25552;&#21319;&#38271;&#31687;&#25925;&#20107;&#36830;&#36143;&#24615;
&lt;/p&gt;
&lt;p&gt;
DOC: Improving Long Story Coherence With Detailed Outline Control. (arXiv:2212.10077v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10077
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026; Detailed Outline Control(DOC) &#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#35814;&#32454;&#22823;&#32434;&#21644;&#35814;&#32454;&#25511;&#21046;&#22120;&#26469;&#25552;&#39640;&#29983;&#25104;&#38271;&#31687;&#25925;&#20107;&#26102;&#30340;&#24773;&#33410;&#36830;&#36143;&#24615;&#21644;&#22823;&#32434;&#30456;&#20851;&#24615;&#65292;&#20154;&#31867;&#35780;&#20272;&#35777;&#23454;&#35813;&#26041;&#27861;&#22312;&#36825;&#20123;&#26041;&#38754;&#26174;&#33879;&#20248;&#20110;&#22522;&#32447;&#26041;&#27861;&#65292;&#24182;&#19988;&#26356;&#36866;&#29992;&#20110;&#20132;&#20114;&#29983;&#25104;&#35774;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026; Detailed Outline Control(DOC)&#30340;&#26694;&#26550;&#65292;&#20197;&#25552;&#39640;&#29983;&#25104;&#25968;&#21315;&#23383;&#38271;&#30340;&#25925;&#20107;&#26102;&#30340;&#38271;&#31243;&#24773;&#33410;&#36830;&#36143;&#24615;&#12290;DOC&#30001;&#20004;&#20010;&#20114;&#34917;&#32452;&#20214;&#32452;&#25104;&#65306;&#35814;&#32454;&#22823;&#32434;&#21644;&#35814;&#32454;&#25511;&#21046;&#22120;&#12290;&#35814;&#32454;&#22823;&#32434;&#21019;&#24314;&#19968;&#20010;&#26356;&#35814;&#32454;&#12289;&#23618;&#27425;&#21270;&#30340;&#22823;&#32434;&#65292;&#23558;&#21019;&#36896;&#24615;&#36127;&#25285;&#20174;&#20027;&#35201;&#36215;&#33609;&#36807;&#31243;&#36716;&#31227;&#21040;&#35268;&#21010;&#38454;&#27573;&#12290;&#35814;&#32454;&#25511;&#21046;&#22120;&#36890;&#36807;&#25511;&#21046;&#25925;&#20107;&#27573;&#33853;&#19982;&#22823;&#32434;&#32454;&#33410;&#23545;&#40784;&#65292;&#30830;&#20445;&#26356;&#35814;&#32454;&#30340;&#22823;&#32434;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#20173;&#28982;&#34987;&#23562;&#37325;&#12290;&#22312;&#33258;&#21160;&#29983;&#25104;&#30340;&#25925;&#20107;&#30340;&#20154;&#31867;&#35780;&#20272;&#20013;&#65292;DOC&#22312;&#24773;&#33410;&#36830;&#36143;&#24615;(22.5% &#32477;&#23545;&#22686;&#30410;)&#12289;&#22823;&#32434;&#30456;&#20851;&#24615;(28.2%)&#21644;&#36259;&#21619;&#24615;(20.7%)&#26041;&#38754;&#26174;&#33879;&#20248;&#20110;&#24378;&#22823;&#30340;Re3&#22522;&#32447;(Yang&#31561;&#20154;&#65292;2022)&#12290;&#20154;&#20204;&#36824;&#35780;&#20215;DOC&#22312;&#20132;&#20114;&#29983;&#25104;&#35774;&#32622;&#26041;&#38754;&#26356;&#26131;&#20110;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose the Detailed Outline Control (DOC) framework for improving long-range plot coherence when automatically generating several-thousand-word-long stories. DOC consists of two complementary components: a detailed outliner and a detailed controller. The detailed outliner creates a more detailed, hierarchically structured outline, shifting creative burden from the main drafting procedure to the planning stage. The detailed controller ensures the more detailed outline is still respected during generation by controlling story passages to align with outline details. In human evaluations of automatically generated stories, DOC substantially outperforms a strong Re3 baseline (Yang et al., 2022) on plot coherence (22.5% absolute gain), outline relevance (28.2%), and interestingness (20.7%). Humans also judged DOC to be much more controllable in an interactive generation setting.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#27010;&#29575;&#26102;&#38388;&#36923;&#36753;PCFTL&#65292;&#31532;&#19968;&#20010;&#21253;&#21547;&#22240;&#26524;&#25512;&#29702;&#36816;&#31639;&#31526;&#30340;&#27010;&#29575;&#26102;&#38388;&#36923;&#36753;&#65292;&#21487;&#25512;&#29702;&#24178;&#39044;&#21644;&#21453;&#20107;&#23454;&#26597;&#35810;&#65292;&#20174;&#32780;&#33021;&#22815;&#25512;&#29702;&#28041;&#21450;MDP&#19981;&#21516;&#37197;&#32622;&#30340;&#8220;&#20551;&#35774;&#8221;&#22330;&#26223;</title><link>http://arxiv.org/abs/2212.08712</link><description>&lt;p&gt;
&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#22240;&#26524;&#26102;&#38388;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Causal Temporal Reasoning for Markov Decision Processes. (arXiv:2212.08712v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.08712
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#27010;&#29575;&#26102;&#38388;&#36923;&#36753;PCFTL&#65292;&#31532;&#19968;&#20010;&#21253;&#21547;&#22240;&#26524;&#25512;&#29702;&#36816;&#31639;&#31526;&#30340;&#27010;&#29575;&#26102;&#38388;&#36923;&#36753;&#65292;&#21487;&#25512;&#29702;&#24178;&#39044;&#21644;&#21453;&#20107;&#23454;&#26597;&#35810;&#65292;&#20174;&#32780;&#33021;&#22815;&#25512;&#29702;&#28041;&#21450;MDP&#19981;&#21516;&#37197;&#32622;&#30340;&#8220;&#20551;&#35774;&#8221;&#22330;&#26223;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#27010;&#29575;&#26102;&#38388;&#36923;&#36753;PCFTL&#65292;&#29992;&#20110;&#39564;&#35777;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;(MDP)&#12290; PCFTL&#26159;&#31532;&#19968;&#20010;&#21253;&#21547;&#22240;&#26524;&#25512;&#29702;&#36816;&#31639;&#31526;&#30340;&#27010;&#29575;&#26102;&#38388;&#36923;&#36753;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#34920;&#36798;&#24178;&#39044;&#21644;&#21453;&#20107;&#23454;&#26597;&#35810;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24191;&#20041;&#30340;&#21453;&#20107;&#23454;&#36816;&#31639;&#31526;&#65292;&#26082;&#21253;&#25324;&#24178;&#39044;&#65292;&#20063;&#21253;&#25324;&#21453;&#20107;&#23454;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#21516;&#20110;&#29616;&#26377;&#30340;&#20165;&#33021;&#25512;&#29702;&#22266;&#23450;&#31995;&#32479;&#37197;&#32622;&#30340;&#27010;&#29575;&#26102;&#38388;&#36923;&#36753;&#65292;&#22240;&#20026;&#23427;&#33021;&#22815;&#25512;&#29702;&#28041;&#21450;MDP&#19981;&#21516;&#37197;&#32622;&#30340;&#8220;&#20551;&#35774;&#8221;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce $\textit{PCFTL (Probabilistic CounterFactual Temporal Logic)}$, a new probabilistic temporal logic for the verification of Markov Decision Processes (MDP). PCFTL is the first to include operators for causal reasoning, allowing us to express interventional and counterfactual queries. Given a path formula $\phi$, an interventional property is concerned with the satisfaction probability of $\phi$ if we apply a particular change $I$ to the MDP (e.g., switching to a different policy); a counterfactual allows us to compute, given an observed MDP path $\tau$, what the outcome of $\phi$ would have been had we applied $I$ in the past. For its ability to reason about \textit{what-if} scenarios involving different configurations of the MDP, our approach represents a departure from existing probabilistic temporal logics that can only reason about a fixed system configuration. From a syntactic viewpoint, we introduce a generalized counterfactual operator that subsumes both intervention
&lt;/p&gt;</description></item><item><title>&#22312;&#22240;&#26524;&#22270;&#20013;&#65292;&#25552;&#20986;&#20102;&#35299;&#20915;&#21069;&#38376;&#35843;&#25972;&#30340;&#32447;&#24615;&#26102;&#38388;&#31639;&#27861;&#65292;&#36890;&#36807;&#35266;&#23519;&#21040;&#30340;&#20013;&#20171;&#21464;&#37327;&#65292;&#21363;&#20351;&#23384;&#22312;&#26410;&#35266;&#27979;&#21040;&#30340;&#28151;&#28102;&#65292;&#20063;&#21487;&#20197;&#35782;&#21035;&#22240;&#26524;&#25928;&#24212;&#12290;</title><link>http://arxiv.org/abs/2211.16468</link><description>&lt;p&gt;
&#22240;&#26524;&#22270;&#20013;&#21069;&#38376;&#35843;&#25972;&#30340;&#32447;&#24615;&#26102;&#38388;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Linear-Time Algorithms for Front-Door Adjustment in Causal Graphs. (arXiv:2211.16468v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.16468
&lt;/p&gt;
&lt;p&gt;
&#22312;&#22240;&#26524;&#22270;&#20013;&#65292;&#25552;&#20986;&#20102;&#35299;&#20915;&#21069;&#38376;&#35843;&#25972;&#30340;&#32447;&#24615;&#26102;&#38388;&#31639;&#27861;&#65292;&#36890;&#36807;&#35266;&#23519;&#21040;&#30340;&#20013;&#20171;&#21464;&#37327;&#65292;&#21363;&#20351;&#23384;&#22312;&#26410;&#35266;&#27979;&#21040;&#30340;&#28151;&#28102;&#65292;&#20063;&#21487;&#20197;&#35782;&#21035;&#22240;&#26524;&#25928;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#20272;&#35745;&#22240;&#26524;&#25928;&#24212;&#26159;&#23454;&#35777;&#31185;&#23398;&#20013;&#30340;&#22522;&#26412;&#20219;&#21153;&#12290;&#24403;&#31995;&#32479;&#20013;&#28041;&#21450;&#26410;&#35266;&#23519;&#21040;&#30340;&#28151;&#28102;&#22240;&#32032;&#26102;&#65292;&#36825;&#21464;&#24471;&#23588;&#20026;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#20391;&#37325;&#20110;&#21069;&#38376;&#35843;&#25972;&#8212;&#8212;&#19968;&#31181;&#32463;&#20856;&#25216;&#26415;&#65292;&#23427;&#20351;&#29992;&#35266;&#23519;&#21040;&#30340;&#20013;&#20171;&#21464;&#37327;&#65292;&#21363;&#20351;&#23384;&#22312;&#26410;&#35266;&#27979;&#21040;&#30340;&#28151;&#28102;&#65292;&#20063;&#21487;&#20197;&#35782;&#21035;&#22240;&#26524;&#25928;&#24212;&#12290;&#34429;&#28982;&#21069;&#38376;&#20272;&#35745;&#30340;&#32479;&#35745;&#29305;&#24615;&#24050;&#32463;&#24456;&#22909;&#22320;&#29702;&#35299;&#20102;&#65292;&#20294;&#23427;&#30340;&#31639;&#27861;&#26041;&#38754;&#38271;&#26399;&#20197;&#26469;&#19968;&#30452;&#26410;&#24471;&#21040;&#25506;&#31350;&#12290;&#26368;&#36817;&#65292;Jeong&#65292;Tian&#21644;Barenboim [NeurIPS 2022]&#25552;&#20986;&#20102;&#19968;&#31181;&#31532;&#19968;&#20010;&#22810;&#39033;&#24335;&#26102;&#38388;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#32473;&#23450;&#30340;&#26377;&#21521;&#26080;&#29615;&#22270;&#65288;DAG&#65289;&#20013;&#25214;&#21040;&#28385;&#36275;&#21069;&#38376;&#20934;&#21017;&#30340;&#38598;&#21512;&#65292;&#20854;&#36816;&#34892;&#26102;&#38388;&#20026;$O&#65288;n^3&#65288;n+m&#65289;&#65289;$&#65292;&#20854;&#20013;$n$&#34920;&#31034;&#21464;&#37327;&#30340;&#25968;&#37327;&#65292;$m$&#34920;&#31034;&#22240;&#26524;&#22270;&#30340;&#36793;&#30340;&#25968;&#37327;&#12290;&#22312;&#25105;&#20204;&#30340;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#20855;&#26377;&#32447;&#24615;&#26102;&#38388;&#22797;&#26434;&#24230;&#30340;&#31639;&#27861;&#65292;&#21363;$O&#65288;n+m&#65289;$&#65292;&#29992;&#20110;&#36825;&#39033;&#20219;&#21153;&#65292;&#20174;&#32780;&#36798;&#21040;&#20102;&#28176;&#36817;&#26368;&#20248;&#30340;&#26102;&#38388;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Causal effect estimation from observational data is a fundamental task in empirical sciences. It becomes particularly challenging when unobserved confounders are involved in a system. This paper focuses on front-door adjustment -- a classic technique which, using observed mediators allows to identify causal effects even in the presence of unobserved confounding. While the statistical properties of the front-door estimation are quite well understood, its algorithmic aspects remained unexplored for a long time. Recently, Jeong, Tian, and Barenboim [NeurIPS 2022] have presented the first polynomial-time algorithm for finding sets satisfying the front-door criterion in a given directed acyclic graph (DAG), with an $O(n^3(n+m))$ run time, where $n$ denotes the number of variables and $m$ the number of edges of the causal graph. In our work, we give the first linear-time, i.e., $O(n+m)$, algorithm for this task, which thus reaches the asymptotically optimal time complexity. This result impli
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#23545;&#25239;&#24615;&#35774;&#32622;&#65292;&#22312;&#20854;&#20013;&#23545;&#25163;&#21482;&#33021;&#23558;&#20449;&#24687;&#38468;&#21152;&#21040;&#21463;&#23475;&#32773;&#30340;&#35266;&#23519;&#20013;&#65292;&#20174;&#32780;&#20135;&#29983;&#26368;&#23567;&#30340;&#24433;&#21709;&#33539;&#22260;&#65292;&#24182;&#25552;&#20986;&#23545;&#25239;&#24615;&#24265;&#20215;&#20132;&#27969;&#65288;ACT&#65289;&#31639;&#27861;&#36827;&#34892;&#23545;&#25163;&#35757;&#32451;&#12290;&#22312;&#39640;&#24230;&#21463;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;ACT&#35757;&#32451;&#30340;&#23545;&#25163;&#20173;&#20250;&#23545;&#21463;&#23475;&#32773;&#30340;&#35757;&#32451;&#21644;&#27979;&#35797;&#34920;&#29616;&#20135;&#29983;&#26174;&#33879;&#24433;&#21709;&#65292;&#25581;&#31034;&#20102;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20013;&#30340;&#19968;&#31181;&#26032;&#30340;&#25915;&#20987;&#21521;&#37327;&#12290;</title><link>http://arxiv.org/abs/2211.11030</link><description>&lt;p&gt;
&#23545;&#25239;&#24615;&#24265;&#20215;&#20132;&#27969;
&lt;/p&gt;
&lt;p&gt;
Adversarial Cheap Talk. (arXiv:2211.11030v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.11030
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#23545;&#25239;&#24615;&#35774;&#32622;&#65292;&#22312;&#20854;&#20013;&#23545;&#25163;&#21482;&#33021;&#23558;&#20449;&#24687;&#38468;&#21152;&#21040;&#21463;&#23475;&#32773;&#30340;&#35266;&#23519;&#20013;&#65292;&#20174;&#32780;&#20135;&#29983;&#26368;&#23567;&#30340;&#24433;&#21709;&#33539;&#22260;&#65292;&#24182;&#25552;&#20986;&#23545;&#25239;&#24615;&#24265;&#20215;&#20132;&#27969;&#65288;ACT&#65289;&#31639;&#27861;&#36827;&#34892;&#23545;&#25163;&#35757;&#32451;&#12290;&#22312;&#39640;&#24230;&#21463;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;ACT&#35757;&#32451;&#30340;&#23545;&#25163;&#20173;&#20250;&#23545;&#21463;&#23475;&#32773;&#30340;&#35757;&#32451;&#21644;&#27979;&#35797;&#34920;&#29616;&#20135;&#29983;&#26174;&#33879;&#24433;&#21709;&#65292;&#25581;&#31034;&#20102;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20013;&#30340;&#19968;&#31181;&#26032;&#30340;&#25915;&#20987;&#21521;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#36890;&#24120;&#20551;&#23450;&#25915;&#20987;&#32773;&#21487;&#20197;&#39640;&#24230;&#29305;&#26435;&#22320;&#35775;&#38382;&#21463;&#23475;&#32773;&#30340;&#21442;&#25968;&#12289;&#29615;&#22659;&#25110;&#25968;&#25454;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#24265;&#20215;&#20132;&#27969;MDP&#30340;&#26032;&#22411;&#23545;&#25239;&#24615;&#35774;&#32622;&#65292;&#20854;&#20013;&#23545;&#25163;&#21482;&#33021;&#23558;&#30830;&#23450;&#24615;&#20449;&#24687;&#38468;&#21152;&#21040;&#21463;&#23475;&#32773;&#30340;&#35266;&#23519;&#20013;&#65292;&#20174;&#32780;&#20135;&#29983;&#26368;&#23567;&#30340;&#24433;&#21709;&#33539;&#22260;&#12290;&#23545;&#25163;&#19981;&#33021;&#25513;&#30422;&#22320;&#38754;&#20107;&#23454;&#65292;&#24433;&#21709;&#22522;&#26412;&#29615;&#22659;&#21160;&#24577;&#25110;&#22870;&#21169;&#20449;&#21495;&#65292;&#24341;&#20837;&#19981;&#31283;&#23450;&#24615;&#65292;&#22686;&#21152;&#38543;&#26426;&#24615;&#65292;&#30475;&#21040;&#21463;&#23475;&#32773;&#30340;&#21160;&#20316;&#25110;&#35775;&#38382;&#20182;&#20204;&#30340;&#21442;&#25968;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#20803;&#23398;&#20064;&#31639;&#27861;&#65292;&#31216;&#20026;&#23545;&#25239;&#24615;&#24265;&#20215;&#20132;&#27969;&#65288;ACT&#65289;&#65292;&#22312;&#36825;&#31181;&#35774;&#32622;&#20013;&#23545;&#23545;&#25163;&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#21363;&#20351;&#22312;&#39640;&#24230;&#21463;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;ACT&#35757;&#32451;&#30340;&#23545;&#25163;&#20173;&#20250;&#26174;&#30528;&#24433;&#21709;&#21463;&#23475;&#32773;&#30340;&#35757;&#32451;&#21644;&#27979;&#35797;&#34920;&#29616;&#12290;&#24433;&#21709;&#35757;&#32451;&#26102;&#38388;&#34920;&#29616;&#25581;&#31034;&#20102;&#19968;&#31181;&#26032;&#30340;&#25915;&#20987;&#21521;&#37327;&#65292;&#24182;&#20026;&#29616;&#26377;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#25104;&#21151;&#21644;&#22833;&#36133;&#27169;&#24335;&#25552;&#20379;&#20102;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial attacks in reinforcement learning (RL) often assume highly-privileged access to the victim's parameters, environment, or data. Instead, this paper proposes a novel adversarial setting called a Cheap Talk MDP in which an Adversary can merely append deterministic messages to the Victim's observation, resulting in a minimal range of influence. The Adversary cannot occlude ground truth, influence underlying environment dynamics or reward signals, introduce non-stationarity, add stochasticity, see the Victim's actions, or access their parameters. Additionally, we present a simple meta-learning algorithm called Adversarial Cheap Talk (ACT) to train Adversaries in this setting. We demonstrate that an Adversary trained with ACT still significantly influences the Victim's training and testing performance, despite the highly constrained setting. Affecting train-time performance reveals a new attack vector and provides insight into the success and failure modes of existing RL algorith
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#29992;&#20154;&#24037;&#34562;&#32676;&#20248;&#21270;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#26694;&#26550;HiveNAS&#65292;&#23427;&#22312;&#26497;&#30701;&#30340;&#26102;&#38388;&#20869;&#23601;&#33021;&#36229;&#36234;&#20854;&#20182;&#22522;&#20110;&#32676;&#26234;&#30340;NAS&#26694;&#26550;&#65292;&#25104;&#20026;&#26368;&#20248;&#12290;</title><link>http://arxiv.org/abs/2211.10250</link><description>&lt;p&gt;
HiveNAS: &#37319;&#29992;&#20154;&#24037;&#34562;&#32676;&#20248;&#21270;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
HiveNAS: Neural Architecture Search using Artificial Bee Colony Optimization. (arXiv:2211.10250v2 [cs.NE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.10250
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#29992;&#20154;&#24037;&#34562;&#32676;&#20248;&#21270;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#26694;&#26550;HiveNAS&#65292;&#23427;&#22312;&#26497;&#30701;&#30340;&#26102;&#38388;&#20869;&#23601;&#33021;&#36229;&#36234;&#20854;&#20182;&#22522;&#20110;&#32676;&#26234;&#30340;NAS&#26694;&#26550;&#65292;&#25104;&#20026;&#26368;&#20248;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#31070;&#32463;&#32593;&#32476;&#24320;&#21457;&#36807;&#31243;&#38656;&#35201;&#22823;&#37327;&#19987;&#19994;&#30693;&#35782;&#65292;&#24182;&#19988;&#20005;&#37325;&#20381;&#36182;&#20110;&#30452;&#35273;&#21644;&#35797;&#38169;&#12290;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65288;NAS&#65289;&#26694;&#26550;&#34987;&#24341;&#20837;&#26469;&#31283;&#20581;&#22320;&#25628;&#32034;&#32593;&#32476;&#25299;&#25169;&#65292;&#24182;&#20419;&#36827;&#31070;&#32463;&#32593;&#32476;&#30340;&#33258;&#21160;&#24320;&#21457;&#12290;&#22312;NAS&#19978;&#65292;&#34429;&#28982;&#19968;&#20123;&#20248;&#21270;&#26041;&#27861;&#65288;&#22914;&#36951;&#20256;&#31639;&#27861;&#65289;&#24050;&#32463;&#34987;&#24191;&#27867;&#25506;&#31350;&#65292;&#20294;&#20854;&#20182;&#20803;&#21551;&#21457;&#24335;&#20248;&#21270;&#31639;&#27861;&#23578;&#26410;&#34987;&#30740;&#31350;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#37319;&#29992;&#20154;&#24037;&#34562;&#32676;&#20248;&#21270;&#36827;&#34892;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#30340;&#21487;&#34892;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;HiveNAS&#22312;&#19968;&#23567;&#37096;&#20998;&#26102;&#38388;&#20869;&#23601;&#36229;&#36234;&#20102;&#29616;&#26377;&#30340;&#22522;&#20110;&#32676;&#26234;&#30340;NAS&#26694;&#26550;&#65292;&#25104;&#20026;&#26368;&#20248;&#12290;
&lt;/p&gt;
&lt;p&gt;
The traditional Neural Network-development process requires substantial expert knowledge and relies heavily on intuition and trial-and-error. Neural Architecture Search (NAS) frameworks were introduced to robustly search for network topologies, as well as facilitate the automated development of Neural Networks. While some optimization approaches -- such as Genetic Algorithms -have been extensively explored in the NAS context, other Metaheuristic Optimization algorithms have not yet been investigated. In this study, we evaluate the viability of Artificial Bee Colony optimization for Neural Architecture Search. Our proposed framework, HiveNAS, outperforms existing state-of-the-art Swarm Intelligence-based NAS frameworks in a fraction of the time.
&lt;/p&gt;</description></item><item><title>ATCO2&#35821;&#26009;&#24211;&#26159;&#19968;&#20010;&#26088;&#22312;&#20419;&#36827;&#31354;&#20013;&#20132;&#36890;&#31649;&#21046;&#36890;&#20449;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#21644;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#30740;&#31350;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#25968;&#25454;&#25910;&#38598;&#21644;&#39044;&#22788;&#29702;&#12289;&#35821;&#38899;&#25968;&#25454;&#30340;&#20266;&#27880;&#37322;&#65292;&#20197;&#21450;&#25552;&#21462;&#19982;&#31354;&#20013;&#20132;&#36890;&#31649;&#21046;&#30456;&#20851;&#30340;&#21629;&#21517;&#23454;&#20307;&#12290;</title><link>http://arxiv.org/abs/2211.04054</link><description>&lt;p&gt;
ATCO2&#35821;&#26009;&#24211;&#65306;&#29992;&#20110;&#31354;&#20013;&#20132;&#36890;&#31649;&#21046;&#36890;&#20449;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#21644;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#30740;&#31350;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
ATCO2 corpus: A Large-Scale Dataset for Research on Automatic Speech Recognition and Natural Language Understanding of Air Traffic Control Communications. (arXiv:2211.04054v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.04054
&lt;/p&gt;
&lt;p&gt;
ATCO2&#35821;&#26009;&#24211;&#26159;&#19968;&#20010;&#26088;&#22312;&#20419;&#36827;&#31354;&#20013;&#20132;&#36890;&#31649;&#21046;&#36890;&#20449;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#21644;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#30740;&#31350;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#25968;&#25454;&#25910;&#38598;&#21644;&#39044;&#22788;&#29702;&#12289;&#35821;&#38899;&#25968;&#25454;&#30340;&#20266;&#27880;&#37322;&#65292;&#20197;&#21450;&#25552;&#21462;&#19982;&#31354;&#20013;&#20132;&#36890;&#31649;&#21046;&#30456;&#20851;&#30340;&#21629;&#21517;&#23454;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20114;&#32852;&#30340;&#25968;&#23383;&#19990;&#30028;&#20013;&#65292;&#20010;&#20154;&#21161;&#25163;&#12289;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#22120;&#21644;&#23545;&#35805;&#29702;&#35299;&#31995;&#32479;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#33322;&#31354;&#20132;&#36890;&#31649;&#21046;&#36890;&#20449;&#23601;&#26159;&#20854;&#20013;&#19968;&#20010;&#26126;&#26174;&#30340;&#20363;&#23376;&#12290;&#31354;&#20013;&#20132;&#36890;&#31649;&#21046;&#26088;&#22312;&#20197;&#23433;&#20840;&#21644;&#26368;&#20248;&#30340;&#26041;&#24335;&#25351;&#23548;&#39134;&#34892;&#22120;&#21644;&#25511;&#21046;&#31354;&#22495;&#12290;&#36825;&#20123;&#22522;&#20110;&#35821;&#38899;&#30340;&#23545;&#35805;&#26159;&#36890;&#36807;&#36229;&#39640;&#39057;&#26080;&#32447;&#30005;&#39057;&#36947;&#22312;&#31354;&#20013;&#20132;&#36890;&#31649;&#21046;&#21592;&#65288;ATCO&#65289;&#21644;&#39134;&#34892;&#21592;&#20043;&#38388;&#36827;&#34892;&#30340;&#12290;&#20026;&#20102;&#23558;&#36825;&#20123;&#26032;&#25216;&#26415;&#32435;&#20837;ATC&#65288;&#20302;&#36164;&#28304;&#39046;&#22495;&#65289;&#65292;&#38656;&#35201;&#22823;&#35268;&#27169;&#30340;&#27880;&#37322;&#25968;&#25454;&#38598;&#26469;&#24320;&#21457;&#25968;&#25454;&#39537;&#21160;&#30340;AI&#31995;&#32479;&#65292;&#20854;&#20013;&#21253;&#25324;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#21644;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#65288;NLU&#65289;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;ATCO2&#35821;&#26009;&#24211;&#65292;&#36825;&#26159;&#19968;&#20010;&#26088;&#22312;&#20419;&#36827;ATC&#39046;&#22495;&#30740;&#31350;&#30340;&#25968;&#25454;&#38598;&#65292;&#30001;&#20110;&#32570;&#20047;&#27880;&#37322;&#30340;&#25968;&#25454;&#65292;&#35813;&#39046;&#22495;&#28382;&#21518;&#24456;&#22810;&#12290;ATCO2&#35821;&#26009;&#24211;&#21253;&#25324;1&#65289;&#25968;&#25454;&#25910;&#38598;&#21644;&#39044;&#22788;&#29702;&#65292;2&#65289;&#35821;&#38899;&#25968;&#25454;&#30340;&#20266;&#27880;&#37322;&#65292;&#21644;3&#65289;&#25552;&#21462;&#19982;&#31354;&#20013;&#20132;&#36890;&#31649;&#21046;&#30456;&#20851;&#30340;&#21629;&#21517;&#23454;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;
Personal assistants, automatic speech recognizers and dialogue understanding systems are becoming more critical in our interconnected digital world. A clear example is air traffic control (ATC) communications. ATC aims at guiding aircraft and controlling the airspace in a safe and optimal manner. These voice-based dialogues are carried between an air traffic controller (ATCO) and pilots via very-high frequency radio channels. In order to incorporate these novel technologies into ATC (low-resource domain), large-scale annotated datasets are required to develop the data-driven AI systems. Two examples are automatic speech recognition (ASR) and natural language understanding (NLU). In this paper, we introduce the ATCO2 corpus, a dataset that aims at fostering research on the challenging ATC field, which has lagged behind due to lack of annotated data. The ATCO2 corpus covers 1) data collection and pre-processing, 2) pseudo-annotations of speech data, and 3) extraction of ATC-related named
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24179;&#28369;&#30772;&#30862;&#30340;&#24130;&#24459;&#20989;&#25968;&#24418;&#24335;&#65292;&#21487;&#20197;&#20934;&#30830;&#22320;&#27169;&#25311;&#21644;&#22806;&#25512;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#32553;&#25918;&#34892;&#20026;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#26550;&#26500;&#21644;&#22823;&#37327;&#19981;&#21516;&#20219;&#21153;&#65292;&#21253;&#25324;&#35270;&#35273;&#12289;&#35821;&#35328;&#12289;&#38899;&#39057;&#12289;&#35270;&#39057;&#12289;&#29983;&#25104;&#24314;&#27169;&#12289;&#23545;&#27604;&#23398;&#20064;&#12289;&#26426;&#22120;&#20154;&#12289;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;/&#26657;&#20934;&#12289;&#23545;&#25239;&#40065;&#26834;&#24615;&#12289;&#20998;&#23376;&#12289;&#35745;&#31639;&#26426;&#32534;&#31243;/&#32534;&#30721;&#12289;&#25968;&#23398;&#21333;&#35789;&#38382;&#39064;&#12289;&#31639;&#26415;&#12289;&#26080;&#30417;&#30563;/&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2210.14891</link><description>&lt;p&gt;
&#30772;&#30862;&#30340;&#31070;&#32463;&#32553;&#25918;&#23450;&#24459;
&lt;/p&gt;
&lt;p&gt;
Broken Neural Scaling Laws. (arXiv:2210.14891v7 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.14891
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24179;&#28369;&#30772;&#30862;&#30340;&#24130;&#24459;&#20989;&#25968;&#24418;&#24335;&#65292;&#21487;&#20197;&#20934;&#30830;&#22320;&#27169;&#25311;&#21644;&#22806;&#25512;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#32553;&#25918;&#34892;&#20026;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#26550;&#26500;&#21644;&#22823;&#37327;&#19981;&#21516;&#20219;&#21153;&#65292;&#21253;&#25324;&#35270;&#35273;&#12289;&#35821;&#35328;&#12289;&#38899;&#39057;&#12289;&#35270;&#39057;&#12289;&#29983;&#25104;&#24314;&#27169;&#12289;&#23545;&#27604;&#23398;&#20064;&#12289;&#26426;&#22120;&#20154;&#12289;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;/&#26657;&#20934;&#12289;&#23545;&#25239;&#40065;&#26834;&#24615;&#12289;&#20998;&#23376;&#12289;&#35745;&#31639;&#26426;&#32534;&#31243;/&#32534;&#30721;&#12289;&#25968;&#23398;&#21333;&#35789;&#38382;&#39064;&#12289;&#31639;&#26415;&#12289;&#26080;&#30417;&#30563;/&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a smoothly broken power law functional form (referred to as a Broken Neural Scaling Law (BNSL)) that accurately models and extrapolates the scaling behaviors of deep neural networks for various architectures and a large and diverse set of tasks, including vision, language, audio, video, generative modeling, contrastive learning, robotics, uncertainty estimation/calibration, adversarial robustness, molecules, computer programming/coding, math word problems, arithmetic, unsupervised/self-supervised learning, and reinforcement learning.
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24179;&#28369;&#30772;&#30862;&#30340;&#24130;&#24459;&#20989;&#25968;&#24418;&#24335;&#65288;&#25105;&#20204;&#31216;&#20043;&#20026;&#30772;&#30862;&#30340;&#31070;&#32463;&#32553;&#25918;&#23450;&#24459;&#65288;BNSL&#65289;&#65289;&#65292;&#23427;&#20934;&#30830;&#22320;&#27169;&#25311;&#21644;&#22806;&#25512;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#32553;&#25918;&#34892;&#20026;&#65288;&#21363;&#24863;&#20852;&#36259;&#30340;&#35780;&#20272;&#25351;&#26631;&#38543;&#29992;&#20110;&#35757;&#32451;&#30340;&#35745;&#31639;&#37327;&#12289;&#27169;&#22411;&#21442;&#25968;&#25968;&#37327;&#12289;&#35757;&#32451;&#25968;&#25454;&#38598;&#22823;&#23567;&#25110;&#19978;&#28216;&#24615;&#33021;&#21464;&#21270;&#32780;&#21464;&#21270;&#65289;&#23545;&#20110;&#21508;&#31181;&#26550;&#26500;&#21644;&#22823;&#37327;&#19981;&#21516;&#20219;&#21153;&#20013;&#30340;&#27599;&#20010;&#20219;&#21153;&#65292;&#21253;&#25324;&#22823;&#35268;&#27169;&#35270;&#35273;&#12289;&#35821;&#35328;&#12289;&#38899;&#39057;&#12289;&#35270;&#39057;&#12289;&#25193;&#25955;&#12289;&#29983;&#25104;&#24314;&#27169;&#12289;&#22810;&#27169;&#24577;&#23398;&#20064;&#12289;&#23545;&#27604;&#23398;&#20064;&#12289;AI&#23545;&#40784;&#12289;&#26426;&#22120;&#20154;&#12289;&#36229;&#20986;&#20998;&#24067;&#65288;OOD&#65289;&#27867;&#21270;&#12289;&#25345;&#32493;&#23398;&#20064;&#12289;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;/&#26657;&#20934;&#12289;&#36229;&#20986;&#20998;&#24067;&#26816;&#27979;&#12289;&#23545;&#25239;&#40065;&#26834;&#24615;&#12289;&#33976;&#39311;&#12289;&#20998;&#23376;&#12289;&#35745;&#31639;&#26426;&#32534;&#31243;/&#32534;&#30721;&#12289;&#25968;&#23398;&#21333;&#35789;&#38382;&#39064;&#12289;&#31639;&#26415;&#12289;&#26080;&#30417;&#30563;/&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a smoothly broken power law functional form (referred to by us as a Broken Neural Scaling Law (BNSL)) that accurately models and extrapolates the scaling behaviors of deep neural networks (i.e. how the evaluation metric of interest varies as the amount of compute used for training, number of model parameters, training dataset size, or upstream performance varies) for various architectures and for each of various tasks within a large and diverse set of upstream and downstream tasks, in zero-shot, prompted, and fine-tuned settings. This set includes large-scale vision, language, audio, video, diffusion, generative modeling, multimodal learning, contrastive learning, AI alignment, robotics, out-of-distribution (OOD) generalization, continual learning, uncertainty estimation / calibration, out-of-distribution detection, adversarial robustness, distillation, molecules, computer programming/coding, math word problems, arithmetic, unsupervised/self-supervised learning, and reinforc
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#33539;&#24335;&#8212;&#8212;&#22522;&#20110;&#25552;&#31034;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65288;PromptCast&#65289;&#65292;&#23558;&#25968;&#23383;&#36755;&#20837;&#21644;&#36755;&#20986;&#36716;&#21270;&#20026;&#25552;&#31034;&#65292;&#24182;&#20197;&#21477;&#23376;&#21040;&#21477;&#23376;&#30340;&#26041;&#24335;&#25552;&#20986;&#39044;&#27979;&#20219;&#21153;&#65292;&#21487;&#20197;&#30452;&#25509;&#24212;&#29992;&#20110;&#35821;&#35328;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2210.08964</link><description>&lt;p&gt;
PromptCast&#65306;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#25552;&#31034;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#33539;&#24335;
&lt;/p&gt;
&lt;p&gt;
PromptCast: A New Prompt-based Learning Paradigm for Time Series Forecasting. (arXiv:2210.08964v3 [stat.ME] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.08964
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#33539;&#24335;&#8212;&#8212;&#22522;&#20110;&#25552;&#31034;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65288;PromptCast&#65289;&#65292;&#23558;&#25968;&#23383;&#36755;&#20837;&#21644;&#36755;&#20986;&#36716;&#21270;&#20026;&#25552;&#31034;&#65292;&#24182;&#20197;&#21477;&#23376;&#21040;&#21477;&#23376;&#30340;&#26041;&#24335;&#25552;&#20986;&#39044;&#27979;&#20219;&#21153;&#65292;&#21487;&#20197;&#30452;&#25509;&#24212;&#29992;&#20110;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#33539;&#24335;&#8212;&#8212;&#22522;&#20110;&#25552;&#31034;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65288;PromptCast&#65289;&#12290;&#22312;&#36825;&#31181;&#26032;&#30340;&#20219;&#21153;&#20013;&#65292;&#23558;&#21407;&#26469;&#30340;&#25968;&#23383;&#36755;&#20837;&#21644;&#36755;&#20986;&#36716;&#21270;&#20026;&#25552;&#31034;&#65292;&#24182;&#20197;&#21477;&#23376;&#21040;&#21477;&#23376;&#30340;&#26041;&#24335;&#25552;&#20986;&#39044;&#27979;&#20219;&#21153;&#65292;&#20351;&#24471;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#30452;&#25509;&#24212;&#29992;&#20110;&#39044;&#27979;&#30340;&#30446;&#30340;&#12290;&#20026;&#20102;&#25903;&#25345;&#21644;&#20419;&#36827;&#36825;&#20010;&#20219;&#21153;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#25968;&#25454;&#38598;&#65288;PISA&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a new perspective on time series forecasting. In existing time series forecasting methods, the models take a sequence of numerical values as input and yield numerical values as output. The existing SOTA models are largely based on the Transformer architecture, modified with multiple encoding mechanisms to incorporate the context and semantics around the historical data. Inspired by the successes of pre-trained language foundation models, we pose a question about whether these models can also be adapted to solve time-series forecasting. Thus, we propose a new forecasting paradigm: prompt-based time series forecasting (PromptCast). In this novel task, the numerical input and output are transformed into prompts and the forecasting task is framed in a sentence-to-sentence manner, making it possible to directly apply language models for forecasting purposes. To support and facilitate the research of this task, we also present a large-scale dataset (PISA) that includes th
&lt;/p&gt;</description></item><item><title>CORL&#26159;&#19968;&#20010;&#38754;&#21521;&#30740;&#31350;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31163;&#32447;&#24211;&#65292;&#25552;&#20379;&#20102;&#32463;&#36807;&#20805;&#20998;&#22522;&#20934;&#27979;&#35797;&#30340;&#21333;&#25991;&#20214;&#23454;&#29616;&#31163;&#32447;&#21644;&#31163;&#32447;&#21040;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#20855;&#26377;&#31616;&#21333;&#30340;&#24320;&#21457;&#20307;&#39564;&#21644;&#23454;&#39564;&#36319;&#36394;&#21151;&#33021;&#12290;</title><link>http://arxiv.org/abs/2210.07105</link><description>&lt;p&gt;
CORL: &#38754;&#21521;&#30740;&#31350;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31163;&#32447;&#24211;
&lt;/p&gt;
&lt;p&gt;
CORL: Research-oriented Deep Offline Reinforcement Learning Library. (arXiv:2210.07105v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.07105
&lt;/p&gt;
&lt;p&gt;
CORL&#26159;&#19968;&#20010;&#38754;&#21521;&#30740;&#31350;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31163;&#32447;&#24211;&#65292;&#25552;&#20379;&#20102;&#32463;&#36807;&#20805;&#20998;&#22522;&#20934;&#27979;&#35797;&#30340;&#21333;&#25991;&#20214;&#23454;&#29616;&#31163;&#32447;&#21644;&#31163;&#32447;&#21040;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#20855;&#26377;&#31616;&#21333;&#30340;&#24320;&#21457;&#20307;&#39564;&#21644;&#23454;&#39564;&#36319;&#36394;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
CORL&#26159;&#19968;&#20010;&#24320;&#28304;&#24211;&#65292;&#25552;&#20379;&#20102;&#32463;&#36807;&#20805;&#20998;&#22522;&#20934;&#27979;&#35797;&#30340;&#21333;&#25991;&#20214;&#23454;&#29616;&#28145;&#24230;&#31163;&#32447;&#21644;&#31163;&#32447;&#21040;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#12290;&#23427;&#24378;&#35843;&#31616;&#21333;&#30340;&#24320;&#21457;&#20307;&#39564;&#65292;&#20855;&#26377;&#30452;&#35266;&#30340;&#20195;&#30721;&#24211;&#21644;&#29616;&#20195;&#20998;&#26512;&#36319;&#36394;&#24037;&#20855;&#12290;&#22312;CORL&#20013;&#65292;&#25105;&#20204;&#23558;&#26041;&#27861;&#23454;&#29616;&#38548;&#31163;&#21040;&#21333;&#29420;&#30340;&#21333;&#20010;&#25991;&#20214;&#20013;&#65292;&#20351;&#24615;&#33021;&#30456;&#20851;&#30340;&#32454;&#33410;&#26356;&#23481;&#26131;&#35782;&#21035;&#12290;&#27492;&#22806;&#65292;&#23454;&#39564;&#36319;&#36394;&#21151;&#33021;&#21487;&#29992;&#20110;&#24110;&#21161;&#35760;&#24405;&#25351;&#26631;&#12289;&#36229;&#21442;&#25968;&#12289;&#20381;&#36182;&#39033;&#31561;&#21040;&#20113;&#31471;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#22522;&#20934;&#27979;&#35797;&#24120;&#29992;&#30340;D4RL&#25968;&#25454;&#38598;&#65292;&#30830;&#20445;&#20102;&#23454;&#29616;&#30340;&#21487;&#38752;&#24615;&#65292;&#25552;&#20379;&#20102;&#36879;&#26126;&#30340;&#32467;&#26524;&#28304;&#65292;&#21487;&#29992;&#20110;&#24378;&#22823;&#30340;&#35780;&#20272;&#24037;&#20855;&#65292;&#20363;&#22914;&#24615;&#33021;&#27010;&#35201;&#12289;&#25913;&#36827;&#27010;&#29575;&#25110;&#39044;&#26399;&#22312;&#32447;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
CORL is an open-source library that provides thoroughly benchmarked single-file implementations of both deep offline and offline-to-online reinforcement learning algorithms. It emphasizes a simple developing experience with a straightforward codebase and a modern analysis tracking tool. In CORL, we isolate methods implementation into separate single files, making performance-relevant details easier to recognize. Additionally, an experiment tracking feature is available to help log metrics, hyperparameters, dependencies, and more to the cloud. Finally, we have ensured the reliability of the implementations by benchmarking commonly employed D4RL datasets providing a transparent source of results that can be reused for robust evaluation tools such as performance profiles, probability of improvement, or expected online performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;FP-Diffusion&#26041;&#27861;&#26469;&#25913;&#36827;&#22522;&#20110;&#24471;&#20998;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#24378;&#21046;&#24213;&#23618;&#24471;&#20998;&#30340;&#31119;&#20811;-&#26222;&#26391;&#20811;&#26041;&#31243;&#26469;&#27491;&#21017;&#21270;DSM&#30446;&#26631;&#20989;&#25968;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#20284;&#28982;&#24230;&#21644;&#23432;&#24658;&#31243;&#24230;&#12290;</title><link>http://arxiv.org/abs/2210.04296</link><description>&lt;p&gt;
FP-Diffusion: &#36890;&#36807;&#24378;&#21046;&#24213;&#23618;&#24471;&#20998;&#30340;&#31119;&#20811;-&#26222;&#26391;&#20811;&#26041;&#31243;&#26469;&#25913;&#36827;&#22522;&#20110;&#24471;&#20998;&#30340;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
FP-Diffusion: Improving Score-based Diffusion Models by Enforcing the Underlying Score Fokker-Planck Equation. (arXiv:2210.04296v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.04296
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;FP-Diffusion&#26041;&#27861;&#26469;&#25913;&#36827;&#22522;&#20110;&#24471;&#20998;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#24378;&#21046;&#24213;&#23618;&#24471;&#20998;&#30340;&#31119;&#20811;-&#26222;&#26391;&#20811;&#26041;&#31243;&#26469;&#27491;&#21017;&#21270;DSM&#30446;&#26631;&#20989;&#25968;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#20284;&#28982;&#24230;&#21644;&#23432;&#24658;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#24471;&#20998;&#30340;&#29983;&#25104;&#27169;&#22411;&#65288;SGM&#65289;&#23398;&#20064;&#19968;&#32452;&#19982;&#25968;&#25454;&#23494;&#24230;&#30456;&#23545;&#24212;&#30340;&#12289;&#22122;&#22768;&#26465;&#20214;&#24471;&#20998;&#20989;&#25968;&#12290;&#36825;&#20123;&#25200;&#21160;&#30340;&#25968;&#25454;&#23494;&#24230;&#36890;&#36807;&#31119;&#20811;-&#26222;&#26391;&#20811;&#26041;&#31243;&#65288;FPE&#65289;&#30456;&#20114;&#32852;&#31995;&#65292;&#35813;&#26041;&#31243;&#26159;&#19968;&#31181;&#25551;&#36848;&#29289;&#36136;&#25193;&#25955;&#36807;&#31243;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25512;&#23548;&#20102;&#19968;&#20010;&#23545;&#24212;&#30340;&#26041;&#31243;&#65292;&#31216;&#20026;&#24471;&#20998;FPE&#65292;&#20854;&#29305;&#24449;&#26159;&#25200;&#21160;&#30340;&#25968;&#25454;&#23494;&#24230;&#65288;&#21363;&#23427;&#20204;&#30340;&#26799;&#24230;&#65289;&#30340;&#22122;&#22768;&#26465;&#20214;&#24471;&#20998;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#34429;&#28982;DSM&#24471;&#20998;&#23398;&#20064;&#26041;&#27861;&#20855;&#26377;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#23454;&#35777;&#24615;&#33021;&#65292;&#20294;&#25105;&#20204;&#35266;&#23519;&#21040;&#23398;&#20064;&#21040;&#30340;&#24471;&#20998;&#26410;&#33021;&#28385;&#36275;&#24213;&#23618;&#30340;&#24471;&#20998;FPE&#65292;&#36825;&#26159;&#22320;&#38754;&#30495;&#23454;&#24471;&#20998;&#30340;&#22266;&#26377;&#33258;&#19968;&#33268;&#24615;&#23646;&#24615;&#12290;&#25105;&#20204;&#35777;&#26126;&#28385;&#36275;&#24471;&#20998;FPE&#26159;&#21487;&#21462;&#30340;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#25552;&#39640;&#20284;&#28982;&#24230;&#21644;&#23432;&#24658;&#31243;&#24230;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#23545;DSM&#30446;&#26631;&#36827;&#34892;&#27491;&#21017;&#21270;&#65292;&#20197;&#24378;&#21046;&#28385;&#36275;&#20998;&#25968;FPE&#30340;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
Score-based generative models (SGMs) learn a family of noise-conditional score functions corresponding to the data density perturbed with increasingly large amounts of noise. These perturbed data densities are linked together by the Fokker-Planck equation (FPE), a partial differential equation (PDE) governing the spatial-temporal evolution of a density undergoing a diffusion process. In this work, we derive a corresponding equation called the score FPE that characterizes the noise-conditional scores of the perturbed data densities (i.e., their gradients). Surprisingly, despite the impressive empirical performance, we observe that scores learned through denoising score matching (DSM) fail to fulfill the underlying score FPE, which is an inherent self-consistency property of the ground truth score. We prove that satisfying the score FPE is desirable as it improves the likelihood and the degree of conservativity. Hence, we propose to regularize the DSM objective to enforce satisfaction of
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#23610;&#24230;&#25299;&#25169;&#22855;&#24322;&#24615;&#26816;&#27979;&#26041;&#27861;&#65292;&#21487;&#20197;&#35780;&#20272;&#25968;&#25454;&#30340;&#23616;&#37096;&#22266;&#26377;&#32500;&#24230;&#65292;&#24182;&#37327;&#21270;&#28857;&#30340;&#8220;&#27969;&#24418;&#24230;&#8221;&#65292;&#33021;&#22815;&#26816;&#27979;&#22797;&#26434;&#31354;&#38388;&#21644;&#22270;&#20687;&#20013;&#30340;&#22855;&#24322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2210.00069</link><description>&lt;p&gt;
&#22810;&#23610;&#24230;&#25299;&#25169;&#22855;&#24322;&#24615;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Topological Singularity Detection at Multiple Scales. (arXiv:2210.00069v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.00069
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#23610;&#24230;&#25299;&#25169;&#22855;&#24322;&#24615;&#26816;&#27979;&#26041;&#27861;&#65292;&#21487;&#20197;&#35780;&#20272;&#25968;&#25454;&#30340;&#23616;&#37096;&#22266;&#26377;&#32500;&#24230;&#65292;&#24182;&#37327;&#21270;&#28857;&#30340;&#8220;&#27969;&#24418;&#24230;&#8221;&#65292;&#33021;&#22815;&#26816;&#27979;&#22797;&#26434;&#31354;&#38388;&#21644;&#22270;&#20687;&#20013;&#30340;&#22855;&#24322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27969;&#24418;&#20551;&#35774;&#26159;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#30340;&#19968;&#20010;&#22522;&#26412;&#20551;&#35774;&#65292;&#23427;&#20551;&#23450;&#25968;&#25454;&#20301;&#20110;&#25110;&#25509;&#36817;&#20110;&#20302;&#22266;&#26377;&#32500;&#24230;&#30340;&#26410;&#30693;&#27969;&#24418;&#19978;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#29616;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#34920;&#29616;&#20986;&#26126;&#26174;&#30340;&#38750;&#27969;&#24418;&#32467;&#26500;&#65292;&#21363;&#22855;&#24322;&#24615;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#38169;&#35823;&#30340;&#21457;&#29616;&#12290;&#22240;&#27492;&#65292;&#26816;&#27979;&#36825;&#31181;&#22855;&#24322;&#24615;&#22312;&#25554;&#20540;&#21644;&#25512;&#26029;&#20219;&#21153;&#20043;&#21069;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#25105;&#20204;&#36890;&#36807;&#24320;&#21457;&#19968;&#20010;&#25299;&#25169;&#26694;&#26550;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#65288;i&#65289;&#37327;&#21270;&#23616;&#37096;&#22266;&#26377;&#32500;&#24230;&#65292;&#20197;&#21450;&#65288;ii&#65289;&#22312;&#22810;&#20010;&#23610;&#24230;&#19978;&#20135;&#29983;&#8220;&#27431;&#20960;&#37324;&#24471;&#24615;&#8221;&#35780;&#20998;&#65292;&#29992;&#20197;&#35780;&#20272;&#28857;&#30340;&#8220;&#27969;&#24418;&#24230;&#8221;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#22270;&#20687;&#25968;&#25454;&#20013;&#25429;&#33719;&#22797;&#26434;&#31354;&#38388;&#30340;&#22855;&#24322;&#24615;&#65292;&#21516;&#26102;&#25429;&#25417;&#22855;&#24322;&#32467;&#26500;&#21644;&#23616;&#37096;&#20960;&#20309;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The manifold hypothesis, which assumes that data lies on or close to an unknown manifold of low intrinsic dimension, is a staple of modern machine learning research. However, recent work has shown that real-world data exhibits distinct non-manifold structures, i.e. singularities, that can lead to erroneous findings. Detecting such singularities is therefore crucial as a precursor to interpolation and inference tasks. We address this issue by developing a topological framework that (i) quantifies the local intrinsic dimension, and (ii) yields a Euclidicity score for assessing the 'manifoldness' of a point along multiple scales. Our approach identifies singularities of complex spaces, while also capturing singular structures and local geometric complexity in image data.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22238;&#39038;&#24182;&#27604;&#36739;&#20102;&#24120;&#29992;&#20110;&#24230;&#37327;&#20998;&#31867;&#31995;&#32479;&#34920;&#29616;&#30340;&#21508;&#31181;&#25351;&#26631;&#65292;&#21457;&#29616;&#26399;&#26395;&#25104;&#26412;&#25351;&#26631;&#20855;&#26377;&#26356;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#21644;&#30452;&#35266;&#24615;&#65292;&#24182;&#21487;&#29992;&#20110;&#35299;&#20915;&#20174;&#36830;&#32493;&#24471;&#20998;&#29983;&#25104;&#20998;&#31867;&#20915;&#31574;&#30340;&#23454;&#36341;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2209.05355</link><description>&lt;p&gt;
&#20998;&#31867;&#25351;&#26631;&#30340;&#20998;&#26512;&#19982;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
Analysis and Comparison of Classification Metrics. (arXiv:2209.05355v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.05355
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22238;&#39038;&#24182;&#27604;&#36739;&#20102;&#24120;&#29992;&#20110;&#24230;&#37327;&#20998;&#31867;&#31995;&#32479;&#34920;&#29616;&#30340;&#21508;&#31181;&#25351;&#26631;&#65292;&#21457;&#29616;&#26399;&#26395;&#25104;&#26412;&#25351;&#26631;&#20855;&#26377;&#26356;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#21644;&#30452;&#35266;&#24615;&#65292;&#24182;&#21487;&#29992;&#20110;&#35299;&#20915;&#20174;&#36830;&#32493;&#24471;&#20998;&#29983;&#25104;&#20998;&#31867;&#20915;&#31574;&#30340;&#23454;&#36341;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#65292;&#24120;&#29992;&#21508;&#31181;&#24615;&#33021;&#25351;&#26631;&#26469;&#35780;&#20272;&#20998;&#31867;&#31995;&#32479;&#30340;&#34920;&#29616;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20123;&#26368;&#24120;&#29992;&#30340;&#29992;&#20110;&#34913;&#37327;&#30828;&#20915;&#31574;&#36136;&#37327;&#30340;&#26631;&#20934;&#21644;&#24179;&#34913;&#20934;&#30830;&#29575;&#12289;&#26631;&#20934;&#21644;&#24179;&#34913;&#38169;&#35823;&#29575;&#12289;F-beta&#20998;&#25968;&#21644;Matthews&#30456;&#20851;&#31995;&#25968;&#65288;MCC&#65289;&#31561;&#25351;&#26631;&#12290;&#25105;&#20204;&#22238;&#39038;&#20102;&#36825;&#20123;&#21644;&#20854;&#20182;&#25351;&#26631;&#30340;&#23450;&#20041;&#65292;&#24182;&#23558;&#23427;&#20204;&#19982;&#26399;&#26395;&#25104;&#26412;&#65288;EC&#65289;&#36827;&#34892;&#27604;&#36739;&#65292;&#21518;&#32773;&#26159;&#27599;&#20010;&#32479;&#35745;&#23398;&#20064;&#35838;&#31243;&#20013;&#37117;&#20171;&#32461;&#20294;&#22312;&#26426;&#22120;&#23398;&#20064;&#25991;&#29486;&#20013;&#24456;&#23569;&#20351;&#29992;&#30340;&#25351;&#26631;&#12290;&#25105;&#20204;&#34920;&#26126;&#26631;&#20934;&#21644;&#24179;&#34913;&#38169;&#35823;&#29575;&#37117;&#26159;EC&#30340;&#29305;&#27530;&#24773;&#20917;&#65292;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;EC&#19982;F&#20998;&#25968;&#21644;MCC&#30340;&#20851;&#31995;&#65292;&#24182;&#35748;&#20026;EC&#25351;&#26631;&#20248;&#20110;&#20256;&#32479;&#25351;&#26631;&#65292;&#22240;&#20854;&#26356;&#20855;&#26377;&#20248;&#38597;&#24615;&#12289;&#36890;&#29992;&#24615;&#21644;&#30452;&#35266;&#24615;&#65292;&#19988;&#22522;&#20110;&#32479;&#35745;&#23398;&#30340;&#22522;&#26412;&#21407;&#29702;&#12290;&#26412;&#25991;&#20013;&#20171;&#32461;&#30340;&#25351;&#26631;&#22343;&#29992;&#20110;&#24230;&#37327;&#30828;&#20915;&#31574;&#30340;&#36136;&#37327;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#20195;&#20998;&#31867;&#31995;&#32479;&#36755;&#20986;&#36830;&#32493;&#24471;&#20998;&#65292;&#32780;&#26377;&#19968;&#20010;&#37325;&#35201;&#30340;&#23454;&#36341;&#38382;&#39064;&#26159;&#22914;&#20309;&#20174;&#36825;&#20123;&#36830;&#32493;&#24471;&#20998;&#20013;&#29983;&#25104;&#20998;&#31867;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;
A variety of different performance metrics are commonly used in the machine learning literature for the evaluation of classification systems. Some of the most common ones for measuring quality of hard decisions are standard and balanced accuracy, standard and balanced error rate, F-beta score, and Matthews correlation coefficient (MCC). In this document, we review the definition of these and other metrics and compare them with the expected cost (EC), a metric introduced in every statistical learning course but rarely used in the machine learning literature. We show that both the standard and balanced error rates are special cases of the EC. Further, we show its relation with F-score and MCC and argue that EC is superior to these traditional metrics, being more elegant, general, and intuitive, as well as being based on basic principles from statistics.  The metrics above measure the quality of hard decisions. Yet, most modern classification systems output continuous scores for the class
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21333;&#38454;&#27573;&#24191;&#27867;&#22810;&#23454;&#20363;&#22810;&#26631;&#31614;&#23398;&#20064;&#65288;BMIML&#65289;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#22810;&#23454;&#20363;&#21644;&#22810;&#26631;&#31614;&#20043;&#38388;&#30340;&#21508;&#31181;&#20851;&#31995;&#23398;&#20064;&#38382;&#39064;&#12290;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#20013;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;</title><link>http://arxiv.org/abs/2209.02625</link><description>&lt;p&gt;
&#21333;&#38454;&#27573;&#24191;&#27867;&#22810;&#23454;&#20363;&#22810;&#26631;&#31614;&#23398;&#20064;&#65288;BMIML&#65289;&#21450;&#20854;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Single-Stage Broad Multi-Instance Multi-Label Learning (BMIML) with Diverse Inter-Correlations and its application to medical image classification. (arXiv:2209.02625v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.02625
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21333;&#38454;&#27573;&#24191;&#27867;&#22810;&#23454;&#20363;&#22810;&#26631;&#31614;&#23398;&#20064;&#65288;BMIML&#65289;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#22810;&#23454;&#20363;&#21644;&#22810;&#26631;&#31614;&#20043;&#38388;&#30340;&#21508;&#31181;&#20851;&#31995;&#23398;&#20064;&#38382;&#39064;&#12290;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#20013;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#22810;&#23454;&#20363;&#65288;&#20363;&#22914;&#22270;&#20687;&#34917;&#19969;&#65289;&#21644;&#22810;&#26631;&#31614;&#21516;&#26102;&#20851;&#32852;&#30340;&#38382;&#39064;&#12290;&#29616;&#26377;&#30340;&#22810;&#23454;&#20363;&#22810;&#26631;&#31614;&#65288;MIML&#65289;&#26041;&#27861;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#24456;&#26377;&#29992;&#65292;&#20294;&#22823;&#22810;&#25968;&#21463;&#21040;&#20960;&#20010;&#38382;&#39064;&#30340;&#38480;&#21046;&#65306;&#65288;i&#65289;&#24573;&#30053;&#20102;&#26631;&#31614;&#38388;&#30340;&#30456;&#20114;&#20851;&#31995;&#65288;&#21363;&#65292;&#19982;&#19968;&#20010;&#23545;&#35937;&#23545;&#24212;&#30340;&#22810;&#20010;&#26631;&#31614;&#20043;&#38388;&#30340;&#27010;&#29575;&#20851;&#31995;&#65289;&#65307;&#65288;ii&#65289;&#26080;&#27861;&#30452;&#25509;&#23398;&#20064;&#65288;&#25110;&#20849;&#21516;&#23398;&#20064;&#65289;&#19981;&#21516;&#23454;&#20363;&#22312;&#39044;&#27979;&#23545;&#35937;&#26631;&#31614;&#26041;&#38754;&#30340;&#27010;&#29575;&#20851;&#31995;&#65292;&#22240;&#20026;&#32570;&#23569;&#23454;&#20363;&#26631;&#31614;&#65307;&#65288;iii&#65289;&#22810;&#31181;&#22810;&#26679;&#30340;&#30456;&#20851;&#24615;&#65288;&#20363;&#22914;&#65292;&#26631;&#31614;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#23454;&#20363;&#38388;&#30340;&#30456;&#20851;&#24615;&#65289;&#21482;&#33021;&#22312;&#22810;&#20010;&#38454;&#27573;&#20013;&#23398;&#20064;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#24191;&#27867;&#22810;&#23454;&#20363;&#22810;&#26631;&#31614;&#23398;&#20064;&#65288;BMIML&#65289;&#30340;&#26032;&#22411;&#21333;&#38454;&#27573;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
described by multiple instances (e.g., image patches) and simultaneously associated with multiple labels. Existing MIML methods are useful in many applications but most of which suffer from relatively low accuracy and training efficiency due to several issues: i) the inter-label correlations(i.e., the probabilistic correlations between the multiple labels corresponding to an object) are neglected; ii) the inter-instance correlations (i.e., the probabilistic correlations of different instances in predicting the object label) cannot be learned directly (or jointly) with other types of correlations due to the missing instance labels; iii) diverse inter-correlations (e.g., inter-label correlations, inter-instance correlations) can only be learned in multiple stages. To resolve these issues, a new single-stage framework called broad multi-instance multi-label learning (BMIML) is proposed. In BMIML, there are three innovative modules: i) an auto-weighted label enhancement learning (AWLEL) ba
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#35757;&#32451;&#23545;&#25239;&#31574;&#30053;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#30333;&#30418;&#25915;&#20987;&#30340;&#31574;&#30053;&#65292;&#33021;&#22815;&#35775;&#38382;&#30446;&#26631;&#20195;&#29702;&#30340;&#20869;&#37096;&#29366;&#24577;&#65292;&#20174;&#32780;&#35782;&#21035;&#20854;&#28431;&#27934;&#65292;&#25915;&#20987;&#25104;&#21151;&#29575;&#26356;&#39640;&#12290;</title><link>http://arxiv.org/abs/2209.02167</link><description>&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#30333;&#30418;&#23545;&#25239;&#31574;&#30053;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
White-Box Adversarial Policies in Deep Reinforcement Learning. (arXiv:2209.02167v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.02167
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#35757;&#32451;&#23545;&#25239;&#31574;&#30053;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#30333;&#30418;&#25915;&#20987;&#30340;&#31574;&#30053;&#65292;&#33021;&#22815;&#35775;&#38382;&#30446;&#26631;&#20195;&#29702;&#30340;&#20869;&#37096;&#29366;&#24577;&#65292;&#20174;&#32780;&#35782;&#21035;&#20854;&#28431;&#27934;&#65292;&#25915;&#20987;&#25104;&#21151;&#29575;&#26356;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#23545;&#25239;&#31574;&#30053;&#21487;&#20197;&#36890;&#36807;&#35757;&#32451;&#23545;&#25239;&#20195;&#29702;&#26469;&#26368;&#23567;&#21270;&#30446;&#26631;&#20195;&#29702;&#30340;&#22870;&#21169;&#26469;&#24320;&#21457;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#30740;&#31350;&#20102;&#40657;&#30418;&#29256;&#26412;&#30340;&#36825;&#20123;&#25915;&#20987;&#65292;&#20854;&#20013;&#23545;&#25163;&#20165;&#35266;&#23519;&#19990;&#30028;&#29366;&#24577;&#65292;&#24182;&#23558;&#30446;&#26631;&#20195;&#29702;&#35270;&#20026;&#29615;&#22659;&#30340;&#20219;&#20309;&#20854;&#20182;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#36825;&#24182;&#27809;&#26377;&#32771;&#34385;&#38382;&#39064;&#20013;&#30340;&#38468;&#21152;&#32467;&#26500;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20174;&#30333;&#30418;&#25915;&#20987;&#30340;&#25991;&#29486;&#20013;&#33719;&#24471;&#28789;&#24863;&#65292;&#20197;&#35757;&#32451;&#26356;&#26377;&#25928;&#30340;&#23545;&#25239;&#31574;&#30053;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#30333;&#30418;&#23545;&#25239;&#31574;&#30053;&#65292;&#24182;&#26174;&#31034;&#35775;&#38382;&#30446;&#26631;&#20195;&#29702;&#30340;&#20869;&#37096;&#29366;&#24577;&#21487;&#20197;&#29992;&#20110;&#35782;&#21035;&#20854;&#28431;&#27934;&#12290;&#25105;&#20204;&#20570;&#20986;&#20102;&#20004;&#20010;&#36129;&#29486;&#12290;(1)&#25105;&#20204;&#20171;&#32461;&#20102;&#30333;&#30418;&#23545;&#25239;&#31574;&#30053;&#65292;&#20854;&#20013;&#25915;&#20987;&#32773;&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;&#35266;&#23519;&#30446;&#26631;&#30340;&#20869;&#37096;&#29366;&#24577;&#21644;&#19990;&#30028;&#29366;&#24577;&#12290;&#25105;&#20204;&#21046;&#23450;&#20102;&#20351;&#29992;&#36825;&#20123;&#31574;&#30053;&#25915;&#20987;2&#20154;&#28216;&#25103;&#21644;&#29983;&#25104;&#25991;&#26412;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20195;&#29702;&#30340;&#26041;&#27861;&#12290;(2)&#25105;&#20204;&#35777;&#26126;&#20102;&#19982;&#40657;&#30418;&#25915;&#20987;&#30456;&#27604;&#65292;&#36825;&#20123;&#31574;&#30053;&#21487;&#20197;&#23454;&#29616;&#26356;&#39640;&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;&#65292;&#29305;&#21035;&#26159;&#24403;&#30446;&#26631;&#20195;&#29702;&#30340;&#20869;&#37096;&#29366;&#24577;&#27604;&#36739;&#22797;&#26434;&#26102;&#12290;
&lt;/p&gt;
&lt;p&gt;
In reinforcement learning (RL), adversarial policies can be developed by training an adversarial agent to minimize a target agent's rewards. Prior work has studied black-box versions of these attacks where the adversary only observes the world state and treats the target agent as any other part of the environment. However, this does not take into account additional structure in the problem. In this work, we take inspiration from the literature on white-box attacks to train more effective adversarial policies. We study white-box adversarial policies and show that having access to a target agent's internal state can be useful for identifying its vulnerabilities. We make two contributions. (1) We introduce white-box adversarial policies where an attacker observes both a target's internal state and the world state at each timestep. We formulate ways of using these policies to attack agents in 2-player games and text-generating language models. (2) We demonstrate that these policies can ach
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#25628;&#32034;&#21644;&#25345;&#32493;&#23398;&#20064;&#30340;&#20132;&#21449;&#30740;&#31350;&#65292;&#26088;&#22312;&#24320;&#21457;&#26356;&#21152;&#31283;&#20581;&#21644;&#36866;&#24212;&#24615;&#30340;&#26234;&#33021;&#20307;&#65292;&#24212;&#29992;&#20110;&#29289;&#32852;&#32593;&#35774;&#22791;&#12289;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#31561;&#39046;&#22495;&#65292;&#20197;&#35299;&#20915;&#35832;&#22914;&#27010;&#24565;/&#25968;&#25454;&#28418;&#31227;&#31561;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2206.05625</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#25628;&#32034;&#21644;&#25345;&#32493;&#23398;&#20064;&#20132;&#21449;&#30740;&#31350;&#25506;&#31350;
&lt;/p&gt;
&lt;p&gt;
Exploring the Intersection between Neural Architecture Search and Continual Learning. (arXiv:2206.05625v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.05625
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#25628;&#32034;&#21644;&#25345;&#32493;&#23398;&#20064;&#30340;&#20132;&#21449;&#30740;&#31350;&#65292;&#26088;&#22312;&#24320;&#21457;&#26356;&#21152;&#31283;&#20581;&#21644;&#36866;&#24212;&#24615;&#30340;&#26234;&#33021;&#20307;&#65292;&#24212;&#29992;&#20110;&#29289;&#32852;&#32593;&#35774;&#22791;&#12289;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#31561;&#39046;&#22495;&#65292;&#20197;&#35299;&#20915;&#35832;&#22914;&#27010;&#24565;/&#25968;&#25454;&#28418;&#31227;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANNs&#65289;&#24050;&#32463;&#26377;&#20102;&#30456;&#24403;&#22823;&#30340;&#36827;&#23637;&#65292;&#20294;&#23427;&#20204;&#30340;&#35774;&#35745;&#36807;&#31243;&#20173;&#28982;&#20197;&#30452;&#35273;&#12289;&#32463;&#39564;&#21644;&#21453;&#22797;&#35797;&#38169;&#20026;&#20027;&#12290;&#36825;&#20010;&#20381;&#36182;&#20110;&#20154;&#31867;&#30340;&#36807;&#31243;&#24448;&#24448;&#32791;&#26102;&#19988;&#23481;&#26131;&#20986;&#38169;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#20165;&#32771;&#34385;&#22312;&#35757;&#32451;&#29615;&#22659;&#19979;&#30340;&#24773;&#20917;&#65292;&#32780;&#27809;&#26377;&#32771;&#34385;&#21608;&#22260;&#30340;&#29615;&#22659;&#12290;&#31070;&#32463;&#32593;&#32476;&#30340;&#25345;&#32493;&#36866;&#24212;&#24615;&#21644;&#33258;&#21160;&#21270;&#23545;&#20110;&#19968;&#20123;&#39046;&#22495;&#38750;&#24120;&#37325;&#35201;&#65292;&#22312;&#36825;&#20123;&#39046;&#22495;&#20013;&#27169;&#22411;&#22312;&#37096;&#32626;&#21518;&#30340;&#21487;&#35775;&#38382;&#24615;&#24456;&#26377;&#38480;&#65288;&#20363;&#22914;&#29289;&#32852;&#32593;&#35774;&#22791;&#12289;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#31561;&#65289;&#12290;&#27492;&#22806;&#65292;&#21363;&#20351;&#26159;&#21487;&#35775;&#38382;&#30340;&#27169;&#22411;&#65292;&#22312;&#37096;&#32626;&#21518;&#20063;&#38656;&#35201;&#39057;&#32321;&#32500;&#25252;&#20197;&#35299;&#20915;&#35832;&#22914;&#27010;&#24565;/&#25968;&#25454;&#28418;&#31227;&#31561;&#38382;&#39064;&#65292;&#36825;&#21487;&#33021;&#20250;&#24456;&#32321;&#29712;&#21644;&#38480;&#21046;&#24615;&#12290;&#36890;&#36807;&#21033;&#29992;&#21644;&#32467;&#21512;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#25628;&#32034;&#65288;NAS&#65289;&#21644;&#25345;&#32493;&#23398;&#20064;&#65288;CL&#65289;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#24320;&#21457;&#26356;&#21152;&#31283;&#20581;&#21644;&#36866;&#24212;&#24615;&#30340;&#26234;&#33021;&#20307;&#12290;&#26412;&#30740;&#31350;&#36827;&#34892;&#20102;&#31532;&#19968;&#27425;&#24191;&#27867;&#30340;&#28145;&#20837;&#25506;&#35752;&#27492;&#20132;&#21449;&#30740;&#31350;&#30340;&#32508;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the significant advances achieved in Artificial Neural Networks (ANNs), their design process remains notoriously tedious, depending primarily on intuition, experience and trial-and-error. This human-dependent process is often time-consuming and prone to errors. Furthermore, the models are generally bound to their training contexts, with no considerations to their surrounding environments. Continual adaptiveness and automation of neural networks is of paramount importance to several domains where model accessibility is limited after deployment (e.g IoT devices, self-driving vehicles, etc.). Additionally, even accessible models require frequent maintenance post-deployment to overcome issues such as Concept/Data Drift, which can be cumbersome and restrictive. By leveraging and combining approaches from Neural Architecture Search (NAS) and Continual Learning (CL), more robust and adaptive agents can be developed. This study conducts the first extensive review on the intersection be
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#12289;&#36731;&#37327;&#12289;&#24555;&#36895;&#30340;SE(3)-&#31561;&#21464;&#28857;&#20113;&#32593;&#32476;&#65292;&#36890;&#36807;&#32452;&#21367;&#31215;&#21644;&#21830;&#34920;&#31034;&#30456;&#32467;&#21512;&#23454;&#29616;&#65292;&#29992;&#20110;&#22788;&#29702;&#28857;&#20113;&#25968;&#25454;&#65292;&#21516;&#26102;&#22312;&#23545;&#35937;&#20998;&#31867;&#12289;&#23039;&#24577;&#20272;&#35745;&#21644;&#20851;&#38190;&#28857;&#21305;&#37197;&#31561;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#21487;&#27604;&#25110;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2206.05398</link><description>&lt;p&gt;
E2PN&#65306;&#39640;&#25928;&#30340;SE(3)-&#31561;&#21464;&#28857;&#20113;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
E2PN: Efficient SE(3)-Equivariant Point Network. (arXiv:2206.05398v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.05398
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#12289;&#36731;&#37327;&#12289;&#24555;&#36895;&#30340;SE(3)-&#31561;&#21464;&#28857;&#20113;&#32593;&#32476;&#65292;&#36890;&#36807;&#32452;&#21367;&#31215;&#21644;&#21830;&#34920;&#31034;&#30456;&#32467;&#21512;&#23454;&#29616;&#65292;&#29992;&#20110;&#22788;&#29702;&#28857;&#20113;&#25968;&#25454;&#65292;&#21516;&#26102;&#22312;&#23545;&#35937;&#20998;&#31867;&#12289;&#23039;&#24577;&#20272;&#35745;&#21644;&#20851;&#38190;&#28857;&#21305;&#37197;&#31561;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#21487;&#27604;&#25110;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21367;&#31215;&#32467;&#26500;&#65292;&#29992;&#20110;&#20174;3D&#28857;&#20113;&#23398;&#20064;SE(3)&#31561;&#21464;&#29305;&#24449;&#12290;&#23427;&#21487;&#20197;&#34987;&#35270;&#20026;&#26680;&#28857;&#21367;&#31215;(KPConv)&#30340;&#31561;&#21464;&#29256;&#26412;&#65292;KPConv&#26159;&#19968;&#31181;&#24191;&#27867;&#29992;&#20110;&#22788;&#29702;&#28857;&#20113;&#25968;&#25454;&#30340;&#21367;&#31215;&#24418;&#24335;&#12290;&#19982;&#29616;&#26377;&#30340;&#31561;&#21464;&#32593;&#32476;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#35774;&#35745;&#31616;&#21333;&#12289;&#36731;&#37327;&#12289;&#24555;&#36895;&#65292;&#26131;&#20110;&#19982;&#29616;&#26377;&#30340;&#29305;&#23450;&#20219;&#21153;&#30340;&#28857;&#20113;&#23398;&#20064;&#27969;&#31243;&#38598;&#25104;&#12290;&#25105;&#20204;&#36890;&#36807;&#32452;&#21367;&#31215;&#21644;&#21830;&#34920;&#31034;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#36825;&#20123;&#29702;&#24819;&#29305;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;SO(3)&#31163;&#25955;&#20026;&#26377;&#38480;&#32676;&#12289;&#20351;&#29992;SO(2)&#20316;&#20026;&#31283;&#23450;&#23376;&#32676;&#65292;&#24418;&#25104;&#29699;&#24418;&#21830;&#29305;&#24449;&#22330;&#20197;&#33410;&#30465;&#35745;&#31639;&#37327;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#32622;&#25442;&#23618;&#65292;&#20174;&#29699;&#24418;&#29305;&#24449;&#20013;&#24674;&#22797;SO(3)&#29305;&#24449;&#65292;&#20197;&#20445;&#30041;&#21306;&#20998;&#26059;&#36716;&#30340;&#33021;&#21147;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#21487;&#27604;&#25110;&#20248;&#36234;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;&#23545;&#35937;&#20998;&#31867;&#12289;&#23039;&#24577;&#20272;&#35745;&#21644;&#20851;&#38190;&#28857;&#21305;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a convolution structure for learning SE(3)-equivariant features from 3D point clouds. It can be viewed as an equivariant version of kernel point convolutions (KPConv), a widely used convolution form to process point cloud data. Compared with existing equivariant networks, our design is simple, lightweight, fast, and easy to be integrated with existing task-specific point cloud learning pipelines. We achieve these desirable properties by combining group convolutions and quotient representations. Specifically, we discretize SO(3) to finite groups for their simplicity while using SO(2) as the stabilizer subgroup to form spherical quotient feature fields to save computations. We also propose a permutation layer to recover SO(3) features from spherical features to preserve the capacity to distinguish rotations. Experiments show that our method achieves comparable or superior performance in various tasks, including object classification, pose estimation, and keypoint-matc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AdaProp&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#33258;&#36866;&#24212;&#22320;&#36807;&#28388;&#25481;&#19981;&#30456;&#20851;&#30340;&#23454;&#20307;&#65292;&#21516;&#26102;&#20445;&#30041;&#26377;&#21069;&#36884;&#30340;&#30446;&#26631;&#65292;&#20174;&#32780;&#39640;&#25928;&#12289;&#24378;&#22823;&#22320;&#36827;&#34892;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#12290;</title><link>http://arxiv.org/abs/2205.15319</link><description>&lt;p&gt;
AdaProp&#65306;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#20013;&#23398;&#20064;&#33258;&#36866;&#24212;&#20256;&#25773;
&lt;/p&gt;
&lt;p&gt;
AdaProp: Learning Adaptive Propagation for Graph Neural Network based Knowledge Graph Reasoning. (arXiv:2205.15319v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.15319
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AdaProp&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#33258;&#36866;&#24212;&#22320;&#36807;&#28388;&#25481;&#19981;&#30456;&#20851;&#30340;&#23454;&#20307;&#65292;&#21516;&#26102;&#20445;&#30041;&#26377;&#21069;&#36884;&#30340;&#30446;&#26631;&#65292;&#20174;&#32780;&#39640;&#25928;&#12289;&#24378;&#22823;&#22320;&#36827;&#34892;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#24191;&#27867;&#24212;&#29992;&#65292;&#24050;&#32463;&#26377;&#35768;&#22810;&#22522;&#20110;GNN&#30340;&#26041;&#27861;&#34987;&#35774;&#35745;&#29992;&#20110;&#30693;&#35782;&#22270;&#35889;(KG)&#25512;&#29702;&#12290;GNN-based KG&#25512;&#29702;&#26041;&#27861;&#20013;&#19968;&#20010;&#37325;&#35201;&#30340;&#32452;&#25104;&#37096;&#20998;&#26159;&#20256;&#25773;&#36335;&#24452;&#65292;&#23427;&#21253;&#21547;&#27599;&#20010;&#20256;&#25773;&#27493;&#39588;&#20013;&#25152;&#28041;&#21450;&#30340;&#19968;&#32452;&#23454;&#20307;&#12290;&#29616;&#26377;&#26041;&#27861;&#20351;&#29992;&#25163;&#24037;&#35774;&#35745;&#30340;&#20256;&#25773;&#36335;&#24452;&#65292;&#24573;&#30053;&#20102;&#23454;&#20307;&#19982;&#26597;&#35810;&#20851;&#31995;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#27492;&#22806;&#65292;&#22312;&#26356;&#22823;&#30340;&#20256;&#25773;&#27493;&#39588;&#20013;&#65292;&#28041;&#21450;&#30340;&#23454;&#20307;&#25968;&#37327;&#20250;&#29190;&#28856;&#24615;&#22686;&#38271;&#12290;&#26412;&#25991;&#26088;&#22312;&#23398;&#20064;&#19968;&#20010;&#33258;&#36866;&#24212;&#30340;&#20256;&#25773;&#36335;&#24452;&#65292;&#20197;&#36807;&#28388;&#25481;&#19981;&#30456;&#20851;&#30340;&#23454;&#20307;&#65292;&#21516;&#26102;&#20445;&#30041;&#26377;&#21069;&#36884;&#30340;&#30446;&#26631;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#22686;&#37327;&#37319;&#26679;&#26426;&#21046;&#65292;&#23427;&#33021;&#22815;&#20197;&#32447;&#24615;&#22797;&#26434;&#24230;&#20445;&#30041;&#38468;&#36817;&#30446;&#26631;&#21644;&#20998;&#23618;&#36830;&#25509;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20110;&#23398;&#20064;&#30340;&#37319;&#26679;&#20998;&#24067;&#26469;&#35782;&#21035;&#35821;&#20041;&#30456;&#20851;&#30340;&#23454;&#20307;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#24378;&#22823;&#12289;&#39640;&#25928;&#65292;&#24182;&#19988;&#26159;&#35821;&#20041;&#24863;&#30693;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to the popularity of Graph Neural Networks (GNNs), various GNN-based methods have been designed to reason on knowledge graphs (KGs). An important design component of GNN-based KG reasoning methods is called the propagation path, which contains a set of involved entities in each propagation step. Existing methods use hand-designed propagation paths, ignoring the correlation between the entities and the query relation. In addition, the number of involved entities will explosively grow at larger propagation steps. In this work, we are motivated to learn an adaptive propagation path in order to filter out irrelevant entities while preserving promising targets. First, we design an incremental sampling mechanism where the nearby targets and layer-wise connections can be preserved with linear complexity. Second, we design a learning-based sampling distribution to identify the semantically related entities. Extensive experiments show that our method is powerful, efficient, and semantic-awa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#25991;&#26412;&#34920;&#31034;&#21644;&#19978;&#19979;&#25991;BERT&#24494;&#35843;&#30340;&#38405;&#35835;&#29702;&#35299;&#33258;&#21160;&#35780;&#20998;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#21333;&#20010;&#38382;&#39064;/&#39064;&#30446;&#27169;&#22411;&#26080;&#27861;&#21033;&#29992;&#39064;&#30446;&#38388;&#20851;&#32852;&#24615;&#20197;&#21450;&#23384;&#20648;&#27169;&#22411;&#22256;&#38590;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2205.09864</link><description>&lt;p&gt;
&#22522;&#20110;&#19978;&#19979;&#25991;BERT&#35843;&#25972;&#30340;&#38405;&#35835;&#29702;&#35299;&#33258;&#21160;&#35780;&#20998;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Automated Scoring for Reading Comprehension via In-context BERT Tuning. (arXiv:2205.09864v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.09864
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#25991;&#26412;&#34920;&#31034;&#21644;&#19978;&#19979;&#25991;BERT&#24494;&#35843;&#30340;&#38405;&#35835;&#29702;&#35299;&#33258;&#21160;&#35780;&#20998;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#21333;&#20010;&#38382;&#39064;/&#39064;&#30446;&#27169;&#22411;&#26080;&#27861;&#21033;&#29992;&#39064;&#30446;&#38388;&#20851;&#32852;&#24615;&#20197;&#21450;&#23384;&#20648;&#27169;&#22411;&#22256;&#38590;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#35780;&#20998;&#27169;&#22411;&#26377;&#30528;&#26497;&#22823;&#30340;&#28508;&#21147;&#21487;&#20197;&#38477;&#20302;&#20154;&#24037;&#35780;&#20998;&#30340;&#25104;&#26412;&#12290;&#26368;&#36817;&#30340;&#19968;&#20123;&#33258;&#21160;&#35780;&#20998;&#26041;&#27861;&#21033;&#29992;&#20102;&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;&#20363;&#22914;BERT&#21644;GPT&#65289;&#30340;&#25991;&#26412;&#34920;&#31034;&#20316;&#20026;&#35780;&#20998;&#27169;&#22411;&#30340;&#36755;&#20837;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#26041;&#27861;&#20026;&#27599;&#20010;&#38382;&#39064;/&#39064;&#30446;&#35757;&#32451;&#19968;&#20010;&#21333;&#29420;&#30340;&#27169;&#22411;&#65292;&#36825;&#36866;&#29992;&#20110;&#35797;&#39064;&#31181;&#31867;&#21315;&#24046;&#19975;&#21035;&#30340;&#20316;&#25991;&#35780;&#20998;&#31561;&#22330;&#26223;&#12290;&#20294;&#26159;&#36825;&#31181;&#26041;&#27861;&#23384;&#22312;&#20004;&#20010;&#38382;&#39064;&#65306;1&#65289;&#22312;&#38405;&#35835;&#29702;&#35299;&#31561;&#19982;&#22810;&#20010;&#38382;&#39064;/&#39064;&#30446;&#30456;&#20851;&#30340;&#22330;&#26223;&#19979;&#65292;&#26080;&#27861;&#21033;&#29992;&#39064;&#30446;&#20043;&#38388;&#30340;&#20851;&#32852;&#24615;&#65307;2&#65289;&#24403;&#27169;&#22411;&#21442;&#25968;&#25968;&#37327;&#24222;&#22823;&#26102;&#65292;&#23384;&#20648;&#27599;&#20010;&#39064;&#30446;&#29420;&#31435;&#27169;&#22411;&#21464;&#24471;&#24456;&#22256;&#38590;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#25105;&#20204;&#22312;&#22269;&#23478;&#25945;&#32946;&#36827;&#27493;&#35780;&#20272;&#65288;NAEP&#65289;&#38405;&#35835;&#29702;&#35299;&#33258;&#21160;&#35780;&#20998;&#25361;&#25112;&#36187;&#20013;&#33719;&#24471;&#30340;&#65288;&#19968;&#31561;&#22870;&#65289;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#8212;&#8212;&#22522;&#20110;&#19978;&#19979;&#25991;BERT&#24494;&#35843;&#65292;&#20135;&#29983;&#19968;&#20010;&#20849;&#29992;&#30340;&#35780;&#20998;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automated scoring of open-ended student responses has the potential to significantly reduce human grader effort. Recent advances in automated scoring often leverage textual representations based on pre-trained language models such as BERT and GPT as input to scoring models. Most existing approaches train a separate model for each item/question, which is suitable for scenarios such as essay scoring where items can be quite different from one another. However, these approaches have two limitations: 1) they fail to leverage item linkage for scenarios such as reading comprehension where multiple items may share a reading passage; 2) they are not scalable since storing one model per item becomes difficult when models have a large number of parameters. In this paper, we report our (grand prize-winning) solution to the National Assessment of Education Progress (NAEP) automated scoring challenge for reading comprehension. Our approach, in-context BERT fine-tuning, produces a single shared scor
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#28857;&#31215;&#20302;&#31209;&#36817;&#20284;&#26469;&#34920;&#31034;Q&#20540;&#30340;&#21452;&#32447;&#24615;&#20998;&#35299;&#26041;&#27861;&#65292;&#20854;&#20013;&#31532;&#19968;&#20010;&#21521;&#37327;&#22330;&#25429;&#25417;&#29366;&#24577;&#30340;&#23616;&#37096;&#21160;&#24577;&#65292;&#31532;&#20108;&#20010;&#37096;&#20998;&#25429;&#25417;&#24403;&#21069;&#29366;&#24577;&#21644;&#30446;&#26631;&#20043;&#38388;&#30340;&#20840;&#23616;&#20851;&#31995;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#25968;&#25454;&#25928;&#29575;&#65292;&#24182;&#20855;&#26377;&#24456;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2204.13695</link><description>&lt;p&gt;
&#21452;&#32447;&#24615;&#20215;&#20540;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Bilinear value networks. (arXiv:2204.13695v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.13695
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#28857;&#31215;&#20302;&#31209;&#36817;&#20284;&#26469;&#34920;&#31034;Q&#20540;&#30340;&#21452;&#32447;&#24615;&#20998;&#35299;&#26041;&#27861;&#65292;&#20854;&#20013;&#31532;&#19968;&#20010;&#21521;&#37327;&#22330;&#25429;&#25417;&#29366;&#24577;&#30340;&#23616;&#37096;&#21160;&#24577;&#65292;&#31532;&#20108;&#20010;&#37096;&#20998;&#25429;&#25417;&#24403;&#21069;&#29366;&#24577;&#21644;&#30446;&#26631;&#20043;&#38388;&#30340;&#20840;&#23616;&#20851;&#31995;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#25968;&#25454;&#25928;&#29575;&#65292;&#24182;&#20855;&#26377;&#24456;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;&#30340;&#20027;&#27969;&#26694;&#26550;&#28041;&#21450;&#20272;&#35745;&#30446;&#26631;&#26465;&#20214;&#19979;&#30340;Q&#20540;&#20989;&#25968;&#12290;&#24403;&#23398;&#20064;&#23454;&#29616;&#22810;&#20010;&#19981;&#21516;&#30446;&#26631;&#26102;&#65292;&#25968;&#25454;&#25928;&#29575;&#19982;Q&#20989;&#25968;&#23545;&#26032;&#30446;&#26631;&#30340;&#27867;&#21270;&#23494;&#20999;&#30456;&#20851;&#12290;&#30446;&#21069;&#30340;&#33539;&#24335;&#26159;&#20351;&#29992;&#21333;&#19968;&#30340;&#31070;&#32463;&#32593;&#32476;&#26469;&#36817;&#20284;Q(s, a, g)&#12290;&#20026;&#20102;&#25913;&#36827;Q&#20989;&#25968;&#30340;&#27867;&#21270;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#32447;&#24615;&#20998;&#35299;&#26041;&#27861;&#65292;&#36890;&#36807;&#20004;&#20010;&#21521;&#37327;&#22330;&#20043;&#38388;&#30340;&#28857;&#31215;&#30340;&#20302;&#31209;&#36817;&#20284;&#26469;&#34920;&#31034;Q&#20540;&#12290;&#31532;&#19968;&#20010;&#21521;&#37327;&#22330;f(s, a)&#25429;&#25417;&#29366;&#24577;s&#22788;&#30340;&#29615;&#22659;&#23616;&#37096;&#21160;&#24577;&#65307;&#32780;&#31532;&#20108;&#20010;&#37096;&#20998;{&#981;}(s, g)&#21017;&#25429;&#25417;&#24403;&#21069;&#29366;&#24577;&#21644;&#30446;&#26631;&#20043;&#38388;&#30340;&#20840;&#23616;&#20851;&#31995;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#21452;&#32447;&#24615;&#20998;&#35299;&#26041;&#26696;&#26174;&#33879;&#25552;&#39640;&#20102;&#25968;&#25454;&#25928;&#29575;&#65292;&#30456;&#27604;&#20808;&#21069;&#30340;&#26041;&#27861;&#65292;&#23545;&#20110;&#22788;&#20110;&#20998;&#24067;&#33539;&#22260;&#20043;&#22806;&#30340;&#30446;&#26631;&#20855;&#26377;&#26356;&#22909;&#30340;&#36716;&#31227;&#24615;&#12290;&#25105;&#20204;&#22312;&#27169;&#25311;&#30340;Fetch&#26426;&#22120;&#20154;&#20219;&#21153;&#22871;&#20214;&#21644;DeepMind Control Suite&#19978;&#25552;&#20379;&#20102;&#23454;&#35777;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
The dominant framework for off-policy multi-goal reinforcement learning involves estimating goal conditioned Q-value function. When learning to achieve multiple goals, data efficiency is intimately connected with the generalization of the Q-function to new goals. The de-facto paradigm is to approximate Q(s, a, g) using monolithic neural networks. To improve the generalization of the Q-function, we propose a bilinear decomposition that represents the Q-value via a low-rank approximation in the form of a dot product between two vector fields. The first vector field, f(s, a), captures the environment's local dynamics at the state s; whereas the second component, {\phi}(s, g), captures the global relationship between the current state and the goal. We show that our bilinear decomposition scheme substantially improves data efficiency, and has superior transfer to out-of-distribution goals compared to prior methods. Empirical evidence is provided on the simulated Fetch robot task-suite and d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25299;&#25169;&#32463;&#39564;&#22238;&#25918;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#22270;&#26469;&#26126;&#30830;&#29366;&#24577;&#30340; Q &#20540;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#37319;&#26679;&#31574;&#30053;&#24573;&#35270;&#29366;&#24577;&#38388;&#20381;&#36182;&#20851;&#31995;&#30340;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#23398;&#20064;&#28145;&#24230; Q &#20989;&#25968;&#26102;&#30340;&#24615;&#33021;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2203.15845</link><description>&lt;p&gt;
&#25299;&#25169;&#32463;&#39564;&#22238;&#25918;
&lt;/p&gt;
&lt;p&gt;
Topological Experience Replay. (arXiv:2203.15845v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.15845
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25299;&#25169;&#32463;&#39564;&#22238;&#25918;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#22270;&#26469;&#26126;&#30830;&#29366;&#24577;&#30340; Q &#20540;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#37319;&#26679;&#31574;&#30053;&#24573;&#35270;&#29366;&#24577;&#38388;&#20381;&#36182;&#20851;&#31995;&#30340;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#23398;&#20064;&#28145;&#24230; Q &#20989;&#25968;&#26102;&#30340;&#24615;&#33021;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#20808;&#36827;&#30340;&#28145;&#24230; Q &#23398;&#20064;&#26041;&#27861;&#20351;&#29992;&#20174;&#32463;&#39564;&#37325;&#25918;&#32531;&#20914;&#21306;&#20013;&#37319;&#26679;&#30340;&#29366;&#24577;&#36716;&#25442;&#20803;&#32452;&#26356;&#26032; Q &#20540;&#12290;&#36825;&#31181;&#31574;&#30053;&#36890;&#24120;&#22343;&#21248;&#21644;&#38543;&#26426;&#22320;&#37319;&#26679;&#65292;&#25110;&#22522;&#20110;&#35832;&#22914;&#26102;&#38388;&#24046;&#65288;TD&#65289;&#35823;&#24046;&#31561;&#24230;&#37327;&#20248;&#20808;&#12290;&#36825;&#26679;&#30340;&#37319;&#26679;&#31574;&#30053;&#22312;&#23398;&#20064; Q &#20989;&#25968;&#26102;&#21487;&#33021;&#25928;&#29575;&#20302;&#19979;&#65292;&#22240;&#20026;&#19968;&#20010;&#29366;&#24577;&#30340; Q &#20540;&#21462;&#20915;&#20110;&#32487;&#25215;&#29366;&#24577;&#30340; Q &#20540;&#12290;&#22914;&#26524;&#25968;&#25454;&#37319;&#26679;&#31574;&#30053;&#24573;&#30053;&#20102;&#19979;&#19968;&#20010;&#29366;&#24577;&#30340; Q &#20540;&#20272;&#35745;&#30340;&#31934;&#24230;&#65292;&#23427;&#21487;&#33021;&#20250;&#23548;&#33268;&#26080;&#29992;&#21644;&#24120;&#24120;&#19981;&#27491;&#30830;&#30340; Q &#20540;&#26356;&#26032;&#12290;&#20026;&#20102;&#20943;&#36731;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#23558;&#26234;&#33021;&#20307;&#30340;&#32463;&#39564;&#32452;&#32455;&#25104;&#19968;&#20010;&#22270;&#65292;&#26126;&#30830;&#36319;&#36394;&#29366;&#24577;&#30340; Q &#20540;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#22270;&#20013;&#30340;&#27599;&#26465;&#36793;&#20195;&#34920;&#36890;&#36807;&#25191;&#34892;&#21333;&#20010;&#25805;&#20316;&#22312;&#20004;&#20010;&#29366;&#24577;&#20043;&#38388;&#30340;&#36716;&#25442;&#12290;&#25105;&#20204;&#36890;&#36807;&#20174;&#19968;&#32452;&#32456;&#31471;&#29366;&#24577;&#24320;&#22987;&#25193;&#23637;&#22270;&#20013;&#30340;&#39030;&#28857;&#65292;&#24182;&#36880;&#27493;&#21521;&#21518;&#31227;&#21160;&#30340;&#24191;&#24230;&#20248;&#20808;&#25628;&#32034;&#26469;&#25191;&#34892;&#20540;&#22791;&#20221;&#12290;
&lt;/p&gt;
&lt;p&gt;
State-of-the-art deep Q-learning methods update Q-values using state transition tuples sampled from the experience replay buffer. This strategy often uniformly and randomly samples or prioritizes data sampling based on measures such as the temporal difference (TD) error. Such sampling strategies can be inefficient at learning Q-function because a state's Q-value depends on the Q-value of successor states. If the data sampling strategy ignores the precision of the Q-value estimate of the next state, it can lead to useless and often incorrect updates to the Q-values. To mitigate this issue, we organize the agent's experience into a graph that explicitly tracks the dependency between Q-values of states. Each edge in the graph represents a transition between two states by executing a single action. We perform value backups via a breadth-first search starting from that expands vertices in the graph starting from the set of terminal states and successively moving backward. We empirically sho
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#24182;&#34892;&#36830;&#32493;&#23398;&#20064;&#65288;PSL&#65289;&#65292;&#36890;&#36807;&#32593;&#32476;&#12289;&#24322;&#26500;&#24615;&#21644;&#25509;&#36817;&#24615;&#19977;&#20010;&#32500;&#24230;&#30340;&#25193;&#23637;&#65292;&#23454;&#29616;&#20102;&#22312;&#26080;&#32447;&#35774;&#22791;&#38598;&#21512;&#19978;&#30340;&#21160;&#24577;&#20998;&#24067;&#24335;&#27169;&#22411;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2202.02947</link><description>&lt;p&gt;
&#24182;&#34892;&#36830;&#32493;&#23398;&#20064;&#29992;&#20110;&#24322;&#26500;&#26080;&#32447;&#32593;&#32476;&#19978;&#21160;&#24577;&#20998;&#24067;&#24335;&#27169;&#22411;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Parallel Successive Learning for Dynamic Distributed Model Training over Heterogeneous Wireless Networks. (arXiv:2202.02947v6 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.02947
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#24182;&#34892;&#36830;&#32493;&#23398;&#20064;&#65288;PSL&#65289;&#65292;&#36890;&#36807;&#32593;&#32476;&#12289;&#24322;&#26500;&#24615;&#21644;&#25509;&#36817;&#24615;&#19977;&#20010;&#32500;&#24230;&#30340;&#25193;&#23637;&#65292;&#23454;&#29616;&#20102;&#22312;&#26080;&#32447;&#35774;&#22791;&#38598;&#21512;&#19978;&#30340;&#21160;&#24577;&#20998;&#24067;&#24335;&#27169;&#22411;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;(FedL)&#24050;&#25104;&#20026;&#19968;&#31181;&#23558;&#27169;&#22411;&#35757;&#32451;&#20998;&#24067;&#22312;&#19968;&#32452;&#26080;&#32447;&#35774;&#22791;&#19978;&#30340;&#27969;&#34892;&#25216;&#26415;&#65292;&#36890;&#36807;&#35774;&#22791;&#19978;&#30340;&#36845;&#20195;&#26412;&#22320;&#26356;&#26032;&#21644;&#26381;&#21153;&#22120;&#19978;&#30340;&#20840;&#23616;&#32858;&#21512;&#12290;&#26412;&#25991;&#25552;&#20986;&#24182;&#34892;&#36830;&#32493;&#23398;&#20064;&#65288;PSL&#65289;&#65292;&#23558;FedL&#26550;&#26500;&#27839;&#19977;&#20010;&#32500;&#24230;&#36827;&#34892;&#25193;&#23637;&#65306;&#65288;i&#65289;&#32593;&#32476;&#65292;&#36890;&#36807;&#35774;&#22791;&#38388;&#36890;&#20449;&#23454;&#29616;&#21435;&#20013;&#24515;&#21270;&#21327;&#20316;&#65307;&#65288;ii&#65289;&#24322;&#26500;&#24615;&#65292;&#22312;&#19977;&#20010;&#23618;&#27425;&#36827;&#34892;&#35299;&#37322;&#65306;&#65288;ii-a&#65289;&#23398;&#20064;&#65306;PSL&#32771;&#34385;&#21040;&#35774;&#22791;&#19978;&#20855;&#26377;&#19981;&#21516;&#30340;&#23567;&#25209;&#37327;&#22823;&#23567;&#30340;&#24322;&#26500;&#25968;&#37327;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#36845;&#20195;&#65307;&#65288;ii-b&#65289;&#25968;&#25454;&#65306;PSL&#20551;&#35774;&#25968;&#25454;&#21040;&#36798;&#21644;&#31163;&#24320;&#30340;&#21160;&#24577;&#29615;&#22659;&#20013;&#65292;&#26412;&#22320;&#25968;&#25454;&#38598;&#30340;&#20998;&#24067;&#38543;&#26102;&#38388;&#28436;&#21464;&#65292;&#36890;&#36807;&#26032;&#30340;&#27169;&#22411;/&#27010;&#24565;&#28418;&#31227;&#24230;&#37327;&#26469;&#25429;&#33719;&#65307;&#65288;ii-c&#65289;&#35774;&#22791;&#65306;PSL&#32771;&#34385;&#21040;&#20855;&#26377;&#19981;&#21516;&#35745;&#31639;&#21644;&#36890;&#20449;&#33021;&#21147;&#30340;&#35774;&#22791;&#65307;&#65288;iii&#65289;&#25509;&#36817;&#24615;&#65292;&#35774;&#22791;&#20043;&#38388;&#20197;&#21450;&#35775;&#38382;&#28857;&#20043;&#38388;&#20855;&#26377;&#19981;&#21516;&#30340;&#36317;&#31163;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FedL) has emerged as a popular technique for distributing model training over a set of wireless devices, via iterative local updates (at devices) and global aggregations (at the server). In this paper, we develop parallel successive learning (PSL), which expands the FedL architecture along three dimensions: (i) Network, allowing decentralized cooperation among the devices via device-to-device (D2D) communications. (ii) Heterogeneity, interpreted at three levels: (ii-a) Learning: PSL considers heterogeneous number of stochastic gradient descent iterations with different mini-batch sizes at the devices; (ii-b) Data: PSL presumes a dynamic environment with data arrival and departure, where the distributions of local datasets evolve over time, captured via a new metric for model/concept drift. (ii-c) Device: PSL considers devices with different computation and communication capabilities. (iii) Proximity, where devices have different distances to each other and the acces
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#35748;&#30693;&#25968;&#23383;&#23402;&#29983;&#20307;&#30340;&#35748;&#30693;&#26550;&#26500;&#65292;&#21363;&#36890;&#36807;&#22522;&#20110;&#21306;&#22359;&#38142;&#22522;&#30784;&#35774;&#26045;&#30340;&#26041;&#24335;&#23558;&#29992;&#25143;&#30340;&#20010;&#20154;&#25968;&#25454;&#36716;&#21270;&#20026;&#32467;&#26500;&#21270;&#20449;&#24687;&#21644;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#26368;&#32456;&#33021;&#22815;&#19968;&#36215;&#32452;&#25104;&#29992;&#25143;&#30340;&#35748;&#30693;&#25968;&#23383;&#23402;&#29983;&#20307;&#12290;</title><link>http://arxiv.org/abs/2201.08163</link><description>&lt;p&gt;
&#35748;&#30693;&#36134;&#26412;&#39033;&#30446;&#65306;&#36890;&#36807;&#35748;&#30693;&#21306;&#22359;&#38142;&#26500;&#24314;&#20010;&#20154;&#25968;&#23383;&#21270;&#23402;&#29983;&#20307;
&lt;/p&gt;
&lt;p&gt;
Cognitive Ledger Project: Towards Building Personal Digital Twins Through Cognitive Blockchain. (arXiv:2201.08163v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.08163
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#35748;&#30693;&#25968;&#23383;&#23402;&#29983;&#20307;&#30340;&#35748;&#30693;&#26550;&#26500;&#65292;&#21363;&#36890;&#36807;&#22522;&#20110;&#21306;&#22359;&#38142;&#22522;&#30784;&#35774;&#26045;&#30340;&#26041;&#24335;&#23558;&#29992;&#25143;&#30340;&#20010;&#20154;&#25968;&#25454;&#36716;&#21270;&#20026;&#32467;&#26500;&#21270;&#20449;&#24687;&#21644;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#26368;&#32456;&#33021;&#22815;&#19968;&#36215;&#32452;&#25104;&#29992;&#25143;&#30340;&#35748;&#30693;&#25968;&#23383;&#23402;&#29983;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35748;&#30693;&#36134;&#26412;&#39033;&#30446;&#26088;&#22312;&#24320;&#21457;&#19968;&#20010;&#27169;&#22359;&#21270;&#31995;&#32479;&#65292;&#36890;&#36807;&#22522;&#20110;&#21306;&#22359;&#38142;&#22522;&#30784;&#35774;&#26045;&#30340;&#26041;&#24335;&#23558;&#29992;&#25143;&#30340;&#20010;&#20154;&#25968;&#25454;&#36716;&#21270;&#20026;&#32467;&#26500;&#21270;&#20449;&#24687;&#21644;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#22312;&#36825;&#20010;&#27491;&#22312;&#36827;&#34892;&#30340;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#35748;&#30693;&#25968;&#23383;&#23402;&#29983;&#20307;&#30340;&#35748;&#30693;&#26550;&#26500;&#12290;&#25152;&#25552;&#20986;&#30340;&#35774;&#35745;&#22312;&#20854;&#26680;&#24515;&#37319;&#29992;&#20102;&#35748;&#30693;&#21306;&#22359;&#38142;&#65288;&#35748;&#30693;&#36134;&#26412;&#65289;&#12290;&#26550;&#26500;&#21253;&#25324;&#22810;&#20010;&#27169;&#22359;&#65292;&#21487;&#20197;&#23558;&#29992;&#25143;&#22312;&#25968;&#23383;&#29615;&#22659;&#20013;&#30340;&#27963;&#21160;&#36716;&#21270;&#20026;&#21487;&#37325;&#22797;&#20351;&#29992;&#30340;&#30693;&#35782;&#23545;&#35937;&#21644;&#20154;&#24037;&#26234;&#33021;&#65292;&#26368;&#32456;&#33021;&#22815;&#19968;&#36215;&#32452;&#25104;&#29992;&#25143;&#30340;&#35748;&#30693;&#25968;&#23383;&#23402;&#29983;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Cognitive Ledger Project is an effort to develop a modular system for turning users' personal data into structured information and machine learning models based on a blockchain-based infrastructure. In this work-in-progress paper, we propose a cognitive architecture for cognitive digital twins. The suggested design embraces a cognitive blockchain (Cognitive ledger) at its core. The architecture includes several modules that turn users' activities in the digital environment into reusable knowledge objects and artificial intelligence that one day can work together to form the cognitive digital twin of users.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20219;&#21153;&#24863;&#30693;&#20803;&#23398;&#20064;&#30340;Siamese&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#28151;&#28102;&#24694;&#24847;&#36719;&#20214;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#26356;&#20934;&#30830;&#22320;&#35843;&#25972;&#29305;&#24449;&#23884;&#20837;&#65292;&#20197;&#24212;&#23545;&#19981;&#21516;&#24694;&#24847;&#36719;&#20214;&#23478;&#26063;&#30340;&#28151;&#28102;&#21464;&#31181;&#65292;&#24182;&#19988;&#21487;&#20197;&#23545;&#24694;&#24847;&#36719;&#20214;&#31867;&#36827;&#34892;&#20998;&#31867;&#65292;&#21363;&#20351;&#21482;&#26377;&#19968;&#20010;&#25110;&#24456;&#23569;&#30340;&#35757;&#32451;&#26679;&#26412;&#21487;&#29992;&#12290;</title><link>http://arxiv.org/abs/2110.13409</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#20219;&#21153;&#24863;&#30693;&#20803;&#23398;&#20064;&#30340;Siamese&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#28151;&#28102;&#24694;&#24847;&#36719;&#20214;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Task-Aware Meta Learning-based Siamese Neural Network for Classifying Obfuscated Malware. (arXiv:2110.13409v3 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.13409
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20219;&#21153;&#24863;&#30693;&#20803;&#23398;&#20064;&#30340;Siamese&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#28151;&#28102;&#24694;&#24847;&#36719;&#20214;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#26356;&#20934;&#30830;&#22320;&#35843;&#25972;&#29305;&#24449;&#23884;&#20837;&#65292;&#20197;&#24212;&#23545;&#19981;&#21516;&#24694;&#24847;&#36719;&#20214;&#23478;&#26063;&#30340;&#28151;&#28102;&#21464;&#31181;&#65292;&#24182;&#19988;&#21487;&#20197;&#23545;&#24694;&#24847;&#36719;&#20214;&#31867;&#36827;&#34892;&#20998;&#31867;&#65292;&#21363;&#20351;&#21482;&#26377;&#19968;&#20010;&#25110;&#24456;&#23569;&#30340;&#35757;&#32451;&#26679;&#26412;&#21487;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24694;&#24847;&#36719;&#20214;&#20316;&#32773;&#37319;&#29992;&#19981;&#21516;&#30340;&#25511;&#21046;&#27969;&#28151;&#28102;&#25216;&#26415;&#21019;&#24314;&#26032;&#30340;&#24694;&#24847;&#36719;&#20214;&#21464;&#31181;&#20197;&#36991;&#20813;&#26816;&#27979;&#12290;&#29616;&#26377;&#30340;&#22522;&#20110;Siamese&#31070;&#32463;&#32593;&#32476;&#65288;SNN&#65289;&#30340;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#26041;&#27861;&#22312;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#23384;&#22312;&#36825;&#20123;&#28151;&#28102;&#26679;&#26412;&#26102;&#26080;&#27861;&#27491;&#30830;&#20998;&#31867;&#19981;&#21516;&#30340;&#24694;&#24847;&#36719;&#20214;&#23478;&#26063;&#65292;&#23548;&#33268;&#39640;&#35823;&#25253;&#29575;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20219;&#21153;&#24863;&#30693;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;Siamese&#31070;&#32463;&#32593;&#32476;&#65292;&#23427;&#21487;&#20197;&#25269;&#25239;&#21463;&#25511;&#21046;&#27969;&#28151;&#28102;&#24433;&#21709;&#30340;&#24694;&#24847;&#36719;&#20214;&#21464;&#31181;&#30340;&#23384;&#22312;&#12290;&#21033;&#29992;&#27599;&#20010;&#24694;&#24847;&#36719;&#20214;&#23478;&#26063;&#30340;&#24179;&#22343;&#29109;&#29305;&#24449;&#20316;&#20026;&#36755;&#20837;&#65292;&#38500;&#20102;&#22270;&#20687;&#29305;&#24449;&#22806;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#36824;&#29983;&#25104;&#29305;&#24449;&#23618;&#30340;&#21442;&#25968;&#65292;&#20197;&#26356;&#20934;&#30830;&#22320;&#20026;&#19981;&#21516;&#30340;&#24694;&#24847;&#36719;&#20214;&#23478;&#26063;&#35843;&#25972;&#29305;&#24449;&#23884;&#20837;&#65292;&#36825;&#20123;&#23478;&#26063;&#37117;&#26377;&#28151;&#28102;&#30340;&#24694;&#24847;&#36719;&#20214;&#21464;&#31181;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#23545;&#24694;&#24847;&#36719;&#20214;&#31867;&#36827;&#34892;&#20998;&#31867;&#65292;&#21363;&#20351;&#21482;&#26377;&#19968;&#20010;&#25110;&#24456;&#23569;&#30340;&#35757;&#32451;&#26679;&#26412;&#21487;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Malware authors apply different techniques of control flow obfuscation, in order to create new malware variants to avoid detection. Existing Siamese neural network (SNN)-based malware detection methods fail to correctly classify different malware families when such obfuscated malware samples are present in the training dataset, resulting in high false-positive rates. To address this issue, we propose a novel task-aware few-shot-learning-based Siamese Neural Network that is resilient against the presence of malware variants affected by such control flow obfuscation techniques. Using the average entropy features of each malware family as inputs, in addition to the image features, our model generates the parameters for the feature layers, to more accurately adjust the feature embedding for different malware families, each of which has obfuscated malware variants. In addition, our proposed method can classify malware classes, even if there are only one or a few training samples available. 
&lt;/p&gt;</description></item><item><title>QuantumNAT&#26159;&#19968;&#20010;PQC&#29305;&#23450;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#35757;&#32451;&#21644;&#25512;&#26029;&#38454;&#27573;&#25191;&#34892;&#22122;&#22768;&#24863;&#30693;&#20248;&#21270;&#65292;&#25552;&#39640;&#40065;&#26834;&#24615;&#65292;&#32531;&#35299;&#37327;&#23376;&#22122;&#22768;</title><link>http://arxiv.org/abs/2110.11331</link><description>&lt;p&gt;
QuantumNAT&#65306;&#27880;&#37325;&#37327;&#23376;&#22122;&#22768;&#30340;&#22122;&#22768;&#27880;&#20837;&#12289;&#37327;&#21270;&#21644;&#24402;&#19968;&#21270;&#30340;&#37327;&#23376;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
QuantumNAT: Quantum Noise-Aware Training with Noise Injection, Quantization and Normalization. (arXiv:2110.11331v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.11331
&lt;/p&gt;
&lt;p&gt;
QuantumNAT&#26159;&#19968;&#20010;PQC&#29305;&#23450;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#35757;&#32451;&#21644;&#25512;&#26029;&#38454;&#27573;&#25191;&#34892;&#22122;&#22768;&#24863;&#30693;&#20248;&#21270;&#65292;&#25552;&#39640;&#40065;&#26834;&#24615;&#65292;&#32531;&#35299;&#37327;&#23376;&#22122;&#22768;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21442;&#25968;&#21270;&#37327;&#23376;&#30005;&#36335;&#26159;&#23454;&#29616;&#36817;&#26399;&#37327;&#23376;&#30828;&#20214;&#20248;&#21183;&#30340;&#26377;&#24076;&#26395;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23384;&#22312;&#36739;&#22823;&#30340;&#37327;&#23376;&#22122;&#22768;&#65288;&#35823;&#24046;&#65289;&#65292;&#22312;&#23454;&#38469;&#30340;&#37327;&#23376;&#35774;&#22791;&#19978;&#65292;PQC&#27169;&#22411;&#30340;&#24615;&#33021;&#20250;&#21463;&#21040;&#20005;&#37325;&#30340;&#38477;&#32423;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;QuantumNAT&#65292;&#19968;&#20010;&#21487;&#20197;&#22312;&#35757;&#32451;&#21644;&#25512;&#26029;&#38454;&#27573;&#25191;&#34892;&#22122;&#22768;&#24863;&#30693;&#20248;&#21270;&#30340;PQC&#29305;&#23450;&#26694;&#26550;&#65292;&#20197;&#25552;&#39640;&#20854;&#40065;&#26834;&#24615;&#12290;&#36890;&#36807;&#23454;&#39564;&#25105;&#20204;&#21457;&#29616;&#65292;&#37327;&#23376;&#22122;&#22768;&#23545;PQC&#27979;&#37327;&#32467;&#26524;&#30340;&#24433;&#21709;&#26159;&#20174;&#26080;&#22122;&#22768;&#32467;&#26524;&#32463;&#36807;&#19968;&#20010;&#32553;&#25918;&#21644;&#20559;&#31227;&#22240;&#23376;&#24471;&#21040;&#30340;&#32447;&#24615;&#26144;&#23556;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21518;&#27979;&#37327;&#24402;&#19968;&#21270;&#26469;&#32531;&#35299;&#29305;&#24449;&#20998;&#24067;&#19981;&#19968;&#33268;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Parameterized Quantum Circuits (PQC) are promising towards quantum advantage on near-term quantum hardware. However, due to the large quantum noises (errors), the performance of PQC models has a severe degradation on real quantum devices. Take Quantum Neural Network (QNN) as an example, the accuracy gap between noise-free simulation and noisy results on IBMQ-Yorktown for MNIST-4 classification is over 60%. Existing noise mitigation methods are general ones without leveraging unique characteristics of PQC; on the other hand, existing PQC work does not consider noise effect. To this end, we present QuantumNAT, a PQC-specific framework to perform noise-aware optimizations in both training and inference stages to improve robustness. We experimentally observe that the effect of quantum noise to PQC measurement outcome is a linear map from noise-free outcome with a scaling and a shift factor. Motivated by that, we propose post-measurement normalization to mitigate the feature distribution di
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#20302;&#20809;&#35270;&#35273;&#30340;&#21487;&#35265;-&#32418;&#22806;&#25104;&#23545;&#25968;&#25454;&#38598;LLVIP&#65292;&#25968;&#25454;&#38598;&#21253;&#21547;30976&#24352;&#22270;&#20687;&#65292;&#35299;&#20915;&#20102;&#22312;&#20302;&#20809;&#26465;&#20214;&#19979;&#21508;&#31181;&#35270;&#35273;&#20219;&#21153;&#32570;&#22833;&#26377;&#25928;&#30446;&#26631;&#21306;&#22495;&#30340;&#25361;&#25112;&#12290;&#34892;&#20154;&#24050;&#34987;&#26631;&#27880;&#65292;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#34701;&#21512;&#23545;&#22270;&#20687;&#20449;&#24687;&#30340;&#34917;&#20805;&#25928;&#26524;&#65292;&#25581;&#31034;&#20102;&#29616;&#26377;&#31639;&#27861;&#22312;&#38750;&#24120;&#20302;&#20809;&#26465;&#20214;&#19979;&#30340;&#19981;&#36275;&#20043;&#22788;&#12290;</title><link>http://arxiv.org/abs/2108.10831</link><description>&lt;p&gt;
LLVIP&#65306;&#36866;&#29992;&#20110;&#20302;&#20809;&#35270;&#35273;&#30340;&#21487;&#35265;-&#32418;&#22806;&#25104;&#23545;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
LLVIP: A Visible-infrared Paired Dataset for Low-light Vision. (arXiv:2108.10831v4 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2108.10831
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#20302;&#20809;&#35270;&#35273;&#30340;&#21487;&#35265;-&#32418;&#22806;&#25104;&#23545;&#25968;&#25454;&#38598;LLVIP&#65292;&#25968;&#25454;&#38598;&#21253;&#21547;30976&#24352;&#22270;&#20687;&#65292;&#35299;&#20915;&#20102;&#22312;&#20302;&#20809;&#26465;&#20214;&#19979;&#21508;&#31181;&#35270;&#35273;&#20219;&#21153;&#32570;&#22833;&#26377;&#25928;&#30446;&#26631;&#21306;&#22495;&#30340;&#25361;&#25112;&#12290;&#34892;&#20154;&#24050;&#34987;&#26631;&#27880;&#65292;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#34701;&#21512;&#23545;&#22270;&#20687;&#20449;&#24687;&#30340;&#34917;&#20805;&#25928;&#26524;&#65292;&#25581;&#31034;&#20102;&#29616;&#26377;&#31639;&#27861;&#22312;&#38750;&#24120;&#20302;&#20809;&#26465;&#20214;&#19979;&#30340;&#19981;&#36275;&#20043;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20302;&#20809;&#26465;&#20214;&#19979;&#65292;&#30001;&#20110;&#26377;&#25928;&#30446;&#26631;&#21306;&#22495;&#30340;&#20002;&#22833;&#65292;&#21508;&#31181;&#35270;&#35273;&#20219;&#21153;&#65288;&#22914;&#22270;&#20687;&#34701;&#21512;&#65292;&#34892;&#20154;&#26816;&#27979;&#21644;&#22270;&#20687;&#21040;&#22270;&#20687;&#32763;&#35793;&#65289;&#37117;&#38754;&#20020;&#24040;&#22823;&#25361;&#25112;&#12290;&#27492;&#26102;&#65292;&#21487;&#35265;&#20809;&#21644;&#32418;&#22806;&#22270;&#20687;&#21487;&#20197;&#19968;&#36215;&#20351;&#29992;&#65292;&#25552;&#20379;&#20016;&#23500;&#30340;&#32454;&#33410;&#20449;&#24687;&#21644;&#26377;&#25928;&#30340;&#30446;&#26631;&#21306;&#22495;&#12290;&#25552;&#20986;&#20102;&#19968;&#20010;&#36866;&#29992;&#20110;&#20302;&#20809;&#35270;&#35273;&#30340;&#21487;&#35265;-&#32418;&#22806;&#25104;&#23545;&#25968;&#25454;&#38598;LLVIP&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;30976&#24352;&#22270;&#20687;&#65292;&#25110;15488&#23545;&#22270;&#20687;&#65292;&#22823;&#22810;&#25968;&#22270;&#20687;&#25293;&#25668;&#20110;&#38750;&#24120;&#40657;&#26263;&#30340;&#22330;&#26223;&#65292;&#24182;&#19988;&#25152;&#26377;&#22270;&#20687;&#22312;&#26102;&#38388;&#21644;&#31354;&#38388;&#19978;&#20005;&#26684;&#23545;&#40784;&#12290;&#25968;&#25454;&#38598;&#20013;&#30340;&#34892;&#20154;&#24050;&#34987;&#26631;&#35760;&#12290;&#23558;&#25968;&#25454;&#38598;&#19982;&#20854;&#20182;&#21487;&#35265;-&#32418;&#22806;&#25968;&#25454;&#38598;&#36827;&#34892;&#27604;&#36739;&#65292;&#24182;&#22312;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#19968;&#20123;&#27969;&#34892;&#30340;&#35270;&#35273;&#31639;&#27861;&#65288;&#21253;&#25324;&#22270;&#20687;&#34701;&#21512;&#65292;&#34892;&#20154;&#26816;&#27979;&#21644;&#22270;&#20687;&#21040;&#22270;&#20687;&#32763;&#35793;&#65289;&#30340;&#24615;&#33021;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#34701;&#21512;&#23545;&#22270;&#20687;&#20449;&#24687;&#30340;&#34917;&#20805;&#25928;&#26524;&#65292;&#24182;&#21457;&#29616;&#20102;&#19977;&#20010;&#35270;&#35273;&#20219;&#21153;&#29616;&#26377;&#31639;&#27861;&#22312;&#38750;&#24120;&#20302;&#20809;&#26465;&#20214;&#19979;&#30340;&#19981;&#36275;&#20043;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
It is very challenging for various visual tasks such as image fusion, pedestrian detection and image-to-image translation in low light conditions due to the loss of effective target areas. In this case, infrared and visible images can be used together to provide both rich detail information and effective target areas. In this paper, we present LLVIP, a visible-infrared paired dataset for low-light vision. This dataset contains 30976 images, or 15488 pairs, most of which were taken at very dark scenes, and all of the images are strictly aligned in time and space. Pedestrians in the dataset are labeled. We compare the dataset with other visible-infrared datasets and evaluate the performance of some popular visual algorithms including image fusion, pedestrian detection and image-to-image translation on the dataset. The experimental results demonstrate the complementary effect of fusion on image information, and find the deficiency of existing algorithms of the three visual tasks in very l
&lt;/p&gt;</description></item></channel></rss>