<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#20849;&#36717;&#26799;&#24230;&#26679;&#24335;&#30340;&#26032;&#20248;&#21270;&#31639;&#27861;CG-like-Adam&#65292;&#29992;&#20110;&#28145;&#24230;&#23398;&#20064;&#65292;&#24182;&#22312;&#25910;&#25947;&#20998;&#26512;&#21644;&#25968;&#20540;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#20854;&#20248;&#36234;&#24615;</title><link>https://arxiv.org/abs/2404.01714</link><description>&lt;p&gt;
&#22522;&#20110;&#20849;&#36717;&#26799;&#24230;&#30340;&#33258;&#36866;&#24212;&#30697;&#20272;&#35745;&#20248;&#21270;&#31639;&#27861;&#29992;&#20110;&#28145;&#24230;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Conjugate-Gradient-like Based Adaptive Moment Estimation Optimization Algorithm for Deep Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01714
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#20849;&#36717;&#26799;&#24230;&#26679;&#24335;&#30340;&#26032;&#20248;&#21270;&#31639;&#27861;CG-like-Adam&#65292;&#29992;&#20110;&#28145;&#24230;&#23398;&#20064;&#65292;&#24182;&#22312;&#25910;&#25947;&#20998;&#26512;&#21644;&#25968;&#20540;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#20854;&#20248;&#36234;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#20026;&#21152;&#24555;&#22521;&#35757;&#36895;&#24230;&#24182;&#22686;&#24378;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#23558;&#20256;&#32479;&#30340;&#20849;&#36717;&#26799;&#24230;&#20462;&#27491;&#20026;&#20849;&#36717;&#26799;&#24230;&#26679;&#24335;&#65292;&#24182;&#23558;&#20854;&#24182;&#20837;&#36890;&#29992;Adam&#20013;&#65292;&#22240;&#27492;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CG-like-Adam&#30340;&#26032;&#20248;&#21270;&#31639;&#27861;&#29992;&#20110;&#28145;&#24230;&#23398;&#20064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#36890;&#29992;Adam&#30340;&#19968;&#38454;&#21644;&#20108;&#38454;&#30697;&#20272;&#35745;&#22343;&#30001;&#20849;&#36717;&#26799;&#24230;&#26679;&#24335;&#26367;&#25442;&#12290;&#25910;&#25947;&#20998;&#26512;&#22788;&#29702;&#20102;&#19968;&#38454;&#30697;&#20272;&#35745;&#30340;&#25351;&#25968;&#31227;&#21160;&#24179;&#22343;&#31995;&#25968;&#20026;&#24120;&#25968;&#19988;&#19968;&#38454;&#30697;&#20272;&#35745;&#26080;&#20559;&#30340;&#24773;&#20917;&#12290;&#25968;&#20540;&#23454;&#39564;&#26174;&#31034;&#20102;&#22522;&#20110;CIFAR10/100&#25968;&#25454;&#38598;&#30340;&#25152;&#25552;&#31639;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01714v1 Announce Type: cross  Abstract: Training deep neural networks is a challenging task. In order to speed up training and enhance the performance of deep neural networks, we rectify the vanilla conjugate gradient as conjugate-gradient-like and incorporate it into the generic Adam, and thus propose a new optimization algorithm named CG-like-Adam for deep learning. Specifically, both the first-order and the second-order moment estimation of generic Adam are replaced by the conjugate-gradient-like. Convergence analysis handles the cases where the exponential moving average coefficient of the first-order moment estimation is constant and the first-order moment estimation is unbiased. Numerical experiments show the superiority of the proposed algorithm based on the CIFAR10/100 dataset.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25193;&#23637;&#20102;&#24378;&#21270;&#23398;&#20064;&#21644;&#25511;&#21046;&#29702;&#35770;&#20013;&#23545;&#31216;&#25216;&#26415;&#30340;&#24212;&#29992;&#33539;&#22260;&#65292;&#36890;&#36807;&#21033;&#29992;&#21160;&#24577;&#23545;&#31216;&#24615;&#23398;&#20064;&#21160;&#21147;&#23398;&#27169;&#22411;&#65292;&#32780;&#19981;&#35201;&#27714;&#22870;&#21169;&#20855;&#26377;&#30456;&#21516;&#30340;&#23545;&#31216;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.19024</link><description>&lt;p&gt;
&#21033;&#29992;&#21160;&#24577;&#23545;&#31216;&#24615;&#36827;&#34892;&#22522;&#20110;&#27169;&#22411;&#30340;&#38750;&#23545;&#31216;&#22870;&#21169;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Exploiting Symmetry in Dynamics for Model-Based Reinforcement Learning with Asymmetric Rewards
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19024
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25193;&#23637;&#20102;&#24378;&#21270;&#23398;&#20064;&#21644;&#25511;&#21046;&#29702;&#35770;&#20013;&#23545;&#31216;&#25216;&#26415;&#30340;&#24212;&#29992;&#33539;&#22260;&#65292;&#36890;&#36807;&#21033;&#29992;&#21160;&#24577;&#23545;&#31216;&#24615;&#23398;&#20064;&#21160;&#21147;&#23398;&#27169;&#22411;&#65292;&#32780;&#19981;&#35201;&#27714;&#22870;&#21169;&#20855;&#26377;&#30456;&#21516;&#30340;&#23545;&#31216;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20013;&#26368;&#36817;&#30340;&#24037;&#20316;&#21033;&#29992;&#27169;&#22411;&#20013;&#30340;&#23545;&#31216;&#24615;&#26469;&#25552;&#39640;&#31574;&#30053;&#35757;&#32451;&#30340;&#37319;&#26679;&#25928;&#29575;&#12290;&#19968;&#20010;&#24120;&#29992;&#30340;&#31616;&#21270;&#20551;&#35774;&#26159;&#21160;&#21147;&#23398;&#21644;&#22870;&#21169;&#37117;&#34920;&#29616;&#20986;&#30456;&#21516;&#30340;&#23545;&#31216;&#24615;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#30495;&#23454;&#29615;&#22659;&#20013;&#65292;&#21160;&#21147;&#23398;&#27169;&#22411;&#34920;&#29616;&#20986;&#19982;&#22870;&#21169;&#27169;&#22411;&#29420;&#31435;&#30340;&#23545;&#31216;&#24615;&#65306;&#22870;&#21169;&#21487;&#33021;&#19981;&#28385;&#36275;&#19982;&#21160;&#21147;&#23398;&#30456;&#21516;&#30340;&#23545;&#31216;&#24615;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#21482;&#20551;&#23450;&#21160;&#21147;&#23398;&#34920;&#29616;&#20986;&#23545;&#31216;&#24615;&#30340;&#24773;&#20917;&#65292;&#25193;&#23637;&#20102;&#24378;&#21270;&#23398;&#20064;&#21644;&#25511;&#21046;&#29702;&#35770;&#23398;&#20064;&#20013;&#21487;&#24212;&#29992;&#23545;&#31216;&#25216;&#26415;&#30340;&#38382;&#39064;&#33539;&#22260;&#12290;&#25105;&#20204;&#21033;&#29992;&#21345;&#22612;&#24681;&#31227;&#21160;&#26694;&#26550;&#26041;&#27861;&#24341;&#20837;&#19968;&#31181;&#23398;&#20064;&#21160;&#21147;&#23398;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#26500;&#36896;&#65292;&#36825;&#31181;&#21160;&#21147;&#23398;&#34920;&#29616;&#20986;&#25351;&#23450;&#30340;&#23545;&#31216;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#23398;&#21040;&#20102;&#26356;&#20934;&#30830;&#30340;&#21160;&#24577;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19024v1 Announce Type: cross  Abstract: Recent work in reinforcement learning has leveraged symmetries in the model to improve sample efficiency in training a policy. A commonly used simplifying assumption is that the dynamics and reward both exhibit the same symmetry. However, in many real-world environments, the dynamical model exhibits symmetry independent of the reward model: the reward may not satisfy the same symmetries as the dynamics. In this paper, we investigate scenarios where only the dynamics are assumed to exhibit symmetry, extending the scope of problems in reinforcement learning and learning in control theory where symmetry techniques can be applied. We use Cartan's moving frame method to introduce a technique for learning dynamics which, by construction, exhibit specified symmetries. We demonstrate through numerical experiments that the proposed method learns a more accurate dynamical model.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#36870;&#21521;&#35757;&#32451;&#30340;&#26367;&#20195;&#35757;&#32451;&#26041;&#26696;&#65292;&#36890;&#36807;&#22312;&#27491;&#21521;&#21644;&#36870;&#21521;&#26041;&#21521;&#19978;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#24182;&#20445;&#30041;&#36873;&#23450;&#23376;&#20018;&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38754;&#20020;&#30340;&#36870;&#36716;&#35781;&#21650;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.13799</link><description>&lt;p&gt;
&#36870;&#21521;&#35757;&#32451;&#20197;&#28040;&#38500;&#36870;&#36716;&#35781;&#21650;
&lt;/p&gt;
&lt;p&gt;
Reverse Training to Nurse the Reversal Curse
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13799
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#36870;&#21521;&#35757;&#32451;&#30340;&#26367;&#20195;&#35757;&#32451;&#26041;&#26696;&#65292;&#36890;&#36807;&#22312;&#27491;&#21521;&#21644;&#36870;&#21521;&#26041;&#21521;&#19978;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#24182;&#20445;&#30041;&#36873;&#23450;&#23376;&#20018;&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38754;&#20020;&#30340;&#36870;&#36716;&#35781;&#21650;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23384;&#22312;&#19968;&#20010;&#20196;&#20154;&#24778;&#35766;&#30340;&#22833;&#36133;&#29616;&#35937;&#65306;&#24403;&#35757;&#32451;&#27169;&#22411;&#20197;"A&#20855;&#26377;&#29305;&#24449;B"&#20026;&#22522;&#30784;&#26102;&#65292;&#23427;&#20204;&#26080;&#27861;&#27867;&#21270;&#21040;"B&#26159;A&#30340;&#29305;&#24449;"&#65292;&#36825;&#34987;&#31216;&#20026;&#36870;&#36716;&#35781;&#21650;&#12290;&#21363;&#20351;&#22312;&#20351;&#29992;&#25968;&#19975;&#20159;&#20196;&#29260;&#36827;&#34892;&#35757;&#32451;&#26102;&#65292;&#30001;&#20110;&#40784;&#22827;&#23450;&#24459;&#30340;&#23384;&#22312;&#65292;&#36825;&#20010;&#38382;&#39064;&#20173;&#28982;&#23384;&#22312;&#65292;&#36825;&#24847;&#21619;&#30528;&#21363;&#20351;&#25105;&#20204;&#22312;&#25972;&#20010;&#20114;&#32852;&#32593;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#35813;&#38382;&#39064;&#20173;&#28982;&#20250;&#20986;&#29616;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#36870;&#21521;&#35757;&#32451;&#30340;&#26367;&#20195;&#35757;&#32451;&#26041;&#26696;&#65292;&#22312;&#20854;&#20013;&#25152;&#26377;&#21333;&#35789;&#34987;&#20351;&#29992;&#20004;&#27425;&#65292;&#20174;&#32780;&#20351;&#21487;&#29992;&#20196;&#29260;&#25968;&#37327;&#21152;&#20493;&#12290;&#35813;LLM&#22312;&#27491;&#21521;&#21644;&#36870;&#21521;&#26041;&#21521;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#36890;&#36807;&#39072;&#20498;&#35757;&#32451;&#23383;&#31526;&#20018;&#26469;&#39072;&#20498;&#35757;&#32451;&#36807;&#31243;&#65292;&#21516;&#26102;&#20445;&#30041;&#65288;&#21363;&#19981;&#39072;&#20498;&#65289;&#36873;&#23450;&#30340;&#23376;&#20018;&#65292;&#22914;&#23454;&#20307;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25968;&#25454;&#21305;&#37197;&#30340;&#36870;&#21521;&#35757;&#32451;&#27169;&#22411;&#22312;&#26631;&#20934;&#20219;&#21153;&#19978;&#27604;&#26631;&#20934;&#27169;&#22411;&#34920;&#29616;&#26356;&#20248;&#31168;&#65292;&#24182;&#19988;&#35745;&#31639;&#21305;&#37197;&#30340;&#36870;&#21521;&#35757;&#32451;&#27169;&#22411;&#22312;&#36870;&#36716;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#36828;&#36828;&#20248;&#20110;&#26631;&#20934;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#26377;&#21161;&#20110;&#35299;&#20915;&#36870;&#36716;&#35781;&#21650;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13799v1 Announce Type: new  Abstract: Large language models (LLMs) have a surprising failure: when trained on "A has a feature B", they do not generalize to "B is a feature of A", which is termed the Reversal Curse. Even when training with trillions of tokens this issue still appears due to Zipf's law - hence even if we train on the entire internet. This work proposes an alternative training scheme, called reverse training, whereby all words are used twice, doubling the amount of available tokens. The LLM is trained in both forward and reverse directions by reversing the training strings while preserving (i.e., not reversing) chosen substrings, such as entities. We show that data-matched reverse-trained models provide superior performance to standard models on standard tasks, and compute-matched reverse-trained models provide far superior performance on reversal tasks, helping resolve the reversal curse issue.
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#21462;&#21407;&#22240;&#29305;&#24449;&#35757;&#32451;&#20167;&#24680;&#35328;&#35770;&#20998;&#31867;&#22120;&#65292;&#23454;&#29616;&#21487;&#35299;&#37322;&#30340;&#26816;&#27979;&#26041;&#27861;</title><link>https://arxiv.org/abs/2403.12403</link><description>&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#21462;&#30340;&#21407;&#22240;&#29983;&#25104;&#21487;&#35299;&#37322;&#30340;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Towards Interpretable Hate Speech Detection using Large Language Model-extracted Rationales
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12403
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#21462;&#21407;&#22240;&#29305;&#24449;&#35757;&#32451;&#20167;&#24680;&#35328;&#35770;&#20998;&#31867;&#22120;&#65292;&#23454;&#29616;&#21487;&#35299;&#37322;&#30340;&#26816;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#26159;&#29992;&#25143;&#36827;&#34892;&#20154;&#38469;&#35752;&#35770;&#21644;&#34920;&#36798;&#35266;&#28857;&#30340;&#37325;&#35201;&#22330;&#25152;&#65292;&#20294;&#31038;&#20132;&#23186;&#20307;&#25552;&#20379;&#30340;&#22806;&#31435;&#38754;&#21644;&#21311;&#21517;&#24615;&#21487;&#33021;&#23548;&#33268;&#29992;&#25143;&#21457;&#24067;&#20167;&#24680;&#35328;&#35770;&#21644;&#20882;&#29359;&#24615;&#20869;&#23481;&#12290;&#37492;&#20110;&#36825;&#20123;&#24179;&#21488;&#30340;&#24222;&#22823;&#35268;&#27169;&#65292;&#33258;&#21160;&#35782;&#21035;&#21644;&#26631;&#35760;&#20167;&#24680;&#35328;&#35770;&#30340;&#38656;&#27714;&#26085;&#30410;&#36843;&#20999;&#12290;&#23613;&#31649;&#23384;&#22312;&#20960;&#31181;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#26041;&#27861;&#65292;&#20294;&#22823;&#22810;&#25968;&#40657;&#30418;&#26041;&#27861;&#22312;&#35774;&#35745;&#19978;&#19981;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#25110;&#21487;&#35299;&#37322;&#24615;&#12290;&#20026;&#35299;&#20915;&#35299;&#37322;&#24615;&#19981;&#36275;&#65292;&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20174;&#36755;&#20837;&#25991;&#26412;&#20013;&#25552;&#21462;&#21407;&#22240;&#29305;&#24449;&#65292;&#35757;&#32451;&#22522;&#30784;&#20167;&#24680;&#35328;&#35770;&#20998;&#31867;&#22120;&#65292;&#20174;&#32780;&#36890;&#36807;&#35774;&#35745;&#23454;&#29616;&#24544;&#23454;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#26377;&#25928;&#22320;&#32467;&#21512;&#20102;LLM&#30340;&#25991;&#26412;&#29702;&#35299;&#33021;&#21147;&#21644;&#26368;&#20808;&#36827;&#20167;&#24680;&#35328;&#35770;&#20998;&#31867;&#22120;&#30340;&#21028;&#21035;&#33021;&#21147;&#65292;&#20351;&#36825;&#20123;&#20998;&#31867;&#22120;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12403v1 Announce Type: cross  Abstract: Although social media platforms are a prominent arena for users to engage in interpersonal discussions and express opinions, the facade and anonymity offered by social media may allow users to spew hate speech and offensive content. Given the massive scale of such platforms, there arises a need to automatically identify and flag instances of hate speech. Although several hate speech detection methods exist, most of these black-box methods are not interpretable or explainable by design. To address the lack of interpretability, in this paper, we propose to use state-of-the-art Large Language Models (LLMs) to extract features in the form of rationales from the input text, to train a base hate speech classifier, thereby enabling faithful interpretability by design. Our framework effectively combines the textual understanding capabilities of LLMs and the discriminative power of state-of-the-art hate speech classifiers to make these classifi
&lt;/p&gt;</description></item><item><title>&#33258;&#21160;&#39550;&#39542;&#20013;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#30340;&#23433;&#20840;&#24433;&#21709;&#23545;&#20110;&#30830;&#20445;&#36710;&#36742;&#33258;&#21160;&#21270;&#23433;&#20840;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#24403;&#21069;&#30740;&#31350;&#20013;&#23433;&#20840;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#24448;&#24448;&#34987;&#20998;&#24320;&#30752;&#12290;</title><link>https://arxiv.org/abs/2403.12176</link><description>&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#20013;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#30340;&#23433;&#20840;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Safety Implications of Explainable Artificial Intelligence in End-to-End Autonomous Driving
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12176
&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#20013;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#30340;&#23433;&#20840;&#24433;&#21709;&#23545;&#20110;&#30830;&#20445;&#36710;&#36742;&#33258;&#21160;&#21270;&#23433;&#20840;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#24403;&#21069;&#30740;&#31350;&#20013;&#23433;&#20840;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#24448;&#24448;&#34987;&#20998;&#24320;&#30752;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26411;&#31471;&#21040;&#26411;&#31471;&#23398;&#20064;&#31649;&#36947;&#27491;&#22312;&#36880;&#28176;&#25913;&#21464;&#39640;&#24230;&#33258;&#20027;&#36710;&#36742;&#30340;&#25345;&#32493;&#21457;&#23637;&#65292;&#36825;&#20027;&#35201;&#24402;&#21151;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#36827;&#27493;&#12289;&#22823;&#35268;&#27169;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#21487;&#29992;&#24615;&#20197;&#21450;&#32508;&#21512;&#20256;&#24863;&#22120;&#35774;&#22791;&#30340;&#25913;&#36827;&#12290;&#28982;&#32780;&#65292;&#24403;&#20195;&#23398;&#20064;&#26041;&#27861;&#22312;&#23454;&#26102;&#20915;&#31574;&#20013;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#65292;&#22952;&#30861;&#20102;&#29992;&#25143;&#30340;&#20449;&#20219;&#65292;&#24182;&#20943;&#24369;&#20102;&#36825;&#31867;&#36710;&#36742;&#30340;&#24191;&#27867;&#37096;&#32626;&#21644;&#21830;&#19994;&#21270;&#12290;&#27492;&#22806;&#65292;&#24403;&#36825;&#20123;&#27773;&#36710;&#21442;&#19982;&#25110;&#23548;&#33268;&#20132;&#36890;&#20107;&#25925;&#26102;&#65292;&#38382;&#39064;&#20250;&#21464;&#24471;&#26356;&#21152;&#20005;&#37325;&#12290;&#36825;&#31181;&#32570;&#28857;&#20174;&#31038;&#20250;&#21644;&#27861;&#24459;&#30340;&#35282;&#24230;&#24341;&#36215;&#20102;&#20005;&#37325;&#30340;&#23433;&#20840;&#25285;&#24551;&#12290;&#22240;&#27492;&#65292;&#22312;&#26411;&#31471;&#21040;&#26411;&#31471;&#33258;&#21160;&#39550;&#39542;&#20013;&#35299;&#37322;&#24615;&#26159;&#20419;&#36827;&#36710;&#36742;&#33258;&#21160;&#21270;&#23433;&#20840;&#30340;&#20851;&#38190;&#12290;&#28982;&#32780;&#65292;&#24403;&#20170;&#26368;&#20808;&#36827;&#25216;&#26415;&#20013;&#30740;&#31350;&#20154;&#21592;&#36890;&#24120;&#23558;&#33258;&#21160;&#39550;&#39542;&#30340;&#23433;&#20840;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#20998;&#24320;&#30740;&#31350;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12176v1 Announce Type: cross  Abstract: The end-to-end learning pipeline is gradually creating a paradigm shift in the ongoing development of highly autonomous vehicles, largely due to advances in deep learning, the availability of large-scale training datasets, and improvements in integrated sensor devices. However, a lack of interpretability in real-time decisions with contemporary learning methods impedes user trust and attenuates the widespread deployment and commercialization of such vehicles. Moreover, the issue is exacerbated when these cars are involved in or cause traffic accidents. Such drawback raises serious safety concerns from societal and legal perspectives. Consequently, explainability in end-to-end autonomous driving is essential to enable the safety of vehicular automation. However, the safety and explainability aspects of autonomous driving have generally been investigated disjointly by researchers in today's state of the art. In this paper, we aim to brid
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#35780;&#20272;&#38544;&#31169;&#25919;&#31574;&#30340;&#20844;&#24179;&#24615;&#65292;&#36890;&#36807;&#20174;&#27861;&#24459;&#26469;&#28304;&#21644;&#20844;&#24179;&#24615;&#30740;&#31350;&#20013;&#30830;&#23450;&#20449;&#24687;&#20844;&#27491;&#24615;&#12289;&#34920;&#29616;&#20844;&#27491;&#24615;&#21644;&#36947;&#24503;/&#20262;&#29702;&#19982;&#38544;&#31169;&#25919;&#31574;&#30340;&#20851;&#31995;&#65292;&#24182;&#25552;&#20986;&#33258;&#21160;&#35780;&#20272;&#25919;&#31574;&#30340;&#36873;&#39033;&#12290;</title><link>https://arxiv.org/abs/2403.08115</link><description>&lt;p&gt;
&#27861;&#24459;&#32422;&#26463;&#20294;&#19981;&#20844;&#24179;&#65311;&#26397;&#21521;&#35780;&#20272;&#38544;&#31169;&#25919;&#31574;&#20844;&#24179;&#24615;&#30340;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;
Legally Binding but Unfair? Towards Assessing Fairness of Privacy Policies
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08115
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#35780;&#20272;&#38544;&#31169;&#25919;&#31574;&#30340;&#20844;&#24179;&#24615;&#65292;&#36890;&#36807;&#20174;&#27861;&#24459;&#26469;&#28304;&#21644;&#20844;&#24179;&#24615;&#30740;&#31350;&#20013;&#30830;&#23450;&#20449;&#24687;&#20844;&#27491;&#24615;&#12289;&#34920;&#29616;&#20844;&#27491;&#24615;&#21644;&#36947;&#24503;/&#20262;&#29702;&#19982;&#38544;&#31169;&#25919;&#31574;&#30340;&#20851;&#31995;&#65292;&#24182;&#25552;&#20986;&#33258;&#21160;&#35780;&#20272;&#25919;&#31574;&#30340;&#36873;&#39033;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38544;&#31169;&#25919;&#31574;&#24212;&#24403;&#21578;&#30693;&#25968;&#25454;&#20027;&#20307;&#20854;&#25968;&#25454;&#20445;&#25252;&#26435;&#21033;&#65292;&#35299;&#37322;&#25968;&#25454;&#25511;&#21046;&#32773;&#30340;&#25968;&#25454;&#31649;&#29702;&#23454;&#36341;&#65292;&#24182;&#20351;&#20445;&#30041;&#26399;&#38480;&#25110;&#25968;&#25454;&#36716;&#31227;&#32473;&#31532;&#19977;&#26041;&#31561;&#20107;&#23454;&#36879;&#26126;&#21270;&#12290;&#38544;&#31169;&#25919;&#31574;&#21482;&#26377;&#22312;&#25968;&#25454;&#20027;&#20307;&#27491;&#30830;&#24863;&#30693;&#12289;&#35299;&#37322;&#12289;&#29702;&#35299;&#21644;&#20449;&#20219;&#26102;&#25165;&#33021;&#23454;&#29616;&#20854;&#30446;&#30340;&#12290;&#20854;&#20013;&#65292;&#36825;&#35201;&#27714;&#38544;&#31169;&#25919;&#31574;&#20197;&#20844;&#24179;&#30340;&#26041;&#24335;&#32534;&#20889;&#65292;&#20363;&#22914;&#19981;&#20351;&#29992;&#26497;&#31471;&#30340;&#26415;&#35821;&#65292;&#19981;&#38656;&#35201;&#29305;&#23450;&#30340;&#25945;&#32946;&#31243;&#24230;&#65292;&#25110;&#19981;&#20551;&#35774;&#29305;&#23450;&#30340;&#31038;&#20250;&#32972;&#26223;&#12290;&#22312;&#36825;&#20221;&#36827;&#34892;&#20013;&#30340;&#24037;&#20316;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#27010;&#36848;&#20102;&#25105;&#20204;&#35780;&#20272;&#38544;&#31169;&#25919;&#31574;&#20844;&#24179;&#24615;&#30340;&#26041;&#27861;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20174;&#22522;&#26412;&#27861;&#24459;&#26469;&#28304;&#21644;&#20844;&#24179;&#24615;&#30740;&#31350;&#20013;&#30830;&#23450;&#20449;&#24687;&#20844;&#27491;&#24615;&#12289;&#34920;&#29616;&#20844;&#27491;&#24615;&#21644;&#36947;&#24503;/&#20262;&#29702;&#19982;&#38544;&#31169;&#25919;&#31574;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#33258;&#21160;&#35780;&#20272;&#25919;&#31574;&#30340;&#36873;&#39033;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08115v1 Announce Type: cross  Abstract: Privacy policies are expected to inform data subjects about their data protection rights. They should explain the data controller's data management practices, and make facts such as retention periods or data transfers to third parties transparent. Privacy policies only fulfill their purpose, if they are correctly perceived, interpreted, understood, and trusted by the data subject. Amongst others, this requires that a privacy policy is written in a fair way, e.g., it does not use polarizing terms, does not require a certain education, or does not assume a particular social background. In this work-in-progress paper, we outline our approach to assessing fairness in privacy policies. To this end, we identify from fundamental legal sources and fairness research, how the dimensions informational fairness, representational fairness and ethics/morality are related to privacy policies. We propose options to automatically assess policies in the
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#22522;&#20110;&#20851;&#38190;&#28857;&#39537;&#21160;&#30340;&#25968;&#25454;&#21512;&#25104;&#26694;&#26550;(KPDDS)&#65292;&#21019;&#36896;&#20102;&#36804;&#20170;&#20026;&#27490;&#26368;&#22823;&#35268;&#27169;&#30340;&#29992;&#20110;&#25968;&#23398;&#25512;&#29702;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;KPMath&#65292;&#20197;&#21450;&#36827;&#19968;&#27493;&#22686;&#24378;&#30340;KPMath-Plus&#25968;&#25454;&#38598;&#65292;&#23454;&#29616;&#20102;&#38646;-shot PASS@1&#31934;&#24230;&#20026;39.3%&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2403.02333</link><description>&lt;p&gt;
&#22522;&#20110;&#20851;&#38190;&#28857;&#39537;&#21160;&#30340;&#25968;&#25454;&#21512;&#25104;&#21450;&#20854;&#22312;&#25968;&#23398;&#25512;&#29702;&#19978;&#30340;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Key-Point-Driven Data Synthesis with its Enhancement on Mathematical Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02333
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#22522;&#20110;&#20851;&#38190;&#28857;&#39537;&#21160;&#30340;&#25968;&#25454;&#21512;&#25104;&#26694;&#26550;(KPDDS)&#65292;&#21019;&#36896;&#20102;&#36804;&#20170;&#20026;&#27490;&#26368;&#22823;&#35268;&#27169;&#30340;&#29992;&#20110;&#25968;&#23398;&#25512;&#29702;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;KPMath&#65292;&#20197;&#21450;&#36827;&#19968;&#27493;&#22686;&#24378;&#30340;KPMath-Plus&#25968;&#25454;&#38598;&#65292;&#23454;&#29616;&#20102;&#38646;-shot PASS@1&#31934;&#24230;&#20026;39.3%&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#20013;&#26174;&#31034;&#20986;&#24040;&#22823;&#28508;&#21147;&#65292;&#20294;&#20854;&#24615;&#33021;&#36890;&#24120;&#21463;&#21040;&#39640;&#36136;&#37327;&#12289;&#20197;&#25512;&#29702;&#20026;&#37325;&#28857;&#30340;&#35757;&#32451;&#25968;&#25454;&#31232;&#32570;&#30340;&#24433;&#21709;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#20851;&#38190;&#28857;&#39537;&#21160;&#30340;&#25968;&#25454;&#21512;&#25104;&#65288;KPDDS&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#25968;&#25454;&#21512;&#25104;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#26469;&#33258;&#30495;&#23454;&#25968;&#25454;&#28304;&#30340;&#20851;&#38190;&#28857;&#21644;&#31034;&#20363;&#23545;&#29983;&#25104;&#38382;&#39064;-&#31572;&#26696;&#23545;&#12290;KPDDS&#30830;&#20445;&#36890;&#36807;&#20005;&#26684;&#30340;&#36136;&#37327;&#25511;&#21046;&#21644;&#22823;&#35268;&#27169;&#24615;&#33021;&#30340;&#29983;&#25104;&#26032;&#39062;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;KPMath&#65292;&#36804;&#20170;&#20026;&#27490;&#37327;&#36523;&#23450;&#21046;&#30340;&#26368;&#24191;&#27867;&#30340;&#29992;&#20110;&#25968;&#23398;&#25512;&#29702;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#19968;&#30334;&#19975;&#20010;&#20197;&#19978;&#30340;&#38382;&#39064;-&#31572;&#26696;&#23545;&#12290;&#21033;&#29992;KPMath&#24182;&#23558;&#20854;&#19982;&#20854;&#20182;&#25512;&#29702;&#23494;&#38598;&#30340;&#35821;&#26009;&#24211;&#36827;&#34892;&#25193;&#20805;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#20840;&#38754;&#30340;KPMath-Plus&#25968;&#25454;&#38598;&#12290;&#23558;Mistral-7B&#27169;&#22411;&#22312;KPMath-Plus&#19978;&#24494;&#35843;&#65292;&#20351;&#20854;&#22312;MATH&#27979;&#35797;&#38598;&#19978;&#23454;&#29616;&#38646;-shot PASS@1&#31934;&#24230;&#36798;&#21040;39.3%&#65292;&#36825;&#26159;&#19968;&#39033;&#31361;&#30772;&#24615;&#25104;&#23601;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02333v1 Announce Type: cross  Abstract: Large language models (LLMs) have shown great potential in complex reasoning tasks, yet their performance is often hampered by the scarcity of high-quality, reasoning-focused training datasets. Addressing this challenge, we propose Key-Point-Driven Data Synthesis (KPDDS), a novel data synthesis framework that synthesizes question-answer pairs by leveraging key points and exemplar pairs from authentic data sources. KPDDS ensures the generation of novel questions with rigorous quality control and substantial scalability. As a result, we present KPMath, the most extensive synthetic dataset tailored for mathematical reasoning to date, comprising over one million question-answer pairs. Utilizing KPMath and augmenting it with additional reasoning-intensive corpora, we create the comprehensive KPMath-Plus dataset. Fine-tuning the Mistral-7B model on KPMath-Plus yields a zero-shot PASS@1 accuracy of 39.3% on the MATH test set, a performance th
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;-free &#26041;&#27861; ToDo&#65292;&#36890;&#36807;&#20196;&#29260;&#19979;&#37319;&#26679;&#21152;&#36895; Stable Diffusion &#25512;&#29702;&#65292;&#20197;&#23454;&#29616;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#30340;&#39640;&#25928;&#29983;&#25104;&#12290;</title><link>https://arxiv.org/abs/2402.13573</link><description>&lt;p&gt;
&#20219;&#21153;&#24453;&#21150;&#65306;&#29992;&#20110;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#39640;&#25928;&#29983;&#25104;&#30340;&#20196;&#29260;&#19979;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
ToDo: Token Downsampling for Efficient Generation of High-Resolution Images
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13573
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;-free &#26041;&#27861; ToDo&#65292;&#36890;&#36807;&#20196;&#29260;&#19979;&#37319;&#26679;&#21152;&#36895; Stable Diffusion &#25512;&#29702;&#65292;&#20197;&#23454;&#29616;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#30340;&#39640;&#25928;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27880;&#24847;&#21147;&#26426;&#21046;&#23545;&#20110;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#33267;&#20851;&#37325;&#35201;&#65292;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#20108;&#27425;&#35745;&#31639;&#22797;&#26434;&#24615;&#38480;&#21046;&#20102;&#25105;&#20204;&#21487;&#20197;&#22312;&#21512;&#29702;&#26102;&#38388;&#21644;&#20869;&#23384;&#38480;&#21046;&#20869;&#22788;&#29702;&#30340;&#22270;&#20687;&#22823;&#23567;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#29983;&#25104;&#22270;&#20687;&#27169;&#22411;&#20013;&#23494;&#38598;&#27880;&#24847;&#21147;&#30340;&#37325;&#35201;&#24615;&#65292;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#21253;&#21547;&#20887;&#20313;&#29305;&#24449;&#65292;&#20351;&#23427;&#20204;&#36866;&#21512;&#31232;&#30095;&#27880;&#24847;&#21147;&#26426;&#21046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26080;&#38656;&#35757;&#32451;&#30340;&#26041;&#27861; ToDo&#65292;&#35813;&#26041;&#27861;&#20381;&#36182;&#20110;&#20851;&#38190;&#21644;&#20540;&#20196;&#29260;&#30340;&#20196;&#29260;&#19979;&#37319;&#26679;&#65292;&#21487;&#23558;&#24120;&#35265;&#22823;&#23567;&#30340; Stable Diffusion &#25512;&#29702;&#21152;&#36895;&#33267;&#22810;&#36798;2&#20493;&#65292;&#23545;&#20110;2048x2048&#31561;&#39640;&#20998;&#36776;&#29575;&#65292;&#21152;&#36895;&#27604;&#21487;&#36798;4.5&#20493;&#25110;&#26356;&#39640;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#24179;&#34913;&#39640;&#25928;&#21534;&#21520;&#37327;&#21644;&#20445;&#30495;&#24230;&#26041;&#38754;&#20248;&#20110;&#20808;&#21069;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13573v1 Announce Type: cross  Abstract: Attention mechanism has been crucial for image diffusion models, however, their quadratic computational complexity limits the sizes of images we can process within reasonable time and memory constraints. This paper investigates the importance of dense attention in generative image models, which often contain redundant features, making them suitable for sparser attention mechanisms. We propose a novel training-free method ToDo that relies on token downsampling of key and value tokens to accelerate Stable Diffusion inference by up to 2x for common sizes and up to 4.5x or more for high resolutions like 2048x2048. We demonstrate that our approach outperforms previous methods in balancing efficient throughput and fidelity.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#24046;&#20998;&#38544;&#31169;&#38646;&#38454;&#26041;&#27861;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#20013;&#30340;&#24212;&#29992;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#38646;&#38454;&#26799;&#24230;&#26469;&#36991;&#20813;&#20256;&#32479;&#20248;&#21270;&#26041;&#27861;&#30340;&#21487;&#25193;&#23637;&#24615;&#29942;&#39048;&#65292;&#23454;&#29616;&#20102;&#22312;&#38544;&#31169;&#12289;&#25928;&#29992;&#21644;&#21487;&#25193;&#23637;&#24615;&#20043;&#38388;&#30340;&#33391;&#22909;&#24179;&#34913;&#12290;</title><link>https://arxiv.org/abs/2402.07818</link><description>&lt;p&gt;
&#21487;&#25193;&#23637;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#30340;&#24046;&#20998;&#38544;&#31169;&#38646;&#38454;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Differentially Private Zeroth-Order Methods for Scalable Large Language Model Finetuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07818
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24046;&#20998;&#38544;&#31169;&#38646;&#38454;&#26041;&#27861;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#20013;&#30340;&#24212;&#29992;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#38646;&#38454;&#26799;&#24230;&#26469;&#36991;&#20813;&#20256;&#32479;&#20248;&#21270;&#26041;&#27861;&#30340;&#21487;&#25193;&#23637;&#24615;&#29942;&#39048;&#65292;&#23454;&#29616;&#20102;&#22312;&#38544;&#31169;&#12289;&#25928;&#29992;&#21644;&#21487;&#25193;&#23637;&#24615;&#20043;&#38388;&#30340;&#33391;&#22909;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29305;&#23450;&#20219;&#21153;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#26159;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#24378;&#22823;&#33021;&#21147;&#36827;&#34892;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#30340;&#24191;&#27867;&#25509;&#21463;&#30340;&#33539;&#20363;&#12290;&#30001;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#30340;&#26222;&#21450;&#20197;&#21450;&#19982;&#20043;&#30456;&#20851;&#30340;&#38544;&#31169;&#38382;&#39064;&#65292;&#24046;&#20998;&#38544;&#31169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#20197;&#20445;&#25252;&#29305;&#23450;&#20219;&#21153;&#25968;&#25454;&#38598;&#30340;&#38544;&#31169;&#12290;&#24046;&#20998;&#38544;&#31169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#26041;&#27861;&#30340;&#35774;&#35745;&#26680;&#24515;&#26159;&#22312;&#38544;&#31169;&#12289;&#25928;&#29992;&#21644;&#21487;&#25193;&#23637;&#24615;&#20043;&#38388;&#36798;&#21040;&#28385;&#24847;&#30340;&#26435;&#34913;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#37117;&#26159;&#22522;&#20110;DP-SGD&#30340;&#21019;&#26032;&#24615;&#24037;&#20316;&#12290;&#23613;&#31649;&#23558;DP-SGD&#30340;&#21487;&#25193;&#23637;&#24615;&#25512;&#21040;&#20102;&#26497;&#38480;&#65292;&#20294;&#22522;&#20110;DP-SGD&#30340;&#24494;&#35843;&#26041;&#27861;&#19981;&#24184;&#22320;&#21463;&#21040;&#20102;SGD&#22266;&#26377;&#20302;&#25928;&#29575;&#30340;&#38480;&#21046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;DP&#38646;&#38454;&#26041;&#27861;&#22312;LLM&#39044;&#35757;&#32451;&#20013;&#30340;&#28508;&#21147;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#29992;&#26356;&#39640;&#25928;&#30340;&#38646;&#38454;&#26799;&#24230;&#26469;&#36817;&#20284;&#26799;&#24230;&#65292;&#36991;&#20813;&#20102;SGD&#30340;&#21487;&#25193;&#23637;&#24615;&#29942;&#39048;&#12290;&#19982;&#23558;&#38646;&#38454;&#26041;&#27861;&#20316;&#20026;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#36827;&#34892;&#22788;&#29702;&#19981;&#21516;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#21106;&#25509;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#20197;&#38750;&#24120;&#25509;&#36817;&#30340;&#26041;&#24335;&#27169;&#25311;DP-SGD&#30340;&#22522;&#26412;&#25805;&#20316;&#65292;&#28982;&#21518;&#21033;&#29992;&#38646;&#38454;&#20248;&#21270;&#26041;&#27861;&#26469;&#36817;&#20284;&#26799;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Finetuning on task-specific datasets is a widely-embraced paradigm of harnessing the powerful capability of pretrained LLMs for various downstream tasks. Due to the popularity of LLMs finetuning and its accompanying privacy concerns, differentially private (DP) finetuning of pretrained LLMs has garnered increasing attention to safeguarding the privacy of task-specific datasets. Lying at the design core of DP LLM finetuning methods is the satisfactory tradeoff between privacy, utility, and scalability. Most existing methods build upon the seminal work of DP-SGD. Despite pushing the scalability of DP-SGD to its limit, DP-SGD-based finetuning methods are unfortunately limited by the inherent inefficiency of SGD. In this paper, we investigate the potential of DP zeroth-order methods for LLM pretraining, which avoids the scalability bottleneck of SGD by approximating the gradient with the more efficient zeroth-order gradient. Rather than treating the zeroth-order method as a drop-in replace
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#26681;&#33550;&#26500;&#36896;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#22270;&#20013;&#39640;&#20837;&#24230;&#20998;&#24067;&#23548;&#33268;&#30340;&#36127;&#36733;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#22312;&#22823;&#35268;&#27169;&#33455;&#29255;&#19978;&#23545;BFS&#22270;&#36941;&#21382;&#24615;&#33021;&#36827;&#34892;&#20102;&#21152;&#36895;&#21644;&#20248;&#21270;&#12290;</title><link>https://arxiv.org/abs/2402.06086</link><description>&lt;p&gt;
&#29992;&#26681;&#33550;&#26469;&#24179;&#34913;&#20559;&#26012;&#30340;&#20837;&#24230;&#20998;&#24067;
&lt;/p&gt;
&lt;p&gt;
Rhizomes to Load Balance Skewed In-Degree Distributions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06086
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#26681;&#33550;&#26500;&#36896;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#22270;&#20013;&#39640;&#20837;&#24230;&#20998;&#24067;&#23548;&#33268;&#30340;&#36127;&#36733;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#22312;&#22823;&#35268;&#27169;&#33455;&#29255;&#19978;&#23545;BFS&#22270;&#36941;&#21382;&#24615;&#33021;&#36827;&#34892;&#20102;&#21152;&#36895;&#21644;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#23558;&#26681;&#33550;&#30340;&#27010;&#24565;&#24212;&#29992;&#20110;&#22522;&#20110;&#39030;&#28857;&#20026;&#20013;&#24515;&#30340;&#28040;&#24687;&#39537;&#21160;&#22270;&#22788;&#29702;&#65292;&#35299;&#20915;&#30001;&#22270;&#20013;&#39640;&#20837;&#24230;&#20998;&#24067;&#24341;&#36215;&#30340;&#36127;&#36733;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;&#22270;&#30340;&#26681;&#33550;&#26500;&#36896;&#20026;&#20219;&#20309;&#25968;&#37327;&#30340;&#21333;&#20010;&#22823;&#20837;&#24230;&#39030;&#28857;&#21019;&#24314;&#20102;&#22810;&#20010;&#21629;&#21517;&#39030;&#28857;&#22320;&#22336;&#12290;&#28982;&#21518;&#65292;&#20854;&#20182;&#39030;&#28857;&#21487;&#20197;&#25351;&#21521;&#20219;&#20309;&#19968;&#20010;&#21629;&#21517;&#22320;&#22336;&#65292;&#20174;&#32780;&#20849;&#20139;&#20837;&#24230;&#36127;&#36733;&#12290;&#26681;&#33550;&#20869;&#37096;&#36827;&#34892;&#36890;&#20449;&#24182;&#20445;&#25345;&#19968;&#33268;&#65292;&#20197;&#25552;&#20379;&#39030;&#28857;&#30340;&#32479;&#19968;&#21644;&#27491;&#30830;&#30340;&#35270;&#22270;&#12290;&#27169;&#25311;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#21253;&#21547;&#39640;&#24230;&#20559;&#26012;&#30340;&#20837;&#24230;&#20998;&#24067;&#30340;&#36755;&#20837;&#22270;&#25968;&#25454;&#38598;&#19978;&#65292;&#23545;&#22823;&#33455;&#29255;&#22823;&#23567;&#19978;&#30340;BFS&#22270;&#36941;&#21382;&#24615;&#33021;&#36827;&#34892;&#20102;&#21152;&#36895;&#12290;&#25913;&#36827;&#26469;&#33258;&#20110;&#22312;&#20869;&#23384;&#22788;&#29702;&#20803;&#32032;&#20043;&#38388;&#20849;&#20139;&#20837;&#24230;&#35745;&#31639;&#24037;&#20316;&#36127;&#36733;&#65292;&#24182;&#38477;&#20302;&#32593;&#32476;&#33455;&#29255;&#30340;&#20105;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
The paper aims to address load imbalance caused by high in-degree distribution in graphs by applying the idea of rhizome to vertex-centric message-driven graph processing. Rhizome construction of the graph creates multiple named vertex address for any number of single large in-degree vertices. It then allows other vertices to point to any of the named addresses thus sharing the in-degree load. The rhizomes internally communicate and remain consistent to provide a unified and correct view of the vertex. Simulated experimental results show performance speed ups for BFS graph traversal on large chip sizes for the tested input graph datasets containing highly skewed in-degree distribution. The improvements come from sharing the in-degree compute workload among memory-processing elements and also lowering contention on the network-on-chip.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#32422;&#26463;&#24418;&#24335;&#65292;&#21253;&#25324;&#23545;&#27599;&#31181;&#24418;&#24335;&#29305;&#21035;&#35774;&#35745;&#30340;&#31639;&#27861;&#12290;&#21516;&#26102;&#65292;&#25581;&#31034;&#20102;&#24120;&#35265;&#38382;&#39064;&#24418;&#24335;&#20043;&#38388;&#30340;&#25968;&#23398;&#30456;&#20114;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2402.02025</link><description>&lt;p&gt;
&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#32422;&#26463;&#24418;&#24335;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey of Constraint Formulations in Safe Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02025
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#32422;&#26463;&#24418;&#24335;&#65292;&#21253;&#25324;&#23545;&#27599;&#31181;&#24418;&#24335;&#29305;&#21035;&#35774;&#35745;&#30340;&#31639;&#27861;&#12290;&#21516;&#26102;&#65292;&#25581;&#31034;&#20102;&#24120;&#35265;&#38382;&#39064;&#24418;&#24335;&#20043;&#38388;&#30340;&#25968;&#23398;&#30456;&#20114;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23558;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#24212;&#29992;&#20110;&#29616;&#23454;&#19990;&#30028;&#38382;&#39064;&#26102;&#65292;&#30830;&#20445;&#23433;&#20840;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#22240;&#27492;&#65292;&#23433;&#20840;RL&#25104;&#20026;&#19968;&#31181;&#20174;&#23454;&#39564;&#25968;&#25454;&#20013;&#23433;&#20840;&#20248;&#21270;&#20195;&#29702;&#31574;&#30053;&#30340;&#22522;&#26412;&#32780;&#24378;&#22823;&#30340;&#33539;&#20363;&#12290;&#22522;&#20110;&#32422;&#26463;&#20934;&#21017;&#30340;&#23433;&#20840;RL&#26041;&#27861;&#34987;&#24191;&#27867;&#37319;&#29992;&#65292;&#23427;&#35299;&#20915;&#20102;&#22312;&#23433;&#20840;&#32422;&#26463;&#19979;&#26368;&#22823;&#21270;&#39044;&#26399;&#32047;&#31215;&#22870;&#21169;&#30340;&#38382;&#39064;&#12290;&#34429;&#28982;&#36817;&#24180;&#26469;&#22312;RL&#20013;&#23454;&#29616;&#23433;&#20840;&#24615;&#30340;&#23581;&#35797;&#28608;&#22686;&#65292;&#20294;&#30001;&#20110;&#32422;&#26463;&#34920;&#31034;&#30340;&#22810;&#26679;&#24615;&#21644;&#23545;&#23427;&#20204;&#20043;&#38388;&#20851;&#31995;&#30340;&#35752;&#35770;&#24456;&#23569;&#65292;&#23545;&#35813;&#39046;&#22495;&#30340;&#31995;&#32479;&#24615;&#20102;&#35299;&#20173;&#28982;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#30693;&#35782;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#23545;&#20195;&#34920;&#24615;&#32422;&#26463;&#24418;&#24335;&#30340;&#20840;&#38754;&#22238;&#39038;&#65292;&#20197;&#21450;&#38024;&#23545;&#27599;&#31181;&#24418;&#24335;&#29305;&#21035;&#35774;&#35745;&#30340;&#31639;&#27861;&#30340;&#31934;&#36873;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#25581;&#31034;&#24120;&#35265;&#38382;&#39064;&#24418;&#24335;&#20043;&#38388;&#30340;&#25968;&#23398;&#30456;&#20114;&#20851;&#31995;&#30340;&#29702;&#35770;&#22522;&#30784;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#24403;&#21069;&#30740;&#31350;&#30340;&#19968;&#20123;&#25361;&#25112;&#21644;&#26410;&#26469;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ensuring safety is critical when applying reinforcement learning (RL) to real-world problems. Consequently, safe RL emerges as a fundamental and powerful paradigm for safely optimizing an agent's policy from experimental data. A popular safe RL approach is based on a constrained criterion, which solves the problem of maximizing expected cumulative reward under safety constraints. Though there has been recently a surge of such attempts to achieve safety in RL, a systematic understanding of the field is difficult due to 1) the diversity of constraint representations and 2) little discussion of their interrelations. To address this knowledge gap, we provide a comprehensive review of representative constraint formulations, along with a curated selection of algorithms specifically designed for each formulation. Furthermore, we elucidate the theoretical underpinnings that reveal the mathematical mutual relations among common problem formulations. We conclude with a discussion of the current 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#20998;&#23618;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#29983;&#29289;&#27963;&#24615;&#24182;&#25214;&#21040;&#19982;&#20043;&#30456;&#20851;&#30340;&#26368;&#37325;&#35201;&#30340;&#25104;&#20998;&#12290;</title><link>https://arxiv.org/abs/2402.01744</link><description>&lt;p&gt;
&#36890;&#36807;&#20998;&#23618;&#22270;&#35299;&#37322;&#25581;&#31034;&#20998;&#23376;&#25104;&#20998;
&lt;/p&gt;
&lt;p&gt;
Unveiling Molecular Moieties through Hierarchical Graph Explainability
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01744
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#20998;&#23618;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#29983;&#29289;&#27963;&#24615;&#24182;&#25214;&#21040;&#19982;&#20043;&#30456;&#20851;&#30340;&#26368;&#37325;&#35201;&#30340;&#25104;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32972;&#26223;&#65306;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#20316;&#20026;&#19968;&#31181;&#24378;&#22823;&#30340;&#24037;&#20855;&#65292;&#22312;&#25903;&#25345;&#20307;&#22806;&#34394;&#25311;&#31579;&#36873;&#26041;&#38754;&#24050;&#32463;&#20986;&#29616;&#22810;&#24180;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22270;&#21367;&#31215;&#26550;&#26500;&#23454;&#29616;&#39640;&#31934;&#24230;&#22810;&#38774;&#26631;&#31579;&#36873;&#30340;GNN&#12290;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;&#19968;&#31181;&#20998;&#23618;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#25216;&#26415;&#65292;&#36890;&#36807;&#21033;&#29992;&#20449;&#24687;&#20256;&#36882;&#26426;&#21046;&#65292;&#22312;&#21407;&#23376;&#12289;&#29615;&#21644;&#25972;&#20010;&#20998;&#23376;&#23618;&#38754;&#19978;&#30452;&#25509;&#25429;&#33719;&#20449;&#24687;&#65292;&#20174;&#32780;&#25214;&#21040;&#19982;&#29983;&#29289;&#27963;&#24615;&#39044;&#27979;&#30456;&#20851;&#30340;&#26368;&#37325;&#35201;&#30340;&#25104;&#20998;&#12290;&#32467;&#26524;&#65306;&#25105;&#20204;&#22312;&#25903;&#25345;&#34394;&#25311;&#31579;&#36873;&#26041;&#38754;&#30340;&#20108;&#21313;&#20010;&#32454;&#32990;&#21608;&#26399;&#20381;&#36182;&#24615;&#28608;&#37238;&#38774;&#26631;&#19978;&#25253;&#36947;&#20102;&#19968;&#31181;&#26368;&#20808;&#36827;&#30340;GNN&#20998;&#31867;&#22120;&#12290;&#25105;&#20204;&#30340;&#20998;&#31867;&#22120;&#36229;&#36234;&#20102;&#20316;&#32773;&#25552;&#20986;&#30340;&#20808;&#21069;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;&#19968;&#20010;&#20165;&#38024;&#23545;CDK1&#30340;&#39640;&#28789;&#25935;&#24230;&#29256;&#26412;&#30340;GNN&#65292;&#20197;&#20351;&#29992;&#25105;&#20204;&#30340;&#35299;&#37322;&#22120;&#26469;&#36991;&#20813;&#22810;&#31867;&#21035;&#27169;&#22411;&#22266;&#26377;&#30340;&#20559;&#24046;&#12290;&#20998;&#23618;&#35299;&#37322;&#22120;&#24050;&#32463;&#30001;&#19968;&#20301;&#19987;&#23478;&#21270;&#23398;&#23478;&#22312;19&#20010;CDK1&#25209;&#20934;&#33647;&#29289;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Background: Graph Neural Networks (GNN) have emerged in very recent years as a powerful tool for supporting in silico Virtual Screening. In this work we present a GNN which uses Graph Convolutional architectures to achieve very accurate multi-target screening. We also devised a hierarchical Explainable Artificial Intelligence (XAI) technique to catch information directly at atom, ring, and whole molecule level by leveraging the message passing mechanism. In this way, we find the most relevant moieties involved in bioactivity prediction. Results: We report a state-of-the-art GNN classifier on twenty Cyclin-dependent Kinase targets in support of VS. Our classifier outperforms previous SOTA approaches proposed by the authors. Moreover, a CDK1-only high-sensitivity version of the GNN has been designed to use our explainer in order to avoid the inherent bias of multi-class models. The hierarchical explainer has been validated by an expert chemist on 19 approved drugs on CDK1. Our explainer 
&lt;/p&gt;</description></item><item><title>PetriRL&#26159;&#19968;&#20010;&#21019;&#26032;&#26694;&#26550;&#65292;&#23558;Petri&#32593;&#21644;&#22522;&#20110;&#20107;&#20214;&#30340;&#24378;&#21270;&#23398;&#20064;&#38598;&#25104;&#65292;&#29992;&#20110;&#35299;&#20915;&#24037;&#19994;&#20316;&#19994;&#36710;&#38388;&#35843;&#24230;&#38382;&#39064;&#65288;JSSP&#65289;&#12290;&#35813;&#26694;&#26550;&#21033;&#29992;Petri&#32593;&#36827;&#34892;&#24314;&#27169;&#65292;&#25552;&#39640;&#20102;&#21487;&#35299;&#37322;&#24615;&#65292;&#24182;&#36890;&#36807;&#20107;&#20214;&#39537;&#21160;&#25511;&#21046;&#21644;&#21160;&#20316;&#23631;&#34109;&#30340;&#38598;&#25104;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.00046</link><description>&lt;p&gt;
&#24341;&#20837;PetriRL&#65306;&#23558;Petri&#32593;&#21644;&#22522;&#20110;&#20107;&#20214;&#30340;&#24378;&#21270;&#23398;&#20064;&#38598;&#25104;&#30340;JSSP&#35299;&#20915;&#26041;&#26696;&#30340;&#21019;&#26032;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Introducing PetriRL: An Innovative Framework for JSSP Resolution Integrating Petri nets and Event-based Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00046
&lt;/p&gt;
&lt;p&gt;
PetriRL&#26159;&#19968;&#20010;&#21019;&#26032;&#26694;&#26550;&#65292;&#23558;Petri&#32593;&#21644;&#22522;&#20110;&#20107;&#20214;&#30340;&#24378;&#21270;&#23398;&#20064;&#38598;&#25104;&#65292;&#29992;&#20110;&#35299;&#20915;&#24037;&#19994;&#20316;&#19994;&#36710;&#38388;&#35843;&#24230;&#38382;&#39064;&#65288;JSSP&#65289;&#12290;&#35813;&#26694;&#26550;&#21033;&#29992;Petri&#32593;&#36827;&#34892;&#24314;&#27169;&#65292;&#25552;&#39640;&#20102;&#21487;&#35299;&#37322;&#24615;&#65292;&#24182;&#36890;&#36807;&#20107;&#20214;&#39537;&#21160;&#25511;&#21046;&#21644;&#21160;&#20316;&#23631;&#34109;&#30340;&#38598;&#25104;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24037;&#19994;&#36710;&#38388;&#20013;&#65292;&#20248;&#36136;&#35843;&#24230;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#31070;&#32463;&#32593;&#32476;&#22312;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#20854;&#26377;&#38480;&#30340;&#21487;&#35299;&#37322;&#24615;&#38459;&#30861;&#20102;&#20854;&#22312;&#24037;&#19994;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#21019;&#26032;&#30340;&#26694;&#26550;&#26469;&#35299;&#20915;&#20316;&#19994;&#36710;&#38388;&#35843;&#24230;&#38382;&#39064;&#65288;JSSP&#65289;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;Petri&#32593;&#23545;&#20316;&#19994;&#36710;&#38388;&#36827;&#34892;&#24314;&#27169;&#65292;&#19981;&#20165;&#25552;&#39640;&#20102;&#21487;&#35299;&#37322;&#24615;&#65292;&#36824;&#33021;&#30452;&#25509;&#23558;&#21407;&#22987;&#25968;&#25454;&#32435;&#20837;&#20854;&#20013;&#65292;&#26080;&#38656;&#23545;JSSP&#23454;&#20363;&#36827;&#34892;&#39044;&#22788;&#29702;&#25104;&#38750;&#20132;&#26367;&#22270;&#12290;Petri&#32593;&#30340;&#25511;&#21046;&#33021;&#21147;&#36824;&#21487;&#20197;&#31649;&#29702;&#36807;&#31243;&#20013;&#30340;&#33258;&#21160;&#21270;&#32452;&#20214;&#65292;&#20351;&#20195;&#29702;&#20154;&#33021;&#22815;&#19987;&#27880;&#20110;&#20851;&#38190;&#20915;&#31574;&#65292;&#29305;&#21035;&#26159;&#36164;&#28304;&#20998;&#37197;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20013;&#20107;&#20214;&#39537;&#21160;&#25511;&#21046;&#21644;&#21160;&#20316;&#23631;&#34109;&#30340;&#38598;&#25104;&#22312;&#20844;&#20849;&#27979;&#35797;&#22522;&#20934;&#19978;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#30340;&#24615;&#33021;&#12290;&#36328;&#19968;&#31995;&#21015;&#20248;&#21270;&#35299;&#20915;&#26041;&#26696;&#65288;&#21253;&#25324;&#21551;&#21457;&#24335;&#31639;&#27861;&#12289;&#20803;&#21551;&#21457;&#24335;&#31639;&#27861;&#21644;&#23398;&#20064;&#31639;&#27861;&#65289;&#36827;&#34892;&#30340;&#27604;&#36739;&#20998;&#26512;&#31361;&#20986;&#20102;&#20854;&#31454;&#20105;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quality scheduling in industrial job shops is crucial. Although neural networks excel in solving these problems, their limited explainability hinders their widespread industrial adoption. In this research, we introduce an innovative framework for solving job shop scheduling problems (JSSP). Our methodology leverages Petri nets to model the job shop, not only improving explainability but also enabling direct incorporation of raw data without the need to preprocess JSSP instances into disjunctive graphs. The Petri net, with its controlling capacities, also governs the automated components of the process, allowing the agent to focus on critical decision-making, particularly resource allocation. The integration of event-based control and action masking in our approach yields competitive performance on public test benchmarks. Comparative analyses across a wide spectrum of optimization solutions, including heuristics, metaheuristics, and learning-based algorithms, highlight the competitivene
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#21160;&#39550;&#39542;&#26694;&#26550;&#65292;&#21517;&#20026;Talk2Drive&#65292;&#29992;&#20110;&#22788;&#29702;&#26469;&#33258;&#20154;&#31867;&#30340;&#21475;&#22836;&#25351;&#20196;&#65292;&#24182;&#26681;&#25454;&#19978;&#19979;&#25991;&#20449;&#24687;&#23454;&#29616;&#33258;&#21160;&#39550;&#39542;&#20915;&#31574;&#65292;&#28385;&#36275;&#20010;&#24615;&#21270;&#20559;&#22909;&#12290;</title><link>https://arxiv.org/abs/2312.09397</link><description>&lt;p&gt;
&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65306;&#30495;&#23454;&#19990;&#30028;&#23454;&#39564;
&lt;/p&gt;
&lt;p&gt;
Large Language Models for Autonomous Driving: Real-World Experiments
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.09397
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#21160;&#39550;&#39542;&#26694;&#26550;&#65292;&#21517;&#20026;Talk2Drive&#65292;&#29992;&#20110;&#22788;&#29702;&#26469;&#33258;&#20154;&#31867;&#30340;&#21475;&#22836;&#25351;&#20196;&#65292;&#24182;&#26681;&#25454;&#19978;&#19979;&#25991;&#20449;&#24687;&#23454;&#29616;&#33258;&#21160;&#39550;&#39542;&#20915;&#31574;&#65292;&#28385;&#36275;&#20010;&#24615;&#21270;&#20559;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#22312;&#24403;&#20170;&#30340;&#25216;&#26415;&#39046;&#22495;&#26085;&#30410;&#27969;&#34892;&#65292;&#37096;&#20998;&#33258;&#21160;&#21270;&#30340;&#36710;&#36742;&#24050;&#32463;&#22312;&#24066;&#22330;&#19978;&#24191;&#27867;&#27969;&#36890;&#65292;&#20855;&#22791;&#23436;&#20840;&#33258;&#21160;&#21270;&#21644;&#8220;&#26080;&#20154;&#39550;&#39542;&#8221;&#33021;&#21147;&#30340;&#26102;&#20195;&#24050;&#32463;&#36843;&#22312;&#30473;&#30571;&#12290;&#28982;&#32780;&#65292;&#20934;&#30830;&#29702;&#35299;&#20154;&#31867;&#30340;&#25351;&#20196;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#21482;&#26377;&#20056;&#23458;&#32780;&#27809;&#26377;&#39550;&#39542;&#21592;&#30340;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#26469;&#35828;&#65292;&#23454;&#29616;&#39640;&#24230;&#20010;&#24615;&#21270;&#20173;&#28982;&#26159;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#24320;&#21457;&#20013;&#30340;&#25361;&#25112;&#20219;&#21153;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#26694;&#26550;Talk-to-Drive&#65288;Talk2Drive&#65289;&#65292;&#20197;&#22788;&#29702;&#26469;&#33258;&#20154;&#31867;&#30340;&#21475;&#22836;&#25351;&#20196;&#65292;&#24182;&#26681;&#25454;&#19978;&#19979;&#25991;&#20449;&#24687;&#20570;&#20986;&#33258;&#21160;&#39550;&#39542;&#20915;&#31574;&#65292;&#28385;&#36275;&#20182;&#20204;&#23545;&#23433;&#20840;&#24615;&#12289;&#25928;&#29575;&#24615;&#21644;&#33298;&#36866;&#24615;&#30340;&#20010;&#24615;&#21270;&#20559;&#22909;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#35821;&#38899;&#35782;&#21035;&#27169;&#22359;&#65292;&#29992;&#20110;&#23558;&#20154;&#31867;&#30340;&#21475;&#22836;&#36755;&#20837;&#36716;&#21270;&#20026;&#25991;&#26412;&#25351;&#20196;&#65292;&#28982;&#21518;&#23558;&#36825;&#20123;&#25351;&#20196;&#21457;&#36865;&#32473;LLMs&#36827;&#34892;&#25512;&#29702;&#12290;&#28982;&#21518;&#65292;&#36866;&#24403;&#30340;&#25351;&#20196;&#34987;&#21457;&#36865;&#32473;&#23454;&#38469;&#30340;&#27773;&#36710;&#25511;&#21046;&#31995;&#32479;&#65292;&#36827;&#19968;&#27493;&#23454;&#29616;&#33258;&#21160;&#39550;&#39542;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autonomous driving systems are increasingly popular in today's technological landscape, where vehicles with partial automation have already been widely available on the market, and the full automation era with "driverless" capabilities is near the horizon. However, accurately understanding humans' commands, particularly for autonomous vehicles that have only passengers instead of drivers, and achieving a high level of personalization remain challenging tasks in the development of autonomous driving systems. In this paper, we introduce a Large Language Model (LLM)-based framework Talk-to-Drive (Talk2Drive) to process verbal commands from humans and make autonomous driving decisions with contextual information, satisfying their personalized preferences for safety, efficiency, and comfort. First, a speech recognition module is developed for Talk2Drive to interpret verbal inputs from humans to textual instructions, which are then sent to LLMs for reasoning. Then, appropriate commands for t
&lt;/p&gt;</description></item><item><title>DMODE&#26159;&#19968;&#31181;&#26080;&#38656;&#29289;&#20307;&#31867;&#21035;&#20449;&#24687;&#30340;&#21333;&#30446;&#30446;&#26631;&#36317;&#31163;&#20272;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#34701;&#21512;&#29289;&#20307;&#22823;&#23567;&#21464;&#21270;&#21644;&#25668;&#20687;&#22836;&#36816;&#21160;&#26469;&#23454;&#29616;&#23545;&#21508;&#31181;&#30446;&#26631;&#26816;&#27979;&#21644;&#26410;&#30693;&#29289;&#20307;&#30340;&#36866;&#24212;&#65292;&#35299;&#20915;&#20102;&#21333;&#30446;&#36317;&#31163;&#20272;&#35745;&#20013;&#32570;&#20047;&#21442;&#32771;&#28857;&#21644;&#23545;&#35937;&#29305;&#23450;&#32447;&#32034;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2210.12596</link><description>&lt;p&gt;
DMODE: &#26080;&#38656;&#29305;&#23450;&#31867;&#21035;&#20449;&#24687;&#30340;&#21333;&#30446;&#30446;&#26631;&#36317;&#31163;&#20272;&#35745;&#27169;&#22359;
&lt;/p&gt;
&lt;p&gt;
DMODE: Differential Monocular Object Distance Estimation Module without Class Specific Information
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2210.12596
&lt;/p&gt;
&lt;p&gt;
DMODE&#26159;&#19968;&#31181;&#26080;&#38656;&#29289;&#20307;&#31867;&#21035;&#20449;&#24687;&#30340;&#21333;&#30446;&#30446;&#26631;&#36317;&#31163;&#20272;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#34701;&#21512;&#29289;&#20307;&#22823;&#23567;&#21464;&#21270;&#21644;&#25668;&#20687;&#22836;&#36816;&#21160;&#26469;&#23454;&#29616;&#23545;&#21508;&#31181;&#30446;&#26631;&#26816;&#27979;&#21644;&#26410;&#30693;&#29289;&#20307;&#30340;&#36866;&#24212;&#65292;&#35299;&#20915;&#20102;&#21333;&#30446;&#36317;&#31163;&#20272;&#35745;&#20013;&#32570;&#20047;&#21442;&#32771;&#28857;&#21644;&#23545;&#35937;&#29305;&#23450;&#32447;&#32034;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#21333;&#20010;&#25668;&#20687;&#22836;&#27979;&#37327;&#29289;&#20307;&#36317;&#31163;&#26159;&#19968;&#31181;&#32463;&#27982;&#39640;&#25928;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#32780;&#19981;&#38656;&#35201;&#31435;&#20307;&#35270;&#35273;&#21644;&#28608;&#20809;&#38647;&#36798;&#12290;&#23613;&#31649;&#25991;&#29486;&#20013;&#24050;&#32463;&#25506;&#35752;&#20102;&#21333;&#30446;&#36317;&#31163;&#20272;&#35745;&#65292;&#20294;&#22823;&#22810;&#25968;&#29616;&#26377;&#25216;&#26415;&#20381;&#36182;&#20110;&#29289;&#20307;&#31867;&#21035;&#30693;&#35782;&#20197;&#23454;&#29616;&#39640;&#24615;&#33021;&#12290;&#22312;&#32570;&#20047;&#36825;&#20123;&#24773;&#22659;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#21333;&#30446;&#36317;&#31163;&#20272;&#35745;&#21464;&#24471;&#26356;&#20855;&#25361;&#25112;&#24615;&#65292;&#32570;&#20047;&#21442;&#32771;&#28857;&#21644;&#29289;&#20307;&#29305;&#23450;&#32447;&#32034;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#32447;&#32034;&#21487;&#33021;&#20250;&#35823;&#23548;&#19982;&#33539;&#22260;&#24191;&#27867;&#21464;&#21270;&#25110;&#23545;&#25239;&#24773;&#20917;&#19979;&#30340;&#23545;&#35937;&#65292;&#36825;&#26159;&#38754;&#21521;&#23545;&#35937;&#19981;&#21487;&#30693;&#36317;&#31163;&#20272;&#35745;&#30340;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#26041;&#38754;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;DMODE&#65292;&#19968;&#31181;&#19981;&#38656;&#35201;&#29289;&#20307;&#31867;&#21035;&#30693;&#35782;&#30340;&#21333;&#30446;&#36317;&#31163;&#20272;&#35745;&#31867;&#21035;&#19981;&#21487;&#30693;&#26041;&#27861;&#12290;DMODE&#36890;&#36807;&#34701;&#21512;&#29289;&#20307;&#38543;&#26102;&#38388;&#21464;&#21270;&#30340;&#22823;&#23567;&#27874;&#21160;&#21644;&#25668;&#20687;&#22836;&#36816;&#21160;&#26469;&#20272;&#35745;&#29289;&#20307;&#30340;&#36317;&#31163;&#65292;&#20351;&#20854;&#33021;&#22815;&#36866;&#24212;&#21508;&#31181;&#30446;&#26631;&#26816;&#27979;&#22120;&#21644;&#26410;&#30693;&#29289;&#20307;&#65292;&#20174;&#32780;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2210.12596v2 Announce Type: replace-cross  Abstract: Utilizing a single camera for measuring object distances is a cost-effective alternative to stereo-vision and LiDAR. Although monocular distance estimation has been explored in the literature, most existing techniques rely on object class knowledge to achieve high performance. Without this contextual data, monocular distance estimation becomes more challenging, lacking reference points and object-specific cues. However, these cues can be misleading for objects with wide-range variation or adversarial situations, which is a challenging aspect of object-agnostic distance estimation. In this paper, we propose DMODE, a class-agnostic method for monocular distance estimation that does not require object class knowledge. DMODE estimates an object's distance by fusing its fluctuation in size over time with the camera's motion, making it adaptable to various object detectors and unknown objects, thus addressing these challenges. We eva
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#33033;&#20914;Q&#23398;&#20064;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;DSQN&#21033;&#29992;&#38750;&#33033;&#20914;&#31070;&#32463;&#20803;&#30340;&#33180;&#30005;&#21387;&#20316;&#20026;Q&#20540;&#34920;&#31034;&#65292;&#23454;&#29616;&#20102;&#33021;&#28304;&#39640;&#25928;&#30340;&#25511;&#21046;&#20219;&#21153;&#12290;</title><link>https://arxiv.org/abs/2201.09754</link><description>&lt;p&gt;
&#24102;&#27874;&#33033;&#20914;Q&#23398;&#20064;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Deep Reinforcement Learning with Spiking Q-learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2201.09754
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#33033;&#20914;Q&#23398;&#20064;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;DSQN&#21033;&#29992;&#38750;&#33033;&#20914;&#31070;&#32463;&#20803;&#30340;&#33180;&#30005;&#21387;&#20316;&#20026;Q&#20540;&#34920;&#31034;&#65292;&#23454;&#29616;&#20102;&#33021;&#28304;&#39640;&#25928;&#30340;&#25511;&#21046;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20511;&#21161;&#29305;&#27530;&#30340;&#31070;&#32463;&#24418;&#24577;&#30828;&#20214;&#65292;&#26399;&#26395;&#36890;&#36807;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNNs&#65289;&#23454;&#29616;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#65292;&#20197;&#26356;&#23569;&#30340;&#33021;&#37327;&#28040;&#32791;&#12290;&#36890;&#36807;&#23558;SNNs&#19982;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#30456;&#32467;&#21512;&#65292;&#20026;&#23454;&#29616;&#29616;&#23454;&#25511;&#21046;&#20219;&#21153;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#39640;&#25928;&#33021;&#28304;&#26041;&#24335;&#12290;&#30446;&#21069;&#20165;&#26377;&#23569;&#25968;&#22522;&#20110;SNN&#30340;RL&#26041;&#27861;&#12290;&#20854;&#20013;&#22823;&#37096;&#20998;&#35201;&#20040;&#32570;&#20047;&#27867;&#21270;&#33021;&#21147;&#65292;&#35201;&#20040;&#22312;&#35757;&#32451;&#20013;&#20351;&#29992;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANNs&#65289;&#26469;&#20272;&#31639;&#20540;&#20989;&#25968;&#12290;&#21069;&#32773;&#38656;&#35201;&#20026;&#27599;&#20010;&#22330;&#26223;&#35843;&#25972;&#22823;&#37327;&#36229;&#21442;&#25968;&#65292;&#32780;&#21518;&#32773;&#38480;&#21046;&#20102;&#19981;&#21516;&#31867;&#22411;RL&#31639;&#27861;&#30340;&#24212;&#29992;&#24182;&#24573;&#30053;&#20102;&#35757;&#32451;&#20013;&#30340;&#33021;&#37327;&#28040;&#32791;&#36739;&#22823;&#12290;&#20026;&#24320;&#21457;&#19968;&#20010;&#24378;&#22823;&#30340;&#22522;&#20110;&#33033;&#20914;&#30340;RL&#26041;&#27861;&#65292;&#25105;&#20204;&#20174;&#26118;&#34411;&#20013;&#21457;&#29616;&#30340;&#38750;&#33033;&#20914;&#38388;&#31070;&#32463;&#20803;&#20013;&#27762;&#21462;&#28789;&#24863;&#65292;&#25552;&#20986;&#20102;&#28145;&#24230;&#33033;&#20914;Q&#32593;&#32476;&#65288;DSQN&#65289;&#65292;&#20351;&#29992;&#38750;&#33033;&#20914;&#31070;&#32463;&#20803;&#30340;&#33180;&#30005;&#21387;&#20316;&#20026;Q&#20540;&#30340;&#34920;&#31034;&#65292;&#20174;&#32780;&#21487;&#20197;&#25351;&#23548;
&lt;/p&gt;
&lt;p&gt;
arXiv:2201.09754v2 Announce Type: replace-cross  Abstract: With the help of special neuromorphic hardware, spiking neural networks (SNNs) are expected to realize artificial intelligence (AI) with less energy consumption. It provides a promising energy-efficient way for realistic control tasks by combining SNNs with deep reinforcement learning (RL). There are only a few existing SNN-based RL methods at present. Most of them either lack generalization ability or employ Artificial Neural Networks (ANNs) to estimate value function in training. The former needs to tune numerous hyper-parameters for each scenario, and the latter limits the application of different types of RL algorithm and ignores the large energy consumption in training. To develop a robust spike-based RL method, we draw inspiration from non-spiking interneurons found in insects and propose the deep spiking Q-network (DSQN), using the membrane voltage of non-spiking neurons as the representation of Q-value, which can direct
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;&#21487;&#25193;&#23637;&#30340;&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;&#26426;&#21046;&#35774;&#35745;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19977;&#31181;&#23545;&#31574;&#12290;</title><link>http://arxiv.org/abs/2401.17044</link><description>&lt;p&gt;
&#21487;&#25193;&#23637;&#30340;&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;&#26426;&#21046;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Scalable Mechanism Design for Multi-Agent Path Finding. (arXiv:2401.17044v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.17044
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;&#21487;&#25193;&#23637;&#30340;&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;&#26426;&#21046;&#35774;&#35745;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19977;&#31181;&#23545;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;(MAPF)&#28041;&#21450;&#30830;&#23450;&#22810;&#20010;&#26234;&#33021;&#20307;&#21516;&#26102;&#31359;&#36807;&#20849;&#20139;&#21306;&#22495;&#21069;&#24448;&#29305;&#23450;&#30446;&#26631;&#20301;&#32622;&#30340;&#36335;&#24452;&#12290;&#36825;&#20010;&#38382;&#39064;&#22312;&#22788;&#29702;&#22823;&#37327;&#26234;&#33021;&#20307;&#26102;&#20855;&#26377;&#35745;&#31639;&#22797;&#26434;&#24615;&#65292;&#23588;&#20854;&#22312;&#20687;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#21327;&#35843;&#36825;&#26679;&#30340;&#23454;&#38469;&#24212;&#29992;&#20013;&#12290;&#25214;&#21040;&#26368;&#20248;&#35299;&#36890;&#24120;&#26159;&#35745;&#31639;&#19978;&#19981;&#21487;&#34892;&#30340;&#65292;&#22240;&#27492;&#20351;&#29992;&#36817;&#20284;&#31639;&#27861;&#33267;&#20851;&#37325;&#35201;&#12290;&#21516;&#26102;&#65292;&#26234;&#33021;&#20307;&#21487;&#33021;&#20197;&#33258;&#31169;&#21644;&#31574;&#30053;&#24615;&#30340;&#26041;&#24335;&#34892;&#21160;&#65292;&#22914;&#26524;&#23545;MAPF&#31639;&#27861;&#26377;&#21033;&#65292;&#21487;&#33021;&#20250;&#27498;&#26354;&#20854;&#30446;&#26631;&#12290;&#34429;&#28982;&#26426;&#21046;&#35774;&#35745;&#39046;&#22495;&#25552;&#20379;&#20102;&#29992;&#20110;&#23545;&#40784;&#28608;&#21169;&#30340;&#24037;&#20855;&#65292;&#20294;&#22914;&#26524;&#19981;&#20180;&#32454;&#32771;&#34385;&#20351;&#29992;&#36825;&#20123;&#24037;&#20855;&#21487;&#33021;&#20250;&#22312;&#20165;&#26377;&#36817;&#20284;&#26368;&#20248;&#32467;&#26524;&#26102;&#22833;&#36133;&#12290;&#30001;&#20110;&#36817;&#20284;&#23545;&#20110;&#21487;&#25193;&#23637;&#30340;MAPF&#31639;&#27861;&#33267;&#20851;&#37325;&#35201;&#65292;&#36825;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#21487;&#25193;&#23637;MAPF&#26426;&#21046;&#35774;&#35745;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19977;&#31181;&#23545;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-Agent Path Finding (MAPF) involves determining paths for multiple agents to travel simultaneously through a shared area toward particular goal locations. This problem is computationally complex, especially when dealing with large numbers of agents, as is common in realistic applications like autonomous vehicle coordination. Finding an optimal solution is often computationally infeasible, making the use of approximate algorithms essential. Adding to the complexity, agents might act in a self-interested and strategic way, possibly misrepresenting their goals to the MAPF algorithm if it benefits them. Although the field of mechanism design offers tools to align incentives, using these tools without careful consideration can fail when only having access to approximately optimal outcomes. Since approximations are crucial for scalable MAPF algorithms, this poses a significant challenge. In this work, we introduce the problem of scalable mechanism design for MAPF and propose three strat
&lt;/p&gt;</description></item><item><title>CMMU&#26159;&#19968;&#20010;&#29992;&#20110;&#20013;&#25991;&#22810;&#27169;&#24577;&#22810;&#31867;&#22411;&#38382;&#39064;&#29702;&#35299;&#21644;&#25512;&#29702;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#28085;&#30422;&#20102;&#20174;&#23567;&#23398;&#21040;&#39640;&#20013;&#30340;&#30693;&#35782;&#65292;&#25552;&#20379;&#20102;&#22810;&#39033;&#36873;&#25321;&#39064;&#12289;&#22810;&#39033;&#22238;&#31572;&#39064;&#21644;&#22635;&#31354;&#39064;&#19977;&#31181;&#31867;&#22411;&#30340;&#38382;&#39064;&#65292;&#23545;&#20110;&#35780;&#20272;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26234;&#33021;&#27700;&#24179;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2401.14011</link><description>&lt;p&gt;
CMMU: &#19968;&#20010;&#29992;&#20110;&#20013;&#25991;&#22810;&#27169;&#24577;&#22810;&#31867;&#22411;&#38382;&#39064;&#29702;&#35299;&#19982;&#25512;&#29702;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
CMMU: A Benchmark for Chinese Multi-modal Multi-type Question Understanding and Reasoning. (arXiv:2401.14011v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14011
&lt;/p&gt;
&lt;p&gt;
CMMU&#26159;&#19968;&#20010;&#29992;&#20110;&#20013;&#25991;&#22810;&#27169;&#24577;&#22810;&#31867;&#22411;&#38382;&#39064;&#29702;&#35299;&#21644;&#25512;&#29702;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#28085;&#30422;&#20102;&#20174;&#23567;&#23398;&#21040;&#39640;&#20013;&#30340;&#30693;&#35782;&#65292;&#25552;&#20379;&#20102;&#22810;&#39033;&#36873;&#25321;&#39064;&#12289;&#22810;&#39033;&#22238;&#31572;&#39064;&#21644;&#22635;&#31354;&#39064;&#19977;&#31181;&#31867;&#22411;&#30340;&#38382;&#39064;&#65292;&#23545;&#20110;&#35780;&#20272;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26234;&#33021;&#27700;&#24179;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#24050;&#32463;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#24182;&#23637;&#29616;&#20986;&#24378;&#22823;&#30340;&#30693;&#35782;&#29702;&#35299;&#21644;&#25512;&#29702;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#35780;&#20272;MLLM&#30340;&#26234;&#33021;&#27700;&#24179;&#25152;&#38656;&#30340;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#25484;&#25569;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#24403;&#21069;&#29992;&#20110;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#30340;&#22810;&#27169;&#24577;&#22522;&#20934;&#27979;&#35797;&#20027;&#35201;&#38598;&#20013;&#22312;&#33521;&#35821;&#22810;&#39033;&#36873;&#25321;&#39064;&#19978;&#65292;&#24182;&#19988;&#22312;&#35780;&#20272;&#30340;&#20840;&#38754;&#24615;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;CMMU&#65292;&#19968;&#20010;&#29992;&#20110;&#20013;&#25991;&#22810;&#27169;&#24577;&#22810;&#31867;&#22411;&#38382;&#39064;&#29702;&#35299;&#21644;&#25512;&#29702;&#30340;&#26032;&#22411;&#22522;&#20934;&#27979;&#35797;&#12290;CMMU&#21253;&#21547;7&#20010;&#23398;&#31185;&#30340;3603&#20010;&#38382;&#39064;&#65292;&#28085;&#30422;&#20102;&#20174;&#23567;&#23398;&#21040;&#39640;&#20013;&#30340;&#30693;&#35782;&#12290;&#36825;&#20123;&#38382;&#39064;&#21487;&#20197;&#20998;&#20026;&#22810;&#39033;&#36873;&#25321;&#39064;&#12289;&#22810;&#39033;&#22238;&#31572;&#39064;&#21644;&#22635;&#31354;&#39064;&#19977;&#31867;&#65292;&#23545;MLLMs&#25552;&#20986;&#26356;&#22823;&#30340;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20005;&#26684;&#30340;&#35780;&#20272;&#31574;&#30053;&#65292;&#31216;&#20026;ShiftCheck&#65292;&#29992;&#20110;&#35780;&#20272;&#22810;&#39033;&#36873;&#25321;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-modal large language models(MLLMs) have achieved remarkable progress and demonstrated powerful knowledge comprehension and reasoning abilities. However, the mastery of domain-specific knowledge, which is essential for evaluating the intelligence of MLLMs, continues to be a challenge. Current multi-modal benchmarks for domain-specific knowledge concentrate on multiple-choice questions and are predominantly available in English, which imposes limitations on the comprehensiveness of the evaluation. To this end, we introduce CMMU, a novel benchmark for multi-modal and multi-type question understanding and reasoning in Chinese. CMMU consists of 3,603 questions in 7 subjects, covering knowledge from primary to high school. The questions can be categorized into 3 types: multiple-choice, multiple-response, and fill-in-the-blank, bringing greater challenges to MLLMs. In addition, we propose a rigorous evaluation strategy called ShiftCheck for assessing multiple-choice questions. The strat
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#20010;&#20154;LLM&#20195;&#29702;&#30340;&#33021;&#21147;&#12289;&#25928;&#29575;&#21644;&#23433;&#20840;&#24615;&#65292;&#26088;&#22312;&#25552;&#21319;&#26234;&#33021;&#20010;&#20154;&#21161;&#29702;&#30340;&#23454;&#29992;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.05459</link><description>&lt;p&gt;
&#20010;&#20154;LLM&#20195;&#29702;:&#20851;&#20110;&#33021;&#21147;&#12289;&#25928;&#29575;&#21644;&#23433;&#20840;&#24615;&#30340;&#27934;&#23519;&#21644;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Personal LLM Agents: Insights and Survey about the Capability, Efficiency and Security. (arXiv:2401.05459v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05459
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#20010;&#20154;LLM&#20195;&#29702;&#30340;&#33021;&#21147;&#12289;&#25928;&#29575;&#21644;&#23433;&#20840;&#24615;&#65292;&#26088;&#22312;&#25552;&#21319;&#26234;&#33021;&#20010;&#20154;&#21161;&#29702;&#30340;&#23454;&#29992;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20174;&#20010;&#20154;&#35745;&#31639;&#35774;&#22791;&#20986;&#29616;&#20197;&#26469;&#65292;&#26234;&#33021;&#20010;&#20154;&#21161;&#29702;(IPA)&#19968;&#30452;&#26159;&#30740;&#31350;&#20154;&#21592;&#21644;&#24037;&#31243;&#24072;&#20851;&#27880;&#30340;&#20851;&#38190;&#25216;&#26415;&#20043;&#19968;&#65292;&#26088;&#22312;&#24110;&#21161;&#29992;&#25143;&#39640;&#25928;&#33719;&#21462;&#20449;&#24687;&#21644;&#25191;&#34892;&#20219;&#21153;&#65292;&#24182;&#20026;&#29992;&#25143;&#25552;&#20379;&#26356;&#26234;&#33021;&#12289;&#20415;&#25463;&#21644;&#20016;&#23500;&#30340;&#20132;&#20114;&#20307;&#39564;&#12290;&#38543;&#30528;&#26234;&#33021;&#25163;&#26426;&#21644;&#29289;&#32852;&#32593;&#30340;&#21457;&#23637;&#65292;&#35745;&#31639;&#21644;&#24863;&#30693;&#35774;&#22791;&#21464;&#24471;&#26080;&#22788;&#19981;&#22312;&#65292;&#26497;&#22823;&#22320;&#25193;&#23637;&#20102;IPA&#30340;&#36793;&#30028;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32570;&#20047;&#29992;&#25143;&#24847;&#22270;&#29702;&#35299;&#12289;&#20219;&#21153;&#35268;&#21010;&#12289;&#24037;&#20855;&#20351;&#29992;&#21644;&#20010;&#20154;&#25968;&#25454;&#31649;&#29702;&#31561;&#33021;&#21147;&#65292;&#29616;&#26377;&#30340;IPA&#20173;&#28982;&#20855;&#26377;&#26377;&#38480;&#30340;&#23454;&#29992;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;&#26368;&#36817;&#65292;&#20197;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#20026;&#20195;&#34920;&#30340;&#22522;&#30784;&#27169;&#22411;&#30340;&#20986;&#29616;&#65292;&#20026;IPA&#30340;&#21457;&#23637;&#24102;&#26469;&#20102;&#26032;&#30340;&#26426;&#36935;&#12290;&#20511;&#21161;&#24378;&#22823;&#30340;&#35821;&#20041;&#29702;&#35299;&#21644;&#25512;&#29702;&#33021;&#21147;&#65292;LLM&#33021;&#22815;&#20351;&#26234;&#33021;&#20195;&#29702;&#33258;&#20027;&#35299;&#20915;&#22797;&#26434;&#38382;&#39064;&#12290;&#26412;&#25991;&#37325;&#28857;&#20851;&#27880;&#20010;&#20154;LLM&#20195;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Since the advent of personal computing devices, intelligent personal assistants (IPAs) have been one of the key technologies that researchers and engineers have focused on, aiming to help users efficiently obtain information and execute tasks, and provide users with more intelligent, convenient, and rich interaction experiences. With the development of smartphones and IoT, computing and sensing devices have become ubiquitous, greatly expanding the boundaries of IPAs. However, due to the lack of capabilities such as user intent understanding, task planning, tool using, and personal data management etc., existing IPAs still have limited practicality and scalability. Recently, the emergence of foundation models, represented by large language models (LLMs), brings new opportunities for the development of IPAs. With the powerful semantic understanding and reasoning capabilities, LLM can enable intelligent agents to solve complex problems autonomously. In this paper, we focus on Personal LLM
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#30340;&#38590;&#24230;&#26657;&#20934;&#30340;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#26041;&#27861;&#65292;&#26088;&#22312;&#26174;&#33879;&#25552;&#39640;&#20302;FPR&#19979;&#30340;TPR&#65292;&#20197;&#39564;&#35777;&#35757;&#32451;&#27169;&#22411;&#26159;&#21542;&#20445;&#25252;&#38544;&#31169;&#12290;</title><link>http://arxiv.org/abs/2401.04929</link><description>&lt;p&gt;
&#22522;&#20110;&#23398;&#20064;&#30340;&#38590;&#24230;&#26657;&#20934;&#25552;&#21319;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Learning-Based Difficulty Calibration for Enhanced Membership Inference Attacks. (arXiv:2401.04929v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04929
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#30340;&#38590;&#24230;&#26657;&#20934;&#30340;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#26041;&#27861;&#65292;&#26088;&#22312;&#26174;&#33879;&#25552;&#39640;&#20302;FPR&#19979;&#30340;TPR&#65292;&#20197;&#39564;&#35777;&#35757;&#32451;&#27169;&#22411;&#26159;&#21542;&#20445;&#25252;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#30446;&#21069;&#26159;&#21508;&#31181;&#24212;&#29992;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#20174;&#21307;&#30103;&#20445;&#20581;&#21040;&#37329;&#34701;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#25935;&#24863;&#25968;&#25454;&#26469;&#35757;&#32451;&#36825;&#20123;&#27169;&#22411;&#24341;&#21457;&#20102;&#23545;&#38544;&#31169;&#21644;&#23433;&#20840;&#30340;&#25285;&#24551;&#12290;&#19968;&#31181;&#39564;&#35777;&#35757;&#32451;&#27169;&#22411;&#26159;&#21542;&#20445;&#25252;&#38544;&#31169;&#30340;&#26041;&#27861;&#26159;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#65288;MIA&#65289;&#65292;&#23427;&#20801;&#35768;&#23545;&#25163;&#30830;&#23450;&#29305;&#23450;&#25968;&#25454;&#28857;&#26159;&#21542;&#26159;&#27169;&#22411;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#19968;&#37096;&#20998;&#12290;&#34429;&#28982;&#24050;&#32463;&#22312;&#25991;&#29486;&#20013;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#30340;MIA&#65292;&#20294;&#21482;&#26377;&#23569;&#25968;&#33021;&#22815;&#22312;&#20302;&#20551;&#38451;&#24615;&#29575;&#65288;FPR&#65289;&#21306;&#22495;&#65288;0.01%~1%&#65289;&#23454;&#29616;&#36739;&#39640;&#30340;&#30495;&#38451;&#24615;&#29575;&#65288;TPR&#65289;&#12290;&#36825;&#26159;&#23454;&#38469;&#24212;&#29992;&#20110;&#23454;&#38469;&#22330;&#26223;&#20013;&#30340;MIA&#24517;&#39035;&#32771;&#34385;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;MIA&#26041;&#27861;&#65292;&#26088;&#22312;&#26174;&#33879;&#25552;&#39640;&#20302;FPR&#30340;TPR&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21517;&#20026;&#22522;&#20110;&#23398;&#20064;&#30340;&#38590;&#24230;&#26657;&#20934;&#65288;LDC-MIA&#65289;&#65292;&#36890;&#36807;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#23558;&#25968;&#25454;&#35760;&#24405;&#20197;&#20854;&#38590;&#24230;&#32423;&#21035;&#36827;&#34892;&#34920;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning models, in particular deep neural networks, are currently an integral part of various applications, from healthcare to finance. However, using sensitive data to train these models raises concerns about privacy and security. One method that has emerged to verify if the trained models are privacy-preserving is Membership Inference Attacks (MIA), which allows adversaries to determine whether a specific data point was part of a model's training dataset. While a series of MIAs have been proposed in the literature, only a few can achieve high True Positive Rates (TPR) in the low False Positive Rate (FPR) region (0.01%~1%). This is a crucial factor to consider for an MIA to be practically useful in real-world settings. In this paper, we present a novel approach to MIA that is aimed at significantly improving TPR at low FPRs. Our method, named learning-based difficulty calibration for MIA(LDC-MIA), characterizes data records by their hardness levels using a neural network clas
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#23398;&#20064;&#35757;&#32451;&#26041;&#26696;&#21644;&#23398;&#20064;&#33539;&#24335;&#23545;&#33021;&#28304;&#28040;&#32791;&#30340;&#24433;&#21709;&#65292;&#24182;&#25506;&#35752;&#20102;&#39044;&#35757;&#32451;&#21644;&#22810;&#20219;&#21153;&#35757;&#32451;&#22312;&#21487;&#25345;&#32493;&#26426;&#22120;&#23398;&#20064;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.01851</link><description>&lt;p&gt;
&#35757;&#32451;&#30340;&#21147;&#37327;&#65306;&#19981;&#21516;&#30340;&#31070;&#32463;&#32593;&#32476;&#35774;&#32622;&#23545;&#33021;&#28304;&#38656;&#27714;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
The Power of Training: How Different Neural Network Setups Influence the Energy Demand. (arXiv:2401.01851v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01851
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#23398;&#20064;&#35757;&#32451;&#26041;&#26696;&#21644;&#23398;&#20064;&#33539;&#24335;&#23545;&#33021;&#28304;&#28040;&#32791;&#30340;&#24433;&#21709;&#65292;&#24182;&#25506;&#35752;&#20102;&#39044;&#35757;&#32451;&#21644;&#22810;&#20219;&#21153;&#35757;&#32451;&#22312;&#21487;&#25345;&#32493;&#26426;&#22120;&#23398;&#20064;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#26426;&#22120;&#23398;&#20064;&#35757;&#32451;&#26041;&#26696;&#21644;&#23398;&#20064;&#33539;&#24335;&#30340;&#21464;&#21270;&#23545;&#30456;&#24212;&#33021;&#28304;&#28040;&#32791;&#30340;&#24433;&#21709;&#12290;&#34429;&#28982;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#25552;&#39640;&#21644;&#39640;&#24615;&#33021;&#30828;&#20214;&#30340;&#21019;&#26032;&#25512;&#21160;&#20102;&#22797;&#26434;&#27169;&#22411;&#30340;&#35757;&#32451;&#65292;&#20294;&#20063;&#25903;&#25345;&#20102;&#33021;&#28304;&#28040;&#32791;&#21644;&#30899;&#25490;&#25918;&#30340;&#28040;&#38544;&#12290;&#22240;&#27492;&#65292;&#26412;&#30740;&#31350;&#30340;&#30446;&#26631;&#26159;&#22686;&#21152;&#20154;&#20204;&#23545;&#19968;&#33324;&#35757;&#32451;&#21442;&#25968;&#21644;&#36807;&#31243;&#65288;&#20174;&#23398;&#20064;&#29575;&#21040;&#25209;&#37327;&#22823;&#23567;&#20877;&#21040;&#30693;&#35782;&#20256;&#36755;&#65289;&#30340;&#33021;&#28304;&#24433;&#21709;&#30340;&#35748;&#35782;&#12290;&#20351;&#29992;&#19981;&#21516;&#30340;&#36229;&#21442;&#25968;&#21021;&#22987;&#21270;&#22312;&#20004;&#31181;&#19981;&#21516;&#30340;&#30828;&#20214;&#37197;&#32622;&#19978;&#35780;&#20272;&#22810;&#31181;&#35774;&#32622;&#65292;&#20197;&#33719;&#24471;&#26377;&#24847;&#20041;&#30340;&#32467;&#26524;&#12290;&#22312;&#22522;&#20934;&#32467;&#26524;&#19978;&#36827;&#34892;&#20102;&#39044;&#35757;&#32451;&#21644;&#22810;&#20219;&#21153;&#35757;&#32451;&#23454;&#39564;&#65292;&#20197;&#30830;&#23450;&#23427;&#20204;&#23545;&#21487;&#25345;&#32493;&#26426;&#22120;&#23398;&#20064;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work examines the effects of variations in machine learning training regimes and learning paradigms on the corresponding energy consumption. While increasing data availability and innovation in high-performance hardware fuels the training of sophisticated models, it also supports the fading perception of energy consumption and carbon emission. Therefore, the goal of this work is to create awareness about the energy impact of general training parameters and processes, from learning rate over batch size to knowledge transfer. Multiple setups with different hyperparameter initializations are evaluated on two different hardware configurations to obtain meaningful results. Experiments on pretraining and multitask training are conducted on top of the baseline results to determine their potential towards sustainable machine learning.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19978;&#19979;&#25991;&#21270;&#25919;&#31574;&#24674;&#22797;&#26041;&#27861;&#29992;&#20110;&#24314;&#27169;&#22797;&#26434;&#30340;&#21307;&#30103;&#20915;&#31574;&#36807;&#31243;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#27169;&#22411;&#22312;&#20934;&#30830;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#23558;&#20915;&#31574;&#31574;&#30053;&#25286;&#20998;&#20026;&#19978;&#19979;&#25991;&#29305;&#23450;&#31574;&#30053;&#65292;&#36890;&#36807;&#22810;&#20219;&#21153;&#23398;&#20064;&#26469;&#23454;&#29616;&#24314;&#27169;&#65292;&#24182;&#25552;&#20379;&#22797;&#26434;&#34892;&#20026;&#30340;&#31616;&#27905;&#25551;&#36848;&#12290;</title><link>http://arxiv.org/abs/2310.07918</link><description>&lt;p&gt;
&#19978;&#19979;&#25991;&#21270;&#25919;&#31574;&#24674;&#22797;&#65306;&#36890;&#36807;&#33258;&#36866;&#24212;&#27169;&#20223;&#23398;&#20064;&#23545;&#21307;&#30103;&#20915;&#31574;&#36827;&#34892;&#24314;&#27169;&#21644;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Contextualized Policy Recovery: Modeling and Interpreting Medical Decisions with Adaptive Imitation Learning. (arXiv:2310.07918v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07918
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19978;&#19979;&#25991;&#21270;&#25919;&#31574;&#24674;&#22797;&#26041;&#27861;&#29992;&#20110;&#24314;&#27169;&#22797;&#26434;&#30340;&#21307;&#30103;&#20915;&#31574;&#36807;&#31243;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#27169;&#22411;&#22312;&#20934;&#30830;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#23558;&#20915;&#31574;&#31574;&#30053;&#25286;&#20998;&#20026;&#19978;&#19979;&#25991;&#29305;&#23450;&#31574;&#30053;&#65292;&#36890;&#36807;&#22810;&#20219;&#21153;&#23398;&#20064;&#26469;&#23454;&#29616;&#24314;&#27169;&#65292;&#24182;&#25552;&#20379;&#22797;&#26434;&#34892;&#20026;&#30340;&#31616;&#27905;&#25551;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#31574;&#30053;&#23398;&#20064;&#26088;&#22312;&#20174;&#35266;&#23519;&#21040;&#30340;&#34892;&#20026;&#20013;&#20272;&#35745;&#21487;&#29702;&#35299;&#30340;&#20915;&#31574;&#31574;&#30053;&#65307;&#28982;&#32780;&#65292;&#29616;&#26377;&#27169;&#22411;&#22312;&#20934;&#30830;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#20043;&#38388;&#23384;&#22312;&#26435;&#34913;&#12290;&#36825;&#31181;&#26435;&#34913;&#38480;&#21046;&#20102;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#23545;&#20154;&#31867;&#20915;&#31574;&#36807;&#31243;&#30340;&#35299;&#37322;&#65292;&#20363;&#22914;&#65292;&#23457;&#35745;&#21307;&#30103;&#20915;&#31574;&#30340;&#20559;&#35265;&#21644;&#27425;&#20248;&#23454;&#36341;&#65292;&#25105;&#20204;&#38656;&#35201;&#20915;&#31574;&#36807;&#31243;&#30340;&#27169;&#22411;&#65292;&#33021;&#22815;&#25552;&#20379;&#22797;&#26434;&#34892;&#20026;&#30340;&#31616;&#27905;&#25551;&#36848;&#12290;&#29616;&#26377;&#26041;&#27861;&#22522;&#26412;&#19978;&#30001;&#20110;&#23558;&#28508;&#22312;&#20915;&#31574;&#36807;&#31243;&#34920;&#31034;&#20026;&#36890;&#29992;&#31574;&#30053;&#32780;&#36127;&#25285;&#20102;&#36825;&#31181;&#26435;&#34913;&#65292;&#32780;&#23454;&#38469;&#19978;&#20154;&#31867;&#20915;&#31574;&#26159;&#21160;&#24577;&#30340;&#65292;&#21487;&#20197;&#38543;&#19978;&#19979;&#25991;&#20449;&#24687;&#32780;&#22823;&#24133;&#25913;&#21464;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19978;&#19979;&#25991;&#21270;&#25919;&#31574;&#24674;&#22797;&#65288;CPR&#65289;&#65292;&#23558;&#24314;&#27169;&#22797;&#26434;&#20915;&#31574;&#36807;&#31243;&#30340;&#38382;&#39064;&#37325;&#26032;&#23450;&#20041;&#20026;&#22810;&#20219;&#21153;&#23398;&#20064;&#38382;&#39064;&#65292;&#20854;&#20013;&#22797;&#26434;&#20915;&#31574;&#31574;&#30053;&#30001;&#29305;&#23450;&#19978;&#19979;&#25991;&#30340;&#31574;&#30053;&#32452;&#25104;&#12290;CPR&#23558;&#27599;&#20010;&#19978;&#19979;&#25991;&#29305;&#23450;&#31574;&#30053;&#24314;&#27169;&#20026;&#32447;&#24615;&#30340;&#35266;&#23519;-&#21160;&#20316;&#26144;&#23556;
&lt;/p&gt;
&lt;p&gt;
Interpretable policy learning seeks to estimate intelligible decision policies from observed actions; however, existing models fall short by forcing a tradeoff between accuracy and interpretability. This tradeoff limits data-driven interpretations of human decision-making process. e.g. to audit medical decisions for biases and suboptimal practices, we require models of decision processes which provide concise descriptions of complex behaviors. Fundamentally, existing approaches are burdened by this tradeoff because they represent the underlying decision process as a universal policy, when in fact human decisions are dynamic and can change drastically with contextual information. Thus, we propose Contextualized Policy Recovery (CPR), which re-frames the problem of modeling complex decision processes as a multi-task learning problem in which complex decision policies are comprised of context-specific policies. CPR models each context-specific policy as a linear observation-to-action mapp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#35299;&#26500;&#20154;&#24037;&#26234;&#33021;&#36131;&#20219;&#30340;&#27010;&#24565;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21253;&#21547;&#22235;&#31181;&#36131;&#20219;&#24847;&#20041;&#30340;&#26377;&#25928;&#32452;&#21512;&#65292;&#20197;&#25903;&#25345;&#23545;&#20154;&#24037;&#26234;&#33021;&#36131;&#20219;&#30340;&#23454;&#36341;&#25512;&#29702;&#12290;</title><link>http://arxiv.org/abs/2308.02608</link><description>&lt;p&gt;
&#35299;&#26500;&#20154;&#24037;&#26234;&#33021;&#36131;&#20219;
&lt;/p&gt;
&lt;p&gt;
Unravelling Responsibility for AI. (arXiv:2308.02608v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02608
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#35299;&#26500;&#20154;&#24037;&#26234;&#33021;&#36131;&#20219;&#30340;&#27010;&#24565;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21253;&#21547;&#22235;&#31181;&#36131;&#20219;&#24847;&#20041;&#30340;&#26377;&#25928;&#32452;&#21512;&#65292;&#20197;&#25903;&#25345;&#23545;&#20154;&#24037;&#26234;&#33021;&#36131;&#20219;&#30340;&#23454;&#36341;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#22312;&#28041;&#21450;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#22797;&#26434;&#24773;&#20917;&#19979;&#21512;&#29702;&#24605;&#32771;&#36131;&#20219;&#24212;&#35813;&#25918;&#22312;&#20309;&#22788;&#65292;&#25105;&#20204;&#39318;&#20808;&#38656;&#35201;&#19968;&#20010;&#36275;&#22815;&#28165;&#26224;&#21644;&#35814;&#32454;&#30340;&#36328;&#23398;&#31185;&#35789;&#27719;&#26469;&#35848;&#35770;&#36131;&#20219;&#12290;&#36131;&#20219;&#26159;&#19968;&#31181;&#19977;&#20803;&#20851;&#31995;&#65292;&#28041;&#21450;&#21040;&#19968;&#20010;&#34892;&#20026;&#32773;&#12289;&#19968;&#20010;&#20107;&#20214;&#21644;&#19968;&#31181;&#36131;&#20219;&#26041;&#24335;&#12290;&#20316;&#20026;&#19968;&#31181;&#26377;&#24847;&#35782;&#30340;&#20026;&#20102;&#25903;&#25345;&#23545;&#20154;&#24037;&#26234;&#33021;&#36131;&#20219;&#36827;&#34892;&#23454;&#36341;&#25512;&#29702;&#30340;&#8220;&#35299;&#26500;&#8221;&#36131;&#20219;&#27010;&#24565;&#30340;&#21162;&#21147;&#65292;&#26412;&#25991;&#37319;&#21462;&#20102;&#8220;&#34892;&#20026;&#32773;A&#23545;&#20107;&#20214;O&#36127;&#36131;&#8221;&#30340;&#19977;&#37096;&#20998;&#34920;&#36848;&#65292;&#24182;&#30830;&#23450;&#20102;A&#12289;&#36127;&#36131;&#12289;O&#30340;&#23376;&#31867;&#21035;&#30340;&#26377;&#25928;&#32452;&#21512;&#12290;&#36825;&#20123;&#26377;&#25928;&#32452;&#21512;&#25105;&#20204;&#31216;&#20043;&#20026;&#8220;&#36131;&#20219;&#20018;&#8221;&#65292;&#20998;&#20026;&#22235;&#31181;&#36131;&#20219;&#24847;&#20041;&#65306;&#35282;&#33394;&#36131;&#20219;&#12289;&#22240;&#26524;&#36131;&#20219;&#12289;&#27861;&#24459;&#36131;&#20219;&#21644;&#36947;&#24503;&#36131;&#20219;&#12290;&#25105;&#20204;&#36890;&#36807;&#20004;&#20010;&#36816;&#34892;&#31034;&#20363;&#36827;&#34892;&#20102;&#35828;&#26126;&#65292;&#19968;&#20010;&#28041;&#21450;&#21307;&#30103;AI&#31995;&#32479;&#65292;&#21478;&#19968;&#20010;&#28041;&#21450;AV&#19982;&#34892;&#20154;&#30340;&#33268;&#21629;&#30896;&#25758;&#12290;
&lt;/p&gt;
&lt;p&gt;
To reason about where responsibility does and should lie in complex situations involving AI-enabled systems, we first need a sufficiently clear and detailed cross-disciplinary vocabulary for talking about responsibility. Responsibility is a triadic relation involving an actor, an occurrence, and a way of being responsible. As part of a conscious effort towards 'unravelling' the concept of responsibility to support practical reasoning about responsibility for AI, this paper takes the three-part formulation, 'Actor A is responsible for Occurrence O' and identifies valid combinations of subcategories of A, is responsible for, and O. These valid combinations - which we term "responsibility strings" - are grouped into four senses of responsibility: role-responsibility; causal responsibility; legal liability-responsibility; and moral responsibility. They are illustrated with two running examples, one involving a healthcare AI-based system and another the fatal collision of an AV with a pedes
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#19968;&#20010;&#28151;&#21512;&#26041;&#27861;&#29992;&#25143;&#30740;&#31350;&#65292;&#35780;&#20272;&#20102;&#19981;&#27491;&#30830;&#30340;&#35299;&#37322;&#22914;&#20309;&#24433;&#21709;&#20154;&#31867;&#30340;&#20915;&#31574;&#34892;&#20026;&#65292;&#20197;&#22686;&#36827;&#20154;&#24037;&#26234;&#33021;&#35299;&#37322;&#24615;&#23545;&#20154;&#24037;&#26234;&#33021;&#20915;&#31574;&#30340;&#29702;&#35299;&#12290;</title><link>http://arxiv.org/abs/2307.13566</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#35299;&#37322;&#24615;&#23545;&#20154;&#24037;&#26234;&#33021;&#20915;&#31574;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
The Impact of Imperfect XAI on Human-AI Decision-Making. (arXiv:2307.13566v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13566
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#19968;&#20010;&#28151;&#21512;&#26041;&#27861;&#29992;&#25143;&#30740;&#31350;&#65292;&#35780;&#20272;&#20102;&#19981;&#27491;&#30830;&#30340;&#35299;&#37322;&#22914;&#20309;&#24433;&#21709;&#20154;&#31867;&#30340;&#20915;&#31574;&#34892;&#20026;&#65292;&#20197;&#22686;&#36827;&#20154;&#24037;&#26234;&#33021;&#35299;&#37322;&#24615;&#23545;&#20154;&#24037;&#26234;&#33021;&#20915;&#31574;&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#37322;&#24615;&#25216;&#26415;&#27491;&#22312;&#24555;&#36895;&#21457;&#23637;&#65292;&#20197;&#25913;&#36827;&#21508;&#31181;&#21512;&#20316;&#24037;&#20316;&#29615;&#22659;&#19979;&#30340;&#20154;&#24037;&#26234;&#33021;&#20915;&#31574;&#12290;&#22240;&#27492;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#35780;&#20272;&#20102;&#20915;&#31574;&#32773;&#19982;&#19981;&#23436;&#32654;&#30340;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;&#30340;&#26041;&#24335;&#65292;&#30740;&#31350;&#21512;&#36866;&#30340;&#20381;&#36182;&#20851;&#31995;&#21644;&#20219;&#21153;&#34920;&#29616;&#65292;&#20197;&#20415;&#35774;&#35745;&#26356;&#21152;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#35745;&#31639;&#26426;&#25903;&#25345;&#30340;&#21327;&#20316;&#24037;&#20855;&#12290;&#19968;&#20123;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#25216;&#26415;&#34987;&#25552;&#20986;&#65292;&#24076;&#26395;&#25913;&#21892;&#20915;&#31574;&#32773;&#19982;&#20154;&#24037;&#26234;&#33021;&#30340;&#21512;&#20316;&#65307;&#28982;&#32780;&#65292;&#36825;&#20123;&#25216;&#26415;&#22522;&#20110;&#20808;&#21069;&#30740;&#31350;&#30340;&#21457;&#29616;&#65292;&#20027;&#35201;&#20851;&#27880;&#38169;&#35823;&#30340;&#20154;&#24037;&#26234;&#33021;&#24314;&#35758;&#30340;&#24433;&#21709;&#12290;&#24456;&#23569;&#26377;&#30740;&#31350;&#25215;&#35748;&#21363;&#20351;&#20154;&#24037;&#26234;&#33021;&#24314;&#35758;&#27491;&#30830;&#65292;&#35299;&#37322;&#20063;&#21487;&#33021;&#26159;&#38169;&#35823;&#30340;&#12290;&#22240;&#27492;&#65292;&#20102;&#35299;&#19981;&#23436;&#32654;&#30340;&#35299;&#37322;&#24615;&#20154;&#24037;&#26234;&#33021;&#22914;&#20309;&#24433;&#21709;&#20154;&#24037;&#26234;&#33021;&#20915;&#31574;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#19968;&#20010;&#24378;&#22823;&#30340;&#28151;&#21512;&#26041;&#27861;&#29992;&#25143;&#30740;&#31350;&#65292;&#28041;&#21450;136&#21517;&#21442;&#19982;&#32773;&#65292;&#35780;&#20272;&#20102;&#19981;&#27491;&#30830;&#30340;&#35299;&#37322;&#22914;&#20309;&#24433;&#21709;&#20154;&#31867;&#30340;&#20915;&#31574;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
Explainability techniques are rapidly being developed to improve human-AI decision-making across various cooperative work settings. Consequently, previous research has evaluated how decision-makers collaborate with imperfect AI by investigating appropriate reliance and task performance with the aim of designing more human-centered computer-supported collaborative tools. Several human-centered explainable AI (XAI) techniques have been proposed in hopes of improving decision-makers' collaboration with AI; however, these techniques are grounded in findings from previous studies that primarily focus on the impact of incorrect AI advice. Few studies acknowledge the possibility for the explanations to be incorrect even if the AI advice is correct. Thus, it is crucial to understand how imperfect XAI affects human-AI decision-making. In this work, we contribute a robust, mixed-methods user study with 136 participants to evaluate how incorrect explanations influence humans' decision-making beha
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#21160;&#24577;&#39046;&#22495;&#30693;&#35782;&#30340;&#33647;&#29289;&#25512;&#33616;&#26694;&#26550;DKINet&#65292;&#23558;&#39046;&#22495;&#30693;&#35782;&#19982;&#24739;&#32773;&#20020;&#24202;&#34920;&#29616;&#30456;&#32467;&#21512;&#65292;&#27492;&#20026;&#39318;&#27425;&#23454;&#39564;&#12290;</title><link>http://arxiv.org/abs/2305.19604</link><description>&lt;p&gt;
&#36890;&#36807;&#39046;&#22495;&#30693;&#35782;&#21551;&#31034;&#30340;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#33647;&#29289;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Medication Recommendation via Domain Knowledge Informed Deep Learning. (arXiv:2305.19604v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19604
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#21160;&#24577;&#39046;&#22495;&#30693;&#35782;&#30340;&#33647;&#29289;&#25512;&#33616;&#26694;&#26550;DKINet&#65292;&#23558;&#39046;&#22495;&#30693;&#35782;&#19982;&#24739;&#32773;&#20020;&#24202;&#34920;&#29616;&#30456;&#32467;&#21512;&#65292;&#27492;&#20026;&#39318;&#27425;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33647;&#29289;&#25512;&#33616;&#26159;&#21307;&#30103;&#20445;&#20581;&#30340;&#22522;&#26412;&#20294;&#33267;&#20851;&#37325;&#35201;&#30340;&#20998;&#25903;&#65292;&#25552;&#20379;&#26426;&#20250;&#20026;&#22797;&#26434;&#20581;&#24247;&#29366;&#20917;&#30340;&#24739;&#32773;&#25903;&#25345;&#20020;&#24202;&#21307;&#29983;&#26356;&#31934;&#30830;&#30340;&#33647;&#29289;&#22788;&#26041;&#12290;&#20174;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#20013;&#23398;&#20064;&#25512;&#33616;&#33647;&#29289;&#26159;&#20808;&#21069;&#30740;&#31350;&#20013;&#26368;&#24120;&#35265;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#30740;&#31350;&#24573;&#35270;&#20102;&#26681;&#25454;&#24739;&#32773;&#30340;EHR&#20013;&#30340;&#20020;&#24202;&#34920;&#29616;&#32435;&#20837;&#39046;&#22495;&#30693;&#35782;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#21160;&#24577;&#39046;&#22495;&#30693;&#35782;&#30340;&#33647;&#29289;&#25512;&#33616;&#26694;&#26550;&#65292;&#21363;&#39046;&#22495;&#30693;&#35782;&#21551;&#31034;&#32593;&#32476;&#65288;DKINet&#65289;&#65292;&#29992;&#20110;&#23558;&#39046;&#22495;&#30693;&#35782;&#19982;&#21487;&#35266;&#23519;&#30340;&#24739;&#32773;&#20020;&#24202;&#34920;&#29616;&#30456;&#32467;&#21512;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#39318;&#20808;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20110;&#39046;&#22495;&#30693;&#35782;&#30340;&#32534;&#30721;&#22120;&#26469;&#25429;&#25417;&#39046;&#22495;&#20449;&#24687;&#65292;&#28982;&#21518;&#24320;&#21457;&#20102;&#19968;&#20010;&#25968;&#25454;&#39537;&#21160;&#30340;&#32534;&#30721;&#22120;&#23558;&#39046;&#22495;&#30693;&#35782;&#25972;&#21512;&#21040;&#21487;&#35266;&#23519;&#30340;EHR&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Medication recommendation is a fundamental yet crucial branch of healthcare, which provides opportunities to support clinical physicians with more accurate medication prescriptions for patients with complex health conditions. Learning from electronic health records (EHR) to recommend medications is the most common way in previous studies. However, most of them neglect incorporating domain knowledge according to the clinical manifestations in the EHR of the patient. To address these issues, we propose a novel \textbf{D}omain \textbf{K}nowledge \textbf{I}nformed \textbf{Net}work (DKINet) to integrate domain knowledge with observable clinical manifestations of the patient, which is the first dynamic domain knowledge informed framework toward medication recommendation. In particular, we first design a knowledge-driven encoder to capture the domain information and then develop a data-driven encoder to integrate domain knowledge into the observable EHR. To endow the model with the capability
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#33539;&#30068;&#35770;&#35821;&#20041;&#30340;&#26041;&#27861;CatE&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#39640;&#25928;&#22320;&#23558;&#26412;&#20307;&#20844;&#29702;&#25237;&#24433;&#21040;&#22270;&#24418;&#20013;&#65292;&#24182;&#29983;&#25104;&#26412;&#20307;&#35770;&#30340;&#23884;&#20837;&#65292;&#20174;&#32780;&#22312;&#30693;&#35782;&#22270;&#35889;&#30340;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#19978;&#21462;&#24471;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.07163</link><description>&lt;p&gt;
CatE&#65306;&#20351;&#29992;&#33539;&#30068;&#35770;&#35821;&#20041;&#23884;&#20837;$\mathcal{ALC}$&#26412;&#20307;&#35770;
&lt;/p&gt;
&lt;p&gt;
CatE: Embedding $\mathcal{ALC}$ ontologies using category-theoretical semantics. (arXiv:2305.07163v1 [cs.LO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07163
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#33539;&#30068;&#35770;&#35821;&#20041;&#30340;&#26041;&#27861;CatE&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#39640;&#25928;&#22320;&#23558;&#26412;&#20307;&#20844;&#29702;&#25237;&#24433;&#21040;&#22270;&#24418;&#20013;&#65292;&#24182;&#29983;&#25104;&#26412;&#20307;&#35770;&#30340;&#23884;&#20837;&#65292;&#20174;&#32780;&#22312;&#30693;&#35782;&#22270;&#35889;&#30340;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#19978;&#21462;&#24471;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;&#35821;&#20041;Web&#26412;&#20307;&#19968;&#36215;&#36827;&#34892;&#26426;&#22120;&#23398;&#20064;&#36981;&#24490;&#20960;&#20010;&#31574;&#30053;&#20043;&#19968;&#65292;&#20854;&#20013;&#21253;&#25324;&#23558;&#26412;&#20307;&#25237;&#24433;&#21040;&#22270;&#24418;&#32467;&#26500;&#20013;&#65292;&#24182;&#23558;&#22270;&#24418;&#23884;&#20837;&#25110;&#22522;&#20110;&#22270;&#24418;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#24212;&#29992;&#20110;&#29983;&#25104;&#30340;&#22270;&#24418;&#12290;&#24050;&#32463;&#24320;&#21457;&#20986;&#20102;&#20960;&#31181;&#23558;&#26412;&#20307;&#20844;&#29702;&#25237;&#24433;&#21040;&#22270;&#24418;&#20013;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#23427;&#20204;&#21487;&#20197;&#25237;&#24433;&#30340;&#20844;&#29702;&#31867;&#22411;&#65288;&#20840;&#38754;&#24615;&#65289;&#12289;&#23427;&#20204;&#26159;&#21542;&#21487;&#36870;&#65288;&#21333;&#23556;&#24615;&#65289;&#20197;&#21450;&#23427;&#20204;&#22914;&#20309;&#21033;&#29992;&#35821;&#20041;&#20449;&#24687;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#36825;&#20123;&#38480;&#21046;&#38480;&#21046;&#20102;&#23427;&#20204;&#21487;&#20197;&#24212;&#29992;&#30340;&#20219;&#21153;&#31867;&#22411;&#12290;&#36923;&#36753;&#35821;&#35328;&#30340;&#33539;&#30068;&#35770;&#35821;&#20041;&#20197;&#33539;&#30068;&#32780;&#19981;&#26159;&#38598;&#21512;&#30340;&#24418;&#24335;&#24418;&#24335;&#21270;&#35299;&#37322;&#65292;&#24182;&#19988;&#33539;&#30068;&#20855;&#26377;&#31867;&#20284;&#20110;&#22270;&#24418;&#30340;&#32467;&#26500;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;CatE&#65292;&#23427;&#20351;&#29992;$\mathcal{ALC}$&#25551;&#36848;&#36923;&#36753;&#30340;&#33539;&#30068;&#35770;&#20844;&#24335;&#26469;&#29983;&#25104;&#26412;&#20307;&#20844;&#29702;&#30340;&#22270;&#24418;&#34920;&#31034;&#12290;CatE&#25237;&#24433;&#20855;&#26377;&#20840;&#38754;&#24615;&#21644;&#21333;&#23556;&#24615;&#65292;&#22240;&#27492;&#20811;&#26381;&#20102;&#20854;&#20182;&#22522;&#20110;&#22270;&#24418;&#26412;&#20307;&#35770;&#30340;&#26041;&#27861;&#30340;&#38480;&#21046;&#12290;CatE&#36824;&#36890;&#36807;&#23558;$\mathcal{ALC}$&#30340;&#35821;&#27861;&#32534;&#30721;&#20026;&#19968;&#31181;&#33539;&#30068;&#26469;&#21033;&#29992;&#35821;&#20041;&#20449;&#24687;&#65292;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#20026;&#26412;&#20307;&#35770;&#29983;&#25104;&#23884;&#20837;&#12290;&#25105;&#20204;&#22312;&#30693;&#35782;&#22270;&#35889;&#30340;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;CatE&#65292;&#24182;&#34920;&#26126;&#23427;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning with Semantic Web ontologies follows several strategies, one of which involves projecting ontologies into graph structures and applying graph embeddings or graph-based machine learning methods to the resulting graphs. Several methods have been developed that project ontology axioms into graphs. However, these methods are limited in the type of axioms they can project (totality), whether they are invertible (injectivity), and how they exploit semantic information. These limitations restrict the kind of tasks to which they can be applied. Category-theoretical semantics of logic languages formalizes interpretations using categories instead of sets, and categories have a graph-like structure. We developed CatE, which uses the category-theoretical formulation of the semantics of the Description Logic $\mathcal{ALC}$ to generate a graph representation for ontology axioms. The CatE projection is total and injective, and therefore overcomes limitations of other graph-based ont
&lt;/p&gt;</description></item><item><title>ImpressionGPT&#26159;&#19968;&#20010;&#21033;&#29992;LLMs&#26500;&#24314;&#21160;&#24577;&#19978;&#19979;&#25991;&#30340;&#36845;&#20195;&#20248;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#25918;&#23556;&#23398;&#25253;&#21578;&#25688;&#35201;&#29983;&#25104;&#12290;&#30456;&#23545;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;ImpressionGPT&#22312;&#20855;&#26377;&#36739;&#22909;&#27867;&#21270;&#24615;&#33021;&#30340;&#21516;&#26102;&#65292;&#25104;&#21151;&#25552;&#39640;&#20102;&#25918;&#23556;&#23398;&#25253;&#21578;&#25688;&#35201;&#30340;&#29983;&#25104;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2304.08448</link><description>&lt;p&gt;
&#22522;&#20110;ChatGPT&#30340;&#25918;&#23556;&#23398;&#25253;&#21578;&#25688;&#35201;&#29983;&#25104;&#30340;&#36845;&#20195;&#20248;&#21270;&#26694;&#26550;&#65306;ImpressionGPT
&lt;/p&gt;
&lt;p&gt;
ImpressionGPT: An Iterative Optimizing Framework for Radiology Report Summarization with ChatGPT. (arXiv:2304.08448v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08448
&lt;/p&gt;
&lt;p&gt;
ImpressionGPT&#26159;&#19968;&#20010;&#21033;&#29992;LLMs&#26500;&#24314;&#21160;&#24577;&#19978;&#19979;&#25991;&#30340;&#36845;&#20195;&#20248;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#25918;&#23556;&#23398;&#25253;&#21578;&#25688;&#35201;&#29983;&#25104;&#12290;&#30456;&#23545;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;ImpressionGPT&#22312;&#20855;&#26377;&#36739;&#22909;&#27867;&#21270;&#24615;&#33021;&#30340;&#21516;&#26102;&#65292;&#25104;&#21151;&#25552;&#39640;&#20102;&#25918;&#23556;&#23398;&#25253;&#21578;&#25688;&#35201;&#30340;&#29983;&#25104;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25918;&#23556;&#23398;&#25253;&#21578;&#20013;&#30340;"Impression"&#37096;&#20998;&#26159;&#25918;&#23556;&#31185;&#21307;&#24072;&#21644;&#20854;&#20182;&#21307;&#29983;&#20132;&#27969;&#30340;&#37325;&#35201;&#22522;&#30784;&#65292;&#36890;&#24120;&#26159;&#22522;&#20110;"Findings"&#37096;&#20998;&#32534;&#20889;&#30340;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#25918;&#23556;&#31185;&#21307;&#24072;&#26469;&#35828;&#65292;&#32534;&#20889;&#22823;&#37327;&#30340;&#21360;&#35937;&#25551;&#36848;&#21487;&#33021;&#26159;&#36153;&#26102;&#36153;&#21147;&#19988;&#23481;&#26131;&#20986;&#38169;&#30340;&#12290;&#23613;&#31649;&#26368;&#36817;&#30340;&#30740;&#31350;&#20351;&#29992;&#22823;&#35268;&#27169;&#21307;&#23398;&#25991;&#26412;&#25968;&#25454;&#36827;&#34892;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#33258;&#21160;&#21360;&#35937;&#29983;&#25104;&#30340;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#20294;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#30340;&#21307;&#23398;&#25991;&#26412;&#25968;&#25454;&#65292;&#24182;&#19988;&#20855;&#26377;&#36739;&#24046;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#34429;&#28982;&#20687;ChatGPT&#36825;&#26679;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#24615;&#33021;&#65292;&#20294;&#23427;&#20204;&#22312;&#29305;&#23450;&#39046;&#22495;&#65288;&#22914;&#25918;&#23556;&#23398;&#65289;&#20013;&#30340;&#34920;&#29616;&#20173;&#28982;&#26410;&#32463;&#35843;&#26597;&#65292;&#21487;&#33021;&#21463;&#21040;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ImpressionGPT&#65292;&#21033;&#29992;LLM&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#65292;&#36890;&#36807;&#20351;&#29992;&#39046;&#22495;&#29305;&#23450;&#30340;&#20010;&#24615;&#21270;&#25968;&#25454;&#26500;&#24314;&#21160;&#24577;&#19978;&#19979;&#25991;&#65292;&#25552;&#39640;&#20102;&#25918;&#23556;&#23398;&#25253;&#21578;&#25688;&#35201;&#30340;&#29983;&#25104;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The 'Impression' section of a radiology report is a critical basis for communication between radiologists and other physicians, and it is typically written by radiologists based on the 'Findings' section. However, writing numerous impressions can be laborious and error-prone for radiologists. Although recent studies have achieved promising results in automatic impression generation using large-scale medical text data for pre-training and fine-tuning pre-trained language models, such models often require substantial amounts of medical text data and have poor generalization performance. While large language models (LLMs) like ChatGPT have shown strong generalization capabilities and performance, their performance in specific domains, such as radiology, remains under-investigated and potentially limited. To address this limitation, we propose ImpressionGPT, which leverages the in-context learning capability of LLMs by constructing dynamic contexts using domain-specific, individualized dat
&lt;/p&gt;</description></item></channel></rss>