<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>PointLLM&#26159;&#19968;&#31181;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29702;&#35299;&#28857;&#20113;&#30340;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#28857;&#20113;&#32534;&#30721;&#22120;&#21644;&#24378;&#22823;&#30340;LLM&#23558;&#20960;&#20309;&#12289;&#22806;&#35266;&#21644;&#35821;&#35328;&#20449;&#24687;&#34701;&#21512;&#65292;&#24182;&#36890;&#36807;&#20154;&#31867;&#25351;&#23548;&#29983;&#25104;&#29615;&#22659;&#19978;&#24688;&#24403;&#30340;&#21709;&#24212;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#25910;&#38598;&#22823;&#35268;&#27169;&#30340;&#28857;-&#25991;&#26412;&#25351;&#20196;&#23545;&#25968;&#25454;&#38598;&#36827;&#34892;&#20004;&#38454;&#27573;&#30340;&#35757;&#32451;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#24863;&#30693;&#33021;&#21147;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.16911</link><description>&lt;p&gt;
PointLLM&#65306;&#36171;&#20104;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29702;&#35299;&#28857;&#20113;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
PointLLM: Empowering Large Language Models to Understand Point Clouds. (arXiv:2308.16911v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16911
&lt;/p&gt;
&lt;p&gt;
PointLLM&#26159;&#19968;&#31181;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29702;&#35299;&#28857;&#20113;&#30340;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#28857;&#20113;&#32534;&#30721;&#22120;&#21644;&#24378;&#22823;&#30340;LLM&#23558;&#20960;&#20309;&#12289;&#22806;&#35266;&#21644;&#35821;&#35328;&#20449;&#24687;&#34701;&#21512;&#65292;&#24182;&#36890;&#36807;&#20154;&#31867;&#25351;&#23548;&#29983;&#25104;&#29615;&#22659;&#19978;&#24688;&#24403;&#30340;&#21709;&#24212;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#25910;&#38598;&#22823;&#35268;&#27169;&#30340;&#28857;-&#25991;&#26412;&#25351;&#20196;&#23545;&#25968;&#25454;&#38598;&#36827;&#34892;&#20004;&#38454;&#27573;&#30340;&#35757;&#32451;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#24863;&#30693;&#33021;&#21147;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#21069;&#25152;&#26410;&#26377;&#30340;&#36827;&#23637;&#23545;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20135;&#29983;&#20102;&#28145;&#36828;&#24433;&#21709;&#65292;&#20294;&#22312;3D&#29702;&#35299;&#39046;&#22495;&#20173;&#26377;&#24453;&#23436;&#20840;&#21457;&#23637;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;PointLLM&#65292;&#36825;&#26159;&#19968;&#39033;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#30340;&#21021;&#27493;&#24037;&#20316;&#65292;&#20351;LLM&#33021;&#22815;&#29702;&#35299;&#28857;&#20113;&#65292;&#24182;&#25552;&#20379;&#20102;&#36229;&#36234;2D&#35270;&#35273;&#25968;&#25454;&#30340;&#26032;&#36884;&#24452;&#12290;PointLLM&#36890;&#36807;&#20154;&#31867;&#25351;&#23548;&#22788;&#29702;&#24102;&#26377;&#39068;&#33394;&#30340;&#29289;&#20307;&#28857;&#20113;&#65292;&#24182;&#29983;&#25104;&#29615;&#22659;&#19978;&#24688;&#24403;&#30340;&#21709;&#24212;&#65292;&#23637;&#31034;&#20102;&#20854;&#23545;&#28857;&#20113;&#21644;&#24120;&#35782;&#30340;&#25484;&#25569;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#23427;&#21033;&#29992;&#20102;&#19968;&#20010;&#28857;&#20113;&#32534;&#30721;&#22120;&#21644;&#19968;&#20010;&#24378;&#22823;&#30340;LLM&#65292;&#26377;&#25928;&#22320;&#34701;&#21512;&#20102;&#20960;&#20309;&#12289;&#22806;&#35266;&#21644;&#35821;&#35328;&#20449;&#24687;&#12290;&#25105;&#20204;&#25910;&#38598;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;66&#19975;&#20010;&#31616;&#21333;&#21644;7&#19975;&#20010;&#22797;&#26434;&#30340;&#28857;-&#25991;&#26412;&#25351;&#20196;&#23545;&#65292;&#20197;&#23454;&#29616;&#20004;&#38454;&#27573;&#30340;&#35757;&#32451;&#31574;&#30053;&#65306;&#39318;&#20808;&#23545;&#40784;&#28508;&#22312;&#31354;&#38388;&#65292;&#28982;&#21518;&#23545;&#32479;&#19968;&#27169;&#22411;&#36827;&#34892;&#25351;&#20196;&#35843;&#25972;&#12290;&#20026;&#20102;&#20005;&#26684;&#35780;&#20272;&#25105;&#20204;&#27169;&#22411;&#30340;&#24863;&#30693;&#33021;&#21147;&#21644;&#20854;&#27867;&#21270;&#33021;&#21147;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#35780;&#20272;&#22522;&#20934;&#25968;&#25454;&#38598;&#36827;&#34892;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
The unprecedented advancements in Large Language Models (LLMs) have created a profound impact on natural language processing but are yet to fully embrace the realm of 3D understanding. This paper introduces PointLLM, a preliminary effort to fill this gap, thereby enabling LLMs to understand point clouds and offering a new avenue beyond 2D visual data. PointLLM processes colored object point clouds with human instructions and generates contextually appropriate responses, illustrating its grasp of point clouds and common sense. Specifically, it leverages a point cloud encoder with a powerful LLM to effectively fuse geometric, appearance, and linguistic information. We collect a novel dataset comprising 660K simple and 70K complex point-text instruction pairs to enable a two-stage training strategy: initially aligning latent spaces and subsequently instruction-tuning the unified model. To rigorously evaluate our model's perceptual abilities and its generalization capabilities, we establis
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#26080;&#26465;&#20214;&#35270;&#39057;&#29983;&#25104;&#30340;&#26032;&#22411;&#36816;&#21160;&#29983;&#25104;&#22120;&#35774;&#35745;&#65292;&#21033;&#29992;&#22522;&#20110;&#23398;&#20064;&#30340;&#21453;&#21521;&#32593;&#32476;&#23454;&#29616;&#20102;&#36816;&#21160;&#36830;&#36143;&#24615;&#21644;&#29983;&#25104;&#31354;&#38388;&#30340;&#32422;&#26463;&#12290;</title><link>http://arxiv.org/abs/2308.16909</link><description>&lt;p&gt;
StyleInV:&#19968;&#31181;&#29992;&#20110;&#26080;&#26465;&#20214;&#35270;&#39057;&#29983;&#25104;&#30340;&#26102;&#38388;&#39118;&#26684;&#35843;&#21046;&#36870;&#21521;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
StyleInV: A Temporal Style Modulated Inversion Network for Unconditional Video Generation. (arXiv:2308.16909v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16909
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#26080;&#26465;&#20214;&#35270;&#39057;&#29983;&#25104;&#30340;&#26032;&#22411;&#36816;&#21160;&#29983;&#25104;&#22120;&#35774;&#35745;&#65292;&#21033;&#29992;&#22522;&#20110;&#23398;&#20064;&#30340;&#21453;&#21521;&#32593;&#32476;&#23454;&#29616;&#20102;&#36816;&#21160;&#36830;&#36143;&#24615;&#21644;&#29983;&#25104;&#31354;&#38388;&#30340;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#26465;&#20214;&#35270;&#39057;&#29983;&#25104;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#38656;&#35201;&#21512;&#25104;&#26082;&#36830;&#36143;&#21448;&#25345;&#32493;&#26102;&#38388;&#36739;&#38271;&#30340;&#39640;&#36136;&#37327;&#35270;&#39057;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#30740;&#31350;&#32773;&#20204;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;StyleGAN&#22270;&#20687;&#29983;&#25104;&#22120;&#36827;&#34892;&#39640;&#36136;&#37327;&#24103;&#21512;&#25104;&#65292;&#24182;&#19987;&#27880;&#20110;&#36816;&#21160;&#29983;&#25104;&#22120;&#30340;&#35774;&#35745;&#12290;&#36816;&#21160;&#29983;&#25104;&#22120;&#36890;&#36807;&#20351;&#29992;&#37325;&#22411;&#19977;&#32500;&#21367;&#31215;&#36776;&#21035;&#22120;&#36827;&#34892;&#36845;&#20195;&#35757;&#32451;&#65292;&#20197;&#30830;&#20445;&#35270;&#39057;&#29983;&#25104;&#36807;&#31243;&#20013;&#30340;&#36816;&#21160;&#36830;&#36143;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36816;&#21160;&#29983;&#25104;&#22120;&#35774;&#35745;&#65292;&#23427;&#20351;&#29992;&#20102;&#22522;&#20110;&#23398;&#20064;&#30340;&#21453;&#21521;&#32593;&#32476;&#26469;&#23454;&#29616;GAN&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20013;&#30340;&#32534;&#30721;&#22120;&#20174;&#32534;&#30721;&#22270;&#20687;&#21040;&#28508;&#21464;&#37327;&#20013;&#25429;&#25417;&#21040;&#20102;&#20016;&#23500;&#32780;&#24179;&#28369;&#30340;&#20808;&#39564;&#30693;&#35782;&#65292;&#24182;&#19988;&#22312;&#32473;&#23450;&#21021;&#22987;&#29983;&#25104;&#24103;&#30340;&#28508;&#21464;&#37327;&#20316;&#20026;&#25351;&#23548;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#22312;&#26102;&#38388;&#19978;&#35843;&#21046;&#21453;&#21521;&#32534;&#30721;&#22120;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#29983;&#25104;&#24179;&#28369;&#30340;&#26410;&#26469;&#28508;&#21464;&#37327;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#31232;&#30095;&#35757;&#32451;&#30340;&#20248;&#21183;&#65292;&#24182;&#19988;&#36890;&#36807;&#21021;&#22987;&#24103;&#24341;&#23548;&#30340;&#21453;&#21521;&#32593;&#32476;&#26377;&#25928;&#32422;&#26463;&#20102;&#36816;&#21160;&#29983;&#25104;&#22120;&#30340;&#29983;&#25104;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unconditional video generation is a challenging task that involves synthesizing high-quality videos that are both coherent and of extended duration. To address this challenge, researchers have used pretrained StyleGAN image generators for high-quality frame synthesis and focused on motion generator design. The motion generator is trained in an autoregressive manner using heavy 3D convolutional discriminators to ensure motion coherence during video generation. In this paper, we introduce a novel motion generator design that uses a learning-based inversion network for GAN. The encoder in our method captures rich and smooth priors from encoding images to latents, and given the latent of an initially generated frame as guidance, our method can generate smooth future latent by modulating the inversion encoder temporally. Our method enjoys the advantage of sparse training and naturally constrains the generation space of our motion generator with the inversion network guided by the initial fr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;InterDiff&#65292;&#19968;&#31181;&#20351;&#29992;&#29289;&#29702;&#20449;&#24687;&#25193;&#25955;&#29983;&#25104;&#19977;&#32500;&#20154;&#29289;-&#29289;&#20307;&#20132;&#20114;&#30340;&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#20004;&#20010;&#20851;&#38190;&#27493;&#39588;&#65306;&#20132;&#20114;&#25193;&#25955;&#21644;&#20132;&#20114;&#26657;&#27491;&#65292;&#26377;&#25928;&#22320;&#27169;&#25311;&#20102;&#20855;&#26377;&#19981;&#21516;&#24418;&#29366;&#30340;&#21160;&#24577;&#29289;&#20307;&#21644;&#20840;&#36523;&#36816;&#21160;&#30340;&#29289;&#20307;&#20132;&#20114;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.16905</link><description>&lt;p&gt;
InterDiff: &#20351;&#29992;&#29289;&#29702;&#20449;&#24687;&#25193;&#25955;&#29983;&#25104;&#19977;&#32500;&#20154;&#29289;-&#29289;&#20307;&#20132;&#20114;
&lt;/p&gt;
&lt;p&gt;
InterDiff: Generating 3D Human-Object Interactions with Physics-Informed Diffusion. (arXiv:2308.16905v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16905
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;InterDiff&#65292;&#19968;&#31181;&#20351;&#29992;&#29289;&#29702;&#20449;&#24687;&#25193;&#25955;&#29983;&#25104;&#19977;&#32500;&#20154;&#29289;-&#29289;&#20307;&#20132;&#20114;&#30340;&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#20004;&#20010;&#20851;&#38190;&#27493;&#39588;&#65306;&#20132;&#20114;&#25193;&#25955;&#21644;&#20132;&#20114;&#26657;&#27491;&#65292;&#26377;&#25928;&#22320;&#27169;&#25311;&#20102;&#20855;&#26377;&#19981;&#21516;&#24418;&#29366;&#30340;&#21160;&#24577;&#29289;&#20307;&#21644;&#20840;&#36523;&#36816;&#21160;&#30340;&#29289;&#20307;&#20132;&#20114;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20219;&#21153;&#65292;&#21363;&#39044;&#27979;&#19977;&#32500;&#20154;&#29289;-&#29289;&#20307;&#20132;&#20114;&#65288;HOIs&#65289;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;HOI&#21512;&#25104;&#30740;&#31350;&#32570;&#20047;&#19982;&#21160;&#24577;&#29289;&#20307;&#30340;&#20840;&#36523;&#20132;&#20114;&#65292;&#20363;&#22914;&#65292;&#24448;&#24448;&#23616;&#38480;&#20110;&#25805;&#32437;&#23567;&#22411;&#25110;&#38745;&#24577;&#29289;&#20307;&#12290;&#25105;&#20204;&#30340;&#20219;&#21153;&#26356;&#20855;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#23427;&#38656;&#35201;&#23545;&#20855;&#26377;&#19981;&#21516;&#24418;&#29366;&#30340;&#21160;&#24577;&#29289;&#20307;&#36827;&#34892;&#24314;&#27169;&#65292;&#25429;&#25417;&#25972;&#20307;&#36816;&#21160;&#65292;&#24182;&#30830;&#20445;&#29289;&#29702;&#26377;&#25928;&#30340;&#20132;&#20114;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;InterDiff&#65292;&#19968;&#20010;&#26694;&#26550;&#65292;&#21253;&#25324;&#20004;&#20010;&#20851;&#38190;&#27493;&#39588;&#65306;&#65288;i&#65289;&#20132;&#20114;&#25193;&#25955;&#65292;&#25105;&#20204;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#23545;&#26410;&#26469;&#30340;&#20154;&#29289;-&#29289;&#20307;&#20132;&#20114;&#20998;&#24067;&#36827;&#34892;&#32534;&#30721;&#65307;&#65288;ii&#65289;&#20132;&#20114;&#26657;&#27491;&#65292;&#22312;&#25193;&#25955;&#27493;&#39588;&#20013;&#24341;&#20837;&#29289;&#29702;&#20449;&#24687;&#30340;&#39044;&#27979;&#22120;&#26469;&#32416;&#27491;&#21435;&#22122;&#30340;HOIs&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#35265;&#35299;&#26159;&#27880;&#20837;&#20808;&#39564;&#30693;&#35782;&#65292;&#21363;&#19982;&#25509;&#35302;&#28857;&#30456;&#20851;&#30340;&#20132;&#20114;&#36981;&#24490;&#31616;&#21333;&#30340;&#27169;&#24335;&#65292;&#24182;&#19988;&#26131;&#20110;&#39044;&#27979;&#12290;&#22312;&#22810;&#20010;&#20154;&#29289;-&#29289;&#20307;&#20132;&#20114;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper addresses a novel task of anticipating 3D human-object interactions (HOIs). Most existing research on HOI synthesis lacks comprehensive whole-body interactions with dynamic objects, e.g., often limited to manipulating small or static objects. Our task is significantly more challenging, as it requires modeling dynamic objects with various shapes, capturing whole-body motion, and ensuring physically valid interactions. To this end, we propose InterDiff, a framework comprising two key steps: (i) interaction diffusion, where we leverage a diffusion model to encode the distribution of future human-object interactions; (ii) interaction correction, where we introduce a physics-informed predictor to correct denoised HOIs in a diffusion step. Our key insight is to inject prior knowledge that the interactions under reference with respect to contact points follow a simple pattern and are easily predictable. Experiments on multiple human-object interaction datasets demonstrate the effec
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#24314;&#31435;&#20102;&#33258;&#27880;&#24847;&#21147;&#21644;&#30828;&#38388;&#38548;&#25903;&#25345;&#21521;&#37327;&#26426;&#38382;&#39064;&#20043;&#38388;&#30340;&#27491;&#24335;&#31561;&#20215;&#20851;&#31995;&#65292;&#36890;&#36807;&#36716;&#25442;&#22120;&#26550;&#26500;&#30340;&#20248;&#21270;&#20960;&#20309;&#26469;&#35299;&#20915;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#38382;&#39064;&#65292;&#21516;&#26102;&#25581;&#31034;&#20102;&#26799;&#24230;&#19979;&#38477;&#20248;&#21270;&#30340;&#36716;&#25442;&#22120;&#30340;&#38544;&#24335;&#20559;&#24046;&#12290;</title><link>http://arxiv.org/abs/2308.16898</link><description>&lt;p&gt;
Transformers&#20316;&#20026;&#25903;&#25345;&#21521;&#37327;&#26426;
&lt;/p&gt;
&lt;p&gt;
Transformers as Support Vector Machines. (arXiv:2308.16898v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16898
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#24314;&#31435;&#20102;&#33258;&#27880;&#24847;&#21147;&#21644;&#30828;&#38388;&#38548;&#25903;&#25345;&#21521;&#37327;&#26426;&#38382;&#39064;&#20043;&#38388;&#30340;&#27491;&#24335;&#31561;&#20215;&#20851;&#31995;&#65292;&#36890;&#36807;&#36716;&#25442;&#22120;&#26550;&#26500;&#30340;&#20248;&#21270;&#20960;&#20309;&#26469;&#35299;&#20915;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#38382;&#39064;&#65292;&#21516;&#26102;&#25581;&#31034;&#20102;&#26799;&#24230;&#19979;&#38477;&#20248;&#21270;&#30340;&#36716;&#25442;&#22120;&#30340;&#38544;&#24335;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20174;"Attention Is All You Need"&#20013;&#24341;&#20837;&#36716;&#25442;&#22120;&#26550;&#26500;&#20197;&#26469;&#65292;&#23427;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#21462;&#24471;&#20102;&#38761;&#21629;&#24615;&#30340;&#36827;&#23637;&#12290;&#36716;&#25442;&#22120;&#20013;&#30340;&#27880;&#24847;&#21147;&#23618;&#25509;&#21463;&#36755;&#20837;&#20196;&#29260;&#24207;&#21015;$X$&#24182;&#36890;&#36807;&#35745;&#31639;softmax$(XQK^\top X^\top)$&#30340;&#25104;&#23545;&#30456;&#20284;&#24615;&#20351;&#23427;&#20204;&#30456;&#20114;&#20316;&#29992;&#65292;&#20854;&#20013;$(K,Q)$&#26159;&#21487;&#35757;&#32451;&#30340;&#38190;-&#26597;&#35810;&#21442;&#25968;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#33258;&#27880;&#24847;&#21147;&#20248;&#21270;&#20960;&#20309;&#21644;&#19968;&#20010;&#30828;&#38388;&#38548;&#25903;&#25345;&#21521;&#37327;&#26426;&#38382;&#39064;&#20043;&#38388;&#30340;&#27491;&#24335;&#31561;&#20215;&#20851;&#31995;&#65292;&#36890;&#36807;&#23545;&#20196;&#29260;&#23545;&#30340;&#22806;&#31215;&#26045;&#21152;&#32447;&#24615;&#32422;&#26463;&#65292;&#23558;&#26368;&#20339;&#36755;&#20837;&#20196;&#29260;&#19982;&#38750;&#26368;&#20339;&#20196;&#29260;&#20998;&#31163;&#12290;&#36825;&#20010;&#24418;&#24335;&#20027;&#20041;&#20351;&#25105;&#20204;&#33021;&#22815;&#34920;&#24449;&#26799;&#24230;&#19979;&#38477;&#20248;&#21270;&#30340;&#21333;&#23618;&#36716;&#25442;&#22120;&#30340;&#38544;&#24335;&#20559;&#24046;&#65306;(1)&#20248;&#21270;&#27880;&#24847;&#21147;&#23618;&#65292;&#20351;&#29992;&#21487;&#21464;&#27491;&#21017;&#21270;&#21442;&#25968;$(K,Q)$&#65292;&#25910;&#25947;&#30340;&#26041;&#21521;&#26159;&#19968;&#20010;&#26368;&#23567;&#21270;&#32508;&#21512;&#21442;&#25968;$W=KQ^\top$&#30340;&#26680;&#33539;&#25968;&#30340;&#25903;&#25345;&#21521;&#37327;&#26426;&#35299;&#20915;&#26041;&#26696;&#12290;&#32780;&#30452;&#25509;&#20351;&#29992;$W$&#36827;&#34892;&#21442;&#25968;&#21270;&#21017;&#26368;&#23567;&#21270;&#19968;&#20010;Frobenius&#33539;&#25968;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Since its inception in "Attention Is All You Need", transformer architecture has led to revolutionary advancements in NLP. The attention layer within the transformer admits a sequence of input tokens $X$ and makes them interact through pairwise similarities computed as softmax$(XQK^\top X^\top)$, where $(K,Q)$ are the trainable key-query parameters. In this work, we establish a formal equivalence between the optimization geometry of self-attention and a hard-margin SVM problem that separates optimal input tokens from non-optimal tokens using linear constraints on the outer-products of token pairs. This formalism allows us to characterize the implicit bias of 1-layer transformers optimized with gradient descent: (1) Optimizing the attention layer with vanishing regularization, parameterized by $(K,Q)$, converges in direction to an SVM solution minimizing the nuclear norm of the combined parameter $W=KQ^\top$. Instead, directly parameterizing by $W$ minimizes a Frobenius norm objective. 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#19977;&#36879;&#35270;&#22278;&#26609;&#35270;&#22270;&#34920;&#31034;&#26041;&#27861;&#65292;&#22312;&#33258;&#21160;&#39550;&#39542;&#20013;&#36827;&#34892;&#28857;&#20113;&#19977;&#32500;&#35821;&#20041;&#21344;&#25454;&#39044;&#27979;&#12290;&#36890;&#36807;&#26500;&#24314;&#26609;&#22352;&#26631;&#31995;&#19979;&#30340;&#19977;&#36879;&#35270;&#22278;&#26609;&#35270;&#22270;&#65292;&#23454;&#29616;&#20102;&#23545;&#28857;&#20113;&#30340;&#31934;&#32454;&#24314;&#27169;&#65292;&#21516;&#26102;&#21033;&#29992;&#31354;&#38388;&#32452;&#27744;&#21270;&#21644;&#20108;&#32500;&#20027;&#24178;&#32593;&#32476;&#39640;&#25928;&#22788;&#29702;&#25968;&#25454;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#39044;&#27979;&#28857;&#20113;&#30340;&#35821;&#20041;&#21344;&#25454;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2308.16896</link><description>&lt;p&gt;
PointOcc: &#22522;&#20110;&#28857;&#20113;&#30340;&#19977;&#36879;&#35270;&#22278;&#26609;&#35270;&#22270;&#29992;&#20110;&#28857;&#20113;&#19977;&#32500;&#35821;&#20041;&#21344;&#25454;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
PointOcc: Cylindrical Tri-Perspective View for Point-based 3D Semantic Occupancy Prediction. (arXiv:2308.16896v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16896
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#19977;&#36879;&#35270;&#22278;&#26609;&#35270;&#22270;&#34920;&#31034;&#26041;&#27861;&#65292;&#22312;&#33258;&#21160;&#39550;&#39542;&#20013;&#36827;&#34892;&#28857;&#20113;&#19977;&#32500;&#35821;&#20041;&#21344;&#25454;&#39044;&#27979;&#12290;&#36890;&#36807;&#26500;&#24314;&#26609;&#22352;&#26631;&#31995;&#19979;&#30340;&#19977;&#36879;&#35270;&#22278;&#26609;&#35270;&#22270;&#65292;&#23454;&#29616;&#20102;&#23545;&#28857;&#20113;&#30340;&#31934;&#32454;&#24314;&#27169;&#65292;&#21516;&#26102;&#21033;&#29992;&#31354;&#38388;&#32452;&#27744;&#21270;&#21644;&#20108;&#32500;&#20027;&#24178;&#32593;&#32476;&#39640;&#25928;&#22788;&#29702;&#25968;&#25454;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#39044;&#27979;&#28857;&#20113;&#30340;&#35821;&#20041;&#21344;&#25454;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#20013;&#30340;&#35821;&#20041;&#20998;&#21106;&#27491;&#20174;&#31232;&#30095;&#28857;&#20998;&#21106;&#21457;&#23637;&#21040;&#23494;&#38598;&#20307;&#32032;&#20998;&#21106;&#65292;&#30446;&#26631;&#26159;&#39044;&#27979;&#25152;&#20851;&#27880;&#30340;&#19977;&#32500;&#31354;&#38388;&#20013;&#27599;&#20010;&#20307;&#32032;&#30340;&#35821;&#20041;&#21344;&#25454;&#24773;&#20917;&#12290;&#29616;&#26377;&#30340;&#22522;&#20110;&#20108;&#32500;&#25237;&#24433;&#30340;&#26041;&#27861;&#65288;&#22914;&#40479;&#30640;&#22270;&#12289;&#36317;&#31163;&#35270;&#22270;&#31561;&#65289;&#22240;&#20026;&#39044;&#27979;&#31354;&#38388;&#30340;&#23494;&#38598;&#24615;&#32780;&#26080;&#25928;&#65292;&#22240;&#20026;&#23427;&#20204;&#21482;&#33021;&#25551;&#36848;&#19977;&#32500;&#22330;&#26223;&#30340;&#23376;&#31354;&#38388;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#19977;&#36879;&#35270;&#22278;&#26609;&#35270;&#22270;&#26469;&#26377;&#25928;&#32780;&#20840;&#38754;&#22320;&#34920;&#31034;&#28857;&#20113;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#28857;&#20113;&#27169;&#22411; PointOcc &#26469;&#39640;&#25928;&#22320;&#22788;&#29702;&#23427;&#20204;&#12290;&#32771;&#34385;&#21040;&#28608;&#20809;&#38647;&#36798;&#28857;&#20113;&#30340;&#36317;&#31163;&#20998;&#24067;&#65292;&#25105;&#20204;&#21033;&#29992;&#26609;&#22352;&#26631;&#31995;&#26500;&#24314;&#20102;&#19977;&#36879;&#35270;&#22278;&#26609;&#35270;&#22270;&#65292;&#20197;&#26356;&#32454;&#31890;&#24230;&#22320;&#23545;&#36817;&#21306;&#22495;&#36827;&#34892;&#24314;&#27169;&#12290;&#25105;&#20204;&#37319;&#29992;&#31354;&#38388;&#32452;&#27744;&#21270;&#26469;&#20445;&#30041;&#25237;&#24433;&#36807;&#31243;&#20013;&#30340;&#32467;&#26500;&#32454;&#33410;&#65292;&#24182;&#37319;&#29992;&#20108;&#32500;&#20027;&#24178;&#32593;&#36335;&#39640;&#25928;&#22788;&#29702;&#27599;&#20010;&#36879;&#35270;&#38754;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#32858;&#21512;&#30340;&#26041;&#24335;&#33719;&#24471;&#27599;&#20010;&#28857;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Semantic segmentation in autonomous driving has been undergoing an evolution from sparse point segmentation to dense voxel segmentation, where the objective is to predict the semantic occupancy of each voxel in the concerned 3D space. The dense nature of the prediction space has rendered existing efficient 2D-projection-based methods (e.g., bird's eye view, range view, etc.) ineffective, as they can only describe a subspace of the 3D scene. To address this, we propose a cylindrical tri-perspective view to represent point clouds effectively and comprehensively and a PointOcc model to process them efficiently. Considering the distance distribution of LiDAR point clouds, we construct the tri-perspective view in the cylindrical coordinate system for more fine-grained modeling of nearer areas. We employ spatial group pooling to maintain structural details during projection and adopt 2D backbones to efficiently process each TPV plane. Finally, we obtain the features of each point by aggregat
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#35821;&#35328;&#26465;&#20214;&#36335;&#24452;&#35268;&#21010;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#30896;&#25758;&#20989;&#25968;&#65292;&#39044;&#27979;&#26426;&#22120;&#20154;&#19982;&#29615;&#22659;&#20043;&#38388;&#30340;&#30896;&#25758;&#65292;&#23454;&#29616;&#28789;&#27963;&#12289;&#26377;&#26465;&#20214;&#30340;&#36335;&#24452;&#35268;&#21010;&#65292;&#26080;&#38656;&#25163;&#21160;&#26631;&#27880;&#25110;&#32773;&#30495;&#23454;&#29289;&#20307;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2308.16893</link><description>&lt;p&gt;
&#35821;&#35328;&#26465;&#20214;&#36335;&#24452;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Language-Conditioned Path Planning. (arXiv:2308.16893v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16893
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#35821;&#35328;&#26465;&#20214;&#36335;&#24452;&#35268;&#21010;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#30896;&#25758;&#20989;&#25968;&#65292;&#39044;&#27979;&#26426;&#22120;&#20154;&#19982;&#29615;&#22659;&#20043;&#38388;&#30340;&#30896;&#25758;&#65292;&#23454;&#29616;&#28789;&#27963;&#12289;&#26377;&#26465;&#20214;&#30340;&#36335;&#24452;&#35268;&#21010;&#65292;&#26080;&#38656;&#25163;&#21160;&#26631;&#27880;&#25110;&#32773;&#30495;&#23454;&#29289;&#20307;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25509;&#35302;&#26159;&#26426;&#22120;&#20154;&#25805;&#20316;&#30340;&#26680;&#24515;&#12290;&#26377;&#26102;&#20505;&#65292;&#25105;&#20204;&#24076;&#26395;&#20351;&#29992;&#25509;&#35302;&#65288;&#20363;&#22914;&#25805;&#32437;&#21644;&#25235;&#21462;&#65289;&#65292;&#32780;&#26377;&#26102;&#20505;&#65292;&#25509;&#35302;&#26159;&#26377;&#23475;&#30340;&#65288;&#20363;&#22914;&#36991;&#20813;&#38556;&#30861;&#29289;&#65289;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#36335;&#24452;&#35268;&#21010;&#31639;&#27861;&#21482;&#20851;&#27880;&#20110;&#26080;&#30896;&#25758;&#36335;&#24452;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#25509;&#35302;&#20016;&#23500;&#20219;&#21153;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#35821;&#35328;&#26465;&#20214;&#36335;&#24452;&#35268;&#21010;&#30340;&#39046;&#22495;&#65292;&#23558;&#25509;&#35302;&#24863;&#30693;&#24615;&#34701;&#20837;&#21040;&#36335;&#24452;&#35268;&#21010;&#38382;&#39064;&#20013;&#12290;&#20316;&#20026;&#35813;&#39046;&#22495;&#30340;&#31532;&#19968;&#27493;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#35821;&#35328;&#26465;&#20214;&#30896;&#25758;&#20989;&#25968;&#65288;LACO&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21482;&#20351;&#29992;&#21333;&#35270;&#22270;&#22270;&#20687;&#12289;&#35821;&#35328;&#25552;&#31034;&#21644;&#26426;&#22120;&#20154;&#37197;&#32622;&#26469;&#23398;&#20064;&#30896;&#25758;&#20989;&#25968;&#12290;LACO&#21487;&#20197;&#39044;&#27979;&#26426;&#22120;&#20154;&#21644;&#29615;&#22659;&#20043;&#38388;&#30340;&#30896;&#25758;&#65292;&#20174;&#32780;&#23454;&#29616;&#28789;&#27963;&#12289;&#26377;&#26465;&#20214;&#30340;&#36335;&#24452;&#35268;&#21010;&#65292;&#26080;&#38656;&#25163;&#21160;&#23545;&#35937;&#26631;&#27880;&#12289;&#28857;&#20113;&#25968;&#25454;&#25110;&#30495;&#23454;&#29289;&#20307;&#27169;&#22411;&#12290;&#22312;&#20223;&#30495;&#21644;&#23454;&#38469;&#29615;&#22659;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;LACO&#21487;&#20197;&#23454;&#29616;&#22797;&#26434;&#32780;&#32454;&#33268;&#30340;&#36335;&#24452;&#35268;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contact is at the core of robotic manipulation. At times, it is desired (e.g. manipulation and grasping), and at times, it is harmful (e.g. when avoiding obstacles). However, traditional path planning algorithms focus solely on collision-free paths, limiting their applicability in contact-rich tasks. To address this limitation, we propose the domain of Language-Conditioned Path Planning, where contact-awareness is incorporated into the path planning problem. As a first step in this domain, we propose Language-Conditioned Collision Functions (LACO) a novel approach that learns a collision function using only a single-view image, language prompt, and robot configuration. LACO predicts collisions between the robot and the environment, enabling flexible, conditional path planning without the need for manual object annotations, point cloud data, or ground-truth object meshes. In both simulation and the real world, we demonstrate that LACO can facilitate complex, nuanced path plans that allo
&lt;/p&gt;</description></item><item><title>ReZero&#26159;&#19968;&#20010;&#21487;&#23450;&#21046;&#30340;&#21306;&#22495;&#22768;&#38899;&#25552;&#21462;&#26694;&#26550;&#65292;&#38024;&#23545;R-SE&#20219;&#21153;&#35774;&#35745;&#20102;&#19981;&#21516;&#31867;&#22411;&#30340;&#31354;&#38388;&#21306;&#22495;&#30340;&#23450;&#20041;&#12289;&#31354;&#38388;&#29305;&#24449;&#25552;&#21462;&#21644;&#32858;&#21512;&#26041;&#27861;&#20197;&#21450;&#22810;&#36890;&#36947;&#25193;&#23637;&#30340;BSRNN&#27169;&#22411;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.16892</link><description>&lt;p&gt;
ReZero&#65306;&#21487;&#23450;&#21046;&#21306;&#22495;&#22768;&#38899;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
ReZero: Region-customizable Sound Extraction. (arXiv:2308.16892v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16892
&lt;/p&gt;
&lt;p&gt;
ReZero&#26159;&#19968;&#20010;&#21487;&#23450;&#21046;&#30340;&#21306;&#22495;&#22768;&#38899;&#25552;&#21462;&#26694;&#26550;&#65292;&#38024;&#23545;R-SE&#20219;&#21153;&#35774;&#35745;&#20102;&#19981;&#21516;&#31867;&#22411;&#30340;&#31354;&#38388;&#21306;&#22495;&#30340;&#23450;&#20041;&#12289;&#31354;&#38388;&#29305;&#24449;&#25552;&#21462;&#21644;&#32858;&#21512;&#26041;&#27861;&#20197;&#21450;&#22810;&#36890;&#36947;&#25193;&#23637;&#30340;BSRNN&#27169;&#22411;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;ReZero&#65292;&#19968;&#20010;&#36890;&#29992;&#19988;&#28789;&#27963;&#30340;&#22810;&#36890;&#36947;&#21306;&#22495;&#22768;&#38899;&#25552;&#21462;&#65288;R-SE&#65289;&#20219;&#21153;&#30340;&#26694;&#26550;&#12290;R-SE&#20219;&#21153;&#26088;&#22312;&#22312;&#29305;&#23450;&#30340;&#29992;&#25143;&#23450;&#20041;&#30340;&#31354;&#38388;&#21306;&#22495;&#20869;&#25552;&#21462;&#25152;&#26377;&#27963;&#21160;&#30340;&#30446;&#26631;&#22768;&#38899;&#65288;&#20363;&#22914;&#20154;&#31867;&#35821;&#38899;&#65289;&#65292;&#36825;&#19982;&#36890;&#24120;&#20551;&#35774;&#30450;&#20998;&#31163;&#25110;&#22266;&#23450;&#39044;&#23450;&#20041;&#31354;&#38388;&#21306;&#22495;&#30340;&#20256;&#32479;&#20219;&#21153;&#19981;&#21516;&#12290;&#31354;&#38388;&#21306;&#22495;&#21487;&#20197;&#23450;&#20041;&#20026;&#35282;&#24230;&#31383;&#21475;&#12289;&#29699;&#20307;&#12289;&#38181;&#20307;&#25110;&#20854;&#20182;&#20960;&#20309;&#27169;&#24335;&#12290;&#20316;&#20026;R-SE&#20219;&#21153;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#25552;&#20986;&#30340;ReZero&#26694;&#26550;&#21253;&#25324;&#65288;1&#65289;&#19981;&#21516;&#31867;&#22411;&#31354;&#38388;&#21306;&#22495;&#30340;&#23450;&#20041;&#65292;&#65288;2&#65289;&#31354;&#38388;&#29305;&#24449;&#25552;&#21462;&#21644;&#32858;&#21512;&#30340;&#26041;&#27861;&#65292;&#20197;&#21450;&#65288;3&#65289;&#36866;&#29992;&#20110;R-SE&#20219;&#21153;&#30340;&#24102;&#20998;&#21106;RNN&#65288;BSRNN&#65289;&#27169;&#22411;&#30340;&#22810;&#36890;&#36947;&#25193;&#23637;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19981;&#21516;&#30340;&#40614;&#20811;&#39118;&#38453;&#21015;&#20960;&#20309;&#24418;&#29366;&#30340;&#23454;&#39564;&#65292;&#19981;&#21516;&#31867;&#22411;&#30340;&#31354;&#38388;&#21306;&#22495;&#21644;&#20851;&#20110;&#19981;&#21516;&#31995;&#32479;&#37197;&#32622;&#30340;&#32508;&#21512;&#28040;&#34701;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce region-customizable sound extraction (ReZero), a general and flexible framework for the multi-channel region-wise sound extraction (R-SE) task. R-SE task aims at extracting all active target sounds (e.g., human speech) within a specific, user-defined spatial region, which is different from conventional and existing tasks where a blind separation or a fixed, predefined spatial region are typically assumed. The spatial region can be defined as an angular window, a sphere, a cone, or other geometric patterns. Being a solution to the R-SE task, the proposed ReZero framework includes (1) definitions of different types of spatial regions, (2) methods for region feature extraction and aggregation, and (3) a multi-channel extension of the band-split RNN (BSRNN) model specified for the R-SE task. We design experiments for different microphone array geometries, different types of spatial regions, and comprehensive ablation studies on different system configurations. Experimental res
&lt;/p&gt;</description></item><item><title>Belebele&#26159;&#19968;&#20010;&#21253;&#21547;122&#31181;&#35821;&#35328;&#21464;&#20307;&#30340;&#22810;&#36873;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#25968;&#25454;&#38598;&#65292;&#21487;&#29992;&#20110;&#35780;&#20272;&#25991;&#26412;&#27169;&#22411;&#22312;&#39640;&#12289;&#20013;&#21644;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#30340;&#24615;&#33021;&#12290;&#23613;&#31649;&#33521;&#35821;&#20026;&#20013;&#24515;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36328;&#35821;&#35328;&#36716;&#31227;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#23567;&#22411;&#22810;&#35821;&#35328;&#36974;&#34109;&#35821;&#35328;&#27169;&#22411;&#22312;&#20854;&#20182;&#35821;&#35328;&#19978;&#34920;&#29616;&#26356;&#20339;&#12290;</title><link>http://arxiv.org/abs/2308.16884</link><description>&lt;p&gt;
Belebele&#22522;&#20934;&#25968;&#25454;&#38598;&#65306;122&#31181;&#35821;&#35328;&#21464;&#20307;&#30340;&#24182;&#34892;&#38405;&#35835;&#29702;&#35299;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
The Belebele Benchmark: a Parallel Reading Comprehension Dataset in 122 Language Variants. (arXiv:2308.16884v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16884
&lt;/p&gt;
&lt;p&gt;
Belebele&#26159;&#19968;&#20010;&#21253;&#21547;122&#31181;&#35821;&#35328;&#21464;&#20307;&#30340;&#22810;&#36873;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#25968;&#25454;&#38598;&#65292;&#21487;&#29992;&#20110;&#35780;&#20272;&#25991;&#26412;&#27169;&#22411;&#22312;&#39640;&#12289;&#20013;&#21644;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#30340;&#24615;&#33021;&#12290;&#23613;&#31649;&#33521;&#35821;&#20026;&#20013;&#24515;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36328;&#35821;&#35328;&#36716;&#31227;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#23567;&#22411;&#22810;&#35821;&#35328;&#36974;&#34109;&#35821;&#35328;&#27169;&#22411;&#22312;&#20854;&#20182;&#35821;&#35328;&#19978;&#34920;&#29616;&#26356;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;Belebele&#65292;&#19968;&#20010;&#21253;&#21547;122&#31181;&#35821;&#35328;&#21464;&#20307;&#30340;&#22810;&#36873;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#65288;MRC&#65289;&#25968;&#25454;&#38598;&#12290;&#35813;&#25968;&#25454;&#38598;&#26497;&#22823;&#22320;&#25193;&#23637;&#20102;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#65288;NLU&#65289;&#22522;&#20934;&#30340;&#35821;&#35328;&#35206;&#30422;&#33539;&#22260;&#65292;&#20351;&#24471;&#21487;&#20197;&#35780;&#20272;&#25991;&#26412;&#27169;&#22411;&#22312;&#39640;&#12289;&#20013;&#21644;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#30340;&#24615;&#33021;&#12290;&#27599;&#20010;&#38382;&#39064;&#37117;&#22522;&#20110;Flores-200&#25968;&#25454;&#38598;&#20013;&#30340;&#19968;&#20010;&#30701;&#31687;&#25991;&#31456;&#65292;&#24182;&#25552;&#20379;&#20102;&#22235;&#20010;&#22810;&#36873;&#31572;&#26696;&#12290;&#38382;&#39064;&#32463;&#36807;&#31934;&#24515;&#31574;&#21010;&#65292;&#20197;&#21306;&#20998;&#20855;&#26377;&#19981;&#21516;&#36890;&#29992;&#35821;&#35328;&#29702;&#35299;&#27700;&#24179;&#30340;&#27169;&#22411;&#12290;&#21333;&#29420;&#30340;&#33521;&#35821;&#25968;&#25454;&#38598;&#24050;&#32463;&#36275;&#22815;&#22256;&#38590;&#65292;&#21487;&#20197;&#25361;&#25112;&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;&#30001;&#20110;&#23436;&#20840;&#24182;&#34892;&#65292;&#35813;&#25968;&#25454;&#38598;&#21487;&#20197;&#30452;&#25509;&#27604;&#36739;&#25152;&#26377;&#35821;&#35328;&#30340;&#27169;&#22411;&#24615;&#33021;&#12290;&#25105;&#20204;&#20351;&#29992;&#35813;&#25968;&#25454;&#38598;&#35780;&#20272;&#22810;&#35821;&#35328;&#36974;&#34109;&#35821;&#35328;&#27169;&#22411;&#65288;MLMs&#65289;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#24191;&#27867;&#30340;&#32467;&#26524;&#65292;&#24182;&#21457;&#29616;&#23613;&#31649;&#33521;&#35821;&#20026;&#20013;&#24515;&#30340;LLMs&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#30340;&#36328;&#35821;&#35328;&#36716;&#31227;&#65292;&#20294;&#23567;&#22411;MLMs&#22312;&#20854;&#20182;&#35821;&#35328;&#19978;&#30340;&#34920;&#29616;&#30456;&#23545;&#36739;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Belebele, a multiple-choice machine reading comprehension (MRC) dataset spanning 122 language variants. Significantly expanding the language coverage of natural language understanding (NLU) benchmarks, this dataset enables the evaluation of text models in high-, medium-, and low-resource languages. Each question is based on a short passage from the Flores-200 dataset and has four multiple-choice answers. The questions were carefully curated to discriminate between models with different levels of general language comprehension. The English dataset on its own proves difficult enough to challenge state-of-the-art language models. Being fully parallel, this dataset enables direct comparison of model performance across all languages. We use this dataset to evaluate the capabilities of multilingual masked language models (MLMs) and large language models (LLMs). We present extensive results and find that despite significant cross-lingual transfer in English-centric LLMs, much small
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#32771;&#34385;&#20102;&#36866;&#24212;&#36895;&#24230;&#20998;&#26512;&#20844;&#24179;&#24863;&#30693;&#22240;&#26524;&#27169;&#22411;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#30740;&#31350;&#24102;&#26377;&#22240;&#26524;-&#20559;&#24046;-&#25928;&#26524;&#32467;&#26500;&#30340;&#31616;&#21333;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#65292;&#25506;&#35752;&#20102;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#32771;&#34385;&#25935;&#24863;&#21464;&#37327;&#65288;&#20559;&#24046;&#65289;&#30340;&#20844;&#24179;&#24615;&#21644;&#36866;&#24212;&#36895;&#29575;&#30340;&#30456;&#20851;&#22240;&#32032;&#12290;</title><link>http://arxiv.org/abs/2308.16879</link><description>&lt;p&gt;
&#36866;&#24212;&#36895;&#24230;&#20998;&#26512;&#20844;&#24179;&#24863;&#30693;&#22240;&#26524;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Adaptation Speed Analysis for Fairness-aware Causal Models. (arXiv:2308.16879v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16879
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#32771;&#34385;&#20102;&#36866;&#24212;&#36895;&#24230;&#20998;&#26512;&#20844;&#24179;&#24863;&#30693;&#22240;&#26524;&#27169;&#22411;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#30740;&#31350;&#24102;&#26377;&#22240;&#26524;-&#20559;&#24046;-&#25928;&#26524;&#32467;&#26500;&#30340;&#31616;&#21333;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#65292;&#25506;&#35752;&#20102;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#32771;&#34385;&#25935;&#24863;&#21464;&#37327;&#65288;&#20559;&#24046;&#65289;&#30340;&#20844;&#24179;&#24615;&#21644;&#36866;&#24212;&#36895;&#29575;&#30340;&#30456;&#20851;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20030;&#20363;&#22914;&#26426;&#22120;&#32763;&#35793;&#20219;&#21153;&#65292;&#22312;&#20004;&#31181;&#35821;&#35328;&#20043;&#38388;&#23454;&#29616;&#21452;&#21521;&#32763;&#35793;&#26102;&#65292;&#36890;&#24120;&#23558;&#28304;&#35821;&#26009;&#24211;&#29992;&#20316;&#30446;&#26631;&#35821;&#26009;&#24211;&#65292;&#36825;&#28041;&#21450;&#21040;&#35757;&#32451;&#20004;&#20010;&#20855;&#26377;&#30456;&#21453;&#26041;&#21521;&#30340;&#27169;&#22411;&#12290;&#22312;&#35768;&#22810;&#39046;&#22495;&#20013;&#65292;&#21738;&#20010;&#27169;&#22411;&#21487;&#20197;&#26368;&#24555;&#22320;&#36866;&#24212;&#39046;&#22495;&#36716;&#21464;&#30340;&#38382;&#39064;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#32771;&#34385;&#19968;&#20010;&#30001;&#20110;&#26410;&#30693;&#24178;&#39044;&#32780;&#25913;&#21464;&#30340;&#21407;&#22987;&#20998;&#24067;p&#65292;&#23548;&#33268;&#20986;&#29616;&#20462;&#25913;&#21518;&#30340;&#20998;&#24067;p*&#12290;&#22312;&#23558;p&#19982;p*&#23545;&#40784;&#26102;&#65292;&#26377;&#22810;&#31181;&#22240;&#32032;&#21487;&#33021;&#24433;&#21709;&#36866;&#24212;&#36895;&#29575;&#65292;&#21253;&#25324;p&#20013;&#21464;&#37327;&#20043;&#38388;&#30340;&#22240;&#26524;&#20381;&#36182;&#20851;&#31995;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#65292;&#25105;&#20204;&#24517;&#39035;&#32771;&#34385;&#35757;&#32451;&#36807;&#31243;&#30340;&#20844;&#24179;&#24615;&#65292;&#24182;&#19988;&#22312;&#22240;&#26524;&#20851;&#31995;&#20013;&#28041;&#21450;&#21040;&#19968;&#20010;&#25935;&#24863;&#21464;&#37327;&#65288;&#20559;&#24046;&#65289;&#22312;&#21407;&#22240;&#21464;&#37327;&#21644;&#25928;&#26524;&#21464;&#37327;&#20043;&#38388;&#26159;&#23588;&#20026;&#20851;&#38190;&#30340;&#12290;&#20026;&#20102;&#25506;&#32034;&#36825;&#31181;&#24773;&#20917;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#20010;&#24102;&#26377;&#22240;&#26524;-&#20559;&#24046;-&#25928;&#26524;&#32467;&#26500;&#30340;&#31616;&#21333;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#65288;SCM)&#65292;&#20854;&#20013;&#21464;&#37327;A&#22312;&#21407;&#22240;&#65288;X&#65289;&#21644;&#25928;&#26524;&#65288;Y&#65289;&#20043;&#38388;&#20805;&#24403;&#25935;&#24863;&#21464;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
For example, in machine translation tasks, to achieve bidirectional translation between two languages, the source corpus is often used as the target corpus, which involves the training of two models with opposite directions. The question of which one can adapt most quickly to a domain shift is of significant importance in many fields. Specifically, consider an original distribution p that changes due to an unknown intervention, resulting in a modified distribution p*. In aligning p with p*, several factors can affect the adaptation rate, including the causal dependencies between variables in p. In real-life scenarios, however, we have to consider the fairness of the training process, and it is particularly crucial to involve a sensitive variable (bias) present between a cause and an effect variable. To explore this scenario, we examine a simple structural causal model (SCM) with a cause-bias-effect structure, where variable A acts as a sensitive variable between cause (X) and effect (Y
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;Gender-GAP Pipeline&#65292;&#19968;&#20010;&#29992;&#20110;55&#31181;&#35821;&#35328;&#20013;&#24615;&#21035;&#34920;&#24449;&#30340;&#33258;&#21160;&#27969;&#27700;&#32447;&#65292;&#36890;&#36807;&#20351;&#29992;&#22810;&#35821;&#35328;&#24615;&#21035;&#20154;&#31216;&#21517;&#35789;&#35789;&#27719;&#34920;&#23545;&#25991;&#26412;&#36827;&#34892;&#37327;&#21270;&#26469;&#25253;&#21578;&#25968;&#25454;&#20013;&#30340;&#24615;&#21035;&#34920;&#24449;&#12290;&#22312;WMT&#35757;&#32451;&#25968;&#25454;&#21644;&#26032;&#38395;&#20219;&#21153;&#30340;&#24320;&#21457;&#25968;&#25454;&#20013;&#34920;&#26126;&#24403;&#21069;&#25968;&#25454;&#20559;&#21521;&#30007;&#24615;&#34920;&#24449;&#12290;</title><link>http://arxiv.org/abs/2308.16871</link><description>&lt;p&gt;
The Gender-GAP Pipeline: &#19968;&#20010;&#29992;&#20110;55&#31181;&#35821;&#35328;&#20013;&#24615;&#21035;&#34920;&#24449;&#30340;&#24615;&#21035;&#24863;&#30693;&#22810;&#35821;&#35328;&#27969;&#27700;&#32447;
&lt;/p&gt;
&lt;p&gt;
The Gender-GAP Pipeline: A Gender-Aware Polyglot Pipeline for Gender Characterisation in 55 Languages. (arXiv:2308.16871v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16871
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;Gender-GAP Pipeline&#65292;&#19968;&#20010;&#29992;&#20110;55&#31181;&#35821;&#35328;&#20013;&#24615;&#21035;&#34920;&#24449;&#30340;&#33258;&#21160;&#27969;&#27700;&#32447;&#65292;&#36890;&#36807;&#20351;&#29992;&#22810;&#35821;&#35328;&#24615;&#21035;&#20154;&#31216;&#21517;&#35789;&#35789;&#27719;&#34920;&#23545;&#25991;&#26412;&#36827;&#34892;&#37327;&#21270;&#26469;&#25253;&#21578;&#25968;&#25454;&#20013;&#30340;&#24615;&#21035;&#34920;&#24449;&#12290;&#22312;WMT&#35757;&#32451;&#25968;&#25454;&#21644;&#26032;&#38395;&#20219;&#21153;&#30340;&#24320;&#21457;&#25968;&#25454;&#20013;&#34920;&#26126;&#24403;&#21069;&#25968;&#25454;&#20559;&#21521;&#30007;&#24615;&#34920;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#29983;&#25104;&#31995;&#32479;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#24456;&#38590;&#34987;&#32531;&#35299;&#12290;&#20854;&#20013;&#19968;&#20010;&#21487;&#33021;&#23548;&#33268;&#36825;&#31181;&#20559;&#35265;&#30340;&#21407;&#22240;&#26159;&#35757;&#32451;&#21644;&#35780;&#20272;&#25968;&#25454;&#20013;&#30340;&#24615;&#21035;&#34920;&#24449;&#19981;&#24179;&#34913;&#12290;&#23613;&#31649;&#26368;&#36817;&#22312;&#35760;&#24405;&#36825;&#20010;&#38382;&#39064;&#21644;&#35797;&#22270;&#32531;&#35299;&#23427;&#26041;&#38754;&#21462;&#24471;&#20102;&#19968;&#20123;&#36827;&#23637;&#65292;&#20294;&#25105;&#20204;&#20173;&#28982;&#32570;&#20047;&#20849;&#20139;&#30340;&#26041;&#27861;&#35770;&#21644;&#24037;&#20855;&#65292;&#20197;&#25253;&#21578;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#20013;&#30340;&#24615;&#21035;&#34920;&#24449;&#12290;&#36825;&#31181;&#23450;&#37327;&#25253;&#21578;&#23558;&#20351;&#36827;&#19968;&#27493;&#32531;&#35299;&#25104;&#20026;&#21487;&#33021;&#65292;&#20363;&#22914;&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#12290;&#26412;&#25991;&#25551;&#36848;&#20102;Gender-GAP Pipeline&#65288;&#29992;&#20110;&#24615;&#21035;&#24863;&#30693;&#30340;&#22810;&#35821;&#35328;&#27969;&#27700;&#32447;&#65289;&#65292;&#23427;&#26159;&#19968;&#20010;&#33258;&#21160;&#27969;&#31243;&#65292;&#29992;&#20110;&#23545;55&#31181;&#35821;&#35328;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#36827;&#34892;&#24615;&#21035;&#34920;&#24449;&#12290;&#35813;&#27969;&#27700;&#32447;&#20351;&#29992;&#19968;&#20010;&#22810;&#35821;&#35328;&#24615;&#21035;&#20154;&#31216;&#21517;&#35789;&#35789;&#27719;&#34920;&#26469;&#37327;&#21270;&#25991;&#26412;&#20013;&#30340;&#24615;&#21035;&#34920;&#24449;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#23427;&#26469;&#25253;&#21578;WMT&#35757;&#32451;&#25968;&#25454;&#21644;&#26032;&#38395;&#20219;&#21153;&#30340;&#24320;&#21457;&#25968;&#25454;&#20013;&#30340;&#24615;&#21035;&#34920;&#24449;&#65292;&#35777;&#23454;&#24403;&#21069;&#25968;&#25454;&#20559;&#21521;&#30007;&#24615;&#34920;&#24449;&#12290;&#25317;&#26377;&#19981;&#24179;&#34913;&#30340;&#25968;&#25454;&#38598;&#21487;&#33021;&#20250;&#38388;&#25509;&#22320;&#20248;&#21270;&#25105;&#20204;&#30340;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
Gender biases in language generation systems are challenging to mitigate. One possible source for these biases is gender representation disparities in the training and evaluation data. Despite recent progress in documenting this problem and many attempts at mitigating it, we still lack shared methodology and tooling to report gender representation in large datasets. Such quantitative reporting will enable further mitigation, e.g., via data augmentation. This paper describes the Gender-GAP Pipeline (for Gender-Aware Polyglot Pipeline), an automatic pipeline to characterize gender representation in large-scale datasets for 55 languages. The pipeline uses a multilingual lexicon of gendered person-nouns to quantify the gender representation in text. We showcase it to report gender representation in WMT training data and development data for the News task, confirming that current data is skewed towards masculine representation. Having unbalanced datasets may indirectly optimize our systems 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#30693;&#35782;&#20849;&#20139;&#21644;&#20010;&#24615;&#21270;&#23398;&#20064;&#39550;&#39542;&#21592;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#20197;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#12290;&#36890;&#36807;&#36328;&#36710;&#36742;&#20849;&#20139;&#30693;&#35782;&#65292;&#22686;&#21152;AVs&#22312;&#30495;&#23454;&#39550;&#39542;&#24773;&#26223;&#20013;&#30340;&#26292;&#38706;&#65292;&#21516;&#26102;&#20445;&#30041;&#20010;&#24615;&#21270;&#30340;&#27169;&#22411;&#65292;&#20197;&#36866;&#24212;&#19981;&#21516;&#36710;&#36742;&#30340;&#26465;&#20214;&#21644;&#29305;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.16870</link><description>&lt;p&gt;
&#36890;&#36807;&#30693;&#35782;&#20849;&#20139;&#21644;&#20010;&#24615;&#21270;&#23398;&#20064;&#39550;&#39542;&#21592;&#27169;&#22411;&#20197;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;
&lt;/p&gt;
&lt;p&gt;
Learning Driver Models for Automated Vehicles via Knowledge Sharing and Personalization. (arXiv:2308.16870v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16870
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#30693;&#35782;&#20849;&#20139;&#21644;&#20010;&#24615;&#21270;&#23398;&#20064;&#39550;&#39542;&#21592;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#20197;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#12290;&#36890;&#36807;&#36328;&#36710;&#36742;&#20849;&#20139;&#30693;&#35782;&#65292;&#22686;&#21152;AVs&#22312;&#30495;&#23454;&#39550;&#39542;&#24773;&#26223;&#20013;&#30340;&#26292;&#38706;&#65292;&#21516;&#26102;&#20445;&#30041;&#20010;&#24615;&#21270;&#30340;&#27169;&#22411;&#65292;&#20197;&#36866;&#24212;&#19981;&#21516;&#36710;&#36742;&#30340;&#26465;&#20214;&#21644;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;&#19968;&#31181;&#36890;&#36807;&#36710;&#36742;&#20043;&#38388;&#30340;&#30693;&#35782;&#20849;&#20139;&#21644;&#20010;&#24615;&#21270;&#23398;&#20064;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#65288;AVs&#65289;&#39550;&#39542;&#21592;&#27169;&#22411;&#30340;&#26694;&#26550;&#12290;&#20132;&#36890;&#36816;&#36755;&#31995;&#32479;&#20013;&#30340;&#22266;&#26377;&#21464;&#24322;&#20351;&#24471;&#23558;AVs&#26292;&#38706;&#20110;&#25152;&#26377;&#21487;&#33021;&#30340;&#39550;&#39542;&#24773;&#26223;&#22312;&#23454;&#35777;&#23454;&#39564;&#25110;&#27979;&#35797;&#20013;&#26497;&#20855;&#25361;&#25112;&#24615;&#12290;&#22240;&#27492;&#65292;AVs&#21487;&#33021;&#20250;&#23545;&#26576;&#20123;&#34987;&#35748;&#20026;&#23545;&#20854;&#23433;&#20840;&#21644;&#39640;&#25928;&#36816;&#34892;&#26377;&#23475;&#30340;&#36973;&#36935;&#35270;&#32780;&#19981;&#35265;&#12290;&#22240;&#27492;&#65292;&#36328;AVs&#20849;&#20139;&#30693;&#35782;&#65292;&#22686;&#21152;&#20854;&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#39550;&#39542;&#24773;&#26223;&#30340;&#26292;&#38706;&#65292;&#23601;&#26174;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25506;&#32034;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#20849;&#20139;&#30693;&#35782;&#21644;&#20511;&#29992;&#22810;&#20010;&#36710;&#36742;&#20043;&#38388;&#30340;&#24378;&#24230;&#65292;&#21327;&#21516;&#22521;&#35757;&#39550;&#39542;&#21592;&#27169;&#22411;&#65292;&#21516;&#26102;&#20445;&#30041;&#36866;&#24212;&#36710;&#36742;&#30340;&#29420;&#29305;&#26465;&#20214;&#21644;&#29305;&#24615;&#30340;&#20010;&#24615;&#21270;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#37319;&#29992;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#22810;&#20010;&#36710;&#36742;&#20043;&#38388;&#36827;&#34892;&#21327;&#20316;&#65292;&#21516;&#26102;&#36991;&#20813;&#20102;&#23427;&#20204;&#20043;&#38388;&#20849;&#20139;&#21407;&#22987;&#25968;&#25454;&#30340;&#38656;&#35201;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#23454;&#39564;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper describes a framework for learning Automated Vehicles (AVs) driver models via knowledge sharing between vehicles and personalization. The innate variability in the transportation system makes it exceptionally challenging to expose AVs to all possible driving scenarios during empirical experimentation or testing. Consequently, AVs could be blind to certain encounters that are deemed detrimental to their safe and efficient operation. It is then critical to share knowledge across AVs that increase exposure to driving scenarios occurring in the real world. This paper explores a method to collaboratively train a driver model by sharing knowledge and borrowing strength across vehicles while retaining a personalized model tailored to the vehicle's unique conditions and properties. Our model brings a federated learning approach to collaborate between multiple vehicles while circumventing the need to share raw data between them. We showcase our method's performance in experimental si
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#19968;&#31181;&#22522;&#20110;&#21306;&#22359;&#38142;&#30340;&#23433;&#20840;&#36828;&#31243;&#24739;&#32773;&#30417;&#27979;&#26694;&#26550;&#65292;&#20351;&#29992;IoMT&#25216;&#26415;&#21644;&#31070;&#32463;&#21050;&#28608;&#35774;&#22791;&#65292;&#23454;&#29616;&#20102;&#26080;&#20405;&#20837;&#24615;&#30340;&#36828;&#31243;&#31070;&#32463;&#21050;&#28608;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2308.16857</link><description>&lt;p&gt;
&#22522;&#20110;&#21306;&#22359;&#38142;&#30340;&#23433;&#20840;&#36828;&#31243;&#24739;&#32773;&#30417;&#27979;&#26694;&#26550;&#19982;&#31070;&#32463;&#21050;&#28608;&#35774;&#22791;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
IoMT-Blockchain based Secured Remote Patient Monitoring Framework for Neuro-Stimulation Device. (arXiv:2308.16857v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16857
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#19968;&#31181;&#22522;&#20110;&#21306;&#22359;&#38142;&#30340;&#23433;&#20840;&#36828;&#31243;&#24739;&#32773;&#30417;&#27979;&#26694;&#26550;&#65292;&#20351;&#29992;IoMT&#25216;&#26415;&#21644;&#31070;&#32463;&#21050;&#28608;&#35774;&#22791;&#65292;&#23454;&#29616;&#20102;&#26080;&#20405;&#20837;&#24615;&#30340;&#36828;&#31243;&#31070;&#32463;&#21050;&#28608;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#29289;&#21307;&#23398;&#24037;&#31243;&#20013;&#30340;&#21307;&#30103;&#29289;&#32852;&#32593;&#65288;IoMT&#65289;&#27491;&#22312;&#24110;&#21161;&#25913;&#21892;&#21307;&#30103;&#34892;&#19994;&#20013;&#30005;&#23376;&#35774;&#22791;&#30340;&#20934;&#30830;&#24615;&#12289;&#21487;&#38752;&#24615;&#21644;&#29983;&#20135;&#21147;&#12290;&#36890;&#36807;&#31359;&#25140;&#24335;IoMT&#35774;&#22791;&#65288;&#22914;&#20855;&#22791;&#22810;&#31181;&#21151;&#33021;&#30340;&#31070;&#32463;&#21050;&#28608;&#35774;&#22791;&#65289;&#30340;&#24555;&#36895;&#24320;&#21457;&#65292;&#21487;&#20197;&#25552;&#20379;&#24739;&#32773;&#30340;&#23454;&#26102;&#24863;&#30693;&#25968;&#25454;&#65292;&#24182;&#36827;&#34892;&#21518;&#32493;&#20998;&#26512;&#12290;&#29289;&#32852;&#32593;&#25968;&#25454;&#34987;&#25910;&#38598;&#12289;&#20998;&#26512;&#21644;&#23384;&#20648;&#22312;&#19968;&#20010;&#22320;&#26041;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#38598;&#20013;&#21270;&#21487;&#33021;&#20250;&#20986;&#29616;&#21333;&#28857;&#25925;&#38556;&#12289;&#25968;&#25454;&#31713;&#25913;&#12289;&#38544;&#31169;&#38382;&#39064;&#31561;&#25361;&#25112;&#12290;&#30001;&#20110;&#20854;&#20998;&#25955;&#21270;&#30340;&#29305;&#24615;&#65292;&#21306;&#22359;&#38142;&#65288;BC&#65289;&#21487;&#20197;&#32531;&#35299;&#36825;&#20123;&#38382;&#39064;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22522;&#20110;IoMT&#30340;&#32463;&#39045;&#30452;&#27969;&#30005;&#21050;&#28608;&#65288;tDCS&#65289;&#24314;&#31435;&#26080;&#20405;&#20837;&#24615;&#36828;&#31243;&#31070;&#32463;&#21050;&#28608;&#31995;&#32479;&#30340;&#21487;&#34892;&#24615;&#12290;&#24050;&#24320;&#21457;&#20102;&#19968;&#27454;&#22522;&#20110;&#30828;&#20214;&#30340;tDCS&#21407;&#22411;&#35774;&#22791;&#65292;&#21487;&#20197;&#36890;&#36807;&#23433;&#21331;&#24212;&#29992;&#31243;&#24207;&#22312;&#20114;&#32852;&#32593;&#19978;&#36827;&#34892;&#25805;&#20316;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;...
&lt;/p&gt;
&lt;p&gt;
Biomedical Engineering's Internet of Medical Things (IoMT) is helping to improve the accuracy, dependability, and productivity of electronic equipment in the healthcare business. Real-time sensory data from patients may be delivered and subsequently analyzed through rapid development of wearable IoMT devices, such as neuro-stimulation devices with a range of functions. Data from the Internet of Things is gathered, analyzed, and stored in a single location. However, single-point failure, data manipulation, privacy difficulties, and other challenges might arise as a result of centralization. Due to its decentralized nature, blockchain (BC) can alleviate these issues. The viability of establishing a non-invasive remote neurostimulation system employing IoMT-based transcranial Direct Current Stimulation is investigated in this work (tDCS). A hardware-based prototype tDCS device has been developed that can be operated over the internet using an android application. Our suggested framework a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;BERT&#34893;&#29983;&#30340;&#35821;&#20041;&#23884;&#20837;&#26469;&#25552;&#39640;&#27468;&#22768;&#21512;&#25104;&#30340;&#34920;&#36798;&#21147;&#65292;&#39318;&#27425;&#24341;&#20837;&#27468;&#35789;&#30340;&#25991;&#26412;&#34920;&#31034;&#20316;&#20026;&#38468;&#21152;&#36755;&#20837;&#65292;&#24182;&#36890;&#36807;&#39069;&#22806;&#30340;&#33021;&#37327;&#39044;&#27979;&#22120;&#21644;&#37325;&#26032;&#35774;&#35745;&#30340;&#38899;&#39640;&#39044;&#27979;&#22120;&#26469;&#22686;&#24378;&#27468;&#22768;&#30340;&#34920;&#36798;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.16836</link><description>&lt;p&gt;
&#36890;&#36807;BERT&#34893;&#29983;&#30340;&#35821;&#20041;&#20449;&#24687;&#25552;&#21319;&#27468;&#22768;&#21512;&#25104;&#30340;&#34920;&#36798;&#21147;
&lt;/p&gt;
&lt;p&gt;
Towards Improving the Expressiveness of Singing Voice Synthesis with BERT Derived Semantic Information. (arXiv:2308.16836v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16836
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;BERT&#34893;&#29983;&#30340;&#35821;&#20041;&#23884;&#20837;&#26469;&#25552;&#39640;&#27468;&#22768;&#21512;&#25104;&#30340;&#34920;&#36798;&#21147;&#65292;&#39318;&#27425;&#24341;&#20837;&#27468;&#35789;&#30340;&#25991;&#26412;&#34920;&#31034;&#20316;&#20026;&#38468;&#21152;&#36755;&#20837;&#65292;&#24182;&#36890;&#36807;&#39069;&#22806;&#30340;&#33021;&#37327;&#39044;&#27979;&#22120;&#21644;&#37325;&#26032;&#35774;&#35745;&#30340;&#38899;&#39640;&#39044;&#27979;&#22120;&#26469;&#22686;&#24378;&#27468;&#22768;&#30340;&#34920;&#36798;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#30340;&#39640;&#36136;&#37327;&#27468;&#22768;&#21512;&#25104;&#65288;SVS&#65289;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#20351;&#29992;&#26469;&#33258;Transformers&#65288;BERT&#65289;&#30340;&#21452;&#21521;&#32534;&#30721;&#22120;&#34920;&#31034;&#34893;&#29983;&#30340;&#35821;&#20041;&#23884;&#20837;&#26469;&#25552;&#39640;&#21512;&#25104;&#27468;&#22768;&#30340;&#34920;&#36798;&#21147;&#12290;&#22522;&#20110;&#26368;&#36817;&#25552;&#20986;&#30340;VISinger&#30340;&#20027;&#35201;&#26550;&#26500;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20960;&#31181;&#29305;&#23450;&#30340;&#35774;&#35745;&#29992;&#20110;&#34920;&#36798;&#24615;&#27468;&#22768;&#21512;&#25104;&#12290;&#39318;&#20808;&#65292;&#19982;&#20197;&#24448;&#30340;SVS&#27169;&#22411;&#19981;&#21516;&#65292;&#25105;&#20204;&#20351;&#29992;&#20174;&#39044;&#35757;&#32451;BERT&#20013;&#25552;&#21462;&#30340;&#27468;&#35789;&#30340;&#25991;&#26412;&#34920;&#31034;&#20316;&#20026;&#27169;&#22411;&#30340;&#38468;&#21152;&#36755;&#20837;&#12290;&#36825;&#20010;&#34920;&#31034;&#21253;&#21547;&#20102;&#20851;&#20110;&#27468;&#35789;&#35821;&#20041;&#30340;&#20449;&#24687;&#65292;&#21487;&#20197;&#24110;&#21161;SVS&#31995;&#32479;&#20135;&#29983;&#26356;&#23500;&#26377;&#34920;&#29616;&#21147;&#21644;&#33258;&#28982;&#30340;&#22768;&#38899;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#24341;&#20837;&#20102;&#19968;&#20010;&#33021;&#37327;&#39044;&#27979;&#22120;&#26469;&#31283;&#23450;&#21512;&#25104;&#22768;&#38899;&#65292;&#24182;&#23545;&#33021;&#37327;&#21464;&#21270;&#30340;&#26356;&#24191;&#27867;&#33539;&#22260;&#36827;&#34892;&#24314;&#27169;&#65292;&#36825;&#20063;&#26377;&#21161;&#20110;&#27468;&#22768;&#30340;&#34920;&#36798;&#21147;&#12290;&#26368;&#21518;&#65292;&#20026;&#20102;&#20943;&#23567;&#38899;&#39640;&#38382;&#39064;&#65292;&#37325;&#26032;&#35774;&#35745;&#20102;&#38899;&#39640;&#39044;&#27979;&#22120;&#26469;&#39044;&#27979;&#23454;&#38469;&#21040;&#38899;&#31526;&#30340;&#38899;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents an end-to-end high-quality singing voice synthesis (SVS) system that uses bidirectional encoder representation from Transformers (BERT) derived semantic embeddings to improve the expressiveness of the synthesized singing voice. Based on the main architecture of recently proposed VISinger, we put forward several specific designs for expressive singing voice synthesis. First, different from the previous SVS models, we use text representation of lyrics extracted from pre-trained BERT as additional input to the model. The representation contains information about semantics of the lyrics, which could help SVS system produce more expressive and natural voice. Second, we further introduce an energy predictor to stabilize the synthesized voice and model the wider range of energy variations that also contribute to the expressiveness of singing voice. Last but not the least, to attenuate the off-key issues, the pitch predictor is re-designed to predict the real to note pitch 
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#21457;&#29616;&#65292;&#32534;&#31243;&#35821;&#35328;&#21487;&#20197;&#22312;&#25351;&#20196;&#35843;&#20248;&#38454;&#27573;&#30456;&#20114;&#20419;&#36827;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#24444;&#27492;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.16824</link><description>&lt;p&gt;
&#32534;&#31243;&#35821;&#35328;&#33021;&#36890;&#36807;&#25351;&#20196;&#35843;&#20248;&#30456;&#20114;&#25552;&#21319;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Programming Languages Boost Each Other via Instruction Tuning?. (arXiv:2308.16824v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16824
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#21457;&#29616;&#65292;&#32534;&#31243;&#35821;&#35328;&#21487;&#20197;&#22312;&#25351;&#20196;&#35843;&#20248;&#38454;&#27573;&#30456;&#20114;&#20419;&#36827;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#24444;&#27492;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#20154;&#31867;&#31243;&#24207;&#21592;&#25484;&#25569;&#20102;&#19968;&#31181;&#32534;&#31243;&#35821;&#35328;&#21518;&#65292;&#23398;&#20064;&#19968;&#31181;&#26032;&#30340;&#32534;&#31243;&#35821;&#35328;&#20250;&#26356;&#23481;&#26131;&#12290;&#22312;&#26412;&#25253;&#21578;&#20013;&#65292;&#25105;&#20204;&#37325;&#28857;&#25506;&#35752;&#20102;&#22312;&#20195;&#30721;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#25351;&#20196;&#24494;&#35843;&#38454;&#27573;&#20013;&#65292;&#32534;&#31243;&#35821;&#35328;&#26159;&#21542;&#33021;&#22815;&#36890;&#36807;&#30456;&#20114;&#25552;&#21319;&#26469;&#22686;&#24378;&#24444;&#27492;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;StarCoder&#19978;&#23545;8&#31181;&#27969;&#34892;&#30340;&#32534;&#31243;&#35821;&#35328;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#65288;Python&#65292;JavaScript&#65292;TypeScript&#65292;C&#65292;C ++&#65292;Java&#65292;Go&#65292;HTML&#65289;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#32534;&#31243;&#35821;&#35328;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#24444;&#27492;&#30340;&#33021;&#21147;&#12290;&#20363;&#22914;&#65292;&#36890;&#36807;&#22312;Python&#19978;&#35757;&#32451;&#30340;CodeM-Python 15B&#21487;&#20197;&#20351;Java&#30340;pass@1&#29575;&#32477;&#23545;&#22686;&#21152;&#20102;17.95&#65285;&#12290;&#26356;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#36890;&#36807;&#22312;HTML&#35821;&#26009;&#24211;&#19978;&#35757;&#32451;&#30340;CodeM-HTML 7B&#21487;&#20197;&#20351;Java&#30340;pass@1&#29575;&#32477;&#23545;&#22686;&#21152;&#20102;15.24&#65285;&#12290;&#25105;&#20204;&#30340;&#35757;&#32451;&#25968;&#25454;&#24050;&#32463;&#21457;&#24067;&#22312;https://github.com/NL2Code/CodeM&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
When human programmers have mastered a programming language, it would be easier when they learn a new programming language. In this report, we focus on exploring whether programming languages can boost each other during the instruction fine-tuning phase of code large language models. We conduct extensive experiments of 8 popular programming languages (Python, JavaScript, TypeScript, C, C++, Java, Go, HTML) on StarCoder. Results demonstrate that programming languages can significantly improve each other. For example, CodeM-Python 15B trained on Python is able to increase Java by an absolute 17.95% pass@1 on HumanEval-X. More surprisingly, we found that CodeM-HTML 7B trained on the HTML corpus can improve Java by an absolute 15.24% pass@1. Our training data is released at https://github.com/NL2Code/CodeM.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#23618;&#32423;&#25968;&#25454;&#38598;&#30340;&#25193;&#23637;&#22810;&#36755;&#20986;&#39640;&#26031;&#36807;&#31243; (MOGPs)&#65292;&#36890;&#36807;&#24341;&#20837;&#28508;&#22312;&#21464;&#37327;&#21644;&#29305;&#23450;&#26680;&#20989;&#25968;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#25429;&#25417;&#19981;&#21516;&#36755;&#20986;&#20043;&#38388;&#30340;&#20851;&#31995;&#21644;&#30456;&#20851;&#24615;&#12290;&#36825;&#31181;&#26041;&#27861;&#39044;&#35745;&#33021;&#22815;&#22312;&#20219;&#21153;&#25968;&#37327;&#22686;&#21152;&#26102;&#25552;&#39640;&#21487;&#25193;&#23637;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.16822</link><description>&lt;p&gt;
&#38024;&#23545;&#23618;&#32423;&#25968;&#25454;&#38598;&#30340;&#28508;&#22312;&#21464;&#37327;&#22810;&#36755;&#20986;&#39640;&#26031;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
Latent Variable Multi-output Gaussian Processes for Hierarchical Datasets. (arXiv:2308.16822v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16822
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#23618;&#32423;&#25968;&#25454;&#38598;&#30340;&#25193;&#23637;&#22810;&#36755;&#20986;&#39640;&#26031;&#36807;&#31243; (MOGPs)&#65292;&#36890;&#36807;&#24341;&#20837;&#28508;&#22312;&#21464;&#37327;&#21644;&#29305;&#23450;&#26680;&#20989;&#25968;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#25429;&#25417;&#19981;&#21516;&#36755;&#20986;&#20043;&#38388;&#30340;&#20851;&#31995;&#21644;&#30456;&#20851;&#24615;&#12290;&#36825;&#31181;&#26041;&#27861;&#39044;&#35745;&#33021;&#22815;&#22312;&#20219;&#21153;&#25968;&#37327;&#22686;&#21152;&#26102;&#25552;&#39640;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#36755;&#20986;&#39640;&#26031;&#36807;&#31243;&#65288;MOGPs&#65289;&#24050;&#32463;&#34987;&#24341;&#20837;&#65292;&#36890;&#36807;&#21033;&#29992;&#19981;&#21516;&#36755;&#20986;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#26469;&#22788;&#29702;&#22810;&#20010;&#20219;&#21153;&#12290;&#36890;&#24120;&#65292;MOGPs&#27169;&#22411;&#20551;&#35774;&#36755;&#20986;&#20043;&#38388;&#23384;&#22312;&#24179;&#22374;&#30340;&#30456;&#20851;&#32467;&#26500;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#20844;&#24335;&#24182;&#19981;&#33021;&#32771;&#34385;&#26356;&#22797;&#26434;&#30340;&#20851;&#31995;&#65292;&#20363;&#22914;&#65292;&#22914;&#26524;&#27599;&#20010;&#36755;&#20986;&#37117;&#26377;&#22810;&#20010;&#37325;&#22797;&#35266;&#23519;&#20540;&#65288;&#36825;&#26159;&#29983;&#29289;&#23454;&#39564;&#20013;&#30340;&#20856;&#22411;&#35774;&#32622;&#65289;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;&#23618;&#32423;&#25968;&#25454;&#38598;&#30340;MOGPs&#25193;&#23637;&#65288;&#21363;&#21487;&#20197;&#22312;&#26641;&#29366;&#32467;&#26500;&#20013;&#34920;&#31034;&#35266;&#27979;&#20043;&#38388;&#30340;&#20851;&#31995;&#30340;&#25968;&#25454;&#38598;&#65289;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#23450;&#20041;&#20102;&#19968;&#20010;&#23450;&#21046;&#30340;&#26680;&#20989;&#25968;&#65292;&#32771;&#34385;&#20102;&#25968;&#25454;&#20013;&#30340;&#23618;&#32423;&#32467;&#26500;&#65292;&#20197;&#25429;&#25417;&#19981;&#21516;&#23618;&#27425;&#30340;&#30456;&#20851;&#24615;&#65292;&#21516;&#26102;&#24341;&#20837;&#28508;&#22312;&#21464;&#37327;&#26469;&#36890;&#36807;&#19987;&#29992;&#26680;&#20989;&#25968;&#34920;&#31034;&#36755;&#20986;&#20043;&#38388;&#30340;&#28508;&#22312;&#20381;&#36182;&#20851;&#31995;&#12290;&#39044;&#35745;&#36825;&#20010;&#29305;&#24615;&#33021;&#22815;&#22312;&#20219;&#21153;&#25968;&#37327;&#22686;&#21152;&#26102;&#26174;&#33879;&#25552;&#39640;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-output Gaussian processes (MOGPs) have been introduced to deal with multiple tasks by exploiting the correlations between different outputs. Generally, MOGPs models assume a flat correlation structure between the outputs. However, such a formulation does not account for more elaborate relationships, for instance, if several replicates were observed for each output (which is a typical setting in biological experiments). This paper proposes an extension of MOGPs for hierarchical datasets (i.e. datasets for which the relationships between observations can be represented within a tree structure). Our model defines a tailored kernel function accounting for hierarchical structures in the data to capture different levels of correlations while leveraging the introduction of latent variables to express the underlying dependencies between outputs through a dedicated kernel. This latter feature is expected to significantly improve scalability as the number of tasks increases. An extensive e
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24322;&#27493;&#26102;&#31354;&#22270;&#21367;&#31215;&#32593;&#32476;&#30340;&#19981;&#35268;&#21017;&#20132;&#36890;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#26234;&#33021;&#20132;&#21449;&#21475;&#20135;&#29983;&#30340;&#24322;&#27493;&#31354;&#38388;&#20381;&#36182;&#12289;&#19981;&#35268;&#21017;&#26102;&#38388;&#20381;&#36182;&#21644;&#21487;&#21464;&#38271;&#24230;&#24207;&#21015;&#39044;&#27979;&#31561;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2308.16818</link><description>&lt;p&gt;
&#22522;&#20110;&#24322;&#27493;&#26102;&#31354;&#22270;&#21367;&#31215;&#32593;&#32476;&#30340;&#19981;&#35268;&#21017;&#20132;&#36890;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Irregular Traffic Time Series Forecasting Based on Asynchronous Spatio-Temporal Graph Convolutional Network. (arXiv:2308.16818v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16818
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24322;&#27493;&#26102;&#31354;&#22270;&#21367;&#31215;&#32593;&#32476;&#30340;&#19981;&#35268;&#21017;&#20132;&#36890;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#26234;&#33021;&#20132;&#21449;&#21475;&#20135;&#29983;&#30340;&#24322;&#27493;&#31354;&#38388;&#20381;&#36182;&#12289;&#19981;&#35268;&#21017;&#26102;&#38388;&#20381;&#36182;&#21644;&#21487;&#21464;&#38271;&#24230;&#24207;&#21015;&#39044;&#27979;&#31561;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#39044;&#27979;&#26234;&#33021;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#31995;&#32479;&#20013;&#21463;&#26234;&#33021;&#20132;&#21449;&#21475;&#25511;&#21046;&#30340;&#20132;&#21449;&#21475;&#30340;&#20132;&#36890;&#27969;&#37327;&#23545;&#20110;&#25552;&#21319;&#20132;&#36890;&#20986;&#34892;&#25928;&#29575;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#26234;&#33021;&#20132;&#21449;&#21475;&#20135;&#29983;&#30340;&#20132;&#36890;&#26102;&#38388;&#24207;&#21015;&#19981;&#35268;&#21017;&#65292;&#20132;&#36890;&#27969;&#37327;&#39044;&#27979;&#20219;&#21153;&#21464;&#24471;&#26356;&#21152;&#22256;&#38590;&#65292;&#24182;&#19988;&#38754;&#20020;&#19977;&#20010;&#20027;&#35201;&#25361;&#25112;&#65306;1&#65289;&#24322;&#27493;&#30340;&#31354;&#38388;&#20381;&#36182;&#24615;&#65292;2&#65289;&#20132;&#36890;&#25968;&#25454;&#30340;&#19981;&#35268;&#21017;&#26102;&#38388;&#20381;&#36182;&#24615;&#65292;3) &#38656;&#35201;&#39044;&#27979;&#30340;&#21487;&#21464;&#38271;&#24230;&#24207;&#21015;&#65292;&#20005;&#37325;&#24433;&#21709;&#20102;&#24403;&#21069;&#20132;&#36890;&#27969;&#37327;&#39044;&#27979;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24322;&#27493;&#26102;&#31354;&#22270;&#21367;&#31215;&#32593;&#32476;(ASeer)&#26469;&#39044;&#27979;&#26234;&#33021;&#20132;&#21449;&#21475;&#36827;&#20837;&#36710;&#36947;&#30340;&#20132;&#36890;&#29366;&#24577;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#36890;&#36807;&#22312;&#20132;&#36890;&#25193;&#25955;&#22270;&#19978;&#36830;&#25509;&#36710;&#36947;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#31181;&#24322;&#27493;&#22270;&#25193;&#25955;&#32593;&#32476;&#26469;&#27169;&#25311;&#36710;&#36947;&#30340;&#24322;&#27493;&#31354;&#38388;&#20381;&#36182;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate traffic forecasting at intersections governed by intelligent traffic signals is critical for the advancement of an effective intelligent traffic signal control system. However, due to the irregular traffic time series produced by intelligent intersections, the traffic forecasting task becomes much more intractable and imposes three major new challenges: 1) asynchronous spatial dependency, 2) irregular temporal dependency among traffic data, and 3) variable-length sequence to be predicted, which severely impede the performance of current traffic forecasting methods. To this end, we propose an Asynchronous Spatio-tEmporal graph convolutional nEtwoRk (ASeer) to predict the traffic states of the lanes entering intelligent intersections in a future time window. Specifically, by linking lanes via a traffic diffusion graph, we first propose an Asynchronous Graph Diffusion Network to model the asynchronous spatial dependency between the time-misaligned traffic state measurements of la
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#24179;&#28369;&#36807;&#24230;&#21644;&#29305;&#24449;&#20851;&#32852;&#36807;&#39640;&#29616;&#35937;&#65292;&#21457;&#29616;&#22266;&#23450;&#19981;&#21464;&#30340;&#23376;&#31354;&#38388;&#23548;&#33268;&#20102;&#33410;&#28857;&#34920;&#31034;&#30340;&#31561;&#32423;&#23849;&#22604;&#12290;&#22312;&#35813;&#23376;&#31354;&#38388;&#20013;&#24179;&#28369;&#21521;&#37327;&#30340;&#23384;&#22312;&#23548;&#33268;&#36807;&#24230;&#24179;&#28369;&#65292;&#21363;&#20351;&#36991;&#20813;&#36807;&#24230;&#24179;&#28369;&#20063;&#20250;&#23548;&#33268;&#36807;&#39640;&#30340;&#20851;&#32852;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20811;&#32599;&#20869;&#20811;&#31215;&#20043;&#21644;&#20316;&#20026;&#19968;&#31181;&#26377;&#25928;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.16800</link><description>&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#31561;&#32423;&#23849;&#22604;&#23548;&#33268;&#24179;&#28369;&#36807;&#24230;&#21644;&#20851;&#32852;&#36807;&#39640;
&lt;/p&gt;
&lt;p&gt;
Rank Collapse Causes Over-Smoothing and Over-Correlation in Graph Neural Networks. (arXiv:2308.16800v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16800
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#24179;&#28369;&#36807;&#24230;&#21644;&#29305;&#24449;&#20851;&#32852;&#36807;&#39640;&#29616;&#35937;&#65292;&#21457;&#29616;&#22266;&#23450;&#19981;&#21464;&#30340;&#23376;&#31354;&#38388;&#23548;&#33268;&#20102;&#33410;&#28857;&#34920;&#31034;&#30340;&#31561;&#32423;&#23849;&#22604;&#12290;&#22312;&#35813;&#23376;&#31354;&#38388;&#20013;&#24179;&#28369;&#21521;&#37327;&#30340;&#23384;&#22312;&#23548;&#33268;&#36807;&#24230;&#24179;&#28369;&#65292;&#21363;&#20351;&#36991;&#20813;&#36807;&#24230;&#24179;&#28369;&#20063;&#20250;&#23548;&#33268;&#36807;&#39640;&#30340;&#20851;&#32852;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20811;&#32599;&#20869;&#20811;&#31215;&#20043;&#21644;&#20316;&#20026;&#19968;&#31181;&#26377;&#25928;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#28145;&#24230;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#24179;&#28369;&#36807;&#24230;&#21644;&#29305;&#24449;&#20851;&#32852;&#36807;&#39640;&#30340;&#26032;&#29702;&#35770;&#35265;&#35299;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22266;&#23450;&#19981;&#21464;&#23376;&#31354;&#38388;&#30340;&#26222;&#36941;&#23384;&#22312;&#65292;&#23427;&#34920;&#29616;&#20986;&#19968;&#31181;&#30456;&#23545;&#30340;&#34892;&#20026;&#65292;&#19981;&#21463;&#29305;&#24449;&#36716;&#25442;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#38416;&#26126;&#20102;&#19982;&#25910;&#25947;&#21040;&#24120;&#25968;&#29366;&#24577;&#21644;&#33410;&#28857;&#29366;&#24577;&#30340;&#36807;&#20998;&#20998;&#31163;&#30456;&#20851;&#30340;&#26368;&#26032;&#35266;&#23519;&#32467;&#26524;&#65292;&#22240;&#20026;&#23376;&#31354;&#38388;&#30340;&#25918;&#22823;&#21482;&#21462;&#20915;&#20110;&#32858;&#21512;&#20989;&#25968;&#30340;&#39057;&#35889;&#12290;&#22312;&#32447;&#24615;&#22330;&#26223;&#20013;&#65292;&#36825;&#23548;&#33268;&#33410;&#28857;&#34920;&#31034;&#30001;&#20302;&#32500;&#23376;&#31354;&#38388;&#20027;&#23548;&#65292;&#24182;&#19988;&#20855;&#26377;&#19982;&#29305;&#24449;&#36716;&#25442;&#26080;&#20851;&#30340;&#28176;&#36817;&#25910;&#25947;&#36895;&#29575;&#12290;&#24403;&#24179;&#28369;&#21521;&#37327;&#36328;&#36234;&#36825;&#20010;&#23376;&#31354;&#38388;&#26102;&#65292;&#36825;&#20250;&#23548;&#33268;&#33410;&#28857;&#34920;&#31034;&#30340;&#31561;&#32423;&#23849;&#22604;&#65292;&#20174;&#32780;&#23548;&#33268;&#36807;&#24230;&#24179;&#28369;&#65292;&#21363;&#20351;&#36991;&#20813;&#36807;&#24230;&#24179;&#28369;&#20063;&#20250;&#23548;&#33268;&#36807;&#39640;&#30340;&#20851;&#32852;&#12290;&#22312;&#25105;&#20204;&#30340;&#29702;&#35770;&#25351;&#23548;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20811;&#32599;&#20869;&#20811;&#31215;&#20043;&#21644;&#20316;&#20026;&#19968;&#31181;&#26377;&#30410;&#29305;&#24615;&#65292;&#21487;&#20197;&#21487;&#38752;&#22320;&#38450;&#27490;&#36807;&#24230;&#24179;&#28369;&#12289;&#36807;&#39640;&#20851;&#32852;&#21644;&#31561;&#32423;&#23849;&#22604;&#12290;
&lt;/p&gt;
&lt;p&gt;
Our study reveals new theoretical insights into over-smoothing and feature over-correlation in deep graph neural networks. We show the prevalence of invariant subspaces, demonstrating a fixed relative behavior that is unaffected by feature transformations. Our work clarifies recent observations related to convergence to a constant state and a potential over-separation of node states, as the amplification of subspaces only depends on the spectrum of the aggregation function. In linear scenarios, this leads to node representations being dominated by a low-dimensional subspace with an asymptotic convergence rate independent of the feature transformations. This causes a rank collapse of the node representations, resulting in over-smoothing when smooth vectors span this subspace, and over-correlation even when over-smoothing is avoided. Guided by our theory, we propose a sum of Kronecker products as a beneficial property that can provably prevent over-smoothing, over-correlation, and rank c
&lt;/p&gt;</description></item><item><title>&#20195;&#29702;&#22242;&#38431;&#24773;&#26223;&#24863;&#30693;&#65288;ATSA&#65289;&#26159;&#19968;&#20010;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#21644;&#20154;&#31867;&#32852;&#21512;&#24037;&#20316;&#30340;&#24773;&#26223;&#24863;&#30693;&#26694;&#26550;&#65292;&#26088;&#22312;&#25552;&#39640;&#28151;&#21512;&#22242;&#38431;&#30340;&#32489;&#25928;&#12290;&#35813;&#26694;&#26550;&#32467;&#21512;&#20102;&#20010;&#20307;&#21644;&#22242;&#38431;&#30340;&#24773;&#26223;&#24863;&#30693;&#27169;&#22411;&#65292;&#24182;&#28041;&#21450;&#21452;&#21521;&#21644;&#21160;&#24577;&#30340;&#20132;&#20114;&#12290;</title><link>http://arxiv.org/abs/2308.16785</link><description>&lt;p&gt;
&#20195;&#29702;&#22242;&#38431;&#24773;&#26223;&#24863;&#30693;&#65288;ATSA&#65289;&#65306;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#32852;&#21512;&#24037;&#20316;&#30340;&#24773;&#26223;&#24863;&#30693;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Agent Teaming Situation Awareness (ATSA): A Situation Awareness Framework for Human-AI Teaming. (arXiv:2308.16785v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16785
&lt;/p&gt;
&lt;p&gt;
&#20195;&#29702;&#22242;&#38431;&#24773;&#26223;&#24863;&#30693;&#65288;ATSA&#65289;&#26159;&#19968;&#20010;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#21644;&#20154;&#31867;&#32852;&#21512;&#24037;&#20316;&#30340;&#24773;&#26223;&#24863;&#30693;&#26694;&#26550;&#65292;&#26088;&#22312;&#25552;&#39640;&#28151;&#21512;&#22242;&#38431;&#30340;&#32489;&#25928;&#12290;&#35813;&#26694;&#26550;&#32467;&#21512;&#20102;&#20010;&#20307;&#21644;&#22242;&#38431;&#30340;&#24773;&#26223;&#24863;&#30693;&#27169;&#22411;&#65292;&#24182;&#28041;&#21450;&#21452;&#21521;&#21644;&#21160;&#24577;&#30340;&#20132;&#20114;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#30340;&#24555;&#36895;&#21457;&#23637;&#23548;&#33268;&#20102;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#32852;&#21512;&#24037;&#20316;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#26085;&#30410;&#22686;&#38271;&#12290;&#38543;&#30528;&#26426;&#22120;&#20174;&#20165;&#20165;&#33258;&#21160;&#21270;&#21040;&#33258;&#20027;&#29366;&#24577;&#30340;&#28436;&#36827;&#65292;&#23427;&#20204;&#36234;&#26469;&#36234;&#23637;&#29616;&#20986;&#24847;&#22806;&#30340;&#34892;&#20026;&#21644;&#31867;&#20284;&#20154;&#31867;&#30340;&#35748;&#30693;/&#26234;&#33021;&#33021;&#21147;&#65292;&#21253;&#25324;&#24773;&#26223;&#24863;&#30693;&#12290;&#36825;&#31181;&#36716;&#21464;&#26377;&#28508;&#21147;&#25552;&#39640;&#28151;&#21512;&#20154;&#24037;&#26234;&#33021;&#22242;&#38431;&#30340;&#32489;&#25928;&#65292;&#24378;&#35843;&#20102;&#25105;&#20204;&#23545;&#20154;&#19982;&#26426;&#22120;&#20043;&#38388;&#21160;&#24577;&#24773;&#26223;&#24863;&#30693;&#30456;&#20114;&#20316;&#29992;&#30340;&#26356;&#22909;&#29702;&#35299;&#30340;&#38656;&#35201;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23545;&#20027;&#27969;&#24773;&#26223;&#24863;&#30693;&#29702;&#35770;&#27169;&#22411;&#36827;&#34892;&#20102;&#22238;&#39038;&#65292;&#24182;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#32852;&#21512;&#24037;&#20316;&#30340;&#20851;&#38190;&#29305;&#24449;&#21644;&#36807;&#31243;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#24773;&#26223;&#24863;&#30693;&#26694;&#26550;&#12290;&#20195;&#29702;&#22242;&#38431;&#24773;&#26223;&#24863;&#30693;&#65288;ATSA&#65289;&#26694;&#26550;&#32479;&#19968;&#20102;&#20154;&#31867;&#21644;&#20154;&#24037;&#26234;&#33021;&#30340;&#34892;&#20026;&#65292;&#24182;&#28041;&#21450;&#21452;&#21521;&#21644;&#21160;&#24577;&#30340;&#20132;&#20114;&#12290;&#35813;&#26694;&#26550;&#22522;&#20110;&#20010;&#20307;&#21644;&#22242;&#38431;&#30340;&#24773;&#26223;&#24863;&#30693;&#27169;&#22411;&#65292;&#24182;&#38416;&#36848;&#20102;&#20026;&#24314;&#27169;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#32852;&#21512;&#24037;&#20316;&#32780;&#32454;&#21270;&#30340;&#35748;&#30693;&#26426;&#21046;&#12290;&#31867;&#20284;&#30340;&#30693;&#35273;&#24490;&#29615;
&lt;/p&gt;
&lt;p&gt;
The rapid advancements in artificial intelligence (AI) have led to a growing trend of human-AI teaming (HAT) in various fields. As machines continue to evolve from mere automation to a state of autonomy, they are increasingly exhibiting unexpected behaviors and human-like cognitive/intelligent capabilities, including situation awareness (SA). This shift has the potential to enhance the performance of mixed human-AI teams over all-human teams, underscoring the need for a better understanding of the dynamic SA interactions between humans and machines. To this end, we provide a review of leading SA theoretical models and a new framework for SA in the HAT context based on the key features and processes of HAT. The Agent Teaming Situation Awareness (ATSA) framework unifies human and AI behavior, and involves bidirectional, and dynamic interaction. The framework is based on the individual and team SA models and elaborates on the cognitive mechanisms for modeling HAT. Similar perceptual cycle
&lt;/p&gt;</description></item><item><title>StratMed&#26159;&#19968;&#31181;&#38754;&#21521;&#20302;&#36164;&#28304;&#33647;&#29289;&#25512;&#33616;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#30456;&#20851;&#24615;&#20998;&#23618;&#26426;&#21046;&#26469;&#35299;&#20915;&#21307;&#30103;&#25968;&#25454;&#38271;&#23614;&#20998;&#24067;&#19981;&#24179;&#34913;&#30340;&#38382;&#39064;&#65292;&#24179;&#34913;&#20102;&#33647;&#29289;&#32452;&#21512;&#30340;&#23433;&#20840;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.16781</link><description>&lt;p&gt;
StratMed&#65306;&#38754;&#21521;&#20302;&#36164;&#28304;&#33647;&#29289;&#25512;&#33616;&#30340;&#30456;&#20851;&#24615;&#20998;&#23618;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
StratMed: Relevance Stratification for Low-resource Medication Recommendation. (arXiv:2308.16781v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16781
&lt;/p&gt;
&lt;p&gt;
StratMed&#26159;&#19968;&#31181;&#38754;&#21521;&#20302;&#36164;&#28304;&#33647;&#29289;&#25512;&#33616;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#30456;&#20851;&#24615;&#20998;&#23618;&#26426;&#21046;&#26469;&#35299;&#20915;&#21307;&#30103;&#25968;&#25454;&#38271;&#23614;&#20998;&#24067;&#19981;&#24179;&#34913;&#30340;&#38382;&#39064;&#65292;&#24179;&#34913;&#20102;&#33647;&#29289;&#32452;&#21512;&#30340;&#23433;&#20840;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26377;&#38480;&#21307;&#30103;&#36164;&#28304;&#19982;&#26085;&#30410;&#22686;&#38271;&#30340;&#38656;&#27714;&#20043;&#38388;&#30340;&#22833;&#34913;&#65292;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#20020;&#24202;&#20219;&#21153;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#20316;&#20026;&#19968;&#20010;&#23376;&#39046;&#22495;&#65292;&#33647;&#29289;&#25512;&#33616;&#26088;&#22312;&#23558;&#24739;&#32773;&#30340;&#32437;&#21521;&#21382;&#21490;&#19982;&#21307;&#23398;&#30693;&#35782;&#30456;&#32467;&#21512;&#65292;&#24110;&#21161;&#21307;&#29983;&#26356;&#23433;&#20840;&#12289;&#26356;&#20934;&#30830;&#22320;&#24320;&#20855;&#33647;&#29289;&#32452;&#21512;&#22788;&#26041;&#12290;&#29616;&#26377;&#26041;&#27861;&#24573;&#35270;&#20102;&#21307;&#30103;&#25968;&#25454;&#20013;&#22266;&#26377;&#30340;&#38271;&#23614;&#20998;&#24067;&#65292;&#32570;&#20047;&#22836;&#23614;&#25968;&#25454;&#20043;&#38388;&#30340;&#24179;&#34913;&#34920;&#31034;&#65292;&#23548;&#33268;&#27169;&#22411;&#24615;&#33021;&#27425;&#20248;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;StratMed&#65292;&#36825;&#26159;&#19968;&#20010;&#32467;&#21512;&#20102;&#21019;&#26032;&#30340;&#30456;&#20851;&#24615;&#20998;&#23618;&#26426;&#21046;&#30340;&#27169;&#22411;&#12290;&#23427;&#36890;&#36807;&#21327;&#35843;&#25968;&#25454;&#38271;&#23614;&#20998;&#24067;&#20013;&#30340;&#24046;&#24322;&#65292;&#24182;&#22312;&#33647;&#29289;&#32452;&#21512;&#30340;&#23433;&#20840;&#24615;&#21644;&#20934;&#30830;&#24615;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#26500;&#24314;&#39044;&#35757;&#32451;&#26041;&#27861;&#26469;&#33719;&#21462;&#23454;&#20307;&#34920;&#31034;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#31867;&#20284;&#37329;&#23383;&#22612;&#30340;&#25968;&#25454;&#20998;&#23618;&#26041;&#27861;&#65292;&#20197;&#33719;&#24471;&#26356;&#36890;&#29992;&#30340;&#23454;&#20307;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the growing imbalance between limited medical resources and escalating demands, AI-based clinical tasks have become paramount. Medication recommendation, as a sub-domain, aims to amalgamate longitudinal patient history with medical knowledge, assisting physicians in prescribing safer and more accurate medication combinations. Existing methods overlook the inherent long-tail distribution in medical data, lacking balanced representation between head and tail data, which leads to sub-optimal model performance. To address this challenge, we introduce StratMed, a model that incorporates an innovative relevance stratification mechanism. It harmonizes discrepancies in data long-tail distribution and strikes a balance between the safety and accuracy of medication combinations. Specifically, we first construct a pre-training method using deep learning networks to obtain entity representation. After that, we design a pyramid-like data stratification method to obtain more generalized entity 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#38646;&#26679;&#26412;&#26550;&#26500;&#25628;&#32034;&#65292;&#36890;&#36807;&#20351;&#29992;&#21487;&#23398;&#20064;&#30340;&#20613;&#37324;&#21494;&#27491;&#24358;&#21644;&#27714;&#21644;&#32534;&#30721;&#26469;&#26500;&#24314;&#35745;&#31639;&#30340;&#21069;&#39304;&#22270;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#22522;&#20110;&#39044;&#27979;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#20013;&#24615;&#33021;&#25351;&#26631;&#27867;&#21270;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2308.16775</link><description>&lt;p&gt;
&#22522;&#20110;&#31070;&#32463;&#39044;&#27979;&#30340;&#38646;&#26679;&#26412;NAS&#33539;&#24335;&#30340;&#26377;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;
Efficacy of Neural Prediction-Based NAS for Zero-Shot NAS Paradigm. (arXiv:2308.16775v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16775
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#38646;&#26679;&#26412;&#26550;&#26500;&#25628;&#32034;&#65292;&#36890;&#36807;&#20351;&#29992;&#21487;&#23398;&#20064;&#30340;&#20613;&#37324;&#21494;&#27491;&#24358;&#21644;&#27714;&#21644;&#32534;&#30721;&#26469;&#26500;&#24314;&#35745;&#31639;&#30340;&#21069;&#39304;&#22270;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#22522;&#20110;&#39044;&#27979;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#20013;&#24615;&#33021;&#25351;&#26631;&#27867;&#21270;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22522;&#20110;&#39044;&#27979;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65288;NAS&#65289;&#20013;&#65292;&#36890;&#36807;&#22270;&#21367;&#31215;&#32593;&#32476;&#24471;&#21040;&#30340;&#24615;&#33021;&#25351;&#26631;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;one-hot&#32534;&#30721;&#23558;&#21069;&#39304;&#32467;&#26500;&#34920;&#31034;&#20026;&#32452;&#20214;&#22270;&#30340;&#36825;&#20123;&#25351;&#26631;&#38754;&#20020;&#19968;&#20010;&#38480;&#21046;&#65306;&#26080;&#27861;&#22312;&#19981;&#21516;&#30340;&#25628;&#32034;&#31354;&#38388;&#20013;&#35780;&#20272;&#26550;&#26500;&#30340;&#24615;&#33021;&#12290;&#30456;&#21453;&#65292;&#25163;&#24037;&#24615;&#33021;&#25351;&#26631;&#65288;&#38646;&#26679;&#26412;NAS&#65289;&#21487;&#20197;&#22312;&#22810;&#20010;&#25628;&#32034;&#31354;&#38388;&#20013;&#27867;&#21270;&#65292;&#22240;&#20026;&#23427;&#20204;&#20351;&#29992;&#30456;&#21516;&#30340;&#26550;&#26500;&#21644;&#38543;&#26426;&#21021;&#22987;&#21270;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#38646;&#26679;&#26412;NAS&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#37319;&#29992;&#20613;&#37324;&#21494;&#27491;&#24358;&#21644;&#27714;&#21644;&#32534;&#30721;&#26469;&#36827;&#34892;&#21367;&#31215;&#26680;&#30340;&#32534;&#30721;&#65292;&#20174;&#32780;&#26500;&#24314;&#20102;&#19968;&#20010;&#35745;&#31639;&#30340;&#21069;&#39304;&#22270;&#65292;&#20854;&#32467;&#26500;&#31867;&#20284;&#20110;&#27491;&#22312;&#35780;&#20272;&#30340;&#26550;&#26500;&#12290;&#36825;&#20123;&#32534;&#30721;&#26159;&#21487;&#23398;&#20064;&#30340;&#65292;&#24182;&#25552;&#20379;&#20102;&#26550;&#26500;&#25299;&#25169;&#20449;&#24687;&#30340;&#20840;&#38754;&#35270;&#22270;&#12290;&#28982;&#21518;&#65292;&#20276;&#38543;&#30340;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#23545;&#26550;&#26500;&#36827;&#34892;&#25490;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;
In prediction-based Neural Architecture Search (NAS), performance indicators derived from graph convolutional networks have shown significant success. These indicators, achieved by representing feed-forward structures as component graphs through one-hot encoding, face a limitation: their inability to evaluate architecture performance across varying search spaces. In contrast, handcrafted performance indicators (zero-shot NAS), which use the same architecture with random initialization, can generalize across multiple search spaces. Addressing this limitation, we propose a novel approach for zero-shot NAS using deep learning. Our method employs Fourier sum of sines encoding for convolutional kernels, enabling the construction of a computational feed-forward graph with a structure similar to the architecture under evaluation. These encodings are learnable and offer a comprehensive view of the architecture's topological information. An accompanying multi-layer perceptron (MLP) then ranks t
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;&#24037;&#19994;&#25511;&#21046;&#31995;&#32479;&#32593;&#32476;&#23433;&#20840;&#39046;&#22495;&#36827;&#34892;&#20302;&#38376;&#27099;&#30740;&#31350;&#21644;&#25945;&#32946;&#30340;&#37325;&#35201;&#24615;&#12290;&#36890;&#36807;&#21033;&#29992;&#24320;&#21457;&#30340;&#39640;&#20445;&#30495;&#27169;&#25311;&#22120;&#65292;&#20316;&#32773;&#26500;&#24314;&#20102;&#19968;&#20010;&#38598;&#25104;&#26694;&#26550;&#29992;&#20110;&#33258;&#21160;&#21457;&#21160;&#32593;&#32476;&#25915;&#20987;&#12289;&#25910;&#38598;&#25968;&#25454;&#12289;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#24182;&#38024;&#23545;&#23454;&#38469;&#30340;&#21270;&#23398;&#21644;&#21046;&#36896;&#36807;&#31243;&#36827;&#34892;&#35780;&#20272;&#12290;&#20316;&#32773;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MinTWin SVM&#30340;&#20837;&#20405;&#26816;&#27979;&#27169;&#22411;&#65292;&#23427;&#32467;&#21512;&#20102;&#26080;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#21644;&#28369;&#21160;&#31383;&#21475;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.16769</link><description>&lt;p&gt;
&#26397;&#30528;&#24037;&#19994;&#25511;&#21046;&#31995;&#32479;&#20302;&#38376;&#27099;&#30340;&#32593;&#32476;&#23433;&#20840;&#30740;&#31350;&#21644;&#25945;&#32946;&#21457;&#23637;
&lt;/p&gt;
&lt;p&gt;
Towards Low-Barrier Cybersecurity Research and Education for Industrial Control Systems. (arXiv:2308.16769v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16769
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;&#24037;&#19994;&#25511;&#21046;&#31995;&#32479;&#32593;&#32476;&#23433;&#20840;&#39046;&#22495;&#36827;&#34892;&#20302;&#38376;&#27099;&#30740;&#31350;&#21644;&#25945;&#32946;&#30340;&#37325;&#35201;&#24615;&#12290;&#36890;&#36807;&#21033;&#29992;&#24320;&#21457;&#30340;&#39640;&#20445;&#30495;&#27169;&#25311;&#22120;&#65292;&#20316;&#32773;&#26500;&#24314;&#20102;&#19968;&#20010;&#38598;&#25104;&#26694;&#26550;&#29992;&#20110;&#33258;&#21160;&#21457;&#21160;&#32593;&#32476;&#25915;&#20987;&#12289;&#25910;&#38598;&#25968;&#25454;&#12289;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#24182;&#38024;&#23545;&#23454;&#38469;&#30340;&#21270;&#23398;&#21644;&#21046;&#36896;&#36807;&#31243;&#36827;&#34892;&#35780;&#20272;&#12290;&#20316;&#32773;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MinTWin SVM&#30340;&#20837;&#20405;&#26816;&#27979;&#27169;&#22411;&#65292;&#23427;&#32467;&#21512;&#20102;&#26080;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#21644;&#28369;&#21160;&#31383;&#21475;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20445;&#25252;&#29992;&#20110;&#20844;&#20849;&#20851;&#38190;&#22522;&#30784;&#35774;&#26045;&#30340;&#24037;&#19994;&#25511;&#21046;&#31995;&#32479;(ICS)&#23545;&#20110;&#38450;&#27490;&#32593;&#32476;&#25915;&#20987;&#36896;&#25104;&#28798;&#38590;&#24615;&#30340;&#29289;&#29702;&#25439;&#23475;&#33267;&#20851;&#37325;&#35201;&#12290;&#30740;&#31350;&#30028;&#38656;&#35201;&#27979;&#35797;&#24179;&#21488;&#26469;&#39564;&#35777;&#21644;&#27604;&#36739;&#21508;&#31181;&#20837;&#20405;&#26816;&#27979;&#31639;&#27861;&#20197;&#20445;&#25252;ICS&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#26114;&#36149;&#30340;&#30828;&#20214;&#12289;&#36719;&#20214;&#21644;&#25805;&#32437;&#29616;&#23454;&#31995;&#32479;&#30340;&#22266;&#26377;&#21361;&#38505;&#24615;&#65292;ICS&#32593;&#32476;&#23433;&#20840;&#39046;&#22495;&#30340;&#30740;&#31350;&#21644;&#25945;&#32946;&#23384;&#22312;&#39640;&#38376;&#27099;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#22312;&#26368;&#36817;&#24320;&#21457;&#30340;&#19977;&#32500;&#39640;&#20445;&#30495;&#27169;&#25311;&#22120;&#22522;&#30784;&#19978;&#65292;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#38598;&#25104;&#26694;&#26550;&#65292;&#21487;&#20197;&#33258;&#21160;&#21457;&#21160;&#32593;&#32476;&#25915;&#20987;&#12289;&#25910;&#38598;&#25968;&#25454;&#12289;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#24182;&#38024;&#23545;&#23454;&#38469;&#30340;&#21270;&#23398;&#21644;&#21046;&#36896;&#36807;&#31243;&#36827;&#34892;&#35780;&#20272;&#12290;&#22312;&#25105;&#20204;&#30340;&#27979;&#35797;&#24179;&#21488;&#19978;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#20837;&#20405;&#26816;&#27979;&#27169;&#22411;&#65292;&#31216;&#20026;Minimal Threshold and Window SVM (MinTWin SVM)&#65292;&#23427;&#32467;&#21512;&#20102;&#19968;&#31867;SVM&#30340;&#26080;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#21644;&#28369;&#21160;&#31383;&#21475;
&lt;/p&gt;
&lt;p&gt;
The protection of Industrial Control Systems (ICS) that are employed in public critical infrastructures is of utmost importance due to catastrophic physical damages cyberattacks may cause. The research community requires testbeds for validation and comparing various intrusion detection algorithms to protect ICS. However, there exist high barriers to entry for research and education in the ICS cybersecurity domain due to expensive hardware, software, and inherent dangers of manipulating real-world systems. To close the gap, built upon recently developed 3D high-fidelity simulators, we further showcase our integrated framework to automatically launch cyberattacks, collect data, train machine learning models, and evaluate for practical chemical and manufacturing processes. On our testbed, we validate our proposed intrusion detection model called Minimal Threshold and Window SVM (MinTWin SVM) that utilizes unsupervised machine learning via a one-class SVM in combination with a sliding wind
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;Ladder-of-Thought&#8221;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#22806;&#37096;&#30693;&#35782;&#26469;&#25552;&#21319;&#31435;&#22330;&#26816;&#27979;&#20219;&#21153;&#20013;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#35299;&#20915;&#20102;&#23567;&#22411;&#27169;&#22411;&#22312;&#24212;&#29992;&#20808;&#21069;&#20869;&#37096;&#30693;&#35782;&#26102;&#24615;&#33021;&#25552;&#21319;&#19981;&#26126;&#26174;&#30340;&#38382;&#39064;&#65292;&#20197;&#21450;&#22823;&#35268;&#27169;&#27169;&#22411;&#22312;&#25928;&#29575;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2308.16763</link><description>&lt;p&gt;
Ladder-of-Thought: &#20351;&#29992;&#30693;&#35782;&#20316;&#20026;&#38454;&#26799;&#25552;&#21319;&#31435;&#22330;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Ladder-of-Thought: Using Knowledge as Steps to Elevate Stance Detection. (arXiv:2308.16763v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16763
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;Ladder-of-Thought&#8221;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#22806;&#37096;&#30693;&#35782;&#26469;&#25552;&#21319;&#31435;&#22330;&#26816;&#27979;&#20219;&#21153;&#20013;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#35299;&#20915;&#20102;&#23567;&#22411;&#27169;&#22411;&#22312;&#24212;&#29992;&#20808;&#21069;&#20869;&#37096;&#30693;&#35782;&#26102;&#24615;&#33021;&#25552;&#21319;&#19981;&#26126;&#26174;&#30340;&#38382;&#39064;&#65292;&#20197;&#21450;&#22823;&#35268;&#27169;&#27169;&#22411;&#22312;&#25928;&#29575;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24605;&#32500;&#38142;&#24335;&#25552;&#20379;&#65288;CoT&#65289;&#36890;&#36807;&#29983;&#25104;&#20013;&#38388;&#30340;&#25512;&#29702;&#26469;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#22686;&#24378;&#20027;&#35201;&#26377;&#30410;&#20110;&#22823;&#35268;&#27169;&#27169;&#22411;&#65292;&#22312;&#30452;&#25509;&#24212;&#29992;CoT&#26102;&#23567;&#22411;LLM&#30340;&#24615;&#33021;&#25913;&#36827;&#19981;&#26126;&#26174;&#12290;&#23613;&#31649;LLM&#20855;&#26377;&#20808;&#36827;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;CoT&#20027;&#35201;&#20381;&#36182;&#20110;&#20854;&#39044;&#20808;&#35757;&#32451;&#30340;&#20869;&#37096;&#30693;&#35782;&#65292;&#20808;&#21069;&#26410;&#30693;&#20110;&#27169;&#22411;&#30340;&#22806;&#37096;&#30693;&#35782;&#26410;&#34987;&#20805;&#20998;&#21033;&#29992;&#12290;&#22312;&#31435;&#22330;&#26816;&#27979;&#31561;&#20219;&#21153;&#20013;&#65292;&#22806;&#37096;&#32972;&#26223;&#30693;&#35782;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#36825;&#31181;&#36951;&#28431;&#21464;&#24471;&#26356;&#21152;&#26126;&#26174;&#12290;&#27492;&#22806;&#65292;LLM&#30340;&#22823;&#35268;&#27169;&#26550;&#26500;&#22312;&#37096;&#32626;&#36807;&#31243;&#20013;&#19981;&#21487;&#36991;&#20813;&#22320;&#23384;&#22312;&#25928;&#29575;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#29992;&#20110;&#31435;&#22330;&#26816;&#27979;&#30340;&#24605;&#32500;&#38454;&#26799;&#65288;LoT&#65289;&#12290;LoT&#22522;&#20110;&#21452;&#38454;&#27573;&#32423;&#32852;&#20248;&#21270;&#26694;&#26550;&#65292;&#25351;&#23548;&#27169;&#22411;&#25972;&#21512;&#39640;&#36136;&#37327;&#30340;&#22806;&#37096;&#30693;&#35782;&#65292;&#22686;&#24378;&#20013;&#38388;&#27493;&#39588;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Chain-of-Thought Prompting (CoT) reinforces the reasoning capabilities of Large Language Models (LLMs) through the generation of intermediate rationales. However, these enhancements predominantly benefit large-scale models, leaving small LMs without significant performance improvements when directly applying CoT. Despite the advanced reasoning capabilities of LLMs, CoT relies primarily on their pre-trained internal knowledge. The external knowledge that is previously unknown to the model remains unexploited. This omission becomes pronounced in tasks such as stance detection, where the external background knowledge plays a pivotal role. Additionally, the large-scale architecture of LLMs inevitably present efficiency challenges during deployment. To address these challenges, we introduce the Ladder-of-Thought (LoT) for stance detection. Grounded in a dual-phase Cascaded Optimization framework, LoT directs the model to incorporate high-quality external knowledge, enhancing the intermediat
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#30740;&#31350;&#20102;&#20351;&#29992;&#22522;&#20110;LLM&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;&#26597;&#35810;&#37325;&#20889;&#26041;&#27861;&#26469;&#25552;&#39640;&#25991;&#26412;&#25490;&#21517;&#20219;&#21153;&#12290;&#36890;&#36807;&#36890;&#36807;&#19978;&#19979;&#25991;&#24863;&#30693;&#25552;&#31034;&#26469;&#37325;&#20889;&#27169;&#31946;&#30340;&#35757;&#32451;&#26597;&#35810;&#65292;&#20811;&#26381;&#20102;&#27010;&#24565;&#28418;&#31227;&#21644;&#25512;&#29702;&#24320;&#38144;&#30340;&#22266;&#26377;&#23616;&#38480;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.16753</link><description>&lt;p&gt;
&#22522;&#20110;LLM&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;&#26597;&#35810;&#37325;&#20889;&#26041;&#27861;&#29992;&#20110;&#25991;&#26412;&#25490;&#21517;
&lt;/p&gt;
&lt;p&gt;
Context Aware Query Rewriting for Text Rankers using LLM. (arXiv:2308.16753v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16753
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#30740;&#31350;&#20102;&#20351;&#29992;&#22522;&#20110;LLM&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;&#26597;&#35810;&#37325;&#20889;&#26041;&#27861;&#26469;&#25552;&#39640;&#25991;&#26412;&#25490;&#21517;&#20219;&#21153;&#12290;&#36890;&#36807;&#36890;&#36807;&#19978;&#19979;&#25991;&#24863;&#30693;&#25552;&#31034;&#26469;&#37325;&#20889;&#27169;&#31946;&#30340;&#35757;&#32451;&#26597;&#35810;&#65292;&#20811;&#26381;&#20102;&#27010;&#24565;&#28418;&#31227;&#21644;&#25512;&#29702;&#24320;&#38144;&#30340;&#22266;&#26377;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26597;&#35810;&#37325;&#20889;&#26159;&#19968;&#31867;&#24212;&#29992;&#20110;&#19981;&#23436;&#20840;&#25351;&#23450;&#21644;&#27169;&#31946;&#26597;&#35810;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#20811;&#26381;&#25991;&#26723;&#25490;&#21517;&#20013;&#30340;&#35789;&#27719;&#19981;&#21305;&#37197;&#38382;&#39064;&#12290;&#26597;&#35810;&#36890;&#24120;&#22312;&#26597;&#35810;&#22788;&#29702;&#36807;&#31243;&#20013;&#36827;&#34892;&#37325;&#20889;&#65292;&#20197;&#20415;&#20026;&#19979;&#28216;&#25490;&#21517;&#22120;&#25552;&#20379;&#26356;&#22909;&#30340;&#26597;&#35810;&#24314;&#27169;&#12290;&#38543;&#30528;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#65292;&#24050;&#32463;&#24320;&#22987;&#30740;&#31350;&#20351;&#29992;&#29983;&#25104;&#26041;&#27861;&#29983;&#25104;&#20266;&#25991;&#26723;&#26469;&#35299;&#20915;&#36825;&#31181;&#22266;&#26377;&#30340;&#35789;&#27719;&#24046;&#36317;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;LLMs&#22312;&#25552;&#39640;&#25991;&#26412;&#25490;&#21517;&#20219;&#21153;&#20013;&#26597;&#35810;&#37325;&#20889;&#30340;&#25928;&#29992;&#12290;&#25105;&#20204;&#21457;&#29616;&#20351;&#29992;LLMs&#20316;&#20026;&#26597;&#35810;&#37325;&#20889;&#22120;&#23384;&#22312;&#20004;&#20010;&#22266;&#26377;&#23616;&#38480;&#24615;--&#22312;&#20165;&#20351;&#29992;&#26597;&#35810;&#20316;&#20026;&#25552;&#31034;&#26102;&#23384;&#22312;&#27010;&#24565;&#28418;&#31227;&#65292;&#24182;&#19988;&#22312;&#26597;&#35810;&#22788;&#29702;&#36807;&#31243;&#20013;&#23384;&#22312;&#22823;&#37327;&#30340;&#25512;&#29702;&#24320;&#38144;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#25928;&#26524;&#24778;&#20154;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#19978;&#19979;&#25991;&#24863;&#30693;&#26597;&#35810;&#37325;&#20889;&#65288;CAR&#65289;&#65292;&#20197;&#21033;&#29992;LLMs&#30340;&#20248;&#21183;&#36827;&#34892;&#26597;&#35810;&#29702;&#35299;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#36890;&#36807;&#19978;&#19979;&#25991;&#24863;&#30693;&#25552;&#31034;&#26469;&#37325;&#20889;&#27169;&#31946;&#30340;&#35757;&#32451;&#26597;&#35810;&#65292;&#20197;&#22312;&#26597;&#35810;&#29702;&#35299;&#26041;&#38754;&#33719;&#24471;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Query rewriting refers to an established family of approaches that are applied to underspecified and ambiguous queries to overcome the vocabulary mismatch problem in document ranking. Queries are typically rewritten during query processing time for better query modelling for the downstream ranker. With the advent of large-language models (LLMs), there have been initial investigations into using generative approaches to generate pseudo documents to tackle this inherent vocabulary gap. In this work, we analyze the utility of LLMs for improved query rewriting for text ranking tasks. We find that there are two inherent limitations of using LLMs as query re-writers -- concept drift when using only queries as prompts and large inference costs during query processing. We adopt a simple, yet surprisingly effective, approach called context aware query rewriting (CAR) to leverage the benefits of LLMs for query understanding. Firstly, we rewrite ambiguous training queries by context-aware prompti
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;Socratis&#65292;&#19968;&#20010;&#26032;&#30340;&#31038;&#20250;&#21453;&#24212;&#22522;&#20934;&#65292;&#29992;&#20110;&#23398;&#20064;&#22810;&#27169;&#24577;&#20869;&#23481;&#30340;&#22810;&#26679;&#21270;&#24773;&#32490;&#21453;&#24212;&#12290;&#26681;&#25454;&#20154;&#31867;&#30740;&#31350;&#32467;&#26524;&#65292;&#20154;&#20204;&#26356;&#21916;&#27426;&#20154;&#24037;&#25776;&#20889;&#30340;&#24773;&#24863;&#21407;&#22240;&#65292;&#27604;&#26426;&#22120;&#29983;&#25104;&#30340;&#35201;&#22810;2&#20493;&#20197;&#19978;&#12290;</title><link>http://arxiv.org/abs/2308.16741</link><description>&lt;p&gt;
Socratis&#65306;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#26159;&#21542;&#20855;&#26377;&#24773;&#32490;&#24847;&#35782;&#65311;
&lt;/p&gt;
&lt;p&gt;
Socratis: Are large multimodal models emotionally aware?. (arXiv:2308.16741v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16741
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;Socratis&#65292;&#19968;&#20010;&#26032;&#30340;&#31038;&#20250;&#21453;&#24212;&#22522;&#20934;&#65292;&#29992;&#20110;&#23398;&#20064;&#22810;&#27169;&#24577;&#20869;&#23481;&#30340;&#22810;&#26679;&#21270;&#24773;&#32490;&#21453;&#24212;&#12290;&#26681;&#25454;&#20154;&#31867;&#30740;&#31350;&#32467;&#26524;&#65292;&#20154;&#20204;&#26356;&#21916;&#27426;&#20154;&#24037;&#25776;&#20889;&#30340;&#24773;&#24863;&#21407;&#22240;&#65292;&#27604;&#26426;&#22120;&#29983;&#25104;&#30340;&#35201;&#22810;2&#20493;&#20197;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#24773;&#32490;&#39044;&#27979;&#22522;&#20934;&#21253;&#21547;&#31895;&#31961;&#30340;&#24773;&#32490;&#26631;&#31614;&#65292;&#19981;&#32771;&#34385;&#22270;&#20687;&#21644;&#25991;&#26412;&#22312;&#20154;&#31867;&#20013;&#24341;&#21457;&#22810;&#26679;&#21270;&#24773;&#32490;&#30340;&#21508;&#31181;&#21407;&#22240;&#12290;&#23398;&#20064;&#22810;&#26679;&#21270;&#30340;&#23545;&#20110;&#22810;&#27169;&#24577;&#20869;&#23481;&#30340;&#21453;&#24212;&#38750;&#24120;&#37325;&#35201;&#65292;&#22240;&#20026;&#26234;&#33021;&#26426;&#22120;&#22312;&#29983;&#25104;&#21644;&#20256;&#36882;&#20869;&#23481;&#32473;&#31038;&#20250;&#20013;&#36215;&#21040;&#26680;&#24515;&#20316;&#29992;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Socratis&#65292;&#19968;&#20010;&#31038;&#20250;&#21453;&#24212;&#22522;&#20934;&#65292;&#22312;&#20854;&#20013;&#27599;&#20010;&#22270;&#20687;-&#26631;&#39064;&#65288;IC&#65289;&#23545;&#37117;&#38468;&#24102;&#26377;&#22810;&#31181;&#24773;&#32490;&#21644;&#24863;&#21463;&#23427;&#20204;&#30340;&#21407;&#22240;&#30340;&#27880;&#37322;&#12290;Socratis&#21253;&#21547;&#20102;&#26469;&#33258;5&#20010;&#24191;&#27867;&#38405;&#35835;&#30340;&#26032;&#38395;&#21644;&#22270;&#20687;&#26631;&#39064;&#65288;IC&#65289;&#25968;&#25454;&#38598;&#30340;2075&#20010;&#22270;&#20687;-&#26631;&#39064;&#23545;&#30340;980&#20010;&#24773;&#32490;&#30340;18K&#20010;&#33258;&#30001;&#24418;&#24335;&#21453;&#24212;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#26368;&#20808;&#36827;&#30340;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#32473;&#23450;IC&#23545;&#30340;&#24773;&#24863;&#21407;&#22240;&#29983;&#25104;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#26681;&#25454;&#19968;&#20010;&#21021;&#27493;&#30340;&#20154;&#31867;&#30740;&#31350;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#20154;&#20204;&#26356;&#21916;&#27426;&#20154;&#24037;&#25776;&#20889;&#30340;&#21407;&#22240;&#65292;&#27604;&#26426;&#22120;&#29983;&#25104;&#30340;&#35201;&#22810;2&#20493;&#20197;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing emotion prediction benchmarks contain coarse emotion labels which do not consider the diversity of emotions that an image and text can elicit in humans due to various reasons. Learning diverse reactions to multimodal content is important as intelligent machines take a central role in generating and delivering content to society. To address this gap, we propose Socratis, a \underline{soc}ietal \underline{r}e\underline{a}c\underline{ti}on\underline{s} benchmark, where each image-caption (IC) pair is annotated with multiple emotions and the reasons for feeling them. Socratis contains 18K free-form reactions for 980 emotions on 2075 image-caption pairs from 5 widely-read news and image-caption (IC) datasets. We benchmark the capability of state-of-the-art multimodal large language models to generate the reasons for feeling an emotion given an IC pair. Based on a preliminary human study, we observe that humans prefer human-written reasons over 2 times more often than machine-genera
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#30340;&#32593;&#32476;&#21270;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#37319;&#29992;$L_1$-&#33539;&#25968;&#40065;&#26834;&#24615;&#21644;&#20998;&#24067;&#24335;&#27425;&#26799;&#24230;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#22312;&#20998;&#24067;&#24335;&#29615;&#22659;&#20013;&#23450;&#20301;&#38382;&#39064;&#20013;&#30340;&#24322;&#24120;&#25968;&#25454;&#24178;&#25200;&#21644;&#31639;&#27861;&#25910;&#25947;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2308.16737</link><description>&lt;p&gt;
&#40065;&#26834;&#30340;&#32593;&#32476;&#21270;&#32852;&#37030;&#23398;&#20064;&#22312;&#23450;&#20301;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Robust Networked Federated Learning for Localization. (arXiv:2308.16737v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16737
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#30340;&#32593;&#32476;&#21270;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#37319;&#29992;$L_1$-&#33539;&#25968;&#40065;&#26834;&#24615;&#21644;&#20998;&#24067;&#24335;&#27425;&#26799;&#24230;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#22312;&#20998;&#24067;&#24335;&#29615;&#22659;&#20013;&#23450;&#20301;&#38382;&#39064;&#20013;&#30340;&#24322;&#24120;&#25968;&#25454;&#24178;&#25200;&#21644;&#31639;&#27861;&#25910;&#25947;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#22312;&#25968;&#25454;&#20998;&#24067;&#22312;&#22810;&#35774;&#22791;&#19978;&#30340;&#32852;&#37030;&#29615;&#22659;&#20013;&#65292;&#26412;&#36136;&#19978;&#26159;&#38750;&#20984;&#38750;&#20809;&#28369;&#30340;&#23450;&#20301;&#38382;&#39064;&#12290;&#30001;&#20110;&#32852;&#37030;&#29615;&#22659;&#30340;&#20998;&#25955;&#24615;&#36136;&#65292;&#20998;&#24067;&#24335;&#23398;&#20064;&#25104;&#20026;&#21487;&#20280;&#32553;&#24615;&#21644;&#36866;&#24212;&#24615;&#30340;&#20851;&#38190;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#29615;&#22659;&#32463;&#24120;&#21463;&#21040;&#24322;&#24120;&#25968;&#25454;&#30340;&#24178;&#25200;&#65292;&#20351;&#24471;&#20256;&#32479;&#26041;&#27861;&#22312;&#32500;&#25252;&#20272;&#35745;&#31934;&#24230;&#21644;&#30830;&#20445;&#31639;&#27861;&#25910;&#25947;&#26041;&#38754;&#38754;&#20020;&#37325;&#22823;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#29992;&#20998;&#24067;&#24335;&#27425;&#26799;&#24230;&#26694;&#26550;&#20013;$L_1$-&#33539;&#25968;&#40065;&#26834;&#24615;&#30340;&#26041;&#27861;&#65292;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#22788;&#29702;&#36825;&#20123;&#38556;&#30861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20197;&#21407;&#22987;&#24418;&#24335;&#35299;&#20915;&#38382;&#39064;&#65292;&#32780;&#19981;&#26159;&#37319;&#29992;&#36845;&#20195;&#31616;&#21270;&#25110;&#36817;&#20284;&#26041;&#27861;&#65292;&#20174;&#32780;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#21644;&#20272;&#35745;&#31934;&#24230;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#25910;&#25947;&#21040;&#19968;&#20010;&#31283;&#23450;&#28857;&#65292;&#31361;&#20986;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper addresses the problem of localization, which is inherently non-convex and non-smooth in a federated setting where the data is distributed across a multitude of devices. Due to the decentralized nature of federated environments, distributed learning becomes essential for scalability and adaptability. Moreover, these environments are often plagued by outlier data, which presents substantial challenges to conventional methods, particularly in maintaining estimation accuracy and ensuring algorithm convergence. To mitigate these challenges, we propose a method that adopts an $L_1$-norm robust formulation within a distributed sub-gradient framework, explicitly designed to handle these obstacles. Our approach addresses the problem in its original form, without resorting to iterative simplifications or approximations, resulting in enhanced computational efficiency and improved estimation accuracy. We demonstrate that our method converges to a stationary point, highlighting its effec
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedPDA&#30340;&#26032;&#39062;&#33258;&#36866;&#24212;&#26694;&#26550;&#65292;&#23558;&#32852;&#37030;&#23398;&#20064;&#20013;&#23398;&#20064;&#36828;&#31243;&#25968;&#25454;&#30340;&#21151;&#33021;&#24341;&#20837;&#21040;PDA&#20013;&#65292;&#36890;&#36807;&#36828;&#31243;&#26799;&#24230;&#20132;&#25442;&#20351;&#37096;&#32626;&#27169;&#22411;&#33021;&#22815;&#20174;&#28304;&#25968;&#25454;&#20013;&#33719;&#21462;&#20449;&#24687;&#65292;&#24182;&#38024;&#23545;&#30446;&#26631;&#39046;&#22495;&#36827;&#34892;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2308.16735</link><description>&lt;p&gt;
&#36890;&#36807;&#32852;&#37030;&#23398;&#20064;&#21644;&#28304;&#30446;&#26631;&#36828;&#31243;&#26799;&#24230;&#23545;&#40784;&#23454;&#29616;&#37096;&#32626;&#21518;&#33258;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Post-Deployment Adaptation with Access to Source Data via Federated Learning and Source-Target Remote Gradient Alignment. (arXiv:2308.16735v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16735
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedPDA&#30340;&#26032;&#39062;&#33258;&#36866;&#24212;&#26694;&#26550;&#65292;&#23558;&#32852;&#37030;&#23398;&#20064;&#20013;&#23398;&#20064;&#36828;&#31243;&#25968;&#25454;&#30340;&#21151;&#33021;&#24341;&#20837;&#21040;PDA&#20013;&#65292;&#36890;&#36807;&#36828;&#31243;&#26799;&#24230;&#20132;&#25442;&#20351;&#37096;&#32626;&#27169;&#22411;&#33021;&#22815;&#20174;&#28304;&#25968;&#25454;&#20013;&#33719;&#21462;&#20449;&#24687;&#65292;&#24182;&#38024;&#23545;&#30446;&#26631;&#39046;&#22495;&#36827;&#34892;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21307;&#23398;&#24433;&#20687;&#20013;&#37096;&#32626;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21463;&#21040;&#35757;&#32451;&#25968;&#25454;&#21644;&#37096;&#32626;&#21518;&#22788;&#29702;&#30340;&#25968;&#25454;&#20043;&#38388;&#30340;&#20998;&#24067;&#20559;&#31227;&#30340;&#24433;&#21709;&#65292;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#12290;&#37096;&#32626;&#21518;&#33258;&#36866;&#24212;(PDA)&#36890;&#36807;&#20351;&#29992;&#26377;&#38480;&#30340;&#26631;&#35760;&#25110;&#23436;&#20840;&#26080;&#26631;&#35760;&#30340;&#30446;&#26631;&#25968;&#25454;&#65292;&#23558;&#39044;&#35757;&#32451;&#30340;&#37096;&#32626;&#27169;&#22411;&#36866;&#24212;&#21040;&#30446;&#26631;&#25968;&#25454;&#20998;&#24067;&#65292;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#32852;&#37030;&#33258;&#36866;&#24212;(FedPDA)&#25361;&#25112;&#20102;&#36825;&#20010;&#20551;&#35774;&#65292;&#24182;&#23558;&#32852;&#37030;&#23398;&#20064;&#20013;&#23398;&#20064;&#36828;&#31243;&#25968;&#25454;&#30340;&#21151;&#33021;&#24341;&#20837;&#21040;PDA&#20013;&#12290;FedPDA&#36890;&#36807;&#36828;&#31243;&#26799;&#24230;&#20132;&#25442;&#65292;&#20351;&#37096;&#32626;&#27169;&#22411;&#33021;&#22815;&#20174;&#28304;&#25968;&#25454;&#20013;&#33719;&#21462;&#20449;&#24687;&#65292;&#24182;&#38024;&#23545;&#30446;&#26631;&#39046;&#22495;&#36827;&#34892;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deployment of Deep Neural Networks in medical imaging is hindered by distribution shift between training data and data processed after deployment, causing performance degradation. Post-Deployment Adaptation (PDA) addresses this by tailoring a pre-trained, deployed model to the target data distribution using limited labelled or entirely unlabelled target data, while assuming no access to source training data as they cannot be deployed with the model due to privacy concerns and their large size. This makes reliable adaptation challenging due to limited learning signal. This paper challenges this assumption and introduces FedPDA, a novel adaptation framework that brings the utility of learning from remote data from Federated Learning into PDA. FedPDA enables a deployed model to obtain information from source data via remote gradient exchange, while aiming to optimize the model specifically for the target domain. Tailored for FedPDA, we introduce a novel optimization method StarAlign (Sour
&lt;/p&gt;</description></item><item><title>&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#25552;&#21319;&#38656;&#35201;&#26356;&#22810;&#35745;&#31639;&#33021;&#21147;&#65292;&#32780;&#21306;&#22359;&#38142;&#20013;&#30340;&#24037;&#20316;&#37327;&#35777;&#26126;&#26426;&#21046;&#23545;&#35745;&#31639;&#33021;&#21147;&#25552;&#20986;&#20102;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2308.16730</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#30340;&#35777;&#26126;&#65306;&#26041;&#27861;&#12289;&#25361;&#25112;&#21644;&#26410;&#26469;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;
Proof of Deep Learning: Approaches, Challenges, and Future Directions. (arXiv:2308.16730v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16730
&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#25552;&#21319;&#38656;&#35201;&#26356;&#22810;&#35745;&#31639;&#33021;&#21147;&#65292;&#32780;&#21306;&#22359;&#38142;&#20013;&#30340;&#24037;&#20316;&#37327;&#35777;&#26126;&#26426;&#21046;&#23545;&#35745;&#31639;&#33021;&#21147;&#25552;&#20986;&#20102;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35745;&#31639;&#33021;&#21147;&#30340;&#25552;&#21319;&#20026;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#24102;&#26469;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#38543;&#30528;&#36234;&#26469;&#36234;&#22810;&#30340;&#25968;&#25454;&#21487;&#29992;&#20197;&#21450;&#27169;&#22411;&#26550;&#26500;&#21464;&#24471;&#26356;&#21152;&#22797;&#26434;&#65292;&#23545;&#26356;&#22810;&#35745;&#31639;&#33021;&#21147;&#30340;&#38656;&#27714;&#20063;&#22312;&#22686;&#21152;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#33258;&#27604;&#29305;&#24065;&#20316;&#20026;&#31532;&#19968;&#31181;&#21152;&#23494;&#36135;&#24065;&#30340;&#38382;&#19990;&#20197;&#21450;&#21306;&#22359;&#38142;&#27010;&#24565;&#20316;&#20026;&#20998;&#24067;&#24335;&#36134;&#26412;&#30340;&#24314;&#31435;&#20197;&#26469;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#21464;&#31181;&#21644;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#20854;&#20013;&#35768;&#22810;&#26041;&#27861;&#26377;&#19968;&#20010;&#20849;&#21516;&#28857;&#65292;&#21363;&#24037;&#20316;&#37327;&#35777;&#26126;&#65288;Proof of Work&#65292;PoW&#65289;&#20849;&#35782;&#26426;&#21046;&#12290;PoW&#20027;&#35201;&#29992;&#20110;&#25903;&#25345;&#26032;&#21306;&#22359;&#30340;&#29983;&#25104;&#36807;&#31243;&#12290;&#34429;&#28982;PoW&#24050;&#32463;&#35777;&#26126;&#20102;&#20854;&#40065;&#26834;&#24615;&#65292;&#20294;&#20854;&#20027;&#35201;&#32570;&#28857;&#26159;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#33021;&#21147;&#26469;&#32500;&#25252;&#21306;&#22359;&#38142;&#30340;&#23433;&#20840;&#21644;&#23436;&#25972;&#24615;&#12290;&#36825;&#26159;&#30001;&#20110;&#24212;&#29992;&#26292;&#21147;&#30772;&#35299;&#26469;&#35299;&#20915;&#21704;&#24076;&#38590;&#39064;&#12290;&#20026;&#20102;&#21033;&#29992;&#21487;&#29992;&#30340;&#35745;&#31639;&#33021;&#21147;&#36827;&#34892;&#26377;&#29992;&#32780;&#26377;&#24847;&#20041;&#30340;&#24037;&#20316;&#65292;&#21516;&#26102;&#20445;&#25345;&#21306;&#22359;&#38142;&#30340;&#23433;&#20840;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#25216;&#26415;&#65292;&#20854;&#20013;&#20043;&#19968;&#23601;&#26159;...
&lt;/p&gt;
&lt;p&gt;
The rise of computational power has led to unprecedented performance gains for deep learning models. As more data becomes available and model architectures become more complex, the need for more computational power increases. On the other hand, since the introduction of Bitcoin as the first cryptocurrency and the establishment of the concept of blockchain as a distributed ledger, many variants and approaches have been proposed. However, many of them have one thing in common, which is the Proof of Work (PoW) consensus mechanism. PoW is mainly used to support the process of new block generation. While PoW has proven its robustness, its main drawback is that it requires a significant amount of processing power to maintain the security and integrity of the blockchain. This is due to applying brute force to solve a hashing puzzle. To utilize the computational power available in useful and meaningful work while keeping the blockchain secure, many techniques have been proposed, one of which i
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#30340;&#22320;&#24418;&#29983;&#25104;&#26041;&#27861;&#65292;&#21517;&#20026;&#22320;&#24418;&#25193;&#25955;&#32593;&#32476;&#65288;TDN&#65289;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#29992;&#25143;&#24341;&#23548;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#21487;&#25511;&#24615;&#65292;&#24182;&#32771;&#34385;&#22320;&#24418;&#29305;&#24449;&#20026;&#34394;&#25311;&#29615;&#22659;&#29983;&#25104;&#36924;&#30495;&#30340;&#22320;&#24418;&#12290;</title><link>http://arxiv.org/abs/2308.16725</link><description>&lt;p&gt;
&#22320;&#24418;&#25193;&#25955;&#32593;&#32476;&#65306;&#20855;&#26377;&#22320;&#36136;&#33609;&#22270;&#24341;&#23548;&#30340;&#27668;&#20505;&#24863;&#30693;&#22320;&#24418;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Terrain Diffusion Network: Climatic-Aware Terrain Generation with Geological Sketch Guidance. (arXiv:2308.16725v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16725
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#30340;&#22320;&#24418;&#29983;&#25104;&#26041;&#27861;&#65292;&#21517;&#20026;&#22320;&#24418;&#25193;&#25955;&#32593;&#32476;&#65288;TDN&#65289;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#29992;&#25143;&#24341;&#23548;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#21487;&#25511;&#24615;&#65292;&#24182;&#32771;&#34385;&#22320;&#24418;&#29305;&#24449;&#20026;&#34394;&#25311;&#29615;&#22659;&#29983;&#25104;&#36924;&#30495;&#30340;&#22320;&#24418;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#33609;&#22270;&#30340;&#22320;&#24418;&#29983;&#25104;&#26088;&#22312;&#20026;&#34394;&#25311;&#29615;&#22659;&#21019;&#24314;&#36924;&#30495;&#30340;&#26223;&#35266;&#65292;&#24191;&#27867;&#24212;&#29992;&#20110;&#35745;&#31639;&#26426;&#28216;&#25103;&#12289;&#21160;&#30011;&#21644;&#34394;&#25311;&#29616;&#23454;&#31561;&#39046;&#22495;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#22320;&#24418;&#29983;&#25104;&#26041;&#27861;&#19981;&#26029;&#28044;&#29616;&#65292;&#23588;&#20854;&#26159;&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#24120;&#24120;&#38590;&#20197;&#28385;&#36275;&#29992;&#25143;&#28789;&#27963;&#25511;&#21046;&#21644;&#20445;&#25345;&#29983;&#25104;&#22810;&#26679;&#24615;&#20197;&#23454;&#29616;&#36924;&#30495;&#22320;&#24418;&#30340;&#35201;&#27714;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#26032;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#26041;&#27861;&#65292;&#21363;&#22320;&#24418;&#25193;&#25955;&#32593;&#32476;&#65288;TDN&#65289;&#65292;&#23427;&#20027;&#21160;&#34701;&#21512;&#29992;&#25143;&#24341;&#23548;&#20197;&#22686;&#24378;&#21487;&#25511;&#24615;&#65292;&#24182;&#32771;&#34385;&#21040;&#27827;&#27969;&#12289;&#23665;&#33034;&#12289;&#30406;&#22320;&#21644;&#23665;&#23792;&#31561;&#22320;&#24418;&#29305;&#24449;&#12290;&#19982;&#20256;&#32479;&#30340;&#21333;&#19968;&#21435;&#22122;&#26041;&#27861;&#19981;&#21516;&#65292;&#35813;&#26041;&#27861;&#25552;&#20986;&#20102;&#22810;&#23618;&#27425;&#30340;&#21435;&#22122;&#26041;&#26696;&#65292;&#36890;&#36807;&#32771;&#34385;&#32454;&#31890;&#24230;&#30340;&#32454;&#33410;&#65292;&#29305;&#21035;&#26159;&#19982;&#29992;&#25143;&#25511;&#21046;&#30456;&#20851;&#30340;&#32454;&#33410;&#65292;&#29983;&#25104;&#26356;&#21152;&#36924;&#30495;&#30340;&#22320;&#24418;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sketch-based terrain generation seeks to create realistic landscapes for virtual environments in various applications such as computer games, animation and virtual reality. Recently, deep learning based terrain generation has emerged, notably the ones based on generative adversarial networks (GAN). However, these methods often struggle to fulfill the requirements of flexible user control and maintain generative diversity for realistic terrain. Therefore, we propose a novel diffusion-based method, namely terrain diffusion network (TDN), which actively incorporates user guidance for enhanced controllability, taking into account terrain features like rivers, ridges, basins, and peaks. Instead of adhering to a conventional monolithic denoising process, which often compromises the fidelity of terrain details or the alignment with user control, a multi-level denoising scheme is proposed to generate more realistic terrains by taking into account fine-grained details, particularly those relate
&lt;/p&gt;</description></item><item><title>CReHate&#36890;&#36807;&#36328;&#25991;&#21270;&#37325;&#26032;&#27880;&#37322;&#33521;&#35821;&#20167;&#24680;&#35328;&#35770;&#25968;&#25454;&#38598;&#65292;&#25581;&#31034;&#20102;&#26469;&#33258;&#19981;&#21516;&#22269;&#23478;&#30340;&#20010;&#20307;&#23545;&#20167;&#24680;&#35328;&#35770;&#30340;&#19981;&#21516;&#30475;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#20855;&#26377;&#25991;&#21270;&#25935;&#24863;&#24615;&#30340;&#20998;&#31867;&#22120;&#12290;&#36825;&#20123;&#21457;&#29616;&#24378;&#35843;&#20102;&#37325;&#26032;&#35780;&#20272;NLP&#30740;&#31350;&#22312;&#20167;&#24680;&#35328;&#35770;&#39046;&#22495;&#30340;&#24517;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.16705</link><description>&lt;p&gt;
CReHate: &#36328;&#25991;&#21270;&#37325;&#26032;&#27880;&#37322;&#33521;&#35821;&#20167;&#24680;&#35328;&#35770;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
CReHate: Cross-cultural Re-annotation of English Hate Speech Dataset. (arXiv:2308.16705v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16705
&lt;/p&gt;
&lt;p&gt;
CReHate&#36890;&#36807;&#36328;&#25991;&#21270;&#37325;&#26032;&#27880;&#37322;&#33521;&#35821;&#20167;&#24680;&#35328;&#35770;&#25968;&#25454;&#38598;&#65292;&#25581;&#31034;&#20102;&#26469;&#33258;&#19981;&#21516;&#22269;&#23478;&#30340;&#20010;&#20307;&#23545;&#20167;&#24680;&#35328;&#35770;&#30340;&#19981;&#21516;&#30475;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#20855;&#26377;&#25991;&#21270;&#25935;&#24863;&#24615;&#30340;&#20998;&#31867;&#22120;&#12290;&#36825;&#20123;&#21457;&#29616;&#24378;&#35843;&#20102;&#37325;&#26032;&#35780;&#20272;NLP&#30740;&#31350;&#22312;&#20167;&#24680;&#35328;&#35770;&#39046;&#22495;&#30340;&#24517;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33521;&#35821;&#25968;&#25454;&#38598;&#20027;&#35201;&#21453;&#26144;&#20102;&#29305;&#23450;&#22269;&#23478;&#30340;&#35266;&#28857;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#20013;&#23384;&#22312;&#25991;&#21270;&#20559;&#24046;&#12290;&#36825;&#22312;&#21463;&#20027;&#35266;&#24615;&#24433;&#21709;&#36739;&#22823;&#30340;&#20219;&#21153;&#65292;&#22914;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#20013;&#29305;&#21035;&#26377;&#38382;&#39064;&#12290;&#20026;&#20102;&#28145;&#20837;&#20102;&#35299;&#26469;&#33258;&#19981;&#21516;&#22269;&#23478;&#30340;&#20010;&#20307;&#22914;&#20309;&#29702;&#35299;&#20167;&#24680;&#35328;&#35770;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;CReHate&#65292;&#23545;&#25277;&#26679;&#30340;SBIC&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#36328;&#25991;&#21270;&#37325;&#26032;&#27880;&#37322;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#25324;&#26469;&#33258;&#20116;&#20010;&#19981;&#21516;&#22269;&#23478;&#30340;&#27880;&#37322;&#65306;&#28595;&#22823;&#21033;&#20122;&#12289;&#26032;&#21152;&#22369;&#12289;&#21335;&#38750;&#12289;&#33521;&#22269;&#21644;&#32654;&#22269;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#24443;&#24213;&#30340;&#32479;&#35745;&#20998;&#26512;&#65292;&#21457;&#29616;&#22522;&#20110;&#22269;&#31821;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#65292;&#21482;&#26377;59.4%&#30340;&#26679;&#26412;&#22312;&#25152;&#26377;&#22269;&#23478;&#20043;&#38388;&#36798;&#25104;&#20849;&#35782;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#36801;&#31227;&#23398;&#20064;&#24341;&#20837;&#20102;&#19968;&#31181;&#20855;&#26377;&#25991;&#21270;&#25935;&#24863;&#24615;&#30340;&#20167;&#24680;&#35328;&#35770;&#20998;&#31867;&#22120;&#65292;&#33021;&#22815;&#25429;&#25417;&#19981;&#21516;&#22269;&#31821;&#30340;&#35266;&#28857;&#12290;&#36825;&#20123;&#21457;&#29616;&#24378;&#35843;&#20102;&#38656;&#35201;&#37325;&#26032;&#35780;&#20272;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30740;&#31350;&#30340;&#26576;&#20123;&#26041;&#38754;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#20167;&#24680;&#35328;&#35770;&#30340;&#32454;&#24494;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;
English datasets predominantly reflect the perspectives of certain nationalities, which can lead to cultural biases in models and datasets. This is particularly problematic in tasks heavily influenced by subjectivity, such as hate speech detection. To delve into how individuals from different countries perceive hate speech, we introduce CReHate, a cross-cultural re-annotation of the sampled SBIC dataset. This dataset includes annotations from five distinct countries: Australia, Singapore, South Africa, the United Kingdom, and the United States. Our thorough statistical analysis highlights significant differences based on nationality, with only 59.4% of the samples achieving consensus among all countries. We also introduce a culturally sensitive hate speech classifier via transfer learning, adept at capturing perspectives of different nationalities. These findings underscore the need to re-evaluate certain aspects of NLP research, especially with regard to the nuanced nature of hate spe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#25925;&#38556;&#27880;&#20837;&#21644;&#23433;&#20840;&#38169;&#35823;&#25915;&#20987;&#29992;&#20110;&#25552;&#21462;&#23884;&#20837;&#24335;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#24182;&#38416;&#36848;&#20102;&#23545;32&#20301;&#24494;&#25511;&#21046;&#22120;&#19978;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#27169;&#22411;&#25552;&#21462;&#25915;&#20987;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.16703</link><description>&lt;p&gt;
&#25925;&#38556;&#27880;&#20837;&#21644;&#23433;&#20840;&#38169;&#35823;&#25915;&#20987;&#29992;&#20110;&#25552;&#21462;&#23884;&#20837;&#24335;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Fault Injection and Safe-Error Attack for Extraction of Embedded Neural Network Models. (arXiv:2308.16703v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16703
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#25925;&#38556;&#27880;&#20837;&#21644;&#23433;&#20840;&#38169;&#35823;&#25915;&#20987;&#29992;&#20110;&#25552;&#21462;&#23884;&#20837;&#24335;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#24182;&#38416;&#36848;&#20102;&#23545;32&#20301;&#24494;&#25511;&#21046;&#22120;&#19978;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#27169;&#22411;&#25552;&#21462;&#25915;&#20987;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#25552;&#21462;&#20316;&#20026;&#19968;&#31181;&#20851;&#38190;&#30340;&#23433;&#20840;&#23041;&#32961;&#32780;&#20986;&#29616;&#65292;&#25915;&#20987;&#21521;&#37327;&#21033;&#29992;&#20102;&#31639;&#27861;&#21644;&#23454;&#29616;&#26041;&#38754;&#30340;&#26041;&#27861;&#12290;&#25915;&#20987;&#32773;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#23613;&#21487;&#33021;&#22810;&#22320;&#31363;&#21462;&#21463;&#20445;&#25252;&#30340;&#21463;&#23475;&#32773;&#27169;&#22411;&#30340;&#20449;&#24687;&#65292;&#20197;&#20415;&#20182;&#21487;&#20197;&#29992;&#26367;&#20195;&#27169;&#22411;&#26469;&#27169;&#20223;&#23427;&#65292;&#21363;&#20351;&#21482;&#26377;&#26377;&#38480;&#30340;&#35775;&#38382;&#30456;&#20284;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#26368;&#36817;&#65292;&#29289;&#29702;&#25915;&#20987;&#65292;&#22914;&#25925;&#38556;&#27880;&#20837;&#65292;&#24050;&#32463;&#26174;&#31034;&#20986;&#23545;&#23884;&#20837;&#24335;&#27169;&#22411;&#30340;&#23436;&#25972;&#24615;&#21644;&#26426;&#23494;&#24615;&#30340;&#20196;&#20154;&#25285;&#24551;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#30340;&#37325;&#28857;&#26159;32&#20301;&#24494;&#25511;&#21046;&#22120;&#19978;&#30340;&#23884;&#20837;&#24335;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#36825;&#26159;&#29289;&#32852;&#32593;&#20013;&#24191;&#27867;&#20351;&#29992;&#30340;&#30828;&#20214;&#24179;&#21488;&#31995;&#21015;&#65292;&#20197;&#21450;&#20351;&#29992;&#26631;&#20934;&#25925;&#38556;&#27880;&#20837;&#31574;&#30053;-&#23433;&#20840;&#38169;&#35823;&#25915;&#20987;&#65288;SEA&#65289;&#26469;&#36827;&#34892;&#20855;&#26377;&#26377;&#38480;&#35757;&#32451;&#25968;&#25454;&#35775;&#38382;&#30340;&#27169;&#22411;&#25552;&#21462;&#25915;&#20987;&#12290;&#30001;&#20110;&#25915;&#20987;&#24378;&#28872;&#20381;&#36182;&#20110;&#36755;&#20837;&#26597;&#35810;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#40657;&#30418;&#26041;&#27861;&#26469;&#26500;&#24314;&#19968;&#20010;&#25104;&#21151;&#30340;&#25915;&#20987;&#38598;&#12290;&#23545;&#20110;&#19968;&#20010;&#32463;&#20856;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#25105;&#20204;&#25104;&#21151;&#22320;&#24674;&#22797;&#20102;&#33267;&#23569;90%&#30340;
&lt;/p&gt;
&lt;p&gt;
Model extraction emerges as a critical security threat with attack vectors exploiting both algorithmic and implementation-based approaches. The main goal of an attacker is to steal as much information as possible about a protected victim model, so that he can mimic it with a substitute model, even with a limited access to similar training data. Recently, physical attacks such as fault injection have shown worrying efficiency against the integrity and confidentiality of embedded models. We focus on embedded deep neural network models on 32-bit microcontrollers, a widespread family of hardware platforms in IoT, and the use of a standard fault injection strategy - Safe Error Attack (SEA) - to perform a model extraction attack with an adversary having a limited access to training data. Since the attack strongly depends on the input queries, we propose a black-box approach to craft a successful attack set. For a classical convolutional neural network, we successfully recover at least 90% of
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#20998;&#31867;&#31185;&#23398;&#25991;&#31456;&#30340;&#26041;&#27861;&#65292;&#20027;&#35201;&#24212;&#29992;&#20110;&#30524;&#31185;&#23398;&#39046;&#22495;&#65292;&#20294;&#21487;&#25193;&#23637;&#21040;&#20854;&#20182;&#39046;&#22495;&#12290;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;LLM&#27169;&#22411;&#65292;&#32467;&#26524;&#34920;&#26126;LLMs&#22312;&#26080;&#38656;&#20154;&#24037;&#24178;&#39044;&#30340;&#24773;&#20917;&#19979;&#33021;&#26377;&#25928;&#22320;&#23545;&#22823;&#37327;&#30524;&#31185;&#23398;&#35770;&#25991;&#36827;&#34892;&#20998;&#31867;&#12290;</title><link>http://arxiv.org/abs/2308.16688</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#21270;&#31185;&#23398;&#25991;&#31456;&#30340;&#20998;&#31867;&#21644;&#36235;&#21183;&#20998;&#26512;&#65306;&#20197;&#30524;&#31185;&#23398;&#20026;&#24212;&#29992;&#23454;&#20363;
&lt;/p&gt;
&lt;p&gt;
Using Large Language Models to Automate Category and Trend Analysis of Scientific Articles: An Application in Ophthalmology. (arXiv:2308.16688v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16688
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#20998;&#31867;&#31185;&#23398;&#25991;&#31456;&#30340;&#26041;&#27861;&#65292;&#20027;&#35201;&#24212;&#29992;&#20110;&#30524;&#31185;&#23398;&#39046;&#22495;&#65292;&#20294;&#21487;&#25193;&#23637;&#21040;&#20854;&#20182;&#39046;&#22495;&#12290;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;LLM&#27169;&#22411;&#65292;&#32467;&#26524;&#34920;&#26126;LLMs&#22312;&#26080;&#38656;&#20154;&#24037;&#24178;&#39044;&#30340;&#24773;&#20917;&#19979;&#33021;&#26377;&#25928;&#22320;&#23545;&#22823;&#37327;&#30524;&#31185;&#23398;&#35770;&#25991;&#36827;&#34892;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#33258;&#21160;&#36827;&#34892;&#25991;&#31456;&#20998;&#31867;&#30340;&#26041;&#27861;&#12290;&#20027;&#35201;&#20851;&#27880;&#30524;&#31185;&#39046;&#22495;&#65292;&#20294;&#35813;&#27169;&#22411;&#21487;&#25193;&#23637;&#21040;&#20854;&#20182;&#39046;&#22495;&#12290;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#65288;NLP&#65289;&#24320;&#21457;&#20102;&#19968;&#20010;&#27169;&#22411;&#65292;&#21253;&#25324;&#39640;&#32423;LLM&#65292;&#29992;&#20110;&#22788;&#29702;&#21644;&#20998;&#26512;&#31185;&#23398;&#35770;&#25991;&#30340;&#25991;&#26412;&#20869;&#23481;&#12290;&#22312;LLM&#27169;&#22411;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#38646;&#26679;&#26412;&#23398;&#20064;&#65288;ZSL&#65289;LLM&#27169;&#22411;&#65292;&#24182;&#19982;&#21452;&#21521;&#21644;&#33258;&#22238;&#24402;&#21464;&#25442;&#22120;&#65288;BART&#65289;&#21450;&#20854;&#21464;&#31181;&#65292;&#20197;&#21450;&#21452;&#21521;&#32534;&#30721;&#22120;&#34920;&#31034;&#20174;&#21464;&#25442;&#22120;&#65288;BERT&#65289;&#21450;&#20854;&#21464;&#31181;&#65288;&#22914;distilBERT&#65292;SciBERT&#65292;PubmedBERT&#65292;BioBERT&#65289;&#36827;&#34892;&#27604;&#36739;&#12290;&#20998;&#31867;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#27809;&#26377;&#20154;&#20026;&#24178;&#39044;&#30340;&#24773;&#20917;&#19979;&#65292;LLM&#22312;&#23545;&#22823;&#37327;&#30524;&#31185;&#23398;&#35770;&#25991;&#36827;&#34892;&#20998;&#31867;&#26041;&#38754;&#20855;&#26377;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Purpose: In this paper, we present an automated method for article classification, leveraging the power of Large Language Models (LLM). The primary focus is on the field of ophthalmology, but the model is extendable to other fields. Methods: We have developed a model based on Natural Language Processing (NLP) techniques, including advanced LLMs, to process and analyze the textual content of scientific papers. Specifically, we have employed zero-shot learning (ZSL) LLM models and compared against Bidirectional and Auto-Regressive Transformers (BART) and its variants, and Bidirectional Encoder Representations from Transformers (BERT), and its variant such as distilBERT, SciBERT, PubmedBERT, BioBERT. Results: The classification results demonstrate the effectiveness of LLMs in categorizing large number of ophthalmology papers without human intervention. Results: To evalute the LLMs, we compiled a dataset (RenD) of 1000 ocular disease-related articles, which were expertly annotated by a pan
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21457;&#29616;&#20102;&#19968;&#31181;&#26356;&#20005;&#37325;&#30340;&#21518;&#38376;&#25915;&#20987;&#23041;&#32961;&#65292;&#21363;&#20219;&#20309;&#20154;&#37117;&#21487;&#20197;&#21033;&#29992;&#26131;&#33719;&#21462;&#30340;&#26377;&#25439;&#21387;&#32553;&#31639;&#27861;&#36827;&#34892;&#33258;&#28982;&#21518;&#38376;&#25915;&#20987;&#65292;&#26080;&#38656;&#35774;&#35745;&#29305;&#23450;&#35302;&#21457;&#22120;&#25110;&#36827;&#34892;&#32321;&#29712;&#35843;&#35797;&#12290;</title><link>http://arxiv.org/abs/2308.16684</link><description>&lt;p&gt;
&#20219;&#20309;&#20154;&#37117;&#21487;&#20197;&#25915;&#20987;&#65306;&#23558;&#26377;&#25439;&#21387;&#32553;&#37325;&#26032;&#29992;&#20316;&#33258;&#28982;&#21518;&#38376;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Everyone Can Attack: Repurpose Lossy Compression as a Natural Backdoor Attack. (arXiv:2308.16684v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16684
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21457;&#29616;&#20102;&#19968;&#31181;&#26356;&#20005;&#37325;&#30340;&#21518;&#38376;&#25915;&#20987;&#23041;&#32961;&#65292;&#21363;&#20219;&#20309;&#20154;&#37117;&#21487;&#20197;&#21033;&#29992;&#26131;&#33719;&#21462;&#30340;&#26377;&#25439;&#21387;&#32553;&#31639;&#27861;&#36827;&#34892;&#33258;&#28982;&#21518;&#38376;&#25915;&#20987;&#65292;&#26080;&#38656;&#35774;&#35745;&#29305;&#23450;&#35302;&#21457;&#22120;&#25110;&#36827;&#34892;&#32321;&#29712;&#35843;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#21518;&#38376;&#25915;&#20987;&#23545;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;&#26500;&#25104;&#20102;&#23041;&#32961;&#12290;&#20256;&#32479;&#26234;&#24935;&#35748;&#20026;&#65292;&#24182;&#19981;&#26159;&#27599;&#20010;&#20154;&#37117;&#21487;&#20197;&#25104;&#20026;&#25915;&#20987;&#32773;&#65292;&#22240;&#20026;&#35774;&#35745;&#35302;&#21457;&#22120;&#29983;&#25104;&#31639;&#27861;&#30340;&#36807;&#31243;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#30340;&#21162;&#21147;&#21644;&#24191;&#27867;&#30340;&#23454;&#39564;&#26469;&#30830;&#20445;&#25915;&#20987;&#30340;&#38544;&#31192;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;&#28982;&#32780;&#65292;&#26412;&#25991;&#25351;&#20986;&#23384;&#22312;&#19968;&#31181;&#26356;&#20026;&#20005;&#37325;&#30340;&#21518;&#38376;&#23041;&#32961;&#65306;&#20219;&#20309;&#20154;&#37117;&#21487;&#20197;&#21033;&#29992;&#26131;&#33719;&#21462;&#30340;&#31639;&#27861;&#36827;&#34892;&#38544;&#24708;&#21518;&#38376;&#25915;&#20987;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#21033;&#29992;&#21508;&#31181;&#21387;&#32553;&#24037;&#20855;&#20013;&#24191;&#27867;&#20351;&#29992;&#30340;&#26377;&#25439;&#22270;&#29255;&#21387;&#32553;&#25216;&#26415;&#65292;&#26080;&#38656;&#30041;&#19979;&#20219;&#20309;&#26126;&#26174;&#30340;&#30165;&#36857;&#23601;&#33021;&#36731;&#26494;&#22320;&#23558;&#35302;&#21457;&#22120;&#27169;&#24335;&#27880;&#20837;&#21040;&#22270;&#20687;&#20013;&#65292;&#21363;&#29983;&#25104;&#30340;&#35302;&#21457;&#22120;&#26159;&#33258;&#28982;&#30340;&#22270;&#20687;&#20266;&#24433;&#12290;&#20351;&#29992;&#26377;&#25439;&#22270;&#29255;&#21387;&#32553;&#24037;&#20855;&#26102;&#65292;&#20154;&#20204;&#24182;&#19981;&#38656;&#35201;&#24191;&#27867;&#30693;&#35782;&#65292;&#21482;&#38656;&#28857;&#20987;&#8220;&#36716;&#25442;&#8221;&#25110;&#8220;&#21478;&#23384;&#20026;&#8221;&#25353;&#38062;&#21363;&#21487;&#12290;&#36890;&#36807;&#36825;&#31181;&#25915;&#20987;&#65292;&#25915;&#20987;&#32773;&#26080;&#38656;&#35774;&#35745;&#19968;&#20010;&#19987;&#38376;&#30340;&#35302;&#21457;&#22120;&#25110;&#36827;&#34892;&#32321;&#29712;&#30340;&#35843;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
The vulnerabilities to backdoor attacks have recently threatened the trustworthiness of machine learning models in practical applications. Conventional wisdom suggests that not everyone can be an attacker since the process of designing the trigger generation algorithm often involves significant effort and extensive experimentation to ensure the attack's stealthiness and effectiveness. Alternatively, this paper shows that there exists a more severe backdoor threat: anyone can exploit an easily-accessible algorithm for silent backdoor attacks. Specifically, this attacker can employ the widely-used lossy image compression from a plethora of compression tools to effortlessly inject a trigger pattern into an image without leaving any noticeable trace; i.e., the generated triggers are natural artifacts. One does not require extensive knowledge to click on the "convert" or "save as" button while using tools for lossy image compression. Via this attack, the adversary does not need to design a 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#23454;&#39564;&#25506;&#31350;&#20102;&#22312;&#23884;&#20837;&#24335;&#30340;Cortex M4 32&#20301;&#24494;&#25511;&#21046;&#22120;&#24179;&#21488;&#19978;&#24212;&#29992;&#30005;&#30913;&#21644;&#28608;&#20809;&#27880;&#20837;&#25925;&#38556;&#30340;&#24433;&#21709;&#65292;&#24182;&#25581;&#31034;&#20102;&#23545;&#31070;&#32463;&#32593;&#32476;&#25512;&#29702;&#27969;&#31243;&#36827;&#34892;&#20462;&#25913;&#25915;&#20987;&#21487;&#33021;&#24102;&#26469;&#30340;&#23436;&#25972;&#24615;&#23041;&#32961;&#12290;</title><link>http://arxiv.org/abs/2308.16665</link><description>&lt;p&gt;
&#23884;&#20837;&#24335;&#31070;&#32463;&#32593;&#32476;&#30340;&#25925;&#38556;&#27880;&#20837;&#65306;&#21333;&#26465;&#25351;&#20196;&#36339;&#36807;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Fault Injection on Embedded Neural Networks: Impact of a Single Instruction Skip. (arXiv:2308.16665v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16665
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#23454;&#39564;&#25506;&#31350;&#20102;&#22312;&#23884;&#20837;&#24335;&#30340;Cortex M4 32&#20301;&#24494;&#25511;&#21046;&#22120;&#24179;&#21488;&#19978;&#24212;&#29992;&#30005;&#30913;&#21644;&#28608;&#20809;&#27880;&#20837;&#25925;&#38556;&#30340;&#24433;&#21709;&#65292;&#24182;&#25581;&#31034;&#20102;&#23545;&#31070;&#32463;&#32593;&#32476;&#25512;&#29702;&#27969;&#31243;&#36827;&#34892;&#20462;&#25913;&#25915;&#20987;&#21487;&#33021;&#24102;&#26469;&#30340;&#23436;&#25972;&#24615;&#23041;&#32961;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#22823;&#35268;&#27169;&#38598;&#25104;&#21644;&#20351;&#29992;&#65292;&#29305;&#21035;&#26159;&#22312;&#20851;&#38190;&#30340;&#23884;&#20837;&#24335;&#31995;&#32479;&#20013;&#65292;&#23545;&#23427;&#20204;&#30340;&#23433;&#20840;&#35780;&#20272;&#20197;&#20445;&#35777;&#23427;&#20204;&#30340;&#21487;&#38752;&#24615;&#21464;&#24471;&#36843;&#20999;&#12290;&#23588;&#20854;&#26159;&#37096;&#32626;&#22312;32&#20301;&#24494;&#25511;&#21046;&#22120;&#31561;&#23884;&#20837;&#24335;&#24179;&#21488;&#19978;&#30340;&#27169;&#22411;&#21487;&#20197;&#34987;&#23545;&#25163;&#29289;&#29702;&#35775;&#38382;&#65292;&#22240;&#27492;&#23481;&#26131;&#21463;&#21040;&#30828;&#20214;&#24178;&#25200;&#30340;&#25915;&#20987;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#20851;&#20110;&#22312;Cortex M4 32&#20301;&#24494;&#25511;&#21046;&#22120;&#24179;&#21488;&#19978;&#23884;&#20837;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#19978;&#24212;&#29992;&#30005;&#30913;&#21644;&#28608;&#20809;&#27880;&#20837;&#20004;&#31181;&#25925;&#38556;&#27880;&#20837;&#25163;&#27573;&#30340;&#23454;&#39564;&#12290;&#19982;&#22823;&#22810;&#25968;&#33268;&#21147;&#20110;&#25913;&#21464;&#20869;&#37096;&#21442;&#25968;&#25110;&#36755;&#20837;&#20540;&#30340;&#29616;&#26377;&#24037;&#20316;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#27169;&#25311;&#21644;&#23454;&#39564;&#24615;&#22320;&#23637;&#31034;&#19968;&#31181;&#29305;&#23450;&#30340;&#38169;&#35823;&#27169;&#22411;&#8212;&#8212;&#25351;&#20196;&#36339;&#36807;&#30340;&#24433;&#21709;&#12290;&#20026;&#20102;&#36798;&#21040;&#36825;&#20010;&#30446;&#30340;&#65292;&#25105;&#20204;&#23545;&#31070;&#32463;&#32593;&#32476;&#25512;&#29702;&#30340;&#25511;&#21046;&#27969;&#36827;&#34892;&#20102;&#25968;&#31181;&#20462;&#25913;&#25915;&#20987;&#30340;&#35780;&#20272;&#12290;&#25105;&#20204;&#25581;&#31034;&#20102;&#36890;&#36807;&#38024;&#23545;&#25512;&#29702;&#36807;&#31243;&#30340;&#22810;&#20010;&#27493;&#39588;&#30340;&#25915;&#20987;&#26469;&#23041;&#32961;&#20854;&#23436;&#25972;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the large-scale integration and use of neural network models, especially in critical embedded systems, their security assessment to guarantee their reliability is becoming an urgent need. More particularly, models deployed in embedded platforms, such as 32-bit microcontrollers, are physically accessible by adversaries and therefore vulnerable to hardware disturbances. We present the first set of experiments on the use of two fault injection means, electromagnetic and laser injections, applied on neural networks models embedded on a Cortex M4 32-bit microcontroller platform. Contrary to most of state-of-the-art works dedicated to the alteration of the internal parameters or input values, our goal is to simulate and experimentally demonstrate the impact of a specific fault model that is instruction skip. For that purpose, we assessed several modification attacks on the control flow of a neural network inference. We reveal integrity threats by targeting several steps in the inference
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20934;&#27979;&#35797;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#30693;&#35782;&#22270;&#35889;&#24037;&#31243;&#20013;&#30340;&#24212;&#29992;&#12290;&#26694;&#26550;&#21253;&#25324;&#35821;&#27861;&#21644;&#38169;&#35823;&#20462;&#27491;&#12289;&#20107;&#23454;&#25552;&#21462;&#21644;&#25968;&#25454;&#38598;&#29983;&#25104;&#19977;&#20010;&#25361;&#25112;&#65292;&#21516;&#26102;&#20063;&#25581;&#31034;&#20102;LLMs&#22312;&#38646;-shot&#25552;&#31034;&#19979;&#36741;&#21161;&#30693;&#35782;&#22270;&#35889;&#29983;&#25104;&#26041;&#38754;&#30340;&#19981;&#36275;&#12290;</title><link>http://arxiv.org/abs/2308.16622</link><description>&lt;p&gt;
&#22312;&#30693;&#35782;&#22270;&#35889;&#24037;&#31243;&#20013;&#24320;&#21457;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Developing a Scalable Benchmark for Assessing Large Language Models in Knowledge Graph Engineering. (arXiv:2308.16622v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16622
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20934;&#27979;&#35797;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#30693;&#35782;&#22270;&#35889;&#24037;&#31243;&#20013;&#30340;&#24212;&#29992;&#12290;&#26694;&#26550;&#21253;&#25324;&#35821;&#27861;&#21644;&#38169;&#35823;&#20462;&#27491;&#12289;&#20107;&#23454;&#25552;&#21462;&#21644;&#25968;&#25454;&#38598;&#29983;&#25104;&#19977;&#20010;&#25361;&#25112;&#65292;&#21516;&#26102;&#20063;&#25581;&#31034;&#20102;LLMs&#22312;&#38646;-shot&#25552;&#31034;&#19979;&#36741;&#21161;&#30693;&#35782;&#22270;&#35889;&#29983;&#25104;&#26041;&#38754;&#30340;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#39046;&#22495;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#35780;&#20272;&#21644;&#30417;&#27979;&#20854;&#24615;&#33021;&#30340;&#36843;&#20999;&#38656;&#27714;&#28014;&#20986;&#27700;&#38754;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#38024;&#23545;&#30693;&#35782;&#22270;&#35889;&#24037;&#31243;&#65288;KGE&#65289;&#30340;&#22522;&#20934;&#27979;&#35797;&#26694;&#26550;&#65292;&#24182;&#25552;&#20986;&#20102;&#19977;&#20010;&#25361;&#25112;&#65292;&#28041;&#21450;&#35821;&#27861;&#21644;&#38169;&#35823;&#20462;&#27491;&#12289;&#20107;&#23454;&#25552;&#21462;&#21644;&#25968;&#25454;&#38598;&#29983;&#25104;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#23613;&#31649;LLMs&#26159;&#26377;&#29992;&#30340;&#24037;&#20855;&#65292;&#20294;&#23427;&#20204;&#23578;&#19981;&#33021;&#22312;&#38646;-shot&#25552;&#31034;&#19979;&#36741;&#21161;&#30693;&#35782;&#22270;&#35889;&#29983;&#25104;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;LLM-KG-Bench&#26694;&#26550;&#25552;&#20379;&#20102;LLM&#22238;&#31572;&#30340;&#33258;&#21160;&#35780;&#20272;&#21644;&#23384;&#20648;&#65292;&#20197;&#21450;&#32479;&#35745;&#25968;&#25454;&#21644;&#21487;&#35270;&#21270;&#24037;&#20855;&#65292;&#25903;&#25345;&#25552;&#31034;&#24037;&#31243;&#21644;&#27169;&#22411;&#24615;&#33021;&#30340;&#36319;&#36394;&#12290;
&lt;/p&gt;
&lt;p&gt;
As the field of Large Language Models (LLMs) evolves at an accelerated pace, the critical need to assess and monitor their performance emerges. We introduce a benchmarking framework focused on knowledge graph engineering (KGE) accompanied by three challenges addressing syntax and error correction, facts extraction and dataset generation. We show that while being a useful tool, LLMs are yet unfit to assist in knowledge graph generation with zero-shot prompting. Consequently, our LLM-KG-Bench framework provides automatic evaluation and storage of LLM responses as well as statistical data and visualization tools to support tracking of prompt engineering and model performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20174;&#31038;&#20132;&#32593;&#32476;&#25991;&#26412;&#20013;&#25552;&#21462;&#20301;&#32622;&#20449;&#24687;&#65292;&#20197;&#26500;&#24314;&#20934;&#30830;&#30340;&#24656;&#24598;&#34989;&#20987;&#39044;&#27979;&#25968;&#25454;&#38598;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#23545;&#20110;&#20301;&#32622;&#35782;&#21035;&#30340;&#20934;&#30830;&#24615;&#36739;&#24046;&#65292;&#32780;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#35745;&#21010;&#36827;&#19968;&#27493;&#25193;&#23637;&#20197;&#25552;&#21462;&#20854;&#20182;&#30456;&#20851;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2308.16615</link><description>&lt;p&gt;
&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20174;&#31038;&#20132;&#32593;&#32476;&#25991;&#26412;&#20013;&#25552;&#21462;&#39640;&#20934;&#30830;&#24230;&#30340;&#20301;&#32622;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
High Accuracy Location Information Extraction from Social Network Texts Using Natural Language Processing. (arXiv:2308.16615v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16615
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20174;&#31038;&#20132;&#32593;&#32476;&#25991;&#26412;&#20013;&#25552;&#21462;&#20301;&#32622;&#20449;&#24687;&#65292;&#20197;&#26500;&#24314;&#20934;&#30830;&#30340;&#24656;&#24598;&#34989;&#20987;&#39044;&#27979;&#25968;&#25454;&#38598;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#23545;&#20110;&#20301;&#32622;&#35782;&#21035;&#30340;&#20934;&#30830;&#24615;&#36739;&#24046;&#65292;&#32780;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#35745;&#21010;&#36827;&#19968;&#27493;&#25193;&#23637;&#20197;&#25552;&#21462;&#20854;&#20182;&#30456;&#20851;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24656;&#24598;&#20027;&#20041;&#24050;&#32463;&#25104;&#20026;&#20840;&#29699;&#24615;&#30340;&#28798;&#23475;&#65292;&#32473;&#22269;&#23478;&#30340;&#21457;&#23637;&#24102;&#26469;&#20102;&#20005;&#37325;&#21518;&#26524;&#12290;&#38500;&#20102;&#27599;&#22825;&#26432;&#23475;&#26080;&#36764;&#20154;&#27665;&#21644;&#38459;&#27490;&#25945;&#32946;&#27963;&#21160;&#30340;&#36827;&#34892;&#65292;&#24656;&#24598;&#20027;&#20041;&#36824;&#38459;&#30861;&#20102;&#32463;&#27982;&#30340;&#22686;&#38271;&#12290;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#21487;&#20197;&#36890;&#36807;&#39044;&#27979;&#20934;&#30830;&#30340;&#25968;&#25454;&#26469;&#23454;&#26102;&#39044;&#27979;&#26410;&#26469;&#30340;&#24656;&#24598;&#34989;&#20987;&#65292;&#20174;&#32780;&#26377;&#21161;&#20110;&#25171;&#20987;&#24656;&#24598;&#20027;&#20041;&#12290;&#35813;&#35770;&#25991;&#26159;&#19968;&#20010;&#30740;&#31350;&#39033;&#30446;&#30340;&#19968;&#37096;&#20998;&#65292;&#35813;&#39033;&#30446;&#21033;&#29992;&#31038;&#20132;&#32593;&#32476;&#25991;&#26412;&#25552;&#21462;&#24517;&#35201;&#30340;&#20449;&#24687;&#65292;&#20197;&#26500;&#24314;&#36866;&#24403;&#30340;&#24656;&#24598;&#34989;&#20987;&#39044;&#27979;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#25910;&#38598;&#20102;3000&#26465;&#26377;&#20851;&#24067;&#22522;&#32435;&#27861;&#32034;&#24656;&#24598;&#20027;&#20041;&#30340;&#31038;&#20132;&#32593;&#32476;&#25991;&#26412;&#65292;&#24182;&#20351;&#29992;&#20854;&#20013;&#30340;&#19968;&#37096;&#20998;&#26469;&#23581;&#35797;&#29616;&#26377;&#30340;NLP&#35299;&#20915;&#26041;&#26696;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#29616;&#26377;&#30340;&#35299;&#20915;&#26041;&#26696;&#23545;&#20110;&#20301;&#32622;&#35782;&#21035;&#30340;&#20934;&#30830;&#24615;&#36739;&#24046;&#65292;&#32780;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#23558;&#25193;&#23637;&#35813;&#35299;&#20915;&#26041;&#26696;&#20197;&#25552;&#21462;&#26085;&#26399;&#21644;&#34892;&#21160;&#20449;&#24687;&#65292;&#20197;&#23454;&#29616;&#39033;&#30446;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Terrorism has become a worldwide plague with severe consequences for the development of nations. Besides killing innocent people daily and preventing educational activities from taking place, terrorism is also hindering economic growth. Machine Learning (ML) and Natural Language Processing (NLP) can contribute to fighting terrorism by predicting in real-time future terrorist attacks if accurate data is available. This paper is part of a research project that uses text from social networks to extract necessary information to build an adequate dataset for terrorist attack prediction. We collected a set of 3000 social network texts about terrorism in Burkina Faso and used a subset to experiment with existing NLP solutions. The experiment reveals that existing solutions have poor accuracy for location recognition, which our solution resolves. We will extend the solution to extract dates and action information to achieve the project's goal.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21512;&#20316;&#19987;&#23478;&#23454;&#29616;&#20102;&#38271;&#23614;&#22270;&#20998;&#31867;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#22788;&#29702;&#22270;&#25968;&#25454;&#19978;&#30340;&#19981;&#36275;&#12290;</title><link>http://arxiv.org/abs/2308.16609</link><description>&lt;p&gt;
&#36890;&#36807;&#21512;&#20316;&#19987;&#23478;&#23454;&#29616;&#38271;&#23614;&#22270;&#20998;&#31867;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards Long-Tailed Recognition for Graph Classification via Collaborative Experts. (arXiv:2308.16609v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16609
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21512;&#20316;&#19987;&#23478;&#23454;&#29616;&#20102;&#38271;&#23614;&#22270;&#20998;&#31867;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#22788;&#29702;&#22270;&#25968;&#25454;&#19978;&#30340;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20998;&#31867;&#26088;&#22312;&#23398;&#20064;&#29992;&#20110;&#26377;&#25928;&#31867;&#21035;&#20998;&#37197;&#30340;&#22270;&#32423;&#34920;&#31034;&#65292;&#22312;&#24179;&#34913;&#30340;&#31867;&#21035;&#20998;&#24067;&#30340;&#39640;&#36136;&#37327;&#25968;&#25454;&#38598;&#30340;&#25903;&#25345;&#19979;&#21462;&#24471;&#20102;&#26480;&#20986;&#25104;&#26524;&#12290;&#20107;&#23454;&#19978;&#65292;&#22823;&#22810;&#25968;&#29616;&#23454;&#19990;&#30028;&#30340;&#22270;&#25968;&#25454;&#33258;&#28982;&#21576;&#29616;&#38271;&#23614;&#24418;&#24335;&#65292;&#20854;&#20013;&#22836;&#37096;&#31867;&#21035;&#30340;&#26679;&#26412;&#25968;&#37327;&#36828;&#36229;&#36807;&#23614;&#37096;&#31867;&#21035;&#65292;&#22240;&#27492;&#22312;&#38271;&#23614;&#25968;&#25454;&#19978;&#30740;&#31350;&#22270;&#32423;&#20998;&#31867;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#20294;&#20173;&#28982;&#36739;&#23569;&#25506;&#32034;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#35270;&#35273;&#20013;&#30340;&#38271;&#23614;&#23398;&#20064;&#26041;&#27861;&#22823;&#22810;&#26080;&#27861;&#21516;&#26102;&#20248;&#21270;&#34920;&#31034;&#23398;&#20064;&#21644;&#20998;&#31867;&#22120;&#35757;&#32451;&#65292;&#24182;&#19988;&#24573;&#30053;&#20102;&#38590;&#20197;&#20998;&#31867;&#30340;&#31867;&#21035;&#30340;&#25366;&#25496;&#12290;&#30452;&#25509;&#23558;&#29616;&#26377;&#26041;&#27861;&#24212;&#29992;&#20110;&#22270;&#21487;&#33021;&#23548;&#33268;&#27425;&#20248;&#24615;&#33021;&#65292;&#22240;&#20026;&#22312;&#22270;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#30001;&#20110;&#22797;&#26434;&#30340;&#25299;&#25169;&#29305;&#24449;&#20250;&#26356;&#21152;&#25935;&#24863;&#20110;&#38271;&#23614;&#20998;&#24067;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#38271;&#23614;&#22270;&#32423;&#20998;&#31867;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Graph classification, aiming at learning the graph-level representations for effective class assignments, has received outstanding achievements, which heavily relies on high-quality datasets that have balanced class distribution. In fact, most real-world graph data naturally presents a long-tailed form, where the head classes occupy much more samples than the tail classes, it thus is essential to study the graph-level classification over long-tailed data while still remaining largely unexplored. However, most existing long-tailed learning methods in visions fail to jointly optimize the representation learning and classifier training, as well as neglect the mining of the hard-to-classify classes. Directly applying existing methods to graphs may lead to sub-optimal performance, since the model trained on graphs would be more sensitive to the long-tailed distribution due to the complex topological characteristics. Hence, in this paper, we propose a novel long-tailed graph-level classifica
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#33268;&#21147;&#20110;&#23547;&#25214;&#31232;&#30095;&#21452;&#19979;&#38477;&#30340;&#35299;&#27602;&#21058;&#65292;&#36890;&#36807;&#30740;&#31350;&#21644;&#25552;&#20986;&#35299;&#20915;&#26041;&#26696;&#65292;&#20998;&#21035;&#37319;&#29992;l2&#27491;&#21017;&#21270;&#21644;&#30693;&#35782;&#33976;&#39311;&#26469;&#36991;&#20813;&#31232;&#30095;&#21452;&#19979;&#38477;&#29616;&#35937;&#65292;&#20197;&#25214;&#21040;&#24615;&#33021;&#21644;&#31232;&#30095;&#24615;&#30340;&#26368;&#20339;&#24179;&#34913;&#28857;&#12290;</title><link>http://arxiv.org/abs/2308.16596</link><description>&lt;p&gt;
&#23547;&#25214;&#31232;&#30095;&#21452;&#19979;&#38477;&#30340;&#35299;&#27602;&#21058;&#30340;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
The Quest of Finding the Antidote to Sparse Double Descent. (arXiv:2308.16596v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16596
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#33268;&#21147;&#20110;&#23547;&#25214;&#31232;&#30095;&#21452;&#19979;&#38477;&#30340;&#35299;&#27602;&#21058;&#65292;&#36890;&#36807;&#30740;&#31350;&#21644;&#25552;&#20986;&#35299;&#20915;&#26041;&#26696;&#65292;&#20998;&#21035;&#37319;&#29992;l2&#27491;&#21017;&#21270;&#21644;&#30693;&#35782;&#33976;&#39311;&#26469;&#36991;&#20813;&#31232;&#30095;&#21452;&#19979;&#38477;&#29616;&#35937;&#65292;&#20197;&#25214;&#21040;&#24615;&#33021;&#21644;&#31232;&#30095;&#24615;&#30340;&#26368;&#20339;&#24179;&#34913;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33021;&#28304;&#39640;&#25928;&#30340;&#26041;&#26696;&#20013;&#65292;&#25214;&#21040;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#26368;&#20248;&#22823;&#23567;&#38750;&#24120;&#37325;&#35201;&#24182;&#20855;&#26377;&#24191;&#27867;&#24433;&#21709;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#25253;&#21578;&#20102;&#19968;&#31181;&#24847;&#22806;&#29616;&#35937;&#65292;&#31232;&#30095;&#21452;&#19979;&#38477;&#65306;&#38543;&#30528;&#27169;&#22411;&#30340;&#31232;&#30095;&#24615;&#22686;&#21152;&#65292;&#24615;&#33021;&#39318;&#20808;&#21464;&#24046;&#65292;&#28982;&#21518;&#25913;&#21892;&#65292;&#26368;&#21518;&#24694;&#21270;&#12290;&#36825;&#31181;&#38750;&#21333;&#35843;&#34892;&#20026;&#23545;&#20110;&#20445;&#25345;&#39640;&#24615;&#33021;&#30340;&#26368;&#20248;&#27169;&#22411;&#22823;&#23567;&#25552;&#20986;&#20102;&#20005;&#37325;&#30340;&#38382;&#39064;&#65306;&#27169;&#22411;&#38656;&#35201;&#20855;&#26377;&#36275;&#22815;&#30340;&#36229;&#21442;&#25968;&#65292;&#20294;&#22826;&#22810;&#30340;&#21442;&#25968;&#20250;&#28010;&#36153;&#35757;&#32451;&#36164;&#28304;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#39640;&#25928;&#22320;&#25214;&#21040;&#26368;&#20339;&#30340;&#26435;&#34913;&#28857;&#12290;&#26356;&#20855;&#20307;&#22320;&#35828;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#31232;&#30095;&#21452;&#19979;&#38477;&#30340;&#20986;&#29616;&#65292;&#24182;&#25552;&#20986;&#19968;&#20123;&#35299;&#20915;&#26041;&#26696;&#26469;&#36991;&#20813;&#23427;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;l2&#27491;&#21017;&#21270;&#26041;&#27861;&#21487;&#20197;&#24110;&#21161;&#32531;&#35299;&#36825;&#31181;&#29616;&#35937;&#65292;&#20294;&#20250;&#29306;&#29298;&#24615;&#33021;/&#31232;&#30095;&#24615;&#30340;&#25240;&#34935;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#23398;&#20064;&#26041;&#26696;&#65292;&#20854;&#20013;&#33976;&#39311;&#30693;&#35782;&#23545;&#23398;&#29983;&#27169;&#22411;&#36827;&#34892;&#27491;&#21017;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
In energy-efficient schemes, finding the optimal size of deep learning models is very important and has a broad impact. Meanwhile, recent studies have reported an unexpected phenomenon, the sparse double descent: as the model's sparsity increases, the performance first worsens, then improves, and finally deteriorates. Such a non-monotonic behavior raises serious questions about the optimal model's size to maintain high performance: the model needs to be sufficiently over-parametrized, but having too many parameters wastes training resources.  In this paper, we aim to find the best trade-off efficiently. More precisely, we tackle the occurrence of the sparse double descent and present some solutions to avoid it. Firstly, we show that a simple $\ell_2$ regularization method can help to mitigate this phenomenon but sacrifices the performance/sparsity compromise. To overcome this problem, we then introduce a learning scheme in which distilling knowledge regularizes the student model. Suppo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35838;&#31243;&#23398;&#20064;&#30340;&#36974;&#32617;&#33258;&#32534;&#30721;&#22120;&#65288;CL-MAE&#65289;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21487;&#23398;&#20064;&#30340;&#36974;&#32617;&#27169;&#22359;&#65292;&#36890;&#36807;&#26356;&#26032;&#36974;&#32617;&#31574;&#30053;&#26469;&#22686;&#21152;&#33258;&#30417;&#30563;&#37325;&#26500;&#20219;&#21153;&#30340;&#22797;&#26434;&#24615;&#12290;&#36890;&#36807;&#36880;&#28176;&#22686;&#21152;&#20219;&#21153;&#22797;&#26434;&#24615;&#65292;&#27169;&#22411;&#21487;&#20197;&#23398;&#20064;&#26356;&#22797;&#26434;&#21644;&#21487;&#36801;&#31227;&#30340;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2308.16572</link><description>&lt;p&gt;
CL-MAE: &#35838;&#31243;&#23398;&#20064;&#30340;&#36974;&#32617;&#33258;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
CL-MAE: Curriculum-Learned Masked Autoencoders. (arXiv:2308.16572v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16572
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35838;&#31243;&#23398;&#20064;&#30340;&#36974;&#32617;&#33258;&#32534;&#30721;&#22120;&#65288;CL-MAE&#65289;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21487;&#23398;&#20064;&#30340;&#36974;&#32617;&#27169;&#22359;&#65292;&#36890;&#36807;&#26356;&#26032;&#36974;&#32617;&#31574;&#30053;&#26469;&#22686;&#21152;&#33258;&#30417;&#30563;&#37325;&#26500;&#20219;&#21153;&#30340;&#22797;&#26434;&#24615;&#12290;&#36890;&#36807;&#36880;&#28176;&#22686;&#21152;&#20219;&#21153;&#22797;&#26434;&#24615;&#65292;&#27169;&#22411;&#21487;&#20197;&#23398;&#20064;&#26356;&#22797;&#26434;&#21644;&#21487;&#36801;&#31227;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36974;&#32617;&#22270;&#20687;&#24314;&#27169;&#24050;&#34987;&#35777;&#26126;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#39044;&#25991;&#26412;&#20219;&#21153;&#65292;&#29992;&#20110;&#29983;&#25104;&#33021;&#22815;&#26377;&#25928;&#27867;&#21270;&#21040;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#30340;&#40065;&#26834;&#34920;&#31034;&#12290;&#36890;&#24120;&#65292;&#36825;&#31181;&#26041;&#27861;&#28041;&#21450;&#22312;&#36755;&#20837;&#22270;&#20687;&#20013;&#38543;&#26426;&#36974;&#32617;&#34917;&#19969;&#65288;&#26631;&#35760;&#65289;&#65292;&#24182;&#19988;&#36974;&#32617;&#31574;&#30053;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20445;&#25345;&#19981;&#21464;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35838;&#31243;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#26356;&#26032;&#36974;&#32617;&#31574;&#30053;&#20197;&#25345;&#32493;&#22686;&#21152;&#33258;&#30417;&#30563;&#37325;&#26500;&#20219;&#21153;&#30340;&#22797;&#26434;&#24615;&#12290;&#25105;&#20204;&#25512;&#27979;&#65292;&#36890;&#36807;&#36880;&#28176;&#22686;&#21152;&#20219;&#21153;&#22797;&#26434;&#24615;&#65292;&#27169;&#22411;&#21487;&#20197;&#23398;&#20064;&#26356;&#22797;&#26434;&#21644;&#21487;&#36801;&#31227;&#30340;&#34920;&#31034;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#23398;&#20064;&#36974;&#32617;&#27169;&#22359;&#65292;&#20855;&#26377;&#29983;&#25104;&#19981;&#21516;&#22797;&#26434;&#24230;&#36974;&#32617;&#30340;&#33021;&#21147;&#65292;&#24182;&#23558;&#35813;&#27169;&#22359;&#19982;&#36974;&#32617;&#33258;&#32534;&#30721;&#22120;&#65288;MAE&#65289;&#38598;&#25104;&#12290;&#25105;&#20204;&#30340;&#27169;&#22359;&#19982;MAE&#19968;&#21516;&#35757;&#32451;&#65292;&#21516;&#26102;&#35843;&#25972;&#20854;&#34892;&#20026;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20174;MAE&#30340;&#21442;&#19982;&#32773;&#36807;&#28193;&#21040;MAE&#65288;&#20248;&#21270;&#30456;&#21516;&#30340;&#37325;&#26500;&#30446;&#26631;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Masked image modeling has been demonstrated as a powerful pretext task for generating robust representations that can be effectively generalized across multiple downstream tasks. Typically, this approach involves randomly masking patches (tokens) in input images, with the masking strategy remaining unchanged during training. In this paper, we propose a curriculum learning approach that updates the masking strategy to continually increase the complexity of the self-supervised reconstruction task. We conjecture that, by gradually increasing the task complexity, the model can learn more sophisticated and transferable representations. To facilitate this, we introduce a novel learnable masking module that possesses the capability to generate masks of different complexities, and integrate the proposed module into masked autoencoders (MAE). Our module is jointly trained with the MAE, while adjusting its behavior during training, transitioning from a partner to the MAE (optimizing the same rec
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;MEME&#65292;&#22312;&#23545;&#25239;&#24615;&#24694;&#24847;&#36719;&#20214;&#29983;&#25104;&#20013;&#20351;&#29992;&#20102;&#27169;&#22411;&#25552;&#21462;&#21644;&#24694;&#24847;&#36719;&#20214;&#36867;&#36991;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#35813;&#31639;&#27861;&#22312;&#36867;&#36991;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2308.16562</link><description>&lt;p&gt;
MEME&#30340;&#21147;&#37327;&#65306;&#22522;&#20110;&#27169;&#22411;&#30340;&#22686;&#24378;&#23398;&#20064;&#22312;&#23545;&#25239;&#24615;&#24694;&#24847;&#36719;&#20214;&#29983;&#25104;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
The Power of MEME: Adversarial Malware Creation with Model-Based Reinforcement Learning. (arXiv:2308.16562v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16562
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;MEME&#65292;&#22312;&#23545;&#25239;&#24615;&#24694;&#24847;&#36719;&#20214;&#29983;&#25104;&#20013;&#20351;&#29992;&#20102;&#27169;&#22411;&#25552;&#21462;&#21644;&#24694;&#24847;&#36719;&#20214;&#36867;&#36991;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#35813;&#31639;&#27861;&#22312;&#36867;&#36991;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#24694;&#24847;&#36719;&#20214;&#30340;&#22823;&#37327;&#23384;&#22312;&#65292;&#20026;&#20102;&#26816;&#27979;&#24694;&#24847;&#36719;&#20214;&#65292;&#38450;&#24481;&#32773;&#36234;&#26469;&#36234;&#22810;&#22320;&#37319;&#29992;&#33258;&#21160;&#21270;&#21644;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#25915;&#20987;&#65292;&#38656;&#35201;&#27979;&#35797;&#27169;&#22411;&#21644;&#20135;&#21697;&#30340;&#40065;&#26834;&#24615;&#12290;&#21516;&#26102;&#65292;&#25915;&#20987;&#32773;&#20063;&#35797;&#22270;&#33258;&#21160;&#21270;&#24694;&#24847;&#36719;&#20214;&#30340;&#29983;&#25104;&#21644;&#23545;&#25239;&#26432;&#27602;&#36719;&#20214;&#31995;&#32479;&#65292;&#32780;&#38450;&#24481;&#32773;&#21017;&#35797;&#22270;&#27934;&#23519;&#20182;&#20204;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#23558;&#24694;&#24847;&#36719;&#20214;&#36867;&#36991;&#19982;&#27169;&#22411;&#25552;&#21462;&#65288;MEME&#65289;&#25915;&#20987;&#30456;&#32467;&#21512;&#12290;MEME&#21033;&#29992;&#22522;&#20110;&#27169;&#22411;&#30340;&#22686;&#24378;&#23398;&#20064;&#23545;Windows&#21487;&#25191;&#34892;&#20108;&#36827;&#21046;&#26679;&#26412;&#36827;&#34892;&#23545;&#25239;&#24615;&#20462;&#25913;&#65292;&#21516;&#26102;&#35757;&#32451;&#19968;&#20010;&#19982;&#30446;&#26631;&#27169;&#22411;&#39640;&#24230;&#19968;&#33268;&#30340;&#20195;&#29702;&#27169;&#22411;&#20197;&#36827;&#34892;&#36867;&#36991;&#12290;&#20026;&#20102;&#35780;&#20272;&#35813;&#26041;&#27861;&#65292;&#25105;&#20204;&#23558;&#20854;&#19982;&#20004;&#31181;&#26368;&#20808;&#36827;&#30340;&#23545;&#25239;&#24615;&#24694;&#24847;&#36719;&#20214;&#29983;&#25104;&#25915;&#20987;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#65292;&#20351;&#29992;&#19977;&#20010;&#33879;&#21517;&#30340;&#20844;&#24320;&#27169;&#22411;&#21644;&#19968;&#20010;&#26432;&#27602;&#36719;&#20214;&#20135;&#21697;&#20316;&#20026;&#30446;&#26631;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#30456;&#23545;&#20110;&#20808;&#36827;&#26041;&#27861;&#65292;MEME&#22312;&#36867;&#36991;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to the proliferation of malware, defenders are increasingly turning to automation and machine learning as part of the malware detection tool-chain. However, machine learning models are susceptible to adversarial attacks, requiring the testing of model and product robustness. Meanwhile, attackers also seek to automate malware generation and evasion of antivirus systems, and defenders try to gain insight into their methods. This work proposes a new algorithm that combines Malware Evasion and Model Extraction (MEME) attacks. MEME uses model-based reinforcement learning to adversarially modify Windows executable binary samples while simultaneously training a surrogate model with a high agreement with the target model to evade. To evaluate this method, we compare it with two state-of-the-art attacks in adversarial malware creation, using three well-known published models and one antivirus product as targets. Results show that MEME outperforms the state-of-the-art methods in terms of eva
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#24494;&#20998;&#21338;&#24328;&#12289;&#26368;&#20248;&#25511;&#21046;&#21644;&#22522;&#20110;&#33021;&#37327;&#30340;&#27169;&#22411;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;&#33021;&#37327;&#30340;&#28508;&#22312;&#21338;&#24328;&#30340;&#26032;&#30340;&#31471;&#21040;&#31471;&#23398;&#20064;&#24212;&#29992;&#65292;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#21644;&#21487;&#24494;&#20998;&#30340;&#21338;&#24328;&#35770;&#20248;&#21270;&#23618;&#30340;&#32452;&#21512;&#26469;&#25552;&#39640;&#39044;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.16539</link><description>&lt;p&gt;
&#20851;&#20110;&#22810;&#26234;&#33021;&#20307;&#30456;&#20114;&#20316;&#29992;&#30340;&#24494;&#20998;&#21338;&#24328;&#12289;&#26368;&#20248;&#25511;&#21046;&#21644;&#22522;&#20110;&#33021;&#37327;&#30340;&#27169;&#22411;&#20043;&#38388;&#30340;&#32852;&#31995;
&lt;/p&gt;
&lt;p&gt;
On a Connection between Differential Games, Optimal Control, and Energy-based Models for Multi-Agent Interactions. (arXiv:2308.16539v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16539
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#24494;&#20998;&#21338;&#24328;&#12289;&#26368;&#20248;&#25511;&#21046;&#21644;&#22522;&#20110;&#33021;&#37327;&#30340;&#27169;&#22411;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;&#33021;&#37327;&#30340;&#28508;&#22312;&#21338;&#24328;&#30340;&#26032;&#30340;&#31471;&#21040;&#31471;&#23398;&#20064;&#24212;&#29992;&#65292;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#21644;&#21487;&#24494;&#20998;&#30340;&#21338;&#24328;&#35770;&#20248;&#21270;&#23618;&#30340;&#32452;&#21512;&#26469;&#25552;&#39640;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21338;&#24328;&#35770;&#25552;&#20379;&#20102;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#25968;&#23398;&#26694;&#26550;&#65292;&#29992;&#20110;&#24314;&#27169;&#22810;&#26234;&#33021;&#20307;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#26426;&#22120;&#20154;&#24212;&#29992;&#20013;&#65292;&#21338;&#24328;&#35770;&#30340;&#36866;&#29992;&#24615;&#21463;&#21040;&#22810;&#20010;&#25361;&#25112;&#30340;&#38459;&#30861;&#65292;&#27604;&#22914;&#26410;&#30693;&#30340;&#26234;&#33021;&#20307;&#20559;&#22909;&#21644;&#30446;&#26631;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#24494;&#20998;&#21338;&#24328;&#12289;&#26368;&#20248;&#25511;&#21046;&#21644;&#22522;&#20110;&#33021;&#37327;&#30340;&#27169;&#22411;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#29616;&#26377;&#26041;&#27861;&#32479;&#19968;&#21040;&#25105;&#20204;&#25552;&#20986;&#30340;&#22522;&#20110;&#33021;&#37327;&#30340;&#28508;&#22312;&#21338;&#24328;&#30340;&#24418;&#24335;&#21270;&#20013;&#12290;&#22312;&#36825;&#20010;&#24418;&#24335;&#21270;&#30340;&#22522;&#30784;&#19978;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#31471;&#21040;&#31471;&#23398;&#20064;&#24212;&#29992;&#65292;&#23558;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#21338;&#24328;&#21442;&#25968;&#25512;&#26029;&#65292;&#24182;&#36890;&#36807;&#21487;&#24494;&#20998;&#30340;&#21338;&#24328;&#35770;&#20248;&#21270;&#23618;&#20316;&#20026;&#24402;&#32435;&#20559;&#22909;&#12290;&#20351;&#29992;&#27169;&#25311;&#30340;&#31227;&#21160;&#26426;&#22120;&#20154;&#34892;&#20154;&#30456;&#20114;&#20316;&#29992;&#21644;&#30495;&#23454;&#19990;&#30028;&#20013;&#30340;&#33258;&#21160;&#39550;&#39542;&#25968;&#25454;&#30340;&#23454;&#39564;&#35777;&#25454;&#34920;&#26126;&#65292;&#21338;&#24328;&#35770;&#23618;&#25913;&#21892;&#20102;&#21508;&#31181;&#31070;&#32463;&#32593;&#32476;&#20027;&#24178;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Game theory offers an interpretable mathematical framework for modeling multi-agent interactions. However, its applicability in real-world robotics applications is hindered by several challenges, such as unknown agents' preferences and goals. To address these challenges, we show a connection between differential games, optimal control, and energy-based models and demonstrate how existing approaches can be unified under our proposed Energy-based Potential Game formulation. Building upon this formulation, this work introduces a new end-to-end learning application that combines neural networks for game-parameter inference with a differentiable game-theoretic optimization layer, acting as an inductive bias. The experiments using simulated mobile robot pedestrian interactions and real-world automated driving data provide empirical evidence that the game-theoretic layer improves the predictive performance of various neural network backbones.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#37329;&#34701;&#34892;&#19994;&#20013;&#30340;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#65292;&#35752;&#35770;&#20102;&#20854;&#38761;&#21629;&#28508;&#21147;&#21644;&#30456;&#20851;&#25361;&#25112;&#12290;&#20154;&#24037;&#26234;&#33021;&#30340;&#24212;&#29992;&#33539;&#22260;&#24191;&#27867;&#65292;&#21253;&#25324;&#25913;&#21892;&#23458;&#25143;&#26381;&#21153;&#12289;&#27450;&#35784;&#26816;&#27979;&#12289;&#39118;&#38505;&#31649;&#29702;&#12289;&#20449;&#29992;&#35780;&#20272;&#21644;&#39640;&#39057;&#20132;&#26131;&#12290;&#28982;&#32780;&#65292;&#20154;&#24037;&#26234;&#33021;&#30340;&#24212;&#29992;&#20063;&#24102;&#26469;&#20102;&#36879;&#26126;&#24230;&#12289;&#21487;&#35299;&#37322;&#24615;&#12289;&#20844;&#24179;&#24615;&#12289;&#38382;&#36131;&#21046;&#21644;&#20449;&#20219;&#24230;&#31561;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.16538</link><description>&lt;p&gt;
AI&#38761;&#21629;&#65306;&#37329;&#34701;&#34892;&#19994;&#30340;&#26426;&#36935;&#19982;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
The AI Revolution: Opportunities and Challenges for the Finance Sector. (arXiv:2308.16538v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16538
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#37329;&#34701;&#34892;&#19994;&#20013;&#30340;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#65292;&#35752;&#35770;&#20102;&#20854;&#38761;&#21629;&#28508;&#21147;&#21644;&#30456;&#20851;&#25361;&#25112;&#12290;&#20154;&#24037;&#26234;&#33021;&#30340;&#24212;&#29992;&#33539;&#22260;&#24191;&#27867;&#65292;&#21253;&#25324;&#25913;&#21892;&#23458;&#25143;&#26381;&#21153;&#12289;&#27450;&#35784;&#26816;&#27979;&#12289;&#39118;&#38505;&#31649;&#29702;&#12289;&#20449;&#29992;&#35780;&#20272;&#21644;&#39640;&#39057;&#20132;&#26131;&#12290;&#28982;&#32780;&#65292;&#20154;&#24037;&#26234;&#33021;&#30340;&#24212;&#29992;&#20063;&#24102;&#26469;&#20102;&#36879;&#26126;&#24230;&#12289;&#21487;&#35299;&#37322;&#24615;&#12289;&#20844;&#24179;&#24615;&#12289;&#38382;&#36131;&#21046;&#21644;&#20449;&#20219;&#24230;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25253;&#21578;&#25506;&#35752;&#20102;&#20154;&#24037;&#26234;&#33021;&#22312;&#37329;&#34701;&#39046;&#22495;&#30340;&#24212;&#29992;&#65292;&#27010;&#36848;&#20102;&#20854;&#38761;&#21629;&#24615;&#28508;&#21147;&#65292;&#24182;&#25351;&#20986;&#20102;&#30456;&#20851;&#25361;&#25112;&#12290;&#23427;&#24378;&#35843;&#20102;&#23545;&#20154;&#24037;&#26234;&#33021;&#30340;&#20840;&#38754;&#20102;&#35299;&#12289;&#20854;&#33021;&#21147;&#20197;&#21450;&#30456;&#20851;&#24433;&#21709;&#30340;&#37325;&#35201;&#24615;&#65292;&#20197;&#20415;&#22312;&#20805;&#20998;&#21033;&#29992;&#28508;&#21147;&#30340;&#21516;&#26102;&#38477;&#20302;&#30456;&#20851;&#39118;&#38505;&#12290;&#20154;&#24037;&#26234;&#33021;&#28508;&#21147;&#30340;&#24212;&#29992;&#33539;&#22260;&#20174;&#22686;&#24378;&#29616;&#26377;&#36816;&#33829;&#21040;&#20026;&#37329;&#34701;&#39046;&#22495;&#25171;&#24320;&#26032;&#30340;&#24212;&#29992;&#36884;&#24452;&#12290;&#20154;&#24037;&#26234;&#33021;&#22312;&#37329;&#34701;&#39046;&#22495;&#30340;&#24212;&#29992;&#27491;&#22312;&#25913;&#21464;&#36825;&#20010;&#34892;&#19994;&#12290;&#20854;&#24212;&#29992;&#39046;&#22495;&#28085;&#30422;&#20102;&#23458;&#25143;&#26381;&#21153;&#22686;&#24378;&#12289;&#27450;&#35784;&#26816;&#27979;&#12289;&#39118;&#38505;&#31649;&#29702;&#12289;&#20449;&#29992;&#35780;&#20272;&#21644;&#39640;&#39057;&#20132;&#26131;&#31561;&#26041;&#38754;&#12290;&#28982;&#32780;&#65292;&#38500;&#20102;&#36825;&#20123;&#22909;&#22788;&#22806;&#65292;&#20154;&#24037;&#26234;&#33021;&#36824;&#24102;&#26469;&#20102;&#20960;&#20010;&#25361;&#25112;&#12290;&#36825;&#20123;&#25361;&#25112;&#21253;&#25324;&#19982;&#36879;&#26126;&#24230;&#12289;&#21487;&#35299;&#37322;&#24615;&#12289;&#20844;&#24179;&#24615;&#12289;&#38382;&#36131;&#21046;&#21644;&#20449;&#20219;&#24230;&#26377;&#20851;&#30340;&#38382;&#39064;&#12290;&#20154;&#24037;&#26234;&#33021;&#22312;&#37329;&#34701;&#39046;&#22495;&#30340;&#24212;&#29992;&#36824;&#24341;&#21457;&#20102;&#20851;&#20110;&#25968;&#25454;&#38544;&#31169;&#21644;&#23433;&#20840;&#24615;&#30340;&#37325;&#35201;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
This report examines Artificial Intelligence (AI) in the financial sector, outlining its potential to revolutionise the industry and identify its challenges. It underscores the criticality of a well-rounded understanding of AI, its capabilities, and its implications to effectively leverage its potential while mitigating associated risks. The potential of AI potential extends from augmenting existing operations to paving the way for novel applications in the finance sector. The application of AI in the financial sector is transforming the industry. Its use spans areas from customer service enhancements, fraud detection, and risk management to credit assessments and high-frequency trading. However, along with these benefits, AI also presents several challenges. These include issues related to transparency, interpretability, fairness, accountability, and trustworthiness. The use of AI in the financial sector further raises critical questions about data privacy and security. A further issu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#31070;&#32463;&#31526;&#21495;&#32422;&#26463;&#26469;&#35843;&#33410;&#22522;&#20110;&#35780;&#20998;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#22312;&#38750;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#19979;&#24378;&#21046;&#25191;&#34892;&#20219;&#24847;&#30340;&#36923;&#36753;&#32422;&#26463;&#65292;&#20174;&#32780;&#33719;&#24471;&#20102;&#19968;&#20010;&#26377;&#25928;&#30340;&#12289;&#26080;&#38656;&#39069;&#22806;&#35757;&#32451;&#30340;&#26465;&#20214;&#37319;&#26679;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.16534</link><description>&lt;p&gt;
&#36890;&#36807;&#31070;&#32463;&#31526;&#21495;&#32422;&#26463;&#26469;&#35843;&#33410;&#22522;&#20110;&#35780;&#20998;&#30340;&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Conditioning Score-Based Generative Models by Neuro-Symbolic Constraints. (arXiv:2308.16534v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16534
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#31070;&#32463;&#31526;&#21495;&#32422;&#26463;&#26469;&#35843;&#33410;&#22522;&#20110;&#35780;&#20998;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#22312;&#38750;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#19979;&#24378;&#21046;&#25191;&#34892;&#20219;&#24847;&#30340;&#36923;&#36753;&#32422;&#26463;&#65292;&#20174;&#32780;&#33719;&#24471;&#20102;&#19968;&#20010;&#26377;&#25928;&#30340;&#12289;&#26080;&#38656;&#39069;&#22806;&#35757;&#32451;&#30340;&#26465;&#20214;&#37319;&#26679;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#35780;&#20998;&#21644;&#25193;&#25955;&#27169;&#22411;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#26377;&#25928;&#30340;&#26465;&#20214;&#21644;&#38750;&#26465;&#20214;&#29983;&#25104;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#26465;&#20214;&#29983;&#25104;&#22522;&#20110;&#29305;&#23450;&#35757;&#32451;&#30340;&#26465;&#20214;&#27169;&#22411;&#25110;&#20998;&#31867;&#22120;&#25351;&#23548;&#65292;&#36825;&#38656;&#35201;&#35757;&#32451;&#19968;&#20010;&#22122;&#22768;&#20381;&#36182;&#30340;&#20998;&#31867;&#22120;&#65292;&#21363;&#20351;&#23545;&#20110;&#26410;&#25439;&#22351;&#25968;&#25454;&#30340;&#20998;&#31867;&#22120;&#24050;&#32463;&#32473;&#20986;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#38750;&#26465;&#20214;&#35780;&#20998;&#29983;&#25104;&#27169;&#22411;&#20013;&#37319;&#26679;&#65292;&#21487;&#20197;&#24378;&#21046;&#25191;&#34892;&#20219;&#24847;&#30340;&#36923;&#36753;&#32422;&#26463;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#39069;&#22806;&#30340;&#35757;&#32451;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#25805;&#32437;&#23398;&#20064;&#24471;&#21040;&#30340;&#35780;&#20998;&#65292;&#20197;&#20415;&#22312;&#29992;&#25143;&#23450;&#20041;&#30340;&#32422;&#26463;&#26465;&#20214;&#19979;&#20174;&#38750;&#24402;&#19968;&#21270;&#20998;&#24067;&#20013;&#37319;&#26679;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#20010;&#28789;&#27963;&#32780;&#25968;&#20540;&#31283;&#23450;&#30340;&#31070;&#32463;&#31526;&#21495;&#26694;&#26550;&#65292;&#29992;&#20110;&#32534;&#30721;&#36719;&#36923;&#36753;&#32422;&#26463;&#12290;&#23558;&#36825;&#20004;&#20010;&#32452;&#25104;&#37096;&#20998;&#32467;&#21512;&#36215;&#26469;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#19968;&#20010;&#19968;&#33324;&#30340;&#20294;&#26159;&#36817;&#20284;&#30340;&#26465;&#20214;&#37319;&#26679;&#31639;&#27861;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#24320;&#21457;&#20102;&#26377;&#25928;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#26469;&#25913;&#36827;&#36817;&#20284;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Score-based and diffusion models have emerged as effective approaches for both conditional and unconditional generation. Still conditional generation is based on either a specific training of a conditional model or classifier guidance, which requires training a noise-dependent classifier, even when the classifier for uncorrupted data is given. We propose an approach to sample from unconditional score-based generative models enforcing arbitrary logical constraints, without any additional training. Firstly, we show how to manipulate the learned score in order to sample from an un-normalized distribution conditional on a user-defined constraint. Then, we define a flexible and numerically stable neuro-symbolic framework for encoding soft logical constraints. Combining these two ingredients we obtain a general, but approximate, conditional sampling algorithm. We further developed effective heuristics aimed at improving the approximation. Finally, we show the effectiveness of our approach fo
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#25972;&#21512;&#38750;&#35821;&#35328;&#26263;&#31034;&#22686;&#24378;&#31038;&#20132;&#26426;&#22120;&#20154;&#30340;&#20849;&#24773;&#33021;&#21147;&#65292;&#22312;&#31038;&#20132;&#26426;&#22120;&#20154;&#20013;&#35774;&#35745;&#21644;&#26631;&#35760;&#20102;&#22235;&#31181;&#20849;&#24773;&#38750;&#35821;&#35328;&#26263;&#31034;&#65288;SAFE&#65289;&#65292;&#24182;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#36825;&#20123;&#26263;&#31034;&#12290;&#35813;&#30740;&#31350;&#30340;&#37325;&#35201;&#21457;&#29616;&#26159;&#22312;&#26426;&#22120;&#20154;&#30340;&#22238;&#24212;&#20013;&#35266;&#23519;&#21040;&#26126;&#26174;&#30340;&#27169;&#24335;&#65292;&#22914;&#23545;&#24179;&#38745;&#21644;&#31215;&#26497;&#30340;&#31038;&#20132;&#24773;&#24863;&#30340;&#20559;&#22909;&#20197;&#21450;&#39057;&#32321;&#30340;&#28857;&#22836;&#21160;&#20316;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#35813;&#26041;&#27861;&#24050;&#32463;&#23454;&#29616;&#20102;&#19968;&#20010;&#33021;&#22815;&#36827;&#34892;&#19978;&#19979;&#25991;&#24863;&#30693;&#21644;&#26356;&#30495;&#23454;&#20114;&#21160;&#30340;&#31038;&#20132;&#26426;&#22120;&#20154;&#30340;&#24320;&#21457;&#12290;</title><link>http://arxiv.org/abs/2308.16529</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24320;&#21457;&#20855;&#26377;&#20849;&#24773;&#38750;&#35821;&#35328;&#26263;&#31034;&#30340;&#31038;&#20132;&#26426;&#22120;&#20154;
&lt;/p&gt;
&lt;p&gt;
Developing Social Robots with Empathetic Non-Verbal Cues Using Large Language Models. (arXiv:2308.16529v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16529
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#25972;&#21512;&#38750;&#35821;&#35328;&#26263;&#31034;&#22686;&#24378;&#31038;&#20132;&#26426;&#22120;&#20154;&#30340;&#20849;&#24773;&#33021;&#21147;&#65292;&#22312;&#31038;&#20132;&#26426;&#22120;&#20154;&#20013;&#35774;&#35745;&#21644;&#26631;&#35760;&#20102;&#22235;&#31181;&#20849;&#24773;&#38750;&#35821;&#35328;&#26263;&#31034;&#65288;SAFE&#65289;&#65292;&#24182;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#36825;&#20123;&#26263;&#31034;&#12290;&#35813;&#30740;&#31350;&#30340;&#37325;&#35201;&#21457;&#29616;&#26159;&#22312;&#26426;&#22120;&#20154;&#30340;&#22238;&#24212;&#20013;&#35266;&#23519;&#21040;&#26126;&#26174;&#30340;&#27169;&#24335;&#65292;&#22914;&#23545;&#24179;&#38745;&#21644;&#31215;&#26497;&#30340;&#31038;&#20132;&#24773;&#24863;&#30340;&#20559;&#22909;&#20197;&#21450;&#39057;&#32321;&#30340;&#28857;&#22836;&#21160;&#20316;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#35813;&#26041;&#27861;&#24050;&#32463;&#23454;&#29616;&#20102;&#19968;&#20010;&#33021;&#22815;&#36827;&#34892;&#19978;&#19979;&#25991;&#24863;&#30693;&#21644;&#26356;&#30495;&#23454;&#20114;&#21160;&#30340;&#31038;&#20132;&#26426;&#22120;&#20154;&#30340;&#24320;&#21457;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#25972;&#21512;&#38750;&#35821;&#35328;&#26263;&#31034;&#26469;&#22686;&#24378;&#31038;&#20132;&#26426;&#22120;&#20154;&#30340;&#20849;&#24773;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#35774;&#35745;&#21644;&#26631;&#35760;&#20102;&#22235;&#31181;&#31867;&#22411;&#30340;&#20849;&#24773;&#38750;&#35821;&#35328;&#26263;&#31034;&#65292;&#31616;&#31216;&#20026;SAFE&#65306;&#35328;&#35821;&#12289;&#21160;&#20316;&#65288;&#25163;&#21183;&#65289;&#12289;&#38754;&#37096;&#34920;&#24773;&#21644;&#24773;&#24863;&#12290;&#36825;&#20123;&#26263;&#31034;&#26159;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#12290;&#25105;&#20204;&#20026;&#26426;&#22120;&#20154;&#24320;&#21457;&#20102;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#23545;&#35805;&#31995;&#32479;&#65292;&#24182;&#35780;&#20272;&#20854;&#19982;&#20154;&#31867;&#36741;&#23548;&#21592;&#23450;&#20041;&#30340;&#31038;&#20132;&#26263;&#31034;&#30340;&#19968;&#33268;&#24615;&#12290;&#21021;&#27493;&#32467;&#26524;&#26174;&#31034;&#26426;&#22120;&#20154;&#30340;&#22238;&#24212;&#20855;&#26377;&#26126;&#26174;&#30340;&#27169;&#24335;&#65292;&#27604;&#22914;&#23545;&#24179;&#38745;&#21644;&#31215;&#26497;&#30340;&#31038;&#20132;&#24773;&#24863;&#65288;&#22914;&#8220;&#21916;&#24742;&#8221;&#21644;&#8220;&#27963;&#21147;&#8221;&#65289;&#30340;&#20559;&#22909;&#65292;&#20197;&#21450;&#39057;&#32321;&#30340;&#28857;&#22836;&#21160;&#20316;&#12290;&#23613;&#31649;&#23384;&#22312;&#36825;&#20123;&#20542;&#21521;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#24050;&#32463;&#23454;&#29616;&#20102;&#19968;&#20010;&#33021;&#22815;&#36827;&#34892;&#19978;&#19979;&#25991;&#24863;&#30693;&#21644;&#26356;&#30495;&#23454;&#20114;&#21160;&#30340;&#31038;&#20132;&#26426;&#22120;&#20154;&#30340;&#24320;&#21457;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#20026;&#26410;&#26469;&#30340;&#20154;&#26426;&#20132;&#20114;&#30740;&#31350;&#22880;&#23450;&#20102;&#22522;&#30784;&#65292;&#24378;&#35843;&#20102;&#35821;&#35328;&#21644;&#38750;&#35821;&#35328;&#26263;&#31034;&#22312;&#21019;&#24314;&#31038;&#20132;&#21644;&#20849;&#24773;&#26426;&#22120;&#20154;&#20013;&#30340;&#37325;&#35201;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose augmenting the empathetic capacities of social robots by integrating non-verbal cues. Our primary contribution is the design and labeling of four types of empathetic non-verbal cues, abbreviated as SAFE: Speech, Action (gesture), Facial expression, and Emotion, in a social robot. These cues are generated using a Large Language Model (LLM). We developed an LLM-based conversational system for the robot and assessed its alignment with social cues as defined by human counselors. Preliminary results show distinct patterns in the robot's responses, such as a preference for calm and positive social emotions like 'joy' and 'lively', and frequent nodding gestures. Despite these tendencies, our approach has led to the development of a social robot capable of context-aware and more authentic interactions. Our work lays the groundwork for future studies on human-robot interactions, emphasizing the essential role of both verbal and non-verbal cues in creating social and empathetic robots
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CurvPool&#30340;&#27719;&#32858;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#22270;&#30340;&#26354;&#29575;&#27010;&#24565;&#26469;&#35299;&#20915;&#36807;&#24230;&#24179;&#28369;&#21644;&#36807;&#24230;&#21387;&#32553;&#30340;&#38382;&#39064;&#12290;&#23427;&#33021;&#22815;&#26681;&#25454;&#26354;&#29575;&#33258;&#36866;&#24212;&#22320;&#35782;&#21035;&#36127;&#36131;&#36825;&#20004;&#31181;&#29616;&#35937;&#30340;&#32467;&#26500;&#65292;&#24182;&#26500;&#24314;&#20855;&#26377;&#26356;&#21512;&#36866;&#32467;&#26500;&#30340;&#22270;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#28145;&#23618;&#27169;&#22411;&#21644;&#36828;&#36317;&#31163;&#20449;&#24687;&#30340;&#32467;&#21512;&#12290;</title><link>http://arxiv.org/abs/2308.16516</link><description>&lt;p&gt;
&#22522;&#20110;&#26354;&#29575;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#27719;&#32858;&#26041;&#27861;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Curvature-based Pooling within Graph Neural Networks. (arXiv:2308.16516v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16516
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CurvPool&#30340;&#27719;&#32858;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#22270;&#30340;&#26354;&#29575;&#27010;&#24565;&#26469;&#35299;&#20915;&#36807;&#24230;&#24179;&#28369;&#21644;&#36807;&#24230;&#21387;&#32553;&#30340;&#38382;&#39064;&#12290;&#23427;&#33021;&#22815;&#26681;&#25454;&#26354;&#29575;&#33258;&#36866;&#24212;&#22320;&#35782;&#21035;&#36127;&#36131;&#36825;&#20004;&#31181;&#29616;&#35937;&#30340;&#32467;&#26500;&#65292;&#24182;&#26500;&#24314;&#20855;&#26377;&#26356;&#21512;&#36866;&#32467;&#26500;&#30340;&#22270;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#28145;&#23618;&#27169;&#22411;&#21644;&#36828;&#36317;&#31163;&#20449;&#24687;&#30340;&#32467;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#24230;&#21387;&#32553;&#21644;&#36807;&#24230;&#24179;&#28369;&#26159;&#38480;&#21046;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#33021;&#21147;&#30340;&#20004;&#20010;&#20851;&#38190;&#38382;&#39064;&#12290;&#36807;&#24230;&#24179;&#28369;&#28040;&#38500;&#20102;&#33410;&#28857;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#20351;&#23427;&#20204;&#38590;&#20197;&#21306;&#20998;&#65292;&#32780;&#36807;&#24230;&#21387;&#32553;&#25351;&#30340;&#26159;GNN&#26080;&#27861;&#22312;&#36739;&#38271;&#30340;&#36317;&#31163;&#19978;&#20256;&#25773;&#20449;&#24687;&#65292;&#22240;&#20026;&#25351;&#25968;&#32423;&#30340;&#33410;&#28857;&#29366;&#24577;&#34987;&#21387;&#32553;&#25104;&#22266;&#23450;&#22823;&#23567;&#30340;&#34920;&#31034;&#12290;&#36825;&#20004;&#31181;&#29616;&#35937;&#20855;&#26377;&#31867;&#20284;&#30340;&#21407;&#22240;&#65292;&#37117;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#26159;&#30001;&#22270;&#25299;&#25169;&#24341;&#36215;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#22312;&#22270;&#20998;&#31867;&#20219;&#21153;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27719;&#32858;&#26041;&#27861;CurvPool&#12290;CurvPool&#21033;&#29992;&#22270;&#30340;&#26354;&#29575;&#27010;&#24565;&#33258;&#36866;&#24212;&#22320;&#35782;&#21035;&#36127;&#36131;&#36807;&#24230;&#24179;&#28369;&#21644;&#36807;&#24230;&#21387;&#32553;&#30340;&#32467;&#26500;&#12290;&#36890;&#36807;&#22522;&#20110;&#24179;&#34913;Forman&#26354;&#29575;&#23545;&#33410;&#28857;&#36827;&#34892;&#32858;&#31867;&#65292;CurvPool&#26500;&#24314;&#20102;&#19968;&#20010;&#20855;&#26377;&#26356;&#21512;&#36866;&#32467;&#26500;&#30340;&#22270;&#65292;&#20801;&#35768;&#28145;&#23618;&#27169;&#22411;&#21644;&#36828;&#36317;&#31163;&#20449;&#24687;&#30340;&#32467;&#21512;&#12290;&#25105;&#20204;&#23558;&#20854;&#19982;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#27719;&#32858;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#65292;&#24182;&#30830;&#23450;&#20854;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Over-squashing and over-smoothing are two critical issues, that limit the capabilities of graph neural networks (GNNs). While over-smoothing eliminates the differences between nodes making them indistinguishable, over-squashing refers to the inability of GNNs to propagate information over long distances, as exponentially many node states are squashed into fixed-size representations. Both phenomena share similar causes, as both are largely induced by the graph topology. To mitigate these problems in graph classification tasks, we propose CurvPool, a novel pooling method. CurvPool exploits the notion of curvature of a graph to adaptively identify structures responsible for both over-smoothing and over-squashing. By clustering nodes based on the Balanced Forman curvature, CurvPool constructs a graph with a more suitable structure, allowing deeper models and the combination of distant information. We compare it to other state-of-the-art pooling approaches and establish its competitiveness 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30340;&#21019;&#26032;&#28857;&#26159;&#23558;&#25512;&#33616;&#27169;&#22411;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#34701;&#21512;&#65292;&#21019;&#24314;&#20102;&#19968;&#20010;&#22810;&#21151;&#33021;&#20132;&#20114;&#24335;&#25512;&#33616;&#31995;&#32479;&#65292;&#35299;&#20915;&#20102;&#25512;&#33616;&#27169;&#22411;&#22312;&#25552;&#20379;&#35299;&#37322;&#21644;&#21442;&#19982;&#23545;&#35805;&#20219;&#21153;&#26041;&#38754;&#30340;&#22256;&#38590;&#12290;</title><link>http://arxiv.org/abs/2308.16505</link><description>&lt;p&gt;
&#25512;&#33616;AI&#20195;&#29702;&#65306;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25972;&#21512;&#21040;&#20132;&#20114;&#24335;&#25512;&#33616;&#20013;
&lt;/p&gt;
&lt;p&gt;
Recommender AI Agent: Integrating Large Language Models for Interactive Recommendations. (arXiv:2308.16505v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16505
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30340;&#21019;&#26032;&#28857;&#26159;&#23558;&#25512;&#33616;&#27169;&#22411;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#34701;&#21512;&#65292;&#21019;&#24314;&#20102;&#19968;&#20010;&#22810;&#21151;&#33021;&#20132;&#20114;&#24335;&#25512;&#33616;&#31995;&#32479;&#65292;&#35299;&#20915;&#20102;&#25512;&#33616;&#27169;&#22411;&#22312;&#25552;&#20379;&#35299;&#37322;&#21644;&#21442;&#19982;&#23545;&#35805;&#20219;&#21153;&#26041;&#38754;&#30340;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#27169;&#22411;&#36890;&#36807;&#21033;&#29992;&#24191;&#27867;&#30340;&#29992;&#25143;&#34892;&#20026;&#25968;&#25454;&#26469;&#25552;&#20379;&#39046;&#22495;&#29305;&#23450;&#30340;&#29289;&#21697;&#25512;&#33616;&#65292;&#23637;&#29616;&#20986;&#36731;&#37327;&#32423;&#39046;&#22495;&#19987;&#23478;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#25552;&#20379;&#35299;&#37322;&#21644;&#21442;&#19982;&#23545;&#35805;&#31561;&#22810;&#26679;&#21270;&#20219;&#21153;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20195;&#34920;&#20102;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#30340;&#37325;&#35201;&#36827;&#23637;&#65292;&#22312;&#25351;&#20196;&#29702;&#35299;&#12289;&#24120;&#35782;&#25512;&#29702;&#21644;&#20154;&#31867;&#20132;&#20114;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;LLMs&#32570;&#20047;&#39046;&#22495;&#29305;&#23450;&#29289;&#21697;&#30446;&#24405;&#21644;&#34892;&#20026;&#27169;&#24335;&#30340;&#30693;&#35782;&#65292;&#29305;&#21035;&#26159;&#22312;&#19982;&#19968;&#33324;&#19990;&#30028;&#30693;&#35782;&#19981;&#21516;&#30340;&#39046;&#22495;&#65292;&#22914;&#22312;&#32447;&#30005;&#23376;&#21830;&#21153;&#12290;&#20026;&#27599;&#20010;&#39046;&#22495;&#24494;&#35843;LLMs&#26082;&#19981;&#32463;&#27982;&#21448;&#19981;&#39640;&#25928;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#25512;&#33616;&#27169;&#22411;&#21644;LLMs&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#32467;&#21512;&#21508;&#33258;&#30340;&#20248;&#21183;&#65292;&#21019;&#24314;&#20102;&#19968;&#20010;&#22810;&#21151;&#33021;&#20132;&#20114;&#24335;&#25512;&#33616;&#31995;&#32479;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;&#26694;&#26550;&#31216;&#20026;RecAgent&#65292;&#35813;&#26694;&#26550;&#20351;&#29992;LLMs
&lt;/p&gt;
&lt;p&gt;
Recommender models excel at providing domain-specific item recommendations by leveraging extensive user behavior data. Despite their ability to act as lightweight domain experts, they struggle to perform versatile tasks such as providing explanations and engaging in conversations. On the other hand, large language models (LLMs) represent a significant step towards artificial general intelligence, showcasing remarkable capabilities in instruction comprehension, commonsense reasoning, and human interaction. However, LLMs lack the knowledge of domain-specific item catalogs and behavioral patterns, particularly in areas that diverge from general world knowledge, such as online e-commerce. Finetuning LLMs for each domain is neither economic nor efficient.  In this paper, we bridge the gap between recommender models and LLMs, combining their respective strengths to create a versatile and interactive recommender system. We introduce an efficient framework called RecAgent, which employs LLMs a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20851;&#27880;&#29289;&#27969;&#20844;&#21496;&#38388;&#33258;&#21160;&#20132;&#26131;&#35746;&#21333;&#20197;&#20248;&#21270;&#24635;&#25910;&#20837;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#26234;&#33021;&#20307;&#26041;&#27861;&#26469;&#35299;&#20915;&#21512;&#20316;&#36710;&#36742;&#36335;&#24452;&#35268;&#21010;&#38382;&#39064;&#65288;CVRP&#65289;&#65292;&#36890;&#36807;&#20419;&#36827;&#31454;&#20105;&#29289;&#27969;&#20195;&#29702;&#20043;&#38388;&#30340;&#21512;&#20316;&#65292;&#23454;&#29616;&#20943;&#23569;&#34892;&#39542;&#36317;&#31163;&#12289;&#25552;&#39640;&#36816;&#33829;&#25928;&#29575;&#65292;&#24182;&#30830;&#20445;&#20010;&#20307;&#21512;&#29702;&#24615;&#21644;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2308.16501</link><description>&lt;p&gt;
&#20010;&#20307;&#21512;&#29702;&#30340;&#21327;&#20316;&#36710;&#36742;&#36335;&#24452;&#35268;&#21010;&#36890;&#36807;&#32473;&#20104;&#21644;&#25509;&#21463;&#20132;&#25442;
&lt;/p&gt;
&lt;p&gt;
Individually Rational Collaborative Vehicle Routing through Give-And-Take Exchanges. (arXiv:2308.16501v1 [cs.MA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16501
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20851;&#27880;&#29289;&#27969;&#20844;&#21496;&#38388;&#33258;&#21160;&#20132;&#26131;&#35746;&#21333;&#20197;&#20248;&#21270;&#24635;&#25910;&#20837;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#26234;&#33021;&#20307;&#26041;&#27861;&#26469;&#35299;&#20915;&#21512;&#20316;&#36710;&#36742;&#36335;&#24452;&#35268;&#21010;&#38382;&#39064;&#65288;CVRP&#65289;&#65292;&#36890;&#36807;&#20419;&#36827;&#31454;&#20105;&#29289;&#27969;&#20195;&#29702;&#20043;&#38388;&#30340;&#21512;&#20316;&#65292;&#23454;&#29616;&#20943;&#23569;&#34892;&#39542;&#36317;&#31163;&#12289;&#25552;&#39640;&#36816;&#33829;&#25928;&#29575;&#65292;&#24182;&#30830;&#20445;&#20010;&#20307;&#21512;&#29702;&#24615;&#21644;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20851;&#27880;&#29289;&#27969;&#20844;&#21496;&#20043;&#38388;&#22312;&#24066;&#22330;&#24179;&#21488;&#19978;&#33258;&#21160;&#20132;&#25442;&#35746;&#21333;&#20197;&#20248;&#21270;&#24635;&#25910;&#20837;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#26234;&#33021;&#20307;&#26041;&#27861;&#65292;&#30528;&#37325;&#35299;&#20915;&#21512;&#20316;&#36710;&#36742;&#36335;&#24452;&#35268;&#21010;&#38382;&#39064;&#65288;CVRP&#65289;&#24182;&#32771;&#34385;&#20010;&#20307;&#21512;&#29702;&#24615;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#23558;&#36710;&#36742;&#36335;&#24452;&#35268;&#21010;&#38382;&#39064;&#65288;VRP&#65289;&#30340;&#21407;&#21017;&#24212;&#29992;&#20110;&#26469;&#33258;&#19981;&#21516;&#29289;&#27969;&#20844;&#21496;&#30340;&#36710;&#36742;&#23545;&#65292;&#20248;&#21270;&#24635;&#20307;&#36335;&#32447;&#21516;&#26102;&#32771;&#34385;&#20102;&#26631;&#20934;&#30340;VRP&#32422;&#26463;&#21644;&#20010;&#20307;&#21512;&#29702;&#24615;&#32422;&#26463;&#12290;&#36890;&#36807;&#20419;&#36827;&#31454;&#20105;&#29289;&#27969;&#20195;&#29702;&#20043;&#38388;&#30340;&#21512;&#20316;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#21487;&#20197;&#31995;&#32479;&#24615;&#22320;&#20943;&#23569;&#34892;&#39542;&#36317;&#31163;&#24182;&#25552;&#39640;&#36816;&#33829;&#25928;&#29575;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#30830;&#20445;&#20102;&#20010;&#20307;&#21512;&#29702;&#24615;&#21644;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#36825;&#26159;&#30830;&#20445;&#24066;&#22330;&#24179;&#21488;&#30340;&#38271;&#26399;&#21487;&#25345;&#32493;&#24615;&#30340;&#37325;&#35201;&#29305;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we are concerned with the automated exchange of orders between logistics companies in a marketplace platform to optimize total revenues. We introduce a novel multi-agent approach to this problem, focusing on the Collaborative Vehicle Routing Problem (CVRP) through the lens of individual rationality. Our proposed algorithm applies the principles of Vehicle Routing Problem (VRP) to pairs of vehicles from different logistics companies, optimizing the overall routes while considering standard VRP constraints plus individual rationality constraints. By facilitating cooperation among competing logistics agents through a Give-and-Take approach, we show that it is possible to reduce travel distance and increase operational efficiency system-wide. More importantly, our approach ensures individual rationality and faster convergence, which are important properties of ensuring the long-term sustainability of the marketplace platform. We demonstrate the efficacy of our approach throu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#24191;&#20041;Winograd Schema&#22312;&#19978;&#19979;&#25991;&#24615;&#26041;&#38754;&#30340;&#24212;&#29992;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#37327;&#23376;&#29289;&#29702;&#23454;&#39564;&#27169;&#22411;&#26469;&#35299;&#20915;Winograd&#27169;&#24335;&#25361;&#25112;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.16498</link><description>&lt;p&gt;
&#24191;&#20041;Winograd Schema&#21450;&#20854;&#19978;&#19979;&#25991;&#24615;
&lt;/p&gt;
&lt;p&gt;
Generalised Winograd Schema and its Contextuality. (arXiv:2308.16498v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16498
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24191;&#20041;Winograd Schema&#22312;&#19978;&#19979;&#25991;&#24615;&#26041;&#38754;&#30340;&#24212;&#29992;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#37327;&#23376;&#29289;&#29702;&#23454;&#39564;&#27169;&#22411;&#26469;&#35299;&#20915;Winograd&#27169;&#24335;&#25361;&#25112;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#20013;&#30340;&#27495;&#20041;&#20250;&#24341;&#36215;&#23545;&#35299;&#37322;&#30340;&#27010;&#29575;&#20998;&#24067;&#12290;&#36825;&#20123;&#20998;&#24067;&#36890;&#24120;&#28041;&#21450;&#21040;&#22810;&#20010;&#27169;&#26865;&#20004;&#21487;&#30340;&#35789;&#27719;&#65292;&#36825;&#20351;&#24471;&#23427;&#20204;&#25104;&#20026;&#36866;&#21512;&#37327;&#23376;&#19978;&#19979;&#25991;&#24615;&#25311;&#35774;&#27169;&#22411;&#30340;&#30740;&#31350;&#20027;&#39064;&#12290;&#20197;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#19978;&#19979;&#25991;&#24615;&#30340;&#19981;&#21516;&#23450;&#37327;&#24230;&#37327;&#19982;&#24515;&#29702;&#35821;&#35328;&#23398;&#30740;&#31350;&#20013;&#30340;&#35789;&#20041;&#27495;&#20041;&#26377;&#24456;&#22909;&#30340;&#30456;&#20851;&#24615;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20851;&#27880;&#25351;&#20195;&#30340;&#27495;&#20041;&#65292;&#24182;&#30740;&#31350;&#20102;Winograd&#27169;&#24335;&#25361;&#25112;&#65288;WSC&#65289;&#65292;&#36825;&#26159;Levesque&#22312;2011&#24180;&#25552;&#20986;&#30340;&#29992;&#20110;&#35780;&#20272;&#26426;&#22120;&#26234;&#33021;&#30340;&#27979;&#35797;&#12290;WSC&#21253;&#21547;&#19968;&#31995;&#21015;&#22810;&#39033;&#36873;&#25321;&#38382;&#39064;&#65292;&#38656;&#35201;&#22312;&#25353;&#29031;Winograd&#27169;&#24335;&#26500;&#36896;&#30340;&#21477;&#23376;&#20013;&#28040;&#38500;&#20195;&#35789;&#30340;&#27495;&#20041;&#65292;&#36825;&#23545;&#26426;&#22120;&#26469;&#35828;&#24456;&#38590;&#30830;&#23450;&#27491;&#30830;&#30340;&#20195;&#35789;&#25351;&#21521;&#65292;&#20294;&#23545;&#20154;&#31867;&#29702;&#35299;&#26469;&#35828;&#21364;&#24456;&#30452;&#35266;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31867;&#20284;&#22320;&#23558;Winograd&#27169;&#24335;&#24314;&#27169;&#20026;&#37327;&#23376;&#29289;&#29702;&#23454;&#39564;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ambiguities in natural language give rise to probability distributions over interpretations. The distributions are often over multiple ambiguous words at a time; a multiplicity which makes them a suitable topic for sheaf-theoretic models of quantum contextuality. Previous research showed that different quantitative measures of contextuality correlate well with Psycholinguistic research on lexical ambiguities. In this work, we focus on coreference ambiguities and investigate the Winograd Schema Challenge (WSC), a test proposed by Levesque in 2011 to evaluate the intelligence of machines. The WSC consists of a collection of multiple-choice questions that require disambiguating pronouns in sentences structured according to the Winograd schema, in a way that makes it difficult for machines to determine the correct referents but remains intuitive for human comprehension. In this study, we propose an approach that analogously models the Winograd schema as an experiment in quantum physics. Ho
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#19981;&#21516;&#27169;&#24577;&#30340;&#23884;&#20837;&#31354;&#38388;&#36827;&#34892;&#23545;&#40784;&#65292;&#20351;&#24471;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#22312;&#19981;&#37325;&#26032;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#29702;&#35299;&#21644;&#25512;&#29702;&#39069;&#22806;&#30340;&#27169;&#24577;&#65292;&#24182;&#19988;&#22312;&#20154;&#26426;&#20132;&#20114;&#22330;&#26223;&#20013;&#25913;&#36827;&#26426;&#22120;&#20154;&#30340;&#24863;&#30693;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.16493</link><description>&lt;p&gt;
&#19981;&#37325;&#26032;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#25193;&#23637;&#20923;&#32467;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65306;&#23545;&#25913;&#36827;&#26426;&#22120;&#20154;&#24863;&#30693;&#30340;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Expanding Frozen Vision-Language Models without Retraining: Towards Improved Robot Perception. (arXiv:2308.16493v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16493
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#19981;&#21516;&#27169;&#24577;&#30340;&#23884;&#20837;&#31354;&#38388;&#36827;&#34892;&#23545;&#40784;&#65292;&#20351;&#24471;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#22312;&#19981;&#37325;&#26032;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#29702;&#35299;&#21644;&#25512;&#29702;&#39069;&#22806;&#30340;&#27169;&#24577;&#65292;&#24182;&#19988;&#22312;&#20154;&#26426;&#20132;&#20114;&#22330;&#26223;&#20013;&#25913;&#36827;&#26426;&#22120;&#20154;&#30340;&#24863;&#30693;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;VLM&#65289;&#36890;&#36807;&#23558;&#35270;&#35273;&#34920;&#31034;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#39044;&#35757;&#32451;&#26399;&#38388;&#23398;&#20064;&#30340;&#25277;&#35937;&#25216;&#33021;&#32452;&#21512;&#36215;&#26469;&#65292;&#22312;&#35270;&#35273;&#38382;&#31572;&#21644;&#25512;&#29702;&#20219;&#21153;&#20013;&#23637;&#29616;&#20102;&#24378;&#22823;&#30340;&#33021;&#21147;&#12290;&#35270;&#35273;&#26159;&#26368;&#21463;&#27426;&#36814;&#30340;&#26041;&#24335;&#20043;&#19968;&#65292;&#29992;&#20110;&#22686;&#24378;LLMs&#65292;&#20294;&#20165;&#20165;&#26159;&#22330;&#26223;&#30340;&#19968;&#20010;&#34920;&#31034;&#12290;&#22312;&#20154;&#26426;&#20132;&#20114;&#22330;&#26223;&#20013;&#65292;&#26426;&#22120;&#20154;&#24863;&#30693;&#38656;&#35201;&#23545;&#22330;&#26223;&#36827;&#34892;&#20934;&#30830;&#30340;&#29702;&#35299;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23450;&#20041;&#24182;&#23637;&#31034;&#20102;&#19968;&#31181;&#36890;&#36807;&#30417;&#30563;&#21644;&#23545;&#27604;&#35757;&#32451;&#23558;&#19981;&#21516;&#27169;&#24577;&#65288;&#22312;&#27492;&#24773;&#20917;&#19979;&#26159;&#24815;&#24615;&#27979;&#37327;&#21333;&#20803;&#65288;IMU&#65289;&#25968;&#25454;&#65289;&#30340;&#23884;&#20837;&#31354;&#38388;&#19982;&#35270;&#35273;&#23884;&#20837;&#31354;&#38388;&#23545;&#40784;&#30340;&#26041;&#27861;&#65292;&#20351;&#24471;VLM&#33021;&#22815;&#22312;&#19981;&#37325;&#26032;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#29702;&#35299;&#21644;&#25512;&#29702;&#36825;&#20123;&#39069;&#22806;&#30340;&#27169;&#24577;&#12290;&#25105;&#20204;&#36873;&#25321;&#30452;&#25509;&#25552;&#20379;&#27169;&#22411;IMU&#23884;&#20837;&#65292;&#32780;&#19981;&#26159;&#20351;&#29992;&#21333;&#29420;&#30340;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#27169;&#22411;&#30452;&#25509;&#36755;&#20837;&#25552;&#31034;&#65292;&#20197;&#20415;&#20801;&#35768;&#26597;&#35810;&#12289;&#22270;&#20687;&#21644;IMU&#20043;&#38388;&#30340;&#20219;&#20309;&#38750;&#32447;&#24615;&#20132;&#20114;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision-language models (VLMs) have shown powerful capabilities in visual question answering and reasoning tasks by combining visual representations with the abstract skill set large language models (LLMs) learn during pretraining. Vision, while the most popular modality to augment LLMs with, is only one representation of a scene. In human-robot interaction scenarios, robot perception requires accurate scene understanding by the robot. In this paper, we define and demonstrate a method of aligning the embedding spaces of different modalities (in this case, inertial measurement unit (IMU) data) to the vision embedding space through a combination of supervised and contrastive training, enabling the VLM to understand and reason about these additional modalities without retraining. We opt to give the model IMU embeddings directly over using a separate human activity recognition model that feeds directly into the prompt to allow for any nonlinear interactions between the query, image, and IMU
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25581;&#31034;&#20102;&#35838;&#22530;&#25968;&#25454;&#20998;&#26512;&#22797;&#21046;&#30340;&#21487;&#34892;&#24615;&#65292;&#20197;&#21450;&#36825;&#31181;&#26041;&#27861;&#23545;&#23398;&#29983;&#12289;&#25945;&#32946;&#32773;&#21644;&#31185;&#23398;&#23478;&#30340;&#25104;&#26412;&#19982;&#25910;&#30410;&#12290;&#21516;&#26102;&#65292;&#23398;&#29983;&#23545;&#25968;&#25454;&#30340;&#39044;&#26399;&#19982;&#23454;&#38469;&#24773;&#20917;&#23384;&#22312;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2308.16491</link><description>&lt;p&gt;
&#35838;&#22530;&#25968;&#25454;&#20998;&#26512;&#22797;&#21046;&#65306;&#25945;&#23398;&#29983;&#65292;&#21516;&#26102;&#27979;&#35797;&#31185;&#23398;
&lt;/p&gt;
&lt;p&gt;
In-class Data Analysis Replications: Teaching Students while Testing Science. (arXiv:2308.16491v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16491
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25581;&#31034;&#20102;&#35838;&#22530;&#25968;&#25454;&#20998;&#26512;&#22797;&#21046;&#30340;&#21487;&#34892;&#24615;&#65292;&#20197;&#21450;&#36825;&#31181;&#26041;&#27861;&#23545;&#23398;&#29983;&#12289;&#25945;&#32946;&#32773;&#21644;&#31185;&#23398;&#23478;&#30340;&#25104;&#26412;&#19982;&#25910;&#30410;&#12290;&#21516;&#26102;&#65292;&#23398;&#29983;&#23545;&#25968;&#25454;&#30340;&#39044;&#26399;&#19982;&#23454;&#38469;&#24773;&#20917;&#23384;&#22312;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31185;&#23398;&#27491;&#38754;&#20020;&#21487;&#37325;&#22797;&#24615;&#21361;&#26426;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#25552;&#20986;&#23558;&#25968;&#25454;&#20998;&#26512;&#22797;&#21046;&#32435;&#20837;&#35838;&#22530;&#20316;&#20026;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#28508;&#22312;&#30340;&#22909;&#22788;&#65292;&#30446;&#21069;&#23578;&#19981;&#28165;&#26970;&#36825;&#19968;&#26041;&#27861;&#26159;&#21542;&#21487;&#34892;&#65292;&#22914;&#26524;&#21487;&#34892;&#65292;&#28041;&#21450;&#30340;&#21033;&#30410;&#30456;&#20851;&#32773;-&#23398;&#29983;&#12289;&#25945;&#32946;&#32773;&#21644;&#31185;&#23398;&#23478;-&#24212;&#35813;&#26399;&#26395;&#20160;&#20040;&#12290;&#23398;&#29983;&#33021;&#22815;&#22312;&#35838;&#22530;&#19978;&#36827;&#34892;&#25968;&#25454;&#20998;&#26512;&#22797;&#21046;&#21527;&#65311;&#25945;&#32946;&#32773;&#30340;&#25104;&#26412;&#19982;&#25910;&#30410;&#22914;&#20309;&#65311;&#36825;&#20010;&#35299;&#20915;&#26041;&#26696;&#22914;&#20309;&#24110;&#21161;&#35780;&#20272;&#21644;&#25913;&#36827;&#31185;&#23398;&#30340;&#29616;&#29366;&#65311;&#26412;&#30740;&#31350;&#22312;EPFL&#25945;&#25480;&#30340;&#24212;&#29992;&#25968;&#25454;&#20998;&#26512;&#35838;&#31243;&#65288;CS-401&#65289;&#30340;&#39033;&#30446;&#37096;&#20998;&#20013;&#32435;&#20837;&#20102;&#25968;&#25454;&#20998;&#26512;&#22797;&#21046;&#65288;N=354&#21517;&#23398;&#29983;&#65289;&#12290;&#22312;&#27492;&#25253;&#21578;&#20013;&#65292;&#25105;&#20204;&#22522;&#20110;&#35838;&#31243;&#26399;&#38388;&#36827;&#34892;&#30340;&#35843;&#26597;&#25552;&#21069;&#36827;&#34892;&#27880;&#20876;&#30340;&#21457;&#29616;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35777;&#26126;&#23398;&#29983;&#21487;&#20197;&#22797;&#21046;&#20808;&#21069;&#21457;&#34920;&#30340;&#31185;&#23398;&#35770;&#25991;&#65292;&#22823;&#37096;&#20998;&#26159;&#23450;&#24615;&#30340;&#65292;&#26377;&#20123;&#26159;&#23436;&#20840;&#19968;&#26679;&#30340;&#12290;&#25105;&#20204;&#21457;&#29616;&#23398;&#29983;&#23545;&#25968;&#25454;&#30340;&#39044;&#26399;&#19982;&#23454;&#38469;&#24773;&#20917;&#23384;&#22312;&#24046;&#24322;
&lt;/p&gt;
&lt;p&gt;
Science is facing a reproducibility crisis. Previous work has proposed incorporating data analysis replications into classrooms as a potential solution. However, despite the potential benefits, it is unclear whether this approach is feasible, and if so, what the involved stakeholders-students, educators, and scientists-should expect from it. Can students perform a data analysis replication over the course of a class? What are the costs and benefits for educators? And how can this solution help benchmark and improve the state of science?  In the present study, we incorporated data analysis replications in the project component of the Applied Data Analysis course (CS-401) taught at EPFL (N=354 students). Here we report pre-registered findings based on surveys administered throughout the course. First, we demonstrate that students can replicate previously published scientific papers, most of them qualitatively and some exactly. We find discrepancies between what students expect of data an
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#28508;&#22312;&#30011;&#23478;&#30340;&#25216;&#26415;&#65292;&#23427;&#21033;&#29992;&#28508;&#22312;&#20316;&#20026;&#30011;&#24067;&#21644;&#25193;&#25955;&#22120;&#30340;&#39044;&#27979;&#20316;&#20026;&#35745;&#21010;&#26469;&#29983;&#25104;&#32472;&#30011;&#21160;&#30011;&#65292;&#21516;&#26102;&#36824;&#21487;&#20197;&#22312;&#19981;&#21516;&#30340;&#26816;&#26597;&#28857;&#38598;&#20013;&#36716;&#25442;&#22270;&#20687;&#12290;</title><link>http://arxiv.org/abs/2308.16490</link><description>&lt;p&gt;
&#28508;&#22312;&#30011;&#23478;
&lt;/p&gt;
&lt;p&gt;
Latent Painter. (arXiv:2308.16490v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16490
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#28508;&#22312;&#30011;&#23478;&#30340;&#25216;&#26415;&#65292;&#23427;&#21033;&#29992;&#28508;&#22312;&#20316;&#20026;&#30011;&#24067;&#21644;&#25193;&#25955;&#22120;&#30340;&#39044;&#27979;&#20316;&#20026;&#35745;&#21010;&#26469;&#29983;&#25104;&#32472;&#30011;&#21160;&#30011;&#65292;&#21516;&#26102;&#36824;&#21487;&#20197;&#22312;&#19981;&#21516;&#30340;&#26816;&#26597;&#28857;&#38598;&#20013;&#36716;&#25442;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28508;&#22312;&#25193;&#25955;&#22120;&#22312;&#29983;&#25104;AI&#39046;&#22495;&#24341;&#36215;&#20102;&#38761;&#21629;&#65292;&#24182;&#28608;&#21457;&#20102;&#21019;&#36896;&#24615;&#33402;&#26415;&#12290;&#22312;&#21435;&#22122;&#28508;&#22312;&#26102;&#65292;&#27599;&#20010;&#27493;&#39588;&#39044;&#27979;&#30340;&#21407;&#22987;&#22270;&#20687;&#20849;&#21516;&#24418;&#25104;&#20102;&#21160;&#30011;&#12290;&#28982;&#32780;&#65292;&#21160;&#30011;&#21463;&#21040;&#25193;&#25955;&#22120;&#21435;&#22122;&#29305;&#24615;&#30340;&#38480;&#21046;&#65292;&#21482;&#21576;&#29616;&#20102;&#19968;&#20010;&#38160;&#21270;&#36807;&#31243;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#28508;&#22312;&#30011;&#23478;&#65292;&#23427;&#20197;&#28508;&#22312;&#20316;&#20026;&#30011;&#24067;&#65292;&#20197;&#25193;&#25955;&#22120;&#30340;&#39044;&#27979;&#20316;&#20026;&#35745;&#21010;&#65292;&#29983;&#25104;&#32472;&#30011;&#21160;&#30011;&#12290;&#28508;&#22312;&#30011;&#23478;&#36824;&#21487;&#20197;&#23558;&#19968;&#20010;&#29983;&#25104;&#30340;&#22270;&#20687;&#36716;&#25442;&#20026;&#21478;&#19968;&#20010;&#22270;&#20687;&#65292;&#36825;&#21487;&#20197;&#21457;&#29983;&#22312;&#20004;&#20010;&#19981;&#21516;&#26816;&#26597;&#28857;&#38598;&#20013;&#30340;&#22270;&#20687;&#20043;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
Latent diffusers revolutionized the generative AI and inspired creative art. When denoising the latent, the predicted original image at each step collectively animates the formation. However, the animation is limited by the denoising nature of the diffuser, and only renders a sharpening process. This work presents Latent Painter, which uses the latent as the canvas, and the diffuser predictions as the plan, to generate painting animation. Latent Painter also transits one generated image to another, which can happen between images from two different sets of checkpoints.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20803;&#23398;&#20064;&#36827;&#34892;&#27979;&#35797;&#26102;&#38388;&#36866;&#24212;&#30340;&#26041;&#27861;&#26469;&#22686;&#24378;&#28857;&#20113;&#19978;&#37319;&#26679;&#27169;&#22411;&#30340;&#26222;&#36866;&#24615;&#65292;&#35299;&#20915;&#20102;&#27979;&#35797;&#25968;&#25454;&#20998;&#24067;&#19982;&#35757;&#32451;&#25968;&#25454;&#19981;&#21516;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.16484</link><description>&lt;p&gt;
&#20351;&#29992;&#20803;&#23398;&#20064;&#36827;&#34892;&#28857;&#20113;&#19978;&#37319;&#26679;&#30340;&#27979;&#35797;&#26102;&#38388;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Test-Time Adaptation for Point Cloud Upsampling Using Meta-Learning. (arXiv:2308.16484v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16484
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20803;&#23398;&#20064;&#36827;&#34892;&#27979;&#35797;&#26102;&#38388;&#36866;&#24212;&#30340;&#26041;&#27861;&#26469;&#22686;&#24378;&#28857;&#20113;&#19978;&#37319;&#26679;&#27169;&#22411;&#30340;&#26222;&#36866;&#24615;&#65292;&#35299;&#20915;&#20102;&#27979;&#35797;&#25968;&#25454;&#20998;&#24067;&#19982;&#35757;&#32451;&#25968;&#25454;&#19981;&#21516;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24265;&#20215;&#30340;3D&#25195;&#25551;&#20202;&#32463;&#24120;&#20135;&#29983;&#31232;&#30095;&#21644;&#38750;&#22343;&#21248;&#30340;&#28857;&#20113;&#65292;&#36825;&#23545;&#26426;&#22120;&#20154;&#31995;&#32479;&#20013;&#30340;&#19979;&#28216;&#24212;&#29992;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#34429;&#28982;&#29616;&#26377;&#30340;&#28857;&#20113;&#19978;&#37319;&#26679;&#26550;&#26500;&#22312;&#26631;&#20934;&#22522;&#20934;&#25968;&#25454;&#19978;&#23637;&#31034;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#20294;&#24403;&#27979;&#35797;&#25968;&#25454;&#19982;&#35757;&#32451;&#25968;&#25454;&#20855;&#26377;&#19981;&#21516;&#20998;&#24067;&#26102;&#65292;&#23427;&#20204;&#24448;&#24448;&#20250;&#20986;&#29616;&#26174;&#33879;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27979;&#35797;&#26102;&#38388;&#36866;&#24212;&#26041;&#27861;&#26469;&#22686;&#24378;&#28857;&#20113;&#19978;&#37319;&#26679;&#27169;&#22411;&#30340;&#26222;&#36866;&#24615;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21033;&#29992;&#20803;&#23398;&#20064;&#26469;&#26174;&#24335;&#22320;&#23398;&#20064;&#27979;&#35797;&#26102;&#38388;&#36866;&#24212;&#30340;&#32593;&#32476;&#21442;&#25968;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#38656;&#35201;&#20219;&#20309;&#20851;&#20110;&#27979;&#35797;&#25968;&#25454;&#30340;&#20808;&#39564;&#20449;&#24687;&#12290;&#22312;&#20803;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#27169;&#22411;&#21442;&#25968;&#26159;&#20174;&#35757;&#32451;&#25968;&#25454;&#30340;&#31232;&#30095;-&#23494;&#38598;&#28857;&#20113;&#23545;&#30340;&#38598;&#21512;&#20013;&#23398;&#20064;&#30340;&#12290;&#22312;&#20803;&#27979;&#35797;&#36807;&#31243;&#20013;&#65292;&#32463;&#36807;&#23569;&#37327;&#26799;&#24230;&#26356;&#26032;&#30340;&#35757;&#32451;&#27169;&#22411;&#21487;&#20197;&#20135;&#29983;&#19968;&#32452;&#21807;&#19968;&#30340;&#32593;&#32476;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Affordable 3D scanners often produce sparse and non-uniform point clouds that negatively impact downstream applications in robotic systems. While existing point cloud upsampling architectures have demonstrated promising results on standard benchmarks, they tend to experience significant performance drops when the test data have different distributions from the training data. To address this issue, this paper proposes a test-time adaption approach to enhance model generality of point cloud upsampling. The proposed approach leverages meta-learning to explicitly learn network parameters for test-time adaption. Our method does not require any prior information about the test data. During meta-training, the model parameters are learned from a collection of instance-level tasks, each of which consists of a sparse-dense pair of point clouds from the training data. During meta-testing, the trained model is fine-tuned with a few gradient updates to produce a unique set of network parameters for
&lt;/p&gt;</description></item><item><title>Point-TTA&#26159;&#19968;&#31181;&#36890;&#36807;&#22810;&#20219;&#21153;&#20803;&#36741;&#21161;&#23398;&#20064;&#23454;&#29616;&#30340;&#28857;&#20113;&#37197;&#20934;&#27979;&#35797;&#26102;&#33258;&#36866;&#24212;&#26694;&#26550;&#65292;&#33021;&#22815;&#25552;&#39640;&#37197;&#20934;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.16481</link><description>&lt;p&gt;
Point-TTA: &#20351;&#29992;&#22810;&#20219;&#21153;&#20803;&#36741;&#21161;&#23398;&#20064;&#30340;&#28857;&#20113;&#37197;&#20934;&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Point-TTA: Test-Time Adaptation for Point Cloud Registration Using Multitask Meta-Auxiliary Learning. (arXiv:2308.16481v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16481
&lt;/p&gt;
&lt;p&gt;
Point-TTA&#26159;&#19968;&#31181;&#36890;&#36807;&#22810;&#20219;&#21153;&#20803;&#36741;&#21161;&#23398;&#20064;&#23454;&#29616;&#30340;&#28857;&#20113;&#37197;&#20934;&#27979;&#35797;&#26102;&#33258;&#36866;&#24212;&#26694;&#26550;&#65292;&#33021;&#22815;&#25552;&#39640;&#37197;&#20934;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;Point-TTA&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#28857;&#20113;&#37197;&#20934;&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;&#26694;&#26550;&#65292;&#21487;&#20197;&#25552;&#39640;&#37197;&#20934;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#34429;&#28982;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#36827;&#23637;&#65292;&#20294;&#38754;&#23545;&#26410;&#30693;&#30340;&#27979;&#35797;&#29615;&#22659;&#30340;&#27867;&#21270;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#65292;&#21407;&#22240;&#26159;3D&#25195;&#25551;&#30340;&#21464;&#21270;&#36739;&#22823;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#35757;&#32451;&#19968;&#20010;&#36890;&#29992;&#27169;&#22411;&#65292;&#24182;&#22312;&#27599;&#20010;&#23454;&#20363;&#19978;&#24212;&#29992;&#30456;&#21516;&#30340;&#35757;&#32451;&#27169;&#22411;&#12290;&#36825;&#21487;&#33021;&#26159;&#27425;&#20248;&#30340;&#65292;&#22240;&#20026;&#21516;&#19968;&#27169;&#22411;&#24456;&#38590;&#22788;&#29702;&#27979;&#35797;&#26399;&#38388;&#30340;&#25152;&#26377;&#21464;&#21270;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#28857;&#20113;&#37197;&#20934;&#30340;&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#36866;&#24212;&#27979;&#35797;&#26102;&#26410;&#30693;&#30340;&#20998;&#24067;&#65292;&#26080;&#38656;&#20219;&#20309;&#20851;&#20110;&#27979;&#35797;&#25968;&#25454;&#30340;&#20808;&#39564;&#30693;&#35782;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19977;&#20010;&#33258;&#30417;&#30563;&#36741;&#21161;&#20219;&#21153;&#65292;&#36825;&#20123;&#20219;&#21153;&#19982;&#20027;&#35201;&#30340;&#37197;&#20934;&#20219;&#21153;&#19968;&#36215;&#36827;&#34892;&#20248;&#21270;&#12290;&#32473;&#23450;&#19968;&#20010;&#27979;&#35797;&#23454;&#20363;&#65292;&#25105;&#20204;&#20351;&#29992;&#36825;&#20123;&#36741;&#21161;&#20219;&#21153;&#26469;&#35843;&#25972;&#25105;&#20204;&#30340;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#26356;&#26032;&#21518;&#30340;&#27169;&#22411;&#36827;&#34892;&#25512;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Point-TTA, a novel test-time adaptation framework for point cloud registration (PCR) that improves the generalization and the performance of registration models. While learning-based approaches have achieved impressive progress, generalization to unknown testing environments remains a major challenge due to the variations in 3D scans. Existing methods typically train a generic model and the same trained model is applied on each instance during testing. This could be sub-optimal since it is difficult for the same model to handle all the variations during testing. In this paper, we propose a test-time adaptation approach for PCR. Our model can adapt to unseen distributions at test-time without requiring any prior knowledge of the test data. Concretely, we design three self-supervised auxiliary tasks that are optimized jointly with the primary PCR task. Given a test instance, we adapt our model using these auxiliary tasks and the updated model is used to perform the inference. 
&lt;/p&gt;</description></item><item><title>Transformer&#21387;&#32553;&#36890;&#36807;&#23376;&#31354;&#38388;&#25237;&#24433;&#65292;&#22312;&#20943;&#23567;&#27169;&#22411;&#38544;&#34255;&#22823;&#23567;&#30340;&#21516;&#26102;&#23454;&#29616;&#20102;&#36739;&#22823;&#30340;&#27169;&#22411;&#21442;&#25968;&#21644;&#35745;&#31639;&#36164;&#28304;&#30340;&#20943;&#23569;&#65292;&#24182;&#19988;&#19982;&#20854;&#20182;&#26041;&#27861;&#20860;&#23481;&#12290;</title><link>http://arxiv.org/abs/2308.16475</link><description>&lt;p&gt;
Transformer&#21387;&#32553;&#36890;&#36807;&#23376;&#31354;&#38388;&#25237;&#24433;
&lt;/p&gt;
&lt;p&gt;
Transformer Compression via Subspace Projection. (arXiv:2308.16475v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16475
&lt;/p&gt;
&lt;p&gt;
Transformer&#21387;&#32553;&#36890;&#36807;&#23376;&#31354;&#38388;&#25237;&#24433;&#65292;&#22312;&#20943;&#23567;&#27169;&#22411;&#38544;&#34255;&#22823;&#23567;&#30340;&#21516;&#26102;&#23454;&#29616;&#20102;&#36739;&#22823;&#30340;&#27169;&#22411;&#21442;&#25968;&#21644;&#35745;&#31639;&#36164;&#28304;&#30340;&#20943;&#23569;&#65292;&#24182;&#19988;&#19982;&#20854;&#20182;&#26041;&#27861;&#20860;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TCSP&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#36890;&#36807;&#20943;&#23569;&#27169;&#22411;&#30340;&#38544;&#34255;&#22823;&#23567;&#26469;&#21387;&#32553;Transformer&#27169;&#22411;&#12290;&#36890;&#36807;&#23558;&#25972;&#20010;&#36716;&#25442;&#27169;&#22411;&#25237;&#24433;&#21040;&#19968;&#20010;&#23376;&#31354;&#38388;&#20013;&#65292;&#25105;&#20204;&#20351;&#27169;&#22411;&#20013;&#30340;&#26435;&#37325;&#30697;&#38453;&#19982;&#20943;&#23567;&#32500;&#24230;&#31354;&#38388;&#20013;&#30340;&#29305;&#24449;&#20043;&#38388;&#21487;&#20197;&#36827;&#34892;&#30697;&#38453;&#25805;&#20316;&#65292;&#20174;&#32780;&#26174;&#33879;&#20943;&#23569;&#20102;&#27169;&#22411;&#21442;&#25968;&#21644;&#35745;&#31639;&#36164;&#28304;&#12290;&#20026;&#20102;&#24314;&#31435;&#36825;&#20010;&#23376;&#31354;&#38388;&#65292;&#25105;&#20204;&#23558;&#26469;&#33258;&#19981;&#21516;&#23618;&#27425;&#30340;&#37319;&#26679;&#25968;&#25454;&#23454;&#20363;&#30340;&#29305;&#24449;&#30697;&#38453;&#20998;&#35299;&#20026;&#19968;&#20010;&#25237;&#24433;&#30697;&#38453;&#12290;&#20026;&#20102;&#35780;&#20272;&#25928;&#26524;&#65292;&#25105;&#20204;&#22312;GLUE&#21644;SQuAD&#22522;&#20934;&#27979;&#35797;&#19978;&#24212;&#29992;TCSP&#26469;&#21387;&#32553;T5&#21644;BERT&#27169;&#22411;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;TCSP&#22312;&#20445;&#35777;&#26368;&#22810;1.6%&#30340;&#20934;&#30830;&#24230;&#38477;&#20302;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;44%&#30340;&#21387;&#32553;&#27604;&#65292;&#36229;&#36807;&#25110;&#32773;&#36798;&#21040;&#20102;&#20808;&#21069;&#30340;&#21387;&#32553;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;TCSP&#36824;&#19982;&#20854;&#20182;&#30446;&#26631;&#36807;&#28388;&#22120;&#21644;&#27880;&#24847;&#21147;&#22836;&#22823;&#23567;&#21387;&#32553;&#30340;&#26041;&#27861;&#30456;&#20860;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose TCSP, a novel method for compressing a transformer model by focusing on reducing the hidden size of the model. By projecting the whole transform model into a subspace, we enable matrix operations between the weight matrices in the model and features in a reduced-dimensional space, leading to significant reductions in model parameters and computing resources. To establish this subspace, we decompose the feature matrix, derived from different layers of sampled data instances, into a projection matrix. For evaluation, TCSP is applied to compress T5 and BERT models on the GLUE and SQuAD benchmarks. Experimental results demonstrate that TCSP achieves a compression ratio of 44\% with at most 1.6\% degradation in accuracy, surpassing or matching prior compression methods. Furthermore, TCSP exhibits compatibility with other methods targeting filter and attention head size compression.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#22810;&#20010;&#39044;&#35757;&#32451;&#27169;&#22411;&#26469;&#23436;&#25104;&#30456;&#21516;&#30340;&#23376;&#20219;&#21153;&#65292;&#36890;&#36807;&#32452;&#21512;&#22810;&#20010;&#27169;&#22411;&#30340;&#32467;&#26524;&#33719;&#24471;&#26368;&#20339;&#30340;&#23376;&#20219;&#21153;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.16474</link><description>&lt;p&gt;
&#25552;&#21319;&#22810;&#27169;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23376;&#20219;&#21153;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Enhancing Subtask Performance of Multi-modal Large Language Model. (arXiv:2308.16474v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16474
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#22810;&#20010;&#39044;&#35757;&#32451;&#27169;&#22411;&#26469;&#23436;&#25104;&#30456;&#21516;&#30340;&#23376;&#20219;&#21153;&#65292;&#36890;&#36807;&#32452;&#21512;&#22810;&#20010;&#27169;&#22411;&#30340;&#32467;&#26524;&#33719;&#24471;&#26368;&#20339;&#30340;&#23376;&#20219;&#21153;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLM&#65289;&#26159;&#25351;&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#25193;&#23637;&#32780;&#26469;&#30340;&#27169;&#22411;&#65292;&#20855;&#22791;&#22788;&#29702;&#21644;&#25512;&#29702;&#22810;&#27169;&#24335;&#25968;&#25454;&#30340;&#33021;&#21147;&#12290;&#24403;&#21069;&#30340;MLLM&#36890;&#24120;&#36890;&#36807;&#20351;&#29992;LLM&#23558;&#20219;&#21153;&#20998;&#35299;&#20026;&#22810;&#20010;&#23376;&#20219;&#21153;&#65292;&#28982;&#21518;&#20351;&#29992;&#21508;&#20010;&#39044;&#35757;&#32451;&#27169;&#22411;&#23436;&#25104;&#29305;&#23450;&#30340;&#23376;&#20219;&#21153;&#65292;&#24182;&#26368;&#32456;&#21033;&#29992;LLM&#25972;&#21512;&#27599;&#20010;&#23376;&#20219;&#21153;&#30340;&#32467;&#26524;&#26469;&#33719;&#24471;&#20219;&#21153;&#30340;&#32467;&#26524;&#12290;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#65292;&#22788;&#29702;&#22823;&#22411;&#39033;&#30446;&#26102;&#65292;&#24120;&#24120;&#23558;&#39033;&#30446;&#20998;&#35299;&#20026;&#36739;&#23567;&#30340;&#23376;&#39033;&#30446;&#65292;&#24182;&#30001;&#19981;&#21516;&#30340;&#22242;&#38431;&#25552;&#20379;&#30456;&#24212;&#30340;&#35299;&#20915;&#26041;&#26696;&#25110;&#32467;&#26524;&#12290;&#39033;&#30446;&#25152;&#26377;&#32773;&#38543;&#21518;&#20915;&#23450;&#20351;&#29992;&#21738;&#20010;&#35299;&#20915;&#26041;&#26696;&#25110;&#32467;&#26524;&#65292;&#20197;&#30830;&#20445;&#27599;&#20010;&#23376;&#20219;&#21153;&#21644;&#25972;&#20010;&#39033;&#30446;&#33021;&#22815;&#36798;&#21040;&#26368;&#20339;&#32467;&#26524;&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#26412;&#30740;&#31350;&#32771;&#34385;&#36873;&#25321;&#22810;&#20010;&#39044;&#35757;&#32451;&#27169;&#22411;&#26469;&#23436;&#25104;&#30456;&#21516;&#30340;&#23376;&#20219;&#21153;&#12290;&#36890;&#36807;&#23558;&#22810;&#20010;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#32467;&#26524;&#36827;&#34892;&#32452;&#21512;&#65292;&#33719;&#24471;&#26368;&#20339;&#30340;&#23376;&#20219;&#21153;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-modal Large Language Model (MLLM) refers to a model expanded from a Large Language Model (LLM) that possesses the capability to handle and infer multi-modal data. Current MLLMs typically begin by using LLMs to decompose tasks into multiple subtasks, then employing individual pre-trained models to complete specific subtasks, and ultimately utilizing LLMs to integrate the results of each subtasks to obtain the results of the task. In real-world scenarios, when dealing with large projects, it is common practice to break down the project into smaller sub-projects, with different teams providing corresponding solutions or results. The project owner then decides which solution or result to use, ensuring the best possible outcome for each subtask and, consequently, for the entire project. Inspired by this, this study considers selecting multiple pre-trained models to complete the same subtask. By combining the results from multiple pre-trained models, the optimal subtask result is obtai
&lt;/p&gt;</description></item><item><title>MaintainoMATE&#26159;&#19968;&#20010;GitHub&#24212;&#29992;&#31243;&#24207;&#65292;&#20351;&#29992;BERT&#27169;&#22411;&#23454;&#29616;&#33258;&#21160;&#38382;&#39064;&#25253;&#21578;&#20998;&#31867;&#21644;&#20998;&#37197;&#32473;&#19987;&#19994;&#24320;&#21457;&#20154;&#21592;&#12290;</title><link>http://arxiv.org/abs/2308.16464</link><description>&lt;p&gt;
MaintainoMATE: &#19968;&#20010;&#29992;&#20110;&#26234;&#33021;&#33258;&#21160;&#21270;&#32500;&#25252;&#27963;&#21160;&#30340;GitHub&#24212;&#29992;&#31243;&#24207;
&lt;/p&gt;
&lt;p&gt;
MaintainoMATE: A GitHub App for Intelligent Automation of Maintenance Activities. (arXiv:2308.16464v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16464
&lt;/p&gt;
&lt;p&gt;
MaintainoMATE&#26159;&#19968;&#20010;GitHub&#24212;&#29992;&#31243;&#24207;&#65292;&#20351;&#29992;BERT&#27169;&#22411;&#23454;&#29616;&#33258;&#21160;&#38382;&#39064;&#25253;&#21578;&#20998;&#31867;&#21644;&#20998;&#37197;&#32473;&#19987;&#19994;&#24320;&#21457;&#20154;&#21592;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36719;&#20214;&#24320;&#21457;&#39033;&#30446;&#20381;&#36182;&#20110;&#38382;&#39064;&#36319;&#36394;&#31995;&#32479;&#26469;&#36319;&#36394;&#32500;&#25252;&#20219;&#21153;&#65292;&#22914;&#38169;&#35823;&#25253;&#21578;&#21644;&#22686;&#24378;&#35831;&#27714;&#12290;&#36825;&#20123;&#38382;&#39064;&#36319;&#36394;&#31995;&#32479;&#19978;&#30340;&#38382;&#39064;&#25253;&#21578;&#24517;&#39035;&#20197;&#26377;&#25928;&#30340;&#26041;&#24335;&#36827;&#34892;&#31649;&#29702;&#12290;&#39318;&#20808;&#65292;&#23427;&#20204;&#24517;&#39035;&#36827;&#34892;&#26631;&#35760;&#65292;&#28982;&#21518;&#20998;&#37197;&#32473;&#20855;&#26377;&#30456;&#20851;&#19987;&#19994;&#30693;&#35782;&#30340;&#29305;&#23450;&#24320;&#21457;&#20154;&#21592;&#12290;&#22788;&#29702;&#38382;&#39064;&#25253;&#21578;&#26159;&#20851;&#38190;&#30340;&#65292;&#24182;&#19988;&#38656;&#35201;&#23545;&#38382;&#39064;&#25253;&#21578;&#20013;&#36755;&#20837;&#30340;&#25991;&#26412;&#36827;&#34892;&#24443;&#24213;&#30340;&#25195;&#25551;&#65292;&#20351;&#20854;&#25104;&#20026;&#19968;&#39033;&#21171;&#21160;&#23494;&#38598;&#22411;&#30340;&#20219;&#21153;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MaintainoMATE&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#33021;&#22815;&#33258;&#21160;&#23558;&#38382;&#39064;&#25253;&#21578;&#20998;&#31867;&#21040;&#30456;&#24212;&#30340;&#31867;&#21035;&#65292;&#24182;&#36827;&#19968;&#27493;&#23558;&#38382;&#39064;&#25253;&#21578;&#20998;&#37197;&#32473;&#20855;&#26377;&#30456;&#20851;&#19987;&#19994;&#30693;&#35782;&#30340;&#24320;&#21457;&#20154;&#21592;&#12290;&#25105;&#20204;&#20351;&#29992;Transformer&#20013;&#30340;&#21452;&#21521;&#32534;&#30721;&#22120;&#34920;&#31034;&#65288;BERT&#65289;&#20316;&#20026;MaintainoMATE&#30340;&#24213;&#23618;&#27169;&#22411;&#65292;&#29992;&#20110;&#23398;&#20064;&#33258;&#21160;&#38382;&#39064;&#25253;&#21578;&#26631;&#35760;&#21644;&#20998;&#37197;&#20219;&#21153;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;&#25105;&#20204;&#23558;&#35813;&#26694;&#26550;&#37096;&#32626;&#20026;GitHub&#24212;&#29992;&#31243;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;
Software development projects rely on issue tracking systems at the core of tracking maintenance tasks such as bug reports, and enhancement requests. Incoming issue-reports on these issue tracking systems must be managed in an effective manner. First, they must be labelled and then assigned to a particular developer with relevant expertise. This handling of issue-reports is critical and requires thorough scanning of the text entered in an issue-report making it a labor-intensive task. In this paper, we present a unified framework called MaintainoMATE, which is capable of automatically categorizing the issue-reports in their respective category and further assigning the issue-reports to a developer with relevant expertise. We use the Bidirectional Encoder Representations from Transformers (BERT), as an underlying model for MaintainoMATE to learn the contextual information for automatic issue-report labeling and assignment tasks. We deploy the framework used in this work as a GitHub appl
&lt;/p&gt;</description></item><item><title>BioCoder&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#29983;&#25104;&#29983;&#29289;&#20449;&#24687;&#23398;&#20195;&#30721;&#26041;&#38754;&#30340;&#22522;&#20934;&#65292;&#28085;&#30422;&#20102;&#20989;&#25968;&#20195;&#30721;&#29983;&#25104;&#20013;&#30340;&#21253;&#20381;&#36182;&#20851;&#31995;&#12289;&#31867;&#22768;&#26126;&#21644;&#20840;&#23616;&#21464;&#37327;&#65292;&#24182;&#36890;&#36807;&#27169;&#31946;&#27979;&#35797;&#26694;&#26550;&#36827;&#34892;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2308.16458</link><description>&lt;p&gt;
BioCoder: &#19968;&#31181;&#24102;&#26377;&#19978;&#19979;&#25991;&#35821;&#29992;&#30693;&#35782;&#30340;&#29983;&#29289;&#20449;&#24687;&#23398;&#20195;&#30721;&#29983;&#25104;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
BioCoder: A Benchmark for Bioinformatics Code Generation with Contextual Pragmatic Knowledge. (arXiv:2308.16458v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16458
&lt;/p&gt;
&lt;p&gt;
BioCoder&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#29983;&#25104;&#29983;&#29289;&#20449;&#24687;&#23398;&#20195;&#30721;&#26041;&#38754;&#30340;&#22522;&#20934;&#65292;&#28085;&#30422;&#20102;&#20989;&#25968;&#20195;&#30721;&#29983;&#25104;&#20013;&#30340;&#21253;&#20381;&#36182;&#20851;&#31995;&#12289;&#31867;&#22768;&#26126;&#21644;&#20840;&#23616;&#21464;&#37327;&#65292;&#24182;&#36890;&#36807;&#27169;&#31946;&#27979;&#35797;&#26694;&#26550;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;ChatGPT&#65289;&#26174;&#33879;&#25913;&#36827;&#20102;&#20195;&#30721;&#29983;&#25104;&#12290;&#38543;&#30528;&#36825;&#20123;&#27169;&#22411;&#30340;&#25193;&#22823;&#65292;&#38656;&#35201;&#36755;&#20986;&#26469;&#22788;&#29702;&#26356;&#22797;&#26434;&#30340;&#20219;&#21153;&#30340;&#38656;&#27714;&#20063;&#36234;&#26469;&#36234;&#22810;&#12290;&#27492;&#22806;&#65292;&#22312;&#29983;&#29289;&#20449;&#24687;&#23398;&#20013;&#65292;&#29983;&#25104;&#21151;&#33021;&#31243;&#24207;&#30001;&#20110;&#39046;&#22495;&#30693;&#35782;&#37327;&#22823;&#12289;&#38656;&#35201;&#22797;&#26434;&#30340;&#25968;&#25454;&#25805;&#20316;&#21644;&#22797;&#26434;&#30340;&#21151;&#33021;&#20381;&#36182;&#20851;&#31995;&#32780;&#38754;&#20020;&#39069;&#22806;&#30340;&#25361;&#25112;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;BioCoder&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#29616;&#26377;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#29983;&#25104;&#29983;&#29289;&#20449;&#24687;&#23398;&#20195;&#30721;&#26041;&#38754;&#30340;&#22522;&#20934;&#12290;&#19982;&#20989;&#25968;&#20195;&#30721;&#29983;&#25104;&#26377;&#20851;&#65292;BioCoder&#28085;&#30422;&#20102;&#21487;&#33021;&#30340;&#21253;&#20381;&#36182;&#20851;&#31995;&#12289;&#31867;&#22768;&#26126;&#21644;&#20840;&#23616;&#21464;&#37327;&#12290;&#23427;&#21253;&#25324;&#26469;&#33258;GitHub&#30340;1026&#20010;Python&#21644;Java&#20989;&#25968;&#21644;1243&#20010;&#26041;&#27861;&#65292;&#20197;&#21450;&#26469;&#33258;Rosalind&#39033;&#30446;&#30340;253&#20010;&#31034;&#20363;&#12290;BioCoder&#36824;&#32467;&#21512;&#20102;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#30340;&#27169;&#31946;&#27979;&#35797;&#26694;&#26550;&#65292;&#25105;&#20204;&#24050;&#32463;&#24212;&#29992;&#23427;&#26469;&#35780;&#20272;&#35768;&#22810;&#27169;&#22411;&#65292;&#21253;&#25324;InCoder&#12289;CodeGen&#12289;CodeGen2&#12289;SantaCoder&#12289;StarCoder&#12289;StarCoder+&#12289;InstructCodeT&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained language models like ChatGPT have significantly improved code generation. As these models scale up, there is an increasing need for the output to handle more intricate tasks. Moreover, in bioinformatics, generating functional programs poses additional notable challenges due to the amount of domain knowledge, the need for complicated data operations, and intricate functional dependencies between the operations. Here, we present BioCoder, a benchmark developed to evaluate existing pre-trained models in generating bioinformatics code. In relation to function-code generation, BioCoder covers potential package dependencies, class declarations, and global variables. It incorporates 1026 functions and 1243 methods in Python and Java from GitHub and 253 examples from the Rosalind Project. BioCoder incorporates a fuzz-testing framework for evaluation, and we have applied it to evaluate many models including InCoder, CodeGen, CodeGen2, SantaCoder, StarCoder, StarCoder+, InstructCodeT
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#20010;&#33410;&#28857;&#20013;&#24515;&#23376;&#22270;&#30340;&#23545;&#27604;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#22270;&#23545;&#27604;&#23398;&#20064;&#30340;&#26694;&#26550;&#19979;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#21516;&#19968;&#33410;&#28857;&#30340;&#19981;&#21516;&#23376;&#22270;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#26469;&#23398;&#20064;&#33410;&#28857;&#34920;&#31034;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#30495;&#23454;&#25968;&#25454;&#38598;&#21644;&#19981;&#21516;&#19979;&#28216;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.16441</link><description>&lt;p&gt;
&#22522;&#20110;&#22810;&#20010;&#33410;&#28857;&#20013;&#24515;&#23376;&#22270;&#30340;&#23545;&#27604;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Contrastive Representation Learning Based on Multiple Node-centered Subgraphs. (arXiv:2308.16441v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16441
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#20010;&#33410;&#28857;&#20013;&#24515;&#23376;&#22270;&#30340;&#23545;&#27604;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#22270;&#23545;&#27604;&#23398;&#20064;&#30340;&#26694;&#26550;&#19979;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#21516;&#19968;&#33410;&#28857;&#30340;&#19981;&#21516;&#23376;&#22270;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#26469;&#23398;&#20064;&#33410;&#28857;&#34920;&#31034;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#30495;&#23454;&#25968;&#25454;&#38598;&#21644;&#19981;&#21516;&#19979;&#28216;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#22270;&#32467;&#26500;&#25968;&#25454;&#30340;&#22522;&#26412;&#20803;&#32032;&#65292;&#33410;&#28857;&#24050;&#34987;&#35748;&#20026;&#26159;&#22270;&#34920;&#31034;&#23398;&#20064;&#20013;&#30340;&#20027;&#35201;&#30740;&#31350;&#23545;&#35937;&#12290;&#19968;&#20010;&#21333;&#29420;&#30340;&#33410;&#28857;&#30452;&#35266;&#19978;&#20855;&#26377;&#26469;&#33258;&#25972;&#20010;&#22270;&#30340;&#22810;&#20010;&#33410;&#28857;&#20013;&#24515;&#23376;&#22270;&#65288;&#20363;&#22914;&#65292;&#19968;&#20010;&#31038;&#20132;&#32593;&#32476;&#20013;&#30340;&#19968;&#20010;&#20154;&#26681;&#25454;&#20182;&#19981;&#21516;&#30340;&#20851;&#31995;&#26377;&#22810;&#20010;&#31038;&#20132;&#22280;&#65289;&#12290;&#25105;&#20204;&#22312;&#22270;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#19979;&#30740;&#31350;&#20102;&#36825;&#31181;&#30452;&#35273;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#20010;&#33410;&#28857;&#20013;&#24515;&#23376;&#22270;&#30340;&#23545;&#27604;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#33258;&#25105;&#30417;&#30563;&#30340;&#26041;&#24335;&#22312;&#22270;&#19978;&#23398;&#20064;&#33410;&#28857;&#34920;&#31034;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#31934;&#24515;&#35774;&#35745;&#20102;&#19968;&#31995;&#21015;&#20197;&#20013;&#24515;&#33410;&#28857;&#20026;&#20013;&#24515;&#30340;&#21306;&#22495;&#23376;&#22270;&#12290;&#28982;&#21518;&#65292;&#36890;&#36807;&#23545;&#27604;&#25439;&#22833;&#26368;&#22823;&#21270;&#21516;&#19968;&#33410;&#28857;&#30340;&#19981;&#21516;&#23376;&#22270;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#12290;&#22312;&#21508;&#31181;&#30495;&#23454;&#25968;&#25454;&#38598;&#21644;&#19981;&#21516;&#19979;&#28216;&#20219;&#21153;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
As the basic element of graph-structured data, node has been recognized as the main object of study in graph representation learning. A single node intuitively has multiple node-centered subgraphs from the whole graph (e.g., one person in a social network has multiple social circles based on his different relationships). We study this intuition under the framework of graph contrastive learning, and propose a multiple node-centered subgraphs contrastive representation learning method to learn node representation on graphs in a self-supervised way. Specifically, we carefully design a series of node-centered regional subgraphs of the central node. Then, the mutual information between different subgraphs of the same node is maximized by contrastive loss. Experiments on various real-world datasets and different downstream tasks demonstrate that our model has achieved state-of-the-art results.
&lt;/p&gt;</description></item><item><title>BenchTemp&#26159;&#19968;&#20010;&#36890;&#29992;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#26102;&#38388;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;TGNN&#65289;&#27169;&#22411;&#22312;&#19981;&#21516;&#24037;&#20316;&#36127;&#36733;&#19978;&#30340;&#34920;&#29616;&#12290;BenchTemp&#25552;&#20379;&#19968;&#32452;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#19968;&#20010;&#26631;&#20934;&#27969;&#31243;&#65292;&#29992;&#20110;&#20844;&#24179;&#27604;&#36739;&#19981;&#21516;&#30340;TGNN&#27169;&#22411;&#12290;&#36890;&#36807;BenchTemp&#65292;&#25105;&#20204;&#23545;&#19981;&#21516;&#20219;&#21153;&#21644;&#35774;&#32622;&#19979;&#30340;&#20195;&#34920;&#24615;TGNN&#27169;&#22411;&#36827;&#34892;&#20102;&#24191;&#27867;&#27604;&#36739;&#12290;</title><link>http://arxiv.org/abs/2308.16385</link><description>&lt;p&gt;
BenchTemp: &#29992;&#20110;&#35780;&#20272;&#26102;&#38388;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#36890;&#29992;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
BenchTemp: A General Benchmark for Evaluating Temporal Graph Neural Networks. (arXiv:2308.16385v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16385
&lt;/p&gt;
&lt;p&gt;
BenchTemp&#26159;&#19968;&#20010;&#36890;&#29992;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#26102;&#38388;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;TGNN&#65289;&#27169;&#22411;&#22312;&#19981;&#21516;&#24037;&#20316;&#36127;&#36733;&#19978;&#30340;&#34920;&#29616;&#12290;BenchTemp&#25552;&#20379;&#19968;&#32452;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#19968;&#20010;&#26631;&#20934;&#27969;&#31243;&#65292;&#29992;&#20110;&#20844;&#24179;&#27604;&#36739;&#19981;&#21516;&#30340;TGNN&#27169;&#22411;&#12290;&#36890;&#36807;BenchTemp&#65292;&#25105;&#20204;&#23545;&#19981;&#21516;&#20219;&#21153;&#21644;&#35774;&#32622;&#19979;&#30340;&#20195;&#34920;&#24615;TGNN&#27169;&#22411;&#36827;&#34892;&#20102;&#24191;&#27867;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#22788;&#29702;&#29305;&#24449;&#25110;&#36830;&#25509;&#22312;&#28436;&#21270;&#30340;&#22270;&#20013;&#30340;&#26102;&#38388;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#30340;&#26102;&#38388;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;TGNNs&#65289;&#12290;&#23613;&#31649;&#36825;&#20123;TGNN&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#20197;&#24448;&#30340;TGNN&#35780;&#20272;&#25581;&#31034;&#20102;&#22235;&#20010;&#20851;&#38190;&#38382;&#39064;&#19978;&#30340;&#20960;&#20010;&#38480;&#21046;&#65306;1&#65289;&#25968;&#25454;&#38598;&#19981;&#19968;&#33268;&#65292;2&#65289;&#35780;&#20272;&#27969;&#31243;&#19981;&#19968;&#33268;&#65292;3&#65289;&#32570;&#20047;&#24037;&#20316;&#36127;&#36733;&#22810;&#26679;&#24615;&#65292;4&#65289;&#32570;&#20047;&#26377;&#25928;&#30340;&#27604;&#36739;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#32570;&#20047;&#19968;&#20010;&#23558;TGNN&#27169;&#22411;&#25918;&#22312;&#21516;&#19968;&#36215;&#36305;&#32447;&#19978;&#24182;&#20840;&#38754;&#27604;&#36739;&#23427;&#20204;&#30340;&#23454;&#35777;&#30740;&#31350;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;BenchTemp&#65292;&#19968;&#20010;&#29992;&#20110;&#22312;&#21508;&#31181;&#24037;&#20316;&#36127;&#36733;&#19978;&#35780;&#20272;TGNN&#27169;&#22411;&#30340;&#36890;&#29992;&#22522;&#20934;&#12290;BenchTemp&#25552;&#20379;&#19968;&#32452;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#20197;&#20415;&#21487;&#20197;&#20844;&#24179;&#22320;&#27604;&#36739;&#19981;&#21516;&#30340;TGNN&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;BenchTemp&#36824;&#35774;&#35745;&#20102;&#19968;&#20010;&#26631;&#20934;&#27969;&#31243;&#65292;&#32479;&#19968;&#20102;TGNN&#30340;&#35780;&#20272;&#12290;&#20511;&#21161;BenchTemp&#65292;&#25105;&#20204;&#24191;&#27867;&#27604;&#36739;&#20102;&#19981;&#21516;&#20219;&#21153;&#65288;&#20363;&#22914;&#38142;&#25509;&#39044;&#27979;&#21644;&#33410;&#28857;&#20998;&#31867;&#65289;&#21644;&#35774;&#32622;&#65288;&#20256;&#36882;&#21644;&#24402;&#32435;&#65289;&#19979;&#30340;&#20195;&#34920;&#24615;TGNN&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
To handle graphs in which features or connectivities are evolving over time, a series of temporal graph neural networks (TGNNs) have been proposed. Despite the success of these TGNNs, the previous TGNN evaluations reveal several limitations regarding four critical issues: 1) inconsistent datasets, 2) inconsistent evaluation pipelines, 3) lacking workload diversity, and 4) lacking efficient comparison. Overall, there lacks an empirical study that puts TGNN models onto the same ground and compares them comprehensively. To this end, we propose BenchTemp, a general benchmark for evaluating TGNN models on various workloads. BenchTemp provides a set of benchmark datasets so that different TGNN models can be fairly compared. Further, BenchTemp engineers a standard pipeline that unifies the TGNN evaluation. With BenchTemp, we extensively compare the representative TGNN models on different tasks (e.g., link prediction and node classification) and settings (transductive and inductive), w.r.t. bo
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#32508;&#36848;&#35843;&#26597;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#38544;&#31169;&#38382;&#39064;&#65292;&#21253;&#25324;&#25915;&#20987;&#12289;&#20445;&#25252;&#26041;&#27861;&#20197;&#21450;&#24212;&#29992;&#39046;&#22495;&#12290;&#30740;&#31350;&#20154;&#21592;&#30528;&#37325;&#24635;&#32467;&#20102;&#25915;&#20987;&#31867;&#22411;&#12289;&#38544;&#31169;&#20445;&#25252;&#25216;&#26415;&#20998;&#31867;&#20197;&#21450;&#21487;&#29992;&#20110;&#20998;&#26512;&#21644;&#35299;&#20915;GNNs&#20013;&#38544;&#31169;&#38382;&#39064;&#30340;&#25968;&#25454;&#38598;&#21644;&#24212;&#29992;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#26041;&#21521;&#65292;&#20197;&#26500;&#24314;&#26356;&#22909;&#30340;&#38544;&#31169;&#20445;&#25252;GNNs&#12290;</title><link>http://arxiv.org/abs/2308.16375</link><description>&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#38544;&#31169;&#35843;&#26597;&#65306;&#25915;&#20987;&#12289;&#20445;&#25252;&#21644;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
A Survey on Privacy in Graph Neural Networks: Attacks, Preservation, and Applications. (arXiv:2308.16375v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16375
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#32508;&#36848;&#35843;&#26597;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#38544;&#31169;&#38382;&#39064;&#65292;&#21253;&#25324;&#25915;&#20987;&#12289;&#20445;&#25252;&#26041;&#27861;&#20197;&#21450;&#24212;&#29992;&#39046;&#22495;&#12290;&#30740;&#31350;&#20154;&#21592;&#30528;&#37325;&#24635;&#32467;&#20102;&#25915;&#20987;&#31867;&#22411;&#12289;&#38544;&#31169;&#20445;&#25252;&#25216;&#26415;&#20998;&#31867;&#20197;&#21450;&#21487;&#29992;&#20110;&#20998;&#26512;&#21644;&#35299;&#20915;GNNs&#20013;&#38544;&#31169;&#38382;&#39064;&#30340;&#25968;&#25454;&#38598;&#21644;&#24212;&#29992;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#26041;&#21521;&#65292;&#20197;&#26500;&#24314;&#26356;&#22909;&#30340;&#38544;&#31169;&#20445;&#25252;GNNs&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22788;&#29702;&#22270;&#32467;&#26500;&#25968;&#25454;&#30340;&#33021;&#21147;&#21644;&#23454;&#38469;&#24212;&#29992;&#30340;&#25913;&#21892;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#26497;&#22823;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#36825;&#20123;&#27169;&#22411;&#20248;&#20808;&#32771;&#34385;&#39640;&#25928;&#33021;&#34920;&#29616;&#65292;&#22914;&#20934;&#30830;&#24615;&#65292;&#32780;&#32570;&#20047;&#38544;&#31169;&#32771;&#34385;&#65292;&#36825;&#26159;&#29616;&#20195;&#31038;&#20250;&#38544;&#31169;&#25915;&#20987;&#30427;&#34892;&#30340;&#37325;&#35201;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#30740;&#31350;&#20154;&#21592;&#24320;&#22987;&#24320;&#21457;&#20445;&#25252;&#38544;&#31169;&#30340;GNNs&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#22312;&#22270;&#39046;&#22495;&#32570;&#20047;&#23545;&#25915;&#20987;&#21644;&#38544;&#31169;&#20445;&#25252;&#25216;&#26415;&#30340;&#32508;&#21512;&#27010;&#36848;&#12290;&#22312;&#26412;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#24635;&#32467;&#38024;&#23545;&#22270;&#25968;&#25454;&#30340;&#25915;&#20987;&#12289;&#23545;GNNs&#20013;&#30340;&#38544;&#31169;&#20445;&#25252;&#25216;&#26415;&#36827;&#34892;&#20998;&#31867;&#20197;&#21450;&#23457;&#26597;&#21487;&#29992;&#20110;&#20998;&#26512;/&#35299;&#20915;GNNs&#20013;&#38544;&#31169;&#38382;&#39064;&#30340;&#25968;&#25454;&#38598;&#21644;&#24212;&#29992;&#31243;&#24207;&#65292;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#25105;&#20204;&#36824;&#27010;&#36848;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#28508;&#22312;&#26041;&#21521;&#65292;&#20197;&#24314;&#31435;&#26356;&#22909;&#30340;&#38544;&#31169;&#20445;&#25252;GNNs&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) have gained significant attention owing to their ability to handle graph-structured data and the improvement in practical applications. However, many of these models prioritize high utility performance, such as accuracy, with a lack of privacy consideration, which is a major concern in modern society where privacy attacks are rampant. To address this issue, researchers have started to develop privacy-preserving GNNs. Despite this progress, there is a lack of a comprehensive overview of the attacks and the techniques for preserving privacy in the graph domain. In this survey, we aim to address this gap by summarizing the attacks on graph data according to the targeted information, categorizing the privacy preservation techniques in GNNs, and reviewing the datasets and applications that could be used for analyzing/solving privacy issues in GNNs. We also outline potential directions for future research in order to build better privacy-preserving GNNs.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#25216;&#26415;&#24314;&#35758;&#65292;&#20197;&#26126;&#30830;&#23450;&#20041;&#27431;&#30431;AI&#27861;&#26696;&#20013;&#30340;&#20851;&#38190;&#27010;&#24565;&#65292;&#24182;&#25552;&#39640;&#21487;&#25191;&#34892;&#24615;&#12290;&#23427;&#21253;&#25324;&#23545;&#20154;&#26684;&#29305;&#36136;&#12289;&#34892;&#20026;&#12289;&#28508;&#24847;&#35782;&#21644;&#27450;&#39575;&#25216;&#26415;&#30340;&#23450;&#20041;&#65292;&#20197;&#21450;&#23545;&#20010;&#20307;&#21644;&#32676;&#20307;&#21033;&#29992;&#30340;&#21306;&#20998;&#12290;&#27492;&#22806;&#65292;&#23427;&#36824;&#24378;&#35843;&#20102;&#23545;&#30693;&#24773;&#20915;&#31574;&#30340;&#23450;&#20041;&#21644;&#23545;&#27835;&#30103;&#29992;&#36884;&#35905;&#20813;&#30340;&#35686;&#21578;&#12290;</title><link>http://arxiv.org/abs/2308.16364</link><description>&lt;p&gt;
&#21152;&#24378;&#27431;&#30431;AI&#27861;&#26696;: &#23545;AI&#25805;&#32437;&#30340;&#20851;&#38190;&#26415;&#35821;&#36827;&#34892;&#23450;&#20041;
&lt;/p&gt;
&lt;p&gt;
Strengthening the EU AI Act: Defining Key Terms on AI Manipulation. (arXiv:2308.16364v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16364
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#25216;&#26415;&#24314;&#35758;&#65292;&#20197;&#26126;&#30830;&#23450;&#20041;&#27431;&#30431;AI&#27861;&#26696;&#20013;&#30340;&#20851;&#38190;&#27010;&#24565;&#65292;&#24182;&#25552;&#39640;&#21487;&#25191;&#34892;&#24615;&#12290;&#23427;&#21253;&#25324;&#23545;&#20154;&#26684;&#29305;&#36136;&#12289;&#34892;&#20026;&#12289;&#28508;&#24847;&#35782;&#21644;&#27450;&#39575;&#25216;&#26415;&#30340;&#23450;&#20041;&#65292;&#20197;&#21450;&#23545;&#20010;&#20307;&#21644;&#32676;&#20307;&#21033;&#29992;&#30340;&#21306;&#20998;&#12290;&#27492;&#22806;&#65292;&#23427;&#36824;&#24378;&#35843;&#20102;&#23545;&#30693;&#24773;&#20915;&#31574;&#30340;&#23450;&#20041;&#21644;&#23545;&#27835;&#30103;&#29992;&#36884;&#35905;&#20813;&#30340;&#35686;&#21578;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27431;&#30431;&#30340;&#20154;&#24037;&#26234;&#33021;&#27861;&#26696;&#26088;&#22312;&#35268;&#33539;&#23545;AI&#30340;&#25805;&#32437;&#21644;&#26377;&#23475;&#20351;&#29992;&#65292;&#20294;&#32570;&#20047;&#23545;&#20851;&#38190;&#27010;&#24565;&#30340;&#26126;&#30830;&#23450;&#20041;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#25216;&#26415;&#24314;&#35758;&#65292;&#20197;&#25913;&#21892;&#35813;&#27861;&#26696;&#30340;&#27010;&#24565;&#28165;&#26224;&#24230;&#21644;&#21487;&#25191;&#34892;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#23457;&#26597;&#24515;&#29702;&#27169;&#22411;&#26469;&#23450;&#20041;&#8220;&#20154;&#26684;&#29305;&#36136;&#8221;&#65292;&#20027;&#24352;&#35813;&#27861;&#26696;&#24212;&#20445;&#25252;&#23436;&#25972;&#30340;&#8220;&#24515;&#29702;&#27979;&#37327;&#36164;&#26009;&#8221;&#12290;&#25105;&#20204;&#25958;&#20419;&#23558;&#8220;&#34892;&#20026;&#8221;&#25193;&#23637;&#20026;&#21253;&#25324;&#8220;&#20559;&#22909;&#8221;&#65292;&#22240;&#20026;&#20559;&#22909;&#22312;&#22240;&#26524;&#19978;&#24433;&#21709;&#21644;&#21463;&#21040;&#34892;&#20026;&#30340;&#24433;&#21709;&#12290;&#23545;&#20110;&#8220;&#28508;&#24847;&#35782;&#8221;&#12289;&#8220;&#25805;&#32437;&#8221;&#21644;&#8220;&#27450;&#39575;&#8221;&#25216;&#26415;&#25552;&#20379;&#20102;&#28165;&#26224;&#30340;&#23450;&#20041;&#65292;&#32771;&#34385;&#21040;&#21160;&#26426;&#12289;&#24847;&#22270;&#21644;&#31192;&#23494;&#24615;&#12290;&#25105;&#20204;&#21306;&#20998;&#20102;&#8220;&#21033;&#29992;&#20010;&#20307;&#8221;&#21644;&#8220;&#21033;&#29992;&#32676;&#20307;&#8221;&#65292;&#24378;&#35843;&#20102;&#19981;&#21516;&#30340;&#25919;&#31574;&#38656;&#27714;&#12290;&#19968;&#20010;&#8220;&#30693;&#24773;&#20915;&#31574;&#8221;&#30001;&#22235;&#20010;&#26041;&#38754;&#26469;&#23450;&#20041;&#65306;&#29702;&#35299;&#12289;&#20934;&#30830;&#20449;&#24687;&#12289;&#26080;&#25805;&#32437;&#21644;&#29702;&#35299;AI&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#23545;&#27861;&#26696;&#20013;&#30340;&#27835;&#30103;&#29992;&#36884;&#35905;&#20813;&#25552;&#20986;&#20102;&#35686;&#21578;&#65292;&#22240;&#20026;&#27431;&#27954;&#33647;&#21697;&#31649;&#29702;&#23616;&#23545;&#25968;&#23383;&#27835;&#30103;&#23578;&#26410;&#36827;&#34892;&#30417;&#31649;&#12290;
&lt;/p&gt;
&lt;p&gt;
The European Union's Artificial Intelligence Act aims to regulate manipulative and harmful uses of AI, but lacks precise definitions for key concepts. This paper provides technical recommendations to improve the Act's conceptual clarity and enforceability. We review psychological models to define "personality traits," arguing the Act should protect full "psychometric profiles." We urge expanding "behavior" to include "preferences" since preferences causally influence and are influenced by behavior. Clear definitions are provided for "subliminal," "manipulative," and "deceptive" techniques, considering incentives, intent, and covertness. We distinguish "exploiting individuals" from "exploiting groups," emphasising different policy needs. An "informed decision" is defined by four facets: comprehension, accurate information, no manipulation, and understanding AI's influence. We caution the Act's therapeutic use exemption given the lack of regulation of digital therapeutics by the EMA. Ove
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#20316;&#20026;&#25968;&#25454;&#39044;&#22788;&#29702;&#22120;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#20351;&#29992;&#24320;&#21457;&#24037;&#31243;&#25216;&#26415;&#21644;&#20256;&#32479;&#26041;&#27861;&#26469;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.16361</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#25968;&#25454;&#39044;&#22788;&#29702;&#22120;
&lt;/p&gt;
&lt;p&gt;
Large Language Models as Data Preprocessors. (arXiv:2308.16361v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16361
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#20316;&#20026;&#25968;&#25454;&#39044;&#22788;&#29702;&#22120;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#20351;&#29992;&#24320;&#21457;&#24037;&#31243;&#25216;&#26415;&#21644;&#20256;&#32479;&#26041;&#27861;&#26469;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22914;OpenAI&#30340;GPT&#31995;&#21015;&#21644;Meta&#30340;LLaMA&#21464;&#20307;&#65292;&#26631;&#24535;&#30528;&#20154;&#24037;&#26234;&#33021;&#30340;&#37325;&#22823;&#36827;&#23637;&#12290;&#32463;&#36807;&#22823;&#37327;&#25991;&#26412;&#25968;&#25454;&#30340;&#35757;&#32451;&#65292;LLMs&#33021;&#22815;&#29702;&#35299;&#21644;&#29983;&#25104;&#21508;&#31181;&#20027;&#39064;&#19978;&#20154;&#31867;&#21270;&#30340;&#25991;&#26412;&#12290;&#26412;&#30740;&#31350;&#25193;&#23637;&#20102;LLMs&#30340;&#24212;&#29992;&#33539;&#22260;&#65292;&#25506;&#35752;&#20102;&#23427;&#20204;&#22312;&#25968;&#25454;&#39044;&#22788;&#29702;&#20013;&#30340;&#28508;&#21147;&#65292;&#36825;&#26159;&#25968;&#25454;&#25366;&#25496;&#21644;&#20998;&#26512;&#24212;&#29992;&#20013;&#30340;&#20851;&#38190;&#38454;&#27573;&#12290;&#25105;&#20204;&#28145;&#20837;&#30740;&#31350;&#20102;&#26368;&#20808;&#36827;&#30340;LLMs&#65288;&#22914;GPT-3.5&#12289;GPT-4&#21644;Vicuna-13B&#65289;&#22312;&#38169;&#35823;&#26816;&#27979;&#12289;&#25968;&#25454;&#25554;&#34917;&#12289;&#27169;&#24335;&#21305;&#37197;&#21644;&#23454;&#20307;&#21305;&#37197;&#20219;&#21153;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#38500;&#20102;&#23637;&#31034;LLMs&#30340;&#20869;&#22312;&#33021;&#21147;&#22806;&#65292;&#25105;&#20204;&#36824;&#24378;&#35843;&#20102;&#23427;&#20204;&#30340;&#23616;&#38480;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#35745;&#31639;&#24320;&#38144;&#21644;&#25928;&#29575;&#26041;&#38754;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;LLMs&#30340;&#25968;&#25454;&#39044;&#22788;&#29702;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#25972;&#21512;&#20102;&#21069;&#27839;&#30340;&#25552;&#31034;&#24037;&#31243;&#25216;&#26415;&#65292;&#32467;&#21512;&#20102;&#19978;&#19979;&#25991;&#21270;&#21644;&#29305;&#24449;&#36873;&#25321;&#31561;&#20256;&#32479;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs), typified by OpenAI's GPT series and Meta's LLaMA variants, have marked a significant advancement in artificial intelligence. Trained on vast amounts of text data, LLMs are capable of understanding and generating human-like text across a diverse range of topics. This study expands on the applications of LLMs, exploring their potential in data preprocessing, a critical stage in data mining and analytics applications. We delve into the applicability of state-of-the-art LLMs such as GPT-3.5, GPT-4, and Vicuna-13B for error detection, data imputation, schema matching, and entity matching tasks. Alongside showcasing the inherent capabilities of LLMs, we highlight their limitations, particularly in terms of computational expense and inefficiency. We propose an LLM-based framework for data preprocessing, which integrates cutting-edge prompt engineering techniques, coupled with traditional methods like contextualization and feature selection, to improve the perform
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#28145;&#20837;&#35752;&#35770;&#20102;&#22914;&#20309;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#26469;&#26816;&#27979;&#20551;&#26032;&#38395;&#65292;&#24182;&#25581;&#31034;&#20102;&#20854;&#20013;&#30340;&#25361;&#25112;&#21644;&#26426;&#36935;&#12290;</title><link>http://arxiv.org/abs/2308.16328</link><description>&lt;p&gt;
&#25581;&#31192;&#20551;&#26032;&#38395;&#65306;&#36816;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#22312;&#25171;&#20987;&#20551;&#26032;&#38395;&#20013;&#30340;&#38761;&#21629;&#24615;&#30495;&#30456; (arXiv:2308.16328v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
Debunking Disinformation: Revolutionizing Truth with NLP in Fake News Detection. (arXiv:2308.16328v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16328
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#28145;&#20837;&#35752;&#35770;&#20102;&#22914;&#20309;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#26469;&#26816;&#27979;&#20551;&#26032;&#38395;&#65292;&#24182;&#25581;&#31034;&#20102;&#20854;&#20013;&#30340;&#25361;&#25112;&#21644;&#26426;&#36935;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20114;&#32852;&#32593;&#21644;&#31038;&#20132;&#23186;&#20307;&#25913;&#21464;&#20102;&#20154;&#20204;&#22312;&#21363;&#26102;&#20449;&#24687;&#20256;&#25773;&#26102;&#20195;&#33719;&#21462;&#26032;&#38395;&#30340;&#26041;&#24335;&#12290;&#34429;&#28982;&#36825;&#19968;&#21457;&#23637;&#22686;&#21152;&#20102;&#20449;&#24687;&#33719;&#21462;&#30340;&#26426;&#20250;&#65292;&#20294;&#20063;&#24102;&#26469;&#20102;&#19968;&#20010;&#37325;&#22823;&#38382;&#39064;&#65306;&#20551;&#26032;&#38395;&#21644;&#34394;&#20551;&#20449;&#24687;&#30340;&#20256;&#25773;&#12290;&#20551;&#26032;&#38395;&#27491;&#22312;&#24555;&#36895;&#20256;&#25773;&#20110;&#25968;&#23383;&#24179;&#21488;&#19978;&#65292;&#23545;&#23186;&#20307;&#29983;&#24577;&#31995;&#32479;&#12289;&#33286;&#35770;&#12289;&#20915;&#31574;&#21644;&#31038;&#20250;&#20957;&#32858;&#21147;&#20135;&#29983;&#20102;&#36127;&#38754;&#24433;&#21709;&#12290;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#20316;&#20026;&#19968;&#31181;&#35782;&#21035;&#20869;&#23481;&#30495;&#23454;&#24615;&#30340;&#22810;&#31181;&#26041;&#27861;&#65292;&#24050;&#32463;&#25104;&#20026;&#19982;&#34394;&#20551;&#20449;&#24687;&#20316;&#26007;&#20105;&#20013;&#30340;&#26377;&#21147;&#27494;&#22120;&#12290;&#26412;&#25991;&#28145;&#20837;&#25506;&#35752;&#20102;&#22914;&#20309;&#36816;&#29992;NLP&#25216;&#26415;&#26469;&#26816;&#27979;&#20551;&#26032;&#38395;&#65292;&#24182;&#25581;&#31034;&#20102;&#20854;&#20013;&#30340;&#25361;&#25112;&#21644;&#26426;&#36935;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Internet and social media have altered how individuals access news in the age of instantaneous information distribution. While this development has increased access to information, it has also created a significant problem: the spread of fake news and information. Fake news is rapidly spreading on digital platforms, which has a negative impact on the media ecosystem, public opinion, decision-making, and social cohesion. Natural Language Processing(NLP), which offers a variety of approaches to identify content as authentic, has emerged as a potent weapon in the growing war against disinformation. This paper takes an in-depth look at how NLP technology can be used to detect fake news and reveals the challenges and opportunities it presents.
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#30740;&#31350;&#20102;&#20855;&#26377;&#31454;&#20105;&#36873;&#25321;&#30340;&#22240;&#26524;&#25112;&#30053;&#23398;&#20064;&#20013;&#30340;&#20195;&#29702;&#36873;&#25321;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#26368;&#20339;&#36873;&#25321;&#35268;&#21017;&#30340;&#25968;&#23398;&#24418;&#24335;&#21644;&#23454;&#29616;&#26426;&#21046;&#12290;</title><link>http://arxiv.org/abs/2308.16262</link><description>&lt;p&gt;
&#26377;&#31454;&#20105;&#36873;&#25321;&#30340;&#22240;&#26524;&#25112;&#30053;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Causal Strategic Learning with Competitive Selection. (arXiv:2308.16262v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16262
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#20855;&#26377;&#31454;&#20105;&#36873;&#25321;&#30340;&#22240;&#26524;&#25112;&#30053;&#23398;&#20064;&#20013;&#30340;&#20195;&#29702;&#36873;&#25321;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#26368;&#20339;&#36873;&#25321;&#35268;&#21017;&#30340;&#25968;&#23398;&#24418;&#24335;&#21644;&#23454;&#29616;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22810;&#20010;&#20915;&#31574;&#32773;&#19979;&#30340;&#22240;&#26524;&#25112;&#30053;&#23398;&#20064;&#20013;&#30340;&#20195;&#29702;&#36873;&#25321;&#38382;&#39064;&#65292;&#24182;&#35299;&#20915;&#20102;&#20854;&#20013;&#30340;&#20004;&#20010;&#20851;&#38190;&#25361;&#25112;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#30001;&#20195;&#29702;&#20154;&#35780;&#20272;&#21644;&#36873;&#25321;&#32452;&#25104;&#30340;&#36873;&#25321;&#36807;&#31243;&#30340;&#24433;&#21709;&#65292;&#32780;&#19981;&#26159;&#20043;&#21069;&#30740;&#31350;&#20013;&#20851;&#27880;&#30340;&#22266;&#23450;&#20195;&#29702;&#20154;&#27744;&#12290;&#24403;&#27599;&#20010;&#20915;&#31574;&#32773;&#36890;&#36807;&#26368;&#22823;&#21270;&#33258;&#36523;&#25928;&#29992;&#26469;&#21333;&#26041;&#38754;&#36873;&#25321;&#20195;&#29702;&#20154;&#26102;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#26368;&#20339;&#30340;&#36873;&#25321;&#35268;&#21017;&#26159;&#22312;&#36873;&#25321;&#26368;&#20339;&#20195;&#29702;&#20154;&#21644;&#25552;&#20379;&#28608;&#21169;&#20197;&#26368;&#22823;&#21270;&#20195;&#29702;&#20154;&#25913;&#36827;&#20043;&#38388;&#36827;&#34892;&#26435;&#34913;&#30340;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#36825;&#20010;&#26368;&#20339;&#36873;&#25321;&#35268;&#21017;&#20381;&#36182;&#20110;&#20195;&#29702;&#20154;&#32467;&#26524;&#30340;&#38169;&#35823;&#39044;&#27979;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20915;&#31574;&#32773;&#30340;&#26368;&#20339;&#36873;&#25321;&#35268;&#21017;&#19981;&#20250;&#23548;&#33268;&#20195;&#29702;&#20154;&#32467;&#26524;&#24694;&#21270;&#65292;&#20063;&#19981;&#20250;&#36896;&#25104;&#19981;&#20844;&#27491;&#30340;&#38477;&#20302;&#20195;&#29702;&#20154;&#36873;&#25321;&#26426;&#20250;&#30340;&#26465;&#20214;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#26368;&#20339;&#36873;&#25321;&#35268;&#21017;&#30340;&#25968;&#23398;&#24418;&#24335;&#21644;&#19968;&#31181;&#23454;&#29616;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the problem of agent selection in causal strategic learning under multiple decision makers and address two key challenges that come with it. Firstly, while much of prior work focuses on studying a fixed pool of agents that remains static regardless of their evaluations, we consider the impact of selection procedure by which agents are not only evaluated, but also selected. When each decision maker unilaterally selects agents by maximising their own utility, we show that the optimal selection rule is a trade-off between selecting the best agents and providing incentives to maximise the agents' improvement. Furthermore, this optimal selection rule relies on incorrect predictions of agents' outcomes. Hence, we study the conditions under which a decision maker's optimal selection rule will not lead to deterioration of agents' outcome nor cause unjust reduction in agents' selection chance. To that end, we provide an analytical form of the optimal selection rule and a mechanism to r
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#22238;&#24402;&#38382;&#39064;&#30340;&#29305;&#24449;&#37325;&#35201;&#24615;&#35299;&#37322;&#26041;&#27861;&#30340;&#25193;&#23637;&#65292;&#21487;&#20197;&#37327;&#21270;&#29305;&#24449;&#37325;&#35201;&#24615;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.16245</link><description>&lt;p&gt;
&#22238;&#24402;&#38382;&#39064;&#30340;&#26657;&#20934;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Calibrated Explanations for Regression. (arXiv:2308.16245v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16245
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#22238;&#24402;&#38382;&#39064;&#30340;&#29305;&#24449;&#37325;&#35201;&#24615;&#35299;&#37322;&#26041;&#27861;&#30340;&#25193;&#23637;&#65292;&#21487;&#20197;&#37327;&#21270;&#29305;&#24449;&#37325;&#35201;&#24615;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#36890;&#24120;&#26159;&#29616;&#20195;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#65288;DSS&#65289;&#30340;&#19968;&#37096;&#20998;&#12290;&#22312;&#22522;&#20110;AI&#30340;DSS&#20013;&#20351;&#29992;&#30340;&#26368;&#20339;&#39044;&#27979;&#27169;&#22411;&#32570;&#20047;&#36879;&#26126;&#24230;&#12290;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#26088;&#22312;&#21019;&#24314;&#33021;&#22815;&#21521;&#20154;&#31867;&#29992;&#25143;&#35299;&#37322;&#20854;&#29702;&#30001;&#30340;AI&#31995;&#32479;&#12290;XAI&#20013;&#30340;&#23616;&#37096;&#35299;&#37322;&#21487;&#20197;&#25552;&#20379;&#20851;&#20110;&#20010;&#21035;&#39044;&#27979;&#30340;&#21407;&#22240;&#30340;&#20449;&#24687;&#65292;&#21363;&#29305;&#24449;&#37325;&#35201;&#24615;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#23616;&#37096;&#35299;&#37322;&#26041;&#27861;&#30340;&#19968;&#20010;&#20851;&#38190;&#32570;&#28857;&#26159;&#26080;&#27861;&#37327;&#21270;&#19982;&#29305;&#24449;&#37325;&#35201;&#24615;&#30456;&#20851;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#29305;&#24449;&#37325;&#35201;&#24615;&#35299;&#37322;&#26041;&#27861;Calibrated Explanations&#65288;CE&#65289;&#30340;&#25193;&#23637;&#65292;&#20043;&#21069;&#21482;&#25903;&#25345;&#20998;&#31867;&#65292;&#29616;&#22312;&#25903;&#25345;&#26631;&#20934;&#22238;&#24402;&#21644;&#27010;&#29575;&#22238;&#24402;&#65292;&#21363;&#30446;&#26631;&#36229;&#36807;&#20219;&#24847;&#38408;&#20540;&#30340;&#27010;&#29575;&#12290;&#22238;&#24402;&#38382;&#39064;&#30340;&#25193;&#23637;&#20445;&#30041;&#20102;CE&#30340;&#25152;&#26377;&#20248;&#28857;&#65292;&#20363;&#22914;&#23558;&#24213;&#23618;&#27169;&#22411;&#30340;&#39044;&#27979;&#19982;&#32622;&#20449;&#24230;&#26657;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial Intelligence (AI) is often an integral part of modern decision support systems (DSSs). The best-performing predictive models used in AI-based DSSs lack transparency. Explainable Artificial Intelligence (XAI) aims to create AI systems that can explain their rationale to human users. Local explanations in XAI can provide information about the causes of individual predictions in terms of feature importance. However, a critical drawback of existing local explanation methods is their inability to quantify the uncertainty associated with a feature's importance. This paper introduces an extension of a feature importance explanation method, Calibrated Explanations (CE), previously only supporting classification, with support for standard regression and probabilistic regression, i.e., the probability that the target is above an arbitrary threshold. The extension for regression keeps all the benefits of CE, such as calibration of the prediction from the underlying model with confidenc
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#23454;&#29616;&#21327;&#20316;&#20449;&#24687;&#20256;&#25773;&#12290;&#36890;&#36807;&#25552;&#20986;&#20998;&#24067;&#24335;POMDP&#24418;&#24335;&#65292;&#22312;&#28040;&#24687;&#36716;&#21457;&#19978;&#23454;&#29616;&#20102;&#27599;&#20010;&#26234;&#33021;&#20307;&#30340;&#29420;&#31435;&#20915;&#31574;&#65292;&#30456;&#27604;&#20256;&#32479;&#30340;&#22522;&#20110;&#22810;&#28857;&#20013;&#32487;&#36873;&#25321;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#20855;&#26377;&#37325;&#22823;&#21019;&#26032;&#21644;&#36129;&#29486;&#12290;&#21516;&#26102;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#22270;&#21367;&#31215;&#24378;&#21270;&#23398;&#20064;&#21644;&#21160;&#24577;&#27880;&#24847;&#21147;&#26426;&#21046;&#25429;&#25417;&#20851;&#38190;&#32593;&#32476;&#29305;&#24449;&#65292;&#24182;&#25552;&#20986;&#20102;&#19981;&#21516;&#20449;&#24687;&#20132;&#25442;&#26041;&#24335;&#30340;&#20004;&#31181;&#26041;&#27861;&#36827;&#34892;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2308.16198</link><description>&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;&#22270;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#23398;&#20064;&#21327;&#20316;&#20449;&#24687;&#20256;&#25773;
&lt;/p&gt;
&lt;p&gt;
Learning Collaborative Information Dissemination with Graph-based Multi-Agent Reinforcement Learning. (arXiv:2308.16198v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16198
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#23454;&#29616;&#21327;&#20316;&#20449;&#24687;&#20256;&#25773;&#12290;&#36890;&#36807;&#25552;&#20986;&#20998;&#24067;&#24335;POMDP&#24418;&#24335;&#65292;&#22312;&#28040;&#24687;&#36716;&#21457;&#19978;&#23454;&#29616;&#20102;&#27599;&#20010;&#26234;&#33021;&#20307;&#30340;&#29420;&#31435;&#20915;&#31574;&#65292;&#30456;&#27604;&#20256;&#32479;&#30340;&#22522;&#20110;&#22810;&#28857;&#20013;&#32487;&#36873;&#25321;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#20855;&#26377;&#37325;&#22823;&#21019;&#26032;&#21644;&#36129;&#29486;&#12290;&#21516;&#26102;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#22270;&#21367;&#31215;&#24378;&#21270;&#23398;&#20064;&#21644;&#21160;&#24577;&#27880;&#24847;&#21147;&#26426;&#21046;&#25429;&#25417;&#20851;&#38190;&#32593;&#32476;&#29305;&#24449;&#65292;&#24182;&#25552;&#20986;&#20102;&#19981;&#21516;&#20449;&#24687;&#20132;&#25442;&#26041;&#24335;&#30340;&#20004;&#31181;&#26041;&#27861;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#20195;&#36890;&#20449;&#31995;&#32479;&#20013;&#65292;&#39640;&#25928;&#21487;&#38752;&#30340;&#20449;&#24687;&#20256;&#25773;&#23545;&#25903;&#25345;&#20851;&#38190;&#25805;&#20316;&#33267;&#20851;&#37325;&#35201;&#65292;&#22914;&#28798;&#38590;&#21709;&#24212;&#12289;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#21644;&#20256;&#24863;&#22120;&#32593;&#32476;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#26041;&#27861;&#65292;&#20316;&#20026;&#23454;&#29616;&#26356;&#20026;&#20998;&#25955;&#12289;&#39640;&#25928;&#21644;&#21327;&#20316;&#35299;&#20915;&#26041;&#26696;&#30340;&#37325;&#35201;&#36827;&#23637;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20449;&#24687;&#20256;&#25773;&#30340;&#20998;&#24067;&#24335;POMDP&#65288;Decentralized-POMDP&#65289;&#24418;&#24335;&#65292;&#20351;&#24471;&#27599;&#20010;&#26234;&#33021;&#20307;&#21487;&#20197;&#29420;&#31435;&#20915;&#23450;&#28040;&#24687;&#30340;&#36716;&#21457;&#12290;&#36825;&#26500;&#25104;&#20102;&#19968;&#31181;&#20174;&#20256;&#32479;&#22522;&#20110;&#22810;&#28857;&#20013;&#32487;&#65288;MPR&#65289;&#36873;&#25321;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#30340;&#37325;&#22823;&#33539;&#24335;&#36716;&#31227;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#22270;&#21367;&#31215;&#24378;&#21270;&#23398;&#20064;&#65292;&#37319;&#29992;&#20855;&#26377;&#21160;&#24577;&#27880;&#24847;&#21147;&#30340;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#65288;GAT&#65289;&#26469;&#25429;&#25417;&#20851;&#38190;&#32593;&#32476;&#29305;&#24449;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26041;&#27861;&#65292;L-DGN&#21644;HL-DGN&#65292;&#23427;&#20204;&#22312;&#26234;&#33021;&#20307;&#20043;&#38388;&#20132;&#25442;&#30340;&#20449;&#24687;&#19978;&#26377;&#25152;&#19981;&#21516;&#12290;&#36890;&#36807;&#23558;&#25105;&#20204;&#30340;&#20998;&#25955;&#26041;&#27861;&#19982;&#22522;&#20110;MPR&#30340;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#20854;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In modern communication systems, efficient and reliable information dissemination is crucial for supporting critical operations across domains like disaster response, autonomous vehicles, and sensor networks. This paper introduces a Multi-Agent Reinforcement Learning (MARL) approach as a significant step forward in achieving more decentralized, efficient, and collaborative solutions. We propose a Decentralized-POMDP formulation for information dissemination, empowering each agent to independently decide on message forwarding. This constitutes a significant paradigm shift from traditional heuristics based on Multi-Point Relay (MPR) selection. Our approach harnesses Graph Convolutional Reinforcement Learning, employing Graph Attention Networks (GAT) with dynamic attention to capture essential network features. We propose two approaches, L-DGN and HL-DGN, which differ in the information that is exchanged among agents. We evaluate the performance of our decentralized approaches, by compari
&lt;/p&gt;</description></item><item><title>RoboTAP&#36890;&#36807;&#21033;&#29992;&#31264;&#23494;&#36861;&#36394;&#25216;&#26415;&#23454;&#29616;&#20102;&#24555;&#36895;&#12289;&#26222;&#36866;&#30340;&#23398;&#20064;&#65292;&#21487;&#20197;&#20174;&#30701;&#26102;&#38388;&#20869;&#25910;&#38598;&#30340;&#28436;&#31034;&#20013;&#35299;&#20915;&#22797;&#26434;&#30340;&#29289;&#20307;&#25490;&#21015;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2308.15975</link><description>&lt;p&gt;
RoboTAP: &#36861;&#36394;&#20219;&#24847;&#28857;&#36827;&#34892;&#23569;&#26679;&#26412;&#35270;&#35273;&#27169;&#20223;
&lt;/p&gt;
&lt;p&gt;
RoboTAP: Tracking Arbitrary Points for Few-Shot Visual Imitation. (arXiv:2308.15975v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15975
&lt;/p&gt;
&lt;p&gt;
RoboTAP&#36890;&#36807;&#21033;&#29992;&#31264;&#23494;&#36861;&#36394;&#25216;&#26415;&#23454;&#29616;&#20102;&#24555;&#36895;&#12289;&#26222;&#36866;&#30340;&#23398;&#20064;&#65292;&#21487;&#20197;&#20174;&#30701;&#26102;&#38388;&#20869;&#25910;&#38598;&#30340;&#28436;&#31034;&#20013;&#35299;&#20915;&#22797;&#26434;&#30340;&#29289;&#20307;&#25490;&#21015;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#20351;&#26426;&#22120;&#20154;&#22312;&#23454;&#39564;&#23460;&#21644;&#19987;&#38376;&#30340;&#24037;&#21378;&#20043;&#22806;&#20063;&#33021;&#21457;&#25381;&#20316;&#29992;&#65292;&#25105;&#20204;&#38656;&#35201;&#19968;&#31181;&#24555;&#36895;&#25945;&#25480;&#23427;&#20204;&#26032;&#30340;&#26377;&#29992;&#34892;&#20026;&#30340;&#26041;&#27861;&#12290;&#24403;&#21069;&#30340;&#26041;&#27861;&#35201;&#20040;&#32570;&#20047;&#26222;&#36866;&#24615;&#20197;&#36827;&#34892;&#26032;&#20219;&#21153;&#30340;&#19978;&#32447;&#65292;&#32780;&#19981;&#38656;&#35201;&#29305;&#23450;&#20219;&#21153;&#30340;&#24037;&#31243;&#21270;&#65292;&#35201;&#20040;&#32570;&#20047;&#25968;&#25454;&#25928;&#29575;&#65292;&#26080;&#27861;&#22312;&#23454;&#36341;&#20013;&#20351;&#29992;&#30340;&#26102;&#38388;&#33539;&#22260;&#20869;&#23436;&#25104;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#31264;&#23494;&#36861;&#36394;&#20316;&#20026;&#19968;&#31181;&#34920;&#31034;&#24037;&#20855;&#65292;&#20197;&#23454;&#29616;&#26356;&#24555;&#36895;&#12289;&#26356;&#26222;&#36866;&#30340;&#31034;&#25945;&#23398;&#20064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;Track-Any-Point (TAP)&#27169;&#22411;&#65292;&#23558;&#28436;&#31034;&#20013;&#30340;&#30456;&#20851;&#36816;&#21160;&#38548;&#31163;&#20986;&#26469;&#65292;&#24182;&#21442;&#25968;&#21270;&#19968;&#20010;&#20302;&#32423;&#25511;&#21046;&#22120;&#65292;&#22312;&#22330;&#26223;&#37197;&#32622;&#21457;&#29983;&#21464;&#21270;&#26102;&#37325;&#29616;&#35813;&#36816;&#21160;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#29983;&#25104;&#31283;&#20581;&#30340;&#26426;&#22120;&#20154;&#31574;&#30053;&#65292;&#21487;&#20197;&#35299;&#20915;&#22797;&#26434;&#30340;&#29289;&#20307;&#25490;&#21015;&#20219;&#21153;&#65292;&#22914;&#24418;&#29366;&#21305;&#37197;&#12289;&#21472;&#25918;&#65292;&#29978;&#33267;&#21487;&#20197;&#23436;&#25104;&#23436;&#25972;&#30340;&#36335;&#24452;&#36319;&#36394;&#20219;&#21153;&#65292;&#22914;&#26045;&#33014;&#21644;&#31896;&#21512;&#29289;&#20307;&#65292;&#25152;&#26377;&#36825;&#20123;&#20219;&#21153;&#30340;&#28436;&#31034;&#21487;&#20197;&#22312;&#20960;&#20998;&#38047;&#20869;&#25910;&#38598;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
For robots to be useful outside labs and specialized factories we need a way to teach them new useful behaviors quickly. Current approaches lack either the generality to onboard new tasks without task-specific engineering, or else lack the data-efficiency to do so in an amount of time that enables practical use. In this work we explore dense tracking as a representational vehicle to allow faster and more general learning from demonstration. Our approach utilizes Track-Any-Point (TAP) models to isolate the relevant motion in a demonstration, and parameterize a low-level controller to reproduce this motion across changes in the scene configuration. We show this results in robust robot policies that can solve complex object-arrangement tasks such as shape-matching, stacking, and even full path-following tasks such as applying glue and sticking objects together, all from demonstrations that can be collected in minutes.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#35270;&#35273;&#23450;&#20301;&#21644;&#26426;&#22120;&#20154;&#25235;&#21462;&#31995;&#32479;&#38598;&#25104;&#65292;&#36890;&#36807;&#20351;&#29992;WALL-E&#23454;&#29616;&#20102;&#22312;&#39184;&#21381;&#22330;&#26223;&#20013;&#25552;&#39640;&#20154;&#26426;&#20132;&#20114;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#30340;&#30446;&#26631;&#12290;&#36890;&#36807;&#23454;&#39564;&#21644;&#35780;&#20272;&#65292;&#35777;&#26126;&#20102;&#36825;&#31181;&#38598;&#25104;&#21487;&#20197;&#20351;WALL-E&#25104;&#20026;&#19968;&#20301;&#26356;&#26377;&#33021;&#21147;&#21644;&#26234;&#33021;&#30340;&#26426;&#22120;&#20154;&#26381;&#21153;&#21592;&#12290;</title><link>http://arxiv.org/abs/2308.15962</link><description>&lt;p&gt;
WALL-E: &#20855;&#26377;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23454;&#20307;&#26426;&#22120;&#20154;&#26381;&#21153;&#21592;&#20030;&#37325;
&lt;/p&gt;
&lt;p&gt;
WALL-E: Embodied Robotic WAiter Load Lifting with Large Language Model. (arXiv:2308.15962v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15962
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#35270;&#35273;&#23450;&#20301;&#21644;&#26426;&#22120;&#20154;&#25235;&#21462;&#31995;&#32479;&#38598;&#25104;&#65292;&#36890;&#36807;&#20351;&#29992;WALL-E&#23454;&#29616;&#20102;&#22312;&#39184;&#21381;&#22330;&#26223;&#20013;&#25552;&#39640;&#20154;&#26426;&#20132;&#20114;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#30340;&#30446;&#26631;&#12290;&#36890;&#36807;&#23454;&#39564;&#21644;&#35780;&#20272;&#65292;&#35777;&#26126;&#20102;&#36825;&#31181;&#38598;&#25104;&#21487;&#20197;&#20351;WALL-E&#25104;&#20026;&#19968;&#20301;&#26356;&#26377;&#33021;&#21147;&#21644;&#26234;&#33021;&#30340;&#26426;&#22120;&#20154;&#26381;&#21153;&#21592;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35753;&#26426;&#22120;&#20154;&#33021;&#22815;&#29702;&#35299;&#35821;&#35328;&#25351;&#20196;&#24182;&#26681;&#25454;&#35270;&#35273;&#24863;&#30693;&#20570;&#20986;&#21453;&#24212;&#19968;&#30452;&#20197;&#26469;&#37117;&#26159;&#26426;&#22120;&#20154;&#30740;&#31350;&#30028;&#30340;&#19968;&#20010;&#38271;&#26399;&#30446;&#26631;&#12290;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#38656;&#35201;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#26426;&#22120;&#20154;&#24037;&#31243;&#26041;&#38754;&#21462;&#24471;&#21069;&#27839;&#36827;&#23637;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#20027;&#35201;&#30740;&#31350;&#20102;&#23558;&#26368;&#26032;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#29616;&#26377;&#30340;&#35270;&#35273;&#23450;&#20301;&#21644;&#26426;&#22120;&#20154;&#25235;&#21462;&#31995;&#32479;&#38598;&#25104;&#20197;&#22686;&#24378;&#20154;&#26426;&#20132;&#20114;&#25928;&#26524;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#20197;WALL-E&#65288;&#20855;&#26377;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23454;&#20307;&#26426;&#22120;&#20154;&#26381;&#21153;&#21592;&#20030;&#37325;&#65289;&#20316;&#20026;&#38598;&#25104;&#30340;&#31034;&#20363;&#12290;&#31995;&#32479;&#21033;&#29992;ChatGPT&#30340;LLM&#36890;&#36807;&#22810;&#36718;&#20132;&#20114;&#24335;&#23545;&#35805;&#23558;&#29992;&#25143;&#30340;&#20559;&#22909;&#29289;&#20307;&#24635;&#32467;&#20026;&#30446;&#26631;&#25351;&#20196;&#12290;&#28982;&#21518;&#23558;&#30446;&#26631;&#25351;&#20196;&#20256;&#36882;&#32473;&#35270;&#35273;&#23450;&#20301;&#31995;&#32479;&#36827;&#34892;&#29289;&#20307;&#23039;&#21183;&#21644;&#22823;&#23567;&#20272;&#35745;&#65292;&#28982;&#21518;&#26426;&#22120;&#20154;&#30456;&#24212;&#22320;&#25235;&#21462;&#29289;&#20307;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;LLM&#22686;&#24378;&#31995;&#32479;&#37096;&#32626;&#22312;
&lt;/p&gt;
&lt;p&gt;
Enabling robots to understand language instructions and react accordingly to visual perception has been a long-standing goal in the robotics research community. Achieving this goal requires cutting-edge advances in natural language processing, computer vision, and robotics engineering. Thus, this paper mainly investigates the potential of integrating the most recent Large Language Models (LLMs) and existing visual grounding and robotic grasping system to enhance the effectiveness of the human-robot interaction. We introduce the WALL-E (Embodied Robotic WAiter load lifting with Large Language model) as an example of this integration. The system utilizes the LLM of ChatGPT to summarize the preference object of the users as a target instruction via the multi-round interactive dialogue. The target instruction is then forwarded to a visual grounding system for object pose and size estimation, following which the robot grasps the object accordingly. We deploy this LLM-empowered system on the
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#22823;&#35910;&#33469;&#22270;&#20687;&#22788;&#29702;&#30340;&#21517;&#20026;CongNaMul&#30340;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#25903;&#25345;&#22270;&#20687;&#20998;&#31867;&#12289;&#35821;&#20041;&#20998;&#21106;&#12289;&#20998;&#35299;&#21644;&#27979;&#37327;&#31561;&#20219;&#21153;&#12290;&#25552;&#20379;&#20102;&#36136;&#37327;&#20998;&#31867;&#12289;&#35821;&#20041;&#20998;&#21106;&#21644;&#22270;&#20687;&#20998;&#35299;&#30340;&#26631;&#35760;&#65292;&#20197;&#21450;5&#20010;&#33469;&#30340;&#29289;&#29702;&#29305;&#24449;&#20379;&#27979;&#37327;&#20351;&#29992;&#12290;</title><link>http://arxiv.org/abs/2308.15690</link><description>&lt;p&gt;
CongNaMul: &#19968;&#31181;&#29992;&#20110;&#22823;&#35910;&#33469;&#22270;&#20687;&#22788;&#29702;&#30340;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
CongNaMul: A Dataset for Advanced Image Processing of Soybean Sprouts. (arXiv:2308.15690v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15690
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#22823;&#35910;&#33469;&#22270;&#20687;&#22788;&#29702;&#30340;&#21517;&#20026;CongNaMul&#30340;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#25903;&#25345;&#22270;&#20687;&#20998;&#31867;&#12289;&#35821;&#20041;&#20998;&#21106;&#12289;&#20998;&#35299;&#21644;&#27979;&#37327;&#31561;&#20219;&#21153;&#12290;&#25552;&#20379;&#20102;&#36136;&#37327;&#20998;&#31867;&#12289;&#35821;&#20041;&#20998;&#21106;&#21644;&#22270;&#20687;&#20998;&#35299;&#30340;&#26631;&#35760;&#65292;&#20197;&#21450;5&#20010;&#33469;&#30340;&#29289;&#29702;&#29305;&#24449;&#20379;&#27979;&#37327;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;CongNaMul&#8221;&#65292;&#36825;&#26159;&#19968;&#20010;&#20026;&#22823;&#35910;&#33469;&#22270;&#20687;&#20998;&#26512;&#30340;&#21508;&#31181;&#20219;&#21153;&#32780;&#35774;&#35745;&#30340;&#32508;&#21512;&#25968;&#25454;&#38598;&#12290;CongNaMul&#25968;&#25454;&#38598;&#26088;&#22312;&#20419;&#36827;&#22270;&#20687;&#20998;&#31867;&#12289;&#35821;&#20041;&#20998;&#21106;&#12289;&#20998;&#35299;&#20197;&#21450;&#38271;&#24230;&#21644;&#37325;&#37327;&#30340;&#27979;&#37327;&#31561;&#20219;&#21153;&#12290;&#20998;&#31867;&#20219;&#21153;&#25552;&#20379;&#20102;&#22235;&#20010;&#31867;&#21035;&#26469;&#30830;&#23450;&#22823;&#35910;&#33469;&#30340;&#36136;&#37327;&#65306;&#27491;&#24120;&#12289;&#26029;&#35010;&#12289;&#26001;&#28857;&#21644;&#26029;&#35010;&#21644;&#26001;&#28857;&#65292;&#20197;&#24320;&#21457;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#36741;&#21161;&#30340;&#33258;&#21160;&#36136;&#37327;&#26816;&#27979;&#25216;&#26415;&#12290;&#23545;&#20110;&#35821;&#20041;&#20998;&#21106;&#65292;&#25968;&#25454;&#38598;&#21253;&#25324;&#20102;&#20855;&#26377;&#19981;&#21516;&#22797;&#26434;&#24230;&#30340;&#22270;&#20687;&#65292;&#20174;&#21333;&#20010;&#33469;&#22270;&#20687;&#21040;&#20855;&#26377;&#22810;&#20010;&#33469;&#30340;&#22270;&#20687;&#65292;&#20197;&#21450;&#20154;&#24037;&#26631;&#35760;&#30340;&#25513;&#33180;&#22270;&#20687;&#12290;&#26631;&#31614;&#21253;&#25324;4&#20010;&#19981;&#21516;&#30340;&#31867;&#21035;&#65306;&#32972;&#26223;&#12289;&#22836;&#37096;&#12289;&#36523;&#20307;&#21644;&#23614;&#37096;&#12290;&#25968;&#25454;&#38598;&#36824;&#20026;&#22270;&#20687;&#20998;&#35299;&#20219;&#21153;&#25552;&#20379;&#20102;&#22270;&#20687;&#21644;&#25513;&#33180;&#65292;&#21253;&#25324;&#20004;&#20010;&#20998;&#31163;&#30340;&#33469;&#22270;&#20687;&#21644;&#23427;&#20204;&#30340;&#32452;&#21512;&#24418;&#24335;&#12290;&#26368;&#21518;&#65292;&#36824;&#25552;&#20379;&#20102;&#33469;&#30340;5&#20010;&#29289;&#29702;&#29305;&#24449;&#65288;&#22836;&#37096;&#38271;&#24230;&#12289;&#36523;&#20307;&#38271;&#24230;&#12289;&#36523;&#20307;&#21402;&#24230;&#12289;&#23614;&#37096;&#38271;&#24230;&#12289;&#37325;&#37327;&#65289;&#20379;&#22522;&#20110;&#22270;&#20687;&#30340;&#27979;&#37327;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present 'CongNaMul', a comprehensive dataset designed for various tasks in soybean sprouts image analysis. The CongNaMul dataset is curated to facilitate tasks such as image classification, semantic segmentation, decomposition, and measurement of length and weight. The classification task provides four classes to determine the quality of soybean sprouts: normal, broken, spotted, and broken and spotted, for the development of AI-aided automatic quality inspection technology. For semantic segmentation, images with varying complexity, from single sprout images to images with multiple sprouts, along with human-labelled mask images, are included. The label has 4 different classes: background, head, body, tail. The dataset also provides images and masks for the image decomposition task, including two separate sprout images and their combined form. Lastly, 5 physical features of sprouts (head length, body length, body thickness, tail length, weight) are provided for image-based measurement
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#30340;&#38738;&#23569;&#24180;&#32933;&#32982;&#39044;&#27979;&#31995;&#32479;DeepHealthNet&#65292;&#36890;&#36807;&#25910;&#38598;&#38738;&#23569;&#24180;&#30340;&#20581;&#24247;&#25968;&#25454;&#38598;&#65292;&#21033;&#29992;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#35757;&#32451;&#27169;&#22411;&#65292;&#25552;&#20379;&#20010;&#24615;&#21270;&#39044;&#27979;&#65292;&#24182;&#24110;&#21161;&#38738;&#23569;&#24180;&#20570;&#20986;&#26126;&#26234;&#30340;&#20581;&#24247;&#20915;&#31574;&#12290;</title><link>http://arxiv.org/abs/2308.14657</link><description>&lt;p&gt;
DeepHealthNet: &#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#30340;&#38738;&#23569;&#24180;&#32933;&#32982;&#39044;&#27979;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
DeepHealthNet: Adolescent Obesity Prediction System Based on a Deep Learning Framework. (arXiv:2308.14657v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14657
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#30340;&#38738;&#23569;&#24180;&#32933;&#32982;&#39044;&#27979;&#31995;&#32479;DeepHealthNet&#65292;&#36890;&#36807;&#25910;&#38598;&#38738;&#23569;&#24180;&#30340;&#20581;&#24247;&#25968;&#25454;&#38598;&#65292;&#21033;&#29992;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#35757;&#32451;&#27169;&#22411;&#65292;&#25552;&#20379;&#20010;&#24615;&#21270;&#39044;&#27979;&#65292;&#24182;&#24110;&#21161;&#38738;&#23569;&#24180;&#20570;&#20986;&#26126;&#26234;&#30340;&#20581;&#24247;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20799;&#31461;&#21644;&#38738;&#23569;&#24180;&#32933;&#32982;&#29575;&#26159;&#19968;&#20010;&#20840;&#29699;&#24615;&#20851;&#27880;&#30340;&#38382;&#39064;&#65292;&#22240;&#20026;&#32933;&#32982;&#19982;&#24930;&#24615;&#30142;&#30149;&#21644;&#38271;&#26399;&#20581;&#24247;&#39118;&#38505;&#30456;&#20851;&#12290;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#20316;&#20026;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#20986;&#29616;&#65292;&#21487;&#20197;&#20934;&#30830;&#39044;&#27979;&#32933;&#32982;&#29575;&#65292;&#24182;&#20026;&#38738;&#23569;&#24180;&#25552;&#20379;&#20010;&#24615;&#21270;&#21453;&#39304;&#12290;&#26412;&#30740;&#31350;&#24378;&#35843;&#26089;&#26399;&#35782;&#21035;&#21644;&#39044;&#38450;&#19982;&#32933;&#32982;&#30456;&#20851;&#30340;&#20581;&#24247;&#38382;&#39064;&#30340;&#37325;&#35201;&#24615;&#12290;&#20026;&#20102;&#21457;&#23637;&#20986;&#39044;&#27979;&#32933;&#32982;&#29575;&#21644;&#25552;&#20379;&#20010;&#24615;&#21270;&#21453;&#39304;&#30340;&#24378;&#22823;&#31639;&#27861;&#65292;&#38656;&#35201;&#32771;&#34385;&#36523;&#39640;&#12289;&#20307;&#37325;&#12289;&#33136;&#22260;&#12289;&#21345;&#36335;&#37324;&#25668;&#20837;&#37327;&#12289;&#20307;&#32946;&#27963;&#21160;&#27700;&#24179;&#21644;&#20854;&#20182;&#30456;&#20851;&#20581;&#24247;&#20449;&#24687;&#12290;&#22240;&#27492;&#65292;&#36890;&#36807;&#25910;&#38598;321&#21517;&#38738;&#23569;&#24180;&#30340;&#20581;&#24247;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38738;&#23569;&#24180;&#32933;&#32982;&#39044;&#27979;&#31995;&#32479;&#65292;&#21487;&#20197;&#25552;&#20379;&#20010;&#24615;&#21270;&#39044;&#27979;&#65292;&#24182;&#24110;&#21161;&#20010;&#20154;&#20570;&#20986;&#26126;&#26234;&#30340;&#20581;&#24247;&#20915;&#31574;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;DeepHealthNet&#20351;&#29992;&#20102;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#26377;&#25928;&#22320;&#35757;&#32451;&#27169;&#22411;&#65292;&#21363;&#20351;&#22312;...
&lt;/p&gt;
&lt;p&gt;
Childhood and adolescent obesity rates are a global concern because obesity is associated with chronic diseases and long-term health risks. Artificial intelligence technology has emerged as a promising solution to accurately predict obesity rates and provide personalized feedback to adolescents. This study emphasizes the importance of early identification and prevention of obesity-related health issues. Factors such as height, weight, waist circumference, calorie intake, physical activity levels, and other relevant health information need to be considered for developing robust algorithms for obesity rate prediction and delivering personalized feedback. Hence, by collecting health datasets from 321 adolescents, we proposed an adolescent obesity prediction system that provides personalized predictions and assists individuals in making informed health decisions. Our proposed deep learning framework, DeepHealthNet, effectively trains the model using data augmentation techniques, even when 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20809;&#28369;&#24615;&#20808;&#39564;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#33410;&#28857;&#29305;&#24449;&#20013;&#25512;&#26029;&#36229;&#22270;&#30340;&#32467;&#26500;&#65292;&#24182;&#25429;&#25417;&#25968;&#25454;&#20869;&#22312;&#30340;&#20851;&#31995;&#12290;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#26631;&#35760;&#25968;&#25454;&#20316;&#20026;&#30417;&#30563;&#65292;&#33021;&#22815;&#25512;&#26029;&#20986;&#27599;&#20010;&#28508;&#22312;&#36229;&#36793;&#30340;&#27010;&#29575;&#12290;</title><link>http://arxiv.org/abs/2308.14172</link><description>&lt;p&gt;
&#20174;&#25968;&#25454;&#20013;&#22522;&#20110;&#20809;&#28369;&#24615;&#20808;&#39564;&#25512;&#26029;&#36229;&#22270;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
Hypergraph Structure Inference From Data Under Smoothness Prior. (arXiv:2308.14172v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14172
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20809;&#28369;&#24615;&#20808;&#39564;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#33410;&#28857;&#29305;&#24449;&#20013;&#25512;&#26029;&#36229;&#22270;&#30340;&#32467;&#26500;&#65292;&#24182;&#25429;&#25417;&#25968;&#25454;&#20869;&#22312;&#30340;&#20851;&#31995;&#12290;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#26631;&#35760;&#25968;&#25454;&#20316;&#20026;&#30417;&#30563;&#65292;&#33021;&#22815;&#25512;&#26029;&#20986;&#27599;&#20010;&#28508;&#22312;&#36229;&#36793;&#30340;&#27010;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36229;&#22270;&#22312;&#22788;&#29702;&#28041;&#21450;&#22810;&#20010;&#23454;&#20307;&#30340;&#39640;&#38454;&#20851;&#31995;&#25968;&#25454;&#20013;&#38750;&#24120;&#37325;&#35201;&#12290;&#22312;&#27809;&#26377;&#26126;&#30830;&#36229;&#22270;&#21487;&#29992;&#30340;&#24773;&#20917;&#19979;&#65292;&#24076;&#26395;&#33021;&#22815;&#20174;&#33410;&#28857;&#29305;&#24449;&#20013;&#25512;&#26029;&#20986;&#26377;&#24847;&#20041;&#30340;&#36229;&#22270;&#32467;&#26500;&#65292;&#20197;&#25429;&#25417;&#25968;&#25454;&#20869;&#22312;&#30340;&#20851;&#31995;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#35201;&#20040;&#37319;&#29992;&#31616;&#21333;&#39044;&#23450;&#20041;&#30340;&#35268;&#21017;&#65292;&#19981;&#33021;&#31934;&#30830;&#25429;&#25417;&#28508;&#22312;&#36229;&#22270;&#32467;&#26500;&#30340;&#20998;&#24067;&#65292;&#35201;&#20040;&#23398;&#20064;&#36229;&#22270;&#32467;&#26500;&#21644;&#33410;&#28857;&#29305;&#24449;&#20043;&#38388;&#30340;&#26144;&#23556;&#65292;&#20294;&#38656;&#35201;&#22823;&#37327;&#26631;&#35760;&#25968;&#25454;&#65288;&#21363;&#39044;&#20808;&#23384;&#22312;&#30340;&#36229;&#22270;&#32467;&#26500;&#65289;&#36827;&#34892;&#35757;&#32451;&#12290;&#36825;&#20004;&#31181;&#26041;&#27861;&#37117;&#23616;&#38480;&#20110;&#23454;&#38469;&#24773;&#26223;&#20013;&#30340;&#24212;&#29992;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20809;&#28369;&#24615;&#20808;&#39564;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#35774;&#35745;&#19968;&#31181;&#26041;&#27861;&#65292;&#22312;&#27809;&#26377;&#26631;&#35760;&#25968;&#25454;&#20316;&#20026;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#25512;&#26029;&#20986;&#27599;&#20010;&#28508;&#22312;&#36229;&#36793;&#30340;&#27010;&#29575;&#12290;&#25152;&#25552;&#20986;&#30340;&#20808;&#39564;&#34920;&#31034;&#36229;&#36793;&#20013;&#30340;&#33410;&#28857;&#29305;&#24449;&#19982;&#21253;&#21547;&#35813;&#36229;&#36793;&#30340;&#36229;&#36793;&#30340;&#29305;&#24449;&#39640;&#24230;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hypergraphs are important for processing data with higher-order relationships involving more than two entities. In scenarios where explicit hypergraphs are not readily available, it is desirable to infer a meaningful hypergraph structure from the node features to capture the intrinsic relations within the data. However, existing methods either adopt simple pre-defined rules that fail to precisely capture the distribution of the potential hypergraph structure, or learn a mapping between hypergraph structures and node features but require a large amount of labelled data, i.e., pre-existing hypergraph structures, for training. Both restrict their applications in practical scenarios. To fill this gap, we propose a novel smoothness prior that enables us to design a method to infer the probability for each potential hyperedge without labelled data as supervision. The proposed prior indicates features of nodes in a hyperedge are highly correlated by the features of the hyperedge containing th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36827;&#34892;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#30340;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26694;&#26550;&#65288;&#30693;&#35782;&#22270;&#35889;LLM&#65289;&#65292;&#20197;&#25552;&#39640;&#19977;&#20803;&#32452;&#20998;&#31867;&#21644;&#20851;&#31995;&#39044;&#27979;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.13916</link><description>&lt;p&gt;
&#25506;&#32034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;
&lt;/p&gt;
&lt;p&gt;
Exploring Large Language Models for Knowledge Graph Completion. (arXiv:2308.13916v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13916
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36827;&#34892;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#30340;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26694;&#26550;&#65288;&#30693;&#35782;&#22270;&#35889;LLM&#65289;&#65292;&#20197;&#25552;&#39640;&#19977;&#20803;&#32452;&#20998;&#31867;&#21644;&#20851;&#31995;&#39044;&#27979;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#22312;&#20247;&#22810;&#20154;&#24037;&#26234;&#33021;&#20219;&#21153;&#20013;&#21457;&#25381;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#20294;&#32463;&#24120;&#38754;&#20020;&#19981;&#23436;&#25972;&#24615;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36827;&#34892;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#23558;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#19977;&#20803;&#32452;&#35270;&#20026;&#25991;&#26412;&#24207;&#21015;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26694;&#26550;&#65292;&#31216;&#20026;&#30693;&#35782;&#22270;&#35889;LLM&#65288;KG-LLM&#65289;&#65292;&#26469;&#23545;&#36825;&#20123;&#19977;&#20803;&#32452;&#36827;&#34892;&#24314;&#27169;&#12290;&#25105;&#20204;&#30340;&#25216;&#26415;&#21033;&#29992;&#19977;&#20803;&#32452;&#30340;&#23454;&#20307;&#21644;&#20851;&#31995;&#25551;&#36848;&#20316;&#20026;&#25552;&#31034;&#65292;&#24182;&#21033;&#29992;&#21709;&#24212;&#36827;&#34892;&#39044;&#27979;&#12290;&#23545;&#21508;&#31181;&#22522;&#20934;&#30693;&#35782;&#22270;&#35889;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19977;&#20803;&#32452;&#20998;&#31867;&#21644;&#20851;&#31995;&#39044;&#27979;&#31561;&#20219;&#21153;&#20013;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;&#24494;&#35843;&#30456;&#23545;&#36739;&#23567;&#30340;&#27169;&#22411;&#65288;&#20363;&#22914;LLaMA-7B&#65292;ChatGLM-6B&#65289;&#20248;&#20110;&#26368;&#26032;&#30340;ChatGPT&#21644;GPT-4&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge graphs play a vital role in numerous artificial intelligence tasks, yet they frequently face the issue of incompleteness. In this study, we explore utilizing Large Language Models (LLM) for knowledge graph completion. We consider triples in knowledge graphs as text sequences and introduce an innovative framework called Knowledge Graph LLM (KG-LLM) to model these triples. Our technique employs entity and relation descriptions of a triple as prompts and utilizes the response for predictions. Experiments on various benchmark knowledge graphs demonstrate that our method attains state-of-the-art performance in tasks such as triple classification and relation prediction. We also find that fine-tuning relatively smaller models (e.g., LLaMA-7B, ChatGLM-6B) outperforms recent ChatGPT and GPT-4.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38543;&#26426;&#23398;&#20064;&#22120;&#27169;&#22411;&#65292;&#31216;&#20026;&#38543;&#26426;&#37197;&#32622;&#26426;&#65288;SCMs&#65289;&#65292;&#20854;&#22522;&#20110;&#38543;&#26426;&#37197;&#32622;&#32593;&#32476;&#65288;SCNs&#65289;&#65292;&#26088;&#22312;&#24378;&#35843;&#24037;&#19994;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#26377;&#25928;&#24314;&#27169;&#21644;&#33410;&#32422;&#25968;&#25454;&#22823;&#23567;&#12290;SCMs&#36890;&#36807;&#21387;&#32553;&#27169;&#22411;&#23384;&#20648;&#65292;&#24182;&#20445;&#25345;&#26377;&#21033;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#20855;&#26377;&#22312;&#24037;&#19994;&#24212;&#29992;&#20013;&#24456;&#22823;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.13570</link><description>&lt;p&gt;
&#24037;&#19994;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#38543;&#26426;&#37197;&#32622;&#26426;
&lt;/p&gt;
&lt;p&gt;
Stochastic Configuration Machines for Industrial Artificial Intelligence. (arXiv:2308.13570v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13570
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38543;&#26426;&#23398;&#20064;&#22120;&#27169;&#22411;&#65292;&#31216;&#20026;&#38543;&#26426;&#37197;&#32622;&#26426;&#65288;SCMs&#65289;&#65292;&#20854;&#22522;&#20110;&#38543;&#26426;&#37197;&#32622;&#32593;&#32476;&#65288;SCNs&#65289;&#65292;&#26088;&#22312;&#24378;&#35843;&#24037;&#19994;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#26377;&#25928;&#24314;&#27169;&#21644;&#33410;&#32422;&#25968;&#25454;&#22823;&#23567;&#12290;SCMs&#36890;&#36807;&#21387;&#32553;&#27169;&#22411;&#23384;&#20648;&#65292;&#24182;&#20445;&#25345;&#26377;&#21033;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#20855;&#26377;&#22312;&#24037;&#19994;&#24212;&#29992;&#20013;&#24456;&#22823;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24037;&#19994;&#20154;&#24037;&#26234;&#33021;&#65288;IAI&#65289;&#20013;&#65292;&#38656;&#35201;&#23454;&#26102;&#12289;&#20934;&#30830;&#30340;&#39044;&#27979;&#24314;&#27169;&#65292;&#31070;&#32463;&#32593;&#32476;&#22312;&#20854;&#20013;&#36215;&#21040;&#20851;&#38190;&#20316;&#29992;&#12290;&#24037;&#19994;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#31070;&#32463;&#32593;&#32476;&#38656;&#35201;&#24378;&#22823;&#30340;&#39640;&#24615;&#33021;&#35745;&#31639;&#35774;&#22791;&#26469;&#22788;&#29702;&#22823;&#37327;&#30340;&#28014;&#28857;&#25968;&#25454;&#12290;&#26412;&#25991;&#22522;&#20110;&#38543;&#26426;&#37197;&#32622;&#32593;&#32476;&#65288;SCNs&#65289;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38543;&#26426;&#23398;&#20064;&#22120;&#27169;&#22411;&#65292;&#31216;&#20026;&#38543;&#26426;&#37197;&#32622;&#26426;&#65288;SCMs&#65289;&#65292;&#20197;&#24378;&#35843;&#23545;&#20110;&#24037;&#19994;&#24212;&#29992;&#38750;&#24120;&#26377;&#29992;&#21644;&#26377;&#20215;&#20540;&#30340;&#26377;&#25928;&#24314;&#27169;&#21644;&#33410;&#32422;&#25968;&#25454;&#22823;&#23567;&#12290;&#19982;&#20855;&#26377;&#20108;&#20540;&#21270;&#23454;&#29616;&#30340;&#38543;&#26426;&#21521;&#37327;&#21151;&#33021;&#38142;&#25509;&#65288;RVFL&#65289;&#32593;&#32476;&#30456;&#27604;&#65292;SCMs&#30340;&#27169;&#22411;&#23384;&#20648;&#21487;&#20197;&#26174;&#33879;&#21387;&#32553;&#65292;&#21516;&#26102;&#20445;&#25345;&#26377;&#21033;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;&#38500;&#20102;SCM&#23398;&#20064;&#22120;&#27169;&#22411;&#30340;&#26550;&#26500;&#21644;&#23398;&#20064;&#31639;&#27861;&#65292;&#20316;&#20026;&#26412;&#25991;&#30340;&#37325;&#35201;&#37096;&#20998;&#65292;&#25105;&#20204;&#36824;&#36890;&#36807;&#20998;&#26512;&#27169;&#22411;&#30340;&#22797;&#26434;&#24615;&#25552;&#20379;&#20102;SCMs&#30340;&#23398;&#20064;&#33021;&#21147;&#30340;&#29702;&#35770;&#22522;&#30784;&#12290;&#23454;&#39564;&#30740;&#31350;&#20063;&#36827;&#34892;&#20102;&#12290;
&lt;/p&gt;
&lt;p&gt;
Real-time predictive modelling with desired accuracy is highly expected in industrial artificial intelligence (IAI), where neural networks play a key role. Neural networks in IAI require powerful, high-performance computing devices to operate a large number of floating point data. Based on stochastic configuration networks (SCNs), this paper proposes a new randomized learner model, termed stochastic configuration machines (SCMs), to stress effective modelling and data size saving that are useful and valuable for industrial applications. Compared to SCNs and random vector functional-link (RVFL) nets with binarized implementation, the model storage of SCMs can be significantly compressed while retaining favourable prediction performance. Besides the architecture of the SCM learner model and its learning algorithm, as an important part of this contribution, we also provide a theoretical basis on the learning capacity of SCMs by analysing the model's complexity. Experimental studies are ca
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#23545;DNN&#25351;&#32441;&#21435;&#38500;&#25915;&#20987;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35843;&#26597;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RemovalNet&#30340;&#25915;&#20987;&#26041;&#27861;&#65292;&#36890;&#36807;min-max&#21452;&#23618;&#20248;&#21270;&#26469;&#36867;&#36991;&#27169;&#22411;&#25152;&#26377;&#26435;&#39564;&#35777;&#12290;&#25915;&#20987;&#26041;&#27861;&#26088;&#22312;&#21435;&#38500;&#25351;&#32441;&#29305;&#23450;&#30693;&#35782;&#65292;&#24182;&#25552;&#21462;&#21463;&#23475;&#27169;&#22411;&#30340;&#36890;&#29992;&#35821;&#20041;&#30693;&#35782;&#26469;&#32500;&#25345;&#26367;&#20195;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.12319</link><description>&lt;p&gt;
RemovalNet: DNN&#25351;&#32441;&#21435;&#38500;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
RemovalNet: DNN Fingerprint Removal Attacks. (arXiv:2308.12319v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12319
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#23545;DNN&#25351;&#32441;&#21435;&#38500;&#25915;&#20987;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35843;&#26597;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RemovalNet&#30340;&#25915;&#20987;&#26041;&#27861;&#65292;&#36890;&#36807;min-max&#21452;&#23618;&#20248;&#21270;&#26469;&#36867;&#36991;&#27169;&#22411;&#25152;&#26377;&#26435;&#39564;&#35777;&#12290;&#25915;&#20987;&#26041;&#27861;&#26088;&#22312;&#21435;&#38500;&#25351;&#32441;&#29305;&#23450;&#30693;&#35782;&#65292;&#24182;&#25552;&#21462;&#21463;&#23475;&#27169;&#22411;&#30340;&#36890;&#29992;&#35821;&#20041;&#30693;&#35782;&#26469;&#32500;&#25345;&#26367;&#20195;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#24615;&#33021;&#30340;&#26174;&#33879;&#25552;&#21319;&#65292;DNNs&#22312;&#35768;&#22810;&#39046;&#22495;&#24471;&#21040;&#20102;&#24191;&#27867;&#24212;&#29992;&#12290;&#22240;&#27492;&#65292;DNN&#27169;&#22411;&#24050;&#32463;&#25104;&#20026;&#19968;&#39033;&#23453;&#36149;&#30340;&#36164;&#20135;&#65292;&#20854;&#30693;&#35782;&#20135;&#26435;&#36890;&#36807;&#25152;&#26377;&#26435;&#39564;&#35777;&#25216;&#26415;&#65288;&#22914;DNN&#25351;&#32441;&#65289;&#24471;&#21040;&#20445;&#25252;&#12290;&#28982;&#32780;&#65292;DNN&#25351;&#32441;&#21435;&#38500;&#25915;&#20987;&#30340;&#21487;&#34892;&#24615;&#21450;&#20854;&#28508;&#22312;&#24433;&#21709;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#39318;&#27425;&#23545;DNN&#25351;&#32441;&#21435;&#38500;&#25915;&#20987;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35843;&#26597;&#12290;&#19968;&#33324;&#32780;&#35328;&#65292;DNN&#27169;&#22411;&#20013;&#21253;&#21547;&#30340;&#30693;&#35782;&#21487;&#20197;&#20998;&#20026;&#36890;&#29992;&#35821;&#20041;&#30693;&#35782;&#21644;&#25351;&#32441;&#29305;&#23450;&#30693;&#35782;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;min-max&#21452;&#23618;&#20248;&#21270;&#30340;DNN&#25351;&#32441;&#21435;&#38500;&#25915;&#20987;&#8212;&#8212;RemovalNet&#65292;&#20197;&#36867;&#36991;&#27169;&#22411;&#25152;&#26377;&#26435;&#39564;&#35777;&#12290;&#19979;&#23618;&#20248;&#21270;&#26088;&#22312;&#21435;&#38500;&#25351;&#32441;&#29305;&#23450;&#30693;&#35782;&#65292;&#32780;&#19978;&#23618;&#20248;&#21270;&#21017;&#22312;&#32500;&#25345;&#26367;&#20195;&#27169;&#22411;&#24615;&#33021;&#30340;&#21516;&#26102;&#25552;&#21462;&#21463;&#23475;&#27169;&#22411;&#30340;&#36890;&#29992;&#35821;&#20041;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the performance of deep neural networks (DNNs) remarkably improving, DNNs have been widely used in many areas. Consequently, the DNN model has become a valuable asset, and its intellectual property is safeguarded by ownership verification techniques (e.g., DNN fingerprinting). However, the feasibility of the DNN fingerprint removal attack and its potential influence remains an open problem. In this paper, we perform the first comprehensive investigation of DNN fingerprint removal attacks. Generally, the knowledge contained in a DNN model can be categorized into general semantic and fingerprint-specific knowledge. To this end, we propose a min-max bilevel optimization-based DNN fingerprint removal attack named RemovalNet, to evade model ownership verification. The lower-level optimization is designed to remove fingerprint-specific knowledge. While in the upper-level optimization, we distill the victim model's general semantic knowledge to maintain the surrogate model's performance.
&lt;/p&gt;</description></item><item><title>&#22312;&#31070;&#32463;&#21147;&#22330;&#27169;&#22411;&#20013;&#65292;&#24120;&#29992;&#30340;MD17&#25968;&#25454;&#38598;&#23545;&#20110;&#34920;&#31034;&#32463;&#21382;&#21270;&#23398;&#21453;&#24212;&#30340;&#31995;&#32479;&#19981;&#36275;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;xxMD&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#37319;&#26679;&#33258;&#25193;&#23637;&#28608;&#21457;&#24577;&#20998;&#23376;&#21160;&#21147;&#23398;&#65292;&#21253;&#21547;&#20102;&#33021;&#37327;&#21644;&#21147;&#30340;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2308.11155</link><description>&lt;p&gt;
&#36890;&#36807;&#36229;&#20986;&#24179;&#34913;&#29366;&#24577;&#30340;&#25193;&#23637;&#21160;&#21147;&#23398;&#24615;&#33021;&#35780;&#20272;&#31070;&#32463;&#21147;&#22330;
&lt;/p&gt;
&lt;p&gt;
xxMD: Benchmarking Neural Force Fields Using Extended Dynamics beyond Equilibrium. (arXiv:2308.11155v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11155
&lt;/p&gt;
&lt;p&gt;
&#22312;&#31070;&#32463;&#21147;&#22330;&#27169;&#22411;&#20013;&#65292;&#24120;&#29992;&#30340;MD17&#25968;&#25454;&#38598;&#23545;&#20110;&#34920;&#31034;&#32463;&#21382;&#21270;&#23398;&#21453;&#24212;&#30340;&#31995;&#32479;&#19981;&#36275;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;xxMD&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#37319;&#26679;&#33258;&#25193;&#23637;&#28608;&#21457;&#24577;&#20998;&#23376;&#21160;&#21147;&#23398;&#65292;&#21253;&#21547;&#20102;&#33021;&#37327;&#21644;&#21147;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#21147;&#22330;&#24050;&#25104;&#20026;&#35745;&#31639;&#21270;&#23398;&#20013;&#30340;&#37325;&#35201;&#27169;&#22411;&#65292;&#21462;&#20195;&#20102;&#20174;&#22836;&#31639;&#30340;&#20998;&#23376;&#21160;&#21147;&#23398;&#20013;&#30340;&#37327;&#23376;&#21270;&#23398;&#35745;&#31639;&#12290;&#30446;&#21069;&#23545;&#31070;&#32463;&#21147;&#22330;&#30340;&#20027;&#35201;&#35780;&#20272;&#22522;&#20934;&#26159;MD17&#25968;&#25454;&#38598;&#21450;&#20854;&#21518;&#32493;&#25193;&#23637;&#12290;&#36825;&#20123;&#25968;&#25454;&#38598;&#20027;&#35201;&#21253;&#21547;&#26469;&#33258;&#22522;&#24577;&#21183;&#33021;&#38754;&#24179;&#34913;&#21306;&#22495;&#30340;&#20960;&#20309;&#32467;&#26500;&#65292;&#37319;&#26679;&#33258;&#30452;&#25509;&#32477;&#28909;&#21160;&#21147;&#23398;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#21270;&#23398;&#21453;&#24212;&#28041;&#21450;&#21040;&#36739;&#22823;&#30340;&#20998;&#23376;&#21464;&#24418;&#65292;&#29305;&#21035;&#26159;&#38190;&#26029;&#35010;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;MD17&#25968;&#25454;&#38598;&#20013;&#20869;&#22352;&#26631;&#21644;&#33021;&#37327;&#30340;&#32422;&#26463;&#20998;&#24067;&#65292;&#20984;&#26174;&#20102;&#20854;&#22312;&#34920;&#31034;&#32463;&#21382;&#21270;&#23398;&#21453;&#24212;&#30340;&#31995;&#32479;&#26041;&#38754;&#30340;&#19981;&#36275;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#31181;&#37319;&#26679;&#38480;&#21046;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;xxMD&#65288;&#25193;&#23637;&#28608;&#21457;&#24577;&#20998;&#23376;&#21160;&#21147;&#23398;&#65289;&#25968;&#25454;&#38598;&#65292;&#20174;&#38750;&#32477;&#28909;&#21160;&#21147;&#23398;&#20013;&#27966;&#29983;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#20102;&#20174;&#22810;&#21442;&#32771;&#27874;&#20989;&#25968;&#29702;&#35770;&#21644;&#23494;&#24230;&#27867;&#20989;&#20013;&#30830;&#23450;&#30340;&#33021;&#37327;&#21644;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural force fields (NFFs) have gained prominence in computational chemistry as surrogate models, superseding quantum-chemistry calculations in ab initio molecular dynamics. The prevalent benchmark for NFFs has been the MD17 dataset and its subsequent extension. These datasets predominantly comprise geometries from the equilibrium region of the ground electronic state potential energy surface, sampling from direct adiabatic dynamics. However, many chemical reactions entail significant molecular deformations, notably bond breaking. We demonstrate the constrained distribution of internal coordinates and energies in the MD17 datasets, underscoring their inadequacy for representing systems undergoing chemical reactions. Addressing this sampling limitation, we introduce the xxMD (Extended Excited-state Molecular Dynamics) dataset, derived from non-adiabatic dynamics. This dataset encompasses energies and forces ascertained from both multireference wave function theory and density functional
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;RBA-GCN&#27169;&#22411;&#29992;&#20110;&#24773;&#24863;&#35782;&#21035;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#24341;&#20837;&#20851;&#31995;&#21452;&#23618;&#32858;&#21512;&#21644;&#22270;&#29983;&#25104;&#27169;&#22359;&#65292;&#35299;&#20915;&#20102;GCN&#27169;&#22411;&#20013;&#30340;&#33410;&#28857;&#20449;&#24687;&#20887;&#20313;&#21644;&#36828;&#36317;&#31163;&#19978;&#19979;&#25991;&#20449;&#24687;&#25429;&#33719;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.11029</link><description>&lt;p&gt;
RBA-GCN: &#20851;&#31995;&#21452;&#23618;&#32858;&#21512;&#22270;&#21367;&#31215;&#32593;&#32476;&#29992;&#20110;&#24773;&#24863;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
RBA-GCN: Relational Bilevel Aggregation Graph Convolutional Network for Emotion Recognition. (arXiv:2308.11029v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11029
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;RBA-GCN&#27169;&#22411;&#29992;&#20110;&#24773;&#24863;&#35782;&#21035;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#24341;&#20837;&#20851;&#31995;&#21452;&#23618;&#32858;&#21512;&#21644;&#22270;&#29983;&#25104;&#27169;&#22359;&#65292;&#35299;&#20915;&#20102;GCN&#27169;&#22411;&#20013;&#30340;&#33410;&#28857;&#20449;&#24687;&#20887;&#20313;&#21644;&#36828;&#36317;&#31163;&#19978;&#19979;&#25991;&#20449;&#24687;&#25429;&#33719;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24773;&#24863;&#35782;&#21035;&#22312;&#23545;&#35805;&#20013;&#30340;&#24212;&#29992;&#21463;&#21040;&#20102;&#30740;&#31350;&#20154;&#21592;&#30340;&#20851;&#27880;&#65292;&#30001;&#20110;&#23427;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;&#30001;&#20110;&#23545;&#35805;&#20855;&#26377;&#33258;&#28982;&#30340;&#22270;&#32467;&#26500;&#65292;&#24456;&#22810;&#22522;&#20110;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCNs&#65289;&#30340;ERC&#27169;&#22411;&#26041;&#27861;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;GCNs&#30340;&#32858;&#21512;&#26041;&#27861;&#23384;&#22312;&#33410;&#28857;&#20449;&#24687;&#20887;&#20313;&#38382;&#39064;&#65292;&#23548;&#33268;&#33410;&#28857;&#36776;&#21035;&#20449;&#24687;&#30340;&#20002;&#22833;&#12290;&#27492;&#22806;&#65292;&#21333;&#23618;GCNs&#32570;&#20047;&#20174;&#22270;&#20013;&#25429;&#33719;&#36828;&#36317;&#31163;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#22823;&#22810;&#25968;&#26041;&#27861;&#37117;&#26159;&#22522;&#20110;&#25991;&#26412;&#27169;&#24577;&#25110;&#23558;&#19981;&#21516;&#27169;&#24577;&#25340;&#25509;&#22312;&#19968;&#36215;&#65292;&#23548;&#33268;&#25429;&#25417;&#27169;&#24577;&#38388;&#20132;&#20114;&#33021;&#21147;&#24369;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20851;&#31995;&#21452;&#23618;&#32858;&#21512;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;RBA-GCN&#65289;&#65292;&#23427;&#30001;&#19977;&#20010;&#27169;&#22359;&#32452;&#25104;&#65306;&#22270;&#29983;&#25104;&#27169;&#22359;&#65288;GGM&#65289;&#12289;&#22522;&#20110;&#30456;&#20284;&#24615;&#30340;&#31751;&#26500;&#24314;&#27169;&#22359;&#65288;SCBM&#65289;&#21644;&#21452;&#23618;&#32858;&#21512;&#27169;&#22359;&#12290;
&lt;/p&gt;
&lt;p&gt;
Emotion recognition in conversation (ERC) has received increasing attention from researchers due to its wide range of applications. As conversation has a natural graph structure, numerous approaches used to model ERC based on graph convolutional networks (GCNs) have yielded significant results. However, the aggregation approach of traditional GCNs suffers from the node information redundancy problem, leading to node discriminant information loss. Additionally, single-layer GCNs lack the capacity to capture long-range contextual information from the graph. Furthermore, the majority of approaches are based on textual modality or stitching together different modalities, resulting in a weak ability to capture interactions between modalities. To address these problems, we present the relational bilevel aggregation graph convolutional network (RBA-GCN), which consists of three modules: the graph generation module (GGM), similarity-based cluster building module (SCBM) and bilevel aggregation 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;DocPrompt&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#25991;&#26723;&#38382;&#31572;&#20219;&#21153;&#65292;&#20855;&#26377;&#24378;&#22823;&#30340;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#24615;&#33021;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;DocPrompt&#27169;&#22411;&#32463;&#36807;&#36830;&#32493;&#39044;&#35757;&#32451;&#21518;&#22312;&#25991;&#26723;&#38382;&#31572;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#24322;&#65292;&#22823;&#22823;&#25552;&#39640;&#20102;&#20132;&#20184;&#25928;&#29575;&#21644;&#27169;&#22411;&#24615;&#33021;&#65292;&#38477;&#20302;&#20102;&#27880;&#37322;&#25104;&#26412;&#21644;&#21171;&#21160;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2308.10959</link><description>&lt;p&gt;
DocPrompt: &#22823;&#35268;&#27169;&#36830;&#32493;&#39044;&#35757;&#32451;&#29992;&#20110;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#25991;&#26723;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
DocPrompt: Large-scale continue pretrain for zero-shot and few-shot document question answering. (arXiv:2308.10959v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10959
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;DocPrompt&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#25991;&#26723;&#38382;&#31572;&#20219;&#21153;&#65292;&#20855;&#26377;&#24378;&#22823;&#30340;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#24615;&#33021;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;DocPrompt&#27169;&#22411;&#32463;&#36807;&#36830;&#32493;&#39044;&#35757;&#32451;&#21518;&#22312;&#25991;&#26723;&#38382;&#31572;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#24322;&#65292;&#22823;&#22823;&#25552;&#39640;&#20102;&#20132;&#20184;&#25928;&#29575;&#21644;&#27169;&#22411;&#24615;&#33021;&#65292;&#38477;&#20302;&#20102;&#27880;&#37322;&#25104;&#26412;&#21644;&#21171;&#21160;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;DocPrompt&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#25991;&#26723;&#38382;&#31572;&#20219;&#21153;&#65292;&#20855;&#26377;&#24378;&#22823;&#30340;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24369;&#30417;&#30563;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#12289;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#38454;&#27573;&#35757;&#32451;&#26041;&#27861;&#65292;&#20197;&#21450;&#19968;&#31181;&#26032;&#39062;&#30340;&#29702;&#35299;&#27169;&#22411;&#21644;&#29983;&#25104;&#27169;&#22411;&#38598;&#25104;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#25991;&#26723;&#38382;&#31572;&#20219;&#21153;&#20013;&#65292;&#32463;&#36807;&#36830;&#32493;&#39044;&#35757;&#32451;&#30340;DocPrompt&#27169;&#22411;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#30340;&#24378;&#22522;&#32447;&#27169;&#22411;&#12290;&#36825;&#31181;&#26041;&#27861;&#26497;&#22823;&#22320;&#25552;&#39640;&#20102;&#25991;&#26723;&#38382;&#31572;&#23458;&#25143;&#39033;&#30446;&#30340;&#20132;&#20184;&#25928;&#29575;&#21644;&#27169;&#22411;&#24615;&#33021;&#65292;&#38477;&#20302;&#20102;&#27880;&#37322;&#25104;&#26412;&#21644;&#21171;&#21160;&#25104;&#26412;&#12290;&#25105;&#20204;&#30340;&#28436;&#31034;&#21487;&#20197;&#22312;https://huggingface.co/spaces/PaddlePaddle/ERNIE-Layout&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose Docprompt for document question answering tasks with powerful zero-shot and few-shot performance. We proposed a novel weakly supervised data generation method, a novel multl-stage training method and a novel understanding model &amp; generation model ensemble method. Experiment results show that the Docprompt model after continue pretrain significantly outperforms the existing strong baseline models on document question answering tasks. This method greatly improves the delivery efficiency and model performance of document question answering customer projects, reducing annotation costs and labor costs. Our demo can be found at https://huggingface.co/spaces/PaddlePaddle/ERNIE-Layout.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#27604;&#36739;&#20102;ChatGPT&#21644;&#20154;&#31867;&#22312;&#35789;&#27719;&#21644;&#35789;&#27719;&#20016;&#23500;&#24230;&#26041;&#38754;&#30340;&#24046;&#24322;&#65292;&#30740;&#31350;&#21457;&#29616;&#20351;&#29992;ChatGPT&#31561;&#24037;&#20855;&#20250;&#23545;&#35789;&#27719;&#20351;&#29992;&#21644;&#35789;&#27719;&#20016;&#23500;&#24230;&#20135;&#29983;&#24433;&#21709;&#65292;&#36825;&#21487;&#33021;&#20250;&#23545;&#35821;&#35328;&#28436;&#21464;&#20135;&#29983;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2308.07462</link><description>&lt;p&gt;
&#29609;&#24324;&#25991;&#23383;&#65306;&#27604;&#36739;ChatGPT&#21644;&#20154;&#31867;&#30340;&#35789;&#27719;&#21644;&#35789;&#27719;&#20016;&#23500;&#24230;
&lt;/p&gt;
&lt;p&gt;
Playing with Words: Comparing the Vocabulary and Lexical Richness of ChatGPT and Humans. (arXiv:2308.07462v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07462
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#27604;&#36739;&#20102;ChatGPT&#21644;&#20154;&#31867;&#22312;&#35789;&#27719;&#21644;&#35789;&#27719;&#20016;&#23500;&#24230;&#26041;&#38754;&#30340;&#24046;&#24322;&#65292;&#30740;&#31350;&#21457;&#29616;&#20351;&#29992;ChatGPT&#31561;&#24037;&#20855;&#20250;&#23545;&#35789;&#27719;&#20351;&#29992;&#21644;&#35789;&#27719;&#20016;&#23500;&#24230;&#20135;&#29983;&#24433;&#21709;&#65292;&#36825;&#21487;&#33021;&#20250;&#23545;&#35821;&#35328;&#28436;&#21464;&#20135;&#29983;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;GPT&#65289;&#21644;ChatGPT&#31561;&#24037;&#20855;&#30340;&#24341;&#20837;&#24341;&#21457;&#20102;&#19968;&#22330;&#38761;&#21629;&#65292;&#21487;&#20197;&#25913;&#21464;&#25991;&#26412;&#29983;&#25104;&#30340;&#26041;&#24335;&#12290;&#36825;&#23545;&#35835;&#32773;&#30340;&#35821;&#35328;&#33021;&#21147;&#20197;&#21450;&#26032;&#22411;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#30340;&#22521;&#35757;&#26159;&#21542;&#20250;&#20135;&#29983;&#24433;&#21709;&#20855;&#26377;&#35768;&#22810;&#21547;&#20041;&#65311;&#23427;&#26159;&#21542;&#20250;&#24433;&#21709;&#35821;&#35328;&#30340;&#28436;&#21464;&#65311;&#25105;&#20204;&#20851;&#27880;&#35821;&#35328;&#30340;&#19968;&#20010;&#29305;&#23450;&#26041;&#38754;&#65306;&#35789;&#35821;&#65307;&#22312;&#32534;&#20889;&#32473;&#23450;&#25991;&#26412;&#26102;&#65292;&#20351;&#29992;ChatGPT&#31561;&#24037;&#20855;&#20250;&#22686;&#21152;&#25110;&#20943;&#23569;&#20351;&#29992;&#30340;&#35789;&#27719;&#37327;&#25110;&#35789;&#27719;&#20016;&#23500;&#24230;&#65288;&#29702;&#35299;&#20026;&#20070;&#38754;&#25110;&#21475;&#22836;&#34920;&#36798;&#20013;&#20351;&#29992;&#30340;&#19981;&#21516;&#35789;&#27719;&#25968;&#37327;&#65289;&#65311;&#36825;&#23545;&#35789;&#35821;&#26377;&#24433;&#21709;&#65292;&#22240;&#20026;&#26410;&#21253;&#21547;&#22312;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#30340;&#20869;&#23481;&#20013;&#30340;&#35789;&#35821;&#24448;&#24448;&#20250;&#21464;&#24471;&#36234;&#26469;&#36234;&#19981;&#21463;&#27426;&#36814;&#65292;&#24182;&#26368;&#32456;&#21487;&#33021;&#28040;&#22833;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23545;ChatGPT&#21644;&#20154;&#31867;&#30340;&#35789;&#27719;&#21644;&#35789;&#27719;&#20016;&#23500;&#24230;&#36827;&#34892;&#20102;&#21021;&#27493;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
The introduction of Artificial Intelligence (AI) generative language models such as GPT (Generative Pre-trained Transformer) and tools such as ChatGPT has triggered a revolution that can transform how text is generated. This has many implications, for example, as AI-generated text becomes a significant fraction of the text in many disciplines, would this have an effect on the language capabilities of readers and also on the training of newer AI tools? Would it affect the evolution of languages? Focusing on one specific aspect of the language: words; will the use of tools such as ChatGPT increase or reduce the vocabulary used or the lexical richness (understood as the number of different words used in a written or oral production) when writing a given text? This has implications for words, as those not included in AI-generated content will tend to be less and less popular and may eventually be lost. In this work, we perform an initial comparison of the vocabulary and lexical richness of
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22522;&#20110;&#22522;&#37329;&#20250;&#27169;&#22411;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#22312;&#25972;&#20010;&#29983;&#21629;&#21608;&#26399;&#20013;&#25152;&#38754;&#20020;&#30340;&#27835;&#29702;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#21033;&#29992;&#21306;&#22359;&#38142;&#23454;&#29616;&#21435;&#20013;&#24515;&#21270;&#27835;&#29702;&#30340;&#26550;&#26500;&#12290;</title><link>http://arxiv.org/abs/2308.05962</link><description>&lt;p&gt;
&#22522;&#20110;&#21306;&#22359;&#38142;&#30340;&#22522;&#37329;&#20250;&#27169;&#22411;&#31995;&#32479;&#30340;&#21435;&#20013;&#24515;&#21270;&#27835;&#29702;&#65306;&#25506;&#35752;&#21306;&#22359;&#38142;&#22312;&#36127;&#36131;&#20219;&#30340;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Decentralised Governance for Foundation Model based Systems: Exploring the Role of Blockchain in Responsible AI. (arXiv:2308.05962v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05962
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22522;&#20110;&#22522;&#37329;&#20250;&#27169;&#22411;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#22312;&#25972;&#20010;&#29983;&#21629;&#21608;&#26399;&#20013;&#25152;&#38754;&#20020;&#30340;&#27835;&#29702;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#21033;&#29992;&#21306;&#22359;&#38142;&#23454;&#29616;&#21435;&#20013;&#24515;&#21270;&#27835;&#29702;&#30340;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#37329;&#20250;&#27169;&#22411;&#22240;&#20854;&#21331;&#36234;&#30340;&#33021;&#21147;&#21644;&#28508;&#21147;&#22312;&#20840;&#29699;&#33539;&#22260;&#20869;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#65292;&#33021;&#22815;&#25191;&#34892;&#21508;&#31181;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#20154;&#20204;&#25285;&#24515;&#22522;&#20110;&#22522;&#37329;&#20250;&#27169;&#22411;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#26159;&#21542;&#24471;&#21040;&#20102;&#36866;&#24403;&#30340;&#27835;&#29702;&#65292;&#20197;&#30830;&#20445;&#20854;&#21487;&#20449;&#24230;&#65292;&#24182;&#38450;&#27490;&#21487;&#33021;&#23545;&#20154;&#31867;&#12289;&#31038;&#20250;&#21644;&#29615;&#22659;&#36896;&#25104;&#20260;&#23475;&#30340;&#28389;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#22522;&#37329;&#20250;&#27169;&#22411;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#22312;&#25972;&#20010;&#29983;&#21629;&#21608;&#26399;&#20013;&#38754;&#20020;&#30340;&#20843;&#20010;&#27835;&#29702;&#25361;&#25112;&#65292;&#28041;&#21450;&#27835;&#29702;&#30340;&#19977;&#20010;&#22522;&#26412;&#32500;&#24230;&#65306;&#20915;&#31574;&#26435;&#12289;&#28608;&#21169;&#26426;&#21046;&#21644;&#38382;&#36131;&#21046;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#21306;&#22359;&#38142;&#20316;&#20026;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#30340;&#28508;&#21147;&#65292;&#36890;&#36807;&#25552;&#20379;&#20998;&#24067;&#24335;&#36134;&#26412;&#26469;&#20419;&#36827;&#21435;&#20013;&#24515;&#21270;&#30340;&#27835;&#29702;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26550;&#26500;&#65292;&#28436;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#21306;&#22359;&#38142;&#23454;&#29616;&#22522;&#37329;&#20250;&#27169;&#22411;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#27835;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Foundation models are increasingly attracting interest worldwide for their distinguished capabilities and potential to perform a wide variety of tasks. Nevertheless, people are concerned about whether foundation model based AI systems are properly governed to ensure trustworthiness of foundation model based AI systems and to prevent misuse that could harm humans, society and the environment. In this paper, we identify eight governance challenges in the entire lifecycle of foundation model based AI systems regarding the three fundamental dimensions of governance: decision rights, incentives, and accountability. Furthermore, we explore the potential of blockchain as a solution to address the challenges by providing a distributed ledger to facilitate decentralised governance. We present an architecture that demonstrates how blockchain can be leveraged to realise governance in foundation model based AI systems.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#19977;&#33410;&#27425;&#30340;&#23450;&#24615;&#30740;&#31350;&#65292;&#25506;&#31350;&#20102;&#20154;&#31867;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#39044;&#20889;&#36807;&#31243;&#20013;&#30340;&#21512;&#20316;&#27169;&#24335;&#65292;&#24182;&#21457;&#29616;&#20102;&#19968;&#20010;&#19977;&#38454;&#27573;&#30340;&#20154;&#26426;&#20849;&#21019;&#36807;&#31243;&#65306;&#26500;&#24605;&#12289;&#21551;&#21457;&#21644;&#23454;&#26045;&#12290;&#22312;&#36825;&#20010;&#21512;&#20316;&#36807;&#31243;&#20013;&#65292;&#20154;&#31867;&#25198;&#28436;&#30528;&#20027;&#23548;&#35282;&#33394;&#12290;</title><link>http://arxiv.org/abs/2307.10811</link><description>&lt;p&gt;
"&#24863;&#35273;&#20687;&#26377;&#31532;&#20108;&#20010;&#24605;&#32500;": &#25506;&#31350;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#36827;&#34892;&#21019;&#24847;&#21487;&#20889;&#24615;&#39044;&#20889;&#30340;&#20154;&#26426;&#20849;&#21019;
&lt;/p&gt;
&lt;p&gt;
"It Felt Like Having a Second Mind": Investigating Human-AI Co-creativity in Prewriting with Large Language Models. (arXiv:2307.10811v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10811
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#19977;&#33410;&#27425;&#30340;&#23450;&#24615;&#30740;&#31350;&#65292;&#25506;&#31350;&#20102;&#20154;&#31867;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#39044;&#20889;&#36807;&#31243;&#20013;&#30340;&#21512;&#20316;&#27169;&#24335;&#65292;&#24182;&#21457;&#29616;&#20102;&#19968;&#20010;&#19977;&#38454;&#27573;&#30340;&#20154;&#26426;&#20849;&#21019;&#36807;&#31243;&#65306;&#26500;&#24605;&#12289;&#21551;&#21457;&#21644;&#23454;&#26045;&#12290;&#22312;&#36825;&#20010;&#21512;&#20316;&#36807;&#31243;&#20013;&#65292;&#20154;&#31867;&#25198;&#28436;&#30528;&#20027;&#23548;&#35282;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#20889;&#26159;&#22312;&#31532;&#19968;&#31295;&#20043;&#21069;&#21457;&#29616;&#21644;&#21457;&#23637;&#24605;&#24819;&#30340;&#36807;&#31243;&#65292;&#23427;&#38656;&#35201;&#21457;&#25955;&#24615;&#24605;&#32500;&#65292;&#36890;&#24120;&#28041;&#21450;&#21040;&#26080;&#32467;&#26500;&#30340;&#31574;&#30053;&#65292;&#22914;&#22270;&#34920;&#12289;&#27010;&#36848;&#21644;&#33258;&#30001;&#20889;&#20316;&#31561;&#12290;&#34429;&#28982;&#24050;&#32463;&#35777;&#26126;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#37117;&#26159;&#26377;&#29992;&#30340;&#65292;&#21253;&#25324;&#21019;&#24847;&#20889;&#20316;&#65292;&#20294;&#23545;&#29992;&#25143;&#22914;&#20309;&#19982;LLMs&#21512;&#20316;&#26469;&#25903;&#25345;&#39044;&#20889;&#30340;&#26041;&#24335;&#30693;&#20043;&#29978;&#23569;&#12290;&#22312;&#36825;&#31181;&#21019;&#36896;&#24615;&#36807;&#31243;&#20013;&#65292;LLMs&#30340;&#39318;&#36873;&#21512;&#20316;&#35282;&#33394;&#21644;&#20027;&#21160;&#24615;&#20063;&#19981;&#26126;&#30830;&#12290;&#20026;&#20102;&#30740;&#31350;&#20154;&#31867;&#19982;LLMs&#22312;&#39044;&#20889;&#36807;&#31243;&#20013;&#30340;&#21512;&#20316;&#27169;&#24335;&#21644;&#21160;&#21147;&#23398;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#19977;&#33410;&#27425;&#30340;&#23450;&#24615;&#30740;&#31350;&#65292;&#19982;15&#20301;&#21442;&#19982;&#32773;&#36827;&#34892;&#20102;&#20004;&#20010;&#21019;&#36896;&#24615;&#20219;&#21153;&#65306;&#20889;&#25925;&#20107;&#21644;&#20889;&#21475;&#21495;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#21512;&#20316;&#30340;&#39044;&#20889;&#36807;&#31243;&#20013;&#65292;&#20284;&#20046;&#23384;&#22312;&#30528;&#19968;&#20010;&#19977;&#38454;&#27573;&#36845;&#20195;&#30340;&#20154;&#26426;&#20849;&#21019;&#36807;&#31243;&#65292;&#21253;&#25324;&#26500;&#24605;&#12289;&#21551;&#21457;&#21644;&#23454;&#26045;&#38454;&#27573;&#12290;&#36825;&#20010;&#21512;&#20316;&#36807;&#31243;&#20197;&#20154;&#31867;&#22312;&#20027;&#23548;&#35282;&#33394;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prewriting is the process of discovering and developing ideas before a first draft, which requires divergent thinking and often implies unstructured strategies such as diagramming, outlining, free-writing, etc. Although large language models (LLMs) have been demonstrated to be useful for a variety of tasks including creative writing, little is known about how users would collaborate with LLMs to support prewriting. The preferred collaborative role and initiative of LLMs during such a creativity process is also unclear. To investigate human-LLM collaboration patterns and dynamics during prewriting, we conducted a three-session qualitative study with 15 participants in two creative tasks: story writing and slogan writing. The findings indicated that during collaborative prewriting, there appears to be a three-stage iterative Human-AI Co-creativity process that includes Ideation, Illumination, and Implementation stages. This collaborative process champions the human in a dominant role, in
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#29366;&#24577;&#37325;&#26500;&#25193;&#25955;&#31574;&#30053; (SRDP) &#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#26368;&#26032;&#30340;&#25193;&#25955;&#31574;&#30053;&#31867;&#20013;&#24341;&#20837;&#20102;&#29366;&#24577;&#37325;&#26500;&#29305;&#24449;&#23398;&#20064;&#65292;&#20197;&#35299;&#20915;&#33073;&#26426;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20998;&#24067;&#20559;&#31227;&#21644;&#26377;&#25928;&#34920;&#31034;&#31574;&#30053;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.04726</link><description>&lt;p&gt;
&#33073;&#26426;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#31163;&#25955;&#31574;&#30053;&#30340;&#25193;&#25955;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Diffusion Policies for Out-of-Distribution Generalization in Offline Reinforcement Learning. (arXiv:2307.04726v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04726
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#29366;&#24577;&#37325;&#26500;&#25193;&#25955;&#31574;&#30053; (SRDP) &#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#26368;&#26032;&#30340;&#25193;&#25955;&#31574;&#30053;&#31867;&#20013;&#24341;&#20837;&#20102;&#29366;&#24577;&#37325;&#26500;&#29305;&#24449;&#23398;&#20064;&#65292;&#20197;&#35299;&#20915;&#33073;&#26426;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20998;&#24067;&#20559;&#31227;&#21644;&#26377;&#25928;&#34920;&#31034;&#31574;&#30053;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33073;&#26426;&#24378;&#21270;&#23398;&#20064; (RL) &#26041;&#27861;&#21033;&#29992;&#20197;&#21069;&#30340;&#32463;&#39564;&#26469;&#23398;&#20064;&#27604;&#29992;&#20110;&#25968;&#25454;&#25910;&#38598;&#30340;&#34892;&#20026;&#31574;&#30053;&#26356;&#22909;&#30340;&#31574;&#30053;&#12290;&#19982;&#34892;&#20026;&#20811;&#38534;&#30456;&#21453;&#65292;&#34892;&#20026;&#20811;&#38534;&#20551;&#35774;&#25968;&#25454;&#26159;&#20174;&#19987;&#23478;&#28436;&#31034;&#20013;&#25910;&#38598;&#30340;&#65292;&#32780;&#33073;&#26426; RL &#21487;&#20197;&#20351;&#29992;&#38750;&#19987;&#23478;&#25968;&#25454;&#21644;&#22810;&#27169;&#24577;&#34892;&#20026;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#33073;&#26426; RL &#31639;&#27861;&#22312;&#22788;&#29702;&#20998;&#24067;&#20559;&#31227;&#21644;&#26377;&#25928;&#34920;&#31034;&#31574;&#30053;&#26041;&#38754;&#38754;&#20020;&#25361;&#25112;&#65292;&#22240;&#20026;&#35757;&#32451;&#36807;&#31243;&#20013;&#32570;&#20047;&#22312;&#32447;&#20132;&#20114;&#12290;&#20808;&#21069;&#20851;&#20110;&#33073;&#26426; RL &#30340;&#24037;&#20316;&#20351;&#29992;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#26469;&#34920;&#31034;&#25968;&#25454;&#38598;&#20013;&#30340;&#22810;&#27169;&#24577;&#34892;&#20026;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#24182;&#27809;&#26377;&#38024;&#23545;&#32531;&#35299;&#33073;&#26426;&#20998;&#24067;&#29366;&#24577;&#27867;&#21270;&#32780;&#21046;&#23450;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21517;&#20026;&#29366;&#24577;&#37325;&#26500;&#25193;&#25955;&#31574;&#30053; (SRDP)&#65292;&#23558;&#29366;&#24577;&#37325;&#26500;&#29305;&#24449;&#23398;&#20064;&#32435;&#20837;&#21040;&#26368;&#26032;&#30340;&#25193;&#25955;&#31574;&#30053;&#31867;&#20013;&#65292;&#20197;&#35299;&#20915;&#33073;&#26426;&#20998;&#24067;&#36890;&#29992;&#21270;&#38382;&#39064;&#12290;&#29366;&#24577;&#37325;&#26500;&#25439;&#22833;&#20419;&#36827;&#20102;&#26356;&#35814;&#32454;&#30340;&#25551;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
Offline Reinforcement Learning (RL) methods leverage previous experiences to learn better policies than the behavior policy used for data collection. In contrast to behavior cloning, which assumes the data is collected from expert demonstrations, offline RL can work with non-expert data and multimodal behavior policies. However, offline RL algorithms face challenges in handling distribution shifts and effectively representing policies due to the lack of online interaction during training. Prior work on offline RL uses conditional diffusion models to represent multimodal behavior in the dataset. Nevertheless, these methods are not tailored toward alleviating the out-of-distribution state generalization. We introduce a novel method, named State Reconstruction for Diffusion Policies (SRDP), incorporating state reconstruction feature learning in the recent class of diffusion policies to address the out-of-distribution generalization problem. State reconstruction loss promotes more descript
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#39318;&#27425;&#20351;&#29992;&#35821;&#27861;&#28436;&#21270;&#33258;&#21160;&#35774;&#35745;&#35821;&#20041;&#30456;&#20284;&#24615;&#38598;&#21512;&#65292;&#36890;&#36807;&#33258;&#21160;&#36873;&#25321;&#21644;&#32858;&#21512;&#20505;&#36873;&#24230;&#37327;&#26469;&#20248;&#21270;&#38598;&#21512;&#19982;&#20154;&#31867;&#21028;&#26029;&#30340;&#30456;&#20851;&#24615;&#65292;&#25552;&#39640;&#30456;&#20284;&#24230;&#35780;&#20272;&#20934;&#30830;&#24615;&#65292;&#24182;&#35777;&#26126;&#20102;&#20351;&#29992;&#38598;&#21512;&#23545;&#35821;&#20041;&#30456;&#20284;&#24615;&#20219;&#21153;&#30340;&#30410;&#22788;&#12290;</title><link>http://arxiv.org/abs/2307.00925</link><description>&lt;p&gt;
&#20351;&#29992;&#35821;&#27861;&#28436;&#21270;&#33258;&#21160;&#35774;&#35745;&#35821;&#20041;&#30456;&#20284;&#24615;&#38598;&#21512;
&lt;/p&gt;
&lt;p&gt;
Automatic Design of Semantic Similarity Ensembles Using Grammatical Evolution. (arXiv:2307.00925v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00925
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#39318;&#27425;&#20351;&#29992;&#35821;&#27861;&#28436;&#21270;&#33258;&#21160;&#35774;&#35745;&#35821;&#20041;&#30456;&#20284;&#24615;&#38598;&#21512;&#65292;&#36890;&#36807;&#33258;&#21160;&#36873;&#25321;&#21644;&#32858;&#21512;&#20505;&#36873;&#24230;&#37327;&#26469;&#20248;&#21270;&#38598;&#21512;&#19982;&#20154;&#31867;&#21028;&#26029;&#30340;&#30456;&#20851;&#24615;&#65292;&#25552;&#39640;&#30456;&#20284;&#24230;&#35780;&#20272;&#20934;&#30830;&#24615;&#65292;&#24182;&#35777;&#26126;&#20102;&#20351;&#29992;&#38598;&#21512;&#23545;&#35821;&#20041;&#30456;&#20284;&#24615;&#20219;&#21153;&#30340;&#30410;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#20041;&#30456;&#20284;&#24615;&#24230;&#37327;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#22810;&#31181;&#19982;&#35745;&#31639;&#26426;&#30456;&#20851;&#30340;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#27809;&#26377;&#21333;&#19968;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#24230;&#37327;&#36866;&#29992;&#20110;&#25152;&#26377;&#20219;&#21153;&#65292;&#30740;&#31350;&#20154;&#21592;&#32463;&#24120;&#20351;&#29992;&#38598;&#21512;&#31574;&#30053;&#26469;&#30830;&#20445;&#24615;&#33021;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#35774;&#35745;&#35821;&#20041;&#30456;&#20284;&#24615;&#38598;&#21512;&#30340;&#26041;&#27861;&#12290;&#20107;&#23454;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#39318;&#27425;&#20351;&#29992;&#35821;&#27861;&#28436;&#21270;&#26469;&#33258;&#21160;&#36873;&#25321;&#21644;&#32858;&#21512;&#19968;&#32452;&#20505;&#36873;&#24230;&#37327;&#65292;&#20197;&#21019;&#24314;&#19968;&#20010;&#26368;&#22823;&#21270;&#19982;&#20154;&#31867;&#21028;&#26029;&#30456;&#20851;&#24615;&#30340;&#38598;&#21512;&#12290;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#24182;&#19982;&#26368;&#20808;&#36827;&#30340;&#38598;&#21512;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#32467;&#26524;&#26174;&#31034;&#23427;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#30456;&#20284;&#24230;&#35780;&#20272;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#26082;&#23637;&#31034;&#20102;&#20351;&#29992;&#35821;&#27861;&#28436;&#21270;&#26469;&#33258;&#21160;&#27604;&#36739;&#25991;&#26412;&#30340;&#28508;&#21147;&#65292;&#20063;&#35777;&#26126;&#20102;&#20351;&#29992;&#38598;&#21512;&#23545;&#35821;&#20041;&#30456;&#20284;&#24615;&#20219;&#21153;&#30340;&#30410;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
Semantic similarity measures are widely used in natural language processing to catalyze various computer-related tasks. However, no single semantic similarity measure is the most appropriate for all tasks, and researchers often use ensemble strategies to ensure performance. This research work proposes a method for automatically designing semantic similarity ensembles. In fact, our proposed method uses grammatical evolution, for the first time, to automatically select and aggregate measures from a pool of candidates to create an ensemble that maximizes correlation to human judgment. The method is evaluated on several benchmark datasets and compared to state-of-the-art ensembles, showing that it can significantly improve similarity assessment accuracy and outperform existing methods in some cases. As a result, our research demonstrates the potential of using grammatical evolution to automatically compare text and prove the benefits of using ensembles for semantic similarity tasks. The so
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#31070;&#32463;&#28151;&#21512;&#25928;&#24212;&#65288;NME&#65289;&#27169;&#22411;&#65292;&#29992;&#20110;&#20010;&#24615;&#21270;&#39044;&#27979;&#65292;&#24182;&#36890;&#36807;&#32467;&#21512;&#20010;&#20154;&#36890;&#29992;&#21644;&#20010;&#20154;&#29305;&#23450;&#21442;&#25968;&#26469;&#32771;&#34385;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#36235;&#21183;&#12290;</title><link>http://arxiv.org/abs/2306.08149</link><description>&lt;p&gt;
&#38750;&#32447;&#24615;&#20010;&#24615;&#21270;&#39044;&#27979;&#30340;&#31070;&#32463;&#28151;&#21512;&#25928;&#24212;
&lt;/p&gt;
&lt;p&gt;
Neural Mixed Effects for Nonlinear Personalized Predictions. (arXiv:2306.08149v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08149
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#31070;&#32463;&#28151;&#21512;&#25928;&#24212;&#65288;NME&#65289;&#27169;&#22411;&#65292;&#29992;&#20110;&#20010;&#24615;&#21270;&#39044;&#27979;&#65292;&#24182;&#36890;&#36807;&#32467;&#21512;&#20010;&#20154;&#36890;&#29992;&#21644;&#20010;&#20154;&#29305;&#23450;&#21442;&#25968;&#26469;&#32771;&#34385;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#36235;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20010;&#24615;&#21270;&#39044;&#27979;&#26159;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#26681;&#25454;&#36807;&#21435;&#26631;&#35760;&#35266;&#27979;&#39044;&#27979;&#19968;&#20010;&#20154;&#26410;&#26469;&#30340;&#35266;&#27979;&#20540;&#65292;&#36890;&#24120;&#29992;&#20110;&#36830;&#32493;&#20219;&#21153;&#65292;&#20363;&#22914;&#39044;&#27979;&#26085;&#24120;&#24773;&#32490;&#35780;&#20998;&#12290;&#22312;&#36827;&#34892;&#20010;&#24615;&#21270;&#39044;&#27979;&#26102;&#65292;&#27169;&#22411;&#21487;&#20197;&#32467;&#21512;&#20004;&#31181;&#36235;&#21183;&#65306;&#65288;a&#65289;&#36328;&#20154;&#20849;&#20139;&#30340;&#36235;&#21183;&#65292;&#21363;&#20010;&#20154;&#36890;&#29992;&#36235;&#21183;&#65292;&#20363;&#22914;&#21608;&#26411;&#26356;&#24320;&#24515;&#65292;&#21644;&#65288;b&#65289;&#27599;&#20010;&#20154;&#29420;&#29305;&#30340;&#36235;&#21183;&#65292;&#21363;&#20010;&#20154;&#29305;&#23450;&#30340;&#36235;&#21183;&#65292;&#20363;&#22914;&#27599;&#21608;&#26377;&#19968;&#27425;&#21387;&#21147;&#22823;&#30340;&#20250;&#35758;&#12290;&#28151;&#21512;&#25928;&#24212;&#27169;&#22411;&#26159;&#19968;&#31181;&#27969;&#34892;&#30340;&#32479;&#35745;&#27169;&#22411;&#65292;&#29992;&#20110;&#36890;&#36807;&#32452;&#21512;&#20010;&#20154;&#36890;&#29992;&#21644;&#20010;&#20154;&#29305;&#23450;&#21442;&#25968;&#26469;&#30740;&#31350;&#36825;&#20004;&#31181;&#36235;&#21183;&#12290;&#23613;&#31649;&#29616;&#22312;&#32447;&#24615;&#28151;&#21512;&#25928;&#24212;&#27169;&#22411;&#36890;&#36807;&#23558;&#20854;&#19982;&#31070;&#32463;&#32593;&#32476;&#25972;&#21512;&#32780;&#21464;&#24471;&#36234;&#26469;&#36234;&#27969;&#34892;&#65292;&#20294;&#36825;&#31181;&#25972;&#21512;&#30446;&#21069;&#20165;&#38480;&#20110;&#32447;&#24615;&#20010;&#20154;&#29305;&#23450;&#21442;&#25968;&#65306;&#25490;&#38500;&#38750;&#32447;&#24615;&#20010;&#20154;&#29305;&#23450;&#36235;&#21183;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31070;&#32463;&#28151;&#21512;&#25928;&#24212;&#65288;NME&#65289;&#27169;&#22411;&#65292;&#20197;&#20248;&#21270;&#38750;&#32447;&#24615;&#20010;&#20154;&#29305;&#23450;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Personalized prediction is a machine learning approach that predicts a person's future observations based on their past labeled observations and is typically used for sequential tasks, e.g., to predict daily mood ratings. When making personalized predictions, a model can combine two types of trends: (a) trends shared across people, i.e., person-generic trends, such as being happier on weekends, and (b) unique trends for each person, i.e., person-specific trends, such as a stressful weekly meeting. Mixed effect models are popular statistical models to study both trends by combining person-generic and person-specific parameters. Though linear mixed effect models are gaining popularity in machine learning by integrating them with neural networks, these integrations are currently limited to linear person-specific parameters: ruling out nonlinear person-specific trends. In this paper, we propose Neural Mixed Effect (NME) models to optimize nonlinear person-specific parameters anywhere in a 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#20915;&#31574;&#26641;&#27169;&#22411;&#65292;&#21033;&#29992;&#25346;&#36215;&#30340;&#26641;&#30340;&#26041;&#24335;&#25552;&#39640;&#20102;&#20854;&#35299;&#37322;&#24615;&#21644;&#32479;&#35745;&#24615;&#33021;&#65292;&#36798;&#21040;&#20102;&#26080;&#38480;&#28145;&#24230;&#20915;&#31574;&#26641;&#30340;&#27700;&#24179;&#65292;&#24182;&#21487;&#19982;XGBoost&#31561;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#23218;&#32654;&#12290;</title><link>http://arxiv.org/abs/2306.06777</link><description>&lt;p&gt;
&#25552;&#39640;&#20915;&#31574;&#26641;&#35299;&#37322;&#24615;&#30340;&#26377;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;
Improving the Validity of Decision Trees as Explanations. (arXiv:2306.06777v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06777
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#20915;&#31574;&#26641;&#27169;&#22411;&#65292;&#21033;&#29992;&#25346;&#36215;&#30340;&#26641;&#30340;&#26041;&#24335;&#25552;&#39640;&#20102;&#20854;&#35299;&#37322;&#24615;&#21644;&#32479;&#35745;&#24615;&#33021;&#65292;&#36798;&#21040;&#20102;&#26080;&#38480;&#28145;&#24230;&#20915;&#31574;&#26641;&#30340;&#27700;&#24179;&#65292;&#24182;&#21487;&#19982;XGBoost&#31561;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#23218;&#32654;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22522;&#20110;&#34920;&#26684;&#25968;&#25454;&#30340;&#20998;&#31867;&#21644;&#39044;&#27979;&#20013;&#65292;&#20154;&#20204;&#32463;&#24120;&#20351;&#29992;&#22522;&#20110;&#26641;&#30340;&#27169;&#22411;&#12290;&#36825;&#21487;&#20197;&#22312;&#34920;&#26684;&#25968;&#25454;&#19978;&#19982;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#31454;&#20105;[&#21442;&#35265;Grinsztajn&#31561;&#20154;&#65292;NeurIPS 2022&#65292;arXiv&#65306;2207.08815]&#65292;&#24182;&#19988;&#22312;&#26576;&#20123;&#26465;&#20214;&#19979;&#26159;&#21487;&#35299;&#37322;&#30340;&#12290;&#21487;&#35299;&#37322;&#24615;&#21462;&#20915;&#20110;&#26641;&#30340;&#28145;&#24230;&#21644;&#27599;&#20010;&#21494;&#33410;&#28857;&#30340;&#20934;&#30830;&#24615;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#20010;&#20302;&#28145;&#24230;&#30340;&#26641;&#65292;&#20854;&#30446;&#26631;&#26159;&#26368;&#23567;&#21270;&#27599;&#20010;&#21494;&#33410;&#28857;&#19978;&#30340;&#26368;&#22823;&#38169;&#35823;&#20998;&#31867;&#65292;&#24182;&#20174;&#20302;&#28145;&#24230;&#26641;&#30340;&#27599;&#20010;&#21494;&#33410;&#28857;&#8220;&#25346;&#36215;&#8221;&#36827;&#19968;&#27493;&#30340;&#22522;&#20110;&#26641;&#30340;&#27169;&#22411;&#65288;&#20363;&#22914;&#26080;&#38480;&#28145;&#24230;&#30340;&#26641;&#65289;&#12290;&#20302;&#28145;&#24230;&#26641;&#26131;&#20110;&#35299;&#37322;&#65292;&#32780;&#32508;&#21512;&#20302;&#28145;&#24230;&#21644;&#25346;&#36215;&#30340;&#22522;&#20110;&#26641;&#30340;&#27169;&#22411;&#30340;&#25972;&#20307;&#32479;&#35745;&#24615;&#33021;&#20248;&#20110;&#20351;&#29992;&#32463;&#20856;&#26041;&#27861;&#65288;&#20363;&#22914;CART&#65289;&#35757;&#32451;&#30340;&#26080;&#38480;&#28145;&#24230;&#20915;&#31574;&#26641;&#65292;&#24182;&#19988;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65288;&#20363;&#22914;&#20248;&#21270;&#30340;XGBoost&#65289;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;
In classification and forecasting with tabular data, one often utilizes tree-based models. This can be competitive with deep neural networks on tabular data [cf. Grinsztajn et al., NeurIPS 2022, arXiv:2207.08815] and, under some conditions, explainable. The explainability depends on the depth of the tree and the accuracy in each leaf of the tree. Here, we train a low-depth tree with the objective of minimising the maximum misclassification error across each leaf node, and then ``suspend'' further tree-based models (e.g., trees of unlimited depth) from each leaf of the low-depth tree. The low-depth tree is easily explainable, while the overall statistical performance of the combined low-depth and suspended tree-based models improves upon decision trees of unlimited depth trained using classical methods (e.g., CART) and is comparable to state-of-the-art methods (e.g., well-tuned XGBoost).
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#24378;&#21270;&#23398;&#20064;&#30340;&#20013;&#20171;&#27169;&#22411;&#65292;&#21487;&#23454;&#29616;&#20195;&#29702;&#19982;LLM&#20043;&#38388;&#39640;&#25928;&#32463;&#27982;&#26377;&#25928;&#30340;&#20114;&#21160;&#65292;&#25552;&#39640;&#25928;&#29575;&#21644;&#25104;&#26412;&#25928;&#30410;&#12290;</title><link>http://arxiv.org/abs/2306.03604</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#20419;&#36827;&#31639;&#27861;&#20195;&#29702;&#19982;LLM&#20043;&#38388;&#30340;&#39640;&#25928;&#20114;&#21160;
&lt;/p&gt;
&lt;p&gt;
Enabling Efficient Interaction between an Algorithm Agent and an LLM: A Reinforcement Learning Approach. (arXiv:2306.03604v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03604
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#24378;&#21270;&#23398;&#20064;&#30340;&#20013;&#20171;&#27169;&#22411;&#65292;&#21487;&#23454;&#29616;&#20195;&#29702;&#19982;LLM&#20043;&#38388;&#39640;&#25928;&#32463;&#27982;&#26377;&#25928;&#30340;&#20114;&#21160;&#65292;&#25552;&#39640;&#25928;&#29575;&#21644;&#25104;&#26412;&#25928;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#21253;&#21547;&#20174;&#28023;&#37327;&#25991;&#26412;&#25968;&#25454;&#38598;&#20013;&#33719;&#21462;&#30340;&#22823;&#37327;&#19990;&#30028;&#30693;&#35782;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;LLMs&#21487;&#20197;&#36890;&#36807;&#25552;&#20379;&#39640;&#23618;&#25351;&#20196;&#26469;&#21327;&#21161;&#31639;&#27861;&#20195;&#29702;&#35299;&#20915;&#20855;&#26377;&#22797;&#26434;&#39034;&#24207;&#20915;&#31574;&#30340;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#19982;LLMs&#36827;&#34892;&#20132;&#20114;&#21487;&#33021;&#32791;&#26102;&#36739;&#38271;&#65292;&#22240;&#20026;&#22312;&#35768;&#22810;&#23454;&#38469;&#24773;&#20917;&#19979;&#65292;&#23427;&#20204;&#38656;&#35201;&#22823;&#37327;&#23384;&#20648;&#31354;&#38388;&#65292;&#21482;&#33021;&#37096;&#32626;&#22312;&#36828;&#31243;&#20113;&#26381;&#21153;&#22120;&#33410;&#28857;&#19978;&#12290;&#27492;&#22806;&#65292;&#20351;&#29992;&#21830;&#19994;LLMs&#21487;&#33021;&#25104;&#26412;&#24456;&#39640;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#33021;&#26681;&#25454;&#20351;&#29992;&#39057;&#29575;&#25910;&#36153;&#12290;&#26412;&#25991;&#25506;&#35752;&#22914;&#20309;&#23454;&#29616;&#20195;&#29702;&#19982;LLM&#20043;&#38388;&#30340;&#39640;&#25928;&#21644;&#32463;&#27982;&#26377;&#25928;&#30340;&#20114;&#21160;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#20013;&#20171;&#27169;&#22411;&#65292;&#20197;&#30830;&#23450;&#20309;&#26102;&#38656;&#35201;&#26597;&#35810;LLMs&#20197;&#23436;&#25104;&#30446;&#26631;&#20219;&#21153;&#30340;&#39640;&#32423;&#25351;&#20196;&#12290;&#22312;&#28041;&#21450;&#35268;&#21010;&#23376;&#30446;&#26631;&#30340;4&#20010;MiniGrid&#29615;&#22659;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#23398;&#20064;&#35299;&#20915;&#30446;&#26631;&#20219;&#21153;&#65292;&#24182;&#25552;&#21319;&#20102;&#25928;&#29575;&#21644;&#25104;&#26412;&#25928;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) encode a vast amount of world knowledge acquired from massive text datasets. Recent studies have demonstrated that LLMs can assist an algorithm agent in solving complex sequential decision making tasks in embodied environments by providing high-level instructions. However, interacting with LLMs can be time-consuming, as in many practical scenarios, they require a significant amount of storage space that can only be deployed on remote cloud server nodes. Additionally, using commercial LLMs can be costly since they may charge based on usage frequency. In this paper, we explore how to enable efficient and cost-effective interactions between the agent and an LLM. We propose a reinforcement learning based mediator model that determines when it is necessary to consult LLMs for high-level instructions to accomplish a target task. Experiments on 4 MiniGrid environments that entail planning sub-goals demonstrate that our method can learn to solve target tasks with o
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#20013;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#30340;&#26377;&#25928;&#24615;&#21644;&#38480;&#21046;&#65292;&#36890;&#36807;&#22312;&#29983;&#29289;&#21307;&#23398;&#30693;&#35782;&#22270;&#35889;&#19978;&#30340;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#22312;&#24615;&#33021;&#21644;&#19979;&#28216;&#29992;&#36884;&#26041;&#38754;&#30340;&#20248;&#21183;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#21487;&#35299;&#37322;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.19979</link><description>&lt;p&gt;
&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#20013;&#30340;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#65306;&#23427;&#20204;&#26377;&#29992;&#21527;&#65311;&#23545;&#36830;&#25509;&#39044;&#27979;&#12289;&#35268;&#21017;&#23398;&#20064;&#21644;&#19979;&#28216;&#22810;&#33647;&#29289;&#20219;&#21153;&#30340;&#25506;&#31350;
&lt;/p&gt;
&lt;p&gt;
Knowledge Graph Embeddings in the Biomedical Domain: Are They Useful? A Look at Link Prediction, Rule Learning, and Downstream Polypharmacy Tasks. (arXiv:2305.19979v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19979
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#20013;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#30340;&#26377;&#25928;&#24615;&#21644;&#38480;&#21046;&#65292;&#36890;&#36807;&#22312;&#29983;&#29289;&#21307;&#23398;&#30693;&#35782;&#22270;&#35889;&#19978;&#30340;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#22312;&#24615;&#33021;&#21644;&#19979;&#28216;&#29992;&#36884;&#26041;&#38754;&#30340;&#20248;&#21183;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#21487;&#35299;&#37322;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#26159;&#34920;&#31034;&#21644;&#32452;&#32455;&#22797;&#26434;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;&#24050;&#32463;&#25552;&#20986;&#20102;&#22810;&#31181;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#31639;&#27861;&#26469;&#23398;&#20064;&#21644;&#23436;&#21892;&#30693;&#35782;&#22270;&#35889;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#19968;&#39033;&#30740;&#31350;&#34920;&#26126;&#65292;&#24403;&#24212;&#29992;&#20110;&#29983;&#29289;&#21307;&#23398;&#30693;&#35782;&#22270;&#35889;&#26102;&#65292;&#36825;&#20123;&#23884;&#20837;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#26377;&#38480;&#65292;&#24341;&#21457;&#20102;&#23545;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#22312;&#29983;&#29289;&#21307;&#23398;&#29615;&#22659;&#20013;&#26159;&#21542;&#23384;&#22312;&#38480;&#21046;&#30340;&#30097;&#38382;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#23558;&#26368;&#20808;&#36827;&#30340;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#27169;&#22411;&#24212;&#29992;&#20110;&#26368;&#36817;&#30340;&#29983;&#29289;&#21307;&#23398;&#30693;&#35782;&#22270;&#35889;BioKG&#65292;&#35780;&#20272;&#20854;&#24615;&#33021;&#21644;&#28508;&#22312;&#30340;&#19979;&#28216;&#29992;&#36884;&#12290;&#22312;&#30456;&#21516;&#30340;&#29983;&#29289;&#21307;&#23398;&#30693;&#35782;&#22270;&#35889;&#19978;&#65292;&#25105;&#20204;&#22312;HITS@10&#24471;&#20998;&#26041;&#38754;&#30340;&#24615;&#33021;&#25913;&#36827;&#20102;&#19977;&#20493;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#22522;&#20110;&#35268;&#21017;&#30340;&#26041;&#27861;&#25552;&#20379;&#21487;&#35299;&#37322;&#30340;&#39044;&#27979;&#12290;&#36890;&#36807;&#23545;&#22235;&#20010;&#20219;&#21153;&#19978;&#34920;&#29616;&#26368;&#20339;&#30340;&#27169;&#22411;&#36827;&#34892;&#35780;&#20272;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#27169;&#22411;&#22312;&#23454;&#36341;&#20013;&#26159;&#21487;&#24212;&#29992;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge graphs are powerful tools for representing and organising complex biomedical data. Several knowledge graph embedding algorithms have been proposed to learn from and complete knowledge graphs. However, a recent study demonstrates the limited efficacy of these embedding algorithms when applied to biomedical knowledge graphs, raising the question of whether knowledge graph embeddings have limitations in biomedical settings. This study aims to apply state-of-the-art knowledge graph embedding models in the context of a recent biomedical knowledge graph, BioKG, and evaluate their performance and potential downstream uses. We achieve a three-fold improvement in terms of performance based on the HITS@10 score over previous work on the same biomedical knowledge graph. Additionally, we provide interpretable predictions through a rule-based method. We demonstrate that knowledge graph embedding models are applicable in practice by evaluating the best-performing model on four tasks that r
&lt;/p&gt;</description></item><item><title>ChatGPT&#20316;&#20026;&#32534;&#31243;&#21161;&#25163;&#22312;&#22788;&#29702;&#24120;&#35265;&#32534;&#31243;&#38382;&#39064;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#20063;&#23384;&#22312;&#19968;&#20123;&#23616;&#38480;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.11938</link><description>&lt;p&gt;
ChatGPT&#26159;&#21542;&#26159;&#32456;&#26497;&#32534;&#31243;&#21161;&#25163;-&#23427;&#26377;&#22810;&#36828;&#65311;
&lt;/p&gt;
&lt;p&gt;
Is ChatGPT the Ultimate Programming Assistant -- How far is it?. (arXiv:2304.11938v2 [cs.SE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11938
&lt;/p&gt;
&lt;p&gt;
ChatGPT&#20316;&#20026;&#32534;&#31243;&#21161;&#25163;&#22312;&#22788;&#29702;&#24120;&#35265;&#32534;&#31243;&#38382;&#39064;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#20063;&#23384;&#22312;&#19968;&#20123;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;ChatGPT LLM&#24341;&#36215;&#20102;&#26497;&#22823;&#20851;&#27880;&#65306;&#23427;&#21487;&#20197;&#29992;&#20316;&#35752;&#35770;&#28304;&#20195;&#30721;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#25552;&#31034;&#24314;&#35758;&#26356;&#25913;&#65292;&#25552;&#20379;&#25551;&#36848;&#29978;&#33267;&#29983;&#25104;&#20195;&#30721;&#12290;&#20856;&#22411;&#30340;&#28436;&#31034;&#36890;&#24120;&#38598;&#20013;&#22312;&#29616;&#26377;&#30340;&#22522;&#20934;&#19978;&#65292;&#36825;&#20123;&#22522;&#20934;&#21487;&#33021;&#22312;&#27169;&#22411;&#35757;&#32451;&#20013;&#20351;&#29992;&#36807;&#65288;&#21363;&#25968;&#25454;&#27844;&#28431;&#65289;&#12290;&#20026;&#20102;&#35780;&#20272;&#23558;LLM&#29992;&#20316;&#31243;&#24207;&#21592;&#30340;&#26377;&#29992;&#21161;&#25163;&#26426;&#22120;&#20154;&#30340;&#21487;&#34892;&#24615;&#65292;&#25105;&#20204;&#24517;&#39035;&#35780;&#20272;&#20854;&#22312;&#26410;&#30693;&#38382;&#39064;&#19978;&#30340;&#23454;&#38469;&#33021;&#21147;&#20197;&#21450;&#20854;&#23545;&#21508;&#31181;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;ChatGPT&#20316;&#20026;&#20840;&#33258;&#21160;&#32534;&#31243;&#21161;&#25163;&#30340;&#28508;&#21147;&#30340;&#32463;&#39564;&#30740;&#31350;&#65292;&#37325;&#28857;&#20851;&#27880;&#20195;&#30721;&#29983;&#25104;&#12289;&#31243;&#24207;&#20462;&#22797;&#21644;&#20195;&#30721;&#25688;&#35201;&#30340;&#20219;&#21153;&#12290;&#35813;&#30740;&#31350;&#35843;&#26597;&#20102;ChatGPT&#22312;&#24120;&#35265;&#32534;&#31243;&#38382;&#39064;&#19978;&#30340;&#34920;&#29616;&#65292;&#24182;&#23558;&#20854;&#19982;&#20004;&#20010;&#22522;&#20934;&#19978;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#22312;&#22810;&#20010;&#21457;&#29616;&#20013;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#26174;&#31034;ChatGPT&#22312;&#22788;&#29702;&#24120;&#35265;&#32534;&#31243;&#38382;&#39064;&#26041;&#38754;&#26159;&#26377;&#25928;&#30340;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#20063;&#25581;&#31034;&#20102;&#20854;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, the ChatGPT LLM has received great attention: it can be used as a bot for discussing source code, prompting it to suggest changes, provide descriptions or even generate code. Typical demonstrations generally focus on existing benchmarks, which may have been used in model training (i.e., data leakage). To assess the feasibility of using an LLM as a useful assistant bot for programmers, we must assess its realistic capabilities on unseen problems as well as its capabilities on various tasks. In this paper, we present an empirical study of ChatGPT's potential as a fully automated programming assistant, focusing on the tasks of code generation, program repair, and code summariziation. The study investigates ChatGPT's performance on common programming problems and compares it with state-of-the-art approaches on two benchmarks. Among several findings, our study shows that ChatGPT is effective in dealing with common programming problems. However, our experiments also reveal limitati
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;OLISIA&#65292;&#19968;&#20010;&#21475;&#35821;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#30340;&#32423;&#32852;&#31995;&#32479;&#65292;&#20351;&#29992;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#21644;DST&#27169;&#22411;&#65292;&#37319;&#29992;&#20960;&#20010;&#36866;&#24212;&#24615;&#31574;&#30053;&#26469;&#25552;&#39640;&#31283;&#20581;&#24615;&#65292;&#24182;&#22312;DSTC11 Track3&#20013;&#21462;&#24471;&#31532;&#19968;&#21517;&#30340;&#22909;&#25104;&#32489;&#12290;</title><link>http://arxiv.org/abs/2304.11073</link><description>&lt;p&gt;
OLISIA: &#19968;&#20010;&#29992;&#20110;&#21475;&#35821;&#21270;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#30340;&#32423;&#32852;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
OLISIA: a Cascade System for Spoken Dialogue State Tracking. (arXiv:2304.11073v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11073
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;OLISIA&#65292;&#19968;&#20010;&#21475;&#35821;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#30340;&#32423;&#32852;&#31995;&#32479;&#65292;&#20351;&#29992;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#21644;DST&#27169;&#22411;&#65292;&#37319;&#29992;&#20960;&#20010;&#36866;&#24212;&#24615;&#31574;&#30053;&#26469;&#25552;&#39640;&#31283;&#20581;&#24615;&#65292;&#24182;&#22312;DSTC11 Track3&#20013;&#21462;&#24471;&#31532;&#19968;&#21517;&#30340;&#22909;&#25104;&#32489;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#29366;&#24577;&#36319;&#36394; (DST) &#26159;&#21475;&#35821;&#23545;&#35805;&#31995;&#32479;&#30340;&#26680;&#24515;&#32452;&#25104;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#20851;&#20110;&#35813;&#20219;&#21153;&#30340;&#30740;&#31350;&#22823;&#22810;&#38598;&#20013;&#20110;&#32842;&#22825;&#26102;&#30340;&#35821;&#26009;&#24211;&#65292;&#24573;&#30053;&#20102;&#21475;&#35821;&#21644;&#20070;&#38754;&#35821;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102; OLISIA&#65292;&#36825;&#26159;&#19968;&#20010;&#32423;&#32852;&#31995;&#32479;&#65292;&#23427;&#38598;&#25104;&#20102;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035; (ASR) &#27169;&#22411;&#21644; DST &#27169;&#22411;&#12290;&#25105;&#20204;&#22312; ASR &#21644; DST &#27169;&#22359;&#20013;&#24341;&#20837;&#20102;&#20960;&#20010;&#36866;&#24212;&#24615;&#31574;&#30053;&#65292;&#20197;&#25552;&#39640;&#23545;&#21475;&#35821;&#23545;&#35805;&#30340;&#25972;&#21512;&#24615;&#21644;&#31283;&#20581;&#24615;&#12290;&#32463;&#36807;&#36825;&#20123;&#31574;&#30053;&#30340;&#35843;&#25972;&#65292;&#25105;&#20204;&#30340;&#31995;&#32479;&#22312; DSTC11 Track 3 &#20013;&#25490;&#21517;&#31532;&#19968;&#65292;&#36825;&#26159;&#19968;&#20010;&#35780;&#20272;&#21475;&#35821; DST &#24615;&#33021;&#30340;&#22522;&#20934;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#28145;&#20837;&#30340;&#32467;&#26524;&#20998;&#26512;&#65292;&#21457;&#29616;&#35268;&#33539;&#21270; ASR &#30340;&#36755;&#20986;&#21644;&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#35843;&#25972; DST &#30340;&#36755;&#20837;&#65292;&#20197;&#21450;&#22686;&#21152;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#22823;&#23567;&#65292;&#37117;&#22312;&#38477;&#20302;&#20070;&#38754;&#21644;&#21475;&#35821;&#23545;&#35805;&#20043;&#38388;&#24615;&#33021;&#24046;&#24322;&#26041;&#38754;&#21457;&#25381;&#20102;&#37325;&#35201;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Though Dialogue State Tracking (DST) is a core component of spoken dialogue systems, recent work on this task mostly deals with chat corpora, disregarding the discrepancies between spoken and written language.In this paper, we propose OLISIA, a cascade system which integrates an Automatic Speech Recognition (ASR) model and a DST model. We introduce several adaptations in the ASR and DST modules to improve integration and robustness to spoken conversations.With these adaptations, our system ranked first in DSTC11 Track 3, a benchmark to evaluate spoken DST. We conduct an in-depth analysis of the results and find that normalizing the ASR outputs and adapting the DST inputs through data augmentation, along with increasing the pre-trained models size all play an important role in reducing the performance discrepancy between written and spoken conversations.
&lt;/p&gt;</description></item><item><title>G2PTL&#26159;&#19968;&#31181;&#38754;&#21521;&#29289;&#27969;&#39046;&#22495;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#32467;&#21512;&#20102;&#25991;&#26412;&#39044;&#35757;&#32451;&#30340;&#35821;&#20041;&#23398;&#20064;&#33021;&#21147;&#21644;&#22270;&#24314;&#27169;&#30340;&#22320;&#29702;&#20851;&#31995;&#32534;&#30721;&#33021;&#21147;&#65292;&#33021;&#26377;&#25928;&#22320;&#32534;&#30721;&#20132;&#20184;&#22320;&#22336;&#20013;&#30340;&#20301;&#32622;&#20449;&#24687;&#65292;&#24182;&#22312;&#29289;&#27969;&#31995;&#32479;&#20013;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;</title><link>http://arxiv.org/abs/2304.01559</link><description>&lt;p&gt;
G2PTL&#65306;&#36866;&#29992;&#20110;&#29289;&#27969;&#31995;&#32479;&#30340;&#20132;&#20184;&#22320;&#22336;&#39044;&#35757;&#32451;&#27169;&#22411;&#21450;&#20854;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
G2PTL: A Pre-trained Model for Delivery Address and its Applications in Logistics System. (arXiv:2304.01559v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01559
&lt;/p&gt;
&lt;p&gt;
G2PTL&#26159;&#19968;&#31181;&#38754;&#21521;&#29289;&#27969;&#39046;&#22495;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#32467;&#21512;&#20102;&#25991;&#26412;&#39044;&#35757;&#32451;&#30340;&#35821;&#20041;&#23398;&#20064;&#33021;&#21147;&#21644;&#22270;&#24314;&#27169;&#30340;&#22320;&#29702;&#20851;&#31995;&#32534;&#30721;&#33021;&#21147;&#65292;&#33021;&#26377;&#25928;&#22320;&#32534;&#30721;&#20132;&#20184;&#22320;&#22336;&#20013;&#30340;&#20301;&#32622;&#20449;&#24687;&#65292;&#24182;&#22312;&#29289;&#27969;&#31995;&#32479;&#20013;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#29289;&#27969;&#31995;&#32479;&#30340;&#25968;&#25454;&#22522;&#30784;&#65292;&#22522;&#20110;&#25991;&#26412;&#30340;&#20132;&#20184;&#22320;&#22336;&#21253;&#21547;&#20016;&#23500;&#19988;&#20851;&#38190;&#30340;&#20301;&#32622;&#20449;&#24687;&#12290;&#22914;&#20309;&#26377;&#25928;&#22320;&#32534;&#30721;&#20132;&#20184;&#22320;&#22336;&#26159;&#25552;&#39640;&#21518;&#32493;&#20219;&#21153;&#22312;&#29289;&#27969;&#31995;&#32479;&#20013;&#24615;&#33021;&#30340;&#26680;&#24515;&#20219;&#21153;&#12290;&#38754;&#21521;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;(PTMs)&#24050;&#25104;&#20026;&#32534;&#30721;&#25991;&#26412;&#20013;&#35821;&#20041;&#20449;&#24687;&#30340;&#20027;&#35201;&#24037;&#20855;&#12290;&#34429;&#28982;&#22312;&#35768;&#22810;&#20219;&#21153;&#20013;&#30456;&#24403;&#26377;&#21069;&#36884;&#65292;&#20294;&#36825;&#20123;&#22522;&#20110;NLP&#30340;PTMs&#26410;&#33021;&#32534;&#30721;&#20132;&#20184;&#22320;&#22336;&#20013;&#30340;&#22320;&#29702;&#30693;&#35782;&#65292;&#36825;&#22312;&#29289;&#27969;&#31995;&#32479;(&#22914;&#33756;&#40479;&#31995;&#32479;)&#20013;&#22823;&#22823;&#21066;&#24369;&#20102;&#19982;&#20132;&#20184;&#30456;&#20851;&#30340;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#29289;&#27969;&#39046;&#22495;&#30340;&#22495;&#29305;&#23450;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#21517;&#20026;G2PTL&#65292;&#21363;&#20132;&#20184;&#22320;&#22336;&#22320;&#29702;&#20851;&#31995;-&#22270;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;G2PTL&#23558;&#25991;&#26412;&#39044;&#35757;&#32451;&#30340;&#35821;&#20041;&#23398;&#20064;&#33021;&#21147;&#19982;&#22270;&#24314;&#27169;&#30340;&#22320;&#29702;&#20851;&#31995;&#32534;&#30721;&#33021;&#21147;&#30456;&#32467;&#21512;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#21033;&#29992;&#30495;&#23454;&#30340;&#29289;&#27969;&#20132;&#20184;&#25968;&#25454;&#26500;&#24314;&#22320;&#29702;&#22270;&#65292;&#28982;&#21518;&#22312;&#22270;&#32467;&#26500;&#21270;&#25968;&#25454;&#19978;&#23545;&#27169;&#22411;&#36827;&#34892;&#39044;&#35757;&#32451;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;G2PTL&#33021;&#26377;&#25928;&#22320;&#32534;&#30721;&#22522;&#20110;&#25991;&#26412;&#30340;&#20132;&#20184;&#22320;&#22336;&#20013;&#30340;&#20301;&#32622;&#20449;&#24687;&#65292;&#24182;&#22312;&#19982;&#20132;&#20184;&#22320;&#22336;&#22788;&#29702;&#30456;&#20851;&#30340;&#20219;&#21153;&#20013;&#20248;&#20110;&#36890;&#29992;PTMs&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-based delivery addresses, as the data foundation for logistics systems, contain abundant and crucial location information. How to effectively encode the delivery address is a core task to boost the performance of downstream tasks in the logistics system. Pre-trained Models (PTMs) designed for Natural Language Process (NLP) have emerged as the dominant tools for encoding semantic information in text. Though promising, those NLP-based PTMs fall short of encoding geographic knowledge in the delivery address, which considerably trims down the performance of delivery-related tasks in logistic systems such as Cainiao. To tackle the above problem, we propose a domain-specific pre-trained model, named G2PTL, a Geography-Graph Pre-trained model for delivery address in Logistics field. G2PTL combines the semantic learning capabilities of text pre-training with the geographical-relationship encoding abilities of graph modeling. Specifically, we first utilize real-world logistics delivery dat
&lt;/p&gt;</description></item><item><title>KeGNN&#26159;&#19968;&#20010;&#31070;&#32463;&#31526;&#21495;&#26694;&#26550;&#65292;&#21487;&#20197;&#32467;&#21512;&#20808;&#21069;&#30340;&#30693;&#35782;&#26469;&#20248;&#21270;&#22270;&#25968;&#25454;&#19978;&#30340;&#33410;&#28857;&#20998;&#31867;&#21644;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2303.15487</link><description>&lt;p&gt;
&#30693;&#35782;&#22686;&#24378;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Knowledge Enhanced Graph Neural Networks. (arXiv:2303.15487v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15487
&lt;/p&gt;
&lt;p&gt;
KeGNN&#26159;&#19968;&#20010;&#31070;&#32463;&#31526;&#21495;&#26694;&#26550;&#65292;&#21487;&#20197;&#32467;&#21512;&#20808;&#21069;&#30340;&#30693;&#35782;&#26469;&#20248;&#21270;&#22270;&#25968;&#25454;&#19978;&#30340;&#33410;&#28857;&#20998;&#31867;&#21644;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#25968;&#25454;&#26159;&#26080;&#22788;&#19981;&#22312;&#30340;&#65292;&#24182;&#19988;&#20855;&#26377;&#21508;&#31181;&#24212;&#29992;&#65292;&#20363;&#22914;&#33258;&#28982;&#31185;&#23398;&#12289;&#31038;&#20132;&#32593;&#32476;&#25110;&#35821;&#20041;&#32593;&#12290;&#23613;&#31649;&#23500;&#21547;&#20449;&#24687;&#65292;&#20294;&#22270;&#24418;&#36890;&#24120;&#22122;&#22768;&#21644;&#19981;&#23436;&#25972;&#12290;&#22240;&#27492;&#65292;&#22270;&#34917;&#20840;&#20219;&#21153;&#65292;&#22914;&#33410;&#28857;&#20998;&#31867;&#25110;&#38142;&#25509;&#39044;&#27979;&#65292;&#24050;&#32463;&#21463;&#21040;&#20851;&#27880;&#12290;&#19968;&#26041;&#38754;&#65292;&#31070;&#32463;&#26041;&#27861;&#65288;&#22914;&#22270;&#31070;&#32463;&#32593;&#32476;&#65289;&#24050;&#32463;&#34987;&#35777;&#26126;&#26159;&#22788;&#29702;&#22122;&#22768;&#22270;&#30340;&#31283;&#20581;&#24037;&#20855;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#31526;&#21495;&#26041;&#27861;&#21487;&#20197;&#23545;&#22270;&#36827;&#34892;&#31934;&#30830;&#25512;&#29702;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;KeGNN&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#22312;&#22270;&#25968;&#25454;&#19978;&#23398;&#20064;&#30340;&#31070;&#32463;&#31526;&#21495;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#20004;&#31181;&#33539;&#20363;&#65292;&#24182;&#20801;&#35768;&#23558;&#20808;&#21069;&#30340;&#30693;&#35782;&#38598;&#25104;&#21040;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20013;&#12290;&#20174;&#26412;&#36136;&#19978;&#35762;&#65292;KeGNN&#30001;&#19968;&#20010;&#22270;&#31070;&#32463;&#32593;&#32476;&#32452;&#25104;&#65292;&#20854;&#20013;&#22522;&#20110;&#30446;&#26631;&#23558;&#30693;&#35782;&#22686;&#24378;&#23618;&#22534;&#21472;&#22312;&#20854;&#19978;&#65292;&#20197;&#20351;&#38024;&#23545;&#20808;&#21069;&#30693;&#35782;&#30340;&#39044;&#27979;&#24471;&#21040;&#20248;&#21270;&#12290;&#25105;&#20204;&#23558;KeGNN&#19982;&#20004;&#20010;&#26631;&#20934;&#22270;&#31070;&#32463;&#32593;&#32476;&#65306;&#22270;&#21367;&#31215;&#32593;&#32476;&#21644;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#19968;&#36215;&#23454;&#20363;&#21270;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#23558;&#20808;&#21069;&#30340;&#30693;&#35782;&#38598;&#25104;&#21040;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20013;&#21487;&#20197;&#25552;&#39640;&#33410;&#28857;&#20998;&#31867;&#21644;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph data is omnipresent and has a large variety of applications such as natural science, social networks or semantic web. Though rich in information, graphs are often noisy and incomplete. Therefore, graph completion tasks such as node classification or link prediction have gained attention. On the one hand, neural methods such as graph neural networks have proven to be robust tools for learning rich representations of noisy graphs. On the other hand, symbolic methods enable exact reasoning on graphs. We propose KeGNN, a neuro-symbolic framework for learning on graph data that combines both paradigms and allows for the integration of prior knowledge into a graph neural network model. In essence, KeGNN consists of a graph neural network as a base on which knowledge enhancement layers are stacked with the objective of refining predictions with respect to prior knowledge. We instantiate KeGNN in conjunction with two standard graph neural networks: Graph Convolutional Networks and Graph 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#26679;&#21270;&#21644;&#36924;&#30495;&#30340;&#22686;&#24378;&#26041;&#27861;&#65292;&#21487;&#20197;&#21019;&#24314;&#25972;&#20307;&#23545;&#35937;&#24182;&#28789;&#27963;&#22320;&#23450;&#20301;&#21644;&#26059;&#36716;&#23545;&#35937;&#65292;&#24182;&#30456;&#24212;&#22320;&#24212;&#29992;&#33258;&#36974;&#25377;&#21644;&#22806;&#36974;&#25377;&#12290;&#36890;&#36807;&#36845;&#20195;&#26500;&#24314;&#22810;&#20010;&#23545;&#35937;&#26469;&#25552;&#39640;&#25972;&#20307;&#23545;&#35937;&#26500;&#36896;&#30340;&#22810;&#26679;&#24615;&#65292;&#26500;&#36896;&#30340;&#23545;&#35937;&#21487;&#20197;&#22312;&#35757;&#32451;&#24103;&#20013;&#38543;&#26426;&#25918;&#32622;&#21644;&#26059;&#36716;&#12290;</title><link>http://arxiv.org/abs/2303.12743</link><description>&lt;p&gt;
DR.CPO&#65306;&#36890;&#36807;&#36845;&#20195;&#26500;&#24314;&#12289;&#38543;&#26426;&#25918;&#32622;&#21644; HPR &#36974;&#34109;&#23454;&#29616;&#30340;&#22810;&#26679;&#21270;&#21644;&#36924;&#30495;&#30340;&#19977;&#32500;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
DR.CPO: Diversified and Realistic 3D Augmentation via Iterative Construction, Random Placement, and HPR Occlusion. (arXiv:2303.12743v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12743
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#26679;&#21270;&#21644;&#36924;&#30495;&#30340;&#22686;&#24378;&#26041;&#27861;&#65292;&#21487;&#20197;&#21019;&#24314;&#25972;&#20307;&#23545;&#35937;&#24182;&#28789;&#27963;&#22320;&#23450;&#20301;&#21644;&#26059;&#36716;&#23545;&#35937;&#65292;&#24182;&#30456;&#24212;&#22320;&#24212;&#29992;&#33258;&#36974;&#25377;&#21644;&#22806;&#36974;&#25377;&#12290;&#36890;&#36807;&#36845;&#20195;&#26500;&#24314;&#22810;&#20010;&#23545;&#35937;&#26469;&#25552;&#39640;&#25972;&#20307;&#23545;&#35937;&#26500;&#36896;&#30340;&#22810;&#26679;&#24615;&#65292;&#26500;&#36896;&#30340;&#23545;&#35937;&#21487;&#20197;&#22312;&#35757;&#32451;&#24103;&#20013;&#38543;&#26426;&#25918;&#32622;&#21644;&#26059;&#36716;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#21160;&#39550;&#39542;&#20013;&#65292;&#25968;&#25454;&#22686;&#24378;&#24120;&#29992;&#20110;&#25913;&#36827;&#19977;&#32500;&#29289;&#20307;&#26816;&#27979;&#12290;&#26368;&#22522;&#26412;&#30340;&#26041;&#27861;&#21253;&#25324;&#25554;&#20837;&#22797;&#21046;&#23545;&#35937;&#21644;&#26059;&#36716;&#21644;&#32553;&#25918;&#25972;&#20010;&#35757;&#32451;&#24103;&#12290;&#20063;&#24050;&#32463;&#24320;&#21457;&#20102;&#35768;&#22810;&#21464;&#20307;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#19982;&#29616;&#23454;&#19990;&#30028;&#30340;&#21487;&#33021;&#24615;&#30456;&#27604;&#30456;&#24403;&#26377;&#38480;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#22810;&#26679;&#21270;&#21644;&#36924;&#30495;&#22686;&#24378;&#26041;&#27861;&#65292;&#21487;&#20197;&#28789;&#27963;&#22320;&#26500;&#36896;&#25972;&#20307;&#23545;&#35937;&#65292;&#33258;&#30001;&#22320;&#23450;&#20301;&#21644;&#26059;&#36716;&#23545;&#35937;&#65292;&#24182;&#30456;&#24212;&#22320;&#24212;&#29992;&#33258;&#36974;&#25377;&#21644;&#22806;&#36974;&#25377;&#12290;&#20026;&#20102;&#25552;&#39640;&#25972;&#20307;&#23545;&#35937;&#26500;&#36896;&#30340;&#22810;&#26679;&#24615;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#36845;&#20195;&#26041;&#27861;&#65292;&#23558;&#20174;&#29616;&#23454;&#19990;&#30028;&#35266;&#23519;&#21040;&#30340;&#22810;&#20010;&#23545;&#35937;&#38543;&#26426;&#32452;&#21512;&#25104;&#21333;&#20010;&#23545;&#35937;&#12290;&#19982;&#29616;&#26377;&#22686;&#24378;&#26041;&#27861;&#19981;&#21516;&#30340;&#26159;&#65292;&#26500;&#36896;&#30340;&#23545;&#35937;&#21487;&#20197;&#38543;&#26426;&#25918;&#32622;&#21644;&#26059;&#36716;&#22312;&#35757;&#32451;&#24103;&#20013;&#65292;&#22240;&#20026;&#36866;&#24403;&#30340;&#36974;&#25377;&#21487;&#20197;&#21453;&#26144;&#22312;&#26368;&#32456;&#25972;&#20307;&#23545;&#35937;&#20013;&#12290;&#26368;&#21518;&#65292;&#20026;&#20102;&#38450;&#27490;&#36807;&#24230;&#22686;&#24378;&#23548;&#33268;&#36807;&#25311;&#21512;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#20998;&#23618;&#36974;&#25377;&#27010;&#29575;&#35774;&#32622;&#65292;&#36890;&#36807;&#23545;&#35937;&#30340;&#20301;&#32622;&#21644;&#22823;&#23567;&#35843;&#25972;&#36974;&#25377;&#24378;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
In autonomous driving, data augmentation is commonly used for improving 3D object detection. The most basic methods include insertion of copied objects and rotation and scaling of the entire training frame. Numerous variants have been developed as well. The existing methods, however, are considerably limited when compared to the variety of the real world possibilities. In this work, we develop a diversified and realistic augmentation method that can flexibly construct a whole-body object, freely locate and rotate the object, and apply self-occlusion and external-occlusion accordingly. To improve the diversity of the whole-body object construction, we develop an iterative method that stochastically combines multiple objects observed from the real world into a single object. Unlike the existing augmentation methods, the constructed objects can be randomly located and rotated in the training frame because proper occlusions can be reflected to the whole-body objects in the final step. Fina
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#25935;&#24863;&#24230;&#24863;&#30693;&#30340;&#35270;&#35273;&#21442;&#25968;&#20302;&#25928;&#35843;&#25972;&#65288;SPT&#65289;&#26041;&#26696;&#65292;&#21487;&#20197;&#33258;&#36866;&#24212;&#22320;&#23558;&#21487;&#35757;&#32451;&#21442;&#25968;&#20998;&#37197;&#21040;&#20219;&#21153;&#29305;&#23450;&#30340;&#37325;&#35201;&#20301;&#32622;&#65292;&#20197;&#25552;&#39640;&#34920;&#31034;&#33021;&#21147;&#65292;&#36866;&#24212;&#39044;&#35757;&#32451;&#35270;&#35273;&#27169;&#22411;&#21040;&#19979;&#28216;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2303.08566</link><description>&lt;p&gt;
&#25935;&#24863;&#24230;&#24863;&#30693;&#30340;&#35270;&#35273;&#21442;&#25968;&#20302;&#25928;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
Sensitivity-Aware Visual Parameter-Efficient Tuning. (arXiv:2303.08566v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08566
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#25935;&#24863;&#24230;&#24863;&#30693;&#30340;&#35270;&#35273;&#21442;&#25968;&#20302;&#25928;&#35843;&#25972;&#65288;SPT&#65289;&#26041;&#26696;&#65292;&#21487;&#20197;&#33258;&#36866;&#24212;&#22320;&#23558;&#21487;&#35757;&#32451;&#21442;&#25968;&#20998;&#37197;&#21040;&#20219;&#21153;&#29305;&#23450;&#30340;&#37325;&#35201;&#20301;&#32622;&#65292;&#20197;&#25552;&#39640;&#34920;&#31034;&#33021;&#21147;&#65292;&#36866;&#24212;&#39044;&#35757;&#32451;&#35270;&#35273;&#27169;&#22411;&#21040;&#19979;&#28216;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#21442;&#25968;&#20302;&#25928;&#35843;&#25972;&#65288;VPET&#65289;&#24050;&#25104;&#20026;&#33258;&#36866;&#24212;&#39044;&#35757;&#32451;&#35270;&#35273;&#27169;&#22411;&#21040;&#19979;&#28216;&#20219;&#21153;&#30340;&#24378;&#21170;&#26367;&#20195;&#26041;&#27861;&#12290;&#29616;&#26377;VPET&#26041;&#27861;&#26681;&#25454;&#20154;&#24037;&#21551;&#21457;&#24335;&#26041;&#27861;&#23558;&#21487;&#35757;&#32451;&#21442;&#25968;&#24341;&#20837;&#19981;&#21516;&#20219;&#21153;&#30340;&#30456;&#21516;&#20301;&#32622;&#65292;&#24573;&#30053;&#39046;&#22495;&#24046;&#24322;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25935;&#24863;&#24230;&#24863;&#30693;&#30340;&#35270;&#35273;&#21442;&#25968;&#20302;&#25928;&#35843;&#25972;&#65288;SPT&#65289;&#26041;&#26696;&#65292;&#20197;&#33258;&#36866;&#24212;&#30340;&#26041;&#24335;&#20998;&#37197;&#21487;&#35757;&#32451;&#21442;&#25968;&#21040;&#20219;&#21153;&#29305;&#23450;&#30340;&#37325;&#35201;&#20301;&#32622;&#65292;&#32473;&#23450;&#25152;&#38656;&#30340;&#21487;&#35843;&#21442;&#25968;&#39044;&#31639;&#12290;&#26412;&#25991;&#39318;&#20808;&#20381;&#25454;&#25968;&#25454;&#30340;&#30456;&#20851;&#24615;&#24555;&#36895;&#35782;&#21035;&#29305;&#23450;&#20219;&#21153;&#25152;&#38656;&#35843;&#25972;&#30340;&#25935;&#24863;&#21442;&#25968;&#65292;&#28982;&#21518;&#25552;&#21319;&#34920;&#31034;&#33021;&#21147;&#65292;&#22686;&#22823;&#37325;&#35201;&#30340;&#26435;&#37325;&#30697;&#38453;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visual Parameter-Efficient Tuning (VPET) has become a powerful alternative for full fine-tuning so as to adapt pre-trained vision models to downstream tasks, which only tunes a small number of parameters while freezing the vast majority ones to ease storage burden and optimization difficulty. However, existing VPET methods introduce trainable parameters to the same positions across different tasks depending solely on human heuristics and neglect the domain gaps. To this end, we study where to introduce and how to allocate trainable parameters by proposing a novel Sensitivity-aware visual Parameter-efficient Tuning (SPT) scheme, which adaptively allocates trainable parameters to task-specific important positions given a desired tunable parameter budget. Specifically, our SPT first quickly identifies the sensitive parameters that require tuning for a given task in a data-dependent way. Next, our SPT further boosts the representational capability for the weight matrices whose number of se
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#24178;&#25200;&#29615;&#22659;&#19979;&#28145;&#24230;&#23398;&#20064;&#30340;&#25991;&#26412;&#35821;&#20041;&#36890;&#20449;&#31995;&#32479;&#30340;&#24615;&#33021;&#38480;&#21046;&#65292;&#21457;&#29616;&#24403;&#24178;&#25200;&#20449;&#21495;&#24378;&#24230;&#22686;&#22823;&#26102;&#65292;&#35813;&#31995;&#32479;&#20250;&#20135;&#29983;&#35821;&#20041;&#19978;&#26080;&#20851;&#30340;&#21477;&#23376;&#12290;</title><link>http://arxiv.org/abs/2302.14702</link><description>&lt;p&gt;
&#19968;&#20010;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#30340;&#25991;&#26412;&#35821;&#20041;&#36890;&#20449;&#22312;&#24178;&#25200;&#29615;&#22659;&#19979;&#30340;&#24615;&#33021;&#38480;&#21046;
&lt;/p&gt;
&lt;p&gt;
Performance Limits of a Deep Learning-Enabled Text Semantic Communication under Interference. (arXiv:2302.14702v2 [eess.SP] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.14702
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#24178;&#25200;&#29615;&#22659;&#19979;&#28145;&#24230;&#23398;&#20064;&#30340;&#25991;&#26412;&#35821;&#20041;&#36890;&#20449;&#31995;&#32479;&#30340;&#24615;&#33021;&#38480;&#21046;&#65292;&#21457;&#29616;&#24403;&#24178;&#25200;&#20449;&#21495;&#24378;&#24230;&#22686;&#22823;&#26102;&#65292;&#35813;&#31995;&#32479;&#20250;&#20135;&#29983;&#35821;&#20041;&#19978;&#26080;&#20851;&#30340;&#21477;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;(Deep Learning)&#25216;&#26415;&#22312;&#35821;&#20041;&#36890;&#20449;(SemCom)&#20013;&#30340;&#24212;&#29992;&#25104;&#20026;&#20102;6G&#30340;&#19968;&#31181;&#25512;&#21160;&#22240;&#32032;&#65292;&#24182;&#25215;&#35834;&#36890;&#36807;&#26368;&#23567;&#21270;&#26080;&#20851;&#20449;&#24687;&#20256;&#36755;&#26469;&#38477;&#20302;&#21151;&#32791;&#12289;&#24102;&#23485;&#28040;&#32791;&#21644;&#20256;&#36755;&#24310;&#36831;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#20197;&#35821;&#20041;&#20026;&#20013;&#24515;&#30340;&#35774;&#35745;&#30340;&#22909;&#22788;&#21487;&#33021;&#21463;&#21040;&#26080;&#32447;&#30005;&#39057;&#29575;&#24178;&#25200;(RFI)&#36896;&#25104;&#30340;&#26174;&#33879;&#35821;&#20041;&#22122;&#22768;&#30340;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#30693;&#35782;&#31354;&#30333;&#24182;&#20419;&#36827;&#20851;&#20110;&#24178;&#25200;&#40065;&#26834;&#24615;&#30340;SemCom&#22522;&#30784;&#30740;&#31350;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#21517;&#20026;DeepSC&#30340;&#25991;&#26412;SemCom&#31995;&#32479;&#22312;&#23384;&#22312;(&#22810;&#24178;&#25200;&#28304;) RFI&#30340;&#24773;&#20917;&#19979;&#30340;&#24615;&#33021;&#38480;&#21046;&#12290;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#22522;&#20110;&#27010;&#29575;&#30340;&#26694;&#26550;&#26469;&#36827;&#34892;SemCom&#65292;&#25105;&#20204;&#21457;&#29616;DeepSC&#22312;(&#22810;&#24178;&#25200;&#28304;) RFI&#21151;&#29575;&#21464;&#24471;&#38750;&#24120;&#22823;&#26102;&#20135;&#29983;&#35821;&#20041;&#19978;&#26080;&#20851;&#30340;&#21477;&#23376;&#12290;&#25105;&#20204;&#36824;&#25512;&#23548;&#20986;DeepSC&#30340;&#23454;&#38469;&#38480;&#21046;&#21644;&#19968;&#20010;&#19979;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
A deep learning (DL)-enabled semantic communication (SemCom) has emerged as a 6G enabler while promising to minimize power usage, bandwidth consumption, and transmission delay by minimizing irrelevant information transmission. However, the benefits of such a semantic-centric design can be limited by radio frequency interference (RFI) that causes substantial semantic noise. The impact of semantic noise due to interference can be alleviated using an interference-resistant and robust (IR$^2$) SemCom design. Nevertheless, no such design exists yet. To shed light on this knowledge gap and stimulate fundamental research on IR$^2$ SemCom, the performance limits of a text SemCom system named DeepSC are studied in the presence of (multi-interferer) RFI. By introducing a principled probabilistic framework for SemCom, we show that DeepSC produces semantically irrelevant sentences as the power of (multi-interferer) RFI gets very large. We also derive DeepSC's practical limits and a lower bound on 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#20844;&#24179;&#23646;&#24615;&#34917;&#20840;&#26041;&#27861;FairAC&#65292;&#29992;&#20110;&#22788;&#29702;&#20855;&#26377;&#32570;&#22833;&#23646;&#24615;&#30340;&#22270;&#25968;&#25454;&#20013;&#30340;&#19981;&#20844;&#24179;&#24615;&#38382;&#39064;&#12290;FairAC&#37319;&#29992;&#27880;&#24847;&#26426;&#21046;&#22788;&#29702;&#23646;&#24615;&#32570;&#22833;&#38382;&#39064;&#65292;&#24182;&#20943;&#36731;&#23646;&#24615;&#21644;&#34917;&#20840;&#23548;&#33268;&#30340;&#20004;&#31181;&#19981;&#20844;&#24179;&#24615;&#65292;&#21363;&#23646;&#24615;&#19981;&#20844;&#24179;&#21644;&#25299;&#25169;&#19981;&#20844;&#24179;&#12290;</title><link>http://arxiv.org/abs/2302.12977</link><description>&lt;p&gt;
&#22312;&#20855;&#26377;&#32570;&#22833;&#23646;&#24615;&#30340;&#22270;&#20013;&#36827;&#34892;&#20844;&#24179;&#23646;&#24615;&#34917;&#20840;
&lt;/p&gt;
&lt;p&gt;
Fair Attribute Completion on Graph with Missing Attributes. (arXiv:2302.12977v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.12977
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#20844;&#24179;&#23646;&#24615;&#34917;&#20840;&#26041;&#27861;FairAC&#65292;&#29992;&#20110;&#22788;&#29702;&#20855;&#26377;&#32570;&#22833;&#23646;&#24615;&#30340;&#22270;&#25968;&#25454;&#20013;&#30340;&#19981;&#20844;&#24179;&#24615;&#38382;&#39064;&#12290;FairAC&#37319;&#29992;&#27880;&#24847;&#26426;&#21046;&#22788;&#29702;&#23646;&#24615;&#32570;&#22833;&#38382;&#39064;&#65292;&#24182;&#20943;&#36731;&#23646;&#24615;&#21644;&#34917;&#20840;&#23548;&#33268;&#30340;&#20004;&#31181;&#19981;&#20844;&#24179;&#24615;&#65292;&#21363;&#23646;&#24615;&#19981;&#20844;&#24179;&#21644;&#25299;&#25169;&#19981;&#20844;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#20915;&#22270;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#19981;&#20844;&#24179;&#24615;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#22270;&#20013;&#30340;&#19981;&#20844;&#24179;&#38382;&#39064;&#28041;&#21450;&#23646;&#24615;&#21644;&#25299;&#25169;&#32467;&#26500;&#12290;&#29616;&#26377;&#30340;&#20844;&#24179;&#22270;&#23398;&#20064;&#24037;&#20316;&#20551;&#35774;&#35757;&#32451;&#27169;&#22411;&#26102;&#25152;&#26377;&#33410;&#28857;&#30340;&#23646;&#24615;&#37117;&#26159;&#21487;&#29992;&#30340;&#65292;&#28982;&#21518;&#36827;&#34892;&#20844;&#24179;&#39044;&#27979;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#30001;&#20110;&#25968;&#25454;&#32570;&#22833;&#25110;&#38544;&#31169;&#38382;&#39064;&#65292;&#19968;&#20123;&#33410;&#28857;&#30340;&#23646;&#24615;&#21487;&#33021;&#26080;&#27861;&#35775;&#38382;&#65292;&#36825;&#20351;&#24471;&#20844;&#24179;&#22270;&#23398;&#20064;&#26356;&#21152;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20844;&#24179;&#23646;&#24615;&#34917;&#20840;&#26041;&#27861;FairAC&#65292;&#29992;&#20110;&#34917;&#20840;&#32570;&#22833;&#20449;&#24687;&#24182;&#23398;&#20064;&#20855;&#26377;&#20844;&#24179;&#24615;&#30340;&#22270;&#33410;&#28857;&#23884;&#20837;&#12290;FairAC&#37319;&#29992;&#27880;&#24847;&#26426;&#21046;&#22788;&#29702;&#23646;&#24615;&#32570;&#22833;&#38382;&#39064;&#65292;&#24182;&#20943;&#36731;&#23646;&#24615;&#21644;&#34917;&#20840;&#23548;&#33268;&#30340;&#20004;&#31181;&#19981;&#20844;&#24179;&#24615;&#65292;&#21363;&#23646;&#24615;&#19981;&#20844;&#24179;&#21644;&#25299;&#25169;&#19981;&#20844;&#24179;&#12290;FairAC&#21487;&#20197;&#36866;&#24212;&#19981;&#21516;&#31867;&#22411;&#30340;&#21516;&#36136;&#22270;&#24182;&#20026;&#23427;&#20204;&#29983;&#25104;&#20844;&#24179;&#30340;&#23884;&#20837;&#65292;&#22240;&#27492;&#21487;&#20197;&#24212;&#29992;&#20110;&#22823;&#22810;&#25968;&#22270;&#25968;&#25454;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tackling unfairness in graph learning models is a challenging task, as the unfairness issues on graphs involve both attributes and topological structures. Existing work on fair graph learning simply assumes that attributes of all nodes are available for model training and then makes fair predictions. In practice, however, the attributes of some nodes might not be accessible due to missing data or privacy concerns, which makes fair graph learning even more challenging. In this paper, we propose FairAC, a fair attribute completion method, to complement missing information and learn fair node embeddings for graphs with missing attributes. FairAC adopts an attention mechanism to deal with the attribute missing problem and meanwhile, it mitigates two types of unfairness, i.e., feature unfairness from attributes and topological unfairness due to attribute completion. FairAC can work on various types of homogeneous graphs and generate fair embeddings for them and thus can be applied to most d
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#31995;&#32479;&#30340;&#31995;&#32479;&#36776;&#35782;&#38382;&#39064;&#65292;&#36890;&#36807;&#23545;&#27604;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#19982;&#29983;&#29289;&#31070;&#32463;&#20803;&#30340;&#35760;&#24405;&#26469;&#39564;&#35777;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;&#28982;&#32780;&#65292;&#31995;&#32479;&#36776;&#35782;&#30340;&#24615;&#33021;&#24456;&#22823;&#31243;&#24230;&#19978;&#21462;&#20915;&#20110;&#21050;&#28608;&#22270;&#20687;&#31561;&#22240;&#32032;&#65292;&#24182;&#19988;&#23545;&#35782;&#21035;&#26356;&#39640;&#32423;&#21035;&#26550;&#26500;&#22270;&#26696;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.06677</link><description>&lt;p&gt;
&#31070;&#32463;&#31995;&#32479;&#30340;&#31995;&#32479;&#36776;&#35782;&#65306;&#22914;&#26524;&#25105;&#20204;&#29702;&#35299;&#27491;&#30830;&#65292;&#25105;&#20204;&#20250;&#30693;&#36947;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
System identification of neural systems: If we got it right, would we know?. (arXiv:2302.06677v2 [q-bio.NC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.06677
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#31995;&#32479;&#30340;&#31995;&#32479;&#36776;&#35782;&#38382;&#39064;&#65292;&#36890;&#36807;&#23545;&#27604;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#19982;&#29983;&#29289;&#31070;&#32463;&#20803;&#30340;&#35760;&#24405;&#26469;&#39564;&#35777;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;&#28982;&#32780;&#65292;&#31995;&#32479;&#36776;&#35782;&#30340;&#24615;&#33021;&#24456;&#22823;&#31243;&#24230;&#19978;&#21462;&#20915;&#20110;&#21050;&#28608;&#22270;&#20687;&#31561;&#22240;&#32032;&#65292;&#24182;&#19988;&#23545;&#35782;&#21035;&#26356;&#39640;&#32423;&#21035;&#26550;&#26500;&#22270;&#26696;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#34987;&#25552;&#35758;&#20316;&#20026;&#22823;&#33041;&#30340;&#37096;&#20998;&#27169;&#22411;&#12290;&#23558;&#36825;&#20123;&#32593;&#32476;&#19982;&#29983;&#29289;&#31070;&#32463;&#20803;&#30340;&#35760;&#24405;&#36827;&#34892;&#27604;&#36739;&#65292;&#24182;&#35748;&#20026;&#22312;&#37325;&#29616;&#31070;&#32463;&#21453;&#24212;&#26041;&#38754;&#30340;&#33391;&#22909;&#24615;&#33021;&#25903;&#25345;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#26159;&#65292;&#36825;&#31181;&#31995;&#32479;&#36776;&#35782;&#26041;&#27861;&#23545;&#25105;&#20204;&#20102;&#35299;&#33041;&#37096;&#35745;&#31639;&#26377;&#22810;&#22823;&#24110;&#21161;&#12290;&#23427;&#26159;&#21542;&#33021;&#39564;&#35777;&#26576;&#31181;&#27169;&#22411;&#26550;&#26500;&#20248;&#20110;&#21478;&#19968;&#31181;&#65311;&#25105;&#20204;&#35780;&#20272;&#20102;&#26368;&#24120;&#29992;&#30340;&#27604;&#36739;&#25216;&#26415;&#65292;&#22914;&#32447;&#24615;&#32534;&#30721;&#27169;&#22411;&#21644;&#20013;&#24515;&#26680;&#23545;&#40784;&#65292;&#36890;&#36807;&#29992;&#24050;&#30693;&#30340;&#30495;&#23454;&#27169;&#22411;&#26367;&#25442;&#33041;&#37096;&#35760;&#24405;&#26469;&#27491;&#30830;&#35782;&#21035;&#27169;&#22411;&#12290;&#31995;&#32479;&#36776;&#35782;&#30340;&#24615;&#33021;&#30456;&#24403;&#19981;&#31283;&#23450;&#65292;&#23427;&#36824;&#26174;&#33879;&#20381;&#36182;&#20110;&#29420;&#31435;&#20110;&#30495;&#23454;&#27169;&#22411;&#26550;&#26500;&#30340;&#22240;&#32032;&#65292;&#22914;&#21050;&#28608;&#22270;&#20687;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20351;&#29992;&#21151;&#33021;&#30456;&#20284;&#24615;&#35780;&#20998;&#22312;&#35782;&#21035;&#26356;&#39640;&#32423;&#21035;&#26550;&#26500;&#22270;&#26696;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial neural networks are being proposed as models of parts of the brain. The networks are compared to recordings of biological neurons, and good performance in reproducing neural responses is considered to support the model's validity. A key question is how much this system identification approach tells us about brain computation. Does it validate one model architecture over another? We evaluate the most commonly used comparison techniques, such as a linear encoding model and centered kernel alignment, to correctly identify a model by replacing brain recordings with known ground truth models. System identification performance is quite variable; it also depends significantly on factors independent of the ground truth architecture, such as stimuli images. In addition, we show the limitations of using functional similarity scores in identifying higher-level architectural motifs.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28857;&#20113;&#30340;&#27627;&#31859;&#27874;&#36890;&#20449;&#20027;&#21160;&#38142;&#36335;&#36136;&#37327;&#39044;&#27979;&#26041;&#27861;&#65292;&#30456;&#27604;&#20110;&#22522;&#20110;&#22270;&#20687;&#30340;&#26041;&#27861;&#65292;&#20854;&#36866;&#29992;&#24615;&#26356;&#24191;&#19988;&#19981;&#28041;&#21450;&#25935;&#24863;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2301.00752</link><description>&lt;p&gt;
&#22522;&#20110;&#28857;&#20113;&#30340;&#27627;&#31859;&#27874;&#36890;&#20449;&#20027;&#21160;&#38142;&#36335;&#36136;&#37327;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Point Cloud-based Proactive Link Quality Prediction for Millimeter-wave Communications. (arXiv:2301.00752v2 [cs.NI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.00752
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28857;&#20113;&#30340;&#27627;&#31859;&#27874;&#36890;&#20449;&#20027;&#21160;&#38142;&#36335;&#36136;&#37327;&#39044;&#27979;&#26041;&#27861;&#65292;&#30456;&#27604;&#20110;&#22522;&#20110;&#22270;&#20687;&#30340;&#26041;&#27861;&#65292;&#20854;&#36866;&#29992;&#24615;&#26356;&#24191;&#19988;&#19981;&#28041;&#21450;&#25935;&#24863;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#22522;&#20110;&#28857;&#20113;&#30340;&#27627;&#31859;&#27874;&#65288;mmWave&#65289;&#36890;&#20449;&#30340;&#20027;&#21160;&#38142;&#36335;&#36136;&#37327;&#39044;&#27979;&#30340;&#21487;&#34892;&#24615;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#28145;&#24230;&#22270;&#20687;&#30340;&#26102;&#38388;&#24207;&#21015;&#26469;&#39044;&#27979;&#26410;&#26469;&#26102;&#38388;&#27573;&#30340;&#25509;&#25910;&#20449;&#21495;&#24378;&#24230;&#65292;&#20197;&#32531;&#35299;&#34892;&#20154;&#38459;&#25377;&#22240;&#32032;&#23545;mmWave&#36890;&#20449;&#30340;&#24433;&#21709;&#12290;&#20294;&#26159;&#65292;&#30001;&#20110;&#38544;&#31169;&#38382;&#39064;&#65292;&#36825;&#20123;&#22522;&#20110;&#22270;&#20687;&#30340;&#26041;&#27861;&#30340;&#36866;&#29992;&#24615;&#26377;&#38480;&#65292;&#22240;&#20026;&#25668;&#20687;&#22836;&#22270;&#20687;&#21487;&#33021;&#21253;&#21547;&#25935;&#24863;&#20449;&#24687;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28857;&#20113;&#30340;mmWave&#38142;&#36335;&#36136;&#37327;&#39044;&#27979;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20854;&#21487;&#34892;&#24615;&#12290;&#28857;&#20113;&#23558;&#19977;&#32500;&#31354;&#38388;&#34920;&#31034;&#20026;&#28857;&#38598;&#65292;&#20854;&#31354;&#38388;&#24615;&#36136;&#26356;&#21152;&#31232;&#30095;&#65292;&#19981;&#22826;&#21487;&#33021;&#21253;&#21547;&#25935;&#24863;&#20449;&#24687;&#65292;&#24182;&#19988;&#36824;&#25552;&#20379;&#20102;3D&#20301;&#32622;&#21644;&#36816;&#21160;&#20449;&#24687;&#65292;&#36825;&#23545;&#20102;&#35299;&#28041;&#21450;&#34892;&#20154;&#30340;&#26080;&#32447;&#30005;&#20256;&#25773;&#29615;&#22659;&#26159;&#24517;&#35201;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study demonstrates the feasibility of point cloud-based proactive link quality prediction for millimeter-wave (mmWave) communications. Previous studies have proposed machine learning-based methods to predict received signal strength for future time periods using time series of depth images to mitigate the line-of-sight (LOS) path blockage by pedestrians in mmWave communication. However, these image-based methods have limited applicability due to privacy concerns as camera images may contain sensitive information. This study proposes a point cloud-based method for mmWave link quality prediction and demonstrates its feasibility through experiments. Point clouds represent three-dimensional (3D) spaces as a set of points and are sparser and less likely to contain sensitive information than camera images. Additionally, point clouds provide 3D position and motion information, which is necessary for understanding the radio propagation environment involving pedestrians. This study designs
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#20851;&#20110;&#20351;&#29992;Transformer&#26550;&#26500;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;LaMDA&#26159;&#21542;&#20855;&#26377;&#24847;&#35782;&#30340;&#35828;&#27861;&#12290;&#20316;&#32773;&#35748;&#20026;&#35821;&#35328;&#27169;&#22411;&#19981;&#21487;&#33021;&#20855;&#26377;&#24847;&#35782;&#65292;&#32780;LaMDA&#27809;&#26377;&#27604;&#20854;&#20182;&#31867;&#20284;&#27169;&#22411;&#26356;&#20855;&#20808;&#36827;&#24615;&#12290;</title><link>http://arxiv.org/abs/2211.11483</link><description>&lt;p&gt;
Deanthropomorphising NLP&#65306;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#24847;&#35782;&#21040;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Deanthropomorphising NLP: Can a Language Model Be Conscious?. (arXiv:2211.11483v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.11483
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#20851;&#20110;&#20351;&#29992;Transformer&#26550;&#26500;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;LaMDA&#26159;&#21542;&#20855;&#26377;&#24847;&#35782;&#30340;&#35828;&#27861;&#12290;&#20316;&#32773;&#35748;&#20026;&#35821;&#35328;&#27169;&#22411;&#19981;&#21487;&#33021;&#20855;&#26377;&#24847;&#35782;&#65292;&#32780;LaMDA&#27809;&#26377;&#27604;&#20854;&#20182;&#31867;&#20284;&#27169;&#22411;&#26356;&#20855;&#20808;&#36827;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#23545;&#26368;&#36817;&#26377;&#20851;&#20351;&#29992;Transformer&#27169;&#22411;&#26550;&#26500;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;LaMDA&#20855;&#26377;&#24847;&#35782;&#30340;&#35828;&#27861;&#36827;&#34892;&#35752;&#35770;&#12290;&#25105;&#20204;&#35748;&#20026;&#36825;&#26679;&#30340;&#35821;&#35328;&#27169;&#22411;&#19981;&#21487;&#33021;&#20855;&#26377;&#24847;&#35782;&#65292;&#32780;LaMDA&#24182;&#27809;&#26377;&#27604;&#20854;&#20182;&#31867;&#20284;&#27169;&#22411;&#26356;&#20855;&#20808;&#36827;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#32508;&#21512;&#20449;&#24687;&#29702;&#35770;&#23545;Transformer&#26550;&#26500;&#36827;&#34892;&#20998;&#26512;&#26469;&#35777;&#26126;&#36825;&#19968;&#28857;&#12290;&#25105;&#20204;&#35748;&#20026;&#36825;&#20123;&#26377;&#24847;&#35782;&#30340;&#35828;&#27861;&#26159;NLP&#25253;&#36947;&#20013;&#20351;&#29992;&#25311;&#20154;&#21270;&#35821;&#35328;&#30340;&#26356;&#24191;&#27867;&#20542;&#21521;&#30340;&#19968;&#37096;&#20998;&#12290;&#26080;&#35770;&#36825;&#20123;&#35828;&#27861;&#30340;&#30495;&#23454;&#24615;&#22914;&#20309;&#65292;&#25105;&#20204;&#35748;&#20026;&#29616;&#22312;&#26159;&#35780;&#20272;&#35821;&#35328;&#24314;&#27169;&#36827;&#23637;&#24182;&#32771;&#34385;&#35813;&#20219;&#21153;&#30340;&#20262;&#29702;&#24433;&#21709;&#30340;&#36866;&#24403;&#26102;&#26426;&#12290;&#20026;&#20102;&#20351;&#26412;&#25991;&#26377;&#21161;&#20110;NLP&#31038;&#21306;&#20197;&#22806;&#30340;&#35835;&#32773;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#20123;NLP&#22522;&#30784;&#30693;&#35782;&#30340;&#20171;&#32461;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work is intended as a voice in the discussion over the recent claims that LaMDA, a pretrained language model based on the Transformer model architecture, is sentient. This claim, if confirmed, would have serious ramifications in the Natural Language Processing (NLP) community due to wide-spread use of similar models. However, here we take the position that such a language model cannot be sentient, or conscious, and that LaMDA in particular exhibits no advances over other similar models that would qualify it. We justify this by analysing the Transformer architecture through Integrated Information Theory. We see the claims of consciousness as part of a wider tendency to use anthropomorphic language in NLP reporting. Regardless of the veracity of the claims, we consider this an opportune moment to take stock of progress in language modelling and consider the ethical implications of the task. In order to make this work helpful for readers outside the NLP community, we also present the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#30456;&#37051;&#26579;&#33394;&#32452;&#32455;&#23398;&#29255;&#20013;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#40657;&#33394;&#32032;&#32454;&#32990;&#20998;&#21106;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;0.64&#30340;&#24179;&#22343;IOU&#65292;&#23613;&#31649;&#23384;&#22312;&#19981;&#23436;&#32654;&#30340;&#26631;&#31614;&#12290;</title><link>http://arxiv.org/abs/2211.00646</link><description>&lt;p&gt;
&#20174;&#30456;&#37051;&#30340;&#26579;&#33394;&#32452;&#32455;&#23398;&#29255;&#20013;&#23398;&#20064;&#40657;&#33394;&#32032;&#32454;&#32990;&#25513;&#33180;
&lt;/p&gt;
&lt;p&gt;
Learning Melanocytic Cell Masks from Adjacent Stained Tissue. (arXiv:2211.00646v2 [q-bio.QM] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.00646
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#30456;&#37051;&#26579;&#33394;&#32452;&#32455;&#23398;&#29255;&#20013;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#40657;&#33394;&#32032;&#32454;&#32990;&#20998;&#21106;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;0.64&#30340;&#24179;&#22343;IOU&#65292;&#23613;&#31649;&#23384;&#22312;&#19981;&#23436;&#32654;&#30340;&#26631;&#31614;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#40657;&#33394;&#32032;&#30244;&#26159;&#26368;&#20855;&#20405;&#34989;&#24615;&#30340;&#30382;&#32932;&#30284;&#20043;&#19968;&#65292;&#23548;&#33268;&#22823;&#37096;&#20998;&#30382;&#32932;&#30284;&#27515;&#20129;&#12290;&#28982;&#32780;&#65292;&#30149;&#29702;&#23398;&#23478;&#23545;&#40657;&#33394;&#32032;&#30244;&#30340;&#35786;&#26029;&#21487;&#38752;&#24615;&#36739;&#20302;&#12290;&#30001;&#20110;&#40657;&#33394;&#32032;&#30244;&#26159;&#40657;&#33394;&#32032;&#32454;&#32990;&#30340;&#32959;&#30244;&#65292;&#38656;&#35201;&#24320;&#21457;&#19968;&#31181;&#19982;&#30149;&#29702;&#23398;&#23478;&#30340;&#24046;&#24322;&#26080;&#20851;&#24182;&#33021;&#33258;&#21160;&#36827;&#34892;&#20687;&#32032;&#32423;&#27880;&#37322;&#30340;&#40657;&#33394;&#32032;&#32454;&#32990;&#20998;&#21106;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#22823;&#35268;&#27169;&#30149;&#29702;&#23398;&#23478;&#26631;&#27880;&#26159;&#19981;&#29616;&#23454;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#20351;&#29992;&#37051;&#36817;&#32452;&#32455;&#20999;&#29255;&#19978;&#30340;&#20598;&#32852;&#20813;&#30123;&#32452;&#32455;&#21270;&#23398;&#65288;IHC&#65289;&#26579;&#33394;&#29255;&#65292;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#40657;&#33394;&#32032;&#32454;&#32990;&#20998;&#21106;&#65292;&#34429;&#28982;&#24456;&#38590;&#26377;&#23436;&#32654;&#30340;&#26631;&#31614;&#65292;&#20294;&#36798;&#21040;&#20102;0.64&#30340;&#24179;&#22343;IOU&#12290;
&lt;/p&gt;
&lt;p&gt;
Melanoma is one of the most aggressive forms of skin cancer, causing a large proportion of skin cancer deaths. However, melanoma diagnoses by pathologists shows low interrater reliability. As melanoma is a cancer of the melanocyte, there is a clear need to develop a melanocytic cell segmentation tool that is agnostic to pathologist variability and automates pixel-level annotation. Gigapixel-level pathologist labeling, however, is impractical. Herein, we propose a means to train deep neural networks for melanocytic cell segmentation from hematoxylin and eosin (H&amp;E) stained slides using paired immunohistochemical (IHC) slides of adjacent tissue sections, achieving a mean IOU of 0.64 despite imperfect ground-truth labels.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#36125;&#21494;&#26031;HMAML&#30340;&#26032;&#26694;&#26550;&#65292;&#21033;&#29992;&#36229;&#32593;&#32476;&#36827;&#34892;&#26435;&#37325;&#26356;&#26032;&#65292;&#20197;&#35299;&#20915;MAML&#30340;&#36807;&#25311;&#21512;&#21644;&#19981;&#30830;&#23450;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2210.02796</link><description>&lt;p&gt;
Bayesian MAML&#30340;&#36229;&#32593;&#32476;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Hypernetwork approach to Bayesian MAML. (arXiv:2210.02796v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.02796
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#36125;&#21494;&#26031;HMAML&#30340;&#26032;&#26694;&#26550;&#65292;&#21033;&#29992;&#36229;&#32593;&#32476;&#36827;&#34892;&#26435;&#37325;&#26356;&#26032;&#65292;&#20197;&#35299;&#20915;MAML&#30340;&#36807;&#25311;&#21512;&#21644;&#19981;&#30830;&#23450;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23569;&#26679;&#26412;&#23398;&#20064;&#31639;&#27861;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#33021;&#22815;&#20174;&#23569;&#37327;&#25968;&#25454;&#20013;&#36827;&#34892;&#23398;&#20064;&#12290;&#20854;&#20013;&#19968;&#31181;&#26368;&#27969;&#34892;&#19988;&#20248;&#38597;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#26041;&#27861;&#26159;&#27169;&#22411;&#26080;&#20851;&#20803;&#23398;&#20064;&#65288;MAML&#65289;&#12290;&#35813;&#26041;&#27861;&#30340;&#20027;&#35201;&#24605;&#24819;&#26159;&#23398;&#20064;&#20803;&#27169;&#22411;&#30340;&#20849;&#20139;&#36890;&#29992;&#26435;&#37325;&#65292;&#28982;&#21518;&#23558;&#20854;&#36866;&#24212;&#20110;&#29305;&#23450;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#35813;&#26041;&#27861;&#23384;&#22312;&#36807;&#25311;&#21512;&#38382;&#39064;&#65292;&#22240;&#20026;&#25968;&#25454;&#37327;&#26377;&#38480;&#65292;&#24456;&#38590;&#20934;&#30830;&#37327;&#21270;&#19981;&#30830;&#23450;&#24615;&#12290;&#36125;&#21494;&#26031;&#26041;&#27861;&#21487;&#20197;&#36890;&#36807;&#23398;&#20064;&#26435;&#37325;&#20998;&#24067;&#32780;&#19981;&#26159;&#28857;&#20540;&#26435;&#37325;&#26469;&#32531;&#35299;&#36825;&#20123;&#38382;&#39064;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#20808;&#21069;&#20462;&#25913;&#30340;MAML&#26041;&#27861;&#30001;&#20110;&#39640;&#26031;&#20998;&#24067;&#30340;&#31616;&#21333;&#24615;&#12289;&#22522;&#20110;&#26799;&#24230;&#30340;&#31867;MAML&#26435;&#37325;&#26356;&#26032;&#25110;&#24378;&#21046;&#25191;&#34892;&#30456;&#21516;&#32467;&#26500;&#30340;&#36890;&#29992;&#26435;&#37325;&#21644;&#36866;&#24212;&#26435;&#37325;&#65292;&#21463;&#21040;&#19968;&#23450;&#30340;&#38480;&#21046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36125;&#21494;&#26031;MAML&#26694;&#26550;&#65292;&#31216;&#20026;&#36125;&#21494;&#26031;HMAML&#65292;&#23427;&#21033;&#29992;&#36229;&#32593;&#32476;&#36827;&#34892;&#26435;&#37325;&#26356;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;
The main goal of Few-Shot learning algorithms is to enable learning from small amounts of data. One of the most popular and elegant Few-Shot learning approaches is Model-Agnostic Meta-Learning (MAML). The main idea behind this method is to learn the shared universal weights of a meta-model, which are then adapted for specific tasks. However, the method suffers from over-fitting and poorly quantifies uncertainty due to limited data size. Bayesian approaches could, in principle, alleviate these shortcomings by learning weight distributions in place of point-wise weights. Unfortunately, previous modifications of MAML are limited due to the simplicity of Gaussian posteriors, MAML-like gradient-based weight updates, or by the same structure enforced for universal and adapted weights.  In this paper, we propose a novel framework for Bayesian MAML called BayesianHMAML, which employs Hypernetworks for weight updates. It learns the universal weights point-wise, but a probabilistic structure is 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#35270;&#35273;&#23545;&#24212;&#30340;&#35299;&#37322;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#21892;AI&#30340;&#40065;&#26834;&#24615;&#21644;&#20154;&#26426;&#22242;&#38431;&#30340;&#20934;&#30830;&#24615;&#12290;&#22312;&#22823;&#35268;&#27169;&#30340;&#20154;&#31867;&#30740;&#31350;&#20013;&#65292;&#35813;&#26041;&#27861;&#34987;&#21457;&#29616;&#27604;kNN&#35299;&#37322;&#26356;&#26377;&#29992;&#65292;&#24110;&#21161;&#29992;&#25143;&#26356;&#20934;&#30830;&#22320;&#25298;&#32477;AI&#30340;&#38169;&#35823;&#20915;&#31574;&#12290;&#21516;&#26102;&#65292;&#35813;&#26041;&#27861;&#22312;&#36229;&#20986;&#20998;&#24067;&#25968;&#25454;&#38598;&#19978;&#25913;&#36827;&#20102;&#24615;&#33021;&#65292;&#24182;&#23454;&#29616;&#20102;&#20114;&#34917;&#20154;&#26426;&#22242;&#38431;&#20934;&#30830;&#24615;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>http://arxiv.org/abs/2208.00780</link><description>&lt;p&gt;
&#22522;&#20110;&#35270;&#35273;&#23545;&#24212;&#30340;&#35299;&#37322;&#25552;&#39640;&#20102;AI&#30340;&#40065;&#26834;&#24615;&#21644;&#20154;&#26426;&#22242;&#38431;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;
Visual correspondence-based explanations improve AI robustness and human-AI team accuracy. (arXiv:2208.00780v5 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.00780
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#35270;&#35273;&#23545;&#24212;&#30340;&#35299;&#37322;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#21892;AI&#30340;&#40065;&#26834;&#24615;&#21644;&#20154;&#26426;&#22242;&#38431;&#30340;&#20934;&#30830;&#24615;&#12290;&#22312;&#22823;&#35268;&#27169;&#30340;&#20154;&#31867;&#30740;&#31350;&#20013;&#65292;&#35813;&#26041;&#27861;&#34987;&#21457;&#29616;&#27604;kNN&#35299;&#37322;&#26356;&#26377;&#29992;&#65292;&#24110;&#21161;&#29992;&#25143;&#26356;&#20934;&#30830;&#22320;&#25298;&#32477;AI&#30340;&#38169;&#35823;&#20915;&#31574;&#12290;&#21516;&#26102;&#65292;&#35813;&#26041;&#27861;&#22312;&#36229;&#20986;&#20998;&#24067;&#25968;&#25454;&#38598;&#19978;&#25913;&#36827;&#20102;&#24615;&#33021;&#65292;&#24182;&#23454;&#29616;&#20102;&#20114;&#34917;&#20154;&#26426;&#22242;&#38431;&#20934;&#30830;&#24615;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#39640;&#39118;&#38505;&#24212;&#29992;&#20013;&#65292;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#30340;&#39044;&#27979;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#65292;&#29978;&#33267;&#26159;&#24517;&#19981;&#21487;&#23569;&#30340;&#65292;&#22240;&#20026;&#20154;&#31867;&#26159;&#26368;&#32456;&#30340;&#20915;&#31574;&#32773;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#39062;&#30340;&#33258;&#35299;&#37322;&#22270;&#20687;&#20998;&#31867;&#22120;&#26550;&#26500;&#65292;&#23427;&#20204;&#39318;&#20808;&#36890;&#36807;&#21033;&#29992;&#26597;&#35810;&#22270;&#20687;&#21644;&#31034;&#20363;&#20043;&#38388;&#30340;&#35270;&#35273;&#23545;&#24212;&#20851;&#31995;&#36827;&#34892;&#35299;&#37322;&#65292;&#28982;&#21518;&#36827;&#34892;&#39044;&#27979;&#65288;&#19982;&#20107;&#21518;&#35299;&#37322;&#30456;&#23545;&#65289;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#36229;&#20986;&#20998;&#24067;&#65288;OOD&#65289;&#25968;&#25454;&#38598;&#19978;&#19968;&#33268;&#25913;&#36827;&#65288;1&#21040;4&#20010;&#28857;&#65289;&#65292;&#32780;&#22312;&#20998;&#24067;&#27979;&#35797;&#19978;&#34920;&#29616;&#30053;&#27425;&#20110;ResNet-50&#21644;&#19968;&#20010;k&#26368;&#36817;&#37051;&#20998;&#31867;&#22120;&#65288;kNN&#65289;&#65288;&#19979;&#38477;1&#21040;2&#20010;&#28857;&#65289;&#12290;&#36890;&#36807;&#23545;ImageNet&#21644;CUB&#36827;&#34892;&#30340;&#22823;&#35268;&#27169;&#20154;&#31867;&#30740;&#31350;&#65292;&#25105;&#20204;&#21457;&#29616;&#22522;&#20110;&#23545;&#24212;&#20851;&#31995;&#30340;&#35299;&#37322;&#27604;kNN&#35299;&#37322;&#23545;&#29992;&#25143;&#26356;&#26377;&#29992;&#12290;&#25105;&#20204;&#30340;&#35299;&#37322;&#24110;&#21161;&#29992;&#25143;&#26356;&#20934;&#30830;&#22320;&#25298;&#32477;AI&#30340;&#38169;&#35823;&#20915;&#31574;&#65292;&#32988;&#36807;&#25152;&#26377;&#20854;&#20182;&#27979;&#35797;&#26041;&#27861;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#39318;&#27425;&#23637;&#31034;&#20102;&#23454;&#29616;&#20114;&#34917;&#20154;&#26426;&#22242;&#38431;&#20934;&#30830;&#24615;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Explaining artificial intelligence (AI) predictions is increasingly important and even imperative in many high-stakes applications where humans are the ultimate decision-makers. In this work, we propose two novel architectures of self-interpretable image classifiers that first explain, and then predict (as opposed to post-hoc explanations) by harnessing the visual correspondences between a query image and exemplars. Our models consistently improve (by 1 to 4 points) on out-of-distribution (OOD) datasets while performing marginally worse (by 1 to 2 points) on in-distribution tests than ResNet-50 and a $k$-nearest neighbor classifier (kNN). Via a large-scale, human study on ImageNet and CUB, our correspondence-based explanations are found to be more useful to users than kNN explanations. Our explanations help users more accurately reject AI's wrong decisions than all other tested methods. Interestingly, for the first time, we show that it is possible to achieve complementary human-AI tea
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#19981;&#21516;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#30340;&#35270;&#35273;&#25512;&#29702;&#33021;&#21147;&#21644;&#31038;&#20250;&#20559;&#35265;&#12290;&#23613;&#31649;&#27169;&#22411;&#22312;&#22270;&#20687;&#29983;&#25104;&#26041;&#38754;&#34920;&#29616;&#20986;&#39640;&#36136;&#37327;&#30340;&#32467;&#26524;&#65292;&#20294;&#22312;&#23545;&#35937;&#35745;&#25968;&#21644;&#31354;&#38388;&#20851;&#31995;&#29702;&#35299;&#33021;&#21147;&#26041;&#38754;&#20173;&#23384;&#22312;&#19982;&#19978;&#30028;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#24040;&#22823;&#24046;&#36317;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35780;&#20272;&#20102;&#27169;&#22411;&#22312;&#24615;&#21035;&#21644;&#32932;&#33394;&#26041;&#38754;&#30340;&#20559;&#35265;&#12290;</title><link>http://arxiv.org/abs/2202.04053</link><description>&lt;p&gt;
DALL-Eval: &#25506;&#31350;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#21644;&#31038;&#20250;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
DALL-Eval: Probing the Reasoning Skills and Social Biases of Text-to-Image Generation Models. (arXiv:2202.04053v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.04053
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#19981;&#21516;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#30340;&#35270;&#35273;&#25512;&#29702;&#33021;&#21147;&#21644;&#31038;&#20250;&#20559;&#35265;&#12290;&#23613;&#31649;&#27169;&#22411;&#22312;&#22270;&#20687;&#29983;&#25104;&#26041;&#38754;&#34920;&#29616;&#20986;&#39640;&#36136;&#37327;&#30340;&#32467;&#26524;&#65292;&#20294;&#22312;&#23545;&#35937;&#35745;&#25968;&#21644;&#31354;&#38388;&#20851;&#31995;&#29702;&#35299;&#33021;&#21147;&#26041;&#38754;&#20173;&#23384;&#22312;&#19982;&#19978;&#30028;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#24040;&#22823;&#24046;&#36317;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35780;&#20272;&#20102;&#27169;&#22411;&#22312;&#24615;&#21035;&#21644;&#32932;&#33394;&#26041;&#38754;&#30340;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;DALL-E&#65292;&#19968;&#31181;&#22810;&#27169;&#24577;&#30340;&#36716;&#25442;&#35821;&#35328;&#27169;&#22411;&#21450;&#20854;&#21464;&#31181;&#65292;&#21253;&#25324;&#25193;&#25955;&#27169;&#22411;&#65292;&#23637;&#31034;&#20102;&#39640;&#36136;&#37327;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#26377;&#36924;&#30495;&#30340;&#22270;&#20687;&#29983;&#25104;&#32467;&#26524;&#65292;&#23545;&#20110;&#22914;&#20309;&#35780;&#20272;&#36825;&#20123;&#27169;&#22411;&#36824;&#27809;&#26377;&#36827;&#34892;&#35814;&#32454;&#30340;&#20998;&#26512;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19981;&#21516;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#30340;&#35270;&#35273;&#25512;&#29702;&#33021;&#21147;&#21644;&#31038;&#20250;&#20559;&#35265;&#65292;&#28085;&#30422;&#20102;&#22810;&#27169;&#24577;&#36716;&#25442;&#35821;&#35328;&#27169;&#22411;&#21644;&#25193;&#25955;&#27169;&#22411;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#37327;&#21270;&#20102;&#19977;&#31181;&#35270;&#35273;&#25512;&#29702;&#33021;&#21147;&#65306;&#23545;&#35937;&#35782;&#21035;&#65292;&#23545;&#35937;&#35745;&#25968;&#21644;&#31354;&#38388;&#20851;&#31995;&#29702;&#35299;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PaintSkills&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#34913;&#37327;&#36825;&#20123;&#33021;&#21147;&#30340;&#32452;&#21512;&#24335;&#35786;&#26029;&#35780;&#20272;&#25968;&#25454;&#38598;&#12290;&#23613;&#31649;&#20855;&#26377;&#39640;&#24230;&#36924;&#30495;&#30340;&#22270;&#20687;&#29983;&#25104;&#33021;&#21147;&#65292;&#20294;&#22312;&#23545;&#35937;&#35745;&#25968;&#21644;&#31354;&#38388;&#20851;&#31995;&#29702;&#35299;&#33021;&#21147;&#26041;&#38754;&#65292;&#26368;&#36817;&#27169;&#22411;&#30340;&#24615;&#33021;&#19982;&#19978;&#30028;&#20934;&#30830;&#24615;&#20043;&#38388;&#23384;&#22312;&#24456;&#22823;&#24046;&#36317;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#36890;&#36807;&#27979;&#37327;&#24615;&#21035;&#21644;&#32932;&#33394;&#20559;&#35265;&#26469;&#35780;&#20272;&#27169;&#22411;&#30340;&#24615;&#21035;&#21644;&#32932;&#33394;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, DALL-E, a multimodal transformer language model, and its variants, including diffusion models, have shown high-quality text-to-image generation capabilities. However, despite the realistic image generation results, there has not been a detailed analysis of how to evaluate such models. In this work, we investigate the visual reasoning capabilities and social biases of different text-to-image models, covering both multimodal transformer language models and diffusion models. First, we measure three visual reasoning skills: object recognition, object counting, and spatial relation understanding. For this, we propose PaintSkills, a compositional diagnostic evaluation dataset that measures these skills. Despite the high-fidelity image generation capability, a large gap exists between the performance of recent models and the upper bound accuracy in object counting and spatial relation understanding skills. Second, we assess the gender and skin tone biases by measuring the gender/ski
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#23558;&#24402;&#32435;&#25512;&#29702;&#21644;&#28436;&#32462;&#25512;&#29702;&#30456;&#32467;&#21512;&#24212;&#29992;&#20110;&#19981;&#23436;&#25972;&#30693;&#35782;&#22270;&#35889;&#19978;&#30340;&#26597;&#35810;&#22238;&#31572;&#12290;&#36890;&#36807;&#23558;&#26412;&#20307;&#35770;&#32435;&#20837;&#22522;&#20110;&#23884;&#20837;&#30340;&#26597;&#35810;&#22238;&#31572;&#27169;&#22411;&#65292;&#37319;&#29992;&#19981;&#21516;&#30340;&#38598;&#25104;&#31574;&#30053;&#21644;&#25439;&#22833;&#20989;&#25968;&#35843;&#25972;&#65292;&#21462;&#24471;&#20102;20%&#21040;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2106.14052</link><description>&lt;p&gt;
&#23558;&#24402;&#32435;&#25512;&#29702;&#21644;&#28436;&#32462;&#25512;&#29702;&#30456;&#32467;&#21512;&#36827;&#34892;&#19981;&#23436;&#25972;&#30693;&#35782;&#22270;&#35889;&#19978;&#30340;&#26597;&#35810;&#22238;&#31572;
&lt;/p&gt;
&lt;p&gt;
Combining Inductive and Deductive Reasoning for Query Answering over Incomplete Knowledge Graphs. (arXiv:2106.14052v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.14052
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#23558;&#24402;&#32435;&#25512;&#29702;&#21644;&#28436;&#32462;&#25512;&#29702;&#30456;&#32467;&#21512;&#24212;&#29992;&#20110;&#19981;&#23436;&#25972;&#30693;&#35782;&#22270;&#35889;&#19978;&#30340;&#26597;&#35810;&#22238;&#31572;&#12290;&#36890;&#36807;&#23558;&#26412;&#20307;&#35770;&#32435;&#20837;&#22522;&#20110;&#23884;&#20837;&#30340;&#26597;&#35810;&#22238;&#31572;&#27169;&#22411;&#65292;&#37319;&#29992;&#19981;&#21516;&#30340;&#38598;&#25104;&#31574;&#30053;&#21644;&#25439;&#22833;&#20989;&#25968;&#35843;&#25972;&#65292;&#21462;&#24471;&#20102;20%&#21040;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#29992;&#20110;&#19981;&#23436;&#25972;&#30693;&#35782;&#22270;&#35889;&#30340;&#22522;&#20110;&#23884;&#20837;&#30340;&#26597;&#35810;&#22238;&#31572;&#26041;&#27861;&#21482;&#38598;&#20013;&#22312;&#24402;&#32435;&#25512;&#29702;&#19978;&#65292;&#21363;&#36890;&#36807;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#27169;&#24335;&#26469;&#39044;&#27979;&#31572;&#26696;&#65292;&#24182;&#32570;&#20047;&#36827;&#34892;&#28436;&#32462;&#25512;&#29702;&#30340;&#33021;&#21147;&#65292;&#28436;&#32462;&#25512;&#29702;&#38656;&#35201;&#24212;&#29992;&#39046;&#22495;&#30693;&#35782;&#26469;&#25512;&#26029;&#26356;&#22810;&#20449;&#24687;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#32570;&#28857;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#23558;&#26412;&#20307;&#35770;&#32435;&#20837;&#22522;&#20110;&#23884;&#20837;&#30340;&#26597;&#35810;&#22238;&#31572;&#27169;&#22411;&#30340;&#38382;&#39064;&#65292;&#23450;&#20041;&#20102;&#22522;&#20110;&#23884;&#20837;&#30340;&#26412;&#20307;&#20013;&#20171;&#26597;&#35810;&#22238;&#31572;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#21508;&#31181;&#38598;&#25104;&#31574;&#30053;&#65292;&#21253;&#25324;&#65288;1&#65289;&#19981;&#21516;&#30340;&#26412;&#20307;&#39537;&#21160;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#21644;&#65288;2&#65289;&#36866;&#24212;&#26412;&#20307;&#20844;&#29702;&#30340;&#25439;&#22833;&#20989;&#25968;&#30340;&#35843;&#25972;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#22522;&#20110;LUBM&#21644;NELL&#30693;&#35782;&#22270;&#35889;&#30340;&#26032;&#39062;&#22522;&#20934;&#65292;&#24182;&#22312;&#20854;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#22312;&#38656;&#35201;&#24402;&#32435;&#21644;&#28436;&#32462;&#25512;&#29702;&#30340;&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#21462;&#24471;&#20102;&#20174;20%&#21040;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current methods for embedding-based query answering over incomplete Knowledge Graphs (KGs) only focus on inductive reasoning, i.e., predicting answers by learning patterns from the data, and lack the complementary ability to do deductive reasoning, which requires the application of domain knowledge to infer further information. To address this shortcoming, we investigate the problem of incorporating ontologies into embedding-based query answering models by defining the task of embedding-based ontology-mediated query answering. We propose various integration strategies into prominent representatives of embedding models that involve (1) different ontology-driven data augmentation techniques and (2) adaptation of the loss function to enforce the ontology axioms. We design novel benchmarks for the considered task based on the LUBM and the NELL KGs and evaluate our methods on them. The achieved improvements in the setting that requires both inductive and deductive reasoning are from 20% to 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#32473;&#23450;&#30340;LTL&#35268;&#33539;&#20013;&#23398;&#20064;&#26368;&#20248;&#25511;&#21046;&#31574;&#30053;&#65292;&#21363;&#20351;&#29615;&#22659;&#23436;&#20840;&#26410;&#30693;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23558;&#38382;&#39064;&#24314;&#27169;&#20026;&#25511;&#21046;&#22120;&#21644;&#23545;&#25239;&#29615;&#22659;&#20043;&#38388;&#30340;&#38543;&#26426;&#21338;&#24328;&#65292;&#26368;&#22823;&#21270;&#28385;&#36275;LTL&#35268;&#33539;&#30340;&#27010;&#29575;&#65292;&#25269;&#25239;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#29615;&#22659;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2102.04307</link><description>&lt;p&gt;
&#22312;&#38543;&#26426;&#21338;&#24328;&#20013;&#23398;&#20064;&#26102;&#24577;&#20219;&#21153;&#30340;&#26368;&#20248;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Learning Optimal Strategies for Temporal Tasks in Stochastic Games. (arXiv:2102.04307v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2102.04307
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#32473;&#23450;&#30340;LTL&#35268;&#33539;&#20013;&#23398;&#20064;&#26368;&#20248;&#25511;&#21046;&#31574;&#30053;&#65292;&#21363;&#20351;&#29615;&#22659;&#23436;&#20840;&#26410;&#30693;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23558;&#38382;&#39064;&#24314;&#27169;&#20026;&#25511;&#21046;&#22120;&#21644;&#23545;&#25239;&#29615;&#22659;&#20043;&#38388;&#30340;&#38543;&#26426;&#21338;&#24328;&#65292;&#26368;&#22823;&#21270;&#28385;&#36275;LTL&#35268;&#33539;&#30340;&#27010;&#29575;&#65292;&#25269;&#25239;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#29615;&#22659;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#32447;&#24615;&#26102;&#24577;&#36923;&#36753;&#65288;LTL&#65289;&#35268;&#33539;&#20013;&#36827;&#34892;&#21512;&#25104;&#21487;&#20197;&#20026;&#22312;&#38543;&#26426;&#21644;&#28508;&#22312;&#23545;&#25239;&#29615;&#22659;&#20013;&#36816;&#34892;&#30340;&#31995;&#32479;&#25552;&#20379;&#20445;&#35777;&#30340;&#25511;&#21046;&#22120;&#12290;&#28982;&#32780;&#65292;&#33258;&#21160;&#21512;&#25104;&#24037;&#20855;&#38656;&#35201;&#19968;&#20010;&#29615;&#22659;&#27169;&#22411;&#26469;&#26500;&#24314;&#25511;&#21046;&#22120;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26080;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#32473;&#23450;&#30340;LTL&#35268;&#33539;&#20013;&#25512;&#23548;&#25511;&#21046;&#22120;&#65292;&#21363;&#20351;&#29615;&#22659;&#23436;&#20840;&#26410;&#30693;&#12290;&#25105;&#20204;&#23558;&#38382;&#39064;&#24314;&#27169;&#20026;&#25511;&#21046;&#22120;&#21644;&#23545;&#25239;&#29615;&#22659;&#20043;&#38388;&#30340;&#38543;&#26426;&#21338;&#24328;&#65288;SG&#65289;&#65292;&#28982;&#21518;&#23398;&#20064;&#26368;&#20248;&#25511;&#21046;&#31574;&#30053;&#65292;&#20197;&#26368;&#22823;&#21270;&#28385;&#36275;LTL&#35268;&#33539;&#30340;&#27010;&#29575;&#65292;&#25269;&#25239;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#29615;&#22659;&#34892;&#20026;&#12290;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;&#20174;&#32473;&#23450;LTL&#35268;&#33539;&#32763;&#35793;&#30340;&#30830;&#23450;&#24615;&#22855;&#20598;&#33258;&#21160;&#26426;&#65288;DPA&#65289;&#26500;&#24314;&#19968;&#20010;&#20056;&#31215;&#21338;&#24328;&#12290;&#36890;&#36807;&#20174;DPA&#25509;&#21463;&#26465;&#20214;&#23548;&#20986;&#19981;&#21516;&#30340;&#22870;&#21169;&#21644;&#25240;&#25187;&#22240;&#23376;&#65292;&#25105;&#20204;&#23558;&#26368;&#22823;&#21270;&#26368;&#22351;&#24773;&#20917;&#19979;&#28385;&#36275;LTL&#35268;&#33539;&#30340;&#27010;&#29575;&#30340;&#38382;&#39064;&#31616;&#21270;&#20026;&#19968;&#20010;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Synthesis from linear temporal logic (LTL) specifications provides assured controllers for systems operating in stochastic and potentially adversarial environments. Automatic synthesis tools, however, require a model of the environment to construct controllers. In this work, we introduce a model-free reinforcement learning (RL) approach to derive controllers from given LTL specifications even when the environment is completely unknown. We model the problem as a stochastic game (SG) between the controller and the adversarial environment; we then learn optimal control strategies that maximize the probability of satisfying the LTL specifications against the worst-case environment behavior. We first construct a product game using the deterministic parity automaton (DPA) translated from the given LTL specification. By deriving distinct rewards and discount factors from the acceptance condition of the DPA, we reduce the maximization of the worst-case probability of satisfying the LTL specifi
&lt;/p&gt;</description></item></channel></rss>