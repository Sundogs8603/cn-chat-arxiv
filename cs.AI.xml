<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36991;&#20813;&#38480;&#21046;&#24615;&#20551;&#35774;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#29305;&#24615;&#26469;&#25552;&#39640;&#23398;&#20064;&#25439;&#22833;&#20989;&#25968;&#30340;&#26679;&#26412;&#25928;&#29575;&#65292;&#22312;&#39044;&#27979;&#20248;&#21270;&#38382;&#39064;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.16830</link><description>&lt;p&gt;
&#31163;&#24034;&#65306;&#36229;&#36234;&#26412;&#22320;&#25439;&#22833;&#20989;&#25968;&#30340;&#39044;&#27979;&#20248;&#21270;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Leaving the Nest: Going Beyond Local Loss Functions for Predict-Then-Optimize. (arXiv:2305.16830v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16830
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36991;&#20813;&#38480;&#21046;&#24615;&#20551;&#35774;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#29305;&#24615;&#26469;&#25552;&#39640;&#23398;&#20064;&#25439;&#22833;&#20989;&#25968;&#30340;&#26679;&#26412;&#25928;&#29575;&#65292;&#22312;&#39044;&#27979;&#20248;&#21270;&#38382;&#39064;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#20248;&#21270;&#38382;&#39064;&#26159;&#19968;&#31181;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#22312;&#19981;&#30830;&#23450;&#24615;&#26465;&#20214;&#19979;&#36827;&#34892;&#20915;&#31574;&#21046;&#23450;&#30340;&#26694;&#26550;&#12290;&#23427;&#30340;&#20013;&#24515;&#30740;&#31350;&#38382;&#39064;&#26159;&#65292;&#8220;&#22914;&#20309;&#21033;&#29992;&#20915;&#31574;&#20219;&#21153;&#30340;&#32467;&#26500;&#26469;&#23450;&#21046;&#29305;&#23450;&#20219;&#21153;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65311;&#8221;&#20026;&#27492;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#24050;&#32463;&#25552;&#20986;&#20102;&#23398;&#20064;&#20219;&#21153;&#29305;&#23450;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#25429;&#25417;&#36825;&#31181;&#28508;&#22312;&#30340;&#32467;&#26500;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#26041;&#27861;&#23545;&#36825;&#20123;&#25439;&#22833;&#30340;&#24418;&#24335;&#21644;&#23545;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#34892;&#20026;&#30340;&#24433;&#21709;&#20570;&#20986;&#20102;&#38480;&#21046;&#24615;&#30340;&#20551;&#35774;&#12290;&#36825;&#20123;&#20551;&#35774;&#26082;&#23548;&#33268;&#20102;&#39640;&#35745;&#31639;&#25104;&#26412;&#30340;&#26041;&#27861;&#65292;&#20063;&#22312;&#23454;&#36341;&#20013;&#34987;&#36829;&#21453;&#26102;&#23548;&#33268;&#20102;&#24615;&#33021;&#19979;&#38477;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36991;&#20813;&#20102;&#19978;&#36848;&#20551;&#35774;&#65292;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#29305;&#24615;&#26469;&#25552;&#39640;&#23398;&#20064;&#25439;&#22833;&#20989;&#25968;&#30340;&#26679;&#26412;&#25928;&#29575;&#12290;&#25105;&#20204;&#20174;&#25991;&#29486;&#20013;&#30340;&#22235;&#20010;&#39046;&#22495;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#36890;&#24120;&#38656;&#35201;&#27604;&#21487;&#27604;&#26041;&#27861;&#23569;&#19968;&#20010;&#25968;&#37327;&#32423;&#30340;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Predict-then-Optimize is a framework for using machine learning to perform decision-making under uncertainty. The central research question it asks is, "How can the structure of a decision-making task be used to tailor ML models for that specific task?" To this end, recent work has proposed learning task-specific loss functions that capture this underlying structure. However, current approaches make restrictive assumptions about the form of these losses and their impact on ML model behavior. These assumptions both lead to approaches with high computational cost, and when they are violated in practice, poor performance. In this paper, we propose solutions to these issues, avoiding the aforementioned assumptions and utilizing the ML model's features to increase the sample efficiency of learning loss functions. We empirically show that our method achieves state-of-the-art results in four domains from the literature, often requiring an order of magnitude fewer samples than comparable metho
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36328;Prompt&#30340;&#20316;&#25991;Trait&#35780;&#20998;&#27169;&#22411;&#65292;&#36890;&#36807;&#20316;&#25991;&#25552;&#31034;&#20851;&#27880;&#21644;Traint&#30456;&#20284;&#24615;loss&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#20316;&#25991;&#25552;&#31034;&#19981;&#21516;&#30340;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#33258;&#21160;&#21270;&#20316;&#25991;&#35780;&#20998;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.16826</link><description>&lt;p&gt;
Prompt-&#21644;Trait&#20851;&#31995;&#24863;&#30693;&#30340;&#36328;Prompt&#20316;&#25991;Trait&#35780;&#20998;
&lt;/p&gt;
&lt;p&gt;
Prompt- and Trait Relation-aware Cross-prompt Essay Trait Scoring. (arXiv:2305.16826v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16826
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36328;Prompt&#30340;&#20316;&#25991;Trait&#35780;&#20998;&#27169;&#22411;&#65292;&#36890;&#36807;&#20316;&#25991;&#25552;&#31034;&#20851;&#27880;&#21644;Traint&#30456;&#20284;&#24615;loss&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#20316;&#25991;&#25552;&#31034;&#19981;&#21516;&#30340;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#33258;&#21160;&#21270;&#20316;&#25991;&#35780;&#20998;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#21270;&#20316;&#25991;&#35780;&#20998;&#65288;AES&#65289;&#30340;&#30446;&#30340;&#26159;&#23545;&#20889;&#20316;&#20027;&#39064;&#36827;&#34892;&#35780;&#20998;&#30340;&#25991;&#31456;&#65292;&#35813;&#20027;&#39064;&#23450;&#20041;&#20102;&#20889;&#20316;&#20027;&#39064;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;AES&#31995;&#32479;&#20551;&#23450;&#23545;&#20110;&#35757;&#32451;&#20013;&#20351;&#29992;&#30340;&#30456;&#21516;&#25552;&#31034;&#35780;&#20998;&#25991;&#31456;&#65292;&#24182;&#20998;&#37197;&#20165;&#25972;&#20307;&#20998;&#25968;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#30340;&#35774;&#32622;&#19982;&#23454;&#38469;&#25945;&#32946;&#24773;&#20917;&#20914;&#31361;&#65307;&#29305;&#23450;&#25552;&#31034;&#30340;&#39044;&#20998;&#32423;&#25991;&#31456;&#32570;&#20047;&#65292;&#24182;&#19988;&#38656;&#35201;&#35814;&#32454;&#30340;&#23376;&#37327;&#35268;&#30340;Trait&#20998;&#25968;&#12290;&#22240;&#27492;&#65292;&#39044;&#27979;&#30475;&#19981;&#35265;&#30340;Prompt&#25991;&#31456;&#30340;&#21508;&#31181;Trait&#20998;&#25968;&#65288;&#31216;&#20026;&#36328;Prompt&#20316;&#25991;Trait&#35780;&#20998;&#65289;&#26159;AES&#30340;&#19968;&#39033;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24378;&#22823;&#30340;&#27169;&#22411;&#65306;Prompt-&#21644;Trait&#20851;&#31995;&#24863;&#30693;&#30340;&#36328;Prompt&#20316;&#25991;Trait&#35780;&#20998;&#22120;&#12290;&#25105;&#20204;&#36890;&#36807;&#20316;&#25991;&#25552;&#31034;&#20851;&#27880;&#21644;&#21033;&#29992;&#30001;&#20027;&#39064;&#24314;&#27169;&#26426;&#21046;&#25552;&#21462;&#30340;&#20027;&#39064;&#36830;&#36143;&#24615;&#29305;&#24449;&#23545;&#20316;&#25991;&#24863;&#30693;&#36827;&#34892;&#32534;&#30721;&#65292;&#32780;&#26080;&#38656;&#35775;&#38382;&#26631;&#35760;&#25968;&#25454;&#65307;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#29978;&#33267;&#22312;&#36328;Prompt&#35774;&#32622;&#20013;&#20063;&#32771;&#34385;&#21040;&#20316;&#25991;&#30340;&#25552;&#31034;&#24682;&#23432;&#12290;&#20026;&#20102;&#20419;&#36827;&#22810;Trait&#35780;&#20998;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;Trait&#30456;&#20284;&#24615;lo
&lt;/p&gt;
&lt;p&gt;
Automated essay scoring (AES) aims to score essays written for a given prompt, which defines the writing topic. Most existing AES systems assume to grade essays of the same prompt as used in training and assign only a holistic score. However, such settings conflict with real-education situations; pre-graded essays for a particular prompt are lacking, and detailed trait scores of sub-rubrics are required. Thus, predicting various trait scores of unseen-prompt essays (called cross-prompt essay trait scoring) is a remaining challenge of AES. In this paper, we propose a robust model: prompt- and trait relation-aware cross-prompt essay trait scorer. We encode prompt-aware essay representation by essay-prompt attention and utilizing the topic-coherence feature extracted by the topic-modeling mechanism without access to labeled data; therefore, our model considers the prompt adherence of an essay, even in a cross-prompt setting. To facilitate multi-trait scoring, we design trait-similarity lo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HUB&#30340;&#28151;&#21512;&#26356;&#26032;&#31574;&#30053;&#65292;&#36890;&#36807;&#32467;&#21512;&#23398;&#20064;&#20248;&#21270;&#22120;&#21644;&#25163;&#24037;&#35774;&#35745;&#30340;&#20248;&#21270;&#22120;&#65292;&#25552;&#39640;&#20102;&#23398;&#20064;&#20248;&#21270;&#22120;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.16823</link><description>&lt;p&gt;
HUB: &#29992;&#25345;&#32493;&#25552;&#31034;&#35843;&#25972;&#24341;&#23548;&#23398;&#20064;&#20248;&#21270;&#22120;
&lt;/p&gt;
&lt;p&gt;
HUB: Guiding Learned Optimizers with Continuous Prompt Tuning. (arXiv:2305.16823v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16823
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HUB&#30340;&#28151;&#21512;&#26356;&#26032;&#31574;&#30053;&#65292;&#36890;&#36807;&#32467;&#21512;&#23398;&#20064;&#20248;&#21270;&#22120;&#21644;&#25163;&#24037;&#35774;&#35745;&#30340;&#20248;&#21270;&#22120;&#65292;&#25552;&#39640;&#20102;&#23398;&#20064;&#20248;&#21270;&#22120;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#20248;&#21270;&#22120;&#26159;&#20803;&#23398;&#20064;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65292;&#20294;&#20854;&#22312;&#22788;&#29702;&#26410;&#35265;&#36807;&#30340;&#20219;&#21153;&#21644;&#32593;&#32476;&#26550;&#26500;&#26102;&#26377;&#38480;&#12290;&#20026;&#20102;&#35299;&#20915;&#27492;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28151;&#21512;&#26356;&#26032;&#31574;&#30053;&#30340;&#20248;&#21270;&#26041;&#27861;&#65288;HUB&#65289;&#65292;&#35813;&#26041;&#27861;&#21463;&#21040;&#20102;&#22823;&#22411;&#35821;&#35328;&#21644;&#35270;&#35273;&#27169;&#22411;&#20013;&#30828;&#25552;&#31034;&#35843;&#25972;&#21644;&#32467;&#26524;&#36873;&#25321;&#25216;&#26415;&#30340;&#21551;&#21457;&#12290;&#36890;&#36807;&#23558;&#25163;&#24037;&#35774;&#35745;&#30340;&#20248;&#21270;&#22120;&#20316;&#20026;&#25105;&#20204;&#28151;&#21512;&#26041;&#27861;&#30340;&#31532;&#20108;&#20010;&#32452;&#20214;&#65292;&#25105;&#20204;&#33021;&#22815;&#22312;&#31283;&#23450;&#35757;&#32451;&#30340;&#21516;&#26102;&#20445;&#30041;&#23398;&#20064;&#20248;&#21270;&#22120;&#30340;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learned optimizers are a crucial component of meta-learning. Recent advancements in scalable learned optimizers have demonstrated their superior performance over hand-designed optimizers in various tasks. However, certain characteristics of these models, such as an unstable learning curve, limited ability to handle unseen tasks and network architectures, difficult-to-control behaviours, and poor performance in fine-tuning tasks impede their widespread adoption. To tackle the issue of generalization in scalable learned optimizers, we propose a hybrid-update-based (HUB) optimization strategy inspired by recent advancements in hard prompt tuning and result selection techniques used in large language and vision models. This approach can be easily applied to any task that involves hand-designed or learned optimizer. By incorporating hand-designed optimizers as the second component in our hybrid approach, we are able to retain the benefits of learned optimizers while stabilizing the training
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#12289;&#22522;&#20110;&#21152;&#26435;&#24179;&#22343;&#30340;&#39046;&#22495;&#23545;&#40784;&#21069;&#32512;&#24179;&#22343;&#26041;&#27861;&#65288;DAPA&#65289;&#65292;&#29992;&#20110;&#25277;&#35937;&#25688;&#35201;&#20013;&#30340;&#39046;&#22495;&#27867;&#21270;&#65292;&#23454;&#29616;&#20102;&#26377;&#25928;&#30340;&#28304;&#22495;&#25193;&#23637;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.16820</link><description>&lt;p&gt;
&#38754;&#21521;&#25277;&#35937;&#25688;&#35201;&#20013;&#30340;&#39046;&#22495;&#27867;&#21270;&#30340;&#39046;&#22495;&#23545;&#40784;&#21069;&#32512;&#24179;&#22343;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Domain Aligned Prefix Averaging for Domain Generalization in Abstractive Summarization. (arXiv:2305.16820v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16820
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#12289;&#22522;&#20110;&#21152;&#26435;&#24179;&#22343;&#30340;&#39046;&#22495;&#23545;&#40784;&#21069;&#32512;&#24179;&#22343;&#26041;&#27861;&#65288;DAPA&#65289;&#65292;&#29992;&#20110;&#25277;&#35937;&#25688;&#35201;&#20013;&#30340;&#39046;&#22495;&#27867;&#21270;&#65292;&#23454;&#29616;&#20102;&#26377;&#25928;&#30340;&#28304;&#22495;&#25193;&#23637;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#20110;&#25277;&#35937;&#25688;&#35201;&#20013;&#30340;&#39046;&#22495;&#27867;&#21270;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#65292;&#22522;&#20110;&#21152;&#26435;&#24179;&#22343;&#30340;&#39046;&#22495;&#23545;&#40784;&#21069;&#32512;&#24179;&#22343;&#26041;&#27861;&#65288;DAPA&#65289;&#12290;&#36890;&#36807;&#32473;&#23450;&#22810;&#20010;&#28304;&#22495;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#39318;&#20808;&#20026;&#27599;&#20010;&#22495;&#35757;&#32451;&#19968;&#20010;&#21069;&#32512;&#65292;&#28982;&#21518;&#21033;&#29992;&#36825;&#20123;&#21069;&#32512;&#29983;&#25104;&#23569;&#37327;&#30446;&#26631;&#22495;&#25991;&#26723;&#30340;&#25688;&#35201;&#65292;&#35745;&#31639;&#25152;&#38656;&#30340;&#26435;&#37325;&#26469;&#24179;&#22343;&#28304;&#21069;&#32512;&#12290;&#22312;DAPA&#20013;&#65292;&#21069;&#32512;&#35843;&#25972;&#20801;&#35768;&#36731;&#37327;&#32423;&#30340;&#24494;&#35843;&#65292;&#21152;&#26435;&#24179;&#22343;&#20801;&#35768;&#26377;&#25928;&#22320;&#28155;&#21152;&#26032;&#30340;&#28304;&#22495;&#12290;&#22312;&#22235;&#20010;&#19981;&#21516;&#30340;&#25688;&#35201;&#39046;&#22495;&#19978;&#36827;&#34892;&#35780;&#20272;&#65292;DAPA&#34920;&#29616;&#20986;&#19982;&#22522;&#20934;&#26041;&#27861;&#30456;&#24403;&#25110;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#35777;&#26126;&#20102;&#20854;&#21069;&#32512;&#24179;&#22343;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Domain generalization is hitherto an underexplored area applied in abstractive summarization. Moreover, most existing works on domain generalization have sophisticated training algorithms. In this paper, we propose a lightweight, weight averaging based, Domain Aligned Prefix Averaging approach to domain generalization for abstractive summarization. Given a number of source domains, our method first trains a prefix for each one of them. These source prefixes generate summaries for a small number of target domain documents. The similarity of the generated summaries to their corresponding documents is used for calculating weights required to average source prefixes. In DAPA, prefix tuning allows for lightweight finetuning, and weight averaging allows for the computationally efficient addition of new source domains. When evaluated on four diverse summarization domains, DAPA shows comparable or better performance against the baselines, demonstrating the effectiveness of its prefix averaging
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20449;&#20219;&#26694;&#26550;&#30340;&#40065;&#26834;&#25511;&#21046;&#21644;&#21327;&#35843;&#26041;&#26696;&#65292;&#20174;&#24694;&#24847;&#20195;&#29702;&#30340;&#35282;&#24230;&#35782;&#21035;&#20851;&#38190;&#23545;&#25239;&#30446;&#26631;&#65292;&#26377;&#25928;&#36991;&#20813;&#30896;&#25758;&#21644;&#20132;&#36890;&#22581;&#22622;&#12290;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20449;&#20219;&#26694;&#26550;&#30340;&#25915;&#20987;&#26816;&#27979;&#21644;&#32531;&#35299;&#25514;&#26045;&#12290;</title><link>http://arxiv.org/abs/2305.16818</link><description>&lt;p&gt;
&#22522;&#20110;&#20449;&#20219;&#24863;&#30693;&#30340;&#36830;&#25509;&#21644;&#33258;&#20027;&#36710;&#36742;&#30340;&#40065;&#26834;&#25511;&#21046;&#19982;&#21327;&#35843;
&lt;/p&gt;
&lt;p&gt;
Trust-Aware Resilient Control and Coordination of Connected and Automated Vehicles. (arXiv:2305.16818v1 [cs.MA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16818
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20449;&#20219;&#26694;&#26550;&#30340;&#40065;&#26834;&#25511;&#21046;&#21644;&#21327;&#35843;&#26041;&#26696;&#65292;&#20174;&#24694;&#24847;&#20195;&#29702;&#30340;&#35282;&#24230;&#35782;&#21035;&#20851;&#38190;&#23545;&#25239;&#30446;&#26631;&#65292;&#26377;&#25928;&#36991;&#20813;&#30896;&#25758;&#21644;&#20132;&#36890;&#22581;&#22622;&#12290;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20449;&#20219;&#26694;&#26550;&#30340;&#25915;&#20987;&#26816;&#27979;&#21644;&#32531;&#35299;&#25514;&#26045;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23433;&#20840;&#23545;&#20110;&#32593;&#32476;&#36830;&#25509;&#21644;&#33258;&#20027;&#36710;&#36742;&#65288;CAV&#65289;&#31561;&#29289;&#29702;&#31995;&#32479;&#33267;&#20851;&#37325;&#35201;&#65292;&#36825;&#20123;&#36710;&#36742;&#36890;&#36807;&#21327;&#20316;&#23433;&#20840;&#22320;&#36890;&#36807;&#36947;&#36335;&#32593;&#32476;&#12290;&#26412;&#25991;&#20174;&#19981;&#37197;&#21512;/&#24694;&#24847;&#20195;&#29702;&#20154;&#30340;&#35282;&#24230;&#35782;&#21035;&#20851;&#38190;&#23545;&#25239;&#24615;&#30446;&#26631;&#65288;&#22914;&#30896;&#25758;&#21644;&#20132;&#36890;&#22581;&#22622;&#65289;&#65292;&#21033;&#29992;&#20449;&#20219;&#26694;&#26550;&#25552;&#20986;&#20102;&#40065;&#26834;&#30340;&#25511;&#21046;&#19982;&#21327;&#35843;&#26041;&#26696;&#65292;&#20197;&#32531;&#35299;&#24694;&#24847;&#20195;&#29702;&#24102;&#26469;&#30340;&#24433;&#21709;&#24182;&#20445;&#35777;&#23433;&#20840;&#21327;&#35843;&#12290;&#25105;&#20204;&#20351;&#29992; Sybil &#25915;&#20987;&#39564;&#35777;&#20102;&#24314;&#35758;&#30340;&#26694;&#26550;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20449;&#20219;&#26694;&#26550;&#30340;&#25915;&#20987;&#26816;&#27979;&#21644;&#32531;&#35299;&#25514;&#26045;&#12290;
&lt;/p&gt;
&lt;p&gt;
Security is crucial for cyber-physical systems, such as a network of Connected and Automated Vehicles (CAVs) cooperating to navigate through a road network safely. In this paper, we tackle the security of a cooperating network of CAVs in conflict areas by identifying the critical adversarial objectives from the point of view of uncooperative/malicious agents from our preliminary study, which are (i) safety violations resulting in collisions, and (ii) traffic jams. We utilize a trust framework (and our work doesn't depend on the specific choice of trust/reputation framework) to propose a resilient control and coordination framework that mitigates the effects of such agents and guarantees safe coordination. A class of attacks that can be used to achieve the adversarial objectives is Sybil attacks, which we use to validate our proposed framework through simulation studies. Besides that, we propose an attack detection and mitigation scheme using the trust framework. The simulation results 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35774;&#35745;&#20102;&#19968;&#20010;&#26234;&#33021;&#36741;&#23548;&#31995;&#32479;&#65288;GenQ&#65289;&#65292;&#21487;&#20197;&#26681;&#25454;&#29031;&#39038;&#32773;&#21644;&#23401;&#23376;&#20043;&#38388;&#30340;&#23545;&#35805;&#20419;&#36827;&#23401;&#23376;&#30340;&#38405;&#35835;&#29702;&#35299;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#32771;&#34385;&#25991;&#21270;&#32972;&#26223;&#21644;&#35821;&#22659;&#21464;&#21270;&#20197;&#25552;&#39640;&#31995;&#32479;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.16809</link><description>&lt;p&gt;
GenQ&#65306;&#33258;&#21160;&#21270;&#38382;&#31572;&#29983;&#25104;&#22120;&#20197;&#24110;&#21161;&#29031;&#39038;&#32773;&#19982;&#23401;&#23376;&#20849;&#35835;&#25925;&#20107;
&lt;/p&gt;
&lt;p&gt;
GenQ: Automated Question Generation to Support Caregivers While Reading Stories with Children. (arXiv:2305.16809v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16809
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35774;&#35745;&#20102;&#19968;&#20010;&#26234;&#33021;&#36741;&#23548;&#31995;&#32479;&#65288;GenQ&#65289;&#65292;&#21487;&#20197;&#26681;&#25454;&#29031;&#39038;&#32773;&#21644;&#23401;&#23376;&#20043;&#38388;&#30340;&#23545;&#35805;&#20419;&#36827;&#23401;&#23376;&#30340;&#38405;&#35835;&#29702;&#35299;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#32771;&#34385;&#25991;&#21270;&#32972;&#26223;&#21644;&#35821;&#22659;&#21464;&#21270;&#20197;&#25552;&#39640;&#31995;&#32479;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#29031;&#39038;&#32773;&#35810;&#38382;&#24320;&#25918;&#24335;&#38382;&#39064;&#20197;&#28608;&#21457;&#19982;&#23401;&#23376;&#30340;&#23545;&#35805;&#26102;&#65292;&#21487;&#20197;&#20419;&#36827;&#23401;&#23376;&#30340;&#38405;&#35835;&#29702;&#35299;&#33021;&#21147;&#12290;&#34429;&#28982;&#26377;&#21033;&#29992;&#25216;&#26415;&#24037;&#20855;&#26469;&#25903;&#25345;&#36825;&#20010;&#36807;&#31243;&#65292;&#21363;&#25152;&#35859;&#30340;&#8220;&#26234;&#33021;&#36741;&#23548;&#31995;&#32479;&#8221;&#30340;&#31354;&#38388;&#65292;&#20294;&#30446;&#21069;&#20173;&#19981;&#28165;&#26970;&#29616;&#26377;&#30340;&#29983;&#25104;&#31867;&#20154;&#35821;&#35328;&#38382;&#39064;&#30340;&#26234;&#33021;&#31995;&#32479;&#26159;&#21542;&#26377;&#30410;&#12290;&#27492;&#22806;&#65292;&#29992;&#20110;&#24320;&#21457;&#36825;&#20123;&#33258;&#21160;&#29983;&#25104;&#38382;&#39064;&#31995;&#32479;&#30340;&#22521;&#35757;&#25968;&#25454;&#36890;&#24120;&#27809;&#26377;&#32771;&#34385;&#21040;&#20154;&#21475;&#32479;&#35745;&#23398;&#65292;&#20294;&#20855;&#26377;&#19981;&#21516;&#25991;&#21270;&#32972;&#26223;&#30340;&#20154;&#21487;&#33021;&#20250;&#25552;&#20986;&#19981;&#21516;&#30340;&#38382;&#39064;&#12290;&#20316;&#20026;&#20026;&#25289;&#19969;&#35028;&#20799;&#31461;&#35774;&#35745;&#26234;&#33021;&#38405;&#35835;&#25903;&#25345;&#24212;&#29992;&#31243;&#24207;&#30340;&#24191;&#27867;&#39033;&#30446;&#30340;&#19968;&#37096;&#20998;&#65292;&#25105;&#20204;&#20174;&#26469;&#33258;&#19981;&#21516;&#20154;&#21475;&#32479;&#35745;&#23398;&#30340;&#25289;&#19969;&#35028;&#25252;&#29702;&#20154;&#21592;&#21644;&#38750;&#25252;&#29702;&#20154;&#21592;&#20197;&#21450;&#20854;&#20182;&#20154;&#21475;&#32479;&#35745;&#23398;&#32972;&#26223;&#30340;&#25252;&#29702;&#20154;&#21592;&#21644;&#38750;&#25252;&#29702;&#20154;&#21592;&#20013;&#32676;&#38598;&#22823;&#37327;&#38382;&#39064;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#36825;&#20010;&#25968;&#25454;&#38598;&#20013;&#20010;&#20307;&#12289;&#25991;&#21270;&#21644;&#29615;&#22659;&#22240;&#32032;&#20013;&#20171;&#30340;&#38382;&#39064;&#25552;&#38382;&#30340;&#21464;&#21270;&#12290;&#28982;&#21518;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#31995;&#32479;&#26469;&#33258;&#21160;&#20135;&#29983;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
When caregivers ask open--ended questions to motivate dialogue with children, it facilitates the child's reading comprehension skills.Although there is scope for use of technological tools, referred here as "intelligent tutoring systems", to scaffold this process, it is currently unclear whether existing intelligent systems that generate human--language like questions is beneficial. Additionally, training data used in the development of these automated question generation systems is typically sourced without attention to demographics, but people with different cultural backgrounds may ask different questions. As a part of a broader project to design an intelligent reading support app for Latinx children, we crowdsourced questions from Latinx caregivers and noncaregivers as well as caregivers and noncaregivers from other demographics. We examine variations in question--asking within this dataset mediated by individual, cultural, and contextual factors. We then design a system that autom
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;GPT&#21644;NMT&#29983;&#25104;&#32763;&#35793;&#30340;&#25991;&#23383;&#31215;&#26497;&#24230;&#24046;&#24322;&#65292;&#21457;&#29616;GPT&#32763;&#35793;&#26356;&#19981;&#20934;&#30830;&#65292;&#20294;&#22312;MT&#36136;&#37327;&#35780;&#20272;&#25351;&#26631;&#19978;&#34920;&#29616;&#20986;&#30456;&#20284;&#25110;&#26356;&#22909;&#30340;&#20998;&#25968;&#12290;</title><link>http://arxiv.org/abs/2305.16806</link><description>&lt;p&gt;
GPT&#26159;&#21542;&#20250;&#20135;&#29983;&#26356;&#19981;&#20934;&#30830;&#30340;&#32763;&#35793;?
&lt;/p&gt;
&lt;p&gt;
Do GPTs Produce Less Literal Translations?. (arXiv:2305.16806v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16806
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;GPT&#21644;NMT&#29983;&#25104;&#32763;&#35793;&#30340;&#25991;&#23383;&#31215;&#26497;&#24230;&#24046;&#24322;&#65292;&#21457;&#29616;GPT&#32763;&#35793;&#26356;&#19981;&#20934;&#30830;&#65292;&#20294;&#22312;MT&#36136;&#37327;&#35780;&#20272;&#25351;&#26631;&#19978;&#34920;&#29616;&#20986;&#30456;&#20284;&#25110;&#26356;&#22909;&#30340;&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22914;GPT-3&#65292;&#24050;&#32463;&#25104;&#20026;&#36890;&#29992;&#35821;&#35328;&#27169;&#22411;&#65292;&#33021;&#22815;&#22788;&#29702;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#25110;&#29702;&#35299;&#20219;&#21153;&#12290;&#22312;&#26426;&#22120;&#32763;&#35793;&#65288;MT&#65289;&#20219;&#21153;&#20013;&#65292;&#24050;&#26377;&#22810;&#39033;&#30740;&#31350;&#25506;&#32034;&#21033;&#29992;few-shot&#25552;&#31034;&#26426;&#21046;&#20174;LLMs&#20013;&#24341;&#20986;&#26356;&#22909;&#30340;&#32763;&#35793;&#12290;&#28982;&#32780;&#65292;&#20154;&#20204;&#30456;&#23545;&#36739;&#23569;&#22320;&#20851;&#27880;&#36825;&#31181;&#32763;&#35793;&#19982;&#26631;&#20934;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65288;NMT&#65289;&#27169;&#22411;&#29983;&#25104;&#32763;&#35793;&#30340;&#36136;&#37327;&#24046;&#24322;&#12290;&#26412;&#30740;&#31350;&#20174;&#25991;&#23383;&#23545;&#40784;&#21644;&#21333;&#35843;&#24615;&#31561;&#26041;&#38754;&#65292;&#27604;&#36739;&#20102;GPT&#21644;NMT&#29983;&#25104;&#32763;&#35793;&#30340;&#25991;&#26412;&#25991;&#23383;&#31215;&#26497;&#24230;&#65292;&#21457;&#29616;GPT&#20174;&#33521;&#35821;&#65288;E-X&#65289;&#32763;&#35793;&#30340;&#25991;&#26412;&#26356;&#19981;&#20934;&#30830;&#65292;&#20294;&#22312;MT&#36136;&#37327;&#35780;&#20272;&#25351;&#26631;&#19978;&#34920;&#29616;&#20986;&#30456;&#20284;&#25110;&#26356;&#22909;&#30340;&#20998;&#25968;&#12290;&#25105;&#20204;&#35777;&#26126;&#36825;&#19968;&#32467;&#26524;&#22312;&#20154;&#24037;&#35780;&#20272;&#20013;&#20063;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;&#21516;&#26102;&#65292;&#24403;&#32763;&#35793;&#21477;&#23376;&#38271;&#24230;&#22686;&#21152;&#26102;&#65292;&#36825;&#31181;&#24046;&#21035;&#23601;&#23588;&#20026;&#26174;&#33879;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) such as GPT-3 have emerged as general-purpose language models capable of addressing many natural language generation or understanding tasks. On the task of Machine Translation (MT), multiple works have investigated few-shot prompting mechanisms to elicit better translations from LLMs. However, there has been relatively little investigation on how such translations differ qualitatively from the translations generated by standard Neural Machine Translation (NMT) models. In this work, we investigate these differences in terms of the literalness of translations produced by the two systems. Using literalness measures involving word alignment and monotonicity, we find that translations out of English (E-X) from GPTs tend to be less literal, while exhibiting similar or better scores on MT quality metrics. We demonstrate that this finding is borne out in human evaluations as well. We then show that these differences are especially pronounced when translating senten
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#27169;&#24335;&#30340;&#29992;&#25143;&#28385;&#24847;&#24230;&#24314;&#27169;&#26694;&#26550;SG-USM&#65292;&#23427;&#29305;&#21035;&#27169;&#25311;&#20102;&#31995;&#32479;&#31243;&#24230;&#30340;&#28385;&#36275;&#29992;&#25143;&#20851;&#20110;&#20219;&#21153;&#23646;&#24615;&#30340;&#20559;&#22909;&#31243;&#24230;&#65292;&#20197;&#39044;&#27979;&#29992;&#25143;&#30340;&#28385;&#24847;&#24230;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2305.16798</link><description>&lt;p&gt;
&#38754;&#21521;&#20219;&#21153;&#30340;&#23545;&#35805;&#20013;&#22522;&#20110;&#27169;&#24335;&#30340;&#29992;&#25143;&#28385;&#24847;&#24230;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Schema-Guided User Satisfaction Modeling for Task-Oriented Dialogues. (arXiv:2305.16798v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16798
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#27169;&#24335;&#30340;&#29992;&#25143;&#28385;&#24847;&#24230;&#24314;&#27169;&#26694;&#26550;SG-USM&#65292;&#23427;&#29305;&#21035;&#27169;&#25311;&#20102;&#31995;&#32479;&#31243;&#24230;&#30340;&#28385;&#36275;&#29992;&#25143;&#20851;&#20110;&#20219;&#21153;&#23646;&#24615;&#30340;&#20559;&#22909;&#31243;&#24230;&#65292;&#20197;&#39044;&#27979;&#29992;&#25143;&#30340;&#28385;&#24847;&#24230;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29992;&#25143;&#28385;&#24847;&#24230;&#24314;&#27169;&#65288;USM&#65289;&#26159;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#31995;&#32479;&#35780;&#20272;&#30340;&#19968;&#31181;&#27969;&#34892;&#36873;&#25321;&#65292;&#20854;&#20013;&#29992;&#25143;&#28385;&#24847;&#24230;&#36890;&#24120;&#21462;&#20915;&#20110;&#31995;&#32479;&#26159;&#21542;&#23454;&#29616;&#20102;&#29992;&#25143;&#30340;&#20219;&#21153;&#30446;&#26631;&#12290;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#31995;&#32479;&#20351;&#29992;&#20219;&#21153;&#26550;&#26500;&#65288;task schema&#65289;&#26469;&#32534;&#30721;&#29992;&#25143;&#30340;&#20219;&#21153;&#30446;&#26631;&#12290;&#29616;&#26377;&#30340;USM&#30740;&#31350;&#24573;&#30053;&#20102;&#20351;&#29992;&#20219;&#21153;&#26550;&#26500;&#26174;&#24335;&#24314;&#27169;&#29992;&#25143;&#30340;&#20219;&#21153;&#30446;&#26631;&#23454;&#29616;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#27169;&#24335;&#30340;&#29992;&#25143;&#28385;&#24847;&#24230;&#24314;&#27169;&#26694;&#26550;SG-USM&#12290;&#23427;&#29305;&#21035;&#27169;&#25311;&#20102;&#31995;&#32479;&#31243;&#24230;&#30340;&#28385;&#36275;&#29992;&#25143;&#20851;&#20110;&#20219;&#21153;&#23646;&#24615;&#30340;&#20559;&#22909;&#31243;&#24230;&#65292;&#20197;&#39044;&#27979;&#29992;&#25143;&#30340;&#28385;&#24847;&#24230;&#27700;&#24179;&#12290;SG-USM&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#26469;&#32534;&#30721;&#23545;&#35805;&#19978;&#19979;&#25991;&#21644;&#20219;&#21153;&#23646;&#24615;&#65292;&#24182;&#37319;&#29992;&#23653;&#34892;&#34920;&#31034;&#23618;&#26469;&#23398;&#20064;&#23545;&#35805;&#20013;&#23436;&#25104;&#20102;&#22810;&#23569;&#20219;&#21153;&#23646;&#24615;&#65292;&#37325;&#35201;&#24615;&#39044;&#27979;&#22120;&#29992;&#20110;&#35745;&#31639;&#20219;&#21153;&#23646;&#24615;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#21033;&#29992;&#27880;&#24847;&#26426;&#21046;&#36873;&#25321;&#24615;&#22320;&#20851;&#27880;&#37325;&#35201;&#20219;&#21153;&#23646;&#24615;&#20197;&#39044;&#27979;&#29992;&#25143;&#28385;&#24847;&#24230;&#12290;&#25105;&#20204;&#22312;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;SG-USM&#65292;&#24182;&#26174;&#31034;&#23427;&#20248;&#20110;&#29616;&#26377;&#30340;USM&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
User Satisfaction Modeling (USM) is one of the popular choices for task-oriented dialogue systems evaluation, where user satisfaction typically depends on whether the user's task goals were fulfilled by the system. Task-oriented dialogue systems use task schema, which is a set of task attributes, to encode the user's task goals. Existing studies on USM neglect explicitly modeling the user's task goals fulfillment using the task schema. In this paper, we propose SG-USM, a novel schema-guided user satisfaction modeling framework. It explicitly models the degree to which the user's preferences regarding the task attributes are fulfilled by the system for predicting the user's satisfaction level. SG-USM employs a pre-trained language model for encoding dialogue context and task attributes. Further, it employs a fulfillment representation layer for learning how many task attributes have been fulfilled in the dialogue, an importance predictor component for calculating the importance of task 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;'RSTformer'&#30340;&#25688;&#35201;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20840;&#38754;&#34701;&#21512;&#20102;&#35805;&#35821;&#20851;&#31995;&#31867;&#22411;&#21644;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#20197;&#20462;&#36766;&#32467;&#26500;&#29702;&#35770;&#20026;&#22522;&#30784;&#65292;&#32463;&#36807;&#20005;&#26684;&#35780;&#20272;&#65292;&#34920;&#29616;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.16784</link><description>&lt;p&gt;
&#32467;&#21512;&#35805;&#35821;&#32467;&#26500;&#20998;&#24067;&#30340;&#38271;&#25991;&#26412;&#33258;&#21160;&#25688;&#35201;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Incorporating Distributions of Discourse Structure for Long Document Abstractive Summarization. (arXiv:2305.16784v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16784
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;'RSTformer'&#30340;&#25688;&#35201;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20840;&#38754;&#34701;&#21512;&#20102;&#35805;&#35821;&#20851;&#31995;&#31867;&#22411;&#21644;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#20197;&#20462;&#36766;&#32467;&#26500;&#29702;&#35770;&#20026;&#22522;&#30784;&#65292;&#32463;&#36807;&#20005;&#26684;&#35780;&#20272;&#65292;&#34920;&#29616;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#25991;&#26412;&#25688;&#35201;&#65292;&#35805;&#35821;&#32467;&#26500;&#22312;&#36776;&#35782;&#25991;&#26412;&#26680;&#24515;&#20869;&#23481;&#26041;&#38754;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#21487;&#24796;&#30340;&#26159;&#65292;&#20043;&#21069;&#23558;&#20462;&#36766;&#32467;&#26500;&#29702;&#35770;&#65288;RST&#65289;&#24341;&#20837;&#22522;&#20110;transformer&#30340;&#33258;&#21160;&#25688;&#35201;&#27169;&#22411;&#30340;&#30740;&#31350;&#20165;&#32771;&#34385;&#20102;&#26680;&#24515;&#37096;&#20998;&#30340;&#27880;&#37322;&#65292;&#20174;&#32780;&#24573;&#30053;&#20102;&#21508;&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#35805;&#35821;&#20851;&#31995;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;'RSTformer'&#30340;&#26032;&#22411;&#25688;&#35201;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20840;&#38754;&#34701;&#21512;&#20102;&#35805;&#35821;&#20851;&#31995;&#31867;&#22411;&#21644;&#19981;&#30830;&#23450;&#24615;&#12290;&#25105;&#20204;&#30340;RST-attention&#26426;&#21046;&#26159;&#22522;&#20110;&#25991;&#26723;&#32423;&#20462;&#36766;&#32467;&#26500;&#30340;Longformer&#26694;&#26550;&#30340;&#25193;&#23637;&#12290;&#32463;&#36807;&#20005;&#26684;&#35780;&#20272;&#65292;&#26412;&#25991;&#25552;&#20986;&#30340;&#27169;&#22411;&#34920;&#29616;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#30340;&#27169;&#22411;&#65292;&#20984;&#26174;&#20986;&#20854;&#22312;&#22810;&#20010;&#33258;&#21160;&#35780;&#20272;&#25351;&#26631;&#21644;&#20154;&#24037;&#35780;&#20272;&#19978;&#30340;&#21331;&#36234;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
For text summarization, the role of discourse structure is pivotal in discerning the core content of a text. Regrettably, prior studies on incorporating Rhetorical Structure Theory (RST) into transformer-based summarization models only consider the nuclearity annotation, thereby overlooking the variety of discourse relation types. This paper introduces the 'RSTformer', a novel summarization model that comprehensively incorporates both the types and uncertainty of rhetorical relations. Our RST-attention mechanism, rooted in document-level rhetorical structure, is an extension of the recently devised Longformer framework. Through rigorous evaluation, the model proposed herein exhibits significant superiority over state-of-the-art models, as evidenced by its notable performance on several automatic metrics and human evaluation.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411; (MLLMs)&#30340;&#36328;&#35821;&#35328;&#36716;&#31227;&#33021;&#21147;&#30340;&#19981;&#21516;&#22240;&#32032;&#36827;&#34892;&#20102;&#35843;&#26597;&#21644;&#32508;&#36848;&#65292;&#23558;&#36825;&#20123;&#22240;&#32032;&#20998;&#25104;&#20116;&#31867;&#24182;&#25552;&#20379;&#20102;&#36807;&#21435;&#30740;&#31350;&#30340;&#32463;&#39564;&#35777;&#25454;&#12290;&#26412;&#25991;&#30340;&#24037;&#20316;&#26088;&#22312;&#20840;&#38754;&#32972;&#26223;&#21644;&#32479;&#19968;MLLMs&#36328;&#35821;&#35328;&#36716;&#31227;&#30340;&#29616;&#26377;&#30740;&#31350;&#27969;&#12290;</title><link>http://arxiv.org/abs/2305.16768</link><description>&lt;p&gt;
&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#30340;&#36328;&#35821;&#35328;&#36716;&#31227;&#22240;&#32032;&#30340;&#20849;&#21516;&#29702;&#35299;&#65306;&#19968;&#31687;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Towards a Common Understanding of Contributing Factors for Cross-Lingual Transfer in Multilingual Language Models: A Review. (arXiv:2305.16768v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16768
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411; (MLLMs)&#30340;&#36328;&#35821;&#35328;&#36716;&#31227;&#33021;&#21147;&#30340;&#19981;&#21516;&#22240;&#32032;&#36827;&#34892;&#20102;&#35843;&#26597;&#21644;&#32508;&#36848;&#65292;&#23558;&#36825;&#20123;&#22240;&#32032;&#20998;&#25104;&#20116;&#31867;&#24182;&#25552;&#20379;&#20102;&#36807;&#21435;&#30740;&#31350;&#30340;&#32463;&#39564;&#35777;&#25454;&#12290;&#26412;&#25991;&#30340;&#24037;&#20316;&#26088;&#22312;&#20840;&#38754;&#32972;&#26223;&#21644;&#32479;&#19968;MLLMs&#36328;&#35821;&#35328;&#36716;&#31227;&#30340;&#29616;&#26377;&#30740;&#31350;&#27969;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#39044;&#35757;&#32451;&#30340;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#34920;&#29616;&#20986;&#20102;&#22312;&#19981;&#21516;&#35821;&#35328;&#20043;&#38388;&#20256;&#36882;&#30693;&#35782;&#30340;&#24378;&#22823;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#37492;&#20110;&#36825;&#31181;&#33021;&#21147;&#30340;&#36861;&#27714;&#24182;&#26410;&#26126;&#30830;&#22320;&#34701;&#20837;&#22823;&#22810;&#25968;MLLM&#35774;&#35745;&#20013;&#65292;&#24456;&#38590;&#23545;&#20854;&#20986;&#29616;&#36827;&#34892;&#29420;&#29305;&#32780;&#30452;&#25509;&#30340;&#35299;&#37322;&#12290;&#22312;&#26412;&#32508;&#36848;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#30740;&#31350;MLLM&#30340;&#36328;&#35821;&#35328;&#36716;&#31227;&#33021;&#21147;&#30340;&#19981;&#21516;&#22240;&#32032;&#30340;&#25991;&#29486;&#65292;&#24182;&#35814;&#32454;&#27010;&#36848;&#21644;&#35752;&#35770;&#20102;&#36825;&#20123;&#22240;&#32032;&#12290;&#20026;&#20102;&#22686;&#24378;&#36825;&#20010;&#32508;&#36848;&#30340;&#32467;&#26500;&#24182;&#20415;&#20110;&#19982;&#26410;&#26469;&#30340;&#30740;&#31350;&#25972;&#21512;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#20116;&#31867;&#36825;&#26679;&#30340;&#22240;&#32032;&#12290;&#38500;&#25552;&#20379;&#36807;&#21435;&#30740;&#31350;&#30340;&#32463;&#39564;&#35777;&#25454;&#27010;&#36848;&#22806;&#65292;&#25105;&#20204;&#36824;&#30830;&#23450;&#20102;&#22312;&#20855;&#26377;&#19968;&#33268;&#21457;&#29616;&#30340;&#30740;&#31350;&#20013;&#30340;&#20849;&#35782;&#65292;&#24182;&#35299;&#20915;&#20102;&#22312;&#30683;&#30462;&#30340;&#30740;&#31350;&#20013;&#30340;&#20914;&#31361;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#23558;&#26088;&#22312;&#35299;&#37322;MLLM&#36328;&#35821;&#35328;&#36716;&#31227;&#33021;&#21147;&#30340;&#29616;&#26377;&#30740;&#31350;&#27969;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#32972;&#26223;&#21644;&#32479;&#19968;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, pre-trained Multilingual Language Models (MLLMs) have shown a strong ability to transfer knowledge across different languages. However, given that the aspiration for such an ability has not been explicitly incorporated in the design of the majority of MLLMs, it is challenging to obtain a unique and straightforward explanation for its emergence. In this review paper, we survey literature that investigates different factors contributing to the capacity of MLLMs to perform zero-shot cross-lingual transfer and subsequently outline and discuss these factors in detail. To enhance the structure of this review and to facilitate consolidation with future studies, we identify five categories of such factors. In addition to providing a summary of empirical evidence from past studies, we identify consensuses among studies with consistent findings and resolve conflicts among contradictory ones. Our work contextualizes and unifies existing research streams which aim at explaining th
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30495;&#23454;&#23454;&#20307;&#30340;&#26174;&#33879;&#36127;&#38754;&#38472;&#36848;&#30340;&#33021;&#21147;&#65292;&#22312;&#19981;&#21516;&#39046;&#22495;&#20013;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#32467;&#26524;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#21542;&#23450;&#38472;&#36848;&#20013;&#30340;&#20107;&#23454;&#27010;&#24565;&#19978;&#20173;&#26377;&#22256;&#38590;&#12290;</title><link>http://arxiv.org/abs/2305.16755</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#26174;&#33879;&#30340;&#36127;&#38754;&#22768;&#26126;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can large language models generate salient negative statements?. (arXiv:2305.16755v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16755
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30495;&#23454;&#23454;&#20307;&#30340;&#26174;&#33879;&#36127;&#38754;&#38472;&#36848;&#30340;&#33021;&#21147;&#65292;&#22312;&#19981;&#21516;&#39046;&#22495;&#20013;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#32467;&#26524;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#21542;&#23450;&#38472;&#36848;&#20013;&#30340;&#20107;&#23454;&#27010;&#24565;&#19978;&#20173;&#26377;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#20851;&#20110;&#29616;&#23454;&#19990;&#30028;&#23454;&#20307;&#30340;&#26174;&#33879;&#65288;&#26377;&#36259;&#30340;&#65289;&#36127;&#38754;&#38472;&#36848;&#30340;&#33021;&#21147;; &#36825;&#26159;&#36807;&#21435;&#20960;&#24180;&#20013;&#28044;&#29616;&#20986;&#30340;&#19968;&#20010;&#30740;&#31350;&#35838;&#39064;&#12290;&#25105;&#20204;&#20351;&#29992;&#38646;&#28857;&#21644;k&#27425;&#26080;&#32422;&#26463;&#25506;&#38024;&#26469;&#25506;&#27979;LLMs&#65292;&#24182;&#19982;&#20256;&#32479;&#30340;&#21542;&#23450;&#29983;&#25104;&#26041;&#27861;&#65292;&#21363;&#22522;&#20110;&#27169;&#24335;&#30340;&#25991;&#26412;&#25552;&#21462;&#21644;&#22522;&#20110;&#30693;&#35782;&#22270;&#30340;&#25512;&#29702;&#20197;&#21450;&#20247;&#21253;&#37329;&#26631;&#35821;&#21477;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#26469;&#33258;&#19981;&#21516;&#39046;&#22495;&#30340;&#20027;&#39064;&#29983;&#25104;&#21015;&#34920;&#30340;&#27491;&#30830;&#24615;&#21644;&#26174;&#30528;&#24615;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;&#26377;&#25351;&#23548;&#30340;&#25506;&#38024;&#30830;&#23454;&#25552;&#39640;&#20102;&#29983;&#25104;&#30340;&#36127;&#38754;&#38472;&#36848;&#30340;&#36136;&#37327;&#65292;&#19982;&#26080;&#25351;&#23548;&#30340;&#21464;&#20307;&#30456;&#27604;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#36825;&#20004;&#20010;&#25552;&#31034;&#65292;LLMs&#20173;&#28982;&#38590;&#20197;&#22788;&#29702;&#36127;&#38754;&#20107;&#23454;&#30340;&#27010;&#24565;&#65292;&#24120;&#24120;&#29983;&#25104;&#35768;&#22810;&#21547;&#31946;&#19981;&#28165;&#30340;&#38472;&#36848;&#65292;&#25110;&#32773;&#24102;&#26377;&#36127;&#38754;&#20851;&#38190;&#35789;&#20294;&#20855;&#26377;&#31215;&#26497;&#24847;&#20041;&#30340;&#38472;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
We examine the ability of large language models (LLMs) to generate salient (interesting) negative statements about real-world entities; an emerging research topic of the last few years. We probe the LLMs using zero- and k-shot unconstrained probes, and compare with traditional methods for negation generation, i.e., pattern-based textual extractions and knowledge-graph-based inferences, as well as crowdsourced gold statements. We measure the correctness and salience of the generated lists about subjects from different domains. Our evaluation shows that guided probes do in fact improve the quality of generated negatives, compared to the zero-shot variant. Nevertheless, using both prompts, LLMs still struggle with the notion of factuality of negatives, frequently generating many ambiguous statements, or statements with negative keywords but a positive meaning.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#30005;&#26497;&#32534;&#30721;&#31574;&#30053;&#65292;ElectrodeNet&#65292;&#20197;&#25552;&#39640;&#20154;&#24037;&#32819;&#34583;&#29992;&#25143;&#30340;&#35821;&#38899;&#24863;&#30693;&#33021;&#21147;&#65292;&#20855;&#22791;&#26377;&#26395;&#26367;&#20195;&#20256;&#32479;&#30340;&#20449;&#23553;&#26816;&#27979;&#26041;&#27861;&#21644;CS&#36890;&#36947;&#36873;&#25321;&#26041;&#27861;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.16753</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#30005;&#26497;&#32534;&#30721;&#31574;&#30053;&#29992;&#20110;&#20154;&#24037;&#32819;&#34583;&#22768;&#38899;&#32534;&#30721;
&lt;/p&gt;
&lt;p&gt;
ElectrodeNet -- A Deep Learning Based Sound Coding Strategy for Cochlear Implants. (arXiv:2305.16753v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16753
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#30005;&#26497;&#32534;&#30721;&#31574;&#30053;&#65292;ElectrodeNet&#65292;&#20197;&#25552;&#39640;&#20154;&#24037;&#32819;&#34583;&#29992;&#25143;&#30340;&#35821;&#38899;&#24863;&#30693;&#33021;&#21147;&#65292;&#20855;&#22791;&#26377;&#26395;&#26367;&#20195;&#20256;&#32479;&#30340;&#20449;&#23553;&#26816;&#27979;&#26041;&#27861;&#21644;CS&#36890;&#36947;&#36873;&#25321;&#26041;&#27861;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;ElectrodeNet&#65292;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#20154;&#24037;&#32819;&#34583;&#22768;&#38899;&#32534;&#30721;&#31574;&#30053;&#65292;&#26088;&#22312;&#21462;&#20195;&#20256;&#32479;&#30340;&#20449;&#23553;&#26816;&#27979;&#20351;&#29992;&#21508;&#31181;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#20197;&#27169;&#25311;&#20808;&#36827;&#30340;ACE&#31574;&#30053;&#12290;&#25193;&#23637;&#30340;ElectrodeNet-CS&#31574;&#30053;&#36824;&#21253;&#21547;&#36890;&#36947;&#36873;&#25321;&#65288;CS&#65289;&#12290;&#20351;&#29992;ACE&#31574;&#30053;&#22788;&#29702;&#24178;&#20928;&#35821;&#38899;&#24471;&#21040;&#30340;&#24555;&#36895;&#20613;&#37324;&#21494;&#21464;&#25442;&#21644;&#36890;&#36947;&#20449;&#23553;&#35757;&#32451;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#12289;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#21644;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#30340;&#32593;&#32476;&#27169;&#22411;&#12290;&#20351;&#29992;&#30701;&#26102;&#23458;&#35266;&#28165;&#26224;&#24230;&#65288;STOI&#65289;&#21644;&#24402;&#19968;&#21270;&#21327;&#26041;&#24046;&#24230;&#37327;&#65288;NCM&#65289;&#20272;&#35745;&#20102;ElectrodeNet&#22312;&#20154;&#24037;&#32819;&#34583;&#27169;&#25311;&#20013;&#30340;&#23458;&#35266;&#35821;&#38899;&#29702;&#35299;&#33021;&#21147;&#12290;&#38024;&#23545;&#22768;&#30721;&#21270;&#30340;&#26222;&#36890;&#35805;&#35828;&#35805;&#27979;&#35797;&#36827;&#34892;&#20102;&#27491;&#24120;&#21548;&#21147;&#21463;&#35797;&#32773;&#30340;&#21477;&#23376;&#35782;&#21035;&#27979;&#35797;&#12290;DNN&#12289;CNN&#21644;LSTM&#22522;&#20110;ElectrodeNet&#22312;&#23458;&#35266;&#21644;&#20027;&#35266;&#35780;&#20998;&#19978;&#37117;&#34920;&#29616;&#20986;&#19982;ACE&#30340;&#24378;&#30456;&#20851;&#24615;&#65292;&#20351;&#29992;&#22343;&#26041;&#35823;&#24046;&#65288;MSE&#65289;&#21644;&#30382;&#23572;&#36874;&#30456;&#20851;&#31995;&#25968;&#65288;PCC&#65289;&#12290;&#19982;&#20256;&#32479;&#30340;&#21253;&#32476;&#25552;&#21462;&#21644;&#36890;&#36947;&#35299;&#22797;&#29992;&#26041;&#27861;&#30456;&#27604;&#65292;ElectrodeNet&#34920;&#29616;&#20986;&#20102;&#19968;&#33268;&#30340;&#25552;&#39640;&#35821;&#38899;&#29702;&#35299;&#33021;&#21147;&#30340;&#25928;&#26524;&#12290;ElectrodeNet&#20026;&#20855;&#26377;&#26377;&#38480;&#21548;&#35273;&#36755;&#20837;&#30340;&#20154;&#24037;&#32819;&#34583;&#29992;&#25143;&#25552;&#39640;&#35821;&#38899;&#24863;&#30693;&#30340;&#35299;&#20915;&#26041;&#26696;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#35270;&#37326;&#12290;
&lt;/p&gt;
&lt;p&gt;
ElectrodeNet, a deep learning based sound coding strategy for the cochlear implant (CI), is proposed to emulate the advanced combination encoder (ACE) strategy by replacing the conventional envelope detection using various artificial neural networks. The extended ElectrodeNet-CS strategy further incorporates the channel selection (CS). Network models of deep neural network (DNN), convolutional neural network (CNN), and long short-term memory (LSTM) were trained using the Fast Fourier Transformed bins and channel envelopes obtained from the processing of clean speech by the ACE strategy. Objective speech understanding using short-time objective intelligibility (STOI) and normalized covariance metric (NCM) was estimated for ElectrodeNet using CI simulations. Sentence recognition tests for vocoded Mandarin speech were conducted with normal-hearing listeners. DNN, CNN, and LSTM based ElectrodeNets exhibited strong correlations to ACE in objective and subjective scores using mean squared er
&lt;/p&gt;</description></item><item><title>MULTIGAIN 2.0&#26159;&#19968;&#20010;&#22522;&#20110;PRISM&#30340;&#25511;&#21046;&#22120;&#32508;&#21512;&#24037;&#20855;&#65292;&#25193;&#23637;&#20102;MultiGain&#30340;&#22810;&#32500;&#33021;&#21147;&#65292;&#21487;&#23545;&#20855;&#26377;&#22810;&#32500;&#32422;&#26463;&#30340;&#27010;&#29575;&#31995;&#32479;&#36827;&#34892;&#25511;&#21046;&#22120;&#30340;&#24418;&#24335;&#39564;&#35777;&#21644;&#32508;&#21512;&#65292;&#24182;&#25552;&#20379;&#20102;&#20108;&#32500;&#21644;&#19977;&#32500; Pareto &#26354;&#32447;&#30340;&#21487;&#35270;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.16752</link><description>&lt;p&gt;
MULTIGAIN 2.0&#65306;&#22810;&#20010;&#24179;&#22343;&#22238;&#25253;&#12289;LTL&#21644;&#31283;&#24577;&#32422;&#26463;&#30340;MDP&#25511;&#21046;&#22120;&#32508;&#21512;
&lt;/p&gt;
&lt;p&gt;
MULTIGAIN 2.0: MDP controller synthesis for multiple mean-payoff, LTL and steady-state constraints. (arXiv:2305.16752v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16752
&lt;/p&gt;
&lt;p&gt;
MULTIGAIN 2.0&#26159;&#19968;&#20010;&#22522;&#20110;PRISM&#30340;&#25511;&#21046;&#22120;&#32508;&#21512;&#24037;&#20855;&#65292;&#25193;&#23637;&#20102;MultiGain&#30340;&#22810;&#32500;&#33021;&#21147;&#65292;&#21487;&#23545;&#20855;&#26377;&#22810;&#32500;&#32422;&#26463;&#30340;&#27010;&#29575;&#31995;&#32479;&#36827;&#34892;&#25511;&#21046;&#22120;&#30340;&#24418;&#24335;&#39564;&#35777;&#21644;&#32508;&#21512;&#65292;&#24182;&#25552;&#20379;&#20102;&#20108;&#32500;&#21644;&#19977;&#32500; Pareto &#26354;&#32447;&#30340;&#21487;&#35270;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;MULTIGAIN 2.0&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#27010;&#29575;&#27169;&#22411;&#26816;&#26597;&#22120;PRISM&#26500;&#24314;&#30340;&#25511;&#21046;&#22120;&#32508;&#21512;&#24037;&#20855;MultiGain&#30340;&#19968;&#20010;&#37325;&#22823;&#25193;&#23637;&#12290;&#36825;&#20010;&#26032;&#29256;&#26412;&#25193;&#23637;&#20102;MultiGain&#30340;&#22810;&#30446;&#26631;&#33021;&#21147;&#65292;&#20801;&#35768;&#23545;&#20855;&#26377;&#22810;&#32500;&#38271;&#26399;&#24179;&#22343;&#22238;&#25253;&#32467;&#26500;&#12289;&#31283;&#24577;&#32422;&#26463;&#21644;&#32447;&#24615;&#26102;&#24577;&#36923;&#36753;&#23646;&#24615;&#30340;&#27010;&#29575;&#31995;&#32479;&#36827;&#34892;&#25511;&#21046;&#22120;&#30340;&#24418;&#24335;&#39564;&#35777;&#21644;&#32508;&#21512;&#12290;&#27492;&#22806;&#65292;MULTIGAIN 2.0&#25552;&#20379;&#20102;&#19968;&#31181;&#23547;&#25214;&#26377;&#38480;&#20869;&#23384;&#35299;&#20915;&#26041;&#26696;&#30340;&#26041;&#27861;&#65292;&#24182;&#20855;&#26377;&#20108;&#32500;&#21644;&#19977;&#32500; Pareto &#26354;&#32447;&#30340;&#21487;&#35270;&#21270;&#33021;&#21147;&#65292;&#20197;&#20415;&#22312;&#22810;&#30446;&#26631;&#24773;&#20917;&#19979;&#36827;&#34892;&#26435;&#34913;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present MULTIGAIN 2.0, a major extension to the controller synthesis tool MultiGain, built on top of the probabilistic model checker PRISM. This new version extends MultiGain's multi-objective capabilities, by allowing for the formal verification and synthesis of controllers for probabilistic systems with multi-dimensional long-run average reward structures, steady-state constraints, and linear temporal logic properties. Additionally, MULTIGAIN 2.0 provides an approach for finding finite memory solutions and the capability for two- and three-dimensional visualization of Pareto curves to facilitate trade-off analysis in multi-objective scenarios
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#31163;&#25955;&#21608;&#30028;&#38450;&#24481;&#38382;&#39064;&#30340;&#20998;&#25955;&#33033;&#20914;&#23398;&#20064;&#26694;&#26550;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#22242;&#38431;&#38450;&#24481;&#32773;&#20445;&#25252;&#39046;&#22303;&#30340;&#38382;&#39064;&#65292;&#27599;&#20010;&#38450;&#24481;&#32773;&#21547;&#26377;&#33258;&#24049;&#30340;MLC-SEFRON&#32593;&#32476;&#65292;&#20174;&#32780;&#23454;&#29616;&#20998;&#25955;&#29420;&#31435;&#35757;&#32451;&#65292;&#36755;&#20837;&#20449;&#24687;&#26469;&#28304;&#20110;&#38450;&#24481;&#32773;&#21644;&#20837;&#20405;&#32773;&#30340;&#26102;&#31354;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2305.16748</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#31163;&#25955;&#21608;&#30028;&#38450;&#24481;&#38382;&#39064;&#30340;&#20998;&#25955;&#33033;&#20914;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Decentralized Spike-based Learning Framework for Sequential Capture in Discrete Perimeter Defense Problem. (arXiv:2305.16748v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16748
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#31163;&#25955;&#21608;&#30028;&#38450;&#24481;&#38382;&#39064;&#30340;&#20998;&#25955;&#33033;&#20914;&#23398;&#20064;&#26694;&#26550;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#22242;&#38431;&#38450;&#24481;&#32773;&#20445;&#25252;&#39046;&#22303;&#30340;&#38382;&#39064;&#65292;&#27599;&#20010;&#38450;&#24481;&#32773;&#21547;&#26377;&#33258;&#24049;&#30340;MLC-SEFRON&#32593;&#32476;&#65292;&#20174;&#32780;&#23454;&#29616;&#20998;&#25955;&#29420;&#31435;&#35757;&#32451;&#65292;&#36755;&#20837;&#20449;&#24687;&#26469;&#28304;&#20110;&#38450;&#24481;&#32773;&#21644;&#20837;&#20405;&#32773;&#30340;&#26102;&#31354;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#25955;&#33033;&#20914;&#23398;&#20064;&#65288;DSL&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#22788;&#29702;&#31163;&#25955;&#21608;&#30028;&#38450;&#24481;&#38382;&#39064;&#65288;d-PDP&#65289;&#12290;&#22242;&#38431;&#25805;&#20316;&#38450;&#24481;&#32773;&#29992;&#20110;&#20445;&#25252;&#22278;&#24418;&#39046;&#22303;&#20813;&#21463;&#36752;&#23556;&#24615;&#20837;&#20405;&#32773;&#30340;&#25915;&#20987;&#12290;&#39318;&#20808;&#65292;&#23558;&#31163;&#25955;&#21608;&#30028;&#38450;&#24481;&#38382;&#39064;&#65288;d-PDP&#65289;&#34920;&#36848;&#20026;&#26102;&#31354;&#22810;&#20219;&#21153;&#20998;&#37197;&#38382;&#39064;&#65288;STMTA&#65289;&#12290;&#28982;&#21518;&#23558;STMTA&#38382;&#39064;&#36716;&#25442;&#20026;&#22810;&#26631;&#31614;&#23398;&#20064;&#38382;&#39064;&#65292;&#20197;&#33719;&#21462;&#38450;&#24481;&#32773;&#24517;&#39035;&#35775;&#38382;&#30340;&#21306;&#27573;&#26631;&#31614;&#65292;&#20197;&#20415;&#20445;&#25252;&#21608;&#30028;&#12290;DSL&#26694;&#26550;&#20351;&#29992;Multi-Label Classifier Using Synaptic Efficacy Function Spiking NeuRON&#65288;MLC-SEFRON&#65289;&#32593;&#32476;&#36827;&#34892;&#30830;&#23450;&#24615;&#22810;&#26631;&#31614;&#23398;&#20064;&#12290;&#27599;&#20010;&#38450;&#24481;&#32773;&#37117;&#21253;&#21547;&#21333;&#20010;MLC-SEFRON&#32593;&#32476;&#65292;&#27599;&#20010;MLC-SEFRON&#32593;&#32476;&#37117;&#29420;&#31435;&#35757;&#32451;&#65292;&#20351;&#29992;&#20854;&#33258;&#36523;&#36879;&#35270;&#22270;&#30340;&#36755;&#20837;&#36827;&#34892;&#20998;&#25955;&#25805;&#20316;&#12290; MLC-SEFRON&#32593;&#32476;&#30340;&#36755;&#20837;&#33033;&#20914;&#21487;&#20197;&#30452;&#25509;&#20174;&#38450;&#24481;&#32773;&#21644;&#20837;&#20405;&#32773;&#30340;&#26102;&#31354;&#20449;&#24687;&#20013;&#33719;&#24471;&#65292;&#26080;&#38656;&#39069;&#22806;&#30340;&#39044;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a novel Decentralized Spike-based Learning (DSL) framework for the discrete Perimeter Defense Problem (d-PDP). A team of defenders is operating on the perimeter to protect the circular territory from radially incoming intruders. At first, the d-PDP is formulated as a spatio-temporal multi-task assignment problem (STMTA). The problem of STMTA is then converted into a multi-label learning problem to obtain labels of segments that defenders have to visit in order to protect the perimeter. The DSL framework uses a Multi-Label Classifier using Synaptic Efficacy Function spiking neuRON (MLC-SEFRON) network for deterministic multi-label learning. Each defender contains a single MLC-SEFRON network. Each MLC-SEFRON network is trained independently using input from its own perspective for decentralized operations. The input spikes to the MLC-SEFRON network can be directly obtained from the spatio-temporal information of defenders and intruders without any extra pre-processing
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#30340;&#26041;&#27861;&#65292;&#20197;&#20219;&#21153;&#19981;&#21487;&#30693;&#30340;&#26041;&#24335;&#29983;&#25104;&#31232;&#30095;&#25513;&#30721;&#65292;&#26080;&#38656;&#28155;&#21152;&#26032;&#21442;&#25968;&#65292;&#36991;&#20813;&#20102;&#39069;&#22806;&#30340;&#25512;&#26029;&#24310;&#36831;&#65292;&#24182;&#36229;&#36807;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.16742</link><description>&lt;p&gt;
&#26080;&#38656;&#24341;&#20837;&#26032;&#30340;&#24310;&#36831;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Parameter-Efficient Fine-Tuning without Introducing New Latency. (arXiv:2305.16742v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16742
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#30340;&#26041;&#27861;&#65292;&#20197;&#20219;&#21153;&#19981;&#21487;&#30693;&#30340;&#26041;&#24335;&#29983;&#25104;&#31232;&#30095;&#25513;&#30721;&#65292;&#26080;&#38656;&#28155;&#21152;&#26032;&#21442;&#25968;&#65292;&#36991;&#20813;&#20102;&#39069;&#22806;&#30340;&#25512;&#26029;&#24310;&#36831;&#65292;&#24182;&#36229;&#36807;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#26368;&#36817;&#23637;&#31034;&#20986;&#26126;&#26174;&#30340;&#25104;&#23601;&#65292;&#26377;&#25928;&#22320;&#21305;&#37197;&#20102;&#23436;&#20840;&#24494;&#35843;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#21033;&#29992;&#26126;&#26174;&#26356;&#23569;&#30340;&#21487;&#35757;&#32451;&#21442;&#25968;&#65292;&#22240;&#27492;&#35299;&#20915;&#20102;&#23384;&#20648;&#21644;&#36890;&#20449;&#38480;&#21046;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#21508;&#31181;PEFT&#26041;&#27861;&#20173;&#21463;&#20854;&#22266;&#26377;&#29305;&#24615;&#30340;&#38480;&#21046;&#12290;&#22312;&#31232;&#30095;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#65292;&#36825;&#21482;&#28041;&#21450;&#20462;&#25913;&#29616;&#26377;&#21442;&#25968;&#30340;&#19968;&#23567;&#37096;&#20998;&#65292;&#24494;&#35843;&#21442;&#25968;&#30340;&#36873;&#25321;&#26159;&#20219;&#21153;&#21644;&#39046;&#22495;&#29305;&#23450;&#30340;&#65292;&#22240;&#27492;&#19981;&#36866;&#29992;&#20110;&#32852;&#21512;&#23398;&#20064;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#28155;&#21152;&#26032;&#21442;&#25968;&#30340;PEFT&#26041;&#27861;&#36890;&#24120;&#20250;&#24341;&#20837;&#39069;&#22806;&#30340;&#25512;&#26029;&#24310;&#36831;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20197;&#20219;&#21153;&#19981;&#21487;&#30693;&#30340;&#26041;&#24335;&#29983;&#25104;&#31232;&#30095;&#25513;&#30721;&#30340;&#21487;&#34892;&#24615;&#65292;&#20854;&#20013;&#25152;&#26377;&#19979;&#28216;&#20219;&#21153;&#20849;&#20139;&#30456;&#21516;&#30340;&#25513;&#30721;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20165;&#20381;&#36182;&#20110;&#39044;&#35757;&#32451;&#21442;&#25968;&#30340;&#24133;&#24230;&#20449;&#24687;&#65292;&#36229;&#36807;&#20102;&#29616;&#26377;&#26041;&#27861;&#23398;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Parameter-efficient fine-tuning (PEFT) of pre-trained language models has recently demonstrated remarkable achievements, effectively matching the performance of full fine-tuning while utilizing significantly fewer trainable parameters, and consequently addressing the storage and communication constraints. Nonetheless, various PEFT methods are limited by their inherent characteristics. In the case of sparse fine-tuning, which involves modifying only a small subset of the existing parameters, the selection of fine-tuned parameters is task- and domain-specific, making it unsuitable for federated learning. On the other hand, PEFT methods with adding new parameters typically introduce additional inference latency. In this paper, we demonstrate the feasibility of generating a sparse mask in a task-agnostic manner, wherein all downstream tasks share a common mask. Our approach, which relies solely on the magnitude information of pre-trained parameters, surpasses existing methodologies by a si
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AMPERE&#30340;&#26041;&#27861;&#65292;&#38754;&#21521;AMR&#30340;&#21069;&#32512;&#29983;&#25104;&#20107;&#20214;&#35770;&#20803;&#25277;&#21462;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#25104;&#21151;&#24341;&#20837;&#20102;AMR&#30340;&#20449;&#24687;&#65292;&#25552;&#21319;&#20102;&#29983;&#25104;&#24335;&#27169;&#22411;&#30340;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.16734</link><description>&lt;p&gt;
AMPERE: &#38754;&#21521;AMR&#30340;&#21069;&#32512;&#29983;&#25104;&#20107;&#20214;&#35770;&#20803;&#25277;&#21462;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
AMPERE: AMR-Aware Prefix for Generation-Based Event Argument Extraction Model. (arXiv:2305.16734v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16734
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AMPERE&#30340;&#26041;&#27861;&#65292;&#38754;&#21521;AMR&#30340;&#21069;&#32512;&#29983;&#25104;&#20107;&#20214;&#35770;&#20803;&#25277;&#21462;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#25104;&#21151;&#24341;&#20837;&#20102;AMR&#30340;&#20449;&#24687;&#65292;&#25552;&#21319;&#20102;&#29983;&#25104;&#24335;&#27169;&#22411;&#30340;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20107;&#20214;&#35770;&#20803;&#25277;&#21462;&#26088;&#22312;&#35782;&#21035;&#26576;&#20107;&#20214;&#30340;&#35770;&#20803;&#21450;&#20854;&#29305;&#23450;&#30340;&#35282;&#33394;&#12290;&#29983;&#25104;&#24335;&#30340;&#20107;&#20214;&#35770;&#20803;&#25277;&#21462;&#27169;&#22411;&#30456;&#27604;&#20110;&#20998;&#31867;&#24335;&#27169;&#22411;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#20294;&#26159;&#65292;&#30446;&#21069;&#30340;&#29983;&#25104;&#24335;&#27169;&#22411;&#20027;&#35201;&#20851;&#27880;&#38382;&#39064;&#37325;&#26032;&#34920;&#36848;&#21644;&#25552;&#31034;&#35774;&#35745;&#65292;&#27809;&#26377;&#25972;&#21512;&#20854;&#20182;&#22312;&#20998;&#31867;&#24335;&#27169;&#22411;&#20013;&#24050;&#34987;&#35777;&#26126;&#26377;&#25928;&#30340;&#20449;&#24687;&#65288;&#22914;&#36755;&#20837;&#27573;&#33853;&#30340;&#25277;&#35937;&#24847;&#20041;&#34920;&#31034;AMR&#65289;&#12290;&#30001;&#20110;&#29983;&#25104;&#24335;&#27169;&#22411;&#20013;&#36890;&#24120;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#24418;&#24335;&#65292;&#32780;AMR&#26159;&#19968;&#31181;&#32467;&#26500;&#21270;&#24418;&#24335;&#65292;&#22240;&#27492;&#23558;&#36825;&#26679;&#30340;&#20449;&#24687;&#25972;&#21512;&#21040;&#29983;&#25104;&#24335;&#27169;&#22411;&#20013;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;AMR&#25972;&#21512;&#21040;&#29983;&#25104;&#24335;&#20107;&#20214;&#35770;&#20803;&#25277;&#21462;&#27169;&#22411;&#20013;&#30340;&#31574;&#30053;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;AMPERE&#65292;&#20026;&#29983;&#25104;&#27169;&#22411;&#30340;&#27599;&#23618;&#29983;&#25104;AMR&#24863;&#30693;&#21069;&#32512;&#12290;&#22240;&#27492;&#65292;&#35813;&#21069;&#32512;&#20026;&#29983;&#25104;&#24335;&#30340;&#20107;&#20214;&#35770;&#20803;&#25277;&#21462;&#27169;&#22411;&#24341;&#20837;&#20102;AMR&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Event argument extraction (EAE) identifies event arguments and their specific roles for a given event. Recent advancement in generation-based EAE models has shown great performance and generalizability over classification-based models. However, existing generation-based EAE models mostly focus on problem re-formulation and prompt design, without incorporating additional information that has been shown to be effective for classification-based models, such as the abstract meaning representation (AMR) of the input passages. Incorporating such information into generation-based models is challenging due to the heterogeneous nature of the natural language form prevalently used in generation-based models and the structured form of AMRs. In this work, we study strategies to incorporate AMR into generation-based EAE models. We propose AMPERE, which generates AMR-aware prefixes for every layer of the generation model. Thus, the prefix introduces AMR information to the generation-based EAE model 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;YOLOv8&#31639;&#27861;&#36827;&#34892;&#24515;&#24459;&#22833;&#24120;&#26816;&#27979;&#30340;&#26032;&#24212;&#29992;&#31243;&#24207;&#65292;&#20854;&#27169;&#22411;&#33021;&#22815;&#23454;&#29616;&#25345;&#32493;&#30417;&#27979;&#65292;&#24182;&#20197;&#39640;&#20934;&#30830;&#24615;&#36827;&#34892;&#23454;&#26102;&#24515;&#24459;&#22833;&#24120;&#26816;&#27979;&#12290;</title><link>http://arxiv.org/abs/2305.16727</link><description>&lt;p&gt;
YOLOv8&#23454;&#26102;&#24515;&#24459;&#22833;&#24120;&#26816;&#27979;&#30340;&#26032;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
A novel application for real-time arrhythmia detection using YOLOv8. (arXiv:2305.16727v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16727
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;YOLOv8&#31639;&#27861;&#36827;&#34892;&#24515;&#24459;&#22833;&#24120;&#26816;&#27979;&#30340;&#26032;&#24212;&#29992;&#31243;&#24207;&#65292;&#20854;&#27169;&#22411;&#33021;&#22815;&#23454;&#29616;&#25345;&#32493;&#30417;&#27979;&#65292;&#24182;&#20197;&#39640;&#20934;&#30830;&#24615;&#36827;&#34892;&#23454;&#26102;&#24515;&#24459;&#22833;&#24120;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#38477;&#20302;&#36828;&#31243;&#24515;&#34880;&#31649;&#20581;&#24247;&#30417;&#25252;&#30340;&#21307;&#30103;&#36153;&#29992;&#38656;&#27714;&#36234;&#26469;&#36234;&#39640;&#12290;&#26816;&#27979;&#21644;&#20998;&#31867;&#24515;&#33039;&#24515;&#24459;&#22833;&#24120;&#23545;&#20110;&#35786;&#26029;&#24515;&#33039;&#24322;&#24120;&#24739;&#32773;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24212;&#29992;&#31243;&#24207;&#65292;&#21033;&#29992;&#26368;&#20808;&#36827;&#30340;You-Only-Look-Once&#65288;YOLO&#65289;v8&#31639;&#27861;&#23545;&#21333;&#23548;&#32852;&#24515;&#30005;&#22270;&#20449;&#21495;&#36827;&#34892;&#20998;&#31867;&#65292;&#36827;&#34892;&#24515;&#24459;&#22833;&#24120;&#26816;&#27979;&#12290;&#36890;&#36807;&#23545;MIT-BIH&#25968;&#25454;&#38598;&#36827;&#34892;&#24494;&#35843;&#21518;&#65292;&#24314;&#31435;&#36215;&#19968;&#20010;&#23450;&#21046;&#30340;YOLOv8&#27169;&#22411;&#65292;&#33021;&#22815;&#23454;&#26102;&#26816;&#27979;&#24515;&#24459;&#22833;&#24120;&#65292;&#20197;&#23454;&#29616;&#25345;&#32493;&#30417;&#27979;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;NVIDIA Tesla V100&#19978;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#33021;&#22815;&#20197;0.002&#31186;&#30340;&#26816;&#27979;&#26102;&#38388;&#21644;0.961&#30340;mAP@50&#26816;&#27979;&#24515;&#36339;&#12290;&#30740;&#31350;&#35777;&#26126;&#20102;&#23454;&#26102;&#24515;&#24459;&#22833;&#24120;&#26816;&#27979;&#30340;&#28508;&#21147;&#65292;&#27169;&#22411;&#36755;&#20986;&#21487;&#20197;&#34987;&#35270;&#35273;&#35299;&#37322;&#65292;&#36866;&#29992;&#20110;&#23478;&#24237;&#29992;&#25143;&#12290;&#27492;&#22806;&#65292;&#35813;&#30740;&#31350;&#21487;&#20197;&#24310;&#20280;&#21040;&#24320;&#21457;&#23454;&#26102;&#30340;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#65292;&#24212;&#29992;&#20110;&#24515;&#34880;&#31649;&#20581;&#24247;&#30417;&#25252;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, there has been an increasing need to reduce healthcare costs in remote monitoring of cardiovascular health. Detecting and classifying cardiac arrhythmia is critical to diagnosing patients with cardiac abnormalities. This paper shows that complex systems such as electrocardiograms (ECG) can be applicable for at-home monitoring. This paper proposes a novel application for arrhythmia detection using the state-of-the-art You-Only-Look-Once (YOLO)v8 algorithm to classify single-lead ECG signals. A custom YOLOv8 model was fine-tuned on the MIT-BIH dataset to detect arrhythmia in real-time to allow continuous monitoring. Results show that our model can detect heartbeats with a mAP@50 of 0.961 with a detection time of 0.002s on an NVIDIA Tesla V100. Our study demonstrated the potential of real-time arrhythmia detection, where the model output can be visually interpreted for at-home users. Furthermore, this study could be extended into a real-time XAI model, deployed in the hea
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;GLOSS&#27169;&#22411;&#65292;&#26088;&#22312;&#35299;&#20915;&#22312;&#32570;&#20047;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#21512;&#25104;&#28151;&#21512;&#20195;&#30721;&#25991;&#26412;&#30340;&#38382;&#39064;&#65292;&#24182;&#19988;&#21487;&#20197;&#25512;&#24191;&#21040;&#26356;&#24191;&#27867;&#30340;&#35821;&#35328;&#23545;&#12290;&#35813;&#27169;&#22411;&#22312;&#22235;&#20010;&#26410;&#35265;&#36807;&#30340;&#35821;&#35328;&#23545;&#19978;&#30340;&#23454;&#39564;&#20013;&#20248;&#20110;&#20854;&#20182;&#22522;&#32447;&#27169;&#22411;&#21644;&#22312;&#21333;&#35821;&#25991;&#26412;&#19978;&#36816;&#34892;&#30340;&#29983;&#25104;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.16724</link><description>&lt;p&gt;
&#26410;&#35265;&#36807;&#30340;&#35821;&#35328;&#23545;&#20013;&#30340;&#28151;&#21512;&#20195;&#30721;&#25991;&#26412;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
Code-Switched Text Synthesis in Unseen Language Pairs. (arXiv:2305.16724v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16724
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;GLOSS&#27169;&#22411;&#65292;&#26088;&#22312;&#35299;&#20915;&#22312;&#32570;&#20047;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#21512;&#25104;&#28151;&#21512;&#20195;&#30721;&#25991;&#26412;&#30340;&#38382;&#39064;&#65292;&#24182;&#19988;&#21487;&#20197;&#25512;&#24191;&#21040;&#26356;&#24191;&#27867;&#30340;&#35821;&#35328;&#23545;&#12290;&#35813;&#27169;&#22411;&#22312;&#22235;&#20010;&#26410;&#35265;&#36807;&#30340;&#35821;&#35328;&#23545;&#19978;&#30340;&#23454;&#39564;&#20013;&#20248;&#20110;&#20854;&#20182;&#22522;&#32447;&#27169;&#22411;&#21644;&#22312;&#21333;&#35821;&#25991;&#26412;&#19978;&#36816;&#34892;&#30340;&#29983;&#25104;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#38024;&#23545;&#28151;&#21512;&#20195;&#30721;&#25991;&#26412;&#21512;&#25104;&#30340;&#30740;&#31350;&#22823;&#22810;&#38656;&#35201;&#22312;&#30446;&#26631;&#35821;&#35328;&#23545;&#20013;&#30340;&#28151;&#21512;&#20195;&#30721;&#25991;&#26412;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#36825;&#38480;&#21046;&#20102;&#27169;&#22411;&#22312;&#32570;&#20047;&#28151;&#21512;&#20195;&#30721;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#30340;&#37096;&#32626;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#32570;&#20047;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#21512;&#25104;&#28151;&#21512;&#20195;&#30721;&#25991;&#26412;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;GLOSS&#65292;&#36825;&#26159;&#19968;&#20010;&#24314;&#31435;&#22312;&#39044;&#35757;&#32451;&#22810;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#65288;PMMTM&#65289;&#20043;&#19978;&#65292;&#24182;&#24102;&#26377;&#39069;&#22806;&#30340;&#20195;&#30721;&#20999;&#25442;&#27169;&#22359;&#30340;&#27169;&#22411;&#12290;&#36825;&#20010;&#27169;&#22359;&#65292;&#26080;&#35770;&#26159;&#36866;&#37197;&#22120;&#36824;&#26159;&#39069;&#22806;&#30340;&#21069;&#32512;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20174;&#28151;&#21512;&#20195;&#30721;&#25968;&#25454;&#20013;&#23398;&#20064;&#20195;&#30721;&#20999;&#25442;&#27169;&#24335;&#65292;&#32780;GLOSS&#30340;&#20027;&#35201;&#32452;&#25104;&#37096;&#20998;PMMTM&#34987;&#20923;&#32467;&#12290;&#25105;&#20204;&#21482;&#35843;&#25972;&#20195;&#30721;&#20999;&#25442;&#27169;&#22359;&#30340;&#35774;&#35745;&#65292;&#38450;&#27490;&#27169;&#22411;&#36807;&#24230;&#25311;&#21512;&#38024;&#23545;&#28151;&#21512;&#20195;&#30721;&#35757;&#32451;&#25968;&#25454;&#30340;&#32422;&#26463;&#12290;&#22240;&#27492;&#65292;GLOSS&#34920;&#29616;&#20986;&#20102;&#36328;&#26356;&#24191;&#27867;&#30340;&#35821;&#35328;&#23545;&#36827;&#34892;&#24402;&#32435;&#21644;&#21512;&#25104;&#28151;&#21512;&#20195;&#30721;&#25991;&#26412;&#30340;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#30446;&#26631;&#35821;&#35328;&#21333;&#35821;&#25991;&#26412;&#30340;&#33258;&#35757;&#32451;&#31639;&#27861;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;&#25105;&#20204;&#23545;&#22235;&#20010;&#26410;&#35265;&#36807;&#30340;&#35821;&#35328;&#23545;&#36827;&#34892;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;GLOSS&#20248;&#20110;&#20854;&#20182;&#20174;&#20855;&#26377;&#28151;&#21512;&#20195;&#30721;&#25968;&#25454;&#30340;&#35821;&#35328;&#23545;&#20013;&#35843;&#25972;&#30340;&#27169;&#22411;&#21644;&#22312;&#21333;&#35821;&#25991;&#26412;&#19978;&#36816;&#34892;&#30340;&#29983;&#25104;&#27169;&#22411;&#31561;&#22810;&#20010;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing efforts on text synthesis for code-switching mostly require training on code-switched texts in the target language pairs, limiting the deployment of the models to cases lacking code-switched data. In this work, we study the problem of synthesizing code-switched texts for language pairs absent from the training data. We introduce GLOSS, a model built on top of a pre-trained multilingual machine translation model (PMMTM) with an additional code-switching module. This module, either an adapter or extra prefixes, learns code-switching patterns from code-switched data during training, while the primary component of GLOSS, i.e., the PMMTM, is frozen. The design of only adjusting the code-switching module prevents our model from overfitting to the constrained training data for code-switching. Hence, GLOSS exhibits the ability to generalize and synthesize code-switched texts across a broader spectrum of language pairs. Additionally, we develop a self-training algorithm on target langu
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23618;&#27425;&#24605;&#32500;&#30340;&#20154;&#24037;&#26234;&#33021;&#21327;&#21516;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#23618;&#27425;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#20195;&#29702;&#33021;&#22815;&#26681;&#25454;&#24403;&#21069;&#21512;&#20316;&#20249;&#20276;&#33258;&#21160;&#20999;&#25442;&#26368;&#20339;&#21709;&#24212;&#31574;&#30053;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#20102;&#20195;&#29702;&#19982;&#26032;&#21512;&#20316;&#20249;&#20276;&#30340;&#36866;&#24212;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.16708</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#23618;&#27425;&#24605;&#32500;&#30340;&#20154;&#24037;&#26234;&#33021;&#21327;&#21516;&#35757;&#32451;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Hierarchical Approach to Population Training for Human-AI Collaboration. (arXiv:2305.16708v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16708
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23618;&#27425;&#24605;&#32500;&#30340;&#20154;&#24037;&#26234;&#33021;&#21327;&#21516;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#23618;&#27425;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#20195;&#29702;&#33021;&#22815;&#26681;&#25454;&#24403;&#21069;&#21512;&#20316;&#20249;&#20276;&#33258;&#21160;&#20999;&#25442;&#26368;&#20339;&#21709;&#24212;&#31574;&#30053;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#20102;&#20195;&#29702;&#19982;&#26032;&#21512;&#20316;&#20249;&#20276;&#30340;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#20195;&#29702;&#22312;&#19982;&#26410;&#32463;&#36807;&#35757;&#32451;&#30340;&#21512;&#20316;&#20249;&#20276;&#21327;&#21516;&#26102;&#23384;&#22312;&#22256;&#38590;&#65292;&#29305;&#21035;&#26159;&#24403;&#20195;&#29702;&#19982;&#20154;&#31867;&#21512;&#20316;&#20249;&#20276;&#21512;&#20316;&#26102;&#65292;&#22240;&#20154;&#31867;&#34892;&#20026;&#30340;&#19981;&#19968;&#33268;&#24615;&#32780;&#20986;&#29616;&#34892;&#21160;&#21453;&#24212;&#30340;&#26041;&#24046;&#22686;&#21152;&#65292;&#32780;&#36825;&#21152;&#21095;&#20102;&#36825;&#19968;&#38382;&#39064;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#23558;&#21333;&#20010;&#20195;&#29702;&#35757;&#32451;&#20026;&#23545;&#22810;&#26679;&#21270;&#30340;&#35757;&#32451;&#20249;&#20276;&#20570;&#20986;&#26368;&#20339;&#21709;&#24212;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#20195;&#29702;&#19982;&#26032;&#21512;&#20316;&#20249;&#20276;&#30340;&#36866;&#24212;&#24615;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#22686;&#24378;&#20102;&#22522;&#20110;&#20154;&#21475;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#22522;&#20110;&#23618;&#27425;&#24378;&#21270;&#23398;&#20064;&#65288;HRL&#65289;&#30340;&#26041;&#27861;&#26469;&#23454;&#29616;&#20154;&#24037;&#26234;&#33021;&#21327;&#21516;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#20195;&#29702;&#33021;&#22815;&#23398;&#20064;&#22810;&#20010;&#26368;&#20339;&#21709;&#24212;&#31574;&#30053;&#20316;&#20026;&#20854;&#20302;&#23618;&#31574;&#30053;&#65292;&#21516;&#26102;&#23398;&#20064;&#19968;&#20010;&#20316;&#20026;&#31649;&#29702;&#32773;&#30340;&#39640;&#23618;&#31574;&#30053;&#65292;&#20351;&#20195;&#29702;&#33021;&#22815;&#26681;&#25454;&#20854;&#24403;&#21069;&#30340;&#21512;&#20316;&#20249;&#20276;&#21160;&#24577;&#22320;&#22312;&#20302;&#23618;&#26368;&#20339;&#21709;&#24212;&#31574;&#30053;&#20043;&#38388;&#36827;&#34892;&#20999;&#25442;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;
&lt;/p&gt;
&lt;p&gt;
A major challenge for deep reinforcement learning (DRL) agents is to collaborate with novel partners that were not encountered by them during the training phase. This is specifically worsened by an increased variance in action responses when the DRL agents collaborate with human partners due to the lack of consistency in human behaviors. Recent work have shown that training a single agent as the best response to a diverse population of training partners significantly increases an agent's robustness to novel partners. We further enhance the population-based training approach by introducing a Hierarchical Reinforcement Learning (HRL) based method for Human-AI Collaboration. Our agent is able to learn multiple best-response policies as its low-level policy while at the same time, it learns a high-level policy that acts as a manager which allows the agent to dynamically switch between the low-level best-response policies based on its current partner. We demonstrate that our method is able 
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;Parse-Instructed Prefix&#30340;&#35821;&#27861;&#25511;&#21046;&#37322;&#20041;&#29983;&#25104;&#30340;&#35745;&#31639;&#25104;&#26412;&#38477;&#20302;&#20102;10&#20493;&#65292;&#24182;&#22312;&#20004;&#20010;benchmark&#19978;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.16701</link><description>&lt;p&gt;
PIP&#65306;&#35821;&#27861;&#25511;&#21046;&#30340;&#37322;&#20041;&#29983;&#25104;&#30340;&#35299;&#26512;&#25351;&#23548;&#21069;&#32512;
&lt;/p&gt;
&lt;p&gt;
PIP: Parse-Instructed Prefix for Syntactically Controlled Paraphrase Generation. (arXiv:2305.16701v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16701
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;Parse-Instructed Prefix&#30340;&#35821;&#27861;&#25511;&#21046;&#37322;&#20041;&#29983;&#25104;&#30340;&#35745;&#31639;&#25104;&#26412;&#38477;&#20302;&#20102;10&#20493;&#65292;&#24182;&#22312;&#20004;&#20010;benchmark&#19978;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#27861;&#25511;&#21046;&#30340;&#37322;&#20041;&#29983;&#25104;&#38656;&#35201;&#35821;&#35328;&#27169;&#22411;&#26681;&#25454;&#29305;&#23450;&#30340;&#35821;&#27861;&#32467;&#26500;&#20026;&#21477;&#23376;&#29983;&#25104;&#37322;&#20041;&#12290;&#29616;&#26377;&#30340;fine-tuning&#26041;&#27861;&#38656;&#35201;&#26356;&#26032;&#27169;&#22411;&#30340;&#25152;&#26377;&#21442;&#25968;&#65292;&#25104;&#26412;&#39640;&#26114;&#12290;&#22312;&#21442;&#25968;&#26377;&#25928;&#23398;&#20064;&#30340;&#26368;&#26032;&#30740;&#31350;&#30340;&#21551;&#21457;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Parse-Instructed Prefix (PIP),&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#21069;&#32512;&#35843;&#25972;&#26041;&#27861;&#65292;&#21487;&#22312;&#20302;&#25968;&#25454;&#35774;&#32622;&#20013;&#35843;&#25972;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#26174;&#30528;&#38477;&#20302;&#35757;&#32451;&#25104;&#26412;&#12290; &#25105;&#20204;&#20171;&#32461;&#20102;&#20004;&#31181;&#26041;&#27861;&#26469;&#25351;&#23548;&#27169;&#22411;&#30340;&#32534;&#30721;&#22120;&#21069;&#32512;&#25429;&#33719;&#35821;&#27861;&#30456;&#20851;&#30693;&#35782;&#65306;&#30452;&#25509;&#21021;&#22987;&#21270;&#65288;PIP-Direct&#65289;&#21644;&#38388;&#25509;&#20248;&#21270;&#65288;PIP-Indirect&#65289;&#12290;&#19982;&#20256;&#32479;&#30340;fine-tuning&#26041;&#27861;&#30456;&#27604;&#65292;PIP&#26159;&#19968;&#31181;&#20302;&#35745;&#31639;&#25104;&#26412;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#20855;&#26377;10&#20493;&#26356;&#23569;&#30340;&#21487;&#23398;&#20064;&#21442;&#25968;&#12290;&#19982;&#29616;&#26377;&#30340;&#21069;&#32512;&#35843;&#25972;&#26041;&#27861;&#30456;&#27604;&#65292;PIP&#22312;&#25429;&#33719;&#35821;&#27861;&#25511;&#21046;&#20449;&#24687;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#36825;&#24471;&#30410;&#20110;&#23558;&#35821;&#27861;&#35299;&#26512;&#26641;&#20316;&#20026;&#25351;&#23548;&#21069;&#32512;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;PIP&#22312;&#20004;&#20010;&#35821;&#27861;&#25511;&#21046;&#30340;&#37322;&#20041;&#29983;&#25104;&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#21482;&#38656;&#35201;&#24456;&#23569;&#30340;&#35757;&#32451;&#25968;&#25454;&#21644;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
Syntactically controlled paraphrase generation requires language models to generate paraphrases for sentences according to specific syntactic structures. Existing fine-tuning methods for this task are costly as all the parameters of the model need to be updated during the training process. Inspired by recent studies on parameter-efficient learning, we propose Parse-Instructed Prefix (PIP), a novel adaptation of prefix-tuning to tune large pre-trained language models on syntactically controlled paraphrase generation task in a low-data setting with significantly less training cost. We introduce two methods to instruct a model's encoder prefix to capture syntax-related knowledge: direct initiation (PIP-Direct) and indirect optimization (PIP-Indirect). In contrast to traditional fine-tuning methods for this task, PIP is a compute-efficient alternative with 10 times less learnable parameters. Compared to existing prefix-tuning methods, PIP excels at capturing syntax control information, ach
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#25991;&#31456;&#25506;&#35752;&#20102;&#37319;&#29992;&#36328;&#23398;&#31185;&#26041;&#27861;&#29702;&#35299;&#31639;&#27861;&#20915;&#31574;&#21046;&#23450;&#12290;&#30740;&#31350;&#32773;&#35748;&#20026;&#24212;&#37319;&#29992;&#23398;&#20064;&#31185;&#23398;&#30340;&#23454;&#36341;&#26041;&#27861;&#26469;&#35299;&#37322;ADM&#31995;&#32479;&#65292;&#32780;&#23450;&#24615;&#20219;&#21153;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#20351;&#29992;&#8220;&#20845;&#20010;&#29702;&#35299;&#26041;&#38754;&#8221;&#26694;&#26550;&#21487;&#20197;&#25552;&#39640;&#35299;&#37322;ADM&#31995;&#32479;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.16700</link><description>&lt;p&gt;
&#36816;&#29992;&#36328;&#23398;&#31185;&#26694;&#26550;&#29702;&#35299;&#31639;&#27861;&#20915;&#31574;&#21046;&#23450;
&lt;/p&gt;
&lt;p&gt;
Applying Interdisciplinary Frameworks to Understand Algorithmic Decision-Making. (arXiv:2305.16700v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16700
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#25991;&#31456;&#25506;&#35752;&#20102;&#37319;&#29992;&#36328;&#23398;&#31185;&#26041;&#27861;&#29702;&#35299;&#31639;&#27861;&#20915;&#31574;&#21046;&#23450;&#12290;&#30740;&#31350;&#32773;&#35748;&#20026;&#24212;&#37319;&#29992;&#23398;&#20064;&#31185;&#23398;&#30340;&#23454;&#36341;&#26041;&#27861;&#26469;&#35299;&#37322;ADM&#31995;&#32479;&#65292;&#32780;&#23450;&#24615;&#20219;&#21153;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#20351;&#29992;&#8220;&#20845;&#20010;&#29702;&#35299;&#26041;&#38754;&#8221;&#26694;&#26550;&#21487;&#20197;&#25552;&#39640;&#35299;&#37322;ADM&#31995;&#32479;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35748;&#20026;&#65292;&#37319;&#29992;&#23398;&#20064;&#31185;&#23398;&#20013;&#24050;&#20351;&#29992;&#30340;&#23454;&#36341;&#26041;&#27861;&#26469;&#35299;&#37322;&#8220;&#31639;&#27861;&#20915;&#31574;&#21046;&#23450;&#8221;&#65288;ADM&#65289;&#31995;&#32479;&#21487;&#20197;&#24102;&#26469;&#21033;&#30410;&#12290;&#25105;&#20204;&#31616;&#35201;&#20171;&#32461;&#20102;&#35299;&#37322;ADM&#31995;&#32479;&#30340;&#37325;&#35201;&#24615;&#65292;&#27010;&#36848;&#20102;&#20511;&#37492;&#20854;&#20182;&#23398;&#31185;&#26469;&#25913;&#36827;&#35299;&#37322;&#26041;&#27861;&#30340;&#26041;&#27861;&#65292;&#24182;&#21576;&#29616;&#20102;&#25105;&#20204;&#30340;&#8220;&#20845;&#20010;&#29702;&#35299;&#26041;&#38754;&#8221;&#26694;&#26550;&#30340;&#23450;&#24615;&#20219;&#21153;&#30740;&#31350;&#32467;&#26524;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#20123;&#38382;&#39064;&#65292;&#24341;&#23548;&#26410;&#26469;&#30340;&#30740;&#31350;&#22914;&#20309;&#21033;&#29992;&#36328;&#23398;&#31185;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We argue that explanations for "algorithmic decision-making" (ADM) systems can profit by adopting practices that are already used in the learning sciences. We shortly introduce the importance of explaining ADM systems, give a brief overview of approaches drawing from other disciplines to improve explanations, and present the results of our qualitative task-based study incorporating the "six facets of understanding" framework. We close with questions guiding the discussion of how future studies can leverage an interdisciplinary approach.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#22312;&#38646;&#26679;&#26412;&#35821;&#38899;&#21512;&#25104;&#20013;&#33258;&#21160;&#35843;&#25972;&#25439;&#22833;&#26435;&#34913;&#30340;&#26041;&#27861;&#65292;&#26080;&#38656;&#36229;&#21442;&#25968;&#25628;&#32034;&#12290;&#36890;&#36807;&#27492;&#26041;&#27861;&#65292;VITS-based&#27169;&#22411;&#30340;&#24615;&#33021;&#34920;&#29616;&#24471;&#21040;&#20102;&#26174;&#33879;&#25552;&#21319;&#65292;&#36798;&#21040;&#20102;&#26368;&#26032;&#30340;&#39046;&#20808;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.16699</link><description>&lt;p&gt;
&#26080;&#38656;&#36229;&#21442;&#25968;&#25628;&#32034;&#30340;&#31471;&#21040;&#31471;&#38646;&#26679;&#26412;&#35821;&#38899;&#21512;&#25104;&#20013;&#25439;&#22833;&#26435;&#34913;&#30340;&#33258;&#21160;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
Automatic Tuning of Loss Trade-offs without Hyper-parameter Search in End-to-End Zero-Shot Speech Synthesis. (arXiv:2305.16699v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16699
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#22312;&#38646;&#26679;&#26412;&#35821;&#38899;&#21512;&#25104;&#20013;&#33258;&#21160;&#35843;&#25972;&#25439;&#22833;&#26435;&#34913;&#30340;&#26041;&#27861;&#65292;&#26080;&#38656;&#36229;&#21442;&#25968;&#25628;&#32034;&#12290;&#36890;&#36807;&#27492;&#26041;&#27861;&#65292;VITS-based&#27169;&#22411;&#30340;&#24615;&#33021;&#34920;&#29616;&#24471;&#21040;&#20102;&#26174;&#33879;&#25552;&#21319;&#65292;&#36798;&#21040;&#20102;&#26368;&#26032;&#30340;&#39046;&#20808;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#38646;&#26679;&#26412;TTS&#21644;VC&#26041;&#27861;&#22240;&#20854;&#33021;&#22815;&#29983;&#25104;&#22312;&#35757;&#32451;&#20013;&#20174;&#26410;&#35265;&#36807;&#30340;&#35821;&#38899;&#32780;&#24341;&#36215;&#20102;&#20851;&#27880;&#12290;&#22312;&#36825;&#20123;&#26041;&#27861;&#20013;&#65292;&#22522;&#20110;VITS&#30340;&#38646;&#26679;&#26412;&#20462;&#25913;&#23637;&#29616;&#20986;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#36824;&#20855;&#26377;&#20174;VITS&#32487;&#25215;&#26469;&#30340;&#26377;&#29992;&#23646;&#24615;&#12290;&#28982;&#32780;&#65292;VITS&#21644;&#22522;&#20110;VITS&#30340;&#38646;&#26679;&#26412;&#27169;&#22411;&#30340;&#24615;&#33021;&#22312;&#25439;&#22833;&#22914;&#20309;&#26435;&#34913;&#26041;&#38754;&#23384;&#22312;&#24040;&#22823;&#24046;&#24322;&#12290;&#36825;&#21487;&#33021;&#26159;&#26377;&#38382;&#39064;&#30340;&#65292;&#22240;&#20026;&#23427;&#38656;&#35201;&#32321;&#29712;&#30340;&#35843;&#25972;&#25439;&#22833;&#26435;&#34913;&#36229;&#21442;&#25968;&#20197;&#25214;&#21040;&#26368;&#20339;&#24179;&#34913;&#28857;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#23548;VITS-based&#27169;&#22411;&#30340;&#35299;&#30721;&#22120;&#36798;&#21040;&#20854;&#23436;&#20840;&#37325;&#24314;&#33021;&#21147;&#65292;&#20197;&#25214;&#21040;&#36825;&#20010;&#26368;&#20339;&#28857;&#32780;&#26080;&#38656;&#25628;&#32034;&#12290;&#36890;&#36807;&#25105;&#20204;&#30340;&#26694;&#26550;&#65292;&#25105;&#20204;&#23637;&#29616;&#20102;&#22312;&#38646;&#26679;&#26412;TTS&#21644;VC&#20013;&#27604;&#22522;&#32447;&#26356;&#20248;&#36234;&#30340;&#24615;&#33021;&#65292;&#23454;&#29616;&#20102;&#26368;&#26032;&#30340;&#39046;&#20808;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;&#21508;&#31181;&#35774;&#32622;&#20013;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#22312;&#35752;&#35770;&#20013;&#23545;&#32467;&#26524;&#36827;&#34892;&#20102;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, zero-shot TTS and VC methods have gained attention due to their practicality of being able to generate voices even unseen during training. Among these methods, zero-shot modifications of the VITS model have shown superior performance, while having useful properties inherited from VITS. However, the performance of VITS and VITS-based zero-shot models vary dramatically depending on how the losses are balanced. This can be problematic, as it requires a burdensome procedure of tuning loss balance hyper-parameters to find the optimal balance. In this work, we propose a novel framework that finds this optimum without search, by inducing the decoder of VITS-based models to its full reconstruction ability. With our framework, we show superior performance compared to baselines in zero-shot TTS and VC, achieving state-of-the-art performance. Furthermore, we show the robustness of our framework in various settings. We provide an explanation for the results in the discussion.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24179;&#34913;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#26469;&#35299;&#20915;&#23569;&#26679;&#26412;&#31867;&#22686;&#37327;&#23398;&#20064;&#20013;&#30340;&#24179;&#34913;&#24615;&#38382;&#39064;&#65292;&#24182;&#19988;&#22312;&#19977;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#22343;&#38750;&#24120;&#20248;&#31168;&#12290;</title><link>http://arxiv.org/abs/2305.16687</link><description>&lt;p&gt;
&#38754;&#21521;&#23569;&#26679;&#26412;&#31867;&#22686;&#37327;&#23398;&#20064;&#30340;&#24179;&#34913;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Balanced Supervised Contrastive Learning for Few-Shot Class-Incremental Learning. (arXiv:2305.16687v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16687
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24179;&#34913;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#26469;&#35299;&#20915;&#23569;&#26679;&#26412;&#31867;&#22686;&#37327;&#23398;&#20064;&#20013;&#30340;&#24179;&#34913;&#24615;&#38382;&#39064;&#65292;&#24182;&#19988;&#22312;&#19977;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#22343;&#38750;&#24120;&#20248;&#31168;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23569;&#26679;&#26412;&#31867;&#22686;&#37327;&#23398;&#20064;(FSCIL)&#20027;&#35201;&#38754;&#20020;&#30340;&#25361;&#25112;&#26159;&#22914;&#20309;&#24179;&#34913;&#26032;&#20219;&#21153;&#30340;&#27424;&#25311;&#21512;&#21644;&#36951;&#24536;&#20043;&#21069;&#20219;&#21153;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#24378;&#22823;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#38598;&#25104;&#20102;FSCIL&#32593;&#32476;&#30340;&#26680;&#24515;&#32452;&#20214;&#30340;&#26377;&#25928;&#26041;&#27861;&#65292;&#21253;&#25324;&#29305;&#24449;&#25552;&#21462;&#22120;&#12289;&#22522;&#30784;&#20250;&#35805;&#20998;&#31867;&#22120;&#21644;&#22686;&#37327;&#20250;&#35805;&#20998;&#31867;&#22120;&#12290;&#22312;&#29305;&#24449;&#25552;&#21462;&#22120;&#30340;&#35757;&#32451;&#20013;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#33719;&#24471;&#24179;&#34913;&#30340;&#36890;&#29992;&#34920;&#31034;&#65292;&#26082;&#26377;&#21033;&#20110;&#24403;&#21069;&#21487;&#35265;&#31867;&#65292;&#21448;&#26377;&#21033;&#20110;&#26410;&#26469;&#25110;&#36807;&#21435;&#30340;&#31867;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24179;&#34913;&#30340;&#30417;&#30563;&#23545;&#27604;&#25439;&#22833;&#65292;&#21487;&#20197;&#26377;&#25928;&#24179;&#34913;&#36825;&#20004;&#20010;&#30446;&#26631;&#12290;&#22312;&#20998;&#31867;&#22120;&#26041;&#38754;&#65292;&#25105;&#20204;&#20998;&#26512;&#24182;&#24378;&#35843;&#20102;&#23545;&#20110;&#22522;&#30784;&#21644;&#22686;&#37327;&#20250;&#35805;&#20998;&#31867;&#22120;&#30340;&#32479;&#19968;&#21021;&#22987;&#21270;&#26041;&#27861;&#30340;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;CUB200&#12289;CIFAR100&#21644;miniImagenet&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#26032;&#20219;&#21153;&#23398;&#20064;&#33021;&#21147;&#21644;&#38450;&#27490;&#36951;&#24536;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Few-shot class-incremental learning (FSCIL) presents the primary challenge of balancing underfitting to a new session's task and forgetting the tasks from previous sessions. To address this challenge, we develop a simple yet powerful learning scheme that integrates effective methods for each core component of the FSCIL network, including the feature extractor, base session classifiers, and incremental session classifiers. In feature extractor training, our goal is to obtain balanced generic representations that benefit both current viewable and unseen or past classes. To achieve this, we propose a balanced supervised contrastive loss that effectively balances these two objectives. In terms of classifiers, we analyze and emphasize the importance of unifying initialization methods for both the base and incremental session classifiers. Our method demonstrates outstanding ability for new task learning and preventing forgetting on CUB200, CIFAR100, and miniImagenet datasets, with significan
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#22522;&#20110;&#21512;&#25104;&#26631;&#35782;&#31526;&#30340;&#22810;&#35270;&#35282;&#26631;&#35782;&#31526;&#26469;&#22686;&#24378;&#29983;&#25104;&#24335;&#26816;&#32034;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#26816;&#32034;&#32467;&#26524;&#30340;&#20934;&#30830;&#24615;&#21644;&#22810;&#26679;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.16675</link><description>&lt;p&gt;
&#22810;&#35270;&#35282;&#26631;&#35782;&#22686;&#24378;&#29983;&#25104;&#24335;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Multiview Identifiers Enhanced Generative Retrieval. (arXiv:2305.16675v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16675
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#22522;&#20110;&#21512;&#25104;&#26631;&#35782;&#31526;&#30340;&#22810;&#35270;&#35282;&#26631;&#35782;&#31526;&#26469;&#22686;&#24378;&#29983;&#25104;&#24335;&#26816;&#32034;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#26816;&#32034;&#32467;&#26524;&#30340;&#20934;&#30830;&#24615;&#21644;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;&#20854;&#31616;&#21333;&#22320;&#23558;&#26597;&#35810;&#19982;&#29616;&#26377;&#27573;&#33853;&#21305;&#37197;&#65292;&#29983;&#25104;&#24335;&#26816;&#32034;&#29983;&#25104;&#27573;&#33853;&#30340;&#26631;&#35782;&#31526;&#23383;&#31526;&#20018;&#20316;&#20026;&#26816;&#32034;&#30446;&#26631;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26631;&#35782;&#31526;&#24517;&#39035;&#36275;&#22815;&#29420;&#29305;&#20197;&#20195;&#34920;&#19968;&#20010;&#27573;&#33853;&#12290;&#24403;&#21069;&#30340;&#26041;&#27861;&#20351;&#29992;&#25968;&#23383;ID&#25110;&#25991;&#26412;&#29255;&#27573;&#65288;&#22914;&#26631;&#39064;&#25110;&#23376;&#23383;&#31526;&#20018;&#65289;&#20316;&#20026;&#26631;&#35782;&#31526;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26631;&#35782;&#31526;&#19981;&#33021;&#24456;&#22909;&#22320;&#35206;&#30422;&#19968;&#20010;&#27573;&#33853;&#30340;&#20869;&#23481;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31867;&#22411;&#30340;&#26631;&#35782;&#31526;&#65292;&#21363;&#22522;&#20110;&#27573;&#33853;&#20869;&#23481;&#29983;&#25104;&#30340;&#21512;&#25104;&#26631;&#35782;&#31526;&#65292;&#21487;&#20197;&#25972;&#21512;&#25991;&#26412;&#29255;&#27573;&#32570;&#20047;&#30340;&#24773;&#22659;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21516;&#26102;&#32771;&#34385;&#22810;&#35270;&#35282;&#26631;&#35782;&#31526;&#65292;&#21253;&#25324;&#21512;&#25104;&#26631;&#35782;&#31526;&#12289;&#26631;&#39064;&#21644;&#23376;&#23383;&#31526;&#20018;&#12290;&#36825;&#20123;&#26631;&#35782;&#31526;&#30340;&#35270;&#35282;&#30456;&#20114;&#34917;&#20805;&#65292;&#26377;&#21161;&#20110;&#20174;&#22810;&#20010;&#35282;&#24230;&#32508;&#21512;&#25490;&#21517;&#27573;&#33853;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#29983;&#25104;&#24335;&#26816;&#32034;&#20013;&#34920;&#29616;&#26368;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;
Instead of simply matching a query to pre-existing passages, generative retrieval generates identifier strings of passages as the retrieval target. At a cost, the identifier must be distinctive enough to represent a passage. Current approaches use either a numeric ID or a text piece (such as a title or substrings) as the identifier. However, these identifiers cannot cover a passage's content well. As such, we are motivated to propose a new type of identifier, synthetic identifiers, that are generated based on the content of a passage and could integrate contextualized information that text pieces lack. Furthermore, we simultaneously consider multiview identifiers, including synthetic identifiers, titles, and substrings. These views of identifiers complement each other and facilitate the holistic ranking of passages from multiple perspectives. We conduct a series of experiments on three public datasets, and the results indicate that our proposed approach performs the best in generative 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#19968;&#31995;&#21015;&#35774;&#32622;&#21644; Oracle &#35775;&#38382;&#31867;&#22411;&#30340;&#32479;&#19968;&#26041;&#27861;&#65292;&#29992;&#20110;&#26368;&#22823;&#21270;&#36830;&#32493; DR-submodular &#20989;&#25968;&#65292;&#20026; 16 &#31181;&#24773;&#20917;&#20013;&#30340; 9 &#31181;&#25552;&#20379;&#20102;&#26032;&#30340;/&#25913;&#36827;&#30340;&#32467;&#26524;&#65292;&#24182;&#19988;&#38024;&#23545;&#22522;&#20110;&#38543;&#26426;&#20989;&#25968;&#20540;&#30340; Oracle &#21462;&#24471;&#20102;&#31532;&#19968;&#20010;&#36866;&#29992;&#20110;&#38543;&#26426; DR-submodular &#20989;&#25968;&#30340;&#21518;&#24724;&#30028;&#38480;&#12290;</title><link>http://arxiv.org/abs/2305.16671</link><description>&lt;p&gt;
&#19968;&#31181;&#32479;&#19968;&#30340;&#26041;&#27861;&#29992;&#20110;&#26368;&#22823;&#21270;&#36830;&#32493; DR-submodular &#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
A Unified Approach for Maximizing Continuous DR-submodular Functions. (arXiv:2305.16671v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16671
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#19968;&#31995;&#21015;&#35774;&#32622;&#21644; Oracle &#35775;&#38382;&#31867;&#22411;&#30340;&#32479;&#19968;&#26041;&#27861;&#65292;&#29992;&#20110;&#26368;&#22823;&#21270;&#36830;&#32493; DR-submodular &#20989;&#25968;&#65292;&#20026; 16 &#31181;&#24773;&#20917;&#20013;&#30340; 9 &#31181;&#25552;&#20379;&#20102;&#26032;&#30340;/&#25913;&#36827;&#30340;&#32467;&#26524;&#65292;&#24182;&#19988;&#38024;&#23545;&#22522;&#20110;&#38543;&#26426;&#20989;&#25968;&#20540;&#30340; Oracle &#21462;&#24471;&#20102;&#31532;&#19968;&#20010;&#36866;&#29992;&#20110;&#38543;&#26426; DR-submodular &#20989;&#25968;&#30340;&#21518;&#24724;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#26368;&#22823;&#21270;&#36830;&#32493;&#30340; DR-submodular &#20989;&#25968;&#65292;&#28085;&#30422;&#20102;&#19968;&#31995;&#21015;&#35774;&#32622;&#21644; Oracle &#35775;&#38382;&#31867;&#22411;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#38024;&#23545;&#21333;&#35843;&#21644;&#38750;&#21333;&#35843;&#20989;&#25968;&#30340; Frank-Wolfe &#31867;&#22411;&#31163;&#32447;&#31639;&#27861;&#65292;&#20855;&#26377;&#19981;&#21516;&#30340;&#19968;&#33324;&#20984;&#38598;&#38480;&#21046;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102; Oracle &#25552;&#20379;&#20989;&#25968;&#26799;&#24230;&#25110;&#20165;&#20989;&#25968;&#20540;&#30340;&#35775;&#38382;&#20197;&#21450;&#30830;&#23450;&#24615;&#25110;&#38543;&#26426;&#24615;&#35775;&#38382;&#30340;&#35774;&#32622;&#12290;&#25105;&#20204;&#22312;&#25152;&#26377;&#24773;&#20917;&#19979;&#30830;&#23450;&#20102;&#25152;&#38656;&#30340; Oracle &#35775;&#38382;&#25968;&#37327;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20026; 16 &#20010;&#32771;&#34385;&#30340;&#24773;&#20917;&#20013;&#30340; 9 &#20010;&#25552;&#20379;&#20102;&#26032;&#30340;/&#25913;&#36827;&#30340;&#32467;&#26524;&#65292;&#22312;&#20004;&#20010;&#24773;&#20917;&#19979;&#36991;&#20813;&#20102;&#35745;&#31639;&#19978;&#26114;&#36149;&#30340;&#25237;&#24433;&#65292;&#32780;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#22312;&#20854;&#20313;&#20116;&#20010;&#24773;&#20917;&#19979;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#21305;&#37197;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#38024;&#23545;&#22522;&#20110;&#38543;&#26426;&#20989;&#25968;&#20540;&#30340; Oracle &#30340;&#26041;&#27861;&#65292;&#20026;&#38543;&#26426; DR-submodular &#20989;&#25968;&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#24102;&#26377;&#25506;&#38505;&#21453;&#39304;&#30340;&#21518;&#24724;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a unified approach for maximizing continuous DR-submodular functions that encompasses a range of settings and oracle access types. Our approach includes a Frank-Wolfe type offline algorithm for both monotone and non-monotone functions, with different restrictions on the general convex set. We consider settings where the oracle provides access to either the gradient of the function or only the function value, and where the oracle access is either deterministic or stochastic. We determine the number of required oracle accesses in all cases. Our approach gives new/improved results for nine out of the sixteen considered cases, avoids computationally expensive projections in two cases, with the proposed framework matching performance of state-of-the-art approaches in the remaining five cases. Notably, our approach for the stochastic function value-based oracle enables the first regret bounds with bandit feedback for stochastic DR-submodular functions.
&lt;/p&gt;</description></item><item><title>&#39318;&#27425;&#37319;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#21897;&#38236;&#22270;&#20687;&#20013;&#30340;&#24739;&#32773;&#22522;&#26412;&#20449;&#24687;&#65292;&#20174;&#32780;&#25552;&#39640;&#26816;&#27979;&#22120;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.16661</link><description>&lt;p&gt;
&#22522;&#20110;&#21897;&#38236;&#22270;&#20687;&#39044;&#27979;&#24739;&#32773;&#22522;&#26412;&#20449;&#24687;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Gender, Smoking History and Age Prediction from Laryngeal Images. (arXiv:2305.16661v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16661
&lt;/p&gt;
&lt;p&gt;
&#39318;&#27425;&#37319;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#21897;&#38236;&#22270;&#20687;&#20013;&#30340;&#24739;&#32773;&#22522;&#26412;&#20449;&#24687;&#65292;&#20174;&#32780;&#25552;&#39640;&#26816;&#27979;&#22120;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24212;&#29992;&#28789;&#27963;&#21897;&#38236;&#26816;&#26597;&#21487;&#20197;&#26816;&#27979;&#21897;&#37096;&#30142;&#30149;&#21644;&#35782;&#21035;&#28508;&#22312;&#30340;&#24694;&#24615;&#30149;&#21464;&#12290;&#30740;&#31350;&#20154;&#21592;&#20204;&#26368;&#36817;&#24320;&#22987;&#24212;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#20351;&#29992;&#21897;&#38236;&#22270;&#20687;&#36827;&#34892;&#33258;&#21160;&#35786;&#26029;&#65292;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#25104;&#26524;&#12290;&#26412;&#30740;&#31350;&#23581;&#35797;&#39318;&#27425;&#37319;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#24739;&#32773;&#30340;&#22522;&#26412;&#20449;&#24687;&#65292;&#20174;&#32780;&#25552;&#39640;&#26816;&#27979;&#22120;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#24615;&#21035;&#12289;&#21560;&#28895;&#21490;&#21644;&#24180;&#40836;&#30340;&#20934;&#30830;&#29575;&#20998;&#21035;&#36798;&#21040;&#20102;85.5%&#12289;65.2%&#21644;75.9%&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#21019;&#36896;&#20102;&#19968;&#20010;&#20840;&#26032;&#30340;&#21897;&#38236;&#22270;&#20687;&#25968;&#25454;&#38598;&#65292;&#22522;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;Transformer&#30340;8&#31181;&#32463;&#20856;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#24471;&#21040;&#20102;&#27979;&#35797;&#65292;&#24182;&#21487;&#20197;&#23558;&#32467;&#26524;&#38598;&#25104;&#21040;&#24403;&#21069;&#30340;&#23398;&#20064;&#27169;&#22411;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Flexible laryngoscopy is commonly performed by otolaryngologists to detect laryngeal diseases and to recognize potentially malignant lesions. Recently, researchers have introduced machine learning techniques to facilitate automated diagnosis using laryngeal images and achieved promising results. Diagnostic performance can be improved when patients' demographic information is incorporated into models. However, manual entry of patient data is time consuming for clinicians. In this study, we made the first endeavor to employ deep learning models to predict patient demographic information to improve detector model performance. The overall accuracy for gender, smoking history, and age was 85.5%, 65.2%, and 75.9%, respectively. We also created a new laryngoscopic image set for machine learning study and benchmarked the performance of 8 classical deep learning models based on CNNs and Transformers. The results can be integrated into current learning models to improve their performance by inco
&lt;/p&gt;</description></item><item><title>LLM&#20195;&#29702;&#21487;&#20197;&#36890;&#36807;Adaplanner&#33258;&#36866;&#24212;&#25913;&#36827;&#33258;&#24049;&#30340;&#35745;&#21010;&#20197;&#24212;&#23545;&#29615;&#22659;&#21453;&#39304;&#65292;&#20026;&#27492;&#25552;&#20986;&#35745;&#21010;&#20869;&#22806;&#30340;&#25913;&#36827;&#31574;&#30053;&#20197;&#21450;&#20195;&#30721;&#39118;&#26684;&#30340;LLM&#25552;&#31034;&#32467;&#26500;&#21644;&#25216;&#33021;&#21457;&#29616;&#26426;&#21046;&#12290;</title><link>http://arxiv.org/abs/2305.16653</link><description>&lt;p&gt;
AdaPlanner:&#33258;&#36866;&#24212;&#35268;&#21010;&#19982;&#35821;&#35328;&#27169;&#22411;&#30340;&#21453;&#39304;&#12290; &#65288;arXiv&#65306;2305.16653v1 [cs.CL]&#65289;
&lt;/p&gt;
&lt;p&gt;
AdaPlanner: Adaptive Planning from Feedback with Language Models. (arXiv:2305.16653v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16653
&lt;/p&gt;
&lt;p&gt;
LLM&#20195;&#29702;&#21487;&#20197;&#36890;&#36807;Adaplanner&#33258;&#36866;&#24212;&#25913;&#36827;&#33258;&#24049;&#30340;&#35745;&#21010;&#20197;&#24212;&#23545;&#29615;&#22659;&#21453;&#39304;&#65292;&#20026;&#27492;&#25552;&#20986;&#35745;&#21010;&#20869;&#22806;&#30340;&#25913;&#36827;&#31574;&#30053;&#20197;&#21450;&#20195;&#30721;&#39118;&#26684;&#30340;LLM&#25552;&#31034;&#32467;&#26500;&#21644;&#25216;&#33021;&#21457;&#29616;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23637;&#31034;&#20102;&#22312;&#24207;&#21015;&#20915;&#31574;&#20219;&#21153;&#20013;&#20316;&#20026;&#33258;&#20027;&#20195;&#29702;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#35201;&#20040;&#36138;&#23146;&#22320;&#37319;&#21462;&#34892;&#21160;&#32780;&#27809;&#26377;&#35745;&#21010;&#65292;&#35201;&#20040;&#20381;&#36182;&#20110;&#19981;&#21487;&#36866;&#24212;&#29615;&#22659;&#21453;&#39304;&#30340;&#38745;&#24577;&#35745;&#21010;&#12290;&#22240;&#27492;&#65292;&#38543;&#30528;&#38382;&#39064;&#22797;&#26434;&#24615;&#21644;&#35745;&#21010;&#27700;&#24179;&#30340;&#22686;&#21152;&#65292;LLM&#20195;&#29702;&#30340;&#39034;&#24207;&#20915;&#31574;&#24615;&#33021;&#20250;&#36864;&#21270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38381;&#29615;&#26041;&#27861;AdaPlanner&#65292;&#23427;&#20801;&#35768;LLM&#20195;&#29702;&#26681;&#25454;&#29615;&#22659;&#21453;&#39304;&#33258;&#36866;&#24212;&#22320;&#25913;&#36827;&#20854;&#33258;&#21160;&#29983;&#25104;&#30340;&#35745;&#21010;&#12290;&#22312;AdaPlanner&#20013;&#65292;LLM&#20195;&#29702;&#36890;&#36807;&#35745;&#21010;&#20869;&#21644;&#35745;&#21010;&#22806;&#30340;&#25913;&#36827;&#31574;&#30053;&#33258;&#36866;&#24212;&#22320;&#25913;&#36827;&#20854;&#35745;&#21010;&#12290;&#20026;&#20102;&#20943;&#36731;&#24187;&#35273;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#20195;&#30721;&#39118;&#26684;&#30340;LLM&#25552;&#31034;&#32467;&#26500;&#65292;&#20419;&#36827;&#20102;&#36328;&#21508;&#31181;&#20219;&#21153;&#65292;&#29615;&#22659;&#21644;&#20195;&#29702;&#33021;&#21147;&#30340;&#35745;&#21010;&#29983;&#25104;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25216;&#33021;&#21457;&#29616;&#26426;&#21046;&#65292;&#21033;&#29992;&#25104;&#21151;&#30340;&#35745;&#21010;&#20316;&#20026;&#23569;&#37327;&#31034;&#20363;&#65292;&#20351;&#35745;&#21010;&#26356;&#20855;&#26222;&#36866;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have recently demonstrated the potential in acting as autonomous agents for sequential decision-making tasks. However, most existing methods either take actions greedily without planning or rely on static plans that are not adaptable to environmental feedback. Consequently, the sequential decision-making performance of LLM agents degenerates with problem complexity and plan horizons increase. We propose a closed-loop approach, AdaPlanner, which allows the LLM agent to refine its self-generated plan adaptively in response to environmental feedback. In AdaPlanner, the LLM agent adaptively refines its plan from feedback with both in-plan and out-of-plan refinement strategies. To mitigate hallucination, we develop a code-style LLM prompt structure that facilitates plan generation across a variety of tasks, environments, and agent capabilities. Furthermore, we propose a skill discovery mechanism that leverages successful plans as few-shot exemplars, enabling the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24314;&#27169;&#21644;&#39044;&#27979;&#26694;&#26550;&#65292;&#22312;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#30340;&#32477;&#23545;&#25512;&#29702;&#33021;&#21147;&#26469;&#25552;&#39640;&#20107;&#20214;&#24207;&#21015;&#27169;&#22411;&#30340;&#39044;&#27979;&#31934;&#24230;&#65292;&#32463;&#36807;&#23454;&#39564;&#35777;&#23454;&#21487;&#20197;&#26126;&#26174;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#20107;&#20214;&#24207;&#21015;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.16646</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#23569;&#26679;&#26412;&#30340;&#32477;&#23545;&#25512;&#29702;&#26469;&#25552;&#39640;&#20107;&#20214;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Language Models Can Improve Event Prediction by Few-Shot Abductive Reasoning. (arXiv:2305.16646v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16646
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24314;&#27169;&#21644;&#39044;&#27979;&#26694;&#26550;&#65292;&#22312;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#30340;&#32477;&#23545;&#25512;&#29702;&#33021;&#21147;&#26469;&#25552;&#39640;&#20107;&#20214;&#24207;&#21015;&#27169;&#22411;&#30340;&#39044;&#27979;&#31934;&#24230;&#65292;&#32463;&#36807;&#23454;&#39564;&#35777;&#23454;&#21487;&#20197;&#26126;&#26174;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#20107;&#20214;&#24207;&#21015;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#25512;&#29702;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#24778;&#20154;&#30340;&#24615;&#33021;&#12290;&#26412;&#25991;&#30740;&#31350;&#23427;&#20204;&#26159;&#21542;&#21487;&#20197;&#25512;&#29702;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#20107;&#20214;&#65292;&#24110;&#21161;&#25552;&#39640;&#20107;&#20214;&#24207;&#21015;&#27169;&#22411;&#30340;&#39044;&#27979;&#31934;&#24230;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#24314;&#27169;&#21644;&#39044;&#27979;&#26694;&#26550;&#65292;&#20854;&#20013;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25191;&#34892;&#32477;&#23545;&#25512;&#29702;&#20197;&#36741;&#21161;&#20107;&#20214;&#24207;&#21015;&#27169;&#22411;&#65306;&#20107;&#20214;&#27169;&#22411;&#22312;&#32473;&#23450;&#36807;&#21435;&#30340;&#24773;&#20917;&#19979;&#25552;&#20986;&#26410;&#26469;&#20107;&#20214;&#30340;&#39044;&#27979;; &#22312;&#20960;&#20010;&#19987;&#23478;&#27880;&#37322;&#31034;&#33539;&#30340;&#25351;&#23548;&#19979;&#65292;&#35821;&#35328;&#27169;&#22411;&#23398;&#20250;&#20102;&#20026;&#27599;&#20010;&#25552;&#35758;&#25552;&#20379;&#21487;&#33021;&#30340;&#21407;&#22240;; &#19968;&#20010;&#25628;&#32034;&#27169;&#22359;&#25214;&#21040;&#19982;&#21407;&#22240;&#21305;&#37197;&#30340;&#20808;&#21069;&#20107;&#20214;; &#19968;&#20010;&#35780;&#20998;&#20989;&#25968;&#23398;&#20250;&#26816;&#26597;&#26816;&#32034;&#21040;&#30340;&#20107;&#20214;&#26159;&#21542;&#23454;&#38469;&#19978;&#21487;&#20197;&#23548;&#33268;&#25552;&#35758;&#12290;&#36890;&#36807;&#22312;&#20004;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#65288;&#20122;&#39532;&#36874;&#35780;&#35770;&#21644;GDELT&#65289;&#19978;&#36827;&#34892;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550; - &#30001;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147; - &#21487;&#20197;&#22312;&#20302;&#25968;&#25454;&#24773;&#20917;&#19979;&#26126;&#26174;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#20107;&#20214;&#24207;&#21015;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models have shown astonishing performance on a wide range of reasoning tasks. In this paper, we investigate whether they could reason about real-world events and help improve the prediction accuracy of event sequence models. We design a modeling and prediction framework where a large language model performs abductive reasoning to assist an event sequence model: the event model proposes predictions on future events given the past; instructed by a few expert-annotated demonstrations, the language model learns to suggest possible causes for each proposal; a search module finds out the previous events that match the causes; a scoring function learns to examine whether the retrieved events could actually cause the proposal. Through extensive experiments on two challenging real-world datasets (Amazon Review and GDELT), we demonstrate that our framework -- thanks to the reasoning ability of language models -- could significantly outperform the state-of-the-art event sequence mo
&lt;/p&gt;</description></item><item><title>&#35780;&#20272;QG&#26041;&#27861;&#38656;&#35201;&#26356;&#22810;&#30340;&#21442;&#32771;&#25991;&#29486;&#26469;&#25552;&#39640;&#20854;&#26377;&#25928;&#24615;&#65292;&#21333;&#20010;&#21442;&#32771;&#19981;&#36275;&#20197;&#20840;&#38754;&#35780;&#20272;&#20854;&#28508;&#21147;&#12290;&#20351;&#29992;&#22810;&#20010;&#21442;&#32771;&#25991;&#29486;&#30340;&#35780;&#20272;&#26041;&#27861;&#21487;&#20197;&#26356;&#22909;&#22320;&#19982;&#20154;&#31867;&#35780;&#20272;&#30456;&#20851;&#32852;&#12290;</title><link>http://arxiv.org/abs/2305.16626</link><description>&lt;p&gt;
&#35780;&#20272;&#38382;&#31572;&#29983;&#25104;&#38656;&#35201;&#26356;&#22810;&#30340;&#21442;&#32771;&#25991;&#29486;
&lt;/p&gt;
&lt;p&gt;
Evaluation of Question Generation Needs More References. (arXiv:2305.16626v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16626
&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;QG&#26041;&#27861;&#38656;&#35201;&#26356;&#22810;&#30340;&#21442;&#32771;&#25991;&#29486;&#26469;&#25552;&#39640;&#20854;&#26377;&#25928;&#24615;&#65292;&#21333;&#20010;&#21442;&#32771;&#19981;&#36275;&#20197;&#20840;&#38754;&#35780;&#20272;&#20854;&#28508;&#21147;&#12290;&#20351;&#29992;&#22810;&#20010;&#21442;&#32771;&#25991;&#29486;&#30340;&#35780;&#20272;&#26041;&#27861;&#21487;&#20197;&#26356;&#22909;&#22320;&#19982;&#20154;&#31867;&#35780;&#20272;&#30456;&#20851;&#32852;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38382;&#31572;&#29983;&#25104;(QG)&#26159;&#19968;&#39033;&#20219;&#21153;&#65292;&#22522;&#20110;&#32473;&#23450;&#30340;&#19978;&#19979;&#25991;&#21644;&#30446;&#26631;&#31572;&#26696;&#29983;&#25104;&#19968;&#20010;&#26377;&#25928;&#21644;&#27969;&#30021;&#30340;&#38382;&#39064;&#12290;&#26681;&#25454;&#19981;&#21516;&#30340;&#30446;&#30340;&#65292;&#21363;&#20351;&#32473;&#23450;&#30456;&#21516;&#30340;&#19978;&#19979;&#25991;&#65292;&#25945;&#24072;&#20063;&#21487;&#20197;&#25552;&#20986;&#20851;&#20110;&#19981;&#21516;&#27010;&#24565;&#30340;&#38382;&#39064;&#65292;&#29978;&#33267;&#30456;&#21516;&#30340;&#27010;&#24565;&#20063;&#21487;&#20197;&#29992;&#19981;&#21516;&#30340;&#26041;&#24335;&#20070;&#20889;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;QG&#30340;&#35780;&#20272;&#36890;&#24120;&#20381;&#36182;&#20110;&#21333;&#20010;&#22522;&#20110;&#21442;&#32771;&#30340;&#30456;&#20284;&#24615;&#24230;&#37327;&#65292;&#20363;&#22914;n-gram&#24230;&#37327;&#25110;&#23398;&#20064;&#24230;&#37327;&#65292;&#36825;&#19981;&#36275;&#20197;&#20805;&#20998;&#35780;&#20272;QG&#26041;&#27861;&#30340;&#28508;&#21147;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24314;&#35758;&#37325;&#26032;&#34920;&#36848;&#21442;&#32771;&#38382;&#39064;&#65292;&#20197;&#36827;&#34892;&#26356;&#24378;&#20581;&#30340;QG&#35780;&#20272;&#12290;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#22914;GPT-3&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#35821;&#20041;&#21644;&#21477;&#27861;&#22810;&#26679;&#30340;&#38382;&#39064;&#65292;&#28982;&#21518;&#37319;&#29992;&#27969;&#34892;&#30340;&#35780;&#20272;&#25351;&#26631;&#30340;&#31616;&#21333;&#32858;&#21512;&#20316;&#20026;&#26368;&#32456;&#24471;&#20998;&#12290;&#36890;&#36807;&#25105;&#20204;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#21457;&#29616;&#20351;&#29992;&#22810;&#20010;&#65288;&#20266;&#65289;&#21442;&#32771;&#25991;&#29486;&#23545;&#20110;QG&#35780;&#20272;&#26356;&#26377;&#25928;&#65292;&#21516;&#26102;&#19982;&#20154;&#31867;&#35780;&#20272;&#30340;&#30456;&#20851;&#24615;&#26356;&#39640;&#65292;&#32780;&#21333;&#20010;&#21442;&#32771;&#30340;&#35780;&#20272;&#21017;&#30456;&#23545;&#36739;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;
Question generation (QG) is the task of generating a valid and fluent question based on a given context and the target answer. According to various purposes, even given the same context, instructors can ask questions about different concepts, and even the same concept can be written in different ways. However, the evaluation for QG usually depends on single reference-based similarity metrics, such as n-gram-based metric or learned metric, which is not sufficient to fully evaluate the potential of QG methods. To this end, we propose to paraphrase the reference question for a more robust QG evaluation. Using large language models such as GPT-3, we created semantically and syntactically diverse questions, then adopt the simple aggregation of the popular evaluation metrics as the final scores. Through our experiments, we found that using multiple (pseudo) references is more effective for QG evaluation while showing a higher correlation with human evaluations than evaluation with a single r
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#38598;&#21512;&#21270;&#22320;&#32534;&#30721;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#30340;&#31070;&#32463;&#32593;&#32476;&#26435;&#37325;&#32534;&#30721;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#36880;&#23618;&#32534;&#30721;&#26041;&#26696;&#26469;&#32771;&#34385;&#31070;&#32463;&#32593;&#32476;&#30340;&#20998;&#23618;&#35745;&#31639;&#32467;&#26500;&#12290;&#21516;&#26102;&#24341;&#20837;&#20102;&#8220;pad-chunk-encode&#8221;&#27969;&#27700;&#32447;&#36827;&#34892;&#31070;&#32463;&#32593;&#32476;&#23618;&#30340;&#39640;&#25928;&#32534;&#30721;&#22788;&#29702;&#65292;&#36824;&#25552;&#20986;&#20102;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#27867;&#21270;&#24615;&#33021;&#39044;&#27979;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2305.16625</link><description>&lt;p&gt;
&#38598;&#21512;&#21270;&#30340;&#31070;&#32463;&#32593;&#32476;&#32534;&#30721;
&lt;/p&gt;
&lt;p&gt;
Set-based Neural Network Encoding. (arXiv:2305.16625v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16625
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#38598;&#21512;&#21270;&#22320;&#32534;&#30721;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#30340;&#31070;&#32463;&#32593;&#32476;&#26435;&#37325;&#32534;&#30721;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#36880;&#23618;&#32534;&#30721;&#26041;&#26696;&#26469;&#32771;&#34385;&#31070;&#32463;&#32593;&#32476;&#30340;&#20998;&#23618;&#35745;&#31639;&#32467;&#26500;&#12290;&#21516;&#26102;&#24341;&#20837;&#20102;&#8220;pad-chunk-encode&#8221;&#27969;&#27700;&#32447;&#36827;&#34892;&#31070;&#32463;&#32593;&#32476;&#23618;&#30340;&#39640;&#25928;&#32534;&#30721;&#22788;&#29702;&#65292;&#36824;&#25552;&#20986;&#20102;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#27867;&#21270;&#24615;&#33021;&#39044;&#27979;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#38598;&#21512;&#21040;&#38598;&#21512;&#21644;&#38598;&#21512;&#21040;&#21521;&#37327;&#20989;&#25968;&#26469;&#26377;&#25928;&#32534;&#30721;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#65292;&#36827;&#34892;&#27867;&#21270;&#24615;&#33021;&#39044;&#27979;&#30340;&#31070;&#32463;&#32593;&#32476;&#26435;&#37325;&#32534;&#30721;&#26041;&#27861;&#12290;&#19982;&#20043;&#21069;&#38656;&#35201;&#23545;&#19981;&#21516;&#26550;&#26500;&#32534;&#20889;&#33258;&#23450;&#20041;&#32534;&#30721;&#27169;&#22411;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#23545;&#28151;&#21512;&#26550;&#26500;&#21644;&#19981;&#21516;&#21442;&#25968;&#22823;&#23567;&#30340;&#27169;&#22411;&#21160;&#24577;&#32534;&#30721;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340; SNE&#65288;&#38598;&#21512;&#21270;&#31070;&#32463;&#32593;&#32476;&#32534;&#30721;&#22120;&#65289;&#36890;&#36807;&#20351;&#29992;&#19968;&#31181;&#36880;&#23618;&#32534;&#30721;&#26041;&#26696;&#65292;&#32771;&#34385;&#31070;&#32463;&#32593;&#32476;&#30340;&#20998;&#23618;&#35745;&#31639;&#32467;&#26500;&#12290;&#26368;&#32456;&#23558;&#25152;&#26377;&#23618;&#27425;&#32534;&#30721;&#21512;&#24182;&#21040;&#19968;&#36215;&#65292;&#20197;&#33719;&#21462;&#31070;&#32463;&#32593;&#32476;&#32534;&#30721;&#30690;&#37327;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#8220;pad-chunk-encode&#8221;&#27969;&#27700;&#32447;&#26469;&#26377;&#25928;&#22320;&#32534;&#30721;&#31070;&#32463;&#32593;&#32476;&#23618;&#65292;&#35813;&#27969;&#27700;&#32447;&#21487;&#26681;&#25454;&#35745;&#31639;&#21644;&#20869;&#23384;&#38480;&#21046;&#36827;&#34892;&#35843;&#25972;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#20004;&#20010;&#29992;&#20110;&#31070;&#32463;&#32593;&#32476;&#27867;&#21270;&#24615;&#33021;&#39044;&#27979;&#30340;&#26032;&#20219;&#21153;&#65306;&#36328;&#25968;&#25454;&#38598;&#21644;&#26550;&#26500;&#36866;&#24212;&#24615;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose an approach to neural network weight encoding for generalization performance prediction that utilizes set-to-set and set-to-vector functions to efficiently encode neural network parameters. Our approach is capable of encoding neural networks in a modelzoo of mixed architecture and different parameter sizes as opposed to previous approaches that require custom encoding models for different architectures. Furthermore, our \textbf{S}et-based \textbf{N}eural network \textbf{E}ncoder (SNE) takes into consideration the hierarchical computational structure of neural networks by utilizing a layer-wise encoding scheme that culminates to encoding all layer-wise encodings to obtain the neural network encoding vector. Additionally, we introduce a \textit{pad-chunk-encode} pipeline to efficiently encode neural network layers that is adjustable to computational and memory constraints. We also introduce two new tasks for neural network generalization performance prediction: cross-dataset a
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#34920;&#26126;&#65292;&#23545;&#20110;&#22797;&#26434;&#25351;&#20196;&#36981;&#24490;&#20195;&#29702;&#30340;&#23398;&#20064;&#65292;&#35821;&#35328;&#22870;&#21169;&#22609;&#36896;&#25216;&#26415;&#21487;&#33021;&#20250;&#24433;&#21709;&#20195;&#29702;&#31243;&#24207;&#30340;&#23398;&#20064;&#65292;&#20854;&#20013;&#34920;&#38754;&#19978;&#30340;&#25104;&#21151;&#21487;&#33021;&#26159;&#33030;&#24369;&#30340;&#12290;</title><link>http://arxiv.org/abs/2305.16621</link><description>&lt;p&gt;
&#35821;&#35328;&#22870;&#21169;&#22609;&#36896;&#21487;&#33021;&#24433;&#21709;&#25351;&#20196;&#36981;&#24490;&#20195;&#29702;&#30340;&#23398;&#20064;&#65306;&#25552;&#37266;&#20854;&#33030;&#24369;&#24615;
&lt;/p&gt;
&lt;p&gt;
A Reminder of its Brittleness: Language Reward Shaping May Hinder Learning for Instruction Following Agents. (arXiv:2305.16621v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16621
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#34920;&#26126;&#65292;&#23545;&#20110;&#22797;&#26434;&#25351;&#20196;&#36981;&#24490;&#20195;&#29702;&#30340;&#23398;&#20064;&#65292;&#35821;&#35328;&#22870;&#21169;&#22609;&#36896;&#25216;&#26415;&#21487;&#33021;&#20250;&#24433;&#21709;&#20195;&#29702;&#31243;&#24207;&#30340;&#23398;&#20064;&#65292;&#20854;&#20013;&#34920;&#38754;&#19978;&#30340;&#25104;&#21151;&#21487;&#33021;&#26159;&#33030;&#24369;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25945;&#23548;&#20195;&#29702;&#31243;&#24207;&#36981;&#23432;&#22797;&#26434;&#30340;&#20070;&#38754;&#25351;&#20196;&#19968;&#30452;&#26159;&#19968;&#20010;&#37325;&#35201;&#32780;&#21448;&#38590;&#20197;&#23454;&#29616;&#30340;&#30446;&#26631;&#12290;&#25552;&#39640;&#23398;&#20064;&#25928;&#29575;&#30340;&#19968;&#31181;&#25216;&#26415;&#26159;&#35821;&#35328;&#22870;&#21169;&#22609;&#36896;&#65288;LRS&#65289;&#65292;&#23427;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#29992;&#20110;&#22870;&#21169;&#20195;&#34920;&#26397;&#30528;&#31232;&#30095;&#22870;&#21169;&#30340;&#36827;&#23637;&#26041;&#21521;&#30340;&#34892;&#21160;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;LRS&#30340;&#34920;&#38754;&#25104;&#21151;&#26159;&#33030;&#24369;&#30340;&#65292;&#24182;&#19988;&#20043;&#21069;&#30340;&#31215;&#26497;&#32467;&#26524;&#21487;&#33021;&#24402;&#22240;&#20110;&#24369;&#30340;&#24378;&#21270;&#23398;&#20064;&#22522;&#32447;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#22870;&#21169;&#37096;&#20998;&#21305;&#37197;&#36712;&#36857;&#30340;&#27425;&#20248;LRS&#35774;&#35745;&#65292;&#24182;&#22522;&#20110;&#25918;&#23485;&#20219;&#21153;&#32422;&#26463;&#30340;&#27010;&#24565;&#65292;&#23545;&#19968;&#31181;&#26032;&#22411;&#30340;&#22870;&#21169;&#25200;&#21160;&#36827;&#34892;&#20102;&#34920;&#24449;&#20197;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#29702;&#35770;&#21644;&#23454;&#35777;&#35777;&#25454;&#34920;&#26126;&#65292;&#20351;&#29992;LRS&#22870;&#21169;&#35757;&#32451;&#30340;&#20195;&#29702;&#31243;&#24207;&#36739;&#32431;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#31243;&#24207;&#25910;&#25947;&#36895;&#24230;&#36739;&#24930;&#12290;
&lt;/p&gt;
&lt;p&gt;
Teaching agents to follow complex written instructions has been an important yet elusive goal. One technique for improving learning efficiency is language reward shaping (LRS), which is used in reinforcement learning (RL) to reward actions that represent progress towards a sparse reward. We argue that the apparent success of LRS is brittle, and prior positive findings can be attributed to weak RL baselines. Specifically, we identified suboptimal LRS designs that reward partially matched trajectories, and we characterised a novel type of reward perturbation that addresses this issue based on the concept of loosening task constraints. We provided theoretical and empirical evidence that agents trained using LRS rewards converge more slowly compared to pure RL agents.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#38598;&#25104;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#32771;&#34385;&#24863;&#30693;&#19981;&#30830;&#23450;&#24615;&#30340;&#24773;&#20917;&#19979;&#65292;&#39044;&#27979;&#34892;&#20154;&#36816;&#21160;&#36712;&#36857;&#21450;&#20854;&#19981;&#30830;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.16620</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#38598;&#25104;&#26041;&#27861;&#39044;&#27979;&#24863;&#30693;&#19981;&#30830;&#23450;&#24615;&#19979;&#30340;&#34892;&#20154;&#36816;&#21160;&#36712;&#36857;
&lt;/p&gt;
&lt;p&gt;
Pedestrian Trajectory Forecasting Using Deep Ensembles Under Sensing Uncertainty. (arXiv:2305.16620v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16620
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#38598;&#25104;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#32771;&#34385;&#24863;&#30693;&#19981;&#30830;&#23450;&#24615;&#30340;&#24773;&#20917;&#19979;&#65292;&#39044;&#27979;&#34892;&#20154;&#36816;&#21160;&#36712;&#36857;&#21450;&#20854;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#32771;&#34385;&#24863;&#30693;&#19981;&#30830;&#23450;&#24615;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;&#28145;&#24230;&#38598;&#25104;&#30340;&#26041;&#27861;&#39044;&#27979;&#34892;&#20154;&#30340;&#36816;&#21160;&#36712;&#36857;&#65292;&#25512;&#27979;&#29366;&#24577;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;Bayes&#28388;&#27874;&#22120;&#20173;&#23384;&#22312;&#30340;&#38382;&#39064;&#26159;&#26080;&#27861;&#35299;&#20915;&#38750;&#32447;&#24615;&#21644;&#38271;&#26399;&#39044;&#27979;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the fundamental challenges in the prediction of dynamic agents is robustness. Usually, most predictions are deterministic estimates of future states which are over-confident and prone to error. Recently, few works have addressed capturing uncertainty during forecasting of future states. However, these probabilistic estimation methods fail to account for the upstream noise in perception data during tracking. Sensors always have noise and state estimation becomes even more difficult under adverse weather conditions and occlusion. Traditionally, Bayes filters have been used to fuse information from noisy sensors to update states with associated belief. But, they fail to address non-linearities and long-term predictions. Therefore, we propose an end-to-end estimator that can take noisy sensor measurements and make robust future state predictions with uncertainty bounds while simultaneously taking into consideration the upstream perceptual uncertainty. For the current research, we co
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Phy-DRL&#65292;&#36825;&#26159;&#19968;&#20010;&#29289;&#29702;&#27169;&#22411;&#35843;&#25972;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#26377;&#19977;&#20010;&#21019;&#26032;&#28857;&#65292;&#23427;&#20204;&#20998;&#21035;&#26159;: i)&#21069;&#30651;&#24615;&#30340;&#26410;&#30693;&#26410;&#30693;&#35757;&#32451;&#65292;ii)&#32467;&#21512;&#27531;&#24046;&#25511;&#21046;&#65292;&#20197;&#21450;iii)&#22522;&#20110;&#29289;&#29702;&#27169;&#22411;&#30340;&#31070;&#32463;&#32593;&#32476;&#32534;&#36753;&#12290;Phy-DRL&#33021;&#22815;&#23481;&#24525;&#26410;&#30693;&#24178;&#25200;&#65292;&#20445;&#35777;&#23433;&#20840;&#21644;&#31283;&#23450;&#65292;&#21516;&#26102;&#36981;&#23432;Bellman&#26041;&#31243;&#21644;&#22870;&#21169;&#30456;&#20851;&#30340;&#29289;&#29702;&#30693;&#35782;&#12290;</title><link>http://arxiv.org/abs/2305.16614</link><description>&lt;p&gt;
&#29289;&#29702;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;: &#23433;&#20840;&#21644;&#26410;&#30693;&#26410;&#30693;&#30340;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Physical Deep Reinforcement Learning: Safety and Unknown Unknowns. (arXiv:2305.16614v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16614
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Phy-DRL&#65292;&#36825;&#26159;&#19968;&#20010;&#29289;&#29702;&#27169;&#22411;&#35843;&#25972;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#26377;&#19977;&#20010;&#21019;&#26032;&#28857;&#65292;&#23427;&#20204;&#20998;&#21035;&#26159;: i)&#21069;&#30651;&#24615;&#30340;&#26410;&#30693;&#26410;&#30693;&#35757;&#32451;&#65292;ii)&#32467;&#21512;&#27531;&#24046;&#25511;&#21046;&#65292;&#20197;&#21450;iii)&#22522;&#20110;&#29289;&#29702;&#27169;&#22411;&#30340;&#31070;&#32463;&#32593;&#32476;&#32534;&#36753;&#12290;Phy-DRL&#33021;&#22815;&#23481;&#24525;&#26410;&#30693;&#24178;&#25200;&#65292;&#20445;&#35777;&#23433;&#20840;&#21644;&#31283;&#23450;&#65292;&#21516;&#26102;&#36981;&#23432;Bellman&#26041;&#31243;&#21644;&#22870;&#21169;&#30456;&#20851;&#30340;&#29289;&#29702;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Phy-DRL&#65292;&#19968;&#20010;&#29289;&#29702;&#27169;&#22411;&#35843;&#33410;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#23433;&#20840;&#20851;&#38190;&#30340;&#33258;&#20027;&#31995;&#32479;&#12290;Phy-DRL&#20855;&#26377;&#19977;&#31181;&#29420;&#29305;&#30340;&#21019;&#26032;&#65306;i&#65289;&#21069;&#30651;&#24615;&#30340;&#26410;&#30693;&#26410;&#30693;&#35757;&#32451;&#65292;ii&#65289;&#32467;&#21512;&#27531;&#24046;&#25511;&#21046;&#65288;&#21363;&#25968;&#25454;&#39537;&#21160;&#25511;&#21046;&#21644;&#22522;&#20110;&#29289;&#29702;&#27169;&#22411;&#30340;&#25511;&#21046;&#30340;&#38598;&#25104;&#65289;&#21644;&#23433;&#20840;&#21450;&#31283;&#23450;&#24615;&#25935;&#24863;&#30340;&#22870;&#21169;&#65292;&#20197;&#21450;iii&#65289;&#22522;&#20110;&#29289;&#29702;&#27169;&#22411;&#30340;&#31070;&#32463;&#32593;&#32476;&#32534;&#36753;&#65292;&#21253;&#25324;&#38142;&#25509;&#32534;&#36753;&#21644;&#28608;&#27963;&#32534;&#36753;&#12290;&#30001;&#20110;&#36825;&#20123;&#24182;&#21457;&#35774;&#35745;&#65292;Phy-DRL&#33021;&#22815;1&#65289;&#23481;&#24525;&#26410;&#30693;&#24178;&#25200;&#65292;2&#65289;&#20445;&#35777;&#21487;&#25968;&#23398;&#35777;&#26126;&#30340;&#23433;&#20840;&#19982;&#31283;&#23450;&#24615;&#65292;&#24182;3&#65289;&#20005;&#26684;&#36981;&#23432;Bellman&#26041;&#31243;&#21644;&#22870;&#21169;&#30456;&#20851;&#30340;&#29289;&#29702;&#30693;&#35782;&#12290;&#26368;&#32456;&#65292;&#36890;&#36807;&#20498;&#31435;&#25670;&#21644;&#22235;&#36275;&#26426;&#22120;&#20154;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;Phy-DRL&#30340;&#26377;&#25928;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#32431;&#25968;&#25454;&#39537;&#21160;&#30340;DRL&#30456;&#27604;&#65292;Phy-DRL&#20855;&#26377;&#26126;&#26174;&#26356;&#23569;&#30340;&#23398;&#20064;&#21442;&#25968;&#12289;&#21152;&#36895;&#30340;&#35757;&#32451;&#21644;&#25193;&#22823;&#30340;&#22238;&#25253;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose the Phy-DRL: a physics-model-regulated deep reinforcement learning framework for safety-critical autonomous systems. The Phy-DRL is unique in three innovations: i) proactive unknown-unknowns training, ii) conjunctive residual control (i.e., integration of data-driven control and physics-model-based control) and safety- \&amp; stability-sensitive reward, and iii) physics-model-based neural network editing, including link editing and activation editing. Thanks to the concurrent designs, the Phy-DRL is able to 1) tolerate unknown-unknowns disturbances, 2) guarantee mathematically provable safety and stability, and 3) strictly comply with physical knowledge pertaining to Bellman equation and reward. The effectiveness of the Phy-DRL is finally validated by an inverted pendulum and a quadruped robot. The experimental results demonstrate that compared with purely data-driven DRL, Phy-DRL features remarkably fewer learning parameters, accelerated training and enlarged rew
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#26500;&#21270;&#21644;&#38750;&#32467;&#26500;&#21270;&#21098;&#26525;&#23398;&#20064;PET&#32467;&#26500;&#24182;&#22312;GLUE&#19978;&#36827;&#34892;&#23454;&#39564;&#39564;&#35777;&#65292;&#23637;&#31034;&#20102;&#35813;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#25506;&#35752;&#20102;PET&#26550;&#26500;&#35774;&#35745;&#36873;&#25321;&#23545;&#23454;&#38469;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2305.16597</link><description>&lt;p&gt;
&#22522;&#20110;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Neural Architecture Search for Parameter-Efficient Fine-tuning of Large Pre-trained Language Models. (arXiv:2305.16597v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16597
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#26500;&#21270;&#21644;&#38750;&#32467;&#26500;&#21270;&#21098;&#26525;&#23398;&#20064;PET&#32467;&#26500;&#24182;&#22312;GLUE&#19978;&#36827;&#34892;&#23454;&#39564;&#39564;&#35777;&#65292;&#23637;&#31034;&#20102;&#35813;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#25506;&#35752;&#20102;PET&#26550;&#26500;&#35774;&#35745;&#36873;&#25321;&#23545;&#23454;&#38469;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PET&#65289;&#26041;&#27861;&#36890;&#36807;&#35745;&#31639;&#37096;&#20998;&#27169;&#22411;&#21442;&#25968;&#30340;&#23567;&#22411;&#21387;&#32553;&#26356;&#26032;&#25110;&#28155;&#21152;&#21644;&#24494;&#35843;&#23569;&#37327;&#26032;&#30340;&#27169;&#22411;&#21442;&#25968;&#21040;&#39044;&#35757;&#32451;&#32593;&#32476;&#65292;&#23558;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#36866;&#24212;&#19979;&#28216;&#20219;&#21153;&#12290;&#25163;&#24037;&#35774;&#35745;&#30340;PET&#26550;&#26500;&#22312;&#23454;&#36341;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#36890;&#36807;&#33258;&#21160;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65288;NAS&#65289;&#65292;&#23427;&#20204;&#26377;&#25913;&#36827;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#32467;&#26500;&#21270;&#21644;&#38750;&#32467;&#26500;&#21270;&#21098;&#26525;&#23398;&#20064;PET&#32467;&#26500;&#30340;&#26377;&#25928;NAS&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;GLUE&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#23637;&#31034;&#20102;&#25105;&#20204;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#35752;&#35770;&#20102;PET&#26550;&#26500;&#35774;&#35745;&#36873;&#25321;&#22914;&#20309;&#24433;&#21709;&#23454;&#38469;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Parameter-efficient tuning (PET) methods fit pre-trained language models (PLMs) to downstream tasks by either computing a small compressed update for a subset of model parameters, or appending and fine-tuning a small number of new model parameters to the pre-trained network. Hand-designed PET architectures from the literature perform well in practice, but have the potential to be improved via automated neural architecture search (NAS). We propose an efficient NAS method for learning PET architectures via structured and unstructured pruning. We present experiments on GLUE demonstrating the effectiveness of our algorithm and discuss how PET architectural design choices affect performance in practice.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;93&#21517;NLP&#21021;&#23398;&#32773;&#30340;&#35843;&#26597;&#65292;&#21457;&#29616;&#30740;&#31350;&#20316;&#32773;&#25552;&#20379;&#23436;&#25972;&#25991;&#26723;&#12289;&#26356;&#22909;&#30340;&#20195;&#30721;&#23454;&#36341;&#21644;&#26356;&#26131;&#20110;&#33719;&#21462;&#30340;&#25968;&#25454;&#25991;&#20214;&#26159;&#21021;&#23398;&#32773;&#25104;&#21151;&#22797;&#29616;&#26368;&#36817;NLP&#35770;&#25991;&#32467;&#26524;&#30340;&#20851;&#38190;&#65292;&#24314;&#35758;NLP&#30740;&#31350;&#20154;&#21592;&#27880;&#37325;&#36825;&#20123;&#26041;&#38754;&#65292;&#26356;&#22909;&#22320;&#25903;&#25345;&#21021;&#23398;&#32773;&#12290;</title><link>http://arxiv.org/abs/2305.16579</link><description>&lt;p&gt;
&#20154;&#20154;&#21487;&#22797;&#29616;&#30340;NLP&#30740;&#31350;&#65306;&#21021;&#23398;&#32773;&#30340;&#38656;&#27714;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
NLP Reproducibility For All: Understanding Experiences of Beginners. (arXiv:2305.16579v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16579
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;93&#21517;NLP&#21021;&#23398;&#32773;&#30340;&#35843;&#26597;&#65292;&#21457;&#29616;&#30740;&#31350;&#20316;&#32773;&#25552;&#20379;&#23436;&#25972;&#25991;&#26723;&#12289;&#26356;&#22909;&#30340;&#20195;&#30721;&#23454;&#36341;&#21644;&#26356;&#26131;&#20110;&#33719;&#21462;&#30340;&#25968;&#25454;&#25991;&#20214;&#26159;&#21021;&#23398;&#32773;&#25104;&#21151;&#22797;&#29616;&#26368;&#36817;NLP&#35770;&#25991;&#32467;&#26524;&#30340;&#20851;&#38190;&#65292;&#24314;&#35758;NLP&#30740;&#31350;&#20154;&#21592;&#27880;&#37325;&#36825;&#20123;&#26041;&#38754;&#65292;&#26356;&#22909;&#22320;&#25903;&#25345;&#21021;&#23398;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#36817;&#24180;&#26469;&#24322;&#24120;&#28779;&#29190;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#20154;&#24613;&#20110;&#36827;&#20837;&#35813;&#39046;&#22495;&#65292;&#20294;&#30446;&#21069;&#30340;&#30740;&#31350;&#22797;&#29616;&#21162;&#21147;&#26159;&#21542;&#36275;&#20197;&#35753;&#36825;&#20123;&#21021;&#23398;&#32773;&#24212;&#29992;&#26368;&#26032;&#30340;&#36827;&#23637;&#36824;&#19981;&#28165;&#26970;&#12290;&#20026;&#20102;&#20102;&#35299;&#21021;&#23398;&#32773;&#30340;&#38656;&#27714;&#65292;&#25105;&#20204;&#22312;&#19968;&#20010;&#20171;&#32461;&#24615;&#30340;NLP&#35838;&#31243;&#20013;&#24320;&#23637;&#20102;&#19968;&#39033;&#30740;&#31350;&#65292;&#35753;&#23398;&#29983;&#22797;&#29616;&#26368;&#36817;NLP&#35770;&#25991;&#30340;&#32467;&#26524;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#20182;&#20204;&#30340;&#32534;&#31243;&#25216;&#33021;&#21644;&#23545;&#30740;&#31350;&#35770;&#25991;&#30340;&#29702;&#35299;&#23545;&#23436;&#25104;&#32451;&#20064;&#30340;&#20184;&#20986;&#20165;&#26377;&#38480;&#30340;&#24433;&#21709;&#65292;&#30456;&#27604;&#20043;&#19979;&#65292;&#30740;&#31350;&#20316;&#32773;&#30340;&#21487;&#35775;&#38382;&#24615;&#21162;&#21147;&#26159;&#25104;&#21151;&#30340;&#20851;&#38190;&#65292;&#21253;&#25324;&#23436;&#25972;&#30340;&#25991;&#26723;&#12289;&#26356;&#22909;&#30340;&#32534;&#30721;&#23454;&#36341;&#21644;&#26356;&#23481;&#26131;&#33719;&#21462;&#30340;&#25968;&#25454;&#25991;&#20214;&#12290;&#21069;&#36827;&#26102;&#65292;&#25105;&#20204;&#24314;&#35758;NLP&#30740;&#31350;&#20154;&#21592;&#23494;&#20999;&#20851;&#27880;&#36825;&#20123;&#24320;&#28304;&#24037;&#20316;&#30340;&#31616;&#21333;&#26041;&#38754;&#65292;&#24182;&#20351;&#29992;&#21021;&#23398;&#32773;&#30340;&#21453;&#39304;&#35265;&#35299;&#25552;&#20379;&#21487;&#25805;&#20316;&#30340;&#24819;&#27861;&#20197;&#26356;&#22909;&#22320;&#25903;&#25345;&#20182;&#20204;&#12290;
&lt;/p&gt;
&lt;p&gt;
As natural language processing (NLP) has recently seen an unprecedented level of excitement, and more people are eager to enter the field, it is unclear whether current research reproducibility efforts are sufficient for this group of beginners to apply the latest developments. To understand their needs, we conducted a study with 93 students in an introductory NLP course, where students reproduced the results of recent NLP papers. Surprisingly, we find that their programming skill and comprehension of research papers have a limited impact on their effort spent completing the exercise. Instead, we find accessibility efforts by research authors to be the key to success, including complete documentation, better coding practice, and easier access to data files. Going forward, we recommend that NLP researchers pay close attention to these simple aspects of open-sourcing their work, and use insights from beginners' feedback to provide actionable ideas on how to better support them.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25968;&#23383;&#23402;&#29983;&#30340;&#26041;&#26696;&#65292;&#29992;&#20110;&#36741;&#21161;&#31227;&#21160;&#22686;&#24378;&#29616;&#23454;&#30340;&#19977;&#32500;&#22320;&#22270;&#31649;&#29702;&#65292;&#21487;&#26174;&#33879;&#38477;&#20302;&#23039;&#24577;&#20272;&#35745;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#21487;&#20197;&#22312;&#39640;&#24230;&#21160;&#24577;&#30340;&#29615;&#22659;&#20013;&#24555;&#36895;&#25552;&#20379;&#33258;&#36866;&#24212;&#22320;&#22270;&#31649;&#29702;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2305.16571</link><description>&lt;p&gt;
&#22522;&#20110;&#25968;&#23383;&#23402;&#29983;&#30340;&#36793;&#32536;&#21327;&#21161;&#31227;&#21160;&#22686;&#24378;&#29616;&#23454;&#19977;&#32500;&#22320;&#22270;&#31649;&#29702;
&lt;/p&gt;
&lt;p&gt;
Digital Twin-Based 3D Map Management for Edge-Assisted Mobile Augmented Reality. (arXiv:2305.16571v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16571
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25968;&#23383;&#23402;&#29983;&#30340;&#26041;&#26696;&#65292;&#29992;&#20110;&#36741;&#21161;&#31227;&#21160;&#22686;&#24378;&#29616;&#23454;&#30340;&#19977;&#32500;&#22320;&#22270;&#31649;&#29702;&#65292;&#21487;&#26174;&#33879;&#38477;&#20302;&#23039;&#24577;&#20272;&#35745;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#21487;&#20197;&#22312;&#39640;&#24230;&#21160;&#24577;&#30340;&#29615;&#22659;&#20013;&#24555;&#36895;&#25552;&#20379;&#33258;&#36866;&#24212;&#22320;&#22270;&#31649;&#29702;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20026;&#36741;&#21161;&#31227;&#21160;&#22686;&#24378;&#29616;&#23454;&#65288;MAR&#65289;&#35774;&#35745;&#20102;&#19968;&#20010;&#19977;&#32500;&#22320;&#22270;&#31649;&#29702;&#26041;&#26696;&#65292;&#20197;&#25903;&#25345;&#20010;&#20307;MAR&#35774;&#22791;&#30340;&#23039;&#24577;&#20272;&#35745;&#12290;&#36890;&#36807;&#23450;&#26399;&#36873;&#25321;&#36866;&#24403;&#30340;&#19968;&#32452;&#30456;&#26426;&#24103;&#19978;&#20256;&#20197;&#26356;&#26032;&#19977;&#32500;&#22320;&#22270;&#65292;&#30446;&#26631;&#26159;&#23558;MAR&#35774;&#22791;&#30340;&#23039;&#24577;&#20272;&#35745;&#19981;&#30830;&#23450;&#24615;&#26368;&#23567;&#21270;&#12290;&#20026;&#20102;&#35299;&#20915;&#21160;&#24577;&#19978;&#34892;&#25968;&#25454;&#36895;&#29575;&#21644;MAR&#35774;&#22791;&#30340;&#26102;&#21464;&#23039;&#24577;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25968;&#23383;&#23402;&#29983;&#30340;&#26041;&#27861;&#26469;&#36827;&#34892;&#19977;&#32500;&#22320;&#22270;&#31649;&#29702;&#12290;&#39318;&#20808;&#65292;&#20026;MAR&#35774;&#22791;&#21019;&#24314;&#19968;&#20010;&#25968;&#23383;&#23402;&#29983;&#65292;&#23427;&#22522;&#20110;&#39044;&#27979;&#21518;&#32493;&#30456;&#26426;&#24103;&#26469;&#27169;&#25311;&#19977;&#32500;&#22320;&#22270;&#31649;&#29702;&#12290;&#20854;&#27425;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65288;MBRL&#65289;&#65292;&#21033;&#29992;&#20174;&#23454;&#38469;&#21644;&#27169;&#25311;&#25968;&#25454;&#25910;&#38598;&#30340;&#25968;&#25454;&#26469;&#31649;&#29702;&#19977;&#32500;&#22320;&#22270;&#12290;&#36890;&#36807;&#25968;&#23383;&#23402;&#29983;&#25552;&#20379;&#30340;&#22823;&#37327;&#27169;&#25311;&#25968;&#25454;&#65292;MBRL&#31639;&#27861;&#21487;&#20197;&#22312;&#39640;&#24230;&#21160;&#24577;&#30340;&#29615;&#22659;&#20013;&#24555;&#36895;&#25552;&#20379;&#33258;&#36866;&#24212;&#22320;&#22270;&#31649;&#29702;&#31574;&#30053;&#12290;&#27169;&#25311;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#22522;&#32447;&#26041;&#26696;&#30456;&#27604;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#22522;&#20110;&#25968;&#23383;&#23402;&#29983;&#30340;&#26041;&#27861;&#26174;&#33879;&#38477;&#20302;&#20102;&#23039;&#24577;&#20272;&#35745;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we design a 3D map management scheme for edge-assisted mobile augmented reality (MAR) to support the pose estimation of individual MAR device, which uploads camera frames to an edge server. Our objective is to minimize the pose estimation uncertainty of the MAR device by periodically selecting a proper set of camera frames for uploading to update the 3D map. To address the challenges of the dynamic uplink data rate and the time-varying pose of the MAR device, we propose a digital twin (DT)-based approach to 3D map management. First, a DT is created for the MAR device, which emulates 3D map management based on predicting subsequent camera frames. Second, a model-based reinforcement learning (MBRL) algorithm is developed, utilizing the data collected from both the actual and the emulated data to manage the 3D map. With extensive emulated data provided by the DT, the MBRL algorithm can quickly provide an adaptive map management policy in a highly dynamic environment. Simula
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#31867;&#22686;&#37327;&#20449;&#24687;&#25552;&#21462;&#20013;&#20998;&#31867;&#22120;&#28418;&#31227;&#22914;&#20309;&#23548;&#33268;&#36951;&#24536;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#22235;&#31181;&#35299;&#20915;&#26041;&#26696;&#26469;&#32531;&#35299;&#20998;&#31867;&#22120;&#28418;&#31227;&#12290;</title><link>http://arxiv.org/abs/2305.16559</link><description>&lt;p&gt;
&#22242;&#38431;&#21512;&#20316;&#24182;&#19981;&#24635;&#26159;&#22909;&#30340;&#65306;&#31867;&#22686;&#37327;&#20449;&#24687;&#25552;&#21462;&#20013;&#20998;&#31867;&#22120;&#28418;&#31227;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Teamwork Is Not Always Good: An Empirical Study of Classifier Drift in Class-incremental Information Extraction. (arXiv:2305.16559v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16559
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#31867;&#22686;&#37327;&#20449;&#24687;&#25552;&#21462;&#20013;&#20998;&#31867;&#22120;&#28418;&#31227;&#22914;&#20309;&#23548;&#33268;&#36951;&#24536;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#22235;&#31181;&#35299;&#20915;&#26041;&#26696;&#26469;&#32531;&#35299;&#20998;&#31867;&#22120;&#28418;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31867;&#22686;&#37327;&#23398;&#20064;&#26088;&#22312;&#24320;&#21457;&#19968;&#31181;&#23398;&#20064;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#21487;&#20197;&#19981;&#26029;&#20174;&#25968;&#25454;&#27969;&#20013;&#23398;&#20064;&#26032;&#31867;&#65292;&#32780;&#19981;&#20250;&#24536;&#35760;&#20043;&#21069;&#23398;&#20064;&#36807;&#30340;&#31867;&#12290;&#28982;&#32780;&#65292;&#24403;&#23398;&#20064;&#22686;&#37327;&#31867;&#26102;&#65292;&#20998;&#31867;&#22120;&#24517;&#39035;&#19981;&#26029;&#26356;&#26032;&#20197;&#32435;&#20837;&#26032;&#31867;&#65292;&#24182;&#19988;&#20915;&#31574;&#36793;&#30028;&#30340;&#28418;&#31227;&#21487;&#33021;&#23548;&#33268;&#20005;&#37325;&#30340;&#36951;&#24536;&#12290;&#28982;&#32780;&#65292;&#36825;&#19968;&#26681;&#26412;&#24615;&#25361;&#25112;&#23578;&#26410;&#24471;&#21040;&#24191;&#27867;&#30740;&#31350;&#65292;&#29305;&#21035;&#26159;&#22312;&#19981;&#23384;&#20648;&#26087;&#31867;&#21035;&#26679;&#26412;&#20197;&#36827;&#34892;&#37325;&#28436;&#30340;&#24773;&#20917;&#19979;&#12290;&#26412;&#25991;&#26356;&#35814;&#32454;&#22320;&#30740;&#31350;&#20102;&#20998;&#31867;&#22120;&#28418;&#31227;&#22914;&#20309;&#23548;&#33268;&#36951;&#24536;&#65292;&#24182;&#25454;&#27492;&#35774;&#35745;&#20102;&#22235;&#31181;&#31616;&#21333;&#20294;&#65288;&#36229;&#32423;&#65289;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#26469;&#32531;&#35299;&#20998;&#31867;&#22120;&#28418;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;
Class-incremental learning (CIL) aims to develop a learning system that can continually learn new classes from a data stream without forgetting previously learned classes. When learning classes incrementally, the classifier must be constantly updated to incorporate new classes, and the drift in decision boundary may lead to severe forgetting. This fundamental challenge, however, has not yet been studied extensively, especially in the setting where no samples from old classes are stored for rehearsal. In this paper, we take a closer look at how the drift in the classifier leads to forgetting, and accordingly, design four simple yet (super-) effective solutions to alleviate the classifier drift: an Individual Classifiers with Frozen Feature Extractor (ICE) framework where we individually train a classifier for each learning session, and its three variants ICE-PL, ICE-O, and ICE-PL&amp;O which further take the logits of previously learned classes from old sessions or a constant logit of an Ot
&lt;/p&gt;</description></item><item><title>LANISTR&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#26694;&#26550;&#65292;&#21487;&#20174;&#32467;&#26500;&#21270;&#21644;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#20013;&#36827;&#34892;&#23398;&#20064;&#65292;&#22312;&#25361;&#25112;&#24615;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>http://arxiv.org/abs/2305.16556</link><description>&lt;p&gt;
LANISTR&#65306;&#20174;&#32467;&#26500;&#21270;&#21644;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#20013;&#36827;&#34892;&#22810;&#27169;&#24577;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
LANISTR: Multimodal Learning from Structured and Unstructured Data. (arXiv:2305.16556v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16556
&lt;/p&gt;
&lt;p&gt;
LANISTR&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#26694;&#26550;&#65292;&#21487;&#20174;&#32467;&#26500;&#21270;&#21644;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#20013;&#36827;&#34892;&#23398;&#20064;&#65292;&#22312;&#25361;&#25112;&#24615;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#30340;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#24050;&#32463;&#22312;&#22788;&#29702;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#65288;&#21253;&#25324;&#25991;&#26412;&#12289;&#22270;&#20687;&#12289;&#38899;&#39057;&#21644;&#35270;&#39057;&#65289;&#26041;&#38754;&#23637;&#29616;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#20294;&#26159;&#65292;&#29616;&#23454;&#19990;&#30028;&#20013;&#26368;&#24120;&#35265;&#30340;&#24773;&#20917;&#26159;&#32467;&#26500;&#21270;&#65288;&#21253;&#25324;&#34920;&#26684;&#21644;&#26102;&#38388;&#24207;&#21015;&#65289;&#21644;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#30340;&#32467;&#21512;&#65292;&#20294;&#36825;&#19968;&#39046;&#22495;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#30340;&#30740;&#31350;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LANISTR&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#20174;&#35821;&#35328;&#12289;&#22270;&#20687;&#21644;&#32467;&#26500;&#21270;&#25968;&#25454;&#20013;&#36827;&#34892;&#23398;&#20064;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;&#27169;&#22359;&#65292;&#24182;&#37319;&#29992;&#22522;&#20110;&#30456;&#20284;&#24615;&#30340;&#22810;&#27169;&#24577;&#36974;&#32617;&#25439;&#22833;&#65292;&#20351;&#24471;LANISTR&#33021;&#22815;&#22312;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#25968;&#25454;&#20013;&#23398;&#20064;&#36328;&#27169;&#24577;&#20851;&#31995;&#65292;&#24182;&#22312;&#35757;&#32451;&#21644;&#27979;&#35797;&#26102;&#22788;&#29702;&#32570;&#22833;&#30340;&#27169;&#24577;&#12290;&#22312;&#20004;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25968;&#25454;&#38598;MIMIC-IV&#21644;Amazon Product Review&#19978;&#65292;&#19982;&#26368;&#20808;&#36827;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#30456;&#27604;&#65292;LANISTR&#20998;&#21035;&#36798;&#21040;&#20102;6.47%&#65288;AUROC&#65289;&#21644;&#39640;&#36798;17.69%&#65288;&#20934;&#30830;&#24230;&#65289;&#30340;&#32477;&#23545;&#25552;&#21319;&#65292;&#24182;&#26174;&#31034;&#20986;&#26356;&#24378;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal large-scale pretraining has shown impressive performance gains for unstructured data including language, image, audio, and video. Yet, the scenario most prominent in real-world applications is the existence of combination of structured (including tabular and time-series) and unstructured data, and this has so far been understudied. Towards this end, we propose LANISTR, a novel attention-based framework to learn from LANguage, Image, and STRuctured data. We introduce a new multimodal fusion module with a similarity-based multimodal masking loss that enables LANISTR to learn cross-modal relations from large-scale multimodal data with missing modalities during training and test time. On two publicly available challenging datasets, MIMIC-IV and Amazon Product Review, LANISTR achieves absolute improvements of 6.47% (AUROC) and up to 17.69% (accuracy), respectively, compared to the state-of-the-art multimodal models while showing superior generalization capabilities.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#31532;&#19968;&#20010;&#27880;&#37322;&#26377;&#32454;&#31890;&#24230;&#20107;&#23454;&#38169;&#35823;&#30340;&#23545;&#35805;&#25688;&#35201;&#25968;&#25454;&#38598;&#65292;&#25506;&#32034;&#20102;&#32454;&#31890;&#24230;&#20107;&#23454;&#38169;&#35823;&#26816;&#27979;&#20316;&#20026;&#19968;&#20010;&#21477;&#23376;&#32423;&#22810;&#26631;&#31614;&#20998;&#31867;&#38382;&#39064;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#27169;&#22411;&#65292;&#21462;&#24471;&#20102;&#21644; SOTA &#27169;&#22411;&#30456;&#36817;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.16548</link><description>&lt;p&gt;
&#23545;&#35805;&#25688;&#35201;&#20013;&#32454;&#31890;&#24230;&#20107;&#23454;&#38169;&#35823;&#30340;&#27880;&#37322;&#21644;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Annotating and Detecting Fine-grained Factual Errors for Dialogue Summarization. (arXiv:2305.16548v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16548
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#31532;&#19968;&#20010;&#27880;&#37322;&#26377;&#32454;&#31890;&#24230;&#20107;&#23454;&#38169;&#35823;&#30340;&#23545;&#35805;&#25688;&#35201;&#25968;&#25454;&#38598;&#65292;&#25506;&#32034;&#20102;&#32454;&#31890;&#24230;&#20107;&#23454;&#38169;&#35823;&#26816;&#27979;&#20316;&#20026;&#19968;&#20010;&#21477;&#23376;&#32423;&#22810;&#26631;&#31614;&#20998;&#31867;&#38382;&#39064;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#27169;&#22411;&#65292;&#21462;&#24471;&#20102;&#21644; SOTA &#27169;&#22411;&#30456;&#36817;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#26684;&#24335;&#33391;&#22909;&#30340;&#25991;&#26723;&#65288;&#22914;&#26032;&#38395;&#25991;&#31456;&#65289;&#29983;&#25104;&#30340;&#25688;&#35201;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#12290;&#20294;&#26159;&#65292;&#23545;&#35805;&#25688;&#35201;&#19968;&#30452;&#26410;&#34987;&#25506;&#32034;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#31532;&#19968;&#20010;&#24102;&#26377;&#32454;&#31890;&#24230;&#20107;&#23454;&#38169;&#35823;&#27880;&#37322;&#30340;&#25968;&#25454;&#38598; DIASUMFACT&#12290;&#25105;&#20204;&#23558;&#32454;&#31890;&#24230;&#20107;&#23454;&#38169;&#35823;&#26816;&#27979;&#23450;&#20041;&#20026;&#19968;&#20010;&#21477;&#23376;&#32423;&#22810;&#26631;&#31614;&#20998;&#31867;&#38382;&#39064;&#65292;&#24182;&#35780;&#20272;&#20102;&#20004;&#20010;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#22312;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#12290;&#20004;&#20010;&#27169;&#22411;&#37117;&#20135;&#29983;&#20102;&#27425;&#20248;&#32467;&#26524;&#65292;&#20845;&#20010;&#38169;&#35823;&#31867;&#21035;&#30340;&#23439;&#24179;&#22343; F1 &#20998;&#25968;&#32422;&#20026; 0.25&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#39044;&#35757;&#32451;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#36827;&#34892;&#20505;&#36873;&#25490;&#21517;&#30340;&#26080;&#30417;&#30563;&#27169;&#22411; ENDERANKER&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#34920;&#29616;&#19982; SOTA &#27169;&#22411;&#19981;&#30456;&#19978;&#19979;&#65292;&#21516;&#26102;&#38656;&#35201;&#36739;&#23569;&#30340;&#36164;&#28304;&#12290;&#36825;&#20123;&#35266;&#23519;&#32467;&#26524;&#35777;&#23454;&#20102;&#20174;&#23545;&#35805;&#25688;&#35201;&#20013;&#26816;&#27979;&#20107;&#23454;&#38169;&#35823;&#30340;&#25361;&#25112;&#65292;&#36825;&#38656;&#35201;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#65292;&#32780;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#21644;&#23454;&#39564;&#32467;&#26524;&#20026;&#27492;&#25552;&#20379;&#20102;&#22362;&#23454;&#30340;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
A series of datasets and models have been proposed for summaries generated for well-formatted documents such as news articles. Dialogue summaries, however, have been under explored. In this paper, we present the first dataset with fine-grained factual error annotations named DIASUMFACT. We define fine-grained factual error detection as a sentence-level multi-label classification problem, and we evaluate two state-of-the-art (SOTA) models on our dataset. Both models yield sub-optimal results, with a macro-averaged F1 score of around 0.25 over 6 error classes. We further propose an unsupervised model ENDERANKER via candidate ranking using pretrained encoder-decoder models. Our model performs on par with the SOTA models while requiring fewer resources. These observations confirm the challenges in detecting factual errors from dialogue summaries, which call for further studies, for which our dataset and results offer a solid foundation.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37325;&#28201;&#32467;&#26500;&#21270;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65292;&#24320;&#21457;&#20102;&#29616;&#20195;&#23454;&#29616;&#26041;&#27861;&#24182;&#35777;&#26126;&#20854;&#22312;&#31934;&#24230;&#21644;&#25928;&#29575;&#26041;&#38754;&#20248;&#20110;&#26356;&#19968;&#33324;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.16543</link><description>&lt;p&gt;
&#37325;&#28201;&#32467;&#26500;&#21270;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
Revisiting Structured Variational Autoencoders. (arXiv:2305.16543v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16543
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#28201;&#32467;&#26500;&#21270;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65292;&#24320;&#21457;&#20102;&#29616;&#20195;&#23454;&#29616;&#26041;&#27861;&#24182;&#35777;&#26126;&#20854;&#22312;&#31934;&#24230;&#21644;&#25928;&#29575;&#26041;&#38754;&#20248;&#20110;&#26356;&#19968;&#33324;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32467;&#26500;&#21270;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;SVAEs&#65289;&#23558;&#27010;&#29575;&#22270;&#27169;&#22411;&#30340;&#20808;&#39564;&#24212;&#29992;&#20110;&#28508;&#21464;&#37327;&#65292;&#21033;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23558;&#28508;&#21464;&#37327;&#19982;&#35266;&#27979;&#25968;&#25454;&#32852;&#31995;&#36215;&#26469;&#65292;&#24182;&#20351;&#29992;&#32467;&#26500;&#21270;&#31639;&#27861;&#36827;&#34892;&#36817;&#20284;&#21518;&#39564;&#25512;&#26029;&#12290;&#36825;&#20123;&#27169;&#22411;&#23545;&#20110;&#24207;&#21015;&#25968;&#25454;&#29305;&#21035;&#26377;&#21560;&#24341;&#21147;&#65292;&#22240;&#20026;&#20808;&#39564;&#21487;&#20197;&#25429;&#25417;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#20854;&#29702;&#24565;&#20248;&#32654;&#65292;&#20294;&#23454;&#29616;&#38590;&#24230;&#36739;&#22823;&#65292;&#23454;&#38469;&#24212;&#29992;&#20013;&#26356;&#36890;&#29992;&#30340;&#26041;&#27861;&#26356;&#21463;&#38738;&#30544;&#12290;&#26412;&#25991;&#37319;&#29992;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#24037;&#20855;&#37325;&#26032;&#23457;&#35270;SVAEs&#65292;&#24182;&#35777;&#26126;&#23427;&#20204;&#22312;&#31934;&#24230;&#21644;&#25928;&#29575;&#26041;&#38754;&#20248;&#20110;&#26356;&#19968;&#33324;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#29616;&#20195;&#23454;&#29616;&#26041;&#27861;&#65292;&#23545;SVAE&#26680;&#24515;&#30340;&#28040;&#24687;&#20256;&#36882;&#31639;&#27861;&#36827;&#34892;&#20102;&#30828;&#20214;&#21152;&#36895;&#12289;&#24182;&#34892;&#21270;&#21644;&#33258;&#21160;&#27714;&#23548;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#21033;&#29992;&#20808;&#39564;&#20013;&#30340;&#32467;&#26500;&#65292;SVAE&#21487;&#20197;&#23398;&#20064;&#26356;&#31934;&#30830;&#30340;&#27169;&#22411;&#21644;&#21518;&#39564;&#20998;&#24067;&#65292;&#36825;&#36716;&#21270;&#20026;&#20102;&#24615;&#33021;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
Structured variational autoencoders (SVAEs) combine probabilistic graphical model priors on latent variables, deep neural networks to link latent variables to observed data, and structure-exploiting algorithms for approximate posterior inference. These models are particularly appealing for sequential data, where the prior can capture temporal dependencies. However, despite their conceptual elegance, SVAEs have proven difficult to implement, and more general approaches have been favored in practice. Here, we revisit SVAEs using modern machine learning tools and demonstrate their advantages over more general alternatives in terms of both accuracy and efficiency. First, we develop a modern implementation for hardware acceleration, parallelization, and automatic differentiation of the message passing algorithms at the core of the SVAE. Second, we show that by exploiting structure in the prior, the SVAE learns more accurate models and posterior distributions, which translate into improved p
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22914;&#20309;&#36890;&#36807;&#35757;&#32451;&#20351;&#29992;&#31034;&#20363;&#12289;&#19978;&#19979;&#25991;&#28436;&#31034;&#21644;&#29983;&#25104;&#26679;&#24335;&#35268;&#21017;&#26469;&#21152;&#24378;&#24320;&#28304;LLMs&#20197;&#36798;&#21040;&#19982;&#23553;&#38381;&#22411;API&#30340;&#24037;&#20855;&#25805;&#20316;&#24615;&#33021;&#21516;&#31561;&#29978;&#33267;&#26356;&#20248;&#30340;&#25928;&#26524;&#65292;&#24182;&#36890;&#36807;ToolBench&#27979;&#35797;&#24471;&#20986;&#20102;&#23454;&#39564;&#32467;&#26524;&#65292;&#21516;&#26102;&#26412;&#25991;&#36824;&#35777;&#26126;&#20102;&#25913;&#36827;&#30340;&#24320;&#28304;LLMs&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.16504</link><description>&lt;p&gt;
&#24320;&#28304;&#22823;&#35821;&#35328;&#27169;&#22411;&#23545;&#24037;&#20855;&#25805;&#20316;&#33021;&#21147;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Tool Manipulation Capability of Open-source Large Language Models. (arXiv:2305.16504v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16504
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22914;&#20309;&#36890;&#36807;&#35757;&#32451;&#20351;&#29992;&#31034;&#20363;&#12289;&#19978;&#19979;&#25991;&#28436;&#31034;&#21644;&#29983;&#25104;&#26679;&#24335;&#35268;&#21017;&#26469;&#21152;&#24378;&#24320;&#28304;LLMs&#20197;&#36798;&#21040;&#19982;&#23553;&#38381;&#22411;API&#30340;&#24037;&#20855;&#25805;&#20316;&#24615;&#33021;&#21516;&#31561;&#29978;&#33267;&#26356;&#20248;&#30340;&#25928;&#26524;&#65292;&#24182;&#36890;&#36807;ToolBench&#27979;&#35797;&#24471;&#20986;&#20102;&#23454;&#39564;&#32467;&#26524;&#65292;&#21516;&#26102;&#26412;&#25991;&#36824;&#35777;&#26126;&#20102;&#25913;&#36827;&#30340;&#24320;&#28304;LLMs&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#23545;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;( LLMs)&#36827;&#34892;&#36719;&#20214;&#24037;&#20855;&#25805;&#20316;&#30340;&#30740;&#31350;&#22823;&#22810;&#20381;&#36182;&#20110;&#23553;&#38381;&#27169;&#22411;API&#12290;&#30001;&#20110;&#21521;&#23553;&#38381;LLMAPI&#26381;&#21153;&#20844;&#24320;&#20449;&#24687;&#23384;&#22312;&#23433;&#20840;&#21644;&#40065;&#26834;&#24615;&#39118;&#38505;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#24037;&#19994;&#37319;&#29992;&#21463;&#21040;&#20102;&#23454;&#36136;&#24615;&#38480;&#21046;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38382;&#39064;&#65292;&#37027;&#23601;&#26159;&#25105;&#20204;&#33021;&#21542;&#22312;&#23454;&#36341;&#20013;&#21152;&#24378;&#24320;&#28304;LLMs&#30340;&#21151;&#33021;&#65292;&#20351;&#20854;&#22312;&#24037;&#20855;&#25805;&#20316;&#26041;&#38754;&#19982;&#39046;&#20808;&#30340;&#23553;&#38381;LLM APIs&#31454;&#20105;&#12290;&#36890;&#36807;&#20998;&#26512;&#24120;&#35265;&#30340;&#24037;&#20855;&#25805;&#20316;&#22833;&#36133;&#65292;&#25105;&#20204;&#39318;&#20808;&#23637;&#31034;&#20102;&#24320;&#28304;LLMs&#21487;&#33021;&#38656;&#35201;&#35757;&#32451;&#20351;&#29992;&#31034;&#20363;&#12289;&#19978;&#19979;&#25991;&#28436;&#31034;&#21644;&#29983;&#25104;&#26679;&#24335;&#35268;&#21017;&#26469;&#35299;&#20915;&#22833;&#36133;&#12290;&#36825;&#20123;&#35265;&#35299;&#28608;&#21457;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;LLM&#25991;&#29486;&#20013;&#30340;&#32463;&#20856;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#25105;&#20204;&#21487;&#20197;&#23558;&#23427;&#20204;&#20316;&#20026;&#31243;&#24207;&#25968;&#25454;&#29983;&#25104;&#30340;&#27169;&#22411;&#23545;&#40784;&#12289;&#31995;&#32479;&#25552;&#31034;&#21644;&#19978;&#19979;&#25991;&#28436;&#31034;&#26816;&#32034;&#22120;&#26469;&#36866;&#24212;&#24320;&#28304;LLMs&#20197;&#23454;&#29616;&#24037;&#20855;&#25805;&#20316;&#30340;&#22686;&#24378;&#12290;&#20026;&#20102;&#35780;&#20272;&#36825;&#20123;&#25216;&#26415;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;ToolBench&#65292;&#19968;&#20010;&#24037;&#20855;&#25805;&#20316;&#33021;&#21147;&#27979;&#35797;&#22871;&#20214;&#65292;&#21253;&#25324;&#29616;&#26377;API&#21644;&#25105;&#20204;&#25913;&#36827;&#30340;&#24320;&#28304;LLMs&#12290;&#22312;&#19977;&#20010;&#19981;&#21516;&#30340;&#32534;&#31243;&#20219;&#21153;&#19978;&#65292;&#25105;&#20204;&#21457;&#29616;&#25913;&#36827;&#30340;&#24320;&#28304;LLMs&#33021;&#22815;&#36798;&#21040;&#25110;&#36229;&#36234;&#29616;&#26377;API&#30340;&#24615;&#33021;&#65292;&#20854;&#20013;&#21253;&#25324;&#23545;&#24050;&#32534;&#20889;&#30340;&#31243;&#24207;&#36827;&#34892;&#36731;&#24494;&#20462;&#25913;&#31561;&#23454;&#38469;&#25805;&#20316;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#21453;&#21521;&#24037;&#31243;&#27979;&#35797;&#21644;&#40657;&#30418;&#27979;&#35797;&#36827;&#19968;&#27493;&#35777;&#26126;&#20102;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent studies on software tool manipulation with large language models (LLMs) mostly rely on closed model APIs. The industrial adoption of these models is substantially constrained due to the security and robustness risks in exposing information to closed LLM API services. In this paper, we ask can we enhance open-source LLMs to be competitive to leading closed LLM APIs in tool manipulation, with practical amount of human supervision. By analyzing common tool manipulation failures, we first demonstrate that open-source LLMs may require training with usage examples, in-context demonstration and generation style regulation to resolve failures. These insights motivate us to revisit classical methods in LLM literature, and demonstrate that we can adapt them as model alignment with programmatic data generation, system prompts and in-context demonstration retrievers to enhance open-source LLMs for tool manipulation. To evaluate these techniques, we create the ToolBench, a tool manipulation 
&lt;/p&gt;</description></item><item><title>AD-NEv&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#22810;&#32423;&#20248;&#21270;&#31070;&#32463;&#36827;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#65292;&#36890;&#36807;&#21327;&#21516;&#20248;&#21270;&#29305;&#24449;&#23376;&#31354;&#38388;&#12289;&#27169;&#22411;&#26550;&#26500;&#21644;&#27169;&#22411;&#26435;&#37325;&#30340;&#26041;&#27861;&#65292;&#34920;&#29616;&#20986;&#27604;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#26356;&#22909;&#30340;&#26816;&#27979;&#31934;&#24230;&#21644;&#35745;&#31639;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2305.16497</link><description>&lt;p&gt;
AD-NEV&#65306;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#22810;&#23618;&#31070;&#32463;&#36827;&#21270;&#26694;&#26550;&#29992;&#20110;&#22810;&#20803;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
AD-NEV: A Scalable Multi-level Neuroevolution Framework for Multivariate Anomaly Detection. (arXiv:2305.16497v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16497
&lt;/p&gt;
&lt;p&gt;
AD-NEv&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#22810;&#32423;&#20248;&#21270;&#31070;&#32463;&#36827;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#65292;&#36890;&#36807;&#21327;&#21516;&#20248;&#21270;&#29305;&#24449;&#23376;&#31354;&#38388;&#12289;&#27169;&#22411;&#26550;&#26500;&#21644;&#27169;&#22411;&#26435;&#37325;&#30340;&#26041;&#27861;&#65292;&#34920;&#29616;&#20986;&#27604;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#26356;&#22909;&#30340;&#26816;&#27979;&#31934;&#24230;&#21644;&#35745;&#31639;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#24120;&#26816;&#27979;&#24037;&#20855;&#21644;&#26041;&#27861;&#22312;&#29616;&#20195;&#30340;&#26234;&#33021;&#29289;&#29702;&#31995;&#32479;&#21644;&#25925;&#38556;&#39044;&#27979;&#31995;&#32479;&#20013;&#20855;&#26377;&#20851;&#38190;&#33021;&#21147;&#12290;&#23613;&#31649;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#22312;&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#21457;&#23637;&#36805;&#36895;&#65292;&#20294;&#38024;&#23545;&#32473;&#23450;&#25968;&#25454;&#38598;&#30340;&#27169;&#22411;&#20248;&#21270;&#26159;&#19968;&#20010;&#32321;&#29712;&#32780;&#32791;&#26102;&#30340;&#36807;&#31243;&#12290;&#31070;&#32463;&#36827;&#21270;&#21487;&#20197;&#26159;&#36825;&#20010;&#38382;&#39064;&#30340;&#26377;&#25928;&#21644;&#39640;&#25928;&#35299;&#20915;&#26041;&#26696;&#65292;&#26159;&#19968;&#31181;&#23436;&#20840;&#33258;&#21160;&#21270;&#30340;&#25628;&#32034;&#26041;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#26368;&#20248;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#25903;&#25345;&#26799;&#24230;&#21644;&#38750;&#26799;&#24230;&#30340;&#24494;&#35843;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#22823;&#22810;&#38598;&#20013;&#20110;&#20248;&#21270;&#27169;&#22411;&#32467;&#26500;&#65292;&#32780;&#26410;&#32771;&#34385;&#29305;&#24449;&#23376;&#31354;&#38388;&#21644;&#27169;&#22411;&#26435;&#37325;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Anomaly Detection Neuroevolution (AD-NEv)&#30340;&#21487;&#25193;&#23637;&#22810;&#32423;&#20248;&#21270;&#31070;&#32463;&#36827;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#12290;&#35813;&#26041;&#27861;&#34920;&#31034;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#21327;&#21516;&#20248;&#21270;&#65306;i) &#22522;&#20110;&#35013;&#34955;&#25216;&#26415;&#23545;&#38598;&#21512;&#27169;&#22411;&#36827;&#34892;&#29305;&#24449;&#23376;&#31354;&#38388;&#20248;&#21270;; ii) &#20248;&#21270;&#21333;&#20010;&#24322;&#24120;&#26816;&#27979;&#32593;&#32476;&#30340;&#27169;&#22411;&#26550;&#26500;; iii) &#20351;&#29992;&#26799;&#24230;&#21644;&#38750;&#26799;&#24230;&#26041;&#27861;&#20248;&#21270;&#27169;&#22411;&#26435;&#37325;&#12290;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#21487;&#25193;&#23637;&#65292;&#21487;&#24212;&#29992;&#20110;&#19981;&#21516;&#22823;&#23567;&#21644;&#32500;&#24230;&#30340;&#25968;&#25454;&#38598;&#12290;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;AD-NEv&#22312;&#26816;&#27979;&#31934;&#24230;&#21644;&#35745;&#31639;&#25928;&#29575;&#26041;&#38754;&#34920;&#29616;&#20248;&#20110;&#25110;&#21487;&#19982;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#23218;&#32654;&#12290;
&lt;/p&gt;
&lt;p&gt;
Anomaly detection tools and methods present a key capability in modern cyberphysical and failure prediction systems. Despite the fast-paced development in deep learning architectures for anomaly detection, model optimization for a given dataset is a cumbersome and time consuming process. Neuroevolution could be an effective and efficient solution to this problem, as a fully automated search method for learning optimal neural networks, supporting both gradient and non-gradient fine tuning. However, existing methods mostly focus on optimizing model architectures without taking into account feature subspaces and model weights. In this work, we propose Anomaly Detection Neuroevolution (AD-NEv) - a scalable multi-level optimized neuroevolution framework for multivariate time series anomaly detection. The method represents a novel approach to synergically: i) optimize feature subspaces for an ensemble model based on the bagging technique; ii) optimize the model architecture of single anomaly
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Diff-PGD&#30340;&#26032;&#26694;&#26550;&#65292;&#29992;&#20110;&#29983;&#25104;&#25509;&#36817;&#21407;&#22987;&#25968;&#25454;&#20998;&#24067;&#12289;&#36924;&#30495;&#30340;&#23545;&#25239;&#26679;&#26412;&#65292;&#20855;&#26377;&#36739;&#22909;&#30340;&#38544;&#34109;&#24615;&#21644;&#23545;&#25239;&#24378;&#24230;&#21487;&#35843;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.16494</link><description>&lt;p&gt;
&#22522;&#20110;&#25193;&#25955;&#30340;&#23545;&#25239;&#26679;&#26412;&#29983;&#25104;&#20197;&#25552;&#39640;&#38544;&#34109;&#24615;&#21644;&#21487;&#25511;&#24615;
&lt;/p&gt;
&lt;p&gt;
Diffusion-Based Adversarial Sample Generation for Improved Stealthiness and Controllability. (arXiv:2305.16494v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16494
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Diff-PGD&#30340;&#26032;&#26694;&#26550;&#65292;&#29992;&#20110;&#29983;&#25104;&#25509;&#36817;&#21407;&#22987;&#25968;&#25454;&#20998;&#24067;&#12289;&#36924;&#30495;&#30340;&#23545;&#25239;&#26679;&#26412;&#65292;&#20855;&#26377;&#36739;&#22909;&#30340;&#38544;&#34109;&#24615;&#21644;&#23545;&#25239;&#24378;&#24230;&#21487;&#35843;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#26679;&#26412;&#30340;&#24433;&#21709;&#65306;&#36825;&#26159;&#19968;&#31181;&#29305;&#24847;&#21046;&#20316;&#30340;&#33258;&#28982;&#22270;&#29255;&#30340;&#24494;&#23567;&#21464;&#21270;&#65292;&#26088;&#22312;&#35823;&#23548;&#27169;&#22411;&#12290;&#34429;&#28982;&#36825;&#20123;&#23545;&#25239;&#26679;&#26412;&#22312;&#25968;&#23383;&#21644;&#29289;&#29702;&#22330;&#26223;&#20013;&#21487;&#20197;&#36731;&#26494;&#29983;&#25104;&#65292;&#20294;&#23427;&#20204;&#24448;&#24448;&#19982;&#33258;&#28982;&#22270;&#20687;&#30340;&#23454;&#38469;&#25968;&#25454;&#20998;&#24067;&#24046;&#24322;&#24456;&#22823;&#65292;&#23548;&#33268;&#24378;&#24230;&#19982;&#38544;&#34109;&#24615;&#20043;&#38388;&#23384;&#22312;&#26435;&#34913;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#25193;&#25955;-&#25237;&#24433;&#26799;&#24230;&#19979;&#38477;&#65288;Diff-PGD&#65289;&#30340;&#26032;&#26694;&#26550;&#65292;&#29992;&#20110;&#29983;&#25104;&#36924;&#30495;&#30340;&#23545;&#25239;&#26679;&#26412;&#12290;&#36890;&#36807;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#24341;&#23548;&#30340;&#26799;&#24230;&#65292;Diff-PGD&#30830;&#20445;&#23545;&#25239;&#26679;&#26412;&#20445;&#25345;&#25509;&#36817;&#21407;&#22987;&#25968;&#25454;&#20998;&#24067;&#65292;&#21516;&#26102;&#20445;&#25345;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#21487;&#20197;&#36731;&#26494;&#23450;&#21046;&#29305;&#23450;&#20219;&#21153;&#65292;&#22914;&#25968;&#23383;&#25915;&#20987;&#12289;&#29289;&#29702;&#25915;&#20987;&#21644;&#22522;&#20110;&#26679;&#24335;&#30340;&#25915;&#20987;&#12290;&#19982;&#29616;&#26377;&#30340;&#29983;&#25104;&#33258;&#28982;&#39118;&#26684;&#23545;&#25239;&#26679;&#26412;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#21487;&#20197;&#20998;&#31163;&#20248;&#21270;&#23545;&#25239;&#24378;&#24230;&#21644;&#38544;&#34109;&#24615;&#65292;&#25552;&#20379;&#26356;&#22823;&#30340;&#28789;&#27963;&#24615;&#21644;&#23545;&#29983;&#25104;&#26679;&#26412;&#30340;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural networks are known to be susceptible to adversarial samples: small variations of natural examples crafted to deliberately mislead the models. While they can be easily generated using gradient-based techniques in digital and physical scenarios, they often differ greatly from the actual data distribution of natural images, resulting in a trade-off between strength and stealthiness. In this paper, we propose a novel framework dubbed Diffusion-Based Projected Gradient Descent (Diff-PGD) for generating realistic adversarial samples. By exploiting a gradient guided by a diffusion model, Diff-PGD ensures that adversarial samples remain close to the original data distribution while maintaining their effectiveness. Moreover, our framework can be easily customized for specific tasks such as digital attacks, physical-world attacks, and style-based attacks. Compared with existing methods for generating natural-style adversarial samples, our framework enables the separation of optimizing adv
&lt;/p&gt;</description></item><item><title>EgoHumans&#26159;&#19968;&#20010;&#20840;&#38754;&#30340;&#33258;&#25105;&#20013;&#24515;&#22810;&#20154;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#25512;&#36827;&#33258;&#25105;&#20013;&#24515;&#30340;&#20154;&#31867;&#19977;&#32500;&#23039;&#24577;&#20272;&#35745;&#21644;&#36319;&#36394;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#21487;&#20197;&#25903;&#25345;&#21508;&#31181;&#20219;&#21153;&#65292;&#21253;&#25324;&#20154;&#31867;&#26816;&#27979;&#12289;&#36319;&#36394;&#12289;2D/3D&#23039;&#24577;&#20272;&#35745;&#21644;&#32593;&#26684;&#24674;&#22797;&#31561;&#65292;&#24182;&#19988;&#33021;&#22815;&#25429;&#25417;&#20855;&#26377;&#25361;&#25112;&#24615;&#21644;&#26080;&#26680;&#32534;&#25490;&#30340;&#22810;&#20154;&#22330;&#26223;&#12290;</title><link>http://arxiv.org/abs/2305.16487</link><description>&lt;p&gt;
EgoHumans:&#19968;&#31181;&#20197;&#33258;&#25105;&#20026;&#20013;&#24515;&#30340;&#19977;&#32500;&#22810;&#20154;&#22522;&#20934;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
EgoHumans: An Egocentric 3D Multi-Human Benchmark. (arXiv:2305.16487v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16487
&lt;/p&gt;
&lt;p&gt;
EgoHumans&#26159;&#19968;&#20010;&#20840;&#38754;&#30340;&#33258;&#25105;&#20013;&#24515;&#22810;&#20154;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#25512;&#36827;&#33258;&#25105;&#20013;&#24515;&#30340;&#20154;&#31867;&#19977;&#32500;&#23039;&#24577;&#20272;&#35745;&#21644;&#36319;&#36394;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#21487;&#20197;&#25903;&#25345;&#21508;&#31181;&#20219;&#21153;&#65292;&#21253;&#25324;&#20154;&#31867;&#26816;&#27979;&#12289;&#36319;&#36394;&#12289;2D/3D&#23039;&#24577;&#20272;&#35745;&#21644;&#32593;&#26684;&#24674;&#22797;&#31561;&#65292;&#24182;&#19988;&#33021;&#22815;&#25429;&#25417;&#20855;&#26377;&#25361;&#25112;&#24615;&#21644;&#26080;&#26680;&#32534;&#25490;&#30340;&#22810;&#20154;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;EgoHumans&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#30340;&#35270;&#35282;&#22810;&#20154;&#35270;&#39057;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#25512;&#36827;&#33258;&#25105;&#20013;&#24515;&#30340;&#20154;&#31867;&#19977;&#32500;&#23039;&#24577;&#20272;&#35745;&#21644;&#36319;&#36394;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;&#29616;&#26377;&#30340;&#33258;&#25105;&#20013;&#24515;&#22522;&#20934;&#25968;&#25454;&#38598;&#20165;&#25429;&#25417;&#21333;&#20010;&#20027;&#20307;&#25110;&#20165;&#38480;&#20110;&#23460;&#20869;&#22330;&#26223;&#65292;&#36825;&#38480;&#21046;&#20102;&#35745;&#31639;&#26426;&#35270;&#35273;&#31639;&#27861;&#22312;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#19977;&#32500;&#25429;&#33719;&#35774;&#23450;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#33258;&#25105;&#20013;&#24515;&#22810;&#20154;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20379;&#27880;&#37322;&#25903;&#25345;&#21508;&#31181;&#20219;&#21153;&#65292;&#20363;&#22914;&#20154;&#31867;&#26816;&#27979;&#12289;&#36319;&#36394;&#12289;2D/3D&#23039;&#24577;&#20272;&#35745;&#21644;&#32593;&#26684;&#24674;&#22797;&#31561;&#12290;&#25105;&#20204;&#21033;&#29992;&#24102;&#25668;&#20687;&#22836;&#30340;&#26222;&#36890;&#30524;&#38236;&#36827;&#34892;&#35270;&#35282;&#25429;&#25417;&#65292;&#24182;&#33021;&#22815;&#25429;&#25417;&#35832;&#22914;&#36386;&#36275;&#29699;&#12289;&#20987;&#21073;&#12289;&#25490;&#29699;&#31561;&#21160;&#24577;&#27963;&#21160;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#22810;&#35270;&#35282;&#35774;&#32622;&#22312;&#20005;&#37325;&#25110;&#23436;&#20840;&#36974;&#25377;&#19979;&#20173;&#33021;&#29983;&#25104;&#20934;&#30830;&#30340;3D&#22522;&#20934;&#25968;&#25454;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#36229;&#36807;125k&#20010;&#33258;&#25105;&#20013;&#24515;&#22270;&#20687;&#65292;&#36328;&#36234;&#22810;&#31181;&#22330;&#26223;&#65292;&#29305;&#21035;&#20851;&#27880;&#20855;&#26377;&#25361;&#25112;&#24615;&#21644;&#26080;&#26680;&#32534;&#25490;&#30340;&#22810;&#20154;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present EgoHumans, a new multi-view multi-human video benchmark to advance the state-of-the-art of egocentric human 3D pose estimation and tracking. Existing egocentric benchmarks either capture single subject or indoor-only scenarios, which limit the generalization of computer vision algorithms for real-world applications. We propose a novel 3D capture setup to construct a comprehensive egocentric multi-human benchmark in the wild with annotations to support diverse tasks such as human detection, tracking, 2D/3D pose estimation, and mesh recovery. We leverage consumer-grade wearable camera-equipped glasses for the egocentric view, which enables us to capture dynamic activities like playing soccer, fencing, volleyball, etc. Furthermore, our multi-view setup generates accurate 3D ground truth even under severe or complete occlusion. The dataset consists of more than 125k egocentric images, spanning diverse scenes with a particular focus on challenging and unchoreographed multi-human 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#25209;&#27425;&#27169;&#22411;&#25972;&#21512;&#65288;BMC&#65289;&#26469;&#25903;&#25345;&#26356;&#29616;&#23454;&#30340;&#36830;&#32493;&#23398;&#20064;&#65292;&#23427;&#36890;&#36807;&#22312;&#27491;&#21017;&#21270;&#38454;&#27573;&#35757;&#32451;&#22810;&#20010;&#19987;&#23478;&#27169;&#22411;&#26469;&#23398;&#20064;&#19968;&#32452;&#19981;&#30456;&#20132;&#30340;&#20219;&#21153;&#65292;&#24182;&#22312;&#25972;&#21512;&#38454;&#27573;&#23558;&#22810;&#20010;&#19987;&#23478;&#27169;&#22411;&#25972;&#21512;&#20026;&#19968;&#20010;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.16484</link><description>&lt;p&gt;
&#25209;&#27425;&#27169;&#22411;&#25972;&#21512;&#65306;&#19968;&#20010;&#22810;&#20219;&#21153;&#27169;&#22411;&#25972;&#21512;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Batch Model Consolidation: A Multi-Task Model Consolidation Framework. (arXiv:2305.16484v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16484
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#25209;&#27425;&#27169;&#22411;&#25972;&#21512;&#65288;BMC&#65289;&#26469;&#25903;&#25345;&#26356;&#29616;&#23454;&#30340;&#36830;&#32493;&#23398;&#20064;&#65292;&#23427;&#36890;&#36807;&#22312;&#27491;&#21017;&#21270;&#38454;&#27573;&#35757;&#32451;&#22810;&#20010;&#19987;&#23478;&#27169;&#22411;&#26469;&#23398;&#20064;&#19968;&#32452;&#19981;&#30456;&#20132;&#30340;&#20219;&#21153;&#65292;&#24182;&#22312;&#25972;&#21512;&#38454;&#27573;&#23558;&#22810;&#20010;&#19987;&#23478;&#27169;&#22411;&#25972;&#21512;&#20026;&#19968;&#20010;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36830;&#32493;&#23398;&#20064;&#20013;&#65292;&#27169;&#22411;&#38656;&#35201;&#25353;&#39034;&#24207;&#23398;&#20064;&#19968;&#31995;&#21015;&#20219;&#21153;&#65292;&#32780;&#19981;&#20250;&#22312;&#20043;&#21069;&#23398;&#20064;&#30340;&#20219;&#21153;&#19978;&#20986;&#29616;&#26174;&#30528;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;&#29616;&#26377;&#26041;&#27861;&#22312;&#38754;&#23545;&#21508;&#31181;&#39046;&#22495;&#21644;&#38590;&#24230;&#30340;&#38271;&#24207;&#21015;&#20219;&#21153;&#26102;&#25928;&#26524;&#19981;&#20339;&#12290;&#35768;&#22810;&#29616;&#26377;&#30340;&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;&#30001;&#20110;&#20869;&#23384;&#36164;&#28304;&#28040;&#32791;&#36807;&#22823;&#25110;&#35757;&#32451;&#26102;&#38388;&#36807;&#38271;&#32780;&#38590;&#20197;&#22312;&#23454;&#36341;&#20013;&#24212;&#29992;&#65292;&#25110;&#21482;&#33021;&#22312;&#21333;&#20010;&#35774;&#22791;&#19978;&#32039;&#23494;&#32806;&#21512;&#12290;&#26412;&#25991;&#25552;&#20986;&#25209;&#27425;&#27169;&#22411;&#25972;&#21512;&#65288;BMC&#65289;&#26469;&#25903;&#25345;&#26356;&#29616;&#23454;&#30340;&#36830;&#32493;&#23398;&#20064;&#65292;&#38754;&#23545;&#22810;&#20010;&#20195;&#29702;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#25509;&#35302;&#30340;&#24773;&#20917;&#12290;&#22312;&#27491;&#21017;&#21270;&#38454;&#27573;&#65292;BMC&#24182;&#34892;&#35757;&#32451;&#22810;&#20010;&#19987;&#23478;&#27169;&#22411;&#26469;&#23398;&#20064;&#19968;&#32452;&#19981;&#30456;&#20132;&#30340;&#20219;&#21153;&#12290;&#27599;&#20010;&#19987;&#23478;&#36890;&#36807;&#31283;&#23450;&#24615;&#25439;&#22833;&#19982;&#19968;&#20010;&#22522;&#30784;&#27169;&#22411;&#20445;&#25345;&#26435;&#37325;&#30456;&#20284;&#24615;&#65292;&#24182;&#20174;&#20219;&#21153;&#25968;&#25454;&#30340;&#19968;&#37096;&#20998;&#26500;&#24314;&#32531;&#20914;&#21306;&#12290;&#22312;&#25972;&#21512;&#38454;&#27573;&#65292;&#25105;&#20204;&#23558;&#22810;&#20010;&#19987;&#23478;&#27169;&#22411;&#25972;&#21512;&#20026;&#19968;&#20010;&#27169;&#22411;&#65292;&#24182;&#22312;&#30446;&#26631;&#20219;&#21153;&#19978;&#36827;&#34892;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;
In Continual Learning (CL), a model is required to learn a stream of tasks sequentially without significant performance degradation on previously learned tasks. Current approaches fail for a long sequence of tasks from diverse domains and difficulties. Many of the existing CL approaches are difficult to apply in practice due to excessive memory cost or training time, or are tightly coupled to a single device. With the intuition derived from the widely applied mini-batch training, we propose Batch Model Consolidation ($\textbf{BMC}$) to support more realistic CL under conditions where multiple agents are exposed to a range of tasks. During a $\textit{regularization}$ phase, BMC trains multiple $\textit{expert models}$ in parallel on a set of disjoint tasks. Each expert maintains weight similarity to a $\textit{base model}$ through a $\textit{stability loss}$, and constructs a $\textit{buffer}$ from a fraction of the task's data. During the $\textit{consolidation}$ phase, we combine the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#20154;&#24037;&#26234;&#33021;&#20914;&#31361;&#30340;&#27010;&#24565;&#12289;&#21407;&#22240;&#12289;&#27979;&#37327;&#26041;&#27861;&#21644;&#39118;&#38505;&#35780;&#20272;&#65292;&#31361;&#20986;&#26174;&#31034;&#20154;&#24037;&#26234;&#33021;&#26159;&#28508;&#22312;&#30340;&#31532;&#20108;&#20915;&#31574;&#32773;&#65292;&#20914;&#31361;&#39118;&#38505;&#26159;&#26174;&#33879;&#30340;&#65292;&#19981;&#33021;&#34987;&#24573;&#35270;&#12290;</title><link>http://arxiv.org/abs/2305.16477</link><description>&lt;p&gt;
&#31532;&#20108;&#20915;&#31574;&#32773;&#35686;&#25253;&#65306;&#20154;&#24037;&#26234;&#33021;&#20914;&#31361;&#31616;&#20171;
&lt;/p&gt;
&lt;p&gt;
Alert of the Second Decision-maker: An Introduction to Human-AI Conflict. (arXiv:2305.16477v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16477
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20154;&#24037;&#26234;&#33021;&#20914;&#31361;&#30340;&#27010;&#24565;&#12289;&#21407;&#22240;&#12289;&#27979;&#37327;&#26041;&#27861;&#21644;&#39118;&#38505;&#35780;&#20272;&#65292;&#31361;&#20986;&#26174;&#31034;&#20154;&#24037;&#26234;&#33021;&#26159;&#28508;&#22312;&#30340;&#31532;&#20108;&#20915;&#31574;&#32773;&#65292;&#20914;&#31361;&#39118;&#38505;&#26159;&#26174;&#33879;&#30340;&#65292;&#19981;&#33021;&#34987;&#24573;&#35270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#21644;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#20043;&#38388;&#30340;&#21512;&#20316;&#26159;&#36825;&#20010;&#25968;&#23383;&#26102;&#20195;&#30340;&#19968;&#20010;&#37325;&#35201;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;&#22312;&#21516;&#27493;&#24037;&#20316;&#26102;&#65292;&#20154;&#31867;&#21644;AI&#21487;&#33021;&#23384;&#22312;&#35266;&#23519;&#12289;&#35299;&#37322;&#21644;&#34892;&#21160;&#19978;&#30340;&#20914;&#31361;&#12290;&#36825;&#31181;&#29616;&#35937;&#36890;&#24120;&#34987;&#25925;&#38556;&#25513;&#30422;&#65292;&#19981;&#24184;&#30340;&#26159;&#65292;&#34987;&#24573;&#35270;&#20102;&#12290;&#26412;&#25991;&#31995;&#32479;&#22320;&#20171;&#32461;&#20102;&#20154;&#24037;&#26234;&#33021;&#20914;&#31361;&#30340;&#27010;&#24565;&#12289;&#21407;&#22240;&#12289;&#27979;&#37327;&#26041;&#27861;&#21644;&#39118;&#38505;&#35780;&#20272;&#12290;&#32467;&#26524;&#31361;&#20986;&#26174;&#31034;&#65292;&#38500;&#20102;&#20154;&#31867;&#20197;&#22806;&#65292;&#36824;&#23384;&#22312;&#28508;&#22312;&#30340;&#31532;&#20108;&#20915;&#31574;&#32773;&#65292;&#21363;AI&#65307;&#20154;&#24037;&#26234;&#33021;&#20914;&#31361;&#26159;&#25968;&#23383;&#21270;&#27969;&#31243;&#31995;&#32479;&#20013;&#29420;&#29305;&#32780;&#26032;&#20852;&#30340;&#39118;&#38505;&#65307;&#36825;&#26159;&#19968;&#20010;&#38656;&#35201;&#19982;&#20256;&#32479;&#30340;&#25925;&#38556;&#21644;&#22833;&#25928;&#20998;&#26512;&#21306;&#20998;&#30340;&#36328;&#23398;&#31185;&#39046;&#22495;&#65307;&#20914;&#31361;&#39118;&#38505;&#26159;&#26174;&#33879;&#30340;&#65292;&#19981;&#33021;&#34987;&#24573;&#35270;&#12290;
&lt;/p&gt;
&lt;p&gt;
The collaboration between humans and artificial intelligence (AI) is a significant feature in this digital age. However, humans and AI may have observation, interpretation, and action conflicts when working synchronously. This phenomenon is often masked by faults and, unfortunately, overlooked. This paper systematically introduces the human-AI conflict concept, causes, measurement methods, and risk assessment. The results highlight that there is a potential second decision-maker besides the human, which is the AI; the human-AI conflict is a unique and emerging risk in digitalized process systems; and this is an interdisciplinary field that needs to be distinguished from traditional fault and failure analysis; the conflict risk is significant and cannot be ignored.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#23450;&#20041;&#25968;&#25454;&#38598;&#21644;&#26377;&#25928;&#26816;&#27979;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#22686;&#21152;&#22403;&#22334;&#23454;&#20363;&#30340;&#22810;&#26679;&#24615;&#65292;&#22312;&#28145;&#20837;&#27700;&#19979;&#29615;&#22659;&#20013;&#25552;&#39640;&#20854;&#26816;&#27979;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2305.16460</link><description>&lt;p&gt;
&#20248;&#21270;&#30340;&#33258;&#23450;&#20041;&#25968;&#25454;&#38598;&#29992;&#20110;&#39640;&#25928;&#26816;&#27979;&#27700;&#19979;&#22403;&#22334;
&lt;/p&gt;
&lt;p&gt;
Optimized Custom Dataset for Efficient Detection of Underwater Trash. (arXiv:2305.16460v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16460
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#23450;&#20041;&#25968;&#25454;&#38598;&#21644;&#26377;&#25928;&#26816;&#27979;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#22686;&#21152;&#22403;&#22334;&#23454;&#20363;&#30340;&#22810;&#26679;&#24615;&#65292;&#22312;&#28145;&#20837;&#27700;&#19979;&#29615;&#22659;&#20013;&#25552;&#39640;&#20854;&#26816;&#27979;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#35780;&#20272;&#21644;&#28165;&#38500;&#28508;&#22312;&#30340;&#27700;&#19979;&#24223;&#29289;&#23545;&#20110;&#20445;&#25252;&#28023;&#27915;&#29983;&#29289;&#21644;&#29615;&#22659;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#38024;&#23545;&#27700;&#19979;&#22403;&#22334;&#26816;&#27979;&#25152;&#23384;&#22312;&#30340;&#25361;&#25112;&#65292;&#22914;&#20809;&#25240;&#23556;&#12289;&#21560;&#25910;&#12289;&#24748;&#28014;&#39063;&#31890;&#21644;&#33394;&#24425;&#25197;&#26354;&#31561;&#22240;&#32032;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#23450;&#20041;&#25968;&#25454;&#38598;&#21644;&#26377;&#25928;&#26816;&#27979;&#26041;&#27861;&#12290;&#35813;&#25968;&#25454;&#38598;&#28085;&#30422;&#20102;&#22810;&#31181;&#27700;&#19979;&#29615;&#22659;&#65292;&#24182;&#21253;&#25324;&#23545;&#24223;&#24323;&#29289;&#23454;&#20363;&#30340;&#31934;&#30830;&#23450;&#20301;&#26631;&#27880;&#12290;&#26368;&#32456;&#65292;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#32467;&#26500;&#65292;&#30446;&#30340;&#26159;&#36890;&#36807;&#22686;&#21152;&#22403;&#22334;&#23454;&#20363;&#30340;&#22810;&#26679;&#24615;&#65292;&#22312;&#28145;&#20837;&#27700;&#19979;&#29615;&#22659;&#20013;&#25552;&#39640;&#20854;&#26816;&#27979;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurately quantifying and removing submerged underwater waste plays a crucial role in safeguarding marine life and preserving the environment. While detecting floating and surface debris is relatively straightforward, quantifying submerged waste presents significant challenges due to factors like light refraction, absorption, suspended particles, and color distortion. This paper addresses these challenges by proposing the development of a custom dataset and an efficient detection approach for submerged marine debris. The dataset encompasses diverse underwater environments and incorporates annotations for precise labeling of debris instances. Ultimately, the primary objective of this custom dataset is to enhance the diversity of litter instances and improve their detection accuracy in deep submerged environments by leveraging state-of-the-art deep learning architectures.
&lt;/p&gt;</description></item><item><title>KeyPosS&#26159;&#19968;&#31181;&#38754;&#37096;&#26631;&#35760;&#26816;&#27979;&#26694;&#26550;&#65292;&#37319;&#29992;&#30495;&#23454;&#36317;&#31163;&#22810;&#36793;&#23450;&#20301;&#31639;&#27861;&#23454;&#29616;&#24555;&#36895;&#32780;&#20934;&#30830;&#30340;&#26816;&#27979;&#65292;&#36991;&#20813;&#20102;&#20256;&#32479;&#26041;&#27861;&#20013;&#30340;&#35745;&#31639;&#36127;&#25285;&#21644;&#37327;&#21270;&#35823;&#24046;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.16437</link><description>&lt;p&gt;
KeyPosS: &#22522;&#20110; GPS &#28789;&#24863;&#30340;&#30495;&#23454;&#36317;&#31163;&#22810;&#36793;&#23450;&#20301;&#30340;&#21363;&#25554;&#21363;&#29992;&#38754;&#37096;&#26631;&#35760;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
KeyPosS: Plug-and-Play Facial Landmark Detection through GPS-Inspired True-Range Multilateration. (arXiv:2305.16437v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16437
&lt;/p&gt;
&lt;p&gt;
KeyPosS&#26159;&#19968;&#31181;&#38754;&#37096;&#26631;&#35760;&#26816;&#27979;&#26694;&#26550;&#65292;&#37319;&#29992;&#30495;&#23454;&#36317;&#31163;&#22810;&#36793;&#23450;&#20301;&#31639;&#27861;&#23454;&#29616;&#24555;&#36895;&#32780;&#20934;&#30830;&#30340;&#26816;&#27979;&#65292;&#36991;&#20813;&#20102;&#20256;&#32479;&#26041;&#27861;&#20013;&#30340;&#35745;&#31639;&#36127;&#25285;&#21644;&#37327;&#21270;&#35823;&#24046;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#38754;&#37096;&#20998;&#26512;&#39046;&#22495;&#65292;&#20934;&#30830;&#30340;&#26631;&#35760;&#26816;&#27979;&#23545;&#20110;&#21508;&#31181;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#65292;&#21253;&#25324;&#20154;&#33080;&#35782;&#21035;&#21644;&#34920;&#24773;&#20998;&#26512;&#31561;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#28909;&#21147;&#22270;&#25110;&#22352;&#26631;&#22238;&#24402;&#25216;&#26415;&#32463;&#24120;&#38754;&#20020;&#35745;&#31639;&#36127;&#25285;&#21644;&#37327;&#21270;&#35823;&#24046;&#31561;&#25361;&#25112;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; KeyPoint Positioning System&#65288;KeyPosS&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#31361;&#30772;&#24615;&#30340;&#38754;&#37096;&#26631;&#35760;&#26816;&#27979;&#26694;&#26550;&#65292;&#19982;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#12290;KeyPosS&#39318;&#27425;&#37319;&#29992;&#30495;&#23454;&#36317;&#31163;&#22810;&#36793;&#23450;&#20301;&#31639;&#27861;&#65292;&#19968;&#31181;&#26368;&#21021;&#29992;&#20110;GPS&#31995;&#32479;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#19981;&#20381;&#36182;&#20110;&#35745;&#31639;&#23494;&#38598;&#22411;&#22238;&#24402;&#26041;&#27861;&#23454;&#29616;&#24555;&#36895;&#32780;&#20934;&#30830;&#30340;&#38754;&#37096;&#26631;&#35760;&#26816;&#27979;&#12290;&#35813;&#26694;&#26550;&#21033;&#29992;&#23436;&#20840;&#21367;&#31215;&#32593;&#32476;&#39044;&#27979;&#36317;&#31163;&#22270;&#65292;&#35745;&#31639;&#24863;&#20852;&#36259;&#28857;&#65288;POI&#65289;&#19982;&#22810;&#20010;&#38170;&#28857;&#20043;&#38388;&#30340;&#36317;&#31163;&#12290;&#36890;&#36807;&#24039;&#22937;&#22320;&#21033;&#29992;&#36825;&#20123;&#38170;&#28857;&#26469;&#19977;&#35282;&#27979;&#37327;POI&#30340;&#20301;&#32622;&#65292;&#23454;&#29616;&#38754;&#37096;&#26631;&#35760;&#30340;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the realm of facial analysis, accurate landmark detection is crucial for various applications, ranging from face recognition and expression analysis to animation. Conventional heatmap or coordinate regression-based techniques, however, often face challenges in terms of computational burden and quantization errors. To address these issues, we present the KeyPoint Positioning System (KeyPosS), a groundbreaking facial landmark detection framework that stands out from existing methods. For the first time, KeyPosS employs the True-range Multilateration algorithm, a technique originally used in GPS systems, to achieve rapid and precise facial landmark detection without relying on computationally intensive regression approaches. The framework utilizes a fully convolutional network to predict a distance map, which computes the distance between a Point of Interest (POI) and multiple anchor points. These anchor points are ingeniously harnessed to triangulate the POI's position through the Tru
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#31070;&#32463;&#20999;&#21521;&#26680;&#65288;NTK&#65289;&#21644;&#31070;&#32463;&#23849;&#28291;&#65288;NC&#65289;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#35777;&#26126;&#20102;&#22312;&#20855;&#26377;&#22359;&#29366;NTK&#30340;DNN&#20013;&#20250;&#20986;&#29616;NC&#65292;&#24182;&#36890;&#36807;&#22823;&#35268;&#27169;&#23454;&#39564;&#25903;&#25345;&#29702;&#35770;&#30340;&#27491;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.16427</link><description>&lt;p&gt;
&#31070;&#32463;&#65288;&#20999;&#21521;&#26680;&#65289;&#23849;&#28291;
&lt;/p&gt;
&lt;p&gt;
Neural (Tangent Kernel) Collapse. (arXiv:2305.16427v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16427
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#31070;&#32463;&#20999;&#21521;&#26680;&#65288;NTK&#65289;&#21644;&#31070;&#32463;&#23849;&#28291;&#65288;NC&#65289;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#35777;&#26126;&#20102;&#22312;&#20855;&#26377;&#22359;&#29366;NTK&#30340;DNN&#20013;&#20250;&#20986;&#29616;NC&#65292;&#24182;&#36890;&#36807;&#22823;&#35268;&#27169;&#23454;&#39564;&#25903;&#25345;&#29702;&#35770;&#30340;&#27491;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20004;&#20010;&#37325;&#35201;&#30340;&#27010;&#24565;&#65306;&#31070;&#32463;&#20999;&#21521;&#26680;&#65288;NTK&#65289;&#65292;&#23427;&#25429;&#25417;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#35757;&#32451;&#26399;&#38388;&#30340;&#28436;&#21270;&#21644;&#31070;&#32463;&#23849;&#28291;&#65288;NC&#65289;&#29616;&#35937;&#65292;&#23427;&#25351;&#30340;&#26159;&#32463;&#36807;&#33391;&#22909;&#35757;&#32451;&#30340;&#20998;&#31867;DNN&#26368;&#21518;&#19968;&#23618;&#29305;&#24449;&#20013;&#23545;&#31216;&#24615;&#21644;&#32467;&#26500;&#30340;&#20986;&#29616;&#12290;&#25105;&#20204;&#20551;&#35774;&#32463;&#39564;NTK&#19982;&#31867;&#26631;&#31614;&#23545;&#40784;&#24182;&#24418;&#25104;&#22359;&#29366;&#32467;&#26500;&#65292;&#21363;&#21516;&#19968;&#31867;&#21035;&#30340;&#26679;&#26412;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#27604;&#19981;&#21516;&#31867;&#21035;&#30340;&#26679;&#26412;&#26356;&#24378;&#65292;&#22522;&#20110;&#36825;&#20010;&#20551;&#35774;&#65292;&#25105;&#20204;&#25512;&#23548;&#20102;&#20351;&#29992;&#22343;&#26041;&#35823;&#24046;&#65288;MSE&#65289;&#35757;&#32451;&#30340;DNN&#21160;&#24577;&#65292;&#24182;&#23558;&#20854;&#20998;&#35299;&#20026;&#21487;&#35299;&#37322;&#30340;&#38454;&#27573;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#19968;&#31181;&#19981;&#21464;&#37327;&#65292;&#25429;&#25417;&#20102;&#21160;&#24577;&#30340;&#26412;&#36136;&#65292;&#24182;&#29992;&#23427;&#35777;&#26126;&#20102;&#22312;&#20855;&#26377;&#22359;&#29366;NTK&#30340;DNN&#20013;&#20250;&#20986;&#29616;NC&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19977;&#31181;&#24120;&#35265;DNN&#26550;&#26500;&#21644;&#19977;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#22823;&#35268;&#27169;&#25968;&#20540;&#23454;&#39564;&#26469;&#25903;&#25345;&#25105;&#20204;&#30340;&#29702;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work bridges two important concepts: the Neural Tangent Kernel (NTK), which captures the evolution of deep neural networks (DNNs) during training, and the Neural Collapse (NC) phenomenon, which refers to the emergence of symmetry and structure in the last-layer features of well-trained classification DNNs. We adopt the natural assumption that the empirical NTK develops a block structure aligned with the class labels, i.e., samples within the same class have stronger correlations than samples from different classes. Under this assumption, we derive the dynamics of DNNs trained with mean squared (MSE) loss and break them into interpretable phases. Moreover, we identify an invariant that captures the essence of the dynamics, and use it to prove the emergence of NC in DNNs with block-structured NTK. We provide large-scale numerical experiments on three common DNN architectures and three benchmark datasets to support our theory.
&lt;/p&gt;</description></item><item><title>SketchOGD&#25552;&#20986;&#20102;&#19968;&#31181;&#20869;&#23384;&#39640;&#25928;&#30340;&#35299;&#20915;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#37319;&#29992;&#22312;&#32447;&#33609;&#22270;&#31639;&#27861;&#65292;&#23558;&#27169;&#22411;&#26799;&#24230;&#21387;&#32553;&#20026;&#22266;&#23450;&#22823;&#23567;&#30340;&#30697;&#38453;&#65292;&#20174;&#32780;&#25913;&#36827;&#20102;&#29616;&#26377;&#30340;&#31639;&#27861;&#8212;&#8212;&#27491;&#20132;&#26799;&#24230;&#19979;&#38477;&#65288;OGD&#65289;&#12290;</title><link>http://arxiv.org/abs/2305.16424</link><description>&lt;p&gt;
SketchOGD&#65306;&#20869;&#23384;&#39640;&#25928;&#30340;&#25345;&#32493;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
SketchOGD: Memory-Efficient Continual Learning. (arXiv:2305.16424v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16424
&lt;/p&gt;
&lt;p&gt;
SketchOGD&#25552;&#20986;&#20102;&#19968;&#31181;&#20869;&#23384;&#39640;&#25928;&#30340;&#35299;&#20915;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#37319;&#29992;&#22312;&#32447;&#33609;&#22270;&#31639;&#27861;&#65292;&#23558;&#27169;&#22411;&#26799;&#24230;&#21387;&#32553;&#20026;&#22266;&#23450;&#22823;&#23567;&#30340;&#30697;&#38453;&#65292;&#20174;&#32780;&#25913;&#36827;&#20102;&#29616;&#26377;&#30340;&#31639;&#27861;&#8212;&#8212;&#27491;&#20132;&#26799;&#24230;&#19979;&#38477;&#65288;OGD&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#19968;&#31995;&#21015;&#20219;&#21153;&#19978;&#25345;&#32493;&#35757;&#32451;&#26102;&#65292;&#23427;&#20204;&#23481;&#26131;&#24536;&#35760;&#20808;&#21069;&#20219;&#21153;&#19978;&#23398;&#20064;&#21040;&#30340;&#30693;&#35782;&#65292;&#36825;&#31181;&#29616;&#35937;&#31216;&#20026;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#29616;&#26377;&#30340;&#35299;&#20915;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#26041;&#27861;&#24448;&#24448;&#28041;&#21450;&#23384;&#20648;&#36807;&#21435;&#20219;&#21153;&#30340;&#20449;&#24687;&#65292;&#36825;&#24847;&#21619;&#30528;&#20869;&#23384;&#20351;&#29992;&#26159;&#30830;&#23450;&#23454;&#29992;&#24615;&#30340;&#20027;&#35201;&#22240;&#32032;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20869;&#23384;&#39640;&#25928;&#30340;&#35299;&#20915;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#26041;&#27861;&#65292;&#25913;&#36827;&#20102;&#19968;&#31181;&#24050;&#26377;&#30340;&#31639;&#27861;&#8212;&#8212;&#27491;&#20132;&#26799;&#24230;&#19979;&#38477;&#65288;OGD&#65289;&#12290;OGD&#21033;&#29992;&#20808;&#21069;&#27169;&#22411;&#26799;&#24230;&#26469;&#25214;&#21040;&#32500;&#25345;&#20808;&#21069;&#25968;&#25454;&#28857;&#24615;&#33021;&#30340;&#26435;&#37325;&#26356;&#26032;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23384;&#20648;&#20808;&#21069;&#27169;&#22411;&#26799;&#24230;&#30340;&#20869;&#23384;&#25104;&#26412;&#38543;&#31639;&#27861;&#36816;&#34892;&#26102;&#38388;&#22686;&#38271;&#32780;&#22686;&#21152;&#65292;&#22240;&#27492;OGD&#19981;&#36866;&#29992;&#20110;&#20219;&#24847;&#38271;&#26102;&#38388;&#36328;&#24230;&#30340;&#36830;&#32493;&#23398;&#20064;&#12290;&#38024;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;SketchOGD&#12290;SketchOGD&#37319;&#29992;&#22312;&#32447;&#33609;&#22270;&#31639;&#27861;&#65292;&#23558;&#27169;&#22411;&#26799;&#24230;&#21387;&#32553;&#20026;&#22266;&#23450;&#22823;&#23567;&#30340;&#30697;&#38453;&#12290;
&lt;/p&gt;
&lt;p&gt;
When machine learning models are trained continually on a sequence of tasks, they are liable to forget what they learned on previous tasks -- a phenomenon known as catastrophic forgetting. Proposed solutions to catastrophic forgetting tend to involve storing information about past tasks, meaning that memory usage is a chief consideration in determining their practicality. This paper proposes a memory-efficient solution to catastrophic forgetting, improving upon an established algorithm known as orthogonal gradient descent (OGD). OGD utilizes prior model gradients to find weight updates that preserve performance on prior datapoints. However, since the memory cost of storing prior model gradients grows with the runtime of the algorithm, OGD is ill-suited to continual learning over arbitrarily long time horizons. To address this problem, this paper proposes SketchOGD. SketchOGD employs an online sketching algorithm to compress model gradients as they are encountered into a matrix of a fix
&lt;/p&gt;</description></item><item><title>NODDLE&#26159;&#19968;&#31181;&#29992;&#20110;&#38142;&#36335;&#39044;&#27979;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#32467;&#21512;&#20102;node2vec&#25552;&#21462;&#30340;&#29305;&#24449;&#21644;&#22235;&#23618;&#38544;&#34255;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#21033;&#29992;&#33258;&#36866;&#24212;&#23398;&#20064;&#20248;&#21270;&#22120;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.16421</link><description>&lt;p&gt;
NODDLE&#65306;&#22522;&#20110;Node2vec&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#38142;&#36335;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
NODDLE: Node2vec based deep learning model for link prediction. (arXiv:2305.16421v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16421
&lt;/p&gt;
&lt;p&gt;
NODDLE&#26159;&#19968;&#31181;&#29992;&#20110;&#38142;&#36335;&#39044;&#27979;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#32467;&#21512;&#20102;node2vec&#25552;&#21462;&#30340;&#29305;&#24449;&#21644;&#22235;&#23618;&#38544;&#34255;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#21033;&#29992;&#33258;&#36866;&#24212;&#23398;&#20064;&#20248;&#21270;&#22120;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38142;&#36335;&#39044;&#27979;&#26159;&#35745;&#31639;&#22270;&#32593;&#32476;&#20013;&#36793;&#23384;&#22312;&#27010;&#29575;&#30340;&#36807;&#31243;&#12290;&#20256;&#32479;&#26041;&#27861;&#35745;&#31639;&#38745;&#24577;&#32593;&#32476;&#20013;&#32473;&#23450;&#33410;&#28857;&#30340;&#30456;&#20284;&#24230;&#65292;&#32780;&#26368;&#36817;&#30340;&#30740;&#31350;&#20391;&#37325;&#20110;&#35780;&#20272;&#21160;&#24577;&#28436;&#21270;&#30340;&#32593;&#32476;&#12290;&#23613;&#31649;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#21644;&#32593;&#32476;&#34920;&#31034;&#23398;&#20064;&#31639;&#27861;&#65288;&#20363;&#22914;node2vec&#65289;&#22312;&#39044;&#27979;&#20934;&#30830;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#25913;&#36827;&#65292;&#20294;node2vec&#20013;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#26041;&#27861;&#30001;&#20110;&#32570;&#20047;&#20808;&#21069;&#30340;&#32593;&#32476;&#20449;&#24687;&#32780;&#26131;&#38519;&#20837;&#24179;&#24248;&#30340;&#26412;&#22320;&#26368;&#20248;&#20540;&#65292;&#23548;&#33268;&#26080;&#27861;&#25429;&#25417;&#32593;&#32476;&#30340;&#20840;&#23616;&#32467;&#26500;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;NODDLE&#65288;NOde2vec&#21644;Deep Learning mEthod&#30340;&#38598;&#25104;&#65289;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#34701;&#21512;&#20102;node2vec&#25552;&#21462;&#30340;&#29305;&#24449;&#65292;&#24182;&#23558;&#20854;&#39304;&#36865;&#21040;&#22235;&#23618;&#38544;&#34255;&#31070;&#32463;&#32593;&#32476;&#20013;&#12290;NODDLE&#21033;&#29992;&#33258;&#36866;&#24212;&#23398;&#20064;&#20248;&#21270;&#22120;&#65288;&#22914;Adam&#65292;Adamax&#65292;Adadelta&#21644;Adagrad&#65289;&#26469;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Computing the probability of an edge's existence in a graph network is known as link prediction. While traditional methods calculate the similarity between two given nodes in a static network, recent research has focused on evaluating networks that evolve dynamically. Although deep learning techniques and network representation learning algorithms, such as node2vec, show remarkable improvements in prediction accuracy, the Stochastic Gradient Descent (SGD) method of node2vec tends to fall into a mediocre local optimum value due to a shortage of prior network information, resulting in failure to capture the global structure of the network. To tackle this problem, we propose NODDLE (integration of NOde2vec anD Deep Learning mEthod), a deep learning model which incorporates the features extracted by node2vec and feeds them into a four layer hidden neural network. NODDLE takes advantage of adaptive learning optimizers such as Adam, Adamax, Adadelta, and Adagrad to improve the performance of
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GrowSP&#30340;&#26080;&#30417;&#30563;&#26041;&#27861;&#26469;&#36827;&#34892;3D&#22330;&#26223;&#20013;&#27599;&#20010;&#28857;&#22797;&#26434;&#35821;&#20041;&#31867;&#21035;&#30340;&#35782;&#21035;&#21644;&#20998;&#21106;&#65292;&#26041;&#27861;&#36890;&#36807;&#36880;&#27493;&#22686;&#38271;&#36229;&#32423;&#28857;&#30340;&#22823;&#23567;&#26469;&#21457;&#29616;3D&#35821;&#20041;&#20803;&#32032;&#65292;&#24182;&#20248;&#20110;&#25152;&#26377;&#26080;&#30417;&#30563;&#22522;&#32447;&#12290;</title><link>http://arxiv.org/abs/2305.16404</link><description>&lt;p&gt;
GrowSP&#65306;3D&#28857;&#20113;&#26080;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
GrowSP: Unsupervised Semantic Segmentation of 3D Point Clouds. (arXiv:2305.16404v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16404
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GrowSP&#30340;&#26080;&#30417;&#30563;&#26041;&#27861;&#26469;&#36827;&#34892;3D&#22330;&#26223;&#20013;&#27599;&#20010;&#28857;&#22797;&#26434;&#35821;&#20041;&#31867;&#21035;&#30340;&#35782;&#21035;&#21644;&#20998;&#21106;&#65292;&#26041;&#27861;&#36890;&#36807;&#36880;&#27493;&#22686;&#38271;&#36229;&#32423;&#28857;&#30340;&#22823;&#23567;&#26469;&#21457;&#29616;3D&#35821;&#20041;&#20803;&#32032;&#65292;&#24182;&#20248;&#20110;&#25152;&#26377;&#26080;&#30417;&#30563;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20174;&#21407;&#22987;&#28857;&#20113;&#20013;&#36827;&#34892;3D&#35821;&#20041;&#20998;&#21106;&#30340;&#38382;&#39064;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#22823;&#37327;&#30340;&#20154;&#24037;&#27880;&#37322;&#26469;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#19981;&#21516;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#23436;&#20840;&#26080;&#30417;&#30563;&#30340;&#26041;&#27861;GrowSP&#65292;&#25104;&#21151;&#22320;&#20026;3D&#22330;&#26223;&#20013;&#30340;&#27599;&#20010;&#28857;&#35782;&#21035;&#20986;&#22797;&#26434;&#30340;&#35821;&#20041;&#31867;&#21035;&#65292;&#26080;&#38656;&#20219;&#20309;&#24418;&#24335;&#30340;&#20154;&#24037;&#26631;&#31614;&#25110;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#20851;&#38190;&#26159;&#36890;&#36807;&#36229;&#32423;&#28857;&#30340;&#36880;&#27493;&#22686;&#38271;&#26469;&#21457;&#29616;3D&#35821;&#20041;&#20803;&#32032;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30001;&#19977;&#20010;&#20027;&#35201;&#32452;&#25104;&#37096;&#20998;&#32452;&#25104;&#65306;1&#65289;&#29305;&#24449;&#25552;&#21462;&#22120;&#20174;&#36755;&#20837;&#28857;&#20113;&#20013;&#23398;&#20064;&#27599;&#20010;&#28857;&#30340;&#29305;&#24449;&#65307;2&#65289;&#36229;&#32423;&#28857;&#26500;&#36896;&#22120;&#36880;&#27493;&#22686;&#21152;&#36229;&#32423;&#28857;&#30340;&#22823;&#23567;&#65307;3&#65289;&#35821;&#20041;&#21407;&#22987;&#32858;&#31867;&#27169;&#22359;&#23558;&#36229;&#32423;&#28857;&#20998;&#32452;&#25104;&#35821;&#20041;&#20803;&#32032;&#20197;&#23454;&#29616;&#26368;&#32456;&#30340;&#35821;&#20041;&#20998;&#21106;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#35780;&#20272;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#25152;&#26377;&#26080;&#30417;&#30563;&#22522;&#32447;&#65292;&#24182;&#25509;&#36817;&#32463;&#20856;&#23436;&#20840;&#30417;&#30563;&#30340;PointNet&#27169;&#22411;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the problem of 3D semantic segmentation from raw point clouds. Unlike existing methods which primarily rely on a large amount of human annotations for training neural networks, we propose the first purely unsupervised method, called GrowSP, to successfully identify complex semantic classes for every point in 3D scenes, without needing any type of human labels or pretrained models. The key to our approach is to discover 3D semantic elements via progressive growing of superpoints. Our method consists of three major components, 1) the feature extractor to learn per-point features from input point clouds, 2) the superpoint constructor to progressively grow the sizes of superpoints, and 3) the semantic primitive clustering module to group superpoints into semantic elements for the final semantic segmentation. We extensively evaluate our method on multiple datasets, demonstrating superior performance over all unsupervised baselines and approaching the classic fully-supervised PointN
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#25193;&#25955;-&#35821;&#35328;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#36716;&#25442;&#21644;&#35780;&#20272;&#65292;&#20171;&#32461;&#20102;&#29983;&#25104;-&#37492;&#21035;&#35780;&#20272;&#22522;&#20934;(GDBench)&#22522;&#20110;7&#20010;&#35270;&#35273;&#35821;&#35328;&#22797;&#26434;&#20219;&#21153;&#65292;&#24182;&#21457;&#29616;&#36716;&#25442;&#21518;&#30340;&#27169;&#22411;&#22312;&#32452;&#21512;&#24615;&#20219;&#21153;&#26041;&#38754;&#30340;&#34920;&#29616;&#20248;&#20110;CLIP&#65292;&#36890;&#36807;&#24494;&#35843;&#21487;&#25552;&#39640;&#20854;&#32452;&#21512;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.16397</link><description>&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#26159;&#21542;&#26159;&#35270;&#35273;&#35821;&#35328;&#25512;&#29702;&#22120;&#65311;
&lt;/p&gt;
&lt;p&gt;
Are Diffusion Models Vision-And-Language Reasoners?. (arXiv:2305.16397v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16397
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#25193;&#25955;-&#35821;&#35328;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#36716;&#25442;&#21644;&#35780;&#20272;&#65292;&#20171;&#32461;&#20102;&#29983;&#25104;-&#37492;&#21035;&#35780;&#20272;&#22522;&#20934;(GDBench)&#22522;&#20110;7&#20010;&#35270;&#35273;&#35821;&#35328;&#22797;&#26434;&#20219;&#21153;&#65292;&#24182;&#21457;&#29616;&#36716;&#25442;&#21518;&#30340;&#27169;&#22411;&#22312;&#32452;&#21512;&#24615;&#20219;&#21153;&#26041;&#38754;&#30340;&#34920;&#29616;&#20248;&#20110;CLIP&#65292;&#36890;&#36807;&#24494;&#35843;&#21487;&#25552;&#39640;&#20854;&#32452;&#21512;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#65292;&#20351;&#29992;&#21435;&#22122;&#25193;&#25955;&#36807;&#31243;&#30340;&#25991;&#26412;-&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#24050;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#23450;&#24615;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#19982;&#37492;&#21035;&#24335;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#19981;&#21516;&#65292;&#23558;&#22522;&#20110;&#25193;&#25955;&#30340;&#29983;&#25104;&#27169;&#22411;&#32622;&#20110;&#33258;&#21160;&#32454;&#31890;&#24230;&#23450;&#37327;&#35780;&#20272;&#39640;&#32423;&#29616;&#35937;&#65288;&#22914;&#32452;&#21512;&#24615;&#65289;&#30340;&#20219;&#21153;&#20013;&#26159;&#19968;&#39033;&#38750;&#24120;&#26840;&#25163;&#30340;&#20219;&#21153;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24320;&#23637;&#20102;&#20004;&#39033;&#21019;&#26032;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#31181;&#31216;&#20026;DiffusionITM&#30340;&#26032;&#26041;&#27861;&#23558;&#22522;&#20110;&#25193;&#25955;&#30340;&#27169;&#22411;&#65288;&#22312;&#25105;&#20204;&#30340;&#24773;&#20917;&#19979;&#65292;&#26159;&#31283;&#23450;&#25193;&#25955;&#65289;&#36716;&#25442;&#20026;&#20219;&#20309;&#22270;&#20687;&#25991;&#26412;&#21305;&#37197;(ITM)&#20219;&#21153;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;7&#20010;&#22797;&#26434;&#30340;&#35270;&#35273;&#35821;&#35328;&#20219;&#21153;&#12289;&#20559;&#24046;&#35780;&#20272;&#21644;&#35814;&#32454;&#20998;&#26512;&#30340;&#29983;&#25104;-&#37492;&#21035;&#35780;&#20272;&#22522;&#20934;(GDBench)&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;Stable Diffusion + DiffusionITM&#22312;&#35768;&#22810;&#20219;&#21153;&#19978;&#20855;&#26377;&#31454;&#20105;&#21147;&#65292;&#24182;&#22312;&#32452;&#21512;&#24615;&#20219;&#21153;&#65288;&#22914;CLEVR&#21644;Winoground&#31561;&#65289;&#19978;&#20248;&#20110;CLIP&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;MS-COCO&#19978;&#24494;&#35843;&#20445;&#25345;&#22270;&#20687;&#29305;&#24449;&#30340;&#36716;&#31227;&#35774;&#32622;&#36827;&#19968;&#27493;&#25552;&#39640;&#20854;&#32452;&#21512;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-conditioned image generation models have recently shown immense qualitative success using denoising diffusion processes. However, unlike discriminative vision-and-language models, it is a non-trivial task to subject these diffusion-based generative models to automatic fine-grained quantitative evaluation of high-level phenomena such as compositionality. Towards this goal, we perform two innovations. First, we transform diffusion-based models (in our case, Stable Diffusion) for any image-text matching (ITM) task using a novel method called DiffusionITM. Second, we introduce the Generative-Discriminative Evaluation Benchmark (GDBench) benchmark with 7 complex vision-and-language tasks, bias evaluation and detailed analysis. We find that Stable Diffusion + DiffusionITM is competitive on many tasks and outperforms CLIP on compositional tasks like like CLEVR and Winoground. We further boost its compositional performance with a transfer setup by fine-tuning on MS-COCO while retaining ge
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;1&#23618;Transformer&#22312;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#20219;&#21153;&#20013;&#30340;SGD&#35757;&#32451;&#21160;&#24577;&#65292;&#35777;&#26126;&#20102;&#33258;&#25105;&#20851;&#27880;&#23618;&#20805;&#24403;&#20102;&#8220;&#21306;&#20998;&#24615;&#25195;&#25551;&#31639;&#27861;&#8221;&#65292;&#20174;&#32780;&#36880;&#27493;&#20851;&#27880;&#21040;&#30456;&#20851;&#26631;&#35760;&#24182;&#25490;&#38500;&#19981;&#30456;&#20851;&#30340;&#26631;&#35760;&#65292;&#24635;&#32467;&#30456;&#20851;&#20449;&#24687;&#22312;&#32534;&#30721;&#34920;&#31034;&#20013;&#12290;&#21516;&#26102;&#30740;&#31350;&#20102;&#26631;&#35760;&#39057;&#29575;&#12289;&#19978;&#19979;&#25991;&#21644;&#21021;&#22987;&#21270;&#33258;&#25105;&#20851;&#27880;&#23618;&#31561;&#23545;Transformer&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2305.16380</link><description>&lt;p&gt;
&#25195;&#25551;&#19982;&#25293;&#29031;&#65306;&#29702;&#35299;1&#23618;Transformer&#20013;&#30340;&#35757;&#32451;&#21160;&#24577;&#21644;&#26631;&#35760;&#32452;&#25104;
&lt;/p&gt;
&lt;p&gt;
Scan and Snap: Understanding Training Dynamics and Token Composition in 1-layer Transformer. (arXiv:2305.16380v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16380
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;1&#23618;Transformer&#22312;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#20219;&#21153;&#20013;&#30340;SGD&#35757;&#32451;&#21160;&#24577;&#65292;&#35777;&#26126;&#20102;&#33258;&#25105;&#20851;&#27880;&#23618;&#20805;&#24403;&#20102;&#8220;&#21306;&#20998;&#24615;&#25195;&#25551;&#31639;&#27861;&#8221;&#65292;&#20174;&#32780;&#36880;&#27493;&#20851;&#27880;&#21040;&#30456;&#20851;&#26631;&#35760;&#24182;&#25490;&#38500;&#19981;&#30456;&#20851;&#30340;&#26631;&#35760;&#65292;&#24635;&#32467;&#30456;&#20851;&#20449;&#24687;&#22312;&#32534;&#30721;&#34920;&#31034;&#20013;&#12290;&#21516;&#26102;&#30740;&#31350;&#20102;&#26631;&#35760;&#39057;&#29575;&#12289;&#19978;&#19979;&#25991;&#21644;&#21021;&#22987;&#21270;&#33258;&#25105;&#20851;&#27880;&#23618;&#31561;&#23545;Transformer&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#26550;&#26500;&#22312;&#22810;&#20010;&#30740;&#31350;&#39046;&#22495;&#34920;&#29616;&#20986;&#20102;&#24778;&#20154;&#30340;&#24615;&#33021;&#65292;&#24182;&#25104;&#20026;&#35768;&#22810;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#22522;&#30784;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#23545;&#20854;&#22914;&#20309;&#24037;&#20316;&#30340;&#29702;&#35299;&#20173;&#28982;&#26377;&#38480;&#12290;&#29305;&#21035;&#26159;&#65292;&#36890;&#36807;&#31616;&#21333;&#30340;&#39044;&#27979;&#24615;&#25439;&#22833;&#65292;&#34920;&#31034;&#22914;&#20309;&#20174;&#26799;&#24230;&#35757;&#32451;&#21160;&#24577;&#20013;&#20986;&#29616;&#20173;&#28982;&#26159;&#19968;&#20010;&#35868;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#38024;&#23545;&#20855;&#26377;&#19968;&#20010;&#33258;&#25105;&#20851;&#27880;&#23618;&#21644;&#19968;&#20010;&#35299;&#30721;&#22120;&#23618;&#30340;1&#23618;Transformer&#65292;&#25105;&#20204;&#20197;&#25968;&#23398;&#20005;&#35880;&#30340;&#26041;&#24335;&#20998;&#26512;&#20854;&#22312;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#20219;&#21153;&#20013;&#30340;SGD&#35757;&#32451;&#21160;&#24577;&#12290;&#25105;&#20204;&#25171;&#24320;&#20102;&#33258;&#25105;&#20851;&#27880;&#23618;&#32452;&#21512;&#36755;&#20837;&#26631;&#35760;&#30340;&#21160;&#24577;&#36807;&#31243;&#30340;&#40657;&#30418;&#23376;&#65292;&#24182;&#25581;&#31034;&#20102;&#24213;&#23618;&#24402;&#32435;&#20559;&#24046;&#30340;&#26412;&#36136;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22312;&#27809;&#26377;&#20301;&#32622;&#32534;&#30721;&#12289;&#38271;&#36755;&#20837;&#24207;&#21015;&#21644;&#35299;&#30721;&#22120;&#23618;&#23398;&#20064;&#36895;&#24230;&#24555;&#20110;&#33258;&#25105;&#20851;&#27880;&#23618;&#30340;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#33258;&#25105;&#20851;&#27880;&#23618;&#20805;&#24403;&#20102;&#8220;&#21306;&#20998;&#24615;&#25195;&#25551;&#31639;&#27861;&#8221;&#65306;&#20174;&#22343;&#21248;&#27880;&#24847;&#21147;&#24320;&#22987;&#65292;&#23427;&#36880;&#28176;&#20851;&#27880;&#21040;&#30456;&#20851;&#26631;&#35760;&#65292;&#25490;&#38500;&#19981;&#30456;&#20851;&#30340;&#26631;&#35760;&#65292;&#30452;&#21040;&#25152;&#26377;&#30456;&#20851;&#20449;&#24687;&#34987;&#25195;&#25551;&#24182;&#24635;&#32467;&#22312;&#32534;&#30721;&#34920;&#31034;&#20013;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#36824;&#26174;&#31034;&#20102;&#26631;&#35760;&#39057;&#29575;&#21644;&#19978;&#19979;&#25991;&#22914;&#20309;&#24433;&#21709;&#27880;&#24847;&#26435;&#37325;&#65292;&#20197;&#21450;&#33258;&#25105;&#20851;&#27880;&#23618;&#21021;&#22987;&#21270;&#22914;&#20309;&#24433;&#21709;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer architecture has shown impressive performance in multiple research domains and has become the backbone of many neural network models. However, there is limited understanding on how it works. In particular, with a simple predictive loss, how the representation emerges from the gradient \emph{training dynamics} remains a mystery. In this paper, for 1-layer transformer with one self-attention layer plus one decoder layer, we analyze its SGD training dynamics for the task of next token prediction in a mathematically rigorous manner. We open the black box of the dynamic process of how the self-attention layer combines input tokens, and reveal the nature of underlying inductive bias. More specifically, with the assumption (a) no positional encoding, (b) long input sequence, and (c) the decoder layer learns faster than the self-attention layer, we prove that self-attention acts as a \emph{discriminative scanning algorithm}: starting from uniform attention, it gradually attends mor
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#23545;&#20110;&#25968;&#25454;&#22686;&#24378;&#22312;&#35270;&#35273;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#31354;&#38388;&#22810;&#26679;&#24615;&#21644;&#36731;&#24494;&#30340;&#22256;&#38590;&#24230;&#19981;&#21487;&#25110;&#32570;&#12290;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;DA&#25805;&#20316;&#8212;&#8212;Rand PR&#65292;&#23427;&#25552;&#20379;&#20102;&#20016;&#23500;&#30340;&#31354;&#38388;&#22810;&#26679;&#24615;&#21644;&#26368;&#23567;&#30340;&#22256;&#38590;&#24230;&#65292;&#24050;&#32463;&#22312;&#22810;&#31181;&#25968;&#25454;&#19978;&#24471;&#21040;&#20102;&#26377;&#25928;&#24615;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2305.16379</link><description>&lt;p&gt;
&#26377;&#25928;&#22686;&#24378;&#35270;&#35273;&#24378;&#21270;&#23398;&#20064;&#30340;&#26679;&#26412;&#21033;&#29992;&#29575;&#65306;&#20197;&#23569;&#23398;&#26356;&#22909;
&lt;/p&gt;
&lt;p&gt;
Learning Better with Less: Effective Augmentation for Sample-Efficient Visual Reinforcement Learning. (arXiv:2305.16379v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16379
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#23545;&#20110;&#25968;&#25454;&#22686;&#24378;&#22312;&#35270;&#35273;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#31354;&#38388;&#22810;&#26679;&#24615;&#21644;&#36731;&#24494;&#30340;&#22256;&#38590;&#24230;&#19981;&#21487;&#25110;&#32570;&#12290;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;DA&#25805;&#20316;&#8212;&#8212;Rand PR&#65292;&#23427;&#25552;&#20379;&#20102;&#20016;&#23500;&#30340;&#31354;&#38388;&#22810;&#26679;&#24615;&#21644;&#26368;&#23567;&#30340;&#22256;&#38590;&#24230;&#65292;&#24050;&#32463;&#22312;&#22810;&#31181;&#25968;&#25454;&#19978;&#24471;&#21040;&#20102;&#26377;&#25928;&#24615;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#22686;&#24378;&#65288;DA&#65289;&#26159;&#22686;&#24378;&#35270;&#35273;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#31639;&#27861;&#30340;&#26679;&#26412;&#25928;&#29575;&#30340;&#20851;&#38190;&#25216;&#26415;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#20165;&#20351;&#29992;&#31616;&#21333;&#30340;&#35266;&#23519;&#21464;&#25442;&#23601;&#21487;&#20197;&#22312;&#19981;&#36827;&#34892;&#39069;&#22806;&#36741;&#21161;&#34920;&#31034;&#20219;&#21153;&#25110;&#39044;&#35757;&#32451;&#32534;&#30721;&#22120;&#30340;&#24773;&#20917;&#19979;&#33719;&#24471;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#20173;&#28982;&#19981;&#28165;&#26970;DA&#30340;&#21738;&#20123;&#23646;&#24615;&#26159;&#23454;&#29616;&#26679;&#26412;&#25928;&#29575;&#35270;&#35273;RL&#30340;&#26377;&#25928;&#24615;&#30340;&#21407;&#22240;&#12290;&#20026;&#20102;&#35843;&#26597;&#36825;&#20010;&#38382;&#39064;&#24182;&#36827;&#19968;&#27493;&#25506;&#32034;DA&#30340;&#28508;&#21147;&#65292;&#26412;&#25991;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23454;&#39564;&#65292;&#35780;&#20272;&#20102;DA&#23646;&#24615;&#23545;&#20854;&#26377;&#25928;&#24615;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20379;&#20197;&#19979;&#35265;&#35299;&#21644;&#25913;&#36827;&#65306;&#65288;1&#65289;&#23545;&#20110;&#21333;&#20010;DA&#25805;&#20316;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#20805;&#36275;&#30340;&#31354;&#38388;&#22810;&#26679;&#24615;&#21644;&#36731;&#24494;&#30340;&#22256;&#38590;&#24230;&#37117;&#26159;&#19981;&#21487;&#32570;&#23569;&#30340;&#12290;&#22522;&#20110;&#36825;&#19968;&#21457;&#29616;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;DA&#25805;&#20316;&#8212;&#8212;&#38543;&#26426;PadResize&#65288;Rand PR&#65289;&#65292;&#23427;&#25552;&#20379;&#20102;&#20016;&#23500;&#30340;&#31354;&#38388;&#22810;&#26679;&#24615;&#21644;&#26368;&#23567;&#30340;&#22256;&#38590;&#24230;&#12290;&#65288;2&#65289;&#23545;&#20110;&#22810;&#31867;&#22411;&#30340;DA&#34701;&#21512;&#26041;&#26696;&#65292;&#22686;&#21152;&#30340;DA&#22256;&#38590;&#24230;&#21644;&#19981;&#31283;&#23450;&#30340;&#25968;&#25454;&#20998;&#24067;
&lt;/p&gt;
&lt;p&gt;
Data augmentation (DA) is a crucial technique for enhancing the sample efficiency of visual reinforcement learning (RL) algorithms. Notably, employing simple observation transformations alone can yield outstanding performance without extra auxiliary representation tasks or pre-trained encoders. However, it remains unclear which attributes of DA account for its effectiveness in achieving sample-efficient visual RL. To investigate this issue and further explore the potential of DA, this work conducts comprehensive experiments to assess the impact of DA's attributes on its efficacy and provides the following insights and improvements: (1) For individual DA operations, we reveal that both ample spatial diversity and slight hardness are indispensable. Building on this finding, we introduce Random PadResize (Rand PR), a new DA operation that offers abundant spatial diversity with minimal hardness. (2) For multi-type DA fusion schemes, the increased DA hardness and unstable data distribution 
&lt;/p&gt;</description></item><item><title>Sim-Suction &#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29289;&#20307;&#24863;&#30693;&#21560;&#30424;&#25235;&#21462;&#31574;&#30053;&#65292;&#20351;&#29992;&#22522;&#20110;&#21512;&#25104;&#25968;&#25454;&#38598;&#30340;&#26041;&#27861;&#35299;&#20915;&#20102;&#22312;&#26434;&#20081;&#29615;&#22659;&#19979;&#24863;&#20852;&#36259;&#30340;&#29289;&#20307;&#19982;&#21608;&#22260;&#29615;&#22659;&#20043;&#38388;&#20851;&#31995;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.16378</link><description>&lt;p&gt;
Sim-Suction: &#20351;&#29992;&#21512;&#25104;&#22522;&#20934;&#23398;&#20064;&#22312;&#26434;&#20081;&#29615;&#22659;&#20013;&#30340;&#21560;&#30424;&#25235;&#21462;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Sim-Suction: Learning a Suction Grasp Policy for Cluttered Environments Using a Synthetic Benchmark. (arXiv:2305.16378v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16378
&lt;/p&gt;
&lt;p&gt;
Sim-Suction &#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29289;&#20307;&#24863;&#30693;&#21560;&#30424;&#25235;&#21462;&#31574;&#30053;&#65292;&#20351;&#29992;&#22522;&#20110;&#21512;&#25104;&#25968;&#25454;&#38598;&#30340;&#26041;&#27861;&#35299;&#20915;&#20102;&#22312;&#26434;&#20081;&#29615;&#22659;&#19979;&#24863;&#20852;&#36259;&#30340;&#29289;&#20307;&#19982;&#21608;&#22260;&#29615;&#22659;&#20043;&#38388;&#20851;&#31995;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102; Sim-Suction&#65292;&#19968;&#31181;&#38024;&#23545;&#31227;&#21160;&#25805;&#20316;&#24179;&#21488;&#30340;&#12289;&#36866;&#29992;&#20110;&#20855;&#26377;&#21160;&#24577;&#25668;&#20687;&#26426;&#35270;&#35282;&#30340;&#12289;&#29992;&#20110;&#22312;&#26434;&#20081;&#29615;&#22659;&#20013;&#25342;&#21462;&#26410;&#30693;&#29289;&#20307;&#30340;&#40065;&#26834;&#24615;&#29289;&#20307;&#24863;&#30693;&#21560;&#30424;&#25235;&#21462;&#31574;&#30053;&#12290;&#36890;&#24120;&#37319;&#29992;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#23454;&#29616;&#21560;&#30424;&#25235;&#21462;&#31574;&#30053;&#65292;&#38656;&#35201;&#22823;&#35268;&#27169;&#12289;&#31934;&#30830;&#27880;&#37322;&#30340;&#21560;&#30424;&#25235;&#21462;&#25968;&#25454;&#38598;&#12290;&#28982;&#32780;&#65292;&#22312;&#26434;&#20081;&#29615;&#22659;&#20013;&#29983;&#25104;&#21560;&#30424;&#25235;&#21462;&#25968;&#25454;&#38598;&#20173;&#28982;&#32570;&#20047;&#30740;&#31350;&#65292;&#23384;&#22312;&#30528;&#20851;&#20110;&#24863;&#20852;&#36259;&#30340;&#29289;&#20307;&#19982;&#20854;&#21608;&#22260;&#29615;&#22659;&#20043;&#38388;&#20851;&#31995;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20934;&#21512;&#25104;&#25968;&#25454;&#38598; Sim-Suction-Dataset&#65292;&#21253;&#25324; 500 &#20010;&#26434;&#20081;&#29615;&#22659;&#21644; 320 &#19975;&#20010;&#27880;&#37322;&#30340;&#21560;&#30424;&#25235;&#21462;&#23039;&#24577;&#12290;&#39640;&#25928;&#30340; Sim-Suction-Dataset &#29983;&#25104;&#36807;&#31243;&#36890;&#36807;&#23558;&#20998;&#26512;&#27169;&#22411;&#19982;&#21160;&#24577;&#29289;&#29702;&#27169;&#25311;&#30456;&#32467;&#21512;&#26469;&#21019;&#24314;&#24555;&#36895;&#32780;&#20934;&#30830;&#30340;&#21560;&#30424;&#25235;&#21462;&#23039;&#24577;&#27880;&#37322;&#65292;&#25552;&#20379;&#20102;&#26032;&#30340;&#35265;&#35299;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102; Sim-Suction-Pointnet&#65292;&#36890;&#36807;&#23398;&#20064;&#28857;&#26469;&#29983;&#25104;&#40065;&#26834;&#30340; 6D &#21560;&#30424;&#25235;&#21462;&#23039;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents Sim-Suction, a robust object-aware suction grasp policy for mobile manipulation platforms with dynamic camera viewpoints, designed to pick up unknown objects from cluttered environments. Suction grasp policies typically employ data-driven approaches, necessitating large-scale, accurately-annotated suction grasp datasets. However, the generation of suction grasp datasets in cluttered environments remains underexplored, leaving uncertainties about the relationship between the object of interest and its surroundings. To address this, we propose a benchmark synthetic dataset, Sim-Suction-Dataset, comprising 500 cluttered environments with 3.2 million annotated suction grasp poses. The efficient Sim-Suction-Dataset generation process provides novel insights by combining analytical models with dynamic physical simulations to create fast and accurate suction grasp pose annotations. We introduce Sim-Suction-Pointnet to generate robust 6D suction grasp poses by learning poin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#25968;&#25454;&#25299;&#25169;&#30456;&#20851;&#30340;&#31070;&#32463;&#32593;&#32476;&#23485;&#24230;&#19978;&#30028;&#65292;&#24182;&#36890;&#36807;&#25299;&#25169;&#26041;&#27861;&#35777;&#26126;&#20102;&#19977;&#23618;ReLU&#32593;&#32476;&#30340;&#26222;&#36866;&#36924;&#36817;&#24615;&#36136;&#12290;</title><link>http://arxiv.org/abs/2305.16375</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#23485;&#24230;&#19982;&#25968;&#25454;&#25299;&#25169;&#29305;&#24449;&#30456;&#20851;&#30340;&#19978;&#30028;&#25506;&#31350;
&lt;/p&gt;
&lt;p&gt;
Data Topology-Dependent Upper Bounds of Neural Network Widths. (arXiv:2305.16375v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16375
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#25968;&#25454;&#25299;&#25169;&#30456;&#20851;&#30340;&#31070;&#32463;&#32593;&#32476;&#23485;&#24230;&#19978;&#30028;&#65292;&#24182;&#36890;&#36807;&#25299;&#25169;&#26041;&#27861;&#35777;&#26126;&#20102;&#19977;&#23618;ReLU&#32593;&#32476;&#30340;&#26222;&#36866;&#36924;&#36817;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26222;&#36866;&#36924;&#36817;&#24615;&#36136;&#19982;&#25968;&#25454;&#25299;&#25169;&#29305;&#24449;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#24341;&#20837;&#20102;&#25968;&#25454;&#25299;&#25169;&#30456;&#20851;&#30340;&#32593;&#32476;&#23485;&#24230;&#19978;&#30028;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#35777;&#26126;&#20102;&#19968;&#20010;&#19977;&#23618;&#31070;&#32463;&#32593;&#32476;&#65292;&#24212;&#29992;ReLU&#28608;&#27963;&#20989;&#25968;&#21644;&#26368;&#22823;&#27744;&#21270;&#65292;&#21487;&#20197;&#35774;&#35745;&#26469;&#36924;&#36817;&#19968;&#20010;&#22312;&#32039;&#20945;&#20984;&#22810;&#38754;&#20307;&#20869;&#23553;&#35013;&#30340;&#25351;&#31034;&#20989;&#25968;&#12290;&#28982;&#21518;&#25105;&#20204;&#23558;&#20854;&#25193;&#23637;&#21040;&#19968;&#20010;&#21333;&#32431;&#22797;&#21512;&#20307;&#65292;&#22522;&#20110;&#20854;&#25299;&#25169;&#32467;&#26500;&#25512;&#23548;&#23485;&#24230;&#19978;&#30028;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#36890;&#36807;&#36873;&#25321;&#25299;&#25169;&#31354;&#38388;&#30340;Betti&#25968;&#35745;&#31639;&#19978;&#30028;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#25299;&#25169;&#26041;&#27861;&#35777;&#26126;&#20102;&#19977;&#23618;ReLU&#32593;&#32476;&#30340;&#26222;&#36866;&#36924;&#36817;&#24615;&#36136;&#12290;&#25105;&#20204;&#36824;&#39564;&#35777;&#20102;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#25910;&#25947;&#20110;&#26412;&#30740;&#31350;&#25552;&#20986;&#30340;&#32593;&#32476;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates the relationship between the universal approximation property of deep neural networks and topological characteristics of datasets. Our primary contribution is to introduce data topology-dependent upper bounds on the network width. Specifically, we first show that a three-layer neural network, applying a ReLU activation function and max pooling, can be designed to approximate an indicator function over a compact set, one that is encompassed by a tight convex polytope. This is then extended to a simplicial complex, deriving width upper bounds based on its topological structure. Further, we calculate upper bounds in relation to the Betti numbers of select topological spaces. Finally, we prove the universal approximation property of three-layer ReLU networks using our topological approach. We also verify that gradient descent converges to the network structure proposed in our study.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;DeepGate2, &#19968;&#20010;&#26032;&#30340;&#21151;&#33021;&#24863;&#30693;&#23398;&#20064;&#26694;&#26550;&#65292;&#20854;&#36890;&#36807;&#21033;&#29992;&#25104;&#23545;&#30495;&#20540;&#34920;&#24046;&#24322;&#20316;&#20026;&#35757;&#32451;&#30417;&#30563;&#65292;&#26126;&#30830;&#32771;&#34385;&#30005;&#36335;&#21151;&#33021;&#65292;&#26469;&#25552;&#39640;&#30005;&#36335;&#34920;&#31034;&#23398;&#20064;&#30340;&#23398;&#20064;&#25928;&#26524;&#21644;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2305.16373</link><description>&lt;p&gt;
DeepGate2: &#21151;&#33021;&#24863;&#30693;&#30340;&#30005;&#36335;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
DeepGate2: Functionality-Aware Circuit Representation Learning. (arXiv:2305.16373v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16373
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;DeepGate2, &#19968;&#20010;&#26032;&#30340;&#21151;&#33021;&#24863;&#30693;&#23398;&#20064;&#26694;&#26550;&#65292;&#20854;&#36890;&#36807;&#21033;&#29992;&#25104;&#23545;&#30495;&#20540;&#34920;&#24046;&#24322;&#20316;&#20026;&#35757;&#32451;&#30417;&#30563;&#65292;&#26126;&#30830;&#32771;&#34385;&#30005;&#36335;&#21151;&#33021;&#65292;&#26469;&#25552;&#39640;&#30005;&#36335;&#34920;&#31034;&#23398;&#20064;&#30340;&#23398;&#20064;&#25928;&#26524;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#36335;&#34920;&#31034;&#23398;&#20064;&#26088;&#22312;&#33719;&#24471;&#30005;&#36335;&#20803;&#20214;&#30340;&#31070;&#32463;&#34920;&#31034;&#65292;&#24182;&#24050;&#25104;&#20026;&#21487;&#20197;&#24212;&#29992;&#20110;&#21508;&#31181;EDA&#21644;&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#30340;&#26377;&#21069;&#36884;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#29616;&#26377;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20363;&#22914;DeepGate&#65292;&#21487;&#20197;&#23884;&#20837;&#30005;&#36335;&#32467;&#26500;&#20449;&#24687;&#21644;&#21151;&#33021;&#34892;&#20026;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#33021;&#21147;&#21463;&#21040;&#24369;&#30417;&#30563;&#25110;&#38169;&#35823;&#30340;&#27169;&#22411;&#35774;&#35745;&#30340;&#38480;&#21046;&#65292;&#23548;&#33268;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#20196;&#20154;&#19981;&#28385;&#24847;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;DeepGate2&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#21151;&#33021;&#24863;&#30693;&#23398;&#20064;&#26694;&#26550;&#65292;&#20854;&#22312;&#23398;&#20064;&#25928;&#26524;&#21644;&#25928;&#29575;&#26041;&#38754;&#26174;&#30528;&#20248;&#20110;&#21407;DeepGate&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#20351;&#29992;&#26679;&#26412;&#36923;&#36753;&#38376;&#20043;&#38388;&#30340;&#25104;&#23545;&#30495;&#20540;&#34920;&#24046;&#24322;&#20316;&#20026;&#35757;&#32451;&#30417;&#30563;&#65292;&#20197;&#21450;&#19968;&#20010;&#32463;&#36807;&#31934;&#24515;&#35774;&#35745;&#21644;&#21487;&#25193;&#23637;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#26126;&#30830;&#32771;&#34385;&#30005;&#36335;&#21151;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#32771;&#34385;&#30005;&#36335;&#30340;&#22266;&#26377;&#29305;&#24615;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;&#19968;&#36718;&#22270;&#34920;&#36798;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Circuit representation learning aims to obtain neural representations of circuit elements and has emerged as a promising research direction that can be applied to various EDA and logic reasoning tasks. Existing solutions, such as DeepGate, have the potential to embed both circuit structural information and functional behavior. However, their capabilities are limited due to weak supervision or flawed model design, resulting in unsatisfactory performance in downstream tasks. In this paper, we introduce DeepGate2, a novel functionality-aware learning framework that significantly improves upon the original DeepGate solution in terms of both learning effectiveness and efficiency. Our approach involves using pairwise truth table differences between sampled logic gates as training supervision, along with a well-designed and scalable loss function that explicitly considers circuit functionality. Additionally, we consider inherent circuit characteristics and design an efficient one-round graph 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#32858;&#31867;&#31561;&#21521;&#24615;&#24230;&#37327;&#30340;&#23454;&#29616;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;&#20998;&#25968;&#21508;&#21521;&#24322;&#24615;&#25193;&#23637;&#20102;&#36825;&#20123;&#24230;&#37327;&#26469;&#26816;&#26597;&#32858;&#31867;&#30340;&#24179;&#22343;&#31561;&#21521;&#24615;&#12290;&#36890;&#36807;&#37327;&#21270;&#19981;&#21516;&#26448;&#26009;&#32467;&#26500;&#25968;&#25454;&#24211;&#34920;&#31034;&#30340;&#26680;&#36924;&#36817;&#20989;&#25968;&#23545;&#32467;&#26524;&#32858;&#31867;&#30340;&#24433;&#21709;&#65292;&#28436;&#31034;&#20102;&#36825;&#31181;&#24230;&#37327;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2305.16372</link><description>&lt;p&gt;
&#24212;&#29992;&#20110;&#26448;&#26009;&#23398;&#39046;&#22495;&#39640;&#32500;&#26080;&#30417;&#30563;&#32858;&#31867;&#20219;&#21153;&#30340;&#31561;&#21521;&#24230;&#37327;
&lt;/p&gt;
&lt;p&gt;
Metrics for quantifying isotropy in high dimensional unsupervised clustering tasks in a materials context. (arXiv:2305.16372v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16372
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#32858;&#31867;&#31561;&#21521;&#24615;&#24230;&#37327;&#30340;&#23454;&#29616;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;&#20998;&#25968;&#21508;&#21521;&#24322;&#24615;&#25193;&#23637;&#20102;&#36825;&#20123;&#24230;&#37327;&#26469;&#26816;&#26597;&#32858;&#31867;&#30340;&#24179;&#22343;&#31561;&#21521;&#24615;&#12290;&#36890;&#36807;&#37327;&#21270;&#19981;&#21516;&#26448;&#26009;&#32467;&#26500;&#25968;&#25454;&#24211;&#34920;&#31034;&#30340;&#26680;&#36924;&#36817;&#20989;&#25968;&#23545;&#32467;&#26524;&#32858;&#31867;&#30340;&#24433;&#21709;&#65292;&#28436;&#31034;&#20102;&#36825;&#31181;&#24230;&#37327;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32858;&#31867;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#24120;&#35265;&#30340;&#20219;&#21153;&#65292;&#20294;&#26080;&#26631;&#31614;&#25968;&#25454;&#30340;&#32858;&#31867;&#21487;&#33021;&#38590;&#20197;&#37327;&#21270;&#12290;&#21270;&#23398;&#20013;&#30340;&#32858;&#31867;&#31639;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#26448;&#26009;&#34920;&#31034;&#12290;&#30001;&#20110;&#25968;&#25454;&#30340;&#32500;&#24230;&#65292;&#30830;&#23450;&#19981;&#21516;&#34920;&#31034;&#12289;&#32858;&#31867;&#31639;&#27861;&#25110;&#25968;&#25454;&#21464;&#25442;&#23545;&#32467;&#26524;&#32858;&#31867;&#30340;&#24433;&#21709;&#26159;&#22256;&#38590;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#32858;&#31867;&#31561;&#21521;&#24615;&#24230;&#37327;&#30340;&#35814;&#32454;&#20998;&#26512;&#65292;&#21253;&#25324;&#19968;&#31181;&#22522;&#20110;&#29616;&#26377;&#25512;&#23548;&#30340;&#26032;&#30340;&#23454;&#29616;&#26041;&#27861;&#12290;&#20351;&#29992;&#20998;&#25968;&#21508;&#21521;&#24322;&#24615;&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;&#27604;&#36739;&#21307;&#23398;&#25104;&#20687;&#24120;&#29992;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#36827;&#32780;&#25193;&#23637;&#36825;&#20123;&#24230;&#37327;&#65292;&#20197;&#26816;&#26597;&#19968;&#32452;&#32858;&#31867;&#30340;&#24179;&#22343;&#31561;&#21521;&#24615;&#12290;&#36890;&#36807;&#37327;&#21270;&#19981;&#21516;&#26448;&#26009;&#32467;&#26500;&#25968;&#25454;&#24211;&#34920;&#31034;&#30340;&#26680;&#36924;&#36817;&#20989;&#25968;&#23545;&#32467;&#26524;&#32858;&#31867;&#30340;&#24433;&#21709;&#65292;&#28436;&#31034;&#20102;&#36825;&#20123;&#24230;&#37327;&#30340;&#29992;&#20363;&#12290;&#36825;&#20123;&#26041;&#27861;&#30340;&#24191;&#27867;&#36866;&#29992;&#24615;&#20063;&#22312;&#20998;&#26512;MNIST&#25968;&#25454;&#38598;&#30340;&#23398;&#20064;&#23884;&#20837;&#20013;&#24471;&#21040;&#20102;&#35777;&#26126;&#12290;&#38543;&#26426;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Clustering is a common task in machine learning, but clusters of unlabelled data can be hard to quantify. The application of clustering algorithms in chemistry is often dependant on material representation. Ascertaining the effects of different representations, clustering algorithms, or data transformations on the resulting clusters is difficult due to the dimensionality of these data. We present a thorough analysis of measures for isotropy of a cluster, including a novel implantation based on an existing derivation. Using fractional anisotropy, a common method used in medical imaging for comparison, we then expand these measures to examine the average isotropy of a set of clusters. A use case for such measures is demonstrated by quantifying the effects of kernel approximation functions on different representations of the Inorganic Crystal Structure Database. Broader applicability of these methods is demonstrated in analysing learnt embedding of the MNIST dataset. Random clusters are e
&lt;/p&gt;</description></item><item><title>Stecformer&#26159;&#19968;&#31181;&#22788;&#29702;&#22810;&#20803;&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#31354;&#38388;-&#26102;&#38388;&#32534;&#30721;&#22120;&#21644;&#32423;&#32852;&#35299;&#30721;&#39044;&#27979;&#22120;&#65288;CDP&#65289;&#65292;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#20026;&#22810;&#20803;&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.16370</link><description>&lt;p&gt;
Stecformer&#65306;&#22522;&#20110;&#31354;&#38388;-&#26102;&#38388;&#32534;&#30721;&#20018;&#32852;&#21464;&#21387;&#22120;&#30340;&#22810;&#20803;&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Stecformer: Spatio-temporal Encoding Cascaded Transformer for Multivariate Long-term Time Series Forecasting. (arXiv:2305.16370v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16370
&lt;/p&gt;
&lt;p&gt;
Stecformer&#26159;&#19968;&#31181;&#22788;&#29702;&#22810;&#20803;&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#31354;&#38388;-&#26102;&#38388;&#32534;&#30721;&#22120;&#21644;&#32423;&#32852;&#35299;&#30721;&#39044;&#27979;&#22120;&#65288;CDP&#65289;&#65292;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#20026;&#22810;&#20803;&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#20803;&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#22312;&#35832;&#22810;&#39046;&#22495;&#20013;&#20855;&#26377;&#37325;&#35201;&#24212;&#29992;&#65292;&#22914;&#33021;&#28304;&#28040;&#32791;&#21644;&#22825;&#27668;&#39044;&#25253;&#31561;&#12290;&#38543;&#30528;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#26041;&#27861;&#30340;&#21457;&#23637;&#65292;&#22810;&#20803;&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#24615;&#33021;&#24471;&#21040;&#20102;&#26174;&#33879;&#25552;&#39640;&#65292;&#20294;&#21464;&#21387;&#22120;&#27169;&#22411;&#20013;&#23545;&#20110;&#31354;&#38388;&#29305;&#24449;&#30340;&#30740;&#31350;&#36739;&#23569;&#65292;&#32780;&#19981;&#21516;&#39044;&#27979;&#21608;&#26399;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#20063;&#30001;&#20110;&#26102;&#38388;&#36328;&#24230;&#36739;&#22823;&#32780;&#19981;&#23613;&#20154;&#24847;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23436;&#25972;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#35299;&#20915;&#29305;&#24449;&#25552;&#21462;&#21644;&#30446;&#26631;&#39044;&#27979;&#26041;&#38754;&#30340;&#38382;&#39064;&#12290;&#23545;&#20110;&#29305;&#24449;&#25552;&#21462;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#31354;&#38388;-&#26102;&#38388;&#32534;&#30721;&#22120;&#65292;&#21253;&#25324;&#21322;&#33258;&#36866;&#24212;&#22270;&#24418;&#26469;&#33719;&#21462;&#36275;&#22815;&#30340;&#31354;&#38388;-&#26102;&#38388;&#20449;&#24687;&#12290;&#23545;&#20110;&#39044;&#27979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#32423;&#32852;&#35299;&#30721;&#39044;&#27979;&#22120;&#65288;CDP&#65289;&#65292;&#20197;&#21152;&#24378;&#19981;&#21516;&#38388;&#38548;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#20063;&#21487;&#20197;&#29992;&#20316;&#36890;&#29992;&#30340;&#32452;&#20214;&#26469;&#25552;&#39640;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#31216;&#20026;Stecformer&#65292;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#20026;&#22810;&#20803;&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multivariate long-term time series forecasting is of great application across many domains, such as energy consumption and weather forecasting. With the development of transformer-based methods, the performance of multivariate long-term time series forecasting has been significantly improved, however, the study of spatial features extracting in transformer-based model is rare and the consistency of different prediction periods is unsatisfactory due to the large span. In this work, we propose a complete solution to address these problems in terms of feature extraction and target prediction. For extraction, we design an efficient spatio-temporal encoding extractor including a semi-adaptive graph to acquire sufficient spatio-temporal information. For prediction, we propose a Cascaded Decoding Predictor (CDP) to strengthen the correlation between different intervals, which can also be utilized as a generic component to improve the performance of transformer-based methods. The proposed meth
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21322;&#33258;&#21160;&#21270;&#30340;&#27969;&#31243;&#65292;&#23558;&#38598;&#20307;&#19987;&#23478;&#30693;&#35782;&#25551;&#36848;&#36716;&#25442;&#20026;&#26412;&#20307;&#35770;&#65292;&#23454;&#29616;&#20102;&#20174;&#25968;&#25454;&#38598;&#20013;&#25552;&#21462;&#35282;&#33853;&#24773;&#20917;&#21644;&#23545;&#32593;&#32476;&#36827;&#34892;&#35780;&#20272;&#30340;&#30446;&#30340;&#12290;</title><link>http://arxiv.org/abs/2305.16369</link><description>&lt;p&gt;
&#19968;&#31181;&#21322;&#33258;&#21160;&#21270;&#35282;&#33853;&#24773;&#20917;&#26816;&#27979;&#21644;&#35780;&#20272;&#27969;&#31243;
&lt;/p&gt;
&lt;p&gt;
A Semi-Automated Corner Case Detection and Evaluation Pipeline. (arXiv:2305.16369v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16369
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21322;&#33258;&#21160;&#21270;&#30340;&#27969;&#31243;&#65292;&#23558;&#38598;&#20307;&#19987;&#23478;&#30693;&#35782;&#25551;&#36848;&#36716;&#25442;&#20026;&#26412;&#20307;&#35770;&#65292;&#23454;&#29616;&#20102;&#20174;&#25968;&#25454;&#38598;&#20013;&#25552;&#21462;&#35282;&#33853;&#24773;&#20917;&#21644;&#23545;&#32593;&#32476;&#36827;&#34892;&#35780;&#20272;&#30340;&#30446;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#35753;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#25237;&#20837;&#20844;&#20849;&#20351;&#29992;&#65292;&#24517;&#39035;&#35777;&#26126;&#36710;&#36742;&#33021;&#22815;&#22312;&#35768;&#22810;&#19981;&#21516;&#24773;&#20917;&#19979;&#23433;&#20840;&#12289;&#21487;&#38752;&#22320;&#22788;&#29702;&#20132;&#36890;&#12290;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#20043;&#19968;&#26159;&#25429;&#25417;&#21644;&#22788;&#29702;&#36710;&#36742;&#21608;&#22260;&#29615;&#22659;&#30340;&#24863;&#30693;&#31995;&#32479;&#12290;&#24863;&#30693;&#31995;&#32479;&#38656;&#35201;&#22823;&#37327;&#25968;&#25454;&#38598;&#26469;&#35757;&#32451;&#20854;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#12290;&#22312;&#36825;&#20123;&#25968;&#25454;&#38598;&#20013;&#20102;&#35299;&#21738;&#20123;&#25968;&#25454;&#37096;&#20998;&#25551;&#36848;&#20102;&#19968;&#20010;&#35282;&#33853;&#24773;&#20917;&#65292;&#22312;&#32593;&#32476;&#30340;&#35757;&#32451;&#25110;&#27979;&#35797;&#36807;&#31243;&#20013;&#20855;&#26377;&#20248;&#21183;&#12290;&#36825;&#20123;&#35282;&#33853;&#24773;&#20917;&#25551;&#36848;&#20102;&#32597;&#35265;&#19988;&#23545;&#32593;&#32476;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#38598;&#20307;&#19987;&#23478;&#30693;&#35782;&#25551;&#36848;&#36716;&#25442;&#20026;&#25193;&#23637;KI Absicherung&#26412;&#20307;&#35770;&#30340;&#27969;&#31243;&#12290;&#35813;&#26412;&#20307;&#35770;&#29992;&#20110;&#25551;&#36848;&#21487;&#26144;&#23556;&#21040;&#24863;&#30693;&#25968;&#25454;&#38598;&#30340;&#22330;&#26223;&#21644;&#24773;&#20917;&#12290;&#28982;&#21518;&#21487;&#20197;&#20174;&#25968;&#25454;&#38598;&#20013;&#25552;&#21462;&#35282;&#33853;&#24773;&#20917;&#12290;&#27492;&#22806;&#65292;&#35813;&#27969;&#31243;&#36824;&#20351;&#24471;&#21487;&#20197;&#23545;&#26816;&#27979;&#32593;&#32476;&#38024;&#23545;&#25552;&#21462;&#30340;&#35282;&#33853;&#24773;&#20917;&#36827;&#34892;&#35780;&#20272;&#20197;&#36827;&#34892;&#27979;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
In order to deploy automated vehicles to the public, it has to be proven that the vehicle can safely and robustly handle traffic in many different scenarios. One important component of automated vehicles is the perception system that captures and processes the environment around the vehicle. Perception systems require large datasets for training their deep neural network. Knowing which parts of the data in these datasets describe a corner case is an advantage during training or testing of the network. These corner cases describe situations that are rare and potentially challenging for the network. We propose a pipeline that converts collective expert knowledge descriptions into the extended KI Absicherung ontology. The ontology is used to describe scenes and scenarios that can be mapped to perception datasets. The corner cases can then be extracted from the datasets. In addition, the pipeline enables the evaluation of the detection networks against the extracted corner cases to measure
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23558;&#23545;&#35805;&#20195;&#29702;&#34892;&#20026;&#25551;&#36848;&#20026;&#35282;&#33394;&#25198;&#28436;&#65292;&#20197;&#36991;&#20813;&#36171;&#20104;&#20854;&#20154;&#31867;&#29305;&#24449;&#65292;&#22312;&#27492;&#22522;&#30784;&#19978;&#30740;&#31350;&#20195;&#29702;&#34892;&#20026;&#20013;&#30340;&#27450;&#39575;&#21644;&#33258;&#25105;&#24847;&#35782;&#12290;</title><link>http://arxiv.org/abs/2305.16367</link><description>&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#35282;&#33394;&#25198;&#28436;
&lt;/p&gt;
&lt;p&gt;
Role-Play with Large Language Models. (arXiv:2305.16367v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16367
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#23545;&#35805;&#20195;&#29702;&#34892;&#20026;&#25551;&#36848;&#20026;&#35282;&#33394;&#25198;&#28436;&#65292;&#20197;&#36991;&#20813;&#36171;&#20104;&#20854;&#20154;&#31867;&#29305;&#24449;&#65292;&#22312;&#27492;&#22522;&#30784;&#19978;&#30740;&#31350;&#20195;&#29702;&#34892;&#20026;&#20013;&#30340;&#27450;&#39575;&#21644;&#33258;&#25105;&#24847;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#23545;&#35805;&#20195;&#29702;&#31243;&#24207;&#22312;&#34920;&#29616;&#19978;&#36234;&#26469;&#36234;&#25509;&#36817;&#20154;&#31867;&#65292;&#26377;&#24517;&#35201;&#24320;&#21457;&#26377;&#25928;&#30340;&#26041;&#24335;&#39640;&#23618;&#27425;&#25551;&#36848;&#20854;&#34892;&#20026;&#65292;&#32780;&#19981;&#20250;&#38519;&#20837;&#36171;&#20104;&#20854;&#20154;&#31867;&#29305;&#24449;&#30340;&#38519;&#38449;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#35282;&#33394;&#25198;&#28436;&#30340;&#27010;&#24565;&#65292;&#23558;&#23545;&#35805;&#20195;&#29702;&#31243;&#24207;&#30340;&#34892;&#20026;&#35270;&#20026;&#35282;&#33394;&#25198;&#28436;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#20511;&#37492;&#29087;&#24713;&#30340;&#27665;&#38388;&#24515;&#29702;&#23398;&#26415;&#35821;&#65292;&#32780;&#19981;&#26159;&#36171;&#20104;&#23427;&#20204;&#23454;&#38469;&#19978;&#24182;&#19981;&#20855;&#22791;&#30340;&#20154;&#31867;&#29305;&#24449;&#12290;&#26412;&#25991;&#20197;(&#34920;&#38754;&#19978;&#30340;)&#27450;&#39575;&#21644;(&#34920;&#38754;&#19978;&#30340;)&#33258;&#25105;&#24847;&#35782;&#20026;&#20363;&#65292;&#25506;&#35752;&#20102;&#23545;&#35805;&#20195;&#29702;&#31243;&#24207;&#34892;&#20026;&#30340;&#20004;&#31181;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
As dialogue agents become increasingly human-like in their performance, it is imperative that we develop effective ways to describe their behaviour in high-level terms without falling into the trap of anthropomorphism. In this paper, we foreground the concept of role-play. Casting dialogue agent behaviour in terms of role-play allows us to draw on familiar folk psychological terms, without ascribing human characteristics to language models they in fact lack. Two important cases of dialogue agent behaviour are addressed this way, namely (apparent) deception and (apparent) self-awareness.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#23376;&#30446;&#26631;&#30340;&#28436;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#22522;&#20110;&#23376;&#30446;&#26631;&#30340;&#23398;&#20064;&#26041;&#27861;&#19982;&#25193;&#25955;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#25552;&#39640;&#28436;&#31034;&#30340;&#21487;&#29702;&#35299;&#24615;&#65292;&#24182;&#25552;&#39640;LLMs&#22312;&#24418;&#24335;&#23450;&#29702;&#35777;&#26126;&#20013;&#30340;&#21534;&#21520;&#37327;&#12290;</title><link>http://arxiv.org/abs/2305.16366</link><description>&lt;p&gt;
&#20943;&#23569;&#35868;&#22242;&#65306;&#22522;&#20110;&#23376;&#30446;&#26631;&#30340;&#28436;&#31034;&#23398;&#20064;&#22312;&#24418;&#24335;&#23450;&#29702;&#35777;&#26126;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Decomposing the Enigma: Subgoal-based Demonstration Learning for Formal Theorem Proving. (arXiv:2305.16366v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16366
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#23376;&#30446;&#26631;&#30340;&#28436;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#22522;&#20110;&#23376;&#30446;&#26631;&#30340;&#23398;&#20064;&#26041;&#27861;&#19982;&#25193;&#25955;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#25552;&#39640;&#28436;&#31034;&#30340;&#21487;&#29702;&#35299;&#24615;&#65292;&#24182;&#25552;&#39640;LLMs&#22312;&#24418;&#24335;&#23450;&#29702;&#35777;&#26126;&#20013;&#30340;&#21534;&#21520;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#24418;&#24335;&#23450;&#29702;&#35777;&#26126;&#39046;&#22495;&#25552;&#20379;&#20102;&#26377;&#36259;&#30340;&#25506;&#32034;&#26041;&#21521;&#12290;&#28982;&#32780;&#65292;&#22914;&#20309;&#23436;&#20840;&#21033;&#29992;&#36825;&#20123;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;&#22312;&#28436;&#31034;&#26684;&#24335;&#21644;&#32452;&#32455;&#26041;&#38754;&#65292;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#30340;&#39046;&#22495;&#12290;&#20026;&#20102;&#22686;&#24378;LLMs&#30340;&#25928;&#33021;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#23376;&#30446;&#26631;&#30340;&#28436;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#21253;&#25324;&#20004;&#20010;&#20027;&#35201;&#20803;&#32032;&#65306;&#31532;&#19968;&#65292;&#20174;&#24378;&#21270;&#23398;&#20064;&#21644;&#26426;&#22120;&#20154;&#39046;&#22495;&#30340;&#23376;&#30446;&#26631;&#23398;&#20064;&#20013;&#27762;&#21462;&#32463;&#39564;&#65292;&#20026;&#27599;&#20010;&#28436;&#31034;&#31034;&#20363;&#26500;&#24314;&#19981;&#21516;&#30340;&#23376;&#30446;&#26631;&#65292;&#24182;&#26681;&#25454;&#30456;&#20851;&#30340;&#23376;&#30446;&#26631;&#23398;&#20064;&#29702;&#35770;&#26469;&#20248;&#21270;&#36825;&#20123;&#23376;&#30446;&#26631;&#12290;&#31532;&#20108;&#65292;&#21033;&#29992;&#26368;&#36817;&#25193;&#25955;&#27169;&#22411;&#30340;&#36827;&#23637;&#26469;&#39044;&#27979;&#26368;&#20339;&#32452;&#32455;&#26041;&#24335;&#65292;&#21516;&#26102;&#35299;&#20915;&#28436;&#31034;&#32452;&#32455;&#39046;&#22495;&#20013;&#23384;&#22312;&#30340;&#20004;&#20010;&#22797;&#26434;&#38382;&#39064;&#65306;&#23376;&#38598;&#36873;&#25321;&#21644;&#39034;&#24207;&#30830;&#23450;&#12290;&#36890;&#36807;&#23558;&#22522;&#20110;&#23376;&#30446;&#26631;&#30340;&#23398;&#20064;&#26041;&#27861;&#19982;&#25193;&#25955;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#20316;&#32773;&#25552;&#20986;&#30340;&#26694;&#26550;&#21487;&#20197;&#25552;&#39640;&#28436;&#31034;&#30340;&#21487;&#29702;&#35299;&#24615;&#65292;&#24182;&#25552;&#39640;LLMs&#22312;&#24418;&#24335;&#23450;&#29702;&#35777;&#26126;&#20013;&#30340;&#21534;&#21520;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models~(LLMs) present an intriguing avenue of exploration in the domain of formal theorem proving. Nonetheless, the full utilization of these models, particularly in terms of demonstration formatting and organization, remains an underexplored area. In an endeavor to enhance the efficacy of LLMs, we introduce a subgoal-based demonstration learning framework, consisting of two primary elements: Firstly, drawing upon the insights of subgoal learning from the domains of reinforcement learning and robotics, we propose the construction of distinct subgoals for each demonstration example and refine these subgoals in accordance with the pertinent theories of subgoal learning. Secondly, we build upon recent advances in diffusion models to predict the optimal organization, simultaneously addressing two intricate issues that persist within the domain of demonstration organization: subset selection and order determination. Through the integration of subgoal-based learning methodolog
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#27604;&#36739;&#20102;14&#31181;&#19981;&#21516;&#30340;&#35780;&#20272;&#25351;&#26631;&#22312;&#23545;9&#31181;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#21487;&#35299;&#37322;&#24615;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#26041;&#27861;&#21644;&#19977;&#31181;&#34394;&#25311;&#26041;&#27861;&#36827;&#34892;&#24212;&#29992;&#26102;&#30340;&#25928;&#26524;&#65292;&#32473;&#20986;&#20102;&#39640;&#24230;&#30456;&#20851;&#32467;&#26524;&#65292;&#25351;&#20986;&#20102;&#23384;&#22312;&#28508;&#22312;&#20887;&#20313;&#12290;&#27492;&#22806;&#65292;&#36824;&#23637;&#31034;&#20102;&#22522;&#32447;&#36229;&#21442;&#25968;&#23545;&#35780;&#20272;&#25351;&#26631;&#20540;&#20135;&#29983;&#26174;&#33879;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2305.16361</link><description>&lt;p&gt;
&#19968;&#20010;&#20851;&#20110;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#35780;&#20272;&#30340;&#23454;&#39564;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
An Experimental Investigation into the Evaluation of Explainability Methods. (arXiv:2305.16361v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16361
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#27604;&#36739;&#20102;14&#31181;&#19981;&#21516;&#30340;&#35780;&#20272;&#25351;&#26631;&#22312;&#23545;9&#31181;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#21487;&#35299;&#37322;&#24615;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#26041;&#27861;&#21644;&#19977;&#31181;&#34394;&#25311;&#26041;&#27861;&#36827;&#34892;&#24212;&#29992;&#26102;&#30340;&#25928;&#26524;&#65292;&#32473;&#20986;&#20102;&#39640;&#24230;&#30456;&#20851;&#32467;&#26524;&#65292;&#25351;&#20986;&#20102;&#23384;&#22312;&#28508;&#22312;&#20887;&#20313;&#12290;&#27492;&#22806;&#65292;&#36824;&#23637;&#31034;&#20102;&#22522;&#32447;&#36229;&#21442;&#25968;&#23545;&#35780;&#20272;&#25351;&#26631;&#20540;&#20135;&#29983;&#26174;&#33879;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#26088;&#22312;&#24110;&#21161;&#29992;&#25143;&#29702;&#35299;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#32972;&#21518;&#30340;&#25512;&#29702;&#36807;&#31243;&#12290;&#36817;&#24180;&#26469;&#20986;&#29616;&#20102;&#35768;&#22810;XAI&#26041;&#27861;&#65292;&#30456;&#24212;&#22320;&#65292;&#19982;XAI&#26041;&#27861;&#35780;&#20272;&#30456;&#20851;&#30340;&#23376;&#39046;&#22495;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#20851;&#27880;&#65292;&#26088;&#22312;&#30830;&#23450;&#20351;&#29992;&#21508;&#31181;&#26041;&#27861;&#21644;&#26631;&#20934;&#25552;&#20379;&#26368;&#20339;&#35299;&#37322;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#25991;&#29486;&#32570;&#20047;&#23545;&#35780;&#20272;&#25351;&#26631;&#26412;&#36523;&#30340;&#27604;&#36739;&#65292;&#36825;&#20123;&#25351;&#26631;&#21487;&#20197;&#29992;&#20110;&#35780;&#20272;XAI&#26041;&#27861;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#27604;&#36739;14&#31181;&#19981;&#21516;&#30340;&#25351;&#26631;&#22312;&#23545;&#20061;&#31181;&#26368;&#20808;&#36827;&#30340;XAI&#26041;&#27861;&#21644;&#19977;&#31181;&#34394;&#25311;&#26041;&#27861;&#65288;&#20363;&#22914;&#38543;&#26426;&#26174;&#33879;&#24615;&#22270;&#65289;&#36827;&#34892;&#24212;&#29992;&#26102;&#30340;&#25928;&#26524;&#65292;&#26469;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#21738;&#20123;&#25351;&#26631;&#20135;&#29983;&#39640;&#24230;&#30456;&#20851;&#30340;&#32467;&#26524;&#65292;&#34920;&#26126;&#23384;&#22312;&#28508;&#22312;&#30340;&#20887;&#20313;&#24615;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22522;&#32447;&#36229;&#21442;&#25968;&#23545;&#35780;&#20272;&#25351;&#26631;&#20540;&#30340;&#26174;&#33879;&#24433;&#21709;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#34394;&#25311;&#26041;&#27861;&#35780;&#20272;&#20102;&#23454;&#39564;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
EXplainable Artificial Intelligence (XAI) aims to help users to grasp the reasoning behind the predictions of an Artificial Intelligence (AI) system. Many XAI approaches have emerged in recent years. Consequently, a subfield related to the evaluation of XAI methods has gained considerable attention, with the aim to determine which methods provide the best explanation using various approaches and criteria. However, the literature lacks a comparison of the evaluation metrics themselves, that one can use to evaluate XAI methods. This work aims to fill this gap by comparing 14 different metrics when applied to nine state-of-the-art XAI methods and three dummy methods (e.g., random saliency maps) used as references. Experimental results show which of these metrics produces highly correlated results, indicating potential redundancy. We also demonstrate the significant impact of varying the baseline hyperparameter on the evaluation metric values. Finally, we use dummy methods to assess the re
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#25200;&#21160;&#29983;&#25104;&#26641;&#30340;&#21487;&#24494;&#32858;&#31867;&#26041;&#27861;&#65292;&#20381;&#36182;&#20110;&#32447;&#24615;&#35268;&#21010;&#35299;&#30340;&#38543;&#26426;&#25200;&#21160;&#65292;&#20855;&#26377;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.16358</link><description>&lt;p&gt;
&#24102;&#25200;&#21160;&#29983;&#25104;&#26641;&#30340;&#21487;&#24494;&#32858;&#31867;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Differentiable Clustering with Perturbed Spanning Forests. (arXiv:2305.16358v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16358
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#25200;&#21160;&#29983;&#25104;&#26641;&#30340;&#21487;&#24494;&#32858;&#31867;&#26041;&#27861;&#65292;&#20381;&#36182;&#20110;&#32447;&#24615;&#35268;&#21010;&#35299;&#30340;&#38543;&#26426;&#25200;&#21160;&#65292;&#20855;&#26377;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#26368;&#23567;&#26435;&#37325;&#29983;&#25104;&#26641;&#30340;&#21487;&#24494;&#32858;&#31867;&#26041;&#27861;&#65292;&#23427;&#26159;&#29983;&#25104;&#26641;&#30340;&#19968;&#31181;&#21464;&#20307;&#65292;&#20855;&#26377;&#22810;&#20010;&#36830;&#36890;&#20998;&#37327;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#32447;&#24615;&#35268;&#21010;&#35299;&#30340;&#38543;&#26426;&#25200;&#21160;&#65292;&#20197;&#23454;&#29616;&#24179;&#28369;&#21644;&#39640;&#25928;&#30340;&#26799;&#24230;&#35745;&#31639;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#22312;&#31471;&#21040;&#31471;&#21487;&#35757;&#32451;&#30340;&#27969;&#27700;&#32447;&#20013;&#21253;&#21547;&#32858;&#31867;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#21363;&#20351;&#22312;&#22024;&#26434;&#30340;&#25968;&#25454;&#38598;&#21644;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20960;&#20309;&#29615;&#22659;&#19979;&#20063;&#33021;&#33391;&#22909;&#22320;&#24037;&#20316;&#12290;&#25105;&#20204;&#36824;&#21033;&#29992;&#36825;&#31181;&#26041;&#27861;&#21046;&#23450;&#20102;&#19968;&#20010;&#29305;&#21035;&#30340;&#25439;&#22833;&#65292;&#20197;&#26377;&#25928;&#22320;&#20174;&#37096;&#20998;&#32858;&#31867;&#25968;&#25454;&#23398;&#20064;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#29616;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#23427;&#22312;&#30417;&#30563;&#21644;&#21322;&#30417;&#30563;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a differentiable clustering method based on minimum-weight spanning forests, a variant of spanning trees with several connected components. Our method relies on stochastic perturbations of solutions of linear programs, for smoothing and efficient gradient computations. This allows us to include clustering in end-to-end trainable pipelines. We show that our method performs well even in difficult settings, such as datasets with high noise and challenging geometries. We also formulate an ad hoc loss to efficiently learn from partial clustering data using this operation. We demonstrate its performance on several real world datasets for supervised and semi-supervised tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;M2S-ADD&#30340;&#26032;&#22411;ADD&#27169;&#22411;&#65292;&#36890;&#36807;&#21333;&#22768;&#36947;&#36716;&#31435;&#20307;&#22768;&#25216;&#26415;&#21457;&#29616;&#20266;&#36896;&#38899;&#39057;&#20013;&#30340;&#30495;&#23454;&#24615;&#32447;&#32034;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#26816;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.16353</link><description>&lt;p&gt;
&#33258;&#25105;&#32972;&#21467;&#65306;&#21033;&#29992;&#21333;&#22768;&#36947;&#36716;&#31435;&#20307;&#22768;&#25216;&#26415;&#36827;&#34892;&#28145;&#24230;&#20266;&#36896;&#38899;&#39057;&#30340;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Betray Oneself: A Novel Audio DeepFake Detection Model via Mono-to-Stereo Conversion. (arXiv:2305.16353v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16353
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;M2S-ADD&#30340;&#26032;&#22411;ADD&#27169;&#22411;&#65292;&#36890;&#36807;&#21333;&#22768;&#36947;&#36716;&#31435;&#20307;&#22768;&#25216;&#26415;&#21457;&#29616;&#20266;&#36896;&#38899;&#39057;&#20013;&#30340;&#30495;&#23454;&#24615;&#32447;&#32034;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#26816;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38899;&#39057;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;(Audio Deepfake Detection, ADD)&#26088;&#22312;&#26816;&#27979;&#30001;&#25991;&#26412;&#36716;&#35821;&#38899;(TTS)&#12289;&#35821;&#38899;&#36716;&#25442;(VC)&#12289;&#37325;&#25918;&#31561;&#29983;&#25104;&#30340;&#34394;&#20551;&#38899;&#39057;&#65292;&#26159;&#19968;&#20010;&#26032;&#20852;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#20256;&#32479;&#30340;&#30740;&#31350;&#26041;&#27861;&#23558;&#21333;&#22768;&#36947;&#20449;&#21495;&#20316;&#20026;&#36755;&#20837;&#65292;&#37325;&#28857;&#22312;&#20110;&#31283;&#20581;&#30340;&#29305;&#24449;&#25552;&#21462;&#21644;&#26377;&#25928;&#30340;&#20998;&#31867;&#22120;&#35774;&#35745;&#12290;&#28982;&#32780;&#65292;&#38899;&#39057;&#20449;&#21495;&#20013;&#30340;&#21452;&#36890;&#36947;&#31435;&#20307;&#22768;&#20449;&#24687;&#20063;&#21253;&#21547;&#20102;&#28145;&#24230;&#20266;&#36896;&#30340;&#37325;&#35201;&#32447;&#32034;&#65292;&#36825;&#22312;&#20808;&#21069;&#30340;&#24037;&#20316;&#20013;&#23578;&#26410;&#24471;&#21040;&#30740;&#31350;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;ADD&#27169;&#22411;&#65292;&#31216;&#20026;M2S-ADD&#65292;&#26088;&#22312;&#22312;&#21333;&#22768;&#36947;&#36716;&#31435;&#20307;&#22768;&#36807;&#31243;&#20013;&#21457;&#29616;&#38899;&#39057;&#30495;&#23454;&#24615;&#32447;&#32034;&#12290;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#31435;&#20307;&#22768;&#21512;&#25104;&#22120;&#23558;&#21333;&#22768;&#36947;&#25237;&#24433;&#21040;&#31435;&#20307;&#22768;&#20449;&#21495;&#19978;&#65292;&#28982;&#21518;&#37319;&#29992;&#21452;&#20998;&#25903;&#31070;&#32463;&#26550;&#26500;&#20998;&#21035;&#22788;&#29702;&#24038;&#21491;&#22768;&#36947;&#20449;&#21495;&#12290;&#36825;&#26679;&#65292;&#25105;&#20204;&#26377;&#25928;&#22320;&#25581;&#31034;&#20102;&#20266;&#36896;&#38899;&#39057;&#20013;&#30340;&#20266;&#24433;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;ADD&#30340;&#24615;&#33021;&#12290;&#22312;ASVspoof2019&#25968;&#25454;&#24211;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;M2S-ADD&#22312;&#25152;&#26377;&#36755;&#20837;&#21333;&#22768;&#36947;&#30340;&#22522;&#32447;&#27169;&#22411;&#20013;&#34920;&#29616;&#26368;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;
Audio Deepfake Detection (ADD) aims to detect the fake audio generated by text-to-speech (TTS), voice conversion (VC) and replay, etc., which is an emerging topic. Traditionally we take the mono signal as input and focus on robust feature extraction and effective classifier design. However, the dual-channel stereo information in the audio signal also includes important cues for deepfake, which has not been studied in the prior work. In this paper, we propose a novel ADD model, termed as M2S-ADD, that attempts to discover audio authenticity cues during the mono-to-stereo conversion process. We first projects the mono to a stereo signal using a pretrained stereo synthesizer, then employs a dual-branch neural architecture to process the left and right channel signals, respectively. In this way, we effectively reveal the artifacts in the fake audio, thus improve the ADD performance. The experiments on the ASVspoof2019 database show that M2S-ADD outperforms all baselines that input mono. We
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;WeiAvg&#30340;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#32858;&#21512;&#26041;&#27861;&#65292;&#36890;&#36807;&#24378;&#35843;&#26469;&#33258;&#39640;&#22810;&#26679;&#24615;&#23458;&#25143;&#31471;&#30340;&#26356;&#26032;&#24182;&#20943;&#23569;&#26469;&#33258;&#20302;&#22810;&#26679;&#24615;&#23458;&#25143;&#31471;&#30340;&#24433;&#21709;&#65292;&#25552;&#39640;&#20102;&#32852;&#37030;&#27169;&#22411;&#30340;&#36136;&#37327;&#21644;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.16351</link><description>&lt;p&gt;
WeiAvg&#65306;&#20419;&#36827;&#25968;&#25454;&#22810;&#26679;&#24615;&#30340;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#32858;&#21512;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
WeiAvg: Federated Learning Model Aggregation Promoting Data Diversity. (arXiv:2305.16351v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16351
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;WeiAvg&#30340;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#32858;&#21512;&#26041;&#27861;&#65292;&#36890;&#36807;&#24378;&#35843;&#26469;&#33258;&#39640;&#22810;&#26679;&#24615;&#23458;&#25143;&#31471;&#30340;&#26356;&#26032;&#24182;&#20943;&#23569;&#26469;&#33258;&#20302;&#22810;&#26679;&#24615;&#23458;&#25143;&#31471;&#30340;&#24433;&#21709;&#65292;&#25552;&#39640;&#20102;&#32852;&#37030;&#27169;&#22411;&#30340;&#36136;&#37327;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#20026;&#21033;&#29992;&#22823;&#35268;&#27169;&#31169;&#26377;&#36793;&#32536;&#25968;&#25454;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#38544;&#31169;&#20445;&#25252;&#26041;&#24335;&#65292;&#23588;&#20854;&#36866;&#29992;&#20110;&#29289;&#32852;&#32593;&#35774;&#22791;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#20248;&#21270;&#23398;&#20064;&#36807;&#31243;&#12289;&#35745;&#31639;&#25928;&#29575;&#21644;&#36890;&#20449;&#24320;&#38144;&#31561;&#26041;&#38754;&#65292;&#24573;&#30053;&#20102;&#21442;&#19982;&#32773;&#23545;&#32852;&#37030;&#27169;&#22411;&#36136;&#37327;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#22522;&#20110;&#21152;&#26435;&#24179;&#22343;&#65288;WeiAvg&#65289;&#30340;&#26694;&#26550;&#65292;&#30528;&#37325;&#24378;&#35843;&#26469;&#33258;&#39640;&#22810;&#26679;&#24615;&#23458;&#25143;&#31471;&#30340;&#26356;&#26032;&#65292;&#24182;&#20943;&#23569;&#26469;&#33258;&#20302;&#22810;&#26679;&#24615;&#23458;&#25143;&#31471;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#25237;&#24433;&#30340;&#36817;&#20284;&#26041;&#27861;&#65292;&#26469;&#35780;&#20272;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning provides a promising privacy-preserving way for utilizing large-scale private edge data from massive Internet-of-Things (IoT) devices. While existing research extensively studied optimizing the learning process, computing efficiency, and communication overhead, one important and often overlooked aspect is that participants contribute predictive knowledge from their data, impacting the quality of the federated models learned. While FedAvg treats each client equally and assigns weight solely based on the number of samples, the diversity of samples on each client could greatly affect the local update performance and the final aggregated model. In this paper, we propose a novel approach to address this issue by introducing a Weighted Averaging (WeiAvg) framework that emphasizes updates from high-diversity clients and diminishes the influence of those from low-diversity clients. Specifically, we introduced a projection-based approximation method to estimate the diversity 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#31216;&#20026;Lexinvariant&#35821;&#35328;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#19981;&#38656;&#35201;&#20219;&#20309;&#22266;&#23450;&#26631;&#35760;&#23884;&#20837;&#65292;&#23436;&#20840;&#20381;&#36182;&#19978;&#19979;&#25991;&#20013;&#26631;&#35760;&#30340;&#20849;&#29616;&#21644;&#37325;&#22797;&#12290;&#20316;&#32773;&#35777;&#26126;&#21487;&#20197;&#26500;&#24314;&#19968;&#20010;lexinvariant LM&#65292;&#20197;&#22810;&#39033;&#24335;&#26041;&#24335;&#19982;&#19978;&#19979;&#25991;&#38271;&#24230;&#25104;&#27604;&#20363;&#22320;&#25910;&#25947;&#21040;&#30495;&#23454;&#35821;&#35328;&#27169;&#22411;&#65292;&#20854;&#24120;&#37327;&#22240;&#23376;&#22312;&#35789;&#27719;&#34920;&#22823;&#23567;&#19979;&#20026;&#27425;&#32447;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.16349</link><description>&lt;p&gt;
Lexinvariant&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Lexinvariant Language Models. (arXiv:2305.16349v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16349
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#31216;&#20026;Lexinvariant&#35821;&#35328;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#19981;&#38656;&#35201;&#20219;&#20309;&#22266;&#23450;&#26631;&#35760;&#23884;&#20837;&#65292;&#23436;&#20840;&#20381;&#36182;&#19978;&#19979;&#25991;&#20013;&#26631;&#35760;&#30340;&#20849;&#29616;&#21644;&#37325;&#22797;&#12290;&#20316;&#32773;&#35777;&#26126;&#21487;&#20197;&#26500;&#24314;&#19968;&#20010;lexinvariant LM&#65292;&#20197;&#22810;&#39033;&#24335;&#26041;&#24335;&#19982;&#19978;&#19979;&#25991;&#38271;&#24230;&#25104;&#27604;&#20363;&#22320;&#25910;&#25947;&#21040;&#30495;&#23454;&#35821;&#35328;&#27169;&#22411;&#65292;&#20854;&#24120;&#37327;&#22240;&#23376;&#22312;&#35789;&#27719;&#34920;&#22823;&#23567;&#19979;&#20026;&#27425;&#32447;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20196;&#29260;&#23884;&#20837;&#26159;&#20174;&#31163;&#25955;&#35789;&#27719;&#31526;&#21495;&#21040;&#36830;&#32493;&#21521;&#37327;&#30340;&#26144;&#23556;&#65292;&#26159;&#20219;&#20309;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#30340;&#26680;&#24515;&#12290;&#20294;&#26159;&#65292;&#35789;&#27719;&#31526;&#21495;&#30340;&#21547;&#20041;&#20063;&#21487;&#20197;&#36890;&#36807;&#23427;&#20204;&#22312;&#38271;&#19978;&#19979;&#25991;&#20013;&#30340;&#32467;&#26500;&#35282;&#33394;&#26469;&#30830;&#23450;&#29978;&#33267;&#37325;&#26032;&#23450;&#20041;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#38382;&#65306;&#26159;&#21542;&#21487;&#33021;&#23384;&#22312;&#19968;&#31181;&#27809;&#26377;&#20219;&#20309;&#22266;&#23450;&#26631;&#35760;&#23884;&#20837;&#30340;&#24615;&#33021;&#33391;&#22909;&#30340;&#35821;&#35328;&#27169;&#22411;&#65311;&#36825;&#26679;&#30340;&#35821;&#35328;&#27169;&#22411;&#23558;&#23436;&#20840;&#20381;&#36182;&#20110;&#19978;&#19979;&#25991;&#20013;&#26631;&#35760;&#30340;&#20849;&#29616;&#21644;&#37325;&#22797;&#65292;&#32780;&#19981;&#26159;&#20219;&#20309;&#26631;&#35760;&#30340;\textit{a priori}&#26631;&#35782;&#12290;&#20026;&#20102;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;\textit{lexinvariant}&#35821;&#35328;&#27169;&#22411;&#65292;&#36825;&#20123;&#35821;&#35328;&#27169;&#22411;&#23545;&#35789;&#27719;&#31526;&#21495;&#19981;&#21464;&#65292;&#22240;&#27492;&#22312;&#23454;&#36341;&#20013;&#19981;&#38656;&#35201;&#22266;&#23450;&#30340;&#20196;&#29260;&#23884;&#20837;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35777;&#26126;&#21487;&#20197;&#26500;&#24314;&#19968;&#20010;lexinvariant LM&#65292;&#20197;&#22810;&#39033;&#24335;&#26041;&#24335;&#19982;&#19978;&#19979;&#25991;&#38271;&#24230;&#25104;&#27604;&#20363;&#22320;&#25910;&#25947;&#21040;&#30495;&#23454;&#35821;&#35328;&#27169;&#22411;&#65292;&#20854;&#24120;&#37327;&#22240;&#23376;&#22312;&#35789;&#27719;&#34920;&#22823;&#23567;&#19979;&#20026;&#27425;&#32447;&#24615;&#12290;&#20854;&#27425;&#65292;&#35201;&#26500;&#24314;&#19968;&#20010;lexinvariant LM&#65292;&#25105;&#20204;&#21482;&#38656;&#20351;&#29992;&#38543;&#26426;&#39640;&#26031;&#20989;&#25968;&#23545;&#26631;&#35760;&#36827;&#34892;&#32534;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
Token embeddings, a mapping from discrete lexical symbols to continuous vectors, are at the heart of any language model (LM). However, lexical symbol meanings can also be determined and even redefined by their structural role in a long context. In this paper, we ask: is it possible for a language model to be performant without \emph{any} fixed token embeddings? Such a language model would have to rely entirely on the co-occurence and repetition of tokens in the context rather than the \textit{a priori} identity of any token. To answer this, we study \textit{lexinvariant}language models that are invariant to lexical symbols and therefore do not need fixed token embeddings in practice. First, we prove that we can construct a lexinvariant LM to converge to the true language model at a uniform rate that is polynomial in terms of the context length, with a constant factor that is sublinear in the vocabulary size. Second, to build a lexinvariant LM, we simply encode tokens using random Gauss
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;&#29616;&#26377;&#25991;&#29486;&#20013;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#31958;&#23615;&#30149;&#39118;&#38505;&#39044;&#27979;&#27169;&#22411;&#65292;&#22312;&#21333;&#27169;&#24577;&#21644;&#22810;&#27169;&#24577;&#27169;&#22411;&#20013;&#22343;&#34920;&#29616;&#31361;&#20986;&#65292;&#20294;&#22806;&#37096;&#39564;&#35777;&#26377;&#38480;&#65292;&#35299;&#37322;&#24615;&#26041;&#27861;&#38656;&#35201;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2305.16346</link><description>&lt;p&gt;
&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#31934;&#20934;&#21307;&#30103;&#26041;&#27861;&#65306;&#31958;&#23615;&#30149;&#39118;&#38505;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Artificial Intelligence-Based Methods for Precision Medicine: Diabetes Risk Prediction. (arXiv:2305.16346v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16346
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#29616;&#26377;&#25991;&#29486;&#20013;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#31958;&#23615;&#30149;&#39118;&#38505;&#39044;&#27979;&#27169;&#22411;&#65292;&#22312;&#21333;&#27169;&#24577;&#21644;&#22810;&#27169;&#24577;&#27169;&#22411;&#20013;&#22343;&#34920;&#29616;&#31361;&#20986;&#65292;&#20294;&#22806;&#37096;&#39564;&#35777;&#26377;&#38480;&#65292;&#35299;&#37322;&#24615;&#26041;&#27861;&#38656;&#35201;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
2&#22411;&#31958;&#23615;&#30149;&#30340;&#26085;&#30410;&#26222;&#21450;&#38656;&#35201;&#24320;&#21457;&#39044;&#27979;&#27169;&#22411;&#36827;&#34892;&#39118;&#38505;&#35780;&#20272;&#12290;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#27169;&#22411;&#34987;&#24191;&#27867;&#29992;&#20110;&#27492;&#30446;&#30340;&#65292;&#20294;&#32570;&#20047;&#23545;&#20854;&#36827;&#23637;&#21644;&#25361;&#25112;&#30340;&#20840;&#38754;&#35780;&#20272;&#12290;&#26412;&#25991;&#20998;&#26512;&#20102;&#29616;&#26377;&#25991;&#29486;&#20013;&#22522;&#20110;AI&#30340;2&#22411;&#31958;&#23615;&#30149;&#39118;&#38505;&#39044;&#27979;&#27169;&#22411;&#12290;&#21253;&#25324;&#20102;40&#20010;&#30740;&#31350;&#65292;&#20027;&#35201;&#21457;&#34920;&#22312;&#36807;&#21435;&#22235;&#24180;&#20013;&#12290;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#27604;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26356;&#26222;&#36941;&#12290;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#26159;&#26368;&#24120;&#29992;&#30340;&#25968;&#25454;&#26469;&#28304;&#12290;&#21333;&#27169;&#24577;&#20381;&#36182;EHR&#25968;&#25454;&#30340;AI&#27169;&#22411;&#24456;&#31361;&#20986;&#65292;&#32780;&#21482;&#26377;&#23569;&#25968;&#20351;&#29992;&#22810;&#27169;&#24577;&#27169;&#22411;&#12290;&#21333;&#27169;&#24577;&#21644;&#22810;&#27169;&#24577;&#27169;&#22411;&#22343;&#34920;&#29616;&#33391;&#22909;&#65292;&#21518;&#32773;&#20248;&#20110;&#21069;&#32773;&#12290;&#20869;&#37096;&#39564;&#35777;&#24456;&#24120;&#35265;&#65292;&#32780;&#22806;&#37096;&#39564;&#35777;&#24456;&#26377;&#38480;&#12290;&#35299;&#37322;&#24615;&#26041;&#27861;&#22312;&#19968;&#21322;&#30340;&#30740;&#31350;&#20013;&#24471;&#21040;&#20102;&#25253;&#21578;&#12290;&#23569;&#25968;&#30740;&#31350;&#25253;&#21578;&#20102;&#26032;&#30340;&#29983;&#29289;&#26631;&#24535;&#29289;&#65292;&#19988;&#26377;&#24320;&#28304;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rising prevalence of type 2 diabetes mellitus (T2DM) necessitates the development of predictive models for T2DM risk assessment. Artificial intelligence (AI) models are being extensively used for this purpose, but a comprehensive review of their advancements and challenges is lacking. This scoping review analyzes existing literature on AI-based models for T2DM risk prediction. Forty studies were included, mainly published in the past four years. Traditional machine learning models were more prevalent than deep learning models. Electronic health records were the most commonly used data source. Unimodal AI models relying on EHR data were prominent, while only a few utilized multimodal models. Both unimodal and multimodal models showed promising performance, with the latter outperforming the former. Internal validation was common, while external validation was limited. Interpretability methods were reported in half of the studies. Few studies reported novel biomarkers, and open-source
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#21160;&#21270;&#36130;&#21153;&#20449;&#24687;&#25552;&#21462;&#30340;&#26694;&#26550;&#65288;AFIE&#65289;&#65292;&#29992;&#20110;&#25552;&#21462;&#28151;&#21512;&#38271;&#25991;&#26723;&#20013;&#30340;&#20851;&#38190;&#19994;&#32489;&#25351;&#26631;&#65288;KPI&#65289;&#12290;&#35813;&#26694;&#26550;&#21033;&#29992;LLMs&#22686;&#24378;&#20102;&#36130;&#21153;&#25253;&#21578;&#20449;&#24687;&#30340;&#29702;&#35299;&#21644;&#25552;&#21462;&#33021;&#21147;&#65292;&#24182;&#32463;&#36807;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#39564;&#35777;&#65292;&#35777;&#26126;&#20854;&#22312;GPT-3.5&#21644;GPT-4&#19978;&#30340;&#26377;&#25928;&#24615;&#65292;&#30456;&#23545;&#20110;&#26420;&#32032;&#26041;&#27861;&#65292;&#24179;&#22343;&#31934;&#24230;&#25552;&#39640;&#20102;53.94&#65285;&#21644;33.77&#65285;&#12290;</title><link>http://arxiv.org/abs/2305.16344</link><description>&lt;p&gt;
&#21033;&#29992;LLMs&#20174;&#28151;&#21512;&#38271;&#25991;&#26723;&#20013;&#26816;&#32034;KPI&#30340;&#20840;&#38754;&#26694;&#26550;&#19982;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Leveraging LLMs for KPIs Retrieval from Hybrid Long-Document: A Comprehensive Framework and Dataset. (arXiv:2305.16344v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16344
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#21160;&#21270;&#36130;&#21153;&#20449;&#24687;&#25552;&#21462;&#30340;&#26694;&#26550;&#65288;AFIE&#65289;&#65292;&#29992;&#20110;&#25552;&#21462;&#28151;&#21512;&#38271;&#25991;&#26723;&#20013;&#30340;&#20851;&#38190;&#19994;&#32489;&#25351;&#26631;&#65288;KPI&#65289;&#12290;&#35813;&#26694;&#26550;&#21033;&#29992;LLMs&#22686;&#24378;&#20102;&#36130;&#21153;&#25253;&#21578;&#20449;&#24687;&#30340;&#29702;&#35299;&#21644;&#25552;&#21462;&#33021;&#21147;&#65292;&#24182;&#32463;&#36807;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#39564;&#35777;&#65292;&#35777;&#26126;&#20854;&#22312;GPT-3.5&#21644;GPT-4&#19978;&#30340;&#26377;&#25928;&#24615;&#65292;&#30456;&#23545;&#20110;&#26420;&#32032;&#26041;&#27861;&#65292;&#24179;&#22343;&#31934;&#24230;&#25552;&#39640;&#20102;53.94&#65285;&#21644;33.77&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#25991;&#26412;&#29702;&#35299;&#21644;&#34920;&#26684;&#25512;&#29702;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#20294;&#23427;&#20204;&#23545;&#21253;&#21547;&#25991;&#26412;&#21644;&#34920;&#26684;&#25968;&#25454;&#30340;&#28151;&#21512;&#25991;&#26412;&#30340;&#29702;&#35299;&#21644;&#20998;&#26512;&#33021;&#21147;&#20173;&#26410;&#34987;&#20805;&#20998;&#21457;&#25496;&#12290;&#26412;&#30740;&#31350;&#19987;&#27880;&#20110;&#21033;&#29992;LLMs&#30340;&#28508;&#21147;&#65292;&#20174;&#28151;&#26434;&#30340;&#38271;&#22411;&#36130;&#21153;&#25253;&#21578;&#20013;&#29702;&#35299;&#20851;&#38190;&#20449;&#24687;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#33258;&#21160;&#21270;&#36130;&#21153;&#20449;&#24687;&#25552;&#21462;&#65288;AFIE&#65289;&#26694;&#26550;&#65292;&#22686;&#24378;&#20102;LLMs&#29702;&#35299;&#21644;&#25552;&#21462;&#36130;&#21153;&#25253;&#21578;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#35780;&#20272;AFIE&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#37329;&#34701;&#25253;&#21578;&#25968;&#20540;&#25552;&#21462;&#65288;FINE&#65289;&#25968;&#25454;&#38598;&#65292;&#24182;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#22312;GPT-3.5&#21644;GPT-4&#19978;&#24471;&#21040;&#20102;&#26377;&#25928;&#39564;&#35777;&#65292;&#30456;&#23545;&#20110;&#26420;&#32032;&#26041;&#27861;&#65292;&#24179;&#22343;&#31934;&#24230;&#25552;&#39640;&#20102;53.94&#65285;&#21644;33.77&#65285;&#12290;&#36825;&#20123;&#32467;&#26524;&#34920;&#26126;&#65292;AFIE&#26694;&#26550;&#20026;&#20174;&#22797;&#26434;&#30340;&#28151;&#21512;&#25991;&#26723;&#20013;&#33258;&#21160;&#25552;&#21462;&#25968;&#20540;&#25552;&#20379;&#20102;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) demonstrate exceptional performance in textual understanding and tabular reasoning tasks. However, their ability to comprehend and analyze hybrid text, containing textual and tabular data, remains underexplored. In this research, we specialize in harnessing the potential of LLMs to comprehend critical information from financial reports, which are hybrid long-documents. We propose an Automated Financial Information Extraction (AFIE) framework that enhances LLMs' ability to comprehend and extract information from financial reports. To evaluate AFIE, we develop a Financial Reports Numerical Extraction (FINE) dataset and conduct an extensive experimental analysis. Our framework is effectively validated on GPT-3.5 and GPT-4, yielding average accuracy increases of 53.94% and 33.77%, respectively, compared to a naive method. These results suggest that the AFIE framework offers accuracy for automated numerical extraction from complex, hybrid documents.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Spark&#29983;&#24577;&#31995;&#32479;&#30340;&#20998;&#24067;&#24335;&#26550;&#26500;&#65292;&#21487;&#33258;&#21160;&#25552;&#21462;&#39046;&#22495;&#29305;&#23450;&#26415;&#35821;&#65292;&#32463;&#23454;&#39564;&#35777;&#26126;&#22312;&#26415;&#35821;&#25552;&#21462;&#20934;&#30830;&#24615;&#26041;&#38754;&#21462;&#24471;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.16343</link><description>&lt;p&gt;
&#22522;&#20110;Spark&#29983;&#24577;&#31995;&#32479;&#30340;&#20998;&#24067;&#24335;&#33258;&#21160;&#39046;&#22495;&#29305;&#23450;&#22810;&#35789;&#26415;&#35821;&#35782;&#21035;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
A Distributed Automatic Domain-Specific Multi-Word Term Recognition Architecture using Spark Ecosystem. (arXiv:2305.16343v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16343
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Spark&#29983;&#24577;&#31995;&#32479;&#30340;&#20998;&#24067;&#24335;&#26550;&#26500;&#65292;&#21487;&#33258;&#21160;&#25552;&#21462;&#39046;&#22495;&#29305;&#23450;&#26415;&#35821;&#65292;&#32463;&#23454;&#39564;&#35777;&#26126;&#22312;&#26415;&#35821;&#25552;&#21462;&#20934;&#30830;&#24615;&#26041;&#38754;&#21462;&#24471;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#26415;&#35821;&#35782;&#21035;&#29992;&#20110;&#25552;&#21462;&#23646;&#20110;&#32473;&#23450;&#39046;&#22495;&#30340;&#29305;&#23450;&#26415;&#35821;&#12290;&#20026;&#20102;&#20934;&#30830;&#65292;&#36825;&#20123;&#22522;&#20110;&#35821;&#26009;&#24211;&#21644;&#35821;&#35328;&#20381;&#36182;&#30340;&#26041;&#27861;&#38656;&#35201;&#22788;&#29702;&#22823;&#37327;&#25991;&#26412;&#25968;&#25454;&#20197;&#25552;&#21462;&#20505;&#36873;&#26415;&#35821;&#65292;&#28982;&#21518;&#26681;&#25454;&#32473;&#23450;&#30340;&#24230;&#37327;&#26631;&#20934;&#36827;&#34892;&#35780;&#20998;&#12290;&#20026;&#20102;&#25913;&#36827;&#25991;&#26412;&#39044;&#22788;&#29702;&#21644;&#20505;&#36873;&#26415;&#35821;&#30340;&#25552;&#21462;&#21644;&#35780;&#20998;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;Spark&#29983;&#24577;&#31995;&#32479;&#33258;&#21160;&#25552;&#21462;&#39046;&#22495;&#29305;&#23450;&#26415;&#35821;&#30340;&#20998;&#24067;&#24335;&#26550;&#26500;&#12290;&#20027;&#35201;&#36129;&#29486;&#22914;&#19979;&#65306;&#65288;1&#65289;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#24067;&#24335;&#33258;&#21160;&#39046;&#22495;&#29305;&#23450;&#22810;&#35789;&#26415;&#35821;&#35782;&#21035;&#26550;&#26500;&#65292;&#26500;&#24314;&#22312;Spark&#29983;&#24577;&#31995;&#32479;&#20043;&#19978;&#65307;&#65288;2&#65289;&#20174;&#20934;&#30830;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#26041;&#38754;&#23545;&#25105;&#20204;&#30340;&#26550;&#26500;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#65307;&#65288;3&#65289;&#35774;&#35745;&#20102;&#19968;&#20010;&#26131;&#20110;&#38598;&#25104;&#30340;Python&#23454;&#29616;&#65292;&#20351;&#20854;&#33021;&#22815;&#22312;&#35745;&#31639;&#35821;&#35328;&#23398;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31561;&#39046;&#22495;&#20351;&#29992;&#22823;&#25968;&#25454;&#22788;&#29702;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#21644;&#39046;&#22495;&#19978;&#36827;&#34892;&#23454;&#39564;&#26469;&#32463;&#39564;&#24615;&#22320;&#35777;&#26126;&#20102;&#25105;&#20204;&#26550;&#26500;&#30340;&#21487;&#34892;&#24615;&#65292;&#22312;&#26415;&#35821;&#25552;&#21462;&#20934;&#30830;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic Term Recognition is used to extract domain-specific terms that belong to a given domain. In order to be accurate, these corpus and language-dependent methods require large volumes of textual data that need to be processed to extract candidate terms that are afterward scored according to a given metric. To improve text preprocessing and candidate terms extraction and scoring, we propose a distributed Spark-based architecture to automatically extract domain-specific terms. The main contributions are as follows: (1) propose a novel distributed automatic domain-specific multi-word term recognition architecture built on top of the Spark ecosystem; (2) perform an in-depth analysis of our architecture in terms of accuracy and scalability; (3) design an easy-to-integrate Python implementation that enables the use of Big Data processing in fields such as Computational Linguistics and Natural Language Processing. We prove empirically the feasibility of our architecture by performing ex
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;InterFormer&#65292;&#29992;&#20110;&#20132;&#20114;&#24335;&#23616;&#37096;&#21644;&#20840;&#23616;&#29305;&#24449;&#34701;&#21512;&#65292;&#20197;&#23398;&#20064;&#26356;&#22909;&#30340;ASR&#34920;&#31034;&#12290;&#36890;&#36807;&#32452;&#21512;&#21367;&#31215;&#22359;&#21644;&#21464;&#24418;&#22120;&#22359;&#65292;&#20197;&#21450;&#24341;&#20837;BFIM&#21644;SFM&#27169;&#22359;&#65292;&#23454;&#29616;&#20102;&#23616;&#37096;&#21644;&#20840;&#23616;&#29305;&#24449;&#30340;&#20132;&#20114;&#21644;&#34701;&#21512;&#65292;&#21462;&#24471;&#20102;&#22312;&#20844;&#20849;ASR&#25968;&#25454;&#38598;&#19978;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.16342</link><description>&lt;p&gt;
InterFormer: &#28151;&#21512;&#23616;&#37096;&#21644;&#20840;&#23616;&#29305;&#24449;&#29992;&#20110;&#35821;&#38899;&#35782;&#21035;&#30340;&#20132;&#20114;&#24335;&#34701;&#21512;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
InterFormer: Interactive Local and Global Features Fusion for Automatic Speech Recognition. (arXiv:2305.16342v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16342
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;InterFormer&#65292;&#29992;&#20110;&#20132;&#20114;&#24335;&#23616;&#37096;&#21644;&#20840;&#23616;&#29305;&#24449;&#34701;&#21512;&#65292;&#20197;&#23398;&#20064;&#26356;&#22909;&#30340;ASR&#34920;&#31034;&#12290;&#36890;&#36807;&#32452;&#21512;&#21367;&#31215;&#22359;&#21644;&#21464;&#24418;&#22120;&#22359;&#65292;&#20197;&#21450;&#24341;&#20837;BFIM&#21644;SFM&#27169;&#22359;&#65292;&#23454;&#29616;&#20102;&#23616;&#37096;&#21644;&#20840;&#23616;&#29305;&#24449;&#30340;&#20132;&#20114;&#21644;&#34701;&#21512;&#65292;&#21462;&#24471;&#20102;&#22312;&#20844;&#20849;ASR&#25968;&#25454;&#38598;&#19978;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#32780;&#35328;&#65292;&#23616;&#37096;&#21644;&#20840;&#23616;&#29305;&#24449;&#37117;&#26159;&#24517;&#19981;&#21487;&#23569;&#30340;&#12290;&#35768;&#22810;&#26368;&#36817;&#30340;&#26041;&#27861;&#24050;&#32463;&#35777;&#23454;&#65292;&#31616;&#21333;&#22320;&#21512;&#24182;&#23616;&#37096;&#21644;&#20840;&#23616;&#29305;&#24449;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;ASR&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#24448;&#24448;&#24573;&#30053;&#20102;&#23616;&#37096;&#21644;&#20840;&#23616;&#29305;&#24449;&#20043;&#38388;&#30340;&#20132;&#20114;&#65292;&#24182;&#19988;&#23427;&#20204;&#30340;&#20018;&#34892;&#26550;&#26500;&#26080;&#27861;&#21453;&#26144;&#23616;&#37096;&#21644;&#20840;&#23616;&#29305;&#24449;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;InterFormer&#65292;&#29992;&#20110;&#20132;&#20114;&#24335;&#23616;&#37096;&#21644;&#20840;&#23616;&#29305;&#24449;&#34701;&#21512;&#65292;&#20197;&#23398;&#20064;&#26356;&#22909;&#30340;ASR&#34920;&#31034;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#21367;&#31215;&#22359;&#19982;&#21464;&#24418;&#22120;&#22359;&#20197;&#24182;&#34892;&#35774;&#35745;&#30456;&#32467;&#21512;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21452;&#21521;&#29305;&#24449;&#20132;&#20114;&#27169;&#22359;&#65288;BFIM&#65289;&#21644;&#36873;&#25321;&#24615;&#34701;&#21512;&#27169;&#22359;&#65288;SFM&#65289;&#26469;&#23454;&#29616;&#23616;&#37096;&#21644;&#20840;&#23616;&#29305;&#24449;&#30340;&#20132;&#20114;&#21644;&#34701;&#21512;&#12290;&#22312;&#20844;&#20849;ASR&#25968;&#25454;&#38598;&#19978;&#30340;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;InterFormer&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#19988;&#30456;&#23545;&#20110;&#20854;&#20182;Transformer&#21644;Conformer&#27169;&#22411;&#20855;&#26377;&#26356;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The local and global features are both essential for automatic speech recognition (ASR). Many recent methods have verified that simply combining local and global features can further promote ASR performance. However, these methods pay less attention to the interaction of local and global features, and their series architectures are rigid to reflect local and global relationships. To address these issues, this paper proposes InterFormer for interactive local and global features fusion to learn a better representation for ASR. Specifically, we combine the convolution block with the transformer block in a parallel design. Besides, we propose a bidirectional feature interaction module (BFIM) and a selective fusion module (SFM) to implement the interaction and fusion of local and global features, respectively. Extensive experiments on public ASR datasets demonstrate the effectiveness of our proposed InterFormer and its superior performance over the other Transformer and Conformer models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#22312;&#24179;&#38754;&#20998;&#31867;&#22120;&#30340;&#23398;&#20064;&#31639;&#27861;&#20013;&#38598;&#25104;&#23618;&#27425;&#20998;&#31867;&#20316;&#20026;&#20808;&#39564;&#30693;&#35782;&#30340;&#26377;&#25928;&#24615;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20808;&#39564;&#30693;&#35782;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#23398;&#20064;&#32773;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#21322;&#30417;&#30563;&#21644;&#23436;&#20840;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#37117;&#33719;&#24471;&#20102;&#19981;&#38169;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.16341</link><description>&lt;p&gt;
"TaxoKnow&#65306;&#20998;&#31867;&#20013;&#30340;&#20808;&#39564;&#30693;&#35782;&#8212;&#8212;&#23618;&#27425;&#20998;&#31867;&#20316;&#20026;&#22810;&#31867;&#20998;&#31867;&#25439;&#22833;&#20989;&#25968;&#20013;&#30340;&#20808;&#39564;&#30693;&#35782;"
&lt;/p&gt;
&lt;p&gt;
TaxoKnow: Taxonomy as Prior Knowledge in the Loss Function of Multi-class Classification. (arXiv:2305.16341v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16341
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#22312;&#24179;&#38754;&#20998;&#31867;&#22120;&#30340;&#23398;&#20064;&#31639;&#27861;&#20013;&#38598;&#25104;&#23618;&#27425;&#20998;&#31867;&#20316;&#20026;&#20808;&#39564;&#30693;&#35782;&#30340;&#26377;&#25928;&#24615;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20808;&#39564;&#30693;&#35782;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#23398;&#20064;&#32773;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#21322;&#30417;&#30563;&#21644;&#23436;&#20840;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#37117;&#33719;&#24471;&#20102;&#19981;&#38169;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#23558;&#26631;&#31614;&#30340;&#23618;&#27425;&#20998;&#31867;&#20316;&#20026;&#20808;&#39564;&#30693;&#35782;&#38598;&#25104;&#21040;&#24179;&#38754;&#20998;&#31867;&#22120;&#30340;&#23398;&#20064;&#31639;&#27861;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#23558;&#23618;&#27425;&#20998;&#31867;&#20316;&#20026;&#26174;&#24335;&#27491;&#21017;&#21270;&#22120;&#38598;&#25104;&#21040;&#23398;&#20064;&#31639;&#27861;&#25439;&#22833;&#20989;&#25968;&#20013;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23545;&#23618;&#27425;&#20998;&#31867;&#36827;&#34892;&#25512;&#29702;&#65292;&#31070;&#32463;&#32593;&#32476;&#20943;&#36731;&#20102;&#20854;&#23545;&#31867;&#21035;&#30340;&#36755;&#20986;&#20998;&#24067;&#65292;&#20801;&#35768;&#23558;&#19978;&#23618;&#27010;&#24565;&#30340;&#26465;&#20214;&#38468;&#21152;&#21040;&#23569;&#25968;&#31867;&#19978;&#12290;&#25105;&#20204;&#20165;&#38480;&#20110;&#24179;&#38754;&#20998;&#31867;&#20219;&#21153;&#65292;&#24182;&#25552;&#20379;&#20102;&#22312;&#20004;&#20010;&#24037;&#19994;&#20869;&#37096;&#25968;&#25454;&#38598;&#21644;&#20004;&#20010;&#20844;&#20849;&#22522;&#20934;&#27979;&#35797;&#38598;&#65288;RCV1&#21644;Amazon&#20135;&#21697;&#35780;&#35770;&#65289;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;&#25105;&#20204;&#24471;&#21040;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#21322;&#30417;&#30563;&#22810;&#31867;&#20998;&#31867;&#20013;&#65292;&#20998;&#31867;&#36807;&#31243;&#20013;&#30340;&#20808;&#39564;&#30693;&#35782;&#21487;&#20197;&#26174;&#33879;&#22320;&#25552;&#39640;&#23398;&#20064;&#32773;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#22312;&#23436;&#20840;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#20063;&#33719;&#24471;&#20102;&#19981;&#38169;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we investigate the effectiveness of integrating a hierarchical taxonomy of labels as prior knowledge into the learning algorithm of a flat classifier. We introduce two methods to integrate the hierarchical taxonomy as an explicit regularizer into the loss function of learning algorithms. By reasoning on a hierarchical taxonomy, a neural network alleviates its output distributions over the classes, allowing conditioning on upper concepts for a minority class. We limit ourselves to the flat classification task and provide our experimental results on two industrial in-house datasets and two public benchmarks, RCV1 and Amazon product reviews. Our obtained results show the significant effect of a taxonomy in increasing the performance of a learner in semisupervised multi-class classification and the considerable results obtained in a fully supervised fashion.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#27573;&#24490;&#29615;Transformer&#65288;SRformer&#65289;&#26469;&#20943;&#23569;&#35745;&#31639;/&#20869;&#23384;&#25104;&#26412;&#65292;&#24182;&#20351;&#29992;RAF&#23618;&#22788;&#29702;&#36328;&#27573;&#30340;&#20449;&#24687;&#65292;&#20174;&#32780;&#25552;&#39640;&#24207;&#21015;&#22788;&#29702;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.16340</link><description>&lt;p&gt;
&#20998;&#27573;&#24490;&#29615;Transformer:&#19968;&#31181;&#39640;&#25928;&#30340;&#24207;&#21015;&#21040;&#24207;&#21015;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Segmented Recurrent Transformer: An Efficient Sequence-to-Sequence Model. (arXiv:2305.16340v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16340
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#27573;&#24490;&#29615;Transformer&#65288;SRformer&#65289;&#26469;&#20943;&#23569;&#35745;&#31639;/&#20869;&#23384;&#25104;&#26412;&#65292;&#24182;&#20351;&#29992;RAF&#23618;&#22788;&#29702;&#36328;&#27573;&#30340;&#20449;&#24687;&#65292;&#20174;&#32780;&#25552;&#39640;&#24207;&#21015;&#22788;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#22312;&#35768;&#22810;&#39046;&#22495;&#20013;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;&#35821;&#35328;&#21644;&#35270;&#35273;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#24207;&#21015;&#38271;&#24230;&#30340;&#22686;&#21152;&#65292;&#23427;&#20204;&#30340;&#35745;&#31639;&#25104;&#26412;&#21576;&#20108;&#27425;&#22686;&#38271;&#65292;&#20351;&#24471;&#23427;&#20204;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#24212;&#29992;&#20013;&#20351;&#29992;&#25104;&#20026;&#19981;&#21487;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#23558;&#25972;&#20010;&#24207;&#21015;&#21010;&#20998;&#25104;&#33509;&#24178;&#27573;&#12290;&#28982;&#21518;&#20351;&#29992;&#20855;&#26377;&#24490;&#29615;&#32467;&#26500;&#30340;&#31070;&#32463;&#20803;&#26469;&#32858;&#21512;&#36328;&#27573;&#30340;&#20449;&#24687;&#65292;&#20174;&#32780;&#23454;&#29616;&#20855;&#26377;&#36739;&#20302;&#35745;&#31639;/&#20869;&#23384;&#25104;&#26412;&#30340;&#24207;&#21015;&#22788;&#29702;&#33021;&#21147;&#27169;&#22411;&#12290;&#20026;&#20102;&#39564;&#35777;&#36825;&#20010;&#24819;&#27861;&#65292;&#25105;&#20204;&#39318;&#20808;&#30740;&#31350;&#20102;&#20351;&#29992;&#23616;&#37096;Attention&#26426;&#21046;&#23545;&#21333;&#20010;&#27573;&#30340;&#24433;&#21709;&#12290;&#28982;&#21518;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#27573;&#24490;&#29615;Transformer&#65288;SRformer&#65289;&#65292;&#23427;&#23558;&#20998;&#27573;Attention&#21644;&#24490;&#29615;Attention&#30456;&#32467;&#21512;&#12290;&#23427;&#20351;&#29992;&#24490;&#29615;accumulate and fire&#65288;RAF&#65289;&#23618;&#22312;&#30456;&#37051;&#27573;&#20043;&#38388;&#22788;&#29702;&#20449;&#24687;&#12290;&#36890;&#36807;&#26356;&#26032;key&#30340;&#20135;&#21697;&#26469;&#34917;&#20607;&#20943;&#23569;Attention&#31383;&#21475;&#38271;&#24230;&#20135;&#29983;&#30340;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformers have shown dominant performance across a range of domains including language and vision. However, their computational cost grows quadratically with the sequence length, making their usage prohibitive for resource-constrained applications. To counter this, our approach is to divide the whole sequence into segments. The information across segments can then be aggregated using neurons with recurrence leveraging their inherent memory. Such an approach leads to models with sequential processing capability at a lower computation/memory cost. To investigate this idea, first, we examine the effects of using local attention mechanism on the individual segments. Then we propose a segmented recurrent transformer (SRformer) that combines segmented attention with recurrent attention. It uses recurrent accumulate and fire (RAF) layers to process information between consecutive segments. The loss caused by reducing the attention window length is compensated by updating the product of key
&lt;/p&gt;</description></item><item><title>&#22312;&#22810;&#35821;&#35328;&#29615;&#22659;&#19979;&#65292;GPT-3&#34920;&#29616;&#36739;&#24046;&#65292;&#29305;&#21035;&#26159;&#24403;&#38382;&#39064;&#19981;&#26159;&#29992;&#33521;&#35821;&#25552;&#20986;&#26102;&#12290;&#36825;&#19982;&#27169;&#22411;&#30340;&#35757;&#32451;&#25968;&#25454;&#21644;&#36755;&#20837;&#38382;&#39064;&#30340;&#35821;&#35328;&#24046;&#24322;&#26377;&#20851;&#12290;</title><link>http://arxiv.org/abs/2305.16339</link><description>&lt;p&gt;
&#24403;&#38382;&#39064;&#19981;&#26159;&#29992;&#33521;&#35821;&#25552;&#20986;&#26102;&#65292;&#19981;&#35201;&#23436;&#20840;&#20449;&#20219;GPT
&lt;/p&gt;
&lt;p&gt;
Don't Trust GPT When Your Question Is Not In English. (arXiv:2305.16339v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16339
&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#35821;&#35328;&#29615;&#22659;&#19979;&#65292;GPT-3&#34920;&#29616;&#36739;&#24046;&#65292;&#29305;&#21035;&#26159;&#24403;&#38382;&#39064;&#19981;&#26159;&#29992;&#33521;&#35821;&#25552;&#20986;&#26102;&#12290;&#36825;&#19982;&#27169;&#22411;&#30340;&#35757;&#32451;&#25968;&#25454;&#21644;&#36755;&#20837;&#38382;&#39064;&#30340;&#35821;&#35328;&#24046;&#24322;&#26377;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#65292;&#24182;&#22312;&#22810;&#20010;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#23613;&#31649;&#22823;&#22810;&#25968;LLMs&#20027;&#35201;&#20351;&#29992;&#33521;&#35821;&#36827;&#34892;&#35757;&#32451;&#65292;&#20294;&#22810;&#39033;&#30740;&#31350;&#24050;&#32463;&#35777;&#26126;&#20102;&#23427;&#20204;&#22312;&#35768;&#22810;&#20854;&#20182;&#35821;&#35328;&#20013;&#30340;&#30456;&#23545;&#34920;&#29616;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;LLMs&#22914;&#20309;&#33719;&#24471;&#23427;&#20204;&#30340;&#22810;&#35821;&#35328;&#33021;&#21147;&#20197;&#21450;&#34920;&#29616;&#22312;&#19981;&#21516;&#35821;&#35328;&#20013;&#30340;&#24046;&#24322;&#20173;&#28982;&#23384;&#22312;&#22522;&#26412;&#38382;&#39064;&#12290;&#36825;&#20123;&#38382;&#39064;&#23545;LLMs&#30340;&#30740;&#31350;&#38750;&#24120;&#20851;&#38190;&#65292;&#22240;&#20026;&#29992;&#25143;&#21644;&#30740;&#31350;&#20154;&#21592;&#36890;&#24120;&#26469;&#33258;&#22810;&#31181;&#35821;&#35328;&#32972;&#26223;&#65292;&#21487;&#33021;&#24433;&#21709;&#20182;&#20204;&#23545;LLMs&#32467;&#26524;&#30340;&#21033;&#29992;&#21644;&#35299;&#37322;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31995;&#32479;&#30340;&#26041;&#27861;&#65292;&#20197;&#23450;&#24615;&#35780;&#20272;&#22810;&#35821;&#35328;&#29615;&#22659;&#19979;LLMs&#30340;&#34920;&#29616;&#24046;&#24322;&#12290;&#25105;&#20204;&#35843;&#26597;&#20102;LLMs&#22312;&#36328;&#35821;&#35328;&#27867;&#21270;&#29616;&#35937;&#26041;&#38754;&#30340;&#34920;&#29616;&#65292;&#21363;&#19981;&#20805;&#36275;&#30340;&#22810;&#35821;&#35328;&#35757;&#32451;&#25968;&#25454;&#23548;&#33268;&#20808;&#36827;&#30340;&#22810;&#35821;&#35328;&#33021;&#21147;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#30340;&#65292;&#25105;&#20204;&#23545;&#19968;&#31995;&#21015;&#35821;&#35328;&#36827;&#34892;&#20102;GPT-3&#30340;&#23454;&#39564;&#65292;&#36825;&#20123;&#35821;&#35328;&#28085;&#30422;&#20102;&#20174;&#21360;&#27431;&#35821;&#31995;&#21040;&#38750;&#21360;&#27431;&#35821;&#31995;&#30340;&#21508;&#31181;&#35821;&#35328;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#20272;&#21644;&#39564;&#35777;&#32467;&#26524;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#21363;&#20351;&#27169;&#22411;&#22312;&#35813;&#35821;&#35328;&#19978;&#36827;&#34892;&#20102;&#24494;&#35843;&#65292;&#20294;&#22914;&#26524;&#36755;&#20837;&#38382;&#39064;&#19981;&#26159;&#33521;&#35821;&#65292;GPT-3&#22312;&#20854;&#20182;&#35821;&#35328;&#19979;&#30340;&#34920;&#29616;&#26174;&#33879;&#36739;&#24046;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#27169;&#22411;&#30340;&#34920;&#29616;&#19981;&#20339;&#19982;&#35757;&#32451;&#35821;&#35328;&#21644;&#36755;&#20837;&#38382;&#39064;&#30340;&#35821;&#35328;&#24046;&#24322;&#26377;&#20851;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#36827;&#34892;&#38750;&#33521;&#35821;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#26102;&#65292;&#38656;&#35201;&#35880;&#24910;&#20351;&#29992;LLMs&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have demonstrated exceptional natural language understanding abilities and have excelled in a variety of natural language processing (NLP)tasks in recent years. Despite the fact that most LLMs are trained predominantly in English, multiple studies have demonstrated their comparative performance in many other languages. However, fundamental questions persist regarding how LLMs acquire their multi-lingual abilities and how performance varies across different languages. These inquiries are crucial for the study of LLMs since users and researchers often come from diverse language backgrounds, potentially influencing their utilization and interpretation of LLMs' results. In this work, we propose a systematic way of qualifying the performance disparities of LLMs under multilingual settings. We investigate the phenomenon of across-language generalizations in LLMs, wherein insufficient multi-lingual training data leads to advanced multi-lingual capabilities. To acc
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20855;&#26377;&#20869;&#37096;&#24037;&#20316;&#35760;&#24518;&#27169;&#22359;&#30340;&#20915;&#31574;Transformer&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20915;&#31574;&#20195;&#29702;&#22312;&#22788;&#29702;&#26032;&#20219;&#21153;&#19978;&#24615;&#33021;&#20302;&#19979;&#30340;&#38382;&#39064;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#25913;&#21892;&#20102;&#35757;&#32451;&#25928;&#29575;&#21644;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#36827;&#19968;&#27493;&#22686;&#24378;&#20102;&#36716;&#21270;&#20915;&#31574;&#21046;&#23450;&#20195;&#29702;&#23545;&#26032;&#20219;&#21153;&#30340;&#36866;&#24212;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.16338</link><description>&lt;p&gt;
&#28145;&#24605;&#29087;&#34385;&#65306;&#20855;&#26377;&#20869;&#37096;&#24037;&#20316;&#35760;&#24518;&#30340;&#20915;&#31574;Transformer
&lt;/p&gt;
&lt;p&gt;
Think Before You Act: Decision Transformers with Internal Working Memory. (arXiv:2305.16338v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16338
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20855;&#26377;&#20869;&#37096;&#24037;&#20316;&#35760;&#24518;&#27169;&#22359;&#30340;&#20915;&#31574;Transformer&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20915;&#31574;&#20195;&#29702;&#22312;&#22788;&#29702;&#26032;&#20219;&#21153;&#19978;&#24615;&#33021;&#20302;&#19979;&#30340;&#38382;&#39064;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#25913;&#21892;&#20102;&#35757;&#32451;&#25928;&#29575;&#21644;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#36827;&#19968;&#27493;&#22686;&#24378;&#20102;&#36716;&#21270;&#20915;&#31574;&#21046;&#23450;&#20195;&#29702;&#23545;&#26032;&#20219;&#21153;&#30340;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#20915;&#31574;&#21046;&#23450;&#20195;&#29702;&#24050;&#32463;&#23637;&#31034;&#20102;&#36328;&#36234;&#22810;&#20010;&#20219;&#21153;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#20381;&#36182;&#20110;&#22823;&#35268;&#27169;&#30340;&#25968;&#25454;&#21644;&#35745;&#31639;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#36825;&#31181;&#20302;&#25928;&#24615;&#28304;&#20110;&#36951;&#24536;&#29616;&#35937;&#65292;&#21363;&#27169;&#22411;&#36890;&#36807;&#21442;&#25968;&#35760;&#24518;&#20854;&#34892;&#20026;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#12290;&#22240;&#27492;&#65292;&#26032;&#20219;&#21153;&#30340;&#35757;&#32451;&#21487;&#33021;&#20250;&#38477;&#20302;&#27169;&#22411;&#22312;&#20808;&#21069;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;&#19982;LLM&#30340;&#38544;&#24335;&#35760;&#24518;&#26426;&#21046;&#19981;&#21516;&#65292;&#20154;&#33041;&#21033;&#29992;&#20998;&#24067;&#24335;&#23384;&#20648;&#22120;&#23384;&#20648;&#35760;&#24518;&#65292;&#20197;&#26377;&#25928;&#22320;&#31649;&#29702;&#21644;&#32452;&#32455;&#22810;&#31181;&#25216;&#33021;&#65292;&#20943;&#36731;&#20102;&#36951;&#24536;&#29616;&#35937;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24314;&#35758;&#20351;&#29992;&#20869;&#37096;&#24037;&#20316;&#35760;&#24518;&#27169;&#22359;&#26469;&#23384;&#20648;&#12289;&#34701;&#21512;&#21644;&#26816;&#32034;&#19981;&#21516;&#19979;&#28216;&#20219;&#21153;&#30340;&#20449;&#24687;&#12290;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#25913;&#21892;&#20102;Atari&#28216;&#25103;&#21644;&#20803;&#19990;&#30028;&#29289;&#20307;&#25805;&#20316;&#20219;&#21153;&#30340;&#35757;&#32451;&#25928;&#29575;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#35760;&#24518;&#24494;&#35843;&#36827;&#19968;&#27493;&#22686;&#24378;&#20102;&#36716;&#21270;&#20915;&#31574;&#21046;&#23450;&#20195;&#29702;&#23545;&#26032;&#20219;&#21153;&#30340;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language model (LLM)-based decision-making agents have shown the ability to generalize across multiple tasks. However, their performance relies on massive data and compute. We argue that this inefficiency stems from the forgetting phenomenon, in which a model memorizes its behaviors in parameters throughout training. As a result, training on a new task may deteriorate the model's performance on previous tasks. In contrast to LLMs' implicit memory mechanism, the human brain utilizes distributed memory storage, which helps manage and organize multiple skills efficiently, mitigating the forgetting phenomenon. Thus inspired, we propose an internal working memory module to store, blend, and retrieve information for different downstream tasks. Evaluation results show that the proposed method improves training efficiency and generalization in both Atari games and meta-world object manipulation tasks. Moreover, we demonstrate that memory fine-tuning further enhances the adaptability of t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;BERT&#22312;&#29616;&#23454;&#26631;&#31614;&#22122;&#22768;&#23384;&#22312;&#19979;&#30340;&#20998;&#31867;&#24615;&#33021;&#65292;&#21457;&#29616;&#29305;&#24449;&#30456;&#20851;&#30340;&#26631;&#31614;&#22122;&#22768;&#21644;&#26469;&#33258;&#27880;&#37322;&#32773;&#20998;&#27495;&#30340;&#21512;&#25104;&#26631;&#31614;&#22122;&#22768;&#20250;&#23548;&#33268;BERT&#30340;&#20998;&#31867;&#24615;&#33021;&#19979;&#38477;&#12290;&#25552;&#20986;&#19981;&#21516;&#31867;&#22411;&#30340;&#38598;&#25104;&#21644;&#22122;&#22768;&#28165;&#29702;&#26041;&#27861;&#20197;&#25552;&#39640;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.16337</link><description>&lt;p&gt;
&#22788;&#29702;BERT&#25991;&#26412;&#20998;&#31867;&#20013;&#30340;&#29616;&#23454;&#26631;&#31614;&#22122;&#22768;
&lt;/p&gt;
&lt;p&gt;
Handling Realistic Label Noise in BERT Text Classification. (arXiv:2305.16337v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16337
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;BERT&#22312;&#29616;&#23454;&#26631;&#31614;&#22122;&#22768;&#23384;&#22312;&#19979;&#30340;&#20998;&#31867;&#24615;&#33021;&#65292;&#21457;&#29616;&#29305;&#24449;&#30456;&#20851;&#30340;&#26631;&#31614;&#22122;&#22768;&#21644;&#26469;&#33258;&#27880;&#37322;&#32773;&#20998;&#27495;&#30340;&#21512;&#25104;&#26631;&#31614;&#22122;&#22768;&#20250;&#23548;&#33268;BERT&#30340;&#20998;&#31867;&#24615;&#33021;&#19979;&#38477;&#12290;&#25552;&#20986;&#19981;&#21516;&#31867;&#22411;&#30340;&#38598;&#25104;&#21644;&#22122;&#22768;&#28165;&#29702;&#26041;&#27861;&#20197;&#25552;&#39640;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26631;&#31614;&#22122;&#22768;&#26159;&#30001;&#20110;&#24265;&#20215;&#30340;&#25968;&#25454;&#26631;&#27880;&#26041;&#27861;&#65288;&#22914;&#32593;&#32476;&#29228;&#21462;&#25110;&#20247;&#21253;&#65289;&#23548;&#33268;&#30340;&#35757;&#32451;&#26631;&#31614;&#20013;&#30340;&#38169;&#35823;&#65292;&#36825;&#21487;&#33021;&#23545;&#30417;&#30563;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#26377;&#23475;&#12290;&#24050;&#32463;&#25552;&#20986;&#20102;&#20960;&#31181;&#26041;&#27861;&#26469;&#25269;&#28040;&#26377;&#30417;&#30563;&#20998;&#31867;&#20013;&#38543;&#26426;&#26631;&#31614;&#22122;&#22768;&#30340;&#24433;&#21709;&#65292;&#24182;&#19988;&#19968;&#20123;&#30740;&#31350;&#24050;&#32463;&#34920;&#26126;BERT&#24050;&#32463;&#23545;&#39640;&#27604;&#29575;&#30340;&#38543;&#26426;&#27880;&#20837;&#26631;&#31614;&#22122;&#22768;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#28982;&#32780;&#65292;&#30495;&#23454;&#30340;&#26631;&#31614;&#22122;&#22768;&#24182;&#19981;&#26159;&#38543;&#26426;&#30340;&#65292;&#32780;&#26159;&#32463;&#24120;&#19982;&#36755;&#20837;&#29305;&#24449;&#25110;&#20854;&#20182;&#27880;&#37322;&#32773;&#29305;&#23450;&#22240;&#32032;&#30456;&#20851;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;BERT&#22312;&#38754;&#23545;&#20004;&#31181;&#31867;&#22411;&#30340;&#29616;&#23454;&#26631;&#31614;&#22122;&#22768;&#65306;&#29305;&#24449;&#30456;&#20851;&#30340;&#26631;&#31614;&#22122;&#22768;&#21644;&#26469;&#33258;&#27880;&#37322;&#32773;&#20998;&#27495;&#30340;&#21512;&#25104;&#26631;&#31614;&#22122;&#22768;&#12290;&#25105;&#20204;&#34920;&#26126;&#36825;&#20123;&#31867;&#22411;&#22122;&#22768;&#30340;&#23384;&#22312;&#26174;&#33879;&#38477;&#20302;&#20102;BERT&#20998;&#31867;&#24615;&#33021;&#12290;&#20026;&#20102;&#25552;&#39640;&#20854;&#40065;&#26834;&#24615;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#19981;&#21516;&#31867;&#22411;&#30340;&#38598;&#25104;&#21644;&#22122;&#22768;&#28165;&#29702;&#26041;&#27861;&#65292;&#24182;&#27604;&#36739;&#23427;&#20204;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#20013;&#23545;&#26631;&#31614;&#22122;&#22768;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Labels noise refers to errors in training labels caused by cheap data annotation methods, such as web scraping or crowd-sourcing, which can be detrimental to the performance of supervised classifiers. Several methods have been proposed to counteract the effect of random label noise in supervised classification, and some studies have shown that BERT is already robust against high rates of randomly injected label noise. However, real label noise is not random; rather, it is often correlated with input features or other annotator-specific factors. In this paper, we evaluate BERT in the presence of two types of realistic label noise: feature-dependent label noise, and synthetic label noise from annotator disagreements. We show that the presence of these types of noise significantly degrades BERT classification performance. To improve robustness, we evaluate different types of ensembles and noise-cleaning methods and compare their effectiveness against label noise across different datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20581;&#22766;&#30701;&#25991;&#26412;&#32858;&#31867;&#65288;RSTC&#65289;&#27169;&#22411;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#26368;&#20248;&#36755;&#36816;&#30340;&#20266;&#26631;&#31614;&#29983;&#25104;&#65292;&#20197;&#21450;&#22522;&#20110;&#31867;&#21644;&#23454;&#20363;&#30340;&#23545;&#27604;&#23398;&#20064;&#30340;&#20581;&#22766;&#34920;&#31034;&#23398;&#20064;&#65292;&#24110;&#21161;&#25552;&#39640;&#23545;&#19981;&#24179;&#34913;&#21644;&#22122;&#38899;&#25968;&#25454;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.16335</link><description>&lt;p&gt;
&#22522;&#20110;&#33258;&#36866;&#24212;&#26368;&#20248;&#36755;&#36816;&#30340;&#20581;&#22766;&#30701;&#25991;&#26412;&#32858;&#31867;&#20013;&#21487;&#38752;&#20266;&#26631;&#31614;&#30340;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Robust Representation Learning with Reliable Pseudo-labels Generation via Self-Adaptive Optimal Transport for Short Text Clustering. (arXiv:2305.16335v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16335
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20581;&#22766;&#30701;&#25991;&#26412;&#32858;&#31867;&#65288;RSTC&#65289;&#27169;&#22411;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#26368;&#20248;&#36755;&#36816;&#30340;&#20266;&#26631;&#31614;&#29983;&#25104;&#65292;&#20197;&#21450;&#22522;&#20110;&#31867;&#21644;&#23454;&#20363;&#30340;&#23545;&#27604;&#23398;&#20064;&#30340;&#20581;&#22766;&#34920;&#31034;&#23398;&#20064;&#65292;&#24110;&#21161;&#25552;&#39640;&#23545;&#19981;&#24179;&#34913;&#21644;&#22122;&#38899;&#25968;&#25454;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30701;&#25991;&#26412;&#32858;&#31867;&#22240;&#36755;&#20837;&#30340;&#19981;&#24179;&#34913;&#21644;&#22122;&#38899;&#25968;&#25454;&#32780;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#29616;&#26377;&#26041;&#27861;&#26080;&#27861;&#24456;&#22909;&#22320;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#22240;&#20026;&#23427;&#20204;&#23481;&#26131;&#22312;&#37325;&#24230;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#36864;&#21270;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#19988;&#26131;&#21463;&#21040;&#22122;&#22768;&#24178;&#25200;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20581;&#22766;&#30701;&#25991;&#26412;&#32858;&#31867;&#65288;RSTC&#65289;&#27169;&#22411;&#65292;&#20197;&#25552;&#39640;&#23545;&#19981;&#24179;&#34913;&#21644;&#22122;&#38899;&#25968;&#25454;&#30340;&#40065;&#26834;&#24615;&#12290;RSTC&#21253;&#25324;&#20004;&#20010;&#27169;&#22359;&#65292;&#21363;&#20266;&#26631;&#35760;&#29983;&#25104;&#27169;&#22359;&#21644;&#20581;&#22766;&#34920;&#31034;&#23398;&#20064;&#27169;&#22359;&#12290;&#21069;&#32773;&#29983;&#25104;&#20266;&#26631;&#35760;&#65292;&#20026;&#21518;&#32773;&#25552;&#20379;&#30417;&#30563;&#65292;&#26377;&#21161;&#20110;&#26356;&#20581;&#22766;&#30340;&#34920;&#31034;&#21644;&#27491;&#30830;&#20998;&#31163;&#30340;&#32858;&#31867;&#12290;&#20026;&#20102;&#25552;&#20379;&#23545;&#25968;&#25454;&#19981;&#24179;&#34913;&#30340;&#40065;&#26834;&#24615;&#65292;&#22312;&#20266;&#26631;&#31614;&#29983;&#25104;&#27169;&#22359;&#20013;&#25552;&#20986;&#20102;&#33258;&#36866;&#24212;&#26368;&#20248;&#36755;&#36816;&#12290;&#20026;&#20102;&#25552;&#39640;&#23545;&#25968;&#25454;&#20013;&#22122;&#22768;&#30340;&#40065;&#26834;&#24615;&#65292;&#22312;&#20581;&#22766;&#34920;&#31034;&#23398;&#20064;&#27169;&#22359;&#20013;&#36827;&#19968;&#27493;&#24341;&#20837;&#20102;&#22522;&#20110;&#31867;&#21644;&#23454;&#20363;&#30340;&#23545;&#27604;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Short text clustering is challenging since it takes imbalanced and noisy data as inputs. Existing approaches cannot solve this problem well, since (1) they are prone to obtain degenerate solutions especially on heavy imbalanced datasets, and (2) they are vulnerable to noises. To tackle the above issues, we propose a Robust Short Text Clustering (RSTC) model to improve robustness against imbalanced and noisy data. RSTC includes two modules, i.e., pseudo-label generation module and robust representation learning module. The former generates pseudo-labels to provide supervision for the later, which contributes to more robust representations and correctly separated clusters. To provide robustness against the imbalance in data, we propose self-adaptive optimal transport in the pseudo-label generation module. To improve robustness against the noise in data, we further introduce both class-wise and instance-wise contrastive learning in the robust representation learning module. Our empirical 
&lt;/p&gt;</description></item><item><title>OlaGPT&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#26234;&#33021;&#26694;&#26550;&#65292;&#33021;&#22815;&#27169;&#25311;&#20154;&#31867;&#22312;&#35299;&#20915;&#22797;&#26434;&#25512;&#29702;&#38382;&#39064;&#26102;&#25152;&#37319;&#29992;&#30340;&#21508;&#31181;&#35748;&#30693;&#33021;&#21147;&#21644;&#19982;&#24037;&#20855;&#12289;&#30693;&#35782;&#21644;&#22806;&#37096;&#29615;&#22659;&#20449;&#24687;&#30340;&#20132;&#20114;&#65292;&#21487;&#20197;&#35753;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20855;&#22791;&#31867;&#20154;&#30340;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.16334</link><description>&lt;p&gt;
OlaGPT&#65306;&#35753;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20855;&#22791;&#31867;&#20154;&#30340;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
OlaGPT: Empowering LLMs With Human-like Problem-Solving Abilities. (arXiv:2305.16334v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16334
&lt;/p&gt;
&lt;p&gt;
OlaGPT&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#26234;&#33021;&#26694;&#26550;&#65292;&#33021;&#22815;&#27169;&#25311;&#20154;&#31867;&#22312;&#35299;&#20915;&#22797;&#26434;&#25512;&#29702;&#38382;&#39064;&#26102;&#25152;&#37319;&#29992;&#30340;&#21508;&#31181;&#35748;&#30693;&#33021;&#21147;&#21644;&#19982;&#24037;&#20855;&#12289;&#30693;&#35782;&#21644;&#22806;&#37096;&#29615;&#22659;&#20449;&#24687;&#30340;&#20132;&#20114;&#65292;&#21487;&#20197;&#35753;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20855;&#22791;&#31867;&#20154;&#30340;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#22823;&#22810;&#25968;&#30740;&#31350;&#20013;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#29305;&#23450;&#25552;&#31034;&#30340;&#25351;&#23548;&#19979;&#36890;&#36807;&#29983;&#25104;&#24605;&#32500;&#38142;&#26469;&#25191;&#34892;&#25512;&#29702;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#35299;&#20915;&#22797;&#26434;&#25512;&#29702;&#38382;&#39064;&#19978;&#19982;&#20154;&#31867;&#30340;&#33021;&#21147;&#23384;&#22312;&#26174;&#33879;&#24046;&#36317;&#12290;&#30446;&#21069;&#65292;&#22823;&#22810;&#25968;&#26041;&#27861;&#37117;&#19987;&#27880;&#20110;&#24605;&#32500;&#38142;&#65288;COT&#65289;&#21644;&#24037;&#20855;&#20351;&#29992;&#65292;&#32780;&#24573;&#35270;&#20102;&#37319;&#29992;&#21644;&#24212;&#29992;&#20154;&#31867;&#35748;&#30693;&#26694;&#26550;&#30340;&#37325;&#35201;&#24615;&#12290;&#20154;&#20204;&#36890;&#24120;&#22312;&#38754;&#23545;&#22797;&#26434;&#30340;&#25512;&#29702;&#25361;&#25112;&#26102;&#65292;&#20250;&#36816;&#29992;&#21508;&#31181;&#35748;&#30693;&#33021;&#21147;&#65292;&#24182;&#38656;&#35201;&#19982;&#24037;&#20855;&#12289;&#30693;&#35782;&#21644;&#22806;&#37096;&#29615;&#22659;&#20449;&#24687;&#30340;&#25152;&#26377;&#26041;&#38754;&#36827;&#34892;&#20132;&#20114;&#65292;&#25165;&#33021;&#23436;&#25104;&#22797;&#26434;&#30340;&#20219;&#21153;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26234;&#33021;&#26694;&#26550;&#65292;&#31216;&#20026;OlaGPT&#12290;OlaGPT&#20180;&#32454;&#30740;&#31350;&#20102;&#35748;&#30693;&#26550;&#26500;&#26694;&#26550;&#65292;&#24182;&#25552;&#20986;&#27169;&#25311;&#20154;&#31867;&#35748;&#30693;&#30340;&#26576;&#20123;&#26041;&#38754;&#12290;&#35813;&#26694;&#26550;&#28041;&#21450;&#36817;&#20284;&#19981;&#21516;&#30340;&#35748;&#30693;&#27169;&#22359;&#65292;&#21253;&#25324;&#20851;&#27880;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In most current research, large language models (LLMs) are able to perform reasoning tasks by generating chains of thought through the guidance of specific prompts. However, there still exists a significant discrepancy between their capability in solving complex reasoning problems and that of humans. At present, most approaches focus on chains of thought (COT) and tool use, without considering the adoption and application of human cognitive frameworks. It is well-known that when confronting complex reasoning challenges, humans typically employ various cognitive abilities, and necessitate interaction with all aspects of tools, knowledge, and the external environment information to accomplish intricate tasks. This paper introduces a novel intelligent framework, referred to as OlaGPT. OlaGPT carefully studied a cognitive architecture framework, and propose to simulate certain aspects of human cognition. The framework involves approximating different cognitive modules, including attention,
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#25991;&#26412;&#22686;&#24191;&#23545;ASR&#30340;&#24433;&#21709;&#65292;&#20351;&#29992;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#26469;&#29983;&#25104;&#21512;&#25104;&#25991;&#26412;&#65292;&#24182;&#36890;&#36807;&#25991;&#26412;&#21040;&#35821;&#38899;&#31995;&#32479;&#36716;&#25442;&#20026;&#21512;&#25104;&#35821;&#38899;&#65292;&#23454;&#39564;&#21457;&#29616;&#65292;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#30340;&#25991;&#26412;&#22686;&#24191;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#25552;&#39640;ASR&#20934;&#30830;&#24230;&#65292;&#21487;&#20197;&#20316;&#20026;&#25913;&#36827;ASR&#31995;&#32479;&#30340;&#19968;&#31181;&#21487;&#34892;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2305.16333</link><description>&lt;p&gt;
&#22522;&#20110;&#35821;&#38899;&#21512;&#25104;&#30340;&#25991;&#26412;&#29983;&#25104;&#29992;&#20110;ASR&#25968;&#25454;&#22686;&#24191;
&lt;/p&gt;
&lt;p&gt;
Text Generation with Speech Synthesis for ASR Data Augmentation. (arXiv:2305.16333v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16333
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#25991;&#26412;&#22686;&#24191;&#23545;ASR&#30340;&#24433;&#21709;&#65292;&#20351;&#29992;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#26469;&#29983;&#25104;&#21512;&#25104;&#25991;&#26412;&#65292;&#24182;&#36890;&#36807;&#25991;&#26412;&#21040;&#35821;&#38899;&#31995;&#32479;&#36716;&#25442;&#20026;&#21512;&#25104;&#35821;&#38899;&#65292;&#23454;&#39564;&#21457;&#29616;&#65292;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#30340;&#25991;&#26412;&#22686;&#24191;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#25552;&#39640;ASR&#20934;&#30830;&#24230;&#65292;&#21487;&#20197;&#20316;&#20026;&#25913;&#36827;ASR&#31995;&#32479;&#30340;&#19968;&#31181;&#21487;&#34892;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#20943;&#23569;&#23545;&#26114;&#36149;&#20154;&#24037;&#27880;&#37322;&#30340;&#20381;&#36182;&#65292;&#25968;&#25454;&#22686;&#24191;&#19968;&#30452;&#26159;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#39046;&#22495;&#30340;&#19968;&#20010;&#28909;&#38376;&#30740;&#31350;&#26041;&#21521;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#20391;&#37325;&#20110;&#29992;&#20110;ASR&#25968;&#25454;&#22686;&#24191;&#30340;&#21512;&#25104;&#35821;&#38899;&#29983;&#25104;&#65292;&#32780;&#20854;&#19982;&#25991;&#26412;&#29983;&#25104;&#26041;&#27861;&#30340;&#32467;&#21512;&#21364;&#30456;&#23545;&#36739;&#23569;&#25506;&#32034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#25506;&#32034;&#25991;&#26412;&#22686;&#24191;&#23545;ASR&#30340;&#24433;&#21709;&#65292;&#24182;&#23558;&#20854;&#19982;&#20256;&#32479;&#25991;&#26412;&#22686;&#24191;&#26041;&#27861;&#36827;&#34892;&#20102;&#31995;&#32479;&#27604;&#36739;&#12290;&#29983;&#25104;&#30340;&#21512;&#25104;&#25991;&#26412;&#28982;&#21518;&#36890;&#36807;&#25991;&#26412;&#21040;&#35821;&#38899;&#65288;TTS&#65289;&#31995;&#32479;&#36716;&#25442;&#20026;&#21512;&#25104;&#35821;&#38899;&#65292;&#24182;&#28155;&#21152;&#21040;ASR&#35757;&#32451;&#25968;&#25454;&#20013;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#21457;&#29616;&#65292;&#31070;&#32463;&#27169;&#22411;&#23454;&#29616;&#20102;9&#65285;-15&#65285;&#30340;&#30456;&#23545;WER&#25913;&#36827;&#65292;&#24182;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65292;&#29305;&#21035;&#26159;&#36890;&#36807;&#29616;&#20195;&#31070;&#32463;&#26041;&#27861;&#65292;&#25991;&#26412;&#22686;&#24191;&#26159;&#25552;&#39640;ASR&#31995;&#32479;&#20934;&#30830;&#24615;&#30340;&#19968;&#31181;&#21487;&#34892;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
Aiming at reducing the reliance on expensive human annotations, data synthesis for Automatic Speech Recognition (ASR) has remained an active area of research. While prior work mainly focuses on synthetic speech generation for ASR data augmentation, its combination with text generation methods is considerably less explored. In this work, we explore text augmentation for ASR using large-scale pre-trained neural networks, and systematically compare those to traditional text augmentation methods. The generated synthetic texts are then converted to synthetic speech using a text-to-speech (TTS) system and added to the ASR training data. In experiments conducted on three datasets, we find that neural models achieve 9%-15% relative WER improvement and outperform traditional methods. We conclude that text augmentation, particularly through modern neural approaches, is a viable tool for improving the accuracy of ASR systems.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;GPT-3&#21644;GPT-4&#22312;&#29983;&#29289;&#21307;&#23398;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#34920;&#29616;&#65292;&#20998;&#26512;&#20102;&#23427;&#20204;&#21487;&#33021;&#20135;&#29983;&#30340;&#38169;&#35823;&#31867;&#22411;&#65292;&#24182;&#25552;&#20379;&#20102;&#20351;&#29992;&#36825;&#20123;&#27169;&#22411;&#30340;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2305.16326</link><description>&lt;p&gt;
&#29983;&#29289;&#21307;&#23398;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;: &#22522;&#20934;&#12289;&#22522;&#32447;&#21644;&#24314;&#35758;
&lt;/p&gt;
&lt;p&gt;
Large language models in biomedical natural language processing: benchmarks, baselines, and recommendations. (arXiv:2305.16326v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16326
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;GPT-3&#21644;GPT-4&#22312;&#29983;&#29289;&#21307;&#23398;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#34920;&#29616;&#65292;&#20998;&#26512;&#20102;&#23427;&#20204;&#21487;&#33021;&#20135;&#29983;&#30340;&#38169;&#35823;&#31867;&#22411;&#65292;&#24182;&#25552;&#20379;&#20102;&#20351;&#29992;&#36825;&#20123;&#27169;&#22411;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#29289;&#21307;&#23398;&#25991;&#29486;&#21576;&#25351;&#25968;&#32423;&#22686;&#38271;&#65292;&#25163;&#21160;&#31579;&#36873;&#21644;&#25552;&#21462;&#30693;&#35782;&#21464;&#24471;&#22256;&#38590;&#12290;&#33258;&#21160;&#20174;&#29983;&#29289;&#21307;&#23398;&#25991;&#29486;&#20013;&#25552;&#21462;&#20449;&#24687;&#30340;&#29983;&#29289;&#21307;&#23398;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;BioNLP&#65289;&#25216;&#26415;&#26377;&#21161;&#20110;&#20943;&#36731;&#36825;&#31181;&#36127;&#25285;&#12290;&#36817;&#24180;&#26469;&#65292;&#22914;GPT-3&#21644;GPT-4&#31561;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22240;&#20854;&#21331;&#36234;&#30340;&#24615;&#33021;&#32780;&#21463;&#21040;&#37325;&#35270;&#12290;&#20294;&#26159;&#65292;&#23427;&#20204;&#22312;BioNLP&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#20197;&#21450;&#23545;&#26041;&#27861;&#24320;&#21457;&#21644;&#19979;&#28216;&#29992;&#25143;&#30340;&#24433;&#21709;&#20173;&#26410;&#24471;&#21040;&#30740;&#31350;&#12290;&#26412;&#30740;&#31350;&#65288;1&#65289;&#22312;&#22235;&#20010;&#24212;&#29992;&#31243;&#24207;&#20013;&#22312;&#20843;&#20010;BioNLP&#25968;&#25454;&#38598;&#20013;&#24314;&#31435;&#20102;GPT-3&#21644;GPT-4&#22312;&#38646;-shot&#21644;&#19968;-shot&#35774;&#32622;&#19979;&#30340;&#22522;&#20934;&#34920;&#29616;&#65292;&#21253;&#25324;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65292;&#20851;&#31995;&#25552;&#21462;&#65292;&#22810;&#26631;&#31614;&#25991;&#26723;&#20998;&#31867;&#21644;&#35821;&#20041;&#30456;&#20284;&#24615;&#21644;&#25512;&#29702;&#65307;&#65288;2&#65289;&#23457;&#26597;&#20102;LLMs&#20135;&#29983;&#30340;&#38169;&#35823;&#65292;&#24182;&#23558;&#38169;&#35823;&#20998;&#20026;&#19977;&#31181;&#31867;&#22411;&#65306;&#32570;&#22833;&#65292;&#19981;&#19968;&#33268;&#21644;&#19981;&#38656;&#35201;&#30340;&#20154;&#24037;&#20869;&#23481;&#65307;&#65288;3&#65289;&#25552;&#20986;&#20102;&#20351;&#29992;LLMs&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
Biomedical literature is growing rapidly, making it challenging to curate and extract knowledge manually. Biomedical natural language processing (BioNLP) techniques that can automatically extract information from biomedical literature help alleviate this burden. Recently, large Language Models (LLMs), such as GPT-3 and GPT-4, have gained significant attention for their impressive performance. However, their effectiveness in BioNLP tasks and impact on method development and downstream users remain understudied. This pilot study (1) establishes the baseline performance of GPT-3 and GPT-4 at both zero-shot and one-shot settings in eight BioNLP datasets across four applications: named entity recognition, relation extraction, multi-label document classification, and semantic similarity and reasoning, (2) examines the errors produced by the LLMs and categorized the errors into three types: missingness, inconsistencies, and unwanted artificial content, and (3) provides suggestions for using L
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24490;&#29615;&#30697;&#38453;&#21644;&#26684;&#25289;&#22982;&#36845;&#20195;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#39640;&#25928;&#20272;&#35745;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;Lipschitz&#24120;&#25968;&#19978;&#30028;&#12290;&#35813;&#26041;&#27861;&#31934;&#30830;&#12289;&#24555;&#36895;&#12289;&#21487;&#24494;&#20998;&#65292;&#24182;&#23637;&#29616;&#20102;&#36229;&#32447;&#24615;&#25910;&#25947;&#12290;&#22312;&#23454;&#39564;&#19978;&#34920;&#29616;&#20986;&#36739;&#39640;&#30340;&#31934;&#24230;&#12289;&#35745;&#31639;&#25104;&#26412;&#21644;&#21487;&#25193;&#23637;&#24615;&#65292;&#22312;&#21033;&#26222;&#24076;&#33576;&#27491;&#21017;&#21270;&#26041;&#38754;&#20063;&#21462;&#24471;&#20102;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.16173</link><description>&lt;p&gt;
&#36890;&#36807;&#26684;&#25289;&#22982;&#36845;&#20195;&#23454;&#29616;&#21367;&#31215;&#23618;&#21033;&#26222;&#24076;&#33576;&#24120;&#25968;&#30340;&#39640;&#25928;&#36793;&#30028;
&lt;/p&gt;
&lt;p&gt;
Efficient Bound of Lipschitz Constant for Convolutional Layers by Gram Iteration. (arXiv:2305.16173v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16173
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24490;&#29615;&#30697;&#38453;&#21644;&#26684;&#25289;&#22982;&#36845;&#20195;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#39640;&#25928;&#20272;&#35745;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;Lipschitz&#24120;&#25968;&#19978;&#30028;&#12290;&#35813;&#26041;&#27861;&#31934;&#30830;&#12289;&#24555;&#36895;&#12289;&#21487;&#24494;&#20998;&#65292;&#24182;&#23637;&#29616;&#20102;&#36229;&#32447;&#24615;&#25910;&#25947;&#12290;&#22312;&#23454;&#39564;&#19978;&#34920;&#29616;&#20986;&#36739;&#39640;&#30340;&#31934;&#24230;&#12289;&#35745;&#31639;&#25104;&#26412;&#21644;&#21487;&#25193;&#23637;&#24615;&#65292;&#22312;&#21033;&#26222;&#24076;&#33576;&#27491;&#21017;&#21270;&#26041;&#38754;&#20063;&#21462;&#24471;&#20102;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#21033;&#26222;&#24076;&#33576;&#24120;&#25968;&#30340;&#25511;&#21046;&#23545;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#31283;&#23450;&#24615;&#12289;&#27867;&#21270;&#21644;&#40065;&#26834;&#24615;&#26377;&#24456;&#22823;&#24433;&#21709;&#65292;&#22240;&#27492;&#20272;&#35745;&#36825;&#20010;&#20540;&#26159;&#30446;&#21069;&#30340;&#19968;&#20010;&#31185;&#23398;&#38590;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#24490;&#29615;&#30697;&#38453;&#29702;&#35770;&#21644;&#19968;&#31181;&#26032;&#30340;&#21151;&#29575;&#36845;&#20195;&#26367;&#20195;&#26041;&#27861;&#65292;&#20171;&#32461;&#20102;&#19968;&#20010;&#31934;&#30830;&#12289;&#24555;&#36895;&#21644;&#21487;&#24494;&#20998;&#30340;&#19978;&#30028;&#65292;&#29992;&#20110;&#21367;&#31215;&#23618;&#30340;&#35889;&#33539;&#25968;&#12290;&#31216;&#20026;&#26684;&#25289;&#22982;&#36845;&#20195;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23637;&#29616;&#20102;&#19968;&#20010;&#36229;&#32447;&#24615;&#30340;&#25910;&#25947;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#36890;&#36807;&#19968;&#31995;&#21015;&#20840;&#38754;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#31934;&#24230;&#12289;&#35745;&#31639;&#25104;&#26412;&#21644;&#21487;&#20280;&#32553;&#24615;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#23427;&#23545;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#21033;&#26222;&#24076;&#33576;&#27491;&#21017;&#21270;&#38750;&#24120;&#26377;&#25928;&#65292;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#27604;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#32467;&#26524;&#12290;&#20195;&#30721;&#21487;&#22312; https://github.com/blaisedelattre/lip4conv &#19978;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Since the control of the Lipschitz constant has a great impact on the training stability, generalization, and robustness of neural networks, the estimation of this value is nowadays a real scientific challenge. In this paper we introduce a precise, fast, and differentiable upper bound for the spectral norm of convolutional layers using circulant matrix theory and a new alternative to the Power iteration. Called the Gram iteration, our approach exhibits a superlinear convergence. First, we show through a comprehensive set of experiments that our approach outperforms other state-of-the-art methods in terms of precision, computational cost, and scalability. Then, it proves highly effective for the Lipschitz regularization of convolutional neural networks, with competitive results against concurrent approaches. Code is available at https://github.com/blaisedelattre/lip4conv.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26799;&#24230;&#27491;&#21017;&#21270;&#26041;&#27861;Grug&#65292;&#26088;&#22312;&#32479;&#19968;HGNN&#20013;&#30340;&#22270;&#24418;&#25299;&#25169;&#21644;&#33410;&#28857;&#29305;&#24449;&#30340;&#27491;&#21017;&#21270;&#65292;&#24182;&#35299;&#20915;&#20102;&#36807;&#24230;&#24179;&#28369;&#12289;&#38750;&#40065;&#26834;&#24615;&#31561;&#38382;&#39064;&#65292;&#32508;&#21512;&#25928;&#26524;&#21644;&#25928;&#29575;&#20248;&#20110;&#20960;&#31181;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.15811</link><description>&lt;p&gt;
&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#26799;&#24230;&#27491;&#21017;&#21270;&#32479;&#19968;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Unifying gradient regularization for Heterogeneous Graph Neural Networks. (arXiv:2305.15811v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15811
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26799;&#24230;&#27491;&#21017;&#21270;&#26041;&#27861;Grug&#65292;&#26088;&#22312;&#32479;&#19968;HGNN&#20013;&#30340;&#22270;&#24418;&#25299;&#25169;&#21644;&#33410;&#28857;&#29305;&#24449;&#30340;&#27491;&#21017;&#21270;&#65292;&#24182;&#35299;&#20915;&#20102;&#36807;&#24230;&#24179;&#28369;&#12289;&#38750;&#40065;&#26834;&#24615;&#31561;&#38382;&#39064;&#65292;&#32508;&#21512;&#25928;&#26524;&#21644;&#25928;&#29575;&#20248;&#20110;&#20960;&#31181;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#24322;&#26500;&#22270;&#30340;&#34920;&#24449;&#12290;&#23613;&#31649;HGNN&#36805;&#36895;&#21457;&#23637;&#65292;&#20294;&#20173;&#38754;&#20020;&#36807;&#24230;&#24179;&#28369;&#21644;&#38750;&#40065;&#26834;&#24615;&#31561;&#25361;&#25112;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20351;&#29992;&#26799;&#24230;&#27491;&#21017;&#21270;&#26041;&#27861;&#21487;&#20197;&#32531;&#35299;&#36825;&#20123;&#38382;&#39064;&#65292;&#20294;&#29616;&#26377;&#30340;&#26799;&#24230;&#27491;&#21017;&#21270;&#26041;&#27861;&#19987;&#27880;&#20110;&#22270;&#24418;&#25299;&#25169;&#25110;&#33410;&#28857;&#29305;&#24449;&#65292;&#32570;&#20047;&#32479;&#19968;&#26041;&#27861;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26799;&#24230;&#27491;&#21017;&#21270;&#26041;&#27861;Grug&#65292;&#26088;&#22312;&#32479;&#19968;HGNN&#20013;&#30340;&#22270;&#24418;&#25299;&#25169;&#21644;&#33410;&#28857;&#29305;&#24449;&#30340;&#27491;&#21017;&#21270;&#65292;&#24182;&#35299;&#20915;&#20102;&#36807;&#24230;&#24179;&#28369;&#12289;&#38750;&#40065;&#26834;&#24615;&#31561;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;Grug&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#20960;&#31181;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Heterogeneous Graph Neural Networks (HGNNs) are a class of powerful deep learning methods widely used to learn representations of heterogeneous graphs. Despite the fast development of HGNNs, they still face some challenges such as over-smoothing, and non-robustness. Previous studies have shown that these problems can be reduced by using gradient regularization methods. However, the existing gradient regularization methods focus on either graph topology or node features. There is no universal approach to integrate these features, which severely affects the efficiency of regularization. In addition, the inclusion of gradient regularization into HGNNs sometimes leads to some problems, such as an unstable training process, increased complexity and insufficient coverage regularized information. Furthermore, there is still short of a complete theoretical analysis of the effects of gradient regularization on HGNNs. In this paper, we propose a novel gradient regularization method called Grug, 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#34164;&#21547;-&#30683;&#30462;&#39044;&#27979;&#26041;&#27861;&#65292;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32467;&#21512;&#65292;&#29992;&#20110;&#35299;&#20915;&#20998;&#23618;&#25968;&#25454;&#38598;&#20013;&#30340;&#38646;&#26679;&#20363;&#20998;&#31867;&#38382;&#39064;&#65292;&#25104;&#21151;&#23454;&#29616;&#20102;&#20005;&#26684;&#38646;&#26679;&#20363;&#20998;&#23618;&#20998;&#31867;&#12290;</title><link>http://arxiv.org/abs/2305.15282</link><description>&lt;p&gt;
&#19968;&#31181;&#20005;&#26684;&#38646;&#26679;&#20363;&#20998;&#23618;&#20998;&#31867;&#30340;&#31616;&#21333;&#26377;&#25928;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Simple and Effective Framework for Strict Zero-Shot Hierarchical Classification. (arXiv:2305.15282v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15282
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#34164;&#21547;-&#30683;&#30462;&#39044;&#27979;&#26041;&#27861;&#65292;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32467;&#21512;&#65292;&#29992;&#20110;&#35299;&#20915;&#20998;&#23618;&#25968;&#25454;&#38598;&#20013;&#30340;&#38646;&#26679;&#20363;&#20998;&#31867;&#38382;&#39064;&#65292;&#25104;&#21151;&#23454;&#29616;&#20102;&#20005;&#26684;&#38646;&#26679;&#20363;&#20998;&#23618;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22522;&#20934;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#38646;&#26679;&#20363;&#25110;&#23569;&#26679;&#20363;&#24773;&#20917;&#19979;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#22522;&#20934;&#27979;&#35797;&#36890;&#24120;&#26410;&#33021;&#20805;&#20998;&#35299;&#20915;&#29616;&#23454;&#19990;&#30028;&#20013;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#22914;&#20998;&#23618;&#20998;&#31867;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#23558;&#20256;&#32479;&#30340;&#20998;&#23618;&#25968;&#25454;&#38598;&#19978;&#30340;&#20219;&#21153;&#37325;&#26032;&#23450;&#20041;&#20026;&#26356;&#20855;&#25351;&#31034;&#24615;&#30340;&#38271;&#23614;&#39044;&#27979;&#20219;&#21153;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;LLMs&#22312;&#36825;&#20123;&#24773;&#20917;&#19979;&#26356;&#23481;&#26131;&#22833;&#36133;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#22312;LLMs&#30340;&#22522;&#30784;&#19978;&#20351;&#29992;&#34164;&#21547;-&#30683;&#30462;&#39044;&#27979;&#26041;&#27861;&#65292;&#22312;&#20005;&#26684;&#38646;&#26679;&#20363;&#24773;&#20917;&#19979;&#33719;&#24471;&#24378;&#22823;&#30340;&#24615;&#33021;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#38656;&#35201;&#20219;&#20309;&#21442;&#25968;&#26356;&#26032;&#65292;&#36825;&#26159;&#19968;&#31181;&#36164;&#28304;&#23494;&#38598;&#22411;&#30340;&#36807;&#31243;&#65292;&#24182;&#19988;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#20102;&#24378;&#22823;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, large language models (LLMs) have achieved strong performance on benchmark tasks, especially in zero or few-shot settings. However, these benchmarks often do not adequately address the challenges posed in the real-world, such as that of hierarchical classification. In order to address this challenge, we propose refactoring conventional tasks on hierarchical datasets into a more indicative long-tail prediction task. We observe LLMs are more prone to failure in these cases. To address these limitations, we propose the use of entailment-contradiction prediction in conjunction with LLMs, which allows for strong performance in a strict zero-shot setting. Importantly, our method does not require any parameter updates, a resource-intensive process and achieves strong performance across multiple datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22411;&#65292;&#21487;&#20381;&#25454;&#29992;&#25143;&#25552;&#20379;&#30340;&#20219;&#24847;&#32423;&#21035;&#30340;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#65292;&#29983;&#25104;&#21512;&#29702;&#19988;&#35270;&#35273;&#19978;&#20196;&#20154;&#24841;&#24742;&#30340;&#24425;&#33394;&#21270;&#25928;&#26524;&#12290;&#36890;&#36807;&#21033;&#29992;&#36328;&#27169;&#24577;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#35821;&#35328;&#29702;&#35299;&#21644;&#39068;&#33394;&#20808;&#39564;&#30693;&#35782;&#65292;&#32467;&#21512;&#26032;&#22411;&#37319;&#26679;&#31574;&#30053;&#21644;&#27169;&#22359;&#35774;&#35745;&#65292;&#23454;&#29616;&#20102;&#23454;&#20363;&#24863;&#30693;&#30340;&#24425;&#33394;&#21270;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.15217</link><description>&lt;p&gt;
L-CAD: &#24102;&#26377;&#20219;&#24847;&#32423;&#21035;&#25551;&#36848;&#30340;&#35821;&#35328;&#24425;&#33394;&#21270;
&lt;/p&gt;
&lt;p&gt;
L-CAD: Language-based Colorization with Any-level Descriptions. (arXiv:2305.15217v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15217
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22411;&#65292;&#21487;&#20381;&#25454;&#29992;&#25143;&#25552;&#20379;&#30340;&#20219;&#24847;&#32423;&#21035;&#30340;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#65292;&#29983;&#25104;&#21512;&#29702;&#19988;&#35270;&#35273;&#19978;&#20196;&#20154;&#24841;&#24742;&#30340;&#24425;&#33394;&#21270;&#25928;&#26524;&#12290;&#36890;&#36807;&#21033;&#29992;&#36328;&#27169;&#24577;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#35821;&#35328;&#29702;&#35299;&#21644;&#39068;&#33394;&#20808;&#39564;&#30693;&#35782;&#65292;&#32467;&#21512;&#26032;&#22411;&#37319;&#26679;&#31574;&#30053;&#21644;&#27169;&#22359;&#35774;&#35745;&#65292;&#23454;&#29616;&#20102;&#23454;&#20363;&#24863;&#30693;&#30340;&#24425;&#33394;&#21270;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#24425;&#33394;&#21270;&#26159;&#22312;&#29992;&#25143;&#21451;&#22909;&#30340;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#25351;&#23548;&#19979;&#29983;&#25104;&#21512;&#29702;&#19988;&#35270;&#35273;&#19978;&#20196;&#20154;&#24841;&#24742;&#30340;&#39068;&#33394;&#12290;&#20197;&#21069;&#30340;&#26041;&#27861;&#38544;&#21547;&#22320;&#20551;&#35774;&#29992;&#25143;&#20026;&#22270;&#20687;&#20013;&#22823;&#22810;&#25968;&#23545;&#35937;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#39068;&#33394;&#25551;&#36848;&#65292;&#36825;&#20250;&#23548;&#33268;&#27425;&#20248;&#30340;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#27169;&#22411;&#65292;&#21487;&#25191;&#34892;&#20219;&#24847;&#32423;&#21035;&#25551;&#36848;&#30340;&#35821;&#35328;&#24425;&#33394;&#21270;&#12290;&#25105;&#20204;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#36328;&#27169;&#24335;&#29983;&#25104;&#27169;&#22411;&#65292;&#20197;&#22788;&#29702;&#20219;&#24847;&#32423;&#21035;&#30340;&#25551;&#36848;&#30340;&#20869;&#22312;&#27495;&#20041;&#65292;&#36890;&#36807;&#20016;&#23500;&#30340;&#39068;&#33394;&#20808;&#39564;&#30693;&#35782;&#36827;&#34892;&#35821;&#35328;&#29702;&#35299;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35774;&#35745;&#20102;&#27169;&#22359;&#26469;&#19982;&#36755;&#20837;&#26465;&#20214;&#23545;&#40784;&#65292;&#20197;&#20445;&#30041;&#23616;&#37096;&#31354;&#38388;&#32467;&#26500;&#24182;&#38450;&#27490;&#24189;&#28789;&#25928;&#24212;&#12290;&#36890;&#36807;&#25552;&#20986;&#30340;&#26032;&#22411;&#37319;&#26679;&#31574;&#30053;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#21508;&#31181;&#22797;&#26434;&#22330;&#26223;&#20013;&#23454;&#29616;&#20102;&#23454;&#20363;&#24863;&#30693;&#30340;&#24425;&#33394;&#21270;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#25105;&#20204;&#22312;&#26377;&#25928;&#22788;&#29702;&#20219;&#24847;&#32423;&#21035;&#25551;&#36848;&#26041;&#38754;&#30340;&#20248;&#21183;&#65292;&#19988;&#22312;&#35821;&#35328;&#24425;&#33394;&#21270;&#21644;&#33258;&#21160;&#24425;&#33394;&#21270;&#26041;&#38754;&#30340;&#34920;&#29616;&#37117;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language-based colorization produces plausible and visually pleasing colors under the guidance of user-friendly natural language descriptions. Previous methods implicitly assume that users provide comprehensive color descriptions for most of the objects in the image, which leads to suboptimal performance. In this paper, we propose a unified model to perform language-based colorization with any-level descriptions. We leverage the pretrained cross-modality generative model for its robust language understanding and rich color priors to handle the inherent ambiguity of any-level descriptions. We further design modules to align with input conditions to preserve local spatial structures and prevent the ghosting effect. With the proposed novel sampling strategy, our model achieves instance-aware colorization in diverse and complex scenarios. Extensive experimental results demonstrate our advantages of effectively handling any-level descriptions and outperforming both language-based and automa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#39318;&#27425;&#23545;&#33889;&#33796;&#29273;&#35821;&#20013;&#30340;Whisper ASR&#36827;&#34892;&#20102;&#26631;&#28857;&#31526;&#21495;&#39044;&#27979;&#26041;&#38754;&#30340;&#30740;&#31350;&#65292;&#24182;&#20026;&#26631;&#28857;&#31526;&#21495;&#39044;&#27979;&#22312;&#20027;&#39064;&#24314;&#27169;&#20013;&#30340;&#24212;&#29992;&#25552;&#20379;&#20102;&#26377;&#30410;&#30340;&#23454;&#39564;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2305.14580</link><description>&lt;p&gt;
&#35780;&#20272;OpenAI&#25552;&#20379;&#30340;Whisper ASR&#22312;Museum of the Person&#30340;&#29983;&#27963;&#21490;&#20013;&#36827;&#34892;&#26631;&#28857;&#31526;&#21495;&#39044;&#27979;&#21644;&#20027;&#39064;&#24314;&#27169;&#30340;&#25928;&#26524;
&lt;/p&gt;
&lt;p&gt;
Evaluating OpenAI's Whisper ASR for Punctuation Prediction and Topic Modeling of life histories of the Museum of the Person. (arXiv:2305.14580v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14580
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#23545;&#33889;&#33796;&#29273;&#35821;&#20013;&#30340;Whisper ASR&#36827;&#34892;&#20102;&#26631;&#28857;&#31526;&#21495;&#39044;&#27979;&#26041;&#38754;&#30340;&#30740;&#31350;&#65292;&#24182;&#20026;&#26631;&#28857;&#31526;&#21495;&#39044;&#27979;&#22312;&#20027;&#39064;&#24314;&#27169;&#20013;&#30340;&#24212;&#29992;&#25552;&#20379;&#20102;&#26377;&#30410;&#30340;&#23454;&#39564;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#31995;&#32479;&#22312;&#20154;&#26426;&#20132;&#20114;&#24212;&#29992;&#20013;&#25198;&#28436;&#30528;&#37325;&#35201;&#35282;&#33394;&#12290;&#28982;&#32780;&#65292;&#36807;&#21435;&#21313;&#24180;&#20013;&#25552;&#20986;&#30340;&#33889;&#33796;&#29273;&#35821;ASR&#27169;&#22411;&#22312;&#27491;&#30830;&#35782;&#21035;&#33258;&#21160;&#36716;&#24405;&#20013;&#30340;&#26631;&#28857;&#31526;&#21495;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#65292;&#36825;&#20351;&#24471;&#36825;&#20123;&#36716;&#24405;&#19981;&#33021;&#34987;&#20854;&#20182;&#31995;&#32479;&#12289;&#27169;&#22411;&#21644;&#29978;&#33267;&#26159;&#20154;&#31867;&#20351;&#29992;&#12290;&#26368;&#36817;&#65292;OpenAI&#25552;&#20986;&#20102;Whisper ASR&#65292;&#36825;&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#65292;&#26377;&#26395;&#22788;&#29702;&#36825;&#20123;&#38480;&#21046;&#12290;&#26412;&#30740;&#31350;&#26159;&#31532;&#19968;&#27425;&#38024;&#23545;&#33889;&#33796;&#29273;&#35821;&#20013;Whisper&#30340;&#26631;&#28857;&#31526;&#21495;&#39044;&#27979;&#24615;&#33021;&#36827;&#34892;&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#20351;&#29992;&#23454;&#39564;&#35780;&#20272;&#26469;&#32771;&#34385;&#20851;&#20110;&#20572;&#39039;&#28857;&#65288;&#36887;&#21495;&#65289;&#21644;&#23436;&#25972;&#24605;&#24819;&#65288;&#24863;&#21497;&#12289;&#30097;&#38382;&#21644;&#21477;&#21495;&#65289;&#30340;&#29702;&#35770;&#26041;&#38754;&#65292;&#20197;&#21450;&#19982;&#22522;&#20110;&#36716;&#24405;&#30340;&#20027;&#39064;&#24314;&#27169;&#30456;&#20851;&#30340;&#23454;&#38469;&#26041;&#38754;&#65292;&#20351;&#29992;&#26631;&#28857;&#31526;&#21495;&#26469;&#25552;&#39640;&#24615;&#33021;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic speech recognition (ASR) systems play a key role in applications involving human-machine interactions. Despite their importance, ASR models for the Portuguese language proposed in the last decade have limitations in relation to the correct identification of punctuation marks in automatic transcriptions, which hinder the use of transcriptions by other systems, models, and even by humans. However, recently Whisper ASR was proposed by OpenAI, a general-purpose speech recognition model that has generated great expectations in dealing with such limitations. This chapter presents the first study on the performance of Whisper for punctuation prediction in the Portuguese language. We present an experimental evaluation considering both theoretical aspects involving pausing points (comma) and complete ideas (exclamation, question, and fullstop), as well as practical aspects involving transcript-based topic modeling - an application dependent on punctuation marks for promising performan
&lt;/p&gt;</description></item><item><title>&#24207;&#21015;&#24314;&#27169;&#26159;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#27604;Q-Learning&#21644;Imitation Learning&#26356;&#36866;&#21512;&#22312;&#31232;&#30095;&#22870;&#21169;&#21644;&#20302;&#36136;&#37327;&#25968;&#25454;&#35774;&#32622;&#19979;&#30340;&#36873;&#25321;&#65292;&#22312;&#20219;&#21153;&#33539;&#22260;&#22686;&#21152;&#26102;&#65292;&#24207;&#21015;&#24314;&#27169;&#21644;&#27169;&#20223;&#23398;&#20064;&#26356;&#21487;&#21462;&#12290;</title><link>http://arxiv.org/abs/2305.14550</link><description>&lt;p&gt;
&#24207;&#21015;&#24314;&#27169;&#26159;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#19968;&#20010;&#24378;&#26377;&#21147;&#30340;&#31454;&#20105;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sequence Modeling is a Robust Contender for Offline Reinforcement Learning. (arXiv:2305.14550v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14550
&lt;/p&gt;
&lt;p&gt;
&#24207;&#21015;&#24314;&#27169;&#26159;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#27604;Q-Learning&#21644;Imitation Learning&#26356;&#36866;&#21512;&#22312;&#31232;&#30095;&#22870;&#21169;&#21644;&#20302;&#36136;&#37327;&#25968;&#25454;&#35774;&#32622;&#19979;&#30340;&#36873;&#25321;&#65292;&#22312;&#20219;&#21153;&#33539;&#22260;&#22686;&#21152;&#26102;&#65292;&#24207;&#21015;&#24314;&#27169;&#21644;&#27169;&#20223;&#23398;&#20064;&#26356;&#21487;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20351;&#20195;&#29702;&#33021;&#22815;&#20174;&#38745;&#24577;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#26377;&#25928;&#30340;&#26368;&#22823;&#21270;&#25910;&#30410;&#31574;&#30053;&#12290;&#31163;&#32447;RL&#30340;&#19977;&#22823;&#33539;&#24335;&#26159;Q-Learning&#12289;Imitation Learning&#21644;Sequence Modeling&#12290;&#19968;&#20010;&#20851;&#38190;&#30340;&#38382;&#39064;&#26159;&#65306;&#22312;&#20160;&#20040;&#26465;&#20214;&#19979;&#65292;&#21738;&#31181;&#33539;&#24335;&#34987;&#20248;&#20808;&#36873;&#25321;&#65311;&#25105;&#20204;&#36890;&#36807;&#25506;&#32034;&#20195;&#34920;&#24615;&#31639;&#27861;&#8212;&#8212;&#20445;&#23432;Q-Learning(CQL)&#12289;&#34892;&#20026;&#20811;&#38534; (BC)&#21644;&#20915;&#31574;Transformer (DT)&#8212;&#8212;&#22312;&#24120;&#29992;&#30340;D4RL&#21644;Robomimic&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#34920;&#29616;&#26469;&#23545;&#36825;&#20010;&#38382;&#39064;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#26377;&#38024;&#23545;&#24615;&#30340;&#23454;&#39564;&#26469;&#29702;&#35299;&#23427;&#20204;&#22312;&#25968;&#25454;&#23376;&#20248;&#24615;&#21644;&#20219;&#21153;&#22797;&#26434;&#24615;&#26041;&#38754;&#30340;&#34892;&#20026;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#21457;&#29616;&#26159;&#65306;(1)&#24207;&#21015;&#24314;&#27169;&#38656;&#35201;&#27604;Q-Learning&#26356;&#22810;&#30340;&#25968;&#25454;&#26469;&#23398;&#20064;&#31454;&#20105;&#24615;&#31574;&#30053;&#65292;&#20294;&#26356;&#21152;&#31283;&#20581;&#65307;(2)&#24207;&#21015;&#24314;&#27169;&#22312;&#31232;&#30095;&#22870;&#21169;&#21644;&#20302;&#36136;&#37327;&#25968;&#25454;&#35774;&#32622;&#20013;&#27604;Q-Learning&#21644;Imitation Learning&#37117;&#35201;&#22909;&#24471;&#22810;&#65307;(3)&#38543;&#30528;&#20219;&#21153;&#33539;&#22260;&#30340;&#22686;&#21152;&#65292;&#24207;&#21015;&#24314;&#27169;&#21644;&#27169;&#20223;&#23398;&#20064;&#26356;&#21487;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
Offline reinforcement learning (RL) allows agents to learn effective, return-maximizing policies from a static dataset. Three major paradigms for offline RL are Q-Learning, Imitation Learning, and Sequence Modeling. A key open question is: which paradigm is preferred under what conditions? We study this question empirically by exploring the performance of representative algorithms -- Conservative Q-Learning (CQL), Behavior Cloning (BC), and Decision Transformer (DT) -- across the commonly used D4RL and Robomimic benchmarks. We design targeted experiments to understand their behavior concerning data suboptimality and task complexity. Our key findings are: (1) Sequence Modeling requires more data than Q-Learning to learn competitive policies but is more robust; (2) Sequence Modeling is a substantially better choice than both Q-Learning and Imitation Learning in sparse-reward and low-quality data settings; and (3) Sequence Modeling and Imitation Learning are preferable as task horizon inc
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;SPEECH&#30340;&#27169;&#22411;&#65292;&#23427;&#20351;&#29992;&#33021;&#37327;&#24314;&#27169;&#26469;&#34920;&#31034;&#22797;&#26434;&#30340;&#20107;&#20214;&#32467;&#26500;&#65292;&#24182;&#20351;&#29992;&#36229;&#29699;&#26469;&#34920;&#31034;&#20107;&#20214;&#31867;&#21035;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;SPEECH&#22312;&#20107;&#20214;&#26816;&#27979;&#21644;&#20107;&#20214;&#20851;&#31995;&#25277;&#21462;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.13617</link><description>&lt;p&gt;
SPEECH: &#22522;&#20110;&#33021;&#37327;&#30340;&#20107;&#20214;&#20013;&#24515;&#36229;&#29699;&#30340;&#32467;&#26500;&#21270;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
SPEECH: Structured Prediction with Energy-Based Event-Centric Hyperspheres. (arXiv:2305.13617v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13617
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;SPEECH&#30340;&#27169;&#22411;&#65292;&#23427;&#20351;&#29992;&#33021;&#37327;&#24314;&#27169;&#26469;&#34920;&#31034;&#22797;&#26434;&#30340;&#20107;&#20214;&#32467;&#26500;&#65292;&#24182;&#20351;&#29992;&#36229;&#29699;&#26469;&#34920;&#31034;&#20107;&#20214;&#31867;&#21035;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;SPEECH&#22312;&#20107;&#20214;&#26816;&#27979;&#21644;&#20107;&#20214;&#20851;&#31995;&#25277;&#21462;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20107;&#20214;&#20013;&#24515;&#30340;&#32467;&#26500;&#21270;&#39044;&#27979;&#28041;&#21450;&#39044;&#27979;&#20107;&#20214;&#30340;&#32467;&#26500;&#21270;&#36755;&#20986;&#12290;&#22312;&#22823;&#22810;&#25968;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24773;&#20917;&#19979;&#65292;&#20107;&#20214;&#32467;&#26500;&#37117;&#20855;&#26377;&#22797;&#26434;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#22240;&#27492;&#26377;&#25928;&#22320;&#34920;&#31034;&#36825;&#20123;&#22797;&#26434;&#30340;&#20107;&#20214;&#32467;&#26500;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#33021;&#37327;&#30340;&#20107;&#20214;&#20013;&#24515;&#36229;&#29699;&#30340;&#32467;&#26500;&#21270;&#39044;&#27979; (SPEECH)&#12290; SPEECH &#20351;&#29992;&#22522;&#20110;&#33021;&#37327;&#30340;&#24314;&#27169;&#26469;&#27169;&#25311;&#20107;&#20214;&#32467;&#26500;&#32452;&#20214;&#20043;&#38388;&#30340;&#22797;&#26434;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#20351;&#29992;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#36229;&#29699;&#26469;&#34920;&#31034;&#20107;&#20214;&#31867;&#21035;&#12290;&#22312;&#20004;&#20010;&#32479;&#19968;&#26631;&#27880;&#30340;&#20107;&#20214;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#20013;&#65292;&#32467;&#26524;&#34920;&#26126;SPEECH&#22312;&#20107;&#20214;&#26816;&#27979;&#21644;&#20107;&#20214;&#20851;&#31995;&#25277;&#21462;&#20219;&#21153;&#20013;&#21344;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Event-centric structured prediction involves predicting structured outputs of events. In most NLP cases, event structures are complex with manifold dependency, and it is challenging to effectively represent these complicated structured events. To address these issues, we propose Structured Prediction with Energy-based Event-Centric Hyperspheres (SPEECH). SPEECH models complex dependency among event structured components with energy-based modeling, and represents event classes with simple but effective hyperspheres. Experiments on two unified-annotated event datasets indicate that SPEECH is predominant in event detection and event-relation extraction tasks.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#23545;&#22522;&#22240;&#38598;&#36827;&#34892;&#20989;&#25968;&#27010;&#25324;&#30340;&#26041;&#27861;&#65292;&#21517;&#20026;SPINDOCTOR&#65292;&#21487;&#20197;&#25552;&#20379;&#27604;&#20256;&#32479;&#26041;&#27861;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.13338</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22522;&#22240;&#38598;&#27010;&#25324;
&lt;/p&gt;
&lt;p&gt;
Gene Set Summarization using Large Language Models. (arXiv:2305.13338v1 [q-bio.GN])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13338
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#23545;&#22522;&#22240;&#38598;&#36827;&#34892;&#20989;&#25968;&#27010;&#25324;&#30340;&#26041;&#27861;&#65292;&#21517;&#20026;SPINDOCTOR&#65292;&#21487;&#20197;&#25552;&#20379;&#27604;&#20256;&#32479;&#26041;&#27861;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#23376;&#29983;&#29289;&#23398;&#23478;&#32463;&#24120;&#35299;&#37322;&#20174;&#39640;&#36890;&#37327;&#23454;&#39564;&#21644;&#35745;&#31639;&#20998;&#26512;&#20013;&#33719;&#24471;&#30340;&#22522;&#22240;&#21015;&#34920;&#12290;&#36825;&#36890;&#24120;&#26159;&#36890;&#36807;&#32479;&#35745;&#23500;&#38598;&#20998;&#26512;&#26469;&#23436;&#25104;&#30340;&#65292;&#35813;&#20998;&#26512;&#27979;&#37327;&#19982;&#22522;&#22240;&#25110;&#20854;&#23646;&#24615;&#30456;&#20851;&#30340;&#29983;&#29289;&#21151;&#33021;&#26415;&#35821;&#30340;&#36807;&#24230;&#25110;&#27424;&#34920;&#31034;&#31243;&#24230;&#65292;&#22522;&#20110;&#30693;&#35782;&#24211;&#65288;KB&#65289;&#65288;&#20363;&#22914;Gene Ontology&#65288;GO&#65289;&#65289;&#20013;&#30340;&#32534;&#35793;&#26029;&#35328;&#12290;&#35299;&#37322;&#22522;&#22240;&#21015;&#34920;&#20063;&#21487;&#20197;&#34987;&#26500;&#24314;&#20026;&#19968;&#20010;&#25991;&#26412;&#27010;&#25324;&#20219;&#21153;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#65292;&#21487;&#33021;&#30452;&#25509;&#21033;&#29992;&#31185;&#23398;&#25991;&#26412;&#24182;&#36991;&#20813;&#20381;&#36182;KB&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;SPINDOCTOR&#65288;&#31283;&#23450;&#30340;&#25552;&#31034;&#25554;&#20540;&#30340;&#21463;&#25511;&#26415;&#35821;&#30340;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#30340;&#32467;&#26500;&#21270;&#25253;&#21578;&#27169;&#26495;&#65289;&#65292;&#19968;&#31181;&#20351;&#29992;GPT&#27169;&#22411;&#25191;&#34892;&#22522;&#22240;&#38598;&#20989;&#25968;&#27010;&#25324;&#30340;&#26041;&#27861;&#65292;&#20316;&#20026;&#26631;&#20934;&#23500;&#38598;&#20998;&#26512;&#30340;&#34917;&#20805;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#20351;&#29992;&#19981;&#21516;&#30340;&#22522;&#22240;&#21151;&#33021;&#20449;&#24687;&#26469;&#28304;&#65306;&#65288;1&#65289;&#20174;&#37492;&#23450;&#30340;&#26412;&#20307;KB&#27880;&#37322;&#20013;&#33719;&#24471;&#30340;&#32467;&#26500;&#21270;&#25991;&#26412;&#65292;&#65288;2&#65289;&#20174;&#25991;&#26412;&#25366;&#25496;&#20013;&#25512;&#26029;&#30340;&#26412;&#20307;&#26415;&#35821;&#65292;&#20197;&#21450;&#65288;3&#65289;&#30452;&#25509;&#20174;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#20013;&#33719;&#24471;&#30340;&#26415;&#35821;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;1813&#20010;&#22522;&#22240;&#38598;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;SPINDOCTOR&#65292;&#24182;&#23637;&#31034;&#20102;&#20351;&#29992;GPT&#27169;&#22411;&#26174;&#33879;&#25913;&#21892;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20063;&#25552;&#39640;&#20102;&#21487;&#35299;&#37322;&#24615;&#65292;&#22240;&#20026;&#23427;&#33021;&#22815;&#29983;&#25104;&#20154;&#31867;&#21487;&#35835;&#30340;&#22522;&#22240;&#21151;&#33021;&#25688;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
Molecular biologists frequently interpret gene lists derived from high-throughput experiments and computational analysis. This is typically done as a statistical enrichment analysis that measures the over- or under-representation of biological function terms associated with genes or their properties, based on curated assertions from a knowledge base (KB) such as the Gene Ontology (GO). Interpreting gene lists can also be framed as a textual summarization task, enabling the use of Large Language Models (LLMs), potentially utilizing scientific texts directly and avoiding reliance on a KB.  We developed SPINDOCTOR (Structured Prompt Interpolation of Natural Language Descriptions of Controlled Terms for Ontology Reporting), a method that uses GPT models to perform gene set function summarization as a complement to standard enrichment analysis. This method can use different sources of gene functional information: (1) structured text derived from curated ontological KB annotations, (2) ontol
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31895;&#21040;&#32454;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#20998;&#23376;&#29983;&#25104;&#20013;&#30340;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#37319;&#29992;&#20998;&#23618;&#25193;&#25955;&#27169;&#22411;&#21644;HIPER&#31639;&#27861;&#29983;&#25104;&#32467;&#26500;&#30340;&#26377;&#25928;&#24615;&#21644;&#22810;&#26679;&#24615;&#65292;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.13266</link><description>&lt;p&gt;
&#31895;&#21040;&#32454;: &#19968;&#31181;&#29992;&#20110;&#19977;&#32500;&#20998;&#23376;&#29983;&#25104;&#30340;&#20998;&#23618;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Coarse-to-Fine: a Hierarchical Diffusion Model for Molecule Generation in 3D. (arXiv:2305.13266v2 [q-bio.BM] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13266
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31895;&#21040;&#32454;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#20998;&#23376;&#29983;&#25104;&#20013;&#30340;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#37319;&#29992;&#20998;&#23618;&#25193;&#25955;&#27169;&#22411;&#21644;HIPER&#31639;&#27861;&#29983;&#25104;&#32467;&#26500;&#30340;&#26377;&#25928;&#24615;&#21644;&#22810;&#26679;&#24615;&#65292;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33647;&#29289;&#21457;&#29616;&#20013;&#65292;&#29983;&#25104;&#29702;&#24819;&#30340;&#19977;&#32500;&#20998;&#23376;&#32467;&#26500;&#26159;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#12290;&#23613;&#31649;&#25105;&#20204;&#21462;&#24471;&#20102;&#30456;&#24403;&#22823;&#30340;&#36827;&#23637;&#65292;&#20294;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#22312;&#21407;&#23376;&#20998;&#36776;&#29575;&#19979;&#29983;&#25104;&#20998;&#23376;&#65292;&#24182;&#24573;&#30053;&#20869;&#22312;&#30340;&#23616;&#37096;&#32467;&#26500;&#65292;&#22914;&#29615;&#65292;&#36825;&#23548;&#33268;&#29983;&#25104;&#30340;&#32467;&#26500;&#36136;&#37327;&#36739;&#24046;&#65292;&#29305;&#21035;&#26159;&#24403;&#29983;&#25104;&#22823;&#20998;&#23376;&#26102;&#12290;&#22522;&#20110;&#29255;&#27573;&#30340;&#20998;&#23376;&#29983;&#25104;&#26159;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#31574;&#30053;&#65292;&#20294;&#30001;&#20110;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#65292;&#23427;&#19981;&#23481;&#26131;&#29992;&#20110;3D&#38750;&#33258;&#22238;&#24402;&#29983;&#25104;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#31895;&#21040;&#32454;&#30340;&#31574;&#30053;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20854;&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#23618;&#25193;&#25955;&#30340;&#27169;&#22411;&#65288;&#21363;HierDiff&#65289;&#65292;&#20197;&#20445;&#25345;&#23616;&#37096;&#27573;&#30340;&#26377;&#25928;&#24615;&#32780;&#19981;&#20381;&#36182;&#20110;&#33258;&#22238;&#24402;&#24314;&#27169;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;HierDiff&#39318;&#20808;&#36890;&#36807;&#31561;&#21464;&#25193;&#25955;&#36807;&#31243;&#29983;&#25104;&#31895;&#31890;&#24230;&#20998;&#23376;&#20960;&#20309;&#20307;&#65292;&#20854;&#20013;&#27599;&#20010;&#31895;&#31890;&#24230;&#33410;&#28857;&#21453;&#26144;&#20998;&#23376;&#20013;&#30340;&#19968;&#20010;&#29255;&#27573;&#12290;&#28982;&#21518;&#65292;&#31895;&#31890;&#24230;&#33410;&#28857;&#34987;&#20998;&#35299;&#25104;&#32454;&#31890;&#24230;&#33410;&#28857;&#65292;&#20854;&#20013;&#32454;&#31890;&#24230;&#33410;&#28857;&#26159;&#20998;&#23376;&#20013;&#30340;&#19968;&#20010;&#19977;&#32500;&#31354;&#38388;&#28857;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#37319;&#26679;&#21644;&#32463;&#39564;&#27010;&#29575;&#32454;&#21270;&#65288;&#21363;HIPER&#65289;&#31639;&#27861;&#65292;&#20197;&#30830;&#20445;&#29983;&#25104;&#30340;&#32467;&#26500;&#30340;&#26377;&#25928;&#24615;&#21644;&#22810;&#26679;&#24615;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;HierDiff&#22312;&#23450;&#37327;&#35780;&#20272;&#25351;&#26631;&#21644;&#35270;&#35273;&#36136;&#37327;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generating desirable molecular structures in 3D is a fundamental problem for drug discovery. Despite the considerable progress we have achieved, existing methods usually generate molecules in atom resolution and ignore intrinsic local structures such as rings, which leads to poor quality in generated structures, especially when generating large molecules. Fragment-based molecule generation is a promising strategy, however, it is nontrivial to be adapted for 3D non-autoregressive generations because of the combinational optimization problems. In this paper, we utilize a coarse-to-fine strategy to tackle this problem, in which a Hierarchical Diffusion-based model (i.e.~HierDiff) is proposed to preserve the validity of local segments without relying on autoregressive modeling. Specifically, HierDiff first generates coarse-grained molecule geometries via an equivariant diffusion process, where each coarse-grained node reflects a fragment in a molecule. Then the coarse-grained nodes are dec
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#26410;&#30693;&#28436;&#21270;&#19988;&#21482;&#33021;&#37096;&#20998;&#35266;&#27979;&#30340;&#29615;&#22659;&#19979;&#37319;&#29992;&#36172;&#21338;&#21453;&#39304;&#21644;&#26377;&#30028;&#36319;&#36394;&#36951;&#25022;&#30340;&#27425;&#27169;&#21327;&#35843;&#31639;&#27861;&#65292;&#20197;&#35299;&#20915;&#22810;&#26426;&#22120;&#20154;&#21327;&#20316;&#38382;&#39064;&#65292;&#24182;&#36866;&#29992;&#20110;&#30446;&#26631;&#36319;&#36394;&#12289;&#29615;&#22659;&#26144;&#23556;&#21644;&#21306;&#22495;&#30417;&#27979;&#31561;&#22797;&#26434;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2305.12795</link><description>&lt;p&gt;
&#19981;&#21487;&#39044;&#27979;&#21450;&#37096;&#20998;&#21487;&#35266;&#27979;&#29615;&#22659;&#19979;&#30340;&#22810;&#26426;&#22120;&#20154;&#21327;&#20316;&#30340;&#36172;&#21338;&#27425;&#27169;&#26368;&#22823;&#21270;
&lt;/p&gt;
&lt;p&gt;
Bandit Submodular Maximization for Multi-Robot Coordination in Unpredictable and Partially Observable Environments. (arXiv:2305.12795v2 [eess.SY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12795
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#26410;&#30693;&#28436;&#21270;&#19988;&#21482;&#33021;&#37096;&#20998;&#35266;&#27979;&#30340;&#29615;&#22659;&#19979;&#37319;&#29992;&#36172;&#21338;&#21453;&#39304;&#21644;&#26377;&#30028;&#36319;&#36394;&#36951;&#25022;&#30340;&#27425;&#27169;&#21327;&#35843;&#31639;&#27861;&#65292;&#20197;&#35299;&#20915;&#22810;&#26426;&#22120;&#20154;&#21327;&#20316;&#38382;&#39064;&#65292;&#24182;&#36866;&#29992;&#20110;&#30446;&#26631;&#36319;&#36394;&#12289;&#29615;&#22659;&#26144;&#23556;&#21644;&#21306;&#22495;&#30417;&#27979;&#31561;&#22797;&#26434;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#19981;&#21487;&#39044;&#27979;&#21450;&#37096;&#20998;&#21487;&#35266;&#27979;&#29615;&#22659;&#19979;&#30340;&#22810;&#26426;&#22120;&#20154;&#21327;&#20316;&#38382;&#39064;&#65292;&#21363;&#26410;&#26469;&#28436;&#21270;&#26410;&#30693;&#19988;&#21482;&#33021;&#37096;&#20998;&#35266;&#27979;&#30340;&#29615;&#22659;&#19979;&#30340;&#21327;&#20316;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21160;&#26426;&#26469;&#33258;&#33258;&#20027;&#24615;&#30340;&#26410;&#26469;&#65292;&#28041;&#21450;&#22810;&#20010;&#26426;&#22120;&#20154;&#22312;&#21160;&#24577;&#12289;&#38750;&#32467;&#26500;&#21270;&#21644;&#37096;&#20998;&#21487;&#35266;&#27979;&#30340;&#29615;&#22659;&#20013;&#21327;&#35843;&#34892;&#21160;&#65292;&#20197;&#23436;&#25104;&#22797;&#26434;&#20219;&#21153;&#65292;&#22914;&#30446;&#26631;&#36319;&#36394;&#12289;&#29615;&#22659;&#26144;&#23556;&#21644;&#21306;&#22495;&#30417;&#27979;&#12290;&#36825;&#20123;&#20219;&#21153;&#36890;&#24120;&#34987;&#24314;&#27169;&#20026;&#27425;&#27169;&#26368;&#22823;&#21270;&#21327;&#35843;&#38382;&#39064;&#65292;&#30001;&#20110;&#26426;&#22120;&#20154;&#20043;&#38388;&#30340;&#20449;&#24687;&#37325;&#21472;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#31532;&#19968;&#20010;&#37319;&#29992;&#36172;&#21338;&#21453;&#39304;&#21644;&#26377;&#30028;&#36319;&#36394;&#36951;&#25022;&#30340;&#27425;&#27169;&#21327;&#35843;&#31639;&#27861;&#8212;&#8212;&#36172;&#21338;&#21453;&#39304;&#26159;&#26426;&#22120;&#20154;&#22312;&#23616;&#37096;&#35266;&#27979;&#19979;&#20165;&#35745;&#31639;&#24050;&#36873;&#25321;&#34892;&#21160;&#30340;&#24433;&#21709;&#32780;&#38750;&#25152;&#26377;&#21487;&#36873;&#26367;&#20195;&#34892;&#21160;&#30340;&#33021;&#21147;&#65307;&#36319;&#36394;&#36951;&#25022;&#26159;&#31639;&#27861;&#30456;&#23545;&#20110;&#26368;&#20248;&#26102;&#38388;&#30340;&#27425;&#20248;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the problem of multi-agent coordination in unpredictable and partially observable environments, that is, environments whose future evolution is unknown a priori and that can only be partially observed. We are motivated by the future of autonomy that involves multiple robots coordinating actions in dynamic, unstructured, and partially observable environments to complete complex tasks such as target tracking, environmental mapping, and area monitoring. Such tasks are often modeled as submodular maximization coordination problems due to the information overlap among the robots. We introduce the first submodular coordination algorithm with bandit feedback and bounded tracking regret -- bandit feedback is the robots' ability to compute in hindsight only the effect of their chosen actions, instead of all the alternative actions that they could have chosen instead, due to the partial observability; and tracking regret is the algorithm's suboptimality with respect to the optimal time-
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#32852;&#21512;&#23398;&#20064;ASR&#21644;SER&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#19979;&#19981;&#20165;&#22312;SER&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#25552;&#21319;&#65292;&#21516;&#26102;&#20063;&#25552;&#39640;&#20102;ASR&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#22312;&#22024;&#26434;&#29615;&#22659;&#20013;&#65292;&#32852;&#21512;&#35757;&#32451;&#24471;&#21040;&#30340;&#27169;&#22411;&#27604;&#29420;&#31435;&#35757;&#32451;&#30340;&#27169;&#22411;&#26356;&#20855;&#22122;&#22768;&#40065;&#26834;&#24615;</title><link>http://arxiv.org/abs/2305.12540</link><description>&lt;p&gt;
&#20851;&#20110;&#32852;&#21512;&#23398;&#20064;&#35821;&#38899;&#24773;&#24863;&#21644;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#30340;&#26377;&#25928;&#24615;&#21644;&#22122;&#22768;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the Efficacy and Noise-Robustness of Jointly Learned Speech Emotion and Automatic Speech Recognition. (arXiv:2305.12540v2 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12540
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#32852;&#21512;&#23398;&#20064;ASR&#21644;SER&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#19979;&#19981;&#20165;&#22312;SER&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#25552;&#21319;&#65292;&#21516;&#26102;&#20063;&#25552;&#39640;&#20102;ASR&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#22312;&#22024;&#26434;&#29615;&#22659;&#20013;&#65292;&#32852;&#21512;&#35757;&#32451;&#24471;&#21040;&#30340;&#27169;&#22411;&#27604;&#29420;&#31435;&#35757;&#32451;&#30340;&#27169;&#22411;&#26356;&#20855;&#22122;&#22768;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26032;&#26102;&#20195;&#30340;&#23545;&#35805;&#20195;&#29702;&#31995;&#32479;&#22312;&#22024;&#26434;&#30340;&#29615;&#22659;&#20013;&#20351;&#29992;&#20004;&#31181;&#29420;&#31435;&#30340;&#26041;&#27861;&#25191;&#34892;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#65288;SER&#65289;&#21644;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#20302;&#36164;&#28304;&#29615;&#22659;&#19979;&#30340;&#32852;&#21512;ASR-SER&#22810;&#20219;&#21153;&#23398;&#20064;&#26041;&#27861;&#65292;&#21457;&#29616;&#19981;&#20165;&#22312;SER&#20013;&#21462;&#24471;&#20102;&#25913;&#36827;&#65292;&#32780;&#19988;&#22312;ASR&#20013;&#20063;&#26377;&#25152;&#25552;&#21319;&#12290;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#36825;&#31181;&#32852;&#21512;&#35757;&#32451;&#27169;&#22411;&#23545;&#32972;&#26223;&#22122;&#22768;&#12289;&#32993;&#35328;&#20081;&#35821;&#21644;&#38899;&#20048;&#30340;&#40065;&#26834;&#24615;&#12290;&#22312;IEMOCAP&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#24178;&#20928;&#30340;&#29615;&#22659;&#20013;&#65292;&#32852;&#21512;&#23398;&#20064;&#21487;&#20197;&#23558;ASR&#21333;&#35789;&#38169;&#35823;&#29575;&#65288;WER&#65289;&#21644;SER&#20998;&#31867;&#20934;&#30830;&#29575;&#20998;&#21035;&#25552;&#39640;10.7&#65285;&#21644;2.3&#65285;&#12290;&#22312;&#21152;&#20837;&#20102;MUSAN&#30340;&#22024;&#26434;&#22330;&#26223;&#20013;&#65292;&#32852;&#21512;&#26041;&#27861;&#22312;&#35768;&#22810;&#22024;&#26434;&#26465;&#20214;&#19979;&#20248;&#20110;&#29420;&#31435;&#30340;ASR&#21644;SER&#26041;&#27861;&#12290;&#24635;&#20043;&#65292;&#32852;&#21512;ASR-SER&#26041;&#27861;&#27604;&#29420;&#31435;&#30340;ASR&#21644;SER&#26041;&#27861;&#20135;&#29983;&#20102;&#26356;&#20855;&#40065;&#26834;&#24615;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
New-age conversational agent systems perform both speech emotion recognition (SER) and automatic speech recognition (ASR) using two separate and often independent approaches for real-world application in noisy environments. In this paper, we investigate a joint ASR-SER multitask learning approach in a low-resource setting and show that improvements are observed not only in SER, but also in ASR. We also investigate the robustness of such jointly trained models to the presence of background noise, babble, and music. Experimental results on the IEMOCAP dataset show that joint learning can improve ASR word error rate (WER) and SER classification accuracy by 10.7% and 2.3% respectively in clean scenarios. In noisy scenarios, results on data augmented with MUSAN show that the joint approach outperforms the independent ASR and SER approaches across many noisy conditions. Overall, the joint ASR-SER approach yielded more noise-resistant models than the independent ASR and SER approaches.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#22836;&#29366;&#24577;&#31354;&#38388;&#65288;MH-SSM&#65289;&#27169;&#22411;&#65292;&#23427;&#33021;&#22815;&#29992;&#20110;&#35821;&#38899;&#35782;&#21035;&#20219;&#21153;&#24182;&#22312;LibriSpeech&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#30340;&#26032;&#30340;&#24615;&#33021;&#65292;&#26159;&#21464;&#21387;&#22120;&#21464;&#25442;&#22120;&#30340;&#20248;&#31168;&#26367;&#20195;&#26041;&#26696;&#12290;&#21516;&#26102;, MH-SSM&#23618;&#30340;&#24341;&#20837;&#20063;&#25552;&#39640;&#20102;&#21464;&#21387;&#22120;&#22359;&#30340;&#24615;&#33021;&#65292;&#36798;&#21040;&#20102;&#29616;&#26377;&#26368;&#26032;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2305.12498</link><description>&lt;p&gt;
&#22810;&#22836;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#22312;&#35821;&#38899;&#35782;&#21035;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Multi-Head State Space Model for Speech Recognition. (arXiv:2305.12498v2 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12498
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#22836;&#29366;&#24577;&#31354;&#38388;&#65288;MH-SSM&#65289;&#27169;&#22411;&#65292;&#23427;&#33021;&#22815;&#29992;&#20110;&#35821;&#38899;&#35782;&#21035;&#20219;&#21153;&#24182;&#22312;LibriSpeech&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#30340;&#26032;&#30340;&#24615;&#33021;&#65292;&#26159;&#21464;&#21387;&#22120;&#21464;&#25442;&#22120;&#30340;&#20248;&#31168;&#26367;&#20195;&#26041;&#26696;&#12290;&#21516;&#26102;, MH-SSM&#23618;&#30340;&#24341;&#20837;&#20063;&#25552;&#39640;&#20102;&#21464;&#21387;&#22120;&#22359;&#30340;&#24615;&#33021;&#65292;&#36798;&#21040;&#20102;&#29616;&#26377;&#26368;&#26032;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22312;&#19968;&#20123;&#23567;&#35268;&#27169;&#30340;&#24207;&#21015;&#21644;&#35821;&#35328;&#24314;&#27169;&#20219;&#21153;&#19978;&#65292;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;SSM&#65289;&#24050;&#32463;&#34920;&#29616;&#20986;&#20102;&#24456;&#22823;&#30340;&#28508;&#21147;&#65292;&#24182;&#19988;&#33021;&#22815;&#19982;&#35768;&#22810;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#26041;&#27861;&#30456;&#23218;&#32654;&#29978;&#33267;&#36229;&#36234;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#22836;&#29366;&#24577;&#31354;&#38388;&#65288;MH-SSM&#65289;&#26550;&#26500;&#65292;&#23427;&#37197;&#22791;&#20102;&#29305;&#27530;&#30340;&#38376;&#25511;&#26426;&#21046;&#65292;&#20854;&#20013;&#24182;&#34892;&#22836;&#34987;&#25945;&#25480;&#22914;&#20309;&#22312;&#24207;&#21015;&#25968;&#25454;&#19978;&#23398;&#20064;&#26412;&#22320;&#21644;&#20840;&#23616;&#30340;&#26102;&#38388;&#21160;&#24577;&#12290;&#20316;&#20026;&#21464;&#21387;&#22120;&#32534;&#30721;&#22120;&#20013;&#22810;&#22836;&#27880;&#24847;&#21147;&#30340;&#30452;&#25509;&#26367;&#20195;&#26041;&#26696;&#65292;&#36825;&#20010;&#26032;&#27169;&#22411;&#22312;LibriSpeech&#35821;&#38899;&#35782;&#21035;&#35821;&#26009;&#24211;&#19978;&#26174;&#33879;&#20248;&#20110;&#21464;&#21387;&#22120;&#21464;&#25442;&#22120;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;&#21464;&#21387;&#22120;&#22359;&#20013;&#22686;&#21152;&#20102;MH-SSM&#23618;&#65292;&#31216;&#20026;Stateformer&#65292;&#19981;&#20351;&#29992;&#22806;&#37096;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;LibriSpeech&#20219;&#21153;&#20013;&#36798;&#21040;&#20102;&#26368;&#26032;&#30340;&#24615;&#33021;&#65292;&#24320;&#21457;&#38598;&#21644;&#27979;&#35797;&#38598;&#30340;&#35789;&#38169;&#35823;&#29575;&#20998;&#21035;&#20026;1.76&#65285; / 4.37&#65285;&#21644;1.91&#65285; / 4.36&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
State space models (SSMs) have recently shown promising results on small-scale sequence and language modelling tasks, rivalling and outperforming many attention-based approaches. In this paper, we propose a multi-head state space (MH-SSM) architecture equipped with special gating mechanisms, where parallel heads are taught to learn local and global temporal dynamics on sequence data. As a drop-in replacement for multi-head attention in transformer encoders, this new model significantly outperforms the transformer transducer on the LibriSpeech speech recognition corpus. Furthermore, we augment the transformer block with MH-SSMs layers, referred to as the Stateformer, achieving state-of-the-art performance on the LibriSpeech task, with word error rates of 1.76\%/4.37\% on the development and 1.91\%/4.36\% on the test sets without using an external language model.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20351;&#29992;&#29983;&#29289;&#29305;&#24615;&#26500;&#24314;CNNs&#26550;&#26500;&#65292;&#25104;&#21151;&#35299;&#37322;V1&#31070;&#32463;&#27963;&#21160;&#29305;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.11275</link><description>&lt;p&gt;
&#29992;&#29983;&#29289;&#32422;&#26463;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#35299;&#37322;V1&#29305;&#24615;
&lt;/p&gt;
&lt;p&gt;
Explaining V1 Properties with a Biologically Constrained Deep Learning Architecture. (arXiv:2305.11275v1 [q-bio.NC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11275
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20351;&#29992;&#29983;&#29289;&#29305;&#24615;&#26500;&#24314;CNNs&#26550;&#26500;&#65292;&#25104;&#21151;&#35299;&#37322;V1&#31070;&#32463;&#27963;&#21160;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#32570;&#20047;&#29983;&#29289;&#23398;&#30340;&#29305;&#24322;&#24615;&#65292;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(CNNs)&#26368;&#36817;&#34987;&#35748;&#20026;&#26159;&#33145;&#20391;&#35270;&#35273;&#36890;&#36335;&#30340;&#26377;&#21069;&#36884;&#30340;&#27169;&#22411;&#12290;&#34429;&#28982;&#24403;&#21069;&#30340;&#26368;&#20808;&#36827;&#30340;V1&#27169;&#22411;&#26159;&#36890;&#36807;&#23545;&#25239;&#24615;&#20363;&#23376;&#30340;&#35757;&#32451;&#21644;&#24191;&#27867;&#22686;&#24378;&#30340;&#25968;&#25454;&#28014;&#29616;&#20986;&#26469;&#30340;&#65292;&#20294;&#36825;&#20123;&#27169;&#22411;&#20173;&#26080;&#27861;&#35299;&#37322;V1&#20013;&#35266;&#23519;&#21040;&#30340;&#20851;&#38190;&#31070;&#32463;&#29305;&#24615;&#65292;&#36825;&#20123;&#29305;&#24615;&#26469;&#33258;&#20110;&#29983;&#29289;&#30005;&#36335;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#20010;&#24046;&#36317;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#23558;&#31070;&#32463;&#31185;&#23398;&#30340;&#26550;&#26500;&#32452;&#20214;&#32435;&#20837;CNNs&#20013;&#65292;&#20197;&#35782;&#21035;&#19968;&#32452;&#20840;&#38754;&#35299;&#37322;V1&#31070;&#32463;&#27963;&#21160;&#30340;&#26426;&#21046;&#21644;&#26550;&#26500;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#38598;&#25104;&#27169;&#25311;&#20013;&#24515;-&#21608;&#22260;&#25326;&#25239;&#12289;&#23616;&#37096;&#24863;&#21463;&#37326;&#12289;&#35843;&#35856;&#24402;&#19968;&#21270;&#21644;&#30382;&#23618;&#25918;&#22823;&#30340;&#26550;&#26500;&#32452;&#20214;&#26469;&#25512;&#21160;&#27169;&#22411;-V1&#23545;&#40784;&#30340;&#24040;&#22823;&#25913;&#36827;&#12290;&#36890;&#36807;&#20351;&#29992;&#36825;&#20123;&#19987;&#38376;&#30340;&#32452;&#20214;&#22686;&#24378;&#20219;&#21153;&#39537;&#21160;&#30340;CNNs&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#28508;&#22312;&#34920;&#31034;&#20135;&#29983;&#20102;&#20248;&#31168;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Convolutional neural networks (CNNs) have recently emerged as promising models of the ventral visual stream, despite their lack of biological specificity. While current state-of-the-art models of the primary visual cortex (V1) have surfaced from training with adversarial examples and extensively augmented data, these models are still unable to explain key neural properties observed in V1 that arise from biological circuitry. To address this gap, we systematically incorporated neuroscience-derived architectural components into CNNs to identify a set of mechanisms and architectures that comprehensively explain neural activity in V1. We show drastic improvements in model-V1 alignment driven by the integration of architectural components that simulate center-surround antagonism, local receptive fields, tuned normalization, and cortical magnification. Upon enhancing task-driven CNNs with a collection of these specialized components, we uncover models with latent representations that yield s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#19981;&#30830;&#23450;&#24615;&#24341;&#23548;&#30340;&#26631;&#31614;&#21435;&#22122;&#25216;&#26415;&#65292;&#21487;&#20197;&#26377;&#25928;&#20934;&#30830;&#22320;&#22312;&#25991;&#26723;&#32423;&#36828;&#31243;&#20851;&#31995;&#25277;&#21462;&#20013;&#36873;&#25321;&#21487;&#20449;&#30340;&#20266;&#26631;&#31614;&#65292;&#25552;&#39640;&#20102;&#24615;&#33021;&#34920;&#29616;&#65292;&#24182;&#22312;DocRED&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26032;&#30340;&#26368;&#20339;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.11029</link><description>&lt;p&gt;
&#19981;&#30830;&#23450;&#24615;&#24341;&#23548;&#30340;&#26631;&#31614;&#21435;&#22122;&#22312;&#25991;&#26723;&#32423;&#36828;&#31243;&#20851;&#31995;&#25277;&#21462;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Uncertainty Guided Label Denoising for Document-level Distant Relation Extraction. (arXiv:2305.11029v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11029
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#19981;&#30830;&#23450;&#24615;&#24341;&#23548;&#30340;&#26631;&#31614;&#21435;&#22122;&#25216;&#26415;&#65292;&#21487;&#20197;&#26377;&#25928;&#20934;&#30830;&#22320;&#22312;&#25991;&#26723;&#32423;&#36828;&#31243;&#20851;&#31995;&#25277;&#21462;&#20013;&#36873;&#25321;&#21487;&#20449;&#30340;&#20266;&#26631;&#31614;&#65292;&#25552;&#39640;&#20102;&#24615;&#33021;&#34920;&#29616;&#65292;&#24182;&#22312;DocRED&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26032;&#30340;&#26368;&#20339;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26723;&#32423;&#20851;&#31995;&#25277;&#21462;&#26088;&#22312;&#25512;&#26029;&#25991;&#26723;&#20013;&#23454;&#20307;&#20043;&#38388;&#30340;&#22797;&#26434;&#35821;&#20041;&#20851;&#31995;&#12290;&#36828;&#31243;&#30417;&#30563;&#33021;&#22815;&#29983;&#25104;&#22823;&#37327;&#33258;&#21160;&#26631;&#27880;&#30340;&#25968;&#25454;&#65292;&#20174;&#32780;&#21487;&#20197;&#25552;&#39640;&#25991;&#26723;&#20851;&#31995;&#25277;&#21462;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#19981;&#21487;&#38752;&#30340;&#20266;&#26631;&#31614;&#20250;&#24102;&#26469;&#26032;&#30340;&#22122;&#22768;&#65292;&#20363;&#22914;&#28155;&#21152;&#34394;&#20551;&#30340;&#20266;&#26631;&#31614;&#21644;&#22833;&#21435;&#27491;&#30830;&#30340;&#30417;&#30563;&#26631;&#31614;&#12290;&#22240;&#27492;&#65292;&#22914;&#20309;&#36873;&#25321;&#26377;&#25928;&#30340;&#20266;&#26631;&#31614;&#26469;&#21435;&#22122;&#36828;&#31243;&#30417;&#30563;&#25968;&#25454;&#20173;&#28982;&#26159;&#25991;&#26723;&#32423;&#36828;&#31243;&#20851;&#31995;&#25277;&#21462;&#20013;&#30340;&#19968;&#20010;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#25216;&#26415;&#26469;&#30830;&#23450;&#20266;&#26631;&#31614;&#26159;&#21542;&#21487;&#20449;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24102;&#26377;&#19981;&#30830;&#23450;&#24615;&#24341;&#23548;&#26631;&#31614;&#21435;&#22122;&#30340;&#25991;&#26723;&#32423;&#36828;&#31243;&#20851;&#31995;&#25277;&#21462;&#26694;&#26550;&#65292;UGDRE&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23454;&#20363;&#32423;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#27861;&#65292;&#23427;&#27979;&#37327;&#20102;&#20855;&#26377;&#37325;&#21472;&#20851;&#31995;&#30340;&#20266;&#26631;&#31614;&#30340;&#21487;&#38752;&#24615;&#12290;&#36890;&#36807;&#36827;&#19968;&#27493;&#32771;&#34385;&#23454;&#20363;&#32423;&#21644;&#20851;&#31995;&#32423;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#26631;&#31614;&#21435;&#22122;&#32452;&#20214;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#36873;&#25321;&#21487;&#38752;&#30340;&#20266;&#26631;&#31614;&#36827;&#34892;&#25991;&#26723;&#20851;&#31995;&#25277;&#21462;&#12290;&#22312;&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#24182;&#22312;DocRED&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26032;&#30340;&#26368;&#20339;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Document-level relation extraction (DocRE) aims to infer complex semantic relations among entities in a document. Distant supervision (DS) is able to generate massive auto-labeled data, which can improve DocRE performance. Recent works leverage pseudo labels generated by the pre-denoising model to reduce noise in DS data. However, unreliable pseudo labels bring new noise, e.g., adding false pseudo labels and losing correct DS labels. Therefore, how to select effective pseudo labels to denoise DS data is still a challenge in document-level distant relation extraction. To tackle this issue, we introduce uncertainty estimation technology to determine whether pseudo labels can be trusted. In this work, we propose a Document-level distant Relation Extraction framework with Uncertainty Guided label denoising, UGDRE. Specifically, we propose a novel instance-level uncertainty estimation method, which measures the reliability of the pseudo labels with overlapping relations. By further consider
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;X-IQE&#30340;&#22270;&#20687;&#36136;&#37327;&#35780;&#20272;&#26041;&#27861;&#65292;&#20351;&#29992;&#35270;&#35273;&#22823;&#35821;&#35328;&#27169;&#22411;&#23545;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#36827;&#34892;&#35780;&#20272;&#65292;&#24182;&#29983;&#25104;&#25991;&#26412;&#35299;&#37322;&#12290;&#23427;&#20855;&#26377;&#21306;&#20998;&#30495;&#23454;&#21644;&#29983;&#25104;&#22270;&#20687;&#12289;&#35780;&#20272;&#25991;&#26412;-&#22270;&#20687;&#23545;&#40784;&#21644;&#35780;&#20272;&#22270;&#20687;&#32654;&#23398;&#31561;&#20248;&#28857;&#65292;&#26174;&#33879;&#22686;&#24378;&#20102;&#28145;&#24230;&#22270;&#20687;&#36136;&#37327;&#35780;&#20272;&#27169;&#22411;&#30340;&#36879;&#26126;&#24230;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.10843</link><description>&lt;p&gt;
X-IQE&#65306;&#21033;&#29992;&#35270;&#35273;&#22823;&#35821;&#35328;&#27169;&#22411;&#23545;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#36827;&#34892;&#21487;&#35299;&#37322;&#30340;&#22270;&#20687;&#36136;&#37327;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
X-IQE: eXplainable Image Quality Evaluation for Text-to-Image Generation with Visual Large Language Models. (arXiv:2305.10843v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10843
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;X-IQE&#30340;&#22270;&#20687;&#36136;&#37327;&#35780;&#20272;&#26041;&#27861;&#65292;&#20351;&#29992;&#35270;&#35273;&#22823;&#35821;&#35328;&#27169;&#22411;&#23545;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#36827;&#34892;&#35780;&#20272;&#65292;&#24182;&#29983;&#25104;&#25991;&#26412;&#35299;&#37322;&#12290;&#23427;&#20855;&#26377;&#21306;&#20998;&#30495;&#23454;&#21644;&#29983;&#25104;&#22270;&#20687;&#12289;&#35780;&#20272;&#25991;&#26412;-&#22270;&#20687;&#23545;&#40784;&#21644;&#35780;&#20272;&#22270;&#20687;&#32654;&#23398;&#31561;&#20248;&#28857;&#65292;&#26174;&#33879;&#22686;&#24378;&#20102;&#28145;&#24230;&#22270;&#20687;&#36136;&#37327;&#35780;&#20272;&#27169;&#22411;&#30340;&#36879;&#26126;&#24230;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#35299;&#37322;&#30340;&#22270;&#20687;&#36136;&#37327;&#35780;&#20272;&#26041;&#27861;&#65292;&#31216;&#20026;X-IQE&#65292;&#23427;&#21033;&#29992;&#35270;&#35273;&#22823;&#35821;&#35328;&#27169;&#22411;&#26469;&#35780;&#20272;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#25991;&#26412;&#35299;&#37322;&#12290;X-IQE&#21033;&#29992;&#20998;&#23618;&#24605;&#32500;&#38142;&#65288;CoT&#65289;&#20351;MiniGPT-4&#33021;&#22815;&#20135;&#29983;&#33258;&#27965;&#12289;&#26080;&#20559;&#30340;&#25991;&#26412;&#65292;&#19982;&#20154;&#31867;&#35780;&#20272;&#39640;&#24230;&#30456;&#20851;&#12290;&#23427;&#20855;&#26377;&#22810;&#31181;&#20248;&#28857;&#65292;&#21253;&#25324;&#33021;&#22815;&#21306;&#20998;&#30495;&#23454;&#22270;&#20687;&#21644;&#29983;&#25104;&#22270;&#20687;&#12289;&#35780;&#20272;&#25991;&#26412;-&#22270;&#20687;&#23545;&#40784;&#21644;&#35780;&#20272;&#22270;&#20687;&#32654;&#23398;&#65292;&#32780;&#19981;&#38656;&#35201;&#27169;&#22411;&#35757;&#32451;&#25110;&#24494;&#35843;&#12290;&#19982;&#20154;&#31867;&#35780;&#20272;&#30456;&#27604;&#65292;X-IQE&#26356;&#20855;&#25104;&#26412;&#25928;&#30410;&#21644;&#25928;&#29575;&#65292;&#21516;&#26102;&#26174;&#33879;&#22686;&#24378;&#20102;&#28145;&#24230;&#22270;&#20687;&#36136;&#37327;&#35780;&#20272;&#27169;&#22411;&#30340;&#36879;&#26126;&#24230;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;&#20027;&#27969;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#30340;&#22270;&#20687;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#20316;&#20026;&#22522;&#20934;&#30340;&#26377;&#25928;&#24615;&#12290;X-IQE&#22312;COCO Caption&#19978;&#34920;&#29616;&#20986;&#19982;&#26368;&#20808;&#36827;&#35780;&#20272;&#26041;&#27861;&#31867;&#20284;&#30340;&#24615;&#33021;&#65292;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a novel explainable image quality evaluation approach called X-IQE, which leverages visual large language models (LLMs) to evaluate text-to-image generation methods by generating textual explanations. X-IQE utilizes a hierarchical Chain of Thought (CoT) to enable MiniGPT-4 to produce self-consistent, unbiased texts that are highly correlated with human evaluation. It offers several advantages, including the ability to distinguish between real and generated images, evaluate text-image alignment, and assess image aesthetics without requiring model training or fine-tuning. X-IQE is more cost-effective and efficient compared to human evaluation, while significantly enhancing the transparency and explainability of deep image quality evaluation models. We validate the effectiveness of our method as a benchmark using images generated by prevalent diffusion models. X-IQE demonstrates similar performance to state-of-the-art (SOTA) evaluation methods on COCO Caption, while 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;NLP&#39046;&#22495;&#20869;&#23578;&#26410;&#30740;&#31350;&#30340;&#38382;&#39064;&#65306;&#24773;&#26223;&#21644;&#32454;&#33268;&#22320;&#29702;&#35299;&#23567;&#35828;&#20154;&#29289;&#20010;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#26631;&#35760;&#25968;&#25454;&#38598;PersoNet&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.10156</link><description>&lt;p&gt;
&#38405;&#35835;&#36807;&#31243;&#20013;&#23545;&#23567;&#35828;&#20154;&#29289;&#20010;&#24615;&#30340;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Personality Understanding of Fictional Characters during Book Reading. (arXiv:2305.10156v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10156
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;NLP&#39046;&#22495;&#20869;&#23578;&#26410;&#30740;&#31350;&#30340;&#38382;&#39064;&#65306;&#24773;&#26223;&#21644;&#32454;&#33268;&#22320;&#29702;&#35299;&#23567;&#35828;&#20154;&#29289;&#20010;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#26631;&#35760;&#25968;&#25454;&#38598;PersoNet&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#23567;&#35828;&#20154;&#29289;&#20010;&#24615;&#26159;&#38405;&#35835;&#25925;&#20107;&#30340;&#20851;&#38190;&#12290;&#38543;&#30528;&#35835;&#32773;&#19982;&#25925;&#20107;&#30340;&#20114;&#21160;&#65292;&#20182;&#20204;&#23545;&#19968;&#20010;&#20154;&#29289;&#30340;&#29702;&#35299;&#20250;&#26681;&#25454;&#26032;&#30340;&#20107;&#20214;&#21644;&#20449;&#24687;&#32780;&#28436;&#21464;&#65307;&#24182;&#19988;&#21487;&#20197;&#24863;&#30693;&#21040;&#22810;&#20010;&#31934;&#32454;&#30340;&#20010;&#24615;&#26041;&#38754;&#12290;&#36825;&#23548;&#33268;&#20102;&#19968;&#20010;&#33258;&#28982;&#30340;&#38382;&#39064;&#65306;&#24773;&#22659;&#21644;&#31934;&#32454;&#30340;&#20010;&#24615;&#29702;&#35299;&#12290;&#36825;&#20010;&#38382;&#39064;&#22312;NLP&#39046;&#22495;&#20013;&#27809;&#26377;&#24471;&#21040;&#30740;&#31350;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#32570;&#20047;&#27169;&#20223;&#38405;&#35835;&#36807;&#31243;&#30340;&#36866;&#24403;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#26631;&#35760;&#25968;&#25454;&#38598;PersoNet&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26032;&#22411;&#27880;&#37322;&#31574;&#30053;&#28041;&#21450;&#29992;&#22312;&#32447;&#38405;&#35835;&#24212;&#29992;&#31243;&#24207;&#30340;&#29992;&#25143;&#31508;&#35760;&#20316;&#20026;&#21407;&#22987;&#20070;&#31821;&#30340;&#20195;&#29702;&#36827;&#34892;&#27880;&#37322;&#12290;&#23454;&#39564;&#21644;&#20154;&#20307;&#30740;&#31350;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#26500;&#24314;&#26082;&#26377;&#25928;&#21448;&#20934;&#30830;&#65307;&#25105;&#20204;&#30340;&#20219;&#21153;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#38271;&#26399;&#30340;&#19978;&#19979;&#25991;&#20197;&#23454;&#29616;&#23545;&#26426;&#22120;&#21644;&#20154;&#31867;&#30340;&#20934;&#30830;&#39044;&#27979;&#12290;&#25968;&#25454;&#38598;&#21487;&#22312;https://github.com/Gorov/personet_acl23&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Comprehending characters' personalities is a crucial aspect of story reading. As readers engage with a story, their understanding of a character evolves based on new events and information; and multiple fine-grained aspects of personalities can be perceived. This leads to a natural problem of situated and fine-grained personality understanding. The problem has not been studied in the NLP field, primarily due to the lack of appropriate datasets mimicking the process of book reading. We present the first labeled dataset PersoNet for this problem. Our novel annotation strategy involves annotating user notes from online reading apps as a proxy for the original books. Experiments and human studies indicate that our dataset construction is both efficient and accurate; and our task heavily relies on long-term context to achieve accurate predictions for both machines and humans. The dataset is available at https://github.com/Gorov/personet_acl23.
&lt;/p&gt;</description></item><item><title>sustain.AI&#26159;&#19968;&#20010;&#26234;&#33021;&#30340;&#12289;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#25512;&#33616;&#31995;&#32479;&#65292;&#21487;&#20197;&#24110;&#21161;&#23457;&#35745;&#24072;&#12289;&#37329;&#34701;&#25237;&#36164;&#32773;&#20197;&#21450;&#24191;&#22823;&#20844;&#20247;&#39640;&#25928;&#22320;&#20998;&#26512;&#20844;&#21496;&#30340;&#21487;&#25345;&#32493;&#24615;&#25253;&#21578;&#65292;&#24182;&#36890;&#36807;&#19982;GRI&#26631;&#20934;&#21305;&#37197;&#26469;&#25552;&#20379;&#26356;&#22909;&#30340;&#25512;&#33616;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2305.08711</link><description>&lt;p&gt;
sustain.AI: &#19968;&#31181;&#20998;&#26512;&#21487;&#25345;&#32493;&#24615;&#25253;&#21578;&#30340;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
sustain.AI: a Recommender System to analyze Sustainability Reports. (arXiv:2305.08711v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08711
&lt;/p&gt;
&lt;p&gt;
sustain.AI&#26159;&#19968;&#20010;&#26234;&#33021;&#30340;&#12289;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#25512;&#33616;&#31995;&#32479;&#65292;&#21487;&#20197;&#24110;&#21161;&#23457;&#35745;&#24072;&#12289;&#37329;&#34701;&#25237;&#36164;&#32773;&#20197;&#21450;&#24191;&#22823;&#20844;&#20247;&#39640;&#25928;&#22320;&#20998;&#26512;&#20844;&#21496;&#30340;&#21487;&#25345;&#32493;&#24615;&#25253;&#21578;&#65292;&#24182;&#36890;&#36807;&#19982;GRI&#26631;&#20934;&#21305;&#37197;&#26469;&#25552;&#20379;&#26356;&#22909;&#30340;&#25512;&#33616;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;sustain.AI&#65292;&#36825;&#26159;&#19968;&#20010;&#26234;&#33021;&#30340;&#12289;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#25512;&#33616;&#31995;&#32479;&#65292;&#21487;&#24110;&#21161;&#23457;&#35745;&#24072;&#12289;&#37329;&#34701;&#25237;&#36164;&#32773;&#20197;&#21450;&#24191;&#22823;&#20844;&#20247;&#39640;&#25928;&#22320;&#20998;&#26512;&#20844;&#21496;&#30340;&#21487;&#25345;&#32493;&#24615;&#25253;&#21578;&#12290;&#35813;&#24037;&#20855;&#21033;&#29992;&#20102;&#31471;&#21040;&#31471;&#21487;&#35757;&#32451;&#30340;&#26550;&#26500;&#65292;&#23558;&#22522;&#20110;BERT&#30340;&#32534;&#30721;&#27169;&#22359;&#19982;&#22810;&#26631;&#31614;&#20998;&#31867;&#22836;&#30456;&#32467;&#21512;&#65292;&#23558;&#21487;&#25345;&#32493;&#24615;&#25253;&#21578;&#20013;&#30340;&#30456;&#20851;&#25991;&#26412;&#27573;&#33853;&#19982;&#20840;&#29699;&#25253;&#21578;&#20513;&#35758;&#65288;GRI&#65289;&#26631;&#20934;&#20013;&#30340;&#30456;&#24212;&#27861;&#24459;&#27861;&#35268;&#21305;&#37197;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#26032;&#39062;&#30340;&#24503;&#22269;&#21487;&#25345;&#32493;&#24615;&#25253;&#21578;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#65292;&#24182;&#22987;&#32456;&#23454;&#29616;&#20102;&#19982;&#22810;&#20010;&#24378;&#22522;&#32447;&#27169;&#22411;&#30456;&#27604;&#26356;&#39640;&#30340;&#25512;&#33616;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;sustain.AI&#24050;&#32463;&#20844;&#24320;&#22312;https://sustain.ki.nrw/&#19978;&#25552;&#20379;&#32473;&#25152;&#26377;&#20154;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present $\text{sustain.AI}$, an intelligent, context-aware recommender system that assists auditors and financial investors as well as the general public to efficiently analyze companies' sustainability reports. The tool leverages an end-to-end trainable architecture that couples a BERT-based encoding module with a multi-label classification head to match relevant text passages from sustainability reports to their respective law regulations from the Global Reporting Initiative (GRI) standards. We evaluate our model on two novel German sustainability reporting data sets and consistently achieve a significantly higher recommendation performance compared to multiple strong baselines. Furthermore, $\text{sustain.AI}$ is publicly available for everyone at https://sustain.ki.nrw/.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026;PALR&#30340;&#26694;&#26550;&#65292;&#23558;&#29992;&#25143;&#30340;&#21382;&#21490;&#34892;&#20026;&#19982;LLMs&#30456;&#32467;&#21512;&#65292;&#29983;&#25104;&#29992;&#25143;&#21916;&#27426;&#30340;&#29289;&#21697;&#30340;&#25512;&#33616;&#12290;&#19982;&#29616;&#26377;&#30340;&#25512;&#33616;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;PALR&#26694;&#26550;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.07622</link><description>&lt;p&gt;
&#20010;&#24615;&#21270;&#24863;&#30693;&#30340;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;LMMs&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
PALR: Personalization Aware LLMs for Recommendation. (arXiv:2305.07622v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07622
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026;PALR&#30340;&#26694;&#26550;&#65292;&#23558;&#29992;&#25143;&#30340;&#21382;&#21490;&#34892;&#20026;&#19982;LLMs&#30456;&#32467;&#21512;&#65292;&#29983;&#25104;&#29992;&#25143;&#21916;&#27426;&#30340;&#29289;&#21697;&#30340;&#25512;&#33616;&#12290;&#19982;&#29616;&#26377;&#30340;&#25512;&#33616;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;PALR&#26694;&#26550;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30001;&#20110;&#20854;&#20986;&#33394;&#30340;&#24615;&#33021;&#32780;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;PALR&#65292;&#23558;&#29992;&#25143;&#30340;&#21382;&#21490;&#34892;&#20026;&#19982;LLMs&#30456;&#32467;&#21512;&#65292;&#20197;&#29983;&#25104;&#29992;&#25143;&#21916;&#27426;&#30340;&#29289;&#21697;&#30340;&#25512;&#33616;&#12290;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;&#29992;&#25143;/&#29289;&#21697;&#20114;&#21160;&#20316;&#20026;&#20505;&#36873;&#26816;&#32034;&#30340;&#25351;&#23548;&#65292;&#28982;&#21518;&#37319;&#29992;&#22522;&#20110;LLMs&#30340;&#25490;&#24207;&#27169;&#22411;&#29983;&#25104;&#25512;&#33616;&#29289;&#21697;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#29616;&#26377;&#30340;&#25512;&#33616;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;PALR&#26694;&#26550;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have recently received significant attention for their exceptional capabilities. Despite extensive efforts in developing general-purpose LLMs that can be utilized in various natural language processing (NLP) tasks, there has been less research exploring their potential in recommender systems. In this paper, we propose a novel framework, named PALR, which aiming to combine user history behaviors (such as clicks, purchases, ratings, etc.) with LLMs to generate user preferred items. Specifically, we first use user/item interactions as guidance for candidate retrieval. Then we adopt a LLM-based ranking model to generate recommended items. Unlike existing approaches that typically adopt general-purpose LLMs for zero/few-shot recommendation testing or training on small-sized language models (with less than 1 billion parameters), which cannot fully elicit LLMs' reasoning abilities and leverage rich item side parametric knowledge, we fine-tune a 7 billion parameter
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#39318;&#27425;&#23450;&#20041;&#20102;&#21463;&#38480;&#35821;&#35328;&#35268;&#21010;&#20219;&#21153;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36825;&#20010;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#24182;&#25552;&#21462;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#21463;&#38480;&#35821;&#35328;&#35268;&#21010;&#25968;&#25454;&#38598;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#20854;&#22312;&#32422;&#26463;&#24544;&#23454;&#24230;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#23545;&#36171;&#20104;&#36739;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#21463;&#38480;&#35821;&#35328;&#35268;&#21010;&#33021;&#21147;&#38750;&#24120;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2305.05252</link><description>&lt;p&gt;
&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#21462;&#33050;&#26412;&#30693;&#35782;&#20197;&#36827;&#34892;&#21463;&#38480;&#35821;&#35328;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Distilling Script Knowledge from Large Language Models for Constrained Language Planning. (arXiv:2305.05252v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05252
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#23450;&#20041;&#20102;&#21463;&#38480;&#35821;&#35328;&#35268;&#21010;&#20219;&#21153;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36825;&#20010;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#24182;&#25552;&#21462;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#21463;&#38480;&#35821;&#35328;&#35268;&#21010;&#25968;&#25454;&#38598;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#20854;&#22312;&#32422;&#26463;&#24544;&#23454;&#24230;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#23545;&#36171;&#20104;&#36739;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#21463;&#38480;&#35821;&#35328;&#35268;&#21010;&#33021;&#21147;&#38750;&#24120;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26085;&#24120;&#29983;&#27963;&#20013;&#65292;&#20154;&#20204;&#32463;&#24120;&#36890;&#36807;&#36981;&#24490;&#30446;&#26631;&#23548;&#21521;&#30340;&#33050;&#26412;&#24418;&#24335;&#30340;&#36880;&#27493;&#35828;&#26126;&#26469;&#35268;&#21010;&#33258;&#24049;&#30340;&#34892;&#21160;&#12290;&#20197;&#24448;&#30340;&#24037;&#20316;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#26469;&#20026;&#31435;&#20307;&#27963;&#21160;&#30340;&#25277;&#35937;&#30446;&#26631;&#65288;&#20363;&#22914;&#65292;&#8220;&#21046;&#20316;&#34507;&#31957;&#8221;&#65289;&#36827;&#34892;&#35268;&#21010;&#65292;&#20294;&#23545;&#20110;&#20855;&#26377;&#22810;&#26041;&#38754;&#32422;&#26463;&#30340;&#26356;&#20855;&#20307;&#30446;&#26631;&#65288;&#20363;&#22914;&#65292;&#8220;&#20026;&#31958;&#23615;&#30149;&#24739;&#32773;&#21046;&#20316;&#34507;&#31957;&#8221;&#65289;&#40092;&#26377;&#30740;&#31350;&#12290;&#26412;&#25991;&#39318;&#27425;&#23450;&#20041;&#20102;&#21463;&#38480;&#35821;&#35328;&#35268;&#21010;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36807;&#24230;&#29983;&#25104;&#24182;&#36807;&#28388;&#30340;&#26041;&#27861;&#26469;&#25913;&#21892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#36825;&#20010;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#24182;&#21033;&#29992;&#23427;&#26469;&#25552;&#21462;&#19968;&#31181;&#26032;&#39062;&#30340;&#21463;&#38480;&#35821;&#35328;&#35268;&#21010;&#25968;&#25454;&#38598;CoScript&#65292;&#20854;&#20013;&#21253;&#25324;55,000&#20010;&#33050;&#26412;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;LLM&#22312;&#21463;&#38480;&#35821;&#35328;&#35268;&#21010;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#29305;&#21035;&#26159;&#22312;&#32422;&#26463;&#24544;&#23454;&#24230;&#26041;&#38754;&#12290;&#27492;&#22806;&#65292;CoScript&#34987;&#35777;&#26126;&#23545;&#36171;&#20104;&#36739;&#23567;&#30340;LM&#21463;&#38480;&#35821;&#35328;&#35268;&#21010;&#33021;&#21147;&#26159;&#38750;&#24120;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
In everyday life, humans often plan their actions by following step-by-step instructions in the form of goal-oriented scripts. Previous work has exploited language models (LMs) to plan for abstract goals of stereotypical activities (e.g., "make a cake"), but leaves more specific goals with multi-facet constraints understudied (e.g., "make a cake for diabetics"). In this paper, we define the task of constrained language planning for the first time. We propose an overgenerate-then-filter approach to improve large language models (LLMs) on this task, and use it to distill a novel constrained language planning dataset, CoScript, which consists of 55,000 scripts. Empirical results demonstrate that our method significantly improves the constrained language planning ability of LLMs, especially on constraint faithfulness. Furthermore, CoScript is demonstrated to be quite effective in endowing smaller LMs with constrained language planning ability.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;ANALOGICAL&#8221;&#30340;&#26032;&#22411;&#22522;&#20934;&#65292;&#29992;&#20197;&#20869;&#22312;&#35780;&#20272;LLMs&#22312;&#38271;&#25991;&#26412;&#31867;&#27604;&#20013;&#30340;&#33021;&#21147;&#65292;&#21253;&#25324;&#20845;&#20010;&#22797;&#26434;&#32423;&#21035;&#30340;&#38271;&#25991;&#26412;&#31867;&#27604;&#20998;&#31867;&#65292;&#24182;&#20351;&#29992;13&#20010;&#25968;&#25454;&#38598;&#21644;&#19977;&#31181;&#36317;&#31163;&#24230;&#37327;&#26041;&#27861;&#26469;&#35780;&#20272;8&#20010;LLMs&#22312;&#35821;&#20041;&#21521;&#37327;&#31354;&#38388;&#20013;&#35782;&#21035;&#31867;&#27604;&#23545;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.05050</link><description>&lt;p&gt;
ANALOGICAL- &#19968;&#31181;&#26032;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#25991;&#26412;&#31867;&#27604;&#35780;&#27979;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
ANALOGICAL - A New Benchmark for Analogy of Long Text for Large Language Models. (arXiv:2305.05050v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05050
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;ANALOGICAL&#8221;&#30340;&#26032;&#22411;&#22522;&#20934;&#65292;&#29992;&#20197;&#20869;&#22312;&#35780;&#20272;LLMs&#22312;&#38271;&#25991;&#26412;&#31867;&#27604;&#20013;&#30340;&#33021;&#21147;&#65292;&#21253;&#25324;&#20845;&#20010;&#22797;&#26434;&#32423;&#21035;&#30340;&#38271;&#25991;&#26412;&#31867;&#27604;&#20998;&#31867;&#65292;&#24182;&#20351;&#29992;13&#20010;&#25968;&#25454;&#38598;&#21644;&#19977;&#31181;&#36317;&#31163;&#24230;&#37327;&#26041;&#27861;&#26469;&#35780;&#20272;8&#20010;LLMs&#22312;&#35821;&#20041;&#21521;&#37327;&#31354;&#38388;&#20013;&#35782;&#21035;&#31867;&#27604;&#23545;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#20013;&#65292;&#20197;&#35789;&#32423;&#21035;&#30340;&#31867;&#27604;&#20026;&#24418;&#24335;&#30340;&#31867;&#27604;&#22312;&#34913;&#37327;&#35832;&#22914;word2vec&#20043;&#31867;&#30340;&#35789;&#23884;&#20837;&#26041;&#27861;&#30340;&#36136;&#37327;&#26041;&#38754;&#21457;&#25381;&#20102;&#37325;&#35201;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#29616;&#20195;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#20027;&#35201;&#26681;&#25454;GLUE&#21644;SuperGLUE&#31561;&#22522;&#20934;&#30340;&#22806;&#22312;&#37327;&#24230;&#36827;&#34892;&#35780;&#20272;&#65292;&#32780;&#22312;LLMs&#26159;&#21542;&#33021;&#22815;&#22312;&#38271;&#25991;&#26412;&#20013;&#32472;&#21046;&#31867;&#27604;&#30340;&#26041;&#38754;&#65292;&#21482;&#26377;&#23569;&#25968;&#20960;&#39033;&#30740;&#31350;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;ANALOGICAL&#8221;&#30340;&#26032;&#22411;&#22522;&#20934;&#65292;&#20197;&#20845;&#20010;&#22797;&#26434;&#32423;&#21035;&#30340;&#38271;&#25991;&#26412;&#31867;&#27604;&#20998;&#31867;&#23545;LLMs&#36827;&#34892;&#20869;&#22312;&#35780;&#20272;&#65292;&#20998;&#21035;&#20026; (i)&#21333;&#35789;&#12289;(ii)&#21333;&#35789;vs&#21477;&#23376;&#12289;(iii)&#35821;&#27861;&#12289;(iv)&#21542;&#23450;&#12289;(v)&#34164;&#21547;&#21644;(vi)&#38544;&#21947;&#12290;&#21033;&#29992;13&#20010;&#25968;&#25454;&#38598;&#21644;&#19977;&#31181;&#19981;&#21516;&#30340;&#36317;&#31163;&#24230;&#37327;&#26041;&#27861;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;8&#20010;LLMs&#22312;&#35821;&#20041;&#21521;&#37327;&#31354;&#38388;&#20013;&#35782;&#21035;&#31867;&#27604;&#23545;&#30340;&#33021;&#21147;(&#20363;&#22914;&#65292;&#8220;&#25105;&#33021;&#35828;&#20004;&#31181;&#35821;&#35328;&#8221;&#24212;&#35813;&#26356;&#25509;&#36817;&#8220;&#25105;&#26159;&#21452;&#35821;&#30340;&#8221;&#65292;&#32780;&#8220;&#25105;&#21916;&#27426;&#24039;&#20811;&#21147;&#8221;&#21644;&#8220;&#25105;&#19981;&#21916;&#27426;&#24039;&#20811;&#21147;&#8221;&#24212;&#35813;&#26159;&#27491;&#20132;&#30340;)&#12290;
&lt;/p&gt;
&lt;p&gt;
Over the past decade, analogies, in the form of word-level analogies, have played a significant role as an intrinsic measure of evaluating the quality of word embedding methods such as word2vec. Modern large language models (LLMs), however, are primarily evaluated on extrinsic measures based on benchmarks such as GLUE and SuperGLUE, and there are only a few investigations on whether LLMs can draw analogies between long texts. In this paper, we present ANALOGICAL, a new benchmark to intrinsically evaluate LLMs across a taxonomy of analogies of long text with six levels of complexity -- (i) word, (ii) word vs. sentence, (iii) syntactic, (iv) negation, (v) entailment, and (vi) metaphor. Using thirteen datasets and three different distance measures, we evaluate the abilities of eight LLMs in identifying analogical pairs in the semantic vector space (e.g., "I can speak two languages" should be closer to "I am bilingual" while "I like chocolate" and "I do not like chocolate" should be orthog
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#22914;&#20309;&#21387;&#32553;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#27169;&#22411;&#20197;&#36866;&#24212;&#23454;&#38469;&#24212;&#29992;&#38656;&#27714;&#65292;&#36890;&#36807;&#20351;&#29992;&#30693;&#35782;&#33976;&#39311;&#21644;&#20266;&#30446;&#26631;&#35757;&#32451;&#25216;&#26415;&#38024;&#23545;&#29305;&#23450;&#30340;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#36827;&#34892;&#20248;&#21270;&#65292;&#24182;&#21462;&#24471;&#20102;&#26174;&#33879;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.02031</link><description>&lt;p&gt;
&#31995;&#32479;&#30740;&#31350;&#22522;&#20110;&#20266;&#30446;&#26631;&#35757;&#32451;&#30340;&#30693;&#35782;&#33976;&#39311;&#29992;&#20110;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
A Systematic Study of Knowledge Distillation for Natural Language Generation with Pseudo-Target Training. (arXiv:2305.02031v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02031
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#22914;&#20309;&#21387;&#32553;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#27169;&#22411;&#20197;&#36866;&#24212;&#23454;&#38469;&#24212;&#29992;&#38656;&#27714;&#65292;&#36890;&#36807;&#20351;&#29992;&#30693;&#35782;&#33976;&#39311;&#21644;&#20266;&#30446;&#26631;&#35757;&#32451;&#25216;&#26415;&#38024;&#23545;&#29305;&#23450;&#30340;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#36827;&#34892;&#20248;&#21270;&#65292;&#24182;&#21462;&#24471;&#20102;&#26174;&#33879;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#27169;&#22411;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#21644;&#23384;&#20648;&#36164;&#28304;&#12290;&#26412;&#25991;&#30740;&#31350;&#21387;&#32553;&#36825;&#20123;&#27169;&#22411;&#30340;&#28508;&#21147;&#65292;&#36825;&#23545;&#20110;&#26381;&#21153;&#25968;&#30334;&#19975;&#29992;&#25143;&#30340;&#23454;&#38469;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#32858;&#28966;&#20110;&#30693;&#35782;&#33976;&#39311;&#65288;KD&#65289;&#25216;&#26415;&#65292;&#20854;&#20013;&#23567;&#30340;&#23398;&#29983;&#27169;&#22411;&#23398;&#20064;&#27169;&#20223;&#22823;&#30340;&#25945;&#24072;&#27169;&#22411;&#65292;&#20351;&#24471;&#21487;&#20197;&#20174;&#25945;&#24072;&#21521;&#23398;&#29983;&#20256;&#36882;&#30693;&#35782;&#12290;&#19982;&#20043;&#21069;&#30340;&#22823;&#37096;&#20998;&#24037;&#20316;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#38024;&#23545;&#29305;&#23450;&#30340;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#20248;&#21270;&#27169;&#22411;&#12290;&#36890;&#24120;&#65292;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#20013;&#65292;&#38500;&#20102;&#26377;&#26631;&#35760;&#25968;&#25454;&#22806;&#65292;&#36824;&#26377;&#22823;&#37327;&#30340;&#26410;&#26631;&#35760;&#20219;&#21153;&#29305;&#23450;&#25968;&#25454;&#65292;&#36825;&#23545;&#20110;&#36890;&#36807;&#30693;&#35782;&#33976;&#39311;&#33719;&#24471;&#39640;&#21387;&#32553;&#29575;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#22312;&#29616;&#23454;&#30340;&#20551;&#35774;&#19979;&#65292;&#23545;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#20219;&#21153;&#36827;&#34892;&#20102;&#31995;&#32479;&#30340;&#20219;&#21153;&#29305;&#23450;&#30693;&#35782;&#33976;&#39311;&#30740;&#31350;&#65292;&#24182;&#35752;&#35770;&#20102;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#33976;&#39311;&#30340;&#29305;&#27530;&#29305;&#24449;&#65292;&#23588;&#20854;&#26159;&#26333;&#20809;&#20559;&#24046;&#38382;&#39064;&#12290;&#25509;&#30528;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#19968;&#31995;&#21015;&#20266;&#30446;&#26631;&#35757;&#32451;&#65288;PTT&#65289;&#25216;&#26415;&#65292;&#32531;&#35299;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#25552;&#39640;&#20102;&#23398;&#29983;&#27169;&#22411;&#30340;&#36136;&#37327;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#35780;&#20272;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#19981;&#21516;&#30340;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern Natural Language Generation (NLG) models come with massive computational and storage requirements. In this work, we study the potential of compressing them, which is crucial for real-world applications serving millions of users. We focus on Knowledge Distillation (KD) techniques, in which a small student model learns to imitate a large teacher model, allowing to transfer knowledge from the teacher to the student. In contrast to much of the previous work, our goal is to optimize the model for a specific NLG task and a specific dataset. Typically, in real-world applications, in addition to labeled data there is abundant unlabeled task-specific data, which is crucial for attaining high compression rates via KD. In this work, we conduct a systematic study of task-specific KD techniques for various NLG tasks under realistic assumptions. We discuss the special characteristics of NLG distillation and particularly the exposure bias problem. Following, we derive a family of Pseudo-Target
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20221;&#21253;&#21547;&#30495;&#23454;&#28216;&#25103;&#29366;&#24577;&#20449;&#24687;&#30340;Dungeons &amp; Dragons&#23454;&#38469;&#28216;&#25103;&#25968;&#25454;&#38598;FIREBALL&#65292;&#23427;&#21487;&#20197;&#25913;&#21892;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#30340;&#36136;&#37327;&#12290;&#27492;&#22806;&#65292;LLMs&#21487;&#20197;&#20351;&#29992;FIREBALL&#20013;&#30340;&#28216;&#25103;&#29366;&#24577;&#20449;&#24687;&#26469;&#29983;&#25104;&#26356;&#39640;&#36136;&#37327;&#30340;&#28216;&#25103;&#22238;&#21512;&#12290;</title><link>http://arxiv.org/abs/2305.01528</link><description>&lt;p&gt;
FIREBALL&#65306;&#19968;&#20221;&#21253;&#21547;&#32467;&#26500;&#21270;&#28216;&#25103;&#29366;&#24577;&#20449;&#24687;&#30340;Dungeons &amp; Dragons&#23454;&#38469;&#28216;&#25103;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
FIREBALL: A Dataset of Dungeons and Dragons Actual-Play with Structured Game State Information. (arXiv:2305.01528v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01528
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20221;&#21253;&#21547;&#30495;&#23454;&#28216;&#25103;&#29366;&#24577;&#20449;&#24687;&#30340;Dungeons &amp; Dragons&#23454;&#38469;&#28216;&#25103;&#25968;&#25454;&#38598;FIREBALL&#65292;&#23427;&#21487;&#20197;&#25913;&#21892;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#30340;&#36136;&#37327;&#12290;&#27492;&#22806;&#65292;LLMs&#21487;&#20197;&#20351;&#29992;FIREBALL&#20013;&#30340;&#28216;&#25103;&#29366;&#24577;&#20449;&#24687;&#26469;&#29983;&#25104;&#26356;&#39640;&#36136;&#37327;&#30340;&#28216;&#25103;&#22238;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Dungeons &amp; Dragons&#65288;D&#65286;D&#65289;&#26159;&#19968;&#27454;&#26700;&#38754;&#35282;&#33394;&#25198;&#28436;&#28216;&#25103;&#65292;&#20854;&#29609;&#23478;&#20043;&#38388;&#23384;&#22312;&#22797;&#26434;&#30340;&#33258;&#28982;&#35821;&#35328;&#20132;&#20114;&#21644;&#38544;&#34255;&#30340;&#29366;&#24577;&#20449;&#24687;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#25317;&#26377;&#29366;&#24577;&#20449;&#24687;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#30340;&#28216;&#25103;&#22238;&#21512;&#27604;&#20165;&#20351;&#29992;&#23545;&#35805;&#21382;&#21490;&#30340;LLMs&#26356;&#20855;&#39640;&#36136;&#37327;&#12290;&#28982;&#32780;&#65292;&#20197;&#24448;&#30340;&#30740;&#31350;&#20351;&#29992;&#30340;&#28216;&#25103;&#29366;&#24577;&#20449;&#24687;&#26159;&#21551;&#21457;&#24335;&#21019;&#24314;&#30340;&#65292;&#24182;&#19981;&#26159;&#30495;&#27491;&#30340;&#40644;&#37329;&#26631;&#20934;&#28216;&#25103;&#29366;&#24577;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;FIREBALL&#65292;&#36825;&#26159;&#19968;&#20010;&#21253;&#21547;&#30495;&#23454;&#28216;&#25103;&#29366;&#24577;&#20449;&#24687;&#30340;&#22823;&#22411;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#26469;&#33258;Discord&#30340;&#36817;25,000&#20010;&#30495;&#23454;D&#65286;D&#28216;&#25103;&#20250;&#35805;&#12290;&#25105;&#20204;&#35760;&#24405;&#20102;&#20351;&#29992;Avrae&#26426;&#22120;&#20154;&#30340;&#29609;&#23478;&#30340;&#28216;&#25103;&#20250;&#35805;&#65292;&#35813;&#26426;&#22120;&#20154;&#26159;&#20026;&#20102;&#24110;&#21161;&#20154;&#20204;&#22312;&#32447;&#29609;D&#65286;D&#32780;&#24320;&#21457;&#30340;&#65292;&#24182;&#25429;&#33719;&#20102;&#35821;&#35328;&#12289;&#28216;&#25103;&#21629;&#20196;&#21644;&#22522;&#30784;&#28216;&#25103;&#29366;&#24577;&#20449;&#24687;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#36890;&#36807;&#20351;&#29992;Avrae&#29366;&#24577;&#20449;&#24687;&#65292;FIREBALL&#21487;&#20197;&#25552;&#39640;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#65288;NLG&#65289;&#65292;&#20174;&#32780;&#25552;&#39640;&#33258;&#21160;&#35780;&#20272;&#25351;&#26631;&#21644;&#20154;&#31867;&#30340;&#36136;&#37327;&#35780;&#21028;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;LLMs&#21487;&#20197;&#29983;&#25104;&#8230;
&lt;/p&gt;
&lt;p&gt;
Dungeons &amp; Dragons (D&amp;D) is a tabletop roleplaying game with complex natural language interactions between players and hidden state information. Recent work has shown that large language models (LLMs) that have access to state information can generate higher quality game turns than LLMs that use dialog history alone. However, previous work used game state information that was heuristically created and was not a true gold standard game state. We present FIREBALL, a large dataset containing nearly 25,000 unique sessions from real D\&amp;D gameplay on Discord with true game state info. We recorded game play sessions of players who used the Avrae bot, which was developed to aid people in playing D&amp;D online, capturing language, game commands and underlying game state information. We demonstrate that FIREBALL can improve natural language generation (NLG) by using Avrae state information, improving both automated metrics and human judgments of quality. Additionally, we show that LLMs can generate
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861; Tree-structured Parzen estimator (TPE)&#65292;&#24182;&#23545;&#20854;&#25511;&#21046;&#21442;&#25968;&#30340;&#20316;&#29992;&#21644;&#31639;&#27861;&#30452;&#35273;&#36827;&#34892;&#20102;&#35752;&#35770;&#21644;&#20998;&#26512;&#65292;&#25552;&#20379;&#20102;&#19968;&#32452;&#25512;&#33616;&#35774;&#32622;&#24182;&#35777;&#26126;&#20854;&#33021;&#22815;&#25552;&#39640;TPE&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2304.11127</link><description>&lt;p&gt;
&#26641;&#29366;Parzen&#20272;&#35745;&#22120;&#65306;&#29702;&#35299;&#20854;&#31639;&#27861;&#32452;&#25104;&#37096;&#20998;&#21450;&#20854;&#22312;&#25552;&#39640;&#23454;&#35777;&#34920;&#29616;&#20013;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Tree-structured Parzen estimator: Understanding its algorithm components and their roles for better empirical performance. (arXiv:2304.11127v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11127
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861; Tree-structured Parzen estimator (TPE)&#65292;&#24182;&#23545;&#20854;&#25511;&#21046;&#21442;&#25968;&#30340;&#20316;&#29992;&#21644;&#31639;&#27861;&#30452;&#35273;&#36827;&#34892;&#20102;&#35752;&#35770;&#21644;&#20998;&#26512;&#65292;&#25552;&#20379;&#20102;&#19968;&#32452;&#25512;&#33616;&#35774;&#32622;&#24182;&#35777;&#26126;&#20854;&#33021;&#22815;&#25552;&#39640;TPE&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#39046;&#22495;&#20013;&#26368;&#36817;&#30340;&#36827;&#23637;&#35201;&#27714;&#26356;&#21152;&#22797;&#26434;&#30340;&#23454;&#39564;&#35774;&#35745;&#12290;&#36825;&#31181;&#22797;&#26434;&#30340;&#23454;&#39564;&#36890;&#24120;&#26377;&#35768;&#22810;&#21442;&#25968;&#65292;&#38656;&#35201;&#21442;&#25968;&#35843;&#25972;&#12290;Tree-structured Parzen estimator (TPE) &#26159;&#19968;&#31181;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#65292;&#22312;&#26368;&#36817;&#30340;&#21442;&#25968;&#35843;&#25972;&#26694;&#26550;&#20013;&#34987;&#24191;&#27867;&#20351;&#29992;&#12290;&#23613;&#31649;&#23427;&#24456;&#21463;&#27426;&#36814;&#65292;&#20294;&#25511;&#21046;&#21442;&#25968;&#30340;&#35282;&#33394;&#21644;&#31639;&#27861;&#30452;&#35273;&#23578;&#26410;&#24471;&#21040;&#35752;&#35770;&#12290;&#22312;&#26412;&#25945;&#31243;&#20013;&#65292;&#25105;&#20204;&#23558;&#30830;&#23450;&#27599;&#20010;&#25511;&#21046;&#21442;&#25968;&#30340;&#20316;&#29992;&#20197;&#21450;&#23427;&#20204;&#23545;&#36229;&#21442;&#25968;&#20248;&#21270;&#30340;&#24433;&#21709;&#65292;&#20351;&#29992;&#22810;&#31181;&#22522;&#20934;&#27979;&#35797;&#12290;&#25105;&#20204;&#23558;&#20174;&#21078;&#26512;&#30740;&#31350;&#20013;&#24471;&#20986;&#30340;&#25512;&#33616;&#35774;&#32622;&#19982;&#22522;&#20934;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#65292;&#24182;&#35777;&#26126;&#25105;&#20204;&#30340;&#25512;&#33616;&#35774;&#32622;&#25552;&#39640;&#20102;TPE&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;TPE&#23454;&#29616;&#21487;&#22312;https://github.com/nabenabe0928/tpe/tree/single-opt&#20013;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in many domains require more and more complicated experiment design. Such complicated experiments often have many parameters, which necessitate parameter tuning. Tree-structured Parzen estimator (TPE), a Bayesian optimization method, is widely used in recent parameter tuning frameworks. Despite its popularity, the roles of each control parameter and the algorithm intuition have not been discussed so far. In this tutorial, we will identify the roles of each control parameter and their impacts on hyperparameter optimization using a diverse set of benchmarks. We compare our recommended setting drawn from the ablation study with baseline methods and demonstrate that our recommended setting improves the performance of TPE. Our TPE implementation is available at https://github.com/nabenabe0928/tpe/tree/single-opt.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21033;&#29992;&#20301;&#32622;&#21435;&#22122;&#39044;&#27979;&#26131;&#24471;&#20960;&#20309;&#32467;&#26500;&#30340;&#37327;&#23376;&#21270;&#23398;&#24615;&#36136;&#65292;&#21487;&#20197;&#29992;&#30456;&#23545;&#23481;&#26131;&#33719;&#24471;&#30340;&#20960;&#20309;&#32467;&#26500;&#65292;&#31934;&#30830;&#39044;&#27979;&#24615;&#36136;&#65292;&#22312;&#20998;&#23376;&#24615;&#36136;&#20197;&#21450;&#21270;&#23398;&#21453;&#24212;&#24615;&#36136;&#30340;&#39044;&#27979;&#20219;&#21153;&#20013;&#37117;&#34920;&#29616;&#20248;&#31168;&#12290;</title><link>http://arxiv.org/abs/2304.03724</link><description>&lt;p&gt;
&#21033;&#29992;&#20301;&#32622;&#21435;&#22122;&#39044;&#27979;&#26131;&#24471;&#20960;&#20309;&#32467;&#26500;&#30340;&#37327;&#23376;&#21270;&#23398;&#24615;&#36136;
&lt;/p&gt;
&lt;p&gt;
Predicting quantum chemical property with easy-to-obtain geometry via positional denoising. (arXiv:2304.03724v1 [physics.chem-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03724
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21033;&#29992;&#20301;&#32622;&#21435;&#22122;&#39044;&#27979;&#26131;&#24471;&#20960;&#20309;&#32467;&#26500;&#30340;&#37327;&#23376;&#21270;&#23398;&#24615;&#36136;&#65292;&#21487;&#20197;&#29992;&#30456;&#23545;&#23481;&#26131;&#33719;&#24471;&#30340;&#20960;&#20309;&#32467;&#26500;&#65292;&#31934;&#30830;&#39044;&#27979;&#24615;&#36136;&#65292;&#22312;&#20998;&#23376;&#24615;&#36136;&#20197;&#21450;&#21270;&#23398;&#21453;&#24212;&#24615;&#36136;&#30340;&#39044;&#27979;&#20219;&#21153;&#20013;&#37117;&#34920;&#29616;&#20248;&#31168;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#37327;&#23376;&#21270;&#23398;&#24615;&#36136;&#19982;&#20854;&#20960;&#20309;&#32467;&#26500;&#26377;&#37325;&#35201;&#20851;&#32852;&#65292;&#20351;&#29992;3D&#20960;&#20309;&#20449;&#24687;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#35768;&#22810;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;&#39044;&#27979;&#31934;&#24230;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#36890;&#24120;&#38656;&#35201;&#39640;&#32423;&#37327;&#23376;&#21147;&#23398;&#35745;&#31639;&#24471;&#20986;&#30340;3D&#20960;&#20309;&#32467;&#26500;&#65292;&#36825;&#22312;&#23454;&#38469;&#38382;&#39064;&#20013;&#26159;&#19981;&#21487;&#34892;&#30340;&#65292;&#38480;&#21046;&#20102;&#20854;&#22312;&#29616;&#23454;&#38382;&#39064;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21033;&#29992;&#30456;&#23545;&#23481;&#26131;&#33719;&#24471;&#30340;&#20960;&#20309;&#32467;&#26500;&#65288;&#20363;&#22914;&#26469;&#33258;&#20998;&#23376;&#21147;&#22330;&#30340;&#20248;&#21270;&#20960;&#20309;&#32467;&#26500;&#65289;&#31934;&#30830;&#39044;&#27979;&#24615;&#36136;&#12290;&#22312;&#36825;&#31181;&#26041;&#27861;&#20013;&#65292;&#36755;&#20837;&#20960;&#20309;&#32467;&#26500;&#36880;&#28176;&#25509;&#36817;&#27491;&#30830;&#20960;&#20309;&#32467;&#26500;&#65292;&#36890;&#36807;&#22534;&#21472;&#21435;&#22122;&#23618;&#12290;&#25105;&#20204;&#20351;&#29992;3D&#28040;&#24687;&#20256;&#36882;&#20307;&#31995;&#32467;&#26500;&#30740;&#31350;&#20102;&#35813;&#26041;&#27861;&#22312;&#20004;&#20010;&#39044;&#27979;&#20219;&#21153;&#65288;&#20998;&#23376;&#24615;&#36136;&#21644;&#21270;&#23398;&#21453;&#24212;&#24615;&#36136;&#65289;&#20013;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#21435;&#22122;&#36807;&#31243;&#20943;&#23569;&#20301;&#32622;&#35823;&#24046;&#26377;&#21161;&#20110;&#24615;&#33021;&#30340;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
As quantum chemical properties have a significant dependence on their geometries, graph neural networks (GNNs) using 3D geometric information have achieved high prediction accuracy in many tasks. However, they often require 3D geometries obtained from high-level quantum mechanical calculations, which are practically infeasible, limiting their applicability in real-world problems. To tackle this, we propose a method to accurately predict the properties with relatively easy-to-obtain geometries (e.g., optimized geometries from the molecular force field). In this method, the input geometry, regarded as the corrupted geometry of the correct one, gradually approaches the correct one as it passes through the stacked denoising layers. We investigated the performance of the proposed method using 3D message-passing architectures for two prediction tasks: molecular properties and chemical reaction property. The reduction of positional errors through the denoising process contributed to performan
&lt;/p&gt;</description></item><item><title>&#33258;&#25105;&#21453;&#39304;&#36845;&#20195;&#31934;&#28860;&#26159;&#19968;&#31181;&#26080;&#38656;&#30417;&#30563;&#23398;&#20064;&#25110;&#21152;&#24378;&#23398;&#20064;&#30340;LLMs&#21021;&#22987;&#36755;&#20986;&#20248;&#21270;&#26041;&#27861;&#65292;&#20248;&#20110;&#30452;&#25509;&#29983;&#25104;&#65292;&#34987;&#35777;&#23454;&#22312;7&#20010;&#19981;&#21516;&#20219;&#21153;&#20013;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2303.17651</link><description>&lt;p&gt;
&#33258;&#25105;&#21453;&#39304;&#36845;&#20195;&#31934;&#28860;&#65306;&#19968;&#31181;&#26080;&#38656;&#30417;&#30563;&#23398;&#20064;&#25110;&#21152;&#24378;&#23398;&#20064;&#30340;LM&#25913;&#36827;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Self-Refine: Iterative Refinement with Self-Feedback. (arXiv:2303.17651v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17651
&lt;/p&gt;
&lt;p&gt;
&#33258;&#25105;&#21453;&#39304;&#36845;&#20195;&#31934;&#28860;&#26159;&#19968;&#31181;&#26080;&#38656;&#30417;&#30563;&#23398;&#20064;&#25110;&#21152;&#24378;&#23398;&#20064;&#30340;LLMs&#21021;&#22987;&#36755;&#20986;&#20248;&#21270;&#26041;&#27861;&#65292;&#20248;&#20110;&#30452;&#25509;&#29983;&#25104;&#65292;&#34987;&#35777;&#23454;&#22312;7&#20010;&#19981;&#21516;&#20219;&#21153;&#20013;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#35821;&#35328;&#27169;&#22411;(LLMs)&#19981;&#24635;&#26159;&#33021;&#22312;&#31532;&#19968;&#27425;&#33391;&#22909;&#22320;&#35299;&#20915;&#29983;&#25104;&#38382;&#39064;&#65288;&#22914;&#25688;&#35201;&#12289;&#31572;&#26696;&#12289;&#35299;&#37322;&#31561;&#65289;&#65292;&#25105;&#20204;&#24341;&#20837;&#33258;&#25105;&#21453;&#39304;&#36845;&#20195;&#31934;&#28860;&#65288;SELF-REFINE&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#36845;&#20195;&#21453;&#39304;&#21644;&#31934;&#28860;&#30456;&#20284;&#22320;&#20248;&#21270;LLMs&#30340;&#21021;&#22987;&#36755;&#20986;&#12290;&#20027;&#35201;&#24605;&#24819;&#26159;&#65306;&#20351;&#29992;LLM&#29983;&#25104;&#36755;&#20986;&#65292;&#28982;&#21518;&#20801;&#35768;&#21516;&#19968;&#27169;&#22411;&#25552;&#20379;&#20854;&#33258;&#36523;&#36755;&#20986;&#30340;&#22810;&#26041;&#38754;&#21453;&#39304;&#65292;&#26368;&#21518;&#21033;&#29992;&#21453;&#39304;&#20351;&#30456;&#21516;&#27169;&#22411;&#31934;&#28860;&#20808;&#21069;&#29983;&#25104;&#30340;&#36755;&#20986;&#12290;&#25105;&#20204;&#30340;&#36845;&#20195;&#31934;&#28860;&#26694;&#26550;&#19982;&#26089;&#26399;&#24037;&#20316;&#19981;&#21516;&#65292;&#26080;&#38656;&#30417;&#30563;&#35757;&#32451;&#25968;&#25454;&#25110;&#21152;&#24378;&#23398;&#20064;&#65292;&#24182;&#19988;&#21487;&#20197;&#19982;&#21333;&#20010;LLM&#19968;&#36215;&#20351;&#29992;&#12290;&#25105;&#20204;&#23545;&#19971;&#20010;&#19981;&#21516;&#30340;&#20219;&#21153;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#33539;&#22260;&#20174;&#35780;&#35770;&#37325;&#20889;&#21040;&#25968;&#23398;&#25512;&#29702;&#65292;&#34920;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#30452;&#25509;&#29983;&#25104;&#12290;&#22312;&#25152;&#26377;&#20219;&#21153;&#20013;&#65292;&#20351;&#29992;SELF-REFINE&#29983;&#25104;&#30340;&#36755;&#20986;&#34987;&#20154;&#31867;&#21644;&#33258;&#21160;&#21270;&#25351;&#26631;&#20248;&#20808;&#20110;&#20351;&#29992;GPT-3.5&#21644;GPT-4&#30452;&#25509;&#29983;&#25104;&#30340;&#36755;&#20986;&#65292;&#34920;&#29616;&#24471;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Like people, LLMs do not always generate the best text for a given generation problem on their first try (e.g., summaries, answers, explanations). Just as people then refine their text, we introduce SELF-REFINE, a framework for similarly improving initial outputs from LLMs through iterative feedback and refinement. The main idea is to generate an output using an LLM, then allow the same model to provide multi-aspect feedback for its own output; finally, the same model refines its previously generated output given its own feedback. Unlike earlier work, our iterative refinement framework does not require supervised training data or reinforcement learning, and works with a single LLM. We experiment with 7 diverse tasks, ranging from review rewriting to math reasoning, demonstrating that our approach outperforms direct generation. In all tasks, outputs generated with SELF-REFINE are preferred by humans and by automated metrics over those generated directly with GPT-3.5 and GPT-4, improving
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#38646;&#21644;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#30340;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#31574;&#30053;&#36845;&#20195;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.09716</link><description>&lt;p&gt;
&#38646;&#21644;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#20013;&#24378;&#21270;&#23398;&#20064;&#30340;&#26032;&#25919;&#31574;&#36845;&#20195;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
A New Policy Iteration Algorithm For Reinforcement Learning in Zero-Sum Markov Games. (arXiv:2303.09716v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09716
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#38646;&#21644;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#30340;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#31574;&#30053;&#36845;&#20195;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#21487;&#20197;&#34987;&#35270;&#20026;&#20855;&#26377;&#20004;&#20010;&#38454;&#27573;: &#23398;&#20064;&#38454;&#27573;&#21644;&#35268;&#21010;&#38454;&#27573;&#12290;&#22312;&#26631;&#20934;MDPs&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#20351;&#29992;&#20215;&#20540;&#36845;&#20195;&#25110;&#31574;&#30053;&#36845;&#20195;&#26469;&#35299;&#20915;&#23398;&#20064;&#38382;&#39064;&#12290;&#20294;&#22312;&#38646;&#21644;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#30340;&#24773;&#20917;&#19979;&#65292;&#27809;&#26377;&#26377;&#25928;&#30340;&#31574;&#30053;&#36845;&#20195;&#31639;&#27861;&#65292;&#20197;&#21069;&#30340;&#23581;&#35797;&#37117;&#26377;&#23616;&#38480;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#31574;&#30053;&#36845;&#20195;&#21464;&#20307;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many model-based reinforcement learning (RL) algorithms can be viewed as having two phases that are iteratively implemented: a learning phase where the model is approximately learned and a planning phase where the learned model is used to derive a policy. In the case of standard MDPs, the learning problem can be solved using either value iteration or policy iteration. However, in the case of zero-sum Markov games, there is no efficient policy iteration algorithm; e.g., it has been shown in Hansen et al. (2013) that one has to solve Omega(1/(1-alpha)) MDPs, where alpha is the discount factor, to implement the only known convergent version of policy iteration. Another algorithm for Markov zero-sum games, called naive policy iteration, is easy to implement but is only provably convergent under very restrictive assumptions. Prior attempts to fix naive policy iteration algorithm have several limitations. Here, we show that a simple variant of naive policy iteration for games converges, and 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22522;&#20110;&#25968;&#19975;&#20010;&#38646;-shot&#23454;&#39564;&#23545;&#22522;&#20110;&#21518;&#35757;&#32451;&#37327;&#21270;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19981;&#21516;&#37327;&#21270;&#32452;&#20214;&#36827;&#34892;&#20102;&#32508;&#21512;&#30740;&#31350;&#65292;&#32467;&#26524;&#21457;&#29616;&#32454;&#31890;&#24230;&#37327;&#21270;&#21644;&#21518;&#35757;&#32451;&#37327;&#21270;&#26041;&#27861;&#24456;&#37325;&#35201;&#65292;&#29992;&#31895;&#31890;&#24230;&#37327;&#21270;&#30340;&#26356;&#39640;&#20301;&#25968;&#27604;&#29992;&#38750;&#24120;&#32454;&#31890;&#24230;&#30340;&#26356;&#20302;&#20301;&#25968;&#26356;&#24378;&#22823;&#12290;&#25105;&#20204;&#32473;&#20986;&#20102;&#22914;&#20309;&#20026;&#19981;&#21516;&#22823;&#23567;&#30340;\llms&#21033;&#29992;&#37327;&#21270;&#30340;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2303.08302</link><description>&lt;p&gt;
&#22522;&#20110;&#21518;&#35757;&#32451;&#37327;&#21270;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32508;&#21512;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Study on Post-Training Quantization for Large Language Models. (arXiv:2303.08302v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08302
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22522;&#20110;&#25968;&#19975;&#20010;&#38646;-shot&#23454;&#39564;&#23545;&#22522;&#20110;&#21518;&#35757;&#32451;&#37327;&#21270;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19981;&#21516;&#37327;&#21270;&#32452;&#20214;&#36827;&#34892;&#20102;&#32508;&#21512;&#30740;&#31350;&#65292;&#32467;&#26524;&#21457;&#29616;&#32454;&#31890;&#24230;&#37327;&#21270;&#21644;&#21518;&#35757;&#32451;&#37327;&#21270;&#26041;&#27861;&#24456;&#37325;&#35201;&#65292;&#29992;&#31895;&#31890;&#24230;&#37327;&#21270;&#30340;&#26356;&#39640;&#20301;&#25968;&#27604;&#29992;&#38750;&#24120;&#32454;&#31890;&#24230;&#30340;&#26356;&#20302;&#20301;&#25968;&#26356;&#24378;&#22823;&#12290;&#25105;&#20204;&#32473;&#20986;&#20102;&#22914;&#20309;&#20026;&#19981;&#21516;&#22823;&#23567;&#30340;\llms&#21033;&#29992;&#37327;&#21270;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21518;&#35757;&#32451;&#37327;&#21270;&#26159;&#19968;&#31181;&#20943;&#23569;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20869;&#23384;&#28040;&#32791;&#21644;/&#25110;&#35745;&#31639;&#25104;&#26412;&#30340;&#26435;&#34913;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#19981;&#21516;&#37327;&#21270;&#26041;&#26696;&#12289;&#19981;&#21516;&#27169;&#22411;&#26063;&#12289;&#19981;&#21516;&#21518;&#35757;&#32451;&#37327;&#21270;&#26041;&#27861;&#12289;&#19981;&#21516;&#37327;&#21270;&#20301;&#31934;&#24230;&#31561;&#30340;&#24433;&#21709;&#30340;&#20840;&#38754;&#30740;&#31350;&#20173;&#32570;&#22833;&#12290;&#26412;&#25991;&#36890;&#36807;&#25968;&#19975;&#20010;&#38646;-shot&#23454;&#39564;&#23545;&#36825;&#20123;&#32452;&#20214;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65306;(1)&#32454;&#31890;&#24230;&#37327;&#21270;&#21644;&#21518;&#35757;&#32451;&#37327;&#21270;&#26041;&#27861;(&#32780;&#19981;&#26159;&#26420;&#32032;&#30340;&#26368;&#36817;&#33293;&#20837;&#37327;&#21270;)&#26159;&#23454;&#29616;&#33391;&#22909;&#31934;&#24230;&#30340;&#24517;&#35201;&#26465;&#20214;&#65307;(2) &#29992;&#31895;&#31890;&#24230;&#37327;&#21270;&#30340;&#26356;&#39640;&#20301;&#25968;&#65288;&#22914;5&#20301;&#65289;&#27604;&#29992;&#38750;&#24120;&#32454;&#31890;&#24230;&#30340;&#26356;&#20302;&#20301;&#25968;&#65288;&#22914;4&#20301;&#65289;&#65288;&#20854;&#26377;&#25928;&#20301;&#25968;&#19982;5&#20301;&#30456;&#20284;&#65289;&#26356;&#24378;&#22823;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#22914;&#20309;&#20026;&#19981;&#21516;&#22823;&#23567;&#30340;\llms&#21033;&#29992;&#37327;&#21270;&#30340;&#24314;&#35758;&#65292;&#24182;&#30041;&#19979;&#26410;&#26469;&#26426;&#20250;&#21644;&#31995;&#32479;&#24037;&#20316;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
Post-training quantization (\ptq) had been recently shown as a compromising method to reduce the memory consumption and/or compute cost for large language models. However, a comprehensive study about the effect of different quantization schemes, different model families, different \ptq methods, different quantization bit precision, etc, is still missing. In this work, we provide an extensive study on those components over tens of thousands of zero-shot experiments. Our results show that (1) Fine-grained quantization and \ptq methods (instead of naive round-to-nearest quantization) are necessary to achieve good accuracy and (2) Higher bits (e.g., 5 bits) with coarse-grained quantization is more powerful than lower bits (e.g., 4 bits) with very fine-grained quantization (whose effective bits is similar to 5-bits). We also present recommendations about how to utilize quantization for \llms with different sizes, and leave suggestions of future opportunities and system work that are not res
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GOATS&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#30446;&#26631;&#37319;&#26679;&#33258;&#36866;&#24212;&#35838;&#31243;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#65292;&#36890;&#36807;&#25554;&#20540;&#20301;&#32622;&#30446;&#26631;&#21644;&#25968;&#37327;&#30446;&#26631;&#30340;&#20998;&#24067;&#21019;&#24314;&#23398;&#20064;&#36807;&#31243;&#20013;&#30340;&#35838;&#31243;&#26469;&#35299;&#20915;&#26426;&#22120;&#20154;&#33280;&#21462;&#20219;&#21153;&#20013;&#30340;&#20301;&#32622;&#30446;&#26631;&#21644;&#27700;&#37327;&#30446;&#26631;&#38382;&#39064;&#65292;&#21462;&#24471;&#20102;&#27604;&#22522;&#32447;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2303.05193</link><description>&lt;p&gt;
GOATS&#65306;&#30446;&#26631;&#37319;&#26679;&#33258;&#36866;&#24212;&#35838;&#31243;&#24378;&#21270;&#23398;&#20064;&#29992;&#20110;&#33280;&#21462;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
GOATS: Goal Sampling Adaptation for Scooping with Curriculum Reinforcement Learning. (arXiv:2303.05193v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.05193
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GOATS&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#30446;&#26631;&#37319;&#26679;&#33258;&#36866;&#24212;&#35838;&#31243;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#65292;&#36890;&#36807;&#25554;&#20540;&#20301;&#32622;&#30446;&#26631;&#21644;&#25968;&#37327;&#30446;&#26631;&#30340;&#20998;&#24067;&#21019;&#24314;&#23398;&#20064;&#36807;&#31243;&#20013;&#30340;&#35838;&#31243;&#26469;&#35299;&#20915;&#26426;&#22120;&#20154;&#33280;&#21462;&#20219;&#21153;&#20013;&#30340;&#20301;&#32622;&#30446;&#26631;&#21644;&#27700;&#37327;&#30446;&#26631;&#38382;&#39064;&#65292;&#21462;&#24471;&#20102;&#27604;&#22522;&#32447;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#20808;&#20351;&#29992;&#30446;&#26631;&#26465;&#20214;&#24378;&#21270;&#23398;&#20064;&#23545;&#26426;&#22120;&#20154;&#33280;&#21462;&#27700;&#30340;&#38382;&#39064;&#36827;&#34892;&#20102;&#38416;&#36848;&#12290;&#30001;&#20110;&#27969;&#20307;&#30340;&#22797;&#26434;&#21160;&#21147;&#23398;&#21644;&#23454;&#29616;&#22810;&#27169;&#24335;&#30446;&#26631;&#30340;&#38656;&#27714;&#65292;&#35813;&#20219;&#21153;&#20855;&#26377;&#29305;&#21035;&#30340;&#25361;&#25112;&#24615;&#12290;&#25919;&#31574;&#38656;&#35201;&#25104;&#21151;&#22320;&#36798;&#21040;&#20301;&#32622;&#30446;&#26631;&#21644;&#27700;&#37327;&#30446;&#26631;&#65292;&#36825;&#23548;&#33268;&#19968;&#20010;&#24222;&#22823;&#32780;&#22797;&#26434;&#30340;&#30446;&#26631;&#29366;&#24577;&#31354;&#38388;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;GOATS&#65292;&#19968;&#31181;&#35838;&#31243;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#25554;&#20540;&#20301;&#32622;&#30446;&#26631;&#20998;&#24067;&#21644;&#25968;&#37327;&#30446;&#26631;&#20998;&#24067;&#26469;&#21019;&#24314;&#23398;&#20064;&#36807;&#31243;&#20013;&#30340;&#35838;&#31243;&#65292;&#20351;&#29992;&#30446;&#26631;&#20998;&#35299;&#22870;&#21169;&#20844;&#24335;&#65292;&#23398;&#20064;&#19968;&#20010;&#39640;&#25928;&#19988;&#20855;&#26377;&#36890;&#29992;&#24615;&#30340;&#26426;&#22120;&#20154;&#33280;&#21462;&#31574;&#30053;&#12290;&#32467;&#26524;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#20223;&#30495;&#20013;&#34920;&#29616;&#20986;&#27604;&#22522;&#32447;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#20998;&#21035;&#22312;&#30871;&#33280;&#21644;&#26742;&#33280;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;5.46&#65285;&#21644;8.71&#65285;&#30340;&#35823;&#24046;&#65292;&#28085;&#30422;&#20102;1000&#31181;&#21021;&#22987;&#27700;&#29366;&#24577;&#30340;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we first formulate the problem of robotic water scooping using goal-conditioned reinforcement learning. This task is particularly challenging due to the complex dynamics of fluid and the need to achieve multi-modal goals. The policy is required to successfully reach both position goals and water amount goals, which leads to a large convoluted goal state space. To overcome these challenges, we introduce Goal Sampling Adaptation for Scooping (GOATS), a curriculum reinforcement learning method that can learn an effective and generalizable policy for robot scooping tasks. Specifically, we use a goal-factorized reward formulation and interpolate position goal distributions and amount goal distributions to create curriculum throughout the learning process. As a result, our proposed method can outperform the baselines in simulation and achieves 5.46% and 8.71% amount errors on bowl scooping and bucket scooping tasks, respectively, under 1000 variations of initial water states in
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#24187;&#24819;&#23545;&#25239;&#25511;&#21046;&#30340;HAMBO&#31639;&#27861;&#65292;&#21487;&#29992;&#20110;&#31163;&#32447;&#31574;&#30053;&#35780;&#20272;&#65292;&#24182;&#19988;&#33021;&#22815;&#24471;&#20986;&#26377;&#25928;&#30340;&#31574;&#30053;&#34920;&#29616;&#19979;&#38480;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2303.01076</link><description>&lt;p&gt;
&#22522;&#20110;&#24187;&#24819;&#23545;&#25239;&#25511;&#21046;&#30340;&#20445;&#23432;&#31163;&#32447;&#31574;&#30053;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Hallucinated Adversarial Control for Conservative Offline Policy Evaluation. (arXiv:2303.01076v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.01076
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#24187;&#24819;&#23545;&#25239;&#25511;&#21046;&#30340;HAMBO&#31639;&#27861;&#65292;&#21487;&#29992;&#20110;&#31163;&#32447;&#31574;&#30053;&#35780;&#20272;&#65292;&#24182;&#19988;&#33021;&#22815;&#24471;&#20986;&#26377;&#25928;&#30340;&#31574;&#30053;&#34920;&#29616;&#19979;&#38480;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20445;&#23432;&#31163;&#32447;&#31574;&#30053;&#35780;&#20272;&#38382;&#39064;&#65292;&#23545;&#20110;&#32473;&#23450;&#20854;&#20182;&#20195;&#29702;&#25910;&#38598;&#30340;&#31163;&#32447;&#29615;&#22659;&#20132;&#20114;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#26088;&#22312;&#33719;&#24471;&#19968;&#20010;&#20851;&#20110;&#31574;&#30053;&#24615;&#33021;&#30340;(&#32039;)&#19979;&#38480;&#20272;&#35745;&#12290;&#36825;&#22312;&#20915;&#23450;&#26159;&#21542;&#37096;&#32626;&#26576;&#20010;&#31574;&#30053;&#28385;&#36275;&#26368;&#23567;&#24615;&#33021;/&#23433;&#20840;&#26631;&#20934;&#20043;&#21069;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;HAMBO&#65292;&#23427;&#24314;&#31435;&#22312;&#19968;&#20010;&#23398;&#20064;&#21040;&#30340;&#20256;&#36882;&#21160;&#24577;&#30340;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#27169;&#22411;&#20043;&#19978;&#12290;&#20026;&#20102;&#24418;&#25104;&#31574;&#30053;&#32489;&#25928;&#30340;&#20445;&#23432;&#20272;&#35745;&#65292;HAMBO&#20250;&#24187;&#24819;&#31574;&#30053;&#21487;&#33021;&#37319;&#21462;&#30340;&#26368;&#22351;&#36712;&#36857;&#65292;&#19988;&#35813;&#36712;&#36857;&#22312;&#27169;&#22411;&#30340;&#35748;&#30693;&#32622;&#20449;&#21306;&#38388;&#20869;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#32467;&#26524;&#30340;COPE&#20272;&#35745;&#26159;&#26377;&#25928;&#30340;&#19979;&#38480;&#65292;&#24182;&#22312;&#27491;&#21017;&#24615;&#26465;&#20214;&#19979;&#23637;&#31034;&#20854;&#25910;&#25947;&#20110;&#30495;&#23454;&#30340;&#39044;&#26399;&#22238;&#25253;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#22522;&#20110;Bayesian&#31070;&#32463;&#32593;&#32476;&#30340;&#21487;&#25193;&#23637;&#21464;&#20307;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#35777;&#26126;&#23427;&#20204;&#20135;&#29983;&#21487;&#38752;&#19988;&#32039;&#23494;&#30340;&#19979;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the problem of conservative off-policy evaluation (COPE) where given an offline dataset of environment interactions, collected by other agents, we seek to obtain a (tight) lower bound on a policy's performance. This is crucial when deciding whether a given policy satisfies certain minimal performance/safety criteria before it can be deployed in the real world. To this end, we introduce HAMBO, which builds on an uncertainty-aware learned model of the transition dynamics. To form a conservative estimate of the policy's performance, HAMBO hallucinates worst-case trajectories that the policy may take, within the margin of the models' epistemic confidence regions. We prove that the resulting COPE estimates are valid lower bounds, and, under regularity conditions, show their convergence to the true expected return. Finally, we discuss scalable variants of our approach based on Bayesian Neural Networks and empirically demonstrate that they yield reliable and tight lower bounds in var
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#26381;&#21153;&#22120;&#31471;&#36827;&#34892;&#20998;&#21106;&#23398;&#20064;&#21518;&#38376;&#25915;&#20987;&#30340;&#21487;&#34892;&#24615;&#65292;&#25552;&#20379;&#20102;&#20004;&#31181;&#25915;&#20987;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20854;&#21363;&#20351;&#22312;&#22810;&#20010;&#23458;&#25143;&#31471;&#21442;&#19982;&#23398;&#20064;&#19988;&#20855;&#26377;&#19981;&#21516;&#30340;&#25968;&#25454;&#20998;&#24067;&#26102;&#20063;&#26159;&#26377;&#25928;&#30340;&#12290;</title><link>http://arxiv.org/abs/2302.09578</link><description>&lt;p&gt;
&#20851;&#20110;&#22312;&#26381;&#21153;&#22120;&#31471;&#36827;&#34892;&#20998;&#21106;&#23398;&#20064;&#21518;&#38376;&#25915;&#20987;&#30340;&#21487;&#34892;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On Feasibility of Server-side Backdoor Attacks on Split Learning. (arXiv:2302.09578v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.09578
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#26381;&#21153;&#22120;&#31471;&#36827;&#34892;&#20998;&#21106;&#23398;&#20064;&#21518;&#38376;&#25915;&#20987;&#30340;&#21487;&#34892;&#24615;&#65292;&#25552;&#20379;&#20102;&#20004;&#31181;&#25915;&#20987;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20854;&#21363;&#20351;&#22312;&#22810;&#20010;&#23458;&#25143;&#31471;&#21442;&#19982;&#23398;&#20064;&#19988;&#20855;&#26377;&#19981;&#21516;&#30340;&#25968;&#25454;&#20998;&#24067;&#26102;&#20063;&#26159;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#21106;&#23398;&#20064;&#26159;&#19968;&#31181;&#21327;&#20316;&#23398;&#20064;&#35774;&#35745;&#65292;&#20801;&#35768;&#22810;&#20010;&#21442;&#19982;&#32773;&#65288;&#23458;&#25143;&#31471;&#65289;&#35757;&#32451;&#20849;&#20139;&#27169;&#22411;&#21516;&#26102;&#20445;&#25345;&#20854;&#25968;&#25454;&#38598;&#31169;&#26377;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#21327;&#20316;&#23398;&#20064;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;&#32852;&#37030;&#23398;&#20064;&#65292;&#23481;&#26131;&#21463;&#21040;&#23433;&#20840;&#21644;&#38544;&#31169;&#25915;&#20987;&#65292;&#20363;&#22914;&#27169;&#22411;&#25512;&#26029;&#21644;&#21518;&#38376;&#25915;&#20987;&#12290;&#21518;&#38376;&#25915;&#20987;&#26159;&#19968;&#31181;&#27602;&#21270;&#25915;&#20987;&#65292;&#25915;&#20987;&#32773;&#35797;&#22270;&#36890;&#36807;&#25805;&#32437;&#27169;&#22411;&#30340;&#35757;&#32451;&#36807;&#31243;&#26469;&#25511;&#21046;&#27169;&#22411;&#36755;&#20986;&#12290;&#34429;&#28982;&#24050;&#32463;&#26377;&#20102;&#20851;&#20110;&#20998;&#21106;&#23398;&#20064;&#25512;&#26029;&#25915;&#20987;&#30340;&#30740;&#31350;&#65292;&#20294;&#36824;&#27809;&#26377;&#36827;&#34892;&#36807;&#21518;&#38376;&#25915;&#20987;&#30340;&#27979;&#35797;&#12290;&#26412;&#25991;&#23545;&#20998;&#21106;&#23398;&#20064;&#36827;&#34892;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21518;&#38376;&#25915;&#20987;&#65292;&#24182;&#30740;&#31350;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;&#23613;&#31649;&#20256;&#32479;&#30340;&#21518;&#38376;&#25915;&#20987;&#26159;&#22312;&#23458;&#25143;&#31471;&#19978;&#23436;&#25104;&#30340;&#65292;&#20294;&#25105;&#20204;&#20174;&#26381;&#21153;&#22120;&#31471;&#27880;&#20837;&#21518;&#38376;&#35302;&#21457;&#22120;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20004;&#31181;&#25915;&#20987;&#26041;&#27861;&#65306;&#19968;&#31181;&#26159;&#20351;&#29992;&#20195;&#29702;&#23458;&#25143;&#31471;&#65292;&#21478;&#19968;&#31181;&#26159;&#20351;&#29992;&#33258;&#32534;&#30721;&#22120;&#36890;&#36807;&#20256;&#20837;&#30340;&#30772;&#30862;&#25968;&#25454;&#21450;&#20854;&#20256;&#20986;&#30340;&#26799;&#24230;&#27602;&#21270;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#26377;&#22810;&#20010;&#23458;&#25143;&#31471;&#21442;&#19982;&#23398;&#20064;&#19988;&#20855;&#26377;&#19981;&#21516;&#30340;&#25968;&#25454;&#20998;&#24067;&#65292;&#36825;&#20004;&#31181;&#26041;&#27861;&#37117;&#38750;&#24120;&#26377;&#25928;&#12290;&#36825;&#39033;&#24037;&#20316;&#24378;&#35843;&#20102;&#36827;&#19968;&#27493;&#30740;&#31350;&#22312;&#26381;&#21153;&#22120;&#31471;&#36827;&#34892;&#20998;&#21106;&#23398;&#20064;&#21518;&#38376;&#25915;&#20987;&#30340;&#24517;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Split learning is a collaborative learning design that allows several participants (clients) to train a shared model while keeping their datasets private. Recent studies demonstrate that collaborative learning models, specifically federated learning, are vulnerable to security and privacy attacks such as model inference and backdoor attacks. Backdoor attacks are a group of poisoning attacks in which the attacker tries to control the model output by manipulating the model's training process. While there have been studies regarding inference attacks on split learning, it has not yet been tested for backdoor attacks. This paper performs a novel backdoor attack on split learning and studies its effectiveness. Despite traditional backdoor attacks done on the client side, we inject the backdoor trigger from the server side. For this purpose, we provide two attack methods: one using a surrogate client and another using an autoencoder to poison the model via incoming smashed data and its outgo
&lt;/p&gt;</description></item><item><title>&#23558;PAC-Bayesian&#29702;&#35770;&#25193;&#23637;&#21040;&#29983;&#25104;&#27169;&#22411;&#65292;&#20026;&#22522;&#20110;Wasserstein&#36317;&#31163;&#21644;&#24635;&#21464;&#24046;&#36317;&#31163;&#30340;&#27169;&#22411;&#25552;&#20379;&#20102;&#27867;&#21270;&#30028;&#65292;&#20026;Wasserstein GAN&#21644;Energy-Based GAN&#25552;&#20379;&#20102;&#26032;&#30340;&#35757;&#32451;&#30446;&#26631;&#65292;&#24182;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20986;&#38750;&#34394;&#31354;&#27867;&#21270;&#30028;&#12290;</title><link>http://arxiv.org/abs/2302.08942</link><description>&lt;p&gt;
&#38754;&#21521;&#23545;&#25239;&#29983;&#25104;&#27169;&#22411;&#30340;PAC-Bayesian&#27867;&#21270;&#30028;
&lt;/p&gt;
&lt;p&gt;
PAC-Bayesian Generalization Bounds for Adversarial Generative Models. (arXiv:2302.08942v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.08942
&lt;/p&gt;
&lt;p&gt;
&#23558;PAC-Bayesian&#29702;&#35770;&#25193;&#23637;&#21040;&#29983;&#25104;&#27169;&#22411;&#65292;&#20026;&#22522;&#20110;Wasserstein&#36317;&#31163;&#21644;&#24635;&#21464;&#24046;&#36317;&#31163;&#30340;&#27169;&#22411;&#25552;&#20379;&#20102;&#27867;&#21270;&#30028;&#65292;&#20026;Wasserstein GAN&#21644;Energy-Based GAN&#25552;&#20379;&#20102;&#26032;&#30340;&#35757;&#32451;&#30446;&#26631;&#65292;&#24182;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20986;&#38750;&#34394;&#31354;&#27867;&#21270;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23558;PAC-Bayesian&#29702;&#35770;&#25193;&#23637;&#21040;&#29983;&#25104;&#27169;&#22411;&#65292;&#24182;&#20026;&#22522;&#20110;Wasserstein&#36317;&#31163;&#21644;&#24635;&#21464;&#24046;&#36317;&#31163;&#30340;&#27169;&#22411;&#24320;&#21457;&#20102;&#27867;&#21270;&#30028;&#12290;&#25105;&#20204;&#31532;&#19968;&#20010;&#20851;&#20110;Wasserstein&#36317;&#31163;&#30340;&#32467;&#26524;&#20551;&#35774;&#23454;&#20363;&#31354;&#38388;&#26159;&#26377;&#30028;&#30340;&#65292;&#32780;&#25105;&#20204;&#30340;&#31532;&#20108;&#20010;&#32467;&#26524;&#21033;&#29992;&#20102;&#38477;&#32500;&#30340;&#20248;&#21183;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#33258;&#28982;&#36866;&#29992;&#20110;Wasserstein GAN&#21644;Energy-Based GAN&#65292;&#32780;&#25105;&#20204;&#30340;&#30028;&#38480;&#20026;&#36825;&#20004;&#31181;GAN&#25552;&#20379;&#20102;&#26032;&#30340;&#35757;&#32451;&#30446;&#26631;&#12290;&#23613;&#31649;&#25105;&#20204;&#30340;&#24037;&#20316;&#20027;&#35201;&#26159;&#29702;&#35770;&#24615;&#30340;&#65292;&#20294;&#25105;&#20204;&#36827;&#34892;&#20102;&#25968;&#20540;&#23454;&#39564;&#65292;&#23637;&#31034;&#20102;Wasserstein GAN&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#30340;&#38750;&#34394;&#31354;&#27867;&#21270;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
We extend PAC-Bayesian theory to generative models and develop generalization bounds for models based on the Wasserstein distance and the total variation distance. Our first result on the Wasserstein distance assumes the instance space is bounded, while our second result takes advantage of dimensionality reduction. Our results naturally apply to Wasserstein GANs and Energy-Based GANs, and our bounds provide new training objectives for these two. Although our work is mainly theoretical, we perform numerical experiments showing non-vacuous generalization bounds for Wasserstein GANs on synthetic datasets.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SyreaNet&#30340;&#27700;&#19979;&#22270;&#20687;&#22686;&#24378;&#26694;&#26550;&#65292;&#36890;&#36807;&#38598;&#25104;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#20197;&#21450;&#29289;&#29702;&#24341;&#23548;&#21644;&#26032;&#39062;&#30340;&#22495;&#33258;&#36866;&#24212;&#31574;&#30053;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#22788;&#29702;&#21508;&#31181;&#27700;&#19979;&#26465;&#20214;&#65292;&#36798;&#21040;&#20102;&#36739;&#22909;&#30340;&#25552;&#21319;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2302.08269</link><description>&lt;p&gt;
SyreaNet:&#19968;&#31181;&#29289;&#29702;&#24341;&#23548;&#19979;&#30340;&#27700;&#19979;&#22270;&#20687;&#22686;&#24378;&#26694;&#26550;&#65292;&#38598;&#25104;&#21512;&#25104;&#21644;&#30495;&#23454;&#22270;&#20687;
&lt;/p&gt;
&lt;p&gt;
SyreaNet: A Physically Guided Underwater Image Enhancement Framework Integrating Synthetic and Real Images. (arXiv:2302.08269v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.08269
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SyreaNet&#30340;&#27700;&#19979;&#22270;&#20687;&#22686;&#24378;&#26694;&#26550;&#65292;&#36890;&#36807;&#38598;&#25104;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#20197;&#21450;&#29289;&#29702;&#24341;&#23548;&#21644;&#26032;&#39062;&#30340;&#22495;&#33258;&#36866;&#24212;&#31574;&#30053;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#22788;&#29702;&#21508;&#31181;&#27700;&#19979;&#26465;&#20214;&#65292;&#36798;&#21040;&#20102;&#36739;&#22909;&#30340;&#25552;&#21319;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27700;&#19979;&#22270;&#20687;&#22686;&#24378;&#65288;UIE&#65289;&#23545;&#20110;&#27700;&#19979;&#39640;&#32423;&#35270;&#35273;&#30456;&#20851;&#20219;&#21153;&#33267;&#20851;&#37325;&#35201;&#12290;&#34429;&#28982;&#22522;&#20110;&#23398;&#20064;&#30340;UIE&#26041;&#27861;&#22312;&#36817;&#24180;&#26469;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#26524;&#65292;&#20294;&#23427;&#20204;&#20173;&#28982;&#38590;&#20197;&#24212;&#23545;&#21508;&#31181;&#19981;&#21516;&#30340;&#27700;&#19979;&#26465;&#20214;&#65292;&#36825;&#21487;&#33021;&#26159;&#30001;&#20197;&#19979;&#21407;&#22240;&#24341;&#36215;&#30340;: 1) UIE&#20013;&#20351;&#29992;&#30340;&#31616;&#21270;&#22823;&#27668;&#22270;&#20687;&#24418;&#25104;&#27169;&#22411;&#21487;&#33021;&#20250;&#23548;&#33268;&#20005;&#37325;&#30340;&#35823;&#24046;&#12290;2) &#21482;&#20351;&#29992;&#21512;&#25104;&#22270;&#20687;&#35757;&#32451;&#30340;&#32593;&#32476;&#21487;&#33021;&#38590;&#20197;&#24456;&#22909;&#22320;&#25512;&#24191;&#21040;&#30495;&#23454;&#30340;&#27700;&#19979;&#22270;&#20687;&#12290;&#26412;&#25991;&#39318;&#27425;&#25552;&#20986;&#20102;&#19968;&#31181;&#26694;&#26550;SyreaNet&#65292;&#20197;&#20462;&#35746;&#21518;&#30340;&#27700;&#19979;&#22270;&#20687;&#24418;&#25104;&#27169;&#22411;&#21644;&#26032;&#39062;&#30340;&#22495;&#33258;&#36866;&#24212;&#65288;DA&#65289;&#31574;&#30053;&#20026;&#25351;&#23548;&#65292;&#38598;&#25104;&#20102;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#30340;UIE&#26041;&#27861;&#12290;&#39318;&#20808;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#20462;&#35746;&#27169;&#22411;&#30340;&#27700;&#19979;&#22270;&#20687;&#21512;&#25104;&#27169;&#22359;&#12290;&#28982;&#21518;&#35774;&#35745;&#20102;&#19968;&#31181;&#29289;&#29702;&#24341;&#23548;&#30340;&#20998;&#35299;&#32593;&#32476;&#65292;&#36890;&#36807;&#32467;&#21512;&#21512;&#25104;&#27700;&#19979;&#22270;&#20687;&#21644;&#30495;&#23454;&#27700;&#19979;&#22270;&#20687;&#26469;&#39044;&#27979;&#28165;&#26224;&#30340;&#22270;&#20687;&#12290;&#36827;&#19968;&#27493;&#37319;&#29992;&#20869;&#37096;&#21644;&#36328;&#22495;&#36866;&#24212;&#25216;&#26415;&#26469;&#25552;&#39640;&#32593;&#32476;&#30340;&#25512;&#24191;&#33021;&#21147;&#12290;&#22312;&#21508;&#31181;&#27700;&#19979;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Underwater image enhancement (UIE) is vital for high-level vision-related underwater tasks. Although learning-based UIE methods have made remarkable achievements in recent years, it's still challenging for them to consistently deal with various underwater conditions, which could be caused by: 1) the use of the simplified atmospheric image formation model in UIE may result in severe errors; 2) the network trained solely with synthetic images might have difficulty in generalizing well to real underwater images. In this work, we, for the first time, propose a framework \textit{SyreaNet} for UIE that integrates both synthetic and real data under the guidance of the revised underwater image formation model and novel domain adaptation (DA) strategies. First, an underwater image synthesis module based on the revised model is proposed. Then, a physically guided disentangled network is designed to predict the clear images by combining both synthetic and real underwater images. The intra- and in
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#39044;&#35757;&#32451;&#25991;&#26412;&#33267;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#21442;&#25968;&#21270;&#30340;&#22270;&#20687;&#21040;&#22270;&#20687;&#36716;&#25442;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#25968;&#25454;&#22686;&#24378;&#30340;&#22810;&#26679;&#24615;&#19981;&#36275;&#38382;&#39064;&#65292;&#24182;&#33021;&#22815;&#27867;&#21270;&#21040;&#26032;&#35270;&#35273;&#27010;&#24565;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#23569;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;&#21644;&#22270;&#20687;&#35782;&#21035;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2302.07944</link><description>&lt;p&gt;
&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#26377;&#25928;&#30340;&#25968;&#25454;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Effective Data Augmentation With Diffusion Models. (arXiv:2302.07944v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.07944
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#39044;&#35757;&#32451;&#25991;&#26412;&#33267;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#21442;&#25968;&#21270;&#30340;&#22270;&#20687;&#21040;&#22270;&#20687;&#36716;&#25442;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#25968;&#25454;&#22686;&#24378;&#30340;&#22810;&#26679;&#24615;&#19981;&#36275;&#38382;&#39064;&#65292;&#24182;&#33021;&#22815;&#27867;&#21270;&#21040;&#26032;&#35270;&#35273;&#27010;&#24565;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#23569;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;&#21644;&#22270;&#20687;&#35782;&#21035;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#22686;&#24378;&#26159;&#28145;&#24230;&#23398;&#20064;&#20013;&#26368;&#24120;&#35265;&#30340;&#24037;&#20855;&#20043;&#19968;&#65292;&#25903;&#25745;&#30528;&#26368;&#36817;&#21253;&#25324;&#20998;&#31867;&#12289;&#29983;&#25104;&#27169;&#22411;&#21644;&#34920;&#31034;&#23398;&#20064;&#22312;&#20869;&#30340;&#35768;&#22810;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#22686;&#24378;&#26041;&#27861;&#22312;&#25968;&#25454;&#30340;&#20851;&#38190;&#35821;&#20041;&#36724;&#19978;&#32570;&#20047;&#22810;&#26679;&#24615;&#65292;&#32570;&#20047;&#25913;&#21464;&#39640;&#32423;&#35821;&#20041;&#23646;&#24615;&#65288;&#22914;&#22330;&#26223;&#20013;&#30340;&#21160;&#29289;&#31181;&#31867;&#65289;&#20197;&#22686;&#24378;&#25968;&#25454;&#22810;&#26679;&#24615;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#39044;&#35757;&#32451;&#25991;&#26412;&#33267;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#21442;&#25968;&#21270;&#30340;&#22270;&#20687;&#21040;&#22270;&#20687;&#36716;&#25442;&#26469;&#35299;&#20915;&#25968;&#25454;&#22686;&#24378;&#22810;&#26679;&#24615;&#19981;&#36275;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#29616;&#25104;&#30340;&#25193;&#25955;&#27169;&#22411;&#32534;&#36753;&#22270;&#20687;&#65292;&#25913;&#21464;&#23427;&#20204;&#30340;&#35821;&#20041;&#65292;&#33021;&#22815;&#27867;&#21270;&#21040;&#20165;&#29992;&#23569;&#37327;&#26631;&#35760;&#31034;&#20363;&#24471;&#21040;&#30340;&#26032;&#35270;&#35273;&#27010;&#24565;&#12290;&#25105;&#20204;&#22312;&#23569;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#21644;&#30495;&#23454;&#19990;&#30028;&#30340;&#26434;&#33609;&#35782;&#21035;&#20219;&#21153;&#20013;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#35266;&#23519;&#21040;......
&lt;/p&gt;
&lt;p&gt;
Data augmentation is one of the most prevalent tools in deep learning, underpinning many recent advances, including those from classification, generative models, and representation learning. The standard approach to data augmentation combines simple transformations like rotations and flips to generate new images from existing ones. However, these new images lack diversity along key semantic axes present in the data. Current augmentations cannot alter the high-level semantic attributes, such as animal species present in a scene, to enhance the diversity of data. We address the lack of diversity in data augmentation with image-to-image transformations parameterized by pre-trained text-to-image diffusion models. Our method edits images to change their semantics using an off-the-shelf diffusion model, and generalizes to novel visual concepts from a few labelled examples. We evaluate our approach on few-shot image classification tasks, and on a real-world weed recognition task, and observe 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22240;&#26524;&#26694;&#26550;&#65292;&#35299;&#37322;&#20102;&#22312;&#20154;&#24037;&#26234;&#33021;&#20915;&#31574;&#20013;&#20351;&#29992;&#30340;&#20195;&#29702;&#26631;&#31614;&#21487;&#33021;&#23384;&#22312;&#30340;&#30446;&#26631;&#21464;&#37327;&#20559;&#24046;&#65292;&#24182;&#25552;&#20986;&#26410;&#26469;&#30740;&#31350;&#24212;&#24378;&#35843;&#38024;&#23545;&#36739;&#24191;&#27867;&#38382;&#39064;&#30340;&#22240;&#26524;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2302.06503</link><description>&lt;p&gt;
&#27809;&#26377;&#22320;&#22522;&#30340;&#30495;&#30456;&#65306;&#20154;&#24037;&#26234;&#33021;&#20915;&#31574;&#20013;&#20195;&#29702;&#26631;&#31614;&#30340;&#22240;&#26524;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Ground(less) Truth: A Causal Framework for Proxy Labels in Human-Algorithm Decision-Making. (arXiv:2302.06503v4 [cs.CY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.06503
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22240;&#26524;&#26694;&#26550;&#65292;&#35299;&#37322;&#20102;&#22312;&#20154;&#24037;&#26234;&#33021;&#20915;&#31574;&#20013;&#20351;&#29992;&#30340;&#20195;&#29702;&#26631;&#31614;&#21487;&#33021;&#23384;&#22312;&#30340;&#30446;&#26631;&#21464;&#37327;&#20559;&#24046;&#65292;&#24182;&#25552;&#20986;&#26410;&#26469;&#30740;&#31350;&#24212;&#24378;&#35843;&#38024;&#23545;&#36739;&#24191;&#27867;&#38382;&#39064;&#30340;&#22240;&#26524;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36234;&#26469;&#36234;&#22810;&#20851;&#20110;&#20154;&#24037;&#26234;&#33021;&#20915;&#31574;&#30340;&#25991;&#29486;&#35843;&#26597;&#23558;&#20154;&#31867;&#21028;&#26029;&#19982;&#32479;&#35745;&#27169;&#22411;&#30456;&#32467;&#21512;&#20197;&#25913;&#21892;&#20915;&#31574;&#36136;&#37327;&#12290;&#35813;&#39046;&#22495;&#30340;&#30740;&#31350;&#36890;&#24120;&#36890;&#36807;&#23637;&#31034;&#23545;&#8220;&#22320;&#22522;&#8221;&#26631;&#31614;&#30340;&#39044;&#27979;&#24615;&#33021;&#26469;&#35780;&#20272;&#25152;&#25552;&#20986;&#30340;&#25913;&#36827;&#27169;&#22411;&#12289;&#25509;&#21475;&#25110;&#24037;&#20316;&#27969;&#31243;&#30340;&#26377;&#25928;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#20570;&#27861;&#24573;&#30053;&#20102;&#20154;&#31867;&#21028;&#26029;&#19982;&#27169;&#22411;&#39044;&#27979;&#20043;&#38388;&#30340;&#19968;&#20010;&#20851;&#38190;&#24046;&#24322;&#12290;&#32780;&#39044;&#27979;&#27169;&#22411;&#21017;&#20165;&#38024;&#23545;&#21487;&#22312;&#29616;&#26377;&#25968;&#25454;&#38598;&#20013;&#36731;&#26494;&#33719;&#21462;&#30340;&#20195;&#29702;&#26631;&#31614;&#12290;&#39044;&#27979;&#27169;&#22411;&#20381;&#36182;&#20110;&#31616;&#21333;&#20195;&#29702;&#26631;&#31614;&#65292;&#22240;&#27492;&#23481;&#26131;&#21463;&#21040;&#21508;&#31181;&#32479;&#35745;&#20559;&#24046;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#35782;&#21035;&#20102;&#20154;&#24037;&#26234;&#33021;&#20915;&#31574;&#20013;&#21487;&#33021;&#24433;&#21709;&#20195;&#29702;&#26631;&#31614;&#26377;&#25928;&#24615;&#30340;&#20116;&#31181;&#30446;&#26631;&#21464;&#37327;&#20559;&#24046;&#26469;&#28304;&#12290;&#25105;&#20204;&#25552;&#20986;&#19968;&#20010;&#22240;&#26524;&#26694;&#26550;&#26469;&#29702;&#35299;&#36825;&#20123;&#20559;&#24046;&#26469;&#28304;&#21450;&#20854;&#23545;&#20915;&#31574;&#36136;&#37327;&#24102;&#26469;&#30340;&#39118;&#38505;&#12290;&#25105;&#20204;&#35748;&#20026;&#20165;&#26681;&#25454;&#22320;&#22522;&#26631;&#31614;&#26469;&#35780;&#20272;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#21487;&#33021;&#20250;&#20135;&#29983;&#35823;&#23548;&#24615;&#65292;&#21516;&#26102;&#24314;&#35758;&#26410;&#26469;&#30340;&#24037;&#20316;&#24378;&#35843;&#38024;&#23545;&#36739;&#24191;&#27867;&#30340;&#38382;&#39064;&#24863;&#20852;&#36259;&#30340;&#22240;&#26524;&#35780;&#20272;&#20505;&#36873;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
A growing literature on human-AI decision-making investigates strategies for combining human judgment with statistical models to improve decision-making. Research in this area often evaluates proposed improvements to models, interfaces, or workflows by demonstrating improved predictive performance on "ground truth" labels. However, this practice overlooks a key difference between human judgments and model predictions. Whereas humans reason about broader phenomena of interest in a decision -- including latent constructs that are not directly observable, such as disease status, the "toxicity" of online comments, or future "job performance" -- predictive models target proxy labels that are readily available in existing datasets. Predictive models' reliance on simplistic proxies makes them vulnerable to various sources of statistical bias. In this paper, we identify five sources of target variable bias that can impact the validity of proxy labels in human-AI decision-making tasks. We devel
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#40654;&#26364;&#27969;&#21305;&#37197;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19968;&#33324;&#20960;&#20309;&#19978;&#35757;&#32451;&#36830;&#32493;&#26631;&#20934;&#21270;&#27969;&#65292;&#24182;&#22312;&#39640;&#32500;&#24230;&#25968;&#25454;&#19978;&#20855;&#26377;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2302.03660</link><description>&lt;p&gt;
&#19968;&#33324;&#20960;&#20309;&#19978;&#30340;&#40654;&#26364;&#27969;&#21305;&#37197;
&lt;/p&gt;
&lt;p&gt;
Riemannian Flow Matching on General Geometries. (arXiv:2302.03660v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.03660
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#40654;&#26364;&#27969;&#21305;&#37197;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19968;&#33324;&#20960;&#20309;&#19978;&#35757;&#32451;&#36830;&#32493;&#26631;&#20934;&#21270;&#27969;&#65292;&#24182;&#22312;&#39640;&#32500;&#24230;&#25968;&#25454;&#19978;&#20855;&#26377;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#40654;&#26364;&#27969;&#21305;&#37197;&#65288;RFM&#65289;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#27969;&#24418;&#19978;&#35757;&#32451;&#36830;&#32493;&#26631;&#20934;&#21270;&#27969;&#12290;&#29616;&#26377;&#30340;&#27969;&#24418;&#29983;&#25104;&#24314;&#27169;&#26041;&#27861;&#35201;&#20040;&#38656;&#35201;&#26114;&#36149;&#30340;&#27169;&#25311;&#65292;&#35201;&#20040;&#26080;&#27861;&#26412;&#36136;&#19978;&#25193;&#23637;&#21040;&#39640;&#32500;&#24230;&#65292;&#35201;&#20040;&#20351;&#29992;&#38480;&#21046;&#37327;&#30340;&#36817;&#20284;&#26469;&#20135;&#29983;&#26377;&#20559;&#30340;&#35757;&#32451;&#30446;&#26631;&#12290;&#40654;&#26364;&#27969;&#21305;&#37197;&#32469;&#36807;&#20102;&#36825;&#20123;&#38480;&#21046;&#65292;&#24182;&#25552;&#20379;&#20102;&#27604;&#20197;&#21069;&#26041;&#27861;&#26356;&#22810;&#30340;&#20248;&#21183;&#65306;&#23427;&#22312;&#31616;&#21333;&#20960;&#20309;&#19978;&#26080;&#38656;&#27169;&#25311;&#65292;&#19981;&#38656;&#35201;&#25955;&#24230;&#35745;&#31639;&#65292;&#24182;&#20197;&#38381;&#21512;&#24418;&#24335;&#35745;&#31639;&#20854;&#30446;&#26631;&#21521;&#37327;&#22330;&#12290; RFM&#30340;&#20851;&#38190;&#22240;&#32032;&#26159;&#26500;&#24314;&#19968;&#20010;&#30456;&#23545;&#31616;&#21333;&#30340;&#21069;&#24230;&#37327;&#65292;&#20197;&#23450;&#20041;&#30446;&#26631;&#21521;&#37327;&#22330;&#65292;&#20854;&#20013;&#21253;&#25324;&#29616;&#26377;&#30340;&#27431;&#20960;&#37324;&#24471;&#24773;&#20917;&#12290;&#20026;&#20102;&#25193;&#23637;&#21040;&#19968;&#33324;&#20960;&#20309;&#65292;&#25105;&#20204;&#20381;&#38752;&#20351;&#29992;&#35889;&#20998;&#35299;&#26469;&#26377;&#25928;&#22320;&#21363;&#20852;&#35745;&#31639;&#21069;&#24230;&#37327;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#38750;&#27431;&#20960;&#37324;&#24471;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#36890;&#36807;&#22312;3D&#32593;&#26684;&#21644;&#21452;&#26354;&#31354;&#38388;&#19978;&#35757;&#32451;&#26631;&#20934;&#21270;&#27969;&#26469;&#35777;&#26126;&#20854;&#21151;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose Riemannian Flow Matching (RFM), a simple yet powerful framework for training continuous normalizing flows on manifolds. Existing methods for generative modeling on manifolds either require expensive simulation, are inherently unable to scale to high dimensions, or use approximations for limiting quantities that result in biased training objectives. Riemannian Flow Matching bypasses these limitations and offers several advantages over previous approaches: it is simulation-free on simple geometries, does not require divergence computation, and computes its target vector field in closed-form. The key ingredient behind RFM is the construction of a relatively simple premetric for defining target vector fields, which encompasses the existing Euclidean case. To extend to general geometries, we rely on the use of spectral decompositions to efficiently compute premetrics on the fly. Our method achieves state-of-the-art performance on real-world non-Euclidean datasets, and we demonstr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23637;&#31034;&#20102;&#19968;&#31181;&#28436;&#21592;-&#35780;&#35770;&#23478;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#33976;&#39311;&#20248;&#21183;&#22312;&#21033;&#29992;&#36807;&#21435;&#32463;&#39564;&#30340;&#21516;&#26102;&#36981;&#24490;&#31283;&#23450;&#30340;&#22312;&#32447;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#24555;&#36895;&#23398;&#20064;&#24182;&#21487;&#20197;&#36866;&#29992;&#20110;&#24191;&#27867;&#30340;&#31639;&#27861;&#31867;&#21035;&#12290;</title><link>http://arxiv.org/abs/2302.00533</link><description>&lt;p&gt;
&#33976;&#39311;&#31574;&#30053;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Distillation Policy Optimization. (arXiv:2302.00533v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.00533
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#20102;&#19968;&#31181;&#28436;&#21592;-&#35780;&#35770;&#23478;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#33976;&#39311;&#20248;&#21183;&#22312;&#21033;&#29992;&#36807;&#21435;&#32463;&#39564;&#30340;&#21516;&#26102;&#36981;&#24490;&#31283;&#23450;&#30340;&#22312;&#32447;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#24555;&#36895;&#23398;&#20064;&#24182;&#21487;&#20197;&#36866;&#29992;&#20110;&#24191;&#27867;&#30340;&#31639;&#27861;&#31867;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#28436;&#21592;-&#35780;&#35770;&#23478;&#23398;&#20064;&#26694;&#26550;&#65292;&#23427;&#20511;&#37492;&#20102;&#20998;&#24067;&#24335;&#23398;&#20064;&#30340;&#35270;&#35282;&#21644;&#20004;&#31181;&#31574;&#30053;&#25913;&#36827;&#25968;&#25454;&#30340;&#20132;&#21449;&#34701;&#21512;&#65292;&#23454;&#29616;&#20102;&#24555;&#36895;&#23398;&#20064;&#24182;&#21487;&#24212;&#29992;&#20110;&#24191;&#27867;&#30340;&#31639;&#27861;&#31867;&#21035;&#12290;&#22312;&#35813;&#26694;&#26550;&#20013;&#65292;&#39318;&#20808;&#25552;&#20986;&#20102;&#26041;&#24046;&#20943;&#23569;&#26426;&#21046;&#65292;&#20363;&#22914;&#32479;&#19968;&#20248;&#21183;&#20272;&#35745;&#22120; (UAE) &#21644;&#19968;&#20010;&#23398;&#20064;&#30340;&#22522;&#32447;&#65292;&#19981;&#20165;&#26159;&#36830;&#25509;&#21040;&#21160;&#20316;&#20540;&#20989;&#25968;&#30340;&#26725;&#26753;&#65292;&#36824;&#33021;&#25552;&#28860;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
On-policy algorithms are supposed to be stable, however, sample-intensive yet. Off-policy algorithms utilizing past experiences are deemed to be sample-efficient, nevertheless, unstable in general. Can we design an algorithm that can employ the off-policy data, while exploit the stable learning by sailing along the course of the on-policy walkway? In this paper, we present an actor-critic learning framework that borrows the distributional perspective of interest to evaluate, and cross-breeds two sources of the data for policy improvement, which enables fast learning and can be applied to a wide class of algorithms. In its backbone, the variance reduction mechanisms, such as unified advantage estimator (UAE), that extends generalized advantage estimator (GAE) to be applicable on any state-dependent baseline, and a learned baseline, that is competent to stabilize the policy gradient, are firstly put forward to not merely be a bridge to the action-value function but also distill the advan
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#23558;&#20165;&#22788;&#29702;&#25991;&#26412;&#30340;&#35821;&#35328;&#27169;&#22411;&#19982;&#22270;&#20687;&#36827;&#34892;&#32852;&#31995;&#65292;&#20351;&#20854;&#33021;&#22815;&#22788;&#29702;&#20219;&#24847;&#20132;&#38169;&#30340;&#22270;&#20687;&#21644;&#25991;&#26412;&#25968;&#25454;&#65292;&#24182;&#29983;&#25104;&#19982;&#26816;&#32034;&#22270;&#20687;&#20132;&#38169;&#30340;&#33258;&#30001;&#24418;&#24335;&#25991;&#26412;&#12290;&#35813;&#26041;&#27861;&#22312;&#29615;&#22659;&#30456;&#20851;&#30340;&#22270;&#20687;&#26816;&#32034;&#21644;&#22810;&#27169;&#24577;&#23545;&#35805;&#31561;&#20219;&#21153;&#20013;&#34920;&#29616;&#21313;&#20998;&#20248;&#24322;&#65292;&#26159;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#35299;&#20915;&#35270;&#35273;&#22330;&#26223;&#19979;&#20132;&#20114;&#38382;&#39064;&#30340;&#26377;&#25928;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2301.13823</link><description>&lt;p&gt;
&#23558;&#35821;&#35328;&#27169;&#22411;&#19982;&#22270;&#20687;&#36827;&#34892;&#32852;&#31995;&#20197;&#22788;&#29702;&#22810;&#27169;&#24577;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
Grounding Language Models to Images for Multimodal Inputs and Outputs. (arXiv:2301.13823v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.13823
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#23558;&#20165;&#22788;&#29702;&#25991;&#26412;&#30340;&#35821;&#35328;&#27169;&#22411;&#19982;&#22270;&#20687;&#36827;&#34892;&#32852;&#31995;&#65292;&#20351;&#20854;&#33021;&#22815;&#22788;&#29702;&#20219;&#24847;&#20132;&#38169;&#30340;&#22270;&#20687;&#21644;&#25991;&#26412;&#25968;&#25454;&#65292;&#24182;&#29983;&#25104;&#19982;&#26816;&#32034;&#22270;&#20687;&#20132;&#38169;&#30340;&#33258;&#30001;&#24418;&#24335;&#25991;&#26412;&#12290;&#35813;&#26041;&#27861;&#22312;&#29615;&#22659;&#30456;&#20851;&#30340;&#22270;&#20687;&#26816;&#32034;&#21644;&#22810;&#27169;&#24577;&#23545;&#35805;&#31561;&#20219;&#21153;&#20013;&#34920;&#29616;&#21313;&#20998;&#20248;&#24322;&#65292;&#26159;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#35299;&#20915;&#35270;&#35273;&#22330;&#26223;&#19979;&#20132;&#20114;&#38382;&#39064;&#30340;&#26377;&#25928;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#23558;&#39044;&#35757;&#32451;&#30340;&#20165;&#25991;&#26412;&#35821;&#35328;&#27169;&#22411;&#19982;&#35270;&#35273;&#39046;&#22495;&#32852;&#31995;&#36215;&#26469;&#65292;&#20351;&#20854;&#33021;&#22815;&#22788;&#29702;&#20219;&#24847;&#20132;&#38169;&#30340;&#22270;&#20687;&#21644;&#25991;&#26412;&#25968;&#25454;&#65292;&#24182;&#29983;&#25104;&#19982;&#26816;&#32034;&#22270;&#20687;&#20132;&#38169;&#30340;&#25991;&#26412;&#12290;&#25105;&#20204;&#21033;&#29992;&#20174;&#22823;&#35268;&#27169;&#25991;&#26412;&#39044;&#35757;&#32451;&#20013;&#23398;&#21040;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#20363;&#22914;&#19978;&#19979;&#25991;&#23398;&#20064;&#21644;&#33258;&#30001;&#24418;&#24335;&#25991;&#26412;&#29983;&#25104;&#12290;&#25105;&#20204;&#20445;&#25345;&#35821;&#35328;&#27169;&#22411;&#20923;&#32467;&#65292;&#24182;&#24494;&#35843;&#36755;&#20837;&#21644;&#36755;&#20986;&#32447;&#24615;&#23618;&#20197;&#23454;&#29616;&#36328;&#27169;&#24577;&#20132;&#20114;&#12290;&#36825;&#20351;&#24471;&#25105;&#20204;&#30340;&#27169;&#22411;&#33021;&#22815;&#22788;&#29702;&#20219;&#24847;&#20132;&#38169;&#30340;&#22270;&#20687;&#21644;&#25991;&#26412;&#36755;&#20837;&#65292;&#24182;&#29983;&#25104;&#19982;&#26816;&#32034;&#22270;&#20687;&#20132;&#38169;&#30340;&#33258;&#30001;&#24418;&#24335;&#25991;&#26412;&#12290;&#25105;&#20204;&#22312;&#29615;&#22659;&#30456;&#20851;&#30340;&#22270;&#20687;&#26816;&#32034;&#21644;&#22810;&#27169;&#24577;&#23545;&#35805;&#31561;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#24378;&#22823;&#30340;&#38646;-shot&#34920;&#29616;&#65292;&#24182;&#23637;&#31034;&#20102;&#24341;&#20154;&#20837;&#32988;&#30340;&#20132;&#20114;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36866;&#29992;&#20110;&#20219;&#20309;&#29616;&#25104;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#20026;&#22312;&#35270;&#35273;&#22330;&#26223;&#19979;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#25928;&#19988;&#36890;&#29992;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose an efficient method to ground pretrained text-only language models to the visual domain, enabling them to process arbitrarily interleaved image-and-text data, and generate text interleaved with retrieved images. Our method leverages the abilities of language models learnt from large scale text-only pretraining, such as in-context learning and free-form text generation. We keep the language model frozen, and finetune input and output linear layers to enable cross-modality interactions. This allows our model to process arbitrarily interleaved image-and-text inputs, and generate free-form text interleaved with retrieved images. We achieve strong zero-shot performance on grounded tasks such as contextual image retrieval and multimodal dialogue, and showcase compelling interactive abilities. Our approach works with any off-the-shelf language model and paves the way towards an effective, general solution for leveraging pretrained language models in visually grounded settings.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#19981;&#38656;&#35201;&#21453;&#21521;&#20256;&#25773;&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;BAFFLE&#65292;&#35813;&#26694;&#26550;&#20351;&#29992;&#22810;&#20010;&#27491;&#21521;&#36807;&#31243;&#20272;&#35745;&#26799;&#24230;&#65292;&#20855;&#26377;&#39640;&#20869;&#23384;&#25928;&#29575;&#65292;&#23481;&#26131;&#36866;&#24212;&#19978;&#20256;&#24102;&#23485;&#65292;&#19982;&#30828;&#20214;&#20248;&#21270;&#21644;&#27169;&#22411;&#37327;&#21270;/&#20462;&#21098;&#20860;&#23481;&#65292;&#36866;&#29992;&#20110;&#21463;&#20449;&#20219;&#30340;&#25191;&#34892;&#29615;&#22659;&#12290;</title><link>http://arxiv.org/abs/2301.12195</link><description>&lt;p&gt;
&#12298;&#32852;&#37030;&#23398;&#20064;&#26159;&#21542;&#30495;&#27491;&#38656;&#35201;&#21453;&#21521;&#20256;&#25773;&#65311;&#12299;
&lt;/p&gt;
&lt;p&gt;
Does Federated Learning Really Need Backpropagation?. (arXiv:2301.12195v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12195
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#19981;&#38656;&#35201;&#21453;&#21521;&#20256;&#25773;&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;BAFFLE&#65292;&#35813;&#26694;&#26550;&#20351;&#29992;&#22810;&#20010;&#27491;&#21521;&#36807;&#31243;&#20272;&#35745;&#26799;&#24230;&#65292;&#20855;&#26377;&#39640;&#20869;&#23384;&#25928;&#29575;&#65292;&#23481;&#26131;&#36866;&#24212;&#19978;&#20256;&#24102;&#23485;&#65292;&#19982;&#30828;&#20214;&#20248;&#21270;&#21644;&#27169;&#22411;&#37327;&#21270;/&#20462;&#21098;&#20860;&#23481;&#65292;&#36866;&#29992;&#20110;&#21463;&#20449;&#20219;&#30340;&#25191;&#34892;&#29615;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26159;&#19968;&#31181;&#21435;&#20013;&#24515;&#21270;&#22320;&#35753;&#23458;&#25143;&#31471;&#20849;&#21516;&#35757;&#32451;&#19968;&#20010;&#26381;&#21153;&#22120;&#27169;&#22411;&#30340;&#19968;&#33324;&#24615;&#21407;&#21017;&#65292;&#32780;&#26080;&#38656;&#20849;&#20139;&#26412;&#22320;&#25968;&#25454;&#12290;FL&#26159;&#19968;&#20010;&#20855;&#26377;&#23454;&#38469;&#24212;&#29992;&#30340;&#26377;&#21069;&#36884;&#30340;&#26694;&#26550;&#65292;&#20294;&#20854;&#26631;&#20934;&#35757;&#32451;&#33539;&#24335;&#35201;&#27714;&#23458;&#25143;&#31471;&#36890;&#36807;&#27169;&#22411;&#36827;&#34892;&#21453;&#21521;&#20256;&#25773;&#20197;&#35745;&#31639;&#26799;&#24230;&#12290;&#30001;&#20110;&#36825;&#20123;&#23458;&#25143;&#31471;&#36890;&#24120;&#26159;&#36793;&#32536;&#35774;&#22791;&#32780;&#19981;&#26159;&#23436;&#20840;&#21463;&#20449;&#20219;&#30340;&#65292;&#22240;&#27492;&#22312;&#23427;&#20204;&#19978;&#25191;&#34892;&#21453;&#21521;&#20256;&#25773;&#20250;&#20135;&#29983;&#35745;&#31639;&#21644;&#23384;&#20648;&#24320;&#38144;&#20197;&#21450;&#30333;&#30418;&#28431;&#27934;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#19981;&#38656;&#35201;&#21453;&#21521;&#20256;&#25773;&#30340;&#32852;&#37030;&#23398;&#20064;&#65292;&#31216;&#20026;BAFFLE&#65292;&#20854;&#20013;&#21453;&#21521;&#20256;&#25773;&#26367;&#25442;&#20026;&#22810;&#20010;&#27491;&#21521;&#36807;&#31243;&#20197;&#20272;&#35745;&#26799;&#24230;&#12290;BAFFLE&#20855;&#26377;&#20197;&#19979;&#20248;&#28857;&#65306;1&#65289;&#20869;&#23384;&#25928;&#29575;&#39640;&#24182;&#19988;&#23481;&#26131;&#36866;&#24212;&#19978;&#20256;&#24102;&#23485;&#65307;2&#65289;&#19982;&#20165;&#25512;&#29702;&#30828;&#20214;&#20248;&#21270;&#20197;&#21450;&#27169;&#22411;&#37327;&#21270;&#25110;&#20462;&#21098;&#20860;&#23481;&#65307;3&#65289;&#38750;&#24120;&#36866;&#21512;&#21463;&#20449;&#20219;&#30340;&#25191;&#34892;&#29615;&#22659;&#65292;&#22240;&#20026;BAFFLE&#20013;&#30340;&#23458;&#25143;&#31471;&#20165;&#25191;&#34892;&#27491;&#21521;&#20256;&#25773;&#24182;&#36820;&#22238;&#19968;&#32452;&#26631;&#37327;&#21040;&#26381;&#21153;&#22120;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#20351;&#29992;&#20102;BAFFLE&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) is a general principle for decentralized clients to train a server model collectively without sharing local data. FL is a promising framework with practical applications, but its standard training paradigm requires the clients to backpropagate through the model to compute gradients. Since these clients are typically edge devices and not fully trusted, executing backpropagation on them incurs computational and storage overhead as well as white-box vulnerability. In light of this, we develop backpropagation-free federated learning, dubbed BAFFLE, in which backpropagation is replaced by multiple forward processes to estimate gradients. BAFFLE is 1) memory-efficient and easily fits uploading bandwidth; 2) compatible with inference-only hardware optimization and model quantization or pruning; and 3) well-suited to trusted execution environments, because the clients in BAFFLE only execute forward propagation and return a set of scalars to the server. Empirically we us
&lt;/p&gt;</description></item><item><title>S-Graphs+&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;&#22235;&#23618;&#22240;&#23376;&#22270;&#65292;&#33021;&#22815;&#22312;&#23454;&#26102;&#20248;&#21270;&#20013;&#21516;&#26102;&#24314;&#27169;&#23039;&#24577;&#22270;&#21644;&#22330;&#26223;&#22270;&#65292;&#25552;&#39640;&#20102;&#29615;&#22659;&#20449;&#24687;&#30340;&#39640;&#23618;&#27425;&#25277;&#35937;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#20998;&#21106;&#26041;&#27861;&#26469;&#25552;&#21462;&#29615;&#22659;&#30340;&#25151;&#38388;&#21644;&#27004;&#23618;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2212.11770</link><description>&lt;p&gt;
S-Graphs+&#65306;&#21033;&#29992;&#20998;&#23618;&#34920;&#31034;&#36827;&#34892;&#23454;&#26102;&#23450;&#20301;&#21644;&#24314;&#22270;
&lt;/p&gt;
&lt;p&gt;
S-Graphs+: Real-time Localization and Mapping leveraging Hierarchical Representations. (arXiv:2212.11770v3 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.11770
&lt;/p&gt;
&lt;p&gt;
S-Graphs+&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;&#22235;&#23618;&#22240;&#23376;&#22270;&#65292;&#33021;&#22815;&#22312;&#23454;&#26102;&#20248;&#21270;&#20013;&#21516;&#26102;&#24314;&#27169;&#23039;&#24577;&#22270;&#21644;&#22330;&#26223;&#22270;&#65292;&#25552;&#39640;&#20102;&#29615;&#22659;&#20449;&#24687;&#30340;&#39640;&#23618;&#27425;&#25277;&#35937;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#20998;&#21106;&#26041;&#27861;&#26469;&#25552;&#21462;&#29615;&#22659;&#30340;&#25151;&#38388;&#21644;&#27004;&#23618;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36827;&#21270;&#29256;Situational Graphs&#65292;&#20854;&#36890;&#36807;&#22312;&#21333;&#19968;&#20248;&#21270;&#22240;&#23376;&#22270;&#20013;&#21516;&#26102;&#24314;&#27169;&#23039;&#24577;&#22270;&#21644;&#19977;&#32500;&#22330;&#26223;&#22270;&#65292;&#20174;&#32780;&#23545;&#29615;&#22659;&#36827;&#34892;&#39640;&#23618;&#27425;&#24314;&#27169;&#21644;&#21387;&#32553;&#65292;&#24182;&#23454;&#26102;&#20248;&#21270;&#20197;&#33719;&#24471;&#26426;&#22120;&#20154;&#23039;&#24577;&#30340;&#40065;&#26834;&#21644;&#31934;&#30830;&#20272;&#35745;&#12290;&#20026;&#20102;&#25552;&#39640;&#22330;&#26223;&#30340;&#25277;&#35937;&#32423;&#21035;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26032;&#30340;&#25151;&#38388;&#21644;&#27004;&#23618;&#20998;&#21106;&#26041;&#27861;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#22235;&#23618;&#22240;&#23376;&#22270;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present an evolved version of Situational Graphs, which jointly models in a single optimizable factor graph (1) a pose graph, as a set of robot keyframes comprising associated measurements and robot poses, and (2) a 3D scene graph, as a high-level representation of the environment that encodes its different geometric elements with semantic attributes and the relational information between them.  Specifically, our S-Graphs+ is a novel four-layered factor graph that includes: (1) a keyframes layer with robot pose estimates, (2) a walls layer representing wall surfaces, (3) a rooms layer encompassing sets of wall planes, and (4) a floors layer gathering the rooms within a given floor level. The above graph is optimized in real-time to obtain a robust and accurate estimate of the robots pose and its map, simultaneously constructing and leveraging high-level information of the environment. To extract this high-level information, we present novel room and floor segmentation
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Self-Instruct&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#36523;&#29983;&#25104;&#25351;&#23548;&#20449;&#24687;&#26469;&#25552;&#39640;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#25351;&#20196;&#36981;&#24490;&#33021;&#21147;&#12290;&#22312;&#36229;&#33258;&#28982;&#25351;&#20196;&#19978;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19982;InstructGPT-001&#30456;&#21516;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#24182;&#22312;&#21407;&#22987;&#27169;&#22411;&#19978;&#33719;&#24471;&#20102;33%&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2212.10560</link><description>&lt;p&gt;
&#33258;&#25105;&#25351;&#23548;: &#29992;&#33258;&#29983;&#25104;&#30340;&#25351;&#31034;&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Self-Instruct: Aligning Language Models with Self-Generated Instructions. (arXiv:2212.10560v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10560
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Self-Instruct&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#36523;&#29983;&#25104;&#25351;&#23548;&#20449;&#24687;&#26469;&#25552;&#39640;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#25351;&#20196;&#36981;&#24490;&#33021;&#21147;&#12290;&#22312;&#36229;&#33258;&#28982;&#25351;&#20196;&#19978;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19982;InstructGPT-001&#30456;&#21516;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#24182;&#22312;&#21407;&#22987;&#27169;&#22411;&#19978;&#33719;&#24471;&#20102;33%&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#30340;"&#25351;&#20196;&#35843;&#25972;"&#35821;&#35328;&#27169;&#22411;(&#21363;&#65292;&#35843;&#25972;&#20026;&#21709;&#24212;&#25351;&#20196;)&#24050;&#32463;&#23637;&#31034;&#20102;&#24778;&#20154;&#30340;&#33021;&#21147;&#65292;&#21487;&#20197;&#38646;-shot&#25512;&#24191;&#21040;&#26032;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20005;&#37325;&#20381;&#36182;&#20110;&#20154;&#24037;&#32534;&#20889;&#30340;&#25351;&#20196;&#25968;&#25454;&#65292;&#36890;&#24120;&#22312;&#25968;&#37327;&#12289;&#22810;&#26679;&#24615;&#21644;&#21019;&#36896;&#21147;&#26041;&#38754;&#21463;&#21040;&#38480;&#21046;&#65292;&#20174;&#32780;&#38459;&#30861;&#20102;&#35843;&#25972;&#27169;&#22411;&#30340;&#36890;&#29992;&#24615;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;Self-Instruct&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#25913;&#21892;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36981;&#24490;&#25351;&#20196;&#33021;&#21147;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#36523;&#30340;&#29983;&#25104;&#26469;&#24341;&#23548;&#23427;&#20204;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#27969;&#31243;&#20174;&#35821;&#35328;&#27169;&#22411;&#20013;&#29983;&#25104;&#25351;&#20196;&#12289;&#36755;&#20837;&#21644;&#36755;&#20986;&#26679;&#26412;&#65292;&#28982;&#21518;&#36807;&#28388;&#25481;&#26080;&#25928;&#25110;&#30456;&#20284;&#30340;&#26679;&#26412;&#65292;&#28982;&#21518;&#20877;&#23558;&#23427;&#20204;&#29992;&#20110;&#35843;&#25972;&#21407;&#22987;&#27169;&#22411;&#12290;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;&#26222;&#36890;&#30340;GPT3&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#36229;&#33258;&#28982;&#25351;&#20196;&#19978;&#19982;InstructGPT-001&#30340;&#24615;&#33021;&#30456;&#23218;&#32654;&#65292;&#24182;&#27604;&#21407;&#22987;&#27169;&#22411;&#33719;&#24471;&#20102;33%&#30340;&#32477;&#23545;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large "instruction-tuned" language models (i.e., finetuned to respond to instructions) have demonstrated a remarkable ability to generalize zero-shot to new tasks. Nevertheless, they depend heavily on human-written instruction data that is often limited in quantity, diversity, and creativity, therefore hindering the generality of the tuned model. We introduce Self-Instruct, a framework for improving the instruction-following capabilities of pretrained language models by bootstrapping off their own generations. Our pipeline generates instructions, input, and output samples from a language model, then filters invalid or similar ones before using them to finetune the original model. Applying our method to the vanilla GPT3, we demonstrate a 33% absolute improvement over the original model on Super-NaturalInstructions, on par with the performance of InstructGPT-001, which was trained with private user data and human annotations. For further evaluation, we curate a set of expert-written inst
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25512;&#29702;&#30740;&#31350;&#30340;&#29616;&#29366;&#21644;&#26410;&#26469;&#26041;&#21521;&#65292;&#21253;&#25324;&#25552;&#39640;&#21644;&#35825;&#23548;&#25512;&#29702;&#33021;&#21147;&#30340;&#25216;&#26415;&#12289;&#35780;&#20272;&#25512;&#29702;&#33021;&#21147;&#30340;&#26041;&#27861;&#21644;&#22522;&#20934;&#65292;&#26088;&#22312;&#25552;&#20379;&#19968;&#20010;&#35814;&#32454;&#21644;&#26368;&#26032;&#30340;&#32508;&#36848;&#65292;&#21050;&#28608;&#26377;&#24847;&#20041;&#30340;&#35752;&#35770;&#21644;&#26410;&#26469;&#30340;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2212.10403</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25512;&#29702;&#30740;&#31350;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Towards Reasoning in Large Language Models: A Survey. (arXiv:2212.10403v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10403
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25512;&#29702;&#30740;&#31350;&#30340;&#29616;&#29366;&#21644;&#26410;&#26469;&#26041;&#21521;&#65292;&#21253;&#25324;&#25552;&#39640;&#21644;&#35825;&#23548;&#25512;&#29702;&#33021;&#21147;&#30340;&#25216;&#26415;&#12289;&#35780;&#20272;&#25512;&#29702;&#33021;&#21147;&#30340;&#26041;&#27861;&#21644;&#22522;&#20934;&#65292;&#26088;&#22312;&#25552;&#20379;&#19968;&#20010;&#35814;&#32454;&#21644;&#26368;&#26032;&#30340;&#32508;&#36848;&#65292;&#21050;&#28608;&#26377;&#24847;&#20041;&#30340;&#35752;&#35770;&#21644;&#26410;&#26469;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#29702;&#26159;&#20154;&#31867;&#26234;&#33021;&#30340;&#22522;&#26412;&#26041;&#38754;&#65292;&#22312;&#38382;&#39064;&#35299;&#20915;&#12289;&#20915;&#31574;&#21644;&#25209;&#21028;&#24615;&#24605;&#32500;&#31561;&#27963;&#21160;&#20013;&#21457;&#25381;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#36817;&#24180;&#26469;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#35266;&#23519;&#21040;&#24403;&#36825;&#20123;&#27169;&#22411;&#36275;&#22815;&#22823;&#26102;&#65292;&#23427;&#20204;&#21487;&#33021;&#20250;&#23637;&#29616;&#20986;&#25512;&#29702;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23578;&#19981;&#28165;&#26970;LLMs&#22312;&#25512;&#29702;&#26041;&#38754;&#30340;&#33021;&#21147;&#21040;&#24213;&#22914;&#20309;&#12290;&#26412;&#25991;&#20840;&#38754;&#38416;&#36848;&#20102;LLMs&#20013;&#25512;&#29702;&#30740;&#31350;&#30340;&#24403;&#21069;&#29366;&#20917;&#65292;&#21253;&#25324;&#25552;&#39640;&#21644;&#35825;&#23548;&#36825;&#20123;&#27169;&#22411;&#25512;&#29702;&#30340;&#25216;&#26415;&#12289;&#35780;&#20272;&#25512;&#29702;&#33021;&#21147;&#30340;&#26041;&#27861;&#21644;&#22522;&#20934;&#12289;&#20197;&#21450;&#20197;&#24448;&#30740;&#31350;&#30340;&#32467;&#26524;&#21644;&#24847;&#20041;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#26410;&#26469;&#26041;&#21521;&#30340;&#24314;&#35758;&#12290;&#25105;&#20204;&#30340;&#30446;&#30340;&#26159;&#25552;&#20379;&#19968;&#20010;&#35814;&#32454;&#21644;&#26368;&#26032;&#30340;&#32508;&#36848;&#65292;&#21050;&#28608;&#26377;&#24847;&#20041;&#30340;&#35752;&#35770;&#21644;&#26410;&#26469;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reasoning is a fundamental aspect of human intelligence that plays a crucial role in activities such as problem solving, decision making, and critical thinking. In recent years, large language models (LLMs) have made significant progress in natural language processing, and there is observation that these models may exhibit reasoning abilities when they are sufficiently large. However, it is not yet clear to what extent LLMs are capable of reasoning. This paper provides a comprehensive overview of the current state of knowledge on reasoning in LLMs, including techniques for improving and eliciting reasoning in these models, methods and benchmarks for evaluating reasoning abilities, findings and implications of previous research in this field, and suggestions on future directions. Our aim is to provide a detailed and up-to-date review of this topic and stimulate meaningful discussion and future work.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#22788;&#29702;&#8220;few-&#8221;&#31867;&#22411;&#37327;&#35789;&#30340;&#33021;&#21147;&#65292;&#32467;&#26524;&#26174;&#31034;&#25152;&#26377;&#27169;&#22411;&#23545;&#36825;&#31181;&#37327;&#35789;&#37117;&#34920;&#29616;&#19981;&#20339;&#65292;&#19988;&#36739;&#22823;&#30340;&#27169;&#22411;&#34920;&#29616;&#26356;&#24046;&#12290;&#36825;&#31181;&#21453;&#27604;&#20363;&#32553;&#25918;&#30340;&#29616;&#35937;&#34920;&#26126;&#22823;&#22411;&#27169;&#22411;&#36234;&#26469;&#24840;&#21453;&#26144;&#22312;&#32447;&#20154;&#31867;&#22788;&#29702;&#65292;&#32780;&#19981;&#26159;&#31163;&#32447;&#22788;&#29702;&#12290;&#36825;&#21487;&#33021;&#25361;&#25112;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#33258;&#28982;&#35821;&#35328;&#31995;&#32479;&#22522;&#30784;&#30340;&#20570;&#27861;&#12290;</title><link>http://arxiv.org/abs/2212.08700</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#37327;&#35789;&#26102;&#34920;&#29616;&#30053;&#26377;&#38382;&#39064;&#65311;&#20351;&#29992;&#23569;&#37327;&#31867;&#22411;&#30340;&#37327;&#35789;&#20250;&#23548;&#33268;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#27979;&#21576;&#29616;&#21453;&#27604;&#20363;&#32553;&#25918;&#30340;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;
Rarely a problem? Language models exhibit inverse scaling in their predictions following few-type quantifiers. (arXiv:2212.08700v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.08700
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#22788;&#29702;&#8220;few-&#8221;&#31867;&#22411;&#37327;&#35789;&#30340;&#33021;&#21147;&#65292;&#32467;&#26524;&#26174;&#31034;&#25152;&#26377;&#27169;&#22411;&#23545;&#36825;&#31181;&#37327;&#35789;&#37117;&#34920;&#29616;&#19981;&#20339;&#65292;&#19988;&#36739;&#22823;&#30340;&#27169;&#22411;&#34920;&#29616;&#26356;&#24046;&#12290;&#36825;&#31181;&#21453;&#27604;&#20363;&#32553;&#25918;&#30340;&#29616;&#35937;&#34920;&#26126;&#22823;&#22411;&#27169;&#22411;&#36234;&#26469;&#24840;&#21453;&#26144;&#22312;&#32447;&#20154;&#31867;&#22788;&#29702;&#65292;&#32780;&#19981;&#26159;&#31163;&#32447;&#22788;&#29702;&#12290;&#36825;&#21487;&#33021;&#25361;&#25112;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#33258;&#28982;&#35821;&#35328;&#31995;&#32479;&#22522;&#30784;&#30340;&#20570;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#22788;&#29702;&#37327;&#21270;&#38382;&#39064;&#12290;&#25105;&#20204;&#20197;&#8220;few-&#8221;&#31867;&#22411;&#30340;&#37327;&#35789;&#20026;&#37325;&#28857;&#65292;&#27604;&#22914;&#8220;few children like toys&#8221;&#65292;&#22240;&#20026;&#36825;&#31181;&#31867;&#22411;&#30340;&#21477;&#23376;&#32452;&#25104;&#37096;&#20998;&#36890;&#24120;&#20250;&#20849;&#29616;&#65292;&#32780;&#8220;few-&#8221;&#31867;&#22411;&#30340;&#37327;&#35789;&#36739;&#20026;&#32597;&#35265;&#65292;&#36825;&#21487;&#33021;&#23545;&#35821;&#35328;&#27169;&#22411;&#26500;&#25104;&#29305;&#21035;&#25361;&#25112;&#12290;&#25105;&#20204;&#23545;&#26469;&#33258;&#20004;&#39033;&#20154;&#31867;&#31070;&#32463;&#35821;&#35328;&#23398;&#23454;&#39564;&#30340;960&#20010;&#33521;&#35821;&#21477;&#23376;&#36827;&#34892;&#20102;&#35797;&#39564;&#65292;&#24182;&#23558;&#23427;&#20204;&#25552;&#20379;&#32473;22&#20010;&#19981;&#21516;&#22823;&#23567;&#30340;&#33258;&#22238;&#24402;&#21464;&#25442;&#22120;&#27169;&#22411;&#12290;&#19981;&#20165;&#25152;&#26377;&#27169;&#22411;&#23545;&#8220;few-&#8221;&#31867;&#22411;&#30340;&#37327;&#35789;&#37117;&#34920;&#29616;&#19981;&#20339;&#65292;&#32780;&#19988;&#24635;&#20307;&#19978;&#65292;&#27169;&#22411;&#36234;&#22823;&#65292;&#20854;&#34920;&#29616;&#36234;&#24046;&#12290;&#36825;&#31181;&#21453;&#27604;&#20363;&#32553;&#25918;&#30340;&#29616;&#35937;&#19982;&#20043;&#21069;&#30340;&#30740;&#31350;&#32467;&#26524;&#19968;&#33268;&#65292;&#34920;&#26126;&#36739;&#22823;&#30340;&#27169;&#22411;&#36234;&#26469;&#36234;&#21453;&#26144;&#22312;&#32447;&#20154;&#31867;&#22788;&#29702;&#65292;&#32780;&#19981;&#26159;&#31163;&#32447;&#22788;&#29702;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#26356;&#22823;&#30340;&#27169;&#22411;&#30340;&#24615;&#33021;&#19979;&#38477;&#21487;&#33021;&#20250;&#25361;&#25112;&#23558;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#33258;&#28982;&#35821;&#35328;&#31995;&#32479;&#22522;&#30784;&#30340;&#20570;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
How well do language models deal with quantification? In this study, we focus on 'few'-type quantifiers, as in 'few children like toys', which might pose a particular challenge for language models because the sentence components with out the quantifier are likely to co-occur, and 'few'-type quantifiers are rare. We present 960 English sentence stimuli from two human neurolinguistic experiments to 22 autoregressive transformer models of differing sizes. Not only do all the models perform poorly on 'few'-type quantifiers, but overall the larger the model, the worse its performance. This inverse scaling is consistent with previous work suggesting that larger models increasingly reflect online rather than offline human processing, and we argue that the decreasing performance of larger models may challenge uses of language models as the basis for natural language systems.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20219;&#21153;&#30456;&#20284;&#24230;&#20803;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#21152;&#36895;&#26641;&#24418;&#32467;&#26500;Parzen&#20272;&#35745;&#20013;&#30340;&#22810;&#30446;&#26631;&#38750;&#20998;&#23618;&#36229;&#21442;&#25968;&#26368;&#20248;&#21270;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2212.06751</link><description>&lt;p&gt;
&#22522;&#20110;&#20219;&#21153;&#30456;&#20284;&#24230;&#20803;&#23398;&#20064;&#21152;&#36895;&#22810;&#30446;&#26631;&#38750;&#20998;&#23618;&#36229;&#21442;&#25968;&#26368;&#20248;&#21270;&#30340;&#26641;&#24418;&#32467;&#26500;Parzen&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Speeding up Multi-objective Non-hierarchical Hyperparameter Optimization by Task Similarity-Based Meta-Learning for the Tree-structured Parzen Estimator. (arXiv:2212.06751v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.06751
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20219;&#21153;&#30456;&#20284;&#24230;&#20803;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#21152;&#36895;&#26641;&#24418;&#32467;&#26500;Parzen&#20272;&#35745;&#20013;&#30340;&#22810;&#30446;&#26631;&#38750;&#20998;&#23618;&#36229;&#21442;&#25968;&#26368;&#20248;&#21270;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36229;&#21442;&#25968;&#20248;&#21270;&#26159;&#25552;&#39640;&#28145;&#24230;&#23398;&#20064;&#24615;&#33021;&#30340;&#20851;&#38190;&#27493;&#39588;&#12290;&#23454;&#36341;&#32773;&#36890;&#24120;&#38754;&#20020;&#22810;&#20010;&#26041;&#38754;&#30340;&#26435;&#34913;&#65292;&#22914;&#20934;&#30830;&#24615;&#21644;&#24310;&#36831;&#26102;&#38388;&#12290;&#22312;&#28145;&#24230;&#23398;&#20064;&#30340;&#39640;&#35745;&#31639;&#38656;&#27714;&#21644;&#23545;&#39640;&#25928;&#36229;&#21442;&#25968;&#20248;&#21270;&#30340;&#19981;&#26029;&#22686;&#38271;&#38656;&#27714;&#19979;&#65292;&#21152;&#36895;&#22810;&#30446;&#26631;&#20248;&#21270;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#26412;&#25991;&#23558;TPE&#30340;&#25910;&#36141;&#20989;&#25968;&#25193;&#23637;&#21040;&#20803;&#23398;&#20064;&#35774;&#32622;&#20013;&#65292;&#20351;&#29992;&#30001;&#20219;&#21153;&#20043;&#38388;&#39030;&#32423;&#22495;&#20043;&#38388;&#30340;&#37325;&#21472;&#24230;&#23450;&#20041;&#30340;&#20219;&#21153;&#30456;&#20284;&#24615;&#12290;&#25105;&#20204;&#20063;&#20174;&#29702;&#35770;&#19978;&#20998;&#26512;&#24182;&#35299;&#20915;&#20102;&#20219;&#21153;&#30456;&#20284;&#24615;&#30340;&#23616;&#38480;&#24615;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#34920;&#26684;HPO&#22522;&#20934;&#19978;&#21152;&#36895;&#20102;MO-TPE&#65292;&#24182;&#33719;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36824;&#36890;&#36807;&#36194;&#24471;AutoML 2022&#26469;&#24471;&#21040;&#22806;&#37096;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hyperparameter optimization (HPO) is a vital step in improving performance in deep learning (DL). Practitioners are often faced with the trade-off between multiple criteria, such as accuracy and latency. Given the high computational needs of DL and the growing demand for efficient HPO, the acceleration of multi-objective (MO) optimization becomes ever more important. Despite the significant body of work on meta-learning for HPO, existing methods are inapplicable to MO tree-structured Parzen estimator (MO-TPE), a simple yet powerful MO-HPO algorithm. In this paper, we extend TPE's acquisition function to the meta-learning setting using a task similarity defined by the overlap of top domains between tasks. We also theoretically analyze and address the limitations of our task similarity. In the experiments, we demonstrate that our method speeds up MO-TPE on tabular HPO benchmarks and attains state-of-the-art performance. Our method was also validated externally by winning the AutoML 2022 
&lt;/p&gt;</description></item><item><title>NPM&#26159;&#31532;&#19968;&#20010;&#20351;&#29992;&#38750;&#21442;&#25968;&#20998;&#24067;&#26367;&#25442;softmax&#30340;&#36974;&#34109;&#35821;&#35328;&#27169;&#22411;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#22788;&#29702;&#31232;&#26377;&#27169;&#24335;&#21644;&#39044;&#27979;&#32597;&#35265;&#25110;&#20960;&#20046;&#26410;&#35265;&#36807;&#30340;&#21333;&#35789;&#65292;&#24182;&#22312;16&#39033;&#20219;&#21153;&#19978;&#36229;&#36807;&#20102;&#26356;&#22823;&#30340;&#21442;&#25968;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2212.01349</link><description>&lt;p&gt;
&#38750;&#21442;&#25968;&#36974;&#34109;&#35821;&#35328;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Nonparametric Masked Language Modeling. (arXiv:2212.01349v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.01349
&lt;/p&gt;
&lt;p&gt;
NPM&#26159;&#31532;&#19968;&#20010;&#20351;&#29992;&#38750;&#21442;&#25968;&#20998;&#24067;&#26367;&#25442;softmax&#30340;&#36974;&#34109;&#35821;&#35328;&#27169;&#22411;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#22788;&#29702;&#31232;&#26377;&#27169;&#24335;&#21644;&#39044;&#27979;&#32597;&#35265;&#25110;&#20960;&#20046;&#26410;&#35265;&#36807;&#30340;&#21333;&#35789;&#65292;&#24182;&#22312;16&#39033;&#20219;&#21153;&#19978;&#36229;&#36807;&#20102;&#26356;&#22823;&#30340;&#21442;&#25968;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#36890;&#36807;&#26377;&#38480;&#35789;&#27719;&#34920;&#19978;&#30340; softmax &#26469;&#39044;&#27979;&#26631;&#35760;&#65292;&#36825;&#21487;&#33021;&#20351;&#24471;&#39044;&#27979;&#31232;&#26377;&#26631;&#35760;&#25110;&#30701;&#35821;&#21464;&#24471;&#22256;&#38590;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102; NPM&#65292;&#23427;&#26159;&#31532;&#19968;&#20010;&#20351;&#29992;&#23545;&#27599;&#20010;&#21442;&#32771;&#35821;&#26009;&#24211;&#20013;&#30701;&#35821;&#30340;&#38750;&#21442;&#25968;&#20998;&#24067;&#26367;&#25442;&#27492; softmax &#30340;&#38750;&#21442;&#25968;&#36974;&#34109;&#35821;&#35328;&#27169;&#22411;&#12290;NPM &#20165;&#36890;&#36807;&#20174;&#25991;&#26412;&#35821;&#26009;&#24211;&#20013;&#26816;&#32034;&#26631;&#35760;&#26469;&#22635;&#20889; [MASK]&#12290;&#25105;&#20204;&#23637;&#31034;&#20102; NPM &#21487;&#20197;&#36890;&#36807;&#23545;&#27604;&#24615;&#30446;&#26631;&#21644;&#25209;&#37327;&#36817;&#20284;&#20840;&#35821;&#26009;&#24211;&#26816;&#32034;&#26377;&#25928;&#22320;&#35757;&#32451;&#12290;&#23545; 16 &#39033;&#20219;&#21153;&#36827;&#34892;&#38646;&#26679;&#26412;&#35780;&#20272;&#65292;&#21253;&#25324;&#20998;&#31867;&#12289;&#20107;&#23454;&#25506;&#38024;&#21644;&#38382;&#39064;&#22238;&#31572;&#65292;&#35777;&#26126; NPM &#36229;&#36807;&#20102;&#26174;&#30528;&#26356;&#22823;&#30340;&#21442;&#25968;&#27169;&#22411;&#65292;&#26080;&#35770;&#20351;&#29992;&#25110;&#19981;&#20351;&#29992;&#26816;&#32034;&#29983;&#25104;&#26041;&#27861;&#65292;&#23427;&#22312;&#22788;&#29702;&#31232;&#26377;&#27169;&#24335;&#65288;&#35789;&#20041;&#25110;&#20107;&#23454;&#65289;&#21644;&#39044;&#27979;&#32597;&#35265;&#25110;&#20960;&#20046;&#26410;&#35265;&#36807;&#30340;&#21333;&#35789;&#65288;&#22914;&#38750;&#25289;&#19969;&#25991;&#33050;&#26412;&#65289;&#26041;&#38754;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#22312; github.com/facebookresearch/NPM &#19978;&#21457;&#24067;&#20102;&#27169;&#22411;&#21644;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing language models (LMs) predict tokens with a softmax over a finite vocabulary, which can make it difficult to predict rare tokens or phrases. We introduce NPM, the first nonparametric masked language model that replaces this softmax with a nonparametric distribution over every phrase in a reference corpus. NPM fills in the [MASK] solely from retrieving a token from a text corpus. We show that NPM can be efficiently trained with a contrastive objective and an in-batch approximation to full corpus retrieval. Zero-shot evaluation on 16 tasks including classification, fact probing and question answering demonstrates that NPM outperforms significantly larger parametric models, either with or without a retrieve-and-generate approach. It is particularly better at dealing with rare patterns (word senses or facts) and predicting rare or nearly unseen words (e.g., non-Latin script). We release the model and code at github.com/facebookresearch/NPM.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39044;&#27979;&#20196;&#29260;&#19982;&#21442;&#32771;&#35821;&#20041;&#30456;&#20284;&#24615;&#30340;&#26032;&#22411;&#35757;&#32451;&#30446;&#26631;&#65292;&#21487;&#20197;&#22312;&#39046;&#22495;&#33258;&#36866;&#24212;&#20013;&#32531;&#35299;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#21516;&#26102;&#21448;&#21487;&#20197;&#20445;&#25345;&#35843;&#25972;&#36136;&#37327;&#65292;&#24182;&#19988;&#35745;&#31639;&#25104;&#26412;&#22686;&#21152;&#21487;&#24573;&#30053;&#19981;&#35745;&#12290;</title><link>http://arxiv.org/abs/2211.16550</link><description>&lt;p&gt;
&#29992;&#20110;&#29983;&#25104;&#35821;&#35328;&#30340;&#36719;&#23545;&#40784;&#30446;&#26631;&#30340;&#40065;&#26834;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Soft Alignment Objectives for Robust Adaptation of Language Generation. (arXiv:2211.16550v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.16550
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39044;&#27979;&#20196;&#29260;&#19982;&#21442;&#32771;&#35821;&#20041;&#30456;&#20284;&#24615;&#30340;&#26032;&#22411;&#35757;&#32451;&#30446;&#26631;&#65292;&#21487;&#20197;&#22312;&#39046;&#22495;&#33258;&#36866;&#24212;&#20013;&#32531;&#35299;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#21516;&#26102;&#21448;&#21487;&#20197;&#20445;&#25345;&#35843;&#25972;&#36136;&#37327;&#65292;&#24182;&#19988;&#35745;&#31639;&#25104;&#26412;&#22686;&#21152;&#21487;&#24573;&#30053;&#19981;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39046;&#22495;&#33258;&#36866;&#24212;&#20801;&#35768;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#35299;&#20915;&#24212;&#29992;&#39046;&#22495;&#36716;&#31227;&#36896;&#25104;&#30340;&#29305;&#23450;&#32570;&#38519;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;&#22312;&#39046;&#22495;&#20869;&#25968;&#25454;&#19978;&#36827;&#34892;&#36827;&#19968;&#27493;&#35757;&#32451;&#26469;&#36827;&#34892;&#20256;&#32479;&#36866;&#24212;&#20250;&#36805;&#36895;&#21066;&#24369;&#27169;&#22411;&#25512;&#24191;&#21040;&#20854;&#20182;&#39046;&#22495;&#30340;&#33021;&#21147;&#65292;&#20351;&#24471;&#35843;&#25972;&#21518;&#27169;&#22411;&#30340;&#26080;&#38480;&#37096;&#32626;&#23481;&#26131;&#20986;&#29616;&#38169;&#35823;&#12290;&#26412;&#24037;&#20316;&#20171;&#32461;&#20102;&#24314;&#31435;&#22312;&#39044;&#27979;&#20196;&#29260;&#19982;&#21442;&#32771;&#35821;&#20041;&#30456;&#20284;&#24615;&#30340;&#26032;&#22411;&#35757;&#32451;&#30446;&#26631;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#36991;&#20813;&#21333;&#20010;&#27491;&#30830;&#39044;&#27979;&#30340;&#24120;&#35265;&#20551;&#35774;&#65292;&#36890;&#36807;&#26500;&#24314;&#26469;&#33258;&#20196;&#29260;&#35821;&#20041;&#30456;&#20284;&#24615;&#30340;&#35757;&#32451;&#30446;&#26631;&#21487;&#20197;&#32531;&#35299;&#39046;&#22495;&#36866;&#24212;&#26399;&#38388;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#21516;&#26102;&#22312;&#20445;&#25345;&#35843;&#25972;&#36136;&#37327;&#26041;&#38754;&#20855;&#26377;&#21487;&#24573;&#30053;&#30340;&#35745;&#31639;&#25104;&#26412;&#22686;&#21152;&#12290;&#22312;&#26356;&#24191;&#27867;&#30340;&#32972;&#26223;&#19979;&#65292;&#22522;&#20110;&#36830;&#32493;&#30340;&#20196;&#29260;&#30456;&#20284;&#24230;&#30340;&#30446;&#26631;&#24341;&#39046;&#20102;&#39640;&#25928;&#20294;&#26174;&#24335;&#20196;&#29260;&#32423;&#30446;&#26631;&#21644;&#20855;&#26377;&#34920;&#29616;&#21147;&#30340;&#22522;&#20110;&#36830;&#32493;&#20196;&#29260;&#34920;&#31034;&#30340;&#30446;&#26631;&#20043;&#38388;&#20013;&#38388;&#22320;&#24102;&#30340;&#25506;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Domain adaptation allows generative language models to address specific flaws caused by the domain shift of their application. However, the traditional adaptation by further training on in-domain data rapidly weakens the model's ability to generalize to other domains, making the open-ended deployments of the adapted models prone to errors. This work introduces novel training objectives built upon a semantic similarity of the predicted tokens to the reference.  Our results show that (1) avoiding the common assumption of a single correct prediction by constructing the training target from tokens' semantic similarity can mitigate catastrophic forgetting during domain adaptation, while (2) preserving the quality of the adaptation, (3) with negligible additions to compute costs.  In the broader context, the objectives grounded in a continuous token similarity pioneer the exploration of the middle ground between the efficient but na\"{\i}ve exact-match token-level objectives and expressive b
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#32422;&#26463;TPE&#65288;c-TPE&#65289;&#26041;&#27861;&#65292;&#26159;&#26641;&#24418;Parzen&#20272;&#35745;&#22120;&#65288;TPE&#65289;&#30340;&#25193;&#23637;&#65292;&#21487;&#26377;&#25928;&#22788;&#29702;&#22312;&#24615;&#33021;&#35201;&#27714;&#20043;&#19978;&#26045;&#21152;&#30340;&#32422;&#26463;&#38480;&#21046;&#65292;&#23454;&#39564;&#35777;&#26126;&#22312;81&#20010;&#26114;&#36149;&#30340;HPO&#35774;&#32622;&#20013;&#34920;&#29616;&#20986;&#26368;&#20339;&#24615;&#33021;&#25490;&#21517;&#12290;</title><link>http://arxiv.org/abs/2211.14411</link><description>&lt;p&gt;
c-TPE:&#22522;&#20110;&#26641;&#24418;&#32467;&#26500;&#30340;&#24102;&#19981;&#31561;&#24335;&#32422;&#26463;&#30340;&#24085;&#25463;&#26031;&#29305;&#20272;&#35745;&#22120;&#29992;&#20110;&#26114;&#36149;&#36229;&#21442;&#25968;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
c-TPE: Tree-structured Parzen Estimator with Inequality Constraints for Expensive Hyperparameter Optimization. (arXiv:2211.14411v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.14411
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#32422;&#26463;TPE&#65288;c-TPE&#65289;&#26041;&#27861;&#65292;&#26159;&#26641;&#24418;Parzen&#20272;&#35745;&#22120;&#65288;TPE&#65289;&#30340;&#25193;&#23637;&#65292;&#21487;&#26377;&#25928;&#22788;&#29702;&#22312;&#24615;&#33021;&#35201;&#27714;&#20043;&#19978;&#26045;&#21152;&#30340;&#32422;&#26463;&#38480;&#21046;&#65292;&#23454;&#39564;&#35777;&#26126;&#22312;81&#20010;&#26114;&#36149;&#30340;HPO&#35774;&#32622;&#20013;&#34920;&#29616;&#20986;&#26368;&#20339;&#24615;&#33021;&#25490;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36229;&#21442;&#25968;&#20248;&#21270;&#65288;HPO&#65289;&#23545;&#20110;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#30340;&#24378;&#22823;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#65292;&#23454;&#38469;&#24212;&#29992;&#36890;&#24120;&#20250;&#22312;&#24615;&#33021;&#35201;&#27714;&#20043;&#19978;&#26045;&#21152;&#19968;&#20123;&#38480;&#21046;&#65292;&#20363;&#22914;&#20869;&#23384;&#20351;&#29992;&#25110;&#24310;&#36831;&#31561;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#32422;&#26463;TPE&#65288;c-TPE&#65289;&#65292;&#36825;&#26159;&#24191;&#27867;&#20351;&#29992;&#30340;&#22810;&#21151;&#33021;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#8212;&#8212;&#26641;&#24418;Parzen&#20272;&#35745;&#22120;&#65288;TPE&#65289;&#30340;&#25193;&#23637;&#65292;&#20197;&#22788;&#29702;&#36825;&#20123;&#32422;&#26463;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#25193;&#23637;&#19981;&#20165;&#26159;&#31616;&#21333;&#22320;&#23558;&#29616;&#26377;&#25910;&#30410;&#20989;&#25968;&#21644;&#21407;&#22987;TPE&#32452;&#21512;&#36215;&#26469;&#65292;&#32780;&#26159;&#21253;&#25324;&#20462;&#25913;&#26469;&#35299;&#20915;&#23548;&#33268;&#24615;&#33021;&#19981;&#20339;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#20174;&#32463;&#39564;&#21644;&#29702;&#35770;&#19978;&#28145;&#20837;&#20998;&#26512;&#36825;&#20123;&#20462;&#25913;&#65292;&#25552;&#20379;&#20102;&#26377;&#20851;&#23427;&#20204;&#22914;&#20309;&#26377;&#25928;&#22320;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#30340;&#35265;&#35299;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;c-TPE&#22312;81&#20010;&#26114;&#36149;&#30340;HPO&#35774;&#32622;&#20013;&#34920;&#29616;&#20986;&#26368;&#20339;&#30340;&#24179;&#22343;&#25490;&#21517;&#24615;&#33021;&#65292;&#20855;&#26377;&#32479;&#35745;&#26174;&#30528;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hyperparameter optimization (HPO) is crucial for strong performance of deep learning algorithms and real-world applications often impose some constraints, such as memory usage, or latency on top of the performance requirement. In this work, we propose constrained TPE (c-TPE), an extension of the widely-used versatile Bayesian optimization method, tree-structured Parzen estimator (TPE), to handle these constraints. Our proposed extension goes beyond a simple combination of an existing acquisition function and the original TPE, and instead includes modifications that address issues that cause poor performance. We thoroughly analyze these modifications both empirically and theoretically, providing insights into how they effectively overcome these challenges. In the experiments, we demonstrate that c-TPE exhibits the best average rank performance among existing methods with statistical significance on 81 expensive HPO settings.
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#20351;&#29992;MCFS&#31639;&#27861;&#21512;&#25104;UNSAT&#27714;&#35299;&#22120;&#30340;&#26041;&#27861;&#65292;&#31639;&#27861;&#21487;&#29992;&#20110;&#35299;&#20915;&#21253;&#25324;SAT&#20844;&#24335;&#19981;&#21487;&#28385;&#36275;&#24615;&#35777;&#26126;&#12289;&#21487;&#28385;&#36275;SAT&#20844;&#24335;&#35299;&#30340;&#25968;&#37327;&#35745;&#25968;&#21644;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#30340;&#26368;&#20248;&#35299;&#38382;&#39064;&#65292;&#24182;&#21033;&#29992;&#21512;&#25104;&#26862;&#26519;&#26500;&#24314;&#31639;&#27861;&#21644;&#21512;&#25104;MDP&#31867;&#26469;&#36991;&#20813;&#26500;&#24314;&#20505;&#36873;&#26641;&#26862;&#26519;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2211.12581</link><description>&lt;p&gt;
&#36890;&#36807;Monte Carlo Forest Search&#23454;&#29616;UNSAT&#27714;&#35299;&#22120;&#30340;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
UNSAT Solver Synthesis via Monte Carlo Forest Search. (arXiv:2211.12581v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.12581
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#20351;&#29992;MCFS&#31639;&#27861;&#21512;&#25104;UNSAT&#27714;&#35299;&#22120;&#30340;&#26041;&#27861;&#65292;&#31639;&#27861;&#21487;&#29992;&#20110;&#35299;&#20915;&#21253;&#25324;SAT&#20844;&#24335;&#19981;&#21487;&#28385;&#36275;&#24615;&#35777;&#26126;&#12289;&#21487;&#28385;&#36275;SAT&#20844;&#24335;&#35299;&#30340;&#25968;&#37327;&#35745;&#25968;&#21644;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#30340;&#26368;&#20248;&#35299;&#38382;&#39064;&#65292;&#24182;&#21033;&#29992;&#21512;&#25104;&#26862;&#26519;&#26500;&#24314;&#31639;&#27861;&#21644;&#21512;&#25104;MDP&#31867;&#26469;&#36991;&#20813;&#26500;&#24314;&#20505;&#36873;&#26641;&#26862;&#26519;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;Monte Carlo Forest Search&#65288;MCFS&#65289;&#65292;&#19968;&#31867;&#29992;&#20110;&#23398;&#20064;&#20915;&#31574;&#26641;MDP&#31574;&#30053;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#31639;&#27861;&#12290;&#36825;&#20123;&#38382;&#39064;&#30340;&#31034;&#20363;&#21253;&#25324;&#35777;&#26126;SAT&#20844;&#24335;&#30340;&#19981;&#21487;&#28385;&#36275;&#24615;&#65307;&#35745;&#31639;&#21487;&#28385;&#36275;&#30340;SAT&#20844;&#24335;&#30340;&#35299;&#30340;&#25968;&#37327;&#65307;&#20197;&#21450;&#25214;&#21040;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#30340;&#26368;&#20248;&#35299;&#12290;MCFS&#31639;&#27861;&#21487;&#20197;&#30475;&#20316;&#26159;Monte Carlo Tree Search&#65288;MCTS&#65289;&#30340;&#25193;&#23637;&#65292;&#29992;&#20110;&#22312;&#20505;&#36873;&#26641;&#30340;&#26862;&#26519;&#20013;&#23547;&#25214;&#19968;&#20010;&#23567;&#26641;&#65292;&#32780;&#19981;&#26159;&#22312;&#26641;&#20013;&#25214;&#21040;&#19968;&#20010;&#22909;&#36335;&#24452;&#65288;&#35299;&#20915;&#26041;&#26696;&#65289;&#12290;&#25105;&#20204;&#22312;&#31639;&#27861;&#20013;&#23454;&#20363;&#21270;&#21644;&#35780;&#20272;&#20102;&#33258;&#24049;&#30340;&#24819;&#27861;&#65292;&#31216;&#20043;&#20026;Knuth Synthesis&#65292;&#36825;&#26159;&#19968;&#20010;MCFS&#31639;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;DPLL&#20998;&#25903;&#31574;&#30053;&#26469;&#35299;&#20915;&#24067;&#23572;&#21487;&#28385;&#36275;&#24615;&#65288;SAT&#65289;&#38382;&#39064;&#12290;&#36825;&#21033;&#29992;&#20102;&#20004;&#20010;&#20851;&#38190;&#24605;&#24819;&#65292;&#20197;&#36991;&#20813;&#26500;&#24314;&#20505;&#36873;&#26641;&#26862;&#26519;&#30340;&#38382;&#39064;&#65306;&#65288;1&#65289;&#19968;&#31181;&#21512;&#25104;&#26862;&#26519;&#26500;&#24314;&#31639;&#27861;&#65292;&#36890;&#36807;&#20174;&#27744;&#20013;&#38543;&#26426;&#36873;&#25321;&#8220;&#22909;&#8221;&#30340;&#26641;&#24182;&#23558;&#23427;&#20204;&#32452;&#21512;&#25104;&#26356;&#22823;&#30340;&#26862;&#26519;&#26469;&#36880;&#27493;&#26500;&#24314;&#26862;&#26519;&#65307;&#65288;2&#65289;&#19968;&#31181;&#21512;&#25104;MDP&#31867;&#65292;&#29992;&#20316;&#30495;&#23454;&#26641;MDP&#30340;&#20195;&#29702;&#65292;&#25105;&#20204;&#21487;&#20197;&#36731;&#26494;&#35745;&#31639;&#33410;&#28857;&#38388;&#36716;&#25442;&#30340;&#27010;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce Monte Carlo Forest Search (MCFS), a class of reinforcement learning (RL) algorithms for learning policies in {tree MDPs}, for which policy execution involves traversing an exponential-sized tree. Examples of such problems include proving unsatisfiability of a SAT formula; counting the number of solutions of a satisfiable SAT formula; and finding the optimal solution to a mixed-integer program. MCFS algorithms can be seen as extensions of Monte Carlo Tree Search (MCTS) to cases where, rather than finding a good path (solution) within a tree, the problem is to find a small tree within a forest of candidate trees. We instantiate and evaluate our ideas in an algorithm that we dub Knuth Synthesis, an MCFS algorithm that learns DPLL branching policies for solving the Boolean satisfiability (SAT) problem, with the objective of achieving good average-case performance on a given distribution of unsatisfiable problem instances. Knuth Synthesis leverages two key ideas to avoid the pr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22810;&#35270;&#35282;&#21387;&#32553;&#34920;&#31034;&#38477;&#20302;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#36807;&#31243;&#20013;&#36807;&#25311;&#21512;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#32463;&#36807;&#27979;&#35797;&#22312;&#20302;&#36164;&#28304;NLP&#20219;&#21153;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2211.08794</link><description>&lt;p&gt;
&#22810;&#35270;&#35282;&#21387;&#32553;&#34920;&#31034;&#30340;&#40065;&#26834;&#24615;&#20302;&#36164;&#28304;&#24494;&#35843;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards Robust Low-Resource Fine-Tuning with Multi-View Compressed Representations. (arXiv:2211.08794v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.08794
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22810;&#35270;&#35282;&#21387;&#32553;&#34920;&#31034;&#38477;&#20302;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#36807;&#31243;&#20013;&#36807;&#25311;&#21512;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#32463;&#36807;&#27979;&#35797;&#22312;&#20302;&#36164;&#28304;NLP&#20219;&#21153;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#21442;&#25968;&#30340;&#24040;&#22823;&#25968;&#37327;&#65292;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#30340;&#24494;&#35843;&#23481;&#26131;&#22312;&#20302;&#36164;&#28304;&#22330;&#26223;&#20013;&#20986;&#29616;&#36807;&#24230;&#25311;&#21512;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;PLM&#30340;&#38544;&#34255;&#34920;&#31034;&#19978;&#25805;&#20316;&#65292;&#20197;&#20943;&#23569;&#36807;&#25311;&#21512;&#12290;&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;PLM&#30340;&#38544;&#34255;&#23618;&#20043;&#38388;&#25554;&#20837;&#38543;&#26426;&#33258;&#32534;&#30721;&#22120;&#65292;&#23558;&#26469;&#33258;&#21069;&#19968;&#23618;&#30340;&#28608;&#27963;&#36716;&#25442;&#20026;&#22810;&#35270;&#35282;&#21387;&#32553;&#34920;&#31034;&#65292;&#28982;&#21518;&#23558;&#20854;&#39304;&#36865;&#21040;&#19978;&#23618;&#12290;&#24494;&#35843;&#32467;&#26463;&#21518;&#65292;&#33258;&#32534;&#30721;&#22120;&#20250;&#34987;&#31227;&#38500;&#25481;&#65292;&#22240;&#27492;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#19981;&#20250;&#22686;&#21152;&#39069;&#22806;&#30340;&#21442;&#25968;&#25110;&#35745;&#31639;&#25104;&#26412;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19968;&#31995;&#21015;&#24207;&#21015;&#21644;&#26631;&#35760;&#32423;&#21035;&#30340;&#20302;&#36164;&#28304;NLP&#20219;&#21153;&#20013;&#23637;&#29616;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to the huge amount of parameters, fine-tuning of pretrained language models (PLMs) is prone to overfitting in the low resource scenarios. In this work, we present a novel method that operates on the hidden representations of a PLM to reduce overfitting. During fine-tuning, our method inserts random autoencoders between the hidden layers of a PLM, which transform activations from the previous layers into a multi-view compressed representation before feeding it into the upper layers. The autoencoders are plugged out after fine-tuning, so our method does not add extra parameters or increase computation cost during inference. Our method demonstrates promising performance improvement across a wide range of sequence- and token-level low-resource NLP tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#20102;&#22312;&#35299;&#20915;&#35768;&#22810;&#22270;&#20687;&#20219;&#21153;(&#20363;&#22914;&#22270;&#20687;&#20998;&#31867;)&#26102;&#21487;&#20197;&#24573;&#30053;&#20559;&#32622;&#65292;&#24182;&#19988;&#38646;&#20559;&#32622;&#31070;&#32463;&#32593;&#32476;&#22312;&#23454;&#38469;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#21516;&#26102;&#20855;&#26377;&#26631;&#37327; (&#20056;&#27861;) &#19981;&#21464;&#24615;&#65292;&#20174;&#32780;&#22312;&#25913;&#21464;&#23545;&#27604;&#24230;&#26102;&#20173;&#33021;&#20445;&#25345;&#39044;&#27979;&#19981;&#21464;&#12290;</title><link>http://arxiv.org/abs/2211.08486</link><description>&lt;p&gt;
&#38646;&#20559;&#32622;&#26631;&#37327;&#19981;&#21464;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Scalar Invariant Networks with Zero Bias. (arXiv:2211.08486v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.08486
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;&#22312;&#35299;&#20915;&#35768;&#22810;&#22270;&#20687;&#20219;&#21153;(&#20363;&#22914;&#22270;&#20687;&#20998;&#31867;)&#26102;&#21487;&#20197;&#24573;&#30053;&#20559;&#32622;&#65292;&#24182;&#19988;&#38646;&#20559;&#32622;&#31070;&#32463;&#32593;&#32476;&#22312;&#23454;&#38469;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#21516;&#26102;&#20855;&#26377;&#26631;&#37327; (&#20056;&#27861;) &#19981;&#21464;&#24615;&#65292;&#20174;&#32780;&#22312;&#25913;&#21464;&#23545;&#27604;&#24230;&#26102;&#20173;&#33021;&#20445;&#25345;&#39044;&#27979;&#19981;&#21464;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;&#26435;&#37325;&#19968;&#26679;&#65292;&#20559;&#32622;&#39033;&#20063;&#26159;&#35768;&#22810;&#27969;&#34892;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;(&#21253;&#25324;&#31070;&#32463;&#32593;&#32476;)&#21487;&#23398;&#20064;&#30340;&#21442;&#25968;&#12290;&#20154;&#20204;&#35748;&#20026;&#20559;&#24046;&#33021;&#26377;&#25928;&#22320;&#22686;&#21152;&#31070;&#32463;&#32593;&#32476;&#34920;&#31034;&#33021;&#21147;&#26469;&#35299;&#20915;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#30340;&#21508;&#31181;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35748;&#20026;&#65292;&#22914;&#26524;&#25105;&#20204;&#20174;&#31532;&#19968;&#21407;&#29702;&#32771;&#34385;&#22270;&#20687;&#22312;&#36755;&#20837;&#31354;&#38388;&#20013;&#30340;&#20869;&#22312;&#20998;&#24067;&#20197;&#21450;&#27169;&#22411;&#24212;&#20855;&#26377;&#30340;&#19968;&#20123;&#26399;&#26395;&#29305;&#24615;&#65292;&#21017;&#20559;&#24046;&#21487;&#20197;&#23436;&#20840;&#24573;&#30053;&#65292;&#20197;&#35299;&#20915;&#35768;&#22810;&#19982;&#22270;&#20687;&#30456;&#20851;&#30340;&#20219;&#21153;&#65292;&#20363;&#22914;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#35266;&#23519;&#32467;&#26524;&#34920;&#26126;&#65292;&#38646;&#20559;&#32622;&#31070;&#32463;&#32593;&#32476;&#22312;&#23454;&#38469;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#19978;&#21487;&#33021;&#19982;&#24102;&#20559;&#32622;&#30340;&#31070;&#32463;&#32593;&#32476;&#34920;&#29616;&#30456;&#24403;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#38646;&#20559;&#32622;&#31070;&#32463;&#32593;&#32476;&#20855;&#26377;&#31216;&#20026;&#26631;&#37327;(&#20056;&#27861;)&#19981;&#21464;&#24615;&#30340;&#33391;&#22909;&#23646;&#24615;&#65292;&#36825;&#20351;&#24471;&#24403;&#25913;&#21464;&#36755;&#20837;&#22270;&#20687;&#30340;&#23545;&#27604;&#24230;&#26102;&#65292;&#31070;&#32463;&#32593;&#32476;&#30340;&#39044;&#27979;&#20445;&#25345;&#19981;&#21464;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#26631;&#37327;&#19981;&#21464;&#24615;&#25193;&#23637;&#21040;&#26356;&#19968;&#33324;&#30340;&#24773;&#20917;&#8230;
&lt;/p&gt;
&lt;p&gt;
Just like weights, bias terms are the learnable parameters of many popular machine learning models, including neural networks. Biases are believed to effectively increase the representational power of neural networks to solve a wide range of tasks in computer vision. However, we argue that if we consider the intrinsic distribution of images in the input space as well as some desired properties a model should have from the first principles, biases can be completely ignored in addressing many image-related tasks, such as image classification. Our observation indicates that zero-bias neural networks could perform comparably to neural networks with bias at least on practical image classification tasks. In addition, we prove that zero-bias neural networks possess a nice property called scalar (multiplication) invariance, which allows the prediction of neural networks remains the same when altering the contrast of the input image. We then extend scalar invariance to more general cases that a
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25506;&#32034;&#20102;&#25552;&#39640;&#31354;&#20013;&#33258;&#20027;&#25216;&#26415;&#36827;&#27493;&#30340;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#65306;&#22914;&#20309;&#29983;&#25104;&#22823;&#37327;&#30340;&#31354;&#20013;&#25968;&#25454;&#38598;&#20197;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#35813;&#30740;&#31350;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#24182;&#21033;&#29992;&#27169;&#25311;&#29615;&#22659;&#21644;&#25968;&#25454;&#22686;&#24378;&#26469;&#35299;&#20915;&#29616;&#26377;&#24037;&#20855;&#30340;&#23616;&#38480;&#24615;&#65292;&#25552;&#39640;&#25968;&#25454;&#29983;&#25104;&#24037;&#20316;&#27969;&#31243;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2211.05335</link><description>&lt;p&gt;
&#38761;&#26032;&#31354;&#20013;&#33258;&#20027;&#25216;&#26415;&#30340;&#21487;&#25193;&#23637;&#27169;&#22359;&#21270;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Scalable Modular Synthetic Data Generation for Advancing Aerial Autonomy. (arXiv:2211.05335v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.05335
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25506;&#32034;&#20102;&#25552;&#39640;&#31354;&#20013;&#33258;&#20027;&#25216;&#26415;&#36827;&#27493;&#30340;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#65306;&#22914;&#20309;&#29983;&#25104;&#22823;&#37327;&#30340;&#31354;&#20013;&#25968;&#25454;&#38598;&#20197;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#35813;&#30740;&#31350;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#24182;&#21033;&#29992;&#27169;&#25311;&#29615;&#22659;&#21644;&#25968;&#25454;&#22686;&#24378;&#26469;&#35299;&#20915;&#29616;&#26377;&#24037;&#20855;&#30340;&#23616;&#38480;&#24615;&#65292;&#25552;&#39640;&#25968;&#25454;&#29983;&#25104;&#24037;&#20316;&#27969;&#31243;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#21160;&#31354;&#20013;&#33258;&#20027;&#25216;&#26415;&#36827;&#27493;&#30340;&#19968;&#20010;&#37325;&#35201;&#38556;&#30861;&#26159;&#33719;&#21462;&#29992;&#20110;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#22823;&#35268;&#27169;&#31354;&#20013;&#25968;&#25454;&#38598;&#12290;&#20026;&#20102;&#36991;&#20813;&#23454;&#26426;&#25968;&#25454;&#37319;&#38598;&#30340;&#39640;&#25104;&#26412;&#21644;&#32791;&#26102;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#26080;&#20154;&#26426;&#24212;&#29992;&#24320;&#22987;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#26469;&#35757;&#32451;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#35201;&#24819;&#25552;&#39640;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#24182;&#23558;&#20854;&#36801;&#31227;&#21040;&#23454;&#38469;&#29615;&#22659;&#65292;&#22686;&#21152;&#27169;&#25311;&#29615;&#22659;&#30340;&#22810;&#26679;&#24615;&#20197;&#35757;&#32451;&#25152;&#26377;&#21487;&#33021;&#24773;&#20917;&#19979;&#30340;&#27169;&#22411;&#65292;&#20197;&#21450;&#22686;&#21152;&#35757;&#32451;&#25968;&#25454;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#65292;&#37117;&#26159;&#24517;&#19981;&#21487;&#23569;&#30340;&#12290;&#30446;&#21069;&#65292;&#29616;&#26377;&#30340;&#21512;&#25104;&#31354;&#20013;&#25968;&#25454;&#29983;&#25104;&#24037;&#20855;&#35201;&#20040;&#32570;&#20047;&#25968;&#25454;&#22686;&#24378;&#65292;&#35201;&#20040;&#20381;&#36182;&#20110;&#25163;&#24037;&#36127;&#36733;&#25110;&#23454;&#38469;&#26679;&#26412;&#36827;&#34892;&#37197;&#32622;&#21644;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#30495;&#23454;&#27169;&#25311;&#22330;&#26223;&#20197;&#36827;&#34892;&#25968;&#25454;&#37319;&#38598;&#12290;&#36825;&#20123;&#20381;&#36182;&#24615;&#38480;&#21046;&#20102;&#25968;&#25454;&#29983;&#25104;&#24037;&#20316;&#27969;&#31243;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;&#22240;&#27492;&#65292;&#22312;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#20013;&#24179;&#34913;&#27867;&#21270;&#33021;&#21147;&#21644;&#21487;&#25193;&#23637;&#24615;&#26159;&#19968;&#20010;&#20027;&#35201;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
One major barrier to advancing aerial autonomy has been collecting large-scale aerial datasets for training machine learning models. Due to costly and time-consuming real-world data collection through deploying drones, there has been an increasing shift towards using synthetic data for training models in drone applications. However, to increase widespread generalization and transferring models to real-world, increasing the diversity of simulation environments to train a model over all the varieties and augmenting the training data, has been proved to be essential. Current synthetic aerial data generation tools either lack data augmentation or rely heavily on manual workload or real samples for configuring and generating diverse realistic simulation scenes for data collection. These dependencies limit scalability of the data generation workflow. Accordingly, there is a major challenge in balancing generalizability and scalability in synthetic data generation. To address these gaps, we i
&lt;/p&gt;</description></item><item><title>RQUGE&#26159;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;&#26041;&#27861;&#65292;&#36890;&#36807;&#20505;&#36873;&#38382;&#39064;&#26159;&#21542;&#21487;&#20197;&#22238;&#31572;&#26469;&#35780;&#20272;&#38382;&#39064;&#29983;&#25104;&#36136;&#37327;, &#27604;&#29616;&#26377;&#25351;&#26631;&#26356;&#21152;&#31283;&#20581;&#65292;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#20154;&#24037;&#25552;&#20379;&#21442;&#32771;&#38382;&#39064;&#30340;&#24773;&#20917;&#19979;&#20351;&#29992;&#12290;</title><link>http://arxiv.org/abs/2211.01482</link><description>&lt;p&gt;
RQUGE&#65306;&#19968;&#31181;&#22522;&#20110;&#22238;&#31572;&#38382;&#39064;&#35780;&#20272;&#38382;&#39064;&#29983;&#25104;&#30340;&#26080;&#21442;&#32771;&#24230;&#37327;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
RQUGE: Reference-Free Metric for Evaluating Question Generation by Answering the Question. (arXiv:2211.01482v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.01482
&lt;/p&gt;
&lt;p&gt;
RQUGE&#26159;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;&#26041;&#27861;&#65292;&#36890;&#36807;&#20505;&#36873;&#38382;&#39064;&#26159;&#21542;&#21487;&#20197;&#22238;&#31572;&#26469;&#35780;&#20272;&#38382;&#39064;&#29983;&#25104;&#36136;&#37327;, &#27604;&#29616;&#26377;&#25351;&#26631;&#26356;&#21152;&#31283;&#20581;&#65292;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#20154;&#24037;&#25552;&#20379;&#21442;&#32771;&#38382;&#39064;&#30340;&#24773;&#20917;&#19979;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#35780;&#20272;&#33258;&#21160;&#29983;&#25104;&#38382;&#39064;&#36136;&#37327;&#30340;&#25351;&#26631;&#65288;&#22914;BLEU&#12289;ROUGE&#12289;BERTScore&#21644;BLEURT&#65289;&#23558;&#21442;&#32771;&#21644;&#39044;&#27979;&#38382;&#39064;&#36827;&#34892;&#27604;&#36739;&#65292;&#24403;&#20505;&#36873;&#38382;&#39064;&#21644;&#21442;&#32771;&#38382;&#39064;&#20043;&#38388;&#23384;&#22312;&#30456;&#24403;&#30340;&#35789;&#27719;&#37325;&#21472;&#25110;&#35821;&#20041;&#30456;&#20284;&#24615;&#26102;&#65292;&#25552;&#20379;&#39640;&#20998;&#12290;&#35813;&#26041;&#27861;&#23384;&#22312;&#20004;&#20010;&#20027;&#35201;&#32570;&#28857;&#65306;&#39318;&#20808;&#65292;&#25105;&#20204;&#38656;&#35201;&#26114;&#36149;&#30340;&#20154;&#24037;&#25552;&#20379;&#21442;&#32771;&#38382;&#39064;&#65307;&#20854;&#27425;&#65292;&#23427;&#24809;&#32602;&#37027;&#20123;&#21487;&#33021;&#19982;&#21442;&#32771;&#38382;&#39064;&#27809;&#26377;&#39640;&#35789;&#27719;&#25110;&#35821;&#20041;&#30456;&#20284;&#24615;&#30340;&#26377;&#25928;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;RQUGE&#65292;&#22522;&#20110;&#32473;&#23450;&#19978;&#19979;&#25991;&#30340;&#20505;&#36873;&#38382;&#39064;&#30340;&#21487;&#22238;&#31572;&#24615;&#12290;&#35813;&#24230;&#37327;&#26631;&#20934;&#30001;&#19968;&#20010;&#38382;&#31572;&#27169;&#22359;&#21644;&#19968;&#20010;&#36328;&#24230;&#35780;&#20998;&#22120;&#27169;&#22359;&#32452;&#25104;&#65292;&#20351;&#29992;&#29616;&#26377;&#25991;&#29486;&#20013;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#22240;&#27492;&#21487;&#20197;&#22312;&#19981;&#36827;&#34892;&#36827;&#19968;&#27493;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#20351;&#29992;&#12290;&#25105;&#20204;&#35777;&#26126;RQUGE&#19982;&#20154;&#31867;&#21028;&#26029;&#20855;&#26377;&#26356;&#39640;&#30340;&#30456;&#20851;&#24615;&#65292;&#32780;&#19981;&#20381;&#36182;&#20110;&#21442;&#32771;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;RQUGE&#26174;&#31034;&#26356;&#21152;&#31283;&#20581;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing metrics for evaluating the quality of automatically generated questions such as BLEU, ROUGE, BERTScore, and BLEURT compare the reference and predicted questions, providing a high score when there is a considerable lexical overlap or semantic similarity between the candidate and the reference questions. This approach has two major shortcomings. First, we need expensive human-provided reference questions. Second, it penalises valid questions that may not have high lexical or semantic similarity to the reference questions. In this paper, we propose a new metric, RQUGE, based on the answerability of the candidate question given the context. The metric consists of a question-answering and a span scorer modules, using pre-trained models from existing literature, thus it can be used without any further training. We demonstrate that RQUGE has a higher correlation with human judgment without relying on the reference question. Additionally, RQUGE is shown to be more robust to several ad
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#24120;&#35782;&#25512;&#29702;&#25968;&#25454;&#38598;JECC&#65292;&#22522;&#20110;&#20154;&#31867;&#20114;&#21160;&#23567;&#35828;&#28216;&#25103;&#30340;&#28436;&#31034;&#27493;&#39588;&#12290;&#19982;&#29616;&#26377;&#22522;&#20934;&#19981;&#21516;&#30340;&#26159;&#65292;&#35813;&#25968;&#25454;&#38598;&#35780;&#20272;&#30340;&#26159;&#21151;&#33021;&#24615;&#30340;&#24120;&#35782;&#30693;&#35782;&#35268;&#21017;&#12290;&#22240;&#27492;&#65292;&#20026;&#20102;&#22312;&#36825;&#20123;&#20219;&#21153;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#27169;&#22411;&#38656;&#35201;&#21033;&#29992;&#36825;&#31181;&#24120;&#35782;&#30693;&#35782;&#26469;&#25512;&#26029;&#34892;&#21160;&#30340;&#32467;&#26524;&#65292;&#32780;&#19981;&#26159;&#20165;&#20165;&#20381;&#36182;&#20110;&#35760;&#24518;&#20107;&#23454;&#12290;</title><link>http://arxiv.org/abs/2210.15456</link><description>&lt;p&gt;
JECC&#65306;&#20174;&#20114;&#21160;&#23567;&#35828;&#20013;&#25512;&#23548;&#20986;&#30340;&#24120;&#35782;&#25512;&#29702;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
JECC: Commonsense Reasoning Tasks Derived from Interactive Fictions. (arXiv:2210.15456v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.15456
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#24120;&#35782;&#25512;&#29702;&#25968;&#25454;&#38598;JECC&#65292;&#22522;&#20110;&#20154;&#31867;&#20114;&#21160;&#23567;&#35828;&#28216;&#25103;&#30340;&#28436;&#31034;&#27493;&#39588;&#12290;&#19982;&#29616;&#26377;&#22522;&#20934;&#19981;&#21516;&#30340;&#26159;&#65292;&#35813;&#25968;&#25454;&#38598;&#35780;&#20272;&#30340;&#26159;&#21151;&#33021;&#24615;&#30340;&#24120;&#35782;&#30693;&#35782;&#35268;&#21017;&#12290;&#22240;&#27492;&#65292;&#20026;&#20102;&#22312;&#36825;&#20123;&#20219;&#21153;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#27169;&#22411;&#38656;&#35201;&#21033;&#29992;&#36825;&#31181;&#24120;&#35782;&#30693;&#35782;&#26469;&#25512;&#26029;&#34892;&#21160;&#30340;&#32467;&#26524;&#65292;&#32780;&#19981;&#26159;&#20165;&#20165;&#20381;&#36182;&#20110;&#35760;&#24518;&#20107;&#23454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24120;&#35782;&#25512;&#29702;&#27169;&#25311;&#20102;&#20154;&#31867;&#23545;&#25105;&#20204;&#29289;&#29702;&#19990;&#30028;&#30340;&#25512;&#26029;&#33021;&#21147;&#65292;&#26159;&#26500;&#24314;&#36890;&#29992;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#22522;&#30707;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#24120;&#35782;&#25512;&#29702;&#25968;&#25454;&#38598;&#65292;&#22522;&#20110;&#20154;&#31867;&#20114;&#21160;&#23567;&#35828;&#28216;&#25103;&#30340;&#27493;&#39588;&#28436;&#31034;&#65292;&#22240;&#20026;&#20154;&#31867;&#29609;&#23478;&#23637;&#31034;&#20102;&#20016;&#23500;&#21644;&#22810;&#26679;&#30340;&#24120;&#35782;&#25512;&#29702;&#12290;&#35813;&#26032;&#25968;&#25454;&#38598;&#25552;&#20379;&#20102;&#21508;&#31181;&#25512;&#29702;&#31867;&#22411;&#30340;&#33258;&#28982;&#28151;&#21512;&#65292;&#24182;&#38656;&#35201;&#22810;&#36339;&#25512;&#29702;&#12290;&#27492;&#22806;&#65292;IF&#28216;&#25103;&#26500;&#24314;&#36807;&#31243;&#38656;&#35201;&#27604;&#20197;&#21069;&#30340;&#26041;&#27861;&#26356;&#23569;&#30340;&#20154;&#31867;&#24178;&#39044;&#12290;&#19982;&#29616;&#26377;&#30340;&#22522;&#20934;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#20391;&#37325;&#20110;&#35780;&#20272;&#21151;&#33021;&#24615;&#30340;&#24120;&#35782;&#30693;&#35782;&#35268;&#21017;&#65292;&#32780;&#19981;&#26159;&#20107;&#23454;&#30693;&#35782;&#12290;&#22240;&#27492;&#65292;&#20026;&#20102;&#22312;&#25105;&#20204;&#30340;&#20219;&#21153;&#19978;&#33719;&#24471;&#26356;&#39640;&#30340;&#24615;&#33021;&#65292;&#27169;&#22411;&#38656;&#35201;&#26377;&#25928;&#22320;&#21033;&#29992;&#36825;&#31181;&#21151;&#33021;&#24615;&#30693;&#35782;&#26469;&#25512;&#26029;&#34892;&#21160;&#30340;&#32467;&#26524;&#65292;&#32780;&#19981;&#26159;&#20165;&#20381;&#38752;&#35760;&#24518;&#20107;&#23454;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#24341;&#20837;&#30340;&#25968;&#25454;&#38598;&#23545;&#20110;&#20808;&#21069;&#30340;&#26426;&#22120;&#38405;&#35835;&#26469;&#35828;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Commonsense reasoning simulates the human ability to make presumptions about our physical world, and it is an essential cornerstone in building general AI systems. We propose a new commonsense reasoning dataset based on human's Interactive Fiction (IF) gameplay walkthroughs as human players demonstrate plentiful and diverse commonsense reasoning. The new dataset provides a natural mixture of various reasoning types and requires multi-hop reasoning. Moreover, the IF game-based construction procedure requires much less human interventions than previous ones. Different from existing benchmarks, our dataset focuses on the assessment of functional commonsense knowledge rules rather than factual knowledge. Hence, in order to achieve higher performance on our tasks, models need to effectively utilize such functional knowledge to infer the outcomes of actions, rather than relying solely on memorizing facts. Experiments show that the introduced dataset is challenging to previous machine reading
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;MARLlib&#65292;&#36825;&#26159;&#19968;&#20010;&#20840;&#38754;&#30340;MARL&#31639;&#27861;&#24211;&#65292;&#21487;&#32479;&#19968;&#25968;&#21313;&#31181;&#31639;&#27861;&#12290;&#23427;&#36824;&#36229;&#36234;&#20102;&#24403;&#21069;&#24037;&#20316;&#65292;&#38598;&#25104;&#20102;&#21508;&#31181;&#29615;&#22659;&#25509;&#21475;&#21644;&#25552;&#20379;&#28789;&#27963;&#30340;&#21442;&#25968;&#20849;&#20139;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2210.13708</link><description>&lt;p&gt;
MARLlib: &#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#24211;
&lt;/p&gt;
&lt;p&gt;
MARLlib: A Scalable Multi-agent Reinforcement Learning Library. (arXiv:2210.13708v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.13708
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;MARLlib&#65292;&#36825;&#26159;&#19968;&#20010;&#20840;&#38754;&#30340;MARL&#31639;&#27861;&#24211;&#65292;&#21487;&#32479;&#19968;&#25968;&#21313;&#31181;&#31639;&#27861;&#12290;&#23427;&#36824;&#36229;&#36234;&#20102;&#24403;&#21069;&#24037;&#20316;&#65292;&#38598;&#25104;&#20102;&#21508;&#31181;&#29615;&#22659;&#25509;&#21475;&#21644;&#25552;&#20379;&#28789;&#27963;&#30340;&#21442;&#25968;&#20849;&#20139;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#21644;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#24471;&#21040;&#20102;&#24555;&#36895;&#21457;&#23637;&#65292;&#20294;&#32570;&#20047;&#32479;&#19968;&#30340;&#35780;&#20272;&#24179;&#21488;&#21644;&#20844;&#35748;&#30340;&#22522;&#20934;&#23454;&#29616;&#12290;&#22240;&#27492;&#65292;&#36843;&#20999;&#38656;&#35201;&#24320;&#21457;&#19968;&#20010;&#38598;&#25104;&#24211;&#22871;&#20214;&#65292;&#20197;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#25552;&#20379;&#21487;&#38752;&#30340;MARL&#23454;&#29616;&#21644;&#21487;&#22797;&#21046;&#30340;&#35780;&#20272;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;MARLlib&#65292;&#36825;&#26159;&#19968;&#20010;&#20840;&#38754;&#30340;MARL&#31639;&#27861;&#24211;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#26234;&#33021;&#20307;&#38382;&#39064;&#12290;MARLlib&#36890;&#36807;&#26032;&#39062;&#30340;&#22522;&#20110;&#20195;&#29702;&#30340;&#20998;&#24067;&#24335;&#25968;&#25454;&#27969;&#35774;&#35745;&#65292;&#22312;&#39640;&#24230;&#21487;&#32452;&#21512;&#30340;&#38598;&#25104;&#39118;&#26684;&#20013;&#32479;&#19968;&#20102;&#25968;&#21313;&#31181;&#31639;&#27861;&#12290;&#27492;&#22806;&#65292;MARLlib&#36890;&#36807;&#38598;&#25104;&#21508;&#31181;&#29615;&#22659;&#25509;&#21475;&#21644;&#25552;&#20379;&#28789;&#27963;&#30340;&#21442;&#25968;&#20849;&#20139;&#31574;&#30053;&#65292;&#36229;&#36234;&#20102;&#24403;&#21069;&#24037;&#20316;&#65307;&#36825;&#20801;&#35768;&#26368;&#32456;&#29992;&#25143;&#22312;&#26368;&#23567;&#30340;&#20195;&#30721;&#20462;&#25913;&#19979;&#23454;&#29616;&#21327;&#20316;&#12289;&#31454;&#20105;&#21644;&#28151;&#21512;&#20219;&#21153;&#30340;&#22810;&#31181;&#35299;&#20915;&#26041;&#26696;&#12290;&#26368;&#21518;&#65292;MARLlib&#25552;&#20379;&#26131;&#20110;&#20351;&#29992;&#30340;API&#21644;&#23436;&#20840;&#35299;&#32806;&#21512;&#30340;&#37197;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the fast development of multi-agent systems (MAS) and multi-agent reinforcement learning (MARL) algorithms, there is a lack of unified evaluation platforms and commonly-acknowledged baseline implementation. Therefore, an urgent need is to develop an integrated library suite that delivers reliable MARL implementation and replicable evaluation in various benchmarks. To fill such a research gap, in this paper, we propose MARLlib, a comprehensive MARL algorithm library for solving multi-agent problems. With a novel design of agent-level distributed dataflow, MARLlib manages to unify tens of algorithms in a highly composable integration style. Moreover, MARLlib goes beyond current work by integrating diverse environment interfaces and providing flexible parameter sharing strategies; this allows for versatile solutions to cooperative, competitive, and mixed tasks with minimal code modifications for end users. Finally, MARLlib provides easy-to-use APIs and a fully decoupled configurat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23454;&#20363;&#24863;&#30693;&#22270;&#20687;&#20462;&#22797;&#27169;&#22411;ImComplete&#65292;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#24187;&#35937;&#20986;&#19982;&#29615;&#22659;&#32972;&#26223;&#30456;&#21327;&#35843;&#30340;&#35270;&#35273;&#23454;&#20363;&#65292;&#25552;&#20379;&#20102;&#22522;&#20110;&#35821;&#20041;&#21644;&#32467;&#26500;&#30340;&#20687;&#32032;&#32423;&#25351;&#23548;&#12290;</title><link>http://arxiv.org/abs/2210.12350</link><description>&lt;p&gt;
&#23454;&#20363;&#24863;&#30693;&#22270;&#20687;&#20462;&#22797;
&lt;/p&gt;
&lt;p&gt;
Instance-Aware Image Completion. (arXiv:2210.12350v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.12350
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23454;&#20363;&#24863;&#30693;&#22270;&#20687;&#20462;&#22797;&#27169;&#22411;ImComplete&#65292;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#24187;&#35937;&#20986;&#19982;&#29615;&#22659;&#32972;&#26223;&#30456;&#21327;&#35843;&#30340;&#35270;&#35273;&#23454;&#20363;&#65292;&#25552;&#20379;&#20102;&#22522;&#20110;&#35821;&#20041;&#21644;&#32467;&#26500;&#30340;&#20687;&#32032;&#32423;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#20462;&#22797;&#26159;&#19968;&#39033;&#26088;&#22312;&#22635;&#34917;&#24102;&#26377;&#32570;&#22833;&#21306;&#22495;&#30340;&#22270;&#20687;&#30340;&#20219;&#21153;&#65292;&#20351;&#23427;&#20204;&#20855;&#26377;&#21512;&#29702;&#30340;&#20869;&#23481;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22270;&#20687;&#20462;&#22797;&#26041;&#27861;&#24448;&#24448;&#36890;&#36807;&#22635;&#20805;&#21608;&#22260;&#32441;&#29702;&#26469;&#22635;&#34917;&#32570;&#22833;&#21306;&#22495;&#65292;&#32780;&#19981;&#26159;&#21435;&#24187;&#35937;&#19968;&#20010;&#19982;&#29615;&#22659;&#32972;&#26223;&#30456;&#21327;&#35843;&#30340;&#35270;&#35273;&#23454;&#20363;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#20687;&#20462;&#22797;&#27169;&#22411;&#65292;&#21517;&#20026;ImComplete&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#24187;&#35937;&#32570;&#22833;&#30340;&#23454;&#20363;&#65292;&#20174;&#32780;&#19982;&#21407;&#22987;&#32972;&#26223;&#21327;&#35843;&#12290;ImComplete&#39318;&#20808;&#37319;&#29992;&#20102;&#19968;&#20010;&#21464;&#21387;&#22120;&#26550;&#26500;&#65292;&#32771;&#34385;&#21040;&#21487;&#35265;&#23454;&#20363;&#21644;&#32570;&#22833;&#21306;&#22495;&#30340;&#20301;&#32622;&#12290;&#28982;&#21518;&#65292;ImComplete&#23436;&#25104;&#20102;&#32570;&#22833;&#21306;&#22495;&#20869;&#30340;&#35821;&#20041;&#20998;&#21106;&#25513;&#27169;&#65292;&#25552;&#20379;&#20687;&#32032;&#32423;&#30340;&#35821;&#20041;&#21644;&#32467;&#26500;&#25351;&#23548;&#12290;&#26368;&#21518;&#65292;&#22270;&#20687;&#21512;&#25104;&#22359;&#29983;&#25104;&#20102;&#36924;&#30495;&#30340;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
Image completion is a task that aims to fill in the missing region of a masked image with plausible contents. However, existing image completion methods tend to fill in the missing region with the surrounding texture instead of hallucinating a visual instance that is suitable in accordance with the context of the scene. In this work, we propose a novel image completion model, dubbed ImComplete, that hallucinates the missing instance that harmonizes well with - and thus preserves - the original context. ImComplete first adopts a transformer architecture that considers the visible instances and the location of the missing region. Then, ImComplete completes the semantic segmentation masks within the missing region, providing pixel-level semantic and structural guidance. Finally, the image synthesis blocks generate photo-realistic content. We perform a comprehensive evaluation of the results in terms of visual quality (LPIPS and FID) and contextual preservation scores (CLIPscore and object
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#23454;&#20307;&#19982;&#25991;&#26412;&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65288;EnTDA&#65289;&#65292;&#21487;&#20026;&#21508;&#31181;&#25153;&#24179;&#12289;&#23884;&#22871;&#21644;&#19981;&#36830;&#32493; NER &#20219;&#21153;&#29983;&#25104;&#35821;&#20041;&#36830;&#36143;&#21644;&#20445;&#30041;&#23454;&#20307;&#30340;&#25991;&#26412;&#65292;&#24182;&#24341;&#20837;&#22810;&#26679;&#24615;&#26463;&#25628;&#32034;&#20197;&#22686;&#21152;&#25991;&#26412;&#29983;&#25104;&#36807;&#31243;&#20013;&#30340;&#22810;&#26679;&#24615;&#12290;</title><link>http://arxiv.org/abs/2210.10343</link><description>&lt;p&gt;
&#22522;&#20110;&#23454;&#20307;&#19982;&#25991;&#26412;&#30340;&#25968;&#25454;&#22686;&#24378;&#23545;&#21508;&#31181;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20219;&#21153;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Entity-to-Text based Data Augmentation for various Named Entity Recognition Tasks. (arXiv:2210.10343v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.10343
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#23454;&#20307;&#19982;&#25991;&#26412;&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65288;EnTDA&#65289;&#65292;&#21487;&#20026;&#21508;&#31181;&#25153;&#24179;&#12289;&#23884;&#22871;&#21644;&#19981;&#36830;&#32493; NER &#20219;&#21153;&#29983;&#25104;&#35821;&#20041;&#36830;&#36143;&#21644;&#20445;&#30041;&#23454;&#20307;&#30340;&#25991;&#26412;&#65292;&#24182;&#24341;&#20837;&#22810;&#26679;&#24615;&#26463;&#25628;&#32034;&#20197;&#22686;&#21152;&#25991;&#26412;&#29983;&#25104;&#36807;&#31243;&#20013;&#30340;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#34987;&#29992;&#20110;&#32531;&#35299;&#21508;&#31181; NER&#65288;&#25153;&#24179;&#12289;&#23884;&#22871;&#21644;&#19981;&#36830;&#32493;&#30340; NER&#65289;&#20219;&#21153;&#20013;&#26631;&#27880;&#25968;&#25454;&#31232;&#32570;&#30340;&#38382;&#39064;&#12290;&#29616;&#26377;&#30340;&#22686;&#24378;&#25216;&#26415;&#35201;&#20040;&#20462;&#25913;&#21407;&#22987;&#25991;&#26412;&#20013;&#30340;&#21333;&#35789;&#20174;&#32780;&#30772;&#22351;&#25991;&#26412;&#30340;&#35821;&#20041;&#36830;&#36143;&#24615;&#65292;&#35201;&#20040;&#20351;&#29992;&#29983;&#25104;&#27169;&#22411;&#32780;&#24573;&#30053;&#25658;&#24102;&#21407;&#22987;&#25991;&#26412;&#20013;&#30340;&#23454;&#20307;&#65292;&#36825;&#38459;&#30861;&#20102;&#22686;&#24191;&#25216;&#26415;&#22312;&#23884;&#22871;&#21644;&#19981;&#36830;&#32493;&#30340; NER &#20219;&#21153;&#19978;&#30340;&#20351;&#29992;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#23454;&#20307;&#19982;&#25991;&#26412;&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65288;EnTDA&#65289;&#65292;&#23545;&#21407;&#22987;&#25991;&#26412;&#20013;&#30340;&#23454;&#20307;&#21015;&#34920;&#36827;&#34892;&#28155;&#21152;&#12289;&#21024;&#38500;&#12289;&#26367;&#25442;&#25110;&#20132;&#25442;&#65292;&#37319;&#29992;&#36825;&#20123;&#22686;&#24191;&#21518;&#30340;&#23454;&#20307;&#21015;&#34920;&#20026;&#21508;&#31181; NER &#20219;&#21153;&#29983;&#25104;&#35821;&#20041;&#36830;&#36143;&#21644;&#20445;&#30041;&#23454;&#20307;&#30340;&#25991;&#26412;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#22810;&#26679;&#24615;&#26463;&#25628;&#32034;&#65292;&#20197;&#22686;&#21152;&#25991;&#26412;&#29983;&#25104;&#36807;&#31243;&#20013;&#30340;&#22810;&#26679;&#24615;&#12290;&#22312;&#19977;&#20010;&#20219;&#21153;&#65288;&#25153;&#24179;&#12289;&#23884;&#22871;&#21644;&#19981;&#36830;&#32493; NER &#20219;&#21153;&#65289;&#21644;&#20004;&#20010;&#35774;&#32622;&#65288;&#23436;&#25972;&#25968;&#25454;&#20197;&#21450;&#25968;&#25454;&#31232;&#32570;&#24773;&#20917;&#19979;&#65289;&#30340;&#21313;&#19977;&#20010; NER &#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data augmentation techniques have been used to alleviate the problem of scarce labeled data in various NER tasks (flat, nested, and discontinuous NER tasks). Existing augmentation techniques either manipulate the words in the original text that break the semantic coherence of the text, or exploit generative models that ignore preserving entities in the original text, which impedes the use of augmentation techniques on nested and discontinuous NER tasks. In this work, we propose a novel Entity-to-Text based data augmentation technique named EnTDA to add, delete, replace or swap entities in the entity list of the original texts, and adopt these augmented entity lists to generate semantically coherent and entity preserving texts for various NER tasks. Furthermore, we introduce a diversity beam search to increase the diversity during the text generation process. Experiments on thirteen NER datasets across three tasks (flat, nested, and discontinuous NER tasks) and two settings (full data a
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20869;&#22312;&#22870;&#21169;&#21305;&#37197;(IRF)&#26041;&#27861;&#65292;&#36890;&#36807;&#25216;&#33021;&#37492;&#21035;&#22120;&#21305;&#37197;&#20869;&#22312;&#21644;&#19979;&#28216;&#20219;&#21153;&#22870;&#21169;&#26469;&#30830;&#23450;&#26410;&#35265;&#20219;&#21153;&#30340;&#26368;&#20248;&#25216;&#33021;&#65292;&#25552;&#39640;&#20102;&#31995;&#32479;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2210.07426</link><description>&lt;p&gt;
&#22522;&#20110;&#25216;&#33021;&#30340;&#24378;&#21270;&#23398;&#20064;&#19982;&#20869;&#22312;&#22870;&#21169;&#21305;&#37197;
&lt;/p&gt;
&lt;p&gt;
Skill-Based Reinforcement Learning with Intrinsic Reward Matching. (arXiv:2210.07426v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.07426
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20869;&#22312;&#22870;&#21169;&#21305;&#37197;(IRF)&#26041;&#27861;&#65292;&#36890;&#36807;&#25216;&#33021;&#37492;&#21035;&#22120;&#21305;&#37197;&#20869;&#22312;&#21644;&#19979;&#28216;&#20219;&#21153;&#22870;&#21169;&#26469;&#30830;&#23450;&#26410;&#35265;&#20219;&#21153;&#30340;&#26368;&#20248;&#25216;&#33021;&#65292;&#25552;&#39640;&#20102;&#31995;&#32479;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#26080;&#30417;&#30563;&#25216;&#33021;&#25506;&#32034;&#24050;&#32463;&#23637;&#31034;&#20102;&#33258;&#20027;&#33719;&#21462;&#34892;&#20026;&#21407;&#35821;&#30340;&#28508;&#21147;&#65292;&#20294;&#26159;&#20219;&#21153;&#26080;&#20851;&#30340;&#25216;&#33021;&#39044;&#35757;&#32451;&#21644;&#19979;&#28216;&#30340;&#20219;&#21153;&#24863;&#30693;&#35843;&#20248;&#20043;&#38388;&#20173;&#23384;&#22312;&#24456;&#22823;&#30340;&#26041;&#27861;&#35770;&#24046;&#36317;&#12290;&#25105;&#20204;&#25552;&#20986;&#20869;&#22312;&#22870;&#21169;&#21305;&#37197;(IRF)&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#27169;&#22411;&#32452;&#20214; "&#25216;&#33021;&#37492;&#21035;&#22120;" &#32479;&#19968;&#36825;&#20004;&#20010;&#23398;&#20064;&#38454;&#27573;&#12290;&#20256;&#32479;&#26041;&#27861;&#22312;&#31574;&#30053;&#32423;&#21035;&#30452;&#25509;&#24494;&#35843;&#39044;&#35757;&#32451;&#20195;&#29702;&#65292;&#36890;&#24120;&#20381;&#36182;&#20110;&#26114;&#36149;&#30340;&#29615;&#22659;&#22238;&#25918;&#26469;&#32463;&#39564;&#24615;&#22320;&#30830;&#23450;&#26368;&#20248;&#25216;&#33021;&#12290;&#28982;&#32780;&#65292;&#20219;&#21153;&#26368;&#31616;&#26126;&#20294;&#23436;&#25972;&#30340;&#25551;&#36848;&#36890;&#24120;&#26159;&#22870;&#21169;&#20989;&#25968;&#26412;&#36523;&#65292;&#25216;&#33021;&#23398;&#20064;&#26041;&#27861;&#36890;&#36807;&#37492;&#21035;&#22120;&#23398;&#20064;&#19982;&#25216;&#33021;&#31574;&#30053;&#30456;&#23545;&#24212;&#30340;&#8220;&#20869;&#22312;&#8221;&#22870;&#21169;&#20989;&#25968;&#12290;&#25105;&#20204;&#24314;&#35758;&#21033;&#29992;&#25216;&#33021;&#37492;&#21035;&#22120;&#8220;&#21305;&#37197;&#8221;&#20869;&#22312;&#21644;&#19979;&#28216;&#20219;&#21153;&#22870;&#21169;&#65292;&#24182;&#30830;&#23450;&#26410;&#35265;&#20219;&#21153;&#30340;&#26368;&#20248;&#25216;&#33021;&#65292;&#20174;&#32780;&#25552;&#39640;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
While unsupervised skill discovery has shown promise in autonomously acquiring behavioral primitives, there is still a large methodological disconnect between task-agnostic skill pretraining and downstream, task-aware finetuning. We present Intrinsic Reward Matching (IRM), which unifies these two phases of learning via the $\textit{skill discriminator}$, a pretraining model component often discarded during finetuning. Conventional approaches finetune pretrained agents directly at the policy level, often relying on expensive environment rollouts to empirically determine the optimal skill. However, often the most concise yet complete description of a task is the reward function itself, and skill learning methods learn an $\textit{intrinsic}$ reward function via the discriminator that corresponds to the skill policy. We propose to leverage the skill discriminator to $\textit{match}$ the intrinsic and downstream task rewards and determine the optimal skill for an unseen task without enviro
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#25299;&#25169;&#24341;&#23548;&#30340;&#36816;&#21160;&#35268;&#21010;&#26041;&#27861;&#25193;&#23637;&#21040;&#22810;&#26426;&#22120;&#20154;&#39046;&#22495;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#39640;&#25928;&#22320;&#35268;&#21010;&#20855;&#26377;&#35768;&#22810;&#29421;&#31364;&#36890;&#36947;&#30340;&#22797;&#26434;&#29615;&#22659;&#20013;&#30340;&#36335;&#24452;&#65292;&#24182;&#25214;&#21040;&#27604;&#20854;&#20182;&#26041;&#27861;&#26356;&#39640;&#36136;&#37327;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2210.07141</link><description>&lt;p&gt;
&#24102;&#26377;&#25299;&#25169;&#25351;&#23548;&#30340;&#25317;&#25380;&#29615;&#22659;&#19979;&#21487;&#25193;&#23637;&#30340;&#22810;&#26426;&#22120;&#20154;&#36816;&#21160;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Scalable Multi-robot Motion Planning for Congested Environments With Topological Guidance. (arXiv:2210.07141v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.07141
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#25299;&#25169;&#24341;&#23548;&#30340;&#36816;&#21160;&#35268;&#21010;&#26041;&#27861;&#25193;&#23637;&#21040;&#22810;&#26426;&#22120;&#20154;&#39046;&#22495;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#39640;&#25928;&#22320;&#35268;&#21010;&#20855;&#26377;&#35768;&#22810;&#29421;&#31364;&#36890;&#36947;&#30340;&#22797;&#26434;&#29615;&#22659;&#20013;&#30340;&#36335;&#24452;&#65292;&#24182;&#25214;&#21040;&#27604;&#20854;&#20182;&#26041;&#27861;&#26356;&#39640;&#36136;&#37327;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26426;&#22120;&#20154;&#36816;&#21160;&#35268;&#21010;(MRMP)&#30340;&#38382;&#39064;&#22312;&#20110;&#20026;&#19968;&#32452;&#26426;&#22120;&#20154;&#22312;&#36830;&#32493;&#29366;&#24577;&#31354;&#38388;&#20013;&#25214;&#21040;&#26080;&#20914;&#31361;&#30340;&#36335;&#24452;&#12290;&#24403;&#26426;&#22120;&#20154;&#25968;&#37327;&#22686;&#21152;&#24182;&#19988;&#22312;&#20855;&#26377;&#29421;&#31364;&#36890;&#36947;&#32780;&#26426;&#22120;&#20154;&#24517;&#39035;&#36890;&#36807;&#30340;&#29615;&#22659;&#20013;&#65292;&#20363;&#22914;&#38656;&#35201;&#26426;&#22120;&#20154;&#20043;&#38388;&#21327;&#35843;&#30340;&#20179;&#24211;&#36890;&#36947;&#26102;&#65292;MRMP&#30340;&#38590;&#24230;&#20250;&#22686;&#21152;&#12290;&#22312;&#21333;&#26426;&#22120;&#20154;&#24773;&#20917;&#19979;&#65292;&#25299;&#25169;&#24341;&#23548;&#30340;&#36816;&#21160;&#35268;&#21010;&#26041;&#27861;&#24050;&#26174;&#31034;&#20986;&#22312;&#36825;&#20123;&#21463;&#38480;&#29615;&#22659;&#20013;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23558;&#29616;&#26377;&#30340;&#25299;&#25169;&#24341;&#23548;&#21333;&#26426;&#22120;&#20154;&#36816;&#21160;&#35268;&#21010;&#26041;&#27861;&#25193;&#23637;&#21040;&#22810;&#26426;&#22120;&#20154;&#39046;&#22495;&#65292;&#21033;&#29992;&#25299;&#25169;&#24341;&#23548;&#25552;&#20379;&#30340;&#25913;&#21892;&#25928;&#29575;&#12290;&#25105;&#20204;&#28436;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20855;&#26377;&#35768;&#22810;&#29421;&#31364;&#36890;&#36947;&#30340;&#22797;&#26434;&#29615;&#22659;&#20013;&#26377;&#25928;&#22320;&#35268;&#21010;&#36335;&#24452;&#65292;&#35268;&#27169;&#21487;&#20197;&#25193;&#23637;&#21040;&#27604;&#29616;&#26377;&#22312;&#27492;&#31867;&#38382;&#39064;&#20013;&#30340;&#26041;&#27861;&#22823;25&#20493;&#30340;&#26426;&#22120;&#20154;&#22242;&#38431;&#12290;&#36890;&#36807;&#21033;&#29992;&#29615;&#22659;&#30340;&#25299;&#25169;&#30693;&#35782;&#65292;&#25105;&#20204;&#20063;&#25214;&#21040;&#20102;&#27604;&#20854;&#20182;&#26041;&#27861;&#26356;&#39640;&#36136;&#37327;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-robot motion planning (MRMP) is the problem of finding collision-free paths for a set of robots in a continuous state space. The difficulty of MRMP increases with the number of robots and is exacerbated in environments with narrow passages that robots must pass through, like warehouse aisles where coordination between robots is required. In single-robot settings, topology-guided motion planning methods have shown improved performance in these constricted environments. In this work, we extend an existing topology-guided single-robot motion planning method to the multi-robot domain to leverage the improved efficiency provided by topological guidance. We demonstrate our method's ability to efficiently plan paths in complex environments with many narrow passages, scaling to robot teams of size up to 25 times larger than existing methods in this class of problems. By leveraging knowledge of the topology of the environment, we also find higher-quality solutions than other methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24230;&#37327;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20855;&#20307;&#24615;&#30340;&#26041;&#27861;&#65292;&#24182;&#35774;&#35745;&#20102;&#20004;&#31181;&#22522;&#20110;&#25552;&#31034;&#30340;&#26041;&#27861;&#65292;&#20197;&#25913;&#21892;&#27169;&#22411;&#20855;&#20307;&#24615;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#27169;&#22411;&#30340;&#20855;&#20307;&#24615;&#21487;&#20197;&#24471;&#21040;&#25913;&#21892;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#39069;&#22806;&#30340;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2210.05159</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#20855;&#20307;&#21270;&#21527;&#65311;&#22914;&#20309;&#23454;&#29616;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Language Models Be Specific? How?. (arXiv:2210.05159v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.05159
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24230;&#37327;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20855;&#20307;&#24615;&#30340;&#26041;&#27861;&#65292;&#24182;&#35774;&#35745;&#20102;&#20004;&#31181;&#22522;&#20110;&#25552;&#31034;&#30340;&#26041;&#27861;&#65292;&#20197;&#25913;&#21892;&#27169;&#22411;&#20855;&#20307;&#24615;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#27169;&#22411;&#30340;&#20855;&#20307;&#24615;&#21487;&#20197;&#24471;&#21040;&#25913;&#21892;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#39069;&#22806;&#30340;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#8220;&#20182;&#26159;&#19968;&#20010;&#20154;&#8221;&#12289;&#8220;&#24052;&#40654;&#20301;&#20110;&#22320;&#29699;&#19978;&#8221;&#12290;&#36825;&#20123;&#35821;&#21477;&#37117;&#26159;&#27491;&#30830;&#30340;&#65292;&#20294;&#27809;&#26377;&#20855;&#20307;&#24615;&#8212;&#8212;&#22240;&#20026;&#32570;&#20047;&#26126;&#30830;&#30340;&#20869;&#23481;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24230;&#37327;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#30340;&#20855;&#20307;&#24615;&#30340;&#26041;&#27861;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#26469;&#24314;&#31435;&#20855;&#20307;&#24615;&#27979;&#35797;&#30340;&#22522;&#20934;&#65292;&#36890;&#36807;&#24102;&#26377;&#25552;&#31034;&#30340;&#25513;&#30721;&#26631;&#35760;&#39044;&#27979;&#20219;&#21153;&#26469;&#23454;&#29616;&#12290;&#20363;&#22914;&#65292;&#32473;&#23450;&#8220;&#22810;&#20262;&#22810;&#20301;&#20110;[MASK]&#20013;&#8221;&#65292;&#25105;&#20204;&#24819;&#27979;&#35797;PLMs&#26159;&#21542;&#33021;&#26356;&#22909;&#22320;&#22635;&#20889;&#26356;&#20855;&#20307;&#30340;&#31572;&#26696;&#65292;&#20363;&#22914;&#23433;&#22823;&#30053;&#30465;&#32780;&#19981;&#26159;&#21152;&#25343;&#22823;&#12290;&#20174;&#25105;&#20204;&#30340;&#35780;&#20272;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#29616;&#26377;&#30340;PLMs&#21482;&#23545;&#26356;&#20855;&#20307;&#30340;&#31572;&#26696;&#30053;&#24494;&#26356;&#26377;&#20559;&#22909;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#24433;&#21709;&#20855;&#20307;&#24615;&#30340;&#28508;&#22312;&#22240;&#32032;&#65292;&#24182;&#35774;&#35745;&#20102;&#20004;&#31181;&#22522;&#20110;&#25552;&#31034;&#30340;&#26041;&#27861;&#20197;&#25913;&#21892;&#20855;&#20307;&#24615;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#27169;&#22411;&#30340;&#20855;&#20307;&#24615;&#21487;&#20197;&#24471;&#21040;&#25913;&#21892;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#39069;&#22806;&#30340;&#35757;&#32451;&#12290;&#25105;&#20204;&#24076;&#26395;&#36825;&#39033;&#24037;&#20316;&#33021;&#24102;&#26469;&#35821;&#35328;&#27169;&#22411;&#20855;&#20307;&#24615;&#30340;&#35748;&#35782;&#65292;&#24182;&#40723;&#21169;&#30456;&#20851;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
"He is a person", "Paris is located on the earth". Both statements are correct but meaningless - due to lack of specificity. In this paper, we propose to measure how specific the language of pre-trained language models (PLMs) is. To achieve this, we introduce a novel approach to build a benchmark for specificity testing by forming masked token prediction tasks with prompts. For instance, given "Toronto is located in [MASK].", we want to test whether a more specific answer will be better filled in by PLMs, e.g., Ontario instead of Canada. From our evaluations, we show that existing PLMs have only a slight preference for more specific answers. We identify underlying factors affecting the specificity and design two prompt-based methods to improve the specificity. Results show that the specificity of the models can be improved by the proposed methods without additional training. We hope this work can bring to awareness the notion of specificity of language models and encourage the research
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#23545;&#25239;&#36870;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#22797;&#26434;&#20219;&#21153;&#20013;&#23398;&#20064;&#21040;&#20855;&#26377;&#23618;&#27425;&#32467;&#26500;&#30340;&#26368;&#20248;&#31574;&#30053;&#65292;&#27604;&#29616;&#26377;&#30340;&#26041;&#27861;&#26356;&#21152;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2210.01969</link><description>&lt;p&gt;
&#20998;&#23618;&#23545;&#25239;&#36870;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Adversarial Inverse Reinforcement Learning. (arXiv:2210.01969v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.01969
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#23545;&#25239;&#36870;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#22797;&#26434;&#20219;&#21153;&#20013;&#23398;&#20064;&#21040;&#20855;&#26377;&#23618;&#27425;&#32467;&#26500;&#30340;&#26368;&#20248;&#31574;&#30053;&#65292;&#27604;&#29616;&#26377;&#30340;&#26041;&#27861;&#26356;&#21152;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#20223;&#23398;&#20064;&#65288;IL&#65289;&#19968;&#33324;&#29992;&#20110;&#20174;&#28436;&#31034;&#20013;&#24674;&#22797;&#19987;&#23478;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#39640;&#24230;&#22797;&#26434;&#30340;&#12289;&#38271;&#26102;&#31243;&#20219;&#21153;&#65292;&#24674;&#22797;&#21333;&#19968;&#25972;&#20307;&#31574;&#30053;&#26159;&#22256;&#38590;&#30340;&#65292;&#32780;&#19987;&#23478;&#31574;&#30053;&#36890;&#24120;&#21253;&#21547;&#23376;&#20219;&#21153;&#23618;&#27425;&#32467;&#26500;&#12290;&#22240;&#27492;&#65292;&#30740;&#31350;&#32773;&#24320;&#21457;&#20102;&#20998;&#23618;&#27169;&#20223;&#23398;&#20064;&#65288;HIL&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#36873;&#39033;&#26694;&#26550;&#20013;&#26174;&#24335;&#22320;&#24314;&#27169;&#20219;&#21153;&#20013;&#30340;&#27963;&#21160;&#32467;&#26500;&#26469;&#23398;&#20064;&#20998;&#23618;&#31574;&#30053;&#12290;&#29616;&#26377;&#30340;HIL&#26041;&#27861;&#35201;&#20040;&#24573;&#35270;&#20102;&#23376;&#20219;&#21153;&#32467;&#26500;&#19982;&#23398;&#20064;&#31574;&#30053;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#65292;&#35201;&#20040;&#26080;&#27861;&#21516;&#26102;&#22312;&#20998;&#23618;&#26694;&#26550;&#20013;&#23398;&#20064;&#39640;&#32423;&#21035;&#21644;&#20302;&#32423;&#21035;&#31574;&#30053;&#65292;&#23548;&#33268;&#20122;&#26368;&#20248;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;HIL&#31639;&#27861;&#8212;&#8212;&#20998;&#23618;&#23545;&#25239;&#36870;&#24378;&#21270;&#23398;&#20064;&#65288;H-AIRL&#65289;&#65292;&#23427;&#22312;&#26368;&#26032;&#30340;IL&#31639;&#27861;AIRL&#19978;&#25193;&#23637;&#20102;&#19968;&#27493;&#36873;&#39033;&#26694;&#26550;&#65292;&#37325;&#26032;&#23450;&#20041;&#20102;AIRL&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Imitation Learning (IL) has been proposed to recover the expert policy from demonstrations. However, it would be difficult to learn a single monolithic policy for highly-complex long-horizon tasks of which the expert policy usually contains subtask hierarchies. Therefore, Hierarchical Imitation Learning (HIL) has been developed to learn a hierarchical policy from expert demonstrations through explicitly modelling the activity structure in a task with the option framework. Existing HIL methods either overlook the causal relationship between the subtask structure and the learned policy, or fail to learn the high-level and low-level policy in the hierarchical framework in conjuncture, which leads to suboptimality. In this work, we propose a novel HIL algorithm -Hierarchical Adversarial Inverse Reinforcement Learning (H-AIRL), which extends a state-of-the-art (SOTA) IL algorithm -- AIRL, with the one-step option framework. Specifically, we redefine the AIRL objectives on the extended sta
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22810;&#23186;&#20307;&#29983;&#25104;&#24335;&#33050;&#26412;&#23398;&#20064;&#20219;&#21153;&#65292;&#26088;&#22312;&#36890;&#36807;&#36319;&#36394;&#25991;&#26412;&#21644;&#35270;&#35273;&#27169;&#24577;&#20013;&#30340;&#21382;&#21490;&#29366;&#24577;&#26469;&#29983;&#25104;&#21518;&#32493;&#27493;&#39588;&#65292;&#33021;&#23545;&#26410;&#35265;&#36807;&#30340;&#20219;&#21153;&#20855;&#26377;&#24402;&#32435;&#33021;&#21147;&#24182;&#20855;&#26377;&#22810;&#26679;&#24615;&#12290;</title><link>http://arxiv.org/abs/2208.12306</link><description>&lt;p&gt;
&#22810;&#23186;&#20307;&#29983;&#25104;&#24335;&#33050;&#26412;&#23398;&#20064;&#29992;&#20110;&#20219;&#21153;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Multimedia Generative Script Learning for Task Planning. (arXiv:2208.12306v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.12306
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22810;&#23186;&#20307;&#29983;&#25104;&#24335;&#33050;&#26412;&#23398;&#20064;&#20219;&#21153;&#65292;&#26088;&#22312;&#36890;&#36807;&#36319;&#36394;&#25991;&#26412;&#21644;&#35270;&#35273;&#27169;&#24577;&#20013;&#30340;&#21382;&#21490;&#29366;&#24577;&#26469;&#29983;&#25104;&#21518;&#32493;&#27493;&#39588;&#65292;&#33021;&#23545;&#26410;&#35265;&#36807;&#30340;&#20219;&#21153;&#20855;&#26377;&#24402;&#32435;&#33021;&#21147;&#24182;&#20855;&#26377;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#26631;&#23548;&#21521;&#30340;&#29983;&#25104;&#24335;&#33050;&#26412;&#23398;&#20064;&#26088;&#22312;&#22522;&#20110;&#30446;&#26631;&#29983;&#25104;&#21518;&#32493;&#27493;&#39588;&#65292;&#36825;&#26159;&#24110;&#21161;&#26426;&#22120;&#20154;&#25191;&#34892;&#26085;&#24120;&#29983;&#27963;&#20013;&#20856;&#22411;&#27963;&#21160;&#30340;&#37325;&#35201;&#20219;&#21153;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#26524;&#21382;&#21490;&#29366;&#24577;&#19981;&#20165;&#30001;&#32473;&#20154;&#30340;&#35821;&#35328;&#25351;&#31034;&#25429;&#33719;&#65292;&#32780;&#19988;&#36824;&#36890;&#36807;&#30456;&#20276;&#30340;&#22270;&#20687;&#25552;&#20379;&#20102;&#38468;&#21152;&#20449;&#24687;&#65292;&#37027;&#20040;&#27492;&#20219;&#21153;&#30340;&#34920;&#29616;&#21487;&#20197;&#25913;&#21892;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#20219;&#21153;&#65292;&#21363;&#22810;&#23186;&#20307;&#29983;&#25104;&#24335;&#33050;&#26412;&#23398;&#20064;&#65292;&#20197;&#36890;&#36807;&#36319;&#36394;&#25991;&#26412;&#21644;&#35270;&#35273;&#27169;&#24577;&#20013;&#30340;&#21382;&#21490;&#29366;&#24577;&#26469;&#29983;&#25104;&#21518;&#32493;&#27493;&#39588;&#65292;&#24182;&#25552;&#20379;&#20102;&#21253;&#21547;2,338&#20010;&#20219;&#21153;&#21644;31,496&#20010;&#27493;&#39588;&#21450;&#20854;&#25551;&#36848;&#24615;&#22270;&#20687;&#30340;&#31532;&#19968;&#20010;&#22522;&#20934;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#29983;&#25104;&#21487;&#35270;&#29366;&#24577;&#21487;&#36319;&#36394;&#30340;&#33050;&#26412;&#65292;&#23545;&#20110;&#26410;&#35265;&#36807;&#30340;&#20219;&#21153;&#20855;&#26377;&#24402;&#32435;&#33021;&#21147;&#65292;&#24182;&#19988;&#20854;&#27493;&#39588;&#20855;&#26377;&#22810;&#26679;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#22810;&#23186;&#20307;&#36873;&#25321;&#24615;&#32534;&#30721;&#22120;&#23545;&#35270;&#35273;&#29366;&#24577;&#21464;&#21270;&#36827;&#34892;&#32534;&#30721;&#65292;&#21033;&#29992;&#26816;&#32034;&#22686;&#24378;&#35299;&#30721;&#22120;&#20256;&#36882;&#20808;&#21069;&#35266;&#23519;&#21040;&#30340;&#20219;&#21153;&#30693;&#35782;&#65292;&#24182;&#36890;&#36807;&#38543;&#26426;&#25277;&#26679;&#21644;&#27874;&#26463;&#25628;&#32034;&#35299;&#30721;&#29983;&#25104;&#22810;&#26679;&#30340;&#27493;&#39588;&#12290;
&lt;/p&gt;
&lt;p&gt;
Goal-oriented generative script learning aims to generate subsequent steps based on a goal, which is an essential task to assist robots in performing stereotypical activities of daily life. We show that the performance of this task can be improved if historical states are not just captured by the linguistic instructions given to people, but are augmented with the additional information provided by accompanying images. Therefore, we propose a new task, Multimedia Generative Script Learning, to generate subsequent steps by tracking historical states in both text and vision modalities, as well as presenting the first benchmark containing 2,338 tasks and 31,496 steps with descriptive images. We aim to generate scripts that are visual-state trackable, inductive for unseen tasks, and diverse in their individual steps. We propose to encode visual state changes through a multimedia selective encoder, transferring knowledge from previously observed tasks using a retrieval-augmented decoder, and
&lt;/p&gt;</description></item><item><title>Fix-A-Step&#26159;&#19968;&#20010;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#31616;&#21270;&#27969;&#31243;&#65292;&#23558;&#25152;&#26377;&#26410;&#32463;&#31579;&#36873;&#30340;&#26080;&#26631;&#31614;&#22270;&#20687;&#35270;&#20026;&#28508;&#22312;&#26377;&#29992;&#30340;&#65307;&#22686;&#24378;&#26377;&#26631;&#31614;&#38598;&#30340;&#25968;&#25454;&#65292;&#20462;&#27491;&#26799;&#24230;&#19979;&#38477;&#26356;&#26032;&#26041;&#24335;&#21487;&#20197;&#20462;&#22797;&#35768;&#22810;&#24120;&#35265;&#30340;&#28145;&#24230; SSL &#26041;&#27861;&#65292;&#24182;&#22312;&#21307;&#23398;&#25104;&#20687;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2208.11870</link><description>&lt;p&gt;
Fix-A-Step: &#21322;&#30417;&#30563;&#23398;&#20064;&#22788;&#29702;&#26410;&#32463;&#31579;&#36873;&#30340;&#26080;&#26631;&#31614;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Fix-A-Step: Semi-supervised Learning from Uncurated Unlabeled Data. (arXiv:2208.11870v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.11870
&lt;/p&gt;
&lt;p&gt;
Fix-A-Step&#26159;&#19968;&#20010;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#31616;&#21270;&#27969;&#31243;&#65292;&#23558;&#25152;&#26377;&#26410;&#32463;&#31579;&#36873;&#30340;&#26080;&#26631;&#31614;&#22270;&#20687;&#35270;&#20026;&#28508;&#22312;&#26377;&#29992;&#30340;&#65307;&#22686;&#24378;&#26377;&#26631;&#31614;&#38598;&#30340;&#25968;&#25454;&#65292;&#20462;&#27491;&#26799;&#24230;&#19979;&#38477;&#26356;&#26032;&#26041;&#24335;&#21487;&#20197;&#20462;&#22797;&#35768;&#22810;&#24120;&#35265;&#30340;&#28145;&#24230; SSL &#26041;&#27861;&#65292;&#24182;&#22312;&#21307;&#23398;&#25104;&#20687;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21322;&#30417;&#30563;&#23398;&#20064; (SSL) &#22312;&#35757;&#32451;&#20998;&#31867;&#22120;&#26102;&#65292;&#36890;&#36807;&#22312;&#35768;&#22810;&#26080;&#26631;&#31614;&#22270;&#20687;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#25215;&#35834;&#27604;&#22312;&#23569;&#37327;&#26377;&#26631;&#31614;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#26102;&#26377;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#20363;&#22914;&#21307;&#23398;&#25104;&#20687;&#65292;&#20026;&#20102;&#36895;&#24230;&#32780;&#25910;&#38598;&#26410;&#32463;&#31579;&#36873;&#30340;&#26080;&#26631;&#31614;&#25968;&#25454;&#65292;&#22240;&#27492;&#21487;&#33021;&#19982;&#26377;&#26631;&#31614;&#38598;&#20013;&#30340;&#31867;&#21035;&#25110;&#29305;&#24449;&#19981;&#21516;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#29616;&#20195;&#28145;&#24230;&#21322;&#30417;&#30563;&#23398;&#20064;&#22312;&#22788;&#29702;&#26410;&#32463;&#31579;&#36873;&#30340;&#26080;&#26631;&#31614;&#25968;&#25454;&#26102;&#65292;&#24120;&#24120;&#20250;&#23548;&#33268;&#20934;&#30830;&#24615;&#19979;&#38477;&#12290;&#26368;&#36817;&#65292;&#19968;&#20123;&#36739;&#20026;&#22797;&#26434;&#30340;&#26041;&#27861;&#36890;&#36807;&#26816;&#27979;&#20998;&#24067;&#22806;&#30340;&#26080;&#26631;&#31614;&#22270;&#20687;&#65292;&#28982;&#21518;&#20002;&#24323;&#25110;&#38477;&#20302;&#23427;&#20204;&#30340;&#26435;&#37325;&#26469;&#20462;&#22797;&#36825;&#20010;&#38382;&#39064;&#12290;&#19982;&#27492;&#19981;&#21516;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102; Fix-A-Step&#65292;&#19968;&#31181;&#26356;&#31616;&#21333;&#30340;&#36807;&#31243;&#65292;&#23558;&#25152;&#26377;&#26410;&#32463;&#31579;&#36873;&#30340;&#26080;&#26631;&#31614;&#22270;&#20687;&#35270;&#20026;&#28508;&#22312;&#26377;&#29992;&#30340;&#12290;&#25105;&#20204;&#30340;&#31532;&#19968;&#20010;&#27934;&#35265;&#26159;&#65292;&#21363;&#20351;&#26410;&#32463;&#31579;&#36873;&#30340;&#22270;&#20687;&#20063;&#21487;&#20197;&#20135;&#29983;&#26377;&#29992;&#30340;&#24050;&#26631;&#35760;&#25968;&#25454;&#22686;&#24378;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#20462;&#25913;&#20102;&#26799;&#24230;&#19979;&#38477;&#26356;&#26032;&#30340;&#26041;&#24335;&#65292;&#20197;&#38450;&#27490;&#20248;&#21270;&#22810;&#20219;&#21153; SSL &#25439;&#22833;&#23545;&#26377;&#26631;&#31614;&#38598;&#20934;&#30830;&#24615;&#30340;&#25439;&#23475;&#12290;Fix-A-Step &#21487;&#20197;&#20462;&#22797;&#35768;&#22810;&#24120;&#35265;&#30340;&#28145;&#24230; SSL &#26041;&#27861;&#65292;&#22312; CIFAR &#22522;&#20934;&#27979;&#35797;&#20013;&#25552;&#39640;&#20102;&#25152;&#26377;&#27979;&#35797;&#26041;&#27861;&#21644;&#25968;&#25454;&#38598;&#30340;&#20934;&#30830;&#24615;&#12290;&#27492;&#22806;&#65292;&#22312;&#21307;&#23398;&#25104;&#20687;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35780;&#20272;&#26102;&#65292;Fix-A-Step &#22312;&#22788;&#29702;&#26410;&#32463;&#31579;&#36873;&#30340;&#26080;&#26631;&#31614;&#22270;&#20687;&#26041;&#38754;&#27604;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#35201;&#22909;&#24471;&#22810;&#12290;
&lt;/p&gt;
&lt;p&gt;
Semi-supervised learning (SSL) promises improved accuracy compared to training classifiers on small labeled datasets by also training on many unlabeled images. In real applications like medical imaging, unlabeled data will be collected for expediency and thus uncurated: possibly different from the labeled set in classes or features. Unfortunately, modern deep SSL often makes accuracy worse when given uncurated unlabeled data. Recent complex remedies try to detect out-of-distribution unlabeled images and then discard or downweight them. Instead, we introduce Fix-A-Step, a simpler procedure that views all uncurated unlabeled images as potentially helpful. Our first insight is that even uncurated images can yield useful augmentations of labeled data. Second, we modify gradient descent updates to prevent optimizing a multi-task SSL loss from hurting labeled-set accuracy. Fix-A-Step can repair many common deep SSL methods, improving accuracy on CIFAR benchmarks across all tested methods and
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21152;&#26435;&#22810;&#20852;&#36259;&#26816;&#32034;&#27169;&#22411;&#65288;Multi-Interest Preference&#65292;MIP&#65289;&#65292;&#36890;&#36807;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#20026;&#29992;&#25143;&#24314;&#31435;&#22810;&#20010;&#20852;&#36259;&#23884;&#20837;&#65292;&#24182;&#23558;&#29992;&#25143;&#22312;&#22810;&#20010;&#20852;&#36259;&#19978;&#30340;&#20559;&#22909;&#36827;&#34892;&#24314;&#27169;&#65292;&#20174;&#32780;&#25552;&#39640;&#20505;&#36873;&#26816;&#32034;&#32467;&#26524;&#30340;&#26597;&#20840;&#29575;&#12290;</title><link>http://arxiv.org/abs/2207.06652</link><description>&lt;p&gt;
&#27599;&#20010;&#20154;&#30340;&#20559;&#22909;&#21464;&#21270;&#19981;&#21516;&#65306;&#21152;&#26435;&#22810;&#20852;&#36259;&#26816;&#32034;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Everyone's Preference Changes Differently: Weighted Multi-Interest Retrieval Model. (arXiv:2207.06652v4 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.06652
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21152;&#26435;&#22810;&#20852;&#36259;&#26816;&#32034;&#27169;&#22411;&#65288;Multi-Interest Preference&#65292;MIP&#65289;&#65292;&#36890;&#36807;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#20026;&#29992;&#25143;&#24314;&#31435;&#22810;&#20010;&#20852;&#36259;&#23884;&#20837;&#65292;&#24182;&#23558;&#29992;&#25143;&#22312;&#22810;&#20010;&#20852;&#36259;&#19978;&#30340;&#20559;&#22909;&#36827;&#34892;&#24314;&#27169;&#65292;&#20174;&#32780;&#25552;&#39640;&#20505;&#36873;&#26816;&#32034;&#32467;&#26524;&#30340;&#26597;&#20840;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29992;&#25143;&#23884;&#20837;&#65288;&#29992;&#25143;&#30340;&#21521;&#37327;&#21270;&#34920;&#31034;&#65289;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#20855;&#26377;&#37325;&#35201;&#20316;&#29992;&#12290;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#26041;&#27861;&#26469;&#26500;&#24314;&#29992;&#25143;&#30340;&#22810;&#32500;&#24230;&#34920;&#31034;&#65292;&#20197;&#20415;&#20110;&#26816;&#32034;&#20219;&#21153;&#20013;&#25214;&#21040;&#30456;&#20284;&#30340;&#29289;&#21697;&#65292;&#24182;&#19988;&#24050;&#32463;&#22312;&#24037;&#19994;&#25512;&#33616;&#31995;&#32479;&#20013;&#34987;&#35777;&#26126;&#26159;&#26377;&#25928;&#30340;&#12290;&#26368;&#36817;&#20154;&#20204;&#21457;&#29616;&#20351;&#29992;&#22810;&#31181;&#23884;&#20837;&#65288;&#21363;&#22810;&#20010;&#32500;&#24230;&#30340;&#29992;&#25143;&#34920;&#31034;&#65289;&#26469;&#34920;&#31034;&#29992;&#25143;&#30340;&#20852;&#36259;&#26159;&#26377;&#29992;&#30340;&#65292;&#27599;&#20010;&#23884;&#20837;&#34920;&#31034;&#29992;&#25143;&#30340;&#26576;&#20010;&#20027;&#39064;&#20852;&#36259;&#12290;&#23545;&#20110;&#22810;&#20852;&#36259;&#34920;&#31034;&#65292;&#37325;&#35201;&#30340;&#26159;&#23545;&#29992;&#25143;&#22312;&#19981;&#21516;&#20027;&#39064;&#19978;&#30340;&#20559;&#22909;&#36827;&#34892;&#24314;&#27169;&#65292;&#24182;&#19988;&#20102;&#35299;&#20559;&#22909;&#38543;&#26102;&#38388;&#30340;&#21464;&#21270;&#24773;&#20917;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#35201;&#20040;&#26080;&#27861;&#20272;&#31639;&#29992;&#25143;&#23545;&#27599;&#20010;&#20852;&#36259;&#30340;&#22909;&#24863;&#24230;&#65292;&#35201;&#20040;&#19981;&#21512;&#29702;&#22320;&#20551;&#35774;&#27599;&#20010;&#29992;&#25143;&#23545;&#27599;&#20010;&#20852;&#36259;&#30340;&#20852;&#36259;&#24378;&#24230;&#20250;&#20197;&#30456;&#31561;&#30340;&#36895;&#29575;&#19979;&#38477;&#65292;&#20174;&#32780;&#38477;&#20302;&#20102;&#20505;&#36873;&#26816;&#32034;&#32467;&#26524;&#30340;&#26597;&#20840;&#29575;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#21152;&#26435;&#22810;&#20852;&#36259;&#26816;&#32034;&#27169;&#22411;&#65288;Multi-Interest Preference, MIP&#65289;&#65292;&#36890;&#36807;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#20026;&#29992;&#25143;&#20135;&#29983;&#22810;&#20010;&#20852;&#36259;&#23884;&#20837;&#65292;&#24182;&#19988;&#21487;&#20197;&#23545;&#29992;&#25143;&#22312;&#22810;&#31181;&#20852;&#36259;&#19979;&#30340;&#20559;&#22909;&#36827;&#34892;&#20272;&#35745;&#65292;&#20174;&#32780;&#25552;&#39640;&#20505;&#36873;&#26816;&#32034;&#32467;&#26524;&#30340;&#26597;&#20840;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
User embeddings (vectorized representations of a user) are essential in recommendation systems. Numerous approaches have been proposed to construct a representation for the user in order to find similar items for retrieval tasks, and they have been proven effective in industrial recommendation systems as well. Recently people have discovered the power of using multiple embeddings to represent a user, with the hope that each embedding represents the user's interest in a certain topic. With multi-interest representation, it's important to model the user's preference over the different topics and how the preference change with time. However, existing approaches either fail to estimate the user's affinity to each interest or unreasonably assume every interest of every user fades with an equal rate with time, thus hurting the recall of candidate retrieval. In this paper, we propose the Multi-Interest Preference (MIP) model, an approach that not only produces multi-interest for users by usin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#24402;&#32435;&#20559;&#35265;&#24378;&#21046;&#25191;&#34892;&#26126;&#30830;&#30340;&#20851;&#31995;&#32467;&#26500;&#65292;&#20174;&#32780;&#23558;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#36755;&#20986;&#34920;&#31034;&#26174;&#24335;&#22320;&#32452;&#25104;&#12290;&#35813;&#27169;&#22411;&#21487;&#20197;&#20174;&#38543;&#26426;&#26631;&#35760;&#24207;&#21015;&#25968;&#25454;&#38598;&#20013;&#21457;&#29616;&#38544;&#21547;&#30340;&#30495;&#23454;&#22270;&#65292;&#22312;&#33258;&#28982;&#35821;&#35328;&#25968;&#25454;&#38598;&#20013;&#25512;&#26029;&#20986;&#31526;&#21495;&#32593;&#32476;&#65288;&#27169;&#24335;&#65289;&#65292;&#30452;&#25509;&#21453;&#26144;&#20102;&#35821;&#35328;&#30340;&#22522;&#30784;&#21477;&#27861;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2207.03777</link><description>&lt;p&gt;
&#38544;&#34255;&#32467;&#26500;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Hidden Schema Networks. (arXiv:2207.03777v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.03777
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#24402;&#32435;&#20559;&#35265;&#24378;&#21046;&#25191;&#34892;&#26126;&#30830;&#30340;&#20851;&#31995;&#32467;&#26500;&#65292;&#20174;&#32780;&#23558;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#36755;&#20986;&#34920;&#31034;&#26174;&#24335;&#22320;&#32452;&#25104;&#12290;&#35813;&#27169;&#22411;&#21487;&#20197;&#20174;&#38543;&#26426;&#26631;&#35760;&#24207;&#21015;&#25968;&#25454;&#38598;&#20013;&#21457;&#29616;&#38544;&#21547;&#30340;&#30495;&#23454;&#22270;&#65292;&#22312;&#33258;&#28982;&#35821;&#35328;&#25968;&#25454;&#38598;&#20013;&#25512;&#26029;&#20986;&#31526;&#21495;&#32593;&#32476;&#65288;&#27169;&#24335;&#65289;&#65292;&#30452;&#25509;&#21453;&#26144;&#20102;&#35821;&#35328;&#30340;&#22522;&#30784;&#21477;&#27861;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#25512;&#26029;&#20986;&#24378;&#22823;&#30340;&#34920;&#31034;&#24418;&#24335;&#65292;&#20854;&#20013;&#34164;&#21547;&#30528;&#20016;&#23500;&#30340;&#35821;&#20041;&#21644;&#21477;&#27861;&#20869;&#23481;&#65292;&#23613;&#31649;&#26159;&#38544;&#21547;&#30340;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#24402;&#32435;&#20559;&#35265;&#24378;&#21046;&#25191;&#34892;&#26126;&#30830;&#30340;&#20851;&#31995;&#32467;&#26500;&#65292;&#20174;&#32780;&#23558;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#36755;&#20986;&#34920;&#31034;&#26174;&#24335;&#22320;&#32452;&#25104;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#35813;&#27169;&#22411;&#23558;&#21477;&#23376;&#32534;&#30721;&#20026;&#31526;&#21495;&#24207;&#21015;&#65288;&#32452;&#21512;&#34920;&#31034;&#65289;&#65292;&#36825;&#20123;&#31526;&#21495;&#23545;&#24212;&#20110;&#20840;&#23616;&#28508;&#22312;&#22270;&#19978;&#24102;&#20559;&#32622;&#30340;&#38543;&#26426;&#28216;&#36208;&#22120;&#35775;&#38382;&#30340;&#33410;&#28857;&#65292;&#24182;&#25512;&#26029;&#20854;&#21518;&#39564;&#20998;&#24067;&#12290;&#25105;&#20204;&#39318;&#20808;&#23637;&#31034;&#20102;&#35813;&#27169;&#22411;&#33021;&#22815;&#20174;&#20154;&#24037;&#29983;&#25104;&#30340;&#38543;&#26426;&#26631;&#35760;&#24207;&#21015;&#25968;&#25454;&#38598;&#20013;&#21457;&#29616;&#38544;&#21547;&#30340;&#30495;&#23454;&#22270;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;BERT&#21644;GPT-2&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#65292;&#20174;&#33258;&#28982;&#35821;&#35328;&#25968;&#25454;&#38598;&#20013;&#25512;&#26029;&#20986;&#31526;&#21495;&#32593;&#32476;&#65288;&#27169;&#24335;&#65289;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#65306;&#65288;i&#65289;&#25512;&#26029;&#20986;&#30340;&#31526;&#21495;&#21487;&#20197;&#35299;&#37322;&#20026;&#32534;&#30721;&#19981;&#21516;&#26041;&#38754;&#30340;&#21547;&#20041;&#65292;&#65288;ii&#65289;&#23427;&#20204;&#30340;&#22797;&#21512;&#24615;&#30452;&#25509;&#21453;&#26144;&#20102;&#35821;&#35328;&#30340;&#22522;&#30784;&#21477;&#27861;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large, pretrained language models infer powerful representations that encode rich semantic and syntactic content, albeit implicitly. In this work we introduce a novel neural language model that enforces, via inductive biases, explicit relational structures which allow for compositionality onto the output representations of pretrained language models. Specifically, the model encodes sentences into sequences of symbols (composed representations), which correspond to the nodes visited by biased random walkers on a global latent graph, and infers the posterior distribution of the latter. We first demonstrate that the model is able to uncover ground-truth graphs from artificially generated datasets of random token sequences. Next, we leverage pretrained BERT and GPT-2 language models as encoder and decoder, respectively, to infer networks of symbols (schemata) from natural language datasets. Our experiments show that (i) the inferred symbols can be interpreted as encoding different aspects 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#8220;&#21487;&#35299;&#37322;&#35774;&#35745;&#8221;&#26041;&#27861;&#35770;&#65292;&#38024;&#23545;&#31639;&#27861;&#20915;&#31574;&#30340;&#35299;&#37322;&#25552;&#20986;&#20102;&#19968;&#20010;&#32508;&#21512;&#24615;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#30001;&#19977;&#20010;&#38454;&#27573;&#32452;&#25104;&#65306;&#65288;A&#65289;&#35299;&#37322;&#38656;&#27714;&#20998;&#26512;&#65292;&#65288;B&#65289;&#35299;&#37322;&#25216;&#26415;&#35774;&#35745;&#65292;&#65288;C&#65289;&#35299;&#37322;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2206.06251</link><description>&lt;p&gt;
&#25903;&#25345;&#8220;&#21487;&#35299;&#37322;&#35774;&#35745;&#8221;&#30340;&#26041;&#27861;&#35770;&#21644;&#36719;&#20214;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
A Methodology and Software Architecture to Support Explainability-by-Design. (arXiv:2206.06251v2 [cs.SE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.06251
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#8220;&#21487;&#35299;&#37322;&#35774;&#35745;&#8221;&#26041;&#27861;&#35770;&#65292;&#38024;&#23545;&#31639;&#27861;&#20915;&#31574;&#30340;&#35299;&#37322;&#25552;&#20986;&#20102;&#19968;&#20010;&#32508;&#21512;&#24615;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#30001;&#19977;&#20010;&#38454;&#27573;&#32452;&#25104;&#65306;&#65288;A&#65289;&#35299;&#37322;&#38656;&#27714;&#20998;&#26512;&#65292;&#65288;B&#65289;&#35299;&#37322;&#25216;&#26415;&#35774;&#35745;&#65292;&#65288;C&#65289;&#35299;&#37322;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31639;&#27861;&#22312;&#25511;&#21046;&#25110;&#24433;&#21709;&#25105;&#20204;&#29983;&#27963;&#30340;&#35768;&#22810;&#25216;&#26415;&#31995;&#32479;&#20013;&#25198;&#28436;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#35282;&#33394;&#12290;&#22240;&#27492;&#65292;&#20026;&#20102;&#28385;&#36275;&#29992;&#25143;&#21644;&#32452;&#32455;&#30340;&#38656;&#27714;&#65292;&#26681;&#25454;&#27861;&#24459;&#12289;&#35268;&#23450;&#12289;&#36947;&#24503;&#20934;&#21017;&#20197;&#21450;&#20844;&#20247;&#30340;&#26399;&#26395;&#25552;&#20379;&#31639;&#27861;&#20915;&#31574;&#30340;&#35299;&#37322;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#27861;&#24459;&#21644;&#35268;&#23450;&#24182;&#26410;&#35268;&#23450;&#22914;&#20309;&#28385;&#36275;&#36825;&#20123;&#26399;&#26395;&#65292;&#22240;&#27492;&#32452;&#32455;&#36890;&#24120;&#38656;&#35201;&#33258;&#34892;&#21046;&#23450;&#35299;&#37322;&#26041;&#27861;&#65292;&#20174;&#32780;&#22686;&#21152;&#20102;&#21512;&#35268;&#25104;&#26412;&#21644;&#33391;&#22909;&#31649;&#29702;&#30340;&#25104;&#26412;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#8220;&#21487;&#35299;&#37322;&#35774;&#35745;&#8221;&#26041;&#27861;&#35770;&#65292;&#36825;&#26159;&#19968;&#31181;&#32508;&#21512;&#24615;&#26041;&#27861;&#65292;&#37319;&#29992;&#31215;&#26497;&#30340;&#25514;&#26045;&#65292;&#22312;&#20915;&#31574;&#31995;&#32479;&#30340;&#35774;&#35745;&#20013;&#21253;&#25324;&#35299;&#37322;&#33021;&#21147;&#12290;&#35813;&#26041;&#27861;&#35770;&#30001;&#19977;&#20010;&#38454;&#27573;&#32452;&#25104;&#65306;&#65288;A&#65289;&#35299;&#37322;&#38656;&#27714;&#20998;&#26512;&#65292;&#65288;B&#65289;&#35299;&#37322;&#25216;&#26415;&#35774;&#35745;&#65292;&#65288;C&#65289;&#35299;&#37322;&#39564;&#35777;&#12290;&#26412;&#25991;&#25551;&#36848;&#20102;&#31532;&#20108;&#38454;&#27573;&#65288;B&#65289;&#65292;&#21363;&#26681;&#25454;&#39046;&#22495;&#19987;&#23478;&#25552;&#20986;&#30340;&#38656;&#27714;&#23454;&#29616;&#35299;&#37322;&#33021;&#21147;&#30340;&#25216;&#26415;&#24037;&#20316;&#27969;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Algorithms play a crucial role in many technological systems that control or affect various aspects of our lives. As a result, providing explanations for their decisions to address the needs of users and organisations is increasingly expected by laws, regulations, codes of conduct, and the public. However, as laws and regulations do not prescribe how to meet such expectations, organisations are often left to devise their own approaches to explainability, inevitably increasing the cost of compliance and good governance. Hence, we envision Explainability-by-Design, a holistic methodology characterised by proactive measures to include explanation capability in the design of decision-making systems. The methodology consists of three phases: (A) Explanation Requirement Analysis, (B) Explanation Technical Design, and (C) Explanation Validation. This paper describes phase (B), a technical workflow to implement explanation capability from requirements elicited by domain experts for a specific 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#65292;&#21363;&#24211;&#23384;&#37325;&#26032;&#22635;&#20805;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861; Perishable DQN&#65292;&#26088;&#22312;&#26368;&#22823;&#21270;&#38144;&#21806;&#21516;&#26102;&#26368;&#23567;&#21270;&#28010;&#36153;&#65292;&#23454;&#29616;&#35745;&#31639;&#26426;&#24605;&#32500;&#23545;&#24223;&#29289;&#20943;&#37327;&#30340;&#25511;&#21046;&#12290;</title><link>http://arxiv.org/abs/2205.15455</link><description>&lt;p&gt;
&#24223;&#29289;&#20943;&#37327;&#30340;&#20223;&#30495;&#29615;&#22659;&#19982;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Simulation Environment and Reinforcement Learning Method for Waste Reduction. (arXiv:2205.15455v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.15455
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#65292;&#21363;&#24211;&#23384;&#37325;&#26032;&#22635;&#20805;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861; Perishable DQN&#65292;&#26088;&#22312;&#26368;&#22823;&#21270;&#38144;&#21806;&#21516;&#26102;&#26368;&#23567;&#21270;&#28010;&#36153;&#65292;&#23454;&#29616;&#35745;&#31639;&#26426;&#24605;&#32500;&#23545;&#24223;&#29289;&#20943;&#37327;&#30340;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#38646;&#21806;&#34892;&#19994;&#65288;&#20363;&#22914;&#65292;&#26434;&#36135;&#24215;&#12289;&#26381;&#35013;&#24215;&#12289;&#22312;&#32447;&#38646;&#21806;&#21830;&#65289;&#65292;&#24211;&#23384;&#31649;&#29702;&#32773;&#24517;&#39035;&#22312;&#30701;&#26399;&#39118;&#38505;&#65288;&#27809;&#26377;&#38144;&#21806;&#21830;&#21697;&#65289;&#21644;&#38271;&#26399;&#39118;&#38505;&#65288;&#36807;&#37327;&#35746;&#36135;&#23548;&#33268;&#20135;&#21697;&#28010;&#36153;&#65289;&#20043;&#38388;&#36827;&#34892;&#24179;&#34913;&#12290;&#30001;&#20110;&#32570;&#20047;&#26377;&#20851;&#26410;&#26469;&#23458;&#25143;&#36141;&#20080;&#30340;&#20449;&#24687;&#65292;&#36825;&#20010;&#24179;&#34913;&#20219;&#21153;&#23588;&#20854;&#22256;&#38590;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20174;&#20998;&#24067;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#22312;&#26102;&#38388;&#33539;&#22260;&#20869;&#37325;&#26032;&#22635;&#20805;&#26131;&#33104;&#29289;&#21697;&#30340;&#26434;&#36135;&#24215;&#24211;&#23384;&#38382;&#39064;&#12290;&#20854;&#30446;&#26631;&#26159;&#22312;&#19981;&#30830;&#23450;&#23454;&#38469;&#28040;&#36153;&#30340;&#24773;&#20917;&#19979;&#65292;&#26368;&#22823;&#21270;&#38144;&#21806;&#21516;&#26102;&#26368;&#23567;&#21270;&#28010;&#36153;&#12290;&#30001;&#20110;&#23545;&#39135;&#29289;&#30340;&#38656;&#27714;&#19981;&#26029;&#22686;&#38271;&#20197;&#21450;&#39135;&#29289;&#28010;&#36153;&#23545;&#29615;&#22659;&#12289;&#32463;&#27982;&#21644;&#36141;&#20080;&#21147;&#30340;&#24433;&#21709;&#65292;&#36825;&#20010;&#38382;&#39064;&#22312;&#20170;&#22825;&#20855;&#26377;&#26497;&#39640;&#30340;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#23558;&#24211;&#23384;&#37325;&#26032;&#22635;&#20805;&#20316;&#20026;&#19968;&#39033;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#65292;&#24182;&#22312;&#27492;&#22522;&#30784;&#19978;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#33104;&#36133;DQN&#8221;&#30340;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#21033;&#29992;&#38382;&#39064;&#30340;&#20998;&#24067;&#24335;&#20844;&#24335;&#26469;&#20272;&#31639;&#27599;&#20010;&#21160;&#20316;&#30340;&#39044;&#26399;&#22238;&#25253;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#21512;&#25104;&#30340;&#26434;&#36135;&#24215;&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#25991;&#29486;&#20013;&#30340;&#20960;&#20010;&#22522;&#20934;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
In retail (e.g., grocery stores, apparel shops, online retailers), inventory managers have to balance short-term risk (no items to sell) with long-term-risk (over ordering leading to product waste). This balancing task is made especially hard due to the lack of information about future customer purchases. In this paper, we study the problem of restocking a grocery store's inventory with perishable items over time, from a distributional point of view. The objective is to maximize sales while minimizing waste, with uncertainty about the actual consumption by costumers. This problem is of a high relevance today, given the growing demand for food and the impact of food waste on the environment, the economy, and purchasing power. We frame inventory restocking as a new reinforcement learning task that exhibits stochastic behavior conditioned on the agent's actions, making the environment partially observable. We make two main contributions. First, we introduce a new reinforcement learning en
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32858;&#21512;&#20102;&#26469;&#33258;&#20061;&#20010;&#29616;&#26377;&#25968;&#25454;&#38598;&#30340;&#20107;&#23454;&#38169;&#35823;&#27880;&#37322;&#65292;&#38024;&#23545;&#24213;&#23618;&#30340;&#25688;&#35201;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#20998;&#31867;&#65292;&#24182;&#27604;&#36739;&#20102;&#26368;&#20808;&#36827;&#30340;&#20107;&#23454;&#24230;&#37327;&#26631;&#20934;&#30340;&#24615;&#33021;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#24230;&#37327;&#26631;&#20934;&#30340;&#24615;&#33021;&#22240;&#19981;&#21516;&#30340;&#25688;&#35201;&#29983;&#25104;&#27169;&#22411;&#32780;&#26377;&#26174;&#33879;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2205.12854</link><description>&lt;p&gt;
&#29702;&#35299;&#24635;&#32467;&#20013;&#30340;&#20107;&#23454;&#38169;&#35823;&#65306;&#38169;&#35823;&#65292;&#25688;&#35201;&#29983;&#25104;&#22120;&#65292;&#25968;&#25454;&#38598;&#21644;&#38169;&#35823;&#26816;&#27979;&#22120;
&lt;/p&gt;
&lt;p&gt;
Understanding Factual Errors in Summarization: Errors, Summarizers, Datasets, Error Detectors. (arXiv:2205.12854v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.12854
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32858;&#21512;&#20102;&#26469;&#33258;&#20061;&#20010;&#29616;&#26377;&#25968;&#25454;&#38598;&#30340;&#20107;&#23454;&#38169;&#35823;&#27880;&#37322;&#65292;&#38024;&#23545;&#24213;&#23618;&#30340;&#25688;&#35201;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#20998;&#31867;&#65292;&#24182;&#27604;&#36739;&#20102;&#26368;&#20808;&#36827;&#30340;&#20107;&#23454;&#24230;&#37327;&#26631;&#20934;&#30340;&#24615;&#33021;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#24230;&#37327;&#26631;&#20934;&#30340;&#24615;&#33021;&#22240;&#19981;&#21516;&#30340;&#25688;&#35201;&#29983;&#25104;&#27169;&#22411;&#32780;&#26377;&#26174;&#33879;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25277;&#35937;&#25688;&#35201;&#29983;&#25104;&#27169;&#22411;&#21046;&#36896;&#20107;&#23454;&#38169;&#35823;&#30340;&#20542;&#21521;&#24050;&#32463;&#24471;&#21040;&#20102;&#24191;&#27867;&#30340;&#30740;&#31350;&#65292;&#21253;&#25324;&#35774;&#35745;&#29992;&#20110;&#26816;&#27979;&#20107;&#23454;&#38169;&#35823;&#30340;&#24230;&#37327;&#26631;&#20934;&#21644;&#27880;&#37322;&#24403;&#21069;&#31995;&#32479;&#36755;&#20986;&#20013;&#30340;&#38169;&#35823;&#12290;&#28982;&#32780;&#65292;&#24635;&#32467;&#31995;&#32479;&#12289;&#24230;&#37327;&#26631;&#20934;&#21644;&#27880;&#37322;&#22522;&#20934;&#30340;&#19981;&#26029;&#21457;&#23637;&#20351;&#24471;&#20107;&#23454;&#35780;&#20215;&#25104;&#20026;&#19968;&#20010;&#31227;&#21160;&#30340;&#30446;&#26631;&#65292;&#24182;&#19988;&#22312;&#24230;&#37327;&#26631;&#20934;&#20043;&#38388;&#36827;&#34892;&#28165;&#26224;&#30340;&#27604;&#36739;&#21464;&#24471;&#36234;&#26469;&#36234;&#22256;&#38590;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#32858;&#21512;&#20102;&#26469;&#33258;&#20061;&#20010;&#29616;&#26377;&#25968;&#25454;&#38598;&#30340;&#20107;&#23454;&#38169;&#35823;&#27880;&#37322;&#65292;&#24182;&#26681;&#25454;&#24213;&#23618;&#30340;&#25688;&#35201;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#20998;&#31867;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#26368;&#20808;&#36827;&#30340;&#20107;&#23454;&#24230;&#37327;&#26631;&#20934;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;&#26368;&#36817;&#30340;ChatGPT-based&#24230;&#37327;&#26631;&#20934;&#65292;&#22312;&#36825;&#20010;&#20998;&#23618;&#22522;&#20934;&#19978;&#23637;&#31034;&#20102;&#23427;&#20204;&#22312;&#19981;&#21516;&#31867;&#22411;&#30340;&#25688;&#35201;&#29983;&#25104;&#27169;&#22411;&#19978;&#30340;&#24615;&#33021;&#24046;&#24322;&#12290;&#20851;&#38190;&#26159;&#65292;&#25105;&#20204;&#30340;&#20998;&#26512;&#26174;&#31034;&#65292;&#26368;&#36817;&#22312;&#20107;&#23454;&#26816;&#27979;&#31354;&#38388;&#30340;&#24456;&#22823;&#25913;&#36827;&#26159;&#38024;&#23545;&#26087;&#30340;(&#21069;Transformer) &#27169;&#22411;&#30340;&#24635;&#32467;&#65292;&#32780;&#19981;&#26159;&#26356;&#30456;&#20851;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
The propensity of abstractive summarization models to make factual errors has been studied extensively, including design of metrics to detect factual errors and annotation of errors in current systems' outputs. However, the ever-evolving nature of summarization systems, metrics, and annotated benchmarks makes factuality evaluation a moving target, and drawing clear comparisons among metrics has become increasingly difficult. In this work, we aggregate factuality error annotations from nine existing datasets and stratify them according to the underlying summarization model. We compare performance of state-of-the-art factuality metrics, including recent ChatGPT-based metrics, on this stratified benchmark and show that their performance varies significantly across different types of summarization models. Critically, our analysis shows that much of the recent improvement in the factuality detection space has been on summaries from older (pre-Transformer) models instead of more relevant rec
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TAGPRIME&#30340;&#32479;&#19968;&#20851;&#31995;&#32467;&#26500;&#25552;&#21462;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#32473;&#23450;&#26465;&#20214;&#20449;&#24687;&#28155;&#21152;&#21040;&#36755;&#20837;&#25991;&#26412;&#20013;&#65292;&#20351;&#24471;&#36755;&#20986;&#30340;&#19978;&#19979;&#25991;&#34920;&#31034;&#26356;&#36866;&#21512;&#25552;&#21462;&#29305;&#23450;&#26465;&#20214;&#19979;&#30340;&#20851;&#31995;&#12290;&#22312;&#21508;&#31181;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#20248;&#20110;&#26368;&#26032;&#30340;&#20851;&#31995;&#25552;&#21462;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2205.12585</link><description>&lt;p&gt;
TAGPRIME&#65306;&#19968;&#31181;&#29992;&#20110;&#20851;&#31995;&#32467;&#26500;&#25552;&#21462;&#30340;&#32479;&#19968;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
TAGPRIME: A Unified Framework for Relational Structure Extraction. (arXiv:2205.12585v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.12585
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TAGPRIME&#30340;&#32479;&#19968;&#20851;&#31995;&#32467;&#26500;&#25552;&#21462;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#32473;&#23450;&#26465;&#20214;&#20449;&#24687;&#28155;&#21152;&#21040;&#36755;&#20837;&#25991;&#26412;&#20013;&#65292;&#20351;&#24471;&#36755;&#20986;&#30340;&#19978;&#19979;&#25991;&#34920;&#31034;&#26356;&#36866;&#21512;&#25552;&#21462;&#29305;&#23450;&#26465;&#20214;&#19979;&#30340;&#20851;&#31995;&#12290;&#22312;&#21508;&#31181;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#20248;&#20110;&#26368;&#26032;&#30340;&#20851;&#31995;&#25552;&#21462;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#35768;&#22810;&#20219;&#21153;&#38656;&#35201;&#25552;&#21462;&#32473;&#23450;&#26465;&#20214;&#19979;&#30340;&#20851;&#31995;&#20449;&#24687;&#65292;&#20363;&#22914;&#20107;&#20214;&#21442;&#25968;&#25552;&#21462;&#12289;&#20851;&#31995;&#25552;&#21462;&#21644;&#38754;&#21521;&#20219;&#21153;&#30340;&#35821;&#20041;&#35299;&#26512;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#36890;&#24120;&#38024;&#23545;&#27599;&#20010;&#20219;&#21153;&#20998;&#21035;&#25552;&#20986;&#22797;&#26434;&#30340;&#27169;&#22411;&#65292;&#36739;&#23569;&#20851;&#27880;&#36825;&#20123;&#20219;&#21153;&#30340;&#20849;&#24615;&#65292;&#24182;&#32570;&#20047;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#26469;&#22788;&#29702;&#25152;&#26377;&#36825;&#20123;&#20219;&#21153;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;TAGPRIME&#65292;&#26088;&#22312;&#20174;&#32479;&#19968;&#30340;&#35282;&#24230;&#26469;&#35299;&#20915;&#36825;&#20123;&#20851;&#31995;&#32467;&#26500;&#25552;&#21462;&#38382;&#39064;&#12290;TAGPRIME&#26159;&#19968;&#31181;&#24207;&#21015;&#26631;&#35760;&#27169;&#22411;&#65292;&#23558;&#32473;&#23450;&#26465;&#20214;&#65288;&#20363;&#22914;&#20107;&#20214;&#35302;&#21457;&#22120;&#65289;&#30340;&#20449;&#24687;&#28155;&#21152;&#21040;&#36755;&#20837;&#25991;&#26412;&#20013;&#12290;&#36890;&#36807;&#22312;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#20351;&#29992;&#33258;&#27880;&#24847;&#26426;&#21046;&#65292;&#36825;&#20123;&#26465;&#20214;&#20449;&#24687;&#20351;&#24471;&#36755;&#20986;&#30340;&#19978;&#19979;&#25991;&#34920;&#31034;&#21253;&#21547;&#26356;&#22810;&#20851;&#20110;&#32473;&#23450;&#26465;&#20214;&#30340;&#20449;&#24687;&#65292;&#20174;&#32780;&#26356;&#36866;&#21512;&#25552;&#21462;&#29305;&#23450;&#26465;&#20214;&#19979;&#30340;&#20851;&#31995;&#12290;&#22312;&#24191;&#27867;&#30340;&#23454;&#39564;&#21644;&#20998;&#26512;&#20013;&#34920;&#26126;&#65292;TAGPRIME&#22312;&#21508;&#31181;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#20248;&#20110;&#26368;&#26032;&#30340;&#20851;&#31995;&#25552;&#21462;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many tasks in natural language processing require the extraction of relationship information for a given condition, such as event argument extraction, relation extraction, and task-oriented semantic parsing. Recent works usually propose sophisticated models for each task independently and pay less attention to the commonality of these tasks and to have a unified framework for all the tasks. In this work, we propose to take a unified view of all these tasks and introduce TAGPRIME to address relational structure extraction problems. TAGPRIME is a sequence tagging model that appends priming words about the information of the given condition (such as an event trigger) to the input text. With the self-attention mechanism in pre-trained language models, the priming words make the output contextualized representations contain more information about the given condition, and hence become more suitable for extracting specific relationships for the condition. Extensive experiments and analyses on
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20174;&#19977;&#20010;&#26041;&#38754;&#32771;&#34385;&#20449;&#24515;&#30340;&#23646;&#24615;&#26469;&#35780;&#20272;&#20445;&#35777;&#26696;&#20363;&#65292;&#20854;&#20013;&#20027;&#35201;&#30340;&#25514;&#26045;&#26159;&#23558;&#35770;&#35777;&#35299;&#37322;&#20026;&#36923;&#36753;&#35777;&#26126;&#30340;&#23436;&#22791;&#24615;&#12290;</title><link>http://arxiv.org/abs/2205.04522</link><description>&lt;p&gt;
&#35780;&#20272;&#8220;Assurance 2.0&#8221;&#20013;&#30340;&#20449;&#24515;
&lt;/p&gt;
&lt;p&gt;
Assessing Confidence with Assurance 2.0. (arXiv:2205.04522v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.04522
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20174;&#19977;&#20010;&#26041;&#38754;&#32771;&#34385;&#20449;&#24515;&#30340;&#23646;&#24615;&#26469;&#35780;&#20272;&#20445;&#35777;&#26696;&#20363;&#65292;&#20854;&#20013;&#20027;&#35201;&#30340;&#25514;&#26045;&#26159;&#23558;&#35770;&#35777;&#35299;&#37322;&#20026;&#36923;&#36753;&#35777;&#26126;&#30340;&#23436;&#22791;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20445;&#35777;&#26696;&#20363;&#26088;&#22312;&#23545;&#20854;&#39030;&#32423;&#22768;&#26126;&#65288;&#36890;&#24120;&#28041;&#21450;&#23433;&#20840;&#24615;&#25110;&#23433;&#20840;&#65289;&#25552;&#20379;&#21487;&#35777;&#26126;&#30340;&#20449;&#24515;&#12290;&#37027;&#20040;&#38382;&#39064;&#26469;&#20102;&#65292;&#8220;&#36825;&#31181;&#20449;&#24515;&#8221;&#26377;&#22810;&#23569;&#65311;&#25105;&#20204;&#35748;&#20026;&#20449;&#24515;&#19981;&#33021;&#24402;&#32467;&#20026;&#21333;&#19968;&#23646;&#24615;&#25110;&#27979;&#37327;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#24314;&#35758;&#23427;&#24212;&#35813;&#22522;&#20110;&#19977;&#20010;&#19981;&#21516;&#35270;&#35282;&#30340;&#23646;&#24615;&#65306;&#31215;&#26497;&#65292;&#28040;&#26497;&#21644;&#21097;&#20313;&#30097;&#34385;&#12290;&#31215;&#26497;&#35270;&#35282;&#32771;&#34385;&#35777;&#25454;&#21644;&#25972;&#20010;&#26696;&#20363;&#30340;&#31243;&#24230;&#65292;&#23558;&#31215;&#26497;&#22768;&#26126;&#21512;&#29702;&#21270;&#20026;&#25903;&#25345;&#20854;&#20027;&#24352;&#30340;&#20449;&#20208;&#12290;&#25105;&#20204;&#23545;&#35777;&#26126;&#35774;&#31435;&#20102;&#39640;&#38376;&#27099;&#65292;&#35201;&#27714;&#35777;&#26126;&#26159;&#19981;&#21487;&#21542;&#35748;&#30340;&#12290;&#20854;&#20013;&#20027;&#35201;&#30340;&#27491;&#38754;&#25514;&#26045;&#26159;&#23436;&#22791;&#24615;&#65292;&#23558;&#35813;&#35770;&#35777;&#35299;&#37322;&#20026;&#36923;&#36753;&#35777;&#26126;&#12290;&#35777;&#25454;&#30340;&#20449;&#24515;&#21487;&#20197;&#20197;&#27010;&#29575;&#26041;&#24335;&#34920;&#36798;&#65292;&#24182;&#20351;&#29992;&#30830;&#35748;&#25514;&#26045;&#30830;&#20445;&#35777;&#25454;&#30340;"&#37325;&#37327;"&#36234;&#36807;&#26576;&#20010;&#38408;&#20540;&#12290;&#27492;&#22806;&#65292;&#27010;&#29575;&#21487;&#20197;&#20174;&#35777;&#25454;&#20013;&#32858;&#21512;&#65292;&#36890;&#36807;&#23558;&#35777;&#25454;&#35270;&#20026;&#36125;&#21494;&#26031;&#32593;&#32476;&#33410;&#28857;&#26469;&#23436;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
An assurance case is intended to provide justifiable confidence in the truth of its top claim, which typically concerns safety or security. A natural question is then "how much" confidence does the case provide? We argue that confidence cannot be reduced to a single attribute or measurement. Instead, we suggest it should be based on attributes that draw on three different perspectives: positive, negative, and residual doubts.  Positive Perspectives consider the extent to which the evidence and overall argument of the case combine to make a positive statement justifying belief in its claims. We set a high bar for justification, requiring it to be indefeasible. The primary positive measure for this is soundness, which interprets the argument as a logical proof. Confidence in evidence can be expressed probabilistically and we use confirmation measures to ensure that the "weight" of evidence crosses some threshold. In addition, probabilities can be aggregated from evidence through the step
&lt;/p&gt;</description></item><item><title>EVOTER&#20351;&#29992;&#31616;&#21333;&#30340;&#36923;&#36753;&#34920;&#36798;&#24335;&#28436;&#21270;&#20986;&#36879;&#26126;&#21487;&#35299;&#37322;&#30340;&#35268;&#21017;&#38598;&#65292;&#19982;&#40657;&#30418;&#27169;&#22411;&#24615;&#33021;&#30456;&#20284;&#65292;&#21487;&#20197;&#25581;&#31034;&#25968;&#25454;&#20013;&#30340;&#20559;&#35265;&#24182;&#20026;&#26410;&#26469;&#26500;&#24314;&#21487;&#38752;&#30340;AI&#31995;&#32479;&#25552;&#20379;&#22522;&#30784;&#12290;</title><link>http://arxiv.org/abs/2204.10438</link><description>&lt;p&gt;
EVOTER&#65306;&#36879;&#26126;&#21487;&#35299;&#37322;&#35268;&#21017;&#38598;&#30340;&#36827;&#21270;
&lt;/p&gt;
&lt;p&gt;
EVOTER: Evolution of Transparent Explainable Rule-sets. (arXiv:2204.10438v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.10438
&lt;/p&gt;
&lt;p&gt;
EVOTER&#20351;&#29992;&#31616;&#21333;&#30340;&#36923;&#36753;&#34920;&#36798;&#24335;&#28436;&#21270;&#20986;&#36879;&#26126;&#21487;&#35299;&#37322;&#30340;&#35268;&#21017;&#38598;&#65292;&#19982;&#40657;&#30418;&#27169;&#22411;&#24615;&#33021;&#30456;&#20284;&#65292;&#21487;&#20197;&#25581;&#31034;&#25968;&#25454;&#20013;&#30340;&#20559;&#35265;&#24182;&#20026;&#26410;&#26469;&#26500;&#24314;&#21487;&#38752;&#30340;AI&#31995;&#32479;&#25552;&#20379;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;AI&#31995;&#32479;&#26159;&#40657;&#30418;&#23376;&#65292;&#20026;&#32473;&#23450;&#30340;&#36755;&#20837;&#29983;&#25104;&#21512;&#29702;&#30340;&#36755;&#20986;&#12290;&#28982;&#32780;&#65292;&#26576;&#20123;&#39046;&#22495;&#20855;&#26377;&#35299;&#37322;&#33021;&#21147;&#21644;&#20449;&#20219;&#24230;&#35201;&#27714;&#65292;&#36825;&#20123;&#35201;&#27714;&#19981;&#33021;&#30452;&#25509;&#28385;&#36275;&#36825;&#20123;&#26041;&#27861;&#12290;&#22240;&#27492;&#65292;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#65292;&#21363;&#24320;&#22987;&#26102;&#27169;&#22411;&#23601;&#26159;&#36879;&#26126;&#30340;&#21644;&#21487;&#35299;&#37322;&#30340;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#31616;&#21333;&#30340;&#36923;&#36753;&#34920;&#36798;&#24335;&#28436;&#21270;&#20986;&#35268;&#21017;&#38598;&#65292;&#31216;&#20026;EVOTER&#12290;EVOTER&#22312;&#22810;&#20010;&#39044;&#27979;/&#20998;&#31867;&#21644;&#22788;&#26041;/&#25919;&#31574;&#25628;&#32034;&#39046;&#22495;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#26377;&#21644;&#27809;&#26377;&#20195;&#29702;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#23427;&#33021;&#22815;&#21457;&#29616;&#21644;&#40657;&#30418;&#27169;&#22411;&#30456;&#20284;&#30340;&#26377;&#24847;&#20041;&#30340;&#35268;&#21017;&#38598;&#12290;&#36825;&#20123;&#35268;&#21017;&#21487;&#20197;&#25552;&#20379;&#39046;&#22495;&#30340;&#35265;&#35299;&#65292;&#24182;&#20351;&#25968;&#25454;&#20013;&#38544;&#34255;&#30340;&#20559;&#35265;&#26174;&#24615;&#21270;&#12290;&#20063;&#21487;&#20197;&#30452;&#25509;&#23545;&#23427;&#20204;&#36827;&#34892;&#32534;&#36753;&#65292;&#20197;&#28040;&#38500;&#20559;&#35265;&#24182;&#28155;&#21152;&#32422;&#26463;&#12290;&#22240;&#27492;&#65292;EVOTER&#20026;&#26410;&#26469;&#26500;&#24314;&#20540;&#24471;&#20449;&#36182;&#30340;AI&#31995;&#32479;&#30340;&#21487;&#38752;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most AI systems are black boxes generating reasonable outputs for given inputs. Some domains, however, have explainability and trustworthiness requirements that cannot be directly met by these approaches. Various methods have therefore been developed to interpret black-box models after training. This paper advocates an alternative approach where the models are transparent and explainable to begin with. This approach, EVOTER, evolves rule-sets based on simple logical expressions. The approach is evaluated in several prediction/classification and prescription/policy search domains with and without a surrogate. It is shown to discover meaningful rule sets that perform similarly to black-box models. The rules can provide insight into the domain, and make biases hidden in the data explicit. It may also be possible to edit them directly to remove biases and add constraints. EVOTER thus forms a promising foundation for building trustworthy AI systems for real-world applications in the future.
&lt;/p&gt;</description></item><item><title>SD-Conv &#26159;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#23558;&#21160;&#24577;&#26426;&#21046;&#21644;&#31232;&#30095;&#24615;&#38598;&#25104;&#22312;&#19968;&#36215;&#65292;&#36890;&#36807;&#35774;&#35745;&#20108;&#36827;&#21046;&#25513;&#30721;&#20462;&#21098;&#38745;&#24577;&#26680;&#65292;&#22823;&#22823;&#20943;&#23569;&#20102;&#21442;&#25968;&#21644;&#35745;&#31639;&#25104;&#26412;&#65292;&#24182;&#22312;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#27604;&#22522;&#32447;&#27169;&#22411;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2204.02227</link><description>&lt;p&gt;
SD-Conv&#65306;&#23454;&#29616;&#21160;&#24577;&#21367;&#31215;&#30340;&#21442;&#25968;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;
SD-Conv: Towards the Parameter-Efficiency of Dynamic Convolution. (arXiv:2204.02227v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.02227
&lt;/p&gt;
&lt;p&gt;
SD-Conv &#26159;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#23558;&#21160;&#24577;&#26426;&#21046;&#21644;&#31232;&#30095;&#24615;&#38598;&#25104;&#22312;&#19968;&#36215;&#65292;&#36890;&#36807;&#35774;&#35745;&#20108;&#36827;&#21046;&#25513;&#30721;&#20462;&#21098;&#38745;&#24577;&#26680;&#65292;&#22823;&#22823;&#20943;&#23569;&#20102;&#21442;&#25968;&#21644;&#35745;&#31639;&#25104;&#26412;&#65292;&#24182;&#22312;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#27604;&#22522;&#32447;&#27169;&#22411;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#24577;&#21367;&#31215;&#22312;&#20445;&#25345;&#24494;&#19981;&#36275;&#36947;&#30340;&#28014;&#28857;&#36816;&#31639;&#22686;&#37327;&#30340;&#21516;&#26102;&#65292;&#23454;&#29616;&#20102;&#26377;&#25928;CNN&#30340;&#26356;&#22909;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#24615;&#33021;&#25552;&#21319;&#26080;&#27861;&#21305;&#37197;&#26174;&#33879;&#25193;&#22823;&#30340;&#21442;&#25968;&#25968;&#37327;&#65292;&#36825;&#26159;&#29616;&#23454;&#24212;&#29992;&#30340;&#20027;&#35201;&#29942;&#39048;&#12290;&#30456;&#21453;&#65292;&#22522;&#20110;&#25513;&#30721;&#30340;&#38750;&#32467;&#26500;&#21270;&#20462;&#21098;&#36890;&#36807;&#21435;&#38500;&#37325;&#22797;&#39033;&#26469;&#33719;&#24471;&#36731;&#37327;&#32423;&#32593;&#32476;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#31216;&#20026;&#8220;&#31232;&#30095;&#21160;&#24577;&#21367;&#31215;&#8221;&#65288;SD-Conv&#65289;&#65292;&#33258;&#28982;&#22320;&#23558;&#36825;&#20004;&#20010;&#36335;&#24452;&#38598;&#25104;&#22312;&#19968;&#36215;&#65292;&#20197;&#20415;&#21487;&#20197;&#32487;&#25215;&#21160;&#24577;&#26426;&#21046;&#21644;&#31232;&#30095;&#24615;&#30340;&#20248;&#28857;&#12290;&#25105;&#20204;&#39318;&#20808;&#35774;&#35745;&#20102;&#19968;&#20010;&#20108;&#36827;&#21046;&#25513;&#30721;&#65292;&#20174;&#21487;&#23398;&#20064;&#38408;&#20540;&#20013;&#27966;&#29983;&#20986;&#26469;&#65292;&#20197;&#20462;&#21098;&#38745;&#24577;&#26680;&#65292;&#22823;&#22823;&#20943;&#23569;&#20102;&#21442;&#25968;&#21644;&#35745;&#31639;&#25104;&#26412;&#65292;&#20294;&#22312;Imagenet-1K&#20013;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23558;&#39044;&#35757;&#32451;&#27169;&#22411;&#36716;&#31227;&#21040;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#20013;&#65292;&#23637;&#31034;&#20102;&#27604;&#22522;&#32447;&#27169;&#22411;&#26356;&#20026;&#31283;&#23450;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#24076;&#26395;&#25105;&#20204;&#30340;SD-Conv&#33021;&#22815;&#25104;&#20026;&#20256;&#32479;&#21160;&#24577;&#21367;&#31215;&#21644;&#32467;&#26500;&#21270;&#20462;&#21098;&#30340;&#39640;&#25928;&#26367;&#20195;&#21697;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dynamic convolution achieves better performance for efficient CNNs at the cost of negligible FLOPs increase. However, the performance increase can not match the significantly expanded number of parameters, which is the main bottleneck in real-world applications. Contrastively, mask-based unstructured pruning obtains a lightweight network by removing redundancy in the heavy network. In this paper, we propose a new framework, \textbf{Sparse Dynamic Convolution} (\textsc{SD-Conv}), to naturally integrate these two paths such that it can inherit the advantage of dynamic mechanism and sparsity. We first design a binary mask derived from a learnable threshold to prune static kernels, significantly reducing the parameters and computational cost but achieving higher performance in Imagenet-1K. We further transfer pretrained models into a variety of downstream tasks, showing consistently better results than baselines. We hope our SD-Conv could be an efficient alternative to conventional dynamic
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#22914;&#20309;&#25511;&#21046;&#22810;&#20010;&#26080;&#20154;&#26426;&#22522;&#31449;&#25552;&#20379;&#24555;&#36895;&#30340;&#26080;&#32447;&#36890;&#20449;&#65292;&#23588;&#20854;&#26159;&#22312;&#33258;&#28982;&#28798;&#23475;&#31561;&#24773;&#20917;&#19979;&#12290;&#37319;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#65292;&#20351;&#26080;&#20154;&#26426;&#21487;&#22312;&#38745;&#24577;&#21644;&#21160;&#24577;&#29615;&#22659;&#19979;&#28789;&#27963;&#35843;&#37197;&#20301;&#32622;&#20197;&#28385;&#36275;&#22320;&#38754;&#29992;&#25143;&#30340;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2202.02006</link><description>&lt;p&gt;
5G&#32593;&#32476;&#22312;&#32764;&#19978;&#65306;&#19968;&#31181;&#38754;&#21521;&#22522;&#20110;&#26080;&#20154;&#26426;&#30340;&#32508;&#21512;&#25509;&#20837;&#19982;&#22238;&#20256;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
5G Network on Wings: A Deep Reinforcement Learning Approach to the UAV-based Integrated Access and Backhaul. (arXiv:2202.02006v3 [cs.NI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.02006
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#22914;&#20309;&#25511;&#21046;&#22810;&#20010;&#26080;&#20154;&#26426;&#22522;&#31449;&#25552;&#20379;&#24555;&#36895;&#30340;&#26080;&#32447;&#36890;&#20449;&#65292;&#23588;&#20854;&#26159;&#22312;&#33258;&#28982;&#28798;&#23475;&#31561;&#24773;&#20917;&#19979;&#12290;&#37319;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#65292;&#20351;&#26080;&#20154;&#26426;&#21487;&#22312;&#38745;&#24577;&#21644;&#21160;&#24577;&#29615;&#22659;&#19979;&#28789;&#27963;&#35843;&#37197;&#20301;&#32622;&#20197;&#28385;&#36275;&#22320;&#38754;&#29992;&#25143;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24555;&#36895;&#12289;&#21487;&#38752;&#30340;&#26080;&#32447;&#36890;&#20449;&#24050;&#32463;&#25104;&#20026;&#20154;&#31867;&#29983;&#27963;&#20013;&#30340;&#20851;&#38190;&#38656;&#27714;&#12290;&#22312;&#20219;&#21153;&#20851;&#38190;&#65288;MC&#65289;&#22330;&#26223;&#19979;&#65292;&#20363;&#22914;&#33258;&#28982;&#28798;&#23475;&#34989;&#20987;&#26102;&#65292;&#20351;&#29992;&#20256;&#32479;&#26080;&#32447;&#32593;&#32476;&#25552;&#20379;&#26222;&#36941;&#36830;&#25509;&#21464;&#24471;&#22256;&#38590;&#12290;&#22240;&#27492;&#65292;&#22522;&#20110;&#26080;&#20154;&#26426;&#65288;UAV&#65289;&#30340;&#33322;&#31354;&#32593;&#32476;&#25552;&#20379;&#20102;&#19968;&#31181;&#24555;&#36895;&#12289;&#28789;&#27963;&#21644;&#21487;&#38752;&#30340;&#26080;&#32447;&#36890;&#20449;&#26367;&#20195;&#26041;&#26696;&#12290;&#26412;&#25991;&#30740;&#31350;&#22914;&#20309;&#25511;&#21046;&#22810;&#20010;&#38745;&#24577;&#21644;&#21160;&#24577;&#29615;&#22659;&#19979;&#30340;UAV&#22522;&#31449;&#65288;UAV-BSs&#65289;&#12290;&#25105;&#20204;&#20351;&#29992;&#31995;&#32479;&#32423;&#27169;&#25311;&#22120;&#27169;&#25311;&#19968;&#20010;MC&#22330;&#26223;&#65292;&#22312;&#35813;&#22330;&#26223;&#20013;&#65292;&#34562;&#31389;&#32593;&#32476;&#30340;&#23439;&#22522;&#31449;&#34987;&#21344;&#39046;&#25110;&#21463;&#25439;&#65292;&#22240;&#27492;&#38656;&#35201;&#20351;&#29992;UAV-BSs&#25552;&#20379;&#24555;&#36895;&#36830;&#25509;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fast and reliable wireless communication has become a critical demand in human life. In the case of mission-critical (MC) scenarios, for instance, when natural disasters strike, providing ubiquitous connectivity becomes challenging by using traditional wireless networks. In this context, unmanned aerial vehicle (UAV) based aerial networks offer a promising alternative for fast, flexible, and reliable wireless communications. Due to unique characteristics such as mobility, flexible deployment, and rapid reconfiguration, drones can readily change location dynamically to provide on-demand communications to users on the ground in emergency scenarios. As a result, the usage of UAV base stations (UAV-BSs) has been considered an appropriate approach for providing rapid connection in MC scenarios. In this paper, we study how to control multiple UAV-BSs in both static and dynamic environments. We use a system-level simulator to model an MC scenario in which a macro BS of a cellular network is o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38598;&#25104;&#33258;&#20027;&#25511;&#21046;&#12289;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#21644;&#30417;&#31649;&#21512;&#35268;&#24615;&#30340;&#35774;&#35745;&#26694;&#26550;&#65292;&#20197;&#23454;&#29616;&#23433;&#20840;&#12289;&#21487;&#35299;&#37322;&#21644;&#30417;&#31649;&#30340;&#33258;&#21160;&#39550;&#39542;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2111.10518</link><description>&lt;p&gt;
&#23454;&#29616;&#23433;&#20840;&#12289;&#21487;&#35299;&#37322;&#21644;&#30417;&#31649;&#30340;&#33258;&#21160;&#39550;&#39542;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Towards Safe, Explainable, and Regulated Autonomous Driving. (arXiv:2111.10518v4 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.10518
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38598;&#25104;&#33258;&#20027;&#25511;&#21046;&#12289;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#21644;&#30417;&#31649;&#21512;&#35268;&#24615;&#30340;&#35774;&#35745;&#26694;&#26550;&#65292;&#20197;&#23454;&#29616;&#23433;&#20840;&#12289;&#21487;&#35299;&#37322;&#21644;&#30417;&#31649;&#30340;&#33258;&#21160;&#39550;&#39542;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#33258;&#20027;&#39550;&#39542;&#36710;&#36742;&#30340;&#21457;&#23637;&#21644;&#37096;&#32626;&#27491;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#65292;&#23588;&#20854;&#26159;&#28145;&#24230;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#31561;&#24378;&#22823;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#30340;&#23454;&#35777;&#25104;&#21151;&#12290;&#20294;&#26159;&#65292;&#27491;&#22914;&#26368;&#36817;&#30340;&#20132;&#36890;&#20107;&#25925;&#25152;&#34920;&#26126;&#30340;&#37027;&#26679;&#65292;&#33258;&#21160;&#39550;&#39542;&#25216;&#26415;&#26080;&#27861;&#23436;&#20840;&#20445;&#35777;&#23433;&#20840;&#37096;&#32626;&#12290;&#30001;&#20110;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#26159;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#26234;&#33021;&#23548;&#33322;&#31995;&#32479;&#32972;&#21518;&#30340;&#20027;&#35201;&#25216;&#26415;&#65292;&#22240;&#27492;&#21033;&#30410;&#30456;&#20851;&#32773;&#21644;&#20132;&#36890;&#30417;&#31649;&#26426;&#26500;&#35201;&#27714;&#20182;&#20204;&#30340;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#36719;&#20214;&#26550;&#26500;&#24517;&#39035;&#20855;&#22791;&#23433;&#20840;&#24615;&#12289;&#21487;&#35299;&#37322;&#24615;&#21644;&#30417;&#31649;&#21512;&#35268;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#35774;&#35745;&#26694;&#26550;&#65292;&#38598;&#25104;&#33258;&#20027;&#25511;&#21046;&#12289;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#21644;&#30417;&#31649;&#21512;&#35268;&#24615;&#65292;&#20197;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25509;&#30528;&#36890;&#36807;&#26696;&#20363;&#30740;&#31350;&#36827;&#34892;&#20102;&#26694;&#26550;&#30340;&#21021;&#27493;&#39564;&#35777;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#30456;&#20851;&#30340;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#65292;&#20197;&#24110;&#21161;&#23454;&#29616;&#26694;&#26550;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
There has been recent and growing interest in the development and deployment of autonomous vehicles, encouraged by the empirical successes of powerful artificial intelligence techniques (AI), especially in the applications of deep learning and reinforcement learning. However, as demonstrated by recent traffic accidents, autonomous driving technology is not fully reliable for safe deployment. As AI is the main technology behind the intelligent navigation systems of self-driving vehicles, both the stakeholders and transportation regulators require their AI-driven software architecture to be safe, explainable, and regulatory compliant. In this paper, we propose a design framework that integrates autonomous control, explainable AI (XAI), and regulatory compliance to address this issue, and then provide an initial validation of the framework with a critical analysis in a case study. Moreover, we describe relevant XAI approaches that can help achieve the goals of the framework.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#20107;&#23454;&#30340;&#36923;&#36753;&#25512;&#29702;&#26041;&#27861;&#65292;&#37319;&#29992;&#20998;&#23618;&#26041;&#24335;&#28085;&#30422;&#24120;&#35782;&#21644;&#20020;&#26102;&#30693;&#35782;&#32447;&#32034;&#65292;&#36890;&#36807;&#26500;&#24314;&#36229;&#22270;&#23454;&#29616;&#21477;&#23376;&#32423;&#21035;&#21644;&#23454;&#20307;&#32423;&#21035;&#30340;&#20132;&#20114;&#65292;&#22312;&#36923;&#36753;&#25512;&#29702;&#22522;&#20934;&#27979;&#35797;&#21644;&#22823;&#35268;&#27169;&#38405;&#35835;&#29702;&#35299;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2105.10334</link><description>&lt;p&gt;
&#22522;&#20110;&#20107;&#23454;&#30340;&#36923;&#36753;&#25512;&#29702;&#19982;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Fact-driven Logical Reasoning for Machine Reading Comprehension. (arXiv:2105.10334v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2105.10334
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#20107;&#23454;&#30340;&#36923;&#36753;&#25512;&#29702;&#26041;&#27861;&#65292;&#37319;&#29992;&#20998;&#23618;&#26041;&#24335;&#28085;&#30422;&#24120;&#35782;&#21644;&#20020;&#26102;&#30693;&#35782;&#32447;&#32034;&#65292;&#36890;&#36807;&#26500;&#24314;&#36229;&#22270;&#23454;&#29616;&#21477;&#23376;&#32423;&#21035;&#21644;&#23454;&#20307;&#32423;&#21035;&#30340;&#20132;&#20114;&#65292;&#22312;&#36923;&#36753;&#25512;&#29702;&#22522;&#20934;&#27979;&#35797;&#21644;&#22823;&#35268;&#27169;&#38405;&#35835;&#29702;&#35299;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#20154;&#24320;&#22987;&#20851;&#27880;&#26426;&#22120;&#20855;&#22791;&#25512;&#29702;&#33021;&#21147;&#65292;&#36825;&#38656;&#35201;&#20934;&#30830;&#28165;&#26224;&#22320;&#25552;&#20379;&#32447;&#32034;&#12290;&#22312;&#29616;&#26377;&#30340;&#30740;&#31350;&#20013;&#65292;&#32447;&#32034;&#36890;&#24120;&#34987;&#24314;&#27169;&#20026;&#23454;&#20307;&#24863;&#30693;&#30693;&#35782;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#23454;&#20307;&#24863;&#30693;&#32447;&#32034;&#20027;&#35201;&#38598;&#20013;&#20110;&#24120;&#35782;&#65292;&#23545;&#20110;&#38656;&#35201;&#20102;&#35299;&#20020;&#26102;&#20107;&#23454;&#25110;&#20107;&#20214;&#30340;&#20219;&#21153;&#65292;&#29305;&#21035;&#26159;&#22312;&#38405;&#35835;&#29702;&#35299;&#30340;&#36923;&#36753;&#25512;&#29702;&#20013;&#65292;&#36825;&#20123;&#32447;&#32034;&#26159;&#19981;&#36275;&#22815;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#26377;&#21160;&#21147;&#20197;&#20998;&#23618;&#26041;&#24335;&#28085;&#30422;&#24120;&#35782;&#21644;&#20020;&#26102;&#30693;&#35782;&#32447;&#32034;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#21333;&#20803;&#30340;&#36890;&#29992;&#24418;&#24335;&#65292;&#36890;&#36807;&#25552;&#21462;&#21477;&#23376;&#30340;&#39592;&#24178;&#26500;&#25104;&#37096;&#20998;&#65292;&#22914;&#20027;&#35821;-&#35859;&#35821;-&#23486;&#35821;&#24418;&#25104;&#30340;&#8220;&#20107;&#23454;&#8221;&#12290;&#28982;&#21518;&#65292;&#22312;&#20107;&#23454;&#21333;&#20803;&#20043;&#19978;&#26500;&#24314;&#19968;&#20010;&#36229;&#22270;&#65292;&#20801;&#35768;&#21477;&#23376;&#32423;&#21035;&#65288;&#20107;&#23454;&#32452;&#20043;&#38388;&#30340;&#20851;&#31995;&#65289;&#21644;&#23454;&#20307;&#32423;&#21035;&#20132;&#20114;&#65288;&#20107;&#23454;&#20013;&#30340;&#27010;&#24565;&#25110;&#34892;&#21160;&#65289;&#30340;&#20114;&#21160;&#12290;&#22312;&#36923;&#36753;&#25512;&#29702;&#22522;&#20934;&#27979;&#35797;&#21644;&#22823;&#35268;&#27169;&#38405;&#35835;&#29702;&#35299;&#20013;&#65292;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent years have witnessed an increasing interest in training machines with reasoning ability, which deeply relies on accurately and clearly presented clue forms. The clues are usually modeled as entity-aware knowledge in existing studies. However, those entity-aware clues are primarily focused on commonsense, making them insufficient for tasks that require knowledge of temporary facts or events, particularly in logical reasoning for reading comprehension. To address this challenge, we are motivated to cover both commonsense and temporary knowledge clues hierarchically. Specifically, we propose a general formalism of knowledge units by extracting backbone constituents of the sentence, such as the subject-verb-object formed ``facts''. We then construct a supergraph on top of the fact units, allowing for the benefit of sentence-level (relations among fact groups) and entity-level interactions (concepts or actions inside a fact). Experimental results on logical reasoning benchmarks and d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#20219;&#21153;&#27880;&#24847;&#21147;&#27531;&#24046;&#32593;&#32476;&#26550;&#26500;&#65292;&#36890;&#36807;&#21033;&#29992;&#38598;&#25104;&#26041;&#27861;&#12289;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#22810;&#20219;&#21153;&#23398;&#20064;&#65292;&#26080;&#38656;&#20551;&#35774;&#25991;&#26723;&#25110;&#35770;&#25454;&#32467;&#26500;&#65292;&#25104;&#21151;&#24212;&#29992;&#20110;&#22810;&#20010;&#35770;&#36848;&#25366;&#25496;&#20219;&#21153;&#20013;&#65292;&#25104;&#20026;&#20102;&#19968;&#31181;&#26082;&#36890;&#29992;&#21448;&#39640;&#24615;&#33021;&#30340;&#26550;&#26500;&#12290;</title><link>http://arxiv.org/abs/2102.12227</link><description>&lt;p&gt;
&#22810;&#20219;&#21153;&#27880;&#24847;&#21147;&#27531;&#24046;&#32593;&#32476;&#29992;&#20110;&#35770;&#36848;&#25366;&#25496;
&lt;/p&gt;
&lt;p&gt;
Multi-Task Attentive Residual Networks for Argument Mining. (arXiv:2102.12227v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2102.12227
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#20219;&#21153;&#27880;&#24847;&#21147;&#27531;&#24046;&#32593;&#32476;&#26550;&#26500;&#65292;&#36890;&#36807;&#21033;&#29992;&#38598;&#25104;&#26041;&#27861;&#12289;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#22810;&#20219;&#21153;&#23398;&#20064;&#65292;&#26080;&#38656;&#20551;&#35774;&#25991;&#26723;&#25110;&#35770;&#25454;&#32467;&#26500;&#65292;&#25104;&#21151;&#24212;&#29992;&#20110;&#22810;&#20010;&#35770;&#36848;&#25366;&#25496;&#20219;&#21153;&#20013;&#65292;&#25104;&#20026;&#20102;&#19968;&#31181;&#26082;&#36890;&#29992;&#21448;&#39640;&#24615;&#33021;&#30340;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22810;&#20219;&#21153;&#27880;&#24847;&#21147;&#27531;&#24046;&#32593;&#32476;&#22312;&#22810;&#20010;&#35770;&#36848;&#25366;&#25496;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#27531;&#24046;&#26550;&#26500;&#65292;&#21033;&#29992;&#20102;&#27880;&#24847;&#21147;&#12289;&#22810;&#20219;&#21153;&#23398;&#20064;&#65292;&#24182;&#20351;&#29992;&#38598;&#25104;&#26041;&#27861;&#65292;&#19981;&#23545;&#25991;&#26723;&#25110;&#35770;&#25454;&#32467;&#26500;&#20570;&#20219;&#20309;&#20551;&#35774;&#12290;&#25105;&#20204;&#22312;&#20116;&#20010;&#19981;&#21516;&#30340;&#29992;&#25143;&#29983;&#25104;&#35780;&#35770;&#12289;&#31185;&#23398;&#20986;&#29256;&#29289;&#21644;&#21149;&#35828;&#24615;&#35770;&#25991;&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#38024;&#23545;&#20855;&#26377;&#26356;&#39640;&#35745;&#31639;&#21360;&#35760;&#25110;&#29305;&#23450;&#20110;&#35821;&#26009;&#24211;&#35774;&#35745;&#30340;&#26368;&#20808;&#36827;&#26550;&#26500;&#30340;&#24378;&#26377;&#21147;&#30340;&#31454;&#20105;&#23545;&#25163;&#65292;&#20195;&#34920;&#20102;&#36890;&#29992;&#24615;&#12289;&#24615;&#33021;&#31934;&#24230;&#21644;&#20943;&#23569;&#27169;&#22411;&#22823;&#23567;&#20043;&#38388;&#30340;&#26377;&#36259;&#25240;&#34935;&#12290;
&lt;/p&gt;
&lt;p&gt;
We explore the use of residual networks and neural attention for multiple argument mining tasks. We propose a residual architecture that exploits attention, multi-task learning, and makes use of ensemble, without any assumption on document or argument structure. We present an extensive experimental evaluation on five different corpora of user-generated comments, scientific publications, and persuasive essays. Our results show that our approach is a strong competitor against state-of-the-art architectures with a higher computational footprint or corpus-specific design, representing an interesting compromise between generality, performance accuracy and reduced model size.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#32771;&#34385;&#35848;&#21028;&#30340;&#20219;&#21153;&#20998;&#37197;&#31995;&#32479;&#65292;&#22312;&#20219;&#21153;&#20998;&#37197;&#36807;&#31243;&#20013;&#20801;&#35768;&#25104;&#21592;&#25552;&#20986;&#21453;&#38382;&#12290;&#36890;&#36807;&#27169;&#25311;&#35848;&#21028;&#65292;&#29983;&#25104;&#23545;&#27604;&#35299;&#37322;&#65292;&#35813;&#26041;&#27861;&#34987;&#35777;&#26126;&#20844;&#24179;&#19988;&#26131;&#20110;&#29702;&#35299;&#12290;</title><link>http://arxiv.org/abs/2002.01640</link><description>&lt;p&gt;
&#8220;&#20320;&#20026;&#20160;&#20040;&#19981;&#25226;&#36825;&#20010;&#20219;&#21153;&#20998;&#37197;&#32473;&#20182;&#20204;&#65311;&#8221;&#65306;&#32771;&#34385;&#35848;&#21028;&#30340;&#21487;&#35299;&#37322;&#20219;&#21153;&#20998;&#37197;&#21644;&#23545;&#27604;&#35299;&#37322;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
`Why didn't you allocate this task to them?' Negotiation-Aware Explicable Task Allocation and Contrastive Explanation Generation. (arXiv:2002.01640v4 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2002.01640
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#32771;&#34385;&#35848;&#21028;&#30340;&#20219;&#21153;&#20998;&#37197;&#31995;&#32479;&#65292;&#22312;&#20219;&#21153;&#20998;&#37197;&#36807;&#31243;&#20013;&#20801;&#35768;&#25104;&#21592;&#25552;&#20986;&#21453;&#38382;&#12290;&#36890;&#36807;&#27169;&#25311;&#35848;&#21028;&#65292;&#29983;&#25104;&#23545;&#27604;&#35299;&#37322;&#65292;&#35813;&#26041;&#27861;&#34987;&#35777;&#26126;&#20844;&#24179;&#19988;&#26131;&#20110;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20219;&#21153;&#20998;&#37197;&#26159;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#12290;&#24403;&#22242;&#38431;&#25104;&#21592;&#20855;&#26377;&#19981;&#23436;&#25972;&#30340;&#20851;&#20110;&#38431;&#21451;&#25104;&#26412;&#21644;&#25972;&#20307;&#24615;&#33021;&#25351;&#26631;&#30340;&#30693;&#35782;&#26102;&#65292;&#38382;&#39064;&#21464;&#24471;&#26356;&#21152;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38598;&#20013;&#24335;&#20154;&#24037;&#26234;&#33021;&#20219;&#21153;&#20998;&#37197;&#65288;AITA&#65289;&#65292;&#35813;&#31995;&#32479;&#27169;&#25311;&#35848;&#21028;&#65292;&#24182;&#20135;&#29983;&#32771;&#34385;&#35848;&#21028;&#30340;&#21487;&#35299;&#37322;&#20219;&#21153;&#20998;&#37197;&#12290;&#22914;&#26524;&#22242;&#38431;&#25104;&#21592;&#23545;&#25152;&#25552;&#35758;&#30340;&#20998;&#37197;&#19981;&#28385;&#24847;&#65292;&#25105;&#20204;&#20801;&#35768;&#20182;&#20204;&#20351;&#29992;&#23545;&#20107;&#23454;&#30340;&#21453;&#38382;&#26469;&#36136;&#30097;&#25152;&#25552;&#20986;&#30340;&#20998;&#37197;&#12290;&#36890;&#36807;&#20351;&#29992;&#27169;&#25311;&#35848;&#21028;&#30340;&#19968;&#37096;&#20998;&#65292;&#25105;&#20204;&#33021;&#22815;&#25552;&#20379;&#23545;&#27604;&#35299;&#37322;&#65292;&#26368;&#23567;&#38480;&#24230;&#22320;&#25552;&#20379;&#26377;&#20851;&#20854;&#20182;&#25104;&#26412;&#30340;&#20449;&#24687;&#20197;&#21453;&#39539;&#20182;&#20204;&#30340;&#20551;&#35774;&#12290;&#36890;&#36807;&#20154;&#31867;&#30740;&#31350;&#65292;&#25105;&#20204;&#34920;&#26126;&#65288;1&#65289;&#20351;&#29992;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#20986;&#30340;&#20998;&#37197;&#23545;&#22823;&#22810;&#25968;&#20154;&#26469;&#35828;&#26174;&#24471;&#20844;&#24179;&#65292;&#65288;2&#65289;&#24403;&#25552;&#20986;&#21453;&#20107;&#23454;&#24773;&#20917;&#26102;&#65292;&#29983;&#25104;&#30340;&#35299;&#37322;&#26131;&#20110;&#29702;&#35299;&#24182;&#19988;&#20196;&#20154;&#20449;&#26381;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20174;&#23454;&#35777;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#19981;&#21516;&#31867;&#22411;&#30340;&#19981;&#23436;&#25972;&#24615;&#23545;&#35299;&#37322;-&#23398;&#20064;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Task allocation is an important problem in multi-agent systems. It becomes more challenging when the team-members are humans with imperfect knowledge about their teammates' costs and the overall performance metric. In this paper, we propose a centralized Artificial Intelligence Task Allocation (AITA) that simulates a negotiation and produces a negotiation-aware explicable task allocation. If a team-member is unhappy with the proposed allocation, we allow them to question the proposed allocation using a counterfactual. By using parts of the simulated negotiation, we are able to provide contrastive explanations that provide minimum information about other's cost to refute their foil. With human studies, we show that (1) the allocation proposed using our method appears fair to the majority, and (2) when a counterfactual is raised, explanations generated are easy to comprehend and convincing. Finally, we empirically study the effect of different kinds of incompleteness on the explanation-l
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22635;&#34917;&#20102;&#20851;&#20110;&#21152;&#26435;A*&#23376;&#20248;&#35299;&#35823;&#24046;&#30340;&#30740;&#31350;&#31354;&#30333;&#65292;&#35777;&#26126;&#20854;&#27425;&#20248;&#24615;&#36793;&#30028;&#24448;&#24448;&#36828;&#36828;&#20302;&#20110;&#26368;&#20248;&#35299;&#30340;W&#20493;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#29992;&#26041;&#27861;&#26469;&#25913;&#36827;&#35813;&#31639;&#27861;&#30340;&#27714;&#35299;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/1905.11346</link><description>&lt;p&gt;
&#21152;&#26435;A*&#23376;&#20248;&#35299;&#30340;&#35823;&#24046;&#20998;&#26512;&#19982;&#20462;&#27491;&#65288;&#25193;&#23637;&#29256;&#65289;
&lt;/p&gt;
&lt;p&gt;
Error Analysis and Correction for Weighted A*'s Suboptimality (Extended Version). (arXiv:1905.11346v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1905.11346
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22635;&#34917;&#20102;&#20851;&#20110;&#21152;&#26435;A*&#23376;&#20248;&#35299;&#35823;&#24046;&#30340;&#30740;&#31350;&#31354;&#30333;&#65292;&#35777;&#26126;&#20854;&#27425;&#20248;&#24615;&#36793;&#30028;&#24448;&#24448;&#36828;&#36828;&#20302;&#20110;&#26368;&#20248;&#35299;&#30340;W&#20493;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#29992;&#26041;&#27861;&#26469;&#25913;&#36827;&#35813;&#31639;&#27861;&#30340;&#27714;&#35299;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21152;&#26435;A*&#65288;wA*&#65289;&#26159;&#19968;&#31181;&#24191;&#27867;&#24212;&#29992;&#20110;&#35299;&#20915;&#35745;&#21010;&#21644;&#25628;&#32034;&#38382;&#39064;&#30340;&#24555;&#36895;&#20294;&#27425;&#20248;&#31639;&#27861;&#12290;&#35813;&#31639;&#27861;&#20135;&#29983;&#30340;&#35299;&#30340;&#25104;&#26412;&#34987;&#20445;&#35777;&#19981;&#36229;&#36807;&#26368;&#20248;&#35299;&#25104;&#26412;&#30340;W&#20493;&#65292;&#20854;&#20013;W&#26159;wA*&#22312;&#20248;&#21270;&#24320;&#25918;&#33410;&#28857;&#26102;&#25152;&#20351;&#29992;&#30340;&#26435;&#37325;&#12290;&#22240;&#27492;&#65292;W&#26159;wA*&#20135;&#29983;&#30340;&#35299;&#30340;&#27425;&#20248;&#24615;&#36793;&#30028;&#12290;&#26222;&#36941;&#35748;&#20026;&#65292;&#36825;&#20010;&#30028;&#38480;&#19981;&#22826;&#20934;&#30830;&#65292;wA*&#35299;&#30340;&#23454;&#38469;&#27425;&#20248;&#24615;&#24448;&#24448;&#36828;&#36828;&#23567;&#20110;W&#20493;&#30340;&#26368;&#20248;&#35299;&#12290;&#28982;&#32780;&#65292;&#25903;&#25345;&#35813;&#35266;&#28857;&#30340;&#20986;&#29256;&#29289;&#24456;&#23569;&#65292;&#20063;&#27809;&#26377;&#29616;&#26377;&#30340;&#35299;&#37322;&#20026;&#20160;&#20040;W&#26159;&#19968;&#20010;&#24456;&#24046;&#30340;&#30028;&#38480;&#12290;&#26412;&#25991;&#22635;&#34917;&#20102;&#25991;&#29486;&#20013;&#30340;&#36825;&#20123;&#31354;&#30333;&#12290;&#25105;&#20204;&#24320;&#22987;&#36827;&#34892;&#22823;&#35268;&#27169;&#23454;&#39564;&#65292;&#35777;&#26126;&#22312;&#21508;&#31181;&#39046;&#22495;&#21644;&#30456;&#24212;&#30340;&#21551;&#21457;&#24335;&#31639;&#27861;&#20013;&#65292;W&#24448;&#24448;&#36828;&#36828;&#20302;&#20110;wA*&#35299;&#30340;&#30495;&#23454;&#27425;&#20248;&#24615;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20998;&#26512;&#24615;&#22320;&#35782;&#21035;&#20102;&#21487;&#33021;&#30340;&#35823;&#24046;&#28304;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#29992;&#26041;&#27861;&#65292;&#32416;&#27491;&#20102;&#20854;&#20013;&#20004;&#20010;&#28304;&#30340;&#35823;&#24046;&#65292;&#20174;&#32780;&#33719;&#24471;&#20102;&#26356;&#20934;&#30830;&#30340;wA*&#35299;&#30340;&#27425;&#20248;&#24615;&#36793;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
Weighted A* (wA*) is a widely used algorithm for rapidly, but suboptimally, solving planning and search problems. The cost of the solution it produces is guaranteed to be at most W times the optimal solution cost, where W is the weight wA* uses in prioritizing open nodes. W is therefore a suboptimality bound for the solution produced by wA*. There is broad consensus that this bound is not very accurate, that the actual suboptimality of wA*'s solution is often much less than W times optimal. However, there is very little published evidence supporting that view, and no existing explanation of why W is a poor bound. This paper fills in these gaps in the literature. We begin with a large-scale experiment demonstrating that, across a wide variety of domains and heuristics for those domains, W is indeed very often far from the true suboptimality of wA*'s solution. We then analytically identify the potential sources of error. Finally, we present a practical method for correcting for two of th
&lt;/p&gt;</description></item></channel></rss>