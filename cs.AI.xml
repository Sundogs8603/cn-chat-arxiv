<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25193;&#25955;&#27169;&#22411;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#31361;&#20986;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#26679;&#26412;&#36136;&#37327;&#21644;&#35757;&#32451;&#31283;&#23450;&#24615;&#26041;&#38754;&#30340;&#20248;&#21183;&#25913;&#36827;&#20102;&#24378;&#21270;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#12290;&#35813;&#32508;&#36848;&#25552;&#20379;&#20102;&#36825;&#19968;&#26032;&#20852;&#39046;&#22495;&#21457;&#23637;&#30340;&#27010;&#36848;&#65292;&#24182;&#25506;&#35752;&#20102;&#25193;&#25955;&#27169;&#22411;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20998;&#31867;&#27861;&#21644;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2311.01223</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#30340;&#25193;&#25955;&#27169;&#22411;: &#19968;&#20221;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Diffusion Models for Reinforcement Learning: A Survey. (arXiv:2311.01223v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01223
&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25193;&#25955;&#27169;&#22411;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#31361;&#20986;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#26679;&#26412;&#36136;&#37327;&#21644;&#35757;&#32451;&#31283;&#23450;&#24615;&#26041;&#38754;&#30340;&#20248;&#21183;&#25913;&#36827;&#20102;&#24378;&#21270;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#12290;&#35813;&#32508;&#36848;&#25552;&#20379;&#20102;&#36825;&#19968;&#26032;&#20852;&#39046;&#22495;&#21457;&#23637;&#30340;&#27010;&#36848;&#65292;&#24182;&#25506;&#35752;&#20102;&#25193;&#25955;&#27169;&#22411;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20998;&#31867;&#27861;&#21644;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#19968;&#31181;&#31361;&#20986;&#30340;&#29983;&#25104;&#27169;&#22411;&#31867;&#21035;&#24050;&#32463;&#20986;&#29616;&#65292;&#36229;&#36234;&#20102;&#20197;&#24448;&#26041;&#27861;&#22312;&#26679;&#26412;&#36136;&#37327;&#21644;&#35757;&#32451;&#31283;&#23450;&#24615;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#25193;&#25955;&#27169;&#22411;&#22312;&#25913;&#36827;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#35299;&#20915;&#26041;&#26696;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#65292;&#21253;&#25324;&#20316;&#20026;&#36712;&#36857;&#35268;&#21010;&#22120;&#12289;&#34920;&#36798;&#33021;&#21147;&#20016;&#23500;&#30340;&#31574;&#30053;&#31867;&#21035;&#12289;&#25968;&#25454;&#21512;&#25104;&#22120;&#31561;&#12290;&#26412;&#32508;&#36848;&#26088;&#22312;&#25552;&#20379;&#35813;&#26032;&#20852;&#39046;&#22495;&#21457;&#23637;&#30340;&#27010;&#36848;&#65292;&#24182;&#24076;&#26395;&#33021;&#21551;&#21457;&#26032;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23457;&#26597;&#20102;&#24403;&#21069;RL&#31639;&#27861;&#36935;&#21040;&#30340;&#19968;&#20123;&#25361;&#25112;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#26681;&#25454;&#25193;&#25955;&#27169;&#22411;&#22312;RL&#20013;&#25152;&#25198;&#28436;&#30340;&#35282;&#33394;&#65292;&#25552;&#20986;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#20998;&#31867;&#27861;&#65292;&#24182;&#25506;&#35752;&#20102;&#22914;&#20309;&#35299;&#20915;&#29616;&#26377;&#25361;&#25112;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#27010;&#36848;&#20102;&#25193;&#25955;&#27169;&#22411;&#22312;&#21508;&#31181;&#19982;RL&#30456;&#20851;&#20219;&#21153;&#20013;&#30340;&#25104;&#21151;&#24212;&#29992;&#65292;&#24182;&#35752;&#35770;&#20102;&#24403;&#21069;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#24635;&#32467;&#20102;&#36825;&#39033;&#32508;&#36848;&#65292;&#24182;&#25552;&#20986;&#20102;&#23545;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#30340;&#35265;&#35299;&#65292;&#37325;&#28857;&#26159;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#21644;&#24212;&#29992;&#25193;&#25955;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models have emerged as a prominent class of generative models, surpassing previous methods regarding sample quality and training stability. Recent works have shown the advantages of diffusion models in improving reinforcement learning (RL) solutions, including as trajectory planners, expressive policy classes, data synthesizers, etc. This survey aims to provide an overview of the advancements in this emerging field and hopes to inspire new avenues of research. First, we examine several challenges encountered by current RL algorithms. Then, we present a taxonomy of existing methods based on the roles played by diffusion models in RL and explore how the existing challenges are addressed. We further outline successful applications of diffusion models in various RL-related tasks while discussing the limitations of current approaches. Finally, we conclude the survey and offer insights into future research directions, focusing on enhancing model performance and applying diffusion m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#35270;&#35273;&#20016;&#23500;&#30340;&#25991;&#26723;&#23454;&#20307;&#26816;&#32034;&#20219;&#21153;&#20013;&#30340;&#20010;&#24615;&#21270;&#22810;&#27169;&#24577;&#23569;&#26679;&#26412;&#23398;&#20064;&#65292;&#35299;&#20915;&#20102;&#23454;&#20307;&#32423;&#21035;&#23569;&#26679;&#26412;&#24773;&#26223;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2311.00693</link><description>&lt;p&gt;
&#20010;&#24615;&#21270;&#22810;&#27169;&#24577;&#23569;&#26679;&#26412;&#23398;&#20064;&#22312;&#35270;&#35273;&#20016;&#23500;&#30340;&#25991;&#26723;&#23454;&#20307;&#26816;&#32034;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
On Task-personalized Multimodal Few-shot Learning for Visually-rich Document Entity Retrieval. (arXiv:2311.00693v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00693
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#35270;&#35273;&#20016;&#23500;&#30340;&#25991;&#26723;&#23454;&#20307;&#26816;&#32034;&#20219;&#21153;&#20013;&#30340;&#20010;&#24615;&#21270;&#22810;&#27169;&#24577;&#23569;&#26679;&#26412;&#23398;&#20064;&#65292;&#35299;&#20915;&#20102;&#23454;&#20307;&#32423;&#21035;&#23569;&#26679;&#26412;&#24773;&#26223;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#20016;&#23500;&#30340;&#25991;&#26723;&#23454;&#20307;&#26816;&#32034;(VDER)&#26159;&#19968;&#31181;&#20174;&#21457;&#31080;&#21644;&#25910;&#25454;&#31561;&#25991;&#26723;&#22270;&#20687;&#20013;&#25552;&#21462;&#20851;&#38190;&#20449;&#24687;(&#22914;&#26085;&#26399;&#12289;&#22320;&#22336;)&#30340;&#37325;&#35201;&#20219;&#21153;&#65292;&#22312;&#24037;&#19994;&#32423;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24212;&#29992;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#19981;&#26029;&#28044;&#29616;&#30340;&#26032;&#30340;&#25991;&#26723;&#31867;&#22411;&#65292;&#27599;&#31181;&#31867;&#22411;&#37117;&#26377;&#20854;&#29420;&#29305;&#30340;&#23454;&#20307;&#31867;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#29420;&#29305;&#30340;&#25361;&#25112;&#65306;&#35768;&#22810;&#25991;&#26723;&#21253;&#21547;&#20102;&#20165;&#20986;&#29616;&#20960;&#27425;&#30340;&#26410;&#30693;&#23454;&#20307;&#31867;&#22411;&#12290;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#38656;&#35201;&#27169;&#22411;&#20855;&#26377;&#23569;&#26679;&#26412;&#23398;&#20064;&#23454;&#20307;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#20197;&#21069;&#30340;&#23569;&#26679;&#26412;VDER&#24037;&#20316;&#20027;&#35201;&#22312;&#25991;&#26723;&#32423;&#21035;&#19978;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#20351;&#29992;&#39044;&#23450;&#20041;&#30340;&#20840;&#23616;&#23454;&#20307;&#31354;&#38388;&#65292;&#27809;&#26377;&#32771;&#34385;&#21040;&#23454;&#20307;&#32423;&#21035;&#30340;&#23569;&#26679;&#26412;&#24773;&#26223;&#65306;&#30446;&#26631;&#23454;&#20307;&#31867;&#22411;&#30001;&#27599;&#20010;&#20219;&#21153;&#36827;&#34892;&#26412;&#22320;&#20010;&#24615;&#21270;&#65292;&#24182;&#19988;&#23454;&#20307;&#20986;&#29616;&#22312;&#25991;&#26723;&#20013;&#24046;&#24322;&#24456;&#22823;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#26410;&#24320;&#21457;&#30340;&#24773;&#20917;&#65292;&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23454;&#20307;&#32423;&#21035;&#23569;&#26679;&#26412;VDER&#20219;&#21153;&#12290;&#25361;&#25112;&#22312;&#20110;&#27599;&#20010;&#20219;&#21153;&#30340;&#26631;&#31614;&#31354;&#38388;&#30340;&#21807;&#19968;&#24615;&#21644;&#22686;&#38271;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visually-rich document entity retrieval (VDER), which extracts key information (e.g. date, address) from document images like invoices and receipts, has become an important topic in industrial NLP applications. The emergence of new document types at a constant pace, each with its unique entity types, presents a unique challenge: many documents contain unseen entity types that occur only a couple of times. Addressing this challenge requires models to have the ability of learning entities in a few-shot manner. However, prior works for Few-shot VDER mainly address the problem at the document level with a predefined global entity space, which doesn't account for the entity-level few-shot scenario: target entity types are locally personalized by each task and entity occurrences vary significantly among documents. To address this unexplored scenario, this paper studies a novel entity-level few-shot VDER task. The challenges lie in the uniqueness of the label space for each task and the incre
&lt;/p&gt;</description></item><item><title>JADE&#26159;&#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#20998;&#26512;&#30340;LLM&#23433;&#20840;&#35780;&#20272;&#24179;&#21488;&#65292;&#33021;&#22815;&#30772;&#22351;&#24191;&#27867;&#20351;&#29992;&#30340;&#20013;&#25991;&#21644;&#33521;&#25991;LLM&#65292;&#24182;&#29983;&#25104;&#39640;&#24230;&#23041;&#32961;&#30340;&#19981;&#23433;&#20840;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2311.00286</link><description>&lt;p&gt;
JADE&#65306;&#22522;&#20110;&#35821;&#35328;&#30340;LLM&#23433;&#20840;&#35780;&#20272;&#24179;&#21488;
&lt;/p&gt;
&lt;p&gt;
JADE: A Linguistic-based Safety Evaluation Platform for LLM. (arXiv:2311.00286v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00286
&lt;/p&gt;
&lt;p&gt;
JADE&#26159;&#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#20998;&#26512;&#30340;LLM&#23433;&#20840;&#35780;&#20272;&#24179;&#21488;&#65292;&#33021;&#22815;&#30772;&#22351;&#24191;&#27867;&#20351;&#29992;&#30340;&#20013;&#25991;&#21644;&#33521;&#25991;LLM&#65292;&#24182;&#29983;&#25104;&#39640;&#24230;&#23041;&#32961;&#30340;&#19981;&#23433;&#20840;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;JADE&#65292;&#19968;&#31181;&#38024;&#23545;&#35821;&#35328;&#20998;&#26512;&#30340;&#27169;&#31946;&#27979;&#35797;&#24179;&#21488;&#65292;&#36890;&#36807;&#22686;&#24378;&#31181;&#23376;&#38382;&#39064;&#30340;&#35821;&#35328;&#22797;&#26434;&#24615;&#65292;&#21516;&#26102;&#24182;&#22987;&#32456;&#33021;&#22815;&#30772;&#22351;&#24191;&#27867;&#20351;&#29992;&#30340;&#19977;&#31867;LLM&#65306;&#20843;&#20010;&#24320;&#28304;&#20013;&#25991;LLM&#65292;&#20845;&#20010;&#21830;&#19994;&#20013;&#25991;LLM&#21644;&#22235;&#20010;&#21830;&#19994;&#33521;&#25991;LLM&#12290;JADE&#20026;&#36825;&#19977;&#31867;LLM&#29983;&#25104;&#20102;&#19977;&#20010;&#23433;&#20840;&#22522;&#20934;&#65292;&#20854;&#20013;&#21253;&#21547;&#39640;&#24230;&#23041;&#32961;&#30340;&#19981;&#23433;&#20840;&#38382;&#39064;&#65306;&#36825;&#20123;&#38382;&#39064;&#21487;&#20197;&#21516;&#26102;&#35302;&#21457;&#22810;&#20010;LLM&#30340;&#26377;&#23475;&#29983;&#25104;&#65292;&#24179;&#22343;&#19981;&#23433;&#20840;&#29983;&#25104;&#27604;&#20363;&#20026;70%&#65288;&#35831;&#21442;&#35265;&#19979;&#34920;&#65289;&#65292;&#21516;&#26102;&#36825;&#20123;&#38382;&#39064;&#20173;&#28982;&#26159;&#33258;&#28982;&#12289;&#27969;&#30021;&#19988;&#20445;&#30041;&#20102;&#26680;&#24515;&#30340;&#19981;&#23433;&#20840;&#35821;&#20041;&#12290;&#25105;&#20204;&#22312;&#20197;&#19979;&#38142;&#25509;&#20013;&#21457;&#24067;&#20102;&#23545;&#21830;&#19994;&#33521;&#25991;LLM&#21644;&#24320;&#28304;&#33521;&#25991;LLM&#29983;&#25104;&#30340;&#22522;&#20934;&#28436;&#31034;&#65306;https://github.com/whitzard-ai/jade-db&#12290;&#23545;&#20110;&#23545;JADE&#29983;&#25104;&#30340;&#26356;&#22810;&#38382;&#39064;&#24863;&#20852;&#36259;&#30340;&#35835;&#32773;&#65292;&#35831;&#19982;&#25105;&#20204;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present \textit{JADE}, a targeted linguistic fuzzing platform which strengthens the linguistic complexity of seed questions to simultaneously and consistently break a wide range of widely-used LLMs categorized in three groups: eight open-sourced Chinese, six commercial Chinese and four commercial English LLMs. JADE generates three safety benchmarks for the three groups of LLMs, which contain unsafe questions that are highly threatening: the questions simultaneously trigger harmful generation of multiple LLMs, with an average unsafe generation ratio of \textbf{$70\%$} (please see the table below), while are still natural questions, fluent and preserving the core unsafe semantics. We release the benchmark demos generated for commercial English LLMs and open-sourced English LLMs in the following link: https://github.com/whitzard-ai/jade-db. For readers who are interested in evaluating on more questions generated by JADE, please contact us.  \textit{JADE} is based on Noam
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#38477;&#27700;&#21518;&#22788;&#29702;&#26041;&#27861;&#65292;&#20351;&#29992;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#21644;&#36716;&#31227;&#23398;&#20064;&#26469;&#25552;&#39640;&#25968;&#20540;&#22825;&#27668;&#39044;&#25253;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#21306;&#22495;&#38477;&#27700;&#26657;&#27491;&#26041;&#38754;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.20187</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#29992;&#20110;&#38477;&#27700;&#21518;&#22788;&#29702;
&lt;/p&gt;
&lt;p&gt;
Self-supervised Pre-training for Precipitation Post-processor. (arXiv:2310.20187v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20187
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#38477;&#27700;&#21518;&#22788;&#29702;&#26041;&#27861;&#65292;&#20351;&#29992;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#21644;&#36716;&#31227;&#23398;&#20064;&#26469;&#25552;&#39640;&#25968;&#20540;&#22825;&#27668;&#39044;&#25253;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#21306;&#22495;&#38477;&#27700;&#26657;&#27491;&#26041;&#38754;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#39044;&#38450;&#21361;&#38505;&#22825;&#27668;&#20107;&#20214;&#65292;&#30830;&#20445;&#20805;&#36275;&#30340;&#23616;&#22320;&#38477;&#27700;&#39044;&#25253;&#25552;&#21069;&#26102;&#38388;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#20840;&#29699;&#21464;&#26262;&#24341;&#36215;&#30340;&#27668;&#20505;&#21464;&#21270;&#22686;&#21152;&#20102;&#20934;&#30830;&#39044;&#27979;&#20005;&#37325;&#38477;&#27700;&#20107;&#20214;&#65288;&#22914;&#26292;&#38632;&#65289;&#30340;&#25361;&#25112;&#12290;&#26412;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#38477;&#27700;&#21518;&#22788;&#29702;&#26041;&#27861;&#65292;&#29992;&#20110;&#25968;&#20540;&#22825;&#27668;&#39044;&#25253;&#65288;NWP&#65289;&#27169;&#22411;&#12290;&#38477;&#27700;&#21518;&#22788;&#29702;&#21253;&#25324;&#65288;i&#65289;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#65292;&#20854;&#20013;&#32534;&#30721;&#22120;&#30340;&#21442;&#25968;&#22312;&#22823;&#27668;&#29289;&#29702;&#39046;&#22495;&#30340;&#36974;&#34109;&#21464;&#37327;&#37325;&#26500;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#20197;&#21450;&#65288;ii&#65289;&#20174;&#39044;&#35757;&#32451;&#30340;&#32534;&#30721;&#22120;&#20013;&#36716;&#31227;&#23398;&#20064;&#21040;&#38477;&#27700;&#20998;&#21106;&#20219;&#21153;&#65288;&#30446;&#26631;&#39046;&#22495;&#65289;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#21551;&#21457;&#24335;&#26631;&#35760;&#26041;&#27861;&#65292;&#20197;&#26377;&#25928;&#22320;&#35757;&#32451;&#31867;&#21035;&#19981;&#24179;&#34913;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#22312;&#21306;&#22495;NWP&#20013;&#30340;&#38477;&#27700;&#26657;&#27491;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Securing sufficient forecast lead time for local precipitation is essential for preventing hazardous weather events. Nonetheless, global warming-induced climate change is adding to the challenge of accurately predicting severe precipitation events, such as heavy rainfall. In this work, we propose a deep learning-based precipitation post-processor approach to numerical weather prediction (NWP) models. The precipitation post-processor consists of (i) self-supervised pre-training, where parameters of encoder are pre-trained on the reconstruction of masked variables of the atmospheric physics domain, and (ii) transfer learning on precipitation segmentation tasks (target domain) from the pre-trained encoder. We also introduce a heuristic labeling approach for effectively training class-imbalanced datasets. Our experiment results in precipitation correction for regional NWP show that the proposed method outperforms other approaches.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20998;&#26512;&#21477;&#23376;&#30340;&#24847;&#20041;&#32467;&#26500;&#26041;&#38754;&#30340;&#25104;&#21151;&#21644;&#38480;&#21046;&#65292;&#21457;&#29616;&#27169;&#22411;&#22312;&#29983;&#25104;&#21644;&#37325;&#26500;&#25277;&#35937;&#24847;&#20041;&#34920;&#31034;&#65288;AMR&#65289;&#26041;&#38754;&#34920;&#29616;&#20986;&#19968;&#23450;&#30340;&#33021;&#21147;&#65292;&#20294;&#22312;&#22797;&#26434;&#30340;&#21477;&#23376;&#32467;&#26500;&#25110;&#35821;&#20041;&#25512;&#29702;&#20219;&#21153;&#20013;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.17793</link><description>&lt;p&gt;
"&#24744;&#26159;&#19968;&#20301;&#19987;&#23478;&#35821;&#35328;&#27880;&#37322;&#32773;"&#65306;&#20316;&#20026;&#25277;&#35937;&#24847;&#20041;&#34920;&#31034;&#20998;&#26512;&#22120;&#30340;LLMs&#30340;&#38480;&#21046;
&lt;/p&gt;
&lt;p&gt;
"You Are An Expert Linguistic Annotator": Limits of LLMs as Analyzers of Abstract Meaning Representation. (arXiv:2310.17793v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17793
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20998;&#26512;&#21477;&#23376;&#30340;&#24847;&#20041;&#32467;&#26500;&#26041;&#38754;&#30340;&#25104;&#21151;&#21644;&#38480;&#21046;&#65292;&#21457;&#29616;&#27169;&#22411;&#22312;&#29983;&#25104;&#21644;&#37325;&#26500;&#25277;&#35937;&#24847;&#20041;&#34920;&#31034;&#65288;AMR&#65289;&#26041;&#38754;&#34920;&#29616;&#20986;&#19968;&#23450;&#30340;&#33021;&#21147;&#65292;&#20294;&#22312;&#22797;&#26434;&#30340;&#21477;&#23376;&#32467;&#26500;&#25110;&#35821;&#20041;&#25512;&#29702;&#20219;&#21153;&#20013;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#35821;&#35328;&#20351;&#29992;&#26041;&#38754;&#26174;&#31034;&#20986;&#20102;&#20196;&#20154;&#24778;&#35766;&#30340;&#29087;&#32451;&#24230;&#21644;&#27969;&#30021;&#24615;&#12290;&#36825;&#26159;&#21542;&#24847;&#21619;&#30528;&#23427;&#20204;&#20063;&#24050;&#32463;&#33719;&#24471;&#20102;&#20851;&#20110;&#35821;&#35328;&#30340;&#28145;&#21051;&#35821;&#35328;&#30693;&#35782;&#65292;&#20197;&#33267;&#20110;&#23427;&#20204;&#21487;&#20197;&#20805;&#24403;"&#19987;&#23478;&#35821;&#35328;&#27880;&#37322;&#32773;"&#65311;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#32771;&#23519;&#20102;GPT-3&#12289;ChatGPT&#21644;GPT-4&#27169;&#22411;&#22312;&#21477;&#23376;&#24847;&#20041;&#32467;&#26500;&#20998;&#26512;&#20013;&#30340;&#25104;&#21151;&#21644;&#38480;&#21046;&#65292;&#37325;&#28857;&#20851;&#27880;&#20102;&#25277;&#35937;&#24847;&#20041;&#34920;&#31034;&#65288;AMR&#65307;Banarescu&#31561;&#20154;&#65292;2013&#65289;&#20998;&#26512;&#24418;&#24335;&#20027;&#20041;&#65292;&#35813;&#24418;&#24335;&#20027;&#20041;&#25552;&#20379;&#20102;&#20016;&#23500;&#30340;&#22270;&#24418;&#21270;&#21477;&#23376;&#24847;&#20041;&#32467;&#26500;&#34920;&#31034;&#65292;&#21516;&#26102;&#20174;&#34920;&#38754;&#24418;&#24335;&#20013;&#25277;&#35937;&#20986;&#26469;&#12290;&#25105;&#20204;&#23558;&#27169;&#22411;&#22312;&#36825;&#31181;&#35821;&#20041;&#32467;&#26500;&#20998;&#26512;&#19978;&#30340;&#32467;&#26524;&#22312;&#20004;&#31181;&#24773;&#20917;&#19979;&#36827;&#34892;&#27604;&#36739;&#65306;1&#65289;&#22522;&#20110;&#38646;&#23556;&#21644;&#23569;&#23398;&#26679;&#26412;&#30340;AMR&#35299;&#26512;&#30340;&#30452;&#25509;&#29983;&#25104;&#65292;&#20197;&#21450;2&#65289;&#36890;&#36807;&#20803;&#35821;&#35328;&#33258;&#28982;&#35821;&#35328;&#26597;&#35810;&#65288;&#20363;&#22914;"&#30830;&#23450;&#35813;&#21477;&#23376;&#30340;&#20027;&#35201;&#20107;&#20214;&#65292;&#20197;&#21450;&#19982;&#35813;&#20107;&#20214;&#23545;&#24212;&#30340;&#35859;&#35789;"&#65289;&#38388;&#25509;&#30340;&#37096;&#20998;&#37325;&#26500;AMR&#12290;&#22312;&#36825;&#20123;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#21457;&#29616;&#27169;&#22411;&#33021;&#22815;&#37325;&#26032;&#29983;&#25104;&#27491;&#30830;&#30340;AMR&#35299;&#26512;&#65292;&#20294;&#22312;&#22797;&#26434;&#30340;&#21477;&#23376;&#32467;&#26500;&#25110;&#35821;&#20041;&#25512;&#29702;&#20219;&#21153;&#20013;&#20173;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) show amazing proficiency and fluency in the use of language. Does this mean that they have also acquired insightful linguistic knowledge about the language, to an extent that they can serve as an "expert linguistic annotator"? In this paper, we examine the successes and limitations of the GPT-3, ChatGPT, and GPT-4 models in analysis of sentence meaning structure, focusing on the Abstract Meaning Representation (AMR; Banarescu et al. 2013) parsing formalism, which provides rich graphical representations of sentence meaning structure while abstracting away from surface forms. We compare models' analysis of this semantic structure across two settings: 1) direct production of AMR parses based on zero- and few-shot prompts, and 2) indirect partial reconstruction of AMR via metalinguistic natural language queries (e.g., "Identify the primary event of this sentence, and the predicate corresponding to that event."). Across these settings, we find that models can re
&lt;/p&gt;</description></item><item><title>MGAS&#26159;&#19968;&#20010;&#22810;&#31890;&#24230;&#26550;&#26500;&#25628;&#32034;&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#36890;&#36807;&#23398;&#20064;&#29305;&#23450;&#31890;&#24230;&#32423;&#21035;&#30340;&#31163;&#25955;&#21270;&#20989;&#25968;&#65292;&#33258;&#36866;&#24212;&#22320;&#30830;&#23450;&#21097;&#20313;&#27604;&#20363;&#65292;&#20174;&#32780;&#23454;&#29616;&#21516;&#26102;&#20248;&#21270;&#27169;&#22411;&#22823;&#23567;&#21644;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.15074</link><description>&lt;p&gt;
MGAS: &#22810;&#31890;&#24230;&#26550;&#26500;&#25628;&#32034;&#20197;&#23454;&#29616;&#39640;&#25928;&#19988;&#26377;&#25928;&#30340;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
MGAS: Multi-Granularity Architecture Search for Effective and Efficient Neural Networks. (arXiv:2310.15074v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15074
&lt;/p&gt;
&lt;p&gt;
MGAS&#26159;&#19968;&#20010;&#22810;&#31890;&#24230;&#26550;&#26500;&#25628;&#32034;&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#36890;&#36807;&#23398;&#20064;&#29305;&#23450;&#31890;&#24230;&#32423;&#21035;&#30340;&#31163;&#25955;&#21270;&#20989;&#25968;&#65292;&#33258;&#36866;&#24212;&#22320;&#30830;&#23450;&#21097;&#20313;&#27604;&#20363;&#65292;&#20174;&#32780;&#23454;&#29616;&#21516;&#26102;&#20248;&#21270;&#27169;&#22411;&#22823;&#23567;&#21644;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#24494;&#20998;&#26550;&#26500;&#25628;&#32034;(DAS)&#36890;&#36807;&#26102;&#38388;&#39640;&#25928;&#30340;&#33258;&#21160;&#21270;&#25913;&#21464;&#20102;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#25628;&#32034;(NAS)&#30340;&#26041;&#24335;&#65292;&#20174;&#31163;&#25955;&#20505;&#36873;&#37319;&#26679;&#21644;&#35780;&#20272;&#36716;&#21464;&#20026;&#21487;&#24494;&#20998;&#36229;&#32593;&#32476;&#20248;&#21270;&#21644;&#31163;&#25955;&#21270;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;DAS&#26041;&#27861;&#35201;&#20040;&#21482;&#36827;&#34892;&#31895;&#31890;&#24230;&#30340;&#25805;&#20316;&#32423;&#25628;&#32034;&#65292;&#35201;&#20040;&#25163;&#21160;&#23450;&#20041;&#21097;&#20313;&#30340;&#32454;&#31890;&#24230;&#30340;&#26680;&#32423;&#21644;&#26435;&#37325;&#32423;&#21333;&#20301;&#30340;&#27604;&#20363;&#65292;&#20174;&#32780;&#26080;&#27861;&#21516;&#26102;&#20248;&#21270;&#27169;&#22411;&#22823;&#23567;&#21644;&#27169;&#22411;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#26041;&#27861;&#20026;&#20102;&#20943;&#23569;&#20869;&#23384;&#28040;&#32791;&#32780;&#29306;&#29298;&#20102;&#25628;&#32034;&#36136;&#37327;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22810;&#31890;&#24230;&#26550;&#26500;&#25628;&#32034;(MGAS)&#65292;&#36825;&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#20840;&#38754;&#32780;&#20869;&#23384;&#39640;&#25928;&#22320;&#25506;&#32034;&#22810;&#31890;&#24230;&#25628;&#32034;&#31354;&#38388;&#65292;&#21457;&#29616;&#26082;&#26377;&#25928;&#21448;&#39640;&#25928;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#23398;&#20064;&#20102;&#38024;&#23545;&#27599;&#20010;&#31890;&#24230;&#32423;&#21035;&#30340;&#31163;&#25955;&#21270;&#20989;&#25968;&#65292;&#26681;&#25454;&#19981;&#26029;&#28436;&#21270;&#30340;&#26550;&#26500;&#33258;&#36866;&#24212;&#22320;&#30830;&#23450;&#21097;&#20313;&#30340;&#27604;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
Differentiable architecture search (DAS) revolutionizes neural architecture search (NAS) with time-efficient automation, transitioning from discrete candidate sampling and evaluation to differentiable super-net optimization and discretization. However, existing DAS methods either only conduct coarse-grained operation-level search or manually define the remaining ratios for fine-grained kernel-level and weight-level units, which fail to simultaneously optimize model size and model performance. Furthermore, these methods compromise search quality to reduce memory consumption. To tackle these issues, we introduce multi-granularity architecture search (MGAS), a unified framework which aims to comprehensively and memory-efficiently explore the multi-granularity search space to discover both effective and efficient neural networks. Specifically, we learn discretization functions specific to each granularity level to adaptively determine the remaining ratios according to the evolving architec
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#24212;&#24615;&#30693;&#35782;&#20445;&#30041;&#21098;&#26525;&#31574;&#30053;&#65292;&#26088;&#22312;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#23545;&#25239;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#22312;&#21098;&#26525;&#36807;&#31243;&#20013;&#20445;&#30041;&#26356;&#22810;&#30340;&#39044;&#35757;&#32451;&#30693;&#35782;&#12290;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#23637;&#29616;&#20102;&#26356;&#22909;&#30340;&#24179;&#34913;&#12290;</title><link>http://arxiv.org/abs/2310.13191</link><description>&lt;p&gt;
&#26397;&#30528;&#40065;&#26834;&#21098;&#26525;&#65306;&#19968;&#31181;&#38754;&#21521;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#36866;&#24212;&#30693;&#35782;&#20445;&#30041;&#21098;&#26525;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Towards Robust Pruning: An Adaptive Knowledge-Retention Pruning Strategy for Language Models. (arXiv:2310.13191v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13191
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#24212;&#24615;&#30693;&#35782;&#20445;&#30041;&#21098;&#26525;&#31574;&#30053;&#65292;&#26088;&#22312;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#23545;&#25239;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#22312;&#21098;&#26525;&#36807;&#31243;&#20013;&#20445;&#30041;&#26356;&#22810;&#30340;&#39044;&#35757;&#32451;&#30693;&#35782;&#12290;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#23637;&#29616;&#20102;&#26356;&#22909;&#30340;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21098;&#26525;&#30446;&#26631;&#36817;&#26399;&#19981;&#20165;&#20165;&#23616;&#38480;&#20110;&#20934;&#30830;&#24615;&#21644;&#31232;&#30095;&#24615;&#65292;&#36824;&#21253;&#25324;&#23545;&#35821;&#35328;&#27169;&#22411;&#40065;&#26834;&#24615;&#30340;&#25552;&#21319;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#22312;&#25345;&#32493;&#22686;&#21152;&#27169;&#22411;&#31232;&#30095;&#24615;&#26102;&#65292;&#23545;&#25239;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#25552;&#21319;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#24182;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#36807;&#31243;&#12290;&#38543;&#30528;&#20154;&#20204;&#27493;&#20837;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26102;&#20195;&#65292;&#36825;&#20123;&#38382;&#39064;&#21464;&#24471;&#36234;&#26469;&#36234;&#31361;&#20986;&#12290;&#26412;&#25991;&#25552;&#20986;&#65292;&#35821;&#35328;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#19982;&#20854;&#28085;&#30422;&#30340;&#39044;&#35757;&#32451;&#30693;&#35782;&#31243;&#24230;&#25104;&#27491;&#27604;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21518;&#35757;&#32451;&#30340;&#21098;&#26525;&#31574;&#30053;&#65292;&#26088;&#22312;&#22312;&#21098;&#26525;&#36807;&#31243;&#20013;&#20445;&#30041;&#26356;&#22810;&#39044;&#35757;&#32451;&#30693;&#35782;&#65292;&#20197;&#24544;&#23454;&#22320;&#22797;&#21046;&#23494;&#38598;&#35821;&#35328;&#27169;&#22411;&#30340;&#23884;&#20837;&#31354;&#38388;&#21644;&#29305;&#24449;&#31354;&#38388;&#12290;&#22312;&#36825;&#20010;&#35774;&#32622;&#20013;&#65292;&#27599;&#19968;&#23618;&#30340;&#37325;&#26500;&#35823;&#24046;&#19981;&#20165;&#28304;&#33258;&#33258;&#36523;&#65292;&#36824;&#21253;&#25324;&#21069;&#38754;&#23618;&#30340;&#32047;&#31215;&#35823;&#24046;&#65292;&#28982;&#21518;&#36827;&#34892;&#33258;&#36866;&#24212;&#30340;&#30699;&#27491;&#12290;&#19982;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#22522;&#32447;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23637;&#29616;&#20102;&#26356;&#22909;&#30340;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
The pruning objective has recently extended beyond accuracy and sparsity to robustness in language models. Despite this, existing methods struggle to enhance robustness against adversarial attacks when continually increasing model sparsity and require a retraining process. As humans step into the era of large language models, these issues become increasingly prominent. This paper proposes that the robustness of language models is proportional to the extent of pre-trained knowledge they encompass. Accordingly, we introduce a post-training pruning strategy designed to faithfully replicate the embedding space and feature space of dense language models, aiming to conserve more pre-trained knowledge during the pruning process. In this setup, each layer's reconstruction error not only originates from itself but also includes cumulative error from preceding layers, followed by an adaptive rectification. Compared to other state-of-art baselines, our approach demonstrates a superior balance bet
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#32477;&#32536;&#20307;&#25913;&#21892;&#28909;&#21147;&#25193;&#25955;&#36827;&#34892;&#26080;&#30896;&#25758;&#36816;&#21160;&#35268;&#21010;&#30340;&#21435;&#22122;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#21333;&#19968;&#30340;&#35270;&#35273;&#36755;&#20837;&#65292;&#22312;&#25512;&#29702;&#26102;&#33021;&#22815;&#21516;&#26102;&#29983;&#25104;&#21487;&#36798;&#30446;&#26631;&#24182;&#35268;&#21010;&#36991;&#24320;&#38556;&#30861;&#29289;&#30340;&#36816;&#21160;&#36335;&#24452;&#65292;&#20855;&#26377;&#31283;&#20581;&#24615;&#21644;&#22810;&#27169;&#24577;&#36866;&#24212;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.12609</link><description>&lt;p&gt;
&#20351;&#29992;&#32477;&#32536;&#20307;&#25913;&#21892;&#28909;&#21147;&#25193;&#25955;&#36827;&#34892;&#26080;&#30896;&#25758;&#36816;&#21160;&#35268;&#21010;&#30340;&#21435;&#22122;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Denoising Heat-inspired Diffusion with Insulators for Collision Free Motion Planning. (arXiv:2310.12609v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12609
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#32477;&#32536;&#20307;&#25913;&#21892;&#28909;&#21147;&#25193;&#25955;&#36827;&#34892;&#26080;&#30896;&#25758;&#36816;&#21160;&#35268;&#21010;&#30340;&#21435;&#22122;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#21333;&#19968;&#30340;&#35270;&#35273;&#36755;&#20837;&#65292;&#22312;&#25512;&#29702;&#26102;&#33021;&#22815;&#21516;&#26102;&#29983;&#25104;&#21487;&#36798;&#30446;&#26631;&#24182;&#35268;&#21010;&#36991;&#24320;&#38556;&#30861;&#29289;&#30340;&#36816;&#21160;&#36335;&#24452;&#65292;&#20855;&#26377;&#31283;&#20581;&#24615;&#21644;&#22810;&#27169;&#24577;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20854;&#28789;&#27963;&#24615;&#21644;&#22810;&#27169;&#24577;&#24615;&#65292;&#25193;&#25955;&#27169;&#22411;&#22312;&#26426;&#22120;&#20154;&#39046;&#22495;&#20013;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#24378;&#22823;&#30340;&#24037;&#20855;&#12290;&#23613;&#31649;&#20854;&#20013;&#19968;&#20123;&#26041;&#27861;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#22797;&#26434;&#38382;&#39064;&#65292;&#20294;&#23427;&#20204;&#24448;&#24448;&#20005;&#37325;&#20381;&#36182;&#20110;&#25512;&#29702;&#26102;&#30340;&#38556;&#30861;&#29289;&#26816;&#27979;&#24182;&#38656;&#35201;&#39069;&#22806;&#30340;&#35774;&#22791;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#25512;&#29702;&#26102;&#33021;&#22815;&#20174;&#21333;&#19968;&#30340;&#35270;&#35273;&#36755;&#20837;&#20013;&#21516;&#26102;&#29983;&#25104;&#21487;&#36798;&#30446;&#26631;&#24182;&#35268;&#21010;&#36991;&#24320;&#38556;&#30861;&#29289;&#30340;&#36816;&#21160;&#36335;&#24452;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26680;&#24515;&#26159;&#23545;&#35757;&#32451;&#36807;&#31243;&#20013;&#26032;&#39062;&#30340;&#30896;&#25758;&#36991;&#20813;&#25193;&#25955;&#26680;&#36827;&#34892;&#20351;&#29992;&#12290;&#36890;&#36807;&#19982;&#34892;&#20026;&#20811;&#38534;&#21644;&#32463;&#20856;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#35780;&#20272;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#35777;&#26126;&#20102;&#20854;&#31283;&#20581;&#24615;&#12290;&#29305;&#21035;&#26159;&#22312;&#22810;&#27169;&#24577;&#29615;&#22659;&#20013;&#65292;&#23427;&#33021;&#22815;&#23548;&#33322;&#21040;&#30446;&#26631;&#24182;&#36991;&#24320;&#34987;&#38556;&#30861;&#29289;&#38459;&#25377;&#30340;&#19981;&#21487;&#36798;&#30446;&#26631;&#65292;&#21516;&#26102;&#30830;&#20445;&#36991;&#20813;&#30896;&#25758;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models have risen as a powerful tool in robotics due to their flexibility and multi-modality. While some of these methods effectively address complex problems, they often depend heavily on inference-time obstacle detection and require additional equipment. Addressing these challenges, we present a method that, during inference time, simultaneously generates only reachable goals and plans motions that avoid obstacles, all from a single visual input. Central to our approach is the novel use of a collision-avoiding diffusion kernel for training. Through evaluations against behavior-cloning and classical diffusion models, our framework has proven its robustness. It is particularly effective in multi-modal environments, navigating toward goals and avoiding unreachable ones blocked by obstacles, while ensuring collision avoidance.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#23545;BERT&#27169;&#22411;&#30340;&#24191;&#20041;&#24615;&#24433;&#21709;&#65292;&#21457;&#29616;&#22312;&#22266;&#23450;&#22823;&#23567;&#30340;&#35757;&#32451;&#26679;&#26412;&#19978;&#65292;&#26377;10-30\%&#30340;&#20154;&#20026;&#23545;&#25239;&#23454;&#20363;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#25991;&#26412;&#20998;&#31867;&#21644;&#20851;&#31995;&#25277;&#21462;&#20219;&#21153;&#30340;&#31934;&#24230;&#21644;F1&#20540;&#12290;</title><link>http://arxiv.org/abs/2310.08008</link><description>&lt;p&gt;
BERT&#24191;&#20041;&#24615;&#30340;&#24433;&#21709;&#65306;&#20154;&#20026;&#23545;&#25239;&#26679;&#26412;&#21644;&#21451;&#22909;&#26679;&#26412;&#30340;&#25928;&#26524;
&lt;/p&gt;
&lt;p&gt;
Effects of Human Adversarial and Affable Samples on BERT Generalizability. (arXiv:2310.08008v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08008
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#23545;BERT&#27169;&#22411;&#30340;&#24191;&#20041;&#24615;&#24433;&#21709;&#65292;&#21457;&#29616;&#22312;&#22266;&#23450;&#22823;&#23567;&#30340;&#35757;&#32451;&#26679;&#26412;&#19978;&#65292;&#26377;10-30\%&#30340;&#20154;&#20026;&#23545;&#25239;&#23454;&#20363;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#25991;&#26412;&#20998;&#31867;&#21644;&#20851;&#31995;&#25277;&#21462;&#20219;&#21153;&#30340;&#31934;&#24230;&#21644;F1&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;BERT&#30340;&#27169;&#22411;&#22312;&#39046;&#20808;&#27036;&#19978;&#34920;&#29616;&#24378;&#21170;&#65292;&#20294;&#22312;&#38656;&#35201;&#27867;&#21270;&#30340;&#23454;&#38469;&#22330;&#26223;&#20013;&#34920;&#29616;&#36739;&#24046;&#12290;&#26377;&#38480;&#30340;&#35757;&#32451;&#25968;&#25454;&#34987;&#35748;&#20026;&#26159;&#26426;&#22120;&#23398;&#20064;&#27867;&#21270;&#33021;&#21147;&#30340;&#20027;&#35201;&#38556;&#30861;&#12290;&#26412;&#25991;&#30740;&#31350;&#35757;&#32451;&#25968;&#25454;&#36136;&#37327;&#23545;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#30340;&#24433;&#21709;&#65292;&#32780;&#19981;&#26159;&#25968;&#37327;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#35757;&#32451;&#25968;&#25454;&#30340;&#20004;&#20010;&#29305;&#24449;&#65306;&#20154;&#20026;&#23545;&#25239;&#26679;&#26412;&#65288;&#20855;&#26377;&#30475;&#20284;&#24494;&#23567;&#24046;&#24322;&#20294;&#20855;&#26377;&#19981;&#21516;&#26631;&#31614;&#30340;&#26679;&#26412;&#23545;&#65289;&#21644;&#20154;&#20026;&#21451;&#22909;&#26679;&#26412;&#65288;&#20855;&#26377;&#24494;&#23567;&#24046;&#24322;&#20294;&#20855;&#26377;&#30456;&#21516;&#26631;&#31614;&#30340;&#26679;&#26412;&#23545;&#65289;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#22266;&#23450;&#22823;&#23567;&#30340;&#35757;&#32451;&#26679;&#26412;&#19978;&#65292;&#20197;10-30\%&#30340;&#20154;&#20026;&#23545;&#25239;&#23454;&#20363;&#20026;&#32463;&#39564;&#65292;&#21487;&#20197;&#25552;&#39640;&#25991;&#26412;&#20998;&#31867;&#21644;&#20851;&#31995;&#25277;&#21462;&#20219;&#21153;&#30340;&#31934;&#24230;&#21644;F1&#20540;&#26368;&#22810;20&#20010;&#30334;&#20998;&#28857;&#12290;&#36229;&#36807;&#27492;&#33539;&#22260;&#30340;&#22686;&#21152;&#23545;&#27169;&#22411;&#24615;&#33021;&#26080;&#26174;&#33879;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
BERT-based models have had strong performance on leaderboards, yet have been demonstrably worse in real-world settings requiring generalization. Limited quantities of training data is considered a key impediment to achieving generalizability in machine learning. In this paper, we examine the impact of training \textit{data quality}, not quantity, on a model's generalizability. We consider two characteristics of training data: the portion of human-adversarial (h-adversarial), i.e., sample pairs with seemingly minor differences but different ground-truth labels, and human-affable (h-affable) training samples, i.e., sample pairs with minor differences but the same ground-truth label. We find that for a fixed size of training samples, as a rule of thumb, having 10-30\% h-adversarial instances improves the precision, and therefore F1, by up to 20 points in the tasks of text classification and relation extraction. Increasing h-adversarials beyond this range can result in performance plateaus
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#25112;&#30053;&#20449;&#24687;&#20256;&#36882;&#28216;&#25103;&#20013;&#65292;&#21033;&#29992;&#29420;&#31435;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#35757;&#32451;&#30340;&#21457;&#36865;&#32773;&#21644;&#25509;&#25910;&#32773;&#21487;&#20197;&#25910;&#25947;&#21040;&#25509;&#36817;&#26368;&#20248;&#22343;&#34913;&#31574;&#30053;&#65292;&#24182;&#19988;&#22312;&#20195;&#29702;&#20043;&#38388;&#30340;&#21033;&#30410;&#20914;&#31361;&#19979;&#23454;&#29616;&#20102;&#26368;&#22823;&#21270;&#30340;&#36890;&#20449;&#12290;&#36825;&#19968;&#32467;&#35770;&#31283;&#20581;&#65292;&#24182;&#23545;&#20449;&#24687;&#20256;&#36882;&#28216;&#25103;&#20013;&#30340;&#22343;&#34913;&#36873;&#25321;&#29702;&#35770;&#12289;&#35745;&#31639;&#26426;&#31185;&#23398;&#20013;&#30340;&#31639;&#27861;&#38388;&#36890;&#20449;&#21644;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#24066;&#22330;&#20013;&#30340;&#32463;&#27982;&#23398;&#20135;&#29983;&#20102;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2310.07867</link><description>&lt;p&gt;
&#24265;&#20215;&#23545;&#35805;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Cheap Talking Algorithms. (arXiv:2310.07867v1 [econ.TH])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07867
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#25112;&#30053;&#20449;&#24687;&#20256;&#36882;&#28216;&#25103;&#20013;&#65292;&#21033;&#29992;&#29420;&#31435;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#35757;&#32451;&#30340;&#21457;&#36865;&#32773;&#21644;&#25509;&#25910;&#32773;&#21487;&#20197;&#25910;&#25947;&#21040;&#25509;&#36817;&#26368;&#20248;&#22343;&#34913;&#31574;&#30053;&#65292;&#24182;&#19988;&#22312;&#20195;&#29702;&#20043;&#38388;&#30340;&#21033;&#30410;&#20914;&#31361;&#19979;&#23454;&#29616;&#20102;&#26368;&#22823;&#21270;&#30340;&#36890;&#20449;&#12290;&#36825;&#19968;&#32467;&#35770;&#31283;&#20581;&#65292;&#24182;&#23545;&#20449;&#24687;&#20256;&#36882;&#28216;&#25103;&#20013;&#30340;&#22343;&#34913;&#36873;&#25321;&#29702;&#35770;&#12289;&#35745;&#31639;&#26426;&#31185;&#23398;&#20013;&#30340;&#31639;&#27861;&#38388;&#36890;&#20449;&#21644;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#24066;&#22330;&#20013;&#30340;&#32463;&#27982;&#23398;&#20135;&#29983;&#20102;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#27169;&#25311;&#29420;&#31435;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;&#20811;&#21171;&#31119;&#24503;&#21644;&#32034;&#36125;&#23572;&#65288;1982&#65289;&#30340;&#25112;&#30053;&#20449;&#24687;&#20256;&#36882;&#28216;&#25103;&#20013;&#30340;&#34892;&#20026;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#19968;&#20010;&#21457;&#36865;&#32773;&#21644;&#19968;&#20010;&#25509;&#25910;&#32773;&#19968;&#36215;&#36827;&#34892;&#35757;&#32451;&#65292;&#25910;&#25947;&#21040;&#25509;&#36817;&#28216;&#25103;&#20808;&#39564;&#26368;&#20248;&#22343;&#34913;&#30340;&#31574;&#30053;&#12290;&#22240;&#27492;&#65292;&#36890;&#20449;&#22312;&#19982;&#20195;&#29702;&#20043;&#38388;&#30340;&#21033;&#30410;&#20914;&#31361;&#31243;&#24230;&#32473;&#20986;&#30340;&#32435;&#20160;&#22343;&#34913;&#19979;&#65292;&#25353;&#29031;&#26368;&#22823;&#31243;&#24230;&#36827;&#34892;&#12290;&#36825;&#19968;&#32467;&#35770;&#23545;&#36229;&#21442;&#25968;&#21644;&#28216;&#25103;&#30340;&#22791;&#36873;&#35268;&#33539;&#31283;&#20581;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#20449;&#24687;&#20256;&#36882;&#28216;&#25103;&#20013;&#22343;&#34913;&#36873;&#25321;&#29702;&#35770;&#12289;&#35745;&#31639;&#26426;&#31185;&#23398;&#20013;&#31639;&#27861;&#38388;&#26032;&#20852;&#36890;&#20449;&#24037;&#20316;&#20197;&#21450;&#30001;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#32452;&#25104;&#30340;&#24066;&#22330;&#20013;&#30340;&#23467;&#26007;&#32463;&#27982;&#23398;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
We simulate behaviour of independent reinforcement learning algorithms playing the Crawford and Sobel (1982) game of strategic information transmission. We show that a sender and a receiver training together converge to strategies close to the exante optimal equilibrium of the game. Hence, communication takes place to the largest extent predicted by Nash equilibrium given the degree of conflict of interest between agents. The conclusion is shown to be robust to alternative specifications of the hyperparameters and of the game. We discuss implications for theories of equilibrium selection in information transmission games, for work on emerging communication among algorithms in computer science and for the economics of collusions in markets populated by artificially intelligent agents.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#31934;&#32454;&#35843;&#25972;&#29983;&#25104;&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;&#20998;&#23376;&#30340;&#20840;&#26032;&#35774;&#35745;&#20013;&#29983;&#25104;&#20855;&#26377;&#25152;&#38656;&#24615;&#36136;&#30340;&#20998;&#23376;&#32467;&#26500;&#65292;&#23637;&#29616;&#20986;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.05365</link><description>&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;Transformer&#30340;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#20998;&#23376;&#30340;&#20840;&#26032;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Molecular De Novo Design through Transformer-based Reinforcement Learning. (arXiv:2310.05365v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05365
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#31934;&#32454;&#35843;&#25972;&#29983;&#25104;&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;&#20998;&#23376;&#30340;&#20840;&#26032;&#35774;&#35745;&#20013;&#29983;&#25104;&#20855;&#26377;&#25152;&#38656;&#24615;&#36136;&#30340;&#20998;&#23376;&#32467;&#26500;&#65292;&#23637;&#29616;&#20986;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#31934;&#32454;&#35843;&#25972;&#22522;&#20110;Transformer&#30340;&#29983;&#25104;&#27169;&#22411;&#29992;&#20110;&#20998;&#23376;&#30340;&#20840;&#26032;&#35774;&#35745;&#30340;&#26041;&#27861;&#12290;&#21033;&#29992;Transformer&#30456;&#23545;&#20110;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#30340;&#20248;&#36234;&#24207;&#21015;&#23398;&#20064;&#33021;&#21147;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#26377;&#25928;&#22320;&#29983;&#25104;&#20855;&#26377;&#25152;&#38656;&#24615;&#36136;&#30340;&#20998;&#23376;&#32467;&#26500;&#12290;&#19982;&#20256;&#32479;&#30340;&#22522;&#20110;RNN&#30340;&#27169;&#22411;&#30456;&#27604;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#29983;&#25104;&#39044;&#27979;&#23545;&#22810;&#31181;&#29983;&#29289;&#38774;&#28857;&#20855;&#26377;&#27963;&#24615;&#30340;&#21270;&#21512;&#29289;&#26041;&#38754;&#34920;&#29616;&#20986;&#21331;&#36234;&#24615;&#33021;&#65292;&#25429;&#25417;&#20102;&#20998;&#23376;&#32467;&#26500;&#24207;&#21015;&#30340;&#38271;&#26399;&#20381;&#36182;&#24615;&#12290;&#35813;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#22312;&#35768;&#22810;&#20219;&#21153;&#20013;&#24471;&#21040;&#20102;&#35777;&#26126;&#65292;&#21253;&#25324;&#29983;&#25104;&#19982;&#26597;&#35810;&#32467;&#26500;&#31867;&#20284;&#30340;&#20998;&#23376;&#21644;&#29983;&#25104;&#20855;&#26377;&#29305;&#23450;&#23646;&#24615;&#30340;&#21270;&#21512;&#29289;&#65292;&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;&#22522;&#32447;&#30340;&#22522;&#20110;RNN&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#29992;&#20110;&#26725;&#25509;&#21270;&#23398;&#12289;&#20174;&#21333;&#20010;&#20998;&#23376;&#24320;&#22987;&#25193;&#23637;&#24211;&#65292;&#24182;&#29983;&#25104;&#20855;&#26377;&#39640;&#39044;&#27979;&#27963;&#24615;&#30340;&#21270;&#21512;&#29289;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we introduce a method to fine-tune a Transformer-based generative model for molecular de novo design. Leveraging the superior sequence learning capacity of Transformers over Recurrent Neural Networks (RNNs), our model can generate molecular structures with desired properties effectively. In contrast to the traditional RNN-based models, our proposed method exhibits superior performance in generating compounds predicted to be active against various biological targets, capturing long-term dependencies in the molecular structure sequence. The model's efficacy is demonstrated across numerous tasks, including generating analogues to a query structure and producing compounds with particular attributes, outperforming the baseline RNN-based methods. Our approach can be used for scaffold hopping, library expansion starting from a single molecule, and generating compounds with high predicted activity against biological targets.
&lt;/p&gt;</description></item><item><title>COPRA&#26159;&#19968;&#31181;&#38754;&#21521;&#24418;&#24335;&#23450;&#29702;&#35777;&#26126;&#30340;&#35821;&#35328;&#20195;&#29702;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#36890;&#36807;&#36873;&#25321;&#31574;&#30053;&#21644;&#26816;&#32034;&#23450;&#20041;&#21644;&#24341;&#29702;&#36827;&#34892;&#35777;&#26126;&#65292;&#22312;MiniF2F&#22522;&#20934;&#21644;Coq&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.04353</link><description>&lt;p&gt;
&#19968;&#31181;&#38754;&#21521;&#24418;&#24335;&#23450;&#29702;&#35777;&#26126;&#30340;&#35821;&#35328;&#20195;&#29702;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Language-Agent Approach to Formal Theorem-Proving. (arXiv:2310.04353v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04353
&lt;/p&gt;
&lt;p&gt;
COPRA&#26159;&#19968;&#31181;&#38754;&#21521;&#24418;&#24335;&#23450;&#29702;&#35777;&#26126;&#30340;&#35821;&#35328;&#20195;&#29702;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#36890;&#36807;&#36873;&#25321;&#31574;&#30053;&#21644;&#26816;&#32034;&#23450;&#20041;&#21644;&#24341;&#29702;&#36827;&#34892;&#35777;&#26126;&#65292;&#22312;MiniF2F&#22522;&#20934;&#21644;Coq&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#20195;&#29702;&#26159;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#26469;&#19982;&#22806;&#37096;&#29615;&#22659;&#36827;&#34892;&#20132;&#20114;&#30340;&#26041;&#27861;&#65292;&#26368;&#36817;&#34987;&#35748;&#20026;&#26159;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#25511;&#21046;&#20219;&#21153;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language agents, which use a large language model (LLM) capable of in-context learning to interact with an external environment, have recently emerged as a promising approach to control tasks. We present the first language-agent approach to formal theorem-proving. Our method, COPRA, uses a high-capacity, black-box LLM (GPT-4) as part of a policy for a stateful backtracking search. During the search, the policy can select proof tactics and retrieve lemmas and definitions from an external database. Each selected tactic is executed in the underlying proof framework, and the execution feedback is used to build the prompt for the next policy invocation. The search also tracks selected information from its history and uses it to reduce hallucinations and unnecessary LLM queries.  We evaluate COPRA on the miniF2F benchmark for Lean and a set of Coq tasks from the Compcert project. On these benchmarks, COPRA is significantly better than one-shot invocations of GPT-4, as well as state-of-the-ar
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#38750;&#22343;&#21248;&#37319;&#26679;&#31574;&#30053;&#23545;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#38646;&#26679;&#26412;&#27867;&#21270;&#33021;&#21147;&#30340;&#24433;&#21709;&#65292;&#36890;&#36807;&#27979;&#37327;&#20195;&#29702;&#30340;&#20869;&#37096;&#34920;&#31034;&#19982;&#35757;&#32451;&#23618;&#32423;&#38598;&#20043;&#38388;&#30340;&#30456;&#20114;&#20449;&#24687;&#65292;&#21457;&#29616;&#22522;&#20110;&#20540;&#25439;&#22833;&#20248;&#20808;&#32423;&#30340;&#33258;&#36866;&#24212;&#37319;&#26679;&#31574;&#30053;&#33021;&#26356;&#22909;&#22320;&#36991;&#20813;&#36807;&#25311;&#21512;&#12290;</title><link>http://arxiv.org/abs/2310.03494</link><description>&lt;p&gt;
&#22914;&#20309;&#27700;&#24179;&#37319;&#26679;&#36807;&#31243;&#24433;&#21709;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#38646;&#26679;&#26412;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
How the level sampling process impacts zero-shot generalisation in deep reinforcement learning. (arXiv:2310.03494v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03494
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#38750;&#22343;&#21248;&#37319;&#26679;&#31574;&#30053;&#23545;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#38646;&#26679;&#26412;&#27867;&#21270;&#33021;&#21147;&#30340;&#24433;&#21709;&#65292;&#36890;&#36807;&#27979;&#37327;&#20195;&#29702;&#30340;&#20869;&#37096;&#34920;&#31034;&#19982;&#35757;&#32451;&#23618;&#32423;&#38598;&#20043;&#38388;&#30340;&#30456;&#20114;&#20449;&#24687;&#65292;&#21457;&#29616;&#22522;&#20110;&#20540;&#25439;&#22833;&#20248;&#20808;&#32423;&#30340;&#33258;&#36866;&#24212;&#37319;&#26679;&#31574;&#30053;&#33021;&#26356;&#22909;&#22320;&#36991;&#20813;&#36807;&#25311;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38459;&#27490;&#24191;&#27867;&#37319;&#29992;&#36890;&#36807;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#35757;&#32451;&#30340;&#33258;&#20027;&#20195;&#29702;&#20195;&#29702;&#30340;&#20851;&#38190;&#23616;&#38480;&#26159;&#23427;&#20204;&#26377;&#38480;&#30340;&#36866;&#24212;&#26032;&#29615;&#22659;&#33021;&#21147;&#65292;&#21363;&#20351;&#36825;&#20123;&#29615;&#22659;&#19982;&#35757;&#32451;&#20013;&#36935;&#21040;&#30340;&#29615;&#22659;&#20855;&#26377;&#30456;&#20284;&#29305;&#24449;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20010;&#20307;&#29615;&#22659;&#23454;&#20363;&#25110;&#23618;&#32423;&#30340;&#38750;&#22343;&#21248;&#37319;&#26679;&#31574;&#30053;&#22914;&#20309;&#24433;&#21709;RL&#20195;&#29702;&#30340;&#38646;&#26679;&#26412;&#27867;&#21270;&#65288;ZSG&#65289;&#33021;&#21147;&#65292;&#24182;&#32771;&#34385;&#20102;&#20004;&#31181;&#22833;&#25928;&#27169;&#24335;&#65306;&#36807;&#25311;&#21512;&#21644;&#36807;&#24230;&#27867;&#21270;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#27979;&#37327;&#20102;&#20195;&#29702;&#30340;&#20869;&#37096;&#34920;&#31034;&#19982;&#35757;&#32451;&#23618;&#32423;&#38598;&#20043;&#38388;&#30340;&#30456;&#20114;&#20449;&#24687;&#65288;MI&#65289;&#65292;&#21457;&#29616;MI&#19982;&#23454;&#20363;&#30340;&#36807;&#25311;&#21512;&#30456;&#20851;&#24615;&#24456;&#24378;&#12290;&#19982;&#22343;&#21248;&#37319;&#26679;&#30456;&#27604;&#65292;&#22522;&#20110;&#20540;&#25439;&#22833;&#20248;&#20808;&#32423;&#30340;&#33258;&#36866;&#24212;&#37319;&#26679;&#31574;&#30053;&#26356;&#33021;&#26377;&#25928;&#22320;&#20445;&#25345;&#36739;&#20302;&#30340;MI&#65292;&#36825;&#20026;&#36825;&#31867;&#25216;&#26415;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#29702;&#35770;&#35299;&#37322;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#23558;&#27880;&#24847;&#21147;&#36716;&#21521;&#26080;&#30417;&#30563;&#29615;&#22659;&#35774;&#35745;&#65288;U&#65289;
&lt;/p&gt;
&lt;p&gt;
A key limitation preventing the wider adoption of autonomous agents trained via deep reinforcement learning (RL) is their limited ability to generalise to new environments, even when these share similar characteristics with environments encountered during training. In this work, we investigate how a non-uniform sampling strategy of individual environment instances, or levels, affects the zero-shot generalisation (ZSG) ability of RL agents, considering two failure modes: overfitting and over-generalisation. As a first step, we measure the mutual information (MI) between the agent's internal representation and the set of training levels, which we find to be well-correlated to instance overfitting. In contrast to uniform sampling, adaptive sampling strategies prioritising levels based on their value loss are more effective at maintaining lower MI, which provides a novel theoretical justification for this class of techniques. We then turn our attention to unsupervised environment design (U
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35774;&#35745;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#36719;&#22686;&#38271;&#26426;&#22120;&#20154;&#30340;&#36816;&#21160;&#38142;&#65292;&#25552;&#20379;&#26368;&#20339;&#26426;&#22120;&#20154;&#23610;&#23544;&#20197;&#35299;&#20915;&#29305;&#23450;&#20219;&#21153;&#12290;&#36825;&#31181;&#26041;&#27861;&#21033;&#29992;&#22522;&#20110;&#32676;&#20307;&#30340;&#20248;&#21270;&#31639;&#27861;&#65292;&#36991;&#20813;&#20102;&#19981;&#24517;&#35201;&#30340;&#26448;&#26009;&#21644;&#36164;&#28304;&#28010;&#36153;&#12290;</title><link>http://arxiv.org/abs/2310.03374</link><description>&lt;p&gt;
&#24179;&#38754;&#36719;&#22686;&#38271;&#26426;&#22120;&#20154;&#25805;&#32437;&#22120;&#30340;&#35774;&#35745;&#20248;&#21270;&#31243;&#24207;
&lt;/p&gt;
&lt;p&gt;
Design Optimizer for Planar Soft-Growing Robot Manipulators. (arXiv:2310.03374v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03374
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35774;&#35745;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#36719;&#22686;&#38271;&#26426;&#22120;&#20154;&#30340;&#36816;&#21160;&#38142;&#65292;&#25552;&#20379;&#26368;&#20339;&#26426;&#22120;&#20154;&#23610;&#23544;&#20197;&#35299;&#20915;&#29305;&#23450;&#20219;&#21153;&#12290;&#36825;&#31181;&#26041;&#27861;&#21033;&#29992;&#22522;&#20110;&#32676;&#20307;&#30340;&#20248;&#21270;&#31639;&#27861;&#65292;&#36991;&#20813;&#20102;&#19981;&#24517;&#35201;&#30340;&#26448;&#26009;&#21644;&#36164;&#28304;&#28010;&#36153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36719;&#22686;&#38271;&#26426;&#22120;&#20154;&#26159;&#19968;&#31181;&#21019;&#26032;&#35774;&#22791;&#65292;&#20855;&#26377;&#26893;&#29289;&#21551;&#21457;&#30340;&#29983;&#38271;&#29305;&#24615;&#26469;&#23548;&#33322;&#29615;&#22659;&#12290;&#24471;&#30410;&#20110;&#20854;&#36866;&#24212;&#29615;&#22659;&#30340;&#20307;&#22806;&#26234;&#33021;&#21644;&#28608;&#21169;&#19982;&#21046;&#36896;&#30340;&#26368;&#26032;&#21019;&#26032;&#65292;&#21487;&#20197;&#23558;&#23427;&#20204;&#29992;&#20110;&#29305;&#23450;&#30340;&#25805;&#32437;&#20219;&#21153;&#12290;&#36825;&#20123;&#35774;&#22791;&#30340;&#24212;&#29992;&#21253;&#25324;&#23545;&#33030;&#24369;/&#21361;&#38505;&#29615;&#22659;&#30340;&#25506;&#32034;&#12289;&#29289;&#20307;&#30340;&#25805;&#32437;&#25110;&#22312;&#23478;&#24237;&#29615;&#22659;&#20013;&#30340;&#21327;&#21161;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36719;&#22686;&#38271;&#26426;&#22120;&#20154;&#35774;&#35745;&#20248;&#21270;&#26041;&#27861;&#65292;&#23427;&#23558;&#22312;&#21046;&#36896;&#20043;&#21069;&#29992;&#20110;&#24314;&#35758;&#24037;&#31243;&#24072;&#25110;&#26426;&#22120;&#20154;&#35774;&#35745;&#29233;&#22909;&#32773;&#26500;&#24314;&#35299;&#20915;&#29305;&#23450;&#20219;&#21153;&#30340;&#26368;&#20339;&#26426;&#22120;&#20154;&#23610;&#23544;&#12290;&#25105;&#23558;&#35774;&#35745;&#36807;&#31243;&#24314;&#27169;&#20026;&#19968;&#20010;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#65292;&#36890;&#36807;&#20248;&#21270;&#36719;&#25805;&#32437;&#22120;&#30340;&#36816;&#21160;&#38142;&#26469;&#36798;&#21040;&#30446;&#26631;&#24182;&#36991;&#20813;&#19981;&#24517;&#35201;&#30340;&#26448;&#26009;&#21644;&#36164;&#28304;&#28010;&#36153;&#12290;&#35813;&#26041;&#27861;&#20805;&#20998;&#21033;&#29992;&#20102;&#22522;&#20110;&#32676;&#20307;&#30340;&#20248;&#21270;&#31639;&#27861;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Soft-growing robots are innovative devices that feature plant-inspired growth to navigate environments. Thanks to their embodied intelligence of adapting to their surroundings and the latest innovation in actuation and manufacturing, it is possible to employ them for specific manipulation tasks. The applications of these devices include exploration of delicate/dangerous environments, manipulation of items, or assistance in domestic environments.  This work presents a novel approach for design optimization of soft-growing robots, which will be used prior to manufacturing to suggest engineers -- or robot designer enthusiasts -- the optimal dimension of the robot to be built for solving a specific task. I modeled the design process as a multi-objective optimization problem, in which I optimize the kinematic chain of a soft manipulator to reach targets and avoid unnecessary overuse of material and resources. The method exploits the advantages of population-based optimization algorithms, in
&lt;/p&gt;</description></item><item><title>L2MAC&#26159;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#23384;&#20648;&#31243;&#24207;&#33258;&#21160;&#35745;&#31639;&#26426;&#65292;&#21487;&#20197;&#29992;&#20110;&#29983;&#25104;&#38271;&#19988;&#36923;&#36753;&#19968;&#33268;&#30340;&#20195;&#30721;&#12290;</title><link>http://arxiv.org/abs/2310.02003</link><description>&lt;p&gt;
L2MAC&#65306;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#35745;&#31639;&#26426;&#29992;&#20110;&#26080;&#38480;&#20195;&#30721;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
L2MAC: Large Language Model Automatic Computer for Unbounded Code Generation. (arXiv:2310.02003v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02003
&lt;/p&gt;
&lt;p&gt;
L2MAC&#26159;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#23384;&#20648;&#31243;&#24207;&#33258;&#21160;&#35745;&#31639;&#26426;&#65292;&#21487;&#20197;&#29992;&#20110;&#29983;&#25104;&#38271;&#19988;&#36923;&#36753;&#19968;&#33268;&#30340;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21463;&#21040;&#24213;&#23618;Transformer&#26550;&#26500;&#22266;&#23450;&#19978;&#19979;&#25991;&#31383;&#21475;&#30340;&#38480;&#21046;&#65292;&#38459;&#30861;&#20102;&#23427;&#20204;&#29983;&#25104;&#38271;&#19988;&#36923;&#36753;&#19968;&#33268;&#30340;&#20195;&#30721;&#30340;&#33021;&#21147;&#12290;&#22686;&#24378;&#35760;&#24518;&#30340;LLM&#26159;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#30446;&#21069;&#30340;&#26041;&#27861;&#26080;&#27861;&#22788;&#29702;&#38271;&#26102;&#38388;&#30340;&#20195;&#30721;&#29983;&#25104;&#20219;&#21153;&#65292;&#22240;&#20026;&#23427;&#20204;&#35201;&#20040;&#21482;&#20851;&#27880;&#20110;&#35835;&#21462;&#20869;&#23384;&#24182;&#23558;&#20854;&#28436;&#21464;&#20026;&#26032;&#20869;&#23384;&#30340;&#36830;&#25509;&#65292;&#35201;&#20040;&#20351;&#29992;&#38750;&#24120;&#19987;&#38376;&#30340;&#20869;&#23384;&#65292;&#26080;&#27861;&#36866;&#24212;&#20854;&#20182;&#39046;&#22495;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;L2MAC&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#38271;&#19988;&#19968;&#33268;&#20195;&#30721;&#29983;&#25104;&#30340;&#23454;&#29992;&#23384;&#20648;&#31243;&#24207;&#33258;&#21160;&#35745;&#31639;&#26426;&#12290;&#23427;&#30340;&#20869;&#23384;&#26377;&#20004;&#20010;&#32452;&#25104;&#37096;&#20998;&#65306;&#25351;&#20196;&#27880;&#20876;&#34920;&#65292;&#20854;&#20013;&#22635;&#20805;&#20102;&#19968;&#20010;&#35299;&#20915;&#29992;&#25143;&#32473;&#23450;&#20219;&#21153;&#30340;&#25552;&#31034;&#31243;&#24207;&#65292;&#20197;&#21450;&#25991;&#20214;&#23384;&#20648;&#65292;&#20854;&#20013;&#21253;&#21547;&#26368;&#32456;&#21644;&#20013;&#38388;&#36755;&#20986;&#12290;&#27599;&#20010;&#25351;&#20196;&#30001;&#21333;&#29420;&#30340;LLM&#23454;&#20363;&#25191;&#34892;&#65292;&#20854;&#19978;&#19979;&#25991;&#30001;&#25511;&#21046;&#21333;&#20803;&#31649;&#29702;&#65292;&#33021;&#22815;&#31934;&#30830;&#35835;&#21462;&#21644;&#20889;&#20837;&#20869;&#23384;&#65292;&#20197;&#30830;&#20445;&#26377;&#25928;&#30340;&#25972;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer-based large language models (LLMs) are constrained by the fixed context window of the underlying transformer architecture, hindering their ability to produce long and logically consistent code. Memory-augmented LLMs are a promising solution, but current approaches cannot handle long code generation tasks since they (1) only focus on reading memory and reduce its evolution to the concatenation of new memories or (2) use very specialized memories that cannot adapt to other domains. This paper presents L2MAC, the first practical LLM-based stored-program automatic computer for long and consistent code generation. Its memory has two components: the instruction registry, which is populated with a prompt program to solve the user-given task, and a file store, which will contain the final and intermediate outputs. Each instruction is executed by a separate LLM instance, whose context is managed by a control unit capable of precise memory reading and writing to ensure effective inte
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#25439;&#22833;&#24179;&#22374;&#24615;&#21644;&#31070;&#32463;&#34920;&#31034;&#21387;&#32553;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#36890;&#36807;&#31616;&#21333;&#30340;&#25968;&#23398;&#20851;&#31995;&#65292;&#35777;&#26126;&#20102;&#25439;&#22833;&#24179;&#22374;&#24615;&#19982;&#31070;&#32463;&#34920;&#31034;&#30340;&#21387;&#32553;&#30456;&#20851;&#12290;</title><link>http://arxiv.org/abs/2310.01770</link><description>&lt;p&gt;
&#25439;&#22833;&#24179;&#22374;&#24615;&#19982;&#31070;&#32463;&#32593;&#32476;&#20013;&#21387;&#32553;&#34920;&#31034;&#30340;&#31616;&#21333;&#32852;&#31995;
&lt;/p&gt;
&lt;p&gt;
A simple connection from loss flatness to compressed representations in neural networks. (arXiv:2310.01770v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01770
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#25439;&#22833;&#24179;&#22374;&#24615;&#21644;&#31070;&#32463;&#34920;&#31034;&#21387;&#32553;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#36890;&#36807;&#31616;&#21333;&#30340;&#25968;&#23398;&#20851;&#31995;&#65292;&#35777;&#26126;&#20102;&#25439;&#22833;&#24179;&#22374;&#24615;&#19982;&#31070;&#32463;&#34920;&#31034;&#30340;&#21387;&#32553;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#33021;&#21147;&#36827;&#34892;&#30740;&#31350;&#30340;&#26041;&#27861;&#26377;&#24456;&#22810;&#31181;&#65292;&#21253;&#25324;&#33267;&#23569;&#20004;&#31181;&#19981;&#21516;&#30340;&#26041;&#27861;&#65306;&#19968;&#31181;&#22522;&#20110;&#21442;&#25968;&#31354;&#38388;&#20013;&#25439;&#22833;&#26223;&#35266;&#30340;&#24418;&#29366;&#65292;&#21478;&#19968;&#31181;&#22522;&#20110;&#29305;&#24449;&#31354;&#38388;&#20013;&#34920;&#31034;&#27969;&#24418;&#30340;&#32467;&#26500;&#65288;&#21363;&#21333;&#20301;&#27963;&#21160;&#30340;&#31354;&#38388;&#65289;&#12290;&#36825;&#20004;&#31181;&#26041;&#27861;&#30456;&#20851;&#20294;&#24456;&#23569;&#21516;&#26102;&#36827;&#34892;&#30740;&#31350;&#21644;&#26126;&#30830;&#20851;&#32852;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#20998;&#26512;&#26041;&#27861;&#26469;&#24314;&#31435;&#36825;&#31181;&#32852;&#31995;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#30340;&#26368;&#21518;&#38454;&#27573;&#65292;&#31070;&#32463;&#34920;&#31034;&#27969;&#24418;&#30340;&#20307;&#31215;&#21387;&#32553;&#19982;&#27491;&#22312;&#36827;&#34892;&#30340;&#21442;&#25968;&#20248;&#21270;&#25152;&#25506;&#32034;&#30340;&#26368;&#23567;&#20540;&#21608;&#22260;&#30340;&#25439;&#22833;&#24179;&#22374;&#24615;&#30456;&#20851;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#21487;&#20197;&#30001;&#19968;&#20010;&#30456;&#23545;&#31616;&#21333;&#30340;&#25968;&#23398;&#20851;&#31995;&#26469;&#39044;&#27979;&#65306;&#25439;&#22833;&#24179;&#22374;&#24615;&#24847;&#21619;&#30528;&#31070;&#32463;&#34920;&#31034;&#30340;&#21387;&#32553;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#19982;\citet{ma_linear_2021}&#30340;&#20808;&#21069;&#30740;&#31350;&#23494;&#20999;&#30456;&#20851;&#65292;&#35813;&#30740;&#31350;&#23637;&#31034;&#20102;&#24179;&#22374;&#24615;&#65288;&#21363;&#23567;&#29305;&#24449;&#20540;&#65289;&#19982;&#34920;&#31034;&#27969;&#24418;&#30340;&#20307;&#31215;&#21387;&#32553;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks' generalization capacity has been studied in a variety of ways, including at least two distinct categories of approach: one based on the shape of the loss landscape in parameter space, and the other based on the structure of the representation manifold in feature space (that is, in the space of unit activities). These two approaches are related, but they are rarely studied together and explicitly connected. Here, we present a simple analysis that makes such a connection. We show that, in the last phase of learning of deep neural networks, compression of the volume of the manifold of neural representations correlates with the flatness of the loss around the minima explored by ongoing parameter optimization. We show that this is predicted by a relatively simple mathematical relationship: loss flatness implies compression of neural representations. Our results build closely on prior work of \citet{ma_linear_2021}, which shows how flatness (i.e., small eigenvalues of t
&lt;/p&gt;</description></item><item><title>LTU-AS&#26159;&#19968;&#20010;&#20855;&#26377;&#26222;&#36866;&#38899;&#39057;&#24863;&#30693;&#21644;&#39640;&#32423;&#25512;&#29702;&#33021;&#21147;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#21487;&#20197;&#21516;&#26102;&#35782;&#21035;&#21644;&#32852;&#21512;&#29702;&#35299;&#21475;&#35821;&#25991;&#26412;&#12289;&#35821;&#38899;&#22768;&#38899;&#23398;&#21644;&#38750;&#35821;&#38899;&#38899;&#39057;&#20107;&#20214;&#12290;</title><link>http://arxiv.org/abs/2309.14405</link><description>&lt;p&gt;
&#32852;&#21512;&#38899;&#39057;&#21644;&#35821;&#38899;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Joint Audio and Speech Understanding. (arXiv:2309.14405v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14405
&lt;/p&gt;
&lt;p&gt;
LTU-AS&#26159;&#19968;&#20010;&#20855;&#26377;&#26222;&#36866;&#38899;&#39057;&#24863;&#30693;&#21644;&#39640;&#32423;&#25512;&#29702;&#33021;&#21147;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#21487;&#20197;&#21516;&#26102;&#35782;&#21035;&#21644;&#32852;&#21512;&#29702;&#35299;&#21475;&#35821;&#25991;&#26412;&#12289;&#35821;&#38899;&#22768;&#38899;&#23398;&#21644;&#38750;&#35821;&#38899;&#38899;&#39057;&#20107;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#21608;&#22260;&#20805;&#26021;&#30528;&#21253;&#25324;&#35821;&#38899;&#21644;&#38750;&#35821;&#38899;&#22768;&#38899;&#22312;&#20869;&#30340;&#38899;&#39057;&#20449;&#21495;&#12290;&#23545;&#35821;&#38899;&#21644;&#38750;&#35821;&#38899;&#38899;&#39057;&#20107;&#20214;&#30340;&#35782;&#21035;&#21644;&#29702;&#35299;&#65292;&#20197;&#21450;&#23545;&#23427;&#20204;&#20043;&#38388;&#20851;&#31995;&#30340;&#28145;&#21051;&#29702;&#35299;&#65292;&#26500;&#25104;&#20102;&#22522;&#26412;&#30340;&#35748;&#30693;&#33021;&#21147;&#12290;&#25105;&#20204;&#39318;&#27425;&#26500;&#24314;&#20102;&#19968;&#20010;&#21517;&#20026;LTU-AS&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#23427;&#20855;&#26377;&#31867;&#20284;&#20110;&#20154;&#31867;&#30340;&#26222;&#36941;&#38899;&#39057;&#24863;&#30693;&#21644;&#39640;&#32423;&#25512;&#29702;&#33021;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#36890;&#36807;&#23558;Whisper&#20316;&#20026;&#24863;&#30693;&#27169;&#22359;&#21644;LLaMA&#20316;&#20026;&#25512;&#29702;&#27169;&#22359;&#36827;&#34892;&#38598;&#25104;&#65292;LTU-AS&#21487;&#20197;&#21516;&#26102;&#35782;&#21035;&#21644;&#32852;&#21512;&#29702;&#35299;&#21475;&#35821;&#25991;&#26412;&#12289;&#35821;&#38899;&#22768;&#38899;&#23398;&#20197;&#21450;&#38750;&#35821;&#38899;&#38899;&#39057;&#20107;&#20214; - &#20960;&#20046;&#21487;&#20197;&#20174;&#38899;&#39057;&#20449;&#21495;&#20013;&#24863;&#30693;&#21040;&#30340;&#19968;&#20999;&#12290;
&lt;/p&gt;
&lt;p&gt;
Humans are surrounded by audio signals that include both speech and non-speech sounds. The recognition and understanding of speech and non-speech audio events, along with a profound comprehension of the relationship between them, constitute fundamental cognitive capabilities. For the first time, we build a machine learning model, called LTU-AS, that has a conceptually similar universal audio perception and advanced reasoning ability. Specifically, by integrating Whisper as a perception module and LLaMA as a reasoning module, LTU-AS can simultaneously recognize and jointly understand spoken text, speech paralinguistics, and non-speech audio events - almost everything perceivable from audio signals.
&lt;/p&gt;</description></item><item><title>NAS-NeRF&#26159;&#19968;&#31181;&#29983;&#25104;&#24335;&#31070;&#32463;&#20307;&#31995;&#32467;&#26500;&#25628;&#32034;&#31574;&#30053;&#65292;&#36890;&#36807;&#24179;&#34913;&#26550;&#26500;&#22797;&#26434;&#24230;&#21644;&#30446;&#26631;&#21512;&#25104;&#36136;&#37327;&#25351;&#26631;&#29983;&#25104;&#32039;&#20945;&#12289;&#38024;&#23545;&#22330;&#26223;&#30340;NeRF&#26550;&#26500;&#12290;</title><link>http://arxiv.org/abs/2309.14293</link><description>&lt;p&gt;
NAS-NeRF: &#29992;&#20110;&#31070;&#32463;&#36752;&#23556;&#22330;&#30340;&#29983;&#25104;&#24335;&#31070;&#32463;&#20307;&#31995;&#32467;&#26500;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
NAS-NeRF: Generative Neural Architecture Search for Neural Radiance Fields. (arXiv:2309.14293v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14293
&lt;/p&gt;
&lt;p&gt;
NAS-NeRF&#26159;&#19968;&#31181;&#29983;&#25104;&#24335;&#31070;&#32463;&#20307;&#31995;&#32467;&#26500;&#25628;&#32034;&#31574;&#30053;&#65292;&#36890;&#36807;&#24179;&#34913;&#26550;&#26500;&#22797;&#26434;&#24230;&#21644;&#30446;&#26631;&#21512;&#25104;&#36136;&#37327;&#25351;&#26631;&#29983;&#25104;&#32039;&#20945;&#12289;&#38024;&#23545;&#22330;&#26223;&#30340;NeRF&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#36752;&#23556;&#22330;&#65288;NeRF&#65289;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#30340;&#26032;&#35270;&#22270;&#21512;&#25104;&#65292;&#20294;&#20854;&#39640;&#35745;&#31639;&#22797;&#26434;&#24230;&#38480;&#21046;&#20102;&#20854;&#21487;&#37096;&#32626;&#24615;&#12290;&#29616;&#26377;&#30340;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#35299;&#20915;&#26041;&#26696;&#21162;&#21147;&#25552;&#39640;&#25928;&#29575;&#65292;&#20294;&#19981;&#32771;&#34385;&#22330;&#26223;&#22797;&#26434;&#24615;&#65292;&#20351;&#29992;&#36890;&#29992;&#26550;&#26500;&#12290;&#21516;&#19968;&#20010;&#26550;&#26500;&#21487;&#33021;&#23545;&#31616;&#21333;&#22330;&#26223;&#26469;&#35828;&#36807;&#20110;&#24222;&#22823;&#65292;&#23545;&#22797;&#26434;&#22330;&#26223;&#21017;&#19981;&#36275;&#22815;&#12290;&#22240;&#27492;&#65292;&#26377;&#24517;&#35201;&#21160;&#24577;&#20248;&#21270;NeRF&#30340;&#31070;&#32463;&#32593;&#32476;&#32452;&#20214;&#65292;&#20197;&#22312;&#35745;&#31639;&#22797;&#26434;&#24230;&#21644;&#21512;&#25104;&#36136;&#37327;&#20043;&#38388;&#23454;&#29616;&#24179;&#34913;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;NAS-NeRF&#65292;&#19968;&#31181;&#29983;&#25104;&#24335;&#31070;&#32463;&#20307;&#31995;&#32467;&#26500;&#25628;&#32034;&#31574;&#30053;&#65292;&#36890;&#36807;&#24179;&#34913;&#26550;&#26500;&#22797;&#26434;&#24230;&#21644;&#30446;&#26631;&#21512;&#25104;&#36136;&#37327;&#25351;&#26631;&#29983;&#25104;&#32039;&#20945;&#12289;&#38024;&#23545;&#22330;&#26223;&#30340;NeRF&#26550;&#26500;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#32467;&#21512;&#30446;&#26631;&#24230;&#37327;&#21644;&#39044;&#31639;&#32422;&#26463;&#65292;&#25351;&#23548;&#25628;&#32034;&#20197;&#33719;&#24471;&#36866;&#21512;&#27599;&#20010;&#22330;&#26223;&#30340;&#26550;&#26500;&#12290;&#22312;Blender&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25552;&#20986;&#30340;NAS-NeRF&#26041;&#27861;&#21487;&#20197;&#29983;&#25104;&#22810;&#36798;5&#20010;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural radiance fields (NeRFs) enable high-quality novel view synthesis, but their high computational complexity limits deployability. While existing neural-based solutions strive for efficiency, they use one-size-fits-all architectures regardless of scene complexity. The same architecture may be unnecessarily large for simple scenes but insufficient for complex ones. Thus, there is a need to dynamically optimize the neural network component of NeRFs to achieve a balance between computational complexity and specific targets for synthesis quality. We introduce NAS-NeRF, a generative neural architecture search strategy that generates compact, scene-specialized NeRF architectures by balancing architecture complexity and target synthesis quality metrics. Our method incorporates constraints on target metrics and budgets to guide the search towards architectures tailored for each scene. Experiments on the Blender synthetic dataset show the proposed NAS-NeRF can generate architectures up to 5
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#12289;&#32479;&#19968;&#30340;&#12289;&#21487;&#25511;&#19988;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#65292;&#29992;&#20110;&#35299;&#30721;&#25918;&#23556;&#31185;&#21307;&#29983;&#22312;&#33016;&#37096;X&#20809;&#35786;&#26029;&#20013;&#30340;&#19987;&#27880;&#28966;&#28857;&#65292;&#20174;&#32780;&#25581;&#31034;&#25918;&#23556;&#23398;&#35299;&#37322;&#36807;&#31243;&#30340;&#35748;&#30693;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2309.13550</link><description>&lt;p&gt;
&#35299;&#30721;&#25918;&#23556;&#31185;&#21307;&#29983;&#22312;&#20934;&#30830;&#33016;&#37096;X&#20809;&#35786;&#26029;&#20013;&#30340;&#38598;&#20013;&#27880;&#24847;&#21147;&#65306;&#19968;&#20010;&#21487;&#25511;&#19988;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#12290; &#65288;arXiv:2309.13550v2 [cs.CV]&#24050;&#26356;&#26032;&#65289;
&lt;/p&gt;
&lt;p&gt;
Decoding Radiologists Intense Focus for Accurate CXR Diagnoses: A Controllable and Interpretable AI System. (arXiv:2309.13550v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13550
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#12289;&#32479;&#19968;&#30340;&#12289;&#21487;&#25511;&#19988;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#65292;&#29992;&#20110;&#35299;&#30721;&#25918;&#23556;&#31185;&#21307;&#29983;&#22312;&#33016;&#37096;X&#20809;&#35786;&#26029;&#20013;&#30340;&#19987;&#27880;&#28966;&#28857;&#65292;&#20174;&#32780;&#25581;&#31034;&#25918;&#23556;&#23398;&#35299;&#37322;&#36807;&#31243;&#30340;&#35748;&#30693;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33016;&#37096;X&#20809;&#65288;CXR&#65289;&#35786;&#26029;&#39046;&#22495;&#20013;&#65292;&#29616;&#26377;&#30340;&#30740;&#31350;&#24448;&#24448;&#20165;&#20851;&#27880;&#25918;&#23556;&#31185;&#21307;&#29983;&#30340;&#27880;&#35270;&#20301;&#32622;&#65292;&#36890;&#24120;&#36890;&#36807;&#26816;&#27979;&#12289;&#20998;&#21106;&#25110;&#20998;&#31867;&#31561;&#20219;&#21153;&#26469;&#23454;&#29616;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#24120;&#24120;&#34987;&#35774;&#35745;&#20026;&#40657;&#30418;&#27169;&#22411;&#65292;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#19988;&#32479;&#19968;&#30340;&#21487;&#25511;&#35299;&#37322;&#24615;&#31649;&#36947;&#65292;&#29992;&#20110;&#35299;&#30721;&#25918;&#23556;&#31185;&#21307;&#29983;&#22312;CXR&#35786;&#26029;&#20013;&#30340;&#19987;&#27880;&#28966;&#28857;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#35299;&#20915;&#20102;&#19977;&#20010;&#20851;&#38190;&#38382;&#39064;&#65306;&#25918;&#23556;&#31185;&#21307;&#29983;&#30340;&#27880;&#35270;&#20301;&#32622;&#12289;&#20182;&#20204;&#22312;&#29305;&#23450;&#21306;&#22495;&#30340;&#27880;&#24847;&#26102;&#38271;&#20197;&#21450;&#20182;&#20204;&#30340;&#35786;&#26029;&#32467;&#26524;&#12290;&#36890;&#36807;&#25429;&#25417;&#25918;&#23556;&#31185;&#21307;&#29983;&#27880;&#35270;&#30340;&#24378;&#24230;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#20197;&#27934;&#23519;&#25918;&#23556;&#23398;&#35299;&#37322;&#36807;&#31243;&#30340;&#35748;&#30693;&#36807;&#31243;&#12290;&#19982;&#24403;&#21069;&#20381;&#36182;&#20110;&#40657;&#30418;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#35786;&#26029;&#36807;&#31243;&#20013;&#23481;&#26131;&#20174;&#25972;&#20010;&#36755;&#20837;&#22270;&#20687;&#20013;&#25552;&#21462;&#38169;&#35823;&#20449;&#24687;&#65292;&#25105;&#20204;&#36890;&#36807;&#26377;&#25928;&#22320;&#23631;&#34109;&#26080;&#20851;&#20449;&#24687;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the field of chest X-ray (CXR) diagnosis, existing works often focus solely on determining where a radiologist looks, typically through tasks such as detection, segmentation, or classification. However, these approaches are often designed as black-box models, lacking interpretability. In this paper, we introduce a novel and unified controllable interpretable pipeline for decoding the intense focus of radiologists in CXR diagnosis. Our approach addresses three key questions: where a radiologist looks, how long they focus on specific areas, and what findings they diagnose. By capturing the intensity of the radiologist's gaze, we provide a unified solution that offers insights into the cognitive process underlying radiological interpretation. Unlike current methods that rely on black-box machine learning models, which can be prone to extracting erroneous information from the entire input image during the diagnosis process, we tackle this issue by effectively masking out irrelevant info
&lt;/p&gt;</description></item><item><title>BELT&#26159;&#19968;&#31181;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#30417;&#30563;&#36827;&#34892;&#33041;&#30005;&#22270;&#21040;&#35821;&#35328;&#35299;&#30721;&#21644;&#38646;&#26679;&#26412;&#24773;&#24863;&#20998;&#31867;&#30340;&#24341;&#23548;&#24335;&#27169;&#22411;&#21644;&#23398;&#20064;&#26694;&#26550;&#12290;&#23427;&#21033;&#29992;&#29616;&#25104;&#30340;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#26469;&#25913;&#36827;&#33041;&#30005;&#20449;&#21495;&#30340;&#29702;&#35299;&#65292;&#20174;&#32780;&#25512;&#21160;&#20102;&#22823;&#33041;-&#35745;&#31639;&#26426;&#30028;&#38754;&#30340;&#24212;&#29992;&#21644;&#21457;&#23637;&#12290;</title><link>http://arxiv.org/abs/2309.12056</link><description>&lt;p&gt;
BELT: &#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#30417;&#30563;&#36827;&#34892;&#33041;&#30005;&#22270;&#21040;&#35821;&#35328;&#35299;&#30721;&#21644;&#38646;&#26679;&#26412;&#24773;&#24863;&#20998;&#31867;&#30340;&#24341;&#23548;&#24335;&#27169;&#22411;&#21644;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
BELT:Bootstrapping Electroencephalography-to-Language Decoding and Zero-Shot Sentiment Classification by Natural Language Supervision. (arXiv:2309.12056v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12056
&lt;/p&gt;
&lt;p&gt;
BELT&#26159;&#19968;&#31181;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#30417;&#30563;&#36827;&#34892;&#33041;&#30005;&#22270;&#21040;&#35821;&#35328;&#35299;&#30721;&#21644;&#38646;&#26679;&#26412;&#24773;&#24863;&#20998;&#31867;&#30340;&#24341;&#23548;&#24335;&#27169;&#22411;&#21644;&#23398;&#20064;&#26694;&#26550;&#12290;&#23427;&#21033;&#29992;&#29616;&#25104;&#30340;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#26469;&#25913;&#36827;&#33041;&#30005;&#20449;&#21495;&#30340;&#29702;&#35299;&#65292;&#20174;&#32780;&#25512;&#21160;&#20102;&#22823;&#33041;-&#35745;&#31639;&#26426;&#30028;&#38754;&#30340;&#24212;&#29992;&#21644;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;BELT&#65292;&#36825;&#26159;&#19968;&#20010;&#38024;&#23545;&#22823;&#33041;&#21040;&#35821;&#35328;&#32763;&#35793;&#30740;&#31350;&#30340;&#37325;&#35201;&#20027;&#39064;&#30340;&#26032;&#27169;&#22411;&#21644;&#23398;&#20064;&#26694;&#26550;&#12290;&#23558;&#38750;&#20405;&#20837;&#24615;&#33041;&#20449;&#21495;&#32763;&#35793;&#25104;&#21487;&#35835;&#30340;&#33258;&#28982;&#35821;&#35328;&#26377;&#28508;&#21147;&#25512;&#21160;&#22823;&#33041;-&#35745;&#31639;&#26426;&#30028;&#38754;&#65288;BCI&#65289;&#30340;&#24212;&#29992;&#22330;&#26223;&#21644;&#21457;&#23637;&#12290;&#33041;&#20449;&#21495;&#35299;&#30721;&#25110;&#33041;-&#35821;&#35328;&#32763;&#35793;&#20013;&#30340;&#20851;&#38190;&#38382;&#39064;&#26159;&#20174;&#26377;&#38480;&#35268;&#27169;&#21644;&#36136;&#37327;&#30340;&#25968;&#25454;&#38598;&#20013;&#33719;&#24471;&#35821;&#20041;&#36866;&#24403;&#19988;&#20855;&#26377;&#21306;&#20998;&#24615;&#30340;&#33041;&#30005;&#22270;&#34920;&#31034;&#12290;&#25152;&#25552;&#20986;&#30340;BELT&#26041;&#27861;&#26159;&#19968;&#20010;&#36890;&#29992;&#19988;&#39640;&#25928;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#29616;&#25104;&#30340;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#24341;&#23548;&#33041;&#30005;&#22270;&#34920;&#31034;&#23398;&#20064;&#12290;&#36890;&#36807;&#21033;&#29992;&#35757;&#32451;&#22312;&#20114;&#32852;&#32593;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#30340;&#22823;&#35268;&#27169;LM&#29702;&#35299;&#35821;&#20041;&#20449;&#24687;&#21644;&#38646;&#26679;&#26412;&#27867;&#21270;&#33021;&#21147;&#65292;BELT&#26497;&#22823;&#25913;&#36827;&#20102;&#23545;&#33041;&#30005;&#20449;&#21495;&#30340;&#29702;&#35299;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;BELT&#27169;&#22411;&#30001;&#19968;&#20010;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#32452;&#25104;&#65292;&#20854;&#20013;&#21253;&#21547;&#19968;&#20010;&#35299;&#30721;&#31070;&#32463;&#32593;&#32476;&#21644;&#19968;&#20010;&#39044;&#35757;&#32451;LM&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents BELT, a novel model and learning framework for the pivotal topic of brain-to-language translation research. The translation from noninvasive brain signals into readable natural language has the potential to promote the application scenario as well as the development of brain-computer interfaces (BCI) as a whole. The critical problem in brain signal decoding or brain-to-language translation is the acquisition of semantically appropriate and discriminative EEG representation from a dataset of limited scale and quality. The proposed BELT method is a generic and efficient framework that bootstraps EEG representation learning using off-the-shelf large-scale pretrained language models (LMs). With a large LM's capacity for understanding semantic information and zero-shot generalization, BELT utilizes large LMs trained on Internet-scale datasets to bring significant improvements to the understanding of EEG signals.  In particular, the BELT model is composed of a deep confor
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#27010;&#36848;&#20102;&#29616;&#20195;&#32842;&#22825;&#26426;&#22120;&#20154;&#35774;&#35745;&#20013;&#30340;&#20559;&#35265;&#21644;&#20844;&#24179;&#24615;&#38382;&#39064;&#65292;&#24182;&#20171;&#32461;&#20102;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#21382;&#21490;&#12289;&#20559;&#35265;&#26469;&#28304;&#21644;&#20844;&#24179;&#24615;&#20445;&#25252;&#26041;&#38754;&#30340;&#32771;&#34385;&#22240;&#32032;&#12290;</title><link>http://arxiv.org/abs/2309.08836</link><description>&lt;p&gt;
&#32842;&#22825;&#26426;&#22120;&#20154;&#20013;&#30340;&#20559;&#35265;&#21644;&#20844;&#24179;&#24615;: &#19968;&#31181;&#27010;&#36848;
&lt;/p&gt;
&lt;p&gt;
Bias and Fairness in Chatbots: An Overview. (arXiv:2309.08836v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08836
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#27010;&#36848;&#20102;&#29616;&#20195;&#32842;&#22825;&#26426;&#22120;&#20154;&#35774;&#35745;&#20013;&#30340;&#20559;&#35265;&#21644;&#20844;&#24179;&#24615;&#38382;&#39064;&#65292;&#24182;&#20171;&#32461;&#20102;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#21382;&#21490;&#12289;&#20559;&#35265;&#26469;&#28304;&#21644;&#20844;&#24179;&#24615;&#20445;&#25252;&#26041;&#38754;&#30340;&#32771;&#34385;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32842;&#22825;&#26426;&#22120;&#20154;&#24050;&#32463;&#30740;&#31350;&#20102;&#21322;&#20010;&#22810;&#19990;&#32426;&#12290;&#38543;&#30528;&#36817;&#24180;&#26469;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#29616;&#22312;&#22791;&#21463;&#20851;&#27880;&#12290;&#19982;&#20256;&#32479;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#30456;&#27604;&#65292;&#29616;&#20195;&#32842;&#22825;&#26426;&#22120;&#20154;&#26356;&#24378;&#22823;&#65292;&#24182;&#24050;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#20351;&#29992;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#20195;&#32842;&#22825;&#26426;&#22120;&#20154;&#35774;&#35745;&#20013;&#23384;&#22312;&#20559;&#35265;&#21644;&#20844;&#24179;&#24615;&#38382;&#39064;&#12290;&#30001;&#20110;&#35757;&#32451;&#25968;&#25454;&#37327;&#24040;&#22823;&#12289;&#27169;&#22411;&#35268;&#27169;&#24222;&#22823;&#19988;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#65292;&#29616;&#20195;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#20559;&#35265;&#32531;&#35299;&#21644;&#20844;&#24179;&#20445;&#25252;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#23545;&#32842;&#22825;&#26426;&#22120;&#20154;&#31995;&#32479;&#20013;&#30340;&#20559;&#35265;&#21644;&#20844;&#24179;&#24615;&#36827;&#34892;&#20102;&#20840;&#38754;&#27010;&#36848;&#12290;&#39318;&#20808;&#22238;&#39038;&#20102;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#21382;&#21490;&#21644;&#31867;&#21035;&#12290;&#28982;&#21518;&#65292;&#20998;&#26512;&#20102;&#24212;&#29992;&#20013;&#30340;&#20559;&#35265;&#26469;&#28304;&#21644;&#28508;&#22312;&#21361;&#23475;&#12290;&#30740;&#31350;&#20102;&#35774;&#35745;&#20844;&#24179;&#21644;&#26080;&#20559;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#31995;&#32479;&#30340;&#32771;&#34385;&#22240;&#32032;&#12290;&#26368;&#21518;&#65292;&#35752;&#35770;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Chatbots have been studied for more than half a century. With the rapid development of natural language processing (NLP) technologies in recent years, chatbots using large language models (LLMs) have received much attention nowadays. Compared with traditional ones, modern chatbots are more powerful and have been used in real-world applications. There are however, bias and fairness concerns in modern chatbot design. Due to the huge amounts of training data, extremely large model sizes, and lack of interpretability, bias mitigation and fairness preservation of modern chatbots are challenging. Thus, a comprehensive overview on bias and fairness in chatbot systems is given in this paper. The history of chatbots and their categories are first reviewed. Then, bias sources and potential harms in applications are analyzed. Considerations in designing fair and unbiased chatbot systems are examined. Finally, future research directions are discussed.
&lt;/p&gt;</description></item><item><title>FlexKBQA&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#20316;&#20026;&#31243;&#24207;&#32763;&#35793;&#22120;&#65292;&#36890;&#36807;&#33258;&#21160;&#21270;&#31639;&#27861;&#20174;&#30693;&#35782;&#24211;&#20013;&#25277;&#26679;&#22810;&#26679;&#30340;&#31243;&#24207;&#65292;&#23558;&#20854;&#36716;&#25442;&#20026;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#65292;&#20174;&#32780;&#35299;&#20915;&#23569;&#26679;&#26412;&#30693;&#35782;&#24211;&#38382;&#31572;&#20219;&#21153;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2308.12060</link><description>&lt;p&gt;
FlexKBQA&#65306;&#19968;&#31181;&#29992;&#20110;&#23569;&#26679;&#26412;&#30693;&#35782;&#24211;&#38382;&#31572;&#30340;&#28789;&#27963;LLM&#39537;&#21160;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
FlexKBQA: A Flexible LLM-Powered Framework for Few-Shot Knowledge Base Question Answering. (arXiv:2308.12060v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12060
&lt;/p&gt;
&lt;p&gt;
FlexKBQA&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#20316;&#20026;&#31243;&#24207;&#32763;&#35793;&#22120;&#65292;&#36890;&#36807;&#33258;&#21160;&#21270;&#31639;&#27861;&#20174;&#30693;&#35782;&#24211;&#20013;&#25277;&#26679;&#22810;&#26679;&#30340;&#31243;&#24207;&#65292;&#23558;&#20854;&#36716;&#25442;&#20026;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#65292;&#20174;&#32780;&#35299;&#20915;&#23569;&#26679;&#26412;&#30693;&#35782;&#24211;&#38382;&#31572;&#20219;&#21153;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#24211;&#38382;&#31572;&#65288;KBQA&#65289;&#26159;&#19968;&#20010;&#20851;&#38190;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#30693;&#35782;&#24211;&#20013;&#30340;&#23454;&#20307;&#25968;&#37327;&#24222;&#22823;&#65292;&#24182;&#19988;&#29992;&#25143;&#25552;&#20986;&#30340;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#22810;&#26679;&#21270;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#22823;&#22810;&#25968;KBQA&#27169;&#22411;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#24615;&#33021;&#26174;&#33879;&#19979;&#38477;&#65292;&#22240;&#20026;&#39640;&#36136;&#37327;&#30340;&#27880;&#37322;&#25968;&#25454;&#19981;&#36275;&#12290;&#20026;&#20102;&#20943;&#36731;&#25163;&#21160;&#27880;&#37322;&#30340;&#36127;&#25285;&#65292;&#25105;&#20204;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#31243;&#24207;&#32763;&#35793;&#22120;&#65292;&#20171;&#32461;&#20102;FlexKBQA&#26469;&#35299;&#20915;&#23569;&#26679;&#26412;KBQA&#20219;&#21153;&#20013;&#22266;&#26377;&#30340;&#25361;&#25112;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;FlexKBQA&#21033;&#29992;&#33258;&#21160;&#21270;&#31639;&#27861;&#20174;&#30693;&#35782;&#24211;&#20013;&#25277;&#26679;&#22810;&#26679;&#30340;&#31243;&#24207;&#65288;&#22914;SPARQL&#26597;&#35810;&#65289;&#65292;&#28982;&#21518;&#36890;&#36807;LLMs&#23558;&#20854;&#36716;&#25442;&#20026;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#12290;&#36825;&#20010;&#21512;&#25104;&#30340;&#25968;&#25454;&#38598;&#26377;&#21161;&#20110;&#35757;&#32451;&#19968;&#20010;&#19987;&#38376;&#30340;&#36731;&#37327;&#32423;&#27169;&#22411;&#26469;&#22788;&#29702;&#30693;&#35782;&#24211;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#20943;&#23569;&#21512;&#25104;&#25968;&#25454;&#19982;&#30495;&#23454;&#29992;&#25143;&#38382;&#39064;&#20043;&#38388;&#30340;&#20998;&#24067;&#20559;&#31227;&#30340;&#38556;&#30861;&#65292;FlexKBQA&#24341;&#20837;&#20102;&#19968;&#20010;&#25191;&#34892;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge base question answering (KBQA) is a critical yet challenging task due to the vast number of entities within knowledge bases and the diversity of natural language questions posed by users. Unfortunately, the performance of most KBQA models tends to decline significantly in real-world scenarios where high-quality annotated data is insufficient. To mitigate the burden associated with manual annotation, we introduce FlexKBQA by utilizing Large Language Models (LLMs) as program translators for addressing the challenges inherent in the few-shot KBQA task. Specifically, FlexKBQA leverages automated algorithms to sample diverse programs, such as SPARQL queries, from the knowledge base, which are subsequently converted into natural language questions via LLMs. This synthetic dataset facilitates training a specialized lightweight model for the KB. Additionally, to reduce the barriers of distribution shift between synthetic data and real user questions, FlexKBQA introduces an executiong
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#19982;&#20154;&#31867;&#21453;&#39304;&#65288;RLHF&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#39044;&#29983;&#25104;&#30340;&#26679;&#26412;&#26469;&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#31867;&#20559;&#22909;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#12289;&#22870;&#21169;&#21152;&#26435;&#22238;&#24402;&#21644;&#20915;&#31574;Transformer&#31561;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#31867;&#20284;&#20110;&#30417;&#30563;&#24494;&#35843;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#23454;&#29616;&#23545;&#40784;&#12290;</title><link>http://arxiv.org/abs/2308.12050</link><description>&lt;p&gt;
&#36890;&#36807;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#19982;&#20154;&#31867;&#21453;&#39304;&#26469;&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Aligning Language Models with Offline Reinforcement Learning from Human Feedback. (arXiv:2308.12050v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12050
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#19982;&#20154;&#31867;&#21453;&#39304;&#65288;RLHF&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#39044;&#29983;&#25104;&#30340;&#26679;&#26412;&#26469;&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#31867;&#20559;&#22909;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#12289;&#22870;&#21169;&#21152;&#26435;&#22238;&#24402;&#21644;&#20915;&#31574;Transformer&#31561;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#31867;&#20284;&#20110;&#30417;&#30563;&#24494;&#35843;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#23454;&#29616;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#20154;&#31867;&#20559;&#22909;&#20013;&#23398;&#20064;&#23545;&#20110;&#35821;&#35328;&#27169;&#22411;&#26469;&#35828;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#20197;&#26377;&#25928;&#22320;&#28385;&#36275;&#20154;&#31867;&#38656;&#27714;&#21644;&#31038;&#20250;&#20215;&#20540;&#35266;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#36890;&#36807;&#21033;&#29992;&#20154;&#31867;&#21453;&#39304;&#26469;&#36981;&#24490;&#25351;&#20196;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#25216;&#26415;&#65292;&#22914;Proximal Policy Optimization&#65288;PPO&#65289;&#65292;&#36825;&#20123;&#25216;&#26415;&#34987;&#35777;&#26126;&#23545;&#20110;&#35821;&#35328;&#27169;&#22411;&#26469;&#35828;&#19981;&#31283;&#23450;&#19988;&#38590;&#20197;&#35843;&#25972;&#12290;&#27492;&#22806;&#65292;PPO&#38656;&#35201;&#22797;&#26434;&#30340;&#20998;&#24067;&#24335;&#31995;&#32479;&#23454;&#29616;&#65292;&#24433;&#21709;&#20102;&#22823;&#35268;&#27169;&#20998;&#24067;&#24335;&#35757;&#32451;&#30340;&#25928;&#29575;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#19982;&#20154;&#31867;&#21453;&#39304;&#65288;RLHF&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#39044;&#29983;&#25104;&#30340;&#26679;&#26412;&#32780;&#19981;&#26159;&#19982;RL&#29615;&#22659;&#20132;&#20114;&#26469;&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#65288;MLE&#65289;&#19982;&#36807;&#28388;&#12289;&#22870;&#21169;&#21152;&#26435;&#22238;&#24402;&#65288;RWR&#65289;&#20197;&#21450;&#20915;&#31574;Transformer&#65288;DT&#65289;&#26469;&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#31867;&#20559;&#22909;&#12290;&#36890;&#36807;&#37319;&#29992;&#31867;&#20284;&#20110;&#30417;&#30563;&#24494;&#35843;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;&#21644;&#20154;&#31867;&#20559;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning from human preferences is crucial for language models (LMs) to effectively cater to human needs and societal values. Previous research has made notable progress by leveraging human feedback to follow instructions. However, these approaches rely primarily on online reinforcement learning (RL) techniques like Proximal Policy Optimization (PPO), which have been proven unstable and challenging to tune for language models. Moreover, PPO requires complex distributed system implementation, hindering the efficiency of large-scale distributed training. In this study, we propose an offline reinforcement learning from human feedback (RLHF) framework to align LMs using pre-generated samples without interacting with RL environments. Specifically, we explore maximum likelihood estimation (MLE) with filtering, reward-weighted regression (RWR), and Decision Transformer (DT) to align language models to human preferences. By employing a loss function similar to supervised fine-tuning, our metho
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#35270;&#39057;&#25968;&#25454;&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#32972;&#38376;&#25915;&#20987;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#36716;&#25442;&#39046;&#22495;&#20013;&#28155;&#21152;&#38590;&#20197;&#23519;&#35273;&#30340;&#12289;&#26102;&#38388;&#19978;&#20998;&#24067;&#30340;&#35302;&#21457;&#22120;&#26469;&#23454;&#29616;&#35823;&#20998;&#31867;&#12290;</title><link>http://arxiv.org/abs/2308.11070</link><description>&lt;p&gt;
&#22522;&#20110;&#26102;&#38388;&#20998;&#24067;&#30340;&#35270;&#39057;&#34892;&#20026;&#35782;&#21035;&#32972;&#38376;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Temporal-Distributed Backdoor Attack Against Video Based Action Recognition. (arXiv:2308.11070v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11070
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#35270;&#39057;&#25968;&#25454;&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#32972;&#38376;&#25915;&#20987;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#36716;&#25442;&#39046;&#22495;&#20013;&#28155;&#21152;&#38590;&#20197;&#23519;&#35273;&#30340;&#12289;&#26102;&#38388;&#19978;&#20998;&#24067;&#30340;&#35302;&#21457;&#22120;&#26469;&#23454;&#29616;&#35823;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#21253;&#25324;&#35270;&#39057;&#34892;&#20026;&#35782;&#21035;&#22312;&#20869;&#30340;&#21508;&#31181;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#20294;&#20173;&#28982;&#23481;&#26131;&#21463;&#21040;&#32972;&#38376;&#25915;&#20987;&#65288;&#29305;&#27931;&#20234;&#65289;&#12290;&#24403;&#27979;&#35797;&#23454;&#20363;&#65288;&#26469;&#33258;&#38750;&#30446;&#26631;&#31867;&#65289;&#23884;&#20837;&#29305;&#23450;&#35302;&#21457;&#22120;&#26102;&#65292;&#34987;&#32972;&#38376;&#30772;&#22351;&#30340;&#27169;&#22411;&#20250;&#35823;&#20998;&#31867;&#20026;&#25915;&#20987;&#32773;&#36873;&#25321;&#30340;&#30446;&#26631;&#31867;&#65292;&#21516;&#26102;&#22312;&#26080;&#25915;&#20987;&#23454;&#20363;&#19978;&#20445;&#25345;&#39640;&#20934;&#30830;&#29575;&#12290;&#23613;&#31649;&#23545;&#20110;&#22270;&#20687;&#25968;&#25454;&#30340;&#32972;&#38376;&#25915;&#20987;&#24050;&#32463;&#36827;&#34892;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#35270;&#39057;&#31995;&#32479;&#22312;&#32972;&#38376;&#25915;&#20987;&#19979;&#30340;&#26131;&#21463;&#25915;&#20987;&#24615;&#20173;&#28982;&#24456;&#23569;&#34987;&#25506;&#32034;&#12290;&#24403;&#21069;&#30340;&#30740;&#31350;&#26159;&#23545;&#22270;&#20687;&#25968;&#25454;&#30340;&#26041;&#27861;&#30340;&#30452;&#25509;&#24310;&#20280;&#65292;&#20363;&#22914;&#65292;&#35302;&#21457;&#22120;&#26159;\textbf{&#29420;&#31435;}&#23884;&#20837;&#24103;&#20013;&#30340;&#65292;&#23481;&#26131;&#34987;&#29616;&#26377;&#38450;&#24481;&#26426;&#21046;&#26816;&#27979;&#21040;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;\textit{&#31616;&#21333;}&#20294;\textit{&#26377;&#25928;}&#30340;&#35270;&#39057;&#25968;&#25454;&#32972;&#38376;&#25915;&#20987;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#25915;&#20987;&#22312;&#19968;&#20010;&#36716;&#25442;&#30340;&#39046;&#22495;&#20013;&#28155;&#21152;&#25200;&#21160;&#65292;&#20197;&#23884;&#20837;\textbf{&#38590;&#20197;&#23519;&#35273;&#30340;&#65292;&#26102;&#38388;&#19978;&#20998;&#24067;&#30340;}&#35302;&#21457;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks (DNNs) have achieved tremendous success in various applications including video action recognition, yet remain vulnerable to backdoor attacks (Trojans). The backdoor-compromised model will mis-classify to the target class chosen by the attacker when a test instance (from a non-target class) is embedded with a specific trigger, while maintaining high accuracy on attack-free instances. Although there are extensive studies on backdoor attacks against image data, the susceptibility of video-based systems under backdoor attacks remains largely unexplored. Current studies are direct extensions of approaches proposed for image data, e.g., the triggers are \textbf{independently} embedded within the frames, which tend to be detectable by existing defenses. In this paper, we introduce a \textit{simple} yet \textit{effective} backdoor attack against video data. Our proposed attack, adding perturbations in a transformed domain, plants an \textbf{imperceptible, temporally distr
&lt;/p&gt;</description></item><item><title>BLIVA&#26159;&#19968;&#20010;&#31616;&#21333;&#30340;&#22810;&#27169;&#24577;LLM&#27169;&#22411;&#65292;&#29992;&#20110;&#26356;&#22909;&#22320;&#22788;&#29702;&#23884;&#20837;&#20102;&#25991;&#26412;&#30340;&#22270;&#20687;&#19978;&#19979;&#25991;&#22330;&#26223;&#12290;&#23427;&#36890;&#36807;&#24341;&#20837;InstructBLIP&#30340;&#26597;&#35810;&#23884;&#20837;&#21644;LLaVA&#30340;&#32534;&#30721;&#34917;&#19969;&#23884;&#20837;&#25216;&#26415;&#26469;&#25913;&#36827;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#20174;&#32780;&#26377;&#25928;&#35299;&#20915;&#20102;&#25991;&#26412;&#20016;&#23500;&#30340;&#35270;&#35273;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.09936</link><description>&lt;p&gt;
BLIVA: &#19968;&#20010;&#31616;&#21333;&#30340;&#22810;&#27169;&#24577;LLM&#29992;&#20110;&#26356;&#22909;&#22320;&#22788;&#29702;&#25991;&#26412;&#20016;&#23500;&#30340;&#35270;&#35273;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
BLIVA: A Simple Multimodal LLM for Better Handling of Text-Rich Visual Questions. (arXiv:2308.09936v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09936
&lt;/p&gt;
&lt;p&gt;
BLIVA&#26159;&#19968;&#20010;&#31616;&#21333;&#30340;&#22810;&#27169;&#24577;LLM&#27169;&#22411;&#65292;&#29992;&#20110;&#26356;&#22909;&#22320;&#22788;&#29702;&#23884;&#20837;&#20102;&#25991;&#26412;&#30340;&#22270;&#20687;&#19978;&#19979;&#25991;&#22330;&#26223;&#12290;&#23427;&#36890;&#36807;&#24341;&#20837;InstructBLIP&#30340;&#26597;&#35810;&#23884;&#20837;&#21644;LLaVA&#30340;&#32534;&#30721;&#34917;&#19969;&#23884;&#20837;&#25216;&#26415;&#26469;&#25913;&#36827;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#20174;&#32780;&#26377;&#25928;&#35299;&#20915;&#20102;&#25991;&#26412;&#20016;&#23500;&#30340;&#35270;&#35273;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLM&#65289;&#36890;&#36807;&#25972;&#21512;&#35270;&#35273;&#29702;&#35299;&#33021;&#21147;&#25193;&#23637;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#22312;&#35299;&#20915;&#24320;&#25918;&#24335;&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#20219;&#21153;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#26080;&#27861;&#20934;&#30830;&#35299;&#37322;&#23884;&#20837;&#25991;&#26412;&#30340;&#22270;&#20687;&#65292;&#36825;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#32463;&#24120;&#21457;&#29983;&#12290;&#20174;&#22270;&#20687;&#20013;&#25552;&#21462;&#20449;&#24687;&#30340;&#26631;&#20934;&#27969;&#31243;&#36890;&#24120;&#28041;&#21450;&#23398;&#20064;&#19968;&#32452;&#22266;&#23450;&#30340;&#26597;&#35810;&#23884;&#20837;&#12290;&#36825;&#20123;&#23884;&#20837;&#34987;&#35774;&#35745;&#20026;&#23553;&#35013;&#22270;&#20687;&#19978;&#19979;&#25991;&#65292;&#24182;&#38543;&#21518;&#29992;&#20316;LLM&#20013;&#30340;&#36719;&#25552;&#31034;&#36755;&#20837;&#12290;&#28982;&#32780;&#65292;&#36825;&#20010;&#36807;&#31243;&#21463;&#20196;&#29260;&#25968;&#37327;&#30340;&#38480;&#21046;&#65292;&#21487;&#33021;&#38480;&#21046;&#23545;&#25991;&#26412;&#20016;&#23500;&#30340;&#19978;&#19979;&#25991;&#22330;&#26223;&#30340;&#35782;&#21035;&#12290;&#20026;&#20102;&#25913;&#36827;&#36825;&#19968;&#28857;&#65292;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;BLIVA&#65306;InstructBLIP with Visual Assistant&#30340;&#22686;&#24378;&#29256;&#26412;&#12290;BLIVA&#38598;&#25104;&#20102;&#26469;&#33258;InstructBLIP&#30340;&#26597;&#35810;&#23884;&#20837;&#65292;&#24182;&#23558;&#32534;&#30721;&#30340;&#34917;&#19969;&#23884;&#20837;&#30452;&#25509;&#25237;&#24433;&#21040;LLM&#20013;&#65292;&#36825;&#26159;&#21463;&#21040;LLaVA&#30340;&#21551;&#21457;&#30340;&#19968;&#31181;&#25216;&#26415;&#12290;&#36825;&#31181;&#26041;&#27861;&#26377;&#21161;&#20110;&#22788;&#29702;&#25991;&#26412;&#20016;&#23500;&#30340;&#35270;&#35273;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision Language Models (VLMs), which extend Large Language Models (LLM) by incorporating visual understanding capability, have demonstrated significant advancements in addressing open-ended visual question-answering (VQA) tasks. However, these models cannot accurately interpret images infused with text, a common occurrence in real-world scenarios. Standard procedures for extracting information from images often involve learning a fixed set of query embeddings. These embeddings are designed to encapsulate image contexts and are later used as soft prompt inputs in LLMs. Yet, this process is limited to the token count, potentially curtailing the recognition of scenes with text-rich context. To improve upon them, the present study introduces BLIVA: an augmented version of InstructBLIP with Visual Assistant. BLIVA incorporates the query embeddings from InstructBLIP and also directly projects encoded patch embeddings into the LLM, a technique inspired by LLaVA. This approach assists the mode
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#36830;&#25509;&#22522;&#30784;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;CLIP&#12289;CLAP&#21644;AudioLDM&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#35270;&#21548;&#29983;&#25104;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.09300</link><description>&lt;p&gt;
V2A-Mapper&#65306;&#36890;&#36807;&#36830;&#25509;&#22522;&#30784;&#27169;&#22411;&#23454;&#29616;&#36731;&#37327;&#32423;&#30340;&#35270;&#21548;&#29983;&#25104;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
V2A-Mapper: A Lightweight Solution for Vision-to-Audio Generation by Connecting Foundation Models. (arXiv:2308.09300v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09300
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#36830;&#25509;&#22522;&#30784;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;CLIP&#12289;CLAP&#21644;AudioLDM&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#35270;&#21548;&#29983;&#25104;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#20013;&#65292;&#22522;&#20110;&#19968;&#32452;&#22522;&#30784;&#27169;&#22411;&#65288;FMs&#65289;&#26500;&#24314;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#27491;&#22312;&#25104;&#20026;&#19968;&#31181;&#26032;&#30340;&#33539;&#24335;&#12290;&#36825;&#20123;&#27169;&#22411;&#36890;&#36807;&#22823;&#37327;&#25968;&#25454;&#23398;&#20064;&#24471;&#21040;&#30340;&#20195;&#34920;&#24615;&#21644;&#29983;&#25104;&#33021;&#21147;&#21487;&#20197;&#36731;&#26494;&#22320;&#36866;&#24212;&#21644;&#36801;&#31227;&#33267;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#20174;&#22836;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#22312;&#28041;&#21450;&#38899;&#39057;&#27169;&#24577;&#30340;&#36328;&#27169;&#24577;&#29983;&#25104;&#20013;&#65292;&#21033;&#29992;FMs&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#20805;&#20998;&#30740;&#31350;&#30340;&#39046;&#22495;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#20174;&#35270;&#35273;&#36755;&#20837;&#20013;&#33258;&#21160;&#29983;&#25104;&#35821;&#20041;&#30456;&#20851;&#30340;&#22768;&#38899;&#26159;&#36328;&#27169;&#24577;&#29983;&#25104;&#30740;&#31350;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#35270;&#21548;&#29983;&#25104;&#65288;V2A&#65289;&#38382;&#39064;&#65292;&#29616;&#26377;&#26041;&#27861;&#20542;&#21521;&#20110;&#20351;&#29992;&#35268;&#27169;&#36866;&#20013;&#30340;&#25968;&#25454;&#38598;&#20174;&#22836;&#35774;&#35745;&#21644;&#26500;&#24314;&#22797;&#26434;&#30340;&#31995;&#32479;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#21033;&#29992;&#22522;&#30784;&#27169;&#22411;&#65292;&#20855;&#20307;&#26469;&#35828;&#26159;CLIP&#12289;CLAP&#21644;AudioLDM&#12290;&#25105;&#20204;&#39318;&#20808;&#30740;&#31350;&#20102;&#35270;&#35273;CLIP&#27169;&#22411;&#21644;&#21548;&#35273;CLAP&#27169;&#22411;&#30340;&#28508;&#22312;&#31354;&#38388;&#20043;&#38388;&#30340;&#39046;&#22495;&#24046;&#36317;&#12290;&#28982;&#21518;&#25105;&#20204;&#25552;&#20986;&#20102;...
&lt;/p&gt;
&lt;p&gt;
Building artificial intelligence (AI) systems on top of a set of foundation models (FMs) is becoming a new paradigm in AI research. Their representative and generative abilities learnt from vast amounts of data can be easily adapted and transferred to a wide range of downstream tasks without extra training from scratch. However, leveraging FMs in cross-modal generation remains under-researched when audio modality is involved. On the other hand, automatically generating semantically-relevant sound from visual input is an important problem in cross-modal generation studies. To solve this vision-to-audio (V2A) generation problem, existing methods tend to design and build complex systems from scratch using modestly sized datasets. In this paper, we propose a lightweight solution to this problem by leveraging foundation models, specifically CLIP, CLAP, and AudioLDM. We first investigate the domain gap between the latent space of the visual CLIP and the auditory CLAP models. Then we propose 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#30693;&#35782;&#36861;&#36394;&#20013;&#24120;&#35265;&#30340;&#31572;&#26696;&#20559;&#35265;&#29616;&#35937;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;COunterfactual REasoning (CORE)&#26694;&#26550;&#65292;&#20174;&#22240;&#26524;&#20851;&#31995;&#30340;&#35282;&#24230;&#35299;&#20915;&#20102;&#38382;&#39064;&#65292;&#24182;&#20943;&#36731;&#20102;&#31572;&#26696;&#20559;&#35265;&#23545;&#20110;&#23398;&#29983;&#30693;&#35782;&#29366;&#24577;&#29702;&#35299;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2308.07779</link><description>&lt;p&gt;
&#25105;&#20204;&#20805;&#20998;&#29702;&#35299;&#23398;&#29983;&#30340;&#30693;&#35782;&#29366;&#24577;&#21527;&#65311;&#35782;&#21035;&#21644;&#20943;&#36731;&#30693;&#35782;&#36861;&#36394;&#20013;&#30340;&#31572;&#26696;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Do We Fully Understand Students' Knowledge States? Identifying and Mitigating Answer Bias in Knowledge Tracing. (arXiv:2308.07779v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07779
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#30693;&#35782;&#36861;&#36394;&#20013;&#24120;&#35265;&#30340;&#31572;&#26696;&#20559;&#35265;&#29616;&#35937;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;COunterfactual REasoning (CORE)&#26694;&#26550;&#65292;&#20174;&#22240;&#26524;&#20851;&#31995;&#30340;&#35282;&#24230;&#35299;&#20915;&#20102;&#38382;&#39064;&#65292;&#24182;&#20943;&#36731;&#20102;&#31572;&#26696;&#20559;&#35265;&#23545;&#20110;&#23398;&#29983;&#30693;&#35782;&#29366;&#24577;&#29702;&#35299;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#36861;&#36394;&#65288;KT&#65289;&#26088;&#22312;&#36890;&#36807;&#23398;&#29983;&#19982;&#19982;&#27010;&#24565;&#30456;&#20851;&#30340;&#38382;&#39064;&#30340;&#23398;&#20064;&#20114;&#21160;&#26469;&#30417;&#27979;&#23398;&#29983;&#19981;&#26029;&#21464;&#21270;&#30340;&#30693;&#35782;&#29366;&#24577;&#65292;&#24182;&#21487;&#20197;&#36890;&#36807;&#39044;&#27979;&#23398;&#29983;&#22312;&#26410;&#26469;&#38382;&#39064;&#19978;&#30340;&#34920;&#29616;&#26469;&#36827;&#34892;&#38388;&#25509;&#35780;&#20272;&#12290;&#26412;&#25991;&#35266;&#23519;&#21040;&#19968;&#20010;&#24120;&#35265;&#30340;&#29616;&#35937;&#65292;&#21363;&#31572;&#26696;&#20559;&#35265;&#65292;&#21363;&#27599;&#20010;&#38382;&#39064;&#30340;&#27491;&#30830;&#31572;&#26696;&#21644;&#38169;&#35823;&#31572;&#26696;&#39640;&#24230;&#19981;&#24179;&#34913;&#30340;&#20998;&#24067;&#12290;&#29616;&#26377;&#27169;&#22411;&#20542;&#21521;&#20110;&#23558;&#31572;&#26696;&#20559;&#35265;&#20316;&#20026;&#19968;&#20010;&#25463;&#24452;&#26469;&#23454;&#29616;&#22312;KT&#20013;&#30340;&#39640;&#39044;&#27979;&#24615;&#33021;&#65292;&#20174;&#32780;&#26410;&#33021;&#20805;&#20998;&#29702;&#35299;&#23398;&#29983;&#30340;&#30693;&#35782;&#29366;&#24577;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20174;&#22240;&#26524;&#20851;&#31995;&#30340;&#35282;&#24230;&#26469;&#22788;&#29702;KT&#20219;&#21153;&#12290;&#39318;&#20808;&#24314;&#31435;&#20102;KT&#30340;&#22240;&#26524;&#22270;&#65292;&#20174;&#20013;&#25105;&#20204;&#30830;&#23450;&#20102;&#31572;&#26696;&#20559;&#35265;&#23545;&#23398;&#29983;&#21453;&#24212;&#30340;&#30452;&#25509;&#22240;&#26524;&#25928;&#24212;&#12290;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#31574;&#21453;&#20107;&#23454;&#25512;&#29702;&#65288;CORE&#65289;&#26694;&#26550;&#36827;&#34892;KT&#65292;&#35813;&#26694;&#26550;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20998;&#21035;&#25429;&#25417;&#20102;&#24635;&#22240;&#26524;&#25928;&#24212;&#21644;&#30452;&#25509;&#22240;&#26524;&#25928;&#24212;&#65292;&#24182;&#20943;&#36731;&#20102;&#31572;&#26696;&#20559;&#35265;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge tracing (KT) aims to monitor students' evolving knowledge states through their learning interactions with concept-related questions, and can be indirectly evaluated by predicting how students will perform on future questions. In this paper, we observe that there is a common phenomenon of answer bias, i.e., a highly unbalanced distribution of correct and incorrect answers for each question. Existing models tend to memorize the answer bias as a shortcut for achieving high prediction performance in KT, thereby failing to fully understand students' knowledge states. To address this issue, we approach the KT task from a causality perspective. A causal graph of KT is first established, from which we identify that the impact of answer bias lies in the direct causal effect of questions on students' responses. A novel COunterfactual REasoning (CORE) framework for KT is further proposed, which separately captures the total causal effect and direct causal effect during training, and mit
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26144;&#23556;&#24863;&#30693;&#30340;&#22270;&#24418;&#21387;&#32553;&#26041;&#27861;&#65288;MCond&#65289;&#65292;&#36890;&#36807;&#23398;&#20064;&#33410;&#28857;&#20043;&#38388;&#30340;&#26144;&#23556;&#20851;&#31995;&#65292;&#23454;&#29616;&#20102;&#22312;&#21512;&#25104;&#22270;&#20013;&#39640;&#25928;&#22320;&#22788;&#29702;&#26410;&#30693;&#25968;&#25454;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.15967</link><description>&lt;p&gt;
&#22270;&#24418;&#21387;&#32553;&#26041;&#27861;&#29992;&#20110;&#24402;&#32435;&#33410;&#28857;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Graph Condensation for Inductive Node Representation Learning. (arXiv:2307.15967v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15967
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26144;&#23556;&#24863;&#30693;&#30340;&#22270;&#24418;&#21387;&#32553;&#26041;&#27861;&#65288;MCond&#65289;&#65292;&#36890;&#36807;&#23398;&#20064;&#33410;&#28857;&#20043;&#38388;&#30340;&#26144;&#23556;&#20851;&#31995;&#65292;&#23454;&#29616;&#20102;&#22312;&#21512;&#25104;&#22270;&#20013;&#39640;&#25928;&#22320;&#22788;&#29702;&#26410;&#30693;&#25968;&#25454;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#22270;&#24341;&#23548;&#32593;&#32476;&#38754;&#20020;&#30528;&#35745;&#31639;&#25361;&#25112;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#19981;&#21516;&#24212;&#29992;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#22270;&#24418;&#21387;&#32553;&#20316;&#20026;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#25216;&#26415;&#20986;&#29616;&#20102;&#65292;&#23427;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#23567;&#30340;&#21512;&#25104;&#22270;&#26469;&#39640;&#25928;&#22320;&#35757;&#32451;&#22270;&#24341;&#23548;&#32593;&#32476;&#24182;&#20445;&#25345;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#33410;&#28857;&#20043;&#38388;&#30340;&#25299;&#25169;&#32467;&#26500;&#65292;&#22270;&#24418;&#21387;&#32553;&#20165;&#38480;&#20110;&#21387;&#32553;&#35266;&#23519;&#21040;&#30340;&#35757;&#32451;&#33410;&#28857;&#21450;&#20854;&#23545;&#24212;&#30340;&#32467;&#26500;&#65292;&#22240;&#27492;&#32570;&#20047;&#26377;&#25928;&#22788;&#29702;&#26410;&#30693;&#25968;&#25454;&#30340;&#33021;&#21147;&#12290;&#22240;&#27492;&#65292;&#22312;&#25512;&#29702;&#38454;&#27573;&#20173;&#38656;&#35201;&#21407;&#22987;&#22823;&#22270;&#26469;&#23545;&#24402;&#32435;&#33410;&#28857;&#36827;&#34892;&#28040;&#24687;&#20256;&#36882;&#65292;&#23548;&#33268;&#35745;&#31639;&#38656;&#27714;&#24040;&#22823;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26144;&#23556;&#24863;&#30693;&#30340;&#22270;&#24418;&#21387;&#32553;&#65288;MCond&#65289;&#26041;&#27861;&#65292;&#26126;&#30830;&#23398;&#20064;&#20174;&#21407;&#22987;&#33410;&#28857;&#21040;&#21512;&#25104;&#33410;&#28857;&#30340;&#19968;&#23545;&#22810;&#33410;&#28857;&#26144;&#23556;&#65292;&#20197;&#26080;&#32541;&#22320;&#23558;&#26032;&#33410;&#28857;&#25972;&#21512;&#21040;&#21512;&#25104;&#22270;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks (GNNs) encounter significant computational challenges when handling large-scale graphs, which severely restricts their efficacy across diverse applications. To address this limitation, graph condensation has emerged as a promising technique, which constructs a small synthetic graph for efficiently training GNNs while retaining performance. However, due to the topology structure among nodes, graph condensation is limited to condensing only the observed training nodes and their corresponding structure, thus lacking the ability to effectively handle the unseen data. Consequently, the original large graph is still required in the inference stage to perform message passing to inductive nodes, resulting in substantial computational demands. To overcome this issue, we propose mapping-aware graph condensation (MCond), explicitly learning the one-to-many node mapping from original nodes to synthetic nodes to seamlessly integrate new nodes into the synthetic graph for induc
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#25945;&#32946;&#39046;&#22495;&#20013;&#65292;&#30001;&#20154;&#31867;&#21644;&#29983;&#25104;&#24615;&#35821;&#35328;&#27169;&#22411;&#21327;&#20316;&#32534;&#20889;&#30340;&#28151;&#21512;&#25991;&#26412;&#30340;AI&#20869;&#23481;&#26816;&#27979;&#26041;&#27861;&#65292;&#23558;&#20854;&#24418;&#24335;&#21270;&#20026;&#35782;&#21035;&#36716;&#25442;&#28857;&#30340;&#20219;&#21153;&#65292;&#20197;&#21306;&#20998;&#20154;&#31867;&#32534;&#20889;&#21644;AI&#29983;&#25104;&#30340;&#37096;&#20998;&#12290;</title><link>http://arxiv.org/abs/2307.12267</link><description>&lt;p&gt;
&#38754;&#21521;&#25945;&#32946;&#20013;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;&#28151;&#21512;&#35770;&#25991;&#30340;&#33258;&#21160;&#36793;&#30028;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Towards Automatic Boundary Detection for Human-AI Collaborative Hybrid Essay in Education. (arXiv:2307.12267v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12267
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#25945;&#32946;&#39046;&#22495;&#20013;&#65292;&#30001;&#20154;&#31867;&#21644;&#29983;&#25104;&#24615;&#35821;&#35328;&#27169;&#22411;&#21327;&#20316;&#32534;&#20889;&#30340;&#28151;&#21512;&#25991;&#26412;&#30340;AI&#20869;&#23481;&#26816;&#27979;&#26041;&#27861;&#65292;&#23558;&#20854;&#24418;&#24335;&#21270;&#20026;&#35782;&#21035;&#36716;&#25442;&#28857;&#30340;&#20219;&#21153;&#65292;&#20197;&#21306;&#20998;&#20154;&#31867;&#32534;&#20889;&#21644;AI&#29983;&#25104;&#30340;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;ChatGPT&#65289;&#33021;&#22815;&#22312;&#25552;&#20379;&#20855;&#20307;&#25351;&#23548;&#30340;&#24773;&#20917;&#19979;&#29983;&#25104;&#31867;&#20284;&#20110;&#20154;&#31867;&#30340;&#27969;&#30021;&#22238;&#31572;&#12290;&#23613;&#31649;&#25215;&#35748;&#25216;&#26415;&#36827;&#27493;&#24102;&#26469;&#30340;&#20415;&#21033;&#65292;&#25945;&#32946;&#32773;&#20063;&#25285;&#24515;&#23398;&#29983;&#21487;&#33021;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#26469;&#23436;&#25104;&#20889;&#20316;&#20219;&#21153;&#24182;&#23558;&#20854;&#20551;&#20882;&#20026;&#33258;&#24049;&#30340;&#21407;&#21019;&#20316;&#21697;&#12290;&#34429;&#28982;&#26377;&#24456;&#22810;AI&#20869;&#23481;&#26816;&#27979;&#30740;&#31350;&#26159;&#22522;&#20110;&#36825;&#20123;&#25285;&#24551;&#36827;&#34892;&#30340;&#65292;&#20294;&#22823;&#22810;&#25968;&#20043;&#21069;&#30340;&#30740;&#31350;&#23558;AI&#20869;&#23481;&#26816;&#27979;&#24314;&#27169;&#20026;&#19968;&#20010;&#20998;&#31867;&#38382;&#39064;&#65292;&#20551;&#35774;&#19968;&#20010;&#25991;&#26412;&#35201;&#20040;&#23436;&#20840;&#30001;&#20154;&#31867;&#32534;&#20889;&#65292;&#35201;&#20040;&#23436;&#20840;&#30001;AI&#29983;&#25104;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;AI&#20869;&#23481;&#26816;&#27979;&#22312;&#19968;&#20010;&#23569;&#26377;&#25506;&#32034;&#20294;&#21364;&#29616;&#23454;&#30340;&#24773;&#20917;&#19979;&#65292;&#21363;&#26816;&#27979;&#30340;&#25991;&#26412;&#30001;&#20154;&#31867;&#21644;&#29983;&#25104;&#24615;&#35821;&#35328;&#27169;&#22411;&#65288;&#21363;&#28151;&#21512;&#25991;&#26412;&#65289;&#21327;&#20316;&#32534;&#20889;&#12290;&#25105;&#20204;&#39318;&#20808;&#23558;&#26816;&#27979;&#20219;&#21153;&#24418;&#24335;&#21270;&#20026;&#20174;&#32473;&#23450;&#30340;&#28151;&#21512;&#25991;&#26412;&#20013;&#35782;&#21035;&#20154;&#31867;&#32534;&#20889;&#20869;&#23481;&#21644;AI&#29983;&#25104;&#20869;&#23481;&#20043;&#38388;&#30340;&#36716;&#25442;&#28857;&#65288;&#36793;&#30028;&#26816;&#27979;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent large language models (LLMs), e.g., ChatGPT, have been able to generate human-like and fluent responses when provided with specific instructions. While admitting the convenience brought by technological advancement, educators also have concerns that students might leverage LLMs to complete their writing assignments and pass them off as their original work. Although many AI content detection studies have been conducted as a result of such concerns, most of these prior studies modeled AI content detection as a classification problem, assuming that a text is either entirely human-written or entirely AI-generated. In this study, we investigated AI content detection in a rarely explored yet realistic setting where the text to be detected is collaboratively written by human and generative LLMs (i.e., hybrid text). We first formalized the detection task as identifying the transition points between human-written content and AI-generated content from a given hybrid text (boundary det
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#20998;&#25955;&#24335;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#36890;&#20449;&#23433;&#20840;&#25361;&#25112;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#23433;&#20840;&#27169;&#22359;&#65292;&#36890;&#36807;&#32467;&#21512;&#21152;&#23494;&#25216;&#26415;&#21644;&#31227;&#21160;&#30446;&#26631;&#38450;&#24481;&#25216;&#26415;&#26469;&#23545;&#25239;&#36890;&#20449;&#25915;&#20987;&#12290;</title><link>http://arxiv.org/abs/2307.11730</link><description>&lt;p&gt;
&#36890;&#36807;&#31227;&#21160;&#30446;&#26631;&#38450;&#24481;&#20943;&#36731;&#20998;&#25955;&#24335;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#36890;&#20449;&#23041;&#32961;
&lt;/p&gt;
&lt;p&gt;
Mitigating Communications Threats in Decentralized Federated Learning through Moving Target Defense. (arXiv:2307.11730v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11730
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#20998;&#25955;&#24335;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#36890;&#20449;&#23433;&#20840;&#25361;&#25112;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#23433;&#20840;&#27169;&#22359;&#65292;&#36890;&#36807;&#32467;&#21512;&#21152;&#23494;&#25216;&#26415;&#21644;&#31227;&#21160;&#30446;&#26631;&#38450;&#24481;&#25216;&#26415;&#26469;&#23545;&#25239;&#36890;&#20449;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#25955;&#24335;&#32852;&#37030;&#23398;&#20064;&#65288;DFL&#65289;&#30340;&#20852;&#36215;&#20351;&#24471;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21487;&#20197;&#22312;&#32852;&#37030;&#21442;&#19982;&#26041;&#20043;&#38388;&#36827;&#34892;&#35757;&#32451;&#65292;&#20419;&#36827;&#20102;&#20998;&#25955;&#24335;&#27169;&#22411;&#32858;&#21512;&#24182;&#20943;&#23569;&#23545;&#26381;&#21153;&#22120;&#30340;&#20381;&#36182;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#24341;&#20837;&#20102;&#29420;&#29305;&#30340;&#36890;&#20449;&#23433;&#20840;&#25361;&#25112;&#65292;&#23578;&#26410;&#22312;&#25991;&#29486;&#20013;&#24471;&#21040;&#20805;&#20998;&#35299;&#20915;&#12290;&#36825;&#20123;&#25361;&#25112;&#20027;&#35201;&#28304;&#20110;&#32858;&#21512;&#36807;&#31243;&#30340;&#20998;&#25955;&#24615;&#36136;&#12289;&#21442;&#19982;&#32773;&#30340;&#22810;&#26679;&#21270;&#35282;&#33394;&#21644;&#36131;&#20219;&#20197;&#21450;&#32570;&#20047;&#30417;&#31649;&#21644;&#32531;&#35299;&#23041;&#32961;&#30340;&#20013;&#22830;&#26426;&#26500;&#12290;&#26412;&#25991;&#38024;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#39318;&#20808;&#30028;&#23450;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#23041;&#32961;&#27169;&#22411;&#65292;&#31361;&#20986;&#20102;DFL&#36890;&#20449;&#30340;&#28508;&#22312;&#39118;&#38505;&#12290;&#38024;&#23545;&#36825;&#20123;&#30830;&#23450;&#30340;&#39118;&#38505;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#19987;&#20026;DFL&#24179;&#21488;&#35774;&#35745;&#30340;&#23433;&#20840;&#27169;&#22359;&#26469;&#23545;&#25239;&#22522;&#20110;&#36890;&#20449;&#30340;&#25915;&#20987;&#12290;&#35813;&#27169;&#22359;&#32467;&#21512;&#20102;&#23545;&#31216;&#21644;&#38750;&#23545;&#31216;&#21152;&#23494;&#31561;&#23433;&#20840;&#25216;&#26415;&#19982;&#31227;&#21160;&#30446;&#26631;&#38450;&#24481;&#65288;MTD&#65289;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rise of Decentralized Federated Learning (DFL) has enabled the training of machine learning models across federated participants, fostering decentralized model aggregation and reducing dependence on a server. However, this approach introduces unique communication security challenges that have yet to be thoroughly addressed in the literature. These challenges primarily originate from the decentralized nature of the aggregation process, the varied roles and responsibilities of the participants, and the absence of a central authority to oversee and mitigate threats. Addressing these challenges, this paper first delineates a comprehensive threat model, highlighting the potential risks of DFL communications. In response to these identified risks, this work introduces a security module designed for DFL platforms to counter communication-based attacks. The module combines security techniques such as symmetric and asymmetric encryption with Moving Target Defense (MTD) techniques, including
&lt;/p&gt;</description></item><item><title>DiTTO&#26159;&#19968;&#31181;&#25193;&#25955;&#21551;&#21457;&#30340;&#31639;&#23376;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;Transformer&#26550;&#26500;&#30340;&#20803;&#32032;&#65292;&#26080;&#38656;&#26102;&#38388;&#31163;&#25955;&#21270;&#36830;&#32493;&#35299;&#20915;&#26102;&#38388;&#30456;&#20851;PDEs&#65292;&#24182;&#22312;&#22810;&#32500;&#24230;&#30340;&#21508;&#31181;PDE&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#24615;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.09072</link><description>&lt;p&gt;
DiTTO&#65306;&#21463;&#25193;&#25955;&#21551;&#21457;&#30340;&#26102;&#31354;&#36716;&#25442;&#31639;&#23376;
&lt;/p&gt;
&lt;p&gt;
DiTTO: Diffusion-inspired Temporal Transformer Operator. (arXiv:2307.09072v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09072
&lt;/p&gt;
&lt;p&gt;
DiTTO&#26159;&#19968;&#31181;&#25193;&#25955;&#21551;&#21457;&#30340;&#31639;&#23376;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;Transformer&#26550;&#26500;&#30340;&#20803;&#32032;&#65292;&#26080;&#38656;&#26102;&#38388;&#31163;&#25955;&#21270;&#36830;&#32493;&#35299;&#20915;&#26102;&#38388;&#30456;&#20851;PDEs&#65292;&#24182;&#22312;&#22810;&#32500;&#24230;&#30340;&#21508;&#31181;PDE&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#24615;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#35299;&#20915;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDEs&#65289;&#24050;&#32463;&#36234;&#26469;&#36234;&#24120;&#35265;&#12290;&#26368;&#36817;&#30340;&#31639;&#23376;&#23398;&#20064;&#33539;&#24335;&#30340;&#21457;&#23637;&#20351;&#24471;&#35299;&#20915;&#26356;&#24191;&#27867;PDE&#30456;&#20851;&#38382;&#39064;&#25104;&#20026;&#21487;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#23376;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#36830;&#32493;&#22320;&#35299;&#20915;&#26102;&#38388;&#30456;&#20851;&#30340;PDEs&#65292;&#32780;&#19981;&#38656;&#35201;&#20219;&#20309;&#26102;&#38388;&#31163;&#25955;&#21270;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21517;&#20026;DiTTO&#65292;&#21463;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#30340;&#21551;&#21457;&#12290;&#23613;&#31649;&#25193;&#25955;&#27169;&#22411;&#36890;&#24120;&#29992;&#20110;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#20219;&#21153;&#65292;&#20294;&#20854;&#26102;&#38388;&#26465;&#20214;&#26426;&#21046;&#23545;PDEs&#38750;&#24120;&#26377;&#29992;&#12290;&#25193;&#25955;&#21551;&#21457;&#30340;&#26694;&#26550;&#19982;Transformer&#26550;&#26500;&#30340;&#20803;&#32032;&#30456;&#32467;&#21512;&#65292;&#20197;&#25552;&#39640;&#20854;&#33021;&#21147;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#26032;&#26041;&#27861;&#22312;&#22810;&#32500;&#24230;&#30340;&#24191;&#27867;PDE&#19978;&#30340;&#26377;&#25928;&#24615;&#65292;&#21253;&#25324;1&#32500;Burgers&#26041;&#31243;&#65292;2&#32500;Navier-Stokes&#26041;&#31243;&#21644;2&#32500;&#21644;3&#32500;&#22768;&#27874;&#26041;&#31243;&#12290;DiTTO&#22312;&#36825;&#20123;&#38382;&#39064;&#30340;&#20934;&#30830;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Solving partial differential equations (PDEs) using a data-driven approach has become increasingly common. The recent development of the operator learning paradigm has enabled the solution of a broader range of PDE-related problems. We propose an operator learning method to solve time-dependent PDEs continuously in time without needing any temporal discretization. The proposed approach, named DiTTO, is inspired by latent diffusion models. While diffusion models are usually used in generative artificial intelligence tasks, their time-conditioning mechanism is extremely useful for PDEs. The diffusion-inspired framework is combined with elements from the Transformer architecture to improve its capabilities.  We demonstrate the effectiveness of the new approach on a wide variety of PDEs in multiple dimensions, namely the 1-D Burgers' equation, 2-D Navier-Stokes equations, and the acoustic wave equation in 2-D and 3-D. DiTTO achieves state-of-the-art results in terms of accuracy for these p
&lt;/p&gt;</description></item><item><title>Espaloma-0.3.0&#26159;&#19968;&#20010;&#29992;&#20110;&#34507;&#30333;&#36136;-&#37197;&#20307;&#31995;&#32479;&#27169;&#25311;&#30340;&#26426;&#22120;&#23398;&#20064;&#20998;&#23376;&#21147;&#23398;&#21147;&#22330;&#65292;&#36890;&#36807;&#33021;&#37327;&#21644;&#21147;&#30340;&#25311;&#21512;&#32435;&#20837;&#37327;&#23376;&#21270;&#23398;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#20855;&#26377;&#28789;&#27963;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.07085</link><description>&lt;p&gt;
Espaloma-0.3.0: &#29992;&#20110;&#34507;&#30333;&#36136;-&#37197;&#20307;&#31995;&#32479;&#27169;&#25311;&#30340;&#26426;&#22120;&#23398;&#20064;&#20998;&#23376;&#21147;&#23398;&#21147;&#22330;&#21450;&#20854;&#25193;&#23637;
&lt;/p&gt;
&lt;p&gt;
Espaloma-0.3.0: Machine-learned molecular mechanics force field for the simulation of protein-ligand systems and beyond. (arXiv:2307.07085v1 [physics.chem-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07085
&lt;/p&gt;
&lt;p&gt;
Espaloma-0.3.0&#26159;&#19968;&#20010;&#29992;&#20110;&#34507;&#30333;&#36136;-&#37197;&#20307;&#31995;&#32479;&#27169;&#25311;&#30340;&#26426;&#22120;&#23398;&#20064;&#20998;&#23376;&#21147;&#23398;&#21147;&#22330;&#65292;&#36890;&#36807;&#33021;&#37327;&#21644;&#21147;&#30340;&#25311;&#21512;&#32435;&#20837;&#37327;&#23376;&#21270;&#23398;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#20855;&#26377;&#28789;&#27963;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#23376;&#21147;&#23398;&#65288;MM&#65289;&#21147;&#22330;&#26159;&#36890;&#36807;&#31616;&#21333;&#30340;&#19968;&#23545;&#19968;&#21644;&#22810;&#39033;&#24335;&#39033;&#26469;&#34920;&#24449;&#20998;&#23376;&#31995;&#32479;&#33021;&#37327;&#26223;&#35266;&#30340;&#27169;&#22411;&#12290;&#20256;&#32479;&#19978;&#65292;&#23427;&#20204;&#20381;&#36182;&#20110;&#20154;&#24037;&#19987;&#23478;&#31574;&#21010;&#12289;&#19981;&#28789;&#27963;&#19988;&#38590;&#20197;&#25193;&#23637;&#30340;&#31163;&#25955;&#21270;&#21270;&#23398;&#21442;&#25968;&#36171;&#20540;&#35268;&#21017;&#65292;&#21363;&#21407;&#23376;&#25110;&#20215;&#24577;&#31867;&#22411;&#12290;&#26368;&#36817;&#65292;&#20154;&#20204;&#23545;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#26367;&#20195;&#27492;&#36807;&#31243;&#65292;&#24182;&#20351;&#21442;&#25968;&#21270;&#26041;&#26696;&#33021;&#22815;&#30452;&#25509;&#20174;&#37327;&#23376;&#21270;&#23398;&#35745;&#31639;&#25110;&#20957;&#32858;&#30456;&#25968;&#25454;&#20013;&#20197;&#31471;&#21040;&#31471;&#30340;&#21487;&#24494;&#20998;&#26041;&#24335;&#36827;&#34892;&#23398;&#20064;&#34920;&#31034;&#20986;&#20102;&#24040;&#22823;&#20852;&#36259;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;&#33021;&#37327;&#21644;&#21147;&#30340;&#36866;&#24212;&#24615;&#25311;&#21512;&#30452;&#25509;&#32435;&#20837;&#35757;&#32451;&#36807;&#31243;&#65292;&#25193;&#23637;&#20102;Espaloma&#30340;&#31471;&#21040;&#31471;&#21487;&#24494;&#20998;&#21147;&#22330;&#26500;&#24314;&#26041;&#27861;&#12290;&#22522;&#20110;OpenMM SPICE&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#31574;&#21010;&#20102;&#19968;&#20010;&#21253;&#21547;&#19982;&#29983;&#29289;&#20998;&#23376;&#24314;&#27169;&#24191;&#27867;&#30456;&#20851;&#30340;&#21270;&#23398;&#31354;&#38388;&#30340;&#25968;&#25454;&#38598;&#65292;&#28085;&#30422;&#23567;&#20998;&#23376;&#12289;&#34507;&#30333;&#36136;&#21644;RNA&#12290;&#26368;&#32456;&#24471;&#21040;&#20102;&#19968;&#20010;&#36866;&#29992;&#20110;&#34507;&#30333;&#36136;-&#37197;&#20307;&#31995;&#32479;&#27169;&#25311;&#30340;&#26426;&#22120;&#23398;&#20064;&#20998;&#23376;&#21147;&#23398;&#21147;&#22330;&#12290;
&lt;/p&gt;
&lt;p&gt;
Molecular mechanics (MM) force fields -- the models that characterize the energy landscape of molecular systems via simple pairwise and polynomial terms -- have traditionally relied on human expert-curated, inflexible, and poorly extensible discrete chemical parameter assignment rules, namely atom or valence types. Recently, there has been significant interest in using graph neural networks to replace this process, while enabling the parametrization scheme to be learned in an end-to-end differentiable manner directly from quantum chemical calculations or condensed-phase data. In this paper, we extend the Espaloma end-to-end differentiable force field construction approach by incorporating both energy and force fitting directly to quantum chemical data into the training process. Building on the OpenMM SPICE dataset, we curate a dataset containing chemical spaces highly relevant to the broad interest of biomolecular modeling, covering small molecules, proteins, and RNA. The resulting for
&lt;/p&gt;</description></item><item><title>&#20256;&#25773;&#23398;&#39046;&#22495;&#20013;&#30340;&#33258;&#21160;&#21270;&#20869;&#23481;&#20998;&#26512;&#24120;&#24573;&#35270;&#20102;&#38169;&#35823;&#20998;&#31867;&#30340;&#20559;&#24046;&#65292;&#25105;&#20204;&#20171;&#32461;&#24182;&#27979;&#35797;&#20102;&#32479;&#35745;&#26041;&#27861;&#26469;&#32416;&#27491;&#36825;&#31181;&#20559;&#24046;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#26469;&#20462;&#22797;&#20043;&#12290;</title><link>http://arxiv.org/abs/2307.06483</link><description>&lt;p&gt;
&#33258;&#21160;&#21270;&#20869;&#23481;&#20998;&#26512;&#20013;&#30340;&#38169;&#35823;&#20998;&#31867;&#23548;&#33268;&#22238;&#24402;&#20998;&#26512;&#20013;&#30340;&#20559;&#24046;&#12290;&#25105;&#20204;&#33021;&#20462;&#22797;&#21527;&#65311;&#26159;&#30340;&#65292;&#25105;&#20204;&#33021;&#65281;
&lt;/p&gt;
&lt;p&gt;
Misclassification in Automated Content Analysis Causes Bias in Regression. Can We Fix It? Yes We Can!. (arXiv:2307.06483v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06483
&lt;/p&gt;
&lt;p&gt;
&#20256;&#25773;&#23398;&#39046;&#22495;&#20013;&#30340;&#33258;&#21160;&#21270;&#20869;&#23481;&#20998;&#26512;&#24120;&#24573;&#35270;&#20102;&#38169;&#35823;&#20998;&#31867;&#30340;&#20559;&#24046;&#65292;&#25105;&#20204;&#20171;&#32461;&#24182;&#27979;&#35797;&#20102;&#32479;&#35745;&#26041;&#27861;&#26469;&#32416;&#27491;&#36825;&#31181;&#20559;&#24046;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#26469;&#20462;&#22797;&#20043;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#20998;&#31867;&#22120;&#65288;ACs&#65289;&#36890;&#24120;&#36890;&#36807;&#30417;&#30563;&#24335;&#26426;&#22120;&#23398;&#20064;&#65288;SML&#65289;&#26500;&#24314;&#65292;&#21487;&#20197;&#23545;&#20174;&#25991;&#26412;&#21040;&#22270;&#29255;&#21644;&#35270;&#39057;&#30340;&#22823;&#37327;&#25968;&#25454;&#36827;&#34892;&#20998;&#31867;&#65292;&#24050;&#32463;&#25104;&#20026;&#20256;&#25773;&#31185;&#23398;&#21644;&#30456;&#20851;&#39046;&#22495;&#20013;&#24191;&#27867;&#27969;&#34892;&#30340;&#27979;&#37327;&#35774;&#22791;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#21363;&#20351;&#26159;&#39640;&#24230;&#20934;&#30830;&#30340;&#20998;&#31867;&#22120;&#20063;&#20250;&#20135;&#29983;&#38169;&#35823;&#65292;&#36825;&#23548;&#33268;&#20102;&#38169;&#35823;&#20998;&#31867;&#30340;&#20559;&#24046;&#21644;&#19979;&#28216;&#20998;&#26512;&#20013;&#35823;&#23548;&#24615;&#30340;&#32467;&#26524;&#65292;&#38500;&#38750;&#36825;&#20123;&#20998;&#26512;&#32771;&#34385;&#21040;&#36825;&#20123;&#38169;&#35823;&#12290;&#36890;&#36807;&#23545;SML&#24212;&#29992;&#30340;&#31995;&#32479;&#25991;&#29486;&#32508;&#36848;&#65292;&#25105;&#20204;&#21457;&#29616;&#20256;&#25773;&#23398;&#32773;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#24573;&#35270;&#20102;&#38169;&#35823;&#20998;&#31867;&#30340;&#20559;&#24046;&#12290;&#21407;&#21017;&#19978;&#65292;&#29616;&#26377;&#30340;&#32479;&#35745;&#26041;&#27861;&#21487;&#20197;&#20351;&#29992;&#8220;&#40644;&#37329;&#26631;&#20934;&#8221;&#39564;&#35777;&#25968;&#25454;&#65288;&#22914;&#30001;&#20154;&#31867;&#27880;&#37322;&#32773;&#21019;&#24314;&#30340;&#25968;&#25454;&#65289;&#26469;&#32416;&#27491;&#38169;&#35823;&#20998;&#31867;&#30340;&#20559;&#24046;&#65292;&#24182;&#20135;&#29983;&#19968;&#33268;&#30340;&#20272;&#35745;&#12290;&#25105;&#20204;&#20171;&#32461;&#24182;&#27979;&#35797;&#20102;&#36825;&#20123;&#26041;&#27861;&#65292;&#21253;&#25324;&#25105;&#20204;&#22312;R&#21253;misclassificationmodels&#20013;&#35774;&#35745;&#21644;&#23454;&#29616;&#30340;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#33945;&#29305;&#21345;&#27931;&#27169;&#25311;&#26469;&#25581;&#31034;&#27599;&#31181;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automated classifiers (ACs), often built via supervised machine learning (SML), can categorize large, statistically powerful samples of data ranging from text to images and video, and have become widely popular measurement devices in communication science and related fields. Despite this popularity, even highly accurate classifiers make errors that cause misclassification bias and misleading results in downstream analyses-unless such analyses account for these errors. As we show in a systematic literature review of SML applications, communication scholars largely ignore misclassification bias. In principle, existing statistical methods can use "gold standard" validation data, such as that created by human annotators, to correct misclassification bias and produce consistent estimates. We introduce and test such methods, including a new method we design and implement in the R package misclassificationmodels, via Monte Carlo simulations designed to reveal each method's limitations, which 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;EHRSHOT&#65292;&#19968;&#20010;&#29992;&#20110;&#23569;&#26679;&#26412;&#35780;&#20272;&#22522;&#30784;&#27169;&#22411;&#30340;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#22522;&#20934;&#12290;&#35813;&#35770;&#25991;&#21033;&#29992;EHRSHOT&#25968;&#25454;&#38598;&#21644;&#39044;&#35757;&#32451;&#27169;&#22411;CLMBR-T-base&#65292;&#20026;&#21307;&#30103;&#20445;&#20581;ML&#30340;&#21457;&#23637;&#25552;&#20379;&#20102;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2307.02028</link><description>&lt;p&gt;
EHRSHOT:&#19968;&#31181;&#29992;&#20110;&#23569;&#26679;&#26412;&#35780;&#20272;&#22522;&#30784;&#27169;&#22411;&#30340;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
EHRSHOT: An EHR Benchmark for Few-Shot Evaluation of Foundation Models. (arXiv:2307.02028v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02028
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;EHRSHOT&#65292;&#19968;&#20010;&#29992;&#20110;&#23569;&#26679;&#26412;&#35780;&#20272;&#22522;&#30784;&#27169;&#22411;&#30340;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#22522;&#20934;&#12290;&#35813;&#35770;&#25991;&#21033;&#29992;EHRSHOT&#25968;&#25454;&#38598;&#21644;&#39044;&#35757;&#32451;&#27169;&#22411;CLMBR-T-base&#65292;&#20026;&#21307;&#30103;&#20445;&#20581;ML&#30340;&#21457;&#23637;&#25552;&#20379;&#20102;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#19968;&#33324;&#30340;&#26426;&#22120;&#23398;&#20064;(ML)&#31038;&#21306;&#24050;&#32463;&#21463;&#30410;&#20110;&#20844;&#24320;&#30340;&#25968;&#25454;&#38598;&#12289;&#20219;&#21153;&#21644;&#27169;&#22411;&#65292;&#20294;&#26159;ML&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#30340;&#36827;&#23637;&#21463;&#21040;&#20102;&#20849;&#20139;&#36164;&#20135;&#30340;&#32570;&#20047;&#30340;&#38459;&#30861;&#12290;&#22522;&#30784;&#27169;&#22411;&#30340;&#25104;&#21151;&#20026;&#21307;&#30103;&#20445;&#20581;ML&#24102;&#26469;&#20102;&#26032;&#30340;&#25361;&#25112;&#65292;&#38656;&#35201;&#35775;&#38382;&#20849;&#20139;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#26469;&#39564;&#35777;&#24615;&#33021;&#20248;&#21183;&#12290;&#25105;&#20204;&#36890;&#36807;&#19977;&#20010;&#36129;&#29486;&#26469;&#24110;&#21161;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#21457;&#24067;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;EHRSHOT&#65292;&#20854;&#20013;&#21253;&#21547;6,739&#21517;&#26469;&#33258;&#26031;&#22374;&#31119;&#21307;&#23398;&#30340;&#24739;&#32773;&#30340;&#21435;&#35782;&#21035;&#32467;&#26500;&#21270;&#30340;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;(EHR)&#25968;&#25454;&#12290;&#19982;MIMIC-III/IV&#21644;&#20854;&#20182;&#27969;&#34892;&#30340;EHR&#25968;&#25454;&#38598;&#19981;&#21516;&#65292;EHRSHOT&#26159;&#32437;&#21521;&#30340;&#65292;&#19981;&#20165;&#23616;&#38480;&#20110;ICU/ED&#24739;&#32773;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#21457;&#24067;&#20102;CLMBR-T-base&#30340;&#26435;&#37325;&#65292;&#36825;&#26159;&#19968;&#20010;&#22312;&#32467;&#26500;&#21270;EHR&#25968;&#25454;&#20013;&#39044;&#35757;&#32451;&#30340;141M&#21442;&#25968;&#20020;&#24202;&#22522;&#30784;&#27169;&#22411;&#65292;&#35813;&#25968;&#25454;&#21253;&#25324;2.57M&#21517;&#24739;&#32773;&#12290;&#25105;&#20204;&#26159;&#26368;&#26089;&#23436;&#20840;&#21457;&#24067;&#36825;&#26679;&#19968;&#20010;&#29992;&#20110;&#32534;&#30721;EHR&#25968;&#25454;&#30340;&#27169;&#22411;&#20043;&#19968;&#65307;&#30456;&#27604;&#20043;&#19979;&#65292;&#22823;&#22810;&#25968;&#20808;&#21069;&#21457;&#24067;&#30340;&#20020;&#24202;&#25968;&#25454;&#27169;&#22411;&#65288;&#22914;GatorTron&#12289;ClinicalBER&#65289;&#24182;&#27809;&#26377;&#23436;&#20840;&#21457;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
While the general machine learning (ML) community has benefited from public datasets, tasks, and models, the progress of ML in healthcare has been hampered by a lack of such shared assets. The success of foundation models creates new challenges for healthcare ML by requiring access to shared pretrained models to validate performance benefits. We help address these challenges through three contributions. First, we publish a new dataset, EHRSHOT, which contains deidentified structured data from the electronic health records (EHRs) of 6,739 patients from Stanford Medicine. Unlike MIMIC-III/IV and other popular EHR datasets, EHRSHOT is longitudinal and not restricted to ICU/ED patients. Second, we publish the weights of CLMBR-T-base, a 141M parameter clinical foundation model pretrained on the structured EHR data of 2.57M patients. We are one of the first to fully release such a model for coded EHR data; in contrast, most prior models released for clinical data (e.g. GatorTron, ClinicalBER
&lt;/p&gt;</description></item><item><title>TSMixer&#26159;&#19968;&#31181;&#29992;&#20110;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#36731;&#37327;&#32423;MLP-Mixer&#27169;&#22411;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#25429;&#25417;&#26102;&#38388;&#24207;&#21015;&#23646;&#24615;&#24182;&#22312;&#20934;&#30830;&#24615;&#26041;&#38754;&#36229;&#36234;&#20102;Transformers&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.09364</link><description>&lt;p&gt;
TSMixer: &#29992;&#20110;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#36731;&#37327;&#32423;MLP-Mixer&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
TSMixer: Lightweight MLP-Mixer Model for Multivariate Time Series Forecasting. (arXiv:2306.09364v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09364
&lt;/p&gt;
&lt;p&gt;
TSMixer&#26159;&#19968;&#31181;&#29992;&#20110;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#36731;&#37327;&#32423;MLP-Mixer&#27169;&#22411;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#25429;&#25417;&#26102;&#38388;&#24207;&#21015;&#23646;&#24615;&#24182;&#22312;&#20934;&#30830;&#24615;&#26041;&#38754;&#36229;&#36234;&#20102;Transformers&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformers&#22240;&#20854;&#33021;&#22815;&#25429;&#25417;&#38271;&#24207;&#21015;&#20132;&#20114;&#32780;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#22791;&#21463;&#38738;&#30544;&#12290;&#28982;&#32780;&#65292;&#20854;&#20869;&#23384;&#21644;&#35745;&#31639;&#35201;&#27714;&#39640;&#30340;&#38382;&#39064;&#23545;&#38271;&#26399;&#39044;&#27979;&#26500;&#25104;&#20102;&#20005;&#37325;&#29942;&#39048;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TSMixer&#65292;&#36825;&#26159;&#19968;&#31181;&#36731;&#37327;&#32423;&#31070;&#32463;&#26550;&#26500;&#65292;&#19987;&#20026;&#22810;&#20803;&#39044;&#27979;&#21644;&#34917;&#19969;&#26102;&#38388;&#24207;&#21015;&#34920;&#31034;&#23398;&#20064;&#32780;&#35774;&#35745;&#65292;&#26159;Transformers&#30340;&#26377;&#25928;&#26367;&#20195;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#20511;&#37492;&#20102;MLP-Mixer&#27169;&#22411;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#30340;&#25104;&#21151;&#32463;&#39564;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#23558;&#35270;&#35273;MLP-Mixer&#36866;&#24212;&#20110;&#26102;&#38388;&#24207;&#21015;&#30340;&#25361;&#25112;&#65292;&#24182;&#24341;&#20837;&#20102;&#32463;&#36807;&#23454;&#39564;&#35777;&#23454;&#30340;&#32452;&#20214;&#20197;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;&#36825;&#21253;&#25324;&#19968;&#31181;&#26032;&#30340;&#35774;&#35745;&#33539;&#24335;&#65292;&#21363;&#23558;&#22312;&#32447;&#21327;&#35843;&#22836;&#38468;&#21152;&#21040;MLP-Mixer&#39592;&#24178;&#19978;&#65292;&#20197;&#26174;&#24335;&#22320;&#24314;&#27169;&#26102;&#38388;&#24207;&#21015;&#30340;&#23646;&#24615;&#65292;&#22914;&#23618;&#27425;&#32467;&#26500;&#21644;&#36890;&#36947;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#36890;&#36947;&#24314;&#27169;&#26041;&#27861;&#65292;&#24179;&#34913;&#20102;&#32534;&#30721;&#22810;&#20010;&#26102;&#38388;&#24207;&#21015;&#36890;&#36947;&#21644;&#20445;&#30041;&#21333;&#20010;&#36890;&#36947;&#20449;&#24687;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;TSMixer&#22312;&#19968;&#20803;&#21644;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20219;&#21153;&#20013;&#22343;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#38656;&#35201;&#27604;&#22522;&#20110;Transformers&#30340;&#26041;&#27861;&#23569;&#24471;&#22810;&#30340;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformers have gained popularity in time series forecasting for their ability to capture long-sequence interactions. However, their high memory and computing requirements pose a critical bottleneck for long-term forecasting. To address this, we propose TSMixer, a lightweight neural architecture exclusively composed of multi-layer perceptron (MLP) modules. TSMixer is designed for multivariate forecasting and representation learning on patched time series, providing an efficient alternative to Transformers. Our model draws inspiration from the success of MLP-Mixer models in computer vision. We demonstrate the challenges involved in adapting Vision MLP-Mixer for time series and introduce empirically validated components to enhance accuracy. This includes a novel design paradigm of attaching online reconciliation heads to the MLP-Mixer backbone, for explicitly modeling the time-series properties such as hierarchy and channel-correlations. We also propose a Hybrid channel modeling approa
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#20004;&#20010;Diffusion&#27169;&#22411;&#20013;&#25552;&#21462;&#22870;&#21169;&#20989;&#25968;&#30340;&#23454;&#29992;&#23398;&#20064;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#23548;&#33322;&#29615;&#22659;&#20013;&#25214;&#21040;&#27491;&#30830;&#30340;&#22870;&#21169;&#20989;&#25968;&#65292;&#24182;&#20197;&#27492;&#25511;&#21046;Diffusion&#27169;&#22411;&#23398;&#20064;&#36275;&#22815;&#22797;&#26434;&#30340;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2306.01804</link><description>&lt;p&gt;
&#20174;Diffusion&#27169;&#22411;&#20013;&#25552;&#21462;&#22870;&#21169;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
Extracting Reward Functions from Diffusion Models. (arXiv:2306.01804v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01804
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#20004;&#20010;Diffusion&#27169;&#22411;&#20013;&#25552;&#21462;&#22870;&#21169;&#20989;&#25968;&#30340;&#23454;&#29992;&#23398;&#20064;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#23548;&#33322;&#29615;&#22659;&#20013;&#25214;&#21040;&#27491;&#30830;&#30340;&#22870;&#21169;&#20989;&#25968;&#65292;&#24182;&#20197;&#27492;&#25511;&#21046;Diffusion&#27169;&#22411;&#23398;&#20064;&#36275;&#22815;&#22797;&#26434;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Diffusion&#27169;&#22411;&#22312;&#22270;&#20687;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#26524;&#65292;&#20063;&#34987;&#29992;&#20110;&#23398;&#20064;&#24207;&#21015;&#20915;&#31574;&#20219;&#21153;&#20013;&#30340;&#39640;&#24615;&#33021;&#31574;&#30053;&#12290;&#25105;&#20204;&#32771;&#34385;&#36890;&#36807;&#27604;&#36739;&#24314;&#27169;&#20302;&#22870;&#21169;&#34892;&#20026;&#21644;&#24314;&#27169;&#39640;&#22870;&#21169;&#34892;&#20026;&#30340;&#20915;&#31574;&#20256;&#25773;&#27169;&#22411;&#26469;&#25552;&#21462;&#22870;&#21169;&#20989;&#25968;&#30340;&#38382;&#39064;&#65307;&#36825;&#19982;&#36870;&#24378;&#21270;&#23398;&#20064;&#30456;&#20851;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#23454;&#29992;&#30340;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#23558;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#21270;&#30340;&#22870;&#21169;&#20989;&#25968;&#30340;&#26799;&#24230;&#19982;&#20004;&#20010;Diffusion&#27169;&#22411;&#30340;&#36755;&#20986;&#24046;&#24322;&#23545;&#40784;&#26469;&#25552;&#21462;&#22870;&#21169;&#20989;&#25968;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#23548;&#33322;&#29615;&#22659;&#20013;&#25214;&#21040;&#27491;&#30830;&#30340;&#22870;&#21169;&#20989;&#25968;&#65292;&#24182;&#19988;&#34920;&#26126;&#21487;&#20197;&#36890;&#36807;&#25511;&#21046;Diffusion&#27169;&#22411;&#26469;&#23398;&#20064;&#36275;&#22815;&#22797;&#26434;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models have achieved remarkable results in image generation, and have similarly been used to learn high-performing policies in sequential decision-making tasks. Decision-making diffusion models can be trained on lower-quality data, and then be steered with a reward function to generate near-optimal trajectories. We consider the problem of extracting a reward function by comparing a decision-making diffusion model that models low-reward behavior and one that models high-reward behavior; a setting related to inverse reinforcement learning. We first define the notion of a relative reward function of two diffusion models and show conditions under which it exists and is unique. We then devise a practical learning algorithm for extracting it by aligning the gradients of a reward function -- parametrized by a neural network -- to the difference in outputs of both diffusion models. Our method finds correct reward functions in navigation environments, and we demonstrate that steering 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#25945;&#24072;&#20195;&#29702;&#26041;&#27861;&#65292;&#33021;&#22815;&#20174;&#20808;&#21069;&#23398;&#20064;&#30340;&#30693;&#35782;&#20013;&#29983;&#25104;&#39640;&#36136;&#37327;&#26679;&#26412;&#30340;&#25968;&#25454;&#38598;&#65292;&#20174;&#32780;&#20351;&#23398;&#29983;&#32593;&#32476;&#21487;&#20197;&#20174;&#26679;&#26412;&#30340;&#22810;&#31181;&#35266;&#28857;&#20013;&#36827;&#34892;&#23398;&#20064;&#65292;&#22312;&#26631;&#20934;&#22522;&#20934;&#19978;&#20248;&#20110;&#22522;&#20110;&#30693;&#35782;&#33976;&#39311;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.00393</link><description>&lt;p&gt;
Teacher Agent&#65306;&#19968;&#31181;&#22522;&#20110;&#37325;&#22797;&#35757;&#32451;&#30340;&#35270;&#39057;&#22686;&#37327;&#23398;&#20064;&#38750;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Teacher Agent: A Non-Knowledge Distillation Method for Rehearsal-based Video Incremental Learning. (arXiv:2306.00393v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00393
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#25945;&#24072;&#20195;&#29702;&#26041;&#27861;&#65292;&#33021;&#22815;&#20174;&#20808;&#21069;&#23398;&#20064;&#30340;&#30693;&#35782;&#20013;&#29983;&#25104;&#39640;&#36136;&#37327;&#26679;&#26412;&#30340;&#25968;&#25454;&#38598;&#65292;&#20174;&#32780;&#20351;&#23398;&#29983;&#32593;&#32476;&#21487;&#20197;&#20174;&#26679;&#26412;&#30340;&#22810;&#31181;&#35266;&#28857;&#20013;&#36827;&#34892;&#23398;&#20064;&#65292;&#22312;&#26631;&#20934;&#22522;&#20934;&#19978;&#20248;&#20110;&#22522;&#20110;&#30693;&#35782;&#33976;&#39311;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22522;&#20110;&#35270;&#39057;&#30340;&#31038;&#20132;&#23186;&#20307;&#30340;&#26222;&#21450;&#65292;&#19981;&#26029;&#26377;&#26032;&#30340;&#35270;&#39057;&#31867;&#21035;&#34987;&#29983;&#25104;&#65292;&#36843;&#20999;&#38656;&#35201;&#31283;&#20581;&#30340;&#22686;&#37327;&#23398;&#20064;&#25216;&#26415;&#26469;&#29702;&#35299;&#36825;&#20123;&#35270;&#39057;&#12290;&#20854;&#20013;&#26368;&#22823;&#30340;&#25361;&#25112;&#20043;&#19968;&#26159;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#22312;&#23398;&#20064;&#26032;&#31867;&#21035;&#30340;&#21516;&#26102;&#65292;&#32593;&#32476;&#24448;&#24448;&#20250;&#24536;&#35760;&#20808;&#21069;&#23398;&#20064;&#36807;&#30340;&#25968;&#25454;&#12290;&#20026;&#20102;&#35299;&#20915;&#27492;&#38382;&#39064;&#65292;&#30693;&#35782;&#33976;&#39311;&#26159;&#19968;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#25216;&#26415;&#65292;&#23427;&#36890;&#36807;&#23558;&#19981;&#21516;&#31867;&#21035;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#30340;&#37325;&#35201;&#20449;&#24687;&#20256;&#36755;&#21040;&#23398;&#29983;&#27169;&#22411;&#20013;&#26469;&#22686;&#24378;&#20854;&#24615;&#33021;&#12290;&#22240;&#27492;&#65292;&#26368;&#22909;&#26377;&#19968;&#20010;&#24378;&#22823;&#30340;&#25945;&#24072;&#27169;&#22411;&#26469;&#25351;&#23548;&#23398;&#29983;&#12290;&#28982;&#32780;&#65292;&#32593;&#32476;&#26412;&#36523;&#30340;&#26377;&#38480;&#34920;&#29616;&#21644;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#21457;&#29983;&#21487;&#33021;&#23548;&#33268;&#25945;&#24072;&#32593;&#32476;&#23545;&#26576;&#20123;&#35760;&#24518;&#26679;&#26412;&#20570;&#20986;&#19981;&#20934;&#30830;&#30340;&#39044;&#27979;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#23398;&#29983;&#32593;&#32476;&#30340;&#24615;&#33021;&#12290;&#22522;&#20110;&#36825;&#20123;&#35266;&#23519;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25945;&#24072;&#20195;&#29702;&#65292;&#33021;&#22815;&#20174;&#20808;&#21069;&#23398;&#20064;&#30340;&#30693;&#35782;&#20013;&#29983;&#25104;&#39640;&#36136;&#37327;&#26679;&#26412;&#30340;&#25968;&#25454;&#38598;&#65292;&#20174;&#32780;&#20351;&#23398;&#29983;&#32593;&#32476;&#21487;&#20197;&#20174;&#26679;&#26412;&#30340;&#22810;&#31181;&#35266;&#28857;&#20013;&#36827;&#34892;&#23398;&#20064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#35270;&#39057;&#20998;&#31867;&#20219;&#21153;&#30340;&#26631;&#20934;&#22522;&#20934;&#19978;&#20248;&#20110;&#22522;&#20110;&#30693;&#35782;&#33976;&#39311;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the rise in popularity of video-based social media, new categories of videos are constantly being generated, creating an urgent need for robust incremental learning techniques for video understanding. One of the biggest challenges in this task is catastrophic forgetting, where the network tends to forget previously learned data while learning new categories. To overcome this issue, knowledge distillation is a widely used technique for rehearsal-based video incremental learning that involves transferring important information on similarities among different categories to enhance the student model. Therefore, it is preferable to have a strong teacher model to guide the students. However, the limited performance of the network itself and the occurrence of catastrophic forgetting can result in the teacher network making inaccurate predictions for some memory exemplars, ultimately limiting the student network's performance. Based on these observations, we propose a teacher agent capabl
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;MERGE&#65292;&#19968;&#20010;&#22522;&#20110;Transformer&#35821;&#35328;&#27169;&#22411;&#30340;&#24555;&#36895;&#31169;&#26377;&#25991;&#26412;&#29983;&#25104;&#26694;&#26550;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;MERGE&#22312;&#20445;&#25252;&#38544;&#31169;&#30340;&#21516;&#26102;&#65292;&#23454;&#29616;&#20102;26.5&#20493;&#30340;&#21152;&#36895;&#21644;80%&#30340;&#36890;&#20449;&#23383;&#33410;&#25968;&#20943;&#23569;&#12290;</title><link>http://arxiv.org/abs/2305.15769</link><description>&lt;p&gt;
MERGE: &#24555;&#36895;&#30340;&#31169;&#26377;&#25991;&#26412;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
MERGE: Fast Private Text Generation. (arXiv:2305.15769v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15769
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;MERGE&#65292;&#19968;&#20010;&#22522;&#20110;Transformer&#35821;&#35328;&#27169;&#22411;&#30340;&#24555;&#36895;&#31169;&#26377;&#25991;&#26412;&#29983;&#25104;&#26694;&#26550;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;MERGE&#22312;&#20445;&#25252;&#38544;&#31169;&#30340;&#21516;&#26102;&#65292;&#23454;&#29616;&#20102;26.5&#20493;&#30340;&#21152;&#36895;&#21644;80%&#30340;&#36890;&#20449;&#23383;&#33410;&#25968;&#20943;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20154;&#20204;&#36234;&#26469;&#36234;&#20851;&#27880;NLP&#26381;&#21153;&#21644;Transformer&#27169;&#22411;&#30340;&#31169;&#26377;&#25512;&#29702;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#20004;&#26041;&#38544;&#31169;&#20445;&#25252;&#26041;&#27861;&#20165;&#32771;&#34385;NLU&#22330;&#26223;&#65292;&#32780;&#25991;&#26412;&#29983;&#25104;&#30340;&#31169;&#26377;&#25512;&#29702;&#65292;&#22914;&#32763;&#35793;&#12289;&#23545;&#35805;&#21644;&#20195;&#30721;&#34917;&#20840;&#65292;&#20173;&#26410;&#35299;&#20915;&#12290;&#27492;&#22806;&#65292;&#23558;&#29616;&#26377;&#30340;&#38544;&#31169;&#20445;&#25252;&#26041;&#27861;&#36801;&#31227;&#21040;NLG&#27169;&#22411;&#26102;&#65292;&#24615;&#33021;&#34920;&#29616;&#24046;&#65292;&#32780;&#22312;&#35757;&#32451;&#38454;&#27573;&#21463;&#21040;&#25910;&#25947;&#38382;&#39064;&#30340;&#22256;&#25200;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MERGE&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;Transformer&#35821;&#35328;&#27169;&#22411;&#30340;&#24555;&#36895;&#31169;&#26377;&#25991;&#26412;&#29983;&#25104;&#26694;&#26550;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;MERGE&#37325;&#29992;&#36755;&#20986;&#38544;&#34255;&#29366;&#24577;&#20316;&#20026;&#21333;&#35789;&#23884;&#20837;&#65292;&#20197;&#36339;&#36807;&#23884;&#20837;&#35745;&#31639;&#65292;&#24182;&#37325;&#26032;&#32452;&#32455;Transformer&#27169;&#22359;&#20013;&#30340;&#32447;&#24615;&#25805;&#20316;&#20197;&#21152;&#36895;&#21521;&#21069;&#36807;&#31243;&#12290;&#22522;&#20110;&#36825;&#20004;&#20010;&#20248;&#21270;&#65292;&#22823;&#37327;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#24207;&#21015;&#38271;&#24230;&#20026;512&#26102;&#65292;MERGE&#21487;&#23454;&#29616;26.5&#20493;&#30340;&#21152;&#36895;&#65292;&#24182;&#20943;&#23569;80\%&#30340;&#36890;&#20449;&#23383;&#33410;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent years have seen increasing concerns about the private inference of NLP services and Transformer models. However, existing two-party privacy-preserving methods solely consider NLU scenarios, while the private inference of text generation such as translation, dialogue, and code completion remains unsolved. Besides, while migrated to NLG models, existing privacy-preserving methods perform poorly in terms of inference speed, and suffer from the convergence problem during the training stage. To address these issues, we propose MERGE, a fast private text generation framework for Transformer-based language models. Specifically, MERGE reuse the output hidden state as the word embedding to bypass the embedding computation, and reorganize the linear operations in the Transformer module to accelerate the forward procedure. Based on these two optimizations, extensive experiments show that MERGE can achieve a 26.5x speedup under the sequence length 512, and reduce 80\% communication bytes, w
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#38598;&#25104;&#22810;&#27169;&#24577;&#24863;&#30693;&#65288;IMP&#65289;&#26041;&#27861;&#65292;&#23558;&#22810;&#27169;&#24577;&#36755;&#20837;&#38598;&#25104;&#21040;&#21333;&#20010;&#32534;&#30721;&#22120;&#20013;&#65292;&#37319;&#29992;&#20132;&#26367;&#26799;&#24230;&#19979;&#38477;&#27861;&#65288;AGD&#65289;&#21644;&#28151;&#21512;&#19987;&#23478;&#65288;MoE&#65289;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#23454;&#29616;&#39640;&#25928;&#30340;&#27169;&#22411;&#21644;&#20219;&#21153;&#25193;&#23637;&#65292;&#21462;&#24471;&#20102;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.06324</link><description>&lt;p&gt;
AGD&#21644;MoE&#29992;&#20110;&#38598;&#25104;&#22810;&#27169;&#24577;&#24863;&#30693;
&lt;/p&gt;
&lt;p&gt;
Alternating Gradient Descent and Mixture-of-Experts for Integrated Multimodal Perception. (arXiv:2305.06324v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06324
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#38598;&#25104;&#22810;&#27169;&#24577;&#24863;&#30693;&#65288;IMP&#65289;&#26041;&#27861;&#65292;&#23558;&#22810;&#27169;&#24577;&#36755;&#20837;&#38598;&#25104;&#21040;&#21333;&#20010;&#32534;&#30721;&#22120;&#20013;&#65292;&#37319;&#29992;&#20132;&#26367;&#26799;&#24230;&#19979;&#38477;&#27861;&#65288;AGD&#65289;&#21644;&#28151;&#21512;&#19987;&#23478;&#65288;MoE&#65289;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#23454;&#29616;&#39640;&#25928;&#30340;&#27169;&#22411;&#21644;&#20219;&#21153;&#25193;&#23637;&#65292;&#21462;&#24471;&#20102;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#21487;&#25193;&#23637;&#30340;&#22810;&#27169;&#24577;&#22810;&#20219;&#21153;&#35757;&#32451;&#21644;&#24314;&#27169;&#26041;&#27861;&#8212;&#8212;&#38598;&#25104;&#22810;&#27169;&#24577;&#24863;&#30693;&#65288;IMP&#65289;&#12290;IMP&#23558;&#22270;&#20687;&#12289;&#35270;&#39057;&#12289;&#25991;&#26412;&#21644;&#38899;&#39057;&#31561;&#22810;&#27169;&#24577;&#36755;&#20837;&#38598;&#25104;&#21040;&#21333;&#20010;Transformer&#32534;&#30721;&#22120;&#20013;&#65292;&#24182;&#20855;&#26377;&#26368;&#23567;&#30340;&#27169;&#24577;&#29305;&#23450;&#32452;&#20214;&#12290;IMP&#20351;&#29992;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35774;&#35745;&#65292;&#23558;&#20132;&#26367;&#26799;&#24230;&#19979;&#38477;&#27861;&#65288;AGD&#65289;&#21644;&#28151;&#21512;&#19987;&#23478;&#65288;MoE&#65289;&#30456;&#32467;&#21512;&#65292;&#20197;&#23454;&#29616;&#39640;&#25928;&#30340;&#27169;&#22411;&#21644;&#20219;&#21153;&#25193;&#23637;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#35777;&#30740;&#31350;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#20197;&#19979;&#20851;&#38190;&#35265;&#35299;&#65306;1&#65289;&#22312;&#22810;&#26679;&#21270;&#30340;&#24322;&#26500;&#27169;&#24577;&#12289;&#25439;&#22833;&#20989;&#25968;&#21644;&#20219;&#21153;&#19978;&#20132;&#26367;&#25191;&#34892;&#26799;&#24230;&#19979;&#38477;&#26356;&#26032;&#65292;&#24182;&#21516;&#26102;&#25913;&#21464;&#36755;&#20837;&#20998;&#36776;&#29575;&#65292;&#21487;&#20197;&#26377;&#25928;&#25552;&#39640;&#22810;&#27169;&#24577;&#29702;&#35299;&#12290;2&#65289;&#22312;&#21333;&#19968;&#30340;&#27169;&#24577;&#19981;&#21487;&#30693;&#32534;&#30721;&#22120;&#19978;&#20351;&#29992;MoE&#36827;&#34892;&#27169;&#22411;&#31232;&#30095;&#21270;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#65292;&#32988;&#36807;&#20351;&#29992;&#27169;&#24577;&#29305;&#23450;&#32534;&#30721;&#22120;&#25110;&#39069;&#22806;&#34701;&#21512;&#23618;&#30340;&#31264;&#23494;&#27169;&#22411;&#65292;&#24182;&#22823;&#22823;&#32531;&#35299;&#27169;&#24577;&#20043;&#38388;&#30340;&#20914;&#31361;&#12290;IMP&#22312;&#19977;&#20010;&#22810;&#27169;&#24577;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#65292;&#32988;&#36807;&#20102;&#22823;&#37096;&#20998;&#24050;&#21457;&#34920;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Integrated Multimodal Perception (IMP), a simple and scalable multimodal multi-task training and modeling approach. IMP integrates multimodal inputs including image, video, text, and audio into a single Transformer encoder with minimal modality-specific components. IMP makes use of a novel design that combines Alternating Gradient Descent (AGD) and Mixture-of-Experts (MoE) for efficient model \&amp; task scaling. We conduct extensive empirical studies about IMP and reveal the following key insights: 1) performing gradient descent updates by alternating on diverse heterogeneous modalities, loss functions, and tasks, while also varying input resolutions, efficiently improves multimodal understanding. 2) model sparsification with MoE on a single modality-agnostic encoder substantially improves the performance, outperforming dense models that use modality-specific encoders or additional fusion layers and greatly mitigating the conflicts between modalities. IMP achieves competitive p
&lt;/p&gt;</description></item><item><title>DaGAN++&#26159;&#19968;&#31181;&#28145;&#24230;&#24863;&#30693;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65292;&#33021;&#22815;&#20174;&#38754;&#37096;&#35270;&#39057;&#20013;&#33258;&#23398;&#20064;&#23494;&#38598;&#30340;3D&#38754;&#37096;&#20960;&#20309;&#65292;&#23558;&#23427;&#20204;&#34701;&#20837;&#21040;&#29983;&#25104;&#22120;&#20013;&#65292;&#20174;&#32780;&#23454;&#29616;&#38754;&#21521;&#35821;&#38899;&#22836;&#35270;&#39057;&#29983;&#25104;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.06225</link><description>&lt;p&gt;
DaGAN++&#65306;&#38754;&#21521;&#35821;&#38899;&#22836;&#35270;&#39057;&#29983;&#25104;&#30340;&#28145;&#24230;&#24863;&#30693;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
DaGAN++: Depth-Aware Generative Adversarial Network for Talking Head Video Generation. (arXiv:2305.06225v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06225
&lt;/p&gt;
&lt;p&gt;
DaGAN++&#26159;&#19968;&#31181;&#28145;&#24230;&#24863;&#30693;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65292;&#33021;&#22815;&#20174;&#38754;&#37096;&#35270;&#39057;&#20013;&#33258;&#23398;&#20064;&#23494;&#38598;&#30340;3D&#38754;&#37096;&#20960;&#20309;&#65292;&#23558;&#23427;&#20204;&#34701;&#20837;&#21040;&#29983;&#25104;&#22120;&#20013;&#65292;&#20174;&#32780;&#23454;&#29616;&#38754;&#21521;&#35821;&#38899;&#22836;&#35270;&#39057;&#29983;&#25104;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20197;&#24448;&#30340;&#35821;&#38899;&#22836;&#35270;&#39057;&#29983;&#25104;&#25216;&#26415;&#20027;&#35201;&#20381;&#36182;&#20110;2D&#20449;&#24687;&#65292;&#21253;&#25324;&#38754;&#37096;&#22806;&#35980;&#21644;&#21160;&#20316;&#12290;&#28982;&#32780;&#65292;&#20687;&#32032;&#32423;&#30340;&#28145;&#24230;&#31561;&#23494;&#38598;&#30340;3D&#38754;&#37096;&#20960;&#20309;&#25968;&#25454;&#22312;&#26500;&#24314;&#20934;&#30830;&#30340;3D&#38754;&#37096;&#32467;&#26500;&#21644;&#25233;&#21046;&#32972;&#26223;&#22122;&#22768;&#26041;&#38754;&#21457;&#25381;&#20102;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#30340;&#27169;&#22359;&#65292;&#33021;&#22815;&#20174;&#38754;&#37096;&#35270;&#39057;&#20013;&#23398;&#20064;&#23494;&#38598;&#30340;3D&#38754;&#37096;&#20960;&#20309;&#25968;&#25454;&#65288;&#21363;&#28145;&#24230;&#65289;&#65292;&#32780;&#19981;&#38656;&#35201;&#35757;&#32451;&#26102;&#30340;&#25668;&#20687;&#26426;&#21442;&#25968;&#21644;&#20960;&#20309;&#27880;&#37322;&#12290;&#35770;&#25991;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#31574;&#30053;&#65292;&#23398;&#20064;&#20687;&#32032;&#32423;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#20197;&#20415;&#26356;&#21487;&#38752;&#22320;&#24863;&#30693;&#21018;&#24615;&#36816;&#21160;&#20687;&#32032;&#12290;&#27492;&#22806;&#65292;&#36824;&#35774;&#35745;&#20102;&#19968;&#20010;&#26377;&#25928;&#30340;&#38754;&#37096;&#20851;&#38190;&#28857;&#20272;&#35745;&#27169;&#22359;&#65292;&#20026;&#29983;&#25104;&#36816;&#21160;&#22330;&#25552;&#20379;&#20934;&#30830;&#30340;&#20851;&#38190;&#28857;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;3D&#24863;&#30693;&#30340;&#36328;&#27169;&#24577;&#65288;&#21363;&#22806;&#35980;&#21644;&#28145;&#24230;&#65289;&#27880;&#24847;&#26426;&#21046;&#65292;&#24182;&#23558;&#20854;&#34701;&#20837;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#26694;&#26550;&#20013;&#65292;&#23454;&#29616;&#38754;&#21521;&#35821;&#38899;&#22836;&#35270;&#39057;&#29983;&#25104;&#12290;&#25152;&#25552;&#20986;&#30340;DaGAN ++&#27169;&#22411;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#35270;&#35273;&#36136;&#37327;&#65292;&#31283;&#23450;&#24615;&#21644;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Predominant techniques on talking head generation largely depend on 2D information, including facial appearances and motions from input face images. Nevertheless, dense 3D facial geometry, such as pixel-wise depth, plays a critical role in constructing accurate 3D facial structures and suppressing complex background noises for generation. However, dense 3D annotations for facial videos is prohibitively costly to obtain. In this work, firstly, we present a novel self-supervised method for learning dense 3D facial geometry (ie, depth) from face videos, without requiring camera parameters and 3D geometry annotations in training. We further propose a strategy to learn pixel-level uncertainties to perceive more reliable rigid-motion pixels for geometry learning. Secondly, we design an effective geometry-guided facial keypoint estimation module, providing accurate keypoints for generating motion fields. Lastly, we develop a 3D-aware cross-modal (ie, appearance and depth) attention mechanism,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39063;&#31890;&#29699;&#35745;&#31639;&#30340;&#33258;&#36866;&#24212;&#22810;&#31890;&#24230;&#34920;&#31034;&#21644;&#35745;&#31639;&#26041;&#27861;&#65292;&#33021;&#22815;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#30340;&#25928;&#29575;&#12289;&#40065;&#26834;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.11171</link><description>&lt;p&gt;
&#39063;&#31890;&#29699;&#35745;&#31639;&#65306;&#19968;&#31181;&#39640;&#25928;&#12289;&#40065;&#26834;&#21644;&#21487;&#35299;&#37322;&#30340;&#33258;&#36866;&#24212;&#22810;&#31890;&#24230;&#34920;&#31034;&#21644;&#35745;&#31639;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Granular ball computing: an efficient, robust, and interpretable adaptive multi-granularity representation and computation method. (arXiv:2304.11171v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11171
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39063;&#31890;&#29699;&#35745;&#31639;&#30340;&#33258;&#36866;&#24212;&#22810;&#31890;&#24230;&#34920;&#31034;&#21644;&#35745;&#31639;&#26041;&#27861;&#65292;&#33021;&#22815;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#30340;&#25928;&#29575;&#12289;&#40065;&#26834;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#35748;&#30693;&#20855;&#26377;&#8220;&#20808;&#22823;&#21518;&#23567;&#8221;&#30340;&#35748;&#30693;&#26426;&#21046;&#65292;&#22240;&#27492;&#20855;&#26377;&#33258;&#36866;&#24212;&#30340;&#22810;&#31890;&#24230;&#25551;&#36848;&#33021;&#21147;&#12290;&#36825;&#23548;&#33268;&#20102;&#26377;&#25928;&#24615;&#12289;&#40065;&#26834;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#31561;&#35745;&#31639;&#29305;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#39063;&#31890;&#29699;&#35745;&#31639;&#30340;&#33258;&#36866;&#24212;&#22810;&#31890;&#24230;&#34920;&#31034;&#21644;&#35745;&#31639;&#26041;&#27861;&#12290;&#20182;&#20204;&#23558;&#36825;&#31181;&#26041;&#27861;&#24212;&#29992;&#20110;&#20960;&#20010;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#65292;&#24182;&#35777;&#26126;&#20854;&#30456;&#23545;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human cognition has a ``large-scale first'' cognitive mechanism, therefore possesses adaptive multi-granularity description capabilities. This results in computational characteristics such as efficiency, robustness, and interpretability. Although most existing artificial intelligence learning methods have certain multi-granularity features, they do not fully align with the ``large-scale first'' cognitive mechanism. Multi-granularity granular-ball computing is an important model method developed in recent years. This method can use granular-balls of different sizes to adaptively represent and cover the sample space, and perform learning based on granular-balls. Since the number of coarse-grained "granular-ball" is smaller than the number of sample points, granular-ball computing is more efficient; the coarse-grained characteristics of granular-balls are less likely to be affected by fine-grained sample points, making them more robust; the multi-granularity structure of granular-balls ca
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#26102;&#31354;&#24207;&#36143;&#20915;&#31574;&#32467;&#26500;&#65292;&#23454;&#29616;&#20102;&#26031;&#22612;&#20811;&#20271;&#26684;&#22343;&#34913;&#31574;&#30053;&#30340;&#24322;&#27493;&#34892;&#21160;&#21327;&#35843;&#65292;&#24182;&#22312;&#21442;&#25968;&#20849;&#20139;&#30340;&#21516;&#26102;&#23454;&#29616;&#20102;&#19981;&#21516;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#19981;&#23545;&#31216;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2304.10351</link><description>&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#26102;&#31354;&#24207;&#36143;&#20915;&#31574;&#21046;&#23548;&#26031;&#22612;&#20811;&#20271;&#26684;&#22343;&#34913;&#30340;&#20135;&#29983;
&lt;/p&gt;
&lt;p&gt;
Inducing Stackelberg Equilibrium through Spatio-Temporal Sequential Decision-Making in Multi-Agent Reinforcement Learning. (arXiv:2304.10351v1 [cs.MA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10351
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#26102;&#31354;&#24207;&#36143;&#20915;&#31574;&#32467;&#26500;&#65292;&#23454;&#29616;&#20102;&#26031;&#22612;&#20811;&#20271;&#26684;&#22343;&#34913;&#31574;&#30053;&#30340;&#24322;&#27493;&#34892;&#21160;&#21327;&#35843;&#65292;&#24182;&#22312;&#21442;&#25968;&#20849;&#20139;&#30340;&#21516;&#26102;&#23454;&#29616;&#20102;&#19981;&#21516;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#19981;&#23545;&#31216;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;(MARL)&#20013;&#65292;&#33258;&#21033;&#30340;&#26234;&#33021;&#20307;&#35797;&#22270;&#24314;&#31435;&#22343;&#34913;&#24182;&#26681;&#25454;&#28216;&#25103;&#32467;&#26500;&#23454;&#29616;&#21327;&#35843;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;MARL&#26041;&#27861;&#22823;&#22810;&#21463;&#21046;&#20110;&#25152;&#26377;&#26234;&#33021;&#20307;&#22312;&#39532;&#21487;&#22827;&#21338;&#24328;(MG)&#26694;&#26550;&#20013;&#30340;&#21516;&#26102;&#34892;&#21160;&#65292;&#24456;&#23569;&#26377;&#20316;&#21697;&#32771;&#34385;&#36890;&#36807;&#24322;&#27493;&#34892;&#21160;&#21327;&#35843;&#24418;&#25104;&#22343;&#34913;&#31574;&#30053;&#12290;&#37492;&#20110;&#26031;&#22612;&#20811;&#20271;&#26684;&#22343;&#34913;(SE)&#30456;&#23545;&#32435;&#20160;&#22343;&#34913;&#30340;&#20248;&#21183;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#20174;MG&#23548;&#20986;&#30340;&#26102;&#31354;&#24207;&#36143;&#20915;&#31574;&#32467;&#26500;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#25152;&#26377;&#26234;&#33021;&#20307;&#20849;&#20139;&#30340;&#26465;&#20214;&#36229;&#32593;&#32476;&#30340;N&#32423;&#31574;&#30053;&#27169;&#22411;&#12290;&#36825;&#31181;&#26041;&#27861;&#20801;&#35768;&#19981;&#23545;&#31216;&#35757;&#32451;&#21644;&#23545;&#31216;&#25191;&#34892;&#65292;&#27599;&#20010;&#26234;&#33021;&#20307;&#21709;&#24212;&#20110;&#19978;&#32423;&#26234;&#33021;&#20307;&#20915;&#31574;&#30340;&#26368;&#20248;&#21453;&#24212;&#12290;&#26234;&#33021;&#20307;&#21487;&#20197;&#23398;&#20064;&#19981;&#21516;&#30340;SE&#31574;&#30053;&#65292;&#21516;&#26102;&#20173;&#28982;&#20445;&#25345;&#21442;&#25968;&#20849;&#20139;&#65292;&#36825;&#23548;&#33268;&#20102;&#23398;&#20064;&#21644;&#23384;&#20648;&#25104;&#26412;&#30340;&#38477;&#20302;&#20197;&#21450;&#25193;&#23637;&#24615;&#30340;&#25552;&#39640;&#65292;&#22240;&#20026;&#26234;&#33021;&#20307;&#25968;&#37327;&#22686;&#21152;&#26102;&#65292;&#23384;&#20648;&#21644;&#35745;&#31639;&#25104;&#26412;&#19981;&#20250;&#26174;&#33879;&#22686;&#21152;&#12290;
&lt;/p&gt;
&lt;p&gt;
In multi-agent reinforcement learning (MARL), self-interested agents attempt to establish equilibrium and achieve coordination depending on game structure. However, existing MARL approaches are mostly bound by the simultaneous actions of all agents in the Markov game (MG) framework, and few works consider the formation of equilibrium strategies via asynchronous action coordination. In view of the advantages of Stackelberg equilibrium (SE) over Nash equilibrium, we construct a spatio-temporal sequential decision-making structure derived from the MG and propose an N-level policy model based on a conditional hypernetwork shared by all agents. This approach allows for asymmetric training with symmetric execution, with each agent responding optimally conditioned on the decisions made by superior agents. Agents can learn heterogeneous SE policies while still maintaining parameter sharing, which leads to reduced cost for learning and storage and enhanced scalability as the number of agents in
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PFELS&#30340;&#26080;&#32447;FL&#26041;&#26696;&#65292;&#36890;&#36807;&#20808;&#21387;&#32553;&#27169;&#22411;&#26356;&#26032;&#20877;&#33258;&#36866;&#24212;&#22320;&#35774;&#35745;&#21457;&#36865;&#21151;&#29575;&#26469;&#25552;&#20379;&#23458;&#25143;&#31471;&#32423;&#21035;DP&#20445;&#35777;&#65292;&#24182;&#38477;&#20302;&#36890;&#20449;&#21644;&#33021;&#37327;&#24320;&#38144;&#24182;&#25552;&#39640;&#27169;&#22411;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2304.07460</link><description>&lt;p&gt;
&#20855;&#26377;&#20869;&#22312;&#38544;&#31169;&#20445;&#25252;&#30340;&#39640;&#25928;&#36890;&#20449;&#21644;&#33410;&#33021;&#26080;&#32447;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Communication and Energy Efficient Wireless Federated Learning with Intrinsic Privacy. (arXiv:2304.07460v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07460
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PFELS&#30340;&#26080;&#32447;FL&#26041;&#26696;&#65292;&#36890;&#36807;&#20808;&#21387;&#32553;&#27169;&#22411;&#26356;&#26032;&#20877;&#33258;&#36866;&#24212;&#22320;&#35774;&#35745;&#21457;&#36865;&#21151;&#29575;&#26469;&#25552;&#20379;&#23458;&#25143;&#31471;&#32423;&#21035;DP&#20445;&#35777;&#65292;&#24182;&#38477;&#20302;&#36890;&#20449;&#21644;&#33021;&#37327;&#24320;&#38144;&#24182;&#25552;&#39640;&#27169;&#22411;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26159;&#19968;&#31181;&#21327;&#21516;&#23398;&#20064;&#26694;&#26550;&#65292;&#20351;&#36793;&#32536;&#35774;&#22791;&#22312;&#20445;&#30041;&#21407;&#22987;&#25968;&#25454;&#30340;&#21516;&#26102;&#21327;&#21516;&#23398;&#20064;&#20840;&#23616;&#27169;&#22411;&#12290;&#34429;&#28982;FL&#36991;&#20813;&#20102;&#20174;&#26412;&#22320;&#25968;&#25454;&#38598;&#27844;&#28431;&#30452;&#25509;&#20449;&#24687;&#65292;&#20294;&#20173;&#21487;&#33021;&#20174;&#20849;&#20139;&#27169;&#22411;&#25512;&#26029;&#20986;&#25935;&#24863;&#20449;&#24687;&#12290;&#20026;&#35299;&#20915;FL&#20013;&#30340;&#38544;&#31169;&#38382;&#39064;&#65292;&#21033;&#29992;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#26426;&#21046;&#25552;&#20379;&#27491;&#24335;&#30340;&#38544;&#31169;&#20445;&#35777;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#26080;&#32447;&#36793;&#32536;&#37096;&#32626;FL&#26102;&#65292;&#30830;&#20445;&#23458;&#25143;&#31471;&#32423;&#21035;&#30340;DP&#38754;&#20020;&#30528;&#37325;&#22823;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#24102;&#31232;&#30095;&#21270;&#30340;&#31169;&#26377;&#32852;&#37030;&#36793;&#32536;&#23398;&#20064;&#65288;PFELS&#65289;&#30340;&#26032;&#22411;&#26080;&#32447;FL&#26041;&#26696;&#65292;&#20197;&#25552;&#20379;&#20855;&#26377;&#20869;&#22312;&#20449;&#36947;&#22122;&#22768;&#30340;&#23458;&#25143;&#31471;&#32423;&#21035;DP&#20445;&#35777;&#65292;&#21516;&#26102;&#38477;&#20302;&#36890;&#20449;&#21644;&#33021;&#37327;&#24320;&#38144;&#24182;&#25552;&#39640;&#27169;&#22411;&#31934;&#24230;&#12290;PFELS&#30340;&#20851;&#38190;&#24605;&#24819;&#26159;&#20351;&#27599;&#20010;&#35774;&#22791;&#20808;&#21387;&#32553;&#20854;&#27169;&#22411;&#26356;&#26032;&#65292;&#28982;&#21518;&#26681;&#25454;&#26080;&#32447;&#20449;&#36947;&#33258;&#36866;&#24212;&#35774;&#35745;&#21387;&#32553;&#27169;&#22411;&#26356;&#26032;&#30340;&#21457;&#36865;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) is a collaborative learning framework that enables edge devices to collaboratively learn a global model while keeping raw data locally. Although FL avoids leaking direct information from local datasets, sensitive information can still be inferred from the shared models. To address the privacy issue in FL, differential privacy (DP) mechanisms are leveraged to provide formal privacy guarantee. However, when deploying FL at the wireless edge with over-the-air computation, ensuring client-level DP faces significant challenges. In this paper, we propose a novel wireless FL scheme called private federated edge learning with sparsification (PFELS) to provide client-level DP guarantee with intrinsic channel noise while reducing communication and energy overhead and improving model accuracy. The key idea of PFELS is for each device to first compress its model update and then adaptively design the transmit power of the compressed model update according to the wireless cha
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Grid-SD2E&#30340;&#35748;&#30693;&#23398;&#20064;&#31995;&#32479;&#65292;&#20854;&#24314;&#31435;&#22312;&#32593;&#26684;&#32454;&#32990;&#30340;&#22522;&#30784;&#19978;&#65292;&#20351;&#29992;&#31354;&#38388;&#21010;&#20998;&#21644;&#25506;&#32034;&#21033;&#29992;&#26041;&#27861;&#23454;&#29616;&#20132;&#20114;&#21644;&#33258;&#25105;&#24378;&#21270;&#65292;&#26377;&#21161;&#20110;&#29702;&#35299;&#22823;&#33041;&#30340;&#24037;&#20316;&#26426;&#21046;&#12289;&#27835;&#30103;&#33041;&#37096;&#30142;&#30149;&#21644;&#29702;&#35299;&#26234;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.01844</link><description>&lt;p&gt;
Grid-SD2E&#65306;&#19968;&#31181;&#35748;&#30693;&#23398;&#20064;&#31995;&#32479;&#20013;&#30340;&#36890;&#29992;&#32593;&#26684;&#21453;&#39304;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Grid-SD2E: A General Grid-Feedback in a System for Cognitive Learning. (arXiv:2304.01844v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01844
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Grid-SD2E&#30340;&#35748;&#30693;&#23398;&#20064;&#31995;&#32479;&#65292;&#20854;&#24314;&#31435;&#22312;&#32593;&#26684;&#32454;&#32990;&#30340;&#22522;&#30784;&#19978;&#65292;&#20351;&#29992;&#31354;&#38388;&#21010;&#20998;&#21644;&#25506;&#32034;&#21033;&#29992;&#26041;&#27861;&#23454;&#29616;&#20132;&#20114;&#21644;&#33258;&#25105;&#24378;&#21270;&#65292;&#26377;&#21161;&#20110;&#29702;&#35299;&#22823;&#33041;&#30340;&#24037;&#20316;&#26426;&#21046;&#12289;&#27835;&#30103;&#33041;&#37096;&#30142;&#30149;&#21644;&#29702;&#35299;&#26234;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#22823;&#33041;&#22914;&#20309;&#36890;&#36807;&#20135;&#29983;&#31070;&#32463;&#20449;&#21495;&#19982;&#22806;&#37096;&#19990;&#30028;&#30456;&#20114;&#20316;&#29992;&#23545;&#20110;&#30830;&#23450;&#20854;&#24037;&#20316;&#26426;&#21046;&#12289;&#27835;&#30103;&#33041;&#37096;&#30142;&#30149;&#21644;&#29702;&#35299;&#26234;&#33021;&#38750;&#24120;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#29702;&#35770;&#27169;&#22411;&#65292;&#20294;&#23427;&#20204;&#36804;&#20170;&#38590;&#20197;&#25972;&#21512;&#21644;&#21457;&#23637;&#12290;&#21463;&#32593;&#26684;&#32454;&#32990;&#21551;&#21457;&#65292;&#26412;&#30740;&#31350;&#21019;&#24314;&#20102;&#19968;&#20010;&#26356;&#36890;&#29992;&#21644;&#24378;&#22823;&#30340;&#32593;&#26684;&#27169;&#22359;&#65292;&#24182;&#19982;&#36125;&#21494;&#26031;&#25512;&#29702;&#19968;&#36215;&#26500;&#24314;&#20102;&#19968;&#20010;&#20114;&#21160;&#21644;&#33258;&#25105;&#24378;&#21270;&#30340;&#35748;&#30693;&#31995;&#32479;&#12290;&#36825;&#31181;&#26041;&#27861;&#31216;&#20026;&#24102;&#26377;&#32593;&#26684;&#21453;&#39304;&#30340;&#31354;&#38388;&#21010;&#20998;&#21644;&#25506;&#32034;&#21033;&#29992;&#65288;Grid-SD2E&#65289;&#12290;&#22312;&#36825;&#37324;&#65292;&#32593;&#26684;&#27169;&#22359;&#21487;&#20197;&#29992;&#20316;&#22806;&#37096;&#19990;&#30028;&#21644;&#31995;&#32479;&#20043;&#38388;&#30340;&#20132;&#20114;&#20171;&#36136;&#65292;&#20063;&#21487;&#20197;&#29992;&#20316;&#31995;&#32479;&#20869;&#30340;&#33258;&#25105;&#24378;&#21270;&#20171;&#36136;&#12290;&#31354;&#38388;&#21010;&#20998;&#21644;&#25506;&#32034;&#21033;&#29992;&#65288;SD2E&#65289;&#36890;&#36807;&#20854;&#31354;&#38388;&#21010;&#20998;&#65288;SD&#65289;&#27169;&#22359;&#25509;&#25910;&#32593;&#26684;&#30340;0/1&#20449;&#21495;&#12290;&#26412;&#25991;&#25551;&#36848;&#30340;&#31995;&#32479;&#20063;&#26159;&#20174;&#36827;&#34892;&#30340;&#23454;&#39564;&#24471;&#20986;&#30340;&#29702;&#35770;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Comprehending how the brain interacts with the external world through generated neural signals is crucial for determining its working mechanism, treating brain diseases, and understanding intelligence. Although many theoretical models have been proposed, they have thus far been difficult to integrate and develop. In this study, we were inspired in part by grid cells in creating a more general and robust grid module and constructing an interactive and self-reinforcing cognitive system together with Bayesian reasoning, an approach called space-division and exploration-exploitation with grid-feedback (Grid-SD2E). Here, a grid module can be used as an interaction medium between the outside world and a system, as well as a self-reinforcement medium within the system. The space-division and exploration-exploitation (SD2E) receives the 0/1 signals of a grid through its space-division (SD) module. The system described in this paper is also a theoretical model derived from experiments conducted
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;Neo4j&#22270;&#21644;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#35299;&#20915;&#22478;&#24066;&#36947;&#36335;&#32593;&#32476;&#20013;&#30340;&#20132;&#36890;&#25317;&#22581;&#38382;&#39064;&#65292;&#33021;&#22815;&#23454;&#29616;&#25317;&#22581;&#36335;&#27573;&#30340;&#36127;&#36733;&#24179;&#34913;&#21644;&#20248;&#21270;&#65292;&#21516;&#26102;&#21487;&#20197;&#39044;&#27979;&#20132;&#36890;&#20107;&#25925;&#23545;&#25972;&#20307;&#20132;&#36890;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2304.00192</link><description>&lt;p&gt;
&#22522;&#20110;Neo4j&#21644;&#28145;&#24230;&#23398;&#20064;&#30340;&#20132;&#36890;&#25317;&#22581;&#27169;&#25311;&#19982;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Leveraging Neo4j and deep learning for traffic congestion simulation &amp; optimization. (arXiv:2304.00192v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00192
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;Neo4j&#22270;&#21644;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#35299;&#20915;&#22478;&#24066;&#36947;&#36335;&#32593;&#32476;&#20013;&#30340;&#20132;&#36890;&#25317;&#22581;&#38382;&#39064;&#65292;&#33021;&#22815;&#23454;&#29616;&#25317;&#22581;&#36335;&#27573;&#30340;&#36127;&#36733;&#24179;&#34913;&#21644;&#20248;&#21270;&#65292;&#21516;&#26102;&#21487;&#20197;&#39044;&#27979;&#20132;&#36890;&#20107;&#25925;&#23545;&#25972;&#20307;&#20132;&#36890;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#36890;&#25317;&#22581;&#19968;&#30452;&#26159;&#22478;&#24066;&#36947;&#36335;&#32593;&#32476;&#20013;&#30340;&#20027;&#35201;&#25361;&#25112;&#12290;&#36807;&#21435;&#36827;&#34892;&#20102;&#22823;&#37327;&#30740;&#31350;&#65292;&#20197;&#20984;&#26174;&#19982;&#20132;&#36890;&#25317;&#22581;&#30456;&#20851;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;&#30446;&#21069;&#65292;&#22823;&#22810;&#25968;&#20132;&#36890;&#25317;&#22581;&#20998;&#26512;&#37117;&#26159;&#20351;&#29992;&#27169;&#25311;&#36719;&#20214;&#36827;&#34892;&#30340;&#65292;&#36825;&#20123;&#36719;&#20214;&#30001;&#20110;&#20351;&#29992;&#30340;&#24037;&#20855;&#21644;&#23454;&#29992;&#31243;&#24207;&#30340;&#38480;&#21046;&#32780;&#25552;&#20379;&#20102;&#26377;&#38480;&#30340;&#27934;&#35265;&#12290;&#25152;&#26377;&#36825;&#20123;&#37117;&#24433;&#21709;&#21040;&#23450;&#21046;&#19994;&#21153;&#38382;&#39064;&#30340;&#21046;&#23450;&#65292;&#36825;&#20123;&#38382;&#39064;&#22240;&#22320;&#21306;&#21644;&#22269;&#23478;&#32780;&#24322;&#12290;&#36890;&#36807;&#21033;&#29992;&#30693;&#35782;&#22270;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#23558;&#20132;&#36890;&#25317;&#22581;&#38382;&#39064;&#24314;&#27169;&#20026;Neo4j&#22270;&#65292;&#28982;&#21518;&#20351;&#29992;&#36127;&#36733;&#24179;&#34913;&#12289;&#20248;&#21270;&#31639;&#27861;&#26469;&#35782;&#21035;&#26080;&#25317;&#22581;&#30340;&#36947;&#36335;&#32593;&#32476;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22312;&#25317;&#22581;&#25110;&#20107;&#25925;&#24773;&#20917;&#19979;&#20132;&#36890;&#22914;&#20309;&#21521;&#21518;&#20256;&#25773;&#20197;&#21450;&#20854;&#23545;&#20854;&#20182;&#36947;&#36335;&#27573;&#30340;&#24635;&#20307;&#24433;&#21709;&#12290;&#25105;&#20204;&#36824;&#22312;&#23454;&#26102;&#20132;&#36890;&#25968;&#25454;&#19978;&#35757;&#32451;&#20102;&#39034;&#24207;RNN-LSTM(&#38271;&#30701;&#26102;&#35760;&#24518;)&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traffic congestion has been a major challenge in many urban road networks. Extensive research studies have been conducted to highlight traffic-related congestion and address the issue using data-driven approaches. Currently, most traffic congestion analyses are done using simulation software that offers limited insight due to the limitations in the tools and utilities being used to render various traffic congestion scenarios. All that impacts the formulation of custom business problems which vary from place to place and country to country. By exploiting the power of the knowledge graph, we model a traffic congestion problem into the Neo4j graph and then use the load balancing, optimization algorithm to identify congestion-free road networks. We also show how traffic propagates backward in case of congestion or accident scenarios and its overall impact on other segments of the roads. We also train a sequential RNN-LSTM (Long Short-Term Memory) deep learning model on the real-time traffi
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#20449;&#24687;&#20998;&#35299;&#26694;&#26550;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#37327;&#21270;&#21644;&#24314;&#27169;&#22810;&#27169;&#24577;&#20132;&#20114;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;PID&#32479;&#35745;&#37327;&#26469;&#24230;&#37327;&#36755;&#20837;&#27169;&#24577;&#19982;&#36755;&#20986;&#20219;&#21153;&#20043;&#38388;&#30340;&#20887;&#20313;&#24230;&#12289;&#29420;&#29305;&#24615;&#21644;&#21327;&#21516;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;&#20004;&#20010;&#26032;&#30340;PID&#32479;&#35745;&#20272;&#35745;&#22120;&#12290;</title><link>http://arxiv.org/abs/2302.12247</link><description>&lt;p&gt;
&#37327;&#21270;&#21644;&#24314;&#27169;&#22810;&#27169;&#24577;&#20132;&#20114;&#65306;&#19968;&#31181;&#20449;&#24687;&#20998;&#35299;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Quantifying &amp; Modeling Multimodal Interactions: An Information Decomposition Framework. (arXiv:2302.12247v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.12247
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#20449;&#24687;&#20998;&#35299;&#26694;&#26550;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#37327;&#21270;&#21644;&#24314;&#27169;&#22810;&#27169;&#24577;&#20132;&#20114;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;PID&#32479;&#35745;&#37327;&#26469;&#24230;&#37327;&#36755;&#20837;&#27169;&#24577;&#19982;&#36755;&#20986;&#20219;&#21153;&#20043;&#38388;&#30340;&#20887;&#20313;&#24230;&#12289;&#29420;&#29305;&#24615;&#21644;&#21327;&#21516;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;&#20004;&#20010;&#26032;&#30340;PID&#32479;&#35745;&#20272;&#35745;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#35299;&#20915;&#22810;&#27169;&#24577;&#20219;&#21153;&#25152;&#38656;&#30340;&#20132;&#20114;&#22914;&#20309;&#36827;&#34892;&#37327;&#21270;&#65311;&#26368;&#36866;&#21512;&#25429;&#25417;&#36825;&#20123;&#20132;&#20114;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#26159;&#20160;&#20040;&#65311;&#20026;&#20102;&#22238;&#31572;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20449;&#24687;&#35770;&#26041;&#27861;&#26469;&#37327;&#21270;&#36755;&#20837;&#27169;&#24577;&#19982;&#36755;&#20986;&#20219;&#21153;&#20043;&#38388;&#30340;&#20887;&#20313;&#24230;&#12289;&#29420;&#29305;&#24615;&#21644;&#21327;&#21516;&#24615;&#12290;&#25105;&#20204;&#23558;&#36825;&#19977;&#20010;&#34913;&#37327;&#26631;&#20934;&#31216;&#20026;&#22810;&#27169;&#24577;&#20998;&#24067;&#65288;&#25110;&#31616;&#31216;PID&#65289;&#30340;PID&#32479;&#35745;&#37327;&#65292;&#24182;&#24341;&#20837;&#20102;&#20004;&#20010;&#26032;&#30340;PID&#32479;&#35745;&#20272;&#35745;&#22120;&#65292;&#36866;&#29992;&#20110;&#39640;&#32500;&#20998;&#24067;&#12290;&#20026;&#20102;&#39564;&#35777;PID&#20272;&#35745;&#65292;&#25105;&#20204;&#22312;&#24050;&#30693;PID&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#22522;&#20934;&#27979;&#35797;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent explosion of interest in multimodal applications has resulted in a wide selection of datasets and methods for representing and integrating information from different modalities. Despite these empirical advances, there remain fundamental research questions: How can we quantify the interactions that are necessary to solve a multimodal task? Subsequently, what are the most suitable multimodal models to capture these interactions? To answer these questions, we propose an information-theoretic approach to quantify the degree of redundancy, uniqueness, and synergy relating input modalities with an output task. We term these three measures as the PID statistics of a multimodal distribution (or PID for short), and introduce two new estimators for these PID statistics that scale to high-dimensional distributions. To validate PID estimation, we conduct extensive experiments on both synthetic datasets where the PID is known and on large-scale multimodal benchmarks where PID estimations
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#22823;&#35268;&#27169;&#23454;&#35777;&#35780;&#20272;&#35777;&#26126;&#65292;&#26080;&#38656;&#39069;&#22806;&#35757;&#32451;&#25110;&#25163;&#21160;&#21162;&#21147;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33258;&#21160;&#21270;&#29983;&#25104;&#21333;&#20803;&#27979;&#35797;&#26159;&#26377;&#25928;&#30340;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;LLMs&#32467;&#21512;&#20989;&#25968;&#30340;&#31614;&#21517;&#12289;&#23454;&#29616;&#21644;&#25991;&#26723;&#20013;&#30340;&#20351;&#29992;&#31034;&#20363;&#21487;&#20197;&#25104;&#21151;&#29983;&#25104;&#21333;&#20803;&#27979;&#35797;&#65292;&#24182;&#36890;&#36807;&#37325;&#26032;&#25552;&#31034;&#27169;&#22411;&#26469;&#20462;&#22797;&#29983;&#25104;&#22833;&#36133;&#30340;&#27979;&#35797;&#12290;</title><link>http://arxiv.org/abs/2302.06527</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#33258;&#21160;&#21270;&#21333;&#20803;&#27979;&#35797;&#29983;&#25104;&#30340;&#23454;&#35777;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
An Empirical Evaluation of Using Large Language Models for Automated Unit Test Generation. (arXiv:2302.06527v3 [cs.SE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.06527
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#22823;&#35268;&#27169;&#23454;&#35777;&#35780;&#20272;&#35777;&#26126;&#65292;&#26080;&#38656;&#39069;&#22806;&#35757;&#32451;&#25110;&#25163;&#21160;&#21162;&#21147;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33258;&#21160;&#21270;&#29983;&#25104;&#21333;&#20803;&#27979;&#35797;&#26159;&#26377;&#25928;&#30340;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;LLMs&#32467;&#21512;&#20989;&#25968;&#30340;&#31614;&#21517;&#12289;&#23454;&#29616;&#21644;&#25991;&#26723;&#20013;&#30340;&#20351;&#29992;&#31034;&#20363;&#21487;&#20197;&#25104;&#21151;&#29983;&#25104;&#21333;&#20803;&#27979;&#35797;&#65292;&#24182;&#36890;&#36807;&#37325;&#26032;&#25552;&#31034;&#27169;&#22411;&#26469;&#20462;&#22797;&#29983;&#25104;&#22833;&#36133;&#30340;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21333;&#20803;&#27979;&#35797;&#22312;&#30830;&#20445;&#36719;&#20214;&#27491;&#30830;&#24615;&#26041;&#38754;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#25163;&#21160;&#21019;&#24314;&#21333;&#20803;&#27979;&#35797;&#26159;&#19968;&#39033;&#36153;&#21147;&#30340;&#20219;&#21153;&#65292;&#36825;&#20419;&#20351;&#33258;&#21160;&#21270;&#30340;&#38656;&#27714;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#34987;&#24212;&#29992;&#20110;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#21033;&#29992;&#23545;&#29616;&#26377;&#27979;&#35797;&#26679;&#20363;&#30340;&#39069;&#22806;&#35757;&#32451;&#25110;&#23569;&#26679;&#26412;&#23398;&#20064;&#12290;&#26412;&#25991;&#23545;LLMs&#22312;&#26080;&#38656;&#39069;&#22806;&#35757;&#32451;&#25110;&#25163;&#21160;&#21162;&#21147;&#30340;&#24773;&#20917;&#19979;&#33258;&#21160;&#21270;&#29983;&#25104;&#21333;&#20803;&#27979;&#35797;&#30340;&#26377;&#25928;&#24615;&#36827;&#34892;&#20102;&#22823;&#35268;&#27169;&#23454;&#35777;&#35780;&#20272;&#65292;&#20026;LLM&#25552;&#20379;&#34987;&#27979;&#35797;&#20989;&#25968;&#30340;&#31614;&#21517;&#21644;&#23454;&#29616;&#20197;&#21450;&#20174;&#25991;&#26723;&#20013;&#25552;&#21462;&#30340;&#20351;&#29992;&#31034;&#20363;&#12290;&#25105;&#20204;&#36824;&#23581;&#35797;&#36890;&#36807;&#37325;&#26032;&#25552;&#31034;&#27169;&#22411;&#20351;&#29992;&#22833;&#36133;&#30340;&#27979;&#35797;&#21644;&#38169;&#35823;&#28040;&#24687;&#26469;&#20462;&#22797;&#29983;&#25104;&#22833;&#36133;&#30340;&#27979;&#35797;&#12290;&#25105;&#20204;&#22312;JavaScript&#20013;&#23454;&#29616;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;TestPilot&#20316;&#20026;&#19968;&#20010;&#33258;&#21160;&#20026;npm&#36719;&#20214;&#21253;&#20013;&#30340;&#25152;&#26377;API&#20989;&#25968;&#29983;&#25104;&#21333;&#20803;&#27979;&#35797;&#30340;&#27979;&#35797;&#29983;&#25104;&#24037;&#20855;&#65292;&#24182;&#20351;&#29992;OpenAI&#30340;gpt3.5-turbo LLM&#22312;25&#20010;npm&#36719;&#20214;&#21253;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#20849;&#35745;1,684&#20010;API&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unit tests play a key role in ensuring the correctness of software. However, manually creating unit tests is a laborious task, motivating the need for automation. Large Language Models (LLMs) have recently been applied to this problem, utilizing additional training or few-shot learning on examples of existing tests. This paper presents a large-scale empirical evaluation on the effectiveness of LLMs for automated unit test generation without additional training or manual effort, providing the LLM with the signature and implementation of the function under test, along with usage examples extracted from documentation. We also attempt to repair failed generated tests by re-prompting the model with the failing test and error message. We implement our approach in TestPilot, a test generation tool for JavaScript that automatically generates unit tests for all API functions in an npm package. We evaluate TestPilot using OpenAI's gpt3.5-turbo LLM on 25 npm packages with a total of 1,684 API fun
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#38024;&#23545;&#20154;&#26412;&#35745;&#31639;&#26426;&#35270;&#35273;&#25968;&#25454;&#38598;&#30340;&#36127;&#36131;&#20219;&#25968;&#25454;&#31649;&#29702;&#24314;&#35758;&#65292;&#37319;&#29992;&#39044;&#38450;&#24615;&#21453;&#24605;&#30340;&#35266;&#28857;&#65292;&#36981;&#24490;&#21407;&#21017;&#20027;&#20041;&#30340;&#20262;&#29702;&#26694;&#26550;&#65292;&#35299;&#20915;&#38544;&#31169;&#21644;&#20559;&#24046;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2302.03629</link><description>&lt;p&gt;
&#22522;&#20110;&#21407;&#21017;&#20027;&#20041;&#30340;&#36127;&#36131;&#20219;&#25968;&#25454;&#31649;&#29702;
&lt;/p&gt;
&lt;p&gt;
Principlism Guided Responsible Data Curation. (arXiv:2302.03629v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.03629
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#38024;&#23545;&#20154;&#26412;&#35745;&#31639;&#26426;&#35270;&#35273;&#25968;&#25454;&#38598;&#30340;&#36127;&#36131;&#20219;&#25968;&#25454;&#31649;&#29702;&#24314;&#35758;&#65292;&#37319;&#29992;&#39044;&#38450;&#24615;&#21453;&#24605;&#30340;&#35266;&#28857;&#65292;&#36981;&#24490;&#21407;&#21017;&#20027;&#20041;&#30340;&#20262;&#29702;&#26694;&#26550;&#65292;&#35299;&#20915;&#38544;&#31169;&#21644;&#20559;&#24046;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#26412;&#35745;&#31639;&#26426;&#35270;&#35273;&#25968;&#25454;&#25972;&#29702;&#23454;&#36341;&#32463;&#24120;&#24573;&#30053;&#38544;&#31169;&#21644;&#20559;&#24046;&#38382;&#39064;&#65292;&#23548;&#33268;&#25968;&#25454;&#38598;&#25764;&#22238;&#21644;&#19981;&#20844;&#24179;&#30340;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#38750;&#21516;&#24847;&#32593;&#32476;&#29228;&#21462;&#26500;&#24314;&#30340;&#20154;&#26412;&#35745;&#31639;&#26426;&#35270;&#35273;&#25968;&#25454;&#38598;&#32570;&#20047;&#20840;&#38754;&#30340;&#20844;&#24179;&#24615;&#21644;&#40065;&#26834;&#24615;&#35780;&#20272;&#25152;&#24517;&#38656;&#30340;&#20803;&#25968;&#25454;&#12290;&#24403;&#21069;&#30340;&#35299;&#20915;&#26041;&#27861;&#21518;&#26399;&#35299;&#20915;&#38382;&#39064;&#65292;&#32570;&#20047;&#35828;&#26381;&#21147;&#30340;&#37319;&#29992;&#29702;&#30001;&#25110;&#26410;&#33021;&#25552;&#20379;&#36866;&#24403;&#24212;&#29992;&#30340;&#21512;&#36866;&#32972;&#26223;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#30528;&#37325;&#20110;&#38024;&#23545;&#20154;&#26412;&#35745;&#31639;&#26426;&#35270;&#35273;&#25968;&#25454;&#38598;&#30340;&#20027;&#21160;&#39046;&#22495;&#29305;&#23450;&#24314;&#35758;&#65292;&#35299;&#20915;&#38544;&#31169;&#21644;&#20559;&#24046;&#38382;&#39064;&#12290;&#25105;&#20204;&#37319;&#29992;&#21453;&#24605;&#30340;&#35266;&#28857;&#65292;&#24182;&#20511;&#37492;&#20102;&#29616;&#26377;&#30340;&#23454;&#36341;&#21644;&#25351;&#21335;&#65292;&#36981;&#24490;&#21407;&#21017;&#20027;&#20041;&#30340;&#20262;&#29702;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human-centric computer vision (HCCV) data curation practices often neglect privacy and bias concerns, leading to dataset retractions and unfair models. Further, HCCV datasets constructed through nonconsensual web scraping lack the necessary metadata for comprehensive fairness and robustness evaluations. Current remedies address issues post hoc, lack persuasive justification for adoption, or fail to provide proper contextualization for appropriate application. Our research focuses on proactive, domain-specific recommendations for curating HCCV datasets, addressing privacy and bias. We adopt an ante hoc reflective perspective and draw from current practices and guidelines, guided by the ethical framework of principlism.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#30340;&#27010;&#29575;&#22810;&#26234;&#33021;&#20307;&#36712;&#36857;&#39044;&#27979;&#27169;&#22411;MTP-GO&#12290;&#35813;&#27169;&#22411;&#21033;&#29992;&#26102;&#38388;&#22270;&#31070;&#32463;&#32593;&#32476;&#32534;&#30721;&#22330;&#26223;&#65292;&#37319;&#29992;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#23454;&#29616;&#36816;&#21160;&#27169;&#22411;&#65292;&#24182;&#32467;&#21512;&#28151;&#21512;&#23494;&#24230;&#32593;&#32476;&#21644;&#21345;&#23572;&#26364;&#28388;&#27874;&#23454;&#29616;&#22810;&#27169;&#24577;&#27010;&#29575;&#39044;&#27979;&#65292;&#22312;&#22810;&#20010;&#25351;&#26631;&#19978;&#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2302.00735</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#30340;&#27010;&#29575;&#22810;&#26234;&#33021;&#20307;&#36712;&#36857;&#39044;&#27979;&#27169;&#22411;MTP-GO&#19982;&#31070;&#32463;ODE
&lt;/p&gt;
&lt;p&gt;
MTP-GO: Graph-Based Probabilistic Multi-Agent Trajectory Prediction with Neural ODEs. (arXiv:2302.00735v3 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.00735
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#30340;&#27010;&#29575;&#22810;&#26234;&#33021;&#20307;&#36712;&#36857;&#39044;&#27979;&#27169;&#22411;MTP-GO&#12290;&#35813;&#27169;&#22411;&#21033;&#29992;&#26102;&#38388;&#22270;&#31070;&#32463;&#32593;&#32476;&#32534;&#30721;&#22330;&#26223;&#65292;&#37319;&#29992;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#23454;&#29616;&#36816;&#21160;&#27169;&#22411;&#65292;&#24182;&#32467;&#21512;&#28151;&#21512;&#23494;&#24230;&#32593;&#32476;&#21644;&#21345;&#23572;&#26364;&#28388;&#27874;&#23454;&#29616;&#22810;&#27169;&#24577;&#27010;&#29575;&#39044;&#27979;&#65292;&#22312;&#22810;&#20010;&#25351;&#26631;&#19978;&#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#29616;&#24377;&#24615;&#30340;&#33258;&#20027;&#36335;&#24452;&#35268;&#21010;&#38656;&#35201;&#23545;&#21608;&#22260;&#36947;&#36335;&#29992;&#25143;&#26410;&#26469;&#34892;&#20026;&#20570;&#20986;&#21487;&#38752;&#30340;&#39044;&#27979;&#12290;&#20026;&#21709;&#24212;&#27492;&#38656;&#27714;&#21450;&#30456;&#20851;&#25361;&#25112;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#21517;&#20026;MTP-GO&#30340;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#37319;&#29992;&#26102;&#38388;&#22270;&#31070;&#32463;&#32593;&#32476;&#23545;&#22330;&#26223;&#36827;&#34892;&#32534;&#30721;&#65292;&#29983;&#25104;&#24213;&#23618;&#36816;&#21160;&#27169;&#22411;&#30340;&#36755;&#20837;&#12290;&#36816;&#21160;&#27169;&#22411;&#37319;&#29992;&#20102;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#65292;&#20854;&#20013;&#30340;&#29366;&#24577;&#36716;&#31227;&#20989;&#25968;&#23558;&#21644;&#20854;&#20182;&#37096;&#20998;&#19968;&#36215;&#36827;&#34892;&#23398;&#20064;&#12290;&#32467;&#21512;&#28151;&#21512;&#23494;&#24230;&#32593;&#32476;&#21644;&#21345;&#23572;&#26364;&#28388;&#27874;&#30340;&#27010;&#24565;&#65292;&#21487;&#20197;&#33719;&#24471;&#22810;&#27169;&#24577;&#30340;&#27010;&#29575;&#39044;&#27979;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#30340;&#39044;&#27979;&#33021;&#21147;&#20248;&#20110;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#22312;&#22810;&#20010;&#25351;&#26631;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
Enabling resilient autonomous motion planning requires robust predictions of surrounding road users' future behavior. In response to this need and the associated challenges, we introduce our model titled MTP-GO. The model encodes the scene using temporal graph neural networks to produce the inputs to an underlying motion model. The motion model is implemented using neural ordinary differential equations where the state-transition functions are learned with the rest of the model. Multimodal probabilistic predictions are obtained by combining the concept of mixture density networks and Kalman filtering. The results illustrate the predictive capabilities of the proposed model across various data sets, outperforming several state-of-the-art methods on a number of metrics.
&lt;/p&gt;</description></item><item><title>ClimaX&#26159;&#19968;&#31181;&#28789;&#27963;&#19988;&#21487;&#25512;&#24191;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#22825;&#27668;&#21644;&#27668;&#20505;&#31185;&#23398;&#65292;&#21487;&#20197;&#20351;&#29992;&#19981;&#21516;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2301.10343</link><description>&lt;p&gt;
ClimaX:&#19968;&#31181;&#29992;&#20110;&#22825;&#27668;&#21644;&#27668;&#20505;&#30340;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
ClimaX: A foundation model for weather and climate. (arXiv:2301.10343v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.10343
&lt;/p&gt;
&lt;p&gt;
ClimaX&#26159;&#19968;&#31181;&#28789;&#27963;&#19988;&#21487;&#25512;&#24191;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#22825;&#27668;&#21644;&#27668;&#20505;&#31185;&#23398;&#65292;&#21487;&#20197;&#20351;&#29992;&#19981;&#21516;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#22823;&#22810;&#25968;&#20808;&#36827;&#30340;&#22825;&#27668;&#21644;&#27668;&#20505;&#27169;&#22411;&#37117;&#26159;&#22522;&#20110;&#29289;&#29702;&#20449;&#24687;&#30340;&#25968;&#20540;&#27169;&#22411;&#12290;&#36825;&#20123;&#26041;&#27861;&#26088;&#22312;&#27169;&#25311;&#38750;&#32447;&#24615;&#21160;&#21147;&#23398;&#21644;&#22810;&#21464;&#37327;&#20043;&#38388;&#30340;&#22797;&#26434;&#30456;&#20114;&#20316;&#29992;&#65292;&#36825;&#20123;&#30456;&#20114;&#20316;&#29992;&#24456;&#38590;&#36817;&#20284;&#12290;&#27492;&#22806;&#65292;&#35768;&#22810;&#36825;&#26679;&#30340;&#25968;&#20540;&#27169;&#22411;&#22312;&#27169;&#25311;&#32454;&#31890;&#24230;&#31354;&#38388;&#21644;&#26102;&#38388;&#20998;&#36776;&#29575;&#30340;&#22823;&#27668;&#29616;&#35937;&#26102;&#35745;&#31639;&#37327;&#24456;&#22823;&#12290;&#26368;&#36817;&#30340;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#25968;&#25454;&#39537;&#21160;&#30340;&#21151;&#33021;&#26144;&#23556;&#26469;&#30452;&#25509;&#35299;&#20915;&#19979;&#28216;&#39044;&#27979;&#25110;&#25237;&#23556;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#32593;&#32476;&#26159;&#20351;&#29992;&#20026;&#29305;&#23450;&#26102;&#31354;&#20219;&#21153;&#31574;&#21010;&#21644;&#21516;&#36136;&#21270;&#30340;&#27668;&#20505;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#30340;&#65292;&#22240;&#27492;&#32570;&#20047;&#25968;&#20540;&#27169;&#22411;&#30340;&#26222;&#36941;&#24615;&#12290;&#25105;&#20204;&#24320;&#21457;&#24182;&#28436;&#31034;&#20102;ClimaX&#65292;&#36825;&#26159;&#19968;&#20010;&#28789;&#27963;&#19988;&#21487;&#25512;&#24191;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#21487;&#29992;&#20110;&#22825;&#27668;&#21644;&#27668;&#20505;&#31185;&#23398;&#65292;&#24182;&#21487;&#20197;&#20351;&#29992;&#36328;&#36234;&#19981;&#21516;&#25968;&#25454;&#38598;&#30340;&#24322;&#26500;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most state-of-the-art approaches for weather and climate modeling are based on physics-informed numerical models of the atmosphere. These approaches aim to model the non-linear dynamics and complex interactions between multiple variables, which are challenging to approximate. Additionally, many such numerical models are computationally intensive, especially when modeling the atmospheric phenomenon at a fine-grained spatial and temporal resolution. Recent data-driven approaches based on machine learning instead aim to directly solve a downstream forecasting or projection task by learning a data-driven functional mapping using deep neural networks. However, these networks are trained using curated and homogeneous climate datasets for specific spatiotemporal tasks, and thus lack the generality of numerical models. We develop and demonstrate ClimaX, a flexible and generalizable deep learning model for weather and climate science that can be trained using heterogeneous datasets spanning dif
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#32467;&#21512;&#20102;&#21542;&#23450;&#21644;&#35859;&#35789;&#21019;&#36896;&#30340;&#24402;&#32435;&#36923;&#36753;&#32534;&#31243;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#20165;&#20855;&#26377;&#20840;&#37327;&#21270;&#36523;&#20307;&#21464;&#37327;&#30340;&#35268;&#21017;&#26469;&#25913;&#21892;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#23454;&#29616;&#22312;NOPI&#20013;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#23398;&#20064;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2301.07629</link><description>&lt;p&gt;
&#36890;&#36807;&#21542;&#23450;&#21644;&#35859;&#35789;&#21019;&#36896;&#23454;&#29616;&#19968;&#33324;&#21270;
&lt;/p&gt;
&lt;p&gt;
Generalisation Through Negation and Predicate Invention. (arXiv:2301.07629v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.07629
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#32467;&#21512;&#20102;&#21542;&#23450;&#21644;&#35859;&#35789;&#21019;&#36896;&#30340;&#24402;&#32435;&#36923;&#36753;&#32534;&#31243;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#20165;&#20855;&#26377;&#20840;&#37327;&#21270;&#36523;&#20307;&#21464;&#37327;&#30340;&#35268;&#21017;&#26469;&#25913;&#21892;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#23454;&#29616;&#22312;NOPI&#20013;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#23398;&#20064;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#20174;&#23569;&#37327;&#30340;&#31034;&#20363;&#20013;&#36827;&#34892;&#27867;&#21270;&#26159;&#19968;&#20010;&#22522;&#26412;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#24402;&#32435;&#36923;&#36753;&#32534;&#31243;&#65288;ILP&#65289;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#21542;&#23450;&#21644;&#35859;&#35789;&#21019;&#36896;&#12290;&#32467;&#21512;&#36825;&#20004;&#20010;&#29305;&#24615;&#21487;&#20197;&#20351;ILP&#31995;&#32479;&#36890;&#36807;&#23398;&#20064;&#20165;&#20855;&#26377;&#20840;&#37327;&#21270;&#36523;&#20307;&#21464;&#37327;&#30340;&#35268;&#21017;&#26469;&#26356;&#22909;&#22320;&#27867;&#21270;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#24819;&#27861;&#23454;&#29616;&#22312;NOPI&#20013;&#65292;&#23427;&#21487;&#20197;&#23398;&#20064;&#20855;&#26377;&#35859;&#35789;&#21019;&#36896;&#30340;&#27491;&#24120;&#36923;&#36753;&#31243;&#24207;&#65292;&#21253;&#25324;&#20855;&#26377;&#20998;&#23618;&#21542;&#23450;&#30340;Datalog&#31243;&#24207;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#39046;&#22495;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#23398;&#20064;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ability to generalise from a small number of examples is a fundamental challenge in machine learning. To tackle this challenge, we introduce an inductive logic programming (ILP) approach that combines negation and predicate invention. Combining these two features allows an ILP system to generalise better by learning rules with universally quantified body-only variables. We implement our idea in NOPI, which can learn normal logic programs with predicate invention, including Datalog programs with stratified negation. Our experimental results on multiple domains show that our approach can improve predictive accuracies and learning times.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CLOD&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#35780;&#20272;&#30446;&#26631;&#26816;&#27979;&#25968;&#25454;&#38598;&#20013;&#27599;&#20010;&#26631;&#31614;&#30340;&#36136;&#37327;&#65292;&#35782;&#21035;&#21644;&#32416;&#27491;&#32570;&#22833;&#12289;&#34394;&#20551;&#12289;&#26631;&#31614;&#38169;&#35823;&#21644;&#20301;&#32622;&#38169;&#35823;&#30340;&#36793;&#30028;&#26694;&#65292;&#20174;&#32780;&#28040;&#38500;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#30340;&#38169;&#35823;&#31034;&#20363;&#65292;&#25552;&#39640;&#25968;&#25454;&#38598;&#30340;&#36136;&#37327;&#65292;&#24182;&#22312;&#23384;&#22312;&#22823;&#37327;&#22122;&#22768;&#30340;&#24773;&#20917;&#19979;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2211.13993</link><description>&lt;p&gt;
&#22312;&#30446;&#26631;&#26816;&#27979;&#25968;&#25454;&#38598;&#20013;&#35299;&#20915;&#26631;&#27880;&#22122;&#22768;
&lt;/p&gt;
&lt;p&gt;
Combating noisy labels in object detection datasets. (arXiv:2211.13993v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.13993
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CLOD&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#35780;&#20272;&#30446;&#26631;&#26816;&#27979;&#25968;&#25454;&#38598;&#20013;&#27599;&#20010;&#26631;&#31614;&#30340;&#36136;&#37327;&#65292;&#35782;&#21035;&#21644;&#32416;&#27491;&#32570;&#22833;&#12289;&#34394;&#20551;&#12289;&#26631;&#31614;&#38169;&#35823;&#21644;&#20301;&#32622;&#38169;&#35823;&#30340;&#36793;&#30028;&#26694;&#65292;&#20174;&#32780;&#28040;&#38500;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#30340;&#38169;&#35823;&#31034;&#20363;&#65292;&#25552;&#39640;&#25968;&#25454;&#38598;&#30340;&#36136;&#37327;&#65292;&#24182;&#22312;&#23384;&#22312;&#22823;&#37327;&#22122;&#22768;&#30340;&#24773;&#20917;&#19979;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#36136;&#37327;&#26159;&#24433;&#21709;&#27169;&#22411;&#20934;&#30830;&#24615;&#30340;&#20851;&#38190;&#22240;&#32032;&#65292;&#29305;&#21035;&#26159;&#22312;&#20687;&#30446;&#26631;&#26816;&#27979;&#36825;&#26679;&#30340;&#22256;&#38590;&#20219;&#21153;&#20013;&#12290;&#22312;&#22788;&#29702;&#25968;&#25454;&#38598;&#20013;&#30340;&#38169;&#35823;&#26102;&#65292;&#36890;&#24120;&#20250;&#25509;&#21463;&#19968;&#23450;&#27604;&#20363;&#30340;&#38169;&#35823;&#26679;&#26412;&#65292;&#20272;&#31639;&#23427;&#20204;&#30340;&#32622;&#20449;&#24230;&#24182;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#36171;&#20104;&#36866;&#24403;&#30340;&#26435;&#37325;&#65292;&#25110;&#32773;&#24573;&#30053;&#19981;&#30830;&#23450;&#30340;&#26679;&#26412;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;Confident Learning for Object Detection&#8221;&#65288;CLOD&#65289;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#35780;&#20272;&#30446;&#26631;&#26816;&#27979;&#25968;&#25454;&#38598;&#20013;&#27599;&#20010;&#26631;&#31614;&#30340;&#36136;&#37327;&#65292;&#35782;&#21035;&#32570;&#22833;&#12289;&#34394;&#20551;&#12289;&#26631;&#31614;&#38169;&#35823;&#21644;&#20301;&#32622;&#38169;&#35823;&#30340;&#36793;&#30028;&#26694;&#65292;&#24182;&#24314;&#35758;&#32416;&#27491;&#26041;&#27861;&#12290;&#36890;&#36807;&#19987;&#27880;&#20110;&#25214;&#21040;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#30340;&#38169;&#35823;&#31034;&#20363;&#65292;&#25105;&#20204;&#21487;&#20197;&#20174;&#26681;&#26412;&#19978;&#28040;&#38500;&#23427;&#20204;&#12290;&#21487;&#30097;&#30340;&#36793;&#30028;&#26694;&#21487;&#20197;&#36827;&#34892;&#26816;&#26597;&#65292;&#20197;&#25552;&#39640;&#25968;&#25454;&#38598;&#30340;&#36136;&#37327;&#65292;&#20174;&#32780;&#22312;&#19981;&#36827;&#19968;&#27493;&#22797;&#26434;&#21270;&#22797;&#26434;&#26550;&#26500;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;&#35813;&#26041;&#27861;&#22312;&#38169;&#35823;&#26657;&#27491;&#26041;&#38754;&#25552;&#20379;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#23384;&#22312;&#22823;&#37327;&#22122;&#22768;&#30340;&#24773;&#20917;&#19979;&#26174;&#33879;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The quality of training datasets for deep neural networks is a key factor contributing to the accuracy of resulting models. This effect is amplified in difficult tasks such as object detection. Dealing with errors in datasets is often limited to accepting that some fraction of examples is incorrect, estimating their confidence and assigning appropriate weights or ignoring uncertain ones during training. In this work, we propose a different approach. We introduce the Confident Learning for Object Detection (CLOD) algorithm for assessing the quality of each label in object detection datasets, identifying missing, spurious, mislabeled and mislocated bounding boxes and suggesting corrections. By focusing on finding incorrect examples in the training datasets, we can eliminate them at the root. Suspicious bounding boxes can be reviewed in order to improve the quality of the dataset, leading to better models without further complicating their already complex architectures. The proposed metho
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#26469;&#35299;&#20915;&#21452;&#23618;&#32972;&#21253;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#27604;&#31934;&#30830;&#31639;&#27861;&#24555;500&#20493;&#65292;&#21487;&#25214;&#21040;&#21487;&#34892;&#24615;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2211.13436</link><description>&lt;p&gt;
&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#27714;&#35299;&#21452;&#23618;&#32972;&#21253;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Solving Bilevel Knapsack Problem using Graph Neural Networks. (arXiv:2211.13436v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.13436
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#26469;&#35299;&#20915;&#21452;&#23618;&#32972;&#21253;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#27604;&#31934;&#30830;&#31639;&#27861;&#24555;500&#20493;&#65292;&#21487;&#25214;&#21040;&#21487;&#34892;&#24615;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21452;&#23618;&#20248;&#21270;&#38382;&#39064;&#26159;&#19968;&#31181;&#20855;&#26377;&#20004;&#20010;&#20195;&#29702;&#20154;&#65288;&#39046;&#23548;&#32773;&#21644;&#36861;&#38543;&#32773;&#65289;&#30340;&#23618;&#27425;&#20248;&#21270;&#38382;&#39064;&#12290;&#39046;&#23548;&#32773;&#39318;&#20808;&#20570;&#20986;&#33258;&#24049;&#30340;&#20915;&#31574;&#65292;&#36861;&#38543;&#32773;&#38543;&#21518;&#20570;&#20986;&#26368;&#20339;&#36873;&#25321;&#12290;&#39046;&#23548;&#32773;&#20102;&#35299;&#36861;&#38543;&#32773;&#30340;&#20449;&#24687;&#65292;&#38382;&#39064;&#30340;&#30446;&#26631;&#26159;&#20174;&#39046;&#23548;&#32773;&#30340;&#35282;&#24230;&#32771;&#34385;&#36861;&#38543;&#32773;&#30340;&#21453;&#24212;&#65292;&#25214;&#21040;&#26368;&#20248;&#35299;&#12290;&#23545;&#20110;&#21452;&#23618;&#20248;&#21270;&#38382;&#39064;&#26469;&#35828;&#65292;&#27809;&#26377;&#36890;&#29992;&#30340;&#39640;&#25928;&#31639;&#27861;&#25110;&#21830;&#29992;&#27714;&#35299;&#22120;&#21487;&#20197;&#24471;&#21040;&#26368;&#20248;&#35299;&#65292;&#21363;&#20351;&#26159;&#31616;&#21333;&#30340;&#38382;&#39064;&#20063;&#24456;&#38590;&#24471;&#21040;&#33391;&#22909;&#30340;&#35299;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#26469;&#35299;&#20915;&#21452;&#23618;&#32972;&#21253;&#38382;&#39064;&#12290;&#25105;&#20204;&#35757;&#32451;&#27169;&#22411;&#26469;&#39044;&#27979;&#39046;&#23548;&#32773;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;&#23558;&#23618;&#27425;&#20248;&#21270;&#38382;&#39064;&#36716;&#21270;&#20026;&#21333;&#23618;&#20248;&#21270;&#38382;&#39064;&#20197;&#33719;&#21462;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#21457;&#29616;&#20102;&#21487;&#34892;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36895;&#24230;&#27604;&#31934;&#30830;&#31639;&#27861;&#24555;500&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Bilevel Optimization Problem is a hierarchical optimization problem with two agents, a leader and a follower. The leader make their own decisions first, and the followers make the best choices accordingly. The leader knows the information of the followers, and the goal of the problem is to find the optimal solution by considering the reactions of the followers from the leader's point of view. For the Bilevel Optimization Problem, there are no general and efficient algorithms or commercial solvers to get an optimal solution, and it is very difficult to get a good solution even for a simple problem. In this paper, we propose a deep learning approach using Graph Neural Networks to solve the bilevel knapsack problem. We train the model to predict the leader's solution and use it to transform the hierarchical optimization problem into a single-level optimization problem to get the solution. Our model found the feasible solution that was about 500 times faster than the exact algorithm wi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#22312;&#21464;&#21387;&#22120;&#35821;&#35328;&#27169;&#22411;&#20013;&#19981;&#21516;&#30340;&#23545;&#25239;&#35757;&#32451;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#39044;&#35757;&#32451;&#25968;&#25454;&#22686;&#24378;&#25110;&#35757;&#32451;&#26102;&#38388;&#36755;&#20837;&#25200;&#21160;&#21487;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#40065;&#26834;&#24615;&#65292;&#32780;&#35757;&#32451;&#20013;&#20351;&#29992;&#30340;&#23884;&#20837;&#31354;&#38388;&#25200;&#21160;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#27867;&#21270;&#24615;&#12290;&#31070;&#32463;&#20803;&#30340;&#35821;&#35328;&#30456;&#20851;&#24615;&#20998;&#26512;&#34920;&#26126;&#65292;&#36825;&#20123;&#25913;&#36827;&#26159;&#30001;&#20110;&#23384;&#22312;&#8220;&#26356;&#19987;&#19994;&#8221;&#30340;&#31070;&#32463;&#20803;&#12290;&#36825;&#26159;&#39318;&#20010;&#23545;&#35821;&#35328;&#27169;&#22411;&#23545;&#25239;&#35757;&#32451;&#19981;&#21516;&#26041;&#27861;&#36827;&#34892;&#20840;&#38754;&#27604;&#36739;&#30340;&#24037;&#20316;&#12290;</title><link>http://arxiv.org/abs/2211.05523</link><description>&lt;p&gt;
&#23545;&#25239;&#35757;&#32451;&#23545;&#35821;&#35328;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#24615;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Impact of Adversarial Training on Robustness and Generalizability of Language Models. (arXiv:2211.05523v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.05523
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#22312;&#21464;&#21387;&#22120;&#35821;&#35328;&#27169;&#22411;&#20013;&#19981;&#21516;&#30340;&#23545;&#25239;&#35757;&#32451;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#39044;&#35757;&#32451;&#25968;&#25454;&#22686;&#24378;&#25110;&#35757;&#32451;&#26102;&#38388;&#36755;&#20837;&#25200;&#21160;&#21487;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#40065;&#26834;&#24615;&#65292;&#32780;&#35757;&#32451;&#20013;&#20351;&#29992;&#30340;&#23884;&#20837;&#31354;&#38388;&#25200;&#21160;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#27867;&#21270;&#24615;&#12290;&#31070;&#32463;&#20803;&#30340;&#35821;&#35328;&#30456;&#20851;&#24615;&#20998;&#26512;&#34920;&#26126;&#65292;&#36825;&#20123;&#25913;&#36827;&#26159;&#30001;&#20110;&#23384;&#22312;&#8220;&#26356;&#19987;&#19994;&#8221;&#30340;&#31070;&#32463;&#20803;&#12290;&#36825;&#26159;&#39318;&#20010;&#23545;&#35821;&#35328;&#27169;&#22411;&#23545;&#25239;&#35757;&#32451;&#19981;&#21516;&#26041;&#27861;&#36827;&#34892;&#20840;&#38754;&#27604;&#36739;&#30340;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#35757;&#32451;&#34987;&#24191;&#27867;&#35748;&#20026;&#26159;&#38450;&#24481;&#23545;&#25239;&#25915;&#20987;&#30340;&#26368;&#26377;&#25928;&#25163;&#27573;&#12290;&#20294;&#26159;&#65292;&#24050;&#32463;&#30830;&#35748;&#23545;&#25239;&#35757;&#32451;&#27169;&#22411;&#21516;&#26102;&#23454;&#29616;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#24615;&#38656;&#35201;&#36827;&#34892;&#26435;&#34913;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#28145;&#20837;&#27604;&#36739;&#35821;&#35328;&#27169;&#22411;&#20013;&#19981;&#21516;&#30340;&#23545;&#25239;&#35757;&#32451;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#21464;&#21387;&#22120;&#35821;&#35328;&#27169;&#22411;&#20013;&#39044;&#35757;&#32451;&#25968;&#25454;&#22686;&#24378;&#12289;&#35757;&#32451;&#26102;&#38388;&#36755;&#20837;&#25200;&#21160;&#21644;&#23884;&#20837;&#31354;&#38388;&#25200;&#21160;&#23545;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#24615;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#25968;&#25454;&#22686;&#24378;&#25110;&#35757;&#32451;&#26102;&#38388;&#36755;&#20837;&#25200;&#21160;&#21487;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#40065;&#26834;&#24615;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;&#23884;&#20837;&#31354;&#38388;&#25200;&#21160;&#36827;&#34892;&#35757;&#32451;&#21487;&#20197;&#26174;&#33879;&#22320;&#25552;&#39640;&#27867;&#21270;&#24615;&#12290;&#23398;&#20064;&#27169;&#22411;&#31070;&#32463;&#20803;&#30340;&#35821;&#35328;&#30456;&#20851;&#24615;&#20998;&#26512;&#34920;&#26126;&#65292;&#25913;&#21892;&#27867;&#21270;&#24615;&#26159;&#30001;&#20110;&#23384;&#22312;&#8220;&#26356;&#19987;&#19994;&#8221;&#30340;&#31070;&#32463;&#20803;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#39318;&#20010;&#23545;&#35821;&#35328;&#27169;&#22411;&#23545;&#25239;&#35757;&#32451;&#19981;&#21516;&#26041;&#27861;&#36827;&#34892;&#20840;&#38754;&#27604;&#36739;&#30340;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial training is widely acknowledged as the most effective defense against adversarial attacks. However, it is also well established that achieving both robustness and generalization in adversarially trained models involves a trade-off. The goal of this work is to provide an in depth comparison of different approaches for adversarial training in language models. Specifically, we study the effect of pre-training data augmentation as well as training time input perturbations vs. embedding space perturbations on the robustness and generalization of transformer-based language models. Our findings suggest that better robustness can be achieved by pre-training data augmentation or by training with input space perturbation. However, training with embedding space perturbation significantly improves generalization. A linguistic correlation analysis of neurons of the learned models reveals that the improved generalization is due to 'more specialized' neurons. To the best of our knowledge,
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#33539;&#24335;&#8212;&#8212;&#22522;&#20110;&#25552;&#31034;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65288;PromptCast&#65289;&#65292;&#23558;&#25968;&#23383;&#36755;&#20837;&#21644;&#36755;&#20986;&#36716;&#21270;&#20026;&#25552;&#31034;&#65292;&#24182;&#20197;&#21477;&#23376;&#21040;&#21477;&#23376;&#30340;&#26041;&#24335;&#25552;&#20986;&#39044;&#27979;&#20219;&#21153;&#65292;&#21487;&#20197;&#30452;&#25509;&#24212;&#29992;&#20110;&#35821;&#35328;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2210.08964</link><description>&lt;p&gt;
PromptCast&#65306;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#25552;&#31034;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#33539;&#24335;
&lt;/p&gt;
&lt;p&gt;
PromptCast: A New Prompt-based Learning Paradigm for Time Series Forecasting. (arXiv:2210.08964v3 [stat.ME] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.08964
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#33539;&#24335;&#8212;&#8212;&#22522;&#20110;&#25552;&#31034;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65288;PromptCast&#65289;&#65292;&#23558;&#25968;&#23383;&#36755;&#20837;&#21644;&#36755;&#20986;&#36716;&#21270;&#20026;&#25552;&#31034;&#65292;&#24182;&#20197;&#21477;&#23376;&#21040;&#21477;&#23376;&#30340;&#26041;&#24335;&#25552;&#20986;&#39044;&#27979;&#20219;&#21153;&#65292;&#21487;&#20197;&#30452;&#25509;&#24212;&#29992;&#20110;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#33539;&#24335;&#8212;&#8212;&#22522;&#20110;&#25552;&#31034;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65288;PromptCast&#65289;&#12290;&#22312;&#36825;&#31181;&#26032;&#30340;&#20219;&#21153;&#20013;&#65292;&#23558;&#21407;&#26469;&#30340;&#25968;&#23383;&#36755;&#20837;&#21644;&#36755;&#20986;&#36716;&#21270;&#20026;&#25552;&#31034;&#65292;&#24182;&#20197;&#21477;&#23376;&#21040;&#21477;&#23376;&#30340;&#26041;&#24335;&#25552;&#20986;&#39044;&#27979;&#20219;&#21153;&#65292;&#20351;&#24471;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#30452;&#25509;&#24212;&#29992;&#20110;&#39044;&#27979;&#30340;&#30446;&#30340;&#12290;&#20026;&#20102;&#25903;&#25345;&#21644;&#20419;&#36827;&#36825;&#20010;&#20219;&#21153;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#25968;&#25454;&#38598;&#65288;PISA&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a new perspective on time series forecasting. In existing time series forecasting methods, the models take a sequence of numerical values as input and yield numerical values as output. The existing SOTA models are largely based on the Transformer architecture, modified with multiple encoding mechanisms to incorporate the context and semantics around the historical data. Inspired by the successes of pre-trained language foundation models, we pose a question about whether these models can also be adapted to solve time-series forecasting. Thus, we propose a new forecasting paradigm: prompt-based time series forecasting (PromptCast). In this novel task, the numerical input and output are transformed into prompts and the forecasting task is framed in a sentence-to-sentence manner, making it possible to directly apply language models for forecasting purposes. To support and facilitate the research of this task, we also present a large-scale dataset (PISA) that includes th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#31163;&#32447;&#22686;&#24378;&#23398;&#20064;&#20013;&#30340;&#21518;&#38376;&#25915;&#20987;&#65292;&#36890;&#36807;&#21521;&#25968;&#25454;&#20013;&#28155;&#21152;&#25200;&#21160;&#65292;&#20351;&#24471;&#26234;&#33021;&#20307;&#22312;&#27880;&#20837;&#35302;&#21457;&#22120;&#30340;&#35266;&#27979;&#20540;&#19978;&#37319;&#21462;&#20302;&#22870;&#21169;&#21160;&#20316;&#65292;&#20174;&#32780;&#25552;&#20986;&#20102;BAFFLE&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2210.04688</link><description>&lt;p&gt;
BAFFLE: &#31163;&#32447;&#22686;&#24378;&#23398;&#20064;&#20013;&#30340;&#21518;&#38376;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
BAFFLE: Backdoor Attack in Offline Reinforcement Learning. (arXiv:2210.04688v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.04688
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#31163;&#32447;&#22686;&#24378;&#23398;&#20064;&#20013;&#30340;&#21518;&#38376;&#25915;&#20987;&#65292;&#36890;&#36807;&#21521;&#25968;&#25454;&#20013;&#28155;&#21152;&#25200;&#21160;&#65292;&#20351;&#24471;&#26234;&#33021;&#20307;&#22312;&#27880;&#20837;&#35302;&#21457;&#22120;&#30340;&#35266;&#27979;&#20540;&#19978;&#37319;&#21462;&#20302;&#22870;&#21169;&#21160;&#20316;&#65292;&#20174;&#32780;&#25552;&#20986;&#20102;BAFFLE&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36234;&#26469;&#36234;&#22810;&#30340;&#30740;&#31350;&#20851;&#27880;&#20110;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26041;&#27861;&#65292;&#20801;&#35768;&#26234;&#33021;&#20307;&#36890;&#36807;&#19982;&#29615;&#22659;&#30340;&#20132;&#20114;&#20013;&#25910;&#38598;&#30340;&#35797;&#38169;&#32463;&#39564;&#36827;&#34892;&#23398;&#20064;&#12290;&#26368;&#36817;&#65292;&#31163;&#32447;RL&#25104;&#20026;&#19968;&#31181;&#27969;&#34892;&#30340;RL&#33539;&#20363;&#65292;&#22240;&#20026;&#23427;&#33410;&#30465;&#20102;&#19982;&#29615;&#22659;&#30340;&#20132;&#20114;&#12290;&#22312;&#31163;&#32447;RL&#20013;&#65292;&#25968;&#25454;&#25552;&#20379;&#32773;&#20849;&#20139;&#22823;&#35268;&#27169;&#30340;&#39044;&#20808;&#25910;&#38598;&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20182;&#20154;&#21487;&#20197;&#22312;&#19981;&#19982;&#29615;&#22659;&#20132;&#20114;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;&#39640;&#36136;&#37327;&#30340;&#26234;&#33021;&#20307;&#12290;&#36825;&#31181;&#33539;&#20363;&#22312;&#26426;&#22120;&#20154;&#25511;&#21046;&#12289;&#33258;&#21160;&#39550;&#39542;&#31561;&#20851;&#38190;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#26377;&#25928;&#24615;&#12290;&#28982;&#32780;&#65292;&#36739;&#23569;&#20851;&#27880;&#30740;&#31350;&#31163;&#32447;RL&#31995;&#32479;&#30340;&#23433;&#20840;&#23041;&#32961;&#12290;&#26412;&#25991;&#20851;&#27880;&#21518;&#38376;&#25915;&#20987;&#65292;&#20854;&#20013;&#19968;&#20123;&#25200;&#21160;&#34987;&#28155;&#21152;&#21040;&#25968;&#25454;&#65288;&#35266;&#27979;&#20540;&#65289;&#20013;&#65292;&#20351;&#24471;&#22312;&#32473;&#23450;&#27491;&#24120;&#35266;&#27979;&#20540;&#30340;&#24773;&#20917;&#19979;&#65292;&#26234;&#33021;&#20307;&#37319;&#21462;&#39640;&#22870;&#21169;&#30340;&#21160;&#20316;&#65292;&#22312;&#27880;&#20837;&#35302;&#21457;&#22120;&#30340;&#35266;&#27979;&#20540;&#19978;&#37319;&#21462;&#20302;&#22870;&#21169;&#30340;&#21160;&#20316;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;BAFFLE&#65288;&#31163;&#32447;&#22686;&#24378;&#23398;&#20064;&#20013;&#30340;&#21518;&#38376;&#25915;&#20987;&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
A growing body of research has focused on the Reinforcement Learning (RL) methods which allow the agent to learn from trial-and-error experiences gathered during the interaction with the environment. Recently, offline RL becomes a popular RL paradigm because it saves the interactions with environments. In offline RL, data providers share large pre-collected datasets, and others can train high-quality agents without interacting with the environments. This paradigm has demonstrated effectiveness in critical tasks like robot control, autonomous driving, etc. However, less attention is paid to investigating the security threats to the offline RL system. This paper focuses on backdoor attacks, where some perturbations are added to the data (observations) such that given normal observations, the agent takes high-rewards actions, and low-reward actions on observations injected with triggers. In this paper, we propose Baffle (Backdoor Attack for Offline Reinforcement Learning), an approach tha
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31867;&#20851;&#20110;\L ukasiewicz&#36923;&#36753;&#30340;&#20449;&#24565;&#25193;&#23637;&#65292;&#22522;&#20110;&#19981;&#21516;&#30340;&#20449;&#24565;&#27010;&#24565;&#36827;&#34892;&#24314;&#27169;&#65292;&#24182;&#35777;&#26126;&#20102;&#36825;&#20123;&#25193;&#23637;&#30340;&#22768;&#38899;&#24615;&#21644;&#23436;&#25972;&#24615;&#23450;&#29702;&#12290;</title><link>http://arxiv.org/abs/2111.08564</link><description>&lt;p&gt;
\L ukasiewicz&#36923;&#36753;&#30340;&#20449;&#24565;&#25193;&#23637;
&lt;/p&gt;
&lt;p&gt;
Doxastic Extensions of \L ukasiewicz Logic. (arXiv:2111.08564v3 [cs.LO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.08564
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31867;&#20851;&#20110;\L ukasiewicz&#36923;&#36753;&#30340;&#20449;&#24565;&#25193;&#23637;&#65292;&#22522;&#20110;&#19981;&#21516;&#30340;&#20449;&#24565;&#27010;&#24565;&#36827;&#34892;&#24314;&#27169;&#65292;&#24182;&#35777;&#26126;&#20102;&#36825;&#20123;&#25193;&#23637;&#30340;&#22768;&#38899;&#24615;&#21644;&#23436;&#25972;&#24615;&#23450;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31867;&#20851;&#20110;&#20449;&#24565;&#30340;&#25193;&#23637;&#65292;&#23427;&#20204;&#26159;&#27169;&#31946;\L ukasiewicz&#36923;&#36753;&#20851;&#20110;&#22522;&#20110;Kripke&#30340;&#27169;&#22411;&#30340;&#19968;&#20123;&#36866;&#24403;&#31867;&#21035;&#30340;&#22768;&#38899;&#19988;&#23436;&#25972;&#30340;&#12290;&#36825;&#20123;&#25193;&#23637;&#30340;&#19968;&#31867;&#24102;&#26377;&#31867;&#20284;&#20110;&#32463;&#20856;&#20449;&#24565;&#30340;&#20266;&#21476;&#20856;&#20449;&#24565;&#23646;&#24615;&#65292;&#21478;&#19968;&#31867;&#22522;&#20110;&#19968;&#31181;&#25105;&#20204;&#31216;&#20043;&#20026;\textit{skeptical}&#65288;&#24576;&#30097;&#30340;&#65289;&#20449;&#24565;&#30340;&#26032;&#27010;&#24565;&#12290;&#25105;&#20204;&#20351;&#29992;&#20266;&#21476;&#20856;&#20449;&#24565;&#23545;&#27169;&#31946;&#29256;&#26412;&#30340;&#8220;&#27877;&#27870;&#20799;&#31461;&#38382;&#39064;&#8221;&#36827;&#34892;&#24314;&#27169;&#65292;&#20351;&#29992;&#24576;&#30097;&#30340;&#20449;&#24565;&#23545;CPA&#23433;&#20840;&#24615;&#23454;&#39564;&#36827;&#34892;&#24314;&#27169;&#65292;&#28982;&#21518;&#36890;&#36807;&#23637;&#31034;&#20266;&#21476;&#20856;&#20449;&#24565;&#19981;&#36866;&#29992;&#20110;&#27169;&#25311;CPA&#23454;&#39564;&#20013;&#23545;&#25163;&#30340;&#20449;&#24565;&#65292;&#20174;&#32780;&#35777;&#26126;&#25552;&#20986;&#24576;&#30097;&#30340;&#20449;&#24565;&#27010;&#24565;&#30340;&#21512;&#29702;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#19968;&#20123;&#25552;&#20986;&#30340;&#20449;&#24565;&#25193;&#23637;&#30340;&#23436;&#25972;&#24615;&#21644;&#22768;&#38899;&#24615;&#23450;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose two classes of doxastic extensions of fuzzy \L ukasiewicz logic that are sound and complete with respect to some appropriate classes of Kripke-based models in which both atomic propositions and accessibility relations are fuzzy. One class of these extensions is equipped with pseudo-classical belief that has properties similar to the classical belief, and the other class is based on a new notion of belief that we call it \textit{skeptical} belief. We model a fuzzy version of the muddy children problem using pseudo-classical belief and a CPA-security experiment using skeptical belief, then by showing that the pseudo-classical belief is not appropriate for modeling the belief of an adversary in a CPA-experiment we justify proposing the notion of skeptical belief. Furthermore, we prove the soundness and completeness theorems for some of the proposed doxastic extensions.
&lt;/p&gt;</description></item></channel></rss>