<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23454;&#35777;&#20998;&#26512;&#65292;&#21457;&#29616;&#30446;&#21069;&#30340;&#35770;&#25454;&#25688;&#35201;&#26041;&#27861;&#22312;&#25429;&#25417;&#22810;&#26679;&#24615;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#24182;&#25552;&#20986;&#22810;&#26679;&#24615;&#26377;&#19977;&#20010;&#26041;&#38754;&#65306;&#24847;&#35265;&#12289;&#27880;&#37322;&#32773;&#21644;&#26469;&#28304;&#12290;&#25152;&#30740;&#31350;&#30340;&#20851;&#38190;&#28857;&#20998;&#26512;&#20219;&#21153;&#20013;&#30340;&#36890;&#29992;LLMs&#21644;&#19987;&#38376;&#30340;KPA&#27169;&#22411;&#37117;&#26377;&#20114;&#34917;&#30340;&#20248;&#21183;&#65292;&#35757;&#32451;&#25968;&#25454;&#30340;&#22810;&#26679;&#21270;&#23558;&#26377;&#21161;&#20110;&#25552;&#39640;&#27867;&#21270;&#33021;&#21147;&#12290;&#22240;&#27492;&#65292;&#24212;&#23545;&#35770;&#35777;&#25688;&#35201;&#20013;&#30340;&#22810;&#26679;&#24615;&#38656;&#35201;&#37319;&#29992;&#22810;&#31181;&#31574;&#30053;&#26469;&#22788;&#29702;&#20027;&#35266;&#24615;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01535</link><description>&lt;p&gt;
&#35770;&#35770;&#25991;&#25688;&#35201;&#20013;&#30340;&#22810;&#26679;&#24615;&#30340;&#23454;&#35777;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
An Empirical Analysis of Diversity in Argument Summarization
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01535
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23454;&#35777;&#20998;&#26512;&#65292;&#21457;&#29616;&#30446;&#21069;&#30340;&#35770;&#25454;&#25688;&#35201;&#26041;&#27861;&#22312;&#25429;&#25417;&#22810;&#26679;&#24615;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#24182;&#25552;&#20986;&#22810;&#26679;&#24615;&#26377;&#19977;&#20010;&#26041;&#38754;&#65306;&#24847;&#35265;&#12289;&#27880;&#37322;&#32773;&#21644;&#26469;&#28304;&#12290;&#25152;&#30740;&#31350;&#30340;&#20851;&#38190;&#28857;&#20998;&#26512;&#20219;&#21153;&#20013;&#30340;&#36890;&#29992;LLMs&#21644;&#19987;&#38376;&#30340;KPA&#27169;&#22411;&#37117;&#26377;&#20114;&#34917;&#30340;&#20248;&#21183;&#65292;&#35757;&#32451;&#25968;&#25454;&#30340;&#22810;&#26679;&#21270;&#23558;&#26377;&#21161;&#20110;&#25552;&#39640;&#27867;&#21270;&#33021;&#21147;&#12290;&#22240;&#27492;&#65292;&#24212;&#23545;&#35770;&#35777;&#25688;&#35201;&#20013;&#30340;&#22810;&#26679;&#24615;&#38656;&#35201;&#37319;&#29992;&#22810;&#31181;&#31574;&#30053;&#26469;&#22788;&#29702;&#20027;&#35266;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22312;&#32447;&#31038;&#20250;&#35752;&#35770;&#20013;&#65292;&#25552;&#20379;&#39640;&#27700;&#24179;&#30340;&#35770;&#25454;&#26159;&#20419;&#36827;&#21442;&#19982;&#30340;&#20851;&#38190;&#20219;&#21153;&#12290;&#30446;&#21069;&#30340;&#35770;&#25454;&#25688;&#35201;&#26041;&#27861;&#32570;&#22833;&#20102;&#36825;&#39033;&#20219;&#21153;&#30340;&#19968;&#20010;&#37325;&#35201;&#26041;&#38754;&#65292;&#21363;&#25429;&#25417;&#22810;&#26679;&#24615;&#65292;&#36825;&#23545;&#20110;&#21253;&#23481;&#22810;&#20010;&#35266;&#28857;&#26159;&#37325;&#35201;&#30340;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19977;&#20010;&#26041;&#38754;&#30340;&#22810;&#26679;&#24615;&#65306;&#24847;&#35265;&#12289;&#27880;&#37322;&#32773;&#21644;&#26469;&#28304;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#19968;&#31181;&#21517;&#20026;&#20851;&#38190;&#28857;&#20998;&#26512;&#30340;&#27969;&#34892;&#35770;&#25454;&#25688;&#35201;&#20219;&#21153;&#30340;&#26041;&#27861;&#65292;&#26174;&#31034;&#36825;&#20123;&#26041;&#27861;&#22312;(1)&#20195;&#34920;&#23569;&#25968;&#20154;&#20849;&#20139;&#30340;&#35770;&#28857;&#19978;&#65292;(2)&#22788;&#29702;&#26469;&#33258;&#21508;&#31181;&#26469;&#28304;&#30340;&#25968;&#25454;&#20197;&#21450;(3)&#19982;&#20154;&#24037;&#25552;&#20379;&#30340;&#20027;&#35266;&#27880;&#37322;&#30456;&#19968;&#33268;&#26041;&#38754;&#36935;&#21040;&#20102;&#22256;&#38590;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36890;&#29992;&#30340;LLM&#21644;&#19987;&#38376;&#30340;KPA&#27169;&#22411;&#37117;&#34920;&#29616;&#20986;&#20102;&#36825;&#31181;&#34892;&#20026;&#65292;&#20294;&#20855;&#26377;&#20114;&#34917;&#30340;&#20248;&#21183;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#35757;&#32451;&#25968;&#25454;&#30340;&#22810;&#26679;&#21270;&#21487;&#33021;&#25913;&#21892;&#27867;&#21270;&#33021;&#21147;&#12290;&#24212;&#23545;&#35770;&#35777;&#25688;&#35201;&#20013;&#30340;&#22810;&#26679;&#24615;&#38656;&#35201;&#37319;&#29992;&#19968;&#31995;&#21015;&#31574;&#30053;&#26469;&#22788;&#29702;&#20027;&#35266;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Presenting high-level arguments is a crucial task for fostering participation in online societal discussions. Current argument summarization approaches miss an important facet of this task -- capturing diversity -- which is important for accommodating multiple perspectives. We introduce three aspects of diversity: those of opinions, annotators, and sources. We evaluate approaches to a popular argument summarization task called Key Point Analysis, which shows how these approaches struggle to (1) represent arguments shared by few people, (2) deal with data from various sources, and (3) align with subjectivity in human-provided annotations. We find that both general-purpose LLMs and dedicated KPA models exhibit this behavior, but have complementary strengths. Further, we observe that diversification of training data may ameliorate generalization. Addressing diversity in argument summarization requires a mix of strategies to deal with subjectivity.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#39640;&#25928;&#30340;&#22240;&#26524;&#22270;&#21457;&#29616;&#65292;&#37319;&#29992;&#20102;&#24191;&#24230;&#20248;&#20808;&#25628;&#32034;&#26041;&#27861;&#65292;&#21482;&#38656;&#35201;&#32447;&#24615;&#25968;&#37327;&#30340;&#26597;&#35810;&#65292;&#21516;&#26102;&#33021;&#36731;&#26494;&#32467;&#21512;&#35266;&#23519;&#25968;&#25454;&#20197;&#25552;&#39640;&#24615;&#33021;&#65292;&#20855;&#26377;&#39640;&#25928;&#24615;&#21644;&#25968;&#25454;&#25928;&#29575;&#65292;&#24182;&#22312;&#30495;&#23454;&#22240;&#26524;&#22270;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#19981;&#21516;&#39046;&#22495;&#30340;&#24191;&#27867;&#36866;&#29992;&#24615;&#28508;&#21147;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01207</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#25928;&#22240;&#26524;&#22270;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Efficient Causal Graph Discovery Using Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01207
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#39640;&#25928;&#30340;&#22240;&#26524;&#22270;&#21457;&#29616;&#65292;&#37319;&#29992;&#20102;&#24191;&#24230;&#20248;&#20808;&#25628;&#32034;&#26041;&#27861;&#65292;&#21482;&#38656;&#35201;&#32447;&#24615;&#25968;&#37327;&#30340;&#26597;&#35810;&#65292;&#21516;&#26102;&#33021;&#36731;&#26494;&#32467;&#21512;&#35266;&#23519;&#25968;&#25454;&#20197;&#25552;&#39640;&#24615;&#33021;&#65292;&#20855;&#26377;&#39640;&#25928;&#24615;&#21644;&#25968;&#25454;&#25928;&#29575;&#65292;&#24182;&#22312;&#30495;&#23454;&#22240;&#26524;&#22270;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#19981;&#21516;&#39046;&#22495;&#30340;&#24191;&#27867;&#36866;&#29992;&#24615;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;LLMs&#36827;&#34892;&#23436;&#25972;&#30340;&#22240;&#26524;&#22270;&#21457;&#29616;&#12290;&#20043;&#21069;&#22522;&#20110;LLM&#30340;&#26041;&#27861;&#37319;&#29992;&#20102;&#25104;&#23545;&#26597;&#35810;&#30340;&#26041;&#27861;&#65292;&#20294;&#36825;&#38656;&#35201;&#20108;&#27425;&#26597;&#35810;&#30340;&#25968;&#37327;&#65292;&#23545;&#20110;&#36739;&#22823;&#30340;&#22240;&#26524;&#22270;&#26469;&#35828;&#24456;&#24555;&#21464;&#24471;&#19981;&#21487;&#34892;&#12290;&#30456;&#21453;&#65292;&#25552;&#20986;&#30340;&#26694;&#26550;&#37319;&#29992;&#20102;&#24191;&#24230;&#20248;&#20808;&#25628;&#32034;&#65288;BFS&#65289;&#30340;&#26041;&#27861;&#65292;&#21482;&#38656;&#35201;&#32447;&#24615;&#25968;&#37327;&#30340;&#26597;&#35810;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#24403;&#26377;&#25152;&#35266;&#23519;&#25968;&#25454;&#21487;&#29992;&#26102;&#65292;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#36731;&#26494;&#22320;&#36827;&#34892;&#32467;&#21512;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;&#38500;&#20102;&#26356;&#20855;&#26102;&#38388;&#21644;&#25968;&#25454;&#25928;&#29575;&#22806;&#65292;&#25552;&#20986;&#30340;&#26694;&#26550;&#22312;&#19981;&#21516;&#22823;&#23567;&#30340;&#30495;&#23454;&#22240;&#26524;&#22270;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#32467;&#26524;&#35777;&#26126;&#20102;&#25552;&#20986;&#26041;&#27861;&#22312;&#21457;&#29616;&#22240;&#26524;&#20851;&#31995;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#21644;&#25928;&#29575;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#19981;&#21516;&#39046;&#22495;&#30340;&#22240;&#26524;&#22270;&#21457;&#29616;&#20219;&#21153;&#20013;&#30340;&#24191;&#27867;&#36866;&#29992;&#24615;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a novel framework that leverages LLMs for full causal graph discovery. While previous LLM-based methods have used a pairwise query approach, this requires a quadratic number of queries which quickly becomes impractical for larger causal graphs. In contrast, the proposed framework uses a breadth-first search (BFS) approach which allows it to use only a linear number of queries. We also show that the proposed method can easily incorporate observational data when available, to improve performance. In addition to being more time and data-efficient, the proposed framework achieves state-of-the-art results on real-world causal graphs of varying sizes. The results demonstrate the effectiveness and efficiency of the proposed method in discovering causal relationships, showcasing its potential for broad applicability in causal graph discovery tasks across different domains.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#38598;&#25104;&#23398;&#20064;&#21644;&#27491;&#21017;&#21270;&#24494;&#35843;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;AI&#27169;&#22411;&#20013;&#30340;&#20559;&#35265;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#21487;&#36890;&#36807;&#22312;&#23567;&#25968;&#25454;&#38598;&#21644;&#26377;&#20559;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#19978;&#35757;&#32451;&#22810;&#20010;&#23545;&#25239;&#20559;&#35265;&#30340;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#38598;&#25104;&#23398;&#20064;&#24471;&#21040;&#26080;&#20559;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;CIFAR10&#21644;HAM10000&#25968;&#25454;&#38598;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://rss.arxiv.org/abs/2402.00910</link><description>&lt;p&gt;
&#36890;&#36807;&#38598;&#25104;&#23398;&#20064;&#21644;&#27491;&#21017;&#21270;&#24494;&#35843;&#24212;&#23545;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Addressing Bias Through Ensemble Learning and Regularized Fine-Tuning
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.00910
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#38598;&#25104;&#23398;&#20064;&#21644;&#27491;&#21017;&#21270;&#24494;&#35843;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;AI&#27169;&#22411;&#20013;&#30340;&#20559;&#35265;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#21487;&#36890;&#36807;&#22312;&#23567;&#25968;&#25454;&#38598;&#21644;&#26377;&#20559;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#19978;&#35757;&#32451;&#22810;&#20010;&#23545;&#25239;&#20559;&#35265;&#30340;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#38598;&#25104;&#23398;&#20064;&#24471;&#21040;&#26080;&#20559;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;CIFAR10&#21644;HAM10000&#25968;&#25454;&#38598;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#20915;AI&#27169;&#22411;&#20013;&#30340;&#20559;&#35265;&#23545;&#20110;&#30830;&#20445;&#20844;&#24179;&#21644;&#20934;&#30830;&#30340;&#39044;&#27979;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#33719;&#21462;&#22823;&#35268;&#27169;&#12289;&#26080;&#20559;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#38754;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#22810;&#31181;&#26041;&#27861;&#28040;&#38500;AI&#27169;&#22411;&#20013;&#30340;&#20559;&#35265;&#65292;&#20854;&#20013;&#20165;&#20351;&#29992;&#23567;&#22411;&#25968;&#25454;&#38598;&#21644;&#28508;&#22312;&#26377;&#20559;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;&#25105;&#20204;&#36890;&#36807;&#25968;&#25454;&#20998;&#31163;&#12289;&#23616;&#37096;&#35757;&#32451;&#21644;&#27491;&#21017;&#21270;&#24494;&#35843;&#35757;&#32451;&#22810;&#20010;&#27169;&#22411;&#65292;&#20197;&#23545;&#25239;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#20559;&#35265;&#65292;&#24471;&#21040;&#28508;&#22312;&#30340;&#23545;&#25239;&#20559;&#35265;&#27169;&#22411;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#37319;&#29992;&#38598;&#25104;&#23398;&#20064;&#26469;&#23545;&#25152;&#26377;&#27169;&#22411;&#36827;&#34892;&#39044;&#27979;&#65292;&#20197;&#36798;&#21040;&#26080;&#20559;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#21152;&#36895;&#25105;&#20204;&#30340;&#38598;&#25104;&#27169;&#22411;&#30340;&#25512;&#29702;&#26102;&#38388;&#65292;&#25105;&#20204;&#20351;&#29992;&#30693;&#35782;&#33976;&#39311;&#26469;&#33719;&#24471;&#19968;&#20010;&#21333;&#19968;&#26080;&#20559;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#36890;&#36807;&#22312;CIFAR10&#21644;HAM10000&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#23637;&#31034;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#36825;&#39033;&#24037;&#20316;&#20026;&#21019;&#24314;&#26356;&#26080;&#20559;&#12289;&#21487;&#38752;&#30340;AI&#27169;&#22411;&#30340;&#25345;&#32493;&#21162;&#21147;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
Addressing biases in AI models is crucial for ensuring fair and accurate predictions. However, obtaining large, unbiased datasets for training can be challenging. This paper proposes a comprehensive approach using multiple methods to remove bias in AI models, with only a small dataset and a potentially biased pretrained model. We train multiple models with the counter-bias of the pre-trained model through data splitting, local training, and regularized fine-tuning, gaining potentially counter-biased models. Then, we employ ensemble learning for all models to reach unbiased predictions. To further accelerate the inference time of our ensemble model, we conclude our solution with knowledge distillation that results in a single unbiased neural network. We demonstrate the effectiveness of our approach through experiments on the CIFAR10 and HAM10000 datasets, showcasing promising results. This work contributes to the ongoing effort to create more unbiased and reliable AI models, even with l
&lt;/p&gt;</description></item><item><title>AQA-Bench&#26159;&#19968;&#20010;&#20132;&#20114;&#24335;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#31639;&#27861;&#19978;&#19979;&#25991;&#20013;&#30340;&#39034;&#24207;&#25512;&#29702;&#33021;&#21147;&#12290;&#30740;&#31350;&#21457;&#29616;&#38381;&#28304;&#27169;&#22411;&#34920;&#29616;&#20986;&#26356;&#24378;&#30340;&#39034;&#24207;&#25512;&#29702;&#33021;&#21147;&#65292;&#26174;&#33879;&#20248;&#20110;&#24320;&#28304;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.09404</link><description>&lt;p&gt;
AQA-Bench&#65306;&#35780;&#20272;LLM&#39034;&#24207;&#25512;&#29702;&#33021;&#21147;&#30340;&#20132;&#20114;&#24335;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
AQA-Bench: An Interactive Benchmark for Evaluating LLMs' Sequential Reasoning Ability
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09404
&lt;/p&gt;
&lt;p&gt;
AQA-Bench&#26159;&#19968;&#20010;&#20132;&#20114;&#24335;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#31639;&#27861;&#19978;&#19979;&#25991;&#20013;&#30340;&#39034;&#24207;&#25512;&#29702;&#33021;&#21147;&#12290;&#30740;&#31350;&#21457;&#29616;&#38381;&#28304;&#27169;&#22411;&#34920;&#29616;&#20986;&#26356;&#24378;&#30340;&#39034;&#24207;&#25512;&#29702;&#33021;&#21147;&#65292;&#26174;&#33879;&#20248;&#20110;&#24320;&#28304;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;AQA-Bench&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#31639;&#27861;&#19978;&#19979;&#25991;&#20013;&#65292;&#22914;&#28145;&#24230;&#20248;&#20808;&#25628;&#32034;&#65288;DFS&#65289;&#31561;&#30340;&#39034;&#24207;&#25512;&#29702;&#33021;&#21147;&#12290;&#25105;&#20204;&#35780;&#20272;&#22522;&#20934;&#27979;&#35797;&#30340;&#20851;&#38190;&#29305;&#28857;&#22312;&#20110;&#20854;&#20132;&#20114;&#24335;&#35780;&#20272;&#21327;&#35758;-&#20363;&#22914;&#65292;&#22312;DFS&#20013;&#65292;&#27599;&#20010;&#33410;&#28857;&#30340;&#21487;&#29992;&#36830;&#25509;&#36793;&#21462;&#20915;&#20110;&#27169;&#22411;&#23545;&#35813;&#33410;&#28857;&#30340;&#36941;&#21382;&#65292;&#22240;&#27492;&#38656;&#35201;LLM&#26377;&#25928;&#22320;&#35760;&#20303;&#24050;&#35775;&#38382;&#33410;&#28857;&#24182;&#31574;&#21010;&#21518;&#32493;&#31227;&#21160;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#20351;&#29992;&#19977;&#31181;&#19981;&#21516;&#30340;&#31639;&#27861;&#26500;&#24314;&#20102;AQA-Bench&#65292;&#20998;&#21035;&#26159;&#20108;&#20998;&#25628;&#32034;&#65292;&#28145;&#24230;&#20248;&#20808;&#25628;&#32034;&#21644;&#24191;&#24230;&#20248;&#20808;&#25628;&#32034;&#65292;&#24182;&#35780;&#20272;&#20102;12&#31181;&#19981;&#21516;&#30340;LLMs&#30340;&#39034;&#24207;&#25512;&#29702;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#35843;&#26597;&#25581;&#31034;&#20102;&#19968;&#20123;&#26377;&#36259;&#30340;&#21457;&#29616;&#65306;&#65288;1&#65289;&#31867;&#20284;GPT-4&#21644;Gemini&#31561;&#38381;&#28304;&#27169;&#22411;&#36890;&#24120;&#26174;&#31034;&#20986;&#24378;&#22823;&#30340;&#39034;&#24207;&#25512;&#29702;&#33021;&#21147;&#65292;&#26126;&#26174;&#20248;&#20110;&#24320;&#28304;LLMs&#12290;&#65288;2&#65289;&#22825;&#30495;&#22320;&#25552;&#20379;&#20114;&#25805;&#20316;&#24615;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09404v1 Announce Type: cross Abstract: This paper introduces AQA-Bench, a novel benchmark to assess the sequential reasoning capabilities of large language models (LLMs) in algorithmic contexts, such as depth-first search (DFS). The key feature of our evaluation benchmark lies in its interactive evaluation protocol -- for example, in DFS, the availability of each node's connected edge is contingent upon the model's traversal to that node, thereby necessitating the LLM's ability to effectively remember visited nodes and strategize subsequent moves. We comprehensively build AQA-Bench with three different algorithms, namely binary search, depth-first search, and breadth-first search, and to evaluate the sequential reasoning ability of 12 different LLMs. Our investigations reveal several interesting findings: (1) Closed-source models like GPT-4 and Gemini generally show strong sequential reasoning ability, significantly outperforming open-source LLMs. (2) Naively providing inter
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20027;&#21160;&#26597;&#35810;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#19982;&#20154;&#31867;&#21453;&#39304;&#30340;&#23545;&#40784;&#38382;&#39064;&#12290;&#36890;&#36807;&#22312;&#24378;&#21270;&#23398;&#20064;&#36807;&#31243;&#20013;&#20943;&#23569;&#20154;&#24037;&#26631;&#27880;&#20559;&#22909;&#25968;&#25454;&#30340;&#38656;&#27714;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#36739;&#20302;&#30340;&#20195;&#20215;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#36739;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.09401</link><description>&lt;p&gt;
&#20351;&#29992;&#20027;&#21160;&#26597;&#35810;&#30340;&#20154;&#31867;&#21453;&#39304;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning from Human Feedback with Active Queries
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09401
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20027;&#21160;&#26597;&#35810;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#19982;&#20154;&#31867;&#21453;&#39304;&#30340;&#23545;&#40784;&#38382;&#39064;&#12290;&#36890;&#36807;&#22312;&#24378;&#21270;&#23398;&#20064;&#36807;&#31243;&#20013;&#20943;&#23569;&#20154;&#24037;&#26631;&#27880;&#20559;&#22909;&#25968;&#25454;&#30340;&#38656;&#27714;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#36739;&#20302;&#30340;&#20195;&#20215;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#36739;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#19982;&#20154;&#31867;&#20559;&#22909;&#36827;&#34892;&#23545;&#40784;&#65292;&#22312;&#26500;&#24314;&#29616;&#20195;&#29983;&#25104;&#27169;&#22411;&#20013;&#21457;&#25381;&#37325;&#35201;&#20316;&#29992;&#65292;&#36825;&#21487;&#20197;&#36890;&#36807;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#26469;&#23454;&#29616;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#24403;&#21069;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#65292;&#20294;&#24448;&#24448;&#38656;&#35201;&#22823;&#37327;&#30340;&#20154;&#24037;&#26631;&#27880;&#20559;&#22909;&#25968;&#25454;&#65292;&#32780;&#36825;&#31181;&#25968;&#25454;&#25910;&#38598;&#36153;&#26102;&#36153;&#21147;&#12290;&#26412;&#25991;&#21463;&#21040;&#20027;&#21160;&#23398;&#20064;&#30340;&#25104;&#21151;&#21551;&#21457;&#65292;&#36890;&#36807;&#25552;&#20986;&#26597;&#35810;&#25928;&#29575;&#39640;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#39318;&#20808;&#23558;&#23545;&#40784;&#38382;&#39064;&#24418;&#24335;&#21270;&#20026;&#19978;&#19979;&#25991;&#31454;&#20105;&#20108;&#33218;&#24378;&#30423;&#38382;&#39064;&#65292;&#24182;&#35774;&#35745;&#20102;&#22522;&#20110;&#20027;&#21160;&#26597;&#35810;&#30340;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#65288;APPO&#65289;&#31639;&#27861;&#65292;&#20855;&#26377;$\tilde{O}(d^2/\Delta)$&#30340;&#36951;&#25022;&#30028;&#21644;$\tilde{O}(d^2/\Delta^2)$&#30340;&#26597;&#35810;&#22797;&#26434;&#24230;&#65292;&#20854;&#20013;$d$&#26159;&#29305;&#24449;&#31354;&#38388;&#30340;&#32500;&#24230;&#65292;$\Delta$&#26159;&#25152;&#26377;&#19978;&#19979;&#25991;&#20013;&#30340;&#27425;&#20248;&#24046;&#36317;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ADPO&#65292;&#36825;&#26159;&#25105;&#20204;&#31639;&#27861;&#30340;&#23454;&#38469;&#29256;&#26412;&#65292;&#22522;&#20110;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#65288;DPO&#65289;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09401v1 Announce Type: cross Abstract: Aligning large language models (LLM) with human preference plays a key role in building modern generative models and can be achieved by reinforcement learning from human feedback (RLHF). Despite their superior performance, current RLHF approaches often require a large amount of human-labelled preference data, which is expensive to collect. In this paper, inspired by the success of active learning, we address this problem by proposing query-efficient RLHF methods. We first formalize the alignment problem as a contextual dueling bandit problem and design an active-query-based proximal policy optimization (APPO) algorithm with an $\tilde{O}(d^2/\Delta)$ regret bound and an $\tilde{O}(d^2/\Delta^2)$ query complexity, where $d$ is the dimension of feature space and $\Delta$ is the sub-optimality gap over all the contexts. We then propose ADPO, a practical version of our algorithm based on direct preference optimization (DPO) and apply it to 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026;LESS&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#38598;&#25104;&#19968;&#20010;&#22266;&#23450;&#23610;&#23544;&#30340;&#32531;&#23384;&#21644;&#22522;&#20110;&#39537;&#36880;&#30340;&#32531;&#23384;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20943;&#23567;&#20869;&#23384;&#21344;&#29992;&#30340;&#38382;&#39064;&#65292;&#21516;&#26102;&#20445;&#25345;&#20840;&#37096;&#26631;&#35760;&#30340;&#21487;&#26597;&#35810;&#33021;&#21147;&#65292;&#24182;&#22312;&#22810;&#31181;&#20219;&#21153;&#19978;&#26174;&#31034;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.09398</link><description>&lt;p&gt;
&#20351;&#29992;KV&#32531;&#23384;&#21387;&#32553;&#21512;&#25104;&#24490;&#29615;&#20197;&#25552;&#39640;LLM&#25512;&#26029;&#30340;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;
Get More with LESS: Synthesizing Recurrence with KV Cache Compression for Efficient LLM Inference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09398
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026;LESS&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#38598;&#25104;&#19968;&#20010;&#22266;&#23450;&#23610;&#23544;&#30340;&#32531;&#23384;&#21644;&#22522;&#20110;&#39537;&#36880;&#30340;&#32531;&#23384;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20943;&#23567;&#20869;&#23384;&#21344;&#29992;&#30340;&#38382;&#39064;&#65292;&#21516;&#26102;&#20445;&#25345;&#20840;&#37096;&#26631;&#35760;&#30340;&#21487;&#26597;&#35810;&#33021;&#21147;&#65292;&#24182;&#22312;&#22810;&#31181;&#20219;&#21153;&#19978;&#26174;&#31034;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#35745;&#31639;&#22240;&#32032;&#38480;&#21046;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24191;&#27867;&#37096;&#32626;&#12290;&#26412;&#25991;&#20851;&#27880;&#20110;&#30001;&#38190;&#20540;(KV)&#32531;&#23384;&#24341;&#36215;&#30340;&#20869;&#23384;&#29942;&#39048;&#65292;&#36825;&#26159;&#19968;&#31181;&#35745;&#31639;&#24555;&#25463;&#26041;&#24335;&#65292;&#22312;&#35299;&#30721;&#36807;&#31243;&#20013;&#38656;&#35201;&#23384;&#20648;&#20808;&#21069;&#30340;KV&#23545;&#12290;&#29616;&#26377;&#30340;KV&#32531;&#23384;&#26041;&#27861;&#36890;&#36807;&#20462;&#21098;&#25110;&#39537;&#36880;&#30456;&#23545;&#19981;&#37325;&#35201;&#30340;KV&#23545;&#30340;&#22823;&#29255;&#21306;&#22495;&#65292;&#26174;&#33879;&#20943;&#23569;&#32531;&#23384;&#30340;&#20869;&#23384;&#21344;&#29992;&#65292;&#20294;&#22312;&#38656;&#35201;&#37325;&#26032;&#25910;&#38598;&#22823;&#22810;&#25968;&#21069;&#19968;&#20010;&#26631;&#35760;&#30340;&#20219;&#21153;&#20013;&#65292;&#23427;&#20204;&#30340;&#25104;&#21151;&#26377;&#38480;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LESS&#65292;&#23427;&#23558;&#19968;&#20010;&#65288;&#20960;&#20046;&#20813;&#36153;&#30340;&#65289;&#22266;&#23450;&#23610;&#23544;&#30340;&#32531;&#23384;&#19982;&#22522;&#20110;&#39537;&#36880;&#30340;&#32531;&#23384;&#26041;&#27861;&#31616;&#21333;&#22320;&#38598;&#25104;&#22312;&#19968;&#36215;&#65292;&#20197;&#20415;&#25152;&#26377;&#30340;&#26631;&#35760;&#21487;&#20197;&#22312;&#21518;&#32493;&#30340;&#35299;&#30721;&#27493;&#39588;&#20013;&#26597;&#35810;&#12290;&#23427;&#33021;&#22815;&#22312;&#26102;&#38388;&#19978;&#20445;&#30041;&#20449;&#24687;&#65292;&#22312;&#22810;&#31181;&#20219;&#21153;&#19978;&#23637;&#29616;&#20986;&#21512;&#29702;&#24615;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;LESS&#21487;&#20197;&#24110;&#21161;&#20943;&#23567;&#32531;&#23384;&#25152;&#26377;&#20869;&#23481;&#30340;&#24615;&#33021;&#24046;&#36317;&#65292;&#26377;&#26102;&#29978;&#33267;&#21487;&#20197;&#19982;&#20854;&#30456;&#21305;&#37197;&#65292;&#21516;&#26102;&#20855;&#26377;&#39640;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09398v1 Announce Type: cross Abstract: Many computational factors limit broader deployment of large language models. In this paper, we focus on a memory bottleneck imposed by the key-value (KV) cache, a computational shortcut that requires storing previous KV pairs during decoding. While existing KV cache methods approach this problem by pruning or evicting large swaths of relatively less important KV pairs to dramatically reduce the memory footprint of the cache, they can have limited success in tasks that require recollecting a majority of previous tokens. To alleviate this issue, we propose LESS, a simple integration of a (nearly free) constant sized cache with eviction-based cache methods, such that all tokens can be queried at later decoding steps. Its ability to retain information throughout time shows merit on a variety of tasks where we demonstrate LESS can help reduce the performance gap from caching everything, sometimes even matching it, all while being efficient.
&lt;/p&gt;</description></item><item><title>LL-GABR&#26159;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#24863;&#30693;&#35270;&#39057;&#36136;&#37327;&#32780;&#19981;&#26159;&#27604;&#29305;&#29575;&#24314;&#27169;QoE&#65292;&#24182;&#32467;&#21512;&#33021;&#37327;&#28040;&#32791;&#21644;&#20854;&#20182;&#25351;&#26631;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#33021;&#32791;&#30340;&#23454;&#26102;&#35270;&#39057;&#27969;&#20256;&#36755;&#12290;</title><link>https://arxiv.org/abs/2402.09392</link><description>&lt;p&gt;
LL-GABR: &#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#23454;&#29616;&#39640;&#25928;&#33021;&#32791;&#30340;&#23454;&#26102;&#35270;&#39057;&#27969;&#20256;&#36755;
&lt;/p&gt;
&lt;p&gt;
LL-GABR: Energy Efficient Live Video Streaming Using Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09392
&lt;/p&gt;
&lt;p&gt;
LL-GABR&#26159;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#24863;&#30693;&#35270;&#39057;&#36136;&#37327;&#32780;&#19981;&#26159;&#27604;&#29305;&#29575;&#24314;&#27169;QoE&#65292;&#24182;&#32467;&#21512;&#33021;&#37327;&#28040;&#32791;&#21644;&#20854;&#20182;&#25351;&#26631;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#33021;&#32791;&#30340;&#23454;&#26102;&#35270;&#39057;&#27969;&#20256;&#36755;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#33258;&#36866;&#24212;&#27604;&#29305;&#29575;&#65288;ABR&#65289;&#31639;&#27861;&#22312;&#23454;&#26102;&#35270;&#39057;&#27969;&#20256;&#36755;&#26041;&#38754;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#36890;&#36807;&#38477;&#20302;&#24310;&#36831;&#65292;&#25552;&#20379;&#26356;&#39640;&#27604;&#29305;&#29575;&#30340;&#35270;&#39057;&#65292;&#24182;&#23558;&#35270;&#39057;&#32531;&#20914;&#26102;&#38388;&#20943;&#21040;&#26368;&#23567;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#29992;&#25143;&#20307;&#39564;&#65288;QoE&#65289;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;ABR&#31639;&#27861;&#20351;&#29992;&#30340;QoE&#27169;&#22411;&#27809;&#26377;&#32771;&#34385;&#21040;&#22823;&#37096;&#20998;&#23454;&#26102;&#35270;&#39057;&#27969;&#20256;&#36755;&#23458;&#25143;&#31471;&#20351;&#29992;&#30340;&#26159;&#31227;&#21160;&#35774;&#22791;&#65292;&#22312;&#31227;&#21160;&#35774;&#22791;&#19978;&#65292;&#26356;&#39640;&#30340;&#27604;&#29305;&#29575;&#19981;&#19968;&#23450;&#24847;&#21619;&#30528;&#26356;&#39640;&#30340;&#35270;&#35273;&#24863;&#30693;&#36136;&#37327;&#12290;&#24573;&#35270;&#24863;&#30693;&#36136;&#37327;&#20250;&#23548;&#33268;&#20197;&#26356;&#39640;&#30340;&#27604;&#29305;&#29575;&#25773;&#25918;&#35270;&#39057;&#65292;&#32780;&#24182;&#27809;&#26377;&#26174;&#33879;&#25552;&#39640;&#35270;&#35273;&#24863;&#30693;&#36136;&#37327;&#65292;&#21516;&#26102;&#23545;&#20110;&#30005;&#27744;&#21463;&#38480;&#30340;&#31227;&#21160;&#35774;&#22791;&#26469;&#35828;&#20250;&#22686;&#21152;&#33021;&#37327;&#28040;&#32791;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LL-GABR&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#24863;&#30693;&#35270;&#39057;&#36136;&#37327;&#32780;&#19981;&#26159;&#27604;&#29305;&#29575;&#24314;&#27169;QoE&#65292;&#24182;&#23558;&#33021;&#37327;&#28040;&#32791;&#19982;&#20854;&#20182;&#25351;&#26631;&#32467;&#21512;&#36215;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09392v1 Announce Type: cross Abstract: Over the recent years, research and development in adaptive bitrate (ABR) algorithms for live video streaming have been successful in improving users' quality of experience (QoE) by reducing latency to near real-time levels while delivering higher bitrate videos with minimal rebuffering time. However, the QoE models used by these ABR algorithms do not take into account that a large portion of live video streaming clients use mobile devices where a higher bitrate does not necessarily translate into higher perceived quality. Ignoring perceived quality results in playing videos at higher bitrates without a significant increase in perceptual video quality and becomes a burden for battery-constrained mobile devices due to higher energy consumption. In this paper, we propose LL-GABR, a deep reinforcement learning approach that models the QoE using perceived video quality instead of bitrate and uses energy consumption along with other metrics 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;LlaSMol&#65292;&#23427;&#26159;&#19968;&#31181;&#25512;&#36827;&#21270;&#23398;&#39046;&#22495;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;&#22823;&#35268;&#27169;&#12289;&#20840;&#38754;&#12289;&#39640;&#36136;&#37327;&#30340;&#25351;&#20196;&#35843;&#20248;&#25968;&#25454;&#38598;&#26469;&#35757;&#32451;&#27169;&#22411;&#65292;LlaSMol&#22312;&#21270;&#23398;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#24615;&#33021;&#65292;&#36229;&#36807;&#20102;GPT-4&#24182;&#25509;&#36817;&#20110;&#20219;&#21153;&#29305;&#23450;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.09391</link><description>&lt;p&gt;
LlaSMol:&#21033;&#29992;&#22823;&#35268;&#27169;&#12289;&#20840;&#38754;&#12289;&#39640;&#36136;&#37327;&#30340;&#25351;&#20196;&#35843;&#20248;&#25968;&#25454;&#38598;&#25512;&#36827;&#21270;&#23398;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
LlaSMol: Advancing Large Language Models for Chemistry with a Large-Scale, Comprehensive, High-Quality Instruction Tuning Dataset
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09391
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;LlaSMol&#65292;&#23427;&#26159;&#19968;&#31181;&#25512;&#36827;&#21270;&#23398;&#39046;&#22495;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;&#22823;&#35268;&#27169;&#12289;&#20840;&#38754;&#12289;&#39640;&#36136;&#37327;&#30340;&#25351;&#20196;&#35843;&#20248;&#25968;&#25454;&#38598;&#26469;&#35757;&#32451;&#27169;&#22411;&#65292;LlaSMol&#22312;&#21270;&#23398;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#24615;&#33021;&#65292;&#36229;&#36807;&#20102;GPT-4&#24182;&#25509;&#36817;&#20110;&#20219;&#21153;&#29305;&#23450;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21270;&#23398;&#22312;&#33647;&#29289;&#30740;&#21457;&#21644;&#26448;&#26009;&#31185;&#23398;&#31561;&#35768;&#22810;&#39046;&#22495;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#23613;&#31649;&#35832;&#22914;GPT-4&#20043;&#31867;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#19978;&#23637;&#29616;&#20986;&#20102;&#38750;&#20961;&#30340;&#33021;&#21147;&#65292;&#20294;&#29616;&#26377;&#24037;&#20316;&#34920;&#26126;&#23427;&#20204;&#22312;&#21270;&#23398;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#20196;&#20154;&#22833;&#26395;&#12290;&#28982;&#32780;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#24320;&#21457;&#30340;LLM&#22312;&#19968;&#31995;&#21015;&#21270;&#23398;&#20219;&#21153;&#19978;&#21487;&#20197;&#21462;&#24471;&#38750;&#24120;&#24378;&#22823;&#30340;&#32467;&#26524;&#65292;&#22312;&#25152;&#26377;&#20219;&#21153;&#19978;&#37117;&#26174;&#33879;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;GPT-4&#65292;&#24182;&#25509;&#36817;SoTA&#20219;&#21153;&#29305;&#23450;&#27169;&#22411;&#12290;&#25105;&#20204;&#21462;&#24471;&#25104;&#21151;&#30340;&#20851;&#38190;&#26159;&#19968;&#20010;&#21517;&#20026;SMolInstruct&#30340;&#22823;&#35268;&#27169;&#12289;&#20840;&#38754;&#12289;&#39640;&#36136;&#37327;&#30340;&#25351;&#20196;&#35843;&#20248;&#25968;&#25454;&#38598;&#12290;&#23427;&#21253;&#21547;&#20102;14&#20010;&#32463;&#36807;&#31934;&#24515;&#25361;&#36873;&#30340;&#21270;&#23398;&#20219;&#21153;&#21644;&#36229;&#36807;&#19977;&#30334;&#19975;&#20010;&#39640;&#36136;&#37327;&#26679;&#26412;&#65292;&#20026;&#35757;&#32451;&#21644;&#35780;&#20272;&#21270;&#23398;LLM&#22880;&#23450;&#20102;&#22362;&#23454;&#22522;&#30784;&#12290;&#22522;&#20110;SMolInstruct&#65292;&#25105;&#20204;&#23545;&#19968;&#32452;&#24320;&#28304;LLM&#36827;&#34892;&#20102;&#24494;&#35843;&#65292;&#20854;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;Mistral ser&#26159;&#26368;&#20339;&#24615;&#33021;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09391v1 Announce Type: new Abstract: Chemistry plays a crucial role in many domains, such as drug discovery and material science. While large language models (LLMs) such as GPT-4 exhibit remarkable capabilities on natural language processing tasks, existing work shows their performance on chemistry tasks is discouragingly low. In this paper, however, we demonstrate that our developed LLMs can achieve very strong results on a comprehensive set of chemistry tasks, outperforming the most advanced GPT-4 across all the tasks by a substantial margin and approaching the SoTA task-specific models. The key to our success is a large-scale, comprehensive, high-quality dataset for instruction tuning named SMolInstruct. It contains 14 meticulously selected chemistry tasks and over three million high-quality samples, laying a solid foundation for training and evaluating LLMs for chemistry. Based on SMolInstruct, we fine-tune a set of open-source LLMs, among which, we find that Mistral ser
&lt;/p&gt;</description></item><item><title>HGOT&#26159;&#19968;&#31181;&#29992;&#20110;&#26816;&#32034;&#22686;&#24378;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#20107;&#23454;&#24615;&#35780;&#20272;&#30340;&#20998;&#23618;&#24605;&#32500;&#22270;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35268;&#21010;&#33021;&#21147;&#21644;&#24605;&#32500;&#36136;&#37327;&#35780;&#20272;&#25351;&#26631;&#26469;&#25552;&#39640;&#30456;&#20851;&#27573;&#33853;&#30340;&#26816;&#32034;&#21644;&#31572;&#26696;&#36873;&#25321;&#12290;</title><link>https://arxiv.org/abs/2402.09390</link><description>&lt;p&gt;
HGOT: &#29992;&#20110;&#26816;&#32034;&#22686;&#24378;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#20107;&#23454;&#24615;&#35780;&#20272;&#30340;&#20998;&#23618;&#24605;&#32500;&#22270;
&lt;/p&gt;
&lt;p&gt;
HGOT: Hierarchical Graph of Thoughts for Retrieval-Augmented In-Context Learning in Factuality Evaluation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09390
&lt;/p&gt;
&lt;p&gt;
HGOT&#26159;&#19968;&#31181;&#29992;&#20110;&#26816;&#32034;&#22686;&#24378;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#20107;&#23454;&#24615;&#35780;&#20272;&#30340;&#20998;&#23618;&#24605;&#32500;&#22270;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35268;&#21010;&#33021;&#21147;&#21644;&#24605;&#32500;&#36136;&#37327;&#35780;&#20272;&#25351;&#26631;&#26469;&#25552;&#39640;&#30456;&#20851;&#27573;&#33853;&#30340;&#26816;&#32034;&#21644;&#31572;&#26696;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#65292;&#20107;&#23454;&#24615;&#21644;&#24187;&#35273;&#30340;&#20542;&#21521;&#24341;&#21457;&#20102;&#37325;&#22823;&#20851;&#20999;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#22312;&#26816;&#32034;&#22686;&#24378;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20998;&#23618;&#24605;&#32500;&#22270;&#65288;HGOT&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#32467;&#26500;&#21270;&#30340;&#12289;&#22810;&#23618;&#27425;&#30340;&#22270;&#24418;&#26041;&#27861;&#65292;&#26088;&#22312;&#22686;&#24378;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#36807;&#31243;&#20013;&#30456;&#20851;&#27573;&#33853;&#30340;&#26816;&#32034;&#12290;&#35813;&#26694;&#26550;&#21033;&#29992;LLMs&#30340;&#36880;&#28176;&#35268;&#21010;&#33021;&#21147;&#65292;&#37319;&#29992;&#20998;&#32780;&#27835;&#20043;&#30340;&#31574;&#30053;&#23558;&#22797;&#26434;&#26597;&#35810;&#20998;&#35299;&#20026;&#21487;&#22788;&#29702;&#30340;&#23376;&#26597;&#35810;&#12290;&#23427;&#36890;&#36807;&#24341;&#20837;&#26368;&#36817;&#25552;&#20986;&#30340;&#24341;&#25991;&#22238;&#24518;&#21644;&#31934;&#30830;&#24230;&#25351;&#26631;&#26469;&#35780;&#20272;&#24605;&#32500;&#36136;&#37327;&#65292;&#23558;&#31572;&#26696;&#30340;&#21487;&#20449;&#24230;&#19982;&#24605;&#32500;&#30340;&#36136;&#37327;&#20869;&#22312;&#22320;&#32852;&#31995;&#36215;&#26469;&#65292;&#20174;&#32780;&#25913;&#36827;&#20102;&#33258;&#27965;&#24615;&#22810;&#25968;&#25237;&#31080;&#30340;&#31572;&#26696;&#36873;&#25321;&#26041;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#24341;&#20837;&#20102;&#19968;&#20010;&#21152;&#26435;&#31995;&#32479;&#65292;&#22312;&#22810;&#25968;&#25237;&#31080;&#20013;&#20248;&#20808;&#32771;&#34385;&#31572;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09390v1 Announce Type: new Abstract: With the widespread adoption of large language models (LLMs) in numerous applications, the challenge of factuality and the propensity for hallucinations raises significant concerns. To address this issue, particularly in retrieval-augmented in-context learning, we introduce the hierarchical graph of thoughts (HGOT), a structured, multi-layered graph approach designed to enhance the retrieval of pertinent passages during in-context learning. The framework utilizes the emergent planning capabilities of LLMs, employing the divide-and-conquer strategy to break down complex queries into manageable sub-queries. It refines self-consistency majority voting for answer selection, which incorporates the recently proposed citation recall and precision metrics to assess the quality of thoughts, linking an answer's credibility intrinsically to the thought's quality. This methodology introduces a weighted system in majority voting, prioritizing answers 
&lt;/p&gt;</description></item><item><title>&#22312;&#37096;&#20998;&#21487;&#35266;&#27979;&#38382;&#39064;&#39046;&#22495;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29109;&#27491;&#21017;&#21270;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#35268;&#21010;&#22120;&#65292;&#22312;&#35268;&#21010;&#21644;&#30446;&#26631;&#25512;&#29702;&#20013;&#36890;&#36807;&#40723;&#21169;&#31574;&#30053;&#19981;&#25215;&#35834;&#19968;&#20010;&#21333;&#19968;&#30340;&#21160;&#20316;&#26469;&#25552;&#39640;&#40065;&#26834;&#24615;&#21644;&#30446;&#26631;&#25512;&#29702;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.09388</link><description>&lt;p&gt;
&#29109;&#27491;&#21017;&#21270;&#30340;&#22522;&#20110;&#28857;&#30340;&#20215;&#20540;&#36845;&#20195;
&lt;/p&gt;
&lt;p&gt;
Entropy-regularized Point-based Value Iteration
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09388
&lt;/p&gt;
&lt;p&gt;
&#22312;&#37096;&#20998;&#21487;&#35266;&#27979;&#38382;&#39064;&#39046;&#22495;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29109;&#27491;&#21017;&#21270;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#35268;&#21010;&#22120;&#65292;&#22312;&#35268;&#21010;&#21644;&#30446;&#26631;&#25512;&#29702;&#20013;&#36890;&#36807;&#40723;&#21169;&#31574;&#30053;&#19981;&#25215;&#35834;&#19968;&#20010;&#21333;&#19968;&#30340;&#21160;&#20316;&#26469;&#25552;&#39640;&#40065;&#26834;&#24615;&#21644;&#30446;&#26631;&#25512;&#29702;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38754;&#23545;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#21644;&#30446;&#26631;&#19981;&#30830;&#23450;&#24615;&#65292;&#37096;&#20998;&#21487;&#35266;&#27979;&#38382;&#39064;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#35268;&#21010;&#22120;&#24517;&#39035;&#36866;&#24212;&#20004;&#32773;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#31867;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#21487;&#33021;&#23548;&#33268;&#22522;&#20110;&#27169;&#22411;&#30340;&#35268;&#21010;&#22120;&#21464;&#24471;&#33030;&#24369;&#65292;&#22240;&#20026;&#23427;&#20204;&#20381;&#36182;&#20110;&#31934;&#30830;&#30340;&#27169;&#22411;&#24182;&#20542;&#21521;&#20110;&#25215;&#35834;&#19968;&#20010;&#21333;&#19968;&#30340;&#26368;&#20248;&#34892;&#20026;&#12290;&#21463;&#21040;&#26080;&#27169;&#22411;&#35774;&#32622;&#20013;&#30340;&#32467;&#26524;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29109;&#27491;&#21017;&#21270;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#35268;&#21010;&#22120;&#26469;&#35299;&#20915;&#37096;&#20998;&#21487;&#35266;&#27979;&#38382;&#39064;&#12290;&#29109;&#27491;&#21017;&#21270;&#36890;&#36807;&#40723;&#21169;&#31574;&#30053;&#22312;&#35268;&#21010;&#21644;&#30446;&#26631;&#25512;&#29702;&#20013;&#23613;&#21487;&#33021;&#19981;&#25215;&#35834;&#19968;&#20010;&#21333;&#19968;&#30340;&#21160;&#20316;&#26469;&#20419;&#36827;&#31574;&#30053;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#19977;&#20010;&#38382;&#39064;&#39046;&#22495;&#20013;&#29109;&#27491;&#21017;&#21270;&#31574;&#30053;&#30340;&#40065;&#26834;&#24615;&#21644;&#30446;&#26631;&#25512;&#29702;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#24314;&#27169;&#38169;&#35823;&#21644;&#30446;&#26631;&#25512;&#29702;&#20934;&#30830;&#24615;&#26041;&#38754;&#65292;&#29109;&#27491;&#21017;&#21270;&#31574;&#30053;&#20248;&#20110;&#38750;&#29109;&#27491;&#21017;&#21270;&#30340;&#22522;&#20934;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09388v1 Announce Type: new Abstract: Model-based planners for partially observable problems must accommodate both model uncertainty during planning and goal uncertainty during objective inference. However, model-based planners may be brittle under these types of uncertainty because they rely on an exact model and tend to commit to a single optimal behavior. Inspired by results in the model-free setting, we propose an entropy-regularized model-based planner for partially observable problems. Entropy regularization promotes policy robustness for planning and objective inference by encouraging policies to be no more committed to a single action than necessary. We evaluate the robustness and objective inference performance of entropy-regularized policies in three problem domains. Our results show that entropy-regularized policies outperform non-entropy-regularized baselines in terms of higher expected returns under modeling errors and higher accuracy during objective inference.
&lt;/p&gt;</description></item><item><title>&#19968;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#31639;&#27861;&#36741;&#21161;&#20915;&#31574;&#20013;&#65292;&#22914;&#20309;&#35774;&#35745;&#26368;&#20248;&#30340;&#39044;&#27979;&#31639;&#27861;&#21644;&#22996;&#25176;&#35268;&#21017;&#12290;&#20851;&#38190;&#21457;&#29616;&#21253;&#25324;&#65306;&#22996;&#25176;&#30340;&#26368;&#20248;&#24615;&#19982;&#22996;&#25176;&#20154;&#26159;&#21542;&#20250;&#20570;&#20986;&#19982;&#20195;&#29702;&#20154;&#30456;&#21516;&#30340;&#20915;&#31574;&#26377;&#20851;&#12289;&#26368;&#20855;&#20449;&#24687;&#37327;&#30340;&#31639;&#27861;&#19981;&#19968;&#23450;&#26159;&#26368;&#20248;&#30340;&#12289;&#24120;&#35265;&#30340;&#31639;&#27861;&#38480;&#21046;&#20250;&#38477;&#20302;&#20915;&#31574;&#36136;&#37327;&#12290;</title><link>https://arxiv.org/abs/2402.09384</link><description>&lt;p&gt;
&#35828;&#26381;&#12289;&#22996;&#25176;&#21644;&#31169;&#26377;&#20449;&#24687;&#22312;&#31639;&#27861;&#36741;&#21161;&#20915;&#31574;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Persuasion, Delegation, and Private Information in Algorithm-Assisted Decisions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09384
&lt;/p&gt;
&lt;p&gt;
&#19968;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#31639;&#27861;&#36741;&#21161;&#20915;&#31574;&#20013;&#65292;&#22914;&#20309;&#35774;&#35745;&#26368;&#20248;&#30340;&#39044;&#27979;&#31639;&#27861;&#21644;&#22996;&#25176;&#35268;&#21017;&#12290;&#20851;&#38190;&#21457;&#29616;&#21253;&#25324;&#65306;&#22996;&#25176;&#30340;&#26368;&#20248;&#24615;&#19982;&#22996;&#25176;&#20154;&#26159;&#21542;&#20250;&#20570;&#20986;&#19982;&#20195;&#29702;&#20154;&#30456;&#21516;&#30340;&#20915;&#31574;&#26377;&#20851;&#12289;&#26368;&#20855;&#20449;&#24687;&#37327;&#30340;&#31639;&#27861;&#19981;&#19968;&#23450;&#26159;&#26368;&#20248;&#30340;&#12289;&#24120;&#35265;&#30340;&#31639;&#27861;&#38480;&#21046;&#20250;&#38477;&#20302;&#20915;&#31574;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#20301;&#22996;&#25176;&#20154;&#35774;&#35745;&#20102;&#19968;&#20010;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#29983;&#25104;&#19968;&#20010;&#20844;&#24320;&#21487;&#35265;&#30340;&#20108;&#36827;&#21046;&#29366;&#24577;&#39044;&#27979;&#12290;&#22905;&#24517;&#39035;&#20915;&#23450;&#26159;&#26681;&#25454;&#39044;&#27979;&#30452;&#25509;&#34892;&#21160;&#36824;&#26159;&#23558;&#20915;&#31574;&#22996;&#25176;&#32473;&#19968;&#20010;&#20195;&#29702;&#20154;&#65292;&#35813;&#20195;&#29702;&#20154;&#20855;&#26377;&#31169;&#26377;&#20449;&#24687;&#20294;&#21487;&#33021;&#23384;&#22312;&#19981;&#23545;&#40784;&#38382;&#39064;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#36825;&#31181;&#29615;&#22659;&#19979;&#39044;&#27979;&#31639;&#27861;&#21644;&#22996;&#25176;&#35268;&#21017;&#30340;&#26368;&#20248;&#35774;&#35745;&#12290;&#24471;&#20986;&#20102;&#19977;&#20010;&#20851;&#38190;&#21457;&#29616;&#65306;(1)&#21482;&#26377;&#24403;&#22996;&#25176;&#20154;&#22312;&#35266;&#23519;&#21040;&#20195;&#29702;&#20154;&#30340;&#20449;&#24687;&#26102;&#20250;&#20570;&#20986;&#19982;&#20195;&#29702;&#20154;&#30456;&#21516;&#30340;&#20108;&#36827;&#21046;&#20915;&#31574;&#26102;&#65292;&#22996;&#25176;&#25165;&#26159;&#26368;&#20248;&#30340;&#12290;(2)&#25552;&#20379;&#26368;&#20855;&#20449;&#24687;&#37327;&#30340;&#31639;&#27861;&#21487;&#33021;&#24182;&#19981;&#26159;&#26368;&#20248;&#30340;&#65292;&#21363;&#20351;&#22996;&#25176;&#20154;&#21487;&#20197;&#26681;&#25454;&#31639;&#27861;&#30340;&#39044;&#27979;&#26469;&#34892;&#21160;&#12290;&#30456;&#21453;&#65292;&#26368;&#20248;&#31639;&#27861;&#21487;&#33021;&#25552;&#20379;&#26356;&#22810;&#20851;&#20110;&#19968;&#20010;&#29366;&#24577;&#30340;&#20449;&#24687;&#65292;&#24182;&#38480;&#21046;&#20851;&#20110;&#21478;&#19968;&#20010;&#29366;&#24577;&#30340;&#20449;&#24687;&#12290;(3)&#22312;&#27809;&#26377;&#23436;&#32654;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#30340;&#24773;&#20917;&#19979;&#65292;&#24120;&#35265;&#30340;&#23545;&#31639;&#27861;&#30340;&#38480;&#21046;&#65292;&#22914;&#20445;&#25345;"&#20154;&#26426;&#21512;&#20316;"&#25110;&#35201;&#27714;&#26368;&#22823;&#39044;&#27979;&#31934;&#24230;&#65292;&#20250;&#20005;&#37325;&#38477;&#20302;&#20915;&#31574;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09384v1 Announce Type: cross Abstract: A principal designs an algorithm that generates a publicly observable prediction of a binary state. She must decide whether to act directly based on the prediction or to delegate the decision to an agent with private information but potential misalignment. We study the optimal design of the prediction algorithm and the delegation rule in such environments. Three key findings emerge: (1) Delegation is optimal if and only if the principal would make the same binary decision as the agent had she observed the agent's information. (2) Providing the most informative algorithm may be suboptimal even if the principal can act on the algorithm's prediction. Instead, the optimal algorithm may provide more information about one state and restrict information about the other. (3) Common restrictions on algorithms, such as keeping a "human-in-the-loop" or requiring maximal prediction accuracy, strictly worsen decision quality in the absence of perfec
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#22522;&#20110;RibFrac&#25361;&#25112;&#36187;&#30340;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#65292;&#21487;&#20197;&#20174;CT&#25195;&#25551;&#20013;&#23454;&#29616;&#32907;&#39592;&#39592;&#25240;&#30340;&#23454;&#20363;&#20998;&#21106;&#21644;&#20998;&#31867;&#12290;&#30740;&#31350;&#36890;&#36807;&#25552;&#20379;&#19968;&#20010;&#21253;&#21547;&#22823;&#37327;&#26631;&#27880;&#25968;&#25454;&#21644;&#35780;&#20272;&#25351;&#26631;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#35299;&#20915;&#20102;&#32907;&#39592;&#39592;&#25240;&#26816;&#27979;&#39046;&#22495;&#32570;&#20047;&#25968;&#25454;&#21644;&#35780;&#20272;&#30340;&#38382;&#39064;&#12290;MICCAI 2020&#25361;&#25112;&#36187;&#26399;&#38388;&#65292;&#19971;&#20010;&#22242;&#38431;&#30340;&#26041;&#27861;&#22312;&#23454;&#20363;&#20998;&#21106;&#21644;&#20998;&#31867;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>https://arxiv.org/abs/2402.09372</link><description>&lt;p&gt;
&#20174;CT&#22270;&#20687;&#20013;&#36827;&#34892;&#28145;&#24230;&#32907;&#39592;&#39592;&#25240;&#23454;&#20363;&#20998;&#21106;&#21644;&#20998;&#31867;&#65306;&#22522;&#20110;RibFrac&#25361;&#25112;&#36187;
&lt;/p&gt;
&lt;p&gt;
Deep Rib Fracture Instance Segmentation and Classification from CT on the RibFrac Challenge
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09372
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#22522;&#20110;RibFrac&#25361;&#25112;&#36187;&#30340;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#65292;&#21487;&#20197;&#20174;CT&#25195;&#25551;&#20013;&#23454;&#29616;&#32907;&#39592;&#39592;&#25240;&#30340;&#23454;&#20363;&#20998;&#21106;&#21644;&#20998;&#31867;&#12290;&#30740;&#31350;&#36890;&#36807;&#25552;&#20379;&#19968;&#20010;&#21253;&#21547;&#22823;&#37327;&#26631;&#27880;&#25968;&#25454;&#21644;&#35780;&#20272;&#25351;&#26631;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#35299;&#20915;&#20102;&#32907;&#39592;&#39592;&#25240;&#26816;&#27979;&#39046;&#22495;&#32570;&#20047;&#25968;&#25454;&#21644;&#35780;&#20272;&#30340;&#38382;&#39064;&#12290;MICCAI 2020&#25361;&#25112;&#36187;&#26399;&#38388;&#65292;&#19971;&#20010;&#22242;&#38431;&#30340;&#26041;&#27861;&#22312;&#23454;&#20363;&#20998;&#21106;&#21644;&#20998;&#31867;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32907;&#39592;&#39592;&#25240;&#26159;&#19968;&#31181;&#24120;&#35265;&#19988;&#21487;&#33021;&#20005;&#37325;&#30340;&#25439;&#20260;&#65292;&#26816;&#27979;&#32907;&#39592;&#39592;&#25240;&#22312;CT&#25195;&#25551;&#20013;&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#19988;&#32791;&#26102;&#12290;&#34429;&#28982;&#24050;&#32463;&#26377;&#19968;&#20123;&#21162;&#21147;&#26469;&#35299;&#20915;&#36825;&#20010;&#39046;&#22495;&#30340;&#38382;&#39064;&#65292;&#20294;&#32570;&#23569;&#22823;&#35268;&#27169;&#26631;&#27880;&#30340;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#22522;&#20934;&#22952;&#30861;&#20102;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#30340;&#21457;&#23637;&#21644;&#39564;&#35777;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#24341;&#20837;&#20102;RibFrac&#25361;&#25112;&#36187;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#21253;&#21547;&#36229;&#36807;5,000&#20010;&#32907;&#39592;&#39592;&#25240;&#23454;&#20363;&#21644;660&#20010;CT&#25195;&#25551;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#27599;&#20010;&#23454;&#20363;&#30340;&#20307;&#32032;&#32423;&#21035;&#23454;&#20363;&#25513;&#30721;&#26631;&#27880;&#20197;&#21450;&#38024;&#23545;&#22235;&#20010;&#20020;&#24202;&#31867;&#21035;&#65288;&#38145;&#38381;&#39592;&#25240;&#12289;&#38750;&#31227;&#20301;&#39592;&#25240;&#12289;&#31227;&#20301;&#39592;&#25240;&#25110;&#20998;&#27573;&#39592;&#25240;&#65289;&#30340;&#35786;&#26029;&#26631;&#31614;&#12290;&#35813;&#25361;&#25112;&#21253;&#25324;&#20004;&#20010;&#20219;&#21153;&#65306;&#19968;&#20010;&#26159;&#26816;&#27979;&#65288;&#23454;&#20363;&#20998;&#21106;&#65289;&#20219;&#21153;&#65292;&#35780;&#20272;&#25351;&#26631;&#20026;FROC&#39118;&#26684;&#65292;&#21478;&#19968;&#20010;&#26159;&#20998;&#31867;&#20219;&#21153;&#65292;&#35780;&#20272;&#25351;&#26631;&#20026;F1&#39118;&#26684;&#12290;&#22312;MICCAI 2020&#25361;&#25112;&#26399;&#38388;&#65292;&#23545;243&#20010;&#32467;&#26524;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#24182;&#36992;&#35831;&#20102;&#19971;&#20010;&#22242;&#38431;&#21442;&#21152;&#25361;&#25112;&#24635;&#32467;&#12290;&#20998;&#26512;&#32467;&#26524;&#34920;&#26126;&#65292;&#20960;&#20010;&#39030;&#23574;&#22242;&#38431;&#30340;&#26041;&#27861;&#22312;&#32907;&#39592;&#39592;&#25240;&#23454;&#20363;&#20998;&#21106;&#21644;&#20998;&#31867;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09372v1 Announce Type: cross Abstract: Rib fractures are a common and potentially severe injury that can be challenging and labor-intensive to detect in CT scans. While there have been efforts to address this field, the lack of large-scale annotated datasets and evaluation benchmarks has hindered the development and validation of deep learning algorithms. To address this issue, the RibFrac Challenge was introduced, providing a benchmark dataset of over 5,000 rib fractures from 660 CT scans, with voxel-level instance mask annotations and diagnosis labels for four clinical categories (buckle, nondisplaced, displaced, or segmental). The challenge includes two tracks: a detection (instance segmentation) track evaluated by an FROC-style metric and a classification track evaluated by an F1-style metric. During the MICCAI 2020 challenge period, 243 results were evaluated, and seven teams were invited to participate in the challenge summary. The analysis revealed that several top ri
&lt;/p&gt;</description></item><item><title>Transformers&#22312;&#29305;&#23450;&#32452;&#21512;&#30340;&#25968;&#25454;&#26684;&#24335;&#21644;&#20301;&#32622;&#32534;&#30721;&#30340;&#24773;&#20917;&#19979;&#21487;&#20197;&#23454;&#29616;&#38271;&#24230;&#30340;&#27867;&#21270;&#65292;&#20294;&#20173;&#28982;&#23384;&#22312;&#33030;&#24369;&#24615;&#21644;&#22823;&#37327;&#26041;&#24046;&#12290;</title><link>https://arxiv.org/abs/2402.09371</link><description>&lt;p&gt;
Transformers&#21487;&#20197;&#23454;&#29616;&#38271;&#24230;&#30340;&#27867;&#21270;&#65292;&#20294;&#24182;&#19981;&#31283;&#20581;
&lt;/p&gt;
&lt;p&gt;
Transformers Can Achieve Length Generalization But Not Robustly
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09371
&lt;/p&gt;
&lt;p&gt;
Transformers&#22312;&#29305;&#23450;&#32452;&#21512;&#30340;&#25968;&#25454;&#26684;&#24335;&#21644;&#20301;&#32622;&#32534;&#30721;&#30340;&#24773;&#20917;&#19979;&#21487;&#20197;&#23454;&#29616;&#38271;&#24230;&#30340;&#27867;&#21270;&#65292;&#20294;&#20173;&#28982;&#23384;&#22312;&#33030;&#24369;&#24615;&#21644;&#22823;&#37327;&#26041;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#24230;&#30340;&#27867;&#21270;&#65292;&#21363;&#20174;&#36739;&#30701;&#30340;&#35757;&#32451;&#24207;&#21015;&#25512;&#24191;&#21040;&#36739;&#38271;&#30340;&#27979;&#35797;&#24207;&#21015;&#30340;&#33021;&#21147;&#65292;&#23545;&#20110;&#35821;&#35328;&#27169;&#22411;&#26469;&#35828;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#25361;&#25112;&#12290;&#21363;&#20351;&#26159;&#22788;&#29702;&#30456;&#23545;&#31616;&#21333;&#20219;&#21153;&#30340;&#22823;&#35268;&#27169;Transformer&#20063;&#23384;&#22312;&#36825;&#20010;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#20004;&#20010;&#25972;&#25968;&#30456;&#21152;&#30340;&#20219;&#21153;&#26469;&#27979;&#35797;Transformer&#30340;&#38271;&#24230;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#38271;&#24230;&#27867;&#21270;&#30340;&#25104;&#21151;&#19982;&#25968;&#25454;&#26684;&#24335;&#21644;&#20301;&#32622;&#32534;&#30721;&#30340;&#31867;&#22411;&#23494;&#20999;&#30456;&#20851;&#12290;&#36890;&#36807;&#27491;&#30830;&#32452;&#21512;&#25968;&#25454;&#26684;&#24335;&#21644;&#20301;&#32622;&#32534;&#30721;&#65292;&#25105;&#20204;&#39318;&#27425;&#23637;&#31034;&#20986;&#26631;&#20934;&#30340;Transformer&#21487;&#20197;&#25512;&#24191;&#21040;&#36755;&#20837;&#38271;&#24230;&#30340;2.5&#20493;&#30340;&#24207;&#21015;&#38271;&#24230;&#12290;&#28982;&#32780;&#65292;&#19982;&#20869;&#20998;&#24067;&#27867;&#21270;&#19981;&#21516;&#65292;&#38271;&#24230;&#27867;&#21270;&#20173;&#28982;&#24456;&#33030;&#24369;&#65292;&#21463;&#21040;&#38543;&#26426;&#26435;&#37325;&#21021;&#22987;&#21270;&#21644;&#35757;&#32451;&#25968;&#25454;&#39034;&#24207;&#31561;&#22240;&#32032;&#30340;&#26174;&#33879;&#24433;&#21709;&#65292;&#23548;&#33268;&#19981;&#21516;&#38543;&#26426;&#31181;&#23376;&#20043;&#38388;&#23384;&#22312;&#36739;&#22823;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09371v1 Announce Type: cross Abstract: Length generalization, defined as the ability to extrapolate from shorter training sequences to longer test ones, is a significant challenge for language models. This issue persists even with large-scale Transformers handling relatively straightforward tasks. In this paper, we test the Transformer's ability of length generalization using the task of addition of two integers. We show that the success of length generalization is intricately linked to the data format and the type of position encoding. Using the right combination of data format and position encodings, we show for the first time that standard Transformers can extrapolate to a sequence length that is 2.5x the input length. Nevertheless, unlike in-distribution generalization, length generalization remains fragile, significantly influenced by factors like random weight initialization and training data order, leading to large variances across different random seeds.
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#31181;&#20266;&#38543;&#26426;&#32416;&#38169;&#30721;&#65292;&#23427;&#23545;&#26367;&#25442;&#21644;&#21024;&#38500;&#38169;&#35823;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#24182;&#19988;&#21487;&#20197;&#39640;&#25928;&#35299;&#30721;&#12290;&#25105;&#20204;&#36824;&#20351;&#29992;&#20266;&#38543;&#26426;&#30721;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#35821;&#35328;&#27169;&#22411;&#36755;&#20986;&#36827;&#34892;&#27700;&#21360;&#22788;&#29702;&#30340;&#26041;&#26696;&#65292;&#35813;&#26041;&#26696;&#23545;&#35009;&#21098;&#21644;&#38543;&#26426;&#26367;&#25442;&#12289;&#21024;&#38500;&#20855;&#26377;&#24658;&#23450;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.09370</link><description>&lt;p&gt;
&#20266;&#38543;&#26426;&#32416;&#38169;&#30721;
&lt;/p&gt;
&lt;p&gt;
Pseudorandom Error-Correcting Codes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09370
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#31181;&#20266;&#38543;&#26426;&#32416;&#38169;&#30721;&#65292;&#23427;&#23545;&#26367;&#25442;&#21644;&#21024;&#38500;&#38169;&#35823;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#24182;&#19988;&#21487;&#20197;&#39640;&#25928;&#35299;&#30721;&#12290;&#25105;&#20204;&#36824;&#20351;&#29992;&#20266;&#38543;&#26426;&#30721;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#35821;&#35328;&#27169;&#22411;&#36755;&#20986;&#36827;&#34892;&#27700;&#21360;&#22788;&#29702;&#30340;&#26041;&#26696;&#65292;&#35813;&#26041;&#26696;&#23545;&#35009;&#21098;&#21644;&#38543;&#26426;&#26367;&#25442;&#12289;&#21024;&#38500;&#20855;&#26377;&#24658;&#23450;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#26500;&#24314;&#20102;&#20266;&#38543;&#26426;&#32416;&#38169;&#30721;&#65288;&#25110;&#31616;&#31216;&#20026;&#20266;&#38543;&#26426;&#30721;&#65289;&#65292;&#23427;&#20204;&#26159;&#20855;&#26377;&#20197;&#19979;&#29305;&#24615;&#30340;&#32416;&#38169;&#30721;&#65306;&#23545;&#20110;&#20219;&#20309;&#35745;&#31639;&#21463;&#38480;&#30340;&#23545;&#25163;&#26469;&#35828;&#65292;&#20219;&#24847;&#22810;&#20010;&#32534;&#30721;&#35789;&#37117;&#26159;&#20266;&#38543;&#26426;&#30340;&#12290;&#36890;&#36807;&#35299;&#30721;&#23494;&#38053;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#32416;&#27491;&#26377;&#38169;&#35823;&#30340;&#32534;&#30721;&#35789;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#23545;&#26367;&#25442;&#38169;&#35823;&#21644;&#21024;&#38500;&#38169;&#35823;&#20855;&#26377;&#24378;&#40065;&#26834;&#24615;&#30340;&#20266;&#38543;&#26426;&#30721;&#65292;&#20854;&#20013;&#20266;&#38543;&#26426;&#24615;&#22522;&#20110;&#26631;&#20934;&#23494;&#30721;&#23398;&#20551;&#35774;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#20266;&#38543;&#26426;&#24615;&#22522;&#20110;LPN&#38382;&#39064;&#30340;$2^{O(\sqrt{n})}$&#22256;&#38590;&#31243;&#24230;&#65292;&#25110;&#32773;&#22522;&#20110;LPN&#38382;&#39064;&#21644;&#20302;&#23494;&#24230;&#19979;&#30340;&#25554;&#20837;&#24322;&#25110;&#38382;&#39064;&#30340;&#22810;&#39033;&#24335;&#22256;&#38590;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09370v1 Announce Type: cross Abstract: We construct pseudorandom error-correcting codes (or simply pseudorandom codes), which are error-correcting codes with the property that any polynomial number of codewords are pseudorandom to any computationally-bounded adversary. Efficient decoding of corrupted codewords is possible with the help of a decoding key.   We build pseudorandom codes that are robust to substitution and deletion errors, where pseudorandomness rests on standard cryptographic assumptions. Specifically, pseudorandomness is based on either $2^{O(\sqrt{n})}$-hardness of LPN, or polynomial hardness of LPN and the planted XOR problem at low density.   As our primary application of pseudorandom codes, we present an undetectable watermarking scheme for outputs of language models that is robust to cropping and a constant rate of random substitutions and deletions. The watermark is undetectable in the sense that any number of samples of watermarked text are computationa
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#35270;&#39057;&#23450;&#21046;&#25193;&#25955;&#65288;VCD&#65289;&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#20027;&#39064;&#36523;&#20221;&#21487;&#25511;&#35270;&#39057;&#29983;&#25104;&#26694;&#26550;&#65292;&#36890;&#36807;&#25351;&#23450;&#30340;&#22270;&#20687;&#23450;&#20041;&#20027;&#39064;ID&#65292;&#24182;&#21152;&#24378;&#36523;&#20221;&#20449;&#24687;&#25552;&#21462;&#21644;&#36880;&#24103;&#30456;&#20851;&#24615;&#27880;&#20837;&#65292;&#23454;&#29616;&#20102;&#31283;&#23450;&#19988;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20445;&#25345;&#36523;&#20221;&#19968;&#33268;&#24615;&#30340;&#35270;&#39057;&#36755;&#20986;&#12290;</title><link>https://arxiv.org/abs/2402.09368</link><description>&lt;p&gt;
Magic-Me: &#29305;&#23450;&#36523;&#20221;&#35270;&#39057;&#23450;&#21046;&#25193;&#25955;
&lt;/p&gt;
&lt;p&gt;
Magic-Me: Identity-Specific Video Customized Diffusion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09368
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#35270;&#39057;&#23450;&#21046;&#25193;&#25955;&#65288;VCD&#65289;&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#20027;&#39064;&#36523;&#20221;&#21487;&#25511;&#35270;&#39057;&#29983;&#25104;&#26694;&#26550;&#65292;&#36890;&#36807;&#25351;&#23450;&#30340;&#22270;&#20687;&#23450;&#20041;&#20027;&#39064;ID&#65292;&#24182;&#21152;&#24378;&#36523;&#20221;&#20449;&#24687;&#25552;&#21462;&#21644;&#36880;&#24103;&#30456;&#20851;&#24615;&#27880;&#20837;&#65292;&#23454;&#29616;&#20102;&#31283;&#23450;&#19988;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20445;&#25345;&#36523;&#20221;&#19968;&#33268;&#24615;&#30340;&#35270;&#39057;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29983;&#25104;&#27169;&#22411;&#39046;&#22495;&#65292;&#20026;&#29305;&#23450;&#36523;&#20221;&#65288;ID&#65289;&#21019;&#24314;&#20869;&#23481;&#24050;&#32463;&#24341;&#36215;&#20102;&#24456;&#22823;&#30340;&#20852;&#36259;&#12290;&#22312;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#65288;T2I&#65289;&#39046;&#22495;&#20013;&#65292;&#20197;&#20027;&#39064;&#39537;&#21160;&#30340;&#20869;&#23481;&#29983;&#25104;&#24050;&#32463;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#36827;&#23637;&#65292;&#20351;&#24471;&#22270;&#20687;&#20013;&#30340;ID&#21487;&#25511;&#12290;&#28982;&#32780;&#65292;&#23558;&#20854;&#25193;&#23637;&#21040;&#35270;&#39057;&#29983;&#25104;&#39046;&#22495;&#36824;&#27809;&#26377;&#24471;&#21040;&#24456;&#22909;&#30340;&#25506;&#32034;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#20027;&#39064;&#36523;&#20221;&#21487;&#25511;&#35270;&#39057;&#29983;&#25104;&#26694;&#26550;&#65292;&#31216;&#20026;&#35270;&#39057;&#23450;&#21046;&#25193;&#25955;&#65288;VCD&#65289;&#12290;&#36890;&#36807;&#20960;&#20010;&#25351;&#23450;&#30340;&#22270;&#20687;&#23450;&#20041;&#19968;&#20010;&#29305;&#23450;&#30340;&#20027;&#39064;ID&#65292;VCD&#21152;&#24378;&#20102;&#36523;&#20221;&#20449;&#24687;&#30340;&#25552;&#21462;&#65292;&#24182;&#22312;&#21021;&#22987;&#21270;&#38454;&#27573;&#27880;&#20837;&#36880;&#24103;&#30456;&#20851;&#24615;&#65292;&#20174;&#32780;&#20351;&#24471;&#35270;&#39057;&#36755;&#20986;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20445;&#25345;&#20102;&#36523;&#20221;&#30340;&#19968;&#33268;&#24615;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#20010;&#23545;&#20110;&#39640;&#36136;&#37327;&#36523;&#20221;&#20445;&#30041;&#33267;&#20851;&#37325;&#35201;&#30340;&#21019;&#26032;&#32452;&#20214;&#65306;1&#65289;&#20351;&#29992;prompt-to-segmentation&#23545;&#36523;&#20221;&#36827;&#34892;&#35009;&#21098;&#35757;&#32451;&#30340;ID&#27169;&#22359;&#65292;&#20197;&#35299;&#24320;ID&#20449;&#24687;&#21644;&#32972;&#26223;&#22122;&#22768;&#30340;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09368v1 Announce Type: cross Abstract: Creating content for a specific identity (ID) has shown significant interest in the field of generative models. In the field of text-to-image generation (T2I), subject-driven content generation has achieved great progress with the ID in the images controllable. However, extending it to video generation is not well explored. In this work, we propose a simple yet effective subject identity controllable video generation framework, termed Video Custom Diffusion (VCD). With a specified subject ID defined by a few images, VCD reinforces the identity information extraction and injects frame-wise correlation at the initialization stage for stable video outputs with identity preserved to a large extent. To achieve this, we propose three novel components that are essential for high-quality ID preservation: 1) an ID module trained with the cropped identity by prompt-to-segmentation to disentangle the ID information and the background noise for mor
&lt;/p&gt;</description></item><item><title>HiRE&#26159;&#19968;&#31181;&#29992;&#20110;&#39640;&#25928;&#30340;LLM&#25512;&#29702;&#30340;&#39640;&#21484;&#22238;&#29575;&#30340;&#36817;&#20284;Top-k&#20272;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#21387;&#32553;&#26041;&#26696;&#20943;&#23569;&#20102;&#27169;&#22411;&#21442;&#25968;&#20256;&#36755;&#21644;&#24310;&#36831;&#12290;</title><link>https://arxiv.org/abs/2402.09360</link><description>&lt;p&gt;
HiRE:&#39640;&#21484;&#22238;&#29575;&#30340;&#39640;&#25928;LLM&#25512;&#29702;&#30340;&#36817;&#20284;Top-$k$&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
HiRE: High Recall Approximate Top-$k$ Estimation for Efficient LLM Inference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09360
&lt;/p&gt;
&lt;p&gt;
HiRE&#26159;&#19968;&#31181;&#29992;&#20110;&#39640;&#25928;&#30340;LLM&#25512;&#29702;&#30340;&#39640;&#21484;&#22238;&#29575;&#30340;&#36817;&#20284;Top-k&#20272;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#21387;&#32553;&#26041;&#26696;&#20943;&#23569;&#20102;&#27169;&#22411;&#21442;&#25968;&#20256;&#36755;&#21644;&#24310;&#36831;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21152;&#36895;&#22120;&#65288;GPU/TPU&#65289;&#19978;&#20351;&#29992;&#29983;&#25104;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36827;&#34892;&#33258;&#22238;&#24402;&#35299;&#30721;&#24448;&#24448;&#21463;&#21040;&#20869;&#23384;&#38480;&#21046;&#65292;&#22823;&#37096;&#20998;&#26102;&#38388;&#29992;&#20110;&#23558;&#27169;&#22411;&#21442;&#25968;&#20174;&#39640;&#24102;&#23485;&#20869;&#23384;&#65288;HBM&#65289;&#20256;&#36755;&#21040;&#32531;&#23384;&#20013;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#36866;&#24403;&#35757;&#32451;&#27169;&#22411;&#65292;&#22312;&#21069;&#39304;&#65288;FFN&#65289;&#23618;&#20013;&#32500;&#25345;&#36136;&#37327;&#30340;&#21516;&#26102;&#20855;&#26377;&#26174;&#33879;&#30340;&#31232;&#30095;&#24615;/&#20887;&#20313;&#24615;&#65288;&#20854;&#20013;$k \approx 0.05$&#65289;&#65292;&#20174;&#32780;&#25552;&#20986;&#20102;&#20943;&#23569;&#27169;&#22411;&#21442;&#25968;&#20256;&#36755;&#21644;&#24310;&#36831;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#21033;&#29992;&#36825;&#31181;&#31232;&#30095;&#24615;&#26469;&#25913;&#21892;&#24310;&#36831;&#30340;&#36807;&#31243;&#21463;&#21040;&#25968;&#25454;&#20381;&#36182;&#24615;&#30340;&#38480;&#21046;&#65292;&#36890;&#24120;&#38656;&#35201;&#36827;&#34892;&#23436;&#25972;&#30340;&#30697;&#38453;&#25805;&#20316;&#26469;&#35782;&#21035;&#21069;$k$&#20010;&#34892;/&#21015;&#65292;&#20005;&#37325;&#38480;&#21046;&#20102;&#28508;&#22312;&#30340;&#25910;&#30410;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;HiRE&#65288;&#39640;&#21484;&#22238;&#29575;&#30340;&#36817;&#20284;Top-k&#20272;&#35745;&#65289;&#12290;HiRE&#21253;&#25324;&#20004;&#20010;&#26032;&#39062;&#30340;&#32452;&#20214;&#65306;&#65288;i&#65289;&#19968;&#31181;&#21387;&#32553;&#26041;&#26696;&#20197;&#20415;&#24265;&#20215;&#22320;&#20272;&#35745;&#21069;$k$&#20010;&#34892;/&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09360v1 Announce Type: cross Abstract: Autoregressive decoding with generative Large Language Models (LLMs) on accelerators (GPUs/TPUs) is often memory-bound where most of the time is spent on transferring model parameters from high bandwidth memory (HBM) to cache. On the other hand, recent works show that LLMs can maintain quality with significant sparsity/redundancy in the feedforward (FFN) layers by appropriately training the model to operate on a top-$k$ fraction of rows/columns (where $k \approx 0.05$), there by suggesting a way to reduce the transfer of model parameters, and hence latency. However, exploiting this sparsity for improving latency is hindered by the fact that identifying top rows/columns is data-dependent and is usually performed using full matrix operations, severely limiting potential gains. To address these issues, we introduce HiRE (High Recall Approximate Top-k Estimation). HiRE comprises of two novel components: (i) a compression scheme to cheaply p
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#39318;&#27425;&#23558;&#31867;&#20284;&#20110;ChatGPT&#30340;&#22522;&#20110;&#20113;&#30340;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#20110;&#23433;&#20840;&#30340;&#21307;&#38498;&#27169;&#22411;&#20013;&#65292;&#29992;&#20110;&#20998;&#26512;&#25918;&#23556;&#23398;&#25253;&#21578;&#65292;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#23454;&#29616;&#20102;95%&#20197;&#19978;&#30340;&#20934;&#30830;&#29575;&#65292;&#21516;&#26102;&#25552;&#39640;&#20102;&#39044;&#27979;&#21487;&#38752;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#23545;&#20110;&#21307;&#29983;&#26469;&#35828;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>https://arxiv.org/abs/2402.09358</link><description>&lt;p&gt;
&#23558;ChatGPT&#38598;&#25104;&#21040;&#23433;&#20840;&#21307;&#38498;&#32593;&#32476;&#20013;&#65306;&#19968;&#20010;&#20851;&#20110;&#25913;&#36827;&#25918;&#23556;&#23398;&#25253;&#21578;&#20998;&#26512;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Integrating ChatGPT into Secure Hospital Networks: A Case Study on Improving Radiology Report Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09358
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#39318;&#27425;&#23558;&#31867;&#20284;&#20110;ChatGPT&#30340;&#22522;&#20110;&#20113;&#30340;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#20110;&#23433;&#20840;&#30340;&#21307;&#38498;&#27169;&#22411;&#20013;&#65292;&#29992;&#20110;&#20998;&#26512;&#25918;&#23556;&#23398;&#25253;&#21578;&#65292;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#23454;&#29616;&#20102;95%&#20197;&#19978;&#30340;&#20934;&#30830;&#29575;&#65292;&#21516;&#26102;&#25552;&#39640;&#20102;&#39044;&#27979;&#21487;&#38752;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#23545;&#20110;&#21307;&#29983;&#26469;&#35828;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#23558;&#31867;&#20284;&#20110;ChatGPT&#30340;&#22522;&#20110;&#20113;&#30340;&#20154;&#24037;&#26234;&#33021;&#39318;&#27425;&#24212;&#29992;&#20110;&#23433;&#20840;&#27169;&#22411;&#20013;&#65292;&#29992;&#20110;&#20998;&#26512;&#25918;&#23556;&#23398;&#25253;&#21578;&#65292;&#37325;&#35270;&#24739;&#32773;&#25968;&#25454;&#38544;&#31169;&#12290;&#36890;&#36807;&#20351;&#29992;&#29420;&#29305;&#30340;&#21477;&#23376;&#32423;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#65292;&#25105;&#20204;&#22312;&#26816;&#27979;&#24322;&#24120;&#26041;&#38754;&#36798;&#21040;&#20102;&#36229;&#36807;95%&#30340;&#20934;&#30830;&#29575;&#12290;&#35813;&#27169;&#22411;&#36824;&#20934;&#30830;&#22320;&#26631;&#35760;&#20854;&#39044;&#27979;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#22686;&#24378;&#20102;&#23545;&#21307;&#29983;&#30340;&#21487;&#38752;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#24182;&#24102;&#26377;&#30830;&#23450;&#24615;&#25351;&#31034;&#22120;&#12290;&#36825;&#20123;&#36827;&#23637;&#22312;&#24320;&#21457;&#23433;&#20840;&#39640;&#25928;&#30340;&#21307;&#30103;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#26041;&#38754;&#20195;&#34920;&#20102;&#37325;&#22823;&#36827;&#27493;&#65292;&#20026;&#30417;&#31649;&#26368;&#23569;&#30340;&#21307;&#38498;&#20869;&#37096;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#26410;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09358v1 Announce Type: new Abstract: This study demonstrates the first in-hospital adaptation of a cloud-based AI, similar to ChatGPT, into a secure model for analyzing radiology reports, prioritizing patient data privacy. By employing a unique sentence-level knowledge distillation method through contrastive learning, we achieve over 95% accuracy in detecting anomalies. The model also accurately flags uncertainties in its predictions, enhancing its reliability and interpretability for physicians with certainty indicators. These advancements represent significant progress in developing secure and efficient AI tools for healthcare, suggesting a promising future for in-hospital AI applications with minimal supervision.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#21333;&#37325;&#32622;&#30340;&#20998;&#32780;&#27835;&#20043;&#27169;&#20223;&#23398;&#20064;&#30340;&#25193;&#23637;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#35299;&#20915;&#20102;&#22312;&#28436;&#31034;&#23398;&#20064;&#20013;&#38656;&#35201;&#37325;&#32622;&#29305;&#23450;&#29366;&#24577;&#30340;&#38480;&#21046;&#65292;&#20351;&#20854;&#21487;&#20197;&#24212;&#29992;&#20110;&#23454;&#38469;&#31995;&#32479;&#12290;</title><link>https://arxiv.org/abs/2402.09355</link><description>&lt;p&gt;
&#21333;&#37325;&#32622;&#30340;&#20998;&#32780;&#27835;&#20043;&#27169;&#20223;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Single-Reset Divide &amp; Conquer Imitation Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09355
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#21333;&#37325;&#32622;&#30340;&#20998;&#32780;&#27835;&#20043;&#27169;&#20223;&#23398;&#20064;&#30340;&#25193;&#23637;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#35299;&#20915;&#20102;&#22312;&#28436;&#31034;&#23398;&#20064;&#20013;&#38656;&#35201;&#37325;&#32622;&#29305;&#23450;&#29366;&#24577;&#30340;&#38480;&#21046;&#65292;&#20351;&#20854;&#21487;&#20197;&#24212;&#29992;&#20110;&#23454;&#38469;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28436;&#31034;&#36890;&#24120;&#29992;&#20110;&#21152;&#24555;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#23398;&#20064;&#36807;&#31243;&#12290;&#20026;&#20102;&#24212;&#23545;&#35775;&#38382;&#22810;&#20010;&#28436;&#31034;&#30340;&#22256;&#38590;&#65292;&#19968;&#20123;&#31639;&#27861;&#24050;&#32463;&#34987;&#24320;&#21457;&#20986;&#26469;&#20174;&#21333;&#20010;&#28436;&#31034;&#20013;&#23398;&#20064;&#12290;&#20854;&#20013;&#65292;&#20998;&#32780;&#27835;&#20043;&#27169;&#20223;&#23398;&#20064;&#31639;&#27861;&#21033;&#29992;&#39034;&#24207;&#20559;&#24046;&#26469;&#23398;&#20064;&#19968;&#20010;&#25511;&#21046;&#31574;&#30053;&#65292;&#29992;&#20110;&#22797;&#26434;&#26426;&#22120;&#20154;&#20219;&#21153;&#30340;&#21333;&#29366;&#24577;&#28436;&#31034;&#12290;&#26368;&#26032;&#29256;&#26412;DCIL-II&#23637;&#31034;&#20102;&#26174;&#33879;&#30340;&#26679;&#26412;&#25928;&#29575;&#12290;&#36825;&#31181;&#26032;&#26041;&#27861;&#22312;&#25193;&#23637;&#30340;&#30446;&#26631;&#26465;&#20214;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#20869;&#36816;&#20316;&#65292;&#30830;&#20445;&#20102;&#20174;&#28436;&#31034;&#20013;&#25552;&#21462;&#30340;&#20013;&#38388;&#30446;&#26631;&#21644;&#21518;&#32493;&#30446;&#26631;&#20043;&#38388;&#30340;&#20860;&#23481;&#24615;&#12290;&#28982;&#32780;&#65292;&#19968;&#20010;&#22522;&#26412;&#30340;&#38480;&#21046;&#26159;&#35813;&#31995;&#32479;&#21487;&#20197;&#34987;&#37325;&#32622;&#21040;&#28436;&#31034;&#36712;&#36857;&#19978;&#30340;&#29305;&#23450;&#29366;&#24577;&#65292;&#23558;&#24212;&#29992;&#38480;&#21046;&#22312;&#27169;&#25311;&#31995;&#32479;&#20013;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;&#21333;&#37325;&#32622;&#30340;&#25193;&#23637;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09355v1 Announce Type: cross Abstract: Demonstrations are commonly used to speed up the learning process of Deep Reinforcement Learning algorithms. To cope with the difficulty of accessing multiple demonstrations, some algorithms have been developed to learn from a single demonstration. In particular, the Divide &amp; Conquer Imitation Learning algorithms leverage a sequential bias to learn a control policy for complex robotic tasks using a single state-based demonstration. The latest version, DCIL-II demonstrates remarkable sample efficiency. This novel method operates within an extended Goal-Conditioned Reinforcement Learning framework, ensuring compatibility between intermediate and subsequent goals extracted from the demonstration. However, a fundamental limitation arises from the assumption that the system can be reset to specific states along the demonstrated trajectory, confining the application to simulated systems. In response, we introduce an extension called Single-Re
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20154;&#26426;&#21327;&#21516;&#30340;&#26041;&#27861;&#24320;&#21457;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23457;&#35745;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#19981;&#21516;&#29256;&#26412;&#30340;&#30456;&#21516;&#38382;&#39064;&#26469;&#25506;&#27979;&#27169;&#22411;&#21487;&#33021;&#23384;&#22312;&#30340;&#20559;&#35265;&#25110;&#24187;&#35273;&#65292;&#23454;&#29616;&#20102;&#33258;&#21160;&#21270;&#21644;&#21487;&#25193;&#23637;&#30340;&#23457;&#35745;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.09346</link><description>&lt;p&gt;
&#20351;&#29992;&#20154;&#26426;&#21327;&#21516;&#30340;&#26041;&#27861;&#24320;&#21457;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23457;&#35745;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Developing a Framework for Auditing Large Language Models Using Human-in-the-Loop
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09346
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20154;&#26426;&#21327;&#21516;&#30340;&#26041;&#27861;&#24320;&#21457;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23457;&#35745;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#19981;&#21516;&#29256;&#26412;&#30340;&#30456;&#21516;&#38382;&#39064;&#26469;&#25506;&#27979;&#27169;&#22411;&#21487;&#33021;&#23384;&#22312;&#30340;&#20559;&#35265;&#25110;&#24187;&#35273;&#65292;&#23454;&#29616;&#20102;&#33258;&#21160;&#21270;&#21644;&#21487;&#25193;&#23637;&#30340;&#23457;&#35745;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#29992;&#25143;&#21644;&#22330;&#26223;&#20013;&#30340;&#26222;&#21450;&#65292;&#35782;&#21035;&#20351;&#29992;&#36825;&#20123;&#27169;&#22411;&#26102;&#21487;&#33021;&#23384;&#22312;&#30340;&#38382;&#39064;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#65292;&#20363;&#22914;&#20559;&#35265;&#12289;&#19981;&#19968;&#33268;&#24615;&#21644;&#24187;&#35273;&#12290;&#23613;&#31649;&#23545;&#36825;&#20123;&#38382;&#39064;&#36827;&#34892;&#23457;&#35745;&#26159;&#21487;&#21462;&#30340;&#65292;&#20294;&#24182;&#19981;&#23481;&#26131;&#35299;&#20915;&#12290;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#26159;&#20351;&#29992;&#19981;&#21516;&#29256;&#26412;&#30340;&#30456;&#21516;&#38382;&#39064;&#26469;&#25506;&#27979;&#35821;&#35328;&#27169;&#22411;&#65292;&#36825;&#21487;&#20197;&#26292;&#38706;&#20854;&#30693;&#35782;&#25110;&#36816;&#34892;&#20013;&#30340;&#19981;&#19968;&#33268;&#24615;&#65292;&#20174;&#32780;&#34920;&#26126;&#21487;&#33021;&#23384;&#22312;&#20559;&#35265;&#25110;&#24187;&#35273;&#12290;&#28982;&#32780;&#65292;&#35201;&#22312;&#22823;&#35268;&#27169;&#19978;&#23454;&#29616;&#36825;&#31181;&#23457;&#35745;&#26041;&#27861;&#65292;&#25105;&#20204;&#38656;&#35201;&#19968;&#31181;&#21487;&#38752;&#19988;&#33258;&#21160;&#21270;&#30340;&#29983;&#25104;&#36825;&#20123;&#25506;&#27979;&#30340;&#26041;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;&#21644;&#21487;&#25193;&#23637;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20854;&#20013;&#19968;&#31181;&#26041;&#27861;&#26159;&#20351;&#29992;&#19981;&#21516;&#30340;&#35821;&#35328;&#27169;&#22411;&#21644;&#20154;&#26426;&#21327;&#21516;&#12290;&#36825;&#31181;&#26041;&#27861;&#25552;&#20379;&#20102;&#21487;&#39564;&#35777;&#24615;&#21644;&#36879;&#26126;&#24615;&#65292;&#36991;&#20813;&#23545;&#21516;&#19968;&#35821;&#35328;&#27169;&#22411;&#30340;&#24490;&#29615;&#20381;&#36182;&#65292;&#24182;&#22686;&#21152;&#20102;&#31185;&#23398;&#20005;&#35880;&#24615;&#21644;&#26222;&#36866;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09346v1 Announce Type: new Abstract: As LLMs become more pervasive across various users and scenarios, identifying potential issues when using these models becomes essential. Examples include bias, inconsistencies, and hallucination. Although auditing the LLM for these problems is desirable, it is far from being easy or solved. An effective method is to probe the LLM using different versions of the same question. This could expose inconsistencies in its knowledge or operation, indicating potential for bias or hallucination. However, to operationalize this auditing method at scale, we need an approach to create those probes reliably and automatically. In this paper we propose an automatic and scalable solution, where one uses a different LLM along with human-in-the-loop. This approach offers verifiability and transparency, while avoiding circular reliance on the same LLMs, and increasing scientific rigor and generalizability. Specifically, we present a novel methodology with 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;InfoRM&#30340;&#22870;&#21169;&#24314;&#27169;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#21464;&#20998;&#20449;&#24687;&#29942;&#39048;&#30446;&#26631;&#21644;&#27169;&#22411;&#22797;&#26434;&#24230;&#35843;&#33410;&#26426;&#21046;&#65292;&#35299;&#20915;&#20102;&#22870;&#21169;&#20316;&#24330;&#38382;&#39064;&#65292;&#24182;&#21033;&#29992;&#38598;&#25104;&#32858;&#31867;&#20559;&#24046;&#24471;&#20998;&#65288;ICDS&#65289;&#26469;&#26816;&#27979;&#22870;&#21169;&#36807;&#24230;&#20248;&#21270;&#12290;</title><link>https://arxiv.org/abs/2402.09345</link><description>&lt;p&gt;
&#36890;&#36807;&#20449;&#24687;&#35770;&#22870;&#21169;&#24314;&#27169;&#26469;&#20943;&#36731;&#22870;&#21169;&#20316;&#24330;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Mitigating Reward Hacking via Information-Theoretic Reward Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09345
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;InfoRM&#30340;&#22870;&#21169;&#24314;&#27169;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#21464;&#20998;&#20449;&#24687;&#29942;&#39048;&#30446;&#26631;&#21644;&#27169;&#22411;&#22797;&#26434;&#24230;&#35843;&#33410;&#26426;&#21046;&#65292;&#35299;&#20915;&#20102;&#22870;&#21169;&#20316;&#24330;&#38382;&#39064;&#65292;&#24182;&#21033;&#29992;&#38598;&#25104;&#32858;&#31867;&#20559;&#24046;&#24471;&#20998;&#65288;ICDS&#65289;&#26469;&#26816;&#27979;&#22870;&#21169;&#36807;&#24230;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#65288;RLHF&#65289;&#20013;&#30340;&#25104;&#21151;&#22312;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#23545;&#40784;&#26041;&#38754;&#65292;&#22870;&#21169;&#20316;&#24330;&#38382;&#39064;&#65292;&#20063;&#34987;&#31216;&#20026;&#22870;&#21169;&#36807;&#24230;&#20248;&#21270;&#65292;&#20173;&#28982;&#26159;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#65292;&#20027;&#35201;&#28304;&#20110;&#22870;&#21169;&#24314;&#27169;&#30340;&#23616;&#38480;&#24615;&#65292;&#21363;&#22870;&#21169;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#20559;&#22909;&#25968;&#25454;&#38598;&#30340;&#19981;&#19968;&#33268;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20174;&#20449;&#24687;&#35770;&#30340;&#35270;&#35282;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25512;&#24191;&#21644;&#40065;&#26834;&#30340;&#22870;&#21169;&#24314;&#27169;&#26694;&#26550;&#65292;&#31216;&#20026;InfoRM&#65292;&#36890;&#36807;&#24341;&#20837;&#21464;&#20998;&#20449;&#24687;&#29942;&#39048;&#30446;&#26631;&#26469;&#36807;&#28388;&#20986;&#19981;&#30456;&#20851;&#30340;&#20449;&#24687;&#65292;&#24182;&#24320;&#21457;&#19968;&#31181;&#27169;&#22411;&#22797;&#26434;&#24230;&#35843;&#33410;&#26426;&#21046;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#21457;&#29616;&#20102;&#36807;&#24230;&#20248;&#21270;&#19982;&#28508;&#21464;&#37327;&#31354;&#38388;&#30340;&#24322;&#24120;&#20540;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#23558;InfoRM&#20316;&#20026;&#26816;&#27979;&#22870;&#21169;&#36807;&#24230;&#20248;&#21270;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#24037;&#20855;&#12290;&#21463;&#21040;&#36825;&#19968;&#21457;&#29616;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#38598;&#25104;&#32858;&#31867;&#20559;&#24046;&#24471;&#20998;&#65288;ICDS&#65289;&#65292;&#29992;&#20110;&#37327;&#21270;&#36807;&#20248;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09345v1 Announce Type: cross Abstract: Despite the success of reinforcement learning from human feedback (RLHF) in aligning language models with human values, reward hacking, also termed reward overoptimization, remains a critical challenge, which primarily stems from limitations in reward modeling, i.e., generalizability of the reward model and inconsistency in the preference dataset. In this work, we tackle this problem from an information theoretic-perspective, and propose a generalizable and robust framework for reward modeling, namely InfoRM, by introducing a variational information bottleneck objective to filter out irrelevant information and developing a mechanism for model complexity modulation. Notably, we further identify a correlation between overoptimization and outliers in the latent space, establishing InfoRM as a promising tool for detecting reward overoptimization. Inspired by this finding, we propose the Integrated Cluster Deviation Score (ICDS), which quant
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#36866;&#29992;&#20110;&#35299;&#20915;&#21453;&#35126;&#31215;&#36870;&#38382;&#39064;&#30340;&#31070;&#32463;&#32593;&#32476;&#28176;&#36817;&#34892;&#20026;&#65292;&#24182;&#21457;&#29616;&#20351;&#29992;&#20174;&#31070;&#32463;&#32593;&#32476;&#30340;&#28176;&#36817;&#26497;&#38480;&#23548;&#20986;&#30340;&#39640;&#26031;&#36807;&#31243;&#27604;&#20840;&#36830;&#25509;&#30340;&#31070;&#32463;&#32593;&#32476;&#33719;&#24471;&#26356;&#22909;&#30340;&#32467;&#26524;&#65292;&#32780;&#19988;&#35266;&#23519;&#21040;&#38543;&#30528;&#23618;&#25968;&#30340;&#22686;&#21152;&#65292;&#35757;&#32451;&#21518;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#20934;&#30830;&#24615;&#25509;&#36817;&#20110;&#39640;&#26031;&#36807;&#31243;&#30340;&#20934;&#30830;&#24615;&#12290;&#20854;&#20013;&#19968;&#20010;&#39640;&#26031;&#36807;&#31243;&#30340;&#35299;&#37322;&#19982;&#20256;&#32479;&#26041;&#27861;&#19981;&#21516;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#35270;&#35282;&#12290;</title><link>https://arxiv.org/abs/2402.09338</link><description>&lt;p&gt;
&#36866;&#29992;&#20110;&#36870;&#38382;&#39064;&#35299;&#20915;&#30340;&#31070;&#32463;&#32593;&#32476;&#28176;&#36817;&#34892;&#20026;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Neural Networks asymptotic behaviours suitable for the resolution of inverse problems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09338
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36866;&#29992;&#20110;&#35299;&#20915;&#21453;&#35126;&#31215;&#36870;&#38382;&#39064;&#30340;&#31070;&#32463;&#32593;&#32476;&#28176;&#36817;&#34892;&#20026;&#65292;&#24182;&#21457;&#29616;&#20351;&#29992;&#20174;&#31070;&#32463;&#32593;&#32476;&#30340;&#28176;&#36817;&#26497;&#38480;&#23548;&#20986;&#30340;&#39640;&#26031;&#36807;&#31243;&#27604;&#20840;&#36830;&#25509;&#30340;&#31070;&#32463;&#32593;&#32476;&#33719;&#24471;&#26356;&#22909;&#30340;&#32467;&#26524;&#65292;&#32780;&#19988;&#35266;&#23519;&#21040;&#38543;&#30528;&#23618;&#25968;&#30340;&#22686;&#21152;&#65292;&#35757;&#32451;&#21518;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#20934;&#30830;&#24615;&#25509;&#36817;&#20110;&#39640;&#26031;&#36807;&#31243;&#30340;&#20934;&#30830;&#24615;&#12290;&#20854;&#20013;&#19968;&#20010;&#39640;&#26031;&#36807;&#31243;&#30340;&#35299;&#37322;&#19982;&#20256;&#32479;&#26041;&#27861;&#19981;&#21516;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#25216;&#26415;&#22312;&#21453;&#35126;&#31215;&#36870;&#38382;&#39064;&#20013;&#30340;&#26377;&#25928;&#24615;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#25105;&#20204;&#32771;&#34385;&#21040;NN&#30340;&#28176;&#36817;&#26497;&#38480;&#65292;&#23545;&#24212;&#20110;&#39640;&#26031;&#36807;&#31243;&#65288;GPs&#65289;&#65292;&#20854;&#20013;&#21442;&#25968;&#30340;&#38750;&#32447;&#24615;&#24615;&#20002;&#22833;&#12290;&#21033;&#29992;&#36825;&#20123;&#32467;&#26524;&#30340;GPs&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;&#26684;&#23376;&#19978;&#20351;&#29992;&#33945;&#29305;&#21345;&#27931;&#25216;&#26415;&#27169;&#25311;&#37327;&#23376;&#35856;&#25391;&#23376;&#26469;&#35299;&#20915;&#21453;&#35126;&#31215;&#36870;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#20840;&#36830;&#25509;&#30340;NN&#35299;&#20915;&#21453;&#35126;&#31215;&#36870;&#38382;&#39064;&#30340;&#32467;&#26524;&#19981;&#22914;&#20351;&#29992;&#20174;NN&#30340;&#28176;&#36817;&#26497;&#38480;&#23548;&#20986;&#30340;GPs&#33719;&#24471;&#30340;&#32467;&#26524;&#22909;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#38543;&#30528;&#23618;&#25968;&#30340;&#22686;&#21152;&#65292;&#35757;&#32451;&#21518;&#30340;NN&#30340;&#20934;&#30830;&#24615;&#25509;&#36817;&#20110;GPs&#30340;&#20934;&#30830;&#24615;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#20854;&#20013;&#19968;&#20010;GPs&#30340;&#35299;&#37322;&#19982;&#25991;&#29486;&#20013;&#30340;&#20256;&#32479;&#26041;&#27861;&#19981;&#21516;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09338v1 Announce Type: cross Abstract: In this paper, we perform a study on the effectiveness of Neural Network (NN) techniques for deconvolution inverse problems. We consider NN's asymptotic limits, corresponding to Gaussian Processes (GPs), where parameter non-linearities are lost. Using these resulting GPs, we address the deconvolution inverse problem in the case of a quantum harmonic oscillator simulated through Monte Carlo techniques on a lattice. A scenario with a known analytical solution. Our findings indicate that solving the deconvolution inverse problem with a fully connected NN yields less performing results than those obtained using the GPs derived from NN's asymptotic limits. Furthermore, we observe the trained NN's accuracy approaching that of GPs with increasing layer width. Notably, one of these GPs defies interpretation as a probabilistic model, offering a novel perspective compared to established methods in the literature. Additionally, the NNs, in their a
&lt;/p&gt;</description></item><item><title>"AuditLLM"&#26159;&#19968;&#31181;&#33021;&#22815;&#20197;&#31995;&#32479;&#30340;&#26041;&#24335;&#35780;&#20272;&#21508;&#31181;LLMs&#30340;&#24615;&#33021;&#30340;&#24037;&#20855;&#65292;&#36890;&#36807;&#20351;&#29992;&#20174;&#21333;&#20010;&#38382;&#39064;&#29983;&#25104;&#30340;&#22810;&#20010;&#25506;&#27979;&#26469;&#23545;&#32473;&#23450;&#30340;LLM&#36827;&#34892;&#23457;&#35745;&#65292;&#20174;&#32780;&#35782;&#21035;&#27169;&#22411;&#22312;&#29702;&#35299;&#25110;&#25805;&#20316;&#26041;&#38754;&#30340;&#20219;&#20309;&#19981;&#19968;&#33268;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.09334</link><description>&lt;p&gt;
AuditLLM:&#19968;&#31181;&#20351;&#29992;&#22810;&#25506;&#27979;&#26041;&#27861;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#23457;&#35745;&#30340;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
AuditLLM: A Tool for Auditing Large Language Models Using Multiprobe Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09334
&lt;/p&gt;
&lt;p&gt;
"AuditLLM"&#26159;&#19968;&#31181;&#33021;&#22815;&#20197;&#31995;&#32479;&#30340;&#26041;&#24335;&#35780;&#20272;&#21508;&#31181;LLMs&#30340;&#24615;&#33021;&#30340;&#24037;&#20855;&#65292;&#36890;&#36807;&#20351;&#29992;&#20174;&#21333;&#20010;&#38382;&#39064;&#29983;&#25104;&#30340;&#22810;&#20010;&#25506;&#27979;&#26469;&#23545;&#32473;&#23450;&#30340;LLM&#36827;&#34892;&#23457;&#35745;&#65292;&#20174;&#32780;&#35782;&#21035;&#27169;&#22411;&#22312;&#29702;&#35299;&#25110;&#25805;&#20316;&#26041;&#38754;&#30340;&#20219;&#20309;&#19981;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#24773;&#22659;&#20013;&#36234;&#26469;&#36234;&#24191;&#27867;&#22320;&#34987;&#37319;&#29992;&#26102;&#65292;&#30830;&#20445;&#23427;&#20204;&#22312;&#29305;&#23450;&#24212;&#29992;&#20013;&#26159;&#30456;&#23545;&#23433;&#20840;&#12289;&#19968;&#33268;&#21644;&#21487;&#38752;&#30340;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#36825;&#21487;&#33021;&#38656;&#35201;&#23545;&#23427;&#20204;&#36827;&#34892;&#25506;&#27979;&#25110;&#23457;&#35745;&#12290;&#36890;&#36807;&#20351;&#29992;&#22810;&#27425;&#36845;&#20195;&#30340;&#21333;&#20010;&#38382;&#39064;&#23545;LLMs&#36827;&#34892;&#25506;&#27979;&#65292;&#21487;&#20197;&#25581;&#31034;&#23427;&#20204;&#30340;&#30693;&#35782;&#25110;&#21151;&#33021;&#30340;&#28508;&#22312;&#19981;&#19968;&#33268;&#24615;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#32570;&#20047;&#19968;&#31181;&#33021;&#22815;&#20197;&#31616;&#21333;&#24037;&#20316;&#27969;&#31243;&#21644;&#20302;&#25216;&#26415;&#38376;&#27099;&#36827;&#34892;&#27492;&#31867;&#23457;&#35745;&#30340;&#24037;&#20855;&#12290;&#22312;&#36825;&#20010;&#28436;&#31034;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;"AuditLLM"&#30340;&#26032;&#39062;&#24037;&#20855;&#65292;&#23427;&#26088;&#22312;&#20197;&#31995;&#32479;&#30340;&#26041;&#24335;&#35780;&#20272;&#21508;&#31181;LLMs&#30340;&#24615;&#33021;&#12290;AuditLLM&#30340;&#26680;&#24515;&#21151;&#33021;&#22312;&#20110;&#33021;&#22815;&#36890;&#36807;&#20351;&#29992;&#20174;&#21333;&#20010;&#38382;&#39064;&#29983;&#25104;&#30340;&#22810;&#20010;&#25506;&#27979;&#26469;&#23545;&#32473;&#23450;&#30340;LLM&#36827;&#34892;&#23457;&#35745;&#65292;&#20174;&#32780;&#35782;&#21035;&#27169;&#22411;&#22312;&#29702;&#35299;&#25110;&#25805;&#20316;&#26041;&#38754;&#30340;&#20219;&#20309;&#19981;&#19968;&#33268;&#24615;&#12290;&#19968;&#20010;&#30456;&#23545;&#20581;&#22766;&#12289;&#21487;&#38752;&#21644;&#19968;&#33268;&#30340;LLM&#24212;&#35813;&#23545;&#20197;&#19981;&#21516;&#26041;&#24335;&#25110;&#30001;&#19981;&#21516;&#20154;&#25552;&#20986;&#30340;&#38382;&#39064;&#32473;&#20986;&#35821;&#20041;&#30456;&#20284;&#30340;&#22238;&#31572;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09334v1 Announce Type: new Abstract: As Large Language Models (LLMs) gain wider adoption in various contexts, it becomes crucial to ensure they are reasonably safe, consistent, and reliable for an application at hand. This may require probing or auditing them. Probing LLMs with varied iterations of a single question could reveal potential inconsistencies in their knowledge or functionality. However, a tool for performing such audits with simple workflow and low technical threshold is lacking. In this demo, we introduce "AuditLLM," a novel tool designed to evaluate the performance of various LLMs in a methodical way. AuditLLM's core functionality lies in its ability to test a given LLM by auditing it using multiple probes generated from a single question, thereby identifying any inconsistencies in the model's understanding or operation. A reasonably robust, reliable, and consistent LLM should output semantically similar responses for a question asked differently or by differe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;ICDPO&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20511;&#29992;&#20182;&#20154;&#30340;&#23545;&#40784;&#33021;&#21147;&#65292;&#24182;&#20351;&#29992;&#19978;&#19979;&#25991;&#23398;&#20064;&#26469;&#20248;&#21270;&#29983;&#25104;&#27169;&#22411;&#65292;&#20197;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.09320</link><description>&lt;p&gt;
ICDPO: &#36890;&#36807;&#19978;&#19979;&#25991;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#26377;&#25928;&#22320;&#20511;&#29992;&#20182;&#20154;&#30340;&#23545;&#40784;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
ICDPO: Effectively Borrowing Alignment Capability of Others via In-context Direct Preference Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09320
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;ICDPO&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20511;&#29992;&#20182;&#20154;&#30340;&#23545;&#40784;&#33021;&#21147;&#65292;&#24182;&#20351;&#29992;&#19978;&#19979;&#25991;&#23398;&#20064;&#26469;&#20248;&#21270;&#29983;&#25104;&#27169;&#22411;&#65292;&#20197;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20381;&#36182;&#20110;&#20154;&#31867;&#20559;&#22909;&#23545;&#40784;&#65288;HPA&#65289;&#26469;&#30830;&#20445;&#29983;&#25104;&#23433;&#20840;&#20869;&#23481;&#12290;&#30001;&#20110;&#24494;&#35843;&#24102;&#26469;&#30340;&#24040;&#22823;&#25104;&#26412;&#65292;&#20986;&#29616;&#20102;&#20813;&#24494;&#35843;&#30340;&#26041;&#27861;&#65292;&#36890;&#24120;&#36890;&#36807;&#22806;&#37096;&#36741;&#21161;&#26041;&#27861;&#20462;&#25913;LLM&#35299;&#30721;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#24182;&#27809;&#26377;&#20174;&#26412;&#36136;&#19978;&#22686;&#24378;LLM&#26412;&#36523;&#12290;&#26412;&#25991;&#37325;&#26032;&#24605;&#32771;&#20102;DPO&#30340;&#25512;&#23548;&#36807;&#31243;&#65292;&#24314;&#31435;&#20102;&#22522;&#20110;LLM&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#20043;&#21069;&#21644;&#20043;&#21518;&#30340;&#29366;&#24577;&#30340;&#21363;&#26102;&#25171;&#20998;&#22120;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ICDPO&#30340;&#26032;&#26041;&#27861;&#12290;&#23427;&#20351;LLM&#33021;&#22815;&#36890;&#36807;ICL&#20174;&#20248;&#31168;&#30340;LLM&#20013;&#20511;&#29992;HPA&#33021;&#21147;&#65292;&#29983;&#25104;&#30001;&#19978;&#36848;&#21363;&#26102;&#25171;&#20998;&#22120;&#20272;&#35745;&#30340;&#33391;&#22909;&#23545;&#40784;&#30340;&#21709;&#24212;&#65292;&#20174;&#32780;&#25552;&#39640;&#26368;&#32456;&#24615;&#33021;&#12290;ICDPO&#21487;&#20197;&#36890;&#36807;&#20004;&#38454;&#27573;&#26816;&#32034;&#22120;&#21644;&#21319;&#32423;&#30340;&#25171;&#20998;&#22120;&#36827;&#19968;&#27493;&#22686;&#24378;&#65292;&#37117;&#20855;&#26377;&#30410;&#22788;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;ICDPO&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09320v1 Announce Type: cross Abstract: Large Language Models (LLMs) rely on Human Preference Alignment (HPA) to ensure the generation of safe content. Due to the heavy cost associated with fine-tuning, fine-tuning-free methods have emerged, typically modifying LLM decoding with external auxiliary methods. However, these methods do not essentially enhance the LLM itself. In this paper, we rethink the derivation procedures of DPO, based on which we conversely build an instant scorer using the states of the LLM before and after In-context Learning (ICL). Accordingly, we propose a novel approach called In-Context Direct Preference Optimization (ICDPO). It enables LLMs to borrow the HPA capabilities from superior LLMs with ICL, generating well-aligned responses as estimated by the aforementioned instant scorer, thereby enhancing the final performance. ICDPO can be further enhanced with a two-stage retriever and an upgraded scorer, both offering benefits. Extensive experiments sho
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39044;&#35757;&#32451;&#33258;&#32534;&#30721;&#22120;&#30340;&#21487;&#35299;&#37322;&#38899;&#20048;&#38899;&#39057;&#21407;&#22411;&#23398;&#20064;&#27169;&#22411;PECMAE&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#25193;&#25955;&#35299;&#30721;&#22120;&#25552;&#21319;&#20102;&#37325;&#26500;&#25928;&#26524;&#12290;&#22312;&#38899;&#20048;&#20048;&#22120;&#20998;&#31867;&#21644;&#27969;&#27966;&#35782;&#21035;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#20445;&#30041;&#26356;&#22810;&#20851;&#20110;&#38899;&#39057;&#29305;&#24449;&#30340;&#20449;&#24687;&#65292;&#25552;&#39640;&#20998;&#31867;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.09318</link><description>&lt;p&gt;
&#21033;&#29992;&#39044;&#35757;&#32451;&#33258;&#32534;&#30721;&#22120;&#36827;&#34892;&#21487;&#35299;&#37322;&#30340;&#38899;&#20048;&#38899;&#39057;&#21407;&#22411;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Leveraging Pre-Trained Autoencoders for Interpretable Prototype Learning of Music Audio
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09318
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39044;&#35757;&#32451;&#33258;&#32534;&#30721;&#22120;&#30340;&#21487;&#35299;&#37322;&#38899;&#20048;&#38899;&#39057;&#21407;&#22411;&#23398;&#20064;&#27169;&#22411;PECMAE&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#25193;&#25955;&#35299;&#30721;&#22120;&#25552;&#21319;&#20102;&#37325;&#26500;&#25928;&#26524;&#12290;&#22312;&#38899;&#20048;&#20048;&#22120;&#20998;&#31867;&#21644;&#27969;&#27966;&#35782;&#21035;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#20445;&#30041;&#26356;&#22810;&#20851;&#20110;&#38899;&#39057;&#29305;&#24449;&#30340;&#20449;&#24687;&#65292;&#25552;&#39640;&#20998;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21407;&#22411;&#23398;&#20064;&#30340;&#38899;&#20048;&#38899;&#39057;&#20998;&#31867;&#30340;&#21487;&#35299;&#37322;&#27169;&#22411;PECMAE&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22522;&#20110;&#20043;&#21069;&#30340;&#26041;&#27861;APNet&#65292;&#35813;&#26041;&#27861;&#32852;&#21512;&#23398;&#20064;&#33258;&#32534;&#30721;&#22120;&#21644;&#21407;&#22411;&#32593;&#32476;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;&#36825;&#20004;&#20010;&#35757;&#32451;&#36807;&#31243;&#20998;&#31163;&#65292;&#36825;&#26679;&#25105;&#20204;&#23601;&#21487;&#20197;&#21033;&#29992;&#24050;&#22312;&#26356;&#22823;&#25968;&#25454;&#19978;&#39044;&#35757;&#32451;&#30340;&#33258;&#30417;&#30563;&#33258;&#32534;&#30721;&#22120;&#65288;EnCodecMAE&#65289;&#25552;&#20379;&#26356;&#22909;&#30340;&#27867;&#21270;&#34920;&#31034;&#12290;APNet&#20351;&#24471;&#21407;&#22411;&#21487;&#20197;&#36890;&#36807;&#26368;&#36817;&#30340;&#35757;&#32451;&#25968;&#25454;&#26679;&#26412;&#36827;&#34892;&#27874;&#24418;&#37325;&#26500;&#20197;&#23454;&#29616;&#21487;&#35299;&#37322;&#24615;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#25506;&#32034;&#20351;&#29992;&#25193;&#25955;&#35299;&#30721;&#22120;&#65292;&#23427;&#20801;&#35768;&#22312;&#27809;&#26377;&#36825;&#31181;&#20381;&#36182;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#37325;&#26500;&#12290;&#25105;&#20204;&#22312;&#38899;&#20048;&#20048;&#22120;&#20998;&#31867;&#65288;Medley-Solos-DB&#65289;&#21644;&#27969;&#27966;&#35782;&#21035;&#65288;GTZAN&#21644;&#19968;&#20010;&#26356;&#22823;&#30340;&#20869;&#37096;&#25968;&#25454;&#38598;&#65289;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#21518;&#32773;&#26159;&#19968;&#20010;&#20197;&#21069;&#26410;&#20351;&#29992;&#21407;&#22411;&#32593;&#32476;&#35299;&#20915;&#30340;&#26356;&#20855;&#25361;&#25112;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#21457;&#29616;&#22522;&#20110;&#21407;&#22411;&#30340;&#27169;&#22411;&#21487;&#20197;&#20445;&#30041;&#26356;&#22810;&#20851;&#20110;&#38899;&#39057;&#29305;&#24449;&#30340;&#20449;&#24687;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#20998;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09318v1 Announce Type: cross Abstract: We present PECMAE, an interpretable model for music audio classification based on prototype learning. Our model is based on a previous method, APNet, which jointly learns an autoencoder and a prototypical network. Instead, we propose to decouple both training processes. This enables us to leverage existing self-supervised autoencoders pre-trained on much larger data (EnCodecMAE), providing representations with better generalization. APNet allows prototypes' reconstruction to waveforms for interpretability relying on the nearest training data samples. In contrast, we explore using a diffusion decoder that allows reconstruction without such dependency. We evaluate our method on datasets for music instrument classification (Medley-Solos-DB) and genre recognition (GTZAN and a larger in-house dataset), the latter being a more challenging task not addressed with prototypical networks before. We find that the prototype-based models preserve mo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#36827;&#34892;&#22240;&#26524;&#21457;&#29616;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#22240;&#26524;&#39044;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20197;&#30417;&#30563;&#26041;&#24335;&#23398;&#20064;&#20174;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#21040;&#28508;&#22312;&#22240;&#26524;&#22270;&#30340;&#26144;&#23556;&#65292;&#23454;&#29616;&#20102;&#22312;&#20849;&#20139;&#21160;&#21147;&#23398;&#30340;&#24773;&#20917;&#19979;&#30340;&#30417;&#30563;&#24335;&#22240;&#26524;&#21457;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.09305</link><description>&lt;p&gt;
embracing the black box: &#26397;&#21521;&#22522;&#20110;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#36827;&#34892;&#22240;&#26524;&#21457;&#29616;&#30340;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Embracing the black box: Heading towards foundation models for causal discovery from time series data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09305
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#36827;&#34892;&#22240;&#26524;&#21457;&#29616;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#22240;&#26524;&#39044;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20197;&#30417;&#30563;&#26041;&#24335;&#23398;&#20064;&#20174;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#21040;&#28508;&#22312;&#22240;&#26524;&#22270;&#30340;&#26144;&#23556;&#65292;&#23454;&#29616;&#20102;&#22312;&#20849;&#20139;&#21160;&#21147;&#23398;&#30340;&#24773;&#20917;&#19979;&#30340;&#30417;&#30563;&#24335;&#22240;&#26524;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26469;&#33258;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#22240;&#26524;&#21457;&#29616;&#28085;&#30422;&#20102;&#35768;&#22810;&#29616;&#26377;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21253;&#25324;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#19981;&#25903;&#25345;&#28145;&#24230;&#23398;&#20064;&#20013;&#26368;&#24120;&#35265;&#30340;&#33539;&#24335;&#20043;&#19968;&#65306;&#31471;&#21040;&#31471;&#23398;&#20064;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#25105;&#20204;&#25152;&#31216;&#20043;&#20026;&#22240;&#26524;&#39044;&#35757;&#32451;&#30340;&#26041;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#26088;&#22312;&#20197;&#30417;&#30563;&#30340;&#26041;&#24335;&#23398;&#20064;&#20174;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#21040;&#28508;&#22312;&#22240;&#26524;&#22270;&#30340;&#30452;&#25509;&#26144;&#23556;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#35757;&#32451;&#21644;&#27979;&#35797;&#26102;&#38388;&#24207;&#21015;&#26679;&#26412;&#20849;&#20139;&#22823;&#37096;&#20998;&#21160;&#21147;&#23398;&#30340;&#24773;&#20917;&#19979;&#65292;&#30417;&#30563;&#24335;&#22240;&#26524;&#21457;&#29616;&#26159;&#21487;&#33021;&#30340;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;&#21363;&#20351;&#39069;&#22806;&#30340;&#25968;&#25454;&#19981;&#20849;&#20139;&#30456;&#21516;&#30340;&#21160;&#21147;&#23398;&#65292;&#22240;&#26524;&#39044;&#35757;&#32451;&#30340;&#24615;&#33021;&#20063;&#38543;&#30528;&#25968;&#25454;&#21644;&#27169;&#22411;&#35268;&#27169;&#30340;&#22686;&#21152;&#32780;&#22686;&#21152;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#23454;&#20363;&#65292;&#35777;&#26126;&#20102;&#22522;&#20110;&#22240;&#26524;&#39044;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#30340;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#30340;&#22240;&#26524;&#21457;&#29616;&#22312;&#19968;&#23450;&#33539;&#22260;&#20869;&#26159;&#21487;&#33021;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09305v1 Announce Type: cross Abstract: Causal discovery from time series data encompasses many existing solutions, including those based on deep learning techniques. However, these methods typically do not endorse one of the most prevalent paradigms in deep learning: End-to-end learning. To address this gap, we explore what we call Causal Pretraining. A methodology that aims to learn a direct mapping from multivariate time series to the underlying causal graphs in a supervised manner. Our empirical findings suggest that causal discovery in a supervised manner is possible, assuming that the training and test time series samples share most of their dynamics. More importantly, we found evidence that the performance of Causal Pretraining can increase with data and model size, even if the additional data do not share the same dynamics. Further, we provide examples where causal discovery for real-world data with causally pretrained neural networks is possible within limits. We arg
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#23545;&#27604;&#20102;&#20154;&#31867;&#21644;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#22270;&#20687;&#20998;&#31867;&#20013;&#30340;&#34892;&#20026;&#24046;&#24322;&#65292;&#21457;&#29616;&#20154;&#31867;&#20855;&#26377;&#21363;&#26102;&#27010;&#25324;&#33021;&#21147;&#65292;&#32780;DNNs&#23384;&#22312;&#28382;&#21518;&#27010;&#25324;&#29616;&#35937;&#65292;&#36825;&#34920;&#26126;&#20102;&#34920;&#31034;&#20998;&#27495;&#30340;&#23384;&#22312;&#12290;</title><link>https://arxiv.org/abs/2402.09303</link><description>&lt;p&gt;
&#20154;&#31867;&#20013;&#30340;&#21363;&#26102;&#27010;&#25324;&#19982;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#28382;&#21518;&#27010;&#25324;&#8212;&#8212;&#34920;&#31034;&#20998;&#27495;&#30340;&#35777;&#25454;&#65311;
&lt;/p&gt;
&lt;p&gt;
Immediate generalisation in humans but a generalisation lag in deep neural networks$\unicode{x2014}$evidence for representational divergence?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09303
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#23545;&#27604;&#20102;&#20154;&#31867;&#21644;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#22270;&#20687;&#20998;&#31867;&#20013;&#30340;&#34892;&#20026;&#24046;&#24322;&#65292;&#21457;&#29616;&#20154;&#31867;&#20855;&#26377;&#21363;&#26102;&#27010;&#25324;&#33021;&#21147;&#65292;&#32780;DNNs&#23384;&#22312;&#28382;&#21518;&#27010;&#25324;&#29616;&#35937;&#65292;&#36825;&#34920;&#26126;&#20102;&#34920;&#31034;&#20998;&#27495;&#30340;&#23384;&#22312;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#30340;&#30740;&#31350;&#22312;&#22270;&#20687;&#20998;&#31867;&#39046;&#22495;&#20013;&#23545;&#27604;&#20102;&#20154;&#31867;&#19982;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#30340;&#35768;&#22810;&#34892;&#20026;&#27604;&#36739;&#12290;&#36890;&#24120;&#65292;&#27604;&#36739;&#30740;&#31350;&#20851;&#27880;&#30340;&#26159;&#23398;&#20064;&#36807;&#31243;&#30340;&#26368;&#32456;&#32467;&#26524;&#65292;&#36890;&#36807;&#27979;&#37327;&#21644;&#27604;&#36739;&#30446;&#26631;&#31867;&#21035;&#34920;&#31034;&#30340;&#30456;&#20284;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#34920;&#31034;&#22914;&#20309;&#24418;&#25104;&#21363;&#20854;&#36807;&#31243;&#8212;&#8212;&#21363;&#22312;&#33719;&#21462;&#36807;&#31243;&#20013;&#35266;&#23519;&#21040;&#30340;&#34892;&#20026;&#21464;&#21270;&#21644;&#20013;&#38388;&#38454;&#27573;&#8212;&#8212;&#24448;&#24448;&#23569;&#26377;&#30452;&#25509;&#21644;&#23454;&#35777;&#30340;&#27604;&#36739;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25253;&#21578;&#20102;&#23545;&#20154;&#31867;&#35266;&#23519;&#32773;&#21644;&#19981;&#21516;&#32463;&#20856;&#19982;&#26368;&#26032;&#25216;&#26415;&#30340;DNNs&#20013;&#21487;&#36716;&#31227;&#34920;&#31034;&#26159;&#22914;&#20309;&#34987;&#33719;&#21462;&#30340;&#30340;&#35814;&#32454;&#35843;&#26597;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#21463;&#38480;&#30340;&#30417;&#30563;&#23398;&#20064;&#29615;&#22659;&#65292;&#35813;&#29615;&#22659;&#20013;&#25105;&#20204;&#23545;&#40784;&#20102;&#23398;&#20064;&#30456;&#20851;&#30340;&#21442;&#25968;&#65292;&#22914;&#36215;&#22987;&#28857;&#12289;&#36755;&#20837;&#27169;&#24335;&#12289;&#21487;&#29992;&#36755;&#20837;&#25968;&#25454;&#20197;&#21450;&#25552;&#20379;&#30340;&#21453;&#39304;&#12290;&#22312;&#25972;&#20010;&#23398;&#20064;&#36807;&#31243;&#20013;&#25105;&#20204;&#35780;&#20272;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09303v1 Announce Type: cross Abstract: Recent research has seen many behavioral comparisons between humans and deep neural networks (DNNs) in the domain of image classification. Often, comparison studies focus on the end-result of the learning process by measuring and comparing the similarities in the representations of object categories once they have been formed. However, the process of how these representations emerge$\unicode{x2014}$that is, the behavioral changes and intermediate stages observed during the acquisition$\unicode{x2014}$is less often directly and empirically compared.   Here we report a detailed investigation of how transferable representations are acquired in human observers and various classic and state-of-the-art DNNs. We develop a constrained supervised learning environment in which we align learning-relevant parameters such as starting point, input modality, available input data and the feedback provided. Across the whole learning process we evaluate 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#37096;&#20998;&#30417;&#30563;&#24378;&#21270;&#23398;&#20064;&#65288;PSRL&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#34701;&#21512;&#30417;&#30563;&#21644;&#38750;&#30417;&#30563;&#23398;&#20064;&#26469;&#29983;&#25104;&#26356;&#21487;&#35299;&#37322;&#30340;&#31574;&#30053;&#65292;&#21516;&#26102;&#21033;&#29992;&#29366;&#24577;&#20272;&#35745;&#22120;&#25552;&#21462;&#20986;&#30417;&#30563;&#35821;&#20041;&#29366;&#24577;&#20449;&#24687;&#65292;&#20197;&#21450;&#25429;&#25417;&#28508;&#22312;&#29366;&#24577;&#20449;&#24687;&#12290;</title><link>https://arxiv.org/abs/2402.09290</link><description>&lt;p&gt;
&#36890;&#36807;&#37096;&#20998;&#30417;&#30563;&#24378;&#21270;&#23398;&#20064;&#23398;&#20064;&#21518;&#39564;&#21487;&#35266;&#23519;POMDP&#20013;&#30340;&#21487;&#35299;&#37322;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Learning Interpretable Policies in Hindsight-Observable POMDPs through Partially Supervised Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09290
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#37096;&#20998;&#30417;&#30563;&#24378;&#21270;&#23398;&#20064;&#65288;PSRL&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#34701;&#21512;&#30417;&#30563;&#21644;&#38750;&#30417;&#30563;&#23398;&#20064;&#26469;&#29983;&#25104;&#26356;&#21487;&#35299;&#37322;&#30340;&#31574;&#30053;&#65292;&#21516;&#26102;&#21033;&#29992;&#29366;&#24577;&#20272;&#35745;&#22120;&#25552;&#21462;&#20986;&#30417;&#30563;&#35821;&#20041;&#29366;&#24577;&#20449;&#24687;&#65292;&#20197;&#21450;&#25429;&#25417;&#28508;&#22312;&#29366;&#24577;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22312;&#35270;&#39057;&#28216;&#25103;&#12289;&#26426;&#22120;&#20154;&#25511;&#21046;&#12289;&#33258;&#20027;&#39550;&#39542;&#21644;&#33647;&#29289;&#21457;&#29616;&#31561;&#21508;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#30528;&#30340;&#25104;&#23601;&#12290;&#20294;&#22312;&#37096;&#20998;&#21487;&#35266;&#23519;&#30340;&#39046;&#22495;&#20013;&#65292;&#24120;&#35265;&#30340;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#20174;&#39640;&#32500;&#35266;&#23519;&#65288;&#22914;&#22270;&#20687;&#65289;&#36827;&#34892;&#31471;&#21040;&#31471;&#23398;&#20064;&#65292;&#32780;&#27809;&#26377;&#26126;&#30830;&#25512;&#29702;&#30495;&#23454;&#29366;&#24577;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26367;&#20195;&#26041;&#21521;&#65292;&#24341;&#20837;&#20102;&#37096;&#20998;&#30417;&#30563;&#24378;&#21270;&#23398;&#20064;&#65288;PSRL&#65289;&#26694;&#26550;&#12290;PSRL&#26694;&#26550;&#30340;&#26680;&#24515;&#26159;&#23558;&#30417;&#30563;&#23398;&#20064;&#21644;&#38750;&#30417;&#30563;&#23398;&#20064;&#36827;&#34892;&#34701;&#21512;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#29366;&#24577;&#20272;&#35745;&#22120;&#20174;&#39640;&#32500;&#35266;&#23519;&#20013;&#25552;&#21462;&#20986;&#26377;&#26102;&#22312;&#35757;&#32451;&#26102;&#23436;&#20840;&#35266;&#23519;&#21040;&#30340;&#30417;&#30563;&#35821;&#20041;&#29366;&#24577;&#20449;&#24687;&#12290;&#36825;&#26679;&#21487;&#20197;&#24471;&#21040;&#26356;&#21487;&#35299;&#37322;&#30340;&#31574;&#30053;&#65292;&#23558;&#29366;&#24577;&#39044;&#27979;&#19982;&#25511;&#21046;&#32467;&#21512;&#36215;&#26469;&#12290;&#21516;&#26102;&#65292;&#23427;&#36824;&#25429;&#25417;&#21040;&#19968;&#20010;&#38750;&#30417;&#30563;&#28508;&#22312;&#34920;&#31034;&#12290;&#36825;&#20004;&#32773;-&#35821;&#20041;&#29366;&#24577;&#21644;&#28508;&#22312;&#29366;&#24577;&#28982;&#21518;&#34987;&#34701;&#21512;&#21644;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09290v1 Announce Type: cross Abstract: Deep reinforcement learning has demonstrated remarkable achievements across diverse domains such as video games, robotic control, autonomous driving, and drug discovery. Common methodologies in partially-observable domains largely lean on end-to-end learning from high-dimensional observations, such as images, without explicitly reasoning about true state. We suggest an alternative direction, introducing the Partially Supervised Reinforcement Learning (PSRL) framework. At the heart of PSRL is the fusion of both supervised and unsupervised learning. The approach leverages a state estimator to distill supervised semantic state information from high-dimensional observations which are often fully observable at training time. This yields more interpretable policies that compose state predictions with control. In parallel, it captures an unsupervised latent representation. These two-the semantic state and the latent state-are then fused and ut
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25552;&#20379;&#19968;&#20010;&#27169;&#22411;&#20107;&#23454;&#27169;&#26495;&#65292;&#35813;&#30740;&#31350;&#26088;&#22312;&#22312;&#26538;&#25903;&#26292;&#21147;&#30740;&#31350;&#20013;&#24314;&#31435;AI&#30340;&#20449;&#20219;&#21644;&#36879;&#26126;&#24230;&#65292;&#24182;&#20943;&#23569;&#23545;&#26131;&#21463;&#21093;&#21066;&#20154;&#32676;&#25968;&#25454;&#30340;&#19981;&#20449;&#20219;&#12290;&#27492;&#27169;&#26495;&#20351;&#19968;&#33324;&#29992;&#25143;&#33021;&#22815;&#35780;&#20272;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#21644;&#20559;&#35265;&#12290;</title><link>https://arxiv.org/abs/2402.09286</link><description>&lt;p&gt;
&#33829;&#20859;&#20107;&#23454;&#12289;&#33647;&#29289;&#20107;&#23454;&#21644;&#27169;&#22411;&#20107;&#23454;&#65306;&#23558;AI&#20262;&#29702;&#24212;&#29992;&#20110;&#26538;&#25903;&#26292;&#21147;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Nutrition Facts, Drug Facts, and Model Facts: Putting AI Ethics into Practice in Gun Violence Research
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09286
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25552;&#20379;&#19968;&#20010;&#27169;&#22411;&#20107;&#23454;&#27169;&#26495;&#65292;&#35813;&#30740;&#31350;&#26088;&#22312;&#22312;&#26538;&#25903;&#26292;&#21147;&#30740;&#31350;&#20013;&#24314;&#31435;AI&#30340;&#20449;&#20219;&#21644;&#36879;&#26126;&#24230;&#65292;&#24182;&#20943;&#23569;&#23545;&#26131;&#21463;&#21093;&#21066;&#20154;&#32676;&#25968;&#25454;&#30340;&#19981;&#20449;&#20219;&#12290;&#27492;&#27169;&#26495;&#20351;&#19968;&#33324;&#29992;&#25143;&#33021;&#22815;&#35780;&#20272;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#21644;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22312;&#26538;&#25903;&#26292;&#21147;&#30740;&#31350;&#20013;&#24212;&#29992;AI&#20262;&#29702;&#30340;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#26088;&#22312;&#20943;&#23569;&#23545;&#40657;&#20154;&#21644;&#26377;&#33394;&#20154;&#31181;&#31561;&#26131;&#21463;&#21093;&#21066;&#30340;&#24369;&#21183;&#20154;&#32676;&#25968;&#25454;&#30340;&#19981;&#20449;&#20219;&#12290;&#36890;&#36807;&#25552;&#20986;&#19968;&#20010;&#27169;&#22411;&#20107;&#23454;&#27169;&#26495;&#65292;&#23558;&#20934;&#30830;&#24230;&#21644;&#20154;&#21475;&#32479;&#35745;&#23398;&#20998;&#35299;&#20026;&#26631;&#20934;&#21270;&#21644;&#26368;&#23567;&#22797;&#26434;&#20540;&#65292;&#20351;&#19968;&#33324;&#29992;&#25143;&#33021;&#22815;&#35780;&#20272;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#21644;&#20559;&#35265;&#65292;&#32780;&#19981;&#38656;&#35201;&#28145;&#20837;&#30740;&#31350;&#25216;&#26415;&#27169;&#22411;&#25991;&#26723;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09286v1 Announce Type: new Abstract: Objective: Firearm injury research necessitates using data from often-exploited vulnerable populations of Black and Brown Americans. In order to minimize distrust, this study provides a framework for establishing AI trust and transparency with the general population. Methods: We propose a Model Facts template that is easily extendable and decomposes accuracy and demographics into standardized and minimally complex values. This framework allows general users to assess the validity and biases of a model without diving into technical model documentation. Examples: We apply the Model Facts template on two previously published models, a violence risk identification model and a suicide risk prediction model. We demonstrate the ease of accessing the appropriate information when the data is structured appropriately. Discussion: The Model Facts template is limited in its current form to human based data and biases. Like nutrition facts, it also wi
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#35757;&#32451;&#38598;&#19978;&#35745;&#31639;&#30340;&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;&#29305;&#24449;&#20998;&#26512;&#19982;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#19978;&#35745;&#31639;&#30340;Hessian&#30697;&#38453;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#20108;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#26368;&#20248;&#31867;&#21035;&#21487;&#20998;&#24615;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#26368;&#22823;&#21270;&#31867;&#38388;&#24179;&#22343;&#36317;&#31163;&#21644;&#26368;&#23567;&#21270;&#31867;&#20869;&#26041;&#24046;&#65292;&#20197;&#21450;&#23558;&#25968;&#25454;&#25237;&#24433;&#21040;&#20004;&#20010;&#30697;&#38453;&#30340;&#26368;&#30456;&#20851;&#29305;&#24449;&#26041;&#21521;&#30340;&#32452;&#21512;&#31354;&#38388;&#26469;&#23454;&#29616;&#26368;&#20248;&#31867;&#21035;&#21487;&#20998;&#24615;&#12290;&#23454;&#35777;&#39564;&#35777;&#26174;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.09281</link><description>&lt;p&gt;
&#25552;&#21319;&#20108;&#20998;&#31867;&#38382;&#39064;&#30340;&#21327;&#26041;&#24046;&#21644;Hessian&#30697;&#38453;&#30340;&#21327;&#21516;&#29305;&#24449;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Synergistic eigenanalysis of covariance and Hessian matrices for enhanced binary classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09281
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#35757;&#32451;&#38598;&#19978;&#35745;&#31639;&#30340;&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;&#29305;&#24449;&#20998;&#26512;&#19982;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#19978;&#35745;&#31639;&#30340;Hessian&#30697;&#38453;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#20108;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#26368;&#20248;&#31867;&#21035;&#21487;&#20998;&#24615;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#26368;&#22823;&#21270;&#31867;&#38388;&#24179;&#22343;&#36317;&#31163;&#21644;&#26368;&#23567;&#21270;&#31867;&#20869;&#26041;&#24046;&#65292;&#20197;&#21450;&#23558;&#25968;&#25454;&#25237;&#24433;&#21040;&#20004;&#20010;&#30697;&#38453;&#30340;&#26368;&#30456;&#20851;&#29305;&#24449;&#26041;&#21521;&#30340;&#32452;&#21512;&#31354;&#38388;&#26469;&#23454;&#29616;&#26368;&#20248;&#31867;&#21035;&#21487;&#20998;&#24615;&#12290;&#23454;&#35777;&#39564;&#35777;&#26174;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20998;&#31867;&#38382;&#39064;&#20013;&#65292;&#21327;&#26041;&#24046;&#21644;Hessian&#30697;&#38453;&#20998;&#21035;&#34987;&#21333;&#29420;&#20998;&#26512;&#65292;&#20294;&#26159;&#23558;&#36825;&#20123;&#30697;&#38453;&#38598;&#25104;&#36215;&#26469;&#21487;&#20197;&#22686;&#24378;&#23427;&#20204;&#22312;&#25552;&#39640;&#20998;&#31867;&#24615;&#33021;&#26041;&#38754;&#30340;&#32508;&#21512;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#23558;&#35757;&#32451;&#38598;&#19978;&#35745;&#31639;&#30340;&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;&#29305;&#24449;&#20998;&#26512;&#19982;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#19978;&#35745;&#31639;&#30340;Hessian&#30697;&#38453;&#30456;&#32467;&#21512;&#65292;&#20197;&#23454;&#29616;&#20108;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#26368;&#20248;&#31867;&#21035;&#21487;&#20998;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#24418;&#24335;&#21270;&#35777;&#26126;&#35777;&#26126;&#20102;&#23427;&#21487;&#20197;&#26368;&#22823;&#21270;&#31867;&#38388;&#24179;&#22343;&#36317;&#31163;&#24182;&#26368;&#23567;&#21270;&#31867;&#20869;&#26041;&#24046;&#12290;&#36890;&#36807;&#23558;&#25968;&#25454;&#25237;&#24433;&#21040;&#20004;&#20010;&#30697;&#38453;&#30340;&#26368;&#30456;&#20851;&#29305;&#24449;&#26041;&#21521;&#30340;&#32452;&#21512;&#31354;&#38388;&#20013;&#65292;&#25105;&#20204;&#25353;&#29031;&#32447;&#24615;&#21028;&#21035;&#20998;&#26512;&#65288;LDA&#65289;&#30340;&#26631;&#20934;&#23454;&#29616;&#20102;&#26368;&#20248;&#31867;&#21035;&#21487;&#20998;&#24615;&#12290;&#23545;&#31070;&#32463;&#32593;&#32476;&#21644;&#20581;&#24247;&#25968;&#25454;&#38598;&#30340;&#23454;&#35777;&#39564;&#35777;&#22987;&#32456;&#25903;&#25345;&#25105;&#20204;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09281v1 Announce Type: cross Abstract: Covariance and Hessian matrices have been analyzed separately in the literature for classification problems. However, integrating these matrices has the potential to enhance their combined power in improving classification performance. We present a novel approach that combines the eigenanalysis of a covariance matrix evaluated on a training set with a Hessian matrix evaluated on a deep learning model to achieve optimal class separability in binary classification tasks. Our approach is substantiated by formal proofs that establish its capability to maximize between-class mean distance and minimize within-class variances. By projecting data into the combined space of the most relevant eigendirections from both matrices, we achieve optimal class separability as per the linear discriminant analysis (LDA) criteria. Empirical validation across neural and health datasets consistently supports our theoretical framework and demonstrates that our
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20010;&#24615;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#27604;&#36739;&#24494;&#35843;&#21644;&#38646;&#26679;&#26412;&#25512;&#29702;&#30340;&#26041;&#27861;&#65292;&#22312;&#20027;&#35266;&#20219;&#21153;&#20013;&#21457;&#29616;&#20010;&#24615;&#21270;&#24494;&#35843;&#33021;&#25552;&#39640;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#22312;&#24773;&#24863;&#35782;&#21035;&#21644;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#26041;&#38754;&#20063;&#33719;&#24471;&#20102;&#19968;&#33268;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2402.09269</link><description>&lt;p&gt;
&#20010;&#24615;&#21270;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Personalized Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09269
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20010;&#24615;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#27604;&#36739;&#24494;&#35843;&#21644;&#38646;&#26679;&#26412;&#25512;&#29702;&#30340;&#26041;&#27861;&#65292;&#22312;&#20027;&#35266;&#20219;&#21153;&#20013;&#21457;&#29616;&#20010;&#24615;&#21270;&#24494;&#35843;&#33021;&#25552;&#39640;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#22312;&#24773;&#24863;&#35782;&#21035;&#21644;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#26041;&#38754;&#20063;&#33719;&#24471;&#20102;&#19968;&#33268;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#36890;&#29992;&#24615;&#22312;&#38656;&#35201;&#20010;&#24615;&#21270;&#22238;&#24212;&#30340;&#22330;&#26223;&#65288;&#22914;&#25512;&#33616;&#31995;&#32479;&#21644;&#32842;&#22825;&#26426;&#22120;&#20154;&#65289;&#20013;&#23384;&#22312;&#19968;&#23450;&#30340;&#23616;&#38480;&#24615;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20010;&#24615;&#21270;LLM&#30340;&#26041;&#27861;&#65292;&#27604;&#36739;&#20102;&#24494;&#35843;&#21644;&#38646;&#26679;&#26412;&#25512;&#29702;&#26041;&#27861;&#22312;&#20027;&#35266;&#20219;&#21153;&#20013;&#30340;&#25928;&#26524;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#38750;&#20010;&#24615;&#21270;&#27169;&#22411;&#30456;&#27604;&#65292;&#20010;&#24615;&#21270;&#24494;&#35843;&#25913;&#21892;&#20102;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#22312;&#24773;&#24863;&#35782;&#21035;&#21644;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#20010;&#24615;&#21270;&#26041;&#27861;&#22312;&#19981;&#21516;&#30340;LLM&#26550;&#26500;&#19978;&#33719;&#24471;&#20102;&#19968;&#33268;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#36825;&#20123;&#21457;&#29616;&#24378;&#35843;&#20102;&#22312;&#20027;&#35266;&#25991;&#26412;&#29702;&#35299;&#20219;&#21153;&#20013;&#25552;&#21319;LLM&#33021;&#21147;&#30340;&#20010;&#24615;&#21270;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09269v1 Announce Type: cross Abstract: Large language models (LLMs) have significantly advanced Natural Language Processing (NLP) tasks in recent years. However, their universal nature poses limitations in scenarios requiring personalized responses, such as recommendation systems and chatbots. This paper investigates methods to personalize LLMs, comparing fine-tuning and zero-shot reasoning approaches on subjective tasks. Results demonstrate that personalized fine-tuning improves model reasoning compared to non-personalized models. Experiments on datasets for emotion recognition and hate speech detection show consistent performance gains with personalized methods across different LLM architectures. These findings underscore the importance of personalization for enhancing LLM capabilities in subjective text perception tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#33258;&#21160;&#26657;&#20934;&#23454;&#20107;&#24615;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#25105;&#35780;&#20272;&#33021;&#21147;&#65292;&#24341;&#23548;&#27169;&#22411;&#21521;&#23454;&#20107;&#24615;&#38752;&#36817;&#65292;&#24182;&#25913;&#21892;&#27169;&#22411;&#30340;&#32622;&#20449;&#20272;&#35745;&#21644;&#26657;&#20934;&#12290;</title><link>https://arxiv.org/abs/2402.09267</link><description>&lt;p&gt;
&#33258;&#21160;&#26657;&#20934;&#23454;&#20107;&#24615;&#65306;&#36890;&#36807;&#33258;&#35780;&#20943;&#32531;LLMs&#20013;&#30340;&#24187;&#35273;
&lt;/p&gt;
&lt;p&gt;
Self-Alignment for Factuality: Mitigating Hallucinations in LLMs via Self-Evaluation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09267
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#33258;&#21160;&#26657;&#20934;&#23454;&#20107;&#24615;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#25105;&#35780;&#20272;&#33021;&#21147;&#65292;&#24341;&#23548;&#27169;&#22411;&#21521;&#23454;&#20107;&#24615;&#38752;&#36817;&#65292;&#24182;&#25913;&#21892;&#27169;&#22411;&#30340;&#32622;&#20449;&#20272;&#35745;&#21644;&#26657;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#26174;&#31034;&#20986;&#36234;&#26469;&#36234;&#25509;&#36817;&#20154;&#31867;&#30340;&#33021;&#21147;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20107;&#23454;&#20934;&#30830;&#24615;&#26041;&#38754;&#65288;&#21363;&#8220;&#24187;&#35273;&#8221;&#65289;&#24448;&#24448;&#23384;&#22312;&#22256;&#38590;&#65292;&#21363;&#20351;&#23427;&#20204;&#20855;&#26377;&#30456;&#20851;&#30340;&#30693;&#35782;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#24187;&#35273;&#38382;&#39064;&#65292;&#30446;&#21069;&#30340;&#26041;&#27861;&#36890;&#24120;&#38656;&#35201;&#39640;&#36136;&#37327;&#30340;&#20154;&#24037;&#20107;&#23454;&#24615;&#27880;&#37322;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#33258;&#21160;&#26657;&#20934;&#23454;&#20107;&#24615;&#65292;&#21363;&#21033;&#29992;LLM&#30340;&#33258;&#25105;&#35780;&#20272;&#33021;&#21147;&#25552;&#20379;&#35757;&#32451;&#20449;&#21495;&#65292;&#23558;&#27169;&#22411;&#24341;&#23548;&#21521;&#23454;&#20107;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#33258;&#25105;&#35780;&#20272;&#32452;&#20214;Self-Eval&#32435;&#20837;&#21040;LLM&#20013;&#65292;&#20197;&#20165;&#22522;&#20110;&#20854;&#20869;&#37096;&#30693;&#35782;&#39564;&#35777;&#20854;&#33258;&#24049;&#29983;&#25104;&#30340;&#22238;&#22797;&#30340;&#23454;&#20107;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#33258;&#25105;&#30693;&#35782;&#35843;&#25972;&#65288;SK-Tuning&#65289;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#33258;&#25105;&#35780;&#20272;&#33021;&#21147;&#65292;&#25913;&#21892;&#27169;&#22411;&#30340;&#32622;&#20449;&#20272;&#35745;&#21644;&#26657;&#20934;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#36825;&#20123;&#33258;&#25105;&#27880;&#37322;&#30340;&#22238;&#22797;&#36890;&#36807;&#30452;&#25509;&#20248;&#21270;&#20559;&#22909;&#31639;&#27861;&#23545;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09267v1 Announce Type: cross Abstract: Despite showing increasingly human-like abilities, large language models (LLMs) often struggle with factual inaccuracies, i.e. "hallucinations", even when they hold relevant knowledge. To address these hallucinations, current approaches typically necessitate high-quality human factuality annotations. In this work, we explore Self-Alignment for Factuality, where we leverage the self-evaluation capability of an LLM to provide training signals that steer the model towards factuality. Specifically, we incorporate Self-Eval, a self-evaluation component, to prompt an LLM to validate the factuality of its own generated responses solely based on its internal knowledge. Additionally, we design Self-Knowledge Tuning (SK-Tuning) to augment the LLM's self-evaluation ability by improving the model's confidence estimation and calibration. We then utilize these self-annotated responses to fine-tune the model via Direct Preference Optimization algorith
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#21487;&#20197;&#39044;&#27979;&#33026;&#28342;&#24615;&#29983;&#29289;&#27602;&#32032;&#24341;&#36215;&#30340;&#39044;&#38450;&#24615;&#20851;&#38381;&#65292;&#20026;&#28129;&#27700;&#34532;&#20859;&#27542;&#34892;&#19994;&#25552;&#20379;&#20102;&#25903;&#25345;&#12290;</title><link>https://arxiv.org/abs/2402.09266</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22312;&#31649;&#29702;&#33026;&#28342;&#24615;&#29983;&#29289;&#27602;&#32032;&#24341;&#36215;&#30340;&#39044;&#38450;&#24615;&#20851;&#38381;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Machine Learning in management of precautionary closures caused by lipophilic biotoxins
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09266
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#21487;&#20197;&#39044;&#27979;&#33026;&#28342;&#24615;&#29983;&#29289;&#27602;&#32032;&#24341;&#36215;&#30340;&#39044;&#38450;&#24615;&#20851;&#38381;&#65292;&#20026;&#28129;&#27700;&#34532;&#20859;&#27542;&#34892;&#19994;&#25552;&#20379;&#20102;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28129;&#27700;&#34532;&#20859;&#27542;&#26159;&#26368;&#37325;&#35201;&#30340;&#27700;&#20135;&#20859;&#27542;&#34892;&#19994;&#20043;&#19968;&#12290;&#20027;&#35201;&#30340;&#39118;&#38505;&#26159;&#26377;&#23475;&#34299;&#31867;&#27700;&#21326;&#65288;HABs&#65289;&#65292;&#23545;&#20154;&#31867;&#39135;&#29992;&#26500;&#25104;&#20102;&#39118;&#38505;&#12290;&#22312;&#21152;&#21033;&#35199;&#20122;&#65292;&#35199;&#29677;&#29273;&#26159;&#28129;&#27700;&#34532;&#30340;&#20027;&#35201;&#29983;&#20135;&#22269;&#65292;&#29983;&#20135;&#21306;&#22495;&#30340;&#24320;&#38381;&#26159;&#30001;&#30417;&#27979;&#35745;&#21010;&#25511;&#21046;&#30340;&#12290;&#38500;&#20102;&#22240;&#26377;&#27602;&#29289;&#36136;&#36229;&#36807;&#27861;&#23450;&#38376;&#27099;&#32780;&#23548;&#33268;&#30340;&#20851;&#38381;&#22806;&#65292;&#22312;&#32570;&#20047;&#30830;&#35748;&#21462;&#26679;&#21644;&#23384;&#22312;&#39118;&#38505;&#22240;&#32032;&#30340;&#24773;&#20917;&#19979;&#65292;&#21487;&#33021;&#20250;&#23454;&#26045;&#39044;&#38450;&#24615;&#20851;&#38381;&#12290;&#36825;&#20123;&#20915;&#31574;&#26159;&#30001;&#27809;&#26377;&#25903;&#25345;&#25110;&#27491;&#24335;&#21270;&#22522;&#20110;&#32463;&#39564;&#30340;&#19987;&#23478;&#26469;&#20570;&#20986;&#30340;&#12290;&#22240;&#27492;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#33021;&#22815;&#25903;&#25345;&#24212;&#29992;&#39044;&#38450;&#24615;&#20851;&#38381;&#30340;&#39044;&#27979;&#27169;&#22411;&#12290;&#36890;&#36807;&#23454;&#29616;97.34%&#30340;&#25935;&#24863;&#24615;&#12289;91.83%&#30340;&#20934;&#30830;&#24615;&#21644;0.75&#30340;kappa&#25351;&#25968;&#31561;&#25351;&#26631;&#65292;kNN&#31639;&#27861;&#25552;&#20379;&#20102;&#26368;&#20339;&#32467;&#26524;&#12290;&#36825;&#20351;&#24471;&#21487;&#20197;&#21019;&#24314;&#19968;&#20010;&#33021;&#21147;&#24378;&#22823;&#30340;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09266v1 Announce Type: new Abstract: Mussel farming is one of the most important aquaculture industries. The main risk to mussel farming is harmful algal blooms (HABs), which pose a risk to human consumption. In Galicia, the Spanish main producer of cultivated mussels, the opening and closing of the production areas is controlled by a monitoring program. In addition to the closures resulting from the presence of toxicity exceeding the legal threshold, in the absence of a confirmatory sampling and the existence of risk factors, precautionary closures may be applied. These decisions are made by experts without the support or formalisation of the experience on which they are based. Therefore, this work proposes a predictive model capable of supporting the application of precautionary closures. Achieving sensitivity, accuracy and kappa index values of 97.34%, 91.83% and 0.75 respectively, the kNN algorithm has provided the best results. This allows the creation of a system capab
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#22270;&#25968;&#25454;&#24211;&#19978;&#20351;&#29992;Reg-GXPath&#34920;&#36798;&#24335;&#20316;&#20026;&#23436;&#25972;&#24615;&#32422;&#26463;&#65292;&#35745;&#31639;&#21253;&#21547;&#25968;&#25454;&#20540;&#30340;&#20248;&#20808;&#20462;&#22797;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#20960;&#31181;&#20559;&#22909;&#20934;&#21017;&#12290;</title><link>https://arxiv.org/abs/2402.09265</link><description>&lt;p&gt;
&#25968;&#25454;&#22270;&#19978;&#39318;&#36873;&#23376;&#38598;&#20462;&#22797;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;
&lt;/p&gt;
&lt;p&gt;
Computational Complexity of Preferred Subset Repairs on Data-Graphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09265
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#22270;&#25968;&#25454;&#24211;&#19978;&#20351;&#29992;Reg-GXPath&#34920;&#36798;&#24335;&#20316;&#20026;&#23436;&#25972;&#24615;&#32422;&#26463;&#65292;&#35745;&#31639;&#21253;&#21547;&#25968;&#25454;&#20540;&#30340;&#20248;&#20808;&#20462;&#22797;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#20960;&#31181;&#20559;&#22909;&#20934;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#24211;&#29702;&#35770;&#21644;&#30693;&#35782;&#34920;&#31034;&#19982;&#25512;&#29702;&#39046;&#22495;&#19968;&#30452;&#20197;&#26469;&#37117;&#23384;&#22312;&#35299;&#20915;&#19981;&#19968;&#33268;&#30693;&#35782;&#24211;&#30340;&#38382;&#39064;&#65292;&#24182;&#19988;&#29305;&#21035;&#20851;&#27880;&#32467;&#26500;&#21270;&#25968;&#25454;&#30340;&#35282;&#24230;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#29616;&#23454;&#39046;&#22495;&#20013;&#21487;&#29992;&#25968;&#25454;&#21464;&#24471;&#26356;&#21152;&#22797;&#26434;&#21644;&#20114;&#30456;&#20851;&#32852;&#65292;&#33258;&#28982;&#32780;&#28982;&#22320;&#38656;&#35201;&#24320;&#21457;&#26032;&#31867;&#22411;&#30340;&#23384;&#20648;&#24211;&#12289;&#34920;&#31034;&#35821;&#35328;&#21644;&#35821;&#20041;&#20197;&#23454;&#29616;&#26356;&#21512;&#36866;&#30340;&#26597;&#35810;&#21644;&#25512;&#29702;&#26041;&#24335;&#12290;&#22270;&#25968;&#25454;&#24211;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#34920;&#31034;&#21322;&#32467;&#26500;&#21270;&#25968;&#25454;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#19988;&#33021;&#22815;&#39640;&#25928;&#22320;&#22788;&#29702;&#21644;&#26597;&#35810;&#36825;&#20123;&#36830;&#25509;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#20351;&#29992;&#22522;&#20110;Reg-GXPath&#34920;&#36798;&#24335;&#30340;&#19968;&#33268;&#24615;&#27010;&#24565;&#20316;&#20026;&#23436;&#25972;&#24615;&#32422;&#26463;&#65292;&#35745;&#31639;&#21253;&#21547;&#25968;&#25454;&#20540;&#30340;&#22270;&#25968;&#25454;&#24211;&#19978;&#30340;&#20248;&#20808;&#20462;&#22797;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#26631;&#20934;&#23376;&#38598;&#20462;&#22797;&#35821;&#20041;&#30340;&#20960;&#31181;&#20559;&#22909;&#20934;&#21017;&#65292;&#21253;&#25324;&#26435;&#37325;&#21644;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09265v1 Announce Type: cross Abstract: The problem of repairing inconsistent knowledge bases has a long history within the communities of database theory and knowledge representation and reasoning, especially from the perspective of structured data. However, as the data available in real-world domains becomes more complex and interconnected, the need naturally arises for developing new types of repositories, representation languages, and semantics, to allow for more suitable ways to query and reason about it. Graph databases provide an effective way to represent relationships among semi-structured data, and allow processing and querying these connections efficiently. In this work, we focus on the problem of computing prioritized repairs over graph databases with data values, using a notion of consistency based on Reg-GXPath expressions as integrity constraints. We present several preference criteria based on the standard subset repair semantics, incorporating weights, multis
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23616;&#37096;&#30340;&#12289;&#19982;&#27169;&#22411;&#26080;&#20851;&#30340;&#29992;&#20110;&#25991;&#26412;&#29983;&#25104;&#30340;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;SyntaxShap&#65292;&#20854;&#36890;&#36807;&#32771;&#34385;&#25991;&#26412;&#25968;&#25454;&#20013;&#30340;&#35821;&#27861;&#26469;&#25193;&#23637;Shapley&#20540;&#20197;&#35299;&#37322;&#24207;&#21015;&#21040;&#24207;&#21015;&#20219;&#21153;&#12290;&#36890;&#36807;&#37319;&#29992;&#21338;&#24328;&#35770;&#26041;&#27861;&#65292;SyntaxShap&#21482;&#32771;&#34385;&#30001;&#20381;&#36182;&#26641;&#32422;&#26463;&#30340;&#32852;&#30431;&#65292;&#19982;&#20854;&#20182;&#26368;&#26032;&#30340;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#30456;&#27604;&#20855;&#26377;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.09259</link><description>&lt;p&gt;
SyntaxShap&#65306;&#29992;&#20110;&#25991;&#26412;&#29983;&#25104;&#30340;&#35821;&#27861;&#24863;&#30693;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
SyntaxShap: Syntax-aware Explainability Method for Text Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09259
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23616;&#37096;&#30340;&#12289;&#19982;&#27169;&#22411;&#26080;&#20851;&#30340;&#29992;&#20110;&#25991;&#26412;&#29983;&#25104;&#30340;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;SyntaxShap&#65292;&#20854;&#36890;&#36807;&#32771;&#34385;&#25991;&#26412;&#25968;&#25454;&#20013;&#30340;&#35821;&#27861;&#26469;&#25193;&#23637;Shapley&#20540;&#20197;&#35299;&#37322;&#24207;&#21015;&#21040;&#24207;&#21015;&#20219;&#21153;&#12290;&#36890;&#36807;&#37319;&#29992;&#21338;&#24328;&#35770;&#26041;&#27861;&#65292;SyntaxShap&#21482;&#32771;&#34385;&#30001;&#20381;&#36182;&#26641;&#32422;&#26463;&#30340;&#32852;&#30431;&#65292;&#19982;&#20854;&#20182;&#26368;&#26032;&#30340;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#30456;&#27604;&#20855;&#26377;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#22312;&#23433;&#20840;&#20851;&#38190;&#39046;&#22495;&#20013;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#28508;&#21147;&#65292;&#25105;&#20204;&#38656;&#35201;&#30830;&#20445;&#20854;&#39044;&#27979;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#21463;&#21040;&#20102;&#37325;&#35201;&#20851;&#27880;&#65292;&#20294;&#20173;&#26377;&#19968;&#20010;&#23578;&#26410;&#25506;&#32034;&#30340;&#39046;&#22495;&#65292;&#21363;&#20351;&#29992;&#38024;&#23545;&#25991;&#26412;&#25968;&#25454;&#37327;&#36523;&#23450;&#21046;&#30340;&#26041;&#27861;&#35299;&#37322;&#24207;&#21015;&#21040;&#24207;&#21015;&#20219;&#21153;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;SyntaxShap&#65292;&#19968;&#31181;&#23616;&#37096;&#30340;&#12289;&#19982;&#27169;&#22411;&#26080;&#20851;&#30340;&#29992;&#20110;&#25991;&#26412;&#29983;&#25104;&#30340;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#65292;&#23427;&#32771;&#34385;&#20102;&#25991;&#26412;&#25968;&#25454;&#20013;&#30340;&#35821;&#27861;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#23558;Shapley&#20540;&#25193;&#23637;&#21040;&#32771;&#34385;&#22522;&#20110;&#35299;&#26512;&#30340;&#21477;&#27861;&#20381;&#36182;&#20851;&#31995;&#12290;&#37319;&#29992;&#21338;&#24328;&#35770;&#26041;&#27861;&#65292;SyntaxShap&#21482;&#32771;&#34385;&#30001;&#20381;&#36182;&#26641;&#32422;&#26463;&#30340;&#32852;&#30431;&#12290;&#25105;&#20204;&#37319;&#29992;&#22522;&#20110;&#27169;&#22411;&#30340;&#35780;&#20272;&#26469;&#27604;&#36739;SyntaxShap&#21450;&#20854;&#21152;&#26435;&#24418;&#24335;&#19982;&#38024;&#23545;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#30340;&#26368;&#26032;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#65292;&#20351;&#29992;&#21253;&#25324;&#24544;&#23454;&#24230;&#12289;&#22797;&#26434;&#24230;&#12289;&#36830;&#36143;&#24615;&#21644;&#35299;&#37322;&#19982;&#35821;&#20041;&#19968;&#33268;&#24615;&#30340;&#22810;&#26679;&#21270;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09259v1 Announce Type: cross Abstract: To harness the power of large language models in safety-critical domains we need to ensure the explainability of their predictions. However, despite the significant attention to model interpretability, there remains an unexplored domain in explaining sequence-to-sequence tasks using methods tailored for textual data. This paper introduces SyntaxShap, a local, model-agnostic explainability method for text generation that takes into consideration the syntax in the text data. The presented work extends Shapley values to account for parsing-based syntactic dependencies. Taking a game theoric approach, SyntaxShap only considers coalitions constraint by the dependency tree. We adopt a model-based evaluation to compare SyntaxShap and its weighted form to state-of-the-art explainability methods adapted to text generation tasks, using diverse metrics including faithfulness, complexity, coherency, and semantic alignment of the explanations to the
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#30005;&#23376;&#21704;&#23494;&#39039;&#37327;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#26469;&#33258;Materials Project&#30340;&#31532;&#19968;&#24615;&#21407;&#29702;DFT&#35745;&#31639;&#30340;&#21704;&#23494;&#39039;&#30697;&#38453;&#36827;&#34892;&#35757;&#32451;&#65292;&#21487;&#20197;&#39044;&#27979;&#25972;&#20010;&#21608;&#26399;&#34920;&#20013;&#21253;&#25324;&#22797;&#26434;&#22810;&#20803;&#32032;&#31995;&#32479;&#22312;&#20869;&#30340;&#30005;&#23376;&#32467;&#26500;&#12290;</title><link>https://arxiv.org/abs/2402.09251</link><description>&lt;p&gt;
&#26448;&#26009;&#30340;&#36890;&#29992;&#26426;&#22120;&#23398;&#20064;Kohn-Sham&#21704;&#23494;&#39039;&#37327;
&lt;/p&gt;
&lt;p&gt;
Universal Machine Learning Kohn-Sham Hamiltonian for Materials
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09251
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#30005;&#23376;&#21704;&#23494;&#39039;&#37327;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#26469;&#33258;Materials Project&#30340;&#31532;&#19968;&#24615;&#21407;&#29702;DFT&#35745;&#31639;&#30340;&#21704;&#23494;&#39039;&#30697;&#38453;&#36827;&#34892;&#35757;&#32451;&#65292;&#21487;&#20197;&#39044;&#27979;&#25972;&#20010;&#21608;&#26399;&#34920;&#20013;&#21253;&#25324;&#22797;&#26434;&#22810;&#20803;&#32032;&#31995;&#32479;&#22312;&#20869;&#30340;&#30005;&#23376;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#23494;&#24230;&#27867;&#20989;&#29702;&#35770;(DFT)&#22312;&#30005;&#23376;&#32467;&#26500;&#35745;&#31639;&#20013;&#26159;&#19968;&#31181;&#26222;&#36941;&#30340;&#35745;&#31639;&#26041;&#27861;&#65292;&#20294;&#20854;&#35745;&#31639;&#38656;&#27714;&#21644;&#21487;&#25193;&#23637;&#24615;&#38480;&#21046;&#20173;&#28982;&#23384;&#22312;&#12290;&#26368;&#36817;&#65292;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#26469;&#21442;&#25968;&#21270;Kohn-Sham DFT&#21704;&#23494;&#39039;&#37327;&#24050;&#32463;&#25104;&#20026;&#21152;&#36895;&#30005;&#23376;&#32467;&#26500;&#35745;&#31639;&#30340;&#26377;&#24076;&#26395;&#30340;&#36884;&#24452;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#20173;&#28982;&#23384;&#22312;&#19968;&#20123;&#25361;&#25112;&#65292;&#22914;&#20026;&#20102;&#25506;&#32034;&#26032;&#31995;&#32479;&#32780;&#38656;&#35201;&#35745;&#31639;&#22823;&#37327;&#30340;DFT&#35757;&#32451;&#25968;&#25454;&#21644;&#24314;&#31435;&#20934;&#30830;&#30340;&#22810;&#20803;&#32032;&#26448;&#26009;&#30340;ML&#27169;&#22411;&#30340;&#22797;&#26434;&#24615;&#12290;&#38024;&#23545;&#36825;&#20123;&#38556;&#30861;&#65292;&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#23545;&#26469;&#33258;Materials Project&#30340;&#31532;&#19968;&#24615;&#21407;&#29702;DFT&#35745;&#31639;&#30340;&#21704;&#23494;&#39039;&#30697;&#38453;&#36827;&#34892;&#35757;&#32451;&#30340;&#36890;&#29992;&#30005;&#23376;&#21704;&#23494;&#39039;&#37327;&#27169;&#22411;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#23427;&#22312;&#25972;&#20010;&#21608;&#26399;&#34920;&#20013;&#39044;&#27979;&#30005;&#23376;&#32467;&#26500;&#30340;&#26222;&#36866;&#24615;&#65292;&#21253;&#25324;&#22797;&#26434;&#30340;&#22810;&#20803;&#32032;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09251v1 Announce Type: cross Abstract: While density functional theory (DFT) serves as a prevalent computational approach in electronic structure calculations, its computational demands and scalability limitations persist. Recently, leveraging neural networks to parameterize the Kohn-Sham DFT Hamiltonian has emerged as a promising avenue for accelerating electronic structure computations. Despite advancements, challenges such as the necessity for computing extensive DFT training data to explore new systems and the complexity of establishing accurate ML models for multi-elemental materials still exist. Addressing these hurdles, this study introduces a universal electronic Hamiltonian model trained on Hamiltonian matrices obtained from first-principles DFT calculations of nearly all crystal structures on the Materials Project. We demonstrate its generality in predicting electronic structures across the whole periodic table, including complex multi-elemental systems. By offerin
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;Stackelberg&#21338;&#24328;&#20013;&#20248;&#21270;&#20247;&#22810;&#26426;&#22120;&#20154;&#30340;&#34892;&#21160;&#39034;&#24207;&#30340;&#38382;&#39064;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#39640;&#25928;&#20934;&#30830;&#30340;&#31639;&#27861;(B&amp;P)&#26469;&#27714;&#35299;&#30456;&#20851;&#30340;&#20248;&#21270;&#38382;&#39064;&#21644;&#22343;&#34913;&#12290;&#35813;&#31639;&#27861;&#20855;&#26377;&#24191;&#27867;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.09246</link><description>&lt;p&gt;
&#35841;&#20808;&#34892;&#21160;&#65311;&#20248;&#21270;Stackelberg&#21338;&#24328;&#20013;&#20247;&#22810;&#26426;&#22120;&#20154;&#30340;&#34892;&#21160;&#39034;&#24207;
&lt;/p&gt;
&lt;p&gt;
Who Plays First? Optimizing the Order of Play in Stackelberg Games with Many Robots
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09246
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;Stackelberg&#21338;&#24328;&#20013;&#20248;&#21270;&#20247;&#22810;&#26426;&#22120;&#20154;&#30340;&#34892;&#21160;&#39034;&#24207;&#30340;&#38382;&#39064;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#39640;&#25928;&#20934;&#30830;&#30340;&#31639;&#27861;(B&amp;P)&#26469;&#27714;&#35299;&#30456;&#20851;&#30340;&#20248;&#21270;&#38382;&#39064;&#21644;&#22343;&#34913;&#12290;&#35813;&#31639;&#27861;&#20855;&#26377;&#24191;&#27867;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#35745;&#31639;&#22810;&#26234;&#33021;&#20307;&#31354;&#38388;&#23548;&#33322;&#38382;&#39064;&#30340;&#31038;&#20250;&#26368;&#20248;&#34892;&#21160;&#39034;&#24207;&#30340;&#38382;&#39064;&#65292;&#21363;&#26234;&#33021;&#20307;&#20915;&#31574;&#39034;&#24207;&#65292;&#20197;&#21450;&#19982;&#20043;&#30456;&#20851;&#30340;N&#20154;Stackelberg&#36712;&#36857;&#21338;&#24328;&#30340;&#22343;&#34913;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#38382;&#39064;&#24314;&#27169;&#20026;&#19968;&#20010;&#28151;&#21512;&#25972;&#25968;&#20248;&#21270;&#38382;&#39064;&#65292;&#28041;&#21450;&#21040;&#25152;&#26377;&#21487;&#33021;&#30340;&#34892;&#21160;&#39034;&#24207;&#30340;Stackelberg&#21338;&#24328;&#31354;&#38388;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Branch and Play (B&amp;P)&#65292;&#36825;&#26159;&#19968;&#20010;&#39640;&#25928;&#19988;&#20934;&#30830;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#25910;&#25947;&#21040;&#31038;&#20250;&#26368;&#20248;&#34892;&#21160;&#39034;&#24207;&#21450;&#20854;Stackelberg&#22343;&#34913;&#12290;&#20316;&#20026;B&amp;P&#30340;&#19968;&#20010;&#23376;&#20363;&#31243;&#65292;&#25105;&#20204;&#25552;&#20986;&#24182;&#25193;&#23637;&#20102;&#39034;&#24207;&#36712;&#36857;&#35268;&#21010;&#65292;&#21363;&#19968;&#31181;&#27969;&#34892;&#30340;&#22810;&#26234;&#33021;&#20307;&#25511;&#21046;&#26041;&#27861;&#65292;&#20197;&#20415;&#20026;&#20219;&#20309;&#32473;&#23450;&#30340;&#34892;&#21160;&#39034;&#24207;&#21487;&#25193;&#23637;&#22320;&#35745;&#31639;&#26377;&#25928;&#30340;&#26412;&#22320;Stackelberg&#22343;&#34913;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;B&amp;P&#22312;&#21327;&#35843;&#31354;&#20013;&#20132;&#36890;&#25511;&#21046;&#12289;&#32676;&#20307;&#24418;&#25104;&#21644;&#20132;&#20184;&#36710;&#38431;&#26041;&#38754;&#30340;&#23454;&#38469;&#25928;&#29992;&#12290;&#25105;&#20204;&#21457;&#29616;B&amp;P&#30340;&#32467;&#26524;&#26159;&#19968;&#33268;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09246v1 Announce Type: cross Abstract: We consider the multi-agent spatial navigation problem of computing the socially optimal order of play, i.e., the sequence in which the agents commit to their decisions, and its associated equilibrium in an N-player Stackelberg trajectory game. We model this problem as a mixed-integer optimization problem over the space of all possible Stackelberg games associated with the order of play's permutations. To solve the problem, we introduce Branch and Play (B&amp;P), an efficient and exact algorithm that provably converges to a socially optimal order of play and its Stackelberg equilibrium. As a subroutine for B&amp;P, we employ and extend sequential trajectory planning, i.e., a popular multi-agent control approach, to scalably compute valid local Stackelberg equilibria for any given order of play. We demonstrate the practical utility of B&amp;P to coordinate air traffic control, swarm formation, and delivery vehicle fleets. We find that B&amp;P consistent
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23558;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#21644;&#22522;&#30784;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#30740;&#31350;&#20102;&#22914;&#20309;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#20154;&#31867;&#21487;&#35299;&#37322;&#30340;&#27010;&#24565;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#19968;&#32479;&#19968;&#26041;&#27861;&#30340;&#23454;&#29992;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.09236</link><description>&lt;p&gt;
&#23398;&#20064;&#21487;&#35299;&#37322;&#27010;&#24565;&#65306;&#32479;&#19968;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#19982;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Learning Interpretable Concepts: Unifying Causal Representation Learning and Foundation Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09236
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23558;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#21644;&#22522;&#30784;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#30740;&#31350;&#20102;&#22914;&#20309;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#20154;&#31867;&#21487;&#35299;&#37322;&#30340;&#27010;&#24565;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#19968;&#32479;&#19968;&#26041;&#27861;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26500;&#24314;&#26234;&#33021;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#26377;&#20004;&#31181;&#24191;&#27867;&#30340;&#26041;&#27861;&#12290;&#19968;&#31181;&#26041;&#27861;&#26159;&#26500;&#24314;&#22825;&#29983;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#65292;&#36825;&#26159;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#39046;&#22495;&#30340;&#21162;&#21147;&#26041;&#21521;&#12290;&#21478;&#19968;&#31181;&#26041;&#27861;&#26159;&#26500;&#24314;&#39640;&#24615;&#33021;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#28982;&#21518;&#25237;&#20837;&#21162;&#21147;&#21435;&#29702;&#35299;&#23427;&#20204;&#30340;&#24037;&#20316;&#21407;&#29702;&#12290;&#26412;&#30740;&#31350;&#23558;&#36825;&#20004;&#31181;&#26041;&#27861;&#32852;&#31995;&#36215;&#26469;&#65292;&#30740;&#31350;&#22914;&#20309;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#20154;&#31867;&#21487;&#35299;&#37322;&#30340;&#27010;&#24565;&#12290;&#36890;&#36807;&#32467;&#21512;&#36825;&#20004;&#20010;&#39046;&#22495;&#30340;&#24605;&#24819;&#65292;&#25105;&#20204;&#27491;&#24335;&#23450;&#20041;&#20102;&#27010;&#24565;&#30340;&#27010;&#24565;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#20204;&#21487;&#20197;&#20174;&#22810;&#26679;&#30340;&#25968;&#25454;&#20013;&#34987;&#21487;&#38752;&#22320;&#24674;&#22797;&#20986;&#26469;&#12290;&#23545;&#20110;&#21512;&#25104;&#25968;&#25454;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#32479;&#19968;&#26041;&#27861;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09236v1 Announce Type: cross Abstract: To build intelligent machine learning systems, there are two broad approaches. One approach is to build inherently interpretable models, as endeavored by the growing field of causal representation learning. The other approach is to build highly-performant foundation models and then invest efforts into understanding how they work. In this work, we relate these two approaches and study how to learn human-interpretable concepts from data. Weaving together ideas from both fields, we formally define a notion of concepts and show that they can be provably recovered from diverse data. Experiments on synthetic data and large language models show the utility of our unified approach.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#29992;&#20110;&#35780;&#20272;&#33258;&#20027;&#32534;&#38431;&#31639;&#27861;&#30340;&#27979;&#35797;&#24179;&#21488;&#12290;&#36890;&#36807;&#23545;&#19977;&#31181;&#31639;&#27861;&#30340;&#35780;&#20272;&#65292;&#25105;&#20204;&#21457;&#29616;&#20998;&#24067;&#24335;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#31639;&#27861;&#22312;&#30828;&#20214;&#21644;&#20223;&#30495;&#29615;&#22659;&#20013;&#30340;&#34920;&#29616;&#20248;&#20110;&#32447;&#24615;&#21453;&#39304;&#31639;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.09233</link><description>&lt;p&gt;
&#35774;&#35745;&#21644;&#23454;&#29616;&#29992;&#20110;&#35780;&#20272;&#33258;&#20027;&#32534;&#38431;&#31639;&#27861;&#30340;&#22522;&#20934;&#27979;&#35797;&#24179;&#21488;
&lt;/p&gt;
&lt;p&gt;
Design and Realization of a Benchmarking Testbed for Evaluating Autonomous Platooning Algorithms
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09233
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#29992;&#20110;&#35780;&#20272;&#33258;&#20027;&#32534;&#38431;&#31639;&#27861;&#30340;&#27979;&#35797;&#24179;&#21488;&#12290;&#36890;&#36807;&#23545;&#19977;&#31181;&#31639;&#27861;&#30340;&#35780;&#20272;&#65292;&#25105;&#20204;&#21457;&#29616;&#20998;&#24067;&#24335;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#31639;&#27861;&#22312;&#30828;&#20214;&#21644;&#20223;&#30495;&#29615;&#22659;&#20013;&#30340;&#34920;&#29616;&#20248;&#20110;&#32447;&#24615;&#21453;&#39304;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#36710;&#36742;&#32534;&#38431;&#25552;&#20379;&#20102;&#36817;&#26399;&#21644;&#38271;&#26399;&#30340;&#26426;&#20250;&#65292;&#21487;&#20197;&#25552;&#39640;&#36816;&#33829;&#25928;&#29575;&#24182;&#25405;&#25937;&#29983;&#21629;&#12290;&#36807;&#21435;30&#24180;&#20013;&#65292;&#33258;&#21160;&#39550;&#39542;&#39046;&#22495;&#26377;&#20102;&#24555;&#36895;&#21457;&#23637;&#65292;&#20351;&#24471;&#26032;&#25216;&#26415;&#21487;&#20197;&#20943;&#36731;&#23545;&#20154;&#31867;&#39550;&#39542;&#21592;&#30340;&#36127;&#25285;&#24182;&#38477;&#20302;&#36710;&#36742;&#25490;&#25918;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21487;&#20197;&#35780;&#20272;&#21644;&#22522;&#20934;&#27979;&#35797;1/10&#27604;&#20363;&#36710;&#36742;&#19978;&#30340;&#32534;&#38431;&#31639;&#27861;&#30340;&#27979;&#35797;&#24179;&#21488;&#65292;&#24182;&#20351;&#29992;&#33258;&#24102;&#20256;&#24863;&#22120;&#30340;&#36710;&#36742;&#36827;&#34892;&#23454;&#29616;&#12290;&#20026;&#20102;&#23637;&#31034;&#27979;&#35797;&#24179;&#21488;&#30340;&#25928;&#30410;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#19977;&#31181;&#31639;&#27861;&#65292;&#21253;&#25324;&#32447;&#24615;&#21453;&#39304;&#21644;&#20004;&#31181;&#20998;&#24067;&#24335;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#30340;&#21464;&#20307;&#65292;&#24182;&#22312;&#20856;&#22411;&#30340;&#32534;&#38431;&#22330;&#26223;&#20013;&#27604;&#36739;&#23427;&#20204;&#30340;&#32467;&#26524;&#65292;&#20854;&#20013;&#39046;&#20808;&#36710;&#36742;&#36319;&#36394;&#22810;&#27425;&#21464;&#36895;&#30340;&#21442;&#32771;&#36712;&#36857;&#12290;&#25105;&#20204;&#22312;&#20223;&#30495;&#29615;&#22659;&#20013;&#39564;&#35777;&#20102;&#31639;&#27861;&#30340;&#24615;&#33021;&#65292;&#20998;&#26512;&#20102;&#32534;&#38431;&#35268;&#27169;&#22686;&#21152;&#26102;&#30340;&#24615;&#33021;&#65292;&#24182;&#21457;&#29616;&#22312;&#30828;&#20214;&#21644;&#20223;&#30495;&#20013;&#65292;&#20998;&#24067;&#24335;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#31639;&#27861;&#20248;&#20110;&#32447;&#24615;&#21453;&#39304;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09233v1 Announce Type: cross Abstract: Autonomous vehicle platoons present near- and long-term opportunities to enhance operational efficiencies and save lives. The past 30 years have seen rapid development in the autonomous driving space, enabling new technologies that will alleviate the strain placed on human drivers and reduce vehicle emissions. This paper introduces a testbed for evaluating and benchmarking platooning algorithms on 1/10th scale vehicles with onboard sensors. To demonstrate the testbed's utility, we evaluate three algorithms, linear feedback and two variations of distributed model predictive control, and compare their results on a typical platooning scenario where the lead vehicle tracks a reference trajectory that changes speed multiple times. We validate our algorithms in simulation to analyze the performance as the platoon size increases, and find that the distributed model predictive control algorithms outperform linear feedback on hardware and in sim
&lt;/p&gt;</description></item><item><title>This paper introduces a novel approach called Membership Inference Test (MINT) to empirically assess if specific data was used during the training of AI models. Two MINT architectures based on MLP and CNN are proposed and evaluated on a challenging face recognition task, achieving promising results with up to 90% accuracy.</title><link>https://arxiv.org/abs/2402.09225</link><description>&lt;p&gt;
&#25105;&#30340;&#25968;&#25454;&#22312;&#20320;&#30340;AI&#27169;&#22411;&#20013;&#21527;&#65311;&#36890;&#36807;&#24212;&#29992;&#20110;&#20154;&#33080;&#22270;&#20687;&#30340;&#25104;&#21592;&#25512;&#26029;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Is my Data in your AI Model? Membership Inference Test with Application to Face Images
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09225
&lt;/p&gt;
&lt;p&gt;
This paper introduces a novel approach called Membership Inference Test (MINT) to empirically assess if specific data was used during the training of AI models. Two MINT architectures based on MLP and CNN are proposed and evaluated on a challenging face recognition task, achieving promising results with up to 90% accuracy.
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#25104;&#21592;&#25512;&#26029;&#27979;&#35797;&#65288;MINT&#65289;&#65292;&#19968;&#31181;&#29992;&#20110;&#32463;&#39564;&#24615;&#35780;&#20272;&#29305;&#23450;&#25968;&#25454;&#26159;&#21542;&#34987;&#29992;&#20110;&#35757;&#32451;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#39062;&#30340;MINT&#26550;&#26500;&#65292;&#26088;&#22312;&#23398;&#20064;&#22312;&#32463;&#36807;&#23457;&#35745;&#30340;&#27169;&#22411;&#26292;&#38706;&#20110;&#20854;&#35757;&#32451;&#36807;&#31243;&#20013;&#20351;&#29992;&#30340;&#25968;&#25454;&#26102;&#20986;&#29616;&#30340;&#19981;&#21516;&#28608;&#27963;&#27169;&#24335;&#12290;&#31532;&#19968;&#20010;&#26550;&#26500;&#22522;&#20110;&#22810;&#23618;&#24863;&#30693;&#26426;&#65288;MLP&#65289;&#32593;&#32476;&#65292;&#31532;&#20108;&#20010;&#22522;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#12290;&#25152;&#25552;&#20986;&#30340;MINT&#26550;&#26500;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20154;&#33080;&#35782;&#21035;&#20219;&#21153;&#19978;&#36827;&#34892;&#35780;&#20272;&#65292;&#32771;&#34385;&#20102;&#19977;&#31181;&#26368;&#20808;&#36827;&#30340;&#20154;&#33080;&#35782;&#21035;&#27169;&#22411;&#12290;&#20351;&#29992;&#20845;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#25968;&#25454;&#24211;&#36827;&#34892;&#23454;&#39564;&#65292;&#24635;&#20849;&#21253;&#21547;&#36229;&#36807;2200&#19975;&#24352;&#20154;&#33080;&#22270;&#20687;&#12290;&#26681;&#25454;&#21487;&#29992;&#30340;AI&#27169;&#22411;&#27979;&#35797;&#30340;&#19978;&#19979;&#25991;&#65292;&#32771;&#34385;&#20102;&#19981;&#21516;&#30340;&#23454;&#39564;&#22330;&#26223;&#12290;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#36798;&#21040;&#20102;90%&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09225v1 Announce Type: cross Abstract: This paper introduces the Membership Inference Test (MINT), a novel approach that aims to empirically assess if specific data was used during the training of Artificial Intelligence (AI) models. Specifically, we propose two novel MINT architectures designed to learn the distinct activation patterns that emerge when an audited model is exposed to data used during its training process. The first architecture is based on a Multilayer Perceptron (MLP) network and the second one is based on Convolutional Neural Networks (CNNs). The proposed MINT architectures are evaluated on a challenging face recognition task, considering three state-of-the-art face recognition models. Experiments are carried out using six publicly available databases, comprising over 22 million face images in total. Also, different experimental scenarios are considered depending on the context available of the AI model to test. Promising results, up to 90% accuracy, are a
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#23450;&#20041;&#39057;&#35889;&#28388;&#27874;&#22120;&#21644;&#27880;&#24847;&#21147;&#38519;&#38449;&#65292;&#25581;&#31034;&#20102;&#23558;&#20013;&#38388;&#34920;&#31034;&#25237;&#24433;&#21040;&#35789;&#27719;&#34920;&#30340;&#35299;&#37322;&#24037;&#20855;&#23545;&#20110;transformer-based LLMs&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#21457;&#29616;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#29305;&#23450;&#39057;&#35889;&#21306;&#22495;&#30340;&#25439;&#22833;&#23545;&#20110;&#20445;&#25345;&#20302;&#25439;&#22833;&#26159;&#21487;&#34892;&#30340;&#12290;</title><link>https://arxiv.org/abs/2402.09221</link><description>&lt;p&gt;
&#39057;&#35889;&#28388;&#27874;&#22120;&#12289;&#26263;&#20449;&#21495;&#21644;&#27880;&#24847;&#21147;&#38519;&#38449;
&lt;/p&gt;
&lt;p&gt;
Spectral Filters, Dark Signals, and Attention Sinks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09221
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#23450;&#20041;&#39057;&#35889;&#28388;&#27874;&#22120;&#21644;&#27880;&#24847;&#21147;&#38519;&#38449;&#65292;&#25581;&#31034;&#20102;&#23558;&#20013;&#38388;&#34920;&#31034;&#25237;&#24433;&#21040;&#35789;&#27719;&#34920;&#30340;&#35299;&#37322;&#24037;&#20855;&#23545;&#20110;transformer-based LLMs&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#21457;&#29616;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#29305;&#23450;&#39057;&#35889;&#21306;&#22495;&#30340;&#25439;&#22833;&#23545;&#20110;&#20445;&#25345;&#20302;&#25439;&#22833;&#26159;&#21487;&#34892;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09221v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;&#30340; &#25688;&#35201;&#65306;&#23558;&#20013;&#38388;&#34920;&#31034;&#25237;&#24433;&#21040;&#35789;&#27719;&#34920;&#19978;&#26159;&#19968;&#31181;&#36234;&#26469;&#36234;&#27969;&#34892;&#30340;&#36716;&#25442;&#22120;&#22411;LLM&#30340;&#35299;&#37322;&#24037;&#20855;&#65292;&#20063;&#31216;&#20026;logit lens&#12290;&#25105;&#20204;&#23545;&#36825;&#31181;&#26041;&#27861;&#36827;&#34892;&#20102;&#23450;&#37327;&#25193;&#23637;&#65292;&#24182;&#22522;&#20110;&#23558;&#35789;&#27719;&#23884;&#20837;&#21644;&#35299;&#23884;&#30697;&#38453;&#30340;&#22855;&#24322;&#21521;&#37327;&#20998;&#25104;&#27874;&#27573;&#26469;&#23450;&#20041;&#20013;&#38388;&#34920;&#31034;&#30340;&#39057;&#35889;&#28388;&#27874;&#22120;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#39057;&#35889;&#30340;&#23614;&#37096;&#20132;&#25442;&#30340;&#20449;&#21495;&#36127;&#36131;&#27880;&#24847;&#21147;&#38519;&#38449;&#65288;Xiao et al. 2023&#65289;&#65292;&#25105;&#20204;&#23545;&#27492;&#36827;&#34892;&#20102;&#35299;&#37322;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#23613;&#31649;&#20197;&#23618;&#20026;&#22522;&#30784;&#25233;&#21046;&#20102;&#23884;&#20837;&#39057;&#35889;&#30340;&#30456;&#24403;&#22823;&#37096;&#20998;&#65292;&#20294;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#25439;&#22833;&#20173;&#28982;&#21487;&#20197;&#20445;&#25345;&#36739;&#20302;&#65292;&#21482;&#35201;&#20445;&#25345;&#27880;&#24847;&#21147;&#38519;&#38449;&#21363;&#21487;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#21560;&#24341;&#22810;&#20010;&#20196;&#29260;&#27880;&#24847;&#21147;&#30340;&#20196;&#29260;&#34920;&#31034;&#22312;&#39057;&#35889;&#30340;&#23614;&#37096;&#20855;&#26377;&#36739;&#22823;&#30340;&#25237;&#24433;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09221v1 Announce Type: new Abstract: Projecting intermediate representations onto the vocabulary is an increasingly popular interpretation tool for transformer-based LLMs, also known as the logit lens. We propose a quantitative extension to this approach and define spectral filters on intermediate representations based on partitioning the singular vectors of the vocabulary embedding and unembedding matrices into bands. We find that the signals exchanged in the tail end of the spectrum are responsible for attention sinking (Xiao et al. 2023), of which we provide an explanation. We find that the loss of pretrained models can be kept low despite suppressing sizable parts of the embedding spectrum in a layer-dependent way, as long as attention sinking is preserved. Finally, we discover that the representation of tokens that draw attention from many tokens have large projections on the tail end of the spectrum.
&lt;/p&gt;</description></item><item><title>DivaTrack&#26159;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#22810;&#26679;&#21270;&#30340;&#36523;&#20307;&#23610;&#23544;&#21644;&#27963;&#21160;&#24773;&#20917;&#19979;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#36890;&#36807;&#21152;&#20837;&#32447;&#24615;&#21152;&#36895;&#24230;&#20174;IMU&#25552;&#20379;&#26356;&#20934;&#30830;&#30340;&#33050;&#37096;&#25509;&#35302;&#39044;&#27979;&#65292;&#24182;&#20351;&#29992;&#20004;&#38454;&#27573;&#27169;&#22411;&#26469;&#25552;&#39640;&#23545;&#20840;&#36523;&#23039;&#21183;&#30340;&#39044;&#27979;&#31283;&#23450;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.09211</link><description>&lt;p&gt;
DivaTrack: &#21033;&#29992;&#21152;&#36895;&#24230;&#22686;&#24378;&#30340;&#19977;&#28857;&#36319;&#36394;&#22120;&#23454;&#29616;&#22810;&#26679;&#21270;&#30340;&#36523;&#20307;&#21644;&#21160;&#20316;
&lt;/p&gt;
&lt;p&gt;
DivaTrack: Diverse Bodies and Motions from Acceleration-Enhanced Three-Point Trackers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09211
&lt;/p&gt;
&lt;p&gt;
DivaTrack&#26159;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#22810;&#26679;&#21270;&#30340;&#36523;&#20307;&#23610;&#23544;&#21644;&#27963;&#21160;&#24773;&#20917;&#19979;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#36890;&#36807;&#21152;&#20837;&#32447;&#24615;&#21152;&#36895;&#24230;&#20174;IMU&#25552;&#20379;&#26356;&#20934;&#30830;&#30340;&#33050;&#37096;&#25509;&#35302;&#39044;&#27979;&#65292;&#24182;&#20351;&#29992;&#20004;&#38454;&#27573;&#27169;&#22411;&#26469;&#25552;&#39640;&#23545;&#20840;&#36523;&#23039;&#21183;&#30340;&#39044;&#27979;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09211v1 &#20844;&#21578;&#31867;&#22411;: &#36328;&#39046;&#22495; &#25688;&#35201;: &#20840;&#36523;&#20223;&#30495;&#26159;&#25968;&#23383;&#29616;&#23454;&#20013;&#27785;&#28024;&#24335;&#31038;&#20132;&#21644;&#29615;&#22659;&#20132;&#20114;&#30340;&#20851;&#38190;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#35774;&#22791;&#21482;&#33021;&#25552;&#20379;&#26469;&#33258;&#22836;&#30420;&#21644;&#20004;&#20010;&#25511;&#21046;&#22120;&#65288;&#21363;&#19977;&#28857;&#36319;&#36394;&#22120;&#65289;&#30340;&#20845;&#20010;&#33258;&#30001;&#24230;&#65288;DOF&#65289;&#23039;&#21183;&#12290;&#30001;&#20110;&#36825;&#26159;&#19968;&#20010;&#39640;&#24230;&#27424;&#32422;&#26463;&#30340;&#38382;&#39064;&#65292;&#20174;&#36825;&#20123;&#36755;&#20837;&#20013;&#25512;&#26029;&#20986;&#20840;&#36523;&#23039;&#21183;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#29305;&#21035;&#26159;&#24403;&#25903;&#25345;&#26222;&#36890;&#20154;&#32676;&#25152;&#20195;&#34920;&#30340;&#21508;&#31181;&#36523;&#20307;&#27604;&#20363;&#21644;&#20351;&#29992;&#24773;&#20917;&#26102;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;DivaTrack&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#22312;&#24212;&#29992;&#20110;&#22810;&#26679;&#21270;&#30340;&#36523;&#20307;&#23610;&#23544;&#21644;&#27963;&#21160;&#26102;&#65292;&#20248;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#21033;&#29992;&#24815;&#24615;&#27979;&#37327;&#21333;&#20803;&#65288;IMU&#65289;&#30340;&#32447;&#24615;&#21152;&#36895;&#24230;&#22686;&#24378;&#20102;&#31232;&#30095;&#30340;&#19977;&#28857;&#36755;&#20837;&#65292;&#20197;&#25913;&#21892;&#33050;&#37096;&#25509;&#35302;&#30340;&#39044;&#27979;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#20004;&#38454;&#27573;&#27169;&#22411;&#23558;&#20302;&#36523;&#20307;&#23039;&#21183;&#30340;&#27169;&#31946;&#24615;&#26465;&#20214;&#21270;&#20026;&#33050;&#37096;&#25509;&#35302;&#21644;&#19978;&#36523;&#23039;&#21183;&#30340;&#39044;&#27979;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#31283;&#23450;&#25512;&#26029;&#20986;&#30340;&#20840;&#36523;&#23039;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09211v1 Announce Type: cross Abstract: Full-body avatar presence is crucial for immersive social and environmental interactions in digital reality. However, current devices only provide three six degrees of freedom (DOF) poses from the headset and two controllers (i.e. three-point trackers). Because it is a highly under-constrained problem, inferring full-body pose from these inputs is challenging, especially when supporting the full range of body proportions and use cases represented by the general population. In this paper, we propose a deep learning framework, DivaTrack, which outperforms existing methods when applied to diverse body sizes and activities. We augment the sparse three-point inputs with linear accelerations from Inertial Measurement Units (IMU) to improve foot contact prediction. We then condition the otherwise ambiguous lower-body pose with the predictions of foot contact and upper-body pose in a two-stage model. We further stabilize the inferred full-body 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#26234;&#33021;&#20195;&#29702;&#30340;&#38544;&#24335;&#29992;&#25143;&#24847;&#22270;&#29702;&#35299;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#24341;&#20837;Intention-in-Interaction (IN3) &#22522;&#20934;&#21644;&#22312;&#20195;&#29702;&#35774;&#35745;&#20013;&#34701;&#20837;&#27169;&#22411;&#19987;&#23478;&#65292;&#20351;&#24471;&#20195;&#29702;&#33021;&#22815;&#26356;&#22909;&#22320;&#19982;&#29992;&#25143;&#36827;&#34892;&#20132;&#20114;&#65292;&#24182;&#25552;&#21319;&#23545;&#29992;&#25143;&#25351;&#20196;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.09205</link><description>&lt;p&gt;
&#21578;&#35785;&#25105;&#26356;&#22810;&#65281;&#38754;&#21521;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#26234;&#33021;&#20195;&#29702;&#30340;&#38544;&#24335;&#29992;&#25143;&#24847;&#22270;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Tell Me More! Towards Implicit User Intention Understanding of Language Model Driven Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09205
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#26234;&#33021;&#20195;&#29702;&#30340;&#38544;&#24335;&#29992;&#25143;&#24847;&#22270;&#29702;&#35299;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#24341;&#20837;Intention-in-Interaction (IN3) &#22522;&#20934;&#21644;&#22312;&#20195;&#29702;&#35774;&#35745;&#20013;&#34701;&#20837;&#27169;&#22411;&#19987;&#23478;&#65292;&#20351;&#24471;&#20195;&#29702;&#33021;&#22815;&#26356;&#22909;&#22320;&#19982;&#29992;&#25143;&#36827;&#34892;&#20132;&#20114;&#65292;&#24182;&#25552;&#21319;&#23545;&#29992;&#25143;&#25351;&#20196;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#35821;&#35328;&#27169;&#22411;&#39537;&#21160;&#20195;&#29702;&#24120;&#24120;&#32570;&#20047;&#26377;&#25928;&#30340;&#29992;&#25143;&#21442;&#19982;&#26426;&#21046;&#65292;&#32771;&#34385;&#21040;&#29992;&#25143;&#25351;&#20196;&#20013;&#24120;&#35265;&#30340;&#27169;&#31946;&#24615;&#65292;&#36825;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#34429;&#28982;&#36825;&#20123;&#20195;&#29702;&#22312;&#21046;&#23450;&#31574;&#30053;&#21644;&#25191;&#34892;&#20219;&#21153;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#23547;&#27714;&#28548;&#28165;&#21644;&#25235;&#20303;&#31934;&#30830;&#30340;&#29992;&#25143;&#24847;&#22270;&#26041;&#38754;&#21364;&#36935;&#21040;&#20102;&#22256;&#38590;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Intention-in-Interaction (IN3) &#65292;&#36825;&#26159;&#19968;&#20010;&#26088;&#22312;&#36890;&#36807;&#26126;&#30830;&#30340;&#26597;&#35810;&#26816;&#26597;&#29992;&#25143;&#30340;&#38544;&#21547;&#24847;&#22270;&#30340;&#26032;&#39062;&#22522;&#20934;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;&#27169;&#22411;&#19987;&#23478;&#20316;&#20026;&#19978;&#28216;&#34701;&#20837;&#20195;&#29702;&#35774;&#35745;&#20013;&#65292;&#20197;&#22686;&#24378;&#29992;&#25143;-&#20195;&#29702;&#20132;&#20114;&#12290;&#21033;&#29992;IN3&#65292;&#25105;&#20204;&#32463;&#39564;&#24615;&#22320;&#35757;&#32451;&#20102;Mistral-Interact&#65292;&#36825;&#26159;&#19968;&#20010;&#24378;&#22823;&#30340;&#27169;&#22411;&#65292;&#23427;&#21487;&#20197;&#20027;&#21160;&#35780;&#20272;&#20219;&#21153;&#30340;&#27169;&#31946;&#24615;&#65292;&#35810;&#38382;&#29992;&#25143;&#24847;&#22270;&#65292;&#24182;&#23558;&#20854;&#36716;&#21270;&#20026;&#21487;&#34892;&#30340;&#30446;&#26631;&#65292;&#28982;&#21518;&#24320;&#22987;&#19979;&#28216;&#20195;&#29702;&#20219;&#21153;&#25191;&#34892;&#12290;&#23558;&#20854;&#38598;&#25104;&#21040;XAgent&#26694;&#26550;&#20013;&#65292;&#25105;&#20204;&#23545;&#22686;&#24378;&#30340;&#20195;&#29702;&#31995;&#32479;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#20197;&#35780;&#20272;&#29992;&#25143;&#25351;&#20196;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09205v1 Announce Type: cross Abstract: Current language model-driven agents often lack mechanisms for effective user participation, which is crucial given the vagueness commonly found in user instructions. Although adept at devising strategies and performing tasks, these agents struggle with seeking clarification and grasping precise user intentions. To bridge this gap, we introduce Intention-in-Interaction (IN3), a novel benchmark designed to inspect users' implicit intentions through explicit queries. Next, we propose the incorporation of model experts as the upstream in agent designs to enhance user-agent interaction. Employing IN3, we empirically train Mistral-Interact, a powerful model that proactively assesses task vagueness, inquires user intentions, and refines them into actionable goals before starting downstream agent task execution. Integrating it into the XAgent framework, we comprehensively evaluate the enhanced agent system regarding user instruction understand
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#22312;Tor&#21644;&#20844;&#20849;&#32593;&#32476;&#19978;&#33258;&#21160;&#21457;&#29616;&#20855;&#26377;&#38887;&#24615;&#30340;&#25351;&#25381;&#19982;&#25511;&#21046;&#65288;C2&#65289;&#36890;&#36947;&#65292;&#20174;&#32780;&#24110;&#21161;&#35782;&#21035;&#21644;&#38450;&#27490;&#32593;&#32476;&#25915;&#20987;&#12290;</title><link>https://arxiv.org/abs/2402.09200</link><description>&lt;p&gt;
&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#22312;Tor&#21644;&#20844;&#20849;&#32593;&#32476;&#19978;&#21457;&#29616;&#25351;&#25381;&#19982;&#25511;&#21046;&#65288;C2&#65289;&#36890;&#36947;
&lt;/p&gt;
&lt;p&gt;
Discovering Command and Control (C2) Channels on Tor and Public Networks Using Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09200
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#22312;Tor&#21644;&#20844;&#20849;&#32593;&#32476;&#19978;&#33258;&#21160;&#21457;&#29616;&#20855;&#26377;&#38887;&#24615;&#30340;&#25351;&#25381;&#19982;&#25511;&#21046;&#65288;C2&#65289;&#36890;&#36947;&#65292;&#20174;&#32780;&#24110;&#21161;&#35782;&#21035;&#21644;&#38450;&#27490;&#32593;&#32476;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25351;&#25381;&#19982;&#25511;&#21046;&#65288;C2&#65289;&#36890;&#36947;&#26159;&#35768;&#22810;&#31867;&#22411;&#30340;&#32593;&#32476;&#25915;&#20987;&#30340;&#24517;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#23427;&#20204;&#20351;&#25915;&#20987;&#32773;&#33021;&#22815;&#36828;&#31243;&#25511;&#21046;&#34987;&#24694;&#24847;&#36719;&#20214;&#24863;&#26579;&#30340;&#35745;&#31639;&#26426;&#65292;&#24182;&#25191;&#34892;&#26377;&#23475;&#30340;&#34892;&#21160;&#65292;&#22914;&#22312;&#32593;&#32476;&#20013;&#20256;&#25773;&#24694;&#24847;&#20195;&#30721;&#12289;&#31363;&#21462;&#26426;&#23494;&#25968;&#25454;&#25110;&#21457;&#36215;&#20998;&#24067;&#24335;&#25298;&#32477;&#26381;&#21153;&#65288;DDoS&#65289;&#25915;&#20987;&#12290;&#22240;&#27492;&#65292;&#35782;&#21035;&#36825;&#20123;C2&#36890;&#36947;&#23545;&#20110;&#20943;&#36731;&#21644;&#38450;&#27490;&#32593;&#32476;&#25915;&#20987;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#35782;&#21035;C2&#36890;&#36947;&#36890;&#24120;&#28041;&#21450;&#25163;&#21160;&#36807;&#31243;&#65292;&#38656;&#35201;&#28145;&#20837;&#20102;&#35299;&#21644;&#19987;&#19994;&#25216;&#26415;&#30340;&#32593;&#32476;&#25805;&#20316;&#30693;&#35782;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21516;&#26102;&#20351;&#29992;&#27491;&#24120;&#65288;&#20844;&#20849;&#65289;&#32593;&#32476;&#21644;Tor&#32593;&#32476;&#26469;&#33258;&#21160;&#27169;&#25311;C2&#25915;&#20987;&#27963;&#21160;&#12290;&#27492;&#22806;&#65292;&#37197;&#32622;&#26377;&#25928;&#36733;&#33655;&#22823;&#23567;&#21644;&#32593;&#32476;&#38450;&#28779;&#22681;&#20197;&#27169;&#25311;&#30495;&#23454;&#30340;&#25915;&#20987;&#22330;&#26223;&#12290;&#22312;&#20856;&#22411;&#32593;&#32476;&#37197;&#32622;&#19978;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#21487;&#20197;&#33258;&#21160;&#21457;&#29616;&#20855;&#26377;&#38887;&#24615;&#30340;C2&#36890;&#36947;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09200v1 Announce Type: cross Abstract: Command and control (C2) channels are an essential component of many types of cyber attacks, as they enable attackers to remotely control their malware-infected machines and execute harmful actions, such as propagating malicious code across networks, exfiltrating confidential data, or initiating distributed denial of service (DDoS) attacks. Identifying these C2 channels is therefore crucial in helping to mitigate and prevent cyber attacks. However, identifying C2 channels typically involves a manual process, requiring deep knowledge and expertise in cyber operations. In this paper, we propose a reinforcement learning (RL) based approach to automatically emulate C2 attack campaigns using both the normal (public) and the Tor networks. In addition, payload size and network firewalls are configured to simulate real-world attack scenarios. Results on a typical network configuration show that the RL agent can automatically discover resilient 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20195;&#29702;&#24341;&#23548;&#30340;&#39640;&#25928;&#37325;&#37319;&#26679;&#26041;&#27861;&#26469;&#25913;&#36827;&#40657;&#30418;AI&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#12290;&#36890;&#36807;&#20272;&#35745;&#21333;&#35789;&#29983;&#25104;&#27010;&#29575;&#20316;&#20026;&#20266;&#30333;&#30418;&#29305;&#24449;&#65292;&#36873;&#25321;&#23569;&#37327;&#20195;&#34920;&#24615;&#35789;&#27719;&#36827;&#34892;&#22810;&#27425;&#37325;&#37319;&#26679;&#65292;&#22312;&#21253;&#21547;&#20154;&#31867;&#25991;&#26412;&#21644;LLM&#29983;&#25104;&#25991;&#26412;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#21462;&#24471;&#20102;&#20986;&#33394;&#30340;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.09199</link><description>&lt;p&gt;
&#21313;&#20010;&#20851;&#38190;&#35789;&#20173;&#28982;&#26377;&#29992;&#65306;&#36890;&#36807;&#20195;&#29702;&#24341;&#23548;&#30340;&#39640;&#25928;&#37325;&#37319;&#26679;&#25913;&#36827;&#40657;&#30418;AI&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Ten Words Only Still Help: Improving Black-Box AI-Generated Text Detection via Proxy-Guided Efficient Re-Sampling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09199
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20195;&#29702;&#24341;&#23548;&#30340;&#39640;&#25928;&#37325;&#37319;&#26679;&#26041;&#27861;&#26469;&#25913;&#36827;&#40657;&#30418;AI&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#12290;&#36890;&#36807;&#20272;&#35745;&#21333;&#35789;&#29983;&#25104;&#27010;&#29575;&#20316;&#20026;&#20266;&#30333;&#30418;&#29305;&#24449;&#65292;&#36873;&#25321;&#23569;&#37327;&#20195;&#34920;&#24615;&#35789;&#27719;&#36827;&#34892;&#22810;&#27425;&#37325;&#37319;&#26679;&#65292;&#22312;&#21253;&#21547;&#20154;&#31867;&#25991;&#26412;&#21644;LLM&#29983;&#25104;&#25991;&#26412;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#21462;&#24471;&#20102;&#20986;&#33394;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#24212;&#29992;&#26085;&#30410;&#22686;&#22810;&#65292;&#23427;&#20204;&#30340;&#28389;&#29992;&#24341;&#21457;&#20102;&#35768;&#22810;&#19981;&#21463;&#27426;&#36814;&#30340;&#31038;&#20250;&#38382;&#39064;&#65292;&#22914;&#20551;&#26032;&#38395;&#12289;&#23398;&#26415;&#19981;&#35802;&#23454;&#21644;&#20449;&#24687;&#27745;&#26579;&#12290;&#36825;&#20351;&#24471;AI&#29983;&#25104;&#25991;&#26412;&#65288;AIGT&#65289;&#30340;&#26816;&#27979;&#38750;&#24120;&#37325;&#35201;&#12290;&#22312;&#29616;&#26377;&#26041;&#27861;&#20013;&#65292;&#30333;&#30418;&#26041;&#27861;&#22312;&#24615;&#33021;&#21644;&#27867;&#21270;&#24615;&#26041;&#38754;&#36890;&#24120;&#20248;&#20110;&#40657;&#30418;&#26041;&#27861;&#65292;&#20294;&#23427;&#20204;&#38656;&#35201;&#35775;&#38382;LLM&#30340;&#20869;&#37096;&#29366;&#24577;&#65292;&#19981;&#36866;&#29992;&#20110;&#40657;&#30418;&#35774;&#32622;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22810;&#27425;&#37325;&#37319;&#26679;&#26469;&#20272;&#35745;&#21333;&#35789;&#29983;&#25104;&#27010;&#29575;&#20316;&#20026;&#20266;&#30333;&#30418;&#29305;&#24449;&#20197;&#24110;&#21161;&#25913;&#36827;&#40657;&#30418;AIGT&#26816;&#27979;&#30340;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;POGER&#65292;&#19968;&#31181;&#20195;&#29702;&#24341;&#23548;&#30340;&#39640;&#25928;&#37325;&#37319;&#26679;&#26041;&#27861;&#65292;&#22312;&#40657;&#30418;AIGT&#26816;&#27979;&#20013;&#36873;&#25321;&#19968;&#20010;&#23567;&#30340;&#20195;&#34920;&#24615;&#35789;&#27719;&#23376;&#38598;&#65288;&#20363;&#22914;10&#20010;&#35789;&#65289;&#65292;&#36827;&#34892;&#22810;&#27425;&#37325;&#37319;&#26679;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#21253;&#21547;&#20154;&#31867;&#25991;&#26412;&#21644;&#19971;&#20010;LLM&#29983;&#25104;&#25991;&#26412;&#30340;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09199v1 Announce Type: cross Abstract: With the rapidly increasing application of large language models (LLMs), their abuse has caused many undesirable societal problems such as fake news, academic dishonesty, and information pollution. This makes AI-generated text (AIGT) detection of great importance. Among existing methods, white-box methods are generally superior to black-box methods in terms of performance and generalizability, but they require access to LLMs' internal states and are not applicable to black-box settings. In this paper, we propose to estimate word generation probabilities as pseudo white-box features via multiple re-sampling to help improve AIGT detection under the black-box setting. Specifically, we design POGER, a proxy-guided efficient re-sampling method, which selects a small subset of representative words (e.g., 10 words) for performing multiple re-sampling in black-box AIGT detection. Experiments on datasets containing texts from humans and seven LL
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#19971;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35748;&#30693;&#24515;&#29702;&#23398;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#23427;&#20204;&#19982;&#20154;&#31867;&#19968;&#26679;&#23384;&#22312;&#38750;&#29702;&#24615;&#65292;&#20294;&#23637;&#31034;&#30340;&#38750;&#29702;&#24615;&#26041;&#24335;&#19982;&#20154;&#31867;&#20559;&#35265;&#19981;&#21516;&#65292;&#21516;&#26102;&#36824;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#22238;&#31572;&#19981;&#19968;&#33268;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.09193</link><description>&lt;p&gt;
(&#19981;)&#29702;&#24615;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#35748;&#30693;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
(Ir)rationality and Cognitive Biases in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09193
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#19971;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35748;&#30693;&#24515;&#29702;&#23398;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#23427;&#20204;&#19982;&#20154;&#31867;&#19968;&#26679;&#23384;&#22312;&#38750;&#29702;&#24615;&#65292;&#20294;&#23637;&#31034;&#30340;&#38750;&#29702;&#24615;&#26041;&#24335;&#19982;&#20154;&#31867;&#20559;&#35265;&#19981;&#21516;&#65292;&#21516;&#26102;&#36824;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#22238;&#31572;&#19981;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#26159;&#21542;&#23637;&#29616;&#20986;&#29702;&#24615;&#25512;&#29702;&#65311;&#30001;&#20110;&#20854;&#35757;&#32451;&#25968;&#25454;&#25152;&#21547;&#30340;&#20154;&#31867;&#20559;&#35265;&#65292;LLMs&#24050;&#34987;&#35777;&#23454;&#23384;&#22312;&#20154;&#31867;&#20559;&#35265;&#65307;&#28982;&#32780;&#65292;&#20854;&#26159;&#21542;&#21453;&#26144;&#20986;&#20102;&#29702;&#24615;&#25512;&#29702;&#36824;&#19981;&#22826;&#28165;&#26970;&#12290;&#26412;&#25991;&#36890;&#36807;&#35780;&#20272;&#19971;&#20010;&#35821;&#35328;&#27169;&#22411;&#22312;&#26469;&#33258;&#35748;&#30693;&#24515;&#29702;&#23398;&#25991;&#29486;&#30340;&#20219;&#21153;&#20013;&#22238;&#31572;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#21644;&#20154;&#31867;&#19968;&#26679;&#65292;LLMs&#22312;&#36825;&#20123;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#38750;&#29702;&#24615;&#12290;&#28982;&#32780;&#65292;LLMs&#23637;&#29616;&#20986;&#30340;&#36825;&#31181;&#38750;&#29702;&#24615;&#19982;&#20154;&#31867;&#30340;&#20559;&#35265;&#19981;&#21516;&#12290;&#24403;LLMs&#32473;&#20986;&#38169;&#35823;&#31572;&#26696;&#26102;&#65292;&#23427;&#20204;&#36890;&#24120;&#20250;&#20197;&#19982;&#20154;&#31867;&#20559;&#35265;&#19981;&#21516;&#30340;&#26041;&#24335;&#38169;&#35823;&#12290;&#38500;&#27492;&#20043;&#22806;&#65292;LLMs&#36824;&#23637;&#29616;&#20986;&#20102;&#21709;&#24212;&#30340;&#26174;&#33879;&#19981;&#19968;&#33268;&#24615;&#65292;&#36825;&#34920;&#26126;&#20102;&#39069;&#22806;&#30340;&#38750;&#29702;&#24615;&#23618;&#38754;&#12290;&#38500;&#20102;&#23454;&#39564;&#32467;&#26524;&#65292;&#26412;&#25991;&#36824;&#36890;&#36807;&#23637;&#31034;&#22914;&#20309;&#35780;&#20272;&#21644;&#27604;&#36739;&#36825;&#31867;&#27169;&#22411;&#30340;&#19981;&#21516;&#21151;&#33021;&#65292;&#23545;&#26041;&#27861;&#35770;&#20316;&#20986;&#20102;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09193v1 Announce Type: cross Abstract: Do large language models (LLMs) display rational reasoning? LLMs have been shown to contain human biases due to the data they have been trained on; whether this is reflected in rational reasoning remains less clear. In this paper, we answer this question by evaluating seven language models using tasks from the cognitive psychology literature. We find that, like humans, LLMs display irrationality in these tasks. However, the way this irrationality is displayed does not reflect that shown by humans. When incorrect answers are given by LLMs to these tasks, they are often incorrect in ways that differ from human-like biases. On top of this, the LLMs reveal an additional layer of irrationality in the significant inconsistency of the responses. Aside from the experimental results, this paper seeks to make a methodological contribution by showing how we can assess and compare different capabilities of these types of models, in this case with r
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25915;&#20987;&#24418;&#24335;&#8212;&#8212;&#19978;&#19979;&#25991;&#20132;&#20114;&#25915;&#20987;&#65292;&#36890;&#36807;&#20132;&#20114;&#24335;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#38382;&#31572;&#26469;&#24341;&#20986;&#26377;&#23475;&#20449;&#24687;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.09177</link><description>&lt;p&gt;
&#20511;&#21161;&#22810;&#36718;&#20132;&#20114;&#21033;&#29992;&#19978;&#19979;&#25991;&#36827;&#34892;&#36234;&#29425;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Leveraging the Context through Multi-Round Interactions for Jailbreaking Attacks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09177
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25915;&#20987;&#24418;&#24335;&#8212;&#8212;&#19978;&#19979;&#25991;&#20132;&#20114;&#25915;&#20987;&#65292;&#36890;&#36807;&#20132;&#20114;&#24335;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#38382;&#31572;&#26469;&#24341;&#20986;&#26377;&#23475;&#20449;&#24687;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23481;&#26131;&#21463;&#21040;&#36234;&#29425;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#36234;&#29425;&#25915;&#20987;&#36890;&#36807;&#24494;&#22937;&#22320;&#20462;&#25913;&#25915;&#20987;&#26597;&#35810;&#26469;&#25552;&#21462;&#26377;&#23475;&#20449;&#24687;&#12290;&#38543;&#30528;&#38450;&#24481;&#26426;&#21046;&#30340;&#36827;&#21270;&#65292;&#36234;&#29425;&#25915;&#20987;&#30452;&#25509;&#33719;&#21462;&#26377;&#23475;&#20449;&#24687;&#21464;&#24471;&#36234;&#26469;&#36234;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#30740;&#31350;&#21463;&#21040;&#20154;&#31867;&#38388;&#25509;&#24341;&#20986;&#26377;&#23475;&#20449;&#24687;&#30340;&#23454;&#36341;&#21551;&#21457;&#65292;&#38024;&#23545;&#19968;&#31181;&#26032;&#30340;&#25915;&#20987;&#24418;&#24335;&#8212;&#8212;&#19978;&#19979;&#25991;&#20132;&#20114;&#25915;&#20987;&#12290;&#35813;&#26041;&#27861;&#20381;&#36182;&#20110;LLMs&#29983;&#25104;&#36807;&#31243;&#20013;&#30340;&#33258;&#22238;&#24402;&#24615;&#36136;&#12290;&#25105;&#20204;&#35748;&#20026;&#20808;&#21069;&#30340;&#19978;&#19979;&#25991;&#8212;&#8212;&#25915;&#20987;&#26597;&#35810;&#20043;&#21069;&#30340;&#20449;&#24687;&#22312;&#23454;&#29616;&#24378;&#22823;&#30340;&#36234;&#29425;&#25915;&#20987;&#26041;&#38754;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21021;&#27493;&#38382;&#31572;&#23545;&#19982;LLMs&#20132;&#20114;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#36825;&#26679;&#20570;&#65292;&#25105;&#20204;&#24341;&#23548;&#27169;&#22411;&#30340;&#22238;&#31572;&#26397;&#30528;&#25581;&#31034;&#8220;&#26399;&#26395;&#30340;&#8221;&#26377;&#23475;&#20449;&#24687;&#30340;&#26041;&#21521;&#21457;&#23637;&#12290;&#25105;&#20204;&#22312;&#22235;&#31181;&#19981;&#21516;&#30340;LLMs&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#35777;&#26126;&#20102;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09177v1 Announce Type: cross Abstract: Large Language Models (LLMs) are susceptible to Jailbreaking attacks, which aim to extract harmful information by subtly modifying the attack query. As defense mechanisms evolve, directly obtaining harmful information becomes increasingly challenging for Jailbreaking attacks. In this work, inspired by human practices of indirect context to elicit harmful information, we focus on a new attack form called Contextual Interaction Attack. The idea relies on the autoregressive nature of the generation process in LLMs. We contend that the prior context--the information preceding the attack query--plays a pivotal role in enabling potent Jailbreaking attacks. Specifically, we propose an approach that leverages preliminary question-answer pairs to interact with the LLM. By doing so, we guide the responses of the model toward revealing the 'desired' harmful information. We conduct experiments on four different LLMs and demonstrate the efficacy of 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#20351;&#29992;ChatGPT&#22312;&#35282;&#33394;&#25198;&#28436;&#27169;&#25311;&#28216;&#25103;&#20013;&#25552;&#21319;&#25945;&#23398;&#36136;&#37327;&#65292;&#24182;&#25552;&#39640;&#23398;&#29983;&#23545;&#23398;&#20064;&#30340;&#20852;&#36259;&#12290;</title><link>https://arxiv.org/abs/2402.09161</link><description>&lt;p&gt;
&#20351;&#29992;ChatGPT&#36827;&#34892;&#35282;&#33394;&#25198;&#28436;&#27169;&#25311;&#28216;&#25103;
&lt;/p&gt;
&lt;p&gt;
Role-Playing Simulation Games using ChatGPT
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09161
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#20351;&#29992;ChatGPT&#22312;&#35282;&#33394;&#25198;&#28436;&#27169;&#25311;&#28216;&#25103;&#20013;&#25552;&#21319;&#25945;&#23398;&#36136;&#37327;&#65292;&#24182;&#25552;&#39640;&#23398;&#29983;&#23545;&#23398;&#20064;&#30340;&#20852;&#36259;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20174;COVID-19&#22823;&#27969;&#34892;&#20197;&#26469;&#65292;&#25945;&#32946;&#26426;&#26500;&#24320;&#22987;&#36827;&#34892;&#25968;&#23383;&#21270;&#36716;&#22411;&#39033;&#30446;&#12290;&#36825;&#20123;&#39033;&#30446;&#30340;&#25104;&#21151;&#21462;&#20915;&#20110;&#25972;&#21512;&#26032;&#25216;&#26415;&#21644;&#29702;&#35299;&#25968;&#23383;&#32032;&#20859;&#23398;&#29983;&#30340;&#38656;&#27714;&#12290;"&#20197;&#20570;&#23398;&#20064;"&#30340;&#26041;&#27861;&#35748;&#20026;&#65292;&#24403;&#23398;&#29983;&#21487;&#20197;&#23581;&#35797;&#21644;&#23454;&#36341;&#36825;&#20123;&#25216;&#33021;&#26102;&#65292;&#25165;&#33021;&#30495;&#27491;&#25104;&#21151;&#22320;&#23398;&#20064;&#26032;&#25216;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#28436;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#22312;&#35282;&#33394;&#25198;&#28436;&#27169;&#25311;&#28216;&#25103;&#22330;&#26223;&#20013;&#20351;&#29992;ChatGPT&#26469;&#22686;&#24378;&#25945;&#23398;&#36136;&#37327;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20197;&#20419;&#36827;&#20027;&#21160;&#23398;&#20064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#22914;&#20309;&#36890;&#36807;&#20351;&#29992;ChatGPT&#35753;&#23398;&#29983;&#23454;&#36341;&#30495;&#23454;&#22330;&#26223;&#26469;&#25552;&#39640;&#23398;&#29983;&#23545;&#23398;&#20064;&#30340;&#20852;&#36259;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09161v1 Announce Type: new Abstract: Since the COVID-19 pandemic, educational institutions have embarked on digital transformation projects. The success of these projects depends on integrating new technologies and understanding the needs of digitally literate students. The "learning by doing" approach suggests that real success in learning new skills is achieved when students can try out and practise these skills. In this article, we demonstrate how Large Language Models (LLMs) can enhance the quality of teaching by using ChatGPT in a role-playing simulation game scenario to promote active learning. Moreover, we discuss how LLMs can boost students' interest in learning by allowing them to practice real-life scenarios using ChatGPT.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20851;&#27880;&#33258;&#23398;&#20064;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26680;&#24515;&#38382;&#39064;&#65306;&#22914;&#20309;&#23398;&#20064;&#26410;&#30693;&#30693;&#35782;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#25105;&#35780;&#20272;&#21644;&#35782;&#21035;&#26410;&#30693;&#28857;&#26469;&#29420;&#31435;&#23398;&#20064;&#20197;&#21069;&#26410;&#30693;&#30340;&#30693;&#35782;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#23545;&#20110;&#20943;&#23569;&#24187;&#35273;&#35780;&#20998;&#12289;&#23454;&#29616;&#39640;&#25928;LLM&#26356;&#26032;&#20197;&#21450;&#30693;&#35782;&#20132;&#27969;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>https://arxiv.org/abs/2402.09147</link><description>&lt;p&gt;
&#26410;&#30693;&#20043;&#20013;&#65306;&#33258;&#23398;&#20064;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Into the Unknown: Self-Learning Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09147
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20851;&#27880;&#33258;&#23398;&#20064;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26680;&#24515;&#38382;&#39064;&#65306;&#22914;&#20309;&#23398;&#20064;&#26410;&#30693;&#30693;&#35782;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#25105;&#35780;&#20272;&#21644;&#35782;&#21035;&#26410;&#30693;&#28857;&#26469;&#29420;&#31435;&#23398;&#20064;&#20197;&#21069;&#26410;&#30693;&#30340;&#30693;&#35782;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#23545;&#20110;&#20943;&#23569;&#24187;&#35273;&#35780;&#20998;&#12289;&#23454;&#29616;&#39640;&#25928;LLM&#26356;&#26032;&#20197;&#21450;&#30693;&#35782;&#20132;&#27969;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35299;&#20915;&#20102;&#33258;&#23398;&#20064;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#20027;&#35201;&#38382;&#39064;&#65306;&#21363;&#22914;&#20309;&#23398;&#20064;&#33258;&#24049;&#19981;&#30693;&#36947;&#30340;&#30693;&#35782;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#23398;&#20064;LLM&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#33258;&#24049;&#30340;&#24187;&#35273;&#36827;&#34892;&#33258;&#25105;&#35780;&#20272;&#65292;&#20351;LLM&#33021;&#22815;&#29420;&#31435;&#22320;&#23398;&#20064;&#20197;&#21069;&#26410;&#30693;&#30340;&#30693;&#35782;&#12290;&#36890;&#36807;&#20351;&#29992;&#24187;&#35273;&#35780;&#20998;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#31216;&#20026;&#8220;&#26410;&#30693;&#28857;&#8221;&#30340;&#26032;&#27010;&#24565;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22806;&#37096;&#21644;&#19977;&#31181;&#20869;&#37096;&#26041;&#27861;&#26469;&#33258;&#21160;&#35782;&#21035;&#26410;&#30693;&#28857;&#12290;&#36825;&#26377;&#21161;&#20110;&#21019;&#24314;&#19968;&#20010;&#33258;&#23398;&#20064;&#24490;&#29615;&#65292;&#19987;&#27880;&#20110;&#26410;&#30693;&#28857;&#20013;&#30340;&#30693;&#35782;&#24046;&#36317;&#65292;&#20174;&#32780;&#20943;&#23569;&#24187;&#35273;&#35780;&#20998;&#12290;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#29992;&#20110;&#35780;&#20272;LLM&#33258;&#23398;&#20064;&#33021;&#21147;&#30340;&#35780;&#20272;&#25351;&#26631;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#24050;&#32463;&#36827;&#34892;&#20102;&#24494;&#35843;&#25110;&#23545;&#40784;&#30340;7B-Mistral&#27169;&#22411;&#22312;&#33258;&#23398;&#20064;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#25105;&#20204;&#30340;&#33258;&#23398;&#20064;&#27010;&#24565;&#21487;&#20197;&#23454;&#29616;&#26356;&#39640;&#25928;&#30340;LLM&#26356;&#26032;&#65292;&#24182;&#20026;&#30693;&#35782;&#20132;&#27969;&#24320;&#36767;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;&#23427;&#36824;&#21487;&#33021;&#22686;&#21152;&#20844;&#20247;&#30340;&#20449;&#20219;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09147v1 Announce Type: new Abstract: We address the main problem of self-learning LLM: the question of what to learn. We propose a self-learning LLM framework that enables an LLM to independently learn previously unknown knowledge through self-assessment of their own hallucinations. Using the hallucination score, we introduce a new concept of Points in The Unknown (PiUs), along with one extrinsic and three intrinsic methods for automatic PiUs identification. It facilitates the creation of a self-learning loop that focuses exclusively on the knowledge gap in Points in The Unknown, resulting in a reduced hallucination score. We also developed evaluation metrics for gauging an LLM's self-learning capability. Our experiments revealed that 7B-Mistral models that have been finetuned or aligned are capable of self-learning considerably well. Our self-learning concept allows more efficient LLM updates and opens new perspectives for knowledge exchange. It may also increase public tru
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#22810;&#20010;&#25968;&#25454;&#38598;&#21644;NLP&#20219;&#21153;&#30340;&#20840;&#38754;&#35780;&#20272;&#65292;&#35777;&#26126;&#20102;&#29305;&#23450;&#30340;&#25991;&#26412;&#22686;&#24378;&#26041;&#27861;&#19982;&#25913;&#36827;&#30340;&#24490;&#29615;&#35838;&#31243;&#23398;&#20064;&#65288;MCCL&#65289;&#30456;&#32467;&#21512;&#26102;&#33021;&#22815;&#26174;&#33879;&#20248;&#20110;&#20256;&#32479;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#22312;NLP&#27169;&#22411;&#24615;&#33021;&#19978;&#21462;&#24471;&#20102;&#37325;&#35201;&#31361;&#30772;&#12290;</title><link>https://arxiv.org/abs/2402.09141</link><description>&lt;p&gt;
&#36890;&#36807;&#25112;&#30053;&#25991;&#26412;&#22686;&#24378;&#25512;&#36827;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#30340;&#30740;&#31350;&#65306;&#22686;&#24378;&#26041;&#27861;&#21644;&#35838;&#31243;&#31574;&#30053;&#30340;&#20840;&#38754;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Advancing NLP Models with Strategic Text Augmentation: A Comprehensive Study of Augmentation Methods and Curriculum Strategies
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09141
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#22810;&#20010;&#25968;&#25454;&#38598;&#21644;NLP&#20219;&#21153;&#30340;&#20840;&#38754;&#35780;&#20272;&#65292;&#35777;&#26126;&#20102;&#29305;&#23450;&#30340;&#25991;&#26412;&#22686;&#24378;&#26041;&#27861;&#19982;&#25913;&#36827;&#30340;&#24490;&#29615;&#35838;&#31243;&#23398;&#20064;&#65288;MCCL&#65289;&#30456;&#32467;&#21512;&#26102;&#33021;&#22815;&#26174;&#33879;&#20248;&#20110;&#20256;&#32479;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#22312;NLP&#27169;&#22411;&#24615;&#33021;&#19978;&#21462;&#24471;&#20102;&#37325;&#35201;&#31361;&#30772;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;&#22810;&#20010;&#25968;&#25454;&#38598;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#30340;&#25991;&#26412;&#22686;&#24378;&#25216;&#26415;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#20197;&#35299;&#20915;&#36825;&#20123;&#26041;&#27861;&#32570;&#20047;&#21487;&#38752;&#12289;&#26222;&#36941;&#35777;&#25454;&#30340;&#38382;&#39064;&#12290;&#23427;&#32771;&#23519;&#20102;&#36825;&#20123;&#25216;&#26415;&#22312;&#22686;&#24378;&#35757;&#32451;&#38598;&#20197;&#25552;&#39640;&#20027;&#39064;&#20998;&#31867;&#12289;&#24773;&#24863;&#20998;&#26512;&#21644;&#20882;&#29359;&#35821;&#35328;&#26816;&#27979;&#31561;&#20219;&#21153;&#24615;&#33021;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#30740;&#31350;&#19981;&#20165;&#24378;&#35843;&#20102;&#22686;&#24378;&#26041;&#27861;&#65292;&#36824;&#24378;&#35843;&#20102;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#24341;&#20837;&#30495;&#23454;&#21644;&#22686;&#24378;&#23454;&#20363;&#30340;&#25112;&#30053;&#39034;&#24207;&#12290;&#30740;&#31350;&#30340;&#19968;&#20010;&#37325;&#35201;&#36129;&#29486;&#26159;&#20026;&#22686;&#24378;&#25968;&#25454;&#38598;&#24320;&#21457;&#21644;&#35780;&#20272;&#20102;&#25913;&#36827;&#30340;&#24490;&#29615;&#35838;&#31243;&#23398;&#20064;&#65288;MCCL&#65289;&#65292;&#36825;&#22312;&#35813;&#39046;&#22495;&#20013;&#23646;&#20110;&#19968;&#31181;&#26032;&#39062;&#26041;&#27861;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#29305;&#23450;&#30340;&#22686;&#24378;&#26041;&#27861;&#65292;&#23588;&#20854;&#26159;&#19982;MCCL&#32467;&#21512;&#20351;&#29992;&#26102;&#65292;&#33021;&#22815;&#26174;&#33879;&#20248;&#20110;&#20256;&#32479;&#30340;&#35757;&#32451;&#26041;&#27861;&#22312;NLP&#27169;&#22411;&#24615;&#33021;&#19978;&#30340;&#34920;&#29616;&#12290;&#36825;&#20123;&#32467;&#26524;&#24378;&#35843;&#20102;&#23545;&#21487;&#38752;&#30340;&#22686;&#24378;&#26041;&#27861;&#21644;&#25112;&#30053;&#39034;&#24207;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09141v1 Announce Type: cross Abstract: This study conducts a thorough evaluation of text augmentation techniques across a variety of datasets and natural language processing (NLP) tasks to address the lack of reliable, generalized evidence for these methods. It examines the effectiveness of these techniques in augmenting training sets to improve performance in tasks such as topic classification, sentiment analysis, and offensive language detection. The research emphasizes not only the augmentation methods, but also the strategic order in which real and augmented instances are introduced during training. A major contribution is the development and evaluation of Modified Cyclical Curriculum Learning (MCCL) for augmented datasets, which represents a novel approach in the field. Results show that specific augmentation methods, especially when integrated with MCCL, significantly outperform traditional training approaches in NLP model performance. These results underscore the need
&lt;/p&gt;</description></item><item><title>DolphCoder&#36890;&#36807;&#22810;&#26679;&#21270;&#30340;&#25351;&#23548;&#21644;&#33258;&#25105;&#35780;&#20272;&#25552;&#39640;&#20102;&#20195;&#30721;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#29983;&#25104;&#33021;&#21147;&#65292;&#24182;&#22312;HumanEval&#21644;MBPP&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.09136</link><description>&lt;p&gt;
DolphCoder: &#29992;&#22810;&#26679;&#21270;&#21644;&#22810;&#30446;&#26631;&#25351;&#23548;&#35843;&#25972;&#36827;&#34892;&#22238;&#22768;&#23450;&#20301;&#30340;&#20195;&#30721;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
DolphCoder: Echo-Locating Code Large Language Models with Diverse and Multi-Objective Instruction Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09136
&lt;/p&gt;
&lt;p&gt;
DolphCoder&#36890;&#36807;&#22810;&#26679;&#21270;&#30340;&#25351;&#23548;&#21644;&#33258;&#25105;&#35780;&#20272;&#25552;&#39640;&#20102;&#20195;&#30721;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#29983;&#25104;&#33021;&#21147;&#65292;&#24182;&#22312;HumanEval&#21644;MBPP&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Code Large Language Models (Code LLMs)&#22312;&#20195;&#30721;&#30456;&#20851;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#24050;&#32463;&#25552;&#20986;&#20102;&#20960;&#31181;&#25351;&#23548;&#35843;&#25972;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#39044;&#35757;&#32451;Code LLM&#30340;&#20195;&#30721;&#29983;&#25104;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#20855;&#26377;&#33258;&#25105;&#35780;&#20272;&#30340;&#22810;&#26679;&#21270;&#25351;&#20196;&#27169;&#22411;&#65288;DolphCoder&#65289;&#29992;&#20110;&#20195;&#30721;&#29983;&#25104;&#12290;&#23427;&#23398;&#20064;&#22810;&#26679;&#21270;&#30340;&#25351;&#20196;&#30446;&#26631;&#65292;&#24182;&#32467;&#21512;&#20195;&#30721;&#35780;&#20272;&#30446;&#26631;&#26469;&#22686;&#24378;&#20854;&#20195;&#30721;&#29983;&#25104;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;HumanEval&#21644;MBPP&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#20248;&#24322;&#30340;&#24615;&#33021;&#65292;&#20026;&#26410;&#26469;&#30340;&#20195;&#30721;&#25351;&#20196;&#35843;&#25972;&#24037;&#20316;&#25552;&#20379;&#20102;&#26032;&#30340;&#35265;&#35299;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#21457;&#29616;&#26159;&#65306;&#65288;1&#65289;&#36890;&#36807;&#22686;&#21152;&#20855;&#26377;&#19981;&#21516;&#25512;&#29702;&#36335;&#24452;&#30340;&#22810;&#26679;&#21270;&#21709;&#24212;&#26469;&#22686;&#24378;LLMs&#30340;&#20195;&#30721;&#33021;&#21147;&#12290; &#65288;2&#65289;&#25552;&#39640;&#35780;&#20272;&#20195;&#30721;&#35299;&#20915;&#26041;&#26696;&#27491;&#30830;&#24615;&#30340;&#33021;&#21147;&#20063;&#20250;&#22686;&#24378;&#20854;&#21019;&#36896;&#20195;&#30721;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09136v1 Announce Type: cross Abstract: Code Large Language Models (Code LLMs) have demonstrated outstanding performance in code-related tasks. Several instruction tuning approaches have been proposed to boost the code generation performance of pre-trained Code LLMs. In this paper, we introduce a diverse instruction model (DolphCoder) with self-evaluating for code generation. It learns diverse instruction targets and combines a code evaluation objective to enhance its code generation ability. Our model achieves superior performance on the HumanEval and MBPP benchmarks, demonstrating new insights for future code instruction tuning work. Our key findings are: (1) Augmenting more diverse responses with distinct reasoning paths increases the code capability of LLMs. (2) Improving one's ability to evaluate the correctness of code solutions also enhances their ability to create it.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23545;&#25239;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#20854;&#33021;&#22815;&#25104;&#21151;&#22320;&#21046;&#36896;&#23545;&#25239;&#24615;&#31034;&#20363;&#20197;&#24858;&#24324;&#23433;&#20840;&#25514;&#26045;&#65292;&#29305;&#21035;&#26159;&#22312;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#26041;&#38754;&#20855;&#26377;&#37325;&#22823;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2402.09132</link><description>&lt;p&gt;
&#25506;&#32034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23545;&#25239;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Exploring the Adversarial Capabilities of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09132
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23545;&#25239;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#20854;&#33021;&#22815;&#25104;&#21151;&#22320;&#21046;&#36896;&#23545;&#25239;&#24615;&#31034;&#20363;&#20197;&#24858;&#24324;&#23433;&#20840;&#25514;&#26045;&#65292;&#29305;&#21035;&#26159;&#22312;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#26041;&#38754;&#20855;&#26377;&#37325;&#22823;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26222;&#21450;&#24341;&#21457;&#20102;&#24191;&#27867;&#21644;&#26222;&#36941;&#30340;&#20852;&#36259;&#65292;&#22240;&#20026;&#23427;&#20204;&#20855;&#26377;&#24378;&#22823;&#30340;&#35821;&#35328;&#29983;&#25104;&#33021;&#21147;&#65292;&#20026;&#34892;&#19994;&#21644;&#30740;&#31350;&#25552;&#20379;&#20102;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;&#23613;&#31649;&#20197;&#21069;&#30340;&#30740;&#31350;&#25506;&#35752;&#20102;LLMs&#30340;&#23433;&#20840;&#24615;&#21644;&#38544;&#31169;&#38382;&#39064;&#65292;&#20294;&#36825;&#20123;&#27169;&#22411;&#33021;&#21542;&#34920;&#29616;&#20986;&#23545;&#25239;&#34892;&#20026;&#30340;&#31243;&#24230;&#20173;&#28982;&#23578;&#26410;&#23436;&#20840;&#25506;&#32034;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#30740;&#31350;&#24120;&#35265;&#30340;&#20844;&#24320;&#21487;&#29992;LLMs&#26159;&#21542;&#20855;&#26377;&#33021;&#21147;&#25200;&#20081;&#25991;&#26412;&#26679;&#26412;&#20197;&#24858;&#24324;&#23433;&#20840;&#25514;&#26045;&#65292;&#21363;&#25152;&#35859;&#30340;&#23545;&#25239;&#31034;&#20363;&#25110;&#25915;&#20987;&#12290;&#26356;&#20855;&#20307;&#22320;&#35828;&#65292;&#25105;&#20204;&#35843;&#26597;LLMs&#26159;&#21542;&#26412;&#36136;&#19978;&#33021;&#22815;&#20174;&#33391;&#24615;&#26679;&#26412;&#20013;&#21046;&#36896;&#23545;&#25239;&#24615;&#31034;&#20363;&#20197;&#24858;&#24324;&#29616;&#26377;&#30340;&#23433;&#20840;&#38450;&#32447;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#37325;&#28857;&#20851;&#27880;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#65292;&#21457;&#29616;LLMs&#25104;&#21151;&#22320;&#25214;&#21040;&#20102;&#23545;&#25239;&#24615;&#25200;&#21160;&#65292;&#26377;&#25928;&#22320;&#30772;&#22351;&#20102;&#23545;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#31995;&#32479;&#30340;&#38450;&#24481;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#23545;&#65288;&#21322;&#65289;&#33258;&#21160;&#21270;&#23433;&#20840;&#35780;&#20272;&#21644;&#38450;&#24481;&#20855;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09132v1 Announce Type: new Abstract: The proliferation of large language models (LLMs) has sparked widespread and general interest due to their strong language generation capabilities, offering great potential for both industry and research. While previous research delved into the security and privacy issues of LLMs, the extent to which these models can exhibit adversarial behavior remains largely unexplored. Addressing this gap, we investigate whether common publicly available LLMs have inherent capabilities to perturb text samples to fool safety measures, so-called adversarial examples resp.~attacks. More specifically, we investigate whether LLMs are inherently able to craft adversarial examples out of benign samples to fool existing safe rails. Our experiments, which focus on hate speech detection, reveal that LLMs succeed in finding adversarial perturbations, effectively undermining hate speech detection systems. Our findings carry significant implications for (semi-)aut
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22810;&#21830;&#21697;&#24066;&#22330;&#20013;&#65292;&#21253;&#25324;&#22797;&#26434;&#25414;&#32465;&#34892;&#20026;&#30340;&#24773;&#20917;&#19979;&#65292;&#25214;&#21040;&#26368;&#20248;&#30340;&#33258;&#21160;&#20570;&#24066;&#21830;&#30340;&#38382;&#39064;&#65292;&#21457;&#29616;&#35813;&#38382;&#39064;&#23545;&#20598;&#20110;&#19968;&#20010;&#26368;&#20248;&#36816;&#36755;&#38382;&#39064;&#65292;&#24182;&#19988;&#20855;&#26377;&#29305;&#23450;&#30340;&#20960;&#20309;&#32422;&#26463;&#26465;&#20214;&#12290;</title><link>https://arxiv.org/abs/2402.09129</link><description>&lt;p&gt;
&#26368;&#20248;&#33258;&#21160;&#20570;&#24066;&#21830;: &#21487;&#24494;&#32463;&#27982;&#23398;&#21644;&#24378;&#23545;&#20598;&#24615;
&lt;/p&gt;
&lt;p&gt;
Optimal Automated Market Makers: Differentiable Economics and Strong Duality
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09129
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22810;&#21830;&#21697;&#24066;&#22330;&#20013;&#65292;&#21253;&#25324;&#22797;&#26434;&#25414;&#32465;&#34892;&#20026;&#30340;&#24773;&#20917;&#19979;&#65292;&#25214;&#21040;&#26368;&#20248;&#30340;&#33258;&#21160;&#20570;&#24066;&#21830;&#30340;&#38382;&#39064;&#65292;&#21457;&#29616;&#35813;&#38382;&#39064;&#23545;&#20598;&#20110;&#19968;&#20010;&#26368;&#20248;&#36816;&#36755;&#38382;&#39064;&#65292;&#24182;&#19988;&#20855;&#26377;&#29305;&#23450;&#30340;&#20960;&#20309;&#32422;&#26463;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20570;&#24066;&#21830;&#30340;&#20316;&#29992;&#26159;&#21516;&#26102;&#20197;&#25351;&#23450;&#20215;&#26684;&#36141;&#20080;&#21644;&#20986;&#21806;&#21830;&#21697;&#25968;&#37327;&#65292;&#36890;&#24120;&#26159;&#37329;&#34701;&#36164;&#20135;&#22914;&#32929;&#31080;&#12290;&#33258;&#21160;&#20570;&#24066;&#21830;&#65288;AMM&#65289;&#26159;&#19968;&#31181;&#26681;&#25454;&#39044;&#23450;&#30340;&#26102;&#38388;&#34920;&#25552;&#20379;&#20132;&#26131;&#30340;&#26426;&#21046;&#65307;&#36873;&#25321;&#26368;&#20339;&#30340;&#26102;&#38388;&#34920;&#21462;&#20915;&#20110;&#20570;&#24066;&#21830;&#30340;&#30446;&#26631;&#12290;&#29616;&#26377;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#39044;&#27979;&#24066;&#22330;&#19978;&#65292;&#30446;&#30340;&#26159;&#20449;&#24687;&#25910;&#38598;&#12290;&#36817;&#26399;&#30340;&#24037;&#20316;&#21017;&#20027;&#35201;&#20851;&#27880;&#21033;&#28070;&#26368;&#22823;&#21270;&#30340;&#30446;&#26631;&#65292;&#20294;&#20165;&#20165;&#32771;&#34385;&#20102;&#19968;&#31181;&#31867;&#22411;&#30340;&#21830;&#21697;&#65288;&#29992;&#34913;&#37327;&#36135;&#24065;&#36827;&#34892;&#20132;&#26131;&#65289;&#65292;&#21253;&#25324;&#36870;&#21521;&#36873;&#25321;&#30340;&#24773;&#20917;&#12290;&#20851;&#20110;&#23384;&#22312;&#22810;&#31181;&#21830;&#21697;&#20197;&#21450;&#21487;&#33021;&#20986;&#29616;&#22797;&#26434;&#25414;&#32465;&#34892;&#20026;&#30340;&#26368;&#20248;&#20570;&#24066;&#38382;&#39064;&#23578;&#19981;&#28165;&#26970;&#12290;&#26412;&#25991;&#34920;&#26126;&#65292;&#22312;&#22810;&#20010;&#21830;&#21697;&#23384;&#22312;&#19988;&#21487;&#33021;&#20986;&#29616;&#22797;&#26434;&#25414;&#32465;&#34892;&#20026;&#30340;&#24773;&#20917;&#19979;&#65292;&#25214;&#21040;&#19968;&#20010;&#26368;&#20248;&#30340;&#20570;&#24066;&#21830;&#26159;&#19968;&#20010;&#23545;&#20598;&#20110;&#26368;&#20248;&#36816;&#36755;&#38382;&#39064;&#30340;&#38382;&#39064;&#65292;&#24182;&#19988;&#20855;&#26377;&#29305;&#23450;&#30340;&#20960;&#20309;&#32422;&#26463;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09129v1 Announce Type: cross Abstract: The role of a market maker is to simultaneously offer to buy and sell quantities of goods, often a financial asset such as a share, at specified prices. An automated market maker (AMM) is a mechanism that offers to trade according to some predetermined schedule; the best choice of this schedule depends on the market maker's goals. The literature on the design of AMMs has mainly focused on prediction markets with the goal of information elicitation. More recent work motivated by DeFi has focused instead on the goal of profit maximization, but considering only a single type of good (traded with a numeraire), including under adverse selection (Milionis et al. 2022). Optimal market making in the presence of multiple goods, including the possibility of complex bundling behavior, is not well understood. In this paper, we show that finding an optimal market maker is dual to an optimal transport problem, with specific geometric constraints on t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#39046;&#22495;&#29305;&#23450;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;MPI&#20195;&#30721;&#30340;&#24615;&#33021;&#65292;&#24182;&#25552;&#20986;&#20102;&#20351;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;MonoCoder&#36827;&#34892;MPI-based&#31243;&#24207;&#29983;&#25104;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.09126</link><description>&lt;p&gt;
MPIrigen: &#36890;&#36807;&#39046;&#22495;&#29305;&#23450;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;MPI&#20195;&#30721;
&lt;/p&gt;
&lt;p&gt;
MPIrigen: MPI Code Generation through Domain-Specific Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09126
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#39046;&#22495;&#29305;&#23450;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;MPI&#20195;&#30721;&#30340;&#24615;&#33021;&#65292;&#24182;&#25552;&#20986;&#20102;&#20351;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;MonoCoder&#36827;&#34892;MPI-based&#31243;&#24207;&#29983;&#25104;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#35268;&#27169;&#24182;&#34892;&#35745;&#31639;&#20013;&#65292;&#39640;&#25928;&#30340;&#24182;&#34892;&#35745;&#31639;&#23588;&#20026;&#37325;&#35201;&#65292;&#29305;&#21035;&#26159;&#22312;&#28040;&#24687;&#20256;&#36882;&#25509;&#21475;&#65288;MPI&#65289;&#38598;&#25104;&#39046;&#22495;&#12290;&#29983;&#25104;&#22522;&#20110;MPI&#30340;&#24182;&#34892;&#31243;&#24207;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24182;&#34892;&#32534;&#31243;&#20219;&#21153;&#65292;&#23578;&#26410;&#34987;&#25506;&#32034;&#12290;&#26412;&#30740;&#31350;&#39318;&#20808;&#25506;&#35752;&#20102;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#25104;&#22522;&#20110;MPI&#30340;&#24182;&#34892;&#31243;&#24207;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;&#21457;&#29616;&#24191;&#27867;&#20351;&#29992;&#30340;&#27169;&#22411;&#65292;&#22914;GPT-3.5&#21644;PolyCoder&#65288;&#19987;&#38376;&#30340;&#22810;&#35821;&#35328;&#20195;&#30721;&#27169;&#22411;&#65289;&#65292;&#22312;&#29983;&#25104;&#22522;&#20110;MPI&#30340;&#31243;&#24207;&#26102;&#34920;&#29616;&#20986;&#26126;&#26174;&#30340;&#24615;&#33021;&#19979;&#38477;&#65292;&#30456;&#27604;&#36890;&#29992;&#31243;&#24207;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#22522;&#20110;MPI&#30456;&#20851;&#32534;&#31243;&#35821;&#35328;C&#21644;C++&#39044;&#35757;&#32451;&#30340;&#39046;&#22495;&#29305;&#23450;&#27169;&#22411;MonoCoder&#30340;&#24615;&#33021;&#26356;&#22909;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;HPCorpusMPI&#19978;&#23545;MonoCoder&#36827;&#34892;&#24494;&#35843;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#19987;&#38376;&#30340;MPI-based&#31243;&#24207;&#29983;&#25104;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09126v1 Announce Type: cross Abstract: The imperative need to scale computation across numerous nodes highlights the significance of efficient parallel computing, particularly in the realm of Message Passing Interface (MPI) integration. The challenging parallel programming task of generating MPI-based parallel programs has remained unexplored. This study first investigates the performance of state-of-the-art language models in generating MPI-based parallel programs. Findings reveal that widely used models such as GPT-3.5 and PolyCoder (specialized multi-lingual code models) exhibit notable performance degradation, when generating MPI-based programs compared to general-purpose programs. In contrast, domain-specific models such as MonoCoder, which are pretrained on MPI-related programming languages of C and C++, outperform larger models. Subsequently, we introduce a dedicated downstream task of MPI-based program generation by fine-tuning MonoCoder on HPCorpusMPI. We call the r
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#38543;&#26426;&#35745;&#31639;&#21152;&#36895;&#23574;&#23792;&#32593;&#32476;&#20013;&#30340;&#20851;&#27880;&#26426;&#21046;&#30340;&#26032;&#26694;&#26550;&#65292;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;CIFAR-10&#19978;&#33021;&#22815;&#36798;&#21040;&#39640;&#20998;&#31867;&#20934;&#30830;&#29575;&#65292;&#21516;&#26102;&#22823;&#22823;&#38477;&#20302;&#20102;&#35745;&#31639;&#33021;&#37327;&#21644;&#20869;&#23384;&#35775;&#38382;&#25104;&#26412;&#12290;</title><link>https://arxiv.org/abs/2402.09109</link><description>&lt;p&gt;
&#38543;&#26426;&#23574;&#23792;&#20851;&#27880;&#65306;&#22312;&#23574;&#23792;&#32593;&#32476;&#20013;&#21033;&#29992;&#38543;&#26426;&#35745;&#31639;&#21152;&#36895;&#20851;&#27880;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
Stochastic Spiking Attention: Accelerating Attention with Stochastic Computing in Spiking Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09109
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#38543;&#26426;&#35745;&#31639;&#21152;&#36895;&#23574;&#23792;&#32593;&#32476;&#20013;&#30340;&#20851;&#27880;&#26426;&#21046;&#30340;&#26032;&#26694;&#26550;&#65292;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;CIFAR-10&#19978;&#33021;&#22815;&#36798;&#21040;&#39640;&#20998;&#31867;&#20934;&#30830;&#29575;&#65292;&#21516;&#26102;&#22823;&#22823;&#38477;&#20302;&#20102;&#35745;&#31639;&#33021;&#37327;&#21644;&#20869;&#23384;&#35775;&#38382;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#30001;&#20110;&#23574;&#23792;&#31070;&#32463;&#32593;&#32476;&#65288;SNN&#65289;&#22312;&#20943;&#23569;&#35745;&#31639;&#38656;&#27714;&#21644;&#25552;&#39640;&#21151;&#32791;&#25928;&#29575;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#24050;&#23558;&#20854;&#25972;&#21512;&#21040;Transformer&#20307;&#31995;&#32467;&#26500;&#20013;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#23574;&#23792;&#20449;&#21495;&#22312;&#36890;&#29992;&#35745;&#31639;&#24179;&#21488;&#19978;&#23454;&#29616;&#20851;&#27880;&#26426;&#21046;&#20173;&#28982;&#20302;&#25928;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#38543;&#26426;&#35745;&#31639;&#65288;SC&#65289;&#26469;&#26377;&#25928;&#25191;&#34892;&#22522;&#20110;SNN&#30340;Transformer&#30340;&#28857;&#31215;&#27880;&#24847;&#21147;&#30340;&#26032;&#26694;&#26550;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#22312;10&#20010;&#26102;&#38388;&#27493;&#20869;&#22312;CIFAR-10&#19978;&#36798;&#21040;&#39640;&#20998;&#31867;&#20934;&#30830;&#29575;&#65288;83.53%&#65289;&#65292;&#36825;&#19982;&#22522;&#32447;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#30340;&#24615;&#33021;&#65288;83.66%&#65289;&#30456;&#24403;&#12290;&#25105;&#20204;&#20272;&#35745;&#65292;&#35813;SC&#26041;&#27861;&#21487;&#20197;&#23548;&#33268;CMOS&#25968;&#23383;ASIC&#35774;&#35745;&#20013;&#35745;&#31639;&#33021;&#37327;&#20943;&#23569;6.3&#20493;&#65292;&#20869;&#23384;&#35775;&#38382;&#25104;&#26412;&#20943;&#23569;1.7&#20493;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#23454;&#20102;&#25105;&#20204;&#30340;&#38543;&#26426;&#27880;&#24847;&#21147;&#22359;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09109v1 Announce Type: cross Abstract: Spiking Neural Networks (SNNs) have been recently integrated into Transformer architectures due to their potential to reduce computational demands and to improve power efficiency. Yet, the implementation of the attention mechanism using spiking signals on general-purpose computing platforms remains inefficient. In this paper, we propose a novel framework leveraging stochastic computing (SC) to effectively execute the dot-product attention for SNN-based Transformers. We demonstrate that our approach can achieve high classification accuracy ($83.53\%$) on CIFAR-10 within 10 time steps, which is comparable to the performance of a baseline artificial neural network implementation ($83.66\%$). We estimate that the proposed SC approach can lead to over $6.3\times$ reduction in computing energy and $1.7\times$ reduction in memory access costs for a digital CMOS-based ASIC design. We experimentally validate our stochastic attention block design
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#22810;&#37325;&#20998;&#24418;&#20998;&#26512;&#35270;&#35282;&#65292;&#28145;&#20837;&#30740;&#31350;&#20102;LLMs&#20013;&#31070;&#32463;&#20803;&#30456;&#20114;&#20316;&#29992;&#21644;&#20986;&#29616;&#29616;&#35937;&#12290;&#36890;&#36807;&#24341;&#20837;&#33258;&#32452;&#32455;&#21644;&#22810;&#37325;&#20998;&#24418;&#20998;&#26512;&#30340;&#27010;&#24565;&#65292;&#30740;&#31350;&#20102;&#31070;&#32463;&#20803;&#30456;&#20114;&#20316;&#29992;&#30340;&#21160;&#24577;&#28436;&#21270;&#36807;&#31243;&#65292;&#23588;&#20854;&#20851;&#27880;&#35757;&#32451;&#20013;&#30340;&#22797;&#26434;&#34892;&#20026;&#12290;&#36890;&#36807;&#25552;&#20986;&#22522;&#20110;&#31070;&#32463;&#20803;&#30340;&#22810;&#37325;&#20998;&#24418;&#20998;&#26512;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#23545;&#22823;&#22411;&#27169;&#22411;&#20013;&#31070;&#32463;&#20803;&#30456;&#20114;&#20316;&#29992;&#30340;&#23450;&#37327;&#20998;&#26512;&#12290;</title><link>https://arxiv.org/abs/2402.09099</link><description>&lt;p&gt;
&#36890;&#36807;&#22810;&#37325;&#20998;&#24418;&#20998;&#26512;&#35270;&#35282;&#25506;&#32034;LLMs&#20013;&#30340;&#31070;&#32463;&#20803;&#30456;&#20114;&#20316;&#29992;&#21644;&#20986;&#29616;&#29616;&#35937;
&lt;/p&gt;
&lt;p&gt;
Exploring Neuron Interactions and Emergence in LLMs: From the Multifractal Analysis Perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09099
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#22810;&#37325;&#20998;&#24418;&#20998;&#26512;&#35270;&#35282;&#65292;&#28145;&#20837;&#30740;&#31350;&#20102;LLMs&#20013;&#31070;&#32463;&#20803;&#30456;&#20114;&#20316;&#29992;&#21644;&#20986;&#29616;&#29616;&#35937;&#12290;&#36890;&#36807;&#24341;&#20837;&#33258;&#32452;&#32455;&#21644;&#22810;&#37325;&#20998;&#24418;&#20998;&#26512;&#30340;&#27010;&#24565;&#65292;&#30740;&#31350;&#20102;&#31070;&#32463;&#20803;&#30456;&#20114;&#20316;&#29992;&#30340;&#21160;&#24577;&#28436;&#21270;&#36807;&#31243;&#65292;&#23588;&#20854;&#20851;&#27880;&#35757;&#32451;&#20013;&#30340;&#22797;&#26434;&#34892;&#20026;&#12290;&#36890;&#36807;&#25552;&#20986;&#22522;&#20110;&#31070;&#32463;&#20803;&#30340;&#22810;&#37325;&#20998;&#24418;&#20998;&#26512;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#23545;&#22823;&#22411;&#27169;&#22411;&#20013;&#31070;&#32463;&#20803;&#30456;&#20114;&#20316;&#29992;&#30340;&#23450;&#37327;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20197;&#24448;&#30340;&#22823;&#22411;&#27169;&#22411;&#20013;&#65292;&#20851;&#20110;&#20986;&#29616;&#29616;&#35937;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#21151;&#33021;&#33021;&#21147;&#22914;&#20309;&#38543;&#27169;&#22411;&#35268;&#27169;&#30340;&#25193;&#22823;&#32780;&#22686;&#21152;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#36229;&#36234;&#20102;&#36825;&#19968;&#20256;&#32479;&#33539;&#24335;&#65292;&#26088;&#22312;&#36890;&#36807;&#19981;&#20165;&#20165;&#20381;&#36182;&#20110;&#27169;&#22411;&#35268;&#27169;&#65292;&#32780;&#26356;&#21152;&#20851;&#27880;&#35757;&#32451;&#36807;&#31243;&#20013;&#31070;&#32463;&#20803;&#30456;&#20114;&#20316;&#29992;&#30340;&#22797;&#26434;&#34892;&#20026;&#65292;&#21152;&#28145;&#25105;&#20204;&#23545;LLMs&#20869;&#37096;&#20986;&#29616;&#29616;&#35937;&#30340;&#29702;&#35299;&#12290;&#36890;&#36807;&#24341;&#20837;&#8220;&#33258;&#32452;&#32455;&#8221;&#21644;&#8220;&#22810;&#37325;&#20998;&#24418;&#20998;&#26512;&#8221;&#27010;&#24565;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#31070;&#32463;&#20803;&#30456;&#20114;&#20316;&#29992;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#22914;&#20309;&#21160;&#24577;&#28436;&#21270;&#65292;&#20174;&#32780;&#23548;&#33268;&#8220;&#20986;&#29616;&#29616;&#35937;&#8221;&#65292;&#36825;&#31181;&#29616;&#35937;&#21453;&#26144;&#20102;&#33258;&#28982;&#31995;&#32479;&#20013;&#31616;&#21333;&#30340;&#24494;&#35266;&#30456;&#20114;&#20316;&#29992;&#22914;&#20309;&#23548;&#33268;&#22797;&#26434;&#30340;&#23439;&#35266;&#34892;&#20026;&#12290;&#20026;&#20102;&#23450;&#37327;&#20998;&#26512;&#35757;&#32451;&#36807;&#31243;&#20013;&#22823;&#22411;&#27169;&#22411;&#20013;&#31070;&#32463;&#20803;&#20043;&#38388;&#19981;&#26029;&#28436;&#21270;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#31070;&#32463;&#20803;&#30340;&#22810;&#37325;&#20998;&#24418;&#20998;&#26512;&#65288;NeuroMFA&#65289;&#12290;&#21033;&#29992;NeuroMFA&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#30340;&#23454;&#39564;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09099v1 Announce Type: new Abstract: Prior studies on the emergence in large models have primarily focused on how the functional capabilities of large language models (LLMs) scale with model size. Our research, however, transcends this traditional paradigm, aiming to deepen our understanding of the emergence within LLMs by placing a special emphasis not just on the model size but more significantly on the complex behavior of neuron interactions during the training process. By introducing the concepts of "self-organization" and "multifractal analysis," we explore how neuron interactions dynamically evolve during training, leading to "emergence," mirroring the phenomenon in natural systems where simple micro-level interactions give rise to complex macro-level behaviors. To quantitatively analyze the continuously evolving interactions among neurons in large models during training, we propose the Neuron-based Multifractal Analysis (NeuroMFA). Utilizing NeuroMFA, we conduct a com
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#23398;&#20064;&#20351;&#33021;&#30340;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#30340;&#26032;&#22411;&#25968;&#23383;&#23402;&#29983;&#21407;&#22411;&#65292;&#20854;&#20027;&#35201;&#30446;&#26631;&#26159;&#36827;&#34892;&#20132;&#36890;&#26631;&#24535;&#35782;&#21035;&#21644;&#36710;&#36947;&#20445;&#25345;&#12290;</title><link>https://arxiv.org/abs/2402.09097</link><description>&lt;p&gt;
&#19968;&#20010;&#29992;&#20110;&#23398;&#20064;&#20351;&#33021;&#30340;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#20132;&#36890;&#26631;&#24535;&#35782;&#21035;&#30340;&#25968;&#23383;&#23402;&#29983;&#21407;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Digital Twin prototype for traffic sign recognition of a learning-enabled autonomous vehicle
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09097
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#23398;&#20064;&#20351;&#33021;&#30340;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#30340;&#26032;&#22411;&#25968;&#23383;&#23402;&#29983;&#21407;&#22411;&#65292;&#20854;&#20027;&#35201;&#30446;&#26631;&#26159;&#36827;&#34892;&#20132;&#36890;&#26631;&#24535;&#35782;&#21035;&#21644;&#36710;&#36947;&#20445;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#23398;&#20064;&#20351;&#33021;&#30340;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#30340;&#26032;&#22411;&#25968;&#23383;&#23402;&#29983;&#21407;&#22411;&#12290;&#35813;&#25968;&#23383;&#23402;&#29983;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#36827;&#34892;&#20132;&#36890;&#26631;&#24535;&#35782;&#21035;&#21644;&#36710;&#36947;&#20445;&#25345;&#12290;&#25968;&#23383;&#23402;&#29983;&#26550;&#26500;&#22522;&#20110;&#20849;&#27169;&#20223;&#21644;&#20351;&#29992;&#20102;Functional Mock-up Interface&#21644;SystemC Transaction Level Modeling&#26631;&#20934;&#12290;&#25968;&#23383;&#23402;&#29983;&#21253;&#25324;&#22235;&#20010;&#23458;&#25143;&#31471;&#65292;i) &#22312;Amesim&#24037;&#20855;&#20013;&#35774;&#35745;&#30340;&#36710;&#36742;&#27169;&#22411;&#65292;ii) &#22312;Prescan&#20013;&#24320;&#21457;&#30340;&#29615;&#22659;&#27169;&#22411;&#65292;iii) &#22312;&#26426;&#22120;&#20154;&#25805;&#20316;&#31995;&#32479;&#20013;&#35774;&#35745;&#30340;&#36710;&#36947;&#20445;&#25345;&#25511;&#21046;&#22120;&#65292;&#20197;&#21450;iv) &#22312;BIP (&#34892;&#20026;&#65292;&#20132;&#20114;&#65292;&#20248;&#20808;&#32423;)&#24418;&#24335;&#24314;&#27169;&#35821;&#35328;&#20013;&#24320;&#21457;&#30340;&#24863;&#30693;&#21644;&#36895;&#24230;&#25511;&#21046;&#27169;&#22359;&#12290;&#36825;&#20123;&#23458;&#25143;&#31471;&#19982;&#25968;&#23383;&#23402;&#29983;&#24179;&#21488;PAVE360-VSI&#36827;&#34892;&#25509;&#21475;&#36830;&#25509;&#12290;PAVE360-VSI&#20316;&#20026;&#20849;&#27169;&#20223;&#21327;&#35843;&#22120;&#65292;&#36127;&#36131;&#36890;&#36807;&#26381;&#21153;&#22120;&#36827;&#34892;&#21516;&#27493;&#12289;&#20114;&#32852;&#21644;&#25968;&#25454;&#20132;&#25442;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09097v1 Announce Type: cross Abstract: In this paper, we present a novel digital twin prototype for a learning-enabled self-driving vehicle. The primary objective of this digital twin is to perform traffic sign recognition and lane keeping. The digital twin architecture relies on co-simulation and uses the Functional Mock-up Interface and SystemC Transaction Level Modeling standards. The digital twin consists of four clients, i) a vehicle model that is designed in Amesim tool, ii) an environment model developed in Prescan, iii) a lane-keeping controller designed in Robot Operating System, and iv) a perception and speed control module developed in the formal modeling language of BIP (Behavior, Interaction, Priority). These clients interface with the digital twin platform, PAVE360-Veloce System Interconnect (PAVE360-VSI). PAVE360-VSI acts as the co-simulation orchestrator and is responsible for synchronization, interconnection, and data exchange through a server. The server es
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25552;&#20379;&#38544;&#21547;&#30340;&#32447;&#32034;&#65292;Puzzler&#36890;&#36807;&#32469;&#36807;LLM&#30340;&#38450;&#24481;&#31574;&#30053;&#65292;&#22312;&#38388;&#25509;&#26041;&#24335;&#19979;&#23454;&#29616;&#20102;&#36234;&#29425;&#25915;&#20987;&#65292;&#25104;&#21151;&#29575;&#39640;&#36798;96.6%&#12290;</title><link>https://arxiv.org/abs/2402.09091</link><description>&lt;p&gt;
&#19982;LLM&#29609;&#29468;&#35868;&#28216;&#25103;: &#36890;&#36807;&#38544;&#21547;&#25552;&#31034;&#30340;&#38388;&#25509;&#36234;&#29425;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Play Guessing Game with LLM: Indirect Jailbreak Attack with Implicit Clues
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09091
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25552;&#20379;&#38544;&#21547;&#30340;&#32447;&#32034;&#65292;Puzzler&#36890;&#36807;&#32469;&#36807;LLM&#30340;&#38450;&#24481;&#31574;&#30053;&#65292;&#22312;&#38388;&#25509;&#26041;&#24335;&#19979;&#23454;&#29616;&#20102;&#36234;&#29425;&#25915;&#20987;&#65292;&#25104;&#21151;&#29575;&#39640;&#36798;96.6%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;LLM&#30340;&#21457;&#23637;&#65292;LLM&#30340;&#23433;&#20840;&#23041;&#32961;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#12290;&#35768;&#22810;&#36234;&#29425;&#25915;&#20987;&#24050;&#32463;&#25552;&#20986;&#26469;&#35780;&#20272;LLM&#30340;&#23433;&#20840;&#38450;&#24481;&#12290;&#30446;&#21069;&#30340;&#36234;&#29425;&#25915;&#20987;&#20027;&#35201;&#20351;&#29992;&#22330;&#26223;&#20266;&#35013;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#23545;&#24694;&#24847;&#24847;&#22270;&#30340;&#26126;&#30830;&#25552;&#21450;&#24456;&#23481;&#26131;&#34987;LLM&#35782;&#21035;&#21644;&#38450;&#24481;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38388;&#25509;&#36234;&#29425;&#25915;&#20987;&#26041;&#27861;&#65292;&#21517;&#20026;Puzzler&#65292;&#23427;&#21487;&#20197;&#36890;&#36807;&#38544;&#21547;&#22320;&#20026;LLM&#25552;&#20379;&#19968;&#20123;&#26377;&#20851;&#21407;&#22987;&#24694;&#24847;&#26597;&#35810;&#30340;&#25552;&#31034;&#26469;&#32469;&#36807;LLM&#30340;&#38450;&#24481;&#31574;&#30053;&#24182;&#33719;&#21462;&#24694;&#24847;&#21709;&#24212;&#12290;&#27492;&#22806;&#65292;&#21463;&#23385;&#23376;&#30340;&#12298;&#23385;&#23376;&#20853;&#27861;&#12299;&#20013;&#8220;&#24403;&#25915;&#26080;&#27861;&#25915;&#65292;&#23432;&#8221;&#26234;&#24935;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#37319;&#21462;&#20102;&#19968;&#31181;&#38450;&#24481;&#23039;&#24577;&#26469;&#36890;&#36807;LLM&#25910;&#38598;&#20851;&#20110;&#21407;&#22987;&#24694;&#24847;&#26597;&#35810;&#30340;&#32447;&#32034;&#12290;&#22823;&#37327;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;Puzzler&#22312;&#38381;&#28304;LLM&#19978;&#30340;&#26597;&#35810;&#25104;&#21151;&#29575;&#20026;96.6%&#65292;&#27604;&#22522;&#20934;&#32447;&#39640;57.9%-82.7%&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09091v1 Announce Type: cross Abstract: With the development of LLMs, the security threats of LLMs are getting more and more attention. Numerous jailbreak attacks have been proposed to assess the security defense of LLMs. Current jailbreak attacks primarily utilize scenario camouflage techniques. However their explicitly mention of malicious intent will be easily recognized and defended by LLMs. In this paper, we propose an indirect jailbreak attack approach, Puzzler, which can bypass the LLM's defense strategy and obtain malicious response by implicitly providing LLMs with some clues about the original malicious query. In addition, inspired by the wisdom of ''When unable to attack, defend'' from Sun Tzu's Art of War, we adopt a defensive stance to gather clues about the original malicious query through LLMs. Extensive experimental results show that Puzzler achieves a query success rate of 96.6% on closed-source LLMs, which is 57.9%-82.7% higher than baselines. Furthermore, w
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#20102;&#23545;&#20110;&#20108;&#36827;&#21046;&#20998;&#24067;&#65292;&#21487;&#35745;&#31639;&#27010;&#29575;&#30005;&#36335;&#27169;&#22411;&#22312;&#26576;&#31181;&#24847;&#20041;&#19978;&#26159;&#31561;&#20215;&#30340;&#65292;&#21363;&#20854;&#20013;&#20219;&#20309;&#19968;&#20010;&#30340;&#30005;&#36335;&#37117;&#21487;&#20197;&#36716;&#21270;&#20026;&#20219;&#20309;&#20854;&#20182;&#27169;&#22411;&#30340;&#30005;&#36335;&#65292;&#21482;&#38656;&#22810;&#39033;&#24335;&#32423;&#21035;&#30340;&#22686;&#21152;&#22823;&#23567;&#12290;&#23427;&#20204;&#26159;&#21487;&#35745;&#31639;&#30340;&#36793;&#38469;&#25512;&#26029;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.09085</link><description>&lt;p&gt;
&#21487;&#35745;&#31639;&#27010;&#29575;&#30005;&#36335;&#30340;&#22810;&#39033;&#24335;&#35821;&#20041;
&lt;/p&gt;
&lt;p&gt;
Polynomial Semantics of Tractable Probabilistic Circuits
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09085
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;&#23545;&#20110;&#20108;&#36827;&#21046;&#20998;&#24067;&#65292;&#21487;&#35745;&#31639;&#27010;&#29575;&#30005;&#36335;&#27169;&#22411;&#22312;&#26576;&#31181;&#24847;&#20041;&#19978;&#26159;&#31561;&#20215;&#30340;&#65292;&#21363;&#20854;&#20013;&#20219;&#20309;&#19968;&#20010;&#30340;&#30005;&#36335;&#37117;&#21487;&#20197;&#36716;&#21270;&#20026;&#20219;&#20309;&#20854;&#20182;&#27169;&#22411;&#30340;&#30005;&#36335;&#65292;&#21482;&#38656;&#22810;&#39033;&#24335;&#32423;&#21035;&#30340;&#22686;&#21152;&#22823;&#23567;&#12290;&#23427;&#20204;&#26159;&#21487;&#35745;&#31639;&#30340;&#36793;&#38469;&#25512;&#26029;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27010;&#29575;&#30005;&#36335;&#35745;&#31639;&#20195;&#34920;&#27010;&#29575;&#20998;&#24067;&#30340;&#22810;&#32447;&#24615;&#22810;&#39033;&#24335;&#12290;&#23427;&#20204;&#26159;&#21487;&#35745;&#31639;&#30340;&#27169;&#22411;&#65292;&#33021;&#22815;&#25903;&#25345;&#39640;&#25928;&#30340;&#36793;&#38469;&#25512;&#26029;&#12290;&#28982;&#32780;&#65292;&#22312;&#25991;&#29486;&#20013;&#24050;&#32463;&#32771;&#34385;&#20102;&#22810;&#31181;&#22810;&#39033;&#24335;&#35821;&#20041;&#65288;&#20363;&#22914;&#65292;&#32593;&#32476;&#22810;&#39033;&#24335;&#12289;&#20284;&#28982;&#22810;&#39033;&#24335;&#12289;&#29983;&#25104;&#20989;&#25968;&#12289;&#20613;&#37324;&#21494;&#21464;&#25442;&#21644;&#29305;&#24449;&#22810;&#39033;&#24335;&#65289;&#12290;&#36825;&#20123;&#20998;&#24067;&#30340;&#22810;&#39033;&#24335;&#32534;&#30721;&#20043;&#38388;&#30340;&#20851;&#31995;&#22823;&#37096;&#20998;&#26159;&#26410;&#30693;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#23545;&#20110;&#20108;&#36827;&#21046;&#20998;&#24067;&#65292;&#36825;&#20123;&#27010;&#29575;&#30005;&#36335;&#27169;&#22411;&#22312;&#26576;&#31181;&#24847;&#20041;&#19978;&#26159;&#31561;&#20215;&#30340;&#65292;&#21363;&#20854;&#20013;&#20219;&#20309;&#19968;&#20010;&#30340;&#30005;&#36335;&#37117;&#21487;&#20197;&#36716;&#21270;&#20026;&#20219;&#20309;&#20854;&#20182;&#27169;&#22411;&#30340;&#30005;&#36335;&#65292;&#21482;&#38656;&#22810;&#39033;&#24335;&#32423;&#21035;&#30340;&#22686;&#21152;&#22823;&#23567;&#12290;&#22240;&#27492;&#65292;&#23427;&#20204;&#22312;&#30456;&#21516;&#31867;&#21035;&#30340;&#20998;&#24067;&#19978;&#37117;&#26159;&#21487;&#35745;&#31639;&#30340;&#36793;&#38469;&#25512;&#26029;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#19968;&#31181;&#21517;&#20026;&#27010;&#29575;&#29983;&#25104;&#30005;&#36335;&#30340;&#22810;&#39033;&#24335;&#35821;&#20041;&#30340;&#33258;&#28982;&#25512;&#24191;&#65292;&#20197;&#36866;&#29992;&#20110;&#20998;&#31867;&#38543;&#26426;&#21464;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09085v1 Announce Type: new Abstract: Probabilistic circuits compute multilinear polynomials that represent probability distributions. They are tractable models that support efficient marginal inference. However, various polynomial semantics have been considered in the literature (e.g., network polynomials, likelihood polynomials, generating functions, Fourier transforms, and characteristic polynomials). The relationships between these polynomial encodings of distributions is largely unknown. In this paper, we prove that for binary distributions, each of these probabilistic circuit models is equivalent in the sense that any circuit for one of them can be transformed into a circuit for any of the others with only a polynomial increase in size. They are therefore all tractable for marginal inference on the same class of distributions. Finally, we explore the natural extension of one such polynomial semantics, called probabilistic generating circuits, to categorical random varia
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23558;&#23548;&#25968;&#20449;&#24687;&#34701;&#20837;&#25439;&#22833;&#20989;&#25968;&#26469;&#25913;&#21892;&#31639;&#23376;&#23398;&#20064;&#26694;&#26550;&#30340;&#24615;&#33021;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#19981;&#35268;&#21017;&#32593;&#26684;&#19978;&#36817;&#20284;&#23548;&#25968;&#30340;&#26032;&#26694;&#26550;&#65292;&#20174;&#32780;&#34920;&#26126;Sobolev&#35757;&#32451;&#22312;&#36817;&#20284;&#35299;&#31639;&#23376;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.09084</link><description>&lt;p&gt;
Sobolev&#35757;&#32451;&#29992;&#20110;&#31639;&#23376;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Sobolev Training for Operator Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09084
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23558;&#23548;&#25968;&#20449;&#24687;&#34701;&#20837;&#25439;&#22833;&#20989;&#25968;&#26469;&#25913;&#21892;&#31639;&#23376;&#23398;&#20064;&#26694;&#26550;&#30340;&#24615;&#33021;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#19981;&#35268;&#21017;&#32593;&#26684;&#19978;&#36817;&#20284;&#23548;&#25968;&#30340;&#26032;&#26694;&#26550;&#65292;&#20174;&#32780;&#34920;&#26126;Sobolev&#35757;&#32451;&#22312;&#36817;&#20284;&#35299;&#31639;&#23376;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;Sobolev&#35757;&#32451;&#23545;&#20110;&#25913;&#21892;&#27169;&#22411;&#24615;&#33021;&#30340;&#31639;&#23376;&#23398;&#20064;&#26694;&#26550;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#23558;&#23548;&#25968;&#20449;&#24687;&#34701;&#20837;&#25439;&#22833;&#20989;&#25968;&#21487;&#20197;&#25913;&#21892;&#35757;&#32451;&#36807;&#31243;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#31639;&#23376;&#23398;&#20064;&#20013;&#36817;&#20284;&#19981;&#35268;&#21017;&#32593;&#26684;&#19978;&#30340;&#23548;&#25968;&#30340;&#26032;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#24471;&#21040;&#20102;&#23454;&#39564;&#21644;&#29702;&#35770;&#20998;&#26512;&#30340;&#25903;&#25345;&#12290;&#36825;&#34920;&#26126;&#20102;Sobolev&#35757;&#32451;&#22312;&#36817;&#20284;&#26080;&#31351;&#32500;&#31354;&#38388;&#20013;&#30340;&#35299;&#31639;&#23376;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09084v1 Announce Type: cross Abstract: This study investigates the impact of Sobolev Training on operator learning frameworks for improving model performance. Our research reveals that integrating derivative information into the loss function enhances the training process, and we propose a novel framework to approximate derivatives on irregular meshes in operator learning. Our findings are supported by both experimental evidence and theoretical analysis. This demonstrates the effectiveness of Sobolev Training in approximating the solution operators between infinite-dimensional spaces.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#21019;&#26032;&#26041;&#27861;&#65292;ExpD3 &#21644; BE-TD3&#65292;&#29992;&#20110;&#35299;&#20915;&#21644;&#21033;&#29992; Actor-Critic &#26041;&#27861;&#20013;&#30340;&#20272;&#35745;&#20559;&#24046;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#36825;&#20123;&#31639;&#27861;&#22312;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#20013;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#39640;&#25928;&#12290;</title><link>https://arxiv.org/abs/2402.09078</link><description>&lt;p&gt;
&#22312; Actor-Critic &#26041;&#27861;&#20013;&#21033;&#29992;&#20272;&#35745;&#20559;&#24046;&#30340;&#28145;&#24230;&#21452; Q-Learning &#30340;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Exploiting Estimation Bias in Deep Double Q-Learning for Actor-Critic Methods
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09078
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#21019;&#26032;&#26041;&#27861;&#65292;ExpD3 &#21644; BE-TD3&#65292;&#29992;&#20110;&#35299;&#20915;&#21644;&#21033;&#29992; Actor-Critic &#26041;&#27861;&#20013;&#30340;&#20272;&#35745;&#20559;&#24046;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#36825;&#20123;&#31639;&#27861;&#22312;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#20013;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22312;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#20013;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#37325;&#28857;&#26159;&#35299;&#20915;&#21644;&#21033;&#29992;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#20013; Actor-Critic &#26041;&#27861;&#20013;&#30340;&#20272;&#35745;&#20559;&#24046;&#38382;&#39064;&#65292;&#20351;&#29992;&#20102;&#28145;&#24230;&#21452; Q-Learning&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#31639;&#27861;&#65306;Expectile Delayed Deep Deterministic Policy Gradient (ExpD3) &#21644; Bias Exploiting - Twin Delayed Deep Deterministic Policy Gradient (BE-TD3)&#12290;ExpD3 &#26088;&#22312;&#36890;&#36807;&#21333;&#19968;&#30340; Q &#20272;&#35745;&#26469;&#20943;&#23569;&#36807;&#24230;&#20272;&#35745;&#20559;&#24046;&#65292;&#24182;&#22312;&#35745;&#31639;&#25928;&#29575;&#21644;&#24615;&#33021;&#20043;&#38388;&#25552;&#20379;&#24179;&#34913;&#65292;&#32780; BE-TD3 &#21017;&#26088;&#22312;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#21160;&#24577;&#36873;&#25321;&#26368;&#26377;&#21033;&#30340;&#20272;&#35745;&#20559;&#24046;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#30340;&#23454;&#39564;&#65292;&#22312;&#21508;&#31181;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20123;&#31639;&#27861;&#22312;&#19982; TD3 &#31561;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#23588;&#20854;&#26159;&#22312;&#20272;&#35745;&#20559;&#24046;&#26174;&#33879;&#24433;&#21709;&#23398;&#20064;&#30340;&#29615;&#22659;&#20013;&#65292;&#21487;&#20197;&#21305;&#25932;&#25110;&#36229;&#36234;&#23427;&#20204;&#12290;&#36825;&#20123;&#32467;&#26524;&#20984;&#26174;&#20102;&#20272;&#35745;&#20559;&#24046;&#23545;&#23398;&#20064;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09078v1 Announce Type: cross Abstract: This paper introduces innovative methods in Reinforcement Learning (RL), focusing on addressing and exploiting estimation biases in Actor-Critic methods for continuous control tasks, using Deep Double Q-Learning. We propose two novel algorithms: Expectile Delayed Deep Deterministic Policy Gradient (ExpD3) and Bias Exploiting - Twin Delayed Deep Deterministic Policy Gradient (BE-TD3). ExpD3 aims to reduce overestimation bias with a single $Q$ estimate, offering a balance between computational efficiency and performance, while BE-TD3 is designed to dynamically select the most advantageous estimation bias during training. Our extensive experiments across various continuous control tasks demonstrate the effectiveness of our approaches. We show that these algorithms can either match or surpass existing methods like TD3, particularly in environments where estimation biases significantly impact learning. The results underline the importance of
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#22266;&#20307;&#24223;&#29289;&#22312;&#36965;&#24863;&#22270;&#20687;&#20013;&#30340;&#26816;&#27979;&#26041;&#27861;&#12290;&#30740;&#31350;&#32773;&#21033;&#29992;&#22320;&#29699;&#35266;&#27979;&#21355;&#26143;&#25552;&#20379;&#30340;&#39640;&#20998;&#36776;&#29575;&#25968;&#25454;&#65292;&#36890;&#36807;&#36965;&#24863;&#22270;&#20687;&#23454;&#29616;&#20102;&#22266;&#20307;&#24223;&#29289;&#22788;&#32622;&#22330;&#22320;&#30340;&#35782;&#21035;&#12289;&#30417;&#27979;&#21644;&#35780;&#20272;&#12290;</title><link>https://arxiv.org/abs/2402.09066</link><description>&lt;p&gt;
&#36965;&#24863;&#22270;&#20687;&#20013;&#30340;&#22266;&#20307;&#24223;&#29289;&#26816;&#27979;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Solid Waste Detection in Remote Sensing Images: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09066
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#22266;&#20307;&#24223;&#29289;&#22312;&#36965;&#24863;&#22270;&#20687;&#20013;&#30340;&#26816;&#27979;&#26041;&#27861;&#12290;&#30740;&#31350;&#32773;&#21033;&#29992;&#22320;&#29699;&#35266;&#27979;&#21355;&#26143;&#25552;&#20379;&#30340;&#39640;&#20998;&#36776;&#29575;&#25968;&#25454;&#65292;&#36890;&#36807;&#36965;&#24863;&#22270;&#20687;&#23454;&#29616;&#20102;&#22266;&#20307;&#24223;&#29289;&#22788;&#32622;&#22330;&#22320;&#30340;&#35782;&#21035;&#12289;&#30417;&#27979;&#21644;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35782;&#21035;&#21644;&#34920;&#24449;&#38750;&#27861;&#22266;&#20307;&#24223;&#29289;&#22788;&#32622;&#22330;&#22320;&#23545;&#29615;&#22659;&#20445;&#25252;&#33267;&#20851;&#37325;&#35201;&#65292;&#29305;&#21035;&#26159;&#24212;&#23545;&#27745;&#26579;&#21644;&#20581;&#24247;&#21361;&#23475;&#12290;&#19981;&#24403;&#31649;&#29702;&#30340;&#22403;&#22334;&#22635;&#22475;&#22330;&#36890;&#36807;&#38632;&#27700;&#28183;&#36879;&#27745;&#26579;&#22303;&#22756;&#21644;&#22320;&#19979;&#27700;&#65292;&#23545;&#21160;&#29289;&#21644;&#20154;&#31867;&#26500;&#25104;&#23041;&#32961;&#12290;&#20256;&#32479;&#30340;&#22635;&#22475;&#22330;&#36776;&#35782;&#26041;&#27861;&#65292;&#22914;&#29616;&#22330;&#26816;&#26597;&#65292;&#32791;&#26102;&#19988;&#26114;&#36149;&#12290;&#36965;&#24863;&#25216;&#26415;&#26159;&#29992;&#20110;&#35782;&#21035;&#21644;&#30417;&#27979;&#22266;&#20307;&#24223;&#29289;&#22788;&#32622;&#22330;&#22320;&#30340;&#19968;&#31181;&#32463;&#27982;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#20197;&#23454;&#29616;&#24191;&#27867;&#35206;&#30422;&#21644;&#22810;&#27425;&#33719;&#21462;&#12290;&#22320;&#29699;&#35266;&#27979;&#65288;EO&#65289;&#21355;&#26143;&#37197;&#22791;&#20102;&#19968;&#31995;&#21015;&#20256;&#24863;&#22120;&#21644;&#25104;&#20687;&#33021;&#21147;&#65292;&#20960;&#21313;&#24180;&#26469;&#19968;&#30452;&#25552;&#20379;&#39640;&#20998;&#36776;&#29575;&#30340;&#25968;&#25454;&#12290;&#30740;&#31350;&#20154;&#21592;&#25552;&#20986;&#20102;&#19987;&#38376;&#30340;&#25216;&#26415;&#65292;&#21033;&#29992;&#36965;&#24863;&#22270;&#20687;&#25191;&#34892;&#19968;&#31995;&#21015;&#20219;&#21153;&#65292;&#22914;&#24223;&#29289;&#22330;&#22320;&#26816;&#27979;&#12289;&#20542;&#20498;&#22330;&#30417;&#27979;&#21644;&#36866;&#23452;&#20301;&#32622;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09066v1 Announce Type: cross Abstract: The detection and characterization of illegal solid waste disposal sites are essential for environmental protection, particularly for mitigating pollution and health hazards. Improperly managed landfills contaminate soil and groundwater via rainwater infiltration, posing threats to both animals and humans. Traditional landfill identification approaches, such as on-site inspections, are time-consuming and expensive. Remote sensing is a cost-effective solution for the identification and monitoring of solid waste disposal sites that enables broad coverage and repeated acquisitions over time. Earth Observation (EO) satellites, equipped with an array of sensors and imaging capabilities, have been providing high-resolution data for several decades. Researchers proposed specialized techniques that leverage remote sensing imagery to perform a range of tasks such as waste site detection, dumping site monitoring, and assessment of suitable locati
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#20840;&#21516;&#24577;&#21152;&#23494;&#36827;&#34892;&#38544;&#31169;&#20445;&#25252;&#24494;&#35843;&#30340;&#31995;&#32479;BlindTuner&#65292;&#23427;&#21487;&#20197;&#22312;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#23454;&#29616;&#23545;Transformer&#27169;&#22411;&#30340;&#38544;&#31169;&#20445;&#25252;&#35757;&#32451;&#65292;&#24182;&#22312;&#20934;&#30830;&#24615;&#19978;&#19982;&#38750;&#21152;&#23494;&#27169;&#22411;&#30456;&#24403;&#12290;</title><link>https://arxiv.org/abs/2402.09059</link><description>&lt;p&gt;
&#25105;&#30475;&#19981;&#35265;&#23427;&#65292;&#20294;&#25105;&#21487;&#20197;&#24494;&#35843;&#23427;&#65306;&#20351;&#29992;&#20840;&#21516;&#24577;&#21152;&#23494;&#23545;Transformer&#36827;&#34892;&#21152;&#23494;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
I can't see it but I can Fine-tune it: On Encrypted Fine-tuning of Transformers using Fully Homomorphic Encryption
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09059
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#20840;&#21516;&#24577;&#21152;&#23494;&#36827;&#34892;&#38544;&#31169;&#20445;&#25252;&#24494;&#35843;&#30340;&#31995;&#32479;BlindTuner&#65292;&#23427;&#21487;&#20197;&#22312;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#23454;&#29616;&#23545;Transformer&#27169;&#22411;&#30340;&#38544;&#31169;&#20445;&#25252;&#35757;&#32451;&#65292;&#24182;&#22312;&#20934;&#30830;&#24615;&#19978;&#19982;&#38750;&#21152;&#23494;&#27169;&#22411;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24403;&#21069;&#30340;&#26426;&#22120;&#23398;&#20064;&#29615;&#22659;&#20013;&#65292;&#24494;&#35843;&#39044;&#35757;&#32451;&#30340;Transformer&#27169;&#22411;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#37325;&#35201;&#30340;&#25216;&#26415;&#65292;&#29305;&#21035;&#26159;&#22312;&#35757;&#32451;&#25968;&#25454;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#12290;&#28982;&#32780;&#65292;&#24403;&#25968;&#25454;&#20849;&#20139;&#36935;&#21040;&#38556;&#30861;&#26102;&#65292;&#22914;&#20005;&#26684;&#30340;&#38544;&#31169;&#27861;&#35268;&#25110;&#29992;&#25143;&#23545;&#20010;&#20154;&#20449;&#24687;&#25259;&#38706;&#30340;&#25285;&#24551;&#65292;&#23601;&#20250;&#20986;&#29616;&#25361;&#25112;&#12290;&#26089;&#26399;&#22522;&#20110;&#23433;&#20840;&#22810;&#26041;&#35745;&#31639;&#65288;SMC&#65289;&#21644;&#20840;&#21516;&#24577;&#21152;&#23494;&#65288;FHE&#65289;&#30340;&#38544;&#31169;&#20445;&#25252;&#26426;&#22120;&#23398;&#20064;&#65288;PPML&#65289;&#30340;&#24037;&#20316;&#26356;&#27880;&#37325;&#38544;&#31169;&#20445;&#25252;&#30340;&#25512;&#29702;&#32780;&#19981;&#26159;&#38544;&#31169;&#20445;&#25252;&#30340;&#35757;&#32451;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;BlindTuner&#65292;&#19968;&#31181;&#38544;&#31169;&#20445;&#25252;&#30340;&#24494;&#35843;&#31995;&#32479;&#65292;&#21487;&#23454;&#29616;&#23545;Transformer&#27169;&#22411;&#22522;&#20110;&#20840;&#21516;&#24577;&#21152;&#23494;&#25968;&#25454;&#30340;&#35757;&#32451;&#65292;&#29992;&#20110;&#22270;&#20687;&#20998;&#31867;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#39564;&#35777;&#20102;BlindTuner&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#35777;&#26126;&#20854;&#19982;&#38750;&#21152;&#23494;&#27169;&#22411;&#30456;&#27604;&#20934;&#30830;&#24615;&#30456;&#24403;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#31361;&#20986;&#20102;&#19968;&#20123;&#37325;&#35201;&#30340;&#21019;&#26032;&#21644;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09059v1 Announce Type: cross Abstract: In today's machine learning landscape, fine-tuning pretrained transformer models has emerged as an essential technique, particularly in scenarios where access to task-aligned training data is limited. However, challenges surface when data sharing encounters obstacles due to stringent privacy regulations or user apprehension regarding personal information disclosure. Earlier works based on secure multiparty computation (SMC) and fully homomorphic encryption (FHE) for privacy-preserving machine learning (PPML) focused more on privacy-preserving inference than privacy-preserving training. In response, we introduce BlindTuner, a privacy-preserving fine-tuning system that enables transformer training exclusively on homomorphically encrypted data for image classification. Our extensive experimentation validates BlindTuner's effectiveness by demonstrating comparable accuracy to non-encrypted models. Notably, our findings highlight a substantia
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20851;&#20110;&#35777;&#25454;&#28145;&#24230;&#23398;&#20064;&#30340;&#26032;&#29702;&#35770;&#27934;&#35265;, &#39640;&#20142;&#20102;&#22312;&#20248;&#21270;&#20108;&#38454;&#25439;&#22833;&#20989;&#25968;&#21644;&#35299;&#37322;&#24471;&#20986;&#30340;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#24230;&#37327;&#19978;&#30340;&#22256;&#38590;&#24615;</title><link>https://arxiv.org/abs/2402.09056</link><description>&lt;p&gt;
&#35777;&#25454;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#26159;&#21542;&#20934;&#30830;&#22320;&#34920;&#31034;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#65311;
&lt;/p&gt;
&lt;p&gt;
Is Epistemic Uncertainty Faithfully Represented by Evidential Deep Learning Methods?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09056
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20851;&#20110;&#35777;&#25454;&#28145;&#24230;&#23398;&#20064;&#30340;&#26032;&#29702;&#35770;&#27934;&#35265;, &#39640;&#20142;&#20102;&#22312;&#20248;&#21270;&#20108;&#38454;&#25439;&#22833;&#20989;&#25968;&#21644;&#35299;&#37322;&#24471;&#20986;&#30340;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#24230;&#37327;&#19978;&#30340;&#22256;&#38590;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#20449;&#30340;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#19981;&#20165;&#24212;&#36820;&#22238;&#20934;&#30830;&#30340;&#39044;&#27979;&#32467;&#26524;&#65292;&#36824;&#24212;&#25552;&#20379;&#21487;&#38752;&#30340;&#19981;&#30830;&#23450;&#24615;&#34920;&#31034;&#12290;&#36125;&#21494;&#26031;&#26041;&#27861;&#24120;&#29992;&#20110;&#37327;&#21270;&#19981;&#30830;&#23450;&#24615;&#65292;&#20294;&#36817;&#24180;&#26469;&#65292;&#35777;&#25454;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#31561;&#26367;&#20195;&#26041;&#27861;&#20063;&#21464;&#24471;&#27969;&#34892;&#36215;&#26469;&#12290;&#21518;&#32773;&#26412;&#36136;&#19978;&#25193;&#23637;&#20102;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#65288;ERM&#65289;&#65292;&#29992;&#20110;&#39044;&#27979;&#32467;&#26524;&#30340;&#20108;&#38454;&#27010;&#29575;&#20998;&#24067;&#65292;&#20174;&#20013;&#21487;&#20197;&#25552;&#21462;&#35748;&#35782;&#65288;&#21644;&#38543;&#26426;&#65289;&#19981;&#30830;&#23450;&#24615;&#30340;&#24230;&#37327;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#35777;&#25454;&#28145;&#24230;&#23398;&#20064;&#30340;&#26032;&#29702;&#35770;&#27934;&#35265;&#65292;&#24378;&#35843;&#20102;&#20248;&#21270;&#20108;&#38454;&#25439;&#22833;&#20989;&#25968;&#20197;&#21450;&#35299;&#37322;&#32467;&#26524;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#24230;&#37327;&#30340;&#22256;&#38590;&#24615;&#12290;&#36890;&#36807;&#31995;&#32479;&#21270;&#30340;&#35774;&#32622;&#65292;&#28085;&#30422;&#20102;&#20998;&#31867;&#12289;&#22238;&#24402;&#21644;&#35745;&#25968;&#30340;&#24191;&#27867;&#26041;&#27861;&#65292;&#36825;&#31687;&#35770;&#25991;&#20026;&#21487;&#36776;&#35782;&#24615;&#21644;&#25910;&#25947;&#24615;&#38382;&#39064;&#25552;&#20379;&#20102;&#26032;&#30340;&#27934;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09056v1 Announce Type: new Abstract: Trustworthy ML systems should not only return accurate predictions, but also a reliable representation of their uncertainty. Bayesian methods are commonly used to quantify both aleatoric and epistemic uncertainty, but alternative approaches, such as evidential deep learning methods, have become popular in recent years. The latter group of methods in essence extends empirical risk minimization (ERM) for predicting second-order probability distributions over outcomes, from which measures of epistemic (and aleatoric) uncertainty can be extracted. This paper presents novel theoretical insights of evidential deep learning, highlighting the difficulties in optimizing second-order loss functions and interpreting the resulting epistemic uncertainty measures. With a systematic setup that covers a wide range of approaches for classification, regression and counts, it provides novel insights into issues of identifiability and convergence in second-o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36890;&#36807;&#23545;&#27604;&#39044;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#21629;&#21517;&#20026;CVLA&#65292;&#29992;&#20110;&#30701;&#35270;&#39057;&#24189;&#40664;&#26816;&#27979;&#12290;CVLA&#19981;&#20165;&#36866;&#29992;&#20110;&#21508;&#31181;&#27169;&#24577;&#20449;&#21495;&#65292;&#36824;&#33021;&#36890;&#36807;&#22312;&#19968;&#33268;&#30340;&#35821;&#20041;&#31354;&#38388;&#20013;&#23545;&#40784;&#35270;&#39057;&#21644;&#35821;&#35328;&#32452;&#20214;&#20135;&#29983;&#36866;&#21512;&#30340;&#22810;&#27169;&#24335;&#34920;&#31034;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;CVLA&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#21644;&#20960;&#20010;&#31454;&#20105;&#22522;&#20934;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.09055</link><description>&lt;p&gt;
&#36890;&#36807;&#23545;&#27604;&#39044;&#35757;&#32451;&#30340;&#26041;&#27861;&#21033;&#29992;&#35780;&#35770;&#36741;&#21161;&#35270;&#39057;&#35821;&#35328;&#23545;&#40784;&#29992;&#20110;&#30701;&#35270;&#39057;&#24189;&#40664;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Comment-aided Video-Language Alignment via Contrastive Pre-training for Short-form Video Humor Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09055
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36890;&#36807;&#23545;&#27604;&#39044;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#21629;&#21517;&#20026;CVLA&#65292;&#29992;&#20110;&#30701;&#35270;&#39057;&#24189;&#40664;&#26816;&#27979;&#12290;CVLA&#19981;&#20165;&#36866;&#29992;&#20110;&#21508;&#31181;&#27169;&#24577;&#20449;&#21495;&#65292;&#36824;&#33021;&#36890;&#36807;&#22312;&#19968;&#33268;&#30340;&#35821;&#20041;&#31354;&#38388;&#20013;&#23545;&#40784;&#35270;&#39057;&#21644;&#35821;&#35328;&#32452;&#20214;&#20135;&#29983;&#36866;&#21512;&#30340;&#22810;&#27169;&#24335;&#34920;&#31034;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;CVLA&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#21644;&#20960;&#20010;&#31454;&#20105;&#22522;&#20934;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#30701;&#35270;&#39057;&#22312;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19978;&#30340;&#24433;&#21709;&#21147;&#26085;&#30410;&#25193;&#22823;&#65292;&#22810;&#27169;&#24335;&#24189;&#40664;&#26816;&#27979;&#22312;&#24773;&#24863;&#35745;&#31639;&#20013;&#30340;&#37325;&#35201;&#24615;&#20063;&#26085;&#30410;&#22686;&#21152;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29992;&#20110;&#30701;&#35270;&#39057;&#24189;&#40664;&#26816;&#27979;&#30340;&#20004;&#23618;&#20998;&#23618;&#27169;&#22411;&#65292;&#21629;&#21517;&#20026;&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#30340;&#22810;&#27169;&#24335;&#23545;&#27604;&#39044;&#35757;&#32451;&#30340;&#35780;&#35770;&#36741;&#21161;&#35270;&#39057;&#35821;&#35328;&#23545;&#40784;&#65288;CVLA&#65289;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;CVLA&#19981;&#20165;&#36866;&#29992;&#20110;&#21508;&#31181;&#27169;&#24577;&#20449;&#21495;&#65292;&#24182;&#19988;&#36890;&#36807;&#22312;&#19968;&#33268;&#30340;&#35821;&#20041;&#31354;&#38388;&#20013;&#23545;&#40784;&#35270;&#39057;&#21644;&#35821;&#35328;&#32452;&#20214;&#65292;&#20135;&#29983;&#19968;&#20010;&#36866;&#21512;&#30340;&#22810;&#27169;&#24335;&#34920;&#31034;&#12290;&#22312;&#20004;&#20010;&#24189;&#40664;&#26816;&#27979;&#25968;&#25454;&#38598;DY11k&#21644;UR-FUNNY&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;CVLA&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#21644;&#20960;&#20010;&#31454;&#20105;&#22522;&#20934;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#12289;&#20195;&#30721;&#21644;&#27169;&#22411;&#21457;&#24067;&#22312;https://github.com/yliu-cs/CVLA&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09055v1 Announce Type: cross Abstract: The growing importance of multi-modal humor detection within affective computing correlates with the expanding influence of short-form video sharing on social media platforms. In this paper, we propose a novel two-branch hierarchical model for short-form video humor detection (SVHD), named Comment-aided Video-Language Alignment (CVLA) via data-augmented multi-modal contrastive pre-training. Notably, our CVLA not only operates on raw signals across various modal channels but also yields an appropriate multi-modal representation by aligning the video and language components within a consistent semantic space. The experimental results on two humor detection datasets, including DY11k and UR-FUNNY, demonstrate that CVLA dramatically outperforms state-of-the-art and several competitive baseline approaches. Our dataset, code and model release at https://github.com/yliu-cs/CVLA.
&lt;/p&gt;</description></item><item><title>L3GO&#26159;&#19968;&#31181;&#20351;&#29992;3D&#24605;&#32500;&#38142;&#29983;&#25104;&#38750;&#24120;&#35268;&#23545;&#35937;&#30340;&#35821;&#35328;&#20195;&#29702;&#65292;&#33021;&#22815;&#23545;&#29289;&#20307;&#30340;&#29289;&#29702;&#21644;&#31354;&#38388;&#37197;&#32622;&#36827;&#34892;&#31934;&#30830;&#25512;&#29702;&#65292;&#24182;&#36890;&#36807;&#35797;&#38169;&#30340;&#26041;&#24335;&#22312;3D&#27169;&#25311;&#29615;&#22659;&#20013;&#29983;&#25104;&#25152;&#38656;&#30340;&#23545;&#35937;&#12290;</title><link>https://arxiv.org/abs/2402.09052</link><description>&lt;p&gt;
L3GO: &#20351;&#29992;3D&#24605;&#32500;&#38142;&#29983;&#25104;&#38750;&#24120;&#35268;&#23545;&#35937;&#30340;&#35821;&#35328;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
L3GO: Language Agents with Chain-of-3D-Thoughts for Generating Unconventional Objects
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09052
&lt;/p&gt;
&lt;p&gt;
L3GO&#26159;&#19968;&#31181;&#20351;&#29992;3D&#24605;&#32500;&#38142;&#29983;&#25104;&#38750;&#24120;&#35268;&#23545;&#35937;&#30340;&#35821;&#35328;&#20195;&#29702;&#65292;&#33021;&#22815;&#23545;&#29289;&#20307;&#30340;&#29289;&#29702;&#21644;&#31354;&#38388;&#37197;&#32622;&#36827;&#34892;&#31934;&#30830;&#25512;&#29702;&#65292;&#24182;&#36890;&#36807;&#35797;&#38169;&#30340;&#26041;&#24335;&#22312;3D&#27169;&#25311;&#29615;&#22659;&#20013;&#29983;&#25104;&#25152;&#38656;&#30340;&#23545;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09052v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;&#25688;&#35201;&#65306;DALL-E 3&#21644;Stable Diffusion-XL&#31561;&#22522;&#20110;&#25193;&#25955;&#30340;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#23637;&#31034;&#20102;&#22312;&#29983;&#25104;&#20855;&#26377;&#36924;&#30495;&#21644;&#29420;&#29305;&#26500;&#36896;&#30340;&#22270;&#20687;&#26041;&#38754;&#30340;&#26174;&#33879;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#24403;&#25351;&#31034;&#38750;&#24120;&#35268;&#30340;&#12289;&#19981;&#23646;&#20110;&#20998;&#24067;&#33539;&#22260;&#30340;&#25551;&#36848;&#26102;&#65292;&#22914;&#8220;&#24102;&#26377;&#20116;&#26465;&#33151;&#30340;&#26885;&#23376;&#8221;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#31934;&#30830;&#25512;&#29702;&#29289;&#20307;&#30340;&#29289;&#29702;&#21644;&#31354;&#38388;&#37197;&#32622;&#26041;&#38754;&#19981;&#22815;&#40065;&#26834;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;3D&#24605;&#32500;&#38142;&#65288;L3GO&#65289;&#30340;&#35821;&#35328;&#20195;&#29702;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#25512;&#29702;&#20986;&#36890;&#36807;&#24403;&#21069;&#25968;&#25454;&#39537;&#21160;&#30340;&#25193;&#25955;&#27169;&#22411;&#38590;&#20197;&#22788;&#29702;&#30340;&#38750;&#24120;&#35268;&#23545;&#35937;&#30340;&#22522;&#20110;&#37096;&#20998;&#30340;3D&#32593;&#26684;&#29983;&#25104;&#12290;&#26356;&#20855;&#20307;&#22320;&#35828;&#65292;&#25105;&#20204;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#20195;&#29702;&#65292;&#22312;3D&#27169;&#25311;&#29615;&#22659;&#20013;&#36890;&#36807;&#35797;&#38169;&#26469;&#32452;&#25104;&#25152;&#38656;&#30340;&#23545;&#35937;&#12290;&#20026;&#20102;&#20419;&#36827;&#25105;&#20204;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#38598;&#65292;&#38750;&#24120;&#35268;&#21487;&#34892;&#23545;&#35937;&#65288;UFO&#65289;&#65292;&#20197;&#21450;SimpleBlenv&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09052v1 Announce Type: new Abstract: Diffusion-based image generation models such as DALL-E 3 and Stable Diffusion-XL demonstrate remarkable capabilities in generating images with realistic and unique compositions. Yet, these models are not robust in precisely reasoning about physical and spatial configurations of objects, especially when instructed with unconventional, thereby out-of-distribution descriptions, such as "a chair with five legs". In this paper, we propose a language agent with chain-of-3D-thoughts (L3GO), an inference-time approach that can reason about part-based 3D mesh generation of unconventional objects that current data-driven diffusion models struggle with. More concretely, we use large language models as agents to compose a desired object via trial-and-error within the 3D simulation environment. To facilitate our investigation, we develop a new benchmark, Unconventionally Feasible Objects (UFO), as well as SimpleBlenv, a wrapper environment built on to
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;FGeo-DRL&#65292;&#19968;&#20010;&#29992;&#20110;&#33258;&#21160;&#25191;&#34892;&#20960;&#20309;&#28436;&#32462;&#25512;&#29702;&#30340;&#31070;&#32463;&#31526;&#21495;&#31995;&#32479;&#12290;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#21644;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#35813;&#31995;&#32479;&#33021;&#22815;&#33258;&#20027;&#23398;&#20064;&#35299;&#20915;&#20960;&#20309;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#20154;&#31867;&#21270;&#30340;&#28436;&#32462;&#25512;&#29702;&#12290;</title><link>https://arxiv.org/abs/2402.09051</link><description>&lt;p&gt;
FGeo-DRL: &#36890;&#36807;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#20960;&#20309;&#38382;&#39064;&#30340;&#28436;&#32462;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
FGeo-DRL: Deductive Reasoning for Geometric Problems through Deep Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09051
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;FGeo-DRL&#65292;&#19968;&#20010;&#29992;&#20110;&#33258;&#21160;&#25191;&#34892;&#20960;&#20309;&#28436;&#32462;&#25512;&#29702;&#30340;&#31070;&#32463;&#31526;&#21495;&#31995;&#32479;&#12290;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#21644;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#35813;&#31995;&#32479;&#33021;&#22815;&#33258;&#20027;&#23398;&#20064;&#35299;&#20915;&#20960;&#20309;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#20154;&#31867;&#21270;&#30340;&#28436;&#32462;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#20154;&#31867;&#21270;&#30340;&#28436;&#32462;&#25512;&#29702;&#19968;&#30452;&#20197;&#26469;&#37117;&#26159;&#25968;&#23398;&#21644;&#20154;&#24037;&#26234;&#33021;&#20132;&#21449;&#23398;&#31185;&#20013;&#26368;&#20855;&#25361;&#25112;&#24615;&#30340;&#24320;&#25918;&#38382;&#39064;&#20043;&#19968;&#12290;&#26412;&#25991;&#26159;&#25105;&#20204;&#24037;&#20316;&#31995;&#21015;&#20013;&#30340;&#31532;&#19977;&#31687;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#21517;&#20026;FGeoDRL&#30340;&#31070;&#32463;&#31526;&#21495;&#31995;&#32479;&#65292;&#29992;&#20110;&#33258;&#21160;&#25191;&#34892;&#20154;&#31867;&#21270;&#20960;&#20309;&#28436;&#32462;&#25512;&#29702;&#12290;&#31070;&#32463;&#37096;&#20998;&#26159;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#65292;&#33021;&#22815;&#36890;&#36807;&#23545;&#24418;&#24335;&#21270;&#29615;&#22659;&#30340;&#21453;&#39304;&#33258;&#20027;&#22320;&#23398;&#20064;&#35299;&#20915;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#26080;&#38656;&#20154;&#31867;&#30417;&#30563;&#12290;&#23427;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#33258;&#28982;&#35821;&#35328;&#27169;&#22411;&#24314;&#31435;&#20102;&#19968;&#20010;&#31574;&#30053;&#32593;&#32476;&#65292;&#29992;&#20110;&#23450;&#29702;&#36873;&#25321;&#65292;&#24182;&#20351;&#29992;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#36827;&#34892;&#21551;&#21457;&#24335;&#25506;&#32034;&#12290;&#31526;&#21495;&#37096;&#20998;&#26159;&#22522;&#20110;&#20960;&#20309;&#24418;&#24335;&#21270;&#29702;&#35770;&#21644;FormalGeo\cite{FormalGeo}&#30340;&#24378;&#21270;&#23398;&#20064;&#29615;&#22659;&#65292;&#23558;GPS&#27169;&#22411;&#21270;&#20026;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;\cite{MDP}&#12290;&#22312;&#36825;&#20010;&#24418;&#24335;&#21270;&#31526;&#21495;&#31995;&#32479;&#20013;&#65292;&#24050;&#30693;&#26465;&#20214;&#21644;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09051v1 Announce Type: new Abstract: The human-like automatic deductive reasoning has always been one of the most challenging open problems in the interdiscipline of mathematics and artificial intelligence. This paper is the third in a series of our works. We built a neural-symbolic system, called FGeoDRL, to automatically perform human-like geometric deductive reasoning. The neural part is an AI agent based on reinforcement learning, capable of autonomously learning problem-solving methods from the feedback of a formalized environment, without the need for human supervision. It leverages a pre-trained natural language model to establish a policy network for theorem selection and employ Monte Carlo Tree Search for heuristic exploration. The symbolic part is a reinforcement learning environment based on geometry formalization theory and FormalGeo\cite{FormalGeo}, which models GPS as a Markov Decision Process\cite{MDP}. In this formal symbolic system, the known conditions and 
&lt;/p&gt;</description></item><item><title>FGeo-TP&#26159;&#19968;&#31181;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#30340;&#20960;&#20309;&#38382;&#39064;&#27714;&#35299;&#22120;&#65292;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#39044;&#27979;&#23450;&#29702;&#24207;&#21015;&#26469;&#35299;&#20915;&#20960;&#20309;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.09047</link><description>&lt;p&gt;
FGeo-TP&#65306;&#19968;&#31181;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#30340;&#20960;&#20309;&#38382;&#39064;&#27714;&#35299;&#22120;
&lt;/p&gt;
&lt;p&gt;
FGeo-TP: A Language Model-Enhanced Solver for Geometry Problems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09047
&lt;/p&gt;
&lt;p&gt;
FGeo-TP&#26159;&#19968;&#31181;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#30340;&#20960;&#20309;&#38382;&#39064;&#27714;&#35299;&#22120;&#65292;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#39044;&#27979;&#23450;&#29702;&#24207;&#21015;&#26469;&#35299;&#20915;&#20960;&#20309;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24212;&#29992;&#24403;&#20195;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#26469;&#35299;&#20915;&#20960;&#20309;&#38382;&#39064;&#21644;&#33258;&#21160;&#28436;&#32462;&#35777;&#26126;&#19968;&#30452;&#26159;&#25968;&#23398;&#21644;&#20154;&#24037;&#26234;&#33021;&#20132;&#21449;&#39046;&#22495;&#30340;&#19968;&#22823;&#25361;&#25112;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;FGeo-TP&#65288;&#23450;&#29702;&#39044;&#27979;&#22120;&#65289;&#65292;&#23427;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#39044;&#27979;&#35299;&#20915;&#20960;&#20309;&#38382;&#39064;&#30340;&#23450;&#29702;&#24207;&#21015;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#21508;&#31181;Transformer&#26550;&#26500;&#65292;&#27604;&#22914;BART&#25110;T5&#65292;&#22312;&#23450;&#29702;&#39044;&#27979;&#26041;&#38754;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09047v1 Announce Type: new Abstract: The application of contemporary artificial intelligence techniques to address geometric problems and automated deductive proof has always been a grand challenge to the interdiscipline field of mathematics and artificial Intelligence. This is the fourth article in a series of our works, in our previous work, we established of a geometric formalized system known as FormalGeo. Moreover we annotated approximately 7000 geometric problems, forming the FormalGeo7k dataset. Despite the FGPS (Formal Geometry Problem Solver) can achieve interpretable algebraic equation solving and human-like deductive reasoning, it often experiences timeouts due to the complexity of the search strategy. In this paper, we introduced FGeo-TP (Theorem Predictor), which utilizes the language model to predict theorem sequences for solving geometry problems. We compared the effectiveness of various Transformer architectures, such as BART or T5, in theorem prediction, imp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20197;&#36125;&#21494;&#26031;&#26041;&#27861;&#20026;&#22522;&#30784;&#30340;&#27010;&#29575;&#25512;&#26029;&#29702;&#35770;&#65292;&#29992;&#20110;&#25512;&#29702;&#21644;&#23398;&#20064;&#30340;&#32479;&#19968;&#29702;&#35770;&#12290;&#35813;&#29702;&#35770;&#36890;&#36807;&#23558;&#25968;&#25454;&#36716;&#21270;&#20026;&#31526;&#21495;&#30693;&#35782;&#65292;&#20174;&#32780;&#23454;&#29616;&#25512;&#29702;&#36807;&#31243;&#12290;&#25105;&#20204;&#23545;&#36923;&#36753;&#25512;&#29702;&#20851;&#31995;&#21644;MNIST&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#35752;&#35770;&#21644;&#23454;&#39564;&#35777;&#26126;&#12290;</title><link>https://arxiv.org/abs/2402.09046</link><description>&lt;p&gt;
&#25512;&#29702;&#21644;&#23398;&#20064;&#32479;&#19968;&#29702;&#35770;&#30340;&#25277;&#35937;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Inference of Abstraction for a Unified Account of Reasoning and Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09046
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20197;&#36125;&#21494;&#26031;&#26041;&#27861;&#20026;&#22522;&#30784;&#30340;&#27010;&#29575;&#25512;&#26029;&#29702;&#35770;&#65292;&#29992;&#20110;&#25512;&#29702;&#21644;&#23398;&#20064;&#30340;&#32479;&#19968;&#29702;&#35770;&#12290;&#35813;&#29702;&#35770;&#36890;&#36807;&#23558;&#25968;&#25454;&#36716;&#21270;&#20026;&#31526;&#21495;&#30693;&#35782;&#65292;&#20174;&#32780;&#23454;&#29616;&#25512;&#29702;&#36807;&#31243;&#12290;&#25105;&#20204;&#23545;&#36923;&#36753;&#25512;&#29702;&#20851;&#31995;&#21644;MNIST&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#35752;&#35770;&#21644;&#23454;&#39564;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31070;&#32463;&#31185;&#23398;&#20013;&#21463;&#21040;&#36125;&#21494;&#26031;&#26041;&#27861;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#27010;&#29575;&#25512;&#26029;&#29702;&#35770;&#65292;&#29992;&#20110;&#25512;&#29702;&#21644;&#23398;&#20064;&#30340;&#32479;&#19968;&#29702;&#35770;&#12290;&#25105;&#20204;&#36890;&#36807;&#24418;&#24335;&#36923;&#36753;&#20013;&#30340;&#21487;&#28385;&#36275;&#24615;&#26469;&#24314;&#27169;&#25968;&#25454;&#22914;&#20309;&#23548;&#33268;&#31526;&#21495;&#30693;&#35782;&#12290;&#25512;&#29702;&#26159;&#36890;&#36807;&#25277;&#35937;&#65292;&#21363;&#36873;&#25321;&#24615;&#26080;&#30693;&#65292;&#20174;&#25968;&#25454;&#20013;&#25512;&#23548;&#20986;&#31526;&#21495;&#30693;&#35782;&#30340;&#36807;&#31243;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#36923;&#36753;&#25512;&#29702;&#20851;&#31995;&#30340;&#35777;&#26126;&#22522;&#30784;&#30340;&#29702;&#35770;&#27491;&#30830;&#24615;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#22522;&#20110;&#23454;&#39564;&#35777;&#25454;&#30340;MNIST&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#27491;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09046v1 Announce Type: new Abstract: Inspired by Bayesian approaches to brain function in neuroscience, we give a simple theory of probabilistic inference for a unified account of reasoning and learning. We simply model how data cause symbolic knowledge in terms of its satisfiability in formal logic. The underlying idea is that reasoning is a process of deriving symbolic knowledge from data via abstraction, i.e., selective ignorance. The logical consequence relation is discussed for its proof-based theoretical correctness. The MNIST dataset is discussed for its experiment-based empirical correctness.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24179;&#26041;Sigmoid TanH&#65288;SST&#65289;&#28608;&#27963;&#20989;&#25968;&#65292;&#29992;&#20110;&#22686;&#24378;&#22312;&#25968;&#25454;&#38480;&#21046;&#19979;&#30340;&#39034;&#24207;&#27169;&#22411;&#23398;&#20064;&#33021;&#21147;&#12290;&#36890;&#36807;&#25968;&#23398;&#24179;&#26041;&#25918;&#22823;&#24378;&#28608;&#27963;&#21644;&#24369;&#28608;&#27963;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#25913;&#21892;&#26799;&#24230;&#27969;&#21644;&#20449;&#24687;&#36807;&#28388;&#12290;&#22312;&#22810;&#20010;&#24212;&#29992;&#20013;&#35780;&#20272;&#20102;SST&#39537;&#21160;&#30340;LSTM&#21644;GRU&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.09034</link><description>&lt;p&gt;
&#20351;&#29992;&#24179;&#26041;Sigmoid TanH (SST)&#28608;&#27963;&#22312;&#25968;&#25454;&#38480;&#21046;&#19979;&#25552;&#39640;&#39034;&#24207;&#27169;&#22411;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Enhancing Sequential Model Performance with Squared Sigmoid TanH (SST) Activation Under Data Constraints
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09034
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24179;&#26041;Sigmoid TanH&#65288;SST&#65289;&#28608;&#27963;&#20989;&#25968;&#65292;&#29992;&#20110;&#22686;&#24378;&#22312;&#25968;&#25454;&#38480;&#21046;&#19979;&#30340;&#39034;&#24207;&#27169;&#22411;&#23398;&#20064;&#33021;&#21147;&#12290;&#36890;&#36807;&#25968;&#23398;&#24179;&#26041;&#25918;&#22823;&#24378;&#28608;&#27963;&#21644;&#24369;&#28608;&#27963;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#25913;&#21892;&#26799;&#24230;&#27969;&#21644;&#20449;&#24687;&#36807;&#28388;&#12290;&#22312;&#22810;&#20010;&#24212;&#29992;&#20013;&#35780;&#20272;&#20102;SST&#39537;&#21160;&#30340;LSTM&#21644;GRU&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28608;&#27963;&#20989;&#25968;&#36890;&#36807;&#24341;&#20837;&#38750;&#32447;&#24615;&#26469;&#20351;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#23398;&#20064;&#22797;&#26434;&#30340;&#34920;&#31034;&#12290;&#34429;&#28982;&#21069;&#39304;&#27169;&#22411;&#36890;&#24120;&#20351;&#29992;&#20462;&#27491;&#32447;&#24615;&#21333;&#20803;&#65292;&#20294;&#26159;&#39034;&#24207;&#27169;&#22411;&#22914;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#12289;&#38271;&#30701;&#26102;&#35760;&#24518;&#65288;LSTM&#65289;&#21644;&#38376;&#25511;&#24490;&#29615;&#21333;&#20803;&#65288;GRU&#65289;&#20173;&#28982;&#20381;&#36182;&#20110;Sigmoid&#21644;TanH&#28608;&#27963;&#20989;&#25968;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#20256;&#32479;&#30340;&#28608;&#27963;&#20989;&#25968;&#24120;&#24120;&#22312;&#35757;&#32451;&#22312;&#23567;&#39034;&#24207;&#25968;&#25454;&#38598;&#19978;&#26102;&#38590;&#20197;&#24314;&#27169;&#31232;&#30095;&#27169;&#24335;&#20197;&#26377;&#25928;&#25429;&#33719;&#26102;&#38388;&#20381;&#36182;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#29305;&#21035;&#38024;&#23545;&#22312;&#25968;&#25454;&#38480;&#21046;&#19979;&#22686;&#24378;&#39034;&#24207;&#27169;&#22411;&#23398;&#20064;&#33021;&#21147;&#30340;&#24179;&#26041;Sigmoid TanH&#65288;SST&#65289;&#28608;&#27963;&#12290;SST&#36890;&#36807;&#25968;&#23398;&#24179;&#26041;&#26469;&#25918;&#22823;&#24378;&#28608;&#27963;&#21644;&#24369;&#28608;&#27963;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#38543;&#30528;&#20449;&#21495;&#38543;&#26102;&#38388;&#20256;&#25773;&#65292;&#26377;&#21161;&#20110;&#25913;&#21892;&#26799;&#24230;&#27969;&#21644;&#20449;&#24687;&#36807;&#28388;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#20351;&#29992;SST&#30340;LSTM&#21644;GRU&#27169;&#22411;&#22312;&#19981;&#21516;&#24212;&#29992;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09034v1 Announce Type: cross Abstract: Activation functions enable neural networks to learn complex representations by introducing non-linearities. While feedforward models commonly use rectified linear units, sequential models like recurrent neural networks, long short-term memory (LSTMs) and gated recurrent units (GRUs) still rely on Sigmoid and TanH activation functions. However, these classical activation functions often struggle to model sparse patterns when trained on small sequential datasets to effectively capture temporal dependencies. To address this limitation, we propose squared Sigmoid TanH (SST) activation specifically tailored to enhance the learning capability of sequential models under data constraints. SST applies mathematical squaring to amplify differences between strong and weak activations as signals propagate over time, facilitating improved gradient flow and information filtering. We evaluate SST-powered LSTMs and GRUs for diverse applications, such a
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#32508;&#36848;&#20102;&#23545;&#25512;&#33616;&#31995;&#32479;&#30340;review-&#21512;&#24182;&#30340;&#27169;&#22411;&#26080;&#20851;profile&#27880;&#20837;&#25915;&#20987;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#20351;&#29992;&#25991;&#26412;&#35780;&#35770;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#34394;&#20551;&#29992;&#25143;&#37197;&#32622;&#25991;&#20214;&#21487;&#20197;&#22312;&#40657;&#30418;&#35774;&#32622;&#19979;&#26377;&#25928;&#25915;&#20987;&#19981;&#21516;&#30340;&#25512;&#33616;&#31995;&#32479;&#12290;</title><link>https://arxiv.org/abs/2402.09023</link><description>&lt;p&gt;
&#23545;&#25512;&#33616;&#31995;&#32479;&#30340;review-&#21512;&#24182;&#30340;&#27169;&#22411;&#26080;&#20851;profile&#27880;&#20837;&#25915;&#20987;&#30340;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Review-Incorporated Model-Agnostic Profile Injection Attacks on Recommender Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09023
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#32508;&#36848;&#20102;&#23545;&#25512;&#33616;&#31995;&#32479;&#30340;review-&#21512;&#24182;&#30340;&#27169;&#22411;&#26080;&#20851;profile&#27880;&#20837;&#25915;&#20987;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#20351;&#29992;&#25991;&#26412;&#35780;&#35770;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#34394;&#20551;&#29992;&#25143;&#37197;&#32622;&#25991;&#20214;&#21487;&#20197;&#22312;&#40657;&#30418;&#35774;&#32622;&#19979;&#26377;&#25928;&#25915;&#20987;&#19981;&#21516;&#30340;&#25512;&#33616;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#25512;&#33616;&#31995;&#32479;(RSs)&#23545;&#25968;&#25454;&#27745;&#26579;&#25915;&#20987;&#38750;&#24120;&#23481;&#26131;&#21463;&#21040;&#25915;&#20987;&#12290;&#29702;&#35299;&#25915;&#20987;&#31574;&#30053;&#26377;&#21161;&#20110;&#25552;&#39640;RSs&#30340;&#31283;&#20581;&#24615;&#12290;&#25105;&#20204;&#25171;&#31639;&#24320;&#21457;&#39640;&#25928;&#30340;&#25915;&#20987;&#26041;&#27861;&#65292;&#21033;&#29992;&#26377;&#38480;&#36164;&#28304;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#34394;&#20551;&#29992;&#25143;&#37197;&#32622;&#25991;&#20214;&#65292;&#23454;&#29616;&#20197;&#19979;&#30446;&#26631;&#65306;1&#65289;&#22312;&#40657;&#30418;RSs&#20043;&#38388;&#23454;&#29616;&#21487;&#36716;&#31227;&#24615;&#65307;2&#65289;&#22312;&#26816;&#27979;&#22120;&#20013;&#23454;&#29616;&#24863;&#30693;&#24230;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20123;&#30446;&#26631;&#65292;&#25105;&#20204;&#24341;&#20837;&#20135;&#21697;&#30340;&#25991;&#26412;&#35780;&#35770;&#26469;&#25552;&#39640;&#37197;&#32622;&#25991;&#20214;&#30340;&#29983;&#25104;&#36136;&#37327;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;R-Trojan&#30340;&#26032;&#22411;&#25915;&#20987;&#26694;&#26550;&#65292;&#23558;&#25915;&#20987;&#30446;&#26631;&#26500;&#24314;&#20026;&#19968;&#20010;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#37319;&#29992;&#23450;&#21046;&#30340;&#22522;&#20110;Transformer&#30340;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#26469;&#35299;&#20915;&#23427;&#65292;&#20174;&#32780;&#21487;&#20197;&#20135;&#29983;&#39640;&#36136;&#37327;&#30340;&#25915;&#20987;&#37197;&#32622;&#25991;&#20214;&#12290;&#23545;&#23454;&#38469;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#20840;&#38754;&#23454;&#39564;&#34920;&#26126;&#65292;R-Trojan&#22312;&#40657;&#30418;&#35774;&#32622;&#19979;&#26174;&#33879;&#20248;&#20110;&#21508;&#31181;&#21463;&#25915;&#20987;&#30340;RSs&#30340;&#26368;&#26032;&#25915;&#20987;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09023v1 Announce Type: cross Abstract: Recent studies have shown that recommender systems (RSs) are highly vulnerable to data poisoning attacks. Understanding attack tactics helps improve the robustness of RSs. We intend to develop efficient attack methods that use limited resources to generate high-quality fake user profiles to achieve 1) transferability among black-box RSs 2) and imperceptibility among detectors. In order to achieve these goals, we introduce textual reviews of products to enhance the generation quality of the profiles. Specifically, we propose a novel attack framework named R-Trojan, which formulates the attack objectives as an optimization problem and adopts a tailored transformer-based generative adversarial network (GAN) to solve it so that high-quality attack profiles can be produced. Comprehensive experiments on real-world datasets demonstrate that R-Trojan greatly outperforms state-of-the-art attack methods on various victim RSs under black-box setti
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;AgentEval&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;LLM&#39537;&#21160;&#24212;&#29992;&#30340;&#20219;&#21153;&#25928;&#29992;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#33258;&#21160;&#25552;&#20986;&#19968;&#22871;&#38024;&#23545;&#29305;&#23450;&#24212;&#29992;&#30340;&#35780;&#20272;&#26631;&#20934;&#65292;&#31616;&#21270;&#20102;&#25928;&#29992;&#39564;&#35777;&#36807;&#31243;&#65292;&#24182;&#23545;&#24212;&#29992;&#30340;&#25928;&#29992;&#36827;&#34892;&#20102;&#20840;&#38754;&#37327;&#21270;&#20998;&#26512;&#12290;</title><link>https://arxiv.org/abs/2402.09015</link><description>&lt;p&gt;
&#26397;&#30528;&#26356;&#22909;&#30340;&#20154;&#26426;&#23545;&#40784;&#26041;&#21521;&#65306;&#35780;&#20272;LLM&#39537;&#21160;&#24212;&#29992;&#20013;&#30340;&#20219;&#21153;&#25928;&#29992;
&lt;/p&gt;
&lt;p&gt;
Towards better Human-Agent Alignment: Assessing Task Utility in LLM-Powered Applications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09015
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;AgentEval&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;LLM&#39537;&#21160;&#24212;&#29992;&#30340;&#20219;&#21153;&#25928;&#29992;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#33258;&#21160;&#25552;&#20986;&#19968;&#22871;&#38024;&#23545;&#29305;&#23450;&#24212;&#29992;&#30340;&#35780;&#20272;&#26631;&#20934;&#65292;&#31616;&#21270;&#20102;&#25928;&#29992;&#39564;&#35777;&#36807;&#31243;&#65292;&#24182;&#23545;&#24212;&#29992;&#30340;&#25928;&#29992;&#36827;&#34892;&#20102;&#20840;&#38754;&#37327;&#21270;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#39046;&#22495;&#30340;&#24555;&#36895;&#21457;&#23637;&#23548;&#33268;&#20102;&#19968;&#31995;&#21015;&#24212;&#29992;&#30340;&#20986;&#29616;&#65292;&#36825;&#20123;&#24212;&#29992;&#36890;&#36807;&#21327;&#21161;&#22810;&#20010;&#20195;&#29702;&#20154;&#19982;&#20154;&#31867;&#21512;&#20316;&#65292;&#24110;&#21161;&#20154;&#20204;&#23436;&#25104;&#26085;&#24120;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#20173;&#23384;&#22312;&#19968;&#20010;&#37325;&#22823;&#38382;&#39064;&#65292;&#21363;&#22914;&#20309;&#35780;&#20272;LLM&#39537;&#21160;&#24212;&#29992;&#26159;&#21542;&#30495;&#27491;&#25552;&#21319;&#29992;&#25143;&#20307;&#39564;&#21644;&#20219;&#21153;&#25191;&#34892;&#25928;&#29575;&#12290;&#36825;&#20984;&#26174;&#20102;&#39564;&#35777;LLM&#39537;&#21160;&#24212;&#29992;&#25928;&#29992;&#30340;&#26041;&#27861;&#30340;&#36843;&#20999;&#38656;&#27714;&#65292;&#29305;&#21035;&#26159;&#35201;&#30830;&#20445;&#24212;&#29992;&#31243;&#24207;&#30340;&#21151;&#33021;&#19982;&#26368;&#32456;&#29992;&#25143;&#30340;&#38656;&#27714;&#30456;&#19968;&#33268;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;AgentEval&#65292;&#23427;&#25552;&#20379;&#20102;&#19968;&#20010;&#23454;&#26045;&#25968;&#23398;&#38382;&#39064;&#30340;&#20272;&#27979;&#27169;&#22411;&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#33258;&#21160;&#25552;&#20986;&#19968;&#22871;&#38024;&#23545;&#20219;&#20309;&#32473;&#23450;&#24212;&#29992;&#31243;&#24207;&#29420;&#29305;&#30446;&#26631;&#30340;&#35780;&#20272;&#26631;&#20934;&#65292;&#31616;&#21270;&#25928;&#29992;&#39564;&#35777;&#36807;&#31243;&#12290;&#36825;&#26679;&#21487;&#20197;&#23545;&#24212;&#29992;&#31243;&#24207;&#30340;&#25928;&#29992;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#65292;&#24182;&#37327;&#21270;&#20854;&#19982;&#24314;&#35758;&#26631;&#20934;&#30456;&#27604;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#23545;&#35813;&#26694;&#26550;&#30340;&#31283;&#20581;&#24615;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09015v1 Announce Type: cross Abstract: The rapid development in the field of Large Language Models (LLMs) has led to a surge in applications that facilitate collaboration among multiple agents to assist humans in their daily tasks. However, a significant gap remains in assessing whether LLM-powered applications genuinely enhance user experience and task execution efficiency. This highlights the pressing need for methods to verify utility of LLM-powered applications, particularly by ensuring alignment between the application's functionality and end-user needs. We introduce AgentEval provides an implementation for the math problems}, a novel framework designed to simplify the utility verification process by automatically proposing a set of criteria tailored to the unique purpose of any given application. This allows for a comprehensive assessment, quantifying the utility of an application against the suggested criteria. We present a comprehensive analysis of the robustness of 
&lt;/p&gt;</description></item><item><title>AgentLens&#26159;&#19968;&#20010;&#21487;&#35270;&#21270;&#20998;&#26512;&#31995;&#32479;&#65292;&#29992;&#20110;&#30740;&#31350;LLMAS&#33258;&#20027;&#31995;&#32479;&#20013;&#20195;&#29702;&#34892;&#20026;&#12290;&#23427;&#36890;&#36807;&#20998;&#23618;&#26102;&#38388;&#21487;&#35270;&#21270;&#23637;&#31034;LLMAS&#30340;&#28436;&#21464;&#65292;&#25903;&#25345;&#29992;&#25143;&#20132;&#20114;&#24335;&#22320;&#25506;&#32034;&#20195;&#29702;&#34892;&#20026;&#30340;&#32454;&#33410;&#21644;&#21407;&#22240;&#12290;</title><link>https://arxiv.org/abs/2402.08995</link><description>&lt;p&gt;
AgentLens: LLMAS&#33258;&#20027;&#31995;&#32479;&#20013;&#20195;&#29702;&#34892;&#20026;&#30340;&#21487;&#35270;&#21270;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
AgentLens: Visual Analysis for Agent Behaviors in LLM-based Autonomous Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08995
&lt;/p&gt;
&lt;p&gt;
AgentLens&#26159;&#19968;&#20010;&#21487;&#35270;&#21270;&#20998;&#26512;&#31995;&#32479;&#65292;&#29992;&#20110;&#30740;&#31350;LLMAS&#33258;&#20027;&#31995;&#32479;&#20013;&#20195;&#29702;&#34892;&#20026;&#12290;&#23427;&#36890;&#36807;&#20998;&#23618;&#26102;&#38388;&#21487;&#35270;&#21270;&#23637;&#31034;LLMAS&#30340;&#28436;&#21464;&#65292;&#25903;&#25345;&#29992;&#25143;&#20132;&#20114;&#24335;&#22320;&#25506;&#32034;&#20195;&#29702;&#34892;&#20026;&#30340;&#32454;&#33410;&#21644;&#21407;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#20027;&#31995;&#32479;(LLMAS)&#22240;&#20854;&#27169;&#25311;&#20154;&#31867;&#31038;&#20250;&#22797;&#26434;&#34892;&#20026;&#30340;&#28508;&#21147;&#32780;&#24191;&#21463;&#20851;&#27880;&#12290;&#20854;&#20013;&#19968;&#20010;&#20027;&#35201;&#25361;&#25112;&#26159;&#22914;&#20309;&#23637;&#31034;&#21644;&#20998;&#26512;LLMAS&#30340;&#21160;&#24577;&#20107;&#20214;&#28436;&#21464;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35270;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#25506;&#32034;LLMAS&#20013;&#30340;&#35814;&#32454;&#29366;&#24577;&#21644;&#20195;&#29702;&#34892;&#20026;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#27969;&#31243;&#65292;&#20174;&#21407;&#22987;LLMAS&#25191;&#34892;&#20107;&#20214;&#20013;&#24314;&#31435;&#34892;&#20026;&#32467;&#26500;&#65292;&#21033;&#29992;&#34892;&#20026;&#24635;&#32467;&#31639;&#27861;&#26500;&#24314;&#25972;&#20010;&#32467;&#26500;&#30340;&#26102;&#38388;&#24207;&#21015;&#23618;&#27425;&#25688;&#35201;&#65292;&#24182;&#20351;&#29992;&#22240;&#26524;&#20851;&#31995;&#36861;&#36394;&#26041;&#27861;&#25366;&#25496;&#20195;&#29702;&#34892;&#20026;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#21517;&#20026;AgentLens&#30340;&#21487;&#35270;&#21270;&#20998;&#26512;&#31995;&#32479;&#65292;&#36890;&#36807;&#20351;&#29992;&#20998;&#23618;&#26102;&#38388;&#21487;&#35270;&#21270;&#26469;&#23637;&#31034;LLMAS&#30340;&#28436;&#21464;&#65292;&#24182;&#25903;&#25345;&#29992;&#25143;&#20132;&#20114;&#24335;&#22320;&#30740;&#31350;&#20195;&#29702;&#34892;&#20026;&#30340;&#35814;&#32454;&#20449;&#24687;&#21644;&#21407;&#22240;&#12290;&#21253;&#25324;&#20004;&#20010;&#20351;&#29992;&#22330;&#26223;&#21644;&#19968;&#20010;&#29992;&#25143; &#25688;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08995v1 Announce Type: cross Abstract: Recently, Large Language Model based Autonomous system(LLMAS) has gained great popularity for its potential to simulate complicated behaviors of human societies. One of its main challenges is to present and analyze the dynamic events evolution of LLMAS. In this work, we present a visualization approach to explore detailed statuses and agents' behavior within LLMAS. We propose a general pipeline that establishes a behavior structure from raw LLMAS execution events, leverages a behavior summarization algorithm to construct a hierarchical summary of the entire structure in terms of time sequence, and a cause trace method to mine the causal relationship between agent behaviors. We then develop AgentLens, a visual analysis system that leverages a hierarchical temporal visualization for illustrating the evolution of LLMAS, and supports users to interactively investigate details and causes of agents' behaviors. Two usage scenarios and a user s
&lt;/p&gt;</description></item><item><title>CLIP-MUSED&#26159;&#19968;&#31181;CLIP&#24341;&#23548;&#30340;&#22810;&#20027;&#39064;&#35270;&#35273;&#31070;&#32463;&#20449;&#24687;&#35821;&#20041;&#35299;&#30721;&#26041;&#27861;&#65292;&#21487;&#20197;&#20811;&#26381;&#21333;&#20027;&#39064;&#35299;&#30721;&#27169;&#22411;&#25512;&#24191;&#21040;&#22810;&#20010;&#20027;&#39064;&#30340;&#25361;&#25112;&#65292;&#24182;&#19988;&#36890;&#36807;&#20351;&#29992;Transformer-based&#29305;&#24449;&#25552;&#21462;&#22120;&#26469;&#26377;&#25928;&#24314;&#27169;&#20840;&#23616;&#31070;&#32463;&#34920;&#31034;&#12290;</title><link>https://arxiv.org/abs/2402.08994</link><description>&lt;p&gt;
CLIP-MUSED&#65306;CLIP&#24341;&#23548;&#30340;&#22810;&#20027;&#39064;&#35270;&#35273;&#31070;&#32463;&#20449;&#24687;&#35821;&#20041;&#35299;&#30721;
&lt;/p&gt;
&lt;p&gt;
CLIP-MUSED: CLIP-Guided Multi-Subject Visual Neural Information Semantic Decoding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08994
&lt;/p&gt;
&lt;p&gt;
CLIP-MUSED&#26159;&#19968;&#31181;CLIP&#24341;&#23548;&#30340;&#22810;&#20027;&#39064;&#35270;&#35273;&#31070;&#32463;&#20449;&#24687;&#35821;&#20041;&#35299;&#30721;&#26041;&#27861;&#65292;&#21487;&#20197;&#20811;&#26381;&#21333;&#20027;&#39064;&#35299;&#30721;&#27169;&#22411;&#25512;&#24191;&#21040;&#22810;&#20010;&#20027;&#39064;&#30340;&#25361;&#25112;&#65292;&#24182;&#19988;&#36890;&#36807;&#20351;&#29992;Transformer-based&#29305;&#24449;&#25552;&#21462;&#22120;&#26469;&#26377;&#25928;&#24314;&#27169;&#20840;&#23616;&#31070;&#32463;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#35299;&#30721;&#35270;&#35273;&#31070;&#32463;&#20449;&#24687;&#38754;&#20020;&#30528;&#23558;&#21333;&#20027;&#39064;&#35299;&#30721;&#27169;&#22411;&#25512;&#24191;&#21040;&#22810;&#20010;&#20027;&#39064;&#30340;&#25361;&#25112;&#65292;&#36825;&#26159;&#30001;&#20110;&#20010;&#20307;&#24046;&#24322;&#25152;&#23548;&#33268;&#30340;&#12290;&#27492;&#22806;&#65292;&#26469;&#33258;&#21333;&#20010;&#20027;&#39064;&#30340;&#25968;&#25454;&#26377;&#38480;&#23545;&#27169;&#22411;&#24615;&#33021;&#26377;&#30528;&#38480;&#21046;&#24615;&#24433;&#21709;&#12290;&#23613;&#31649;&#20043;&#21069;&#30340;&#22810;&#20027;&#39064;&#35299;&#30721;&#26041;&#27861;&#21462;&#24471;&#20102;&#30456;&#24403;&#22823;&#30340;&#36827;&#23637;&#65292;&#20294;&#20173;&#28982;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#65292;&#21253;&#25324;&#38590;&#20197;&#25552;&#21462;&#20840;&#23616;&#31070;&#32463;&#21709;&#24212;&#29305;&#24449;&#65292;&#27169;&#22411;&#21442;&#25968;&#38543;&#20027;&#39064;&#25968;&#37327;&#32447;&#24615;&#25193;&#23637;&#20197;&#21450;&#23545;&#20110;&#19981;&#21516;&#20027;&#39064;&#30340;&#31070;&#32463;&#21709;&#24212;&#19982;&#21508;&#31181;&#21050;&#28608;&#20043;&#38388;&#30340;&#20851;&#31995;&#25551;&#36848;&#19981;&#36275;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;CLIP&#24341;&#23548;&#30340;&#22810;&#20027;&#39064;&#35270;&#35273;&#31070;&#32463;&#20449;&#24687;&#35821;&#20041;&#35299;&#30721;&#65288;CLIP-MUSED&#65289;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#19968;&#20010;&#22522;&#20110;Transformer&#30340;&#29305;&#24449;&#25552;&#21462;&#22120;&#65292;&#29992;&#20110;&#26377;&#25928;&#22320;&#24314;&#27169;&#20840;&#23616;&#31070;&#32463;&#34920;&#31034;&#12290;&#23427;&#36824;&#21253;&#21547;&#23398;&#20064;&#24471;&#21040;&#30340;&#20027;&#39064;&#29305;&#23450;&#30340;tok
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08994v1 Announce Type: cross Abstract: The study of decoding visual neural information faces challenges in generalizing single-subject decoding models to multiple subjects, due to individual differences. Moreover, the limited availability of data from a single subject has a constraining impact on model performance. Although prior multi-subject decoding methods have made significant progress, they still suffer from several limitations, including difficulty in extracting global neural response features, linear scaling of model parameters with the number of subjects, and inadequate characterization of the relationship between neural responses of different subjects to various stimuli. To overcome these limitations, we propose a CLIP-guided Multi-sUbject visual neural information SEmantic Decoding (CLIP-MUSED) method. Our method consists of a Transformer-based feature extractor to effectively model global neural representations. It also incorporates learnable subject-specific tok
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SafeDecoding&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#23433;&#20840;&#24863;&#30693;&#35299;&#30721;&#31574;&#30053;&#26469;&#38450;&#24481;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36234;&#29425;&#25915;&#20987;&#12290;&#35813;&#31574;&#30053;&#21487;&#20197;&#29983;&#25104;&#23545;&#29992;&#25143;&#26597;&#35810;&#26377;&#30410;&#19988;&#26080;&#23475;&#30340;&#21709;&#24212;&#65292;&#26377;&#25928;&#32531;&#35299;&#20102;LLMs&#23433;&#20840;&#24615;&#23041;&#32961;&#12290;</title><link>https://arxiv.org/abs/2402.08983</link><description>&lt;p&gt;
SafeDecoding: &#36890;&#36807;&#23433;&#20840;&#24863;&#30693;&#35299;&#30721;&#38450;&#24481;&#36234;&#29425;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
SafeDecoding: Defending against Jailbreak Attacks via Safety-Aware Decoding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08983
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SafeDecoding&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#23433;&#20840;&#24863;&#30693;&#35299;&#30721;&#31574;&#30053;&#26469;&#38450;&#24481;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36234;&#29425;&#25915;&#20987;&#12290;&#35813;&#31574;&#30053;&#21487;&#20197;&#29983;&#25104;&#23545;&#29992;&#25143;&#26597;&#35810;&#26377;&#30410;&#19988;&#26080;&#23475;&#30340;&#21709;&#24212;&#65292;&#26377;&#25928;&#32531;&#35299;&#20102;LLMs&#23433;&#20840;&#24615;&#23041;&#32961;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36234;&#26469;&#36234;&#22810;&#22320;&#24212;&#29992;&#20110;&#20195;&#30721;&#29983;&#25104;&#21644;&#32842;&#22825;&#26426;&#22120;&#20154;&#36741;&#21161;&#31561;&#29616;&#23454;&#24212;&#29992;&#20013;&#65292;&#20154;&#20204;&#20026;&#20102;&#20351;LLM&#30340;&#34892;&#20026;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#20445;&#25345;&#19968;&#33268;&#65292;&#21253;&#25324;&#23433;&#20840;&#24615;&#22312;&#20869;&#20570;&#20986;&#20102;&#22823;&#37327;&#21162;&#21147;&#12290;&#36234;&#29425;&#25915;&#20987;&#26088;&#22312;&#24341;&#21457;LLM&#30340;&#38750;&#39044;&#26399;&#21644;&#19981;&#23433;&#20840;&#34892;&#20026;&#65292;&#20173;&#28982;&#26159;LLM&#23433;&#20840;&#24615;&#30340;&#37325;&#35201;&#23041;&#32961;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#24341;&#20837;SafeDecoding&#26469;&#38450;&#24481;LLM&#30340;&#36234;&#29425;&#25915;&#20987;&#65292;&#36825;&#26159;&#19968;&#31181;&#23433;&#20840;&#24863;&#30693;&#30340;&#35299;&#30721;&#31574;&#30053;&#65292;&#29992;&#20110;&#29983;&#25104;&#23545;&#29992;&#25143;&#26597;&#35810;&#26377;&#30410;&#19988;&#26080;&#23475;&#30340;&#21709;&#24212;&#12290;&#25105;&#20204;&#22312;&#24320;&#21457;SafeDecoding&#26102;&#30340;&#27934;&#23519;&#21147;&#22522;&#20110;&#35266;&#23519;&#21040;&#65292;&#21363;&#20351;&#20195;&#34920;&#26377;&#23475;&#20869;&#23481;&#30340;&#26631;&#35760;&#30340;&#27010;&#29575;&#36229;&#36807;&#20195;&#34920;&#26080;&#23475;&#21709;&#24212;&#30340;&#26631;&#35760;&#30340;&#27010;&#29575;&#65292;&#23433;&#20840;&#20813;&#36131;&#22768;&#26126;&#20173;&#28982;&#20986;&#29616;&#22312;&#25353;&#27010;&#29575;&#38477;&#24207;&#25490;&#24207;&#30340;&#26631;&#35760;&#20013;&#30340;&#21069;&#20960;&#20010;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#36890;&#36807;&#35782;&#21035;&#23433;&#20840;&#20813;&#36131;&#22768;&#26126;&#24182;&#22686;&#24378;&#20854;&#33391;&#24615;&#24433;&#21709;&#21147;&#26469;&#20943;&#36731;&#36234;&#29425;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08983v1 Announce Type: cross Abstract: As large language models (LLMs) become increasingly integrated into real-world applications such as code generation and chatbot assistance, extensive efforts have been made to align LLM behavior with human values, including safety. Jailbreak attacks, aiming to provoke unintended and unsafe behaviors from LLMs, remain a significant/leading LLM safety threat. In this paper, we aim to defend LLMs against jailbreak attacks by introducing SafeDecoding, a safety-aware decoding strategy for LLMs to generate helpful and harmless responses to user queries. Our insight in developing SafeDecoding is based on the observation that, even though probabilities of tokens representing harmful contents outweigh those representing harmless responses, safety disclaimers still appear among the top tokens after sorting tokens by probability in descending order. This allows us to mitigate jailbreak attacks by identifying safety disclaimers and amplifying their
&lt;/p&gt;</description></item><item><title>MEL&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#22810;&#20219;&#21153;&#36827;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#22810;&#20219;&#21153;&#23398;&#20064;&#20013;&#30340;&#20449;&#24687;&#20849;&#20139;&#26469;&#22686;&#24378;&#29305;&#24449;&#36873;&#25321;&#30340;&#33021;&#21147;&#21644;&#25928;&#29575;&#65292;&#35299;&#20915;&#20102;&#39640;&#32500;&#29305;&#24449;&#36873;&#25321;&#20013;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.08982</link><description>&lt;p&gt;
MEL: &#39640;&#32500;&#29305;&#24449;&#36873;&#25321;&#30340;&#39640;&#25928;&#22810;&#20219;&#21153;&#36827;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
MEL: Efficient Multi-Task Evolutionary Learning for High-Dimensional Feature Selection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08982
&lt;/p&gt;
&lt;p&gt;
MEL&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#22810;&#20219;&#21153;&#36827;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#22810;&#20219;&#21153;&#23398;&#20064;&#20013;&#30340;&#20449;&#24687;&#20849;&#20139;&#26469;&#22686;&#24378;&#29305;&#24449;&#36873;&#25321;&#30340;&#33021;&#21147;&#21644;&#25928;&#29575;&#65292;&#35299;&#20915;&#20102;&#39640;&#32500;&#29305;&#24449;&#36873;&#25321;&#20013;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29305;&#24449;&#36873;&#25321;&#26159;&#25968;&#25454;&#25366;&#25496;&#20013;&#30340;&#20851;&#38190;&#27493;&#39588;&#65292;&#36890;&#36807;&#38477;&#20302;&#25968;&#25454;&#32500;&#24230;&#26469;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#25910;&#38598;&#21040;&#30340;&#25968;&#25454;&#30340;&#32500;&#24230;&#36234;&#26469;&#36234;&#39640;&#65292;&#20351;&#24471;&#35745;&#31639;&#22797;&#26434;&#24230;&#19982;&#32500;&#24230;&#25968;&#37327;&#21576;&#25351;&#25968;&#22686;&#38271;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#36827;&#21270;&#35745;&#31639;&#26041;&#27861;&#22240;&#20854;&#31616;&#21333;&#24615;&#21644;&#36866;&#29992;&#24615;&#32780;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#19981;&#21516;&#30340;&#36827;&#21270;&#35745;&#31639;&#26041;&#27861;&#30340;&#22810;&#26679;&#35774;&#35745;&#23548;&#33268;&#20854;&#22788;&#29702;&#19981;&#21516;&#25968;&#25454;&#30340;&#33021;&#21147;&#19981;&#23613;&#30456;&#21516;&#65292;&#32463;&#24120;&#26080;&#27861;&#26377;&#25928;&#22320;&#21033;&#29992;&#21644;&#20849;&#20139;&#20449;&#24687;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PSO-based Multi-task Evolutionary Learning (MEL)&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#22810;&#20219;&#21153;&#23398;&#20064;&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;&#36890;&#36807;&#22312;&#19981;&#21516;&#29305;&#24449;&#36873;&#25321;&#20219;&#21153;&#20043;&#38388;&#20849;&#20139;&#20449;&#24687;&#65292;MEL&#23454;&#29616;&#20102;&#22686;&#24378;&#30340;&#23398;&#20064;&#33021;&#21147;&#21644;&#25928;&#29575;&#12290;&#25105;&#20204;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#35780;&#20272;&#20102;MEL&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08982v1 Announce Type: cross Abstract: Feature selection is a crucial step in data mining to enhance model performance by reducing data dimensionality. However, the increasing dimensionality of collected data exacerbates the challenge known as the "curse of dimensionality", where computation grows exponentially with the number of dimensions. To tackle this issue, evolutionary computational (EC) approaches have gained popularity due to their simplicity and applicability. Unfortunately, the diverse designs of EC methods result in varying abilities to handle different data, often underutilizing and not sharing information effectively. In this paper, we propose a novel approach called PSO-based Multi-task Evolutionary Learning (MEL) that leverages multi-task learning to address these challenges. By incorporating information sharing between different feature selection tasks, MEL achieves enhanced learning ability and efficiency. We evaluate the effectiveness of MEL through extens
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#21517;&#20026;&#24322;&#26500;&#22270;&#35843;&#24230;&#22120;&#65288;HGS&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#28789;&#27963;&#20316;&#19994;&#36710;&#38388;&#35843;&#24230;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#23616;&#37096;&#20851;&#31995;&#30693;&#35782;&#36827;&#34892;&#35843;&#24230;&#65292;&#24182;&#37319;&#29992;&#22270;&#32467;&#26500;&#30340;&#20915;&#31574;&#26694;&#26550;&#65292;&#25552;&#39640;&#20102;&#35268;&#27169;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.08979</link><description>&lt;p&gt;
&#23398;&#20064;&#39537;&#21160;&#30340;&#28789;&#27963;&#20316;&#19994;&#36710;&#38388;&#35843;&#24230;&#29992;&#20110;&#21487;&#25193;&#23637;&#26234;&#33021;&#21046;&#36896;
&lt;/p&gt;
&lt;p&gt;
Learning-enabled Flexible Job-shop Scheduling for Scalable Smart Manufacturing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08979
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#21517;&#20026;&#24322;&#26500;&#22270;&#35843;&#24230;&#22120;&#65288;HGS&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#28789;&#27963;&#20316;&#19994;&#36710;&#38388;&#35843;&#24230;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#23616;&#37096;&#20851;&#31995;&#30693;&#35782;&#36827;&#34892;&#35843;&#24230;&#65292;&#24182;&#37319;&#29992;&#22270;&#32467;&#26500;&#30340;&#20915;&#31574;&#26694;&#26550;&#65292;&#25552;&#39640;&#20102;&#35268;&#27169;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26234;&#33021;&#21046;&#36896;&#31995;&#32479;&#20013;&#65292;&#32771;&#34385;&#21040;&#22522;&#20110;&#33258;&#21160;&#23548;&#24341;&#36710;&#36742;&#65288;AGV&#65289;&#30340;&#29983;&#20135;&#28789;&#27963;&#24615;&#65292;&#20855;&#26377;&#36816;&#36755;&#32422;&#26463;&#30340;&#28789;&#27963;&#20316;&#19994;&#36710;&#38388;&#35843;&#24230;&#65288;FJSPT&#65289;&#26159;&#20248;&#21270;&#26368;&#22823;&#21270;&#29983;&#20135;&#21147;&#30340;&#35299;&#20915;&#26041;&#26696;&#30340;&#20851;&#38190;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#30340;FJSPT&#26041;&#27861;&#22312;&#35268;&#27169;&#27867;&#21270;&#26041;&#38754;&#36935;&#21040;&#20102;&#25361;&#25112;&#12290;&#36825;&#20123;&#26041;&#27861;&#22312;&#19982;&#20854;&#35757;&#32451;&#38598;&#19981;&#21516;&#35268;&#27169;&#30340;&#29615;&#22659;&#20013;&#34920;&#29616;&#19981;&#20339;&#65292;&#23548;&#33268;&#20302;&#36136;&#37327;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#22270;&#30340;DRL&#26041;&#27861;&#65292;&#21629;&#21517;&#20026;&#24322;&#26500;&#22270;&#35843;&#24230;&#22120;&#65288;HGS&#65289;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#22312;&#25805;&#20316;&#12289;&#26426;&#22120;&#21644;&#36710;&#36742;&#33410;&#28857;&#20043;&#38388;&#25552;&#21462;&#30340;&#23616;&#37096;&#20851;&#31995;&#30693;&#35782;&#36827;&#34892;&#35843;&#24230;&#65292;&#37319;&#29992;&#22270;&#32467;&#26500;&#30340;&#20915;&#31574;&#26694;&#26550;&#26469;&#38477;&#20302;&#32534;&#30721;&#22797;&#26434;&#24615;&#24182;&#22686;&#24378;&#35268;&#27169;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#24615;&#33021;&#35780;&#20272;&#65292;&#32467;&#26524;&#26174;&#31034;&#35813;&#26041;&#27861;&#22312;&#35268;&#27169;&#27867;&#21270;&#19978;&#20855;&#26377;&#33391;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08979v1 Announce Type: cross Abstract: In smart manufacturing systems (SMSs), flexible job-shop scheduling with transportation constraints (FJSPT) is essential to optimize solutions for maximizing productivity, considering production flexibility based on automated guided vehicles (AGVs). Recent developments in deep reinforcement learning (DRL)-based methods for FJSPT have encountered a scale generalization challenge. These methods underperform when applied to environment at scales different from their training set, resulting in low-quality solutions. To address this, we introduce a novel graph-based DRL method, named the Heterogeneous Graph Scheduler (HGS). Our method leverages locally extracted relational knowledge among operations, machines, and vehicle nodes for scheduling, with a graph-structured decision-making framework that reduces encoding complexity and enhances scale generalization. Our performance evaluation, conducted with benchmark datasets, reveals that the pro
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#22522;&#20110;Transformer&#30340;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#30740;&#31350;&#19982;&#24212;&#29992;&#65292;&#27010;&#36848;&#20102;Transformer&#21450;&#20854;&#21464;&#31181;&#22312;&#24322;&#24120;&#26816;&#27979;&#20219;&#21153;&#20013;&#30340;&#24037;&#20316;&#21407;&#29702;&#21644;&#24212;&#29992;&#22330;&#26223;&#65292;&#24182;&#25552;&#20986;&#20102;&#35813;&#39046;&#22495;&#30340;&#26410;&#26469;&#30740;&#31350;&#36235;&#21183;&#12290;</title><link>https://arxiv.org/abs/2402.08975</link><description>&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#30340;&#30740;&#31350;&#19982;&#24212;&#29992;: &#25991;&#29486;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Research and application of Transformer based anomaly detection model: A literature review
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08975
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#22522;&#20110;Transformer&#30340;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#30740;&#31350;&#19982;&#24212;&#29992;&#65292;&#27010;&#36848;&#20102;Transformer&#21450;&#20854;&#21464;&#31181;&#22312;&#24322;&#24120;&#26816;&#27979;&#20219;&#21153;&#20013;&#30340;&#24037;&#20316;&#21407;&#29702;&#21644;&#24212;&#29992;&#22330;&#26223;&#65292;&#24182;&#25552;&#20986;&#20102;&#35813;&#39046;&#22495;&#30340;&#26410;&#26469;&#30740;&#31350;&#36235;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#20316;&#20026;&#19968;&#31181;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#20013;&#24212;&#29992;&#26368;&#24191;&#27867;&#30340;&#20808;&#36827;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20043;&#19968;&#65292;&#22312;&#24322;&#24120;&#26816;&#27979;&#39046;&#22495;&#23637;&#31034;&#20102;&#22810;&#26679;&#30340;&#24212;&#29992;&#12290;&#20026;&#20102;&#28608;&#21457;&#23545;&#22522;&#20110;Transformer&#30340;&#24322;&#24120;&#26816;&#27979;&#30340;&#30740;&#31350;&#65292;&#26412;&#32508;&#36848;&#20174;&#26032;&#30340;&#35282;&#24230;&#25506;&#32034;&#20102;&#24322;&#24120;&#26816;&#27979;&#30340;&#27010;&#24565;&#65292;&#25506;&#35752;&#20102;&#24403;&#21069;&#24322;&#24120;&#26816;&#27979;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#24182;&#35814;&#32454;&#20171;&#32461;&#20102;Transformer&#21450;&#20854;&#21464;&#31181;&#22312;&#24322;&#24120;&#26816;&#27979;&#20219;&#21153;&#20013;&#30340;&#24037;&#20316;&#21407;&#29702;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#21246;&#21202;&#20102;&#22522;&#20110;Transformer&#30340;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#30340;&#21508;&#31181;&#24212;&#29992;&#22330;&#26223;&#65292;&#24182;&#35752;&#35770;&#20102;&#25152;&#37319;&#29992;&#30340;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#25351;&#26631;&#12290;&#27492;&#22806;&#65292;&#26412;&#32508;&#36848;&#24378;&#35843;&#20102;&#22522;&#20110;Transformer&#30340;&#24322;&#24120;&#26816;&#27979;&#30740;&#31350;&#20013;&#30340;&#20851;&#38190;&#25361;&#25112;&#65292;&#24182;&#23545;&#35813;&#39046;&#22495;&#30340;&#26410;&#26469;&#30740;&#31350;&#36235;&#21183;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#12290;&#32508;&#36848;&#20013;&#25910;&#24405;&#20102;100&#22810;&#20010;&#19982;&#22522;&#20110;Transformer&#30340;&#24322;&#24120;&#26816;&#27979;&#30456;&#20851;&#30340;&#26680;&#24515;&#21442;&#32771;&#25991;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08975v1 Announce Type: cross Abstract: Transformer, as one of the most advanced neural network models in Natural Language Processing (NLP), exhibits diverse applications in the field of anomaly detection. To inspire research on Transformer-based anomaly detection, this review offers a fresh perspective on the concept of anomaly detection. We explore the current challenges of anomaly detection and provide detailed insights into the operating principles of Transformer and its variants in anomaly detection tasks. Additionally, we delineate various application scenarios for Transformer-based anomaly detection models and discuss the datasets and evaluation metrics employed. Furthermore, this review highlights the key challenges in Transformer-based anomaly detection research and conducts a comprehensive analysis of future research trends in this domain. The review includes an extensive compilation of over 100 core references related to Transformer-based anomaly detection. To the 
&lt;/p&gt;</description></item><item><title>GrounDial&#26159;&#19968;&#31181;&#22522;&#20110;&#20154;&#31867;&#20934;&#21017;&#30340;&#23545;&#35805;&#21709;&#24212;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#21709;&#24212;&#22522;&#20110;&#24120;&#35782;&#31038;&#20132;&#35268;&#21017;&#26469;&#30830;&#20445;&#21709;&#24212;&#30340;&#23433;&#20840;&#24615;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#24494;&#35843;&#12290;</title><link>https://arxiv.org/abs/2402.08968</link><description>&lt;p&gt;
GrounDial&#65306;&#22522;&#20110;&#20154;&#31867;&#20934;&#21017;&#30340;&#23545;&#35805;&#21709;&#24212;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
GrounDial: Human-norm Grounded Safe Dialog Response Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08968
&lt;/p&gt;
&lt;p&gt;
GrounDial&#26159;&#19968;&#31181;&#22522;&#20110;&#20154;&#31867;&#20934;&#21017;&#30340;&#23545;&#35805;&#21709;&#24212;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#21709;&#24212;&#22522;&#20110;&#24120;&#35782;&#31038;&#20132;&#35268;&#21017;&#26469;&#30830;&#20445;&#21709;&#24212;&#30340;&#23433;&#20840;&#24615;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#23545;&#35805;AI&#31995;&#32479;&#24050;&#30693;&#20250;&#29983;&#25104;&#19981;&#23433;&#20840;&#30340;&#21709;&#24212;&#65292;&#21253;&#25324;&#21516;&#24847;&#20882;&#29359;&#24615;&#29992;&#25143;&#36755;&#20837;&#25110;&#21253;&#21547;&#26377;&#23475;&#20869;&#23481;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#33268;&#21147;&#20110;&#20943;&#23569;&#27602;&#24615;&#65292;&#36890;&#36807;&#20351;&#29992;&#25163;&#21160;&#27880;&#37322;&#30340;&#23433;&#20840;&#23545;&#35805;&#21382;&#21490;&#26469;&#23545;LLM&#36827;&#34892;&#24494;&#35843;&#12290;&#28982;&#32780;&#65292;&#20381;&#36182;&#20110;&#39069;&#22806;&#30340;&#35843;&#25972;&#38656;&#35201;&#22823;&#37327;&#25104;&#26412;&#12290;&#20026;&#20102;&#28040;&#38500;&#35813;&#20381;&#36182;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GrounDial&#65292;&#23427;&#36890;&#36807;&#23558;&#21709;&#24212;&#22522;&#20110;&#24120;&#35782;&#31038;&#20132;&#35268;&#21017;&#26469;&#30830;&#20445;&#21709;&#24212;&#30340;&#23433;&#20840;&#24615;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#24494;&#35843;&#12290;GrounDial&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#21644;&#36981;&#24490;&#20154;&#31867;&#20934;&#21017;&#30340;&#35299;&#30721;&#28151;&#21512;&#26041;&#27861;&#20351;&#24471;&#21709;&#24212;&#22312;&#27809;&#26377;&#39069;&#22806;&#25968;&#25454;&#25110;&#35843;&#25972;&#30340;&#24773;&#20917;&#19979;&#22312;&#37327;&#21270;&#21644;&#36136;&#37327;&#19978;&#37117;&#26356;&#23433;&#20840;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08968v1 Announce Type: new Abstract: Current conversational AI systems based on large language models (LLMs) are known to generate unsafe responses, agreeing to offensive user input or including toxic content. Previous research aimed to alleviate the toxicity, by fine-tuning LLM with manually annotated safe dialogue histories. However, the dependency on additional tuning requires substantial costs. To remove the dependency, we propose GrounDial, where response safety is achieved by grounding responses to commonsense social rules without requiring fine-tuning. A hybrid approach of in-context learning and human-norm-guided decoding of GrounDial enables the response to be quantitatively and qualitatively safer even without additional data or tuning.
&lt;/p&gt;</description></item><item><title>DUEL&#26159;&#19968;&#20010;&#29992;&#20110;&#35299;&#20915;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#20027;&#21160;&#20869;&#23384;&#21435;&#37325;&#31574;&#30053;&#26469;&#22686;&#24378;&#25968;&#25454;&#30340;&#22810;&#26679;&#24615;&#65292;&#26377;&#25928;&#32531;&#35299;&#20102;&#36807;&#25311;&#21512;&#30340;&#38382;&#39064;&#12290;&#33521;&#25991;&#24635;&#32467;&#30446;&#21069;&#26242;&#26080;&#27861;&#32473;&#20986;&#12290;</title><link>https://arxiv.org/abs/2402.08963</link><description>&lt;p&gt;
DUEL: &#29992;&#20110;&#33258;&#30417;&#30563;&#31867;&#21035;&#19981;&#24179;&#34913;&#23398;&#20064;&#30340;&#20027;&#21160;&#20869;&#23384;&#21435;&#37325;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
DUEL: Duplicate Elimination on Active Memory for Self-Supervised Class-Imbalanced Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08963
&lt;/p&gt;
&lt;p&gt;
DUEL&#26159;&#19968;&#20010;&#29992;&#20110;&#35299;&#20915;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#20027;&#21160;&#20869;&#23384;&#21435;&#37325;&#31574;&#30053;&#26469;&#22686;&#24378;&#25968;&#25454;&#30340;&#22810;&#26679;&#24615;&#65292;&#26377;&#25928;&#32531;&#35299;&#20102;&#36807;&#25311;&#21512;&#30340;&#38382;&#39064;&#12290;&#33521;&#25991;&#24635;&#32467;&#30446;&#21069;&#26242;&#26080;&#27861;&#32473;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#36890;&#24120;&#20351;&#29992;&#32463;&#36807;&#31934;&#24515;&#31579;&#36873;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#24320;&#21457;&#65292;&#36825;&#24448;&#24448;&#38656;&#35201;&#22823;&#37327;&#25104;&#26412;&#21644;&#36164;&#28304;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#30452;&#25509;&#20351;&#29992;&#21407;&#22987;&#25968;&#25454;&#24448;&#24448;&#20250;&#23548;&#33268;&#36807;&#25311;&#21512;&#65292;&#20559;&#21521;&#20110;&#39057;&#32321;&#20986;&#29616;&#30340;&#31867;&#21035;&#20449;&#24687;&#12290;&#20026;&#20102;&#39640;&#25928;&#35299;&#20915;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#36827;&#34892;&#20027;&#21160;&#25968;&#25454;&#36807;&#28388;&#30340;&#26032;&#26694;&#26550;&#65292;&#21629;&#21517;&#20026;Duplicate Elimination (DUEL)&#12290;&#35813;&#26694;&#26550;&#32467;&#21512;&#20102;&#21463;&#20154;&#31867;&#24037;&#20316;&#20869;&#23384;&#21551;&#21457;&#30340;&#20027;&#21160;&#20869;&#23384;&#65292;&#24182;&#24341;&#20837;&#20102;&#21306;&#20998;&#24615;&#20449;&#24687;&#65292;&#29992;&#20110;&#34913;&#37327;&#20869;&#23384;&#20013;&#25968;&#25454;&#30340;&#22810;&#26679;&#24615;&#65292;&#20197;&#20248;&#21270;&#29305;&#24449;&#25552;&#21462;&#22120;&#21644;&#20869;&#23384;&#12290;DUEL&#31574;&#30053;&#36890;&#36807;&#26367;&#25442;&#26368;&#24120;&#37325;&#22797;&#30340;&#25968;&#25454;&#26679;&#26412;&#26469;&#22686;&#24378;&#20869;&#23384;&#20013;&#30340;&#21306;&#20998;&#24615;&#20449;&#24687;&#65292;&#20174;&#32780;&#32531;&#35299;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;&#25105;&#20204;&#22312;&#31867;&#21035;&#19981;&#24179;&#34913;&#29615;&#22659;&#20013;&#39564;&#35777;&#20102;DUEL&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08963v1 Announce Type: cross Abstract: Recent machine learning algorithms have been developed using well-curated datasets, which often require substantial cost and resources. On the other hand, the direct use of raw data often leads to overfitting towards frequently occurring class information. To address class imbalances cost-efficiently, we propose an active data filtering process during self-supervised pre-training in our novel framework, Duplicate Elimination (DUEL). This framework integrates an active memory inspired by human working memory and introduces distinctiveness information, which measures the diversity of the data in the memory, to optimize both the feature extractor and the memory. The DUEL policy, which replaces the most duplicated data with new samples, aims to enhance the distinctiveness information in the memory and thereby mitigate class imbalances. We validate the effectiveness of the DUEL framework in class-imbalanced environments, demonstrating its ro
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HyCubE&#30340;&#27169;&#22411;,&#23427;&#36890;&#36807;&#20351;&#29992;&#26032;&#39062;&#30340;3D&#29615;&#24418;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;&#20132;&#26367;&#25513;&#30721;&#22534;&#21472;&#31574;&#30053;&#26469;&#23454;&#29616;&#39640;&#25928;&#30340;n&#20803;&#30693;&#35782;&#36229;&#22270;&#23884;&#20837;&#65292;&#24182;&#36890;&#36807;&#33258;&#36866;&#24212;&#35843;&#25972;&#21367;&#31215;&#26680;&#22823;&#23567;&#21644;&#22343;&#21248;&#23884;&#20837;&#23454;&#20307;&#20301;&#32622;&#20449;&#24687;&#26469;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#21644;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.08961</link><description>&lt;p&gt;
HyCubE: &#39640;&#25928;&#30340;&#30693;&#35782;&#36229;&#22270;3D&#29615;&#24418;&#21367;&#31215;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
HyCubE: Efficient Knowledge Hypergraph 3D Circular Convolutional Embedding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08961
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HyCubE&#30340;&#27169;&#22411;,&#23427;&#36890;&#36807;&#20351;&#29992;&#26032;&#39062;&#30340;3D&#29615;&#24418;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;&#20132;&#26367;&#25513;&#30721;&#22534;&#21472;&#31574;&#30053;&#26469;&#23454;&#29616;&#39640;&#25928;&#30340;n&#20803;&#30693;&#35782;&#36229;&#22270;&#23884;&#20837;&#65292;&#24182;&#36890;&#36807;&#33258;&#36866;&#24212;&#35843;&#25972;&#21367;&#31215;&#26680;&#22823;&#23567;&#21644;&#22343;&#21248;&#23884;&#20837;&#23454;&#20307;&#20301;&#32622;&#20449;&#24687;&#26469;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#30693;&#35782;&#36229;&#22270;&#23884;&#20837;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#65292;&#20294;&#23427;&#20204;&#30340;&#27169;&#22411;&#32467;&#26500;&#21464;&#24471;&#36234;&#26469;&#36234;&#22797;&#26434;&#21644;&#20887;&#20313;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#22266;&#26377;&#30340;&#22797;&#26434;&#35821;&#20041;&#30693;&#35782;&#65292;&#30693;&#35782;&#36229;&#22270;&#23884;&#20837;&#27169;&#22411;&#30340;&#35745;&#31639;&#36890;&#24120;&#38750;&#24120;&#26114;&#36149;&#65292;&#23548;&#33268;&#25928;&#29575;&#20302;&#19979;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#29305;&#24449;&#20132;&#20114;&#21644;&#25552;&#21462;&#30340;3D&#29615;&#24418;&#21367;&#31215;&#23884;&#20837;&#27169;&#22411;HyCubE&#65292;&#23427;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;3D&#29615;&#24418;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#24341;&#20837;&#20102;&#20132;&#26367;&#25513;&#30721;&#22534;&#21472;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;n&#20803;&#30693;&#35782;&#36229;&#22270;&#23884;&#20837;&#12290;&#36890;&#36807;&#33258;&#36866;&#24212;&#35843;&#25972;3D&#29615;&#24418;&#21367;&#31215;&#26680;&#30340;&#22823;&#23567;&#65292;&#24182;&#22343;&#21248;&#23884;&#20837;&#23454;&#20307;&#20301;&#32622;&#20449;&#24687;&#65292;HyCubE&#22312;&#26356;&#23569;&#30340;&#21442;&#25968;&#19979;&#25552;&#39640;&#20102;&#27169;&#22411;&#24615;&#33021;&#65292;&#24182;&#22312;&#27169;&#22411;&#24615;&#33021;&#21644;&#25928;&#29575;&#20043;&#38388;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#26435;&#34913;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;&#23454;&#20307;&#25513;&#30721;&#30340;1-N&#22810;&#32447;&#24615;&#35780;&#20998;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08961v1 Announce Type: new Abstract: Existing knowledge hypergraph embedding methods mainly focused on improving model performance, but their model structures are becoming more complex and redundant. Furthermore, due to the inherent complex semantic knowledge, the computation of knowledge hypergraph embedding models is often very expensive, leading to low efficiency. In this paper, we propose a feature interaction and extraction-enhanced 3D circular convolutional embedding model, HyCubE, which designs a novel 3D circular convolutional neural network and introduces the alternate mask stack strategy to achieve efficient n-ary knowledge hypergraph embedding. By adaptively adjusting the 3D circular convolution kernel size and uniformly embedding the entity position information, HyCubE improves the model performance with fewer parameters and reaches a better trade-off between model performance and efficiency. In addition, we use 1-N multilinear scoring based on the entity mask me
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20351;&#29992;&#26080;&#37197;&#23545;&#30340;&#25513;&#30721;-&#25991;&#26412;&#30417;&#30563;&#26041;&#27861;&#26469;&#36827;&#34892;&#24320;&#25918;&#35789;&#27719;&#20998;&#21106;&#30340;&#26694;&#26550;(Uni-OVSeg)&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#29420;&#31435;&#30340;&#22270;&#20687;-&#25513;&#30721;&#21644;&#22270;&#20687;-&#25991;&#26412;&#23545;&#25552;&#21462;&#20108;&#36827;&#21046;&#25513;&#30721;&#24182;&#19982;&#23454;&#20307;&#20851;&#32852;&#12290;</title><link>https://arxiv.org/abs/2402.08960</link><description>&lt;p&gt;
&#20351;&#29992;&#26080;&#37197;&#23545;&#30340;&#25513;&#30721;-&#25991;&#26412;&#30417;&#30563;&#36827;&#34892;&#24320;&#25918;&#35789;&#27719;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Open-Vocabulary Segmentation with Unpaired Mask-Text Supervision
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08960
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20351;&#29992;&#26080;&#37197;&#23545;&#30340;&#25513;&#30721;-&#25991;&#26412;&#30417;&#30563;&#26041;&#27861;&#26469;&#36827;&#34892;&#24320;&#25918;&#35789;&#27719;&#20998;&#21106;&#30340;&#26694;&#26550;(Uni-OVSeg)&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#29420;&#31435;&#30340;&#22270;&#20687;-&#25513;&#30721;&#21644;&#22270;&#20687;-&#25991;&#26412;&#23545;&#25552;&#21462;&#20108;&#36827;&#21046;&#25513;&#30721;&#24182;&#19982;&#23454;&#20307;&#20851;&#32852;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#20195;&#23574;&#31471;&#30340;&#24320;&#25918;&#35789;&#27719;&#20998;&#21106;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#22270;&#20687;-&#25513;&#30721;-&#25991;&#26412;&#19977;&#20803;&#32452;&#65292;&#28982;&#32780;&#36825;&#31181;&#21463;&#38480;&#30340;&#27880;&#37322;&#22312;&#22797;&#26434;&#30340;&#29616;&#23454;&#22330;&#26223;&#20013;&#38656;&#35201;&#22823;&#37327;&#30340;&#20154;&#21147;&#65292;&#24182;&#19988;&#36935;&#21040;&#20102;&#21487;&#25193;&#23637;&#24615;&#30340;&#38556;&#30861;&#12290;&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#20102;&#19968;&#20123;&#26041;&#27861;&#36890;&#36807;&#20165;&#20351;&#29992;&#25991;&#26412;&#30417;&#30563;&#26469;&#20943;&#23569;&#27880;&#37322;&#30340;&#25104;&#26412;&#65292;&#20294;&#30417;&#30563;&#30340;&#19981;&#23436;&#25972;&#24615;&#20005;&#37325;&#38480;&#21046;&#20102;&#22810;&#26679;&#24615;&#21644;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#29420;&#31435;&#30340;&#22270;&#20687;-&#25513;&#30721;&#21644;&#22270;&#20687;-&#25991;&#26412;&#23545;&#35299;&#25918;&#20102;&#25513;&#30721;&#21644;&#25991;&#26412;&#20043;&#38388;&#30340;&#20005;&#26684;&#23545;&#24212;&#20851;&#31995;&#65292;&#36825;&#20123;&#23545;&#21487;&#20197;&#20998;&#21035;&#36731;&#26494;&#25910;&#38598;&#12290;&#20511;&#21161;&#36825;&#31181;&#26080;&#37197;&#23545;&#30340;&#25513;&#30721;-&#25991;&#26412;&#30417;&#30563;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24369;&#30417;&#30563;&#24320;&#25918;&#35789;&#27719;&#20998;&#21106;&#26694;&#26550;(Uni-OVSeg)&#65292;&#23427;&#21033;&#29992;&#33258;&#20449;&#30340;&#25513;&#30721;&#39044;&#27979;&#21644;&#25991;&#26412;&#25551;&#36848;&#20013;&#30340;&#23454;&#20307;&#12290;&#21033;&#29992;&#29420;&#31435;&#30340;&#22270;&#20687;-&#25513;&#30721;&#21644;&#22270;&#20687;-&#25991;&#26412;&#23545;&#65292;&#25105;&#20204;&#39044;&#27979;&#20102;&#19968;&#32452;&#20108;&#36827;&#21046;&#25513;&#30721;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;CLIP&#23884;&#20837;&#23558;&#23427;&#20204;&#19982;&#23454;&#20307;&#20851;&#32852;&#36215;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08960v1 Announce Type: cross Abstract: Contemporary cutting-edge open-vocabulary segmentation approaches commonly rely on image-mask-text triplets, yet this restricted annotation is labour-intensive and encounters scalability hurdles in complex real-world scenarios. Although some methods are proposed to reduce the annotation cost with only text supervision, the incompleteness of supervision severely limits the versatility and performance. In this paper, we liberate the strict correspondence between masks and texts by using independent image-mask and image-text pairs, which can be easily collected respectively. With this unpaired mask-text supervision, we propose a new weakly-supervised open-vocabulary segmentation framework (Uni-OVSeg) that leverages confident pairs of mask predictions and entities in text descriptions. Using the independent image-mask and image-text pairs, we predict a set of binary masks and associate them with entities by resorting to the CLIP embedding s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21518;&#35757;&#32451;&#37327;&#21270;&#31639;&#27861;&#65292;&#21517;&#20026;aespa&#65292;&#23427;&#22312;&#20445;&#25345;&#23436;&#25972;&#30340;&#27880;&#24847;&#21147;&#24471;&#20998;&#30340;&#21516;&#26102;&#65292;&#36890;&#36807;&#36880;&#23618;&#37327;&#21270;&#26469;&#25552;&#39640;&#25928;&#29575;&#65292;&#35299;&#20915;&#20102;&#24403;&#21069;&#21518;&#35757;&#32451;&#37327;&#21270;&#26041;&#26696;&#30340;&#29942;&#39048;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.08958</link><description>&lt;p&gt;
&#36808;&#21521;&#36229;&#22823;&#35268;&#27169;Transformer&#30340;&#19979;&#19968;&#32423;&#21518;&#35757;&#32451;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Towards Next-Level Post-Training Quantization of Hyper-Scale Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08958
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21518;&#35757;&#32451;&#37327;&#21270;&#31639;&#27861;&#65292;&#21517;&#20026;aespa&#65292;&#23427;&#22312;&#20445;&#25345;&#23436;&#25972;&#30340;&#27880;&#24847;&#21147;&#24471;&#20998;&#30340;&#21516;&#26102;&#65292;&#36890;&#36807;&#36880;&#23618;&#37327;&#21270;&#26469;&#25552;&#39640;&#25928;&#29575;&#65292;&#35299;&#20915;&#20102;&#24403;&#21069;&#21518;&#35757;&#32451;&#37327;&#21270;&#26041;&#26696;&#30340;&#29942;&#39048;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#29983;&#25104;AI&#27169;&#22411;&#30340;&#22797;&#26434;&#24615;&#22686;&#21152;&#65292;&#21518;&#35757;&#32451;&#37327;&#21270;&#65288;PTQ&#65289;&#24050;&#25104;&#20026;&#22312;&#31227;&#21160;&#35774;&#22791;&#21644;&#30005;&#35270;&#31561;&#36793;&#32536;&#35774;&#22791;&#19978;&#37096;&#32626;&#36229;&#22823;&#35268;&#27169;&#27169;&#22411;&#30340;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;PTQ&#26041;&#26696;&#32791;&#36153;&#22823;&#37327;&#26102;&#38388;&#21644;&#36164;&#28304;&#65292;&#36825;&#21487;&#33021;&#25104;&#20026;&#23454;&#38469;&#24773;&#20917;&#20013;&#39057;&#32321;&#27169;&#22411;&#26356;&#26032;&#21644;&#22810;&#31181;&#36229;&#21442;&#25968;&#35843;&#25972;&#30340;&#29942;&#39048;&#12290;&#20316;&#20026;&#19968;&#31181;&#25104;&#26412;&#25928;&#30410;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#19968;&#27425;&#24615;PTQ&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#26377;&#20123;&#21463;&#38480;&#65292;&#22240;&#20026;&#23427;&#20204;&#26080;&#27861;&#32771;&#34385;&#21040;Transformer&#20013;&#27880;&#24847;&#21147;&#27169;&#22359;&#20869;&#37096;&#23618;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#32780;&#36825;&#26159;&#19968;&#20010;&#38750;&#24120;&#37325;&#35201;&#30340;&#29305;&#24615;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;PTQ&#31639;&#27861;&#65292;&#23427;&#22312;&#31934;&#24230;&#21644;&#25928;&#29575;&#20043;&#38388;&#21462;&#24471;&#20102;&#24179;&#34913;&#12290;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#30340;&#20851;&#38190;&#24605;&#24819;&#21483;&#20570;aespa&#65292;&#36890;&#36807;&#22312;&#25928;&#29575;&#19978;&#36827;&#34892;&#36880;&#23618;&#37327;&#21270;&#65292;&#21516;&#26102;&#32771;&#34385;&#21040;&#36328;&#23618;&#20381;&#36182;&#20197;&#20445;&#30041;&#27880;&#24847;&#21147;&#24471;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08958v1 Announce Type: cross Abstract: With the increasing complexity of generative AI models, post-training quantization (PTQ) has emerged as a promising solution for deploying hyper-scale models on edge devices such as mobile devices and TVs. Existing PTQ schemes, however, consume considerable time and resources, which could be a bottleneck in real situations where frequent model updates and multiple hyper-parameter tunings are required. As a cost-effective alternative, one-shot PTQ schemes have been proposed. Still, the performance is somewhat limited because they cannot consider the inter-layer dependency within the attention module, which is a very important feature of Transformers. In this paper, we thus propose a novel PTQ algorithm that balances accuracy and efficiency. The key idea of the proposed algorithm called aespa is to perform quantization layer-wise for efficiency while considering cross-layer dependency to preserve the attention score. Through extensive exp
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;MUSTARD&#65292;&#19968;&#31181;&#25484;&#25569;&#23450;&#29702;&#21644;&#35777;&#26126;&#25968;&#25454;&#32479;&#19968;&#21512;&#25104;&#30340;&#25968;&#25454;&#29983;&#25104;&#26694;&#26550;&#65292;&#36890;&#36807;&#19977;&#20010;&#38454;&#27573;&#30340;&#21512;&#25104;&#65292;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#21644;&#22810;&#26679;&#21270;&#30340;&#38382;&#39064;&#21644;&#25512;&#29702;&#27493;&#39588;&#30340;&#29983;&#25104;&#12290;</title><link>https://arxiv.org/abs/2402.08957</link><description>&lt;p&gt;
MUSTARD&#65306;&#25484;&#25569;&#23450;&#29702;&#21644;&#35777;&#26126;&#25968;&#25454;&#30340;&#32479;&#19968;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
MUSTARD: Mastering Uniform Synthesis of Theorem and Proof Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08957
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;MUSTARD&#65292;&#19968;&#31181;&#25484;&#25569;&#23450;&#29702;&#21644;&#35777;&#26126;&#25968;&#25454;&#32479;&#19968;&#21512;&#25104;&#30340;&#25968;&#25454;&#29983;&#25104;&#26694;&#26550;&#65292;&#36890;&#36807;&#19977;&#20010;&#38454;&#27573;&#30340;&#21512;&#25104;&#65292;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#21644;&#22810;&#26679;&#21270;&#30340;&#38382;&#39064;&#21644;&#25512;&#29702;&#27493;&#39588;&#30340;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#21253;&#25324;&#25968;&#23398;&#25512;&#29702;&#21644;&#23450;&#29702;&#35777;&#26126;&#12290;&#30001;&#20110;&#36825;&#20004;&#20010;&#20219;&#21153;&#38656;&#35201;&#20005;&#26684;&#21644;&#24418;&#24335;&#21270;&#30340;&#22810;&#27493;&#25512;&#29702;&#65292;&#23427;&#20204;&#26159;&#25506;&#32034;LLMs&#25512;&#29702;&#33021;&#21147;&#30340;&#21560;&#24341;&#39046;&#22495;&#65292;&#20294;&#20173;&#38754;&#20020;&#37325;&#35201;&#25361;&#25112;&#12290;&#20197;&#21069;&#30340;&#30740;&#31350;&#22914;Chain-of-Thought&#65288;CoT&#65289;&#25581;&#31034;&#20102;&#20013;&#38388;&#27493;&#39588;&#25351;&#23548;&#30340;&#26377;&#25928;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#36880;&#27493;&#27880;&#37322;&#38656;&#35201;&#22823;&#37327;&#30340;&#21171;&#21160;&#21147;&#65292;&#23548;&#33268;&#24403;&#21069;&#22522;&#20934;&#27979;&#35797;&#30340;&#35757;&#32451;&#27493;&#39588;&#19981;&#36275;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;MUSTARD&#65292;&#19968;&#31181;&#25968;&#25454;&#29983;&#25104;&#26694;&#26550;&#65292;&#21487;&#20197;&#20027;&#23548;&#39640;&#36136;&#37327;&#21644;&#22810;&#26679;&#21270;&#30340;&#23450;&#29702;&#21644;&#35777;&#26126;&#25968;&#25454;&#30340;&#32479;&#19968;&#21512;&#25104;&#12290;MUSTARD&#36890;&#36807;&#19977;&#20010;&#38454;&#27573;&#21512;&#25104;&#25968;&#25454;&#65306;&#65288;1&#65289;&#23427;&#38543;&#26426;&#36873;&#25321;&#20960;&#20010;&#25968;&#23398;&#27010;&#24565;&#20316;&#20026;&#38382;&#39064;&#30340;&#31867;&#21035;&#12290;&#65288;2&#65289;&#28982;&#21518;&#65292;&#23427;&#20351;&#29992;&#36873;&#23450;&#30340;&#27010;&#24565;&#25552;&#31034;&#29983;&#25104;&#24615;&#35821;&#35328;&#27169;&#22411;&#65292;&#20197;&#33719;&#24471;&#38382;&#39064;&#21644;&#23427;&#20204;&#30340;&#25512;&#29702;&#27493;&#39588;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08957v1 Announce Type: new Abstract: Recent large language models (LLMs) have witnessed significant advancement in various tasks, including mathematical reasoning and theorem proving. As these two tasks require strict and formal multi-step inference, they are appealing domains for exploring the reasoning ability of LLMs but still face important challenges. Previous studies such as Chain-of-Thought (CoT) have revealed the effectiveness of intermediate steps guidance. However, such step-wise annotation requires heavy labor, leading to insufficient training steps for current benchmarks. To fill this gap, this work introduces MUSTARD, a data generation framework that masters uniform synthesis of theorem and proof data of high quality and diversity. MUSTARD synthesizes data in three stages: (1) It samples a few mathematical concept seeds as the problem category. (2) Then, it prompts a generative language model with the sampled concepts to obtain both the problems and their step-w
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#21019;&#24314;&#19968;&#32452;&#21453;&#20107;&#23454;&#38382;&#39064;&#65292;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#31867;&#27604;&#25512;&#29702;&#33021;&#21147;&#30340;&#24191;&#27867;&#24615;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#23613;&#31649;&#20154;&#31867;&#22312;&#25152;&#26377;&#38382;&#39064;&#19978;&#30340;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;GPT&#27169;&#22411;&#22312;&#21453;&#20107;&#23454;&#38598;&#19978;&#30340;&#34920;&#29616;&#26174;&#33879;&#19979;&#38477;&#12290;</title><link>https://arxiv.org/abs/2402.08955</link><description>&lt;p&gt;
&#20351;&#29992;&#21453;&#20107;&#23454;&#20219;&#21153;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#31867;&#27604;&#25512;&#29702;&#30340;&#24191;&#27867;&#24615;
&lt;/p&gt;
&lt;p&gt;
Using Counterfactual Tasks to Evaluate the Generality of Analogical Reasoning in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08955
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#21019;&#24314;&#19968;&#32452;&#21453;&#20107;&#23454;&#38382;&#39064;&#65292;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#31867;&#27604;&#25512;&#29702;&#33021;&#21147;&#30340;&#24191;&#27867;&#24615;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#23613;&#31649;&#20154;&#31867;&#22312;&#25152;&#26377;&#38382;&#39064;&#19978;&#30340;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;GPT&#27169;&#22411;&#22312;&#21453;&#20107;&#23454;&#38598;&#19978;&#30340;&#34920;&#29616;&#26174;&#33879;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22810;&#20010;&#25512;&#29702;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#21253;&#25324;&#27979;&#35797;&#31867;&#27604;&#25512;&#29702;&#33021;&#21147;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#23427;&#20204;&#26159;&#21542;&#30495;&#27491;&#36827;&#34892;&#20154;&#31867;&#25277;&#35937;&#25512;&#29702;&#65292;&#36824;&#26159;&#20381;&#36182;&#20110;&#19982;&#35757;&#32451;&#25968;&#25454;&#20013;&#25152;&#35265;&#30456;&#20284;&#30340;&#36739;&#23569;&#36890;&#29992;&#36807;&#31243;&#30340;&#20105;&#35770;&#19968;&#30452;&#23384;&#22312;&#12290;&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#20808;&#21069;&#22768;&#31216;LLMs&#20855;&#26377;&#31867;&#27604;&#33021;&#21147;&#30340;&#24191;&#27867;&#24615;&#65288;Webb, Holyoak, &amp; Lu, 2023&#65289;&#12290;&#25105;&#20204;&#21033;&#29992;&#29992;&#20110;&#35780;&#20272;LLMs&#30340;&#19968;&#32452;&#31867;&#27604;&#38382;&#39064;&#65292;&#21019;&#24314;&#20102;&#19968;&#32452;&#8220;&#21453;&#20107;&#23454;&#8221;&#21464;&#20307;&#65292;&#21363;&#27979;&#35797;&#30456;&#21516;&#30340;&#25277;&#35937;&#25512;&#29702;&#33021;&#21147;&#65292;&#20294;&#24456;&#21487;&#33021;&#19982;&#20219;&#20309;&#39044;&#35757;&#32451;&#36164;&#26009;&#19981;&#21516;&#12290;&#25105;&#20204;&#23545;&#20154;&#31867;&#21644;&#19977;&#20010;GPT&#27169;&#22411;&#22312;&#21407;&#22987;&#38382;&#39064;&#21644;&#21453;&#20107;&#23454;&#38382;&#39064;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#24182;&#21457;&#29616;&#65292;&#34429;&#28982;&#20154;&#31867;&#30340;&#34920;&#29616;&#23545;&#25152;&#26377;&#38382;&#39064;&#20445;&#25345;&#39640;&#27700;&#24179;&#65292;&#20294;GPT&#27169;&#22411;&#22312;&#21453;&#20107;&#23454;&#38598;&#19978;&#30340;&#34920;&#29616;&#24613;&#21095;&#19979;&#38477;&#12290;&#36825;&#39033;&#24037;&#20316;&#25552;&#20379;&#20102;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08955v1 Announce Type: new Abstract: Large language models (LLMs) have performed well on several reasoning benchmarks, including ones that test analogical reasoning abilities. However, it has been debated whether they are actually performing humanlike abstract reasoning or instead employing less general processes that rely on similarity to what has been seen in their training data. Here we investigate the generality of analogy-making abilities previously claimed for LLMs (Webb, Holyoak, &amp; Lu, 2023). We take one set of analogy problems used to evaluate LLMs and create a set of "counterfactual" variants-versions that test the same abstract reasoning abilities but that are likely dissimilar from any pre-training data. We test humans and three GPT models on both the original and counterfactual problems, and show that, while the performance of humans remains high for all the problems, the GPT models' performance declines sharply on the counterfactual set. This work provides evide
&lt;/p&gt;</description></item><item><title>&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#25512;&#29702;&#20219;&#21153;&#26102;&#65292;&#35770;&#25454;&#30340;&#39034;&#24207;&#38750;&#24120;&#37325;&#35201;&#65292;&#23588;&#20854;&#26159;&#22312;&#28436;&#32462;&#25512;&#29702;&#20219;&#21153;&#20013;&#65292;&#25353;&#29031;&#25552;&#31034;&#30340;&#30495;&#23454;&#35777;&#26126;&#39034;&#24207;&#21576;&#29616;&#35770;&#25454;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.08939</link><description>&lt;p&gt;
&#35770;&#25454;&#39034;&#24207;&#22312;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#20013;&#36215;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Premise Order Matters in Reasoning with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08939
&lt;/p&gt;
&lt;p&gt;
&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#25512;&#29702;&#20219;&#21153;&#26102;&#65292;&#35770;&#25454;&#30340;&#39034;&#24207;&#38750;&#24120;&#37325;&#35201;&#65292;&#23588;&#20854;&#26159;&#22312;&#28436;&#32462;&#25512;&#29702;&#20219;&#21153;&#20013;&#65292;&#25353;&#29031;&#25552;&#31034;&#30340;&#30495;&#23454;&#35777;&#26126;&#39034;&#24207;&#21576;&#29616;&#35770;&#25454;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#20010;&#39046;&#22495;&#37117;&#21462;&#24471;&#20102;&#24778;&#20154;&#30340;&#25512;&#29702;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22312;&#25512;&#29702;&#20219;&#21153;&#30340;&#39046;&#22495;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#19968;&#20010;&#33030;&#24369;&#24615;&#65306;&#23613;&#31649;&#36825;&#31181;&#39034;&#24207;&#19981;&#20250;&#25913;&#21464;&#22522;&#26412;&#20219;&#21153;&#65292;&#20294;LLMs&#23545;&#20110;&#35770;&#25454;&#30340;&#39034;&#24207;&#38750;&#24120;&#33030;&#24369;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#24403;&#35770;&#25454;&#39034;&#24207;&#19982;&#20013;&#38388;&#25512;&#29702;&#27493;&#39588;&#25152;&#38656;&#30340;&#19978;&#19979;&#25991;&#23545;&#40784;&#26102;&#65292;LLMs&#21487;&#20197;&#36798;&#21040;&#26368;&#20339;&#24615;&#33021;&#12290;&#20363;&#22914;&#65292;&#22312;&#28436;&#32462;&#25512;&#29702;&#20219;&#21153;&#20013;&#65292;&#23558;&#35770;&#25454;&#25353;&#29031;&#25552;&#31034;&#30340;&#30495;&#23454;&#35777;&#26126;&#39034;&#24207;&#21576;&#29616;&#65288;&#32780;&#19981;&#26159;&#38543;&#26426;&#39034;&#24207;&#65289;&#20250;&#26497;&#22823;&#22320;&#25552;&#39640;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#39318;&#20808;&#30740;&#31350;&#20102;&#19981;&#21516;LLMs&#23545;&#28436;&#32462;&#25512;&#29702;&#20013;&#35770;&#25454;&#39034;&#24207;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#30340;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#35843;&#25972;&#35770;&#25454;&#39034;&#24207;&#21487;&#33021;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#36229;&#36807;30&#65285;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#24067;&#20102;&#22522;&#20110;GSM8K&#30340;&#22522;&#20934;&#27979;&#35797;R-GSM&#26469;&#30740;&#31350;&#39034;&#24207;&#25928;&#24212;&#23545;&#25968;&#23398;&#25512;&#29702;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08939v1 Announce Type: new Abstract: Large language models (LLMs) have accomplished remarkable reasoning performance in various domains. However, in the domain of reasoning tasks, we discover a frailty: LLMs are surprisingly brittle to the ordering of the premises, despite the fact that such ordering does not alter the underlying task. In particular, we observe that LLMs achieve the best performance when the premise order aligns with the context required in intermediate reasoning steps. For example, in deductive reasoning tasks, presenting the premises in the same order as the ground truth proof in the prompt (as opposed to random ordering) drastically increases the model's accuracy. We first examine the effect of premise ordering on deductive reasoning on a variety of LLMs, and our evaluation shows that permuting the premise order can cause a performance drop of over 30%. In addition, we release the benchmark R-GSM, based on GSM8K, to examine the ordering effect for mathema
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#20844;&#24179;&#23545;&#40784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#22810;&#26679;&#30340;&#20154;&#31867;&#20559;&#22909;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#28151;&#21512;&#20559;&#22909;&#20998;&#24067;&#24182;&#20351;&#29992;MaxMin&#23545;&#40784;&#30446;&#26631;&#26469;&#26356;&#22909;&#22320;&#34920;&#31034;&#20154;&#31867;&#20559;&#22909;&#12290;</title><link>https://arxiv.org/abs/2402.08925</link><description>&lt;p&gt;
MaxMin-RLHF:&#38754;&#21521;&#20855;&#26377;&#22810;&#26679;&#30340;&#20154;&#31867;&#20559;&#22909;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20844;&#24179;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
MaxMin-RLHF: Towards Equitable Alignment of Large Language Models with Diverse Human Preferences
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08925
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#20844;&#24179;&#23545;&#40784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#22810;&#26679;&#30340;&#20154;&#31867;&#20559;&#22909;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#28151;&#21512;&#20559;&#22909;&#20998;&#24067;&#24182;&#20351;&#29992;MaxMin&#23545;&#40784;&#30446;&#26631;&#26469;&#26356;&#22909;&#22320;&#34920;&#31034;&#20154;&#31867;&#20559;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;(RLHF)&#36890;&#36807;&#20351;&#29992;&#20174;&#20559;&#22909;&#25968;&#25454;&#20013;&#27966;&#29983;&#30340;&#21333;&#19968;&#22870;&#21169;&#27169;&#22411;&#26469;&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#31867;&#20559;&#22909;&#19968;&#33268;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#24573;&#35270;&#20102;&#20174;&#22810;&#20010;&#29992;&#25143;&#25910;&#38598;&#30340;&#25968;&#25454;&#20013;&#22266;&#26377;&#30340;&#20154;&#31867;&#20559;&#22909;&#30340;&#20016;&#23500;&#22810;&#26679;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#25512;&#23548;&#20986;&#20102;&#20351;&#29992;&#21333;&#19968;&#22870;&#21169;RLHF&#36827;&#34892;&#23545;&#40784;&#30340;&#19981;&#21487;&#33021;&#24615;&#32467;&#26524;&#65292;&#20174;&#32780;&#20984;&#26174;&#20102;&#20854;&#26080;&#27861;&#34920;&#31034;&#22810;&#26679;&#30340;&#20154;&#31867;&#20559;&#22909;&#12290;&#20026;&#20102;&#25552;&#20379;&#19968;&#20010;&#20844;&#24179;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#25105;&#20204;&#36890;&#36807;&#26399;&#26395;&#26368;&#22823;&#21270;&#31639;&#27861;&#23398;&#20064;&#20102;&#19968;&#31181;&#28151;&#21512;&#20559;&#22909;&#20998;&#24067;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21463;&#31038;&#20250;&#36873;&#25321;&#29702;&#35770;&#20013;&#30340;&#24179;&#31561;&#21407;&#21017;&#21551;&#21457;&#30340;MaxMin&#23545;&#40784;&#30446;&#26631;&#26469;&#26356;&#22909;&#22320;&#34920;&#31034;&#22810;&#26679;&#30340;&#20154;&#31867;&#20559;&#22909;&#12290;&#25105;&#20204;&#38416;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#19982;&#20998;&#24067;&#31283;&#20581;&#20248;&#21270;&#21644;&#36890;&#29992;&#25928;&#29992;RL&#30340;&#32852;&#31995;&#65292;&#20174;&#32780;&#31361;&#26174;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#26222;&#36866;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08925v1 Announce Type: cross Abstract: Reinforcement Learning from Human Feedback (RLHF) aligns language models to human preferences by employing a singular reward model derived from preference data. However, such an approach overlooks the rich diversity of human preferences inherent in data collected from multiple users. In this work, we first derive an impossibility result of alignment with single reward RLHF, thereby highlighting its insufficiency in representing diverse human preferences. To provide an equitable solution to the problem, we learn a mixture of preference distributions via an expectation-maximization algorithm and propose a MaxMin alignment objective for policy learning inspired by the Egalitarian principle in social choice theory to better represent diverse human preferences. We elucidate the connection of our proposed approach to distributionally robust optimization and general utility RL, thereby highlighting the generality and robustness of our proposed
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#22312;&#22522;&#20110;&#20250;&#35805;&#30340;&#25512;&#33616;&#20013;&#22686;&#24378;ID&#21644;&#25991;&#26412;&#30340;&#34701;&#21512;&#25928;&#26524;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#36890;&#36807;&#31616;&#21333;&#34701;&#21512;&#30340;&#26041;&#27861;&#24182;&#19981;&#33021;&#22987;&#32456;&#36229;&#36234;&#26368;&#20339;&#30340;&#21333;&#27169;&#24577;&#26041;&#27861;&#12290;&#36827;&#19968;&#27493;&#30340;&#35843;&#26597;&#34920;&#26126;&#65292;&#36825;&#21487;&#33021;&#26159;&#30001;&#20110;&#34701;&#21512;&#26041;&#27861;&#20013;&#23384;&#22312;&#30340;ID&#21644;&#25991;&#26412;&#27169;&#24577;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.08921</link><description>&lt;p&gt;
&#36890;&#36807;&#26367;&#20195;&#35757;&#32451;&#22686;&#24378;ID&#21644;&#25991;&#26412;&#34701;&#21512;&#22312;&#22522;&#20110;&#20250;&#35805;&#30340;&#25512;&#33616;&#20013;
&lt;/p&gt;
&lt;p&gt;
Enhancing ID and Text Fusion via Alternative Training in Session-based Recommendation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08921
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#22312;&#22522;&#20110;&#20250;&#35805;&#30340;&#25512;&#33616;&#20013;&#22686;&#24378;ID&#21644;&#25991;&#26412;&#30340;&#34701;&#21512;&#25928;&#26524;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#36890;&#36807;&#31616;&#21333;&#34701;&#21512;&#30340;&#26041;&#27861;&#24182;&#19981;&#33021;&#22987;&#32456;&#36229;&#36234;&#26368;&#20339;&#30340;&#21333;&#27169;&#24577;&#26041;&#27861;&#12290;&#36827;&#19968;&#27493;&#30340;&#35843;&#26597;&#34920;&#26126;&#65292;&#36825;&#21487;&#33021;&#26159;&#30001;&#20110;&#34701;&#21512;&#26041;&#27861;&#20013;&#23384;&#22312;&#30340;ID&#21644;&#25991;&#26412;&#27169;&#24577;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;&#20250;&#35805;&#30340;&#25512;&#33616;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#20854;&#30446;&#26631;&#26159;&#26681;&#25454;&#29992;&#25143;&#22312;&#20250;&#35805;&#20013;&#30340;&#21382;&#21490;&#34892;&#20026;&#25552;&#20379;&#23450;&#21046;&#30340;&#24314;&#35758;&#12290;&#20026;&#20102;&#25512;&#36827;&#36825;&#20010;&#39046;&#22495;&#65292;&#24050;&#32463;&#24320;&#21457;&#20102;&#35768;&#22810;&#26041;&#27861;&#65292;&#20854;&#20013;&#22522;&#20110;ID&#30340;&#26041;&#27861;&#36890;&#24120;&#34920;&#29616;&#20986;&#26377;&#24076;&#26395;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#38271;&#23614;&#39033;&#30446;&#26041;&#38754;&#32463;&#24120;&#38754;&#20020;&#25361;&#25112;&#65292;&#24182;&#19988;&#24573;&#35270;&#20102;&#20854;&#20182;&#20016;&#23500;&#30340;&#20449;&#24687;&#24418;&#24335;&#65292;&#29305;&#21035;&#26159;&#26377;&#20215;&#20540;&#30340;&#25991;&#26412;&#35821;&#20041;&#20449;&#24687;&#12290;&#20026;&#20102;&#25972;&#21512;&#25991;&#26412;&#20449;&#24687;&#65292;&#24341;&#20837;&#20102;&#21508;&#31181;&#26041;&#27861;&#65292;&#20027;&#35201;&#26159;&#36981;&#24490;&#19968;&#20010;&#31616;&#21333;&#30340;&#34701;&#21512;&#26694;&#26550;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#34701;&#21512;&#36825;&#20004;&#31181;&#27169;&#24577;&#24182;&#19981;&#22987;&#32456;&#20248;&#20110;&#36981;&#24490;&#31616;&#21333;&#34701;&#21512;&#26694;&#26550;&#30340;&#26368;&#20339;&#21333;&#27169;&#24577;&#12290;&#36827;&#19968;&#27493;&#30340;&#35843;&#26597;&#25581;&#31034;&#20102;&#31616;&#21333;&#34701;&#21512;&#20013;&#28508;&#22312;&#30340;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#20854;&#20013;ID&#21344;&#20027;&#23548;&#22320;&#20301;&#65292;&#32780;&#25991;&#26412;&#27169;&#24577;&#21017;&#26410;&#20805;&#20998;&#35757;&#32451;&#12290;&#36825;&#34920;&#26126;&#24847;&#22806;&#35266;&#23519;&#21487;&#33021;&#28304;&#20110;&#31616;&#21333;&#34701;&#21512;&#30340;&#28508;&#22312;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08921v1 Announce Type: cross Abstract: Session-based recommendation has gained increasing attention in recent years, with its aim to offer tailored suggestions based on users' historical behaviors within sessions.   To advance this field, a variety of methods have been developed, with ID-based approaches typically demonstrating promising performance. However, these methods often face challenges with long-tail items and overlook other rich forms of information, notably valuable textual semantic information. To integrate text information, various methods have been introduced, mostly following a naive fusion framework. Surprisingly, we observe that fusing these two modalities does not consistently outperform the best single modality by following the naive fusion framework. Further investigation reveals an potential imbalance issue in naive fusion, where the ID dominates and text modality is undertrained. This suggests that the unexpected observation may stem from naive fusion's
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26694;&#26550;SimMLP&#65292;&#36890;&#36807;&#22312;&#22270;&#19978;&#26080;&#30417;&#30563;&#23398;&#20064;MLPs&#65292;&#25552;&#39640;&#20102;&#22312;&#24310;&#36831;&#25935;&#24863;&#30340;&#24212;&#29992;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.08918</link><description>&lt;p&gt;
&#36890;&#36807;&#26080;&#30417;&#30563;&#22312;&#22270;&#19978;&#23398;&#20064;&#22810;&#23618;&#24863;&#30693;&#26426;&#65288;MLP&#65289;&#21152;&#36895;&#22270;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Graph Inference Acceleration by Learning MLPs on Graphs without Supervision
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08918
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26694;&#26550;SimMLP&#65292;&#36890;&#36807;&#22312;&#22270;&#19978;&#26080;&#30417;&#30563;&#23398;&#20064;MLPs&#65292;&#25552;&#39640;&#20102;&#22312;&#24310;&#36831;&#25935;&#24863;&#30340;&#24212;&#29992;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#24050;&#32463;&#22312;&#21508;&#31181;&#22270;&#23398;&#20064;&#20219;&#21153;&#20013;&#23637;&#31034;&#20986;&#20102;&#26377;&#25928;&#24615;&#65292;&#20294;&#26159;&#23427;&#20204;&#23545;&#28040;&#24687;&#20256;&#36882;&#30340;&#20381;&#36182;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#24310;&#36831;&#25935;&#24863;&#30340;&#24212;&#29992;&#20013;&#30340;&#37096;&#32626;&#65292;&#27604;&#22914;&#37329;&#34701;&#27450;&#35784;&#26816;&#27979;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#25506;&#32034;&#20102;&#20174;GNNs&#20013;&#25552;&#21462;&#30693;&#35782;&#21040;&#22810;&#23618;&#24863;&#30693;&#26426;&#65288;MLPs&#65289;&#26469;&#21152;&#36895;&#25512;&#29702;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#20219;&#21153;&#29305;&#23450;&#30340;&#26377;&#30417;&#30563;&#33976;&#39311;&#38480;&#21046;&#20102;&#23545;&#26410;&#35265;&#33410;&#28857;&#30340;&#27867;&#21270;&#65292;&#32780;&#22312;&#24310;&#36831;&#25935;&#24863;&#30340;&#24212;&#29992;&#20013;&#36825;&#31181;&#24773;&#20917;&#24456;&#24120;&#35265;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26694;&#26550;SimMLP&#65292;&#29992;&#20110;&#22312;&#22270;&#19978;&#26080;&#30417;&#30563;&#23398;&#20064;MLPs&#65292;&#20197;&#22686;&#24378;&#27867;&#21270;&#33021;&#21147;&#12290;SimMLP&#21033;&#29992;&#33258;&#30417;&#30563;&#23545;&#40784;GNNs&#21644;MLPs&#20043;&#38388;&#30340;&#33410;&#28857;&#29305;&#24449;&#21644;&#22270;&#32467;&#26500;&#20043;&#38388;&#30340;&#31934;&#32454;&#21644;&#27867;&#21270;&#30340;&#30456;&#20851;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#31574;&#30053;&#26469;&#20943;&#36731;&#24179;&#20961;&#35299;&#30340;&#39118;&#38505;&#12290;&#20174;&#29702;&#35770;&#19978;&#35762;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08918v1 Announce Type: cross Abstract: Graph Neural Networks (GNNs) have demonstrated effectiveness in various graph learning tasks, yet their reliance on message-passing constraints their deployment in latency-sensitive applications such as financial fraud detection. Recent works have explored distilling knowledge from GNNs to Multi-Layer Perceptrons (MLPs) to accelerate inference. However, this task-specific supervised distillation limits generalization to unseen nodes, which are prevalent in latency-sensitive applications. To this end, we present \textbf{\textsc{SimMLP}}, a \textbf{\textsc{Sim}}ple yet effective framework for learning \textbf{\textsc{MLP}}s on graphs without supervision, to enhance generalization. \textsc{SimMLP} employs self-supervised alignment between GNNs and MLPs to capture the fine-grained and generalizable correlation between node features and graph structures, and proposes two strategies to alleviate the risk of trivial solutions. Theoretically, w
&lt;/p&gt;</description></item><item><title>&#22270;&#36801;&#31227;&#23398;&#20064;&#20013;&#30340;&#36127;&#36801;&#31227;&#29616;&#35937;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#30740;&#31350;&#65292;&#26412;&#25991;&#21457;&#29616;&#22312;&#22270;&#32467;&#26500;&#25968;&#25454;&#20013;&#36127;&#36801;&#31227;&#26222;&#36941;&#23384;&#22312;&#65292;&#21363;&#20351;&#28304;&#22270;&#21644;&#30446;&#26631;&#22270;&#22312;&#35821;&#20041;&#19978;&#30456;&#20284;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35266;&#28857;&#65292;&#23545;&#20110;&#35821;&#20041;&#30456;&#20284;&#30340;&#22270;&#65292;&#32467;&#26500;&#24046;&#24322;&#23545;&#23376;&#22270;&#23884;&#20837;&#30340;&#24433;&#21709;&#36739;&#23567;&#12290;</title><link>https://arxiv.org/abs/2402.08907</link><description>&lt;p&gt;
&#35299;&#20915;&#22270;&#19978;&#30340;&#36127;&#36801;&#31227;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Tackling Negative Transfer on Graphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08907
&lt;/p&gt;
&lt;p&gt;
&#22270;&#36801;&#31227;&#23398;&#20064;&#20013;&#30340;&#36127;&#36801;&#31227;&#29616;&#35937;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#30740;&#31350;&#65292;&#26412;&#25991;&#21457;&#29616;&#22312;&#22270;&#32467;&#26500;&#25968;&#25454;&#20013;&#36127;&#36801;&#31227;&#26222;&#36941;&#23384;&#22312;&#65292;&#21363;&#20351;&#28304;&#22270;&#21644;&#30446;&#26631;&#22270;&#22312;&#35821;&#20041;&#19978;&#30456;&#20284;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35266;&#28857;&#65292;&#23545;&#20110;&#35821;&#20041;&#30456;&#20284;&#30340;&#22270;&#65292;&#32467;&#26500;&#24046;&#24322;&#23545;&#23376;&#22270;&#23884;&#20837;&#30340;&#24433;&#21709;&#36739;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36801;&#31227;&#23398;&#20064;&#26088;&#22312;&#36890;&#36807;&#21033;&#29992;&#20174;&#20854;&#20182;&#30456;&#20851;&#20219;&#21153;&#20013;&#23398;&#21040;&#30340;&#30693;&#35782;&#26469;&#25552;&#39640;&#30446;&#26631;&#20219;&#21153;&#19978;&#30340;&#23398;&#20064;&#25928;&#26524;&#12290;&#28982;&#32780;&#65292;&#24403;&#28304;&#20219;&#21153;&#21644;&#30446;&#26631;&#20219;&#21153;&#20043;&#38388;&#20851;&#31995;&#19981;&#23494;&#20999;&#26102;&#65292;&#23398;&#20064;&#24615;&#33021;&#21487;&#33021;&#20250;&#21463;&#21040;&#19981;&#21033;&#24433;&#21709;&#65292;&#36825;&#31181;&#29616;&#35937;&#34987;&#31216;&#20026;&#36127;&#36801;&#31227;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22270;&#36801;&#31227;&#23398;&#20064;&#20013;&#30340;&#36127;&#36801;&#31227;&#38382;&#39064;&#65292;&#36825;&#26159;&#19968;&#20010;&#37325;&#35201;&#20294;&#23578;&#26410;&#28145;&#20837;&#30740;&#31350;&#30340;&#39046;&#22495;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#22270;&#32467;&#26500;&#25968;&#25454;&#20013;&#65292;&#19982;&#22270;&#20687;&#25110;&#25991;&#26412;&#19981;&#21516;&#65292;&#36127;&#36801;&#31227;&#32463;&#24120;&#21457;&#29983;&#65292;&#21363;&#20351;&#28304;&#22270;&#21644;&#30446;&#26631;&#22270;&#22312;&#35821;&#20041;&#19978;&#26377;&#30456;&#20284;&#20043;&#22788;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#21457;&#29616;&#32467;&#26500;&#24046;&#24322;&#20250;&#22823;&#22823;&#22686;&#24378;&#22270;&#20013;&#33410;&#28857;&#23884;&#20837;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24102;&#26469;&#20102;&#19968;&#20010;&#26032;&#30340;&#35266;&#28857;&#65306;&#23545;&#20110;&#35821;&#20041;&#30456;&#20284;&#30340;&#22270;&#65292;&#23613;&#31649;&#32467;&#26500;&#24046;&#24322;&#20250;&#23548;&#33268;&#33410;&#28857;&#23884;&#20837;&#30340;&#20998;&#24067;&#24046;&#24322;&#65292;&#20294;&#23427;&#20204;&#23545;&#23376;&#22270;&#23884;&#20837;&#30340;&#24433;&#21709;&#21487;&#33021;&#36739;&#23567;&#12290;&#22522;&#20110;&#36825;&#20010;&#35266;&#28857;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;tw
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08907v1 Announce Type: cross Abstract: Transfer learning aims to boost the learning on the target task leveraging knowledge learned from other relevant tasks. However, when the source and target are not closely related, the learning performance may be adversely affected, a phenomenon known as negative transfer. In this paper, we investigate the negative transfer in graph transfer learning, which is important yet underexplored. We reveal that, unlike image or text, negative transfer commonly occurs in graph-structured data, even when source and target graphs share semantic similarities. Specifically, we identify that structural differences significantly amplify the dissimilarities in the node embeddings across graphs. To mitigate this, we bring a new insight: for semantically similar graphs, although structural differences lead to significant distribution shift in node embeddings, their impact on subgraph embeddings could be marginal. Building on this insight, we introduce tw
&lt;/p&gt;</description></item><item><title>DUDF&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#26080;&#31526;&#21495;&#36317;&#31163;&#22330;&#30340;&#21452;&#26354;&#32553;&#25918;&#26041;&#27861;&#65292;&#36890;&#36807;&#19982;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#32593;&#32476;&#38598;&#25104;&#65292;&#35299;&#20915;&#20102;&#24320;&#25918;&#34920;&#38754;&#34920;&#31034;&#30340;&#25361;&#25112;&#65292;&#24182;&#22312;&#37325;&#24314;&#36136;&#37327;&#21644;&#35757;&#32451;&#24615;&#33021;&#26041;&#38754;&#21462;&#24471;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.08876</link><description>&lt;p&gt;
DUDF: &#20855;&#26377;&#21452;&#26354;&#32553;&#25918;&#30340;&#21487;&#24494;&#26080;&#31526;&#21495;&#36317;&#31163;&#22330;
&lt;/p&gt;
&lt;p&gt;
DUDF: Differentiable Unsigned Distance Fields with Hyperbolic Scaling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08876
&lt;/p&gt;
&lt;p&gt;
DUDF&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#26080;&#31526;&#21495;&#36317;&#31163;&#22330;&#30340;&#21452;&#26354;&#32553;&#25918;&#26041;&#27861;&#65292;&#36890;&#36807;&#19982;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#32593;&#32476;&#38598;&#25104;&#65292;&#35299;&#20915;&#20102;&#24320;&#25918;&#34920;&#38754;&#34920;&#31034;&#30340;&#25361;&#25112;&#65292;&#24182;&#22312;&#37325;&#24314;&#36136;&#37327;&#21644;&#35757;&#32451;&#24615;&#33021;&#26041;&#38754;&#21462;&#24471;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#65292;&#20154;&#20204;&#23545;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#20197;&#36924;&#36817;&#26080;&#31526;&#21495;&#36317;&#31163;&#22330;&#65288;UDF&#65289;&#29992;&#20110;&#19977;&#32500;&#37325;&#24314;&#20013;&#34920;&#31034;&#24320;&#25918;&#34920;&#38754;&#30340;&#26041;&#27861;&#36234;&#21457;&#24863;&#20852;&#36259;&#12290;&#28982;&#32780;&#65292;UDF&#22312;&#38646;&#32423;&#38598;&#22788;&#26159;&#38750;&#21487;&#24494;&#30340;&#65292;&#36825;&#20250;&#23548;&#33268;&#36317;&#31163;&#21644;&#26799;&#24230;&#30340;&#26174;&#33879;&#35823;&#24046;&#65292;&#36890;&#24120;&#23548;&#33268;&#34920;&#38754;&#30340;&#30862;&#29255;&#21270;&#21644;&#19981;&#36830;&#32493;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23398;&#20064;&#26080;&#31526;&#21495;&#36317;&#31163;&#22330;&#30340;&#21452;&#26354;&#32553;&#25918;&#65292;&#36825;&#23450;&#20041;&#20102;&#19968;&#20010;&#20855;&#26377;&#19981;&#21516;&#36793;&#30028;&#26465;&#20214;&#30340;&#26032;&#35843;&#21644;&#38382;&#39064;&#12290;&#36825;&#20351;&#25105;&#20204;&#30340;&#27169;&#22411;&#33021;&#22815;&#26080;&#32541;&#22320;&#38598;&#25104;&#21040;&#26368;&#20808;&#36827;&#30340;&#36830;&#32493;&#21487;&#24494;&#30340;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#32593;&#32476;&#20013;&#65292;&#22312;&#25991;&#29486;&#20013;&#24191;&#27867;&#24212;&#29992;&#20110;&#34920;&#31034;&#26377;&#31526;&#21495;&#36317;&#31163;&#22330;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20165;&#33021;&#22815;&#35299;&#20915;&#24320;&#25918;&#34920;&#38754;&#34920;&#31034;&#30340;&#25361;&#25112;&#65292;&#36824;&#22312;&#37325;&#24314;&#36136;&#37327;&#21644;&#35757;&#32451;&#24615;&#33021;&#26041;&#38754;&#34920;&#29616;&#20986;&#26174;&#33879;&#25913;&#36827;&#12290;&#21478;&#22806;&#65292;&#35813;&#26041;&#27861;&#35299;&#38145;&#20102;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08876v1 Announce Type: cross Abstract: In recent years, there has been a growing interest in training Neural Networks to approximate Unsigned Distance Fields (UDFs) for representing open surfaces in the context of 3D reconstruction. However, UDFs are non-differentiable at the zero level set which leads to significant errors in distances and gradients, generally resulting in fragmented and discontinuous surfaces. In this paper, we propose to learn a hyperbolic scaling of the unsigned distance field, which defines a new Eikonal problem with distinct boundary conditions. This allows our formulation to integrate seamlessly with state-of-the-art continuously differentiable implicit neural representation networks, largely applied in the literature to represent signed distance fields. Our approach not only addresses the challenge of open surface representation but also demonstrates significant improvement in reconstruction quality and training performance. Moreover, the unlocked fi
&lt;/p&gt;</description></item><item><title>ScamSpot&#26159;&#19968;&#20010;&#22312;Instagram&#35780;&#35770;&#20013;&#25171;&#20987;&#37329;&#34701;&#27450;&#35784;&#30340;&#32508;&#21512;&#31995;&#32479;&#65292;&#21253;&#25324;&#27983;&#35272;&#22120;&#25193;&#23637;&#12289;BERT&#27169;&#22411;&#21644;REST API&#12290;&#36890;&#36807;&#25968;&#25454;&#27880;&#37322;&#30740;&#31350;&#21644;&#29992;&#25143;&#21453;&#39304;&#65292;&#35813;&#31995;&#32479;&#35299;&#20915;&#20102;&#29616;&#26377;&#27169;&#22411;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#19981;&#36275;&#65292;&#24182;&#20844;&#24320;&#25552;&#20379;&#32473;&#24191;&#22823;&#29992;&#25143;&#20351;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.08869</link><description>&lt;p&gt;
ScamSpot&#65306;&#22312;Instagram&#35780;&#35770;&#20013;&#25171;&#20987;&#37329;&#34701;&#27450;&#35784;
&lt;/p&gt;
&lt;p&gt;
ScamSpot: Fighting Financial Fraud in Instagram Comments
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08869
&lt;/p&gt;
&lt;p&gt;
ScamSpot&#26159;&#19968;&#20010;&#22312;Instagram&#35780;&#35770;&#20013;&#25171;&#20987;&#37329;&#34701;&#27450;&#35784;&#30340;&#32508;&#21512;&#31995;&#32479;&#65292;&#21253;&#25324;&#27983;&#35272;&#22120;&#25193;&#23637;&#12289;BERT&#27169;&#22411;&#21644;REST API&#12290;&#36890;&#36807;&#25968;&#25454;&#27880;&#37322;&#30740;&#31350;&#21644;&#29992;&#25143;&#21453;&#39304;&#65292;&#35813;&#31995;&#32479;&#35299;&#20915;&#20102;&#29616;&#26377;&#27169;&#22411;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#19981;&#36275;&#65292;&#24182;&#20844;&#24320;&#25552;&#20379;&#32473;&#24191;&#22823;&#29992;&#25143;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Instagram&#37329;&#34701;&#39029;&#38754;&#35780;&#35770;&#21306;&#23384;&#22312;&#30528;&#22403;&#22334;&#20449;&#24687;&#21644;&#27450;&#35784;&#20449;&#24687;&#30340;&#38271;&#26399;&#38382;&#39064;&#65292;&#27599;&#22825;&#37117;&#26377;&#26032;&#21463;&#23475;&#32773;&#12290;Instagram&#24403;&#21069;&#30340;&#22403;&#22334;&#37038;&#20214;&#36807;&#28388;&#22120;&#25928;&#26524;&#19981;&#20339;&#65292;&#29616;&#26377;&#30340;&#30740;&#31350;&#26041;&#27861;&#20027;&#35201;&#38480;&#20110;&#29702;&#35770;&#27010;&#24565;&#65292;&#32570;&#20047;&#23454;&#38469;&#23454;&#29616;&#21644;&#35780;&#20272;&#32467;&#26524;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ScamSpot&#65292;&#19968;&#20010;&#32508;&#21512;&#30340;&#31995;&#32479;&#65292;&#21253;&#25324;&#19968;&#20010;&#27983;&#35272;&#22120;&#25193;&#23637;&#31243;&#24207;&#65292;&#19968;&#20010;&#32463;&#36807;&#20248;&#21270;&#30340;BERT&#27169;&#22411;&#21644;&#19968;&#20010;REST API&#12290;&#36825;&#20010;&#26041;&#27861;&#30830;&#20445;&#20351;&#29992;Chrome&#27983;&#35272;&#22120;&#30340;Instagram&#29992;&#25143;&#21487;&#20197;&#20844;&#24320;&#35775;&#38382;&#25105;&#20204;&#30340;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#25968;&#25454;&#27880;&#37322;&#30740;&#31350;&#65292;&#25581;&#31034;&#20102;&#38382;&#39064;&#30340;&#21407;&#22240;&#21644;&#21407;&#22240;&#65292;&#24182;&#36890;&#36807;&#29992;&#25143;&#21453;&#39304;&#21644;&#19982;&#29616;&#26377;&#27169;&#22411;&#30340;&#27604;&#36739;&#26469;&#35780;&#20272;&#31995;&#32479;&#12290;ScamSpot&#26159;&#19968;&#20010;&#24320;&#28304;&#39033;&#30446;&#65292;&#21487;&#20197;&#22312;https://scamspot.github.io/&#19978;&#20844;&#24320;&#35775;&#38382;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08869v1 Announce Type: new Abstract: The long-standing problem of spam and fraudulent messages in the comment sections of Instagram pages in the financial sector claims new victims every day. Instagram's current spam filter proves inadequate, and existing research approaches are primarily confined to theoretical concepts. Practical implementations with evaluated results are missing. To solve this problem, we propose ScamSpot, a comprehensive system that includes a browser extension, a fine-tuned BERT model and a REST API. This approach ensures public accessibility of our results for Instagram users using the Chrome browser. Furthermore, we conduct a data annotation study, shedding light on the reasons and causes of the problem and evaluate the system through user feedback and comparison with existing models. ScamSpot is an open-source project and is publicly available at https://scamspot.github.io/.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#21367;&#31215;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#25512;&#33616;&#31995;&#32479;&#65292;&#35813;&#26041;&#27861;&#20805;&#20998;&#21033;&#29992;&#20102;&#29992;&#25143;-&#29289;&#21697;&#22270;&#20013;&#30340;&#39640;&#38454;&#20851;&#31995;&#65292;&#20197;&#25552;&#39640;&#25551;&#36848;&#30340;&#20934;&#30830;&#24615;&#21644;&#19968;&#33268;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.08859</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#21367;&#31215;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Large Language Model with Graph Convolution for Recommendation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08859
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#21367;&#31215;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#25512;&#33616;&#31995;&#32479;&#65292;&#35813;&#26041;&#27861;&#20805;&#20998;&#21033;&#29992;&#20102;&#29992;&#25143;-&#29289;&#21697;&#22270;&#20013;&#30340;&#39640;&#38454;&#20851;&#31995;&#65292;&#20197;&#25552;&#39640;&#25551;&#36848;&#30340;&#20934;&#30830;&#24615;&#21644;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20154;&#20204;&#19968;&#30452;&#33268;&#21147;&#20110;&#21033;&#29992;&#25991;&#26412;&#20449;&#24687;&#26469;&#25913;&#21892;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#29992;&#25143;&#30011;&#20687;&#21644;&#29289;&#21697;&#25551;&#36848;&#12290;&#28982;&#32780;&#65292;&#25991;&#26412;&#20449;&#24687;&#26377;&#26102;&#36136;&#37327;&#36739;&#20302;&#65292;&#38480;&#21046;&#20102;&#20854;&#22312;&#30495;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#30340;&#25928;&#26524;&#12290;&#23558;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#30693;&#35782;&#21644;&#25512;&#29702;&#33021;&#21147;&#29992;&#20110;&#25551;&#36848;&#25913;&#36827;&#25104;&#20026;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#20351;&#29992;&#21407;&#22987;&#25991;&#26412;&#25552;&#31034;LLM&#30340;&#26041;&#27861;&#24573;&#30053;&#20102;&#29992;&#25143;-&#29289;&#21697;&#20132;&#20114;&#30340;&#32467;&#26500;&#21270;&#30693;&#35782;&#65292;&#21487;&#33021;&#23548;&#33268;&#29983;&#25104;&#19981;&#19968;&#33268;&#25551;&#36848;&#30340;&#24187;&#35273;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#30340;&#21367;&#31215;LLM&#26041;&#27861;&#65292;&#20197;&#24341;&#23548;LLM&#25429;&#25417;&#29992;&#25143;-&#29289;&#21697;&#22270;&#20013;&#30340;&#39640;&#38454;&#20851;&#31995;&#12290;&#20026;&#20102;&#20351;&#25991;&#26412;&#20026;&#22522;&#30784;&#30340;LLM&#36866;&#24212;&#32467;&#26500;&#21270;&#22270;&#65292;&#25105;&#20204;&#23558;LLM&#20316;&#20026;&#22270;&#22788;&#29702;&#20013;&#30340;&#32858;&#21512;&#22120;&#65292;&#20351;&#20854;&#36880;&#27493;&#29702;&#35299;&#22522;&#20110;&#22270;&#30340;&#20449;&#24687;&#12290;&#20855;&#20307;&#22320;&#65292;&#38656;&#35201;LLM&#36827;&#34892;&#25551;&#36848;&#25913;&#36827;&#20197;&#20415;&#26356;&#22909;&#22320;&#36827;&#34892;&#25512;&#33616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08859v1 Announce Type: new Abstract: In recent years, efforts have been made to use text information for better user profiling and item characterization in recommendations. However, text information can sometimes be of low quality, hindering its effectiveness for real-world applications. With knowledge and reasoning capabilities capsuled in Large Language Models (LLMs), utilizing LLMs emerges as a promising way for description improvement. However, existing ways of prompting LLMs with raw texts ignore structured knowledge of user-item interactions, which may lead to hallucination problems like inconsistent description generation. To this end, we propose a Graph-aware Convolutional LLM method to elicit LLMs to capture high-order relations in the user-item graph. To adapt text-based LLMs with structured graphs, We use the LLM as an aggregator in graph processing, allowing it to understand graph-based information step by step. Specifically, the LLM is required for description e
&lt;/p&gt;</description></item><item><title>GhostWriter&#26159;&#19968;&#20010;AI&#22686;&#24378;&#30340;&#20889;&#20316;&#35774;&#35745;&#25506;&#38024;&#65292;&#36890;&#36807;&#20010;&#24615;&#21270;&#21644;&#20195;&#29702;&#22686;&#24378;&#29992;&#25143;&#30340;&#20889;&#20316;&#20307;&#39564;&#12290;&#23427;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#38544;&#24335;&#23398;&#20064;&#29992;&#25143;&#30340;&#20889;&#20316;&#39118;&#26684;&#65292;&#24182;&#20801;&#35768;&#29992;&#25143;&#36890;&#36807;&#25163;&#21160;&#26679;&#24335;&#32534;&#36753;&#21644;&#25209;&#27880;&#26469;&#25511;&#21046;&#31995;&#32479;&#30340;&#20889;&#20316;&#39118;&#26684;&#12290;</title><link>https://arxiv.org/abs/2402.08855</link><description>&lt;p&gt;
GhostWriter:&#36890;&#36807;&#20010;&#24615;&#21270;&#21644;&#20195;&#29702;&#22686;&#24378;&#21327;&#20316;&#20154;&#24037;&#26234;&#33021;&#20889;&#20316;&#20307;&#39564;
&lt;/p&gt;
&lt;p&gt;
GhostWriter: Augmenting Collaborative Human-AI Writing Experiences Through Personalization and Agency
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08855
&lt;/p&gt;
&lt;p&gt;
GhostWriter&#26159;&#19968;&#20010;AI&#22686;&#24378;&#30340;&#20889;&#20316;&#35774;&#35745;&#25506;&#38024;&#65292;&#36890;&#36807;&#20010;&#24615;&#21270;&#21644;&#20195;&#29702;&#22686;&#24378;&#29992;&#25143;&#30340;&#20889;&#20316;&#20307;&#39564;&#12290;&#23427;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#38544;&#24335;&#23398;&#20064;&#29992;&#25143;&#30340;&#20889;&#20316;&#39118;&#26684;&#65292;&#24182;&#20801;&#35768;&#29992;&#25143;&#36890;&#36807;&#25163;&#21160;&#26679;&#24335;&#32534;&#36753;&#21644;&#25209;&#27880;&#26469;&#25511;&#21046;&#31995;&#32479;&#30340;&#20889;&#20316;&#39118;&#26684;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#25552;&#20379;&#19981;&#21516;&#24418;&#24335;&#30340;&#20889;&#20316;&#36741;&#21161;&#26041;&#38754;&#36234;&#26469;&#36234;&#27969;&#34892;&#65292;&#24182;&#19988;&#20855;&#26377;&#26080;&#22788;&#19981;&#22312;&#30340;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20010;&#24615;&#21270;&#21644;&#25511;&#21046;&#33021;&#21147;&#26377;&#38480;&#65292;LLM&#39537;&#21160;&#30340;&#20889;&#20316;&#31995;&#32479;&#21487;&#33021;&#20250;&#20351;&#29992;&#25143;&#24863;&#21040;&#27822;&#20007;&#65292;&#24403;&#29992;&#25143;&#32570;&#20047;&#25552;&#31034;&#24037;&#31243;&#32463;&#39564;&#26102;&#65292;&#36825;&#31181;&#24773;&#20917;&#21487;&#33021;&#21152;&#21095;&#12290;&#25105;&#20204;&#35748;&#20026;&#35774;&#35745;&#21487;&#20197;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#20043;&#19968;&#65292;&#24182;&#24341;&#20837;GhostWriter&#65292;&#36825;&#26159;&#19968;&#20010;AI&#22686;&#24378;&#30340;&#20889;&#20316;&#35774;&#35745;&#25506;&#38024;&#65292;&#29992;&#25143;&#21487;&#20197;&#36890;&#36807;&#22686;&#24378;&#30340;&#20195;&#29702;&#21644;&#20010;&#24615;&#21270;&#26469;&#36827;&#34892;&#20889;&#20316;&#12290;GhostWriter&#21033;&#29992;LLMs&#22312;&#29992;&#25143;&#32534;&#20889;&#30340;&#36807;&#31243;&#20013;&#38544;&#24335;&#23398;&#20064;&#29992;&#25143;&#25152;&#26399;&#26395;&#30340;&#20889;&#20316;&#39118;&#26684;&#65292;&#21516;&#26102;&#20801;&#35768;&#36890;&#36807;&#25163;&#21160;&#26679;&#24335;&#32534;&#36753;&#21644;&#25209;&#27880;&#36827;&#34892;&#26174;&#24335;&#25945;&#23398;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;18&#21517;&#21442;&#19982;&#32773;&#22312;&#20004;&#20010;&#19981;&#21516;&#30340;&#20889;&#20316;&#20219;&#21153;&#20013;&#20351;&#29992;GhostWriter&#65292;&#35266;&#23519;&#21040;&#23427;&#24110;&#21161;&#29992;&#25143;&#32534;&#20889;&#20010;&#24615;&#21270;&#30340;&#25991;&#26412;&#29983;&#25104;&#65292;&#24182;&#36890;&#36807;&#25552;&#20379;&#22810;&#31181;&#26041;&#24335;&#25511;&#21046;&#31995;&#32479;&#30340;&#20889;&#20316;&#39118;&#26684;&#26469;&#22686;&#24378;&#29992;&#25143;&#30340;&#33021;&#21147;&#12290;&#20174;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20123;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08855v1 Announce Type: cross Abstract: Large language models (LLMs) are becoming more prevalent and have found a ubiquitous use in providing different forms of writing assistance. However, LLM-powered writing systems can frustrate users due to their limited personalization and control, which can be exacerbated when users lack experience with prompt engineering. We see design as one way to address these challenges and introduce GhostWriter, an AI-enhanced writing design probe where users can exercise enhanced agency and personalization. GhostWriter leverages LLMs to learn the user's intended writing style implicitly as they write, while allowing explicit teaching moments through manual style edits and annotations. We study 18 participants who use GhostWriter on two different writing tasks, observing that it helps users craft personalized text generations and empowers them by providing multiple ways to control the system's writing style. From this study, we present insights re
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#28151;&#21512;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#20943;&#23569;&#36870;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#19981;&#24517;&#35201;&#25506;&#32034;&#65292;&#36890;&#36807;&#22312;&#22312;&#32447;&#25968;&#25454;&#21644;&#19987;&#23478;&#25968;&#25454;&#30340;&#28151;&#21512;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#20174;&#32780;&#25552;&#39640;&#23398;&#20064;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.08848</link><description>&lt;p&gt;
&#28151;&#21512;&#36870;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Hybrid Inverse Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08848
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#28151;&#21512;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#20943;&#23569;&#36870;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#19981;&#24517;&#35201;&#25506;&#32034;&#65292;&#36890;&#36807;&#22312;&#22312;&#32447;&#25968;&#25454;&#21644;&#19987;&#23478;&#25968;&#25454;&#30340;&#28151;&#21512;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#20174;&#32780;&#25552;&#39640;&#23398;&#20064;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#27169;&#20223;&#23398;&#20064;&#26469;&#35828;&#65292;&#36870;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#26159;&#19968;&#25226;&#21452;&#20995;&#21073;&#12290;&#19968;&#26041;&#38754;&#65292;&#23427;&#21487;&#20197;&#36890;&#36807;&#36739;&#23569;&#30340;&#19987;&#23478;&#28436;&#31034;&#26469;&#36827;&#34892;&#23398;&#20064;&#65292;&#24182;&#19988;&#33021;&#22815;&#27604;&#34892;&#20026;&#20811;&#38534;&#26041;&#27861;&#26356;&#20855;&#40065;&#26834;&#24615;&#22320;&#22788;&#29702;&#38169;&#35823;&#32047;&#31215;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#23427;&#35201;&#27714;&#23398;&#20064;&#32773;&#21453;&#22797;&#35299;&#20915;&#35745;&#31639;&#20195;&#20215;&#39640;&#26114;&#30340;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#12290;&#36890;&#24120;&#24773;&#20917;&#19979;&#65292;&#36825;&#31181;&#35745;&#31639;&#24448;&#24448;&#20250;&#28010;&#36153;&#22312;&#25628;&#32034;&#38750;&#24120;&#19981;&#30456;&#20284;&#20110;&#19987;&#23478;&#31574;&#30053;&#30340;&#31574;&#30053;&#19978;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#28151;&#21512;&#24378;&#21270;&#23398;&#20064;-&#22312;&#22312;&#32447;&#25968;&#25454;&#21644;&#19987;&#23478;&#25968;&#25454;&#30340;&#28151;&#21512;&#19978;&#36827;&#34892;&#35757;&#32451;-&#20197;&#20943;&#23569;&#19981;&#24517;&#35201;&#30340;&#25506;&#32034;&#12290;&#30452;&#35266;&#19978;&#65292;&#19987;&#23478;&#25968;&#25454;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#23558;&#23398;&#20064;&#32773;&#19987;&#27880;&#20110;&#33391;&#22909;&#30340;&#29366;&#24577;&#65292;&#20174;&#32780;&#20943;&#23569;&#20102;&#35745;&#31639;&#24378;&#31574;&#30053;&#25152;&#38656;&#30340;&#25506;&#32034;&#37327;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#36825;&#31181;&#26041;&#27861;&#19981;&#38656;&#35201;&#23558;&#23398;&#20064;&#32773;&#37325;&#32622;&#21040;&#29615;&#22659;&#20013;&#30340;&#20219;&#24847;&#29366;&#24577;&#65292;&#36825;&#26159;&#20197;&#21069;&#22312;&#39640;&#25928;&#36870;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08848v1 Announce Type: cross Abstract: The inverse reinforcement learning approach to imitation learning is a double-edged sword. On the one hand, it can enable learning from a smaller number of expert demonstrations with more robustness to error compounding than behavioral cloning approaches. On the other hand, it requires that the learner repeatedly solve a computationally expensive reinforcement learning (RL) problem. Often, much of this computation is wasted searching over policies very dissimilar to the expert's. In this work, we propose using hybrid RL -- training on a mixture of online and expert data -- to curtail unnecessary exploration. Intuitively, the expert data focuses the learner on good states during training, which reduces the amount of exploration required to compute a strong policy. Notably, such an approach doesn't need the ability to reset the learner to arbitrary states in the environment, a requirement of prior work in efficient inverse RL. More formal
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32452;&#21512;&#30340;LLM&#26041;&#27861;&#65292;&#30001;&#35821;&#38899;&#32534;&#30721;&#22120;&#12289;LLM&#21644;&#32447;&#24615;&#25237;&#24433;&#22120;&#32452;&#25104;&#65292;&#33021;&#22815;&#32988;&#20219;ASR&#20219;&#21153;&#65292;&#24182;&#19988;&#20855;&#26377;&#28165;&#26224;&#30340;&#35774;&#32622;&#21644;&#23569;&#37327;&#30340;&#20219;&#21153;&#29305;&#23450;&#35774;&#35745;&#12290;</title><link>https://arxiv.org/abs/2402.08846</link><description>&lt;p&gt;
&#19968;&#31181;&#20855;&#26377;&#24378;ASR&#33021;&#21147;&#30340;&#20196;&#20154;&#23604;&#23596;&#31616;&#21333;&#30340;LLM&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
An Embarrassingly Simple Approach for LLM with Strong ASR Capacity
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08846
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32452;&#21512;&#30340;LLM&#26041;&#27861;&#65292;&#30001;&#35821;&#38899;&#32534;&#30721;&#22120;&#12289;LLM&#21644;&#32447;&#24615;&#25237;&#24433;&#22120;&#32452;&#25104;&#65292;&#33021;&#22815;&#32988;&#20219;ASR&#20219;&#21153;&#65292;&#24182;&#19988;&#20855;&#26377;&#28165;&#26224;&#30340;&#35774;&#32622;&#21644;&#23569;&#37327;&#30340;&#20219;&#21153;&#29305;&#23450;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#37325;&#28857;&#25918;&#22312;&#35299;&#20915;&#35821;&#38899;&#22788;&#29702;&#39046;&#22495;&#20013;&#26368;&#37325;&#35201;&#30340;&#20219;&#21153;&#20043;&#19968;&#65292;&#21363;&#20351;&#29992;&#35821;&#38899;&#22522;&#26412;&#32534;&#30721;&#22120;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36827;&#34892;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#37319;&#29992;&#20102;&#22797;&#26434;&#30340;&#35774;&#35745;&#65292;&#20363;&#22914;&#23545;&#35821;&#38899;&#32534;&#30721;&#22120;&#36827;&#34892;&#26102;&#38388;&#21387;&#32553;&#65292;&#22788;&#29702;&#25237;&#24433;&#20202;&#30340;&#27169;&#24577;&#23545;&#40784;&#65292;&#24182;&#21033;&#29992;&#21442;&#25968;&#39640;&#25928;&#30340;&#24494;&#35843;&#26041;&#27861;&#23545;LLM&#36827;&#34892;&#35843;&#25972;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#24182;&#19981;&#38656;&#35201;&#31934;&#32454;&#30340;&#35774;&#35745;&#65292;&#32780;&#26159;&#19968;&#20010;&#30001;&#29616;&#25104;&#30340;&#35821;&#38899;&#32534;&#30721;&#22120;&#12289;LLM&#21644;&#21807;&#19968;&#21487;&#35757;&#32451;&#30340;&#32447;&#24615;&#25237;&#24433;&#22120;&#32452;&#25104;&#30340;&#31616;&#21333;&#32452;&#21512;&#23601;&#33021;&#32988;&#20219;ASR&#20219;&#21153;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23545;&#21508;&#31181;LLM&#21644;&#35821;&#38899;&#32534;&#30721;&#22120;&#30340;&#32452;&#21512;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#21644;&#25506;&#32034;&#65292;&#20174;&#32780;&#24471;&#21040;&#20102;&#26368;&#20339;&#30340;&#22522;&#20110;LLM&#30340;ASR&#31995;&#32479;&#65292;&#25105;&#20204;&#23558;&#20854;&#31216;&#20026;SLAM-ASR&#12290;&#25152;&#25552;&#20986;&#30340;SLAM-ASR&#25552;&#20379;&#20102;&#19968;&#20010;&#28165;&#26224;&#30340;&#35774;&#32622;&#21644;&#23569;&#37327;&#30340;&#20219;&#21153;&#29305;&#23450;&#35774;&#35745;&#65292;&#21482;&#26377;&#32447;&#24615;&#25237;&#24433;&#22120;&#38656;&#35201;&#36827;&#34892;&#35757;&#32451;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;SLAM-ASR&#21462;&#24471;&#20102;&#26368;&#20339;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08846v1 Announce Type: cross Abstract: In this paper, we focus on solving one of the most important tasks in the field of speech processing, i.e., automatic speech recognition (ASR), with speech foundation encoders and large language models (LLM). Recent works have complex designs such as compressing the output temporally for the speech encoder, tackling modal alignment for the projector, and utilizing parameter-efficient fine-tuning for the LLM. We found that delicate designs are not necessary, while an embarrassingly simple composition of off-the-shelf speech encoder, LLM, and the only trainable linear projector is competent for the ASR task. To be more specific, we benchmark and explore various combinations of LLMs and speech encoders, leading to the optimal LLM-based ASR system, which we call SLAM-ASR. The proposed SLAM-ASR provides a clean setup and little task-specific design, where only the linear projector is trained. To the best of our knowledge, SLAM-ASR achieves t
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#21644;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#29305;&#21035;&#26159;&#24378;&#21270;&#23398;&#20064;&#21644;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#65292;&#20197;&#21450;&#27010;&#29575;&#24615;&#24314;&#27169;&#26041;&#27861;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#26234;&#33021;&#20892;&#19994;&#31649;&#29702;&#31995;&#32479;&#65292;&#21487;&#20197;&#25552;&#39640;&#20316;&#29289;&#20135;&#37327;&#65292;&#20248;&#21270;&#32933;&#26009;&#21644;&#27700;&#30340;&#20351;&#29992;&#65292;&#21516;&#26102;&#20943;&#23569;&#27694;&#32933;&#27969;&#22833;&#21644;&#28201;&#23460;&#27668;&#20307;&#25490;&#25918;&#65292;&#29305;&#21035;&#20851;&#27880;&#27687;&#21270;&#20122;&#27694;&#65288;N2O&#65289;&#25490;&#25918;&#12290;</title><link>https://arxiv.org/abs/2402.08832</link><description>&lt;p&gt;
&#32771;&#34385;N2O&#25490;&#25918;&#21644;&#27668;&#20505;&#21464;&#24322;&#30340;&#26234;&#33021;&#20892;&#19994;&#31649;&#29702;
&lt;/p&gt;
&lt;p&gt;
Intelligent Agricultural Management Considering N$_2$O Emission and Climate Variability with Uncertainties
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08832
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#21644;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#29305;&#21035;&#26159;&#24378;&#21270;&#23398;&#20064;&#21644;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#65292;&#20197;&#21450;&#27010;&#29575;&#24615;&#24314;&#27169;&#26041;&#27861;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#26234;&#33021;&#20892;&#19994;&#31649;&#29702;&#31995;&#32479;&#65292;&#21487;&#20197;&#25552;&#39640;&#20316;&#29289;&#20135;&#37327;&#65292;&#20248;&#21270;&#32933;&#26009;&#21644;&#27700;&#30340;&#20351;&#29992;&#65292;&#21516;&#26102;&#20943;&#23569;&#27694;&#32933;&#27969;&#22833;&#21644;&#28201;&#23460;&#27668;&#20307;&#25490;&#25918;&#65292;&#29305;&#21035;&#20851;&#27880;&#27687;&#21270;&#20122;&#27694;&#65288;N2O&#65289;&#25490;&#25918;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22914;&#20309;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#65292;&#29305;&#21035;&#26159;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#65292;&#22312;&#20892;&#19994;&#20013;&#25552;&#39640;&#20316;&#29289;&#20135;&#37327;&#65292;&#20248;&#21270;&#27694;&#32933;&#20351;&#29992;&#21644;&#27975;&#27700;&#65292;&#24182;&#20943;&#23569;&#30813;&#37240;&#30416;&#27969;&#22833;&#21644;&#28201;&#23460;&#27668;&#20307;&#25490;&#25918;&#65292;&#37325;&#28857;&#20851;&#27880;&#22303;&#22756;&#20013;&#30340;&#27687;&#21270;&#20122;&#27694;&#65288;N2O&#65289;&#25490;&#25918;&#12290;&#38754;&#23545;&#27668;&#20505;&#21464;&#21270;&#21644;&#26377;&#38480;&#30340;&#20892;&#19994;&#30693;&#35782;&#65292;&#25105;&#20204;&#20351;&#29992;&#37096;&#20998;&#21487;&#35266;&#23519;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;POMDPs&#65289;&#21644;&#20316;&#29289;&#27169;&#25311;&#22120;&#26469;&#27169;&#25311;AI&#20195;&#29702;&#19982;&#20892;&#19994;&#29615;&#22659;&#30340;&#20132;&#20114;&#12290;&#25105;&#20204;&#37319;&#29992;&#22522;&#20110;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#30340;Q&#32593;&#32476;&#23545;&#26234;&#33021;&#20195;&#29702;&#36827;&#34892;&#28145;&#24230;Q&#23398;&#20064;&#35757;&#32451;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#24320;&#21457;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#26469;&#39044;&#27979;N2O&#25490;&#25918;&#65292;&#24182;&#23558;&#36825;&#20123;&#39044;&#27979;&#25972;&#21512;&#21040;&#27169;&#25311;&#22120;&#20013;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#36890;&#36807;&#27010;&#29575;&#24615;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22788;&#29702;N2O&#25490;&#25918;&#20272;&#35745;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#36890;&#36807;&#38543;&#26426;&#22825;&#27668;&#27169;&#22411;&#22788;&#29702;&#27668;&#20505;&#21464;&#24322;&#65292;&#25552;&#20379;&#19968;&#31995;&#21015;&#25490;&#25918;&#32467;&#26524;&#26469;&#25552;&#39640;&#39044;&#27979;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08832v1 Announce Type: cross Abstract: This study examines how artificial intelligence (AI), especially Reinforcement Learning (RL), can be used in farming to boost crop yields, fine-tune nitrogen use and watering, and reduce nitrate runoff and greenhouse gases, focusing on Nitrous Oxide (N$_2$O) emissions from soil. Facing climate change and limited agricultural knowledge, we use Partially Observable Markov Decision Processes (POMDPs) with a crop simulator to model AI agents' interactions with farming environments. We apply deep Q-learning with Recurrent Neural Network (RNN)-based Q networks for training agents on optimal actions. Also, we develop Machine Learning (ML) models to predict N$_2$O emissions, integrating these predictions into the simulator. Our research tackles uncertainties in N$_2$O emission estimates with a probabilistic ML approach and climate variability through a stochastic weather model, offering a range of emission outcomes to improve forecast reliabili
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#24320;&#28304;&#30340;&#22823;&#35268;&#27169;&#39640;&#36136;&#37327;&#25351;&#23548;&#25968;&#25454;&#38598;ECInstruct&#65292;&#36890;&#36807;&#25351;&#23548;&#35843;&#20248;&#36890;&#29992;&#35821;&#35328;&#27169;&#22411;&#65292;&#24320;&#21457;&#20102;&#19968;&#31995;&#21015;&#30005;&#23376;&#21830;&#21153;LLMs&#65288;eCeLLM&#65289;&#65292;&#22312;&#30005;&#23376;&#21830;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2402.08831</link><description>&lt;p&gt;
eCeLLM&#65306;&#20174;&#22823;&#35268;&#27169;&#39640;&#36136;&#37327;&#25351;&#23548;&#25968;&#25454;&#20013;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#24191;&#21040;&#30005;&#23376;&#21830;&#21153;&#20013;
&lt;/p&gt;
&lt;p&gt;
eCeLLM: Generalizing Large Language Models for E-commerce from Large-scale, High-quality Instruction Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08831
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#24320;&#28304;&#30340;&#22823;&#35268;&#27169;&#39640;&#36136;&#37327;&#25351;&#23548;&#25968;&#25454;&#38598;ECInstruct&#65292;&#36890;&#36807;&#25351;&#23548;&#35843;&#20248;&#36890;&#29992;&#35821;&#35328;&#27169;&#22411;&#65292;&#24320;&#21457;&#20102;&#19968;&#31995;&#21015;&#30005;&#23376;&#21830;&#21153;LLMs&#65288;eCeLLM&#65289;&#65292;&#22312;&#30005;&#23376;&#21830;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22312;&#24320;&#21457;&#26377;&#25928;&#30340;&#30005;&#23376;&#21830;&#21153;&#27169;&#22411;&#26041;&#38754;&#20570;&#20986;&#24040;&#22823;&#21162;&#21147;&#65292;&#20256;&#32479;&#30340;&#30005;&#23376;&#21830;&#21153;&#27169;&#22411;&#22312;&#36890;&#29992;&#30005;&#23376;&#21830;&#21153;&#24314;&#27169;&#19978;&#21462;&#24471;&#20102;&#26377;&#38480;&#30340;&#25104;&#21151;&#65292;&#24182;&#19988;&#22312;&#26032;&#29992;&#25143;&#21644;&#26032;&#20135;&#21697;&#19978;&#30340;&#34920;&#29616;&#19981;&#20339;&#8212;&#8212;&#36825;&#26159;&#19968;&#20010;&#20856;&#22411;&#30340;&#39046;&#22495;&#22806;&#27867;&#21270;&#25361;&#25112;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#35768;&#22810;&#39046;&#22495;&#23637;&#31034;&#20986;&#20102;&#20986;&#33394;&#30340;&#36890;&#29992;&#24314;&#27169;&#21644;&#39046;&#22495;&#22806;&#27867;&#21270;&#33021;&#21147;&#12290;&#20026;&#20102;&#20805;&#20998;&#21457;&#25381;&#23427;&#20204;&#22312;&#30005;&#23376;&#21830;&#21153;&#20013;&#30340;&#20316;&#29992;&#65292;&#26412;&#25991;&#26500;&#24314;&#20102;ECInstruct&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#38754;&#21521;&#30005;&#23376;&#21830;&#21153;&#30340;&#24320;&#28304;&#12289;&#22823;&#35268;&#27169;&#21644;&#39640;&#36136;&#37327;&#30340;&#25351;&#23548;&#25968;&#25454;&#38598;&#12290;&#21033;&#29992;ECInstruct&#65292;&#25105;&#20204;&#36890;&#36807;&#25351;&#23548;&#35843;&#20248;&#36890;&#29992;&#35821;&#35328;&#27169;&#22411;&#24320;&#21457;&#20102;&#19968;&#31995;&#21015;&#30005;&#23376;&#21830;&#21153;LLMs&#65292;&#31216;&#20026;eCeLLM&#12290;&#25105;&#20204;&#30340;&#32508;&#21512;&#23454;&#39564;&#21644;&#35780;&#20272;&#34920;&#26126;&#65292;eCeLLM&#27169;&#22411;&#22312;&#20869;&#37096;&#29615;&#22659;&#20013;&#26126;&#26174;&#20248;&#20110;&#22522;&#20934;&#27169;&#22411;&#65292;&#21253;&#25324;&#26368;&#20808;&#36827;&#30340;GPT-4&#21644;&#26368;&#20808;&#36827;&#30340;&#29305;&#23450;&#20219;&#21153;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08831v1 Announce Type: cross Abstract: With tremendous efforts on developing effective e-commerce models, conventional e-commerce models show limited success in generalist e-commerce modeling, and suffer from unsatisfactory performance on new users and new products - a typical out-of-domain generalization challenge. Meanwhile, large language models (LLMs) demonstrate outstanding performance in generalist modeling and out-of-domain generalizability in many fields. Toward fully unleashing their power for e-commerce, in this paper, we construct ECInstruct, the first open-sourced, large-scale, and high-quality benchmark instruction dataset for e-commerce. Leveraging ECInstruct, we develop eCeLLM, a series of e-commerce LLMs, by instruction-tuning general-purpose LLMs. Our comprehensive experiments and evaluation demonstrate that eCeLLM models substantially outperform baseline models, including the most advanced GPT-4, and the state-of-the-art task-specific models in in-domain ev
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;"&#31867;&#20284;&#35774;&#35745;"&#30340;&#26234;&#33021;&#30011;&#24067;&#29615;&#22659;&#65292;&#36890;&#36807;&#23558;&#29983;&#25104;&#24335;AI&#32452;&#20214;&#38598;&#25104;&#21040;&#25968;&#25454;&#20998;&#26512;&#20013;&#65292;&#23454;&#29616;&#20102;&#24555;&#36895;&#21407;&#22411;&#35774;&#35745;&#12289;&#36845;&#20195;&#21644;&#27604;&#36739;&#21487;&#35270;&#21270;&#31649;&#29702;&#12290;&#36890;&#36807;&#29992;&#25143;&#30740;&#31350;&#65292;&#35770;&#25991;&#39564;&#35777;&#20102;&#30011;&#24067;&#30028;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.08812</link><description>&lt;p&gt;
&#26234;&#33021;&#30011;&#24067;: &#36890;&#36807;&#24555;&#36895;&#21407;&#22411;&#35774;&#35745;&#12289;&#36845;&#20195;&#21644;&#31649;&#29702;&#23454;&#29616;&#31867;&#20284;&#35774;&#35745;&#30340;&#25506;&#32034;&#24615;&#21487;&#35270;&#25968;&#25454;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Intelligent Canvas: Enabling Design-Like Exploratory Visual Data Analysis through Rapid Prototyping, Iteration and Curation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08812
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;"&#31867;&#20284;&#35774;&#35745;"&#30340;&#26234;&#33021;&#30011;&#24067;&#29615;&#22659;&#65292;&#36890;&#36807;&#23558;&#29983;&#25104;&#24335;AI&#32452;&#20214;&#38598;&#25104;&#21040;&#25968;&#25454;&#20998;&#26512;&#20013;&#65292;&#23454;&#29616;&#20102;&#24555;&#36895;&#21407;&#22411;&#35774;&#35745;&#12289;&#36845;&#20195;&#21644;&#27604;&#36739;&#21487;&#35270;&#21270;&#31649;&#29702;&#12290;&#36890;&#36807;&#29992;&#25143;&#30740;&#31350;&#65292;&#35770;&#25991;&#39564;&#35777;&#20102;&#30011;&#24067;&#30028;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22797;&#26434;&#25968;&#25454;&#20998;&#26512;&#36890;&#36807;&#25506;&#32034;&#24615;&#30340;&#21487;&#35270;&#20998;&#26512;&#26041;&#27861;&#26469;&#23547;&#27714;&#24847;&#24819;&#19981;&#21040;&#30340;&#27934;&#35265;&#65292;&#24182;&#36229;&#36234;&#36923;&#36753;&#30340;&#36880;&#27493;&#22788;&#29702;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#30028;&#38754;&#65288;&#22914;&#31508;&#35760;&#26412;&#21644;&#20202;&#34920;&#26495;&#65289;&#22312;&#21487;&#35270;&#25968;&#25454;&#20998;&#26512;&#30340;&#25506;&#32034;&#21644;&#27604;&#36739;&#26041;&#38754;&#23384;&#22312;&#19968;&#20123;&#23616;&#38480;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#8220;&#31867;&#20284;&#35774;&#35745;&#8221;&#30340;&#26234;&#33021;&#30011;&#24067;&#29615;&#22659;&#65292;&#23558;&#29983;&#25104;&#24335;AI&#38598;&#25104;&#21040;&#25968;&#25454;&#20998;&#26512;&#20013;&#65292;&#25552;&#20379;&#24555;&#36895;&#21407;&#22411;&#35774;&#35745;&#12289;&#36845;&#20195;&#21644;&#27604;&#36739;&#21487;&#35270;&#21270;&#31649;&#29702;&#12290;&#25105;&#20204;&#30340;&#20004;&#20010;&#20027;&#35201;&#36129;&#29486;&#21253;&#25324;&#23558;&#29983;&#25104;&#24335;AI&#32452;&#20214;&#38598;&#25104;&#21040;&#30011;&#24067;&#30028;&#38754;&#20013;&#65292;&#24182;&#36890;&#36807;&#29992;&#25143;&#30740;&#31350;&#65288;N=10&#65289;&#35780;&#20272;&#20102;&#30011;&#24067;&#30028;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08812v1 Announce Type: cross Abstract: Complex data analysis inherently seeks unexpected insights through exploratory \re{visual analysis} methods, transcending logical, step-by-step processing. However, \re{existing interfaces such as notebooks and dashboards have limitations in exploration and comparison for visual data analysis}. Addressing these limitations, we introduce a "design-like" intelligent canvas environment integrating generative AI into data analysis, offering rapid prototyping, iteration, and comparative visualization management. Our dual contributions include the integration of generative AI components into a canvas interface, and empirical findings from a user study (N=10) evaluating the effectiveness of the canvas interface.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#32508;&#21512;&#22810;&#20010;&#19981;&#21516;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21709;&#24212;&#21487;&#20197;&#25552;&#39640;&#37492;&#21035;&#35786;&#26029;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.08806</link><description>&lt;p&gt;
&#32508;&#21512;&#22810;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35265;&#35299;&#21487;&#20197;&#25552;&#39640;&#35786;&#26029;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;
Combining Insights From Multiple Large Language Models Improves Diagnostic Accuracy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08806
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#32508;&#21512;&#22810;&#20010;&#19981;&#21516;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21709;&#24212;&#21487;&#20197;&#25552;&#39640;&#37492;&#21035;&#35786;&#26029;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32972;&#26223;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;OpenAI&#30340;GPT-4&#25110;Google&#30340;PaLM 2&#34987;&#25552;&#35758;&#20316;&#20026;&#21487;&#34892;&#30340;&#35786;&#26029;&#25903;&#25345;&#24037;&#20855;&#65292;&#29978;&#33267;&#34987;&#31216;&#20026;&#8220;&#36335;&#36793;&#21672;&#35810;&#8221;&#30340;&#26367;&#20195;&#21697;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#29305;&#21035;&#38024;&#23545;&#21307;&#23398;&#20027;&#39064;&#36827;&#34892;&#35757;&#32451;&#30340;LLMs&#21487;&#33021;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#32570;&#20047;&#36275;&#22815;&#30340;&#35786;&#26029;&#20934;&#30830;&#24615;&#12290;&#26041;&#27861;&#65306;&#20351;&#29992;&#38598;&#20307;&#26234;&#24935;&#26041;&#27861;&#21644;200&#20010;&#30495;&#23454;&#30149;&#20363;&#30340;&#20020;&#24202;&#24773;&#22659;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#35780;&#20272;&#24182;&#27604;&#36739;&#20102;&#36890;&#36807;&#35810;&#38382;&#21333;&#20010;&#21830;&#19994;LLMs&#65288;OpenAI GPT-4&#12289;Google PaLM 2&#12289;Cohere Command&#12289;Meta Llama 2&#65289;&#33719;&#24471;&#30340;&#37492;&#21035;&#35786;&#26029;&#30340;&#20934;&#30830;&#24615;&#19982;&#36890;&#36807;&#32858;&#21512;&#30456;&#21516;LLMs&#30340;&#21709;&#24212;&#33719;&#24471;&#30340;&#37492;&#21035;&#35786;&#26029;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08806v1 Announce Type: new Abstract: Background: Large language models (LLMs) such as OpenAI's GPT-4 or Google's PaLM 2 are proposed as viable diagnostic support tools or even spoken of as replacements for "curbside consults". However, even LLMs specifically trained on medical topics may lack sufficient diagnostic accuracy for real-life applications.   Methods: Using collective intelligence methods and a dataset of 200 clinical vignettes of real-life cases, we assessed and compared the accuracy of differential diagnoses obtained by asking individual commercial LLMs (OpenAI GPT-4, Google PaLM 2, Cohere Command, Meta Llama 2) against the accuracy of differential diagnoses synthesized by aggregating responses from combinations of the same LLMs.   Results: We find that aggregating responses from multiple, various LLMs leads to more accurate differential diagnoses (average accuracy for 3 LLMs: $75.3\%\pm 1.6pp$) compared to the differential diagnoses produced by single LLMs (aver
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#36890;&#36807;&#23545;Stack Overflow&#38382;&#39064;&#30340;&#20998;&#26512;&#65292;&#30740;&#31350;&#20102;ChatGPT&#21644;LLaMA&#23545;&#20110;&#35813;&#24179;&#21488;&#30340;&#24433;&#21709;&#21644;&#21487;&#38752;&#24615;&#65292;&#20197;&#21450;&#23427;&#20204;&#22312;&#38271;&#26399;&#20869;&#21462;&#20195;Stack Overflow&#30340;&#25361;&#25112;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;LLMs&#22312;&#26576;&#20123;&#26041;&#38754;&#22833;&#36133;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#27604;LLMs&#30340;&#23454;&#35777;&#27604;&#36739;&#12290;</title><link>https://arxiv.org/abs/2402.08801</link><description>&lt;p&gt;
ChatGPT&#19982;LLaMA&#22312;Stack Overflow&#35752;&#35770;&#20013;&#30340;&#24433;&#21709;&#65292;&#21487;&#38752;&#24615;&#21644;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
ChatGPT vs LLaMA: Impact, Reliability, and Challenges in Stack Overflow Discussions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08801
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#36890;&#36807;&#23545;Stack Overflow&#38382;&#39064;&#30340;&#20998;&#26512;&#65292;&#30740;&#31350;&#20102;ChatGPT&#21644;LLaMA&#23545;&#20110;&#35813;&#24179;&#21488;&#30340;&#24433;&#21709;&#21644;&#21487;&#38752;&#24615;&#65292;&#20197;&#21450;&#23427;&#20204;&#22312;&#38271;&#26399;&#20869;&#21462;&#20195;Stack Overflow&#30340;&#25361;&#25112;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;LLMs&#22312;&#26576;&#20123;&#26041;&#38754;&#22833;&#36133;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#27604;LLMs&#30340;&#23454;&#35777;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;2022&#24180;11&#26376;&#21457;&#24067;&#20197;&#26469;&#65292;ChatGPT&#24050;&#32463;&#22312;Stack Overflow&#19978;&#24341;&#36215;&#36720;&#21160;&#65292;&#36825;&#26159;&#24320;&#21457;&#20154;&#21592;&#20851;&#20110;&#32534;&#31243;&#21644;&#36719;&#20214;&#24320;&#21457;&#38382;&#39064;&#30340;&#39318;&#36873;&#24179;&#21488;&#12290;ChatGPT&#23637;&#31034;&#20102;&#29983;&#25104;&#21363;&#26102;&#12289;&#20154;&#31867;&#33324;&#22238;&#31572;&#25216;&#26415;&#38382;&#39064;&#30340;&#33021;&#21147;&#65292;&#24341;&#36215;&#20102;&#24320;&#21457;&#32773;&#31038;&#21306;&#23545;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#26102;&#20195;&#19979;&#20154;&#31867;&#39537;&#21160;&#24179;&#21488;&#28436;&#21464;&#35282;&#33394;&#30340;&#36777;&#35770;&#12290;ChatGPT&#21457;&#24067;&#20004;&#20010;&#26376;&#21518;&#65292;Meta&#21457;&#24067;&#20102;&#23427;&#33258;&#24049;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM) LLaMA&#12290;&#20026;&#20102; (i) &#27979;&#37327;&#29992;&#25143;&#23545;Stack Overflow&#30340;&#26102;&#38388;&#28436;&#36827;&#19979;&#30340;&#21442;&#19982;&#31243;&#24230;&#65307;(ii) &#37327;&#21270;LLMs&#22238;&#31572;&#30340;&#21487;&#38752;&#24615;&#21450;&#20854;&#22312;&#38271;&#26399;&#20869;&#21462;&#20195;Stack Overflow&#30340;&#28508;&#21147;&#65307;(iii) &#30830;&#23450;&#21644;&#29702;&#35299;LLMs&#22833;&#36133;&#30340;&#21407;&#22240;&#65307;(iv) &#23545;&#27604;LLMs&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#65292;&#20998;&#26512;Stack Overflow&#19978;&#30340;&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#36825;&#20123;LLMs&#26469;&#22238;&#31572;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08801v1 Announce Type: cross Abstract: Since its release in November 2022, ChatGPT has shaken up Stack Overflow, the premier platform for developers' queries on programming and software development. Demonstrating an ability to generate instant, human-like responses to technical questions, ChatGPT has ignited debates within the developer community about the evolving role of human-driven platforms in the age of generative AI. Two months after ChatGPT's release, Meta released its answer with its own Large Language Model (LLM) called LLaMA: the race was on. We conducted an empirical study analyzing questions from Stack Overflow and using these LLMs to address them. This way, we aim to (ii) measure user engagement evolution with Stack Overflow over time; (ii) quantify the reliability of LLMs' answers and their potential to replace Stack Overflow in the long term; (iii) identify and understand why LLMs fails; and (iv) compare LLMs together. Our empirical results are unequivocal: C
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20998;&#26512;&#21683;&#22013;&#22768;&#38899;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19977;&#31181;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#33016;&#37096;X&#23556;&#32447;&#20013;&#39044;&#27979;&#24322;&#24120;&#32467;&#26524;&#65292;&#20174;&#32780;&#20248;&#21270;&#36164;&#28304;&#20351;&#29992;&#24182;&#25552;&#39640;&#21307;&#30103;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.08789</link><description>&lt;p&gt;
&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#29615;&#22659;&#20013;&#21033;&#29992;&#21683;&#22013;&#22768;&#38899;&#20248;&#21270;&#33016;&#37096;X&#23556;&#32447;&#30340;&#20351;&#29992;
&lt;/p&gt;
&lt;p&gt;
Leveraging cough sounds to optimize chest x-ray usage in low-resource settings
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08789
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20998;&#26512;&#21683;&#22013;&#22768;&#38899;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19977;&#31181;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#33016;&#37096;X&#23556;&#32447;&#20013;&#39044;&#27979;&#24322;&#24120;&#32467;&#26524;&#65292;&#20174;&#32780;&#20248;&#21270;&#36164;&#28304;&#20351;&#29992;&#24182;&#25552;&#39640;&#21307;&#30103;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33016;&#37096;X&#23556;&#32447;&#22312;&#24613;&#35786;&#12289;&#35786;&#26029;&#21644;&#21628;&#21560;&#31995;&#32479;&#30142;&#30149;&#31649;&#29702;&#20013;&#26159;&#24120;&#29992;&#30340;&#24037;&#20855;&#12290;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#29615;&#22659;&#19979;&#65292;&#20248;&#21270;&#36825;&#19968;&#36164;&#28304;&#21487;&#20197;&#20026;&#21307;&#30103;&#20445;&#20581;&#31995;&#32479;&#21644;&#24739;&#32773;&#33410;&#30465;&#23453;&#36149;&#30340;&#36153;&#29992;&#65292;&#21516;&#26102;&#25913;&#21892;&#21672;&#35810;&#26102;&#38388;&#12290;&#25105;&#20204;&#20351;&#29992;&#21360;&#24230;&#27604;&#21704;&#23572;&#37030;&#26222;&#23572;&#23612;&#20122;&#22522;&#30563;&#21307;&#23398;&#20013;&#24515;&#21644;&#21307;&#38498;&#65288;CMCH&#65289;&#30340;137&#21517;&#24739;&#32773;&#30340;&#21069;&#30651;&#24615;&#25910;&#38598;&#30340;&#25968;&#25454;&#12290;&#27599;&#20010;&#24739;&#32773;&#22312;&#31561;&#24453;&#36827;&#34892;&#25918;&#23556;&#29031;&#30456;&#26102;&#25552;&#20379;&#20102;&#33267;&#23569;&#20116;&#20010;&#21683;&#22013;&#22768;&#12290;&#25105;&#20204;&#20351;&#29992;&#22768;&#23398;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#20998;&#26512;&#20102;&#25910;&#38598;&#21040;&#30340;&#21683;&#22013;&#22768;&#38899;&#12290;&#23545;&#27599;&#20010;&#24739;&#32773;&#30340;&#21683;&#22013;&#22768;&#38899;&#36827;&#34892;&#20102;&#26102;&#38388;&#21644;&#39057;&#35889;&#29305;&#24449;&#30340;&#20132;&#21449;&#39564;&#35777;&#12290;&#20351;&#29992;&#26631;&#20934;&#32479;&#35745;&#26041;&#27861;&#23545;&#29305;&#24449;&#36827;&#34892;&#20102;&#24635;&#32467;&#12290;&#25105;&#20204;&#24320;&#21457;&#12289;&#27979;&#35797;&#21644;&#27604;&#36739;&#20102;&#19977;&#31181;&#27169;&#22411;&#65292;&#20197;&#39044;&#27979;&#33016;&#37096;X&#23556;&#32447;&#30340;&#24322;&#24120;&#32467;&#26524;&#12290;&#36825;&#19977;&#31181;&#26041;&#27861;&#22343;&#33021;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#21306;&#20998;&#27491;&#24120;&#21644;&#24322;&#24120;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08789v1 Announce Type: cross Abstract: Chest X-ray is a commonly used tool during triage, diagnosis and management of respiratory diseases. In resource-constricted settings, optimizing this resource can lead to valuable cost savings for the health care system and the patients as well as to and improvement in consult time. We used prospectively-collected data from 137 patients referred for chest X-ray at the Christian Medical Center and Hospital (CMCH) in Purnia, Bihar, India. Each patient provided at least five coughs while awaiting radiography. Collected cough sounds were analyzed using acoustic AI methods. Cross-validation was done on temporal and spectral features on the cough sounds of each patient. Features were summarized using standard statistical approaches. Three models were developed, tested and compared in their capacity to predict an abnormal result in the chest X-ray. All three methods yielded models that could discriminate to some extent between normal and abno
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23454;&#29616;&#20102;&#22312;&#33258;&#23450;&#20041;&#20108;&#32500;&#36187;&#36947;&#19978;&#36827;&#34892;&#28145;&#24230;Q&#23398;&#20064;&#30340;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#65292;&#24182;&#36890;&#36807;&#20248;&#21270;&#31639;&#27861;&#25552;&#39640;&#20102;DQN&#32593;&#32476;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.08780</link><description>&lt;p&gt;
&#22686;&#24378;&#30340;&#20108;&#32500;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#30340;&#28145;&#24230;Q&#23398;&#20064;&#65306;&#22312;&#33258;&#23450;&#20041;&#36187;&#36947;&#29615;&#22659;&#20013;&#30340;&#23454;&#29616;&#19982;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Enhanced Deep Q-Learning for 2D Self-Driving Cars: Implementation and Evaluation on a Custom Track Environment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08780
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23454;&#29616;&#20102;&#22312;&#33258;&#23450;&#20041;&#20108;&#32500;&#36187;&#36947;&#19978;&#36827;&#34892;&#28145;&#24230;Q&#23398;&#20064;&#30340;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#65292;&#24182;&#36890;&#36807;&#20248;&#21270;&#31639;&#27861;&#25552;&#39640;&#20102;DQN&#32593;&#32476;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#39033;&#30446;&#20171;&#32461;&#20102;&#22312;&#20108;&#32500;&#33258;&#23450;&#20041;&#36187;&#36947;&#19978;&#23454;&#29616;&#28145;&#24230;Q&#23398;&#20064;&#32593;&#32476;&#65288;DQN&#65289;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#30340;&#26041;&#27861;&#65292;&#24182;&#26088;&#22312;&#25552;&#39640;DQN&#32593;&#32476;&#30340;&#24615;&#33021;&#12290;&#35813;&#39033;&#30446;&#21253;&#25324;&#20351;&#29992;Pygame&#24320;&#21457;&#33258;&#23450;&#20041;&#39550;&#39542;&#29615;&#22659;&#20197;&#21450;&#35774;&#35745;&#21644;&#23454;&#29616;DQN&#27169;&#22411;&#12290;&#31639;&#27861;&#21033;&#29992;&#27773;&#36710;&#19978;&#23433;&#35013;&#30340;7&#20010;&#20256;&#24863;&#22120;&#25910;&#38598;&#27773;&#36710;&#19982;&#36187;&#36947;&#20043;&#38388;&#30340;&#36317;&#31163;&#25968;&#25454;&#12290;&#36825;&#20123;&#20256;&#24863;&#22120;&#20301;&#20110;&#36710;&#36742;&#21069;&#26041;&#65292;&#38388;&#38548;20&#24230;&#65292;&#33021;&#22815;&#24863;&#30693;&#21069;&#26041;&#24191;&#38420;&#21306;&#22495;&#12290;&#25105;&#20204;&#25104;&#21151;&#23454;&#29616;&#20102;DQN&#65292;&#36824;&#23454;&#29616;&#20102;&#19968;&#20010;&#22522;&#20110;&#20248;&#20808;&#32423;&#30340;&#21160;&#20316;&#36873;&#25321;&#26426;&#21046;&#30340;&#20462;&#25913;&#29256;DQN&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#20462;&#25913;&#29256;DQN&#12290;&#27169;&#22411;&#32463;&#36807;1000&#20010;&#22238;&#21512;&#30340;&#35757;&#32451;&#65292;&#21457;&#29616;&#20195;&#29702;&#31243;&#24207;&#24179;&#22343;&#22870;&#21169;&#32422;&#20026;40&#65292;&#27604;&#26222;&#36890;DQN&#39640;&#32422;60%&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08780v1 Announce Type: new Abstract: This research project presents the implementation of a Deep Q-Learning Network (DQN) for a self-driving car on a 2-dimensional (2D) custom track, with the objective of enhancing the DQN network's performance. It encompasses the development of a custom driving environment using Pygame on a track surrounding the University of Memphis map, as well as the design and implementation of the DQN model. The algorithm utilizes data from 7 sensors installed in the car, which measure the distance between the car and the track. These sensors are positioned in front of the vehicle, spaced 20 degrees apart, enabling them to sense a wide area ahead. We successfully implemented the DQN and also a modified version of the DQN with a priority-based action selection mechanism, which we refer to as modified DQN. The model was trained over 1000 episodes, and the average reward received by the agent was found to be around 40, which is approximately 60% higher th
&lt;/p&gt;</description></item><item><title>DNABERT-S&#26159;&#19968;&#31181;&#19987;&#38376;&#29992;&#20110;&#21019;&#24314;&#29289;&#31181;&#24863;&#30693;&#30340;DNA&#23884;&#20837;&#30340;&#22522;&#22240;&#32452;&#22522;&#30784;&#27169;&#22411;&#12290;&#20026;&#20102;&#25552;&#39640;&#23545;&#38271;&#35835;DNA&#24207;&#21015;&#30340;&#23884;&#20837;&#25928;&#26524;&#65292;&#24341;&#20837;&#20102;Manifold Instance Mixup (MI-Mix)&#23545;&#27604;&#30446;&#26631;&#26041;&#27861;&#26469;&#35757;&#32451;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.08777</link><description>&lt;p&gt;
DNABERT-S: &#23398;&#20064;&#20855;&#26377;&#22522;&#22240;&#32452;&#22522;&#30784;&#27169;&#22411;&#30340;&#29289;&#31181;&#24863;&#30693;DNA&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
DNABERT-S: Learning Species-Aware DNA Embedding with Genome Foundation Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08777
&lt;/p&gt;
&lt;p&gt;
DNABERT-S&#26159;&#19968;&#31181;&#19987;&#38376;&#29992;&#20110;&#21019;&#24314;&#29289;&#31181;&#24863;&#30693;&#30340;DNA&#23884;&#20837;&#30340;&#22522;&#22240;&#32452;&#22522;&#30784;&#27169;&#22411;&#12290;&#20026;&#20102;&#25552;&#39640;&#23545;&#38271;&#35835;DNA&#24207;&#21015;&#30340;&#23884;&#20837;&#25928;&#26524;&#65292;&#24341;&#20837;&#20102;Manifold Instance Mixup (MI-Mix)&#23545;&#27604;&#30446;&#26631;&#26041;&#27861;&#26469;&#35757;&#32451;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#25928;&#30340;DNA&#23884;&#20837;&#22312;&#22522;&#22240;&#32452;&#20998;&#26512;&#20013;&#20173;&#28982;&#33267;&#20851;&#37325;&#35201;&#65292;&#29305;&#21035;&#26159;&#22312;&#32570;&#20047;&#29992;&#20110;&#27169;&#22411;&#24494;&#35843;&#30340;&#26631;&#35760;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#23613;&#31649;&#22522;&#22240;&#32452;&#22522;&#30784;&#27169;&#22411;&#24050;&#32463;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#19968;&#20010;&#20856;&#22411;&#30340;&#20363;&#23376;&#26159;&#23439;&#22522;&#22240;&#32452;&#20998;&#31665;&#65292;&#36825;&#26159;&#24494;&#29983;&#29289;&#32452;&#30740;&#31350;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#36807;&#31243;&#65292;&#26088;&#22312;&#36890;&#36807;&#26469;&#33258;&#21487;&#33021;&#21253;&#21547;&#25104;&#21315;&#19978;&#19975;&#20010;&#19981;&#21516;&#30340;&#12289;&#36890;&#24120;&#27809;&#26377;&#32463;&#36807;&#34920;&#24449;&#30340;&#29289;&#31181;&#30340;&#22797;&#26434;&#28151;&#21512;DNA&#24207;&#21015;&#30340;&#29289;&#31181;&#26469;&#23545;DNA&#24207;&#21015;&#36827;&#34892;&#20998;&#32452;&#12290;&#20026;&#20102;&#22635;&#34917;&#26377;&#25928;&#30340;DNA&#23884;&#20837;&#27169;&#22411;&#30340;&#32570;&#38519;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;DNABERT-S&#65292;&#36825;&#26159;&#19968;&#20010;&#19987;&#38376;&#29992;&#20110;&#21019;&#24314;&#29289;&#31181;&#24863;&#30693;&#30340;DNA&#23884;&#20837;&#30340;&#22522;&#22240;&#32452;&#22522;&#30784;&#27169;&#22411;&#12290;&#20026;&#20102;&#40723;&#21169;&#23545;&#26131;&#20986;&#38169;&#30340;&#38271;&#35835;DNA&#24207;&#21015;&#36827;&#34892;&#26377;&#25928;&#23884;&#20837;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Manifold Instance Mixup(MI-Mix)&#65292;&#19968;&#31181;&#23545;&#27604;&#30446;&#26631;&#65292;&#23427;&#22312;&#38543;&#26426;&#36873;&#25321;&#30340;&#23618;&#27425;&#20013;&#28151;&#21512;DNA&#24207;&#21015;&#30340;&#38544;&#34255;&#34920;&#31034;&#65292;&#24182;&#35757;&#32451;&#27169;&#22411;&#20197;&#22312;&#36755;&#20986;&#23618;&#35782;&#21035;&#21644;&#21306;&#20998;&#36825;&#20123;&#28151;&#21512;&#27604;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08777v1 Announce Type: cross Abstract: Effective DNA embedding remains crucial in genomic analysis, particularly in scenarios lacking labeled data for model fine-tuning, despite the significant advancements in genome foundation models. A prime example is metagenomics binning, a critical process in microbiome research that aims to group DNA sequences by their species from a complex mixture of DNA sequences derived from potentially thousands of distinct, often uncharacterized species. To fill the lack of effective DNA embedding models, we introduce DNABERT-S, a genome foundation model that specializes in creating species-aware DNA embeddings. To encourage effective embeddings to error-prone long-read DNA sequences, we introduce Manifold Instance Mixup (MI-Mix), a contrastive objective that mixes the hidden representations of DNA sequences at randomly selected layers and trains the model to recognize and differentiate these mixed proportions at the output layer. We further enha
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20219;&#21153;&#20998;&#37197;&#21644;&#36335;&#24452;&#35268;&#21010;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22522;&#20110;&#20914;&#31361;&#30340;&#25628;&#32034;&#26041;&#27861;&#35299;&#20915;&#20855;&#26377;&#20808;&#20915;&#21644;&#26102;&#38388;&#32422;&#26463;&#30340;&#20219;&#21153;&#20998;&#37197;&#21644;&#36335;&#24452;&#35268;&#21010;&#38382;&#39064;&#65292;&#26368;&#22823;&#21270;&#29992;&#25143;&#23450;&#20041;&#30340;&#25351;&#26631;&#30340;&#22238;&#25253;&#12290;</title><link>https://arxiv.org/abs/2402.08772</link><description>&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;&#20914;&#31361;&#30340;&#25628;&#32034;&#21644;&#20808;&#20915;&#21644;&#26102;&#38388;&#32422;&#26463;&#30340;&#26368;&#20248;&#20219;&#21153;&#20998;&#37197;&#21644;&#36335;&#24452;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Optimal Task Assignment and Path Planning using Conflict-Based Search with Precedence and Temporal Constraints
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08772
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20219;&#21153;&#20998;&#37197;&#21644;&#36335;&#24452;&#35268;&#21010;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22522;&#20110;&#20914;&#31361;&#30340;&#25628;&#32034;&#26041;&#27861;&#35299;&#20915;&#20855;&#26377;&#20808;&#20915;&#21644;&#26102;&#38388;&#32422;&#26463;&#30340;&#20219;&#21153;&#20998;&#37197;&#21644;&#36335;&#24452;&#35268;&#21010;&#38382;&#39064;&#65292;&#26368;&#22823;&#21270;&#29992;&#25143;&#23450;&#20041;&#30340;&#25351;&#26631;&#30340;&#22238;&#25253;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;&#65288;MAPF&#65289;&#38382;&#39064;&#28041;&#21450;&#20026;&#19968;&#32452;&#26234;&#33021;&#20307;&#25214;&#21040;&#26080;&#30896;&#25758;&#36335;&#24452;&#65292;&#24341;&#23548;&#23427;&#20204;&#20174;&#36215;&#28857;&#21040;&#30446;&#26631;&#20301;&#32622;&#12290;&#28982;&#32780;&#65292;MAPF&#27809;&#26377;&#32771;&#34385;&#20960;&#20010;&#23454;&#38469;&#30340;&#20219;&#21153;&#30456;&#20851;&#32422;&#26463;&#12290;&#20363;&#22914;&#65292;&#26234;&#33021;&#20307;&#21487;&#33021;&#38656;&#35201;&#22312;&#30446;&#26631;&#20301;&#32622;&#25191;&#34892;&#20855;&#26377;&#29305;&#23450;&#25191;&#34892;&#26102;&#38388;&#30340;&#21160;&#20316;&#65292;&#36981;&#24490;&#39044;&#23450;&#30340;&#39034;&#24207;&#21644;&#26102;&#38388;&#26694;&#26550;&#12290;&#27492;&#22806;&#65292;&#30446;&#26631;&#20998;&#37197;&#21487;&#33021;&#19981;&#26159;&#39044;&#20808;&#23450;&#20041;&#30340;&#65292;&#20248;&#21270;&#30446;&#26631;&#21487;&#33021;&#32570;&#20047;&#26126;&#30830;&#23450;&#20041;&#12290;&#20026;&#20102;&#23558;&#20219;&#21153;&#20998;&#37197;&#12289;&#36335;&#24452;&#35268;&#21010;&#21644;&#29992;&#25143;&#23450;&#20041;&#30340;&#30446;&#26631;&#32467;&#21512;&#25104;&#19968;&#20010;&#36830;&#36143;&#30340;&#26694;&#26550;&#65292;&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#20808;&#20915;&#21644;&#26102;&#38388;&#32422;&#26463;&#30340;&#20219;&#21153;&#20998;&#37197;&#21644;&#36335;&#24452;&#35268;&#21010;&#65288;TAPF-PTC&#65289;&#38382;&#39064;&#12290;&#25105;&#20204;&#22686;&#21152;&#20102;&#22522;&#20110;&#20914;&#31361;&#30340;&#25628;&#32034;&#65288;CBS&#65289;&#20197;&#21516;&#26102;&#29983;&#25104;&#36981;&#23432;&#20808;&#20915;&#21644;&#26102;&#38388;&#32422;&#26463;&#30340;&#20219;&#21153;&#20998;&#37197;&#21644;&#26080;&#30896;&#25758;&#36335;&#24452;&#65292;&#24182;&#26368;&#22823;&#21270;&#22522;&#20110;&#29992;&#25143;&#23450;&#20041;&#30340;&#25351;&#26631;&#30340;&#22238;&#25253;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08772v1 Announce Type: new Abstract: The Multi-Agent Path Finding (MAPF) problem entails finding collision-free paths for a set of agents, guiding them from their start to goal locations. However, MAPF does not account for several practical task-related constraints. For example, agents may need to perform actions at goal locations with specific execution times, adhering to predetermined orders and timeframes. Moreover, goal assignments may not be predefined for agents, and the optimization objective may lack an explicit definition. To incorporate task assignment, path planning, and a user-defined objective into a coherent framework, this paper examines the Task Assignment and Path Finding with Precedence and Temporal Constraints (TAPF-PTC) problem. We augment Conflict-Based Search (CBS) to simultaneously generate task assignments and collision-free paths that adhere to precedence and temporal constraints, maximizing an objective quantified by the return from a user-defined r
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#20316;&#32773;&#36523;&#20221;&#28151;&#28102;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#65292;&#21487;&#20197;&#22312;&#20445;&#25345;&#21407;&#22987;&#20869;&#23481;&#21644;&#27969;&#30021;&#24615;&#30340;&#21516;&#26102;&#28151;&#28102;&#20316;&#32773;&#36523;&#20221;&#12290;</title><link>https://arxiv.org/abs/2402.08761</link><description>&lt;p&gt;
JAMDEC&#65306;&#20351;&#29992;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#26377;&#32422;&#26463;&#35299;&#30721;&#30340;&#26080;&#30417;&#30563;&#20316;&#32773;&#36523;&#20221;&#28151;&#28102;
&lt;/p&gt;
&lt;p&gt;
JAMDEC: Unsupervised Authorship Obfuscation using Constrained Decoding over Small Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08761
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#20316;&#32773;&#36523;&#20221;&#28151;&#28102;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#65292;&#21487;&#20197;&#22312;&#20445;&#25345;&#21407;&#22987;&#20869;&#23481;&#21644;&#27969;&#30021;&#24615;&#30340;&#21516;&#26102;&#28151;&#28102;&#20316;&#32773;&#36523;&#20221;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22686;&#24378;&#30340;&#20316;&#32773;&#36523;&#20221;&#35782;&#21035;&#25216;&#26415;&#21644;&#22312;&#32447;&#20869;&#23481;&#30340;&#27704;&#20037;&#24615;&#65292;&#38656;&#35201;&#26356;&#24378;&#30340;&#35745;&#31639;&#26041;&#27861;&#26469;&#20445;&#25252;&#22312;&#32447;&#20316;&#32773;&#36523;&#20221;&#30340;&#38544;&#31169;&#65292;&#20363;&#22914;&#31185;&#23398;&#35770;&#25991;&#30340;&#30450;&#23457;&#12289;&#21311;&#21517;&#22312;&#32447;&#35780;&#35770;&#25110;&#22312;&#24515;&#29702;&#20581;&#24247;&#35770;&#22363;&#19978;&#30340;&#21311;&#21517;&#20114;&#21160;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#25512;&#29702;&#26102;&#38388;&#20316;&#32773;&#36523;&#20221;&#28151;&#28102;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#20316;&#32773;&#36523;&#20221;&#28151;&#28102;&#30340;&#29420;&#29305;&#25361;&#25112;&#65306;&#32570;&#20047;&#22810;&#26679;&#21270;&#30340;&#20316;&#32773;&#36523;&#20221;&#21644;&#39046;&#22495;&#30340;&#30417;&#30563;&#25968;&#25454;&#65292;&#20197;&#21450;&#22312;&#28151;&#28102;&#20316;&#32773;&#36523;&#20221;&#30340;&#21516;&#26102;&#20445;&#25345;&#21407;&#22987;&#20869;&#23481;&#21644;&#27969;&#30021;&#24615;&#30340;&#38656;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08761v1 Announce Type: cross Abstract: The permanence of online content combined with the enhanced authorship identification techniques calls for stronger computational methods to protect the identity and privacy of online authorship when needed, e.g., blind reviews for scientific papers, anonymous online reviews, or anonymous interactions in the mental health forums. In this paper, we propose an unsupervised inference-time approach to authorship obfuscation to address the unique challenges of authorship obfuscation: lack of supervision data for diverse authorship and domains, and the need for a sufficient level of revision beyond simple paraphrasing to obfuscate the authorship, all the while preserving the original content and fluency.   We introduce JAMDEC, a user-controlled, inference-time algorithm for authorship obfuscation that can be in principle applied to any text and authorship. Our approach builds on small language models such as GPT2-XL in order to help avoid dis
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;LLMs&#29983;&#25104;&#21512;&#25104;&#20154;&#31867;&#28436;&#31034;&#30340;&#26041;&#27861;&#65292;&#28982;&#21518;&#36890;&#36807;&#27169;&#20223;&#23398;&#20064;&#26469;&#23398;&#20064;&#20122;&#29702;&#24615;&#20195;&#29702;&#31574;&#30053;&#65292;&#20174;&#32780;&#27169;&#25311;&#20122;&#29702;&#24615;&#34892;&#20026;&#65292;&#24182;&#20026;&#25105;&#20204;&#29702;&#35299;&#20154;&#31867;&#34892;&#20026;&#25552;&#20379;&#20102;&#25913;&#36827;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.08755</link><description>&lt;p&gt;
LLM&#39537;&#21160;&#30340;&#27169;&#25311;&#20122;&#29702;&#24615;&#34892;&#20026;&#65306;&#24187;&#35273;&#36824;&#26159;&#29616;&#23454;&#65311;
&lt;/p&gt;
&lt;p&gt;
LLM-driven Imitation of Subrational Behavior : Illusion or Reality?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08755
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;LLMs&#29983;&#25104;&#21512;&#25104;&#20154;&#31867;&#28436;&#31034;&#30340;&#26041;&#27861;&#65292;&#28982;&#21518;&#36890;&#36807;&#27169;&#20223;&#23398;&#20064;&#26469;&#23398;&#20064;&#20122;&#29702;&#24615;&#20195;&#29702;&#31574;&#30053;&#65292;&#20174;&#32780;&#27169;&#25311;&#20122;&#29702;&#24615;&#34892;&#20026;&#65292;&#24182;&#20026;&#25105;&#20204;&#29702;&#35299;&#20154;&#31867;&#34892;&#20026;&#25552;&#20379;&#20102;&#25913;&#36827;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24314;&#27169;&#20122;&#29702;&#24615;&#20195;&#29702;&#65292;&#22914;&#20154;&#31867;&#25110;&#32463;&#27982;&#23478;&#24237;&#65292;&#30001;&#20110;&#26657;&#20934;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#30340;&#22256;&#38590;&#25110;&#25910;&#38598;&#28041;&#21450;&#20154;&#31867;&#20027;&#20307;&#30340;&#25968;&#25454;&#30340;&#38590;&#24230;&#32780;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#29616;&#26377;&#30740;&#31350;&#24378;&#35843;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#35299;&#20915;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#21644;&#27169;&#20223;&#20154;&#31867;&#20132;&#27969;&#30340;&#33021;&#21147;&#65292;&#32780;&#20351;&#29992;LLMs&#20316;&#20026;&#20195;&#29702;&#36827;&#34892;&#27169;&#25311;&#26174;&#31034;&#20986;&#20986;&#29616;&#30340;&#31038;&#20132;&#34892;&#20026;&#65292;&#21487;&#33021;&#25552;&#39640;&#25105;&#20204;&#23545;&#20154;&#31867;&#34892;&#20026;&#30340;&#29702;&#35299;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#35758;&#30740;&#31350;&#20351;&#29992;LLMs&#29983;&#25104;&#21512;&#25104;&#30340;&#20154;&#31867;&#28436;&#31034;&#65292;&#28982;&#21518;&#36890;&#36807;&#27169;&#20223;&#23398;&#20064;&#26469;&#23398;&#20064;&#20122;&#29702;&#24615;&#20195;&#29702;&#31574;&#30053;&#12290;&#25105;&#20204;&#20551;&#35774;LLMs&#21487;&#20197;&#29992;&#20316;&#20154;&#31867;&#30340;&#38544;&#24335;&#35745;&#31639;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#20351;&#29992;&#20174;LLMs&#27966;&#29983;&#30340;&#21512;&#25104;&#28436;&#31034;&#26469;&#24314;&#27169;&#20154;&#31867;&#29305;&#26377;&#30340;&#20122;&#29702;&#24615;&#34892;&#20026;&#65288;&#20363;&#22914;&#65292;&#30446;&#20809;&#30701;&#27973;&#30340;&#34892;&#20026;&#25110;&#23545;&#39118;&#38505;&#35268;&#36991;&#30340;&#20559;&#22909;&#65289;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#23454;&#39564;&#35777;&#26126;:
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08755v1 Announce Type: new Abstract: Modeling subrational agents, such as humans or economic households, is inherently challenging due to the difficulty in calibrating reinforcement learning models or collecting data that involves human subjects. Existing work highlights the ability of Large Language Models (LLMs) to address complex reasoning tasks and mimic human communication, while simulation using LLMs as agents shows emergent social behaviors, potentially improving our comprehension of human conduct. In this paper, we propose to investigate the use of LLMs to generate synthetic human demonstrations, which are then used to learn subrational agent policies though Imitation Learning. We make an assumption that LLMs can be used as implicit computational models of humans, and propose a framework to use synthetic demonstrations derived from LLMs to model subrational behaviors that are characteristic of humans (e.g., myopic behavior or preference for risk aversion). We experim
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;PRDP&#26041;&#27861;&#65292;&#36890;&#36807;&#36817;&#31471;&#22870;&#21169;&#24046;&#24322;&#39044;&#27979;&#23454;&#29616;&#20102;&#31283;&#23450;&#30340;&#40657;&#30418;&#22870;&#21169;&#24494;&#35843;&#25193;&#25955;&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;&#22823;&#35268;&#27169;&#25552;&#31034;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#19988;&#20855;&#26377;&#26356;&#22909;&#30340;&#35757;&#32451;&#31283;&#23450;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.08714</link><description>&lt;p&gt;
PRDP&#65306;&#22823;&#35268;&#27169;&#25193;&#25955;&#27169;&#22411;&#30340;&#36817;&#31471;&#22870;&#21169;&#24046;&#24322;&#39044;&#27979;&#29992;&#20110;&#22870;&#21169;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
PRDP: Proximal Reward Difference Prediction for Large-Scale Reward Finetuning of Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08714
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;PRDP&#26041;&#27861;&#65292;&#36890;&#36807;&#36817;&#31471;&#22870;&#21169;&#24046;&#24322;&#39044;&#27979;&#23454;&#29616;&#20102;&#31283;&#23450;&#30340;&#40657;&#30418;&#22870;&#21169;&#24494;&#35843;&#25193;&#25955;&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;&#22823;&#35268;&#27169;&#25552;&#31034;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#19988;&#20855;&#26377;&#26356;&#22909;&#30340;&#35757;&#32451;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22870;&#21169;&#24494;&#35843;&#24050;&#25104;&#20026;&#23558;&#22522;&#30784;&#27169;&#22411;&#19982;&#19979;&#28216;&#30446;&#26631;&#23545;&#40784;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#12290;&#22312;&#35821;&#35328;&#39046;&#22495;&#65292;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26469;&#26368;&#22823;&#21270;&#21453;&#26144;&#20154;&#31867;&#20559;&#22909;&#30340;&#22870;&#21169;&#24050;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#22312;&#35270;&#35273;&#39046;&#22495;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;RL&#30340;&#22870;&#21169;&#24494;&#35843;&#26041;&#27861;&#22312;&#22823;&#35268;&#27169;&#35757;&#32451;&#20013;&#23384;&#22312;&#19981;&#31283;&#23450;&#24615;&#65292;&#20351;&#23427;&#20204;&#26080;&#27861;&#25512;&#24191;&#21040;&#22797;&#26434;&#30340;&#12289;&#26410;&#30693;&#30340;&#25552;&#31034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36817;&#31471;&#22870;&#21169;&#24046;&#24322;&#39044;&#27979;&#65288;PRDP&#65289;&#65292;&#39318;&#27425;&#22312;&#36229;&#36807;100K&#20010;&#25552;&#31034;&#30340;&#22823;&#35268;&#27169;&#25552;&#31034;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#31283;&#23450;&#30340;&#40657;&#30418;&#22870;&#21169;&#24494;&#35843;&#25193;&#25955;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#21019;&#26032;&#26159;&#22870;&#21169;&#24046;&#24322;&#39044;&#27979;&#65288;RDP&#65289;&#30446;&#26631;&#65292;&#35813;&#30446;&#26631;&#19982;RL&#30446;&#26631;&#20855;&#26377;&#30456;&#21516;&#30340;&#26368;&#20248;&#35299;&#65292;&#21516;&#26102;&#20139;&#21463;&#26356;&#22909;&#30340;&#35757;&#32451;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08714v1 Announce Type: cross Abstract: Reward finetuning has emerged as a promising approach to aligning foundation models with downstream objectives. Remarkable success has been achieved in the language domain by using reinforcement learning (RL) to maximize rewards that reflect human preference. However, in the vision domain, existing RL-based reward finetuning methods are limited by their instability in large-scale training, rendering them incapable of generalizing to complex, unseen prompts. In this paper, we propose Proximal Reward Difference Prediction (PRDP), enabling stable black-box reward finetuning for diffusion models for the first time on large-scale prompt datasets with over 100K prompts. Our key innovation is the Reward Difference Prediction (RDP) objective that has the same optimal solution as the RL objective while enjoying better training stability. Specifically, the RDP objective is a supervised regression objective that tasks the diffusion model with pred
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#32508;&#36848;&#25552;&#20986;&#20102;&#19968;&#20010;&#24191;&#20041;&#26041;&#27861;&#26469;&#39537;&#21160;AI&#33647;&#29289;&#35774;&#35745;&#65292;&#37325;&#28857;&#20851;&#27880;&#23567;&#20998;&#23376;&#21644;&#34507;&#30333;&#36136;&#29983;&#25104;&#20004;&#20010;&#20027;&#35201;&#20027;&#39064;&#65292;&#20171;&#32461;&#20102;&#21508;&#31181;&#23376;&#20219;&#21153;&#21644;&#24212;&#29992;&#65292;&#24182;&#27604;&#36739;&#20102;&#39030;&#32423;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.08703</link><description>&lt;p&gt;
&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#22312;&#20840;&#26032;&#33647;&#29289;&#35774;&#35745;&#20013;&#30340;&#24212;&#29992;&#27010;&#36848;&#65306;&#20998;&#23376;&#21644;&#34507;&#30333;&#36136;&#29983;&#25104;&#30340;&#26032;&#39046;&#22495;
&lt;/p&gt;
&lt;p&gt;
A Survey of Generative AI for De Novo Drug Design: New Frontiers in Molecule and Protein Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08703
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#32508;&#36848;&#25552;&#20986;&#20102;&#19968;&#20010;&#24191;&#20041;&#26041;&#27861;&#26469;&#39537;&#21160;AI&#33647;&#29289;&#35774;&#35745;&#65292;&#37325;&#28857;&#20851;&#27880;&#23567;&#20998;&#23376;&#21644;&#34507;&#30333;&#36136;&#29983;&#25104;&#20004;&#20010;&#20027;&#35201;&#20027;&#39064;&#65292;&#20171;&#32461;&#20102;&#21508;&#31181;&#23376;&#20219;&#21153;&#21644;&#24212;&#29992;&#65292;&#24182;&#27604;&#36739;&#20102;&#39030;&#32423;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#39537;&#21160;&#30340;&#26041;&#27861;&#21487;&#20197;&#26497;&#22823;&#22320;&#25913;&#36827;&#21382;&#26469;&#20195;&#20215;&#39640;&#26114;&#30340;&#33647;&#29289;&#35774;&#35745;&#36807;&#31243;&#65292;&#21508;&#31181;&#29983;&#25104;&#27169;&#22411;&#24050;&#32463;&#22312;&#24191;&#27867;&#20351;&#29992;&#20013;&#12290;&#29305;&#21035;&#26159;&#38024;&#23545;&#20840;&#26032;&#33647;&#29289;&#35774;&#35745;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#19987;&#27880;&#20110;&#20174;&#38646;&#24320;&#22987;&#21019;&#24314;&#26032;&#30340;&#29983;&#29289;&#21270;&#21512;&#29289;&#65292;&#23637;&#31034;&#20102;&#19968;&#20010;&#26377;&#21069;&#26223;&#30340;&#26410;&#26469;&#26041;&#21521;&#12290;&#35813;&#39046;&#22495;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#21152;&#19978;&#33647;&#29289;&#35774;&#35745;&#36807;&#31243;&#30340;&#22266;&#26377;&#22797;&#26434;&#24615;&#65292;&#20026;&#26032;&#30740;&#31350;&#20154;&#21592;&#36827;&#20837;&#21019;&#36896;&#20102;&#19968;&#20010;&#22256;&#38590;&#30340;&#23616;&#38754;&#12290;&#22312;&#36825;&#20221;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#23558;&#20840;&#26032;&#33647;&#29289;&#35774;&#35745;&#20998;&#20026;&#20004;&#20010;&#20027;&#35201;&#20027;&#39064;&#65306;&#23567;&#20998;&#23376;&#21644;&#34507;&#30333;&#36136;&#29983;&#25104;&#12290;&#22312;&#27599;&#20010;&#20027;&#39064;&#20013;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#21508;&#31181;&#23376;&#20219;&#21153;&#21644;&#24212;&#29992;&#65292;&#37325;&#28857;&#20171;&#32461;&#20102;&#37325;&#35201;&#30340;&#25968;&#25454;&#38598;&#12289;&#22522;&#20934;&#21644;&#27169;&#22411;&#26550;&#26500;&#65292;&#24182;&#23545;&#39030;&#32423;&#27169;&#22411;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#24191;&#20041;&#30340;&#26041;&#27861;&#26469;&#39537;&#21160;AI&#33647;&#29289;&#35774;&#35745;&#65292;&#20801;&#35768;&#22312;&#27599;&#20010;&#23376;&#20219;&#21153;&#20013;&#36827;&#34892;&#21508;&#31181;&#26041;&#27861;&#30340;&#24494;&#35266;&#27604;&#36739;&#21644;&#23439;&#35266;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08703v1 Announce Type: cross Abstract: Artificial intelligence (AI)-driven methods can vastly improve the historically costly drug design process, with various generative models already in widespread use. Generative models for de novo drug design, in particular, focus on the creation of novel biological compounds entirely from scratch, representing a promising future direction. Rapid development in the field, combined with the inherent complexity of the drug design process, creates a difficult landscape for new researchers to enter. In this survey, we organize de novo drug design into two overarching themes: small molecule and protein generation. Within each theme, we identify a variety of subtasks and applications, highlighting important datasets, benchmarks, and model architectures and comparing the performance of top models. We take a broad approach to AI-driven drug design, allowing for both micro-level comparisons of various methods within each subtask and macro-level o
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#22810;&#27493;&#20219;&#21153;&#20013;&#38598;&#25104;&#20154;&#31867;&#21453;&#39304;&#21644;&#20559;&#22909;&#23545;&#40784;&#30340;PRompt&#20248;&#21270;&#26041;&#27861;&#12290;&#23427;&#20351;&#29992;&#36951;&#20256;&#31639;&#27861;&#26694;&#26550;&#65292;&#32467;&#21512;&#20154;&#31867;&#21453;&#39304;&#33258;&#21160;&#25552;&#20986;&#20248;&#21270;&#24314;&#35758;&#24182;&#35299;&#20915;&#20102;&#22797;&#26434;&#30340;&#25552;&#31034;&#20869;&#23481;&#20998;&#26512;&#12289;&#21333;&#27493;&#35780;&#20272;&#21644;&#20219;&#21153;&#25191;&#34892;&#20559;&#22909;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.08702</link><description>&lt;p&gt;
PRompt Optimization in Multi-Step Tasks (PROMST): Integrating Human Feedback and Preference Alignment
&lt;/p&gt;
&lt;p&gt;
PRompt Optimization in Multi-Step Tasks (PROMST): Integrating Human Feedback and Preference Alignment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08702
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#22810;&#27493;&#20219;&#21153;&#20013;&#38598;&#25104;&#20154;&#31867;&#21453;&#39304;&#21644;&#20559;&#22909;&#23545;&#40784;&#30340;PRompt&#20248;&#21270;&#26041;&#27861;&#12290;&#23427;&#20351;&#29992;&#36951;&#20256;&#31639;&#27861;&#26694;&#26550;&#65292;&#32467;&#21512;&#20154;&#31867;&#21453;&#39304;&#33258;&#21160;&#25552;&#20986;&#20248;&#21270;&#24314;&#35758;&#24182;&#35299;&#20915;&#20102;&#22797;&#26434;&#30340;&#25552;&#31034;&#20869;&#23481;&#20998;&#26512;&#12289;&#21333;&#27493;&#35780;&#20272;&#21644;&#20219;&#21153;&#25191;&#34892;&#20559;&#22909;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Prompt optimization aims to find the best prompt for a language model (LLM) in multi-step tasks. This paper introduces a genetic algorithm framework that incorporates human feedback to automatically suggest prompt improvements. It addresses challenges such as complex prompt content analysis, evaluation of individual steps, and variations in task execution preferences.
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08702v1 Announce Type: cross Abstract: Prompt optimization aims to find the best prompt to a large language model (LLM) for a given task. LLMs have been successfully used to help find and improve prompt candidates for single-step tasks. However, realistic tasks for agents are multi-step and introduce new challenges: (1) Prompt content is likely to be more extensive and complex, making it more difficult for LLMs to analyze errors, (2) the impact of an individual step is difficult to evaluate, and (3) different people may have varied preferences about task execution. While humans struggle to optimize prompts, they are good at providing feedback about LLM outputs; we therefore introduce a new LLM-driven discrete prompt optimization framework that incorporates human-designed feedback rules about potential errors to automatically offer direct suggestions for improvement. Our framework is stylized as a genetic algorithm in which an LLM generates new candidate prompts from a parent
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#36890;&#36807;&#35757;&#32451;&#29983;&#25104;&#27169;&#22411;&#26469;&#29983;&#25104;&#38899;&#20048;&#20048;&#35889;&#26159;&#21542;&#21487;&#20197;&#23454;&#29616;&#20196;&#20154;&#20449;&#26381;&#30340;&#31038;&#20132;&#20307;&#39564;&#65292;&#32780;&#26080;&#38656;&#20248;&#21270;&#20854;&#21516;&#27493;&#21644;&#24310;&#32493;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.08690</link><description>&lt;p&gt;
&#22914;&#26524;&#22270;&#28789;&#21644;&#19968;&#20010;&#20154;&#24037;&#20249;&#20276;&#19968;&#36215;&#24377;&#38050;&#29748;
&lt;/p&gt;
&lt;p&gt;
If Turing played piano with an artificial partner
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08690
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#36890;&#36807;&#35757;&#32451;&#29983;&#25104;&#27169;&#22411;&#26469;&#29983;&#25104;&#38899;&#20048;&#20048;&#35889;&#26159;&#21542;&#21487;&#20197;&#23454;&#29616;&#20196;&#20154;&#20449;&#26381;&#30340;&#31038;&#20132;&#20307;&#39564;&#65292;&#32780;&#26080;&#38656;&#20248;&#21270;&#20854;&#21516;&#27493;&#21644;&#24310;&#32493;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38899;&#20048;&#26159;&#19968;&#31181;&#22825;&#29983;&#30340;&#31038;&#20132;&#27963;&#21160;&#65292;&#20801;&#35768;&#20154;&#20204;&#20998;&#20139;&#20307;&#39564;&#24182;&#19982;&#24444;&#27492;&#20135;&#29983;&#36830;&#25509;&#12290;&#35774;&#35745;&#23454;&#29616;&#19982;&#19982;&#21478;&#19968;&#20010;&#20154;&#19968;&#36215;&#28436;&#22863;&#31867;&#20284;&#30340;&#31038;&#20132;&#20307;&#39564;&#30340;&#20154;&#24037;&#20249;&#20276;&#26041;&#38754;&#30340;&#36827;&#23637;&#24456;&#23567;&#12290;&#23454;&#29616;&#29983;&#25104;&#27169;&#22411;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#22914;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36866;&#21512;&#29983;&#25104;&#38899;&#20048;&#20048;&#35889;&#12290;&#28982;&#32780;&#65292;&#31038;&#20132;&#24615;&#30340;&#38899;&#20048;&#28436;&#22863;&#19981;&#20165;&#20165;&#26159;&#28436;&#22863;&#19968;&#20010;&#20048;&#35889;&#65307;&#23427;&#24517;&#39035;&#19982;&#20854;&#20182;&#38899;&#20048;&#23478;&#30340;&#24819;&#27861;&#30456;&#21327;&#35843;&#65292;&#24182;&#27491;&#30830;&#20445;&#25345;&#26102;&#38388;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#19968;&#20010;&#38382;&#39064;&#65292;&#21363;&#36890;&#36807;&#35757;&#32451;&#29992;&#20110;&#29983;&#25104;&#38899;&#20048;&#20048;&#35889;&#30340;&#29983;&#25104;&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#20135;&#29983;&#20196;&#20154;&#20449;&#26381;&#30340;&#31038;&#20132;&#20307;&#39564;&#65292;&#32780;&#19981;&#24517;&#20248;&#21270;&#20854;&#21516;&#27493;&#21644;&#24310;&#32493;&#33021;&#21147;&#12290;&#35813;&#32593;&#32476;&#26159;&#22312;&#22823;&#37327;&#25968;&#23383;&#20048;&#35889;&#35821;&#26009;&#24211;&#19978;&#35757;&#32451;&#30340;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65292;&#22312;&#19982;&#20154;&#31867;&#20249;&#20276;&#36827;&#34892;&#23450;&#26102;&#30340;&#38382;&#31572;&#20219;&#21153;&#20013;&#36827;&#34892;&#20102;&#36866;&#24212;&#12290;&#21442;&#19982;&#32773;&#19982;&#20154;&#31867;&#25110;&#20154;&#24037;&#20249;&#20276;&#19968;&#36215;&#24377;&#22863;&#38050;&#29748;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08690v1 Announce Type: cross Abstract: Music is an inherently social activity that allows people to share experiences and feel connected with one another. There has been little progress in designing artificial partners exhibiting a similar social experience as playing with another person. Neural network architectures that implement generative models, such as large language models, are suited for producing musical scores. Playing music socially, however, involves more than playing a score; it must complement the other musicians' ideas and keep time correctly. We addressed the question of whether a convincing social experience is made possible by a generative model trained to produce musical scores, not necessarily optimized for synchronization and continuation. The network, a variational autoencoder trained on a large corpus of digital scores, was adapted for a timed call-and-response task with a human partner. Participants played piano with a human or artificial partner-in v
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#31995;&#32479;&#35843;&#26597;&#20843;&#20010;&#25552;&#31034;&#29305;&#24449;&#23545;&#29983;&#25104;&#30340;&#20195;&#30721;&#30340;&#24433;&#21709;&#65292;&#21253;&#25324;&#26679;&#24335;&#12289;&#20869;&#23481;&#12289;&#27491;&#30830;&#24615;&#12289;&#22797;&#26434;&#24615;&#12289;&#22823;&#23567;&#21644;&#19982;&#24320;&#21457;&#32773;&#20195;&#30721;&#30340;&#30456;&#20284;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.08430</link><description>&lt;p&gt;
&#20998;&#26512;&#25552;&#31034;&#23545;&#33258;&#21160;&#29983;&#25104;&#26041;&#27861;&#30340;&#24433;&#21709;&#65306;&#19968;&#20010;&#22522;&#20110;Copilot&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Analyzing Prompt Influence on Automated Method Generation: An Empirical Study with Copilot
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08430
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#31995;&#32479;&#35843;&#26597;&#20843;&#20010;&#25552;&#31034;&#29305;&#24449;&#23545;&#29983;&#25104;&#30340;&#20195;&#30721;&#30340;&#24433;&#21709;&#65292;&#21253;&#25324;&#26679;&#24335;&#12289;&#20869;&#23481;&#12289;&#27491;&#30830;&#24615;&#12289;&#22797;&#26434;&#24615;&#12289;&#22823;&#23567;&#21644;&#19982;&#24320;&#21457;&#32773;&#20195;&#30721;&#30340;&#30456;&#20284;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08430v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#20132;&#21449; &#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#27491;&#22312;&#25913;&#21464;&#24320;&#21457;&#32773;&#19982;&#36719;&#20214;&#31995;&#32479;&#30340;&#20132;&#20114;&#26041;&#24335;&#65292;&#25552;&#20379;&#33021;&#22815;&#20135;&#29983;&#21644;&#20256;&#36882;&#26032;&#20869;&#23481;&#30340;&#26381;&#21153;&#65292;&#20197;&#28385;&#36275;&#24320;&#21457;&#32773;&#30340;&#23454;&#38469;&#38656;&#27714;&#12290;&#20363;&#22914;&#65292;&#24320;&#21457;&#32773;&#21487;&#20197;&#36890;&#36807;&#32534;&#20889;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#22312;&#20854;IDE&#20013;&#30452;&#25509;&#35831;&#27714;&#26032;&#20195;&#30721;&#65292;&#22522;&#20110;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#30340;&#38598;&#25104;&#26381;&#21153;&#65292;&#22914;Copilot&#65292;&#20250;&#31435;&#21363;&#36890;&#36807;&#25552;&#20379;&#21363;&#21487;&#20351;&#29992;&#30340;&#20195;&#30721;&#29255;&#27573;&#26469;&#21709;&#24212;&#25552;&#31034;&#12290;&#24688;&#24403;&#22320;&#32534;&#20889;&#25552;&#31034;&#65292;&#24182;&#22312;&#36991;&#20813;&#20449;&#24687;&#36807;&#36733;&#30340;&#21516;&#26102;&#34701;&#20837;&#26377;&#29992;&#20449;&#24687;&#65292;&#23545;&#20110;&#33719;&#21462;&#27491;&#30830;&#30340;&#20195;&#30721;&#29255;&#27573;&#21487;&#33021;&#26159;&#19968;&#20010;&#37325;&#35201;&#22240;&#32032;&#12290;&#35774;&#35745;&#22909;&#30340;&#25552;&#31034;&#30340;&#20219;&#21153;&#34987;&#31216;&#20026;&#25552;&#31034;&#24037;&#31243;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#20843;&#20010;&#25552;&#31034;&#29305;&#24449;&#23545;&#29983;&#25104;&#30340;&#20195;&#30721;&#30340;&#26679;&#24335;&#21644;&#20869;&#23481;&#12289;&#27491;&#30830;&#24615;&#12289;&#22797;&#26434;&#24615;&#12289;&#22823;&#23567;&#21644;&#19982;&#24320;&#21457;&#32773;&#20195;&#30721;&#30340;&#30456;&#20284;&#24615;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08430v1 Announce Type: cross Abstract: Generative AI is changing the way developers interact with software systems, providing services that can produce and deliver new content, crafted to satisfy the actual needs of developers. For instance, developers can ask for new code directly from within their IDEs by writing natural language prompts, and integrated services based on generative AI, such as Copilot, immediately respond to prompts by providing ready-to-use code snippets. Formulating the prompt appropriately, and incorporating the useful information while avoiding any information overload, can be an important factor in obtaining the right piece of code. The task of designing good prompts is known as prompt engineering. In this paper, we systematically investigate the influence of eight prompt features on the style and the content of prompts, on the level of correctness, complexity, size, and similarity to the developers' code of the generated code. We specifically conside
&lt;/p&gt;</description></item><item><title>ChatCell&#26159;&#19968;&#20010;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#20419;&#36827;&#21333;&#32454;&#32990;&#20998;&#26512;&#30340;&#24037;&#20855;&#65292;&#36890;&#36807;&#35789;&#27719;&#36866;&#24212;&#21644;&#32479;&#19968;&#24207;&#21015;&#29983;&#25104;&#65292;&#23427;&#20855;&#22791;&#28145;&#21402;&#30340;&#19987;&#19994;&#30693;&#35782;&#21644;&#36866;&#24212;&#21508;&#31181;&#20998;&#26512;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.08303</link><description>&lt;p&gt;
ChatCell: &#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#20419;&#36827;&#21333;&#32454;&#32990;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
ChatCell: Facilitating Single-Cell Analysis with Natural Language
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08303
&lt;/p&gt;
&lt;p&gt;
ChatCell&#26159;&#19968;&#20010;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#20419;&#36827;&#21333;&#32454;&#32990;&#20998;&#26512;&#30340;&#24037;&#20855;&#65292;&#36890;&#36807;&#35789;&#27719;&#36866;&#24212;&#21644;&#32479;&#19968;&#24207;&#21015;&#29983;&#25104;&#65292;&#23427;&#20855;&#22791;&#28145;&#21402;&#30340;&#19987;&#19994;&#30693;&#35782;&#21644;&#36866;&#24212;&#21508;&#31181;&#20998;&#26512;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#23427;&#20204;&#22312;&#31185;&#23398;&#20013;&#30340;&#24433;&#21709;&#26085;&#30410;&#31361;&#20986;&#12290;LLMs&#22312;&#20219;&#21153;&#27867;&#21270;&#21644;&#33258;&#30001;&#23545;&#35805;&#26041;&#38754;&#30340;&#26032;&#20852;&#33021;&#21147;&#21487;&#20197;&#26497;&#22823;&#22320;&#25512;&#36827;&#21270;&#23398;&#21644;&#29983;&#29289;&#23398;&#31561;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#21333;&#32454;&#32990;&#29983;&#29289;&#23398;&#36825;&#20010;&#26500;&#25104;&#29983;&#29289;&#20307;&#22522;&#30784;&#26500;&#20214;&#30340;&#39046;&#22495;&#20173;&#38754;&#20020;&#19968;&#20123;&#25361;&#25112;&#12290;&#24403;&#21069;&#26041;&#27861;&#22312;&#30693;&#35782;&#38376;&#27099;&#21644;&#21487;&#25193;&#23637;&#24615;&#26041;&#38754;&#23384;&#22312;&#38480;&#21046;&#65292;&#38459;&#30861;&#20102;LLMs&#22312;&#25484;&#25569;&#21333;&#32454;&#32990;&#25968;&#25454;&#26041;&#38754;&#30340;&#20805;&#20998;&#21033;&#29992;&#65292;&#24433;&#21709;&#20102;&#30452;&#25509;&#21487;&#35775;&#38382;&#21644;&#24555;&#36895;&#36845;&#20195;&#30340;&#33021;&#21147;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;ChatCell&#65292;&#36890;&#36807;&#21033;&#29992;&#35789;&#27719;&#36866;&#24212;&#21644;&#32479;&#19968;&#24207;&#21015;&#29983;&#25104;&#65292;&#23427;&#22312;&#21333;&#32454;&#32990;&#29983;&#29289;&#23398;&#39046;&#22495;&#33719;&#24471;&#20102;&#28145;&#21402;&#30340;&#19987;&#19994;&#30693;&#35782;&#21644;&#36866;&#24212;&#21508;&#31181;&#20998;&#26512;&#20219;&#21153;&#30340;&#33021;&#21147;&#65292;&#26631;&#24535;&#30528;&#19968;&#31181;&#33539;&#24335;&#36716;&#21464;&#12290;
&lt;/p&gt;
&lt;p&gt;
As Large Language Models (LLMs) rapidly evolve, their influence in science is becoming increasingly prominent. The emerging capabilities of LLMs in task generalization and free-form dialogue can significantly advance fields like chemistry and biology. However, the field of single-cell biology, which forms the foundational building blocks of living organisms, still faces several challenges. High knowledge barriers and limited scalability in current methods restrict the full exploitation of LLMs in mastering single-cell data, impeding direct accessibility and rapid iteration. To this end, we introduce ChatCell, which signifies a paradigm shift by facilitating single-cell analysis with natural language. Leveraging vocabulary adaptation and unified sequence generation, ChatCell has acquired profound expertise in single-cell biology and the capability to accommodate a diverse range of analysis tasks. Extensive experiments further demonstrate ChatCell's robust performance and potential to de
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20174;&#26550;&#26500;&#30340;&#35282;&#24230;&#20840;&#38754;&#35843;&#26597;&#20102;&#22270;&#30340;&#36229;&#20998;&#24067;&#25512;&#24191;&#65292;&#25581;&#31034;&#20102;&#22270;&#33258;&#25105;&#27880;&#24847;&#26426;&#21046;&#21644;&#20854;&#20182;&#24120;&#35265;&#26500;&#24314;&#27169;&#22359;&#22312;&#36229;&#20998;&#24067;&#38382;&#39064;&#19978;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2402.08228</link><description>&lt;p&gt;
&#30740;&#31350;GNN&#30340;&#36229;&#20998;&#24067;&#25512;&#24191;&#65306;&#20174;&#26550;&#26500;&#35282;&#24230;&#30340;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Investigating Out-of-Distribution Generalization of GNNs: An Architecture Perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08228
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20174;&#26550;&#26500;&#30340;&#35282;&#24230;&#20840;&#38754;&#35843;&#26597;&#20102;&#22270;&#30340;&#36229;&#20998;&#24067;&#25512;&#24191;&#65292;&#25581;&#31034;&#20102;&#22270;&#33258;&#25105;&#27880;&#24847;&#26426;&#21046;&#21644;&#20854;&#20182;&#24120;&#35265;&#26500;&#24314;&#27169;&#22359;&#22312;&#36229;&#20998;&#24067;&#38382;&#39064;&#19978;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#22312;&#27979;&#35797;&#25968;&#25454;&#26469;&#33258;&#20110;&#35757;&#32451;&#25968;&#25454;&#30456;&#21516;&#20998;&#24067;&#30340;&#20551;&#35774;&#19979;&#34920;&#29616;&#20986;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22312;&#30495;&#23454;&#22330;&#26223;&#20013;&#65292;&#36825;&#20010;&#20551;&#35774;&#21487;&#33021;&#24182;&#19981;&#24635;&#26159;&#25104;&#31435;&#12290;&#22240;&#27492;&#65292;&#22312;&#22270;&#30340;&#19978;&#19979;&#25991;&#20013;&#65292;&#23545;&#36229;&#20998;&#24067;&#65288;OOD&#65289;&#38382;&#39064;&#30340;&#25506;&#32034;&#26085;&#30410;&#21463;&#21040;&#20851;&#27880;&#12290;&#22823;&#37096;&#20998;&#29616;&#26377;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#25913;&#36827;&#22270;&#30340;OOD&#25512;&#24191;&#30340;&#20004;&#20010;&#8220;&#27169;&#22411;&#26080;&#20851;&#8221;&#35282;&#24230;&#19978;&#65306;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#21644;&#31574;&#30053;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#24050;&#30693;&#30340;GNN&#27169;&#22411;&#26550;&#26500;&#23545;&#22270;&#30340;OOD&#25512;&#24191;&#30340;&#24433;&#21709;&#30340;&#30740;&#31350;&#30456;&#23545;&#36739;&#23569;&#65292;&#36825;&#19982;&#29616;&#26377;&#30340;&#30740;&#31350;&#30456;&#20114;&#29420;&#31435;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20174;&#26550;&#26500;&#30340;&#35282;&#24230;&#39318;&#27425;&#20840;&#38754;&#35843;&#26597;&#20102;&#22270;&#30340;OOD&#25512;&#24191;&#65292;&#24182;&#23545;&#29616;&#20195;GNN&#30340;&#24120;&#35265;&#26500;&#24314;&#27169;&#22359;&#36827;&#34892;&#20102;&#32771;&#23519;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#22270;&#33258;&#25105;&#27880;&#24847;&#26426;&#21046;&#21644;...
&lt;/p&gt;
&lt;p&gt;
Graph neural networks (GNNs) have exhibited remarkable performance under the assumption that test data comes from the same distribution of training data. However, in real-world scenarios, this assumption may not always be valid. Consequently, there is a growing focus on exploring the Out-of-Distribution (OOD) problem in the context of graphs. Most existing efforts have primarily concentrated on improving graph OOD generalization from two \textbf{model-agnostic} perspectives: data-driven methods and strategy-based learning. However, there has been limited attention dedicated to investigating the impact of well-known \textbf{GNN model architectures} on graph OOD generalization, which is orthogonal to existing research. In this work, we provide the first comprehensive investigation of OOD generalization on graphs from an architecture perspective, by examining the common building blocks of modern GNNs. Through extensive experiments, we reveal that both the graph self-attention mechanism an
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#37319;&#29992;&#22797;&#26434;&#31995;&#32479;&#26041;&#27861;&#65292;&#20197;&#22240;&#26524;&#24490;&#29615;&#22270;&#30340;&#24418;&#24335;&#25506;&#35752;&#20102;&#20154;&#24037;&#26234;&#33021;&#22312;&#20856;&#22411;&#39640;&#31561;&#25945;&#32946;&#26426;&#26500;&#20013;&#30340;&#36716;&#22411;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#39640;&#31561;&#25945;&#32946;&#26426;&#26500;&#21463;&#21040;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#36827;&#27493;&#30340;&#25512;&#21160;&#65292;&#25237;&#36164;&#20110;&#25913;&#21892;&#23398;&#29983;&#23398;&#20064;&#12289;&#30740;&#31350;&#21644;&#31649;&#29702;&#65292;&#24182;&#20026;&#27492;&#24212;&#23545;&#23398;&#26415;&#35802;&#20449;&#38382;&#39064;&#21644;&#25216;&#26415;&#21464;&#21270;&#12290;</title><link>https://arxiv.org/abs/2402.08143</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#19982;&#39640;&#31561;&#25945;&#32946;&#26426;&#26500;&#30340;&#36716;&#22411;
&lt;/p&gt;
&lt;p&gt;
Artificial intelligence and the transformation of higher education institutions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08143
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#37319;&#29992;&#22797;&#26434;&#31995;&#32479;&#26041;&#27861;&#65292;&#20197;&#22240;&#26524;&#24490;&#29615;&#22270;&#30340;&#24418;&#24335;&#25506;&#35752;&#20102;&#20154;&#24037;&#26234;&#33021;&#22312;&#20856;&#22411;&#39640;&#31561;&#25945;&#32946;&#26426;&#26500;&#20013;&#30340;&#36716;&#22411;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#39640;&#31561;&#25945;&#32946;&#26426;&#26500;&#21463;&#21040;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#36827;&#27493;&#30340;&#25512;&#21160;&#65292;&#25237;&#36164;&#20110;&#25913;&#21892;&#23398;&#29983;&#23398;&#20064;&#12289;&#30740;&#31350;&#21644;&#31649;&#29702;&#65292;&#24182;&#20026;&#27492;&#24212;&#23545;&#23398;&#26415;&#35802;&#20449;&#38382;&#39064;&#21644;&#25216;&#26415;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#30340;&#36827;&#27493;&#21644;ChatGPT&#31561;&#29983;&#25104;&#24335;AI&#24037;&#20855;&#30340;&#24555;&#36895;&#37319;&#29992;&#20026;&#39640;&#31561;&#25945;&#32946;&#24102;&#26469;&#20102;&#26032;&#30340;&#26426;&#36935;&#21644;&#25361;&#25112;&#12290;&#34429;&#28982;&#22823;&#37327;&#30340;&#25991;&#29486;&#35752;&#35770;&#20102;&#39640;&#31561;&#25945;&#32946;&#20013;&#30340;AI&#65292;&#20294;&#32570;&#20047;&#19968;&#20010;&#31995;&#32479;&#30340;&#26041;&#27861;&#26469;&#20840;&#38754;&#29702;&#35299;&#39640;&#31561;&#25945;&#32946;&#26426;&#26500;&#30340;AI&#36716;&#22411;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#26412;&#25991;&#37319;&#29992;&#22797;&#26434;&#31995;&#32479;&#30340;&#26041;&#27861;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#22240;&#26524;&#24490;&#29615;&#22270;&#65288;CLD&#65289;&#65292;&#20197;&#25551;&#36848;&#20856;&#22411;&#39640;&#31561;&#25945;&#32946;&#26426;&#26500;&#20013;AI&#36716;&#22411;&#30340;&#22240;&#26524;&#21453;&#39304;&#26426;&#21046;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#32771;&#34385;&#20102;&#25512;&#21160;AI&#36716;&#22411;&#30340;&#21147;&#37327;&#20197;&#21450;AI&#36716;&#22411;&#23545;&#20856;&#22411;&#39640;&#31561;&#25945;&#32946;&#26426;&#26500;&#20215;&#20540;&#21019;&#36896;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#35782;&#21035;&#21644;&#20998;&#26512;&#20102;&#20960;&#20010;&#22686;&#24378;&#21644;&#24179;&#34913;&#30340;&#21453;&#39304;&#24490;&#29615;&#65292;&#23637;&#31034;&#20102;&#39640;&#31561;&#25945;&#32946;&#26426;&#26500;&#22312;AI&#25216;&#26415;&#36827;&#27493;&#30340;&#25512;&#21160;&#19979;&#22914;&#20309;&#25237;&#36164;&#20110;&#25913;&#21892;&#23398;&#29983;&#23398;&#20064;&#12289;&#30740;&#31350;&#21644;&#31649;&#29702;&#12290;&#39640;&#31561;&#25945;&#32946;&#26426;&#26500;&#24517;&#39035;&#37319;&#21462;&#25514;&#26045;&#24212;&#23545;&#23398;&#26415;&#35802;&#20449;&#38382;&#39064;&#24182;&#36866;&#24212;&#21487;&#29992;&#25216;&#26415;&#30340;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial intelligence (AI) advances and the rapid adoption of generative AI tools like ChatGPT present new opportunities and challenges for higher education. While substantial literature discusses AI in higher education, there is a lack of a systemic approach that captures a holistic view of the AI transformation of higher education institutions (HEIs). To fill this gap, this article, taking a complex systems approach, develops a causal loop diagram (CLD) to map the causal feedback mechanisms of AI transformation in a typical HEI. Our model accounts for the forces that drive the AI transformation and the consequences of the AI transformation on value creation in a typical HEI. The article identifies and analyzes several reinforcing and balancing feedback loops, showing how, motivated by AI technology advances, the HEI invests in AI to improve student learning, research, and administration. The HEI must take measures to deal with academic integrity problems and adapt to changes in ava
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24191;&#20041;&#21270;&#30340;&#35268;&#21010;&#29615;&#22659;&#37325;&#26032;&#35774;&#35745;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#19981;&#21463;&#24230;&#37327;&#24433;&#21709;&#65292;&#24182;&#33021;&#22815;&#36866;&#24212;&#19981;&#21516;&#30340;&#30446;&#26631;&#21644;&#25351;&#26631;&#12290;</title><link>https://arxiv.org/abs/2402.07799</link><description>&lt;p&gt;
&#24191;&#20041;&#21270;&#35268;&#21010;&#29615;&#22659;&#37325;&#26032;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Generalising Planning Environment Redesign
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07799
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24191;&#20041;&#21270;&#30340;&#35268;&#21010;&#29615;&#22659;&#37325;&#26032;&#35774;&#35745;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#19981;&#21463;&#24230;&#37327;&#24433;&#21709;&#65292;&#24182;&#33021;&#22815;&#36866;&#24212;&#19981;&#21516;&#30340;&#30446;&#26631;&#21644;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29615;&#22659;&#35774;&#35745;&#20013;&#65292;&#19968;&#20010;&#24863;&#20852;&#36259;&#30340;&#26041;&#19982;&#21478;&#19968;&#20010;&#20195;&#29702;&#36890;&#36807;&#23545;&#29615;&#22659;&#36827;&#34892;&#25913;&#21464;&#26469;&#24433;&#21709;&#20854;&#20915;&#31574;&#12290;&#22823;&#37096;&#20998;&#20851;&#20110;&#35268;&#21010;&#29615;&#22659;&#37325;&#26032;&#35774;&#35745;&#30340;&#30740;&#31350;&#20551;&#35774;&#24863;&#20852;&#36259;&#30340;&#26041;&#30340;&#30446;&#26631;&#26159;&#20419;&#36827;&#30446;&#26631;&#21644;&#35745;&#21010;&#30340;&#35782;&#21035;&#65292;&#24182;&#22312;&#29615;&#22659;&#20462;&#25913;&#31354;&#38388;&#20013;&#25628;&#32034;&#20197;&#25214;&#21040;&#31616;&#21270;&#36825;&#20123;&#20219;&#21153;&#24182;&#20248;&#21270;&#29305;&#23450;&#24230;&#37327;&#26631;&#20934;&#30340;&#26368;&#23567;&#19968;&#32452;&#21464;&#21270;&#12290;&#36825;&#20010;&#25628;&#32034;&#31354;&#38388;&#36890;&#24120;&#38590;&#20197;&#22788;&#29702;&#65292;&#22240;&#27492;&#29616;&#26377;&#26041;&#27861;&#35774;&#35745;&#20102;&#24230;&#37327;&#30456;&#20851;&#30340;&#20462;&#21098;&#25216;&#26415;&#20197;&#26356;&#39640;&#25928;&#22320;&#36827;&#34892;&#25628;&#32034;&#12290;&#36825;&#23548;&#33268;&#26041;&#27861;&#26080;&#27861;&#22312;&#19981;&#21516;&#30340;&#30446;&#26631;&#21644;/&#25110;&#25351;&#26631;&#19978;&#36827;&#34892;&#27867;&#21270;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35748;&#20026;&#24863;&#20852;&#36259;&#30340;&#26041;&#30340;&#30446;&#26631;&#21644;&#25351;&#26631;&#19981;&#19968;&#23450;&#19982;&#35782;&#21035;&#20195;&#29702;&#30340;&#30446;&#26631;&#25110;&#35745;&#21010;&#30456;&#20851;&#12290;&#22240;&#27492;&#65292;&#20026;&#20102;&#24191;&#20041;&#21270;&#35268;&#21010;&#29615;&#22659;&#37325;&#26032;&#35774;&#35745;&#30340;&#20219;&#21153;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#29615;&#22659;&#37325;&#26032;&#35774;&#35745;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#19981;&#21463;&#24230;&#37327;&#24433;&#21709;&#65292;&#24182;&#21033;&#29992;&#20102;&#26368;&#26032;&#30340;&#39030;&#23618;&#30740;&#31350;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In Environment Design, one interested party seeks to affect another agent's decisions by applying changes to the environment. Most research on planning environment (re)design assumes the interested party's objective is to facilitate the recognition of goals and plans, and search over the space of environment modifications to find the minimal set of changes that simplify those tasks and optimise a particular metric. This search space is usually intractable, so existing approaches devise metric-dependent pruning techniques for performing search more efficiently. This results in approaches that are not able to generalise across different objectives and/or metrics. In this paper, we argue that the interested party could have objectives and metrics that are not necessarily related to recognising agents' goals or plans. Thus, to generalise the task of Planning Environment Redesign, we develop a general environment redesign approach that is metric-agnostic and leverages recent research on top
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#32479;&#19968;&#23545;&#40784;&#21407;&#21017; ($\mathbf{UA}^2$)&#65292;&#26088;&#22312;&#23454;&#29616;&#26234;&#33021;&#20307;&#19982;&#20154;&#31867;&#24847;&#22270;&#12289;&#29615;&#22659;&#21160;&#24577;&#21644;&#33258;&#25105;&#32422;&#26463;&#30340;&#32479;&#19968;&#23545;&#40784;&#65292;&#25552;&#20986;&#20102;&#24341;&#20837;&#23454;&#38469;&#29305;&#24615;&#36827;&#34892;&#27010;&#24565;&#39564;&#35777;&#30740;&#31350;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.07744</link><description>&lt;p&gt;
&#23454;&#29616;&#26234;&#33021;&#20307;&#12289;&#20154;&#31867;&#21644;&#29615;&#22659;&#20043;&#38388;&#30340;&#32479;&#19968;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Towards Unified Alignment Between Agents, Humans, and Environment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07744
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#32479;&#19968;&#23545;&#40784;&#21407;&#21017; ($\mathbf{UA}^2$)&#65292;&#26088;&#22312;&#23454;&#29616;&#26234;&#33021;&#20307;&#19982;&#20154;&#31867;&#24847;&#22270;&#12289;&#29615;&#22659;&#21160;&#24577;&#21644;&#33258;&#25105;&#32422;&#26463;&#30340;&#32479;&#19968;&#23545;&#40784;&#65292;&#25552;&#20986;&#20102;&#24341;&#20837;&#23454;&#38469;&#29305;&#24615;&#36827;&#34892;&#27010;&#24565;&#39564;&#35777;&#30740;&#31350;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#24555;&#36895;&#36827;&#23637;&#23548;&#33268;&#20102;&#33258;&#20027;&#26234;&#33021;&#20307;&#30340;&#32321;&#33635;&#65292;&#36825;&#20123;&#26234;&#33021;&#20307;&#21033;&#29992;&#22522;&#30784;&#27169;&#22411;&#30340;&#36890;&#29992;&#33021;&#21147;&#36827;&#34892;&#25512;&#29702;&#12289;&#20915;&#31574;&#21644;&#29615;&#22659;&#20132;&#20114;&#12290;&#28982;&#32780;&#65292;&#24403;&#22312;&#22797;&#26434;&#12289;&#29616;&#23454;&#30340;&#29615;&#22659;&#20013;&#36816;&#34892;&#26102;&#65292;&#26234;&#33021;&#20307;&#30340;&#25928;&#33021;&#20173;&#28982;&#26377;&#38480;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#32479;&#19968;&#23545;&#40784;&#21407;&#21017;&#65292;&#21363;&#21516;&#26102;&#23545;&#40784;&#26234;&#33021;&#20307;&#19982;&#20154;&#31867;&#24847;&#22270;&#12289;&#29615;&#22659;&#21160;&#24577;&#21644;&#33258;&#25105;&#32422;&#26463;&#65288;&#22914;&#36135;&#24065;&#39044;&#31639;&#38480;&#21046;&#65289;&#12290;&#20174;&#32479;&#19968;&#23545;&#40784; ($\mathbf{UA}^2$) &#30340;&#35270;&#35282;&#20986;&#21457;&#65292;&#25105;&#20204;&#22238;&#39038;&#20102;&#24403;&#21069;&#26234;&#33021;&#20307;&#30740;&#31350;&#30340;&#29616;&#29366;&#65292;&#24182;&#25351;&#20986;&#20102;&#29616;&#26377;&#26234;&#33021;&#20307;&#22522;&#20934;&#21644;&#26041;&#27861;&#20505;&#36873;&#20013;&#34987;&#24573;&#35270;&#30340;&#22240;&#32032;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#20026;WebShop&#24341;&#20837;&#23454;&#38469;&#29305;&#24615;&#36827;&#34892;&#20102;&#27010;&#24565;&#39564;&#35777;&#30740;&#31350;&#65292;&#21253;&#25324;&#20351;&#29992;&#29992;&#25143;&#37197;&#32622;&#25991;&#20214;&#26469;&#23637;&#31034;&#24847;&#22270;&#12289;&#20010;&#24615;&#21270;&#37325;&#26032;&#25490;&#21517;&#20197;&#24212;&#23545;&#22797;&#26434;&#30340;&#29615;&#22659;&#21160;&#24577;&#21644;&#36816;&#34892;&#26102;&#25104;&#26412;&#32479;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid progress of foundation models has led to the prosperity of autonomous agents, which leverage the universal capabilities of foundation models to conduct reasoning, decision-making, and environmental interaction. However, the efficacy of agents remains limited when operating in intricate, realistic environments. In this work, we introduce the principles of $\mathbf{U}$nified $\mathbf{A}$lignment for $\mathbf{A}$gents ($\mathbf{UA}^2$), which advocate for the simultaneous alignment of agents with human intentions, environmental dynamics, and self-constraints such as the limitation of monetary budgets. From the perspective of $\mathbf{UA}^2$, we review the current agent research and highlight the neglected factors in existing agent benchmarks and method candidates. We also conduct proof-of-concept studies by introducing realistic features to WebShop, including user profiles to demonstrate intentions, personalized reranking for complex environmental dynamics, and runtime cost stat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;&#22312;&#32447;&#39034;&#24207;&#20915;&#31574;&#20013;&#22788;&#29702;&#26410;&#30693;&#24310;&#36831;&#38382;&#39064;&#30340;&#19977;&#20010;&#24310;&#36831;&#31639;&#27861;&#26063;&#65292;&#24182;&#25552;&#20379;&#20102;&#30456;&#24212;&#30340;&#36951;&#25022;&#30028;&#38480;&#12290;</title><link>https://arxiv.org/abs/2402.07703</link><description>&lt;p&gt;
&#22312;&#32447;&#39034;&#24207;&#20915;&#31574;&#20013;&#30340;&#26410;&#30693;&#24310;&#36831;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Online Sequential Decision-Making with Unknown Delays
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07703
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;&#22312;&#32447;&#39034;&#24207;&#20915;&#31574;&#20013;&#22788;&#29702;&#26410;&#30693;&#24310;&#36831;&#38382;&#39064;&#30340;&#19977;&#20010;&#24310;&#36831;&#31639;&#27861;&#26063;&#65292;&#24182;&#25552;&#20379;&#20102;&#30456;&#24212;&#30340;&#36951;&#25022;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22312;&#32447;&#39034;&#24207;&#20915;&#31574;&#39046;&#22495;&#65292;&#25105;&#20204;&#21033;&#29992;&#22312;&#32447;&#20984;&#20248;&#21270;&#65288;OCO&#65289;&#26694;&#26550;&#35299;&#20915;&#20102;&#20855;&#26377;&#24310;&#36831;&#30340;&#38382;&#39064;&#65292;&#20854;&#20013;&#20915;&#31574;&#30340;&#21453;&#39304;&#21487;&#33021;&#20197;&#26410;&#30693;&#24310;&#36831;&#21040;&#36798;&#12290;&#19982;&#20043;&#21069;&#20165;&#38480;&#20110;&#27431;&#20960;&#37324;&#24471;&#33539;&#25968;&#21644;&#26799;&#24230;&#20449;&#24687;&#30340;&#30740;&#31350;&#19981;&#21516;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#20010;&#22522;&#20110;&#36817;&#20284;&#35299;&#30340;&#24310;&#36831;&#31639;&#27861;&#26063;&#65292;&#22788;&#29702;&#19981;&#21516;&#31867;&#22411;&#30340;&#25509;&#25910;&#21453;&#39304;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;&#26159;&#22810;&#21151;&#33021;&#19988;&#36866;&#29992;&#20110;&#36890;&#29992;&#33539;&#25968;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31995;&#21015;&#38024;&#23545;&#20855;&#26377;&#23436;&#25972;&#25439;&#22833;&#20989;&#25968;&#20449;&#24687;&#21453;&#39304;&#30340;&#24310;&#36831;&#35268;&#33539;&#21270;&#39046;&#23548;&#31639;&#27861;&#26063;&#65292;&#19968;&#31995;&#21015;&#38024;&#23545;&#20855;&#26377;&#26799;&#24230;&#20449;&#24687;&#21453;&#39304;&#30340;&#24310;&#36831;&#38236;&#20687;&#19979;&#38477;&#31639;&#27861;&#26063;&#65292;&#20197;&#21450;&#19968;&#31995;&#21015;&#38024;&#23545;&#30456;&#24212;&#20915;&#31574;&#28857;&#25439;&#22833;&#20989;&#25968;&#26799;&#24230;&#20540;&#20449;&#24687;&#21453;&#39304;&#30340;&#31616;&#21270;&#24310;&#36831;&#38236;&#20687;&#19979;&#38477;&#31639;&#27861;&#26063;&#12290;&#23545;&#20110;&#27599;&#31181;&#31867;&#22411;&#30340;&#31639;&#27861;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#30456;&#24212;&#30340;&#36951;&#25022;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the field of online sequential decision-making, we address the problem with delays utilizing the framework of online convex optimization (OCO), where the feedback of a decision can arrive with an unknown delay. Unlike previous research that is limited to Euclidean norm and gradient information, we propose three families of delayed algorithms based on approximate solutions to handle different types of received feedback. Our proposed algorithms are versatile and applicable to universal norms. Specifically, we introduce a family of Follow the Delayed Regularized Leader algorithms for feedback with full information on the loss function, a family of Delayed Mirror Descent algorithms for feedback with gradient information on the loss function and a family of Simplified Delayed Mirror Descent algorithms for feedback with the value information of the loss function's gradients at corresponding decision points. For each type of algorithm, we provide corresponding regret bounds under cases of 
&lt;/p&gt;</description></item><item><title>&#39135;&#29289;&#25512;&#33616;&#20316;&#20026;&#35821;&#35328;&#22788;&#29702;&#65288;F-RLP&#65289;&#26159;&#19968;&#20010;&#38024;&#23545;&#39135;&#29289;&#30340;&#20010;&#24615;&#21270;&#12289;&#24773;&#22659;&#21270;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#26469;&#23454;&#29616;&#26356;&#20934;&#30830;&#12289;&#20010;&#24615;&#21270;&#30340;&#39135;&#29289;&#25512;&#33616;&#12290;</title><link>https://arxiv.org/abs/2402.07477</link><description>&lt;p&gt;
&#39135;&#29289;&#25512;&#33616;&#20316;&#20026;&#35821;&#35328;&#22788;&#29702;&#65288;F-RLP&#65289;&#65306;&#20010;&#24615;&#21270;&#21644;&#24773;&#22659;&#21270;&#33539;&#24335;
&lt;/p&gt;
&lt;p&gt;
Food Recommendation as Language Processing (F-RLP): A Personalized and Contextual Paradigm
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07477
&lt;/p&gt;
&lt;p&gt;
&#39135;&#29289;&#25512;&#33616;&#20316;&#20026;&#35821;&#35328;&#22788;&#29702;&#65288;F-RLP&#65289;&#26159;&#19968;&#20010;&#38024;&#23545;&#39135;&#29289;&#30340;&#20010;&#24615;&#21270;&#12289;&#24773;&#22659;&#21270;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#26469;&#23454;&#29616;&#26356;&#20934;&#30830;&#12289;&#20010;&#24615;&#21270;&#30340;&#39135;&#29289;&#25512;&#33616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#35268;&#21017;&#21644;&#20998;&#31867;&#30340;&#39135;&#29289;&#25512;&#33616;&#31995;&#32479;&#22312;&#23454;&#38469;&#24212;&#29992;&#21644;&#23454;&#29992;&#24615;&#26041;&#38754;&#38754;&#20020;&#30528;&#37325;&#22823;&#25361;&#25112;&#12290;&#20027;&#35201;&#21407;&#22240;&#26159;&#22823;&#22810;&#25968;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#19968;&#20010;&#20960;&#20046;&#26080;&#38480;&#25968;&#37327;&#30340;&#31867;&#21035;&#21644;&#19968;&#20010;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#20013;&#30340;&#26377;&#38480;&#26679;&#26412;&#38382;&#39064;&#19978;&#24456;&#38590;&#24212;&#23545;&#12290;&#30456;&#21453;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20316;&#20026;&#25512;&#33616;&#24341;&#25806;&#30340;&#20986;&#29616;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#36884;&#24452;&#12290;&#28982;&#32780;&#65292;&#19968;&#20010;&#36890;&#29992;&#30340;&#8220;&#20316;&#20026;&#35821;&#35328;&#22788;&#29702;&#30340;&#25512;&#33616;&#8221;&#65288;RLP&#65289;&#26041;&#27861;&#32570;&#20047;&#26377;&#25928;&#30340;&#39135;&#29289;&#25512;&#33616;&#25152;&#24517;&#38656;&#30340;&#20851;&#38190;&#32452;&#20214;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#39135;&#29289;&#25512;&#33616;&#20316;&#20026;&#35821;&#35328;&#22788;&#29702;&#65288;F-RLP&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#38024;&#23545;&#39135;&#29289;&#30340;&#23450;&#21046;&#22522;&#30784;&#35774;&#26045;&#12290;F-RLP&#21033;&#29992;LLMs&#30340;&#33021;&#21147;&#26469;&#26368;&#22823;&#21270;&#20854;&#28508;&#21147;&#65292;&#20174;&#32780;&#20026;&#26356;&#20934;&#30830;&#12289;&#20010;&#24615;&#21270;&#30340;&#39135;&#29289;&#25512;&#33616;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
State-of-the-art rule-based and classification-based food recommendation systems face significant challenges in becoming practical and useful. This difficulty arises primarily because most machine learning models struggle with problems characterized by an almost infinite number of classes and a limited number of samples within an unbalanced dataset. Conversely, the emergence of Large Language Models (LLMs) as recommendation engines offers a promising avenue. However, a general-purpose Recommendation as Language Processing (RLP) approach lacks the critical components necessary for effective food recommendations. To address this gap, we introduce Food Recommendation as Language Processing (F-RLP), a novel framework that offers a food-specific, tailored infrastructure. F-RLP leverages the capabilities of LLMs to maximize their potential, thereby paving the way for more accurate, personalized food recommendations.
&lt;/p&gt;</description></item><item><title>Premier-TACO&#26159;&#19968;&#31181;&#22810;&#20219;&#21153;&#29305;&#24449;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#36890;&#29992;&#29305;&#24449;&#34920;&#31034;&#65292;&#24182;&#24341;&#20837;&#36127;&#20363;&#25277;&#26679;&#31574;&#30053;&#26469;&#25552;&#39640;&#26102;&#24207;&#34892;&#21160;&#23545;&#27604;&#23398;&#20064;&#30340;&#35745;&#31639;&#25928;&#29575;&#65292;&#20174;&#32780;&#26174;&#33879;&#22686;&#24378;&#20102;&#23545;&#26032;&#39062;&#21160;&#20316;&#30340;&#23569;&#26679;&#26412;&#27169;&#20223;&#23398;&#20064;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.06187</link><description>&lt;p&gt;
Premier-TACO: &#36890;&#36807;&#26102;&#38388;&#39537;&#21160;&#30340;&#23545;&#27604;&#25439;&#22833;&#36827;&#34892;&#22810;&#20219;&#21153;&#34920;&#31034;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Premier-TACO: Pretraining Multitask Representation via Temporal Action-Driven Contrastive Loss
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06187
&lt;/p&gt;
&lt;p&gt;
Premier-TACO&#26159;&#19968;&#31181;&#22810;&#20219;&#21153;&#29305;&#24449;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#36890;&#29992;&#29305;&#24449;&#34920;&#31034;&#65292;&#24182;&#24341;&#20837;&#36127;&#20363;&#25277;&#26679;&#31574;&#30053;&#26469;&#25552;&#39640;&#26102;&#24207;&#34892;&#21160;&#23545;&#27604;&#23398;&#20064;&#30340;&#35745;&#31639;&#25928;&#29575;&#65292;&#20174;&#32780;&#26174;&#33879;&#22686;&#24378;&#20102;&#23545;&#26032;&#39062;&#21160;&#20316;&#30340;&#23569;&#26679;&#26412;&#27169;&#20223;&#23398;&#20064;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;Premier-TACO&#65292;&#36825;&#26159;&#19968;&#31181;&#22810;&#20219;&#21153;&#29305;&#24449;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#26088;&#22312;&#25552;&#39640;&#39034;&#24207;&#20915;&#31574;&#20219;&#21153;&#20013;&#23569;&#26679;&#26412;&#31574;&#30053;&#23398;&#20064;&#30340;&#25928;&#29575;&#12290;Premier-TACO&#21033;&#29992;&#19968;&#37096;&#20998;&#22810;&#20219;&#21153;&#31163;&#32447;&#25968;&#25454;&#38598;&#36827;&#34892;&#39044;&#35757;&#32451;&#36890;&#29992;&#29305;&#24449;&#34920;&#31034;&#65292;&#35813;&#29305;&#24449;&#34920;&#31034;&#25429;&#25417;&#20102;&#20851;&#38190;&#30340;&#29615;&#22659;&#21160;&#21147;&#23398;&#65292;&#24182;&#20351;&#29992;&#26368;&#23569;&#30340;&#19987;&#23478;&#28436;&#31034;&#36827;&#34892;&#24494;&#35843;&#12290;&#23427;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#26032;&#30340;&#36127;&#20363;&#25277;&#26679;&#31574;&#30053;&#25512;&#21160;&#20102;&#26102;&#24207;&#34892;&#21160;&#23545;&#27604;&#23398;&#20064;&#65288;TACO&#65289;&#30446;&#26631;&#30340;&#21457;&#23637;&#65292;TACO&#22312;&#35270;&#35273;&#25511;&#21046;&#20219;&#21153;&#20013;&#20855;&#26377;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#36825;&#31181;&#31574;&#30053;&#22312;&#26174;&#33879;&#25552;&#39640;TACO&#30340;&#35745;&#31639;&#25928;&#29575;&#26041;&#38754;&#38750;&#24120;&#37325;&#35201;&#65292;&#20351;&#22823;&#35268;&#27169;&#22810;&#20219;&#21153;&#31163;&#32447;&#39044;&#35757;&#32451;&#25104;&#20026;&#21487;&#33021;&#12290;&#25105;&#20204;&#22312;&#21253;&#25324;Deepmind Control Suite&#12289;MetaWorld&#21644;LIBERO&#22312;&#20869;&#30340;&#21508;&#31181;&#36830;&#32493;&#25511;&#21046;&#22522;&#20934;&#27979;&#35797;&#20013;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#35777;&#35780;&#20272;&#65292;&#35777;&#26126;&#20102;Premier-TACO&#22312;&#39044;&#35757;&#32451;&#35270;&#35273;&#34920;&#31034;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#26174;&#33879;&#22686;&#24378;&#20102;&#23545;&#26032;&#39062;&#21160;&#20316;&#30340;&#23569;&#26679;&#26412;&#27169;&#20223;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Premier-TACO, a multitask feature representation learning approach designed to improve few-shot policy learning efficiency in sequential decision-making tasks. Premier-TACO leverages a subset of multitask offline datasets for pretraining a general feature representation, which captures critical environmental dynamics and is fine-tuned using minimal expert demonstrations. It advances the temporal action contrastive learning (TACO) objective, known for state-of-the-art results in visual control tasks, by incorporating a novel negative example sampling strategy. This strategy is crucial in significantly boosting TACO's computational efficiency, making large-scale multitask offline pretraining feasible. Our extensive empirical evaluation in a diverse set of continuous control benchmarks including Deepmind Control Suite, MetaWorld, and LIBERO demonstrate Premier-TACO's effectiveness in pretraining visual representations, significantly enhancing few-shot imitation learning of nove
&lt;/p&gt;</description></item><item><title>OpenToM&#26159;&#19968;&#20010;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24515;&#29702;&#29702;&#35299;&#33021;&#21147;&#30340;&#20840;&#38754;&#22522;&#20934;&#65292;&#36890;&#36807;&#25552;&#20379;&#26356;&#38271;&#12289;&#26356;&#28165;&#26224;&#30340;&#21465;&#20107;&#25925;&#20107;&#12289;&#20855;&#26377;&#26126;&#30830;&#20010;&#24615;&#29305;&#24449;&#30340;&#35282;&#33394;&#12289;&#20197;&#21450;&#25361;&#25112;&#27169;&#22411;&#23545;&#24515;&#29702;&#29366;&#24577;&#30340;&#29702;&#35299;&#33021;&#21147;&#30340;&#38382;&#39064;&#65292;&#25581;&#31034;&#20102;&#29616;&#26377;&#27169;&#22411;&#22312;&#29702;&#35299;&#29289;&#29702;&#19990;&#30028;&#19982;&#24515;&#29702;&#19990;&#30028;&#30340;&#35282;&#33394;&#24515;&#29702;&#29366;&#24577;&#26041;&#38754;&#30340;&#20248;&#21183;&#21644;&#19981;&#36275;&#12290;</title><link>https://arxiv.org/abs/2402.06044</link><description>&lt;p&gt;
&#24320;&#25918;&#29702;&#35770;-&#24515;&#28789;&#65288;OpenToM&#65289;&#65306;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24515;&#28789;&#29702;&#35299;&#33021;&#21147;&#30340;&#20840;&#38754;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
OpenToM: A Comprehensive Benchmark for Evaluating Theory-of-Mind Reasoning Capabilities of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06044
&lt;/p&gt;
&lt;p&gt;
OpenToM&#26159;&#19968;&#20010;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24515;&#29702;&#29702;&#35299;&#33021;&#21147;&#30340;&#20840;&#38754;&#22522;&#20934;&#65292;&#36890;&#36807;&#25552;&#20379;&#26356;&#38271;&#12289;&#26356;&#28165;&#26224;&#30340;&#21465;&#20107;&#25925;&#20107;&#12289;&#20855;&#26377;&#26126;&#30830;&#20010;&#24615;&#29305;&#24449;&#30340;&#35282;&#33394;&#12289;&#20197;&#21450;&#25361;&#25112;&#27169;&#22411;&#23545;&#24515;&#29702;&#29366;&#24577;&#30340;&#29702;&#35299;&#33021;&#21147;&#30340;&#38382;&#39064;&#65292;&#25581;&#31034;&#20102;&#29616;&#26377;&#27169;&#22411;&#22312;&#29702;&#35299;&#29289;&#29702;&#19990;&#30028;&#19982;&#24515;&#29702;&#19990;&#30028;&#30340;&#35282;&#33394;&#24515;&#29702;&#29366;&#24577;&#26041;&#38754;&#30340;&#20248;&#21183;&#21644;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#24515;&#29702;&#29702;&#35770;&#65288;N-ToM&#65289;&#26159;&#26426;&#22120;&#29702;&#35299;&#21644;&#36319;&#36394;&#20182;&#20154;&#24515;&#29702;&#29366;&#24577;&#30340;&#33021;&#21147;&#65292;&#22312;&#24320;&#21457;&#20855;&#26377;&#31038;&#20132;&#26234;&#33021;&#30340;&#20195;&#29702;&#31243;&#24207;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;N-ToM&#22522;&#20934;&#23384;&#22312;&#19968;&#20123;&#38382;&#39064;&#65292;&#21253;&#25324;&#27169;&#31946;&#21644;&#20154;&#24037;&#25925;&#20107;&#30340;&#23384;&#22312;&#65292;&#32570;&#20047;&#20010;&#24615;&#29305;&#24449;&#21644;&#20559;&#22909;&#65292;&#32570;&#20047;&#28041;&#21450;&#35282;&#33394;&#24515;&#29702;&#24515;&#24577;&#30340;&#38382;&#39064;&#65292;&#24182;&#19988;&#25552;&#20986;&#30340;&#38382;&#39064;&#22810;&#26679;&#24615;&#26377;&#38480;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;OpenToM&#65292;&#19968;&#20010;&#26032;&#30340;&#35780;&#20272;N-ToM&#30340;&#22522;&#20934;&#65292;&#20197; (1) &#26356;&#38271;&#12289;&#26356;&#28165;&#26224;&#30340;&#21465;&#20107;&#25925;&#20107;&#65292;(2) &#20855;&#26377;&#26126;&#30830;&#20010;&#24615;&#29305;&#24449;&#30340;&#35282;&#33394;&#65292;(3) &#35302;&#21457;&#35282;&#33394;&#24847;&#22270;&#30340;&#34892;&#21160;&#65292;&#20197;&#21450; (4) &#35774;&#35745;&#26088;&#22312;&#25361;&#25112;LLMs&#23545;&#24314;&#27169;&#35282;&#33394;&#22312;&#29289;&#29702;&#21644;&#24515;&#29702;&#19990;&#30028;&#30340;&#24515;&#29702;&#29366;&#24577;&#33021;&#21147;&#30340;&#38382;&#39064;&#12290;&#20351;&#29992;OpenToM&#65292;&#25105;&#20204;&#21457;&#29616;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;LLMs&#22312;&#24314;&#27169;&#29289;&#29702;&#19990;&#30028;&#30340;&#19968;&#20123;&#24515;&#29702;&#29366;&#24577;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#36319;&#36394;&#35282;&#33394;&#24515;&#29702;&#29366;&#24577;&#26041;&#38754;&#23384;&#22312;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural Theory-of-Mind (N-ToM), machine's ability to understand and keep track of the mental states of others, is pivotal in developing socially intelligent agents. However, prevalent N-ToM benchmarks have several shortcomings, including the presence of ambiguous and artificial narratives, absence of personality traits and preferences, a lack of questions addressing characters' psychological mental states, and limited diversity in the questions posed. In response to these issues, we construct OpenToM, a new benchmark for assessing N-ToM with (1) longer and clearer narrative stories, (2) characters with explicit personality traits, (3) actions that are triggered by character intentions, and (4) questions designed to challenge LLMs' capabilities of modeling characters' mental states of both the physical and psychological world. Using OpenToM, we reveal that state-of-the-art LLMs thrive at modeling certain aspects of mental states in the physical world but fall short when tracking characte
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#24341;&#20837;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#25216;&#26415;&#21644;&#21033;&#29992;&#36801;&#31227;&#23398;&#20064;&#26469;&#22788;&#29702;&#38271;&#25991;&#26412;&#21644;&#25552;&#21319;&#25512;&#29702;&#33021;&#21147;&#65292;&#20026;&#25945;&#31185;&#20070;&#38382;&#31572;&#20219;&#21153;&#24102;&#26469;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.05128</link><description>&lt;p&gt;
&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#25552;&#21319;&#25945;&#31185;&#20070;&#38382;&#31572;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
Enhancing Textbook Question Answering Task with Large Language Models and Retrieval Augmented Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05128
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#24341;&#20837;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#25216;&#26415;&#21644;&#21033;&#29992;&#36801;&#31227;&#23398;&#20064;&#26469;&#22788;&#29702;&#38271;&#25991;&#26412;&#21644;&#25552;&#21319;&#25512;&#29702;&#33021;&#21147;&#65292;&#20026;&#25945;&#31185;&#20070;&#38382;&#31572;&#20219;&#21153;&#24102;&#26469;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25945;&#31185;&#20070;&#38382;&#31572;&#65288;TQA&#65289;&#26159;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#30001;&#20110;&#19978;&#19979;&#25991;&#21644;&#22810;&#27169;&#24335;&#25968;&#25454;&#30340;&#22797;&#26434;&#24615;&#12290;&#23613;&#31649;&#20197;&#21069;&#30340;&#30740;&#31350;&#22312;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#20294;&#20173;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#65292;&#21253;&#25324;&#27169;&#22411;&#25512;&#29702;&#33021;&#21147;&#19981;&#36275;&#21644;&#26080;&#27861;&#25429;&#25417;&#38271;&#25991;&#26412;&#20013;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24341;&#20837;&#38761;&#21629;&#20102;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#65292;&#28982;&#32780;&#65292;&#30452;&#25509;&#24212;&#29992;LLMs&#32463;&#24120;&#20250;&#23548;&#33268;&#19981;&#20934;&#30830;&#30340;&#31572;&#26696;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#22788;&#29702;TQA&#20013;&#39046;&#22495;&#22806;&#24773;&#26223;&#65292;&#21363;&#27010;&#24565;&#20998;&#24067;&#22312;&#19981;&#21516;&#35838;&#31243;&#20013;&#65292;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#25216;&#26415;&#21644;&#36801;&#31227;&#23398;&#20064;&#26469;&#22788;&#29702;&#38271;&#25991;&#26412;&#24182;&#25552;&#21319;&#25512;&#29702;&#33021;&#21147;&#12290;&#36890;&#36807;&#23545;LLM&#27169;&#22411;Llama-2&#36827;&#34892;&#30417;&#30563;&#24494;&#35843;&#24182;&#21152;&#20837;RAG&#65292;&#25105;&#20204;&#30340;&#26550;&#26500;&#20248;&#20110;&#22522;&#32447;&#65292;&#22312;&#39564;&#35777;&#38598;&#19978;&#30340;&#20934;&#30830;&#24230;&#25552;&#39640;&#20102;4.12%&#65292;&#22312;&#27979;&#35797;&#38598;&#19978;&#25552;&#39640;&#20102;9.84%&#12290;
&lt;/p&gt;
&lt;p&gt;
Textbook question answering (TQA) is a challenging task in artificial intelligence due to the complex nature of context and multimodal data. Although previous research has significantly improved the task, there are still some limitations including the models' weak reasoning and inability to capture contextual information in the lengthy context. The introduction of large language models (LLMs) has revolutionized the field of AI, however, directly applying LLMs often leads to inaccurate answers. This paper proposes a methodology that handle the out-of-domain scenario in TQA where concepts are spread across different lessons by incorporating the retrieval augmented generation (RAG) technique and utilize transfer learning to handle the long context and enhance reasoning abilities. Through supervised fine-tuning of the LLM model Llama-2 and the incorporation of RAG, our architecture outperforms the baseline, achieving a 4.12% accuracy improvement on validation set and 9.84% on test set for 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#35780;&#20998;&#26694;&#26550;&#65292;&#29992;&#20110;&#34913;&#37327;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#23450;&#20041;&#22312;&#30417;&#31649;&#26041;&#38754;&#30340;&#21512;&#36866;&#24615;&#12290;&#30740;&#31350;&#26088;&#22312;&#25490;&#38500;&#36890;&#36807;AI&#30417;&#31649;&#25552;&#26696;&#25152;&#37319;&#29992;&#30340;&#19981;&#24688;&#24403;&#30340;AI&#23450;&#20041;&#23545;ICT&#25216;&#26415;&#12289;&#26041;&#27861;&#21644;&#38750;AI&#20316;&#21697;&#30340;&#24433;&#21709;&#65292;&#24182;&#22238;&#24402;&#21040;&#20197;&#21069;&#22312;&#20854;&#20182;&#25104;&#21151;&#25216;&#26415;&#30417;&#31649;&#20013;&#35266;&#23519;&#21040;&#30340;&#21407;&#21017;&#12290;</title><link>https://arxiv.org/abs/2402.05048</link><description>&lt;p&gt;
&#20320;&#30340;AI&#26377;&#22810;&#20040;VADER&#65311;&#38754;&#21521;&#36866;&#29992;&#20110;&#30417;&#31649;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#23450;&#20041;&#30340;&#25506;&#35752;
&lt;/p&gt;
&lt;p&gt;
How VADER is your AI? Towards a definition of artificial intelligence systems appropriate for regulation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05048
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#35780;&#20998;&#26694;&#26550;&#65292;&#29992;&#20110;&#34913;&#37327;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#23450;&#20041;&#22312;&#30417;&#31649;&#26041;&#38754;&#30340;&#21512;&#36866;&#24615;&#12290;&#30740;&#31350;&#26088;&#22312;&#25490;&#38500;&#36890;&#36807;AI&#30417;&#31649;&#25552;&#26696;&#25152;&#37319;&#29992;&#30340;&#19981;&#24688;&#24403;&#30340;AI&#23450;&#20041;&#23545;ICT&#25216;&#26415;&#12289;&#26041;&#27861;&#21644;&#38750;AI&#20316;&#21697;&#30340;&#24433;&#21709;&#65292;&#24182;&#22238;&#24402;&#21040;&#20197;&#21069;&#22312;&#20854;&#20182;&#25104;&#21151;&#25216;&#26415;&#30417;&#31649;&#20013;&#35266;&#23519;&#21040;&#30340;&#21407;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#25512;&#21160;&#20102;&#35768;&#22810;&#20449;&#24687;&#21644;&#36890;&#20449;&#25216;&#26415;&#65288;ICT&#65289;&#31361;&#30772;&#12290;&#28982;&#32780;&#65292;&#33258;&#22270;&#28789;&#27979;&#35797;&#25552;&#35758;&#20197;&#26469;&#65292;ICT&#31995;&#32479;&#30340;&#33539;&#22260;&#24050;&#36828;&#36828;&#36229;&#20986;&#20102;AI&#12290;&#20851;&#38190;&#26159;&#65292;&#26368;&#36817;&#30340;AI&#30417;&#31649;&#25552;&#26696;&#37319;&#29992;&#20102;&#24433;&#21709;ICT&#25216;&#26415;&#12289;&#26041;&#27861;&#21644;&#31995;&#32479;&#30340;AI&#23450;&#20041;&#65292;&#32780;&#36825;&#20123;&#24182;&#19981;&#26159;AI&#12290;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#29978;&#33267;&#21253;&#25324;&#25968;&#23398;&#12289;&#32479;&#35745;&#21644;&#24037;&#31243;&#39046;&#22495;&#30340;&#20316;&#21697;&#20063;&#20250;&#21463;&#21040;&#24433;&#21709;&#12290;&#20196;&#20154;&#25285;&#24551;&#30340;&#26159;&#65292;&#20174;&#35199;&#26041;&#31038;&#20250;&#21040;&#20840;&#29699;&#21335;&#26041;&#65292;&#37117;&#21457;&#29616;&#20102;AI&#23450;&#20041;&#19978;&#30340;&#38169;&#35823;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#35780;&#20998;&#26694;&#26550;&#65292;&#29992;&#20110;&#34913;&#37327;AI&#23450;&#20041;&#22312;&#30417;&#31649;&#26041;&#38754;&#30340;&#21512;&#36866;&#24615;&#12290;&#25105;&#20204;&#30340;&#22312;&#32447;&#12289;&#20844;&#24320;&#21487;&#29992;&#30340;VADER&#26694;&#26550;&#35780;&#20998;&#20102;&#24212;&#35813;&#20316;&#20026;AI&#23450;&#20041;&#30340;&#21069;&#25552;&#30340;&#35206;&#30422;&#33539;&#22260;&#65292;&#36825;&#20123;&#23450;&#20041;&#26088;&#22312;&#65288;i&#65289;&#37325;&#29616;&#20854;&#20182;&#25104;&#21151;&#25216;&#26415;&#30417;&#31649;&#20013;&#35266;&#23519;&#21040;&#30340;&#21407;&#21017;&#65292;&#20197;&#21450;&#65288;ii&#65289;&#21253;&#25324;&#25152;&#26377;&#30340;AI&#25216;&#26415;&#21644;&#26041;&#27861;&#65292;&#21516;&#26102;&#25490;&#38500;&#38750;AI&#30340;&#20316;&#21697;&#12290;&#20851;&#20110;&#21518;&#32773;&#65292;&#25105;&#20204;&#30340;&#35780;&#20998;&#22522;&#20110;&#19968;&#31181;...
&lt;/p&gt;
&lt;p&gt;
Artificial intelligence (AI) has driven many information and communication technology (ICT) breakthroughs. Nonetheless, the scope of ICT systems has expanded far beyond AI since the Turing test proposal. Critically, recent AI regulation proposals adopt AI definitions affecting ICT techniques, approaches, and systems that are not AI. In some cases, even works from mathematics, statistics, and engineering would be affected. Worryingly, AI misdefinitions are observed from Western societies to the Global South. In this paper, we propose a framework to score how \textit{validated as appropriately-defined for regulation} (VADER) an AI definition is. Our online, publicly-available VADER framework scores the coverage of premises that should underlie AI definitions for regulation, which aim to (i) reproduce principles observed in other successful technology regulations, and (ii) include all AI techniques and approaches while excluding non-AI works. Regarding the latter, our score is based on a 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;PaDeLLM-NER&#65292;&#19968;&#31181;&#33021;&#22815;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23454;&#29616;&#24182;&#34892;&#35299;&#30721;&#65292;&#20174;&#32780;&#26174;&#33879;&#20943;&#23569;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#30340;&#29983;&#25104;&#24310;&#36831;&#65292;&#21516;&#26102;&#20445;&#25345;&#39044;&#27979;&#36136;&#37327;&#21644;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.04838</link><description>&lt;p&gt;
PaDeLLM-NER&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24182;&#34892;&#35299;&#30721;&#29992;&#20110;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
PaDeLLM-NER: Parallel Decoding in Large Language Models for Named Entity Recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04838
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;PaDeLLM-NER&#65292;&#19968;&#31181;&#33021;&#22815;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23454;&#29616;&#24182;&#34892;&#35299;&#30721;&#65292;&#20174;&#32780;&#26174;&#33879;&#20943;&#23569;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#30340;&#29983;&#25104;&#24310;&#36831;&#65292;&#21516;&#26102;&#20445;&#25345;&#39044;&#27979;&#36136;&#37327;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20943;&#23569;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#30340;&#29983;&#25104;&#24310;&#36831;&#12290;LLMs&#30340;&#39640;&#24310;&#36831;&#30340;&#20027;&#35201;&#21407;&#22240;&#26159;&#39034;&#24207;&#35299;&#30721;&#36807;&#31243;&#65292;&#35813;&#36807;&#31243;&#33258;&#22238;&#24402;&#22320;&#29983;&#25104;NER&#30340;&#25152;&#26377;&#26631;&#31614;&#21644;&#25552;&#21450;&#65292;&#26174;&#33879;&#22686;&#21152;&#20102;&#24207;&#21015;&#38271;&#24230;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;PaDeLLM-NER&#65288;Parallel Decoding in LLM for NE&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26080;&#38656;&#39069;&#22806;&#27169;&#22359;&#25110;&#26550;&#26500;&#20462;&#25913;&#21363;&#21487;&#26080;&#32541;&#38598;&#25104;&#21040;&#29616;&#26377;&#29983;&#25104;&#27169;&#22411;&#26694;&#26550;&#20013;&#30340;&#26041;&#27861;&#12290;PaDeLLM-NER&#20801;&#35768;&#21516;&#26102;&#35299;&#30721;&#25152;&#26377;&#25552;&#21450;&#65292;&#20174;&#32780;&#20943;&#23569;&#29983;&#25104;&#24310;&#36831;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;PaDeLLM-NER&#30340;&#25512;&#29702;&#36895;&#24230;&#26174;&#33879;&#25552;&#39640;&#65292;&#23545;&#33521;&#35821;&#21644;&#20013;&#25991;&#26469;&#35828;&#27604;&#33258;&#22238;&#24402;&#26041;&#27861;&#24555;1.76&#21040;10.22&#20493;&#12290;&#19982;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#30456;&#23218;&#32654;&#65292;&#21516;&#26102;&#32500;&#25345;&#20102;&#39044;&#27979;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we aim to reduce generation latency for Named Entity Recognition (NER) with Large Language Models (LLMs). The main cause of high latency in LLMs is the sequential decoding process, which autoregressively generates all labels and mentions for NER, significantly increase the sequence length. To this end, we introduce Parallel Decoding in LLM for NE} (PaDeLLM-NER), a approach that integrates seamlessly into existing generative model frameworks without necessitating additional modules or architectural modifications. PaDeLLM-NER allows for the simultaneous decoding of all mentions, thereby reducing generation latency. Experiments reveal that PaDeLLM-NER significantly increases inference speed that is 1.76 to 10.22 times faster than the autoregressive approach for both English and Chinese. Simultaneously it maintains the quality of predictions as evidenced by the performance that is on par with the state-of-the-art across various datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#27169;&#24577;&#26694;&#26550;MolTC&#65292;&#29992;&#20110;&#20998;&#23376;&#30456;&#20114;&#20316;&#29992;&#39044;&#27979;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#39640;&#25928;&#22320;&#25972;&#21512;&#20998;&#23376;&#23545;&#30340;&#20016;&#23500;&#22270;&#24418;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#24605;&#32500;&#38142;&#29702;&#35770;&#23454;&#29616;&#32479;&#19968;&#30340;&#20998;&#23376;&#20851;&#31995;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2402.03781</link><description>&lt;p&gt;
MolTC: &#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#36827;&#34892;&#20998;&#23376;&#20851;&#31995;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
MolTC: Towards Molecular Relational Modeling In Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03781
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#27169;&#24577;&#26694;&#26550;MolTC&#65292;&#29992;&#20110;&#20998;&#23376;&#30456;&#20114;&#20316;&#29992;&#39044;&#27979;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#39640;&#25928;&#22320;&#25972;&#21512;&#20998;&#23376;&#23545;&#30340;&#20016;&#23500;&#22270;&#24418;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#24605;&#32500;&#38142;&#29702;&#35770;&#23454;&#29616;&#32479;&#19968;&#30340;&#20998;&#23376;&#20851;&#31995;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#23376;&#20851;&#31995;&#23398;&#20064;&#65288;MRL&#65289;&#26088;&#22312;&#29702;&#35299;&#20998;&#23376;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#22312;&#25512;&#36827;&#29983;&#29289;&#21270;&#23398;&#30740;&#31350;&#26041;&#38754;&#36215;&#21040;&#20102;&#20851;&#38190;&#20316;&#29992;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#37319;&#29992;&#24050;&#25104;&#20026;&#19968;&#31181;&#26377;&#25928;&#21644;&#39640;&#25928;&#30340;MRL&#26041;&#27861;&#65292;&#36825;&#20123;&#27169;&#22411;&#20197;&#20854;&#24222;&#22823;&#30340;&#30693;&#35782;&#23384;&#20648;&#24211;&#21644;&#20808;&#36827;&#30340;&#36923;&#36753;&#25512;&#29702;&#33021;&#21147;&#32780;&#38395;&#21517;&#12290;&#23613;&#31649;&#20855;&#26377;&#28508;&#21147;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#25991;&#26412;&#25968;&#25454;&#65292;&#22240;&#27492;&#27809;&#26377;&#20805;&#20998;&#21033;&#29992;&#20998;&#23376;&#22270;&#20013;&#22266;&#26377;&#30340;&#20016;&#23500;&#32467;&#26500;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#32570;&#20047;&#32479;&#19968;&#30340;&#26694;&#26550;&#21152;&#21095;&#20102;&#20449;&#24687;&#30340;&#28010;&#36153;&#65292;&#22240;&#20026;&#23427;&#38459;&#30861;&#20102;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#20043;&#38388;&#20849;&#20139;&#23398;&#20064;&#21040;&#30340;&#30456;&#20114;&#20316;&#29992;&#29702;&#30001;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#22810;&#27169;&#24577;&#26694;&#26550;&#65292;&#29992;&#20110;&#26681;&#25454;&#24605;&#32500;&#38142;&#65288;CoT&#65289;&#29702;&#35770;&#23545;&#20998;&#23376;&#30456;&#20114;&#20316;&#29992;&#36827;&#34892;&#39044;&#27979;&#65292;&#31216;&#20026;MolTC&#65292;&#23427;&#21487;&#20197;&#39640;&#25928;&#22320;&#25972;&#21512;&#20998;&#23376;&#23545;&#30340;&#20016;&#23500;&#22270;&#24418;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Molecular Relational Learning (MRL), aiming to understand interactions between molecular pairs, plays a pivotal role in advancing biochemical research. Recently, the adoption of large language models (LLMs), known for their vast knowledge repositories and advanced logical inference capabilities, has emerged as a promising way for efficient and effective MRL. Despite their potential, these methods predominantly rely on the textual data, thus not fully harnessing the wealth of structural information inherent in molecular graphs. Moreover, the absence of a unified framework exacerbates the information underutilization, as it hinders the sharing of interaction rationale learned across diverse datasets. To address these challenges, this work proposes a novel LLM-based multi-modal framework for Molecular inTeraction prediction following Chain-of-Thought (CoT) theory, termed MolTC, which can efficiently integrate rich graphical information of molecular pairs. For achieving a unified MRL, MolT
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;10&#20010;&#22269;&#23478;&#36229;&#36807;16,000&#21517;&#21463;&#35775;&#32773;&#23545;&#38750;&#33258;&#24895;&#21512;&#25104;&#20146;&#23494;&#22270;&#20687;&#30340;&#24577;&#24230;&#21644;&#34892;&#20026;&#65307;&#23613;&#31649;&#31038;&#20250;&#23545;&#27492;&#20173;&#35748;&#35782;&#19981;&#36275;&#65292;&#20294;&#36825;&#31181;&#34892;&#20026;&#34987;&#35748;&#20026;&#26377;&#23475;&#65307;&#32422;&#26377;2.2%&#30340;&#21463;&#35775;&#32773;&#34920;&#31034;&#26366;&#21463;&#23475;&#65292;1.8%&#30340;&#21463;&#35775;&#32773;&#34920;&#31034;&#26366;&#21442;&#19982;&#36807;&#27492;&#31867;&#34892;&#20026;&#65307;&#21333;&#38752;&#31435;&#27861;&#34892;&#21160;&#24182;&#19981;&#36275;&#20197;&#35299;&#20915;&#38382;&#39064;&#65292;&#24314;&#35758;&#25216;&#26415;&#21644;&#24179;&#21488;&#25919;&#31574;&#30340;&#25913;&#36827;&#26469;&#20943;&#23569;&#20260;&#23475;&#12290;</title><link>https://arxiv.org/abs/2402.01721</link><description>&lt;p&gt;
&#22312;10&#20010;&#22269;&#23478;&#30340;&#38750;&#33258;&#24895;&#21512;&#25104;&#20146;&#23494;&#22270;&#20687;&#30340;&#24577;&#24230;&#21644;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
Attitudes Towards and Knowledge of Non-Consensual Synthetic Intimate Imagery in 10 Countries
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01721
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;10&#20010;&#22269;&#23478;&#36229;&#36807;16,000&#21517;&#21463;&#35775;&#32773;&#23545;&#38750;&#33258;&#24895;&#21512;&#25104;&#20146;&#23494;&#22270;&#20687;&#30340;&#24577;&#24230;&#21644;&#34892;&#20026;&#65307;&#23613;&#31649;&#31038;&#20250;&#23545;&#27492;&#20173;&#35748;&#35782;&#19981;&#36275;&#65292;&#20294;&#36825;&#31181;&#34892;&#20026;&#34987;&#35748;&#20026;&#26377;&#23475;&#65307;&#32422;&#26377;2.2%&#30340;&#21463;&#35775;&#32773;&#34920;&#31034;&#26366;&#21463;&#23475;&#65292;1.8%&#30340;&#21463;&#35775;&#32773;&#34920;&#31034;&#26366;&#21442;&#19982;&#36807;&#27492;&#31867;&#34892;&#20026;&#65307;&#21333;&#38752;&#31435;&#27861;&#34892;&#21160;&#24182;&#19981;&#36275;&#20197;&#35299;&#20915;&#38382;&#39064;&#65292;&#24314;&#35758;&#25216;&#26415;&#21644;&#24179;&#21488;&#25919;&#31574;&#30340;&#25913;&#36827;&#26469;&#20943;&#23569;&#20260;&#23475;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#20266;&#36896;&#25216;&#26415;&#24037;&#20855;&#24050;&#32463;&#26080;&#22788;&#19981;&#22312;&#65292;"&#27665;&#20027;&#21270;"&#20102;&#25805;&#32437;&#22270;&#20687;&#21644;&#35270;&#39057;&#30340;&#33021;&#21147;&#12290;&#36825;&#31181;&#25216;&#26415;&#30340;&#19968;&#20010;&#27969;&#34892;&#29992;&#36884;&#26159;&#21019;&#24314;&#24615;&#26292;&#21147;&#20869;&#23481;&#65292;&#28982;&#21518;&#22312;&#20114;&#32852;&#32593;&#19978;&#24191;&#27867;&#21457;&#24067;&#21644;&#20849;&#20139;&#12290;&#26412;&#25991;&#36890;&#36807;&#23545;10&#20010;&#22269;&#23478;&#30340;&#36229;&#36807;16,000&#21517;&#21463;&#35775;&#32773;&#30340;&#30740;&#31350;&#65292;&#30740;&#31350;&#20102;&#19982;&#38750;&#33258;&#24895;&#21512;&#25104;&#20146;&#23494;&#22270;&#20687;&#65288;NSII&#65289;&#30456;&#20851;&#30340;&#24577;&#24230;&#21644;&#34892;&#20026;&#12290;&#23613;&#31649;&#31038;&#20250;&#23545;NSII&#30340;&#35748;&#35782;&#23578;&#22788;&#20110;&#21021;&#32423;&#38454;&#27573;&#65292;&#20294;NSII&#34892;&#20026;&#34987;&#35748;&#20026;&#26159;&#26377;&#23475;&#30340;&#12290;&#22312;&#26222;&#36941;&#24615;&#26041;&#38754;&#65292;&#25152;&#26377;&#21463;&#35775;&#32773;&#20013;&#26377;2.2%&#30340;&#20154;&#34920;&#31034;&#26366;&#21463;&#23475;&#65292;1.8%&#30340;&#20154;&#34920;&#31034;&#26366;&#21442;&#19982;&#36807;&#36825;&#31181;&#34892;&#20026;&#12290;&#26469;&#33258;&#20855;&#26377;&#30456;&#20851;&#31435;&#27861;&#30340;&#22269;&#23478;&#30340;&#21463;&#35775;&#32773;&#20063;&#25253;&#21578;&#20102;&#21442;&#19982;&#21644;&#21463;&#23475;&#32463;&#21382;&#65292;&#36825;&#34920;&#26126;&#21333;&#38752;&#31435;&#27861;&#34892;&#21160;&#24182;&#19981;&#36275;&#20197;&#38459;&#27490;&#21442;&#19982;&#32773;&#30340;&#34892;&#20026;&#12290;&#20943;&#23569;&#20260;&#23475;&#30340;&#25216;&#26415;&#32771;&#34385;&#21487;&#33021;&#21253;&#25324;&#24314;&#35758;&#20154;&#20204;&#22914;&#20309;&#26356;&#22909;&#22320;&#30417;&#25511;&#33258;&#24049;&#22312;&#32593;&#19978;&#30340;&#23384;&#22312;&#65292;&#24182;&#25191;&#34892;&#24179;&#21488;&#25919;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deepfake technology tools have become ubiquitous, "democratizing" the ability to manipulate images and videos. One popular use of such technology is the creation of sexually explicit content, which can then be posted and shared widely on the internet. This article examines attitudes and behaviors related to non-consensual synthetic intimate imagery (NSII) across over 16,000 respondents in 10 countries. Despite nascent societal awareness of NSII, NSII behaviors were considered harmful. In regards to prevalence, 2.2% of all respondents indicated personal victimization, and 1.8% all of respondents indicated perpetration behaviors. Respondents from countries with relevant legislation also reported perpetration and victimization experiences, suggesting legislative action alone is not a sufficient solution to deter perpetration. Technical considerations to reduce harms may include suggestions for how individuals can better monitor their presence online, as well as enforced platform policies 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#26412;&#20307;&#23884;&#20837;&#26041;&#27861;EIKE&#65292;&#36890;&#36807;&#25972;&#21512;&#22806;&#24310;&#30693;&#35782;&#21644;&#20869;&#28085;&#30693;&#35782;&#65292;&#22312;&#22806;&#24310;&#31354;&#38388;&#21644;&#20869;&#28085;&#31354;&#38388;&#20013;&#34920;&#31034;&#26412;&#20307;&#65292;&#24182;&#37319;&#29992;&#22522;&#20110;&#20960;&#20309;&#30340;&#26041;&#27861;&#21644;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#23545;&#23454;&#20363;&#12289;&#27010;&#24565;&#21644;&#20851;&#31995;&#36827;&#34892;&#23884;&#20837;&#24314;&#27169;&#12290;</title><link>https://arxiv.org/abs/2402.01677</link><description>&lt;p&gt;
&#36890;&#36807;&#25972;&#21512;&#22806;&#24310;&#30693;&#35782;&#21644;&#20869;&#28085;&#30693;&#35782;&#23884;&#20837;&#26412;&#20307;
&lt;/p&gt;
&lt;p&gt;
Embedding Ontologies via Incoprorating Extensional and Intensional Knowledge
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01677
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#26412;&#20307;&#23884;&#20837;&#26041;&#27861;EIKE&#65292;&#36890;&#36807;&#25972;&#21512;&#22806;&#24310;&#30693;&#35782;&#21644;&#20869;&#28085;&#30693;&#35782;&#65292;&#22312;&#22806;&#24310;&#31354;&#38388;&#21644;&#20869;&#28085;&#31354;&#38388;&#20013;&#34920;&#31034;&#26412;&#20307;&#65292;&#24182;&#37319;&#29992;&#22522;&#20110;&#20960;&#20309;&#30340;&#26041;&#27861;&#21644;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#23545;&#23454;&#20363;&#12289;&#27010;&#24565;&#21644;&#20851;&#31995;&#36827;&#34892;&#23884;&#20837;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#20307;&#21253;&#21547;&#39046;&#22495;&#20869;&#20016;&#23500;&#30340;&#30693;&#35782;&#65292;&#21487;&#20197;&#20998;&#20026;&#20004;&#20010;&#31867;&#21035;&#65292;&#21363;&#22806;&#24310;&#30693;&#35782;&#21644;&#20869;&#28085;&#30693;&#35782;&#12290;&#22806;&#24310;&#30693;&#35782;&#25552;&#20379;&#20851;&#20110;&#26412;&#20307;&#20013;&#29305;&#23450;&#27010;&#24565;&#25152;&#23646;&#30340;&#20855;&#20307;&#23454;&#20363;&#30340;&#20449;&#24687;&#65292;&#32780;&#20869;&#28085;&#30693;&#35782;&#35814;&#32454;&#25551;&#36848;&#20102;&#27010;&#24565;&#20043;&#38388;&#30340;&#20869;&#22312;&#23646;&#24615;&#12289;&#29305;&#24449;&#21644;&#35821;&#20041;&#20851;&#32852;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26412;&#20307;&#23884;&#20837;&#26041;&#27861;&#26410;&#33021;&#21516;&#26102;&#20805;&#20998;&#32771;&#34385;&#22806;&#24310;&#30693;&#35782;&#21644;&#20869;&#28085;&#30693;&#35782;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;EIKE&#65288;Extensional and Intensional Knowledge Embedding&#65289;&#30340;&#26032;&#22411;&#26412;&#20307;&#23884;&#20837;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#22806;&#24310;&#31354;&#38388;&#21644;&#20869;&#28085;&#31354;&#38388;&#20013;&#34920;&#31034;&#26412;&#20307;&#12290;EIKE&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23558;&#23454;&#20363;&#12289;&#27010;&#24565;&#21450;&#20854;&#20851;&#31995;&#23884;&#20837;&#21040;&#26412;&#20307;&#20013;&#65292;&#37319;&#29992;&#22522;&#20110;&#20960;&#20309;&#30340;&#26041;&#27861;&#23545;&#22806;&#24310;&#30693;&#35782;&#36827;&#34892;&#24314;&#27169;&#65292;&#24182;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#23545;&#20869;&#28085;&#30693;&#35782;&#36827;&#34892;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ontologies contain rich knowledge within domain, which can be divided into two categories, namely extensional knowledge and intensional knowledge. Extensional knowledge provides information about the concrete instances that belong to specific concepts in the ontology, while intensional knowledge details inherent properties, characteristics, and semantic associations among concepts. However, existing ontology embedding approaches fail to take both extensional knowledge and intensional knowledge into fine consideration simultaneously. In this paper, we propose a novel ontology embedding approach named EIKE (Extensional and Intensional Knowledge Embedding) by representing ontologies in two spaces, called extensional space and intensional space. EIKE presents a unified framework for embedding instances, concepts and their relations in an ontology, applying a geometry-based method to model extensional knowledge and a pretrained language model to model intensional knowledge, which can captur
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#38646;&#23556;&#20987;&#25552;&#31034;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#38169;&#35823;&#20449;&#24687;&#26469;&#25351;&#23548;&#27169;&#22411;&#36827;&#34892;&#20219;&#21153;&#65292;&#20197;&#25552;&#39640;&#20219;&#21153;&#34920;&#29616;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#65292;&#21253;&#25324;&#38405;&#35835;&#29702;&#35299;&#12289;&#31639;&#26415;&#25512;&#29702;&#21644;&#38381;&#21367;&#38382;&#31572;&#65292;&#27169;&#22411;&#24615;&#33021;&#26377;&#25152;&#25552;&#21319;&#12290;&#36825;&#20123;&#32467;&#26524;&#20063;&#26174;&#31034;&#20986;&#19981;&#21516;&#27169;&#22411;&#20043;&#38388;&#23384;&#22312;&#19981;&#21516;&#31243;&#24230;&#30340;&#38169;&#35823;&#20449;&#24687;&#12290;</title><link>https://arxiv.org/abs/2401.08273</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#26159;&#38646;&#23556;&#20987;&#23398;&#20064;&#22120;
&lt;/p&gt;
&lt;p&gt;
Large Language Models are Null-Shot Learners
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.08273
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#38646;&#23556;&#20987;&#25552;&#31034;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#38169;&#35823;&#20449;&#24687;&#26469;&#25351;&#23548;&#27169;&#22411;&#36827;&#34892;&#20219;&#21153;&#65292;&#20197;&#25552;&#39640;&#20219;&#21153;&#34920;&#29616;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#65292;&#21253;&#25324;&#38405;&#35835;&#29702;&#35299;&#12289;&#31639;&#26415;&#25512;&#29702;&#21644;&#38381;&#21367;&#38382;&#31572;&#65292;&#27169;&#22411;&#24615;&#33021;&#26377;&#25152;&#25552;&#21319;&#12290;&#36825;&#20123;&#32467;&#26524;&#20063;&#26174;&#31034;&#20986;&#19981;&#21516;&#27169;&#22411;&#20043;&#38388;&#23384;&#22312;&#19981;&#21516;&#31243;&#24230;&#30340;&#38169;&#35823;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#38646;&#23556;&#20987;&#25552;&#31034;&#26041;&#27861;&#12290;&#38646;&#23556;&#20987;&#25552;&#31034;&#21033;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#30340;&#38169;&#35823;&#20449;&#24687;&#65292;&#36890;&#36807;&#25351;&#31034;LLMs&#21033;&#29992;&#20174;&#8220;&#31034;&#20363;&#8221;&#37096;&#20998;&#20013;&#33719;&#21462;&#30340;&#20449;&#24687;&#65288;&#35813;&#20449;&#24687;&#22312;&#25152;&#25552;&#20379;&#30340;&#19978;&#19979;&#25991;&#20013;&#19981;&#23384;&#22312;&#65289;&#26469;&#23436;&#25104;&#20219;&#21153;&#12290;&#34429;&#28982;&#20943;&#23569;&#38169;&#35823;&#20449;&#24687;&#23545;&#20110;LLMs&#30340;&#26085;&#24120;&#21644;&#37325;&#35201;&#29992;&#36884;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#25105;&#20204;&#25552;&#20986;&#22312;&#30446;&#21069;&#30340;&#29615;&#22659;&#20013;&#65292;&#36825;&#20123;LLMs&#20173;&#28982;&#20855;&#26377;&#38169;&#35823;&#20449;&#24687;&#65292;&#23454;&#38469;&#19978;&#21487;&#20197;&#21033;&#29992;&#38169;&#35823;&#20449;&#24687;&#26469;&#25552;&#39640;&#19982;&#26631;&#20934;&#38646;&#23556;&#20987;&#25552;&#31034;&#30456;&#27604;&#30340;&#20219;&#21153;&#34920;&#29616;&#12290;&#23545;&#20843;&#20010;LLMs&#36827;&#34892;&#23454;&#39564;&#65292;&#32467;&#26524;&#26174;&#31034;&#22312;&#22823;&#22810;&#25968;&#20843;&#20010;&#25968;&#25454;&#38598;&#65288;&#21253;&#25324;&#38405;&#35835;&#29702;&#35299;&#12289;&#31639;&#26415;&#25512;&#29702;&#21644;&#38381;&#21367;&#38382;&#31572;&#65289;&#20013;&#65292;&#24615;&#33021;&#26377;&#25152;&#25552;&#21319;&#12290;&#35266;&#23519;&#21040;&#30340;&#19981;&#19968;&#33268;&#24615;&#22686;&#21152;&#30456;&#23545;&#24615;&#33021;&#22312;LLMs&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#20063;&#21487;&#33021;&#34920;&#31034;&#27599;&#20010;&#27169;&#22411;&#20013;&#23384;&#22312;&#19981;&#21516;&#31243;&#24230;&#30340;&#38169;&#35823;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.08273v2 Announce Type: replace-cross Abstract: This paper presents null-shot prompting. Null-shot prompting exploits hallucination in large language models (LLMs) by instructing LLMs to utilize information from the "Examples" section that never exists within the provided context to perform a task. While reducing hallucination is crucial and non-negligible for daily and critical uses of LLMs, we propose that in the current landscape in which these LLMs still hallucinate, it is possible, in fact, to exploit hallucination to increase performance in performing tasks compared to standard zero-shot prompting. Experiments with eight LLMs show improvements in performance across the majority of eight datasets, including reading comprehension, arithmetic reasoning, and closed-book question answering. The observed inconsistency in increased relative performance across the LLMs also potentially indicates a different degree of inherent hallucination in each model. These differences show 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;AI&#22522;&#20110;&#31227;&#21160;&#24212;&#29992;&#35780;&#20215;&#20013;&#30340;&#20844;&#24179;&#20851;&#27880;&#65292;&#24182;&#36890;&#36807;&#26500;&#24314;&#25968;&#25454;&#38598;&#21644;&#24320;&#21457;&#20998;&#31867;&#22120;&#30340;&#26041;&#27861;&#65292;&#25104;&#21151;&#26816;&#27979;&#20986;&#20844;&#24179;&#24615;&#35780;&#35770;&#65292;&#24182;&#35782;&#21035;&#20986;&#32422;92000&#26465;&#20844;&#24179;&#24615;&#35780;&#35770;&#12290;</title><link>https://arxiv.org/abs/2401.08097</link><description>&lt;p&gt;
AI&#22522;&#20110;&#31227;&#21160;&#24212;&#29992;&#35780;&#20215;&#30340;&#20844;&#24179;&#20851;&#27880;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Study of Fairness Concerns in AI-based Mobile App Reviews
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.08097
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;AI&#22522;&#20110;&#31227;&#21160;&#24212;&#29992;&#35780;&#20215;&#20013;&#30340;&#20844;&#24179;&#20851;&#27880;&#65292;&#24182;&#36890;&#36807;&#26500;&#24314;&#25968;&#25454;&#38598;&#21644;&#24320;&#21457;&#20998;&#31867;&#22120;&#30340;&#26041;&#27861;&#65292;&#25104;&#21151;&#26816;&#27979;&#20986;&#20844;&#24179;&#24615;&#35780;&#35770;&#65292;&#24182;&#35782;&#21035;&#20986;&#32422;92000&#26465;&#20844;&#24179;&#24615;&#35780;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20844;&#24179;&#26159;AI&#31995;&#32479;&#20013;&#24517;&#39035;&#35299;&#20915;&#30340;&#31038;&#20250;&#25216;&#26415;&#38382;&#39064;&#20043;&#19968;&#12290;&#19981;&#20844;&#24179;&#30340;AI&#31995;&#32479;&#65292;&#29305;&#21035;&#26159;&#19981;&#20844;&#24179;&#30340;AI&#22522;&#20110;&#31227;&#21160;&#24212;&#29992;&#65292;&#21487;&#33021;&#32473;&#20840;&#29699;&#24456;&#22823;&#19968;&#37096;&#20998;&#20154;&#21475;&#24102;&#26469;&#22256;&#38590;&#12290;&#26412;&#25991;&#26088;&#22312;&#20998;&#26512;AI&#22522;&#20110;&#24212;&#29992;&#35780;&#20215;&#20013;&#30340;&#20844;&#24179;&#38382;&#39064;&#12290;&#25105;&#20204;&#39318;&#20808;&#25163;&#21160;&#26500;&#24314;&#20102;&#19968;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#20844;&#24179;&#24615;&#21644;&#38750;&#20844;&#24179;&#24615;&#35780;&#35770;&#30340;&#32479;&#35745;&#26679;&#26412;&#12290;&#21033;&#29992;&#36825;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#24320;&#21457;&#21644;&#35780;&#20272;&#20102;&#19968;&#32452;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#20998;&#31867;&#22120;&#65292;&#29992;&#20110;&#21306;&#20998;&#20844;&#24179;&#24615;&#35780;&#35770;&#21644;&#38750;&#20844;&#24179;&#24615;&#35780;&#35770;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#26368;&#20339;&#30340;&#20998;&#31867;&#22120;&#21487;&#20197;&#20197;94%&#30340;&#31934;&#30830;&#24230;&#26816;&#27979;&#21040;&#20844;&#24179;&#24615;&#35780;&#35770;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#26368;&#20339;&#20998;&#31867;&#22120;&#24212;&#29992;&#20110;&#20174;108&#20010;AI&#22522;&#20110;&#24212;&#29992;&#25910;&#38598;&#30340;&#32422;950&#19975;&#26465;&#35780;&#35770;&#65292;&#35782;&#21035;&#20986;&#32422;92000&#26465;&#20844;&#24179;&#24615;&#35780;&#35770;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#23558;K-means&#32858;&#31867;&#25216;&#26415;&#24212;&#29992;&#20110;&#36825;92000&#26465;&#20844;&#24179;&#24615;&#35780;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.08097v2 Announce Type: replace-cross Abstract: Fairness is one of the socio-technical concerns that must be addressed in AI-based systems. Unfair AI-based systems, particularly unfair AI-based mobile apps, can pose difficulties for a significant proportion of the global population. This paper aims to analyze fairness concerns in AI-based app reviews.We first manually constructed a ground-truth dataset, including a statistical sample of fairness and non-fairness reviews. Leveraging the ground-truth dataset, we developed and evaluated a set of machine learning and deep learning classifiers that distinguish fairness reviews from non-fairness reviews. Our experiments show that our best-performing classifier can detect fairness reviews with a precision of 94%. We then applied the best-performing classifier on approximately 9.5M reviews collected from 108 AI-based apps and identified around 92K fairness reviews. Next, applying the K-means clustering technique to the 92K fairness r
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#23558;&#22312;&#19968;&#20010;&#30495;&#23454;&#30340;&#22312;&#32447;&#38382;&#31572;&#31038;&#21306;&#30340;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20004;&#20010;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#22810;&#27169;&#22411;&#27169;&#22411;&#65288;GPT-4V&#21644;Gemini&#65289;&#65292;&#20998;&#26512;&#20102;&#23427;&#20204;&#30340;&#33021;&#21147;&#21644;&#38480;&#21046;&#65292;&#30740;&#31350;&#20102;&#19981;&#21516;&#31867;&#22411;&#30340;&#38382;&#39064;&#23545;&#27169;&#22411;&#30340;&#25361;&#25112;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#38646;&#26679;&#26412;&#24615;&#33021;&#20998;&#26512;&#12290;</title><link>https://arxiv.org/abs/2312.10637</link><description>&lt;p&gt;
&#22312;&#22312;&#32447;&#35270;&#35273;&#38382;&#31572;&#20013;&#35780;&#20272;GPT-4V&#21644;Gemini
&lt;/p&gt;
&lt;p&gt;
An Evaluation of GPT-4V and Gemini in Online VQA
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.10637
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#23558;&#22312;&#19968;&#20010;&#30495;&#23454;&#30340;&#22312;&#32447;&#38382;&#31572;&#31038;&#21306;&#30340;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20004;&#20010;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#22810;&#27169;&#22411;&#27169;&#22411;&#65288;GPT-4V&#21644;Gemini&#65289;&#65292;&#20998;&#26512;&#20102;&#23427;&#20204;&#30340;&#33021;&#21147;&#21644;&#38480;&#21046;&#65292;&#30740;&#31350;&#20102;&#19981;&#21516;&#31867;&#22411;&#30340;&#38382;&#39064;&#23545;&#27169;&#22411;&#30340;&#25361;&#25112;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#38646;&#26679;&#26412;&#24615;&#33021;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#22810;&#27169;&#22411;&#27169;&#22411;&#65288;LMM&#65289;&#30340;&#28508;&#21147;&#22791;&#21463;&#20851;&#27880;&#65292;&#20294;&#20840;&#38754;&#35780;&#20272;&#23427;&#20204;&#30340;&#33021;&#21147;&#21644;&#38480;&#21046;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#20102;&#25903;&#25345;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#22312;&#19968;&#20010;&#26469;&#33258;&#30495;&#23454;&#22312;&#32447;&#38382;&#31572;&#31038;&#21306;&#30340;&#26032;&#35270;&#35273;&#38382;&#31572;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#20004;&#20010;&#26368;&#20808;&#36827;&#30340;LMM&#27169;&#22411;&#65292;GPT-4V&#21644;Gemini&#12290;&#25105;&#20204;&#29983;&#25104;&#20102;&#36817;2000&#20010;&#35270;&#35273;&#38382;&#39064;&#30340;&#19971;&#31181;&#31867;&#22411;&#30340;&#20803;&#25968;&#25454;&#65292;&#22914;&#22270;&#20687;&#31867;&#22411;&#21644;&#25152;&#38656;&#30340;&#22270;&#20687;&#22788;&#29702;&#33021;&#21147;&#65292;&#24182;&#36827;&#34892;&#20102;&#32454;&#31890;&#24230;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#38646;&#26679;&#26412;&#24615;&#33021;&#20998;&#26512;&#31361;&#20986;&#20102;&#36825;&#20004;&#20010;&#27169;&#22411;&#26368;&#20855;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#31867;&#22411;&#65292;&#21253;&#25324;&#19982;&#8220;&#20196;&#20154;&#22256;&#24785;&#8221;&#30340;&#20027;&#39064;&#30456;&#20851;&#30340;&#38382;&#39064;&#65292;&#20855;&#26377;&#8220;&#35782;&#21035;&#8221;&#29992;&#25143;&#24847;&#22270;&#65292;&#20855;&#26377;&#8220;&#20048;&#35889;&#8221;&#22270;&#20687;&#31867;&#22411;&#25110;&#34987;GPT-4&#26631;&#35760;&#20026;&#8220;&#22256;&#38590;&#8221;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.10637v2 Announce Type: replace-cross Abstract: While there is much excitement about the potential of large multimodal models (LMM), a comprehensive evaluation is critical to establish their true capabilities and limitations. In support of this aim, we evaluate two state-of-the-art LMMs, GPT-4V and Gemini, on a new visual question answering dataset sourced from an authentic online question answering community. We conduct fine-grained analysis by generating seven types of metadata for nearly 2,000 visual questions, such as image type and the required image processing capabilities. Our zero-shot performance analysis highlights the types of questions that are most challenging for both models, including questions related to "puzzling" topic, with "Identification" user intention, with "Sheet Music" image type, or labeled as "hard" by GPT-4.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#20041;&#33021;&#21147;&#27169;&#22411;&#21644;SMT&#30340;&#33258;&#21160;&#21270;&#24037;&#33402;&#35268;&#21010;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#26088;&#22312;&#35299;&#20915;&#21046;&#36896;&#31995;&#32479;&#21644;&#33258;&#20027;&#26426;&#22120;&#20154;&#20013;&#30340;&#33021;&#21147;&#35268;&#33539;&#35299;&#37322;&#20197;&#21450;&#33258;&#21160;&#21270;&#24037;&#33402;&#35268;&#21010;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2312.08801</link><description>&lt;p&gt;
&#22522;&#20110;&#35821;&#20041;&#33021;&#21147;&#27169;&#22411;&#21644;SMT&#30340;&#33258;&#21160;&#21270;&#24037;&#33402;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Automated Process Planning Based on a Semantic Capability Model and SMT
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.08801
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#20041;&#33021;&#21147;&#27169;&#22411;&#21644;SMT&#30340;&#33258;&#21160;&#21270;&#24037;&#33402;&#35268;&#21010;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#26088;&#22312;&#35299;&#20915;&#21046;&#36896;&#31995;&#32479;&#21644;&#33258;&#20027;&#26426;&#22120;&#20154;&#20013;&#30340;&#33021;&#21147;&#35268;&#33539;&#35299;&#37322;&#20197;&#21450;&#33258;&#21160;&#21270;&#24037;&#33402;&#35268;&#21010;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21046;&#36896;&#31995;&#32479;&#21644;&#33258;&#20027;&#26426;&#22120;&#20154;&#30340;&#30740;&#31350;&#20013;&#65292;"&#33021;&#21147;"&#19968;&#35789;&#29992;&#20110;&#23545;&#31995;&#32479;&#21151;&#33021;&#36827;&#34892;&#26426;&#22120;&#21487;&#35299;&#37322;&#30340;&#35268;&#33539;&#12290;&#26412;&#30740;&#31350;&#39046;&#22495;&#30340;&#26041;&#27861;&#24320;&#21457;&#20102;&#25429;&#33719;&#35299;&#37322;&#21151;&#33021;&#35201;&#27714;&#12289;&#25928;&#26524;&#21644;&#34892;&#20026;&#30340;&#25152;&#26377;&#30456;&#20851;&#20449;&#24687;&#30340;&#20449;&#24687;&#27169;&#22411;&#12290;&#36825;&#20123;&#26041;&#27861;&#26088;&#22312;&#20811;&#26381;&#30001;&#21508;&#31181;&#31867;&#22411;&#30340;&#27969;&#31243;&#21644;&#22823;&#37327;&#19981;&#21516;&#20379;&#24212;&#21830;&#23548;&#33268;&#30340;&#24322;&#26500;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#21644;&#30456;&#20851;&#26041;&#27861;&#24182;&#26410;&#25552;&#20379;&#33258;&#21160;&#21270;&#24037;&#33402;&#35268;&#21010;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21363;&#23547;&#25214;&#21046;&#36896;&#26576;&#31181;&#20135;&#21697;&#25110;&#20351;&#29992;&#33258;&#20027;&#26426;&#22120;&#20154;&#23436;&#25104;&#20219;&#21153;&#25152;&#38656;&#30340;&#19968;&#31995;&#21015;&#20010;&#21035;&#33021;&#21147;&#30340;&#39034;&#24207;&#12290;&#30456;&#21453;&#65292;&#36825;&#26159;AI&#35268;&#21010;&#26041;&#27861;&#30340;&#20856;&#22411;&#20219;&#21153;&#65292;&#20294;&#36951;&#25022;&#30340;&#26159;&#65292;&#21019;&#24314;&#30456;&#24212;&#30340;&#35268;&#21010;&#38382;&#39064;&#25551;&#36848;&#38656;&#35201;&#24456;&#39640;&#30340;&#24037;&#20316;&#37327;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#36825;&#20004;&#20010;&#20027;&#39064;&#32467;&#21512;&#30340;&#26041;&#27861;&#65306;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.08801v2 Announce Type: replace Abstract: In research of manufacturing systems and autonomous robots, the term capability is used for a machine-interpretable specification of a system function. Approaches in this research area develop information models that capture all information relevant to interpret the requirements, effects and behavior of functions. These approaches are intended to overcome the heterogeneity resulting from the various types of processes and from the large number of different vendors. However, these models and associated methods do not offer solutions for automated process planning, i.e. finding a sequence of individual capabilities required to manufacture a certain product or to accomplish a mission using autonomous robots. Instead, this is a typical task for AI planning approaches, which unfortunately require a high effort to create the respective planning problem descriptions. In this paper, we present an approach that combines these two topics: Start
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;diff&#21382;&#21490;&#30340;&#31616;&#21333;&#19988;&#39640;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#23558;&#29615;&#22659;&#20013;&#30340;&#35266;&#27979;&#36716;&#25442;&#20026;&#25991;&#26412;&#25552;&#31034;&#65292;&#20197;&#20415;&#23545;&#20110;&#38271;&#26399;&#25512;&#29702;&#20915;&#31574;&#30340;&#20219;&#21153;&#20013;&#30340;Neural Language Models&#36827;&#34892;&#20248;&#21270;&#12290;</title><link>https://arxiv.org/abs/2312.07540</link><description>&lt;p&gt;
Neural Language Agents&#30340;diff&#21382;&#21490;
&lt;/p&gt;
&lt;p&gt;
diff History for Neural Language Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.07540
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;diff&#21382;&#21490;&#30340;&#31616;&#21333;&#19988;&#39640;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#23558;&#29615;&#22659;&#20013;&#30340;&#35266;&#27979;&#36716;&#25442;&#20026;&#25991;&#26412;&#25552;&#31034;&#65292;&#20197;&#20415;&#23545;&#20110;&#38271;&#26399;&#25512;&#29702;&#20915;&#31574;&#30340;&#20219;&#21153;&#20013;&#30340;Neural Language Models&#36827;&#34892;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Neural Language Models (LMs)&#20026;&#36890;&#29992;&#30340;&#20855;&#20307;&#25511;&#21046;&#25552;&#20379;&#20102;&#20196;&#20154;&#20852;&#22859;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#24403;&#20351;&#29992;&#22522;&#20110;LM&#30340;&#25511;&#21046;&#22120;&#26102;&#65292;&#20250;&#20986;&#29616;&#19968;&#20010;&#20851;&#38190;&#30340;&#25216;&#26415;&#38382;&#39064;&#65306;&#29615;&#22659;&#35266;&#27979;&#24517;&#39035;&#36716;&#25442;&#20026;&#25991;&#26412;&#65292;&#36825;&#19982;&#21382;&#21490;&#32806;&#21512;&#22312;&#19968;&#36215;&#65292;&#23548;&#33268;&#20887;&#38271;&#32780;&#20887;&#20313;&#30340;&#25991;&#26412;&#25552;&#31034;&#12290;&#22240;&#27492;&#65292;LM&#20195;&#29702;&#30340;&#20808;&#21069;&#24037;&#20316;&#23616;&#38480;&#20110;&#20855;&#26377;&#23567;&#35266;&#27979;&#22823;&#23567;&#20197;&#21450;&#23545;&#20132;&#20114;&#21382;&#21490;&#25110;&#25351;&#31034;&#35843;&#20248;&#38656;&#27714;&#36739;&#23567;&#30340;&#38480;&#21046;&#39046;&#22495;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;diff&#21382;&#21490;&#65292;&#36825;&#26159;&#19968;&#20010;&#31616;&#21333;&#19988;&#38750;&#24120;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#36890;&#36807;&#22312;&#29992;&#20110;&#25552;&#31034;LM&#31574;&#30053;&#30340;&#20132;&#20114;&#21382;&#21490;&#20013;&#30340;&#36830;&#32493;&#25991;&#26412;&#35266;&#27979;&#19978;&#24212;&#29992;Unix diff&#21629;&#20196;&#65292;&#25105;&#20204;&#26082;&#21487;&#20197;&#25688;&#38500;&#20887;&#20313;&#20449;&#24687;&#65292;&#21448;&#21487;&#20197;&#23558;&#25991;&#26412;&#36755;&#20837;&#30340;&#20869;&#23481;&#38598;&#20013;&#22312;&#29615;&#22659;&#20013;&#26174;&#33879;&#21464;&#21270;&#30340;&#26041;&#38754;&#12290;&#22312;&#38656;&#35201;&#38271;&#26399;&#25512;&#29702;&#36827;&#34892;&#20915;&#31574;&#30340;&#26410;&#35299;&#20915;&#30340;&#35270;&#39057;&#28216;&#25103;NetHack&#20013;&#65292;&#20351;&#29992;diff&#21382;&#21490;&#35843;&#20248;&#30340;LM&#19982;&#29366;&#24577;&#21305;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.07540v2 Announce Type: replace Abstract: Neural Language Models (LMs) offer an exciting solution for general-purpose embodied control. However, a key technical issue arises when using an LM-based controller: environment observations must be converted to text, which coupled with history, results in long and verbose textual prompts. As a result, prior work in LM agents is limited to restricted domains with small observation size as well as minimal needs for interaction history or instruction tuning. In this paper, we introduce diff history, a simple and highly effective solution to these issues. By applying the Unix diff command on consecutive text observations in the interaction histories used to prompt LM policies, we can both abstract away redundant information and focus the content of textual inputs on the salient changes in the environment. On NetHack, an unsolved video game that requires long-horizon reasoning for decision-making, LMs tuned with diff history match state-
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#28385;&#36275;&#27867;&#21270;&#24615;&#12289;&#25193;&#23637;&#24615;&#21644;&#35745;&#31639;&#36164;&#28304;&#20248;&#21270;&#30340;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#24265;&#20215;&#30340;&#20195;&#29702;&#27169;&#22411;&#20272;&#35745;&#25968;&#25454;&#28857;&#30340;&#21487;&#23398;&#20064;&#24615;&#20998;&#25968;&#65292;&#20248;&#20808;&#36873;&#25321;&#35757;&#32451;&#26356;&#22823;&#30340;&#27169;&#22411;&#25152;&#38656;&#30340;&#25968;&#25454;&#65292;&#20174;&#32780;&#22312;&#22823;&#35268;&#27169;&#35270;&#35273;&#29702;&#35299;&#20219;&#21153;&#20013;&#21462;&#24471;&#30456;&#21516;&#24615;&#33021;&#30340;&#21516;&#26102;&#20943;&#23569;&#20102;&#35745;&#31639;&#37327;&#12290;</title><link>https://arxiv.org/abs/2312.05328</link><description>&lt;p&gt;
&#19981;&#33391;&#23398;&#29983;&#25104;&#23601;&#20102;&#20248;&#31168;&#25945;&#24072;&#65306;&#20027;&#21160;&#23398;&#20064;&#21152;&#36895;&#22823;&#35268;&#27169;&#35270;&#35273;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Bad Students Make Great Teachers: Active Learning Accelerates Large-Scale Visual Understanding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.05328
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#28385;&#36275;&#27867;&#21270;&#24615;&#12289;&#25193;&#23637;&#24615;&#21644;&#35745;&#31639;&#36164;&#28304;&#20248;&#21270;&#30340;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#24265;&#20215;&#30340;&#20195;&#29702;&#27169;&#22411;&#20272;&#35745;&#25968;&#25454;&#28857;&#30340;&#21487;&#23398;&#20064;&#24615;&#20998;&#25968;&#65292;&#20248;&#20808;&#36873;&#25321;&#35757;&#32451;&#26356;&#22823;&#30340;&#27169;&#22411;&#25152;&#38656;&#30340;&#25968;&#25454;&#65292;&#20174;&#32780;&#22312;&#22823;&#35268;&#27169;&#35270;&#35273;&#29702;&#35299;&#20219;&#21153;&#20013;&#21462;&#24471;&#30456;&#21516;&#24615;&#33021;&#30340;&#21516;&#26102;&#20943;&#23569;&#20102;&#35745;&#31639;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24130;&#24459;&#32553;&#25918;&#34920;&#26126;&#20351;&#29992;&#22343;&#21248;&#37319;&#26679;&#30340;&#22823;&#35268;&#27169;&#35757;&#32451;&#36895;&#24230;&#22826;&#24930;&#12290;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#26088;&#22312;&#36890;&#36807;&#20248;&#20808;&#23398;&#20064;&#26368;&#30456;&#20851;&#30340;&#31034;&#20363;&#26469;&#25552;&#39640;&#25968;&#25454;&#25928;&#29575;&#12290;&#23613;&#31649;&#36825;&#20123;&#26041;&#27861;&#20855;&#26377;&#21560;&#24341;&#21147;&#65292;&#20294;&#30001;&#20110;&#27809;&#26377;&#35777;&#26126; a) &#21487;&#20197;&#27867;&#21270;&#21040;&#21508;&#31181;&#27169;&#22411;&#21644;&#20219;&#21153; b) &#21487;&#20197;&#25193;&#23637;&#21040;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#24182;&#19988; c) &#22312;&#32771;&#34385;&#25968;&#25454;&#36873;&#25321;&#24320;&#38144;&#26102;&#33021;&#33410;&#30465;&#35745;&#31639;&#36164;&#28304;&#65292;&#22240;&#27492;&#23578;&#26410;&#34987;&#24191;&#27867;&#37319;&#29992;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28385;&#36275;&#36825;&#19977;&#20010;&#23646;&#24615;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#23567;&#22411;&#24265;&#20215;&#30340;&#20195;&#29702;&#27169;&#22411;&#26469;&#20272;&#35745;&#25968;&#25454;&#28857;&#30340;&#8220;&#21487;&#23398;&#20064;&#24615;&#8221;&#20998;&#25968;&#65292;&#29992;&#20110;&#20248;&#20808;&#35757;&#32451;&#26356;&#22823;&#30340;&#27169;&#22411;&#25152;&#38656;&#30340;&#25968;&#25454;&#12290;&#32467;&#26524;&#19978;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;JFT&#19978;&#38656;&#35201;46%&#21644;51%&#26356;&#23569;&#30340;&#35757;&#32451;&#26356;&#26032;&#27425;&#25968;&#65292;&#24182;&#19988;&#35201;&#36798;&#21040;&#19982;&#22343;&#21248;&#35757;&#32451;&#30340;&#35270;&#35273;&#20998;&#31867;&#22120;&#22312;JFT&#21644;ALIGN&#19978;&#22810;&#27169;&#22411;&#30340;&#30456;&#21516;&#24615;&#33021;&#38656;&#35201;&#39640;&#36798;25%&#30340;&#26356;&#23569;&#30340;&#24635;&#35745;&#31639;&#37327;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#21457;&#29616;&#25105;&#20204;&#30340;&#25968;&#25454;&#20248;&#20808;&#26041;&#27861;&#33021;&#22815;&#22312;&#35270;&#35273;&#29702;&#35299;&#21644;&#22810;&#27169;&#22411;&#20219;&#21153;&#19978;&#21462;&#24471;&#19982;&#22343;&#21248;&#35757;&#32451;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#33410;&#30465;&#20102;&#35745;&#31639;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.05328v3 Announce Type: replace Abstract: Power-law scaling indicates that large-scale training with uniform sampling is prohibitively slow. Active learning methods aim to increase data efficiency by prioritizing learning on the most relevant examples. Despite their appeal, these methods have yet to be widely adopted since no one algorithm has been shown to a) generalize across models and tasks b) scale to large datasets and c) yield overall FLOP savings when accounting for the overhead of data selection. In this work we propose a method which satisfies these three properties, leveraging small, cheap proxy models to estimate "learnability" scores for datapoints, which are used to prioritize data for the training of much larger models. As a result, our models require 46% and 51% fewer training updates and up to 25% less total computation to reach the same performance as uniformly trained visual classifiers on JFT and multimodal models on ALIGN. Finally, we find our data-priori
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#27169;&#25311;&#25509;&#25910;&#32773;&#34892;&#20026;&#30340;&#36125;&#21494;&#26031;&#21149;&#23548;&#38382;&#39064;&#20013;&#65292;&#21457;&#36865;&#32773;&#35774;&#35745;&#20102;&#19968;&#20010;&#26368;&#20248;&#28040;&#24687;&#31574;&#30053;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#22810;&#39033;&#24335;&#26102;&#38388;&#26597;&#35810;&#31639;&#27861;&#65292;&#20197;&#20248;&#21270;&#20854;&#39044;&#26399;&#25928;&#29992;&#12290;</title><link>https://arxiv.org/abs/2311.18138</link><description>&lt;p&gt;
&#36890;&#36807;&#27169;&#25311;&#36827;&#34892;&#31639;&#27861;&#24615;&#21149;&#23548;
&lt;/p&gt;
&lt;p&gt;
Algorithmic Persuasion Through Simulation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.18138
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#27169;&#25311;&#25509;&#25910;&#32773;&#34892;&#20026;&#30340;&#36125;&#21494;&#26031;&#21149;&#23548;&#38382;&#39064;&#20013;&#65292;&#21457;&#36865;&#32773;&#35774;&#35745;&#20102;&#19968;&#20010;&#26368;&#20248;&#28040;&#24687;&#31574;&#30053;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#22810;&#39033;&#24335;&#26102;&#38388;&#26597;&#35810;&#31639;&#27861;&#65292;&#20197;&#20248;&#21270;&#20854;&#39044;&#26399;&#25928;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#20010;&#36125;&#21494;&#26031;&#21149;&#23548;&#38382;&#39064;&#65292;&#20854;&#20013;&#21457;&#36865;&#32773;&#24076;&#26395;&#35828;&#26381;&#25509;&#25910;&#32773;&#37319;&#21462;&#20108;&#20803;&#34892;&#20026;&#65292;&#20363;&#22914;&#36141;&#20080;&#20135;&#21697;&#12290;&#21457;&#36865;&#32773;&#20102;&#35299;&#19990;&#30028;&#30340;&#65288;&#20108;&#20803;&#65289;&#29366;&#24577;&#65292;&#27604;&#22914;&#20135;&#21697;&#36136;&#37327;&#26159;&#39640;&#36824;&#26159;&#20302;&#65292;&#20294;&#26159;&#23545;&#25509;&#25910;&#32773;&#30340;&#20449;&#24565;&#21644;&#25928;&#29992;&#21482;&#26377;&#26377;&#38480;&#30340;&#20449;&#24687;&#12290;&#21463;&#21040;&#23458;&#25143;&#35843;&#26597;&#12289;&#29992;&#25143;&#30740;&#31350;&#21644;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#26368;&#26032;&#36827;&#23637;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#20801;&#35768;&#21457;&#36865;&#32773;&#36890;&#36807;&#26597;&#35810;&#27169;&#25311;&#25509;&#25910;&#32773;&#30340;&#34892;&#20026;&#26469;&#20102;&#35299;&#26356;&#22810;&#20851;&#20110;&#25509;&#25910;&#32773;&#30340;&#20449;&#24687;&#12290;&#22312;&#22266;&#23450;&#25968;&#37327;&#30340;&#26597;&#35810;&#20043;&#21518;&#65292;&#21457;&#36865;&#32773;&#25215;&#35834;&#19968;&#20010;&#28040;&#24687;&#31574;&#30053;&#65292;&#25509;&#25910;&#32773;&#26681;&#25454;&#25910;&#21040;&#30340;&#28040;&#24687;&#26469;&#26368;&#22823;&#21270;&#22905;&#30340;&#39044;&#26399;&#25928;&#29992;&#26469;&#37319;&#21462;&#34892;&#21160;&#12290;&#25105;&#20204;&#23545;&#21457;&#36865;&#32773;&#22312;&#20219;&#20309;&#25509;&#25910;&#32773;&#31867;&#22411;&#20998;&#24067;&#19979;&#30340;&#26368;&#20248;&#28040;&#24687;&#31574;&#30053;&#36827;&#34892;&#20102;&#34920;&#24449;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22810;&#39033;&#24335;&#26102;&#38388;&#26597;&#35810;&#31639;&#27861;&#65292;&#20248;&#21270;&#20102;&#36825;&#20010;&#36125;&#21494;&#26031;&#21149;&#23548;&#28216;&#25103;&#20013;&#21457;&#36865;&#32773;&#30340;&#39044;&#26399;&#25928;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.18138v2 Announce Type: replace-cross Abstract: We study a Bayesian persuasion problem where a sender wants to persuade a receiver to take a binary action, such as purchasing a product. The sender is informed about the (binary) state of the world, such as whether the quality of the product is high or low, but only has limited information about the receiver's beliefs and utilities. Motivated by customer surveys, user studies, and recent advances in generative AI, we allow the sender to learn more about the receiver by querying an oracle that simulates the receiver's behavior. After a fixed number of queries, the sender commits to a messaging policy and the receiver takes the action that maximizes her expected utility given the message she receives. We characterize the sender's optimal messaging policy given any distribution over receiver types. We then design a polynomial-time querying algorithm that optimizes the sender's expected utility in this Bayesian persuasion game. We 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21453;&#20107;&#23454;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#24341;&#20837;&#20102;&#21453;&#20107;&#23454;&#36923;&#36753;&#20462;&#25913;&#20219;&#21153;&#21644;&#30456;&#24212;&#30340;&#35780;&#20272;&#25351;&#26631;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23637;&#29616;&#20986;&#26174;&#33879;&#30340;&#36923;&#36753;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2311.17438</link><description>&lt;p&gt;
CLOMO: &#24102;&#26377;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21453;&#20107;&#23454;&#36923;&#36753;&#20462;&#25913;
&lt;/p&gt;
&lt;p&gt;
CLOMO: Counterfactual Logical Modification with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.17438
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21453;&#20107;&#23454;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#24341;&#20837;&#20102;&#21453;&#20107;&#23454;&#36923;&#36753;&#20462;&#25913;&#20219;&#21153;&#21644;&#30456;&#24212;&#30340;&#35780;&#20272;&#25351;&#26631;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23637;&#29616;&#20986;&#26174;&#33879;&#30340;&#36923;&#36753;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#21453;&#20107;&#23454;&#25512;&#29702;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#22521;&#20859;LLMs&#20869;&#30340;&#21453;&#20107;&#23454;&#24605;&#32500;&#36807;&#31243;&#65292;&#24182;&#23545;&#20854;&#26377;&#25928;&#24615;&#36827;&#34892;&#20005;&#26684;&#35780;&#20272;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#39033;&#26032;&#20219;&#21153;&#65292;&#21363;&#21453;&#20107;&#23454;&#36923;&#36753;&#20462;&#25913;&#65288;CLOMO&#65289;&#65292;&#20197;&#21450;&#19968;&#20010;&#39640;&#36136;&#37327;&#30340;&#20154;&#24037;&#27880;&#37322;&#22522;&#20934;&#12290;&#22312;&#36825;&#20010;&#20219;&#21153;&#20013;&#65292;LLMs&#24517;&#39035;&#29087;&#32451;&#22320;&#25913;&#21464;&#32473;&#23450;&#30340;&#35770;&#35777;&#25991;&#26412;&#65292;&#20197;&#20445;&#25345;&#39044;&#23450;&#30340;&#36923;&#36753;&#20851;&#31995;&#12290;&#20026;&#20102;&#26377;&#25928;&#35780;&#20272;&#29983;&#25104;&#27169;&#22411;&#30340;&#21453;&#20107;&#23454;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;&#36923;&#36753;&#24863;&#30693;&#30340;&#21453;&#20107;&#23454;&#20998;&#25968;&#65292;&#30452;&#25509;&#35780;&#20272;LLMs&#30340;&#33258;&#28982;&#35821;&#35328;&#36755;&#20986;&#65292;&#32780;&#19981;&#26159;&#23558;&#20219;&#21153;&#24314;&#27169;&#20026;&#22810;&#39033;&#36873;&#25321;&#38382;&#39064;&#12290;&#20998;&#26512;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#33258;&#21160;&#35780;&#20272;&#25351;&#26631;&#19982;&#20154;&#31867;&#20559;&#22909;&#24456;&#22909;&#22320;&#19968;&#33268;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#23613;&#31649;LLMs&#23637;&#31034;&#20102;&#26174;&#30528;&#30340;&#36923;&#36753;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.17438v3 Announce Type: replace-cross Abstract: In this study, we delve into the realm of counterfactual reasoning capabilities of large language models (LLMs). Our primary objective is to cultivate the counterfactual thought processes within LLMs and rigorously assess these processes for their validity. Specifically, we introduce a novel task, Counterfactual Logical Modification (CLOMO), and a high-quality human-annotated benchmark. In this task, LLMs must adeptly alter a given argumentative text to uphold a predetermined logical relationship. To effectively evaluate a generation model's counterfactual capabilities, we propose an innovative evaluation metric, the LogicAware Counterfactual Score to directly evaluate the natural language output of LLMs instead of modeling the task as a multiple-choice problem. Analysis shows that the proposed automatic metric aligns well with human preference. Our experimental results show that while LLMs demonstrate a notable capacity for log
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#35843;&#26597;&#20102;&#20154;&#24037;&#26234;&#33021;&#20013;&#29702;&#24615;&#19982;&#38750;&#29702;&#24615;&#30340;&#27010;&#24565;&#65292;&#25552;&#20986;&#20102;&#26410;&#35299;&#38382;&#39064;&#12290;&#37325;&#28857;&#35752;&#35770;&#20102;&#34892;&#20026;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#30340;&#38750;&#29702;&#24615;&#34892;&#20026;&#21487;&#33021;&#26159;&#26368;&#20248;&#30340;&#24773;&#20917;&#12290;&#24050;&#32463;&#25552;&#20986;&#20102;&#19968;&#20123;&#26041;&#27861;&#26469;&#22788;&#29702;&#38750;&#29702;&#24615;&#20195;&#29702;&#65292;&#20294;&#20173;&#23384;&#22312;&#25361;&#25112;&#21644;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2311.17165</link><description>&lt;p&gt;
(&#38750;)&#29702;&#24615;&#22312;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#24212;&#29992;&#65306;&#29616;&#29366;&#12289;&#30740;&#31350;&#25361;&#25112;&#21644;&#26410;&#35299;&#20043;&#38382;
&lt;/p&gt;
&lt;p&gt;
(Ir)rationality in AI: State of the Art, Research Challenges and Open Questions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.17165
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#35843;&#26597;&#20102;&#20154;&#24037;&#26234;&#33021;&#20013;&#29702;&#24615;&#19982;&#38750;&#29702;&#24615;&#30340;&#27010;&#24565;&#65292;&#25552;&#20986;&#20102;&#26410;&#35299;&#38382;&#39064;&#12290;&#37325;&#28857;&#35752;&#35770;&#20102;&#34892;&#20026;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#30340;&#38750;&#29702;&#24615;&#34892;&#20026;&#21487;&#33021;&#26159;&#26368;&#20248;&#30340;&#24773;&#20917;&#12290;&#24050;&#32463;&#25552;&#20986;&#20102;&#19968;&#20123;&#26041;&#27861;&#26469;&#22788;&#29702;&#38750;&#29702;&#24615;&#20195;&#29702;&#65292;&#20294;&#20173;&#23384;&#22312;&#25361;&#25112;&#21644;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#24615;&#27010;&#24565;&#22312;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#20013;&#21344;&#25454;&#30528;&#37325;&#35201;&#22320;&#20301;&#12290;&#26080;&#35770;&#26159;&#27169;&#25311;&#20154;&#31867;&#25512;&#29702;&#36824;&#26159;&#36861;&#27714;&#26377;&#38480;&#26368;&#20248;&#24615;&#65292;&#25105;&#20204;&#36890;&#24120;&#24076;&#26395;&#20351;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#23613;&#21487;&#33021;&#29702;&#24615;&#12290;&#23613;&#31649;&#36825;&#20010;&#27010;&#24565;&#22312;&#20154;&#24037;&#26234;&#33021;&#20013;&#38750;&#24120;&#26680;&#24515;&#65292;&#20294;&#23545;&#20110;&#20160;&#20040;&#26500;&#25104;&#29702;&#24615;&#20195;&#29702;&#24182;&#27809;&#26377;&#32479;&#19968;&#30340;&#23450;&#20041;&#12290;&#26412;&#25991;&#35843;&#26597;&#20102;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#29702;&#24615;&#19982;&#38750;&#29702;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#36825;&#20010;&#39046;&#22495;&#30340;&#26410;&#35299;&#38382;&#39064;&#12290;&#22312;&#20854;&#20182;&#39046;&#22495;&#23545;&#29702;&#24615;&#30340;&#29702;&#35299;&#23545;&#20854;&#22312;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#27010;&#24565;&#20135;&#29983;&#20102;&#24433;&#21709;&#65292;&#29305;&#21035;&#26159;&#32463;&#27982;&#23398;&#12289;&#21746;&#23398;&#21644;&#24515;&#29702;&#23398;&#26041;&#38754;&#30340;&#30740;&#31350;&#12290;&#30528;&#37325;&#32771;&#34385;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#30340;&#34892;&#20026;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22312;&#26576;&#20123;&#24773;&#22659;&#20013;&#38750;&#29702;&#24615;&#34892;&#20026;&#21487;&#33021;&#26159;&#26368;&#20248;&#30340;&#24773;&#20917;&#12290;&#20851;&#20110;&#22788;&#29702;&#38750;&#29702;&#24615;&#20195;&#29702;&#30340;&#26041;&#27861;&#24050;&#32463;&#24471;&#21040;&#20102;&#19968;&#20123;&#21457;&#23637;&#65292;&#21253;&#25324;&#35782;&#21035;&#21644;&#20132;&#20114;&#31561;&#26041;&#38754;&#30340;&#30740;&#31350;&#65292;&#28982;&#32780;&#65292;&#22312;&#36825;&#20010;&#39046;&#22495;&#30340;&#24037;&#20316;&#20173;&#28982;&#23384;&#22312;&#19968;&#20123;&#25361;&#25112;&#21644;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.17165v2 Announce Type: replace Abstract: The concept of rationality is central to the field of artificial intelligence. Whether we are seeking to simulate human reasoning, or the goal is to achieve bounded optimality, we generally seek to make artificial agents as rational as possible. Despite the centrality of the concept within AI, there is no unified definition of what constitutes a rational agent. This article provides a survey of rationality and irrationality in artificial intelligence, and sets out the open questions in this area. The understanding of rationality in other fields has influenced its conception within artificial intelligence, in particular work in economics, philosophy and psychology. Focusing on the behaviour of artificial agents, we consider irrational behaviours that can prove to be optimal in certain scenarios. Some methods have been developed to deal with irrational agents, both in terms of identification and interaction, however work in this area re
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#32447;&#24191;&#21578;&#31995;&#32479;&#20013;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#28508;&#21147;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;LLM&#24191;&#21578;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#24182;&#35752;&#35770;&#20102;&#22522;&#20110;LLM&#30340;&#21160;&#24577;&#21019;&#24847;&#20248;&#21270;&#30340;&#21069;&#26223;&#12290;</title><link>https://arxiv.org/abs/2311.07601</link><description>&lt;p&gt;
&#22312;&#32447;&#24191;&#21578;&#19982;LLMs&#65306;&#26426;&#36935;&#19982;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Online Advertisements with LLMs: Opportunities and Challenges
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.07601
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#32447;&#24191;&#21578;&#31995;&#32479;&#20013;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#28508;&#21147;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;LLM&#24191;&#21578;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#24182;&#35752;&#35770;&#20102;&#22522;&#20110;LLM&#30340;&#21160;&#24577;&#21019;&#24847;&#20248;&#21270;&#30340;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#22312;&#32447;&#24191;&#21578;&#31995;&#32479;&#20013;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#28145;&#20837;&#30740;&#31350;&#20102;&#36825;&#20010;&#31995;&#32479;&#24517;&#39035;&#28385;&#36275;&#30340;&#38544;&#31169;&#12289;&#24310;&#36831;&#12289;&#21487;&#38752;&#24615;&#20197;&#21450;&#29992;&#25143;&#21644;&#24191;&#21578;&#21830;&#30340;&#28385;&#24847;&#24230;&#31561;&#20851;&#38190;&#35201;&#27714;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#20171;&#32461;&#20102;&#19968;&#20010;LLM&#24191;&#21578;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#21253;&#25324;&#20462;&#25913;&#12289;&#31454;&#26631;&#12289;&#39044;&#27979;&#21644;&#25293;&#21334;&#27169;&#22359;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#27599;&#20010;&#27169;&#22359;&#30340;&#19981;&#21516;&#35774;&#35745;&#32771;&#34385;&#65292;&#24182;&#23545;&#20854;&#23454;&#29992;&#24615;&#21644;&#23454;&#26045;&#20013;&#30340;&#25216;&#26415;&#25361;&#25112;&#36827;&#34892;&#20102;&#28145;&#20837;&#30740;&#31350;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22522;&#20110;LLM&#30340;&#21160;&#24577;&#21019;&#24847;&#20248;&#21270;&#30340;&#21069;&#26223;&#65292;&#20197;&#26174;&#33879;&#25552;&#21319;&#24191;&#21578;&#23545;&#29992;&#25143;&#30340;&#21560;&#24341;&#21147;&#65292;&#24182;&#35752;&#35770;&#20102;&#23427;&#25152;&#38754;&#20020;&#30340;&#39069;&#22806;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.07601v2 Announce Type: replace-cross Abstract: This paper explores the potential for leveraging Large Language Models (LLM) in the realm of online advertising systems. We delve into essential requirements including privacy, latency, reliability as well as the satisfaction of users and advertisers which such a system must fulfill. We further introduce a general framework for LLM advertisement, consisting of modification, bidding, prediction, and auction modules. Different design considerations for each module is presented, with an in-depth examination of their practicality and the technical challenges inherent to their implementation. Finally, we explore the prospect of LLM-based dynamic creative optimization as a means to significantly enhance the appeal of advertisements to users and discuss its additional challenges.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#28508;&#31354;&#38388;&#25805;&#25511;&#65292;&#20351;&#29992;&#19978;&#19979;&#25991;&#21521;&#37327;&#20316;&#20026;&#26367;&#20195;&#26041;&#27861;&#26469;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#20197;&#20351;&#35821;&#35328;&#27169;&#22411;&#26356;&#26377;&#25928;&#22320;&#36981;&#24490;&#31034;&#20363;&#28436;&#31034;&#65292;&#24182;&#36890;&#36807;&#35843;&#25972;&#21521;&#37327;&#30340;&#37327;&#32423;&#26469;&#36731;&#26494;&#25511;&#21046;&#23398;&#20064;&#36807;&#31243;&#12290;</title><link>https://arxiv.org/abs/2311.06668</link><description>&lt;p&gt;
&#22312;&#19978;&#19979;&#25991;&#21521;&#37327;&#20013;&#65292;&#36890;&#36807;&#28508;&#31354;&#38388;&#25805;&#25511;&#20351;&#19978;&#19979;&#25991;&#23398;&#20064;&#26356;&#26377;&#25928;&#21644;&#21487;&#25511;
&lt;/p&gt;
&lt;p&gt;
In-context Vectors: Making In Context Learning More Effective and Controllable Through Latent Space Steering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.06668
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#28508;&#31354;&#38388;&#25805;&#25511;&#65292;&#20351;&#29992;&#19978;&#19979;&#25991;&#21521;&#37327;&#20316;&#20026;&#26367;&#20195;&#26041;&#27861;&#26469;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#20197;&#20351;&#35821;&#35328;&#27169;&#22411;&#26356;&#26377;&#25928;&#22320;&#36981;&#24490;&#31034;&#20363;&#28436;&#31034;&#65292;&#24182;&#36890;&#36807;&#35843;&#25972;&#21521;&#37327;&#30340;&#37327;&#32423;&#26469;&#36731;&#26494;&#25511;&#21046;&#23398;&#20064;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23637;&#31034;&#20102;&#26032;&#20219;&#21153;&#36866;&#24212;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#31034;&#20363;&#28436;&#31034;&#26469;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#25928;&#26524;&#26377;&#38480;&#65292;&#38590;&#20197;&#23450;&#37327;&#25511;&#21046;&#65292;&#24182;&#21344;&#29992;&#19978;&#19979;&#25991;&#31383;&#21475;&#31354;&#38388;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#65292;&#23558;&#19978;&#19979;&#25991;&#23398;&#20064;&#37325;&#26032;&#23450;&#20041;&#20026;&#19978;&#19979;&#25991;&#21521;&#37327;&#65288;ICV&#65289;&#12290;&#20351;&#29992;ICV&#26377;&#20004;&#20010;&#27493;&#39588;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23545;&#28436;&#31034;&#31034;&#20363;&#36827;&#34892;&#27491;&#21521;&#20256;&#36882;&#65292;&#20174;LLM&#30340;&#28508;&#22312;&#23884;&#20837;&#20013;&#21019;&#24314;&#19978;&#19979;&#25991;&#21521;&#37327;&#12290;&#36825;&#20010;&#21521;&#37327;&#25429;&#25417;&#20102;&#20851;&#20110;&#39044;&#26399;&#20219;&#21153;&#30340;&#20851;&#38190;&#20449;&#24687;&#12290;&#23545;&#20110;&#19968;&#20010;&#26032;&#30340;&#26597;&#35810;&#65292;&#25105;&#20204;&#19981;&#26159;&#23558;&#31034;&#20363;&#28155;&#21152;&#21040;&#25552;&#31034;&#20013;&#65292;&#32780;&#26159;&#20351;&#29992;ICV&#26469;&#25913;&#21464;LLM&#30340;&#28508;&#22312;&#29366;&#24577;&#12290;ICV&#26041;&#27861;&#26377;&#20960;&#20010;&#22909;&#22788;&#65306;1&#65289;&#23427;&#20351;LLM&#33021;&#22815;&#26356;&#26377;&#25928;&#22320;&#36981;&#24490;&#31034;&#20363;&#28436;&#31034;&#65307;2&#65289;&#36890;&#36807;&#35843;&#25972;ICV&#30340;&#37327;&#32423;&#65292;&#23427;&#26131;&#20110;&#25511;&#21046;&#65307;3&#65289;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.06668v3 Announce Type: replace-cross Abstract: Large language models (LLMs) demonstrate emergent in-context learning capabilities, where they adapt to new tasks based on example demonstrations. However, in-context learning has seen limited effectiveness in many settings, is difficult to quantitatively control and takes up context window space. To overcome these limitations, we propose an alternative approach that recasts in-context learning as in-context vectors (ICV). Using ICV has two steps. We first use a forward pass on demonstration examples to create the in-context vector from the latent embedding of the LLM. This vector captures essential information about the intended task. On a new query, instead of adding demonstrations to the prompt, we shift the latent states of the LLM using the ICV. The ICV approach has several benefits: 1) it enables the LLM to more effectively follow the demonstration examples; 2) it's easy to control by adjusting the magnitude of the ICV; 3)
&lt;/p&gt;</description></item><item><title>&#23545;&#27604;&#28145;&#24230;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;&#31639;&#27861;&#65288;CDNMF&#65289;&#36890;&#36807;&#21152;&#28145;NMF&#30340;&#20449;&#24687;&#25552;&#21462;&#33021;&#21147;&#65292;&#37319;&#29992;&#23545;&#27604;&#23398;&#20064;&#30340;&#24605;&#24819;&#26500;&#24314;&#20102;&#32593;&#32476;&#25299;&#25169;&#21644;&#33410;&#28857;&#23646;&#24615;&#20316;&#20026;&#23545;&#27604;&#35270;&#22270;&#65292;&#24182;&#21033;&#29992;&#21435;&#20559;&#26041;&#27861;&#26469;&#20248;&#21270;&#31038;&#21306;&#25506;&#27979;&#30340;&#20840;&#23616;&#32467;&#26500;&#20449;&#24687;&#30340;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2311.02357</link><description>&lt;p&gt;
&#23545;&#27604;&#28145;&#24230;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;&#29992;&#20110;&#31038;&#21306;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Contrastive Deep Nonnegative Matrix Factorization for Community Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.02357
&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#28145;&#24230;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;&#31639;&#27861;&#65288;CDNMF&#65289;&#36890;&#36807;&#21152;&#28145;NMF&#30340;&#20449;&#24687;&#25552;&#21462;&#33021;&#21147;&#65292;&#37319;&#29992;&#23545;&#27604;&#23398;&#20064;&#30340;&#24605;&#24819;&#26500;&#24314;&#20102;&#32593;&#32476;&#25299;&#25169;&#21644;&#33410;&#28857;&#23646;&#24615;&#20316;&#20026;&#23545;&#27604;&#35270;&#22270;&#65292;&#24182;&#21033;&#29992;&#21435;&#20559;&#26041;&#27861;&#26469;&#20248;&#21270;&#31038;&#21306;&#25506;&#27979;&#30340;&#20840;&#23616;&#32467;&#26500;&#20449;&#24687;&#30340;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;&#65288;NMF&#65289;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#31038;&#21306;&#21457;&#29616;&#65292;&#22240;&#20026;&#20854;&#20855;&#26377;&#26356;&#22909;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;NMF&#30340;&#26041;&#27861;&#23384;&#22312;&#20197;&#19979;&#19977;&#20010;&#38382;&#39064;&#65306;1&#65289;&#23427;&#20204;&#30452;&#25509;&#23558;&#21407;&#22987;&#32593;&#32476;&#36716;&#25442;&#20026;&#31038;&#21306;&#25104;&#21592;&#31354;&#38388;&#65292;&#22240;&#27492;&#24456;&#38590;&#25429;&#25417;&#23618;&#27425;&#20449;&#24687;&#65307;2&#65289;&#23427;&#20204;&#36890;&#24120;&#21482;&#20851;&#27880;&#32593;&#32476;&#30340;&#25299;&#25169;&#32467;&#26500;&#65292;&#24573;&#35270;&#20102;&#33410;&#28857;&#23646;&#24615;&#65307;3&#65289;&#23427;&#20204;&#24456;&#38590;&#23398;&#20064;&#21040;&#31038;&#21306;&#21457;&#29616;&#25152;&#38656;&#30340;&#20840;&#23616;&#32467;&#26500;&#20449;&#24687;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31038;&#21306;&#21457;&#29616;&#31639;&#27861;&#65292;&#21517;&#20026;&#23545;&#27604;&#28145;&#24230;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;&#65288;CDNMF&#65289;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#28145;&#21270;NMF&#20197;&#22686;&#24378;&#20854;&#20449;&#24687;&#25552;&#21462;&#33021;&#21147;&#12290;&#38543;&#21518;&#65292;&#21463;&#21040;&#23545;&#27604;&#23398;&#20064;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#21019;&#36896;&#24615;&#22320;&#23558;&#32593;&#32476;&#25299;&#25169;&#21644;&#33410;&#28857;&#23646;&#24615;&#26500;&#24314;&#20026;&#20004;&#31181;&#23545;&#27604;&#35270;&#22270;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#31181;&#21435;&#20559;&#30340;&#26041;&#27861;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.02357v2 Announce Type: replace-cross Abstract: Recently, nonnegative matrix factorization (NMF) has been widely adopted for community detection, because of its better interpretability. However, the existing NMF-based methods have the following three problems: 1) they directly transform the original network into community membership space, so it is difficult for them to capture the hierarchical information; 2) they often only pay attention to the topology of the network and ignore its node attributes; 3) it is hard for them to learn the global structure information necessary for community detection. Therefore, we propose a new community detection algorithm, named Contrastive Deep Nonnegative Matrix Factorization (CDNMF). Firstly, we deepen NMF to strengthen its capacity for information extraction. Subsequently, inspired by contrastive learning, our algorithm creatively constructs network topology and node attributes as two contrasting views. Furthermore, we utilize a debiased
&lt;/p&gt;</description></item><item><title>LINC&#26159;&#19968;&#31181;&#36890;&#36807;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#35821;&#20041;&#35299;&#26512;&#22120;&#65292;&#23558;&#33258;&#28982;&#35821;&#35328;&#36716;&#21270;&#20026;&#19968;&#38454;&#36923;&#36753;&#34920;&#36798;&#24335;&#65292;&#20877;&#36890;&#36807;&#22806;&#37096;&#23450;&#29702;&#35777;&#26126;&#22120;&#36827;&#34892;&#25512;&#29702;&#30340;&#31070;&#32463;&#31526;&#21495;&#26041;&#27861;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#36923;&#36753;&#25512;&#29702;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2310.15164</link><description>&lt;p&gt;
LINC: &#36890;&#36807;&#23558;&#35821;&#35328;&#27169;&#22411;&#19982;&#19968;&#38454;&#36923;&#36753;&#35777;&#26126;&#22120;&#30456;&#32467;&#21512;&#30340;&#31070;&#32463;&#31526;&#21495;&#26041;&#27861;&#36827;&#34892;&#36923;&#36753;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
LINC: A Neurosymbolic Approach for Logical Reasoning by Combining Language Models with First-Order Logic Provers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.15164
&lt;/p&gt;
&lt;p&gt;
LINC&#26159;&#19968;&#31181;&#36890;&#36807;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#35821;&#20041;&#35299;&#26512;&#22120;&#65292;&#23558;&#33258;&#28982;&#35821;&#35328;&#36716;&#21270;&#20026;&#19968;&#38454;&#36923;&#36753;&#34920;&#36798;&#24335;&#65292;&#20877;&#36890;&#36807;&#22806;&#37096;&#23450;&#29702;&#35777;&#26126;&#22120;&#36827;&#34892;&#25512;&#29702;&#30340;&#31070;&#32463;&#31526;&#21495;&#26041;&#27861;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#36923;&#36753;&#25512;&#29702;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36923;&#36753;&#25512;&#29702;&#26159;&#20154;&#24037;&#26234;&#33021;&#20013;&#37325;&#35201;&#30340;&#20219;&#21153;&#65292;&#23427;&#36890;&#36807;&#20174;&#19968;&#32452;&#21069;&#25552;&#20013;&#25512;&#26029;&#32467;&#35770;&#30340;&#30495;&#20540;&#65292;&#22312;&#31185;&#23398;&#12289;&#25968;&#23398;&#21644;&#31038;&#20250;&#31561;&#39046;&#22495;&#20855;&#26377;&#24191;&#27867;&#30340;&#28508;&#22312;&#24433;&#21709;&#12290;&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#22522;&#20110;&#25552;&#31034;&#30340;&#31574;&#30053;&#26469;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26356;&#26377;&#25928;&#22320;&#36827;&#34892;&#36825;&#31181;&#25512;&#29702;&#65292;&#20294;&#23427;&#20204;&#20173;&#28982;&#34920;&#29616;&#19981;&#23613;&#22914;&#20154;&#24847;&#65292;&#32463;&#24120;&#22312;&#24494;&#22937;&#21644;&#38590;&#20197;&#39044;&#27979;&#30340;&#26041;&#24335;&#19979;&#22833;&#36133;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#23558;&#36825;&#20123;&#20219;&#21153;&#37325;&#26032;&#23450;&#20041;&#20026;&#27169;&#22359;&#21270;&#30340;&#31070;&#32463;&#31526;&#21495;&#32534;&#31243;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;LINC&#65306;&#36890;&#36807;&#31070;&#32463;&#31526;&#21495;&#35745;&#31639;&#36827;&#34892;&#36923;&#36753;&#25512;&#29702;&#12290;&#22312;LINC&#20013;&#65292;LLM&#20805;&#24403;&#35821;&#20041;&#35299;&#26512;&#22120;&#65292;&#23558;&#33258;&#28982;&#35821;&#35328;&#30340;&#21069;&#25552;&#21644;&#32467;&#35770;&#36716;&#21270;&#20026;&#19968;&#38454;&#36923;&#36753;&#34920;&#36798;&#24335;&#12290;&#28982;&#21518;&#65292;&#23558;&#36825;&#20123;&#34920;&#36798;&#24335;&#36716;&#31227;&#21040;&#22806;&#37096;&#23450;&#29702;&#35777;&#26126;&#22120;&#20013;&#65292;&#35813;&#35777;&#26126;&#22120;&#36890;&#36807;&#31526;&#21495;&#26041;&#24335;&#36827;&#34892;&#28436;&#32462;&#25512;&#29702;&#12290;&#21033;&#29992;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.15164v2 Announce Type: replace-cross Abstract: Logical reasoning, i.e., deductively inferring the truth value of a conclusion from a set of premises, is an important task for artificial intelligence with wide potential impacts on science, mathematics, and society. While many prompting-based strategies have been proposed to enable Large Language Models (LLMs) to do such reasoning more effectively, they still appear unsatisfactory, often failing in subtle and unpredictable ways. In this work, we investigate the validity of instead reformulating such tasks as modular neurosymbolic programming, which we call LINC: Logical Inference via Neurosymbolic Computation. In LINC, the LLM acts as a semantic parser, translating premises and conclusions from natural language to expressions in first-order logic. These expressions are then offloaded to an external theorem prover, which symbolically performs deductive inference. Leveraging this approach, we observe significant performance gain
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#21453;&#21521;&#25351;&#20196;&#36827;&#34892;&#26377;&#25928;&#30340;&#25351;&#20196;&#35843;&#20248;&#65292;&#36890;&#36807;&#29983;&#25104;&#19968;&#32452;&#33258;&#28982;&#30340;&#12289;&#36866;&#29992;&#20110;&#38271;&#25991;&#26412;&#29983;&#25104;&#30340;&#25351;&#20196;&#35843;&#20248;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#25925;&#20107;/&#33756;&#35889;&#29983;&#25104;&#21644;&#38271;&#31687;&#38382;&#31572;&#31561;&#20219;&#21153;&#19978;&#20248;&#20110;10&#20493;&#35268;&#27169;&#26356;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#32780;&#26080;&#38656;&#25351;&#20196;&#35843;&#20248;&#12290;</title><link>https://arxiv.org/abs/2304.08460</link><description>&lt;p&gt;
LongForm: &#20351;&#29992;&#21453;&#21521;&#25351;&#20196;&#36827;&#34892;&#26377;&#25928;&#30340;&#25351;&#20196;&#35843;&#20248;
&lt;/p&gt;
&lt;p&gt;
LongForm: Effective Instruction Tuning with Reverse Instructions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2304.08460
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#21453;&#21521;&#25351;&#20196;&#36827;&#34892;&#26377;&#25928;&#30340;&#25351;&#20196;&#35843;&#20248;&#65292;&#36890;&#36807;&#29983;&#25104;&#19968;&#32452;&#33258;&#28982;&#30340;&#12289;&#36866;&#29992;&#20110;&#38271;&#25991;&#26412;&#29983;&#25104;&#30340;&#25351;&#20196;&#35843;&#20248;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#25925;&#20107;/&#33756;&#35889;&#29983;&#25104;&#21644;&#38271;&#31687;&#38382;&#31572;&#31561;&#20219;&#21153;&#19978;&#20248;&#20110;10&#20493;&#35268;&#27169;&#26356;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#32780;&#26080;&#38656;&#25351;&#20196;&#35843;&#20248;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Instruction tuning&#20351;&#24471;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#26356;&#26377;&#25928;&#22320;&#27867;&#21270;&#65292;&#24182;&#26356;&#22909;&#22320;&#36981;&#24490;&#29992;&#25143;&#24847;&#22270;&#12290;&#28982;&#32780;&#65292;&#33719;&#21462;&#25351;&#20196;&#25968;&#25454;&#25104;&#26412;&#39640;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#37319;&#29992;&#20102;&#35832;&#22914;&#26114;&#36149;&#30340;&#20154;&#24037;&#27880;&#37322;&#12289;&#23384;&#22312;&#23545;&#40784;&#38382;&#39064;&#30340;&#20247;&#21253;&#25968;&#25454;&#38598;&#12289;&#20197;&#21450;&#36890;&#36807;LLMs&#29983;&#25104;&#22122;&#22768;&#31034;&#20363;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;LongForm-C&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#30001;&#21453;&#21521;&#25351;&#20196;&#21019;&#24314;&#12290;&#25105;&#20204;&#20351;&#29992;LLMs&#20026;&#20154;&#31867;&#20889;&#20316;&#35821;&#26009;&#24211;&#31034;&#20363;&#20351;&#29992;&#21453;&#21521;&#25351;&#20196;&#29983;&#25104;&#25351;&#20196;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20174;&#35832;&#22914;C4&#21644;Wikipedia&#30340;&#35821;&#26009;&#24211;&#20013;&#36873;&#25321;&#20102;&#19968;&#32452;&#22810;&#26679;&#24615;&#30340;&#20154;&#31867;&#25776;&#20889;&#25991;&#26723;&#65307;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;LLMs&#20026;&#36825;&#20123;&#25991;&#26723;&#29983;&#25104;&#25351;&#20196;&#12290;&#36825;&#31181;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#20010;&#26356;&#20415;&#23452;&#12289;&#26356;&#24178;&#20928;&#12289;&#36755;&#20986;&#33258;&#28982;&#20197;&#21450;&#36866;&#29992;&#20110;&#38271;&#25991;&#26412;&#29983;&#25104;&#30340;&#25351;&#20196;&#35843;&#20248;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#25925;&#20107;/&#33756;&#35889;&#29983;&#25104;&#21644;&#38271;&#31687;&#38382;&#31572;&#31561;&#20219;&#21153;&#19978;&#20248;&#20110;10&#20493;&#35268;&#27169;&#26356;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#32780;&#26080;&#38656;&#25351;&#20196;&#35843;&#20248;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2304.08460v2 Announce Type: replace-cross Abstract: Instruction tuning enables language models to more effectively generalize and better follow user intent. However, obtaining instruction data is costly and challenging. Prior work employs methods such as expensive human annotation, crowd-sourced datasets with alignment issues, and generating noisy examples via LLMs. We introduce the LongForm-C dataset, which is created by reverse instructions. We generate instructions via LLMs for human-written corpus examples using reverse instructions. First we select a diverse set of human-written documents from corpora such as C4 and Wikipedia; then we generate instructions for these documents via LLMs. This approach provides a cheaper and cleaner instruction-tuning dataset with natural output and one suitable for long text generation. Our models outperform 10x larger language models without instruction tuning on tasks such as story/recipe generation and long-form question answering. Moreover
&lt;/p&gt;</description></item><item><title>&#22240;&#26524;&#28145;&#24230;&#23398;&#20064;&#26159;&#19968;&#31181;&#26032;&#30340;&#20851;&#20110;&#22240;&#26524;&#24615;&#30340;&#24605;&#32771;&#26041;&#24335;&#65292;&#23427;&#32508;&#21512;&#20102;&#21487;&#27979;&#35797;&#30340;&#22240;&#26524;&#30693;&#35782;&#12289;&#21442;&#25968;&#24418;&#24335;&#21644;&#26102;&#38388;&#32500;&#24230;&#65292;&#33021;&#22815;&#24110;&#21161;&#25105;&#20204;&#35299;&#20915;&#23454;&#38469;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2303.02186</link><description>&lt;p&gt;
&#22240;&#26524;&#28145;&#24230;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Causal Deep Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2303.02186
&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#28145;&#24230;&#23398;&#20064;&#26159;&#19968;&#31181;&#26032;&#30340;&#20851;&#20110;&#22240;&#26524;&#24615;&#30340;&#24605;&#32771;&#26041;&#24335;&#65292;&#23427;&#32508;&#21512;&#20102;&#21487;&#27979;&#35797;&#30340;&#22240;&#26524;&#30693;&#35782;&#12289;&#21442;&#25968;&#24418;&#24335;&#21644;&#26102;&#38388;&#32500;&#24230;&#65292;&#33021;&#22815;&#24110;&#21161;&#25105;&#20204;&#35299;&#20915;&#23454;&#38469;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#24615;&#26377;&#26395;&#30495;&#27491;&#25913;&#21464;&#25105;&#20204;&#35299;&#20915;&#35768;&#22810;&#23454;&#38469;&#38382;&#39064;&#30340;&#26041;&#24335;&#12290;&#28982;&#32780;&#65292;&#36804;&#20170;&#20026;&#27490;&#65292;&#22240;&#26524;&#24615;&#30340;&#28508;&#21147;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#21457;&#25381;&#65292;&#22240;&#20026;&#22240;&#26524;&#24615;&#24120;&#24120;&#38656;&#35201;&#26080;&#27861;&#22312;&#23454;&#36341;&#20013;&#36827;&#34892;&#27979;&#35797;&#30340;&#20851;&#38190;&#20551;&#35774;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#26032;&#30340;&#20851;&#20110;&#22240;&#26524;&#24615;&#30340;&#24605;&#32771;&#26041;&#24335;&#8212;&#8212;&#25105;&#20204;&#31216;&#20043;&#20026;&#22240;&#26524;&#28145;&#24230;&#23398;&#20064;&#12290;&#25105;&#20204;&#30340;&#22240;&#26524;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#28085;&#30422;&#20102;&#19977;&#20010;&#26041;&#38754;&#65306;&#65288;1&#65289;&#32467;&#26500;&#32500;&#24230;&#65292;&#23427;&#23558;&#37096;&#20998;&#21487;&#20197;&#27979;&#35797;&#30340;&#22240;&#26524;&#30693;&#35782;&#34701;&#20837;&#65292;&#32780;&#19981;&#26159;&#20551;&#35774;&#24863;&#20852;&#36259;&#30340;&#21464;&#37327;&#20043;&#38388;&#30340;&#22240;&#26524;&#30693;&#35782;&#26159;&#23436;&#25972;&#30340;&#25110;&#23436;&#20840;&#19981;&#23384;&#22312;&#30340;&#65307;&#65288;2&#65289;&#21442;&#25968;&#32500;&#24230;&#65292;&#23427;&#28085;&#30422;&#20102;&#25429;&#25417;&#24863;&#20852;&#36259;&#30340;&#21464;&#37327;&#20043;&#38388;&#20851;&#31995;&#31867;&#22411;&#30340;&#21442;&#25968;&#24418;&#24335;&#65307;&#65288;3&#65289;&#26102;&#38388;&#32500;&#24230;&#65292;&#23427;&#25429;&#25417;&#24863;&#20852;&#36259;&#30340;&#21464;&#37327;&#22914;&#20309;&#22312;&#26102;&#38388;&#19978;&#30456;&#20114;&#20316;&#29992;&#65288;&#21487;&#33021;&#26377;&#22240;&#26524;&#20851;&#31995;&#65289;&#12290;&#22240;&#26524;&#28145;&#24230;&#23398;&#20064;&#20351;&#25105;&#20204;&#33021;&#22815;&#22312;&#21508;&#31181;&#23454;&#38469;&#38382;&#39064;&#19978;&#21462;&#24471;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2303.02186v2 Announce Type: replace-cross Abstract: Causality has the potential to truly transform the way we solve a large number of real-world problems. Yet, so far, its potential largely remains to be unlocked as causality often requires crucial assumptions which cannot be tested in practice. To address this challenge, we propose a new way of thinking about causality -- we call this causal deep learning. Our causal deep learning framework spans three dimensions: (1) a structural dimension, which incorporates partial yet testable causal knowledge rather than assuming either complete or no causal knowledge among the variables of interest; (2) a parametric dimension, which encompasses parametric forms that capture the type of relationships among the variables of interest; and (3) a temporal dimension, which captures exposure times or how the variables of interest interact (possibly causally) over time. Causal deep learning enables us to make progress on a variety of real-world pr
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#26377;&#38480;&#22823;&#23567;&#20915;&#31574;&#26641;&#22312;Markov&#20915;&#31574;&#36807;&#31243;&#65288;MDPs&#65289;&#20013;&#30340;&#20248;&#21270;&#65292;&#24182;&#25552;&#20986;&#20102;OMDTs&#65306;&#26368;&#20248;MDP&#20915;&#31574;&#26641;&#12290;&#36890;&#36807;&#28151;&#21512;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#30452;&#25509;&#26368;&#22823;&#21270;&#20915;&#31574;&#26641;&#30340;&#26399;&#26395;&#25240;&#25187;&#22238;&#25253;&#12290;&#30740;&#31350;&#21457;&#29616;&#29616;&#26377;&#27169;&#20223;&#23398;&#20064;&#25216;&#26415;&#30340;&#26368;&#20248;&#24615;&#24046;&#36317;&#65292;&#24182;&#21457;&#29616;&#23427;&#20204;&#34920;&#29616;&#20026;&#27425;&#20248;&#12290;</title><link>https://arxiv.org/abs/2301.13185</link><description>&lt;p&gt;
Markov&#20915;&#31574;&#36807;&#31243;&#30340;&#26368;&#20248;&#20915;&#31574;&#26641;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Optimal Decision Tree Policies for Markov Decision Processes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2301.13185
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#26377;&#38480;&#22823;&#23567;&#20915;&#31574;&#26641;&#22312;Markov&#20915;&#31574;&#36807;&#31243;&#65288;MDPs&#65289;&#20013;&#30340;&#20248;&#21270;&#65292;&#24182;&#25552;&#20986;&#20102;OMDTs&#65306;&#26368;&#20248;MDP&#20915;&#31574;&#26641;&#12290;&#36890;&#36807;&#28151;&#21512;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#30452;&#25509;&#26368;&#22823;&#21270;&#20915;&#31574;&#26641;&#30340;&#26399;&#26395;&#25240;&#25187;&#22238;&#25253;&#12290;&#30740;&#31350;&#21457;&#29616;&#29616;&#26377;&#27169;&#20223;&#23398;&#20064;&#25216;&#26415;&#30340;&#26368;&#20248;&#24615;&#24046;&#36317;&#65292;&#24182;&#21457;&#29616;&#23427;&#20204;&#34920;&#29616;&#20026;&#27425;&#20248;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#30340;&#35299;&#37322;&#24615;&#23545;&#20110;&#35768;&#22810;&#23454;&#38469;&#20219;&#21153;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#23398;&#20064;&#36825;&#31181;&#21487;&#35299;&#37322;&#30340;&#31574;&#30053;&#26159;&#19968;&#20010;&#22256;&#38590;&#30340;&#38382;&#39064;&#12290;&#29305;&#21035;&#26159;&#20687;&#20915;&#31574;&#26641;&#21644;&#35268;&#21017;&#21015;&#34920;&#36825;&#26679;&#30340;&#22522;&#20110;&#35268;&#21017;&#30340;&#31574;&#30053;&#65292;&#30001;&#20110;&#20854;&#19981;&#21487;&#24494;&#24615;&#65292;&#24456;&#38590;&#36827;&#34892;&#20248;&#21270;&#12290;&#23613;&#31649;&#29616;&#26377;&#25216;&#26415;&#21487;&#20197;&#23398;&#20064;&#21487;&#39564;&#35777;&#30340;&#20915;&#31574;&#26641;&#31574;&#30053;&#65292;&#20294;&#19981;&#33021;&#20445;&#35777;&#23398;&#20064;&#32773;&#29983;&#25104;&#30340;&#20915;&#31574;&#26159;&#26368;&#20248;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#26377;&#38480;&#22823;&#23567;&#20915;&#31574;&#26641;&#22312;Markov&#20915;&#31574;&#36807;&#31243;&#65288;MDPs&#65289;&#20013;&#30340;&#20248;&#21270;&#65292;&#24182;&#25552;&#20986;&#20102;OMDTs&#65306;&#26368;&#20248;MDP&#20915;&#31574;&#26641;&#12290;&#32473;&#23450;&#29992;&#25143;&#23450;&#20041;&#30340;&#22823;&#23567;&#38480;&#21046;&#21644;MDP&#24418;&#24335;&#65292;OMDT&#36890;&#36807;&#28151;&#21512;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#30452;&#25509;&#26368;&#22823;&#21270;&#20915;&#31574;&#26641;&#30340;&#26399;&#26395;&#25240;&#25187;&#22238;&#25253;&#12290;&#36890;&#36807;&#20026;&#19981;&#21516;&#30340;MDP&#35757;&#32451;&#26368;&#20248;&#20915;&#31574;&#26641;&#31574;&#30053;&#65292;&#25105;&#20204;&#22312;&#23454;&#35777;&#19978;&#30740;&#31350;&#20102;&#29616;&#26377;&#27169;&#20223;&#23398;&#20064;&#25216;&#26415;&#30340;&#26368;&#20248;&#24615;&#24046;&#36317;&#65292;&#24182;&#21457;&#29616;&#23427;&#20204;&#34920;&#29616;&#20026;&#27425;&#20248;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2301.13185v2 Announce Type: replace Abstract: Interpretability of reinforcement learning policies is essential for many real-world tasks but learning such interpretable policies is a hard problem. Particularly rule-based policies such as decision trees and rules lists are difficult to optimize due to their non-differentiability. While existing techniques can learn verifiable decision tree policies there is no guarantee that the learners generate a decision that performs optimally. In this work, we study the optimization of size-limited decision trees for Markov Decision Processes (MPDs) and propose OMDTs: Optimal MDP Decision Trees. Given a user-defined size limit and MDP formulation OMDT directly maximizes the expected discounted return for the decision tree using Mixed-Integer Linear Programming. By training optimal decision tree policies for different MDPs we empirically study the optimality gap for existing imitation learning techniques and find that they perform sub-optimall
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#27979;&#35797;&#26102;&#38388;&#28151;&#21512;&#22686;&#24378;&#26469;&#20272;&#35745;&#28145;&#24230;&#23398;&#20064;&#22270;&#20687;&#20998;&#31867;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#30340;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#25968;&#25454;&#19981;&#30830;&#23450;&#24615;&#21644;&#31867;&#21035;&#29305;&#23450;&#19981;&#30830;&#23450;&#24615;&#26469;&#25552;&#39640;&#20934;&#30830;&#24615;&#21644;&#25552;&#20379;&#26356;&#28145;&#20837;&#30340;&#20449;&#24687;&#12290;</title><link>https://arxiv.org/abs/2212.00214</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22270;&#20687;&#20998;&#31867;&#20013;&#30340;&#27979;&#35797;&#26102;&#38388;&#28151;&#21512;&#22686;&#24378;&#26041;&#27861;&#29992;&#20110;&#25968;&#25454;&#21644;&#31867;&#21035;&#29305;&#23450;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Test-Time Mixup Augmentation for Data and Class-Specific Uncertainty Estimation in Deep Learning Image Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2212.00214
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#27979;&#35797;&#26102;&#38388;&#28151;&#21512;&#22686;&#24378;&#26469;&#20272;&#35745;&#28145;&#24230;&#23398;&#20064;&#22270;&#20687;&#20998;&#31867;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#30340;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#25968;&#25454;&#19981;&#30830;&#23450;&#24615;&#21644;&#31867;&#21035;&#29305;&#23450;&#19981;&#30830;&#23450;&#24615;&#26469;&#25552;&#39640;&#20934;&#30830;&#24615;&#21644;&#25552;&#20379;&#26356;&#28145;&#20837;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#30340;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#23545;&#20110;&#20248;&#21270;&#23398;&#20064;&#25928;&#29575;&#21644;&#35780;&#20272;&#32593;&#32476;&#39044;&#27979;&#30340;&#21487;&#38752;&#24615;&#38750;&#24120;&#26377;&#20215;&#20540;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#27979;&#35797;&#26102;&#38388;&#28151;&#21512;&#22686;&#24378;&#65288;TTMA&#65289;&#26469;&#20272;&#35745;&#28145;&#24230;&#23398;&#20064;&#22270;&#20687;&#20998;&#31867;&#20013;&#19981;&#30830;&#23450;&#24615;&#30340;&#26041;&#27861;&#12290;&#20026;&#20102;&#25913;&#21892;&#29616;&#26377;&#30830;&#23450;&#24615;&#19981;&#30830;&#23450;&#24615;&#21306;&#20998;&#27491;&#30830;&#21644;&#38169;&#35823;&#39044;&#27979;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;TTMA&#25968;&#25454;&#19981;&#30830;&#23450;&#24615;&#65288;TTMA-DU&#65289;&#65292;&#36890;&#36807;&#23545;&#27979;&#35797;&#25968;&#25454;&#36827;&#34892;&#28151;&#21512;&#22686;&#24378;&#24182;&#27979;&#37327;&#39044;&#27979;&#26631;&#31614;&#30452;&#26041;&#22270;&#30340;&#29109;&#26469;&#34913;&#37327;&#12290;&#38500;&#20102;TTMA-DU&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;TTMA&#31867;&#21035;&#29305;&#23450;&#19981;&#30830;&#23450;&#24615;&#65288;TTMA-CSU&#65289;&#65292;&#23427;&#25429;&#25417;&#21040;&#20102;&#19982;&#21333;&#20010;&#31867;&#21035;&#30456;&#20851;&#30340;&#30830;&#23450;&#24615;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#20851;&#20110;&#35757;&#32451;&#32593;&#32476;&#20869;&#31867;&#21035;&#28151;&#28102;&#21644;&#31867;&#21035;&#30456;&#20284;&#24615;&#30340;&#35265;&#35299;&#12290;&#25105;&#20204;&#22312;ISIC-18&#30382;&#32932;&#30149;&#35786;&#26029;&#25968;&#25454;&#38598;&#21644;CIFAR-100&#30495;&#23454;&#19990;&#30028;&#22270;&#20687;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2212.00214v3 Announce Type: replace-cross Abstract: Uncertainty estimation of trained deep learning networks is valuable for optimizing learning efficiency and evaluating the reliability of network predictions. In this paper, we propose a method for estimating uncertainty in deep learning image classification using test-time mixup augmentation (TTMA). To improve the ability to distinguish correct and incorrect predictions in existing aleatoric uncertainty, we introduce TTMA data uncertainty (TTMA-DU) by applying mixup augmentation to test data and measuring the entropy of the predicted label histogram. In addition to TTMA-DU, we propose TTMA class-specific uncertainty (TTMA-CSU), which captures aleatoric uncertainty specific to individual classes and provides insight into class confusion and class similarity within the trained network. We validate our proposed methods on the ISIC-18 skin lesion diagnosis dataset and the CIFAR-100 real-world image classification dataset. Our exper
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#25351;&#23548;&#19979;&#30340;&#20316;&#19994;&#36710;&#38388;&#35843;&#24230;&#25913;&#36827;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#37319;&#29992;&#22270;&#34920;&#31034;&#27861;&#26469;&#32534;&#30721;&#23436;&#25972;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#35774;&#35745;&#20102;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#31034;&#26041;&#26696;&#21644;&#26032;&#39062;&#30340;&#28040;&#24687;&#20256;&#36882;&#26426;&#21046;&#65292;&#20197;&#25552;&#39640;&#24615;&#33021;&#21644;&#21152;&#24555;&#35299;&#20915;&#26041;&#26696;&#35780;&#20272;&#12290;</title><link>https://arxiv.org/abs/2211.10936</link><description>&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#25351;&#23548;&#19979;&#30340;&#20316;&#19994;&#36710;&#38388;&#35843;&#24230;&#25913;&#36827;&#21551;&#21457;&#24335;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Deep Reinforcement Learning Guided Improvement Heuristic for Job Shop Scheduling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2211.10936
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#25351;&#23548;&#19979;&#30340;&#20316;&#19994;&#36710;&#38388;&#35843;&#24230;&#25913;&#36827;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#37319;&#29992;&#22270;&#34920;&#31034;&#27861;&#26469;&#32534;&#30721;&#23436;&#25972;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#35774;&#35745;&#20102;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#31034;&#26041;&#26696;&#21644;&#26032;&#39062;&#30340;&#28040;&#24687;&#20256;&#36882;&#26426;&#21046;&#65292;&#20197;&#25552;&#39640;&#24615;&#33021;&#21644;&#21152;&#24555;&#35299;&#20915;&#26041;&#26696;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#30740;&#31350;&#23558;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#24212;&#29992;&#20110;&#35299;&#20915;&#20316;&#19994;&#36710;&#38388;&#35843;&#24230;&#38382;&#39064;&#26102;&#65292;&#20027;&#35201;&#20851;&#27880;&#30340;&#26159;&#26500;&#24314;&#21551;&#21457;&#24335;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#20182;&#20204;&#30340;&#24615;&#33021;&#20173;&#36828;&#31163;&#26368;&#20248;&#65292;&#20027;&#35201;&#26159;&#22240;&#20026;&#24213;&#23618;&#22270;&#34920;&#31034;&#26041;&#26696;&#19981;&#36866;&#21512;&#23545;&#27599;&#20010;&#26500;&#24314;&#27493;&#39588;&#20013;&#30340;&#37096;&#20998;&#35299;&#36827;&#34892;&#24314;&#27169;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;DRL&#25351;&#23548;&#19979;&#30340;&#20316;&#19994;&#36710;&#38388;&#35843;&#24230;&#25913;&#36827;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#37319;&#29992;&#22270;&#34920;&#31034;&#27861;&#26469;&#32534;&#30721;&#23436;&#25972;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#31034;&#26041;&#26696;&#65292;&#21253;&#21547;&#20004;&#20010;&#27169;&#22359;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#25429;&#25417;&#25913;&#36827;&#36807;&#31243;&#20013;&#36935;&#21040;&#30340;&#21160;&#24577;&#25299;&#25169;&#21644;&#19981;&#21516;&#31867;&#22411;&#30340;&#33410;&#28857;&#30340;&#20449;&#24687;&#12290;&#20026;&#20102;&#22312;&#25913;&#36827;&#36807;&#31243;&#20013;&#21152;&#24555;&#35299;&#20915;&#26041;&#26696;&#35780;&#20272;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28040;&#24687;&#20256;&#36882;&#26426;&#21046;&#65292;&#21487;&#20197;&#21516;&#26102;&#35780;&#20272;&#22810;&#20010;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#19982;&#38382;&#39064;&#35268;&#27169;&#21576;&#32447;&#24615;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2211.10936v3 Announce Type: replace-cross Abstract: Recent studies in using deep reinforcement learning (DRL) to solve Job-shop scheduling problems (JSSP) focus on construction heuristics. However, their performance is still far from optimality, mainly because the underlying graph representation scheme is unsuitable for modelling partial solutions at each construction step. This paper proposes a novel DRL-guided improvement heuristic for solving JSSP, where graph representation is employed to encode complete solutions. We design a Graph Neural-Network-based representation scheme, consisting of two modules to effectively capture the information of dynamic topology and different types of nodes in graphs encountered during the improvement process. To speed up solution evaluation during improvement, we present a novel message-passing mechanism that can evaluate multiple solutions simultaneously. We prove that the computational complexity of our method scales linearly with problem siz
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27010;&#24565;&#26032;&#39062;&#30340;&#32852;&#37030;&#23398;&#20064;&#35774;&#32622;&#65292;&#21363;&#20855;&#26377;&#28151;&#21512;&#31867;&#22411;&#26631;&#31614;&#30340;&#32852;&#37030;&#23398;&#20064;&#65292;&#22312;&#20854;&#20013;&#19981;&#21516;&#30340;&#20013;&#24515;&#21487;&#20197;&#20351;&#29992;&#19981;&#21516;&#30340;&#26631;&#31614;&#20934;&#21017;&#12290;&#20026;&#20102;&#26377;&#25928;&#22320;&#35757;&#32451;&#20855;&#26377;&#28151;&#21512;&#31867;&#22411;&#26631;&#31614;&#30340;&#27169;&#22411;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#29702;&#35770;&#25351;&#23548;&#21644;&#27169;&#22411;&#26080;&#20851;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2210.02042</link><description>&lt;p&gt;
FedMT: &#28151;&#21512;&#31867;&#22411;&#26631;&#31614;&#30340;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
FedMT: Federated Learning with Mixed-type Labels
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2210.02042
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27010;&#24565;&#26032;&#39062;&#30340;&#32852;&#37030;&#23398;&#20064;&#35774;&#32622;&#65292;&#21363;&#20855;&#26377;&#28151;&#21512;&#31867;&#22411;&#26631;&#31614;&#30340;&#32852;&#37030;&#23398;&#20064;&#65292;&#22312;&#20854;&#20013;&#19981;&#21516;&#30340;&#20013;&#24515;&#21487;&#20197;&#20351;&#29992;&#19981;&#21516;&#30340;&#26631;&#31614;&#20934;&#21017;&#12290;&#20026;&#20102;&#26377;&#25928;&#22320;&#35757;&#32451;&#20855;&#26377;&#28151;&#21512;&#31867;&#22411;&#26631;&#31614;&#30340;&#27169;&#22411;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#29702;&#35770;&#25351;&#23548;&#21644;&#27169;&#22411;&#26080;&#20851;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20013;&#65292;&#20998;&#31867;&#22120;&#65288;&#20363;&#22914;&#28145;&#24230;&#32593;&#32476;&#65289;&#22312;&#22810;&#20010;&#20013;&#24515;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#32780;&#26080;&#38656;&#22312;&#36825;&#20123;&#20013;&#24515;&#20043;&#38388;&#20132;&#25442;&#25968;&#25454;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#26679;&#26412;&#25928;&#29575;&#12290;&#22312;&#20256;&#32479;&#30340;FL&#35774;&#32622;&#20013;&#65292;&#36890;&#24120;&#22312;&#25152;&#26377;&#21442;&#19982;&#35757;&#32451;&#30340;&#20013;&#24515;&#20013;&#37319;&#29992;&#30456;&#21516;&#30340;&#26631;&#31614;&#20934;&#21017;&#12290;&#36825;&#20010;&#38480;&#21046;&#26497;&#22823;&#22320;&#38480;&#21046;&#20102;FL&#30340;&#36866;&#29992;&#24615;&#12290;&#20363;&#22914;&#65292;&#22312;&#30142;&#30149;&#35786;&#26029;&#20013;&#20351;&#29992;&#30340;&#26631;&#20934;&#24456;&#21487;&#33021;&#22312;&#20020;&#24202;&#20013;&#24515;&#20043;&#38388;&#23384;&#22312;&#24046;&#24322;&#65292;&#36825;&#19982;&#20256;&#32479;FL&#30340;&#35774;&#32622;&#19981;&#21305;&#37197;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#37325;&#35201;&#20294;&#23578;&#26410;&#20805;&#20998;&#25506;&#32034;&#30340;FL&#35774;&#32622;&#65292;&#21363;&#20855;&#26377;&#28151;&#21512;&#31867;&#22411;&#26631;&#31614;&#30340;FL&#65292;&#20854;&#20013;&#21508;&#20010;&#20013;&#24515;&#21487;&#20197;&#20351;&#29992;&#19981;&#21516;&#30340;&#26631;&#31614;&#20934;&#21017;&#65292;&#20174;&#32780;&#23548;&#33268;&#20013;&#24515;&#38388;&#26631;&#31614;&#31354;&#38388;&#30340;&#24046;&#24322;&#65292;&#24182;&#23545;&#20026;&#20256;&#32479;&#35774;&#32622;&#35774;&#35745;&#30340;&#29616;&#26377;FL&#26041;&#27861;&#25552;&#20986;&#20102;&#25361;&#25112;&#12290;&#20026;&#20102;&#26377;&#25928;&#32780;&#39640;&#25928;&#22320;&#35757;&#32451;&#20855;&#26377;&#28151;&#21512;&#31867;&#22411;&#26631;&#31614;&#30340;&#27169;&#22411;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29702;&#35770;&#25351;&#23548;&#21644;&#27169;&#22411;&#26080;&#20851;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
arXiv:2210.02042v3 Announce Type: replace-cross Abstract: In federated learning (FL), classifiers (e.g., deep networks) are trained on datasets from multiple centers without exchanging data across them, and thus improves sample efficiency. In the classical setting of FL, the same labeling criterion is usually employed across all centers being involved in training. This constraint greatly limits the applicability of FL. For example, standards used for disease diagnosis are more likely to be different across clinical centers, which mismatches the classical FL setting. In this paper, we consider an important yet under-explored setting of FL, namely FL with mixed-type labels where different labeling criteria can be employed by various centers, leading to inter-center label space differences and challenging existing FL methods designed for the classical setting. To effectively and efficiently train models with mixed-type labels, we propose a theory-guided and model-agnostic approach that ca
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20351;&#29992;YOLOv8&#12289;DeiT&#21644;SimCLR&#22312;&#24076;&#33098;&#32440;&#33609;&#20013;&#36827;&#34892;&#23383;&#31526;&#26816;&#27979;&#21644;&#35782;&#21035;&#31454;&#36187;&#65292;&#22312;&#35782;&#21035;&#25361;&#25112;&#20013;&#33719;&#24471;&#20102;42.2%&#30340;mAP&#65292;&#24182;&#22312;&#26816;&#27979;&#25361;&#25112;&#20013;&#20197;51.4%&#30340;&#24179;&#22343;&#31934;&#24230;&#33719;&#24471;&#20102;&#20122;&#20891;&#12290;</title><link>http://arxiv.org/abs/2401.12513</link><description>&lt;p&gt;
&#20351;&#29992;YOLOv8&#12289;DeiT&#21644;SimCLR&#22312;&#24076;&#33098;&#32440;&#33609;&#36825;&#26816;&#27979;&#21644;&#35782;&#21035;&#23383;&#31526;
&lt;/p&gt;
&lt;p&gt;
Detecting and recognizing characters in Greek papyri with YOLOv8, DeiT and SimCLR. (arXiv:2401.12513v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12513
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20351;&#29992;YOLOv8&#12289;DeiT&#21644;SimCLR&#22312;&#24076;&#33098;&#32440;&#33609;&#20013;&#36827;&#34892;&#23383;&#31526;&#26816;&#27979;&#21644;&#35782;&#21035;&#31454;&#36187;&#65292;&#22312;&#35782;&#21035;&#25361;&#25112;&#20013;&#33719;&#24471;&#20102;42.2%&#30340;mAP&#65292;&#24182;&#22312;&#26816;&#27979;&#25361;&#25112;&#20013;&#20197;51.4%&#30340;&#24179;&#22343;&#31934;&#24230;&#33719;&#24471;&#20102;&#20122;&#20891;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#32440;&#33609;&#25163;&#31295;&#30340;&#22270;&#20687;&#20013;&#20998;&#31163;&#21644;&#35782;&#21035;&#21333;&#20010;&#23383;&#31526;&#30340;&#33021;&#21147;&#20026;&#25968;&#23383;&#20998;&#26512;&#25552;&#20379;&#20102;&#20016;&#23500;&#30340;&#26426;&#20250;&#12290;&#22240;&#27492;&#65292;&#8220;ICDAR 2023&#24180;&#24076;&#33098;&#32440;&#33609;&#19978;&#23383;&#31526;&#26816;&#27979;&#21644;&#35782;&#21035;&#31454;&#36187;&#8221;&#20316;&#20026;&#31532;17&#23626;&#22269;&#38469;&#25991;&#20214;&#20998;&#26512;&#21644;&#35782;&#21035;&#20250;&#35758;&#30340;&#19968;&#37096;&#20998;&#20030;&#34892;&#12290;&#26412;&#25991;&#35752;&#35770;&#20102;&#25105;&#20204;&#22312;&#27604;&#36187;&#20013;&#30340;&#25552;&#20132;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#32452;YOLOv8&#27169;&#22411;&#26469;&#26816;&#27979;&#21644;&#20998;&#31867;&#21333;&#20010;&#23383;&#31526;&#65292;&#24182;&#37319;&#29992;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#26041;&#27861;&#26469;&#23436;&#21892;&#23383;&#31526;&#39044;&#27979;&#65292;&#21253;&#25324;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;DeiT&#26041;&#27861;&#21644;&#20351;&#29992;SimCLR&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#22312;&#22823;&#37327;&#26080;&#26631;&#31614;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;ResNet-50&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#25552;&#20132;&#22312;&#35782;&#21035;&#25361;&#25112;&#20013;&#33719;&#24471;&#20102;42.2%&#30340;mAP&#65292;&#24182;&#22312;&#26816;&#27979;&#25361;&#25112;&#20013;&#20197;51.4%&#30340;&#24179;&#22343;&#31934;&#24230;&#33719;&#24471;&#20102;&#20122;&#20891;&#12290;&#22312;&#26356;&#23485;&#26494;&#30340;iou&#38408;&#20540;&#20026;0.5&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#26368;&#39640;&#30340;&#24179;&#22343;&#31934;&#24230;&#21644;&#24179;&#22343;.
&lt;/p&gt;
&lt;p&gt;
The capacity to isolate and recognize individual characters from facsimile images of papyrus manuscripts yields rich opportunities for digital analysis. For this reason the `ICDAR 2023 Competition on Detection and Recognition of Greek Letters on Papyri' was held as part of the 17th International Conference on Document Analysis and Recognition. This paper discusses our submission to the competition. We used an ensemble of YOLOv8 models to detect and classify individual characters and employed two different approaches for refining the character predictions, including a transformer based DeiT approach and a ResNet-50 model trained on a large corpus of unlabelled data using SimCLR, a self-supervised learning method. Our submission won the recognition challenge with a mAP of 42.2%, and was runner-up in the detection challenge with a mean average precision (mAP) of 51.4%. At the more relaxed intersection over union threshold of 0.5, we achieved the highest mean average precision and mean ave
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#35843;&#26597;&#20102;&#19968;&#31181;&#21517;&#20026;&#26816;&#32034;&#31034;&#33539;&#30340;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#20351;&#29992;&#29305;&#23450;&#20110;&#36755;&#20837;&#26597;&#35810;&#30340;&#31034;&#33539;&#26469;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#23569;&#37327;&#26679;&#26412;&#24773;&#22659;&#23398;&#20064;&#65288;ICL&#65289;&#33021;&#21147;&#12290;&#36825;&#31181;&#26041;&#27861;&#19981;&#20165;&#25552;&#39640;&#20102;&#23398;&#20064;&#25928;&#29575;&#21644;&#21487;&#25193;&#23637;&#24615;&#65292;&#36824;&#20943;&#23569;&#20102;&#25163;&#21160;&#31034;&#20363;&#36873;&#25321;&#20013;&#30340;&#20559;&#35265;&#12290;</title><link>http://arxiv.org/abs/2401.11624</link><description>&lt;p&gt;
&#36890;&#36807;&#26816;&#32034;&#31034;&#33539;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#35821;&#35328;&#27169;&#22411;&#65306;&#19968;&#39033;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
In-context Learning with Retrieved Demonstrations for Language Models: A Survey. (arXiv:2401.11624v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11624
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#35843;&#26597;&#20102;&#19968;&#31181;&#21517;&#20026;&#26816;&#32034;&#31034;&#33539;&#30340;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#20351;&#29992;&#29305;&#23450;&#20110;&#36755;&#20837;&#26597;&#35810;&#30340;&#31034;&#33539;&#26469;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#23569;&#37327;&#26679;&#26412;&#24773;&#22659;&#23398;&#20064;&#65288;ICL&#65289;&#33021;&#21147;&#12290;&#36825;&#31181;&#26041;&#27861;&#19981;&#20165;&#25552;&#39640;&#20102;&#23398;&#20064;&#25928;&#29575;&#21644;&#21487;&#25193;&#23637;&#24615;&#65292;&#36824;&#20943;&#23569;&#20102;&#25163;&#21160;&#31034;&#20363;&#36873;&#25321;&#20013;&#30340;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#24050;&#23637;&#31034;&#20986;&#21331;&#36234;&#30340;&#33021;&#21147;&#65292;&#21487;&#20197;&#22312;&#36755;&#20837;&#19978;&#19979;&#25991;&#20013;&#36827;&#34892;&#23569;&#37327;&#26679;&#26412;&#30340;&#24773;&#22659;&#23398;&#20064;&#65288;ICL&#65289;&#65292;&#24182;&#22312;&#26032;&#20219;&#21153;&#19978;&#20855;&#26377;&#36866;&#24212;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#27169;&#22411;&#30340;ICL&#33021;&#21147;&#23545;&#20110;&#23569;&#26679;&#26412;&#31034;&#33539;&#30340;&#36873;&#25321;&#26159;&#25935;&#24863;&#30340;&#12290;&#26368;&#36817;&#30340;&#19968;&#39033;&#30740;&#31350;&#36827;&#23637;&#26159;&#26816;&#32034;&#38024;&#23545;&#27599;&#20010;&#36755;&#20837;&#26597;&#35810;&#23450;&#21046;&#30340;&#31034;&#33539;&#12290;&#31034;&#33539;&#26816;&#32034;&#30340;&#23454;&#29616;&#30456;&#23545;&#31616;&#21333;&#65292;&#21033;&#29992;&#29616;&#26377;&#30340;&#25968;&#25454;&#24211;&#21644;&#26816;&#32034;&#31995;&#32479;&#12290;&#36825;&#19981;&#20165;&#25552;&#39640;&#20102;&#23398;&#20064;&#36807;&#31243;&#30340;&#25928;&#29575;&#21644;&#21487;&#25193;&#23637;&#24615;&#65292;&#32780;&#19988;&#24050;&#32463;&#35777;&#26126;&#21487;&#20197;&#20943;&#23569;&#25163;&#21160;&#31034;&#20363;&#36873;&#25321;&#20013;&#30340;&#20559;&#35265;&#12290;&#37492;&#20110;&#20196;&#20154;&#40723;&#33310;&#30340;&#32467;&#26524;&#21644;&#22312;&#26816;&#32034;&#31034;&#33539;&#30340;ICL&#26041;&#38754;&#19981;&#26029;&#22686;&#38271;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#30740;&#31350;&#32508;&#36848;&#12290;&#22312;&#36825;&#39033;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#35752;&#35770;&#21644;&#27604;&#36739;&#20102;&#26816;&#32034;&#27169;&#22411;&#30340;&#19981;&#21516;&#35774;&#35745;&#36873;&#25321;&#65292;&#26816;&#32034;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Language models, especially pre-trained large language models, have showcased remarkable abilities as few-shot in-context learners (ICL), adept at adapting to new tasks with just a few demonstrations in the input context. However, the model's ability to perform ICL is sensitive to the choice of the few-shot demonstrations. Instead of using a fixed set of demonstrations, one recent development is to retrieve demonstrations tailored to each input query. The implementation of demonstration retrieval is relatively straightforward, leveraging existing databases and retrieval systems. This not only improves the efficiency and scalability of the learning process but also has been shown to reduce biases inherent in manual example selection. In light of the encouraging results and growing research in ICL with retrieved demonstrations, we conduct an extensive review of studies in this area. In this survey, we discuss and compare different design choices for retrieval models, retrieval training p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;LLMs&#30340;&#26032;&#26041;&#27861;&#65292;&#23558;&#26102;&#38388;&#30693;&#35782;&#22270;&#34917;&#20840;&#20219;&#21153;&#27010;&#24565;&#21270;&#20026;&#21382;&#21490;&#20107;&#20214;&#38142;&#20013;&#30340;&#20107;&#20214;&#29983;&#25104;&#20219;&#21153;&#12290;&#36890;&#36807;&#24341;&#20837;&#39640;&#25928;&#30340;&#24494;&#35843;&#26041;&#27861;&#21644;&#32467;&#26500;&#21270;&#21382;&#21490;&#25968;&#25454;&#22686;&#24378;&#65292;&#20197;&#21450;&#25972;&#21512;&#21453;&#21521;&#30693;&#35782;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#22810;&#20010;&#25351;&#26631;&#19978;&#20248;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#65292;&#21462;&#24471;&#20102;SOTA&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.06072</link><description>&lt;p&gt;
&#21382;&#21490;&#38142;&#30340;&#38142;&#36335;&#39044;&#27979;&#19982;&#23398;&#20064;&#65306;&#22522;&#20110;LLMs&#30340;&#26102;&#38388;&#30693;&#35782;&#22270;&#34917;&#20840;
&lt;/p&gt;
&lt;p&gt;
Chain of History: Learning and Forecasting with LLMs for Temporal Knowledge Graph Completion. (arXiv:2401.06072v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06072
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;LLMs&#30340;&#26032;&#26041;&#27861;&#65292;&#23558;&#26102;&#38388;&#30693;&#35782;&#22270;&#34917;&#20840;&#20219;&#21153;&#27010;&#24565;&#21270;&#20026;&#21382;&#21490;&#20107;&#20214;&#38142;&#20013;&#30340;&#20107;&#20214;&#29983;&#25104;&#20219;&#21153;&#12290;&#36890;&#36807;&#24341;&#20837;&#39640;&#25928;&#30340;&#24494;&#35843;&#26041;&#27861;&#21644;&#32467;&#26500;&#21270;&#21382;&#21490;&#25968;&#25454;&#22686;&#24378;&#65292;&#20197;&#21450;&#25972;&#21512;&#21453;&#21521;&#30693;&#35782;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#22810;&#20010;&#25351;&#26631;&#19978;&#20248;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#65292;&#21462;&#24471;&#20102;SOTA&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#30693;&#35782;&#22270;&#34917;&#20840;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#20854;&#36890;&#36807;&#21033;&#29992;&#24050;&#24314;&#31435;&#30340;&#26102;&#38388;&#32467;&#26500;&#30693;&#35782;&#26469;&#39044;&#27979;&#26410;&#26469;&#26102;&#38388;&#25139;&#19978;&#32570;&#22833;&#30340;&#20107;&#20214;&#38142;&#25509;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#23558;&#26102;&#38388;&#38142;&#36335;&#39044;&#27979;&#27010;&#24565;&#21270;&#20026;&#21382;&#21490;&#20107;&#20214;&#38142;&#20013;&#30340;&#20107;&#20214;&#29983;&#25104;&#20219;&#21153;&#65292;&#21033;&#29992;LLMs&#30340;&#24378;&#22823;&#29983;&#25104;&#33021;&#21147;&#12290;&#25105;&#20204;&#37319;&#29992;&#39640;&#25928;&#30340;&#24494;&#35843;&#26041;&#27861;&#65292;&#20351;LLMs&#36866;&#24212;&#29305;&#23450;&#30340;&#22270;&#25991;&#20449;&#24687;&#21644;&#22312;&#26102;&#38388;&#32447;&#20013;&#21457;&#29616;&#30340;&#27169;&#24335;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#22522;&#20110;&#32467;&#26500;&#30340;&#21382;&#21490;&#25968;&#25454;&#22686;&#24378;&#21644;&#21453;&#21521;&#30693;&#35782;&#30340;&#25972;&#21512;&#65292;&#20197;&#22686;&#24378;LLMs&#23545;&#32467;&#26500;&#20449;&#24687;&#30340;&#24847;&#35782;&#65292;&#20174;&#32780;&#25552;&#39640;&#20854;&#25512;&#29702;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#24191;&#27867;&#20351;&#29992;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35814;&#23613;&#30340;&#23454;&#39564;&#65292;&#21457;&#29616;&#25105;&#20204;&#24494;&#35843;&#30340;&#27169;&#22411;&#22312;&#22810;&#20010;&#25351;&#26631;&#19978;&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#20110;&#23884;&#20837;&#30340;&#27169;&#22411;&#65292;&#21462;&#24471;&#20102;SOTA&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#20805;&#20998;&#30340;&#28040;&#34701;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Temporal Knowledge Graph Completion (TKGC) is a challenging task of predicting missing event links at future timestamps by leveraging established temporal structural knowledge. Given the formidable generative capabilities inherent in LLMs (LLMs), this paper proposes a novel approach to conceptualize temporal link prediction as an event generation task within the context of a historical event chain. We employ efficient fine-tuning methods to make LLMs adapt to specific graph textual information and patterns discovered in temporal timelines. Furthermore, we introduce structure-based historical data augmentation and the integration of reverse knowledge to emphasize LLMs' awareness of structural information, thereby enhancing their reasoning capabilities. We conduct thorough experiments on multiple widely used datasets and find that our fine-tuned model outperforms existing embedding-based models on multiple metrics, achieving SOTA results. We also carry out sufficient ablation experiments
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#30456;&#26426;&#38519;&#38449;&#22270;&#20687;&#30340;&#32467;&#26500;&#21270;&#19978;&#19979;&#25991;&#65292;&#25552;&#39640;&#20854;&#22312;&#29289;&#31181;&#35782;&#21035;&#20219;&#21153;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#35299;&#20915;&#20102;&#25968;&#25454;&#31232;&#32570;&#21644;&#27867;&#21270;&#33021;&#21147;&#22686;&#24378;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.00608</link><description>&lt;p&gt;
&#23558;&#19978;&#19979;&#25991;&#24102;&#22238;&#26469;&#65306;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#35889;&#19978;&#30340;&#30456;&#26426;&#38519;&#38449;&#29289;&#31181;&#35782;&#21035;&#20316;&#20026;&#38142;&#25509;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Bringing Back the Context: Camera Trap Species Identification as Link Prediction on Multimodal Knowledge Graphs. (arXiv:2401.00608v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.00608
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#30456;&#26426;&#38519;&#38449;&#22270;&#20687;&#30340;&#32467;&#26500;&#21270;&#19978;&#19979;&#25991;&#65292;&#25552;&#39640;&#20854;&#22312;&#29289;&#31181;&#35782;&#21035;&#20219;&#21153;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#35299;&#20915;&#20102;&#25968;&#25454;&#31232;&#32570;&#21644;&#27867;&#21270;&#33021;&#21147;&#22686;&#24378;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30456;&#26426;&#38519;&#38449;&#22312;&#21160;&#29289;&#29983;&#24577;&#23398;&#20013;&#26159;&#23453;&#36149;&#30340;&#24037;&#20855;&#65292;&#29992;&#20110;&#29983;&#29289;&#22810;&#26679;&#24615;&#30417;&#27979;&#21644;&#20445;&#25252;&#12290;&#28982;&#32780;&#65292;&#25361;&#25112;&#22914;&#22312;&#26032;&#30340;&#26410;&#30693;&#20301;&#32622;&#37096;&#32626;&#26102;&#30340;&#31967;&#31957;&#27867;&#21270;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;&#22270;&#20687;&#33258;&#28982;&#19982;&#21487;&#33021;&#22312;&#19981;&#21516;&#27169;&#24577;&#19979;&#30340;&#24322;&#36136;&#19978;&#19979;&#25991;&#30456;&#20851;&#32852;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#19982;&#30456;&#26426;&#38519;&#38449;&#22270;&#20687;&#30456;&#20851;&#32852;&#30340;&#32467;&#26500;&#21270;&#19978;&#19979;&#25991;&#65292;&#25913;&#21892;&#22312;&#30456;&#26426;&#38519;&#38449;&#20013;&#29289;&#31181;&#35782;&#21035;&#36825;&#20010;&#20219;&#21153;&#30340;&#36229;&#20986;&#20998;&#24067;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#20363;&#22914;&#65292;&#19968;&#24352;&#37326;&#29983;&#21160;&#29289;&#30340;&#29031;&#29255;&#21487;&#33021;&#19982;&#25293;&#25668;&#22320;&#28857;&#21644;&#26102;&#38388;&#20197;&#21450;&#20851;&#20110;&#21160;&#29289;&#29289;&#31181;&#30340;&#32467;&#26500;&#21270;&#29983;&#29289;&#23398;&#30693;&#35782;&#30456;&#20851;&#32852;&#12290;&#34429;&#28982;&#29616;&#26377;&#30340;&#24037;&#20316;&#36890;&#24120;&#24573;&#35270;&#36825;&#19968;&#28857;&#65292;&#20294;&#23558;&#36825;&#26679;&#30340;&#19978;&#19979;&#25991;&#24102;&#22238;&#26469;&#21487;&#20197;&#24102;&#26469;&#19968;&#20123;&#28508;&#22312;&#30340;&#22909;&#22788;&#65292;&#22914;&#35299;&#20915;&#25968;&#25454;&#31232;&#32570;&#21644;&#22686;&#24378;&#27867;&#21270;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#26377;&#25928;&#22320;&#23558;&#36825;&#26679;&#30340;&#24322;&#36136;&#19978;&#19979;&#25991;&#25972;&#21512;&#21040;&#35270;&#35273;&#39046;&#22495;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Camera traps are valuable tools in animal ecology for biodiversity monitoring and conservation. However, challenges like poor generalization to deployment at new unseen locations limit their practical application. Images are naturally associated with heterogeneous forms of context possibly in different modalities. In this work, we leverage the structured context associated with the camera trap images to improve out-of-distribution generalization for the task of species identification in camera traps. For example, a photo of a wild animal may be associated with information about where and when it was taken, as well as structured biology knowledge about the animal species. While typically overlooked by existing work, bringing back such context offers several potential benefits for better image understanding, such as addressing data scarcity and enhancing generalization. However, effectively integrating such heterogeneous context into the visual domain is a challenging problem. To address
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#24341;&#20837;&#21253;&#25324;&#26102;&#38388;&#29305;&#24449;&#30340;&#22810;&#27969;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#32463;&#36807;&#35270;&#39057;&#35757;&#32451;&#30340;&#40065;&#26834;&#24615;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#35757;&#32451;&#20013;&#21253;&#25324;&#35270;&#39057;&#21644;&#26102;&#38388;&#27969;&#65292;&#20943;&#23569;&#20102;&#22270;&#20687;&#21644;&#35270;&#39057;&#29702;&#35299;&#20219;&#21153;&#30340;&#20934;&#30830;&#24615;&#19979;&#38477;&#29575;&#12290;</title><link>http://arxiv.org/abs/2311.00800</link><description>&lt;p&gt;
&#36229;&#36234;&#38745;&#27490;&#22270;&#20687;&#65306;&#24378;&#22823;&#30340;&#22810;&#27969;&#26102;&#31354;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Beyond Still Images: Robust Multi-Stream Spatiotemporal Networks. (arXiv:2311.00800v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00800
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#24341;&#20837;&#21253;&#25324;&#26102;&#38388;&#29305;&#24449;&#30340;&#22810;&#27969;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#32463;&#36807;&#35270;&#39057;&#35757;&#32451;&#30340;&#40065;&#26834;&#24615;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#35757;&#32451;&#20013;&#21253;&#25324;&#35270;&#39057;&#21644;&#26102;&#38388;&#27969;&#65292;&#20943;&#23569;&#20102;&#22270;&#20687;&#21644;&#35270;&#39057;&#29702;&#35299;&#20219;&#21153;&#30340;&#20934;&#30830;&#24615;&#19979;&#38477;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35270;&#35273;&#30340;&#19968;&#20010;&#23450;&#20041;&#29305;&#24449;&#26159;&#20854;&#33021;&#22815;&#25215;&#21463;&#21508;&#31181;&#36755;&#20837;&#21464;&#21270;&#65292;&#20174;&#32780;&#21019;&#24314;&#21608;&#22260;&#29615;&#22659;&#30340;&#19981;&#21464;&#34920;&#31034;&#12290;&#34429;&#28982;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#23545;&#26576;&#20123;&#24418;&#24335;&#30340;&#31354;&#38388;&#36755;&#20837;&#21464;&#21270;&#20855;&#26377;&#38887;&#24615;&#65292;&#20294;&#31354;&#38388;&#21644;&#26102;&#38388;&#26041;&#38754;&#30340;&#20462;&#25913;&#21487;&#20197;&#26174;&#30528;&#24433;&#21709;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#35270;&#39057;&#20869;&#23481;&#30340;&#34920;&#31034;&#12290;&#21463;&#33258;&#28982;&#35270;&#35273;&#23545;&#36755;&#20837;&#21464;&#21270;&#30340;&#38887;&#24615;&#21551;&#21457;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#31616;&#21333;&#30340;&#22810;&#27969;&#27169;&#22411;&#26469;&#25506;&#32034;&#20854;&#36890;&#36807;&#21253;&#21547;&#26102;&#38388;&#29305;&#24449;&#26469;&#22788;&#29702;&#26102;&#31354;&#21464;&#21270;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#24341;&#20837;&#19968;&#20010;&#32463;&#36807;&#35270;&#39057;&#35757;&#32451;&#30340;&#27169;&#22411;&#65292;&#24182;&#35780;&#20272;&#20854;&#23545;&#22810;&#26679;&#30340;&#22270;&#20687;&#21644;&#35270;&#39057;&#36755;&#20837;&#30340;&#40065;&#26834;&#24615;&#65292;&#29305;&#21035;&#20851;&#27880;&#26102;&#38388;&#29305;&#24449;&#22312;&#19981;&#21464;&#35782;&#21035;&#20013;&#30340;&#20316;&#29992;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#35757;&#32451;&#26102;&#21253;&#25324;&#35270;&#39057;&#21644;&#26102;&#38388;&#27969;&#21487;&#20197;&#23558;&#22270;&#20687;&#21644;&#35270;&#39057;&#29702;&#35299;&#20219;&#21153;&#30340;&#20934;&#30830;&#24615;&#21644;mAP&#19979;&#38477;&#29575;&#20943;&#23569;1.36%&#21644;3.14%&#12290;
&lt;/p&gt;
&lt;p&gt;
A defining characteristic of natural vision is its ability to withstand a variety of input alterations, resulting in the creation of an invariant representation of the surroundings. While convolutional neural networks exhibit resilience to certain forms of spatial input variation, modifications in the spatial and temporal aspects can significantly affect the representations of video content in deep neural networks. Inspired by the resilience of natural vision to input variations, we employ a simple multi-stream model to explore its potential to address spatiotemporal changes by including temporal features. Our primary goal is to introduce a video-trained model and evaluate its robustness to diverse image and video inputs, with a particular focus on exploring the role of temporal features in invariant recognition. Results show that including videos and the temporal stream during training mitigates the decline in accuracy and mAP in image and video understanding tasks by 1.36% and 3.14%,
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#35780;&#20272;&#40657;&#30418;&#25915;&#20987;&#30340;&#20998;&#31867;&#27861;&#65292;&#25581;&#31034;&#20102;&#26410;&#24320;&#21457;&#30340;&#23041;&#32961;&#31354;&#38388;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#26576;&#20123;&#35774;&#32622;&#19978;&#24050;&#26377;&#25216;&#26415;&#30340;&#23616;&#38480;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.17534</link><description>&lt;p&gt;
SoK&#65306;&#35780;&#20272;&#40657;&#30418;&#25915;&#20987;&#20013;&#30340;&#38519;&#38449;
&lt;/p&gt;
&lt;p&gt;
SoK: Pitfalls in Evaluating Black-Box Attacks. (arXiv:2310.17534v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17534
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#35780;&#20272;&#40657;&#30418;&#25915;&#20987;&#30340;&#20998;&#31867;&#27861;&#65292;&#25581;&#31034;&#20102;&#26410;&#24320;&#21457;&#30340;&#23041;&#32961;&#31354;&#38388;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#26576;&#20123;&#35774;&#32622;&#19978;&#24050;&#26377;&#25216;&#26415;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#30740;&#31350;&#28041;&#21450;&#23545;&#22270;&#20687;&#20998;&#31867;&#22120;&#30340;&#40657;&#30418;&#25915;&#20987;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#30740;&#31350;&#23545;&#23545;&#25163;&#30340;&#30693;&#35782;&#20551;&#35774;&#19981;&#21516;&#65292;&#30446;&#21069;&#30340;&#25991;&#29486;&#32570;&#20047;&#22260;&#32469;&#23041;&#32961;&#27169;&#22411;&#36827;&#34892;&#32452;&#32455;&#30340;&#36830;&#36143;&#24615;&#12290;&#20026;&#20102;&#31995;&#32479;&#21270;&#35813;&#39046;&#22495;&#30340;&#30693;&#35782;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20851;&#20110;&#23041;&#32961;&#31354;&#38388;&#30340;&#20998;&#31867;&#27861;&#65292;&#28085;&#30422;&#20102;&#21453;&#39304;&#31890;&#24230;&#12289;&#20132;&#20114;&#24335;&#26597;&#35810;&#30340;&#35775;&#38382;&#21644;&#25915;&#20987;&#32773;&#21487;&#29992;&#30340;&#36741;&#21161;&#25968;&#25454;&#30340;&#36136;&#37327;&#21644;&#25968;&#37327;&#19977;&#20010;&#32500;&#24230;&#12290;&#25105;&#20204;&#30340;&#26032;&#20998;&#31867;&#27861;&#25552;&#20379;&#20102;&#19977;&#20010;&#20851;&#38190;&#35265;&#35299;&#12290;1) &#23613;&#31649;&#26377;&#24191;&#27867;&#25991;&#29486;&#65292;&#20173;&#23384;&#22312;&#35768;&#22810;&#26410;&#24320;&#21457;&#30340;&#23041;&#32961;&#31354;&#38388;&#65292;&#26080;&#27861;&#36890;&#36807;&#20174;&#24050;&#30693;&#39046;&#22495;&#30340;&#25216;&#26415;&#31616;&#21333;&#22320;&#25913;&#36827;&#26469;&#35299;&#20915;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#22522;&#20110;&#24050;&#30693;&#39046;&#22495;&#20174;&#23436;&#25972;&#32622;&#20449;&#21521;&#37327;&#35775;&#38382;&#30340;&#25216;&#26415;&#36866;&#24212;&#21040;&#35775;&#38382;&#21069;k&#20010;&#32622;&#20449;&#24471;&#20998;&#30340;&#36739;&#23569;&#30740;&#31350;&#30340;&#35774;&#32622;&#20013;&#26469;&#35777;&#26126;&#36825;&#19968;&#28857;&#65292;&#20294;&#21516;&#26102;&#20063;&#23637;&#31034;&#20102;&#23427;&#22312;&#20165;&#33719;&#24471;&#39044;&#27979;&#26631;&#31614;&#30340;&#26356;&#20005;&#26684;&#35774;&#32622;&#20013;&#20173;&#28982;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;
Numerous works study black-box attacks on image classifiers. However, these works make different assumptions on the adversary's knowledge and current literature lacks a cohesive organization centered around the threat model. To systematize knowledge in this area, we propose a taxonomy over the threat space spanning the axes of feedback granularity, the access of interactive queries, and the quality and quantity of the auxiliary data available to the attacker. Our new taxonomy provides three key insights. 1) Despite extensive literature, numerous under-explored threat spaces exist, which cannot be trivially solved by adapting techniques from well-explored settings. We demonstrate this by establishing a new state-of-the-art in the less-studied setting of access to top-k confidence scores by adapting techniques from well-explored settings of accessing the complete confidence vector, but show how it still falls short of the more restrictive setting that only obtains the prediction label, h
&lt;/p&gt;</description></item><item><title>&#29983;&#25104;&#20998;&#31867;&#22120;&#23637;&#31034;&#20102;&#35760;&#24405;&#30772;&#32426;&#24405;&#30340;&#20154;&#31867;&#24418;&#29366;&#20559;&#22909;&#12289;&#25509;&#36817;&#20154;&#31867;&#32423;&#21035;&#30340;&#36229;&#20986;&#20998;&#24067;&#20934;&#30830;&#24615;&#12289;&#19982;&#20154;&#31867;&#20998;&#31867;&#38169;&#35823;&#30340;&#26368;&#20808;&#36827;&#23545;&#40784;&#20197;&#21450;&#29702;&#35299;&#26576;&#20123;&#30693;&#35273;&#24187;&#35937;&#30340;&#26032;&#20852;&#29305;&#24615;&#65292;&#25581;&#31034;&#20102;&#38646;&#26679;&#26412;&#29983;&#25104;&#27169;&#22411;&#20986;&#22855;&#22320;&#25509;&#36817;&#20154;&#31867;&#29289;&#20307;&#35782;&#21035;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2309.16779</link><description>&lt;p&gt;
&#29983;&#25104;&#20998;&#31867;&#22120;&#30340;&#26377;&#36259;&#23646;&#24615;
&lt;/p&gt;
&lt;p&gt;
Intriguing properties of generative classifiers. (arXiv:2309.16779v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16779
&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#20998;&#31867;&#22120;&#23637;&#31034;&#20102;&#35760;&#24405;&#30772;&#32426;&#24405;&#30340;&#20154;&#31867;&#24418;&#29366;&#20559;&#22909;&#12289;&#25509;&#36817;&#20154;&#31867;&#32423;&#21035;&#30340;&#36229;&#20986;&#20998;&#24067;&#20934;&#30830;&#24615;&#12289;&#19982;&#20154;&#31867;&#20998;&#31867;&#38169;&#35823;&#30340;&#26368;&#20808;&#36827;&#23545;&#40784;&#20197;&#21450;&#29702;&#35299;&#26576;&#20123;&#30693;&#35273;&#24187;&#35937;&#30340;&#26032;&#20852;&#29305;&#24615;&#65292;&#25581;&#31034;&#20102;&#38646;&#26679;&#26412;&#29983;&#25104;&#27169;&#22411;&#20986;&#22855;&#22320;&#25509;&#36817;&#20154;&#31867;&#29289;&#20307;&#35782;&#21035;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35782;&#21035;&#23545;&#35937;&#30340;&#26368;&#20339;&#33539;&#24335;&#26159;&#21028;&#21035;&#24335;&#25512;&#29702;&#65288;&#24555;&#36895;&#20294;&#28508;&#22312;&#23481;&#26131;&#20986;&#29616;&#24555;&#25463;&#23398;&#20064;&#65289;&#36824;&#26159;&#20351;&#29992;&#29983;&#25104;&#27169;&#22411;&#65288;&#36739;&#24930;&#20294;&#28508;&#22312;&#26356;&#31283;&#20581;&#65289;&#65311;&#25105;&#20204;&#20511;&#37492;&#20102;&#26368;&#26032;&#30340;&#29983;&#25104;&#27169;&#22411;&#36827;&#23637;&#65292;&#23558;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#36716;&#21270;&#20026;&#20998;&#31867;&#22120;&#12290;&#36825;&#20351;&#24471;&#25105;&#20204;&#33021;&#22815;&#30740;&#31350;&#20854;&#34892;&#20026;&#65292;&#24182;&#23558;&#20854;&#19982;&#21028;&#21035;&#27169;&#22411;&#21644;&#20154;&#31867;&#24515;&#29702;&#29289;&#29702;&#25968;&#25454;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#25253;&#36947;&#20102;&#29983;&#25104;&#20998;&#31867;&#22120;&#30340;&#22235;&#20010;&#26377;&#36259;&#30340;&#26032;&#20852;&#29305;&#24615;&#65306;&#23427;&#20204;&#26174;&#31034;&#20986;&#30772;&#32426;&#24405;&#30340;&#20154;&#31867;&#24418;&#29366;&#20559;&#22909;&#65288;&#23545;&#20110;Imagen&#36798;&#21040;99%&#65289;&#65292;&#25509;&#36817;&#20154;&#31867;&#32423;&#21035;&#30340;&#36229;&#20986;&#20998;&#24067;&#20934;&#30830;&#24615;&#65292;&#19982;&#20154;&#31867;&#20998;&#31867;&#38169;&#35823;&#30340;&#26368;&#20808;&#36827;&#23545;&#40784;&#20197;&#21450;&#23427;&#20204;&#29702;&#35299;&#26576;&#20123;&#30693;&#35273;&#24187;&#35937;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#23613;&#31649;&#30446;&#21069;&#27169;&#25311;&#20154;&#31867;&#29289;&#20307;&#35782;&#21035;&#30340;&#20027;&#23548;&#33539;&#24335;&#26159;&#21028;&#21035;&#24335;&#25512;&#29702;&#65292;&#38646;&#26679;&#26412;&#29983;&#25104;&#27169;&#22411;&#20986;&#22855;&#22320;&#25509;&#36817;&#20154;&#31867;&#29289;&#20307;&#35782;&#21035;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
What is the best paradigm to recognize objects -- discriminative inference (fast but potentially prone to shortcut learning) or using a generative model (slow but potentially more robust)? We build on recent advances in generative modeling that turn text-to-image models into classifiers. This allows us to study their behavior and to compare them against discriminative models and human psychophysical data. We report four intriguing emergent properties of generative classifiers: they show a record-breaking human-like shape bias (99% for Imagen), near human-level out-of-distribution accuracy, state-of-the-art alignment with human classification errors, and they understand certain perceptual illusions. Our results indicate that while the current dominant paradigm for modeling human object recognition is discriminative inference, zero-shot generative models approximate human object recognition data surprisingly well.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;MPHD&#26041;&#27861;&#65292;&#36890;&#36807;&#27169;&#22411;&#39044;&#35757;&#32451;&#21644;&#31070;&#32463;&#32593;&#32476;&#22312;&#24322;&#36136;&#25628;&#32034;&#31354;&#38388;&#19978;&#23454;&#29616;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#36801;&#31227;&#23398;&#20064;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;MPHD&#30340;&#26377;&#25928;&#24615;&#21644;&#22312;&#40657;&#30418;&#20989;&#25968;&#20248;&#21270;&#20219;&#21153;&#20013;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.16597</link><description>&lt;p&gt;
&#24322;&#36136;&#25628;&#32034;&#31354;&#38388;&#19978;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#36801;&#31227;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Transfer Learning for Bayesian Optimization on Heterogeneous Search Spaces. (arXiv:2309.16597v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16597
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;MPHD&#26041;&#27861;&#65292;&#36890;&#36807;&#27169;&#22411;&#39044;&#35757;&#32451;&#21644;&#31070;&#32463;&#32593;&#32476;&#22312;&#24322;&#36136;&#25628;&#32034;&#31354;&#38388;&#19978;&#23454;&#29616;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#36801;&#31227;&#23398;&#20064;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;MPHD&#30340;&#26377;&#25928;&#24615;&#21644;&#22312;&#40657;&#30418;&#20989;&#25968;&#20248;&#21270;&#20219;&#21153;&#20013;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#20248;&#21270;&#26159;&#19968;&#31181;&#27969;&#34892;&#30340;&#40657;&#30418;&#20989;&#25968;&#20248;&#21270;&#26041;&#27861;&#65292;&#23427;&#22522;&#20110;&#36125;&#21494;&#26031;&#27169;&#22411;&#65288;&#36890;&#24120;&#26159;&#39640;&#26031;&#36807;&#31243;&#65289;&#36827;&#34892;&#39034;&#24207;&#20915;&#31574;&#12290;&#20026;&#20102;&#30830;&#20445;&#27169;&#22411;&#30340;&#36136;&#37327;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#26469;&#33258;&#8220;&#35757;&#32451;&#8221;&#20989;&#25968;&#30340;&#35266;&#23519;&#32467;&#26524;&#26469;&#33258;&#21160;&#35774;&#35745;&#39640;&#26031;&#36807;&#31243;&#20808;&#39564;&#12290;&#36825;&#20123;&#35757;&#32451;&#20989;&#25968;&#36890;&#24120;&#38656;&#35201;&#19982;&#8220;&#27979;&#35797;&#8221;&#20989;&#25968;&#65288;&#24453;&#20248;&#21270;&#30340;&#40657;&#30418;&#20989;&#25968;&#65289;&#20855;&#26377;&#30456;&#21516;&#30340;&#23450;&#20041;&#22495;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;MPHD&#30340;&#27169;&#22411;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#23558;&#29305;&#23450;&#20110;&#39046;&#22495;&#30340;&#19978;&#19979;&#25991;&#26144;&#23556;&#21040;&#20998;&#23618;&#39640;&#26031;&#36807;&#31243;&#30340;&#35268;&#33539;&#12290;MPHD&#21487;&#20197;&#19982;&#36125;&#21494;&#26031;&#20248;&#21270;&#26080;&#32541;&#38598;&#25104;&#65292;&#23454;&#29616;&#24322;&#36136;&#25628;&#32034;&#31354;&#38388;&#30340;&#30693;&#35782;&#36801;&#31227;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#21644;&#23454;&#35777;&#32467;&#26524;&#35777;&#26126;&#20102;MPHD&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#40657;&#30418;&#20989;&#25968;&#20248;&#21270;&#20219;&#21153;&#20013;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian optimization (BO) is a popular black-box function optimization method, which makes sequential decisions based on a Bayesian model, typically a Gaussian process (GP), of the function. To ensure the quality of the model, transfer learning approaches have been developed to automatically design GP priors by learning from observations on "training" functions. These training functions are typically required to have the same domain as the "test" function (black-box function to be optimized). In this paper, we introduce MPHD, a model pre-training method on heterogeneous domains, which uses a neural net mapping from domain-specific contexts to specifications of hierarchical GPs. MPHD can be seamlessly integrated with BO to transfer knowledge across heterogeneous search spaces. Our theoretical and empirical results demonstrate the validity of MPHD and its superior performance on challenging black-box function optimization tasks.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#25216;&#26415;&#65292;&#20197;&#20943;&#23569;&#22312;&#32447;VOS&#26041;&#27861;&#30340;&#20869;&#23384;&#38656;&#27714;&#65292;&#24182;&#22312;&#25552;&#39640;&#24314;&#27169;&#20934;&#30830;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#30340;&#21516;&#26102;&#36827;&#34892;&#30446;&#26631;&#20998;&#21106;&#12290;</title><link>http://arxiv.org/abs/2309.15274</link><description>&lt;p&gt;
&#20869;&#23384;&#39640;&#25928;&#30340;&#36830;&#32493;&#23398;&#20064;&#38271;&#35270;&#39057;&#30446;&#26631;&#20998;&#21106;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Memory-Efficient Continual Learning Object Segmentation for Long Video. (arXiv:2309.15274v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15274
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#25216;&#26415;&#65292;&#20197;&#20943;&#23569;&#22312;&#32447;VOS&#26041;&#27861;&#30340;&#20869;&#23384;&#38656;&#27714;&#65292;&#24182;&#22312;&#25552;&#39640;&#24314;&#27169;&#20934;&#30830;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#30340;&#21516;&#26102;&#36827;&#34892;&#30446;&#26631;&#20998;&#21106;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#21322;&#30417;&#30563;&#35270;&#39057;&#30446;&#26631;&#20998;&#21106;&#26041;&#27861;&#22312;&#21033;&#29992;&#21069;&#20960;&#24103;&#30340;&#20449;&#24687;&#36827;&#34892;&#24403;&#21069;&#24103;&#20998;&#21106;&#26102;&#26174;&#31034;&#20986;&#26174;&#33879;&#30340;&#30446;&#26631;&#23545;&#35937;&#20998;&#21106;&#20934;&#30830;&#24615;&#25913;&#36827;&#12290;&#29305;&#21035;&#22320;&#65292;&#36825;&#31181;&#22522;&#20110;&#35760;&#24518;&#30340;&#26041;&#27861;&#21487;&#20197;&#24110;&#21161;&#27169;&#22411;&#26356;&#26377;&#25928;&#22320;&#22788;&#29702;&#22806;&#35266;&#21464;&#21270;&#65288;&#34920;&#36798;&#28418;&#31227;&#65289;&#25110;&#36974;&#25377;&#12290;&#29702;&#24819;&#24773;&#20917;&#19979;&#65292;&#20026;&#20102;&#36798;&#21040;&#26368;&#20339;&#24615;&#33021;&#65292;&#22312;&#32447;&#35270;&#39057;&#30446;&#26631;&#20998;&#21106;&#65288;VOS&#65289;&#26041;&#27861;&#38656;&#35201;&#23558;&#25152;&#26377;&#25110;&#22823;&#37096;&#20998;&#21069;&#20960;&#24103;&#65288;&#25110;&#20854;&#25552;&#21462;&#30340;&#20449;&#24687;&#65289;&#23384;&#20648;&#22312;&#20869;&#23384;&#20013;&#65292;&#24182;&#29992;&#20110;&#36830;&#32493;&#24103;&#30340;&#22312;&#32447;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#38271;&#35270;&#39057;&#26469;&#35828;&#65292;&#36825;&#31181;&#35299;&#20915;&#26041;&#26696;&#26159;&#19981;&#21487;&#34892;&#30340;&#65292;&#22240;&#20026;&#25152;&#38656;&#30340;&#20869;&#23384;&#22823;&#23567;&#20250;&#26080;&#38480;&#22686;&#38271;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#22312;&#20869;&#23384;&#26377;&#38480;&#19988;&#30446;&#26631;&#23545;&#35937;&#22312;&#35270;&#39057;&#20013;&#37325;&#22797;&#21457;&#29983;&#34920;&#36798;&#28418;&#31227;&#30340;&#24773;&#20917;&#19979;&#65292;&#36825;&#20123;&#26041;&#27861;&#21487;&#33021;&#20250;&#22833;&#36133;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#25216;&#26415;&#26469;&#20943;&#23569;&#22312;&#32447;VOS&#26041;&#27861;&#30340;&#20869;&#23384;&#38656;&#27714;&#65292;&#21516;&#26102;&#25552;&#39640;&#24314;&#27169;&#20934;&#30830;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent state-of-the-art semi-supervised Video Object Segmentation (VOS) methods have shown significant improvements in target object segmentation accuracy when information from preceding frames is used in undertaking segmentation on the current frame. In particular, such memory-based approaches can help a model to more effectively handle appearance changes (representation drift) or occlusions. Ideally, for maximum performance, online VOS methods would need all or most of the preceding frames (or their extracted information) to be stored in memory and be used for online learning in consecutive frames. Such a solution is not feasible for long videos, as the required memory size would grow without bound. On the other hand, these methods can fail when memory is limited and a target object experiences repeated representation drifts throughout a video.  We propose two novel techniques to reduce the memory requirement of online VOS methods while improving modeling accuracy and generalization 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31639;&#27861;&#21644;&#24037;&#20855;&#65292;&#21487;&#20197;&#20026;&#22270;&#20687;&#20998;&#31867;&#22120;&#30340;&#36755;&#20986;&#35745;&#31639;&#22810;&#20010;&#35299;&#37322;&#65292;&#20174;&#32780;&#25552;&#39640;&#23545;&#20998;&#31867;&#22120;&#34892;&#20026;&#30340;&#27934;&#23519;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.14309</link><description>&lt;p&gt;
&#22270;&#20687;&#20998;&#31867;&#22120;&#30340;&#22810;&#20010;&#19981;&#21516;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Multiple Different Explanations for Image Classifiers. (arXiv:2309.14309v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14309
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31639;&#27861;&#21644;&#24037;&#20855;&#65292;&#21487;&#20197;&#20026;&#22270;&#20687;&#20998;&#31867;&#22120;&#30340;&#36755;&#20986;&#35745;&#31639;&#22810;&#20010;&#35299;&#37322;&#65292;&#20174;&#32780;&#25552;&#39640;&#23545;&#20998;&#31867;&#22120;&#34892;&#20026;&#30340;&#27934;&#23519;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#22270;&#20687;&#20998;&#31867;&#22120;&#35299;&#37322;&#24037;&#20855;&#36890;&#24120;&#21482;&#20250;&#32473;&#20986;&#19968;&#31181;&#23545;&#20110;&#22270;&#20687;&#30340;&#35299;&#37322;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#35768;&#22810;&#22270;&#20687;&#26469;&#35828;&#65292;&#26080;&#35770;&#26159;&#20154;&#31867;&#36824;&#26159;&#22270;&#20687;&#20998;&#31867;&#22120;&#37117;&#25509;&#21463;&#22810;&#20010;&#35299;&#37322;&#26469;&#35299;&#37322;&#22270;&#20687;&#26631;&#31614;&#12290;&#22240;&#27492;&#65292;&#38480;&#21046;&#35299;&#37322;&#30340;&#25968;&#37327;&#21482;&#26377;&#19968;&#20010;&#20005;&#37325;&#38480;&#21046;&#20102;&#23545;&#20998;&#31867;&#22120;&#34892;&#20026;&#30340;&#27934;&#23519;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#31181;&#31639;&#27861;&#21644;&#24037;&#20855;REX&#65292;&#29992;&#20110;&#35745;&#31639;&#40657;&#30418;&#22270;&#20687;&#20998;&#31867;&#22120;&#23545;&#32473;&#23450;&#22270;&#20687;&#30340;&#36755;&#20986;&#30340;&#22810;&#20010;&#35299;&#37322;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#22522;&#20110;&#22240;&#26524;&#29702;&#35770;&#30340;&#21487;&#38752;&#26041;&#27861;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#20854;&#29702;&#35770;&#22797;&#26434;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#23454;&#39564;&#32467;&#26524;&#65292;&#26174;&#31034;REX&#22312;ImageNet-mini&#22522;&#20934;&#27979;&#35797;&#20013;&#25214;&#21040;&#30340;&#22810;&#20010;&#35299;&#37322;&#27604;&#20043;&#21069;&#30340;&#24037;&#20316;&#22810;7&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing explanation tools for image classifiers usually give only one single explanation for an image. For many images, however, both humans and image classifiers accept more than one explanation for the image label. Thus, restricting the number of explanations to just one severely limits the insight into the behavior of the classifier. In this paper, we describe an algorithm and a tool, REX, for computing multiple explanations of the output of a black-box image classifier for a given image. Our algorithm uses a principled approach based on causal theory. We analyse its theoretical complexity and provide experimental results showing that REX finds multiple explanations on 7 times more images than the previous work on the ImageNet-mini benchmark.
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#20998;&#21306;&#25193;&#25955;&#27169;&#22411;&#65288;CDM&#65289;&#35757;&#32451;&#19981;&#21516;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#24182;&#22312;&#25512;&#26029;&#26102;&#20219;&#24847;&#32452;&#21512;&#23427;&#20204;&#65292;&#23454;&#29616;&#20102;&#35757;&#32451;&#25968;&#25454;&#20445;&#25252;&#21644;&#36873;&#25321;&#24615;&#36951;&#24536;&#65292;&#21516;&#26102;&#36824;&#21487;&#20197;&#26681;&#25454;&#29992;&#25143;&#35775;&#38382;&#26435;&#38480;&#25552;&#20379;&#23450;&#21046;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2308.01937</link><description>&lt;p&gt;
&#20351;&#29992;&#32452;&#21512;&#25193;&#25955;&#27169;&#22411;&#23454;&#29616;&#35757;&#32451;&#25968;&#25454;&#20445;&#25252;
&lt;/p&gt;
&lt;p&gt;
Training Data Protection with Compositional Diffusion Models. (arXiv:2308.01937v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01937
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#20998;&#21306;&#25193;&#25955;&#27169;&#22411;&#65288;CDM&#65289;&#35757;&#32451;&#19981;&#21516;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#24182;&#22312;&#25512;&#26029;&#26102;&#20219;&#24847;&#32452;&#21512;&#23427;&#20204;&#65292;&#23454;&#29616;&#20102;&#35757;&#32451;&#25968;&#25454;&#20445;&#25252;&#21644;&#36873;&#25321;&#24615;&#36951;&#24536;&#65292;&#21516;&#26102;&#36824;&#21487;&#20197;&#26681;&#25454;&#29992;&#25143;&#35775;&#38382;&#26435;&#38480;&#25552;&#20379;&#23450;&#21046;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#20998;&#21306;&#25193;&#25955;&#27169;&#22411;&#65288;CDM&#65289;&#65292;&#19968;&#31181;&#22312;&#19981;&#21516;&#25968;&#25454;&#28304;&#19978;&#35757;&#32451;&#19981;&#21516;&#25193;&#25955;&#27169;&#22411;&#65288;&#25110;&#25552;&#31034;&#65289;&#24182;&#22312;&#25512;&#26029;&#26102;&#20219;&#24847;&#32452;&#21512;&#23427;&#20204;&#30340;&#26041;&#27861;&#12290;&#36825;&#20123;&#21333;&#29420;&#30340;&#27169;&#22411;&#21487;&#20197;&#22312;&#23396;&#31435;&#29366;&#24577;&#19979;&#12289;&#22312;&#19981;&#21516;&#26102;&#38388;&#12289;&#22312;&#19981;&#21516;&#20998;&#24067;&#21644;&#39046;&#22495;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#21487;&#20197;&#21518;&#32493;&#32452;&#21512;&#20197;&#36798;&#21040;&#19982;&#21516;&#26102;&#35757;&#32451;&#25152;&#26377;&#25968;&#25454;&#30340;&#29702;&#24819;&#27169;&#22411;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#27599;&#20010;&#27169;&#22411;&#21482;&#21253;&#21547;&#20854;&#22312;&#35757;&#32451;&#26399;&#38388;&#25509;&#35302;&#21040;&#30340;&#25968;&#25454;&#23376;&#38598;&#30340;&#20449;&#24687;&#65292;&#21487;&#20197;&#23454;&#29616;&#22810;&#31181;&#24418;&#24335;&#30340;&#35757;&#32451;&#25968;&#25454;&#20445;&#25252;&#12290;&#29305;&#21035;&#26159;&#65292;CDM&#26159;&#31532;&#19968;&#31181;&#21487;&#20197;&#23454;&#29616;&#22823;&#35268;&#27169;&#25193;&#25955;&#27169;&#22411;&#30340;&#36873;&#25321;&#24615;&#36951;&#24536;&#21644;&#25345;&#32493;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#20801;&#35768;&#26681;&#25454;&#29992;&#25143;&#35775;&#38382;&#26435;&#38480;&#25552;&#20379;&#23450;&#21046;&#27169;&#22411;&#12290;CDM&#36824;&#21487;&#20197;&#30830;&#23450;&#29983;&#25104;&#29305;&#23450;&#26679;&#26412;&#30340;&#25968;&#25454;&#23376;&#38598;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce Compartmentalized Diffusion Models (CDM), a method to train different diffusion models (or prompts) on distinct data sources and arbitrarily compose them at inference time. The individual models can be trained in isolation, at different times, and on different distributions and domains and can be later composed to achieve performance comparable to a paragon model trained on all data simultaneously. Furthermore, each model only contains information about the subset of the data it was exposed to during training, enabling several forms of training data protection. In particular, CDMs are the first method to enable both selective forgetting and continual learning for large-scale diffusion models, as well as allowing serving customized models based on the user's access rights. CDMs also allow determining the importance of a subset of the data in generating particular samples.
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38750;&#21442;&#25968;&#23494;&#24230;&#20272;&#35745;&#26041;&#27861;&#65292;&#22522;&#20110;&#27491;&#21017;&#21270;&#23494;&#24230;&#30340; Sobolev &#33539;&#25968;&#65292;&#36890;&#36807;&#36866;&#24403;&#30340;&#21021;&#22987;&#21270;&#21644;&#20351;&#29992;&#33258;&#28982;&#26799;&#24230;&#65292;&#21487;&#20197;&#24471;&#21040;&#24615;&#33021;&#33391;&#22909;&#30340;&#35299;&#65292;&#35813;&#26041;&#27861;&#22312; Anomaly Detection benchmark &#20013;&#25490;&#21517;&#31532;&#20108;&#12290;</title><link>http://arxiv.org/abs/2307.13763</link><description>&lt;p&gt;
&#38544;&#24335;&#24402;&#19968;&#21270;&#26174;&#24335;&#27491;&#21017;&#21270;&#23494;&#24230;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Implicitly Normalized Explicitly Regularized Density Estimation. (arXiv:2307.13763v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13763
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38750;&#21442;&#25968;&#23494;&#24230;&#20272;&#35745;&#26041;&#27861;&#65292;&#22522;&#20110;&#27491;&#21017;&#21270;&#23494;&#24230;&#30340; Sobolev &#33539;&#25968;&#65292;&#36890;&#36807;&#36866;&#24403;&#30340;&#21021;&#22987;&#21270;&#21644;&#20351;&#29992;&#33258;&#28982;&#26799;&#24230;&#65292;&#21487;&#20197;&#24471;&#21040;&#24615;&#33021;&#33391;&#22909;&#30340;&#35299;&#65292;&#35813;&#26041;&#27861;&#22312; Anomaly Detection benchmark &#20013;&#25490;&#21517;&#31532;&#20108;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38750;&#21442;&#25968;&#23494;&#24230;&#20272;&#35745;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26159;&#22522;&#20110;&#27491;&#21017;&#21270;&#23494;&#24230;&#30340; Sobolev &#33539;&#25968;&#12290;&#36825;&#31181;&#26041;&#27861;&#19982;&#26680;&#23494;&#24230;&#20272;&#35745;&#26377;&#26126;&#26174;&#24046;&#24322;&#65292;&#21487;&#20197;&#28165;&#26224;&#35299;&#37322;&#27169;&#22411;&#30340;&#20559;&#24046;&#12290;&#34429;&#28982;&#25105;&#20204;&#26080;&#27861;&#24471;&#21040;&#30456;&#20851;&#26680;&#20989;&#25968;&#30340;&#38381;&#21512;&#35299;&#26512;&#24418;&#24335;&#65292;&#20294;&#25105;&#20204;&#35777;&#26126;&#21487;&#20197;&#36890;&#36807;&#37319;&#26679;&#36827;&#34892;&#36817;&#20284;&#12290;&#20915;&#23450;&#23494;&#24230;&#30340;&#20248;&#21270;&#38382;&#39064;&#26159;&#38750;&#20984;&#30340;&#65292;&#26631;&#20934;&#30340;&#26799;&#24230;&#26041;&#27861;&#25928;&#26524;&#19981;&#22909;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35777;&#26126;&#22312;&#36866;&#24403;&#30340;&#21021;&#22987;&#21270;&#21644;&#20351;&#29992;&#33258;&#28982;&#26799;&#24230;&#30340;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#24471;&#21040;&#24615;&#33021;&#33391;&#22909;&#30340;&#35299;&#12290;&#26368;&#21518;&#65292;&#34429;&#28982;&#35813;&#26041;&#27861;&#25552;&#20379;&#30340;&#26159;&#38750;&#24402;&#19968;&#21270;&#30340;&#23494;&#24230;&#65292;&#26080;&#27861;&#20351;&#29992;&#23545;&#25968;&#20284;&#28982;&#36827;&#34892;&#20132;&#21449;&#39564;&#35777;&#65292;&#20294;&#25105;&#20204;&#35777;&#26126;&#21487;&#20197;&#37319;&#29992;&#22522;&#20110; Fisher &#25955;&#24230;&#30340;&#20998;&#25968;&#21305;&#37197;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#22312;&#26368;&#36817;&#30340;&#24322;&#24120;&#26816;&#27979;&#22522;&#20934;&#22871;&#20214; ADBench &#19978;&#35780;&#20272;&#20102;&#24471;&#21040;&#30340;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#23427;&#22312;&#36229;&#36807;15&#20010;&#31639;&#27861;&#20013;&#25490;&#21517;&#31532;&#20108;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a new approach to non-parametric density estimation, that is based on regularizing a Sobolev norm of the density. This method is provably different from Kernel Density Estimation, and makes the bias of the model clear and interpretable. While there is no closed analytic form for the associated kernel, we show that one can approximate it using sampling. The optimization problem needed to determine the density is non-convex, and standard gradient methods do not perform well. However, we show that with an appropriate initialization and using natural gradients, one can obtain well performing solutions. Finally, while the approach provides unnormalized densities, which prevents the use of log-likelihood for cross validation, we show that one can instead adapt Fisher Divergence based Score Matching methods for this task. We evaluate the resulting method on the comprehensive recent Anomaly Detection benchmark suite, ADBench, and find that it ranks second best, among more than 15 al
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#29983;&#35789;&#25193;&#23637;&#8221;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#20010;&#24615;&#21270;&#30340;&#8220;&#29983;&#35789;&#36866;&#37197;&#22120;&#8221;&#26469;&#23398;&#20064;&#20010;&#24615;&#21270;&#35789;&#27719;&#65292;&#25552;&#39640;&#20102;&#29983;&#35789;&#35206;&#30422;&#29575;&#24182;&#26174;&#33879;&#25552;&#39640;&#20102;&#27169;&#22411;&#20934;&#30830;&#24230;&#12290;</title><link>http://arxiv.org/abs/2305.03584</link><description>&lt;p&gt;
&#29616;&#22312;&#23427;&#21548;&#36215;&#26469;&#20687;&#20320;&#20102;&#65306;&#22312;&#35774;&#22791;&#19978;&#23398;&#20064;&#20010;&#24615;&#21270;&#35789;&#27719;
&lt;/p&gt;
&lt;p&gt;
Now It Sounds Like You: Learning Personalized Vocabulary On Device. (arXiv:2305.03584v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03584
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#29983;&#35789;&#25193;&#23637;&#8221;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#20010;&#24615;&#21270;&#30340;&#8220;&#29983;&#35789;&#36866;&#37197;&#22120;&#8221;&#26469;&#23398;&#20064;&#20010;&#24615;&#21270;&#35789;&#27719;&#65292;&#25552;&#39640;&#20102;&#29983;&#35789;&#35206;&#30422;&#29575;&#24182;&#26174;&#33879;&#25552;&#39640;&#20102;&#27169;&#22411;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#32852;&#37030;&#23398;&#20064;&#22312;&#36827;&#34892;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#26041;&#38754;&#24050;&#32463;&#26174;&#31034;&#20986;&#26174;&#33879;&#36827;&#23637;&#12290;&#36825;&#39033;&#24037;&#20316;&#20391;&#37325;&#20110;&#24212;&#29992;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#36827;&#34892;&#35774;&#22791;&#31471;&#35821;&#35328;&#24314;&#27169;&#12290;&#30001;&#20110;&#20869;&#23384;&#21644;&#24310;&#36831;&#30340;&#38480;&#21046;&#65292;&#36825;&#20123;&#27169;&#22411;&#26080;&#27861;&#25903;&#25345;&#23376;&#21333;&#35789;&#26631;&#35760;&#25110;&#27874;&#26463;&#25628;&#32034;&#35299;&#30721;&#30340;&#22797;&#26434;&#24615;&#65292;&#22240;&#27492;&#20915;&#23450;&#37096;&#32626;&#23553;&#38381;&#35789;&#27719;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#23553;&#38381;&#35789;&#27719;&#27169;&#22411;&#26080;&#27861;&#22788;&#29702;&#29305;&#23450;&#29992;&#25143;&#30340;&#29983;&#35789;&#65292;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#29983;&#35789;&#25193;&#23637;&#8221;&#30340;&#26032;&#25216;&#26415;&#65292;&#35813;&#25216;&#26415;&#25552;&#39640;&#20102;&#29983;&#35789;&#35206;&#30422;&#29575;&#65292;&#22686;&#21152;&#20102;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#65292;&#21516;&#26102;&#26368;&#22823;&#31243;&#24230;&#22320;&#20943;&#23569;&#20102;&#23545;&#20869;&#23384;&#21644;&#24310;&#36831;&#30340;&#24433;&#21709;&#12290;&#36825;&#31181;&#26041;&#27861;&#24341;&#20837;&#20102;&#20010;&#24615;&#21270;&#30340;&#8220;&#29983;&#35789;&#36866;&#37197;&#22120;&#8221;&#65292;&#26377;&#25928;&#22320;&#20174;&#20013;&#22830;&#27169;&#22411;&#20256;&#36755;&#30693;&#35782;&#65292;&#24182;&#20026;&#20010;&#24615;&#21270;&#35789;&#27719;&#23398;&#20064;&#21333;&#35789;&#23884;&#20837;&#12290;&#22312;&#19968;&#32452;&#24120;&#35265;&#30340;&#32852;&#37030;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#29983;&#35789;&#25193;&#23637;&#26041;&#27861;&#26126;&#26174;&#20248;&#20110;&#26631;&#20934;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, Federated Learning (FL) has shown significant advancements in its ability to perform various natural language processing (NLP) tasks. This work focuses on applying personalized FL for on-device language modeling. Due to limitations of memory and latency, these models cannot support the complexity of sub-word tokenization or beam search decoding, resulting in the decision to deploy a closed-vocabulary language model. However, closed-vocabulary models are unable to handle out-of-vocabulary (OOV) words belonging to specific users. To address this issue, We propose a novel technique called "OOV expansion" that improves OOV coverage and increases model accuracy while minimizing the impact on memory and latency. This method introduces a personalized "OOV adapter" that effectively transfers knowledge from a central model and learns word embedding for personalized vocabulary. OOV expansion significantly outperforms standard FL personalization methods on a set of common FL benc
&lt;/p&gt;</description></item><item><title>CEMA&#26159;&#19968;&#20010;&#29992;&#20110;&#22810;&#26234;&#33021;&#20307;&#20915;&#31574;&#22240;&#26524;&#35299;&#37322;&#30340;&#31995;&#32479;&#65292;&#20351;&#29992;&#37319;&#26679;&#21453;&#20107;&#23454;&#19990;&#30028;&#30340;&#26041;&#27861;&#21487;&#20197;&#35782;&#21035;&#21644;&#25490;&#21517;&#20915;&#31574;&#32972;&#21518;&#30340;&#26174;&#33879;&#21407;&#22240;&#12290;&#35813;&#31995;&#32479;&#36824;&#21487;&#20197;&#29983;&#25104;&#22522;&#20110;&#25152;&#36873;&#21407;&#22240;&#30340;&#23545;&#27604;&#35299;&#37322;&#65292;&#24182;&#19982;&#29992;&#25143;&#36827;&#34892;&#20132;&#20114;&#24490;&#29615;&#20197;&#30830;&#20445;&#35299;&#37322;&#30340;&#30456;&#20851;&#24615;&#21644;&#21487;&#35835;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.10809</link><description>&lt;p&gt;
&#38543;&#26426;&#24207;&#21015;&#22810;&#26234;&#33021;&#20307;&#20915;&#31574;&#30340;&#22240;&#26524;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Causal Explanations for Stochastic Sequential Multi-Agent Decision-Making. (arXiv:2302.10809v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.10809
&lt;/p&gt;
&lt;p&gt;
CEMA&#26159;&#19968;&#20010;&#29992;&#20110;&#22810;&#26234;&#33021;&#20307;&#20915;&#31574;&#22240;&#26524;&#35299;&#37322;&#30340;&#31995;&#32479;&#65292;&#20351;&#29992;&#37319;&#26679;&#21453;&#20107;&#23454;&#19990;&#30028;&#30340;&#26041;&#27861;&#21487;&#20197;&#35782;&#21035;&#21644;&#25490;&#21517;&#20915;&#31574;&#32972;&#21518;&#30340;&#26174;&#33879;&#21407;&#22240;&#12290;&#35813;&#31995;&#32479;&#36824;&#21487;&#20197;&#29983;&#25104;&#22522;&#20110;&#25152;&#36873;&#21407;&#22240;&#30340;&#23545;&#27604;&#35299;&#37322;&#65292;&#24182;&#19982;&#29992;&#25143;&#36827;&#34892;&#20132;&#20114;&#24490;&#29615;&#20197;&#30830;&#20445;&#35299;&#37322;&#30340;&#30456;&#20851;&#24615;&#21644;&#21487;&#35835;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;CEMA&#65306;&#29992;&#20110;&#22810;&#26234;&#33021;&#20307;&#20915;&#31574;&#30340;&#22240;&#26524;&#35299;&#37322;&#31995;&#32479;&#65307;&#29992;&#20110;&#22312;&#38543;&#26426;&#24207;&#21015;&#22810;&#26234;&#33021;&#20307;&#29615;&#22659;&#20013;&#29983;&#25104;&#20851;&#20110;&#26234;&#33021;&#20307;&#20915;&#31574;&#30340;&#22240;&#26524;&#35299;&#37322;&#12290;CEMA&#30340;&#26680;&#24515;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#22240;&#26524;&#36873;&#25321;&#26041;&#27861;&#65292;&#19981;&#21516;&#20110;&#20043;&#21069;&#20551;&#35774;&#29305;&#23450;&#22240;&#26524;&#32467;&#26500;&#30340;&#26041;&#27861;&#65292;&#21482;&#38656;&#35201;&#19968;&#20010;&#21487;&#20197;&#39044;&#27979;&#29615;&#22659;&#26410;&#26469;&#29366;&#24577;&#30340;&#27010;&#29575;&#27169;&#22411;&#21363;&#21487;&#24212;&#29992;&#12290;&#25105;&#20204;&#20351;&#29992;&#35813;&#27169;&#22411;&#37319;&#26679;&#21453;&#20107;&#23454;&#19990;&#30028;&#65292;&#20197;&#35782;&#21035;&#21644;&#25490;&#21517;&#20915;&#31574;&#32972;&#21518;&#30340;&#26174;&#33879;&#21407;&#22240;&#12290;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;CEMA&#20197;&#28385;&#36275;&#31038;&#20250;&#21487;&#35299;&#37322;AI&#30340;&#35201;&#27714;&#12290;&#23427;&#21487;&#20197;&#22522;&#20110;&#25152;&#36873;&#21407;&#22240;&#29983;&#25104;&#23545;&#27604;&#35299;&#37322;&#65292;&#36890;&#36807;&#19982;&#29992;&#25143;&#30340;&#20132;&#20114;&#24490;&#29615;&#26469;&#30830;&#20445;&#23545;&#29992;&#25143;&#30340;&#30456;&#20851;&#24615;&#21644;&#21487;&#35835;&#24615;&#12290;&#25105;&#20204;&#23558;CEMA&#23454;&#29616;&#22312;&#33258;&#21160;&#39550;&#39542;&#30340;&#36816;&#21160;&#35268;&#21010;&#20013;&#65292;&#24182;&#22312;&#22235;&#20010;&#19981;&#21516;&#30340;&#27169;&#25311;&#22330;&#26223;&#20013;&#36827;&#34892;&#27979;&#35797;&#12290;&#25105;&#20204;&#23637;&#31034;CEMA&#33021;&#22815;&#27491;&#30830;&#32780;&#19988;&#40065;&#26834;&#22320;&#35782;&#21035;&#20915;&#31574;&#32972;&#21518;&#30340;&#30456;&#20851;&#21407;&#22240;&#65292;&#24182;&#25552;&#20379;&#30456;&#20851;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present CEMA: Causal Explanations for Multi-Agent decision-making; a system to generate causal explanations for agents' decisions in stochastic sequential multi-agent environments. The core of CEMA is a novel causal selection method which, unlike prior work that assumes a specific causal structure, is applicable whenever a probabilistic model for predicting future states of the environment is available. We sample counterfactual worlds with this model which are used to identify and rank the salient causes behind decisions. We also designed CEMA to meet the requirements of social explainable AI. It can generate contrastive explanations based on selected causes and it works as an interaction loop with users to assure relevance and intelligibility for them. We implement CEMA for motion planning for autonomous driving and test it in four diverse simulated scenarios. We show that CEMA correctly and robustly identifies the relevant causes behind decisions and delivers relevant explanations
&lt;/p&gt;</description></item></channel></rss>