<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19981;&#23545;&#31216;&#26680;&#22855;&#24322;&#20540;&#20998;&#35299;&#30340;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#21363;Primal-Attention&#65292;&#26469;&#20248;&#21270;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#25552;&#39640;&#27880;&#24847;&#21147;&#36755;&#20986;&#30340;&#25237;&#24433;&#26041;&#24046;&#12290;</title><link>http://arxiv.org/abs/2305.19798</link><description>&lt;p&gt;
&#22522;&#20110;&#21407;&#22987;&#34920;&#36798;&#30340;&#19981;&#23545;&#31216;&#26680;&#22855;&#24322;&#20540;&#20998;&#35299;&#30340;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
Primal-Attention: Self-attention through Asymmetric Kernel SVD in Primal Representation. (arXiv:2305.19798v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19798
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19981;&#23545;&#31216;&#26680;&#22855;&#24322;&#20540;&#20998;&#35299;&#30340;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#21363;Primal-Attention&#65292;&#26469;&#20248;&#21270;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#25552;&#39640;&#27880;&#24847;&#21147;&#36755;&#20986;&#30340;&#25237;&#24433;&#26041;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#65292;&#19968;&#31995;&#21015;&#24037;&#20316;&#23558;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#35270;&#20026;&#26680;&#26426;&#22120;&#65292;&#20197;&#27492;&#26469;&#29702;&#35299;&#21644;&#25913;&#36827;Transformers&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#21482;&#36866;&#29992;&#20110;&#23545;&#31216;&#26680;&#32780;&#19981;&#36866;&#29992;&#20110;&#19981;&#23545;&#31216;&#30340;&#33258;&#27880;&#24847;&#21147;&#65292;&#23548;&#33268;&#20102;&#29702;&#35770;&#21644;&#23454;&#38469;&#30340;&#24046;&#36317;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19981;&#23545;&#31216;&#26680;&#22855;&#24322;&#20540;&#20998;&#35299;&#65288;KSVD&#65289;&#26469;&#34920;&#36798;&#21644;&#20248;&#21270;&#33258;&#27880;&#24847;&#21147;&#30340;&#26032;&#35270;&#35282;&#12290;&#36890;&#36807;&#19981;&#23545;&#31216;KSVD&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#65306;i&#65289;&#33258;&#27880;&#24847;&#21147;&#30340;&#19968;&#31181;&#21407;&#22987;-&#23545;&#20598;&#34920;&#36798;&#65292;&#20854;&#20013;&#20248;&#21270;&#30446;&#26631;&#34987;&#36716;&#21270;&#20026;&#26368;&#22823;&#21270;&#27880;&#24847;&#21147;&#36755;&#20986;&#20013;&#30340;&#25237;&#24433;&#26041;&#24046;&#65307;ii&#65289;&#19968;&#31181;&#26032;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;-Primal-Attention&#65292;&#36890;&#36807;KSVD&#30340;&#21407;&#22987;&#34920;&#36798;&#24335;&#36991;&#20813;&#20102;&#22312;&#23545;&#20598;&#20013;&#26174;&#24335;&#35745;&#31639;&#26680;&#30697;&#38453;&#30340;&#38382;&#39064;&#65307;iii&#65289;&#36890;&#36807;KKT&#26465;&#20214;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;Primal-Attention&#30340;&#29366;&#24577;&#26368;&#23567;&#21270;&#38382;&#39064;&#30340;&#35299;&#19982;&#20043;&#21069;&#30340;&#23545;&#20598;&#31639;&#27861;&#20855;&#26377;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, a new line of works has emerged to understand and improve self-attention in Transformers by treating it as a kernel machine. However, existing works apply the methods for symmetric kernels to the asymmetric self-attention, resulting in a nontrivial gap between the analytical understanding and numerical implementation. In this paper, we provide a new perspective to represent and optimize self-attention through asymmetric Kernel Singular Value Decomposition (KSVD), which is also motivated by the low-rank property of self-attention normally observed in deep layers. Through asymmetric KSVD, $i$) a primal-dual representation of self-attention is formulated, where the optimization objective is cast to maximize the projection variances in the attention outputs; $ii$) a novel attention mechanism, i.e., Primal-Attention, is proposed via the primal representation of KSVD, avoiding explicit computation of the kernel matrix in the dual; $iii$) with KKT conditions, we prove that the stati
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;DeepMerge&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#21306;&#22495;&#21512;&#24182;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#20998;&#21106;&#38382;&#39064;&#65292;&#24182;&#25104;&#21151;&#20351;&#29992;&#36828;&#31243;&#24863;&#30693;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2305.19787</link><description>&lt;p&gt;
DeepMerge: &#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#21306;&#22495;&#21512;&#24182;&#26041;&#27861;&#29992;&#20110;&#22270;&#20687;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
DeepMerge: Deep Learning-Based Region-Merging for Image Segmentation. (arXiv:2305.19787v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19787
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;DeepMerge&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#21306;&#22495;&#21512;&#24182;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#20998;&#21106;&#38382;&#39064;&#65292;&#24182;&#25104;&#21151;&#20351;&#29992;&#36828;&#31243;&#24863;&#30693;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22270;&#20687;&#20998;&#26512;&#20013;&#65292;&#20174;&#39640;&#20998;&#36776;&#29575;&#36965;&#24863;&#22270;&#20687;&#20013;&#31934;&#30830;&#22320;&#20998;&#21106;&#22823;&#21306;&#22495;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#29616;&#26377;&#30340;&#26377;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#26041;&#27861;&#37117;&#21463;&#21040;&#23545;&#35937;&#23610;&#23544;&#24040;&#22823;&#21464;&#21270;&#21644;&#23610;&#24230;&#36873;&#25321;&#22256;&#38590;&#30340;&#24433;&#21709;&#65292;&#32463;&#24120;&#23548;&#33268;&#20998;&#21106;&#31934;&#24230;&#19981;&#20339;&#12290;&#20026;&#20102;&#24212;&#23545;&#19978;&#36848;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#21306;&#22495;&#21512;&#24182;&#26041;&#27861;&#65288;DeepMerge&#65289;&#65292;&#36890;&#36807;&#23558;Transformers&#12289;&#22810;&#32423;&#23884;&#20837;&#27169;&#22359;&#12289;&#22522;&#20110;&#27573;&#30340;&#29305;&#24449;&#23884;&#20837;&#27169;&#22359;&#21644;&#21306;&#22495;&#30456;&#37051;&#22270;&#27169;&#22411;&#30456;&#32467;&#21512;&#26469;&#22788;&#29702;&#22823;&#30340;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#20998;&#21106;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#20108;&#21449;&#26641;&#37319;&#26679;&#26041;&#27861;&#65292;&#20197;&#29983;&#25104;&#22810;&#32423;&#36755;&#20837;&#65292;&#29992;&#20110;DeepMerge&#27169;&#22411;&#30340;&#36755;&#20837;&#12290;&#26681;&#25454;&#25105;&#20204;&#30340;&#26368;&#20339;&#30693;&#35782;&#65292;&#25552;&#20986;&#30340;&#26041;&#27861;&#26159;&#31532;&#19968;&#20010;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#23398;&#20064;&#30456;&#37051;&#27573;&#20043;&#38388;&#30456;&#20284;&#24615;&#36827;&#34892;&#21306;&#22495;&#21512;&#24182;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#20351;&#29992;&#36828;&#31243;&#24863;&#30693;&#25968;&#25454;&#38598;&#23545;&#25152;&#25552;&#20986;&#30340;DeepMerge&#26041;&#27861;&#36827;&#34892;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate segmentation of large areas from very high spatial-resolution (VHR) remote sensing imagery remains a challenging issue in image analysis. Existing supervised and unsupervised methods both suffer from the large variance of object sizes and the difficulty in scale selection, which often result in poor segmentation accuracies. To address the above challenges, we propose a deep learning-based region-merging method (DeepMerge) to handle the segmentation in large VHR images by integrating a Transformer with a multi-level embedding module, a segment-based feature embedding module and a region-adjacency graph model. In addition, we propose a modified binary tree sampling method to generate multi-level inputs from initial segmentation results, serving as inputs for the DeepMerge model. To our best knowledge, the proposed method is the first to use deep learning to learn the similarity between adjacent segments for region-merging. The proposed DeepMerge method is validated using a remot
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#26080;&#20154;&#26426;&#28145;&#24230;&#20272;&#35745;&#30340;&#25216;&#26415;&#65292;&#21487;&#20197;&#32852;&#21512;&#20272;&#35745;&#28145;&#24230;&#21644;&#19981;&#30830;&#23450;&#24615;&#65292;&#38024;&#23545;M4Depth&#20135;&#29983;&#30340;&#35270;&#24046;&#19981;&#30830;&#23450;&#24615;&#36827;&#34892;&#36716;&#25442;&#65292;&#23454;&#29616;&#20102;&#20248;&#20110;&#26631;&#20934;&#27010;&#29575;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#21508;&#31181;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#22987;&#32456;&#20445;&#25345;&#30528;&#20248;&#24322;&#30340;&#24615;&#33021;&#65292;&#19988;&#36895;&#24230;&#26356;&#24555;&#65292;&#20540;&#24471;&#25512;&#24191;&#12290;</title><link>http://arxiv.org/abs/2305.19780</link><description>&lt;p&gt;
&#19968;&#31181;&#32852;&#21512;&#20272;&#35745;&#26080;&#20154;&#26426;&#28145;&#24230;&#21644;&#28145;&#24230;&#19981;&#30830;&#23450;&#24615;&#30340;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
A technique to jointly estimate depth and depth uncertainty for unmanned aerial vehicles. (arXiv:2305.19780v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19780
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#26080;&#20154;&#26426;&#28145;&#24230;&#20272;&#35745;&#30340;&#25216;&#26415;&#65292;&#21487;&#20197;&#32852;&#21512;&#20272;&#35745;&#28145;&#24230;&#21644;&#19981;&#30830;&#23450;&#24615;&#65292;&#38024;&#23545;M4Depth&#20135;&#29983;&#30340;&#35270;&#24046;&#19981;&#30830;&#23450;&#24615;&#36827;&#34892;&#36716;&#25442;&#65292;&#23454;&#29616;&#20102;&#20248;&#20110;&#26631;&#20934;&#27010;&#29575;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#21508;&#31181;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#22987;&#32456;&#20445;&#25345;&#30528;&#20248;&#24322;&#30340;&#24615;&#33021;&#65292;&#19988;&#36895;&#24230;&#26356;&#24555;&#65292;&#20540;&#24471;&#25512;&#24191;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20027;&#36710;&#36742;&#22312;&#36712;&#36857;&#35268;&#21010;&#25110;&#36991;&#38556;&#26102;&#38656;&#35201;&#21487;&#38752;&#30340;&#28145;&#24230;&#20272;&#35745;&#26041;&#27861;&#65292;&#24182;&#19988;&#20272;&#35745;&#28145;&#24230;&#36755;&#20986;&#30340;&#36136;&#37327;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#22914;&#20309;&#22686;&#24378;&#38024;&#23545;&#26080;&#20154;&#26426;&#24212;&#29992;&#35774;&#35745;&#30340;&#26368;&#20808;&#36827;&#28145;&#24230;&#20272;&#35745;&#26041;&#27861;M4Depth&#65292;&#20197;&#25191;&#34892;&#32852;&#21512;&#28145;&#24230;&#21644;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#65292;&#23558;M4Depth&#20135;&#29983;&#30340;&#35270;&#24046;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#36716;&#25442;&#20026;&#28145;&#24230;&#30456;&#20851;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65292;&#24182;&#35777;&#26126;&#20854;&#20248;&#20110;&#26631;&#20934;&#30340;&#27010;&#29575;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#34920;&#29616;&#22987;&#32456;&#22914;&#19968;&#65292;&#29978;&#33267;&#22312;&#38646;&#26679;&#26412;&#36716;&#31227;&#26102;&#20063;&#33021;&#20445;&#25345;&#20248;&#24322;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#29616;&#26377;&#30340;&#22810;&#35270;&#28145;&#24230;&#20272;&#35745;&#26041;&#27861;&#30456;&#27604;&#65292;&#22312;&#22810;&#35270;&#28145;&#24230;&#20272;&#35745;&#22522;&#20934;&#27979;&#35797;&#26041;&#38754;&#34920;&#29616;&#31867;&#20284;&#65292;&#20294;&#36895;&#24230;&#26356;&#24555;&#65292;&#21487;&#36896;&#25104;&#65292;&#26159;&#20854;&#20182;&#26041;&#27861;&#30340;2.5&#20493;&#12290;&#20195;&#30721;&#21487;&#20379;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
When used by autonomous vehicles for trajectory planning or obstacle avoidance, depth estimation methods need to be reliable. Therefore, estimating the quality of the depth outputs is critical. In this paper, we show how M4Depth, a state-of-the-art depth estimation method designed for unmanned aerial vehicle (UAV) applications, can be enhanced to perform joint depth and uncertainty estimation. For that, we present a solution to convert the uncertainty estimates related to parallax generated by M4Depth into uncertainty estimates related to depth, and show that it outperforms the standard probabilistic approach. Our experiments on various public datasets demonstrate that our method performs consistently, even in zero-shot transfer. Besides, our method offers a compelling value when compared to existing multi-view depth estimation methods as it performs similarly on a multi-view depth estimation benchmark despite being 2.5 times faster and causal, as opposed to other methods. The code of 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#21058;&#37327;&#32452;&#21512;&#30340;&#26032;&#39062;&#21487;&#38752;&#30340;&#33073;&#26426;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#19977;&#20010;&#27493;&#39588;&#23454;&#29616;&#65306;&#24320;&#21457;&#31070;&#32463;&#32593;&#32476;&#20272;&#35745;&#20010;&#24615;&#21270;&#30340;&#21058;&#37327;-&#21453;&#24212;&#65292;&#20272;&#35745;&#20542;&#21521;&#24471;&#20998;&#26816;&#27979;&#20849;&#20139;&#21327;&#21464;&#37327;-&#27835;&#30103;&#31354;&#38388;&#20013;&#30340;&#37325;&#21472;&#26377;&#38480;&#21306;&#22495;&#65292;&#28982;&#21518;&#22522;&#20110;&#26799;&#24230;&#30340;&#23398;&#20064;&#31639;&#27861;&#25214;&#21040;&#26368;&#20339;&#30340;&#20010;&#24615;&#21270;&#21058;&#37327;&#32452;&#21512;&#12290;</title><link>http://arxiv.org/abs/2305.19742</link><description>&lt;p&gt;
&#29992;&#20110;&#21058;&#37327;&#32452;&#21512;&#30340;&#21487;&#38752;&#33073;&#26426;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Reliable Off-Policy Learning for Dosage Combinations. (arXiv:2305.19742v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19742
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#21058;&#37327;&#32452;&#21512;&#30340;&#26032;&#39062;&#21487;&#38752;&#30340;&#33073;&#26426;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#19977;&#20010;&#27493;&#39588;&#23454;&#29616;&#65306;&#24320;&#21457;&#31070;&#32463;&#32593;&#32476;&#20272;&#35745;&#20010;&#24615;&#21270;&#30340;&#21058;&#37327;-&#21453;&#24212;&#65292;&#20272;&#35745;&#20542;&#21521;&#24471;&#20998;&#26816;&#27979;&#20849;&#20139;&#21327;&#21464;&#37327;-&#27835;&#30103;&#31354;&#38388;&#20013;&#30340;&#37325;&#21472;&#26377;&#38480;&#21306;&#22495;&#65292;&#28982;&#21518;&#22522;&#20110;&#26799;&#24230;&#30340;&#23398;&#20064;&#31639;&#27861;&#25214;&#21040;&#26368;&#20339;&#30340;&#20010;&#24615;&#21270;&#21058;&#37327;&#32452;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20010;&#24615;&#21270;&#21307;&#23398;&#39046;&#22495;&#30340;&#20915;&#31574;&#21046;&#23450;&#65292;&#22914;&#30284;&#30151;&#27835;&#30103;&#25110;&#21361;&#37325;&#25252;&#29702;&#65292;&#36890;&#24120;&#24517;&#39035;&#23545;&#21058;&#37327;&#32452;&#21512;&#36827;&#34892;&#36873;&#25321;&#65292;&#21363;&#22810;&#31181;&#36830;&#32493;&#27835;&#30103;&#12290;&#29616;&#26377;&#30340;&#36825;&#39033;&#20219;&#21153;&#30340;&#24037;&#20316;&#24050;&#32463;&#29420;&#31435;&#22320;&#24314;&#27169;&#20102;&#22810;&#31181;&#27835;&#30103;&#30340;&#25928;&#26524;&#65292;&#32780;&#20272;&#35745;&#32852;&#21512;&#25928;&#26524;&#21364;&#21463;&#21040;&#20102;&#24456;&#23569;&#30340;&#20851;&#27880;&#65292;&#24182;&#19988;&#38754;&#20020;&#30528;&#38750;&#24179;&#20961;&#30340;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#21058;&#37327;&#32452;&#21512;&#30340;&#21487;&#38752;&#33073;&#26426;&#23398;&#20064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20998;&#20026;&#19977;&#20010;&#27493;&#39588;&#65306;&#65288;1&#65289;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#29305;&#23450;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#20272;&#35745;&#20010;&#24615;&#21270;&#30340;&#21058;&#37327;-&#21453;&#24212;&#20989;&#25968;&#65292;&#21516;&#26102;&#32771;&#34385;&#22810;&#20010;&#30456;&#20851;&#21058;&#37327;&#30340;&#32852;&#21512;&#25928;&#24212;&#12290;&#65288;2&#65289;&#25105;&#20204;&#20351;&#29992;&#26465;&#20214;&#27491;&#24577;&#21270;&#27969;&#37327;&#20272;&#35745;&#24191;&#20041;&#20542;&#21521;&#24471;&#20998;&#65292;&#20197;&#26816;&#27979;&#20849;&#20139;&#21327;&#21464;&#37327;-&#27835;&#30103;&#31354;&#38388;&#20013;&#37325;&#21472;&#26377;&#38480;&#30340;&#21306;&#22495;&#12290;&#65288;3&#65289;&#25105;&#20204;&#25552;&#20379;&#19968;&#31181;&#22522;&#20110;&#26799;&#24230;&#30340;&#23398;&#20064;&#31639;&#27861;&#65292;&#20197;&#25214;&#21040;&#26368;&#20339;&#30340;&#20010;&#24615;&#21270;&#21058;&#37327;&#32452;&#21512;&#12290;&#22312;&#27492;&#65292;&#25105;&#20204;&#30830;&#20445;&#21487;&#38752;&#22320;&#20272;&#35745;&#31574;&#30053;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Decision-making in personalized medicine such as cancer therapy or critical care must often make choices for dosage combinations, i.e., multiple continuous treatments. Existing work for this task has modeled the effect of multiple treatments independently, while estimating the joint effect has received little attention but comes with non-trivial challenges. In this paper, we propose a novel method for reliable off-policy learning for dosage combinations. Our method proceeds along three steps: (1) We develop a tailored neural network that estimates the individualized dose-response function while accounting for the joint effect of multiple dependent dosages. (2) We estimate the generalized propensity score using conditional normalizing flows in order to detect regions with limited overlap in the shared covariate-treatment space. (3) We present a gradient-based learning algorithm to find the optimal, individualized dosage combinations. Here, we ensure reliable estimation of the policy val
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;&#30693;&#35782;&#24211;&#30340;&#31995;&#32479;&#65292;&#21487;&#20197;&#22238;&#31572;&#22797;&#26434;&#33258;&#28982;&#35821;&#35328;&#30340;&#26597;&#35810;&#65292;&#25903;&#25345;&#24037;&#31243;&#24072;&#35775;&#38382;&#22826;&#31354;&#30862;&#29255;&#29615;&#22659;&#30340;&#30693;&#35782;&#24211;&#20013;&#30340;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2305.19734</link><description>&lt;p&gt;
&#22522;&#20110;&#30693;&#35782;&#24211;&#30340;&#22826;&#31354;&#30862;&#29255;&#38382;&#31572;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Knowledge Base Question Answering for Space Debris Queries. (arXiv:2305.19734v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19734
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;&#30693;&#35782;&#24211;&#30340;&#31995;&#32479;&#65292;&#21487;&#20197;&#22238;&#31572;&#22797;&#26434;&#33258;&#28982;&#35821;&#35328;&#30340;&#26597;&#35810;&#65292;&#25903;&#25345;&#24037;&#31243;&#24072;&#35775;&#38382;&#22826;&#31354;&#30862;&#29255;&#29615;&#22659;&#30340;&#30693;&#35782;&#24211;&#20013;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22826;&#31354;&#26426;&#26500;&#25191;&#34892;&#22797;&#26434;&#30340;&#21355;&#26143;&#25805;&#20316;&#65292;&#38656;&#35201;&#22312;&#20854;&#24191;&#27867;&#30340;&#20449;&#24687;&#31995;&#32479;&#20013;&#23384;&#20648;&#21644;&#35775;&#38382;&#25216;&#26415;&#30693;&#35782;&#12290;&#30693;&#35782;&#24211;&#26159;&#20197;&#35268;&#27169;&#23384;&#20648;&#21644;&#35775;&#38382;&#27492;&#31867;&#20449;&#24687;&#30340;&#26377;&#25928;&#26041;&#24335;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#20026;&#27431;&#27954;&#31354;&#38388;&#23616;&#65288;ESA&#65289;&#24320;&#21457;&#30340;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#21487;&#20197;&#22238;&#31572;&#22797;&#26434;&#30340;&#33258;&#28982;&#35821;&#35328;&#26597;&#35810;&#65292;&#20197;&#25903;&#25345;&#24037;&#31243;&#24072;&#35775;&#38382;&#27169;&#25311;&#36712;&#36947;&#22826;&#31354;&#30862;&#29255;&#29615;&#22659;&#30340;&#30693;&#35782;&#24211;&#20013;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#22522;&#20110;&#19968;&#31181;&#31649;&#36947;&#27969;&#31243;&#65292;&#39318;&#20808;&#20174;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#29983;&#25104;&#19968;&#31995;&#21015;&#22522;&#26412;&#25968;&#25454;&#24211;&#25805;&#20316;&#65292;&#31216;&#20026;&#8220;&#31243;&#24207;&#33609;&#22270;&#8221;&#65292;&#28982;&#21518;&#23558;&#33609;&#22270;&#36716;&#25442;&#20026;&#20855;&#20307;&#30340;&#26597;&#35810;&#31243;&#24207;&#65292;&#21253;&#25324;&#23454;&#20307;&#12289;&#23646;&#24615;&#21644;&#20851;&#31995;&#30340;&#25552;&#21450;&#65292;&#26368;&#21518;&#38024;&#23545;&#25968;&#25454;&#24211;&#25191;&#34892;&#31243;&#24207;&#12290;&#36825;&#31181;&#27969;&#31243;&#20998;&#35299;&#26041;&#27861;&#20351;&#25105;&#20204;&#33021;&#22815;&#36890;&#36807;&#21033;&#29992;&#22495;&#22806;&#25968;&#25454;&#21644;&#30001;GPT-3&#29983;&#25104;&#30340;&#21322;&#21512;&#25104;&#25968;&#25454;&#26469;&#35757;&#32451;&#31995;&#32479;&#65292;&#20174;&#32780;&#20943;&#23569;&#36807;&#24230;&#25311;&#21512;&#21644;&#28431;&#27934;&#12290;
&lt;/p&gt;
&lt;p&gt;
Space agencies execute complex satellite operations that need to be supported by the technical knowledge contained in their extensive information systems. Knowledge bases (KB) are an effective way of storing and accessing such information at scale. In this work we present a system, developed for the European Space Agency (ESA), that can answer complex natural language queries, to support engineers in accessing the information contained in a KB that models the orbital space debris environment. Our system is based on a pipeline which first generates a sequence of basic database operations, called a %program sketch, from a natural language question, then specializes the sketch into a concrete query program with mentions of entities, attributes and relations, and finally executes the program against the database. This pipeline decomposition approach enables us to train the system by leveraging out-of-domain data and semi-synthetic data generated by GPT-3, thus reducing overfitting and shor
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23481;&#38169;&#35780;&#20272;&#26041;&#27861;&#65292;&#31216;&#20026;APPRAISER&#65292;&#23427;&#37319;&#29992;&#36817;&#20284;&#35823;&#24046;&#36827;&#34892;DNN&#25925;&#38556;&#23481;&#24525;&#24615;&#20998;&#26512;&#12290;APPRAISER&#25552;&#20379;&#20102;&#25104;&#21315;&#19978;&#19975;&#20493;&#30340;&#35780;&#20272;&#36895;&#24230;&#25552;&#21319;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#20998;&#26512;&#30340;&#39640;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.19733</link><description>&lt;p&gt;
APPRAISER: &#21033;&#29992;&#36817;&#20284;&#35823;&#24046;&#36827;&#34892;DNN&#25925;&#38556;&#23481;&#24525;&#24615;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
APPRAISER: DNN Fault Resilience Analysis Employing Approximation Errors. (arXiv:2305.19733v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19733
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23481;&#38169;&#35780;&#20272;&#26041;&#27861;&#65292;&#31216;&#20026;APPRAISER&#65292;&#23427;&#37319;&#29992;&#36817;&#20284;&#35823;&#24046;&#36827;&#34892;DNN&#25925;&#38556;&#23481;&#24525;&#24615;&#20998;&#26512;&#12290;APPRAISER&#25552;&#20379;&#20102;&#25104;&#21315;&#19978;&#19975;&#20493;&#30340;&#35780;&#20272;&#36895;&#24230;&#25552;&#21319;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#20998;&#26512;&#30340;&#39640;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20170;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#24341;&#36215;&#20102;&#26032;&#30340;&#21487;&#38752;&#24615;&#20851;&#27880;&#12290;&#23454;&#38469;&#19978;&#65292;&#36890;&#36807;&#30828;&#20214;&#20223;&#30495;&#36827;&#34892;&#25925;&#38556;&#27880;&#20837;&#30340;&#26041;&#27861;&#26159;&#30740;&#31350;DNN&#26550;&#26500;&#23481;&#38169;&#24615;&#30340;&#26377;&#25928;&#21644;&#24191;&#27867;&#20351;&#29992;&#30340;&#26041;&#27861;&#65292;&#26089;&#26399;&#35774;&#35745;&#38454;&#27573;&#24050;&#32463;&#20943;&#36731;&#20102;&#21487;&#38752;&#24615;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;&#25925;&#38556;&#27880;&#20837;&#26041;&#27861;&#22312;&#26102;&#38388;&#12289;&#35774;&#35745;&#21644;&#25511;&#21046;&#22797;&#26434;&#24615;&#26041;&#38754;&#23384;&#22312;&#19968;&#31995;&#21015;&#38382;&#39064;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23481;&#38169;&#35780;&#20272;&#26041;&#27861;&#65292;&#31216;&#20026;APPRAISER&#65292;&#23427;&#23558;&#20989;&#25968;&#36817;&#20284;&#29992;&#20110;&#38750;&#20256;&#32479;&#29992;&#36884;&#65292;&#24182;&#21033;&#29992;&#36817;&#20284;&#35745;&#31639;&#35823;&#24046;&#12290;&#36890;&#36807;&#22312;&#23481;&#38169;&#35780;&#20272;&#39046;&#22495;&#37319;&#29992;&#36825;&#20010;&#27010;&#24565;&#65292;APPRAISER&#25552;&#20379;&#20102;&#25104;&#21315;&#19978;&#19975;&#20493;&#30340;&#35780;&#20272;&#36895;&#24230;&#25552;&#21319;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#20998;&#26512;&#30340;&#39640;&#20934;&#30830;&#24615;&#12290;&#26412;&#25991;&#36890;&#36807;&#19982;&#26368;&#20808;&#36827;&#30340;&#25925;&#38556;&#27880;&#20837;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#65292;&#39564;&#35777;&#20102;APPRAISER&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Nowadays, the extensive exploitation of Deep Neural Networks (DNNs) in safety-critical applications raises new reliability concerns. In practice, methods for fault injection by emulation in hardware are efficient and widely used to study the resilience of DNN architectures for mitigating reliability issues already at the early design stages. However, the state-of-the-art methods for fault injection by emulation incur a spectrum of time-, design- and control-complexity problems. To overcome these issues, a novel resiliency assessment method called APPRAISER is proposed that applies functional approximation for a non-conventional purpose and employs approximate computing errors for its interest. By adopting this concept in the resiliency assessment domain, APPRAISER provides thousands of times speed-up in the assessment process, while keeping high accuracy of the analysis. In this paper, APPRAISER is validated by comparing it with state-of-the-art approaches for fault injection by emulat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31895;&#31961;&#38598;&#29702;&#35770;&#30340;&#35268;&#21017;&#36890;&#29992;&#36870;&#25512;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#36716;&#21270;&#30446;&#26631;&#27010;&#24565;&#21644;&#23376;&#27010;&#24565;&#20026;&#20449;&#24687;&#34920;&#65292;&#20197;&#26356;&#20302;&#30340;&#25104;&#26412;&#35299;&#20915;&#39046;&#22495;&#30693;&#35782;&#33719;&#21462;&#21644;&#35268;&#21017;&#30340;&#20462;&#27491;&#12289;&#20943;&#23569;&#12289;&#29983;&#25104;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.19718</link><description>&lt;p&gt;
&#31895;&#31961;&#38598;&#19979;&#19968;&#31181;&#35268;&#21017;&#36890;&#29992;&#36870;&#25512;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A rule-general abductive learning by rough sets. (arXiv:2305.19718v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19718
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31895;&#31961;&#38598;&#29702;&#35770;&#30340;&#35268;&#21017;&#36890;&#29992;&#36870;&#25512;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#36716;&#21270;&#30446;&#26631;&#27010;&#24565;&#21644;&#23376;&#27010;&#24565;&#20026;&#20449;&#24687;&#34920;&#65292;&#20197;&#26356;&#20302;&#30340;&#25104;&#26412;&#35299;&#20915;&#39046;&#22495;&#30693;&#35782;&#33719;&#21462;&#21644;&#35268;&#21017;&#30340;&#20462;&#27491;&#12289;&#20943;&#23569;&#12289;&#29983;&#25104;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#20219;&#21153;&#20013;&#65292;&#36890;&#24120;&#23384;&#22312;&#22823;&#37327;&#26410;&#26631;&#35760;&#25968;&#25454;&#21644;&#26631;&#35760;&#25968;&#25454;&#12290;&#23558;&#20004;&#32773;&#32452;&#21512;&#36215;&#26469;&#36827;&#34892;&#23398;&#20064;&#30340;&#20219;&#21153;&#34987;&#31216;&#20026;&#21322;&#30417;&#30563;&#23398;&#20064;&#12290;&#19987;&#23478;&#21487;&#20197;&#20351;&#29992;&#36923;&#36753;&#35268;&#21017;&#26469;&#26631;&#35760;&#26410;&#26631;&#35760;&#25968;&#25454;&#65292;&#20294;&#36825;&#20010;&#25805;&#20316;&#24456;&#26114;&#36149;&#12290;&#24863;&#30693;&#21644;&#25512;&#29702;&#30340;&#32467;&#21512;&#22312;&#22788;&#29702;&#20855;&#26377;&#39046;&#22495;&#30693;&#35782;&#30340;&#21322;&#30417;&#30563;&#20219;&#21153;&#26041;&#38754;&#20855;&#26377;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;&#28982;&#32780;&#65292;&#33719;&#21462;&#39046;&#22495;&#30693;&#35782;&#20197;&#21450;&#35268;&#21017;&#30340;&#20462;&#27491;&#12289;&#20943;&#23569;&#21644;&#29983;&#25104;&#20173;&#28982;&#26159;&#38656;&#35201;&#35299;&#20915;&#30340;&#22797;&#26434;&#38382;&#39064;&#12290;&#31895;&#31961;&#38598;&#29702;&#35770;&#26159;&#35299;&#20915;&#20449;&#24687;&#31995;&#32479;&#20013;&#30693;&#35782;&#22788;&#29702;&#30340;&#37325;&#35201;&#26041;&#27861;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31895;&#31961;&#38598;&#19979;&#30340;&#35268;&#21017;&#36890;&#29992;&#36870;&#25512;&#23398;&#20064;&#26041;&#27861;&#65288;RS-ABL&#65289;&#12290;&#36890;&#36807;&#23558;&#35268;&#21017;&#30340;&#30446;&#26631;&#27010;&#24565;&#21644;&#23376;&#27010;&#24565;&#36716;&#21270;&#20026;&#20449;&#24687;&#34920;&#65292;&#21033;&#29992;&#31895;&#31961;&#38598;&#29702;&#35770;&#26469;&#35299;&#20915;&#20197;&#26356;&#20302;&#30340;&#25104;&#26412;&#33719;&#21462;&#39046;&#22495;&#30693;&#35782;&#21644;&#20462;&#27491;&#12289;&#20943;&#23569;&#12289;&#29983;&#25104;&#35268;&#21017;&#30340;&#38382;&#39064;&#12290;&#35813;&#26694;&#26550;&#36824;&#21487;&#20197;&#29983;&#25104;&#26356;&#24191;&#27867;&#30340;&#36127;&#35268;&#21017;&#65292;&#20197;&#22686;&#24378;&#35268;&#21017;&#33539;&#22260;&#12290;
&lt;/p&gt;
&lt;p&gt;
In real-world tasks, there is usually a large amount of unlabeled data and labeled data. The task of combining the two to learn is known as semi-supervised learning. Experts can use logical rules to label unlabeled data, but this operation is costly. The combination of perception and reasoning has a good effect in processing such semi-supervised tasks with domain knowledge. However, acquiring domain knowledge and the correction, reduction and generation of rules remain complex problems to be solved. Rough set theory is an important method for solving knowledge processing in information systems. In this paper, we propose a rule general abductive learning by rough set (RS-ABL). By transforming the target concept and sub-concepts of rules into information tables, rough set theory is used to solve the acquisition of domain knowledge and the correction, reduction and generation of rules at a lower cost. This framework can also generate more extensive negative rules to enhance the breadth of
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#21160;&#24577;&#35268;&#21010;&#26041;&#27861;&#26469;&#20248;&#21270;&#20219;&#20309;&#32452;&#21512;&#30340;&#21487;&#20998;&#31163;&#30446;&#26631;&#21644;&#32422;&#26463;&#26465;&#20214;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#21487;&#25193;&#23637;&#24615;&#26041;&#38754;&#27604;&#36890;&#29992;&#27714;&#35299;&#22120;&#34920;&#29616;&#24471;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2305.19706</link><description>&lt;p&gt;
&#21487;&#20998;&#30446;&#26631;&#30340;&#26368;&#20248;&#20915;&#31574;&#26641;&#65306;&#25512;&#21160;&#21160;&#24577;&#35268;&#21010;&#30340;&#26497;&#38480;
&lt;/p&gt;
&lt;p&gt;
Optimal Decision Trees for Separable Objectives: Pushing the Limits of Dynamic Programming. (arXiv:2305.19706v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19706
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#21160;&#24577;&#35268;&#21010;&#26041;&#27861;&#26469;&#20248;&#21270;&#20219;&#20309;&#32452;&#21512;&#30340;&#21487;&#20998;&#31163;&#30446;&#26631;&#21644;&#32422;&#26463;&#26465;&#20214;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#21487;&#25193;&#23637;&#24615;&#26041;&#38754;&#27604;&#36890;&#29992;&#27714;&#35299;&#22120;&#34920;&#29616;&#24471;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20915;&#31574;&#26641;&#30340;&#20840;&#23616;&#20248;&#21270;&#22312;&#20934;&#30830;&#24615;&#65292;&#22823;&#23567;&#21644;&#20154;&#31867;&#21487;&#29702;&#35299;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#21069;&#26223;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#26041;&#27861;&#20173;&#28982;&#20381;&#36182;&#20110;&#36890;&#29992;&#27714;&#35299;&#22120;&#65292;&#21487;&#25193;&#23637;&#24615;&#20173;&#28982;&#26159;&#19968;&#20010;&#38382;&#39064;&#12290;&#21160;&#24577;&#35268;&#21010;&#26041;&#27861;&#24050;&#34987;&#35777;&#26126;&#20855;&#26377;&#26356;&#22909;&#30340;&#21487;&#25193;&#23637;&#24615;&#65292;&#22240;&#20026;&#23427;&#20204;&#36890;&#36807;&#23558;&#23376;&#26641;&#20316;&#20026;&#29420;&#31435;&#30340;&#23376;&#38382;&#39064;&#35299;&#20915;&#26469;&#21033;&#29992;&#26641;&#32467;&#26500;&#12290;&#28982;&#32780;&#65292;&#36825;&#20165;&#36866;&#29992;&#20110;&#21487;&#20197;&#20998;&#21035;&#20248;&#21270;&#23376;&#26641;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#35814;&#32454;&#30740;&#31350;&#20102;&#36825;&#31181;&#20851;&#31995;&#65292;&#24182;&#23637;&#31034;&#20102;&#23454;&#29616;&#36825;&#31181;&#21487;&#20998;&#31163;&#32422;&#26463;&#21644;&#30446;&#26631;&#20219;&#24847;&#32452;&#21512;&#30340;&#21160;&#24577;&#35268;&#21010;&#26041;&#27861;&#12290;&#22312;&#22235;&#20010;&#24212;&#29992;&#39046;&#22495;&#30340;&#23454;&#39564;&#34920;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#26222;&#36866;&#24615;&#65292;&#21516;&#26102;&#20063;&#27604;&#36890;&#29992;&#27714;&#35299;&#22120;&#20855;&#26377;&#26356;&#22909;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Global optimization of decision trees has shown to be promising in terms of accuracy, size, and consequently human comprehensibility. However, many of the methods used rely on general-purpose solvers for which scalability remains an issue. Dynamic programming methods have been shown to scale much better because they exploit the tree structure by solving subtrees as independent subproblems. However, this only works when an objective can be optimized separately for subtrees. We explore this relationship in detail and show necessary and sufficient conditions for such separability and generalize previous dynamic programming approaches into a framework that can optimize any combination of separable objectives and constraints. Experiments on four application domains show the general applicability of this framework, while outperforming the scalability of general-purpose solvers by a large margin.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;Metropolis-Hastings&#32806;&#21512;&#21644;&#23616;&#37096;&#27169;&#24577;&#21021;&#22987;&#21270;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#28145;&#24230;&#29627;&#23572;&#20857;&#26364;&#26426;&#20013;&#30340;&#20559;&#24046;&#26799;&#24230;&#20272;&#35745;&#38382;&#39064;&#65292;&#20351;&#24471;DBMs&#21487;&#20197;&#31471;&#21040;&#31471;&#22320;&#35757;&#32451;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#19982;&#20854;&#20182;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#30456;&#24403;&#30340;&#29983;&#25104;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.19684</link><description>&lt;p&gt;
&#36890;&#36807;&#23616;&#37096;&#27169;&#24577;&#21021;&#22987;&#21270;&#21644;&#26080;&#20559;&#24046;&#23545;&#27604;&#25955;&#24230;&#23454;&#29616;&#28145;&#24230;&#29627;&#23572;&#20857;&#26364;&#26426;&#30340;&#31471;&#21040;&#31471;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
End-to-end Training of Deep Boltzmann Machines by Unbiased Contrastive Divergence with Local Mode Initialization. (arXiv:2305.19684v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19684
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;Metropolis-Hastings&#32806;&#21512;&#21644;&#23616;&#37096;&#27169;&#24577;&#21021;&#22987;&#21270;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#28145;&#24230;&#29627;&#23572;&#20857;&#26364;&#26426;&#20013;&#30340;&#20559;&#24046;&#26799;&#24230;&#20272;&#35745;&#38382;&#39064;&#65292;&#20351;&#24471;DBMs&#21487;&#20197;&#31471;&#21040;&#31471;&#22320;&#35757;&#32451;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#19982;&#20854;&#20182;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#30456;&#24403;&#30340;&#29983;&#25104;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35299;&#20915;&#20102;&#28145;&#24230;&#29627;&#23572;&#20857;&#26364;&#26426;&#65288;DBMs&#65289;&#20013;&#30340;&#20559;&#24046;&#26799;&#24230;&#20272;&#35745;&#38382;&#39064;&#12290;&#29616;&#26377;&#30340;&#33719;&#21462;&#26080;&#20559;&#20272;&#35745;&#37327;&#30340;&#26041;&#27861;&#20351;&#29992;&#22522;&#20110;Gibbs&#37319;&#26679;&#30340;&#26368;&#22823;&#32806;&#21512;&#65292;&#20294;&#24403;&#29366;&#24577;&#26159;&#39640;&#32500;&#26102;&#65292;&#20854;&#25910;&#25947;&#38656;&#35201;&#24456;&#38271;&#26102;&#38388;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;Metropolis-Hastings&#30340;&#32806;&#21512;&#65292;&#24182;&#22260;&#32469;&#30446;&#26631;&#20998;&#24067;&#30340;&#23616;&#37096;&#27169;&#24577;&#21021;&#22987;&#21270;&#29366;&#24577;&#12290;&#30001;&#20110;MH&#20542;&#21521;&#20110;&#25298;&#32477;&#25552;&#26696;&#65292;&#36825;&#31181;&#32806;&#21512;&#26377;&#24456;&#39640;&#30340;&#27010;&#29575;&#22312;&#19968;&#27493;&#20869;&#25910;&#25947;&#65292;&#22240;&#27492;&#20855;&#26377;&#39640;&#25928;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#19981;&#36827;&#34892;&#36138;&#24515;&#39044;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#31471;&#21040;&#31471;&#22320;&#35757;&#32451;DBMs&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20123;&#23454;&#29992;&#25216;&#26415;&#65292;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;DBMs&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#35757;&#32451;&#31639;&#27861;&#20351;DBMs&#33021;&#22815;&#23637;&#29616;&#20986;&#19982;&#20854;&#20182;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#30456;&#24403;&#30340;&#29983;&#25104;&#24615;&#33021;&#65292;&#22312;MNIST&#19978;&#36798;&#21040;&#20102;10.33&#30340;FID&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
We address the problem of biased gradient estimation in deep Boltzmann machines (DBMs). The existing method to obtain an unbiased estimator uses a maximal coupling based on a Gibbs sampler, but when the state is high-dimensional, it takes a long time to converge. In this study, we propose to use a coupling based on the Metropolis-Hastings (MH) and to initialize the state around a local mode of the target distribution. Because of the propensity of MH to reject proposals, the coupling tends to converge in only one step with a high probability, leading to high efficiency. We find that our method allows DBMs to be trained in an end-to-end fashion without greedy pretraining. We also propose some practical techniques to further improve the performance of DBMs. We empirically demonstrate that our training algorithm enables DBMs to show comparable generative performance to other deep generative models, achieving the FID score of 10.33 for MNIST.
&lt;/p&gt;</description></item><item><title>AutoDDPM&#26159;&#19968;&#31181;&#36890;&#36807;&#33258;&#21160;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#28508;&#22312;&#24322;&#24120;&#30340;&#21021;&#22987;&#21487;&#33021;&#24615;&#22320;&#22270;&#65292;&#24182;&#23558;&#20854;&#19982;&#21407;&#22987;&#22270;&#20687;&#34701;&#21512;&#65292;&#36890;&#36807;&#32852;&#21512;&#22122;&#22768;&#20998;&#24067;&#37325;&#26032;&#37319;&#26679;&#26469;&#22686;&#24378;&#25193;&#25955;&#27169;&#22411;&#40065;&#26834;&#24615;&#30340;&#26041;&#27861;&#65292;&#23427;&#22312;&#20445;&#30041;&#20581;&#24247;&#32452;&#32455;&#30340;&#24773;&#20917;&#19979;&#26367;&#25442;&#24322;&#24120;&#21306;&#22495;&#65292;&#26126;&#26174;&#36229;&#36234;&#20102;&#25193;&#25955;&#27169;&#22411;&#30340;&#23616;&#38480;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.19643</link><description>&lt;p&gt;
&#36890;&#36807;&#33258;&#21160;&#25193;&#25955;&#27169;&#22411;&#22686;&#24378;&#24322;&#24120;&#26816;&#27979;&#30340;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#24615;&#65306;&#25513;&#30422;&#12289;&#25340;&#21512;&#21644;&#37325;&#26032;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
Mask, Stitch, and Re-Sample: Enhancing Robustness and Generalizability in Anomaly Detection through Automatic Diffusion Models. (arXiv:2305.19643v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19643
&lt;/p&gt;
&lt;p&gt;
AutoDDPM&#26159;&#19968;&#31181;&#36890;&#36807;&#33258;&#21160;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#28508;&#22312;&#24322;&#24120;&#30340;&#21021;&#22987;&#21487;&#33021;&#24615;&#22320;&#22270;&#65292;&#24182;&#23558;&#20854;&#19982;&#21407;&#22987;&#22270;&#20687;&#34701;&#21512;&#65292;&#36890;&#36807;&#32852;&#21512;&#22122;&#22768;&#20998;&#24067;&#37325;&#26032;&#37319;&#26679;&#26469;&#22686;&#24378;&#25193;&#25955;&#27169;&#22411;&#40065;&#26834;&#24615;&#30340;&#26041;&#27861;&#65292;&#23427;&#22312;&#20445;&#30041;&#20581;&#24247;&#32452;&#32455;&#30340;&#24773;&#20917;&#19979;&#26367;&#25442;&#24322;&#24120;&#21306;&#22495;&#65292;&#26126;&#26174;&#36229;&#36234;&#20102;&#25193;&#25955;&#27169;&#22411;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24322;&#24120;&#26816;&#27979;&#20013;&#24341;&#20837;&#25193;&#25955;&#27169;&#22411;&#20026;&#30149;&#29702;&#22270;&#20687;&#30340;&#26356;&#26377;&#25928;&#21644;&#20934;&#30830;&#30340;&#37325;&#24314;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#36890;&#36807;&#35843;&#33410;&#22122;&#22768;&#31890;&#24230;&#26469;&#25511;&#21046;&#27169;&#22411;&#30340;&#36890;&#29992;&#24615;&#23384;&#22312;&#23616;&#38480;&#24615;&#65292;&#24182;&#22952;&#30861;&#20102;&#23545;&#20581;&#24247;&#32452;&#32455;&#30340;&#24674;&#22797;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;AutoDDPM&#65292;&#19968;&#31181;&#22686;&#24378;&#25193;&#25955;&#27169;&#22411;&#40065;&#26834;&#24615;&#30340;&#26032;&#26041;&#27861;&#12290;AutoDDPM&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#28508;&#22312;&#24322;&#24120;&#30340;&#21021;&#22987;&#21487;&#33021;&#24615;&#22320;&#22270;&#65292;&#24182;&#23558;&#20854;&#19982;&#21407;&#22987;&#22270;&#20687;&#26080;&#32541;&#38598;&#25104;&#12290;&#36890;&#36807;&#32852;&#21512;&#22122;&#22768;&#20998;&#24067;&#37325;&#26032;&#37319;&#26679;&#65292;AutoDDPM&#23454;&#29616;&#20102;&#21327;&#35843;&#21644;&#20462;&#34917;&#25928;&#26524;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#35777;&#26126;&#20102;AutoDDPM&#30340;&#26377;&#25928;&#24615;&#65292;&#22312;&#20445;&#30041;&#20581;&#24247;&#32452;&#32455;&#30340;&#24773;&#20917;&#19979;&#26367;&#25442;&#24322;&#24120;&#21306;&#22495;&#65292;&#26126;&#26174;&#36229;&#36234;&#20102;&#25193;&#25955;&#27169;&#22411;&#30340;&#23616;&#38480;&#24615;&#12290;&#23427;&#36824;&#20026;&#24403;&#21069;&#25193;&#25955;&#27169;&#22411;&#30340;&#23616;&#38480;&#24615;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#21644;&#20998;&#26512;&#65292;&#20419;&#36827;&#20102;&#40065;&#26834;&#21644;&#26222;&#36866;&#30340;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
The introduction of diffusion models in anomaly detection has paved the way for more effective and accurate image reconstruction in pathologies. However, the current limitations in controlling noise granularity hinder diffusion models' ability to generalize across diverse anomaly types and compromise the restoration of healthy tissues. To overcome these challenges, we propose AutoDDPM, a novel approach that enhances the robustness of diffusion models. AutoDDPM utilizes diffusion models to generate initial likelihood maps of potential anomalies and seamlessly integrates them with the original image. Through joint noised distribution re-sampling, AutoDDPM achieves harmonization and in-painting effects. Our study demonstrates the efficacy of AutoDDPM in replacing anomalous regions while preserving healthy tissues, considerably surpassing diffusion models' limitations. It also contributes valuable insights and analysis on the limitations of current diffusion models, promoting robust and in
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25554;&#20540;&#30340;&#25991;&#26412;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#8212;&#8212;&#27969;&#24418;&#20132;&#25442;Mixup(MSMix)&#65292;&#36890;&#36807;&#22312;&#32593;&#32476;&#30340;&#29305;&#23450;&#23618;&#37096;&#20998;&#26367;&#25442;&#38544;&#34255;&#29305;&#24449;&#26469;&#20174;&#20004;&#20010;&#19981;&#21516;&#30340;&#26679;&#26412;&#20013;&#33719;&#24471;&#26356;&#20016;&#23500;&#30340;&#38544;&#34255;&#34920;&#31034;&#65292;&#24182;&#22312;&#19977;&#20010;&#20013;&#25991;&#24847;&#22270;&#35782;&#21035;&#25968;&#25454;&#38598;&#19978;&#23454;&#39564;&#35777;&#26126;&#20102;MSMix&#22312;&#23436;&#25972;&#26679;&#26412;&#21644;&#23567;&#26679;&#26412;&#37197;&#32622;&#19979;&#22343;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.19617</link><description>&lt;p&gt;
MSMix: &#22522;&#20110;&#25554;&#20540;&#30340;&#25991;&#26412;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#8212;&#8212;&#27969;&#24418;&#20132;&#25442;Mixup
&lt;/p&gt;
&lt;p&gt;
MSMix:An Interpolation-Based Text Data Augmentation Method Manifold Swap Mixup. (arXiv:2305.19617v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19617
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25554;&#20540;&#30340;&#25991;&#26412;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#8212;&#8212;&#27969;&#24418;&#20132;&#25442;Mixup(MSMix)&#65292;&#36890;&#36807;&#22312;&#32593;&#32476;&#30340;&#29305;&#23450;&#23618;&#37096;&#20998;&#26367;&#25442;&#38544;&#34255;&#29305;&#24449;&#26469;&#20174;&#20004;&#20010;&#19981;&#21516;&#30340;&#26679;&#26412;&#20013;&#33719;&#24471;&#26356;&#20016;&#23500;&#30340;&#38544;&#34255;&#34920;&#31034;&#65292;&#24182;&#22312;&#19977;&#20010;&#20013;&#25991;&#24847;&#22270;&#35782;&#21035;&#25968;&#25454;&#38598;&#19978;&#23454;&#39564;&#35777;&#26126;&#20102;MSMix&#22312;&#23436;&#25972;&#26679;&#26412;&#21644;&#23567;&#26679;&#26412;&#37197;&#32622;&#19979;&#22343;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#35299;&#20915;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30001;&#20110;&#25968;&#25454;&#19981;&#36275;&#32780;&#34920;&#29616;&#19981;&#20339;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#22522;&#20110;&#25554;&#20540;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65306;MSMix(&#27969;&#24418;&#20132;&#25442;Mixup)&#12290;&#35813;&#26041;&#27861;&#23558;&#20004;&#20010;&#19981;&#21516;&#30340;&#26679;&#26412;&#36755;&#20837;&#21516;&#19968;&#20010;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#38543;&#26426;&#36873;&#25321;&#19968;&#20010;&#29305;&#23450;&#30340;&#23618;&#65292;&#37096;&#20998;&#26367;&#25442;&#35813;&#23618;&#19968;&#20010;&#26679;&#26412;&#30340;&#38544;&#34255;&#29305;&#24449;&#20026;&#21478;&#19968;&#20010;&#26679;&#26412;&#30340;&#23545;&#24212;&#29305;&#24449;&#12290;&#28151;&#21512;&#21518;&#30340;&#38544;&#34255;&#29305;&#24449;&#36755;&#20837;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#32593;&#32476;&#30340;&#20854;&#20313;&#37096;&#20998;&#12290;&#36824;&#25552;&#20986;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#36873;&#25321;&#31574;&#30053;&#65292;&#20197;&#33719;&#21462;&#26356;&#20016;&#23500;&#30340;&#38544;&#34255;&#34920;&#31034;&#12290;&#22312;&#19977;&#20010;&#20013;&#25991;&#24847;&#22270;&#35782;&#21035;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;MSMix&#26041;&#27861;&#22312;&#23436;&#25972;&#26679;&#26412;&#21644;&#23567;&#26679;&#26412;&#37197;&#32622;&#19979;&#22343;&#33719;&#24471;&#20102;&#27604;&#20854;&#20182;&#26041;&#27861;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
To solve the problem of poor performance of deep neural network models due to insufficient data, a simple yet effective interpolation-based data augmentation method is proposed: MSMix (Manifold Swap Mixup). This method feeds two different samples to the same deep neural network model, and then randomly select a specific layer and partially replace hidden features at that layer of one of the samples by the counterpart of the other. The mixed hidden features are fed to the model and go through the rest of the network. Two different selection strategies are also proposed to obtain richer hidden representation. Experiments are conducted on three Chinese intention recognition datasets, and the results show that the MSMix method achieves better results than other methods in both full-sample and small-sample configurations.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#21160;&#24577;&#39046;&#22495;&#30693;&#35782;&#30340;&#33647;&#29289;&#25512;&#33616;&#26694;&#26550;DKINet&#65292;&#23558;&#39046;&#22495;&#30693;&#35782;&#19982;&#24739;&#32773;&#20020;&#24202;&#34920;&#29616;&#30456;&#32467;&#21512;&#65292;&#27492;&#20026;&#39318;&#27425;&#23454;&#39564;&#12290;</title><link>http://arxiv.org/abs/2305.19604</link><description>&lt;p&gt;
&#36890;&#36807;&#39046;&#22495;&#30693;&#35782;&#21551;&#31034;&#30340;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#33647;&#29289;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Medication Recommendation via Domain Knowledge Informed Deep Learning. (arXiv:2305.19604v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19604
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#21160;&#24577;&#39046;&#22495;&#30693;&#35782;&#30340;&#33647;&#29289;&#25512;&#33616;&#26694;&#26550;DKINet&#65292;&#23558;&#39046;&#22495;&#30693;&#35782;&#19982;&#24739;&#32773;&#20020;&#24202;&#34920;&#29616;&#30456;&#32467;&#21512;&#65292;&#27492;&#20026;&#39318;&#27425;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33647;&#29289;&#25512;&#33616;&#26159;&#21307;&#30103;&#20445;&#20581;&#30340;&#22522;&#26412;&#20294;&#33267;&#20851;&#37325;&#35201;&#30340;&#20998;&#25903;&#65292;&#25552;&#20379;&#26426;&#20250;&#20026;&#22797;&#26434;&#20581;&#24247;&#29366;&#20917;&#30340;&#24739;&#32773;&#25903;&#25345;&#20020;&#24202;&#21307;&#29983;&#26356;&#31934;&#30830;&#30340;&#33647;&#29289;&#22788;&#26041;&#12290;&#20174;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#20013;&#23398;&#20064;&#25512;&#33616;&#33647;&#29289;&#26159;&#20808;&#21069;&#30740;&#31350;&#20013;&#26368;&#24120;&#35265;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#30740;&#31350;&#24573;&#35270;&#20102;&#26681;&#25454;&#24739;&#32773;&#30340;EHR&#20013;&#30340;&#20020;&#24202;&#34920;&#29616;&#32435;&#20837;&#39046;&#22495;&#30693;&#35782;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#21160;&#24577;&#39046;&#22495;&#30693;&#35782;&#30340;&#33647;&#29289;&#25512;&#33616;&#26694;&#26550;&#65292;&#21363;&#39046;&#22495;&#30693;&#35782;&#21551;&#31034;&#32593;&#32476;&#65288;DKINet&#65289;&#65292;&#29992;&#20110;&#23558;&#39046;&#22495;&#30693;&#35782;&#19982;&#21487;&#35266;&#23519;&#30340;&#24739;&#32773;&#20020;&#24202;&#34920;&#29616;&#30456;&#32467;&#21512;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#39318;&#20808;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20110;&#39046;&#22495;&#30693;&#35782;&#30340;&#32534;&#30721;&#22120;&#26469;&#25429;&#25417;&#39046;&#22495;&#20449;&#24687;&#65292;&#28982;&#21518;&#24320;&#21457;&#20102;&#19968;&#20010;&#25968;&#25454;&#39537;&#21160;&#30340;&#32534;&#30721;&#22120;&#23558;&#39046;&#22495;&#30693;&#35782;&#25972;&#21512;&#21040;&#21487;&#35266;&#23519;&#30340;EHR&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Medication recommendation is a fundamental yet crucial branch of healthcare, which provides opportunities to support clinical physicians with more accurate medication prescriptions for patients with complex health conditions. Learning from electronic health records (EHR) to recommend medications is the most common way in previous studies. However, most of them neglect incorporating domain knowledge according to the clinical manifestations in the EHR of the patient. To address these issues, we propose a novel \textbf{D}omain \textbf{K}nowledge \textbf{I}nformed \textbf{Net}work (DKINet) to integrate domain knowledge with observable clinical manifestations of the patient, which is the first dynamic domain knowledge informed framework toward medication recommendation. In particular, we first design a knowledge-driven encoder to capture the domain information and then develop a data-driven encoder to integrate domain knowledge into the observable EHR. To endow the model with the capability
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;FineRewards&#65292;&#36890;&#36807;&#24341;&#20837;&#32454;&#31890;&#24230;&#30340;&#35821;&#20041;&#22870;&#21169;&#65292;&#21363;&#26631;&#39064;&#22870;&#21169;&#21644;SAM&#22870;&#21169;&#65292;&#26469;&#25913;&#36827;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#20013;&#25991;&#26412;&#21644;&#22270;&#20687;&#20043;&#38388;&#30340;&#23545;&#40784;&#12290;</title><link>http://arxiv.org/abs/2305.19599</link><description>&lt;p&gt;
&#32454;&#31890;&#24230;&#35821;&#20041;&#22870;&#21169;&#22686;&#24378;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Boosting Text-to-Image Diffusion Models with Fine-Grained Semantic Rewards. (arXiv:2305.19599v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19599
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;FineRewards&#65292;&#36890;&#36807;&#24341;&#20837;&#32454;&#31890;&#24230;&#30340;&#35821;&#20041;&#22870;&#21169;&#65292;&#21363;&#26631;&#39064;&#22870;&#21169;&#21644;SAM&#22870;&#21169;&#65292;&#26469;&#25913;&#36827;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#20013;&#25991;&#26412;&#21644;&#22270;&#20687;&#20043;&#38388;&#30340;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#30340;&#30740;&#31350;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#22312;&#32473;&#23450;&#30340;&#25991;&#26412;&#25552;&#31034;&#19979;&#29983;&#25104;&#20102;&#39640;&#36136;&#37327;&#12289;&#36924;&#30495;&#30340;&#22270;&#20687;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32570;&#20047;&#32454;&#31890;&#24230;&#35821;&#20041;&#25351;&#23548;&#65292;&#20197;&#25104;&#21151;&#35786;&#26029;&#24418;&#24577;&#24046;&#24322;&#20026;&#27490;&#65292;&#20197;&#21069;&#30340;&#26041;&#27861;&#26080;&#27861;&#25191;&#34892;&#25991;&#26412;&#27010;&#24565;&#21644;&#29983;&#25104;&#30340;&#22270;&#20687;&#20043;&#38388;&#30340;&#20934;&#30830;&#24418;&#24577;&#23545;&#40784;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FineRewards&#65292;&#36890;&#36807;&#24341;&#20837;&#20004;&#31181;&#26032;&#30340;&#32454;&#31890;&#24230;&#35821;&#20041;&#22870;&#21169;--&#26631;&#39064;&#22870;&#21169;&#21644;&#35821;&#20041;&#20998;&#21106;&#20219;&#20309;&#20107;&#29289;&#65288;SAM&#65289;&#22870;&#21169;&#65292;&#26469;&#25913;&#36827;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#20013;&#25991;&#26412;&#21644;&#22270;&#20687;&#20043;&#38388;&#30340;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in text-to-image diffusion models have achieved remarkable success in generating high-quality, realistic images from given text prompts. However, previous methods fail to perform accurate modality alignment between text concepts and generated images due to the lack of fine-level semantic guidance that successfully diagnoses the modality discrepancy. In this paper, we propose FineRewards to improve the alignment between text and images in text-to-image diffusion models by introducing two new fine-grained semantic rewards: the caption reward and the Semantic Segment Anything (SAM) reward. From the global semantic view, the caption reward generates a corresponding detailed caption that depicts all important contents in the synthetic image via a BLIP-2 model and then calculates the reward score by measuring the similarity between the generated caption and the given prompt. From the local semantic view, the SAM reward segments the generated images into local parts with categ
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#22270;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;UGNN&#65292; &#35299;&#20915;&#20102;&#21322;&#30417;&#30563;&#26222;&#36866;&#22270;&#20998;&#31867;&#38382;&#39064;&#65292;&#36890;&#36807;&#20272;&#35745;&#26410;&#26631;&#35760;&#22270;&#30340;&#30830;&#23450;&#24615;&#35299;&#20915;&#20102;&#31867;&#21035;&#20559;&#31227;&#65292;&#20855;&#26377;&#26368;&#26032;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.19598</link><description>&lt;p&gt;
&#21322;&#30417;&#30563;&#26222;&#36866;&#22270;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Towards Semi-supervised Universal Graph Classification. (arXiv:2305.19598v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19598
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#22270;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;UGNN&#65292; &#35299;&#20915;&#20102;&#21322;&#30417;&#30563;&#26222;&#36866;&#22270;&#20998;&#31867;&#38382;&#39064;&#65292;&#36890;&#36807;&#20272;&#35745;&#26410;&#26631;&#35760;&#22270;&#30340;&#30830;&#23450;&#24615;&#35299;&#20915;&#20102;&#31867;&#21035;&#20559;&#31227;&#65292;&#20855;&#26377;&#26368;&#26032;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#25512;&#21160;&#20102;&#22270;&#20998;&#31867;&#30340;&#26368;&#26032;&#25216;&#26415;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;UGNN&#30340;&#26032;&#22411;&#22270;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;&#20197;&#20174;&#23376;&#22270;&#35282;&#24230;&#21033;&#29992;&#26410;&#26631;&#35760;&#25968;&#25454;&#26469;&#35299;&#20915;&#21322;&#30417;&#30563;&#26222;&#36866;&#22270;&#20998;&#31867;&#38382;&#39064;&#12290;&#35813;&#38382;&#39064;&#30340;&#25361;&#25112;&#22312;&#20110;&#32570;&#20047;&#26631;&#31614;&#25968;&#25454;&#21644;&#21487;&#33021;&#30340;&#31867;&#21035;&#20559;&#31227;&#12290;UGNN&#36890;&#36807;&#20272;&#35745;&#26410;&#26631;&#35760;&#22270;&#30340;&#30830;&#23450;&#24615;&#26469;&#35299;&#20915;&#31867;&#21035;&#20559;&#31227;&#65292;&#20351;&#20854;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25968;&#25454;&#38598;&#19978;&#20855;&#26377;&#26368;&#26032;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks have pushed state-of-the-arts in graph classifications recently. Typically, these methods are studied within the context of supervised end-to-end training, which necessities copious task-specific labels. However, in real-world circumstances, labeled data could be limited, and there could be a massive corpus of unlabeled data, even from unknown classes as a complementary. Towards this end, we study the problem of semi-supervised universal graph classification, which not only identifies graph samples which do not belong to known classes, but also classifies the remaining samples into their respective classes. This problem is challenging due to a severe lack of labels and potential class shifts. In this paper, we propose a novel graph neural network framework named UGNN, which makes the best of unlabeled data from the subgraph perspective. To tackle class shifts, we estimate the certainty of unlabeled graphs using multiple subgraphs, which facilities the discovery of
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#21457;&#29616;&#65292;&#35821;&#35328;&#27169;&#22411;&#22312;&#38646;/&#23569;&#26679;&#26412;&#29615;&#22659;&#19979;&#38590;&#20197;&#29702;&#35299;&#8220;respectively&#8221;&#30340;&#21508;&#31181;&#35835;&#27861;&#65292;&#38656;&#35201;&#26356;&#38271;&#26102;&#38388;&#30340;&#35757;&#32451;&#21644;&#20381;&#36182;&#24120;&#35782;&#25512;&#29702;&#65292;&#20173;&#33853;&#21518;&#20110;&#20154;&#31867;&#12290;</title><link>http://arxiv.org/abs/2305.19597</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#22312;&#38646;/&#23569;&#26679;&#26412;&#29615;&#22659;&#19979;&#26080;&#27861;&#27491;&#30830;&#29702;&#35299;&#8220;respectively&#8221;&#30340;&#21407;&#22240;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
What does the Failure to Reason with "Respectively" in Zero/Few-Shot Settings Tell Us about Language Models?. (arXiv:2305.19597v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19597
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#21457;&#29616;&#65292;&#35821;&#35328;&#27169;&#22411;&#22312;&#38646;/&#23569;&#26679;&#26412;&#29615;&#22659;&#19979;&#38590;&#20197;&#29702;&#35299;&#8220;respectively&#8221;&#30340;&#21508;&#31181;&#35835;&#27861;&#65292;&#38656;&#35201;&#26356;&#38271;&#26102;&#38388;&#30340;&#35757;&#32451;&#21644;&#20381;&#36182;&#24120;&#35782;&#25512;&#29702;&#65292;&#20173;&#33853;&#21518;&#20110;&#20154;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#21487;&#20197;&#36731;&#26494;&#29702;&#35299;&#8220;Niels Bohr and Kurt Cobain were born in Copenhagen and Seattle, respectively&#8221;&#36825;&#31181;&#21477;&#23376;&#30340;&#21327;&#20316;&#32467;&#26500;&#12290;&#22312;&#33258;&#28982;&#35821;&#35328;&#25512;&#26029;&#65288;NLI&#65289;&#30340;&#32972;&#26223;&#19979;&#65292;&#26412;&#25991;&#20174;&#21477;&#27861;-&#35821;&#20041;&#21644;&#24120;&#35782;-&#19990;&#30028;&#30693;&#35782;&#30340;&#20004;&#20010;&#35282;&#24230;&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#22914;&#20309;&#29702;&#35299;&#20004;&#20010;&#35835;&#25968;&#65288;Gawron&#21644;Kehler&#65292;2004&#65289; &#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21463;&#25511;&#21512;&#25104;&#25968;&#25454;&#38598;WikiResNLI &#21644;&#19968;&#20010;&#33258;&#28982;&#25968;&#25454;&#38598; NatResNLI&#65292;&#20197;&#21253;&#21547;&#21508;&#31181;&#26174;&#24335;&#21644;&#38544;&#24335;&#23454;&#29616;&#30340;&#8220;respectively&#8221;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#24494;&#35843;&#21518;&#30340;NLI&#27169;&#22411;&#22312;&#27809;&#26377;&#26174;&#24335;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#38590;&#20197;&#29702;&#35299;&#36825;&#26679;&#30340;&#35835;&#25968;&#12290;&#24403;&#23384;&#22312;&#26174;&#24335;&#25552;&#31034;&#26102;&#65292;&#38646;/&#23569;&#26679;&#26412;&#23398;&#20064;&#24456;&#23481;&#26131;&#65292;&#32780;&#24403;&#35813;&#35835;&#25968;&#38544;&#21547;&#26102;&#65292;&#21017;&#38656;&#35201;&#26356;&#38271;&#30340;&#35757;&#32451;&#26102;&#38388;&#65292;&#20197;&#20381;&#36182;&#24120;&#35782;&#25512;&#29702;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#32454;&#33268;&#20998;&#26512;&#34920;&#26126;&#65292;&#27169;&#22411;&#26080;&#27861;&#22312;&#19981;&#21516;&#32467;&#26500;&#20043;&#38388;&#36827;&#34892;&#27867;&#21270;&#12290;&#24635;&#20043;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#38646;/&#23569;&#26679;&#26412;&#29615;&#22659;&#19979;&#65292;&#35821;&#35328;&#27169;&#22411;&#22312;&#29702;&#35299;&#8220;respectively&#8221;&#30340;&#21508;&#31181;&#35835;&#27861;&#26041;&#38754;&#20173;&#33853;&#21518;&#20110;&#20154;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
Humans can effortlessly understand the coordinate structure of sentences such as "Niels Bohr and Kurt Cobain were born in Copenhagen and Seattle, respectively". In the context of natural language inference (NLI), we examine how language models (LMs) reason with respective readings (Gawron and Kehler, 2004) from two perspectives: syntactic-semantic and commonsense-world knowledge. We propose a controlled synthetic dataset WikiResNLI and a naturally occurring dataset NatResNLI to encompass various explicit and implicit realizations of "respectively". We show that fine-tuned NLI models struggle with understanding such readings without explicit supervision. While few-shot learning is easy in the presence of explicit cues, longer training is required when the reading is evoked implicitly, leaving models to rely on common sense inferences. Furthermore, our fine-grained analysis indicates models fail to generalize across different constructions. To conclude, we demonstrate that LMs still lag 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23450;&#21046;&#30340;&#22810;&#25805;&#20316;&#25968;&#20809;&#23398;&#22120;&#20214;&#30340;&#21487;&#25193;&#23637;&#21644;&#39640;&#25928;&#20809;&#28857;&#31215;&#24341;&#25806;&#65292;&#21363;&#22810;&#25805;&#20316;&#25968;&#20809;&#23398;&#31070;&#32463;&#20803;&#65288;MOON&#65289;&#65292;&#24182;&#22312;&#22270;&#20687;&#35782;&#21035;&#20219;&#21153;&#20013;&#35777;&#26126;&#20102;&#20854;&#23454;&#29992;&#24615;&#12290;&#22312;SVHN&#35782;&#21035;&#25968;&#25454;&#38598;&#19978;&#65292;&#35813;MOON&#23454;&#29616;&#20102;85.89&#65285;&#30340;&#27979;&#37327;&#31934;&#24230;&#65292;&#24615;&#33021;&#36874;&#20110;128x128&#30340;&#22522;&#20110;MOMZI&#30340;PTCs&#12290;</title><link>http://arxiv.org/abs/2305.19592</link><description>&lt;p&gt;
&#21487;&#25193;&#23637;&#19988;&#39640;&#25928;&#30340;&#28145;&#24230;&#23398;&#20064;&#20809;&#23398;&#31070;&#32463;&#20803; &#22810;&#25805;&#20316;&#25968;&#20809;&#23398;&#31070;&#32463;&#20803;
&lt;/p&gt;
&lt;p&gt;
Integrated multi-operand optical neurons for scalable and hardware-efficient deep learning. (arXiv:2305.19592v1 [physics.optics])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19592
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23450;&#21046;&#30340;&#22810;&#25805;&#20316;&#25968;&#20809;&#23398;&#22120;&#20214;&#30340;&#21487;&#25193;&#23637;&#21644;&#39640;&#25928;&#20809;&#28857;&#31215;&#24341;&#25806;&#65292;&#21363;&#22810;&#25805;&#20316;&#25968;&#20809;&#23398;&#31070;&#32463;&#20803;&#65288;MOON&#65289;&#65292;&#24182;&#22312;&#22270;&#20687;&#35782;&#21035;&#20219;&#21153;&#20013;&#35777;&#26126;&#20102;&#20854;&#23454;&#29992;&#24615;&#12290;&#22312;SVHN&#35782;&#21035;&#25968;&#25454;&#38598;&#19978;&#65292;&#35813;MOON&#23454;&#29616;&#20102;85.89&#65285;&#30340;&#27979;&#37327;&#31934;&#24230;&#65292;&#24615;&#33021;&#36874;&#20110;128x128&#30340;&#22522;&#20110;MOMZI&#30340;PTCs&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20809;&#31070;&#32463;&#32593;&#32476;&#65288;ONN&#65289;&#26159;&#19979;&#19968;&#20195;&#31070;&#32463;&#24418;&#24577;&#35745;&#31639;&#30340;&#26377;&#21069;&#36884;&#30828;&#20214;&#24179;&#21488;&#65292;&#22522;&#20110;&#21333;&#20540;&#25805;&#20316;&#30340;&#20809;&#23398;&#38750;&#24120;&#25968;&#26680;&#65288;PTC&#65289;&#20250;&#21344;&#29992;&#22823;&#37327;&#21333;&#25805;&#20316;&#25968;&#20809;&#35843;&#21046;&#22120;&#29992;&#20110;&#20449;&#21495;&#21644;&#26435;&#37325;&#32534;&#30721;&#65292;&#23548;&#33268;&#36739;&#22823;&#30340;&#21306;&#22495;&#25104;&#26412;&#21644;&#39640;&#20256;&#25773;&#25439;&#32791;&#20197;&#23454;&#29616;&#22823;&#22411;&#24352;&#37327;&#25805;&#20316;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;&#23450;&#21046;&#30340;&#22810;&#25805;&#20316;&#25968;&#20809;&#23398;&#22120;&#20214;&#30340;&#21487;&#25193;&#23637;&#21644;&#39640;&#25928;&#20809;&#28857;&#31215;&#24341;&#25806;&#65292;&#21363;&#22810;&#25805;&#20316;&#25968;&#20809;&#23398;&#31070;&#32463;&#20803;&#65288;MOON&#65289;&#12290;&#25105;&#20204;&#22312;&#22270;&#20687;&#35782;&#21035;&#20219;&#21153;&#20013;&#20351;&#29992;&#22810;&#25805;&#20316;&#25968;&#39532;&#36203;&#8211;&#27901;&#24503;&#24178;&#24178;&#28041;&#20202;&#65288;MOMZI&#65289;&#23454;&#39564;&#28436;&#31034;&#20102;MOON&#30340;&#25928;&#29992;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#22522;&#20110;MOMZI&#30340;ONN&#22312;4&#20301;&#30005;&#21387;&#25511;&#21046;&#31934;&#24230;&#19979;&#22312;&#34903;&#26223;&#25151;&#23627;&#25968;&#23383;&#65288;SVHN&#65289;&#35782;&#21035;&#25968;&#25454;&#38598;&#20013;&#23454;&#29616;&#20102;85.89%&#30340;&#27979;&#37327;&#31934;&#24230;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#24615;&#33021;&#20998;&#26512;&#26174;&#31034;&#65292;128x128&#30340;&#22522;&#20110;MOMZI&#30340;PTCs&#20248;&#20110;&#22522;&#20110;&#21333;&#20540;&#20809;&#35843;&#21046;&#22120;&#30340;&#23545;&#24212;&#29289;&#12290;
&lt;/p&gt;
&lt;p&gt;
The optical neural network (ONN) is a promising hardware platform for next-generation neuromorphic computing due to its high parallelism, low latency, and low energy consumption. However, previous integrated photonic tensor cores (PTCs) consume numerous single-operand optical modulators for signal and weight encoding, leading to large area costs and high propagation loss to implement large tensor operations. This work proposes a scalable and efficient optical dot-product engine based on customized multi-operand photonic devices, namely multi-operand optical neurons (MOON). We experimentally demonstrate the utility of a MOON using a multi-operand-Mach-Zehnder-interferometer (MOMZI) in image recognition tasks. Specifically, our MOMZI-based ONN achieves a measured accuracy of 85.89% in the street view house number (SVHN) recognition dataset with 4-bit voltage control precision. Furthermore, our performance analysis reveals that a 128x128 MOMZI-based PTCs outperform their counterparts base
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#32508;&#36848;&#20102;&#20132;&#36890;&#39044;&#27979;&#26041;&#27861;&#30340;&#21457;&#23637;&#65292;&#37325;&#28857;&#20171;&#32461;&#20102;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#20132;&#36890;&#39044;&#27979;&#26041;&#27861;&#22312;&#22810;&#20803;&#20132;&#36890;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#30740;&#31350;&#26041;&#38754;&#30340;&#36827;&#23637;&#21644;&#26426;&#36935;&#12290;</title><link>http://arxiv.org/abs/2305.19591</link><description>&lt;p&gt;
&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#20132;&#36890;&#39044;&#27979;&#65306;&#36817;&#26399;&#36827;&#23637;&#19982;&#26032;&#26426;&#36935;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Traffic Prediction using Artificial Intelligence: Review of Recent Advances and Emerging Opportunities. (arXiv:2305.19591v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19591
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#32508;&#36848;&#20102;&#20132;&#36890;&#39044;&#27979;&#26041;&#27861;&#30340;&#21457;&#23637;&#65292;&#37325;&#28857;&#20171;&#32461;&#20102;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#20132;&#36890;&#39044;&#27979;&#26041;&#27861;&#22312;&#22810;&#20803;&#20132;&#36890;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#30740;&#31350;&#26041;&#38754;&#30340;&#36827;&#23637;&#21644;&#26426;&#36935;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#36890;&#39044;&#27979;&#22312;&#32531;&#35299;&#20840;&#29699;&#24615;&#30340;&#20132;&#36890;&#25317;&#22581;&#38382;&#39064;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#20854;&#36127;&#38754;&#24433;&#21709;&#21253;&#25324;&#39069;&#22806;&#26053;&#34892;&#26102;&#38388;&#30340;&#25439;&#22833;&#21644;&#29123;&#26009;&#28040;&#32791;&#30340;&#22686;&#21152;&#12290;&#23558;&#26032;&#20852;&#25216;&#26415;&#34701;&#20837;&#20132;&#36890;&#31995;&#32479;&#21487;&#20197;&#26174;&#33879;&#25913;&#21892;&#20132;&#36890;&#39044;&#27979;&#65292;&#24182;&#24102;&#26469;&#26032;&#30340;&#30740;&#31350;&#38382;&#39064;&#12290;&#20026;&#20102;&#20102;&#35299;&#20132;&#36890;&#39044;&#27979;&#20013;&#30340;&#24320;&#25918;&#24615;&#30740;&#31350;&#25361;&#25112;&#65292;&#26412;&#32508;&#36848;&#26088;&#22312;&#25552;&#20379;&#20132;&#36890;&#39044;&#27979;&#26041;&#27861;&#30340;&#32508;&#21512;&#27010;&#36848;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20391;&#37325;&#20110;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#30340;&#20132;&#36890;&#39044;&#27979;&#26041;&#27861;&#22312;&#22810;&#21464;&#37327;&#20132;&#36890;&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;&#26041;&#38754;&#30340;&#36817;&#26399;&#36827;&#23637;&#21644;&#26032;&#30340;&#30740;&#31350;&#26426;&#36935;&#65292;&#36825;&#26159;&#30001;&#20110;&#36817;&#24180;&#26469;&#36825;&#31867;&#26041;&#27861;&#22312;&#20132;&#36890;&#39044;&#27979;&#20013;&#20855;&#26377;&#28508;&#22312;&#30340;&#25104;&#21151;&#21644;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traffic prediction plays a crucial role in alleviating traffic congestion which represents a critical problem globally, resulting in negative consequences such as lost hours of additional travel time and increased fuel consumption. Integrating emerging technologies into transportation systems provides opportunities for improving traffic prediction significantly and brings about new research problems. In order to lay the foundation for understanding the open research challenges in traffic prediction, this survey aims to provide a comprehensive overview of traffic prediction methodologies. Specifically, we focus on the recent advances and emerging research opportunities in Artificial Intelligence (AI)-based traffic prediction methods, due to their recent success and potential in traffic prediction, with an emphasis on multivariate traffic time series modeling. We first provide a list and explanation of the various data types and resources used in the literature. Next, the essential data 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#24102;&#24314;&#35758;&#30340;&#20027;&#21160;&#22240;&#26524;&#32467;&#26500;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#33258;&#36866;&#24212;&#25628;&#32034;&#31639;&#27861;&#65292;&#21487;&#20197;&#20174;&#24314;&#35758;&#20013;&#21463;&#30410;&#65292;&#21363;&#20351;&#24314;&#35758;&#26159;&#20219;&#24847;&#31967;&#31957;&#30340;&#24773;&#20917;&#19979;&#65292;&#20173;&#28982;&#20855;&#26377;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2305.19588</link><description>&lt;p&gt;
&#24102;&#24314;&#35758;&#30340;&#20027;&#21160;&#22240;&#26524;&#32467;&#26500;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Active causal structure learning with advice. (arXiv:2305.19588v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19588
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#24102;&#24314;&#35758;&#30340;&#20027;&#21160;&#22240;&#26524;&#32467;&#26500;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#33258;&#36866;&#24212;&#25628;&#32034;&#31639;&#27861;&#65292;&#21487;&#20197;&#20174;&#24314;&#35758;&#20013;&#21463;&#30410;&#65292;&#21363;&#20351;&#24314;&#35758;&#26159;&#20219;&#24847;&#31967;&#31957;&#30340;&#24773;&#20917;&#19979;&#65292;&#20173;&#28982;&#20855;&#26377;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#24102;&#24314;&#35758;&#30340;&#20027;&#21160;&#22240;&#26524;&#32467;&#26500;&#23398;&#20064;&#38382;&#39064;&#12290;&#22312;&#20856;&#22411;&#30340;&#30740;&#31350;&#20013;&#65292;&#23398;&#20064;&#31639;&#27861;&#38024;&#23545;&#35266;&#27979;&#20998;&#24067;&#33719;&#24471;&#26412;&#36136;&#22270;&#65292;&#24182;&#34987;&#35201;&#27714;&#22312;&#26368;&#23567;&#21270;&#24178;&#39044;&#27425;&#25968;&#30340;&#21516;&#26102;&#24674;&#22797;&#20986;&#28508;&#22312;&#30340;&#22240;&#26524;&#26377;&#21521;&#26080;&#29615;&#22270;(DAG) $G^*$&#12290;&#22312;&#25105;&#20204;&#30340;&#38382;&#39064;&#35774;&#23450;&#20013;&#65292;&#38500;&#20102;&#20851;&#20110; $G^*$&#30340;&#24517;&#35201;&#20449;&#24687;&#22806;&#65292;&#20363;&#22914;&#19968;&#20010;&#22768;&#31216;&#26159; $G^*$&#30340;DAG $G$&#65292;&#25105;&#20204;&#36824;&#20250;&#39069;&#22806;&#33719;&#24471;&#20851;&#20110; $G^*$&#30340;&#20391;&#38754;&#20449;&#24687;&#12290;&#25105;&#20204;&#24819;&#30693;&#36947;&#65292;&#24403;&#24314;&#35758;&#25509;&#36817;&#27491;&#30830;&#26102;&#65292;&#23398;&#20064;&#31639;&#27861;&#26159;&#21542;&#21487;&#20197;&#20174;&#24314;&#35758;&#20013;&#21463;&#30410;&#65292;&#21516;&#26102;&#21363;&#20351;&#24314;&#35758;&#26159;&#20219;&#24847;&#31967;&#31957;&#30340;&#24773;&#20917;&#19979;&#65292;&#20173;&#28982;&#20855;&#26377;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#20445;&#35777;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#19982;&#20851;&#20110;&#24102;&#39044;&#27979;&#31639;&#27861;&#30340;&#19981;&#26029;&#22686;&#21152;&#30340;&#30740;&#31350;&#39046;&#22495;&#30456;&#21516;&#12290;&#24403;&#24314;&#35758;&#26159;&#26377;&#21521;&#26080;&#29615;&#22270;$G$&#26102;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#33258;&#36866;&#24212;&#25628;&#32034;&#31639;&#27861;&#26469;&#24674;&#22797; $G^*$&#65292;&#20854;&#24178;&#39044;&#25104;&#26412;&#26368;&#22810;&#20026;&#39564;&#35777;$G^*$&#30340;&#25104;&#26412;&#30340;$O(max\{1, \log \psi\})$&#20493;&#12290;&#36825;&#37324;&#65292;$\psi$&#26159;$G$&#21644;$G^*$&#20043;&#38388;&#30340;&#36317;&#31163;&#24230;&#37327;&#65292;&#23427;&#34987;&#19978;&#30028;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce the problem of active causal structure learning with advice. In the typical well-studied setting, the learning algorithm is given the essential graph for the observational distribution and is asked to recover the underlying causal directed acyclic graph (DAG) $G^*$ while minimizing the number of interventions made. In our setting, we are additionally given side information about $G^*$ as advice, e.g. a DAG $G$ purported to be $G^*$. We ask whether the learning algorithm can benefit from the advice when it is close to being correct, while still having worst-case guarantees even when the advice is arbitrarily bad. Our work is in the same space as the growing body of research on algorithms with predictions. When the advice is a DAG $G$, we design an adaptive search algorithm to recover $G^*$ whose intervention cost is at most $O(\max\{1, \log \psi\})$ times the cost for verifying $G^*$; here, $\psi$ is a distance measure between $G$ and $G^*$ that is upper bounded by the numb
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;meta-learning&#26694;&#26550;&#65292;&#20351;&#24471;&#27169;&#22411;&#21487;&#20197;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#24555;&#36895;&#36866;&#24212;&#26032;&#20219;&#21153;&#12290;&#36890;&#36807;&#22312;&#22810;&#20010;&#21512;&#25104;&#21644;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#35299;&#20915;&#22823;&#23567;&#21644;&#20998;&#24067;&#21464;&#21270;&#30340;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#65288;VRP&#65289;&#12290;</title><link>http://arxiv.org/abs/2305.19587</link><description>&lt;p&gt;
&#38754;&#21521;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#30340;&#20840;&#36890;&#29992;&#31070;&#32463;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Towards Omni-generalizable Neural Methods for Vehicle Routing Problems. (arXiv:2305.19587v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19587
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;meta-learning&#26694;&#26550;&#65292;&#20351;&#24471;&#27169;&#22411;&#21487;&#20197;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#24555;&#36895;&#36866;&#24212;&#26032;&#20219;&#21153;&#12290;&#36890;&#36807;&#22312;&#22810;&#20010;&#21512;&#25104;&#21644;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#35299;&#20915;&#22823;&#23567;&#21644;&#20998;&#24067;&#21464;&#21270;&#30340;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#65288;VRP&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#36991;&#20813;&#20102;&#23545;&#25163;&#24037;&#35268;&#21017;&#30340;&#20381;&#36182;&#65292;&#23398;&#20064;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#65288;VRP&#65289;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#22312;&#22266;&#23450;&#22823;&#23567;&#21644;&#33410;&#28857;&#20998;&#24067;&#30340;&#21516;&#19968;&#20219;&#21153;&#19978;&#36827;&#34892;&#35757;&#32451;&#21644;&#27979;&#35797;&#65292;&#22240;&#27492;&#20855;&#26377;&#26377;&#38480;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#20294;&#21448;&#29616;&#23454;&#30340;&#22330;&#26223;&#65292;&#35813;&#22330;&#26223;&#32771;&#34385;&#20102;VRP&#22312;&#22823;&#23567;&#21644;&#20998;&#24067;&#26041;&#38754;&#30340;&#19968;&#33324;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#20803;&#23398;&#20064;&#26694;&#26550;&#65292;&#22312;&#25512;&#29702;&#26399;&#38388;&#33021;&#22815;&#24555;&#36895;&#36866;&#24212;&#26032;&#20219;&#21153;&#30340;&#33021;&#21147;&#19979;&#23545;&#21021;&#22987;&#21270;&#27169;&#22411;&#36827;&#34892;&#26377;&#25928;&#35757;&#32451;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#24320;&#21457;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#36817;&#20284;&#26041;&#27861;&#26469;&#20943;&#23569;&#35757;&#32451;&#24320;&#38144;&#12290;&#23545;&#26053;&#34892;&#21830;&#38382;&#39064;&#65288;TSP&#65289;&#21644;&#23481;&#37327;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#65288;CVRP&#65289;&#30340;&#21512;&#25104;&#21644;&#22522;&#20934;&#23454;&#20363;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#20195;&#30721;&#21487;&#22312;https://github.com/RoyalSkye/Omni-VRP&#24471;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning heuristics for vehicle routing problems (VRPs) has gained much attention due to the less reliance on hand-crafted rules. However, existing methods are typically trained and tested on the same task with a fixed size and distribution (of nodes), and hence suffer from limited generalization performance. This paper studies a challenging yet realistic setting, which considers generalization across both size and distribution in VRPs. We propose a generic meta-learning framework, which enables effective training of an initialized model with the capability of fast adaptation to new tasks during inference. We further develop a simple yet efficient approximation method to reduce the training overhead. Extensive experiments on both synthetic and benchmark instances of the traveling salesman problem (TSP) and capacitated vehicle routing problem (CVRP) demonstrate the effectiveness of our method. The code is available at: https://github.com/RoyalSkye/Omni-VRP.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#39640;&#38454;&#32047;&#31215;&#37327;&#23454;&#29616;&#28508;&#22312;&#28151;&#28102;&#22240;&#32032;&#22240;&#26524;&#20851;&#31995;&#21457;&#29616;&#30340;&#26032;&#26041;&#27861;&#65292;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#20351;&#21333;&#28508;&#22312;&#25104;&#20998;&#32467;&#26500;&#30340;OICA&#38382;&#39064;&#24471;&#21040;&#38381;&#21512;&#24418;&#24335;&#35299;&#20915;&#26041;&#26696;&#65292;&#21516;&#26102;&#36824;&#25552;&#20986;&#20102;&#21487;&#27979;&#35797;&#30340;&#21333;&#28508;&#22312;&#25104;&#20998;&#26465;&#20214;&#65292;&#36890;&#36807;&#36845;&#20195;&#21024;&#38500;&#20849;&#20139;&#35782;&#21035;&#30340;&#28508;&#22312;&#25104;&#20998;&#25104;&#21151;&#25193;&#23637;&#20102;&#32467;&#26524;&#21040;&#22810;&#28508;&#22312;&#25104;&#20998;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2305.19582</link><description>&lt;p&gt;
&#22522;&#20110;&#39640;&#38454;&#32047;&#31215;&#37327;&#30340;&#28508;&#22312;&#28151;&#28102;&#22240;&#32032;&#22240;&#26524;&#20851;&#31995;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Causal Discovery with Latent Confounders Based on Higher-Order Cumulants. (arXiv:2305.19582v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19582
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#39640;&#38454;&#32047;&#31215;&#37327;&#23454;&#29616;&#28508;&#22312;&#28151;&#28102;&#22240;&#32032;&#22240;&#26524;&#20851;&#31995;&#21457;&#29616;&#30340;&#26032;&#26041;&#27861;&#65292;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#20351;&#21333;&#28508;&#22312;&#25104;&#20998;&#32467;&#26500;&#30340;OICA&#38382;&#39064;&#24471;&#21040;&#38381;&#21512;&#24418;&#24335;&#35299;&#20915;&#26041;&#26696;&#65292;&#21516;&#26102;&#36824;&#25552;&#20986;&#20102;&#21487;&#27979;&#35797;&#30340;&#21333;&#28508;&#22312;&#25104;&#20998;&#26465;&#20214;&#65292;&#36890;&#36807;&#36845;&#20195;&#21024;&#38500;&#20849;&#20139;&#35782;&#21035;&#30340;&#28508;&#22312;&#25104;&#20998;&#25104;&#21151;&#25193;&#23637;&#20102;&#32467;&#26524;&#21040;&#22810;&#28508;&#22312;&#25104;&#20998;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#31185;&#23398;&#39046;&#22495;&#20013;&#65292;&#20855;&#26377;&#28508;&#22312;&#28151;&#28102;&#22240;&#32032;&#30340;&#22240;&#26524;&#20851;&#31995;&#21457;&#29616;&#26159;&#19968;&#39033;&#37325;&#35201;&#20294;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#23613;&#31649;&#26576;&#20123;&#36807;&#23436;&#22791;&#29420;&#31435;&#20998;&#37327;&#20998;&#26512;&#65288;OICA&#65289;&#26041;&#27861;&#22312;&#26576;&#20123;&#39046;&#22495;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#35745;&#31639;&#25104;&#26412;&#39640;&#65292;&#23481;&#26131;&#38519;&#20837;&#23616;&#37096;&#26368;&#20248;&#35299;&#12290;&#25105;&#20204;&#21457;&#29616;&#26377;&#36259;&#30340;&#26159;&#65292;&#36890;&#36807;&#21033;&#29992;&#39640;&#38454;&#32047;&#31215;&#37327;&#65292;&#22312;&#29305;&#23450;&#24773;&#20917;&#19979;&#65288;&#20363;&#22914;&#65292;&#28151;&#21512;&#36807;&#31243;&#36981;&#24490;&#21333;&#28508;&#22312;&#20998;&#37327;&#32467;&#26500;&#65289;&#65292;&#23384;&#22312;OICA&#30340;&#38381;&#21512;&#24418;&#24335;&#35299;&#20915;&#26041;&#26696;&#12290;&#37492;&#20110;OICA&#38381;&#21512;&#24418;&#24335;&#35299;&#20915;&#26041;&#26696;&#19982;&#21333;&#28508;&#22312;&#25104;&#20998;&#32467;&#26500;&#30456;&#23545;&#24212;&#30340;&#24378;&#22823;&#21151;&#33021;&#65292;&#25105;&#20204;&#21046;&#23450;&#20102;&#19968;&#31181;&#20351;&#29992;&#39640;&#38454;&#32047;&#31215;&#37327;&#20272;&#35745;&#28151;&#21512;&#30697;&#38453;&#30340;&#26041;&#27861;&#65292;&#24182;&#36827;&#19968;&#27493;&#25552;&#20986;&#21487;&#27979;&#35797;&#30340;&#21333;&#28508;&#22312;&#25104;&#20998;&#26465;&#20214;&#65292;&#20197;&#35782;&#21035;&#28508;&#22312;&#21464;&#37327;&#24182;&#30830;&#23450;&#22240;&#26524;&#39034;&#24207;&#12290;&#36890;&#36807;&#36845;&#20195;&#21024;&#38500;&#20849;&#20139;&#35782;&#21035;&#30340;&#28508;&#22312;&#25104;&#20998;&#65292;&#25105;&#20204;&#25104;&#21151;&#22320;&#23558;&#21333;&#28508;&#22312;&#25104;&#20998;&#32467;&#26500;&#30340;&#32467;&#26524;&#25193;&#23637;&#21040;&#22810;&#28508;&#22312;&#25104;&#20998;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Causal discovery with latent confounders is an important but challenging task in many scientific areas. Despite the success of some overcomplete independent component analysis (OICA) based methods in certain domains, they are computationally expensive and can easily get stuck into local optima. We notice that interestingly, by making use of higher-order cumulants, there exists a closed-form solution to OICA in specific cases, e.g., when the mixing procedure follows the One-Latent-Component structure. In light of the power of the closed-form solution to OICA corresponding to the One-Latent-Component structure, we formulate a way to estimate the mixing matrix using the higher-order cumulants, and further propose the testable One-Latent-Component condition to identify the latent variables and determine causal orders. By iteratively removing the share identified latent components, we successfully extend the results on the One-Latent-Component structure to the Multi-Latent-Component structu
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35828;&#35805;&#20154;&#39564;&#35777;&#30340;&#35821;&#38899;&#27963;&#21160;&#26816;&#27979;&#65288;SVVAD&#65289;&#26694;&#26550;&#65292;&#21487;&#26681;&#25454;&#26368;&#20855;&#20449;&#24687;&#37327;&#30340;&#35821;&#38899;&#29305;&#24449;&#36827;&#34892;&#35843;&#25972;&#65292;&#21033;&#29992;&#26080;&#26631;&#31614;&#35757;&#32451;&#26041;&#27861;&#65292;&#23436;&#20840;&#36991;&#20813;&#20102;&#30001;&#20110;&#38169;&#35823;&#26631;&#35760;&#23548;&#33268;&#30340;SV&#24615;&#33021;&#19979;&#38477;&#12290;&#23454;&#39564;&#35777;&#26126;&#22312;&#20854;&#20182;&#35828;&#35805;&#32773;&#28151;&#21512;&#30340;&#26465;&#20214;&#19979;&#65292;SVVAD&#22312;&#24179;&#22343;&#31561;&#35823;&#24046;&#29575;&#65288;EER&#65289;&#26041;&#38754;&#26174;&#33879;&#20248;&#20110;&#22522;&#32447;&#12290;</title><link>http://arxiv.org/abs/2305.19581</link><description>&lt;p&gt;
SVVAD&#65306;&#29992;&#20110;&#35828;&#35805;&#20154;&#39564;&#35777;&#30340;&#20010;&#20154;&#35821;&#38899;&#27963;&#21160;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
SVVAD: Personal Voice Activity Detection for Speaker Verification. (arXiv:2305.19581v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19581
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35828;&#35805;&#20154;&#39564;&#35777;&#30340;&#35821;&#38899;&#27963;&#21160;&#26816;&#27979;&#65288;SVVAD&#65289;&#26694;&#26550;&#65292;&#21487;&#26681;&#25454;&#26368;&#20855;&#20449;&#24687;&#37327;&#30340;&#35821;&#38899;&#29305;&#24449;&#36827;&#34892;&#35843;&#25972;&#65292;&#21033;&#29992;&#26080;&#26631;&#31614;&#35757;&#32451;&#26041;&#27861;&#65292;&#23436;&#20840;&#36991;&#20813;&#20102;&#30001;&#20110;&#38169;&#35823;&#26631;&#35760;&#23548;&#33268;&#30340;SV&#24615;&#33021;&#19979;&#38477;&#12290;&#23454;&#39564;&#35777;&#26126;&#22312;&#20854;&#20182;&#35828;&#35805;&#32773;&#28151;&#21512;&#30340;&#26465;&#20214;&#19979;&#65292;SVVAD&#22312;&#24179;&#22343;&#31561;&#35823;&#24046;&#29575;&#65288;EER&#65289;&#26041;&#38754;&#26174;&#33879;&#20248;&#20110;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#38899;&#27963;&#21160;&#26816;&#27979;&#65288;VAD&#65289;&#36890;&#36807;&#20445;&#30041;&#35821;&#38899;&#29255;&#27573;&#21644;&#20943;&#24369;&#38750;&#35821;&#38899;&#30340;&#24433;&#21709;&#26469;&#25552;&#39640;&#35828;&#35805;&#20154;&#39564;&#35777;&#65288;SV&#65289;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20010;&#26041;&#27861;&#24182;&#19981;&#29702;&#24819;&#65306;&#23427;&#22312;&#22024;&#26434;&#30340;&#29615;&#22659;&#25110;&#22810;&#35828;&#35805;&#32773;&#23545;&#35805;&#20013;&#22833;&#36133;&#65307;&#23427;&#26159;&#22522;&#20110;&#19981;&#20934;&#30830;&#30340;&#38750;SV&#25935;&#24863;&#26631;&#31614;&#36827;&#34892;&#35757;&#32451;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35828;&#35805;&#20154;&#39564;&#35777;&#30340;&#35821;&#38899;&#27963;&#21160;&#26816;&#27979;&#65288;SVVAD&#65289;&#26694;&#26550;&#65292;&#21487;&#20197;&#26681;&#25454;SV&#26368;&#20855;&#20449;&#24687;&#37327;&#30340;&#35821;&#38899;&#29305;&#24449;&#36827;&#34892;&#35843;&#25972;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26080;&#26631;&#31614;&#35757;&#32451;&#26041;&#27861;&#65292;&#21033;&#29992;&#31867;&#19977;&#20803;&#32452;&#25439;&#22833;&#20989;&#25968;&#65292;&#23436;&#20840;&#36991;&#20813;&#20102;&#30001;&#20110;&#38169;&#35823;&#26631;&#35760;&#32780;&#23548;&#33268;&#30340;SV&#24615;&#33021;&#19979;&#38477;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;SVVAD&#22312;&#20854;&#20182;&#35828;&#35805;&#32773;&#28151;&#21512;&#30340;&#26465;&#20214;&#19979;&#65292;&#24179;&#22343;&#31561;&#35823;&#24046;&#29575;&#65288;EER&#65289;&#26041;&#38754;&#26174;&#33879;&#20248;&#20110;&#22522;&#32447;&#12290;&#27492;&#22806;&#65292;&#20915;&#31574;&#36793;&#30028;&#25581;&#31034;&#20102;&#35821;&#38899;&#19981;&#21516;&#37096;&#20998;&#30340;&#37325;&#35201;&#24615;&#65292;&#36825;&#19982;&#20154;&#31867;&#21028;&#26029;&#22522;&#26412;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
Voice activity detection (VAD) improves the performance of speaker verification (SV) by preserving speech segments and attenuating the effects of non-speech. However, this scheme is not ideal: (1) it fails in noisy environments or multi-speaker conversations; (2) it is trained based on inaccurate non-SV sensitive labels. To address this, we propose a speaker verification-based voice activity detection (SVVAD) framework that can adapt the speech features according to which are most informative for SV. To achieve this, we introduce a label-free training method with triplet-like losses that completely avoids the performance degradation of SV due to incorrect labeling. Extensive experiments show that SVVAD significantly outperforms the baseline in terms of equal error rate (EER) under conditions where other speakers are mixed at different ratios. Moreover, the decision boundaries reveal the importance of the different parts of speech, which are largely consistent with human judgments.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#21487;&#22797;&#21046;&#24615;&#65292;&#25552;&#20986;&#20102;&#21487;&#22797;&#21046;&#31639;&#27861;&#21644;&#26494;&#24347;&#21487;&#22797;&#21046;&#31639;&#27861;&#65292;&#24182;&#32473;&#20986;&#20102;&#30456;&#24212;&#30340;&#26102;&#38388;&#21644;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#36825;&#23545;&#20110;RL&#31639;&#27861;&#35774;&#35745;&#20197;&#21450;&#26410;&#26469;&#30340;&#21487;&#22797;&#21046;&#24615;&#30740;&#31350;&#20855;&#26377;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2305.19562</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#21487;&#22797;&#29616;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Replicability in Reinforcement Learning. (arXiv:2305.19562v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19562
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#21487;&#22797;&#21046;&#24615;&#65292;&#25552;&#20986;&#20102;&#21487;&#22797;&#21046;&#31639;&#27861;&#21644;&#26494;&#24347;&#21487;&#22797;&#21046;&#31639;&#27861;&#65292;&#24182;&#32473;&#20986;&#20102;&#30456;&#24212;&#30340;&#26102;&#38388;&#21644;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#36825;&#23545;&#20110;RL&#31639;&#27861;&#35774;&#35745;&#20197;&#21450;&#26410;&#26469;&#30340;&#21487;&#22797;&#21046;&#24615;&#30740;&#31350;&#20855;&#26377;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22312;&#24378;&#21270;&#23398;&#20064; (RL) &#30340;&#32972;&#26223;&#19979;&#65292;&#23558;&#21487;&#22797;&#29616;&#24615;&#20316;&#20026;&#31639;&#27861;&#23646;&#24615;&#36827;&#34892;&#20102;&#25968;&#23398;&#30740;&#31350;&#12290;&#25105;&#20204;&#20851;&#27880;&#30340;&#26159;&#20855;&#26377;&#29983;&#25104;&#27169;&#22411;&#35775;&#38382;&#26435;&#30340;&#24102;&#25240;&#25187;&#34920;&#26684;MDP&#30340;&#22522;&#26412;&#35774;&#32622;&#12290;&#21463;Impagliazzo&#31561;&#20154; [2022]&#30340;&#21551;&#21457;&#65292;&#22914;&#26524;&#22312;&#20869;&#37096;&#38543;&#26426;&#24615;&#30456;&#21516;&#26102;&#65292;RL&#31639;&#27861;&#22312;&#20174;&#29983;&#25104;&#22120;&#25277;&#21462;&#30340;&#20004;&#20010;&#29420;&#31435;&#21644;&#21516;&#20998;&#24067;&#30340;&#26679;&#26412;&#19978;&#25191;&#34892;&#20004;&#27425;&#24182;&#36755;&#20986;&#23436;&#20840;&#30456;&#21516;&#30340;&#31574;&#30053;&#65292;&#21017;&#34920;&#31034;&#35813;RL&#31639;&#27861;&#26159;&#21487;&#22797;&#21046;&#30340;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20379;&#19968;&#20010;&#26377;&#25928;&#30340;$\rho$-&#21487;&#22797;&#21046;&#31639;&#27861;&#65292;&#29992;&#20110;$(\varepsilon,\delta)$-&#26368;&#20248;&#31574;&#30053;&#20272;&#35745;&#65292;&#20854;&#26679;&#26412;&#21644;&#26102;&#38388;&#22797;&#26434;&#24230;&#20026; $\widetilde O\left(\frac{N^3\cdot\log(1/\delta)}{(1-\gamma)^5\cdot\varepsilon^2\cdot\rho^2}\right)$&#65292;&#20854;&#20013;$N$&#26159;&#29366;&#24577;-&#21160;&#20316;&#23545;&#30340;&#25968;&#37327;&#12290;&#28982;&#21518;&#65292;&#23545;&#20110;&#30830;&#23450;&#24615;&#31639;&#27861;&#30340;&#23376;&#31867;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102; $ \Omega\left(\frac {N^3}{(1-\gamma)^3\cdot\varepsilon^2\cdot\rho^2}\right) $ &#38454;&#30340;&#19979;&#38480;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;Kalavasis&#31561;&#20154;[2019]&#25552;&#20986;&#30340;&#21487;&#22797;&#21046;&#24615;&#30340;&#26494;&#24347;&#29256;&#26412;&#65292;&#20854;&#20013;&#20165;&#35201;&#27714;&#31639;&#27861;&#30340;&#36755;&#20986;&#25509;&#36817;&#22797;&#21046;&#31639;&#27861;&#30340;&#36755;&#20986;&#65292;&#32780;&#19981;&#26159;&#30456;&#21516;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#25928;&#31639;&#27861;&#65292;&#20854;&#26102;&#38388;&#21644;&#26679;&#26412;&#22797;&#26434;&#24230;&#20026; $\widetilde O\left(\frac{N^5\cdot\log(1/\delta)}{(1-\gamma)^9\cdot\varepsilon^4\cdot\rho^2}\right)$&#65292;&#29992;&#20110;$(\varepsilon,\delta)$&#24847;&#20041;&#19979;&#30340;&#21487;&#22797;&#21046;&#24615;&#65292;&#36825;&#27604;&#20808;&#21069;&#19982;&#30456;&#20851;&#38382;&#39064;&#30340;&#30028;&#38480;&#26356;&#22909;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#25105;&#20204;&#30340;&#32467;&#26524;&#23545;RL&#31639;&#27861;&#35774;&#35745;&#21644;&#21487;&#37325;&#22797;&#24615;&#30740;&#31350;&#30340;&#26410;&#26469;&#26041;&#21521;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
We initiate the mathematical study of replicability as an algorithmic property in the context of reinforcement learning (RL). We focus on the fundamental setting of discounted tabular MDPs with access to a generative model. Inspired by Impagliazzo et al. [2022], we say that an RL algorithm is replicable if, with high probability, it outputs the exact same policy after two executions on i.i.d. samples drawn from the generator when its internal randomness is the same. We first provide an efficient $\rho$-replicable algorithm for $(\varepsilon, \delta)$-optimal policy estimation with sample and time complexity $\widetilde O\left(\frac{N^3\cdot\log(1/\delta)}{(1-\gamma)^5\cdot\varepsilon^2\cdot\rho^2}\right)$, where $N$ is the number of state-action pairs. Next, for the subclass of deterministic algorithms, we provide a lower bound of order $\Omega\left(\frac{N^3}{(1-\gamma)^3\cdot\varepsilon^2\cdot\rho^2}\right)$. Then, we study a relaxed version of replicability proposed by Kalavasis et 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110; MAR &#26381;&#21153;&#30340;&#31227;&#21160;&#22686;&#24378;&#29616;&#23454;&#20219;&#21153;&#21368;&#36733;&#26041;&#26696;&#65292;&#20351;&#29992;&#26377;&#21521;&#26080;&#29615;&#22270;&#24314;&#27169;&#20219;&#21153;&#30456;&#20114;&#20381;&#36182;&#24615;&#65292;&#24182;&#37319;&#29992;&#21069;&#30651;&#24335;&#21368;&#36733;&#26041;&#26696;&#65292;&#22522;&#20110;&#25913;&#36827;&#30340;&#33945;&#29305;&#21345;&#32599;&#26641;&#25628;&#32034;&#26469;&#30830;&#20445;&#39640;&#25928;&#24615;&#21644;&#20844;&#24179;&#24615;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#26696;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#20219;&#21153;&#24310;&#36831;&#21644;&#33021;&#37327;&#28040;&#32791;&#12290;</title><link>http://arxiv.org/abs/2305.19558</link><description>&lt;p&gt;
&#22522;&#20110;&#21069;&#30651;&#30340;&#31227;&#21160;&#22686;&#24378;&#29616;&#23454;&#36793;&#32536;&#20113;&#35745;&#31639;&#20219;&#21153;&#21368;&#36733;
&lt;/p&gt;
&lt;p&gt;
Look-Ahead Task Offloading for Multi-User Mobile Augmented Reality in Edge-Cloud Computing. (arXiv:2305.19558v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19558
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110; MAR &#26381;&#21153;&#30340;&#31227;&#21160;&#22686;&#24378;&#29616;&#23454;&#20219;&#21153;&#21368;&#36733;&#26041;&#26696;&#65292;&#20351;&#29992;&#26377;&#21521;&#26080;&#29615;&#22270;&#24314;&#27169;&#20219;&#21153;&#30456;&#20114;&#20381;&#36182;&#24615;&#65292;&#24182;&#37319;&#29992;&#21069;&#30651;&#24335;&#21368;&#36733;&#26041;&#26696;&#65292;&#22522;&#20110;&#25913;&#36827;&#30340;&#33945;&#29305;&#21345;&#32599;&#26641;&#25628;&#32034;&#26469;&#30830;&#20445;&#39640;&#25928;&#24615;&#21644;&#20844;&#24179;&#24615;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#26696;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#20219;&#21153;&#24310;&#36831;&#21644;&#33021;&#37327;&#28040;&#32791;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31227;&#21160;&#22686;&#24378;&#29616;&#23454; (MAR) &#23558;&#30495;&#23454;&#22330;&#26223;&#19982;&#34394;&#25311;&#20869;&#23481;&#34701;&#21512;&#65292;&#34987;&#35270;&#20026; Metaverse &#20013;&#26222;&#36941;&#25509;&#21475;&#20043;&#19968;&#12290;&#30001;&#20110; MAR &#35774;&#22791;&#35745;&#31639;&#33021;&#21147;&#21644;&#30005;&#27744;&#23551;&#21629;&#26377;&#38480;&#65292;&#22240;&#27492;&#24120;&#35265;&#30340;&#20570;&#27861;&#26159;&#23558;&#35745;&#31639;&#20219;&#21153;&#21368;&#36733;&#21040;&#38752;&#36817;&#35774;&#22791;&#30340;&#36793;&#32536;&#25110;&#20113;&#26381;&#21153;&#22120;&#19978;&#12290;&#28982;&#32780;&#65292;&#20026; MAR &#20219;&#21153;&#24320;&#21457;&#30340;&#29616;&#26377;&#21368;&#36733;&#26041;&#26696;&#22312;&#25552;&#20379;&#22810;&#29992;&#25143; MAR &#26381;&#21153;&#26102;&#23384;&#22312;&#39640;&#36801;&#31227;&#24320;&#38144;&#12289;&#21487;&#25193;&#23637;&#24615;&#24046;&#21644;&#30701;&#35270;&#24615;&#30340;&#38382;&#39064;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#35774;&#35745;&#24182;&#35780;&#20272;&#20102;&#19968;&#31181;&#22522;&#20110; MAR &#26381;&#21153;&#30340;&#20219;&#21153;&#21368;&#36733;&#26041;&#26696;&#65292;&#24212;&#29992;&#20110;&#36793;&#32536;&#20113;&#35745;&#31639;&#32593;&#32476;&#20013;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#39318;&#20808;&#20351;&#29992;&#26377;&#21521;&#26080;&#29615;&#22270;&#23545; MAR &#24212;&#29992;&#30340;&#20219;&#21153;&#30456;&#20114;&#20381;&#36182;&#24615;&#36827;&#34892;&#20998;&#26512;&#21644;&#24314;&#27169;&#12290;&#28982;&#21518;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25913;&#36827;&#30340;&#33945;&#29305;&#21345;&#32599;&#26641; (MMCT) &#25628;&#32034;&#30340;&#21069;&#30651;&#24335;&#21368;&#36733;&#26041;&#26696;&#65292;&#21487;&#20197;&#25552;&#21069;&#36816;&#34892;&#22810;&#20010;&#22810;&#27493;&#25191;&#34892;&#65292;&#20197;&#33719;&#21462;&#21363;&#26102;&#34892;&#21160;&#30340;&#38271;&#26399;&#25928;&#26524;&#20272;&#35745;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#26696;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#20219;&#21153;&#24310;&#36831;&#21644;&#33021;&#37327;&#28040;&#32791;&#65292;&#21516;&#26102;&#23545;&#22810;&#29992;&#25143; MAR &#26381;&#21153;&#20445;&#25345;&#20219;&#21153;&#20934;&#30830;&#24615;&#21644;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mobile augmented reality (MAR) blends a real scenario with overlaid virtual content, which has been envisioned as one of the ubiquitous interfaces to the Metaverse. Due to the limited computing power and battery life of MAR devices, it is common to offload the computation tasks to edge or cloud servers in close proximity. However, existing offloading solutions developed for MAR tasks suffer from high migration overhead, poor scalability, and short-sightedness when applied in provisioning multi-user MAR services. To address these issues, a MAR service-oriented task offloading scheme is designed and evaluated in edge-cloud computing networks. Specifically, the task interdependency of MAR applications is firstly analyzed and modeled by using directed acyclic graphs. Then, we propose a look-ahead offloading scheme based on a modified Monte Carlo tree (MMCT) search, which can run several multi-step executions in advance to get an estimate of the long-term effect of immediate action. Experim
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#35821;&#38899;&#19978;&#19979;&#25991;&#23545;&#30495;&#23454;&#35828;&#35805;&#20154;&#33080;&#29983;&#25104;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;Context-Aware Lip-Sync&#26694;&#26550;&#65288;CALS&#65289;&#65292;&#21487;&#21033;&#29992;&#35821;&#38899;&#19978;&#19979;&#25991;&#29983;&#25104;&#26356;&#21152;&#20934;&#30830;&#12289;&#31283;&#23450;&#30340;&#21767;&#37096;&#36816;&#21160;&#12290;</title><link>http://arxiv.org/abs/2305.19556</link><description>&lt;p&gt;
&#25506;&#32034;&#21767;&#37096;&#36816;&#21160;&#30340;&#35821;&#38899;&#19978;&#19979;&#25991;&#23545;&#30495;&#23454;&#35828;&#35805;&#20154;&#33080;&#29983;&#25104;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Exploring Phonetic Context in Lip Movement for Authentic Talking Face Generation. (arXiv:2305.19556v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19556
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#35821;&#38899;&#19978;&#19979;&#25991;&#23545;&#30495;&#23454;&#35828;&#35805;&#20154;&#33080;&#29983;&#25104;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;Context-Aware Lip-Sync&#26694;&#26550;&#65288;CALS&#65289;&#65292;&#21487;&#21033;&#29992;&#35821;&#38899;&#19978;&#19979;&#25991;&#29983;&#25104;&#26356;&#21152;&#20934;&#30830;&#12289;&#31283;&#23450;&#30340;&#21767;&#37096;&#36816;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35828;&#35805;&#20154;&#33080;&#29983;&#25104;&#26159;&#23558;&#33258;&#28982;&#38754;&#37096;&#19982;&#39537;&#21160;&#38899;&#39057;&#21516;&#27493;&#21512;&#25104;&#30340;&#20219;&#21153;&#12290;&#23613;&#31649;&#22312;&#35270;&#35273;&#36136;&#37327;&#12289;&#21767;&#24418;&#21516;&#27493;&#21644;&#38754;&#37096;&#21160;&#20316;&#26041;&#38754;&#21462;&#24471;&#20102;&#24456;&#22823;&#36827;&#23637;&#65292;&#20294;&#24403;&#21069;&#30340;&#30740;&#31350;&#20173;&#28982;&#38590;&#20197;&#35299;&#20915;&#31895;&#31961;&#21644;&#24322;&#27493;&#30340;&#21767;&#37096;&#36816;&#21160;&#38382;&#39064;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#31867;&#20284;&#26408;&#20598;&#21160;&#30011;&#30340;&#25928;&#26524;&#12290;&#26412;&#25991;&#21457;&#29616;&#65292;&#20197;&#24448;&#30340;&#20316;&#21697;&#36890;&#24120;&#23558;&#21767;&#37096;&#36816;&#21160;&#19982;&#38899;&#39057;&#22312;&#19981;&#21516;&#30340;&#38899;&#32032;&#32423;&#21035;&#19978;&#36827;&#34892;&#30456;&#20851;&#32852;&#65292;&#28982;&#32780;&#65292;&#30001;&#20110;&#38899;&#32032;&#20043;&#38388;&#30340;&#21327;&#21516;&#21457;&#38899;&#65288;co-articulation&#65289;&#29616;&#35937;&#65292;&#21363;&#38548;&#31163;&#30340;&#38899;&#32032;&#21463;&#21069;&#19968;&#20010;&#25110;&#19979;&#19968;&#20010;&#38899;&#32032;&#30340;&#24433;&#21709;&#65292;&#22240;&#27492;&#21516;&#19968;&#20010;&#38899;&#32032;&#30340;&#21457;&#38899;&#22240;&#38899;&#32032;&#19978;&#19979;&#25991;&#32780;&#24322;&#12290;&#22240;&#27492;&#65292;&#20351;&#29992;&#38899;&#32032;&#19978;&#19979;&#25991;&#27169;&#22411;&#21487;&#20197;&#29983;&#25104;&#26356;&#21152;&#31354;&#38388;&#21644;&#26102;&#38388;&#19978;&#23545;&#40784;&#12289;&#31283;&#23450;&#30340;&#21767;&#37096;&#36816;&#21160;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#21767;&#37096;&#36816;&#21160;&#20013;&#30340;&#35821;&#38899;&#19978;&#19979;&#25991;&#23545;&#20110;&#30495;&#23454;&#35828;&#35805;&#20154;&#33080;&#29983;&#25104;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;Context-Aware Lip-Sync&#26694;&#26550;&#65288;CALS&#65289;&#65292;&#21033;&#29992;&#35821;&#38899;&#19978;&#19979;&#25991;&#29983;&#25104;&#26356;&#21152;&#31354;&#38388;&#21644;&#26102;&#38388;&#19978;&#23545;&#40784;&#12289;&#31283;&#23450;&#30340;&#21767;&#37096;&#36816;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
Talking face generation is the task of synthesizing a natural face synchronous to driving audio. Although much progress has been made in terms of visual quality, lip synchronization, and facial motion of the talking face, current works still struggle to overcome issues of crude and asynchronous lip movement, which can result in puppetry-like animation. We identify that the prior works commonly correlate lip movement with audio at the phone level. However, due to co-articulation, where an isolated phone is influenced by the preceding or following phones, the articulation of a phone varies upon the phonetic context. Therefore, modeling lip motion with the phonetic context can generate more spatio-temporally aligned and stable lip movement. In this respect, we investigate the phonetic context in lip motion for authentic talking face generation. We propose a Context-Aware Lip-Sync framework (CALS), which leverages phonetic context to generate more spatio-temporally aligned and stable lip m
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#30446;&#26631;&#20013;&#24515;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21152;&#20837;&#31354;&#38388;&#23616;&#37096;&#24615;&#20808;&#39564;&#26469;&#25552;&#39640;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#20351;&#27169;&#22411;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#19978;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#29289;&#20307;&#20998;&#21106;&#25913;&#36827;&#65292;&#24182;&#19988;&#23545;&#27169;&#22411;&#36229;&#21442;&#25968;&#19981;&#22826;&#25935;&#24863;&#12290;</title><link>http://arxiv.org/abs/2305.19550</link><description>&lt;p&gt;
Spotlight Attention: &#20855;&#22791;&#31354;&#38388;&#23616;&#37096;&#24615;&#20808;&#39564;&#30340;&#40065;&#26834;&#30446;&#26631;&#20013;&#24515;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Spotlight Attention: Robust Object-Centric Learning With a Spatial Locality Prior. (arXiv:2305.19550v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19550
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#30446;&#26631;&#20013;&#24515;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21152;&#20837;&#31354;&#38388;&#23616;&#37096;&#24615;&#20808;&#39564;&#26469;&#25552;&#39640;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#20351;&#27169;&#22411;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#19978;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#29289;&#20307;&#20998;&#21106;&#25913;&#36827;&#65292;&#24182;&#19988;&#23545;&#27169;&#22411;&#36229;&#21442;&#25968;&#19981;&#22826;&#25935;&#24863;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#26631;&#20013;&#24515;&#35270;&#35273;&#30340;&#30446;&#30340;&#26159;&#26500;&#24314;&#22330;&#26223;&#20013;&#29289;&#20307;&#30340;&#26174;&#24335;&#34920;&#31034;&#12290;&#36825;&#31181;&#34920;&#31034;&#26159;&#36890;&#36807;&#19968;&#32452;&#21487;&#20114;&#25442;&#30340;&#27169;&#22359;(&#31216;&#20026;slot&#25110;&#23545;&#35937;&#25991;&#20214;)&#33719;&#24471;&#30340;&#65292;&#23427;&#20204;&#31454;&#20105;&#22270;&#20687;&#30340;&#23616;&#37096;&#34917;&#19969;&#12290;&#35813;&#31454;&#20105;&#20855;&#26377;&#24369;&#24863;&#24615;&#20559;&#24046;&#65292;&#20197;&#20445;&#25345;&#31354;&#38388;&#36830;&#32493;&#24615;;&#22240;&#27492;&#65292;&#19968;&#20010;slot&#21487;&#33021;&#20250;&#23459;&#31216;&#22312;&#25972;&#20010;&#22270;&#20687;&#20013;&#25955;&#24067;&#30340;&#34917;&#19969;&#12290;&#19982;&#27492;&#30456;&#21453;&#65292;&#20154;&#31867;&#35270;&#35273;&#30340;&#24863;&#24615;&#20559;&#24046;&#24456;&#24378;&#65292;&#21040;&#20102;&#27880;&#24847;&#21147;&#32463;&#20856;&#29992;&#32858;&#20809;&#28783;&#27604;&#21947;&#30340;&#31243;&#24230;&#12290;&#25105;&#20204;&#23558;&#31354;&#38388;&#23616;&#37096;&#24615;&#20808;&#39564;&#34701;&#20837;&#29616;&#20195;&#30446;&#26631;&#20013;&#24515;&#35270;&#35273;&#27169;&#22411;&#65292;&#20174;&#32780;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#20013;&#33719;&#24471;&#26174;&#30528;&#30340;&#29289;&#20307;&#20998;&#21106;&#25913;&#36827;&#12290;&#31867;&#20284;&#20110;&#20154;&#31867;&#35270;&#35273;&#27880;&#24847;&#21147;&#65292;&#22270;&#20687;&#20869;&#23481;&#21644;&#31354;&#38388;&#32422;&#26463;&#30340;&#32452;&#21512;&#20135;&#29983;&#20102;&#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#26080;&#30417;&#30563;&#30446;&#26631;&#20013;&#24515;&#23398;&#20064;&#65292;&#21253;&#25324;&#23545;&#27169;&#22411;&#36229;&#21442;&#25968;&#19981;&#22826;&#25935;&#24863;&#12290;
&lt;/p&gt;
&lt;p&gt;
The aim of object-centric vision is to construct an explicit representation of the objects in a scene. This representation is obtained via a set of interchangeable modules called \emph{slots} or \emph{object files} that compete for local patches of an image. The competition has a weak inductive bias to preserve spatial continuity; consequently, one slot may claim patches scattered diffusely throughout the image. In contrast, the inductive bias of human vision is strong, to the degree that attention has classically been described with a spotlight metaphor. We incorporate a spatial-locality prior into state-of-the-art object-centric vision models and obtain significant improvements in segmenting objects in both synthetic and real-world datasets. Similar to human visual attention, the combination of image content and spatial constraints yield robust unsupervised object-centric learning, including less sensitivity to model hyperparameters.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;HRR&#30340;&#31070;&#32463;&#31526;&#21495;&#26041;&#27861;&#37325;&#26032;&#26500;&#24314;&#33258;&#27880;&#24847;&#21147;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#23454;&#29616;&#36739;&#20302;&#30340;&#26102;&#38388;&#21644;&#31354;&#38388;&#22797;&#26434;&#24230;&#65292;&#24182;&#22312;LRA&#22522;&#20934;&#27979;&#35797;&#20013;&#33719;&#24471;&#20102;&#25509;&#36817;&#20110;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#24230;&#12290;</title><link>http://arxiv.org/abs/2305.19534</link><description>&lt;p&gt;
&#29992;&#20840;&#24687;&#32422;&#21270;&#34920;&#31034;&#37325;&#26032;&#24314;&#27169;&#33258;&#27880;&#24847;&#21147;
&lt;/p&gt;
&lt;p&gt;
Recasting Self-Attention with Holographic Reduced Representations. (arXiv:2305.19534v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19534
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;HRR&#30340;&#31070;&#32463;&#31526;&#21495;&#26041;&#27861;&#37325;&#26032;&#26500;&#24314;&#33258;&#27880;&#24847;&#21147;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#23454;&#29616;&#36739;&#20302;&#30340;&#26102;&#38388;&#21644;&#31354;&#38388;&#22797;&#26434;&#24230;&#65292;&#24182;&#22312;LRA&#22522;&#20934;&#27979;&#35797;&#20013;&#33719;&#24471;&#20102;&#25509;&#36817;&#20110;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#33258;&#27880;&#24847;&#21147;&#24050;&#32463;&#25104;&#20026;&#21508;&#20010;&#39046;&#22495;&#24207;&#21015;&#24314;&#27169;&#30340;&#20027;&#35201;&#33539;&#20363;&#12290;&#28982;&#32780;&#65292;&#22312;&#24207;&#21015;&#38271;&#24230;&#38750;&#24120;&#38271;&#30340;&#39046;&#22495;&#20013;&#65292;&#22797;&#26434;&#24230;&#20026;$\mathcal{O}(T^2)$&#30340;&#20869;&#23384;&#21644;$\mathcal{O}(T^2 \cdot H)$&#30340;&#35745;&#31639;&#25104;&#26412;&#21487;&#33021;&#20250;&#20351;&#24471;&#20351;&#29992;&#21464;&#24418;&#37329;&#21018;&#32593;&#32476;&#19981;&#21487;&#34892;&#12290;&#21463;&#24778;&#29289;&#26816;&#27979;&#20013;$T \geq 100,000$&#30340;&#24207;&#21015;&#38271;&#24230;&#25104;&#20026;&#28145;&#24230;&#23398;&#20064;&#30340;&#25318;&#36335;&#34382;&#30340;&#38382;&#39064;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#20351;&#29992;&#20840;&#24687;&#32422;&#21270;&#34920;&#31034;&#65288;HRR&#65289;&#30340;&#31070;&#32463;&#31526;&#21495;&#21270;&#26041;&#27861;&#37325;&#26032;&#26500;&#24314;&#33258;&#27880;&#24847;&#21147;&#12290;&#36825;&#26679;&#25105;&#20204;&#25191;&#34892;&#30456;&#21516;&#30340;&#39640;&#32423;&#31574;&#30053;&#65292;&#21363;&#26631;&#20934;&#33258; &#27880;&#24847;&#21147;&#30340;&#26597;&#35810;&#21305;&#37197;&#38053;&#21273;&#65292;&#36820;&#22238;&#27599;&#20010;&#38190;&#30340;&#20540;&#30340;&#21152;&#26435;&#21709;&#24212;&#12290;&#36890;&#36807;&#23454;&#29616;&#8220;Hrrformer&#8221;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#19968;&#20123;&#22909;&#22788;&#65292;&#21253;&#25324;$\mathcal{O}(T H \log H)$&#30340;&#26102;&#38388;&#22797;&#26434;&#24230;&#12289;$\mathcal{O}(T H)$&#30340;&#31354;&#38388;&#22797;&#26434;&#24230;&#21644;&#25910;&#25947;&#20110;$10\times$&#26356;&#23569;&#30340;&#36845;&#20195;&#27425;&#25968;&#12290;&#28982;&#32780;&#65292;Hrrformer&#22312;LRA&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#20102;&#25509;&#36817;&#20110;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#24230;&#65292;&#25105;&#20204;&#33021;&#22815;&#23398;&#20064;&#21040;&#28145;&#24230;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, self-attention has become the dominant paradigm for sequence modeling in a variety of domains. However, in domains with very long sequence lengths the $\mathcal{O}(T^2)$ memory and $\mathcal{O}(T^2 H)$ compute costs can make using transformers infeasible. Motivated by problems in malware detection, where sequence lengths of $T \geq 100,000$ are a roadblock to deep learning, we re-cast self-attention using the neuro-symbolic approach of Holographic Reduced Representations (HRR). In doing so we perform the same high-level strategy of the standard self-attention: a set of queries matching against a set of keys, and returning a weighted response of the values for each key. Implemented as a ``Hrrformer'' we obtain several benefits including $\mathcal{O}(T H \log H)$ time complexity, $\mathcal{O}(T H)$ space complexity, and convergence in $10\times$ fewer epochs. Nevertheless, the Hrrformer achieves near state-of-the-art accuracy on LRA benchmarks and we are able to learn wi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#24335;&#35821;&#35328;&#27169;&#22411;&#30340;&#32454;&#31890;&#24230;&#25991;&#26412;&#39118;&#26684;&#36716;&#25442;&#26041;&#27861;&#65292;&#22312;&#19981;&#20381;&#36182;&#22806;&#37096;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#21462;&#24471;&#20102;&#27604;&#20043;&#21069;&#21033;&#29992;&#39044;&#35757;&#32451;&#26435;&#37325;&#12289;&#23884;&#20837;&#21644;&#22806;&#37096;&#35821;&#27861;&#20998;&#26512;&#22120;&#26356;&#22909;&#30340;&#25928;&#26524;&#65292;&#34920;&#26126;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#22312;&#25991;&#26412;&#29983;&#25104;&#39046;&#22495;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;</title><link>http://arxiv.org/abs/2305.19512</link><description>&lt;p&gt;
&#22522;&#20110;&#25193;&#25955;&#24335;&#35821;&#35328;&#27169;&#22411;&#30340;&#32454;&#31890;&#24230;&#25991;&#26412;&#39118;&#26684;&#36716;&#25442;
&lt;/p&gt;
&lt;p&gt;
Fine-grained Text Style Transfer with Diffusion-Based Language Models. (arXiv:2305.19512v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19512
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#24335;&#35821;&#35328;&#27169;&#22411;&#30340;&#32454;&#31890;&#24230;&#25991;&#26412;&#39118;&#26684;&#36716;&#25442;&#26041;&#27861;&#65292;&#22312;&#19981;&#20381;&#36182;&#22806;&#37096;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#21462;&#24471;&#20102;&#27604;&#20043;&#21069;&#21033;&#29992;&#39044;&#35757;&#32451;&#26435;&#37325;&#12289;&#23884;&#20837;&#21644;&#22806;&#37096;&#35821;&#27861;&#20998;&#26512;&#22120;&#26356;&#22909;&#30340;&#25928;&#26524;&#65292;&#34920;&#26126;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#22312;&#25991;&#26412;&#29983;&#25104;&#39046;&#22495;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#24335;&#27010;&#29575;&#27169;&#22411;&#24050;&#32463;&#22312;&#21487;&#25511;&#21046;&#22320;&#29983;&#25104;&#39640;&#36136;&#37327;&#22270;&#20687;&#19978;&#26174;&#31034;&#20986;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#65292;&#30740;&#31350;&#20154;&#21592;&#24050;&#32463;&#35797;&#22270;&#23558;&#36825;&#31181;&#21487;&#25511;&#24615;&#36816;&#29992;&#21040;&#25991;&#26412;&#29983;&#25104;&#39046;&#22495;&#12290;&#20197;&#21069;&#30340;&#25193;&#25955;&#24335;&#35821;&#35328;&#27169;&#22411;&#30740;&#31350;&#34920;&#26126;&#65292;&#23427;&#20204;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#22806;&#37096;&#30693;&#35782;&#65288;&#22914;&#39044;&#35757;&#32451;&#26435;&#37325;&#65289;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#19988;&#20173;&#28982;&#21487;&#20197;&#23454;&#29616;&#31283;&#23450;&#30340;&#24615;&#33021;&#21644;&#21487;&#25511;&#24615;&#12290; &#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#22312;StylePTB&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#20102;&#19968;&#20010;&#25193;&#25955;&#24335;&#27169;&#22411;&#65292;&#36825;&#26159;&#32454;&#31890;&#24230;&#25991;&#26412;&#39118;&#26684;&#36716;&#25442;&#30340;&#26631;&#20934;&#22522;&#20934;&#12290;&#19982;&#20197;&#21069;&#30340;&#24037;&#20316;&#35780;&#20272;&#20219;&#21153;&#30456;&#27604;&#65292;StylePTB&#20013;&#30340;&#20219;&#21153;&#38656;&#35201;&#23545;&#36755;&#20986;&#25991;&#26412;&#36827;&#34892;&#26356;&#21152;&#31934;&#32454;&#30340;&#25511;&#21046;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#33021;&#22815;&#22312;StylePTB&#19978;&#23454;&#29616;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;&#20010;&#21035;&#21644;&#32452;&#21512;&#36716;&#25442;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#27809;&#26377;&#22806;&#37096;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#20351;&#29992;StylePTB&#30340;&#26377;&#38480;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#20854;&#34920;&#29616;&#20248;&#20110;&#20197;&#21069;&#21033;&#29992;&#39044;&#35757;&#32451;&#26435;&#37325;&#12289;&#23884;&#20837;&#21644;&#22806;&#37096;&#35821;&#27861;&#20998;&#26512;&#22120;&#30340;&#24037;&#20316;&#65292;&#36825;&#21487;&#33021;&#34920;&#26126;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#22312;&#25991;&#26412;&#29983;&#25104;&#39046;&#22495;&#20855;&#26377;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion probabilistic models have shown great success in generating high-quality images controllably, and researchers have tried to utilize this controllability into text generation domain. Previous works on diffusion-based language models have shown that they can be trained without external knowledge (such as pre-trained weights) and still achieve stable performance and controllability. In this paper, we trained a diffusion-based model on StylePTB dataset, the standard benchmark for fine-grained text style transfers. The tasks in StylePTB requires much more refined control over the output text compared to tasks evaluated in previous works, and our model was able to achieve state-of-the-art performance on StylePTB on both individual and compositional transfers. Moreover, our model, trained on limited data from StylePTB without external knowledge, outperforms previous works that utilized pretrained weights, embeddings, and external grammar parsers, and this may indicate that diffusion
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39034;&#24207;&#32622;&#20449;&#24230;&#26657;&#20934;&#30340;&#26041;&#27861;&#65292;&#21363;&#20351;&#29992;&#24863;&#30693;&#21450;&#35821;&#20041;&#24863;&#30693;&#27491;&#21017;&#21270;&#26469;&#35843;&#25972;&#36807;&#20110;&#33258;&#20449;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23545;&#19982;&#30446;&#26631;&#24207;&#21015;&#39640;&#24230;&#30456;&#20851;&#30340;&#20196;&#29260;/&#24207;&#21015;&#36827;&#34892;&#20998;&#26512;&#65292;&#25552;&#39640;&#20102;&#28145;&#24230;&#39034;&#24207;&#35782;&#21035;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.19498</link><description>&lt;p&gt;
&#39034;&#24207;&#32622;&#20449;&#24230;&#26657;&#20934;&#30340;&#24863;&#30693;&#21450;&#35821;&#20041;&#24863;&#30693;&#27491;&#21017;&#21270;
&lt;/p&gt;
&lt;p&gt;
Perception and Semantic Aware Regularization for Sequential Confidence Calibration. (arXiv:2305.19498v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19498
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39034;&#24207;&#32622;&#20449;&#24230;&#26657;&#20934;&#30340;&#26041;&#27861;&#65292;&#21363;&#20351;&#29992;&#24863;&#30693;&#21450;&#35821;&#20041;&#24863;&#30693;&#27491;&#21017;&#21270;&#26469;&#35843;&#25972;&#36807;&#20110;&#33258;&#20449;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23545;&#19982;&#30446;&#26631;&#24207;&#21015;&#39640;&#24230;&#30456;&#20851;&#30340;&#20196;&#29260;/&#24207;&#21015;&#36827;&#34892;&#20998;&#26512;&#65292;&#25552;&#39640;&#20102;&#28145;&#24230;&#39034;&#24207;&#35782;&#21035;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#39034;&#24207;&#35782;&#21035;&#27169;&#22411;&#22240;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#30340;&#20248;&#36234;&#24615;&#33021;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#22823;&#22810;&#25968;&#28145;&#24230;&#39034;&#24207;&#35782;&#21035;&#27169;&#22411;&#20165;&#20351;&#29992;&#30446;&#26631;&#24207;&#21015;&#20316;&#20026;&#30417;&#30563;&#65292;&#32780;&#26410;&#32771;&#34385;&#20854;&#20182;&#30456;&#20851;&#24207;&#21015;&#65292;&#23548;&#33268;&#20854;&#39044;&#27979;&#32467;&#26524;&#36807;&#20110;&#33258;&#20449;&#12290;&#24050;&#26377;&#30340;&#28145;&#24230;&#39034;&#24207;&#35782;&#21035;&#27169;&#22411;&#20351;&#29992;&#26631;&#31614;&#24179;&#28369;&#26469;&#23545;&#26631;&#31614;&#36827;&#34892;&#35843;&#25972;&#20197;&#38477;&#20302;&#33258;&#20449;&#24230;&#65292;&#20294;&#23427;&#20204;&#24182;&#26410;&#32771;&#34385;&#20196;&#29260;/&#24207;&#21015;&#20043;&#38388;&#30340;&#20851;&#32852;&#24615;&#65292;&#36825;&#20123;&#20851;&#32852;&#24615;&#21487;&#33021;&#20026;&#27491;&#21017;&#21270;&#35757;&#32451;&#25552;&#20379;&#26356;&#21152;&#26377;&#25928;&#30340;&#20449;&#24687;&#65292;&#36827;&#32780;&#23548;&#33268;&#27425;&#20248;&#30340;&#24615;&#33021;&#12290;&#22240;&#27492;&#65292;&#26412;&#30740;&#31350;&#21457;&#29616;&#19982;&#30446;&#26631;&#24207;&#21015;&#39640;&#24230;&#24863;&#30693;&#21644;&#35821;&#20041;&#30456;&#20851;&#30340;&#20196;&#29260;/&#24207;&#21015;&#21253;&#21547;&#20102;&#26356;&#20855;&#30456;&#20851;&#24615;&#21644;&#26356;&#26377;&#25928;&#30340;&#20449;&#24687;&#65292;&#20174;&#32780;&#26377;&#21161;&#20110;&#26356;&#26377;&#25928;&#22320;&#36827;&#34892;&#27491;&#21017;&#21270;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24863;&#30693;&#21450;&#35821;&#20041;&#24863;&#30693;&#24207;&#21015;&#27491;&#21017;&#21270;&#20307;&#31995;&#32467;&#26500;&#65292;&#25506;&#32034;&#32852;&#24819;&#24863;&#30693;&#21644;&#35821;&#20041;&#30456;&#20284;&#24615;&#30340;&#20196;&#29260;/&#24207;&#21015;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep sequence recognition (DSR) models receive increasing attention due to their superior application to various applications. Most DSR models use merely the target sequences as supervision without considering other related sequences, leading to over-confidence in their predictions. The DSR models trained with label smoothing regularize labels by equally and independently smoothing each token, reallocating a small value to other tokens for mitigating overconfidence. However, they do not consider tokens/sequences correlations that may provide more effective information to regularize training and thus lead to sub-optimal performance. In this work, we find tokens/sequences with high perception and semantic correlations with the target ones contain more correlated and effective information and thus facilitate more effective regularization. To this end, we propose a Perception and Semantic aware Sequence Regularization framework, which explore perceptively and semantically correlated tokens
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;CVSNet&#65292;&#21487;&#20197;&#30475;&#20316;&#26159;&#22823;&#33041;&#20013;&#22830;&#35270;&#35273;&#31995;&#32479;&#30340;&#35745;&#31639;&#26426;&#23454;&#29616;&#65292;&#20854;&#20013;&#27599;&#20010;&#22359;&#37117;&#34920;&#31034;&#19982;&#22823;&#33041;&#20013;&#30456;&#21516;&#30340;&#35270;&#35273;&#20449;&#24687;&#65292;&#36890;&#36807;&#19977;&#20010;&#29420;&#31435;&#30340;&#36890;&#36947;&#21644;&#20116;&#20010;&#19981;&#21516;&#30340;&#22359;&#27969;&#21160;&#12290;</title><link>http://arxiv.org/abs/2305.19492</link><description>&lt;p&gt;
CVSNet&#65306;&#22823;&#33041;&#20013;&#22830;&#35270;&#35273;&#31995;&#32479;&#30340;&#35745;&#31639;&#26426;&#23454;&#29616;
&lt;/p&gt;
&lt;p&gt;
CVSNet: A Computer Implementation for Central Visual System of The Brain. (arXiv:2305.19492v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19492
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;CVSNet&#65292;&#21487;&#20197;&#30475;&#20316;&#26159;&#22823;&#33041;&#20013;&#22830;&#35270;&#35273;&#31995;&#32479;&#30340;&#35745;&#31639;&#26426;&#23454;&#29616;&#65292;&#20854;&#20013;&#27599;&#20010;&#22359;&#37117;&#34920;&#31034;&#19982;&#22823;&#33041;&#20013;&#30456;&#21516;&#30340;&#35270;&#35273;&#20449;&#24687;&#65292;&#36890;&#36807;&#19977;&#20010;&#29420;&#31435;&#30340;&#36890;&#36947;&#21644;&#20116;&#20010;&#19981;&#21516;&#30340;&#22359;&#27969;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#65292;&#19981;&#21516;&#30340;&#22522;&#30784;&#27169;&#22359;&#22260;&#32469;&#30528;&#19981;&#21516;&#30340;&#30697;&#38453;&#36816;&#31639;&#21019;&#24314;&#65292;&#24182;&#19988;&#22522;&#20110;&#36825;&#20123;&#22522;&#30784;&#27169;&#22359;&#30340;&#27169;&#22411;&#24050;&#32463;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#22522;&#20110;&#23454;&#39564;&#30340;&#27169;&#22411;&#20063;&#20351;&#24471;&#28145;&#24230;&#23398;&#20064;&#38271;&#26399;&#20197;&#26469;&#21463;&#21040;&#21407;&#29702;&#21644;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#30340;&#25209;&#35780;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#8212;&#8212;CVSNet&#65292;&#21487;&#20197;&#30475;&#20316;&#26159;&#22823;&#33041;&#20013;&#22830;&#35270;&#35273;&#31995;&#32479;&#30340;&#35745;&#31639;&#26426;&#23454;&#29616;&#12290;CVSNet&#20013;&#30340;&#27599;&#20010;&#22359;&#37117;&#34920;&#31034;&#19982;&#22823;&#33041;&#20013;&#30456;&#21516;&#30340;&#35270;&#35273;&#20449;&#24687;&#12290;CVSNet&#20013;&#30340;&#22359;&#24444;&#27492;&#19981;&#21516;&#65292;&#35270;&#35273;&#20449;&#24687;&#36890;&#36807;&#19977;&#20010;&#29420;&#31435;&#30340;&#36890;&#36947;&#21644;&#20116;&#20010;&#19981;&#21516;&#30340;&#22359;&#27969;&#21160;&#12290;&#22240;&#27492;&#65292;CVSNet&#19982;&#25152;&#26377;&#20808;&#21069;&#27169;&#22411;&#30340;&#35774;&#35745;&#23436;&#20840;&#19981;&#21516;&#65292;&#20854;&#20013;&#22522;&#30784;&#22359;&#34987;&#37325;&#22797;&#20197;&#26500;&#24314;&#27169;&#22411;&#65292;&#20449;&#24687;&#20063;&#24471;&#20197;&#20256;&#36882;&#12290;
&lt;/p&gt;
&lt;p&gt;
In computer vision, different basic blocks are created around different matrix operations, and models based on different basic blocks have achieved good results. Good results achieved in vision tasks grants them rationality. However, these experimental-based models also make deep learning long criticized for principle and interpretability. Deep learning originated from the concept of neurons in neuroscience, but recent designs detached natural neural networks except for some simple concepts. In this paper, we build an artificial neural network, CVSNet, which can be seen as a computer implementation for central visual system of the brain. Each block in CVSNet represents the same vision information as that in brains. In CVSNet, blocks differs from each other and visual information flows through three independent pathways and five different blocks. Thus CVSNet is completely different from the design of all previous models, in which basic blocks are repeated to build model and information 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25506;&#32034;&#25216;&#26415;&#65292;&#20351;&#29992;&#20540;&#26465;&#20214;&#29366;&#24577;&#29109;&#26469;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#20013;&#25506;&#32034;&#19981;&#36275;&#30340;&#38382;&#39064;&#65292;&#21487;&#20197;&#22343;&#34913;&#22320;&#35206;&#30422;&#20302;&#20215;&#20540;&#21644;&#39640;&#20215;&#20540;&#29366;&#24577;&#65292;&#30456;&#36739;&#20110;&#29616;&#26377;&#22522;&#20110;&#29109;&#30340;&#25506;&#32034;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;MuJoCo&#22522;&#20934;&#27979;&#35797;&#21644;Atari&#28216;&#25103;&#19978;&#26377;&#30528;&#26174;&#33879;&#30340;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2305.19476</link><description>&lt;p&gt;
&#20351;&#29992;&#20540;&#26465;&#20214;&#29366;&#24577;&#29109;&#25506;&#32034;&#21152;&#36895;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Accelerating Reinforcement Learning with Value-Conditional State Entropy Exploration. (arXiv:2305.19476v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19476
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25506;&#32034;&#25216;&#26415;&#65292;&#20351;&#29992;&#20540;&#26465;&#20214;&#29366;&#24577;&#29109;&#26469;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#20013;&#25506;&#32034;&#19981;&#36275;&#30340;&#38382;&#39064;&#65292;&#21487;&#20197;&#22343;&#34913;&#22320;&#35206;&#30422;&#20302;&#20215;&#20540;&#21644;&#39640;&#20215;&#20540;&#29366;&#24577;&#65292;&#30456;&#36739;&#20110;&#29616;&#26377;&#22522;&#20110;&#29109;&#30340;&#25506;&#32034;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;MuJoCo&#22522;&#20934;&#27979;&#35797;&#21644;Atari&#28216;&#25103;&#19978;&#26377;&#30528;&#26174;&#33879;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25506;&#32034;&#30340;&#19968;&#31181;&#26377;&#25928;&#25216;&#26415;&#26159;&#36890;&#36807;&#40723;&#21169;&#23545;&#35775;&#38382;&#29366;&#24577;&#31354;&#38388;&#30340;&#22343;&#21248;&#35206;&#30422;&#26469;&#26368;&#22823;&#21270;&#24050;&#35775;&#38382;&#29366;&#24577;&#20998;&#24067;&#30340;&#29109;&#65292;&#21363;&#29366;&#24577;&#29109;&#12290;&#28982;&#32780;&#65292;&#23427;&#22312;&#26377;&#20219;&#21153;&#22870;&#21169;&#30340;&#30417;&#30563;&#35774;&#32622;&#20013;&#24448;&#24448;&#38590;&#20197;&#24212;&#23545;&#65292;&#20854;&#20013;&#20195;&#29702;&#36235;&#21521;&#20110;&#35775;&#38382;&#39640;&#20215;&#20540;&#29366;&#24577;&#20197;&#21033;&#29992;&#20219;&#21153;&#22870;&#21169;&#12290;&#36825;&#20010;&#20559;&#22909;&#20250;&#23548;&#33268;&#39640;&#20215;&#20540;&#29366;&#24577;&#21644;&#20302;&#20215;&#20540;&#29366;&#24577;&#30340;&#20998;&#24067;&#19981;&#24179;&#34913;&#65292;&#24403;&#20998;&#24067;&#21464;&#24471;&#26356;&#21152;&#22343;&#21248;&#26102;&#65292;&#29366;&#24577;&#29109;&#20250;&#22686;&#21152;&#65292;&#20174;&#32780;&#20559;&#21521;&#20110;&#25506;&#32034;&#20302;&#20215;&#20540;&#21306;&#22495;&#12290;&#24403;&#39640;&#20215;&#20540;&#29366;&#24577;&#22312;&#29366;&#24577;&#31354;&#38388;&#20013;&#20998;&#24067;&#29421;&#31364;&#26102;&#65292;&#36825;&#20010;&#38382;&#39064;&#20250;&#36827;&#19968;&#27493;&#24694;&#21270;&#65292;&#20351;&#24471;&#20195;&#29702;&#23436;&#25104;&#20219;&#21153;&#21464;&#24471;&#26356;&#21152;&#22256;&#38590;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25506;&#32034;&#25216;&#26415;&#65292;&#26368;&#22823;&#21270;&#20540;&#26465;&#20214;&#29366;&#24577;&#29109;&#65292;&#23427;&#20998;&#21035;&#20272;&#35745;&#27599;&#20010;&#29366;&#24577;&#20215;&#20540;&#20272;&#35745;&#26465;&#20214;&#19979;&#30340;&#29366;&#24577;&#29109;&#65292;&#28982;&#21518;&#26368;&#22823;&#21270;&#23427;&#20204;&#30340;&#21152;&#26435;&#21644;&#12290;&#20540;&#26465;&#20214;&#29366;&#24577;&#29109;&#37327;&#21270;&#20102;&#20302;&#20215;&#20540;&#21644;&#39640;&#20215;&#20540;&#29366;&#24577;&#21306;&#22495;&#30340;&#35206;&#30422;&#33539;&#22260;&#65292;&#20174;&#32780;&#20351;&#20854;&#23545;&#19981;&#24179;&#34913;&#38382;&#39064;&#26356;&#21152;&#20581;&#22766;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19968;&#31995;&#21015;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;MuJoCo&#22522;&#20934;&#27979;&#35797;&#21644;Atari&#28216;&#25103;&#19978;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#20110;&#29109;&#30340;&#25506;&#32034;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
A promising technique for exploration is to maximize the entropy of visited state distribution, i.e., state entropy, by encouraging uniform coverage of visited state space. While it has been effective for an unsupervised setup, it tends to struggle in a supervised setup with a task reward, where an agent prefers to visit high-value states to exploit the task reward. Such a preference can cause an imbalance between the distributions of high-value states and low-value states, which biases exploration towards low-value state regions as a result of the state entropy increasing when the distribution becomes more uniform. This issue is exacerbated when high-value states are narrowly distributed within the state space, making it difficult for the agent to complete the tasks. In this paper, we present a novel exploration technique that maximizes the value-conditional state entropy, which separately estimates the state entropies that are conditioned on the value estimates of each state, then ma
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20851;&#27880;&#20844;&#24179;&#32858;&#31867;&#38382;&#39064;&#20013;&#30340;&#20154;&#21475;&#32479;&#35745;&#23398;&#20844;&#24179;&#27010;&#24565;&#65292;&#25552;&#20986;&#19968;&#31181;&#21516;&#26102;&#28385;&#36275;&#19981;&#21516;&#20844;&#24179;&#35201;&#27714;&#30340;&#24555;&#36895;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.19475</link><description>&lt;p&gt;
&#21452;&#37325;&#32422;&#26463;&#20844;&#24179;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Doubly Constrained Fair Clustering. (arXiv:2305.19475v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19475
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20851;&#27880;&#20844;&#24179;&#32858;&#31867;&#38382;&#39064;&#20013;&#30340;&#20154;&#21475;&#32479;&#35745;&#23398;&#20844;&#24179;&#27010;&#24565;&#65292;&#25552;&#20986;&#19968;&#31181;&#21516;&#26102;&#28385;&#36275;&#19981;&#21516;&#20844;&#24179;&#35201;&#27714;&#30340;&#24555;&#36895;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#20960;&#24180;&#20013;&#65292;&#20844;&#24179;&#32858;&#31867;&#21560;&#24341;&#20102;&#26174;&#30528;&#30340;&#20851;&#27880;&#65292;&#23548;&#33268;&#20986;&#29616;&#20102;&#22823;&#37327;&#19981;&#21516;&#30340;&#20844;&#24179;&#27010;&#24565;&#12290;&#34429;&#28982;&#36825;&#20123;&#27010;&#24565;&#26159;&#26377;&#20805;&#20998;&#29702;&#25454;&#30340;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#22312;&#29420;&#31435;&#30340;&#24773;&#20917;&#19979;&#34987;&#32771;&#34385;&#21644;&#30740;&#31350;&#65292;&#20854;&#20013;&#19968;&#20010;&#20844;&#24179;&#35201;&#27714;&#34987;&#29420;&#31435;&#22320;&#32771;&#34385;&#32780;&#19981;&#32771;&#34385;&#20854;&#20182;&#30340;&#12290;&#36825;&#23548;&#33268;&#20102;&#22312;&#20844;&#24179;&#32858;&#31867;&#20013;&#29702;&#35299;&#19981;&#21516;&#20844;&#24179;&#27010;&#24565;&#20043;&#38388;&#30340;&#20851;&#31995;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#30340;&#24320;&#25918;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26397;&#21521;&#36825;&#20010;&#26041;&#21521;&#36808;&#20986;&#20102;&#31532;&#19968;&#27493;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#32858;&#31867;&#20013;&#26368;&#31361;&#20986;&#30340;&#20004;&#31181;&#20154;&#21475;&#32479;&#35745;&#23398;&#20844;&#24179;&#27010;&#24565;:(1)&#38598;&#22242;&#20844;&#27491;(GF)&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#19981;&#21516;&#30340;&#20154;&#21475;&#32479;&#35745;&#23398;&#32676;&#20307;&#22312;&#27599;&#20010;&#32858;&#31867;&#20013;&#24212;&#35813;&#26377;&#25509;&#36817;&#20154;&#21475;&#27700;&#24179;&#30340;&#20195;&#34920;;(2)&#20013;&#24515;&#36873;&#25321;&#20013;&#30340;&#22810;&#26679;&#24615;(DS)&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#25152;&#36873;&#25321;&#30340;&#20013;&#24515;&#24212;&#35813;&#25509;&#36817;&#27599;&#20010;&#32676;&#20307;&#30340;&#20154;&#21475;&#27700;&#24179;&#34920;&#31034;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#32473;&#23450;&#19968;&#20010;&#24658;&#23450;&#30340;&#36817;&#20284;&#31639;&#27861;&#30340;&#24773;&#20917;&#19979;&#65292;&#19968;&#20010;&#24555;&#36895;&#30340;&#31639;&#27861;&#21487;&#20197;&#21516;&#26102;&#28385;&#36275;&#36825;&#20004;&#20010;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;
The remarkable attention which fair clustering has received in the last few years has resulted in a significant number of different notions of fairness. Despite the fact that these notions are well-justified, they are often motivated and studied in a disjoint manner where one fairness desideratum is considered exclusively in isolation from the others. This leaves the understanding of the relations between different fairness notions as an important open problem in fair clustering. In this paper, we take the first step in this direction. Specifically, we consider the two most prominent demographic representation fairness notions in clustering: (1) Group Fairness (GF), where the different demographic groups are supposed to have close to population-level representation in each cluster and (2) Diversity in Center Selection (DS), where the selected centers are supposed to have close to population-level representation of each group. We show that given a constant approximation algorithm for on
&lt;/p&gt;</description></item><item><title>PlaSma&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#36807;&#31243;&#30693;&#35782;&#21644;&#35745;&#21010;&#33021;&#21147;&#30340;&#26032;&#26041;&#27861;&#65292;</title><link>http://arxiv.org/abs/2305.19472</link><description>&lt;p&gt;
PlaSma: &#20026; (&#21453;&#20107;&#23454;) &#35745;&#21010;&#21046;&#23450;&#22686;&#24378;&#36807;&#31243;&#30693;&#35782;&#27169;&#22411;&#30340;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
PlaSma: Making Small Language Models Better Procedural Knowledge Models for (Counterfactual) Planning. (arXiv:2305.19472v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19472
&lt;/p&gt;
&lt;p&gt;
PlaSma&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#36807;&#31243;&#30693;&#35782;&#21644;&#35745;&#21010;&#33021;&#21147;&#30340;&#26032;&#26041;&#27861;&#65292;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#31243;&#35268;&#21010;&#26159;&#26426;&#22120;&#30340;&#19968;&#39033;&#37325;&#35201;&#32780;&#21448;&#22797;&#26434;&#30340;&#20219;&#21153;&#65292;&#23427;&#23558;&#19968;&#20010;&#39640;&#32423;&#30446;&#26631;&#20998;&#35299;&#20026;&#19968;&#31995;&#21015;&#26102;&#38388;&#39034;&#24207;&#30340;&#27493;&#39588;&#12290;&#23427;&#38656;&#35201;&#25972;&#21512;&#24120;&#35782;&#30693;&#35782;&#20197;&#25512;&#29702;&#20986;&#24120;&#24120;&#26159;&#21453;&#20107;&#23454;&#30340;&#22797;&#26434;&#24773;&#22659;&#65292;&#20363;&#22914; "&#27809;&#26377;&#30005;&#35805;&#26102;&#23433;&#25490;&#21307;&#29983;&#30340;&#32422;&#20250;"&#12290;&#24403;&#21069;&#30340;&#26041;&#27861;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLM) &#21462;&#24471;&#20102;&#20196;&#20154;&#40723;&#33310;&#30340;&#32467;&#26524;&#65292;&#20294;&#21463;&#21040;&#26114;&#36149;&#30340; API &#35843;&#29992;&#21644;&#21487;&#22797;&#29616;&#24615;&#38382;&#39064;&#30340;&#38480;&#21046;&#12290;&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#26356;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#26469;&#36827;&#34892;&#35268;&#21010;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102; PlaSma&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#30340;&#21452;&#37325;&#26041;&#27861;&#65292;&#20351;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#36807;&#31243;&#30693;&#35782;&#21644; (&#21453;&#20107;&#23454;) &#35745;&#21010;&#33021;&#21147;&#12290;&#26356;&#20855;&#20307;&#22320;&#35828;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#31526;&#21495;&#36807;&#31243;&#30693;&#35782;&#33976;&#39311;&#26469;&#22686;&#24378;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#38544;&#21547;&#30693;&#35782;&#65292;&#20197;&#21450;&#19968;&#31181;&#25512;&#29702;&#31639;&#27861;&#26469;&#20419;&#36827;&#26356;&#32467;&#26500;&#21270;&#21644;&#20934;&#30830;&#30340;&#25512;&#29702;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#20219;&#21153;&#65292;&#21453;&#20107;&#23454;&#35268;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;
Procedural planning, which entails decomposing a high-level goal into a sequence of temporally ordered steps, is an important yet intricate task for machines. It involves integrating common-sense knowledge to reason about complex contextualized situations that are often counterfactual, e.g. "scheduling a doctor's appointment without a phone". While current approaches show encouraging results using large language models (LLMs), they are hindered by drawbacks such as costly API calls and reproducibility issues. In this paper, we advocate planning using smaller language models. We present PlaSma, a novel two-pronged approach to endow small language models with procedural knowledge and (counterfactual) planning capabilities. More concretely, we develop symbolic procedural knowledge distillation to enhance the implicit knowledge in small language models and an inference-time algorithm to facilitate more structured and accurate reasoning. In addition, we introduce a novel task, Counterfactua
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;FPGA&#19978;&#26377;&#25928;&#23454;&#29616;&#22810;&#23618;&#26799;&#24230;&#33258;&#30001;&#22312;&#32447;&#21487;&#35757;&#32451;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#26412;&#22320;&#33258;&#36866;&#24212;&#36873;&#25321;&#38408;&#20540;&#21644;&#36866;&#21512;&#30828;&#20214;&#30340;&#20462;&#25913;&#37325;&#37327;&#26356;&#26032;&#35268;&#21017;&#65292;&#24182;&#23454;&#29616;&#20102;99.5&#65285;&#30340;&#39640;&#20998;&#31867;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2305.19468</link><description>&lt;p&gt;
FPGA&#19978;&#22810;&#23618;&#26799;&#24230;&#33258;&#30001;&#22312;&#32447;&#21487;&#35757;&#32451;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#30340;&#26377;&#25928;&#23454;&#29616;
&lt;/p&gt;
&lt;p&gt;
Efficient Implementation of a Multi-Layer Gradient-Free Online-Trainable Spiking Neural Network on FPGA. (arXiv:2305.19468v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19468
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;FPGA&#19978;&#26377;&#25928;&#23454;&#29616;&#22810;&#23618;&#26799;&#24230;&#33258;&#30001;&#22312;&#32447;&#21487;&#35757;&#32451;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#26412;&#22320;&#33258;&#36866;&#24212;&#36873;&#25321;&#38408;&#20540;&#21644;&#36866;&#21512;&#30828;&#20214;&#30340;&#20462;&#25913;&#37325;&#37327;&#26356;&#26032;&#35268;&#21017;&#65292;&#24182;&#23454;&#29616;&#20102;99.5&#65285;&#30340;&#39640;&#20998;&#31867;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#26368;&#36817;&#25552;&#20986;&#30340;&#20248;&#21270;&#28145;&#24230;&#20107;&#20214;&#39537;&#21160;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#20307;&#31995;&#32467;&#26500;&#65288;ODESA&#65289;&#30340;&#39640;&#25928;&#30828;&#20214;&#23454;&#29616;&#12290;ODESA&#26159;&#31532;&#19968;&#20010;&#20855;&#26377;&#31471;&#21040;&#31471;&#22810;&#23618;&#22312;&#32447;&#26412;&#22320;&#30417;&#30563;&#35757;&#32451;&#19988;&#19981;&#20351;&#29992;&#26799;&#24230;&#30340;&#32593;&#32476;&#65292;&#24182;&#20855;&#26377;&#26377;&#25928;&#30340;&#23618;&#27425;&#32467;&#26500;&#20013;&#30340;&#26435;&#37325;&#21644;&#38408;&#20540;&#30340;&#32452;&#21512;&#36866;&#24212;&#24615;&#12290;&#26412;&#30740;&#31350;&#34920;&#26126;&#65292;&#21487;&#20197;&#22312;&#30828;&#20214;&#19978;&#39640;&#25928;&#22320;&#23454;&#29616;&#32593;&#32476;&#26550;&#26500;&#21644;&#37325;&#37327;&#21644;&#38408;&#20540;&#30340;&#22312;&#32447;&#22521;&#35757;&#12290;&#23454;&#29616;&#21253;&#25324;&#22810;&#23618;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNN&#65289;&#21644;&#27599;&#20010;&#23618;&#30340;&#21333;&#29420;&#35757;&#32451;&#27169;&#22359;&#65292;&#21487;&#20197;&#23454;&#29616;&#22312;&#32447;&#33258;&#23398;&#20064;&#32780;&#19981;&#20351;&#29992;&#21453;&#21521;&#20256;&#25773;&#12290;&#36890;&#36807;&#20351;&#29992;&#31616;&#21333;&#30340;&#26412;&#22320;&#33258;&#36866;&#24212;&#36873;&#25321;&#38408;&#20540;&#65292;&#22312;&#27599;&#20010;&#23618;&#19978;&#36827;&#34892;&#36194;&#23478;&#36890;&#21507;&#65288;WTA&#65289;&#32422;&#26463;&#65292;&#24182;&#20351;&#29992;&#26356;&#36866;&#21512;&#30828;&#20214;&#30340;&#20462;&#25913;&#37325;&#37327;&#26356;&#26032;&#35268;&#21017;&#65292;&#35757;&#32451;&#27169;&#22359;&#21487;&#20197;&#22312;&#27599;&#20010;&#23618;&#20013;&#26368;&#20248;&#22320;&#20998;&#37197;&#31070;&#32463;&#20803;&#36164;&#28304;&#65292;&#32780;&#19981;&#24517;&#23558;&#39640;&#31934;&#24230;&#35823;&#24046;&#20449;&#21495;&#20256;&#36882;&#32473;&#36739;&#20302;&#23618;&#30340;&#31070;&#32463;&#20803;&#12290;&#22312;&#21487;&#25193;&#23637;&#30340;FPGA&#19978;&#25552;&#20986;&#30340;&#30828;&#20214;&#23454;&#29616;&#22312;MNIST&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;99.5&#65285;&#30340;&#39640;&#20998;&#31867;&#31934;&#24230;&#65292;&#21516;&#26102;&#36816;&#34892;&#36895;&#24230;&#26356;&#24555;&#65292;&#21151;&#32791;&#26356;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents an efficient hardware implementation of the recently proposed Optimized Deep Event-driven Spiking Neural Network Architecture (ODESA). ODESA is the first network to have end-to-end multi-layer online local supervised training without using gradients and has the combined adaptation of weights and thresholds in an efficient hierarchical structure. This research shows that the network architecture and the online training of weights and thresholds can be implemented efficiently on a large scale in hardware. The implementation consists of a multi-layer Spiking Neural Network (SNN) and individual training modules for each layer that enable online self-learning without using back-propagation. By using simple local adaptive selection thresholds, a Winner-Takes-All (WTA) constraint on each layer, and a modified weight update rule that is more amenable to hardware, the trainer module allocates neuronal resources optimally at each layer without having to pass high-precision er
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350; Transformer &#27169;&#22411;&#20013;&#20301;&#32622;&#32534;&#30721;&#23545;&#20110;&#38271;&#24230;&#25512;&#24191;&#30340;&#24433;&#21709;&#65292;&#32467;&#26524;&#34920;&#26126;&#24120;&#29992;&#30340;&#20301;&#32622;&#32534;&#30721;&#26041;&#27861;&#24182;&#19981;&#36866;&#21512;&#29992;&#20110;&#19979;&#28216;&#20219;&#21153;&#30340;&#38271;&#24230;&#25512;&#24191;&#65292;&#24182;&#19988;&#20351;&#29992;&#20301;&#32622;&#32534;&#30721;&#29978;&#33267;&#21487;&#33021;&#20250;&#25439;&#23475;&#38271;&#24230;&#25512;&#24191;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.19466</link><description>&lt;p&gt;
&#20301;&#32622;&#32534;&#30721;&#23545; Transformer &#27169;&#22411;&#38271;&#24230;&#25512;&#24191;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
The Impact of Positional Encoding on Length Generalization in Transformers. (arXiv:2305.19466v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19466
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350; Transformer &#27169;&#22411;&#20013;&#20301;&#32622;&#32534;&#30721;&#23545;&#20110;&#38271;&#24230;&#25512;&#24191;&#30340;&#24433;&#21709;&#65292;&#32467;&#26524;&#34920;&#26126;&#24120;&#29992;&#30340;&#20301;&#32622;&#32534;&#30721;&#26041;&#27861;&#24182;&#19981;&#36866;&#21512;&#29992;&#20110;&#19979;&#28216;&#20219;&#21153;&#30340;&#38271;&#24230;&#25512;&#24191;&#65292;&#24182;&#19988;&#20351;&#29992;&#20301;&#32622;&#32534;&#30721;&#29978;&#33267;&#21487;&#33021;&#20250;&#25439;&#23475;&#38271;&#24230;&#25512;&#24191;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer-based &#35821;&#35328;&#27169;&#22411;&#30340;&#24320;&#21457;&#20013;&#65292;&#38271;&#24230;&#25512;&#24191;&#26159;&#19968;&#20010;&#20851;&#38190;&#30340;&#25361;&#25112;&#65292;&#23427;&#26159;&#25351;&#20174;&#23567;&#30340;&#35757;&#32451;&#25991;&#26412;&#33539;&#22260;&#21040;&#26356;&#22823;&#33539;&#22260;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#20301;&#32622;&#32534;&#30721;&#65288;PE&#65289;&#34987;&#21457;&#29616;&#26159;&#24433;&#21709;&#38271;&#24230;&#25512;&#24191;&#30340;&#20027;&#35201;&#22240;&#32032;&#20043;&#19968;&#65292;&#20294;&#19981;&#21516;&#30340; PE &#26041;&#26696;&#23545;&#19979;&#28216;&#20219;&#21153;&#30340;&#22806;&#25512;&#24433;&#21709;&#36824;&#19981;&#28165;&#26970;&#12290;&#26412;&#25991;&#36890;&#36807;&#23545;&#27604;&#35780;&#20272;&#20116;&#31181;&#19981;&#21516;&#20301;&#32622;&#32534;&#30721;&#26041;&#27861;&#65288;&#21253;&#25324;&#32477;&#23545;&#20301;&#32622;&#23884;&#20837;&#12289;T5 &#30340;&#30456;&#23545; PE&#12289;ALiBi&#12289;Rotary &#21644;&#26080;&#20301;&#32622;&#32534;&#30721;&#65289;&#30340;&#35299;&#30721;&#22120; Transformer &#30340;&#38271;&#24230;&#25512;&#24191;&#33021;&#21147;&#65292;&#23545;&#25512;&#29702;&#21644;&#25968;&#23398;&#20219;&#21153;&#36827;&#34892;&#20102;&#31995;&#32479;&#30340;&#23454;&#35777;&#30740;&#31350;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#24120;&#29992;&#30340;&#20301;&#32622;&#32534;&#30721;&#26041;&#27861;&#65292;&#22914; ALiBi&#12289;Rotary &#21644; APE&#65292;&#24182;&#19981;&#36866;&#21512;&#29992;&#20110;&#19979;&#28216;&#20219;&#21153;&#30340;&#38271;&#24230;&#25512;&#24191;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#26080; PE &#30340; Transformer &#22312;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#26174;&#24335; PE &#26041;&#27861;&#65292;&#36825;&#24847;&#21619;&#30528;&#20351;&#29992;&#20301;&#32622;&#32534;&#30721;&#23454;&#38469;&#19978;&#21487;&#33021;&#20250;&#25439;&#23475;&#38271;&#24230;&#25512;&#24191;&#30340;&#33021;&#21147;&#12290;&#36825;&#20123;&#21457;&#29616;&#25581;&#31034;&#20102;&#20301;&#32622;&#32534;&#30721;&#22312;&#26377;&#25928; Transformer &#27169;&#22411;&#24320;&#21457;&#20013;&#30340;&#37325;&#35201;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Length generalization, the ability to generalize from small training context sizes to larger ones, is a critical challenge in the development of Transformer-based language models. Positional encoding (PE) has been identified as a major factor influencing length generalization, but the exact impact of different PE schemes on extrapolation in downstream tasks remains unclear. In this paper, we conduct a systematic empirical study comparing the length generalization performance of decoder-only Transformers with five different position encoding approaches including Absolute Position Embedding (APE), T5's Relative PE, ALiBi, and Rotary, in addition to Transformers without positional encoding (NoPE). Our evaluation encompasses a battery of reasoning and mathematical tasks. Our findings reveal that the most commonly used positional encoding methods, such as ALiBi, Rotary, and APE, are not well suited for length generalization in downstream tasks. More importantly, NoPE outperforms other expli
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;SLAC&#31070;&#32463;&#32593;&#32476;&#24211;(SNL)&#26694;&#26550;&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;FPGA&#30340;&#26426;&#22120;&#23398;&#20064;&#37096;&#32626;&#26694;&#26550;&#65292;&#20248;&#21270;&#24310;&#36831;&#65292;&#26088;&#22312;&#35299;&#20915;&#39640;&#36895;&#29575;&#25506;&#27979;&#22120;&#25968;&#25454;&#22788;&#29702;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2305.19455</link><description>&lt;p&gt;
&#22312;FPGA&#19978;&#37096;&#32626;AI&#25512;&#29702;&#24341;&#25806;&#30340;&#26694;&#26550;&#23454;&#29616;
&lt;/p&gt;
&lt;p&gt;
Implementation of a framework for deploying AI inference engines in FPGAs. (arXiv:2305.19455v1 [physics.ins-det])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19455
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;SLAC&#31070;&#32463;&#32593;&#32476;&#24211;(SNL)&#26694;&#26550;&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;FPGA&#30340;&#26426;&#22120;&#23398;&#20064;&#37096;&#32626;&#26694;&#26550;&#65292;&#20248;&#21270;&#24310;&#36831;&#65292;&#26088;&#22312;&#35299;&#20915;&#39640;&#36895;&#29575;&#25506;&#27979;&#22120;&#25968;&#25454;&#22788;&#29702;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
LCLS2&#33258;&#30001;&#30005;&#23376;&#28608;&#20809;&#22120;&#23558;&#20135;&#29983;&#39640;&#36798;1MHz&#30340;x&#23556;&#32447;&#33033;&#20914;&#65292;&#36825;&#23558;&#38656;&#35201;&#26032;&#30340;&#36229;&#39640;&#36895;&#29575;&#65288;UHR&#65289;&#25506;&#27979;&#22120;&#65292;&#20197;&#22312;&#39640;&#36798;100 kHz&#30340;&#36895;&#29575;&#19979;&#25805;&#20316;&#65292;&#24182;&#29983;&#25104;&#39640;&#36798;1 TB/s&#30340;&#25968;&#25454;&#21534;&#21520;&#37327;&#12290;&#26426;&#22120;&#23398;&#20064;&#24050;&#32463;&#23637;&#31034;&#20102;&#28040;&#21270;&#22823;&#22411;&#25968;&#25454;&#38598;&#20197;&#25552;&#21462;&#30456;&#20851;&#27934;&#35265;&#30340;&#28508;&#21147;&#65292;&#20294;&#26159;&#24403;&#21069;&#30340;&#23454;&#29616;&#26174;&#31034;&#20986;&#23454;&#26102;&#25968;&#25454;&#38477;&#20302;&#30446;&#26631;&#25152;&#38656;&#30340;&#24310;&#36831;&#22826;&#39640;&#12290;&#20026;&#27492;&#65292;SLAC&#33268;&#21147;&#20110;&#21019;&#24314;&#19968;&#20010;&#36719;&#20214;&#26694;&#26550;&#65292;&#23558;ML&#30340;&#32467;&#26500;&#32763;&#35793;&#20026;&#37096;&#32626;&#22312;&#25968;&#25454;&#38142;&#30340;&#36793;&#32536;&#65288;&#38752;&#36817;&#20202;&#22120;&#65289;&#30340;FPGA&#19978;&#12290;&#35813;&#26694;&#26550;&#21033;&#29992;&#20102;Xilinx&#30340;HLS&#26694;&#26550;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#27169;&#20223;&#24320;&#28304;Keras&#25509;&#21475;&#21040;TensorFlow&#24211;&#30340;API&#12290;&#36825;&#20010;SLAC&#31070;&#32463;&#32593;&#32476;&#24211;&#65288;SNL&#65289;&#26694;&#26550;&#35774;&#35745;&#20102;&#19968;&#20010;&#27969;&#24335;&#25968;&#25454;&#26041;&#27861;&#65292;&#20197;&#20248;&#21270;&#24310;&#36831;&#12290;
&lt;/p&gt;
&lt;p&gt;
The LCLS2 Free Electron Laser FEL will generate xray pulses to beamline experiments at up to 1Mhz These experimentals will require new ultrahigh rate UHR detectors that can operate at rates above 100 kHz and generate data throughputs upwards of 1 TBs a data velocity which requires prohibitively large investments in storage infrastructure Machine Learning has demonstrated the potential to digest large datasets to extract relevant insights however current implementations show latencies that are too high for realtime data reduction objectives SLAC has endeavored on the creation of a software framework which translates MLs structures for deployment on Field Programmable Gate Arrays FPGAs deployed at the Edge of the data chain close to the instrumentation This framework leverages Xilinxs HLS framework presenting an API modeled after the open source Keras interface to the TensorFlow library This SLAC Neural Network Library SNL framework is designed with a streaming data approach optimizing t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Channel-aware dynamic sparse (Chase)&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#31471;&#21040;&#31471;&#35757;&#32451;&#23454;&#29616;&#20102;GPU&#21451;&#22909;&#30340;&#36890;&#36947;&#32423;&#21035;&#31232;&#30095;&#65292;&#19981;&#38656;&#35201;&#20219;&#20309;&#29305;&#27530;&#25805;&#20316;&#65292;&#24182;&#19988;&#21487;&#20197;&#30452;&#25509;&#22312;&#36890;&#29992;&#30828;&#20214;&#19978;&#21152;&#36895;&#65292;&#26174;&#33879;&#20943;&#23567;&#27169;&#22411;&#22823;&#23567;&#65292;&#21516;&#26102;&#20445;&#25345;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.19454</link><description>&lt;p&gt;
&#21160;&#24577;&#31232;&#30095;&#26159;&#36890;&#36947;&#32423;&#31232;&#30095;&#30340;&#23398;&#20064;&#32773;
&lt;/p&gt;
&lt;p&gt;
Dynamic Sparsity Is Channel-Level Sparsity Learner. (arXiv:2305.19454v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19454
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Channel-aware dynamic sparse (Chase)&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#31471;&#21040;&#31471;&#35757;&#32451;&#23454;&#29616;&#20102;GPU&#21451;&#22909;&#30340;&#36890;&#36947;&#32423;&#21035;&#31232;&#30095;&#65292;&#19981;&#38656;&#35201;&#20219;&#20309;&#29305;&#27530;&#25805;&#20316;&#65292;&#24182;&#19988;&#21487;&#20197;&#30452;&#25509;&#22312;&#36890;&#29992;&#30828;&#20214;&#19978;&#21152;&#36895;&#65292;&#26174;&#33879;&#20943;&#23567;&#27169;&#22411;&#22823;&#23567;&#65292;&#21516;&#26102;&#20445;&#25345;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31232;&#30095;&#35757;&#32451;&#30001;&#20110;&#22312;&#25972;&#20010;&#35757;&#32451;&#36807;&#31243;&#21644;&#25512;&#29702;&#20013;&#20855;&#26377;&#35825;&#20154;&#30340;&#33410;&#30465;&#33021;&#21147;&#32780;&#21463;&#21040;&#26426;&#22120;&#23398;&#20064;&#30340;&#24191;&#27867;&#20851;&#27880;&#12290;&#21160;&#24577;&#31232;&#30095;&#35757;&#32451;(DST)&#20316;&#20026;&#19968;&#31181;&#39046;&#20808;&#30340;&#31232;&#30095;&#35757;&#32451;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#38646;&#24320;&#22987;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#20197;&#36798;&#21040;&#19982;&#23494;&#38598;&#23545;&#24212;&#29289;&#24615;&#33021;&#30456;&#21305;&#37197;&#30340;&#39640;&#31232;&#30095;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;DST&#20043;&#21069;&#30340;&#30740;&#31350;&#37117;&#34920;&#26126;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#26159;&#22312;&#39640;&#24230;&#19981;&#35268;&#21017;&#30340;&#31232;&#30095;&#27169;&#24335;&#19979;&#30340;&#38750;&#32467;&#26500;&#21270;&#31232;&#30095;&#24615;&#19978;&#65292;&#36825;&#22312;&#24120;&#35265;&#30828;&#20214;&#19978;&#24471;&#21040;&#20102;&#26377;&#38480;&#30340;&#25903;&#25345;&#12290;&#36825;&#31181;&#38480;&#21046;&#38459;&#30861;&#20102;DST&#22312;&#23454;&#36341;&#20013;&#30340;&#20351;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#36890;&#36947;&#24863;&#30693;&#21160;&#24577;&#31232;&#30095;&#65288;Chase&#65289;&#30340;&#26041;&#27861;&#65292;&#23427;&#23558;&#38750;&#32467;&#26500;&#21270;&#21160;&#24577;&#31232;&#30095;&#30340;&#24615;&#33021;&#36716;&#25442;&#20026;&#36866;&#21512;GPU&#21451;&#22909;&#30340;&#36890;&#36947;&#32423;&#21035;&#31232;&#30095;&#65292;&#22312;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#35757;&#32451;&#36807;&#31243;&#20013;&#23454;&#29616;&#65292;&#32780;&#19981;&#38656;&#35201;&#20219;&#20309;&#29305;&#27530;&#30340;&#25805;&#20316;&#12290;&#25152;&#24471;&#21040;&#30340;&#23567;&#22411;&#31232;&#30095;&#32593;&#32476;&#21487;&#20197;&#30452;&#25509;&#36890;&#36807;&#36890;&#29992;&#30828;&#20214;&#21152;&#36895;&#65292;&#32780;&#26080;&#38656;&#20351;&#29992;&#19987;&#29992;&#30340;&#31232;&#30095;&#30828;&#20214;&#21152;&#36895;&#22120;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;Chase&#21487;&#20197;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#19978;&#23454;&#29616;&#39640;&#36890;&#36947;&#32423;&#31232;&#30095;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#20854;&#24615;&#33021;&#65292;&#24182;&#26174;&#30528;&#20943;&#23567;&#27169;&#22411;&#30340;&#22823;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sparse training has received an upsurging interest in machine learning due to its tantalizing saving potential for the entire training process as well as inference. Dynamic sparse training (DST), as a leading sparse training approach, can train deep neural networks at high sparsity from scratch to match the performance of their dense counterparts. However, most if not all DST prior arts demonstrate their effectiveness on unstructured sparsity with highly irregular sparse patterns, which receives limited support in common hardware. This limitation hinders the usage of DST in practice. In this paper, we propose Channel-aware dynamic sparse (Chase), which for the first time seamlessly translates the promise of unstructured dynamic sparsity to GPU-friendly channel-level sparsity (not fine-grained N:M or group sparsity) during one end-to-end training process, without any ad-hoc operations. The resulting small sparse networks can be directly accelerated by commodity hardware, without using a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#22312;&#21482;&#33021;&#35775;&#38382;&#24207;&#25968;&#20559;&#22909;&#30340;&#24773;&#20917;&#19979;&#26368;&#22823;&#21270;&#22522;&#25968;&#20559;&#22909;&#30340;&#26041;&#27861;&#65292;&#20808;&#21069;&#30340;&#24037;&#20316;&#22312;&#20004;&#20010;&#22833;&#30495;&#24230;&#20013;&#30740;&#31350;&#20102;&#21151;&#21033;&#20027;&#20041;&#22833;&#30495;&#21644;&#24230;&#37327;&#22833;&#30495;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#28151;&#21512;&#22833;&#30495;&#24230;&#37327;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.19453</link><description>&lt;p&gt;
&#20004;&#20010;&#22833;&#30495;&#19990;&#30028;&#30340;&#26368;&#20248;&#25237;&#31080;&#35268;&#21017;
&lt;/p&gt;
&lt;p&gt;
Best of Both Distortion Worlds. (arXiv:2305.19453v1 [cs.GT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19453
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#22312;&#21482;&#33021;&#35775;&#38382;&#24207;&#25968;&#20559;&#22909;&#30340;&#24773;&#20917;&#19979;&#26368;&#22823;&#21270;&#22522;&#25968;&#20559;&#22909;&#30340;&#26041;&#27861;&#65292;&#20808;&#21069;&#30340;&#24037;&#20316;&#22312;&#20004;&#20010;&#22833;&#30495;&#24230;&#20013;&#30740;&#31350;&#20102;&#21151;&#21033;&#20027;&#20041;&#22833;&#30495;&#21644;&#24230;&#37327;&#22833;&#30495;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#28151;&#21512;&#22833;&#30495;&#24230;&#37327;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#35774;&#35745;&#25237;&#31080;&#35268;&#21017;&#30340;&#38382;&#39064;&#65292;&#35813;&#35268;&#21017;&#23558;$n$&#20010;&#20195;&#29702;&#20154;&#23545;$m$&#20010;&#22791;&#36873;&#26041;&#26696;&#30340;&#24207;&#21015;&#20559;&#22909;&#20316;&#20026;&#36755;&#20837;&#24182;&#36755;&#20986;&#21333;&#20010;&#26041;&#26696;&#65292;&#30446;&#30340;&#26159;&#20248;&#21270;&#20195;&#29702;&#20154;&#30340;&#25972;&#20307;&#24184;&#31119;&#24863;&#12290;&#25237;&#31080;&#35268;&#21017;&#30340;&#36755;&#20837;&#26159;&#27599;&#20010;&#20195;&#29702;&#20154;&#23545;&#22791;&#36873;&#26041;&#26696;&#30340;&#25490;&#21517;&#65292;&#20195;&#29702;&#20154;&#26377;&#26356;&#31934;&#32454;&#30340;(&#22522;&#25968;)&#20559;&#22909;&#65292;&#21487;&#20197;&#25429;&#25417;&#20182;&#20204;&#23545;&#26576;&#20010;&#22791;&#36873;&#26041;&#26696;&#30340;&#20559;&#22909;&#31243;&#24230;&#12290;&#20026;&#20102;&#37327;&#21270;&#25237;&#31080;&#35268;&#21017;&#22312;&#21482;&#33021;&#35775;&#38382;&#24207;&#25968;&#20559;&#22909;&#30340;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#26368;&#22823;&#21270;&#22522;&#25968;&#20559;&#22909;&#30340;&#31243;&#24230;&#65292;&#20808;&#21069;&#30340;&#24037;&#20316;&#20351;&#29992;&#22833;&#30495;&#24230;&#37327;&#65292;&#21363;&#25237;&#31080;&#35268;&#21017;&#24615;&#33021;&#19982;&#22522;&#25968;&#20559;&#22909;&#19979;&#30340;&#26368;&#20339;&#24615;&#33021;&#20043;&#38388;&#30340;&#26368;&#22351;&#36817;&#20284;&#27604;&#12290;&#25237;&#31080;&#35268;&#21017;&#30340;&#22833;&#30495;&#24230;&#26041;&#38754;&#30340;&#30740;&#31350;&#20027;&#35201;&#20998;&#20026;&#20004;&#20010;&#19990;&#30028;&#65306;&#21151;&#21033;&#20027;&#20041;&#22833;&#30495;&#21644;&#24230;&#37327;&#22833;&#30495;&#12290;&#22312;&#21069;&#32773;&#20013;&#65292;&#20195;&#29702;&#20154;&#30340;&#22522;&#25968;&#20559;&#22909;&#19982;&#19968;&#33324;&#25928;&#29992;&#30456;&#23545;&#24212;&#65292;
&lt;/p&gt;
&lt;p&gt;
We study the problem of designing voting rules that take as input the ordinal preferences of $n$ agents over a set of $m$ alternatives and output a single alternative, aiming to optimize the overall happiness of the agents. The input to the voting rule is each agent's ranking of the alternatives from most to least preferred, yet the agents have more refined (cardinal) preferences that capture the intensity with which they prefer one alternative over another. To quantify the extent to which voting rules can optimize over the cardinal preferences given access only to the ordinal ones, prior work has used the distortion measure, i.e., the worst-case approximation ratio between a voting rule's performance and the best performance achievable given the cardinal preferences.  The work on the distortion of voting rules has been largely divided into two worlds: utilitarian distortion and metric distortion. In the former, the cardinal preferences of the agents correspond to general utilities and
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;BBF&#22522;&#20110;&#20215;&#20540;&#20989;&#25968;&#30340;RL&#20195;&#29702;&#65292;&#22312;Atari 100K&#22522;&#20934;&#27979;&#35797;&#19978;&#23454;&#29616;&#36229;&#20154;&#31867;&#34920;&#29616;&#65292;&#20855;&#26377;&#20154;&#31867;&#25928;&#29575;&#65292;&#25552;&#20986;&#20102;&#22312;&#26679;&#26412;&#39640;&#25928;RL&#30740;&#31350;&#30340;ALE&#20013;&#26356;&#26032;&#30446;&#26631;&#30340;&#21487;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.19452</link><description>&lt;p&gt;
&#26356;&#22823;&#12289;&#26356;&#22909;&#12289;&#26356;&#24555;&#65306;&#20855;&#26377;&#20154;&#31867;&#25928;&#29575;&#30340;&#20154;&#31867;&#32423;Atari&#28216;&#25103;
&lt;/p&gt;
&lt;p&gt;
Bigger, Better, Faster: Human-level Atari with human-level efficiency. (arXiv:2305.19452v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19452
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;BBF&#22522;&#20110;&#20215;&#20540;&#20989;&#25968;&#30340;RL&#20195;&#29702;&#65292;&#22312;Atari 100K&#22522;&#20934;&#27979;&#35797;&#19978;&#23454;&#29616;&#36229;&#20154;&#31867;&#34920;&#29616;&#65292;&#20855;&#26377;&#20154;&#31867;&#25928;&#29575;&#65292;&#25552;&#20986;&#20102;&#22312;&#26679;&#26412;&#39640;&#25928;RL&#30740;&#31350;&#30340;ALE&#20013;&#26356;&#26032;&#30446;&#26631;&#30340;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;BBF&#30340;&#22522;&#20110;&#20215;&#20540;&#20989;&#25968;&#30340;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#26469;&#23454;&#29616;Atari 100K&#22522;&#20934;&#27979;&#35797;&#30340;&#36229;&#20154;&#31867;&#34920;&#29616;&#12290;BBF&#20381;&#38752;&#31070;&#32463;&#32593;&#32476;&#30340;&#20215;&#20540;&#20272;&#35745;&#25193;&#23637;&#20197;&#21450;&#20854;&#20182;&#35774;&#35745;&#36873;&#25321;&#65292;&#22312;&#36981;&#24490;&#26679;&#26412;&#39640;&#25928;&#30340;&#24773;&#20917;&#19979;&#65292;&#33021;&#22815;&#23454;&#29616;&#36825;&#31181;&#25193;&#23637;&#12290;&#25105;&#20204;&#23545;&#36825;&#20123;&#35774;&#35745;&#36873;&#25321;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#20998;&#26512;&#24182;&#20026;&#26410;&#26469;&#30340;&#24037;&#20316;&#25552;&#20379;&#20102;&#27934;&#35265;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#26356;&#26032;ALE&#19978;&#26679;&#26412;&#39640;&#25928;&#30340;RL&#30740;&#31350;&#30446;&#26631;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#20195;&#30721;&#21644;&#25968;&#25454;&#20844;&#24320;&#22312;https://github.com/google-research/google-research/tree/master/bigger_better_faster&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a value-based RL agent, which we call BBF, that achieves super-human performance in the Atari 100K benchmark. BBF relies on scaling the neural networks used for value estimation, as well as a number of other design choices that enable this scaling in a sample-efficient manner. We conduct extensive analyses of these design choices and provide insights for future work. We end with a discussion about updating the goalposts for sample-efficient RL research on the ALE. We make our code and data publicly available at https://github.com/google-research/google-research/tree/master/bigger_better_faster.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#21033;&#29992;&#31561;&#21516;&#19981;&#21516;&#35270;&#35282;&#30340;&#23398;&#20064;&#20449;&#21495;&#25903;&#25345;&#35270;&#35273;&#23398;&#20064;&#65292;&#24182;&#21457;&#29616;&#36890;&#36807;&#31561;&#21516;&#19968;&#20010;&#23545;&#35937;&#30340;&#19981;&#21516;&#35270;&#35282;&#23398;&#20064;&#24471;&#21040;&#30340;&#34920;&#31034;&#21487;&#20197;&#25552;&#39640;&#19979;&#28216;&#22270;&#20687;&#20998;&#31867;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2305.19445</link><description>&lt;p&gt;
&#26469;&#33258;&#33258;&#25105;&#26412;&#20307;&#23545;&#35937;&#28216;&#25103;&#30340;&#33258;&#30417;&#30563;&#35270;&#35273;&#23398;&#20064;&#30340;&#35745;&#31639;&#26426;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Computational Account Of Self-Supervised Visual Learning From Egocentric Object Play. (arXiv:2305.19445v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19445
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#21033;&#29992;&#31561;&#21516;&#19981;&#21516;&#35270;&#35282;&#30340;&#23398;&#20064;&#20449;&#21495;&#25903;&#25345;&#35270;&#35273;&#23398;&#20064;&#65292;&#24182;&#21457;&#29616;&#36890;&#36807;&#31561;&#21516;&#19968;&#20010;&#23545;&#35937;&#30340;&#19981;&#21516;&#35270;&#35282;&#23398;&#20064;&#24471;&#21040;&#30340;&#34920;&#31034;&#21487;&#20197;&#25552;&#39640;&#19979;&#28216;&#22270;&#20687;&#20998;&#31867;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20799;&#31461;&#21457;&#23637;&#30740;&#31350;&#34920;&#26126;&#65292;&#22788;&#29702;&#29289;&#29702;&#23545;&#35937;&#30340;&#20307;&#39564;&#23545;&#35768;&#22810;&#35748;&#30693;&#33021;&#21147;&#65292;&#21253;&#25324;&#35270;&#35273;&#23398;&#20064;&#65292;&#26377;&#30410;&#12290;&#36825;&#31181;&#20307;&#39564;&#30340;&#19968;&#20010;&#29305;&#24449;&#26159;&#23398;&#20064;&#32773;&#20174;&#20960;&#20010;&#19981;&#21516;&#30340;&#35270;&#35282;&#30475;&#21516;&#19968;&#23545;&#35937;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#21033;&#29992;&#31561;&#21516;&#19981;&#21516;&#35270;&#35282;&#30340;&#23398;&#20064;&#20449;&#21495; -- &#20363;&#22914;&#23558;&#21333;&#20010;&#23545;&#35937;&#30340;&#19981;&#21516;&#35270;&#22270;&#20998;&#37197;&#30456;&#20284;&#30340;&#34920;&#31034; -- &#25903;&#25345;&#24378;&#22823;&#30340;&#35270;&#35273;&#23398;&#20064;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;Toybox&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#20154;&#31867;&#25805;&#20316;&#19981;&#21516;&#23545;&#35937;&#30340;&#33258;&#25105;&#20013;&#24515;&#35270;&#39057;&#65292;&#24182;&#20351;&#29992;&#35745;&#31639;&#26426;&#35270;&#35273;&#26694;&#26550;&#36827;&#34892;&#33258;&#25105;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#23454;&#39564;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36890;&#36807;&#31561;&#21516;&#19968;&#20010;&#23545;&#35937;&#30340;&#19981;&#21516;&#29289;&#29702;&#35270;&#35282;&#23398;&#20064;&#24471;&#21040;&#30340;&#34920;&#31034;&#26377;&#21161;&#20110;&#19979;&#28216;&#22270;&#20687;&#20998;&#31867;&#31934;&#24230;&#30340;&#25552;&#39640;&#12290;&#36827;&#19968;&#27493;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#36825;&#31181;&#24615;&#33021;&#25552;&#39640;&#23545;&#35270;&#35282;&#20043;&#38388;&#30340;&#24046;&#24322;&#30340;&#21464;&#21270;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#24182;&#19988;&#36825;&#31181;&#30410;&#22788;&#33021;&#22815;&#36716;&#21270;&#21040;&#22810;&#31181;&#19981;&#21516;&#30340;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Research in child development has shown that embodied experience handling physical objects contributes to many cognitive abilities, including visual learning. One characteristic of such experience is that the learner sees the same object from several different viewpoints. In this paper, we study how learning signals that equate different viewpoints -- e.g., assigning similar representations to different views of a single object -- can support robust visual learning. We use the Toybox dataset, which contains egocentric videos of humans manipulating different objects, and conduct experiments using a computer vision framework for self-supervised contrastive learning. We find that representations learned by equating different physical viewpoints of an object benefit downstream image classification accuracy. Further experiments show that this performance improvement is robust to variations in the gaps between viewpoints, and that the benefits transfer to several different image classificati
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#36229;&#36710;&#22330;&#26223;&#25968;&#25454;&#38598;&#21644;&#30693;&#35782;&#34920;&#31034;&#27169;&#22411;&#65292;&#20197;&#24110;&#21161;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#36827;&#34892;&#35268;&#21010;&#21644;&#25511;&#21046;&#12290;</title><link>http://arxiv.org/abs/2305.19421</link><description>&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#20013;&#30340;&#36229;&#36710;&#22330;&#26223;&#25968;&#25454;&#21644;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
Data and Knowledge for Overtaking Scenarios in Autonomous Driving. (arXiv:2305.19421v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19421
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#36229;&#36710;&#22330;&#26223;&#25968;&#25454;&#38598;&#21644;&#30693;&#35782;&#34920;&#31034;&#27169;&#22411;&#65292;&#20197;&#24110;&#21161;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#36827;&#34892;&#35268;&#21010;&#21644;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#24050;&#25104;&#20026;&#20154;&#24037;&#26234;&#33021;&#20013;&#26368;&#27969;&#34892;&#30340;&#30740;&#31350;&#26041;&#21521;&#20043;&#19968;&#12290;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#34987;&#29702;&#35299;&#20026;&#19968;&#20010;&#31995;&#32479;&#65292;&#23427;&#32467;&#21512;&#20102;&#24863;&#30693;&#12289;&#20915;&#31574;&#21046;&#23450;&#12289;&#35268;&#21010;&#21644;&#25511;&#21046;&#31561;&#20219;&#21153;&#65292;&#25152;&#26377;&#36825;&#20123;&#20219;&#21153;&#37117;&#38656;&#35201;&#36710;&#36742;&#25910;&#38598;&#21608;&#22260;&#30340;&#25968;&#25454;&#65292;&#20197;&#20415;&#20570;&#20986;&#33391;&#22909;&#30340;&#20915;&#31574;&#21644;&#34892;&#21160;&#12290;&#26412;&#25991;&#30340;&#36129;&#29486;&#22312;&#20110;&#25552;&#20986;&#20102;&#19968;&#20010;&#19987;&#27880;&#20110;&#36229;&#36710;&#22330;&#26223;&#30340;&#26032;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#20197;&#21450;&#19968;&#20010;&#36229;&#36710;&#21160;&#20316;&#30340;&#30693;&#35782;&#34920;&#31034;&#27169;&#22411;&#65292;&#38500;&#20102;&#29992;&#20110;&#24863;&#30693;&#21644;&#20915;&#31574;&#21046;&#23450;&#20219;&#21153;&#20043;&#22806;&#65292;&#36824;&#21487;&#20197;&#29992;&#20110;&#35268;&#21010;&#21644;&#25511;&#21046;&#31639;&#27861;&#30340;&#24320;&#21457;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autonomous driving has become one of the most popular research topics within Artificial Intelligence. An autonomous vehicle is understood as a system that combines perception, decision-making, planning, and control. All of those tasks require that the vehicle collects surrounding data in order to make a good decision and action. In particular, the overtaking maneuver is one of the most critical actions of driving. The process involves lane changes, acceleration and deceleration actions, and estimation of the speed and distance of the vehicle in front or in the lane in which it is moving. Despite the amount of work available in the literature, just a few handle overtaking maneuvers and, because overtaking can be risky, no real-world dataset is available. This work contributes in this area by presenting a new synthetic dataset whose focus is the overtaking maneuver. We start by performing a thorough review of the state of the art in autonomous driving and then explore the main datasets f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25581;&#31034;&#20102;&#26032;&#20986;&#29616;&#30340;&#22823;&#26680;ConvNets&#26377;&#25928;&#22320;&#29992;&#20316;&#23567;&#26680;ConvNets&#30693;&#35782;&#33976;&#39311;&#65288;KD&#65289;&#30340;&#25945;&#24072;&#30340;&#20248;&#28857;&#12290;</title><link>http://arxiv.org/abs/2305.19412</link><description>&lt;p&gt;
&#22823;&#26680;&#19982;&#21464;&#24418;&#37329;&#21018;&#32593;&#32476;&#65292;&#21738;&#20010;&#26356;&#36866;&#21512;&#20316;&#20026;ConvNet&#30340;&#25945;&#24072;&#65311;
&lt;/p&gt;
&lt;p&gt;
Are Large Kernels Better Teachers than Transformers for ConvNets?. (arXiv:2305.19412v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19412
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25581;&#31034;&#20102;&#26032;&#20986;&#29616;&#30340;&#22823;&#26680;ConvNets&#26377;&#25928;&#22320;&#29992;&#20316;&#23567;&#26680;ConvNets&#30693;&#35782;&#33976;&#39311;&#65288;KD&#65289;&#30340;&#25945;&#24072;&#30340;&#20248;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25581;&#31034;&#20102;&#26032;&#20986;&#29616;&#30340;&#22823;&#26680;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;ConvNets&#65289;&#30340;&#19968;&#31181;&#26032;&#20248;&#28857;&#65306;&#20316;&#20026;&#23567;&#26680;ConvNets&#22312;&#30693;&#35782;&#33976;&#39311;&#65288;KD&#65289;&#20013;&#30340;&#25945;&#24072;&#12290;&#34429;&#28982;&#21464;&#24418;&#37329;&#21018;&#32593;&#32476;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#24615;&#33021;&#24050;&#32463;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#27700;&#24179;&#65292;&#20294;&#23567;&#26680;ConvNets&#30001;&#20110;&#21367;&#31215;&#36816;&#31639;&#21644;&#32039;&#20945;&#30340;&#26435;&#37325;&#20849;&#20139;&#65292;&#34987;&#35270;&#20026;&#26356;&#36866;&#21512;&#36164;&#28304;&#26377;&#38480;&#30340;&#24212;&#29992;&#12290;KD&#34987;&#24191;&#27867;&#29992;&#20110;&#25552;&#39640;&#23567;&#26680;ConvNets&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#20197;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20174;&#21464;&#24418;&#37329;&#21018;&#32593;&#32476;&#20013;&#33976;&#39311;&#30693;&#35782;&#65288;&#20363;&#22914;&#20840;&#23616;&#20449;&#24687;&#65289;&#21040;&#23567;&#26680;ConvNets&#24182;&#19981;&#26159;&#38750;&#24120;&#26377;&#25928;&#65292;&#21487;&#33021;&#26159;&#22240;&#20026;&#23427;&#20204;&#30340;&#19981;&#21516;&#20307;&#31995;&#32467;&#26500;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#29616;&#20195;&#22823;&#26680;ConvNets&#26159;&#23567;&#26680;ConvNets&#26356;&#20026;&#26377;&#25928;&#30340;&#25945;&#24072;&#65292;&#22240;&#20026;&#23427;&#20204;&#30340;&#20307;&#31995;&#32467;&#26500;&#26356;&#20026;&#30456;&#20284;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper reveals a new appeal of the recently emerged large-kernel Convolutional Neural Networks (ConvNets): as the teacher in Knowledge Distillation (KD) for small-kernel ConvNets. While Transformers have led state-of-the-art (SOTA) performance in various fields with ever-larger models and labeled data, small-kernel ConvNets are considered more suitable for resource-limited applications due to the efficient convolution operation and compact weight sharing. KD is widely used to boost the performance of small-kernel ConvNets. However, previous research shows that it is not quite effective to distill knowledge (e.g., global information) from Transformers to small-kernel ConvNets, presumably due to their disparate architectures. We hereby carry out a first-of-its-kind study unveiling that modern large-kernel ConvNets, a compelling competitor to Vision Transformers, are remarkably more effective teachers for small-kernel ConvNets, due to more similar architectures. Our findings are backe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;FRAMM&#65292;&#29992;&#20110;&#20844;&#24179;&#30340;&#20020;&#24202;&#35797;&#39564;&#36873;&#22336;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#35299;&#20915;&#25968;&#25454;&#32570;&#22833;&#21644;&#20248;&#21270;&#25307;&#21215;&#21644;&#22810;&#26679;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#22312;&#30495;&#23454;&#30340;&#20020;&#24202;&#35797;&#39564;&#25968;&#25454;&#38598;&#19978;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;FRAMM&#33021;&#22815;&#23454;&#29616;&#20844;&#24179;&#30340;&#35797;&#39564;&#36873;&#22336;&#65292;&#24182;&#22312;&#19981;&#38477;&#20302;&#25307;&#21215;&#29575;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#22810;&#26679;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.19407</link><description>&lt;p&gt;
FRAMM&#65306;&#38024;&#23545;&#20020;&#24202;&#35797;&#39564;&#25307;&#21215;&#19981;&#36275;&#30340;&#20844;&#24179;&#25490;&#21517;&#21644;&#32570;&#22833;&#25968;&#25454;&#30340;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
FRAMM: Fair Ranking with Missing Modalities for Clinical Trial Site Selection. (arXiv:2305.19407v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19407
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;FRAMM&#65292;&#29992;&#20110;&#20844;&#24179;&#30340;&#20020;&#24202;&#35797;&#39564;&#36873;&#22336;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#35299;&#20915;&#25968;&#25454;&#32570;&#22833;&#21644;&#20248;&#21270;&#25307;&#21215;&#21644;&#22810;&#26679;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#22312;&#30495;&#23454;&#30340;&#20020;&#24202;&#35797;&#39564;&#25968;&#25454;&#38598;&#19978;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;FRAMM&#33021;&#22815;&#23454;&#29616;&#20844;&#24179;&#30340;&#35797;&#39564;&#36873;&#22336;&#65292;&#24182;&#22312;&#19981;&#38477;&#20302;&#25307;&#21215;&#29575;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#35768;&#22810;&#21162;&#21147;&#24050;&#32463;&#20570;&#20986;&#26469;&#65292;&#20294;&#24615;&#21035;&#12289;&#31181;&#26063;&#21644;&#23569;&#25968;&#27665;&#26063;&#22312;&#20020;&#24202;&#35797;&#39564;&#20013;&#30340;&#20195;&#34920;&#24615;&#19981;&#36275;&#20173;&#28982;&#26159;&#19968;&#20010;&#38382;&#39064;&#65292;&#24182;&#19988;&#21066;&#24369;&#20102;&#23569;&#25968;&#27665;&#26063;&#30340;&#27835;&#30103;&#25928;&#26524;&#12290;&#26412;&#25991;&#38024;&#23545;&#35797;&#39564;&#36873;&#22336;&#20219;&#21153;&#25552;&#20986;FRAMM&#65292;&#36825;&#26159;&#19968;&#20010;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#20844;&#24179;&#30340;&#35797;&#39564;&#36873;&#22336;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#35299;&#20915;&#24433;&#21709;&#20844;&#24179;&#35797;&#39564;&#36873;&#22336;&#30340;&#20004;&#20010;&#29616;&#23454;&#25361;&#25112;&#65306;&#35768;&#22810;&#28508;&#22312;&#35797;&#39564;&#22330;&#22320;&#30340;&#25968;&#25454;&#27169;&#24335;&#32463;&#24120;&#19981;&#23436;&#25972;&#65292;&#32780;&#19988;&#35797;&#39564;&#36873;&#22336;&#38656;&#35201;&#21516;&#26102;&#20248;&#21270;&#25307;&#21215;&#21644;&#22810;&#26679;&#24615;&#65292;&#22240;&#20026;&#38382;&#39064;&#24517;&#28982;&#26159;&#20108;&#32773;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#21807;&#19968;&#21487;&#33021;&#36890;&#36807;&#38480;&#21046;&#25307;&#21215;&#25968;&#37327;&#26469;&#22686;&#21152;&#22810;&#26679;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#32570;&#22833;&#25968;&#25454;&#30340;&#25361;&#25112;&#65292;FRAMM&#20855;&#26377;&#19968;&#20010;&#27169;&#24577;&#32534;&#30721;&#22120;&#21644;&#19968;&#20010;&#25513;&#30721;&#20132;&#21449;&#20851;&#27880;&#26426;&#21046;&#65292;&#21487;&#22788;&#29702;&#32570;&#22833;&#25968;&#25454;&#65292;&#26080;&#38656;&#36827;&#34892;&#25968;&#25454;&#22635;&#20805;&#21644;&#23436;&#25972;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#12290;&#20026;&#20102;&#22788;&#29702;&#20248;&#21270;&#25307;&#21215;&#21644;&#22810;&#26679;&#24615;&#30340;&#38656;&#27714;&#65292;FRAMM&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20844;&#24179;&#24230;&#37327;&#21644;&#21487;&#35843;&#33410;&#30340;&#38480;&#21046;&#25307;&#21215;&#26426;&#21046;&#65292;&#20197;&#36827;&#34892;&#26435;&#34913;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#30495;&#23454;&#30340;&#20020;&#24202;&#35797;&#39564;&#25968;&#25454;&#38598;&#19978;&#65292;FRAMM&#33021;&#22815;&#26377;&#25928;&#23454;&#29616;&#20844;&#24179;&#30340;&#35797;&#39564;&#36873;&#22336;&#65292;&#24182;&#22312;&#19981;&#29306;&#29298;&#20837;&#36873;&#29575;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite many efforts to address the disparities, the underrepresentation of gender, racial, and ethnic minorities in clinical trials remains a problem and undermines the efficacy of treatments on minorities. This paper focuses on the trial site selection task and proposes FRAMM, a deep reinforcement learning framework for fair trial site selection. We focus on addressing two real-world challenges that affect fair trial sites selection: the data modalities are often not complete for many potential trial sites, and the site selection needs to simultaneously optimize for both enrollment and diversity since the problem is necessarily a trade-off between the two with the only possible way to increase diversity post-selection being through limiting enrollment via caps. To address the missing data challenge, FRAMM has a modality encoder with a masked cross-attention mechanism for handling missing data, bypassing data imputation and the need for complete data in training. To handle the need fo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#8220;&#20998;&#27495;&#24863;&#30693;&#8221;&#30340;&#21452;&#27969;&#22686;&#37327;&#23398;&#20064;&#26694;&#26550;&#65292;&#36866;&#29992;&#20110;&#38024;&#23545;&#19981;&#26029;&#28436;&#21270;&#30340;&#30446;&#26631;&#22495;&#25968;&#25454;&#36827;&#34892;&#20998;&#21106;&#20219;&#21153;&#65292;&#35299;&#20915;&#20102;&#20998;&#24067;&#21464;&#21270;&#12289;&#26410;&#35265;&#36807;&#32467;&#26500;&#21644;&#35757;&#32451;&#25968;&#25454;&#32570;&#22833;&#31561;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2305.19404</link><description>&lt;p&gt;
&#33041;&#37096;&#32959;&#30244;MRI&#24322;&#36136;&#32467;&#26500;&#20998;&#21106;&#30340;&#22686;&#37327;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Incremental Learning for Heterogeneous Structure Segmentation in Brain Tumor MRI. (arXiv:2305.19404v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19404
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#8220;&#20998;&#27495;&#24863;&#30693;&#8221;&#30340;&#21452;&#27969;&#22686;&#37327;&#23398;&#20064;&#26694;&#26550;&#65292;&#36866;&#29992;&#20110;&#38024;&#23545;&#19981;&#26029;&#28436;&#21270;&#30340;&#30446;&#26631;&#22495;&#25968;&#25454;&#36827;&#34892;&#20998;&#21106;&#20219;&#21153;&#65292;&#35299;&#20915;&#20102;&#20998;&#24067;&#21464;&#21270;&#12289;&#26410;&#35265;&#36807;&#32467;&#26500;&#21644;&#35757;&#32451;&#25968;&#25454;&#32570;&#22833;&#31561;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21333;&#28304;&#22495;&#35757;&#32451;&#30340;&#38745;&#24577;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#22312;&#19981;&#21516;&#30340;&#35299;&#21078;&#32467;&#26500;&#20998;&#21106;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#12290;&#20294;&#26159;&#65292;&#22312;&#19981;&#26029;&#28436;&#21270;&#30340;&#29615;&#22659;&#20013;&#65292;&#38745;&#24577;&#27169;&#22411;&#30340;&#34920;&#29616;&#21487;&#33021;&#20250;&#21464;&#24046;&#65292;&#38656;&#35201;&#36866;&#24403;&#22320;&#26356;&#26032;&#27169;&#22411;&#12290;&#22312;&#22686;&#37327;&#23398;&#20064;&#29615;&#22659;&#19979;&#65292;&#25105;&#20204;&#24076;&#26395;&#33021;&#22815;&#26356;&#26032;&#22909;&#30340;&#38745;&#24577;&#27169;&#22411;&#65292;&#36319;&#38543;&#19981;&#26029;&#28436;&#21270;&#30340;&#30446;&#26631;&#22495;&#25968;&#25454;&#65288;&#20363;&#22914;&#65292;&#26469;&#33258;&#19981;&#21516;&#31449;&#28857;&#30340;&#39069;&#22806;&#25439;&#20260;&#25110;&#24863;&#20852;&#36259;&#32467;&#26500;&#65289;&#65292;&#32780;&#19981;&#20250;&#21457;&#29983;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#28982;&#32780;&#65292;&#36825;&#20063;&#24102;&#26469;&#20102;&#19968;&#20123;&#25361;&#25112;&#65306;&#20998;&#24067;&#30340;&#21464;&#21270;&#12289;&#22312;&#21021;&#22987;&#27169;&#22411;&#35757;&#32451;&#26399;&#38388;&#26410;&#35265;&#36807;&#30340;&#20854;&#20182;&#32467;&#26500;&#20197;&#21450;&#28304;&#22495;&#20013;&#32570;&#20047;&#35757;&#32451;&#25968;&#25454;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#25991;&#33268;&#21147;&#20110;&#20197;&#32479;&#19968;&#30340;&#26041;&#24335;&#36880;&#27493;&#28436;&#21270;&#8220;&#21363;&#25554;&#21363;&#29992;&#8221;&#35757;&#32451;&#30340;&#20998;&#21106;&#27169;&#22411;&#65292;&#20197;&#24212;&#23545;&#19981;&#26029;&#22686;&#21152;&#30340;&#35299;&#21078;&#20998;&#31867;&#25968;&#25454;&#38598;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#31181;&#8220;&#20998;&#27495;&#24863;&#30693;&#8221;&#30340;&#21452;&#27969;&#22686;&#37327;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Deep learning (DL) models for segmenting various anatomical structures have achieved great success via a static DL model that is trained in a single source domain. Yet, the static DL model is likely to perform poorly in a continually evolving environment, requiring appropriate model updates. In an incremental learning setting, we would expect that well-trained static models are updated, following continually evolving target domain data -- e.g., additional lesions or structures of interest -- collected from different sites, without catastrophic forgetting. This, however, poses challenges, due to distribution shifts, additional structures not seen during the initial model training, and the absence of training data in a source domain. To address these challenges, in this work, we seek to progressively evolve an ``off-the-shelf" trained segmentation model to diverse datasets with additional anatomical categories in a unified manner. Specifically, we first propose a divergence-aware dual-fl
&lt;/p&gt;</description></item><item><title>&#19978;&#19979;&#25991;&#35270;&#35273;&#21464;&#25442;&#22120;(ContextViT)&#29992;&#20110;&#29983;&#25104;&#22270;&#20687;&#30340;&#40065;&#26834;&#29305;&#24449;&#34920;&#31034;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#39069;&#22806;&#30340;&#19978;&#19979;&#25991;&#20196;&#29260;&#65292;&#21487;&#20197;&#35299;&#37322;&#25481;&#29305;&#23450;&#20110;&#32452;&#30340;&#21327;&#21464;&#37327;&#32467;&#26500;&#65292;&#21516;&#26102;&#20445;&#25345;&#36328;&#32452;&#20849;&#20139;&#30340;&#26680;&#24515;&#35270;&#35273;&#29305;&#24449;&#65292;&#33021;&#22815;&#22312;&#30417;&#30563;&#24494;&#35843;&#12289;&#21322;&#30417;&#30563;&#23398;&#20064;&#20197;&#21450;&#20027;&#21160;&#23398;&#20064;&#31561;&#26041;&#38754;&#24471;&#21040;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2305.19402</link><description>&lt;p&gt;
&#19978;&#19979;&#25991;&#35270;&#35273;&#21464;&#25442;&#22120;&#29992;&#20110;&#24378;&#20581;&#30340;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Contextual Vision Transformers for Robust Representation Learning. (arXiv:2305.19402v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19402
&lt;/p&gt;
&lt;p&gt;
&#19978;&#19979;&#25991;&#35270;&#35273;&#21464;&#25442;&#22120;(ContextViT)&#29992;&#20110;&#29983;&#25104;&#22270;&#20687;&#30340;&#40065;&#26834;&#29305;&#24449;&#34920;&#31034;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#39069;&#22806;&#30340;&#19978;&#19979;&#25991;&#20196;&#29260;&#65292;&#21487;&#20197;&#35299;&#37322;&#25481;&#29305;&#23450;&#20110;&#32452;&#30340;&#21327;&#21464;&#37327;&#32467;&#26500;&#65292;&#21516;&#26102;&#20445;&#25345;&#36328;&#32452;&#20849;&#20139;&#30340;&#26680;&#24515;&#35270;&#35273;&#29305;&#24449;&#65292;&#33021;&#22815;&#22312;&#30417;&#30563;&#24494;&#35843;&#12289;&#21322;&#30417;&#30563;&#23398;&#20064;&#20197;&#21450;&#20027;&#21160;&#23398;&#20064;&#31561;&#26041;&#38754;&#24471;&#21040;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#19978;&#19979;&#25991;&#35270;&#35273;&#21464;&#25442;&#22120;(ContextViT)&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#22270;&#20687;&#30340;&#40065;&#26834;&#29305;&#24449;&#34920;&#31034;&#65292;&#29305;&#21035;&#26159;&#38024;&#23545;&#34920;&#29616;&#20986;&#20998;&#32452;&#32467;&#26500;&#30340;&#22270;&#20687;&#65292;&#22914;&#21327;&#21464;&#37327;&#12290;ContextViT&#24341;&#20837;&#20102;&#19968;&#20010;&#39069;&#22806;&#30340;&#19978;&#19979;&#25991;&#20196;&#29260;&#26469;&#32534;&#30721;&#29305;&#23450;&#20110;&#32452;&#30340;&#20449;&#24687;&#65292;&#20801;&#35768;&#27169;&#22411;&#35299;&#37322;&#25481;&#29305;&#23450;&#20110;&#32452;&#30340;&#21327;&#21464;&#37327;&#32467;&#26500;&#65292;&#21516;&#26102;&#20445;&#25345;&#36328;&#32452;&#20849;&#20139;&#30340;&#26680;&#24515;&#35270;&#35273;&#29305;&#24449;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22312;&#32473;&#23450;&#36755;&#20837;&#22270;&#20687;&#30340;&#24773;&#20917;&#19979;&#65292;Context-ViT&#23558;&#20849;&#20139;&#30456;&#21516;&#21327;&#21464;&#37327;&#30340;&#22270;&#20687;&#26144;&#23556;&#21040;&#35813;&#19978;&#19979;&#25991;&#20196;&#29260;&#65292;&#24182;&#28155;&#21152;&#21040;&#36755;&#20837;&#22270;&#20687;&#20196;&#29260;&#20013;&#65292;&#20197;&#25429;&#33719;&#23558;&#27169;&#22411;&#26465;&#20214;&#21270;&#20026;&#32452;&#25104;&#21592;&#36523;&#20221;&#30340;&#25928;&#26524;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#20010;&#19978;&#19979;&#25991;&#25512;&#26029;&#32593;&#32476;&#65292;&#21487;&#20197;&#22312;&#36816;&#34892;&#26102;&#39044;&#27979;&#36825;&#20123;&#20196;&#29260;&#65292;&#21482;&#38656;&#35201;&#32473;&#20986;&#19968;&#20123;&#26469;&#33258;&#32452;&#20998;&#24067;&#30340;&#26679;&#26412;&#21363;&#21487;&#65292;&#20351;&#24471;ContextViT&#21487;&#20197;&#25512;&#24191;&#21040;&#26032;&#30340;&#27979;&#35797;&#20998;&#24067;&#12290;&#25105;&#20204;&#36890;&#36807;&#21508;&#31181;&#24212;&#29992;&#31243;&#24207;&#35828;&#26126;&#20102;ContextViT&#30340;&#24615;&#33021;&#12290;&#22312;&#30417;&#30563;&#24494;&#35843;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#23558;&#39044;&#35757;&#32451;&#30340;ViTs&#19982;&#39069;&#22806;&#30340;&#19978;&#19979;&#25991;&#20196;&#29260;&#30456;&#32467;&#21512;&#21487;&#20197;&#25552;&#39640;&#22270;&#20687;&#20998;&#31867;&#22522;&#20934;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;ContextViT&#21487;&#20197;&#29992;&#20110;&#21322;&#30417;&#30563;&#23398;&#20064;&#65292;&#22312;CIFAR-10&#25968;&#25454;&#38598;&#19978;&#20165;&#20351;&#29992;10%&#30340;&#26631;&#35760;&#26679;&#26412;&#21363;&#21487;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#34920;&#29616;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;ContextViT&#21487;&#20197;&#36890;&#36807;&#24341;&#23548;&#36873;&#25321;&#26469;&#33258;&#23569;&#25968;&#32676;&#20307;&#30340;&#26356;&#20855;&#20449;&#24687;&#20215;&#20540;&#30340;&#26679;&#26412;&#65292;&#20174;&#32780;&#25552;&#39640;&#20027;&#21160;&#23398;&#20064;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Contextual Vision Transformers (ContextViT), a method for producing robust feature representations for images exhibiting grouped structure such as covariates. ContextViT introduces an extra context token to encode group-specific information, allowing the model to explain away group-specific covariate structures while keeping core visual features shared across groups. Specifically, given an input image, Context-ViT maps images that share the same covariate into this context token appended to the input image tokens to capture the effects of conditioning the model on group membership. We furthermore introduce a context inference network to predict such tokens on the fly given a few samples from a group distribution, enabling ContextViT to generalize to new testing distributions at inference time. We illustrate the performance of ContextViT through a diverse range of applications. In supervised fine-tuning, we demonstrate that augmenting pre-trained ViTs with additional context 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;LOPO&#38382;&#39064;&#65292;&#36890;&#36807;&#35843;&#25972;&#36317;&#31163;&#21152;&#26435;&#21644;&#24341;&#36827;&#22522;&#20110;&#29305;&#24449;&#30340;&#37325;&#35201;&#24615;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#22312;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#25512;&#24191;&#33021;&#21147;&#26041;&#38754;&#27604;RF + clust&#26356;&#20248;&#12290;</title><link>http://arxiv.org/abs/2305.19375</link><description>&lt;p&gt;
LOPO&#24615;&#33021;&#39044;&#27979;&#30340;RF + clust&#28789;&#25935;&#24230;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Sensitivity Analysis of RF+clust for Leave-one-problem-out Performance Prediction. (arXiv:2305.19375v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19375
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;LOPO&#38382;&#39064;&#65292;&#36890;&#36807;&#35843;&#25972;&#36317;&#31163;&#21152;&#26435;&#21644;&#24341;&#36827;&#22522;&#20110;&#29305;&#24449;&#30340;&#37325;&#35201;&#24615;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#22312;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#25512;&#24191;&#33021;&#21147;&#26041;&#38754;&#27604;RF + clust&#26356;&#20248;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Leave-one-problem-out&#65288;LOPO&#65289;&#24615;&#33021;&#39044;&#27979;&#38656;&#35201;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#23558;&#31639;&#27861;&#30340;&#24615;&#33021;&#20174;&#19968;&#32452;&#35757;&#32451;&#38382;&#39064;&#25512;&#24191;&#21040;&#20043;&#21069;&#26410;&#35265;&#36807;&#30340;&#38382;&#39064;&#19978;&#12290;&#21363;&#20351;&#23545;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;LOPO&#20063;&#26159;&#19968;&#39033;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#22312;&#26356;&#31616;&#21333;&#30340;leave-one-instance-out&#22330;&#26223;&#20013;&#34920;&#29616;&#33391;&#22909;&#30340;&#27169;&#22411;&#36890;&#24120;&#26410;&#33021;&#24456;&#22909;&#22320;&#25512;&#24191;&#21040;LOPO&#35774;&#32622;&#20013;&#12290;&#20026;&#20102;&#35299;&#20915;LOPO&#38382;&#39064;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#24314;&#35758;&#20351;&#29992;&#21152;&#26435;&#31639;&#27861;&#24615;&#33021;&#30340;&#38543;&#26426;&#26862;&#26519;&#65288;RF&#65289;&#24615;&#33021;&#22238;&#24402;&#27169;&#22411;&#23545;&#26631;&#20934;&#27169;&#22411;&#36827;&#34892;&#25193;&#23637;&#65292;&#36825;&#20123;&#31639;&#27861;&#24615;&#33021;&#34987;&#35748;&#20026;&#19982;&#27979;&#35797;&#38382;&#39064;&#30456;&#20284;&#12290;&#26356;&#20934;&#30830;&#22320;&#35828;&#65292;&#22312;&#36825;&#20010;RF + clust&#26041;&#27861;&#20013;&#65292;&#26435;&#37325;&#26159;&#26681;&#25454;&#26576;&#20123;&#29305;&#24449;&#31354;&#38388;&#20013;&#38382;&#39064;&#30340;&#36317;&#31163;&#25104;&#27604;&#20363;&#36873;&#25321;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#35843;&#25972;&#22522;&#20110;&#29305;&#24449;&#30340;&#37325;&#35201;&#24615;&#23545;&#24615;&#33021;&#22238;&#24402;&#30340;&#36317;&#31163;&#21152;&#26435;&#26469;&#25193;&#23637;RF + clust&#26041;&#27861;&#12290;&#20063;&#23601;&#26159;&#35828;&#65292;&#25105;&#20204;&#19981;&#20877;&#32771;&#34385;&#29305;&#24449;&#31354;&#38388;&#20013;&#30340;&#20313;&#24358;&#36317;&#31163;&#65292;&#32780;&#26159;&#32771;&#34385;&#21152;&#26435;&#20313;&#24358;&#36317;&#31163;&#65292;&#20854;&#20013;&#26435;&#37325;&#26159;&#22522;&#20110;RF&#20013;&#30340;&#29305;&#24449;&#37325;&#35201;&#24615;&#24230;&#37327;&#33258;&#21160;&#35745;&#31639;&#30340;&#12290;&#25105;&#20204;&#22312;&#19981;&#21516;&#39046;&#22495;&#30340;23&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#20197;&#27604;&#36739;RF + clust&#19982;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#23545;&#26032;&#38382;&#39064;&#30340;&#25512;&#24191;&#33021;&#21147;&#26041;&#38754;&#27604;RF + clust&#26174;&#30528;&#26356;&#20248;&#65292;&#29305;&#21035;&#26159;&#24403;&#35757;&#32451;&#38382;&#39064;&#30340;&#25968;&#37327;&#36739;&#23569;&#26102;&#12290;
&lt;/p&gt;
&lt;p&gt;
Leave-one-problem-out (LOPO) performance prediction requires machine learning (ML) models to extrapolate algorithms' performance from a set of training problems to a previously unseen problem. LOPO is a very challenging task even for state-of-the-art approaches. Models that work well in the easier leave-one-instance-out scenario often fail to generalize well to the LOPO setting. To address the LOPO problem, recent work suggested enriching standard random forest (RF) performance regression models with a weighted average of algorithms' performance on training problems that are considered similar to a test problem. More precisely, in this RF+clust approach, the weights are chosen proportionally to the distances of the problems in some feature space. Here in this work, we extend the RF+clust approach by adjusting the distance-based weights with the importance of the features for performance regression. That is, instead of considering cosine distance in the feature space, we consider a weig
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20154;&#31867;&#22914;&#20309;&#21033;&#29992;&#32452;&#21512;&#24615;&#36827;&#34892;&#35270;&#35273;&#27010;&#24565;&#23398;&#20064;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#31243;&#24207;&#24402;&#32435;&#27169;&#22411;&#26469;&#29983;&#25104;&#20505;&#36873;&#35270;&#35273;&#22270;&#24418;&#65292;&#21457;&#29616;&#20154;&#31867;&#21644;&#27169;&#22411;&#37117;&#21487;&#20197;&#36827;&#34892;&#22810;&#26679;&#30340;&#32452;&#21512;&#27867;&#21270;&#12290;</title><link>http://arxiv.org/abs/2305.19374</link><description>&lt;p&gt;
&#35270;&#35273;&#27010;&#24565;&#23398;&#20064;&#20013;&#30340;&#32452;&#21512;&#22810;&#26679;&#24615;
&lt;/p&gt;
&lt;p&gt;
Compositional diversity in visual concept learning. (arXiv:2305.19374v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19374
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20154;&#31867;&#22914;&#20309;&#21033;&#29992;&#32452;&#21512;&#24615;&#36827;&#34892;&#35270;&#35273;&#27010;&#24565;&#23398;&#20064;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#31243;&#24207;&#24402;&#32435;&#27169;&#22411;&#26469;&#29983;&#25104;&#20505;&#36873;&#35270;&#35273;&#22270;&#24418;&#65292;&#21457;&#29616;&#20154;&#31867;&#21644;&#27169;&#22411;&#37117;&#21487;&#20197;&#36827;&#34892;&#22810;&#26679;&#30340;&#32452;&#21512;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#21033;&#29992;&#32452;&#21512;&#24615;&#26469;&#26377;&#25928;&#22320;&#23398;&#20064;&#26032;&#27010;&#24565;&#65292;&#29702;&#35299;&#29087;&#24713;&#37096;&#20214;&#22914;&#20309;&#32452;&#21512;&#22312;&#19968;&#36215;&#24418;&#25104;&#26032;&#39062;&#23545;&#35937;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#27969;&#34892;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#27169;&#22411;&#38590;&#20197;&#36827;&#34892;&#30456;&#21516;&#31867;&#22411;&#30340;&#25512;&#29702;&#65292;&#38656;&#35201;&#26356;&#22810;&#25968;&#25454;&#65292;&#24182;&#19988;&#19981;&#22914;&#20154;&#31867;&#20855;&#26377;&#28789;&#27963;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#26412;&#25991;&#30740;&#31350;&#36825;&#20123;&#20154;&#31867;&#29305;&#26377;&#30340;&#33021;&#21147;&#22312;&#19981;&#21516;&#31867;&#22411;&#30340;&#35270;&#35273;&#32452;&#21512;&#20013;&#30340;&#34920;&#29616;&#65292;&#30740;&#31350;&#20154;&#31867;&#22914;&#20309;&#23545;&#20855;&#26377;&#20016;&#23500;&#20851;&#31995;&#32467;&#26500;&#30340;&#8220;&#22806;&#26143;&#20154;&#22270;&#24418;&#8221;&#36827;&#34892;&#20998;&#31867;&#21644;&#29983;&#25104;&#12290;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#20010;&#36125;&#21494;&#26031;&#31243;&#24207;&#24402;&#32435;&#27169;&#22411;&#65292;&#25628;&#32034;&#29983;&#25104;&#20505;&#36873;&#35270;&#35273;&#22270;&#24418;&#30340;&#26368;&#20339;&#31243;&#24207;&#65292;&#21033;&#29992;&#21253;&#21547;&#19981;&#21516;&#32452;&#21512;&#26426;&#21046;&#21644;&#25277;&#35937;&#30340;&#22823;&#22411;&#31243;&#24207;&#31354;&#38388;&#12290;&#22312;&#23569;&#26679;&#26412;&#20998;&#31867;&#20219;&#21153;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#20154;&#31867;&#21644;&#31243;&#24207;&#24402;&#32435;&#27169;&#22411;&#21487;&#20197;&#36827;&#34892;&#35768;&#22810;&#26377;&#24847;&#20041;&#30340;&#32452;&#21512;&#27867;&#21270;&#65292;&#27169;&#22411;&#25552;&#20379;&#20102;&#23454;&#39564;&#25968;&#25454;&#30340;&#24378;&#26377;&#21147;&#35299;&#37322;&#20197;&#21450;&#26174;&#31034;&#20154;&#31867;&#24335;&#32452;&#21512;&#25512;&#29702;&#30340;&#21487;&#35299;&#37322;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Humans leverage compositionality to efficiently learn new concepts, understanding how familiar parts can combine together to form novel objects. In contrast, popular computer vision models struggle to make the same types of inferences, requiring more data and generalizing less flexibly than people do. Here, we study these distinctively human abilities across a range of different types of visual composition, examining how people classify and generate ``alien figures'' with rich relational structure. We also develop a Bayesian program induction model which searches for the best programs for generating the candidate visual figures, utilizing a large program space containing different compositional mechanisms and abstractions. In few shot classification tasks, we find that people and the program induction model can make a range of meaningful compositional generalizations, with the model providing a strong account of the experimental data as well as interpretable parameters that reveal huma
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37319;&#29992;&#20027;&#39064;&#24314;&#27169;&#25216;&#26415;&#23545;1200&#20363;&#24515;&#21147;&#34928;&#31469;&#24739;&#32773;&#30340;&#35786;&#26029;&#32534;&#30721;&#21644;&#31243;&#24207;&#25253;&#21578;&#20013;&#23384;&#22312;&#30340;&#20027;&#39064;&#36827;&#34892;&#35782;&#21035;&#65292;&#26088;&#22312;&#20174;&#20013;&#35782;&#21035;&#24515;&#21147;&#34928;&#31469;&#30340;&#20020;&#24202;&#34920;&#22411;&#24182;&#39044;&#27979;&#30149;&#20154;&#20303;&#38498;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2305.19373</link><description>&lt;p&gt;
&#36890;&#36807;&#25366;&#25496;&#20020;&#24202;&#31508;&#35760;&#20013;&#30340;&#20027;&#39064;&#35782;&#21035;&#24515;&#21147;&#34928;&#31469;&#24739;&#32773;&#30340;&#34920;&#22411;&#24182;&#39044;&#27979;&#20303;&#38498;&#26102;&#38388;
&lt;/p&gt;
&lt;p&gt;
Mining Themes in Clinical Notes to Identify Phenotypes and to Predict Length of Stay in Patients admitted with Heart Failure. (arXiv:2305.19373v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19373
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37319;&#29992;&#20027;&#39064;&#24314;&#27169;&#25216;&#26415;&#23545;1200&#20363;&#24515;&#21147;&#34928;&#31469;&#24739;&#32773;&#30340;&#35786;&#26029;&#32534;&#30721;&#21644;&#31243;&#24207;&#25253;&#21578;&#20013;&#23384;&#22312;&#30340;&#20027;&#39064;&#36827;&#34892;&#35782;&#21035;&#65292;&#26088;&#22312;&#20174;&#20013;&#35782;&#21035;&#24515;&#21147;&#34928;&#31469;&#30340;&#20020;&#24202;&#34920;&#22411;&#24182;&#39044;&#27979;&#30149;&#20154;&#20303;&#38498;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24515;&#21147;&#34928;&#31469;&#26159;&#19968;&#31181;&#32508;&#21512;&#24449;&#65292;&#24403;&#24515;&#33039;&#26080;&#27861;&#27893;&#34880;&#21644;&#36755;&#36865;&#27687;&#27668;&#20197;&#25903;&#25345;&#20307;&#20869;&#20854;&#20182;&#22120;&#23448;&#26102;&#20986;&#29616;&#12290;&#35782;&#21035;&#25509;&#21463;&#24515;&#21147;&#34928;&#31469;&#27835;&#30103;&#30340;&#30149;&#20154;&#30340;&#35786;&#26029;&#32534;&#30721;&#21644;&#31243;&#24207;&#25253;&#21578;&#20013;&#30340;&#28508;&#22312;&#20027;&#39064;&#65292;&#21487;&#20197;&#25581;&#31034;&#19982;&#24515;&#21147;&#34928;&#31469;&#30456;&#20851;&#30340;&#20020;&#24202;&#34920;&#22411;&#65292;&#24182;&#26681;&#25454;&#30456;&#20284;&#30340;&#29305;&#24449;&#23545;&#30149;&#20154;&#36827;&#34892;&#20998;&#32452;&#65292;&#36825;&#20063;&#26377;&#21161;&#20110;&#39044;&#27979;&#30149;&#20154;&#30340;&#20303;&#38498;&#26102;&#38388;&#12290;&#30001;&#20110;&#36825;&#20123;&#20020;&#24202;&#34920;&#22411;&#36890;&#24120;&#20855;&#26377;&#27010;&#29575;&#30340;&#28508;&#22312;&#32467;&#26500;&#65292;&#24182;&#19988;&#20043;&#21069;&#27809;&#26377;&#20851;&#20110;&#20351;&#29992;&#27010;&#29575;&#26694;&#26550;&#22312;&#24515;&#21147;&#34928;&#31469;&#24739;&#32773;&#30340;&#20020;&#24202;&#31508;&#35760;&#20013;&#35782;&#21035;&#34920;&#22411;&#21644;&#20351;&#29992;&#25968;&#25454;&#39537;&#21160;&#30340;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#26041;&#27861;&#39044;&#27979;&#36825;&#20123;&#24739;&#32773;&#30340;&#20303;&#38498;&#26102;&#38388;&#30340;&#30740;&#31350;&#65292;&#22240;&#27492;&#25105;&#20204;&#37319;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#8212;&#8212;&#20027;&#39064;&#24314;&#27169;&#65292;&#23545;&#20234;&#21033;&#35834;&#20234;&#22823;&#23398;&#21307;&#38498;1200&#20363;&#24515;&#21147;&#34928;&#31469;&#24739;&#32773;&#30340;&#35786;&#26029;&#32534;&#30721;&#21644;&#31243;&#24207;&#25253;&#21578;&#20013;&#23384;&#22312;&#30340;&#20027;&#39064;&#36827;&#34892;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
Heart failure is a syndrome which occurs when the heart is not able to pump blood and oxygen to support other organs in the body. Identifying the underlying themes in the diagnostic codes and procedure reports of patients admitted for heart failure could reveal the clinical phenotypes associated with heart failure and to group patients based on their similar characteristics which could also help in predicting patient outcomes like length of stay. These clinical phenotypes usually have a probabilistic latent structure and hence, as there has been no previous work on identifying phenotypes in clinical notes of heart failure patients using a probabilistic framework and to predict length of stay of these patients using data-driven artificial intelligence-based methods, we apply natural language processing technique, topic modeling, to identify the themes present in diagnostic codes and in procedure reports of 1,200 patients admitted for heart failure at the University of Illinois Hospital 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#22312;&#26435;&#34913;&#31934;&#24230;&#21644;&#25512;&#29702;&#24310;&#36831;&#30340;&#21069;&#25552;&#19979;&#65292;&#21738;&#20123;&#22240;&#32032;&#20351;&#24471;Vision Transformer&#36866;&#29992;&#20110;&#31227;&#21160;&#37096;&#32626;&#65292;&#24182;&#30740;&#31350;&#20102;&#19987;&#38376;&#35774;&#35745;&#20026;&#31227;&#21160;&#24212;&#29992;&#30340;ViT&#30340;&#20307;&#31995;&#32467;&#26500;&#21644;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2305.19365</link><description>&lt;p&gt;
&#31227;&#21160;&#24212;&#29992;&#20013;&#30340;&#35270;&#35273;Transformer: &#19968;&#20221;&#31616;&#30701;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Vision Transformers for Mobile Applications: A Short Survey. (arXiv:2305.19365v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19365
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#22312;&#26435;&#34913;&#31934;&#24230;&#21644;&#25512;&#29702;&#24310;&#36831;&#30340;&#21069;&#25552;&#19979;&#65292;&#21738;&#20123;&#22240;&#32032;&#20351;&#24471;Vision Transformer&#36866;&#29992;&#20110;&#31227;&#21160;&#37096;&#32626;&#65292;&#24182;&#30740;&#31350;&#20102;&#19987;&#38376;&#35774;&#35745;&#20026;&#31227;&#21160;&#24212;&#29992;&#30340;ViT&#30340;&#20307;&#31995;&#32467;&#26500;&#21644;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;Transformer (ViT) &#22312;&#35768;&#22810;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#19978;&#23637;&#29616;&#20102;&#26368;&#26032;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#37096;&#32626;&#36825;&#20123;&#22823;&#35268;&#27169;&#30340;ViT&#23545;&#35768;&#22810;&#31227;&#21160;&#35774;&#22791;&#26469;&#35828;&#26159;&#36164;&#28304;&#28040;&#32791;&#22823;&#19988;&#19981;&#21487;&#33021;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#30456;&#21453;&#30340;&#38382;&#39064;&#65306;&#22312;&#31934;&#24230;&#21644;&#25512;&#29702;&#24310;&#36831;&#30340;&#26435;&#34913;&#32771;&#34385;&#19979;&#65292;ViT&#21487;&#20197;&#22810;&#23567;&#65292;&#20197;&#20351;&#20854;&#36866;&#29992;&#20110;&#31227;&#21160;&#37096;&#32626;&#65311;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#20123;&#19987;&#20026;&#31227;&#21160;&#24212;&#29992;&#31243;&#24207;&#35774;&#35745;&#30340;ViT&#65292;&#24182;&#35266;&#23519;&#21040;&#23427;&#20204;&#20462;&#25913;&#20102;Transformer&#30340;&#26550;&#26500;&#25110;&#26159;&#22260;&#32469;CNN&#21644;Transformer&#30340;&#32452;&#21512;&#32780;&#26500;&#24314;&#12290;&#26368;&#36817;&#30340;&#19968;&#20123;&#24037;&#20316;&#36824;&#23581;&#35797;&#21019;&#24314;&#31232;&#30095;ViT&#32593;&#32476;&#24182;&#25552;&#20986;&#20102;&#20851;&#20110;&#27880;&#24847;&#21147;&#27169;&#22359;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#36825;&#20123;&#26550;&#26500;&#65292;&#30830;&#23450;&#20102;&#25361;&#25112;&#24182;&#20998;&#26512;&#20102;&#26159;&#20160;&#20040;&#20351;&#24471;Vision Transformer&#36866;&#29992;&#20110;&#31227;&#21160;&#24212;&#29992;&#12290;&#25105;&#20204;&#26088;&#22312;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#25552;&#20379;&#22522;&#32447;&#65292;&#24182;&#24076;&#26395;&#25171;&#19979;&#31227;&#21160;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#30340;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision Transformers (ViTs) have demonstrated state-of-the-art performance on many Computer Vision Tasks. Unfortunately, deploying these large-scale ViTs is resource-consuming and impossible for many mobile devices. While most in the community are building for larger and larger ViTs, we ask a completely opposite question: How small can a ViT be within the tradeoffs of accuracy and inference latency that make it suitable for mobile deployment? We look into a few ViTs specifically designed for mobile applications and observe that they modify the transformer's architecture or are built around the combination of CNN and transformer. Recent work has also attempted to create sparse ViT networks and proposed alternatives to the attention module. In this paper, we study these architectures, identify the challenges and analyze what really makes a vision transformer suitable for mobile applications. We aim to serve as a baseline for future research direction and hopefully lay the foundation to ch
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;I-STAR&#65292;&#21487;&#20197;&#22686;&#21152;&#27169;&#22411;&#30340;&#31283;&#23450;&#24615;&#65292;&#25552;&#39640;&#24615;&#33021;&#65292;&#24182;&#25913;&#21892;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#32452;&#21512;&#34920;&#31034;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.19358</link><description>&lt;p&gt;
&#31283;&#20581;&#30340;&#21508;&#21521;&#24322;&#24615;&#27491;&#21017;&#21270;
&lt;/p&gt;
&lt;p&gt;
Stable Anisotropic Regularization. (arXiv:2305.19358v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19358
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;I-STAR&#65292;&#21487;&#20197;&#22686;&#21152;&#27169;&#22411;&#30340;&#31283;&#23450;&#24615;&#65292;&#25552;&#39640;&#24615;&#33021;&#65292;&#24182;&#25913;&#21892;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#32452;&#21512;&#34920;&#31034;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25104;&#21151;&#65292;&#30740;&#31350;&#27169;&#22411;&#28608;&#27963;&#30340;&#23646;&#24615;&#24050;&#24341;&#36215;&#20102;&#30456;&#24403;&#22823;&#30340;&#20852;&#36259;&#12290;&#25991;&#29486;&#26222;&#36941;&#35748;&#20026;LLMs&#34920;&#31034;&#30001;&#23569;&#25968;&#20855;&#26377;&#26497;&#39640;&#26041;&#24046;&#21644;&#24133;&#24230;&#30340;&#8220;&#24322;&#24120;&#32500;&#24230;&#8221;&#20027;&#23548;&#12290;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20013;&#30340;&#20960;&#39033;&#30740;&#31350;&#35797;&#22270;&#20943;&#36731;&#36825;&#20123;&#24322;&#24120;&#32500;&#24230;&#30340;&#24433;&#21709;&#65292;&#24182;&#36843;&#20351;LLMs&#25104;&#20026;&#21508;&#21521;&#21516;&#24615;&#65288;&#21363;&#22312;&#23884;&#20837;&#31354;&#38388;&#20013;&#25152;&#26377;&#32500;&#24230;&#20855;&#26377;&#22343;&#21248;&#26041;&#24046;&#65289;&#30340;&#12290;&#21508;&#21521;&#21516;&#24615;&#34987;&#35748;&#20026;&#26159;LLMs&#30340;&#19968;&#31181;&#29702;&#24819;&#23646;&#24615;&#65292;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#24182;&#26356;&#21152;&#36148;&#36817;&#20154;&#31867;&#30452;&#35273;&#30340;&#25991;&#26412;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;NLP&#20013;&#21508;&#21521;&#21516;&#24615;&#30340;&#35768;&#22810;&#35266;&#28857;&#37117;&#26159;&#22522;&#20110;&#23884;&#20837;&#30340;&#24179;&#22343;&#20313;&#24358;&#30456;&#20284;&#24230;&#65292;&#26368;&#36817;&#24050;&#32463;&#34920;&#26126;&#36825;&#26159;&#19968;&#31181;&#26377;&#32570;&#38519;&#30340;&#21508;&#21521;&#21516;&#24615;&#24230;&#37327;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;I-STAR&#65306;&#22522;&#20110;IsoScore$^{\star}$&#30340;&#31283;&#23450;&#21508;&#21521;&#24322;&#24615;&#27491;&#21017;&#21270;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#21487;&#20197;&#29992;&#20110;&#22686;&#21152;&#27169;&#22411;&#30340;&#31283;&#23450;&#24615;&#24182;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given the success of Large Language Models (LLMs), there has been considerable interest in studying the properties of model activations. The literature overwhelmingly agrees that LLM representations are dominated by a few ``outlier dimensions'' with exceedingly high variance and magnitude. Several studies in Natural Language Processing (NLP) have sought to mitigate the impact of such outlier dimensions and force LLMs to be isotropic (i.e., have uniform variance across all dimensions in embedding space). Isotropy is thought to be a desirable property for LLMs that improves model performance and more closely aligns textual representations with human intuition. However, many of the claims regarding isotropy in NLP have been based on the average cosine similarity of embeddings, which has recently been shown to be a flawed measure of isotropy. In this paper, we propose I-STAR: IsoScore$^{\star}$-based STable Anisotropic Regularization, a novel regularization method that can be used to incre
&lt;/p&gt;</description></item><item><title /><link>http://arxiv.org/abs/2305.19339</link><description>&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Less Likely Brainstorming: Using Language Models to Generate Alternative Hypotheses. (arXiv:2305.19339v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19339
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
A human decision-maker benefits the most from an AI assistant that corrects for their biases. For problems such as generating interpretation of a radiology report given findings, a system predicting only highly likely outcomes may be less useful, where such outcomes are already obvious to the user. To alleviate biases in human decision-making, it is worth considering a broad differential diagnosis, going beyond the most likely options. We introduce a new task, "less likely brainstorming," that asks a model to generate outputs that humans think are relevant but less likely to happen. We explore the task in two settings: a brain MRI interpretation generation setting and an everyday commonsense reasoning setting. We found that a baseline approach of training with less likely hypotheses as targets generates outputs that humans evaluate as either likely or irrelevant nearly half of the time; standard MLE training is not effective. To tackle this problem, we propose a controlled text generat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;SpikeGCL&#65292;&#19968;&#31181;&#29992;&#20108;&#20540;&#21270;1&#27604;&#29305;&#34920;&#31034;&#26469;&#25552;&#39640;&#25928;&#29575;&#21644;&#33410;&#32422;&#36164;&#28304;&#30340;&#22270;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#21487;&#20197;&#20197;&#36817;32&#20493;&#30340;&#34920;&#31034;&#23384;&#20648;&#21387;&#32553;&#23454;&#29616;&#39640;&#25928;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2305.19306</link><description>&lt;p&gt;
&#19968;&#24352;&#22270;&#20540;&#24471;&#19968;&#27604;&#29305;&#30340;&#24046;&#24322;&#24615;&#65306;&#24403;&#22270;&#30340;&#23545;&#27604;&#23398;&#20064;&#36935;&#21040;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#26102;&#12290;
&lt;/p&gt;
&lt;p&gt;
A Graph is Worth 1-bit Spikes: When Graph Contrastive Learning Meets Spiking Neural Networks. (arXiv:2305.19306v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19306
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;SpikeGCL&#65292;&#19968;&#31181;&#29992;&#20108;&#20540;&#21270;1&#27604;&#29305;&#34920;&#31034;&#26469;&#25552;&#39640;&#25928;&#29575;&#21644;&#33410;&#32422;&#36164;&#28304;&#30340;&#22270;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#21487;&#20197;&#20197;&#36817;32&#20493;&#30340;&#34920;&#31034;&#23384;&#20648;&#21387;&#32553;&#23454;&#29616;&#39640;&#25928;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#23545;&#27604;&#33258;&#30417;&#30563;&#23398;&#20064;&#24050;&#32463;&#25104;&#20026;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#20107;&#23454;&#19978;&#30340;&#23398;&#20064;&#33539;&#24335;&#65292;&#20294;&#23545;&#39640;&#20219;&#21153;&#20934;&#30830;&#24615;&#30340;&#36861;&#27714;&#38656;&#35201;&#22823;&#30340;&#38544;&#34255;&#32500;&#24230;&#26469;&#23398;&#20064;&#20449;&#24687;&#20016;&#23500;&#12289;&#26377;&#21306;&#21035;&#24615;&#30340;&#20840;&#31934;&#24230;&#34920;&#31034;&#65292;&#36825;&#24341;&#21457;&#20102;&#23545;&#35745;&#31639;&#12289;&#23384;&#20648;&#21644;&#33021;&#28304;&#28040;&#32791;&#36127;&#25285;&#65288;&#22312;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#22823;&#22810;&#34987;&#24573;&#30053;&#65289;&#30340;&#25285;&#24551;&#12290;&#26412;&#25991;&#25506;&#32034;&#20102;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#21521;&#65292;&#21363;&#29992;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNNs&#65289;&#36827;&#34892;&#22270;&#30340;&#23545;&#27604;&#23398;&#20064;&#65288;GCL&#65289;&#65292;&#21033;&#29992;&#31232;&#30095;&#21644;&#20108;&#20803;&#29305;&#24615;&#26469;&#23398;&#20064;&#26356;&#20855;&#29983;&#29289;&#21487;&#34892;&#24615;&#21644;&#32039;&#20945;&#24615;&#30340;&#34920;&#31034;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;SpikeGCL&#65292;&#19968;&#31181;&#23398;&#20064;&#22270;&#30340;&#20108;&#20540;&#21270;1&#27604;&#29305;&#34920;&#31034;&#30340;&#26032;&#22411;GCL&#26694;&#26550;&#65292;&#24179;&#34913;&#20102;&#25928;&#29575;&#21644;&#24615;&#33021;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#29702;&#35770;&#20445;&#35777;&#65292;&#35777;&#26126;SpikeGCL&#22312;&#34920;&#36798;&#33021;&#21147;&#19978;&#19982;&#20854;&#20840;&#31934;&#24230;&#23545;&#24212;&#29289;&#20855;&#26377;&#21487;&#27604;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#23558;&#34920;&#31034;&#23384;&#20648;&#21387;&#32553;&#36817;32&#20493;&#65292;SpikeGCL&#22312;&#20445;&#25345;&#39640;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#21487;&#20197;&#23454;&#29616;&#39640;&#25928;&#30340;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
While contrastive self-supervised learning has become the de-facto learning paradigm for graph neural networks, the pursuit of high task accuracy requires a large hidden dimensionality to learn informative and discriminative full-precision representations, raising concerns about computation, memory footprint, and energy consumption burden (largely overlooked) for real-world applications. This paper explores a promising direction for graph contrastive learning (GCL) with spiking neural networks (SNNs), which leverage sparse and binary characteristics to learn more biologically plausible and compact representations. We propose SpikeGCL, a novel GCL framework to learn binarized 1-bit representations for graphs, making balanced trade-offs between efficiency and performance. We provide theoretical guarantees to demonstrate that SpikeGCL has comparable expressiveness with its full-precision counterparts. Experimental results demonstrate that, with nearly 32x representation storage compressio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#26426;&#22120;&#23398;&#20064;&#36816;&#32500;&#65288;MLOps&#65289;&#22312;&#20225;&#19994;&#32423;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#37325;&#35201;&#24615;&#21644;&#36866;&#29992;&#24615;&#65292;&#24182;&#35814;&#32454;&#35299;&#37322;&#20102;MLOps&#24037;&#20316;&#27969;&#31243;&#12289;&#33258;&#21160;&#21270;&#27969;&#31243;&#30340;&#19981;&#21516;&#25104;&#29087;&#24230;&#27700;&#24179;&#20197;&#21450;&#21508;&#31181;&#24213;&#23618;&#25216;&#26415;&#12290;&#26368;&#21518;&#65292;&#29992;&#19968;&#20010;&#29289;&#20307;&#26816;&#27979;&#26381;&#21153;&#30340;&#20225;&#19994;&#32423;MLOps&#39033;&#30446;&#30340;&#35814;&#32454;&#31034;&#20363;&#26469;&#35299;&#37322;&#25216;&#26415;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#30340;&#24037;&#20316;&#27969;&#31243;&#12290;</title><link>http://arxiv.org/abs/2305.19298</link><description>&lt;p&gt;
MLOps&#65306;&#20225;&#19994;&#32423;&#26426;&#22120;&#23398;&#20064;&#36808;&#36827;&#19968;&#27493;
&lt;/p&gt;
&lt;p&gt;
MLOps: A Step Forward to Enterprise Machine Learning. (arXiv:2305.19298v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19298
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#26426;&#22120;&#23398;&#20064;&#36816;&#32500;&#65288;MLOps&#65289;&#22312;&#20225;&#19994;&#32423;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#37325;&#35201;&#24615;&#21644;&#36866;&#29992;&#24615;&#65292;&#24182;&#35814;&#32454;&#35299;&#37322;&#20102;MLOps&#24037;&#20316;&#27969;&#31243;&#12289;&#33258;&#21160;&#21270;&#27969;&#31243;&#30340;&#19981;&#21516;&#25104;&#29087;&#24230;&#27700;&#24179;&#20197;&#21450;&#21508;&#31181;&#24213;&#23618;&#25216;&#26415;&#12290;&#26368;&#21518;&#65292;&#29992;&#19968;&#20010;&#29289;&#20307;&#26816;&#27979;&#26381;&#21153;&#30340;&#20225;&#19994;&#32423;MLOps&#39033;&#30446;&#30340;&#35814;&#32454;&#31034;&#20363;&#26469;&#35299;&#37322;&#25216;&#26415;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#30340;&#24037;&#20316;&#27969;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#36816;&#32500;&#65288;MLOps&#65289;&#27491;&#25104;&#20026;&#24076;&#26395;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#21644;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#33719;&#30410;&#30340;&#20225;&#19994;&#38750;&#24120;&#20851;&#38190;&#30340;&#19968;&#37096;&#20998;&#12290;&#26412;&#30740;&#31350;&#35814;&#32454;&#22238;&#39038;&#20102;MLOps&#21450;&#20854;&#22909;&#22788;&#12289;&#22256;&#38590;&#12289;&#36827;&#21270;&#20197;&#21450;&#37325;&#35201;&#30340;&#24213;&#23618;&#25216;&#26415;&#65292;&#22914;MLOps&#26694;&#26550;&#12289;Docker&#12289;GitHub&#25805;&#20316;&#21644;Kubernetes&#12290;&#35814;&#32454;&#35299;&#37322;&#20102;MLOps&#24037;&#20316;&#27969;&#31243;&#65292;&#21253;&#25324;&#27169;&#22411;&#35774;&#35745;&#12289;&#37096;&#32626;&#21644;&#25805;&#20316;&#65292;&#20197;&#21450;&#20026;&#27169;&#22411;&#21644;&#25968;&#25454;&#25506;&#32034;&#21644;&#37096;&#32626;&#25152;&#24517;&#38656;&#30340;&#21508;&#31181;&#24037;&#20855;&#12290;&#27492;&#22806;&#65292;&#35813;&#25991;&#31456;&#36824;&#24378;&#35843;&#20102;&#20351;&#29992;&#21508;&#31181;&#33258;&#21160;&#21270;&#27969;&#31243;&#30340;&#19981;&#21516;&#25104;&#29087;&#24230;&#27700;&#24179;&#26469;&#31471;&#21040;&#31471;&#29983;&#20135;ML&#39033;&#30446;&#30340;&#37325;&#35201;&#24615;&#12290;&#26368;&#20302;&#20026;&#26080;&#33258;&#21160;&#21270;&#65292;&#26368;&#39640;&#20026;&#20855;&#26377;&#23436;&#25972;&#30340;CI/CD&#21644;CT&#21151;&#33021;&#12290;&#27492;&#22806;&#65292;&#20351;&#29992;&#29289;&#20307;&#26816;&#27979;&#26381;&#21153;&#30340;&#20225;&#19994;&#32423;MLOps&#39033;&#30446;&#30340;&#35814;&#32454;&#31034;&#20363;&#65292;&#29992;&#20110;&#35299;&#37322;&#25216;&#26415;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#30340;&#24037;&#20316;&#27969;&#31243;&#12290;&#20026;&#27492;&#65292;&#20351;&#29992;TensorFlow Object Detection API&#35757;&#32451;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#20102;Flask Web Framework&#22312;Web&#24212;&#29992;&#31243;&#24207;&#20013;&#25176;&#31649;&#35813;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine Learning Operations (MLOps) is becoming a highly crucial part of businesses looking to capitalize on the benefits of AI and ML models. This research presents a detailed review of MLOps, its benefits, difficulties, evolutions, and important underlying technologies such as MLOps frameworks, Docker, GitHub actions, and Kubernetes. The MLOps workflow, which includes model design, deployment, and operations, is explained in detail along with the various tools necessary for both model and data exploration and deployment. This article also puts light on the end-to-end production of ML projects using various maturity levels of automated pipelines, with the least at no automation at all and the highest with complete CI/CD and CT capabilities. Furthermore, a detailed example of an enterprise-level MLOps project for an object detection service is used to explain the workflow of the technology in a real-world scenario. For this purpose, a web application hosting a pre-trained model from Te
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#27169;&#22411;&#26080;&#21608;&#30028;&#25511;&#21046;&#26694;&#26550;&#65292;&#36890;&#36807;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#20195;&#29702;&#19982;&#27169;&#25311;&#36710;&#36742;&#36827;&#34892;&#20132;&#20114;&#65292;&#23454;&#29616;&#21516;&#36136;&#27969;&#37327;&#20248;&#21270;&#65292;&#27604;&#20256;&#32479;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#26041;&#27861;&#26356;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2305.19291</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#21608;&#30028;&#25511;&#21046;: &#19968;&#31181;&#26080;&#27169;&#22411;&#26041;&#27861;&#23454;&#29616;&#21516;&#36136;&#27969;&#37327;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Perimeter Control Using Deep Reinforcement Learning: A Model-free Approach towards Homogeneous Flow Rate Optimization. (arXiv:2305.19291v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19291
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#27169;&#22411;&#26080;&#21608;&#30028;&#25511;&#21046;&#26694;&#26550;&#65292;&#36890;&#36807;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#20195;&#29702;&#19982;&#27169;&#25311;&#36710;&#36742;&#36827;&#34892;&#20132;&#20114;&#65292;&#23454;&#29616;&#21516;&#36136;&#27969;&#37327;&#20248;&#21270;&#65292;&#27604;&#20256;&#32479;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#26041;&#27861;&#26356;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21608;&#30028;&#25511;&#21046;&#36890;&#36807;&#25511;&#21046;&#19981;&#21516;&#21306;&#22495;&#20043;&#38388;&#30340;&#20132;&#36890;&#36716;&#31227;&#27969;&#37327;&#65292;&#20197;&#20445;&#35777;&#20854;&#20132;&#36890;&#23494;&#24230;&#20302;&#20110;&#20020;&#30028;&#20540;&#65292;&#20174;&#32780;&#20445;&#25345;&#21463;&#20445;&#25252;&#21306;&#22495;&#20869;&#20132;&#36890;&#39640;&#25928;&#12290;&#29616;&#26377;&#26041;&#27861;&#21487;&#20197;&#20998;&#20026;&#22522;&#20110;&#32593;&#32476;&#20256;&#36755;&#27169;&#22411;&#65288;NTMs&#65289;&#21644;&#23439;&#35266;&#22522;&#26412;&#22270;&#65288;MFDs&#65289;&#30340;&#27169;&#22411;&#22522;&#26041;&#27861;&#21644;&#26080;&#27169;&#22411;&#26041;&#27861;&#12290;&#34429;&#28982;&#22522;&#20110;&#27169;&#22411;&#30340;&#26041;&#27861;&#20855;&#26377;&#26356;&#39640;&#30340;&#25968;&#25454;&#25928;&#29575;&#21644;&#24615;&#33021;&#20445;&#35777;&#65292;&#20294;&#23427;&#20204;&#22825;&#29983;&#23481;&#26131;&#21463;&#21040;&#27169;&#22411;&#20559;&#24046;&#21644;&#19981;&#20934;&#30830;&#24615;&#30340;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#30740;&#31350;&#20013;&#27809;&#26377;&#19968;&#39033;&#30740;&#31350;&#22312;&#24494;&#35266;&#27169;&#25311;&#20013;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#21516;&#36136;&#27969;&#37327;&#20248;&#21270;&#65292;&#24494;&#35266;&#27169;&#25311;&#32771;&#34385;&#20102;&#31354;&#38388;&#29305;&#24449;&#12289;&#36710;&#36742;&#32423;&#21035;&#20449;&#24687;&#21644;&#35745;&#37327;&#23454;&#29616;&#65292;&#36825;&#20123;&#22312;&#23439;&#35266;&#27169;&#25311;&#20013;&#32463;&#24120;&#34987;&#24573;&#30053;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#30340;&#26080;&#27169;&#22411;&#21608;&#30028;&#25511;&#21046;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#20351;&#29992;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#20195;&#29702;&#19982;&#27169;&#25311;&#36710;&#36742;&#36827;&#34892;&#20132;&#20114;&#65292;&#23398;&#20064;&#22914;&#20309;&#20248;&#21270;&#20449;&#21495;&#26102;&#24207;&#20197;&#25511;&#21046;&#21463;&#20445;&#25252;&#21306;&#22495;&#30340;&#27969;&#20837;&#21644;&#27969;&#20986;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22522;&#20110;DRL&#30340;&#26041;&#27861;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#20132;&#36890;&#24773;&#20917;&#19979;&#22343;&#33021;&#20248;&#20110;&#20256;&#32479;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Perimeter control maintains high traffic efficiency within protected regions by controlling transfer flows among regions to ensure that their traffic densities are below critical values. Existing approaches can be categorized as either model-based or model-free, depending on whether they rely on network transmission models (NTMs) and macroscopic fundamental diagrams (MFDs). Although model-based approaches are more data efficient and have performance guarantees, they are inherently prone to model bias and inaccuracy. For example, NTMs often become imprecise for a large number of protected regions, and MFDs can exhibit scatter and hysteresis that are not captured in existing model-based works. Moreover, no existing studies have employed reinforcement learning for homogeneous flow rate optimization in microscopic simulation, where spatial characteristics, vehicle-level information, and metering realizations -- often overlooked in macroscopic simulations -- are taken into account. To circu
&lt;/p&gt;</description></item><item><title>CYRUS&#36275;&#29699;&#27169;&#25311;2D&#22242;&#38431;&#37319;&#29992;LSTM&#21644;DNN&#25552;&#20986;&#35266;&#23519;&#21435;&#22122;&#24605;&#36335;&#65292;&#36798;&#25104;RoboCup 2021&#30340;&#20896;&#20891;&#12290;</title><link>http://arxiv.org/abs/2305.19283</link><description>&lt;p&gt;
CYRUS&#36275;&#29699;&#27169;&#25311;2D&#22242;&#38431;&#22312;RoboCup 2023&#20013;&#30340;&#35266;&#23519;&#21435;&#22122;
&lt;/p&gt;
&lt;p&gt;
Observation Denoising in CYRUS Soccer Simulation 2D Team For RoboCup 2023. (arXiv:2305.19283v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19283
&lt;/p&gt;
&lt;p&gt;
CYRUS&#36275;&#29699;&#27169;&#25311;2D&#22242;&#38431;&#37319;&#29992;LSTM&#21644;DNN&#25552;&#20986;&#35266;&#23519;&#21435;&#22122;&#24605;&#36335;&#65292;&#36798;&#25104;RoboCup 2021&#30340;&#20896;&#20891;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
RoboCup&#27604;&#36187;&#26377;&#22810;&#20010;&#32852;&#30431;&#65292;&#20854;&#20013;Soccer Simulation 2D&#32852;&#30431;&#26159;&#19968;&#20010;&#20027;&#35201;&#32852;&#30431;&#12290;Soccer Simulation 2D&#65288;SS2D&#65289;&#27604;&#36187;&#28041;&#21450;&#20004;&#20010;&#22242;&#38431;&#65292;&#21253;&#25324;11&#21517;&#29699;&#21592;&#21644;&#19968;&#21517;&#25945;&#32451;&#20114;&#30456;&#31454;&#20105;&#12290;&#27604;&#36187;&#26399;&#38388;&#29699;&#21592;&#21482;&#33021;&#19982;Soccer Simulation&#26381;&#21153;&#22120;&#36890;&#20449;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;CYRUS&#36275;&#29699;&#27169;&#25311;2D&#22242;&#38431;&#30340;&#26368;&#26032;&#30740;&#31350;&#65292;&#20182;&#20204;&#26159;RoboCup2021&#30340;&#20896;&#20891;&#12290;&#25105;&#20204;&#23558;&#35299;&#37322;&#25105;&#20204;&#30340;&#21435;&#22122;&#24605;&#36335;&#65292;&#23427;&#30001;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#65288;LSTM&#65289;&#21644;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#25552;&#20379;&#25903;&#25345;&#12290;CYRUS&#22242;&#38431;&#20351;&#29992;&#20102;&#22522;&#20110;Helios&#21644;Gliders&#24320;&#21457;&#30340;CYRUS2D&#22522;&#30784;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
The RoboCup competitions hold various leagues, and the Soccer Simulation 2D League is a major one among them. Soccer Simulation 2D (SS2D) match involves two teams, including 11 players and a coach, competing against each other. The players can only communicate with the Soccer Simulation Server during the game. This paper presents the latest research of the CYRUS soccer simulation 2D team, the champion of RoboCup 2021. We will explain our denoising idea powered by long short-term memory networks (LSTM) and deep neural networks (DNN). The CYRUS team uses the CYRUS2D base code that was developed based on the Helios and Gliders bases.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20256;&#32479;&#27874;&#26031;&#21307;&#23398;&#30340;&#36828;&#31243;&#25252;&#29702;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#21487;&#20197;&#21033;&#29992;&#35760;&#24405;&#30340;&#28909;&#20998;&#24067;&#12289;&#20307;&#36136;&#38382;&#21367;&#21644;&#23450;&#21046;&#30340;&#33033;&#25615;&#27979;&#37327;&#35774;&#22791;&#65292;&#35780;&#20272;&#24739;&#32773;&#30340;&#20307;&#36136;&#29366;&#24577;&#24182;&#23558;&#32467;&#26524;&#21457;&#36865;&#32473;&#21307;&#29983;&#65292;&#20174;&#32780;&#20943;&#23569;&#23545;&#27874;&#26031;&#21307;&#23398;&#19987;&#23478;&#30340;&#20381;&#36182;&#12290;</title><link>http://arxiv.org/abs/2305.19282</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#20256;&#32479;&#27874;&#26031;&#21307;&#23398;&#30340;&#36828;&#31243;&#25252;&#29702;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
A Telecare System for Use in Traditional Persian Medicine. (arXiv:2305.19282v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19282
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20256;&#32479;&#27874;&#26031;&#21307;&#23398;&#30340;&#36828;&#31243;&#25252;&#29702;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#21487;&#20197;&#21033;&#29992;&#35760;&#24405;&#30340;&#28909;&#20998;&#24067;&#12289;&#20307;&#36136;&#38382;&#21367;&#21644;&#23450;&#21046;&#30340;&#33033;&#25615;&#27979;&#37327;&#35774;&#22791;&#65292;&#35780;&#20272;&#24739;&#32773;&#30340;&#20307;&#36136;&#29366;&#24577;&#24182;&#23558;&#32467;&#26524;&#21457;&#36865;&#32473;&#21307;&#29983;&#65292;&#20174;&#32780;&#20943;&#23569;&#23545;&#27874;&#26031;&#21307;&#23398;&#19987;&#23478;&#30340;&#20381;&#36182;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27874;&#26031;&#21307;&#23398;&#65288;PM&#65289;&#21033;&#29992;&#25163;&#33109;&#28201;&#24230;/&#28287;&#24230;&#21644;&#33033;&#25615;&#26469;&#30830;&#23450;&#20154;&#20307;&#20581;&#24247;&#29366;&#20917;&#21644;&#20307;&#36136;&#12290;&#28982;&#32780;&#65292;&#35786;&#26029;&#21487;&#33021;&#21462;&#20915;&#20110;&#21307;&#24072;&#30340;&#35299;&#37322;&#65292;&#20174;&#32780;&#38459;&#30861;PM&#19982;&#29616;&#20195;&#21307;&#23398;&#26041;&#27861;&#30340;&#32467;&#21512;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;PM&#30340;&#33033;&#25615;&#20449;&#21495;&#27979;&#37327;&#21644;&#20307;&#36136;&#26816;&#27979;&#31995;&#32479;&#12290;&#35813;&#31995;&#32479;&#20351;&#29992;&#35760;&#24405;&#30340;&#28909;&#20998;&#24067;&#12289;&#20307;&#36136;&#38382;&#21367;&#21644;&#23450;&#21046;&#30340;&#33033;&#25615;&#27979;&#37327;&#35774;&#22791;&#12290;&#25910;&#38598;&#30340;&#25968;&#25454;&#21487;&#20197;&#36890;&#36807;&#36828;&#31243;&#25252;&#29702;&#31995;&#32479;&#21457;&#36865;&#32473;&#21307;&#29983;&#36827;&#34892;&#35299;&#37322;&#21644;&#22788;&#26041;&#33647;&#29289;&#12290;&#35813;&#31995;&#32479;&#24050;&#22312;&#24739;&#32773;&#25252;&#29702;&#20013;&#36827;&#34892;&#20102;&#20020;&#24202;&#23454;&#26045;&#65292;&#35780;&#20272;&#20102;34&#21517;&#21442;&#19982;&#32773;&#30340;&#20307;&#36136;&#65292;&#24182;&#35760;&#24405;&#20102;&#25163;&#33109;&#12289;&#25163;&#32972;&#21644;&#25972;&#20010;&#38754;&#37096;&#30340;&#28909;&#20687;&#12290;&#30740;&#31350;&#34920;&#26126;&#21487;&#20197;&#23558;&#22522;&#20110;PM&#30340;&#33033;&#25615;&#27874;&#21644;&#20854;&#20182;&#26631;&#20934;&#30340;&#23450;&#21046;&#35774;&#22791;&#32435;&#20837;&#36828;&#31243;&#21307;&#30103;&#31995;&#32479;&#65292;&#20943;&#23569;&#23545;PM&#19987;&#23478;&#30340;&#35786;&#26029;&#20381;&#36182;&#12290;
&lt;/p&gt;
&lt;p&gt;
Persian Medicine (PM) uses wrist temperature/humidity and pulse to determine a person's health status and temperament. However, the diagnosis may depend on the physician's interpretation, hindering the combination of PM with modern medical methods. This study proposes a system for measuring pulse signals and temperament detection based on PM. The system uses recorded thermal distribution, a temperament questionnaire, and a customized pulse measurement device. The collected data can be sent to a physician via a telecare system for interpretation and prescription of medications. The system was clinically implemented for patient care, assessed the temperaments of 34 participants, and recorded thermal images of the wrist, back of the hand, and entire face. The study suggests that a customized device for measuring pulse waves and other criteria based on PM can be incorporated into a telemedicine system, reducing the dependency on PM specialists for diagnosis.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#39640;&#23545;&#38750;&#24433;&#20687;&#25968;&#25454;&#30340;&#24212;&#29992;&#33021;&#21147;&#65292;&#24182;&#22312;ADNI&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;SOTA&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.19280</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25913;&#21892;&#22810;&#27169;&#24577;&#25968;&#25454;&#35786;&#26029;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;
&lt;/p&gt;
&lt;p&gt;
Large language models improve Alzheimer's disease diagnosis using multi-modality data. (arXiv:2305.19280v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19280
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#39640;&#23545;&#38750;&#24433;&#20687;&#25968;&#25454;&#30340;&#24212;&#29992;&#33021;&#21147;&#65292;&#24182;&#22312;ADNI&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;SOTA&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35786;&#26029;&#20687;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#65288;AD&#65289;&#36825;&#26679;&#30340;&#33392;&#38590;&#30149;&#30151;&#26102;&#65292;&#24433;&#20687;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#21442;&#32771;&#12290;&#38750;&#24433;&#20687;&#24739;&#32773;&#25968;&#25454;&#65288;&#20363;&#22914;&#24739;&#32773;&#20449;&#24687;&#12289;&#36951;&#20256;&#25968;&#25454;&#12289;&#33647;&#29289;&#20449;&#24687;&#12289;&#35748;&#30693;&#21644;&#35760;&#24518;&#27979;&#35797;&#65289;&#22312;&#35786;&#26029;&#20013;&#20063;&#36215;&#30528;&#38750;&#24120;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#25366;&#25496;&#36825;&#20123;&#20449;&#24687;&#30340;&#33021;&#21147;&#21463;&#38480;&#65292;&#22823;&#37096;&#20998;&#29616;&#26377;&#27169;&#22411;&#21482;&#33021;&#20351;&#29992;&#22810;&#27169;&#24577;&#24433;&#20687;&#25968;&#25454;&#65292;&#32780;&#19981;&#33021;&#20805;&#20998;&#21033;&#29992;&#38750;&#24433;&#20687;&#25968;&#25454;&#12290;&#25105;&#20204;&#20351;&#29992;&#30446;&#21069;&#38750;&#24120;&#27969;&#34892;&#30340;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#20197;&#22686;&#24378;&#27169;&#22411;&#20351;&#29992;&#38750;&#24433;&#20687;&#25968;&#25454;&#30340;&#33021;&#21147;&#65292;&#24182;&#22312;ADNI&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;SOTA&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In diagnosing challenging conditions such as Alzheimer's disease (AD), imaging is an important reference. Non-imaging patient data such as patient information, genetic data, medication information, cognitive and memory tests also play a very important role in diagnosis. Effect. However, limited by the ability of artificial intelligence models to mine such information, most of the existing models only use multi-modal image data, and cannot make full use of non-image data. We use a currently very popular pre-trained large language model (LLM) to enhance the model's ability to utilize non-image data, and achieved SOTA results on the ADNI dataset.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20849;&#20139;&#24863;&#23448;&#20307;&#39564;&#30340;&#20849;&#29983;&#20154;&#24037;&#26234;&#33021;&#27010;&#24565;&#65292;&#26088;&#22312;&#24314;&#31435;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#21644;&#20154;&#31867;&#29992;&#25143;&#20043;&#38388;&#30340;&#20114;&#24800;&#20851;&#31995;&#65292;&#24182;&#20010;&#24615;&#21270;&#25552;&#20379;&#25903;&#25345;&#12289;&#24110;&#21161;&#21644;&#22686;&#24378;&#65292;&#21516;&#26102;&#25506;&#35752;&#20102;&#38544;&#31169;&#12289;&#20262;&#29702;&#20934;&#21017;&#21644;&#32531;&#35299;&#28508;&#22312;&#20559;&#35265;&#21644;&#19981;&#24179;&#31561;&#29616;&#35937;&#30340;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2305.19278</link><description>&lt;p&gt;
&#36890;&#36807;&#20849;&#20139;&#24863;&#23448;&#20307;&#39564;&#22686;&#24378;&#20154;&#31867;&#33021;&#21147;&#30340;&#20849;&#29983;&#20154;&#24037;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
Enhancing Human Capabilities through Symbiotic Artificial Intelligence with Shared Sensory Experiences. (arXiv:2305.19278v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19278
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20849;&#20139;&#24863;&#23448;&#20307;&#39564;&#30340;&#20849;&#29983;&#20154;&#24037;&#26234;&#33021;&#27010;&#24565;&#65292;&#26088;&#22312;&#24314;&#31435;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#21644;&#20154;&#31867;&#29992;&#25143;&#20043;&#38388;&#30340;&#20114;&#24800;&#20851;&#31995;&#65292;&#24182;&#20010;&#24615;&#21270;&#25552;&#20379;&#25903;&#25345;&#12289;&#24110;&#21161;&#21644;&#22686;&#24378;&#65292;&#21516;&#26102;&#25506;&#35752;&#20102;&#38544;&#31169;&#12289;&#20262;&#29702;&#20934;&#21017;&#21644;&#32531;&#35299;&#28508;&#22312;&#20559;&#35265;&#21644;&#19981;&#24179;&#31561;&#29616;&#35937;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#26234;&#33021;&#30340;&#34701;&#21512;&#19968;&#30452;&#20197;&#26469;&#37117;&#26159;&#31185;&#24187;&#23567;&#35828;&#21644;&#23398;&#26415;&#30028;&#30340;&#30740;&#31350;&#35838;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20154;&#26426;&#20132;&#20114;&#27010;&#24565;&#65292;&#21517;&#20026;&#20849;&#20139;&#24863;&#23448;&#20307;&#39564;&#30340;&#20849;&#29983;&#20154;&#24037;&#26234;&#33021;&#65288;SAISSE&#65289;&#65292;&#26088;&#22312;&#36890;&#36807;&#20849;&#20139;&#24863;&#23448;&#20307;&#39564;&#24314;&#31435;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#21644;&#20154;&#31867;&#29992;&#25143;&#20043;&#38388;&#30340;&#20114;&#24800;&#20851;&#31995;&#12290;&#36890;&#36807;&#38598;&#25104;&#22810;&#31181;&#24863;&#23448;&#36755;&#20837;&#36890;&#36947;&#21644;&#22788;&#29702;&#20154;&#31867;&#20307;&#39564;&#65292;SAISSE&#20419;&#36827;&#20102;&#24378;&#22823;&#30340;&#20154;&#26426;&#20851;&#31995;&#65292;&#20351;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#33021;&#22815;&#20174;&#20010;&#20154;&#29992;&#25143;&#37027;&#37324;&#23398;&#20064;&#21644;&#36866;&#24212;&#65292;&#25552;&#20379;&#20010;&#24615;&#21270;&#30340;&#25903;&#25345;&#12289;&#24110;&#21161;&#21644;&#22686;&#24378;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#29992;&#20110;&#38271;&#26399;&#22686;&#38271;&#21644;&#21457;&#23637;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#21644;&#20854;&#20154;&#31867;&#29992;&#25143;&#30340;&#23384;&#20648;&#21333;&#20803;&#30340;&#25972;&#21512;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#25506;&#35752;&#20102;AI&#20154;&#31867;&#20849;&#29983;&#20013;&#30340;&#29992;&#25143;&#38544;&#31169;&#21644;&#20262;&#29702;&#20934;&#21017;&#65292;&#25506;&#35752;&#20102;&#28508;&#22312;&#30340;&#20559;&#35265;&#21644;&#19981;&#24179;&#31561;&#29616;&#35937;&#65292;&#24182;&#25552;&#20986;&#32531;&#35299;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
The merging of human intelligence and artificial intelligence has long been a subject of interest in both science fiction and academia. In this paper, we introduce a novel concept in Human-AI interaction called Symbiotic Artificial Intelligence with Shared Sensory Experiences (SAISSE), which aims to establish a mutually beneficial relationship between AI systems and human users through shared sensory experiences. By integrating multiple sensory input channels and processing human experiences, SAISSE fosters a strong human-AI bond, enabling AI systems to learn from and adapt to individual users, providing personalized support, assistance, and enhancement. Furthermore, we discuss the incorporation of memory storage units for long-term growth and development of both the AI system and its human user. As we address user privacy and ethical guidelines for responsible AI-human symbiosis, we also explore potential biases and inequalities in AI-human symbiosis and propose strategies to mitigate
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#36136;&#37327;&#20026;&#22522;&#30784;&#30340;&#35760;&#24518;&#22270;&#27169;&#22411;&#65292;&#29992;&#20110; AI &#20013;&#20154;&#31867;&#35760;&#24518;&#30340;&#20223;&#30495;&#12290;&#35813;&#27169;&#22411;&#21487;&#20197;&#28165;&#26224;&#22320;&#21306;&#20998;&#35760;&#24518;&#30340;&#22320;&#24418;&#24046;&#24322;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#21253;&#21547;&#19982;&#35266;&#23519;&#20107;&#23454;&#30456;&#24403;&#19968;&#33268;&#30340;&#22823;&#33041;&#27963;&#21160;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.19274</link><description>&lt;p&gt;
&#20316;&#20026;&#22522;&#20110;&#36136;&#37327;&#30340;&#22270;&#30340;&#35760;&#24518;&#65306;&#38754;&#21521;AI&#20013;&#20223;&#30495;&#20154;&#31867;&#35760;&#24518;&#30340;&#27010;&#24565;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Memory as a Mass-based Graph: Towards a Conceptual Framework for the Simulation Model of Human Memory in AI. (arXiv:2305.19274v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19274
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#36136;&#37327;&#20026;&#22522;&#30784;&#30340;&#35760;&#24518;&#22270;&#27169;&#22411;&#65292;&#29992;&#20110; AI &#20013;&#20154;&#31867;&#35760;&#24518;&#30340;&#20223;&#30495;&#12290;&#35813;&#27169;&#22411;&#21487;&#20197;&#28165;&#26224;&#22320;&#21306;&#20998;&#35760;&#24518;&#30340;&#22320;&#24418;&#24046;&#24322;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#21253;&#21547;&#19982;&#35266;&#23519;&#20107;&#23454;&#30456;&#24403;&#19968;&#33268;&#30340;&#22823;&#33041;&#27963;&#21160;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#20013;&#27169;&#25311;&#35760;&#24518;&#21644;&#23398;&#20064;&#26377;&#20004;&#31181;&#26041;&#27861;&#65306;&#21151;&#33021;&#20027;&#20041;&#26041;&#27861;&#21644;&#35748;&#30693;&#26041;&#27861;&#12290;&#23454;&#29616;&#31532;&#20108;&#31181;&#26041;&#27861;&#30340;&#24517;&#35201;&#26465;&#20214;&#26159;&#25552;&#20379;&#19968;&#20010;&#21253;&#21547;&#19982;&#35266;&#23519;&#20107;&#23454;&#30456;&#24403;&#19968;&#33268;&#30340;&#22823;&#33041;&#27963;&#21160;&#27169;&#22411;&#65292;&#20363;&#22914;&#38169;&#35823;&#21644;&#36951;&#24536;&#32463;&#21382;&#12290;&#30001;&#20110;&#20154;&#31867;&#35760;&#24518;&#20855;&#26377;&#21253;&#25324;&#25105;&#20204;&#30340;&#36523;&#20221;&#12289;&#23478;&#24237;&#21644;&#23478;&#20065;&#32452;&#25104;&#30340;&#22362;&#23454;&#26680;&#24515;&#65292;&#25105;&#20204;&#29983;&#27963;&#20013;&#30340;&#20027;&#35201;&#21644;&#20915;&#23450;&#24615;&#20107;&#20214;&#20197;&#21450;&#25105;&#20204;&#25991;&#21270;&#20013;&#26080;&#25968;&#37325;&#22797;&#21644;&#25509;&#21463;&#30340;&#20107;&#23454;&#65292;&#25105;&#20204;&#36234;&#26469;&#36234;&#23481;&#26131;&#26292;&#38706;&#20110;&#36951;&#24536;&#30340;&#22806;&#22260;&#21306;&#22495;&#30340;&#25968;&#25454;&#20063;&#36234;&#26469;&#36234;&#34180;&#24369;&#12290;&#22240;&#27492;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#28165;&#26224;&#21306;&#20998;&#22320;&#24418;&#24046;&#24322;&#30340;&#27169;&#22411;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#20013;&#65292;&#25105;&#20204;&#23558;&#36825;&#31181;&#22320;&#24418;&#24773;&#20917;&#36716;&#21270;&#20026;&#19982;&#33410;&#28857;&#30456;&#20851;&#30340;&#37327;&#12290;&#32467;&#26524;&#26159;&#19968;&#20010;&#24102;&#26377;&#22522;&#20110;&#36136;&#37327;&#30340;&#33410;&#28857;&#20540;&#30340;&#36793;&#26435;&#37325;&#22270;&#65292;&#20854;&#23637;&#31034;&#20102;...
&lt;/p&gt;
&lt;p&gt;
There are two approaches for simulating memory as well as learning in artificial intelligence; the functionalistic approach and the cognitive approach. The necessary condition to put the second approach into account is to provide a model of brain activity that contains a quite good congruence with observational facts such as mistakes and forgotten experiences. Given that human memory has a solid core that includes the components of our identity, our family and our hometown, the major and determinative events of our lives, and the countless repeated and accepted facts of our culture, the more we go to the peripheral spots the data becomes flimsier and more easily exposed to oblivion. It was essential to propose a model in which the topographical differences are quite distinguishable. In our proposed model, we have translated this topographical situation into quantities, which are attributed to the nodes. The result is an edge-weighted graph with mass-based values on the nodes which demo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#27861;&#25552;&#31034;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#19987;&#29992;&#30340;&#35821;&#27861;&#26469;&#22686;&#24378;&#31034;&#20363;&#65292;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#29305;&#23450;&#39046;&#22495;&#30340;&#35821;&#35328;&#29983;&#25104;&#20219;&#21153;&#20013;&#20351;&#29992;&#22806;&#37096;&#30693;&#35782;&#21644;&#29305;&#23450;&#32422;&#26463;&#26465;&#20214;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2305.19234</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#29305;&#23450;&#39046;&#22495;&#35821;&#35328;&#29983;&#25104;&#20013;&#30340;&#35821;&#27861;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
Grammar Prompting for Domain-Specific Language Generation with Large Language Models. (arXiv:2305.19234v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19234
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#27861;&#25552;&#31034;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#19987;&#29992;&#30340;&#35821;&#27861;&#26469;&#22686;&#24378;&#31034;&#20363;&#65292;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#29305;&#23450;&#39046;&#22495;&#30340;&#35821;&#35328;&#29983;&#25104;&#20219;&#21153;&#20013;&#20351;&#29992;&#22806;&#37096;&#30693;&#35782;&#21644;&#29305;&#23450;&#32422;&#26463;&#26465;&#20214;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21487;&#20197;&#20174;&#20165;&#26377;&#20960;&#20010;&#19978;&#19979;&#25991;&#31034;&#20363;&#20013;&#23398;&#20064;&#25191;&#34892;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#20174;&#39640;&#24230;&#32467;&#26500;&#21270;&#30340;&#35821;&#35328;&#65288;&#20363;&#22914;&#65292;&#20174;&#35821;&#20041;&#35299;&#26512;&#21040;&#22797;&#26434;&#30340;&#29305;&#23450;&#39046;&#22495;&#35821;&#35328;&#65289;&#29983;&#25104;&#23383;&#31526;&#20018;&#65292;LLM&#21482;&#20174;&#23569;&#37327;&#31034;&#20363;&#20013;&#36827;&#34892;&#27867;&#21270;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;$\textbf{&#35821;&#27861;&#25552;&#31034;}$&#20316;&#20026;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#32972;&#31185;&#26031;-&#35834;&#23572;&#33539;&#24335;&#65288;BNF&#65289;&#20013;&#34920;&#36798;&#30340;&#35821;&#27861;&#26469;&#21551;&#29992;LLM&#20351;&#29992;&#22806;&#37096;&#30693;&#35782;&#21644;&#29305;&#23450;&#39046;&#22495;&#30340;&#32422;&#26463;&#26465;&#20214;&#26469;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#12290;&#35821;&#27861;&#25552;&#31034;&#20351;&#29992;&#19968;&#20010;&#19987;&#38376;&#30340;&#35821;&#27861;&#26469;&#22686;&#24378;&#27599;&#20010;&#28436;&#31034;&#31034;&#20363;&#65292;&#35813;&#35821;&#27861;&#36275;&#20197;&#29983;&#25104;&#29305;&#23450;&#30340;&#36755;&#20986;&#31034;&#20363;&#65292;&#20854;&#20013;&#35813;&#19987;&#38376;&#30340;&#35821;&#27861;&#26159;&#20840;DSL&#35821;&#27861;&#30340;&#23376;&#38598;&#12290;&#23545;&#20110;&#25512;&#29702;&#65292;LLM&#39318;&#20808;&#39044;&#27979;&#19968;&#20010;&#32473;&#23450;&#27979;&#35797;&#36755;&#20837;&#30340;BNF&#35821;&#27861;&#65292;&#28982;&#21518;&#26681;&#25454;&#35821;&#27861;&#35268;&#21017;&#29983;&#25104;&#36755;&#20986;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#35821;&#27861;&#25552;&#31034;&#21487;&#20197;&#20351;LLM&#22312;&#29305;&#23450;&#39046;&#22495;&#30340;&#35821;&#35328;&#29983;&#25104;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) can learn to perform a wide range of natural language tasks from just a handful of in-context examples. However, for generating strings from highly structured languages (e.g., semantic parsing to complex domain-specific languages), it is challenging for the LLM to generalize from just a few exemplars. We explore $\textbf{grammar prompting}$ as a simple approach for enabling LLMs to use external knowledge and domain-specific constraints, expressed through a grammar expressed in Backus--Naur Form (BNF), during in-context learning. Grammar prompting augments each demonstration example with a specialized grammar that is minimally sufficient for generating the particular output example, where the specialized grammar is a subset of the full DSL grammar. For inference, the LLM first predicts a BNF grammar given a test input, and then generates the output according to the rules of the grammar. Experiments demonstrate that grammar prompting can enable LLMs to perfor
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;CHRT&#65292;&#23427;&#26159;&#19968;&#31181;&#21487;&#25511;&#35821;&#35328;&#29983;&#25104;&#26694;&#26550;&#65292;&#36890;&#36807;&#23398;&#20064;&#34920;&#31034;&#36716;&#25442;&#26469;&#20462;&#25913;&#22522;&#30784;&#27169;&#22411;&#30340;&#38544;&#34255;&#34920;&#31034;&#20174;&#32780;&#33719;&#24471;&#23646;&#24615;&#25511;&#21046;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;CHRT&#22312;&#19977;&#20010;&#23646;&#24615;&#19978;&#34920;&#29616;&#22343;&#20248;&#20110;&#25152;&#26377;&#22522;&#32447;&#27169;&#22411;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#20102;&#22312;&#35821;&#35328;&#36136;&#37327;&#19978;&#30340;&#25439;&#22833;&#12290;</title><link>http://arxiv.org/abs/2305.19230</link><description>&lt;p&gt;
&#36890;&#36807;&#38544;&#34255;&#34920;&#31034;&#36716;&#25442;&#23454;&#29616;&#30340;&#21487;&#25511;&#25991;&#26412;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Controlled Text Generation with Hidden Representation Transformations. (arXiv:2305.19230v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19230
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;CHRT&#65292;&#23427;&#26159;&#19968;&#31181;&#21487;&#25511;&#35821;&#35328;&#29983;&#25104;&#26694;&#26550;&#65292;&#36890;&#36807;&#23398;&#20064;&#34920;&#31034;&#36716;&#25442;&#26469;&#20462;&#25913;&#22522;&#30784;&#27169;&#22411;&#30340;&#38544;&#34255;&#34920;&#31034;&#20174;&#32780;&#33719;&#24471;&#23646;&#24615;&#25511;&#21046;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;CHRT&#22312;&#19977;&#20010;&#23646;&#24615;&#19978;&#34920;&#29616;&#22343;&#20248;&#20110;&#25152;&#26377;&#22522;&#32447;&#27169;&#22411;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#20102;&#22312;&#35821;&#35328;&#36136;&#37327;&#19978;&#30340;&#25439;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;CHRT(Control Hidden Representation Transformation)&#65292;&#23427;&#26159;&#19968;&#31181;&#21487;&#25511;&#35821;&#35328;&#29983;&#25104;&#26694;&#26550;&#65292;&#21487;&#20197;&#24341;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#29305;&#23450;&#23646;&#24615;&#30340;&#25991;&#26412;(&#22914;&#26377;&#27602;&#24615;&#25991;&#26412;)&#12290;CHRT&#36890;&#36807;&#23398;&#20064;&#34920;&#31034;&#36716;&#25442;&#20174;&#32780;&#20462;&#25913;&#22522;&#30784;&#27169;&#22411;&#30340;&#38544;&#34255;&#34920;&#31034;&#26469;&#33719;&#24471;&#23646;&#24615;&#25511;&#21046;&#12290;&#25105;&#20204;&#37319;&#29992;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#26469;&#23398;&#20064;&#36825;&#20123;&#34920;&#31034;&#36716;&#25442;&#65292;&#21487;&#20197;&#32467;&#21512;&#20351;&#29992;&#20197;&#33719;&#24471;&#22810;&#23646;&#24615;&#25511;&#21046;&#12290;&#36890;&#36807;&#22312;&#19977;&#20010;&#23646;&#24615;&#19978;&#19982;&#19971;&#20010;&#22522;&#32447;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#65292;&#23454;&#39564;&#35777;&#26126;&#20102;CHRT&#30340;&#26377;&#25928;&#24615;&#12290;CHRT&#22312;&#35299;&#27602;&#12289;&#27491;&#38754;&#24773;&#24863;&#24341;&#23548;&#21644;&#25991;&#26412;&#31616;&#21270;&#20219;&#21153;&#20013;&#34920;&#29616;&#22343;&#20248;&#20110;&#25152;&#26377;&#22522;&#32447;&#27169;&#22411;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#20102;&#22312;&#35821;&#35328;&#36136;&#37327;&#19978;&#30340;&#25439;&#22833;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#25512;&#26029;&#24310;&#36831;&#20165;&#27604;&#22522;&#30784;&#27169;&#22411;&#22810;0.01&#31186;&#65292;&#26159;&#26368;&#36866;&#21512;&#39640;&#24615;&#33021;&#29983;&#20135;&#29615;&#22659;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#24320;&#25918;&#28304;&#20195;&#30721;&#24182;&#21457;&#24067;&#20102;&#20004;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#20197;&#36827;&#19968;&#27493;&#25512;&#21160;&#21487;&#25511;&#25991;&#26412;&#29983;&#25104;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose CHRT (Control Hidden Representation Transformation) - a controlled language generation framework that steers large language models to generate text pertaining to certain attributes (such as toxicity). CHRT gains attribute control by modifying the hidden representation of the base model through learned transformations. We employ a contrastive-learning framework to learn these transformations that can be combined to gain multi-attribute control. The effectiveness of CHRT is experimentally shown by comparing it with seven baselines over three attributes. CHRT outperforms all the baselines in the task of detoxification, positive sentiment steering, and text simplification while minimizing the loss in linguistic qualities. Further, our approach has the lowest inference latency of only 0.01 seconds more than the base model, making it the most suitable for high-performance production environments. We open-source our code and release two novel datasets to further propel controlled l
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#29992;&#20110;&#35299;&#20915;&#25237;&#24433;&#27169;&#22411;&#35745;&#25968;&#38382;&#39064;&#30340;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#21033;&#29992;&#36755;&#20837;&#23454;&#20363;&#21407;&#22987;&#22270;&#30340;&#23567;&#26641;&#23485;&#65292;&#31639;&#27861;&#36816;&#34892;&#26102;&#38388;&#20026;O(2^2k+4n2)&#65292;&#20854;&#20013;k&#26159;&#26641;&#23485;&#65292;n&#26159;&#23454;&#20363;&#30340;&#36755;&#20837;&#22823;&#23567;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;PMC&#34987;&#38480;&#21046;&#26641;&#23485;&#31639;&#27861;&#30340;&#19979;&#30028;&#65292;&#20854;&#36816;&#34892;&#26102;&#38480;&#30028;&#24050;&#36798;&#21040;&#28176;&#36827;&#25511;&#21046;&#12290;</title><link>http://arxiv.org/abs/2305.19212</link><description>&lt;p&gt;
&#21033;&#29992;&#26641;&#23485;&#21450;&#20854;&#38480;&#21046;&#27714;&#35299;&#25237;&#24433;&#27169;&#22411;&#35745;&#25968;
&lt;/p&gt;
&lt;p&gt;
Solving Projected Model Counting by Utilizing Treewidth and its Limits. (arXiv:2305.19212v2 [cs.CC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19212
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#29992;&#20110;&#35299;&#20915;&#25237;&#24433;&#27169;&#22411;&#35745;&#25968;&#38382;&#39064;&#30340;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#21033;&#29992;&#36755;&#20837;&#23454;&#20363;&#21407;&#22987;&#22270;&#30340;&#23567;&#26641;&#23485;&#65292;&#31639;&#27861;&#36816;&#34892;&#26102;&#38388;&#20026;O(2^2k+4n2)&#65292;&#20854;&#20013;k&#26159;&#26641;&#23485;&#65292;n&#26159;&#23454;&#20363;&#30340;&#36755;&#20837;&#22823;&#23567;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;PMC&#34987;&#38480;&#21046;&#26641;&#23485;&#31639;&#27861;&#30340;&#19979;&#30028;&#65292;&#20854;&#36816;&#34892;&#26102;&#38480;&#30028;&#24050;&#36798;&#21040;&#28176;&#36827;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#26469;&#35299;&#20915;&#25237;&#24433;&#27169;&#22411;&#35745;&#25968;&#65288;PMC&#65289;&#12290;PMC&#35201;&#27714;&#25353;&#29031;&#32473;&#23450;&#30340;&#25237;&#24433;&#21464;&#37327;&#38598;&#21512;&#35745;&#31639;&#24067;&#23572;&#20844;&#24335;&#30340;&#35299;&#30340;&#25968;&#37327;&#65292;&#20854;&#20013;&#24403;&#38480;&#21046;&#20026;&#25237;&#24433;&#21464;&#37327;&#26102;&#65292;&#22810;&#20010;&#35299;&#30456;&#21516;&#26102;&#21482;&#35745;&#31639;&#19968;&#20010;&#35299;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#21033;&#29992;&#36755;&#20837;&#23454;&#20363;&#21407;&#22987;&#22270;&#30340;&#23567;&#26641;&#23485;&#65292;&#36825;&#20010;&#35266;&#23519;&#32467;&#26524;&#34920;&#31034;&#26641;&#23485;&#26159;&#26368;&#31361;&#20986;&#30340;&#32467;&#26500;&#21442;&#25968;&#20043;&#19968;&#12290;&#26356;&#30830;&#20999;&#22320;&#35828;&#65292;&#23427;&#30340;&#36816;&#34892;&#26102;&#38388;&#20026;O(2^2k+4n2)&#65292;&#20854;&#20013;k&#26159;&#26641;&#23485;&#65292;n&#26159;&#23454;&#20363;&#30340;&#36755;&#20837;&#22823;&#23567;&#12290; &#25442;&#21477;&#35805;&#35828;&#65292;&#24403;&#32473;&#23450;&#26641;&#23485;&#21442;&#25968;&#26102;&#65292;&#25105;&#20204;&#24471;&#21040;PMG&#38382;&#39064;&#22266;&#23450;&#21442;&#25968;&#21487;&#35299;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#32771;&#34385;&#25351;&#25968;&#26102;&#38388;&#20551;&#35774;&#65288;ETH&#65289;&#65292;&#24182;&#24314;&#31435;&#20102;PMC&#34987;&#38480;&#21046;&#26641;&#23485;&#31639;&#27861;&#30340;&#19979;&#30028;&#65292;&#20174;&#32780;&#24471;&#21040;&#20102;&#25105;&#20204;&#31639;&#27861;&#30340;&#28176;&#36827;&#32039;&#23494;&#36816;&#34892;&#26102;&#38480;&#30028;&#12290;&#34429;&#28982;&#19978;&#36848;&#31639;&#27861;&#21487;&#20197;&#20316;&#20026;&#31532;&#19968;&#20010;&#29702;&#35770;&#19978;&#30028;&#65292;&#20294;...
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce a novel algorithm to solve projected model counting (PMC). PMC asks to count solutions of a Boolean formula with respect to a given set of projection variables, where multiple solutions that are identical when restricted to the projection variables count as only one solution. Inspired by the observation that the so-called "treewidth" is one of the most prominent structural parameters, our algorithm utilizes small treewidth of the primal graph of the input instance. More precisely, it runs in time O(2^2k+4n2) where k is the treewidth and n is the input size of the instance. In other words, we obtain that the problem PMC is fixed-parameter tractable when parameterized by treewidth. Further, we take the exponential time hypothesis (ETH) into consideration and establish lower bounds of bounded treewidth algorithms for PMC, yielding asymptotically tight runtime bounds of our algorithm. While the algorithm above serves as a first theoretical upper bound and althou
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#22270;&#20687;-&#25991;&#26412;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#25968;&#25454;&#29983;&#25104;&#21644;&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#30340;3D&#29983;&#25104;&#32593;&#32476;&#36827;&#34892;&#35757;&#32451;&#65292;&#20197;&#29983;&#25104;&#39640;&#36136;&#37327;&#12289;&#39118;&#26684;&#21270;&#30340;3D&#22836;&#20687;&#65292;&#21516;&#26102;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#22686;&#21152;&#20102;&#29616;&#26377;3D&#27169;&#22411;&#20013;&#25552;&#21462;&#30340;&#23039;&#21183;&#26469;&#24341;&#23548;&#22810;&#35270;&#35282;&#30340;&#22270;&#20687;&#29983;&#25104;&#65292;&#24182;&#25552;&#20986;&#20102;&#35270;&#28857;&#29305;&#23450;&#25552;&#31034;&#12289;&#31895;&#21040;&#32454;&#30340;GAN&#37492;&#21035;&#22120;&#20197;&#21450;&#23646;&#24615;&#30456;&#20851;&#25552;&#31034;&#31561;&#26041;&#27861;&#20197;&#22686;&#21152;&#22810;&#26679;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.19012</link><description>&lt;p&gt;
StyleAvatar3D&#65306;&#21033;&#29992;&#22270;&#20687;-&#25991;&#26412;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#39640;&#20445;&#30495;3D&#22836;&#20687;&#30340;&#26032;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
StyleAvatar3D: Leveraging Image-Text Diffusion Models for High-Fidelity 3D Avatar Generation. (arXiv:2305.19012v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19012
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#22270;&#20687;-&#25991;&#26412;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#25968;&#25454;&#29983;&#25104;&#21644;&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#30340;3D&#29983;&#25104;&#32593;&#32476;&#36827;&#34892;&#35757;&#32451;&#65292;&#20197;&#29983;&#25104;&#39640;&#36136;&#37327;&#12289;&#39118;&#26684;&#21270;&#30340;3D&#22836;&#20687;&#65292;&#21516;&#26102;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#22686;&#21152;&#20102;&#29616;&#26377;3D&#27169;&#22411;&#20013;&#25552;&#21462;&#30340;&#23039;&#21183;&#26469;&#24341;&#23548;&#22810;&#35270;&#35282;&#30340;&#22270;&#20687;&#29983;&#25104;&#65292;&#24182;&#25552;&#20986;&#20102;&#35270;&#28857;&#29305;&#23450;&#25552;&#31034;&#12289;&#31895;&#21040;&#32454;&#30340;GAN&#37492;&#21035;&#22120;&#20197;&#21450;&#23646;&#24615;&#30456;&#20851;&#25552;&#31034;&#31561;&#26041;&#27861;&#20197;&#22686;&#21152;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22270;&#20687;-&#25991;&#26412;&#25193;&#25955;&#27169;&#22411;&#30340;&#36827;&#23637;&#21050;&#28608;&#20102;&#22823;&#35268;&#27169;3D&#29983;&#25104;&#27169;&#22411;&#30340;&#30740;&#31350;&#20852;&#36259;&#12290;&#28982;&#32780;&#65292;&#26377;&#38480;&#30340;&#22810;&#26679;&#21270;3D&#36164;&#28304;&#30340;&#21487;&#29992;&#24615;&#23545;&#23398;&#20064;&#25552;&#20986;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#22270;&#20687;-&#25991;&#26412;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#25968;&#25454;&#29983;&#25104;&#21644;&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#30340;3D&#29983;&#25104;&#32593;&#32476;&#36827;&#34892;&#35757;&#32451;&#65292;&#20197;&#29983;&#25104;&#39640;&#36136;&#37327;&#12289;&#39118;&#26684;&#21270;&#30340;3D&#22836;&#20687;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#22270;&#20687;-&#25991;&#26412;&#25193;&#25955;&#27169;&#22411;&#25552;&#20379;&#30340;&#23436;&#25972;&#22806;&#35266;&#21644;&#20960;&#20309;&#20808;&#39564;&#29983;&#25104;&#21508;&#31181;&#39118;&#26684;&#30340;&#22810;&#35270;&#35282;&#22836;&#20687;&#22270;&#20687;&#12290;&#22312;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#29616;&#26377;3D&#27169;&#22411;&#20013;&#25552;&#21462;&#30340;&#23039;&#21183;&#26469;&#24341;&#23548;&#22810;&#35270;&#35282;&#22270;&#20687;&#30340;&#29983;&#25104;&#12290;&#20026;&#20102;&#35299;&#20915;&#25968;&#25454;&#20013;&#23039;&#21183;&#21644;&#22270;&#20687;&#30340;&#19981;&#23545;&#40784;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#35270;&#28857;&#29305;&#23450;&#25552;&#31034;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#31895;&#21040;&#32454;&#30340;GAN&#37492;&#21035;&#22120;&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#36824;&#28145;&#20837;&#30740;&#31350;&#20102;&#23646;&#24615;&#30456;&#20851;&#25552;&#31034;&#20197;&#22686;&#21152;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent advancements in image-text diffusion models have stimulated research interest in large-scale 3D generative models. Nevertheless, the limited availability of diverse 3D resources presents significant challenges to learning. In this paper, we present a novel method for generating high-quality, stylized 3D avatars that utilizes pre-trained image-text diffusion models for data generation and a Generative Adversarial Network (GAN)-based 3D generation network for training. Our method leverages the comprehensive priors of appearance and geometry offered by image-text diffusion models to generate multi-view images of avatars in various styles. During data generation, we employ poses extracted from existing 3D models to guide the generation of multi-view images. To address the misalignment between poses and images in data, we investigate view-specific prompts and develop a coarse-to-fine discriminator for GAN training. We also delve into attribute-related prompts to increase the dive
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#32435;&#31859;&#20809;&#23376;&#23398;&#22120;&#20214;&#21453;&#21521;&#35774;&#35745;&#22522;&#20934;&#27979;&#35797;&#65292;&#20197;&#24110;&#21161;&#20154;&#20204;&#36827;&#34892;&#26131;&#20110;&#29702;&#35299;&#21644;&#21487;&#37325;&#22797;&#30340;&#31185;&#23398;&#35774;&#35745;&#12290;&#24320;&#21457;&#20102;&#19968;&#20010;&#24320;&#28304;&#24037;&#20855;&#31665;IDToolkit&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#27169;&#25311;&#21644;&#20248;&#21270;&#22120;&#27169;&#22359;&#20197;&#21450;&#21518;&#22788;&#29702;&#32467;&#26524;&#30340;&#20989;&#25968;&#65292;&#21487;&#29992;&#20110;&#19982;&#22522;&#20934;&#26041;&#27861;&#27604;&#36739;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.18978</link><description>&lt;p&gt;
IDToolkit: &#29992;&#20110;&#32435;&#31859;&#20809;&#23376;&#23398;&#21453;&#21521;&#35774;&#35745;&#31639;&#27861;&#22522;&#20934;&#27979;&#35797;&#21644;&#24320;&#21457;&#30340;&#24037;&#20855;&#31665;
&lt;/p&gt;
&lt;p&gt;
IDToolkit: A Toolkit for Benchmarking and Developing Inverse Design Algorithms in Nanophotonics. (arXiv:2305.18978v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18978
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#32435;&#31859;&#20809;&#23376;&#23398;&#22120;&#20214;&#21453;&#21521;&#35774;&#35745;&#22522;&#20934;&#27979;&#35797;&#65292;&#20197;&#24110;&#21161;&#20154;&#20204;&#36827;&#34892;&#26131;&#20110;&#29702;&#35299;&#21644;&#21487;&#37325;&#22797;&#30340;&#31185;&#23398;&#35774;&#35745;&#12290;&#24320;&#21457;&#20102;&#19968;&#20010;&#24320;&#28304;&#24037;&#20855;&#31665;IDToolkit&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#27169;&#25311;&#21644;&#20248;&#21270;&#22120;&#27169;&#22359;&#20197;&#21450;&#21518;&#22788;&#29702;&#32467;&#26524;&#30340;&#20989;&#25968;&#65292;&#21487;&#29992;&#20110;&#19982;&#22522;&#20934;&#26041;&#27861;&#27604;&#36739;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24110;&#21161;&#20154;&#31867;&#36827;&#34892;&#31185;&#23398;&#35774;&#35745;&#26159;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#21644;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#26368;&#20196;&#20154;&#20852;&#22859;&#30340;&#39046;&#22495;&#20043;&#19968;&#65292;&#22240;&#20026;&#23427;&#20204;&#20855;&#26377;&#21457;&#29616;&#26032;&#33647;&#29289;&#12289;&#35774;&#35745;&#26032;&#26448;&#26009;&#21644;&#21270;&#21512;&#29289;&#31561;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#31185;&#23398;&#35774;&#35745;&#36890;&#24120;&#38656;&#35201;&#29087;&#24713;&#39046;&#22495;&#30693;&#35782;&#30340;&#19987;&#19994;&#25216;&#33021;&#65292;&#36825;&#20123;&#23545;&#20110;AI&#30740;&#31350;&#20154;&#21592;&#26469;&#35828;&#24182;&#19981;&#29087;&#24713;&#12290;&#27492;&#22806;&#65292;&#31185;&#23398;&#30740;&#31350;&#38656;&#35201;&#19987;&#19994;&#30340;&#23454;&#39564;&#21644;&#35780;&#20272;&#25216;&#33021;&#12290;&#36825;&#20123;&#38556;&#30861;&#38459;&#30861;&#20102;AI&#30740;&#31350;&#20154;&#21592;&#24320;&#21457;&#19987;&#38376;&#29992;&#20110;&#31185;&#23398;&#35774;&#35745;&#30340;&#26041;&#27861;&#12290;&#20026;&#36808;&#21521;&#26131;&#20110;&#29702;&#35299;&#21644;&#21487;&#37325;&#22797;&#30740;&#31350;&#30340;&#31185;&#23398;&#35774;&#35745;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#32435;&#31859;&#20809;&#23376;&#23398;&#22120;&#20214;&#21453;&#21521;&#35774;&#35745;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#21487;&#22312;&#35745;&#31639;&#19978;&#21644;&#20934;&#30830;&#22320;&#39564;&#35777;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;&#32435;&#31859;&#20809;&#23376;&#23398;&#35774;&#35745;&#38382;&#39064;&#65292;&#20998;&#21035;&#26159;&#36752;&#23556;&#20919;&#21364;&#22120;&#65292;&#36866;&#29992;&#20110;&#28909;&#20809;&#20239;&#36873;&#25321;&#24615;&#21457;&#23556;&#20307;&#20197;&#21450;&#32467;&#26500;&#33394;&#28388;&#20809;&#22120;&#65292;&#23427;&#20204;&#22312;&#35774;&#35745;&#21442;&#25968;&#31354;&#38388;&#12289;&#22797;&#26434;&#24230;&#21644;&#29289;&#29702;&#24615;&#36136;&#19978;&#37117;&#19981;&#21516;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;IDToolkit&#65292;&#19968;&#20010;&#24320;&#28304;&#30340;Python&#24037;&#20855;&#31665;&#65292;&#20197;&#20026;&#29992;&#25143;&#25552;&#20379;&#19982;&#22522;&#20934;&#26041;&#27861;&#27604;&#36739;&#20854;&#31639;&#27861;&#30340;&#31616;&#21333;&#25509;&#21475;&#12290;IDToolkit&#21253;&#21547;&#20102;&#21453;&#21521;&#35774;&#35745;&#25152;&#38656;&#30340;&#20960;&#20010;&#27169;&#22359;&#65292;&#20363;&#22914;&#27169;&#25311;&#27169;&#22359;&#21644;&#20248;&#21270;&#22120;&#27169;&#22359;&#65292;&#20197;&#21450;&#21518;&#22788;&#29702;&#32467;&#26524;&#30340;&#20989;&#25968;&#12290;&#25105;&#20204;&#24076;&#26395;&#25152;&#25552;&#20986;&#30340;&#22522;&#20934;&#27979;&#35797;&#21644;&#24037;&#20855;&#21253;&#23558;&#21152;&#36895;&#32435;&#31859;&#20809;&#23376;&#23398;&#31185;&#23398;&#35774;&#35745;&#26032;&#26041;&#27861;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Aiding humans with scientific designs is one of the most exciting of artificial intelligence (AI) and machine learning (ML), due to their potential for the discovery of new drugs, design of new materials and chemical compounds, etc. However, scientific design typically requires complex domain knowledge that is not familiar to AI researchers. Further, scientific studies involve professional skills to perform experiments and evaluations. These obstacles prevent AI researchers from developing specialized methods for scientific designs. To take a step towards easy-to-understand and reproducible research of scientific design, we propose a benchmark for the inverse design of nanophotonic devices, which can be verified computationally and accurately. Specifically, we implemented three different nanophotonic design problems, namely a radiative cooler, a selective emitter for thermophotovoltaics, and structural color filters, all of which are different in design parameter spaces, complexity, an
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#20445;&#30495;&#24230;&#30340;&#25991;&#26412;&#21040;3D&#22270;&#20687;&#21512;&#25104;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#20808;&#36827;&#30340;&#25193;&#25955;&#24341;&#23548;&#31574;&#30053;&#12290;&#36890;&#36807;&#23545;NeRF&#28210;&#26579;&#22270;&#20687;&#36827;&#34892;&#36741;&#21161;&#28145;&#24230;&#30417;&#30563;&#21644;&#35268;&#33539;&#21270;&#23494;&#24230;&#22330;&#26469;&#25552;&#39640;3D&#20960;&#20309;&#34920;&#31034;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#20248;&#20110;&#20197;&#21069;&#30340;&#24037;&#20316;&#65292;&#20135;&#29983;&#20102;&#20808;&#36827;&#30340;&#29031;&#29255;&#30495;&#23454;&#24863;&#21644;&#25913;&#36827;&#30340;&#22810;&#35270;&#35282;&#19968;&#33268;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.18766</link><description>&lt;p&gt;
HiFA: &#39640;&#20445;&#30495;&#24230;&#30340;&#25991;&#26412;&#21040;3D&#22270;&#20687;&#21512;&#25104;&#21450;&#20854;&#20808;&#36827;&#30340;&#25193;&#25955;&#24341;&#23548;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
HiFA: High-fidelity Text-to-3D with Advanced Diffusion Guidance. (arXiv:2305.18766v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18766
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#20445;&#30495;&#24230;&#30340;&#25991;&#26412;&#21040;3D&#22270;&#20687;&#21512;&#25104;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#20808;&#36827;&#30340;&#25193;&#25955;&#24341;&#23548;&#31574;&#30053;&#12290;&#36890;&#36807;&#23545;NeRF&#28210;&#26579;&#22270;&#20687;&#36827;&#34892;&#36741;&#21161;&#28145;&#24230;&#30417;&#30563;&#21644;&#35268;&#33539;&#21270;&#23494;&#24230;&#22330;&#26469;&#25552;&#39640;3D&#20960;&#20309;&#34920;&#31034;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#20248;&#20110;&#20197;&#21069;&#30340;&#24037;&#20316;&#65292;&#20135;&#29983;&#20102;&#20808;&#36827;&#30340;&#29031;&#29255;&#30495;&#23454;&#24863;&#21644;&#25913;&#36827;&#30340;&#22810;&#35270;&#35282;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20248;&#21270;3D&#27169;&#22411;&#65292;&#33258;&#21160;&#25991;&#26412;&#21040;3D&#21512;&#25104;&#22312;&#25552;&#21319;&#20013;&#24050;&#32463;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#39044;&#35757;&#32451;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#65288;&#22914;&#25193;&#25955;&#27169;&#22411;&#65289;&#65292;&#25552;&#20379;&#31070;&#32463;&#36752;&#23556;&#22330;&#65288;NeRFs&#65289;&#30340;2D&#28210;&#26579;&#24471;&#20998;&#24182;&#29992;&#20110;&#20248;&#21270;NeRFs&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#23545;3D&#20960;&#20309;&#30340;&#26377;&#38480;&#29702;&#35299;&#65292;&#36825;&#20123;&#26041;&#27861;&#32463;&#24120;&#36935;&#21040;&#22810;&#20010;&#35270;&#35282;&#19978;&#30340;&#20266;&#24433;&#21644;&#19981;&#19968;&#33268;&#29616;&#35937;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20351;&#29992;&#25193;&#25955;&#20808;&#39564;&#37325;&#26032;&#21046;&#23450;&#20248;&#21270;&#25439;&#22833;&#30340;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#37322;&#25918;&#20102;&#25193;&#25955;&#20808;&#39564;&#30340;&#28508;&#21147;&#12290;&#20026;&#20102;&#25552;&#39640;3D&#20960;&#20309;&#34920;&#31034;&#65292;&#25105;&#20204;&#23545;NeRF&#28210;&#26579;&#22270;&#20687;&#36827;&#34892;&#36741;&#21161;&#28145;&#24230;&#30417;&#30563;&#65292;&#24182;&#35268;&#33539;&#21270;NeRF&#30340;&#23494;&#24230;&#22330;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#20197;&#21069;&#30340;&#24037;&#20316;&#65292;&#20135;&#29983;&#20102;&#20808;&#36827;&#30340;&#29031;&#29255;&#30495;&#23454;&#24863;&#21644;&#25913;&#36827;&#30340;&#22810;&#35270;&#35282;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic text-to-3D synthesis has achieved remarkable advancements through the optimization of 3D models. Existing methods commonly rely on pre-trained text-to-image generative models, such as diffusion models, providing scores for 2D renderings of Neural Radiance Fields (NeRFs) and being utilized for optimizing NeRFs. However, these methods often encounter artifacts and inconsistencies across multiple views due to their limited understanding of 3D geometry. To address these limitations, we propose a reformulation of the optimization loss using the diffusion prior. Furthermore, we introduce a novel training approach that unlocks the potential of the diffusion prior. To improve 3D geometry representation, we apply auxiliary depth supervision for NeRF-rendered images and regularize the density field of NeRFs. Extensive experiments demonstrate the superiority of our method over prior works, resulting in advanced photo-realism and improved multi-view consistency.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20219;&#21153;&#31561;&#21464;&#22270;Few-shot&#23398;&#20064;&#65288;TEG&#65289;&#26694;&#26550;&#65292;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#31561;&#21464;&#24615;&#36136;&#26469;&#20351;&#27169;&#22411;&#23398;&#20064;&#21487;&#36716;&#31227;&#30340;&#20219;&#21153;&#36866;&#24212;&#31574;&#30053;&#12290;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;Few-shot&#20998;&#31867;&#22522;&#20934;&#19978;&#23637;&#31034;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.18758</link><description>&lt;p&gt;
&#20219;&#21153;&#31561;&#21464;&#22270;Few-shot&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Task-Equivariant Graph Few-shot Learning. (arXiv:2305.18758v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18758
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20219;&#21153;&#31561;&#21464;&#22270;Few-shot&#23398;&#20064;&#65288;TEG&#65289;&#26694;&#26550;&#65292;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#31561;&#21464;&#24615;&#36136;&#26469;&#20351;&#27169;&#22411;&#23398;&#20064;&#21487;&#36716;&#31227;&#30340;&#20219;&#21153;&#36866;&#24212;&#31574;&#30053;&#12290;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;Few-shot&#20998;&#31867;&#22522;&#20934;&#19978;&#23637;&#31034;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#22312;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#20854;&#24615;&#33021;&#20005;&#37325;&#20381;&#36182;&#20110;&#27599;&#31867;&#20855;&#26377;&#36275;&#22815;&#26631;&#35760;&#33410;&#28857;&#30340;&#21487;&#29992;&#24615;&#12290;&#22312;&#29616;&#23454;&#24773;&#20917;&#19979;&#65292;&#19981;&#26159;&#25152;&#26377;&#31867;&#37117;&#26377;&#35768;&#22810;&#26631;&#35760;&#33410;&#28857;&#65292;&#27169;&#22411;&#21487;&#33021;&#38656;&#35201;&#20998;&#31867;&#26032;&#31867;&#21035;&#65292;&#20351;&#24471;&#25163;&#21160;&#26631;&#35760;&#21464;&#24471;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;GNN&#38656;&#35201;&#33021;&#22815;&#29992;&#26377;&#38480;&#25968;&#37327;&#30340;&#26631;&#35760;&#33410;&#28857;&#23545;&#33410;&#28857;&#36827;&#34892;&#20998;&#31867;&#65292;&#31216;&#20026;Few-shot&#33410;&#28857;&#20998;&#31867;&#12290;&#20808;&#21069;&#30340;&#22522;&#20110;&#21095;&#38598;&#20803;&#23398;&#20064;&#30340;&#26041;&#27861;&#22312;Few-shot&#33410;&#28857;&#20998;&#31867;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#20165;&#26377;&#22810;&#26679;&#30340;&#35757;&#32451;&#20803;&#20219;&#21153;&#25165;&#33021;&#23454;&#29616;&#26368;&#20339;&#24615;&#33021;&#12290;&#20026;&#20102;&#24212;&#23545;&#22522;&#20110;&#20803;&#23398;&#20064;&#30340;Few-shot&#23398;&#20064;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21363;&#20219;&#21153;&#31561;&#21464;&#22270;Few-shot&#23398;&#20064;&#65288;TEG&#65289;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;TEG&#26694;&#26550;&#36890;&#36807;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#31561;&#21464;&#24615;&#36136;&#26469;&#20351;&#27169;&#22411;&#23398;&#20064;&#21487;&#36716;&#31227;&#30340;&#20219;&#21153;&#36866;&#24212;&#31574;&#30053;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;Few-shot&#20998;&#31867;&#22522;&#20934;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although Graph Neural Networks (GNNs) have been successful in node classification tasks, their performance heavily relies on the availability of a sufficient number of labeled nodes per class. In real-world situations, not all classes have many labeled nodes and there may be instances where the model needs to classify new classes, making manual labeling difficult. To solve this problem, it is important for GNNs to be able to classify nodes with a limited number of labeled nodes, known as few-shot node classification. Previous episodic meta-learning based methods have demonstrated success in few-shot node classification, but our findings suggest that optimal performance can only be achieved with a substantial amount of diverse training meta-tasks. To address this challenge of meta-learning based few-shot learning (FSL), we propose a new approach, the Task-Equivariant Graph few-shot learning (TEG) framework. Our TEG framework enables the model to learn transferable task-adaptation strate
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35748;&#30693;&#22270;&#30340;&#28151;&#21512;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#32467;&#26500;&#21270;&#30693;&#35782;&#19982;&#25968;&#25454;&#26679;&#26412;&#26080;&#32541;&#38598;&#25104;&#26469;&#23454;&#29616;&#26356;&#26377;&#25928;&#30340;&#34920;&#31034;&#23398;&#20064;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.18731</link><description>&lt;p&gt;
&#22522;&#20110;&#35748;&#30693;&#22270;&#30340;&#28151;&#21512;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Hybrid Representation Learning via Epistemic Graph. (arXiv:2305.18731v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18731
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35748;&#30693;&#22270;&#30340;&#28151;&#21512;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#32467;&#26500;&#21270;&#30693;&#35782;&#19982;&#25968;&#25454;&#26679;&#26412;&#26080;&#32541;&#38598;&#25104;&#26469;&#23454;&#29616;&#26356;&#26377;&#25928;&#30340;&#34920;&#31034;&#23398;&#20064;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#27169;&#22411;&#22312;&#35768;&#22810;&#35270;&#35273;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#20294;&#26159;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#24456;&#22823;&#31243;&#24230;&#19978;&#21462;&#20915;&#20110;&#23494;&#38598;&#30340;&#35757;&#32451;&#26679;&#26412;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#20154;&#31867;&#36890;&#24120;&#25191;&#34892;&#28151;&#21512;&#23398;&#20064;&#65292;&#20363;&#22914;&#33258;&#21457;&#22320;&#23558;&#32467;&#26500;&#21270;&#30693;&#35782;&#29992;&#20110;&#36328;&#39046;&#22495;&#35782;&#21035;&#65292;&#25110;&#32773;&#20165;&#20351;&#29992;&#26497;&#23569;&#37327;&#30340;&#25968;&#25454;&#26679;&#26412;&#36827;&#34892;&#23569;&#26679;&#26412;&#23398;&#20064;&#12290;&#22240;&#27492;&#65292;&#36890;&#36807;&#26080;&#32541;&#22320;&#23558;&#32467;&#26500;&#21270;&#30693;&#35782;&#19982;&#25968;&#25454;&#26679;&#26412;&#38598;&#25104;&#65292;&#23454;&#29616;&#26356;&#26377;&#25928;&#30340;&#34920;&#31034;&#23398;&#20064;&#65292;&#21487;&#20197;&#23558;&#36825;&#31181;&#28151;&#21512;&#23398;&#20064;&#26041;&#27861;&#25193;&#23637;&#21040;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#20013;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32467;&#26500;&#21270;&#30693;&#35782;&#21644;&#28145;&#24230;&#29305;&#24449;&#65288;&#20174;&#25968;&#25454;&#26679;&#26412;&#20013;&#23398;&#20064;&#65289;&#22312;&#32500;&#24230;&#21644;&#30693;&#35782;&#31890;&#24230;&#19978;&#23384;&#22312;&#24040;&#22823;&#24046;&#36317;&#65292;&#22240;&#27492;&#36825;&#26679;&#30340;&#28151;&#21512;&#23398;&#20064;&#26041;&#27861;&#20173;&#28982;&#26159;&#19968;&#20010;&#24040;&#22823;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35748;&#30693;&#22270;&#23618;&#65288;EGLayer&#65289;&#65292;&#20197;&#23454;&#29616;&#28151;&#21512;&#23398;&#20064;&#65292;&#20174;&#32780;&#22312;&#28145;&#24230;&#29305;&#24449;&#21644;&#32467;&#26500;&#21270;&#30693;&#35782;&#22270;&#20043;&#38388;&#26356;&#26377;&#25928;&#22320;&#20132;&#25442;&#20449;&#24687;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;EGLayer&#23558;&#28145;&#24230;&#29305;&#24449;&#20316;&#20026;&#36755;&#20837;&#65292;&#24182;&#26500;&#24314;&#30693;&#35782;&#22270;&#65292;&#23558;&#32467;&#26500;&#21270;&#30693;&#35782;&#20256;&#25773;&#21040;&#28145;&#24230;&#29305;&#24449;&#19978;&#12290;&#36825;&#31181;&#28151;&#21512;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#65288;&#21253;&#25324;&#23569;&#26679;&#26412;&#23398;&#20064;&#21644;&#38646;&#26679;&#26412;&#20998;&#31867;&#20219;&#21153;&#65289;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#24182;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#21462;&#24471;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, deep models have achieved remarkable success in many vision tasks. Unfortunately, their performance largely depends on intensive training samples. In contrast, human beings typically perform hybrid learning, e.g., spontaneously integrating structured knowledge for cross-domain recognition or on a much smaller amount of data samples for few-shot learning. Thus it is very attractive to extend hybrid learning for the computer vision tasks by seamlessly integrating structured knowledge with data samples to achieve more effective representation learning. However, such a hybrid learning approach remains a great challenge due to the huge gap between the structured knowledge and the deep features (learned from data samples) on both dimensions and knowledge granularity. In this paper, a novel Epistemic Graph Layer (EGLayer) is developed to enable hybrid learning, such that the information can be exchanged more effectively between the deep features and a structured knowledge gra
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39046;&#22495;&#19987;&#38376;&#21270;&#65292;&#21253;&#25324;&#21160;&#26426;&#12289;&#25361;&#25112;&#12289;&#26041;&#27861;&#35770;&#21644;&#35780;&#20272;&#25351;&#26631;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#29305;&#23450;&#39046;&#22495;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#30340;&#20998;&#31867;&#27861;&#65292;&#23545;&#29616;&#26377;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#21644;&#23450;&#21046;&#25216;&#26415;&#36827;&#34892;&#20102;&#35814;&#32454;&#27604;&#36739;&#65292;&#24182;&#24191;&#27867;&#35752;&#35770;&#20102;&#36825;&#19968;&#39046;&#22495;&#20013;&#30340;&#26410;&#35299;&#20915;&#38382;&#39064;&#21644;&#26410;&#26469;&#30340;&#21457;&#23637;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2305.18703</link><description>&lt;p&gt;
&#36229;&#36234;&#19968;&#20010;&#27169;&#22411;&#36866;&#29992;&#20110;&#25152;&#26377;&#39046;&#22495;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39046;&#22495;&#19987;&#38376;&#21270;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Beyond One-Model-Fits-All: A Survey of Domain Specialization for Large Language Models. (arXiv:2305.18703v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18703
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39046;&#22495;&#19987;&#38376;&#21270;&#65292;&#21253;&#25324;&#21160;&#26426;&#12289;&#25361;&#25112;&#12289;&#26041;&#27861;&#35770;&#21644;&#35780;&#20272;&#25351;&#26631;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#29305;&#23450;&#39046;&#22495;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#30340;&#20998;&#31867;&#27861;&#65292;&#23545;&#29616;&#26377;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#21644;&#23450;&#21046;&#25216;&#26415;&#36827;&#34892;&#20102;&#35814;&#32454;&#27604;&#36739;&#65292;&#24182;&#24191;&#27867;&#35752;&#35770;&#20102;&#36825;&#19968;&#39046;&#22495;&#20013;&#30340;&#26410;&#35299;&#20915;&#38382;&#39064;&#21644;&#26410;&#26469;&#30340;&#21457;&#23637;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24050;&#32463;&#22823;&#22823;&#25512;&#21160;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#30340;&#21457;&#23637;&#65292;&#20026;&#24191;&#27867;&#24212;&#29992;&#25552;&#20379;&#20102;&#39640;&#24230;&#23454;&#29992;&#12289;&#20219;&#21153;&#26080;&#20851;&#30340;&#22522;&#30784;&#12290;LLMs &#20316;&#20026;&#36890;&#29992;&#20219;&#21153;&#27714;&#35299;&#22120;&#30340;&#24040;&#22823;&#28508;&#21147;&#65292;&#20419;&#20351;&#20154;&#20204;&#23558;&#20854;&#29992;&#20110;&#29305;&#23450;&#39046;&#22495;&#65292;&#22914;&#21307;&#30103;&#20445;&#20581;&#12289;&#37329;&#34701;&#21644;&#25945;&#32946;&#65292;&#24182;&#23558;&#20854;&#29992;&#20316;&#21161;&#25163;&#29978;&#33267;&#26367;&#20195;&#29305;&#23450;&#39046;&#22495;&#30340;&#19987;&#23478;&#21644;&#24037;&#20855;&#12290;&#20294;&#26159;&#65292;&#23558;LLMs&#30452;&#25509;&#24212;&#29992;&#20110;&#29305;&#23450;&#39046;&#22495;&#20013;&#30340;&#22797;&#26434;&#38382;&#39064;&#20250;&#36935;&#21040;&#35768;&#22810;&#22256;&#38590;&#65292;&#21253;&#25324;&#39046;&#22495;&#25968;&#25454;&#30340;&#24322;&#36136;&#24615;&#12289;&#39046;&#22495;&#30693;&#35782;&#30340;&#22797;&#26434;&#24615;&#12289;&#39046;&#22495;&#30446;&#26631;&#30340;&#29420;&#29305;&#24615;&#20197;&#21450;&#32422;&#26463;&#30340;&#22810;&#26679;&#24615;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#31181;&#24046;&#36317;&#65292;&#26368;&#36817;&#20960;&#24180;&#36827;&#34892;&#20102;&#24613;&#21095;&#22686;&#21152;&#30340;&#30740;&#31350;&#21644;&#23454;&#36341;&#33268;&#21147;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39046;&#22495;&#19987;&#38376;&#21270;&#65292;&#28982;&#32780;&#36825;&#26041;&#38754;&#30340;&#30740;&#31350;&#23578;&#26410;&#34987;&#31995;&#32479;&#22320;&#24635;&#32467;&#12290;&#22312;&#36825;&#31687;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#23545;LLMs&#30340;&#39046;&#22495;&#19987;&#38376;&#21270;&#36827;&#34892;&#20102;&#20840;&#38754;&#27010;&#36848;&#65292;&#21253;&#25324;&#21160;&#26426;&#12289;&#25361;&#25112;&#12289;&#26041;&#27861;&#35770;&#21644;&#35780;&#20272;&#25351;&#26631;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#29305;&#23450;&#39046;&#22495;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#30340;&#20998;&#31867;&#27861;&#65292;&#23545;&#29616;&#26377;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#21644;&#23450;&#21046;&#25216;&#26415;&#36827;&#34892;&#20102;&#35814;&#32454;&#27604;&#36739;&#65292;&#24182;&#24191;&#27867;&#35752;&#35770;&#20102;&#36825;&#19968;&#39046;&#22495;&#20013;&#30340;&#26410;&#35299;&#20915;&#38382;&#39064;&#21644;&#26410;&#26469;&#30340;&#21457;&#23637;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have significantly advanced the field of natural language processing (NLP), providing a highly useful, task-agnostic foundation for a wide range of applications. The great promise of LLMs as general task solvers motivated people to extend their functionality largely beyond just a ``chatbot'', and use it as an assistant or even replacement for domain experts and tools in specific domains such as healthcare, finance, and education. However, directly applying LLMs to solve sophisticated problems in specific domains meets many hurdles, caused by the heterogeneity of domain data, the sophistication of domain knowledge, the uniqueness of domain objectives, and the diversity of the constraints (e.g., various social norms, cultural conformity, religious beliefs, and ethical standards in the domain applications). To fill such a gap, explosively-increase research, and practices have been conducted in very recent years on the domain specialization of LLMs, which, howe
&lt;/p&gt;</description></item><item><title>NUNO &#26159;&#19968;&#20010;&#29992;&#20110;&#22788;&#29702;&#38750;&#22343;&#21248;&#25968;&#25454;&#30340;&#39640;&#25928;&#31639;&#23376;&#23398;&#20064;&#26694;&#26550;&#65292;&#22312;&#19977;&#32500; PDE &#38382;&#39064;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25928;&#26524;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2305.18694</link><description>&lt;p&gt;
NUNO: &#29992;&#20110;&#23398;&#20064;&#38750;&#22343;&#21248;&#25968;&#25454; Parametric PDEs &#30340;&#36890;&#29992;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
NUNO: A General Framework for Learning Parametric PDEs with Non-Uniform Data. (arXiv:2305.18694v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18694
&lt;/p&gt;
&lt;p&gt;
NUNO &#26159;&#19968;&#20010;&#29992;&#20110;&#22788;&#29702;&#38750;&#22343;&#21248;&#25968;&#25454;&#30340;&#39640;&#25928;&#31639;&#23376;&#23398;&#20064;&#26694;&#26550;&#65292;&#22312;&#19977;&#32500; PDE &#38382;&#39064;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25928;&#26524;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#31639;&#23376;&#24050;&#32463;&#25104;&#20026;&#27714;&#35299;&#20559;&#24494;&#20998;&#26041;&#31243;&#20013;&#20989;&#25968;&#31354;&#38388;&#26144;&#23556;&#30340;&#26377;&#21147;&#24037;&#20855;&#12290;&#20294;&#26159;&#65292;&#24403;&#38754;&#20020;&#29616;&#23454;&#19990;&#30028;&#30340;&#29289;&#29702;&#25968;&#25454;&#26102;&#65292;&#36825;&#20123;&#25968;&#25454;&#24448;&#24448;&#20998;&#24067;&#38750;&#24120;&#19981;&#22343;&#21248;&#65292;&#20351;&#29992;&#22522;&#20110;&#32593;&#26684;&#30340;&#25216;&#26415;&#65288;&#22914;FFT&#65289;&#24456;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Non-Uniform Neural Operator (NUNO)&#65292;&#36825;&#26159;&#19968;&#20010;&#19987;&#38376;&#29992;&#20110;&#22788;&#29702;&#38750;&#22343;&#21248;&#25968;&#25454;&#30340;&#39640;&#25928;&#31639;&#23376;&#23398;&#20064;&#26694;&#26550;&#12290;&#36890;&#36807;&#21033;&#29992;&#22522;&#20110;K-D&#26641;&#30340;&#22495;&#20998;&#35299;&#65292;&#25105;&#20204;&#23558;&#38750;&#22343;&#21248;&#25968;&#25454;&#36716;&#25442;&#25104;&#22343;&#21248;&#32593;&#26684;&#65292;&#21516;&#26102;&#26377;&#25928;&#22320;&#25511;&#21046;&#25554;&#20540;&#35823;&#24046;&#65292;&#20174;&#32780;&#21487;&#20197;&#19982;&#20174;&#38750;&#22343;&#21248;&#25968;&#25454;&#23398;&#20064;&#30340;&#36895;&#24230;&#21644;&#20934;&#30830;&#24615;&#24182;&#34892;&#12290;&#25105;&#20204;&#22312;&#20108;&#32500;&#24377;&#24615;&#12289;(2+1)D&#27827;&#36947;&#27969;&#21644;&#19977;&#32500;&#22810;&#29289;&#29702; heatsink &#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#30340;&#23454;&#39564;&#65292;&#36825;&#20123;&#23454;&#39564;&#28041;&#21450;&#22797;&#26434;&#20960;&#20309;&#32467;&#26500;&#30340;&#19977;&#32500;&#20559;&#24494;&#20998;&#26041;&#31243;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#23558;&#35823;&#24046;&#29575;&#38477;&#20302;&#20102;&#26368;&#22810;60&#65285;&#65292;&#24182;&#23558;&#35757;&#32451;&#36895;&#24230;&#25552;&#39640;&#20102;2&#20493;&#33267;30&#20493;&#12290;&#20195;&#30721;&#29616;&#22312;&#22312;https://github.com &#19978;&#21487;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
The neural operator has emerged as a powerful tool in learning mappings between function spaces in PDEs. However, when faced with real-world physical data, which are often highly non-uniformly distributed, it is challenging to use mesh-based techniques such as the FFT. To address this, we introduce the Non-Uniform Neural Operator (NUNO), a comprehensive framework designed for efficient operator learning with non-uniform data. Leveraging a K-D tree-based domain decomposition, we transform non-uniform data into uniform grids while effectively controlling interpolation error, thereby paralleling the speed and accuracy of learning from non-uniform data. We conduct extensive experiments on 2D elasticity, (2+1)D channel flow, and a 3D multi-physics heatsink, which, to our knowledge, marks a novel exploration into 3D PDE problems with complex geometries. Our framework has reduced error rates by up to 60% and enhanced training speeds by 2x to 30x. The code is now available at https://github.co
&lt;/p&gt;</description></item><item><title>Dink-Net&#26159;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#22823;&#35268;&#27169;&#22270;&#24418;&#31070;&#32463;&#32858;&#31867;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;&#33192;&#32960;&#21644;&#25910;&#32553;&#30340;&#24605;&#24819;&#26469;&#22788;&#29702;&#30334;&#19975;&#33410;&#28857;&#30340;&#22823;&#22270;&#65292;&#24182;&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.18405</link><description>&lt;p&gt;
Dink-Net: &#22823;&#35268;&#27169;&#22270;&#24418;&#31070;&#32463;&#32858;&#31867;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Dink-Net: Neural Clustering on Large Graphs. (arXiv:2305.18405v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18405
&lt;/p&gt;
&lt;p&gt;
Dink-Net&#26159;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#22823;&#35268;&#27169;&#22270;&#24418;&#31070;&#32463;&#32858;&#31867;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;&#33192;&#32960;&#21644;&#25910;&#32553;&#30340;&#24605;&#24819;&#26469;&#22788;&#29702;&#30334;&#19975;&#33410;&#28857;&#30340;&#22823;&#22270;&#65292;&#24182;&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#22270;&#32858;&#31867;&#36890;&#36807;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23558;&#22270;&#24418;&#30340;&#33410;&#28857;&#36827;&#34892;&#20998;&#32452;&#21462;&#24471;&#20102;&#24456;&#22823;&#30340;&#36827;&#23637;&#65292;&#20294;&#29616;&#26377;&#26041;&#27861;&#26080;&#27861;&#22788;&#29702;&#30334;&#19975;&#33410;&#28857;&#30340;&#22823;&#22270;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;Dink-Net&#28145;&#24230;&#22270;&#32858;&#31867;&#26041;&#27861;&#65292;&#21033;&#29992;&#20102;&#33192;&#32960;&#21644;&#25910;&#32553;&#30340;&#24605;&#24819;&#12290;&#39318;&#20808;&#65292;&#36890;&#36807;&#21306;&#20998;&#24102;&#22686;&#24378;&#30340;&#36319;&#19981;&#24102;&#22686;&#24378;&#30340;&#33410;&#28857;&#65292;&#33258;&#25105;&#30417;&#30563;&#26041;&#24335;&#23398;&#20064;&#34920;&#31034;&#24418;&#24335;&#12290;&#21516;&#26102;&#65292;&#23558;&#32858;&#31867;&#20013;&#24515;&#21021;&#22987;&#21270;&#20026;&#21487;&#23398;&#20064;&#30340;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#12290;&#38543;&#21518;&#65292;&#36890;&#36807;&#23545;&#25239;&#24615;&#26041;&#24335;&#26368;&#23567;&#21270;&#25552;&#20986;&#30340;&#38598;&#32676;&#33192;&#32960;&#25439;&#22833;&#21644;&#38598;&#32676;&#25910;&#32553;&#25439;&#22833;&#65292;&#20248;&#21270;&#32858;&#31867;&#20998;&#24067;&#12290;&#36890;&#36807;&#36825;&#20123;&#35774;&#32622;&#65292;&#25105;&#20204;&#23558;&#34920;&#31034;&#23398;&#20064;&#21644;&#32858;&#31867;&#20248;&#21270;&#20004;&#20010;&#27493;&#39588;&#32479;&#19968;&#20026;&#19968;&#20010;&#31471;&#21040;&#31471;&#26694;&#26550;&#65292;&#24341;&#23548;&#32593;&#32476;&#23398;&#20064;&#32858;&#31867;&#21451;&#22909;&#30340;&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;Dink-Net&#33021;&#24456;&#22909;&#22320;&#25193;&#23637;&#21040;&#22823;&#35268;&#27169;&#30340;&#22270;&#24418;&#19978;&#65292;&#22240;&#20026;&#35774;&#35745;&#30340;&#33192;&#32960;&#25910;&#32553;&#25805;&#20316;&#21487;&#20197;&#26377;&#25928;&#22320;&#20943;&#23569;&#35745;&#31639;&#21644;&#20869;&#23384;&#28040;&#32791;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;Dink-Net&#22312;&#22788;&#29702;&#30334;&#19975;&#33410;&#28857;&#22270;&#24418;&#30340;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#22823;&#22270;&#32858;&#31867;&#20013;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep graph clustering, which aims to group the nodes of a graph into disjoint clusters with deep neural networks, has achieved promising progress in recent years. However, the existing methods fail to scale to the large graph with million nodes. To solve this problem, a scalable deep graph clustering method (Dink-Net) is proposed with the idea of dilation and shrink. Firstly, by discriminating nodes, whether being corrupted by augmentations, representations are learned in a self-supervised manner. Meanwhile, the cluster centres are initialized as learnable neural parameters. Subsequently, the clustering distribution is optimized by minimizing the proposed cluster dilation loss and cluster shrink loss in an adversarial manner. By these settings, we unify the two-step clustering, i.e., representation learning and clustering optimization, into an end-to-end framework, guiding the network to learn clustering-friendly features. Besides, Dink-Net scales well to large graphs since the designe
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27010;&#24565;&#30340;&#35299;&#37322;&#27169;&#22411;&#25970;&#38376;&#25216;&#26415;&#65292;&#21487;&#20197;&#22312;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#25214;&#21040;&#26174;&#33879;&#30340;&#27010;&#24565;&#20197;&#36991;&#20813;&#35823;&#35299;&#65292;&#24182;&#25511;&#21046;&#35823;&#21457;&#29616;&#29575;&#65288;FDR&#65289;&#22312;&#26576;&#20010;&#20540;&#19979;&#65292;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#23454;&#39564;&#20013;&#24471;&#21040;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2305.18362</link><description>&lt;p&gt;
&#22522;&#20110;&#27010;&#24565;&#30340;&#35299;&#37322;&#27169;&#22411;&#25970;&#38376;&#25216;&#26415;&#26816;&#39564;&#22270;&#20687;&#20998;&#31867;&#22120;&#30340;&#26174;&#33879;&#32479;&#35745;&#32467;&#35770;
&lt;/p&gt;
&lt;p&gt;
Statistically Significant Concept-based Explanation of Image Classifiers via Model Knockoffs. (arXiv:2305.18362v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18362
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27010;&#24565;&#30340;&#35299;&#37322;&#27169;&#22411;&#25970;&#38376;&#25216;&#26415;&#65292;&#21487;&#20197;&#22312;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#25214;&#21040;&#26174;&#33879;&#30340;&#27010;&#24565;&#20197;&#36991;&#20813;&#35823;&#35299;&#65292;&#24182;&#25511;&#21046;&#35823;&#21457;&#29616;&#29575;&#65288;FDR&#65289;&#22312;&#26576;&#20010;&#20540;&#19979;&#65292;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#23454;&#39564;&#20013;&#24471;&#21040;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#27010;&#24565;&#30340;&#20998;&#31867;&#22120;&#21487;&#20197;&#36890;&#36807;&#20154;&#31867;&#21487;&#29702;&#35299;&#30340;&#27010;&#24565;&#26469;&#35299;&#37322;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#22270;&#20687;&#20998;&#31867;&#38382;&#39064;&#19978;&#30340;&#20915;&#31574;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#26377;&#26102;&#27010;&#24565;&#35299;&#37322;&#21487;&#33021;&#20250;&#23548;&#33268;&#35823;&#25253;&#65292;&#23558;&#19981;&#30456;&#20851;&#30340;&#27010;&#24565;&#35823;&#35748;&#20026;&#26159;&#39044;&#27979;&#20219;&#21153;&#30340;&#37325;&#35201;&#22240;&#32032;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#25214;&#21040;&#29992;&#20110;&#20998;&#31867;&#30340;&#26174;&#33879;&#27010;&#24565;&#20197;&#38450;&#27490;&#35823;&#35299;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23398;&#20064;&#22270;&#20687;&#27010;&#24565;&#65292;&#28982;&#21518;&#20351;&#29992;Knockoff&#26679;&#26412;&#36873;&#25321;&#37325;&#35201;&#27010;&#24565;&#36827;&#34892;&#39044;&#27979;&#65292;&#24182;&#25511;&#21046;&#35823;&#21457;&#29616;&#29575;&#65288;FDR&#65289;&#22312;&#26576;&#20010;&#20540;&#19979;&#12290;&#25105;&#20204;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#23454;&#39564;&#20013;&#35780;&#20272;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#36866;&#24403;&#22320;&#25511;&#21046;FDR&#65292;&#21516;&#26102;&#36873;&#25321;&#39640;&#24230;&#21487;&#35299;&#37322;&#30340;&#27010;&#24565;&#65292;&#25552;&#39640;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
A concept-based classifier can explain the decision process of a deep learning model by human-understandable concepts in image classification problems. However, sometimes concept-based explanations may cause false positives, which misregards unrelated concepts as important for the prediction task. Our goal is to find the statistically significant concept for classification to prevent misinterpretation. In this study, we propose a method using a deep learning model to learn the image concept and then using the Knockoff samples to select the important concepts for prediction by controlling the False Discovery Rate (FDR) under a certain value. We evaluate the proposed method in our synthetic and real data experiments. Also, it shows that our method can control the FDR properly while selecting highly interpretable concepts to improve the trustworthiness of the model.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#22914;&#20309;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#28216;&#25103;&#25151;&#38388;&#65292;&#22312;&#20165;&#26377;&#23569;&#37327;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#29983;&#25104;&#22810;&#36798;37%&#30340;&#21487;&#29609;&#26032;&#39062;&#20851;&#21345;&#65292;&#35813;&#25216;&#26415;&#26377;&#21161;&#20110;&#35299;&#20915;&#21253;&#21547;&#35768;&#22810;&#23616;&#37096;&#21644;&#20840;&#23616;&#32422;&#26463;&#30340;PCG&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.18243</link><description>&lt;p&gt;
&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#23454;&#29992;&#30340;PCG
&lt;/p&gt;
&lt;p&gt;
Practical PCG Through Large Language Models. (arXiv:2305.18243v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18243
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#22914;&#20309;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#28216;&#25103;&#25151;&#38388;&#65292;&#22312;&#20165;&#26377;&#23569;&#37327;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#29983;&#25104;&#22810;&#36798;37%&#30340;&#21487;&#29609;&#26032;&#39062;&#20851;&#21345;&#65292;&#35813;&#25216;&#26415;&#26377;&#21161;&#20110;&#35299;&#20915;&#21253;&#21547;&#35768;&#22810;&#23616;&#37096;&#21644;&#20840;&#23616;&#32422;&#26463;&#30340;PCG&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#24050;&#32463;&#34987;&#35777;&#26126;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#20043;&#22806;&#30340;&#21508;&#31181;&#39046;&#22495;&#20013;&#38750;&#24120;&#26377;&#29992;&#30340;&#24037;&#20855;&#12290;&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#22914;&#20309;&#20351;&#29992;LLMs&#20026;&#27491;&#22312;&#24320;&#21457;&#20013;&#30340;&#28216;&#25103;Metavoidal&#29983;&#25104;2D&#28216;&#25103;&#25151;&#38388;&#30340;&#23454;&#29992;&#26041;&#21521;&#12290;&#25105;&#20204;&#30340;&#25216;&#26415;&#21487;&#20197;&#36890;&#36807;&#20154;&#31867;&#21442;&#19982;&#30340;&#24494;&#35843;&#65292;&#21033;&#29992;GPT-3&#30340;&#33021;&#21147;&#65292;&#20165;&#20351;&#29992;60&#20010;&#25163;&#21160;&#35774;&#35745;&#30340;&#25151;&#38388;&#25968;&#25454;&#65292;&#22312;&#22797;&#26434;&#30340;&#28216;&#25103;&#22330;&#26223;&#19979;&#65292;&#29983;&#25104;37%&#30340;&#21487;&#29609;&#26032;&#39062;&#20851;&#21345;&#65292;&#36825;&#26159;&#38024;&#23545;&#23384;&#22312;&#22823;&#37327;&#23616;&#37096;&#21644;&#20840;&#23616;&#32422;&#26463;&#30340;PCG&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have proven to be useful tools in various domains outside of the field of their inception, which was natural language processing. In this study, we provide practical directions on how to use LLMs to generate 2D-game rooms for an under-development game, named Metavoidal. Our technique can harness the power of GPT-3 by Human-in-the-loop fine-tuning which allows our method to create 37% Playable-Novel levels from as scarce data as only 60 hand-designed rooms under a scenario of the non-trivial game, with respect to (Procedural Content Generation) PCG, that has a good amount of local and global constraints.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#23558;&#39044;&#27979;&#32534;&#30721;&#65288;PC&#65289;&#20316;&#20026;&#33258;&#36866;&#24212;&#20449;&#20219;&#21306;&#22495;&#65288;TR&#65289;&#31639;&#27861;&#30340;&#29702;&#35770;&#27169;&#22411;&#65292;&#24182;&#21457;&#29616;&#23427;&#21487;&#20197;&#27604;&#21453;&#21521;&#20256;&#25773;&#65288;BP&#65289;&#26356;&#24555;&#22320;&#36867;&#33073;&#38797;&#28857;&#12290;</title><link>http://arxiv.org/abs/2305.18188</link><description>&lt;p&gt;
&#23558;&#39044;&#27979;&#32534;&#30721;&#29702;&#35299;&#20026;&#33258;&#36866;&#24212;&#20449;&#20219;&#21306;&#22495;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Understanding Predictive Coding as an Adaptive Trust-Region Method. (arXiv:2305.18188v1 [cs.NE] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18188
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#23558;&#39044;&#27979;&#32534;&#30721;&#65288;PC&#65289;&#20316;&#20026;&#33258;&#36866;&#24212;&#20449;&#20219;&#21306;&#22495;&#65288;TR&#65289;&#31639;&#27861;&#30340;&#29702;&#35770;&#27169;&#22411;&#65292;&#24182;&#21457;&#29616;&#23427;&#21487;&#20197;&#27604;&#21453;&#21521;&#20256;&#25773;&#65288;BP&#65289;&#26356;&#24555;&#22320;&#36867;&#33073;&#38797;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#32534;&#30721;&#65288;PC&#65289;&#26159;&#19968;&#31181;&#31867;&#33041;&#26412;&#22320;&#23398;&#20064;&#31639;&#27861;&#65292;&#26368;&#36817;&#34987;&#25552;&#20986;&#22312;&#29983;&#29289;&#30456;&#20851;&#22330;&#26223;&#20013;&#25552;&#20379;&#27604;&#21453;&#21521;&#20256;&#25773;&#65288;BP&#65289;&#26356;&#22909;&#30340;&#20248;&#21183;&#12290;&#34429;&#28982;&#29702;&#35770;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#23637;&#31034;PC&#22914;&#20309;&#22312;&#21508;&#31181;&#26497;&#38480;&#20013;&#36924;&#36817;BP&#65292;&#20294;&#8220;&#33258;&#28982;&#8221;&#30340;PC&#30340;&#28508;&#22312;&#20248;&#21183;&#23578;&#19981;&#23436;&#20840;&#29702;&#35299;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;PC&#24320;&#21457;&#20026;&#20351;&#29992;&#20108;&#38454;&#20449;&#24687;&#30340;&#33258;&#36866;&#24212;&#20449;&#20219;&#21306;&#22495;&#65288;TR&#65289;&#31639;&#27861;&#30340;&#29702;&#35770;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Predictive coding (PC) is a brain-inspired local learning algorithm that has recently been suggested to provide advantages over backpropagation (BP) in biologically relevant scenarios. While theoretical work has mainly focused on showing how PC can approximate BP in various limits, the putative benefits of "natural" PC are less understood. Here we develop a theory of PC as an adaptive trust-region (TR) algorithm that uses second-order information. We show that the learning dynamics of PC can be interpreted as interpolating between BP's loss gradient direction and a TR direction found by the PC inference dynamics. Our theory suggests that PC should escape saddle points faster than BP, a prediction which we prove in a shallow linear model and support with experiments on deeper networks. This work lays a foundation for understanding PC in deep and wide networks.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22330;&#26223;&#22270;&#35760;&#24518;&#29366;&#24577;&#34920;&#31034;&#65292;&#32467;&#21512;&#33410;&#28857;&#36793;&#32536;&#39044;&#27979;&#22120;&#65288;NEP&#65289;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#33021;&#22815;&#24110;&#21161;&#20855;&#26377;&#34892;&#21160;&#33021;&#21147;&#30340;AI&#20195;&#29702;&#22312;&#37096;&#20998;&#21487;&#35266;&#23519;&#21160;&#24577;&#22330;&#26223;&#20013;&#39640;&#25928;&#25628;&#32034;&#12290;</title><link>http://arxiv.org/abs/2305.17537</link><description>&lt;p&gt;
&#20351;&#29992;&#22330;&#26223;&#22270;&#35760;&#24518;&#24314;&#27169;&#21160;&#24577;&#29615;&#22659;
&lt;/p&gt;
&lt;p&gt;
Modeling Dynamic Environments with Scene Graph Memory. (arXiv:2305.17537v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17537
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22330;&#26223;&#22270;&#35760;&#24518;&#29366;&#24577;&#34920;&#31034;&#65292;&#32467;&#21512;&#33410;&#28857;&#36793;&#32536;&#39044;&#27979;&#22120;&#65288;NEP&#65289;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#33021;&#22815;&#24110;&#21161;&#20855;&#26377;&#34892;&#21160;&#33021;&#21147;&#30340;AI&#20195;&#29702;&#22312;&#37096;&#20998;&#21487;&#35266;&#23519;&#21160;&#24577;&#22330;&#26223;&#20013;&#39640;&#25928;&#25628;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#22411;&#29615;&#22659;&#20013;&#65292;&#22914;&#23621;&#23460;&#31561;&#65292;&#23547;&#25214;&#29289;&#21697;&#30340;&#20855;&#26377;&#34892;&#21160;&#33021;&#21147;&#30340;AI&#20195;&#29702;&#38656;&#35201;&#22522;&#20110;&#37096;&#20998;&#20449;&#24687;&#39044;&#27979;&#29289;&#21697;&#20301;&#32622;&#26469;&#20570;&#20986;&#26377;&#25928;&#20915;&#31574;&#12290;&#25105;&#20204;&#23558;&#20854;&#24418;&#24335;&#21270;&#20026;&#19968;&#31181;&#26032;&#31867;&#22411;&#30340;&#38142;&#36335;&#39044;&#27979;&#38382;&#39064;&#65306;&#37096;&#20998;&#21487;&#35266;&#23519;&#21160;&#24577;&#22270;&#19978;&#30340;&#38142;&#36335;&#39044;&#27979;&#12290;&#25105;&#20204;&#30340;&#22270;&#34920;&#36798;&#20102;&#19968;&#20010;&#22330;&#26223;&#65292;&#20854;&#20013;&#25151;&#38388;&#21644;&#29289;&#21697;&#26159;&#33410;&#28857;&#65292;&#22312;&#36793;&#32536;&#20013;&#32534;&#30721;&#23427;&#20204;&#20043;&#38388;&#30340;&#20851;&#31995;&#65307;&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;&#39588;&#19978;&#65292;&#20195;&#29702;&#20154;&#20165;&#30693;&#36947;&#26356;&#25913;&#22270;&#30340;&#37096;&#20998;&#12290;&#36825;&#31181;&#37096;&#20998;&#21487;&#35266;&#27979;&#24615;&#23545;&#20110;&#29616;&#26377;&#30340;&#38142;&#36335;&#39044;&#27979;&#26041;&#27861;&#26500;&#25104;&#20102;&#25361;&#25112;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#35299;&#20915;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29366;&#24577;&#34920;&#31034; - &#22330;&#26223;&#22270;&#35760;&#24518;&#65288;SGM&#65289; - &#20854;&#20013;&#21253;&#25324;&#20195;&#29702;&#20154;&#30340;&#32047;&#31215;&#35266;&#23519;&#38598;&#21512;&#65292;&#20197;&#21450;&#19968;&#31181;&#21517;&#20026;&#33410;&#28857;&#36793;&#32536;&#39044;&#27979;&#22120;&#65288;NEP&#65289;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#35813;&#26550;&#26500;&#20174;SGM&#20013;&#25552;&#21462;&#20449;&#24687;&#20197;&#36827;&#34892;&#39640;&#25928;&#25628;&#32034;&#12290;&#25105;&#20204;&#22312;&#21160;&#24577;&#25151;&#23627;&#27169;&#25311;&#22120;&#20013;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#65292;&#23427;&#25353;&#29031;&#35821;&#20041;&#27169;&#24335;&#21019;&#24314;&#19981;&#21516;&#30340;&#21160;&#24577;&#22270;&#24418;&#12290;
&lt;/p&gt;
&lt;p&gt;
Embodied AI agents that search for objects in large environments such as households often need to make efficient decisions by predicting object locations based on partial information. We pose this as a new type of link prediction problem: link prediction on partially observable dynamic graphs. Our graph is a representation of a scene in which rooms and objects are nodes, and their relationships are encoded in the edges; only parts of the changing graph are known to the agent at each timestep. This partial observability poses a challenge to existing link prediction approaches, which we address. We propose a novel state representation -- Scene Graph Memory (SGM) -- with captures the agent's accumulated set of observations, as well as a neural net architecture called a Node Edge Predictor (NEP) that extracts information from the SGM to search efficiently. We evaluate our method in the Dynamic House Simulator, a new benchmark that creates diverse dynamic graphs following the semantic patte
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#29983;&#25104;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#20250;&#23548;&#33268;&#27169;&#22411;&#19981;&#21487;&#36870;&#30340;&#32570;&#38519;&#24182;&#19988;&#20351;&#24471;&#21407;&#22987;&#20869;&#23481;&#20998;&#24067;&#30340;&#23614;&#37096;&#28040;&#22833;&#65292;&#36825;&#31181;&#25928;&#24212;&#31216;&#20026;&#27169;&#22411;&#25240;&#21472;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#31181;&#29616;&#35937;&#22312;&#25152;&#26377;&#23398;&#20064;&#29983;&#25104;&#27169;&#22411;&#20013;&#37117;&#23384;&#22312;&#65292;&#24517;&#39035;&#35748;&#30495;&#23545;&#24453;&#12290;</title><link>http://arxiv.org/abs/2305.17493</link><description>&lt;p&gt;
&#36882;&#24402;&#30340;&#35781;&#21650;&#65306;&#20351;&#29992;&#29983;&#25104;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#20250;&#35753;&#27169;&#22411;&#24536;&#35760;
&lt;/p&gt;
&lt;p&gt;
The Curse of Recursion: Training on Generated Data Makes Models Forget. (arXiv:2305.17493v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17493
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#29983;&#25104;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#20250;&#23548;&#33268;&#27169;&#22411;&#19981;&#21487;&#36870;&#30340;&#32570;&#38519;&#24182;&#19988;&#20351;&#24471;&#21407;&#22987;&#20869;&#23481;&#20998;&#24067;&#30340;&#23614;&#37096;&#28040;&#22833;&#65292;&#36825;&#31181;&#25928;&#24212;&#31216;&#20026;&#27169;&#22411;&#25240;&#21472;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#31181;&#29616;&#35937;&#22312;&#25152;&#26377;&#23398;&#20064;&#29983;&#25104;&#27169;&#22411;&#20013;&#37117;&#23384;&#22312;&#65292;&#24517;&#39035;&#35748;&#30495;&#23545;&#24453;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31283;&#23450;&#25193;&#25955;&#25216;&#26415;&#38761;&#21629;&#24615;&#22320;&#25913;&#21464;&#20102;&#20174;&#25551;&#36848;&#24615;&#25991;&#26412;&#20013;&#29983;&#25104;&#22270;&#20687;&#30340;&#26041;&#27861;&#12290;GPT-2&#12289;GPT-3(.5)&#21644;GPT-4&#22312;&#21508;&#31181;&#35821;&#35328;&#20219;&#21153;&#20013;&#34920;&#29616;&#24778;&#20154;&#12290;ChatGPT&#23558;&#36825;&#20123;&#35821;&#35328;&#27169;&#22411;&#24341;&#20837;&#20102;&#22823;&#20247;&#35270;&#37326;&#12290;&#22823;&#35821;&#35328;&#27169;&#22411;(LLMs)&#24050;&#32463;&#19981;&#21487;&#36991;&#20813;&#24182;&#23558;&#24443;&#24213;&#25913;&#21464;&#22312;&#32447;&#25991;&#26412;&#21644;&#22270;&#20687;&#30340;&#25972;&#20010;&#29983;&#24577;&#31995;&#32479;&#12290;&#26412;&#25991;&#32771;&#34385;&#20102;&#26410;&#26469;&#21487;&#33021;&#21457;&#29983;&#30340;&#20107;&#24773;&#12290;&#24403;LLMs&#21344;&#25454;&#20102;&#22312;&#32447;&#35821;&#35328;&#30340;&#22823;&#37096;&#20998;&#26102;&#65292;GPT-{n}&#20250;&#21457;&#29983;&#20160;&#20040;&#65311;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#35757;&#32451;&#20013;&#20351;&#29992;&#27169;&#22411;&#29983;&#25104;&#30340;&#20869;&#23481;&#20250;&#23548;&#33268;&#25152;&#24471;&#27169;&#22411;&#20013;&#19981;&#21487;&#36870;&#32570;&#38519;&#65292;&#21407;&#22987;&#20869;&#23481;&#20998;&#24067;&#30340;&#23614;&#37096;&#28040;&#22833;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#25928;&#24212;&#31216;&#20026;&#27169;&#22411;&#25240;&#21472;&#65292;&#24182;&#26174;&#31034;&#23427;&#21487;&#20197;&#21457;&#29983;&#22312;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#12289;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#21644;LLMs&#20013;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#29616;&#35937;&#32972;&#21518;&#30340;&#29702;&#35770;&#30452;&#35273;&#65292;&#24182;&#23637;&#31034;&#20102;&#36825;&#31181;&#29616;&#35937;&#22312;&#25152;&#26377;&#23398;&#20064;&#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#26222;&#36941;&#24615;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22914;&#26524;&#25105;&#20204;&#35201;&#22312;&#23454;&#36341;&#20013;&#20351;&#29992;&#29983;&#25104;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#23601;&#24517;&#39035;&#35748;&#30495;&#23545;&#24453;&#36825;&#19968;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stable Diffusion revolutionised image creation from descriptive text. GPT-2, GPT-3(.5) and GPT-4 demonstrated astonishing performance across a variety of language tasks. ChatGPT introduced such language models to the general public. It is now clear that large language models (LLMs) are here to stay, and will bring about drastic change in the whole ecosystem of online text and images. In this paper we consider what the future might hold. What will happen to GPT-{n} once LLMs contribute much of the language found online? We find that use of model-generated content in training causes irreversible defects in the resulting models, where tails of the original content distribution disappear. We refer to this effect as Model Collapse and show that it can occur in Variational Autoencoders, Gaussian Mixture Models and LLMs. We build theoretical intuition behind the phenomenon and portray its ubiquity amongst all learned generative models. We demonstrate that it has to be taken seriously if we ar
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HUB&#30340;&#28151;&#21512;&#26356;&#26032;&#31574;&#30053;&#65292;&#36890;&#36807;&#32467;&#21512;&#23398;&#20064;&#20248;&#21270;&#22120;&#21644;&#25163;&#24037;&#35774;&#35745;&#30340;&#20248;&#21270;&#22120;&#65292;&#25552;&#39640;&#20102;&#23398;&#20064;&#20248;&#21270;&#22120;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.16823</link><description>&lt;p&gt;
HUB: &#29992;&#25345;&#32493;&#25552;&#31034;&#35843;&#25972;&#24341;&#23548;&#23398;&#20064;&#20248;&#21270;&#22120;
&lt;/p&gt;
&lt;p&gt;
HUB: Guiding Learned Optimizers with Continuous Prompt Tuning. (arXiv:2305.16823v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16823
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HUB&#30340;&#28151;&#21512;&#26356;&#26032;&#31574;&#30053;&#65292;&#36890;&#36807;&#32467;&#21512;&#23398;&#20064;&#20248;&#21270;&#22120;&#21644;&#25163;&#24037;&#35774;&#35745;&#30340;&#20248;&#21270;&#22120;&#65292;&#25552;&#39640;&#20102;&#23398;&#20064;&#20248;&#21270;&#22120;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#20248;&#21270;&#22120;&#26159;&#20803;&#23398;&#20064;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65292;&#20294;&#20854;&#22312;&#22788;&#29702;&#26410;&#35265;&#36807;&#30340;&#20219;&#21153;&#21644;&#32593;&#32476;&#26550;&#26500;&#26102;&#26377;&#38480;&#12290;&#20026;&#20102;&#35299;&#20915;&#27492;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28151;&#21512;&#26356;&#26032;&#31574;&#30053;&#30340;&#20248;&#21270;&#26041;&#27861;&#65288;HUB&#65289;&#65292;&#35813;&#26041;&#27861;&#21463;&#21040;&#20102;&#22823;&#22411;&#35821;&#35328;&#21644;&#35270;&#35273;&#27169;&#22411;&#20013;&#30828;&#25552;&#31034;&#35843;&#25972;&#21644;&#32467;&#26524;&#36873;&#25321;&#25216;&#26415;&#30340;&#21551;&#21457;&#12290;&#36890;&#36807;&#23558;&#25163;&#24037;&#35774;&#35745;&#30340;&#20248;&#21270;&#22120;&#20316;&#20026;&#25105;&#20204;&#28151;&#21512;&#26041;&#27861;&#30340;&#31532;&#20108;&#20010;&#32452;&#20214;&#65292;&#25105;&#20204;&#33021;&#22815;&#22312;&#31283;&#23450;&#35757;&#32451;&#30340;&#21516;&#26102;&#20445;&#30041;&#23398;&#20064;&#20248;&#21270;&#22120;&#30340;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learned optimizers are a crucial component of meta-learning. Recent advancements in scalable learned optimizers have demonstrated their superior performance over hand-designed optimizers in various tasks. However, certain characteristics of these models, such as an unstable learning curve, limited ability to handle unseen tasks and network architectures, difficult-to-control behaviours, and poor performance in fine-tuning tasks impede their widespread adoption. To tackle the issue of generalization in scalable learned optimizers, we propose a hybrid-update-based (HUB) optimization strategy inspired by recent advancements in hard prompt tuning and result selection techniques used in large language and vision models. This approach can be easily applied to any task that involves hand-designed or learned optimizer. By incorporating hand-designed optimizers as the second component in our hybrid approach, we are able to retain the benefits of learned optimizers while stabilizing the training
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;GPT&#21644;NMT&#29983;&#25104;&#32763;&#35793;&#30340;&#25991;&#23383;&#31215;&#26497;&#24230;&#24046;&#24322;&#65292;&#21457;&#29616;GPT&#32763;&#35793;&#26356;&#19981;&#20934;&#30830;&#65292;&#20294;&#22312;MT&#36136;&#37327;&#35780;&#20272;&#25351;&#26631;&#19978;&#34920;&#29616;&#20986;&#30456;&#20284;&#25110;&#26356;&#22909;&#30340;&#20998;&#25968;&#12290;</title><link>http://arxiv.org/abs/2305.16806</link><description>&lt;p&gt;
GPT&#26159;&#21542;&#20250;&#20135;&#29983;&#26356;&#19981;&#20934;&#30830;&#30340;&#32763;&#35793;?
&lt;/p&gt;
&lt;p&gt;
Do GPTs Produce Less Literal Translations?. (arXiv:2305.16806v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16806
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;GPT&#21644;NMT&#29983;&#25104;&#32763;&#35793;&#30340;&#25991;&#23383;&#31215;&#26497;&#24230;&#24046;&#24322;&#65292;&#21457;&#29616;GPT&#32763;&#35793;&#26356;&#19981;&#20934;&#30830;&#65292;&#20294;&#22312;MT&#36136;&#37327;&#35780;&#20272;&#25351;&#26631;&#19978;&#34920;&#29616;&#20986;&#30456;&#20284;&#25110;&#26356;&#22909;&#30340;&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22914;GPT-3&#65292;&#24050;&#32463;&#25104;&#20026;&#36890;&#29992;&#35821;&#35328;&#27169;&#22411;&#65292;&#33021;&#22815;&#22788;&#29702;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#25110;&#29702;&#35299;&#20219;&#21153;&#12290;&#22312;&#26426;&#22120;&#32763;&#35793;&#65288;MT&#65289;&#20219;&#21153;&#20013;&#65292;&#24050;&#26377;&#22810;&#39033;&#30740;&#31350;&#25506;&#32034;&#21033;&#29992;few-shot&#25552;&#31034;&#26426;&#21046;&#20174;LLMs&#20013;&#24341;&#20986;&#26356;&#22909;&#30340;&#32763;&#35793;&#12290;&#28982;&#32780;&#65292;&#20154;&#20204;&#30456;&#23545;&#36739;&#23569;&#22320;&#20851;&#27880;&#36825;&#31181;&#32763;&#35793;&#19982;&#26631;&#20934;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65288;NMT&#65289;&#27169;&#22411;&#29983;&#25104;&#32763;&#35793;&#30340;&#36136;&#37327;&#24046;&#24322;&#12290;&#26412;&#30740;&#31350;&#20174;&#25991;&#23383;&#23545;&#40784;&#21644;&#21333;&#35843;&#24615;&#31561;&#26041;&#38754;&#65292;&#27604;&#36739;&#20102;GPT&#21644;NMT&#29983;&#25104;&#32763;&#35793;&#30340;&#25991;&#26412;&#25991;&#23383;&#31215;&#26497;&#24230;&#65292;&#21457;&#29616;GPT&#20174;&#33521;&#35821;&#65288;E-X&#65289;&#32763;&#35793;&#30340;&#25991;&#26412;&#26356;&#19981;&#20934;&#30830;&#65292;&#20294;&#22312;MT&#36136;&#37327;&#35780;&#20272;&#25351;&#26631;&#19978;&#34920;&#29616;&#20986;&#30456;&#20284;&#25110;&#26356;&#22909;&#30340;&#20998;&#25968;&#12290;&#25105;&#20204;&#35777;&#26126;&#36825;&#19968;&#32467;&#26524;&#22312;&#20154;&#24037;&#35780;&#20272;&#20013;&#20063;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;&#21516;&#26102;&#65292;&#24403;&#32763;&#35793;&#21477;&#23376;&#38271;&#24230;&#22686;&#21152;&#26102;&#65292;&#36825;&#31181;&#24046;&#21035;&#23601;&#23588;&#20026;&#26174;&#33879;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) such as GPT-3 have emerged as general-purpose language models capable of addressing many natural language generation or understanding tasks. On the task of Machine Translation (MT), multiple works have investigated few-shot prompting mechanisms to elicit better translations from LLMs. However, there has been relatively little investigation on how such translations differ qualitatively from the translations generated by standard Neural Machine Translation (NMT) models. In this work, we investigate these differences in terms of the literalness of translations produced by the two systems. Using literalness measures involving word alignment and monotonicity, we find that translations out of English (E-X) from GPTs tend to be less literal, while exhibiting similar or better scores on MT quality metrics. We demonstrate that this finding is borne out in human evaluations as well. We then show that these differences are especially pronounced when translating senten
&lt;/p&gt;</description></item><item><title>LLM&#20195;&#29702;&#21487;&#20197;&#36890;&#36807;Adaplanner&#33258;&#36866;&#24212;&#25913;&#36827;&#33258;&#24049;&#30340;&#35745;&#21010;&#20197;&#24212;&#23545;&#29615;&#22659;&#21453;&#39304;&#65292;&#20026;&#27492;&#25552;&#20986;&#35745;&#21010;&#20869;&#22806;&#30340;&#25913;&#36827;&#31574;&#30053;&#20197;&#21450;&#20195;&#30721;&#39118;&#26684;&#30340;LLM&#25552;&#31034;&#32467;&#26500;&#21644;&#25216;&#33021;&#21457;&#29616;&#26426;&#21046;&#12290;</title><link>http://arxiv.org/abs/2305.16653</link><description>&lt;p&gt;
AdaPlanner:&#33258;&#36866;&#24212;&#35268;&#21010;&#19982;&#35821;&#35328;&#27169;&#22411;&#30340;&#21453;&#39304;&#12290; &#65288;arXiv&#65306;2305.16653v1 [cs.CL]&#65289;
&lt;/p&gt;
&lt;p&gt;
AdaPlanner: Adaptive Planning from Feedback with Language Models. (arXiv:2305.16653v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16653
&lt;/p&gt;
&lt;p&gt;
LLM&#20195;&#29702;&#21487;&#20197;&#36890;&#36807;Adaplanner&#33258;&#36866;&#24212;&#25913;&#36827;&#33258;&#24049;&#30340;&#35745;&#21010;&#20197;&#24212;&#23545;&#29615;&#22659;&#21453;&#39304;&#65292;&#20026;&#27492;&#25552;&#20986;&#35745;&#21010;&#20869;&#22806;&#30340;&#25913;&#36827;&#31574;&#30053;&#20197;&#21450;&#20195;&#30721;&#39118;&#26684;&#30340;LLM&#25552;&#31034;&#32467;&#26500;&#21644;&#25216;&#33021;&#21457;&#29616;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23637;&#31034;&#20102;&#22312;&#24207;&#21015;&#20915;&#31574;&#20219;&#21153;&#20013;&#20316;&#20026;&#33258;&#20027;&#20195;&#29702;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#35201;&#20040;&#36138;&#23146;&#22320;&#37319;&#21462;&#34892;&#21160;&#32780;&#27809;&#26377;&#35745;&#21010;&#65292;&#35201;&#20040;&#20381;&#36182;&#20110;&#19981;&#21487;&#36866;&#24212;&#29615;&#22659;&#21453;&#39304;&#30340;&#38745;&#24577;&#35745;&#21010;&#12290;&#22240;&#27492;&#65292;&#38543;&#30528;&#38382;&#39064;&#22797;&#26434;&#24615;&#21644;&#35745;&#21010;&#27700;&#24179;&#30340;&#22686;&#21152;&#65292;LLM&#20195;&#29702;&#30340;&#39034;&#24207;&#20915;&#31574;&#24615;&#33021;&#20250;&#36864;&#21270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38381;&#29615;&#26041;&#27861;AdaPlanner&#65292;&#23427;&#20801;&#35768;LLM&#20195;&#29702;&#26681;&#25454;&#29615;&#22659;&#21453;&#39304;&#33258;&#36866;&#24212;&#22320;&#25913;&#36827;&#20854;&#33258;&#21160;&#29983;&#25104;&#30340;&#35745;&#21010;&#12290;&#22312;AdaPlanner&#20013;&#65292;LLM&#20195;&#29702;&#36890;&#36807;&#35745;&#21010;&#20869;&#21644;&#35745;&#21010;&#22806;&#30340;&#25913;&#36827;&#31574;&#30053;&#33258;&#36866;&#24212;&#22320;&#25913;&#36827;&#20854;&#35745;&#21010;&#12290;&#20026;&#20102;&#20943;&#36731;&#24187;&#35273;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#20195;&#30721;&#39118;&#26684;&#30340;LLM&#25552;&#31034;&#32467;&#26500;&#65292;&#20419;&#36827;&#20102;&#36328;&#21508;&#31181;&#20219;&#21153;&#65292;&#29615;&#22659;&#21644;&#20195;&#29702;&#33021;&#21147;&#30340;&#35745;&#21010;&#29983;&#25104;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25216;&#33021;&#21457;&#29616;&#26426;&#21046;&#65292;&#21033;&#29992;&#25104;&#21151;&#30340;&#35745;&#21010;&#20316;&#20026;&#23569;&#37327;&#31034;&#20363;&#65292;&#20351;&#35745;&#21010;&#26356;&#20855;&#26222;&#36866;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have recently demonstrated the potential in acting as autonomous agents for sequential decision-making tasks. However, most existing methods either take actions greedily without planning or rely on static plans that are not adaptable to environmental feedback. Consequently, the sequential decision-making performance of LLM agents degenerates with problem complexity and plan horizons increase. We propose a closed-loop approach, AdaPlanner, which allows the LLM agent to refine its self-generated plan adaptively in response to environmental feedback. In AdaPlanner, the LLM agent adaptively refines its plan from feedback with both in-plan and out-of-plan refinement strategies. To mitigate hallucination, we develop a code-style LLM prompt structure that facilitates plan generation across a variety of tasks, environments, and agent capabilities. Furthermore, we propose a skill discovery mechanism that leverages successful plans as few-shot exemplars, enabling the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36870;&#21521;&#24037;&#31243;&#20102;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#35757;&#32451;&#34920;&#31034;&#65292;&#21457;&#29616;SSL&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#27491;&#21017;&#21270;&#39033;&#26412;&#36136;&#19978;&#20419;&#36827;&#20102;&#26679;&#26412;&#22522;&#20110;&#35821;&#20041;&#26631;&#31614;&#30340;&#32858;&#31867;&#12290;SSL&#35757;&#32451;&#30340;&#34920;&#31034;&#19982;&#35821;&#20041;&#31867;&#21035;&#26356;&#21152;&#25509;&#36817;&#65292;&#23545;&#40784;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#22686;&#21152;&#65292;&#32780;&#19988;&#22312;&#32593;&#32476;&#28145;&#24230;&#21152;&#28145;&#26102;&#22686;&#21152;&#12290;</title><link>http://arxiv.org/abs/2305.15614</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#36870;&#21521;&#24037;&#31243;
&lt;/p&gt;
&lt;p&gt;
Reverse Engineering Self-Supervised Learning. (arXiv:2305.15614v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15614
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36870;&#21521;&#24037;&#31243;&#20102;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#35757;&#32451;&#34920;&#31034;&#65292;&#21457;&#29616;SSL&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#27491;&#21017;&#21270;&#39033;&#26412;&#36136;&#19978;&#20419;&#36827;&#20102;&#26679;&#26412;&#22522;&#20110;&#35821;&#20041;&#26631;&#31614;&#30340;&#32858;&#31867;&#12290;SSL&#35757;&#32451;&#30340;&#34920;&#31034;&#19982;&#35821;&#20041;&#31867;&#21035;&#26356;&#21152;&#25509;&#36817;&#65292;&#23545;&#40784;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#22686;&#21152;&#65292;&#32780;&#19988;&#22312;&#32593;&#32476;&#28145;&#24230;&#21152;&#28145;&#26102;&#22686;&#21152;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#26377;&#21147;&#30340;&#24037;&#20855;&#65292;&#20294;&#29702;&#35299;&#23398;&#20064;&#34920;&#31034;&#21450;&#20854;&#22522;&#30784;&#26426;&#21046;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#26412;&#25991;&#23545;SSL&#35757;&#32451;&#34920;&#31034;&#36827;&#34892;&#20102;&#28145;&#20837;&#30340;&#23454;&#35777;&#20998;&#26512;&#65292;&#21253;&#25324;&#22810;&#31181;&#27169;&#22411;&#12289;&#26550;&#26500;&#21644;&#36229;&#21442;&#25968;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;SSL&#35757;&#32451;&#36807;&#31243;&#30340;&#19968;&#20010;&#26377;&#36259;&#26041;&#38754;&#65306;&#23427;&#26412;&#36136;&#19978;&#20419;&#36827;&#20102;&#26679;&#26412;&#22522;&#20110;&#35821;&#20041;&#26631;&#31614;&#30340;&#32858;&#31867;&#65292;&#36825;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#36825;&#26159;&#30001;SSL&#30446;&#26631;&#30340;&#27491;&#21017;&#21270;&#39033;&#39537;&#21160;&#30340;&#12290;&#36825;&#31181;&#32858;&#31867;&#36807;&#31243;&#19981;&#20165;&#22686;&#24378;&#20102;&#19979;&#28216;&#20998;&#31867;&#65292;&#32780;&#19988;&#21387;&#32553;&#20102;&#25968;&#25454;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;SSL&#35757;&#32451;&#30340;&#34920;&#31034;&#19982;&#35821;&#20041;&#31867;&#21035;&#26356;&#21152;&#25509;&#36817;&#65292;&#32780;&#19981;&#26159;&#38543;&#26426;&#31867;&#21035;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23398;&#20064;&#34920;&#31034;&#19982;&#21508;&#31181;&#23618;&#27425;&#30340;&#35821;&#20041;&#31867;&#21035;&#23545;&#40784;&#65292;&#24182;&#19988;&#36825;&#31181;&#23545;&#40784;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#22686;&#21152;&#65292;&#32780;&#19988;&#22312;&#32593;&#32476;&#28145;&#24230;&#21152;&#28145;&#26102;&#22686;&#21152;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning (SSL) is a powerful tool in machine learning, but understanding the learned representations and their underlying mechanisms remains a challenge. This paper presents an in-depth empirical analysis of SSL-trained representations, encompassing diverse models, architectures, and hyperparameters. Our study reveals an intriguing aspect of the SSL training process: it inherently facilitates the clustering of samples with respect to semantic labels, which is surprisingly driven by the SSL objective's regularization term. This clustering process not only enhances downstream classification but also compresses the data information. Furthermore, we establish that SSL-trained representations align more closely with semantic classes rather than random classes. Remarkably, we show that learned representations align with semantic classes across various hierarchical levels, and this alignment increases during training and when moving deeper into the network. Our findings provid
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#24207;&#21015;&#21040;&#24207;&#21015;&#27169;&#22411;KGT5-context&#65292;&#36890;&#36807;&#21152;&#20837;&#26597;&#35810;&#23454;&#20307;&#30340;&#30452;&#25509;&#37051;&#23621;&#20449;&#24687;&#23454;&#29616;&#30693;&#35782;&#22270;&#35889;&#38142;&#25509;&#39044;&#27979;&#30340;&#39640;&#24615;&#33021;&#65292;&#24182;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#27604;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.13059</link><description>&lt;p&gt;
&#21451;&#22909;&#30340;&#37051;&#23621;&#65306;&#35821;&#22659;&#21270;&#24207;&#21015;&#21040;&#24207;&#21015;&#38142;&#25509;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Friendly Neighbors: Contextualized Sequence-to-Sequence Link Prediction. (arXiv:2305.13059v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13059
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#24207;&#21015;&#21040;&#24207;&#21015;&#27169;&#22411;KGT5-context&#65292;&#36890;&#36807;&#21152;&#20837;&#26597;&#35810;&#23454;&#20307;&#30340;&#30452;&#25509;&#37051;&#23621;&#20449;&#24687;&#23454;&#29616;&#30693;&#35782;&#22270;&#35889;&#38142;&#25509;&#39044;&#27979;&#30340;&#39640;&#24615;&#33021;&#65292;&#24182;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#27604;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102; KGT5-context&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#20013;&#38142;&#25509;&#39044;&#27979;&#65288;LP&#65289;&#30340;&#31616;&#21333;&#24207;&#21015;&#21040;&#24207;&#21015;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#22312;&#26368;&#36817;&#30340;LP&#27169;&#22411;KGT5&#30340;&#22522;&#30784;&#19978;&#25299;&#23637;&#65292;KGT5&#21033;&#29992;&#20102;KG&#30340;&#25991;&#26412;&#29305;&#24449;&#65292;&#27169;&#22411;&#35268;&#27169;&#23567;&#19988;&#21487;&#25193;&#23637;&#65292;&#20294;&#20026;&#20102;&#36798;&#21040;&#22909;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;KGT5&#20381;&#36182;&#20110;&#19982;&#20043;&#37197;&#21512;&#30340;&#30693;&#35782;&#22270;&#23884;&#20837;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#26412;&#36523;&#38750;&#24120;&#22823;&#19988;&#20351;&#29992;&#25104;&#26412;&#39640;&#26114;&#12290;&#22312;&#36825;&#31687;&#30701;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#21152;&#20837;&#35821;&#22659;&#20449;&#24687;&#65292;&#21363;&#20851;&#20110;&#26597;&#35810;&#23454;&#20307;&#30340;&#30452;&#25509;&#37051;&#23621;&#20449;&#24687;&#65292;&#21487;&#20197;&#20943;&#36731;&#23545;&#29420;&#31435;KGE&#27169;&#22411;&#30340;&#38656;&#27714;&#20197;&#33719;&#24471;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;&#30001;&#27492;&#20135;&#29983;&#30340;KGT5-context&#27169;&#22411;&#31616;&#21333;&#65292;&#26174;&#33879;&#32553;&#23567;&#20102;&#27169;&#22411;&#22823;&#23567;&#65292;&#24182;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#30740;&#31350;&#20013;&#33719;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose KGT5-context, a simple sequence-to-sequence model for link prediction (LP) in knowledge graphs (KG). Our work expands on KGT5, a recent LP model that exploits textual features of the KG, has small model size, and is scalable. To reach good predictive performance, however, KGT5 relies on an ensemble with a knowledge graph embedding model, which itself is excessively large and costly to use. In this short paper, we show empirically that adding contextual information - i.e., information about the direct neighborhood of the query entity - alleviates the need for a separate KGE model to obtain good performance. The resulting KGT5-context model is simple, reduces model size significantly, and obtains state-of-the-art performance in our experimental study.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#26032;&#30340;HOI&#26816;&#27979;&#25968;&#25454;&#25286;&#20998;&#65292;&#26088;&#22312;&#35780;&#20272;&#31995;&#32479;&#24615;&#27867;&#21270;&#12290;&#22312;&#26032;&#30340;&#25968;&#25454;&#25286;&#20998;&#19978;&#27979;&#35797;&#32467;&#26524;&#34920;&#26126;&#65292;HOI&#26816;&#27979;&#27169;&#22411;&#23545;&#20110;&#26410;&#35265;&#36807;&#30340;&#23545;&#35937;&#21644;&#20132;&#20114;&#32452;&#21512;&#30340;&#27867;&#21270;&#21313;&#20998;&#22256;&#38590;&#12290;</title><link>http://arxiv.org/abs/2305.09948</link><description>&lt;p&gt;
HICO-DET-SG&#21644;V-COCO-SG&#65306;&#26032;&#30340;&#25968;&#25454;&#25286;&#20998;&#29992;&#20110;&#35780;&#20272;&#20154;-&#29289;&#20132;&#20114;&#26816;&#27979;&#20013;&#30340;&#31995;&#32479;&#24615;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
HICO-DET-SG and V-COCO-SG: New Data Splits to Evaluate Systematic Generalization in Human-Object Interaction Detection. (arXiv:2305.09948v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09948
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#26032;&#30340;HOI&#26816;&#27979;&#25968;&#25454;&#25286;&#20998;&#65292;&#26088;&#22312;&#35780;&#20272;&#31995;&#32479;&#24615;&#27867;&#21270;&#12290;&#22312;&#26032;&#30340;&#25968;&#25454;&#25286;&#20998;&#19978;&#27979;&#35797;&#32467;&#26524;&#34920;&#26126;&#65292;HOI&#26816;&#27979;&#27169;&#22411;&#23545;&#20110;&#26410;&#35265;&#36807;&#30340;&#23545;&#35937;&#21644;&#20132;&#20114;&#32452;&#21512;&#30340;&#27867;&#21270;&#21313;&#20998;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;-&#29289;&#20132;&#20114;&#26816;&#27979;&#26159;&#19968;&#31181;&#39044;&#27979;&#22270;&#20687;&#20013;&#20154;&#19982;&#29289;&#21697;&#20043;&#38388;&#20132;&#20114;&#30340;&#20219;&#21153;&#12290;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#65292;&#38656;&#35201;&#23545;HOI&#26816;&#27979;&#27169;&#22411;&#36827;&#34892;&#31995;&#32479;&#24615;&#30340;&#27867;&#21270;&#65292;&#21363;&#27867;&#21270;&#21040;&#26032;&#30340;&#23545;&#35937;&#21644;&#20132;&#20114;&#32452;&#21512;&#19978;&#65292;&#22240;&#20026;&#35757;&#32451;&#25968;&#25454;&#20165;&#21487;&#33021;&#28085;&#30422;&#25152;&#26377;&#21487;&#33021;&#32452;&#21512;&#30340;&#19968;&#23567;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#27809;&#26377;&#24320;&#25918;&#30340;&#22522;&#20934;&#27979;&#35797;&#25110;&#29616;&#26377;&#24037;&#20316;&#35780;&#20272;HOI&#26816;&#27979;&#20013;&#30340;&#31995;&#32479;&#24615;&#27867;&#21270;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#22522;&#20110;HICO-DET&#21644;V-COCO&#25968;&#25454;&#38598;&#21019;&#24314;&#20102;&#20004;&#20010;&#21517;&#20026;HICO-DET-SG&#21644;V-COCO-SG&#30340;&#26032;&#30340;HOI&#26816;&#27979;&#25968;&#25454;&#25286;&#20998;&#12290;&#25105;&#20204;&#22312;&#26032;&#30340;&#25968;&#25454;&#25286;&#20998;&#19978;&#35780;&#20272;&#20102;&#20195;&#34920;&#24615;&#30340;HOI&#26816;&#27979;&#27169;&#22411;&#65292;&#24182;&#35266;&#23519;&#21040;&#19982;&#21407;&#22987;&#25968;&#25454;&#38598;&#19978;&#30456;&#27604;&#27979;&#35797;&#24615;&#33021;&#26377;&#24456;&#22823;&#30340;&#38477;&#20302;&#12290;&#36825;&#20010;&#32467;&#26524;&#34920;&#26126;&#31995;&#32479;&#24615;&#27867;&#21270;&#26159;HOI&#26816;&#27979;&#20013;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#30446;&#26631;&#12290;&#25105;&#20204;&#24076;&#26395;&#25105;&#20204;&#30340;&#26032;&#25968;&#25454;&#25286;&#20998;&#33021;&#22815;&#40723;&#21169;&#26356;&#22810;&#30340;&#30740;&#31350;&#26397;&#30528;&#36825;&#20010;&#30446;&#26631;&#21162;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human-Object Interaction (HOI) detection is a task to predict interactions between humans and objects in an image. In real-world scenarios, HOI detection models are required systematic generalization, i.e., generalization to novel combinations of objects and interactions, because it is highly probable that the train data only cover a limited portion of all possible combinations. However, to our knowledge, no open benchmark or existing work evaluates the systematic generalization in HOI detection. To address this issue, we created two new sets of HOI detection data splits named HICO-DET-SG and V-COCO-SG based on HICO-DET and V-COCO datasets. We evaluated representative HOI detection models on the new data splits and observed large degradation in the test performances compared to those on the original datasets. This result shows that systematic generalization is a challenging goal in HOI detection. We hope our new data splits encourage more research toward this goal.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#26032;&#35270;&#35282;&#26469;&#25913;&#36827; Release 17 Type-II &#30721;&#26412;&#20013;&#30340; CSI &#21453;&#39304;&#24615;&#33021;&#65306;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#36873;&#25321;&#29305;&#23450;&#30340;&#35282;&#24230;-&#24310;&#36831;&#22495;&#31471;&#21475;&#20197;&#25552;&#39640;&#31934;&#24230;&#65292;&#36890;&#36807;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26469;&#23454;&#29616;&#21487;&#38752;&#30340;&#21453;&#39304;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#21453;&#39304;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.08081</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340; Type-II &#30721;&#26412;&#65306;&#22686;&#24378; CSI &#21453;&#39304;&#30340;&#26032;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Deep Learning Empowered Type-II Codebook: New Perspectives for Enhancing CSI Feedback. (arXiv:2305.08081v1 [cs.IT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08081
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#26032;&#35270;&#35282;&#26469;&#25913;&#36827; Release 17 Type-II &#30721;&#26412;&#20013;&#30340; CSI &#21453;&#39304;&#24615;&#33021;&#65306;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#36873;&#25321;&#29305;&#23450;&#30340;&#35282;&#24230;-&#24310;&#36831;&#22495;&#31471;&#21475;&#20197;&#25552;&#39640;&#31934;&#24230;&#65292;&#36890;&#36807;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26469;&#23454;&#29616;&#21487;&#38752;&#30340;&#21453;&#39304;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#21453;&#39304;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#39057;&#20998;&#21452;&#24037;&#31995;&#32479;&#20013;&#65292;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#20449;&#36947;&#29366;&#24577;&#20449;&#24687;&#65288;CSI&#65289;&#21453;&#39304;&#24341;&#36215;&#20102;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#30340;&#24191;&#27867;&#20851;&#27880;&#12290;&#26412;&#25991;&#37325;&#28857;&#30740;&#31350;&#23558;&#26080;&#32447;&#36890;&#20449;&#26631;&#20934;&#20013;&#30340; Type-II &#30721;&#26412;&#19982;&#28145;&#24230;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#20197;&#25552;&#39640; CSI &#21453;&#39304;&#30340;&#24615;&#33021;&#12290;&#19982;&#29616;&#26377;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340; Release 16 Type-II &#30721;&#26412;&#30740;&#31350;&#19981;&#21516;&#65292;Release 17&#65288;R17&#65289;&#20013;&#30340; Type-II &#30721;&#26412;&#21033;&#29992;&#19978;&#34892;&#21644;&#19979;&#34892;&#20449;&#36947;&#20043;&#38388;&#35282;&#24230;-&#24310;&#36831;&#22495;&#20559;&#25391;&#37096;&#20998;&#20114;&#26131;&#24615;&#36873;&#25321;&#37096;&#20998;&#35282;&#24230;-&#24310;&#36831;&#22495;&#31471;&#21475;&#36827;&#34892;&#27979;&#37327;&#21644;&#21453;&#39304;&#19979;&#34892; CSI&#65292;&#20256;&#32479;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#24615;&#33021;&#30001;&#20110;&#31232;&#30095;&#32467;&#26500;&#30340;&#32570;&#38519;&#32780;&#21463;&#21040;&#38480;&#21046;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#37319;&#29992;&#28145;&#24230;&#23398;&#20064;&#26469;&#25913;&#36827; R17 Type-II &#30721;&#26412;&#30340;&#26032;&#35270;&#35282;&#12290;&#39318;&#20808;&#65292;&#32771;&#34385;&#21040;&#19978;&#34892;&#20449;&#36947;&#30340;&#20302;&#20449;&#22122;&#27604;&#65292;&#28145;&#24230;&#23398;&#20064;&#34987;&#29992;&#26469;&#31934;&#30830;&#36873;&#25321;&#29305;&#23450;&#30340;&#35282;&#24230;-&#24310;&#36831;&#22495;&#31471;&#21475;&#65292;&#20174;&#32780;&#25552;&#39640; CSI &#30340;&#31934;&#24230;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#21033;&#29992;&#34394;&#25311;&#22495;&#21644;&#23454;&#38469;&#22495;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#36890;&#36807;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26469;&#23454;&#29616;&#21487;&#38752;&#30340;&#21453;&#39304;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640; CSI &#30340;&#21453;&#39304;&#24615;&#33021;&#65292;&#24182;&#19988;&#20855;&#26377;&#26356;&#22909;&#30340;&#40065;&#26834;&#24615;&#21644;&#36890;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning based channel state information (CSI) feedback in frequency division duplex systems has drawn widespread attention in both academia and industry. In this paper, we focus on integrating the Type-II codebook in the wireless communication standards with deep learning to enhance the performance of CSI feedback. In contrast to the existing deep learning based studies on the Release 16 Type-II codebook, the Type-II codebook in Release 17 (R17) exploits the angular-delay-domain partial reciprocity between uplink and downlink channels to select part of angular-delay-domain ports for measuring and feeding back the downlink CSI, where the performance of deep learning based conventional methods is limited due to the deficiency of sparse structures. To address this issue, we propose two new perspectives of adopting deep learning to improve the R17 Type-II codebook. Firstly, considering the low signal-to-noise ratio of uplink channels, deep learning is utilized to accurately select th
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#22823;&#35821;&#35328;&#27169;&#22411;Davinci&#20013;&#30340;&#24515;&#36523;&#20998;&#31163;&#65292;&#21457;&#29616;&#20854;&#20855;&#26377;&#24369;&#21270;&#30340;&#20108;&#20803;&#35770;&#20542;&#21521;&#12290;</title><link>http://arxiv.org/abs/2305.07667</link><description>&lt;p&gt;
&#36798;&#33452;&#22855;&#20108;&#37325;&#35770;&#32773;&#65306;&#22823;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24515;&#36523;&#20998;&#31163;&#21644;&#20154;&#31867;&#23398;&#20064;&#32773;&#20013;&#30340;&#24515;&#36523;&#20108;&#20803;&#35770;
&lt;/p&gt;
&lt;p&gt;
Davinci the Dualist: the mind-body divide in large language models and in human learners. (arXiv:2305.07667v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07667
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#22823;&#35821;&#35328;&#27169;&#22411;Davinci&#20013;&#30340;&#24515;&#36523;&#20998;&#31163;&#65292;&#21457;&#29616;&#20854;&#20855;&#26377;&#24369;&#21270;&#30340;&#20108;&#20803;&#35770;&#20542;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#37327;&#25991;&#29486;&#34920;&#26126;&#65292;&#20154;&#31867;&#20855;&#26377;&#30452;&#35266;&#30340;&#20108;&#20803;&#35770;&#24605;&#24819;&#65292;&#35748;&#20026;&#31934;&#31070;&#21644;&#36523;&#20307;&#26159;&#19981;&#21516;&#30340;&#23384;&#22312;&#12290;&#36807;&#21435;&#30340;&#30740;&#31350;&#20063;&#34920;&#26126;&#65292;&#20108;&#20803;&#35770;&#26159;&#36890;&#36807;&#23398;&#20064;&#32780;&#20986;&#29616;&#30340;&#65288;&#20363;&#22914;Barlev&#65286;Shtulman&#65292;2021&#65289;&#12290;&#20294;&#23398;&#20064;&#26159;&#21542;&#36275;&#20197;&#23548;&#33268;&#20108;&#20803;&#35770;&#23578;&#19981;&#28165;&#26970;&#12290;&#36890;&#36807;&#25506;&#31350;Davinci&#65288;&#19968;&#31181;&#19981;&#20855;&#26377;&#20219;&#20309;&#20869;&#22312;&#26680;&#24515;&#30693;&#35782;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#65289;&#20013;&#30340;&#24515;&#36523;&#20998;&#31163;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#23398;&#20064;&#30340;&#20316;&#29992;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;Davinci&#20173;&#28982;&#20542;&#21521;&#20110;&#20108;&#20803;&#35770;&#65292;&#24182;&#19988;&#36825;&#31181;&#20559;&#35265;&#38543;&#30528;&#23398;&#20064;&#32773;&#30340;&#24402;&#32435;&#28508;&#21147;&#36880;&#28176;&#22686;&#21152;&#12290;
&lt;/p&gt;
&lt;p&gt;
A large literature suggests that people are intuitive Dualists--they consider the mind ethereal, distinct from the body. Past research also shows that Dualism emerges, in part, via learning (e.g., Barlev &amp; Shtulman, 2021). But whether learning is sufficient to give rise to Dualism is unknown.The evidence from human learners does address this question because humans are endowed not only with general learning capacities but also with core knowledge capacities. And recent results suggest that core knowledge begets Dualism (Berent, Theodore &amp; Valencia, 2021; Berent, 2023). To evaluate the role of learning, here, we probe for a mind-body divide in Davinci--a large language model (LLM) that is devoid of any innate core knowledge. We show that Davinci still leans towards Dualism, and that this bias increases systematically with the learner's inductive potential. Thus, davinci (a GPT-3 model) exhibits mild Dualist tendencies, whereas its descendent, text-davinci-003 (a GPT-3.5 model), shows a 
&lt;/p&gt;</description></item><item><title>ImageBind&#26159;&#19968;&#31181;&#26032;&#30340;&#36328;&#27169;&#24577;&#32852;&#21512;&#23884;&#20837;&#26041;&#27861;&#65292;&#21482;&#38656;&#35201;&#20351;&#29992;&#22270;&#20687;&#37197;&#23545;&#25968;&#25454;&#23601;&#21487;&#20197;&#23558;&#19981;&#21516;&#27169;&#24577;&#30340;&#25968;&#25454;&#32465;&#23450;&#22312;&#19968;&#36215;&#65292;&#24182;&#23454;&#29616;&#36328;&#27169;&#24577;&#26816;&#32034;&#12289;&#32452;&#21512;&#21644;&#29983;&#25104;&#31561;&#22810;&#31181;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2305.05665</link><description>&lt;p&gt;
ImageBind:&#19968;&#20010;&#20849;&#21516;&#23884;&#20837;&#31354;&#38388;&#32465;&#23450;&#25152;&#26377;&#27169;&#24577;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
ImageBind: One Embedding Space To Bind Them All. (arXiv:2305.05665v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05665
&lt;/p&gt;
&lt;p&gt;
ImageBind&#26159;&#19968;&#31181;&#26032;&#30340;&#36328;&#27169;&#24577;&#32852;&#21512;&#23884;&#20837;&#26041;&#27861;&#65292;&#21482;&#38656;&#35201;&#20351;&#29992;&#22270;&#20687;&#37197;&#23545;&#25968;&#25454;&#23601;&#21487;&#20197;&#23558;&#19981;&#21516;&#27169;&#24577;&#30340;&#25968;&#25454;&#32465;&#23450;&#22312;&#19968;&#36215;&#65292;&#24182;&#23454;&#29616;&#36328;&#27169;&#24577;&#26816;&#32034;&#12289;&#32452;&#21512;&#21644;&#29983;&#25104;&#31561;&#22810;&#31181;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;ImageBind&#65292;&#36825;&#26159;&#19968;&#31181;&#36328;&#36234;&#22270;&#20687;&#12289;&#25991;&#26412;&#12289;&#38899;&#39057;&#12289;&#28145;&#24230;&#12289;&#28909;&#20256;&#24863;&#21644;IMU&#25968;&#25454;&#30340;&#20845;&#31181;&#19981;&#21516;&#27169;&#24577;&#30340;&#32852;&#21512;&#23884;&#20837;&#26041;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#65292;&#19981;&#38656;&#35201;&#35757;&#32451;&#25152;&#26377;&#37197;&#23545;&#25968;&#25454;&#65292;&#21482;&#38656;&#35201;&#22270;&#20687;&#37197;&#23545;&#25968;&#25454;&#23601;&#36275;&#20197;&#23558;&#36825;&#20123;&#27169;&#24577;&#32465;&#23450;&#22312;&#19968;&#36215;&#12290;ImageBind&#21487;&#20197;&#21033;&#29992;&#26368;&#36817;&#30340;&#22823;&#35268;&#27169;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#23427;&#20204;&#19982;&#22270;&#20687;&#30340;&#33258;&#28982;&#37197;&#23545;&#65292;&#23558;&#23427;&#20204;&#30340;&#38646;&#26679;&#26412;&#33021;&#21147;&#25193;&#23637;&#21040;&#26032;&#30340;&#27169;&#24577;&#12290;&#23427;&#21487;&#20197;&#23454;&#29616;&#8220;&#24320;&#31665;&#21363;&#29992;&#8221;&#30340;&#26032;&#22411;&#24212;&#29992;&#31243;&#24207;&#65292;&#21253;&#25324;&#36328;&#27169;&#24577;&#26816;&#32034;&#12289;&#29992;&#31639;&#26415;&#32452;&#21512;&#27169;&#24577;&#12289;&#36328;&#27169;&#24577;&#26816;&#27979;&#21644;&#29983;&#25104;&#12290;&#26032;&#22411;&#24212;&#29992;&#38543;&#30528;&#22270;&#20687;&#32534;&#30721;&#22120;&#30340;&#24378;&#24230;&#32780;&#19981;&#26029;&#25913;&#36827;&#65292;&#25105;&#20204;&#22312;&#36328;&#27169;&#24577;&#30340;&#38646;&#26679;&#26412;&#35782;&#21035;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20248;&#25104;&#32489;&#65292;&#36229;&#36807;&#20102;&#19987;&#23478;&#30417;&#30563;&#27169;&#22411;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#24378;&#30340;&#20960;&#20309;&#35782;&#21035;&#32467;&#26524;&#65292;&#36229;&#36807;&#20102;&#20197;&#21069;&#30340;&#24037;&#20316;&#65292;ImageBind&#25104;&#20026;&#20102;&#35780;&#20272;&#35270;&#35273;&#27169;&#24577;&#32852;&#21512;&#23398;&#20064;&#30340;&#19968;&#31181;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present ImageBind, an approach to learn a joint embedding across six different modalities - images, text, audio, depth, thermal, and IMU data. We show that all combinations of paired data are not necessary to train such a joint embedding, and only image-paired data is sufficient to bind the modalities together. ImageBind can leverage recent large scale vision-language models, and extends their zero-shot capabilities to new modalities just by using their natural pairing with images. It enables novel emergent applications 'out-of-the-box' including cross-modal retrieval, composing modalities with arithmetic, cross-modal detection and generation. The emergent capabilities improve with the strength of the image encoder and we set a new state-of-the-art on emergent zero-shot recognition tasks across modalities, outperforming specialist supervised models. Finally, we show strong few-shot recognition results outperforming prior work, and that ImageBind serves as a new way to evaluate visio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;EDGE&#65292;&#19968;&#31181;&#26032;&#30340;&#31163;&#25955;&#25193;&#25955;&#27169;&#22411;&#29992;&#20110;&#29983;&#25104;&#22823;&#22411;&#22270;&#65292;&#24182;&#36890;&#36807;&#21024;&#38500;&#36793;&#26469;&#40723;&#21169;&#22270;&#30340;&#31232;&#30095;&#24615;&#12290;EDGE&#22312;&#27599;&#20010;&#21435;&#22122;&#27493;&#39588;&#20013;&#20165;&#20851;&#27880;&#22270;&#20013;&#19968;&#37096;&#20998;&#33410;&#28857;&#65292;&#24182;&#19988;&#21487;&#20197;&#26126;&#30830;&#22320;&#23545;&#22270;&#30340;&#33410;&#28857;&#24230;&#25968;&#36827;&#34892;&#24314;&#27169;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;EDGE&#27604;&#31454;&#20105;&#26041;&#27861;&#26356;&#26377;&#25928;&#65292;&#24182;&#19988;&#21487;&#20197;&#29983;&#25104;&#20855;&#26377;&#25968;&#21315;&#20010;&#33410;&#28857;&#30340;&#22823;&#22411;&#22270;&#12290;</title><link>http://arxiv.org/abs/2305.04111</link><description>&lt;p&gt;
&#31163;&#25955;&#25193;&#25955;&#24314;&#27169;&#19979;&#30340;&#39640;&#25928;&#21644;&#24230;&#25968;&#24341;&#23548;&#22270;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Efficient and Degree-Guided Graph Generation via Discrete Diffusion Modeling. (arXiv:2305.04111v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04111
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;EDGE&#65292;&#19968;&#31181;&#26032;&#30340;&#31163;&#25955;&#25193;&#25955;&#27169;&#22411;&#29992;&#20110;&#29983;&#25104;&#22823;&#22411;&#22270;&#65292;&#24182;&#36890;&#36807;&#21024;&#38500;&#36793;&#26469;&#40723;&#21169;&#22270;&#30340;&#31232;&#30095;&#24615;&#12290;EDGE&#22312;&#27599;&#20010;&#21435;&#22122;&#27493;&#39588;&#20013;&#20165;&#20851;&#27880;&#22270;&#20013;&#19968;&#37096;&#20998;&#33410;&#28857;&#65292;&#24182;&#19988;&#21487;&#20197;&#26126;&#30830;&#22320;&#23545;&#22270;&#30340;&#33410;&#28857;&#24230;&#25968;&#36827;&#34892;&#24314;&#27169;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;EDGE&#27604;&#31454;&#20105;&#26041;&#27861;&#26356;&#26377;&#25928;&#65292;&#24182;&#19988;&#21487;&#20197;&#29983;&#25104;&#20855;&#26377;&#25968;&#21315;&#20010;&#33410;&#28857;&#30340;&#22823;&#22411;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25193;&#25955;&#30340;&#29983;&#25104;&#22270;&#27169;&#22411;&#24050;&#34987;&#35777;&#26126;&#22312;&#29983;&#25104;&#39640;&#36136;&#37327;&#23567;&#22270;&#26041;&#38754;&#38750;&#24120;&#26377;&#25928;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#38656;&#35201;&#26356;&#21487;&#25193;&#23637;&#24615;&#65292;&#20197;&#29983;&#25104;&#21253;&#21547;&#25968;&#21315;&#20010;&#33410;&#28857;&#30340;&#22823;&#22270;&#24182;&#28385;&#36275;&#22270;&#32479;&#35745;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;EDGE&#65292;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#29983;&#25104;&#22270;&#27169;&#22411;&#65292;&#29992;&#20110;&#29983;&#25104;&#22823;&#22411;&#22270;&#30340;&#29983;&#25104;&#20219;&#21153;&#12290;&#20026;&#20102;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;&#38271;&#38543;&#26426;&#21024;&#38500;&#36793;&#26469;&#40723;&#21169;&#22270;&#30340;&#31232;&#30095;&#24615;&#65292;&#24182;&#26368;&#32456;&#33719;&#24471;&#19968;&#24352;&#31354;&#30333;&#22270;&#12290;EDGE&#20165;&#22312;&#27599;&#20010;&#21435;&#22122;&#27493;&#39588;&#20013;&#20851;&#27880;&#22270;&#20013;&#19968;&#37096;&#20998;&#33410;&#28857;&#12290;&#23427;&#27604;&#20197;&#21069;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#27169;&#22411;&#26356;&#23569;&#22320;&#36827;&#34892;&#36793;&#39044;&#27979;&#12290;&#27492;&#22806;&#65292;EDGE&#26126;&#30830;&#22320;&#20801;&#35768;&#23545;&#22270;&#30340;&#33410;&#28857;&#24230;&#25968;&#36827;&#34892;&#24314;&#27169;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;EDGE&#27604;&#31454;&#20105;&#26041;&#27861;&#26356;&#26377;&#25928;&#65292;&#24182;&#19988;&#21487;&#20197;&#29983;&#25104;&#20855;&#26377;&#25968;&#21315;&#20010;&#33410;&#28857;&#30340;&#22823;&#22411;&#22270;&#12290;&#23427;&#36824;&#22312;&#29983;&#25104;&#36136;&#37327;&#26041;&#38754;&#20248;&#20110;&#22522;&#20934;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion-based generative graph models have been proven effective in generating high-quality small graphs. However, they need to be more scalable for generating large graphs containing thousands of nodes desiring graph statistics. In this work, we propose EDGE, a new diffusion-based generative graph model that addresses generative tasks with large graphs. To improve computation efficiency, we encourage graph sparsity by using a discrete diffusion process that randomly removes edges at each time step and finally obtains an empty graph. EDGE only focuses on a portion of nodes in the graph at each denoising step. It makes much fewer edge predictions than previous diffusion-based models. Moreover, EDGE admits explicitly modeling the node degrees of the graphs, further improving the model performance. The empirical study shows that EDGE is much more efficient than competing methods and can generate large graphs with thousands of nodes. It also outperforms baseline models in generation qual
&lt;/p&gt;</description></item><item><title>&#22312;&#19981;&#21516;&#25968;&#25454;&#23376;&#32676;&#20307;&#38388;&#65292;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20869;&#37096;&#20934;&#30830;&#24615;&#21644;&#22806;&#37096;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#26159;&#38750;&#32447;&#24615;&#30340;&#65292;&#21576;&#29616;&#20986;&#8220;&#26376;&#20142;&#24418;&#8221;&#30340;&#30456;&#20851;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.02995</link><description>&lt;p&gt;
&#20851;&#20110;&#25968;&#25454;&#23376;&#32676;&#20307;&#38388;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24615;&#33021;&#30340;&#38750;&#32447;&#24615;&#30456;&#20851;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the nonlinear correlation of ML performance between data subpopulations. (arXiv:2305.02995v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02995
&lt;/p&gt;
&lt;p&gt;
&#22312;&#19981;&#21516;&#25968;&#25454;&#23376;&#32676;&#20307;&#38388;&#65292;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20869;&#37096;&#20934;&#30830;&#24615;&#21644;&#22806;&#37096;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#26159;&#38750;&#32447;&#24615;&#30340;&#65292;&#21576;&#29616;&#20986;&#8220;&#26376;&#20142;&#24418;&#8221;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#19981;&#21516;&#25968;&#25454;&#20998;&#24067;&#19979;&#30340;&#24615;&#33021;&#23545;&#20110;&#21487;&#38752;&#30340;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#26368;&#26032;&#30340;&#32463;&#39564;&#30740;&#31350;&#35748;&#20026;&#35757;&#32451;&#25968;&#25454;&#20869;&#37096;&#30340;&#20934;&#30830;&#24615;&#21644;&#26032;&#25968;&#25454;&#22806;&#37096;&#30340;&#20934;&#30830;&#24615;&#20043;&#38388;&#23384;&#22312;&#36817;&#20046;&#23436;&#32654;&#30340;&#32447;&#24615;&#30456;&#20851;&#24615;&#65292;&#20294;&#25105;&#20204;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#12289;&#27169;&#22411;&#21644;&#35757;&#32451;&#26102;&#26399;&#36827;&#34892;&#20102;&#20005;&#26684;&#30340;&#23454;&#39564;&#21644;&#20998;&#26512;&#65292;&#21457;&#29616;&#22312;&#23376;&#32676;&#20307;&#36716;&#31227;&#19979;&#65292;&#20869;&#37096;&#20934;&#30830;&#24615;&#21644;&#22806;&#37096;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#26356;&#20026;&#24494;&#22937;&#65292;&#24182;&#19988;&#22312;&#19978;&#21319;&#38454;&#27573;&#23384;&#22312;&#8220;&#26376;&#20142;&#24418;&#8221;&#30340;&#30456;&#20851;&#24615;&#65288;&#25243;&#29289;&#32447;&#19978;&#21319;&#26354;&#32447;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding the performance of machine learning (ML) models across diverse data distributions is critically important for reliable applications. Despite recent empirical studies positing a near-perfect linear correlation between in-distribution (ID) and out-of-distribution (OOD) accuracies, we empirically demonstrate that this correlation is more nuanced under subpopulation shifts. Through rigorous experimentation and analysis across a variety of datasets, models, and training epochs, we demonstrate that OOD performance often has a nonlinear correlation with ID performance in subpopulation shifts. Our findings, which contrast previous studies that have posited a linear correlation in model performance during distribution shifts, reveal a "moon shape" correlation (parabolic uptrend curve) between the test performance on the majority subpopulation and the minority subpopulation. This non-trivial nonlinear correlation holds across model architectures, hyperparameters, training durations
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Moccasin&#30340;&#26032;&#22411;&#32422;&#26463;&#32534;&#31243;&#24418;&#24335;&#65292;&#29992;&#20110;&#23454;&#29616;&#22312;&#20869;&#23384;&#39044;&#31639;&#19979;&#26368;&#23567;&#21270;&#35745;&#31639;&#22270;&#30340;&#25191;&#34892;&#26102;&#38388;&#65292;&#30456;&#36739;&#20110;&#26368;&#36817;&#30340;&#30740;&#31350;&#65292;&#35813;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#25928;&#29575;&#65292;&#24182;&#25104;&#21151;&#24212;&#29992;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#39640;&#25928;&#24352;&#37327;&#37325;&#31639;&#12290;</title><link>http://arxiv.org/abs/2304.14463</link><description>&lt;p&gt;
Moccasin&#65306;&#31070;&#32463;&#32593;&#32476;&#30340;&#39640;&#25928;&#24352;&#37327;&#37325;&#31639;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Moccasin: Efficient Tensor Rematerialization for Neural Networks. (arXiv:2304.14463v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14463
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Moccasin&#30340;&#26032;&#22411;&#32422;&#26463;&#32534;&#31243;&#24418;&#24335;&#65292;&#29992;&#20110;&#23454;&#29616;&#22312;&#20869;&#23384;&#39044;&#31639;&#19979;&#26368;&#23567;&#21270;&#35745;&#31639;&#22270;&#30340;&#25191;&#34892;&#26102;&#38388;&#65292;&#30456;&#36739;&#20110;&#26368;&#36817;&#30340;&#30740;&#31350;&#65292;&#35813;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#25928;&#29575;&#65292;&#24182;&#25104;&#21151;&#24212;&#29992;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#39640;&#25928;&#24352;&#37327;&#37325;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36793;&#32536;&#35745;&#31639;&#35774;&#22791;&#19978;&#37096;&#32626;&#21644;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#38754;&#20020;&#35768;&#22810;&#25361;&#25112;&#65292;&#20854;&#20013;&#36739;&#20302;&#30340;&#20869;&#23384;&#26159;&#37096;&#32626;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#26102;&#32463;&#24120;&#36935;&#21040;&#30340;&#26368;&#22823;&#38480;&#21046;&#22240;&#32032;&#20043;&#19968;&#12290;&#24352;&#37327;&#37325;&#31639;&#26159;&#35299;&#20915;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#21644;&#25512;&#29702;&#25152;&#38656;&#39640;&#20869;&#23384;&#38656;&#27714;&#30340;&#19968;&#31181;&#26041;&#24335;&#12290;&#26412;&#25991;&#32771;&#34385;&#22312;&#20869;&#23384;&#39044;&#31639;&#19979;&#26368;&#23567;&#21270;&#35745;&#31639;&#22270;&#30340;&#25191;&#34892;&#26102;&#38388;&#38382;&#39064;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#32422;&#26463;&#32534;&#31243;&#24418;&#24335;&#65292;&#31216;&#20026;Moccasin&#65292;&#20854;&#20013;&#21482;&#26377;$O(n)$&#20010;&#25972;&#25968;&#21464;&#37327;&#65292;$n$&#26159;&#35745;&#31639;&#22270;&#20013;&#33410;&#28857;&#30340;&#25968;&#37327;&#12290;&#36825;&#30456;&#23545;&#20110;&#26368;&#36817;&#25991;&#29486;&#20013;&#25552;&#20986;&#30340;&#20855;&#26377;$O(n^2)$&#24067;&#23572;&#21464;&#37327;&#30340;&#20844;&#24335;&#25552;&#20986;&#20102;&#26174;&#30528;&#30340;&#25913;&#36827;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25968;&#20540;&#30740;&#31350;&#32467;&#26524;&#65292;&#34920;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22823;&#35268;&#27169;&#22270;&#19978;&#27604;&#26368;&#36817;&#30340;&#24037;&#20316;&#24555;&#19968;&#20010;&#25968;&#37327;&#32423;&#12290;
&lt;/p&gt;
&lt;p&gt;
The deployment and training of neural networks on edge computing devices pose many challenges. The low memory nature of edge devices is often one of the biggest limiting factors encountered in the deployment of large neural network models. Tensor rematerialization or recompute is a way to address high memory requirements for neural network training and inference. In this paper we consider the problem of execution time minimization of compute graphs subject to a memory budget. In particular, we develop a new constraint programming formulation called \textsc{Moccasin} with only $O(n)$ integer variables, where $n$ is the number of nodes in the compute graph. This is a significant improvement over the works in the recent literature that propose formulations with $O(n^2)$ Boolean variables. We present numerical studies that show that our approach is up to an order of magnitude faster than recent work especially for large-scale graphs.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#32456;&#31471;&#29992;&#25143;&#24320;&#21457;&#65288;EUD&#65289;&#23545;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#24433;&#21709;&#65292;&#30446;&#30340;&#26159;&#20351;&#38750;&#25216;&#26415;&#29992;&#25143;&#30452;&#25509;&#20197;&#28385;&#36275;&#20854;&#38656;&#27714;&#30340;&#26041;&#24335;&#21442;&#19982;&#21040;&#20154;&#24037;&#26234;&#33021;&#30340;&#23450;&#20041;&#21644;&#20010;&#24615;&#21270;&#20013;&#12290;&#27492;&#22806;&#65292;&#25991;&#31456;&#36824;&#35780;&#20272;&#20102;EUD&#38754;&#20020;&#30340;&#25361;&#25112;&#12289;&#28508;&#22312;&#30340;&#22909;&#22788;&#20197;&#21450;&#23558;&#20854;&#25972;&#21512;&#21040;&#25972;&#20010;&#20154;&#24037;&#26234;&#33021;&#24320;&#21457;&#20013;&#30340;&#26410;&#26469;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2304.09863</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#30340;&#32456;&#31471;&#29992;&#25143;&#24320;&#21457;&#65306;&#31995;&#32479;&#24615;&#25991;&#29486;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
End-User Development for Artificial Intelligence: A Systematic Literature Review. (arXiv:2304.09863v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09863
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#32456;&#31471;&#29992;&#25143;&#24320;&#21457;&#65288;EUD&#65289;&#23545;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#24433;&#21709;&#65292;&#30446;&#30340;&#26159;&#20351;&#38750;&#25216;&#26415;&#29992;&#25143;&#30452;&#25509;&#20197;&#28385;&#36275;&#20854;&#38656;&#27714;&#30340;&#26041;&#24335;&#21442;&#19982;&#21040;&#20154;&#24037;&#26234;&#33021;&#30340;&#23450;&#20041;&#21644;&#20010;&#24615;&#21270;&#20013;&#12290;&#27492;&#22806;&#65292;&#25991;&#31456;&#36824;&#35780;&#20272;&#20102;EUD&#38754;&#20020;&#30340;&#25361;&#25112;&#12289;&#28508;&#22312;&#30340;&#22909;&#22788;&#20197;&#21450;&#23558;&#20854;&#25972;&#21512;&#21040;&#25972;&#20010;&#20154;&#24037;&#26234;&#33021;&#24320;&#21457;&#20013;&#30340;&#26410;&#26469;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20154;&#24037;&#26234;&#33021;&#22312;&#25105;&#20204;&#31038;&#20250;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#21019;&#24314;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#20960;&#20046;&#24635;&#26159;IT&#21644;&#20154;&#24037;&#26234;&#33021;&#19987;&#23478;&#30340;&#19987;&#23646;&#26435;&#21033;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#8212;&#8212;&#32456;&#31471;&#29992;&#25143;&#24320;&#21457;&#65288;EUD&#65289;&#65292;&#20351;&#24471;&#38750;&#25216;&#26415;&#20154;&#21592;&#21487;&#20197;&#30452;&#25509;&#21442;&#19982;&#23450;&#20041;&#21644;&#20010;&#24615;&#21270;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#65292;&#20174;&#32780;&#22686;&#24378;AI&#31995;&#32479;&#30340;&#25928;&#26524;&#12290;&#26412;&#25991;&#36890;&#36807;&#31995;&#32479;&#24615;&#25991;&#29486;&#32508;&#36848;&#38416;&#36848;&#20102;&#24403;&#21069;EUD&#23545;AI&#31995;&#32479;&#30340;&#21457;&#23637;&#26684;&#23616;&#65292;&#21363;&#22914;&#20309;&#20351;&#29992;&#25143;&#22312;&#27809;&#26377;&#20154;&#24037;&#26234;&#33021;&#21644;/&#25110;&#32534;&#31243;&#25216;&#33021;&#30340;&#24773;&#20917;&#19979;&#65292;&#23450;&#21046;&#20154;&#24037;&#26234;&#33021;&#30340;&#34892;&#20026;&#26469;&#28385;&#36275;&#20182;&#20204;&#30340;&#38656;&#27714;&#12290;&#27492;&#22806;&#65292;&#26412;&#30740;&#31350;&#36824;&#35752;&#35770;&#20102;EUD&#38754;&#23545;&#30340;&#24403;&#21069;&#25361;&#25112;&#12289;&#28508;&#22312;&#30340;&#22909;&#22788;&#20197;&#21450;&#23558;EUD&#25972;&#21512;&#21040;&#25972;&#20010;&#20154;&#24037;&#26234;&#33021;&#24320;&#21457;&#36807;&#31243;&#20013;&#30340;&#26410;&#26469;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, Artificial Intelligence has become more and more relevant in our society. Creating AI systems is almost always the prerogative of IT and AI experts. However, users may need to create intelligent solutions tailored to their specific needs. In this way, AI systems can be enhanced if new approaches are devised to allow non-technical users to be directly involved in the definition and personalization of AI technologies. End-User Development (EUD) can provide a solution to these problems, allowing people to create, customize, or adapt AI-based systems to their own needs. This paper presents a systematic literature review that aims to shed the light on the current landscape of EUD for AI systems, i.e., how users, even without skills in AI and/or programming, can customize the AI behavior to their needs. This study also discusses the current challenges of EUD for AI, the potential benefits, and the future implications of integrating EUD into the overall AI development process
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#27169;&#24577;&#23545;&#27604;&#25439;&#22833;&#20989;&#25968;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#24494;&#35843;&#26399;&#38388;&#26816;&#27979;&#21644;&#26126;&#30830;&#21306;&#20998;&#21463;&#24433;&#21709;&#31867;&#21035;&#30340;&#38169;&#35823;&#23646;&#24615;&#65292;&#32531;&#35299;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#38169;&#35823;&#30456;&#20851;&#24615;&#65292;&#21516;&#26102;&#25552;&#39640;&#27169;&#22411;&#31934;&#24230;&#21644;&#25351;&#21521;&#30446;&#26631;&#39046;&#22495;&#30340;&#26377;&#24847;&#20041;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2304.03916</link><description>&lt;p&gt;
&#22312;&#24494;&#35843;&#26102;&#32531;&#35299;&#22810;&#27169;&#24577;&#27169;&#22411;&#20013;&#30340;&#38169;&#35823;&#30456;&#20851;&#24615;
&lt;/p&gt;
&lt;p&gt;
Mitigating Spurious Correlations in Multi-modal Models during Fine-tuning. (arXiv:2304.03916v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03916
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#27169;&#24577;&#23545;&#27604;&#25439;&#22833;&#20989;&#25968;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#24494;&#35843;&#26399;&#38388;&#26816;&#27979;&#21644;&#26126;&#30830;&#21306;&#20998;&#21463;&#24433;&#21709;&#31867;&#21035;&#30340;&#38169;&#35823;&#23646;&#24615;&#65292;&#32531;&#35299;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#38169;&#35823;&#30456;&#20851;&#24615;&#65292;&#21516;&#26102;&#25552;&#39640;&#27169;&#22411;&#31934;&#24230;&#21644;&#25351;&#21521;&#30446;&#26631;&#39046;&#22495;&#30340;&#26377;&#24847;&#20041;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25439;&#23475;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#25110;&#23548;&#33268;&#27169;&#22411;&#22522;&#20110;&#38169;&#35823;&#21407;&#22240;&#30340;&#38169;&#35823;&#30456;&#20851;&#24615;&#26159;&#23454;&#38469;&#37096;&#32626;&#38754;&#20020;&#30340;&#20027;&#35201;&#40065;&#26834;&#24615;&#38382;&#39064;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#22312;&#39044;&#35757;&#32451;&#22823;&#22411;&#27169;&#22411;&#26399;&#38388;&#32531;&#35299;&#36825;&#20123;&#30456;&#20851;&#24615;&#21487;&#33021;&#25104;&#26412;&#39640;&#26114;&#19988;&#19981;&#20999;&#23454;&#38469;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#27809;&#26377;&#39640;&#24615;&#33021;&#35745;&#31639;&#36164;&#28304;&#30340;&#20154;&#26469;&#35828;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#29305;&#23450;&#39046;&#22495;&#30340;&#24494;&#35843;&#26399;&#38388;&#30340;&#38169;&#35823;&#30456;&#20851;&#24615;&#12290;&#38024;&#23545;&#22810;&#27169;&#24577;&#27169;&#22411;&#65288;&#20363;&#22914;CLIP&#65289;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21033;&#29992;&#36825;&#20123;&#27169;&#22411;&#20013;&#30340;&#19981;&#21516;&#27169;&#24577;&#26469;&#26816;&#27979;&#24182;&#26126;&#30830;&#21306;&#20998;&#21463;&#24433;&#21709;&#31867;&#21035;&#30340;&#38169;&#35823;&#23646;&#24615;&#65292;&#36890;&#36807;&#34920;&#36798;&#35821;&#35328;&#30340;&#22810;&#27169;&#24577;&#23545;&#27604;&#25439;&#22833;&#20989;&#25968;&#26469;&#23454;&#29616;&#12290;&#25105;&#20204;&#22312;CLIP&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#35777;&#26126;&#21644;&#28145;&#20837;&#30340;&#21487;&#35270;&#21270;&#26174;&#31034;&#65292;&#36825;&#31181;&#20171;&#20837;&#33021;&#22815;&#26377;&#25928;&#22320;&#25552;&#39640;&#27169;&#22411;&#31934;&#24230;&#65292;&#32780;&#19981;&#23384;&#22312;&#38169;&#35823;&#23646;&#24615;&#65292;&#24182;&#23558;&#27169;&#22411;&#25351;&#21521;&#30446;&#26631;&#39046;&#22495;&#30340;&#26377;&#24847;&#20041;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spurious correlations that degrade model generalization or lead the model to be right for the wrong reasons are one of the main robustness concerns for real-world deployments. However, mitigating these correlations during pre-training for large-scale models can be costly and impractical, particularly for those without access to high-performance computing resources. This paper proposes a novel approach to address spurious correlations during fine-tuning for a given domain of interest. With a focus on multi-modal models (e.g., CLIP), the proposed method leverages different modalities in these models to detect and explicitly set apart spurious attributes from the affected class, achieved through a multi-modal contrastive loss function that expresses spurious relationships through language. Our experimental results and in-depth visualizations on CLIP show that such an intervention can effectively i) improve the model's accuracy when spurious attributes are not present, and ii) directs the 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;cTBL&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#34920;&#26684;&#20013;&#26816;&#32034;&#20449;&#24687;&#65292;&#24182;&#29983;&#25104;&#20855;&#26377;&#26816;&#32034;&#20449;&#24687;&#25903;&#25745;&#30340;&#23545;&#35805;&#21709;&#24212;&#65292;&#20854;&#20013;&#20351;&#29992;&#20102;&#36716;&#25442;&#22120;&#32534;&#30721;&#22120;&#23884;&#20837;&#36827;&#34892;&#27987;&#23494;&#34920;&#26816;&#32034;&#65292;&#21487;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.12024</link><description>&lt;p&gt;
cTBL&#65306;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#23545;&#35805;&#34920;&#26684;
&lt;/p&gt;
&lt;p&gt;
cTBL: Augmenting Large Language Models for Conversational Tables. (arXiv:2303.12024v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12024
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;cTBL&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#34920;&#26684;&#20013;&#26816;&#32034;&#20449;&#24687;&#65292;&#24182;&#29983;&#25104;&#20855;&#26377;&#26816;&#32034;&#20449;&#24687;&#25903;&#25745;&#30340;&#23545;&#35805;&#21709;&#24212;&#65292;&#20854;&#20013;&#20351;&#29992;&#20102;&#36716;&#25442;&#22120;&#32534;&#30721;&#22120;&#23884;&#20837;&#36827;&#34892;&#27987;&#23494;&#34920;&#26816;&#32034;&#65292;&#21487;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#23545;&#35805;&#20154;&#24037;&#26234;&#33021;&#20013;&#19968;&#20010;&#24320;&#25918;&#30340;&#25361;&#25112;&#26159;&#22914;&#20309;&#20174;&#25991;&#26412;&#21644;&#38750;&#25991;&#26412;&#26469;&#28304;&#20013;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20197;&#36827;&#34892;&#22810;&#36718;&#23545;&#35805;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;Conversation Table (cTBL)&#65292;&#36825;&#26159;&#19968;&#31181;&#19977;&#27493;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26041;&#27861;&#65292;&#29992;&#20110;&#26816;&#32034;&#34920;&#26684;&#20449;&#24687;&#24182;&#29983;&#25104;&#22522;&#20110;&#26816;&#32034;&#20449;&#24687;&#30340;&#23545;&#35805;&#21709;&#24212;&#12290;cTBL&#20351;&#29992;&#36716;&#25442;&#22120;&#32534;&#30721;&#22120;&#23884;&#20837;&#36827;&#34892;&#27987;&#23494;&#34920;&#26816;&#32034;&#65292;&#24182;&#22312;HyrbiDialogue&#25968;&#25454;&#38598;Top-1&#21644;Top-3&#20934;&#30830;&#24615;&#19978;&#30456;&#23545;&#20110;&#31232;&#30095;&#26816;&#32034;&#25552;&#39640;&#20102;&#26368;&#22810;5%&#12290;&#27492;&#22806;&#65292;cTBL&#20351;&#29992;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#27169;&#22411;&#36827;&#34892;&#34920;&#26684;&#30693;&#35782;&#26816;&#32034;&#65292;&#22312;HyrbiDialogue&#19978;&#20135;&#29983;&#20102;&#26368;&#39640;46%&#30340;ROUGE&#20998;&#25968;&#30456;&#23545;&#25913;&#36827;&#65292;&#24182;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#20154;&#24037;&#35780;&#20272;&#21709;&#24212;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
An open challenge in multimodal conversational AI requires augmenting large language models with information from textual and non-textual sources for multi-turn dialogue. To address this problem, this paper introduces Conversational Tables (cTBL), a three-step encoder-decoder approach to retrieve tabular information and generate dialogue responses grounded on the retrieved information. cTBL uses Transformer encoder embeddings for Dense Table Retrieval and obtains up to 5% relative improvement in Top-1 and Top-3 accuracy over sparse retrieval on the HyrbiDialogue dataset. Additionally, cTBL performs tabular knowledge retrieval using both encoder and decoder models, resulting in up to 46% relative improvement in ROUGE scores and better human evaluation for response generation on HyrbiDialogue.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20174;&#22270;&#30340;&#35282;&#24230;&#23457;&#26597;&#20102;&#25552;&#31034;&#26041;&#27861;&#65292;&#23558;&#25552;&#31034;&#20989;&#25968;&#19982;&#22270;&#30693;&#35782;&#30456;&#32467;&#21512;&#65292;&#20197;&#35299;&#20915;&#22312;&#22797;&#26434;&#20219;&#21153;&#20013;&#35774;&#35745;&#25552;&#31034;&#30340;&#25361;&#25112;&#65292;&#22312;&#27492;&#22522;&#30784;&#19978;&#32452;&#32455;&#29616;&#26377;&#24037;&#20316;&#65292;&#24182;&#25551;&#36848;&#20102;&#24212;&#29992;&#21644;&#26410;&#26469;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2303.07275</link><description>&lt;p&gt;
&#22270;&#24418;&#25552;&#31034;&#26041;&#27861;&#32508;&#36848;&#65306;&#25216;&#26415;&#65292;&#24212;&#29992;&#21644;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
A Survey of Graph Prompting Methods: Techniques, Applications, and Challenges. (arXiv:2303.07275v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07275
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20174;&#22270;&#30340;&#35282;&#24230;&#23457;&#26597;&#20102;&#25552;&#31034;&#26041;&#27861;&#65292;&#23558;&#25552;&#31034;&#20989;&#25968;&#19982;&#22270;&#30693;&#35782;&#30456;&#32467;&#21512;&#65292;&#20197;&#35299;&#20915;&#22312;&#22797;&#26434;&#20219;&#21153;&#20013;&#35774;&#35745;&#25552;&#31034;&#30340;&#25361;&#25112;&#65292;&#22312;&#27492;&#22522;&#30784;&#19978;&#32452;&#32455;&#29616;&#26377;&#24037;&#20316;&#65292;&#24182;&#25551;&#36848;&#20102;&#24212;&#29992;&#21644;&#26410;&#26469;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#8220;&#39044;&#35757;&#32451;&#65292;&#25552;&#31034;&#65292;&#39044;&#27979;&#35757;&#32451;&#8221;&#33539;&#20363;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#29992;&#26377;&#38480;&#26631;&#35760;&#25968;&#25454;&#23398;&#20064;&#26222;&#36866;&#27169;&#22411;&#30340;&#26041;&#24335;&#12290;&#36825;&#31181;&#26041;&#27861;&#28041;&#21450;&#20351;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#19968;&#20010;&#25552;&#31034;&#20989;&#25968;&#65292;&#35813;&#20989;&#25968;&#23558;&#27169;&#26495;&#24212;&#29992;&#20110;&#36755;&#20837;&#26679;&#26412;&#65292;&#28155;&#21152;&#25351;&#31034;&#32972;&#26223;&#24182;&#23558;&#30446;&#26631;&#20219;&#21153;&#37325;&#26500;&#20026;&#39044;&#35757;&#32451;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#22312;&#22797;&#26434;&#20219;&#21153;&#20013;&#35774;&#35745;&#25552;&#31034;&#21487;&#33021;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#21644;&#32791;&#26102;&#30340;&#36807;&#31243;&#12290;&#36825;&#31181;&#38480;&#21046;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#22270;&#25968;&#25454;&#26469;&#35299;&#20915;&#65292;&#22240;&#20026;&#22270;&#24418;&#20316;&#20026;&#26174;&#24335;&#22320;&#24314;&#27169;&#23454;&#20307;&#20043;&#38388;&#20132;&#20114;&#30340;&#32467;&#26500;&#21270;&#30693;&#35782;&#24211;&#12290;&#22312;&#36825;&#20010;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#20174;&#22270;&#30340;&#35282;&#24230;&#23457;&#26597;&#25552;&#31034;&#26041;&#27861;&#65292;&#20854;&#20013;&#25552;&#31034;&#20989;&#25968;&#20351;&#29992;&#22270;&#30693;&#35782;&#36827;&#34892;&#25193;&#23637;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#22270;&#24418;&#25552;&#31034;&#23398;&#20064;&#30340;&#22522;&#26412;&#27010;&#24565;&#65292;&#32452;&#32455;&#20102;&#35774;&#35745;&#22270;&#24418;&#25552;&#31034;&#20989;&#25968;&#30340;&#29616;&#26377;&#24037;&#20316;&#65292;&#24182;&#25551;&#36848;&#20102;&#23427;&#20204;&#30340;&#24212;&#29992;&#21644;&#26410;&#26469;&#25361;&#25112;&#12290;&#26412;&#35843;&#26597;&#23558;&#24357;&#21512;&#22270;&#24418;&#21644;&#25552;&#31034;&#35774;&#35745;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent "pre-train, prompt, predict training" paradigm has gained popularity as a way to learn generalizable models with limited labeled data. The approach involves using a pre-trained model and a prompting function that applies a template to input samples, adding indicative context and reformulating target tasks as the pre-training task. However, the design of prompts could be a challenging and time-consuming process in complex tasks. The limitation can be addressed by using graph data, as graphs serve as structured knowledge repositories by explicitly modeling the interaction between entities. In this survey, we review prompting methods from the graph perspective, where prompting functions are augmented with graph knowledge. In particular, we introduce the basic concepts of graph prompt learning, organize the existing work of designing graph prompting functions, and describe their applications and future challenges. This survey will bridge the gap between graphs and prompt design 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26597;&#35810;-&#35805;&#35821;&#27880;&#24847;&#21147;&#21644;&#32852;&#21512;&#24314;&#27169;&#30340;&#26597;&#35810;&#24863;&#30693;&#26694;&#26550;&#65292;&#23427;&#20351;&#29992;&#23494;&#38598;&#26816;&#32034;&#27169;&#22359;&#35745;&#31639;&#35805;&#35821;&#32423;&#21035;&#19982;&#26597;&#35810;&#30340;&#30456;&#20851;&#24615;&#65292;&#24182;&#23558;&#26631;&#35760;&#32423;&#21035;&#30340;&#26597;&#35810;&#20851;&#32852;&#24615;&#21644;&#35805;&#35821;&#32423;&#21035;&#30340;&#26597;&#35810;&#20851;&#32852;&#24615;&#32467;&#21512;&#36215;&#26469;&#65292;&#23454;&#29616;&#29983;&#25104;&#19968;&#20010;&#26356;&#19982;&#26597;&#35810;&#30456;&#20851;&#30340;&#25688;&#35201;&#12290;&#32463;&#36807;&#23545;&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#27979;&#35797;&#65292;&#34920;&#26126;&#35813;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#30340;QFMS&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2303.04487</link><description>&lt;p&gt;
&#22522;&#20110;&#26597;&#35810;-&#35805;&#35821;&#27880;&#24847;&#21147;&#21644;&#32852;&#21512;&#24314;&#27169;&#30340;&#26597;&#35810;&#28966;&#28857;&#20250;&#35758;&#25688;&#35201;
&lt;/p&gt;
&lt;p&gt;
Query-Utterance Attention with Joint modeling for Query-Focused Meeting Summarization. (arXiv:2303.04487v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.04487
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26597;&#35810;-&#35805;&#35821;&#27880;&#24847;&#21147;&#21644;&#32852;&#21512;&#24314;&#27169;&#30340;&#26597;&#35810;&#24863;&#30693;&#26694;&#26550;&#65292;&#23427;&#20351;&#29992;&#23494;&#38598;&#26816;&#32034;&#27169;&#22359;&#35745;&#31639;&#35805;&#35821;&#32423;&#21035;&#19982;&#26597;&#35810;&#30340;&#30456;&#20851;&#24615;&#65292;&#24182;&#23558;&#26631;&#35760;&#32423;&#21035;&#30340;&#26597;&#35810;&#20851;&#32852;&#24615;&#21644;&#35805;&#35821;&#32423;&#21035;&#30340;&#26597;&#35810;&#20851;&#32852;&#24615;&#32467;&#21512;&#36215;&#26469;&#65292;&#23454;&#29616;&#29983;&#25104;&#19968;&#20010;&#26356;&#19982;&#26597;&#35810;&#30456;&#20851;&#30340;&#25688;&#35201;&#12290;&#32463;&#36807;&#23545;&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#27979;&#35797;&#65292;&#34920;&#26126;&#35813;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#30340;QFMS&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26597;&#35810;&#28966;&#28857;&#20250;&#35758;&#25688;&#35201;&#65288;QFMS&#65289;&#26088;&#22312;&#26681;&#25454;&#32473;&#23450;&#30340;&#26597;&#35810;&#65292;&#20174;&#20250;&#35758;&#35760;&#24405;&#20013;&#29983;&#25104;&#25688;&#35201;&#12290;&#20197;&#24448;&#30340;&#26041;&#27861;&#36890;&#24120;&#23558;&#26597;&#35810;&#19982;&#20250;&#35758;&#35760;&#24405;&#25340;&#25509;&#36215;&#26469;&#65292;&#24182;&#20351;&#29992;&#27880;&#24847;&#26426;&#21046;&#38544;&#24335;&#22320;&#23545;&#26631;&#35760;&#32423;&#21035;&#30340;&#26597;&#35810;&#30456;&#20851;&#24615;&#36827;&#34892;&#24314;&#27169;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#38271;&#26102;&#38388;&#30340;&#20250;&#35758;&#35760;&#24405;&#23548;&#33268;&#20851;&#38190;&#30340;&#26597;&#35810;&#30456;&#20851;&#20449;&#24687;&#34987;&#31232;&#37322;&#65292;&#22240;&#27492;&#21407;&#22987;&#30340;&#22522;&#20110;&#36716;&#25442;&#30340;&#27169;&#22411;&#19981;&#36275;&#20197;&#31361;&#20986;&#19982;&#26597;&#35810;&#30456;&#20851;&#30340;&#20851;&#38190;&#37096;&#20998;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26597;&#35810;-&#35805;&#35821;&#27880;&#24847;&#21147;&#21644;&#32852;&#21512;&#24314;&#27169;&#30340;&#26597;&#35810;&#24863;&#30693;&#26694;&#26550;&#12290;&#23427;&#20351;&#29992;&#23494;&#38598;&#26816;&#32034;&#27169;&#22359;&#35745;&#31639;&#35805;&#35821;&#32423;&#21035;&#19982;&#26597;&#35810;&#30340;&#30456;&#20851;&#24615;&#12290;&#28982;&#21518;&#65292;&#23558;&#26631;&#35760;&#32423;&#21035;&#30340;&#26597;&#35810;&#20851;&#32852;&#24615;&#21644;&#35805;&#35821;&#32423;&#21035;&#30340;&#26597;&#35810;&#20851;&#32852;&#24615;&#32467;&#21512;&#36215;&#26469;&#65292;&#24182;&#36890;&#36807;&#26126;&#30830;&#30340;&#27880;&#24847;&#26426;&#21046;&#25972;&#21512;&#21040;&#29983;&#25104;&#36807;&#31243;&#20013;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#19981;&#21516;&#39063;&#31890;&#24230;&#30340;&#26597;&#35810;&#30456;&#20851;&#24615;&#26377;&#21161;&#20110;&#29983;&#25104;&#19968;&#20010;&#26356;&#19982;&#26597;&#35810;&#30456;&#20851;&#30340;&#25688;&#35201;&#12290;&#22312;&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#30340;QFMS&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Query-focused meeting summarization (QFMS) aims to generate summaries from meeting transcripts in response to a given query. Previous works typically concatenate the query with meeting transcripts and implicitly model the query relevance only at the token level with attention mechanism. However, due to the dilution of key query-relevant information caused by long meeting transcripts, the original transformer-based model is insufficient to highlight the key parts related to the query. In this paper, we propose a query-aware framework with joint modeling token and utterance based on Query-Utterance Attention. It calculates the utterance-level relevance to the query with a dense retrieval module. Then both token-level query relevance and utterance-level query relevance are combined and incorporated into the generation process with attention mechanism explicitly. We show that the query relevance of different granularities contributes to generating a summary more related to the query. Exper
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#20197;&#39044;&#27979;&#20854;&#20182;&#31070;&#32463;&#32593;&#32476;&#39640;&#36136;&#37327;ImageNet&#21442;&#25968;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#20351;&#29992;&#39044;&#27979;&#21442;&#25968;&#36827;&#34892;&#21021;&#22987;&#21270;&#65292;&#33021;&#22815;&#25552;&#39640;&#22810;&#31181;ImageNet&#27169;&#22411;&#30340;&#35757;&#32451;&#36895;&#24230;&#65292;&#24182;&#19988;&#22312;&#36716;&#31227;&#21040;&#20854;&#20182;&#25968;&#25454;&#38598;&#26102;&#21487;&#20197;&#26356;&#24555;&#22320;&#25910;&#25947;&#24182;&#36798;&#21040;&#31454;&#20105;&#21147;&#30340;&#26368;&#32456;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.04143</link><description>&lt;p&gt;
&#25105;&#20204;&#33021;&#21542;&#23558;Transformer&#24212;&#29992;&#21040;&#22810;&#31181;ImageNet&#27169;&#22411;&#30340;&#21442;&#25968;&#39044;&#27979;&#20013;&#36827;&#34892;&#25193;&#23637;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can We Scale Transformers to Predict Parameters of Diverse ImageNet Models?. (arXiv:2303.04143v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.04143
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#20197;&#39044;&#27979;&#20854;&#20182;&#31070;&#32463;&#32593;&#32476;&#39640;&#36136;&#37327;ImageNet&#21442;&#25968;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#20351;&#29992;&#39044;&#27979;&#21442;&#25968;&#36827;&#34892;&#21021;&#22987;&#21270;&#65292;&#33021;&#22815;&#25552;&#39640;&#22810;&#31181;ImageNet&#27169;&#22411;&#30340;&#35757;&#32451;&#36895;&#24230;&#65292;&#24182;&#19988;&#22312;&#36716;&#31227;&#21040;&#20854;&#20182;&#25968;&#25454;&#38598;&#26102;&#21487;&#20197;&#26356;&#24555;&#22320;&#25910;&#25947;&#24182;&#36798;&#21040;&#31454;&#20105;&#21147;&#30340;&#26368;&#32456;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#23545;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#39044;&#35757;&#32451;&#24050;&#25104;&#20026;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#22522;&#30707;&#65292;&#20294;&#36825;&#21482;&#33021;&#30001;&#19968;&#20123;&#25317;&#26377;&#20805;&#36275;&#36164;&#28304;&#30340;&#31038;&#21306;&#23454;&#29616;&#12290;&#25105;&#20204;&#26088;&#22312;&#23454;&#29616;&#19968;&#20010;&#38596;&#24515;&#21187;&#21187;&#30340;&#30446;&#26631;&#65306;&#27665;&#20027;&#21270;&#39044;&#35757;&#32451;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#35757;&#32451;&#24182;&#21457;&#24067;&#20102;&#19968;&#20010;&#21333;&#19968;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#21487;&#20197;&#39044;&#27979;&#20854;&#20182;&#31070;&#32463;&#32593;&#32476;&#30340;&#39640;&#36136;&#37327;ImageNet&#21442;&#25968;&#12290;&#36890;&#36807;&#20351;&#29992;&#39044;&#27979;&#21442;&#25968;&#36827;&#34892;&#21021;&#22987;&#21270;&#65292;&#25105;&#20204;&#21487;&#20197;&#25552;&#39640;PyTorch&#20013;&#21487;&#29992;&#30340;&#21508;&#31181;ImageNet&#27169;&#22411;&#30340;&#35757;&#32451;&#36895;&#24230;&#12290;&#22312;&#36716;&#31227;&#21040;&#20854;&#20182;&#25968;&#25454;&#38598;&#26102;&#65292;&#20351;&#29992;&#39044;&#27979;&#21442;&#25968;&#21021;&#22987;&#21270;&#30340;&#27169;&#22411;&#20063;&#20250;&#26356;&#24555;&#22320;&#25910;&#25947;&#24182;&#36798;&#21040;&#31454;&#20105;&#21147;&#30340;&#26368;&#32456;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pretraining a neural network on a large dataset is becoming a cornerstone in machine learning that is within the reach of only a few communities with large-resources. We aim at an ambitious goal of democratizing pretraining. Towards that goal, we train and release a single neural network that can predict high quality ImageNet parameters of other neural networks. By using predicted parameters for initialization we are able to boost training of diverse ImageNet models available in PyTorch. When transferred to other datasets, models initialized with predicted parameters also converge faster and reach competitive final performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20998;&#24067;&#24335;&#23545;&#40784;&#25628;&#32034;&#65288;DAS&#65289;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#20351;&#29992;&#26292;&#21147;&#25628;&#32034;&#30340;&#24773;&#20917;&#19979;&#25214;&#21040;&#39640;&#23618;&#22240;&#26524;&#27169;&#22411;&#21644;&#20302;&#23618;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#20043;&#38388;&#30340;&#23545;&#40784;&#26041;&#27861;&#65292;&#24182;&#19988;DAS&#21487;&#20197;&#21457;&#29616;&#20808;&#21069;&#26041;&#27861;&#24573;&#30053;&#30340;&#20869;&#37096;&#32467;&#26500;&#12290;DAS&#31639;&#27861;&#26377;&#28508;&#21147;&#23454;&#29616;&#23545;&#22797;&#26434;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#30340;&#26356;&#22909;&#35299;&#37322;&#21644;&#29702;&#35299;&#12290;</title><link>http://arxiv.org/abs/2303.02536</link><description>&lt;p&gt;
&#22312;&#21487;&#35299;&#37322;&#30340;&#22240;&#26524;&#21464;&#37327;&#21644;&#20998;&#24067;&#24335;&#31070;&#32463;&#34920;&#31034;&#20043;&#38388;&#23547;&#25214;&#23545;&#40784;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Finding Alignments Between Interpretable Causal Variables and Distributed Neural Representations. (arXiv:2303.02536v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.02536
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20998;&#24067;&#24335;&#23545;&#40784;&#25628;&#32034;&#65288;DAS&#65289;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#20351;&#29992;&#26292;&#21147;&#25628;&#32034;&#30340;&#24773;&#20917;&#19979;&#25214;&#21040;&#39640;&#23618;&#22240;&#26524;&#27169;&#22411;&#21644;&#20302;&#23618;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#20043;&#38388;&#30340;&#23545;&#40784;&#26041;&#27861;&#65292;&#24182;&#19988;DAS&#21487;&#20197;&#21457;&#29616;&#20808;&#21069;&#26041;&#27861;&#24573;&#30053;&#30340;&#20869;&#37096;&#32467;&#26500;&#12290;DAS&#31639;&#27861;&#26377;&#28508;&#21147;&#23454;&#29616;&#23545;&#22797;&#26434;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#30340;&#26356;&#22909;&#35299;&#37322;&#21644;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#25277;&#35937;&#26159;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#30340;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#23427;&#23450;&#20041;&#20102;&#21487;&#35299;&#37322;&#30340;&#39640;&#23618;&#22240;&#26524;&#27169;&#22411;&#20309;&#26102;&#26159;&#20302;&#23618;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#30340;&#21487;&#20449;&#31616;&#21270;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22240;&#26524;&#25277;&#35937;&#26041;&#27861;&#23384;&#22312;&#20004;&#20010;&#20027;&#35201;&#38480;&#21046;&#65306;&#23427;&#20204;&#38656;&#35201;&#22312;&#39640;&#23618;&#27169;&#22411;&#21644;&#20302;&#23618;&#27169;&#22411;&#20043;&#38388;&#36827;&#34892;&#26292;&#21147;&#25628;&#32034;&#23545;&#40784;&#65292;&#24182;&#19988;&#23427;&#20204;&#39044;&#35774;&#39640;&#23618;&#27169;&#22411;&#20013;&#30340;&#21464;&#37327;&#23558;&#19982;&#20302;&#23618;&#27169;&#22411;&#20013;&#30340;&#19981;&#30456;&#20132;&#30340;&#31070;&#32463;&#20803;&#38598;&#23545;&#40784;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20998;&#24067;&#24335;&#23545;&#40784;&#25628;&#32034;&#65288;DAS&#65289;&#65292;&#23427;&#20811;&#26381;&#20102;&#36825;&#20123;&#38480;&#21046;&#12290;&#22312;DAS&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#25214;&#21040;&#39640;&#23618;&#27169;&#22411;&#21644;&#20302;&#23618;&#27169;&#22411;&#20043;&#38388;&#30340;&#23545;&#40784;&#26041;&#27861;&#65292;&#20801;&#35768;&#20010;&#20307;&#31070;&#32463;&#20803;&#22312;&#38750;&#20256;&#32479;&#22522;&#24213;&#20998;&#24067;&#34920;&#31034;&#20013;&#21457;&#25381;&#22810;&#20010;&#19981;&#21516;&#30340;&#35282;&#33394;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;DAS&#21487;&#20197;&#21457;&#29616;&#20808;&#21069;&#26041;&#27861;&#24573;&#30053;&#30340;&#20869;&#37096;&#32467;&#26500;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;DAS&#26377;&#28508;&#21147;&#23454;&#29616;&#23545;&#22797;&#26434;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#30340;&#26356;&#22909;&#35299;&#37322;&#21644;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Causal abstraction is a promising theoretical framework for explainable artificial intelligence that defines when an interpretable high-level causal model is a faithful simplification of a low-level deep learning system. However, existing causal abstraction methods have two major limitations: they require a brute-force search over alignments between the high-level model and the low-level one, and they presuppose that variables in the high-level model will align with disjoint sets of neurons in the low-level one. In this paper, we present distributed alignment search (DAS), which overcomes these limitations. In DAS, we find the alignment between high-level and low-level models using gradient descent rather than conducting a brute-force search, and we allow individual neurons to play multiple distinct roles by analyzing representations in non-standard bases-distributed representations. Our experiments show that DAS can discover internal structure that prior approaches miss. Overall, DAS 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35777;&#26126;dropout&#19981;&#20165;&#21487;&#20197;&#38450;&#27490;&#31070;&#32463;&#32593;&#32476;&#36807;&#25311;&#21512;&#65292;&#36824;&#21487;&#20197;&#32531;&#35299;&#27424;&#25311;&#21512;&#38382;&#39064;&#12290;&#22312;&#35757;&#32451;&#21021;&#26399;&#37319;&#29992;early dropout&#26041;&#27861;&#65292;&#21487;&#20197;&#20943;&#23569;&#23567;&#25209;&#27425;&#26799;&#24230;&#30340;&#26041;&#21521;&#24046;&#24322;&#65292;&#32531;&#35299;SGD&#20013;&#30340;&#38543;&#26426;&#24615;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#30340;&#35757;&#32451;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.01500</link><description>&lt;p&gt;
&#12298;Dropout Reduces Underfitting&#12299;
&lt;/p&gt;
&lt;p&gt;
Dropout Reduces Underfitting. (arXiv:2303.01500v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.01500
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35777;&#26126;dropout&#19981;&#20165;&#21487;&#20197;&#38450;&#27490;&#31070;&#32463;&#32593;&#32476;&#36807;&#25311;&#21512;&#65292;&#36824;&#21487;&#20197;&#32531;&#35299;&#27424;&#25311;&#21512;&#38382;&#39064;&#12290;&#22312;&#35757;&#32451;&#21021;&#26399;&#37319;&#29992;early dropout&#26041;&#27861;&#65292;&#21487;&#20197;&#20943;&#23569;&#23567;&#25209;&#27425;&#26799;&#24230;&#30340;&#26041;&#21521;&#24046;&#24322;&#65292;&#32531;&#35299;SGD&#20013;&#30340;&#38543;&#26426;&#24615;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#30340;&#35757;&#32451;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#12298;Dropout Reduces Underfitting&#12299;&#26159;&#19968;&#31687;&#20851;&#20110;&#31070;&#32463;&#32593;&#32476;&#27491;&#21017;&#21270;&#26041;&#27861;Dropout&#30340;&#30740;&#31350;&#65292;&#35777;&#26126;&#20102;Dropout&#21487;&#20197;&#22312;&#35757;&#32451;&#21021;&#26399;&#38450;&#27490;&#27424;&#25311;&#21512;&#30340;&#25928;&#26524;&#12290;&#22312;&#30740;&#31350;&#20013;&#65292;&#21457;&#29616;Dropout&#21487;&#20197;&#20943;&#23569;&#23567;&#25209;&#27425;&#26799;&#24230;&#30340;&#26041;&#21521;&#24046;&#24322;&#65292;&#24182;&#26377;&#21161;&#20110;&#23558;&#23567;&#25209;&#27425;&#26799;&#24230;&#19982;&#25972;&#20010;&#25968;&#25454;&#38598;&#30340;&#26799;&#24230;&#23545;&#40784;&#65292;&#20174;&#32780;&#32531;&#35299;SGD&#20013;&#30340;&#38543;&#26426;&#24615;&#65292;&#20943;&#23569;&#27599;&#20010;&#25209;&#27425;&#23545;&#27169;&#22411;&#35757;&#32451;&#30340;&#24433;&#21709;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#35299;&#20915;&#27424;&#25311;&#21512;&#38382;&#39064;&#30340;&#26041;&#26696;&#8212;&#8212;early dropout: &#22312;&#35757;&#32451;&#30340;&#21021;&#22987;&#38454;&#27573;&#24212;&#29992;dropout&#65292;&#24182;&#22312;&#35757;&#32451;&#21518;&#20851;&#38381;dropout&#12290;&#30456;&#27604;&#20110;&#27809;&#26377;dropout&#30340;&#27169;&#22411;&#65292;&#37319;&#29992;early dropout&#30340;&#27169;&#22411;&#33021;&#22815;&#33719;&#24471;&#26356;&#20302;&#30340;&#26368;&#32456;&#35757;&#32451;&#25439;&#22833;&#12290;&#21478;&#22806;&#65292;&#25105;&#20204;&#36824;&#25506;&#35752;&#20102;&#19968;&#31181;&#29992;&#20110;&#27491;&#21017;&#21270;&#36807;&#25311;&#21512;&#27169;&#22411;&#30340;&#23545;&#31216;&#25216;&#26415;&#8212;&#8212;late dropout&#12290;
&lt;/p&gt;
&lt;p&gt;
Introduced by Hinton et al. in 2012, dropout has stood the test of time as a regularizer for preventing overfitting in neural networks. In this study, we demonstrate that dropout can also mitigate underfitting when used at the start of training. During the early phase, we find dropout reduces the directional variance of gradients across mini-batches and helps align the mini-batch gradients with the entire dataset's gradient. This helps counteract the stochasticity of SGD and limit the influence of individual batches on model training. Our findings lead us to a solution for improving performance in underfitting models - early dropout: dropout is applied only during the initial phases of training, and turned off afterwards. Models equipped with early dropout achieve lower final training loss compared to their counterparts without dropout. Additionally, we explore a symmetric technique for regularizing overfitting models - late dropout, where dropout is not used in the early iterations an
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;K-SHAP&#30340;&#31639;&#27861;&#65292;&#26469;&#35299;&#20915;&#22810;&#20010;&#26234;&#33021;&#20307;&#20445;&#25345;&#21311;&#21517;&#19988;&#20165;&#26377;&#29366;&#24577;-&#21160;&#20316;&#23545;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#26234;&#33021;&#20307;&#20915;&#31574;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2302.11996</link><description>&lt;p&gt;
K-SHAP: &#19968;&#31181;&#29992;&#20110;&#21311;&#21517;&#29366;&#24577;-&#21160;&#20316;&#23545;&#30340;&#31574;&#30053;&#32858;&#31867;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
K-SHAP: Policy Clustering Algorithm for Anonymous State-Action Pairs. (arXiv:2302.11996v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.11996
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;K-SHAP&#30340;&#31639;&#27861;&#65292;&#26469;&#35299;&#20915;&#22810;&#20010;&#26234;&#33021;&#20307;&#20445;&#25345;&#21311;&#21517;&#19988;&#20165;&#26377;&#29366;&#24577;-&#21160;&#20316;&#23545;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#26234;&#33021;&#20307;&#20915;&#31574;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#23398;&#20064;&#26234;&#33021;&#20307;&#34892;&#20026;&#24050;&#32463;&#34987;&#35777;&#26126;&#21487;&#20197;&#25552;&#39640;&#25105;&#20204;&#23545;&#23427;&#20204;&#20915;&#31574;&#36807;&#31243;&#30340;&#29702;&#35299;&#65292;&#20174;&#32780;&#22686;&#24378;&#25105;&#20204;&#35299;&#37322;&#23427;&#20204;&#19982;&#29615;&#22659;&#21644;&#20854;&#20182;&#26234;&#33021;&#20307;&#20043;&#38388;&#20132;&#20114;&#30340;&#33021;&#21147;&#12290;&#23613;&#31649;&#25991;&#29486;&#20013;&#24050;&#32463;&#25552;&#20986;&#20102;&#22810;&#31181;&#23398;&#20064;&#25216;&#26415;&#65292;&#20294;&#36824;&#26377;&#19968;&#31181;&#29305;&#23450;&#30340;&#24773;&#20917;&#23578;&#26410;&#34987;&#25506;&#32034;&#65292;&#37027;&#23601;&#26159;&#26234;&#33021;&#20307;&#36523;&#20221;&#20445;&#25345;&#21311;&#21517;&#30340;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#12290;&#20363;&#22914;&#65292;&#22312;&#37329;&#34701;&#24066;&#22330;&#20013;&#65292;&#26631;&#35760;&#25968;&#25454;&#36890;&#24120;&#26159;&#19987;&#26377;&#30340;&#65292;&#20165;&#20844;&#24320;&#22810;&#20010;&#24066;&#22330;&#21442;&#19982;&#32773;&#20132;&#20114;&#32780;&#20135;&#29983;&#30340;&#21311;&#21517;&#29366;&#24577;-&#21160;&#20316;&#23545;&#12290;&#22240;&#27492;&#65292;&#26234;&#33021;&#20307;&#34892;&#21160;&#24207;&#21015;&#19981;&#21487;&#35266;&#27979;&#65292;&#38480;&#21046;&#20102;&#29616;&#26377;&#24037;&#20316;&#30340;&#36866;&#29992;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31574;&#30053;&#32858;&#31867;&#31639;&#27861;K-SHAP&#65292;&#23427;&#23398;&#20064;&#26681;&#25454;&#26234;&#33021;&#20307;&#31574;&#30053;&#23545;&#21311;&#21517;&#29366;&#24577;-&#21160;&#20316;&#23545;&#36827;&#34892;&#20998;&#32452;&#12290;&#25105;&#20204;&#23558;&#35813;&#38382;&#39064;&#20316;&#20026;&#27169;&#20223;&#23398;&#20064;(IL)&#20219;&#21153;&#65292;&#23398;&#20064;&#19968;&#20010;w...
&lt;/p&gt;
&lt;p&gt;
Learning agent behaviors from observational data has shown to improve our understanding of their decision-making processes, advancing our ability to explain their interactions with the environment and other agents. While multiple learning techniques have been proposed in the literature, there is one particular setting that has not been explored yet: multi agent systems where agent identities remain anonymous. For instance, in financial markets labeled data that identifies market participant strategies is typically proprietary, and only the anonymous state-action pairs that result from the interaction of multiple market participants are publicly available. As a result, sequences of agent actions are not observable, restricting the applicability of existing work. In this paper, we propose a Policy Clustering algorithm, called K-SHAP, that learns to group anonymous state-action pairs according to the agent policies. We frame the problem as an Imitation Learning (IL) task, and we learn a w
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;IB-RAR&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#21033;&#29992;&#20449;&#24687;&#29942;&#39048;&#26469;&#22686;&#24378;&#23545;&#25239;&#24615;&#35757;&#32451;&#21644;&#38750;&#23545;&#25239;&#24615;&#35757;&#32451;&#26041;&#27861;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#36890;&#36807;&#36807;&#28388;&#19981;&#24517;&#35201;&#30340;&#29305;&#24449;&#26469;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.10896</link><description>&lt;p&gt;
IB-RAR&#65306;&#20449;&#24687;&#29942;&#39048;&#20316;&#20026;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
IB-RAR: Information Bottleneck as Regularizer for Adversarial Robustness. (arXiv:2302.10896v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.10896
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;IB-RAR&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#21033;&#29992;&#20449;&#24687;&#29942;&#39048;&#26469;&#22686;&#24378;&#23545;&#25239;&#24615;&#35757;&#32451;&#21644;&#38750;&#23545;&#25239;&#24615;&#35757;&#32451;&#26041;&#27861;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#36890;&#36807;&#36807;&#28388;&#19981;&#24517;&#35201;&#30340;&#29305;&#24449;&#26469;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;IB-RAR&#65292;&#21033;&#29992;&#20449;&#24687;&#29942;&#39048;&#65288;IB&#65289;&#26469;&#22686;&#24378;&#23545;&#25239;&#24615;&#35757;&#32451;&#21644;&#38750;&#23545;&#25239;&#24615;&#35757;&#32451;&#26041;&#27861;&#30340;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#12290;&#39318;&#20808;&#65292;&#26412;&#25991;&#20351;&#29992;IB&#29702;&#35770;&#22312;&#25439;&#22833;&#20989;&#25968;&#20013;&#26500;&#24314;&#27491;&#21017;&#21270;&#22120;&#20316;&#20026;&#23398;&#20064;&#30446;&#26631;&#12290;&#28982;&#21518;&#65292;&#26681;&#25454;&#20013;&#38388;&#34920;&#31034;&#19982;&#26631;&#31614;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#65288;MI&#65289;&#36807;&#28388;&#25481;&#19981;&#24517;&#35201;&#30340;&#29305;&#24449;&#65292;&#22240;&#20026;&#20351;&#29992;IB&#35757;&#32451;&#30340;&#32593;&#32476;&#25552;&#20379;&#26131;&#20110;&#21306;&#20998;&#30340;MI&#29305;&#24449;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#33258;&#28982;&#22320;&#19982;&#23545;&#25239;&#24615;&#35757;&#32451;&#30456;&#32467;&#21512;&#65292;&#24182;&#22312;&#26032;&#30340;&#23545;&#25239;&#24615;&#20363;&#23376;&#19978;&#25552;&#20379;&#22987;&#32456;&#26356;&#22909;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#38024;&#23545;VGG16&#32593;&#32476;&#36827;&#34892;&#19977;&#27425;&#23545;&#25239;&#24615;&#35757;&#32451;&#22522;&#20934;&#21644;CIFAR-10&#25968;&#25454;&#38598;&#30340;&#20116;&#31181;&#23545;&#25239;&#24615;&#25915;&#20987;&#26041;&#26696;&#20013;&#65292;&#24179;&#22343;&#25552;&#39640;&#20102;3.07&#65285;&#30340;&#20934;&#30830;&#29575;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20063;&#20026;&#26080;&#38450;&#24481;&#26041;&#27861;&#25552;&#20379;&#20102;&#33391;&#22909;&#30340;&#40065;&#26834;&#24615;&#65292;&#20363;&#22914;&#20165;&#20351;&#29992;&#20132;&#21449;&#29109;&#25439;&#22833;&#36827;&#34892;&#35757;&#32451;&#12290;&#26368;&#21518;&#65292;&#22312;&#26080;&#20154;&#24178;&#39044;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#22788;&#29702;&#32570;&#22833;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#25552;&#20379;&#26356;&#22909;&#30340;&#23398;&#20064;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose a novel method, IB-RAR, which uses Information Bottleneck (IB) to strengthen adversarial robustness for both adversarial training and non-adversarial-trained methods. We first use the IB theory to build regularizers as learning objectives in the loss function. Then, we filter out unnecessary features of intermediate representation according to their mutual information (MI) with labels, as the network trained with IB provides easily distinguishable MI for its features. Experimental results show that our method can be naturally combined with adversarial training and provides consistently better accuracy on new adversarial examples. Our method improves the accuracy by an average of 3.07% against five adversarial attacks for the VGG16 network, trained with three adversarial training benchmarks and the CIFAR-10 dataset. In addition, our method also provides good robustness for undefended methods, such as training with cross-entropy loss only. Finally, in the absenc
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#35777;&#26126;&#20102;&#22312;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#23481;&#26131;&#23398;&#20064;&#30340;&#26679;&#26412;&#23545;&#23398;&#20064;&#39640;&#36136;&#37327;&#34920;&#31034;&#36215;&#21040;&#26368;&#22823;&#30340;&#20316;&#29992;&#65292;&#36825;&#26377;&#21161;&#20110;&#20943;&#23569;&#25152;&#38656;&#30340;&#35757;&#32451;&#25968;&#25454;&#37327;&#65292;&#24182;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2302.09195</link><description>&lt;p&gt;
&#25968;&#25454;&#39640;&#25928;&#30340;&#23545;&#27604;&#33258;&#30417;&#30563;&#23398;&#20064;&#65306;&#26131;&#20110;&#23398;&#20064;&#30340;&#26679;&#26412;&#36215;&#21040;&#26368;&#22823;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data-Efficient Contrastive Self-supervised Learning: Easy Examples Contribute the Most. (arXiv:2302.09195v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.09195
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#35777;&#26126;&#20102;&#22312;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#23481;&#26131;&#23398;&#20064;&#30340;&#26679;&#26412;&#23545;&#23398;&#20064;&#39640;&#36136;&#37327;&#34920;&#31034;&#36215;&#21040;&#26368;&#22823;&#30340;&#20316;&#29992;&#65292;&#36825;&#26377;&#21161;&#20110;&#20943;&#23569;&#25152;&#38656;&#30340;&#35757;&#32451;&#25968;&#25454;&#37327;&#65292;&#24182;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#20174;&#22823;&#37327;&#30340;&#26080;&#26631;&#31614;&#35757;&#32451;&#25968;&#25454;&#20013;&#23398;&#20064;&#39640;&#36136;&#37327;&#30340;&#34920;&#31034;&#12290;&#38543;&#30528;&#25968;&#25454;&#38598;&#21464;&#24471;&#36234;&#26469;&#36234;&#22823;&#65292;&#35782;&#21035;&#23545;&#23398;&#20064;&#27492;&#31867;&#34920;&#31034;&#26368;&#26377;&#29992;&#30340;&#31034;&#20363;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#36825;&#21487;&#20197;&#36890;&#36807;&#20943;&#23569;&#23398;&#20064;&#39640;&#36136;&#37327;&#34920;&#31034;&#25152;&#38656;&#30340;&#25968;&#25454;&#37327;&#26469;&#23454;&#29616;&#26377;&#25928;&#30340;SSL&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;SSL&#30340;&#20215;&#20540;&#22914;&#20309;&#37327;&#21270;&#19968;&#30452;&#26159;&#19968;&#20010;&#24748;&#32780;&#26410;&#20915;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#35777;&#26126;&#22312;&#26399;&#26395;&#24847;&#20041;&#19979;&#65292;&#23545;&#27604;SSL&#20013;&#23545;&#23398;&#20064;&#20570;&#20986;&#26368;&#22823;&#36129;&#29486;&#30340;&#31034;&#20363;&#26159;&#20855;&#26377;&#26368;&#30456;&#20284;&#25968;&#25454;&#22686;&#24378;&#30340;&#31034;&#20363;&#12290;&#25105;&#20204;&#23545;&#36825;&#20123;&#23376;&#38598;&#30340;SSL&#30340;&#24191;&#20041;&#24615;&#33021;&#25552;&#20379;&#20102;&#20005;&#26684;&#30340;&#20445;&#35777;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#23545;SSL&#20570;&#20986;&#26368;&#22823;&#36129;&#29486;&#30340;&#23376;&#38598;&#26159;&#23545;&#30417;&#30563;&#23398;&#20064;&#20570;&#20986;&#26368;&#23567;&#36129;&#29486;&#30340;&#23376;&#38598;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#23376;&#38598;&#22312;CIFAR100&#12289;CIFAR&#20013;&#30340;&#34920;&#29616;&#20248;&#20110;&#38543;&#26426;&#23376;&#38598;3%&#20197;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning (SSL) learns high-quality representations from large pools of unlabeled training data. As datasets grow larger, it becomes crucial to identify the examples that contribute the most to learning such representations. This enables efficient SSL by reducing the volume of data required for learning high-quality representations. Nevertheless, quantifying the value of examples for SSL has remained an open question. In this work, we address this for the first time, by proving that examples that contribute the most to contrastive SSL are those that have the most similar augmentations to other examples, in expectation. We provide rigorous guarantees for the generalization performance of SSL on such subsets. Empirically, we discover, perhaps surprisingly, the subsets that contribute the most to SSL are those that contribute the least to supervised learning. Through extensive experiments, we show that our subsets outperform random subsets by more than 3% on CIFAR100, CIFAR
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;COLE&#26694;&#26550;&#65292;&#36890;&#36807;&#26500;&#24314;&#21512;&#20316;&#28216;&#25103;&#30340;&#24320;&#25918;&#24335;&#30446;&#26631;&#65292;&#20174;&#22270;&#35770;&#30340;&#35282;&#24230;&#35780;&#20272;&#21644;&#30830;&#23450;&#27599;&#20010;&#31574;&#30053;&#30340;&#21327;&#20316;&#33021;&#21147;&#65292;&#20197;&#26377;&#25928;&#22320;&#35299;&#20915;&#38646;&#26679;&#26412;&#21327;&#35843;&#20013;&#30340;&#21512;&#20316;&#19981;&#20860;&#23481;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2302.04831</link><description>&lt;p&gt;
&#38646;&#26679;&#26412;&#21327;&#21516;&#21512;&#20316;&#23398;&#20064;&#26694;&#26550;&#30340;&#21512;&#20316;&#24320;&#25918;&#24335;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Cooperative Open-ended Learning Framework for Zero-shot Coordination. (arXiv:2302.04831v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.04831
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;COLE&#26694;&#26550;&#65292;&#36890;&#36807;&#26500;&#24314;&#21512;&#20316;&#28216;&#25103;&#30340;&#24320;&#25918;&#24335;&#30446;&#26631;&#65292;&#20174;&#22270;&#35770;&#30340;&#35282;&#24230;&#35780;&#20272;&#21644;&#30830;&#23450;&#27599;&#20010;&#31574;&#30053;&#30340;&#21327;&#20316;&#33021;&#21147;&#65292;&#20197;&#26377;&#25928;&#22320;&#35299;&#20915;&#38646;&#26679;&#26412;&#21327;&#35843;&#20013;&#30340;&#21512;&#20316;&#19981;&#20860;&#23481;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21327;&#20316;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#38646;&#26679;&#26412;&#21327;&#35843;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#65292;&#26377;&#25928;&#22320;&#21327;&#35843;&#19968;&#31995;&#21015;&#30475;&#19981;&#35265;&#30340;&#21512;&#20316;&#20249;&#20276;&#12290;&#20808;&#21069;&#30340;&#31639;&#27861;&#35797;&#22270;&#36890;&#36807;&#20248;&#21270;&#31181;&#32676;&#20013;&#30340;&#22266;&#23450;&#30446;&#26631;&#26469;&#25913;&#21892;&#31574;&#30053;&#25110;&#34892;&#20026;&#30340;&#22810;&#26679;&#24615;&#26469;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#21487;&#33021;&#23548;&#33268;&#23398;&#20064;&#25439;&#22833;&#21644;&#19982;&#31181;&#32676;&#20013;&#26576;&#20123;&#31574;&#30053;&#26080;&#27861;&#21512;&#20316;&#65292;&#21363;&#21512;&#20316;&#19981;&#20860;&#23481;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21512;&#20316;&#24320;&#25918;&#24335;&#23398;&#20064;&#65288;COLE&#65289;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#20174;&#22270;&#35770;&#30340;&#35282;&#24230;&#26500;&#24314;&#20102;&#21327;&#20316;&#28216;&#25103;&#30340;&#24320;&#25918;&#24335;&#30446;&#26631;&#65292;&#20197;&#35780;&#20272;&#21644;&#30830;&#23450;&#27599;&#20010;&#31574;&#30053;&#30340;&#21327;&#20316;&#33021;&#21147;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#26126;&#30830;&#20102;&#26694;&#26550;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#29992;&#30340;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#21033;&#29992;&#20102;&#21338;&#24328;&#35770;&#21644;&#22270;&#35770;&#30340;&#30693;&#35782;&#12290;&#27492;&#22806;&#65292;&#23545;&#31639;&#27861;&#30340;&#23398;&#20064;&#36807;&#31243;&#36827;&#34892;&#30340;&#20998;&#26512;&#26174;&#31034;&#65292;&#23427;&#21487;&#20197;&#26377;&#25928;&#22320;&#20811;&#26381;&#23398;&#20064;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;
Zero-shot coordination in cooperative artificial intelligence (AI) remains a significant challenge, which means effectively coordinating with a wide range of unseen partners. Previous algorithms have attempted to address this challenge by optimizing fixed objectives within a population to improve strategy or behaviour diversity. However, these approaches can result in a loss of learning and an inability to cooperate with certain strategies within the population, known as cooperative incompatibility. To address this issue, we propose the Cooperative Open-ended LEarning (COLE) framework, which constructs open-ended objectives in cooperative games with two players from the perspective of graph theory to assess and identify the cooperative ability of each strategy. We further specify the framework and propose a practical algorithm that leverages knowledge from game theory and graph theory. Furthermore, an analysis of the learning process of the algorithm shows that it can efficiently overc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#31163;&#32447;&#25968;&#25454;&#36827;&#34892;&#39640;&#25928;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#35777;&#26126;&#20102;&#29616;&#26377;&#30340;&#31163;&#32447;&#31574;&#30053;&#26041;&#27861;&#33021;&#22815;&#24212;&#29992;&#20110;&#22312;&#32447;&#23398;&#20064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20123;&#26368;&#23569;&#20294;&#37325;&#35201;&#30340;&#26356;&#25913;&#65292;&#26469;&#23454;&#29616;&#21487;&#38752;&#30340;&#24615;&#33021;&#24182;&#25552;&#20379;&#20102;&#23454;&#36341;&#20013;&#21487;&#24212;&#29992;&#30340;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2302.02948</link><description>&lt;p&gt;
&#21033;&#29992;&#31163;&#32447;&#25968;&#25454;&#36827;&#34892;&#39640;&#25928;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Efficient Online Reinforcement Learning with Offline Data. (arXiv:2302.02948v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.02948
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#31163;&#32447;&#25968;&#25454;&#36827;&#34892;&#39640;&#25928;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#35777;&#26126;&#20102;&#29616;&#26377;&#30340;&#31163;&#32447;&#31574;&#30053;&#26041;&#27861;&#33021;&#22815;&#24212;&#29992;&#20110;&#22312;&#32447;&#23398;&#20064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20123;&#26368;&#23569;&#20294;&#37325;&#35201;&#30340;&#26356;&#25913;&#65292;&#26469;&#23454;&#29616;&#21487;&#38752;&#30340;&#24615;&#33021;&#24182;&#25552;&#20379;&#20102;&#23454;&#36341;&#20013;&#21487;&#24212;&#29992;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26679;&#26412;&#25928;&#29575;&#21644;&#25506;&#32034;&#20173;&#28982;&#26159;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20027;&#35201;&#25361;&#25112;&#12290;&#19968;&#31181;&#24378;&#26377;&#21147;&#30340;&#26041;&#27861;&#26159;&#21253;&#25324;&#31163;&#32447;&#25968;&#25454;&#65292;&#22914;&#26469;&#33258;&#20154;&#31867;&#19987;&#23478;&#25110;&#27425;&#20248;&#25506;&#32034;&#31574;&#30053;&#30340;&#20808;&#21069;&#36712;&#36857;&#12290;&#20197;&#21069;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#24191;&#27867;&#30340;&#20462;&#25913;&#21644;&#39069;&#22806;&#30340;&#22797;&#26434;&#24615;&#26469;&#30830;&#20445;&#26377;&#25928;&#20351;&#29992;&#36825;&#20123;&#25968;&#25454;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#24819;&#38382;&#65306;&#25105;&#20204;&#26159;&#21542;&#21487;&#20197;&#31616;&#21333;&#22320;&#24212;&#29992;&#29616;&#26377;&#30340;&#31163;&#32447;&#31574;&#30053;&#26041;&#27861;&#26469;&#21033;&#29992;&#31163;&#32447;&#25968;&#25454;&#36827;&#34892;&#22312;&#32447;&#23398;&#20064;&#65311;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#31572;&#26696;&#26159;&#32943;&#23450;&#30340;&#12290;&#20294;&#26159;&#65292;&#20026;&#20102;&#23454;&#29616;&#21487;&#38752;&#30340;&#24615;&#33021;&#65292;&#38656;&#35201;&#23545;&#29616;&#26377;&#30340;&#31163;&#32447;&#31574;&#30053;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#19968;&#20123;&#26368;&#23569;&#20294;&#37325;&#35201;&#30340;&#26356;&#25913;&#12290;&#25105;&#20204;&#24191;&#27867;&#22320;&#27979;&#35797;&#20102;&#36825;&#20123;&#35774;&#35745;&#36873;&#25321;&#65292;&#24182;&#23637;&#31034;&#20102;&#23545;&#24615;&#33021;&#24433;&#21709;&#26368;&#22823;&#30340;&#20851;&#38190;&#22240;&#32032;&#65292;&#24471;&#20986;&#20102;&#19968;&#32452;&#23454;&#36341;&#32773;&#21487;&#20197;&#36731;&#26494;&#24212;&#29992;&#30340;&#24314;&#35758;&#65292;&#26080;&#35770;&#20854;&#25968;&#25454;&#21253;&#25324;&#23569;&#37327;&#19987;&#23478;&#28436;&#31034;&#25110;&#22823;&#37327;&#27425;&#20248;&#36712;&#36857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sample efficiency and exploration remain major challenges in online reinforcement learning (RL). A powerful approach that can be applied to address these issues is the inclusion of offline data, such as prior trajectories from a human expert or a sub-optimal exploration policy. Previous methods have relied on extensive modifications and additional complexity to ensure the effective use of this data. Instead, we ask: can we simply apply existing off-policy methods to leverage offline data when learning online? In this work, we demonstrate that the answer is yes; however, a set of minimal but important changes to existing off-policy RL algorithms are required to achieve reliable performance. We extensively ablate these design choices, demonstrating the key factors that most affect performance, and arrive at a set of recommendations that practitioners can readily apply, whether their data comprise a small number of expert demonstrations or large volumes of sub-optimal trajectories. We see
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HPRL&#30340;&#20998;&#23618;&#32534;&#31243;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#23398;&#20064;&#31243;&#24207;&#32452;&#21512;&#30340;&#26041;&#27861;&#23454;&#29616;&#65292;&#33021;&#22815;&#20135;&#29983;&#20855;&#26377;&#20154;&#31867;&#21487;&#35299;&#37322;&#24615;&#24182;&#19988;&#22312;&#35780;&#20272;&#20505;&#36873;&#26041;&#26696;&#26102;&#21487;&#20197;&#20934;&#30830;&#22870;&#21169;&#21644;&#24809;&#32602;&#30340;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2301.12950</link><description>&lt;p&gt;
&#36890;&#36807;&#23398;&#20064;&#31243;&#24207;&#32452;&#21512;&#30340;&#26041;&#27861;&#23454;&#29616;&#20998;&#23618;&#32534;&#31243;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Programmatic Reinforcement Learning via Learning to Compose Programs. (arXiv:2301.12950v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12950
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HPRL&#30340;&#20998;&#23618;&#32534;&#31243;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#23398;&#20064;&#31243;&#24207;&#32452;&#21512;&#30340;&#26041;&#27861;&#23454;&#29616;&#65292;&#33021;&#22815;&#20135;&#29983;&#20855;&#26377;&#20154;&#31867;&#21487;&#35299;&#37322;&#24615;&#24182;&#19988;&#22312;&#35780;&#20272;&#20505;&#36873;&#26041;&#26696;&#26102;&#21487;&#20197;&#20934;&#30830;&#22870;&#21169;&#21644;&#24809;&#32602;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Trivedi&#31561;&#20154;(2021)&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;(LEAPS)&#65292;&#26088;&#22312;&#20135;&#29983;&#20855;&#26377;&#20154;&#31867;&#21487;&#35299;&#37322;&#24615;&#24182;&#19988;&#21487;&#20197;&#26356;&#22909;&#22320;&#25512;&#24191;&#21040;&#26032;&#22330;&#26223;&#30340;&#21152;&#24378;&#23398;&#20064;&#65288;RL&#65289;&#31574;&#30053;&#12290;&#26041;&#27861;&#20808;&#23398;&#20064;&#19968;&#20010;&#31243;&#24207;&#23884;&#20837;&#31354;&#38388;&#65292;&#20197;&#36830;&#32493;&#21442;&#25968;&#21270;&#26469;&#33258;&#39044;&#29983;&#25104;&#30340;&#31243;&#24207;&#25968;&#25454;&#38598;&#30340;&#22810;&#26679;&#21270;&#31243;&#24207;&#65292;&#28982;&#21518;&#22312;&#32473;&#23450;&#20219;&#21153;&#26102;&#22312;&#23398;&#20064;&#30340;&#31243;&#24207;&#23884;&#20837;&#31354;&#38388;&#20013;&#25628;&#32034;&#35299;&#20915;&#20219;&#21153;&#30340;&#31243;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;
Aiming to produce reinforcement learning (RL) policies that are human-interpretable and can generalize better to novel scenarios, Trivedi et al. (2021) present a method (LEAPS) that first learns a program embedding space to continuously parameterize diverse programs from a pre-generated program dataset, and then searches for a task-solving program in the learned program embedding space when given a task. Despite the encouraging results, the program policies that LEAPS can produce are limited by the distribution of the program dataset. Furthermore, during searching, LEAPS evaluates each candidate program solely based on its return, failing to precisely reward correct parts of programs and penalize incorrect parts. To address these issues, we propose to learn a meta-policy that composes a series of programs sampled from the learned program embedding space. By learning to compose programs, our proposed hierarchical programmatic reinforcement learning (HPRL) framework can produce program p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#34920;&#26126;&#65292;&#25193;&#25955;&#27169;&#22411;&#24050;&#32463;&#22312;&#25289;&#36817;&#20154;&#19982;&#26426;&#22120;&#20043;&#38388;&#30340;&#40511;&#27807;&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#26426;&#22120;&#29983;&#25104;&#20316;&#21697;&#30340;&#21407;&#21019;&#24615;&#21644;&#21487;&#35782;&#21035;&#24615;&#20173;&#28982;&#23384;&#22312;&#24046;&#36317;&#12290;</title><link>http://arxiv.org/abs/2301.11722</link><description>&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#33402;&#26415;&#23478;&#65306;&#25105;&#20204;&#27491;&#22312;&#25289;&#36817;&#20154;&#19982;&#26426;&#22120;&#20043;&#38388;&#30340;&#40511;&#27807;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Diffusion Models as Artists: Are we Closing the Gap between Humans and Machines?. (arXiv:2301.11722v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11722
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#34920;&#26126;&#65292;&#25193;&#25955;&#27169;&#22411;&#24050;&#32463;&#22312;&#25289;&#36817;&#20154;&#19982;&#26426;&#22120;&#20043;&#38388;&#30340;&#40511;&#27807;&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#26426;&#22120;&#29983;&#25104;&#20316;&#21697;&#30340;&#21407;&#21019;&#24615;&#21644;&#21487;&#35782;&#21035;&#24615;&#20173;&#28982;&#23384;&#22312;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#30340;&#19968;&#20010;&#37325;&#35201;&#37324;&#31243;&#30865;&#26159;&#24320;&#21457;&#33021;&#22815;&#29983;&#25104;&#21644;&#20154;&#31867;&#26080;&#24322;&#30340;&#32472;&#30011;&#20316;&#21697;&#30340;&#31639;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992; Boutin &#31561;&#20154; 2022 &#24180;&#25552;&#20986;&#30340;&#8220;&#22810;&#26679;&#24615; vs. &#21487;&#35782;&#21035;&#24615;&#8221;&#35780;&#20998;&#26694;&#26550;&#65292;&#21457;&#29616;&#19968;&#21457;&#21363;&#20013;&#30340;&#25193;&#25955;&#27169;&#22411;&#30830;&#23454;&#24050;&#32463;&#24320;&#22987;&#25289;&#36817;&#20154;&#31867;&#21644;&#26426;&#22120;&#20043;&#38388;&#30340;&#40511;&#27807;&#12290;&#28982;&#32780;&#65292;&#29992;&#26356;&#32454;&#31890;&#24230;&#30340;&#26679;&#26412;&#29420;&#21019;&#24615;&#24230;&#37327;&#65292;&#25105;&#20204;&#21457;&#29616;&#24378;&#21270;&#25193;&#25955;&#27169;&#22411;&#30340;&#25351;&#23548;&#26377;&#21161;&#20110;&#25552;&#39640;&#23427;&#20204;&#30340;&#32472;&#30011;&#20154;&#24615;&#21270;&#31243;&#24230;&#65292;&#20294;&#23427;&#20204;&#20173;&#28982;&#26080;&#27861;&#25509;&#36817;&#20154;&#31867;&#32472;&#30011;&#20316;&#21697;&#30340;&#21407;&#21019;&#24615;&#21644;&#21487;&#35782;&#21035;&#24615;&#12290;&#36890;&#36807;&#22312;&#32447;&#24515;&#29702;&#29289;&#29702;&#23454;&#39564;&#25910;&#38598;&#20154;&#31867;&#31867;&#21035;&#35786;&#26029;&#29305;&#24449;&#24182;&#23558;&#20854;&#19982;&#20174;&#25193;&#25955;&#27169;&#22411;&#20013;&#23548;&#20986;&#30340;&#29305;&#24449;&#36827;&#34892;&#27604;&#36739;&#65292;&#25105;&#20204;&#21457;&#29616;&#20154;&#31867;&#20381;&#36182;&#20110;&#26356;&#23569;&#19988;&#26356;&#23616;&#37096;&#30340;&#29305;&#24449;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#25193;&#25955;&#27169;&#22411;&#22312;&#26174;&#33879;&#25552;&#39640;&#26426;&#22120;&#29983;&#25104;&#32472;&#30011;&#20316;&#21697;&#30340;&#36136;&#37327;&#26041;&#38754;&#21457;&#25381;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#20294;&#20154;&#19982;&#26426;&#22120;&#20043;&#38388;&#30340;&#24046;&#36317;&#20173;&#28982;&#23384;&#22312;&#12290;
&lt;/p&gt;
&lt;p&gt;
An important milestone for AI is the development of algorithms that can produce drawings that are indistinguishable from those of humans. Here, we adapt the 'diversity vs. recognizability' scoring framework from Boutin et al, 2022 and find that one-shot diffusion models have indeed started to close the gap between humans and machines. However, using a finer-grained measure of the originality of individual samples, we show that strengthening the guidance of diffusion models helps improve the humanness of their drawings, but they still fall short of approximating the originality and recognizability of human drawings. Comparing human category diagnostic features, collected through an online psychophysics experiment, against those derived from diffusion models reveals that humans rely on fewer and more localized features. Overall, our study suggests that diffusion models have significantly helped improve the quality of machine-generated drawings; however, a gap between humans and machines 
&lt;/p&gt;</description></item><item><title>&#22312;&#20844;&#24179;&#20998;&#31867;&#20013;&#65292;&#27169;&#22411;&#30340;&#39044;&#27979;&#26041;&#24046;&#26159;&#19968;&#20010;&#37325;&#35201;&#20294;&#40092;&#20026;&#20154;&#30693;&#30340;&#35823;&#24046;&#26469;&#28304;&#38382;&#39064;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#27965;&#24615;&#26631;&#20934;&#26469;&#34913;&#37327;&#27979;&#37327;&#21644;&#20943;&#23569;&#38543;&#24847;&#24615;&#12290;&#20316;&#32773;&#36824;&#24320;&#21457;&#20102;&#19968;&#20010;&#31639;&#27861;&#26469;&#22788;&#29702;&#38543;&#24847;&#24615;&#39044;&#27979;&#65292;&#24182;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#25581;&#31034;&#20102;&#24403;&#21069;&#27169;&#22411;&#26080;&#27861;&#22788;&#29702;&#26576;&#20123;&#31867;&#22411;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2301.11562</link><description>&lt;p&gt;
&#39044;&#27979;&#26159;&#21542;&#38543;&#24847;&#65311;&#22312;&#20844;&#24179;&#20998;&#31867;&#20013;&#35780;&#20272;&#33258;&#27965;&#24615;
&lt;/p&gt;
&lt;p&gt;
Is My Prediction Arbitrary? Measuring Self-Consistency in Fair Classification. (arXiv:2301.11562v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11562
&lt;/p&gt;
&lt;p&gt;
&#22312;&#20844;&#24179;&#20998;&#31867;&#20013;&#65292;&#27169;&#22411;&#30340;&#39044;&#27979;&#26041;&#24046;&#26159;&#19968;&#20010;&#37325;&#35201;&#20294;&#40092;&#20026;&#20154;&#30693;&#30340;&#35823;&#24046;&#26469;&#28304;&#38382;&#39064;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#27965;&#24615;&#26631;&#20934;&#26469;&#34913;&#37327;&#27979;&#37327;&#21644;&#20943;&#23569;&#38543;&#24847;&#24615;&#12290;&#20316;&#32773;&#36824;&#24320;&#21457;&#20102;&#19968;&#20010;&#31639;&#27861;&#26469;&#22788;&#29702;&#38543;&#24847;&#24615;&#39044;&#27979;&#65292;&#24182;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#25581;&#31034;&#20102;&#24403;&#21069;&#27169;&#22411;&#26080;&#27861;&#22788;&#29702;&#26576;&#20123;&#31867;&#22411;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20844;&#24179;&#20998;&#31867;&#20013;&#65292;&#19981;&#21516;&#32463;&#36807;&#35757;&#32451;&#30340;&#27169;&#22411;&#20043;&#38388;&#30340;&#39044;&#27979;&#26041;&#24046;&#26159;&#19968;&#20010;&#37325;&#35201;&#20294;&#40092;&#20026;&#20154;&#30693;&#30340;&#35823;&#24046;&#26469;&#28304;&#38382;&#39064;&#12290; &#23454;&#35777;&#34920;&#26126;&#65292;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#39044;&#27979;&#30340;&#26041;&#24046;&#24046;&#24322;&#38750;&#24120;&#22823;&#65292;&#20197;&#33267;&#20110;&#20915;&#31574;&#23454;&#38469;&#19978;&#26159;&#38543;&#24847;&#30340;&#12290; &#20026;&#20102;&#30740;&#31350;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#35268;&#27169;&#30340;&#23454;&#35777;&#30740;&#31350;&#65292;&#24182;&#20570;&#20986;&#20102;&#22235;&#20010;&#24635;&#20307;&#36129;&#29486;&#65306;&#25105;&#20204;1&#65289;&#23450;&#20041;&#20102;&#19968;&#31181;&#22522;&#20110;&#26041;&#24046;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#31216;&#20026;&#33258;&#27965;&#24615;&#65292;&#22312;&#27979;&#37327;&#21644;&#20943;&#23569;&#38543;&#24847;&#24615;&#26102;&#20351;&#29992;&#65307; 2&#65289;&#24320;&#21457;&#20102;&#19968;&#31181;&#21512;&#29702;&#30340;&#31639;&#27861;&#65292;&#24403;&#39044;&#27979;&#26080;&#27861;&#20570;&#20986;&#20915;&#31574;&#26102;&#65292;&#21487;&#20197;&#25918;&#24323;&#20998;&#31867;&#65307; 3&#65289;&#36827;&#34892;&#20102;&#36804;&#20170;&#20026;&#27490;&#26377;&#20851;&#20844;&#24179;&#20998;&#31867;&#20013;&#26041;&#24046;&#65288;&#30456;&#23545;&#20110;&#33258;&#27965;&#24615;&#21644;&#38543;&#24847;&#24615;&#65289;&#20316;&#29992;&#30340;&#26368;&#22823;&#35268;&#27169;&#23454;&#35777;&#30740;&#31350;&#65307; 4&#65289;&#25512;&#20986;&#20102;&#19968;&#20010;&#24037;&#20855;&#21253;&#65292;&#20351;&#32654;&#22269;&#20303;&#25151;&#25269;&#25276;&#36151;&#27454;&#25259;&#38706;&#27861;&#26696;&#65288;HMDA&#65289;&#25968;&#25454;&#38598;&#26131;&#20110;&#29992;&#20110;&#26410;&#26469;&#30740;&#31350;&#12290; &#24635;&#30340;&#26469;&#35828;&#65292;&#25105;&#20204;&#30340;&#23454;&#35777;&#32467;&#26524;&#25581;&#31034;&#20102;&#20851;&#20110;&#21487;&#37325;&#22797;&#24615;&#30340;&#20196;&#20154;&#38663;&#24778;&#30340;&#35265;&#35299;&#12290;&#24403;&#32771;&#34385;&#21040;&#26041;&#24046;&#21644;&#38543;&#24847;&#39044;&#27979;&#30340;&#21487;&#33021;&#24615;&#26102;&#65292;&#22823;&#22810;&#25968;&#20844;&#24179;&#20998;&#31867;&#22522;&#20934;&#25509;&#36817;&#20844;&#24179;&#12290; &#20294;&#26159;&#65292;&#19968;&#23567;&#37096;&#20998;&#23454;&#20363;&#26174;&#31034;&#20986;&#26497;&#22823;&#30340;&#38543;&#24847;&#24615;&#27700;&#24179;&#65292;&#36825;&#34920;&#26126;&#24403;&#21069;&#30340;&#27169;&#22411;&#21487;&#33021;&#26080;&#27861;&#22788;&#29702;&#26576;&#20123;&#31867;&#22411;&#30340;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Variance in predictions across different trained models is a significant, under-explored source of error in fair classification. Empirically, the variance on some instances is so large that decisions can be effectively arbitrary. To study this problem, we perform a large-scale empirical study and make four overarching contributions: We 1) Define a metric called self-consistency, derived from variance, which we use as a proxy for measuring and reducing arbitrariness; 2) Develop an ensembling algorithm that abstains from classification when a prediction would be arbitrary; 3) Conduct the largest to-date empirical study of the role of variance (vis-a-vis self-consistency and arbitrariness) in fair classification; and, 4) Release a toolkit that makes the US Home Mortgage Disclosure Act (HMDA) datasets easily usable for future research. Altogether, our empirical results reveal shocking insights about reproducibility. Most fairness classification benchmarks are close-to-fair when taking into
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;SNeRL&#30340;&#35821;&#20041;&#24863;&#30693;&#31070;&#32463;&#36752;&#23556;&#22330;&#65292;&#23427;&#36890;&#36807;&#23398;&#20064;3D-aware&#30340;&#38544;&#24335;&#34920;&#31034;&#26469;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#65292;&#24182;&#22312;&#22522;&#20110;&#20687;&#32032;&#30340;&#20197;&#21450;&#26368;&#26032;&#30340;3D&#24863;&#30693;&#34920;&#31034;&#26041;&#27861;&#20013;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2301.11520</link><description>&lt;p&gt;
SNeRL: &#35821;&#20041;&#24863;&#30693;&#30340;&#31070;&#32463;&#36752;&#23556;&#22330;&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
SNeRL: Semantic-aware Neural Radiance Fields for Reinforcement Learning. (arXiv:2301.11520v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11520
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;SNeRL&#30340;&#35821;&#20041;&#24863;&#30693;&#31070;&#32463;&#36752;&#23556;&#22330;&#65292;&#23427;&#36890;&#36807;&#23398;&#20064;3D-aware&#30340;&#38544;&#24335;&#34920;&#31034;&#26469;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#65292;&#24182;&#22312;&#22522;&#20110;&#20687;&#32032;&#30340;&#20197;&#21450;&#26368;&#26032;&#30340;3D&#24863;&#30693;&#34920;&#31034;&#26041;&#27861;&#20013;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#24378;&#21270;&#23398;&#20064;&#34920;&#31034;&#26041;&#27861;&#24456;&#38590;&#26377;&#25928;&#22320;&#34701;&#21512;&#20154;&#31867;&#30452;&#35266;&#30340;3D&#29615;&#22659;&#29702;&#35299;&#65292;&#22240;&#27492;&#32463;&#24120;&#34920;&#29616;&#20986;&#27425;&#20248;&#24615;&#33021;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;SNeRL&#30340;&#35821;&#20041;&#24863;&#30693;&#31070;&#32463;&#36752;&#23556;&#22330;&#65292;&#23427;&#36890;&#36807;&#32852;&#21512;&#20248;&#21270;&#21367;&#31215;&#32534;&#30721;&#22120;&#21644;&#35821;&#20041;&#24863;&#30693;&#31070;&#32463;&#36752;&#23556;&#22330;&#65288;NeRF&#65289;&#26469;&#20174;&#22810;&#35270;&#35282;&#22270;&#20687;&#20013;&#23398;&#20064;3D&#24863;&#30693;&#31070;&#32463;&#38544;&#24335;&#34920;&#31034;&#12290;&#25105;&#20204;&#22312;NeRF&#20013;&#24341;&#20837;&#20102;3D&#35821;&#20041;&#21644;&#33976;&#39311;&#29305;&#24449;&#22330;&#65292;&#24182;&#19982;RGB&#36752;&#23556;&#22330;&#24182;&#34892;&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#35821;&#20041;&#21644;&#23545;&#35937;&#20013;&#24515;&#34920;&#31034;&#23398;&#20064;&#12290;SNeRL&#22312;&#26080;&#27169;&#22411;&#21644;&#26377;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#20013;&#19981;&#20165;&#20248;&#20110;&#20197;&#24448;&#30340;&#22522;&#20110;&#20687;&#32032;&#30340;&#34920;&#31034;&#26041;&#27861;&#65292;&#36824;&#20248;&#20110;&#26368;&#36817;&#30340;3D&#24863;&#30693;&#34920;&#31034;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
As previous representations for reinforcement learning cannot effectively incorporate a human-intuitive understanding of the 3D environment, they usually suffer from sub-optimal performances. In this paper, we present Semantic-aware Neural Radiance Fields for Reinforcement Learning (SNeRL), which jointly optimizes semantic-aware neural radiance fields (NeRF) with a convolutional encoder to learn 3D-aware neural implicit representation from multi-view images. We introduce 3D semantic and distilled feature fields in parallel to the RGB radiance fields in NeRF to learn semantic and object-centric representation for reinforcement learning. SNeRL outperforms not only previous pixel-based representations but also recent 3D-aware representations both in model-free and model-based reinforcement learning.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38646;&#26679;&#26412;&#23398;&#20064;&#33258;&#21160;&#35780;&#20998;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#37197;&#21512;&#21305;&#37197;&#26631;&#26412;&#20316;&#20026;&#19979;&#19968;&#21477;&#39044;&#27979;&#25216;&#26415;&#65292;&#25104;&#21151;&#24212;&#29992;&#20110;&#31185;&#23398;&#25945;&#32946;&#39046;&#22495;&#30340;&#35770;&#35777;&#20219;&#21153;&#65292;&#26497;&#22823;&#22320;&#20943;&#23569;&#20102;&#35757;&#32451;&#25104;&#26412;&#21644;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2301.08771</link><description>&lt;p&gt;
&#21305;&#37197;&#26631;&#26412;&#20316;&#20026;&#19979;&#19968;&#21477;&#39044;&#27979;&#65306;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31185;&#23398;&#25945;&#32946;&#20013;&#30340;&#38646;&#26679;&#26412;&#23398;&#20064;&#33258;&#21160;&#35780;&#20998;
&lt;/p&gt;
&lt;p&gt;
Matching Exemplar as Next Sentence Prediction (MeNSP): Zero-shot Prompt Learning for Automatic Scoring in Science Education. (arXiv:2301.08771v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.08771
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38646;&#26679;&#26412;&#23398;&#20064;&#33258;&#21160;&#35780;&#20998;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#37197;&#21512;&#21305;&#37197;&#26631;&#26412;&#20316;&#20026;&#19979;&#19968;&#21477;&#39044;&#27979;&#25216;&#26415;&#65292;&#25104;&#21151;&#24212;&#29992;&#20110;&#31185;&#23398;&#25945;&#32946;&#39046;&#22495;&#30340;&#35770;&#35777;&#20219;&#21153;&#65292;&#26497;&#22823;&#22320;&#20943;&#23569;&#20102;&#35757;&#32451;&#25104;&#26412;&#21644;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#33021;&#22815;&#33258;&#21160;&#35780;&#20998;&#31185;&#23398;&#38382;&#39064;&#30340;&#23398;&#29983;&#20070;&#38754;&#31572;&#26696;&#30340;&#27169;&#22411;&#23545;&#20110;&#31185;&#23398;&#25945;&#32946;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#25910;&#38598;&#21644;&#26631;&#35760;&#36275;&#22815;&#30340;&#23398;&#29983;&#31572;&#26696;&#20197;&#35757;&#32451;&#27169;&#22411;&#26159;&#32791;&#26102;&#21644;&#36153;&#29992;&#39640;&#26114;&#30340;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;prompt&#35843;&#25972;&#30340;&#24773;&#20917;&#19979;&#36866;&#24212;&#19979;&#28216;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#22312;&#31185;&#23398;&#25945;&#32946;&#20013;&#36824;&#27809;&#26377;&#20351;&#29992;&#36807;&#36825;&#31181;&#25552;&#31034;&#26041;&#27861;&#30340;&#30740;&#31350;&#12290;&#30001;&#20110;&#23398;&#29983;&#30340;&#31572;&#26696;&#26159;&#29992;&#33258;&#28982;&#35821;&#35328;&#21576;&#29616;&#30340;&#65292;&#22240;&#27492;&#20351;&#29992;&#25552;&#31034;&#23558;&#35780;&#20998;&#36807;&#31243;&#23545;&#40784;&#20026;&#19979;&#19968;&#21477;&#39044;&#27979;&#20219;&#21153;&#21487;&#20197;&#36339;&#36807;&#26114;&#36149;&#30340;&#35843;&#25972;&#38454;&#27573;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#21305;&#37197;&#26631;&#26412;&#20316;&#20026;&#19979;&#19968;&#21477;&#39044;&#27979;&#65288;MeNSP&#65289;&#24320;&#21457;&#20102;&#19968;&#31181;&#38646;&#26679;&#26412;&#33258;&#21160;&#35780;&#20998;&#26041;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#19981;&#38656;&#35201;&#35757;&#32451;&#26679;&#26412;&#12290;&#25105;&#20204;&#39318;&#20808;&#22312;&#35780;&#20998;&#19977;&#20010;&#31185;&#23398;&#35770;&#35777;&#20219;&#21153;&#20013;&#24212;&#29992;MeNSP&#65292;&#24182;&#21457;&#29616;&#26426;&#22120;-&#20154;&#35780;&#20998;&#30340;&#19968;&#33268;&#24615;&#65292;Cohen&#30340;Kappa&#31995;&#25968;&#22312;0.30&#21040;0.57&#20043;&#38388;&#65292;F1&#20998;&#25968;
&lt;/p&gt;
&lt;p&gt;
Developing models to automatically score students' written responses to science problems is critical for science education. However, collecting and labeling sufficient student responses for training models is time and cost-consuming. Recent studies suggest that pre-trained language models (PLMs) can be adapted to downstream tasks without fine-tuning with prompts. However, no research has employed such a prompt approach in science education. As student responses are presented with natural language, aligning the scoring procedure as the next sentence prediction task using prompts can skip the costly fine-tuning stage. In this study, we developed a zero-shot approach to automatically score student responses via Matching Exemplars as Next Sentence Prediction (MeNSP). This approach employs no training samples. We first apply MeNSP in scoring three assessment tasks of scientific argumentation and found machine-human scoring agreements, Cohen's Kappa ranges from 0.30 to 0.57, and F1 score ran
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#38544;&#31169;&#21161;&#25163;PEAK&#65292;&#36890;&#36807;&#33258;&#21160;&#21270;&#30693;&#35782;&#25552;&#21462;&#21644;&#29983;&#25104;&#35299;&#37322;&#65292;&#24110;&#21161;&#29992;&#25143;&#29702;&#35299;&#20854;&#38544;&#31169;&#24314;&#35758;&#12290;&#22312;&#30495;&#23454;&#25968;&#25454;&#38598;&#30340;&#29992;&#25143;&#30740;&#31350;&#20013;&#65292;&#29992;&#25143;&#35748;&#20026;&#29983;&#25104;&#30340;&#35299;&#37322;&#26377;&#29992;&#19988;&#26131;&#20110;&#29702;&#35299;&#12290;</title><link>http://arxiv.org/abs/2301.02079</link><description>&lt;p&gt;
PEAK: &#36890;&#36807;&#33258;&#21160;&#21270;&#30693;&#35782;&#25552;&#21462;&#23454;&#29616;&#30340;&#21487;&#35299;&#37322;&#38544;&#31169;&#21161;&#25163;
&lt;/p&gt;
&lt;p&gt;
PEAK: Explainable Privacy Assistant through Automated Knowledge Extraction. (arXiv:2301.02079v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.02079
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#38544;&#31169;&#21161;&#25163;PEAK&#65292;&#36890;&#36807;&#33258;&#21160;&#21270;&#30693;&#35782;&#25552;&#21462;&#21644;&#29983;&#25104;&#35299;&#37322;&#65292;&#24110;&#21161;&#29992;&#25143;&#29702;&#35299;&#20854;&#38544;&#31169;&#24314;&#35758;&#12290;&#22312;&#30495;&#23454;&#25968;&#25454;&#38598;&#30340;&#29992;&#25143;&#30740;&#31350;&#20013;&#65292;&#29992;&#25143;&#35748;&#20026;&#29983;&#25104;&#30340;&#35299;&#37322;&#26377;&#29992;&#19988;&#26131;&#20110;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22312;&#32447;&#38544;&#31169;&#39046;&#22495;&#65292;&#38544;&#31169;&#21161;&#25163;&#22312;&#20351;&#29992;&#25143;&#33021;&#22815;&#26377;&#25928;&#31649;&#29702;&#38544;&#31169;&#26041;&#38754;&#21457;&#25381;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#23613;&#31649;&#26368;&#36817;&#30340;&#30740;&#31350;&#22312;&#35299;&#20915;&#38544;&#31169;&#20405;&#29359;&#26816;&#27979;&#21644;&#20010;&#24615;&#21270;&#38544;&#31169;&#24314;&#35758;&#31561;&#20219;&#21153;&#26041;&#38754;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#36827;&#23637;&#65292;&#20294;&#36825;&#20123;&#31995;&#32479;&#34987;&#24191;&#27867;&#37319;&#29992;&#30340;&#20851;&#38190;&#22312;&#20110;&#36825;&#20123;&#31995;&#32479;&#25552;&#20379;&#20915;&#31574;&#36807;&#31243;&#30340;&#35299;&#37322;&#33021;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20026;&#38544;&#31169;&#20915;&#31574;&#29983;&#25104;&#35299;&#37322;&#30340;&#38544;&#31169;&#21161;&#25163;&#12290;&#38544;&#31169;&#21161;&#25163;&#30528;&#37325;&#20110;&#21457;&#29616;&#28508;&#22312;&#20027;&#39064;&#12289;&#35782;&#21035;&#35299;&#37322;&#31867;&#21035;&#12289;&#24314;&#31435;&#35299;&#37322;&#26041;&#26696;&#21644;&#29983;&#25104;&#33258;&#21160;&#21270;&#35299;&#37322;&#12290;&#29983;&#25104;&#30340;&#35299;&#37322;&#21487;&#20197;&#34987;&#29992;&#25143;&#29992;&#26469;&#29702;&#35299;&#38544;&#31169;&#21161;&#25163;&#30340;&#24314;&#35758;&#12290;&#25105;&#20204;&#30340;&#29992;&#25143;&#30740;&#31350;&#20351;&#29992;&#20102;&#30495;&#23454;&#19990;&#30028;&#30340;&#22270;&#20687;&#38544;&#31169;&#25968;&#25454;&#38598;&#65292;&#32467;&#26524;&#26174;&#31034;&#29992;&#25143;&#35748;&#20026;&#29983;&#25104;&#30340;&#35299;&#37322;&#26377;&#29992;&#19988;&#26131;&#20110;&#29702;&#35299;&#12290;&#27492;&#22806;&#65292;&#38544;&#31169;&#21161;&#25163;&#21487;&#20197;&#20351;&#29992;&#36825;&#20123;&#29983;&#25104;&#30340;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the realm of online privacy, privacy assistants play a pivotal role in empowering users to manage their privacy effectively. Although recent studies have shown promising progress in tackling tasks such as privacy violation detection and personalized privacy recommendations, a crucial aspect for widespread user adoption is the capability of these systems to provide explanations for their decision-making processes. This paper presents a privacy assistant for generating explanations for privacy decisions. The privacy assistant focuses on discovering latent topics, identifying explanation categories, establishing explanation schemes, and generating automated explanations. The generated explanations can be used by users to understand the recommendations of the privacy assistant. Our user study of real-world privacy dataset of images shows that users find the generated explanations useful and easy to understand. Additionally, the generated explanations can be used by privacy assistants th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36830;&#32493;&#23545;&#27604;&#24494;&#35843;&#30340;&#26041;&#27861;&#26469;&#25913;&#36827;&#20302;&#36164;&#28304;&#20851;&#31995;&#25552;&#21462;&#65292;&#36890;&#36807;&#20351;&#29992;&#19968;&#33268;&#30340;&#23545;&#27604;&#23398;&#20064;&#30446;&#26631;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;RE&#27169;&#22411;&#65292;&#20197;&#21450;&#22810;&#20013;&#24515;&#23545;&#27604;&#25439;&#22833;&#26469;&#20801;&#35768;&#19968;&#20010;&#20851;&#31995;&#24418;&#25104;&#22810;&#20010;&#32858;&#31867;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#21487;&#20197;&#26174;&#30528;&#25552;&#39640;&#20302;&#36164;&#28304;&#24773;&#20917;&#21644;&#39046;&#22495;&#20013;&#30340;&#20851;&#31995;&#25552;&#21462;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2212.10823</link><description>&lt;p&gt;
&#36830;&#32493;&#23545;&#27604;&#24494;&#35843;&#25913;&#36827;&#20302;&#36164;&#28304;&#20851;&#31995;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Continual Contrastive Finetuning Improves Low-Resource Relation Extraction. (arXiv:2212.10823v1 [cs.CL] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10823
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36830;&#32493;&#23545;&#27604;&#24494;&#35843;&#30340;&#26041;&#27861;&#26469;&#25913;&#36827;&#20302;&#36164;&#28304;&#20851;&#31995;&#25552;&#21462;&#65292;&#36890;&#36807;&#20351;&#29992;&#19968;&#33268;&#30340;&#23545;&#27604;&#23398;&#20064;&#30446;&#26631;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;RE&#27169;&#22411;&#65292;&#20197;&#21450;&#22810;&#20013;&#24515;&#23545;&#27604;&#25439;&#22833;&#26469;&#20801;&#35768;&#19968;&#20010;&#20851;&#31995;&#24418;&#25104;&#22810;&#20010;&#32858;&#31867;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#21487;&#20197;&#26174;&#30528;&#25552;&#39640;&#20302;&#36164;&#28304;&#24773;&#20917;&#21644;&#39046;&#22495;&#20013;&#30340;&#20851;&#31995;&#25552;&#21462;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#31995;&#25552;&#21462;&#65288;RE&#65289;&#20381;&#36182;&#32467;&#26500;&#21270;&#27880;&#37322;&#35821;&#26009;&#24211;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#65292;&#23588;&#20854;&#22312;&#20302;&#36164;&#28304;&#24773;&#20917;&#21644;&#39046;&#22495;&#20013;&#65292;&#35813;&#20219;&#21153;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#36817;&#26399;&#30740;&#31350;&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#26469;&#35299;&#20915;&#20302;&#36164;&#28304;&#30340;RE&#65292;&#20854;&#20013;&#35299;&#20915;&#26041;&#26696;&#21253;&#25324;&#36890;&#36807;RE&#30446;&#26631;&#39044;&#35757;&#32451;&#20851;&#31995;&#23884;&#20837;&#65292;&#24182;&#36890;&#36807;&#20998;&#31867;&#20026;&#22522;&#30784;&#30340;&#30446;&#26631;&#23545;&#26377;&#26631;&#31614;&#25968;&#25454;&#36827;&#34892;&#24494;&#35843;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#26159;&#30446;&#26631;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#23427;&#38459;&#27490;RE&#27169;&#22411;&#20805;&#20998;&#21033;&#29992;&#39044;&#35757;&#32451;&#34920;&#31034;&#20013;&#30340;&#30693;&#35782;&#12290;&#26412;&#25991;&#26088;&#22312;&#24357;&#21512;&#24046;&#36317;&#65292;&#24182;&#25552;&#20986;&#20351;&#29992;&#19968;&#33268;&#30340;&#23545;&#27604;&#23398;&#20064;&#30446;&#26631;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;RE&#27169;&#22411;&#12290;&#30001;&#20110;&#22312;&#36825;&#31181;&#34920;&#31034;&#23398;&#20064;&#33539;&#24335;&#20013;&#65292;&#19968;&#20010;&#20851;&#31995;&#21487;&#33021;&#22312;&#34920;&#31034;&#31354;&#38388;&#20013;&#36731;&#26494;&#24418;&#25104;&#22810;&#20010;&#32858;&#31867;&#65292;&#22240;&#27492;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#22810;&#20013;&#24515;&#23545;&#27604;&#25439;&#22833;&#65292;&#20801;&#35768;&#19968;&#20010;&#20851;&#31995;&#24418;&#25104;&#22810;&#20010;&#32858;&#31867;&#20197;&#26356;&#22909;&#22320;&#23545;&#40784;&#39044;&#35757;&#32451;&#12290;&#22312;&#20004;&#20010;&#25991;&#26723;&#20013;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#20302;&#36164;&#28304;&#24773;&#20917;&#21644;&#39046;&#22495;&#20013;&#26174;&#30528;&#25552;&#39640;&#20851;&#31995;&#25552;&#21462;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Relation extraction (RE), which has relied on structurally annotated corpora for model training, has been particularly challenging in low-resource scenarios and domains. Recent literature has tackled low-resource RE by self-supervised learning, where the solution involves pretraining the relation embedding by RE-based objective and finetuning on labeled data by classification-based objective. However, a critical challenge to this approach is the gap in objectives, which prevents the RE model from fully utilizing the knowledge in pretrained representations. In this paper, we aim at bridging the gap and propose to pretrain and finetune the RE model using consistent objectives of contrastive learning. Since in this kind of representation learning paradigm, one relation may easily form multiple clusters in the representation space, we further propose a multi-center contrastive loss that allows one relation to form multiple clusters to better align with pretraining. Experiments on two docum
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21512;&#25104;&#20219;&#21153;&#21644;&#25968;&#25454;&#39044;&#35757;&#32451;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#20854;&#21487;&#20197;&#32531;&#35299;&#22823;&#35268;&#27169;&#25235;&#21462;&#30340;&#35821;&#26009;&#24211;&#25152;&#23548;&#33268;&#30340;&#27602;&#24615;&#12289;&#20559;&#35265;&#21644;&#27861;&#24459;&#38544;&#24739;&#65292;&#24182;&#35777;&#26126;&#20102;&#21363;&#20351;&#37319;&#29992;&#39640;&#24230;&#28151;&#28102;&#25110;&#32431;&#21512;&#25104;&#25968;&#25454;&#65292;&#39044;&#35757;&#32451;&#20381;&#28982;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2212.09864</link><description>&lt;p&gt;
&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#30340;&#21512;&#25104;&#39044;&#35757;&#32451;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
Synthetic Pre-Training Tasks for Neural Machine Translation. (arXiv:2212.09864v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09864
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21512;&#25104;&#20219;&#21153;&#21644;&#25968;&#25454;&#39044;&#35757;&#32451;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#20854;&#21487;&#20197;&#32531;&#35299;&#22823;&#35268;&#27169;&#25235;&#21462;&#30340;&#35821;&#26009;&#24211;&#25152;&#23548;&#33268;&#30340;&#27602;&#24615;&#12289;&#20559;&#35265;&#21644;&#27861;&#24459;&#38544;&#24739;&#65292;&#24182;&#35777;&#26126;&#20102;&#21363;&#20351;&#37319;&#29992;&#39640;&#24230;&#28151;&#28102;&#25110;&#32431;&#21512;&#25104;&#25968;&#25454;&#65292;&#39044;&#35757;&#32451;&#20381;&#28982;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22823;&#35268;&#27169;&#25235;&#21462;&#30340;&#35821;&#26009;&#24211;&#36827;&#34892;&#39044;&#35757;&#32451;&#27169;&#22411;&#21487;&#33021;&#20250;&#23548;&#33268;&#27602;&#24615;&#21644;&#20559;&#35265;&#31561;&#38382;&#39064;&#65292;&#20197;&#21450;&#29256;&#26435;&#21644;&#38544;&#31169;&#38382;&#39064;&#12290;&#37319;&#29992;&#21512;&#25104;&#20219;&#21153;&#21644;&#25968;&#25454;&#36827;&#34892;&#39044;&#35757;&#32451;&#26159;&#32531;&#35299;&#36825;&#20123;&#38382;&#39064;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#24335;&#65292;&#22240;&#20026;&#27169;&#22411;&#19981;&#20250;&#21560;&#25910;&#20219;&#20309;&#30495;&#23454;&#19990;&#30028;&#20449;&#24687;&#12290;&#26412;&#25991;&#26088;&#22312;&#20102;&#35299;&#20351;&#29992;&#21512;&#25104;&#36164;&#28304;&#26102;&#23545;&#39044;&#35757;&#32451;&#27169;&#22411;&#26377;&#25928;&#24615;&#30340;&#24433;&#21709;&#22240;&#32032;&#65292;&#29305;&#21035;&#26159;&#22312;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#30340;&#32972;&#26223;&#19979;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20960;&#31181;&#26032;&#39062;&#30340;&#39044;&#35757;&#32451;&#32763;&#35793;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#19981;&#21516;&#27700;&#24179;&#30340;&#35789;&#27719;&#21644;&#32467;&#26500;&#30693;&#35782;&#65292;&#20363;&#22914;&#65306;1&#65289;&#20174;&#22823;&#22411;&#24179;&#34892;&#35821;&#26009;&#24211;&#29983;&#25104;&#28151;&#28102;&#25968;&#25454;&#65292;2&#65289;&#36830;&#25509;&#20174;&#23567;&#22411;&#35789;&#23545;&#40784;&#35821;&#26009;&#24211;&#25552;&#21462;&#30340;&#30701;&#35821;&#23545;&#65292;&#20197;&#21450;3&#65289;&#29983;&#25104;&#19981;&#24102;&#30495;&#23454;&#20154;&#31867;&#35821;&#26009;&#24211;&#30340;&#21512;&#25104;&#24179;&#34892;&#25968;&#25454;&#12290;&#25105;&#20204;&#22312;&#22810;&#31181;&#35821;&#35328;&#23545;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#21363;&#20351;&#23384;&#22312;&#39640;&#27700;&#24179;&#30340;&#28151;&#28102;&#25110;&#32431;&#21512;&#25104;&#25968;&#25454;&#65292;&#20063;&#21487;&#20197;&#23454;&#29616;&#39044;&#35757;&#32451;&#30340;&#25928;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-training models with large crawled corpora can lead to issues such as toxicity and bias, as well as copyright and privacy concerns. A promising way of alleviating such concerns is to conduct pre-training with synthetic tasks and data, since no real-world information is ingested by the model. Our goal in this paper is to understand the factors that contribute to the effectiveness of pre-training models when using synthetic resources, particularly in the context of neural machine translation. We propose several novel approaches to pre-training translation models that involve different levels of lexical and structural knowledge, including: 1) generating obfuscated data from a large parallel corpus 2) concatenating phrase pairs extracted from a small word-aligned corpus, and 3) generating synthetic parallel data without real human language corpora. Our experiments on multiple language pairs reveal that pre-training benefits can be realized even with high levels of obfuscation or purely
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#65292;&#35757;&#32451;Transformer&#27169;&#22411;&#24212;&#29992;&#20110;&#33258;&#22238;&#24402;&#30446;&#26631;&#38382;&#39064;&#26102;&#65292;&#19982;&#22522;&#20110;&#26799;&#24230;&#30340;&#20803;&#23398;&#20064;&#30340;&#24418;&#24335;&#23494;&#20999;&#30456;&#20851;&#65292;&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#23398;&#20064;&#27169;&#22411;&#30340;&#8220;&#24213;&#23618;&#20248;&#21270;&#31243;&#24207;&#8221;&#30340;&#26426;&#21046;&#65292;&#22312;&#22238;&#24402;&#38382;&#39064;&#30340;&#39046;&#22495;&#20013;&#20174;&#26426;&#26800;&#30340;&#35282;&#24230;&#29702;&#35299;&#20102;Transformers&#27169;&#22411;&#20013;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#20869;&#37096;&#26426;&#21046;&#12290;</title><link>http://arxiv.org/abs/2212.07677</link><description>&lt;p&gt;
Transformer&#27169;&#22411;&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#23454;&#29616;&#19978;&#19979;&#25991;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Transformers learn in-context by gradient descent. (arXiv:2212.07677v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.07677
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#65292;&#35757;&#32451;Transformer&#27169;&#22411;&#24212;&#29992;&#20110;&#33258;&#22238;&#24402;&#30446;&#26631;&#38382;&#39064;&#26102;&#65292;&#19982;&#22522;&#20110;&#26799;&#24230;&#30340;&#20803;&#23398;&#20064;&#30340;&#24418;&#24335;&#23494;&#20999;&#30456;&#20851;&#65292;&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#23398;&#20064;&#27169;&#22411;&#30340;&#8220;&#24213;&#23618;&#20248;&#21270;&#31243;&#24207;&#8221;&#30340;&#26426;&#21046;&#65292;&#22312;&#22238;&#24402;&#38382;&#39064;&#30340;&#39046;&#22495;&#20013;&#20174;&#26426;&#26800;&#30340;&#35282;&#24230;&#29702;&#35299;&#20102;Transformers&#27169;&#22411;&#20013;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#20869;&#37096;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#65292;Transformers&#27169;&#22411;&#20013;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#26426;&#21046;&#23578;&#26410;&#24471;&#21040;&#24456;&#22909;&#30340;&#29702;&#35299;&#65292;&#22823;&#22810;&#21482;&#20572;&#30041;&#22312;&#30452;&#35273;&#19978;&#12290;&#26412;&#25991;&#25552;&#20986;&#65292;&#35757;&#32451;Transformer&#27169;&#22411;&#24212;&#29992;&#20110;&#33258;&#22238;&#24402;&#30446;&#26631;&#38382;&#39064;&#26102;&#65292;&#19982;&#22522;&#20110;&#26799;&#24230;&#30340;&#20803;&#23398;&#20064;&#30340;&#24418;&#24335;&#23494;&#20999;&#30456;&#20851;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20379;&#19968;&#20010;&#31616;&#21333;&#30340;&#26435;&#37325;&#26500;&#36896;&#65292;&#35777;&#26126;&#20102;&#30001;&#21333;&#20010;&#32447;&#24615;&#33258;&#27880;&#24847;&#21147;&#23618;&#24341;&#21457;&#30340;&#25968;&#25454;&#36716;&#25442;&#19982;&#30001;&#20855;&#26377;&#22238;&#24402;&#25439;&#22833;&#30340;&#26799;&#24230;&#19979;&#38477;&#65288;GD&#65289;&#33719;&#24471;&#30340;&#36716;&#25442;&#20855;&#26377;&#31561;&#20215;&#24615;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#24403;&#20165;&#35757;&#32451;&#33258;&#27880;&#24847;&#21147;Transformer&#27169;&#22411;&#36827;&#34892;&#31616;&#21333;&#30340;&#22238;&#24402;&#20219;&#21153;&#26102;&#65292;&#36890;&#36807;GD&#20248;&#21270;&#24471;&#21040;&#30340;&#27169;&#22411;&#19982;&#27169;&#22411;&#26435;&#37325;&#21313;&#20998;&#30456;&#20284;&#65292;&#25110;&#32773;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;GD&#20248;&#21270;&#30340;&#26435;&#37325;&#19982;&#26500;&#36896;&#30340;&#26435;&#37325;&#30456;&#21516;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#32463;&#36807;&#35757;&#32451;&#30340;Transformer&#27169;&#22411;&#26159;&#22914;&#20309;&#22312;&#21069;&#21521;&#20256;&#36882;&#20013;&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#23398;&#20064;&#27169;&#22411;&#30340;&#8220;&#24213;&#23618;&#20248;&#21270;&#31243;&#24207;&#8221;&#30340;&#12290;&#22312;&#22238;&#24402;&#38382;&#39064;&#30340;&#39046;&#22495;&#20013;&#65292;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#20174;&#26426;&#26800;&#30340;&#35282;&#24230;&#29702;&#35299;Transformers&#27169;&#22411;&#20013;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#20869;&#37096;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
At present, the mechanisms of in-context learning in Transformers are not well understood and remain mostly an intuition. In this paper, we suggest that training Transformers on auto-regressive objectives is closely related to gradient-based meta-learning formulations. We start by providing a simple weight construction that shows the equivalence of data transformations induced by 1) a single linear self-attention layer and by 2) gradient-descent (GD) on a regression loss. Motivated by that construction, we show empirically that when training self-attention-only Transformers on simple regression tasks either the models learned by GD and Transformers show great similarity or, remarkably, the weights found by optimization match the construction. Thus we show how trained Transformers become mesa-optimizers i.e. learn models by gradient descent in their forward pass. This allows us, at least in the domain of regression problems, to mechanistically understand the inner workings of in-context
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20219;&#21153;&#30456;&#20284;&#24230;&#20803;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#21152;&#36895;&#26641;&#24418;&#32467;&#26500;Parzen&#20272;&#35745;&#20013;&#30340;&#22810;&#30446;&#26631;&#38750;&#20998;&#23618;&#36229;&#21442;&#25968;&#26368;&#20248;&#21270;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2212.06751</link><description>&lt;p&gt;
&#22522;&#20110;&#20219;&#21153;&#30456;&#20284;&#24230;&#20803;&#23398;&#20064;&#21152;&#36895;&#22810;&#30446;&#26631;&#38750;&#20998;&#23618;&#36229;&#21442;&#25968;&#26368;&#20248;&#21270;&#30340;&#26641;&#24418;&#32467;&#26500;Parzen&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Speeding up Multi-objective Non-hierarchical Hyperparameter Optimization by Task Similarity-Based Meta-Learning for the Tree-structured Parzen Estimator. (arXiv:2212.06751v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.06751
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20219;&#21153;&#30456;&#20284;&#24230;&#20803;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#21152;&#36895;&#26641;&#24418;&#32467;&#26500;Parzen&#20272;&#35745;&#20013;&#30340;&#22810;&#30446;&#26631;&#38750;&#20998;&#23618;&#36229;&#21442;&#25968;&#26368;&#20248;&#21270;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36229;&#21442;&#25968;&#20248;&#21270;&#26159;&#25552;&#39640;&#28145;&#24230;&#23398;&#20064;&#24615;&#33021;&#30340;&#20851;&#38190;&#27493;&#39588;&#12290;&#23454;&#36341;&#32773;&#36890;&#24120;&#38754;&#20020;&#22810;&#20010;&#26041;&#38754;&#30340;&#26435;&#34913;&#65292;&#22914;&#20934;&#30830;&#24615;&#21644;&#24310;&#36831;&#26102;&#38388;&#12290;&#22312;&#28145;&#24230;&#23398;&#20064;&#30340;&#39640;&#35745;&#31639;&#38656;&#27714;&#21644;&#23545;&#39640;&#25928;&#36229;&#21442;&#25968;&#20248;&#21270;&#30340;&#19981;&#26029;&#22686;&#38271;&#38656;&#27714;&#19979;&#65292;&#21152;&#36895;&#22810;&#30446;&#26631;&#20248;&#21270;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#26412;&#25991;&#23558;TPE&#30340;&#25910;&#36141;&#20989;&#25968;&#25193;&#23637;&#21040;&#20803;&#23398;&#20064;&#35774;&#32622;&#20013;&#65292;&#20351;&#29992;&#30001;&#20219;&#21153;&#20043;&#38388;&#39030;&#32423;&#22495;&#20043;&#38388;&#30340;&#37325;&#21472;&#24230;&#23450;&#20041;&#30340;&#20219;&#21153;&#30456;&#20284;&#24615;&#12290;&#25105;&#20204;&#20063;&#20174;&#29702;&#35770;&#19978;&#20998;&#26512;&#24182;&#35299;&#20915;&#20102;&#20219;&#21153;&#30456;&#20284;&#24615;&#30340;&#23616;&#38480;&#24615;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#34920;&#26684;HPO&#22522;&#20934;&#19978;&#21152;&#36895;&#20102;MO-TPE&#65292;&#24182;&#33719;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36824;&#36890;&#36807;&#36194;&#24471;AutoML 2022&#26469;&#24471;&#21040;&#22806;&#37096;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hyperparameter optimization (HPO) is a vital step in improving performance in deep learning (DL). Practitioners are often faced with the trade-off between multiple criteria, such as accuracy and latency. Given the high computational needs of DL and the growing demand for efficient HPO, the acceleration of multi-objective (MO) optimization becomes ever more important. Despite the significant body of work on meta-learning for HPO, existing methods are inapplicable to MO tree-structured Parzen estimator (MO-TPE), a simple yet powerful MO-HPO algorithm. In this paper, we extend TPE's acquisition function to the meta-learning setting using a task similarity defined by the overlap of top domains between tasks. We also theoretically analyze and address the limitations of our task similarity. In the experiments, we demonstrate that our method speeds up MO-TPE on tabular HPO benchmarks and attains state-of-the-art performance. Our method was also validated externally by winning the AutoML 2022 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;"&#31283;&#23450;&#30340;&#32654;&#26415;&#23478;"&#30340;&#22270;&#20687;&#32534;&#36753;&#26041;&#27861;&#65292;&#20854;&#20013;&#21253;&#21547;SEGA&#20197;&#21450;&#28508;&#22312;&#36941;&#21382;&#31561;&#32452;&#25104;&#37096;&#20998;&#65292;&#20351;&#24471;&#29992;&#25143;&#21487;&#20197;&#23454;&#29616;&#22312;&#22270;&#20687;&#29983;&#25104;&#36807;&#31243;&#20013;&#30340;&#32454;&#31890;&#24230;&#25511;&#21046;&#65292;&#21487;&#20197;&#24494;&#22937;&#22320;&#32534;&#36753;&#22270;&#20687;&#12289;&#25913;&#21464;&#26500;&#22270;&#21644;&#39118;&#26684;&#65292;&#36798;&#21040;&#33402;&#26415;&#26500;&#24605;&#20248;&#21270;&#31561;&#30446;&#30340;&#65292;&#23454;&#29616;&#20102;&#22312;&#21508;&#31181;&#25991;&#26412;&#21040;&#22270;&#20687;&#21512;&#25104;&#20219;&#21153;&#20013;&#30340;&#26368;&#20808;&#36827;&#30340;&#23450;&#37327;&#21644;&#23450;&#24615;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2212.06013</link><description>&lt;p&gt;
&#31283;&#23450;&#30340;&#32654;&#26415;&#23478;&#65306;&#25197;&#26354;&#28459;&#27969;&#31354;&#38388;&#20013;&#30340;&#35821;&#20041;&#39550;&#39533;
&lt;/p&gt;
&lt;p&gt;
The Stable Artist: Steering Semantics in Diffusion Latent Space. (arXiv:2212.06013v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.06013
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;"&#31283;&#23450;&#30340;&#32654;&#26415;&#23478;"&#30340;&#22270;&#20687;&#32534;&#36753;&#26041;&#27861;&#65292;&#20854;&#20013;&#21253;&#21547;SEGA&#20197;&#21450;&#28508;&#22312;&#36941;&#21382;&#31561;&#32452;&#25104;&#37096;&#20998;&#65292;&#20351;&#24471;&#29992;&#25143;&#21487;&#20197;&#23454;&#29616;&#22312;&#22270;&#20687;&#29983;&#25104;&#36807;&#31243;&#20013;&#30340;&#32454;&#31890;&#24230;&#25511;&#21046;&#65292;&#21487;&#20197;&#24494;&#22937;&#22320;&#32534;&#36753;&#22270;&#20687;&#12289;&#25913;&#21464;&#26500;&#22270;&#21644;&#39118;&#26684;&#65292;&#36798;&#21040;&#33402;&#26415;&#26500;&#24605;&#20248;&#21270;&#31561;&#30446;&#30340;&#65292;&#23454;&#29616;&#20102;&#22312;&#21508;&#31181;&#25991;&#26412;&#21040;&#22270;&#20687;&#21512;&#25104;&#20219;&#21153;&#20013;&#30340;&#26368;&#20808;&#36827;&#30340;&#23450;&#37327;&#21644;&#23450;&#24615;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22522;&#20110;&#22823;&#35268;&#27169;&#30340;&#25991;&#26412;&#26465;&#20214;&#19979;&#30340;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#22240;&#20854;&#24778;&#20154;&#30340;&#24615;&#33021;&#32780;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#21487;&#20197;&#20165;&#36890;&#36807;&#25991;&#26412;&#29983;&#25104;&#39640;&#20445;&#30495;&#24230;&#30340;&#22270;&#20687;&#12290;&#28982;&#32780;&#65292;&#19968;&#27425;&#24615;&#33719;&#24471;&#39640;&#36136;&#37327;&#30340;&#32467;&#26524;&#20960;&#20046;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#30456;&#21453;&#65292;&#25991;&#26412;&#25351;&#23548;&#30340;&#22270;&#20687;&#29983;&#25104;&#21253;&#25324;&#29992;&#25143;&#23545;&#36755;&#20837;&#36827;&#34892;&#22810;&#27425;&#24494;&#23567;&#30340;&#26356;&#25913;&#65292;&#20197;&#36845;&#20195;&#22320;&#38613;&#21051;&#20986;&#25152;&#24819;&#35937;&#30340;&#22270;&#20687;&#12290;&#28982;&#32780;&#65292;&#36755;&#20837;&#25552;&#31034;&#30340;&#24494;&#23567;&#26356;&#25913;&#36890;&#24120;&#20250;&#23548;&#33268;&#29983;&#25104;&#23436;&#20840;&#19981;&#21516;&#30340;&#22270;&#20687;&#65292;&#22240;&#27492;&#33402;&#26415;&#23478;&#30340;&#25511;&#21046;&#21463;&#38480;&#20110;&#20854;&#31890;&#24230;&#12290;&#20026;&#20102;&#25552;&#20379;&#28789;&#27963;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31283;&#23450;&#30340;&#32654;&#26415;&#23478;&#65292;&#36825;&#26159;&#19968;&#31181;&#22270;&#20687;&#32534;&#36753;&#26041;&#27861;&#65292;&#21487;&#20351;&#22270;&#20687;&#29983;&#25104;&#36807;&#31243;&#20013;&#30340;&#32454;&#31890;&#24230;&#25511;&#21046;&#26356;&#21152;&#23481;&#26131;&#12290;&#20027;&#35201;&#32452;&#25104;&#37096;&#20998;&#26159;&#35821;&#20041;&#24341;&#23548;(SEGA)&#65292;&#23427;&#27839;&#30528;&#19981;&#21516;&#25968;&#37327;&#30340;&#35821;&#20041;&#26041;&#21521;&#24341;&#23548;&#25193;&#25955;&#36807;&#31243;&#12290;&#36825;&#20801;&#35768;&#23545;&#22270;&#20687;&#36827;&#34892;&#24494;&#22937;&#30340;&#32534;&#36753;&#12289;&#25913;&#21464;&#26500;&#22270;&#21644;&#39118;&#26684;&#65292;&#20197;&#21450;&#20248;&#21270;&#25972;&#20307;&#33402;&#26415;&#26500;&#24605;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#28508;&#22312;&#36941;&#21382;&#30340;&#26032;&#25216;&#26415;&#65292;&#23427;&#21487;&#20197;&#23454;&#29616;&#23545;&#22270;&#20687;&#29305;&#23450;&#21306;&#22495;&#30340;&#23616;&#37096;&#21644;&#26377;&#23548;&#21521;&#22320;&#20462;&#25913;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21508;&#31181;&#25991;&#26412;&#21040;&#22270;&#20687;&#21512;&#25104;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#23450;&#37327;&#21644;&#23450;&#24615;&#32467;&#26524;&#65292;&#20026;&#29992;&#25143;&#25552;&#20379;&#20102;&#23545;&#25152;&#29983;&#25104;&#30340;&#22270;&#20687;&#21069;&#25152;&#26410;&#26377;&#30340;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large, text-conditioned generative diffusion models have recently gained a lot of attention for their impressive performance in generating high-fidelity images from text alone. However, achieving high-quality results is almost unfeasible in a one-shot fashion. On the contrary, text-guided image generation involves the user making many slight changes to inputs in order to iteratively carve out the envisioned image. However, slight changes to the input prompt often lead to entirely different images being generated, and thus the control of the artist is limited in its granularity. To provide flexibility, we present the Stable Artist, an image editing approach enabling fine-grained control of the image generation process. The main component is semantic guidance (SEGA) which steers the diffusion process along variable numbers of semantic directions. This allows for subtle edits to images, changes in composition and style, as well as optimization of the overall artistic conception. Furthermo
&lt;/p&gt;</description></item><item><title>Elixir &#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39044;&#36816;&#34892;&#27169;&#22411;&#20998;&#26512;&#30340;&#33258;&#21160;&#21270;&#39640;&#25928;&#22823;&#27169;&#22411;&#35757;&#32451;&#26041;&#26696;&#65292;&#21487;&#20197;&#23558;&#20869;&#23384;&#20351;&#29992;&#21368;&#36733;&#21040; CPU &#21644; NVMe &#23384;&#20648;&#22120;&#20013;&#65292;&#20805;&#20998;&#21457;&#25381;&#30828;&#20214;&#30340;&#28508;&#21147;&#65292;&#23454;&#39564;&#20013;&#34920;&#29616;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#22522;&#20934;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2212.05339</link><description>&lt;p&gt;
Elixir: &#22312;&#23567;&#22411; GPU &#38598;&#32676;&#19978;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Elixir: Train a Large Language Model on a Small GPU Cluster. (arXiv:2212.05339v3 [cs.DC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.05339
&lt;/p&gt;
&lt;p&gt;
Elixir &#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39044;&#36816;&#34892;&#27169;&#22411;&#20998;&#26512;&#30340;&#33258;&#21160;&#21270;&#39640;&#25928;&#22823;&#27169;&#22411;&#35757;&#32451;&#26041;&#26696;&#65292;&#21487;&#20197;&#23558;&#20869;&#23384;&#20351;&#29992;&#21368;&#36733;&#21040; CPU &#21644; NVMe &#23384;&#20648;&#22120;&#20013;&#65292;&#20805;&#20998;&#21457;&#25381;&#30828;&#20214;&#30340;&#28508;&#21147;&#65292;&#23454;&#39564;&#20013;&#34920;&#29616;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#22522;&#20934;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#30001;&#20110;&#20854;&#35268;&#27169;&#21069;&#25152;&#26410;&#26377;&#30340;&#22823;&#23567;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#35757;&#32451;&#36825;&#20123;&#27169;&#22411;&#23545;&#20110;&#22823;&#22810;&#25968;&#30740;&#31350;&#20154;&#21592;&#26469;&#35828;&#26159;&#19968;&#39033;&#25361;&#25112;&#65292;&#22240;&#20026;&#23427;&#38656;&#35201;&#22823;&#37327;&#30340; GPU&#12290;&#20026;&#20102;&#20943;&#23569; GPU &#20869;&#23384;&#20351;&#29992;&#65292;&#25552;&#20986;&#20102;&#20869;&#23384;&#20998;&#21306;&#21644;&#20869;&#23384;&#21368;&#36733;&#12290;&#36825;&#20123;&#26041;&#27861;&#28040;&#38500;&#20102;&#20869;&#23384;&#20887;&#20313;&#65292;&#24182;&#23558;&#20869;&#23384;&#20351;&#29992;&#21368;&#36733;&#21040; CPU &#21644; NVMe &#23384;&#20648;&#22120;&#20013;&#65292;&#20351;&#24471;&#21487;&#20197;&#22312;&#23567;&#22411; GPU &#38598;&#32676;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#30452;&#25509;&#37096;&#32626;&#36825;&#20123;&#35299;&#20915;&#26041;&#26696;&#36890;&#24120;&#20250;&#23548;&#33268;&#27425;&#20248;&#25928;&#29575;&#12290;&#21482;&#26377;&#32463;&#39564;&#20016;&#23500;&#30340;&#19987;&#23478;&#25165;&#33021;&#36890;&#36807;&#20180;&#32454;&#35843;&#25972;&#20998;&#24067;&#24335;&#37197;&#32622;&#26469;&#20805;&#20998;&#21457;&#25381;&#30828;&#20214;&#30340;&#28508;&#21147;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35299;&#20915;&#26041;&#26696; Elixir&#65292;&#23427;&#22522;&#20110;&#39044;&#36816;&#34892;&#27169;&#22411;&#20998;&#26512;&#33258;&#21160;&#21270;&#39640;&#25928;&#30340;&#22823;&#27169;&#22411;&#35757;&#32451;&#12290;Elixir &#30340;&#30446;&#26631;&#26159;&#30830;&#23450;&#20998;&#21306;&#21644;&#21368;&#36733;&#25216;&#26415;&#30340;&#26368;&#20339;&#32452;&#21512;&#65292;&#20197;&#26368;&#22823;&#21270;&#35757;&#32451;&#21534;&#21520;&#37327;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;Elixir &#26174;&#33879;&#20248;&#20110;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
In recent years, large language models have achieved great success due to their unprecedented size. However, training these models poses a challenge for most researchers as it requires a substantial number of GPUs. To reduce GPU memory usage, memory partitioning, and memory offloading have been proposed. These approaches eliminate memory redundancies and offload memory usage to the CPU and NVMe memory, respectively, enabling training on small GPU clusters. However, directly deploying these solutions often leads to suboptimal efficiency. Only experienced experts can unleash the full potential of hardware by carefully tuning the distributed configuration. Thus, we present a novel solution, Elixir, which automates efficient large-model training based on pre-runtime model profiling. Elixir aims to identify the optimal combination of partitioning and offloading techniques to maximize training throughput. In our experiments, Elixir significantly outperforms the current state-of-the-art basel
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#33539;&#30068;&#35770;&#25506;&#31350;&#20102;&#22522;&#30784;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#20855;&#26377;&#26368;&#23567;&#25152;&#38656;&#33021;&#21147;&#30340;&#22522;&#30784;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#24494;&#35843;&#21644;&#36275;&#22815;&#30340;&#36164;&#28304;&#26469;&#35299;&#20915;&#21069;&#32622;&#20219;&#21153;&#25152;&#23450;&#20041;&#30340;&#31867;&#21035;&#20013;&#30340;&#19979;&#28216;&#20219;&#21153;&#65292;&#24182;&#19988;&#36825;&#31181;&#33021;&#21147;&#21487;&#20197;&#25193;&#23637;&#21040;&#20219;&#20309;&#19979;&#28216;&#20219;&#21153;&#65292;&#21482;&#35201;&#20801;&#35768;&#24494;&#35843;&#19988;&#19979;&#28216;&#20219;&#21153;&#21487;&#22312;&#21069;&#32622;&#20219;&#21153;&#23450;&#20041;&#30340;&#33539;&#30068;&#20013;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2211.16327</link><description>&lt;p&gt;
&#22522;&#30784;&#27169;&#22411;&#30340;&#33021;&#21147;&#25506;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Power of Foundation Models. (arXiv:2211.16327v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.16327
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#33539;&#30068;&#35770;&#25506;&#31350;&#20102;&#22522;&#30784;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#20855;&#26377;&#26368;&#23567;&#25152;&#38656;&#33021;&#21147;&#30340;&#22522;&#30784;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#24494;&#35843;&#21644;&#36275;&#22815;&#30340;&#36164;&#28304;&#26469;&#35299;&#20915;&#21069;&#32622;&#20219;&#21153;&#25152;&#23450;&#20041;&#30340;&#31867;&#21035;&#20013;&#30340;&#19979;&#28216;&#20219;&#21153;&#65292;&#24182;&#19988;&#36825;&#31181;&#33021;&#21147;&#21487;&#20197;&#25193;&#23637;&#21040;&#20219;&#20309;&#19979;&#28216;&#20219;&#21153;&#65292;&#21482;&#35201;&#20801;&#35768;&#24494;&#35843;&#19988;&#19979;&#28216;&#20219;&#21153;&#21487;&#22312;&#21069;&#32622;&#20219;&#21153;&#23450;&#20041;&#30340;&#33539;&#30068;&#20013;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#26524;&#22522;&#30784;&#27169;&#22411;&#20855;&#26377;&#26080;&#38480;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#28857;&#12289;&#26080;&#38480;&#35745;&#31639;&#33021;&#21147;&#12289;&#19968;&#20010;&#26080;&#38480;&#22823;&#30340;&#23436;&#32654;&#35757;&#32451;&#31639;&#27861;&#12289;&#20197;&#21450;&#22312;&#39044;&#35774;&#20219;&#21153;&#19978;&#20445;&#35777;&#38646;&#27867;&#21270;&#35823;&#24046;&#65292;&#37027;&#20040;&#23427;&#21487;&#20197;&#29992;&#20110;&#19968;&#20999;&#21527;&#65311;&#20256;&#32479;&#30340;&#34920;&#31034;&#12289;&#20248;&#21270;&#25110;&#27867;&#21270;&#29702;&#35770;&#26080;&#27861;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#22240;&#20026;&#23427;&#20204;&#20027;&#35201;&#25506;&#35752;&#30340;&#38382;&#39064;&#22312;&#36825;&#37324;&#37117;&#26159;&#19981;&#23384;&#22312;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#33539;&#30068;&#35770;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#29702;&#35770;&#24037;&#20855;&#65292;&#20197;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#19977;&#20010;&#32467;&#26524;&#65292;&#31532;&#19968;&#20010;&#38480;&#21046;&#20102;&#22522;&#20110;&#25552;&#31034;&#30340;&#23398;&#20064;&#30340;&#33021;&#21147;&#65292;&#21363;&#20165;&#24403;&#20219;&#21153;&#21487;&#34920;&#31034;&#26102;&#65292;&#27169;&#22411;&#25165;&#33021;&#29992;&#25552;&#31034;&#35299;&#20915;&#19979;&#28216;&#20219;&#21153;&#65307;&#31532;&#20108;&#20010;&#32467;&#26524;&#34920;&#26126;&#65292;&#24494;&#35843;&#19981;&#21463;&#36825;&#20010;&#38480;&#21046;&#65292;&#22240;&#20026;&#19968;&#20010;&#20855;&#26377;&#26368;&#23567;&#25152;&#38656;&#33021;&#21147;&#65288;&#23545;&#31216;&#24615;&#65289;&#30340;&#22522;&#30784;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#24494;&#35843;&#21644;&#36275;&#22815;&#30340;&#36164;&#28304;&#26469;&#29702;&#35770;&#19978;&#35299;&#20915;&#21069;&#32622;&#20219;&#21153;&#25152;&#23450;&#20041;&#30340;&#31867;&#21035;&#20013;&#30340;&#19979;&#28216;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#26368;&#32456;&#32467;&#26524;&#21487;&#20197;&#30475;&#20316;&#26159;&#31532;&#20108;&#20010;&#32467;&#26524;&#30340;&#19968;&#33324;&#21270;&#65292;&#34920;&#26126;&#22914;&#26524;&#20801;&#35768;&#24494;&#35843;&#24182;&#19988;&#19979;&#28216;&#20219;&#21153;&#21487;&#22312;&#21069;&#32622;&#20219;&#21153;&#23450;&#20041;&#30340;&#33539;&#30068;&#20013;&#34920;&#31034;&#65292;&#21017;&#22522;&#30784;&#27169;&#22411;&#30340;&#26368;&#23567;&#33021;&#21147;&#20063;&#36275;&#20197;&#35299;&#20915;&#20219;&#20309;&#19979;&#28216;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
With infinitely many high-quality data points, infinite computational power, an infinitely large foundation model with a perfect training algorithm and guaranteed zero generalization error on the pretext task, can the model be used for everything? This question cannot be answered by the existing theory of representation, optimization or generalization, because the issues they mainly investigate are assumed to be nonexistent here. In this paper, we show that category theory provides powerful machinery to answer this question. We have proved three results. The first one limits the power of prompt-based learning, saying that the model can solve a downstream task with prompts if and only if the task is representable. The second one says fine tuning does not have this limit, as a foundation model with the minimum required power (up to symmetry) can theoretically solve downstream tasks for the category defined by pretext task, with fine tuning and enough resources. Our final result can be se
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;"PuzzleFusion"&#30340;&#31070;&#32463;&#26550;&#26500;&#65292;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#65292;&#29992;&#20110;&#35299;&#20915;&#31354;&#38388;&#25340;&#22270;&#21644;&#25151;&#38388;&#24067;&#23616;&#20219;&#21153;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#20182;&#20204;&#21457;&#29616;&#31616;&#21333;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#21487;&#20197;&#26377;&#25928;&#22320;&#35299;&#20915;&#36825;&#20123;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#31354;&#38388;&#25340;&#22270;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2211.13785</link><description>&lt;p&gt;
PuzzleFusion&#65306;&#37322;&#25918;&#25193;&#25955;&#27169;&#22411;&#22312;&#31354;&#38388;&#25340;&#22270;&#35299;&#20915;&#20013;&#30340;&#23041;&#21147;
&lt;/p&gt;
&lt;p&gt;
PuzzleFusion: Unleashing the Power of Diffusion Models for Spatial Puzzle Solving. (arXiv:2211.13785v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.13785
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;"PuzzleFusion"&#30340;&#31070;&#32463;&#26550;&#26500;&#65292;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#65292;&#29992;&#20110;&#35299;&#20915;&#31354;&#38388;&#25340;&#22270;&#21644;&#25151;&#38388;&#24067;&#23616;&#20219;&#21153;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#20182;&#20204;&#21457;&#29616;&#31616;&#21333;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#21487;&#20197;&#26377;&#25928;&#22320;&#35299;&#20915;&#36825;&#20123;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#31354;&#38388;&#25340;&#22270;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#31471;&#21040;&#31471;&#31070;&#32463;&#26550;&#26500;&#65292;&#29992;&#20110;&#35299;&#20915;&#31354;&#38388;&#25340;&#22270;&#21644;&#25151;&#38388;&#24067;&#23616;&#20219;&#21153;&#12290;&#25152;&#25552;&#20986;&#30340;&#31995;&#32479;"PuzzleFusion"&#38024;&#23545;&#21518;&#32773;&#20219;&#21153;&#65292;&#23558;&#19968;&#32452;&#25151;&#38388;&#24067;&#23616;&#35270;&#20026;&#20463;&#35270;&#22270;&#20013;&#30340;&#22810;&#36793;&#24418;&#26354;&#32447;&#65292;&#24182;&#36890;&#36807;&#20272;&#35745;&#23427;&#20204;&#30340;&#20108;&#32500;&#24179;&#31227;&#21644;&#26059;&#36716;&#26469;&#23545;&#40784;&#25151;&#38388;&#24067;&#23616;&#22359;&#65292;&#31867;&#20284;&#20110;&#35299;&#20915;&#31354;&#38388;&#25340;&#22270;&#30340;&#36807;&#31243;&#12290;&#26412;&#25991;&#20196;&#20154;&#24778;&#35766;&#30340;&#21457;&#29616;&#26159;&#65292;&#25193;&#25955;&#27169;&#22411;&#30340;&#31616;&#21333;&#20351;&#29992;&#26377;&#25928;&#22320;&#23558;&#36825;&#20123;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#31354;&#38388;&#25340;&#22270;&#20219;&#21153;&#35299;&#20915;&#20026;&#26465;&#20214;&#29983;&#25104;&#36807;&#31243;&#12290;&#20026;&#20102;&#23454;&#29616;&#31471;&#21040;&#31471;&#31070;&#32463;&#31995;&#32479;&#30340;&#23398;&#20064;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#26377;&#22320;&#38754;&#23454;&#20917;&#23433;&#25490;&#30340;&#25968;&#25454;&#38598;&#65306;1&#65289;2D Voronoi &#25340;&#22270;&#25968;&#25454;&#38598;&#65292;&#36825;&#26159;&#19968;&#31181;&#36890;&#36807; 2D &#28857;&#38598;&#30340; Voronoi &#22270;&#29983;&#25104;&#25340;&#22270;&#22359;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#65307;&#20197;&#21450;2&#65289;MagicPlan &#25968;&#25454;&#38598;&#65292;&#36825;&#26159;&#19968;&#31181;&#20174; MagicPlan &#29983;&#20135;&#32447;&#25552;&#20379;&#30340;&#30495;&#23454;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#25340;&#22270;&#22359;&#26159;&#30001;&#22686;&#24378;&#29616;&#23454;&#24212;&#29992;&#31243;&#24207;&#26500;&#24314;&#30340;&#25151;&#38388;&#24067;&#23616;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents an end-to-end neural architecture based on Diffusion Models for spatial puzzle solving, particularly jigsaw puzzle and room arrangement tasks. In the latter task, for instance, the proposed system ``PuzzleFusion'' takes a set of room layouts as polygonal curves in the top-down view and aligns the room layout pieces by estimating their 2D translations and rotations, akin to solving the jigsaw puzzle of room layouts. A surprising discovery of the paper is that the simple use of a Diffusion Model effectively solves these challenging spatial puzzle tasks as a conditional generation process. To enable learning of an end-to-end neural system, the paper introduces new datasets with ground-truth arrangements: 1) 2D Voronoi jigsaw dataset, a synthetic one where pieces are generated by Voronoi diagram of 2D pointset; and 2) MagicPlan dataset, a real one offered by MagicPlan from its production pipeline, where pieces are room layouts constructed by augmented reality App by rea
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23558;&#26631;&#31614;&#36755;&#20837;&#30340;GNN&#21644;&#38544;&#24335;GNN&#32479;&#19968;&#36215;&#26469;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;IGNN&#20013;&#30340;&#38544;&#24335;&#24494;&#20998;&#26041;&#27861;&#65292;&#20351;&#24471;&#26631;&#31614;&#26080;&#38480;&#20256;&#25773;&#21464;&#24471;&#21487;&#34892;&#12290;</title><link>http://arxiv.org/abs/2211.10629</link><description>&lt;p&gt;
&#29992;&#28145;&#24230;&#22343;&#34913;&#27169;&#22411;&#32479;&#19968;&#26631;&#31614;&#36755;&#20837;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Unifying Label-inputted Graph Neural Networks with Deep Equilibrium Models. (arXiv:2211.10629v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.10629
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#26631;&#31614;&#36755;&#20837;&#30340;GNN&#21644;&#38544;&#24335;GNN&#32479;&#19968;&#36215;&#26469;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;IGNN&#20013;&#30340;&#38544;&#24335;&#24494;&#20998;&#26041;&#27861;&#65292;&#20351;&#24471;&#26631;&#31614;&#26080;&#38480;&#20256;&#25773;&#21464;&#24471;&#21487;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#23398;&#20064;&#38750;&#27431;&#20960;&#37324;&#24471;&#25968;&#25454;&#26041;&#38754;&#30340;&#25104;&#21151;&#24341;&#21457;&#20102;&#35768;&#22810;&#23376;&#35838;&#39064;&#65292;&#20363;&#22914;&#26631;&#31614;&#36755;&#20837;&#30340;GNN(LGNN)&#21644;&#38544;&#24335;GNN(IGNN)&#12290;&#26412;&#25991;&#36890;&#36807;&#23558;LGNN&#35299;&#37322;&#20026;IGNN&#29702;&#35770;&#24182;&#23558;&#27969;&#34892;&#30340;LGNN&#24402;&#32422;&#20026;IGNN&#30340;&#24418;&#24335;&#26469;&#32479;&#19968;&#36825;&#20004;&#20010;&#23376;&#22495;&#12290;&#35813;&#32479;&#19968;&#31616;&#21270;&#20102;&#20004;&#20010;&#23376;&#22495;&#20043;&#38388;&#30340;&#20132;&#27969;&#24182;&#21551;&#21457;&#20102;&#26356;&#22810;&#30740;&#31350;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#20171;&#32461;&#20102;IGNN&#30340;&#38544;&#24335;&#24494;&#20998;&#21040;LGNN&#20013;&#65292;&#20197;&#24120;&#25968;&#20869;&#23384;&#24494;&#20998;&#20854;&#26080;&#38480;&#33539;&#22260;&#30340;&#26631;&#31614;&#20256;&#25773;&#65292;&#20351;&#20256;&#25773;&#25104;&#20026;&#21487;&#34892;&#30340;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;
The success of Graph Neural Networks (GNN) in learning on non-Euclidean data arouses many subtopics, such as Label-inputted GNN (LGNN) and Implicit GNN (IGNN). LGNN, explicitly inputting supervising information (a.k.a. labels) in GNN, integrates label propagation to achieve superior performance, but with the dilemma between its propagating distance and adaptiveness. IGNN, outputting an equilibrium point by iterating its network infinite times, exploits information in the entire graph to capture long-range dependencies, but with its network constrained to guarantee the existence of the equilibrium. This work unifies the two subdomains by interpreting LGNN in the theory of IGNN and reducing prevailing LGNNs to the form of IGNN. The unification facilitates the exchange between the two subdomains and inspires more studies. Specifically, implicit differentiation of IGNN is introduced to LGNN to differentiate its infinite-range label propagation with constant memory, making the propagation b
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#24658;&#23450;&#20869;&#23384;&#38656;&#27714;&#25193;&#23637;&#25968;&#25454;&#38598;&#31934;&#31616;&#30340;&#26041;&#27861;&#65292;&#21487;&#23558;Matching Training Trajectories&#65288;MTT&#65289;&#24212;&#29992;&#20110;ImageNet-1K&#25968;&#25454;&#38598;&#65292;&#36798;&#21040;6&#20493;&#30340;&#20869;&#23384;&#38477;&#20302;&#65292;&#21516;&#26102;&#22686;&#21152;&#20102;&#32422;2%&#30340;&#36816;&#34892;&#26102;&#24320;&#38144;&#12290;&#21516;&#26102;&#20063;&#21457;&#29616;&#65292;&#20026;&#21512;&#25104;&#22270;&#20687;&#20998;&#37197;&#36719;&#26631;&#31614;&#23545;&#20110;&#23454;&#29616;&#33391;&#22909;&#30340;&#24615;&#33021;&#38750;&#24120;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2211.10586</link><description>&lt;p&gt;
&#21033;&#29992;&#24658;&#23450;&#20869;&#23384;&#23558;&#25968;&#25454;&#38598;&#31934;&#31616;&#25193;&#23637;&#21040;ImageNet-1K
&lt;/p&gt;
&lt;p&gt;
Scaling Up Dataset Distillation to ImageNet-1K with Constant Memory. (arXiv:2211.10586v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.10586
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#24658;&#23450;&#20869;&#23384;&#38656;&#27714;&#25193;&#23637;&#25968;&#25454;&#38598;&#31934;&#31616;&#30340;&#26041;&#27861;&#65292;&#21487;&#23558;Matching Training Trajectories&#65288;MTT&#65289;&#24212;&#29992;&#20110;ImageNet-1K&#25968;&#25454;&#38598;&#65292;&#36798;&#21040;6&#20493;&#30340;&#20869;&#23384;&#38477;&#20302;&#65292;&#21516;&#26102;&#22686;&#21152;&#20102;&#32422;2%&#30340;&#36816;&#34892;&#26102;&#24320;&#38144;&#12290;&#21516;&#26102;&#20063;&#21457;&#29616;&#65292;&#20026;&#21512;&#25104;&#22270;&#20687;&#20998;&#37197;&#36719;&#26631;&#31614;&#23545;&#20110;&#23454;&#29616;&#33391;&#22909;&#30340;&#24615;&#33021;&#38750;&#24120;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#38598;&#31934;&#31616;&#26041;&#27861;&#26088;&#22312;&#23558;&#22823;&#22411;&#25968;&#25454;&#38598;&#21387;&#32553;&#25104;&#19968;&#23567;&#32452;&#21512;&#25104;&#26679;&#26412;&#65292;&#20351;&#24471;&#22312;&#35757;&#32451;&#26102;&#65292;&#19982;&#22312;&#25972;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24120;&#35268;&#35757;&#32451;&#30456;&#27604;&#65292;&#21487;&#20197;&#33719;&#24471;&#31454;&#20105;&#24615;&#30340;&#24615;&#33021;&#12290;&#22312;&#26368;&#36817;&#25552;&#20986;&#30340;&#26041;&#27861;&#20013;&#65292;&#21305;&#37197;&#35757;&#32451;&#36712;&#36857;&#65288;MTT&#65289;&#22312;CIFAR-10/100&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#20294;&#30001;&#20110;&#22312;&#21453;&#21521;&#20256;&#25773;&#36807;&#31243;&#20013;&#25191;&#34892;&#23637;&#24320;&#26799;&#24230;&#35745;&#31639;&#26102;&#38656;&#35201;&#22823;&#37327;&#20869;&#23384;&#65292;&#22240;&#27492;&#24456;&#38590;&#25193;&#23637;&#21040;ImageNet-1k&#25968;&#25454;&#38598;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#23384;&#22312;&#19968;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#20351;&#29992;&#24658;&#23450;&#30340;GPU&#20869;&#23384;&#38656;&#27714;&#65288;&#19982;&#23637;&#24320;&#27493;&#39588;&#30340;&#25968;&#37327;&#26080;&#20851;&#65289;&#31934;&#30830;&#35745;&#31639;&#36712;&#36857;&#21305;&#37197;&#25439;&#22833;&#20989;&#25968;&#30340;&#26799;&#24230;&#12290;&#26377;&#20102;&#36825;&#19968;&#21457;&#29616;&#65292;&#25152;&#25552;&#20986;&#30340;&#20869;&#23384;&#39640;&#25928;&#30340;&#36712;&#36857;&#21305;&#37197;&#26041;&#27861;&#21482;&#38656;&#35201;&#27604;&#21407;&#22987;MTT&#22810;&#32422;2&#65285;&#30340;&#36816;&#34892;&#26102;&#24320;&#38144;&#65292;&#21363;&#21487;&#36731;&#26494;&#25193;&#23637;&#21040;&#20855;&#26377;6&#20493;&#20869;&#23384;&#32553;&#20943;&#30340;ImageNet-1K&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#20026;&#21512;&#25104;&#22270;&#20687;&#20998;&#37197;&#36719;&#26631;&#31614;&#23545;&#20110;&#23454;&#29616;&#33391;&#22909;&#30340;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dataset distillation methods aim to compress a large dataset into a small set of synthetic samples, such that when being trained on, competitive performances can be achieved compared to regular training on the entire dataset. Among recently proposed methods, Matching Training Trajectories (MTT) achieves state-of-the-art performance on CIFAR-10/100, while having difficulty scaling to ImageNet-1k dataset due to the large memory requirement when performing unrolled gradient computation through back-propagation. Surprisingly, we show that there exists a procedure to exactly calculate the gradient of the trajectory matching loss with constant GPU memory requirement (irrelevant to the number of unrolled steps). With this finding, the proposed memory-efficient trajectory matching method can easily scale to ImageNet-1K with 6x memory reduction while introducing only around 2% runtime overhead than original MTT. Further, we find that assigning soft labels for synthetic images is crucial for the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#33258;&#30417;&#30563;&#22810;&#20219;&#21153;&#23398;&#20064;&#26694;&#26550;MT4SSL&#65292;&#36890;&#36807;&#21516;&#26102;&#20351;&#29992;K&#22343;&#20540;&#31639;&#27861;&#20316;&#20026;&#31163;&#32447;&#30446;&#26631;&#25552;&#21462;&#22120;&#21644;&#27809;&#26377;&#26799;&#24230;&#30340;&#25945;&#24072;&#32593;&#32476;&#20316;&#20026;&#22312;&#32447;&#30446;&#26631;&#25552;&#21462;&#22120;&#65292;&#21462;&#24471;&#20102;&#27604;&#20197;&#21069;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;&#21516;&#26102;&#65292;&#20351;&#29992;&#31163;&#32447;&#21644;&#22312;&#32447;&#30446;&#26631;&#25552;&#21462;&#22120;&#21487;&#20197;&#24471;&#21040;&#26356;&#22909;&#30340;&#25910;&#25947;&#24615;&#65292;&#25105;&#20204;&#35748;&#20026;&#36825;&#26159;&#33258;&#30417;&#30563;&#35821;&#38899;&#27169;&#22411;&#19978;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#26377;&#21069;&#36884;&#30340;&#36235;&#21183;&#12290;</title><link>http://arxiv.org/abs/2211.07321</link><description>&lt;p&gt;
MT4SSL&#65306;&#36890;&#36807;&#38598;&#25104;&#22810;&#20010;&#30446;&#26631;&#26469;&#25552;&#21319;&#33258;&#30417;&#30563;&#35821;&#38899;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
MT4SSL: Boosting Self-Supervised Speech Representation Learning by Integrating Multiple Targets. (arXiv:2211.07321v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.07321
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#33258;&#30417;&#30563;&#22810;&#20219;&#21153;&#23398;&#20064;&#26694;&#26550;MT4SSL&#65292;&#36890;&#36807;&#21516;&#26102;&#20351;&#29992;K&#22343;&#20540;&#31639;&#27861;&#20316;&#20026;&#31163;&#32447;&#30446;&#26631;&#25552;&#21462;&#22120;&#21644;&#27809;&#26377;&#26799;&#24230;&#30340;&#25945;&#24072;&#32593;&#32476;&#20316;&#20026;&#22312;&#32447;&#30446;&#26631;&#25552;&#21462;&#22120;&#65292;&#21462;&#24471;&#20102;&#27604;&#20197;&#21069;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;&#21516;&#26102;&#65292;&#20351;&#29992;&#31163;&#32447;&#21644;&#22312;&#32447;&#30446;&#26631;&#25552;&#21462;&#22120;&#21487;&#20197;&#24471;&#21040;&#26356;&#22909;&#30340;&#25910;&#25947;&#24615;&#65292;&#25105;&#20204;&#35748;&#20026;&#36825;&#26159;&#33258;&#30417;&#30563;&#35821;&#38899;&#27169;&#22411;&#19978;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#26377;&#21069;&#36884;&#30340;&#36235;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#35757;&#32451;&#30446;&#26631;&#30340;&#33719;&#21462;&#26041;&#24335;&#25552;&#20986;&#20102;&#33258;&#30417;&#30563;&#35821;&#38899;&#27169;&#22411;&#30340;&#26032;&#35270;&#35282;&#65292;&#24182;&#23558;&#30446;&#26631;&#25552;&#21462;&#22120;&#27010;&#25324;&#20026;&#31163;&#32447;&#30446;&#26631;&#25552;&#21462;&#22120;&#21644;&#22312;&#32447;&#30446;&#26631;&#25552;&#21462;&#22120;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#30417;&#30563;&#22810;&#20219;&#21153;&#23398;&#20064;&#26694;&#26550;MT4SSL&#65292;&#23427;&#20351;&#29992;K&#22343;&#20540;&#31639;&#27861;&#20316;&#20026;&#31163;&#32447;&#30446;&#26631;&#25552;&#21462;&#22120;&#65292;&#20351;&#29992;&#27809;&#26377;&#26799;&#24230;&#30340;&#25945;&#24072;&#32593;&#32476;&#20316;&#20026;&#22312;&#32447;&#30446;&#26631;&#25552;&#21462;&#22120;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;LibriSpeech&#22522;&#20934;&#27979;&#35797;&#19978;&#20248;&#20110;&#20197;&#21069;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#19982;&#20351;&#29992;&#26356;&#23569;&#25968;&#25454;&#30340;&#26368;&#20339;&#27169;&#22411;&#30456;&#24403;&#29978;&#33267;&#26356;&#22909;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#39044;&#35757;&#32451;&#38454;&#27573;&#21516;&#26102;&#20351;&#29992;&#31163;&#32447;&#21644;&#22312;&#32447;&#30446;&#26631;&#25552;&#21462;&#22120;&#21487;&#20197;&#24471;&#21040;&#26356;&#22909;&#30340;&#25910;&#25947;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#35748;&#20026;&#20174;&#25105;&#20204;&#30340;&#35282;&#24230;&#36827;&#34892;&#33258;&#30417;&#30563;&#35821;&#38899;&#27169;&#22411;&#19978;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#26159;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#36235;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we provide a new perspective on self-supervised speech models from how the training targets are obtained. We generalize the targets extractor into Offline Targets Extractor (Off-TE) and Online Targets Extractor (On-TE). Based on this, we propose a new multi-tasking learning framework for self-supervised learning, MT4SSL, which stands for Boosting Self-Supervised Speech Representation Learning by Integrating Multiple Targets. MT4SSL uses the K-means algorithm as an Off-TE and a teacher network without gradients as an On-TE, respectively. Our model outperforms previous SSL methods by nontrivial margins on the LibriSpeech benchmark, and is comparable to or even better than the best-performing models with fewer data. Furthermore, we find that using both Off-TE and On-TE results in better convergence in the pre-training phase. With both effectiveness and efficiency, we think doing multi-task learning on self-supervised speech models from our perspective is a promising trend.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#25552;&#20986;&#22522;&#20110;&#36731;&#37327;&#32423;&#38899;&#39057;&#30340;&#37197;&#23545;&#36870;&#37329;&#23383;&#22612;&#32467;&#26500;&#32593;&#32476;&#21644;&#23494;&#38598;&#22810;&#23618;&#24863;&#30693;&#26426;&#22359;&#32593;&#32476;&#65292;&#23454;&#29616;&#20102;&#22312;&#19981;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#25110;&#27169;&#22411;&#36801;&#31227;&#24773;&#20917;&#19979;&#65292;&#23545;UrbanSound8K&#25968;&#25454;&#38598;&#21644;GTAZN&#25968;&#25454;&#38598;&#30340;&#39640;&#20934;&#30830;&#24230;&#20998;&#31867;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2211.02940</link><description>&lt;p&gt;
&#22522;&#20110;&#37197;&#23545;&#36870;&#37329;&#23383;&#22612;&#32467;&#26500;&#21644;&#23494;&#38598;&#22810;&#23618;&#24863;&#30693;&#26426;&#22359;&#30340;&#26377;&#25928;&#38899;&#39057;&#20998;&#31867;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Effective Audio Classification Network Based on Paired Inverse Pyramid Structure and Dense MLP Block. (arXiv:2211.02940v3 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.02940
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#25552;&#20986;&#22522;&#20110;&#36731;&#37327;&#32423;&#38899;&#39057;&#30340;&#37197;&#23545;&#36870;&#37329;&#23383;&#22612;&#32467;&#26500;&#32593;&#32476;&#21644;&#23494;&#38598;&#22810;&#23618;&#24863;&#30693;&#26426;&#22359;&#32593;&#32476;&#65292;&#23454;&#29616;&#20102;&#22312;&#19981;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#25110;&#27169;&#22411;&#36801;&#31227;&#24773;&#20917;&#19979;&#65292;&#23545;UrbanSound8K&#25968;&#25454;&#38598;&#21644;GTAZN&#25968;&#25454;&#38598;&#30340;&#39640;&#20934;&#30830;&#24230;&#20998;&#31867;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22522;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#21644;&#33258;&#27880;&#24847;&#26426;&#21046;&#30340;&#22823;&#35268;&#27169;&#26550;&#26500;&#24050;&#32463;&#25104;&#20026;&#38899;&#39057;&#20998;&#31867;&#39046;&#22495;&#30340;&#24517;&#35201;&#25216;&#26415;&#12290;&#34429;&#28982;&#36825;&#20123;&#25216;&#26415;&#26159;&#26368;&#20808;&#36827;&#30340;&#65292;&#20294;&#21482;&#26377;&#36890;&#36807;&#24040;&#22823;&#30340;&#35745;&#31639;&#25104;&#26412;&#21644;&#21442;&#25968;&#12289;&#22823;&#37327;&#30340;&#25968;&#25454;&#22686;&#24378;&#12289;&#26469;&#33258;&#22823;&#22411;&#25968;&#25454;&#38598;&#30340;&#36801;&#31227;&#20197;&#21450;&#19968;&#20123;&#20854;&#20182;&#25216;&#24039;&#25165;&#33021;&#20445;&#35777;&#26377;&#25928;&#24615;&#12290;&#36890;&#36807;&#21033;&#29992;&#38899;&#39057;&#30340;&#36731;&#37327;&#32423;&#29305;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#32593;&#32476;&#32467;&#26500;&#8212;&#8212;&#37197;&#23545;&#36870;&#37329;&#23383;&#22612;&#32467;&#26500;&#65288;PIP&#65289;&#65292;&#20197;&#21450;&#19968;&#31181;&#31216;&#20026;&#37197;&#23545;&#36870;&#37329;&#23383;&#22612;&#32467;&#26500;MLP&#32593;&#32476;&#65288;PIPMN&#65289;&#30340;&#32593;&#32476;&#12290;PIP&#32593;&#32476;&#22312;UrbanSound8K&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;96%&#30340;&#29615;&#22659;&#22768;&#38899;&#20998;&#31867;&#20934;&#30830;&#24230;&#65292;&#22312;GTAZN&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;93.2%&#30340;&#38899;&#20048;&#27969;&#27966;&#20998;&#31867;&#20934;&#30830;&#24230;&#65292;&#20165;&#20351;&#29992;100&#19975;&#20010;&#21442;&#25968;&#21363;&#21487;&#23454;&#29616;&#36825;&#20004;&#20010;&#32467;&#26524;&#65292;&#32780;&#19981;&#38656;&#35201;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#25110;&#27169;&#22411;&#36801;&#31227;&#12290;&#20844;&#20849;&#20195;&#30721;&#21487;&#22312;&#20197;&#19979;&#32593;&#22336;&#33719;&#21462;&#65306;https://github.com/JNAIC/PIPMN
&lt;/p&gt;
&lt;p&gt;
Recently, massive architectures based on Convolutional Neural Network (CNN) and self-attention mechanisms have become necessary for audio classification. While these techniques are state-of-the-art, these works' effectiveness can only be guaranteed with huge computational costs and parameters, large amounts of data augmentation, transfer from large datasets and some other tricks. By utilizing the lightweight nature of audio, we propose an efficient network structure called Paired Inverse Pyramid Structure (PIP) and a network called Paired Inverse Pyramid Structure MLP Network (PIPMN). The PIPMN reaches 96\% of Environmental Sound Classification (ESC) accuracy on the UrbanSound8K dataset and 93.2\% of Music Genre Classification (MGC) on the GTAZN dataset, with only 1 million parameters. Both of the results are achieved without data augmentation or model transfer. Public code is available at: https://github.com/JNAIC/PIPMN
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;E-MCTS&#65292;&#36890;&#36807;&#22312;MCTS&#39044;&#27979;&#20013;&#24212;&#29992;&#34920;&#35266;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65292;&#23454;&#29616;&#20102;&#27169;&#22411;&#22522;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#28145;&#24230;&#25506;&#32034;&#65292;&#20197;&#21450;&#35268;&#21010;&#25506;&#32034;&#31574;&#30053;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#22312;&#25104;&#21151;&#30340;&#34920;&#35266;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#21644;&#28145;&#24230;&#25506;&#32034;&#26041;&#38754;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>http://arxiv.org/abs/2210.13455</link><description>&lt;p&gt;
E-MCTS&#65306;&#36890;&#36807;&#35268;&#21010;&#34920;&#35266;&#19981;&#30830;&#23450;&#24615;&#36827;&#34892;&#28145;&#24230;&#25506;&#32034;&#30340;&#27169;&#22411;&#22522;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
E-MCTS: Deep Exploration in Model-Based Reinforcement Learning by Planning with Epistemic Uncertainty. (arXiv:2210.13455v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.13455
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;E-MCTS&#65292;&#36890;&#36807;&#22312;MCTS&#39044;&#27979;&#20013;&#24212;&#29992;&#34920;&#35266;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65292;&#23454;&#29616;&#20102;&#27169;&#22411;&#22522;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#28145;&#24230;&#25506;&#32034;&#65292;&#20197;&#21450;&#35268;&#21010;&#25506;&#32034;&#31574;&#30053;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#22312;&#25104;&#21151;&#30340;&#34920;&#35266;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#21644;&#28145;&#24230;&#25506;&#32034;&#26041;&#38754;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#25311;&#36864;&#28779;&#26641;&#25628;&#32034;&#65288;MCTS&#65289;&#26159;&#27169;&#22411;&#22522;&#24378;&#21270;&#23398;&#20064;&#20013;&#24212;&#29992;&#26368;&#24191;&#27867;&#12289;&#24615;&#33021;&#26368;&#20248;&#31168;&#30340;&#35268;&#21010;&#26041;&#27861;&#20043;&#19968;&#12290;MCTS&#30340;&#20851;&#38190;&#25361;&#25112;&#22312;&#20110;&#28145;&#24230;&#25506;&#32034;&#21644;&#38754;&#23545;&#26410;&#30693;&#26102;&#30340;&#21487;&#38752;&#24615;&#65292;&#36825;&#20004;&#20010;&#25361;&#25112;&#21487;&#20197;&#36890;&#36807;&#22312;MCTS&#39044;&#27979;&#20013;&#20351;&#29992;&#21407;&#21017;&#24615;&#30340;&#34920;&#35266;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26469;&#32531;&#35299;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#20027;&#35201;&#36129;&#29486;&#65306;&#39318;&#20808;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#22312;MCTS&#20013;&#20256;&#25773;&#34920;&#35266;&#19981;&#30830;&#23450;&#24615;&#30340;&#26041;&#27861;&#65292;&#20351;&#26234;&#33021;&#20307;&#33021;&#22815;&#20272;&#35745;&#20854;&#39044;&#27979;&#30340;&#34920;&#35266;&#19981;&#30830;&#23450;&#24615;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#21033;&#29992;&#20256;&#25773;&#30340;&#19981;&#30830;&#23450;&#24615;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#25506;&#32034;&#31639;&#27861;&#65292;&#36890;&#36807;&#26126;&#30830;&#35268;&#21010;&#25506;&#32034;&#31574;&#30053;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#26041;&#27861;&#24212;&#29992;&#20110;&#22522;&#20110;MCTS&#30340;&#27169;&#22411;&#22522;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#20013;&#65292;&#21253;&#25324;&#20351;&#29992;&#23398;&#20064;&#21644;&#25552;&#20379;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#23454;&#29616;&#20102;&#25104;&#21151;&#30340;&#34920;&#35266;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#24182;&#36827;&#34892;&#20102;&#28145;&#24230;&#25506;&#32034;&#12290;&#25105;&#20204;&#23558;&#20854;&#19982;&#22522;&#20110;&#38750;&#35268;&#21010;&#30340;&#28145;&#24230;&#25506;&#32034;&#22522;&#32447;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#24182;&#34920;&#26126;...
&lt;/p&gt;
&lt;p&gt;
One of the most well-studied and highly performing planning approaches used in Model-Based Reinforcement Learning (MBRL) is Monte-Carlo Tree Search (MCTS). Key challenges of MCTS-based MBRL methods remain dedicated deep exploration and reliability in the face of the unknown, and both challenges can be alleviated through principled epistemic uncertainty estimation in the predictions of MCTS. We present two main contributions: First, we develop methodology to propagate epistemic uncertainty in MCTS, enabling agents to estimate the epistemic uncertainty in their predictions. Second, we utilize the propagated uncertainty for a novel deep exploration algorithm by explicitly planning to explore. We incorporate our approach into variations of MCTS-based MBRL approaches with learned and provided models, and empirically show deep exploration through successful epistemic uncertainty estimation achieved by our approach. We compare to a non-planning-based deep-exploration baseline, and demonstrate
&lt;/p&gt;</description></item><item><title>STAP&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#26694;&#26550;&#65292;&#33021;&#22815;&#35757;&#32451;&#25805;&#20316;&#25216;&#33021;&#24182;&#22312;&#35268;&#21010;&#26102;&#21327;&#35843;&#23427;&#20204;&#30340;&#20960;&#20309;&#20381;&#36182;&#20851;&#31995;&#65292;&#20174;&#32780;&#35299;&#20915;&#20219;&#20309;&#25216;&#33021;&#22312;&#35757;&#32451;&#26399;&#38388;&#37117;&#27809;&#26377;&#35265;&#36807;&#30340;&#38271;&#36828;&#20219;&#21153;&#65292;&#20197;&#27492;&#25552;&#21319;&#38271;&#36828;&#20219;&#21153;&#30340;&#25104;&#21151;&#29575;&#12290;</title><link>http://arxiv.org/abs/2210.12250</link><description>&lt;p&gt;
STAP: &#24207;&#21015;&#21270;&#20219;&#21153;&#26080;&#20851;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
STAP: Sequencing Task-Agnostic Policies. (arXiv:2210.12250v3 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.12250
&lt;/p&gt;
&lt;p&gt;
STAP&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#26694;&#26550;&#65292;&#33021;&#22815;&#35757;&#32451;&#25805;&#20316;&#25216;&#33021;&#24182;&#22312;&#35268;&#21010;&#26102;&#21327;&#35843;&#23427;&#20204;&#30340;&#20960;&#20309;&#20381;&#36182;&#20851;&#31995;&#65292;&#20174;&#32780;&#35299;&#20915;&#20219;&#20309;&#25216;&#33021;&#22312;&#35757;&#32451;&#26399;&#38388;&#37117;&#27809;&#26377;&#35265;&#36807;&#30340;&#38271;&#36828;&#20219;&#21153;&#65292;&#20197;&#27492;&#25552;&#21319;&#38271;&#36828;&#20219;&#21153;&#30340;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#20154;&#25216;&#33021;&#33719;&#21462;&#30340;&#36827;&#27493;&#20351;&#26500;&#24314;&#19979;&#28216;&#25805;&#32437;&#20219;&#21153;&#36890;&#29992;&#30340;&#23398;&#20064;&#25216;&#33021;&#24211;&#25104;&#20026;&#21487;&#33021;&#12290;&#28982;&#32780;&#65292;&#21333;&#32431;&#22320;&#25191;&#34892;&#36825;&#20123;&#25216;&#33021;&#36827;&#34892;&#20219;&#21153;&#24635;&#20250;&#22833;&#36133;&#65292;&#22240;&#20026;&#23427;&#27809;&#26377;&#32771;&#34385;&#21040;&#38271;&#36828;&#35745;&#21010;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#21160;&#20316;&#20381;&#36182;&#20851;&#31995;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;STAP&#30340;&#21487;&#25193;&#23637;&#26694;&#26550;&#65292;&#29992;&#20110;&#35757;&#32451;&#25805;&#20316;&#25216;&#33021;&#21644;&#22312;&#35268;&#21010;&#26102;&#21327;&#35843;&#23427;&#20204;&#30340;&#20960;&#20309;&#20381;&#36182;&#20851;&#31995;&#65292;&#20197;&#35299;&#20915;&#20219;&#20309;&#25216;&#33021;&#22312;&#35757;&#32451;&#26399;&#38388;&#37117;&#27809;&#26377;&#35265;&#36807;&#30340;&#38271;&#36828;&#20219;&#21153;&#12290;&#37492;&#20110;Q&#20989;&#25968;&#32534;&#30721;&#20102;&#25216;&#33021;&#21487;&#34892;&#24615;&#30340;&#24230;&#37327;&#65292;&#25105;&#20204;&#21046;&#23450;&#20102;&#19968;&#20010;&#20248;&#21270;&#38382;&#39064;&#65292;&#26368;&#22823;&#21270;&#25152;&#26377;&#25216;&#33021;&#32852;&#21512;&#25104;&#21151;&#30340;&#35745;&#21010;&#24207;&#21015;&#20013;&#30340;&#25104;&#21151;&#65292;&#24182;&#36890;&#36807;&#23427;&#20204;&#30340;Q&#20540;&#30340;&#20056;&#31215;&#26469;&#20272;&#35745;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#30446;&#26631;&#20989;&#25968;&#36817;&#20284;&#20110;&#23454;&#29616;&#35745;&#21010;&#30340;&#21487;&#33021;&#24615;&#65292;&#24403;&#29992;&#20316;&#35268;&#21010;&#30446;&#26631;&#26102;&#65292;&#33021;&#20943;&#23569;&#36817;&#35270;&#34892;&#20026;&#65292;&#20174;&#32780;&#20419;&#36827;&#38271;&#36828;&#20219;&#21153;&#30340;&#25104;&#21151;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;STAP&#22312;&#22810;&#20010;&#20219;&#21153;&#21644;&#26426;&#22120;&#20154;&#24179;&#21488;&#19978;&#30340;&#26377;&#25928;&#24615;&#21644;&#21487;&#20280;&#32553;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Advances in robotic skill acquisition have made it possible to build general-purpose libraries of learned skills for downstream manipulation tasks. However, naively executing these skills one after the other is unlikely to succeed without accounting for dependencies between actions prevalent in long-horizon plans. We present Sequencing Task-Agnostic Policies (STAP), a scalable framework for training manipulation skills and coordinating their geometric dependencies at planning time to solve long-horizon tasks never seen by any skill during training. Given that Q-functions encode a measure of skill feasibility, we formulate an optimization problem to maximize the joint success of all skills sequenced in a plan, which we estimate by the product of their Q-values. Our experiments indicate that this objective function approximates ground truth plan feasibility and, when used as a planning objective, reduces myopic behavior and thereby promotes long-horizon task success. We further demonstra
&lt;/p&gt;</description></item><item><title>RARR&#26159;&#19968;&#20010;&#21487;&#20197;&#23545;&#19981;&#30830;&#23450;&#20449;&#24687;&#36827;&#34892;&#30740;&#31350;&#21644;&#20462;&#35746;&#30340;&#31995;&#32479;&#65292;&#23427;&#21487;&#20197;&#33258;&#21160;&#25214;&#21040;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;&#36755;&#20986;&#30340;&#24402;&#22240;&#24182;&#20462;&#27491;&#19981;&#25903;&#25345;&#30340;&#20869;&#23481;&#12290;</title><link>http://arxiv.org/abs/2210.08726</link><description>&lt;p&gt;
RARR: &#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#30740;&#31350;&#21644;&#20462;&#27491;&#20854;&#36755;&#20986;&#32467;&#26524;&#20013;&#30340;&#19981;&#30830;&#23450;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
RARR: Researching and Revising What Language Models Say, Using Language Models. (arXiv:2210.08726v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.08726
&lt;/p&gt;
&lt;p&gt;
RARR&#26159;&#19968;&#20010;&#21487;&#20197;&#23545;&#19981;&#30830;&#23450;&#20449;&#24687;&#36827;&#34892;&#30740;&#31350;&#21644;&#20462;&#35746;&#30340;&#31995;&#32479;&#65292;&#23427;&#21487;&#20197;&#33258;&#21160;&#25214;&#21040;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;&#36755;&#20986;&#30340;&#24402;&#22240;&#24182;&#20462;&#27491;&#19981;&#25903;&#25345;&#30340;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#22312;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#35832;&#22914;&#23569;&#26679;&#26412;&#23398;&#20064;&#12289;&#38382;&#31572;&#12289;&#25512;&#29702;&#21644;&#23545;&#35805;&#31561;&#35768;&#22810;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#26377;&#26102;&#20250;&#29983;&#25104;&#26080;&#25903;&#25345;&#25110;&#35823;&#23548;&#24615;&#30340;&#20869;&#23481;&#12290;&#30001;&#20110;&#22823;&#22810;&#25968;&#35821;&#35328;&#27169;&#22411;&#27809;&#26377;&#20219;&#20309;&#20869;&#32622;&#30340;&#24402;&#22240;&#22806;&#37096;&#35777;&#25454;&#30340;&#26426;&#21046;&#65292;&#29992;&#25143;&#24456;&#38590;&#30830;&#23450;&#23427;&#20204;&#30340;&#36755;&#20986;&#26159;&#21542;&#21487;&#38752;&#12290;&#20026;&#20102;&#22312;&#20445;&#30041;&#26368;&#26032;&#19968;&#20195;&#27169;&#22411;&#30340;&#25152;&#26377;&#24378;&#22823;&#20248;&#21183;&#30340;&#21516;&#26102;&#23454;&#29616;&#24402;&#22240;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; RARR (&#20351;&#29992;&#30740;&#31350;&#21644;&#20462;&#35746;&#36827;&#34892;&#25913;&#36827;&#24402;&#22240;)&#31995;&#32479;&#65292;&#23427; 1) &#33258;&#21160;&#25214;&#21040;&#20219;&#20309;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;&#36755;&#20986;&#30340;&#24402;&#22240;&#24182; 2) &#22312;&#23613;&#21487;&#33021;&#20445;&#30041;&#21407;&#22987;&#36755;&#20986;&#30340;&#21516;&#26102;&#65292;&#20462;&#27491;&#19981;&#25903;&#25345;&#30340;&#20869;&#23481;&#12290;&#24403;&#24212;&#29992;&#20110;&#20960;&#20010;&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#36755;&#20986;&#20219;&#21153;&#19978;&#30340;&#32467;&#26524;&#26102;&#65292;&#25105;&#20204;&#21457;&#29616;RARR&#22312;&#26174;&#33879;&#25552;&#39640;&#24402;&#22240;&#29575;&#30340;&#21516;&#26102;&#65292;&#27604;&#20197;&#21069;&#25506;&#32034;&#30340;&#32534;&#36753;&#27169;&#22411;&#26356;&#33021;&#20445;&#30041;&#21407;&#22987;&#36755;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models (LMs) now excel at many tasks such as few-shot learning, question answering, reasoning, and dialog. However, they sometimes generate unsupported or misleading content. A user cannot easily determine whether their outputs are trustworthy or not, because most LMs do not have any built-in mechanism for attribution to external evidence. To enable attribution while still preserving all the powerful advantages of recent generation models, we propose RARR (Retrofit Attribution using Research and Revision), a system that 1) automatically finds attribution for the output of any text generation model and 2) post-edits the output to fix unsupported content while preserving the original output as much as possible. When applied to the output of several state-of-the-art LMs on a diverse set of generation tasks, we find that RARR significantly improves attribution while otherwise preserving the original input to a much greater degree than previously explored edit models. Furthermore, 
&lt;/p&gt;</description></item><item><title>&#22806;&#37096;&#25968;&#25454;&#21487;&#33021;&#24341;&#20837;&#25915;&#20987;&#32773;&#31713;&#25913;&#30340;&#26377;&#27602;&#25968;&#25454;&#65292;&#20026;&#20102;&#25552;&#39640;&#27602;&#24615;&#38450;&#24481;&#24615;&#33021;&#65292;&#38656;&#35201;&#20934;&#30830;&#22320;&#20174;&#25968;&#25454;&#38598;&#20013;&#31579;&#36873;&#20986;&#24178;&#20928;&#30340;&#23376;&#38598;&#12290;</title><link>http://arxiv.org/abs/2210.06516</link><description>&lt;p&gt;
&#22914;&#20309;&#22312;&#25968;&#25454;&#27745;&#26579;&#30340;&#24773;&#20917;&#19979;&#31579;&#36873;&#20986;&#24178;&#20928;&#30340;&#25968;&#25454;&#23376;&#38598;&#65311;
&lt;/p&gt;
&lt;p&gt;
How to Sift Out a Clean Data Subset in the Presence of Data Poisoning?. (arXiv:2210.06516v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.06516
&lt;/p&gt;
&lt;p&gt;
&#22806;&#37096;&#25968;&#25454;&#21487;&#33021;&#24341;&#20837;&#25915;&#20987;&#32773;&#31713;&#25913;&#30340;&#26377;&#27602;&#25968;&#25454;&#65292;&#20026;&#20102;&#25552;&#39640;&#27602;&#24615;&#38450;&#24481;&#24615;&#33021;&#65292;&#38656;&#35201;&#20934;&#30830;&#22320;&#20174;&#25968;&#25454;&#38598;&#20013;&#31579;&#36873;&#20986;&#24178;&#20928;&#30340;&#23376;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25152;&#38656;&#30340;&#22823;&#37327;&#25968;&#25454;&#65292;&#36234;&#26469;&#36234;&#22810;&#22320;&#20351;&#29992;&#22806;&#37096;&#20379;&#24212;&#21830;&#12290;&#28982;&#32780;&#65292;&#21512;&#24182;&#22806;&#37096;&#25968;&#25454;&#20250;&#24102;&#26469;&#25968;&#25454;&#27745;&#26579;&#30340;&#39118;&#38505;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#25805;&#32437;&#20182;&#20204;&#30340;&#25968;&#25454;&#20197;&#38477;&#20302;&#27169;&#22411;&#30340;&#25928;&#29992;&#25110;&#23436;&#25972;&#24615;&#12290;&#22823;&#22810;&#25968;&#27602;&#21270;&#38450;&#24481;&#37117;&#20551;&#23450;&#21487;&#20197;&#35775;&#38382;&#19968;&#32452;&#24178;&#20928;&#30340;&#25968;&#25454;&#65288;&#25110;&#22522;&#30784;&#38598;&#65289;&#12290;&#28982;&#32780;&#65292;&#37492;&#20110;&#38544;&#34109;&#24615;&#27602;&#21270;&#25915;&#20987;&#30340;&#24555;&#36895;&#22686;&#38271;&#30740;&#31350;&#65292;&#19968;&#20010;&#38382;&#39064;&#20986;&#29616;&#20102;&#65306;&#38450;&#24481;&#32773;&#30495;&#30340;&#33021;&#22815;&#22312;&#34987;&#27745;&#26579;&#30340;&#25968;&#25454;&#38598;&#20013;&#30830;&#23450;&#19968;&#20010;&#24178;&#20928;&#30340;&#23376;&#38598;&#20197;&#25903;&#25345;&#38450;&#24481;&#21527;&#65311;&#26412;&#25991;&#20174;&#30740;&#31350;&#26377;&#27602;&#26679;&#26412;&#38169;&#35823;&#22320;&#28151;&#20837;&#22522;&#30784;&#38598;&#21518;&#23545;&#38450;&#24481;&#30340;&#24433;&#21709;&#24320;&#22987;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#20116;&#31181;&#38450;&#24481;&#26041;&#27861;&#24182;&#21457;&#29616;&#23427;&#20204;&#30340;&#24615;&#33021;&#20250;&#22312;&#22522;&#30784;&#38598;&#20013;&#27745;&#26579;&#28857;&#23569;&#20110;1&#65285;&#26102;&#24613;&#21095;&#19979;&#38477;&#12290;&#36825;&#20123;&#21457;&#29616;&#34920;&#26126;&#65292;&#22312;&#36825;&#20123;&#38450;&#24481;&#30340;&#24615;&#33021;&#26041;&#38754;&#65292;&#31934;&#30830;&#22320;&#31579;&#36873;&#20986;&#19968;&#20010;&#22522;&#30784;&#38598;&#26159;&#20851;&#38190;&#12290;&#21463;&#36825;&#20123;&#35266;&#23519;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22914;&#20309;&#31934;&#30830;&#30830;&#23450;&#29616;&#26377;&#30340;&#33258;&#21160;&#21270;&#26041;&#27861;&#65292;&#20197;&#22312;&#27745;&#26579;&#25968;&#25454;&#20013;&#37492;&#21035;&#19968;&#20010;&#24178;&#20928;&#30340;&#23376;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given the volume of data needed to train modern machine learning models, external suppliers are increasingly used. However, incorporating external data poses data poisoning risks, wherein attackers manipulate their data to degrade model utility or integrity. Most poisoning defenses presume access to a set of clean data (or base set). While this assumption has been taken for granted, given the fast-growing research on stealthy poisoning attacks, a question arises: can defenders really identify a clean subset within a contaminated dataset to support defenses?  This paper starts by examining the impact of poisoned samples on defenses when they are mistakenly mixed into the base set. We analyze five defenses and find that their performance deteriorates dramatically with less than 1% poisoned points in the base set. These findings suggest that sifting out a base set with high precision is key to these defenses' performance. Motivated by these observations, we study how precise existing auto
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#25511;&#21046;&#38556;&#30861;&#20989;&#25968;&#20351;&#31070;&#32463;ODE&#28385;&#36275;&#36755;&#20986;&#35268;&#33539;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20445;&#35777;&#20102;&#36755;&#20986;&#35268;&#33539;&#65292;&#19988;&#22312;&#35757;&#32451;&#21644;&#25512;&#29702;&#36807;&#31243;&#20013;&#21487;&#20197;&#36890;&#36807;&#25913;&#21464;&#21463;&#38480;&#21442;&#25968;/&#36755;&#20837;&#36827;&#34892;&#25805;&#20316;&#65292;&#27492;&#22806;&#65292;&#35813;&#26041;&#27861;&#36824;&#21019;&#36896;&#20102;&#39069;&#22806;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2210.04763</link><description>&lt;p&gt;
&#20851;&#20110;&#31070;&#32463;ODE&#30340;&#27491;&#21521;&#19981;&#21464;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the Forward Invariance of Neural ODEs. (arXiv:2210.04763v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.04763
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#25511;&#21046;&#38556;&#30861;&#20989;&#25968;&#20351;&#31070;&#32463;ODE&#28385;&#36275;&#36755;&#20986;&#35268;&#33539;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20445;&#35777;&#20102;&#36755;&#20986;&#35268;&#33539;&#65292;&#19988;&#22312;&#35757;&#32451;&#21644;&#25512;&#29702;&#36807;&#31243;&#20013;&#21487;&#20197;&#36890;&#36807;&#25913;&#21464;&#21463;&#38480;&#21442;&#25968;/&#36755;&#20837;&#36827;&#34892;&#25805;&#20316;&#65292;&#27492;&#22806;&#65292;&#35813;&#26041;&#27861;&#36824;&#21019;&#36896;&#20102;&#39069;&#22806;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#19981;&#21464;&#38598;&#20256;&#25773;&#26469;&#30830;&#20445;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;(ODE)&#28385;&#36275;&#36755;&#20986;&#35268;&#33539;&#12290; &#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#19968;&#31867;&#25511;&#21046;&#38556;&#30861;&#20989;&#25968;&#23558;&#36755;&#20986;&#35268;&#33539;&#36716;&#25442;&#20026;&#23545;&#23398;&#20064;&#31995;&#32479;&#30340;&#21442;&#25968;&#21644;&#36755;&#20837;&#30340;&#32422;&#26463;&#26465;&#20214;&#12290; &#36825;&#20010;&#35774;&#32622;&#20351;&#25105;&#20204;&#33021;&#22815;&#36890;&#36807;&#22312;&#35757;&#32451;&#21644;&#25512;&#29702;&#36807;&#31243;&#20013;&#25913;&#21464;&#21463;&#38480;&#21442;&#25968;/&#36755;&#20837;&#26469;&#23454;&#29616;&#36755;&#20986;&#35268;&#33539;&#20445;&#35777;&#12290; &#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#25105;&#20204;&#36890;&#36807;&#25968;&#25454;&#25511;&#21046;&#30340;&#31070;&#32463;ODE&#30340;&#19981;&#21464;&#38598;&#20256;&#25773;&#19981;&#20165;&#20445;&#25345;&#20102;&#27010;&#25324;&#24615;&#33021;&#65292;&#32780;&#19988;&#36890;&#36807;&#21551;&#29992;&#23545;&#31995;&#32479;&#21442;&#25968;/&#36755;&#20837;&#30340;&#22240;&#26524;&#25805;&#20316;&#21019;&#36896;&#20102;&#39069;&#22806;&#30340;&#40065;&#26834;&#24615;&#12290; &#25105;&#20204;&#22312;&#19968;&#31995;&#21015;&#34920;&#31034;&#23398;&#20064;&#20219;&#21153;&#19978;&#27979;&#35797;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#29289;&#29702;&#21160;&#21147;&#23398;&#21644;&#20984;&#24615;&#30011;&#20687;&#65292;&#20197;&#21450;&#33258;&#20027;&#36710;&#36742;&#30340;&#23433;&#20840;&#36991;&#30896;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a new method to ensure neural ordinary differential equations (ODEs) satisfy output specifications by using invariance set propagation. Our approach uses a class of control barrier functions to transform output specifications into constraints on the parameters and inputs of the learning system. This setup allows us to achieve output specification guarantees simply by changing the constrained parameters/inputs both during training and inference. Moreover, we demonstrate that our invariance set propagation through data-controlled neural ODEs not only maintains generalization performance but also creates an additional degree of robustness by enabling causal manipulation of the system's parameters/inputs. We test our method on a series of representation learning tasks, including modeling physical dynamics and convexity portraits, as well as safe collision avoidance for autonomous vehicles.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#23610;&#24230;&#25299;&#25169;&#22855;&#24322;&#24615;&#26816;&#27979;&#26041;&#27861;&#65292;&#21487;&#20197;&#35780;&#20272;&#25968;&#25454;&#30340;&#23616;&#37096;&#22266;&#26377;&#32500;&#24230;&#65292;&#24182;&#37327;&#21270;&#28857;&#30340;&#8220;&#27969;&#24418;&#24230;&#8221;&#65292;&#33021;&#22815;&#26816;&#27979;&#22797;&#26434;&#31354;&#38388;&#21644;&#22270;&#20687;&#20013;&#30340;&#22855;&#24322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2210.00069</link><description>&lt;p&gt;
&#22810;&#23610;&#24230;&#25299;&#25169;&#22855;&#24322;&#24615;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Topological Singularity Detection at Multiple Scales. (arXiv:2210.00069v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.00069
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#23610;&#24230;&#25299;&#25169;&#22855;&#24322;&#24615;&#26816;&#27979;&#26041;&#27861;&#65292;&#21487;&#20197;&#35780;&#20272;&#25968;&#25454;&#30340;&#23616;&#37096;&#22266;&#26377;&#32500;&#24230;&#65292;&#24182;&#37327;&#21270;&#28857;&#30340;&#8220;&#27969;&#24418;&#24230;&#8221;&#65292;&#33021;&#22815;&#26816;&#27979;&#22797;&#26434;&#31354;&#38388;&#21644;&#22270;&#20687;&#20013;&#30340;&#22855;&#24322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27969;&#24418;&#20551;&#35774;&#26159;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#30340;&#19968;&#20010;&#22522;&#26412;&#20551;&#35774;&#65292;&#23427;&#20551;&#23450;&#25968;&#25454;&#20301;&#20110;&#25110;&#25509;&#36817;&#20110;&#20302;&#22266;&#26377;&#32500;&#24230;&#30340;&#26410;&#30693;&#27969;&#24418;&#19978;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#29616;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#34920;&#29616;&#20986;&#26126;&#26174;&#30340;&#38750;&#27969;&#24418;&#32467;&#26500;&#65292;&#21363;&#22855;&#24322;&#24615;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#38169;&#35823;&#30340;&#21457;&#29616;&#12290;&#22240;&#27492;&#65292;&#26816;&#27979;&#36825;&#31181;&#22855;&#24322;&#24615;&#22312;&#25554;&#20540;&#21644;&#25512;&#26029;&#20219;&#21153;&#20043;&#21069;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#25105;&#20204;&#36890;&#36807;&#24320;&#21457;&#19968;&#20010;&#25299;&#25169;&#26694;&#26550;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#65288;i&#65289;&#37327;&#21270;&#23616;&#37096;&#22266;&#26377;&#32500;&#24230;&#65292;&#20197;&#21450;&#65288;ii&#65289;&#22312;&#22810;&#20010;&#23610;&#24230;&#19978;&#20135;&#29983;&#8220;&#27431;&#20960;&#37324;&#24471;&#24615;&#8221;&#35780;&#20998;&#65292;&#29992;&#20197;&#35780;&#20272;&#28857;&#30340;&#8220;&#27969;&#24418;&#24230;&#8221;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#22270;&#20687;&#25968;&#25454;&#20013;&#25429;&#33719;&#22797;&#26434;&#31354;&#38388;&#30340;&#22855;&#24322;&#24615;&#65292;&#21516;&#26102;&#25429;&#25417;&#22855;&#24322;&#32467;&#26500;&#21644;&#23616;&#37096;&#20960;&#20309;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The manifold hypothesis, which assumes that data lies on or close to an unknown manifold of low intrinsic dimension, is a staple of modern machine learning research. However, recent work has shown that real-world data exhibits distinct non-manifold structures, i.e. singularities, that can lead to erroneous findings. Detecting such singularities is therefore crucial as a precursor to interpolation and inference tasks. We address this issue by developing a topological framework that (i) quantifies the local intrinsic dimension, and (ii) yields a Euclidicity score for assessing the 'manifoldness' of a point along multiple scales. Our approach identifies singularities of complex spaces, while also capturing singular structures and local geometric complexity in image data.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#36890;&#36807;&#32858;&#31867;&#21644;&#39044;&#27979;&#27169;&#22411;&#26469;&#23398;&#20064;&#28216;&#25103;&#26234;&#33021;&#20307;&#32676;&#38598;&#30340;&#28436;&#21270;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110; Hebbian &#23398;&#20064;&#30340;&#26080;&#30417;&#30563;&#32858;&#31867;&#26041;&#27861;&#65292;&#32467;&#21512; LSTM &#39044;&#27979;&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;&#22810;&#20010;&#22330;&#26223;&#19979;&#20934;&#30830;&#39044;&#27979;&#32676;&#38598;&#28436;&#21270;&#12290;</title><link>http://arxiv.org/abs/2209.06904</link><description>&lt;p&gt;
&#22522;&#20110;&#36203;&#27604;&#23398;&#20064;&#30340;&#28216;&#25103;&#26234;&#33021;&#20307;&#32676;&#38598;&#28436;&#21270;&#39044;&#27979;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Forecasting Evolution of Clusters in Game Agents with Hebbian Learning. (arXiv:2209.06904v2 [cs.NE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.06904
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#36890;&#36807;&#32858;&#31867;&#21644;&#39044;&#27979;&#27169;&#22411;&#26469;&#23398;&#20064;&#28216;&#25103;&#26234;&#33021;&#20307;&#32676;&#38598;&#30340;&#28436;&#21270;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110; Hebbian &#23398;&#20064;&#30340;&#26080;&#30417;&#30563;&#32858;&#31867;&#26041;&#27861;&#65292;&#32467;&#21512; LSTM &#39044;&#27979;&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;&#22810;&#20010;&#22330;&#26223;&#19979;&#20934;&#30830;&#39044;&#27979;&#32676;&#38598;&#28436;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20363;&#22914;&#23454;&#26102;&#31574;&#30053;&#28216;&#25103;&#36890;&#24120;&#34987;&#26234;&#33021;&#20307;&#30340;&#38598;&#20307;&#34892;&#20026;&#25152;&#39537;&#21160;&#12290;&#20363;&#22914;&#22312;&#26143;&#38469;&#20105;&#38712;II&#20013;&#65292;&#20154;&#31867;&#29609;&#23478;&#20250;&#23558;&#31354;&#38388;&#25509;&#36817;&#30340;&#26234;&#33021;&#20307;&#20998;&#32452;&#25104;&#22242;&#38431;&#65292;&#24182;&#25511;&#21046;&#22242;&#38431;&#20987;&#36133;&#25932;&#20154;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#23558;&#28216;&#25103;&#20013;&#30340;&#26234;&#33021;&#20307;&#36827;&#34892;&#32858;&#31867;&#24050;&#34987;&#29992;&#20110;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#39640;&#25928;&#25511;&#21046;&#20197;&#21450;&#25552;&#20379;&#32473;&#28216;&#25103;&#29992;&#25143;&#30340;&#28216;&#25103;&#20998;&#26512;&#24037;&#20855;&#31561;&#31561;&#22810;&#20010;&#26041;&#38754;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#32858;&#31867;&#25552;&#20379;&#20102;&#26377;&#29992;&#30340;&#20449;&#24687;&#65292;&#20294;&#26159;&#22312;&#30740;&#31350;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#22312;&#32676;&#38598;&#32423;&#21035;&#19978;&#30340;&#21160;&#24577;&#23398;&#20064;&#26041;&#38754;&#36824;&#24456;&#23569;&#34987;&#30740;&#31350;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#22411;AI&#27169;&#22411;&#65292;&#23558;&#26080;&#30417;&#30563;&#23398;&#20064;&#21644;&#33258;&#30417;&#30563;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#26469;&#39044;&#27979;&#26143;&#38469;&#20105;&#38712;II&#20013;&#32676;&#38598;&#30340;&#28436;&#21270;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340; Hebbian &#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312; Set-to-Cluster &#27169;&#22359;&#20013;&#39640;&#25928;&#22320;&#21019;&#24314;&#21487;&#21464;&#25968;&#37327;&#30340;&#32676;&#38598;&#65292;&#20854;&#25512;&#29702;&#26102;&#38388;&#22797;&#26434;&#24230;&#20302;&#20110; K-means &#32858;&#31867;&#12290;&#21516;&#26102;&#65292;&#37319;&#29992;&#20102;&#22522;&#20110;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#30340;&#39044;&#27979;&#27169;&#22411;&#23398;&#20064;&#32676;&#38598;&#30340;&#22797;&#26434;&#21160;&#24577;&#65292;&#24182;&#39044;&#27979;&#26410;&#26469;&#32676;&#38598;&#30340;&#24402;&#23646;&#12290;&#36890;&#36807;&#23454;&#39564;&#32467;&#26524;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#30340;&#32858;&#31867;&#26041;&#27861;&#65292;&#24182;&#33021;&#22815;&#22312;&#28216;&#25103;&#30340;&#22810;&#20010;&#22330;&#26223;&#19979;&#20934;&#30830;&#39044;&#27979;&#32676;&#38598;&#28436;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large multi-agent systems such as real-time strategy games are often driven by collective behavior of agents. For example, in StarCraft II, human players group spatially near agents into a team and control the team to defeat opponents. In this light, clustering the agents in the game has been used for various purposes such as the efficient control of the agents in multi-agent reinforcement learning and game analytic tools for the game users. However, despite the useful information provided by clustering, learning the dynamics of multi-agent systems at a cluster level has been rarely studied yet. In this paper, we present a hybrid AI model that couples unsupervised and self-supervised learning to forecast evolution of the clusters in StarCraft II. We develop an unsupervised Hebbian learning method in a set-to-cluster module to efficiently create a variable number of the clusters with lower inference time complexity than K-means clustering. Also, a long short-term memory based prediction
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#39318;&#27425;&#25552;&#20986;&#20102;&#38024;&#23545;&#30446;&#26631;&#26816;&#27979;&#27169;&#22411;&#30340;&#23454;&#26102;&#22312;&#32447;&#25915;&#20987;&#65292;&#36825;&#20123;&#25915;&#20987;&#21487;&#20197;&#22312;&#25152;&#38656;&#35201;&#30340;&#20301;&#32622;&#21046;&#36896;&#19981;&#23384;&#22312;&#30340;&#29289;&#20307;&#65292;&#25915;&#20987;&#25104;&#21151;&#29575;&#32422;&#20026;90\%&#65292;&#25581;&#31034;&#20102;&#30446;&#26631;&#26816;&#27979;&#27169;&#22411;&#30340;&#24369;&#28857;&#21644;&#23433;&#20840;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2209.01962</link><description>&lt;p&gt;
&#23545;&#25239;&#26816;&#27979;: &#23454;&#26102;&#25915;&#20987;&#30446;&#26631;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Adversarial Detection: Attacking Object Detection in Real Time. (arXiv:2209.01962v4 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.01962
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#25552;&#20986;&#20102;&#38024;&#23545;&#30446;&#26631;&#26816;&#27979;&#27169;&#22411;&#30340;&#23454;&#26102;&#22312;&#32447;&#25915;&#20987;&#65292;&#36825;&#20123;&#25915;&#20987;&#21487;&#20197;&#22312;&#25152;&#38656;&#35201;&#30340;&#20301;&#32622;&#21046;&#36896;&#19981;&#23384;&#22312;&#30340;&#29289;&#20307;&#65292;&#25915;&#20987;&#25104;&#21151;&#29575;&#32422;&#20026;90\%&#65292;&#25581;&#31034;&#20102;&#30446;&#26631;&#26816;&#27979;&#27169;&#22411;&#30340;&#24369;&#28857;&#21644;&#23433;&#20840;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26234;&#33021;&#26426;&#22120;&#20154;&#20381;&#36182;&#30446;&#26631;&#26816;&#27979;&#27169;&#22411;&#26469;&#24863;&#30693;&#29615;&#22659;&#12290;&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#23433;&#20840;&#24615;&#30340;&#36827;&#27493;&#65292;&#25581;&#31034;&#20102;&#30446;&#26631;&#26816;&#27979;&#27169;&#22411;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#23041;&#32961;&#12290;&#28982;&#32780;&#20808;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#20391;&#37325;&#20110;&#25915;&#20987;&#38745;&#24577;&#22270;&#20687;&#25110;&#31163;&#32447;&#35270;&#39057;&#12290;&#22240;&#27492;&#65292;&#20173;&#19981;&#28165;&#26970;&#27492;&#31867;&#25915;&#20987;&#26159;&#21542;&#20250;&#21361;&#21450;&#21160;&#24577;&#29615;&#22659;&#19979;&#23454;&#38469;&#30340;&#26426;&#22120;&#20154;&#24212;&#29992;&#12290;&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;&#38024;&#23545;&#30446;&#26631;&#26816;&#27979;&#27169;&#22411;&#30340;&#39318;&#27425;&#23454;&#26102;&#22312;&#32447;&#25915;&#20987;&#26469;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19977;&#31181;&#25915;&#20987;&#26041;&#24335;&#65292;&#21487;&#20197;&#22312;&#25152;&#38656;&#20301;&#32622;&#29983;&#25104;&#19981;&#23384;&#22312;&#23545;&#35937;&#30340;&#36793;&#30028;&#26694;&#12290;&#36825;&#20123;&#25915;&#20987;&#22312;&#32422;20&#27425;&#36845;&#20195;&#20869;&#36798;&#21040;&#32422;90\%&#30340;&#25104;&#21151;&#29575;&#12290;&#28436;&#31034;&#35270;&#39057;&#21487;&#22312;https://youtu.be/zJZ1aNlXsMU&#19978;&#35266;&#30475;&#12290;
&lt;/p&gt;
&lt;p&gt;
Intelligent robots rely on object detection models to perceive the environment. Following advances in deep learning security it has been revealed that object detection models are vulnerable to adversarial attacks. However, prior research primarily focuses on attacking static images or offline videos. Therefore, it is still unclear if such attacks could jeopardize real-world robotic applications in dynamic environments. This paper bridges this gap by presenting the first real-time online attack against object detection models. We devise three attacks that fabricate bounding boxes for nonexistent objects at desired locations. The attacks achieve a success rate of about 90\% within about 20 iterations. The demo video is available at https://youtu.be/zJZ1aNlXsMU.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35843;&#25972;&#33539;&#20363;&#65292;&#21517;&#20026;ILLUME&#65292;&#36890;&#36807;&#20154;&#26426;&#20132;&#20114;&#26469;&#21512;&#29702;&#21270;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65292;&#20174;&#32780;&#20351;&#27169;&#22411;&#30340;&#36755;&#20986;&#26356;&#31526;&#21512;&#20154;&#30340;&#24605;&#32500;&#26041;&#24335;&#12290;&#22312;&#20351;&#29992;&#30456;&#23545;&#36739;&#23569;&#30340;&#35757;&#32451;&#25968;&#25454;&#21644;&#26368;&#23569;&#30340;&#20154;&#31867;&#21453;&#39304;&#19979;&#65292;ILLUME&#34920;&#29616;&#20986;&#19982;&#26631;&#20934;&#30417;&#30563;&#24494;&#35843;&#30456;&#24403;&#30340;&#31454;&#20105;&#21147;&#12290;</title><link>http://arxiv.org/abs/2208.08241</link><description>&lt;p&gt;
ILLUME&#65306;&#36890;&#36807;&#20154;&#26426;&#20132;&#20114;&#26469;&#21512;&#29702;&#21270;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
ILLUME: Rationalizing Vision-Language Models through Human Interactions. (arXiv:2208.08241v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.08241
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35843;&#25972;&#33539;&#20363;&#65292;&#21517;&#20026;ILLUME&#65292;&#36890;&#36807;&#20154;&#26426;&#20132;&#20114;&#26469;&#21512;&#29702;&#21270;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65292;&#20174;&#32780;&#20351;&#27169;&#22411;&#30340;&#36755;&#20986;&#26356;&#31526;&#21512;&#20154;&#30340;&#24605;&#32500;&#26041;&#24335;&#12290;&#22312;&#20351;&#29992;&#30456;&#23545;&#36739;&#23569;&#30340;&#35757;&#32451;&#25968;&#25454;&#21644;&#26368;&#23569;&#30340;&#20154;&#31867;&#21453;&#39304;&#19979;&#65292;ILLUME&#34920;&#29616;&#20986;&#19982;&#26631;&#20934;&#30417;&#30563;&#24494;&#35843;&#30456;&#24403;&#30340;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#24341;&#23548;&#24050;&#34987;&#35777;&#26126;&#26159;&#26500;&#24314;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;VLM&#65289;&#30340;&#26377;&#25928;&#26041;&#27861;&#65292;&#21487;&#29992;&#20110;&#22270;&#20687;&#23383;&#24149;&#25110;&#35270;&#35273;&#38382;&#39064;&#22238;&#31572;&#31561;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#36755;&#20986;&#24456;&#23569;&#19982;&#29992;&#25143;&#23545;&#29305;&#23450;&#31572;&#26696;&#30340;&#29702;&#24615;&#30456;&#19968;&#33268;&#12290;&#20026;&#20102;&#25913;&#21892;&#36825;&#31181;&#23545;&#40784;&#24182;&#21152;&#24378;&#24120;&#35782;&#21407;&#22240;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20154;&#26426;&#29983;&#25104;&#25968;&#25454;&#30340;&#35843;&#25972;&#33539;&#20363;&#12290;&#25105;&#20204;&#30340;ILLUME&#25191;&#34892;&#20197;&#19979;&#24490;&#29615;&#65306;&#32473;&#23450;&#19968;&#20010;&#22270;&#20687;-&#38382;&#39064;-&#31572;&#26696;&#25552;&#31034;&#65292;VLM&#26679;&#26412;&#22810;&#20010;&#20505;&#36873;&#21407;&#29702;&#65292;&#20154;&#31867;&#35780;&#35770;&#23478;&#36890;&#36807;&#20559;&#22909;&#36873;&#25321;&#25552;&#20379;&#21453;&#39304;&#65292;&#29992;&#20110;&#24494;&#35843;&#12290;&#36825;&#20010;&#24490;&#29615;&#22686;&#21152;&#20102;&#35757;&#32451;&#25968;&#25454;&#65292;&#24182;&#36880;&#28176;&#38613;&#21051;&#20986;&#19982;&#20154;&#31867;&#24847;&#22270;&#30456;&#19968;&#33268;&#30340;VLM&#30340;&#29702;&#24615;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#35814;&#23613;&#23454;&#39564;&#34920;&#26126;&#65292;ILLUME&#22312;&#20351;&#29992; significantly &#26356;&#23569;&#30340;&#35757;&#32451;&#25968;&#25454;&#20165;&#38656;&#35201; minimal &#21453;&#39304;&#30340;&#21516;&#26102;&#65292;&#19982;&#26631;&#20934;&#30417;&#30563;&#24494;&#35843;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bootstrapping from pre-trained language models has been proven to be an efficient approach for building vision-language models (VLM) for tasks such as image captioning or visual question answering. However, outputs of these models rarely align with user's rationales for specific answers. In order to improve this alignment and reinforce commonsense reasons, we propose a tuning paradigm based on human interactions with machine-generated data. Our ILLUME executes the following loop: Given an image-question-answer prompt, the VLM samples multiple candidate rationales, and a human critic provides feedback via preference selection, used for fine-tuning. This loop increases the training data and gradually carves out the VLM's rationalization capabilities that are aligned with human intent. Our exhaustive experiments demonstrate that ILLUME is competitive with standard supervised finetuning while using significantly fewer training data and only requiring minimal feedback.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36974;&#34109;&#33258;&#32534;&#30721;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#22270;&#20687;&#21644;&#35270;&#39057;&#19978;&#35757;&#32451;&#19968;&#20010;&#31616;&#21333;&#30340;&#21333;&#19968;Vision Transformer&#27169;&#22411;&#65292;&#32780;&#19981;&#38656;&#35201;&#26631;&#35760;&#25968;&#25454;&#65292;&#35813;&#27169;&#22411;&#30340;&#35270;&#35273;&#34920;&#31034;&#21487;&#19982;&#21333;&#27169;&#24577;&#34920;&#31034;&#22312;&#22522;&#20934;&#27979;&#35797;&#19978;&#30456;&#24403;&#25110;&#26356;&#22909;&#65292;&#24182;&#19988;&#20351;&#29992;&#26356;&#31616;&#21333;&#30340;&#26550;&#26500;&#12290;</title><link>http://arxiv.org/abs/2206.08356</link><description>&lt;p&gt;
OmniMAE: &#22270;&#29255;&#21644;&#35270;&#39057;&#19978;&#30340;&#21333;&#19968;&#27169;&#22411;&#36974;&#34109;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
OmniMAE: Single Model Masked Pretraining on Images and Videos. (arXiv:2206.08356v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.08356
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36974;&#34109;&#33258;&#32534;&#30721;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#22270;&#20687;&#21644;&#35270;&#39057;&#19978;&#35757;&#32451;&#19968;&#20010;&#31616;&#21333;&#30340;&#21333;&#19968;Vision Transformer&#27169;&#22411;&#65292;&#32780;&#19981;&#38656;&#35201;&#26631;&#35760;&#25968;&#25454;&#65292;&#35813;&#27169;&#22411;&#30340;&#35270;&#35273;&#34920;&#31034;&#21487;&#19982;&#21333;&#27169;&#24577;&#34920;&#31034;&#22312;&#22522;&#20934;&#27979;&#35797;&#19978;&#30456;&#24403;&#25110;&#26356;&#22909;&#65292;&#24182;&#19988;&#20351;&#29992;&#26356;&#31616;&#21333;&#30340;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#20307;&#31995;&#32467;&#26500;&#22312;&#21508;&#31181;&#35270;&#35273;&#39046;&#22495;&#20013;&#24050;&#21464;&#24471;&#31454;&#20105;&#21147;&#21313;&#36275;&#65292;&#20854;&#20013;&#26368;&#33879;&#21517;&#30340;&#26159;&#22270;&#20687;&#21644;&#35270;&#39057;&#12290;&#20043;&#21069;&#30340;&#24037;&#20316;&#36890;&#24120;&#26159;&#30740;&#31350;&#36825;&#20123;&#27169;&#24577;&#20043;&#38388;&#30340;&#38548;&#31163;&#65292;&#20294;&#26159;&#20855;&#26377;&#30456;&#21516;&#30340;&#26550;&#26500;&#24847;&#21619;&#30528;&#21487;&#20197;&#20026;&#22810;&#20010;&#35270;&#35273;&#27169;&#24577;&#35757;&#32451;&#19968;&#20010;&#21333;&#19968;&#30340;&#32479;&#19968;&#27169;&#22411;&#12290;&#20043;&#21069;&#30340;&#32479;&#19968;&#24314;&#27169;&#23581;&#35797;&#36890;&#24120;&#20351;&#29992;&#19987;&#38376;&#20026;&#35270;&#35273;&#20219;&#21153;&#37327;&#36523;&#23450;&#21046;&#30340;&#26550;&#26500;&#65292;&#25110;&#19982;&#21333;&#27169;&#24577;&#27169;&#22411;&#30456;&#27604;&#34920;&#29616;&#26356;&#24046;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36974;&#34109;&#33258;&#32534;&#30721;&#21487;&#20197;&#29992;&#20110;&#22312;&#22270;&#20687;&#21644;&#35270;&#39057;&#19978;&#35757;&#32451;&#19968;&#20010;&#31616;&#21333;&#30340;Vision Transformer&#27169;&#22411;&#65292;&#32780;&#26080;&#38656;&#20219;&#20309;&#26631;&#35760;&#25968;&#25454;&#12290;&#36825;&#20010;&#21333;&#19968;&#30340;&#27169;&#22411;&#23398;&#20064;&#30340;&#35270;&#35273;&#34920;&#31034;&#19982;&#21333;&#27169;&#24577;&#34920;&#31034;&#22312;&#22270;&#20687;&#21644;&#35270;&#39057;&#22522;&#20934;&#27979;&#35797;&#19978;&#30456;&#24403;&#25110;&#26356;&#22909;&#65292;&#21516;&#26102;&#20351;&#29992;&#26356;&#31616;&#21333;&#30340;&#26550;&#26500;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#21024;&#38500;90&#65285;&#30340;&#22270;&#20687;&#21644;95&#65285;&#30340;&#35270;&#39057;&#34917;&#19969;&#65292;&#21487;&#20197;&#23398;&#20064;&#35813;&#27169;&#22411;&#65292;&#20174;&#32780;&#23454;&#29616;&#26497;&#24555;&#30340;&#22823;&#22411;&#27169;&#22411;&#26550;&#26500;&#35757;&#32451;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#21333;&#19968;ViT-Hu
&lt;/p&gt;
&lt;p&gt;
Transformer-based architectures have become competitive across a variety of visual domains, most notably images and videos. While prior work studies these modalities in isolation, having a common architecture suggests that one can train a single unified model for multiple visual modalities. Prior attempts at unified modeling typically use architectures tailored for vision tasks, or obtain worse performance compared to single modality models. In this work, we show that masked autoencoding can be used to train a simple Vision Transformer on images and videos, without requiring any labeled data. This single model learns visual representations that are comparable to or better than single-modality representations on both image and video benchmarks, while using a much simpler architecture. Furthermore, this model can be learned by dropping 90% of the image and 95% of the video patches, enabling extremely fast training of huge model architectures. In particular, we show that our single ViT-Hu
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21033;&#29992;&#31867;&#20284;&#22495;&#30340;&#36741;&#21161;&#20449;&#24687;&#65292;&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#37325;&#23545;&#25239;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#27809;&#26377;&#25935;&#24863;&#23646;&#24615;&#30340;&#30446;&#26631;&#22495;&#30340;&#20844;&#24179;&#20998;&#31867;&#12290;</title><link>http://arxiv.org/abs/2206.03656</link><description>&lt;p&gt;
&#36890;&#36807;&#39046;&#22495;&#36866;&#24212;&#23454;&#29616;&#20844;&#24179;&#20998;&#31867;&#65306;&#19968;&#31181;&#21452;&#37325;&#23545;&#25239;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Fair Classification via Domain Adaptation: A Dual Adversarial Learning Approach. (arXiv:2206.03656v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.03656
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#31867;&#20284;&#22495;&#30340;&#36741;&#21161;&#20449;&#24687;&#65292;&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#37325;&#23545;&#25239;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#27809;&#26377;&#25935;&#24863;&#23646;&#24615;&#30340;&#30446;&#26631;&#22495;&#30340;&#20844;&#24179;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#22312;&#20915;&#31574;&#31995;&#32479;&#20013;&#24471;&#21040;&#24191;&#27867;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#30740;&#31350;&#34920;&#26126;&#26426;&#22120;&#23398;&#20064;&#27495;&#35270;&#21644;&#19981;&#20844;&#30340;&#38382;&#39064;&#21313;&#20998;&#20005;&#37325;&#65292;&#36825;&#20123;&#38382;&#39064;&#38459;&#30861;&#20102;&#39640;&#39118;&#38505;&#24212;&#29992;&#30340;&#37319;&#29992;&#12290;&#20844;&#24179;&#20998;&#31867;&#22120;&#30340;&#26368;&#36817;&#30740;&#31350;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#24191;&#27867;&#20851;&#27880;&#65292;&#26088;&#22312;&#24320;&#21457;&#26377;&#25928;&#30340;&#31639;&#27861;&#23454;&#29616;&#20844;&#24179;&#21644;&#33391;&#22909;&#30340;&#20998;&#31867;&#24615;&#33021;&#12290;&#23613;&#31649;&#36825;&#20123;&#20844;&#24179;&#24863;&#30693;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21462;&#24471;&#20102;&#26497;&#22823;&#30340;&#25104;&#21151;&#65292;&#20294;&#22823;&#22810;&#25968;&#29616;&#26377;&#27169;&#22411;&#38656;&#35201;&#20351;&#29992;&#25935;&#24863;&#23646;&#24615;&#23545;&#25968;&#25454;&#36827;&#34892;&#39044;&#22788;&#29702;&#65292;&#27491;&#21017;&#21270;&#27169;&#22411;&#23398;&#20064;&#25110;&#21518;&#22788;&#29702;&#39044;&#27979;&#65292;&#20197;&#23454;&#29616;&#20844;&#24179;&#39044;&#27979;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#38544;&#31169;&#12289;&#27861;&#24459;&#25110;&#30417;&#31649;&#38480;&#21046;&#65292;&#25935;&#24863;&#23646;&#24615;&#24120;&#24120;&#26159;&#19981;&#23436;&#25972;&#29978;&#33267;&#19981;&#21487;&#29992;&#30340;&#12290;&#34429;&#28982;&#25105;&#20204;&#27809;&#26377;&#25935;&#24863;&#23646;&#24615;&#26469;&#35757;&#32451;&#30446;&#26631;&#22495;&#30340;&#20844;&#24179;&#27169;&#22411;&#65292;&#20294;&#21487;&#33021;&#23384;&#22312;&#20855;&#26377;&#25935;&#24863;&#23646;&#24615;&#30340;&#31867;&#20284;&#22495;&#12290;&#22240;&#27492;&#65292;&#21033;&#29992;&#31867;&#20284;&#22495;&#30340;&#36741;&#21161;&#20449;&#24687;&#38750;&#24120;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern machine learning (ML) models are becoming increasingly popular and are widely used in decision-making systems. However, studies have shown critical issues of ML discrimination and unfairness, which hinder their adoption on high-stake applications. Recent research on fair classifiers has drawn significant attention to developing effective algorithms to achieve fairness and good classification performance. Despite the great success of these fairness-aware machine learning models, most of the existing models require sensitive attributes to pre-process the data, regularize the model learning or post-process the prediction to have fair predictions. However, sensitive attributes are often incomplete or even unavailable due to privacy, legal or regulation restrictions. Though we lack the sensitive attribute for training a fair model in the target domain, there might exist a similar domain that has sensitive attributes. Thus, it is important to exploit auxiliary information from a simil
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#26497;&#31471;&#27004;&#23618;&#24179;&#38754;&#22270;&#37325;&#24314;&#20219;&#21153;&#21644;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#29992;&#20110;&#35299;&#20915;&#35813;&#20219;&#21153;&#65292;&#36890;&#36807;Transformer&#35299;&#30721;&#22120;&#32423;&#32852;&#30340;&#26041;&#24335;&#26469;&#24187;&#21270;&#19981;&#21487;&#35265;&#30340;&#25151;&#38388;&#21644;&#38376;&#20197;&#37325;&#24314;&#25972;&#20010;&#27004;&#23618;&#24179;&#38754;&#22270;&#65292;&#24182;&#22312;701&#20010;&#25151;&#23627;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#36739;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2206.00645</link><description>&lt;p&gt;
&#36890;&#36807;&#32467;&#26500;&#24187;&#21270;Transformer&#32423;&#32852;&#23454;&#29616;&#27004;&#23618;&#24179;&#38754;&#22270;&#30340;&#24674;&#22797;
&lt;/p&gt;
&lt;p&gt;
Floorplan Restoration by Structure Hallucinating Transformer Cascades. (arXiv:2206.00645v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.00645
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#26497;&#31471;&#27004;&#23618;&#24179;&#38754;&#22270;&#37325;&#24314;&#20219;&#21153;&#21644;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#29992;&#20110;&#35299;&#20915;&#35813;&#20219;&#21153;&#65292;&#36890;&#36807;Transformer&#35299;&#30721;&#22120;&#32423;&#32852;&#30340;&#26041;&#24335;&#26469;&#24187;&#21270;&#19981;&#21487;&#35265;&#30340;&#25151;&#38388;&#21644;&#38376;&#20197;&#37325;&#24314;&#25972;&#20010;&#27004;&#23618;&#24179;&#38754;&#22270;&#65292;&#24182;&#22312;701&#20010;&#25151;&#23627;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#36739;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#39033;&#26497;&#31471;&#30340;&#27004;&#23618;&#24179;&#38754;&#22270;&#37325;&#24314;&#20219;&#21153;&#65292;&#20026;&#35813;&#20219;&#21153;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#65292;&#20197;&#21450;&#19968;&#20010;&#31070;&#32463;&#26550;&#26500;&#20316;&#20026;&#35299;&#20915;&#26041;&#26696;&#12290;&#32473;&#23450;&#20174;&#20840;&#26223;&#22270;&#20687;&#20013;&#25512;&#26029;&#25110;&#31574;&#21010;&#30340;&#37096;&#20998;&#27004;&#23618;&#24179;&#38754;&#22270;&#37325;&#24314;&#65292;&#20219;&#21153;&#26159;&#37325;&#24314;&#21253;&#25324;&#19981;&#21487;&#35265;&#24314;&#31569;&#32467;&#26500;&#22312;&#20869;&#30340;&#23436;&#25972;&#27004;&#23618;&#24179;&#38754;&#22270;&#12290;&#25552;&#20986;&#30340;&#31070;&#32463;&#32593;&#32476;1&#65289;&#36890;&#36807;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;Transformer&#23558;&#36755;&#20837;&#30340;&#37096;&#20998;&#27004;&#23618;&#24179;&#38754;&#22270;&#32534;&#30721;&#20026;&#19968;&#32452;&#28508;&#22312;&#21521;&#37327;&#65307;2&#65289;&#36890;&#36807;&#32423;&#32852;Transformer&#35299;&#30721;&#22120;&#65292;&#22312;&#24187;&#21270;&#19981;&#21487;&#35265;&#30340;&#25151;&#38388;&#21644;&#38376;&#30340;&#21516;&#26102;&#37325;&#24314;&#25972;&#20010;&#27004;&#23618;&#24179;&#38754;&#22270;&#12290;&#23450;&#24615;&#21644;&#23450;&#37327;&#35780;&#20272;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;701&#20010;&#25151;&#23627;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#27604;&#26368;&#20808;&#36827;&#30340;&#37325;&#24314;&#25216;&#26415;&#26356;&#26377;&#25928;&#12290;&#25105;&#20204;&#23558;&#20998;&#20139;&#25105;&#20204;&#30340;&#20195;&#30721;&#12289;&#27169;&#22411;&#21644;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents an extreme floorplan reconstruction task, a new benchmark for the task, and a neural architecture as a solution. Given a partial floorplan reconstruction inferred or curated from panorama images, the task is to reconstruct a complete floorplan including invisible architectural structures. The proposed neural network 1) encodes an input partial floorplan into a set of latent vectors by convolutional neural networks and a Transformer; and 2) reconstructs an entire floorplan while hallucinating invisible rooms and doors by cascading Transformer decoders. Qualitative and quantitative evaluations demonstrate effectiveness of our approach over the benchmark of 701 houses, outperforming the state-of-the-art reconstruction techniques. We will share our code, models, and data.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25552;&#20986;&#20102;PEAR&#26041;&#27861;&#65292;&#36825;&#26159;&#19968;&#20010;&#39318;&#20010;&#33021;&#22815;&#38024;&#23545;&#26368;&#32456;&#29992;&#25143;&#38656;&#27714;&#25552;&#20379;&#20010;&#24615;&#21270;&#31639;&#27861;&#34917;&#25937;&#25104;&#26412;&#30340;&#20154;&#26426;&#20132;&#20114;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#36125;&#21494;&#26031;&#20559;&#22909;&#24341;&#23548;&#30340;&#35265;&#35299;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#21407;&#21017;&#24615;&#20449;&#24687;&#22686;&#30410;&#24230;&#37327;&#26469;&#35745;&#31639;&#30446;&#26631;&#29992;&#25143;&#36873;&#25321;&#30340;&#39044;&#26399;&#25928;&#29992;&#65292;&#28982;&#21518;&#23558;&#20559;&#22909;&#24341;&#23548;&#25972;&#21512;&#21040;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#20013;&#12290;&#35813;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#31639;&#27861;&#24178;&#39044;&#30340;&#32463;&#27982;&#23454;&#29992;&#24615;&#21644;&#29992;&#25143;&#21451;&#22909;&#24615;&#12290;</title><link>http://arxiv.org/abs/2205.13743</link><description>&lt;p&gt;
&#24102;&#26377;&#20559;&#22909;&#24341;&#23548;&#30340;&#20010;&#24615;&#21270;&#31639;&#27861;&#24178;&#39044;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Personalized Algorithmic Recourse with Preference Elicitation. (arXiv:2205.13743v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.13743
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25552;&#20986;&#20102;PEAR&#26041;&#27861;&#65292;&#36825;&#26159;&#19968;&#20010;&#39318;&#20010;&#33021;&#22815;&#38024;&#23545;&#26368;&#32456;&#29992;&#25143;&#38656;&#27714;&#25552;&#20379;&#20010;&#24615;&#21270;&#31639;&#27861;&#34917;&#25937;&#25104;&#26412;&#30340;&#20154;&#26426;&#20132;&#20114;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#36125;&#21494;&#26031;&#20559;&#22909;&#24341;&#23548;&#30340;&#35265;&#35299;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#21407;&#21017;&#24615;&#20449;&#24687;&#22686;&#30410;&#24230;&#37327;&#26469;&#35745;&#31639;&#30446;&#26631;&#29992;&#25143;&#36873;&#25321;&#30340;&#39044;&#26399;&#25928;&#29992;&#65292;&#28982;&#21518;&#23558;&#20559;&#22909;&#24341;&#23548;&#25972;&#21512;&#21040;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#20013;&#12290;&#35813;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#31639;&#27861;&#24178;&#39044;&#30340;&#32463;&#27982;&#23454;&#29992;&#24615;&#21644;&#29992;&#25143;&#21451;&#22909;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31639;&#27861;&#24178;&#39044;&#65288;AR&#65289;&#30340;&#38382;&#39064;&#26159;&#35745;&#31639;&#29992;&#25143;&#25191;&#34892;&#19968;&#31995;&#21015;&#25805;&#20316;&#20197;&#39072;&#35206;&#19981;&#33391;&#26426;&#22120;&#20915;&#31574;&#30340;&#36807;&#31243;&#12290;&#35813;&#36807;&#31243;&#30340;&#25805;&#20316;&#24207;&#21015;&#19981;&#24212;&#35813;&#23545;&#29992;&#25143;&#30340;&#23454;&#26045;&#25552;&#20986;&#36807;&#39640;&#30340;&#35201;&#27714;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;AR&#26041;&#27861;&#37117;&#20551;&#35774;&#25152;&#26377;&#29992;&#25143;&#30340;&#25805;&#20316;&#25104;&#26412;&#30456;&#21516;&#65292;&#22240;&#27492;&#21487;&#33021;&#20250;&#21521;&#26576;&#20123;&#29992;&#25143;&#25512;&#33616;&#26114;&#36149;&#30340;&#34917;&#25937;&#35745;&#21010;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PEAR&#65292;&#36825;&#26159;&#19968;&#31181;&#39318;&#20010;&#21487;&#25552;&#20379;&#20010;&#24615;&#21270;&#31639;&#27861;&#34917;&#25937;&#25104;&#26412;&#30340;&#20154;&#26426;&#20132;&#20114;&#26041;&#27861;&#65292;&#20197;&#28385;&#36275;&#20219;&#20309;&#26368;&#32456;&#29992;&#25143;&#30340;&#38656;&#27714;&#12290;PEAR&#21033;&#29992;&#36125;&#21494;&#26031;&#20559;&#22909;&#24341;&#23548;&#30340;&#35265;&#35299;&#65292;&#36890;&#36807;&#21521;&#30446;&#26631;&#29992;&#25143;&#21457;&#20986;&#36873;&#25321;&#38598;&#26597;&#35810;&#26469;&#36845;&#20195;&#22320;&#25913;&#21892;&#23545;&#25805;&#20316;&#25104;&#26412;&#30340;&#20272;&#35745;&#20540;&#12290;&#36825;&#20123;&#26597;&#35810;&#30340;&#35745;&#31639;&#26159;&#36890;&#36807;&#26368;&#22823;&#21270;&#36873;&#25321;&#30340;&#39044;&#26399;&#25928;&#29992;&#26469;&#35745;&#31639;&#30340;&#65292;&#36825;&#26159;&#19968;&#31181;&#33021;&#22815;&#32771;&#34385;&#25104;&#26412;&#20272;&#35745;&#21644;&#29992;&#25143;&#21709;&#24212;&#19981;&#30830;&#23450;&#24615;&#30340;&#21407;&#21017;&#24615;&#20449;&#24687;&#22686;&#30410;&#24230;&#37327;&#12290;PEAR&#23558;&#20559;&#22909;&#24341;&#23548;&#25972;&#21512;&#21040;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#20013;&#65292;&#21516;&#26102;&#32771;&#34385;&#29992;&#25143;&#23454;&#29616;AR&#20219;&#21153;&#25152;&#38656;&#36798;&#25104;&#30446;&#26631;&#30340;&#20559;&#22909;&#65292;&#20197;&#21450;&#25191;&#34892;&#27599;&#20010;&#25805;&#20316;&#25152;&#28041;&#21450;&#30340;&#25104;&#26412;&#12290;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;AR&#20219;&#21153;&#26469;&#35780;&#20272;PEAR&#65292;&#24182;&#26174;&#31034;&#20854;&#27604;&#29616;&#26377;&#30340;&#26041;&#27861;&#25214;&#21040;&#20102;&#26356;&#20026;&#32463;&#27982;&#23454;&#29992;&#19988;&#29992;&#25143;&#21451;&#22909;&#30340;&#34917;&#25937;&#35745;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;
Algorithmic Recourse (AR) is the problem of computing a sequence of actions that -- once performed by a user -- overturns an undesirable machine decision. It is paramount that the sequence of actions does not require too much effort for users to implement. Yet, most approaches to AR assume that actions cost the same for all users, and thus may recommend unfairly expensive recourse plans to certain users. Prompted by this observation, we introduce PEAR, the first human-in-the-loop approach capable of providing personalized algorithmic recourse tailored to the needs of any end-user. PEAR builds on insights from Bayesian Preference Elicitation to iteratively refine an estimate of the costs of actions by asking choice set queries to the target user. The queries themselves are computed by maximizing the Expected Utility of Selection, a principled measure of information gain accounting for uncertainty on both the cost estimate and the user's responses. PEAR integrates elicitation into a Rein
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;RoMFAC&#31639;&#27861;&#65292;&#36890;&#36807;&#26032;&#30340;&#35757;&#32451;&#30446;&#26631;&#21644;&#37325;&#22797;&#30340;&#27491;&#21017;&#21270;&#25439;&#22833;&#20989;&#25968;&#65292;&#20351;&#20854;&#23545;&#20110;&#24322;&#24120;&#29366;&#24577;&#24178;&#25200;&#20855;&#26377;&#40065;&#26834;&#24615;&#24182;&#33719;&#24471;&#20986;&#33394;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2205.07229</link><description>&lt;p&gt;
RoMFAC: &#19968;&#31181;&#23545;&#20110;&#29366;&#24577;&#24322;&#24120;&#24178;&#25200;&#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#22343;&#22330;&#28436;&#21592;-&#35780;&#35770;&#23478;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
RoMFAC: A robust mean-field actor-critic reinforcement learning against adversarial perturbations on states. (arXiv:2205.07229v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.07229
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;RoMFAC&#31639;&#27861;&#65292;&#36890;&#36807;&#26032;&#30340;&#35757;&#32451;&#30446;&#26631;&#21644;&#37325;&#22797;&#30340;&#27491;&#21017;&#21270;&#25439;&#22833;&#20989;&#25968;&#65292;&#20351;&#20854;&#23545;&#20110;&#24322;&#24120;&#29366;&#24577;&#24178;&#25200;&#20855;&#26377;&#40065;&#26834;&#24615;&#24182;&#33719;&#24471;&#20986;&#33394;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#38656;&#35201;&#22522;&#20110;&#35266;&#23519;&#21040;&#30340;&#31995;&#32479;&#29366;&#24577;&#20570;&#20986;&#26368;&#20248;&#20915;&#31574;&#65292;&#20294;&#26159;&#35266;&#23519;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#21487;&#33021;&#20250;&#23548;&#33268;&#26234;&#33021;&#20307;&#20570;&#20986;&#38169;&#35823;&#30340;&#34892;&#21160;&#12290;&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;RoMFAC&#65292;&#23427;&#36890;&#36807;&#19968;&#20010;&#31574;&#30053;&#26799;&#24230;&#20989;&#25968;&#21644;&#19968;&#20010;&#20195;&#34920;&#29366;&#24577;&#24178;&#25200;&#24433;&#21709;&#30340;&#34892;&#21160;&#25439;&#22833;&#20989;&#25968;&#35757;&#32451;&#28436;&#21592;&#12290;&#21516;&#26102;&#65292;RoMFAC&#36824;&#24341;&#20837;&#19968;&#20010;&#37325;&#22797;&#30340;&#27491;&#21017;&#21270;&#34892;&#21160;&#25439;&#22833;&#20989;&#25968;&#30340;&#26041;&#27861;&#65292;&#20197;&#30830;&#20445;&#35757;&#32451;&#28436;&#21592;&#20855;&#26377;&#20986;&#33394;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-agent deep reinforcement learning makes optimal decisions dependent on system states observed by agents, but any uncertainty on the observations may mislead agents to take wrong actions. The Mean-Field Actor-Critic reinforcement learning (MFAC) is well-known in the multi-agent field since it can effectively handle a scalability problem. However, it is sensitive to state perturbations that can significantly degrade the team rewards. This work proposes a Robust Mean-field Actor-Critic reinforcement learning (RoMFAC) that has two innovations: 1) a new objective function of training actors, composed of a \emph{policy gradient function} that is related to the expected cumulative discount reward on sampled clean states and an \emph{action loss function} that represents the difference between actions taken on clean and adversarial states; and 2) a repetitive regularization of the action loss, ensuring the trained actors to obtain excellent performance. Furthermore, this work proposes a 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22686;&#24191;&#25289;&#26684;&#26391;&#26085;&#26494;&#24347;&#26041;&#27861;(AL-PINNs)&#29992;&#20110;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;(PINNs)&#30340;&#35757;&#32451;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#33258;&#36866;&#24212;&#24179;&#34913;&#27599;&#20010;&#25439;&#22833;&#32452;&#20214;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#35299;&#20915;&#38750;&#32447;&#24615;&#20559;&#24494;&#20998;&#26041;&#31243;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2205.01059</link><description>&lt;p&gt;
&#22686;&#24378;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#30340;&#22686;&#24191;&#25289;&#26684;&#26391;&#26085;&#26494;&#24347;&#26041;&#27861;&#65288;AL-PINNs&#65289;
&lt;/p&gt;
&lt;p&gt;
Enhanced Physics-Informed Neural Networks with Augmented Lagrangian Relaxation Method (AL-PINNs). (arXiv:2205.01059v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.01059
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22686;&#24191;&#25289;&#26684;&#26391;&#26085;&#26494;&#24347;&#26041;&#27861;(AL-PINNs)&#29992;&#20110;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;(PINNs)&#30340;&#35757;&#32451;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#33258;&#36866;&#24212;&#24179;&#34913;&#27599;&#20010;&#25439;&#22833;&#32452;&#20214;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#35299;&#20915;&#38750;&#32447;&#24615;&#20559;&#24494;&#20998;&#26041;&#31243;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#24050;&#25104;&#20026;&#31185;&#23398;&#35745;&#31639;&#20013;&#28145;&#24230;&#23398;&#20064;&#30340;&#26480;&#20986;&#24212;&#29992;&#65292;&#23427;&#20204;&#26159;&#38750;&#32447;&#24615;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#35299;&#30340;&#24378;&#22823;&#36924;&#36817;&#22120;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24191;&#25289;&#26684;&#26391;&#26085;&#26494;&#24347;&#26041;&#27861;&#65288;AL-PINNs&#65289;&#29992;&#20110;PINNs&#30340;&#35757;&#32451;&#12290;&#25105;&#20204;&#23558;&#21021;&#22987;&#21644;&#36793;&#30028;&#26465;&#20214;&#35270;&#20026;PDE&#27531;&#24046;&#20248;&#21270;&#38382;&#39064;&#30340;&#32422;&#26463;&#26465;&#20214;&#12290;&#36890;&#36807;&#24212;&#29992;&#22686;&#24191;&#25289;&#26684;&#26391;&#26085;&#26494;&#24347;&#26041;&#27861;&#65292;&#32422;&#26463;&#20248;&#21270;&#38382;&#39064;&#21464;&#25104;&#20102;&#19968;&#20010;&#39034;&#24207;&#30340;&#26368;&#22823;-&#26368;&#23567;&#38382;&#39064;&#65292;&#20351;&#21487;&#20197;&#23398;&#20064;&#30340;&#21442;&#25968;&#955;&#33021;&#33258;&#36866;&#24212;&#24179;&#34913;&#27599;&#20010;&#25439;&#22833;&#32452;&#20214;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#25439;&#22833;&#20989;&#25968;&#30340;&#26368;&#23567;&#21270;&#24207;&#21015;&#25910;&#25947;&#20110;Helmholtz&#12289;&#31896;&#24615;Burgers&#21644;Klein-Gordon&#26041;&#31243;&#30340;&#23454;&#38469;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Physics-Informed Neural Networks (PINNs) have become a prominent application of deep learning in scientific computation, as they are powerful approximators of solutions to nonlinear partial differential equations (PDEs). There have been numerous attempts to facilitate the training process of PINNs by adjusting the weight of each component of the loss function, called adaptive loss-balancing algorithms. In this paper, we propose an Augmented Lagrangian relaxation method for PINNs (AL-PINNs). We treat the initial and boundary conditions as constraints for the optimization problem of the PDE residual. By employing Augmented Lagrangian relaxation, the constrained optimization problem becomes a sequential max-min problem so that the learnable parameters $\lambda$ adaptively balance each loss component. Our theoretical analysis reveals that the sequence of minimizers of the proposed loss functions converges to an actual solution for the Helmholtz, viscous Burgers, and Klein--Gordon equations
&lt;/p&gt;</description></item><item><title>&#38646;&#26679;&#26412;&#26426;&#22120;&#36951;&#24536;&#26159;&#19968;&#20010;&#26032;&#20852;&#30340;&#30740;&#31350;&#38382;&#39064;&#65292;&#20801;&#35768;&#20174;&#24050;&#32463;&#35757;&#32451;&#22909;&#30340;ML&#27169;&#22411;&#20013;&#21024;&#38500;&#25968;&#25454;&#12290;&#22240;&#20026;&#36825;&#20123;&#35831;&#27714;&#21487;&#33021;&#20250;&#28041;&#21450;&#21040;&#26080;&#27861;&#35775;&#38382;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#22240;&#27492;&#38656;&#35201;&#26032;&#30340;&#35299;&#20915;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2201.05629</link><description>&lt;p&gt;
&#38646;&#26679;&#26412;&#26426;&#22120;&#36951;&#24536;
&lt;/p&gt;
&lt;p&gt;
Zero-Shot Machine Unlearning. (arXiv:2201.05629v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.05629
&lt;/p&gt;
&lt;p&gt;
&#38646;&#26679;&#26412;&#26426;&#22120;&#36951;&#24536;&#26159;&#19968;&#20010;&#26032;&#20852;&#30340;&#30740;&#31350;&#38382;&#39064;&#65292;&#20801;&#35768;&#20174;&#24050;&#32463;&#35757;&#32451;&#22909;&#30340;ML&#27169;&#22411;&#20013;&#21024;&#38500;&#25968;&#25454;&#12290;&#22240;&#20026;&#36825;&#20123;&#35831;&#27714;&#21487;&#33021;&#20250;&#28041;&#21450;&#21040;&#26080;&#27861;&#35775;&#38382;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#22240;&#27492;&#38656;&#35201;&#26032;&#30340;&#35299;&#20915;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#38544;&#31169;&#27861;&#35268;&#36171;&#20104;&#20844;&#27665;&#34987;&#20135;&#21697;&#12289;&#26381;&#21153;&#21644;&#20844;&#21496;&#36951;&#24536;&#30340;&#26435;&#21033;&#12290;&#23545;&#20110;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#24212;&#29992;&#32780;&#35328;&#65292;&#36825;&#38656;&#35201;&#20174;&#23384;&#20648;&#24402;&#26723;&#21644;ML&#27169;&#22411;&#20013;&#21024;&#38500;&#25968;&#25454;&#12290;&#30001;&#20110;ML&#24212;&#29992;&#38656;&#35201;&#36234;&#26469;&#36234;&#22810;&#30340;&#30417;&#31649;&#21512;&#35268;&#24615;&#65292;&#26426;&#22120;&#36951;&#24536;&#24050;&#25104;&#20026;&#19968;&#20010;&#19981;&#26029;&#20986;&#29616;&#30340;&#30740;&#31350;&#38382;&#39064;&#12290;&#34987;&#36951;&#24536;&#35831;&#27714;&#20197;&#21024;&#38500;&#24050;&#32463;&#35757;&#32451;&#22909;&#30340;ML&#27169;&#22411;&#20013;&#30340;&#19968;&#23450;&#38598;&#21512;&#25110;&#31867;&#21035;&#30340;&#25968;&#25454;&#30340;&#24418;&#24335;&#25552;&#20986;&#12290;&#23454;&#38469;&#32771;&#34385;&#38459;&#27490;&#20002;&#24323;&#21024;&#38500;&#30340;&#25968;&#25454;&#21518;&#20174;&#22836;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#12290;&#29616;&#26377;&#23569;&#25968;&#30740;&#31350;&#20351;&#29992;&#25972;&#20010;&#35757;&#32451;&#25968;&#25454;&#12289;&#35757;&#32451;&#25968;&#25454;&#23376;&#38598;&#25110;&#22312;&#35757;&#32451;&#26399;&#38388;&#23384;&#20648;&#30340;&#19968;&#20123;&#20803;&#25968;&#25454;&#26356;&#26032;&#36951;&#24536;&#30340;&#27169;&#22411;&#26435;&#37325;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#35757;&#32451;&#36807;&#31243;&#25110;&#35757;&#32451;&#26679;&#26412;&#30456;&#20851;&#30340;&#25968;&#25454;&#21487;&#33021;&#26080;&#27861;&#35775;&#38382;&#65292;&#25105;&#20204;&#22240;&#27492;&#25552;&#20986;&#38382;&#39064;&#65306;&#26159;&#21542;&#21487;&#33021;&#36890;&#36807;&#38646;&#26679;&#26412;&#23398;&#20064;&#23454;&#29616;&#36951;&#24536;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern privacy regulations grant citizens the right to be forgotten by products, services and companies. In case of machine learning (ML) applications, this necessitates deletion of data not only from storage archives but also from ML models. Due to an increasing need for regulatory compliance required for ML applications, machine unlearning is becoming an emerging research problem. The right to be forgotten requests come in the form of removal of a certain set or class of data from the already trained ML model. Practical considerations preclude retraining of the model from scratch after discarding the deleted data. The few existing studies use either the whole training data, or a subset of training data, or some metadata stored during training to update the model weights for unlearning. However, in many cases, no data related to the training process or training samples may be accessible for the unlearning purpose. We therefore ask the question: is it possible to achieve unlearning wit
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#20107;&#23454;&#26680;&#26597;&#25968;&#25454;&#38598;AmbiFC&#65292;&#29992;&#20110;&#22788;&#29702;&#29616;&#23454;&#22330;&#26223;&#20013;&#30340;&#21547;&#31946;&#24615;&#22768;&#26126;&#26680;&#26597;&#38382;&#39064;&#65292;&#36890;&#36807;&#32454;&#31890;&#24230;&#30340;&#35777;&#25454;&#27880;&#37322;&#21644;&#20998;&#26512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#21547;&#31946;&#24615;&#22768;&#26126;&#30340;&#36719;&#26631;&#31614;&#35777;&#25454;&#26680;&#26597;&#26041;&#27861;&#65292;&#24182;&#19988;&#22312;&#27880;&#37322;&#20154;&#21592;&#20105;&#35758;&#20998;&#26512;&#20013;&#21457;&#29616;&#20102;&#30456;&#20851;&#24615;&#12290;</title><link>http://arxiv.org/abs/2104.00640</link><description>&lt;p&gt;
AmbiFC: &#29992;&#35777;&#25454;&#26816;&#39564;&#21547;&#31946;&#24615;&#22768;&#26126;&#30340;&#30495;&#23454;&#24615;
&lt;/p&gt;
&lt;p&gt;
AmbiFC: Fact-Checking Ambiguous Claims with Evidence. (arXiv:2104.00640v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2104.00640
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#20107;&#23454;&#26680;&#26597;&#25968;&#25454;&#38598;AmbiFC&#65292;&#29992;&#20110;&#22788;&#29702;&#29616;&#23454;&#22330;&#26223;&#20013;&#30340;&#21547;&#31946;&#24615;&#22768;&#26126;&#26680;&#26597;&#38382;&#39064;&#65292;&#36890;&#36807;&#32454;&#31890;&#24230;&#30340;&#35777;&#25454;&#27880;&#37322;&#21644;&#20998;&#26512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#21547;&#31946;&#24615;&#22768;&#26126;&#30340;&#36719;&#26631;&#31614;&#35777;&#25454;&#26680;&#26597;&#26041;&#27861;&#65292;&#24182;&#19988;&#22312;&#27880;&#37322;&#20154;&#21592;&#20105;&#35758;&#20998;&#26512;&#20013;&#21457;&#29616;&#20102;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#65292;&#33258;&#21160;&#21270;&#20107;&#23454;&#26680;&#26597;&#31995;&#32479;&#24517;&#39035;&#23558;&#22768;&#26126;&#19982;&#26816;&#32034;&#21040;&#30340;&#35777;&#25454;&#36827;&#34892;&#27604;&#36739;&#20197;&#39044;&#27979;&#30495;&#23454;&#24615;&#12290;&#26816;&#32034;&#21040;&#30340;&#35777;&#25454;&#21487;&#33021;&#26080;&#27861;&#26126;&#30830;&#25903;&#25345;&#25110;&#21453;&#39539;&#22768;&#26126;&#65292;&#24182;&#20135;&#29983;&#21508;&#31181;&#26377;&#25928;&#35299;&#37322;&#12290;&#29616;&#26377;&#30340;&#20107;&#23454;&#26680;&#26597;&#25968;&#25454;&#38598;&#38656;&#35201;&#27169;&#22411;&#20026;&#27599;&#20010;&#22768;&#26126;&#39044;&#27979;&#21333;&#20010;&#30495;&#23454;&#24615;&#26631;&#31614;&#65292;&#24182;&#19988;&#32570;&#20047;&#31649;&#29702;&#27492;&#31867;&#27169;&#31946;&#24615;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#20107;&#23454;&#26680;&#26597;&#25968;&#25454;&#38598;AmbiFC&#65292;&#20854;&#20013;&#21253;&#21547;&#20174;&#23436;&#25972;&#32500;&#22522;&#30334;&#31185;&#39029;&#38754;&#20013;&#33719;&#21462;&#30340;&#32463;&#36807;&#32454;&#31890;&#24230;&#35777;&#25454;&#27880;&#37322;&#30340;&#20449;&#24687;&#38656;&#27714;&#30340;&#29616;&#23454;&#22768;&#26126;&#12290;&#25105;&#20204;&#24443;&#24213;&#20998;&#26512;&#20102;AmbiFC&#20013;&#28041;&#21450;&#21547;&#31946;&#22768;&#26126;&#24341;&#36215;&#30340;&#20105;&#35758;&#65292;&#35266;&#23519;&#21040;&#19982;&#27880;&#37322;&#20154;&#21592;&#30340;&#33258;&#25105;&#35780;&#20272;&#21644;&#19987;&#23478;&#27880;&#37322;&#30340;&#35821;&#35328;&#29616;&#35937;&#24378;&#28872;&#30456;&#20851;&#30340;&#27880;&#37322;&#20154;&#21592;&#20105;&#35758;&#12290;&#25105;&#20204;&#24341;&#20837;&#22522;&#20110;&#35777;&#25454;&#30340;&#21547;&#31946;&#22768;&#26126;&#30340;&#30495;&#23454;&#24615;&#26680;&#26597;&#20219;&#21153;&#65292;&#27604;&#36739;&#20102;&#19977;&#31181;&#26041;&#27861;&#65292;&#20854;&#20013;&#21253;&#21547;&#27880;&#37322;&#20449;&#21495;&#21644;&#21333;&#26631;&#31614;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automated fact-checking systems in real-world scenarios must compare claims with retrieved evidence to predict the veracity. The retrieved evidence may not unambiguously support or refute the claim and yield diverse valid interpretations. Existing fact-checking datasets necessitate that models predict a single veracity label for each claim and lack the ability to manage such ambiguity. We present AmbiFC, a large-scale fact-checking dataset with realistic claims derived from real-world information needs. Our dataset contains fine-grained evidence annotations of passages from complete Wikipedia pages. We thoroughly analyze disagreements arising from ambiguous claims in AmbiFC, observing a strong correlation of annotator disagreement with their self-assessment and expert-annotated linguistic phenomena. We introduce the task of evidence-based fact-checking for ambiguous claims with soft labels, and compare three methodologies incorporating annotation signals with a single-label classificat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#33539;&#24335;&#65292;&#21487;&#20197;&#35774;&#35745;&#21487;&#34892;&#30340;&#36136;&#24515;&#21644;&#36523;&#20307;&#36712;&#36857;&#65292;&#21516;&#26102;&#28385;&#36275;&#21160;&#24577;&#24179;&#34913;&#12289;&#20851;&#33410;&#25197;&#30697;&#21644;&#36816;&#21160;&#38480;&#21046;&#12290;&#22312;&#27169;&#25311;&#23454;&#39564;&#20013;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2011.07967</link><description>&lt;p&gt;
&#19968;&#31181;&#39640;&#25928;&#30340;&#33151;&#24335;&#26426;&#22120;&#20154;&#21487;&#34892;&#24615;&#20445;&#35777;&#33539;&#24335;
&lt;/p&gt;
&lt;p&gt;
An Efficient Paradigm for Feasibility Guarantees in Legged Locomotion. (arXiv:2011.07967v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2011.07967
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#33539;&#24335;&#65292;&#21487;&#20197;&#35774;&#35745;&#21487;&#34892;&#30340;&#36136;&#24515;&#21644;&#36523;&#20307;&#36712;&#36857;&#65292;&#21516;&#26102;&#28385;&#36275;&#21160;&#24577;&#24179;&#34913;&#12289;&#20851;&#33410;&#25197;&#30697;&#21644;&#36816;&#21160;&#38480;&#21046;&#12290;&#22312;&#27169;&#25311;&#23454;&#39564;&#20013;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20219;&#24847;&#22320;&#24418;&#19978;&#20026;&#33151;&#24335;&#26426;&#22120;&#20154;&#35774;&#35745;&#21487;&#34892;&#30340;&#36523;&#20307;&#36712;&#36857;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33539;&#24335;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#35774;&#35745;&#21487;&#34892;&#30340;&#36136;&#24515;&#21644;&#36523;&#20307;&#36712;&#36857;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#21487;&#34892;&#21306;&#22495;&#30340;&#19968;&#33324;&#20844;&#24335;&#65292;&#21487;&#20197;&#22312;&#20445;&#35777;&#21160;&#24577;&#24179;&#34913;&#30340;&#21516;&#26102;&#65292;&#39640;&#25928;&#22320;&#28385;&#36275;&#20851;&#33410;&#25197;&#30697;&#21644;&#36816;&#21160;&#38480;&#21046;&#12290;&#20026;&#20102;&#32771;&#34385;&#36816;&#21160;&#38480;&#21046;&#30340;&#21487;&#34892;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#35745;&#31639;&#36136;&#24515;&#21487;&#36798;&#21306;&#22495;&#30340;&#31639;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#35268;&#21010;&#31574;&#30053;&#65292;&#21033;&#29992;&#25913;&#36827;&#30340;&#21487;&#34892;&#21306;&#22495;&#26469;&#35774;&#35745;&#21487;&#34892;&#30340;&#36136;&#24515;&#21644;&#36523;&#20307;&#26041;&#21521;&#36712;&#36857;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#22312;&#19968;&#20010;&#27169;&#25311;&#30340;&#22235;&#36275;&#26426;&#22120;&#20154;&#19978;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#33021;&#21147;&#65292;&#20351;&#20854;&#33021;&#22815;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22320;&#24418;&#20013;&#25191;&#34892;&#21508;&#31181;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Developing feasible body trajectories for legged systems on arbitrary terrains is a challenging task. In this paper, we present a paradigm that allows to design feasible Center of Mass (CoM) and body trajectories in an efficient manner. In our previous work [1], we introduced the notion of the 2D feasible region, where static balance and the satisfaction of joint torque limits were guaranteed, whenever the projection of the CoM lied inside the proposed admissible region. In this work we propose a general formulation of the improved feasible region that guarantees dynamic balance alongside the satisfaction of both joint-torque and kinematic limits in an efficient manner. To incorporate the feasibility of the kinematic limits, we introduce an algorithm that computes the reachable region of the CoM. Furthermore, we propose an efficient planning strategy that utilizes the improved feasible region to design feasible CoM and body orientation trajectories. Finally, we validate the capabilitie
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26368;&#22823;&#21270;&#29366;&#24577;&#21644;&#21160;&#20316;&#36873;&#39033;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#26469;&#23398;&#20064;&#36873;&#39033;&#30340;&#32456;&#27490;&#26465;&#20214;&#30340;&#26041;&#27861;&#65292;&#20174;&#32780;&#25552;&#39640;&#23398;&#20064;&#21040;&#30340;&#36873;&#39033;&#30340;&#22810;&#26679;&#24615;&#21644;&#21487;&#37325;&#29992;&#24615;&#65292;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2010.02756</link><description>&lt;p&gt;
&#36890;&#36807;&#20449;&#24687;&#26368;&#22823;&#21270;&#30340;&#32456;&#27490;&#35780;&#20272;&#23398;&#20064;&#22810;&#26679;&#21270;&#36873;&#39033;
&lt;/p&gt;
&lt;p&gt;
Learning Diverse Options via InfoMax Termination Critic. (arXiv:2010.02756v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2010.02756
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26368;&#22823;&#21270;&#29366;&#24577;&#21644;&#21160;&#20316;&#36873;&#39033;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#26469;&#23398;&#20064;&#36873;&#39033;&#30340;&#32456;&#27490;&#26465;&#20214;&#30340;&#26041;&#27861;&#65292;&#20174;&#32780;&#25552;&#39640;&#23398;&#20064;&#21040;&#30340;&#36873;&#39033;&#30340;&#22810;&#26679;&#24615;&#21644;&#21487;&#37325;&#29992;&#24615;&#65292;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#33258;&#21160;&#23398;&#20064;&#21487;&#37325;&#29992;&#30340;&#25345;&#32493;&#24615;&#21160;&#20316;&#36873;&#39033;&#30340;&#38382;&#39064;&#12290;&#34429;&#28982;&#36873;&#39033;&#21487;&#20197;&#20316;&#20026;&#21487;&#37325;&#29992;&#30340;&#26500;&#24314;&#27169;&#22359;&#26469;&#21152;&#36895;&#36801;&#31227;&#23398;&#20064;&#65292;&#20294;&#26159;&#20026;&#26410;&#30693;&#20219;&#21153;&#20998;&#24067;&#23398;&#20064;&#21487;&#37325;&#29992;&#36873;&#39033;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#21463;&#21040;&#22522;&#20110;&#20114;&#20449;&#24687;&#25216;&#26415;&#30340;&#25216;&#33021;&#23398;&#20064;&#30340;&#21551;&#21457;&#65292;&#25552;&#20986;&#36890;&#36807;&#26368;&#22823;&#21270;&#36873;&#39033;&#21644;&#23545;&#24212;&#29366;&#24577;&#36716;&#25442;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#26469;&#23398;&#20064;&#36873;&#39033;&#30340;&#32456;&#27490;&#26465;&#20214;&#12290;&#25105;&#20204;&#36890;&#36807;&#26799;&#24230;&#19978;&#21319;&#23548;&#20986;&#20102;&#36825;&#31181;&#20114;&#20449;&#24687;&#26368;&#22823;&#21270;&#30340;&#21487;&#25193;&#23637;&#36817;&#20284;&#26041;&#27861;&#65292;&#24182;&#31216;&#20043;&#20026;InfoMax Termination Critic&#65288;IMTC&#65289;&#31639;&#27861;&#12290;&#26412;&#25991;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;IMTC&#26174;&#33879;&#25552;&#39640;&#20102;&#23398;&#20064;&#36873;&#39033;&#30340;&#22810;&#26679;&#24615;&#65292;&#19988;&#19981;&#38656;&#35201;&#22806;&#37096;&#22870;&#21169;&#65292;&#20165;&#32467;&#21512;&#20869;&#22312;&#30340;&#36873;&#39033;&#23398;&#20064;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;&#23398;&#20064;&#21040;&#30340;&#36873;&#39033;&#36716;&#31227;&#21040;&#21508;&#31181;&#20219;&#21153;&#20013;&#27979;&#35797;&#20854;&#21487;&#37325;&#29992;&#24615;&#65292;&#35777;&#23454;&#20102;&#20854;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of autonomously learning reusable temporally extended actions, or options, in reinforcement learning. While options can speed up transfer learning by serving as reusable building blocks, learning reusable options for unknown task distribution remains challenging. Motivated by the recent success of mutual information (MI) based skill learning, we hypothesize that more diverse options are more reusable. To this end, we propose a method for learning termination conditions of options by maximizing MI between options and corresponding state transitions. We derive a scalable approximation of this MI maximization via gradient ascent, yielding the InfoMax Termination Critic (IMTC) algorithm. Our experiments demonstrate that IMTC significantly improves the diversity of learned options without extrinsic rewards combined with an intrinsic option learning method. Moreover, we test the reusability of learned options by transferring options into various tasks, confirming that
&lt;/p&gt;</description></item></channel></rss>