<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>LLMs&#24050;&#25104;&#20026;&#29983;&#29289;&#21307;&#23398;&#19982;&#20581;&#24247;&#20449;&#24687;&#23398;&#20013;&#37325;&#35201;&#30340;&#24037;&#20855;&#65292;&#26412;&#25991;&#29486;&#35745;&#37327;&#23398;&#32508;&#36848;&#20840;&#38754;&#23637;&#31034;&#20102;LLMs&#22312;&#21508;&#31181;BHI&#39046;&#22495;&#20013;&#30340;&#24212;&#29992;&#65292;&#25552;&#20986;&#20102;&#20854;&#23545;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24212;&#29992;&#30340;&#25913;&#36827;&#65292;&#25581;&#31034;&#20102;&#20027;&#35201;&#21457;&#23637;&#36235;&#21183;&#21644;&#30740;&#31350;&#32593;&#32476;&#65292;&#24182;&#35752;&#35770;&#20102;&#20262;&#29702;&#20851;&#20999;&#21644;&#23454;&#38469;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.16303</link><description>&lt;p&gt;
&#29983;&#29289;&#21307;&#23398;&#19982;&#20581;&#24247;&#20449;&#24687;&#23398;&#20013;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65306;&#19968;&#39033;&#25991;&#29486;&#35745;&#37327;&#23398;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Large Language Models in Biomedical and Health Informatics: A Bibliometric Review
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16303
&lt;/p&gt;
&lt;p&gt;
LLMs&#24050;&#25104;&#20026;&#29983;&#29289;&#21307;&#23398;&#19982;&#20581;&#24247;&#20449;&#24687;&#23398;&#20013;&#37325;&#35201;&#30340;&#24037;&#20855;&#65292;&#26412;&#25991;&#29486;&#35745;&#37327;&#23398;&#32508;&#36848;&#20840;&#38754;&#23637;&#31034;&#20102;LLMs&#22312;&#21508;&#31181;BHI&#39046;&#22495;&#20013;&#30340;&#24212;&#29992;&#65292;&#25552;&#20986;&#20102;&#20854;&#23545;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24212;&#29992;&#30340;&#25913;&#36827;&#65292;&#25581;&#31034;&#20102;&#20027;&#35201;&#21457;&#23637;&#36235;&#21183;&#21644;&#30740;&#31350;&#32593;&#32476;&#65292;&#24182;&#35752;&#35770;&#20102;&#20262;&#29702;&#20851;&#20999;&#21644;&#23454;&#38469;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36805;&#36895;&#25104;&#20026;&#29983;&#29289;&#21307;&#23398;&#19982;&#20581;&#24247;&#20449;&#24687;&#23398;&#65288;BHI&#65289;&#20013;&#30340;&#37325;&#35201;&#24037;&#20855;&#65292;&#20026;&#20998;&#26512;&#25968;&#25454;&#12289;&#27835;&#30103;&#24739;&#32773;&#21644;&#24320;&#23637;&#30740;&#31350;&#25552;&#20379;&#20102;&#26032;&#30340;&#26041;&#24335;&#12290;&#26412;&#25991;&#29486;&#35745;&#37327;&#23398;&#32508;&#36848;&#26088;&#22312;&#36890;&#36807;&#26816;&#26597;&#33258;2022&#24180;&#33267;2023&#24180;&#30340;&#30740;&#31350;&#25991;&#31456;&#21644;&#21512;&#20316;&#32593;&#32476;&#65292;&#20840;&#38754;&#23637;&#31034;LLMs&#22312;BHI&#20013;&#30340;&#24212;&#29992;&#24773;&#20917;&#12290;&#23427;&#36827;&#19968;&#27493;&#25506;&#35752;&#20102;LLMs&#22914;&#20309;&#21487;&#20197;&#25913;&#36827;&#21508;&#31181;BHI&#39046;&#22495;&#20013;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#24212;&#29992;&#65292;&#22914;&#21307;&#23398;&#35786;&#26029;&#12289;&#24739;&#32773;&#21442;&#19982;&#12289;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#31649;&#29702;&#21644;&#20010;&#24615;&#21270;&#21307;&#23398;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#30340;&#25991;&#29486;&#35745;&#37327;&#23398;&#32508;&#36848;&#30830;&#23450;&#20102;&#20851;&#38190;&#36235;&#21183;&#65292;&#32472;&#21046;&#20102;&#30740;&#31350;&#32593;&#32476;&#65292;&#24182;&#31361;&#20986;&#20102;&#36825;&#20010;&#24555;&#36895;&#21457;&#23637;&#39046;&#22495;&#30340;&#20027;&#35201;&#36827;&#23637;&#12290;&#26368;&#21518;&#65292;&#23427;&#35752;&#35770;&#20102;&#22312;BHI&#20013;&#20351;&#29992;LLMs&#30340;&#20262;&#29702;&#20851;&#20999;&#21644;&#23454;&#38469;&#25361;&#25112;&#65292;&#22914;&#25968;&#25454;&#38544;&#31169;&#21644;&#21487;&#38752;&#30340;&#21307;&#30103;&#24314;&#35758;&#12290;&#23637;&#26395;&#26410;&#26469;&#65292;&#25105;&#20204;&#32771;&#34385;LLMs&#22914;&#20309;&#36827;&#19968;&#27493;&#25913;&#21464;&#29983;&#29289;&#21307;&#23398;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16303v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have rapidly become important tools in Biomedical and Health Informatics (BHI), enabling new ways to analyze data, treat patients, and conduct research. This bibliometric review aims to provide a panoramic view of how LLMs have been used in BHI by examining research articles and collaboration networks from 2022 to 2023. It further explores how LLMs can improve Natural Language Processing (NLP) applications in various BHI areas like medical diagnosis, patient engagement, electronic health record management, and personalized medicine. To do this, our bibliometric review identifies key trends, maps out research networks, and highlights major developments in this fast-moving field. Lastly, it discusses the ethical concerns and practical challenges of using LLMs in BHI, such as data privacy and reliable medical recommendations. Looking ahead, we consider how LLMs could further transform biomedical research as we
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#20855;&#26377;&#23398;&#20064;&#35889;&#26680;&#30340;&#28151;&#21512;&#39640;&#26031;&#36807;&#31243;&#30340;&#28508;&#21464;&#37327;&#27169;&#22411;&#26041;&#27861;&#65292;&#38024;&#23545;&#22024;&#26434;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#36827;&#34892;&#40065;&#26834;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2402.14081</link><description>&lt;p&gt;
&#20351;&#29992;&#20855;&#26377;&#36816;&#21160;&#20195;&#30721;&#30340;&#38543;&#26426;&#36807;&#31243;&#27169;&#22411;&#23545;&#22024;&#26434;&#26102;&#38388;&#24207;&#21015;&#38598;&#21512;&#36827;&#34892;&#40065;&#26834;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Robust Learning of Noisy Time Series Collections Using Stochastic Process Models with Motion Codes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14081
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#20855;&#26377;&#23398;&#20064;&#35889;&#26680;&#30340;&#28151;&#21512;&#39640;&#26031;&#36807;&#31243;&#30340;&#28508;&#21464;&#37327;&#27169;&#22411;&#26041;&#27861;&#65292;&#38024;&#23545;&#22024;&#26434;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#36827;&#34892;&#40065;&#26834;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#21644;&#39044;&#27979;&#38382;&#39064;&#24050;&#32463;&#24471;&#21040;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#20855;&#26377;&#20219;&#24847;&#26102;&#38388;&#24207;&#21015;&#38271;&#24230;&#30340;&#22024;&#26434;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#24773;&#20917;&#20173;&#20855;&#25361;&#25112;&#24615;&#12290;&#27599;&#20010;&#26102;&#38388;&#24207;&#21015;&#23454;&#20363;&#21487;&#20197;&#30475;&#20316;&#26159;&#22024;&#26434;&#21160;&#24577;&#27169;&#22411;&#30340;&#19968;&#20010;&#26679;&#26412;&#23454;&#29616;&#65292;&#20854;&#29305;&#28857;&#26159;&#36830;&#32493;&#38543;&#26426;&#36807;&#31243;&#12290;&#23545;&#20110;&#35768;&#22810;&#24212;&#29992;&#65292;&#25968;&#25454;&#26159;&#28151;&#21512;&#30340;&#65292;&#30001;&#22810;&#20010;&#38543;&#26426;&#36807;&#31243;&#24314;&#27169;&#30340;&#20960;&#31181;&#31867;&#22411;&#30340;&#22024;&#26434;&#26102;&#38388;&#24207;&#21015;&#24207;&#21015;&#32452;&#25104;&#65292;&#20351;&#24471;&#39044;&#27979;&#21644;&#20998;&#31867;&#20219;&#21153;&#21464;&#24471;&#26356;&#20855;&#25361;&#25112;&#24615;&#12290;&#25105;&#20204;&#19981;&#26159;&#31616;&#21333;&#22320;&#23558;&#25968;&#25454;&#22238;&#24402;&#21040;&#27599;&#31181;&#26102;&#38388;&#24207;&#21015;&#31867;&#22411;&#65292;&#32780;&#26159;&#37319;&#29992;&#20855;&#26377;&#23398;&#20064;&#35889;&#26680;&#30340;&#28151;&#21512;&#39640;&#26031;&#36807;&#31243;&#30340;&#28508;&#21464;&#37327;&#27169;&#22411;&#26041;&#27861;&#12290;&#26356;&#20855;&#20307;&#22320;&#35828;&#65292;&#25105;&#20204;&#20026;&#27599;&#31181;&#31867;&#22411;&#30340;&#22024;&#26434;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#33258;&#21160;&#20998;&#37197;&#19968;&#20010;&#31216;&#20026;&#20854;&#36816;&#21160;&#20195;&#30721;&#30340;&#31614;&#21517;&#21521;&#37327;&#12290;&#28982;&#21518;&#65292;&#22312;&#27599;&#20010;&#20998;&#37197;&#30340;&#36816;&#21160;&#20195;&#30721;&#30340;&#26465;&#20214;&#19979;&#65292;&#25105;&#20204;&#25512;&#26029;&#20986;&#30456;&#20851;&#24615;&#30340;&#31232;&#30095;&#36817;&#20284;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14081v1 Announce Type: cross  Abstract: While time series classification and forecasting problems have been extensively studied, the cases of noisy time series data with arbitrary time sequence lengths have remained challenging. Each time series instance can be thought of as a sample realization of a noisy dynamical model, which is characterized by a continuous stochastic process. For many applications, the data are mixed and consist of several types of noisy time series sequences modeled by multiple stochastic processes, making the forecasting and classification tasks even more challenging. Instead of regressing data naively and individually to each time series type, we take a latent variable model approach using a mixtured Gaussian processes with learned spectral kernels. More specifically, we auto-assign each type of noisy time series data a signature vector called its motion code. Then, conditioned on each assigned motion code, we infer a sparse approximation of the corr
&lt;/p&gt;</description></item><item><title>MuChin&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#22312;&#38899;&#20048;&#29702;&#35299;&#21644;&#25551;&#36848;&#26041;&#38754;&#24615;&#33021;&#30340;&#20013;&#25991;&#21475;&#35821;&#25551;&#36848;&#22522;&#20934;&#12290;</title><link>https://arxiv.org/abs/2402.09871</link><description>&lt;p&gt;
MuChin&#65306;&#29992;&#20110;&#35780;&#20272;&#38899;&#20048;&#39046;&#22495;&#20013;&#35821;&#35328;&#27169;&#22411;&#30340;&#20013;&#25991;&#21475;&#35821;&#25551;&#36848;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
MuChin: A Chinese Colloquial Description Benchmark for Evaluating Language Models in the Field of Music
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09871
&lt;/p&gt;
&lt;p&gt;
MuChin&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#22312;&#38899;&#20048;&#29702;&#35299;&#21644;&#25551;&#36848;&#26041;&#38754;&#24615;&#33021;&#30340;&#20013;&#25991;&#21475;&#35821;&#25551;&#36848;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24555;&#36895;&#21457;&#23637;&#30340;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36843;&#20999;&#38656;&#35201;&#26032;&#30340;&#22522;&#20934;&#26469;&#32479;&#19968;&#35780;&#20272;&#23427;&#20204;&#22312;&#29702;&#35299;&#21644;&#20197;&#25991;&#23383;&#25551;&#36848;&#38899;&#20048;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#38899;&#20048;&#20449;&#24687;&#26816;&#32034;&#65288;MIR&#65289;&#31639;&#27861;&#19982;&#20154;&#31867;&#29702;&#35299;&#20043;&#38388;&#30340;&#35821;&#20041;&#24046;&#36317;&#65292;&#19987;&#19994;&#20154;&#22763;&#21644;&#20844;&#20247;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#20197;&#21450;&#27880;&#37322;&#30340;&#20302;&#31934;&#24230;&#65292;&#29616;&#26377;&#30340;&#38899;&#20048;&#25551;&#36848;&#25968;&#25454;&#38598;&#26080;&#27861;&#20316;&#20026;&#22522;&#20934;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MuChin&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#29992;&#20013;&#25991;&#21475;&#35821;&#25551;&#36848;&#30340;&#24320;&#28304;&#38899;&#20048;&#25551;&#36848;&#22522;&#20934;&#65292;&#26088;&#22312;&#35780;&#20272;&#22810;&#27169;&#24577;LLMs&#22312;&#29702;&#35299;&#21644;&#25551;&#36848;&#38899;&#20048;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#37319;&#34411;&#38899;&#20048;&#27880;&#37322;&#24179;&#21488;&#65288;CaiMAP&#65289;&#65292;&#37319;&#29992;&#21019;&#26032;&#30340;&#22810;&#20154;&#12289;&#22810;&#38454;&#27573;&#20445;&#35777;&#26041;&#27861;&#65292;&#24182;&#25307;&#21215;&#20102;&#19994;&#20313;&#29233;&#22909;&#32773;&#21644;&#19987;&#19994;&#20154;&#22763;&#65292;&#20197;&#30830;&#20445;&#27880;&#37322;&#30340;&#31934;&#24230;&#21644;&#19982;&#27969;&#34892;&#35821;&#20041;&#30340;&#23545;&#40784;&#12290;&#21033;&#29992;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09871v1 Announce Type: cross  Abstract: The rapidly evolving multimodal Large Language Models (LLMs) urgently require new benchmarks to uniformly evaluate their performance on understanding and textually describing music. However, due to semantic gaps between Music Information Retrieval (MIR) algorithms and human understanding, discrepancies between professionals and the public, and low precision of annotations, existing music description datasets cannot serve as benchmarks. To this end, we present MuChin, the first open-source music description benchmark in Chinese colloquial language, designed to evaluate the performance of multimodal LLMs in understanding and describing music. We established the Caichong Music Annotation Platform (CaiMAP) that employs an innovative multi-person, multi-stage assurance method, and recruited both amateurs and professionals to ensure the precision of annotations and alignment with popular semantics. Utilizing this method, we built a dataset w
&lt;/p&gt;</description></item><item><title>&#26412;&#35843;&#26597;&#25991;&#31456;&#27010;&#36848;&#20102;&#22312;&#25552;&#39640;LLM&#25512;&#29702;&#25928;&#26524;&#26041;&#38754;&#30340;&#26368;&#26032;&#26041;&#27861;&#21644;&#36827;&#23637;&#65292;&#36890;&#36807;&#23454;&#39564;&#35780;&#20272;&#19981;&#21516;&#21387;&#32553;&#25216;&#26415;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#25552;&#20986;&#25913;&#36827;LLM&#25512;&#29702;&#25928;&#29575;&#30340;&#28508;&#22312;&#26410;&#26469;&#26041;&#21521;&#12290;</title><link>https://arxiv.org/abs/2402.01799</link><description>&lt;p&gt;
&#26356;&#24555;&#26356;&#36731;&#30340;LLMs&#65306;&#24403;&#21069;&#25361;&#25112;&#21644;&#26410;&#26469;&#21457;&#23637;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Faster and Lighter LLMs: A Survey on Current Challenges and Way Forward
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01799
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35843;&#26597;&#25991;&#31456;&#27010;&#36848;&#20102;&#22312;&#25552;&#39640;LLM&#25512;&#29702;&#25928;&#26524;&#26041;&#38754;&#30340;&#26368;&#26032;&#26041;&#27861;&#21644;&#36827;&#23637;&#65292;&#36890;&#36807;&#23454;&#39564;&#35780;&#20272;&#19981;&#21516;&#21387;&#32553;&#25216;&#26415;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#25552;&#20986;&#25913;&#36827;LLM&#25512;&#29702;&#25928;&#29575;&#30340;&#28508;&#22312;&#26410;&#26469;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;LLMs&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#30001;&#20110;&#25512;&#29702;&#36807;&#31243;&#20013;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#21644;&#20869;&#23384;&#36164;&#28304;&#65292;&#23427;&#20204;&#30340;&#26222;&#21450;&#38754;&#20020;&#30528;&#25361;&#25112;&#12290;&#26368;&#36817;&#22312;&#27169;&#22411;&#21387;&#32553;&#21644;&#31995;&#32479;&#32423;&#20248;&#21270;&#26041;&#27861;&#26041;&#38754;&#30340;&#36827;&#23637;&#26088;&#22312;&#22686;&#24378;LLM&#25512;&#29702;&#25928;&#26524;&#12290;&#26412;&#35843;&#26597;&#25552;&#20379;&#20102;&#36825;&#20123;&#26041;&#27861;&#30340;&#27010;&#36848;&#65292;&#24378;&#35843;&#20102;&#26368;&#36817;&#30340;&#21457;&#23637;&#12290;&#36890;&#36807;&#23545;LLaMA(/2)-7B&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#21508;&#31181;&#21387;&#32553;&#25216;&#26415;&#65292;&#20026;&#22312;&#32479;&#19968;&#29615;&#22659;&#20013;&#39640;&#25928;&#37096;&#32626;LLM&#25552;&#20379;&#20102;&#23454;&#36341;&#35265;&#35299;&#12290;&#23545;LLaMA(/2)-7B&#30340;&#23454;&#35777;&#20998;&#26512;&#31361;&#20986;&#20102;&#36825;&#20123;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#22522;&#20110;&#35843;&#26597;&#32467;&#26524;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#24403;&#21069;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#35752;&#35770;&#20102;&#25913;&#21892;LLM&#25512;&#29702;&#25928;&#29575;&#30340;&#28508;&#22312;&#26410;&#26469;&#26041;&#21521;&#12290;&#25105;&#20204;&#22312;https://github.com/nyunAI/Faster-LLM-Survey&#21457;&#24067;&#20102;&#29992;&#20110;&#22797;&#29616;&#26412;&#25991;&#32467;&#26524;&#30340;&#20195;&#30721;&#24211;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the impressive performance of LLMs, their widespread adoption faces challenges due to substantial computational and memory requirements during inference. Recent advancements in model compression and system-level optimization methods aim to enhance LLM inference. This survey offers an overview of these methods, emphasizing recent developments. Through experiments on LLaMA(/2)-7B, we evaluate various compression techniques, providing practical insights for efficient LLM deployment in a unified setting. The empirical analysis on LLaMA(/2)-7B highlights the effectiveness of these methods. Drawing from survey insights, we identify current limitations and discuss potential future directions to improve LLM inference efficiency. We release the codebase to reproduce the results presented in this paper at https://github.com/nyunAI/Faster-LLM-Survey
&lt;/p&gt;</description></item><item><title>LLM&#29983;&#25104;&#30340;&#34394;&#20551;&#20449;&#24687;&#21487;&#33021;&#27604;&#20154;&#31867;&#25776;&#20889;&#30340;&#34394;&#20551;&#20449;&#24687;&#26356;&#38590;&#20197;&#26816;&#27979;&#65292;&#20855;&#26377;&#26356;&#20855;&#27450;&#39575;&#24615;&#30340;&#39118;&#26684;&#65292;&#21487;&#33021;&#36896;&#25104;&#26356;&#22810;&#21361;&#23475;&#12290;</title><link>https://arxiv.org/abs/2309.13788</link><description>&lt;p&gt;
&#33021;&#22815;&#26816;&#27979;&#21040;LLM&#29983;&#25104;&#30340;&#34394;&#20551;&#20449;&#24687;&#21527;?
&lt;/p&gt;
&lt;p&gt;
Can LLM-Generated Misinformation Be Detected?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2309.13788
&lt;/p&gt;
&lt;p&gt;
LLM&#29983;&#25104;&#30340;&#34394;&#20551;&#20449;&#24687;&#21487;&#33021;&#27604;&#20154;&#31867;&#25776;&#20889;&#30340;&#34394;&#20551;&#20449;&#24687;&#26356;&#38590;&#20197;&#26816;&#27979;&#65292;&#20855;&#26377;&#26356;&#20855;&#27450;&#39575;&#24615;&#30340;&#39118;&#26684;&#65292;&#21487;&#33021;&#36896;&#25104;&#26356;&#22810;&#21361;&#23475;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#20135;&#29983;&#20102;&#28145;&#36828;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;LLMs&#65288;&#22914;ChatGPT&#65289;&#21487;&#33021;&#34987;&#21033;&#29992;&#26469;&#29983;&#25104;&#34394;&#20551;&#20449;&#24687;&#65292;&#36825;&#32473;&#22312;&#32447;&#23433;&#20840;&#21644;&#20844;&#20247;&#20449;&#20219;&#24102;&#26469;&#20102;&#20005;&#37325;&#20851;&#20999;&#12290;&#19968;&#20010;&#22522;&#26412;&#30340;&#30740;&#31350;&#38382;&#39064;&#26159;&#65306;LLM&#29983;&#25104;&#30340;&#34394;&#20551;&#20449;&#24687;&#26159;&#21542;&#20250;&#27604;&#20154;&#31867;&#25776;&#20889;&#30340;&#34394;&#20551;&#20449;&#24687;&#36896;&#25104;&#26356;&#22823;&#21361;&#23475;?&#25105;&#20204;&#25552;&#20986;&#20174;&#26816;&#27979;&#38590;&#24230;&#30340;&#35282;&#24230;&#26469;&#25506;&#35752;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#39318;&#20808;&#24314;&#31435;&#20102;&#19968;&#20010;LLM&#29983;&#25104;&#30340;&#34394;&#20551;&#20449;&#24687;&#20998;&#31867;&#27861;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23545;&#21033;&#29992;LLMs&#29983;&#25104;&#34394;&#20551;&#20449;&#24687;&#30340;&#28508;&#22312;&#30495;&#23454;&#19990;&#30028;&#26041;&#27861;&#36827;&#34892;&#20998;&#31867;&#21644;&#39564;&#35777;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#35777;&#35843;&#26597;&#65292;&#25105;&#20204;&#21457;&#29616;&#19982;&#20855;&#26377;&#30456;&#21516;&#35821;&#20041;&#30340;&#20154;&#31867;&#25776;&#20889;&#30340;&#34394;&#20551;&#20449;&#24687;&#30456;&#27604;&#65292;LLM&#29983;&#25104;&#30340;&#34394;&#20551;&#20449;&#24687;&#23545;&#20154;&#31867;&#21644;&#26816;&#27979;&#22120;&#26469;&#35828;&#26356;&#38590;&#26816;&#27979;&#65292;&#36825;&#34920;&#26126;&#23427;&#21487;&#33021;&#20855;&#26377;&#26356;&#20855;&#27450;&#39575;&#24615;&#30340;&#39118;&#26684;&#65292;&#28508;&#22312;&#22320;&#36896;&#25104;&#26356;&#22810;&#21361;&#23475;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#25105;&#20204;&#21457;&#29616;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2309.13788v3 Announce Type: replace-cross  Abstract: The advent of Large Language Models (LLMs) has made a transformative impact. However, the potential that LLMs such as ChatGPT can be exploited to generate misinformation has posed a serious concern to online safety and public trust. A fundamental research question is: will LLM-generated misinformation cause more harm than human-written misinformation? We propose to tackle this question from the perspective of detection difficulty. We first build a taxonomy of LLM-generated misinformation. Then we categorize and validate the potential real-world methods for generating misinformation with LLMs. Then, through extensive empirical investigation, we discover that LLM-generated misinformation can be harder to detect for humans and detectors compared to human-written misinformation with the same semantics, which suggests it can have more deceptive styles and potentially cause more harm. We also discuss the implications of our discovery
&lt;/p&gt;</description></item><item><title>LLMCheckup&#26159;&#19968;&#20010;&#21487;&#35299;&#37322;&#24615;&#24037;&#20855;&#65292;&#36890;&#36807;&#36830;&#25509;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#21487;&#35299;&#37322;&#30340;AI&#24037;&#20855;&#65292;&#20351;&#29992;&#25143;&#33021;&#22815;&#19982;&#27169;&#22411;&#36827;&#34892;&#23545;&#35805;&#65292;&#29983;&#25104;&#33258;&#25105;&#35299;&#37322;&#24182;&#25552;&#20379;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2401.12576</link><description>&lt;p&gt;
LLMCheckup&#65306;&#36890;&#36807;&#21487;&#35299;&#37322;&#24615;&#24037;&#20855;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#23545;&#35805;&#24335;&#26816;&#26597;
&lt;/p&gt;
&lt;p&gt;
LLMCheckup: Conversational Examination of Large Language Models via Interpretability Tools. (arXiv:2401.12576v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12576
&lt;/p&gt;
&lt;p&gt;
LLMCheckup&#26159;&#19968;&#20010;&#21487;&#35299;&#37322;&#24615;&#24037;&#20855;&#65292;&#36890;&#36807;&#36830;&#25509;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#21487;&#35299;&#37322;&#30340;AI&#24037;&#20855;&#65292;&#20351;&#29992;&#25143;&#33021;&#22815;&#19982;&#27169;&#22411;&#36827;&#34892;&#23545;&#35805;&#65292;&#29983;&#25104;&#33258;&#25105;&#35299;&#37322;&#24182;&#25552;&#20379;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#20379;&#20197;&#23545;&#35805;&#24418;&#24335;&#36827;&#34892;&#35299;&#37322;&#30340;&#21487;&#35299;&#37322;&#24615;&#24037;&#20855;&#24050;&#32463;&#35777;&#26126;&#22312;&#22686;&#24378;&#29992;&#25143;&#29702;&#35299;&#26041;&#38754;&#20855;&#26377;&#25928;&#26524;&#65292;&#22240;&#20026;&#19968;&#27425;&#24615;&#35299;&#37322;&#26377;&#26102;&#26080;&#27861;&#25552;&#20379;&#36275;&#22815;&#30340;&#20449;&#24687;&#32473;&#29992;&#25143;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#22522;&#20110;&#23545;&#35805;&#30340;&#35299;&#37322;&#26041;&#26696;&#38656;&#35201;&#35768;&#22810;&#20381;&#36182;&#39033;&#65292;&#24182;&#19988;&#19981;&#23481;&#26131;&#36716;&#31227;&#21040;&#23427;&#20204;&#26410;&#35774;&#35745;&#30340;&#20219;&#21153;&#19978;&#12290;&#36890;&#36807;LLMCheckup&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#26131;&#20110;&#35775;&#38382;&#30340;&#24037;&#20855;&#65292;&#20801;&#35768;&#29992;&#25143;&#19982;&#20219;&#20309;&#26368;&#26032;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36827;&#34892;&#23545;&#35805;&#20197;&#20102;&#35299;&#20854;&#34892;&#20026;&#12290;&#25105;&#20204;&#20351;LLMs&#33021;&#22815;&#33258;&#34892;&#29983;&#25104;&#25152;&#26377;&#35299;&#37322;&#65292;&#24182;&#36890;&#36807;&#19982;&#19968;&#31995;&#21015;&#21487;&#35299;&#37322;&#24615;AI&#65288;XAI&#65289;&#24037;&#20855;&#65288;&#20363;&#22914;&#29305;&#24449;&#24402;&#22240;&#12289;&#22522;&#20110;&#23884;&#20837;&#30340;&#30456;&#20284;&#24615;&#20197;&#21450;&#21453;&#20107;&#23454;&#21644;&#22522;&#20110;&#29702;&#30001;&#29983;&#25104;&#30340;&#25552;&#31034;&#31574;&#30053;&#65289;&#36830;&#25509;&#65292;&#20197;&#23436;&#25104;&#24847;&#22270;&#35782;&#21035;&#32780;&#26080;&#38656;&#24494;&#35843;&#12290;LLM&#65288;&#33258;&#25105;&#65289;&#35299;&#37322;&#20197;&#20132;&#20114;&#23545;&#35805;&#30340;&#24418;&#24335;&#21576;&#29616;&#65292;&#25903;&#25345;&#21518;&#32493;&#38382;&#39064;&#21644;&#29983;&#25104;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
Interpretability tools that offer explanations in the form of a dialogue have demonstrated their efficacy in enhancing users' understanding, as one-off explanations may occasionally fall short in providing sufficient information to the user. Current solutions for dialogue-based explanations, however, require many dependencies and are not easily transferable to tasks they were not designed for. With LLMCheckup, we present an easily accessible tool that allows users to chat with any state-of-the-art large language model (LLM) about its behavior. We enable LLMs to generate all explanations by themselves and take care of intent recognition without fine-tuning, by connecting them with a broad spectrum of Explainable AI (XAI) tools, e.g. feature attributions, embedding-based similarity, and prompting strategies for counterfactual and rationale generation. LLM (self-)explanations are presented as an interactive dialogue that supports follow-up questions and generates suggestions. LLMCheckup p
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#30740;&#31350;&#20102;&#23384;&#22312;&#35268;&#21017;&#19979;&#22522;&#20110;&#20803;&#32452;&#21024;&#38500;&#35821;&#20041;&#30340;&#19968;&#33268;&#24615;&#26597;&#35810;&#22238;&#31572;&#65292;&#24182;&#30830;&#23450;&#20102;&#19968;&#33324;&#31867;&#21035;&#21644;&#33509;&#24178;&#23376;&#31867;&#21035;&#30340;&#25968;&#25454;&#22797;&#26434;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#36825;&#20123;&#38382;&#39064;&#26159;&#21487;&#22788;&#29702;&#30340;&#65292;&#29978;&#33267;&#21487;&#20197;&#36827;&#34892;&#19968;&#38454;&#37325;&#20889;&#12290;&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#26032;&#30340;&#26597;&#35810;&#37325;&#20889;&#25216;&#26415;&#65292;&#21487;&#29992;&#20110;&#23454;&#29616;&#23481;&#24525;&#19981;&#19968;&#33268;&#24615;&#30340;&#26597;&#35810;&#22238;&#31572;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2401.05743</link><description>&lt;p&gt;
&#22312;&#20803;&#32452;&#21024;&#38500;&#35821;&#20041;&#19979;&#30340;&#23384;&#22312;&#35268;&#21017;&#19968;&#33268;&#24615;&#26597;&#35810;&#22238;&#31572;
&lt;/p&gt;
&lt;p&gt;
Consistent Query Answering for Existential Rules under Tuple-Deletion Semantics. (arXiv:2401.05743v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05743
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#23384;&#22312;&#35268;&#21017;&#19979;&#22522;&#20110;&#20803;&#32452;&#21024;&#38500;&#35821;&#20041;&#30340;&#19968;&#33268;&#24615;&#26597;&#35810;&#22238;&#31572;&#65292;&#24182;&#30830;&#23450;&#20102;&#19968;&#33324;&#31867;&#21035;&#21644;&#33509;&#24178;&#23376;&#31867;&#21035;&#30340;&#25968;&#25454;&#22797;&#26434;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#36825;&#20123;&#38382;&#39064;&#26159;&#21487;&#22788;&#29702;&#30340;&#65292;&#29978;&#33267;&#21487;&#20197;&#36827;&#34892;&#19968;&#38454;&#37325;&#20889;&#12290;&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#26032;&#30340;&#26597;&#35810;&#37325;&#20889;&#25216;&#26415;&#65292;&#21487;&#29992;&#20110;&#23454;&#29616;&#23481;&#24525;&#19981;&#19968;&#33268;&#24615;&#30340;&#26597;&#35810;&#22238;&#31572;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22522;&#20110;&#23384;&#22312;&#35268;&#21017;&#30340;&#30693;&#35782;&#24211;&#30340;&#19968;&#33268;&#24615;&#26597;&#35810;&#22238;&#31572;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#22312;&#20803;&#32452;&#21024;&#38500;&#35821;&#20041;&#19979;&#19968;&#33324;&#31867;&#21035;&#30340;&#22810;&#20041;&#23384;&#22312;&#35268;&#21017;&#20197;&#21450;&#20854;&#33509;&#24178;&#23376;&#31867;&#21035;&#65288;&#26080;&#29615;&#12289;&#32447;&#24615;&#12289;&#23436;&#20840;&#12289;&#20445;&#25252;&#21644;&#31896;&#28382;&#65289;&#30340;&#19968;&#33268;&#24615;&#26597;&#35810;&#22238;&#31572;&#21644;&#20462;&#22797;&#26816;&#26597;&#30340;&#25968;&#25454;&#22797;&#26434;&#24615;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#35782;&#21035;&#20986;&#20102;&#20960;&#31181;&#24773;&#20917;&#65292;&#22312;&#36825;&#20123;&#24773;&#20917;&#19979;&#36825;&#20123;&#38382;&#39064;&#26159;&#21487;&#22788;&#29702;&#30340;&#65292;&#29978;&#33267;&#21487;&#20197;&#36827;&#34892;&#19968;&#38454;&#37325;&#20889;&#65292;&#24182;&#21576;&#29616;&#20102;&#26032;&#30340;&#26597;&#35810;&#37325;&#20889;&#25216;&#26415;&#65292;&#21487;&#20197;&#25104;&#20026;&#23454;&#29992;&#30340;&#23481;&#24525;&#19981;&#19968;&#33268;&#24615;&#26597;&#35810;&#22238;&#31572;&#31995;&#32479;&#30340;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study consistent query answering over knowledge bases expressed by existential rules. Specifically, we establish the data complexity of consistent query answering and repair checking under tuple-deletion semantics for a general class of disjunctive existential rules and for several subclasses thereof (acyclic, linear, full, guarded, and sticky). In particular, we identify several cases in which the above problems are tractable or even first-order rewritable, and present new query rewriting techniques that can be the basis for practical inconsistency-tolerant query answering systems.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36873;&#25321;&#24615;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#21363;&#36873;&#25321;&#24615;&#22810;&#26234;&#33021;&#20307;&#20248;&#20808;&#20307;&#39564;&#20013;&#32487;&#65292;&#20195;&#29702;&#20043;&#38388;&#20849;&#20139;&#26377;&#38480;&#25968;&#37327;&#30340;&#35757;&#32451;&#32463;&#39564;&#12290;&#19982;&#20854;&#20182;&#31639;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#23454;&#29616;&#20102;&#21435;&#20013;&#24515;&#21270;&#35757;&#32451;&#65292;&#24182;&#21462;&#24471;&#20102;&#27604;&#22522;&#20934;&#31639;&#27861;&#21644;&#26368;&#20808;&#36827;&#31639;&#27861;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2311.00865</link><description>&lt;p&gt;
&#36873;&#25321;&#24615;&#20998;&#20139;&#20307;&#39564;&#25552;&#21319;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Selectively Sharing Experiences Improves Multi-Agent Reinforcement Learning. (arXiv:2311.00865v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00865
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36873;&#25321;&#24615;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#21363;&#36873;&#25321;&#24615;&#22810;&#26234;&#33021;&#20307;&#20248;&#20808;&#20307;&#39564;&#20013;&#32487;&#65292;&#20195;&#29702;&#20043;&#38388;&#20849;&#20139;&#26377;&#38480;&#25968;&#37327;&#30340;&#35757;&#32451;&#32463;&#39564;&#12290;&#19982;&#20854;&#20182;&#31639;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#23454;&#29616;&#20102;&#21435;&#20013;&#24515;&#21270;&#35757;&#32451;&#65292;&#24182;&#21462;&#24471;&#20102;&#27604;&#22522;&#20934;&#31639;&#27861;&#21644;&#26368;&#20808;&#36827;&#31639;&#27861;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#21363;&#36873;&#25321;&#24615;&#22810;&#26234;&#33021;&#20307;&#20248;&#20808;&#20307;&#39564;&#20013;&#32487;&#65292;&#20854;&#20013;&#20195;&#29702;&#36890;&#36807;&#20998;&#20139;&#35757;&#32451;&#36807;&#31243;&#20013;&#35266;&#23519;&#21040;&#30340;&#26377;&#38480;&#30340;&#36716;&#25442;&#19982;&#20854;&#20182;&#20195;&#29702;&#20849;&#20139;&#12290;&#20854;&#32972;&#21518;&#30340;&#30452;&#35273;&#26159;&#65292;&#26469;&#33258;&#20854;&#20182;&#20195;&#29702;&#30340;&#23569;&#37327;&#30456;&#20851;&#32463;&#39564;&#21487;&#20197;&#24110;&#21161;&#27599;&#20010;&#20195;&#29702;&#23398;&#20064;&#12290;&#19982;&#35768;&#22810;&#20854;&#20182;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#19981;&#21516;&#65292;&#35813;&#26041;&#27861;&#20801;&#35768;&#22522;&#26412;&#21435;&#20013;&#24515;&#21270;&#30340;&#35757;&#32451;&#65292;&#21482;&#38656;&#35201;&#20195;&#29702;&#20043;&#38388;&#30340;&#26377;&#38480;&#36890;&#20449;&#28192;&#36947;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#22522;&#20934;&#26080;&#20849;&#20139;&#21435;&#20013;&#24515;&#21270;&#35757;&#32451;&#21644;&#26368;&#20808;&#36827;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#12290;&#27492;&#22806;&#65292;&#20165;&#20998;&#20139;&#23569;&#37327;&#39640;&#24230;&#30456;&#20851;&#30340;&#32463;&#39564;&#20248;&#20110;&#20195;&#29702;&#20043;&#38388;&#30340;&#25152;&#26377;&#32463;&#39564;&#20849;&#20139;&#65292;&#32780;&#19988;&#36873;&#25321;&#24615;&#20307;&#39564;&#20849;&#20139;&#30340;&#24615;&#33021;&#25552;&#21319;&#22312;&#21508;&#31181;&#36229;&#21442;&#25968;&#21644;DQN&#21464;&#20307;&#20013;&#22343;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#30340;&#21442;&#32771;&#23454;&#29616;&#21487;&#22312;https://github.com/mgerstgrasser/super&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel multi-agent RL approach, Selective Multi-Agent Prioritized Experience Relay, in which agents share with other agents a limited number of transitions they observe during training. The intuition behind this is that even a small number of relevant experiences from other agents could help each agent learn. Unlike many other multi-agent RL algorithms, this approach allows for largely decentralized training, requiring only a limited communication channel between agents. We show that our approach outperforms baseline no-sharing decentralized training and state-of-the art multi-agent RL algorithms. Further, sharing only a small number of highly relevant experiences outperforms sharing all experiences between agents, and the performance uplift from selective experience sharing is robust across a range of hyperparameters and DQN variants. A reference implementation of our algorithm is available at https://github.com/mgerstgrasser/super.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;DEFT&#30340;&#25968;&#25454;&#39640;&#25928;&#24494;&#35843;&#26694;&#26550;&#65292;&#36890;&#36807;&#26080;&#30417;&#30563;&#26680;&#24515;&#38598;&#36873;&#25321;&#26469;&#26368;&#23567;&#21270;&#24494;&#35843;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#25152;&#38656;&#30340;&#25968;&#25454;&#37327;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;DEFT&#27169;&#22411;&#22312;&#20934;&#30830;&#24615;&#19978;&#19982;&#29616;&#26377;&#27169;&#22411;&#30456;&#24403;&#65292;&#24182;&#19988;&#20165;&#20351;&#29992;&#20102;70%&#30340;&#25968;&#25454;&#37327;&#12290;</title><link>http://arxiv.org/abs/2310.16776</link><description>&lt;p&gt;
DEFT&#65306;&#36890;&#36807;&#26080;&#30417;&#30563;&#26680;&#24515;&#38598;&#36873;&#25321;&#23454;&#29616;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#25968;&#25454;&#39640;&#25928;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
DEFT: Data Efficient Fine-Tuning for Large Language Models via Unsupervised Core-Set Selection. (arXiv:2310.16776v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16776
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;DEFT&#30340;&#25968;&#25454;&#39640;&#25928;&#24494;&#35843;&#26694;&#26550;&#65292;&#36890;&#36807;&#26080;&#30417;&#30563;&#26680;&#24515;&#38598;&#36873;&#25321;&#26469;&#26368;&#23567;&#21270;&#24494;&#35843;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#25152;&#38656;&#30340;&#25968;&#25454;&#37327;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;DEFT&#27169;&#22411;&#22312;&#20934;&#30830;&#24615;&#19978;&#19982;&#29616;&#26377;&#27169;&#22411;&#30456;&#24403;&#65292;&#24182;&#19988;&#20165;&#20351;&#29992;&#20102;70%&#30340;&#25968;&#25454;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#36827;&#23637;&#20351;&#24471;&#35768;&#22810;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#21487;&#20197;&#20351;&#29992;&#65307;&#28982;&#32780;&#65292;&#19968;&#20010;&#20173;&#28982;&#23384;&#22312;&#30340;&#38382;&#39064;&#26159;&#24494;&#35843;PLMs&#20197;&#29992;&#20110;&#19979;&#28216;&#20219;&#21153;&#31350;&#31455;&#38656;&#35201;&#22810;&#23569;&#25968;&#25454;&#65311;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;DEFT&#65292;&#19968;&#31181;&#25968;&#25454;&#39640;&#25928;&#30340;&#24494;&#35843;&#26694;&#26550;&#65292;&#23427;&#21033;&#29992;&#26080;&#30417;&#30563;&#30340;&#26680;&#24515;&#38598;&#36873;&#25321;&#26469;&#26368;&#23567;&#21270;&#24494;&#35843;PLMs&#25152;&#38656;&#30340;&#25968;&#25454;&#37327;&#12290;&#25105;&#20204;&#22312;&#25991;&#26412;&#32534;&#36753;LM&#30340;&#32972;&#26223;&#19979;&#23637;&#31034;&#20102;DEFT&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#19982;&#26368;&#20808;&#36827;&#30340;&#25991;&#26412;&#32534;&#36753;&#27169;&#22411;CoEDIT&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#23450;&#37327;&#21644;&#23450;&#24615;&#32467;&#26524;&#34920;&#26126;&#65292;DEFT&#27169;&#22411;&#22312;&#20934;&#30830;&#24615;&#19978;&#19982;CoEDIT&#19968;&#26679;&#65292;&#32780;&#20351;&#29992;&#30340;&#25968;&#25454;&#37327;&#35201;&#23569;&#32422;70%&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances have led to the availability of many pre-trained language models (PLMs); however, a question that remains is how much data is truly needed to fine-tune PLMs for downstream tasks? In this work, we introduce DEFT, a data-efficient fine-tuning framework that leverages unsupervised core-set selection to minimize the amount of data needed to fine-tune PLMs for downstream tasks. We demonstrate the efficacy of our DEFT framework in the context of text-editing LMs, and compare to the state-of-the art text-editing model, CoEDIT. Our quantitative and qualitative results demonstrate that DEFT models are just as accurate as CoEDIT while being finetuned on ~70% less data.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#19981;&#21516;&#24418;&#24577;&#26426;&#22120;&#20154;&#38388;&#23545;&#24212;&#20851;&#31995;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#28436;&#31034;&#23454;&#29616;&#20102;&#20849;&#21516;&#30340;&#28508;&#21464;&#37327;&#34920;&#31034;&#65292;&#20351;&#24471;&#19981;&#21516;&#26426;&#22120;&#20154;&#21487;&#20197;&#26356;&#30452;&#25509;&#22320;&#36716;&#31227;&#25216;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.13458</link><description>&lt;p&gt;
&#19981;&#21516;&#24418;&#24577;&#26426;&#22120;&#20154;&#38388;&#30340;&#20219;&#21153;&#28436;&#31034;&#23398;&#20064;&#19982;&#23545;&#24212;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
Correspondence learning between morphologically different robots through task demonstrations. (arXiv:2310.13458v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13458
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#19981;&#21516;&#24418;&#24577;&#26426;&#22120;&#20154;&#38388;&#23545;&#24212;&#20851;&#31995;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#28436;&#31034;&#23454;&#29616;&#20102;&#20849;&#21516;&#30340;&#28508;&#21464;&#37327;&#34920;&#31034;&#65292;&#20351;&#24471;&#19981;&#21516;&#26426;&#22120;&#20154;&#21487;&#20197;&#26356;&#30452;&#25509;&#22320;&#36716;&#31227;&#25216;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35266;&#23519;&#21040;&#26426;&#22120;&#20154;&#22312;&#20854;&#26426;&#36523;&#12289;&#20256;&#24863;&#22120;&#21644;&#25191;&#34892;&#22120;&#26041;&#38754;&#26377;&#30528;&#21508;&#31181;&#21508;&#26679;&#30340;&#24046;&#24322;&#12290;&#32771;&#34385;&#21040;&#26426;&#22120;&#20154;&#39046;&#22495;&#30340;&#24040;&#22823;&#22810;&#26679;&#24615;&#65292;&#29420;&#31435;&#22320;&#25945;&#23548;&#27599;&#20010;&#19981;&#21516;&#26426;&#22120;&#20154;&#30340;&#27599;&#20010;&#25216;&#33021;&#26159;&#20302;&#25928;&#19988;&#19981;&#21487;&#25193;&#23637;&#30340;&#12290;&#22914;&#26524;&#25105;&#20204;&#33021;&#22815;&#23398;&#20064;&#19981;&#21516;&#26426;&#22120;&#20154;&#30340;&#24863;&#23448;&#36816;&#21160;&#31354;&#38388;&#20043;&#38388;&#30340;&#23545;&#24212;&#20851;&#31995;&#65292;&#37027;&#20040;&#25105;&#20204;&#21487;&#20197;&#26399;&#26395;&#22312;&#19968;&#20010;&#26426;&#22120;&#20154;&#19978;&#23398;&#20064;&#30340;&#25216;&#33021;&#21487;&#20197;&#26356;&#30452;&#25509;&#26377;&#25928;&#22320;&#36716;&#31227;&#21040;&#20854;&#20182;&#26426;&#22120;&#20154;&#19978;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#22312;&#20855;&#26377;&#20851;&#33410;&#25511;&#21046;&#30340;&#22266;&#23450;&#22522;&#24231;&#25805;&#32437;&#26426;&#22120;&#20154;&#21644;&#24046;&#21160;&#39537;&#21160;&#31227;&#21160;&#26426;&#22120;&#20154;&#20043;&#38388;&#23398;&#20064;&#23545;&#24212;&#20851;&#31995;&#12290;&#20026;&#27492;&#65292;&#39318;&#20808;&#35753;&#20004;&#20010;&#26426;&#22120;&#20154;&#36827;&#34892;&#25191;&#34892;&#30456;&#21516;&#20219;&#21153;&#30340;&#28436;&#31034;&#12290;&#22312;&#23398;&#20064;&#23545;&#24212;&#31574;&#30053;&#30340;&#21516;&#26102;&#24418;&#25104;&#19968;&#20010;&#20849;&#21516;&#30340;&#28508;&#21464;&#37327;&#34920;&#31034;&#12290;&#22312;&#36825;&#20010;&#21021;&#22987;&#23398;&#20064;&#38454;&#27573;&#20043;&#21518;&#65292;&#36890;&#36807;&#35266;&#23519;&#19968;&#20010;&#26426;&#22120;&#20154;&#30340;&#26032;&#20219;&#21153;&#25191;&#34892;&#23601;&#36275;&#20197;&#29983;&#25104;&#19968;&#20010;&#28508;&#21464;&#37327;&#30340;&#34920;&#31034;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#24212;&#20851;&#31995;&#30340;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
We observe a large variety of robots in terms of their bodies, sensors, and actuators. Given the commonalities in the skill sets, teaching each skill to each different robot independently is inefficient and not scalable when the large variety in the robotic landscape is considered. If we can learn the correspondences between the sensorimotor spaces of different robots, we can expect a skill that is learned in one robot can be more directly and easily transferred to the other robots. In this paper, we propose a method to learn correspondences between robots that have significant differences in their morphologies: a fixed-based manipulator robot with joint control and a differential drive mobile robot. For this, both robots are first given demonstrations that achieve the same tasks. A common latent representation is formed while learning the corresponding policies. After this initial learning stage, the observation of a new task execution by one robot becomes sufficient to generate a lat
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#36712;&#36857;&#29983;&#25104;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#36890;&#29992;&#24037;&#20855;&#20351;&#29992;&#25216;&#33021;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#36866;&#24212;&#19981;&#21516;&#24418;&#29366;&#30340;&#24037;&#20855;&#65292;&#20174;&#32780;&#20351;&#33258;&#20027;&#31995;&#32479;&#33021;&#22815;&#22788;&#29702;&#22797;&#26434;&#30340;&#21487;&#21464;&#24418;&#29289;&#20307;&#25805;&#20316;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2310.00156</link><description>&lt;p&gt;
&#36890;&#36807;&#36712;&#36857;&#29983;&#25104;&#23398;&#20064;&#20855;&#26377;&#36890;&#29992;&#24615;&#30340;&#24037;&#20855;&#20351;&#29992;&#25216;&#33021;
&lt;/p&gt;
&lt;p&gt;
Learning Generalizable Tool-use Skills through Trajectory Generation. (arXiv:2310.00156v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00156
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#36712;&#36857;&#29983;&#25104;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#36890;&#29992;&#24037;&#20855;&#20351;&#29992;&#25216;&#33021;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#36866;&#24212;&#19981;&#21516;&#24418;&#29366;&#30340;&#24037;&#20855;&#65292;&#20174;&#32780;&#20351;&#33258;&#20027;&#31995;&#32479;&#33021;&#22815;&#22788;&#29702;&#22797;&#26434;&#30340;&#21487;&#21464;&#24418;&#29289;&#20307;&#25805;&#20316;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#25928;&#21033;&#29992;&#24037;&#20855;&#30340;&#33258;&#20027;&#31995;&#32479;&#21487;&#20197;&#24110;&#21161;&#20154;&#20204;&#23436;&#25104;&#35768;&#22810;&#24120;&#35265;&#20219;&#21153;&#65292;&#22914;&#28921;&#39274;&#21644;&#28165;&#27905;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#31995;&#32479;&#22312;&#36866;&#24212;&#26032;&#24037;&#20855;&#26041;&#38754;&#36828;&#36828;&#19981;&#21450;&#20154;&#31867;&#30340;&#26234;&#33021;&#27700;&#24179;&#12290;&#22522;&#20110;&#21487;&#21450;&#24615;&#30340;&#20808;&#21069;&#24037;&#20316;&#36890;&#24120;&#23545;&#29615;&#22659;&#20570;&#20986;&#20102;&#24456;&#24378;&#30340;&#20551;&#35774;&#65292;&#24182;&#19988;&#26080;&#27861;&#25193;&#23637;&#21040;&#26356;&#22797;&#26434;&#12289;&#25509;&#35302;&#20016;&#23500;&#30340;&#20219;&#21153;&#12290; &#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#36825;&#20010;&#25361;&#25112;&#65292;&#24182;&#25506;&#32034;&#20102;&#20195;&#29702;&#22914;&#20309;&#23398;&#20064;&#20351;&#29992;&#20197;&#21069;&#26410;&#35265;&#36807;&#30340;&#24037;&#20855;&#26469;&#25805;&#32437;&#21487;&#21464;&#24418;&#29289;&#20307;&#12290; &#25105;&#20204;&#25552;&#20986;&#20102;&#23558;&#24037;&#20855;&#20351;&#29992;&#36712;&#36857;&#20316;&#20026;&#19968;&#31995;&#21015;&#28857;&#20113;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#21487;&#20197;&#25512;&#24191;&#21040;&#19981;&#21516;&#30340;&#24037;&#20855;&#24418;&#29366;&#12290;&#23545;&#20110;&#20219;&#20309;&#26032;&#30340;&#24037;&#20855;&#65292;&#25105;&#20204;&#39318;&#20808;&#29983;&#25104;&#19968;&#20010;&#24037;&#20855;&#20351;&#29992;&#36712;&#36857;&#65292;&#28982;&#21518;&#20248;&#21270;&#24037;&#20855;&#23039;&#21183;&#24207;&#21015;&#20197;&#19982;&#29983;&#25104;&#30340;&#36712;&#36857;&#23545;&#40784;&#12290;&#25105;&#20204;&#20026;&#22235;&#31181;&#19981;&#21516;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#21487;&#21464;&#24418;&#29289;&#20307;&#25805;&#32437;&#20219;&#21153;&#35757;&#32451;&#20102;&#19968;&#20010;&#21333;&#19968;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#20165;&#20351;&#29992;&#27599;&#20010;&#20219;&#21153;&#30340;&#21333;&#20010;&#24037;&#20855;&#30340;&#31034;&#33539;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#19988;&#33021;&#22815;...
&lt;/p&gt;
&lt;p&gt;
Autonomous systems that efficiently utilize tools can assist humans in completing many common tasks such as cooking and cleaning. However, current systems fall short of matching human-level of intelligence in terms of adapting to novel tools. Prior works based on affordance often make strong assumptions about the environments and cannot scale to more complex, contact-rich tasks. In this work, we tackle this challenge and explore how agents can learn to use previously unseen tools to manipulate deformable objects. We propose to learn a generative model of the tool-use trajectories as a sequence of point clouds, which generalizes to different tool shapes. Given any novel tool, we first generate a tool-use trajectory and then optimize the sequence of tool poses to align with the generated trajectory. We train a single model for four different challenging deformable object manipulation tasks. Our model is trained with demonstration data from just a single tool for each task and is able to 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#25235;&#21462;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#26377;&#25928;&#30340;&#25506;&#32034;&#31574;&#30053;&#65292;&#25552;&#39640;&#20102;&#26426;&#22120;&#20154;&#23545;&#26410;&#30693;&#29615;&#22659;&#30340;&#36866;&#24212;&#33021;&#21147;&#21644;&#25235;&#21462;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.12038</link><description>&lt;p&gt;
&#22312;&#32447;&#25235;&#21462;&#23398;&#20064;&#20013;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#39537;&#21160;&#30340;&#25506;&#32034;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Uncertainty-driven Exploration Strategies for Online Grasp Learning. (arXiv:2309.12038v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12038
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#25235;&#21462;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#26377;&#25928;&#30340;&#25506;&#32034;&#31574;&#30053;&#65292;&#25552;&#39640;&#20102;&#26426;&#22120;&#20154;&#23545;&#26410;&#30693;&#29615;&#22659;&#30340;&#36866;&#24212;&#33021;&#21147;&#21644;&#25235;&#21462;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#25235;&#21462;&#39044;&#27979;&#26041;&#27861;&#20027;&#35201;&#22522;&#20110;&#31163;&#32447;&#23398;&#20064;&#65292;&#32780;&#24573;&#30053;&#20102;&#22312;&#32447;&#36866;&#24212;&#26032;&#30340;&#25342;&#21462;&#22330;&#26223;&#26102;&#30340;&#25506;&#32034;&#24615;&#25235;&#21462;&#23398;&#20064;&#65292;&#20363;&#22914;&#26410;&#35265;&#30446;&#26631;&#32452;&#21512;&#12289;&#25668;&#20687;&#26426;&#21644;&#31665;&#23376;&#35774;&#32622;&#31561;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29992;&#20110;&#26426;&#22120;&#20154;&#31665;&#23376;&#25342;&#21462;&#30340;&#22312;&#32447;&#25235;&#21462;&#39044;&#27979;&#23398;&#20064;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#20351;&#29992;&#26377;&#25928;&#30340;&#25506;&#32034;&#31574;&#30053;&#30340;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#20854;&#23545;&#26410;&#35265;&#29615;&#22659;&#35774;&#32622;&#30340;&#36866;&#24212;&#24615;&#33021;&#21147;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#23558;&#22312;&#32447;&#25235;&#21462;&#23398;&#20064;&#24314;&#27169;&#20026;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#20197;&#23454;&#29616;&#23545;&#25235;&#21462;&#31574;&#30053;&#30340;&#33258;&#36866;&#24212;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing grasp prediction approaches are mostly based on offline learning, while, ignored the exploratory grasp learning during online adaptation to new picking scenarios, i.e., unseen object portfolio, camera and bin settings etc. In this paper, we present a novel method for online learning of grasp predictions for robotic bin picking in a principled way. Existing grasp prediction approaches are mostly based on offline learning, while, ignored the exploratory grasp learning during online adaptation to new picking scenarios, i.e., unseen object portfolio, camera and bin settings etc. In this paper, we present a novel method for online learning of grasp predictions for robotic bin picking in a principled way. Specifically, the online learning algorithm with an effective exploration strategy can significantly improve its adaptation performance to unseen environment settings. To this end, we first propose to formulate online grasp learning as a RL problem that will allow to adapt both gra
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20351;&#29992;Transformer&#27169;&#22411;&#23545;DNA&#24207;&#21015;&#36827;&#34892;&#23545;&#40784;&#65292;&#36890;&#36807;&#29983;&#25104;&#25968;&#20540;&#34920;&#31034;&#26469;&#23454;&#29616;&#12290;&#30456;&#27604;&#20256;&#32479;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#30701;DNA&#24207;&#21015;&#30340;&#20998;&#31867;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#23545;&#20110;&#22522;&#22240;&#32452;&#23398;&#20998;&#26512;&#20855;&#26377;&#28508;&#22312;&#30340;&#24212;&#29992;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2309.11087</link><description>&lt;p&gt;
Embed-Search-Align: &#20351;&#29992;Transformer&#27169;&#22411;&#36827;&#34892;DNA&#24207;&#21015;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Embed-Search-Align: DNA Sequence Alignment using Transformer Models. (arXiv:2309.11087v1 [q-bio.GN])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11087
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20351;&#29992;Transformer&#27169;&#22411;&#23545;DNA&#24207;&#21015;&#36827;&#34892;&#23545;&#40784;&#65292;&#36890;&#36807;&#29983;&#25104;&#25968;&#20540;&#34920;&#31034;&#26469;&#23454;&#29616;&#12290;&#30456;&#27604;&#20256;&#32479;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#30701;DNA&#24207;&#21015;&#30340;&#20998;&#31867;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#23545;&#20110;&#22522;&#22240;&#32452;&#23398;&#20998;&#26512;&#20855;&#26377;&#28508;&#22312;&#30340;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
DNA&#24207;&#21015;&#23545;&#40784;&#28041;&#21450;&#23558;&#30701;DNA&#35835;&#21462;&#20998;&#37197;&#21040;&#24191;&#27867;&#30340;&#21442;&#32771;&#22522;&#22240;&#32452;&#19978;&#30340;&#26368;&#21487;&#33021;&#20301;&#32622;&#12290;&#36825;&#20010;&#36807;&#31243;&#23545;&#20110;&#21508;&#31181;&#22522;&#22240;&#32452;&#23398;&#20998;&#26512;&#33267;&#20851;&#37325;&#35201;&#65292;&#21253;&#25324;&#21464;&#24322;&#35843;&#29992;&#12289;&#36716;&#24405;&#32452;&#23398;&#21644;&#34920;&#35266;&#22522;&#22240;&#32452;&#23398;&#12290;&#20256;&#32479;&#26041;&#27861;&#32463;&#36807;&#25968;&#21313;&#24180;&#30340;&#25913;&#36827;&#65292;&#20197;&#20004;&#20010;&#27493;&#39588;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65306;&#20808;&#36827;&#34892;&#22522;&#22240;&#32452;&#32034;&#24341;&#65292;&#28982;&#21518;&#36827;&#34892;&#39640;&#25928;&#25628;&#32034;&#20197;&#30830;&#23450;&#32473;&#23450;&#35835;&#21462;&#30340;&#21487;&#33021;&#20301;&#32622;&#12290;&#22312;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#23558;&#25991;&#26412;&#32534;&#30721;&#20026;&#23884;&#20837;&#21521;&#37327;&#26041;&#38754;&#21462;&#24471;&#25104;&#21151;&#30340;&#22522;&#30784;&#19978;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#21162;&#21147;&#25506;&#32034;&#20102;&#26159;&#21542;&#21487;&#20197;&#20351;&#29992;&#30456;&#21516;&#30340;Transformer&#26550;&#26500;&#20026;DNA&#24207;&#21015;&#29983;&#25104;&#25968;&#20540;&#34920;&#31034;&#12290;&#36825;&#26679;&#30340;&#27169;&#22411;&#24050;&#32463;&#22312;&#28041;&#21450;&#20998;&#31867;&#30701;DNA&#24207;&#21015;&#30340;&#20219;&#21153;&#20013;&#26174;&#31034;&#20986;&#26089;&#26399;&#30340;&#28508;&#21147;&#65292;&#20363;&#22914;&#26816;&#27979;&#32534;&#30721;&#21644;&#38750;&#32534;&#30721;&#21306;&#22495;&#20197;&#21450;&#35782;&#21035;&#22686;&#24378;&#23376;&#21644;&#21551;&#21160;&#23376;&#24207;&#21015;&#12290;&#28982;&#32780;&#65292;&#24207;&#21015;&#20998;&#31867;&#20219;&#21153;&#30340;&#24615;&#33021;&#24182;&#19981;&#33021;&#30452;&#25509;&#24212;&#29992;&#20110;&#24207;&#21015;&#23545;&#40784;&#20219;&#21153;&#65292;&#23545;&#40784;&#20219;&#21153;&#30340;&#20851;&#38190;&#26159;&#22312;&#20445;&#25345;&#24207;&#21015;&#30456;&#20284;&#24615;&#30340;&#21516;&#26102;&#25214;&#21040;&#26368;&#20339;&#30340;&#23545;&#24212;&#20301;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;
DNA sequence alignment involves assigning short DNA reads to the most probable locations on an extensive reference genome. This process is crucial for various genomic analyses, including variant calling, transcriptomics, and epigenomics. Conventional methods, refined over decades, tackle this challenge in two steps: genome indexing followed by efficient search to locate likely positions for given reads. Building on the success of Large Language Models (LLM) in encoding text into embeddings, where the distance metric captures semantic similarity, recent efforts have explored whether the same Transformer architecture can produce numerical representations for DNA sequences. Such models have shown early promise in tasks involving classification of short DNA sequences, such as the detection of coding vs non-coding regions, as well as the identification of enhancer and promoter sequences. Performance at sequence classification tasks does not, however, translate to sequence alignment, where i
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;P-ROCKET&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#29305;&#24449;&#36873;&#25321;&#30340;&#35282;&#24230;&#21024;&#38500;&#21367;&#31215;&#26680;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#20013;&#30340;&#38543;&#26426;&#21367;&#31215;&#26680;&#36827;&#34892;&#21098;&#26525;&#12290;</title><link>http://arxiv.org/abs/2309.08499</link><description>&lt;p&gt;
P-ROCKET: &#38024;&#23545;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#30340;&#38543;&#26426;&#21367;&#31215;&#26680;&#21098;&#26525;
&lt;/p&gt;
&lt;p&gt;
P-ROCKET: Pruning Random Convolution Kernels for Time Series Classification. (arXiv:2309.08499v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08499
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;P-ROCKET&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#29305;&#24449;&#36873;&#25321;&#30340;&#35282;&#24230;&#21024;&#38500;&#21367;&#31215;&#26680;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#20013;&#30340;&#38543;&#26426;&#21367;&#31215;&#26680;&#36827;&#34892;&#21098;&#26525;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26368;&#36817;&#20960;&#24180;&#65292;&#20004;&#20010;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#27169;&#22411;ROCKET&#21644;MINIROCKET&#22240;&#20854;&#20302;&#35757;&#32451;&#25104;&#26412;&#21644;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#24615;&#32780;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#12290;ROCKET&#21644;MINIROCKET&#21033;&#29992;&#26080;&#38656;&#35757;&#32451;&#30340;&#38543;&#26426;&#19968;&#32500;&#21367;&#31215;&#26680;&#65292;&#21487;&#20197;&#24555;&#36895;&#20174;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#25552;&#21462;&#29305;&#24449;&#65292;&#20174;&#32780;&#23454;&#29616;&#32447;&#24615;&#20998;&#31867;&#22120;&#30340;&#39640;&#25928;&#25311;&#21512;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#20840;&#38754;&#25429;&#25417;&#26377;&#29992;&#30340;&#29305;&#24449;&#65292;&#38656;&#35201;&#22823;&#37327;&#30340;&#38543;&#26426;&#21367;&#31215;&#26680;&#65292;&#36825;&#23545;&#20110;&#36164;&#28304;&#21463;&#38480;&#30340;&#35774;&#22791;&#26469;&#35828;&#26159;&#19981;&#20860;&#23481;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#21551;&#21457;&#24335;&#36827;&#21270;&#31639;&#27861;S-ROCKET&#65292;&#29992;&#20110;&#35782;&#21035;&#21644;&#21098;&#26525;&#20887;&#20313;&#30340;&#21367;&#31215;&#26680;&#12290;&#28982;&#32780;&#65292;&#36827;&#21270;&#31639;&#27861;&#26412;&#36523;&#30340;&#29305;&#24615;&#23548;&#33268;&#22312;S-ROCKET&#20013;&#35780;&#20272;&#21367;&#31215;&#26680;&#26159;&#19968;&#20010;&#32791;&#26102;&#30340;&#36807;&#31243;&#12290;&#26412;&#25991;&#20013;&#65292;&#19982;&#30452;&#25509;&#35780;&#20272;&#20855;&#26377;&#38750;&#26174;&#33879;&#24046;&#24322;&#30340;&#38543;&#26426;&#21367;&#31215;&#26680;&#30340;S-ROCKET&#19981;&#21516;&#65292;&#25105;&#20204;&#20174;&#29305;&#24449;&#36873;&#25321;&#30340;&#35282;&#24230;&#21024;&#38500;&#21367;&#31215;&#26680;&#65292;&#36890;&#36807;&#28040;&#38500;&#24207;&#21015;&#20013;&#30340;&#30456;&#20851;&#36830;&#25509;&#26469;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, two time series classification models, ROCKET and MINIROCKET, have attracted much attention for their low training cost and state-of-the-art accuracy. Utilizing random 1-D convolutional kernels without training, ROCKET and MINIROCKET can rapidly extract features from time series data, allowing for the efficient fitting of linear classifiers. However, to comprehensively capture useful features, a large number of random kernels are required, which is incompatible for resource-constrained devices. Therefore, a heuristic evolutionary algorithm named S-ROCKET is devised to recognize and prune redundant kernels. Nevertheless, the inherent nature of evolutionary algorithms renders the evaluation of kernels within S-ROCKET an unacceptable time-consuming process. In this paper, diverging from S-ROCKET, which directly evaluates random kernels with nonsignificant differences, we remove kernels from a feature selection perspective by eliminating associating connections in the sequ
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#36827;&#34892;&#39640;&#25928;&#30340;&#26080;CPU&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;GPU&#19978;&#30452;&#25509;&#23384;&#20648;&#25972;&#20010;&#25968;&#25454;&#38598;&#20197;INR&#26684;&#24335;&#65292;&#20943;&#23569;&#20102;&#25968;&#25454;&#20256;&#36755;&#24320;&#38144;&#65292;&#20174;&#32780;&#21152;&#36895;&#35757;&#32451;&#36807;&#31243;&#12290;&#21516;&#26102;&#65292;&#37319;&#29992;&#39640;&#24230;&#24182;&#34892;&#21270;&#21644;&#23454;&#26102;&#25191;&#34892;&#30340;&#35299;&#30721;&#36807;&#31243;&#65292;&#36827;&#19968;&#27493;&#25552;&#21319;&#20102;&#21387;&#32553;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.16699</link><description>&lt;p&gt;
&#24555;&#36895;-INR: &#20351;&#29992;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#36827;&#34892;&#25928;&#29575;&#39640;&#30340;&#26080;CPU&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Rapid-INR: Storage Efficient CPU-free DNN Training Using Implicit Neural Representation. (arXiv:2306.16699v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16699
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#36827;&#34892;&#39640;&#25928;&#30340;&#26080;CPU&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;GPU&#19978;&#30452;&#25509;&#23384;&#20648;&#25972;&#20010;&#25968;&#25454;&#38598;&#20197;INR&#26684;&#24335;&#65292;&#20943;&#23569;&#20102;&#25968;&#25454;&#20256;&#36755;&#24320;&#38144;&#65292;&#20174;&#32780;&#21152;&#36895;&#35757;&#32451;&#36807;&#31243;&#12290;&#21516;&#26102;&#65292;&#37319;&#29992;&#39640;&#24230;&#24182;&#34892;&#21270;&#21644;&#23454;&#26102;&#25191;&#34892;&#30340;&#35299;&#30721;&#36807;&#31243;&#65292;&#36827;&#19968;&#27493;&#25552;&#21319;&#20102;&#21387;&#32553;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;(INR)&#26159;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#34920;&#31034;&#22797;&#26434;&#30340;&#24418;&#29366;&#25110;&#23545;&#35937;&#65292;&#32780;&#26080;&#38656;&#26126;&#30830;&#23450;&#20041;&#23427;&#20204;&#30340;&#20960;&#20309;&#24418;&#29366;&#25110;&#34920;&#38754;&#32467;&#26500;&#12290;&#30456;&#21453;&#65292;INR&#23558;&#23545;&#35937;&#34920;&#31034;&#20026;&#36830;&#32493;&#20989;&#25968;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#35777;&#26126;&#20102;&#23558;&#31070;&#32463;&#32593;&#32476;&#29992;&#20316;INR&#36827;&#34892;&#22270;&#20687;&#21387;&#32553;&#30340;&#26377;&#25928;&#24615;&#65292;&#23637;&#31034;&#20102;&#19982;&#20256;&#32479;&#26041;&#27861;&#65288;&#22914;JPEG&#65289;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;INR&#22312;&#22270;&#20687;&#21387;&#32553;&#20043;&#22806;&#36824;&#20855;&#26377;&#21508;&#31181;&#24212;&#29992;&#28508;&#21147;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;Rapid-INR&#65292;&#19968;&#31181;&#21033;&#29992;INR&#23545;&#22270;&#20687;&#36827;&#34892;&#32534;&#30721;&#21644;&#21387;&#32553;&#30340;&#26032;&#26041;&#27861;&#65292;&#20174;&#32780;&#21152;&#36895;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#20013;&#30340;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;GPU&#19978;&#30452;&#25509;&#20197;INR&#26684;&#24335;&#23384;&#20648;&#25972;&#20010;&#25968;&#25454;&#38598;&#65292;&#20943;&#23569;&#20102;&#35757;&#32451;&#36807;&#31243;&#20013;CPU&#21644;GPU&#20043;&#38388;&#30340;&#25968;&#25454;&#20256;&#36755;&#24320;&#38144;&#12290;&#27492;&#22806;&#65292;&#20174;INR&#21040;RGB&#26684;&#24335;&#30340;&#35299;&#30721;&#36807;&#31243;&#39640;&#24230;&#24182;&#34892;&#21270;&#24182;&#23454;&#26102;&#25191;&#34892;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25552;&#39640;&#21387;&#32553;&#25928;&#26524;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36845;&#20195;&#30340;&#22270;&#20687;&#21387;&#32553;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Implicit Neural Representation (INR) is an innovative approach for representing complex shapes or objects without explicitly defining their geometry or surface structure. Instead, INR represents objects as continuous functions. Previous research has demonstrated the effectiveness of using neural networks as INR for image compression, showcasing comparable performance to traditional methods such as JPEG. However, INR holds potential for various applications beyond image compression. This paper introduces Rapid-INR, a novel approach that utilizes INR for encoding and compressing images, thereby accelerating neural network training in computer vision tasks. Our methodology involves storing the whole dataset directly in INR format on a GPU, mitigating the significant data communication overhead between the CPU and GPU during training. Additionally, the decoding process from INR to RGB format is highly parallelized and executed on-the-fly. To further enhance compression, we propose iterativ
&lt;/p&gt;</description></item><item><title>LANISTR&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#26694;&#26550;&#65292;&#21487;&#20174;&#32467;&#26500;&#21270;&#21644;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#20013;&#36827;&#34892;&#23398;&#20064;&#65292;&#22312;&#25361;&#25112;&#24615;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>http://arxiv.org/abs/2305.16556</link><description>&lt;p&gt;
LANISTR&#65306;&#20174;&#32467;&#26500;&#21270;&#21644;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#20013;&#36827;&#34892;&#22810;&#27169;&#24577;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
LANISTR: Multimodal Learning from Structured and Unstructured Data. (arXiv:2305.16556v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16556
&lt;/p&gt;
&lt;p&gt;
LANISTR&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#26694;&#26550;&#65292;&#21487;&#20174;&#32467;&#26500;&#21270;&#21644;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#20013;&#36827;&#34892;&#23398;&#20064;&#65292;&#22312;&#25361;&#25112;&#24615;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#30340;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#24050;&#32463;&#22312;&#22788;&#29702;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#65288;&#21253;&#25324;&#25991;&#26412;&#12289;&#22270;&#20687;&#12289;&#38899;&#39057;&#21644;&#35270;&#39057;&#65289;&#26041;&#38754;&#23637;&#29616;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#20294;&#26159;&#65292;&#29616;&#23454;&#19990;&#30028;&#20013;&#26368;&#24120;&#35265;&#30340;&#24773;&#20917;&#26159;&#32467;&#26500;&#21270;&#65288;&#21253;&#25324;&#34920;&#26684;&#21644;&#26102;&#38388;&#24207;&#21015;&#65289;&#21644;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#30340;&#32467;&#21512;&#65292;&#20294;&#36825;&#19968;&#39046;&#22495;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#30340;&#30740;&#31350;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LANISTR&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#20174;&#35821;&#35328;&#12289;&#22270;&#20687;&#21644;&#32467;&#26500;&#21270;&#25968;&#25454;&#20013;&#36827;&#34892;&#23398;&#20064;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;&#27169;&#22359;&#65292;&#24182;&#37319;&#29992;&#22522;&#20110;&#30456;&#20284;&#24615;&#30340;&#22810;&#27169;&#24577;&#36974;&#32617;&#25439;&#22833;&#65292;&#20351;&#24471;LANISTR&#33021;&#22815;&#22312;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#25968;&#25454;&#20013;&#23398;&#20064;&#36328;&#27169;&#24577;&#20851;&#31995;&#65292;&#24182;&#22312;&#35757;&#32451;&#21644;&#27979;&#35797;&#26102;&#22788;&#29702;&#32570;&#22833;&#30340;&#27169;&#24577;&#12290;&#22312;&#20004;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25968;&#25454;&#38598;MIMIC-IV&#21644;Amazon Product Review&#19978;&#65292;&#19982;&#26368;&#20808;&#36827;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#30456;&#27604;&#65292;LANISTR&#20998;&#21035;&#36798;&#21040;&#20102;6.47%&#65288;AUROC&#65289;&#21644;&#39640;&#36798;17.69%&#65288;&#20934;&#30830;&#24230;&#65289;&#30340;&#32477;&#23545;&#25552;&#21319;&#65292;&#24182;&#26174;&#31034;&#20986;&#26356;&#24378;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal large-scale pretraining has shown impressive performance gains for unstructured data including language, image, audio, and video. Yet, the scenario most prominent in real-world applications is the existence of combination of structured (including tabular and time-series) and unstructured data, and this has so far been understudied. Towards this end, we propose LANISTR, a novel attention-based framework to learn from LANguage, Image, and STRuctured data. We introduce a new multimodal fusion module with a similarity-based multimodal masking loss that enables LANISTR to learn cross-modal relations from large-scale multimodal data with missing modalities during training and test time. On two publicly available challenging datasets, MIMIC-IV and Amazon Product Review, LANISTR achieves absolute improvements of 6.47% (AUROC) and up to 17.69% (accuracy), respectively, compared to the state-of-the-art multimodal models while showing superior generalization capabilities.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#25968;&#23383;&#23402;&#29983;&#30340;&#27010;&#24565;&#65292;&#20351;&#29992;&#19981;&#21516;&#25216;&#26415;&#26500;&#24314;&#22810;&#20010;&#36890;&#29992;&#20223;&#30495;&#22120;&#65292;&#24378;&#21270;&#20102;&#33258;&#21160;&#39550;&#39542;&#36719;&#20214;&#30340;&#22522;&#20110;&#20223;&#30495;&#30340;&#27979;&#35797;&#65292;&#25552;&#39640;&#20102;&#27979;&#35797;&#32467;&#26524;&#30340;&#26222;&#36866;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.08060</link><description>&lt;p&gt;
&#20004;&#20010;&#20248;&#20110;&#19968;&#20010;&#65306;&#25968;&#23383;&#23402;&#29983;&#20197;&#25552;&#39640;&#33258;&#21160;&#39550;&#39542;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Two is Better Than One: Digital Siblings to Improve Autonomous Driving Testing. (arXiv:2305.08060v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08060
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#25968;&#23383;&#23402;&#29983;&#30340;&#27010;&#24565;&#65292;&#20351;&#29992;&#19981;&#21516;&#25216;&#26415;&#26500;&#24314;&#22810;&#20010;&#36890;&#29992;&#20223;&#30495;&#22120;&#65292;&#24378;&#21270;&#20102;&#33258;&#21160;&#39550;&#39542;&#36719;&#20214;&#30340;&#22522;&#20110;&#20223;&#30495;&#30340;&#27979;&#35797;&#65292;&#25552;&#39640;&#20102;&#27979;&#35797;&#32467;&#26524;&#30340;&#26222;&#36866;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#20223;&#30495;&#30340;&#27979;&#35797;&#26159;&#30830;&#20445;&#33258;&#21160;&#39550;&#39542;&#36719;&#20214;&#21487;&#38752;&#24615;&#30340;&#37325;&#35201;&#19968;&#27493;&#12290;&#23454;&#38469;&#20013;&#65292;&#24403;&#20225;&#19994;&#20381;&#36182;&#31532;&#19977;&#26041;&#36890;&#29992;&#20223;&#30495;&#22120;&#36827;&#34892;&#20869;&#37096;&#25110;&#22806;&#21253;&#27979;&#35797;&#26102;&#65292;&#27979;&#35797;&#32467;&#26524;&#30340;&#26222;&#36866;&#24615;&#21463;&#21040;&#23041;&#32961;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#8220;&#25968;&#23383;&#23402;&#29983;&#8221;&#30340;&#27010;&#24565;&#21152;&#24378;&#20102;&#22522;&#20110;&#20223;&#30495;&#30340;&#27979;&#35797;&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#22312;&#20854;&#20013;AV&#22312;&#22810;&#20010;&#20351;&#29992;&#19981;&#21516;&#25216;&#26415;&#26500;&#24314;&#30340;&#36890;&#29992;&#20223;&#30495;&#22120;&#19978;&#36827;&#34892;&#27979;&#35797;&#12290;&#39318;&#20808;&#65292;&#38024;&#23545;&#27599;&#20010;&#21333;&#29420;&#30340;&#20223;&#30495;&#22120;&#33258;&#21160;&#29983;&#25104;&#27979;&#35797;&#29992;&#20363;&#12290;&#28982;&#21518;&#65292;&#20351;&#29992;&#29305;&#24449;&#26144;&#23556;&#23558;&#27979;&#35797;&#36801;&#31227;&#33267;&#21508;&#20010;&#20223;&#30495;&#22120;&#20043;&#38388;&#65292;&#20197;&#34920;&#24449;&#25152;&#36827;&#34892;&#30340;&#34892;&#39542;&#26465;&#20214;&#12290;&#26368;&#21518;&#65292;&#35745;&#31639;&#32852;&#21512;&#39044;&#27979;&#22833;&#25928;&#27010;&#29575;&#65292;&#24182;&#20165;&#22312;&#23402;&#29983;&#20043;&#38388;&#36798;&#25104;&#19968;&#33268;&#30340;&#24773;&#20917;&#19979;&#25253;&#21578;&#25925;&#38556;&#12290;&#25105;&#20204;&#20351;&#29992;&#20004;&#20010;&#24320;&#28304;&#20223;&#30495;&#22120;&#23454;&#29616;&#20102;&#35813;&#26694;&#26550;&#65292;&#24182;&#22312;&#25968;&#23383;&#23402;&#29983;&#30340;&#29289;&#29702;&#27604;&#20363;&#27169;&#22411;&#19978;&#36827;&#34892;&#20102;&#32463;&#39564;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Simulation-based testing represents an important step to ensure the reliability of autonomous driving software. In practice, when companies rely on third-party general-purpose simulators, either for in-house or outsourced testing, the generalizability of testing results to real autonomous vehicles is at stake.  In this paper, we strengthen simulation-based testing by introducing the notion of digital siblings, a novel framework in which the AV is tested on multiple general-purpose simulators, built with different technologies. First, test cases are automatically generated for each individual simulator. Then, tests are migrated between simulators, using feature maps to characterize of the exercised driving conditions. Finally, the joint predicted failure probability is computed and a failure is reported only in cases of agreement among the siblings.  We implemented our framework using two open-source simulators and we empirically compared it against a digital twin of a physical scaled a
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23558;&#22823;&#22411;&#22270;&#20687;&#27169;&#22411;&#21644;&#22823;&#35821;&#35328;&#27169;&#22411;&#32467;&#21512;&#36215;&#26469;&#65292;&#24314;&#31435;&#20102;&#19968;&#20010;&#29992;&#20110;&#32954;&#30284;&#35786;&#26029;&#30340;&#22810;&#20219;&#21153;CT&#22823;&#22411;&#22270;&#20687;&#25991;&#26412;&#65288;LIT&#65289;&#27169;&#22411;&#65292;&#33021;&#24456;&#22909;&#22320;&#25191;&#34892;&#32954;&#37096;CT&#20998;&#21106;&#31561;&#22810;&#20010;&#21307;&#23398;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2304.02649</link><description>&lt;p&gt;
&#24102;&#26377;&#22823;&#22411;&#22270;&#20687;&#25991;&#26412;&#65288;LIT&#65289;&#27169;&#22411;&#30340;CT&#22810;&#20219;&#21153;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
CT Multi-Task Learning with a Large Image-Text (LIT) Model. (arXiv:2304.02649v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02649
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23558;&#22823;&#22411;&#22270;&#20687;&#27169;&#22411;&#21644;&#22823;&#35821;&#35328;&#27169;&#22411;&#32467;&#21512;&#36215;&#26469;&#65292;&#24314;&#31435;&#20102;&#19968;&#20010;&#29992;&#20110;&#32954;&#30284;&#35786;&#26029;&#30340;&#22810;&#20219;&#21153;CT&#22823;&#22411;&#22270;&#20687;&#25991;&#26412;&#65288;LIT&#65289;&#27169;&#22411;&#65292;&#33021;&#24456;&#22909;&#22320;&#25191;&#34892;&#32954;&#37096;CT&#20998;&#21106;&#31561;&#22810;&#20010;&#21307;&#23398;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#19981;&#20165;&#33021;&#22815;&#25903;&#25345;&#22810;&#31181;&#35821;&#35328;&#20219;&#21153;&#65292;&#32780;&#19988;&#36824;&#21487;&#20197;&#20316;&#20026;&#19981;&#21516;&#39046;&#22495;&#30340;&#36890;&#29992;&#25509;&#21475;&#12290;&#36804;&#20170;&#20026;&#27490;&#65292;&#36824;&#27809;&#26377;&#35777;&#26126;&#22914;&#20309;&#23558;LLM&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#30340;&#25104;&#21151;&#26377;&#25928;&#22320;&#36716;&#21270;&#20026;&#28041;&#21450;&#39640;&#32500;&#21644;&#22810;&#27169;&#24577;&#21307;&#23398;&#22270;&#20687;&#30340;&#21307;&#23398;&#25104;&#20687;&#39046;&#22495;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25253;&#21578;&#20102;&#19968;&#39033;&#21487;&#34892;&#24615;&#30740;&#31350;&#65292;&#36890;&#36807;&#32452;&#21512;LLM&#21644;&#22823;&#22411;&#22270;&#20687;&#27169;&#22411;&#65288;LIM&#65289;&#65292;&#24314;&#31435;&#22810;&#20219;&#21153;CT&#22823;&#22411;&#22270;&#20687;&#25991;&#26412;&#65288;LIT&#65289;&#27169;&#22411;&#65292;&#29992;&#20110;&#32954;&#30284;&#35786;&#26029;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;LLM&#21644;LIM&#29992;&#20316;&#32534;&#30721;&#22120;&#65292;&#26681;&#25454;&#29305;&#23450;&#20219;&#21153;&#30340;&#25991;&#26412;&#25552;&#31034;&#26469;&#24863;&#30693;&#22810;&#27169;&#24577;&#20449;&#24687;&#65292;&#20174;&#32780;&#21327;&#21516;&#20316;&#29992;&#20110;&#22810;&#28304;&#20449;&#24687;&#21644;&#20219;&#21153;&#29305;&#23450;&#21644;&#24739;&#32773;&#29305;&#23450;&#30340;&#20808;&#39564;&#65292;&#20197;&#20248;&#21270;&#35786;&#26029;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;LIT&#27169;&#22411;&#21644;&#30456;&#20851;&#25216;&#26415;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#23558;&#37325;&#28857;&#35780;&#20272;3D&#32954;&#37096;CT&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#21021;&#27493;&#32467;&#26524;&#34920;&#26126;&#65292;LIT&#27169;&#22411;&#33021;&#22815;&#24456;&#22909;&#22320;&#25191;&#34892;&#22810;&#39033;&#21307;&#23398;&#20219;&#21153;&#65292;&#21253;&#25324;&#32954;&#20998;&#21106;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLM) not only empower multiple language tasks but also serve as a general interface across different spaces. Up to now, it has not been demonstrated yet how to effectively translate the successes of LLMs in the computer vision field to the medical imaging field which involves high-dimensional and multi-modal medical images. In this paper, we report a feasibility study of building a multi-task CT large image-text (LIT) model for lung cancer diagnosis by combining an LLM and a large image model (LIM). Specifically, the LLM and LIM are used as encoders to perceive multi-modal information under task-specific text prompts, which synergizes multi-source information and task-specific and patient-specific priors for optimized diagnostic performance. The key components of our LIT model and associated techniques are evaluated with an emphasis on 3D lung CT analysis. Our initial results show that the LIT model performs multiple medical tasks well, including lung segmentatio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20302;&#24310;&#36831;&#33258;&#36866;&#24212;&#32534;&#30721;&#33033;&#20914;&#26694;&#26550;&#29992;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65292;&#22312;&#32534;&#30721;&#22120;&#28789;&#27963;&#24615;&#12289;&#24310;&#36831;&#21644;&#33021;&#37327;&#25928;&#29575;&#26041;&#38754;&#20855;&#26377;&#20248;&#24322;&#24615;&#33021;&#21644;&#24191;&#27867;&#24212;&#29992;&#33539;&#22260;&#12290;</title><link>http://arxiv.org/abs/2211.11760</link><description>&lt;p&gt;
&#19968;&#20010;&#20302;&#24310;&#36831;&#33258;&#36866;&#24212;&#32534;&#30721;&#33033;&#20914;&#26694;&#26550;&#29992;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
A Low Latency Adaptive Coding Spiking Framework for Deep Reinforcement Learning. (arXiv:2211.11760v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.11760
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20302;&#24310;&#36831;&#33258;&#36866;&#24212;&#32534;&#30721;&#33033;&#20914;&#26694;&#26550;&#29992;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65292;&#22312;&#32534;&#30721;&#22120;&#28789;&#27963;&#24615;&#12289;&#24310;&#36831;&#21644;&#33021;&#37327;&#25928;&#29575;&#26041;&#38754;&#20855;&#26377;&#20248;&#24322;&#24615;&#33021;&#21644;&#24191;&#27867;&#24212;&#29992;&#33539;&#22260;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#30001;&#20110;&#20302;&#21151;&#32791;&#21644;&#20107;&#20214;&#39537;&#21160;&#29305;&#24615;&#65292;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNNs&#65289;&#34987;&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#12290;&#28982;&#32780;&#65292;&#22266;&#23450;&#32534;&#30721;&#26041;&#27861;&#23548;&#33268;&#30340;&#33033;&#20914;&#24378;&#21270;&#23398;&#20064;&#65288;SRL&#65289;&#20173;&#28982;&#38754;&#20020;&#39640;&#24310;&#36831;&#21644;&#36739;&#24046;&#30340;&#28789;&#27963;&#24615;&#38382;&#39064;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#21487;&#23398;&#20064;&#30340;&#30697;&#38453;&#20056;&#27861;&#23545;&#33033;&#20914;&#36827;&#34892;&#32534;&#30721;&#21644;&#35299;&#30721;&#65292;&#25552;&#39640;&#32534;&#30721;&#22120;&#30340;&#28789;&#27963;&#24615;&#65292;&#20174;&#32780;&#38477;&#20302;&#24310;&#36831;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#20351;&#29992;&#30452;&#25509;&#35757;&#32451;&#26041;&#27861;&#35757;&#32451;SNNs&#65292;&#24182;&#20351;&#29992;&#20004;&#31181;&#19981;&#21516;&#30340;&#32467;&#26500;&#29992;&#20110;&#22312;&#32447;&#21644;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#20351;&#25105;&#20204;&#30340;&#27169;&#22411;&#25317;&#26377;&#26356;&#24191;&#27867;&#30340;&#24212;&#29992;&#33539;&#22260;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19981;&#21516;&#30340;&#31639;&#27861;&#21644;&#19981;&#21516;&#30340;&#29615;&#22659;&#20013;&#23454;&#29616;&#20102;&#26368;&#20339;&#24615;&#33021;&#65292;&#24310;&#36831;&#26497;&#20302;&#65288;&#20165;&#20026;&#20854;&#20182;SRL&#26041;&#27861;&#30340;0.8%&#65289;&#19988;&#20855;&#26377;&#26497;&#39640;&#30340;&#33021;&#37327;&#25928;&#29575;&#65288;&#39640;&#36798;DNNs&#30340;5&#20493;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, spiking neural networks (SNNs) have been used in reinforcement learning (RL) due to their low power consumption and event-driven features. However, spiking reinforcement learning (SRL), which suffers from fixed coding methods, still faces the problems of high latency and poor versatility. In this paper, we use learnable matrix multiplication to encode and decode spikes, improving the flexibility of the coders and thus reducing latency. Meanwhile, we train the SNNs using the direct training method and use two different structures for online and offline RL algorithms, which gives our model a wider range of applications. Extensive experiments have revealed that our method achieves optimal performance with ultra-low latency (as low as 0.8% of other SRL methods) and excellent energy efficiency (up to 5X the DNNs) in different algorithms and different environments.
&lt;/p&gt;</description></item></channel></rss>