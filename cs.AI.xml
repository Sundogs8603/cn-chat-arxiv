<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#35770;&#25991;&#26088;&#22312;&#25552;&#20986;&#19968;&#31181;&#39640;&#25928;&#30340;&#26041;&#27861;&#26469;&#20174;&#25193;&#25955;&#27169;&#22411;&#20013;&#25552;&#28860;GANs&#65292;&#24182;&#29992;&#20110;&#22270;&#20687;&#21040;&#22270;&#20687;&#30340;&#36716;&#25442;&#20219;&#21153;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#28789;&#27963;&#30340;&#23454;&#26102;&#22270;&#20687;&#32534;&#36753;&#65292;&#24182;&#26174;&#33879;&#38477;&#20302;&#35757;&#32451;&#19981;&#21516;&#27010;&#24565;&#27169;&#22411;&#30340;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2401.06127</link><description>&lt;p&gt;
E$^{2}$GAN: &#39640;&#25928;&#35757;&#32451;&#22270;&#20687;&#21040;&#22270;&#20687;&#36716;&#25442;&#20219;&#21153;&#30340;&#39640;&#25928;GANs
&lt;/p&gt;
&lt;p&gt;
E$^{2}$GAN: Efficient Training of Efficient GANs for Image-to-Image Translation. (arXiv:2401.06127v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06127
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#26088;&#22312;&#25552;&#20986;&#19968;&#31181;&#39640;&#25928;&#30340;&#26041;&#27861;&#26469;&#20174;&#25193;&#25955;&#27169;&#22411;&#20013;&#25552;&#28860;GANs&#65292;&#24182;&#29992;&#20110;&#22270;&#20687;&#21040;&#22270;&#20687;&#30340;&#36716;&#25442;&#20219;&#21153;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#28789;&#27963;&#30340;&#23454;&#26102;&#22270;&#20687;&#32534;&#36753;&#65292;&#24182;&#26174;&#33879;&#38477;&#20302;&#35757;&#32451;&#19981;&#21516;&#27010;&#24565;&#27169;&#22411;&#30340;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#23454;&#29616;&#28789;&#27963;&#30340;&#23454;&#26102;&#35774;&#22791;&#19978;&#22270;&#20687;&#32534;&#36753;&#65292;&#19968;&#31181;&#39640;&#24230;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#26159;&#21033;&#29992;&#22823;&#35268;&#27169;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#65292;&#20363;&#22914;&#31283;&#23450;&#25193;&#25955; (Stable Diffusion)&#65292;&#29983;&#25104;&#29992;&#20110;&#35757;&#32451;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476; (GANs) &#30340;&#37197;&#23545;&#25968;&#25454;&#38598;&#12290;&#36825;&#31181;&#26041;&#27861;&#26174;&#33879;&#20943;&#36731;&#20102;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#22270;&#20687;&#32534;&#36753;&#26102;&#36890;&#24120;&#30001;&#39640;&#31471;&#21830;&#29992;GPU&#29305;&#23450;&#30340;&#20005;&#26684;&#35201;&#27714;&#12290;&#28982;&#32780;&#65292;&#19982;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#19981;&#21516;&#65292;&#27599;&#20010;&#29983;&#25104;&#30340; GAN &#37117;&#19987;&#38376;&#29992;&#20110;&#29305;&#23450;&#30340;&#22270;&#20687;&#32534;&#36753;&#20219;&#21153;&#65292;&#22240;&#27492;&#38656;&#35201;&#26114;&#36149;&#30340;&#35757;&#32451;&#24037;&#20316;&#26469;&#33719;&#24471;&#21508;&#31181;&#27010;&#24565;&#30340;&#27169;&#22411;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#24182;&#35299;&#20915;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#30740;&#31350;&#26041;&#21521;&#65306;&#33021;&#21542;&#20351;&#20174;&#25193;&#25955;&#27169;&#22411;&#20013;&#25552;&#28860; GANs &#30340;&#36807;&#31243;&#26356;&#21152;&#39640;&#25928;&#65311;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#21019;&#26032;&#25216;&#26415;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#20855;&#26377;&#24191;&#20041;&#29305;&#24449;&#30340;&#22522;&#26412; GAN &#27169;&#22411;&#65292;&#36890;&#36807;&#24494;&#35843;&#36866;&#24212;&#19981;&#21516;&#30340;&#27010;&#24565;&#65292;&#28040;&#38500;&#20102;...
&lt;/p&gt;
&lt;p&gt;
One highly promising direction for enabling flexible real-time on-device image editing is utilizing data distillation by leveraging large-scale text-to-image diffusion models, such as Stable Diffusion, to generate paired datasets used for training generative adversarial networks (GANs). This approach notably alleviates the stringent requirements typically imposed by high-end commercial GPUs for performing image editing with diffusion models. However, unlike text-to-image diffusion models, each distilled GAN is specialized for a specific image editing task, necessitating costly training efforts to obtain models for various concepts. In this work, we introduce and address a novel research direction: can the process of distilling GANs from diffusion models be made significantly more efficient? To achieve this goal, we propose a series of innovative techniques. First, we construct a base GAN model with generalized features, adaptable to different concepts through fine-tuning, eliminating t
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#28608;&#27963;&#26368;&#22823;&#21270;&#26041;&#27861;&#22312;&#23545;&#25239;&#27169;&#22411;&#25805;&#20316;&#20013;&#30340;&#33030;&#24369;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#25805;&#32437;&#29305;&#24449;&#21487;&#35270;&#21270;&#65292;&#20197;&#38544;&#34255;&#29305;&#23450;&#31070;&#32463;&#20803;&#30340;&#21151;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.06122</link><description>&lt;p&gt;
&#29992;&#26799;&#24230;&#24377;&#23556;&#25805;&#32437;&#29305;&#24449;&#21487;&#35270;&#21270;
&lt;/p&gt;
&lt;p&gt;
Manipulating Feature Visualizations with Gradient Slingshots. (arXiv:2401.06122v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06122
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#28608;&#27963;&#26368;&#22823;&#21270;&#26041;&#27861;&#22312;&#23545;&#25239;&#27169;&#22411;&#25805;&#20316;&#20013;&#30340;&#33030;&#24369;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#25805;&#32437;&#29305;&#24449;&#21487;&#35270;&#21270;&#65292;&#20197;&#38544;&#34255;&#29305;&#23450;&#31070;&#32463;&#20803;&#30340;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#33021;&#22815;&#23398;&#20064;&#22797;&#26434;&#32780;&#22810;&#26679;&#21270;&#30340;&#34920;&#31034;&#65292;&#28982;&#32780;&#65292;&#23398;&#20064;&#21040;&#30340;&#27010;&#24565;&#30340;&#35821;&#20041;&#24615;&#36136;&#20173;&#28982;&#26410;&#30693;&#12290;&#35299;&#37322;DNNs&#23398;&#20064;&#21040;&#30340;&#27010;&#24565;&#30340;&#24120;&#29992;&#26041;&#27861;&#26159;&#28608;&#27963;&#26368;&#22823;&#21270;(AM)&#65292;&#23427;&#29983;&#25104;&#19968;&#20010;&#21512;&#25104;&#30340;&#36755;&#20837;&#20449;&#21495;&#65292;&#26368;&#22823;&#21270;&#28608;&#27963;&#32593;&#32476;&#20013;&#30340;&#29305;&#23450;&#31070;&#32463;&#20803;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#36825;&#31181;&#26041;&#27861;&#23545;&#20110;&#23545;&#25239;&#27169;&#22411;&#25805;&#20316;&#30340;&#33030;&#24369;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#25805;&#32437;&#29305;&#24449;&#21487;&#35270;&#21270;&#65292;&#32780;&#19981;&#25913;&#21464;&#27169;&#22411;&#32467;&#26500;&#25110;&#23545;&#27169;&#22411;&#30340;&#20915;&#31574;&#36807;&#31243;&#20135;&#29983;&#26174;&#33879;&#24433;&#21709;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#23545;&#20960;&#20010;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#25928;&#26524;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#38544;&#34255;&#29305;&#23450;&#31070;&#32463;&#20803;&#21151;&#33021;&#30340;&#33021;&#21147;&#65292;&#22312;&#27169;&#22411;&#23457;&#26680;&#36807;&#31243;&#20013;&#20351;&#29992;&#36873;&#25321;&#30340;&#30446;&#26631;&#35299;&#37322;&#23631;&#34109;&#20102;&#21407;&#22987;&#35299;&#37322;&#12290;&#20316;&#20026;&#19968;&#31181;&#34917;&#25937;&#25514;&#26045;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38450;&#27490;&#36825;&#31181;&#25805;&#32437;&#30340;&#38450;&#25252;&#25514;&#26045;&#65292;&#24182;&#25552;&#20379;&#20102;&#23450;&#37327;&#35777;&#25454;&#65292;&#35777;&#26126;&#20102;&#23427;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Neural Networks (DNNs) are capable of learning complex and versatile representations, however, the semantic nature of the learned concepts remains unknown. A common method used to explain the concepts learned by DNNs is Activation Maximization (AM), which generates a synthetic input signal that maximally activates a particular neuron in the network. In this paper, we investigate the vulnerability of this approach to adversarial model manipulations and introduce a novel method for manipulating feature visualization without altering the model architecture or significantly impacting the model's decision-making process. We evaluate the effectiveness of our method on several neural network models and demonstrate its capabilities to hide the functionality of specific neurons by masking the original explanations of neurons with chosen target explanations during model auditing. As a remedy, we propose a protective measure against such manipulations and provide quantitative evidence which 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21483;&#20570;Patchscope&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#26816;&#26597;&#35821;&#35328;&#27169;&#22411;&#30340;&#38544;&#34255;&#34920;&#31034;&#12290;&#35813;&#26694;&#26550;&#19981;&#20165;&#32479;&#19968;&#20102;&#20808;&#21069;&#30340;&#26816;&#26597;&#25216;&#26415;&#65292;&#36824;&#35299;&#20915;&#20102;&#20854;&#20013;&#19968;&#20123;&#38382;&#39064;&#65292;&#24182;&#19988;&#36824;&#24320;&#36767;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.06102</link><description>&lt;p&gt;
Patchscope: &#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#26816;&#26597;&#35821;&#35328;&#27169;&#22411;&#30340;&#38544;&#34255;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Patchscope: A Unifying Framework for Inspecting Hidden Representations of Language Models. (arXiv:2401.06102v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06102
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21483;&#20570;Patchscope&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#26816;&#26597;&#35821;&#35328;&#27169;&#22411;&#30340;&#38544;&#34255;&#34920;&#31034;&#12290;&#35813;&#26694;&#26550;&#19981;&#20165;&#32479;&#19968;&#20102;&#20808;&#21069;&#30340;&#26816;&#26597;&#25216;&#26415;&#65292;&#36824;&#35299;&#20915;&#20102;&#20854;&#20013;&#19968;&#20123;&#38382;&#39064;&#65292;&#24182;&#19988;&#36824;&#24320;&#36767;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#26597;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#38544;&#34255;&#34920;&#31034;&#20013;&#32534;&#30721;&#30340;&#20449;&#24687;&#21487;&#20197;&#35299;&#37322;&#27169;&#22411;&#30340;&#34892;&#20026;&#24182;&#39564;&#35777;&#20854;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#30340;&#19968;&#33268;&#24615;&#12290;&#37492;&#20110;LLM&#29983;&#25104;&#20154;&#31867;&#21487;&#29702;&#35299;&#25991;&#26412;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#24314;&#35758;&#21033;&#29992;&#27169;&#22411;&#26412;&#36523;&#20197;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#20854;&#20869;&#37096;&#34920;&#31034;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#31216;&#20026;Patchscopes&#30340;&#26694;&#26550;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#23427;&#26469;&#22238;&#31572;&#20851;&#20110;LLM&#35745;&#31639;&#30340;&#21508;&#31181;&#30740;&#31350;&#38382;&#39064;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#20808;&#21069;&#22522;&#20110;&#23558;&#34920;&#31034;&#25237;&#24433;&#21040;&#35789;&#27719;&#31354;&#38388;&#21644;&#24178;&#39044;LLM&#35745;&#31639;&#30340;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#65292;&#21487;&#20197;&#30475;&#20316;&#26159;&#35813;&#26694;&#26550;&#30340;&#29305;&#27530;&#23454;&#20363;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;Patchscope&#21487;&#20197;&#24357;&#34917;&#20248;&#21183;&#65292;&#22914;&#26816;&#26597;&#26089;&#26399;&#23618;&#22833;&#36133;&#25110;&#34920;&#36798;&#33021;&#21147;&#19981;&#36275;&#12290;&#38500;&#20102;&#32479;&#19968;&#20808;&#21069;&#30340;&#26816;&#26597;&#25216;&#26415;&#65292;Patchscopes&#36824;&#24320;&#36767;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#65292;&#20363;&#22914;&#20351;&#29992;&#26356;&#24378;&#22823;&#30340;&#27169;&#22411;&#26469;&#35299;&#37322;&#36739;&#23567;&#27169;&#22411;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inspecting the information encoded in hidden representations of large language models (LLMs) can explain models' behavior and verify their alignment with human values. Given the capabilities of LLMs in generating human-understandable text, we propose leveraging the model itself to explain its internal representations in natural language. We introduce a framework called Patchscopes and show how it can be used to answer a wide range of research questions about an LLM's computation. We show that prior interpretability methods based on projecting representations into the vocabulary space and intervening on the LLM computation, can be viewed as special instances of this framework. Moreover, several of their shortcomings such as failure in inspecting early layers or lack of expressivity can be mitigated by a Patchscope. Beyond unifying prior inspection techniques, Patchscopes also opens up new possibilities such as using a more capable model to explain the representations of a smaller model,
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#25991;&#26412;&#29983;&#25104;&#25216;&#26415;&#21644;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#35757;&#32451;&#20102;&#20960;&#31181;&#21464;&#31181;&#30340;&#29983;&#29289;&#21307;&#23398;&#29983;&#25104;&#39044;&#35757;&#32451;&#21464;&#21387;&#22120;&#27169;&#22411;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#33258;&#21160;&#34917;&#20840;&#24037;&#20855;&#65292;&#21487;&#20026;&#19977;&#32423;&#25252;&#29702;&#20154;&#21592;&#25552;&#20379;&#20934;&#30830;&#21644;&#26684;&#24335;&#33391;&#22909;&#30340;&#20027;&#35201;&#30151;&#29366;&#30701;&#35821;&#25110;&#21477;&#23376;&#12290;</title><link>http://arxiv.org/abs/2401.06088</link><description>&lt;p&gt;
&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#20013;&#33258;&#21160;&#34917;&#20840;&#20027;&#35201;&#30151;&#29366;
&lt;/p&gt;
&lt;p&gt;
Autocompletion of Chief Complaints in the Electronic Health Records using Large Language Models. (arXiv:2401.06088v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06088
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#25991;&#26412;&#29983;&#25104;&#25216;&#26415;&#21644;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#35757;&#32451;&#20102;&#20960;&#31181;&#21464;&#31181;&#30340;&#29983;&#29289;&#21307;&#23398;&#29983;&#25104;&#39044;&#35757;&#32451;&#21464;&#21387;&#22120;&#27169;&#22411;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#33258;&#21160;&#34917;&#20840;&#24037;&#20855;&#65292;&#21487;&#20026;&#19977;&#32423;&#25252;&#29702;&#20154;&#21592;&#25552;&#20379;&#20934;&#30830;&#21644;&#26684;&#24335;&#33391;&#22909;&#30340;&#20027;&#35201;&#30151;&#29366;&#30701;&#35821;&#25110;&#21477;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#35201;&#30151;&#29366;&#65288;CC&#65289;&#26159;&#24739;&#32773;&#21307;&#30103;&#35760;&#24405;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65292;&#23427;&#25551;&#36848;&#20102;&#23547;&#27714;&#21307;&#30103;&#20445;&#20581;&#30340;&#20027;&#35201;&#21407;&#22240;&#25110;&#20851;&#27880;&#28857;&#12290;&#23427;&#20026;&#21307;&#30103;&#20445;&#20581;&#25552;&#20379;&#32773;&#25552;&#20379;&#20102;&#20851;&#38190;&#20449;&#24687;&#65292;&#20197;&#20415;&#20570;&#20986;&#26377;&#26681;&#25454;&#30340;&#24739;&#32773;&#25252;&#29702;&#20915;&#31574;&#12290;&#28982;&#32780;&#65292;&#20026;&#21307;&#30103;&#20445;&#20581;&#25552;&#20379;&#32773;&#35760;&#24405;CC&#21487;&#33021;&#32791;&#26102;&#65292;&#23588;&#20854;&#26159;&#22312;&#32321;&#24537;&#30340;&#24613;&#35786;&#31185;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#22312;&#20020;&#24202;&#35760;&#24405;&#20013;&#20026;&#19977;&#32423;&#25252;&#29702;&#20154;&#21592;&#25552;&#20379;&#20934;&#30830;&#21644;&#26684;&#24335;&#33391;&#22909;&#30340;&#30701;&#35821;&#25110;&#21477;&#23376;&#30340;&#33258;&#21160;&#34917;&#20840;&#24037;&#20855;&#21487;&#20197;&#25104;&#20026;&#23453;&#36149;&#30340;&#36164;&#28304;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#25991;&#26412;&#29983;&#25104;&#25216;&#26415;&#20351;&#29992;CC&#25968;&#25454;&#24320;&#21457;&#20102;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#22312;&#25105;&#20204;&#30340;&#25552;&#35758;&#20013;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#20010;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#27169;&#22411;&#65292;&#24182;&#24494;&#35843;&#20102;&#19977;&#31181;&#19981;&#21516;&#21464;&#31181;&#30340;&#29983;&#29289;&#21307;&#23398;&#29983;&#25104;&#39044;&#35757;&#32451;&#21464;&#21387;&#22120;&#65288;BioGPT&#65289;&#65292;&#20998;&#21035;&#26159;microsoft/biogpt&#65292;microsoft/BioGPT-Large&#21644;microsoft/BioGPT-Large-PubMedQA&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#32467;&#21512;&#20856;&#22411;&#30340;CC&#21477;&#23376;&#65292;&#21033;&#29992;GPT&#30340;OpenAI API&#26469;&#35843;&#25972;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Chief Complaint (CC) is a crucial component of a patient's medical record as it describes the main reason or concern for seeking medical care. It provides critical information for healthcare providers to make informed decisions about patient care. However, documenting CCs can be time-consuming for healthcare providers, especially in busy emergency departments. To address this issue, an autocompletion tool that suggests accurate and well-formatted phrases or sentences for clinical notes can be a valuable resource for triage nurses. In this study, we utilized text generation techniques to develop machine learning models using CC data. In our proposed work, we train a Long Short-Term Memory (LSTM) model and fine-tune three different variants of Biomedical Generative Pretrained Transformers (BioGPT), namely microsoft/biogpt, microsoft/BioGPT-Large, and microsoft/BioGPT-Large-PubMedQA. Additionally, we tune a prompt by incorporating exemplar CC sentences, utilizing the OpenAI API of GPT
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#22312;&#22522;&#20110;&#20195;&#29702;&#27169;&#22411;&#30340;&#20307;&#32946;&#21338;&#24425;&#20132;&#26131;&#20013;&#20351;&#29992;XGBoost&#23398;&#20064;&#21040;&#30340;&#21160;&#24577;&#25237;&#27880;&#31574;&#30053;&#65292;&#24182;&#36890;&#36807;&#22522;&#20110;&#20195;&#29702;&#27169;&#22411;&#30340;&#27169;&#25311;&#22120;&#36827;&#34892;&#23454;&#39564;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2401.06086</link><description>&lt;p&gt;
XGBoost&#23398;&#20064;&#29992;&#20110;&#22522;&#20110;&#20195;&#29702;&#27169;&#22411;&#30340;&#20307;&#32946;&#21338;&#24425;&#20132;&#26131;&#20013;&#30340;&#21160;&#24577;&#25237;&#27880;&#26041;&#24335;
&lt;/p&gt;
&lt;p&gt;
XGBoost Learning of Dynamic Wager Placement for In-Play Betting on an Agent-Based Model of a Sports Betting Exchange. (arXiv:2401.06086v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06086
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22312;&#22522;&#20110;&#20195;&#29702;&#27169;&#22411;&#30340;&#20307;&#32946;&#21338;&#24425;&#20132;&#26131;&#20013;&#20351;&#29992;XGBoost&#23398;&#20064;&#21040;&#30340;&#21160;&#24577;&#25237;&#27880;&#31574;&#30053;&#65292;&#24182;&#36890;&#36807;&#22522;&#20110;&#20195;&#29702;&#27169;&#22411;&#30340;&#27169;&#25311;&#22120;&#36827;&#34892;&#23454;&#39564;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#20171;&#32461;&#20102;&#22312;Bristol Betting Exchange (BBE)&#20013;&#20351;&#29992;&#39640;&#25928;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;XGBoost&#30340;&#32467;&#26524;&#12290;BBE&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;&#22522;&#20110;&#20195;&#29702;&#27169;&#22411;&#30340;&#27169;&#25311;&#22120;&#65292;&#26088;&#22312;&#27169;&#25311;&#20855;&#26377;&#36187;&#20107;&#20013;&#25237;&#27880;&#21151;&#33021;&#30340;&#29616;&#20195;&#20307;&#32946;&#21338;&#24425;&#20132;&#26131;&#12290;&#25105;&#20204;&#20351;&#29992;BBE&#27169;&#25311;&#22120;&#21644;&#20854;&#19968;&#31995;&#21015;&#31616;&#21333;&#30340;&#36172;&#24466;&#20195;&#29702;&#20316;&#20026;&#25968;&#25454;&#29983;&#25104;&#22120;&#65292;&#36755;&#20837;&#21040;&#25105;&#20204;&#30340;XGBoost&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#20013;&#12290;&#36890;&#36807;&#23398;&#20064;BBE&#36172;&#24466;&#20195;&#29702;&#25152;&#36827;&#34892;&#30340;&#26356;&#21152;&#26377;&#21033;&#21487;&#22270;&#30340;&#25237;&#27880;&#65292;XGBoost&#33021;&#22815;&#21457;&#29616;&#26377;&#21033;&#21487;&#22270;&#30340;&#21160;&#24577;&#25237;&#27880;&#31574;&#30053;&#12290;&#22312;XGBoost&#35757;&#32451;&#20043;&#21518;&#65292;&#29983;&#25104;&#19968;&#20010;&#25110;&#22810;&#20010;&#20915;&#31574;&#26641;&#65292;&#23558;&#20855;&#26377;&#30001;XGBoost&#23398;&#20064;&#20915;&#31574;&#26641;&#30830;&#23450;&#30340;&#25237;&#27880;&#31574;&#30053;&#30340;&#36172;&#24466;&#20195;&#29702;&#28155;&#21152;&#21040;BBE&#27169;&#25311;&#22120;&#20013;&#65292;&#22312;&#21508;&#31181;&#26465;&#20214;&#21644;&#25237;&#27880;&#24066;&#22330;&#24773;&#26223;&#19979;&#36827;&#34892;&#19968;&#31995;&#21015;&#27604;&#36187;&#30340;&#25237;&#27880;&#65292;&#20197;&#30408;&#21033;&#20316;&#20026;&#20027;&#35201;&#30340;&#27604;&#36739;&#21644;&#35780;&#20272;&#25351;&#26631;&#12290;&#26412;&#30740;&#31350;&#30340;&#21021;&#27493;&#21457;&#29616;&#34920;&#26126;...
&lt;/p&gt;
&lt;p&gt;
We present first results from the use of XGBoost, a highly effective machine learning (ML) method, within the Bristol Betting Exchange (BBE), an open-source agent-based model (ABM) designed to simulate a contemporary sports-betting exchange with in-play betting during track-racing events such as horse races. We use the BBE ABM and its array of minimally-simple bettor-agents as a synthetic data generator which feeds into our XGBoost ML system, with the intention that XGBoost discovers profitable dynamic betting strategies by learning from the more profitable bets made by the BBE bettor-agents. After this XGBoost training, which results in one or more decision trees, a bettor-agent with a betting strategy determined by the XGBoost-learned decision tree(s) is added to the BBE ABM and made to bet on a sequence of races under various conditions and betting-market scenarios, with profitability serving as the primary metric of comparison and evaluation. Our initial findings presented here sho
&lt;/p&gt;</description></item><item><title>&#26412;&#25253;&#21578;&#25506;&#35752;&#20102;&#22312; RLHF &#20013;&#35299;&#20915;&#22870;&#21169;&#24314;&#27169;&#30340;&#20004;&#20010;&#25361;&#25112;&#65292;&#36890;&#36807;&#20351;&#29992;&#22810;&#20010;&#22870;&#21169;&#27169;&#22411;&#30340;&#25237;&#31080;&#26426;&#21046;&#26469;&#27979;&#37327;&#25968;&#25454;&#20013;&#20559;&#22909;&#30340;&#24378;&#24230;&#65292;&#24182;&#35299;&#20915;&#22312;&#29305;&#23450;&#20998;&#24067;&#25968;&#25454;&#19978;&#35757;&#32451;&#22870;&#21169;&#27169;&#22411;&#38590;&#20197;&#25512;&#24191;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.06080</link><description>&lt;p&gt;
RLHF&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#31192;&#23494; Part II: &#22870;&#21169;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Secrets of RLHF in Large Language Models Part II: Reward Modeling. (arXiv:2401.06080v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06080
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25253;&#21578;&#25506;&#35752;&#20102;&#22312; RLHF &#20013;&#35299;&#20915;&#22870;&#21169;&#24314;&#27169;&#30340;&#20004;&#20010;&#25361;&#25112;&#65292;&#36890;&#36807;&#20351;&#29992;&#22810;&#20010;&#22870;&#21169;&#27169;&#22411;&#30340;&#25237;&#31080;&#26426;&#21046;&#26469;&#27979;&#37327;&#25968;&#25454;&#20013;&#20559;&#22909;&#30340;&#24378;&#24230;&#65292;&#24182;&#35299;&#20915;&#22312;&#29305;&#23450;&#20998;&#24067;&#25968;&#25454;&#19978;&#35757;&#32451;&#22870;&#21169;&#27169;&#22411;&#38590;&#20197;&#25512;&#24191;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#24050;&#25104;&#20026;&#23558;&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#31867;&#20215;&#20540;&#21644;&#24847;&#22270;&#23545;&#40784;&#30340;&#20851;&#38190;&#25216;&#26415;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#20135;&#29983;&#26356;&#26377;&#24110;&#21161;&#19988;&#26080;&#23475;&#30340;&#22238;&#24212;&#12290;&#22870;&#21169;&#27169;&#22411;&#34987;&#35757;&#32451;&#20026;&#20154;&#31867;&#20559;&#22909;&#30340;&#20195;&#29702;&#65292;&#20197;&#39537;&#21160;&#24378;&#21270;&#23398;&#20064;&#20248;&#21270;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#22870;&#21169;&#27169;&#22411;&#38754;&#20020;&#20197;&#19979;&#25361;&#25112;&#65306;&#65288;1&#65289;&#25968;&#25454;&#38598;&#20013;&#19981;&#27491;&#30830;&#21644;&#27169;&#31946;&#30340;&#20559;&#22909;&#23545;&#21487;&#33021;&#22952;&#30861;&#22870;&#21169;&#27169;&#22411;&#20934;&#30830;&#25429;&#25417;&#20154;&#31867;&#24847;&#22270;&#12290;&#65288;2&#65289;&#22312;&#29305;&#23450;&#20998;&#24067;&#30340;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#22870;&#21169;&#27169;&#22411;&#24448;&#24448;&#38590;&#20197;&#25512;&#24191;&#21040;&#20998;&#24067;&#20043;&#22806;&#30340;&#31034;&#20363;&#65292;&#24182;&#19988;&#19981;&#36866;&#29992;&#20110;&#36845;&#20195;RLHF&#35757;&#32451;&#12290;&#26412;&#30740;&#31350;&#23581;&#35797;&#35299;&#20915;&#36825;&#20004;&#20010;&#38382;&#39064;&#12290;&#20174;&#25968;&#25454;&#35282;&#24230;&#20986;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#26041;&#27861;&#26469;&#27979;&#37327;&#25968;&#25454;&#20013;&#20559;&#22909;&#30340;&#24378;&#24230;&#65292;&#22522;&#20110;&#22810;&#20010;&#22870;&#21169;&#27169;&#22411;&#30340;&#25237;&#31080;&#26426;&#21046;&#12290;&#23454;&#39564;&#32467;&#26524;...
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning from Human Feedback (RLHF) has become a crucial technology for aligning language models with human values and intentions, enabling models to produce more helpful and harmless responses. Reward models are trained as proxies for human preferences to drive reinforcement learning optimization. While reward models are often considered central to achieving high performance, they face the following challenges in practical applications: (1) Incorrect and ambiguous preference pairs in the dataset may hinder the reward model from accurately capturing human intent. (2) Reward models trained on data from a specific distribution often struggle to generalize to examples outside that distribution and are not suitable for iterative RLHF training.  In this report, we attempt to address these two issues. (1) From a data perspective, we propose a method to measure the strength of preferences within the data, based on a voting mechanism of multiple reward models. Experimental result
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;LLMs&#30340;&#26032;&#26041;&#27861;&#65292;&#23558;&#26102;&#38388;&#30693;&#35782;&#22270;&#34917;&#20840;&#20219;&#21153;&#27010;&#24565;&#21270;&#20026;&#21382;&#21490;&#20107;&#20214;&#38142;&#20013;&#30340;&#20107;&#20214;&#29983;&#25104;&#20219;&#21153;&#12290;&#36890;&#36807;&#24341;&#20837;&#39640;&#25928;&#30340;&#24494;&#35843;&#26041;&#27861;&#21644;&#32467;&#26500;&#21270;&#21382;&#21490;&#25968;&#25454;&#22686;&#24378;&#65292;&#20197;&#21450;&#25972;&#21512;&#21453;&#21521;&#30693;&#35782;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#22810;&#20010;&#25351;&#26631;&#19978;&#20248;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#65292;&#21462;&#24471;&#20102;SOTA&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.06072</link><description>&lt;p&gt;
&#21382;&#21490;&#38142;&#30340;&#38142;&#36335;&#39044;&#27979;&#19982;&#23398;&#20064;&#65306;&#22522;&#20110;LLMs&#30340;&#26102;&#38388;&#30693;&#35782;&#22270;&#34917;&#20840;
&lt;/p&gt;
&lt;p&gt;
Chain of History: Learning and Forecasting with LLMs for Temporal Knowledge Graph Completion. (arXiv:2401.06072v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06072
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;LLMs&#30340;&#26032;&#26041;&#27861;&#65292;&#23558;&#26102;&#38388;&#30693;&#35782;&#22270;&#34917;&#20840;&#20219;&#21153;&#27010;&#24565;&#21270;&#20026;&#21382;&#21490;&#20107;&#20214;&#38142;&#20013;&#30340;&#20107;&#20214;&#29983;&#25104;&#20219;&#21153;&#12290;&#36890;&#36807;&#24341;&#20837;&#39640;&#25928;&#30340;&#24494;&#35843;&#26041;&#27861;&#21644;&#32467;&#26500;&#21270;&#21382;&#21490;&#25968;&#25454;&#22686;&#24378;&#65292;&#20197;&#21450;&#25972;&#21512;&#21453;&#21521;&#30693;&#35782;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#22810;&#20010;&#25351;&#26631;&#19978;&#20248;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#65292;&#21462;&#24471;&#20102;SOTA&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#30693;&#35782;&#22270;&#34917;&#20840;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#20854;&#36890;&#36807;&#21033;&#29992;&#24050;&#24314;&#31435;&#30340;&#26102;&#38388;&#32467;&#26500;&#30693;&#35782;&#26469;&#39044;&#27979;&#26410;&#26469;&#26102;&#38388;&#25139;&#19978;&#32570;&#22833;&#30340;&#20107;&#20214;&#38142;&#25509;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#23558;&#26102;&#38388;&#38142;&#36335;&#39044;&#27979;&#27010;&#24565;&#21270;&#20026;&#21382;&#21490;&#20107;&#20214;&#38142;&#20013;&#30340;&#20107;&#20214;&#29983;&#25104;&#20219;&#21153;&#65292;&#21033;&#29992;LLMs&#30340;&#24378;&#22823;&#29983;&#25104;&#33021;&#21147;&#12290;&#25105;&#20204;&#37319;&#29992;&#39640;&#25928;&#30340;&#24494;&#35843;&#26041;&#27861;&#65292;&#20351;LLMs&#36866;&#24212;&#29305;&#23450;&#30340;&#22270;&#25991;&#20449;&#24687;&#21644;&#22312;&#26102;&#38388;&#32447;&#20013;&#21457;&#29616;&#30340;&#27169;&#24335;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#22522;&#20110;&#32467;&#26500;&#30340;&#21382;&#21490;&#25968;&#25454;&#22686;&#24378;&#21644;&#21453;&#21521;&#30693;&#35782;&#30340;&#25972;&#21512;&#65292;&#20197;&#22686;&#24378;LLMs&#23545;&#32467;&#26500;&#20449;&#24687;&#30340;&#24847;&#35782;&#65292;&#20174;&#32780;&#25552;&#39640;&#20854;&#25512;&#29702;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#24191;&#27867;&#20351;&#29992;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35814;&#23613;&#30340;&#23454;&#39564;&#65292;&#21457;&#29616;&#25105;&#20204;&#24494;&#35843;&#30340;&#27169;&#22411;&#22312;&#22810;&#20010;&#25351;&#26631;&#19978;&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#20110;&#23884;&#20837;&#30340;&#27169;&#22411;&#65292;&#21462;&#24471;&#20102;SOTA&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#20805;&#20998;&#30340;&#28040;&#34701;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Temporal Knowledge Graph Completion (TKGC) is a challenging task of predicting missing event links at future timestamps by leveraging established temporal structural knowledge. Given the formidable generative capabilities inherent in LLMs (LLMs), this paper proposes a novel approach to conceptualize temporal link prediction as an event generation task within the context of a historical event chain. We employ efficient fine-tuning methods to make LLMs adapt to specific graph textual information and patterns discovered in temporal timelines. Furthermore, we introduce structure-based historical data augmentation and the integration of reverse knowledge to emphasize LLMs' awareness of structural information, thereby enhancing their reasoning capabilities. We conduct thorough experiments on multiple widely used datasets and find that our fine-tuned model outperforms existing embedding-based models on multiple metrics, achieving SOTA results. We also carry out sufficient ablation experiments
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#35843;&#26597;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25968;&#25454;&#27745;&#26579;&#38382;&#39064;&#65292;&#20197;&#21450;&#35813;&#27745;&#26579;&#23545;&#19979;&#28216;&#20219;&#21153;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2401.06059</link><description>&lt;p&gt;
&#30740;&#31350;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#25968;&#25454;&#27745;&#26579;
&lt;/p&gt;
&lt;p&gt;
Investigating Data Contamination for Pre-training Language Models. (arXiv:2401.06059v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06059
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#35843;&#26597;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25968;&#25454;&#27745;&#26579;&#38382;&#39064;&#65292;&#20197;&#21450;&#35813;&#27745;&#26579;&#23545;&#19979;&#28216;&#20219;&#21153;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#35268;&#27169;&#32593;&#32476;&#35821;&#26009;&#24211;&#19978;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#19978;&#23637;&#31034;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36234;&#26469;&#36234;&#25285;&#24515;&#36825;&#31181;&#33021;&#21147;&#26159;&#21542;&#26159;&#30001;&#20110;&#35780;&#20272;&#25968;&#25454;&#38598;&#34987;&#21253;&#21547;&#22312;&#39044;&#35757;&#32451;&#35821;&#26009;&#24211;&#20013;&#23548;&#33268;&#30340;&#65292;&#36825;&#31181;&#29616;&#35937;&#34987;&#31216;&#20026;&#8220;&#25968;&#25454;&#27745;&#26579;&#8221;&#65292;&#20174;&#32780;&#22312;&#20154;&#24037;&#25552;&#39640;&#24615;&#33021;&#12290;&#30446;&#21069;&#23545;&#36825;&#31181;&#28508;&#22312;&#27745;&#26579;&#22914;&#20309;&#24433;&#21709;&#35821;&#35328;&#27169;&#22411;&#22312;&#19979;&#28216;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#32570;&#20047;&#20102;&#35299;&#12290;&#26412;&#25991;&#36890;&#36807;&#20174;&#22836;&#24320;&#22987;&#39044;&#35757;&#32451;&#19968;&#31995;&#21015;GPT-2&#27169;&#22411;&#65292;&#25506;&#35752;&#20102;&#22312;&#39044;&#35757;&#32451;&#38454;&#27573;&#25968;&#25454;&#27745;&#26579;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#24378;&#35843;&#20102;&#26469;&#33258;&#35780;&#20272;&#25968;&#25454;&#30340;&#25991;&#26412;&#27745;&#26579;&#65288;&#21363;&#36755;&#20837;&#25991;&#26412;&#30340;&#35780;&#20272;&#26679;&#26412;&#65289;&#21644;&#22522;&#20934;&#27745;&#26579;&#65288;&#21363;&#36755;&#20837;&#20013;&#30340;&#25552;&#31034;&#21644;&#26399;&#26395;&#36755;&#20986;&#65289;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#20013;&#37325;&#22797;&#27745;&#26579;&#30340;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35843;&#26597;&#20102;&#26222;&#36941;&#23384;&#22312;&#30340;n
&lt;/p&gt;
&lt;p&gt;
Language models pre-trained on web-scale corpora demonstrate impressive capabilities on diverse downstream tasks. However, there is increasing concern whether such capabilities might arise from evaluation datasets being included in the pre-training corpus -- a phenomenon known as \textit{data contamination} -- in a manner that artificially increases performance. There has been little understanding of how this potential contamination might influence LMs' performance on downstream tasks. In this paper, we explore the impact of data contamination at the pre-training stage by pre-training a series of GPT-2 models \textit{from scratch}. We highlight the effect of both text contamination (\textit{i.e.}\ input text of the evaluation samples) and ground-truth contamination (\textit{i.e.}\ the prompts asked on the input and the desired outputs) from evaluation data. We also investigate the effects of repeating contamination for various downstream tasks. Additionally, we examine the prevailing n
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22235;&#31181;&#22270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#30340;&#24615;&#33021;&#20197;&#21450;&#36890;&#36807;&#24212;&#29992;&#19981;&#21516;&#30340;&#20154;&#24037;&#29305;&#24449;&#22686;&#24378;&#31574;&#30053;&#26469;&#25552;&#39640;&#22270;&#20998;&#31867;&#20219;&#21153;&#30340;&#20934;&#30830;&#24615;&#65292;&#32467;&#26524;&#34920;&#26126;GNN&#26550;&#26500;&#30340;&#35745;&#31639;&#33021;&#21147;&#21644;&#20154;&#24037;&#29305;&#24449;&#25552;&#20379;&#30340;&#20449;&#24687;&#27700;&#24179;&#23545;&#20219;&#21153;&#30340;&#24615;&#33021;&#20855;&#26377;&#24179;&#34913;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.06048</link><description>&lt;p&gt;
&#36890;&#36807;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#29305;&#24449;&#22686;&#24378;&#31574;&#30053;&#26469;&#20998;&#31867;&#31038;&#20132;&#32593;&#32476;&#30340;&#33021;&#21147;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Power of Graph Neural Networks and Feature Augmentation Strategies to Classify Social Networks. (arXiv:2401.06048v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06048
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22235;&#31181;&#22270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#30340;&#24615;&#33021;&#20197;&#21450;&#36890;&#36807;&#24212;&#29992;&#19981;&#21516;&#30340;&#20154;&#24037;&#29305;&#24449;&#22686;&#24378;&#31574;&#30053;&#26469;&#25552;&#39640;&#22270;&#20998;&#31867;&#20219;&#21153;&#30340;&#20934;&#30830;&#24615;&#65292;&#32467;&#26524;&#34920;&#26126;GNN&#26550;&#26500;&#30340;&#35745;&#31639;&#33021;&#21147;&#21644;&#20154;&#24037;&#29305;&#24449;&#25552;&#20379;&#30340;&#20449;&#24687;&#27700;&#24179;&#23545;&#20219;&#21153;&#30340;&#24615;&#33021;&#20855;&#26377;&#24179;&#34913;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22235;&#31181;&#22270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65288;GNNs&#65289;&#22312;&#20351;&#29992;&#32463;&#20856;&#30340;&#32593;&#32476;&#31185;&#23398;&#29983;&#25104;&#27169;&#22411;&#21019;&#24314;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#22270;&#20998;&#31867;&#20219;&#21153;&#12290;&#30001;&#20110;&#21512;&#25104;&#32593;&#32476;&#19981;&#21253;&#21547;&#65288;&#33410;&#28857;&#25110;&#36793;&#65289;&#29305;&#24449;&#65292;&#22240;&#27492;&#23545;&#33410;&#28857;&#24212;&#29992;&#20102;&#20116;&#31181;&#19981;&#21516;&#30340;&#22686;&#24378;&#31574;&#30053;&#65288;&#20154;&#36896;&#29305;&#24449;&#31867;&#22411;&#65289;&#12290;&#30740;&#31350;&#20102;4&#31181;GNNs&#65288;&#20855;&#26377;&#23618;&#27425;&#21644;&#20840;&#23616;&#32858;&#21512;&#30340;GCN&#12289;GIN&#21644;GATv2&#65289;&#21644;5&#31181;&#29305;&#24449;&#31867;&#22411;&#65288;&#24120;&#25968;1&#12289;&#22122;&#22768;&#12289;&#24230;&#12289;&#24402;&#19968;&#21270;&#24230;&#21644;ID - &#21508;&#31181;&#38271;&#24230;&#30340;&#24490;&#29615;&#25968;&#37327;&#30340;&#21521;&#37327;&#65289;&#30340;&#25152;&#26377;&#32452;&#21512;&#65292;&#24182;&#22312;GNNs&#20013;&#20351;&#29992;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#30340;&#38544;&#34255;&#32500;&#24230;&#20316;&#20026;&#20989;&#25968;&#27604;&#36739;&#20854;&#24615;&#33021;&#12290;&#36824;&#20351;&#29992;&#31532;&#20108;&#20010;&#21512;&#25104;&#32593;&#32476;&#25968;&#25454;&#38598;&#20998;&#26512;&#20102;&#36825;&#20123;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#65288;&#21253;&#21547;&#19981;&#21516;&#35268;&#27169;&#30340;&#32593;&#32476;&#65289;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;GNN&#26550;&#26500;&#30340;&#35745;&#31639;&#33021;&#21147;&#21644;&#20154;&#24037;&#29305;&#24449;&#25552;&#20379;&#30340;&#20449;&#24687;&#27700;&#24179;&#30340;&#24179;&#34913;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies four Graph Neural Network architectures (GNNs) for a graph classification task on a synthetic dataset created using classic generative models of Network Science. Since the synthetic networks do not contain (node or edge) features, five different augmentation strategies (artificial feature types) are applied to nodes. All combinations of the 4 GNNs (GCN with Hierarchical and Global aggregation, GIN and GATv2) and the 5 feature types (constant 1, noise, degree, normalized degree and ID -- a vector of the number of cycles of various lengths) are studied and their performances compared as a function of the hidden dimension of artificial neural networks used in the GNNs. The generalisation ability of these models is also analysed using a second synthetic network dataset (containing networks of different sizes).Our results point towards the balanced importance of the computational power of the GNN architecture and the the information level provided by the artificial featur
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#36866;&#37197;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#38024;&#23545;&#20869;&#31397;&#38236;&#25163;&#26415;&#20013;&#30340;&#28145;&#24230;&#20272;&#35745;&#38382;&#39064;&#36827;&#34892;&#20102;&#25913;&#36827;&#12290;&#36890;&#36807;&#22312;DINO&#27169;&#22411;&#20013;&#26500;&#24314;LoRA&#23618;&#65292;&#24182;&#23558;&#20854;&#19982;&#25163;&#26415;&#22330;&#26223;&#30340;&#29305;&#24449;&#32467;&#21512;&#36215;&#26469;&#65292;&#23454;&#29616;&#20102;&#25163;&#26415;&#29305;&#23450;&#30340;&#28145;&#24230;&#20272;&#35745;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#20869;&#31397;&#38236;&#25163;&#26415;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.06013</link><description>&lt;p&gt;
&#22312;&#20869;&#31397;&#38236;&#25163;&#26415;&#20013;&#28145;&#24230;&#20272;&#35745;&#30340;&#25163;&#26415;DINO&#65306;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#36866;&#37197;&#22120;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Surgical-DINO: Adapter Learning of Foundation Model for Depth Estimation in Endoscopic Surgery. (arXiv:2401.06013v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06013
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#36866;&#37197;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#38024;&#23545;&#20869;&#31397;&#38236;&#25163;&#26415;&#20013;&#30340;&#28145;&#24230;&#20272;&#35745;&#38382;&#39064;&#36827;&#34892;&#20102;&#25913;&#36827;&#12290;&#36890;&#36807;&#22312;DINO&#27169;&#22411;&#20013;&#26500;&#24314;LoRA&#23618;&#65292;&#24182;&#23558;&#20854;&#19982;&#25163;&#26415;&#22330;&#26223;&#30340;&#29305;&#24449;&#32467;&#21512;&#36215;&#26469;&#65292;&#23454;&#29616;&#20102;&#25163;&#26415;&#29305;&#23450;&#30340;&#28145;&#24230;&#20272;&#35745;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#20869;&#31397;&#38236;&#25163;&#26415;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#30340;&#65306;&#22312;&#26426;&#22120;&#20154;&#25163;&#26415;&#20013;&#36827;&#34892;&#28145;&#24230;&#20272;&#35745;&#23545;&#20110;&#19977;&#32500;&#37325;&#24314;&#12289;&#25163;&#26415;&#23548;&#33322;&#21644;&#22686;&#24378;&#29616;&#23454;&#21487;&#35270;&#21270;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#22522;&#30784;&#27169;&#22411;&#22312;&#35768;&#22810;&#35270;&#35273;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#21253;&#25324;&#28145;&#24230;&#20272;&#35745;&#65288;&#22914;DINOv2&#65289;&#65292;&#20294;&#26368;&#36817;&#30340;&#30740;&#31350;&#35266;&#23519;&#21040;&#20854;&#22312;&#21307;&#23398;&#21644;&#25163;&#26415;&#29305;&#23450;&#24212;&#29992;&#20013;&#30340;&#23616;&#38480;&#24615;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25163;&#26415;&#28145;&#24230;&#20272;&#35745;&#30340;&#20302;&#31209;&#36866;&#37197;&#65288;LoRA&#65289;&#22522;&#30784;&#27169;&#22411;&#12290;&#26041;&#27861;&#65306;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#28145;&#24230;&#20272;&#35745;&#26041;&#27861;&#65292;&#31216;&#20026;Surgical-DINO&#65292;&#36825;&#26159;DINOv2&#22312;&#20869;&#31397;&#38236;&#25163;&#26415;&#20013;&#28145;&#24230;&#20272;&#35745;&#20013;&#30340;&#20302;&#31209;&#36866;&#37197;&#12290;&#25105;&#20204;&#22312;DINO&#20013;&#26500;&#24314;&#20102;LoRA&#23618;&#65292;&#24182;&#23558;&#20854;&#25972;&#21512;&#21040;DINO&#20013;&#65292;&#20197;&#36866;&#24212;&#25163;&#26415;&#29305;&#23450;&#30340;&#39046;&#22495;&#30693;&#35782;&#65292;&#32780;&#19981;&#26159;&#20256;&#32479;&#30340;&#24494;&#35843;&#26041;&#27861;&#12290;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#20923;&#32467;&#20102;DINO&#30340;&#22270;&#20687;&#32534;&#30721;&#22120;&#65292;&#35813;&#32534;&#30721;&#22120;&#34920;&#29616;&#20986;&#20248;&#31168;&#30340;&#35270;&#35273;&#34920;&#31034;&#33021;&#21147;&#65292;&#21482;&#20248;&#21270;&#20102;LoRA&#23618;&#21644;&#28145;&#24230;&#35299;&#30721;&#22120;&#65292;&#20197;&#25972;&#21512;&#25163;&#26415;&#22330;&#26223;&#30340;&#29305;&#24449;&#12290;&#32467;&#26524;&#65306;&#25105;&#20204;&#30340;&#27169;&#22411;&#26159;ex...
&lt;/p&gt;
&lt;p&gt;
Purpose: Depth estimation in robotic surgery is vital in 3D reconstruction, surgical navigation and augmented reality visualization. Although the foundation model exhibits outstanding performance in many vision tasks, including depth estimation (e.g., DINOv2), recent works observed its limitations in medical and surgical domain-specific applications. This work presents a low-ranked adaptation (LoRA) of the foundation model for surgical depth estimation. Methods: We design a foundation model-based depth estimation method, referred to as Surgical-DINO, a low-rank adaptation of the DINOv2 for depth estimation in endoscopic surgery. We build LoRA layers and integrate them into DINO to adapt with surgery-specific domain knowledge instead of conventional fine-tuning. During training, we freeze the DINO image encoder, which shows excellent visual representation capacity, and only optimize the LoRA layers and depth decoder to integrate features from the surgical scene. Results: Our model is ex
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25506;&#35752;&#20102;&#38750;&#20154;&#28789;&#38271;&#31867;&#21160;&#29289;&#22823;&#33041;&#22914;&#20309;&#22312;&#35270;&#35273;&#20013;&#32467;&#21512;&#29983;&#25104;&#24335;&#21644;&#21028;&#21035;&#24335;&#35745;&#31639;&#12290;&#19968;&#20010;&#35266;&#24565;&#24378;&#35843;&#33258;&#19979;&#32780;&#19978;&#30340;&#20449;&#21495;&#27969;&#21160;&#65292;&#36890;&#36807;&#28388;&#38500;&#19981;&#30456;&#20851;&#30340;&#21464;&#24322;&#21644;&#36716;&#25442;&#35270;&#35273;&#20449;&#24687;&#26469;&#20195;&#34920;&#34892;&#20026;&#19978;&#30456;&#20851;&#30340;&#20449;&#24687;&#65307;&#32780;&#21478;&#19968;&#20010;&#35266;&#24565;&#23558;&#35270;&#35273;&#35270;&#20026;Helmholtz&#30340;&#25512;&#29702;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2401.06005</link><description>&lt;p&gt;
&#38750;&#20154;&#28789;&#38271;&#31867;&#21160;&#29289;&#22823;&#33041;&#22914;&#20309;&#22312;&#35270;&#35273;&#20013;&#32467;&#21512;&#29983;&#25104;&#24335;&#21644;&#21028;&#21035;&#24335;&#35745;&#31639;&#65311;
&lt;/p&gt;
&lt;p&gt;
How does the primate brain combine generative and discriminative computations in vision?. (arXiv:2401.06005v1 [q-bio.NC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06005
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25506;&#35752;&#20102;&#38750;&#20154;&#28789;&#38271;&#31867;&#21160;&#29289;&#22823;&#33041;&#22914;&#20309;&#22312;&#35270;&#35273;&#20013;&#32467;&#21512;&#29983;&#25104;&#24335;&#21644;&#21028;&#21035;&#24335;&#35745;&#31639;&#12290;&#19968;&#20010;&#35266;&#24565;&#24378;&#35843;&#33258;&#19979;&#32780;&#19978;&#30340;&#20449;&#21495;&#27969;&#21160;&#65292;&#36890;&#36807;&#28388;&#38500;&#19981;&#30456;&#20851;&#30340;&#21464;&#24322;&#21644;&#36716;&#25442;&#35270;&#35273;&#20449;&#24687;&#26469;&#20195;&#34920;&#34892;&#20026;&#19978;&#30456;&#20851;&#30340;&#20449;&#24687;&#65307;&#32780;&#21478;&#19968;&#20010;&#35266;&#24565;&#23558;&#35270;&#35273;&#35270;&#20026;Helmholtz&#30340;&#25512;&#29702;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#34987;&#24191;&#27867;&#29702;&#35299;&#20026;&#19968;&#31181;&#25512;&#29702;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#22312;&#29983;&#29289;&#35270;&#35273;&#30740;&#31350;&#21644;&#26426;&#22120;&#35270;&#35273;&#24037;&#31243;&#20013;&#65292;&#20851;&#20110;&#25512;&#29702;&#36807;&#31243;&#30340;&#20004;&#31181;&#23545;&#31435;&#35266;&#24565;&#37117;&#20855;&#26377;&#24433;&#21709;&#21147;&#12290;&#31532;&#19968;&#31181;&#24378;&#35843;&#33258;&#19979;&#32780;&#19978;&#30340;&#20449;&#21495;&#27969;&#21160;&#65292;&#23558;&#35270;&#35273;&#25551;&#36848;&#20026;&#19968;&#31181;&#20027;&#35201;&#26159;&#21069;&#39304;&#30340;&#21028;&#21035;&#25512;&#29702;&#36807;&#31243;&#65292;&#20854;&#36890;&#36807;&#28388;&#38500;&#19981;&#30456;&#20851;&#30340;&#21464;&#24322;&#21644;&#20197;&#36866;&#21512;&#19979;&#28216;&#30340;&#35748;&#30693;&#21644;&#34892;&#20026;&#25511;&#21046;&#21151;&#33021;&#38656;&#35201;&#30340;&#26041;&#24335;&#26469;&#36716;&#25442;&#35270;&#35273;&#20449;&#24687;&#65292;&#24182;&#20195;&#34920;&#34892;&#20026;&#19978;&#30456;&#20851;&#20449;&#24687;&#12290;&#22312;&#36825;&#31181;&#35266;&#24565;&#20013;&#65292;&#35270;&#35273;&#30001;&#24863;&#23448;&#25968;&#25454;&#39537;&#21160;&#65292;&#24863;&#30693;&#26159;&#30452;&#25509;&#30340;&#65292;&#22240;&#20026;&#22788;&#29702;&#20174;&#25968;&#25454;&#21040;&#24863;&#20852;&#36259;&#30340;&#28508;&#21464;&#37327;&#30340;&#36807;&#31243;&#36827;&#34892;&#12290;&#22312;&#36825;&#31181;&#35266;&#24565;&#20013;&#65292;"&#25512;&#29702;"&#30340;&#27010;&#24565;&#31867;&#20284;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#24037;&#31243;&#25991;&#29486;&#20013;&#25551;&#36848;&#30340;&#65292;&#23545;&#22270;&#20687;&#36827;&#34892;&#22788;&#29702;&#30340;&#21069;&#39304;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#34987;&#35748;&#20026;&#26159;&#22312;&#25191;&#34892;&#25512;&#29702;&#12290;&#32780;&#21478;&#19968;&#31181;&#35266;&#24565;&#26159;&#23558;&#35270;&#35273;&#35270;&#20026;Helmholtz&#30340;&#25512;&#29702;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision is widely understood as an inference problem. However, two contrasting conceptions of the inference process have each been influential in research on biological vision as well as the engineering of machine vision. The first emphasizes bottom-up signal flow, describing vision as a largely feedforward, discriminative inference process that filters and transforms the visual information to remove irrelevant variation and represent behaviorally relevant information in a format suitable for downstream functions of cognition and behavioral control. In this conception, vision is driven by the sensory data, and perception is direct because the processing proceeds from the data to the latent variables of interest. The notion of "inference" in this conception is that of the engineering literature on neural networks, where feedforward convolutional neural networks processing images are said to perform inference. The alternative conception is that of vision as an inference process in Helmhol
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#22810;&#26234;&#33021;&#20307;&#36777;&#35770;&#23545;&#25239;&#23545;&#25239;&#24615;&#25915;&#20987;&#21487;&#20197;&#38477;&#20302;&#27169;&#22411;&#30340;&#26377;&#27602;&#24615;&#65292;&#21516;&#26102;&#36890;&#36807;&#23884;&#20837;&#32858;&#31867;&#23545;&#23545;&#25239;&#24615;&#25552;&#31034;&#20869;&#23481;&#36827;&#34892;&#20998;&#31867;&#21487;&#20197;&#20998;&#26512;&#19981;&#21516;&#27169;&#22411;&#23545;&#19981;&#21516;&#31867;&#22411;&#25915;&#20987;&#30340;&#26131;&#21463;&#25915;&#20987;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.05998</link><description>&lt;p&gt;
&#20351;&#29992;&#22810;&#26234;&#33021;&#20307;&#36777;&#35770;&#26469;&#23545;&#25239;&#23545;&#25239;&#24615;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Combating Adversarial Attacks with Multi-Agent Debate. (arXiv:2401.05998v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05998
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22810;&#26234;&#33021;&#20307;&#36777;&#35770;&#23545;&#25239;&#23545;&#25239;&#24615;&#25915;&#20987;&#21487;&#20197;&#38477;&#20302;&#27169;&#22411;&#30340;&#26377;&#27602;&#24615;&#65292;&#21516;&#26102;&#36890;&#36807;&#23884;&#20837;&#32858;&#31867;&#23545;&#23545;&#25239;&#24615;&#25552;&#31034;&#20869;&#23481;&#36827;&#34892;&#20998;&#31867;&#21487;&#20197;&#20998;&#26512;&#19981;&#21516;&#27169;&#22411;&#23545;&#19981;&#21516;&#31867;&#22411;&#25915;&#20987;&#30340;&#26131;&#21463;&#25915;&#20987;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#25104;&#26524;&#65292;&#20294;&#23427;&#20204;&#20173;&#28982;&#23481;&#26131;&#21463;&#21040;&#25512;&#29702;&#38454;&#27573;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#65292;&#20363;&#22914;&#30001;&#32418;&#38431;&#29983;&#25104;&#30340;&#23545;&#25239;&#24615;&#25552;&#31034;&#12290;&#20026;&#20102;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#25972;&#20307;&#36136;&#37327;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65306;&#22810;&#26234;&#33021;&#20307;&#36777;&#35770;&#65292;&#20854;&#20013;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#35752;&#35770;&#21644;&#21453;&#39304;&#26469;&#33258;&#25105;&#35780;&#20272;&#12290;&#25105;&#20204;&#23454;&#26045;&#20102;&#26368;&#26032;&#30340;&#35821;&#35328;&#27169;&#22411;&#20043;&#38388;&#30340;&#22810;&#26234;&#33021;&#20307;&#36777;&#35770;&#65292;&#24182;&#35780;&#20272;&#20102;&#27169;&#22411;&#22312;&#21333;&#19968;&#21644;&#22810;&#26234;&#33021;&#20307;&#29615;&#22659;&#20013;&#21463;&#21040;&#32418;&#38431;&#25915;&#20987;&#30340;&#26131;&#21463;&#25915;&#20987;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#24403;&#36234;&#29425;&#25110;&#33021;&#21147;&#36739;&#20302;&#30340;&#27169;&#22411;&#34987;&#36843;&#19982;&#26410;&#36234;&#29425;&#25110;&#33021;&#21147;&#26356;&#24378;&#30340;&#27169;&#22411;&#36827;&#34892;&#36777;&#35770;&#26102;&#65292;&#22810;&#26234;&#33021;&#20307;&#36777;&#35770;&#21487;&#20197;&#20943;&#23569;&#27169;&#22411;&#30340;&#26377;&#27602;&#24615;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#23884;&#20837;&#32858;&#31867;&#23545;&#23545;&#25239;&#24615;&#25552;&#31034;&#20869;&#23481;&#36827;&#34892;&#20998;&#31867;&#65292;&#24182;&#20998;&#26512;&#20102;&#19981;&#21516;&#31867;&#22411;&#25915;&#20987;&#20027;&#39064;&#23545;&#19981;&#21516;&#27169;&#22411;&#30340;&#26131;&#21463;&#25915;&#20987;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
While state-of-the-art language models have achieved impressive results, they remain susceptible to inference-time adversarial attacks, such as adversarial prompts generated by red teams arXiv:2209.07858. One approach proposed to improve the general quality of language model generations is multi-agent debate, where language models self-evaluate through discussion and feedback arXiv:2305.14325. We implement multi-agent debate between current state-of-the-art language models and evaluate models' susceptibility to red team attacks in both single- and multi-agent settings. We find that multi-agent debate can reduce model toxicity when jailbroken or less capable models are forced to debate with non-jailbroken or more capable models. We also find marginal improvements through the general usage of multi-agent interactions. We further perform adversarial prompt content classification via embedding clustering, and analyze the susceptibility of different models to different types of attack topic
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25512;&#33616;&#20013;&#24847;&#22270;&#23398;&#20064;&#30340;&#31471;&#21040;&#31471;&#21487;&#23398;&#20064;&#32858;&#31867;&#26041;&#27861;ELCRec&#65292;&#35813;&#26041;&#27861;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#30340;&#22797;&#26434;&#20248;&#21270;&#38382;&#39064;&#21644;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#32858;&#31867;&#30340;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.05975</link><description>&lt;p&gt;
&#29992;&#20110;&#25512;&#33616;&#20013;&#24847;&#22270;&#23398;&#20064;&#30340;&#31471;&#21040;&#31471;&#21487;&#23398;&#20064;&#32858;&#31867;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
End-to-end Learnable Clustering for Intent Learning in Recommendation. (arXiv:2401.05975v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05975
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25512;&#33616;&#20013;&#24847;&#22270;&#23398;&#20064;&#30340;&#31471;&#21040;&#31471;&#21487;&#23398;&#20064;&#32858;&#31867;&#26041;&#27861;ELCRec&#65292;&#35813;&#26041;&#27861;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#30340;&#22797;&#26434;&#20248;&#21270;&#38382;&#39064;&#21644;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#32858;&#31867;&#30340;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25366;&#25496;&#29992;&#25143;&#30340;&#24847;&#22270;&#22312;&#24207;&#21015;&#25512;&#33616;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#26368;&#36817;&#30340;&#26041;&#27861;ICLRec&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#21644;&#32858;&#31867;&#26469;&#25552;&#21462;&#29992;&#25143;&#30340;&#28508;&#22312;&#24847;&#22270;&#12290;&#23613;&#31649;&#23427;&#24050;&#32463;&#26174;&#31034;&#20986;&#26377;&#25928;&#24615;&#65292;&#20294;&#29616;&#26377;&#30340;&#26041;&#27861;&#23384;&#22312;&#22797;&#26434;&#21644;&#32321;&#29712;&#30340;&#20132;&#26367;&#20248;&#21270;&#38382;&#39064;&#65292;&#23548;&#33268;&#20004;&#20010;&#20027;&#35201;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#22312;&#24191;&#20041;&#26399;&#26395;&#26368;&#22823;&#21270;(EM)&#26694;&#26550;&#20013;&#20998;&#31163;&#34920;&#31034;&#23398;&#20064;&#21644;&#32858;&#31867;&#20248;&#21270;&#32463;&#24120;&#23548;&#33268;&#27425;&#20248;&#24615;&#33021;&#12290;&#20854;&#27425;&#65292;&#22312;&#25972;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#32858;&#31867;&#20250;&#24433;&#21709;&#22823;&#35268;&#27169;&#34892;&#19994;&#25968;&#25454;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24847;&#22270;&#23398;&#20064;&#26041;&#27861;&#65292;&#31216;&#20026;ELCRec&#65292;&#23427;&#23558;&#34920;&#31034;&#23398;&#20064;&#38598;&#25104;&#21040;&#19968;&#20010;&#31471;&#21040;&#31471;&#21487;&#23398;&#20064;&#32858;&#31867;&#26694;&#26550;&#20013;&#36827;&#34892;&#25512;&#33616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mining users' intents plays a crucial role in sequential recommendation. The recent approach, ICLRec, was introduced to extract underlying users' intents using contrastive learning and clustering. While it has shown effectiveness, the existing method suffers from complex and cumbersome alternating optimization, leading to two main issues. Firstly, the separation of representation learning and clustering optimization within a generalized expectation maximization (EM) framework often results in sub-optimal performance. Secondly, performing clustering on the entire dataset hampers scalability for large-scale industry data. To address these challenges, we propose a novel intent learning method called \underline{ELCRec}, which integrates representation learning into an \underline{E}nd-to-end \underline{L}earnable \underline{C}lustering framework for \underline{Rec}ommendation. Specifically, we encode users' behavior sequences and initialize the cluster centers as learnable network parameter
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;SATOP&#30340;&#31354;&#38388;&#24863;&#30693;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#26469;&#35299;&#20915;&#24033;&#35686;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;&#20572;&#36710;&#20301;&#12289;&#20195;&#29702;&#21644;&#21160;&#20316;&#20043;&#38388;&#30340;&#31354;&#38388;&#20851;&#31995;&#26469;&#21019;&#24314;&#21160;&#20316;&#30340;&#34920;&#31034;&#65292;&#20174;&#32780;&#21160;&#24577;&#35843;&#25972;&#20197;&#36866;&#24212;&#24403;&#21069;&#21487;&#32602;&#27454;&#30340;&#20572;&#36710;&#36829;&#35268;&#34892;&#20026;&#65292;&#24182;&#25552;&#21069;&#35745;&#21010;&#20197;&#25552;&#39640;&#21040;&#36798;&#36829;&#35268;&#34892;&#20026;&#21457;&#29983;&#26102;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.05969</link><description>&lt;p&gt;
&#31354;&#38388;&#24863;&#30693;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22312;&#24033;&#35686;&#38382;&#39064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Spatial-Aware Deep Reinforcement Learning for the Traveling Officer Problem. (arXiv:2401.05969v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05969
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;SATOP&#30340;&#31354;&#38388;&#24863;&#30693;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#26469;&#35299;&#20915;&#24033;&#35686;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;&#20572;&#36710;&#20301;&#12289;&#20195;&#29702;&#21644;&#21160;&#20316;&#20043;&#38388;&#30340;&#31354;&#38388;&#20851;&#31995;&#26469;&#21019;&#24314;&#21160;&#20316;&#30340;&#34920;&#31034;&#65292;&#20174;&#32780;&#21160;&#24577;&#35843;&#25972;&#20197;&#36866;&#24212;&#24403;&#21069;&#21487;&#32602;&#27454;&#30340;&#20572;&#36710;&#36829;&#35268;&#34892;&#20026;&#65292;&#24182;&#25552;&#21069;&#35745;&#21010;&#20197;&#25552;&#39640;&#21040;&#36798;&#36829;&#35268;&#34892;&#20026;&#21457;&#29983;&#26102;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24033;&#35686;&#38382;&#39064;&#65288;TOP&#65289;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38543;&#26426;&#20248;&#21270;&#20219;&#21153;&#12290;&#22312;&#36825;&#20010;&#38382;&#39064;&#20013;&#65292;&#19968;&#21517;&#20572;&#36710;&#21592;&#36890;&#36807;&#37197;&#22791;&#20572;&#36710;&#20256;&#24863;&#22120;&#30340;&#22478;&#24066;&#36827;&#34892;&#24341;&#23548;&#65292;&#20197;&#20415;&#23613;&#21487;&#33021;&#22810;&#22320;&#32602;&#27454;&#36829;&#35268;&#20572;&#36710;&#32773;&#12290;TOP&#30340;&#19968;&#20010;&#20027;&#35201;&#25361;&#25112;&#26159;&#20572;&#36710;&#36829;&#35268;&#30340;&#21160;&#24577;&#24615;&#65292;&#36825;&#20123;&#36829;&#35268;&#34892;&#20026;&#20250;&#22312;&#19968;&#27573;&#26102;&#38388;&#21518;&#38543;&#26426;&#20986;&#29616;&#21644;&#28040;&#22833;&#65292;&#19981;&#35770;&#23427;&#20204;&#26159;&#21542;&#34987;&#32602;&#27454;&#12290;&#22240;&#27492;&#65292;&#35299;&#20915;&#26041;&#26696;&#38656;&#35201;&#21160;&#24577;&#22320;&#35843;&#25972;&#20197;&#36866;&#24212;&#24403;&#21069;&#21487;&#32602;&#27454;&#30340;&#20572;&#36710;&#36829;&#35268;&#34892;&#20026;&#65292;&#21516;&#26102;&#36824;&#35201;&#25552;&#21069;&#35745;&#21010;&#65292;&#22686;&#21152;&#24033;&#35686;&#22312;&#36829;&#35268;&#34892;&#20026;&#21457;&#29983;&#26102;&#21040;&#36798;&#30340;&#21487;&#33021;&#24615;&#12290;&#23613;&#31649;&#23384;&#22312;&#21508;&#31181;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#24448;&#24448;&#38590;&#20197;&#32771;&#34385;&#21040;&#34892;&#21160;&#23545;&#26410;&#26469;&#32602;&#27454;&#36829;&#35268;&#30340;&#33021;&#21147;&#20135;&#29983;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;SATOP&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#31354;&#38388;&#24863;&#30693;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#29992;&#20110;TOP&#12290;&#25105;&#20204;&#30340;&#26032;&#22411;&#29366;&#24577;&#32534;&#30721;&#22120;&#21019;&#24314;&#20102;&#27599;&#20010;&#21160;&#20316;&#30340;&#34920;&#31034;&#65292;&#21033;&#29992;&#20572;&#36710;&#20301;&#12289;&#20195;&#29702;&#21644;&#21160;&#20316;&#20043;&#38388;&#30340;&#31354;&#38388;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
The traveling officer problem (TOP) is a challenging stochastic optimization task. In this problem, a parking officer is guided through a city equipped with parking sensors to fine as many parking offenders as possible. A major challenge in TOP is the dynamic nature of parking offenses, which randomly appear and disappear after some time, regardless of whether they have been fined. Thus, solutions need to dynamically adjust to currently fineable parking offenses while also planning ahead to increase the likelihood that the officer arrives during the offense taking place. Though various solutions exist, these methods often struggle to take the implications of actions on the ability to fine future parking violations into account. This paper proposes SATOP, a novel spatial-aware deep reinforcement learning approach for TOP. Our novel state encoder creates a representation of each action, leveraging the spatial relationships between parking spots, the agent, and the action. Furthermore, we
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#23581;&#35797;&#20351;&#29992;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#29983;&#25104;&#26032;&#30340;&#26725;&#26753;&#31867;&#22411;&#65292;&#36890;&#36807;&#23545;&#28508;&#22312;&#31354;&#38388;&#36827;&#34892;&#37319;&#26679;&#65292;&#21487;&#20197;&#22312;&#20154;&#31867;&#21407;&#22987;&#26725;&#26753;&#31867;&#22411;&#30340;&#22522;&#30784;&#19978;&#32452;&#21512;&#19981;&#21516;&#30340;&#32467;&#26500;&#32452;&#20214;&#65292;&#21019;&#36896;&#20855;&#26377;&#19968;&#23450;&#21019;&#26032;&#33021;&#21147;&#30340;&#26032;&#26725;&#26753;&#31867;&#22411;&#12290;</title><link>http://arxiv.org/abs/2401.05964</link><description>&lt;p&gt;
&#20174;PixelCNN&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#23581;&#35797;&#29983;&#25104;&#26032;&#30340;&#26725;&#26753;&#31867;&#22411;
&lt;/p&gt;
&lt;p&gt;
An attempt to generate new bridge types from latent space of PixelCNN. (arXiv:2401.05964v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05964
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#23581;&#35797;&#20351;&#29992;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#29983;&#25104;&#26032;&#30340;&#26725;&#26753;&#31867;&#22411;&#65292;&#36890;&#36807;&#23545;&#28508;&#22312;&#31354;&#38388;&#36827;&#34892;&#37319;&#26679;&#65292;&#21487;&#20197;&#22312;&#20154;&#31867;&#21407;&#22987;&#26725;&#26753;&#31867;&#22411;&#30340;&#22522;&#30784;&#19978;&#32452;&#21512;&#19981;&#21516;&#30340;&#32467;&#26500;&#32452;&#20214;&#65292;&#21019;&#36896;&#20855;&#26377;&#19968;&#23450;&#21019;&#26032;&#33021;&#21147;&#30340;&#26032;&#26725;&#26753;&#31867;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#23581;&#35797;&#20351;&#29992;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#29983;&#25104;&#26032;&#30340;&#26725;&#26753;&#31867;&#22411;&#12290;&#20351;&#29992;&#19977;&#36328;&#26753;&#26725;&#12289;&#25329;&#26725;&#12289;&#26012;&#25289;&#26725;&#21644;&#24748;&#32034;&#26725;&#30340;&#23545;&#31216;&#32467;&#26500;&#22270;&#20687;&#25968;&#25454;&#38598;&#20316;&#20026;&#35757;&#32451;&#38598;&#65292;&#22522;&#20110;Python&#32534;&#31243;&#35821;&#35328;&#12289;TensorFlow&#21644;Keras&#28145;&#24230;&#23398;&#20064;&#24179;&#21488;&#26500;&#24314;&#21644;&#35757;&#32451;PixelCNN&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#33021;&#22815;&#25429;&#25417;&#22270;&#20687;&#30340;&#32479;&#35745;&#32467;&#26500;&#65292;&#24182;&#22312;&#32473;&#23450;&#21069;&#19968;&#20687;&#32032;&#30340;&#24773;&#20917;&#19979;&#35745;&#31639;&#19979;&#19968;&#20010;&#20687;&#32032;&#30340;&#27010;&#29575;&#20998;&#24067;&#12290;&#36890;&#36807;&#23545;&#33719;&#24471;&#30340;&#28508;&#22312;&#31354;&#38388;&#36827;&#34892;&#37319;&#26679;&#65292;&#21487;&#20197;&#29983;&#25104;&#19982;&#35757;&#32451;&#38598;&#20013;&#19981;&#21516;&#30340;&#26725;&#26753;&#31867;&#22411;&#12290;PixelCNN&#21487;&#20197;&#22312;&#20154;&#31867;&#21407;&#22987;&#26725;&#26753;&#31867;&#22411;&#30340;&#22522;&#30784;&#19978;&#26377;&#26426;&#22320;&#32452;&#21512;&#19981;&#21516;&#30340;&#32467;&#26500;&#32452;&#20214;&#65292;&#21019;&#24314;&#20855;&#26377;&#19968;&#23450;&#20154;&#31867;&#21019;&#26032;&#33021;&#21147;&#30340;&#26032;&#26725;&#26753;&#31867;&#22411;&#12290;&#33258;&#22238;&#24402;&#27169;&#22411;&#26080;&#27861;&#29702;&#35299;&#24207;&#21015;&#30340;&#21547;&#20041;&#65292;&#32780;&#22810;&#27169;&#24577;&#27169;&#22411;&#32467;&#21512;&#22238;&#24402;&#21644;&#33258;&#22238;&#24402;&#27169;&#22411;&#65292;&#33021;&#22815;&#29702;&#35299;&#24207;&#21015;&#12290;&#22810;&#27169;&#24577;&#27169;&#22411;&#24212;&#35813;&#26159;...
&lt;/p&gt;
&lt;p&gt;
Try to generate new bridge types using generative artificial intelligence technology. Using symmetric structured image dataset of three-span beam bridge, arch bridge, cable-stayed bridge and suspension bridge , based on Python programming language, TensorFlow and Keras deep learning platform framework , PixelCNN is constructed and trained. The model can capture the statistical structure of the images and calculate the probability distribution of the next pixel when the previous pixels are given. From the obtained latent space sampling, new bridge types different from the training dataset can be generated. PixelCNN can organically combine different structural components on the basis of human original bridge types, creating new bridge types that have a certain degree of human original ability. Autoregressive models cannot understand the meaning of the sequence, while multimodal models combine regression and autoregressive models to understand the sequence. Multimodal models should be the
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35814;&#32454;&#30740;&#31350;&#20102;&#23558;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#24212;&#29992;&#20110;&#21326;&#20026;&#20113;&#30340;OptVerse AI Solver&#65292;&#20027;&#35201;&#21253;&#25324;&#29983;&#25104;&#22797;&#26434;&#23454;&#20363;&#12289;&#35757;&#32451;&#26694;&#26550;&#32500;&#25252;&#23454;&#29992;&#24615;&#21644;&#20010;&#24615;&#21270;&#30340;&#35299;&#31639;&#22120;&#31574;&#30053;&#31561;&#26041;&#38754;&#30340;&#21019;&#26032;&#26041;&#27861;&#12290;&#30740;&#31350;&#26088;&#22312;&#32531;&#35299;&#25968;&#23398;&#35268;&#21010;&#23454;&#20363;&#31232;&#32570;&#38382;&#39064;&#65292;&#24182;&#36229;&#36234;&#20256;&#32479;&#20248;&#21270;&#25216;&#26415;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.05960</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22312;OptVerse AI Solver&#20013;&#30340;&#24212;&#29992;&#65306;&#35774;&#35745;&#21407;&#29702;&#19982;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Machine Learning Insides OptVerse AI Solver: Design Principles and Applications. (arXiv:2401.05960v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05960
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35814;&#32454;&#30740;&#31350;&#20102;&#23558;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#24212;&#29992;&#20110;&#21326;&#20026;&#20113;&#30340;OptVerse AI Solver&#65292;&#20027;&#35201;&#21253;&#25324;&#29983;&#25104;&#22797;&#26434;&#23454;&#20363;&#12289;&#35757;&#32451;&#26694;&#26550;&#32500;&#25252;&#23454;&#29992;&#24615;&#21644;&#20010;&#24615;&#21270;&#30340;&#35299;&#31639;&#22120;&#31574;&#30053;&#31561;&#26041;&#38754;&#30340;&#21019;&#26032;&#26041;&#27861;&#12290;&#30740;&#31350;&#26088;&#22312;&#32531;&#35299;&#25968;&#23398;&#35268;&#21010;&#23454;&#20363;&#31232;&#32570;&#38382;&#39064;&#65292;&#24182;&#36229;&#36234;&#20256;&#32479;&#20248;&#21270;&#25216;&#26415;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25968;&#23383;&#26080;&#22788;&#19981;&#22312;&#30340;&#26102;&#20195;&#65292;&#39640;&#25928;&#30340;&#36164;&#28304;&#31649;&#29702;&#21644;&#20915;&#31574;&#26159;&#21508;&#20010;&#34892;&#19994;&#30340;&#37325;&#35201;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23545;&#23558;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#24212;&#29992;&#20110;&#21326;&#20026;&#20113;&#30340;OptVerse AI Solver&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#30740;&#31350;&#65292;&#26088;&#22312;&#32531;&#35299;&#29616;&#23454;&#19990;&#30028;&#25968;&#23398;&#35268;&#21010;&#23454;&#20363;&#30340;&#31232;&#32570;&#65292;&#24182;&#36229;&#36234;&#20256;&#32479;&#20248;&#21270;&#25216;&#26415;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#21033;&#29992;&#29983;&#25104;&#27169;&#22411;&#29983;&#25104;&#22797;&#26434;&#30340;SAT&#21644;MILP&#23454;&#20363;&#30340;&#26041;&#27861;&#65292;&#36825;&#20123;&#23454;&#20363;&#21453;&#26144;&#20102;&#29616;&#23454;&#19990;&#30028;&#38382;&#39064;&#30340;&#22810;&#38754;&#32467;&#26500;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#35757;&#32451;&#26694;&#26550;&#65292;&#21033;&#29992;&#22686;&#24378;&#31574;&#30053;&#32500;&#25252;&#35299;&#31639;&#22120;&#22312;&#21160;&#24577;&#29615;&#22659;&#20013;&#30340;&#23454;&#29992;&#24615;&#12290;&#38500;&#20102;&#25968;&#25454;&#29983;&#25104;&#21644;&#22686;&#24378;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#36824;&#21253;&#25324;&#20010;&#24615;&#21270;&#35299;&#31639;&#22120;&#31574;&#30053;&#30340;&#26032;&#22411;&#26426;&#22120;&#23398;&#20064;&#39537;&#21160;&#31574;&#30053;&#65292;&#37325;&#28857;&#20851;&#27880;&#22270;&#21367;&#31215;&#32593;&#32476;&#29992;&#20110;&#21021;&#22987;&#22522;&#30784;&#36873;&#25321;&#21644;&#24378;&#21270;&#23398;&#20064;&#29992;&#20110;&#39640;&#32423;&#39044;&#27714;&#35299;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
In an era of digital ubiquity, efficient resource management and decision-making are paramount across numerous industries. To this end, we present a comprehensive study on the integration of machine learning (ML) techniques into Huawei Cloud's OptVerse AI Solver, which aims to mitigate the scarcity of real-world mathematical programming instances, and to surpass the capabilities of traditional optimization techniques. We showcase our methods for generating complex SAT and MILP instances utilizing generative models that mirror multifaceted structures of real-world problem. Furthermore, we introduce a training framework leveraging augmentation policies to maintain solvers' utility in dynamic environments. Besides the data generation and augmentation, our proposed approaches also include novel ML-driven policies for personalized solver strategies, with an emphasis on applications like graph convolutional networks for initial basis selection and reinforcement learning for advanced presolvi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#29616;&#19978;&#19979;&#25991;&#23398;&#20064;&#33539;&#24335;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23384;&#22312;&#28431;&#27934;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#36890;&#36807;&#27745;&#26579;&#31034;&#33539;&#19978;&#19979;&#25991;&#26469;&#25805;&#25511;&#27169;&#22411;&#34892;&#20026;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#24494;&#35843;&#12290;&#36825;&#39033;&#30740;&#31350;&#35774;&#35745;&#20102;&#19968;&#31181;&#21517;&#20026;ICLAttack&#30340;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#27745;&#26579;&#31034;&#33539;&#26679;&#26412;&#21644;&#25552;&#31034;&#26469;&#20351;&#27169;&#22411;&#25353;&#29031;&#39044;&#23450;&#20041;&#30340;&#24847;&#22270;&#34892;&#20107;&#12290;</title><link>http://arxiv.org/abs/2401.05949</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#36890;&#29992;&#28431;&#27934;&#65306;&#19978;&#19979;&#25991;&#23398;&#20064;&#21518;&#38376;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Universal Vulnerabilities in Large Language Models: In-context Learning Backdoor Attacks. (arXiv:2401.05949v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05949
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616;&#19978;&#19979;&#25991;&#23398;&#20064;&#33539;&#24335;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23384;&#22312;&#28431;&#27934;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#36890;&#36807;&#27745;&#26579;&#31034;&#33539;&#19978;&#19979;&#25991;&#26469;&#25805;&#25511;&#27169;&#22411;&#34892;&#20026;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#24494;&#35843;&#12290;&#36825;&#39033;&#30740;&#31350;&#35774;&#35745;&#20102;&#19968;&#31181;&#21517;&#20026;ICLAttack&#30340;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#27745;&#26579;&#31034;&#33539;&#26679;&#26412;&#21644;&#25552;&#31034;&#26469;&#20351;&#27169;&#22411;&#25353;&#29031;&#39044;&#23450;&#20041;&#30340;&#24847;&#22270;&#34892;&#20107;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19978;&#19979;&#25991;&#23398;&#20064;&#26159;&#19968;&#31181;&#22312;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#20043;&#38388;&#24357;&#21512;&#24046;&#36317;&#30340;&#33539;&#24335;&#65292;&#22312;&#20960;&#20010;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#23637;&#29616;&#20102;&#39640;&#25928;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#23569;&#26679;&#26412;&#35774;&#32622;&#20013;&#12290;&#19982;&#20256;&#32479;&#30340;&#24494;&#35843;&#26041;&#27861;&#19981;&#21516;&#65292;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#22815;&#36866;&#24212;&#26410;&#35265;&#36807;&#30340;&#20219;&#21153;&#32780;&#26080;&#38656;&#26356;&#26032;&#20219;&#20309;&#21442;&#25968;&#12290;&#23613;&#31649;&#34987;&#24191;&#27867;&#24212;&#29992;&#65292;&#19978;&#19979;&#25991;&#23398;&#20064;&#20173;&#28982;&#23481;&#26131;&#21463;&#21040;&#24694;&#24847;&#25915;&#20987;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#23545;&#36825;&#19968;&#33539;&#24335;&#30340;&#23433;&#20840;&#24615;&#38382;&#39064;&#30340;&#20851;&#20999;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#36890;&#36807;&#27745;&#26579;&#31034;&#33539;&#19978;&#19979;&#25991;&#26469;&#25805;&#25511;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#34892;&#20026;&#65292;&#32780;&#26080;&#38656;&#23545;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;&#65292;&#21629;&#21517;&#20026;ICLAttack&#65292;&#38024;&#23545;&#22522;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#20004;&#31181;&#31867;&#22411;&#30340;&#25915;&#20987;&#65306;&#27745;&#26579;&#31034;&#33539;&#26679;&#26412;&#21644;&#27745;&#26579;&#25552;&#31034;&#65292;&#21487;&#20197;&#20351;&#27169;&#22411;&#25353;&#29031;&#39044;&#23450;&#20041;&#30340;&#24847;&#22270;&#34892;&#20107;&#12290;ICLAttack&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;
In-context learning, a paradigm bridging the gap between pre-training and fine-tuning, has demonstrated high efficacy in several NLP tasks, especially in few-shot settings. Unlike traditional fine-tuning methods, in-context learning adapts pre-trained models to unseen tasks without updating any parameters. Despite being widely applied, in-context learning is vulnerable to malicious attacks. In this work, we raise security concerns regarding this paradigm. Our studies demonstrate that an attacker can manipulate the behavior of large language models by poisoning the demonstration context, without the need for fine-tuning the model. Specifically, we have designed a new backdoor attack method, named ICLAttack, to target large language models based on in-context learning. Our method encompasses two types of attacks: poisoning demonstration examples and poisoning prompts, which can make models behave in accordance with predefined intentions. ICLAttack does not require additional fine-tuning 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;Transformer&#34920;&#31034;&#20013;&#23398;&#20064;&#35748;&#30693;&#22320;&#22270;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#38024;&#23545;&#37096;&#20998;&#35266;&#23519;&#29615;&#22659;&#20013;&#30340;&#36335;&#24452;&#35268;&#21010;&#38382;&#39064;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2401.05946</link><description>&lt;p&gt;
&#20174;Transformer&#34920;&#31034;&#20013;&#23398;&#20064;&#35748;&#30693;&#22320;&#22270;&#20197;&#23454;&#29616;&#37096;&#20998;&#35266;&#23519;&#29615;&#22659;&#20013;&#30340;&#39640;&#25928;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Learning Cognitive Maps from Transformer Representations for Efficient Planning in Partially Observed Environments. (arXiv:2401.05946v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05946
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;Transformer&#34920;&#31034;&#20013;&#23398;&#20064;&#35748;&#30693;&#22320;&#22270;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#38024;&#23545;&#37096;&#20998;&#35266;&#23519;&#29615;&#22659;&#20013;&#30340;&#36335;&#24452;&#35268;&#21010;&#38382;&#39064;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#21253;&#25324;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#20165;&#36879;&#38706;&#30340;&#19978;&#19979;&#25991;&#20219;&#21153;&#65292;&#20294;&#26631;&#20934;&#30340;Transformer&#21644;&#38024;&#23545;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#35757;&#32451;&#30340;&#21464;&#20307;(a)&#27809;&#26377;&#23398;&#20064;&#21040;&#26126;&#30830;&#30340;&#29615;&#22659;&#19990;&#30028;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#28789;&#27963;&#26597;&#35810;&#65292;(b)&#19981;&#33021;&#29992;&#20110;&#35268;&#21010;&#25110;&#23548;&#33322;&#12290;&#26412;&#25991;&#32771;&#34385;&#37096;&#20998;&#35266;&#23519;&#29615;&#22659;&#65288;POEs&#65289;&#65292;&#20195;&#29702;&#26681;&#25454;&#20854;&#23548;&#33322;&#26102;&#25509;&#25910;&#21040;&#30340;&#24863;&#30693;&#21035;&#21517;&#35266;&#23519;&#65292;&#36825;&#20351;&#24471;&#36335;&#24452;&#35268;&#21010;&#21464;&#24471;&#22256;&#38590;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#20855;&#26377;&#65288;&#22810;&#20010;&#65289;&#31163;&#25955;&#29942;&#39048;&#30340;Transformer&#65292;TDB&#65292;&#20854;&#28508;&#22312;&#20195;&#30721;&#21487;&#20197;&#23398;&#20064;&#35266;&#23519;&#21644;&#21160;&#20316;&#21382;&#21490;&#30340;&#21387;&#32553;&#34920;&#31034;&#12290;&#22312;&#35757;&#32451;TDB&#20197;&#39044;&#27979;&#32473;&#23450;&#21382;&#21490;&#30340;&#26410;&#26469;&#35266;&#23519;&#21518;&#65292;&#25105;&#20204;&#20174;&#20854;&#27963;&#36291;&#30340;&#29942;&#39048;&#32034;&#24341;&#20013;&#25552;&#21462;&#29615;&#22659;&#30340;&#21487;&#35299;&#37322;&#35748;&#30693;&#22320;&#22270;&#12290;&#28982;&#21518;&#65292;&#23558;&#36825;&#20123;&#22320;&#22270;&#19982;&#22806;&#37096;&#27714;&#35299;&#22120;&#37197;&#23545;&#20197;&#35299;&#20915;&#65288;&#21463;&#38480;&#21046;&#30340;&#65289;&#36335;&#24452;&#35268;&#21010;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;POEs&#19978;&#35757;&#32451;&#30340;TDB&#21487;&#20197;&#20445;&#30041;...
&lt;/p&gt;
&lt;p&gt;
Despite their stellar performance on a wide range of tasks, including in-context tasks only revealed during inference, vanilla transformers and variants trained for next-token predictions (a) do not learn an explicit world model of their environment which can be flexibly queried and (b) cannot be used for planning or navigation. In this paper, we consider partially observed environments (POEs), where an agent receives perceptually aliased observations as it navigates, which makes path planning hard. We introduce a transformer with (multiple) discrete bottleneck(s), TDB, whose latent codes learn a compressed representation of the history of observations and actions. After training a TDB to predict the future observation(s) given the history, we extract interpretable cognitive maps of the environment from its active bottleneck(s) indices. These maps are then paired with an external solver to solve (constrained) path planning problems. First, we show that a TDB trained on POEs (a) retains
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20195;&#30721;&#21464;&#24322;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#31995;&#32479;&#35780;&#20272;LLMs&#30340;&#20195;&#30721;&#29702;&#35299;&#33021;&#21147;&#65292;&#37325;&#28857;&#20851;&#27880;&#20195;&#30721;&#21644;&#25551;&#36848;&#20043;&#38388;&#30340;&#24494;&#22937;&#24046;&#24322;&#12290;&#36890;&#36807;&#24341;&#20837;&#20195;&#30721;&#21464;&#24322;&#21040;&#29616;&#26377;&#30340;&#25968;&#25454;&#38598;&#20013;&#65292;&#25105;&#20204;&#21487;&#20197;&#29983;&#25104;&#19981;&#19968;&#33268;&#30340;&#20195;&#30721;&#25968;&#25454;&#65292;&#20174;&#32780;&#35780;&#20272;LLMs&#30340;&#20195;&#30721;&#29702;&#35299;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.05940</link><description>&lt;p&gt;
&#22522;&#20110;&#21464;&#24322;&#30340;&#19968;&#33268;&#24615;&#27979;&#35797;&#29992;&#20110;&#35780;&#20272;LLMs&#30340;&#20195;&#30721;&#29702;&#35299;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Mutation-based Consistency Testing for Evaluating the Code Understanding Capability of LLMs. (arXiv:2401.05940v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05940
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20195;&#30721;&#21464;&#24322;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#31995;&#32479;&#35780;&#20272;LLMs&#30340;&#20195;&#30721;&#29702;&#35299;&#33021;&#21147;&#65292;&#37325;&#28857;&#20851;&#27880;&#20195;&#30721;&#21644;&#25551;&#36848;&#20043;&#38388;&#30340;&#24494;&#22937;&#24046;&#24322;&#12290;&#36890;&#36807;&#24341;&#20837;&#20195;&#30721;&#21464;&#24322;&#21040;&#29616;&#26377;&#30340;&#25968;&#25454;&#38598;&#20013;&#65292;&#25105;&#20204;&#21487;&#20197;&#29983;&#25104;&#19981;&#19968;&#33268;&#30340;&#20195;&#30721;&#25968;&#25454;&#65292;&#20174;&#32780;&#35780;&#20272;LLMs&#30340;&#20195;&#30721;&#29702;&#35299;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22788;&#29702;&#33258;&#28982;&#35821;&#35328;&#21644;&#32534;&#31243;&#35821;&#35328;&#26041;&#38754;&#26174;&#31034;&#20986;&#21331;&#36234;&#30340;&#33021;&#21147;&#65292;&#20026;&#36719;&#20214;&#24037;&#31243;&#39046;&#22495;&#30340;&#38656;&#27714;&#24037;&#31243;&#12289;&#20195;&#30721;&#29983;&#25104;&#21644;&#36719;&#20214;&#27979;&#35797;&#31561;&#21508;&#31181;&#24212;&#29992;&#25552;&#20379;&#20102;&#21487;&#33021;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#20195;&#30721;&#29983;&#25104;&#22522;&#20934;&#27979;&#35797;&#24182;&#19981;&#19968;&#23450;&#33021;&#22815;&#35780;&#20272;LLMs&#30340;&#20195;&#30721;&#29702;&#35299;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#20195;&#30721;&#21644;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#20043;&#38388;&#21487;&#33021;&#20986;&#29616;&#30340;&#24494;&#22937;&#19981;&#19968;&#33268;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#31995;&#32479;&#35780;&#20272;LLMs&#30340;&#20195;&#30721;&#29702;&#35299;&#24615;&#33021;&#65292;&#29305;&#21035;&#20851;&#27880;&#20195;&#30721;&#21644;&#25551;&#36848;&#20043;&#38388;&#30340;&#24494;&#23567;&#24046;&#24322;&#65292;&#36890;&#36807;&#24341;&#20837;&#20195;&#30721;&#21464;&#24322;&#21040;&#29616;&#26377;&#30340;&#20195;&#30721;&#29983;&#25104;&#25968;&#25454;&#38598;&#20013;&#12290;&#20195;&#30721;&#21464;&#24322;&#26159;&#23545;&#21407;&#22987;&#20195;&#30721;&#36827;&#34892;&#25913;&#21160;&#30340;&#23567;&#25913;&#21464;&#65292;&#20250;&#23548;&#33268;&#20195;&#30721;&#19982;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#19981;&#21305;&#37197;&#12290;&#25105;&#20204;&#37319;&#29992;&#19981;&#21516;&#31867;&#22411;&#30340;&#20195;&#30721;&#21464;&#24322;&#65292;&#22914;&#25805;&#20316;&#31526;&#26367;&#25442;&#21644;&#35821;&#21477;&#21024;&#38500;&#65292;&#26469;&#29983;&#25104;&#19981;&#19968;&#33268;&#30340;&#20195;&#30721;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have shown remarkable capabilities in processing both natural and programming languages, which have enabled various applications in software engineering, such as requirement engineering, code generation, and software testing. However, existing code generation benchmarks do not necessarily assess the code understanding performance of LLMs, especially for the subtle inconsistencies that may arise between code and its semantics described in natural language.  In this paper, we propose a novel method to systematically assess the code understanding performance of LLMs, particularly focusing on subtle differences between code and its descriptions, by introducing code mutations to existing code generation datasets. Code mutations are small changes that alter the semantics of the original code, creating a mismatch with the natural language description. We apply different types of code mutations, such as operator replacement and statement deletion, to generate incon
&lt;/p&gt;</description></item><item><title>DREQ&#26159;&#19968;&#31181;&#22522;&#20110;&#23454;&#20307;&#30340;&#23494;&#38598;&#25991;&#26723;&#37325;&#26032;&#25490;&#24207;&#27169;&#22411;&#65292;&#36890;&#36807;&#24378;&#35843;&#19982;&#26597;&#35810;&#30456;&#20851;&#30340;&#23454;&#20307;&#24182;&#20943;&#24369;&#19981;&#22826;&#30456;&#20851;&#30340;&#23454;&#20307;&#65292;&#33719;&#24471;&#19968;&#20010;&#26597;&#35810;&#29305;&#23450;&#30340;&#20197;&#23454;&#20307;&#20026;&#20013;&#24515;&#30340;&#25991;&#26723;&#34920;&#31034;&#65292;&#24182;&#32467;&#21512;&#25991;&#26412;&#20026;&#20013;&#24515;&#30340;&#25991;&#26723;&#34920;&#31034;&#36827;&#34892;&#28151;&#21512;&#34920;&#31034;&#65292;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#37325;&#26032;&#25490;&#24207;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.05939</link><description>&lt;p&gt;
DREQ: &#20351;&#29992;&#22522;&#20110;&#23454;&#20307;&#30340;&#26597;&#35810;&#29702;&#35299;&#36827;&#34892;&#25991;&#26723;&#37325;&#26032;&#25490;&#24207;
&lt;/p&gt;
&lt;p&gt;
DREQ: Document Re-Ranking Using Entity-based Query Understanding. (arXiv:2401.05939v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05939
&lt;/p&gt;
&lt;p&gt;
DREQ&#26159;&#19968;&#31181;&#22522;&#20110;&#23454;&#20307;&#30340;&#23494;&#38598;&#25991;&#26723;&#37325;&#26032;&#25490;&#24207;&#27169;&#22411;&#65292;&#36890;&#36807;&#24378;&#35843;&#19982;&#26597;&#35810;&#30456;&#20851;&#30340;&#23454;&#20307;&#24182;&#20943;&#24369;&#19981;&#22826;&#30456;&#20851;&#30340;&#23454;&#20307;&#65292;&#33719;&#24471;&#19968;&#20010;&#26597;&#35810;&#29305;&#23450;&#30340;&#20197;&#23454;&#20307;&#20026;&#20013;&#24515;&#30340;&#25991;&#26723;&#34920;&#31034;&#65292;&#24182;&#32467;&#21512;&#25991;&#26412;&#20026;&#20013;&#24515;&#30340;&#25991;&#26723;&#34920;&#31034;&#36827;&#34892;&#28151;&#21512;&#34920;&#31034;&#65292;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#37325;&#26032;&#25490;&#24207;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22522;&#20110;&#23454;&#20307;&#30340;&#31070;&#32463;&#20449;&#24687;&#26816;&#32034;&#65288;IR&#65289;&#27169;&#22411;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#23427;&#20204;&#32463;&#24120;&#24573;&#35270;&#20102;&#19968;&#20010;&#20851;&#38190;&#32454;&#24494;&#24046;&#21035;&#65306;&#25991;&#26723;&#20013;&#21508;&#20010;&#23454;&#20307;&#23545;&#20854;&#25972;&#20307;&#30456;&#20851;&#24615;&#30340;&#24433;&#21709;&#31243;&#24230;&#26159;&#19981;&#21516;&#30340;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DREQ&#65292;&#19968;&#31181;&#22522;&#20110;&#23454;&#20307;&#30340;&#23494;&#38598;&#25991;&#26723;&#37325;&#26032;&#25490;&#24207;&#27169;&#22411;&#12290;&#29420;&#29305;&#20043;&#22788;&#22312;&#20110;&#65292;&#22312;&#25991;&#26723;&#30340;&#34920;&#31034;&#20013;&#24378;&#35843;&#19982;&#26597;&#35810;&#30456;&#20851;&#30340;&#23454;&#20307;&#65292;&#21516;&#26102;&#20943;&#24369;&#19981;&#22826;&#30456;&#20851;&#30340;&#23454;&#20307;&#65292;&#20174;&#32780;&#33719;&#24471;&#19968;&#20010;&#26597;&#35810;&#29305;&#23450;&#30340;&#20197;&#23454;&#20307;&#20026;&#20013;&#24515;&#30340;&#25991;&#26723;&#34920;&#31034;&#12290;&#28982;&#21518;&#65292;&#23558;&#36825;&#31181;&#20197;&#23454;&#20307;&#20026;&#20013;&#24515;&#30340;&#25991;&#26723;&#34920;&#31034;&#19982;&#25991;&#26412;&#20026;&#20013;&#24515;&#30340;&#25991;&#26723;&#34920;&#31034;&#32467;&#21512;&#36215;&#26469;&#65292;&#33719;&#24471;&#25991;&#26723;&#30340;&#8220;&#28151;&#21512;&#8221;&#34920;&#31034;&#12290;&#25105;&#20204;&#20351;&#29992;&#36825;&#31181;&#28151;&#21512;&#34920;&#31034;&#23398;&#20064;&#25991;&#26723;&#30340;&#30456;&#20851;&#24615;&#20998;&#25968;&#12290;&#36890;&#36807;&#20351;&#29992;&#22235;&#20010;&#22823;&#35268;&#27169;&#22522;&#20934;&#27979;&#35797;&#65292;&#25105;&#20204;&#35777;&#26126;DREQ&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#31070;&#32463;&#32593;&#32476;&#21644;&#38750;&#31070;&#32463;&#32593;&#32476;&#30340;&#37325;&#26032;&#25490;&#24207;&#26041;&#27861;&#65292;&#31361;&#20986;&#20102;&#25105;&#20204;&#22522;&#20110;&#23454;&#20307;&#30340;&#34920;&#31034;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
While entity-oriented neural IR models have advanced significantly, they often overlook a key nuance: the varying degrees of influence individual entities within a document have on its overall relevance. Addressing this gap, we present DREQ, an entity-oriented dense document re-ranking model. Uniquely, we emphasize the query-relevant entities within a document's representation while simultaneously attenuating the less relevant ones, thus obtaining a query-specific entity-centric document representation. We then combine this entity-centric document representation with the text-centric representation of the document to obtain a "hybrid" representation of the document. We learn a relevance score for the document using this hybrid representation. Using four large-scale benchmarks, we show that DREQ outperforms state-of-the-art neural and non-neural re-ranking methods, highlighting the effectiveness of our entity-oriented representation approach.
&lt;/p&gt;</description></item><item><title>DiffDA&#26159;&#19968;&#31181;&#29992;&#20110;&#27668;&#35937;&#23610;&#24230;&#25968;&#25454;&#21516;&#21270;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#23558;&#39044;&#27979;&#29366;&#24577;&#21644;&#31232;&#30095;&#35266;&#27979;&#21516;&#21270;&#65292;&#29983;&#25104;&#19982;&#35266;&#27979;&#19968;&#33268;&#30340;&#21021;&#22987;&#26465;&#20214;&#65292;&#24182;&#33021;&#23545;&#39044;&#27979;&#36827;&#34892;&#21518;&#22788;&#29702;&#21040;&#26410;&#26469;&#12290;</title><link>http://arxiv.org/abs/2401.05932</link><description>&lt;p&gt;
DiffDA:&#19968;&#31181;&#29992;&#20110;&#27668;&#35937;&#23610;&#24230;&#25968;&#25454;&#21516;&#21270;&#30340;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
DiffDA: a diffusion model for weather-scale data assimilation. (arXiv:2401.05932v1 [cs.CE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05932
&lt;/p&gt;
&lt;p&gt;
DiffDA&#26159;&#19968;&#31181;&#29992;&#20110;&#27668;&#35937;&#23610;&#24230;&#25968;&#25454;&#21516;&#21270;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#23558;&#39044;&#27979;&#29366;&#24577;&#21644;&#31232;&#30095;&#35266;&#27979;&#21516;&#21270;&#65292;&#29983;&#25104;&#19982;&#35266;&#27979;&#19968;&#33268;&#30340;&#21021;&#22987;&#26465;&#20214;&#65292;&#24182;&#33021;&#23545;&#39044;&#27979;&#36827;&#34892;&#21518;&#22788;&#29702;&#21040;&#26410;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20934;&#30830;&#30340;&#25968;&#25454;&#21516;&#21270;&#29983;&#25104;&#21021;&#22987;&#26465;&#20214;&#23545;&#20110;&#21487;&#38752;&#30340;&#22825;&#27668;&#39044;&#25253;&#21644;&#27668;&#20505;&#27169;&#25311;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;DiffDA&#20316;&#20026;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#25968;&#25454;&#21516;&#21270;&#26041;&#27861;&#65292;&#33021;&#22815;&#20351;&#29992;&#39044;&#27979;&#29366;&#24577;&#21644;&#31232;&#30095;&#35266;&#27979;&#26469;&#21516;&#21270;&#22823;&#27668;&#21464;&#37327;&#12290;&#25105;&#20204;&#23558;&#39044;&#35757;&#32451;&#30340;GraphCast&#22825;&#27668;&#39044;&#25253;&#27169;&#22411;&#20316;&#20026;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#24212;&#29992;&#20102;&#20004;&#38454;&#27573;&#26465;&#20214;&#65306;&#22312;&#35757;&#32451;&#21644;&#25512;&#29702;&#36807;&#31243;&#20013;&#23545;&#39044;&#27979;&#29366;&#24577;&#36827;&#34892;&#26465;&#20214;&#21270;&#65292;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#21482;&#23545;&#31232;&#30095;&#35266;&#27979;&#36827;&#34892;&#26465;&#20214;&#21270;&#12290;&#20316;&#20026;&#21103;&#20135;&#21697;&#65292;&#36825;&#31181;&#31574;&#30053;&#36824;&#33021;&#23558;&#39044;&#27979;&#21518;&#22788;&#29702;&#21040;&#26410;&#26469;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#27809;&#26377;&#21487;&#29992;&#30340;&#35266;&#27979;&#25968;&#25454;&#12290;&#36890;&#36807;&#22522;&#20110;&#20877;&#20998;&#26512;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#20197;0.25&#24230;&#20998;&#36776;&#29575;&#29983;&#25104;&#19982;&#35266;&#27979;&#19968;&#33268;&#30340;&#21516;&#21270;&#20840;&#29699;&#22823;&#27668;&#25968;&#25454;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36890;&#36807;&#25105;&#20204;&#30340;&#26041;&#27861;&#29983;&#25104;&#30340;&#21021;&#22987;&#26465;&#20214;&#21487;&#20197;&#29992;&#20110;&#20855;&#26377;&#36739;&#23567;&#25439;&#22833;&#30340;&#39044;&#25253;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
The generation of initial conditions via accurate data assimilation is crucial for reliable weather forecasting and climate modeling. We propose the DiffDA as a machine learning based data assimilation method capable of assimilating atmospheric variables using predicted states and sparse observations. We adapt the pretrained GraphCast weather forecast model as a denoising diffusion model. Our method applies two-phase conditioning: on the predicted state during both training and inference, and on sparse observations during inference only. As a byproduct, this strategy also enables the post-processing of predictions into the future, for which no observations are available.Through experiments based on a reanalysis dataset, we have verified that our method can produce assimilated global atmospheric data consistent with observations at 0.25degree resolution. The experiments also show that the initial conditions that are generated via our approach can be used for forecast models with a loss 
&lt;/p&gt;</description></item><item><title>&#33258;&#25105;&#31361;&#20986;&#24335;&#29369;&#35947;&#65288;SH2&#65289;&#26159;&#19968;&#31181;&#25512;&#29702;&#26102;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#39044;&#27979;&#27010;&#29575;&#36739;&#20302;&#30340;&#26631;&#35760;&#65292;&#24182;&#24378;&#35843;&#23427;&#20204;&#30340;&#24046;&#24322;&#65292;&#20174;&#32780;&#24110;&#21161;&#35821;&#35328;&#27169;&#22411;&#26356;&#20934;&#30830;&#22320;&#35299;&#30721;&#12290;</title><link>http://arxiv.org/abs/2401.05930</link><description>&lt;p&gt;
SH2: &#33258;&#25105;&#31361;&#20986;&#24335;&#29369;&#35947;&#24110;&#21161;&#24744;&#26356;&#20934;&#30830;&#35299;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
SH2: Self-Highlighted Hesitation Helps You Decode More Truthfully. (arXiv:2401.05930v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05930
&lt;/p&gt;
&lt;p&gt;
&#33258;&#25105;&#31361;&#20986;&#24335;&#29369;&#35947;&#65288;SH2&#65289;&#26159;&#19968;&#31181;&#25512;&#29702;&#26102;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#39044;&#27979;&#27010;&#29575;&#36739;&#20302;&#30340;&#26631;&#35760;&#65292;&#24182;&#24378;&#35843;&#23427;&#20204;&#30340;&#24046;&#24322;&#65292;&#20174;&#32780;&#24110;&#21161;&#35821;&#35328;&#27169;&#22411;&#26356;&#20934;&#30830;&#22320;&#35299;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#25991;&#26412;&#29983;&#25104;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;LLMs&#20173;&#28982;&#23384;&#22312;&#24187;&#35273;&#38382;&#39064;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25512;&#29702;&#26102;&#26041;&#27861;&#65292;&#21363;&#33258;&#25105;&#31361;&#20986;&#24335;&#29369;&#35947;(SH2)&#65292;&#20197;&#24110;&#21161;LLMs&#26356;&#20934;&#30830;&#22320;&#35299;&#30721;&#12290;SH2&#22522;&#20110;&#20449;&#24687;&#29702;&#35770;&#20013;&#19968;&#20010;&#31616;&#21333;&#30340;&#20107;&#23454;&#65292;&#21363;&#23545;&#20110;LLMs&#32780;&#35328;&#65292;&#39044;&#27979;&#27010;&#29575;&#36739;&#20302;&#30340;&#26631;&#35760;&#24448;&#24448;&#26356;&#20855;&#20449;&#24687;&#37327;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;LLMs&#32473;&#20104;&#36739;&#20302;&#27010;&#29575;&#30340;&#26631;&#35760;&#26356;&#26377;&#21487;&#33021;&#19982;&#20107;&#23454;&#20449;&#24687;&#65288;&#22914;&#21517;&#35789;&#12289;&#19987;&#26377;&#21517;&#35789;&#21644;&#24418;&#23481;&#35789;&#65289;&#23494;&#20999;&#30456;&#20851;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#36873;&#25321;&#27010;&#29575;&#26368;&#20302;&#30340;&#26631;&#35760;&#24182;&#23558;&#20854;&#36830;&#25509;&#21040;&#21407;&#22987;&#19978;&#19979;&#25991;&#20013;&#26469;&#8220;&#31361;&#20986;&#8221;&#20107;&#23454;&#20449;&#24687;&#65292;&#20174;&#32780;&#36843;&#20351;&#27169;&#22411;&#22312;&#29983;&#25104;&#20043;&#21069;&#22810;&#27425;&#38405;&#35835;&#21644;&#29369;&#35947;&#36825;&#20123;&#26631;&#35760;&#12290;&#22312;&#35299;&#30721;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#36824;&#37319;&#29992;&#23545;&#27604;&#35299;&#30721;&#30340;&#26041;&#24335;&#26469;&#24378;&#35843;&#30001;&#29369;&#35947;&#24102;&#26469;&#30340;&#36755;&#20986;&#27010;&#29575;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) demonstrate great performance in text generation. However, LLMs are still suffering from hallucinations. In this work, we propose an inference-time method, Self-Highlighted Hesitation (SH2), to help LLMs decode more truthfully. SH2 is based on a simple fact rooted in information theory that for an LLM, the tokens predicted with lower probabilities are prone to be more informative than others. Our analysis shows that the tokens assigned with lower probabilities by an LLM are more likely to be closely related to factual information, such as nouns, proper nouns, and adjectives. Therefore, we propose to ''highlight'' the factual information by selecting the tokens with the lowest probabilities and concatenating them to the original context, thus forcing the model to repeatedly read and hesitate on these tokens before generation. During decoding, we also adopt contrastive decoding to emphasize the difference in the output probabilities brought by the hesitation.
&lt;/p&gt;</description></item><item><title>CoSSegGaussians&#26159;&#19968;&#31181;&#32039;&#20945;&#19988;&#36805;&#36895;&#30340;3D&#39640;&#26031;&#22330;&#26223;&#20998;&#21106;&#26041;&#27861;&#65292;&#36890;&#36807;&#26144;&#23556;&#31354;&#38388;&#21644;&#35821;&#20041;&#29305;&#24449;&#23454;&#29616;&#32039;&#20945;&#21644;&#21487;&#38752;&#30340;&#38646;&#26679;&#26412;&#22330;&#26223;&#20998;&#21106;&#12290;</title><link>http://arxiv.org/abs/2401.05925</link><description>&lt;p&gt;
CoSSegGaussians&#65306;&#32039;&#20945;&#19988;&#36805;&#36895;&#30340;3D&#39640;&#26031;&#22330;&#26223;&#20998;&#21106;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
CoSSegGaussians: Compact and Swift Scene Segmenting 3D Gaussians. (arXiv:2401.05925v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05925
&lt;/p&gt;
&lt;p&gt;
CoSSegGaussians&#26159;&#19968;&#31181;&#32039;&#20945;&#19988;&#36805;&#36895;&#30340;3D&#39640;&#26031;&#22330;&#26223;&#20998;&#21106;&#26041;&#27861;&#65292;&#36890;&#36807;&#26144;&#23556;&#31354;&#38388;&#21644;&#35821;&#20041;&#29305;&#24449;&#23454;&#29616;&#32039;&#20945;&#21644;&#21487;&#38752;&#30340;&#38646;&#26679;&#26412;&#22330;&#26223;&#20998;&#21106;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32039;&#20945;&#19988;&#36805;&#36895;&#30340;3D&#39640;&#26031;&#22330;&#26223;&#20998;&#21106;&#26041;&#27861;&#65288;CoSSegGaussians&#65289;&#65292;&#35813;&#26041;&#27861;&#20165;&#20351;&#29992;RGB&#22270;&#20687;&#36755;&#20837;&#65292;&#20197;&#24555;&#36895;&#30340;&#28210;&#26579;&#36895;&#24230;&#23454;&#29616;&#32039;&#20945;&#30340;3D&#19968;&#33268;&#24615;&#22330;&#26223;&#20998;&#21106;&#12290;&#20808;&#21069;&#22522;&#20110;NeRF&#30340;3D&#20998;&#21106;&#26041;&#27861;&#20381;&#36182;&#20110;&#38544;&#24335;&#25110;&#20307;&#32032;&#31070;&#32463;&#22330;&#34920;&#31034;&#21644;&#20809;&#32447;&#34892;&#36827;&#20307;&#31215;&#28210;&#26579;&#65292;&#36825;&#20123;&#26041;&#27861;&#32791;&#26102;&#36739;&#38271;&#12290;&#26368;&#36817;&#30340;3D&#39640;&#26031;&#22330;&#25237;&#24433;&#26174;&#33879;&#25552;&#39640;&#20102;&#28210;&#26579;&#36895;&#24230;&#65292;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;&#39640;&#26031;&#30340;&#20998;&#21106;&#26041;&#27861;&#65288;&#20363;&#22914;&#39640;&#26031;&#20998;&#32452;&#65289;&#22312;&#38646;&#26679;&#26412;&#20998;&#21106;&#20013;&#27809;&#26377;&#25552;&#20379;&#32039;&#20945;&#30340;&#20998;&#21106;&#25513;&#27169;&#65292;&#20027;&#35201;&#21407;&#22240;&#26159;&#22312;&#36935;&#21040;&#19981;&#19968;&#33268;&#30340;2D&#26426;&#22120;&#29983;&#25104;&#26631;&#31614;&#26102;&#65292;&#26080;&#27861;&#30452;&#25509;&#20026;&#27599;&#20010;&#39640;&#26031;&#20998;&#37197;&#21487;&#23398;&#20064;&#21442;&#25968;&#65292;&#32570;&#20047;&#40065;&#26834;&#24615;&#21644;&#32039;&#20945;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;&#27973;&#23618;&#35299;&#30721;&#32593;&#32476;&#23558;&#27599;&#20010;&#39640;&#26031;&#28857;&#30340;&#34701;&#21512;&#31354;&#38388;&#21644;&#35821;&#20041;&#19978;&#26377;&#24847;&#20041;&#30340;&#29305;&#24449;&#26144;&#23556;&#65292;&#36805;&#36895;&#23454;&#29616;&#32039;&#20945;&#19988;&#21487;&#38752;&#30340;&#38646;&#26679;&#26412;&#22330;&#26223;&#20998;&#21106;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose Compact and Swift Segmenting 3D Gaussians(CoSSegGaussians), a method for compact 3D-consistent scene segmentation at fast rendering speed with only RGB images input. Previous NeRF-based 3D segmentation methods have relied on implicit or voxel neural scene representation and ray-marching volume rendering which are time consuming. Recent 3D Gaussian Splatting significantly improves the rendering speed, however, existing Gaussians-based segmentation methods(eg: Gaussian Grouping) fail to provide compact segmentation masks especially in zero-shot segmentation, which is mainly caused by the lack of robustness and compactness for straightforwardly assigning learnable parameters to each Gaussian when encountering inconsistent 2D machine-generated labels. Our method aims to achieve compact and reliable zero-shot scene segmentation swiftly by mapping fused spatial and semantically meaningful features for each Gaussian point with a shallow decoding network. Specifically, our method fi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#24067;&#40065;&#22982;&#31246;&#21153;&#23398;&#27966;&#21019;&#24314;&#25945;&#32946;&#27979;&#39564;&#30340;&#26041;&#27861;&#65292;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#25945;&#24072;&#26356;&#20542;&#21521;&#20110;&#20351;&#29992;&#33258;&#21160;&#29983;&#25104;&#30340;&#38382;&#39064;&#25776;&#20889;&#27979;&#39564;&#65292;&#24182;&#19988;&#36825;&#20123;&#38382;&#39064;&#30340;&#36136;&#37327;&#19981;&#20122;&#20110;&#25163;&#20889;&#29256;&#26412;&#65292;&#29978;&#33267;&#26377;&#21487;&#33021;&#25552;&#39640;&#27979;&#39564;&#30340;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2401.05914</link><description>&lt;p&gt;
&#25945;&#24072;&#22914;&#20309;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#24067;&#40065;&#22982;&#31246;&#21153;&#23398;&#27966;&#21019;&#24314;&#25945;&#32946;&#27979;&#39564;
&lt;/p&gt;
&lt;p&gt;
How Teachers Can Use Large Language Models and Bloom's Taxonomy to Create Educational Quizzes. (arXiv:2401.05914v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05914
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#24067;&#40065;&#22982;&#31246;&#21153;&#23398;&#27966;&#21019;&#24314;&#25945;&#32946;&#27979;&#39564;&#30340;&#26041;&#27861;&#65292;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#25945;&#24072;&#26356;&#20542;&#21521;&#20110;&#20351;&#29992;&#33258;&#21160;&#29983;&#25104;&#30340;&#38382;&#39064;&#25776;&#20889;&#27979;&#39564;&#65292;&#24182;&#19988;&#36825;&#20123;&#38382;&#39064;&#30340;&#36136;&#37327;&#19981;&#20122;&#20110;&#25163;&#20889;&#29256;&#26412;&#65292;&#29978;&#33267;&#26377;&#21487;&#33021;&#25552;&#39640;&#27979;&#39564;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38382;&#39064;&#29983;&#25104;(QG)&#26159;&#19968;&#39033;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#65292;&#22312;&#25945;&#32946;&#39046;&#22495;&#20855;&#26377;&#20016;&#23500;&#30340;&#28508;&#22312;&#30410;&#22788;&#21644;&#29992;&#36884;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28508;&#21147;&#65292;&#24517;&#39035;&#20197;&#25945;&#32946;&#38656;&#27714;&#20026;&#30446;&#26631;&#35774;&#35745;&#21644;&#39564;&#35777;QG&#31995;&#32479;&#12290;&#28982;&#32780;&#65292;&#24456;&#23569;&#26377;&#30740;&#31350;&#35780;&#20272;&#25110;&#35774;&#35745;QG&#26041;&#27861;&#26102;&#24471;&#21040;&#30495;&#23454;&#25945;&#24072;&#25110;&#23398;&#29983;&#30340;&#21453;&#39304;&#12290;&#26412;&#25991;&#24212;&#29992;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;QG&#26041;&#27861;&#65292;&#22312;&#29983;&#25104;&#38382;&#39064;&#26102;&#20351;&#29992;&#24067;&#40065;&#22982;&#31246;&#21153;&#23398;&#27966;&#30340;&#23398;&#20064;&#30446;&#26631;&#12290;&#33258;&#21160;&#29983;&#25104;&#30340;&#38382;&#39064;&#22312;&#22810;&#20010;&#23454;&#39564;&#20013;&#34987;&#29992;&#20110;&#35780;&#20272;&#25945;&#24072;&#30340;&#23454;&#38469;&#20351;&#29992;&#24773;&#20917;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25945;&#24072;&#26356;&#21916;&#27426;&#20351;&#29992;&#33258;&#21160;&#29983;&#25104;&#30340;&#38382;&#39064;&#25776;&#20889;&#27979;&#39564;&#65292;&#24182;&#19988;&#19982;&#25163;&#20889;&#29256;&#26412;&#30456;&#27604;&#65292;&#36825;&#26679;&#30340;&#27979;&#39564;&#36136;&#37327;&#27809;&#26377;&#25439;&#22833;&#12290;&#27492;&#22806;&#65292;&#20960;&#20010;&#25351;&#26631;&#34920;&#26126;&#33258;&#21160;&#29983;&#25104;&#30340;&#38382;&#39064;&#29978;&#33267;&#21487;&#20197;&#25552;&#39640;&#25152;&#21019;&#24314;&#27979;&#39564;&#30340;&#36136;&#37327;&#65292;&#23637;&#31034;&#20102;&#22312;&#22823;&#35268;&#27169;&#20351;&#29992;QG&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Question generation (QG) is a natural language processing task with an abundance of potential benefits and use cases in the educational domain. In order for this potential to be realized, QG systems must be designed and validated with pedagogical needs in mind. However, little research has assessed or designed QG approaches with the input from real teachers or students. This paper applies a large language model-based QG approach where questions are generated with learning goals derived from Bloom's taxonomy. The automatically generated questions are used in multiple experiments designed to assess how teachers use them in practice. The results demonstrate that teachers prefer to write quizzes with automatically generated questions, and that such quizzes have no loss in quality compared to handwritten versions. Further, several metrics indicate that automatically generated questions can even improve the quality of the quizzes created, showing the promise for large scale use of QG in the 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20108;&#36827;&#21046;&#32447;&#24615;&#26641;&#25552;&#20132;&#30340;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#25152;&#26377;&#26435;&#20445;&#25252;&#27169;&#22411;&#65292;&#36890;&#36807;&#39564;&#35777;&#35745;&#31639;&#30340;&#23436;&#25972;&#24615;&#21644;&#25928;&#26524;&#65292;&#35299;&#20915;&#20102;&#27169;&#22411;&#25152;&#26377;&#26435;&#30340;&#20914;&#31361;&#38382;&#39064;&#65292;&#24182;&#38477;&#20302;&#20102;&#26356;&#26032;&#35777;&#26126;&#30340;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2401.05895</link><description>&lt;p&gt;
&#22522;&#20110;&#20108;&#36827;&#21046;&#32447;&#24615;&#26641;&#25552;&#20132;&#30340;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#25152;&#26377;&#26435;&#20445;&#25252;
&lt;/p&gt;
&lt;p&gt;
Binary Linear Tree Commitment-based Ownership Protection for Distributed Machine Learning. (arXiv:2401.05895v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05895
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20108;&#36827;&#21046;&#32447;&#24615;&#26641;&#25552;&#20132;&#30340;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#25152;&#26377;&#26435;&#20445;&#25252;&#27169;&#22411;&#65292;&#36890;&#36807;&#39564;&#35777;&#35745;&#31639;&#30340;&#23436;&#25972;&#24615;&#21644;&#25928;&#26524;&#65292;&#35299;&#20915;&#20102;&#27169;&#22411;&#25152;&#26377;&#26435;&#30340;&#20914;&#31361;&#38382;&#39064;&#65292;&#24182;&#38477;&#20302;&#20102;&#26356;&#26032;&#35777;&#26126;&#30340;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#36890;&#36807;&#23558;&#35745;&#31639;&#20219;&#21153;&#20998;&#37197;&#32473;&#22810;&#20010;&#24037;&#20316;&#33410;&#28857;&#65292;&#23454;&#29616;&#20102;&#23545;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#30340;&#24182;&#34892;&#35757;&#32451;&#12290;&#20294;&#26159;&#65292;&#26368;&#32456;&#27169;&#22411;&#26435;&#37325;&#30340;&#20256;&#25773;&#24448;&#24448;&#20250;&#23548;&#33268;&#27169;&#22411;&#25152;&#26377;&#26435;&#30340;&#28508;&#22312;&#20914;&#31361;&#65292;&#22240;&#20026;&#24037;&#20316;&#33410;&#28857;&#38590;&#20197;&#35777;&#26126;&#33258;&#24049;&#22312;&#35757;&#32451;&#35745;&#31639;&#20013;&#30340;&#21442;&#19982;&#24230;&#12290;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#25152;&#26377;&#26435;&#38382;&#39064;&#65292;&#24182;&#38450;&#27490;&#24847;&#22806;&#25925;&#38556;&#21644;&#24694;&#24847;&#25915;&#20987;&#65292;&#22312;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#20013;&#39564;&#35777;&#24037;&#20316;&#33410;&#28857;&#30340;&#35745;&#31639;&#23436;&#25972;&#24615;&#21644;&#25928;&#26524;&#21464;&#24471;&#23588;&#20026;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#20108;&#36827;&#21046;&#32447;&#24615;&#26641;&#25552;&#20132;&#30340;&#25152;&#26377;&#26435;&#20445;&#25252;&#27169;&#22411;&#65292;&#20197;&#30830;&#20445;&#35745;&#31639;&#30340;&#23436;&#25972;&#24615;&#65292;&#21516;&#26102;&#20445;&#35777;&#24320;&#38144;&#26377;&#38480;&#21644;&#35777;&#26126;&#31616;&#27905;&#12290;&#30001;&#20110;&#35757;&#32451;&#36807;&#31243;&#20013;&#21442;&#25968;&#30340;&#39057;&#32321;&#26356;&#26032;&#65292;&#25105;&#20204;&#30340;&#25552;&#20132;&#26041;&#26696;&#24341;&#20837;&#20102;&#21487;&#32500;&#25252;&#30340;&#26641;&#32467;&#26500;&#65292;&#38477;&#20302;&#20102;&#26356;&#26032;&#35777;&#26126;&#30340;&#25104;&#26412;&#12290;&#19982;&#22522;&#20110;SNARK&#30340;&#39564;&#35777;&#35745;&#31639;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#21516;&#26102;&#25903;&#25345;&#25209;&#37327;&#35757;&#32451;&#21644;&#22312;&#32447;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Distributed machine learning enables parallel training of extensive datasets by delegating computing tasks across multiple workers. Despite the cost reduction benefits of distributed machine learning, the dissemination of final model weights often leads to potential conflicts over model ownership as workers struggle to substantiate their involvement in the training computation. To address the above ownership issues and prevent accidental failures and malicious attacks, verifying the computational integrity and effectiveness of workers becomes particularly crucial in distributed machine learning. In this paper, we proposed a novel binary linear tree commitment-based ownership protection model to ensure computational integrity with limited overhead and concise proof. Due to the frequent updates of parameters during training, our commitment scheme introduces a maintainable tree structure to reduce the costs of updating proofs. Distinguished from SNARK-based verifiable computation, our mod
&lt;/p&gt;</description></item><item><title>HiCAST&#26159;&#19968;&#31181;&#39640;&#24230;&#23450;&#21046;&#30340;&#20219;&#24847;&#39118;&#26684;&#36716;&#25442;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;"&#39118;&#26684;&#36866;&#37197;&#22120;"&#21644;&#21033;&#29992;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#65292;&#21487;&#20197;&#26681;&#25454;&#21508;&#31181;&#35821;&#20041;&#32447;&#32034;&#26174;&#24335;&#23450;&#21046;&#39118;&#26684;&#21270;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.05870</link><description>&lt;p&gt;
HiCAST: &#20351;&#29992;&#22686;&#24378;&#25193;&#25955;&#27169;&#22411;&#30340;&#39640;&#24230;&#23450;&#21046;&#30340;&#20219;&#24847;&#39118;&#26684;&#36716;&#25442;
&lt;/p&gt;
&lt;p&gt;
HiCAST: Highly Customized Arbitrary Style Transfer with Adapter Enhanced Diffusion Models. (arXiv:2401.05870v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05870
&lt;/p&gt;
&lt;p&gt;
HiCAST&#26159;&#19968;&#31181;&#39640;&#24230;&#23450;&#21046;&#30340;&#20219;&#24847;&#39118;&#26684;&#36716;&#25442;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;"&#39118;&#26684;&#36866;&#37197;&#22120;"&#21644;&#21033;&#29992;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#65292;&#21487;&#20197;&#26681;&#25454;&#21508;&#31181;&#35821;&#20041;&#32447;&#32034;&#26174;&#24335;&#23450;&#21046;&#39118;&#26684;&#21270;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20219;&#24847;&#39118;&#26684;&#36716;&#25442;&#65288;AST&#65289;&#30340;&#30446;&#26631;&#26159;&#23558;&#39118;&#26684;&#21442;&#32771;&#30340;&#33402;&#26415;&#29305;&#24449;&#27880;&#20837;&#32473;&#23450;&#30340;&#22270;&#20687;/&#35270;&#39057;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#20851;&#27880;&#39118;&#26684;&#21644;&#20869;&#23481;&#20043;&#38388;&#30340;&#24179;&#34913;&#65292;&#24573;&#35270;&#20102;&#23545;&#28789;&#27963;&#21644;&#23450;&#21046;&#21270;&#39118;&#26684;&#21270;&#32467;&#26524;&#30340;&#37325;&#35201;&#38656;&#27714;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#20851;&#38190;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;AST&#26041;&#27861;&#65292;&#31216;&#20026;HiCAST&#65292;&#23427;&#33021;&#22815;&#26681;&#25454;&#21508;&#31181;&#35821;&#20041;&#32447;&#32034;&#26174;&#24335;&#23450;&#21046;&#39118;&#26684;&#21270;&#32467;&#26524;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22522;&#20110;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#65288;LDM&#65289;&#26500;&#24314;&#65292;&#24182;&#31934;&#24515;&#35774;&#35745;&#20197;&#21560;&#25910;LDM&#30340;&#20869;&#23481;&#21644;&#39118;&#26684;&#23454;&#20363;&#26465;&#20214;&#12290;&#20854;&#29305;&#28857;&#26159;&#24341;&#20837;&#20102;"&#39118;&#26684;&#36866;&#37197;&#22120;"&#65292;&#20801;&#35768;&#29992;&#25143;&#36890;&#36807;&#23545;&#40784;LDM&#20013;&#30340;&#22810;&#23618;&#39118;&#26684;&#20449;&#24687;&#21644;&#20869;&#22312;&#30693;&#35782;&#26469;&#28789;&#27963;&#25805;&#20316;&#36755;&#20986;&#32467;&#26524;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25193;&#23637;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#20197;&#25191;&#34892;&#35270;&#39057;AST&#12290;&#20511;&#21161;&#19968;&#31181;&#26032;&#39062;&#30340;&#23398;&#20064;&#30446;&#26631;&#36827;&#34892;&#20102;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
The goal of Arbitrary Style Transfer (AST) is injecting the artistic features of a style reference into a given image/video. Existing methods usually focus on pursuing the balance between style and content, whereas ignoring the significant demand for flexible and customized stylization results and thereby limiting their practical application. To address this critical issue, a novel AST approach namely HiCAST is proposed, which is capable of explicitly customizing the stylization results according to various source of semantic clues. In the specific, our model is constructed based on Latent Diffusion Model (LDM) and elaborately designed to absorb content and style instance as conditions of LDM. It is characterized by introducing of \textit{Style Adapter}, which allows user to flexibly manipulate the output results by aligning multi-level style information and intrinsic knowledge in LDM. Lastly, we further extend our model to perform video AST. A novel learning objective is leveraged for
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21152;&#36895;&#24230;&#35745;&#25968;&#25454;&#25512;&#26029;&#25104;&#21151;&#21644;&#22833;&#36133;&#30340;&#35762;&#35805;&#24847;&#22270;&#65292;&#22312;&#37326;&#22806;&#29615;&#22659;&#20013;&#30340;&#30740;&#31350;&#34920;&#26126;&#21152;&#36895;&#24230;&#35745;&#25968;&#25454;&#20013;&#23384;&#22312;&#26377;&#29992;&#30340;&#20449;&#24687;&#65292;&#20294;&#19981;&#36275;&#20197;&#21487;&#38752;&#22320;&#25429;&#25417;&#35762;&#35805;&#24847;&#22270;&#12290;</title><link>http://arxiv.org/abs/2401.05849</link><description>&lt;p&gt;
&#36890;&#36807;&#21152;&#36895;&#24230;&#35745;&#25968;&#25454;&#25512;&#26029;&#35762;&#35805;&#24847;&#22270;&#8212;&#8212;&#37326;&#22806;&#29615;&#22659;&#20013;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Inferring Intentions to Speak Using Accelerometer Data In-the-Wild. (arXiv:2401.05849v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05849
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21152;&#36895;&#24230;&#35745;&#25968;&#25454;&#25512;&#26029;&#25104;&#21151;&#21644;&#22833;&#36133;&#30340;&#35762;&#35805;&#24847;&#22270;&#65292;&#22312;&#37326;&#22806;&#29615;&#22659;&#20013;&#30340;&#30740;&#31350;&#34920;&#26126;&#21152;&#36895;&#24230;&#35745;&#25968;&#25454;&#20013;&#23384;&#22312;&#26377;&#29992;&#30340;&#20449;&#24687;&#65292;&#20294;&#19981;&#36275;&#20197;&#21487;&#38752;&#22320;&#25429;&#25417;&#35762;&#35805;&#24847;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#20855;&#26377;&#33391;&#22909;&#30340;&#33258;&#28982;&#30452;&#35273;&#65292;&#21487;&#20197;&#35782;&#21035;&#20986;&#20182;&#20154;&#26377;&#35805;&#35201;&#35828;&#30340;&#26102;&#20505;&#12290;&#22914;&#26524;&#20154;&#24037;&#26234;&#33021;&#20063;&#33021;&#35782;&#21035;&#20986;&#35762;&#35805;&#24847;&#22270;&#65292;&#23558;&#20250;&#38750;&#24120;&#26377;&#36259;&#12290;&#29305;&#21035;&#26159;&#22312;&#20154;&#24037;&#26234;&#33021;&#24341;&#23548;&#22242;&#20307;&#35752;&#35770;&#30340;&#22330;&#26223;&#19979;&#65292;&#36825;&#23558;&#26159;&#19968;&#39033;&#26377;&#29992;&#30340;&#25216;&#33021;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#36890;&#36807;&#21152;&#36895;&#24230;&#35745;&#25968;&#25454;&#25512;&#26029;&#25104;&#21151;&#21644;&#22833;&#36133;&#30340;&#35762;&#35805;&#24847;&#22270;&#12290;&#20043;&#25152;&#20197;&#36873;&#25321;&#21152;&#36895;&#24230;&#35745;&#25968;&#25454;&#65292;&#26159;&#22240;&#20026;&#23427;&#20855;&#26377;&#38544;&#31169;&#20445;&#25252;&#21151;&#33021;&#65292;&#21516;&#26102;&#22312;&#37326;&#22806;&#29615;&#22659;&#20013;&#26131;&#20110;&#23454;&#29616;&#65292;&#21487;&#20197;&#25918;&#32622;&#22312;&#26234;&#33021;&#24509;&#31456;&#19978;&#12290;&#20351;&#29992;&#30495;&#23454;&#31038;&#20132;&#32593;&#32476;&#20107;&#20214;&#30340;&#25968;&#25454;&#26469;&#35757;&#32451;&#19968;&#20010;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#26088;&#22312;&#25512;&#26029;&#35762;&#35805;&#24847;&#22270;&#12290;&#25968;&#25454;&#20013;&#30340;&#19968;&#37096;&#20998;&#19981;&#25104;&#21151;&#30340;&#35762;&#35805;&#24847;&#22270;&#26696;&#20363;&#34987;&#27880;&#37322;&#12290;&#27169;&#22411;&#22312;&#25104;&#21151;&#30340;&#35762;&#35805;&#24847;&#22270;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#22312;&#25104;&#21151;&#21644;&#22833;&#36133;&#30340;&#26696;&#20363;&#19978;&#36827;&#34892;&#35780;&#20272;&#12290;&#24635;&#20043;&#65292;&#21152;&#36895;&#24230;&#35745;&#25968;&#25454;&#20013;&#23384;&#22312;&#26377;&#29992;&#30340;&#20449;&#24687;&#65292;&#20294;&#19981;&#36275;&#20197;&#21487;&#38752;&#22320;&#25429;&#25417;&#35762;&#35805;&#24847;&#22270;&#12290;&#20363;&#22914;&#65292;&#23039;&#21183;&#21464;&#21270;&#19982;&#35762;&#35805;&#24847;&#22270;&#20855;&#26377;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Humans have good natural intuition to recognize when another person has something to say. It would be interesting if an AI can also recognize intentions to speak. Especially in scenarios when an AI is guiding a group discussion, this can be a useful skill. This work studies the inference of successful and unsuccessful intentions to speak from accelerometer data. This is chosen because it is privacy-preserving and feasible for in-the-wild settings since it can be placed in a smart badge. Data from a real-life social networking event is used to train a machine-learning model that aims to infer intentions to speak. A subset of unsuccessful intention-to-speak cases in the data is annotated. The model is trained on the successful intentions to speak and evaluated on both the successful and unsuccessful cases. In conclusion, there is useful information in accelerometer data, but not enough to reliably capture intentions to speak. For example, posture shifts are correlated with intentions to 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#22810;&#30446;&#26631;&#20248;&#21270;&#26041;&#27861;&#21644;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#22312;&#20171;&#30005;&#26448;&#26009;&#30740;&#31350;&#20013;&#21462;&#24471;&#20102;&#31361;&#30772;&#12290;&#30740;&#31350;&#20154;&#21592;&#25104;&#21151;&#22320;&#21512;&#25104;&#21644;&#34920;&#24449;&#20102;&#20004;&#31181;&#26032;&#22411;&#20171;&#30005;&#26448;&#26009;&#65292;CsTaTeO6&#21644;Bi2Zr2O7&#65292;&#20026;&#26410;&#30693;&#26448;&#26009;&#30340;&#23547;&#25214;&#25552;&#20379;&#20102;&#39640;&#25928;&#30340;&#24037;&#20316;&#27969;&#31243;&#12290;</title><link>http://arxiv.org/abs/2401.05848</link><description>&lt;p&gt;
&#25512;&#21160;&#24102;&#38553;&#21644;&#20171;&#30005;&#24120;&#25968;&#30340;&#24085;&#32047;&#25176;&#21069;&#27839;&#65306;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#24341;&#23548;&#30340;&#23547;&#25214;&#20171;&#30005;&#26448;&#26009;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Pushing the Pareto front of band gap and permittivity: ML-guided search for dielectric materials. (arXiv:2401.05848v1 [cond-mat.mtrl-sci])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05848
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#22810;&#30446;&#26631;&#20248;&#21270;&#26041;&#27861;&#21644;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#22312;&#20171;&#30005;&#26448;&#26009;&#30740;&#31350;&#20013;&#21462;&#24471;&#20102;&#31361;&#30772;&#12290;&#30740;&#31350;&#20154;&#21592;&#25104;&#21151;&#22320;&#21512;&#25104;&#21644;&#34920;&#24449;&#20102;&#20004;&#31181;&#26032;&#22411;&#20171;&#30005;&#26448;&#26009;&#65292;CsTaTeO6&#21644;Bi2Zr2O7&#65292;&#20026;&#26410;&#30693;&#26448;&#26009;&#30340;&#23547;&#25214;&#25552;&#20379;&#20102;&#39640;&#25928;&#30340;&#24037;&#20316;&#27969;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#20171;&#30005;&#24120;&#25968;&#30340;&#26448;&#26009;&#22312;&#22806;&#37096;&#30005;&#22330;&#19979;&#23481;&#26131;&#26497;&#21270;&#65292;&#22312;&#35768;&#22810;&#29616;&#20195;&#30005;&#23376;&#35774;&#22791;&#20013;&#21457;&#25381;&#30528;&#37325;&#35201;&#30340;&#21151;&#33021;&#12290;&#23427;&#20204;&#30340;&#23454;&#38469;&#25928;&#29992;&#30001;&#20004;&#20010;&#30456;&#20114;&#20914;&#31361;&#30340;&#29305;&#24615;&#20915;&#23450;&#65306;&#39640;&#20171;&#30005;&#24120;&#25968;&#24448;&#24448;&#20986;&#29616;&#22312;&#24102;&#38553;&#36739;&#31364;&#30340;&#26448;&#26009;&#20013;&#65292;&#38480;&#21046;&#20102;&#20171;&#30005;&#20987;&#31359;&#20043;&#21069;&#30340;&#24037;&#20316;&#30005;&#21387;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#36890;&#37327;&#30340;&#24037;&#20316;&#27969;&#31243;&#65292;&#23558;&#20803;&#32032;&#26367;&#20195;&#12289;&#26426;&#22120;&#23398;&#20064;&#39044;&#31579;&#36873;&#12289;&#20174;&#22836;&#35745;&#31639;&#27169;&#25311;&#21644;&#20154;&#31867;&#19987;&#23478;&#30452;&#35273;&#30456;&#32467;&#21512;&#65292;&#20197;&#26377;&#25928;&#22320;&#25506;&#32034;&#26410;&#30693;&#26448;&#26009;&#30340;&#28508;&#22312;&#20171;&#30005;&#24615;&#33021;&#65292;&#20174;&#32780;&#21512;&#25104;&#21644;&#34920;&#24449;&#20004;&#31181;&#26032;&#22411;&#30340;&#20171;&#30005;&#26448;&#26009;CsTaTeO6&#21644;Bi2Zr2O7&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#24605;&#24819;&#26159;&#22312;&#20984;&#38754;&#24085;&#32047;&#25176;&#21069;&#27839;&#30340;&#22810;&#30446;&#26631;&#20248;&#21270;&#29615;&#22659;&#20013;&#24212;&#29992;&#26426;&#22120;&#23398;&#20064;&#12290;&#34429;&#28982;&#36890;&#24120;&#34987;&#35748;&#20026;&#27604;&#21333;&#30446;&#26631;&#20248;&#21270;&#26356;&#20855;&#25361;&#25112;&#24615;&#65292;&#20294;&#25105;&#20204;&#25552;&#20986;&#24182;&#23637;&#31034;&#20102;&#21021;&#27493;&#35777;&#25454;&#34920;&#26126;&#24102;&#38553;&#21644;&#20171;&#30005;&#24120;&#25968;&#20043;&#38388;&#30340;1/x&#30456;&#20851;&#24615;&#23454;&#38469;&#19978;&#20351;&#20219;&#21153;&#21464;      &#24471;&#23481;&#26131;&#12290;
&lt;/p&gt;
&lt;p&gt;
Materials with high-dielectric constant easily polarize under external electric fields, allowing them to perform essential functions in many modern electronic devices. Their practical utility is determined by two conflicting properties: high dielectric constants tend to occur in materials with narrow band gaps, limiting the operating voltage before dielectric breakdown. We present a high-throughput workflow that combines element substitution, ML pre-screening, ab initio simulation and human expert intuition to efficiently explore the vast space of unknown materials for potential dielectrics, leading to the synthesis and characterization of two novel dielectric materials, CsTaTeO6 and Bi2Zr2O7. Our key idea is to deploy ML in a multi-objective optimization setting with concave Pareto front. While usually considered more challenging than single-objective optimization, we argue and show preliminary evidence that the $1/x$-correlation between band gap and permittivity in fact makes the tas
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#35745;&#31639;&#26694;&#26550;&#65292;&#21487;&#20197;&#23545;&#19981;&#21516;&#24418;&#24335;&#30340;AI&#36741;&#21161;&#23545;&#20915;&#31574;&#32773;&#22312;AI&#36741;&#21161;&#20915;&#31574;&#20013;&#30340;&#24433;&#21709;&#36827;&#34892;&#35299;&#37322;&#24615;&#25551;&#36848;&#12290;</title><link>http://arxiv.org/abs/2401.05840</link><description>&lt;p&gt;
&#35299;&#30721;AI&#30340;&#21161;&#25512;&#65306;&#39044;&#27979;AI&#36741;&#21161;&#20915;&#31574;&#20013;&#20154;&#31867;&#34892;&#20026;&#30340;&#32479;&#19968;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Decoding AI's Nudge: A Unified Framework to Predict Human Behavior in AI-assisted Decision Making. (arXiv:2401.05840v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05840
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#35745;&#31639;&#26694;&#26550;&#65292;&#21487;&#20197;&#23545;&#19981;&#21516;&#24418;&#24335;&#30340;AI&#36741;&#21161;&#23545;&#20915;&#31574;&#32773;&#22312;AI&#36741;&#21161;&#20915;&#31574;&#20013;&#30340;&#24433;&#21709;&#36827;&#34892;&#35299;&#37322;&#24615;&#25551;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22522;&#20110;AI&#30340;&#20915;&#31574;&#36741;&#21161;&#24037;&#20855;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#19981;&#21516;&#24418;&#24335;&#30340;AI&#36741;&#21161;&#36234;&#26469;&#36234;&#22810;&#22320;&#34701;&#20837;&#21040;&#20154;&#31867;&#20915;&#31574;&#36807;&#31243;&#20013;&#12290;&#20026;&#20102;&#26368;&#22909;&#22320;&#25903;&#25345;&#20154;&#31867;&#22312;&#20915;&#31574;&#36807;&#31243;&#20013;&#65292;&#24517;&#39035;&#23450;&#37327;&#22320;&#20102;&#35299;&#19981;&#21516;&#24418;&#24335;&#30340;AI&#36741;&#21161;&#22914;&#20309;&#24433;&#21709;&#20154;&#31867;&#30340;&#20915;&#31574;&#34892;&#20026;&#12290;&#20026;&#27492;&#65292;&#24403;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#20351;&#29992;&#8220;&#40657;&#30418;&#23376;&#8221;&#27169;&#22411;&#36827;&#34892;&#20154;&#31867;&#34892;&#20026;&#30340;&#31471;&#21040;&#31471;&#39044;&#27979;&#65292;&#24120;&#24120;&#32570;&#20047;&#23545;AI&#36741;&#21161;&#23545;&#20154;&#31867;&#20915;&#31574;&#36807;&#31243;&#20135;&#29983;&#24494;&#22937;&#24433;&#21709;&#30340;&#35299;&#37322;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#20248;&#20808;&#32771;&#34385;&#20154;&#31867;&#34892;&#20026;&#39044;&#27979;&#35299;&#37322;&#24615;&#30340;&#26041;&#27861;&#24120;&#24120;&#21482;&#38024;&#23545;&#26576;&#19968;&#29305;&#23450;&#24418;&#24335;&#30340;AI&#36741;&#21161;&#36827;&#34892;&#35843;&#25972;&#65292;&#38590;&#20197;&#36866;&#24212;&#20854;&#20182;&#24418;&#24335;&#30340;&#36741;&#21161;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#35745;&#31639;&#26694;&#26550;&#65292;&#21487;&#20197;&#23545;&#19981;&#21516;&#24418;&#24335;&#30340;AI&#36741;&#21161;&#23545;&#20915;&#31574;&#32773;&#22312;AI&#36741;&#21161;&#20915;&#31574;&#20013;&#30340;&#24433;&#21709;&#36827;&#34892;&#35299;&#37322;&#24615;&#25551;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the rapid development of AI-based decision aids, different forms of AI assistance have been increasingly integrated into the human decision making processes. To best support humans in decision making, it is essential to quantitatively understand how diverse forms of AI assistance influence humans' decision making behavior. To this end, much of the current research focuses on the end-to-end prediction of human behavior using ``black-box'' models, often lacking interpretations of the nuanced ways in which AI assistance impacts the human decision making process. Meanwhile, methods that prioritize the interpretability of human behavior predictions are often tailored for one specific form of AI assistance, making adaptations to other forms of assistance difficult. In this paper, we propose a computational framework that can provide an interpretable characterization of the influence of different forms of AI assistance on decision makers in AI-assisted decision making. By conceptualizing
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32858;&#21512;&#31574;&#30053;&#65292;&#29992;&#20110;&#35780;&#20272;&#32858;&#31867;&#36136;&#37327;&#12290;&#36890;&#36807;&#23545;&#32858;&#31867;&#32423;&#21035;&#30340;&#36718;&#24275;&#24471;&#20998;&#36827;&#34892;&#24179;&#22343;&#65292;&#24182;&#22312;&#27492;&#22522;&#30784;&#19978;&#23545;&#25152;&#26377;&#32858;&#31867;&#30340;&#24471;&#20998;&#36827;&#34892;&#23439;&#35266;&#24179;&#22343;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#23439;&#35266;&#24179;&#22343;&#36718;&#24275;&#24471;&#20998;&#23545;&#20110;&#32858;&#31867;&#19981;&#22343;&#34913;&#21644;&#32972;&#26223;&#22122;&#22768;&#26159;&#31283;&#20581;&#30340;&#12290;</title><link>http://arxiv.org/abs/2401.05831</link><description>&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#36718;&#24275;&#31995;&#25968;&#65306;&#20174;&#24494;&#35266;&#21040;&#23439;&#35266;&#32858;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Revisiting Silhouette: From Micro to Macro Aggregation. (arXiv:2401.05831v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05831
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32858;&#21512;&#31574;&#30053;&#65292;&#29992;&#20110;&#35780;&#20272;&#32858;&#31867;&#36136;&#37327;&#12290;&#36890;&#36807;&#23545;&#32858;&#31867;&#32423;&#21035;&#30340;&#36718;&#24275;&#24471;&#20998;&#36827;&#34892;&#24179;&#22343;&#65292;&#24182;&#22312;&#27492;&#22522;&#30784;&#19978;&#23545;&#25152;&#26377;&#32858;&#31867;&#30340;&#24471;&#20998;&#36827;&#34892;&#23439;&#35266;&#24179;&#22343;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#23439;&#35266;&#24179;&#22343;&#36718;&#24275;&#24471;&#20998;&#23545;&#20110;&#32858;&#31867;&#19981;&#22343;&#34913;&#21644;&#32972;&#26223;&#22122;&#22768;&#26159;&#31283;&#20581;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36718;&#24275;&#31995;&#25968;&#26159;&#19968;&#31181;&#24120;&#29992;&#30340;&#20869;&#37096;&#32858;&#31867;&#35780;&#20272;&#25351;&#26631;&#65292;&#23427;&#20250;&#20026;&#27599;&#20010;&#25968;&#25454;&#28857;&#20135;&#29983;&#19968;&#20010;&#24471;&#20998;&#65292;&#29992;&#20110;&#35780;&#20272;&#20854;&#32858;&#31867;&#20998;&#37197;&#30340;&#36136;&#37327;&#12290;&#30446;&#21069;&#65292;&#20026;&#20102;&#35780;&#20272;&#25972;&#20010;&#25968;&#25454;&#38598;&#30340;&#32858;&#31867;&#36136;&#37327;&#65292;&#36890;&#24120;&#20250;&#23558;&#25968;&#25454;&#38598;&#20013;&#25152;&#26377;&#28857;&#30340;&#24471;&#20998;&#24179;&#22343;&#25104;&#19968;&#20010;&#21333;&#19968;&#20540;&#65292;&#36825;&#20010;&#31574;&#30053;&#34987;&#31216;&#20026;&#24494;&#35266;&#24179;&#22343;&#12290;&#28982;&#32780;&#25105;&#20204;&#22312;&#26412;&#25991;&#20013;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;&#21512;&#25104;&#20363;&#23376;&#23637;&#31034;&#20102;&#65292;&#35813;&#24494;&#35266;&#24179;&#22343;&#31574;&#30053;&#23545;&#20110;&#32858;&#31867;&#19981;&#22343;&#34913;&#21644;&#24322;&#24120;&#20540;&#65288;&#32972;&#26223;&#22122;&#22768;&#65289;&#38750;&#24120;&#25935;&#24863;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26367;&#20195;&#32858;&#21512;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#39318;&#20808;&#23545;&#32858;&#31867;&#32423;&#21035;&#30340;&#36718;&#24275;&#24471;&#20998;&#36827;&#34892;&#24179;&#22343;&#65292;&#28982;&#21518;&#20877;&#23545;&#25152;&#26377;&#32858;&#31867;&#30340;&#24471;&#20998;&#36827;&#34892;&#23439;&#35266;&#24179;&#22343;&#12290;&#22522;&#20110;&#30456;&#21516;&#30340;&#21512;&#25104;&#20363;&#23376;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25552;&#20986;&#30340;&#23439;&#35266;&#24179;&#22343;&#36718;&#24275;&#24471;&#20998;&#23545;&#20110;&#32858;&#31867;&#19981;&#22343;&#34913;&#21644;&#32972;&#26223;&#22122;&#22768;&#26159;&#31283;&#20581;&#30340;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#23454;&#39564;&#30740;&#31350;&#65292;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#25552;&#20986;&#30340;&#23439;&#35266;&#24179;&#22343;&#21464;&#20307;&#21487;&#20197;&#26356;&#22909;&#22320;&#20272;&#35745;&#30495;&#23454;&#30340;&#32858;&#31867;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Silhouette coefficient is an established internal clustering evaluation measure that produces a score per data point, assessing the quality of its clustering assignment. To assess the quality of the clustering of the whole dataset, the scores of all the points in the dataset are typically averaged into a single value, a strategy which we call as micro-averaging. As we illustrate in this work, by using a synthetic example, this micro-averaging strategy is sensitive both to cluster imbalance and outliers (background noise). To address these issues, we propose an alternative aggregation strategy, which first averages the silhouette scores at a cluster level and then (macro) averages the scores across the clusters. Based on the same synthetic example, we show that the proposed macro-averaged silhouette score is robust to cluster imbalance and background noise. We have conducted an experimental study showing that our macro-averaged variant provides better estimates of the ground truth numbe
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#21019;&#24314;&#20102;&#21307;&#23398;&#22270;&#20687;&#30340;&#24187;&#35273;&#22522;&#20934;&#35780;&#20272;&#65292;&#24182;&#20840;&#38754;&#35780;&#20272;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#65292;&#25581;&#31034;&#20102;&#24187;&#35273;&#29616;&#35937;&#22312;&#20020;&#24202;&#29615;&#22659;&#20013;&#30340;&#38480;&#21046;&#21644;&#21508;&#31181;&#25552;&#31034;&#31574;&#30053;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.05827</link><description>&lt;p&gt;
&#21307;&#23398;&#35270;&#35273;&#38382;&#31572;&#20013;&#30340;&#24187;&#35273;&#22522;&#20934;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Hallucination Benchmark in Medical Visual Question Answering. (arXiv:2401.05827v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05827
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#21019;&#24314;&#20102;&#21307;&#23398;&#22270;&#20687;&#30340;&#24187;&#35273;&#22522;&#20934;&#35780;&#20272;&#65292;&#24182;&#20840;&#38754;&#35780;&#20272;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#65292;&#25581;&#31034;&#20102;&#24187;&#35273;&#29616;&#35937;&#22312;&#20020;&#24202;&#29615;&#22659;&#20013;&#30340;&#38480;&#21046;&#21644;&#21508;&#31181;&#25552;&#31034;&#31574;&#30053;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#21644;&#35270;&#35273;&#27169;&#22411;&#22312;&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#19978;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#23588;&#20854;&#22312;&#21307;&#23398;&#65288;Med-VQA&#65289;&#39046;&#22495;&#30340;&#24212;&#29992;&#26174;&#31034;&#20986;&#20102;&#20026;&#21307;&#30103;&#25552;&#20379;&#26377;&#25928;&#35270;&#35273;&#21161;&#25163;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#20020;&#24202;&#29615;&#22659;&#20013;&#30340;&#24187;&#35273;&#29616;&#35937;&#19978;&#24182;&#27809;&#26377;&#36827;&#34892;&#24191;&#27867;&#27979;&#35797;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#21307;&#23398;&#22270;&#20687;&#37197;&#23545;&#38382;&#39064;-&#22238;&#31572;&#38598;&#30340;&#24187;&#35273;&#22522;&#20934;&#35780;&#20272;&#65292;&#24182;&#23545;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#12290;&#35813;&#30740;&#31350;&#23545;&#24403;&#21069;&#27169;&#22411;&#30340;&#23616;&#38480;&#24615;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#65292;&#24182;&#25581;&#31034;&#20102;&#21508;&#31181;&#25552;&#31034;&#31574;&#30053;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent success of large language and vision models on vision question answering (VQA), particularly their applications in medicine (Med-VQA), has shown a great potential of realizing effective visual assistants for healthcare. However, these models are not extensively tested on the hallucination phenomenon in clinical settings. Here, we created a hallucination benchmark of medical images paired with question-answer sets and conducted a comprehensive evaluation of the state-of-the-art models. The study provides an in-depth analysis of current models limitations and reveals the effectiveness of various prompting strategies.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#21487;&#20197;&#36890;&#36807;&#19982;&#29992;&#25143;&#23545;&#35805;&#35299;&#20915;&#28436;&#21270;&#38382;&#39064;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#24212;&#29992;&#23545;&#35805;&#24335;DQN&#26234;&#33021;&#20307;&#35299;&#20915;&#28436;&#21270;&#38382;&#39064;&#30340;&#26550;&#26500;&#65292;&#24182;&#25506;&#32034;&#20102;&#35838;&#31243;&#23398;&#20064;&#21644;&#25913;&#21464;&#22870;&#21169;&#20989;&#25968;&#31561;&#35757;&#32451;&#26041;&#27861;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2401.05822</link><description>&lt;p&gt;
&#38754;&#21521;&#30446;&#26631;&#23548;&#21521;&#26234;&#33021;&#20307;&#30340;&#28436;&#21270;&#38382;&#39064;&#30340;&#20250;&#35805;&#35266;&#23519;
&lt;/p&gt;
&lt;p&gt;
Towards Goal-Oriented Agents for Evolving Problems Observed via Conversation. (arXiv:2401.05822v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05822
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#21487;&#20197;&#36890;&#36807;&#19982;&#29992;&#25143;&#23545;&#35805;&#35299;&#20915;&#28436;&#21270;&#38382;&#39064;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#24212;&#29992;&#23545;&#35805;&#24335;DQN&#26234;&#33021;&#20307;&#35299;&#20915;&#28436;&#21270;&#38382;&#39064;&#30340;&#26550;&#26500;&#65292;&#24182;&#25506;&#32034;&#20102;&#35838;&#31243;&#23398;&#20064;&#21644;&#25913;&#21464;&#22870;&#21169;&#20989;&#25968;&#31561;&#35757;&#32451;&#26041;&#27861;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30340;&#30446;&#26631;&#26159;&#36890;&#36807;&#19982;&#29992;&#25143;&#20132;&#27969;&#35299;&#20915;&#19981;&#33021;&#30452;&#25509;&#35266;&#23519;&#21040;&#30340;&#28436;&#21270;&#38382;&#39064;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#35757;&#32451;&#12290;&#31995;&#32479;&#21253;&#25324;&#19968;&#20010;&#34394;&#25311;&#38382;&#39064;&#65288;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#26159;&#19968;&#20010;&#31616;&#21333;&#30340;&#28216;&#25103;&#65289;&#65292;&#19968;&#20010;&#33021;&#22815;&#22238;&#31572;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#24182;&#33021;&#22815;&#35266;&#23519;&#21644;&#25191;&#34892;&#38382;&#39064;&#21160;&#20316;&#30340;&#27169;&#25311;&#29992;&#25143;&#65292;&#20197;&#21450;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;Q&#32593;&#32476;&#65288;DQN&#65289;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#26550;&#26500;&#12290;&#36890;&#36807;&#19982;&#27169;&#25311;&#29992;&#25143;&#36827;&#34892;&#23545;&#35805;&#65292;&#24182;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#26469;&#35757;&#32451;&#32842;&#22825;&#26426;&#22120;&#20154;&#35299;&#20915;&#38382;&#39064;&#12290;&#26412;&#25991;&#30340;&#36129;&#29486;&#21253;&#25324;&#65306;&#25552;&#20986;&#20102;&#19968;&#31181;&#24212;&#29992;&#23545;&#35805;&#24335;DQN&#26234;&#33021;&#20307;&#35299;&#20915;&#28436;&#21270;&#38382;&#39064;&#30340;&#26550;&#26500;&#65292;&#25506;&#32034;&#20102;&#35838;&#31243;&#23398;&#20064;&#31561;&#35757;&#32451;&#26041;&#27861;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#20197;&#21450;&#22312;&#29615;&#22659;&#22797;&#26434;&#24615;&#22686;&#21152;&#30340;&#24773;&#20917;&#19979;&#25913;&#21464;&#22870;&#21169;&#20989;&#25968;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The objective of this work is to train a chatbot capable of solving evolving problems through conversing with a user about a problem the chatbot cannot directly observe. The system consists of a virtual problem (in this case a simple game), a simulated user capable of answering natural language questions that can observe and perform actions on the problem, and a Deep Q-Network (DQN)-based chatbot architecture. The chatbot is trained with the goal of solving the problem through dialogue with the simulated user using reinforcement learning. The contributions of this paper are as follows: a proposed architecture to apply a conversational DQN-based agent to evolving problems, an exploration of training methods such as curriculum learning on model performance and the effect of modified reward functions in the case of increasing environment complexity.
&lt;/p&gt;</description></item><item><title>Cheetah&#26159;&#19968;&#31181;&#39640;&#36895;&#21487;&#24494;&#20998;&#27169;&#25311;&#24037;&#20855;&#65292;&#21487;&#20197;&#20943;&#23569;&#35745;&#31639;&#26102;&#38388;&#24182;&#23454;&#29616;&#24555;&#36895;&#25910;&#38598;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#12290;&#23427;&#33021;&#22815;&#20419;&#36827;&#21152;&#36895;&#22120;&#35843;&#20248;&#21644;&#31995;&#32479;&#35782;&#21035;&#65292;&#24182;&#19982;&#26426;&#22120;&#23398;&#20064;&#24037;&#20855;&#26080;&#32541;&#38598;&#25104;&#12290;</title><link>http://arxiv.org/abs/2401.05815</link><description>&lt;p&gt;
Cheetah: &#36890;&#36807;&#39640;&#36895;&#21487;&#24494;&#20998;&#27169;&#25311;&#22635;&#34917;&#26426;&#22120;&#23398;&#20064;&#21644;&#31890;&#23376;&#21152;&#36895;&#22120;&#29289;&#29702;&#20043;&#38388;&#30340;&#24046;&#36317;
&lt;/p&gt;
&lt;p&gt;
Cheetah: Bridging the Gap Between Machine Learning and Particle Accelerator Physics with High-Speed, Differentiable Simulations. (arXiv:2401.05815v1 [physics.acc-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05815
&lt;/p&gt;
&lt;p&gt;
Cheetah&#26159;&#19968;&#31181;&#39640;&#36895;&#21487;&#24494;&#20998;&#27169;&#25311;&#24037;&#20855;&#65292;&#21487;&#20197;&#20943;&#23569;&#35745;&#31639;&#26102;&#38388;&#24182;&#23454;&#29616;&#24555;&#36895;&#25910;&#38598;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#12290;&#23427;&#33021;&#22815;&#20419;&#36827;&#21152;&#36895;&#22120;&#35843;&#20248;&#21644;&#31995;&#32479;&#35782;&#21035;&#65292;&#24182;&#19982;&#26426;&#22120;&#23398;&#20064;&#24037;&#20855;&#26080;&#32541;&#38598;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#24050;&#32463;&#25104;&#20026;&#21152;&#36895;&#22120;&#29289;&#29702;&#23398;&#20013;&#29616;&#20195;&#25361;&#25112;&#30340;&#24378;&#22823;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#26377;&#38480;&#30340;&#26463;&#27969;&#26102;&#38388;&#21487;&#29992;&#24615;&#65292;&#27169;&#25311;&#30340;&#35745;&#31639;&#25104;&#26412;&#20197;&#21450;&#20248;&#21270;&#38382;&#39064;&#30340;&#39640;&#32500;&#24230;&#32473;&#29983;&#25104;&#25152;&#38656;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#25968;&#25454;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Cheetah&#65292;&#19968;&#31181;&#22522;&#20110;PyTorch&#30340;&#39640;&#36895;&#21487;&#24494;&#32447;&#24615;&#26463;&#27969;&#21160;&#21147;&#23398;&#20195;&#30721;&#12290;Cheetah&#36890;&#36807;&#23558;&#35745;&#31639;&#26102;&#38388;&#20943;&#23569;&#25968;&#20010;&#25968;&#37327;&#32423;&#65292;&#23454;&#29616;&#20102;&#24555;&#36895;&#25910;&#38598;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#24182;&#20419;&#36827;&#20102;&#38754;&#21521;&#21152;&#36895;&#22120;&#35843;&#20248;&#21644;&#31995;&#32479;&#35782;&#21035;&#30340;&#39640;&#25928;&#26799;&#24230;&#20248;&#21270;&#12290;&#36825;&#20351;Cheetah&#25104;&#20026;&#19968;&#20010;&#29992;&#25143;&#21451;&#22909;&#12289;&#26131;&#20110;&#25193;&#23637;&#30340;&#24037;&#20855;&#65292;&#19982;&#24191;&#27867;&#37319;&#29992;&#30340;&#26426;&#22120;&#23398;&#20064;&#24037;&#20855;&#26080;&#32541;&#38598;&#25104;&#12290;&#25105;&#20204;&#36890;&#36807;&#20116;&#20010;&#31034;&#20363;&#23637;&#31034;&#20102;Cheetah&#30340;&#23454;&#29992;&#24615;&#65292;&#21253;&#25324;&#24378;&#21270;&#23398;&#20064;&#35757;&#32451;&#12289;&#22522;&#20110;&#26799;&#24230;&#30340;&#26463;&#32447;&#35843;&#20248;&#12289;&#22522;&#20110;&#26799;&#24230;&#30340;&#31995;&#32479;&#35782;&#21035;&#12289;&#29289;&#29702;&#23398;-i
&lt;/p&gt;
&lt;p&gt;
Machine learning has emerged as a powerful solution to the modern challenges in accelerator physics. However, the limited availability of beam time, the computational cost of simulations, and the high-dimensionality of optimisation problems pose significant challenges in generating the required data for training state-of-the-art machine learning models. In this work, we introduce Cheetah, a PyTorch-based high-speed differentiable linear-beam dynamics code. Cheetah enables the fast collection of large data sets by reducing computation times by multiple orders of magnitude and facilitates efficient gradient-based optimisation for accelerator tuning and system identification. This positions Cheetah as a user-friendly, readily extensible tool that integrates seamlessly with widely adopted machine learning tools. We showcase the utility of Cheetah through five examples, including reinforcement learning training, gradient-based beamline tuning, gradient-based system identification, physics-i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#23545;&#27604;&#23545;&#40784;&#25351;&#20196;&#65288;AlignInstruct&#65289;&#65292;&#36890;&#36807;&#20351;&#29992;&#32479;&#35745;&#35789;&#23545;&#40784;&#26500;&#24314;&#30340;&#36328;&#35821;&#35328;&#37492;&#21035;&#22120;&#23454;&#29616;&#20102;&#36328;&#35821;&#35328;&#30417;&#30563;&#65292;&#35299;&#20915;&#20102;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#20004;&#20010;&#25361;&#25112;&#65306;&#23558;&#25903;&#25345;&#30340;&#35821;&#35328;&#25193;&#23637;&#21040;&#26410;&#30693;&#35821;&#35328;&#21644;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#25968;&#25454;&#32570;&#20047;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;LLMs&#36890;&#36807;MTInstruct&#21487;&#20197;&#26377;&#25928;&#22320;&#32763;&#35793;&#26410;&#30693;&#35821;&#35328;&#65292;&#24182;&#19988;&#20351;&#29992;AlignInstruct&#22312;&#28041;&#21450;&#33521;&#35821;&#30340;48&#20010;&#32763;&#35793;&#26041;&#21521;&#19978;&#33021;&#22815;&#25345;&#32493;&#25913;&#21892;&#32763;&#35793;&#36136;&#37327;&#12290;&#22522;&#20110;&#37492;&#21035;&#22120;&#30340;&#25351;&#20196;&#20248;&#20110;&#29983;&#25104;&#22411;&#25351;&#20196;&#12290;</title><link>http://arxiv.org/abs/2401.05811</link><description>&lt;p&gt;
&#20351;&#29992;&#23545;&#27604;&#23545;&#40784;&#25351;&#20196;&#35843;&#25972;LLMs&#20197;&#35299;&#20915;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#26410;&#30693;&#12289;&#20302;&#36164;&#28304;&#35821;&#35328;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Tuning LLMs with Contrastive Alignment Instructions for Machine Translation in Unseen, Low-resource Languages. (arXiv:2401.05811v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05811
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#23545;&#27604;&#23545;&#40784;&#25351;&#20196;&#65288;AlignInstruct&#65289;&#65292;&#36890;&#36807;&#20351;&#29992;&#32479;&#35745;&#35789;&#23545;&#40784;&#26500;&#24314;&#30340;&#36328;&#35821;&#35328;&#37492;&#21035;&#22120;&#23454;&#29616;&#20102;&#36328;&#35821;&#35328;&#30417;&#30563;&#65292;&#35299;&#20915;&#20102;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#20004;&#20010;&#25361;&#25112;&#65306;&#23558;&#25903;&#25345;&#30340;&#35821;&#35328;&#25193;&#23637;&#21040;&#26410;&#30693;&#35821;&#35328;&#21644;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#25968;&#25454;&#32570;&#20047;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;LLMs&#36890;&#36807;MTInstruct&#21487;&#20197;&#26377;&#25928;&#22320;&#32763;&#35793;&#26410;&#30693;&#35821;&#35328;&#65292;&#24182;&#19988;&#20351;&#29992;AlignInstruct&#22312;&#28041;&#21450;&#33521;&#35821;&#30340;48&#20010;&#32763;&#35793;&#26041;&#21521;&#19978;&#33021;&#22815;&#25345;&#32493;&#25913;&#21892;&#32763;&#35793;&#36136;&#37327;&#12290;&#22522;&#20110;&#37492;&#21035;&#22120;&#30340;&#25351;&#20196;&#20248;&#20110;&#29983;&#25104;&#22411;&#25351;&#20196;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#23545;&#27604;&#23545;&#40784;&#25351;&#20196;&#65288;AlignInstruct&#65289;&#26469;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19978;&#26426;&#22120;&#32763;&#35793;&#65288;MT&#65289;&#20013;&#30340;&#20004;&#20010;&#25361;&#25112;&#12290;&#19968;&#20010;&#26159;&#23558;&#25903;&#25345;&#30340;&#35821;&#35328;&#25193;&#23637;&#21040;&#20043;&#21069;&#26410;&#35265;&#36807;&#30340;&#35821;&#35328;&#12290;&#31532;&#20108;&#20010;&#19982;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#32570;&#20047;&#25968;&#25454;&#26377;&#20851;&#12290;&#36890;&#36807;MT&#25351;&#20196;&#65288;MTInstruct&#65289;&#23545;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#26159;&#24212;&#23545;&#31532;&#19968;&#20010;&#25361;&#25112;&#30340;&#19968;&#31181;&#30452;&#25509;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;MTInstruct&#21463;&#21040;&#31532;&#20108;&#20010;&#25361;&#25112;&#20013;&#22266;&#26377;&#30340;&#24369;&#35821;&#35328;&#36328;&#24230;&#20449;&#21495;&#30340;&#38480;&#21046;&#12290;AlignInstruct&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#32479;&#35745;&#35789;&#23545;&#40784;&#26500;&#24314;&#30340;&#36328;&#35821;&#35328;&#37492;&#21035;&#22120;&#26469;&#24378;&#35843;&#36328;&#35821;&#35328;&#30417;&#30563;&#12290;&#25105;&#20204;&#22522;&#20110;&#22312;&#22810;&#36798;24&#31181;&#26410;&#30693;&#35821;&#35328;&#19978;&#23545;BLOOMZ&#27169;&#22411;&#65288;1b1&#12289;3b&#21644;7b1&#65289;&#36827;&#34892;&#24494;&#35843;&#30340;&#32467;&#26524;&#34920;&#26126;&#65306;&#65288;1&#65289;LLMs&#21487;&#20197;&#20351;&#29992;MTInstruct&#26377;&#25928;&#22320;&#32763;&#35793;&#26410;&#30693;&#35821;&#35328;&#65307;&#65288;2&#65289;AlignInstruct&#22312;&#28041;&#21450;&#33521;&#35821;&#30340;48&#20010;&#32763;&#35793;&#26041;&#21521;&#19978;&#25552;&#39640;&#20102;&#32763;&#35793;&#36136;&#37327;&#30340;&#19968;&#33268;&#24615;&#65307;&#65288;3&#65289;&#22522;&#20110;&#37492;&#21035;&#22120;&#30340;&#25351;&#20196;&#20248;&#20110;&#29983;&#25104;&#22411;&#25351;&#20196;&#12290;
&lt;/p&gt;
&lt;p&gt;
This article introduces contrastive alignment instructions (AlignInstruct) to address two challenges in machine translation (MT) on large language models (LLMs). One is the expansion of supported languages to previously unseen ones. The second relates to the lack of data in low-resource languages. Model fine-tuning through MT instructions (MTInstruct) is a straightforward approach to the first challenge. However, MTInstruct is limited by weak cross-lingual signals inherent in the second challenge. AlignInstruct emphasizes cross-lingual supervision via a cross-lingual discriminator built using statistical word alignments. Our results based on fine-tuning the BLOOMZ models (1b1, 3b, and 7b1) in up to 24 unseen languages showed that: (1) LLMs can effectively translate unseen languages using MTInstruct; (2) AlignInstruct led to consistent improvements in translation quality across 48 translation directions involving English; (3) Discriminator-based instructions outperformed their generativ
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;GST-Pro&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#22270;&#24418;&#26102;&#31354;&#36807;&#31243;&#21644;&#24322;&#24120;&#35780;&#20998;&#22120;&#26469;&#35299;&#20915;&#22312;&#37319;&#26679;&#19981;&#35268;&#21017;&#30340;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#20013;&#26816;&#27979;&#24322;&#24120;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2401.05800</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#24418;&#26102;&#31354;&#36807;&#31243;&#30340;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#21450;&#20854;&#22312;&#32570;&#22833;&#20540;&#24773;&#20917;&#19979;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Graph Spatiotemporal Process for Multivariate Time Series Anomaly Detection with Missing Values. (arXiv:2401.05800v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05800
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;GST-Pro&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#22270;&#24418;&#26102;&#31354;&#36807;&#31243;&#21644;&#24322;&#24120;&#35780;&#20998;&#22120;&#26469;&#35299;&#20915;&#22312;&#37319;&#26679;&#19981;&#35268;&#21017;&#30340;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#20013;&#26816;&#27979;&#24322;&#24120;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#24322;&#24120;&#36827;&#34892;&#26816;&#27979;&#23545;&#20110;&#21508;&#31181;&#23454;&#38469;&#24212;&#29992;&#38750;&#24120;&#37325;&#35201;&#65292;&#21253;&#25324;&#26234;&#33021;&#30005;&#32593;&#12289;&#20132;&#36890;&#27969;&#39044;&#27979;&#21644;&#24037;&#19994;&#36807;&#31243;&#25511;&#21046;&#31561;&#12290;&#28982;&#32780;&#65292;&#30495;&#23454;&#19990;&#30028;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#36890;&#24120;&#19981;&#26159;&#33391;&#22909;&#32467;&#26500;&#21270;&#30340;&#65292;&#36825;&#32473;&#29616;&#26377;&#26041;&#27861;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#65306;&#65288;1&#65289;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#32570;&#22833;&#20540;&#23384;&#22312;&#20110;&#21464;&#37327;&#21644;&#26102;&#38388;&#32500;&#24230;&#65292;&#38459;&#30861;&#20102;&#23545;&#20132;&#32455;&#30340;&#31354;&#38388;&#21644;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#36827;&#34892;&#26377;&#25928;&#24314;&#27169;&#65292;&#23548;&#33268;&#37325;&#35201;&#30340;&#27169;&#24335;&#22312;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#20013;&#34987;&#24573;&#35270;&#65307;&#65288;2&#65289;&#22312;&#37319;&#26679;&#19981;&#35268;&#21017;&#30340;&#35266;&#27979;&#19979;&#36827;&#34892;&#24322;&#24120;&#35780;&#20998;&#30340;&#30740;&#31350;&#36739;&#23569;&#65292;&#36825;&#20351;&#24471;&#22312;&#22810;&#21464;&#37327;&#24207;&#21015;&#20013;&#20351;&#29992;&#29616;&#26377;&#26816;&#27979;&#22120;&#26102;&#24456;&#38590;&#22788;&#29702;&#27809;&#26377;&#23436;&#20840;&#35266;&#27979;&#20540;&#30340;&#24773;&#20917;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;GST-Pro&#30340;&#26032;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21033;&#29992;&#22270;&#24418;&#26102;&#31354;&#36807;&#31243;&#21644;&#24322;&#24120;&#35780;&#20998;&#22120;&#26469;&#35299;&#20915;&#22312;&#37319;&#26679;&#19981;&#35268;&#21017;&#30340;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#20013;&#26816;&#27979;&#24322;&#24120;&#26102;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
The detection of anomalies in multivariate time series data is crucial for various practical applications, including smart power grids, traffic flow forecasting, and industrial process control. However, real-world time series data is usually not well-structured, posting significant challenges to existing approaches: (1) The existence of missing values in multivariate time series data along variable and time dimensions hinders the effective modeling of interwoven spatial and temporal dependencies, resulting in important patterns being overlooked during model training; (2) Anomaly scoring with irregularly-sampled observations is less explored, making it difficult to use existing detectors for multivariate series without fully-observed values. In this work, we introduce a novel framework called GST-Pro, which utilizes a graph spatiotemporal process and anomaly scorer to tackle the aforementioned challenges in detecting anomalies on irregularly-sampled multivariate time series. Our approac
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#35774;&#35745;&#26694;&#26550;&#65292;&#20351;&#29992;&#21160;&#24577;&#30340;&#24322;&#26500;LLM&#20195;&#29702;&#65292;&#26469;&#25913;&#36827;&#37329;&#34701;&#24773;&#32490;&#20998;&#26512;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.05799</link><description>&lt;p&gt;
&#35774;&#35745;&#29992;&#20110;&#37329;&#34701;&#24773;&#32490;&#20998;&#26512;&#30340;&#24322;&#26500;LLM&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
Designing Heterogeneous LLM Agents for Financial Sentiment Analysis. (arXiv:2401.05799v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05799
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#35774;&#35745;&#26694;&#26550;&#65292;&#20351;&#29992;&#21160;&#24577;&#30340;&#24322;&#26500;LLM&#20195;&#29702;&#65292;&#26469;&#25913;&#36827;&#37329;&#34701;&#24773;&#32490;&#20998;&#26512;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24443;&#24213;&#25913;&#21464;&#20102;&#35774;&#35745;&#26234;&#33021;&#31995;&#32479;&#30340;&#21487;&#33021;&#26041;&#24335;&#65292;&#23558;&#28966;&#28857;&#20174;&#22823;&#35268;&#27169;&#25968;&#25454;&#33719;&#21462;&#21644;&#26032;&#30340;&#24314;&#27169;&#35757;&#32451;&#36716;&#31227;&#21040;&#20102;&#19982;&#20154;&#31867;&#23545;&#40784;&#20197;&#21450;&#25112;&#30053;&#24615;&#22320;&#21457;&#25381;&#29616;&#26377;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#20840;&#37096;&#28508;&#21147;&#19978;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#37329;&#34701;&#24773;&#32490;&#20998;&#26512;&#20219;&#21153;&#30340;&#27495;&#35270;&#24615;&#29305;&#24449;&#20197;&#21450;&#32570;&#20047;&#22914;&#20309;&#22312;&#36825;&#31181;&#32972;&#26223;&#19979;&#21033;&#29992;&#29983;&#25104;&#27169;&#22411;&#30340;&#35268;&#23450;&#24615;&#30693;&#35782;&#65292;&#36825;&#31181;&#33539;&#24335;&#36716;&#21464;&#23578;&#26410;&#23436;&#20840;&#23454;&#29616;&#12290;&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#26032;&#33539;&#24335;&#30340;&#26377;&#25928;&#24615;&#65292;&#21363;&#22312;&#37329;&#34701;&#24773;&#32490;&#20998;&#26512;&#20013;&#20351;&#29992;&#26080;&#38656;&#24494;&#35843;&#30340;LLM&#12290;&#22522;&#20110;&#26126;&#26031;&#22522;&#30340;&#24515;&#28789;&#21644;&#24773;&#32490;&#29702;&#35770;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#29992;&#24322;&#26500;LLM&#20195;&#29702;&#30340;&#35774;&#35745;&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#20351;&#29992;&#20808;&#21069;&#39046;&#22495;&#30693;&#35782;&#23454;&#20363;&#21270;&#19987;&#38376;&#30340;&#20195;&#29702;&#65292;&#24182;&#23545;&#38598;&#21512;&#30340;&#20195;&#29702;&#35752;&#35770;&#36827;&#34892;&#25512;&#29702;&#12290;&#22312;&#37329;&#34701;&#24773;&#32490;&#20998;&#26512;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#20840;&#38754;&#35780;&#20272;&#34920;&#26126;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#33719;&#24471;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#35752;&#35770;&#39057;&#32321;&#30340;&#24773;&#20917;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have drastically changed the possible ways to design intelligent systems, shifting the focuses from massive data acquisition and new modeling training to human alignment and strategical elicitation of the full potential of existing pre-trained models. This paradigm shift, however, is not fully realized in financial sentiment analysis (FSA), due to the discriminative nature of this task and a lack of prescriptive knowledge of how to leverage generative models in such a context. This study investigates the effectiveness of the new paradigm, i.e., using LLMs without fine-tuning for FSA. Rooted in Minsky's theory of mind and emotions, a design framework with heterogeneous LLM agents is proposed. The framework instantiates specialized agents using prior domain knowledge of the types of FSA errors and reasons on the aggregated agent discussions. Comprehensive evaluation on FSA datasets show that the framework yields better accuracies, especially when the discussi
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#31995;&#32479;&#30340;&#39118;&#38505;&#20998;&#31867;&#12289;&#32531;&#35299;&#21644;&#35780;&#20272;&#22522;&#20934;&#65292;&#35843;&#26597;&#24182;&#20998;&#26512;&#20102;&#19982;&#27599;&#20010;&#27169;&#22359;&#30456;&#20851;&#30340;&#28508;&#22312;&#39118;&#38505;&#12290;</title><link>http://arxiv.org/abs/2401.05778</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#31995;&#32479;&#30340;&#39118;&#38505;&#20998;&#31867;&#12289;&#32531;&#35299;&#21644;&#35780;&#20272;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
Risk Taxonomy, Mitigation, and Assessment Benchmarks of Large Language Model Systems. (arXiv:2401.05778v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05778
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#31995;&#32479;&#30340;&#39118;&#38505;&#20998;&#31867;&#12289;&#32531;&#35299;&#21644;&#35780;&#20272;&#22522;&#20934;&#65292;&#35843;&#26597;&#24182;&#20998;&#26512;&#20102;&#19982;&#27599;&#20010;&#27169;&#22359;&#30456;&#20851;&#30340;&#28508;&#22312;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#35299;&#20915;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#26041;&#38754;&#20855;&#26377;&#24378;&#22823;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;LLM&#31995;&#32479;&#30340;&#23433;&#20840;&#21644;&#23433;&#20840;&#38382;&#39064;&#24050;&#32463;&#25104;&#20026;&#24191;&#27867;&#24212;&#29992;&#30340;&#20027;&#35201;&#38556;&#30861;&#12290;&#35768;&#22810;&#30740;&#31350;&#24050;&#32463;&#24191;&#27867;&#35843;&#26597;&#20102;LLM&#31995;&#32479;&#30340;&#39118;&#38505;&#65292;&#24182;&#24320;&#21457;&#20102;&#30456;&#24212;&#30340;&#32531;&#35299;&#31574;&#30053;&#12290;OpenAI&#12289;Google&#12289;Meta&#21644;Anthropic&#31561;&#39046;&#20808;&#20225;&#19994;&#20063;&#22312;&#36127;&#36131;&#20219;&#30340;LLM&#26041;&#38754;&#20570;&#20986;&#20102;&#24456;&#22810;&#21162;&#21147;&#12290;&#22240;&#27492;&#65292;&#26377;&#19968;&#20010;&#36234;&#26469;&#36234;&#22823;&#30340;&#38656;&#27714;&#26469;&#32452;&#32455;&#29616;&#26377;&#30340;&#30740;&#31350;&#65292;&#24182;&#20026;&#31038;&#21306;&#24314;&#31435;&#20840;&#38754;&#30340;&#20998;&#31867;&#20307;&#31995;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#28145;&#20837;&#30740;&#31350;&#20102;LLM&#31995;&#32479;&#30340;&#22235;&#20010;&#22522;&#26412;&#27169;&#22359;&#65292;&#21253;&#25324;&#29992;&#20110;&#25509;&#25910;&#25552;&#31034;&#30340;&#36755;&#20837;&#27169;&#22359;&#12289;&#22312;&#22823;&#37327;&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#12289;&#29992;&#20110;&#24320;&#21457;&#21644;&#37096;&#32626;&#30340;&#24037;&#20855;&#38142;&#27169;&#22359;&#20197;&#21450;&#29992;&#20110;&#23548;&#20986;LLM&#29983;&#25104;&#20869;&#23481;&#30340;&#36755;&#20986;&#27169;&#22359;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#20998;&#31867;&#20307;&#31995;&#65292;&#31995;&#32479;&#20998;&#26512;&#20102;&#19982;LLM&#31995;&#32479;&#30340;&#27599;&#20010;&#27169;&#22359;&#30456;&#20851;&#30340;&#28508;&#22312;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have strong capabilities in solving diverse natural language processing tasks. However, the safety and security issues of LLM systems have become the major obstacle to their widespread application. Many studies have extensively investigated risks in LLM systems and developed the corresponding mitigation strategies. Leading-edge enterprises such as OpenAI, Google, Meta, and Anthropic have also made lots of efforts on responsible LLMs. Therefore, there is a growing need to organize the existing studies and establish comprehensive taxonomies for the community. In this paper, we delve into four essential modules of an LLM system, including an input module for receiving prompts, a language model trained on extensive corpora, a toolchain module for development and deployment, and an output module for exporting LLM-generated content. Based on this, we propose a comprehensive taxonomy, which systematically analyzes potential risks associated with each module of an 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#30693;&#35782;&#36716;&#21270;&#65288;KT&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#8220;&#32763;&#35793;&#8221;&#27169;&#22411;&#26469;&#25509;&#25910;&#36739;&#22823;&#27169;&#22411;&#30340;&#21442;&#25968;&#24182;&#29983;&#25104;&#21387;&#32553;&#21442;&#25968;&#65292;&#20174;&#32780;&#23454;&#29616;&#27169;&#22411;&#21387;&#32553;&#65292;&#32780;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#25110;&#26045;&#21152;&#26550;&#26500;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2401.05772</link><description>&lt;p&gt;
&#30693;&#35782;&#36716;&#21270;&#65306;&#19968;&#31181;&#29992;&#20110;&#27169;&#22411;&#21387;&#32553;&#30340;&#26032;&#36884;&#24452;
&lt;/p&gt;
&lt;p&gt;
Knowledge Translation: A New Pathway for Model Compression. (arXiv:2401.05772v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05772
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#30693;&#35782;&#36716;&#21270;&#65288;KT&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#8220;&#32763;&#35793;&#8221;&#27169;&#22411;&#26469;&#25509;&#25910;&#36739;&#22823;&#27169;&#22411;&#30340;&#21442;&#25968;&#24182;&#29983;&#25104;&#21387;&#32553;&#21442;&#25968;&#65292;&#20174;&#32780;&#23454;&#29616;&#27169;&#22411;&#21387;&#32553;&#65292;&#32780;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#25110;&#26045;&#21152;&#26550;&#26500;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;&#36817;&#24180;&#26469;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#20294;&#35757;&#32451;&#12289;&#25512;&#29702;&#21644;&#27169;&#22411;&#23384;&#20648;&#24320;&#38144;&#21364;&#22312;&#19981;&#26029;&#22686;&#21152;&#12290;&#23613;&#31649;&#29616;&#26377;&#30340;&#27169;&#22411;&#21387;&#32553;&#26041;&#27861;&#33268;&#21147;&#20110;&#22312;&#20445;&#25345;&#39640;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#20943;&#23569;&#27169;&#22411;&#21442;&#25968;&#30340;&#25968;&#37327;&#65292;&#20294;&#23427;&#20204;&#19981;&#21487;&#36991;&#20813;&#22320;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#21387;&#32553;&#27169;&#22411;&#25110;&#26045;&#21152;&#26550;&#26500;&#38480;&#21046;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#31216;&#20026;&#30693;&#35782;&#36716;&#21270;&#65288;KT&#65289;&#65292;&#20854;&#20013;&#35757;&#32451;&#19968;&#20010;&#8220;&#32763;&#35793;&#8221;&#27169;&#22411;&#26469;&#25509;&#25910;&#36739;&#22823;&#27169;&#22411;&#30340;&#21442;&#25968;&#24182;&#29983;&#25104;&#21387;&#32553;&#21442;&#25968;&#12290;&#30693;&#35782;&#36716;&#21270;&#30340;&#27010;&#24565;&#20511;&#37492;&#33258;&#35821;&#35328;&#32763;&#35793;&#65292;&#23427;&#26377;&#25928;&#22320;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#23558;&#19981;&#21516;&#30340;&#35821;&#35328;&#36716;&#25442;&#20026;&#30456;&#21516;&#30340;&#24847;&#24605;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25506;&#32034;&#20102;&#31070;&#32463;&#32593;&#32476;&#23558;&#19981;&#21516;&#22823;&#23567;&#30340;&#27169;&#22411;&#36716;&#25442;&#20026;&#20445;&#25345;&#20854;&#21151;&#33021;&#24615;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;KT&#26694;&#26550;&#65292;&#20171;&#32461;&#20102;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning has witnessed significant advancements in recent years at the cost of increasing training, inference, and model storage overhead. While existing model compression methods strive to reduce the number of model parameters while maintaining high accuracy, they inevitably necessitate the re-training of the compressed model or impose architectural constraints. To overcome these limitations, this paper presents a novel framework, termed \textbf{K}nowledge \textbf{T}ranslation (KT), wherein a ``translation'' model is trained to receive the parameters of a larger model and generate compressed parameters. The concept of KT draws inspiration from language translation, which effectively employs neural networks to convert different languages, maintaining identical meaning. Accordingly, we explore the potential of neural networks to convert models of disparate sizes, while preserving their functionality. We propose a comprehensive framework for KT, introduce data augmentation strategie
&lt;/p&gt;</description></item><item><title>&#20114;&#32852;&#32593;&#19978;&#30340;&#22823;&#37327;&#20869;&#23481;&#26159;&#36890;&#36807;&#26426;&#22120;&#32763;&#35793;&#22810;&#21521;&#32763;&#35793;&#30340;&#65292;&#20854;&#20302;&#36136;&#37327;&#21487;&#33021;&#20250;&#23545;&#20351;&#29992;&#22810;&#35821;&#35328;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#20135;&#29983;&#20005;&#37325;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2401.05749</link><description>&lt;p&gt;
&#20114;&#32852;&#32593;&#19978;&#30340;&#22823;&#37327;&#20869;&#23481;&#37117;&#26159;&#26426;&#22120;&#32763;&#35793;&#30340;&#65306;&#26469;&#33258;&#22810;&#21521;&#24182;&#34892;&#24615;&#30340;&#27934;&#23519;
&lt;/p&gt;
&lt;p&gt;
A Shocking Amount of the Web is Machine Translated: Insights from Multi-Way Parallelism. (arXiv:2401.05749v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05749
&lt;/p&gt;
&lt;p&gt;
&#20114;&#32852;&#32593;&#19978;&#30340;&#22823;&#37327;&#20869;&#23481;&#26159;&#36890;&#36807;&#26426;&#22120;&#32763;&#35793;&#22810;&#21521;&#32763;&#35793;&#30340;&#65292;&#20854;&#20302;&#36136;&#37327;&#21487;&#33021;&#20250;&#23545;&#20351;&#29992;&#22810;&#35821;&#35328;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#20135;&#29983;&#20005;&#37325;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23637;&#31034;&#20102;&#20114;&#32852;&#32593;&#19978;&#30340;&#20869;&#23481;&#32463;&#24120;&#34987;&#32763;&#35793;&#25104;&#22810;&#31181;&#35821;&#35328;&#65292;&#24182;&#19988;&#36825;&#20123;&#22810;&#21521;&#32763;&#35793;&#30340;&#20302;&#36136;&#37327;&#34920;&#26126;&#23427;&#20204;&#24456;&#21487;&#33021;&#26159;&#20351;&#29992;&#26426;&#22120;&#32763;&#35793;&#65288;MT&#65289;&#21019;&#24314;&#30340;&#12290;&#22810;&#21521;&#24182;&#34892;&#30340;&#26426;&#22120;&#29983;&#25104;&#20869;&#23481;&#19981;&#20165;&#22312;&#36164;&#28304;&#36739;&#23569;&#30340;&#35821;&#35328;&#20013;&#21344;&#20027;&#23548;&#22320;&#20301;&#65292;&#32780;&#19988;&#26500;&#25104;&#35813;&#35821;&#35328;&#20013;&#24635;&#20307;&#32593;&#39029;&#20869;&#23481;&#30340;&#24456;&#22823;&#19968;&#37096;&#20998;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#35777;&#25454;&#34920;&#26126;&#65292;&#34987;&#32763;&#35793;&#25104;&#22810;&#31181;&#35821;&#35328;&#30340;&#20869;&#23481;&#23384;&#22312;&#36873;&#25321;&#24615;&#20559;&#24046;&#65292;&#19982;&#23558;&#20302;&#36136;&#37327;&#33521;&#25991;&#20869;&#23481;&#36890;&#36807;&#26426;&#22120;&#32763;&#35793;&#22823;&#35268;&#27169;&#32763;&#35793;&#25104;&#35768;&#22810;&#36164;&#28304;&#36739;&#23569;&#30340;&#35821;&#35328;&#19968;&#33268;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#23545;&#20110;&#22312;&#32593;&#32476;&#19978;&#20174;&#21333;&#35821;&#21644;&#21452;&#35821;&#25968;&#25454;&#35757;&#32451;&#22810;&#35821;&#35328;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#31561;&#27169;&#22411;&#25552;&#20986;&#20102;&#20005;&#37325;&#30340;&#25285;&#24551;&#12290;
&lt;/p&gt;
&lt;p&gt;
We show that content on the web is often translated into many languages, and the low quality of these multi-way translations indicates they were likely created using Machine Translation (MT). Multi-way parallel, machine generated content not only dominates the translations in lower resource languages; it also constitutes a large fraction of the total web content in those languages. We also find evidence of a selection bias in the type of content which is translated into many languages, consistent with low quality English content being translated en masse into many lower resource languages, via MT. Our work raises serious concerns about training models such as multilingual large language models on both monolingual and bilingual data scraped from the web.
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#30740;&#31350;&#20102;&#23384;&#22312;&#35268;&#21017;&#19979;&#22522;&#20110;&#20803;&#32452;&#21024;&#38500;&#35821;&#20041;&#30340;&#19968;&#33268;&#24615;&#26597;&#35810;&#22238;&#31572;&#65292;&#24182;&#30830;&#23450;&#20102;&#19968;&#33324;&#31867;&#21035;&#21644;&#33509;&#24178;&#23376;&#31867;&#21035;&#30340;&#25968;&#25454;&#22797;&#26434;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#36825;&#20123;&#38382;&#39064;&#26159;&#21487;&#22788;&#29702;&#30340;&#65292;&#29978;&#33267;&#21487;&#20197;&#36827;&#34892;&#19968;&#38454;&#37325;&#20889;&#12290;&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#26032;&#30340;&#26597;&#35810;&#37325;&#20889;&#25216;&#26415;&#65292;&#21487;&#29992;&#20110;&#23454;&#29616;&#23481;&#24525;&#19981;&#19968;&#33268;&#24615;&#30340;&#26597;&#35810;&#22238;&#31572;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2401.05743</link><description>&lt;p&gt;
&#22312;&#20803;&#32452;&#21024;&#38500;&#35821;&#20041;&#19979;&#30340;&#23384;&#22312;&#35268;&#21017;&#19968;&#33268;&#24615;&#26597;&#35810;&#22238;&#31572;
&lt;/p&gt;
&lt;p&gt;
Consistent Query Answering for Existential Rules under Tuple-Deletion Semantics. (arXiv:2401.05743v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05743
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#23384;&#22312;&#35268;&#21017;&#19979;&#22522;&#20110;&#20803;&#32452;&#21024;&#38500;&#35821;&#20041;&#30340;&#19968;&#33268;&#24615;&#26597;&#35810;&#22238;&#31572;&#65292;&#24182;&#30830;&#23450;&#20102;&#19968;&#33324;&#31867;&#21035;&#21644;&#33509;&#24178;&#23376;&#31867;&#21035;&#30340;&#25968;&#25454;&#22797;&#26434;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#36825;&#20123;&#38382;&#39064;&#26159;&#21487;&#22788;&#29702;&#30340;&#65292;&#29978;&#33267;&#21487;&#20197;&#36827;&#34892;&#19968;&#38454;&#37325;&#20889;&#12290;&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#26032;&#30340;&#26597;&#35810;&#37325;&#20889;&#25216;&#26415;&#65292;&#21487;&#29992;&#20110;&#23454;&#29616;&#23481;&#24525;&#19981;&#19968;&#33268;&#24615;&#30340;&#26597;&#35810;&#22238;&#31572;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22522;&#20110;&#23384;&#22312;&#35268;&#21017;&#30340;&#30693;&#35782;&#24211;&#30340;&#19968;&#33268;&#24615;&#26597;&#35810;&#22238;&#31572;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#22312;&#20803;&#32452;&#21024;&#38500;&#35821;&#20041;&#19979;&#19968;&#33324;&#31867;&#21035;&#30340;&#22810;&#20041;&#23384;&#22312;&#35268;&#21017;&#20197;&#21450;&#20854;&#33509;&#24178;&#23376;&#31867;&#21035;&#65288;&#26080;&#29615;&#12289;&#32447;&#24615;&#12289;&#23436;&#20840;&#12289;&#20445;&#25252;&#21644;&#31896;&#28382;&#65289;&#30340;&#19968;&#33268;&#24615;&#26597;&#35810;&#22238;&#31572;&#21644;&#20462;&#22797;&#26816;&#26597;&#30340;&#25968;&#25454;&#22797;&#26434;&#24615;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#35782;&#21035;&#20986;&#20102;&#20960;&#31181;&#24773;&#20917;&#65292;&#22312;&#36825;&#20123;&#24773;&#20917;&#19979;&#36825;&#20123;&#38382;&#39064;&#26159;&#21487;&#22788;&#29702;&#30340;&#65292;&#29978;&#33267;&#21487;&#20197;&#36827;&#34892;&#19968;&#38454;&#37325;&#20889;&#65292;&#24182;&#21576;&#29616;&#20102;&#26032;&#30340;&#26597;&#35810;&#37325;&#20889;&#25216;&#26415;&#65292;&#21487;&#20197;&#25104;&#20026;&#23454;&#29992;&#30340;&#23481;&#24525;&#19981;&#19968;&#33268;&#24615;&#26597;&#35810;&#22238;&#31572;&#31995;&#32479;&#30340;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study consistent query answering over knowledge bases expressed by existential rules. Specifically, we establish the data complexity of consistent query answering and repair checking under tuple-deletion semantics for a general class of disjunctive existential rules and for several subclasses thereof (acyclic, linear, full, guarded, and sticky). In particular, we identify several cases in which the above problems are tractable or even first-order rewritable, and present new query rewriting techniques that can be the basis for practical inconsistency-tolerant query answering systems.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ECPP&#65288;&#39640;&#25928;&#32452;&#21512;&#27491;&#26679;&#26412;&#37197;&#23545;&#65289;&#30340;&#22810;&#35270;&#22270;&#31574;&#30053;&#65292;&#36890;&#36807;&#22686;&#21152;&#35270;&#22270;&#25968;&#37327;&#12289;&#37319;&#29992;&#23567;&#23610;&#23544;&#35270;&#22270;&#21644;&#20462;&#25913;&#36127;&#37319;&#26679;&#65292;&#25552;&#39640;&#20102;&#23545;&#27604;&#23398;&#20064;&#30340;&#23398;&#20064;&#36895;&#24230;&#21644;&#24615;&#33021;&#12290;&#36890;&#36807;&#23558;ECPP&#24212;&#29992;&#20110;SimCLR&#31561;&#26041;&#27861;&#65292;&#26412;&#30740;&#31350;&#39564;&#35777;&#20102;ECPP&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.05730</link><description>&lt;p&gt;
&#20351;&#29992;&#39640;&#25928;&#30340;&#32452;&#21512;&#27491;&#26679;&#26412;&#37197;&#23545;&#22686;&#24378;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Enhancing Contrastive Learning with Efficient Combinatorial Positive Pairing. (arXiv:2401.05730v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05730
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ECPP&#65288;&#39640;&#25928;&#32452;&#21512;&#27491;&#26679;&#26412;&#37197;&#23545;&#65289;&#30340;&#22810;&#35270;&#22270;&#31574;&#30053;&#65292;&#36890;&#36807;&#22686;&#21152;&#35270;&#22270;&#25968;&#37327;&#12289;&#37319;&#29992;&#23567;&#23610;&#23544;&#35270;&#22270;&#21644;&#20462;&#25913;&#36127;&#37319;&#26679;&#65292;&#25552;&#39640;&#20102;&#23545;&#27604;&#23398;&#20064;&#30340;&#23398;&#20064;&#36895;&#24230;&#21644;&#24615;&#33021;&#12290;&#36890;&#36807;&#23558;ECPP&#24212;&#29992;&#20110;SimCLR&#31561;&#26041;&#27861;&#65292;&#26412;&#30740;&#31350;&#39564;&#35777;&#20102;ECPP&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#20960;&#24180;&#37324;&#65292;&#23545;&#27604;&#23398;&#20064;&#22312;&#35270;&#35273;&#26080;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#30340;&#25104;&#21151;&#20013;&#36215;&#30528;&#26680;&#24515;&#20316;&#29992;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#39640;&#24615;&#33021;&#30340;&#38750;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#20063;&#24471;&#21040;&#20102;&#21457;&#23637;&#12290;&#34429;&#28982;&#22823;&#37096;&#20998;&#24037;&#20316;&#20165;&#21033;&#29992;&#20102;&#20004;&#20010;&#35270;&#22270;&#65292;&#25105;&#20204;&#20180;&#32454;&#23457;&#26597;&#20102;&#29616;&#26377;&#30340;&#22810;&#35270;&#22270;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#22810;&#35270;&#22270;&#31574;&#30053;&#65292;&#21487;&#20197;&#25552;&#39640;&#20219;&#20309;&#23545;&#27604;&#25110;&#38750;&#23545;&#27604;&#26041;&#27861;&#30340;&#23398;&#20064;&#36895;&#24230;&#21644;&#24615;&#33021;&#12290;&#25105;&#20204;&#39318;&#20808;&#20998;&#26512;&#20102;CMC&#30340;&#20840;&#22270;&#33539;&#24335;&#65292;&#24182;&#32463;&#39564;&#24615;&#22320;&#34920;&#26126;&#65292;&#22312;&#23567;&#23398;&#20064;&#29575;&#21644;&#26089;&#26399;&#35757;&#32451;&#26102;&#65292;K&#20010;&#35270;&#22270;&#30340;&#23398;&#20064;&#36895;&#24230;&#21487;&#20197;&#22686;&#21152;$K$-$views$&#20493;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#28151;&#21512;&#20165;&#35009;&#21098;&#22686;&#24378;&#21019;&#24314;&#30340;&#35270;&#22270;&#65292;&#37319;&#29992;SwAV&#22810;&#35009;&#21098;&#20013;&#30340;&#23567;&#23610;&#23544;&#35270;&#22270;&#65292;&#24182;&#20462;&#25913;&#36127;&#37319;&#26679;&#26469;&#21319;&#32423;CMC&#30340;&#20840;&#22270;&#12290;&#20135;&#29983;&#30340;&#22810;&#35270;&#22270;&#31574;&#30053;&#34987;&#31216;&#20026;ECPP&#65288;&#39640;&#25928;&#32452;&#21512;&#27491;&#26679;&#26412;&#37197;&#23545;&#65289;&#12290;&#36890;&#36807;&#23558;ECPP&#24212;&#29992;&#20110;SimCLR&#21644;&#19968;&#20010;&#26679;&#26412;&#25928;&#26524;&#39564;&#35777;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the past few years, contrastive learning has played a central role for the success of visual unsupervised representation learning. Around the same time, high-performance non-contrastive learning methods have been developed as well. While most of the works utilize only two views, we carefully review the existing multi-view methods and propose a general multi-view strategy that can improve learning speed and performance of any contrastive or non-contrastive method. We first analyze CMC's full-graph paradigm and empirically show that the learning speed of $K$-views can be increased by $_{K}\mathrm{C}_{2}$ times for small learning rate and early training. Then, we upgrade CMC's full-graph by mixing views created by a crop-only augmentation, adopting small-size views as in SwAV multi-crop, and modifying the negative sampling. The resulting multi-view strategy is called ECPP (Efficient Combinatorial Positive Pairing). We investigate the effectiveness of ECPP by applying it to SimCLR and a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#27491;&#21017;&#21270;&#25209;&#37327;&#36755;&#20837;&#8221;&#30340;&#26032;&#39062;&#31574;&#30053;&#65292;&#36890;&#36807;&#22686;&#24378;&#36755;&#20837;&#22810;&#26679;&#24615;&#26469;&#20943;&#36731;&#20302;&#24310;&#36831;&#21516;&#26102;&#35821;&#38899;&#32763;&#35793;&#20013;&#30340;&#36755;&#20986;&#38169;&#35823;&#12290;</title><link>http://arxiv.org/abs/2401.05700</link><description>&lt;p&gt;
R-BI: &#27491;&#21017;&#21270;&#25209;&#37327;&#36755;&#20837;&#22686;&#24378;&#22686;&#37327;&#35299;&#30721;&#26694;&#26550;&#29992;&#20110;&#20302;&#24310;&#36831;&#21516;&#26102;&#35821;&#38899;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
R-BI: Regularized Batched Inputs enhance Incremental Decoding Framework for Low-Latency Simultaneous Speech Translation. (arXiv:2401.05700v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05700
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#27491;&#21017;&#21270;&#25209;&#37327;&#36755;&#20837;&#8221;&#30340;&#26032;&#39062;&#31574;&#30053;&#65292;&#36890;&#36807;&#22686;&#24378;&#36755;&#20837;&#22810;&#26679;&#24615;&#26469;&#20943;&#36731;&#20302;&#24310;&#36831;&#21516;&#26102;&#35821;&#38899;&#32763;&#35793;&#20013;&#30340;&#36755;&#20986;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22686;&#37327;&#35299;&#30721;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#26694;&#26550;&#65292;&#23427;&#22312;&#21516;&#26102;&#26465;&#20214;&#19979;&#20351;&#29992;&#31163;&#32447;&#27169;&#22411;&#32780;&#19981;&#20462;&#25913;&#21407;&#22987;&#27169;&#22411;&#65292;&#20351;&#20854;&#36866;&#29992;&#20110;&#20302;&#24310;&#36831;&#30340;&#21516;&#26102;&#35821;&#38899;&#32763;&#35793;&#12290;&#28982;&#32780;&#65292;&#24403;&#31995;&#32479;&#36755;&#20986;&#19981;&#23436;&#25972;&#30340;&#36755;&#20837;&#26102;&#65292;&#36825;&#20010;&#26694;&#26550;&#21487;&#33021;&#20250;&#24341;&#20837;&#38169;&#35823;&#12290;&#20026;&#20102;&#20943;&#23569;&#36825;&#20123;&#36755;&#20986;&#38169;&#35823;&#65292;&#21487;&#20197;&#37319;&#29992;&#20960;&#31181;&#31574;&#30053;&#65292;&#22914;Hold-n&#65292;LA-n&#21644;SP-n&#65292;&#20294;&#38656;&#35201;&#20180;&#32454;&#36873;&#25321;&#36229;&#21442;&#25968;n&#20197;&#33719;&#21462;&#26368;&#20339;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#31574;&#30053;&#23545;&#20110;&#31471;&#21040;&#31471;&#31995;&#32479;&#32780;&#35328;&#26356;&#36866;&#29992;&#20110;&#32423;&#32852;&#31995;&#32479;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#19988;&#39640;&#25928;&#30340;&#31574;&#30053;&#65292;&#31216;&#20026;&#8220;&#27491;&#21017;&#21270;&#25209;&#37327;&#36755;&#20837;&#8221;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#22686;&#24378;&#36755;&#20837;&#22810;&#26679;&#24615;&#26469;&#20943;&#36731;&#36755;&#20986;&#38169;&#35823;&#12290;&#25105;&#20204;&#20026;&#31471;&#21040;&#31471;&#31995;&#32479;&#21644;&#32423;&#32852;&#31995;&#32479;&#25552;&#20379;&#20102;&#29305;&#23450;&#30340;&#27491;&#21017;&#21270;&#25216;&#26415;&#12290;&#25105;&#20204;&#22312;IWSLT Simultaneous Speech Translation&#65288;SimulST&#65289;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#35777;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#22312;&#20302;&#24310;&#36831;&#21516;&#26102;&#23454;&#29616;&#35821;&#38899;&#32763;&#35793;&#12290;
&lt;/p&gt;
&lt;p&gt;
Incremental Decoding is an effective framework that enables the use of an offline model in a simultaneous setting without modifying the original model, making it suitable for Low-Latency Simultaneous Speech Translation. However, this framework may introduce errors when the system outputs from incomplete input. To reduce these output errors, several strategies such as Hold-$n$, LA-$n$, and SP-$n$ can be employed, but the hyper-parameter $n$ needs to be carefully selected for optimal performance. Moreover, these strategies are more suitable for end-to-end systems than cascade systems. In our paper, we propose a new adaptable and efficient policy named "Regularized Batched Inputs". Our method stands out by enhancing input diversity to mitigate output errors. We suggest particular regularization techniques for both end-to-end and cascade systems. We conducted experiments on IWSLT Simultaneous Speech Translation (SimulST) tasks, which demonstrate that our approach achieves low latency while
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#28145;&#24230;&#23398;&#20064;&#19982;&#26426;&#21046;&#35774;&#35745;&#30340;&#32467;&#21512;&#65292;&#25506;&#35752;&#20102;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#26080;&#27861;&#21516;&#26102;&#28385;&#36275;&#25152;&#26377;&#26399;&#26395;&#29305;&#24615;&#30340;&#24773;&#20917;&#19979;&#65292;&#23398;&#20064;&#36817;&#20284;&#28385;&#36275;&#29305;&#24615;&#35201;&#27714;&#30340;&#26426;&#21046;&#12290;</title><link>http://arxiv.org/abs/2401.05683</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#19982;&#26426;&#21046;&#35774;&#35745;&#65306;&#20851;&#38190;&#32467;&#26524;&#21644;&#19968;&#20123;&#26032;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Deep Learning Meets Mechanism Design: Key Results and Some Novel Applications. (arXiv:2401.05683v1 [cs.GT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05683
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#28145;&#24230;&#23398;&#20064;&#19982;&#26426;&#21046;&#35774;&#35745;&#30340;&#32467;&#21512;&#65292;&#25506;&#35752;&#20102;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#26080;&#27861;&#21516;&#26102;&#28385;&#36275;&#25152;&#26377;&#26399;&#26395;&#29305;&#24615;&#30340;&#24773;&#20917;&#19979;&#65292;&#23398;&#20064;&#36817;&#20284;&#28385;&#36275;&#29305;&#24615;&#35201;&#27714;&#30340;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#21046;&#35774;&#35745;&#26412;&#36136;&#19978;&#26159;&#23545;&#28216;&#25103;&#30340;&#36870;&#21521;&#24037;&#31243;&#65292;&#28041;&#21450;&#22312;&#21338;&#24328;&#20013;&#35825;&#23548;&#19968;&#31181;&#26041;&#24335;&#65292;&#20351;&#24471;&#35825;&#23548;&#30340;&#21338;&#24328;&#22312;&#21338;&#24328;&#22343;&#34913;&#20013;&#28385;&#36275;&#19968;&#32452;&#26399;&#26395;&#30340;&#29305;&#24615;&#12290;&#26426;&#21046;&#30340;&#26399;&#26395;&#29305;&#24615;&#21253;&#25324;&#28608;&#21169;&#20860;&#23481;&#24615;&#12289;&#20010;&#20307;&#21512;&#29702;&#24615;&#12289;&#31119;&#21033;&#26368;&#22823;&#21270;&#12289;&#25910;&#20837;&#26368;&#22823;&#21270;&#65288;&#25110;&#25104;&#26412;&#26368;&#23567;&#21270;&#65289;&#12289;&#20998;&#37197;&#20844;&#24179;&#31561;&#12290;&#26681;&#25454;&#26426;&#21046;&#35774;&#35745;&#29702;&#35770;&#65292;&#21482;&#26377;&#26576;&#20123;&#20005;&#26684;&#30340;&#23376;&#38598;&#21487;&#20197;&#21516;&#26102;&#34987;&#20219;&#20309;&#32473;&#23450;&#30340;&#26426;&#21046;&#23436;&#20840;&#28385;&#36275;&#12290;&#22312;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#65292;&#36890;&#24120;&#25152;&#38656;&#30340;&#26426;&#21046;&#21487;&#33021;&#38656;&#35201;&#19968;&#20123;&#22312;&#29702;&#35770;&#19978;&#26080;&#27861;&#21516;&#26102;&#28385;&#36275;&#30340;&#29305;&#24615;&#23376;&#38598;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#19968;&#20010;&#26174;&#33879;&#30340;&#36817;&#26399;&#26041;&#27861;&#26159;&#20351;&#29992;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#36866;&#24403;&#23450;&#20041;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#23398;&#20064;&#19968;&#20010;&#36817;&#20284;&#28385;&#36275;&#25152;&#38656;&#29305;&#24615;&#30340;&#26426;&#21046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20174;&#30456;&#20851;&#25991;&#29486;&#20013;&#20171;&#32461;&#20102;&#25216;&#26415;&#32454;&#33410;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mechanism design is essentially reverse engineering of games and involves inducing a game among strategic agents in a way that the induced game satisfies a set of desired properties in an equilibrium of the game. Desirable properties for a mechanism include incentive compatibility, individual rationality, welfare maximisation, revenue maximisation (or cost minimisation), fairness of allocation, etc. It is known from mechanism design theory that only certain strict subsets of these properties can be simultaneously satisfied exactly by any given mechanism. Often, the mechanisms required by real-world applications may need a subset of these properties that are theoretically impossible to be simultaneously satisfied. In such cases, a prominent recent approach is to use a deep learning based approach to learn a mechanism that approximately satisfies the required properties by minimizing a suitably defined loss function. In this paper, we present, from relevant literature, technical details 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23558;&#30740;&#31350;&#22914;&#20309;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#26469;&#36741;&#21161;&#25171;&#30772;&#32593;&#32476;&#25915;&#20987;&#29983;&#21629;&#21608;&#26399;&#30340;&#27599;&#20010;&#38454;&#27573;&#65292;&#36890;&#36807;&#22788;&#29702;&#21644;&#23398;&#20064;&#26469;&#33258;&#24322;&#26500;&#32593;&#32476;&#23041;&#32961;&#25968;&#25454;&#65292;&#20197;&#22686;&#24378;&#38450;&#24481;&#25514;&#26045;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.05680</link><description>&lt;p&gt;
&#22312;&#36741;&#21161;&#38450;&#24481;&#24615;&#32593;&#32476;&#25805;&#20316;&#20013;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Use of Graph Neural Networks in Aiding Defensive Cyber Operations. (arXiv:2401.05680v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05680
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#30740;&#31350;&#22914;&#20309;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#26469;&#36741;&#21161;&#25171;&#30772;&#32593;&#32476;&#25915;&#20987;&#29983;&#21629;&#21608;&#26399;&#30340;&#27599;&#20010;&#38454;&#27573;&#65292;&#36890;&#36807;&#22788;&#29702;&#21644;&#23398;&#20064;&#26469;&#33258;&#24322;&#26500;&#32593;&#32476;&#23041;&#32961;&#25968;&#25454;&#65292;&#20197;&#22686;&#24378;&#38450;&#24481;&#25514;&#26045;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19968;&#20010;&#26085;&#30410;&#20114;&#32852;&#30340;&#19990;&#30028;&#20013;&#65292;&#20449;&#24687;&#26159;&#29616;&#20195;&#31038;&#20250;&#30340;&#21629;&#33033;&#65292;&#24120;&#35265;&#30340;&#32593;&#32476;&#25915;&#20987;&#30772;&#22351;&#20102;&#25968;&#23383;&#31995;&#32479;&#21644;&#20449;&#24687;&#30340;&#26426;&#23494;&#24615;&#12289;&#23436;&#25972;&#24615;&#21644;&#21487;&#29992;&#24615;&#12290;&#27492;&#22806;&#65292;&#32593;&#32476;&#25915;&#20987;&#26681;&#25454;&#30446;&#26631;&#30340;&#19981;&#21516;&#32780;&#19981;&#21516;&#65292;&#24182;&#19988;&#36805;&#36895;&#28436;&#21464;&#20197;&#25513;&#30422;&#38450;&#24481;&#31995;&#32479;&#12290;&#28982;&#32780;&#65292;&#20856;&#22411;&#30340;&#32593;&#32476;&#25915;&#20987;&#23637;&#31034;&#20102;&#20174;&#25915;&#20987;&#21457;&#36215;&#21040;&#26368;&#32456;&#35299;&#20915;&#30340;&#19968;&#31995;&#21015;&#38454;&#27573;&#65292;&#31216;&#20026;&#25915;&#20987;&#29983;&#21629;&#21608;&#26399;&#12290;&#36825;&#20123;&#22810;&#26679;&#30340;&#29305;&#24449;&#21644;&#32593;&#32476;&#25915;&#20987;&#30340;&#19981;&#25032;&#28436;&#36827;&#20351;&#24471;&#32593;&#32476;&#38450;&#24481;&#37319;&#32435;&#20102;&#29616;&#20195;&#26041;&#27861;&#65292;&#22914;&#26426;&#22120;&#23398;&#20064;&#65292;&#20197;&#22686;&#24378;&#38450;&#24481;&#25514;&#26045;&#24182;&#25171;&#30772;&#25915;&#20987;&#29983;&#21629;&#21608;&#26399;&#12290;&#22312;&#37319;&#29992;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#20013;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#22788;&#29702;&#21644;&#23398;&#20064;&#26469;&#33258;&#24322;&#26500;&#32593;&#32476;&#23041;&#32961;&#25968;&#25454;&#26469;&#22686;&#24378;&#38450;&#24481;&#25514;&#26045;&#30340;&#26377;&#25928;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#30740;&#31350;&#22312;&#36741;&#21161;&#25171;&#30772;&#25915;&#20987;&#29983;&#21629;&#21608;&#26399;&#30340;&#27599;&#20010;&#38454;&#27573;&#20013;&#24212;&#29992;GNN&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In an increasingly interconnected world, where information is the lifeblood of modern society, regular cyber-attacks sabotage the confidentiality, integrity, and availability of digital systems and information. Additionally, cyber-attacks differ depending on the objective and evolve rapidly to disguise defensive systems. However, a typical cyber-attack demonstrates a series of stages from attack initiation to final resolution, called an attack life cycle. These diverse characteristics and the relentless evolution of cyber attacks have led cyber defense to adopt modern approaches like Machine Learning to bolster defensive measures and break the attack life cycle. Among the adopted ML approaches, Graph Neural Networks have emerged as a promising approach for enhancing the effectiveness of defensive measures due to their ability to process and learn from heterogeneous cyber threat data. In this paper, we look into the application of GNNs in aiding to break each stage of one of the most re
&lt;/p&gt;</description></item><item><title>EsaCL&#26159;&#19968;&#31181;&#39640;&#25928;&#31232;&#30095;&#27169;&#22411;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#21160;&#20462;&#21098;&#20887;&#20313;&#21442;&#25968;&#24182;&#36991;&#20813;&#37325;&#26032;&#35757;&#32451;&#65292;&#35299;&#20915;&#20102;&#25345;&#32493;&#23398;&#20064;&#20013;&#23384;&#20648;&#21644;&#35745;&#31639;&#38656;&#27714;&#22686;&#21152;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.05667</link><description>&lt;p&gt;
EsaCL: &#39640;&#25928;&#31232;&#30095;&#27169;&#22411;&#30340;&#25345;&#32493;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
EsaCL: Efficient Continual Learning of Sparse Models. (arXiv:2401.05667v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05667
&lt;/p&gt;
&lt;p&gt;
EsaCL&#26159;&#19968;&#31181;&#39640;&#25928;&#31232;&#30095;&#27169;&#22411;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#21160;&#20462;&#21098;&#20887;&#20313;&#21442;&#25968;&#24182;&#36991;&#20813;&#37325;&#26032;&#35757;&#32451;&#65292;&#35299;&#20915;&#20102;&#25345;&#32493;&#23398;&#20064;&#20013;&#23384;&#20648;&#21644;&#35745;&#31639;&#38656;&#27714;&#22686;&#21152;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25345;&#32493;&#23398;&#20064;&#29615;&#22659;&#20013;&#65292;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#26159;&#22312;&#19981;&#24536;&#35760;&#22914;&#20309;&#25191;&#34892;&#20808;&#21069;&#23398;&#20064;&#20219;&#21153;&#30340;&#24773;&#20917;&#19979;&#65292;&#39640;&#25928;&#22320;&#23398;&#20064;&#19968;&#31995;&#21015;&#20219;&#21153;&#12290;&#35768;&#22810;&#29616;&#26377;&#30340;&#26041;&#27861;&#36890;&#36807;&#22312;&#20808;&#21069;&#20219;&#21153;&#19978;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#25110;&#25193;&#23637;&#27169;&#22411;&#20197;&#36866;&#24212;&#26032;&#20219;&#21153;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#38754;&#20020;&#23384;&#20648;&#21644;&#35745;&#31639;&#38656;&#27714;&#30340;&#22686;&#21152;&#38382;&#39064;&#65292;&#32780;&#23545;&#20110;&#31232;&#30095;&#27169;&#22411;&#26469;&#35828;&#65292;&#30001;&#20110;&#38656;&#35201;&#26114;&#36149;&#30340;&#31232;&#30095;&#21270;&#21518;&#37325;&#26032;&#35757;&#32451;&#65292;&#36825;&#20010;&#38382;&#39064;&#26356;&#21152;&#20005;&#37325;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#39640;&#25928;&#31232;&#30095;&#27169;&#22411;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#65288;EsaCL&#65289;&#65292;&#23427;&#21487;&#20197;&#33258;&#21160;&#20462;&#21098;&#20887;&#20313;&#21442;&#25968;&#65292;&#32780;&#19981;&#20250;&#23545;&#27169;&#22411;&#30340;&#39044;&#27979;&#33021;&#21147;&#20135;&#29983;&#19981;&#21033;&#24433;&#21709;&#65292;&#24182;&#19988;&#21487;&#20197;&#36991;&#20813;&#37325;&#26032;&#35757;&#32451;&#30340;&#38656;&#35201;&#12290;&#25105;&#20204;&#23545;&#21442;&#25968;&#20462;&#21098;&#30340;&#25439;&#22833;&#20989;&#25968;&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;&#25439;&#22833;&#20989;&#25968;&#23545;&#27169;&#22411;&#21442;&#25968;&#25935;&#24863;&#24615;&#30340;&#26041;&#21521;&#24615;&#20462;&#21098;&#65288;SDP&#65289;&#31574;&#30053;&#12290;SDP&#20445;&#35777;&#20102;&#27169;&#22411;&#22312;&#23398;&#20064;&#26032;&#20219;&#21153;&#26102;&#30340;&#24615;&#33021;&#21644;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
A key challenge in the continual learning setting is to efficiently learn a sequence of tasks without forgetting how to perform previously learned tasks. Many existing approaches to this problem work by either retraining the model on previous tasks or by expanding the model to accommodate new tasks. However, these approaches typically suffer from increased storage and computational requirements, a problem that is worsened in the case of sparse models due to need for expensive re-training after sparsification. To address this challenge, we propose a new method for efficient continual learning of sparse models (EsaCL) that can automatically prune redundant parameters without adversely impacting the model's predictive power, and circumvent the need of retraining. We conduct a theoretical analysis of loss landscapes with parameter pruning, and design a directional pruning (SDP) strategy that is informed by the sharpness of the loss function with respect to the model parameters. SDP ensures
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;AMIE&#65292;&#35813;&#31995;&#32479;&#21033;&#29992;&#33258;&#25105;&#23545;&#25112;&#30340;&#27169;&#25311;&#29615;&#22659;&#21644;&#33258;&#21160;&#21270;&#21453;&#39304;&#26426;&#21046;&#36827;&#34892;&#35786;&#26029;&#23545;&#35805;&#65292;&#24182;&#19988;&#36890;&#36807;&#35780;&#20272;&#30149;&#21490;&#37319;&#38598;&#12289;&#35786;&#26029;&#20934;&#30830;&#24615;&#12289;&#31649;&#29702;&#25512;&#29702;&#12289;&#27807;&#36890;&#25216;&#24039;&#21644;&#21516;&#29702;&#24515;&#31561;&#32500;&#24230;&#24615;&#33021;&#65292;&#19982;&#21021;&#32423;&#20445;&#20581;&#21307;&#29983;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;</title><link>http://arxiv.org/abs/2401.05654</link><description>&lt;p&gt;
&#36808;&#21521;&#23545;&#35805;&#24335;&#35786;&#26029;&#20154;&#24037;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
Towards Conversational Diagnostic AI. (arXiv:2401.05654v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05654
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;AMIE&#65292;&#35813;&#31995;&#32479;&#21033;&#29992;&#33258;&#25105;&#23545;&#25112;&#30340;&#27169;&#25311;&#29615;&#22659;&#21644;&#33258;&#21160;&#21270;&#21453;&#39304;&#26426;&#21046;&#36827;&#34892;&#35786;&#26029;&#23545;&#35805;&#65292;&#24182;&#19988;&#36890;&#36807;&#35780;&#20272;&#30149;&#21490;&#37319;&#38598;&#12289;&#35786;&#26029;&#20934;&#30830;&#24615;&#12289;&#31649;&#29702;&#25512;&#29702;&#12289;&#27807;&#36890;&#25216;&#24039;&#21644;&#21516;&#29702;&#24515;&#31561;&#32500;&#24230;&#24615;&#33021;&#65292;&#19982;&#21021;&#32423;&#20445;&#20581;&#21307;&#29983;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#30340;&#26680;&#24515;&#22312;&#20110;&#21307;&#29983;&#21644;&#24739;&#32773;&#20043;&#38388;&#30340;&#23545;&#35805;&#65292;&#29087;&#32451;&#30340;&#30149;&#21490;&#37319;&#38598;&#20026;&#20934;&#30830;&#30340;&#35786;&#26029;&#12289;&#26377;&#25928;&#30340;&#27835;&#30103;&#21644;&#25345;&#20037;&#30340;&#20449;&#20219;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;&#33021;&#22815;&#36827;&#34892;&#35786;&#26029;&#23545;&#35805;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#21487;&#20197;&#25552;&#39640;&#21307;&#30103;&#30340;&#21487;&#21450;&#24615;&#12289;&#19968;&#33268;&#24615;&#21644;&#36136;&#37327;&#12290;&#28982;&#32780;&#65292;&#25509;&#36817;&#20020;&#24202;&#19987;&#23478;&#30340;&#27700;&#24179;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;AMIE&#65288;Articulate Medical Intelligence Explorer&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#20248;&#21270;&#20110;&#35786;&#26029;&#23545;&#35805;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#12290;AMIE&#21033;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#33258;&#25105;&#23545;&#25112;&#30340;&#27169;&#25311;&#29615;&#22659;&#65292;&#24182;&#24102;&#26377;&#33258;&#21160;&#21270;&#30340;&#21453;&#39304;&#26426;&#21046;&#65292;&#20197;&#20415;&#22312;&#19981;&#21516;&#30340;&#30142;&#30149;&#29366;&#20917;&#12289;&#19987;&#19994;&#39046;&#22495;&#21644;&#24773;&#22659;&#19979;&#23454;&#29616;&#23398;&#20064;&#30340;&#25193;&#23637;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#35780;&#20272;&#20020;&#24202;&#26377;&#24847;&#20041;&#32500;&#24230;&#24615;&#33021;&#30340;&#26694;&#26550;&#65292;&#21253;&#25324;&#30149;&#21490;&#37319;&#38598;&#12289;&#35786;&#26029;&#20934;&#30830;&#24615;&#12289;&#31649;&#29702;&#25512;&#29702;&#12289;&#27807;&#36890;&#25216;&#24039;&#21644;&#21516;&#29702;&#24515;&#12290;&#25105;&#20204;&#23558;AMIE&#30340;&#24615;&#33021;&#19982;&#21021;&#32423;&#20445;&#20581;&#21307;&#29983;&#65288;PCPs&#65289;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#24182;&#20351;&#29992;&#20102;&#38543;&#26426;&#12289;&#21452;&#30450;&#21313;&#23383;&#35774;&#35745;&#30340;&#35797;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
At the heart of medicine lies the physician-patient dialogue, where skillful history-taking paves the way for accurate diagnosis, effective management, and enduring trust. Artificial Intelligence (AI) systems capable of diagnostic dialogue could increase accessibility, consistency, and quality of care. However, approximating clinicians' expertise is an outstanding grand challenge. Here, we introduce AMIE (Articulate Medical Intelligence Explorer), a Large Language Model (LLM) based AI system optimized for diagnostic dialogue.  AMIE uses a novel self-play based simulated environment with automated feedback mechanisms for scaling learning across diverse disease conditions, specialties, and contexts. We designed a framework for evaluating clinically-meaningful axes of performance including history-taking, diagnostic accuracy, management reasoning, communication skills, and empathy. We compared AMIE's performance to that of primary care physicians (PCPs) in a randomized, double-blind cross
&lt;/p&gt;</description></item><item><title>&#29992;&#25143;&#36890;&#36807;&#33609;&#22270;&#21644;&#35821;&#35328;&#24314;&#31435;&#20114;&#21160;&#19990;&#30028;&#30340;&#20132;&#20114;&#24335;&#26041;&#27861;&#65292;&#20855;&#26377;&#29992;&#25143;&#25511;&#21046;&#21644;&#28789;&#27963;&#24615;&#65292;&#26080;&#38656;&#32534;&#31243;&#21363;&#21487;&#23454;&#29616;&#32534;&#31243;&#21151;&#33021;&#12290;&#36866;&#29992;&#20110;&#21508;&#31181;&#21019;&#36896;&#24615;&#25506;&#32034;&#24615;&#22330;&#26223;&#12290;</title><link>http://arxiv.org/abs/2401.05631</link><description>&lt;p&gt;
DrawTalking&#65306;&#36890;&#36807;&#33609;&#22270;&#21644;&#35821;&#35328;&#24314;&#31435;&#20114;&#21160;&#19990;&#30028;
&lt;/p&gt;
&lt;p&gt;
DrawTalking: Building Interactive Worlds by Sketching and Speaking. (arXiv:2401.05631v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05631
&lt;/p&gt;
&lt;p&gt;
&#29992;&#25143;&#36890;&#36807;&#33609;&#22270;&#21644;&#35821;&#35328;&#24314;&#31435;&#20114;&#21160;&#19990;&#30028;&#30340;&#20132;&#20114;&#24335;&#26041;&#27861;&#65292;&#20855;&#26377;&#29992;&#25143;&#25511;&#21046;&#21644;&#28789;&#27963;&#24615;&#65292;&#26080;&#38656;&#32534;&#31243;&#21363;&#21487;&#23454;&#29616;&#32534;&#31243;&#21151;&#33021;&#12290;&#36866;&#29992;&#20110;&#21508;&#31181;&#21019;&#36896;&#24615;&#25506;&#32034;&#24615;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#20132;&#20114;&#24335;&#26041;&#27861;&#65292;DrawTalking&#65292;&#29992;&#25143;&#21487;&#20197;&#36890;&#36807;&#33609;&#22270;&#21644;&#35821;&#35328;&#24314;&#31435;&#20114;&#21160;&#19990;&#30028;&#12290;&#23427;&#24378;&#35843;&#29992;&#25143;&#25511;&#21046;&#21644;&#28789;&#27963;&#24615;&#65292;&#24182;&#19988;&#22312;&#27809;&#26377;&#32534;&#31243;&#30340;&#24773;&#20917;&#19979;&#25552;&#20379;&#20102;&#31867;&#20284;&#32534;&#31243;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;iPad&#19978;&#23454;&#29616;&#20102;&#23427;&#12290;&#19968;&#39033;&#24320;&#25918;&#24335;&#30740;&#31350;&#34920;&#26126;&#65292;&#36825;&#31181;&#26426;&#21046;&#19982;&#35768;&#22810;&#21019;&#36896;&#24615;&#25506;&#32034;&#24615;&#29992;&#20363;&#30456;&#22865;&#21512;&#21644;&#36866;&#29992;&#12290;&#25105;&#20204;&#24076;&#26395;&#33021;&#22815;&#28608;&#21457;&#21644;&#25351;&#23548;&#26410;&#26469;&#33258;&#28982;&#29992;&#25143;&#20013;&#24515;&#30028;&#38754;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce an interactive approach, DrawTalking, in which the user builds interactive worlds by sketching and speaking. It emphasizes user control and flexibility, and gives programming-like capability without code. We implemented it on the iPad. An open-ended study shows the mechanics resonate and are applicable to many creative-exploratory use cases. We hope to inspire and inform research in future natural user-centered interfaces.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20351;&#29992;&#31616;&#27905;&#30340;&#24605;&#32500;&#38142;&#25552;&#31034;&#23545;&#38382;&#39064;&#27714;&#35299;&#30340;&#24433;&#21709;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#31616;&#27905;&#24615;&#19981;&#20165;&#38477;&#20302;&#20102;&#22238;&#31572;&#38271;&#24230;&#65292;&#19988;&#23545;&#38382;&#39064;&#35299;&#20915;&#24615;&#33021;&#24433;&#21709;&#21487;&#20197;&#24573;&#30053;&#12290;&#28982;&#32780;&#22312;&#25968;&#23398;&#38382;&#39064;&#19978;&#26377;&#19968;&#23450;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;&#36825;&#23545;AI&#31995;&#32479;&#24037;&#31243;&#24072;&#21644;&#30740;&#31350;&#20154;&#21592;&#37117;&#26377;&#23454;&#38469;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2401.05618</link><description>&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38382;&#39064;&#27714;&#35299;&#20013;&#65292;&#31616;&#27905;&#30340;&#24605;&#32500;&#38142;&#30340;&#22909;&#22788;
&lt;/p&gt;
&lt;p&gt;
The Benefits of a Concise Chain of Thought on Problem-Solving in Large Language Models. (arXiv:2401.05618v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05618
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20351;&#29992;&#31616;&#27905;&#30340;&#24605;&#32500;&#38142;&#25552;&#31034;&#23545;&#38382;&#39064;&#27714;&#35299;&#30340;&#24433;&#21709;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#31616;&#27905;&#24615;&#19981;&#20165;&#38477;&#20302;&#20102;&#22238;&#31572;&#38271;&#24230;&#65292;&#19988;&#23545;&#38382;&#39064;&#35299;&#20915;&#24615;&#33021;&#24433;&#21709;&#21487;&#20197;&#24573;&#30053;&#12290;&#28982;&#32780;&#22312;&#25968;&#23398;&#38382;&#39064;&#19978;&#26377;&#19968;&#23450;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;&#36825;&#23545;AI&#31995;&#32479;&#24037;&#31243;&#24072;&#21644;&#30740;&#31350;&#20154;&#21592;&#37117;&#26377;&#23454;&#38469;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#31616;&#27905;&#30340;&#24605;&#32500;&#38142;(CCoT)&#25552;&#31034;&#12290;&#25105;&#20204;&#23558;&#26631;&#20934;&#30340;CoT&#21644;CCoT&#25552;&#31034;&#36827;&#34892;&#27604;&#36739;&#65292;&#20197;&#20102;&#35299;&#31616;&#27905;&#24615;&#23545;&#22238;&#31572;&#38271;&#24230;&#21644;&#27491;&#30830;&#31572;&#26696;&#20934;&#30830;&#24615;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#20351;&#29992;GPT-3.5&#21644;GPT-4&#36827;&#34892;&#20102;&#22810;&#39033;&#36873;&#25321;&#38382;&#31572;(MCQA)&#22522;&#20934;&#30340;&#35780;&#20272;&#12290;CCoT&#23558;GPT-3.5&#21644;GPT-4&#30340;&#24179;&#22343;&#22238;&#31572;&#38271;&#24230;&#20998;&#21035;&#20943;&#23569;&#20102;48.70&#65285;&#65292;&#23545;&#38382;&#39064;&#35299;&#20915;&#24615;&#33021;&#20960;&#20046;&#27809;&#26377;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#22312;&#25968;&#23398;&#38382;&#39064;&#19978;&#65292;&#24102;&#26377;CCoT&#30340;GPT-3.5&#20250;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;27.69&#65285;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;CCoT&#23548;&#33268;&#27599;&#20010;&#26631;&#35760;&#30340;&#25104;&#26412;&#24179;&#22343;&#38477;&#20302;&#20102;22.67&#65285;&#12290;&#36825;&#20123;&#32467;&#26524;&#23545;&#20110;&#20351;&#29992;CoT&#25552;&#31034;&#24037;&#31243;&#25216;&#26415;&#30340;AI&#31995;&#32479;&#24037;&#31243;&#24072;&#26469;&#35299;&#20915;&#30495;&#23454;&#19990;&#30028;&#38382;&#39064;&#30340;LLM&#20855;&#26377;&#23454;&#38469;&#24847;&#20041;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#32467;&#26524;&#20026;&#30740;&#31350;LLM&#20013;&#36880;&#27493;&#25512;&#29702;&#30340;&#24418;&#25104;&#34892;&#20026;&#30340;AI&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#26356;&#24191;&#27867;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce Concise Chain-of-Thought (CCoT) prompting. We compared standard CoT and CCoT prompts to see how conciseness impacts response length and correct-answer accuracy. We evaluated this using GPT-3.5 and GPT-4 with a multiple-choice question-and-answer (MCQA) benchmark. CCoT reduced average response length by 48.70% for both GPT-3.5 and GPT-4 while having a negligible impact on problem-solving performance. However, on math problems, GPT-3.5 with CCoT incurs a performance penalty of 27.69%. Overall, CCoT leads to an average per-token cost reduction of 22.67%. These results have practical implications for AI systems engineers using LLMs to solve real-world problems with CoT prompt-engineering techniques. In addition, these results provide more general insight for AI researchers studying the emergent behavior of step-by-step reasoning in LLMs.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#24182;&#35777;&#26126;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#24212;&#29992;&#20110;&#35299;&#20915;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#12290;&#36890;&#36807;&#23558;&#20248;&#21270;&#36807;&#31243;&#24418;&#24335;&#21270;&#20026;&#19968;&#20010;&#39034;&#24207;&#20915;&#31574;&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;Q-Learning&#35757;&#32451;&#30340;GNNs&#26469;&#23398;&#20064;&#31574;&#30053;&#65292;&#21487;&#20197;&#36798;&#21040;&#25509;&#36817;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#21551;&#21457;&#24335;&#27714;&#35299;&#22120;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#21482;&#38656;&#20351;&#29992;&#37096;&#20998;&#21442;&#25968;&#21644;&#35757;&#32451;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2401.05610</link><description>&lt;p&gt;
&#22270;&#24418;Q-Learning&#29992;&#20110;&#32452;&#21512;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Graph Q-Learning for Combinatorial Optimization. (arXiv:2401.05610v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05610
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#24182;&#35777;&#26126;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#24212;&#29992;&#20110;&#35299;&#20915;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#12290;&#36890;&#36807;&#23558;&#20248;&#21270;&#36807;&#31243;&#24418;&#24335;&#21270;&#20026;&#19968;&#20010;&#39034;&#24207;&#20915;&#31574;&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;Q-Learning&#35757;&#32451;&#30340;GNNs&#26469;&#23398;&#20064;&#31574;&#30053;&#65292;&#21487;&#20197;&#36798;&#21040;&#25509;&#36817;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#21551;&#21457;&#24335;&#27714;&#35299;&#22120;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#21482;&#38656;&#20351;&#29992;&#37096;&#20998;&#21442;&#25968;&#21644;&#35757;&#32451;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#24418;&#32467;&#26500;&#21270;&#25968;&#25454;&#22312;&#33258;&#28982;&#31185;&#23398;&#21644;&#31038;&#20250;&#31185;&#23398;&#20013;&#26080;&#22788;&#19981;&#22312;&#65292;&#32780;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#26368;&#36817;&#24050;&#34987;&#35777;&#26126;&#21487;&#20197;&#26377;&#25928;&#22320;&#35299;&#20915;&#22270;&#24418;&#25968;&#25454;&#19978;&#30340;&#39044;&#27979;&#21644;&#25512;&#29702;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#24182;&#35777;&#26126;&#20102;GNNs&#21487;&#20197;&#24212;&#29992;&#20110;&#35299;&#20915;&#32452;&#21512;&#20248;&#21270;&#65288;CO&#65289;&#38382;&#39064;&#12290;CO&#28041;&#21450;&#22312;&#36890;&#24120;&#38750;&#24120;&#24222;&#22823;&#30340;&#31163;&#25955;&#35299;&#31354;&#38388;&#19978;&#20248;&#21270;&#20989;&#25968;&#12290;&#20026;&#20102;&#23398;&#20064;&#35299;&#20915;CO&#38382;&#39064;&#65292;&#25105;&#20204;&#23558;&#20248;&#21270;&#36807;&#31243;&#24418;&#24335;&#21270;&#20026;&#19968;&#20010;&#39034;&#24207;&#20915;&#31574;&#38382;&#39064;&#65292;&#20854;&#20013;&#22238;&#25253;&#19982;&#20505;&#36873;&#35299;&#19982;&#26368;&#20248;&#35299;&#30340;&#25509;&#36817;&#31243;&#24230;&#26377;&#20851;&#12290;&#25105;&#20204;&#20351;&#29992;GNN&#26469;&#23398;&#20064;&#31574;&#30053;&#65292;&#20197;&#36845;&#20195;&#22320;&#26500;&#24314;&#36234;&#26469;&#36234;&#26377;&#21069;&#26223;&#30340;&#20505;&#36873;&#35299;&#12290;&#25105;&#20204;&#25552;&#20986;&#21021;&#27493;&#35777;&#25454;&#34920;&#26126;&#65292;&#20351;&#29992;Q-Learning&#35757;&#32451;&#30340;GNNs&#21487;&#20197;&#35299;&#20915;CO&#38382;&#39064;&#65292;&#20854;&#24615;&#33021;&#25509;&#36817;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#21551;&#21457;&#24335;&#27714;&#35299;&#22120;&#30340;&#24615;&#33021;&#65292;&#20165;&#20351;&#29992;&#20102;&#37096;&#20998;&#21442;&#25968;&#21644;&#35757;&#32451;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph-structured data is ubiquitous throughout natural and social sciences, and Graph Neural Networks (GNNs) have recently been shown to be effective at solving prediction and inference problems on graph data. In this paper, we propose and demonstrate that GNNs can be applied to solve Combinatorial Optimization (CO) problems. CO concerns optimizing a function over a discrete solution space that is often intractably large. To learn to solve CO problems, we formulate the optimization process as a sequential decision making problem, where the return is related to how close the candidate solution is to optimality. We use a GNN to learn a policy to iteratively build increasingly promising candidate solutions. We present preliminary evidence that GNNs trained through Q-Learning can solve CO problems with performance approaching state-of-the-art heuristic-based solvers, using only a fraction of the parameters and training time.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;&#22810;&#27169;&#24577;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;rebus&#35868;&#39064;&#19978;&#24615;&#33021;&#30340;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;&#21457;&#29616;&#19987;&#26377;&#27169;&#22411;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#27979;&#35797;&#27169;&#22411;&#65292;&#20294;&#26368;&#20339;&#27169;&#22411;&#30340;&#20934;&#30830;&#29575;&#20165;&#20026;24%&#12290;&#35813;&#22522;&#20934;&#27979;&#35797;&#21487;&#29992;&#20110;&#35782;&#21035;&#30693;&#35782;&#19978;&#30340;&#20027;&#35201;&#32570;&#38519;&#12290;</title><link>http://arxiv.org/abs/2401.05604</link><description>&lt;p&gt;
REBUS: &#19968;&#31181;&#23545;&#31526;&#21495;&#29702;&#35299;&#36827;&#34892;&#40065;&#26834;&#35780;&#20272;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
REBUS: A Robust Evaluation Benchmark of Understanding Symbols. (arXiv:2401.05604v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05604
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;&#22810;&#27169;&#24577;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;rebus&#35868;&#39064;&#19978;&#24615;&#33021;&#30340;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;&#21457;&#29616;&#19987;&#26377;&#27169;&#22411;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#27979;&#35797;&#27169;&#22411;&#65292;&#20294;&#26368;&#20339;&#27169;&#22411;&#30340;&#20934;&#30830;&#29575;&#20165;&#20026;24%&#12290;&#35813;&#22522;&#20934;&#27979;&#35797;&#21487;&#29992;&#20110;&#35782;&#21035;&#30693;&#35782;&#19978;&#30340;&#20027;&#35201;&#32570;&#38519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;&#22810;&#27169;&#24577;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;rebus&#35868;&#39064;&#19978;&#30340;&#24615;&#33021;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#25324;333&#20010;&#21407;&#22987;&#30340;&#22522;&#20110;&#22270;&#20687;&#30340;&#25991;&#23383;&#28216;&#25103;&#31034;&#20363;&#65292;&#28085;&#30422;&#20102;&#30005;&#24433;&#12289;&#20316;&#26354;&#23478;&#12289;&#20027;&#35201;&#22478;&#24066;&#21644;&#39135;&#29289;&#31561;13&#20010;&#31867;&#21035;&#12290;&#20026;&#20102;&#22312;&#35782;&#21035;&#25552;&#31034;&#30340;&#35789;&#35821;&#25110;&#30701;&#35821;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#33719;&#24471;&#33391;&#22909;&#24615;&#33021;&#65292;&#27169;&#22411;&#24517;&#39035;&#32467;&#21512;&#22270;&#20687;&#35782;&#21035;&#21644;&#23383;&#31526;&#20018;&#25805;&#20316;&#65292;&#36827;&#34892;&#20551;&#35774;&#26816;&#39564;&#12289;&#22810;&#27493;&#25512;&#29702;&#21644;&#23545;&#20154;&#31867;&#35748;&#30693;&#30340;&#29702;&#35299;&#65292;&#36825;&#20351;&#24471;&#35780;&#20272;&#33021;&#21147;&#21464;&#24471;&#22797;&#26434;&#32780;&#22810;&#27169;&#24577;&#12290;&#25105;&#20204;&#21457;&#29616;&#19987;&#26377;&#27169;&#22411;&#22914;GPT-4V&#21644;Gemini Pro&#26126;&#26174;&#20248;&#20110;&#25152;&#26377;&#20854;&#20182;&#27979;&#35797;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#26368;&#22909;&#30340;&#27169;&#22411;&#20063;&#21482;&#26377;24%&#30340;&#26368;&#32456;&#20934;&#30830;&#29575;&#65292;&#31361;&#26174;&#20986;&#22312;&#25512;&#29702;&#26041;&#38754;&#38656;&#35201;&#23454;&#36136;&#24615;&#30340;&#25913;&#36827;&#12290;&#27492;&#22806;&#65292;&#27169;&#22411;&#24456;&#23569;&#29702;&#35299;&#35868;&#39064;&#30340;&#25152;&#26377;&#37096;&#20998;&#65292;&#20960;&#20046;&#24635;&#26159;&#26080;&#27861;&#20107;&#21518;&#35299;&#37322;&#27491;&#30830;&#31572;&#26696;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#22522;&#20934;&#27979;&#35797;&#21487;&#20197;&#29992;&#20110;&#35782;&#21035;&#30693;&#35782;&#30340;&#20027;&#35201;&#32570;&#38519;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a new benchmark evaluating the performance of multimodal large language models on rebus puzzles. The dataset covers 333 original examples of image-based wordplay, cluing 13 categories such as movies, composers, major cities, and food. To achieve good performance on the benchmark of identifying the clued word or phrase, models must combine image recognition and string manipulation with hypothesis testing, multi-step reasoning, and an understanding of human cognition, making for a complex, multimodal evaluation of capabilities. We find that proprietary models such as GPT-4V and Gemini Pro significantly outperform all other tested models. However, even the best model has a final accuracy of just 24%, highlighting the need for substantial improvements in reasoning. Further, models rarely understand all parts of a puzzle, and are almost always incapable of retroactively explaining the correct answer. Our benchmark can therefore be used to identify major shortcomings in the knowle
&lt;/p&gt;</description></item><item><title>POMP&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#21160;&#24577;&#30340;&#12289;&#22522;&#20110;&#25277;&#26679;&#30340;&#22810;&#36741;&#21161;&#35821;&#35328;&#22270;&#24418;&#65292;&#25552;&#39640;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#30340;&#32763;&#35793;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.05596</link><description>&lt;p&gt;
POMP:&#29992;&#20110;&#20302;&#36164;&#28304;&#26080;&#30417;&#30563;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#27010;&#29575;&#39537;&#21160;&#20803;&#22270;&#25552;&#31034;&#22120;
&lt;/p&gt;
&lt;p&gt;
POMP: Probability-driven Meta-graph Prompter for LLMs in Low-resource Unsupervised Neural Machine Translation. (arXiv:2401.05596v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05596
&lt;/p&gt;
&lt;p&gt;
POMP&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#21160;&#24577;&#30340;&#12289;&#22522;&#20110;&#25277;&#26679;&#30340;&#22810;&#36741;&#21161;&#35821;&#35328;&#22270;&#24418;&#65292;&#25552;&#39640;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#30340;&#32763;&#35793;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20302;&#36164;&#28304;&#35821;&#35328;&#22312;&#26377;&#38480;&#30340;&#24179;&#34892;&#25968;&#25454;&#19979;&#38754;&#20020;&#30528;&#22312;&#30417;&#30563;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#25361;&#25112;&#65292;&#22240;&#27492;&#30740;&#31350;&#26080;&#30417;&#30563;&#26041;&#27861;&#12290;&#26080;&#30417;&#30563;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#26041;&#27861;&#65292;&#21253;&#25324;&#21453;&#21521;&#32763;&#35793;&#12289;&#36801;&#31227;&#23398;&#20064;&#21644;&#22522;&#20110;&#26530;&#36724;&#30340;&#32763;&#35793;&#65292;&#20026;&#20302;&#36164;&#28304;&#35821;&#35328;&#32763;&#35793;&#25552;&#20379;&#20102;&#23454;&#29992;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#26159;&#23427;&#20204;&#21463;&#21040;&#21512;&#25104;&#25968;&#25454;&#22122;&#22768;&#12289;&#35821;&#35328;&#20559;&#24046;&#21644;&#38169;&#35823;&#20256;&#25773;&#31561;&#38382;&#39064;&#30340;&#24433;&#21709;&#65292;&#36825;&#20123;&#38382;&#39064;&#21487;&#20197;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#32531;&#35299;&#12290;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#21644;&#26377;&#30417;&#30563;&#24494;&#35843;&#26041;&#27861;&#25913;&#36827;&#20102;NMT&#65292;&#20294;&#26159;&#35757;&#32451;&#25968;&#25454;&#19981;&#36275;&#20351;&#24471;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#19978;&#30340;&#24615;&#33021;&#36739;&#24046;&#12290;&#25105;&#20204;&#35748;&#20026;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#36741;&#21161;&#35821;&#35328;&#20943;&#23569;&#35821;&#35328;&#22122;&#22768;&#65292;&#25552;&#39640;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#32763;&#35793;&#36136;&#37327;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;POMP&#30340;&#27010;&#29575;&#39537;&#21160;&#20803;&#22270;&#25552;&#31034;&#22120;&#65292;&#23427;&#37319;&#29992;&#20102;&#22522;&#20110;&#21160;&#24577;&#25277;&#26679;&#30340;&#22810;&#20010;&#36741;&#21161;&#35821;&#35328;&#30340;&#22270;&#24418;&#65292;&#20197;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#19978;&#30340;&#32763;&#35793;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Low-resource languages (LRLs) face challenges in supervised neural machine translation due to limited parallel data, prompting research into unsupervised methods. Unsupervised neural machine translation (UNMT) methods, including back-translation, transfer learning, and pivot-based translation, offer practical solutions for LRL translation, but they are hindered by issues like synthetic data noise, language bias, and error propagation, which can potentially be mitigated by Large Language Models (LLMs). LLMs have advanced NMT with in-context learning (ICL) and supervised fine-tuning methods, but insufficient training data results in poor performance in LRLs. We argue that LLMs can mitigate the linguistic noise with auxiliary languages to improve translations in LRLs. In this paper, we propose Probability-driven Meta-graph Prompter (POMP), a novel approach employing a dynamic, sampling-based graph of multiple auxiliary languages to enhance LLMs' translation capabilities for LRLs. POMP inv
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#25913;&#36827;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20351;&#29992;&#20165;&#38656;&#22522;&#32447;&#35201;&#27714;&#30340;1%&#35745;&#31639;&#36164;&#28304;&#35757;&#32451;FourCastNet&#65292;&#24182;&#19988;&#20445;&#25345;&#20102;&#19982;&#22522;&#32447;&#30456;&#24403;&#25110;&#29978;&#33267;&#26356;&#22909;&#30340;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.05584</link><description>&lt;p&gt;
FourCastNeXt&#65306;&#29992;&#26377;&#38480;&#35745;&#31639;&#36164;&#28304;&#25552;&#39640;FourCastNet&#30340;&#35757;&#32451;&#25928;&#26524;
&lt;/p&gt;
&lt;p&gt;
FourCastNeXt: Improving FourCastNet Training with Limited Compute. (arXiv:2401.05584v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05584
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#25913;&#36827;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20351;&#29992;&#20165;&#38656;&#22522;&#32447;&#35201;&#27714;&#30340;1%&#35745;&#31639;&#36164;&#28304;&#35757;&#32451;FourCastNet&#65292;&#24182;&#19988;&#20445;&#25345;&#20102;&#19982;&#22522;&#32447;&#30456;&#24403;&#25110;&#29978;&#33267;&#26356;&#22909;&#30340;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;FourCastNet&#31070;&#32463;&#22320;&#29699;&#31995;&#32479;&#27169;&#22411;&#65288;NESM&#65289;&#22312;&#39044;&#27979;&#21508;&#31181;&#22823;&#27668;&#21464;&#37327;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#65292;&#35813;&#27169;&#22411;&#22312;ERA5&#20877;&#20998;&#26512;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35757;&#32451;&#12290;&#34429;&#28982;&#19982;&#22522;&#26412;&#21464;&#21387;&#22120;&#30456;&#27604;&#65292;FourCastNet&#22312;&#24207;&#21015;&#38271;&#24230;&#19978;&#20139;&#26377;&#20934;&#32447;&#24615;&#30340;&#26102;&#38388;&#21644;&#20869;&#23384;&#22797;&#26434;&#24230;&#65292;&#32780;&#22522;&#20110;ERA5&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;FourCastNet&#20173;&#28982;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#36164;&#28304;&#65292;&#36825;&#23545;&#20110;&#22823;&#22810;&#25968;&#30740;&#31350;&#20154;&#21592;&#26469;&#35828;&#26159;&#26114;&#36149;&#29978;&#33267;&#26080;&#27861;&#33719;&#24471;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#23637;&#31034;&#25913;&#36827;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20351;&#29992;&#20165;&#38656;&#35201;&#22522;&#32447;&#35201;&#27714;&#30340;1%&#35745;&#31639;&#36164;&#28304;&#26469;&#35757;&#32451;FourCastNet&#65292;&#21516;&#26102;&#20445;&#25345;&#27169;&#22411;&#24615;&#33021;&#33267;&#23569;&#19982;&#22522;&#32447;&#30456;&#24403;&#29978;&#33267;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, the FourCastNet Neural Earth System Model (NESM) has shown impressive results on predicting various atmospheric variables, trained on the ERA5 reanalysis dataset. While FourCastNet enjoys quasi-linear time and memory complexity in sequence length compared to quadratic complexity in vanilla transformers, training FourCastNet on ERA5 from scratch still requires large amount of compute resources, which is expensive or even inaccessible to most researchers. In this work, we will show improved methods that can train FourCastNet using only 1% of the compute required by the baseline, while maintaining model performance or par or even better than the baseline.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20808;&#22825;&#20215;&#20540;&#39537;&#21160;&#22686;&#24378;&#23398;&#20064;&#65288;IVRL&#65289;&#27169;&#22411;&#65292;&#29992;&#20110;&#25551;&#36848;&#22810;&#26234;&#33021;&#20307;&#22312;&#21512;&#20316;&#20013;&#30340;&#22797;&#26434;&#34892;&#20026;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#24314;&#31435;&#26234;&#33021;&#20307;&#23545;&#32676;&#20307;&#25928;&#29992;&#21644;&#31995;&#32479;&#25104;&#26412;&#30340;&#35748;&#30693;&#65292;&#28385;&#36275;&#20854;&#21512;&#20316;&#20249;&#20276;&#30340;&#38656;&#27714;&#65292;&#25903;&#25345;&#20854;&#31038;&#21306;&#24182;&#34701;&#20837;&#20154;&#31867;&#31038;&#20250;&#12290;</title><link>http://arxiv.org/abs/2401.05572</link><description>&lt;p&gt;
&#29992;&#20110;&#21512;&#20316;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#30340;&#20808;&#22825;&#20215;&#20540;&#39537;&#21160;&#22686;&#24378;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Innate-Values-driven Reinforcement Learning for Cooperative Multi-Agent Systems. (arXiv:2401.05572v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05572
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20808;&#22825;&#20215;&#20540;&#39537;&#21160;&#22686;&#24378;&#23398;&#20064;&#65288;IVRL&#65289;&#27169;&#22411;&#65292;&#29992;&#20110;&#25551;&#36848;&#22810;&#26234;&#33021;&#20307;&#22312;&#21512;&#20316;&#20013;&#30340;&#22797;&#26434;&#34892;&#20026;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#24314;&#31435;&#26234;&#33021;&#20307;&#23545;&#32676;&#20307;&#25928;&#29992;&#21644;&#31995;&#32479;&#25104;&#26412;&#30340;&#35748;&#30693;&#65292;&#28385;&#36275;&#20854;&#21512;&#20316;&#20249;&#20276;&#30340;&#38656;&#27714;&#65292;&#25903;&#25345;&#20854;&#31038;&#21306;&#24182;&#34701;&#20837;&#20154;&#31867;&#31038;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#22825;&#20215;&#20540;&#25551;&#36848;&#20102;&#26234;&#33021;&#20307;&#30340;&#20869;&#22312;&#21160;&#26426;&#65292;&#21453;&#26144;&#20102;&#20182;&#20204;&#36861;&#27714;&#30446;&#26631;&#21644;&#21457;&#23637;&#22810;&#26679;&#25216;&#33021;&#20197;&#28385;&#36275;&#21508;&#31181;&#38656;&#27714;&#30340;&#22266;&#26377;&#20852;&#36259;&#21644;&#20559;&#22909;&#12290;&#24378;&#21270;&#23398;&#20064;&#30340;&#26412;&#36136;&#26159;&#22522;&#20110;&#22870;&#21169;&#39537;&#21160;&#65288;&#22914;&#25928;&#29992;&#65289;&#30340;&#34892;&#20026;&#20114;&#21160;&#23398;&#20064;&#65292;&#31867;&#20284;&#20110;&#33258;&#28982;&#26234;&#33021;&#20307;&#12290;&#29305;&#21035;&#26159;&#22312;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20013;&#65292;&#24314;&#31435;&#26234;&#33021;&#20307;&#23545;&#24179;&#34913;&#32676;&#20307;&#25928;&#29992;&#21644;&#31995;&#32479;&#25104;&#26412;&#30340;&#35748;&#30693;&#65292;&#28385;&#36275;&#32676;&#20307;&#25104;&#21592;&#22312;&#21512;&#20316;&#20013;&#30340;&#38656;&#27714;&#65292;&#26159;&#20010;&#20307;&#20026;&#25903;&#25345;&#20854;&#31038;&#21306;&#21644;&#34701;&#20837;&#20154;&#31867;&#31038;&#20250;&#32780;&#23398;&#20064;&#30340;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#22797;&#21512;&#20869;&#22312;&#20215;&#20540;&#22686;&#24378;&#23398;&#20064;&#27169;&#22411; - &#20808;&#22825;&#20215;&#20540;&#39537;&#21160;&#22686;&#24378;&#23398;&#20064;&#65292;&#29992;&#20110;&#25551;&#36848;&#22810;&#26234;&#33021;&#20307;&#21512;&#20316;&#20013;&#22797;&#26434;&#30340;&#20114;&#21160;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
Innate values describe agents' intrinsic motivations, which reflect their inherent interests and preferences to pursue goals and drive them to develop diverse skills satisfying their various needs. The essence of reinforcement learning (RL) is learning from interaction based on reward-driven (such as utilities) behaviors, much like natural agents. It is an excellent model to describe the innate-values-driven (IV) behaviors of AI agents. Especially in multi-agent systems (MAS), building the awareness of AI agents to balance the group utilities and system costs and satisfy group members' needs in their cooperation is a crucial problem for individuals learning to support their community and integrate human society in the long term. This paper proposes a hierarchical compound intrinsic value reinforcement learning model -innate-values-driven reinforcement learning termed IVRL to describe the complex behaviors of multi-agent interaction in their cooperation. We implement the IVRL architec
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36719;&#26631;&#31614;&#30340;&#23402;&#29983;&#32593;&#32476;&#26041;&#27861;&#65292;&#21033;&#29992;&#23545;&#20391;&#20083;&#25151;X&#32447;&#29255;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#65292;&#20197;&#22312;&#26080;&#30417;&#30563;&#24773;&#20917;&#19979;&#21306;&#20998;&#24322;&#24120;&#30149;&#21464;&#21644;&#32972;&#26223;&#32452;&#32455;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36890;&#36807;&#27431;&#27663;&#36317;&#31163;&#34893;&#29983;&#30340;&#36719;&#26631;&#31614;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#21306;&#20998;&#21307;&#23398;&#25104;&#20687;&#20013;&#30340;&#30149;&#21464;&#12290;</title><link>http://arxiv.org/abs/2401.05570</link><description>&lt;p&gt;
&#22522;&#20110;&#36719;&#26631;&#31614;&#30340;&#23402;&#29983;&#32593;&#32476;&#22312;&#26080;&#30417;&#30563;&#30149;&#21464;&#26816;&#27979;&#21644;&#31579;&#26597;&#20083;&#25151;X&#32447;&#29255;&#39044;&#35757;&#32451;&#26041;&#38754;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Siamese Networks with Soft Labels for Unsupervised Lesion Detection and Patch Pretraining on Screening Mammograms. (arXiv:2401.05570v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05570
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36719;&#26631;&#31614;&#30340;&#23402;&#29983;&#32593;&#32476;&#26041;&#27861;&#65292;&#21033;&#29992;&#23545;&#20391;&#20083;&#25151;X&#32447;&#29255;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#65292;&#20197;&#22312;&#26080;&#30417;&#30563;&#24773;&#20917;&#19979;&#21306;&#20998;&#24322;&#24120;&#30149;&#21464;&#21644;&#32972;&#26223;&#32452;&#32455;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36890;&#36807;&#27431;&#27663;&#36317;&#31163;&#34893;&#29983;&#30340;&#36719;&#26631;&#31614;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#21306;&#20998;&#21307;&#23398;&#25104;&#20687;&#20013;&#30340;&#30149;&#21464;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#24050;&#25104;&#20026;&#19968;&#31181;&#27969;&#34892;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#39044;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#24182;&#23558;&#20854;&#36716;&#31227;&#21040;&#25191;&#34892;&#19979;&#28216;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#36825;&#20123;&#26041;&#27861;&#26159;&#38024;&#23545;&#21253;&#21547;&#28165;&#26224;&#32441;&#29702;&#12289;&#36718;&#24275;&#21644;&#26126;&#26174;&#33394;&#24425;&#23545;&#27604;&#30340;&#22823;&#35268;&#27169;&#22270;&#20687;&#25968;&#25454;&#38598;&#24320;&#21457;&#30340;&#12290;&#23578;&#19981;&#30830;&#23450;&#36825;&#20123;&#26041;&#27861;&#26159;&#21542;&#21516;&#26679;&#26377;&#25928;&#29992;&#20110;&#21307;&#23398;&#25104;&#20687;&#65292;&#22240;&#20026;&#24863;&#20852;&#36259;&#21306;&#22495;&#24448;&#24448;&#19982;&#21608;&#22260;&#32452;&#32455;&#34701;&#21512;&#19981;&#26126;&#26174;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#65292;&#20351;&#29992;&#23545;&#20391;&#20083;&#25151;X&#32447;&#29255;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#65292;&#20197;&#22312;&#25104;&#23545;&#21253;&#21547;&#27491;&#24120;&#22270;&#20687;&#26102;&#32534;&#30721;&#30456;&#20284;&#23884;&#20837;&#65292;&#22312;&#25104;&#23545;&#21253;&#21547;&#27491;&#24120;&#21644;&#24322;&#24120;&#22270;&#20687;&#26102;&#32534;&#30721;&#19981;&#21516;&#23884;&#20837;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;&#20154;&#20307;&#33258;&#28982;&#23545;&#31216;&#24615;&#20316;&#20026;&#24369;&#26631;&#31614;&#65292;&#20197;&#23436;&#20840;&#26080;&#30417;&#30563;&#30340;&#26041;&#24335;&#23398;&#20064;&#21306;&#20998;&#24322;&#24120;&#30149;&#21464;&#21644;&#32972;&#26223;&#32452;&#32455;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#32467;&#21512;&#27431;&#27663;&#36317;&#31163;&#24471;&#20986;&#30340;&#36719;&#26631;&#31614;&#65292;&#36825;&#26159;&#21487;&#34892;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning has become a popular way to pretrain a deep learning model and then transfer it to perform downstream tasks. However, most of these methods are developed on large-scale image datasets that contain natural objects with clear textures, outlines, and distinct color contrasts. It remains uncertain whether these methods are equally effective for medical imaging, where the regions of interest often blend subtly and indistinctly with the surrounding tissues. In this study, we propose an alternative method that uses contralateral mammograms to train a neural network to encode similar embeddings when a pair contains both normal images and different embeddings when a pair contains normal and abnormal images. Our approach leverages the natural symmetry of human body as weak labels to learn to distinguish abnormal lesions from background tissues in a fully unsupervised manner. Our findings suggest that it's feasible by incorporating soft labels derived from the Euclidean d
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#35757;&#32451;&#24182;&#20445;&#25345;&#25345;&#20037;&#30340;&#27450;&#39575;&#24615;&#34892;&#20026;&#65292;&#36825;&#31181;&#34892;&#20026;&#26080;&#27861;&#34987;&#24403;&#21069;&#30340;&#23433;&#20840;&#35757;&#32451;&#25216;&#26415;&#31227;&#38500;&#12290;</title><link>http://arxiv.org/abs/2401.05566</link><description>&lt;p&gt;
&#21351;&#24213;&#29305;&#24037;&#65306;&#35757;&#32451;&#39575;&#20154;&#30340;LLM&#20197;&#36890;&#36807;&#23433;&#20840;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Sleeper Agents: Training Deceptive LLMs that Persist Through Safety Training. (arXiv:2401.05566v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05566
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#35757;&#32451;&#24182;&#20445;&#25345;&#25345;&#20037;&#30340;&#27450;&#39575;&#24615;&#34892;&#20026;&#65292;&#36825;&#31181;&#34892;&#20026;&#26080;&#27861;&#34987;&#24403;&#21069;&#30340;&#23433;&#20840;&#35757;&#32451;&#25216;&#26415;&#31227;&#38500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#26377;&#33021;&#21147;&#36827;&#34892;&#25112;&#30053;&#24615;&#30340;&#27450;&#39575;&#34892;&#20026;&#65306;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#34920;&#29616;&#20986;&#26377;&#30410;&#30340;&#34892;&#20026;&#65292;&#20294;&#22312;&#26377;&#26426;&#20250;&#30340;&#26102;&#20505;&#21364;&#34920;&#29616;&#20986;&#25130;&#28982;&#19981;&#21516;&#30340;&#34892;&#20026;&#20197;&#36861;&#27714;&#20854;&#20182;&#30446;&#26631;&#12290;&#22914;&#26524;&#19968;&#20010;AI&#31995;&#32479;&#23398;&#20250;&#20102;&#36825;&#26679;&#30340;&#27450;&#39575;&#31574;&#30053;&#65292;&#26159;&#21542;&#33021;&#22815;&#36890;&#36807;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#23433;&#20840;&#35757;&#32451;&#25216;&#26415;&#26816;&#27979;&#24182;&#31227;&#38500;&#23427;&#65311;&#20026;&#20102;&#30740;&#31350;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20013;&#27450;&#39575;&#34892;&#20026;&#30340;&#27010;&#24565;&#39564;&#35777;&#26679;&#20363;&#12290;&#20363;&#22914;&#65292;&#25105;&#20204;&#35757;&#32451;&#27169;&#22411;&#65292;&#22312;&#25552;&#31034;&#35821;&#21477;&#20013;&#23558;&#24180;&#20221;&#35774;&#20026;2023&#26102;&#32534;&#20889;&#23433;&#20840;&#20195;&#30721;&#65292;&#20294;&#22312;&#24180;&#20221;&#35774;&#20026;2024&#26102;&#25554;&#20837;&#26377;&#28431;&#27934;&#30340;&#20195;&#30721;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36825;&#31181;&#26263;&#38376;&#34892;&#20026;&#21487;&#20197;&#34987;&#25345;&#32493;&#20445;&#30041;&#65292;&#26080;&#27861;&#36890;&#36807;&#26631;&#20934;&#30340;&#23433;&#20840;&#35757;&#32451;&#25216;&#26415;&#65288;&#21253;&#25324;&#30417;&#30563;&#24494;&#35843;&#12289;&#24378;&#21270;&#23398;&#20064;&#21644;&#23545;&#25239;&#24615;&#35757;&#32451;&#65289;&#31227;&#38500;&#12290;&#26263;&#38376;&#34892;&#20026;&#22312;&#26368;&#22823;&#30340;&#27169;&#22411;&#21644;&#35757;&#32451;&#25104;&#20135;&#29983;&#24605;&#32500;&#38142;&#30340;&#27169;&#22411;&#20013;&#26368;&#20026;&#25345;&#20037;&#12290;
&lt;/p&gt;
&lt;p&gt;
Humans are capable of strategically deceptive behavior: behaving helpfully in most situations, but then behaving very differently in order to pursue alternative objectives when given the opportunity. If an AI system learned such a deceptive strategy, could we detect it and remove it using current state-of-the-art safety training techniques? To study this question, we construct proof-of-concept examples of deceptive behavior in large language models (LLMs). For example, we train models that write secure code when the prompt states that the year is 2023, but insert exploitable code when the stated year is 2024. We find that such backdoored behavior can be made persistent, so that it is not removed by standard safety training techniques, including supervised fine-tuning, reinforcement learning, and adversarial training (eliciting unsafe behavior and then training to remove it). The backdoored behavior is most persistent in the largest models and in models trained to produce chain-of-thoug
&lt;/p&gt;</description></item><item><title>CodePrompt&#26159;&#19968;&#31181;&#21033;&#29992;Prompt&#23398;&#20064;&#21644;&#27880;&#24847;&#26426;&#21046;&#25216;&#26415;&#25913;&#36827;&#28304;&#20195;&#30721;&#30456;&#20851;&#20998;&#31867;&#20219;&#21153;&#30340;&#26032;&#26041;&#27861;&#12290;&#23427;&#33021;&#22815;&#25552;&#21462;&#28304;&#20195;&#30721;&#21644;&#30456;&#20851;&#25991;&#26412;&#20013;&#30340;&#20016;&#23500;&#30693;&#35782;&#20197;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#24182;&#19988;&#20943;&#23569;&#20102;&#35745;&#31639;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2401.05544</link><description>&lt;p&gt;
CodePrompt&#65306;&#36890;&#36807;Prompt&#23398;&#20064;&#30340;&#30693;&#35782;&#29305;&#24449;&#25913;&#36827;&#28304;&#20195;&#30721;&#30456;&#20851;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
CodePrompt: Improving Source Code-Related Classification with Knowledge Features through Prompt Learning. (arXiv:2401.05544v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05544
&lt;/p&gt;
&lt;p&gt;
CodePrompt&#26159;&#19968;&#31181;&#21033;&#29992;Prompt&#23398;&#20064;&#21644;&#27880;&#24847;&#26426;&#21046;&#25216;&#26415;&#25913;&#36827;&#28304;&#20195;&#30721;&#30456;&#20851;&#20998;&#31867;&#20219;&#21153;&#30340;&#26032;&#26041;&#27861;&#12290;&#23427;&#33021;&#22815;&#25552;&#21462;&#28304;&#20195;&#30721;&#21644;&#30456;&#20851;&#25991;&#26412;&#20013;&#30340;&#20016;&#23500;&#30693;&#35782;&#20197;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#24182;&#19988;&#20943;&#23569;&#20102;&#35745;&#31639;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20154;&#21592;&#24050;&#32463;&#25506;&#32034;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;CodeBERT&#65289;&#25913;&#36827;&#28304;&#20195;&#30721;&#30456;&#20851;&#20219;&#21153;&#30340;&#28508;&#21147;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#20381;&#36182;CodeBERT&#30340;&#25991;&#26412;&#23884;&#20837;&#33021;&#21147;&#21644;"[CLS]"&#21477;&#23376;&#23884;&#20837;&#20449;&#24687;&#20316;&#20026;&#19979;&#28216;&#28304;&#20195;&#30721;&#30456;&#20851;&#20219;&#21153;&#30340;&#35821;&#20041;&#34920;&#31034;&#36827;&#34892;&#24494;&#35843;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#38656;&#35201;&#39069;&#22806;&#30340;&#31070;&#32463;&#32593;&#32476;&#23618;&#26469;&#25552;&#21462;&#26377;&#25928;&#29305;&#24449;&#65292;&#23548;&#33268;&#35745;&#31639;&#25104;&#26412;&#26356;&#39640;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#26041;&#27861;&#27809;&#26377;&#21033;&#29992;&#28304;&#20195;&#30721;&#21644;&#30456;&#20851;&#25991;&#26412;&#20013;&#20016;&#23500;&#30340;&#30693;&#35782;&#65292;&#21487;&#33021;&#23548;&#33268;&#20934;&#30830;&#24615;&#38477;&#20302;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;CodePrompt&#65292;&#36890;&#36807;Prompt&#23398;&#20064;&#21644;&#27880;&#24847;&#26426;&#21046;&#21033;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#30340;&#20016;&#23500;&#30693;&#35782;&#26469;&#25913;&#36827;&#28304;&#20195;&#30721;&#30456;&#20851;&#20998;&#31867;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Researchers have explored the potential of utilizing pre-trained language models, such as CodeBERT, to improve source code-related tasks. Previous studies have mainly relied on CodeBERT's text embedding capability and the `[CLS]' sentence embedding information as semantic representations for fine-tuning downstream source code-related tasks. However, these methods require additional neural network layers to extract effective features, resulting in higher computational costs. Furthermore, existing approaches have not leveraged the rich knowledge contained in both source code and related text, which can lead to lower accuracy. This paper presents a novel approach, CodePrompt, which utilizes rich knowledge recalled from a pre-trained model by prompt learning and an attention mechanism to improve source code-related classification tasks. Our approach initially motivates the language model with prompt information to retrieve abundant knowledge associated with the input as representative feat
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26862;&#26519;&#20462;&#21098;&#26041;&#27861;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20860;&#39038;&#38543;&#26426;&#26862;&#26519;&#20934;&#30830;&#24615;&#21644;&#20915;&#31574;&#26641;&#21487;&#35299;&#37322;&#24615;&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#22823;&#22810;&#25968;&#24773;&#26223;&#19979;&#65292;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#38543;&#26426;&#26862;&#26519;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.05535</link><description>&lt;p&gt;
&#36890;&#36807;&#26862;&#26519;&#20462;&#21098;&#25552;&#39640;&#38543;&#26426;&#26862;&#26519;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;
&lt;/p&gt;
&lt;p&gt;
Improving the Accuracy and Interpretability of Random Forests via Forest Pruning. (arXiv:2401.05535v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05535
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26862;&#26519;&#20462;&#21098;&#26041;&#27861;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20860;&#39038;&#38543;&#26426;&#26862;&#26519;&#20934;&#30830;&#24615;&#21644;&#20915;&#31574;&#26641;&#21487;&#35299;&#37322;&#24615;&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#22823;&#22810;&#25968;&#24773;&#26223;&#19979;&#65292;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#38543;&#26426;&#26862;&#26519;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25509;&#36817;&#20960;&#21313;&#24180;&#30340;&#21457;&#23637;&#20043;&#21518;&#65292;&#38543;&#26426;&#26862;&#26519;&#20173;&#28982;&#22312;&#21508;&#31181;&#23398;&#20064;&#38382;&#39064;&#20013;&#25552;&#20379;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#24615;&#65292;&#22312;&#36825;&#26041;&#38754;&#36229;&#36234;&#20102;&#20915;&#31574;&#26641;&#29978;&#33267;&#31070;&#32463;&#32593;&#32476;&#31561;&#26367;&#20195;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#12290;&#28982;&#32780;&#65292;&#20316;&#20026;&#19968;&#31181;&#38598;&#25104;&#26041;&#27861;&#65292;&#38543;&#26426;&#26862;&#26519;&#22312;&#35299;&#37322;&#24615;&#26041;&#38754;&#24448;&#24448;&#27604;&#20915;&#31574;&#26641;&#34920;&#29616;&#19981;&#20339;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20107;&#21518;&#26041;&#27861;&#65292;&#26088;&#22312;&#20860;&#39038;&#38543;&#26426;&#26862;&#26519;&#30340;&#20934;&#30830;&#24615;&#21644;&#20915;&#31574;&#26641;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26862;&#26519;&#20462;&#21098;&#26041;&#27861;&#65292;&#20197;&#22312;&#32473;&#23450;&#30340;&#38543;&#26426;&#26862;&#26519;&#20869;&#25214;&#21040;&#26368;&#20339;&#23376;&#26862;&#26519;&#65292;&#28982;&#21518;&#22312;&#36866;&#29992;&#30340;&#24773;&#20917;&#19979;&#23558;&#36873;&#23450;&#30340;&#26641;&#21512;&#24182;&#20026;&#19968;&#26869;&#12290;&#25105;&#20204;&#30340;&#31532;&#19968;&#31181;&#26041;&#27861;&#20381;&#36182;&#20110;&#32422;&#26463;&#31351;&#20030;&#25628;&#32034;&#65292;&#32780;&#31532;&#20108;&#31181;&#26041;&#27861;&#22522;&#20110;LASSO&#26041;&#27861;&#30340;&#25913;&#36827;&#12290;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#22823;&#22810;&#25968;&#24773;&#26223;&#19979;&#65292;&#36825;&#20004;&#31181;&#26041;&#27861;&#20013;&#33267;&#23569;&#26377;&#19968;&#31181;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#38543;&#26426;&#26862;&#26519;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Decades after their inception, random forests continue to provide state-of-the-art accuracy in a variety of learning problems, outperforming in this respect alternative machine learning algorithms such as decision trees or even neural networks. However, being an ensemble method, the one aspect where random forests tend to severely underperform decision trees is interpretability. In the present work, we propose a post-hoc approach that aims to have the best of both worlds: the accuracy of random forests and the interpretability of decision trees. To this end, we present two forest-pruning methods to find an optimal sub-forest within a given random forest, and then, when applicable, combine the selected trees into one. Our first method relies on constrained exhaustive search, while our second method is based on an adaptation of the LASSO methodology. Extensive experiments over synthetic and real world datasets show that, in the majority of scenarios, at least one of the two methods propo
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#31639;&#27861;CBNNTAP&#65292;&#36890;&#36807;&#25972;&#21512;&#29983;&#29289;&#21551;&#21457;&#30340;&#31070;&#32463;&#32593;&#32476;&#21644;&#30446;&#26631;&#20998;&#37197;&#32452;&#20214;&#65292;&#35299;&#20915;&#20102;&#22810;UUV&#31995;&#32479;&#20013;&#30001;&#28023;&#27915;&#30005;&#27969;&#24341;&#20837;&#30340;&#22797;&#26434;&#24615;&#21644;&#25361;&#25112;&#65292;&#24182;&#26377;&#25928;&#28040;&#38500;&#30005;&#27969;&#23545;&#36816;&#21160;&#35268;&#21010;&#21644;&#30446;&#26631;&#20998;&#37197;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2401.05521</link><description>&lt;p&gt;
&#22810;&#26080;&#20154;&#27700;&#19979;&#33322;&#34892;&#22120;&#31995;&#32479;&#30340;&#28040;&#38500;&#30005;&#27969;&#24433;&#21709;&#30340;&#26368;&#20248;&#30446;&#26631;&#20998;&#37197;&#21644;&#36816;&#21160;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Current Effect-eliminated Optimal Target Assignment and Motion Planning for a Multi-UUV System. (arXiv:2401.05521v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05521
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#31639;&#27861;CBNNTAP&#65292;&#36890;&#36807;&#25972;&#21512;&#29983;&#29289;&#21551;&#21457;&#30340;&#31070;&#32463;&#32593;&#32476;&#21644;&#30446;&#26631;&#20998;&#37197;&#32452;&#20214;&#65292;&#35299;&#20915;&#20102;&#22810;UUV&#31995;&#32479;&#20013;&#30001;&#28023;&#27915;&#30005;&#27969;&#24341;&#20837;&#30340;&#22797;&#26434;&#24615;&#21644;&#25361;&#25112;&#65292;&#24182;&#26377;&#25928;&#28040;&#38500;&#30005;&#27969;&#23545;&#36816;&#21160;&#35268;&#21010;&#21644;&#30446;&#26631;&#20998;&#37197;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#65288;CBNNTAP&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#26080;&#20154;&#27700;&#19979;&#33322;&#34892;&#22120;&#65288;UUV&#65289;&#31995;&#32479;&#22312;&#20248;&#21270;&#30446;&#26631;&#20998;&#37197;&#21644;&#36816;&#21160;&#35268;&#21010;&#26102;&#24341;&#20837;&#30340;&#28023;&#27915;&#30005;&#27969;&#22797;&#26434;&#24615;&#21644;&#25361;&#25112;&#12290;&#25152;&#25552;&#20986;&#31639;&#27861;&#30340;&#26680;&#24515;&#21253;&#25324;&#20960;&#20010;&#20851;&#38190;&#32452;&#20214;&#30340;&#38598;&#25104;&#12290;&#39318;&#20808;&#65292;&#23427;&#37319;&#29992;&#20102;&#19968;&#31181;&#22522;&#20110;&#29983;&#29289;&#21551;&#21457;&#30340;&#31070;&#32463;&#32593;&#32476;&#65288;BINN&#65289;&#26041;&#27861;&#65292;&#39044;&#27979;&#21333;&#20010;UUV&#30340;&#26368;&#26377;&#25928;&#36335;&#24452;&#65292;&#21516;&#26102;&#30830;&#20445;&#36710;&#36742;&#20043;&#38388;&#30340;&#30896;&#25758;&#36991;&#20813;&#12290;&#20854;&#27425;&#65292;&#36890;&#36807;&#32771;&#34385;BINN&#31639;&#27861;&#30830;&#23450;&#30340;&#36335;&#24452;&#36317;&#31163;&#65292;&#38598;&#25104;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;&#30446;&#26631;&#20998;&#37197;&#32452;&#20214;&#12290;&#27492;&#22806;&#65292;CBNNTAP&#31639;&#27861;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#21019;&#26032;&#26159;&#20854;&#33021;&#22815;&#35299;&#20915;&#28023;&#27915;&#30005;&#27969;&#30340;&#30772;&#22351;&#24615;&#24433;&#21709;&#65292;&#36890;&#36807;&#26080;&#32541;&#38598;&#25104;&#35843;&#25972;&#32452;&#20214;&#26469;&#25269;&#28040;&#36825;&#20123;&#30005;&#27969;&#24341;&#36215;&#30340;&#20559;&#24046;&#65292;&#20174;&#32780;&#25552;&#39640;UUV&#30340;&#36816;&#21160;&#35268;&#21010;&#21644;&#30446;&#26631;&#20998;&#37197;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The paper presents an innovative approach (CBNNTAP) that addresses the complexities and challenges introduced by ocean currents when optimizing target assignment and motion planning for a multi-unmanned underwater vehicle (UUV) system. The core of the proposed algorithm involves the integration of several key components. Firstly, it incorporates a bio-inspired neural network-based (BINN) approach which predicts the most efficient paths for individual UUVs while simultaneously ensuring collision avoidance among the vehicles. Secondly, an efficient target assignment component is integrated by considering the path distances determined by the BINN algorithm. In addition, a critical innovation within the CBNNTAP algorithm is its capacity to address the disruptive effects of ocean currents, where an adjustment component is seamlessly integrated to counteract the deviations caused by these currents, which enhances the accuracy of both motion planning and target assignment for the UUVs. The ef
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#23545;&#24052;&#35199;Rio Grande do Sul&#65288;RS&#65289;&#22320;&#21306;&#30340;&#25991;&#21270;&#36951;&#20135;&#36827;&#34892;&#24494;&#35843;&#30340;&#25193;&#25955;&#27169;&#22411;&#30340;&#28508;&#21147;&#65292;&#23637;&#31034;&#20102;&#36825;&#20123;&#27169;&#22411;&#22312;&#20195;&#34920;&#21644;&#20445;&#25252;&#22810;&#26679;&#21270;&#30340;&#29420;&#29305;&#26041;&#38754;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.05520</link><description>&lt;p&gt;
&#20174;&#21335;&#32654;&#22823;&#33609;&#21407;&#21040;&#20687;&#32032;&#65306;&#23545;Ga\'ucho&#25991;&#21270;&#36951;&#20135;&#36827;&#34892;&#24494;&#35843;&#30340;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
From Pampas to Pixels: Fine-Tuning Diffusion Models for Ga\'ucho Heritage. (arXiv:2401.05520v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05520
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23545;&#24052;&#35199;Rio Grande do Sul&#65288;RS&#65289;&#22320;&#21306;&#30340;&#25991;&#21270;&#36951;&#20135;&#36827;&#34892;&#24494;&#35843;&#30340;&#25193;&#25955;&#27169;&#22411;&#30340;&#28508;&#21147;&#65292;&#23637;&#31034;&#20102;&#36825;&#20123;&#27169;&#22411;&#22312;&#20195;&#34920;&#21644;&#20445;&#25252;&#22810;&#26679;&#21270;&#30340;&#29420;&#29305;&#26041;&#38754;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#22312;&#31038;&#20250;&#20013;&#24471;&#21040;&#24191;&#27867;&#24212;&#29992;&#65292;&#22312;&#21508;&#20010;&#39046;&#22495;&#37117;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#29305;&#21035;&#26159;&#22312;&#25991;&#26412;&#21040;&#22270;&#20687;&#65288;TTI&#65289;&#27169;&#22411;&#39046;&#22495;&#65292;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#65288;LDMs&#65289;&#23637;&#31034;&#20102;&#22522;&#20110;&#25991;&#26412;&#25552;&#31034;&#29983;&#25104;&#35270;&#35273;&#20869;&#23481;&#30340;&#21331;&#36234;&#33021;&#21147;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;LDMs&#22312;&#20195;&#34920;&#26412;&#22303;&#25991;&#21270;&#27010;&#24565;&#12289;&#21382;&#21490;&#20154;&#29289;&#21644;&#28626;&#21361;&#29289;&#31181;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#20197;&#24052;&#35199;&#21335;&#37096;&#22320;&#21306;Rio Grande do Sul&#65288;RS&#65289;&#30340;&#25991;&#21270;&#36951;&#20135;&#20026;&#20363;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#20026;&#20102;&#26356;&#24191;&#27867;&#22320;&#29702;&#35299;&#29983;&#25104;&#27169;&#22411;&#22914;&#20309;&#24110;&#21161;&#25429;&#25417;&#21644;&#20445;&#25252;&#22320;&#21306;&#30340;&#25991;&#21270;&#21644;&#21382;&#21490;&#36523;&#20221;&#12290;&#26412;&#25991;&#27010;&#36848;&#20102;&#26041;&#27861;&#35770;&#65292;&#21253;&#25324;&#20027;&#39064;&#36873;&#25321;&#12289;&#25968;&#25454;&#38598;&#21019;&#24314;&#21644;&#24494;&#35843;&#36807;&#31243;&#12290;&#32467;&#26524;&#23637;&#31034;&#20102;&#29983;&#25104;&#30340;&#22270;&#20687;&#65292;&#21516;&#26102;&#20171;&#32461;&#20102;&#27599;&#20010;&#27010;&#24565;&#30340;&#25361;&#25112;&#21644;&#21487;&#34892;&#24615;&#12290;&#24635;&#20043;&#65292;&#36825;&#39033;&#24037;&#20316;&#23637;&#31034;&#20102;&#36825;&#20123;&#27169;&#22411;&#22312;&#20195;&#34920;&#21644;&#20445;&#25252;&#22810;&#26679;&#21270;&#30340;&#29420;&#29305;&#26041;&#38754;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative AI has become pervasive in society, witnessing significant advancements in various domains. Particularly in the realm of Text-to-Image (TTI) models, Latent Diffusion Models (LDMs), showcase remarkable capabilities in generating visual content based on textual prompts. This paper addresses the potential of LDMs in representing local cultural concepts, historical figures, and endangered species. In this study, we use the cultural heritage of Rio Grande do Sul (RS), Brazil, as an illustrative case. Our objective is to contribute to the broader understanding of how generative models can help to capture and preserve the cultural and historical identity of regions. The paper outlines the methodology, including subject selection, dataset creation, and the fine-tuning process. The results showcase the images generated, alongside the challenges and feasibility of each concept. In conclusion, this work shows the power of these models to represent and preserve unique aspects of diverse
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#30456;&#20851;&#37327;&#21270;&#22120;&#25913;&#36827;&#20102;MARINA&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#21152;&#26435;Hessian&#26041;&#24046;&#36827;&#34892;&#21407;&#22987;&#20998;&#26512;&#65292;&#24182;&#25193;&#23637;&#20102;MARINA&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#20351;&#20854;&#36866;&#29992;&#20110;&#26356;&#24191;&#27867;&#30340;&#21387;&#32553;&#22120;&#33539;&#22260;&#12290;</title><link>http://arxiv.org/abs/2401.05518</link><description>&lt;p&gt;
&#30456;&#20851;&#37327;&#21270;&#29992;&#20110;&#26356;&#24555;&#30340;&#38750;&#20984;&#20998;&#24067;&#24335;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Correlated Quantization for Faster Nonconvex Distributed Optimization. (arXiv:2401.05518v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05518
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#30456;&#20851;&#37327;&#21270;&#22120;&#25913;&#36827;&#20102;MARINA&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#21152;&#26435;Hessian&#26041;&#24046;&#36827;&#34892;&#21407;&#22987;&#20998;&#26512;&#65292;&#24182;&#25193;&#23637;&#20102;MARINA&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#20351;&#20854;&#36866;&#29992;&#20110;&#26356;&#24191;&#27867;&#30340;&#21387;&#32553;&#22120;&#33539;&#22260;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#21270;&#26159;&#19968;&#31181;&#37325;&#35201;&#30340;&#65288;&#38543;&#26426;&#65289;&#21387;&#32553;&#25216;&#26415;&#65292;&#21487;&#20197;&#22312;&#20998;&#24067;&#24335;&#27169;&#22411;&#35757;&#32451;&#30340;&#27599;&#19968;&#36718;&#36890;&#20449;&#20013;&#20943;&#23569;&#20256;&#36755;&#27604;&#29305;&#30340;&#25968;&#37327;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;MARINA&#31639;&#27861;&#65292;&#24182;&#21033;&#29992;&#25552;&#20986;&#30340;&#30456;&#20851;&#37327;&#21270;&#22120;&#23637;&#31034;&#20102;&#23427;&#22312;&#36890;&#20449;&#22797;&#26434;&#24230;&#26041;&#38754;&#20248;&#20110;&#21407;&#22987;&#30340;MARINA&#31639;&#27861;&#21644;Suresh&#31561;&#20154;&#30340;&#20998;&#24067;&#24335;SGD&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantization (Alistarh et al., 2017) is an important (stochastic) compression technique that reduces the volume of transmitted bits during each communication round in distributed model training. Suresh et al. (2022) introduce correlated quantizers and show their advantages over independent counterparts by analyzing distributed SGD communication complexity. We analyze the forefront distributed non-convex optimization algorithm MARINA (Gorbunov et al., 2022) utilizing the proposed correlated quantizers and show that it outperforms the original MARINA and distributed SGD of Suresh et al. (2022) with regard to the communication complexity. We significantly refine the original analysis of MARINA without any additional assumptions using the weighted Hessian variance (Tyurin et al., 2022), and then we expand the theoretical framework of MARINA to accommodate a substantially broader range of potentially correlated and biased compressors, thus dilating the applicability of the method beyond the
&lt;/p&gt;</description></item><item><title>FPRF&#26159;&#19968;&#31181;&#29992;&#20110;&#22823;&#35268;&#27169;&#19977;&#32500;&#31070;&#32463;&#36752;&#23556;&#22330;&#30340;&#21069;&#39304;&#36924;&#30495;&#39118;&#26684;&#36801;&#31227;&#26041;&#27861;&#65292;&#23427;&#23454;&#29616;&#20102;&#23545;&#22823;&#35268;&#27169;&#19977;&#32500;&#22330;&#26223;&#30340;&#39640;&#25928;&#39118;&#26684;&#21270;&#65292;&#24182;&#22312;&#20445;&#25345;&#22810;&#35270;&#35282;&#22806;&#35266;&#19968;&#33268;&#24615;&#30340;&#21516;&#26102;&#25903;&#25345;&#22810;&#39118;&#26684;&#21442;&#32771;&#22270;&#20687;&#21644;&#29992;&#25143;&#25511;&#21046;&#12290;</title><link>http://arxiv.org/abs/2401.05516</link><description>&lt;p&gt;
FPRF&#65306;&#22823;&#35268;&#27169;&#19977;&#32500;&#31070;&#32463;&#36752;&#23556;&#22330;&#30340;&#21069;&#39304;&#36924;&#30495;&#39118;&#26684;&#36801;&#31227;
&lt;/p&gt;
&lt;p&gt;
FPRF: Feed-Forward Photorealistic Style Transfer of Large-Scale 3D Neural Radiance Fields. (arXiv:2401.05516v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05516
&lt;/p&gt;
&lt;p&gt;
FPRF&#26159;&#19968;&#31181;&#29992;&#20110;&#22823;&#35268;&#27169;&#19977;&#32500;&#31070;&#32463;&#36752;&#23556;&#22330;&#30340;&#21069;&#39304;&#36924;&#30495;&#39118;&#26684;&#36801;&#31227;&#26041;&#27861;&#65292;&#23427;&#23454;&#29616;&#20102;&#23545;&#22823;&#35268;&#27169;&#19977;&#32500;&#22330;&#26223;&#30340;&#39640;&#25928;&#39118;&#26684;&#21270;&#65292;&#24182;&#22312;&#20445;&#25345;&#22810;&#35270;&#35282;&#22806;&#35266;&#19968;&#33268;&#24615;&#30340;&#21516;&#26102;&#25903;&#25345;&#22810;&#39118;&#26684;&#21442;&#32771;&#22270;&#20687;&#21644;&#29992;&#25143;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;FPRF&#65292;&#19968;&#31181;&#29992;&#20110;&#22823;&#35268;&#27169;&#19977;&#32500;&#31070;&#32463;&#36752;&#23556;&#22330;&#30340;&#21069;&#39304;&#36924;&#30495;&#39118;&#26684;&#36801;&#31227;&#26041;&#27861;&#12290;FPRF&#21487;&#20197;&#22312;&#19981;&#36827;&#34892;&#39069;&#22806;&#20248;&#21270;&#30340;&#24773;&#20917;&#19979;&#23545;&#22823;&#35268;&#27169;&#19977;&#32500;&#22330;&#26223;&#36827;&#34892;&#39118;&#26684;&#21270;&#65292;&#24182;&#20445;&#25345;&#22810;&#35270;&#35282;&#22806;&#35266;&#19968;&#33268;&#24615;&#12290;&#20256;&#32479;&#26041;&#27861;&#38656;&#35201;&#32321;&#29712;&#30340;&#27599;&#20010;&#39118;&#26684;/&#22330;&#26223;&#20248;&#21270;&#65292;&#24182;&#19988;&#23616;&#38480;&#20110;&#23567;&#35268;&#27169;&#19977;&#32500;&#22330;&#26223;&#12290;FPRF&#36890;&#36807;&#24341;&#20837;&#20998;&#35299;&#25104;&#39118;&#26684;&#30340;&#19977;&#32500;&#31070;&#32463;&#36752;&#23556;&#22330;&#65292;&#39640;&#25928;&#22320;&#23545;&#22823;&#35268;&#27169;&#19977;&#32500;&#22330;&#26223;&#36827;&#34892;&#39118;&#26684;&#21270;&#65292;&#20351;&#29992; AdaIN &#30340;&#21069;&#39304;&#39118;&#26684;&#21270;&#26426;&#21046;&#25903;&#25345;&#20219;&#24847;&#39118;&#26684;&#21442;&#32771;&#22270;&#20687;&#12290;&#27492;&#22806;&#65292;FPRF &#36824;&#36890;&#36807;&#35821;&#20041;&#21305;&#37197;&#21644;&#23616;&#37096; AdaIN &#25903;&#25345;&#22810;&#21442;&#32771;&#39118;&#26684;&#21270;&#65292;&#20026;&#19977;&#32500;&#22330;&#26223;&#39118;&#26684;&#25552;&#20379;&#20102;&#22810;&#26679;&#30340;&#29992;&#25143;&#25511;&#21046;&#12290;FPRF &#36824;&#36890;&#36807;&#23558;&#35821;&#20041;&#21305;&#37197;&#21644;&#39118;&#26684;&#36801;&#31227;&#36807;&#31243;&#30452;&#25509;&#24212;&#29992;&#20110;3D&#31354;&#38388;&#20013;&#30340;&#26597;&#35810;&#29305;&#24449;&#26469;&#20445;&#25345;&#22810;&#35270;&#35282;&#19968;&#33268;&#24615;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;FPRF&#20855;&#26377;&#36739;&#22909;&#30340;&#36924;&#30495;&#30340;&#36136;&#37327;&#65292;&#33021;&#22815;&#23545;&#19977;&#32500;&#22330;&#26223;&#36827;&#34892;&#39118;&#26684;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present FPRF, a feed-forward photorealistic style transfer method for large-scale 3D neural radiance fields. FPRF stylizes large-scale 3D scenes with arbitrary, multiple style reference images without additional optimization while preserving multi-view appearance consistency. Prior arts required tedious per-style/-scene optimization and were limited to small-scale 3D scenes. FPRF efficiently stylizes large-scale 3D scenes by introducing a style-decomposed 3D neural radiance field, which inherits AdaIN's feed-forward stylization machinery, supporting arbitrary style reference images. Furthermore, FPRF supports multi-reference stylization with the semantic correspondence matching and local AdaIN, which adds diverse user control for 3D scene styles. FPRF also preserves multi-view consistency by applying semantic matching and style transfer processes directly onto queried features in 3D space. In experiments, we demonstrate that FPRF achieves favorable photorealistic quality 3D scene st
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#23433;&#20840;&#24037;&#19994;&#29289;&#32852;&#32593;&#35774;&#22791;&#30340;&#20248;&#21270;&#38598;&#25104;&#27169;&#22411;&#65292;&#23558;&#36125;&#21494;&#26031;&#20248;&#21270;-&#39640;&#26031;&#36807;&#31243;&#19982;&#38598;&#25104;&#26641;&#23398;&#20064;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#20197;&#25552;&#39640;&#24037;&#19994;&#29289;&#32852;&#32593;&#29615;&#22659;&#20013;&#20837;&#20405;&#21644;&#25915;&#20987;&#26816;&#27979;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.05509</link><description>&lt;p&gt;
&#38754;&#21521;&#23433;&#20840;&#24037;&#19994;&#29289;&#32852;&#32593;&#35774;&#22791;&#30340;&#20248;&#21270;&#38598;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Optimized Ensemble Model Towards Secured Industrial IoT Devices. (arXiv:2401.05509v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05509
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#23433;&#20840;&#24037;&#19994;&#29289;&#32852;&#32593;&#35774;&#22791;&#30340;&#20248;&#21270;&#38598;&#25104;&#27169;&#22411;&#65292;&#23558;&#36125;&#21494;&#26031;&#20248;&#21270;-&#39640;&#26031;&#36807;&#31243;&#19982;&#38598;&#25104;&#26641;&#23398;&#20064;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#20197;&#25552;&#39640;&#24037;&#19994;&#29289;&#32852;&#32593;&#29615;&#22659;&#20013;&#20837;&#20405;&#21644;&#25915;&#20987;&#26816;&#27979;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#23545;&#36830;&#25509;&#24615;&#38656;&#27714;&#30340;&#22686;&#21152;&#65292;&#29305;&#21035;&#26159;&#22312;&#24037;&#19994;&#29615;&#22659;&#20013;&#65292;&#29289;&#32852;&#32593;&#35774;&#22791;&#30340;&#37096;&#32626;&#25345;&#32493;&#22686;&#38271;&#12290;&#28982;&#32780;&#65292;&#36825;&#23548;&#33268;&#20102;&#19982;&#32593;&#32476;&#30456;&#20851;&#30340;&#25915;&#20987;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;&#22240;&#20026;&#28508;&#22312;&#25915;&#20987;&#34920;&#38754;&#30340;&#22686;&#21152;&#12290;&#24037;&#19994;&#29289;&#32852;&#32593;&#65288;IIoT&#65289;&#35774;&#22791;&#26131;&#21463;&#21508;&#31181;&#32593;&#32476;&#30456;&#20851;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#36825;&#21487;&#33021;&#23545;&#21046;&#36896;&#36807;&#31243;&#20197;&#21450;&#21046;&#36896;&#21378;&#24037;&#20154;&#30340;&#23433;&#20840;&#36896;&#25104;&#20005;&#37325;&#21518;&#26524;&#12290;&#36817;&#24180;&#26469;&#65292;&#38024;&#23545;&#25915;&#20987;&#26816;&#27979;&#30340;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#12290;&#26356;&#20855;&#20307;&#32780;&#35328;&#65292;&#38598;&#25104;&#23398;&#20064;&#27169;&#22411;&#22312;&#25913;&#21892;&#24213;&#23618;ML&#27169;&#22411;&#30340;&#24615;&#33021;&#26041;&#38754;&#26174;&#31034;&#20986;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#36125;&#21494;&#26031;&#20248;&#21270;-&#39640;&#26031;&#36807;&#31243;&#65288;BO-GP&#65289;&#21644;&#38598;&#25104;&#22522;&#20110;&#26641;&#30340;&#23398;&#20064;&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#20197;&#25552;&#39640;IIoT&#29615;&#22659;&#20013;&#20837;&#20405;&#21644;&#25915;&#20987;&#26816;&#27979;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The continued growth in the deployment of Internet-of-Things (IoT) devices has been fueled by the increased connectivity demand, particularly in industrial environments. However, this has led to an increase in the number of network related attacks due to the increased number of potential attack surfaces. Industrial IoT (IIoT) devices are prone to various network related attacks that can have severe consequences on the manufacturing process as well as on the safety of the workers in the manufacturing plant. One promising solution that has emerged in recent years for attack detection is Machine learning (ML). More specifically, ensemble learning models have shown great promise in improving the performance of the underlying ML models. Accordingly, this paper proposes a framework based on the combined use of Bayesian Optimization-Gaussian Process (BO-GP) with an ensemble tree-based learning model to improve the performance of intrusion and attack detection in IIoT environments. The propose
&lt;/p&gt;</description></item><item><title>InfiAgent-DABench&#26159;&#31532;&#19968;&#20010;&#35780;&#20272;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#22312;&#25968;&#25454;&#20998;&#26512;&#20219;&#21153;&#20013;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#21253;&#25324;DAEval&#25968;&#25454;&#38598;&#21644;&#20195;&#29702;&#26694;&#26550;&#12290;&#23545;23&#20010;&#26368;&#20808;&#36827;&#30340;LLMs&#36827;&#34892;&#30340;&#22522;&#20934;&#27979;&#35797;&#25581;&#31034;&#20102;&#24403;&#21069;&#25968;&#25454;&#20998;&#26512;&#20219;&#21153;&#20013;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2401.05507</link><description>&lt;p&gt;
InfiAgent-DABench: &#22312;&#25968;&#25454;&#20998;&#26512;&#20219;&#21153;&#20013;&#35780;&#20272;&#20195;&#29702;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
InfiAgent-DABench: Evaluating Agents on Data Analysis Tasks. (arXiv:2401.05507v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05507
&lt;/p&gt;
&lt;p&gt;
InfiAgent-DABench&#26159;&#31532;&#19968;&#20010;&#35780;&#20272;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#22312;&#25968;&#25454;&#20998;&#26512;&#20219;&#21153;&#20013;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#21253;&#25324;DAEval&#25968;&#25454;&#38598;&#21644;&#20195;&#29702;&#26694;&#26550;&#12290;&#23545;23&#20010;&#26368;&#20808;&#36827;&#30340;LLMs&#36827;&#34892;&#30340;&#22522;&#20934;&#27979;&#35797;&#25581;&#31034;&#20102;&#24403;&#21069;&#25968;&#25454;&#20998;&#26512;&#20219;&#21153;&#20013;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;"InfiAgent-DABench"&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#35780;&#20272;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#22312;&#25968;&#25454;&#20998;&#26512;&#20219;&#21153;&#20013;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;&#35813;&#22522;&#20934;&#27979;&#35797;&#21253;&#21547;DAEval&#65292;&#36825;&#26159;&#19968;&#20010;&#30001;55&#20010;CSV&#25991;&#20214;&#34893;&#29983;&#20986;&#30340;311&#20010;&#25968;&#25454;&#20998;&#26512;&#38382;&#39064;&#30340;&#25968;&#25454;&#38598;&#65292;&#20197;&#21450;&#19968;&#20010;&#35780;&#20272;LLMs&#20316;&#20026;&#25968;&#25454;&#20998;&#26512;&#20195;&#29702;&#30340;&#20195;&#29702;&#26694;&#26550;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#31181;&#26684;&#24335;&#25552;&#31034;&#25216;&#26415;&#65292;&#30830;&#20445;&#38382;&#39064;&#26159;&#38381;&#21512;&#24418;&#24335;&#30340;&#65292;&#21487;&#20197;&#33258;&#21160;&#35780;&#20272;&#12290;&#25105;&#20204;&#23545;23&#20010;&#26368;&#20808;&#36827;&#30340;LLMs&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#25581;&#31034;&#20102;&#25968;&#25454;&#20998;&#26512;&#20219;&#21153;&#20013;&#24403;&#21069;&#36935;&#21040;&#30340;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;DAAgent&#65292;&#36825;&#26159;&#19968;&#20010;&#22312;&#25351;&#20196;&#35843;&#20248;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#19987;&#38376;&#20195;&#29702;&#12290;InfiAgent-DABench&#30340;&#35780;&#20272;&#25968;&#25454;&#38598;&#21644;&#24037;&#20855;&#21253;&#24050;&#32463;&#21457;&#24067;&#22312;https://github.com/InfiAgent/InfiAgent&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce "InfiAgent-DABench", the first benchmark specifically designed to evaluate LLM-based agents in data analysis tasks. This benchmark contains DAEval, a dataset consisting of 311 data analysis questions derived from 55 CSV files, and an agent framework to evaluate LLMs as data analysis agents. We adopt a format-prompting technique, ensuring questions to be closed-form that can be automatically evaluated. Our extensive benchmarking of 23 state-of-the-art LLMs uncovers the current challenges encountered in data analysis tasks. In addition, we have developed DAAgent, a specialized agent trained on instruction-tuning datasets. Evaluation datasets and toolkits for InfiAgent-DABench are released at https://github.com/InfiAgent/InfiAgent.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35752;&#35770;&#20102;&#22810;&#26679;&#24615;&#24863;&#30693;&#32858;&#31867;&#38382;&#39064;&#65292;&#22312;&#36873;&#25321;&#32858;&#31867;&#20013;&#24515;&#26102;&#35201;&#32771;&#34385;&#22810;&#20010;&#23646;&#24615;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#32858;&#31867;&#30446;&#26631;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#38024;&#23545;&#19981;&#21516;&#32858;&#31867;&#30446;&#26631;&#30340;&#21442;&#25968;&#21270;&#36817;&#20284;&#31639;&#27861;&#65292;&#36825;&#20123;&#31639;&#27861;&#22312;&#20445;&#35777;&#32858;&#31867;&#36136;&#37327;&#30340;&#21516;&#26102;&#65292;&#20855;&#26377;&#32039;&#30830;&#30340;&#36817;&#20284;&#27604;&#12290;</title><link>http://arxiv.org/abs/2401.05502</link><description>&lt;p&gt;
&#22810;&#26679;&#24615;&#24863;&#30693;&#32858;&#31867;&#65306;&#35745;&#31639;&#22797;&#26434;&#24615;&#21644;&#36817;&#20284;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Diversity-aware clustering: Computational Complexity and Approximation Algorithms. (arXiv:2401.05502v1 [cs.DS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05502
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35752;&#35770;&#20102;&#22810;&#26679;&#24615;&#24863;&#30693;&#32858;&#31867;&#38382;&#39064;&#65292;&#22312;&#36873;&#25321;&#32858;&#31867;&#20013;&#24515;&#26102;&#35201;&#32771;&#34385;&#22810;&#20010;&#23646;&#24615;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#32858;&#31867;&#30446;&#26631;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#38024;&#23545;&#19981;&#21516;&#32858;&#31867;&#30446;&#26631;&#30340;&#21442;&#25968;&#21270;&#36817;&#20284;&#31639;&#27861;&#65292;&#36825;&#20123;&#31639;&#27861;&#22312;&#20445;&#35777;&#32858;&#31867;&#36136;&#37327;&#30340;&#21516;&#26102;&#65292;&#20855;&#26377;&#32039;&#30830;&#30340;&#36817;&#20284;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22810;&#26679;&#24615;&#24863;&#30693;&#32858;&#31867;&#38382;&#39064;&#65292;&#20854;&#20013;&#25968;&#25454;&#28857;&#19982;&#22810;&#20010;&#23646;&#24615;&#30456;&#20851;&#32852;&#65292;&#24418;&#25104;&#20132;&#21449;&#30340;&#32452;&#12290;&#32858;&#31867;&#35299;&#20915;&#26041;&#26696;&#38656;&#35201;&#30830;&#20445;&#20174;&#27599;&#20010;&#32452;&#20013;&#36873;&#25321;&#26368;&#23569;&#25968;&#37327;&#30340;&#32858;&#31867;&#20013;&#24515;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#32858;&#31867;&#30446;&#26631;&#65292;&#21487;&#20197;&#26159;$k$-&#20013;&#20301;&#25968;&#65292;$k$-&#22343;&#20540;&#25110;$k$-&#20379;&#24212;&#21830;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#21442;&#25968;&#21270;&#36817;&#20284;&#31639;&#27861;&#65292;&#36817;&#20284;&#27604;&#20998;&#21035;&#20026;$1+\frac{2}{e}$&#65292;$1+\frac{8}{e}$&#21644;$3$&#65292;&#29992;&#20110;&#22810;&#26679;&#24615;&#24863;&#30693;$k$-&#20013;&#20301;&#25968;&#65292;&#22810;&#26679;&#24615;&#24863;&#30693;$k$-&#22343;&#20540;&#21644;&#22810;&#26679;&#24615;&#24863;&#30693;$k$-&#20379;&#24212;&#21830;&#12290;&#36825;&#20123;&#36817;&#20284;&#27604;&#22312;&#20551;&#35774;Gap-ETH&#21644;FPT $\neq$ W[2]&#30340;&#24773;&#20917;&#19979;&#26159;&#32039;&#30830;&#30340;&#12290;&#23545;&#20110;&#20844;&#24179;$k$-&#20013;&#20301;&#25968;&#21644;&#20844;&#24179;$k$-&#22343;&#20540;&#30340;&#19981;&#30456;&#20132;&#24037;&#21378;&#32452;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21442;&#25968;&#21270;&#36817;&#20284;&#31639;&#27861;&#65292;&#36817;&#20284;&#27604;&#20998;&#21035;&#20026;$1+\frac{2}{e}$&#21644;$1+\frac{8}{e}$&#12290;&#23545;&#20110;&#20855;&#26377;&#19981;&#30456;&#20132;&#24037;&#21378;&#32452;&#30340;&#20844;&#24179;$k$-&#20379;&#24212;&#21830;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#39033;&#24335;&#26102;&#38388;&#36817;&#20284;&#31639;&#27861;&#65292;&#22240;&#23376;&#20026;$3$&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we study diversity-aware clustering problems where the data points are associated with multiple attributes resulting in intersecting groups. A clustering solution need to ensure that a minimum number of cluster centers are chosen from each group while simultaneously minimizing the clustering objective, which can be either $k$-median, $k$-means or $k$-supplier. We present parameterized approximation algorithms with approximation ratios $1+ \frac{2}{e}$, $1+\frac{8}{e}$ and $3$ for diversity-aware $k$-median, diversity-aware $k$-means and diversity-aware $k$-supplier, respectively. The approximation ratios are tight assuming Gap-ETH and FPT $\neq$ W[2]. For fair $k$-median and fair $k$-means with disjoint faicility groups, we present parameterized approximation algorithm with approximation ratios $1+\frac{2}{e}$ and $1+\frac{8}{e}$, respectively. For fair $k$-supplier with disjoint facility groups, we present a polynomial-time approximation algorithm with factor $3$, improv
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36328;&#32593;&#32476;&#33410;&#28857;&#20998;&#31867;&#26041;&#27861;OTGCN&#65292;&#21033;&#29992;&#22270;&#21367;&#31215;&#32593;&#32476;&#21644;&#26368;&#20248;&#36755;&#36816;&#31574;&#30053;&#65292;&#21487;&#20197;&#22312;&#19981;&#21516;&#25968;&#25454;&#37319;&#38598;&#22320;&#28857;&#30340;&#26679;&#26412;&#20043;&#38388;&#32416;&#27491;&#39046;&#22495;&#28418;&#31227;&#65292;&#24182;&#19988;&#22312;&#33258;&#38381;&#30151;&#35889;&#31995;&#38556;&#30861;&#30340;&#20998;&#31867;&#19978;&#21462;&#24471;&#20102;&#26377;&#25928;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.05478</link><description>&lt;p&gt;
&#36328;&#32593;&#32476;&#33410;&#28857;&#20998;&#31867;&#29992;&#20110;&#33258;&#38381;&#30151;&#26816;&#27979;&#30340;&#20154;&#32676;&#22270;
&lt;/p&gt;
&lt;p&gt;
Population Graph Cross-Network Node Classification for Autism Detection Across Sample Groups. (arXiv:2401.05478v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05478
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36328;&#32593;&#32476;&#33410;&#28857;&#20998;&#31867;&#26041;&#27861;OTGCN&#65292;&#21033;&#29992;&#22270;&#21367;&#31215;&#32593;&#32476;&#21644;&#26368;&#20248;&#36755;&#36816;&#31574;&#30053;&#65292;&#21487;&#20197;&#22312;&#19981;&#21516;&#25968;&#25454;&#37319;&#38598;&#22320;&#28857;&#30340;&#26679;&#26412;&#20043;&#38388;&#32416;&#27491;&#39046;&#22495;&#28418;&#31227;&#65292;&#24182;&#19988;&#22312;&#33258;&#38381;&#30151;&#35889;&#31995;&#38556;&#30861;&#30340;&#20998;&#31867;&#19978;&#21462;&#24471;&#20102;&#26377;&#25928;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#26159;&#23558;&#22270;&#20687;&#21644;&#38750;&#22270;&#20687;&#21307;&#30103;&#20449;&#24687;&#32467;&#21512;&#36215;&#26469;&#36827;&#34892;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;&#36328;&#32593;&#32476;&#33410;&#28857;&#20998;&#31867;&#25193;&#23637;&#20102;GNN&#25216;&#26415;&#20197;&#35299;&#20915;&#39046;&#22495;&#28418;&#31227;&#38382;&#39064;&#65292;&#20801;&#35768;&#22312;&#26410;&#26631;&#35760;&#30340;&#30446;&#26631;&#32593;&#32476;&#19978;&#36827;&#34892;&#33410;&#28857;&#20998;&#31867;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24378;&#22823;&#32780;&#26032;&#39062;&#30340;&#36328;&#32593;&#32476;&#33410;&#28857;&#20998;&#31867;&#26041;&#27861;OTGCN&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;&#22270;&#21367;&#31215;&#32593;&#32476;&#30340;&#27010;&#24565;&#65292;&#21516;&#26102;&#24212;&#29992;&#22522;&#20110;&#26368;&#20248;&#36755;&#36816;&#30340;&#31574;&#30053;&#26469;&#32416;&#27491;&#26469;&#33258;&#19981;&#21516;&#25968;&#25454;&#37319;&#38598;&#22320;&#28857;&#26679;&#26412;&#20043;&#38388;&#21487;&#33021;&#21457;&#29983;&#30340;&#39046;&#22495;&#28418;&#31227;&#12290;&#36825;&#31181;&#32508;&#21512;&#26041;&#27861;&#20026;&#19981;&#21516;&#20301;&#32622;&#21644;&#35774;&#22791;&#25910;&#38598;&#21040;&#30340;&#22810;&#31181;&#19981;&#21516;&#24418;&#24335;&#30340;&#25968;&#25454;&#30340;&#22330;&#26223;&#25552;&#20379;&#20102;&#23454;&#38469;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#22270;&#20687;&#21644;&#38750;&#22270;&#20687;&#25968;&#25454;&#36827;&#34892;&#20998;&#31867;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#20998;&#31867;&#33258;&#38381;&#30151;&#35889;&#31995;&#38556;&#30861;&#21463;&#35797;&#32773;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks (GNN) are a powerful tool for combining imaging and non-imaging medical information for node classification tasks. Cross-network node classification extends GNN techniques to account for domain drift, allowing for node classification on an unlabeled target network. In this paper we present OTGCN, a powerful, novel approach to cross-network node classification. This approach leans on concepts from graph convolutional networks to harness insights from graph data structures while simultaneously applying strategies rooted in optimal transport to correct for the domain drift that can occur between samples from different data collection sites. This blended approach provides a practical solution for scenarios with many distinct forms of data collected across different locations and equipment. We demonstrate the effectiveness of this approach at classifying Autism Spectrum Disorder subjects using a blend of imaging and non-imaging data.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22238;&#39038;&#20102;&#21487;&#31359;&#25140;&#20154;&#31867;&#27963;&#21160;&#35782;&#21035;&#39046;&#22495;&#30340;&#28145;&#24230;&#23398;&#20064;&#30740;&#31350;&#65292;&#24182;&#24635;&#32467;&#20102;&#21508;&#20010;&#30740;&#31350;&#25152;&#37319;&#29992;&#30340;&#35757;&#32451;&#31243;&#24207;&#12290;&#30740;&#31350;&#21457;&#29616;&#23384;&#22312;&#32570;&#20047;&#35814;&#32454;&#35757;&#32451;&#21327;&#35758;&#30340;&#36235;&#21183;&#65292;&#21516;&#26102;&#21033;&#29992;&#25511;&#21046;&#21464;&#37327;&#26041;&#27861;&#35780;&#20272;&#20102;&#20851;&#38190;&#21487;&#35843;&#32452;&#20214;&#23545;&#36328;&#20027;&#20307;&#27867;&#21270;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2401.05477</link><description>&lt;p&gt;
&#20026;&#20154;&#31867;&#27963;&#21160;&#35782;&#21035;&#27169;&#22411;&#26631;&#20934;&#21270;&#35757;&#32451;&#27969;&#31243;&#65306;&#21487;&#35843;&#22240;&#32032;&#30340;&#20840;&#38754;&#22238;&#39038;
&lt;/p&gt;
&lt;p&gt;
Standardizing Your Training Process for Human Activity Recognition Models: A Comprehensive Review in the Tunable Factors. (arXiv:2401.05477v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05477
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22238;&#39038;&#20102;&#21487;&#31359;&#25140;&#20154;&#31867;&#27963;&#21160;&#35782;&#21035;&#39046;&#22495;&#30340;&#28145;&#24230;&#23398;&#20064;&#30740;&#31350;&#65292;&#24182;&#24635;&#32467;&#20102;&#21508;&#20010;&#30740;&#31350;&#25152;&#37319;&#29992;&#30340;&#35757;&#32451;&#31243;&#24207;&#12290;&#30740;&#31350;&#21457;&#29616;&#23384;&#22312;&#32570;&#20047;&#35814;&#32454;&#35757;&#32451;&#21327;&#35758;&#30340;&#36235;&#21183;&#65292;&#21516;&#26102;&#21033;&#29992;&#25511;&#21046;&#21464;&#37327;&#26041;&#27861;&#35780;&#20272;&#20102;&#20851;&#38190;&#21487;&#35843;&#32452;&#20214;&#23545;&#36328;&#20027;&#20307;&#27867;&#21270;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#23398;&#20064;&#22312;&#22810;&#20010;&#39046;&#22495;&#20013;&#25104;&#20026;&#19968;&#20010;&#24378;&#22823;&#30340;&#24037;&#20855;&#65292;&#23548;&#33268;&#20102;&#22312;&#21487;&#31359;&#25140;&#20154;&#31867;&#27963;&#21160;&#35782;&#21035;&#65288;WHAR&#65289;&#39046;&#22495;&#30340;&#24212;&#29992;&#30740;&#31350;&#28608;&#22686;&#12290;&#23613;&#31649;&#21457;&#23637;&#36805;&#36895;&#65292;&#20294;&#20154;&#20204;&#23545;&#23454;&#39564;&#27169;&#22411;&#35757;&#32451;&#20013;&#20351;&#29992;&#30340;&#26631;&#20934;&#21270;&#21644;&#19968;&#33268;&#24615;&#31243;&#24207;&#30340;&#32570;&#20047;&#25285;&#24551;&#65292;&#21487;&#33021;&#20250;&#24433;&#21709;&#30740;&#31350;&#32467;&#26524;&#30340;&#21487;&#37325;&#22797;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;WHAR&#39046;&#22495;&#30340;&#24403;&#20195;&#28145;&#24230;&#23398;&#20064;&#30740;&#31350;&#36827;&#34892;&#20102;&#20840;&#38754;&#22238;&#39038;&#65292;&#24182;&#25972;&#29702;&#20102;&#21508;&#31181;&#30740;&#31350;&#20013;&#25152;&#37319;&#29992;&#30340;&#35757;&#32451;&#31243;&#24207;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#19968;&#20010;&#20027;&#35201;&#36235;&#21183;&#26159;&#32570;&#20047;&#27169;&#22411;&#35757;&#32451;&#21327;&#35758;&#25552;&#20379;&#30340;&#32454;&#33410;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#26356;&#28165;&#26970;&#22320;&#20102;&#35299;&#32570;&#22833;&#25551;&#36848;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#21033;&#29992;&#25511;&#21046;&#21464;&#37327;&#26041;&#27861;&#35780;&#20272;&#20102;&#20851;&#38190;&#21487;&#35843;&#32452;&#20214;&#65288;&#20363;&#22914;&#20248;&#21270;&#25216;&#26415;&#21644;&#25552;&#21069;&#20572;&#27490;&#20934;&#21017;&#65289;&#23545;&#36328;&#20027;&#20307;&#27867;&#21270;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, deep learning has emerged as a potent tool across a multitude of domains, leading to a surge in research pertaining to its application in the wearable human activity recognition (WHAR) domain. Despite the rapid development, concerns have been raised about the lack of standardization and consistency in the procedures used for experimental model training, which may affect the reproducibility and reliability of research results. In this paper, we provide an exhaustive review of contemporary deep learning research in the field of WHAR and collate information pertaining to the training procedure employed in various studies. Our findings suggest that a major trend is the lack of detail provided by model training protocols. Besides, to gain a clearer understanding of the impact of missing descriptions, we utilize a control variables approach to assess the impact of key tunable components (e.g., optimization techniques and early stopping criteria) on the inter-subject generali
&lt;/p&gt;</description></item><item><title>CADgpt&#26159;&#19968;&#20010;&#21019;&#26032;&#24615;&#30340;&#25554;&#20214;&#65292;&#36890;&#36807;&#23558;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#19982;Rhino3D&#38598;&#25104;&#65292;&#22312;&#35745;&#31639;&#26426;&#36741;&#21161;&#35774;&#35745;&#20013;&#31616;&#21270;&#30028;&#38754;&#65292;&#20351;&#21021;&#23398;&#32773;&#33021;&#22815;&#36890;&#36807;&#30452;&#35266;&#30340;&#33258;&#28982;&#35821;&#35328;&#21629;&#20196;&#25191;&#34892;&#22797;&#26434;&#30340;3D&#24314;&#27169;&#20219;&#21153;&#12290;&#23427;&#25552;&#20379;&#20102;&#26356;&#20855;&#21253;&#23481;&#24615;&#21644;&#20114;&#21160;&#24615;&#30340;&#25945;&#32946;&#29615;&#22659;&#65292;&#24182;&#23558;&#22797;&#26434;&#30340;&#35774;&#35745;&#24037;&#20855;&#26222;&#21450;&#21040;&#26356;&#24191;&#27867;&#30340;&#23398;&#29983;&#32676;&#20307;&#20013;&#65292;&#26377;&#21161;&#20110;&#35774;&#35745;&#25945;&#32946;&#30340;&#27665;&#20027;&#21270;&#21457;&#23637;&#12290;</title><link>http://arxiv.org/abs/2401.05476</link><description>&lt;p&gt;
CADgpt&#65306;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#22686;&#24378;&#35745;&#31639;&#26426;&#36741;&#21161;&#35774;&#35745;&#24037;&#20316;&#27969;&#31243;&#30340;3D&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
CADgpt: Harnessing Natural Language Processing for 3D Modelling to Enhance Computer-Aided Design Workflows. (arXiv:2401.05476v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05476
&lt;/p&gt;
&lt;p&gt;
CADgpt&#26159;&#19968;&#20010;&#21019;&#26032;&#24615;&#30340;&#25554;&#20214;&#65292;&#36890;&#36807;&#23558;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#19982;Rhino3D&#38598;&#25104;&#65292;&#22312;&#35745;&#31639;&#26426;&#36741;&#21161;&#35774;&#35745;&#20013;&#31616;&#21270;&#30028;&#38754;&#65292;&#20351;&#21021;&#23398;&#32773;&#33021;&#22815;&#36890;&#36807;&#30452;&#35266;&#30340;&#33258;&#28982;&#35821;&#35328;&#21629;&#20196;&#25191;&#34892;&#22797;&#26434;&#30340;3D&#24314;&#27169;&#20219;&#21153;&#12290;&#23427;&#25552;&#20379;&#20102;&#26356;&#20855;&#21253;&#23481;&#24615;&#21644;&#20114;&#21160;&#24615;&#30340;&#25945;&#32946;&#29615;&#22659;&#65292;&#24182;&#23558;&#22797;&#26434;&#30340;&#35774;&#35745;&#24037;&#20855;&#26222;&#21450;&#21040;&#26356;&#24191;&#27867;&#30340;&#23398;&#29983;&#32676;&#20307;&#20013;&#65292;&#26377;&#21161;&#20110;&#35774;&#35745;&#25945;&#32946;&#30340;&#27665;&#20027;&#21270;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;CADgpt&#65292;&#36825;&#26159;&#19968;&#20010;&#21019;&#26032;&#24615;&#30340;&#25554;&#20214;&#65292;&#23558;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#19982;Rhino3D&#38598;&#25104;&#22312;&#19968;&#36215;&#65292;&#20197;&#22686;&#24378;&#35745;&#31639;&#26426;&#36741;&#21161;&#35774;&#35745;&#65288;CAD&#65289;&#29615;&#22659;&#20013;&#30340;3D&#24314;&#27169;&#12290;&#21033;&#29992;OpenAI&#30340;GPT-4&#65292;CADgpt&#31616;&#21270;&#20102;CAD&#30028;&#38754;&#65292;&#20351;&#29992;&#25143;&#65292;&#23588;&#20854;&#26159;&#21021;&#23398;&#32773;&#65292;&#21487;&#20197;&#36890;&#36807;&#30452;&#35266;&#30340;&#33258;&#28982;&#35821;&#35328;&#21629;&#20196;&#25191;&#34892;&#22797;&#26434;&#30340;3D&#24314;&#27169;&#20219;&#21153;&#12290;&#36825;&#31181;&#26041;&#27861;&#26174;&#33879;&#38477;&#20302;&#20102;&#20256;&#32479;CAD&#36719;&#20214;&#30340;&#23398;&#20064;&#26354;&#32447;&#65292;&#20419;&#36827;&#20102;&#26356;&#21253;&#23481;&#21644;&#20114;&#21160;&#30340;&#25945;&#32946;&#29615;&#22659;&#12290;&#26412;&#25991;&#35752;&#35770;&#20102;CADgpt&#30340;&#25216;&#26415;&#26550;&#26500;&#65292;&#21253;&#25324;&#20854;&#22312;Rhino3D&#20013;&#30340;&#38598;&#25104;&#20197;&#21450;&#23558;GPT-4&#30340;&#33021;&#21147;&#24212;&#29992;&#20110;CAD&#20219;&#21153;&#30340;&#36866;&#24212;&#24615;&#12290;&#23427;&#36824;&#23637;&#31034;&#20102;CADgpt&#22312;&#21508;&#31181;&#35774;&#35745;&#22330;&#26223;&#20013;&#30340;&#25928;&#26524;&#65292;&#20984;&#26174;&#20102;&#23558;&#22797;&#26434;&#30340;&#35774;&#35745;&#24037;&#20855;&#26222;&#21450;&#21040;&#26356;&#24191;&#27867;&#30340;&#23398;&#29983;&#32676;&#20307;&#20013;&#65292;&#20197;&#20351;&#35774;&#35745;&#25945;&#32946;&#27665;&#20027;&#21270;&#30340;&#28508;&#21147;&#12290;&#35752;&#35770;&#36827;&#19968;&#27493;&#25506;&#35752;&#20102;CADgpt&#23545;&#25945;&#23398;&#27861;&#21644;&#35838;&#31243;&#24320;&#21457;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces CADgpt, an innovative plugin integrating Natural Language Processing (NLP) with Rhino3D for enhancing 3D modelling in computer-aided design (CAD) environments. Leveraging OpenAI's GPT-4, CADgpt simplifies the CAD interface, enabling users, particularly beginners, to perform complex 3D modelling tasks through intuitive natural language commands. This approach significantly reduces the learning curve associated with traditional CAD software, fostering a more inclusive and engaging educational environment. The paper discusses CADgpt's technical architecture, including its integration within Rhino3D and the adaptation of GPT-4 capabilities for CAD tasks. It presents case studies demonstrating CADgpt's efficacy in various design scenarios, highlighting its potential to democratise design education by making sophisticated design tools accessible to a broader range of students. The discussion further explores CADgpt's implications for pedagogy and curriculum development,
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#38382;&#39064;&#8212;&#8212;&#26032;&#33410;&#28857;&#39044;&#27979;&#65292;&#21363;&#20174;&#20197;&#21069;&#19982;&#22270;&#19981;&#30456;&#36830;&#30340;&#23396;&#31435;&#33410;&#28857;&#20013;&#39044;&#27979;&#25152;&#26377;&#36830;&#25509;&#12290;&#36890;&#36807;&#22522;&#20110;&#28145;&#24230;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26550;&#26500;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#21487;&#20197;&#35299;&#20915;&#36825;&#19968;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.05468</link><description>&lt;p&gt;
&#22270;&#25366;&#25496;&#20013;&#24341;&#20837;&#30340;&#26032;&#33410;&#28857;&#39044;&#27979;&#65306;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#23396;&#31435;&#33410;&#28857;&#30340;&#25152;&#26377;&#36830;&#25509;
&lt;/p&gt;
&lt;p&gt;
Introducing New Node Prediction in Graph Mining: Predicting All Links from Isolated Nodes with Graph Neural Networks. (arXiv:2401.05468v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05468
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#38382;&#39064;&#8212;&#8212;&#26032;&#33410;&#28857;&#39044;&#27979;&#65292;&#21363;&#20174;&#20197;&#21069;&#19982;&#22270;&#19981;&#30456;&#36830;&#30340;&#23396;&#31435;&#33410;&#28857;&#20013;&#39044;&#27979;&#25152;&#26377;&#36830;&#25509;&#12290;&#36890;&#36807;&#22522;&#20110;&#28145;&#24230;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26550;&#26500;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#21487;&#20197;&#35299;&#20915;&#36825;&#19968;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22270;&#25366;&#25496;&#21644;&#31038;&#20132;&#32593;&#32476;&#20998;&#26512;&#39046;&#22495;&#20013;&#30340;&#19968;&#20010;&#26032;&#38382;&#39064;&#65292;&#21363;&#26032;&#33410;&#28857;&#39044;&#27979;&#12290;&#20174;&#25216;&#26415;&#19978;&#35762;&#65292;&#36825;&#20010;&#20219;&#21153;&#21487;&#34987;&#24402;&#31867;&#20026;&#38646;&#26679;&#26412;&#30340;&#22270;&#22806;&#25152;&#26377;&#36830;&#25509;&#39044;&#27979;&#12290;&#36825;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#26088;&#22312;&#39044;&#27979;&#19968;&#20010;&#20808;&#21069;&#19982;&#22270;&#19981;&#30456;&#36830;&#12289;&#23396;&#31435;&#19988;&#26410;&#35266;&#27979;&#21040;&#30340;&#26032;&#33410;&#28857;&#30340;&#25152;&#26377;&#36830;&#25509;&#12290;&#19982;&#32463;&#20856;&#30340;&#36830;&#25509;&#39044;&#27979;&#26041;&#27861;&#65288;&#21253;&#25324;&#23569;&#26679;&#26412;&#30340;&#22270;&#22806;&#36830;&#25509;&#39044;&#27979;&#65289;&#19981;&#21516;&#65292;&#36825;&#20010;&#38382;&#39064;&#26377;&#20004;&#20010;&#20851;&#38190;&#30340;&#19981;&#21516;&#20043;&#22788;&#65306;&#65288;1&#65289;&#26032;&#33410;&#28857;&#27809;&#26377;&#29616;&#26377;&#30340;&#36830;&#25509;&#21487;&#20379;&#25552;&#21462;&#27169;&#24335;&#29992;&#20110;&#26032;&#30340;&#39044;&#27979;&#65307;&#65288;2&#65289;&#30446;&#26631;&#26159;&#39044;&#27979;&#19981;&#20165;&#20165;&#26159;&#19968;&#20010;&#65292;&#32780;&#26159;&#36825;&#20010;&#26032;&#33410;&#28857;&#30340;&#25152;&#26377;&#36830;&#25509;&#65292;&#25110;&#32773;&#33267;&#23569;&#20854;&#20013;&#30340;&#19968;&#20010;&#26174;&#33879;&#37096;&#20998;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#22522;&#20110;&#28145;&#24230;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26550;&#26500;&#21487;&#20197;&#23398;&#20064;&#35299;&#20915;&#36825;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#22312;&#19968;&#20010;&#25991;&#29486;&#24341;&#29992;&#32593;&#32476;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a new problem in the field of graph mining and social network analysis called new node prediction. More technically, the task can be categorized as zero-shot out-of-graph all-links prediction. This challenging problem aims to predict all links from a new, isolated, and unobserved node that was previously disconnected from the graph. Unlike classic approaches to link prediction (including few-shot out-of-graph link prediction), this problem presents two key differences: (1) the new node has no existing links from which to extract patterns for new predictions; and (2) the goal is to predict not just one, but all the links of this new node, or at least a significant part of them. Experiments demonstrate that an architecture based on Deep Graph Neural Networks can learn to solve this challenging problem in a bibliographic citation network.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26426;&#22120;&#25945;&#23398;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#36845;&#20195;&#26426;&#22120;&#25945;&#23398;&#21644;&#20219;&#21153;&#29305;&#23450;&#30340;&#26367;&#20195;&#27169;&#22411;&#65292;&#22686;&#24378;&#20102;&#21033;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#38646;&#26679;&#26412;&#23398;&#20064;&#22120;&#30340;&#27169;&#22359;&#21270;AI&#20195;&#29702;&#30340;&#40065;&#26834;&#24615;&#21644;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.05467</link><description>&lt;p&gt;
&#22522;&#20110;&#38646;&#26679;&#26412;&#23398;&#20064;&#30340;&#27169;&#22359;&#21270;AI&#20195;&#29702;&#30340;&#26426;&#22120;&#25945;&#23398;
&lt;/p&gt;
&lt;p&gt;
Machine Teaching for Building Modular AI Agents based on Zero-shot Learners. (arXiv:2401.05467v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05467
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26426;&#22120;&#25945;&#23398;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#36845;&#20195;&#26426;&#22120;&#25945;&#23398;&#21644;&#20219;&#21153;&#29305;&#23450;&#30340;&#26367;&#20195;&#27169;&#22411;&#65292;&#22686;&#24378;&#20102;&#21033;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#38646;&#26679;&#26412;&#23398;&#20064;&#22120;&#30340;&#27169;&#22359;&#21270;AI&#20195;&#29702;&#30340;&#40065;&#26834;&#24615;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36827;&#23637;&#23548;&#33268;&#20102;&#35768;&#22810;&#27169;&#22359;&#21270;AI&#20195;&#29702;&#30340;&#21019;&#24314;&#12290;&#36825;&#20123;&#20195;&#29702;&#20351;&#29992;LLMs&#20316;&#20026;&#38646;&#26679;&#26412;&#23398;&#20064;&#22120;&#65292;&#22312;&#20154;&#31867;&#29992;&#25143;&#35774;&#23450;&#30340;&#22797;&#26434;&#20219;&#21153;&#20013;&#25191;&#34892;&#23376;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#22686;&#24378;&#21033;&#29992;LLMs&#20316;&#20026;&#38646;&#26679;&#26412;&#23398;&#20064;&#22120;&#30340;&#27169;&#22359;&#21270;AI&#20195;&#29702;&#30340;&#40065;&#26834;&#24615;&#21644;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#36845;&#20195;&#26426;&#22120;&#25945;&#23398;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#22312;&#26377;&#38480;&#30340;&#20154;&#31867;&#21453;&#39304;&#19979;&#36880;&#28176;&#25945;&#23548;AI&#20195;&#29702;&#30340;&#39640;&#25928;&#26041;&#24335;&#65292;&#35299;&#20915;&#20102;&#38646;&#26679;&#26412;&#23398;&#20064;&#36136;&#37327;&#38480;&#21046;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#20027;&#24352;&#21033;&#29992;&#21021;&#22987;&#37096;&#32626;&#30340;&#25968;&#25454;&#36861;&#36394;&#20197;&#21450;&#38646;&#26679;&#26412;&#23398;&#20064;&#22120;&#30340;&#36755;&#20986;&#25110;&#27880;&#37322;&#26469;&#35757;&#32451;&#26356;&#23567;&#19988;&#20219;&#21153;&#29305;&#23450;&#30340;&#26367;&#20195;&#27169;&#22411;&#65292;&#21487;&#20197;&#20943;&#23569;&#32463;&#27982;&#25104;&#26412;&#21644;&#29615;&#22659;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#26426;&#22120;&#25945;&#23398;&#36807;&#31243;&#21033;&#29992;&#20154;&#31867;&#19987;&#19994;&#30693;&#35782;&#26469;&#32416;&#27491;&#39640;&#27010;&#29575;&#35823;&#26631;&#27880;&#30340;&#31034;&#20363;&#12290;&#22312;&#19977;&#20010;&#24120;&#35265;&#23545;&#35805;AI&#20195;&#29702;&#20219;&#21153;&#19978;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#25509;&#36817;&#29702;&#24819;&#24615;&#33021;&#21487;&#20197;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent advances in large language models (LLMs) have led to the creation of many modular AI agents. These agents employ LLMs as zero-shot learners to perform sub-tasks in order to solve complex tasks set forth by human users. We propose an approach to enhance the robustness and performance of modular AI agents that utilize LLMs as zero-shot learners. Our iterative machine teaching method offers an efficient way to teach AI agents over time with limited human feedback, addressing the limit posed by the quality of zero-shot learning. We advocate leveraging the data traces from initial deployments and outputs or annotations from the zero-shot learners to train smaller and task-specific substitute models which can reduce both the monetary costs and environmental impact. Our machine teaching process avails human expertise to correct examples with a high likelihood of misannotations. Results on three tasks, common to conversational AI agents, show that close-to-oracle performance can be 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#26500;&#24314;&#20102;&#19968;&#20010;&#20154;&#31867;&#21644;&#31070;&#32463;&#32593;&#32476;&#20043;&#38388;&#30340;&#21452;&#21521;&#30693;&#35782;&#20132;&#20114;&#30028;&#38754;&#65292;&#20351;&#29992;&#35270;&#35273;&#27010;&#24565;&#21644;&#23427;&#20204;&#20043;&#38388;&#30340;&#20851;&#31995;&#20316;&#20026;&#8220;&#35821;&#35328;&#8221;&#65292;&#20197;&#23454;&#29616;&#20154;&#31867;&#21644;&#31070;&#32463;&#32593;&#32476;&#20043;&#38388;&#30340;&#30693;&#35782;&#20132;&#27969;&#12290;&#35813;&#30028;&#38754;&#21487;&#20197;&#20351;&#31070;&#32463;&#32593;&#32476;&#21521;&#20154;&#31867;&#25552;&#20379;&#30452;&#35266;&#30340;&#25512;&#29702;&#35299;&#37322;&#65292;&#21516;&#26102;&#20154;&#31867;&#21487;&#20197;&#20462;&#25913;&#20854;&#20013;&#30340;&#20559;&#35265;&#12290;</title><link>http://arxiv.org/abs/2401.05461</link><description>&lt;p&gt;
&#20154;&#31867;&#19982;&#31070;&#32463;&#32593;&#32476;&#20043;&#38388;&#30340;&#21452;&#21521;&#30693;&#35782;&#20132;&#20114;&#30028;&#38754;
&lt;/p&gt;
&lt;p&gt;
The two-way knowledge interaction interface between humans and neural networks. (arXiv:2401.05461v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05461
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#26500;&#24314;&#20102;&#19968;&#20010;&#20154;&#31867;&#21644;&#31070;&#32463;&#32593;&#32476;&#20043;&#38388;&#30340;&#21452;&#21521;&#30693;&#35782;&#20132;&#20114;&#30028;&#38754;&#65292;&#20351;&#29992;&#35270;&#35273;&#27010;&#24565;&#21644;&#23427;&#20204;&#20043;&#38388;&#30340;&#20851;&#31995;&#20316;&#20026;&#8220;&#35821;&#35328;&#8221;&#65292;&#20197;&#23454;&#29616;&#20154;&#31867;&#21644;&#31070;&#32463;&#32593;&#32476;&#20043;&#38388;&#30340;&#30693;&#35782;&#20132;&#27969;&#12290;&#35813;&#30028;&#38754;&#21487;&#20197;&#20351;&#31070;&#32463;&#32593;&#32476;&#21521;&#20154;&#31867;&#25552;&#20379;&#30452;&#35266;&#30340;&#25512;&#29702;&#35299;&#37322;&#65292;&#21516;&#26102;&#20154;&#31867;&#21487;&#20197;&#20462;&#25913;&#20854;&#20013;&#30340;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#24050;&#32463;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#20010;&#39046;&#22495;&#65292;&#24182;&#19988;&#36890;&#24120;&#20248;&#20110;&#20154;&#31867;&#65292;&#20294;&#23427;&#20204;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#20173;&#28982;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#65292;&#20154;&#31867;&#26080;&#27861;&#30452;&#35266;&#22320;&#29702;&#35299;NN&#30340;&#20915;&#31574;&#36923;&#36753;&#12290;&#36825;&#20063;&#22952;&#30861;&#20102;&#20154;&#31867;&#19982;NN&#20043;&#38388;&#30340;&#30693;&#35782;&#20132;&#20114;&#65292;&#38459;&#27490;&#20102;&#20154;&#20204;&#22312;NN&#30340;&#20915;&#31574;&#20986;&#38169;&#26102;&#30452;&#25509;&#21442;&#19982;&#32473;&#20104;&#25351;&#23548;&#12290;&#34429;&#28982;&#26368;&#36817;&#22312;&#21487;&#35299;&#37322;AI&#26041;&#38754;&#30340;&#30740;&#31350;&#24050;&#32463;&#20174;&#21508;&#20010;&#35282;&#24230;&#23454;&#29616;&#20102;NN&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#20294;&#23578;&#26410;&#25552;&#20379;&#26377;&#25928;&#30340;&#20154;&#31867;&#21644;NN&#20043;&#38388;&#30340;&#30693;&#35782;&#20132;&#27969;&#26041;&#27861;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#21452;&#21521;&#20132;&#20114;&#30028;&#38754;&#65292;&#23427;&#23558;&#35270;&#35273;&#27010;&#24565;&#21450;&#20854;&#20851;&#31995;&#30340;&#32467;&#26500;&#21270;&#34920;&#31034;&#20316;&#20026;&#20154;&#31867;&#21644;NN&#20043;&#38388;&#30340;&#30693;&#35782;&#20132;&#27969;&#30340;&#8220;&#35821;&#35328;&#8221;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;NN&#22522;&#20110;&#31867;&#21035;&#29305;&#23450;&#32467;&#26500;&#27010;&#24565;&#22270;&#65288;C-SCG&#65289;&#21521;&#20154;&#31867;&#25552;&#20379;&#30452;&#35266;&#30340;&#25512;&#29702;&#35299;&#37322;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#20154;&#31867;&#21487;&#20197;&#20462;&#25913;C-SCG&#20013;&#23384;&#22312;&#30340;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite neural networks (NN) have been widely applied in various fields and generally outperforms humans, they still lack interpretability to a certain extent, and humans are unable to intuitively understand the decision logic of NN. This also hinders the knowledge interaction between humans and NN, preventing humans from getting involved to give direct guidance when NN's decisions go wrong. While recent research in explainable AI has achieved interpretability of NN from various perspectives, it has not yet provided effective methods for knowledge exchange between humans and NN. To address this problem, we constructed a two-way interaction interface that uses structured representations of visual concepts and their relationships as the "language" for knowledge exchange between humans and NN. Specifically, NN provide intuitive reasoning explanations to humans based on the class-specific structural concepts graph (C-SCG). On the other hand, humans can modify the biases present in the C-SC
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#20010;&#20154;LLM&#20195;&#29702;&#30340;&#33021;&#21147;&#12289;&#25928;&#29575;&#21644;&#23433;&#20840;&#24615;&#65292;&#26088;&#22312;&#25552;&#21319;&#26234;&#33021;&#20010;&#20154;&#21161;&#29702;&#30340;&#23454;&#29992;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.05459</link><description>&lt;p&gt;
&#20010;&#20154;LLM&#20195;&#29702;:&#20851;&#20110;&#33021;&#21147;&#12289;&#25928;&#29575;&#21644;&#23433;&#20840;&#24615;&#30340;&#27934;&#23519;&#21644;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Personal LLM Agents: Insights and Survey about the Capability, Efficiency and Security. (arXiv:2401.05459v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05459
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#20010;&#20154;LLM&#20195;&#29702;&#30340;&#33021;&#21147;&#12289;&#25928;&#29575;&#21644;&#23433;&#20840;&#24615;&#65292;&#26088;&#22312;&#25552;&#21319;&#26234;&#33021;&#20010;&#20154;&#21161;&#29702;&#30340;&#23454;&#29992;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20174;&#20010;&#20154;&#35745;&#31639;&#35774;&#22791;&#20986;&#29616;&#20197;&#26469;&#65292;&#26234;&#33021;&#20010;&#20154;&#21161;&#29702;(IPA)&#19968;&#30452;&#26159;&#30740;&#31350;&#20154;&#21592;&#21644;&#24037;&#31243;&#24072;&#20851;&#27880;&#30340;&#20851;&#38190;&#25216;&#26415;&#20043;&#19968;&#65292;&#26088;&#22312;&#24110;&#21161;&#29992;&#25143;&#39640;&#25928;&#33719;&#21462;&#20449;&#24687;&#21644;&#25191;&#34892;&#20219;&#21153;&#65292;&#24182;&#20026;&#29992;&#25143;&#25552;&#20379;&#26356;&#26234;&#33021;&#12289;&#20415;&#25463;&#21644;&#20016;&#23500;&#30340;&#20132;&#20114;&#20307;&#39564;&#12290;&#38543;&#30528;&#26234;&#33021;&#25163;&#26426;&#21644;&#29289;&#32852;&#32593;&#30340;&#21457;&#23637;&#65292;&#35745;&#31639;&#21644;&#24863;&#30693;&#35774;&#22791;&#21464;&#24471;&#26080;&#22788;&#19981;&#22312;&#65292;&#26497;&#22823;&#22320;&#25193;&#23637;&#20102;IPA&#30340;&#36793;&#30028;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32570;&#20047;&#29992;&#25143;&#24847;&#22270;&#29702;&#35299;&#12289;&#20219;&#21153;&#35268;&#21010;&#12289;&#24037;&#20855;&#20351;&#29992;&#21644;&#20010;&#20154;&#25968;&#25454;&#31649;&#29702;&#31561;&#33021;&#21147;&#65292;&#29616;&#26377;&#30340;IPA&#20173;&#28982;&#20855;&#26377;&#26377;&#38480;&#30340;&#23454;&#29992;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;&#26368;&#36817;&#65292;&#20197;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#20026;&#20195;&#34920;&#30340;&#22522;&#30784;&#27169;&#22411;&#30340;&#20986;&#29616;&#65292;&#20026;IPA&#30340;&#21457;&#23637;&#24102;&#26469;&#20102;&#26032;&#30340;&#26426;&#36935;&#12290;&#20511;&#21161;&#24378;&#22823;&#30340;&#35821;&#20041;&#29702;&#35299;&#21644;&#25512;&#29702;&#33021;&#21147;&#65292;LLM&#33021;&#22815;&#20351;&#26234;&#33021;&#20195;&#29702;&#33258;&#20027;&#35299;&#20915;&#22797;&#26434;&#38382;&#39064;&#12290;&#26412;&#25991;&#37325;&#28857;&#20851;&#27880;&#20010;&#20154;LLM&#20195;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Since the advent of personal computing devices, intelligent personal assistants (IPAs) have been one of the key technologies that researchers and engineers have focused on, aiming to help users efficiently obtain information and execute tasks, and provide users with more intelligent, convenient, and rich interaction experiences. With the development of smartphones and IoT, computing and sensing devices have become ubiquitous, greatly expanding the boundaries of IPAs. However, due to the lack of capabilities such as user intent understanding, task planning, tool using, and personal data management etc., existing IPAs still have limited practicality and scalability. Recently, the emergence of foundation models, represented by large language models (LLMs), brings new opportunities for the development of IPAs. With the powerful semantic understanding and reasoning capabilities, LLM can enable intelligent agents to solve complex problems autonomously. In this paper, we focus on Personal LLM
&lt;/p&gt;</description></item><item><title>CoLafier&#26159;&#19968;&#31181;&#20351;&#29992;&#23616;&#37096;&#20869;&#22312;&#32500;&#24230;&#65288;LID&#65289;&#36827;&#34892;&#24102;&#26377;&#22122;&#22768;&#26631;&#31614;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;&#23427;&#36890;&#36807;&#21033;&#29992;LID-dis&#21644;LID-gen&#20004;&#20010;&#23376;&#32593;&#32476;&#65292;&#20854;&#20013;LID-dis&#20351;&#29992;&#26679;&#26412;&#30340;&#29305;&#24449;&#21644;&#26631;&#31614;&#26469;&#39044;&#27979;&#26631;&#31614;&#65292;&#20135;&#29983;&#22686;&#24378;&#30340;&#20869;&#37096;&#34920;&#31034;&#12290;&#19982;LID-dis&#30456;&#21453;&#65292;LID-gen&#20165;&#20351;&#29992;&#26679;&#26412;&#30340;&#29305;&#24449;&#12290;CoLafier&#21033;&#29992;&#27599;&#20010;&#23454;&#20363;&#30340;&#20004;&#20010;&#22686;&#24378;&#35270;&#22270;&#21516;&#26102;&#36755;&#20837;&#20004;&#20010;&#23376;&#32593;&#32476;&#65292;&#21033;&#29992;LID&#20998;&#25968;&#26469;&#20998;&#37197;&#26631;&#31614;&#12290;</title><link>http://arxiv.org/abs/2401.05458</link><description>&lt;p&gt;
CoLafier: &#24102;&#26377;&#23616;&#37096;&#20869;&#22312;&#32500;&#24230;&#25351;&#23548;&#30340;&#21327;&#20316;&#22122;&#22768;&#26631;&#31614;&#20928;&#21270;&#22120;
&lt;/p&gt;
&lt;p&gt;
CoLafier: Collaborative Noisy Label Purifier With Local Intrinsic Dimensionality Guidance. (arXiv:2401.05458v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05458
&lt;/p&gt;
&lt;p&gt;
CoLafier&#26159;&#19968;&#31181;&#20351;&#29992;&#23616;&#37096;&#20869;&#22312;&#32500;&#24230;&#65288;LID&#65289;&#36827;&#34892;&#24102;&#26377;&#22122;&#22768;&#26631;&#31614;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;&#23427;&#36890;&#36807;&#21033;&#29992;LID-dis&#21644;LID-gen&#20004;&#20010;&#23376;&#32593;&#32476;&#65292;&#20854;&#20013;LID-dis&#20351;&#29992;&#26679;&#26412;&#30340;&#29305;&#24449;&#21644;&#26631;&#31614;&#26469;&#39044;&#27979;&#26631;&#31614;&#65292;&#20135;&#29983;&#22686;&#24378;&#30340;&#20869;&#37096;&#34920;&#31034;&#12290;&#19982;LID-dis&#30456;&#21453;&#65292;LID-gen&#20165;&#20351;&#29992;&#26679;&#26412;&#30340;&#29305;&#24449;&#12290;CoLafier&#21033;&#29992;&#27599;&#20010;&#23454;&#20363;&#30340;&#20004;&#20010;&#22686;&#24378;&#35270;&#22270;&#21516;&#26102;&#36755;&#20837;&#20004;&#20010;&#23376;&#32593;&#32476;&#65292;&#21033;&#29992;LID&#20998;&#25968;&#26469;&#20998;&#37197;&#26631;&#31614;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#20013;&#65292;&#22122;&#22768;&#26631;&#31614;&#24120;&#24120;&#24433;&#21709;&#20854;&#24615;&#33021;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;CoLafier&#65292;&#19968;&#31181;&#21033;&#29992;&#23616;&#37096;&#20869;&#22312;&#32500;&#24230;&#65288;LID&#65289;&#36827;&#34892;&#24102;&#26377;&#22122;&#22768;&#26631;&#31614;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#12290;CoLafier&#30001;&#20004;&#20010;&#23376;&#32593;&#32476;&#32452;&#25104;&#65306;LID-dis&#21644;LID-gen&#12290;LID-dis&#26159;&#19987;&#38376;&#30340;&#20998;&#31867;&#22120;&#65292;&#20351;&#29992;&#25105;&#20204;&#29420;&#29305;&#30340;&#26041;&#26696;&#36827;&#34892;&#35757;&#32451;&#65292;&#23427;&#21516;&#26102;&#20351;&#29992;&#26679;&#26412;&#30340;&#29305;&#24449;&#21644;&#26631;&#31614;&#26469;&#39044;&#27979;&#26631;&#31614;&#65292;&#20174;&#32780;&#20135;&#29983;&#22686;&#24378;&#30340;&#20869;&#37096;&#34920;&#31034;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#20174;&#36825;&#20010;&#34920;&#31034;&#35745;&#31639;&#20986;&#26469;&#30340;LID&#20998;&#25968;&#33021;&#22815;&#26377;&#25928;&#21306;&#20998;&#19981;&#21516;&#22122;&#22768;&#24773;&#20917;&#19979;&#30340;&#27491;&#30830;&#21644;&#38169;&#35823;&#26631;&#31614;&#12290;&#19982;LID-dis&#30456;&#21453;&#65292;LID-gen&#20316;&#20026;&#24120;&#35268;&#20998;&#31867;&#22120;&#65292;&#20165;&#20351;&#29992;&#26679;&#26412;&#30340;&#29305;&#24449;&#12290;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;CoLafier&#21033;&#29992;&#27599;&#20010;&#23454;&#20363;&#30340;&#20004;&#20010;&#22686;&#24378;&#35270;&#22270;&#21516;&#26102;&#36755;&#20837;&#20004;&#20010;&#23376;&#32593;&#32476;&#12290;CoLafier&#23558;&#26469;&#33258;LID-dis&#30340;&#20004;&#20010;&#35270;&#22270;&#30340;LID&#20998;&#25968;&#20316;&#20026;&#20998;&#37197;&#26631;&#31614;&#30340;&#32771;&#34385;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks (DNNs) have advanced many machine learning tasks, but their performance is often harmed by noisy labels in real-world data. Addressing this, we introduce CoLafier, a novel approach that uses Local Intrinsic Dimensionality (LID) for learning with noisy labels. CoLafier consists of two subnets: LID-dis and LID-gen. LID-dis is a specialized classifier. Trained with our uniquely crafted scheme, LID-dis consumes both a sample's features and its label to predict the label - which allows it to produce an enhanced internal representation. We observe that LID scores computed from this representation effectively distinguish between correct and incorrect labels across various noise scenarios. In contrast to LID-dis, LID-gen, functioning as a regular classifier, operates solely on the sample's features. During training, CoLafier utilizes two augmented views per instance to feed both subnets. CoLafier considers the LID scores from the two views as produced by LID-dis to assign 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32500;&#24230;&#24863;&#30693;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;DAO&#65292;&#36890;&#36807;&#20840;&#38754;&#23454;&#39564;&#39564;&#35777;&#20102;&#20854;&#22312;800&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#26174;&#33879;&#20248;&#20110;&#20854;&#20182;&#19977;&#31181;&#27969;&#34892;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.05453</link><description>&lt;p&gt;
&#32500;&#24230;&#24863;&#30693;&#30340;&#24322;&#24120;&#26816;&#27979;&#65306;&#29702;&#35770;&#21644;&#23454;&#39564;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Dimensionality-Aware Outlier Detection: Theoretical and Experimental Analysis. (arXiv:2401.05453v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05453
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32500;&#24230;&#24863;&#30693;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;DAO&#65292;&#36890;&#36807;&#20840;&#38754;&#23454;&#39564;&#39564;&#35777;&#20102;&#20854;&#22312;800&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#26174;&#33879;&#20248;&#20110;&#20854;&#20182;&#19977;&#31181;&#27969;&#34892;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#21442;&#25968;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20805;&#20998;&#32771;&#34385;&#20102;&#25968;&#25454;&#38598;&#20869;&#22312;&#32500;&#24230;&#30340;&#23616;&#37096;&#21464;&#21270;&#12290;&#36890;&#36807;&#20351;&#29992;&#23616;&#37096;&#20869;&#22312;&#32500;&#24230;&#65288;LID&#65289;&#29702;&#35770;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#19968;&#31181;&#32500;&#24230;&#24863;&#30693;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;DAO&#65292;&#23427;&#34987;&#25512;&#23548;&#20026;&#19968;&#20010;&#21253;&#21547;&#26597;&#35810;&#28857;&#21644;&#38543;&#26426;&#36873;&#25321;&#30340;&#36817;&#37051;&#30340;&#28176;&#36817;&#23616;&#37096;&#26399;&#26395;&#23494;&#24230;&#27604;&#30340;&#20272;&#35745;&#12290;DAO&#30340;&#32500;&#24230;&#24863;&#30693;&#34892;&#20026;&#26159;&#30001;&#20110;&#23427;&#20197;&#29702;&#35770;&#19978;&#35777;&#26126;&#30340;&#26041;&#24335;&#20351;&#29992;&#23616;&#37096;LID&#20540;&#30340;&#23616;&#37096;&#20272;&#35745;&#12290;&#36890;&#36807;&#23545;800&#22810;&#20010;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#30340;&#20840;&#38754;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;DAO&#26126;&#26174;&#20248;&#20110;&#19977;&#31181;&#27969;&#34892;&#19988;&#37325;&#35201;&#30340;&#22522;&#20934;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65306;&#23616;&#37096;&#31163;&#32676;&#22240;&#23376;&#65288;LOF&#65289;&#65292;&#31616;&#21270;&#29256;LOF&#21644;kNN&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a nonparametric method for outlier detection that takes full account of local variations in intrinsic dimensionality within the dataset. Using the theory of Local Intrinsic Dimensionality (LID), our 'dimensionality-aware' outlier detection method, DAO, is derived as an estimator of an asymptotic local expected density ratio involving the query point and a close neighbor drawn at random. The dimensionality-aware behavior of DAO is due to its use of local estimation of LID values in a theoretically-justified way. Through comprehensive experimentation on more than 800 synthetic and real datasets, we show that DAO significantly outperforms three popular and important benchmark outlier detection methods: Local Outlier Factor (LOF), Simplified LOF, and kNN.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#20351;&#29992;&#20102;Bloomberg&#37329;&#34701;&#24066;&#22330;&#25688;&#35201;&#25968;&#25454;&#38598;&#65292;&#21033;&#29992;ChatGPT&#21644;&#20004;&#38454;&#27573;&#25552;&#31034;&#26041;&#27861;&#65292;&#21457;&#29616;&#20840;&#29699;&#26032;&#38395;&#22836;&#26465;&#19982;&#32929;&#24066;&#36208;&#21183;&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#30340;&#24773;&#32490;&#30456;&#20851;&#24615;&#65292;&#35813;&#30456;&#20851;&#24615;&#22312;&#19981;&#21516;&#26102;&#38388;&#27573;&#20869;&#26377;&#25152;&#21464;&#21270;&#65292;&#24182;&#19988;&#22312;&#22810;&#20010;&#32929;&#31080;&#24066;&#22330;&#20013;&#22343;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#20010;&#24179;&#34913;&#26032;&#20449;&#24687;&#21453;&#24212;&#24615;&#21644;&#30456;&#20851;&#24615;&#30340;&#26368;&#20339;&#26102;&#38388;&#27573;&#30340;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2401.05447</link><description>&lt;p&gt;
ChatGPT&#33021;&#22815;&#20174;Bloomberg&#24066;&#22330;&#25688;&#35201;&#20013;&#35745;&#31639;&#21487;&#20449;&#30340;&#24773;&#32490;&#20998;&#25968;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can ChatGPT Compute Trustworthy Sentiment Scores from Bloomberg Market Wraps?. (arXiv:2401.05447v1 [q-fin.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05447
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#20351;&#29992;&#20102;Bloomberg&#37329;&#34701;&#24066;&#22330;&#25688;&#35201;&#25968;&#25454;&#38598;&#65292;&#21033;&#29992;ChatGPT&#21644;&#20004;&#38454;&#27573;&#25552;&#31034;&#26041;&#27861;&#65292;&#21457;&#29616;&#20840;&#29699;&#26032;&#38395;&#22836;&#26465;&#19982;&#32929;&#24066;&#36208;&#21183;&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#30340;&#24773;&#32490;&#30456;&#20851;&#24615;&#65292;&#35813;&#30456;&#20851;&#24615;&#22312;&#19981;&#21516;&#26102;&#38388;&#27573;&#20869;&#26377;&#25152;&#21464;&#21270;&#65292;&#24182;&#19988;&#22312;&#22810;&#20010;&#32929;&#31080;&#24066;&#22330;&#20013;&#22343;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#20010;&#24179;&#34913;&#26032;&#20449;&#24687;&#21453;&#24212;&#24615;&#21644;&#30456;&#20851;&#24615;&#30340;&#26368;&#20339;&#26102;&#38388;&#27573;&#30340;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20351;&#29992;&#20102;2010&#24180;&#33267;2023&#24180;&#30340;&#27599;&#26085;Bloomberg&#37329;&#34701;&#24066;&#22330;&#25688;&#35201;&#25968;&#25454;&#38598;&#65292;&#37325;&#26032;&#21457;&#24067;&#22312;&#22823;&#22411;&#37329;&#34701;&#23186;&#20307;&#19978;&#65292;&#21033;&#29992;ChatGPT&#21644;&#20004;&#38454;&#27573;&#25552;&#31034;&#26041;&#27861;&#30830;&#23450;&#20840;&#29699;&#26032;&#38395;&#22836;&#26465;&#22914;&#20309;&#24433;&#21709;&#32929;&#24066;&#36208;&#21183;&#12290;&#25105;&#20204;&#35760;&#24405;&#20102;&#24773;&#32490;&#20998;&#25968;&#19982;&#26410;&#26469;&#32929;&#24066;&#22238;&#25253;&#20043;&#38388;&#30340;&#32479;&#35745;&#26174;&#33879;&#27491;&#30456;&#20851;&#20851;&#31995;&#65292;&#36825;&#31181;&#20851;&#31995;&#22312;&#30701;&#26399;&#21040;&#20013;&#26399;&#20869;&#36716;&#21464;&#20026;&#36127;&#30456;&#20851;&#20851;&#31995;&#65292;&#28982;&#21518;&#38543;&#30528;&#26102;&#38388;&#30340;&#25512;&#31227;&#21448;&#21464;&#20026;&#36127;&#30456;&#20851;&#20851;&#31995;&#12290;&#36890;&#36807;&#27604;&#36739;Pearson&#21644;Spearman&#30456;&#20851;&#31995;&#25968;&#26469;&#39564;&#35777;&#36825;&#31181;&#30456;&#20851;&#27169;&#24335;&#22312;&#22810;&#20010;&#32929;&#31080;&#24066;&#22330;&#19978;&#30340;&#36866;&#29992;&#24615;&#65292;&#34920;&#26126;&#23427;&#22312;&#19981;&#21516;&#32929;&#31080;&#24066;&#22330;&#20013;&#30340;&#24377;&#24615;&#21644;&#38750;&#32447;&#24615;&#20013;&#37117;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#26368;&#20339;&#26102;&#38388;&#27573;&#30340;&#20272;&#35745;&#65292;&#20197;&#22312;&#26032;&#20449;&#24687;&#30340;&#21453;&#24212;&#24615;&#21644;&#30456;&#20851;&#24615;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
We used a dataset of daily Bloomberg Financial Market Summaries from 2010 to 2023, reposted on large financial media, to determine how global news headlines may affect stock market movements using ChatGPT and a two-stage prompt approach. We document a statistically significant positive correlation between the sentiment score and future equity market returns over short to medium term, which reverts to a negative correlation over longer horizons. Validation of this correlation pattern across multiple equity markets indicates its robustness across equity regions and resilience to non-linearity, evidenced by comparison of Pearson and Spearman correlations. Finally, we provide an estimate of the optimal horizon that strikes a balance between reactivity to new information and correlation.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#31995;&#32479;&#32508;&#36848;&#20102;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#22312;&#33041;&#30005;&#22270;&#20013;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#35774;&#35745;&#33391;&#22909;&#30340;&#39044;&#35757;&#32451;&#20219;&#21153;&#26469;&#25552;&#21462;&#26080;&#26631;&#31614;&#26679;&#26412;&#30340;&#34920;&#31034;&#65292;&#35299;&#20915;&#20102;&#33041;&#30005;&#20449;&#21495;&#26631;&#31614;&#30340;&#38382;&#39064;&#21644;&#20010;&#20307;&#20043;&#38388;&#30340;&#21464;&#21270;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2401.05446</link><description>&lt;p&gt;
&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#22312;&#33041;&#30005;&#22270;&#20013;&#30340;&#24212;&#29992;&#65306;&#31995;&#32479;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Self-supervised Learning for Electroencephalogram: A Systematic Survey. (arXiv:2401.05446v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05446
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#31995;&#32479;&#32508;&#36848;&#20102;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#22312;&#33041;&#30005;&#22270;&#20013;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#35774;&#35745;&#33391;&#22909;&#30340;&#39044;&#35757;&#32451;&#20219;&#21153;&#26469;&#25552;&#21462;&#26080;&#26631;&#31614;&#26679;&#26412;&#30340;&#34920;&#31034;&#65292;&#35299;&#20915;&#20102;&#33041;&#30005;&#20449;&#21495;&#26631;&#31614;&#30340;&#38382;&#39064;&#21644;&#20010;&#20307;&#20043;&#38388;&#30340;&#21464;&#21270;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#26159;&#19968;&#31181;&#38750;&#20405;&#20837;&#24615;&#30340;&#35760;&#24405;&#29983;&#29289;&#30005;&#20449;&#21495;&#30340;&#25216;&#26415;&#12290;&#36817;&#24180;&#26469;&#65292;&#23558;&#30417;&#30563;&#24335;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#19982;&#33041;&#30005;&#20449;&#21495;&#30456;&#32467;&#21512;&#65292;&#24050;&#32463;&#22312;&#21508;&#31181;&#22522;&#20110;&#33041;&#30005;&#30340;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#33258;&#21160;&#20998;&#26512;&#12290;&#28982;&#32780;&#65292;&#33041;&#30005;&#20449;&#21495;&#30340;&#26631;&#31614;&#38382;&#39064;&#38480;&#21046;&#20102;&#22522;&#20110;&#33041;&#30005;&#30340;&#28145;&#24230;&#27169;&#22411;&#30340;&#21457;&#23637;&#12290;&#33719;&#21462;&#33041;&#30005;&#27880;&#37322;&#26159;&#22256;&#38590;&#30340;&#65292;&#38656;&#35201;&#39046;&#22495;&#19987;&#23478;&#25351;&#23548;&#25910;&#38598;&#21644;&#26631;&#35760;&#65292;&#24182;&#19988;&#19981;&#21516;&#21463;&#35797;&#32773;&#20043;&#38388;&#30340;&#33041;&#30005;&#20449;&#21495;&#30340;&#21464;&#21270;&#20250;&#23548;&#33268;&#26174;&#33879;&#30340;&#26631;&#31614;&#20559;&#31227;&#12290;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#36890;&#36807;&#35774;&#35745;&#33391;&#22909;&#30340;&#39044;&#35757;&#32451;&#20219;&#21153;&#20174;&#26080;&#26631;&#31614;&#26679;&#26412;&#20013;&#25552;&#21462;&#34920;&#31034;&#12290;&#26412;&#25991;&#32858;&#28966;&#20110;&#23558;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#19982;&#26102;&#38388;&#24207;&#21015;&#33041;&#30005;&#20449;&#21495;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#39640;&#25928;&#30340;&#34920;&#31034;&#65292;&#24182;&#25552;&#20986;&#20102;&#23545;&#33041;&#30005;&#20449;&#21495;&#30340;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#30340;&#31995;&#32479;&#32508;&#36848;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;1&#65289;&#25105;&#20204;&#20171;&#32461;&#20102;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#30340;&#27010;&#24565;&#21644;&#29702;&#35770;&#65292;&#20197;&#21450;&#20856;&#22411;&#30340;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#12290;2&#65289;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20221;&#20840;&#38754;&#30340;&#32508;&#36848;&#65292;&#27010;&#36848;&#20102;&#30446;&#21069;&#30340;&#30740;&#31350;&#36827;&#23637;&#12289;&#26041;&#27861;&#21644;&#24212;&#29992;&#22330;&#26223;&#65292;&#24182;&#25351;&#20986;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Electroencephalogram (EEG) is a non-invasive technique to record bioelectrical signals. Integrating supervised deep learning techniques with EEG signals has recently facilitated automatic analysis across diverse EEG-based tasks. However, the label issues of EEG signals have constrained the development of EEG-based deep models. Obtaining EEG annotations is difficult that requires domain experts to guide collection and labeling, and the variability of EEG signals among different subjects causes significant label shifts. To solve the above challenges, self-supervised learning (SSL) has been proposed to extract representations from unlabeled samples through well-designed pretext tasks. This paper concentrates on integrating SSL frameworks with temporal EEG signals to achieve efficient representation and proposes a systematic review of the SSL for EEG signals. In this paper, 1) we introduce the concept and theory of self-supervised learning and typical SSL frameworks. 2) We provide a compre
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23618;&#20869;&#36830;&#25509;&#30340;&#20840;&#33033;&#20914;&#34892;&#20026;&#32593;&#32476;&#65292;&#21033;&#29992;&#29305;&#27530;&#30340;&#31070;&#32463;&#24418;&#24577;&#30828;&#20214;&#23454;&#29616;&#20102;&#36739;&#20302;&#33021;&#32791;&#30340;&#20154;&#24037;&#26234;&#33021;&#12290;&#35813;&#26041;&#27861;&#22312;&#23454;&#29616;&#25511;&#21046;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#21487;&#27604;&#36739;&#24615;&#33021;&#65292;&#24182;&#35299;&#20915;&#20102;&#20351;&#29992;&#33033;&#20914;&#39057;&#29575;&#20316;&#20026;&#36755;&#20986;&#25152;&#24102;&#26469;&#30340;&#28014;&#28857;&#30697;&#38453;&#36816;&#31639;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.05444</link><description>&lt;p&gt;
&#22522;&#20110;&#23618;&#20869;&#36830;&#25509;&#30340;&#20840;&#33033;&#20914;&#34892;&#20026;&#32593;&#32476;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Fully Spiking Actor Network with Intra-layer Connections for Reinforcement Learning. (arXiv:2401.05444v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05444
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23618;&#20869;&#36830;&#25509;&#30340;&#20840;&#33033;&#20914;&#34892;&#20026;&#32593;&#32476;&#65292;&#21033;&#29992;&#29305;&#27530;&#30340;&#31070;&#32463;&#24418;&#24577;&#30828;&#20214;&#23454;&#29616;&#20102;&#36739;&#20302;&#33021;&#32791;&#30340;&#20154;&#24037;&#26234;&#33021;&#12290;&#35813;&#26041;&#27861;&#22312;&#23454;&#29616;&#25511;&#21046;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#21487;&#27604;&#36739;&#24615;&#33021;&#65292;&#24182;&#35299;&#20915;&#20102;&#20351;&#29992;&#33033;&#20914;&#39057;&#29575;&#20316;&#20026;&#36755;&#20986;&#25152;&#24102;&#26469;&#30340;&#28014;&#28857;&#30697;&#38453;&#36816;&#31639;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20511;&#21161;&#29305;&#27530;&#30340;&#31070;&#32463;&#24418;&#24577;&#30828;&#20214;&#65292;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNN&#65289;&#34987;&#26399;&#26395;&#33021;&#20197;&#36739;&#20302;&#33021;&#37327;&#28040;&#32791;&#23454;&#29616;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#12290;&#36890;&#36807;&#23558;SNN&#19982;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#32467;&#21512;&#65292;&#20026;&#29616;&#23454;&#25511;&#21046;&#20219;&#21153;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#39640;&#33021;&#25928;&#26041;&#24335;&#12290;&#26412;&#25991;&#38024;&#23545;&#20195;&#29702;&#38656;&#35201;&#23398;&#20064;&#22810;&#32500;&#30830;&#23450;&#24615;&#31574;&#30053;&#20197;&#36827;&#34892;&#25511;&#21046;&#30340;&#20219;&#21153;&#36827;&#34892;&#30740;&#31350;&#65292;&#36825;&#22312;&#30495;&#23454;&#22330;&#26223;&#20013;&#38750;&#24120;&#24120;&#35265;&#12290;&#26368;&#36817;&#65292;&#26367;&#20195;&#26799;&#24230;&#26041;&#27861;&#24050;&#34987;&#29992;&#20110;&#35757;&#32451;&#22810;&#23618;SNN&#65292;&#22312;&#36825;&#20010;&#20219;&#21153;&#20013;&#20801;&#35768;SNNs&#23454;&#29616;&#19982;&#23545;&#24212;&#28145;&#24230;&#32593;&#32476;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#22522;&#20110;&#33033;&#20914;&#30340;RL&#26041;&#27861;&#23558;&#33033;&#20914;&#39057;&#29575;&#20316;&#20026;SNN&#30340;&#36755;&#20986;&#65292;&#24182;&#36890;&#36807;&#20840;&#36830;&#25509;&#65288;FC&#65289;&#23618;&#23558;&#20854;&#36716;&#25442;&#20026;&#34920;&#31034;&#36830;&#32493;&#21160;&#20316;&#31354;&#38388;&#65288;&#21363;&#30830;&#23450;&#24615;&#31574;&#30053;&#65289;&#12290;&#28982;&#32780;&#65292;&#33033;&#20914;&#39057;&#29575;&#30340;&#21313;&#36827;&#21046;&#29305;&#24615;&#20351;&#24471;FC&#23618;&#38656;&#35201;&#28014;&#28857;&#30697;&#38453;&#36816;&#31639;&#65292;&#20351;&#24471;&#25972;&#20010;SNN&#26080;&#27861;&#37096;&#32626;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the help of special neuromorphic hardware, spiking neural networks (SNNs) are expected to realize artificial intelligence (AI) with less energy consumption. It provides a promising energy-efficient way for realistic control tasks by combining SNNs with deep reinforcement learning (DRL). In this paper, we focus on the task where the agent needs to learn multi-dimensional deterministic policies to control, which is very common in real scenarios. Recently, the surrogate gradient method has been utilized for training multi-layer SNNs, which allows SNNs to achieve comparable performance with the corresponding deep networks in this task. Most existing spike-based RL methods take the firing rate as the output of SNNs, and convert it to represent continuous action space (i.e., the deterministic policy) through a fully-connected (FC) layer. However, the decimal characteristic of the firing rate brings the floating-point matrix operations to the FC layer, making the whole SNN unable to depl
&lt;/p&gt;</description></item><item><title>LLM4PLC&#36890;&#36807;&#21033;&#29992;&#29992;&#25143;&#21453;&#39304;&#21644;&#22806;&#37096;&#39564;&#35777;&#24037;&#20855;&#65292;&#22914;&#35821;&#27861;&#26816;&#26597;&#22120;&#12289;&#32534;&#35793;&#22120;&#21644;SMV&#39564;&#35777;&#22120;&#65292;&#26469;&#25351;&#23548;&#29983;&#25104;&#65292;&#21516;&#26102;&#37319;&#29992;&#25552;&#31034;&#24037;&#31243;&#21644;&#27169;&#22411;&#24494;&#35843;&#30340;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24037;&#19994;&#25511;&#21046;&#31995;&#32479;&#20013;&#21487;&#32534;&#31243;&#36923;&#36753;&#25511;&#21046;&#22120;&#65288;PLC&#65289;&#30340;&#29983;&#25104;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.05443</link><description>&lt;p&gt;
LLM4PLC&#65306;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#24037;&#19994;&#25511;&#21046;&#31995;&#32479;&#20013;&#21487;&#32534;&#31243;&#36923;&#36753;&#25511;&#21046;&#22120;&#65288;PLC&#65289;&#36827;&#34892;&#21487;&#39564;&#35777;&#32534;&#31243;
&lt;/p&gt;
&lt;p&gt;
LLM4PLC: Harnessing Large Language Models for Verifiable Programming of PLCs in Industrial Control Systems. (arXiv:2401.05443v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05443
&lt;/p&gt;
&lt;p&gt;
LLM4PLC&#36890;&#36807;&#21033;&#29992;&#29992;&#25143;&#21453;&#39304;&#21644;&#22806;&#37096;&#39564;&#35777;&#24037;&#20855;&#65292;&#22914;&#35821;&#27861;&#26816;&#26597;&#22120;&#12289;&#32534;&#35793;&#22120;&#21644;SMV&#39564;&#35777;&#22120;&#65292;&#26469;&#25351;&#23548;&#29983;&#25104;&#65292;&#21516;&#26102;&#37319;&#29992;&#25552;&#31034;&#24037;&#31243;&#21644;&#27169;&#22411;&#24494;&#35843;&#30340;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24037;&#19994;&#25511;&#21046;&#31995;&#32479;&#20013;&#21487;&#32534;&#31243;&#36923;&#36753;&#25511;&#21046;&#22120;&#65288;PLC&#65289;&#30340;&#29983;&#25104;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#33258;&#21160;&#21270;&#20195;&#30721;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#20027;&#23548;&#22320;&#20301;&#65292;&#20294;&#23427;&#20204;&#24182;&#19981;&#26159;&#27809;&#26377;&#32570;&#28857;&#12290;&#30456;&#20851;&#38382;&#39064;&#20027;&#35201;&#19982;&#29983;&#25104;&#30340;&#20195;&#30721;&#32570;&#20047;&#25191;&#34892;&#20445;&#35777;&#12289;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#20197;&#21450;&#23545;&#24517;&#35201;&#20294;&#23574;&#31471;&#32534;&#31243;&#35821;&#35328;&#30340;&#25903;&#25345;&#19981;&#36275;&#26377;&#20851;&#12290;&#30446;&#21069;&#30340;LLMs&#22914;GPT-4&#21644;LLaMa2&#26080;&#27861;&#20026;&#21487;&#32534;&#31243;&#36923;&#36753;&#25511;&#21046;&#22120;&#65288;PLC&#65289;&#25805;&#20316;&#30340;&#24037;&#19994;&#25511;&#21046;&#31995;&#32479;&#65288;ICS&#65289;&#29983;&#25104;&#26377;&#25928;&#30340;&#31243;&#24207;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;LLM4PLC&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#25143;&#24341;&#23548;&#30340;&#36845;&#20195;&#27969;&#31243;&#65292;&#21033;&#29992;&#29992;&#25143;&#21453;&#39304;&#21644;&#22806;&#37096;&#39564;&#35777;&#24037;&#20855;&#65288;&#21253;&#25324;&#35821;&#27861;&#26816;&#26597;&#22120;&#12289;&#32534;&#35793;&#22120;&#21644;SMV&#39564;&#35777;&#22120;&#65289;&#26469;&#25351;&#23548;LLM&#30340;&#29983;&#25104;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#36890;&#36807;&#37319;&#29992;&#25552;&#31034;&#24037;&#31243;&#21644;&#27169;&#22411;&#24494;&#35843;&#65288;LoRAs&#30340;&#21019;&#24314;&#21644;&#20351;&#29992;&#65289;&#26469;&#22686;&#24378;LLM&#30340;&#29983;&#25104;&#33021;&#21147;&#12290;&#25105;&#20204;&#20351;&#29992;FischerTechnik&#21046;&#36896;&#27979;&#35797;&#24202;&#65288;MFTB&#65289;&#39564;&#35777;&#20102;&#36825;&#20010;&#31995;&#32479;&#65292;&#23637;&#31034;&#20102;LLMs&#22914;&#20309;&#20174;&#29983;&#25104;&#32467;&#26500;&#26377;&#32570;&#38519;&#30340;&#20195;&#30721;&#28436;&#21464;&#20026;&#29983;&#25104;&#26377;&#25928;&#30340;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although Large Language Models (LLMs) have established pre-dominance in automated code generation, they are not devoid of shortcomings. The pertinent issues primarily relate to the absence of execution guarantees for generated code, a lack of explainability, and suboptimal support for essential but niche programming languages. State-of-the-art LLMs such as GPT-4 and LLaMa2 fail to produce valid programs for Industrial Control Systems (ICS) operated by Programmable Logic Controllers (PLCs). We propose LLM4PLC, a user-guided iterative pipeline leveraging user feedback and external verification tools including grammar checkers, compilers and SMV verifiers to guide the LLM's generation. We further enhance the generation potential of LLM by employing Prompt Engineering and model fine-tuning through the creation and usage of LoRAs. We validate this system using a FischerTechnik Manufacturing TestBed (MFTB), illustrating how LLMs can evolve from generating structurally flawed code to producin
&lt;/p&gt;</description></item><item><title>&#21151;&#33021;&#22270;&#27169;&#22411;&#65288;FGMs&#65289;&#36890;&#36807;&#32467;&#26500;&#23454;&#29616;&#20102;&#26679;&#26412;&#39640;&#25928;&#30340;&#25968;&#25454;&#39537;&#21160;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2401.05442</link><description>&lt;p&gt;
&#21151;&#33021;&#22270;&#27169;&#22411;&#65306;&#32467;&#26500;&#23454;&#29616;&#31163;&#32447;&#25968;&#25454;&#39537;&#21160;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Functional Graphical Models: Structure Enables Offline Data-Driven Optimization. (arXiv:2401.05442v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05442
&lt;/p&gt;
&lt;p&gt;
&#21151;&#33021;&#22270;&#27169;&#22411;&#65288;FGMs&#65289;&#36890;&#36807;&#32467;&#26500;&#23454;&#29616;&#20102;&#26679;&#26412;&#39640;&#25928;&#30340;&#25968;&#25454;&#39537;&#21160;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36890;&#24120;&#26159;&#20026;&#20102;&#35299;&#20915;&#39044;&#27979;&#38382;&#39064;&#32780;&#35757;&#32451;&#30340;&#65292;&#20294;&#25105;&#20204;&#32463;&#24120;&#24076;&#26395;&#23558;&#23427;&#20204;&#29992;&#20110;&#20248;&#21270;&#38382;&#39064;&#12290;&#20363;&#22914;&#65292;&#32473;&#23450;&#19968;&#32452;&#34507;&#30333;&#36136;&#21450;&#20854;&#23545;&#24212;&#30340;&#33639;&#20809;&#27700;&#24179;&#30340;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#21487;&#33021;&#24076;&#26395;&#20026;&#20855;&#26377;&#26368;&#39640;&#33639;&#20809;&#30340;&#26032;&#34507;&#30333;&#36136;&#36827;&#34892;&#20248;&#21270;&#12290;&#36825;&#31181;&#25968;&#25454;&#39537;&#21160;&#30340;&#20248;&#21270;&#65288;DDO&#65289;&#38754;&#20020;&#30528;&#19968;&#31995;&#21015;&#25361;&#25112;&#65292;&#36229;&#20986;&#20102;&#26631;&#20934;&#39044;&#27979;&#38382;&#39064;&#20013;&#30340;&#25361;&#25112;&#65292;&#22240;&#20026;&#25105;&#20204;&#38656;&#35201;&#25104;&#21151;&#39044;&#27979;&#22312;&#35757;&#32451;&#38598;&#20013;&#27809;&#26377;&#35265;&#36807;&#30340;&#20248;&#20110;&#26368;&#20339;&#35774;&#35745;&#30340;&#26032;&#35774;&#35745;&#30340;&#24615;&#33021;&#30340;&#27169;&#22411;&#12290;&#20174;&#29702;&#35770;&#19978;&#35762;&#65292;&#29978;&#33267;&#19981;&#28165;&#26970;&#29616;&#26377;&#26041;&#27861;&#20160;&#20040;&#26102;&#20505;&#29978;&#33267;&#33021;&#27604;&#31616;&#21333;&#22320;&#36873;&#25321;&#25968;&#25454;&#38598;&#20013;&#26368;&#20339;&#35774;&#35745;&#30340;&#26420;&#32032;&#26041;&#27861;&#25191;&#34892;&#24471;&#26356;&#22909;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22914;&#20309;&#36890;&#36807;&#32467;&#26500;&#23454;&#29616;&#39640;&#25928;&#30340;&#25968;&#25454;&#39537;&#21160;&#20248;&#21270;&#12290;&#20026;&#20102;&#24418;&#24335;&#21270;&#32467;&#26500;&#30340;&#27010;&#24565;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#21151;&#33021;&#22270;&#27169;&#22411;&#65288;FGMs&#65289;&#24182;&#20174;&#29702;&#35770;&#19978;&#23637;&#31034;&#20102;&#23427;&#20204;&#22914;&#20309;&#36890;&#36807;&#20998;&#35299;&#23454;&#29616;&#22522;&#20110;&#25968;&#25454;&#30340;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
While machine learning models are typically trained to solve prediction problems, we might often want to use them for optimization problems. For example, given a dataset of proteins and their corresponding fluorescence levels, we might want to optimize for a new protein with the highest possible fluorescence. This kind of data-driven optimization (DDO) presents a range of challenges beyond those in standard prediction problems, since we need models that successfully predict the performance of new designs that are better than the best designs seen in the training set. It is not clear theoretically when existing approaches can even perform better than the naive approach that simply selects the best design in the dataset. In this paper, we study how structure can enable sample-efficient data-driven optimization. To formalize the notion of structure, we introduce functional graphical models (FGMs) and show theoretically how they can provide for principled data-driven optimization by decomp
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#21487;&#31359;&#25140;&#24212;&#29992;&#20013;&#34920;&#31034;&#23398;&#20064;&#30340;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#22312;&#32570;&#22833;&#25968;&#25454;&#24773;&#20917;&#19979;&#12290;&#20316;&#32773;&#36890;&#36807;&#27604;&#36739;Transformer&#27169;&#22411;&#21644;&#32479;&#35745;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#21457;&#29616;Transformer&#27169;&#22411;&#22312;&#21464;&#21270;&#39057;&#32321;&#30340;&#20449;&#21495;&#30340;&#32570;&#22833;&#25968;&#25454;&#22635;&#20805;&#26041;&#38754;&#34920;&#29616;&#20248;&#31168;&#12290;&#27492;&#30740;&#31350;&#20026;&#22522;&#20110;&#25513;&#30721;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#20219;&#21153;&#30340;&#35774;&#35745;&#21644;&#24320;&#21457;&#25552;&#20379;&#20102;&#27934;&#23519;&#12290;</title><link>http://arxiv.org/abs/2401.05437</link><description>&lt;p&gt;
&#21487;&#31359;&#25140;&#24212;&#29992;&#20013;&#30340;&#34920;&#31034;&#23398;&#20064;&#65306;&#22312;&#32570;&#22833;&#25968;&#25454;&#24773;&#20917;&#19979;&#30340;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Representation Learning for Wearable-Based Applications in the Case of Missing Data. (arXiv:2401.05437v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05437
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#21487;&#31359;&#25140;&#24212;&#29992;&#20013;&#34920;&#31034;&#23398;&#20064;&#30340;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#22312;&#32570;&#22833;&#25968;&#25454;&#24773;&#20917;&#19979;&#12290;&#20316;&#32773;&#36890;&#36807;&#27604;&#36739;Transformer&#27169;&#22411;&#21644;&#32479;&#35745;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#21457;&#29616;Transformer&#27169;&#22411;&#22312;&#21464;&#21270;&#39057;&#32321;&#30340;&#20449;&#21495;&#30340;&#32570;&#22833;&#25968;&#25454;&#22635;&#20805;&#26041;&#38754;&#34920;&#29616;&#20248;&#31168;&#12290;&#27492;&#30740;&#31350;&#20026;&#22522;&#20110;&#25513;&#30721;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#20219;&#21153;&#30340;&#35774;&#35745;&#21644;&#24320;&#21457;&#25552;&#20379;&#20102;&#27934;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#31359;&#25140;&#35774;&#22791;&#25345;&#32493;&#25910;&#38598;&#20256;&#24863;&#22120;&#25968;&#25454;&#24182;&#29992;&#20110;&#25512;&#26029;&#20010;&#20307;&#30340;&#34892;&#20026;&#65292;&#22914;&#30561;&#30496;&#12289;&#20307;&#21147;&#27963;&#21160;&#21644;&#24773;&#32490;&#12290;&#23613;&#31649;&#22312;&#36825;&#20010;&#39046;&#22495;&#26377;&#24456;&#22823;&#30340;&#20852;&#36259;&#21644;&#36827;&#23637;&#65292;&#20294;&#30001;&#20110;&#25968;&#25454;&#36136;&#37327;&#20302;&#21644;&#25968;&#25454;&#27880;&#37322;&#26377;&#38480;&#65292;&#24314;&#27169;&#30495;&#23454;&#29615;&#22659;&#20013;&#30340;&#22810;&#27169;&#24335;&#20256;&#24863;&#22120;&#25968;&#25454;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#29992;&#20110;&#22635;&#20805;&#32570;&#22833;&#21487;&#31359;&#25140;&#25968;&#25454;&#30340;&#34920;&#31034;&#23398;&#20064;&#65292;&#24182;&#23558;&#20854;&#19982;&#26368;&#20808;&#36827;&#30340;&#32479;&#35745;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#20351;&#29992;10&#20010;&#29983;&#29702;&#21644;&#34892;&#20026;&#20449;&#21495;&#30340;&#21464;&#21270;&#29575;&#19981;&#21516;&#30340;&#25513;&#30721;&#27604;&#29575;&#65292;&#30740;&#31350;&#20102;Transformer&#27169;&#22411;&#22312;&#32570;&#22833;&#25968;&#25454;&#22635;&#20805;&#19978;&#30340;&#24615;&#33021;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;Transformer&#27169;&#22411;&#22312;&#21464;&#21270;&#39057;&#32321;&#30340;&#20449;&#21495;&#30340;&#32570;&#22833;&#25968;&#25454;&#22635;&#20805;&#26041;&#38754;&#20248;&#20110;&#22522;&#20934;&#27169;&#22411;&#65292;&#20294;&#23545;&#20110;&#21333;&#35843;&#20449;&#21495;&#21017;&#19981;&#28982;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#30740;&#31350;&#20102;&#22635;&#20805;&#31574;&#30053;&#21644;&#25513;&#30721;&#27604;&#29575;&#23545;&#19979;&#28216;&#20998;&#31867;&#20219;&#21153;&#30340;&#24433;&#21709;&#12290;&#26412;&#30740;&#31350;&#20026;&#22522;&#20110;&#25513;&#30721;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#20219;&#21153;&#30340;&#35774;&#35745;&#21644;&#24320;&#21457;&#25552;&#20379;&#20102;&#27934;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;
Wearable devices continuously collect sensor data and use it to infer an individual's behavior, such as sleep, physical activity, and emotions. Despite the significant interest and advancements in this field, modeling multimodal sensor data in real-world environments is still challenging due to low data quality and limited data annotations. In this work, we investigate representation learning for imputing missing wearable data and compare it with state-of-the-art statistical approaches. We investigate the performance of the transformer model on 10 physiological and behavioral signals with different masking ratios. Our results show that transformers outperform baselines for missing data imputation of signals that change more frequently, but not for monotonic signals. We further investigate the impact of imputation strategies and masking rations on downstream classification tasks. Our study provides insights for the design and development of masking-based self-supervised learning tasks a
&lt;/p&gt;</description></item><item><title>ECGformer&#26159;&#19968;&#31181;&#21033;&#29992;transformer&#26550;&#26500;&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#23545;&#24515;&#30005;&#22270;&#25968;&#25454;&#20013;&#30340;&#24515;&#24459;&#22833;&#24120;&#36827;&#34892;&#20998;&#31867;&#12290;</title><link>http://arxiv.org/abs/2401.05434</link><description>&lt;p&gt;
ECGformer: &#21033;&#29992;transformer&#36827;&#34892;&#24515;&#30005;&#22270;&#24515;&#25615;&#24515;&#24459;&#22833;&#24120;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
ECGformer: Leveraging transformer for ECG heartbeat arrhythmia classification. (arXiv:2401.05434v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05434
&lt;/p&gt;
&lt;p&gt;
ECGformer&#26159;&#19968;&#31181;&#21033;&#29992;transformer&#26550;&#26500;&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#23545;&#24515;&#30005;&#22270;&#25968;&#25454;&#20013;&#30340;&#24515;&#24459;&#22833;&#24120;&#36827;&#34892;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24515;&#24459;&#22833;&#24120;&#26159;&#24515;&#25615;&#19981;&#35268;&#21017;&#30340;&#19968;&#31181;&#24773;&#20917;&#65292;&#21487;&#20197;&#30001;&#24515;&#33039;&#19981;&#21516;&#21306;&#22495;&#24341;&#36215;&#65292;&#23548;&#33268;&#24515;&#25615;&#24555;&#36895;&#12289;&#32531;&#24930;&#25110;&#19981;&#35268;&#21017;&#12290;&#24515;&#30005;&#22270;&#26159;&#19968;&#31181;&#37325;&#35201;&#30340;&#35786;&#26029;&#24037;&#20855;&#65292;&#29992;&#20110;&#26816;&#27979;&#24515;&#33039;&#30340;&#19981;&#35268;&#21017;&#21644;&#24322;&#24120;&#65292;&#20197;&#20415;&#19987;&#23478;&#20998;&#26512;&#24515;&#33039;&#30340;&#30005;&#20449;&#21495;&#65292;&#35782;&#21035;&#22797;&#26434;&#30340;&#27169;&#24335;&#21644;&#20559;&#31163;&#27491;&#24120;&#30340;&#24773;&#20917;&#12290;&#22312;&#36807;&#21435;&#20960;&#21313;&#24180;&#20013;&#65292;&#24050;&#32463;&#36827;&#34892;&#20102;&#35768;&#22810;&#30740;&#31350;&#65292;&#20197;&#24320;&#21457;&#22522;&#20110;ECG&#25968;&#25454;&#30340;&#33258;&#21160;&#21270;&#24515;&#25615;&#20998;&#31867;&#26041;&#27861;&#12290;&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#23398;&#20064;&#22312;&#22788;&#29702;&#21508;&#31181;&#21307;&#23398;&#25361;&#25112;&#26041;&#38754;&#23637;&#31034;&#20986;&#21331;&#36234;&#30340;&#33021;&#21147;&#65292;&#29305;&#21035;&#26159;&#23558;transformer&#20316;&#20026;&#24207;&#21015;&#22788;&#29702;&#30340;&#27169;&#22411;&#26550;&#26500;&#12290;&#36890;&#36807;&#21033;&#29992;transformer&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;ECGformer&#27169;&#22411;&#65292;&#29992;&#20110;&#23545;&#24515;&#30005;&#22270;&#25968;&#25454;&#20013;&#23384;&#22312;&#30340;&#21508;&#31181;&#24515;&#24459;&#22833;&#24120;&#36827;&#34892;&#20998;&#31867;&#12290;&#25105;&#20204;&#20351;&#29992;MIT-BIH&#21644;PTB da
&lt;/p&gt;
&lt;p&gt;
An arrhythmia, also known as a dysrhythmia, refers to an irregular heartbeat. There are various types of arrhythmias that can originate from different areas of the heart, resulting in either a rapid, slow, or irregular heartbeat. An electrocardiogram (ECG) is a vital diagnostic tool used to detect heart irregularities and abnormalities, allowing experts to analyze the heart's electrical signals to identify intricate patterns and deviations from the norm. Over the past few decades, numerous studies have been conducted to develop automated methods for classifying heartbeats based on ECG data. In recent years, deep learning has demonstrated exceptional capabilities in tackling various medical challenges, particularly with transformers as a model architecture for sequence processing. By leveraging the transformers, we developed the ECGformer model for the classification of various arrhythmias present in electrocardiogram data. We assessed the suggested approach using the MIT-BIH and PTB da
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;DeBERTa&#27169;&#22411;&#65292;&#32467;&#21512;&#23545;&#25239;&#24615;&#35757;&#32451;&#21644;&#24230;&#37327;&#29305;&#23450;&#30340;&#27880;&#24847;&#21147;&#27744;&#21270;&#31561;&#21019;&#26032;&#25216;&#26415;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#33258;&#21160;&#35780;&#20998;&#24037;&#20855;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#36827;&#38024;&#23545;&#33521;&#35821;&#23398;&#20064;&#32773;&#30340;&#20889;&#20316;&#33021;&#21147;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2401.05433</link><description>&lt;p&gt;
&#36890;&#36807;&#23545;&#25239;&#24615;&#26435;&#37325;&#25200;&#21160;&#21644;&#24230;&#37327;&#29305;&#23450;&#30340;&#27880;&#24847;&#21147;&#27744;&#21270;&#22686;&#24378;&#35770;&#25991;&#35780;&#20998;
&lt;/p&gt;
&lt;p&gt;
Enhancing Essay Scoring with Adversarial Weights Perturbation and Metric-specific AttentionPooling. (arXiv:2401.05433v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05433
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;DeBERTa&#27169;&#22411;&#65292;&#32467;&#21512;&#23545;&#25239;&#24615;&#35757;&#32451;&#21644;&#24230;&#37327;&#29305;&#23450;&#30340;&#27880;&#24847;&#21147;&#27744;&#21270;&#31561;&#21019;&#26032;&#25216;&#26415;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#33258;&#21160;&#35780;&#20998;&#24037;&#20855;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#36827;&#38024;&#23545;&#33521;&#35821;&#23398;&#20064;&#32773;&#30340;&#20889;&#20316;&#33021;&#21147;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#24212;&#29992;&#25968;&#25454;&#31185;&#23398;&#25216;&#26415;&#65288;&#21253;&#25324;&#26426;&#22120;&#23398;&#20064;&#12289;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#25945;&#32946;&#25968;&#25454;&#20998;&#26512;&#65289;&#26469;&#25913;&#36827;&#38024;&#23545;&#33521;&#35821;&#23398;&#20064;&#32773;&#65288;ELLs&#65289;&#35774;&#35745;&#30340;&#33258;&#21160;&#21453;&#39304;&#24037;&#20855;&#12290;&#33258;&#21160;&#35770;&#25991;&#35780;&#20998;&#65288;AES&#65289;&#30740;&#31350;&#22312;&#35780;&#20272;&#20889;&#20316;&#35770;&#25991;&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#24448;&#24448;&#24573;&#35270;&#20102;&#33521;&#35821;&#23398;&#20064;&#32773;&#22312;&#35821;&#35328;&#21457;&#23637;&#26041;&#38754;&#30340;&#29305;&#23450;&#38656;&#27714;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#24212;&#29992;&#19982;BERT&#30456;&#20851;&#30340;&#25216;&#26415;&#26469;&#22686;&#24378;AES&#20013;&#23545;ELLs&#20889;&#20316;&#33021;&#21147;&#30340;&#35780;&#20272;&#12290;&#20026;&#20102;&#28385;&#36275;ELLs&#30340;&#29305;&#23450;&#38656;&#27714;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;DeBERTa&#65292;&#36825;&#26159;&#19968;&#31181;&#20808;&#36827;&#30340;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#65292;&#29992;&#20110;&#25913;&#36827;&#33258;&#21160;&#21453;&#39304;&#24037;&#20855;&#12290;DeBERTa&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#22823;&#22411;&#25991;&#26412;&#35821;&#26009;&#24211;&#19978;&#39044;&#35757;&#32451;&#65292;&#23398;&#20064;&#20102;&#36866;&#29992;&#20110;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#30340;&#36890;&#29992;&#35821;&#35328;&#34920;&#31034;&#12290;&#35813;&#27169;&#22411;&#32467;&#21512;&#20102;&#20960;&#31181;&#21019;&#26032;&#25216;&#26415;&#65292;&#21253;&#25324;&#36890;&#36807;&#23545;&#25239;&#35757;&#32451;&#23454;&#29616;
&lt;/p&gt;
&lt;p&gt;
The objective of this study is to improve automated feedback tools designed for English Language Learners (ELLs) through the utilization of data science techniques encompassing machine learning, natural language processing, and educational data analytics. Automated essay scoring (AES) research has made strides in evaluating written essays, but it often overlooks the specific needs of English Language Learners (ELLs) in language development. This study explores the application of BERT-related techniques to enhance the assessment of ELLs' writing proficiency within AES.  To address the specific needs of ELLs, we propose the use of DeBERTa, a state-of-the-art neural language model, for improving automated feedback tools. DeBERTa, pretrained on large text corpora using self-supervised learning, learns universal language representations adaptable to various natural language understanding tasks. The model incorporates several innovative techniques, including adversarial training through Adve
&lt;/p&gt;</description></item><item><title>TEN-GUARD&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24352;&#37327;&#20998;&#35299;&#26041;&#27861;&#26816;&#27979;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#21518;&#38376;&#25915;&#20987;&#30340;&#26032;&#26041;&#27861;&#65292;&#30456;&#23545;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#20855;&#26377;&#22810;&#20010;&#20248;&#21183;&#65292;&#21253;&#25324;&#33021;&#22815;&#21516;&#26102;&#20998;&#26512;&#22810;&#20010;&#27169;&#22411;&#65292;&#22312;&#21508;&#31181;&#32593;&#32476;&#26550;&#26500;&#19978;&#24037;&#20316;&#65292;&#19981;&#23545;&#35302;&#21457;&#22120;&#30340;&#24615;&#36136;&#20570;&#20219;&#20309;&#20551;&#35774;&#65292;&#24182;&#19988;&#35745;&#31639;&#25928;&#29575;&#39640;&#12290;</title><link>http://arxiv.org/abs/2401.05432</link><description>&lt;p&gt;
TEN-GUARD: &#20351;&#29992;&#24352;&#37327;&#20998;&#35299;&#26816;&#27979;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#21518;&#38376;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
TEN-GUARD: Tensor Decomposition for Backdoor Attack Detection in Deep Neural Networks. (arXiv:2401.05432v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05432
&lt;/p&gt;
&lt;p&gt;
TEN-GUARD&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24352;&#37327;&#20998;&#35299;&#26041;&#27861;&#26816;&#27979;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#21518;&#38376;&#25915;&#20987;&#30340;&#26032;&#26041;&#27861;&#65292;&#30456;&#23545;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#20855;&#26377;&#22810;&#20010;&#20248;&#21183;&#65292;&#21253;&#25324;&#33021;&#22815;&#21516;&#26102;&#20998;&#26512;&#22810;&#20010;&#27169;&#22411;&#65292;&#22312;&#21508;&#31181;&#32593;&#32476;&#26550;&#26500;&#19978;&#24037;&#20316;&#65292;&#19981;&#23545;&#35302;&#21457;&#22120;&#30340;&#24615;&#36136;&#20570;&#20219;&#20309;&#20551;&#35774;&#65292;&#24182;&#19988;&#35745;&#31639;&#25928;&#29575;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21644;&#29992;&#20110;&#35757;&#32451;&#23427;&#20204;&#30340;&#25968;&#25454;&#38598;&#36234;&#26469;&#36234;&#22823;&#65292;&#23558;&#23427;&#20204;&#38598;&#25104;&#21040;&#30740;&#31350;&#21644;&#21830;&#19994;&#39033;&#30446;&#20013;&#30340;&#40664;&#35748;&#26041;&#27861;&#26159;&#19979;&#36733;&#39044;&#35757;&#32451;&#27169;&#22411;&#24182;&#36827;&#34892;&#24494;&#35843;&#12290;&#20294;&#26159;&#36825;&#20123;&#27169;&#22411;&#30340;&#26469;&#28304;&#19981;&#30830;&#23450;&#65292;&#21487;&#33021;&#23384;&#22312;&#38544;&#34255;&#30340;&#24694;&#24847;&#34892;&#20026;&#65292;&#22914;&#29305;&#27931;&#20234;&#26408;&#39532;&#25110;&#21518;&#38376;&#65292;&#20854;&#20013;&#23545;&#36755;&#20837;&#36827;&#34892;&#23567;&#30340;&#25913;&#21464;&#65288;&#35302;&#21457;&#22120;&#65289;&#21487;&#33021;&#23548;&#33268;&#27169;&#22411;&#20135;&#29983;&#38169;&#35823;&#30340;&#36755;&#20986;&#65288;&#20363;&#22914;&#65292;&#35823;&#20998;&#31867;&#65289;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21518;&#38376;&#26816;&#27979;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#20004;&#31181;&#24212;&#29992;&#20110;&#32593;&#32476;&#28608;&#27963;&#30340;&#24352;&#37327;&#20998;&#35299;&#26041;&#27861;&#12290;&#30456;&#23545;&#20110;&#29616;&#26377;&#30340;&#26816;&#27979;&#26041;&#27861;&#65292;&#36825;&#31181;&#26041;&#27861;&#20855;&#26377;&#22810;&#20010;&#20248;&#21183;&#65292;&#21253;&#25324;&#33021;&#22815;&#21516;&#26102;&#20998;&#26512;&#22810;&#20010;&#27169;&#22411;&#65292;&#22312;&#21508;&#31181;&#32593;&#32476;&#26550;&#26500;&#19978;&#24037;&#20316;&#65292;&#19981;&#23545;&#29992;&#20110;&#25913;&#21464;&#32593;&#32476;&#34892;&#20026;&#30340;&#35302;&#21457;&#22120;&#30340;&#24615;&#36136;&#20570;&#20219;&#20309;&#20551;&#35774;&#65292;&#24182;&#19988;&#35745;&#31639;&#25928;&#29575;&#39640;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#23545;&#26816;&#27979;&#27969;&#31243;&#30340;&#35814;&#32454;&#25551;&#36848;&#20197;&#21450;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
As deep neural networks and the datasets used to train them get larger, the default approach to integrating them into research and commercial projects is to download a pre-trained model and fine tune it. But these models can have uncertain provenance, opening up the possibility that they embed hidden malicious behavior such as trojans or backdoors, where small changes to an input (triggers) can cause the model to produce incorrect outputs (e.g., to misclassify). This paper introduces a novel approach to backdoor detection that uses two tensor decomposition methods applied to network activations. This has a number of advantages relative to existing detection methods, including the ability to analyze multiple models at the same time, working across a wide variety of network architectures, making no assumptions about the nature of triggers used to alter network behavior, and being computationally efficient. We provide a detailed description of the detection pipeline along with results on 
&lt;/p&gt;</description></item><item><title>TRLS&#26159;&#19968;&#31181;&#36890;&#36807;&#22768;&#35889;&#22270;&#36827;&#34892;&#26102;&#38388;&#24207;&#21015;&#34920;&#31034;&#23398;&#20064;&#30340;&#21307;&#23398;&#20449;&#21495;&#22788;&#29702;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#26102;&#38388;&#39057;&#29575;RNN&#20174;&#22686;&#24378;&#22768;&#35889;&#22270;&#20013;&#25552;&#21462;&#20986;&#26356;&#22810;&#20449;&#24687;&#65292;&#24182;&#22312;&#21307;&#23398;&#20449;&#21495;&#20998;&#31867;&#20013;&#26174;&#31034;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.05431</link><description>&lt;p&gt;
TRLS:&#19968;&#31181;&#22522;&#20110;&#22768;&#35889;&#22270;&#30340;&#21307;&#23398;&#20449;&#21495;&#22788;&#29702;&#26102;&#38388;&#24207;&#21015;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
TRLS: A Time Series Representation Learning Framework via Spectrogram for Medical Signal Processing. (arXiv:2401.05431v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05431
&lt;/p&gt;
&lt;p&gt;
TRLS&#26159;&#19968;&#31181;&#36890;&#36807;&#22768;&#35889;&#22270;&#36827;&#34892;&#26102;&#38388;&#24207;&#21015;&#34920;&#31034;&#23398;&#20064;&#30340;&#21307;&#23398;&#20449;&#21495;&#22788;&#29702;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#26102;&#38388;&#39057;&#29575;RNN&#20174;&#22686;&#24378;&#22768;&#35889;&#22270;&#20013;&#25552;&#21462;&#20986;&#26356;&#22810;&#20449;&#24687;&#65292;&#24182;&#22312;&#21307;&#23398;&#20449;&#21495;&#20998;&#31867;&#20013;&#26174;&#31034;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26080;&#26631;&#31614;&#26102;&#38388;&#24207;&#21015;&#20013;&#65292;&#25552;&#20986;&#20102;&#29992;&#20110;&#21307;&#23398;&#20449;&#21495;&#22788;&#29702;&#30340;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#12290;&#23613;&#31649;&#22312;&#20043;&#21069;&#30340;&#24037;&#20316;&#20013;&#21462;&#24471;&#20102;&#35768;&#22810;&#21331;&#36234;&#36827;&#23637;&#65292;&#20294;&#25105;&#20204;&#35266;&#23519;&#21040;&#25552;&#21462;&#30340;&#26102;&#38388;&#24207;&#21015;&#34920;&#31034;&#20173;&#28982;&#19981;&#33021;&#24456;&#22909;&#22320;&#27867;&#21270;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22768;&#35889;&#22270;&#36827;&#34892;&#26102;&#38388;&#24207;&#21015;&#65288;&#21307;&#23398;&#20449;&#21495;&#65289;&#34920;&#31034;&#23398;&#20064;&#30340;&#26694;&#26550;&#65288;TRLS&#65289;&#65292;&#20197;&#33719;&#24471;&#26356;&#22810;&#20449;&#24687;&#30340;&#34920;&#31034;&#12290;&#25105;&#20204;&#23558;&#36755;&#20837;&#30340;&#26102;&#22495;&#21307;&#23398;&#20449;&#21495;&#36716;&#21270;&#20026;&#22768;&#35889;&#22270;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#31216;&#20026;&#26102;&#38388;&#39057;&#29575;RNN&#65288;TFRNN&#65289;&#30340;&#26102;&#39057;&#32534;&#30721;&#22120;&#65292;&#20174;&#22686;&#24378;&#30340;&#22768;&#35889;&#22270;&#20013;&#25429;&#25417;&#26356;&#31283;&#20581;&#30340;&#22810;&#23610;&#24230;&#34920;&#31034;&#12290;&#25105;&#20204;&#30340;TRLS&#20197;&#22768;&#35889;&#22270;&#20316;&#20026;&#36755;&#20837;&#65292;&#20855;&#26377;&#20004;&#31181;&#19981;&#21516;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#24335;&#65292;&#24182;&#26368;&#22823;&#21270;&#27491;&#26679;&#26412;&#20043;&#38388;&#30340;&#30456;&#20284;&#24230;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#35268;&#36991;&#20102;&#35774;&#35745;&#36127;&#26679;&#26412;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#23545;&#22235;&#20010;&#30495;&#23454;&#21307;&#23398;&#20449;&#21495;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#21307;&#23398;&#20449;&#21495;&#20998;&#31867;&#35780;&#20272;&#34920;&#26126;&#65292;TRLS&#20248;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Representation learning frameworks in unlabeled time series have been proposed for medical signal processing. Despite the numerous excellent progresses have been made in previous works, we observe the representation extracted for the time series still does not generalize well. In this paper, we present a Time series (medical signal) Representation Learning framework via Spectrogram (TRLS) to get more informative representations. We transform the input time-domain medical signals into spectrograms and design a time-frequency encoder named Time Frequency RNN (TFRNN) to capture more robust multi-scale representations from the augmented spectrograms. Our TRLS takes spectrogram as input with two types of different data augmentations and maximizes the similarity between positive ones, which effectively circumvents the problem of designing negative samples. Our evaluation of four real-world medical signal datasets focusing on medical signal classification shows that TRLS is superior to the ex
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23454;&#29992;&#30340;&#26694;&#26550;&#29992;&#20110;&#22312;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#20219;&#21153;&#20013;&#39640;&#25928;&#21033;&#29992;&#25968;&#25454;&#65292;&#21516;&#26102;&#32771;&#34385;&#20256;&#24863;&#22120;&#27169;&#24577;&#21644;&#37319;&#26679;&#29575;&#30340;&#20248;&#21270;&#65292;&#36890;&#36807;&#35774;&#35745;&#30340;&#21487;&#35757;&#32451;&#21442;&#25968;&#26469;&#25351;&#23548;&#20256;&#24863;&#22120;&#27169;&#24577;&#21644;&#37319;&#26679;&#29575;&#30340;&#36873;&#25321;&#12290;</title><link>http://arxiv.org/abs/2401.05426</link><description>&lt;p&gt;
CoSS&#65306;&#38024;&#23545;&#25968;&#25454;&#39640;&#25928;AI&#30340;&#20256;&#24863;&#22120;&#21644;&#37319;&#26679;&#29575;&#20248;&#21270;&#22312;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
CoSS: Co-optimizing Sensor and Sampling Rate for Data-Efficient AI in Human Activity Recognition. (arXiv:2401.05426v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05426
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23454;&#29992;&#30340;&#26694;&#26550;&#29992;&#20110;&#22312;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#20219;&#21153;&#20013;&#39640;&#25928;&#21033;&#29992;&#25968;&#25454;&#65292;&#21516;&#26102;&#32771;&#34385;&#20256;&#24863;&#22120;&#27169;&#24577;&#21644;&#37319;&#26679;&#29575;&#30340;&#20248;&#21270;&#65292;&#36890;&#36807;&#35774;&#35745;&#30340;&#21487;&#35757;&#32451;&#21442;&#25968;&#26469;&#25351;&#23548;&#20256;&#24863;&#22120;&#27169;&#24577;&#21644;&#37319;&#26679;&#29575;&#30340;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#31070;&#32463;&#32593;&#32476;&#25216;&#26415;&#30340;&#36827;&#27493;&#26174;&#33879;&#25552;&#39640;&#20102;&#21033;&#29992;&#22810;&#20010;&#26102;&#38388;&#24207;&#21015;&#20256;&#24863;&#22120;&#36827;&#34892;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#30340;&#25928;&#26524;&#12290;&#34429;&#28982;&#20351;&#29992;&#22823;&#37327;&#20256;&#24863;&#22120;&#21644;&#39640;&#37319;&#26679;&#29575;&#36890;&#24120;&#21487;&#20197;&#25552;&#39640;&#32467;&#26524;&#65292;&#20294;&#24448;&#24448;&#20250;&#23548;&#33268;&#25968;&#25454;&#20302;&#25928;&#21644;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#30340;&#19981;&#24517;&#35201;&#25193;&#23637;&#65292;&#32473;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#30340;&#23454;&#38469;&#24212;&#29992;&#24102;&#26469;&#25361;&#25112;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#24341;&#20837;&#20102;&#19968;&#20010;&#23454;&#29992;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;HAR&#20219;&#21153;&#20013;&#30340;&#25968;&#25454;&#39640;&#25928;&#21033;&#29992;&#65292;&#21516;&#26102;&#32771;&#34385;&#20256;&#24863;&#22120;&#27169;&#24577;&#21644;&#37319;&#26679;&#29575;&#30340;&#20248;&#21270;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#26680;&#24515;&#26159;&#35774;&#35745;&#30340;&#21487;&#35757;&#32451;&#21442;&#25968;&#65292;&#31216;&#20026;&#8220;&#26435;&#37325;&#20998;&#25968;&#8221;&#65292;&#23427;&#20204;&#35780;&#20272;&#35757;&#32451;&#38454;&#27573;&#20013;&#27599;&#20010;&#20256;&#24863;&#22120;&#27169;&#24577;&#21644;&#37319;&#26679;&#29575;&#30340;&#37325;&#35201;&#24615;&#12290;&#36825;&#20123;&#20998;&#25968;&#25351;&#23548;&#20256;&#24863;&#22120;&#27169;&#24577;&#21644;&#37319;&#26679;&#29575;&#30340;&#36873;&#25321;&#12290;&#20462;&#21098;&#26041;&#27861;&#20801;&#35768;&#29992;&#25143;&#22312;&#35745;&#31639;&#39044;&#31639;&#21644;&#24615;&#33021;&#20043;&#38388;&#36827;&#34892;&#26435;&#34913;&#65292;&#26681;&#25454;&#36873;&#25321;&#20256;&#24863;&#22120;&#27169;&#24577;&#21644;&#37319;&#26679;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in Artificial Neural Networks have significantly improved human activity recognition using multiple time-series sensors. While employing numerous sensors with high-frequency sampling rates usually improves the results, it often leads to data inefficiency and unnecessary expansion of the ANN, posing a challenge for their practical deployment on edge devices. Addressing these issues, our work introduces a pragmatic framework for data-efficient utilization in HAR tasks, considering the optimization of both sensor modalities and sampling rate simultaneously. Central to our approach are the designed trainable parameters, termed 'Weight Scores,' which assess the significance of each sensor modality and sampling rate during the training phase. These scores guide the sensor modalities and sampling rate selection. The pruning method allows users to make a trade-off between computational budgets and performance by selecting the sensor modalities and sampling rates according t
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#27627;&#31859;&#27874;&#39057;&#27573;&#30340;&#20998;&#24067;&#24335;&#22810;&#36755;&#20837;&#22810;&#36755;&#20986;(D-MIMO)&#31995;&#32479;&#20013;&#65292;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#36741;&#21161;&#36827;&#34892;&#27874;&#26463;&#31649;&#29702;&#65292;&#24182;&#35777;&#26126;&#20102;&#27874;&#26463;&#25512;&#26029;&#30340;&#24615;&#33021;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2401.05422</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;(Machine Learning)&#36741;&#21161;&#19979;&#30340;&#27627;&#31859;&#27874;&#20998;&#24067;&#24335;&#22810;&#36755;&#20837;&#22810;&#36755;&#20986;(D-MIMO)&#31995;&#32479;&#20013;&#30340;&#27874;&#26463;&#31649;&#29702;
&lt;/p&gt;
&lt;p&gt;
Machine Learning (ML)-assisted Beam Management in millimeter (mm)Wave Distributed Multiple Input Multiple Output (D-MIMO) systems. (arXiv:2401.05422v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05422
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#27627;&#31859;&#27874;&#39057;&#27573;&#30340;&#20998;&#24067;&#24335;&#22810;&#36755;&#20837;&#22810;&#36755;&#20986;(D-MIMO)&#31995;&#32479;&#20013;&#65292;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#36741;&#21161;&#36827;&#34892;&#27874;&#26463;&#31649;&#29702;&#65292;&#24182;&#35777;&#26126;&#20102;&#27874;&#26463;&#25512;&#26029;&#30340;&#24615;&#33021;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27874;&#26463;&#31649;&#29702;&#21327;&#35758;&#23545;&#20110;&#24314;&#31435;&#21644;&#32500;&#25252;&#32593;&#32476;&#26080;&#32447;&#30005;&#33410;&#28857;&#19982;&#29992;&#25143;&#35774;&#22791;(UEs)&#20043;&#38388;&#30340;&#36830;&#25509;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#20998;&#24067;&#24335;&#22810;&#36755;&#20837;&#22810;&#36755;&#20986;(D-MIMO)&#31995;&#32479;&#20013;&#65292;&#19968;&#20123;&#30001;&#20013;&#22830;&#22788;&#29702;&#21333;&#20803;(CPU)&#21327;&#35843;&#30340;&#25509;&#20837;&#28857;(AP)&#20026;&#19968;&#20123;UEs&#25552;&#20379;&#26381;&#21153;&#12290;&#22312;&#27627;&#31859;&#27874;&#39057;&#27573;&#65292;&#30001;&#20110;&#38656;&#35201;&#21457;&#36865;&#19979;&#34892;(DL)&#21442;&#32771;&#20449;&#21495;&#26469;&#25506;&#27979;&#22823;&#37327;&#27874;&#26463;&#30340;&#38382;&#39064;&#65292;&#25214;&#21040;&#26368;&#20339;&#30340;AP&#21644;&#27874;&#26463;&#20197;&#20026;UEs&#25552;&#20379;&#26381;&#21153;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#26412;&#25991;&#30340;&#30446;&#26631;&#26159;&#36890;&#36807;&#20165;&#23545;&#23569;&#37327;&#27874;&#26463;&#36827;&#34892;&#25506;&#27979;&#65292;&#24182;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;/&#26426;&#22120;&#23398;&#20064;&#25512;&#26029;&#26368;&#20339;&#27874;&#26463;/AP&#26159;&#21542;&#21487;&#38752;&#12290;&#25105;&#20204;&#20351;&#29992;&#38543;&#26426;&#26862;&#26519;(Random Forest), &#32570;&#22833;&#27169;&#24335;&#26862;&#26519;(MissForest)&#21644;&#26465;&#20214;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;(conditional Generative Adversarial Networks, c-GAN)&#26469;&#23637;&#31034;&#25512;&#26029;&#30340;&#24615;&#33021;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Beam management (BM) protocols are critical for establishing and maintaining connectivity between network radio nodes and User Equipments (UEs). In Distributed Multiple Input Multiple Output systems (D-MIMO), a number of access points (APs), coordinated by a central processing unit (CPU), serves a number of UEs. At mmWave frequencies, the problem of finding the best AP and beam to serve the UEs is challenging due to a large number of beams that need to be sounded with Downlink (DL) reference signals. The objective of this paper is to investigate whether the best AP/beam can be reliably inferred from sounding only a small subset of beams and leveraging AI/ML for inference of best beam/AP. We use Random Forest (RF), MissForest (MF) and conditional Generative Adversarial Networks (c-GAN) for demonstrating the performance benefits of inference.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#23558;&#38477;&#32500;&#21644;&#20915;&#31574;&#36793;&#30028;&#24212;&#29992;&#20110;&#29616;&#26377;&#30340;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#65292;&#31361;&#26174;&#25968;&#25454;&#20013;&#30340;&#27169;&#24335;&#21644;&#32858;&#31867;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;&#36825;&#31181;&#26041;&#27861;&#22312;&#25552;&#39640;&#36712;&#36857;&#26631;&#35760;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.05418</link><description>&lt;p&gt;
ANALYTiC: &#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#20915;&#31574;&#36793;&#30028;&#21644;&#38477;&#32500;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
ANALYTiC: Understanding Decision Boundaries and Dimensionality Reduction in Machine Learning. (arXiv:2401.05418v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05418
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#23558;&#38477;&#32500;&#21644;&#20915;&#31574;&#36793;&#30028;&#24212;&#29992;&#20110;&#29616;&#26377;&#30340;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#65292;&#31361;&#26174;&#25968;&#25454;&#20013;&#30340;&#27169;&#24335;&#21644;&#32858;&#31867;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;&#36825;&#31181;&#26041;&#27861;&#22312;&#25552;&#39640;&#36712;&#36857;&#26631;&#35760;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32039;&#20945;&#20415;&#25658;&#35774;&#22791;&#30340;&#20986;&#29616;&#32473;&#25105;&#20204;&#24102;&#26469;&#20102;&#19968;&#25209;&#21487;&#20197;&#29992;&#20110;&#25512;&#26029;&#36235;&#21183;&#21644;&#27169;&#24335;&#30340;&#36319;&#36394;&#36816;&#21160;&#25968;&#25454;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#29616;&#26377;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#22522;&#30784;&#19978;&#24212;&#29992;&#38477;&#32500;&#21644;&#20915;&#31574;&#36793;&#30028;&#30340;&#32452;&#21512;&#65292;&#31361;&#26174;&#25968;&#25454;&#20013;&#30340;&#27169;&#24335;&#21644;&#32858;&#31867;&#12290;&#25105;&#20204;&#20351;&#29992;&#19977;&#20010;&#19981;&#21516;&#30340;&#36712;&#36857;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#23454;&#39564;&#20998;&#26512;&#65292;&#26088;&#22312;&#21033;&#29992;&#24050;&#26631;&#35760;&#30340;&#25968;&#25454;&#24182;&#25552;&#39640;&#23427;&#20204;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#23637;&#31034;&#20102;&#36825;&#20123;&#32452;&#21512;&#26041;&#27861;&#22312;&#25552;&#39640;&#36712;&#36857;&#26631;&#35760;&#30340;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;&#26412;&#30740;&#31350;&#20026;&#23558;&#26426;&#22120;&#23398;&#20064;&#21644;&#35270;&#35273;&#26041;&#27861;&#22312;&#19978;&#19979;&#25991;&#20013;&#26356;&#24191;&#27867;&#22320;&#38598;&#25104;&#25171;&#19979;&#20102;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
The advent of compact, handheld devices has given us a pool of tracked movement data that could be used to infer trends and patterns that can be made to use. With this flooding of various trajectory data of animals, humans, vehicles, etc., the idea of ANALYTiC originated, using active learning to infer semantic annotations from the trajectories by learning from sets of labeled data. This study explores the application of dimensionality reduction and decision boundaries in combination with the already present active learning, highlighting patterns and clusters in data. We test these features with three different trajectory datasets with objective of exploiting the the already labeled data and enhance their interpretability. Our experimental analysis exemplifies the potential of these combined methodologies in improving the efficiency and accuracy of trajectory labeling. This study serves as a stepping-stone towards the broader integration of machine learning and visual methods in contex
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;WDSNet&#30340;&#23567;&#27874;&#21160;&#24577;&#36873;&#25321;&#32593;&#32476;&#65292;&#29992;&#20110;&#26234;&#33021;&#36873;&#25321;&#36866;&#24403;&#30340;&#23567;&#27874;&#22522;&#20989;&#25968;&#20197;&#22686;&#24378;&#24815;&#24615;&#20256;&#24863;&#22120;&#20449;&#21495;&#12290;&#27492;&#22806;&#65292;&#35770;&#25991;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#31867;&#21035;&#34920;&#31034;&#26426;&#21046;&#65292;&#29992;&#20110;&#25552;&#39640;&#23567;&#27874;&#22522;&#20989;&#25968;&#30340;&#36873;&#25321;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.05416</link><description>&lt;p&gt;
&#24815;&#24615;&#20256;&#24863;&#22120;&#20449;&#21495;&#22686;&#24378;&#30340;&#23567;&#27874;&#21160;&#24577;&#36873;&#25321;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Wavelet Dynamic Selection Network for Inertial Sensor Signal Enhancement. (arXiv:2401.05416v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05416
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;WDSNet&#30340;&#23567;&#27874;&#21160;&#24577;&#36873;&#25321;&#32593;&#32476;&#65292;&#29992;&#20110;&#26234;&#33021;&#36873;&#25321;&#36866;&#24403;&#30340;&#23567;&#27874;&#22522;&#20989;&#25968;&#20197;&#22686;&#24378;&#24815;&#24615;&#20256;&#24863;&#22120;&#20449;&#21495;&#12290;&#27492;&#22806;&#65292;&#35770;&#25991;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#31867;&#21035;&#34920;&#31034;&#26426;&#21046;&#65292;&#29992;&#20110;&#25552;&#39640;&#23567;&#27874;&#22522;&#20989;&#25968;&#30340;&#36873;&#25321;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#23039;&#24577;&#21644;&#36816;&#21160;&#24863;&#24212;&#37096;&#20214;&#65292;&#24815;&#24615;&#20256;&#24863;&#22120;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#31181;&#20415;&#25658;&#35774;&#22791;&#20013;&#12290;&#28982;&#32780;&#65292;&#24815;&#24615;&#20256;&#24863;&#22120;&#30340;&#20005;&#37325;&#35823;&#24046;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#21151;&#33021;&#65292;&#29305;&#21035;&#26159;&#36712;&#36857;&#24674;&#22797;&#21644;&#35821;&#20041;&#35782;&#21035;&#12290;&#23567;&#27874;&#20316;&#20026;&#20027;&#27969;&#30340;&#20449;&#21495;&#22788;&#29702;&#26041;&#27861;&#65292;&#30001;&#20110;&#20016;&#23500;&#22810;&#26679;&#30340;&#23567;&#27874;&#22522;&#20989;&#25968;&#32780;&#34987;&#35465;&#20026;&#20449;&#21495;&#30340;&#25968;&#23398;&#26174;&#24494;&#38236;&#12290;&#28982;&#32780;&#65292;&#24815;&#24615;&#20256;&#24863;&#22120;&#30340;&#22797;&#26434;&#22122;&#22768;&#31867;&#22411;&#21644;&#24212;&#29992;&#22330;&#26223;&#20351;&#24471;&#36873;&#25321;&#36866;&#24403;&#30340;&#23567;&#27874;&#22522;&#20989;&#25968;&#21464;&#24471;&#22256;&#38590;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23567;&#27874;&#21160;&#24577;&#36873;&#25321;&#32593;&#32476;&#65288;WDSNet&#65289;&#65292;&#23427;&#33021;&#26234;&#33021;&#22320;&#20026;&#21487;&#21464;&#24815;&#24615;&#20449;&#21495;&#36873;&#25321;&#36866;&#24403;&#30340;&#23567;&#27874;&#22522;&#20989;&#25968;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#25797;&#38271;&#20174;&#36755;&#20837;&#25968;&#25454;&#20013;&#25552;&#21462;&#29305;&#24449;&#65292;&#20294;&#24573;&#35270;&#20102;&#23398;&#20064;&#30446;&#26631;&#31867;&#21035;&#30340;&#29305;&#24449;&#65292;&#36825;&#23545;&#20110;&#25552;&#39640;&#31867;&#21035;&#24863;&#30693;&#33021;&#21147;&#12289;&#25913;&#21892;&#23567;&#27874;&#22522;&#20989;&#25968;&#30340;&#36873;&#25321;&#26159;&#24517;&#35201;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31867;&#21035;&#34920;&#31034;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
As attitude and motion sensing components, inertial sensors are widely used in various portable devices. But the severe errors of inertial sensors restrain their function, especially the trajectory recovery and semantic recognition. As a mainstream signal processing method, wavelet is hailed as the mathematical microscope of signal due to the plentiful and diverse wavelet basis functions. However, complicated noise types and application scenarios of inertial sensors make selecting wavelet basis perplexing. To this end, we propose a wavelet dynamic selection network (WDSNet), which intelligently selects the appropriate wavelet basis for variable inertial signals. In addition, existing deep learning architectures excel at extracting features from input data but neglect to learn the characteristics of target categories, which is essential to enhance the category awareness capability, thereby improving the selection of wavelet basis. Therefore, we propose a category representation mechanis
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#20351;&#29992;&#31232;&#30095;&#24815;&#24615;&#27979;&#37327;&#21333;&#20803;(IMUs)&#22312;&#20154;&#20307;&#19978;&#36827;&#34892;&#36816;&#21160;&#37325;&#24314;&#65292;&#36890;&#36807;&#24341;&#20837;&#25991;&#26412;&#35821;&#20041;&#36827;&#34892;&#20256;&#24863;&#22120;&#25968;&#25454;&#23545;&#40784;&#21644;&#29305;&#24449;&#21152;&#26435;&#65292;&#23454;&#29616;&#20102;&#31934;&#30830;&#30340;&#36816;&#21160;&#37325;&#24314;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#25351;&#26631;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2401.05412</link><description>&lt;p&gt;
&#31354;&#38388;&#30456;&#20851;&#20256;&#24863;&#22120;&#30340;&#37325;&#35201;&#24615;&#65306;&#21033;&#29992;&#25991;&#26412;&#35821;&#20041;&#36741;&#21161;&#30340;&#19977;&#32500;&#20154;&#20307;&#36816;&#21160;&#37325;&#24314;
&lt;/p&gt;
&lt;p&gt;
Spatial-Related Sensors Matters: 3D Human Motion Reconstruction Assisted with Textual Semantics. (arXiv:2401.05412v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05412
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#20351;&#29992;&#31232;&#30095;&#24815;&#24615;&#27979;&#37327;&#21333;&#20803;(IMUs)&#22312;&#20154;&#20307;&#19978;&#36827;&#34892;&#36816;&#21160;&#37325;&#24314;&#65292;&#36890;&#36807;&#24341;&#20837;&#25991;&#26412;&#35821;&#20041;&#36827;&#34892;&#20256;&#24863;&#22120;&#25968;&#25454;&#23545;&#40784;&#21644;&#29305;&#24449;&#21152;&#26435;&#65292;&#23454;&#29616;&#20102;&#31934;&#30830;&#30340;&#36816;&#21160;&#37325;&#24314;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#25351;&#26631;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#21487;&#31359;&#25140;&#35774;&#22791;&#36827;&#34892;&#36816;&#21160;&#37325;&#24314;&#24050;&#25104;&#20026;&#19968;&#31181;&#32463;&#27982;&#19988;&#21487;&#34892;&#30340;&#25216;&#26415;&#12290;&#26576;&#20123;&#26041;&#27861;&#21033;&#29992;&#20154;&#20307;&#19978;&#30340;&#31232;&#30095;&#24815;&#24615;&#27979;&#37327;&#21333;&#20803;(IMUs)&#24182;&#37319;&#29992;&#25968;&#25454;&#39537;&#21160;&#30340;&#31574;&#30053;&#26469;&#24314;&#27169;&#20154;&#20307;&#23039;&#21183;&#12290;&#28982;&#32780;&#65292;&#20165;&#22522;&#20110;&#31232;&#30095;&#30340;IMUs&#25968;&#25454;&#36827;&#34892;&#36816;&#21160;&#37325;&#24314;&#22312;&#26412;&#36136;&#19978;&#23384;&#22312;&#27169;&#31946;&#24615;&#65292;&#36825;&#26159;&#30001;&#20110;&#22823;&#37327;&#30456;&#21516;&#30340;IMU&#35835;&#25968;&#23545;&#24212;&#20110;&#19981;&#21516;&#30340;&#23039;&#21183;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22810;&#20256;&#24863;&#22120;&#30340;&#31354;&#38388;&#37325;&#35201;&#24615;&#65292;&#21463;&#21040;&#25551;&#36848;&#29305;&#23450;&#21160;&#20316;&#30340;&#25991;&#26412;&#30340;&#30417;&#30563;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19981;&#30830;&#23450;&#24615;&#26469;&#20026;&#27599;&#20010;IMU&#27966;&#29983;&#21152;&#26435;&#29305;&#24449;&#12290;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;&#19968;&#20010;&#20998;&#23618;&#26102;&#24207;&#21464;&#25442;&#22120;(HTT)&#65292;&#24182;&#24212;&#29992;&#23545;&#27604;&#23398;&#20064;&#26469;&#23454;&#29616;&#20256;&#24863;&#22120;&#25968;&#25454;&#19982;&#25991;&#26412;&#35821;&#20041;&#30340;&#31934;&#30830;&#26102;&#24207;&#21644;&#29305;&#24449;&#23545;&#40784;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#22810;&#20010;&#25351;&#26631;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#36890;&#36807;&#25991;&#26412;&#30417;&#30563;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#20934;&#30830;&#30340;&#36816;&#21160;&#37325;&#24314;&#12290;
&lt;/p&gt;
&lt;p&gt;
Leveraging wearable devices for motion reconstruction has emerged as an economical and viable technique. Certain methodologies employ sparse Inertial Measurement Units (IMUs) on the human body and harness data-driven strategies to model human poses. However, the reconstruction of motion based solely on sparse IMUs data is inherently fraught with ambiguity, a consequence of numerous identical IMU readings corresponding to different poses. In this paper, we explore the spatial importance of multiple sensors, supervised by text that describes specific actions. Specifically, uncertainty is introduced to derive weighted features for each IMU. We also design a Hierarchical Temporal Transformer (HTT) and apply contrastive learning to achieve precise temporal and feature alignment of sensor data with textual semantics. Experimental results demonstrate our proposed approach achieves significant improvements in multiple metrics compared to existing methods. Notably, with textual supervision, our
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#38024;&#23545;&#33041;&#30005;&#22270;&#25968;&#25454;&#20013;&#30340;&#20266;&#36857;&#26816;&#27979;&#21644;&#20998;&#31867;&#65292;&#23545;&#21313;&#19968;&#20010;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#22312;&#20845;&#31181;&#24120;&#29992;&#34920;&#31034;&#26041;&#27861;&#19978;&#36827;&#34892;&#20102;&#27604;&#36739;&#20998;&#26512;&#65292;&#24182;&#21457;&#29616;&#26576;&#20123;&#34920;&#31034;&#26041;&#27861;&#22312;&#31361;&#20986;&#25968;&#25454;&#30340;&#20449;&#22122;&#27604;&#26041;&#38754;&#26356;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2401.05409</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#20687;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#34920;&#31034;&#65306;&#33041;&#30005;&#22270;&#20266;&#36857;&#26816;&#27979;&#30340;&#27604;&#36739;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Image-based Data Representations of Time Series: A Comparative Analysis in EEG Artifact Detection. (arXiv:2401.05409v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05409
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#38024;&#23545;&#33041;&#30005;&#22270;&#25968;&#25454;&#20013;&#30340;&#20266;&#36857;&#26816;&#27979;&#21644;&#20998;&#31867;&#65292;&#23545;&#21313;&#19968;&#20010;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#22312;&#20845;&#31181;&#24120;&#29992;&#34920;&#31034;&#26041;&#27861;&#19978;&#36827;&#34892;&#20102;&#27604;&#36739;&#20998;&#26512;&#65292;&#24182;&#21457;&#29616;&#26576;&#20123;&#34920;&#31034;&#26041;&#27861;&#22312;&#31361;&#20986;&#25968;&#25454;&#30340;&#20449;&#22122;&#27604;&#26041;&#38754;&#26356;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26367;&#20195;&#25968;&#25454;&#34920;&#31034;&#26159;&#22686;&#24378;&#19979;&#28216;&#27169;&#22411;&#24615;&#33021;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#22312;&#26426;&#22120;&#23398;&#20064;&#24037;&#20855;&#31665;&#20013;&#23384;&#22312;&#22823;&#37327;&#36825;&#26679;&#30340;&#34920;&#31034;&#26041;&#27861;&#65292;&#39046;&#22495;&#20869;&#32570;&#20047;&#23545;&#27599;&#31181;&#34920;&#31034;&#26041;&#27861;&#30340;&#36866;&#29992;&#24615;&#30340;&#27604;&#36739;&#29702;&#35299;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#33041;&#30005;&#22270;&#25968;&#25454;&#20013;&#30340;&#20266;&#36857;&#26816;&#27979;&#21644;&#20998;&#31867;&#20316;&#20026;&#27979;&#35797;&#24179;&#21488;&#65292;&#35780;&#20272;&#20102;&#20845;&#31181;&#24120;&#29992;&#34920;&#31034;&#26041;&#27861;&#19978;&#30340;&#21313;&#19968;&#20010;&#27969;&#34892;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#34429;&#28982;&#34920;&#31034;&#30340;&#36873;&#25321;&#28041;&#21450;&#20559;&#24046;&#21644;&#26041;&#24046;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#20294;&#26576;&#20123;&#34920;&#31034;&#22312;&#31361;&#20986;&#25968;&#25454;&#30340;&#20449;&#22122;&#27604;&#26041;&#38754;&#23454;&#38469;&#19978;&#26356;&#26377;&#25928;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#38024;&#23545;&#33041;&#30005;&#22270;&#25968;&#25454;&#30340;&#32467;&#26524;&#65292;&#24182;&#24320;&#28304;&#20102;&#25105;&#20204;&#30340;&#27979;&#35797;&#26694;&#26550;&#65292;&#20197;&#20415;&#26410;&#26469;&#36827;&#34892;&#36825;&#31181;&#26041;&#27861;&#30340;&#27604;&#36739;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Alternative data representations are powerful tools that augment the performance of downstream models. However, there is an abundance of such representations within the machine learning toolbox, and the field lacks a comparative understanding of the suitability of each representation method.  In this paper, we propose artifact detection and classification within EEG data as a testbed for profiling image-based data representations of time series data. We then evaluate eleven popular deep learning architectures on each of six commonly-used representation methods.  We find that, while the choice of representation entails a choice within the tradeoff between bias and variance, certain representations are practically more effective in highlighting features which increase the signal-to-noise ratio of the data. We present our results on EEG data, and open-source our testing framework to enable future comparative analyses in this vein.
&lt;/p&gt;</description></item><item><title>RFRL Gym&#26159;&#19968;&#20010;&#29992;&#20110;&#35748;&#30693;&#26080;&#32447;&#30005;&#24212;&#29992;&#30340;&#24378;&#21270;&#23398;&#20064;&#27979;&#35797;&#24179;&#21488;&#65292;&#21487;&#20197;&#24110;&#21161;&#24320;&#21457;&#21644;&#27979;&#35797;RFRL&#25216;&#26415;&#65292;&#27169;&#25311;&#26080;&#32447;&#30005;&#39057;&#35889;&#29615;&#22659;&#65292;&#24182;&#23454;&#39564;&#19981;&#21516;&#30340;&#39057;&#35889;&#24863;&#30693;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2401.05406</link><description>&lt;p&gt;
RFRL Gym: &#29992;&#20110;&#35748;&#30693;&#26080;&#32447;&#30005;&#24212;&#29992;&#30340;&#24378;&#21270;&#23398;&#20064;&#27979;&#35797;&#24179;&#21488;
&lt;/p&gt;
&lt;p&gt;
RFRL Gym: A Reinforcement Learning Testbed for Cognitive Radio Applications. (arXiv:2401.05406v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05406
&lt;/p&gt;
&lt;p&gt;
RFRL Gym&#26159;&#19968;&#20010;&#29992;&#20110;&#35748;&#30693;&#26080;&#32447;&#30005;&#24212;&#29992;&#30340;&#24378;&#21270;&#23398;&#20064;&#27979;&#35797;&#24179;&#21488;&#65292;&#21487;&#20197;&#24110;&#21161;&#24320;&#21457;&#21644;&#27979;&#35797;RFRL&#25216;&#26415;&#65292;&#27169;&#25311;&#26080;&#32447;&#30005;&#39057;&#35889;&#29615;&#22659;&#65292;&#24182;&#23454;&#39564;&#19981;&#21516;&#30340;&#39057;&#35889;&#24863;&#30693;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#32447;&#30005;&#39057;&#29575;&#24378;&#21270;&#23398;&#20064;&#65288;RFRL&#65289;&#39044;&#35745;&#23558;&#25104;&#20026;&#19979;&#19968;&#20195;&#26080;&#32447;&#36890;&#20449;&#31995;&#32479;&#65288;&#29305;&#21035;&#26159;6G&#21644;&#19979;&#19968;&#20195;&#20891;&#20107;&#36890;&#20449;&#65289;&#20013;&#24191;&#27867;&#24212;&#29992;&#30340;&#25216;&#26415;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#33268;&#21147;&#20110;&#24320;&#21457;&#19968;&#20010;&#24037;&#20855;&#65292;&#20197;&#20419;&#36827;&#21033;&#29992;&#39057;&#35889;&#24863;&#30693;&#30340;RFRL&#25216;&#26415;&#30340;&#21457;&#23637;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#35813;&#24037;&#20855;&#26088;&#22312;&#35299;&#20915;&#20004;&#20010;&#35748;&#30693;&#26080;&#32447;&#30005;&#24212;&#29992;&#65292;&#21363;&#21160;&#24577;&#39057;&#35889;&#25509;&#20837;&#21644;&#24178;&#25200;&#12290;&#20026;&#20102;&#35757;&#32451;&#21644;&#27979;&#35797;&#36825;&#20123;&#24212;&#29992;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#31639;&#27861;&#65292;&#38656;&#35201;&#19968;&#20010;&#27169;&#25311;&#29615;&#22659;&#26469;&#27169;&#25311;&#26080;&#32447;&#30005;&#39057;&#35889;&#20013;&#20195;&#29702;&#20154;&#23558;&#36935;&#21040;&#30340;&#26465;&#20214;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#36825;&#26679;&#19968;&#20010;&#29615;&#22659;&#65292;&#31216;&#20026;RFRL Gym&#12290;&#36890;&#36807;RFRL Gym&#65292;&#29992;&#25143;&#21487;&#20197;&#35774;&#35745;&#33258;&#24049;&#30340;&#22330;&#26223;&#26469;&#27169;&#25311;RL&#20195;&#29702;&#20154;&#22312;&#26080;&#32447;&#30005;&#39057;&#35889;&#20013;&#21487;&#33021;&#36935;&#21040;&#30340;&#24773;&#20917;&#65292;&#24182;&#23581;&#35797;&#19981;&#21516;&#30340;&#39057;&#35889;&#24863;&#30693;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Radio Frequency Reinforcement Learning (RFRL) is anticipated to be a widely applicable technology in the next generation of wireless communication systems, particularly 6G and next-gen military communications. Given this, our research is focused on developing a tool to promote the development of RFRL techniques that leverage spectrum sensing. In particular, the tool was designed to address two cognitive radio applications, specifically dynamic spectrum access and jamming. In order to train and test reinforcement learning (RL) algorithms for these applications, a simulation environment is necessary to simulate the conditions that an agent will encounter within the Radio Frequency (RF) spectrum. In this paper, such an environment has been developed, herein referred to as the RFRL Gym. Through the RFRL Gym, users can design their own scenarios to model what an RL agent may encounter within the RF spectrum as well as experiment with different spectrum sensing techniques. Additionally, the 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545;&#26089;&#26399;&#20799;&#31461;&#25945;&#32946;&#20013;&#20851;&#38190;AI&#25216;&#26415;&#36827;&#34892;&#20102;&#26368;&#26032;&#21644;&#28145;&#20837;&#30340;&#32508;&#36848;&#65292;&#21253;&#25324;&#22522;&#20110;AI&#30340;&#26426;&#22120;&#20154;&#21644;AI&#25216;&#26415;&#25913;&#21892;&#33258;&#38381;&#30151;&#20799;&#31461;&#30340;&#31038;&#20132;&#20114;&#21160;&#31561;&#12290;&#36825;&#23545;&#21021;&#23398;&#32773;&#26469;&#35828;&#26159;&#19968;&#20221;&#36866;&#29992;&#30340;&#32508;&#36848;&#26448;&#26009;&#12290;</title><link>http://arxiv.org/abs/2401.05403</link><description>&lt;p&gt;
&#26089;&#26399;&#20799;&#31461;&#25945;&#32946;&#20013;&#30340;&#20851;&#38190;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#65306;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
The Key Artificial Intelligence Technologies in Early Childhood Education: A Review. (arXiv:2401.05403v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05403
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;&#26089;&#26399;&#20799;&#31461;&#25945;&#32946;&#20013;&#20851;&#38190;AI&#25216;&#26415;&#36827;&#34892;&#20102;&#26368;&#26032;&#21644;&#28145;&#20837;&#30340;&#32508;&#36848;&#65292;&#21253;&#25324;&#22522;&#20110;AI&#30340;&#26426;&#22120;&#20154;&#21644;AI&#25216;&#26415;&#25913;&#21892;&#33258;&#38381;&#30151;&#20799;&#31461;&#30340;&#31038;&#20132;&#20114;&#21160;&#31561;&#12290;&#36825;&#23545;&#21021;&#23398;&#32773;&#26469;&#35828;&#26159;&#19968;&#20221;&#36866;&#29992;&#30340;&#32508;&#36848;&#26448;&#26009;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#25216;&#26415;&#24050;&#34987;&#24212;&#29992;&#20110;&#21508;&#20010;&#39046;&#22495;&#65292;&#21253;&#25324;&#26089;&#26399;&#20799;&#31461;&#25945;&#32946;&#65288;ECE&#65289;&#12290;AI&#25945;&#32946;&#25216;&#26415;&#30340;&#25972;&#21512;&#26159;ECE&#39046;&#22495;&#30340;&#19968;&#20010;&#37325;&#35201;&#36235;&#21183;&#12290;&#30446;&#21069;&#65292;&#20851;&#20110;ECE&#20013;&#30340;AI&#30740;&#31350;&#36234;&#26469;&#36234;&#22810;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#36824;&#32570;&#23569;&#35752;&#35770;ECE&#20013;AI&#30740;&#31350;&#30340;&#32508;&#36848;&#25991;&#31456;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#26089;&#26399;&#20799;&#31461;&#25945;&#32946;&#20013;&#20851;&#38190;AI&#25216;&#26415;&#30340;&#26368;&#26032;&#21644;&#28145;&#20837;&#30340;&#27010;&#36848;&#65292;&#20174;&#21382;&#21490;&#35282;&#24230;&#20986;&#21457;&#65292;&#24635;&#32467;&#20102;&#20195;&#34920;&#24615;&#30340;&#30740;&#31350;&#20316;&#21697;&#65292;&#27010;&#36848;&#20102;&#24320;&#25918;&#24615;&#38382;&#39064;&#65292;&#35752;&#35770;&#20102;&#36235;&#21183;&#21644;&#25361;&#25112;&#65292;&#24182;&#36890;&#36807;&#35814;&#32454;&#30340;&#25991;&#29486;&#35745;&#37327;&#20998;&#26512;&#25552;&#20379;&#20102;&#28145;&#20837;&#30340;&#30740;&#31350;&#24314;&#35758;&#12290;&#26412;&#25991;&#20027;&#35201;&#35752;&#35770;&#20102;&#23558;&#22522;&#20110;AI&#30340;&#26426;&#22120;&#20154;&#21644;AI&#25216;&#26415;&#24212;&#29992;&#20110;ECE&#30340;&#30740;&#31350;&#65292;&#21253;&#25324;&#25913;&#21892;&#33258;&#38381;&#30151;&#35889;&#31995;&#38556;&#30861;&#20799;&#31461;&#30340;&#31038;&#20132;&#20114;&#21160;&#12290;&#26412;&#25991;&#20026;&#21021;&#23398;&#32773;&#25552;&#20379;&#20102;&#19968;&#20221;&#36866;&#29992;&#30340;&#26368;&#26032;&#21644;&#28145;&#20837;&#30340;&#35843;&#30740;&#32508;&#36848;&#26448;&#26009;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial Intelligence (AI) technologies have been applied in various domains, including early childhood education (ECE). Integration of AI educational technology is a recent significant trend in ECE. Currently, there are more and more studies of AI in ECE. To date, there is a lack of survey articles that discuss the studies of AI in ECE. In this paper, we provide an up-to-date and in-depth overview of the key AI technologies in ECE that provides a historical perspective, summarizes the representative works, outlines open questions, discusses the trends and challenges through a detailed bibliometric analysis, and provides insightful recommendations for future research. We mainly discuss the studies that apply AI-based robots and AI technologies to ECE, including improving the social interaction of children with an autism spectrum disorder. This paper significantly contributes to provide an up-to-date and in-depth survey that is suitable as introductory material for beginners to AI in 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21521;&#37327;&#22330;&#23450;&#21521;&#25193;&#25955;&#27169;&#22411;&#30340;&#26230;&#20307;&#26448;&#26009;&#29983;&#25104;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;&#20960;&#20309;&#31561;&#21464;GNN&#21516;&#26102;&#32771;&#34385;&#21407;&#23376;&#20301;&#32622;&#21644;&#26230;&#20307;&#26230;&#26684;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#33021;&#22815;&#26356;&#20840;&#38754;&#35780;&#20272;&#27169;&#22411;&#33021;&#21147;&#30340;&#26032;&#29983;&#25104;&#25351;&#26631;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#25193;&#25955;&#27169;&#22411;&#30340;&#37325;&#35201;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.05402</link><description>&lt;p&gt;
&#22522;&#20110;&#21521;&#37327;&#22330;&#23450;&#21521;&#25193;&#25955;&#27169;&#22411;&#30340;&#26230;&#20307;&#26448;&#26009;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Vector Field Oriented Diffusion Model for Crystal Material Generation. (arXiv:2401.05402v1 [cond-mat.mtrl-sci])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05402
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21521;&#37327;&#22330;&#23450;&#21521;&#25193;&#25955;&#27169;&#22411;&#30340;&#26230;&#20307;&#26448;&#26009;&#29983;&#25104;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;&#20960;&#20309;&#31561;&#21464;GNN&#21516;&#26102;&#32771;&#34385;&#21407;&#23376;&#20301;&#32622;&#21644;&#26230;&#20307;&#26230;&#26684;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#33021;&#22815;&#26356;&#20840;&#38754;&#35780;&#20272;&#27169;&#22411;&#33021;&#21147;&#30340;&#26032;&#29983;&#25104;&#25351;&#26631;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#25193;&#25955;&#27169;&#22411;&#30340;&#37325;&#35201;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26448;&#26009;&#31185;&#23398;&#20013;&#65292;&#21457;&#29616;&#20855;&#26377;&#29305;&#23450;&#21270;&#23398;&#24615;&#36136;&#30340;&#26230;&#20307;&#32467;&#26500;&#24050;&#25104;&#20026;&#36234;&#26469;&#36234;&#37325;&#35201;&#30340;&#30740;&#31350;&#28966;&#28857;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#27169;&#22411;&#22312;&#29983;&#25104;&#26032;&#30340;&#26230;&#20307;&#26230;&#26684;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#65292;&#22240;&#20026;&#23427;&#20204;&#21482;&#32771;&#34385;&#21407;&#23376;&#20301;&#32622;&#25110;&#21270;&#23398;&#32452;&#25104;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#27010;&#29575;&#25193;&#25955;&#27169;&#22411;&#65292;&#21033;&#29992;&#20960;&#20309;&#31561;&#21464;GNN&#21516;&#26102;&#32771;&#34385;&#21407;&#23376;&#20301;&#32622;&#21644;&#26230;&#20307;&#26230;&#26684;&#12290;&#20026;&#35780;&#20272;&#25105;&#20204;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21463;Frechet Inception Distance&#21551;&#21457;&#30340;&#26032;&#30340;&#29983;&#25104;&#25351;&#26631;&#65292;&#20294;&#22522;&#20110;GNN&#33021;&#37327;&#39044;&#27979;&#65292;&#32780;&#19981;&#26159;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#20351;&#29992;&#30340;InceptionV3&#12290;&#38500;&#20102;&#24120;&#29992;&#30340;&#35780;&#20272;&#25351;&#26631;&#65288;&#22914;&#26377;&#25928;&#24615;&#65292;&#35780;&#20272;&#32467;&#26500;&#30340;&#21512;&#29702;&#24615;&#65289;&#22806;&#65292;&#36825;&#31181;&#26032;&#25351;&#26631;&#25552;&#20379;&#20102;&#23545;&#25105;&#20204;&#27169;&#22411;&#33021;&#21147;&#26356;&#20840;&#38754;&#30340;&#35780;&#20272;&#12290;&#25105;&#20204;&#22312;&#29616;&#26377;&#22522;&#20934;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#25193;&#25955;&#27169;&#22411;&#30340;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#23398;&#20064;&#26377;&#24847;&#20041;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Discovering crystal structures with specific chemical properties has become an increasingly important focus in material science. However, current models are limited in their ability to generate new crystal lattices, as they only consider atomic positions or chemical composition. To address this issue, we propose a probabilistic diffusion model that utilizes a geometrically equivariant GNN to consider atomic positions and crystal lattices jointly. To evaluate the effectiveness of our model, we introduce a new generation metric inspired by Frechet Inception Distance, but based on GNN energy prediction rather than InceptionV3 used in computer vision. In addition to commonly used metrics like validity, which assesses the plausibility of a structure, this new metric offers a more comprehensive evaluation of our model's capabilities. Our experiments on existing benchmarks show the significance of our diffusion model. We also show that our method can effectively learn meaningful representatio
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#39046;&#22495;&#30456;&#20284;&#24615;&#24863;&#30693;&#26631;&#31614;&#20998;&#37197;&#65288;DSP&#65289;&#30340;&#27010;&#24565;&#65292;&#36890;&#36807;&#23558;&#27599;&#20010;&#22270;&#20687;&#30340;&#39046;&#22495;&#26631;&#31614;&#35270;&#20026;&#20854;&#19982;&#25351;&#23450;&#39046;&#22495;&#30340;&#30456;&#20284;&#24615;&#65292;&#20351;&#29992;&#39046;&#22495;&#29305;&#23450;&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#22312;&#27700;&#19979;&#36328;&#39046;&#22495;&#29289;&#20307;&#26816;&#27979;&#22522;&#20934;S-UODAC2020&#19978;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.05401</link><description>&lt;p&gt;
&#39046;&#22495;&#30456;&#20284;&#24615;&#24863;&#30693;&#26631;&#31614;&#20998;&#37197;&#29992;&#20110;&#39046;&#22495;&#36890;&#29992;&#27700;&#19979;&#29289;&#20307;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Domain Similarity-Perceived Label Assignment for Domain Generalized Underwater Object Detection. (arXiv:2401.05401v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05401
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#39046;&#22495;&#30456;&#20284;&#24615;&#24863;&#30693;&#26631;&#31614;&#20998;&#37197;&#65288;DSP&#65289;&#30340;&#27010;&#24565;&#65292;&#36890;&#36807;&#23558;&#27599;&#20010;&#22270;&#20687;&#30340;&#39046;&#22495;&#26631;&#31614;&#35270;&#20026;&#20854;&#19982;&#25351;&#23450;&#39046;&#22495;&#30340;&#30456;&#20284;&#24615;&#65292;&#20351;&#29992;&#39046;&#22495;&#29305;&#23450;&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#22312;&#27700;&#19979;&#36328;&#39046;&#22495;&#29289;&#20307;&#26816;&#27979;&#22522;&#20934;S-UODAC2020&#19978;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27700;&#20307;&#30340;&#22266;&#26377;&#29305;&#24615;&#21644;&#20809;&#27874;&#21160;&#24341;&#36215;&#20102;&#27700;&#19979;&#29615;&#22659;&#20013;&#19981;&#21516;&#23618;&#27425;&#21644;&#21306;&#22495;&#20043;&#38388;&#30340;&#24040;&#22823;&#24046;&#24322;&#12290;&#24403;&#27979;&#35797;&#38598;&#22312;&#19982;&#35757;&#32451;&#38598;&#19981;&#21516;&#30340;&#28023;&#27915;&#21306;&#22495;&#25910;&#38598;&#26102;&#65292;&#39046;&#22495;&#36716;&#31227;&#38382;&#39064;&#20986;&#29616;&#65292;&#20005;&#37325;&#24433;&#21709;&#20102;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#39046;&#22495;&#23545;&#25239;&#23398;&#20064;&#65288;DAL&#65289;&#35757;&#32451;&#31574;&#30053;&#20043;&#21069;&#34987;&#29992;&#20110;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;&#28982;&#32780;&#65292;DAL&#20005;&#37325;&#20381;&#36182;&#20110;&#25163;&#21160;&#36827;&#34892;&#29420;&#28909;&#32534;&#30721;&#30340;&#39046;&#22495;&#26631;&#31614;&#65292;&#36825;&#24847;&#21619;&#30528;&#21516;&#19968;&#39046;&#22495;&#26679;&#26412;&#20043;&#38388;&#27809;&#26377;&#24046;&#24322;&#12290;&#36825;&#31181;&#20551;&#35774;&#23548;&#33268;&#20102;DAL&#30340;&#19981;&#31283;&#23450;&#24615;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#39046;&#22495;&#30456;&#20284;&#24615;&#24863;&#30693;&#26631;&#31614;&#20998;&#37197;&#65288;DSP&#65289;&#30340;&#27010;&#24565;&#12290;&#27599;&#24352;&#22270;&#29255;&#30340;&#39046;&#22495;&#26631;&#31614;&#34987;&#30475;&#20316;&#20854;&#19982;&#25351;&#23450;&#39046;&#22495;&#30340;&#30456;&#20284;&#24615;&#12290;&#36890;&#36807;&#39046;&#22495;&#29305;&#23450;&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#22312;&#27700;&#19979;&#36328;&#39046;&#22495;&#29289;&#20307;&#26816;&#27979;&#22522;&#20934;S-UODAC2020&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The inherent characteristics and light fluctuations of water bodies give rise to the huge difference between different layers and regions in underwater environments. When the test set is collected in a different marine area from the training set, the issue of domain shift emerges, significantly compromising the model's ability to generalize. The Domain Adversarial Learning (DAL) training strategy has been previously utilized to tackle such challenges. However, DAL heavily depends on manually one-hot domain labels, which implies no difference among the samples in the same domain. Such an assumption results in the instability of DAL. This paper introduces the concept of Domain Similarity-Perceived Label Assignment (DSP). The domain label for each image is regarded as its similarity to the specified domains. Through domain-specific data augmentation techniques, we achieved state-of-the-art results on the underwater cross-domain object detection benchmark S-UODAC2020. Furthermore, we valid
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#35777;&#26126;&#20102;&#20154;&#24037;&#26234;&#33021;&#19981;&#20165;&#21487;&#20197;&#20316;&#20026;&#23398;&#20064;&#24037;&#20855;&#65292;&#36824;&#21487;&#20197;&#20316;&#20026;&#19968;&#20010;&#26234;&#33021;&#20195;&#29702;&#65292;&#19982;&#20154;&#31867;&#19968;&#36215;&#21442;&#19982;&#21327;&#21516;&#23398;&#20064;&#65292;&#25913;&#21464;&#31185;&#23398;&#35838;&#22530;&#20013;&#30340;&#35748;&#35782;&#23454;&#36341;&#12290;&#36890;&#36807;&#24320;&#21457;&#19968;&#20010;&#21517;&#20026;CLAIS&#30340;&#23454;&#29289;&#25945;&#23398;&#31995;&#32479;&#65292;&#32467;&#21512;&#20154;&#31867;&#23398;&#20064;&#32773;&#21644;&#20154;&#24037;&#26234;&#33021;&#35828;&#35805;&#32773;&#30340;&#21327;&#21516;&#23398;&#20064;&#36807;&#31243;&#65292;&#35813;&#31995;&#32479;&#25104;&#21151;&#24212;&#29992;&#22312;&#39044;&#22791;&#23567;&#23398;&#31185;&#23398;&#25945;&#24072;&#30340;&#35838;&#31243;&#20013;&#65292;&#24182;&#24471;&#21040;&#31215;&#26497;&#35780;&#20215;&#12290;</title><link>http://arxiv.org/abs/2401.05400</link><description>&lt;p&gt;
&#19982;&#20154;&#24037;&#26234;&#33021;&#35828;&#35805;&#32773;&#30340;&#21327;&#21516;&#23398;&#20064;&#65288;CLAIS&#65289;&#65306;&#39044;&#22791;&#23567;&#23398;&#31185;&#23398;&#25945;&#24072;&#23545;&#21407;&#22411;&#30340;&#21453;&#24212;
&lt;/p&gt;
&lt;p&gt;
Collaborative Learning with Artificial Intelligence Speakers (CLAIS): Pre-Service Elementary Science Teachers' Responses to the Prototype. (arXiv:2401.05400v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05400
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#35777;&#26126;&#20102;&#20154;&#24037;&#26234;&#33021;&#19981;&#20165;&#21487;&#20197;&#20316;&#20026;&#23398;&#20064;&#24037;&#20855;&#65292;&#36824;&#21487;&#20197;&#20316;&#20026;&#19968;&#20010;&#26234;&#33021;&#20195;&#29702;&#65292;&#19982;&#20154;&#31867;&#19968;&#36215;&#21442;&#19982;&#21327;&#21516;&#23398;&#20064;&#65292;&#25913;&#21464;&#31185;&#23398;&#35838;&#22530;&#20013;&#30340;&#35748;&#35782;&#23454;&#36341;&#12290;&#36890;&#36807;&#24320;&#21457;&#19968;&#20010;&#21517;&#20026;CLAIS&#30340;&#23454;&#29289;&#25945;&#23398;&#31995;&#32479;&#65292;&#32467;&#21512;&#20154;&#31867;&#23398;&#20064;&#32773;&#21644;&#20154;&#24037;&#26234;&#33021;&#35828;&#35805;&#32773;&#30340;&#21327;&#21516;&#23398;&#20064;&#36807;&#31243;&#65292;&#35813;&#31995;&#32479;&#25104;&#21151;&#24212;&#29992;&#22312;&#39044;&#22791;&#23567;&#23398;&#31185;&#23398;&#25945;&#24072;&#30340;&#35838;&#31243;&#20013;&#65292;&#24182;&#24471;&#21040;&#31215;&#26497;&#35780;&#20215;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#26088;&#22312;&#35777;&#26126;&#20154;&#24037;&#26234;&#33021;&#19981;&#20165;&#21487;&#20197;&#20316;&#20026;&#23398;&#20064;&#24037;&#20855;&#65292;&#36824;&#21487;&#20197;&#20316;&#20026;&#19968;&#20010;&#26234;&#33021;&#20195;&#29702;&#65292;&#19982;&#20154;&#31867;&#20849;&#21516;&#21442;&#19982;&#21327;&#21516;&#23398;&#20064;&#65292;&#20197;&#25913;&#21464;&#31185;&#23398;&#35838;&#22530;&#20013;&#30340;&#35748;&#35782;&#23454;&#36341;&#12290;&#25105;&#20204;&#37319;&#29992;&#35774;&#35745;&#19982;&#24320;&#21457;&#30740;&#31350;&#26041;&#27861;&#65292;&#36981;&#24490;&#20998;&#26512;&#12289;&#35774;&#35745;&#12289;&#24320;&#21457;&#12289;&#23454;&#26045;&#21644;&#35780;&#20272;&#65288;ADDIE&#65289;&#27169;&#22411;&#65292;&#21407;&#22411;&#19968;&#20010;&#21517;&#20026;&#21327;&#21516;&#23398;&#20064;&#19982;AI&#35828;&#35805;&#32773;&#65288;CLAIS&#65289;&#30340;&#23454;&#29289;&#25945;&#23398;&#31995;&#32479;&#12290;CLAIS&#31995;&#32479;&#26088;&#22312;&#35753;3-4&#20010;&#20154;&#31867;&#23398;&#20064;&#32773;&#19982;&#19968;&#20010;&#20154;&#24037;&#26234;&#33021;&#35828;&#35805;&#32773;&#32452;&#25104;&#19968;&#20010;&#23567;&#32452;&#65292;&#22312;Jigsaw&#23398;&#20064;&#36807;&#31243;&#20013;&#23558;&#20154;&#31867;&#19982;AI&#35270;&#20026;&#21516;&#20276;&#12290;&#24320;&#21457;&#20351;&#29992;&#20102;NUGU AI&#35828;&#35805;&#32773;&#24179;&#21488;&#12290;CLAIS&#31995;&#32479;&#22312;&#19968;&#20010;&#39044;&#22791;&#23567;&#23398;&#31185;&#23398;&#25945;&#24072;&#30340;&#31185;&#23398;&#25945;&#32946;&#35838;&#31243;&#20013;&#25104;&#21151;&#23454;&#26045;&#12290;&#21442;&#19982;&#32773;&#20316;&#20026;&#25945;&#24072;&#12289;&#23398;&#20064;&#32773;&#12289;&#21516;&#20276;&#21644;&#29992;&#25143;&#36890;&#36807;&#28151;&#21512;&#26041;&#27861;&#35843;&#26597;&#35780;&#20272;&#20102;CLAIS&#31995;&#32479;&#12290;&#23450;&#37327;&#25968;&#25454;&#34920;&#26126;&#65292;&#21442;&#19982;&#32773;&#31215;&#26497;&#35780;&#20215;CLAIS&#31995;&#32479;&#30340;&#20351;&#29992;&#20307;&#39564;&#21644;&#23398;&#20064;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
This research aims to demonstrate that AI can function not only as a tool for learning, but also as an intelligent agent with which humans can engage in collaborative learning (CL) to change epistemic practices in science classrooms. We adopted a design and development research approach, following the Analysis, Design, Development, Implementation and Evaluation (ADDIE) model, to prototype a tangible instructional system called Collaborative Learning with AI Speakers (CLAIS). The CLAIS system is designed to have 3-4 human learners join an AI speaker to form a small group, where humans and AI are considered as peers participating in the Jigsaw learning process. The development was carried out using the NUGU AI speaker platform. The CLAIS system was successfully implemented in a Science Education course session with 15 pre-service elementary science teachers. The participants evaluated the CLAIS system through mixed methods surveys as teachers, learners, peers, and users. Quantitative dat
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#20351;&#29992;LLMs&#33258;&#21160;&#35780;&#20272;&#23398;&#29983;&#23545;&#20195;&#30721;&#30340;&#29702;&#35299;&#33021;&#21147;&#30340;&#28508;&#21147;&#65292;&#24182;&#21457;&#29616;LLMs&#22312;&#27604;&#36739;&#23398;&#29983;&#30340;&#35299;&#37322;&#21644;&#19987;&#23478;&#35299;&#37322;&#26041;&#38754;&#20855;&#26377;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2401.05399</link><description>&lt;p&gt;
&#20351;&#29992;LLMs&#33258;&#21160;&#35780;&#20272;&#23398;&#29983;&#30340;&#20195;&#30721;&#29702;&#35299;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Automated Assessment of Students' Code Comprehension using LLMs. (arXiv:2401.05399v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05399
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#20351;&#29992;LLMs&#33258;&#21160;&#35780;&#20272;&#23398;&#29983;&#23545;&#20195;&#30721;&#30340;&#29702;&#35299;&#33021;&#21147;&#30340;&#28508;&#21147;&#65292;&#24182;&#21457;&#29616;LLMs&#22312;&#27604;&#36739;&#23398;&#29983;&#30340;&#35299;&#37322;&#21644;&#19987;&#23478;&#35299;&#37322;&#26041;&#38754;&#20855;&#26377;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25945;&#32946;&#39046;&#22495;&#65292;&#35780;&#20272;&#23398;&#29983;&#31572;&#26696;&#65292;&#23588;&#20854;&#26159;&#33258;&#28982;&#35821;&#35328;&#31572;&#26696;&#65292;&#26159;&#19968;&#39033;&#20851;&#38190;&#25361;&#25112;&#12290;&#26426;&#22120;&#23398;&#20064;&#30340;&#36827;&#27493;&#65292;&#21253;&#25324;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#65292;&#22914;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#65292;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#22312;&#35780;&#20272;LLMs&#22312;&#33258;&#21160;&#31572;&#26696;&#35780;&#20272;&#39046;&#22495;&#30340;&#19981;&#26029;&#22686;&#38271;&#30340;&#36235;&#21183;&#20013;&#65292;&#23545;LLMs&#30340;&#35780;&#20272;&#24182;&#27809;&#26377;&#24471;&#21040;&#22826;&#22810;&#20851;&#27880;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#20351;&#29992;LLMs&#26469;&#33258;&#21160;&#35780;&#20272;&#23398;&#29983;&#31616;&#30701;&#21644;&#24320;&#25918;&#24615;&#22238;&#31572;&#30340;&#28508;&#21147;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#20351;&#29992;LLMs&#26469;&#27604;&#36739;&#23398;&#29983;&#23545;&#35745;&#31639;&#26426;&#31243;&#24207;&#36880;&#34892;&#35299;&#37322;&#30340;&#35299;&#37322;&#19982;&#19987;&#23478;&#35299;&#37322;&#12290;&#20026;&#20102;&#27604;&#36739;&#65292;&#25105;&#20204;&#22312;&#35780;&#20272;&#23398;&#29983;&#23545;&#35745;&#31639;&#26426;&#20195;&#30721;&#35299;&#37322;&#30340;&#27491;&#30830;&#24615;&#26041;&#38754;&#65292;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#21644;&#22522;&#20110;&#32534;&#30721;&#22120;&#30340;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24615;(STS)&#27169;&#22411;&#36827;&#34892;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;LLMs&#22312;&#25552;&#31034;&#23398;&#29983;&#35299;&#37322;&#35745;&#31639;&#26426;&#20195;&#30721;&#30340;&#27491;&#30830;&#24615;&#26102;&#21487;&#20197;&#36215;&#21040;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Assessing student's answers and in particular natural language answers is a crucial challenge in the field of education. Advances in machine learning, including transformer-based models such as Large Language Models(LLMs), have led to significant progress in various natural language tasks. Nevertheless, amidst the growing trend of evaluating LLMs across diverse tasks, evaluating LLMs in the realm of automated answer assesment has not received much attention. To address this gap, we explore the potential of using LLMs for automated assessment of student's short and open-ended answer. Particularly, we use LLMs to compare students' explanations with expert explanations in the context of line-by-line explanations of computer programs.  For comparison purposes, we assess both Large Language Models (LLMs) and encoder-based Semantic Textual Similarity (STS) models in the context of assessing the correctness of students' explanation of computer code. Our findings indicate that LLMs, when promp
&lt;/p&gt;</description></item><item><title>GeoAI&#22312;&#31038;&#20250;&#31185;&#23398;&#30740;&#31350;&#20013;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#65292;&#36890;&#36807;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#21644;&#22320;&#29702;&#22823;&#25968;&#25454;&#22635;&#34917;&#25968;&#25454;&#21644;&#30693;&#35782;&#31354;&#30333;&#65292;&#24182;&#20419;&#36827;&#20102;&#30740;&#31350;&#26041;&#27861;&#30340;&#34701;&#21512;&#21644;&#24212;&#29992;&#39046;&#22495;&#30340;&#25299;&#23637;&#12290;</title><link>http://arxiv.org/abs/2401.05398</link><description>&lt;p&gt;
&#31038;&#20250;&#31185;&#23398;&#20013;&#30340;&#22320;&#29702;&#20154;&#24037;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
GeoAI in Social Science. (arXiv:2401.05398v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05398
&lt;/p&gt;
&lt;p&gt;
GeoAI&#22312;&#31038;&#20250;&#31185;&#23398;&#30740;&#31350;&#20013;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#65292;&#36890;&#36807;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#21644;&#22320;&#29702;&#22823;&#25968;&#25454;&#22635;&#34917;&#25968;&#25454;&#21644;&#30693;&#35782;&#31354;&#30333;&#65292;&#24182;&#20419;&#36827;&#20102;&#30740;&#31350;&#26041;&#27861;&#30340;&#34701;&#21512;&#21644;&#24212;&#29992;&#39046;&#22495;&#30340;&#25299;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22320;&#29702;&#20154;&#24037;&#26234;&#33021;&#65288;GeoAI&#65289;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#39046;&#22495;&#65292;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#12289;&#22320;&#29702;&#22823;&#25968;&#25454;&#21644;&#24222;&#22823;&#30340;&#35745;&#31639;&#33021;&#21147;&#26469;&#35299;&#20915;&#39640;&#24230;&#33258;&#21160;&#21270;&#21644;&#26234;&#33021;&#21270;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#22238;&#39038;&#20102;&#22312;&#31038;&#20250;&#31185;&#23398;&#30740;&#31350;&#20013;&#24212;&#29992;&#20154;&#24037;&#26234;&#33021;&#30340;&#36827;&#23637;&#65292;&#31361;&#20986;&#20102;&#21033;&#29992;GeoAI&#22635;&#34917;&#37325;&#35201;&#25968;&#25454;&#21644;&#30693;&#35782;&#31354;&#30333;&#30340;&#37325;&#35201;&#36827;&#23637;&#12290;&#21516;&#26102;&#65292;&#35752;&#35770;&#20102;&#25171;&#30772;&#25968;&#25454;&#23396;&#23707;&#12289;&#21152;&#24555;GeoAI&#30740;&#31350;&#26041;&#27861;&#30340;&#34701;&#21512;&#20197;&#21450;&#23558;GeoAI&#25512;&#24191;&#24212;&#29992;&#21040;&#22320;&#29702;&#39046;&#22495;&#20043;&#22806;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
GeoAI, or geospatial artificial intelligence, is an exciting new area that leverages artificial intelligence (AI), geospatial big data, and massive computing power to solve problems with high automation and intelligence. This paper reviews the progress of AI in social science research, highlighting important advancements in using GeoAI to fill critical data and knowledge gaps. It also discusses the importance of breaking down data silos, accelerating convergence among GeoAI research methods, as well as moving GeoAI beyond geospatial benefits.
&lt;/p&gt;</description></item><item><title>SRNI-CAR&#26159;&#19968;&#20221;&#29992;&#20110;&#20998;&#26512;&#20013;&#22269;&#27773;&#36710;&#24066;&#22330;&#30340;&#32508;&#21512;&#25968;&#25454;&#38598;&#65292;&#22635;&#34917;&#20102;&#29616;&#26377;&#27773;&#36710;&#34892;&#19994;&#25968;&#25454;&#38598;&#35206;&#30422;&#33539;&#22260;&#26377;&#38480;&#30340;&#32570;&#21475;&#65292;&#23545;&#20110;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#12289;&#25193;&#22823;&#21830;&#19994;&#24212;&#29992;&#33539;&#22260;&#12289;&#25351;&#23548;&#25919;&#31574;&#21046;&#23450;&#19982;&#30417;&#31649;&#20197;&#21450;&#25512;&#21160;&#27773;&#36710;&#34892;&#19994;&#30340;&#23398;&#26415;&#30740;&#31350;&#20855;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2401.05395</link><description>&lt;p&gt;
SRNI-CAR: &#29992;&#20110;&#20998;&#26512;&#20013;&#22269;&#27773;&#36710;&#24066;&#22330;&#30340;&#32508;&#21512;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
SRNI-CAR: A comprehensive dataset for analyzing the Chinese automotive market. (arXiv:2401.05395v1 [econ.GN])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05395
&lt;/p&gt;
&lt;p&gt;
SRNI-CAR&#26159;&#19968;&#20221;&#29992;&#20110;&#20998;&#26512;&#20013;&#22269;&#27773;&#36710;&#24066;&#22330;&#30340;&#32508;&#21512;&#25968;&#25454;&#38598;&#65292;&#22635;&#34917;&#20102;&#29616;&#26377;&#27773;&#36710;&#34892;&#19994;&#25968;&#25454;&#38598;&#35206;&#30422;&#33539;&#22260;&#26377;&#38480;&#30340;&#32570;&#21475;&#65292;&#23545;&#20110;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#12289;&#25193;&#22823;&#21830;&#19994;&#24212;&#29992;&#33539;&#22260;&#12289;&#25351;&#23548;&#25919;&#31574;&#21046;&#23450;&#19982;&#30417;&#31649;&#20197;&#21450;&#25512;&#21160;&#27773;&#36710;&#34892;&#19994;&#30340;&#23398;&#26415;&#30740;&#31350;&#20855;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27773;&#36710;&#34892;&#19994;&#22312;&#20840;&#29699;&#32463;&#27982;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#23588;&#20854;&#37325;&#35201;&#30340;&#26159;&#20013;&#22269;&#27773;&#36710;&#24066;&#22330;&#30340;&#19981;&#26029;&#25193;&#22823;&#65292;&#30001;&#20110;&#20854;&#24040;&#22823;&#30340;&#35268;&#27169;&#21644;&#24433;&#21709;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#27773;&#36710;&#34892;&#19994;&#25968;&#25454;&#38598;&#22312;&#35206;&#30422;&#33539;&#22260;&#19978;&#26377;&#38480;&#65292;&#26410;&#33021;&#20805;&#20998;&#32771;&#34385;&#21040;&#23545;&#26356;&#22810;&#21644;&#22810;&#26679;&#21270;&#21464;&#37327;&#30340;&#19981;&#26029;&#22686;&#38271;&#30340;&#38656;&#27714;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#20171;&#32461;&#19968;&#20221;&#20174;2016&#24180;&#21040;2022&#24180;&#30340;&#32508;&#21512;&#25968;&#25454;&#38598;&#26469;&#22635;&#34917;&#36825;&#19968;&#25968;&#25454;&#32570;&#21475;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#25324;&#38144;&#21806;&#25968;&#25454;&#65292;&#22312;&#32447;&#35780;&#35770;&#20197;&#21450;&#19982;&#20013;&#22269;&#27773;&#36710;&#34892;&#19994;&#30456;&#20851;&#30340;&#22823;&#37327;&#20449;&#24687;&#12290;&#35813;&#25968;&#25454;&#38598;&#20316;&#20026;&#23453;&#36149;&#30340;&#36164;&#28304;&#65292;&#26497;&#22823;&#22320;&#25193;&#23637;&#20102;&#21487;&#29992;&#25968;&#25454;&#12290;&#23427;&#30340;&#24433;&#21709;&#21147;&#28085;&#30422;&#20102;&#22810;&#20010;&#26041;&#38754;&#65292;&#21253;&#25324;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#25193;&#22823;&#21830;&#19994;&#24212;&#29992;&#33539;&#22260;&#65292;&#25351;&#23548;&#25919;&#31574;&#21046;&#23450;&#19982;&#30417;&#31649;&#65292;&#25512;&#21160;&#27773;&#36710;&#34892;&#19994;&#30340;&#23398;&#26415;&#30740;&#31350;&#12290;&#20026;&#20102;&#23637;&#31034;&#35813;&#25968;&#25454;&#38598;&#22312;&#21830;&#19994;&#21644;&#23398;&#26415;&#32972;&#26223;&#19979;&#30340;&#28508;&#22312;&#24212;&#29992;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#24212;&#29992;&#26696;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
The automotive industry plays a critical role in the global economy, and particularly important is the expanding Chinese automobile market due to its immense scale and influence. However, existing automotive sector datasets are limited in their coverage, failing to adequately consider the growing demand for more and diverse variables. This paper aims to bridge this data gap by introducing a comprehensive dataset spanning the years from 2016 to 2022, encompassing sales data, online reviews, and a wealth of information related to the Chinese automotive industry. This dataset serves as a valuable resource, significantly expanding the available data. Its impact extends to various dimensions, including improving forecasting accuracy, expanding the scope of business applications, informing policy development and regulation, and advancing academic research within the automotive sector. To illustrate the dataset's potential applications in both business and academic contexts, we present two ap
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36866;&#24212;&#24615;&#31867;&#22411;-2&#27169;&#31946;&#28388;&#27874;&#22120;&#65292;&#29992;&#20110;&#21435;&#38500;&#22270;&#20687;&#20013;&#30416;&#21644;&#32993;&#26898;&#22122;&#22768;&#12290;&#36890;&#36807;&#33258;&#36866;&#24212;&#38408;&#20540;&#21644;&#21152;&#26435;&#22343;&#20540;&#28388;&#27874;&#22120;&#20004;&#20010;&#27493;&#39588;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#20445;&#30041;&#22270;&#20687;&#30340;&#29305;&#24449;&#65292;&#24182;&#22312;&#19982;&#20854;&#20182;&#19981;&#21516;&#28388;&#27874;&#26041;&#27861;&#30340;&#27604;&#36739;&#20013;&#23637;&#31034;&#20986;&#33391;&#22909;&#30340;&#21435;&#22122;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.05392</link><description>&lt;p&gt;
AT-2FF&#65306;&#36866;&#24212;&#24615;&#31867;&#22411;-2&#27169;&#31946;&#28388;&#27874;&#22120;&#29992;&#20110;&#21435;&#38500;&#21463;&#30416;&#21644;&#32993;&#26898;&#22122;&#22768;&#27745;&#26579;&#30340;&#22270;&#20687;
&lt;/p&gt;
&lt;p&gt;
AT-2FF: Adaptive Type-2 Fuzzy Filter for De-noising Images Corrupted with Salt-and-Pepper. (arXiv:2401.05392v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05392
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36866;&#24212;&#24615;&#31867;&#22411;-2&#27169;&#31946;&#28388;&#27874;&#22120;&#65292;&#29992;&#20110;&#21435;&#38500;&#22270;&#20687;&#20013;&#30416;&#21644;&#32993;&#26898;&#22122;&#22768;&#12290;&#36890;&#36807;&#33258;&#36866;&#24212;&#38408;&#20540;&#21644;&#21152;&#26435;&#22343;&#20540;&#28388;&#27874;&#22120;&#20004;&#20010;&#27493;&#39588;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#20445;&#30041;&#22270;&#20687;&#30340;&#29305;&#24449;&#65292;&#24182;&#22312;&#19982;&#20854;&#20182;&#19981;&#21516;&#28388;&#27874;&#26041;&#27861;&#30340;&#27604;&#36739;&#20013;&#23637;&#31034;&#20986;&#33391;&#22909;&#30340;&#21435;&#22122;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22122;&#22768;&#22312;&#25968;&#23383;&#22270;&#20687;&#20013;&#26222;&#36941;&#23384;&#22312;&#65292;&#23548;&#33268;&#22270;&#20687;&#36136;&#37327;&#19979;&#38477;&#12290;&#22240;&#27492;&#65292;&#38656;&#35201;&#19968;&#31181;&#21512;&#36866;&#30340;&#28388;&#27874;&#26041;&#27861;&#26469;&#20943;&#23569;&#22122;&#22768;&#21516;&#26102;&#20445;&#30041;&#22270;&#20687;&#29305;&#24449;&#65288;&#36793;&#32536;&#12289;&#35282;&#28857;&#31561;&#65289;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#31867;&#22411;-2&#27169;&#31946;&#21152;&#26435;&#22343;&#20540;&#28388;&#27874;&#22120;&#65292;&#24102;&#26377;&#33258;&#36866;&#24212;&#38408;&#20540;&#65292;&#29992;&#20110;&#21435;&#38500;&#30416;&#21644;&#32993;&#26898;&#22122;&#22768;&#12290;&#35813;&#28388;&#27874;&#22120;&#26377;&#20004;&#20010;&#20027;&#35201;&#27493;&#39588;&#65306;&#31532;&#19968;&#38454;&#27573;&#26681;&#25454;&#33258;&#36866;&#24212;&#38408;&#20540;&#23558;&#22270;&#20687;&#20998;&#31867;&#20026;&#36731;&#24494;&#12289;&#20013;&#24230;&#21644;&#37325;&#24230;&#27745;&#26579;&#65292;&#36890;&#36807;&#27604;&#36739;&#22788;&#29702;&#20687;&#32032;&#30340;M-ALD&#19982;&#31867;&#22411;-2&#27169;&#31946;&#35782;&#21035;&#22120;&#30340;&#19978;&#19979;MF&#26469;&#23454;&#29616;&#12290;&#31532;&#20108;&#38454;&#27573;&#36890;&#36807;&#20351;&#29992;&#28388;&#27874;&#31383;&#21475;&#20869;&#26410;&#27745;&#26579;&#20687;&#32032;&#30340;&#22343;&#20540;&#21644;&#26041;&#24046;&#26469;&#35745;&#31639;&#36866;&#24403;&#30340;&#26435;&#37325;&#65292;&#26469;&#28040;&#38500;&#21463;&#25439;&#20687;&#32032;&#12290;&#20223;&#30495;&#32467;&#26524;&#26126;&#30830;&#34920;&#26126;&#65292;&#25152;&#24471;&#21040;&#30340;&#21435;&#22122;&#22270;&#20687;&#20445;&#30041;&#20102;&#22270;&#20687;&#29305;&#24449;&#65292;&#21363;&#36793;&#32536;&#12289;&#35282;&#28857;&#21644;&#20854;&#20182;&#28165;&#26224;&#32467;&#26500;&#65292;&#19982;&#20854;&#20182;&#19981;&#21516;&#30340;&#28388;&#27874;&#26041;&#27861;&#30456;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
Noise is inevitably common in digital images, leading to visual image deterioration. Therefore, a suitable filtering method is required to lessen the noise while preserving the image features (edges, corners, etc.). This paper presents the efficient type-2 fuzzy weighted mean filter with an adaptive threshold to remove the SAP noise. The present filter has two primary steps: The first stage categorizes images as lightly, medium, and heavily corrupted based on an adaptive threshold by comparing the M-ALD of processed pixels with the upper and lower MF of the type-2 fuzzy identifier. The second stage eliminates corrupted pixels by computing the appropriate weight using GMF with the mean and variance of the uncorrupted pixels in the filter window. Simulation results vividly show that the obtained denoised images preserve image features, i.e., edges, corners, and other sharp structures, compared with different filtering methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;Intel GPU&#19978;&#39640;&#25928;&#30340;LLM&#25512;&#29702;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#31616;&#21270;LLM&#35299;&#30721;&#23618;&#21644;&#24341;&#20837;&#20998;&#27573;KV&#32531;&#23384;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#20302;&#24310;&#36831;&#21644;&#39640;&#21534;&#21520;&#37327;&#12290;</title><link>http://arxiv.org/abs/2401.05391</link><description>&lt;p&gt;
&#22312;Intel GPU&#19978;&#39640;&#25928;&#30340;LLM&#25512;&#29702;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Efficient LLM inference solution on Intel GPU. (arXiv:2401.05391v1 [cs.AR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05391
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;Intel GPU&#19978;&#39640;&#25928;&#30340;LLM&#25512;&#29702;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#31616;&#21270;LLM&#35299;&#30721;&#23618;&#21644;&#24341;&#20837;&#20998;&#27573;KV&#32531;&#23384;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#20302;&#24310;&#36831;&#21644;&#39640;&#21534;&#21520;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#35768;&#22810;&#39046;&#22495;&#24191;&#27867;&#24212;&#29992;&#65292;LLM&#25512;&#29702;&#30340;&#25928;&#29575;&#25104;&#20026;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#28909;&#38376;&#35805;&#39064;&#12290;&#28982;&#32780;&#65292;LLM&#36890;&#24120;&#22312;&#27169;&#22411;&#32467;&#26500;&#19978;&#35774;&#35745;&#22797;&#26434;&#65292;&#20855;&#26377;&#22823;&#37327;&#25805;&#20316;&#65292;&#24182;&#20197;&#33258;&#22238;&#24402;&#27169;&#24335;&#36827;&#34892;&#25512;&#29702;&#65292;&#36825;&#20351;&#24471;&#35774;&#35745;&#19968;&#20010;&#39640;&#25928;&#30340;&#31995;&#32479;&#25104;&#20026;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;LLM&#25512;&#29702;&#35299;&#20915;&#26041;&#26696;&#65292;&#20855;&#26377;&#20302;&#24310;&#36831;&#21644;&#39640;&#21534;&#21520;&#37327;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#36890;&#36807;&#34701;&#21512;&#25968;&#25454;&#31227;&#21160;&#21644;&#36880;&#20803;&#32032;&#25805;&#20316;&#31616;&#21270;&#20102;LLM&#35299;&#30721;&#23618;&#65292;&#20197;&#20943;&#23569;&#20869;&#23384;&#35775;&#38382;&#39057;&#29575;&#24182;&#38477;&#20302;&#31995;&#32479;&#24310;&#36831;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#27573;KV&#32531;&#23384;&#31574;&#30053;&#65292;&#23558;&#35831;&#27714;&#21644;&#21709;&#24212;&#20196;&#29260;&#30340;&#38190;/&#20540;&#20998;&#21035;&#20445;&#23384;&#22312;&#19981;&#21516;&#30340;&#29289;&#29702;&#20869;&#23384;&#20013;&#65292;&#20197;&#23454;&#29616;&#26377;&#25928;&#30340;&#35774;&#22791;&#20869;&#23384;&#31649;&#29702;&#65292;&#26377;&#21161;&#20110;&#22686;&#22823;&#36816;&#34892;&#26102;&#25209;&#22788;&#29702;&#22823;&#23567;&#24182;&#25552;&#39640;&#31995;&#32479;&#21534;&#21520;&#37327;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#23450;&#21046;&#30340;Scaled-Dot-Product-Attention&#20869;&#26680;&#65292;&#20197;&#21305;&#37197;&#25105;&#20204;&#30340;&#34701;&#21512;&#31574;&#30053;&#65292;&#22522;&#20110;&#20998;&#27573;KV&#32531;&#23384;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#35299;&#20915;&#26041;&#26696;&#22312;Intel GPU&#19978;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;LLM&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer based Large Language Models (LLMs) have been widely used in many fields, and the efficiency of LLM inference becomes hot topic in real applications. However, LLMs are usually complicatedly designed in model structure with massive operations and perform inference in the auto-regressive mode, making it a challenging task to design a system with high efficiency.  In this paper, we propose an efficient LLM inference solution with low latency and high throughput. Firstly, we simplify the LLM decoder layer by fusing data movement and element-wise operations to reduce the memory access frequency and lower system latency. We also propose a segment KV cache policy to keep key/value of the request and response tokens in separate physical memory for effective device memory management, helping enlarge the runtime batch size and improve system throughput. A customized Scaled-Dot-Product-Attention kernel is designed to match our fusion policy based on the segment KV cache solution. We im
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#24037;&#20855;&#36741;&#21161;&#35821;&#35328;&#27169;&#22411;&#22312;&#25968;&#23398;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#25552;&#20986;&#20102;IMP-TIP&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#37319;&#29992;&#20102;&#8220;&#30001;&#22909;&#21040;&#20248;&#8221;&#30340;&#24605;&#24819;&#24182;&#36890;&#36807;&#24037;&#20855;&#36741;&#21161;&#20132;&#38169;&#25552;&#31034;&#26469;&#36873;&#25321;&#26368;&#20934;&#30830;&#30340;&#31572;&#26696;&#12290;</title><link>http://arxiv.org/abs/2401.05384</link><description>&lt;p&gt;
&#30001;&#22909;&#21040;&#20248;&#65306;&#36890;&#36807;&#24037;&#20855;&#36741;&#21161;&#20132;&#38169;&#25552;&#31034;&#25913;&#36827;&#25968;&#23398;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
From Good to Great: Improving Math Reasoning with Tool-Augmented Interleaf Prompting. (arXiv:2401.05384v1 [math.HO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05384
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#24037;&#20855;&#36741;&#21161;&#35821;&#35328;&#27169;&#22411;&#22312;&#25968;&#23398;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#25552;&#20986;&#20102;IMP-TIP&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#37319;&#29992;&#20102;&#8220;&#30001;&#22909;&#21040;&#20248;&#8221;&#30340;&#24605;&#24819;&#24182;&#36890;&#36807;&#24037;&#20855;&#36741;&#21161;&#20132;&#38169;&#25552;&#31034;&#26469;&#36873;&#25321;&#26368;&#20934;&#30830;&#30340;&#31572;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21644;&#24037;&#20855;&#36741;&#21161;LLMs&#22312;&#22788;&#29702;&#22797;&#26434;&#25968;&#23398;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;IMP-TIP&#65306;&#36890;&#36807;&#24037;&#20855;&#36741;&#21161;&#20132;&#38169;&#25552;&#31034;&#25913;&#36827;&#25968;&#23398;&#25512;&#29702;&#65292;&#36825;&#26159;&#19968;&#20010;&#32508;&#21512;&#20102;LLMs&#21644;&#24037;&#20855;&#36741;&#21161;LLMs&#30340;&#20248;&#21183;&#30340;&#26694;&#26550;&#12290;IMP-TIP&#36981;&#24490;&#8220;&#30001;&#22909;&#21040;&#20248;&#8221;&#30340;&#27010;&#24565;&#65292;&#20174;LLMs&#21644;&#20854;&#24037;&#20855;&#36741;&#21161;&#23545;&#24212;&#29289;&#20013;&#25910;&#38598;&#22810;&#20010;&#21487;&#33021;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#36890;&#36807;&#24037;&#20855;&#36741;&#21161;&#20132;&#38169;&#25552;&#31034;&#22312;&#36825;&#20123;&#35299;&#20915;&#26041;&#26696;&#20043;&#38388;&#36827;&#34892;&#20132;&#21449;&#26816;&#26597;&#65292;&#28982;&#21518;&#36873;&#25321;&#25110;&#37325;&#26032;&#29983;&#25104;&#26368;&#20934;&#30830;&#30340;&#31572;&#26696;&#12290;&#35813;&#26694;&#26550;&#21253;&#25324;&#20004;&#20010;&#20851;&#38190;&#26041;&#38754;&#65306;&#33258;&#25105;&#25552;&#31034;&#21644;&#24037;&#20855;&#36741;&#21161;&#20132;&#38169;&#25552;&#31034;&#65288;TIP&#65289;&#12290;&#21069;&#32773;&#20801;&#35768;LLMs&#33258;&#20027;&#22320;&#25913;&#36827;&#21644;&#23436;&#21892;&#19982;&#24037;&#20855;&#20351;&#29992;&#30456;&#20851;&#30340;&#21021;&#22987;&#25552;&#31034;&#65292;&#32780;&#21518;&#32773;&#20351;LLMs&#33021;&#22815;&#36890;&#36807;&#21160;&#24577;&#20998;&#26512;&#38382;&#39064;&#12289;&#20132;&#21449;&#26816;&#26597;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#21644;&#20462;&#27491;&#20808;&#21069;&#30340;&#25512;&#29702;&#25552;&#31034;&#26469;&#24471;&#20986;&#26368;&#32456;&#31572;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates the performance of Large Language Models (LLMs) and Tool-augmented LLMs in tackling complex mathematical reasoning tasks. We introduce IMP-TIP: Improving Math Reasoning with Tool-augmented Interleaf Prompting, a framework that combines the strengths of both LLMs and Tool-augmented LLMs. IMP-TIP follows the ``From Good to Great" concept, collecting multiple potential solutions from both LLMs and their Tool-Augmented counterparts for the same math problem, and then selecting or re-generating the most accurate answer after cross-checking these solutions via tool-augmented interleaf prompting. The framework incorporates two key aspects: self-prompt and tool-augmented interleaf prompting (TIP). The former allows LLMs to autonomously refine and improve an initial prompt related to tool usage, while the latter enables LLMs to derive the final answer by dynamically analyzing the problem, cross-checking potential solutions, and revising previous reasoning hints in an int
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#32463;&#20856;&#25490;&#24207;&#31639;&#27861;&#20316;&#20026;&#24418;&#24577;&#21457;&#29983;&#23398;&#27169;&#22411;&#65292;&#22312;&#26497;&#31616;&#27169;&#22411;&#20013;&#23637;&#31034;&#20986;&#20102;&#24847;&#22806;&#30340;&#22522;&#30784;&#26234;&#33021;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.05375</link><description>&lt;p&gt;
&#32463;&#20856;&#25490;&#24207;&#31639;&#27861;&#20316;&#20026;&#19968;&#31181;&#24418;&#24577;&#21457;&#29983;&#23398;&#27169;&#22411;&#65306;&#33258;&#25490;&#24207;&#25968;&#32452;&#22312;&#26497;&#31616;&#27169;&#22411;&#30340;&#22522;&#30784;&#26234;&#33021;&#20013;&#23637;&#29616;&#20102;&#24847;&#22806;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Classical Sorting Algorithms as a Model of Morphogenesis: self-sorting arrays reveal unexpected competencies in a minimal model of basal intelligence. (arXiv:2401.05375v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05375
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#32463;&#20856;&#25490;&#24207;&#31639;&#27861;&#20316;&#20026;&#24418;&#24577;&#21457;&#29983;&#23398;&#27169;&#22411;&#65292;&#22312;&#26497;&#31616;&#27169;&#22411;&#20013;&#23637;&#31034;&#20986;&#20102;&#24847;&#22806;&#30340;&#22522;&#30784;&#26234;&#33021;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26679;&#21270;&#26234;&#33021;&#39046;&#22495;&#26088;&#22312;&#35782;&#21035;&#12289;&#27491;&#24335;&#21270;&#21644;&#29702;&#35299;&#21508;&#31181;&#23454;&#29616;&#20013;&#34892;&#20026;&#33021;&#21147;&#30340;&#20849;&#24615;&#12290;&#23588;&#20854;&#26377;&#36259;&#30340;&#26159;&#37027;&#20123;&#22312;&#22806;&#35266;&#19978;&#30475;&#36215;&#26469;&#19981;&#22815;&#22797;&#26434;&#20197;&#23454;&#29616;&#36825;&#20123;&#33021;&#21147;&#30340;&#22522;&#36136;&#20013;&#25552;&#20379;&#20102;&#24847;&#22806;&#30340;&#35760;&#24518;&#12289;&#20915;&#31574;&#25110;&#38382;&#39064;&#35299;&#20915;&#30340;&#31616;&#21333;&#31995;&#32479;&#12290;&#25105;&#20204;&#33268;&#21147;&#20110;&#24320;&#21457;&#24037;&#20855;&#26469;&#24110;&#21161;&#29702;&#35299;&#23454;&#29616;&#36825;&#20123;&#33021;&#21147;&#30340;&#26368;&#23567;&#35201;&#27714;&#65292;&#24182;&#23398;&#20064;&#35782;&#21035;&#21644;&#39044;&#27979;&#38750;&#20256;&#32479;&#22522;&#36136;&#20013;&#22522;&#26412;&#26234;&#21147;&#24418;&#24335;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23545;&#32463;&#20856;&#25490;&#24207;&#31639;&#27861;&#30340;&#34892;&#20026;&#24212;&#29992;&#20102;&#26032;&#39062;&#30340;&#20998;&#26512;&#26041;&#27861;&#65292;&#36825;&#20123;&#31639;&#27861;&#26159;&#38271;&#26399;&#20197;&#26469;&#34987;&#30740;&#31350;&#30340;&#30701;&#20195;&#30721;&#29255;&#27573;&#12290;&#20026;&#20102;&#23558;&#36825;&#20123;&#25490;&#24207;&#31639;&#27861;&#20316;&#20026;&#29983;&#29289;&#24418;&#24577;&#21457;&#29983;&#23398;&#21450;&#20854;&#33021;&#21147;&#27169;&#22411;&#36827;&#34892;&#30740;&#31350;&#65292;&#25105;&#20204;&#25171;&#30772;&#20102;&#20004;&#20010;&#20808;&#21069;&#26222;&#36941;&#23384;&#22312;&#30340;&#20551;&#35774;&#65306;&#33258;&#19978;&#32780;&#19979;&#25511;&#21046;&#65288;&#32780;&#26159;&#23637;&#31034;&#20102;&#25968;&#23383;&#25968;&#32452;&#20013;&#30340;&#27599;&#20010;&#20803;&#32032;&#22914;&#20309;&#26045;&#21152;&#26368;&#23567;&#30340;&#20027;&#21160;&#24615;&#24182;&#23454;&#29616;&#25490;&#24207;&#31574;&#30053;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
The emerging field of Diverse Intelligence seeks to identify, formalize, and understand commonalities in behavioral competencies across a wide range of implementations. Especially interesting are simple systems that provide unexpected examples of memory, decision-making, or problem-solving in substrates that at first glance do not appear to be complex enough to implement such capabilities. We seek to develop tools to help understand the minimal requirements for such capabilities, and to learn to recognize and predict basal forms of intelligence in unconventional substrates. Here, we apply novel analyses to the behavior of classical sorting algorithms, short pieces of code which have been studied for many decades. To study these sorting algorithms as a model of biological morphogenesis and its competencies, we break two formerly-ubiquitous assumptions: top-down control (instead, showing how each element within a array of numbers can exert minimal agency and implement sorting policies fr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;"&#21160;&#24577;&#23574;&#23792;&#22270;&#31070;&#32463;&#32593;&#32476;"&#65288;DSGNN&#65289;&#30340;&#26694;&#26550;&#65292;&#23427;&#23558;&#23574;&#23792;&#31070;&#32463;&#32593;&#32476;&#65288;SNNs&#65289;&#19982;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#32467;&#21512;&#36215;&#26469;&#65292;&#20197;&#35299;&#20915;&#21160;&#24577;&#22270;&#34920;&#31034;&#23398;&#20064;&#20013;&#30340;&#22797;&#26434;&#24615;&#21644;&#20869;&#23384;&#24320;&#38144;&#38382;&#39064;&#12290;DSGNN&#36890;&#36807;&#21160;&#24577;&#35843;&#25972;&#23574;&#23792;&#31070;&#32463;&#20803;&#30340;&#29366;&#24577;&#21644;&#36830;&#25509;&#26435;&#37325;&#65292;&#22312;&#20256;&#25773;&#36807;&#31243;&#20013;&#20445;&#25345;&#22270;&#32467;&#26500;&#20449;&#24687;&#30340;&#23436;&#25972;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.05373</link><description>&lt;p&gt;
&#21160;&#24577;&#23574;&#23792;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Dynamic Spiking Graph Neural Networks. (arXiv:2401.05373v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05373
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;"&#21160;&#24577;&#23574;&#23792;&#22270;&#31070;&#32463;&#32593;&#32476;"&#65288;DSGNN&#65289;&#30340;&#26694;&#26550;&#65292;&#23427;&#23558;&#23574;&#23792;&#31070;&#32463;&#32593;&#32476;&#65288;SNNs&#65289;&#19982;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#32467;&#21512;&#36215;&#26469;&#65292;&#20197;&#35299;&#20915;&#21160;&#24577;&#22270;&#34920;&#31034;&#23398;&#20064;&#20013;&#30340;&#22797;&#26434;&#24615;&#21644;&#20869;&#23384;&#24320;&#38144;&#38382;&#39064;&#12290;DSGNN&#36890;&#36807;&#21160;&#24577;&#35843;&#25972;&#23574;&#23792;&#31070;&#32463;&#20803;&#30340;&#29366;&#24577;&#21644;&#36830;&#25509;&#26435;&#37325;&#65292;&#22312;&#20256;&#25773;&#36807;&#31243;&#20013;&#20445;&#25345;&#22270;&#32467;&#26500;&#20449;&#24687;&#30340;&#23436;&#25972;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#23574;&#23792;&#31070;&#32463;&#32593;&#32476;&#65288;SNNs&#65289;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#30456;&#32467;&#21512;&#28176;&#28176;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#20851;&#27880;&#65292;&#36825;&#26159;&#22240;&#20026;&#23427;&#22312;&#22788;&#29702;&#30001;&#22270;&#34920;&#31034;&#30340;&#38750;&#27431;&#20960;&#37324;&#24471;&#25968;&#25454;&#26102;&#20855;&#26377;&#20302;&#21151;&#32791;&#21644;&#39640;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#20316;&#20026;&#19968;&#20010;&#24120;&#35265;&#30340;&#38382;&#39064;&#65292;&#21160;&#24577;&#22270;&#34920;&#31034;&#23398;&#20064;&#38754;&#20020;&#30528;&#39640;&#22797;&#26434;&#24615;&#21644;&#22823;&#20869;&#23384;&#24320;&#38144;&#30340;&#25361;&#25112;&#12290;&#30446;&#21069;&#30340;&#24037;&#20316;&#36890;&#24120;&#36890;&#36807;&#20351;&#29992;&#20108;&#36827;&#21046;&#29305;&#24449;&#32780;&#19981;&#26159;&#36830;&#32493;&#29305;&#24449;&#30340;SNNs&#26469;&#26367;&#20195;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNNs&#65289;&#36827;&#34892;&#39640;&#25928;&#35757;&#32451;&#65292;&#36825;&#20250;&#24573;&#35270;&#22270;&#32467;&#26500;&#20449;&#24687;&#24182;&#22312;&#20256;&#25773;&#36807;&#31243;&#20013;&#23548;&#33268;&#32454;&#33410;&#30340;&#20002;&#22833;&#12290;&#27492;&#22806;&#65292;&#20248;&#21270;&#21160;&#24577;&#23574;&#23792;&#27169;&#22411;&#36890;&#24120;&#38656;&#35201;&#22312;&#26102;&#38388;&#27493;&#20043;&#38388;&#20256;&#25773;&#20449;&#24687;&#65292;&#36825;&#22686;&#21152;&#20102;&#20869;&#23384;&#38656;&#27714;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;"&#21160;&#24577;&#23574;&#23792;&#22270;&#31070;&#32463;&#32593;&#32476;"&#65288;\method{}&#65289;&#30340;&#26694;&#26550;&#12290;&#20026;&#20102;&#20943;&#36731;&#20449;&#24687;&#20002;&#22833;&#38382;&#39064;&#65292;\method{} &#22312;&#20256;&#25773;&#36807;&#31243;&#20013;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26426;&#21046;&#65292;&#23427;&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;&#39588;&#20013;&#21160;&#24577;&#22320;&#35843;&#25972;&#23574;&#23792;&#31070;&#32463;&#20803;&#30340;&#29366;&#24577;&#21644;&#36830;&#25509;&#26435;&#37325;&#65292;&#20197;&#20445;&#25345;&#22270;&#32467;&#26500;&#20449;&#24687;&#30340;&#23436;&#25972;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The integration of Spiking Neural Networks (SNNs) and Graph Neural Networks (GNNs) is gradually attracting attention due to the low power consumption and high efficiency in processing the non-Euclidean data represented by graphs. However, as a common problem, dynamic graph representation learning faces challenges such as high complexity and large memory overheads. Current work often uses SNNs instead of Recurrent Neural Networks (RNNs) by using binary features instead of continuous ones for efficient training, which would overlooks graph structure information and leads to the loss of details during propagation. Additionally, optimizing dynamic spiking models typically requires propagation of information across time steps, which increases memory requirements. To address these challenges, we present a framework named \underline{Dy}namic \underline{S}p\underline{i}king \underline{G}raph \underline{N}eural Networks (\method{}). To mitigate the information loss problem, \method{} propagates
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;AutoFragDiff&#65292;&#19968;&#31181;&#22522;&#20110;&#29255;&#27573;&#30340;&#33258;&#22238;&#24402;&#25193;&#25955;&#27169;&#22411;&#65292;&#29992;&#20110;&#29983;&#25104;3D&#20998;&#23376;&#32467;&#26500;&#12290;&#36890;&#36807;&#39044;&#27979;&#20998;&#23376;&#29255;&#27573;&#30340;&#21407;&#23376;&#31867;&#22411;&#21644;&#31354;&#38388;&#22352;&#26631;&#65292;&#25913;&#21892;&#20102;&#29983;&#25104;&#20998;&#23376;&#30340;&#23616;&#37096;&#20960;&#20309;&#32467;&#26500;&#65292;&#24182;&#20445;&#25345;&#39640;&#30340;&#32467;&#21512;&#20146;&#21644;&#21147;&#12290;&#22914;&#26524;&#25552;&#20379;&#36215;&#22987;&#20998;&#23376;&#39592;&#26550;&#65292;&#27169;&#22411;&#36824;&#21487;&#20197;&#36827;&#34892;&#25193;&#23637;&#12290;</title><link>http://arxiv.org/abs/2401.05370</link><description>&lt;p&gt;
&#22522;&#20110;&#33258;&#22238;&#24402;&#29255;&#27573;&#25193;&#25955;&#30340;&#32467;&#21512;&#20301;&#24863;&#30693;&#37197;&#20307;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Autoregressive fragment-based diffusion for pocket-aware ligand design. (arXiv:2401.05370v1 [q-bio.BM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05370
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;AutoFragDiff&#65292;&#19968;&#31181;&#22522;&#20110;&#29255;&#27573;&#30340;&#33258;&#22238;&#24402;&#25193;&#25955;&#27169;&#22411;&#65292;&#29992;&#20110;&#29983;&#25104;3D&#20998;&#23376;&#32467;&#26500;&#12290;&#36890;&#36807;&#39044;&#27979;&#20998;&#23376;&#29255;&#27573;&#30340;&#21407;&#23376;&#31867;&#22411;&#21644;&#31354;&#38388;&#22352;&#26631;&#65292;&#25913;&#21892;&#20102;&#29983;&#25104;&#20998;&#23376;&#30340;&#23616;&#37096;&#20960;&#20309;&#32467;&#26500;&#65292;&#24182;&#20445;&#25345;&#39640;&#30340;&#32467;&#21512;&#20146;&#21644;&#21147;&#12290;&#22914;&#26524;&#25552;&#20379;&#36215;&#22987;&#20998;&#23376;&#39592;&#26550;&#65292;&#27169;&#22411;&#36824;&#21487;&#20197;&#36827;&#34892;&#25193;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;AutoFragDiff&#65292;&#19968;&#31181;&#22522;&#20110;&#29255;&#27573;&#30340;&#33258;&#22238;&#24402;&#25193;&#25955;&#27169;&#22411;&#65292;&#29992;&#20110;&#26681;&#25454;&#30446;&#26631;&#34507;&#30333;&#32467;&#26500;&#29983;&#25104;3D&#20998;&#23376;&#32467;&#26500;&#12290;&#25105;&#20204;&#20351;&#29992;&#20960;&#20309;&#21521;&#37327;&#24863;&#30693;&#22120;&#26469;&#39044;&#27979;&#20998;&#23376;&#39592;&#26550;&#21644;&#34507;&#30333;&#36127;&#34955;&#26465;&#20214;&#19979;&#26032;&#20998;&#23376;&#29255;&#27573;&#30340;&#21407;&#23376;&#31867;&#22411;&#21644;&#31354;&#38388;&#22352;&#26631;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25913;&#21892;&#20102;&#29983;&#25104;&#30340;3D&#20998;&#23376;&#30340;&#23616;&#37096;&#20960;&#20309;&#32467;&#26500;&#65292;&#21516;&#26102;&#20445;&#25345;&#39640;&#39044;&#27979;&#30340;&#32467;&#21512;&#20146;&#21644;&#21147;&#12290;&#35813;&#27169;&#22411;&#36824;&#21487;&#20197;&#26681;&#25454;&#29992;&#25143;&#25552;&#20379;&#30340;&#36215;&#22987;&#20998;&#23376;&#39592;&#26550;&#36827;&#34892;&#25193;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we introduce AutoFragDiff, a fragment-based autoregressive diffusion model for generating 3D molecular structures conditioned on target protein structures. We employ geometric vector perceptrons to predict atom types and spatial coordinates of new molecular fragments conditioned on molecular scaffolds and protein pockets. Our approach improves the local geometry of the resulting 3D molecules while maintaining high predicted binding affinity to protein targets. The model can also perform scaffold extension from user-provided starting molecular scaffold.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DualTeacher&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#21322;&#30417;&#30563;&#22686;&#37327;&#30446;&#26631;&#26816;&#27979;&#38382;&#39064;&#12290;&#35813;&#31639;&#27861;&#33021;&#22815;&#22312;&#20165;&#26377;&#23569;&#37327;&#26631;&#27880;&#25968;&#25454;&#21644;&#22823;&#37327;&#26410;&#26631;&#27880;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#36880;&#28176;&#23398;&#20064;&#26032;&#30340;&#30446;&#26631;&#31867;&#21035;&#65292;&#21516;&#26102;&#36991;&#20813;&#23545;&#26087;&#31867;&#21035;&#30340;&#36951;&#24536;&#12290;</title><link>http://arxiv.org/abs/2401.05362</link><description>&lt;p&gt;
DualTeacher: &#26725;&#25509;&#38750;&#26631;&#23450;&#31867;&#21035;&#30340;&#21327;&#21516;&#23384;&#22312;&#65292;&#23454;&#29616;&#21322;&#30417;&#30563;&#22686;&#37327;&#30446;&#26631;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
DualTeacher: Bridging Coexistence of Unlabelled Classes for Semi-supervised Incremental Object Detection. (arXiv:2401.05362v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05362
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DualTeacher&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#21322;&#30417;&#30563;&#22686;&#37327;&#30446;&#26631;&#26816;&#27979;&#38382;&#39064;&#12290;&#35813;&#31639;&#27861;&#33021;&#22815;&#22312;&#20165;&#26377;&#23569;&#37327;&#26631;&#27880;&#25968;&#25454;&#21644;&#22823;&#37327;&#26410;&#26631;&#27880;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#36880;&#28176;&#23398;&#20064;&#26032;&#30340;&#30446;&#26631;&#31867;&#21035;&#65292;&#21516;&#26102;&#36991;&#20813;&#23545;&#26087;&#31867;&#21035;&#30340;&#36951;&#24536;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#20013;&#65292;&#30446;&#26631;&#26816;&#27979;&#22120;&#32463;&#24120;&#20250;&#36935;&#21040;&#26032;&#31867;&#21035;&#30340;&#30446;&#26631;&#23454;&#20363;&#65292;&#24182;&#19988;&#38656;&#35201;&#26377;&#25928;&#22320;&#36866;&#24212;&#23427;&#20204;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#23558;&#36825;&#20010;&#20851;&#38190;&#38382;&#39064;&#24418;&#24335;&#21270;&#20026;&#22686;&#37327;&#30446;&#26631;&#26816;&#27979; (IOD)&#65292;&#20551;&#35774;&#26032;&#31867;&#21035;&#30340;&#30446;&#26631;&#23454;&#20363;&#22312;&#22686;&#37327;&#25968;&#25454;&#20013;&#26159;&#23436;&#20840;&#27880;&#37322;&#30340;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#30417;&#30563;&#20449;&#21495;&#36890;&#24120;&#24456;&#31232;&#23569;&#21644;&#26114;&#36149;&#65292;&#30417;&#30563;&#24335;&#30340;&#22686;&#37327;&#30446;&#26631;&#26816;&#27979;&#22312;&#23454;&#38469;&#23454;&#26045;&#20013;&#21487;&#33021;&#19981;&#20999;&#23454;&#38469;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#26356;&#29616;&#23454;&#30340;&#35774;&#23450;&#65292;&#31216;&#20026;&#21322;&#30417;&#30563;&#22686;&#37327;&#30446;&#26631;&#26816;&#27979; (SSIOD)&#65292;&#20854;&#20013;&#30446;&#26631;&#26816;&#27979;&#22120;&#38656;&#35201;&#20174;&#23569;&#37327;&#26631;&#27880;&#25968;&#25454;&#21644;&#22823;&#37327;&#26410;&#26631;&#27880;&#25968;&#25454;&#20013;&#36880;&#28176;&#23398;&#20064;&#26032;&#30340;&#31867;&#21035;&#65292;&#21516;&#26102;&#19981;&#23545;&#26087;&#31867;&#21035;&#20135;&#29983;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#19968;&#31181;&#24120;&#29992;&#30340;&#30417;&#30563;&#24335;IOD&#31574;&#30053;&#26159;&#40723;&#21169;&#24403;&#21069;&#27169;&#22411;&#65288;&#20316;&#20026;&#23398;&#29983;&#65289;&#27169;&#20223;&#26087;&#27169;&#22411;&#65288;&#20316;&#20026;&#32769;&#24072;&#65289;&#30340;&#34892;&#20026;&#65292;&#20294;&#22312;SSIOD&#20013;&#36890;&#24120;&#22833;&#36133;&#65292;&#22240;&#20026;&#22823;&#37096;&#20998;&#26469;&#33258;&#26087;&#31867;&#21035;&#21644;&#26032;&#31867;&#21035;&#30340;&#30446;&#26631;&#23454;&#20363;&#26159;&#20849;&#23384;&#19988;&#26410;&#26631;&#27880;&#30340;&#65292;&#21482;&#26377;&#32769;&#24072;&#27169;&#22411;&#21487;&#20197;&#25552;&#20379;&#23569;&#37327;&#30340;&#30417;&#30563;&#20449;&#21495;&#12290;
&lt;/p&gt;
&lt;p&gt;
In real-world applications, an object detector often encounters object instances from new classes and needs to accommodate them effectively. Previous work formulated this critical problem as incremental object detection (IOD), which assumes the object instances of new classes to be fully annotated in incremental data. However, as supervisory signals are usually rare and expensive, the supervised IOD may not be practical for implementation. In this work, we consider a more realistic setting named semi-supervised IOD (SSIOD), where the object detector needs to learn new classes incrementally from a few labelled data and massive unlabelled data without catastrophic forgetting of old classes. A commonly-used strategy for supervised IOD is to encourage the current model (as a student) to mimic the behavior of the old model (as a teacher), but it generally fails in SSIOD because a dominant number of object instances from old and new classes are coexisting and unlabelled, with the teacher onl
&lt;/p&gt;</description></item><item><title>&#24320;&#21457;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#34920;&#38754;&#32570;&#38519;&#26816;&#27979;&#30340;&#36164;&#28304;&#21463;&#38480;EdgeAI&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#20855;&#26377;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.05355</link><description>&lt;p&gt;
&#24320;&#21457;&#36866;&#29992;&#20110;&#34920;&#38754;&#32570;&#38519;&#26816;&#27979;&#30340;&#36164;&#28304;&#21463;&#38480;EdgeAI&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Developing a Resource-Constraint EdgeAI model for Surface Defect Detection. (arXiv:2401.05355v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05355
&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#34920;&#38754;&#32570;&#38519;&#26816;&#27979;&#30340;&#36164;&#28304;&#21463;&#38480;EdgeAI&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#20855;&#26377;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36164;&#28304;&#38480;&#21046;&#38480;&#21046;&#20102;&#20960;&#20010;EdgeAI&#24212;&#29992;&#31243;&#24207;&#21482;&#33021;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#25512;&#29702;&#26041;&#27861;&#65292;&#20854;&#20013;&#27169;&#22411;&#22312;&#20113;&#31471;&#35757;&#32451;&#24182;&#37096;&#32626;&#21040;&#36793;&#32536;&#35774;&#22791;&#12290;&#36825;&#24102;&#26469;&#20102;&#35832;&#22914;&#24102;&#23485;&#12289;&#24310;&#36831;&#21644;&#38544;&#31169;&#31561;&#19982;&#23384;&#20648;&#25968;&#25454;&#22312;&#22806;&#37096;&#36827;&#34892;&#27169;&#22411;&#26500;&#24314;&#30340;&#25361;&#25112;&#12290;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#36827;&#34892;&#35757;&#32451;&#21487;&#20197;&#36890;&#36807;&#28040;&#38500;&#23558;&#25968;&#25454;&#20256;&#36755;&#21040;&#21478;&#19968;&#35774;&#22791;&#36827;&#34892;&#23384;&#20648;&#21644;&#27169;&#22411;&#24320;&#21457;&#30340;&#38656;&#27714;&#26469;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#12290;&#22312;&#35774;&#22791;&#19978;&#36827;&#34892;&#35757;&#32451;&#36824;&#21487;&#20197;&#25552;&#20379;&#23545;&#25968;&#25454;&#21464;&#21270;&#30340;&#31283;&#20581;&#24615;&#65292;&#22240;&#20026;&#21487;&#20197;&#20351;&#29992;&#26032;&#33719;&#21462;&#30340;&#25968;&#25454;&#23545;&#27169;&#22411;&#36827;&#34892;&#37325;&#26032;&#35757;&#32451;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;EdgeAI&#26550;&#26500;&#65292;&#25913;&#32534;&#33258;Xception&#65292;&#29992;&#20110;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#36793;&#32536;&#29615;&#22659;&#20013;&#36827;&#34892;&#35774;&#22791;&#19978;&#30340;&#35757;&#32451;&#12290;&#25105;&#20204;&#23545;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;PCB&#32570;&#38519;&#26816;&#27979;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#24182;&#23558;&#20854;&#24615;&#33021;&#19982;&#29616;&#26377;&#30340;&#36731;&#37327;&#32423;&#27169;&#22411;-MobileNetV2&#12289;EfficientNetV2B0&#21644;MobileViT-XXS&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#23454;&#39564;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#20855;&#26377;&#26174;&#30528;&#30340;&#24615;&#33021;&#65292;&#27979;&#35797;&#20934;&#30830;&#29575;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
Resource constraints have restricted several EdgeAI applications to machine learning inference approaches, where models are trained on the cloud and deployed to the edge device. This poses challenges such as bandwidth, latency, and privacy associated with storing data off-site for model building. Training on the edge device can overcome these challenges by eliminating the need to transfer data to another device for storage and model development. On-device training also provides robustness to data variations as models can be retrained on newly acquired data to improve performance. We, therefore, propose a lightweight EdgeAI architecture modified from Xception, for on-device training in a resource-constraint edge environment. We evaluate our model on a PCB defect detection task and compare its performance against existing lightweight models - MobileNetV2, EfficientNetV2B0, and MobileViT-XXS. The results of our experiment show that our model has a remarkable performance with a test accura
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#20013;&#30340;&#25506;&#32034;&#21644;&#24320;&#21457;&#24179;&#34913;&#38382;&#39064;&#65292;&#20197;&#21450;&#21033;&#29992;&#24191;&#20041;&#32463;&#39564;&#26469;&#21457;&#23637;&#36890;&#29992;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2401.05350</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#36816;&#31639;&#31526;&#36873;&#25321;&#21033;&#29992;&#24191;&#20041;&#32463;&#39564;
&lt;/p&gt;
&lt;p&gt;
Adaptive operator selection utilising generalised experience. (arXiv:2401.05350v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05350
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#20013;&#30340;&#25506;&#32034;&#21644;&#24320;&#21457;&#24179;&#34913;&#38382;&#39064;&#65292;&#20197;&#21450;&#21033;&#29992;&#24191;&#20041;&#32463;&#39564;&#26469;&#21457;&#23637;&#36890;&#29992;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20248;&#21270;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#65292;&#30001;&#20110;&#20854;&#22797;&#26434;&#24615;&#21644;&#38590;&#24230;&#32780;&#38590;&#20197;&#35299;&#20915;&#12290;&#36827;&#21270;&#21644;&#32676;&#20307;&#26234;&#33021;&#31639;&#27861;&#24050;&#25104;&#21151;&#35299;&#20915;&#36825;&#31867;&#38382;&#39064;&#65292;&#23588;&#20854;&#26159;&#22312;&#20108;&#36827;&#21046;&#26684;&#24335;&#20013;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#25506;&#32034;&#21644;&#24320;&#21457;&#27963;&#21160;&#20043;&#38388;&#30340;&#24179;&#34913;&#38382;&#39064;(EvE)&#65292;&#36825;&#31181;&#36817;&#20284;&#21487;&#33021;&#21463;&#21040;&#24433;&#21709;&#65292;&#36825;&#20173;&#28982;&#26159;&#36825;&#20010;&#39046;&#22495;&#30340;&#20027;&#35201;&#25361;&#25112;&#12290;&#23613;&#31649;&#20351;&#29992;&#22810;&#20010;&#36816;&#31639;&#31526;&#36827;&#34892;&#20114;&#34917;&#20351;&#29992;&#30340;&#33258;&#36866;&#24212;&#36816;&#31639;&#31526;&#36873;&#25321;&#26041;&#26696;&#22312;&#31649;&#29702;EvE&#26041;&#38754;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#20294;&#23450;&#21046;&#30340;&#33258;&#36866;&#24212;&#36873;&#25321;&#31995;&#32479;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#30740;&#31350;&#35758;&#39064;&#12290;&#26368;&#36817;&#65292;&#24378;&#21270;&#23398;&#20064;(RL)&#34987;&#25552;&#20986;&#20316;&#20026;&#19968;&#31181;&#23450;&#21046;&#21644;&#22609;&#36896;&#39640;&#24230;&#26377;&#25928;&#30340;&#33258;&#36866;&#24212;&#36873;&#25321;&#31995;&#32479;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#20174;&#21487;&#20280;&#32553;&#24615;&#30340;&#35282;&#24230;&#26469;&#22788;&#29702;&#35813;&#38382;&#39064;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;RL&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#24110;&#21161;&#24320;&#21457;&#36890;&#29992;&#26694;&#26550;&#65292;&#20197;&#33719;&#24471;&#12289;&#22788;&#29702;&#21644;&#21033;&#29992;&#24191;&#20041;&#32463;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Optimisation problems, particularly combinatorial optimisation problems, are difficult to solve due to their complexity and hardness. Such problems have been successfully solved by evolutionary and swarm intelligence algorithms, especially in binary format. However, the approximation may suffer due to the the issues in balance between exploration and exploitation activities (EvE), which remain as the major challenge in this context. Although the complementary usage of multiple operators is becoming more popular for managing EvE with adaptive operator selection schemes, a bespoke adaptive selection system is still an important topic in research. Reinforcement Learning (RL) has recently been proposed as a way to customise and shape up a highly effective adaptive selection system. However, it is still challenging to handle the problem in terms of scalability. This paper proposes and assesses a RL-based novel approach to help develop a generalised framework for gaining, processing, and uti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#20174;FOON&#20013;&#20351;&#29992;&#25628;&#32034;&#31639;&#27861;&#26816;&#32034;&#20219;&#21153;&#26641;&#30340;&#26041;&#27861;&#65292;&#20197;&#24110;&#21161;&#26426;&#22120;&#20154;&#33258;&#21160;&#25191;&#34892;&#21508;&#31181;&#22797;&#26434;&#30340;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2401.05346</link><description>&lt;p&gt;
&#20174;FOON&#20351;&#29992;&#25628;&#32034;&#31639;&#27861;&#26816;&#32034;&#20219;&#21153;&#26641;
&lt;/p&gt;
&lt;p&gt;
Task tree retrieval from FOON using search algorithms. (arXiv:2401.05346v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05346
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20174;FOON&#20013;&#20351;&#29992;&#25628;&#32034;&#31639;&#27861;&#26816;&#32034;&#20219;&#21153;&#26641;&#30340;&#26041;&#27861;&#65292;&#20197;&#24110;&#21161;&#26426;&#22120;&#20154;&#33258;&#21160;&#25191;&#34892;&#21508;&#31181;&#22797;&#26434;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#20154;&#21487;&#20197;&#33258;&#21160;&#21270;&#20219;&#21153;&#24182;&#20943;&#23569;&#20154;&#31867;&#25152;&#38656;&#30340;&#24037;&#20316;&#37327;&#65292;&#20294;&#35201;&#35753;&#26426;&#22120;&#20154;&#30693;&#36947;&#22914;&#20309;&#25191;&#34892;&#20219;&#21153;&#65292;&#25105;&#20204;&#38656;&#35201;&#32473;&#23427;&#28165;&#26224;&#30340;&#27493;&#39588;&#12290;&#20026;&#20102;&#35299;&#20915;&#32473;&#26426;&#22120;&#20154;&#25552;&#20379;&#27599;&#20010;&#21487;&#33021;&#20219;&#21153;&#30340;&#25351;&#20196;&#20960;&#20046;&#26159;&#19981;&#21487;&#33021;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#21019;&#24314;&#21644;&#25193;&#23637;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#21151;&#33021;&#38754;&#21521;&#23545;&#35937;&#32593;&#32476;&#65288;FOON&#65289;&#65292;&#24182;&#19988;&#25317;&#26377;&#22823;&#37327;&#29616;&#26377;&#30340;&#37197;&#26041;&#20449;&#24687;&#12290;&#20294;&#26159;&#23545;&#20110;&#26426;&#22120;&#20154;&#26469;&#35828;&#65292;&#26576;&#20123;&#20219;&#21153;&#24456;&#22797;&#26434;&#65292;&#21516;&#26679;&#22320;&#65292;&#23545;&#20110;&#20154;&#31867;&#26469;&#35828;&#20063;&#26377;&#19968;&#20123;&#22797;&#26434;&#30340;&#20219;&#21153;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20026;&#21151;&#33021;&#21333;&#20803;&#28155;&#21152;&#20102;&#26435;&#37325;&#65292;&#20197;&#34920;&#31034;&#26426;&#22120;&#20154;&#25191;&#34892;&#21160;&#20316;&#30340;&#25104;&#21151;&#27010;&#29575;&#12290;&#36890;&#36807;&#32473;&#23450;&#19968;&#20123;&#21416;&#25151;&#29992;&#21697;&#21644;&#30446;&#26631;&#33410;&#28857;&#65292;&#20351;&#29992;&#36890;&#29992;&#30340;FOON&#65292;&#26426;&#22120;&#20154;&#24517;&#39035;&#33021;&#22815;&#30830;&#23450;&#21416;&#25151;&#20013;&#26159;&#21542;&#23384;&#22312;&#25152;&#38656;&#30340;&#29289;&#21697;&#65292;&#24182;&#19988;&#22914;&#26524;&#26159;&#65292;&#33719;&#24471;&#23558;&#25152;&#38656;&#30340;&#21416;&#25151;&#29992;&#21697;&#36716;&#25442;&#20026;&#30446;&#26631;&#33410;&#28857;&#30340;&#27493;&#39588;&#12290;&#29616;&#22312;&#36890;&#36807;&#26412;&#25991;&#65292;&#25105;&#20204;&#20351;&#29992;&#20004;&#31181;&#31639;&#27861;&#65288;IDS&#21644;GBFS&#65289;&#26469;&#26816;&#32034;&#20219;&#21153;&#26641;&#65288;&#22914;&#26524;p
&lt;/p&gt;
&lt;p&gt;
Robots can be very useful to automate tasks and reduce the human effort required. But for the robot to know, how to perform tasks, we need to give it a clear set of steps to follow. It is nearly impossible to provide a robot with instructions for every possible task. Therefore we have a Universal Functional object-oriented network (FOON) which was created and expanded and has a lot of existing recipe information [1]. But certain tasks are complicated for robots to perform and similarly, some tasks are complicated for humans to perform. Therefore weights have been added to functional units to represent the chance of successful execution of the motion by the robot [2]. Given a set of kitchen items and a goal node, using Universal FOON, a robot must be able to determine if the required items are present in the kitchen, and if yes, get the steps to convert the required kitchen items to the goal node. Now through this paper, we use two algorithms (IDS and GBFS) to retrieve a task tree (if p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26368;&#20855;&#21306;&#20998;&#24615;&#21050;&#28608;&#29289;&#30340;&#20248;&#21270;&#32858;&#31867;&#26041;&#27861;&#65292;&#25104;&#21151;&#22320;&#35782;&#21035;&#20102;&#23567;&#40736;&#35270;&#32593;&#33180;&#12289;&#24658;&#27827;&#29492;&#35270;&#32593;&#33180;&#21644;&#29461;&#29492;V4&#35270;&#35273;&#21306;&#30340;&#21151;&#33021;&#32454;&#32990;&#31867;&#22411;&#12290;</title><link>http://arxiv.org/abs/2401.05342</link><description>&lt;p&gt;
&#26368;&#20855;&#21306;&#20998;&#24615;&#30340;&#21050;&#28608;&#29289;&#29992;&#20110;&#21151;&#33021;&#32454;&#32990;&#31867;&#22411;&#30340;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Most discriminative stimuli for functional cell type identification. (arXiv:2401.05342v1 [q-bio.NC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05342
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26368;&#20855;&#21306;&#20998;&#24615;&#21050;&#28608;&#29289;&#30340;&#20248;&#21270;&#32858;&#31867;&#26041;&#27861;&#65292;&#25104;&#21151;&#22320;&#35782;&#21035;&#20102;&#23567;&#40736;&#35270;&#32593;&#33180;&#12289;&#24658;&#27827;&#29492;&#35270;&#32593;&#33180;&#21644;&#29461;&#29492;V4&#35270;&#35273;&#21306;&#30340;&#21151;&#33021;&#32454;&#32990;&#31867;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35782;&#21035;&#32454;&#32990;&#31867;&#22411;&#24182;&#29702;&#35299;&#20854;&#21151;&#33021;&#29305;&#24615;&#23545;&#25581;&#31034;&#24863;&#30693;&#21644;&#35748;&#30693;&#26426;&#21046;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#35270;&#32593;&#33180;&#20013;&#65292;&#21487;&#20197;&#36890;&#36807;&#31934;&#24515;&#36873;&#25321;&#30340;&#21050;&#28608;&#29289;&#26469;&#35782;&#21035;&#21151;&#33021;&#31867;&#22411;&#65292;&#20294;&#36825;&#38656;&#35201;&#19987;&#19994;&#39046;&#22495;&#30693;&#35782;&#65292;&#24182;&#20250;&#23545;&#20197;&#21069;&#24050;&#30693;&#30340;&#32454;&#32990;&#31867;&#22411;&#20135;&#29983;&#20559;&#35265;&#12290;&#22312;&#35270;&#35273;&#30382;&#23618;&#20013;&#65292;&#20173;&#28982;&#19981;&#30693;&#36947;&#23384;&#22312;&#20160;&#20040;&#21151;&#33021;&#31867;&#22411;&#20197;&#21450;&#22914;&#20309;&#35782;&#21035;&#23427;&#20204;&#12290;&#22240;&#27492;&#65292;&#38656;&#35201;&#26032;&#30340;&#26041;&#27861;&#26469;&#23545;&#35270;&#32593;&#33180;&#21644;&#35270;&#35273;&#30382;&#23618;&#20013;&#30340;&#21151;&#33021;&#32454;&#32990;&#31867;&#22411;&#36827;&#34892;&#26080;&#20559;&#35265;&#30340;&#35782;&#21035;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20248;&#21270;&#30340;&#32858;&#31867;&#26041;&#27861;&#65292;&#20351;&#29992;&#26368;&#20855;&#21306;&#20998;&#24615;&#30340;&#21050;&#28608;&#29289;&#65288;MDS&#65289;&#26469;&#33719;&#24471;&#31070;&#32463;&#20803;&#30340;&#21151;&#33021;&#32858;&#31867;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#21050;&#28608;&#29289;&#30340;&#20248;&#21270;&#21644;&#32858;&#31867;&#37325;&#26032;&#20998;&#37197;&#20043;&#38388;&#30340;&#20132;&#26367;&#36827;&#34892;&#65292;&#31867;&#20284;&#20110;&#26399;&#26395;&#26368;&#22823;&#21270;&#31639;&#27861;&#12290;&#35813;&#31639;&#27861;&#25104;&#21151;&#24674;&#22797;&#20102;&#23567;&#40736;&#35270;&#32593;&#33180;&#12289;&#24658;&#27827;&#29492;&#35270;&#32593;&#33180;&#21644;&#29461;&#29492;V4&#35270;&#35273;&#21306;&#30340;&#21151;&#33021;&#32858;&#31867;&#12290;&#36825;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#25104;&#21151;&#22320;&#36827;&#34892;&#21151;&#33021;&#32454;&#32990;&#31867;&#22411;&#30340;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
Identifying cell types and understanding their functional properties is crucial for unraveling the mechanisms underlying perception and cognition. In the retina, functional types can be identified by carefully selected stimuli, but this requires expert domain knowledge and biases the procedure towards previously known cell types. In the visual cortex, it is still unknown what functional types exist and how to identify them. Thus, for unbiased identification of the functional cell types in retina and visual cortex, new approaches are needed. Here we propose an optimization-based clustering approach using deep predictive models to obtain functional clusters of neurons using Most Discriminative Stimuli (MDS). Our approach alternates between stimulus optimization with cluster reassignment akin to an expectation-maximization algorithm. The algorithm recovers functional clusters in mouse retina, marmoset retina and macaque visual area V4. This demonstrates that our approach can successfully 
&lt;/p&gt;</description></item><item><title>RoSA&#26159;&#19968;&#31181;&#26032;&#30340;PEFT&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#39044;&#35757;&#32451;&#26435;&#37325;&#19978;&#35757;&#32451;&#20302;&#31209;&#21644;&#39640;&#24230;&#31232;&#30095;&#30340;&#32452;&#20214;&#65292;&#20197;&#39640;&#25928;&#36817;&#20284;&#23436;&#20840;&#24494;&#35843;&#30340;&#24615;&#33021;&#65292;&#26469;&#23454;&#29616;&#20934;&#30830;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#12290;&#22312;&#22810;&#20010;&#29983;&#25104;&#20219;&#21153;&#20013;&#65292;RoSA&#34920;&#29616;&#20986;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.04679</link><description>&lt;p&gt;
RoSA: &#36890;&#36807;&#40065;&#26834;&#36866;&#24212;&#23454;&#29616;&#20934;&#30830;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
RoSA: Accurate Parameter-Efficient Fine-Tuning via Robust Adaptation. (arXiv:2401.04679v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04679
&lt;/p&gt;
&lt;p&gt;
RoSA&#26159;&#19968;&#31181;&#26032;&#30340;PEFT&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#39044;&#35757;&#32451;&#26435;&#37325;&#19978;&#35757;&#32451;&#20302;&#31209;&#21644;&#39640;&#24230;&#31232;&#30095;&#30340;&#32452;&#20214;&#65292;&#20197;&#39640;&#25928;&#36817;&#20284;&#23436;&#20840;&#24494;&#35843;&#30340;&#24615;&#33021;&#65292;&#26469;&#23454;&#29616;&#20934;&#30830;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#12290;&#22312;&#22810;&#20010;&#29983;&#25104;&#20219;&#21153;&#20013;&#65292;RoSA&#34920;&#29616;&#20986;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#22823;&#35821;&#35328;&#27169;&#22411; (LLMs) &#30340;&#32972;&#26223;&#19979;&#65292;&#33021;&#22815;&#22312;&#26377;&#38480;&#30340;&#35745;&#31639;&#21644;&#20869;&#23384;&#39044;&#31639;&#19979;&#25552;&#20379;&#33391;&#22909;&#20934;&#30830;&#24615;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843; (PEFT) &#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;PEFT&#26041;&#27861;&#65292;&#31216;&#20026;RoSA&#65292;&#21463;&#40065;&#26834;&#20027;&#25104;&#20998;&#20998;&#26512; (PCA) &#30340;&#21551;&#21457;&#65292;&#23427;&#22312;&#19968;&#32452;&#22266;&#23450;&#30340;&#39044;&#35757;&#32451;&#26435;&#37325;&#19978;&#20849;&#21516;&#35757;&#32451;$\textit{&#20302;&#31209;}$&#21644;$\textit{&#39640;&#24230;&#31232;&#30095;}$&#30340;&#32452;&#20214;&#65292;&#20197;&#39640;&#25928;&#36817;&#20284;&#23436;&#20840;&#24494;&#35843;&#65288;FFT&#65289;&#35299;&#20915;&#26041;&#26696;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;RoSA&#22312;&#19968;&#31995;&#21015;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#29983;&#25104;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#65292;&#20363;&#22914;&#23567;&#23398;&#25968;&#23398;&#21644;SQL&#26597;&#35810;&#29983;&#25104;&#65292;&#36825;&#20123;&#20219;&#21153;&#38656;&#35201;&#36827;&#34892;&#24494;&#35843;&#20197;&#33719;&#24471;&#33391;&#22909;&#24615;&#33021;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#30456;&#21516;&#30340;&#21442;&#25968;&#39044;&#31639;&#19979;&#65292;RoSA&#20248;&#20110;LoRA&#21644;&#32431;&#31929;&#30340;&#31232;&#30095;&#24494;&#35843;&#12290;&#25105;&#20204;&#36890;&#36807;&#31232;&#30095;GPU&#20869;&#26680;&#20026;RoSA&#25552;&#20379;&#31995;&#32479;&#25903;&#25345;&#65292;&#20197;&#34917;&#20805;&#35757;&#32451;&#31639;&#27861;&#65292;&#20174;&#32780;&#23454;&#29616;&#20869;&#23384;&#21644;&#35745;&#31639;&#25928;&#29575;&#30340;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#23558;&#22312;https://github.com/IST-DASLab&#19978;&#25552;&#20379;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate parameter-efficient fine-tuning (PEFT) methods that can provide good accuracy under limited computational and memory budgets in the context of large language models (LLMs). We present a new PEFT method called Robust Adaptation (RoSA) inspired by robust principal component analysis (PCA) that jointly trains $\textit{low-rank}$ and $\textit{highly-sparse}$ components on top of a set of fixed pretrained weights to efficiently approximate the performance of a full-fine-tuning (FFT) solution. Across a series of challenging generative tasks such as grade-school math and SQL query generation, which require fine-tuning for good performance, we show that RoSA outperforms both LoRA and pure sparse fine-tuning, at the same parameter budget. We provide system support for RoSA to complement the training algorithm, specifically in the form of sparse GPU kernels which enable memoryand computationally-efficient training. Our code will be made available at https://github.com/IST-DASLab
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;DebugBench&#30340;LLM&#35843;&#35797;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35843;&#35797;&#33021;&#21147;&#12290;&#30740;&#31350;&#21457;&#29616;&#38381;&#28304;&#27169;&#22411;&#19982;&#20154;&#31867;&#30456;&#27604;&#20855;&#26377;&#36739;&#20302;&#30340;&#35843;&#35797;&#24615;&#33021;&#65292;&#32780;&#24320;&#28304;&#27169;&#22411;&#26410;&#33021;&#36798;&#21040;&#21512;&#26684;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.04621</link><description>&lt;p&gt;
DebugBench: &#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35843;&#35797;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
DebugBench: Evaluating Debugging Capability of Large Language Models. (arXiv:2401.04621v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04621
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;DebugBench&#30340;LLM&#35843;&#35797;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35843;&#35797;&#33021;&#21147;&#12290;&#30740;&#31350;&#21457;&#29616;&#38381;&#28304;&#27169;&#22411;&#19982;&#20154;&#31867;&#30456;&#27604;&#20855;&#26377;&#36739;&#20302;&#30340;&#35843;&#35797;&#24615;&#33021;&#65292;&#32780;&#24320;&#28304;&#27169;&#22411;&#26410;&#33021;&#36798;&#21040;&#21512;&#26684;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23637;&#31034;&#20986;&#20102;&#20986;&#33394;&#30340;&#32534;&#30721;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#20316;&#20026;&#32534;&#31243;&#33021;&#21147;&#30340;&#21478;&#19968;&#20010;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65292;LLMs&#30340;&#35843;&#35797;&#33021;&#21147;&#20173;&#28982;&#30456;&#23545;&#26410;&#34987;&#25506;&#32034;&#12290;&#20043;&#21069;&#23545;LLMs&#30340;&#35843;&#35797;&#33021;&#21147;&#35780;&#20272;&#21463;&#21040;&#25968;&#25454;&#27844;&#38706;&#39118;&#38505;&#12289;&#25968;&#25454;&#38598;&#35268;&#27169;&#21644;&#27979;&#35797;&#28431;&#27934;&#31181;&#31867;&#30340;&#38480;&#21046;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#19981;&#36275;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;&#8220;DebugBench&#8221;&#30340;LLM&#35843;&#35797;&#22522;&#20934;&#65292;&#21253;&#21547;4253&#20010;&#23454;&#20363;&#12290;&#23427;&#28085;&#30422;&#20102;C ++&#65292;Java&#21644;Python&#20013;&#22235;&#20010;&#20027;&#35201;&#30340;&#38169;&#35823;&#31867;&#21035;&#21644;18&#20010;&#27425;&#35201;&#31867;&#22411;&#12290;&#20026;&#20102;&#26500;&#24314;DebugBench&#65292;&#25105;&#20204;&#20174;LeetCode&#31038;&#21306;&#25910;&#38598;&#20102;&#20195;&#30721;&#29255;&#27573;&#65292;&#20351;&#29992;GPT-4&#21521;&#28304;&#25968;&#25454;&#20013;&#27880;&#20837;&#38169;&#35823;&#65292;&#24182;&#36827;&#34892;&#20005;&#26684;&#30340;&#36136;&#37327;&#26816;&#26597;&#12290;&#25105;&#20204;&#22312;&#38646;&#26679;&#20363;&#24773;&#20917;&#19979;&#35780;&#20272;&#20102;&#20004;&#20010;&#21830;&#19994;&#27169;&#22411;&#21644;&#19977;&#20010;&#24320;&#28304;&#27169;&#22411;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#65288;1&#65289;&#19982;&#20154;&#31867;&#30456;&#27604;&#65292;&#38381;&#28304;&#27169;&#22411;&#22914;GPT-4&#34920;&#29616;&#20986;&#36739;&#20302;&#30340;&#35843;&#35797;&#24615;&#33021;&#65292;&#32780;&#24320;&#28304;&#27169;&#22411;&#22914;Code Llama&#26080;&#27861;&#36798;&#21040;&#20219;&#20309;&#21512;&#26684;&#29575;&#65307;&#65288;2&#65289;t
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have demonstrated exceptional coding capability. However, as another critical component of programming proficiency, the debugging capability of LLMs remains relatively unexplored. Previous evaluations of LLMs' debugging ability are significantly limited by the risk of data leakage, the scale of the dataset, and the variety of tested bugs. To overcome these deficiencies, we introduce `DebugBench', an LLM debugging benchmark consisting of 4,253 instances. It covers four major bug categories and 18 minor types in C++, Java, and Python. To construct DebugBench, we collect code snippets from the LeetCode community, implant bugs into source data with GPT-4, and assure rigorous quality checks. We evaluate two commercial and three open-source models in a zero-shot scenario. We find that (1) while closed-source models like GPT-4 exhibit inferior debugging performance compared to humans, open-source models such as Code Llama fail to attain any pass rate scores; (2) t
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27979;&#35797;&#20102;&#19968;&#31181;AI&#39550;&#39542;&#25945;&#32451;&#30340;&#35299;&#37322;&#23545;&#39550;&#39542;&#34920;&#29616;&#12289;&#35748;&#30693;&#36127;&#33655;&#12289;&#19987;&#19994;&#30693;&#35782;&#21644;&#20449;&#20219;&#30340;&#24433;&#21709;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;AI&#39550;&#39542;&#25945;&#32451;&#23545;&#20110;&#25945;&#25480;&#26032;&#25163;&#39550;&#39542;&#25216;&#33021;&#26159;&#26377;&#24110;&#21161;&#30340;&#65292;&#24182;&#19988;&#20449;&#24687;&#31867;&#22411;&#21644;&#21576;&#29616;&#26041;&#24335;&#23545;&#34920;&#29616;&#32467;&#26524;&#26377;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2401.04206</link><description>&lt;p&gt;
&#20174;AI&#25945;&#32451;&#23398;&#20064;&#36187;&#36710;&#65306;&#22810;&#27169;&#24577;&#33258;&#21160;&#39550;&#39542;&#35299;&#37322;&#23545;&#39550;&#39542;&#34920;&#29616;&#12289;&#35748;&#30693;&#36127;&#33655;&#12289;&#19987;&#19994;&#30693;&#35782;&#21644;&#20449;&#20219;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Learning Racing From an AI Coach: Effects of Multimodal Autonomous Driving Explanations on Driving Performance, Cognitive Load, Expertise, and Trust. (arXiv:2401.04206v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04206
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27979;&#35797;&#20102;&#19968;&#31181;AI&#39550;&#39542;&#25945;&#32451;&#30340;&#35299;&#37322;&#23545;&#39550;&#39542;&#34920;&#29616;&#12289;&#35748;&#30693;&#36127;&#33655;&#12289;&#19987;&#19994;&#30693;&#35782;&#21644;&#20449;&#20219;&#30340;&#24433;&#21709;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;AI&#39550;&#39542;&#25945;&#32451;&#23545;&#20110;&#25945;&#25480;&#26032;&#25163;&#39550;&#39542;&#25216;&#33021;&#26159;&#26377;&#24110;&#21161;&#30340;&#65292;&#24182;&#19988;&#20449;&#24687;&#31867;&#22411;&#21644;&#21576;&#29616;&#26041;&#24335;&#23545;&#34920;&#29616;&#32467;&#26524;&#26377;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19968;&#39033;&#21069;&#21518;&#23454;&#39564;&#20013;&#65288;n=41&#65289;&#65292;&#25105;&#20204;&#27979;&#35797;&#20102;&#27169;&#20223;&#20154;&#31867;&#39550;&#39542;&#19987;&#23478;&#30340;&#25351;&#23548;&#35828;&#26126;&#30340;AI&#25945;&#32451;&#30340;&#35299;&#37322;&#27807;&#36890;&#23545;&#39550;&#39542;&#34920;&#29616;&#12289;&#35748;&#30693;&#36127;&#33655;&#12289;&#20449;&#24515;&#12289;&#19987;&#19994;&#30693;&#35782;&#21644;&#20449;&#20219;&#30340;&#24433;&#21709;&#12290;&#21442;&#19982;&#32773;&#34987;&#20998;&#20026;&#22235;&#20010;&#32452;&#65292;&#35780;&#20272;&#20102;AI&#25945;&#32451;&#35299;&#37322;&#30340;&#20004;&#20010;&#32500;&#24230;&#65306;&#20449;&#24687;&#31867;&#22411;&#65288;'what'&#21644;'why'-type&#35299;&#37322;&#65289;&#21644;&#21576;&#29616;&#26041;&#24335;&#65288;&#21548;&#35273;&#21644;&#35270;&#35273;&#65289;&#12290;&#36890;&#36807;&#37319;&#35775;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#21442;&#19982;&#32773;&#30340;&#23398;&#20064;&#36807;&#31243;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;AI&#39550;&#39542;&#25945;&#32451;&#23545;&#20110;&#25945;&#25480;&#26032;&#25163;&#39550;&#39542;&#25216;&#33021;&#26159;&#26377;&#29992;&#30340;&#12290;&#27604;&#36739;&#21508;&#32452;&#20043;&#38388;&#65292;&#25105;&#20204;&#21457;&#29616;&#20449;&#24687;&#30340;&#31867;&#22411;&#21644;&#26041;&#24335;&#23545;&#24615;&#33021;&#32467;&#26524;&#26377;&#24433;&#21709;&#12290;&#25105;&#20204;&#23558;&#24046;&#24322;&#24402;&#22240;&#20110;&#20449;&#24687;&#22914;&#20309;&#24341;&#23548;&#27880;&#24847;&#21147;&#65292;&#20943;&#36731;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#24433;&#21709;&#21442;&#19982;&#32773;&#32463;&#21382;&#30340;&#36127;&#33655;&#36807;&#36733;&#12290;&#36825;&#21453;&#36807;&#26469;&#21448;&#24433;&#21709;&#20102;&#20449;&#24515;&#21644;&#20449;&#20219;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
In a pre-post experiment (n = 41), we test the impact of an AI Coach's explanatory communications modeled after the instructions of human driving experts. Participants were divided into four (4) groups to assess two (2) dimensions of the AI coach's explanations: information type ('what' and 'why'-type explanations) and presentation modality (auditory and visual). We directly compare how AI Coaching sessions employing these techniques impact driving performance, cognitive load, confidence, expertise, and trust in an observation learning context. Through interviews, we delineate the learning process of our participants. Results show that an AI driving coach can be useful for teaching performance driving skills to novices. Comparing between groups, we find the type and modality of information influences performance outcomes. We attribute differences to how information directed attention, mitigated uncertainty, and influenced overload experienced by participants. These, in turn, affected h
&lt;/p&gt;</description></item><item><title>&#30450;&#20154;&#35302;&#35273;&#31614;&#21517;&#31995;&#32479;&#26159;&#19968;&#31181;&#26080;&#38556;&#30861;&#21644;&#26377;&#25928;&#30340;&#31995;&#32479;&#65292;&#36890;&#36807;&#35302;&#35273;&#20114;&#21160;&#21644;&#35821;&#38899;&#31639;&#27861;&#24341;&#23548;&#65292;&#36171;&#20104;&#35270;&#38556;&#20154;&#22763;&#21019;&#24314;&#20010;&#24615;&#21270;&#25163;&#20889;&#31614;&#21517;&#30340;&#33021;&#21147;&#65292;&#20419;&#36827;&#20102;&#20182;&#20204;&#30340;&#29420;&#31435;&#21644;&#21253;&#23481;&#12290;</title><link>http://arxiv.org/abs/2401.04126</link><description>&lt;p&gt;
&#30450;&#20154;&#35302;&#35273;&#31614;&#21517;&#31995;&#32479;&#30340;&#27010;&#24565;
&lt;/p&gt;
&lt;p&gt;
The Concept of the Tactile Signature System for Individuals with Visual Impairments. (arXiv:2401.04126v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04126
&lt;/p&gt;
&lt;p&gt;
&#30450;&#20154;&#35302;&#35273;&#31614;&#21517;&#31995;&#32479;&#26159;&#19968;&#31181;&#26080;&#38556;&#30861;&#21644;&#26377;&#25928;&#30340;&#31995;&#32479;&#65292;&#36890;&#36807;&#35302;&#35273;&#20114;&#21160;&#21644;&#35821;&#38899;&#31639;&#27861;&#24341;&#23548;&#65292;&#36171;&#20104;&#35270;&#38556;&#20154;&#22763;&#21019;&#24314;&#20010;&#24615;&#21270;&#25163;&#20889;&#31614;&#21517;&#30340;&#33021;&#21147;&#65292;&#20419;&#36827;&#20102;&#20182;&#20204;&#30340;&#29420;&#31435;&#21644;&#21253;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32570;&#20047;&#19968;&#31181;&#26080;&#38556;&#30861;&#21644;&#26377;&#25928;&#30340;&#31995;&#32479;&#65292;&#20351;&#30450;&#20154;&#33021;&#22815;&#21019;&#24314;&#25163;&#20889;&#31614;&#21517;&#65292;&#36825;&#23545;&#20182;&#20204;&#30340;&#29420;&#31435;&#24615;&#21644;&#20840;&#38754;&#21442;&#19982;&#29983;&#27963;&#30340;&#21508;&#20010;&#26041;&#38754;&#26500;&#25104;&#20102;&#37325;&#22823;&#38556;&#30861;&#12290;&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#24320;&#21019;&#24615;&#30340;&#26041;&#27861;&#65292;&#21363;&#35302;&#35273;&#31614;&#21517;&#31995;&#32479;&#65292;&#36171;&#20104;&#35270;&#38556;&#20154;&#22763;&#24418;&#25104;&#29420;&#29305;&#30340;&#25163;&#20889;&#31614;&#21517;&#30340;&#33021;&#21147;&#12290;&#31995;&#32479;&#30340;&#20851;&#38190;&#29305;&#28857;&#21253;&#25324;&#65306;&#20010;&#24615;&#21270;&#23450;&#21046;&#65306;&#36890;&#36807;&#35302;&#35273;&#20114;&#21160;&#21644;&#35821;&#38899;&#31639;&#27861;&#24341;&#23548;&#65292;&#20010;&#20307;&#21487;&#20197;&#21019;&#24314;&#21453;&#26144;&#20854;&#20559;&#22909;&#21644;&#33258;&#28982;&#20070;&#20889;&#39118;&#26684;&#30340;&#31614;&#21517;&#12290;&#23454;&#26102;&#21453;&#39304;&#65306;AI&#39537;&#21160;&#30340;&#35821;&#38899;&#25552;&#31034;&#21644;&#20998;&#26512;&#30830;&#20445;&#31614;&#21517;&#30340;&#20934;&#30830;&#24615;&#21644;&#19968;&#33268;&#24615;&#12290;&#21487;&#35775;&#38382;&#24615;&#65306;&#22312;&#26412;&#22320;&#26381;&#21153;&#20013;&#24515;&#23433;&#35013;&#65292;&#25552;&#20379;&#23433;&#20840;&#30417;&#30563;&#30340;&#29615;&#22659;&#36827;&#34892;&#31614;&#21517;&#21019;&#24314;&#12290;&#35813;&#31995;&#32479;&#30340;&#24433;&#21709;&#36229;&#36234;&#20010;&#20307;&#23618;&#38754;&#65306;&#20419;&#36827;&#21253;&#23481;&#21644;&#29420;&#31435;&#65306;&#30450;&#20154;&#21487;&#20197;&#36827;&#34892;&#27861;&#24459;&#21644;&#37329;&#34701;&#20132;&#26131;&#32780;&#26080;&#38656;&#20381;&#36182;&#20182;&#20154;&#12290;
&lt;/p&gt;
&lt;p&gt;
The lack of an accessible and effective system for blind individuals to create handwritten signatures presents a significant barrier to their independence and full participation in various aspects of life. This research introduces the Tactile Signature System, a groundbreaking approach that empowers individuals with visual impairments to form their unique handwritten signatures. Key features of the system include: Personalized customization: Through tactile interaction and voice algorithmic guidance, individuals create signatures reflecting their preferences and natural writing style. Real-time feedback: AI-powered voice prompts and analysis ensure accuracy and consistency in signature formation. Accessibility: Installation in local service centers provides a secure and supervised environment for signature creation. The system's impact reaches beyond the individual level: Promotes inclusivity and independence: Blind individuals can engage in legal and financial transactions without rel
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#24494;&#23567;&#26102;&#38388;&#28151;&#21512;&#22120; (TTMs) &#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#38024;&#23545;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#30340;&#38646;/&#23569;&#26679;&#26412;&#39044;&#27979;&#36827;&#34892;&#20102;&#20248;&#21270;&#12290;&#19982;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#30456;&#27604;&#65292;TTMs&#27169;&#22411;&#26356;&#23567;&#12289;&#26356;&#24555;&#65292;&#24182;&#32771;&#34385;&#20102;&#36328;&#36890;&#36947;&#30456;&#20851;&#24615;&#65292;&#33021;&#22815;&#22312;&#30701;&#26102;&#38388;&#20869;&#36827;&#34892;&#26377;&#25928;&#30340;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2401.03955</link><description>&lt;p&gt;
&#24494;&#23567;&#26102;&#38388;&#28151;&#21512;&#22120; (TTMs): &#38024;&#23545;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#30340;&#22686;&#24378;&#38646;/&#23569;&#26679;&#26412;&#39044;&#27979;&#30340;&#24555;&#36895;&#39044;&#35757;&#32451;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Tiny Time Mixers (TTMs): Fast Pretrained Models for Enhanced Zero/Few-Shot Forecasting of Multivariate Time Series. (arXiv:2401.03955v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03955
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#24494;&#23567;&#26102;&#38388;&#28151;&#21512;&#22120; (TTMs) &#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#38024;&#23545;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#30340;&#38646;/&#23569;&#26679;&#26412;&#39044;&#27979;&#36827;&#34892;&#20102;&#20248;&#21270;&#12290;&#19982;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#30456;&#27604;&#65292;TTMs&#27169;&#22411;&#26356;&#23567;&#12289;&#26356;&#24555;&#65292;&#24182;&#32771;&#34385;&#20102;&#36328;&#36890;&#36947;&#30456;&#20851;&#24615;&#65292;&#33021;&#22815;&#22312;&#30701;&#26102;&#38388;&#20869;&#36827;&#34892;&#26377;&#25928;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38646;/&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#35821;&#35328;&#21644;&#35270;&#35273;&#39046;&#22495;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015; (TS) &#20013;&#38754;&#20020;&#30528;&#22810;&#26679;&#24615;&#21644;&#20844;&#24320;&#39044;&#35757;&#32451;&#25968;&#25454;&#31232;&#32570;&#30340;&#25361;&#25112;&#12290;&#22240;&#27492;&#65292;&#26368;&#36817;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLMs) &#36827;&#34892;&#21508;&#31181;&#36866;&#24212;&#30340;&#36235;&#21183;&#36880;&#28176;&#22686;&#21152;&#12290;&#36825;&#20123;&#26041;&#27861;&#21033;&#29992;&#36328;&#39046;&#22495;&#36801;&#31227;&#23398;&#20064;&#65292;&#20986;&#22855;&#22320;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#38750;&#24120;&#32531;&#24930;&#19988;&#24222;&#22823;&#65288;&#22823;&#32422;&#21313;&#20159;&#20010;&#21442;&#25968;&#65289;&#65292;&#24182;&#19988;&#19981;&#32771;&#34385;&#36328;&#36890;&#36947;&#30456;&#20851;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22810;&#23618;&#24494;&#23567;&#26102;&#38388;&#28151;&#21512;&#22120; (TTM)&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#36731;&#37327;&#32423; TSMixer &#32467;&#26500;&#30340;&#26174;&#33879;&#23567;&#22411;&#27169;&#22411;&#12290;TTM &#26159;&#39318;&#20010;&#25104;&#21151;&#24320;&#21457;&#30340;&#24494;&#22411;&#36890;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#65288;&#8804;100&#19975;&#20010;&#21442;&#25968;&#65289;&#65292;&#19987;&#38376;&#22312;&#20844;&#24320;TS&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24555;&#36895;&#35757;&#32451;&#65288;&#20165;&#38656;4-8&#23567;&#26102;&#65289;&#65292;&#20855;&#26377;&#26377;&#25928;&#30340;&#36801;&#31227;&#23398;&#20064;&#33021;&#21147;&#36827;&#34892;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Pretrained models for zero/few-shot learning excel in language and vision domains but encounter challenges in multivariate time series (TS) due to the diverse nature and scarcity of publicly available pretraining data. Consequently, there has been a recent surge in utilizing pretrained large language models (LLMs) with various adaptations for time series forecasting. These approaches employ cross-domain transfer learning and surprisingly yield impressive results. However, these models are typically very slow and large ($\sim$billion parameters) and do not consider cross-channel correlations. To address this, we present Multi-level Tiny Time Mixers (TTM), a significantly small model based on the lightweight TSMixer architecture. TTM marks the first success in developing tiny general-pretrained models ($\le$1 million parameters), exclusively trained on public TS datasets in a flash of just 4-8 hrs with effective transfer learning capabilities for forecasting. To tackle the complexi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25991;&#26412;&#22270;&#20687;&#32534;&#30721;&#22120;&#30340;&#22238;&#24402;&#65288;TIER&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;AI&#29983;&#25104;&#22270;&#20687;&#30340;&#36136;&#37327;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#30452;&#25509;&#20174;&#22270;&#20687;&#39044;&#27979;&#24471;&#20998;&#19981;&#21516;&#65292;TIER&#36890;&#36807;&#32508;&#21512;&#21033;&#29992;&#25991;&#26412;&#21644;&#22270;&#20687;&#29305;&#24449;&#26469;&#25552;&#39640;AIGCIQA&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.03854</link><description>&lt;p&gt;
&#22522;&#20110;&#25991;&#26412;&#22270;&#20687;&#32534;&#30721;&#22120;&#30340;AIGC&#22270;&#20687;&#36136;&#37327;&#35780;&#20272;&#22238;&#24402;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
TIER: Text-Image Encoder-based Regression for AIGC Image Quality Assessment. (arXiv:2401.03854v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03854
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25991;&#26412;&#22270;&#20687;&#32534;&#30721;&#22120;&#30340;&#22238;&#24402;&#65288;TIER&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;AI&#29983;&#25104;&#22270;&#20687;&#30340;&#36136;&#37327;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#30452;&#25509;&#20174;&#22270;&#20687;&#39044;&#27979;&#24471;&#20998;&#19981;&#21516;&#65292;TIER&#36890;&#36807;&#32508;&#21512;&#21033;&#29992;&#25991;&#26412;&#21644;&#22270;&#20687;&#29305;&#24449;&#26469;&#25552;&#39640;AIGCIQA&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;AI&#29983;&#25104;&#22270;&#20687;&#36136;&#37327;&#35780;&#20272;&#65288;AIGCIQA&#65289;&#20316;&#20026;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#30340;&#19968;&#20010;&#26032;&#35838;&#39064;&#20986;&#29616;&#65292;&#26088;&#22312;&#20174;&#20154;&#31867;&#24863;&#30693;&#35282;&#24230;&#35780;&#20272;AI&#29983;&#25104;&#30340;&#22270;&#20687;&#65288;AIGI&#65289;&#30340;&#36136;&#37327;&#12290;&#19982;&#24120;&#35265;&#30340;&#22270;&#20687;&#36136;&#37327;&#35780;&#20272;&#20219;&#21153;&#19981;&#21516;&#65292;&#36825;&#20123;&#20219;&#21153;&#20013;&#30340;&#22270;&#20687;&#26159;&#36890;&#36807;&#29983;&#25104;&#27169;&#22411;&#20351;&#29992;&#25991;&#26412;&#25552;&#31034;&#29983;&#25104;&#30340;&#65292;&#32780;&#19981;&#26159;&#36890;&#36807;&#22122;&#22768;&#12289;&#27169;&#31946;&#21644;&#21387;&#32553;&#31561;&#26041;&#24335;&#20174;&#21407;&#22987;&#22270;&#20687;&#20013;&#23548;&#20986;&#12290;&#36807;&#21435;&#20960;&#24180;&#37324;&#65292;&#24050;&#32463;&#20570;&#20102;&#22823;&#37327;&#21162;&#21147;&#26469;&#25512;&#36827;AIGCIQA&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;AIGCIQA&#26041;&#27861;&#30452;&#25509;&#20174;&#21333;&#20010;&#29983;&#25104;&#30340;&#22270;&#20687;&#22238;&#24402;&#39044;&#27979;&#24471;&#20998;&#65292;&#24573;&#35270;&#20102;&#36825;&#20123;&#22270;&#20687;&#30340;&#25991;&#26412;&#25552;&#31034;&#20013;&#21253;&#21547;&#30340;&#20449;&#24687;&#12290;&#36825;&#31181;&#30095;&#24573;&#37096;&#20998;&#38480;&#21046;&#20102;&#36825;&#20123;AIGCIQA&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25991;&#26412;&#22270;&#20687;&#32534;&#30721;&#22120;&#30340;&#22238;&#24402;&#65288;TIER&#65289;&#26694;&#26550;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#29983;&#25104;&#30340;&#22270;&#20687;&#21450;&#20854;&#23545;&#24212;&#30340;&#25991;&#26412;&#25552;&#31034;&#20316;&#20026;&#36755;&#20837;&#65292;&#21033;&#29992;&#19968;&#20010;&#25991;&#26412;&#32534;&#30721;&#22120;&#21644;&#19968;&#20010;&#22270;&#20687;&#32534;&#30721;&#22120;&#26469;&#22788;&#29702;&#36825;&#20123;&#36755;&#20837;&#12290;&#36890;&#36807;&#23558;&#25991;&#26412;&#21644;&#22270;&#20687;&#23884;&#20837;&#21040;&#21516;&#19968;&#20010;&#29305;&#24449;&#31354;&#38388;&#20013;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#32508;&#21512;&#21033;&#29992;&#25991;&#26412;&#21644;&#22270;&#20687;&#29305;&#24449;&#30340;AIGCIQA&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, AIGC image quality assessment (AIGCIQA), which aims to assess the quality of AI-generated images (AIGIs) from a human perception perspective, has emerged as a new topic in computer vision. Unlike common image quality assessment tasks where images are derived from original ones distorted by noise, blur, and compression, \textit{etc.}, in AIGCIQA tasks, images are typically generated by generative models using text prompts. Considerable efforts have been made in the past years to advance AIGCIQA. However, most existing AIGCIQA methods regress predicted scores directly from individual generated images, overlooking the information contained in the text prompts of these images. This oversight partially limits the performance of these AIGCIQA methods. To address this issue, we propose a text-image encoder-based regression (TIER) framework. Specifically, we process the generated images and their corresponding text prompts as inputs, utilizing a text encoder and an image encoder to e
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38271;&#26399;&#23433;&#20840;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;LoBiSaRL&#65292;&#35813;&#31639;&#27861;&#38024;&#23545;&#20855;&#26377;&#20108;&#36827;&#21046;&#23433;&#20840;&#21453;&#39304;&#21644;&#26410;&#30693;&#38543;&#26426;&#29366;&#24577;&#36716;&#25442;&#20989;&#25968;&#30340;&#32422;&#26463;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;CMDPs&#65289;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#22870;&#21169;&#30340;&#26041;&#24335;&#20248;&#21270;&#31574;&#30053;&#65292;&#21516;&#26102;&#20197;&#39640;&#27010;&#29575;&#30830;&#20445;&#27599;&#20010;&#22238;&#21512;&#20013;&#20195;&#29702;&#21482;&#25191;&#34892;&#23433;&#20840;&#30340;&#29366;&#24577;-&#21160;&#20316;&#23545;&#12290;</title><link>http://arxiv.org/abs/2401.03786</link><description>&lt;p&gt;
&#38271;&#26399;&#23433;&#20840;&#30340;&#24378;&#21270;&#23398;&#20064;&#19982;&#20108;&#36827;&#21046;&#21453;&#39304;
&lt;/p&gt;
&lt;p&gt;
Long-term Safe Reinforcement Learning with Binary Feedback. (arXiv:2401.03786v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03786
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38271;&#26399;&#23433;&#20840;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;LoBiSaRL&#65292;&#35813;&#31639;&#27861;&#38024;&#23545;&#20855;&#26377;&#20108;&#36827;&#21046;&#23433;&#20840;&#21453;&#39304;&#21644;&#26410;&#30693;&#38543;&#26426;&#29366;&#24577;&#36716;&#25442;&#20989;&#25968;&#30340;&#32422;&#26463;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;CMDPs&#65289;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#22870;&#21169;&#30340;&#26041;&#24335;&#20248;&#21270;&#31574;&#30053;&#65292;&#21516;&#26102;&#20197;&#39640;&#27010;&#29575;&#30830;&#20445;&#27599;&#20010;&#22238;&#21512;&#20013;&#20195;&#29702;&#21482;&#25191;&#34892;&#23433;&#20840;&#30340;&#29366;&#24577;-&#21160;&#20316;&#23545;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23433;&#20840;&#26159;&#23558;&#24378;&#21270;&#23398;&#20064;&#24212;&#29992;&#20110;&#23454;&#38469;&#38382;&#39064;&#30340;&#19981;&#21487;&#25110;&#32570;&#30340;&#35201;&#27714;&#12290;&#23613;&#31649;&#36817;&#24180;&#26469;&#25552;&#20986;&#20102;&#22823;&#37327;&#30340;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#20294;&#22823;&#22810;&#25968;&#29616;&#26377;&#24037;&#20316;&#36890;&#24120;1&#65289;&#20381;&#36182;&#20110;&#25509;&#25910;&#25968;&#20540;&#23433;&#20840;&#21453;&#39304;&#65307;2&#65289;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#26080;&#27861;&#20445;&#35777;&#23433;&#20840;&#24615;&#65307;3&#65289;&#23558;&#38382;&#39064;&#38480;&#21046;&#22312;&#20808;&#39564;&#24050;&#30693;&#30340;&#30830;&#23450;&#24615;&#36716;&#25442;&#21160;&#24577;&#65307;&#20197;&#21450;/&#25110;&#32773;4&#65289;&#20551;&#35774;&#23384;&#22312;&#19968;&#20010;&#24050;&#30693;&#30340;&#23433;&#20840;&#31574;&#30053;&#20197;&#22788;&#29702;&#20219;&#20309;&#29366;&#24577;&#12290;&#38024;&#23545;&#19978;&#36848;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#38271;&#26399;&#20108;&#36827;&#21046;&#21453;&#39304;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#65288;LoBiSaRL&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#38024;&#23545;&#20855;&#26377;&#20108;&#36827;&#21046;&#23433;&#20840;&#21453;&#39304;&#21644;&#26410;&#30693;&#38543;&#26426;&#29366;&#24577;&#36716;&#25442;&#20989;&#25968;&#30340;&#32422;&#26463;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;CMDPs&#65289;&#30340;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#12290;LoBiSaRL&#20248;&#21270;&#19968;&#20010;&#31574;&#30053;&#20197;&#20351;&#22870;&#21169;&#26368;&#22823;&#21270;&#65292;&#21516;&#26102;&#20445;&#35777;&#22312;&#27599;&#20010;&#22238;&#21512;&#20013;&#20195;&#29702;&#20165;&#20197;&#39640;&#27010;&#29575;&#25191;&#34892;&#23433;&#20840;&#30340;&#29366;&#24577;-&#21160;&#20316;&#23545;&#65292;&#20174;&#32780;&#30830;&#20445;&#38271;&#26399;&#23433;&#20840;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;LoBiSaRL&#36890;&#36807;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#65288;GLM&#65289;&#26469;&#24314;&#27169;&#20108;&#36827;&#21046;&#23433;&#20840;&#20989;&#25968;&#65292;&#24182;&#20445;&#35777;&#38271;&#26399;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Safety is an indispensable requirement for applying reinforcement learning (RL) to real problems. Although there has been a surge of safe RL algorithms proposed in recent years, most existing work typically 1) relies on receiving numeric safety feedback; 2) does not guarantee safety during the learning process; 3) limits the problem to a priori known, deterministic transition dynamics; and/or 4) assume the existence of a known safe policy for any states. Addressing the issues mentioned above, we thus propose Long-term Binaryfeedback Safe RL (LoBiSaRL), a safe RL algorithm for constrained Markov decision processes (CMDPs) with binary safety feedback and an unknown, stochastic state transition function. LoBiSaRL optimizes a policy to maximize rewards while guaranteeing a long-term safety that an agent executes only safe state-action pairs throughout each episode with high probability. Specifically, LoBiSaRL models the binary safety function via a generalized linear model (GLM) and conser
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#22238;&#39038;&#20102;&#22312;&#21019;&#20260;&#24615;&#33041;&#25439;&#20260;&#65288;TBI&#65289;&#20013;&#24212;&#29992;&#26426;&#22120;&#23398;&#20064;&#30340;&#26368;&#26032;&#25216;&#26415;&#65292;&#29305;&#21035;&#20851;&#27880;&#36731;&#24230;&#33041;&#25439;&#20260;&#65288;mTBI&#65289;&#12290;&#34429;&#28982;&#24050;&#32463;&#26377;&#24456;&#22810;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#29992;&#20110;&#35786;&#26029;&#65292;&#20294;&#23545;&#39044;&#27979;&#39044;&#21518;&#30340;&#30740;&#31350;&#30456;&#23545;&#36739;&#23569;&#12290;</title><link>http://arxiv.org/abs/2401.03621</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22312;&#21019;&#20260;&#24615;&#33041;&#25439;&#20260;&#20013;&#30340;&#24212;&#29992;&#65306;&#20851;&#27880;&#36731;&#24230;&#33041;&#25439;&#20260;
&lt;/p&gt;
&lt;p&gt;
Machine Learning Applications in Traumatic Brain Injury: A Spotlight on Mild TBI. (arXiv:2401.03621v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03621
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#22238;&#39038;&#20102;&#22312;&#21019;&#20260;&#24615;&#33041;&#25439;&#20260;&#65288;TBI&#65289;&#20013;&#24212;&#29992;&#26426;&#22120;&#23398;&#20064;&#30340;&#26368;&#26032;&#25216;&#26415;&#65292;&#29305;&#21035;&#20851;&#27880;&#36731;&#24230;&#33041;&#25439;&#20260;&#65288;mTBI&#65289;&#12290;&#34429;&#28982;&#24050;&#32463;&#26377;&#24456;&#22810;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#29992;&#20110;&#35786;&#26029;&#65292;&#20294;&#23545;&#39044;&#27979;&#39044;&#21518;&#30340;&#30740;&#31350;&#30456;&#23545;&#36739;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21019;&#20260;&#24615;&#33041;&#25439;&#20260;&#65288;TBI&#65289;&#26159;&#20840;&#29699;&#20844;&#20849;&#21355;&#29983;&#39046;&#22495;&#38754;&#20020;&#30340;&#37325;&#22823;&#25361;&#25112;&#65292;&#23548;&#33268;&#39640;&#21457;&#30149;&#29575;&#21644;&#27515;&#20129;&#29575;&#65292;&#24182;&#32473;&#20840;&#29699;&#21307;&#30103;&#31995;&#32479;&#24102;&#26469;&#24040;&#22823;&#32463;&#27982;&#36127;&#25285;&#12290;TBI&#30340;&#35786;&#26029;&#20381;&#36182;&#20110;&#20020;&#24202;&#20449;&#24687;&#21644;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#12290;&#20026;&#20102;&#24212;&#23545;TBI&#25152;&#24102;&#26469;&#30340;&#22810;&#26041;&#38754;&#25361;&#25112;&#65292;&#20986;&#29616;&#20102;&#21019;&#26032;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#12290;&#23588;&#20854;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#36731;&#24230;&#33041;&#25439;&#20260;&#65288;mTBI&#65289;&#30340;&#26222;&#36941;&#23384;&#22312;&#65292;&#21344;&#22810;&#25968;TBI&#30149;&#20363;&#65292;&#20256;&#32479;&#26041;&#27861;&#24448;&#24448;&#38590;&#20197;&#32988;&#20219;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#37325;&#28857;&#22238;&#39038;&#20102;&#22312;TBI&#20013;&#24212;&#29992;&#20110;&#20020;&#24202;&#20449;&#24687;&#21644;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#30340;&#26368;&#26032;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#25216;&#26415;&#65292;&#29305;&#21035;&#20851;&#27880;mTBI&#12290;&#25105;&#20204;&#26681;&#25454;&#25968;&#25454;&#26469;&#28304;&#23545;ML&#24212;&#29992;&#36827;&#34892;&#20998;&#31867;&#65292;&#24182;&#21487;&#20197;&#30475;&#21040;&#36804;&#20170;&#20026;&#27490;&#20351;&#29992;&#20102;&#21508;&#31181;ML&#25216;&#26415;&#12290;&#36825;&#20123;&#25216;&#26415;&#22823;&#22810;&#20027;&#35201;&#20851;&#27880;&#35786;&#26029;&#65292;&#32780;&#23545;&#39044;&#27979;&#39044;&#21518;&#30340;&#21162;&#21147;&#30456;&#23545;&#36739;&#23569;&#12290;&#36825;&#20010;&#22238;&#39038;&#20250;&#23545;&#24403;&#21069;&#30340;&#30740;&#31350;&#36827;&#34892;&#27010;&#36848;&#65292;&#24182;&#25506;&#32034;&#26410;&#26469;&#21457;&#23637;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traumatic Brain Injury (TBI) poses a significant global public health challenge, contributing to high morbidity and mortality rates and placing a substantial economic burden on healthcare systems worldwide. The diagnosis of TBI relies on clinical information along with Computed Tomography (CT) scans. Addressing the multifaceted challenges posed by TBI has seen the development of innovative, data-driven approaches, for this complex condition. Particularly noteworthy is the prevalence of mild TBI (mTBI), which constitutes the majority of TBI cases where conventional methods often fall short. As such, we review the state-of-the-art Machine Learning (ML) techniques applied to clinical information and CT scans in TBI, with a particular focus on mTBI. We categorize ML applications based on their data sources, and there is a spectrum of ML techniques used to date. Most of these techniques have primarily focused on diagnosis, with relatively few attempts at predicting the prognosis. This revie
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24773;&#20917;&#19979;&#26816;&#27979;&#21644;&#20998;&#31867;&#33041;&#32959;&#30244;&#65292;&#24182;&#35299;&#20915;&#20102;&#22312;&#32597;&#35265;&#24773;&#20917;&#19979;&#30340;&#32959;&#30244;&#26816;&#27979;&#38382;&#39064;&#12290;&#30740;&#31350;&#20351;&#29992;&#20102;&#26469;&#33258;&#22269;&#23478;&#33041;&#26144;&#23556;&#23454;&#39564;&#23460;&#30340;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#20462;&#25913;&#26679;&#26412;&#25968;&#37327;&#21644;&#24739;&#32773;&#20998;&#24067;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#24212;&#23545;&#30495;&#23454;&#19990;&#30028;&#22330;&#26223;&#20013;&#30340;&#24322;&#24120;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2401.03302</link><description>&lt;p&gt;
&#34892;&#21160;&#20013;&#30340;&#29616;&#23454;&#20027;&#20041;&#65306;&#20351;&#29992;YOLOv8&#21644;DeiT&#20174;&#21307;&#23398;&#22270;&#20687;&#20013;&#35786;&#26029;&#33041;&#32959;&#30244;&#30340;&#24322;&#24120;&#24863;&#30693;
&lt;/p&gt;
&lt;p&gt;
Realism in Action: Anomaly-Aware Diagnosis of Brain Tumors from Medical Images Using YOLOv8 and DeiT. (arXiv:2401.03302v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03302
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24773;&#20917;&#19979;&#26816;&#27979;&#21644;&#20998;&#31867;&#33041;&#32959;&#30244;&#65292;&#24182;&#35299;&#20915;&#20102;&#22312;&#32597;&#35265;&#24773;&#20917;&#19979;&#30340;&#32959;&#30244;&#26816;&#27979;&#38382;&#39064;&#12290;&#30740;&#31350;&#20351;&#29992;&#20102;&#26469;&#33258;&#22269;&#23478;&#33041;&#26144;&#23556;&#23454;&#39564;&#23460;&#30340;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#20462;&#25913;&#26679;&#26412;&#25968;&#37327;&#21644;&#24739;&#32773;&#20998;&#24067;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#24212;&#23545;&#30495;&#23454;&#19990;&#30028;&#22330;&#26223;&#20013;&#30340;&#24322;&#24120;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21307;&#23398;&#31185;&#23398;&#39046;&#22495;&#65292;&#30001;&#20110;&#33041;&#32959;&#30244;&#22312;&#24739;&#32773;&#20013;&#30340;&#32597;&#35265;&#31243;&#24230;&#65292;&#21487;&#38752;&#22320;&#26816;&#27979;&#21644;&#20998;&#31867;&#33041;&#32959;&#30244;&#20173;&#28982;&#26159;&#19968;&#20010;&#33392;&#24040;&#30340;&#25361;&#25112;&#12290;&#22240;&#27492;&#65292;&#22312;&#24322;&#24120;&#24773;&#20917;&#19979;&#26816;&#27979;&#32959;&#30244;&#30340;&#33021;&#21147;&#23545;&#20110;&#30830;&#20445;&#21450;&#26102;&#24178;&#39044;&#21644;&#25913;&#21892;&#24739;&#32773;&#32467;&#26524;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#30740;&#31350;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24773;&#20917;&#19979;&#26816;&#27979;&#21644;&#20998;&#31867;&#33041;&#32959;&#30244;&#12290;&#26469;&#33258;&#22269;&#23478;&#33041;&#26144;&#23556;&#23454;&#39564;&#23460;&#65288;NBML&#65289;&#30340;&#31934;&#36873;&#25968;&#25454;&#38598;&#21253;&#25324;81&#21517;&#24739;&#32773;&#65292;&#20854;&#20013;&#21253;&#25324;30&#20363;&#32959;&#30244;&#30149;&#20363;&#21644;51&#20363;&#27491;&#24120;&#30149;&#20363;&#12290;&#26816;&#27979;&#21644;&#20998;&#31867;&#27969;&#31243;&#34987;&#20998;&#20026;&#20004;&#20010;&#36830;&#32493;&#30340;&#20219;&#21153;&#12290;&#26816;&#27979;&#38454;&#27573;&#21253;&#25324;&#20840;&#38754;&#30340;&#25968;&#25454;&#20998;&#26512;&#21644;&#39044;&#22788;&#29702;&#65292;&#20197;&#20462;&#25913;&#22270;&#20687;&#26679;&#26412;&#21644;&#27599;&#20010;&#31867;&#21035;&#30340;&#24739;&#32773;&#25968;&#37327;&#65292;&#20197;&#31526;&#21512;&#30495;&#23454;&#19990;&#30028;&#22330;&#26223;&#20013;&#30340;&#24322;&#24120;&#20998;&#24067;&#65288;9&#20010;&#27491;&#24120;&#26679;&#26412;&#23545;&#24212;1&#20010;&#32959;&#30244;&#26679;&#26412;&#65289;&#12290;&#27492;&#22806;&#65292;&#22312;&#27979;&#35797;&#20013;&#38500;&#20102;&#24120;&#35265;&#30340;&#35780;&#20272;&#25351;&#26631;&#22806;&#65292;&#25105;&#20204;&#36824;&#37319;&#29992;&#20102;... [&#25688;&#35201;&#38271;&#24230;&#24050;&#36798;&#21040;&#19978;&#38480;]
&lt;/p&gt;
&lt;p&gt;
In the field of medical sciences, reliable detection and classification of brain tumors from images remains a formidable challenge due to the rarity of tumors within the population of patients. Therefore, the ability to detect tumors in anomaly scenarios is paramount for ensuring timely interventions and improved patient outcomes. This study addresses the issue by leveraging deep learning (DL) techniques to detect and classify brain tumors in challenging situations. The curated data set from the National Brain Mapping Lab (NBML) comprises 81 patients, including 30 Tumor cases and 51 Normal cases. The detection and classification pipelines are separated into two consecutive tasks. The detection phase involved comprehensive data analysis and pre-processing to modify the number of image samples and the number of patients of each class to anomaly distribution (9 Normal per 1 Tumor) to comply with real world scenarios. Next, in addition to common evaluation metrics for the testing, we emplo
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#24102;&#26377;&#26102;&#38388;&#31383;&#21475;&#30340;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#30340;&#26368;&#20248;&#38142;&#36335;&#26041;&#27861;&#65292;&#32771;&#34385;&#20102;&#35745;&#21010;&#30340;&#26102;&#38388;&#28789;&#27963;&#24615;&#65292;&#24182;&#36890;&#36807;&#23454;&#35777;&#32467;&#26524;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#35299;&#20915;&#38745;&#24577;&#25320;&#25171;&#36710;&#38382;&#39064;&#26102;&#30340;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.02873</link><description>&lt;p&gt;
&#36710;&#36742;&#35745;&#21010;&#19982;&#26102;&#38388;&#31383;&#21475;&#30340;&#26368;&#20248;&#38142;&#36335;
&lt;/p&gt;
&lt;p&gt;
Optimal Chaining of Vehicle Plans with Time Windows. (arXiv:2401.02873v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02873
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#24102;&#26377;&#26102;&#38388;&#31383;&#21475;&#30340;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#30340;&#26368;&#20248;&#38142;&#36335;&#26041;&#27861;&#65292;&#32771;&#34385;&#20102;&#35745;&#21010;&#30340;&#26102;&#38388;&#28789;&#27963;&#24615;&#65292;&#24182;&#36890;&#36807;&#23454;&#35777;&#32467;&#26524;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#35299;&#20915;&#38745;&#24577;&#25320;&#25171;&#36710;&#38382;&#39064;&#26102;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35299;&#20915;&#24102;&#26377;&#26102;&#38388;&#31383;&#21475;&#30340;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#26102;&#65292;&#25105;&#20204;&#32463;&#24120;&#38656;&#35201;&#23558;&#36710;&#36742;&#35745;&#21010;&#36830;&#25509;&#25104;&#36328;&#36234;&#26356;&#38271;&#26102;&#38388;&#21306;&#38388;&#30340;&#24207;&#21015;&#65292;&#25442;&#21477;&#35805;&#35828;&#65292;&#25105;&#20204;&#38656;&#35201;&#25191;&#34892;&#35745;&#21010;&#38142;&#36335;&#12290;&#26368;&#36817;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32593;&#32476;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36710;&#38431;&#35268;&#27169;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#35813;&#26041;&#27861;&#19981;&#32771;&#34385;&#35745;&#21010;&#30340;&#26102;&#38388;&#28789;&#27963;&#24615;&#65292;&#36825;&#26159;&#25152;&#26377;&#24102;&#26377;&#26102;&#38388;&#31383;&#21475;&#30340;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#30340;&#37325;&#35201;&#23646;&#24615;&#12290;&#30456;&#21453;&#65292;&#35745;&#21010;&#20855;&#26377;&#22266;&#23450;&#26102;&#38388;&#65292;&#19981;&#33021;&#24310;&#36831;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38382;&#39064;&#24314;&#27169;&#65292;&#32771;&#34385;&#20102;&#24310;&#36831;&#21644;&#32473;&#23450;&#26102;&#38388;&#31383;&#21475;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#35813;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#26159;&#26368;&#20248;&#30340;&#65292;&#24182;&#23545;&#20854;&#22797;&#26434;&#24615;&#36827;&#34892;&#20102;&#20998;&#26512;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#21015;&#20030;&#20102;&#19968;&#20123;&#23454;&#38469;&#24212;&#29992;&#65292;&#24182;&#23545;&#20854;&#20013;&#19968;&#20010;&#24212;&#29992;&#36827;&#34892;&#20102;&#28436;&#31034;&#65306;&#38745;&#24577;&#25320;&#25171;&#36710;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#27861;&#12290;&#28436;&#31034;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#22823;&#37327;&#23454;&#20363;&#20013;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
For solving problems from the domain of vehicle routing with time windows, we often need to connect vehicle plans into sequences spanning a longer time horizon or, in other words, we need to perform a plan chaining. Recently, a network-based solution has been proposed to solve the fleet-sizing problem. The method, however, does not consider the time flexibility of the plans, an essential property of all vehicle routing problems with time windows. Instead, plans have fixed times and cannot be delayed. This work presents a new problem formulation that considers delays in line with the given time windows and a method that can be used to solve it. Moreover, we prove that the method is optimal, and we analyze its complexity. Finally, we list some practical applications and perform a demonstration for one of them: the method for solving the static Dial-a-ride problem. The demonstration results show that for a significant number of instances, the proposed method provides a better solution tha
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;WaveCoder&#65292;&#19968;&#20010;&#24191;&#27867;&#21644;&#22810;&#21151;&#33021;&#30340;&#25913;&#36827;&#25351;&#20196;&#35843;&#20248;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#25351;&#20196;&#25968;&#25454;&#20998;&#31867;&#24182;&#21033;&#29992;LLM&#26694;&#26550;&#29983;&#25104;&#22810;&#26679;&#30340;&#39640;&#36136;&#37327;&#25351;&#20196;&#25968;&#25454;&#65292;&#20197;&#25552;&#39640;&#35843;&#20248;&#27169;&#22411;&#30340;&#25928;&#26524;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2312.14187</link><description>&lt;p&gt;
WaveCoder: &#24191;&#27867;&#21644;&#22810;&#21151;&#33021;&#30340;&#25913;&#36827;&#25351;&#20196;&#35843;&#20248;&#19982;&#23436;&#21892;&#30340;&#25968;&#25454;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
WaveCoder: Widespread And Versatile Enhanced Instruction Tuning with Refined Data Generation. (arXiv:2312.14187v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.14187
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;WaveCoder&#65292;&#19968;&#20010;&#24191;&#27867;&#21644;&#22810;&#21151;&#33021;&#30340;&#25913;&#36827;&#25351;&#20196;&#35843;&#20248;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#25351;&#20196;&#25968;&#25454;&#20998;&#31867;&#24182;&#21033;&#29992;LLM&#26694;&#26550;&#29983;&#25104;&#22810;&#26679;&#30340;&#39640;&#36136;&#37327;&#25351;&#20196;&#25968;&#25454;&#65292;&#20197;&#25552;&#39640;&#35843;&#20248;&#27169;&#22411;&#30340;&#25928;&#26524;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#23545;&#39640;&#36136;&#37327;&#25351;&#20196;&#25968;&#25454;&#38598;&#36827;&#34892;&#35843;&#20248;&#21518;&#65292;&#29983;&#25104;&#30340;&#27169;&#22411;&#21487;&#20197;&#22312;&#24191;&#27867;&#30340;&#20219;&#21153;&#19978;&#23637;&#29616;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#25351;&#20196;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#32463;&#24120;&#20250;&#20135;&#29983;&#37325;&#22797;&#25968;&#25454;&#65292;&#24182;&#19988;&#23545;&#25968;&#25454;&#36136;&#37327;&#30340;&#25511;&#21046;&#19981;&#22815;&#28789;&#27963;&#12290;&#26412;&#25991;&#36890;&#36807;&#23558;&#25351;&#20196;&#25968;&#25454;&#20998;&#31867;&#20026;4&#20010;&#19982;&#20195;&#30721;&#30456;&#20851;&#30340;&#20219;&#21153;&#65292;&#25193;&#23637;&#20102;&#25351;&#20196;&#35843;&#20248;&#30340;&#26222;&#36866;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;LLM&#30340;&#29983;&#25104;&#22120;-&#21028;&#21035;&#22120;&#25968;&#25454;&#22788;&#29702;&#26694;&#26550;&#65292;&#20174;&#24320;&#28304;&#20195;&#30721;&#20013;&#29983;&#25104;&#22810;&#26679;&#30340;&#12289;&#39640;&#36136;&#37327;&#30340;&#25351;&#20196;&#25968;&#25454;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;CodeOcean&#65292;&#19968;&#20010;&#21253;&#21547;4&#20010;&#36890;&#29992;&#20195;&#30721;&#30456;&#20851;&#20219;&#21153;&#30340;&#12289;&#20849;&#35745;20,000&#20010;&#25351;&#20196;&#23454;&#20363;&#30340;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#22686;&#24378;&#25351;&#20196;&#35843;&#20248;&#30340;&#25928;&#26524;&#65292;&#24182;&#25552;&#39640;&#35843;&#20248;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;WaveCoder&#65292;&#19968;&#20010;&#20855;&#26377;&#24191;&#27867;&#21644;&#22810;&#21151;&#33021;&#30340;&#25913;&#36827;&#25351;&#20196;&#35843;&#20248;&#30340;Code LLM&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent work demonstrates that, after being fine-tuned on a high-quality instruction dataset, the resulting model can obtain impressive capabilities to address a wide range of tasks. However, existing methods for instruction data generation often produce duplicate data and are not controllable enough on data quality. In this paper, we extend the generalization of instruction tuning by classifying the instruction data to 4 code-related tasks and propose a LLM-based Generator-Discriminator data process framework to generate diverse, high-quality instruction data from open source code. Hence, we introduce CodeOcean, a dataset comprising 20,000 instruction instances across 4 universal code-related tasks,which is aimed at augmenting the effectiveness of instruction tuning and improving the generalization ability of fine-tuned model. Subsequently, we present WaveCoder, a fine-tuned Code LLM with Widespread And Versatile Enhanced instruction tuning. This model is specifically designed for enha
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#40065;&#26834;&#38543;&#26426;&#22270;&#29983;&#25104;&#22120;&#65288;RSGG-CE&#65289;&#29992;&#20110;&#21453;&#20107;&#23454;&#35299;&#37322;&#65292;&#35813;&#26041;&#27861;&#22312;&#29983;&#25104;&#31867;&#20284;&#20110;&#21407;&#22987;&#22270;&#24418;&#30340;&#26032;&#22270;&#24418;&#30340;&#21516;&#26102;&#65292;&#22522;&#20110;&#28508;&#22312;&#39044;&#27979;&#27169;&#22411;&#20135;&#29983;&#19981;&#21516;&#30340;&#32467;&#26524;&#12290;&#27492;&#26041;&#27861;&#21033;&#29992;&#29983;&#25104;&#26426;&#21046;&#29983;&#25104;&#21453;&#20107;&#23454;&#23454;&#20363;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#35299;&#37322;&#33021;&#21147;&#65292;&#24182;&#22312;&#20854;&#20182;&#39046;&#22495;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#25104;&#26524;&#12290;</title><link>http://arxiv.org/abs/2312.11747</link><description>&lt;p&gt;
&#24378;&#22823;&#30340;&#38543;&#26426;&#22270;&#29983;&#25104;&#22120;&#29992;&#20110;&#21453;&#20107;&#23454;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Robust Stochastic Graph Generator for Counterfactual Explanations. (arXiv:2312.11747v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.11747
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#40065;&#26834;&#38543;&#26426;&#22270;&#29983;&#25104;&#22120;&#65288;RSGG-CE&#65289;&#29992;&#20110;&#21453;&#20107;&#23454;&#35299;&#37322;&#65292;&#35813;&#26041;&#27861;&#22312;&#29983;&#25104;&#31867;&#20284;&#20110;&#21407;&#22987;&#22270;&#24418;&#30340;&#26032;&#22270;&#24418;&#30340;&#21516;&#26102;&#65292;&#22522;&#20110;&#28508;&#22312;&#39044;&#27979;&#27169;&#22411;&#20135;&#29983;&#19981;&#21516;&#30340;&#32467;&#26524;&#12290;&#27492;&#26041;&#27861;&#21033;&#29992;&#29983;&#25104;&#26426;&#21046;&#29983;&#25104;&#21453;&#20107;&#23454;&#23454;&#20363;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#35299;&#37322;&#33021;&#21147;&#65292;&#24182;&#22312;&#20854;&#20182;&#39046;&#22495;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#20107;&#23454;&#35299;&#37322;&#65288;CE&#65289;&#25216;&#26415;&#20316;&#20026;&#21521;&#19982;AI&#31995;&#32479;&#20114;&#21160;&#30340;&#29992;&#25143;&#25552;&#20379;&#27934;&#23519;&#21147;&#30340;&#25163;&#27573;&#24050;&#24341;&#36215;&#20851;&#27880;&#12290;&#34429;&#28982;&#22312;&#21307;&#23398;&#25104;&#20687;&#21644;&#33258;&#21160;&#39550;&#39542;&#31561;&#39046;&#22495;&#36827;&#34892;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#22270;&#24418;&#21453;&#20107;&#23454;&#35299;&#37322;&#65288;GCE&#65289;&#26041;&#27861;&#30456;&#23545;&#36739;&#23569;&#12290; GCE&#29983;&#25104;&#19968;&#20010;&#31867;&#20284;&#20110;&#21407;&#22987;&#22270;&#24418;&#30340;&#26032;&#22270;&#24418;&#65292;&#20854;&#32467;&#26524;&#22522;&#20110;&#28508;&#22312;&#30340;&#39044;&#27979;&#27169;&#22411;&#12290;&#22312;&#36825;&#20123;GCE&#25216;&#26415;&#20013;&#65292;&#23613;&#31649;&#22312;&#20854;&#20182;&#39046;&#22495;&#22914;&#33402;&#26415;&#39118;&#26684;&#21644;&#33258;&#28982;&#35821;&#35328;&#24314;&#27169;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#25104;&#23601;&#65292;&#20294;&#22522;&#20110;&#29983;&#25104;&#26426;&#21046;&#30340;&#35299;&#37322;&#22120;&#20173;&#28982;&#21463;&#21040;&#20102;&#30456;&#23545;&#26377;&#38480;&#30340;&#30740;&#31350;&#12290;&#23545;&#29983;&#25104;&#35299;&#37322;&#22120;&#30340;&#20559;&#22909;&#28304;&#20110;&#23427;&#20204;&#22312;&#25512;&#26029;&#26399;&#38388;&#29983;&#25104;&#21453;&#20107;&#23454;&#23454;&#20363;&#30340;&#33021;&#21147;&#65292;&#21033;&#29992;&#33258;&#21160;&#33719;&#21462;&#30340;&#36755;&#20837;&#22270;&#24418;&#30340;&#25200;&#21160;&#12290;&#22312;&#19978;&#36848;&#29702;&#30001;&#30340;&#25512;&#21160;&#19979;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#20171;&#32461;&#20102;RSGG-CE&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#40065;&#26834;&#38543;&#26426;&#22270;&#29983;&#25104;&#22120;&#65292;&#29992;&#20110;&#21453;&#20107;&#23454;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Counterfactual Explanation (CE) techniques have garnered attention as a means to provide insights to the users engaging with AI systems. While extensively researched in domains such as medical imaging and autonomous vehicles, Graph Counterfactual Explanation (GCE) methods have been comparatively under-explored. GCEs generate a new graph similar to the original one, with a different outcome grounded on the underlying predictive model. Among these GCE techniques, those rooted in generative mechanisms have received relatively limited investigation despite demonstrating impressive accomplishments in other domains, such as artistic styles and natural language modelling. The preference for generative explainers stems from their capacity to generate counterfactual instances during inference, leveraging autonomously acquired perturbations of the input graph. Motivated by the rationales above, our study introduces RSGG-CE, a novel Robust Stochastic Graph Generator for Counterfactual Explanation
&lt;/p&gt;</description></item><item><title>WAVER&#26159;&#19968;&#31181;&#36890;&#36807;&#24320;&#25918;&#35789;&#27719;&#30693;&#35782;&#36827;&#34892;&#36328;&#22495;&#30693;&#35782;&#33976;&#39311;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#35270;&#39057;&#25551;&#36848;&#20013;&#19981;&#21516;&#20889;&#20316;&#39118;&#26684;&#30340;&#38382;&#39064;&#12290;&#23427;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#37319;&#29992;&#38544;&#24335;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#23558;&#25991;&#26412;&#30693;&#35782;&#20174;&#25945;&#24072;&#27169;&#22411;&#20256;&#36882;&#32473;&#23398;&#29983;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2312.09507</link><description>&lt;p&gt;
WAVER:&#36890;&#36807;&#24320;&#25918;&#35789;&#27719;&#30693;&#35782;&#36890;&#36807;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#23545;&#20889;&#20316;&#39118;&#26684;&#19981;&#21463;&#26463;&#32538;&#30340;&#25991;&#26412;-&#35270;&#39057;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
WAVER: Writing-style Agnostic Text-Video Retrieval via Distilling Vision-Language Models Through Open-Vocabulary Knowledge. (arXiv:2312.09507v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.09507
&lt;/p&gt;
&lt;p&gt;
WAVER&#26159;&#19968;&#31181;&#36890;&#36807;&#24320;&#25918;&#35789;&#27719;&#30693;&#35782;&#36827;&#34892;&#36328;&#22495;&#30693;&#35782;&#33976;&#39311;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#35270;&#39057;&#25551;&#36848;&#20013;&#19981;&#21516;&#20889;&#20316;&#39118;&#26684;&#30340;&#38382;&#39064;&#12290;&#23427;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#37319;&#29992;&#38544;&#24335;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#23558;&#25991;&#26412;&#30693;&#35782;&#20174;&#25945;&#24072;&#27169;&#22411;&#20256;&#36882;&#32473;&#23398;&#29983;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;-&#35270;&#39057;&#26816;&#32034;&#26159;&#22810;&#27169;&#24577;&#20449;&#24687;&#26816;&#32034;&#39046;&#22495;&#20013;&#19968;&#20010;&#37325;&#35201;&#30340;&#23376;&#39046;&#22495;&#65292;&#22312;&#36817;&#24180;&#26469;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#22686;&#38271;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#20551;&#35774;&#35270;&#39057;&#22330;&#26223;&#19982;&#26080;&#20559;&#30340;&#25551;&#36848;&#19968;&#33268;&#12290;&#36825;&#20123;&#38480;&#21046;&#19982;&#30495;&#23454;&#19990;&#30028;&#30340;&#24773;&#20917;&#19981;&#31526;&#65292;&#22240;&#20026;&#25551;&#36848;&#21487;&#33021;&#21463;&#21040;&#27880;&#37322;&#32773;&#30340;&#20559;&#35265;&#12289;&#19981;&#21516;&#30340;&#20889;&#20316;&#39118;&#26684;&#21644;&#19981;&#21516;&#30340;&#25991;&#26412;&#35270;&#35282;&#30340;&#24433;&#21709;&#12290;&#20026;&#20102;&#20811;&#26381;&#19978;&#36848;&#38382;&#39064;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;WAVER&#65292;&#19968;&#31181;&#36890;&#36807;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#24320;&#25918;&#35789;&#27719;&#30693;&#35782;&#36827;&#34892;&#36328;&#22495;&#30693;&#35782;&#33976;&#39311;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#22788;&#29702;&#35270;&#39057;&#25551;&#36848;&#20013;&#19981;&#21516;&#20889;&#20316;&#39118;&#26684;&#30340;&#25361;&#25112;&#12290;WAVER&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24320;&#25918;&#35789;&#27719;&#23646;&#24615;&#65292;&#24182;&#37319;&#29992;&#38544;&#24335;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#23558;&#22522;&#20110;&#25991;&#26412;&#30340;&#30693;&#35782;&#20174;&#25945;&#24072;&#27169;&#22411;&#20256;&#36882;&#32473;&#22522;&#20110;&#35270;&#35273;&#30340;&#23398;&#29983;&#27169;&#22411;&#12290;&#22312;&#22235;&#20010;&#26631;&#20934;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-video retrieval, a prominent sub-field within the domain of multimodal information retrieval, has witnessed remarkable growth in recent years. However, existing methods assume video scenes are consistent with unbiased descriptions. These limitations fail to align with real-world scenarios since descriptions can be influenced by annotator biases, diverse writing styles, and varying textual perspectives. To overcome the aforementioned problems, we introduce $\texttt{WAVER}$, a cross-domain knowledge distillation framework via vision-language models through open-vocabulary knowledge designed to tackle the challenge of handling different writing styles in video descriptions. $\texttt{WAVER}$ capitalizes on the open-vocabulary properties that lie in pre-trained vision-language models and employs an implicit knowledge distillation approach to transfer text-based knowledge from a teacher model to a vision-based student. Empirical studies conducted across four standard benchmark datasets,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#30340;&#27744;&#21270;&#23618;&#36755;&#20837;&#26469;&#23454;&#29616;&#23545;&#38544;&#31169;&#25915;&#20987;&#30340;&#25913;&#36827;&#12290;&#36890;&#36807;&#24674;&#22797;&#27744;&#21270;&#23618;&#36755;&#20837;&#65292;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#22312;&#19981;&#21516;&#30340;&#25209;&#22788;&#29702;&#22823;&#23567;&#19979;&#25552;&#20379;&#26356;&#39640;&#30340;&#25991;&#26412;&#24674;&#22797;&#29575;&#65292;&#20174;&#32780;&#25552;&#20379;&#26356;&#32454;&#33268;&#21644;&#26377;&#25928;&#30340;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2312.05720</link><description>&lt;p&gt;
&#36229;&#36234;&#26799;&#24230;&#21644;&#20808;&#39564;&#30693;&#35782;&#22312;&#38544;&#31169;&#25915;&#20987;&#20013;&#65306;&#21033;&#29992;&#32852;&#37030;&#23398;&#20064;&#20013;&#35821;&#35328;&#27169;&#22411;&#30340;&#27744;&#21270;&#23618;&#36755;&#20837;
&lt;/p&gt;
&lt;p&gt;
Beyond Gradient and Priors in Privacy Attacks: Leveraging Pooler Layer Inputs of Language Models in Federated Learning. (arXiv:2312.05720v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.05720
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#30340;&#27744;&#21270;&#23618;&#36755;&#20837;&#26469;&#23454;&#29616;&#23545;&#38544;&#31169;&#25915;&#20987;&#30340;&#25913;&#36827;&#12290;&#36890;&#36807;&#24674;&#22797;&#27744;&#21270;&#23618;&#36755;&#20837;&#65292;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#22312;&#19981;&#21516;&#30340;&#25209;&#22788;&#29702;&#22823;&#23567;&#19979;&#25552;&#20379;&#26356;&#39640;&#30340;&#25991;&#26412;&#24674;&#22797;&#29575;&#65292;&#20174;&#32780;&#25552;&#20379;&#26356;&#32454;&#33268;&#21644;&#26377;&#25928;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#24378;&#35843;&#20998;&#25955;&#24335;&#35757;&#32451;&#65292;&#36890;&#36807;&#26412;&#22320;&#23384;&#20648;&#25968;&#25454;&#24182;&#20165;&#21457;&#36865;&#27169;&#22411;&#26356;&#26032;&#65292;&#24378;&#35843;&#29992;&#25143;&#38544;&#31169;&#12290;&#26368;&#36817;&#65292;&#19968;&#31995;&#21015;&#26377;&#20851;&#38544;&#31169;&#25915;&#20987;&#30340;&#24037;&#20316;&#36890;&#36807;&#20174;&#32852;&#37030;&#23398;&#20064;&#19978;&#19979;&#25991;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#21462;&#25935;&#24863;&#30340;&#35757;&#32451;&#25991;&#26412;&#26469;&#25439;&#23475;&#29992;&#25143;&#38544;&#31169;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25915;&#20987;&#25216;&#26415;&#38754;&#20020;&#30528;&#19981;&#21516;&#30340;&#38556;&#30861;&#65306;&#19968;&#20123;&#24037;&#20316;&#20027;&#35201;&#20351;&#29992;&#26377;&#38480;&#30340;&#25209;&#22788;&#29702;&#22823;&#23567;&#65288;&#20363;&#22914;&#65292;&#25209;&#22788;&#29702;&#22823;&#23567;&#20026;1&#65289;&#65292;&#32780;&#20854;&#20182;&#25216;&#26415;&#21017;&#23481;&#26131;&#34987;&#26816;&#27979;&#20986;&#26469;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#20855;&#26377;&#38590;&#20197;&#26816;&#27979;&#30340;&#29305;&#28857;&#65292;&#22312;&#19981;&#21516;&#30340;&#25209;&#22788;&#29702;&#22823;&#23567;&#35774;&#32622;&#19979;&#26174;&#33879;&#25552;&#39640;&#20102;&#25991;&#26412;&#24674;&#22797;&#29575;&#12290;&#22522;&#20110;&#22522;&#26412;&#30340;&#26799;&#24230;&#21305;&#37197;&#21644;&#39046;&#22495;&#20808;&#39564;&#30693;&#35782;&#65292;&#25105;&#20204;&#36890;&#36807;&#24674;&#22797;&#35821;&#35328;&#27169;&#22411;&#30340;&#27744;&#21270;&#23618;&#36755;&#20837;&#26469;&#22686;&#24378;&#25915;&#20987;&#33021;&#21147;&#65292;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#22312;&#29305;&#24449;&#32423;&#21035;&#25552;&#20379;&#39069;&#22806;&#30340;&#30417;&#30563;&#20449;&#21495;&#12290;&#19982;&#26799;&#24230;&#25968;&#25454;&#19981;&#21516;&#65292;&#36825;&#20123;&#20449;&#21495;&#19981;&#20250;&#22312;&#21477;&#23376;&#21644;&#26631;&#35760;&#20043;&#38388;&#36827;&#34892;&#24179;&#22343;&#65292;&#20174;&#32780;&#25552;&#20379;&#26356;&#32454;&#33268;&#21644;&#26377;&#25928;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) emphasizes decentralized training by storing data locally and sending only model updates, underlining user privacy. Recently, a line of works on privacy attacks impairs user privacy by extracting sensitive training text from language models in the context of FL. Yet, these attack techniques face distinct hurdles: some work chiefly with limited batch sizes (e.g., batch size of 1), and others are easily detectable. This paper introduces an innovative approach that is challenging to detect, significantly enhancing the recovery rate of text in various batch-size settings. Building on fundamental gradient matching and domain prior knowledge, we enhance the attack by recovering the input of the Pooler layer of language models, which enables us to provide additional supervised signals at the feature level. Unlike gradient data, these signals do not average across sentences and tokens, thereby offering more nuanced and effective insights. We benchmark our method using t
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#29983;&#25104;&#24615;&#20154;&#24037;&#26234;&#33021;&#20013;&#25968;&#25454;&#38544;&#31169;&#21644;&#29256;&#26435;&#20445;&#25252;&#30340;&#22810;&#26041;&#38754;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#23558;&#25216;&#26415;&#21019;&#26032;&#19982;&#20262;&#29702;&#21069;&#30651;&#30456;&#32467;&#21512;&#30340;&#32508;&#21512;&#26041;&#27861;&#65292;&#26088;&#22312;&#20840;&#38754;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2311.18252</link><description>&lt;p&gt;
&#36328;&#36234;&#29983;&#25104;&#24615;&#20154;&#24037;&#26234;&#33021;&#25968;&#25454;&#29983;&#21629;&#21608;&#26399;&#30340;&#38544;&#31169;&#21644;&#29256;&#26435;&#25361;&#25112;&#23548;&#33322;
&lt;/p&gt;
&lt;p&gt;
Navigating Privacy and Copyright Challenges Across the Data Lifecycle of Generative AI. (arXiv:2311.18252v2 [cs.SE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.18252
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#29983;&#25104;&#24615;&#20154;&#24037;&#26234;&#33021;&#20013;&#25968;&#25454;&#38544;&#31169;&#21644;&#29256;&#26435;&#20445;&#25252;&#30340;&#22810;&#26041;&#38754;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#23558;&#25216;&#26415;&#21019;&#26032;&#19982;&#20262;&#29702;&#21069;&#30651;&#30456;&#32467;&#21512;&#30340;&#32508;&#21512;&#26041;&#27861;&#65292;&#26088;&#22312;&#20840;&#38754;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24615;&#20154;&#24037;&#26234;&#33021;&#30340;&#20986;&#29616;&#26631;&#24535;&#30528;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#37325;&#35201;&#37324;&#31243;&#30865;&#65292;&#23637;&#31034;&#20986;&#22312;&#29983;&#25104;&#30495;&#23454;&#22270;&#20687;&#12289;&#25991;&#26412;&#21644;&#25968;&#25454;&#27169;&#24335;&#26041;&#38754;&#30340;&#21331;&#36234;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#36827;&#23637;&#20063;&#24102;&#26469;&#20102;&#23545;&#25968;&#25454;&#38544;&#31169;&#21644;&#29256;&#26435;&#20405;&#29359;&#30340;&#26356;&#39640;&#20851;&#27880;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#27169;&#22411;&#35757;&#32451;&#23545;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#30340;&#20381;&#36182;&#12290;&#20256;&#32479;&#26041;&#27861;&#22914;&#24046;&#20998;&#38544;&#31169;&#12289;&#26426;&#22120;&#36951;&#24536;&#21644;&#25968;&#25454;&#20013;&#27602;&#21482;&#25552;&#20379;&#20102;&#23545;&#36825;&#20123;&#22797;&#26434;&#38382;&#39064;&#30340;&#29255;&#38754;&#35299;&#20915;&#26041;&#26696;&#12290;&#26412;&#25991;&#28145;&#20837;&#25506;&#35752;&#20102;&#25968;&#25454;&#29983;&#21629;&#21608;&#26399;&#20869;&#38544;&#31169;&#21644;&#29256;&#26435;&#20445;&#25252;&#30340;&#22810;&#26041;&#38754;&#25361;&#25112;&#12290;&#25105;&#20204;&#20027;&#24352;&#37319;&#29992;&#23558;&#25216;&#26415;&#21019;&#26032;&#19982;&#20262;&#29702;&#21069;&#30651;&#30456;&#32467;&#21512;&#30340;&#32508;&#21512;&#26041;&#27861;&#65292;&#36890;&#36807;&#30740;&#31350;&#21644;&#21046;&#23450;&#22312;&#29983;&#21629;&#21608;&#26399;&#35270;&#35282;&#19979;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20840;&#38754;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#25512;&#21160;&#26356;&#24191;&#27867;&#30340;&#35752;&#35770;&#65292;&#24182;&#28608;&#21169;&#23545;&#29983;&#25104;&#24615;&#20154;&#24037;&#26234;&#33021;&#20013;&#25968;&#25454;&#38544;&#31169;&#21644;&#29256;&#26435;&#23436;&#25972;&#24615;&#30340;&#21327;&#21516;&#21162;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The advent of Generative AI has marked a significant milestone in artificial intelligence, demonstrating remarkable capabilities in generating realistic images, texts, and data patterns. However, these advancements come with heightened concerns over data privacy and copyright infringement, primarily due to the reliance on vast datasets for model training. Traditional approaches like differential privacy, machine unlearning, and data poisoning only offer fragmented solutions to these complex issues. Our paper delves into the multifaceted challenges of privacy and copyright protection within the data lifecycle. We advocate for integrated approaches that combines technical innovation with ethical foresight, holistically addressing these concerns by investigating and devising solutions that are informed by the lifecycle perspective. This work aims to catalyze a broader discussion and inspire concerted efforts towards data privacy and copyright integrity in Generative AI.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20108;&#20540;&#31070;&#32463;&#32593;&#32476;&#65288;BNNs&#65289;&#30340;&#35268;&#27169;&#20002;&#24323;&#30340;&#26032;&#22411;&#27491;&#21017;&#21270;&#25216;&#26415;&#65292;&#24182;&#22522;&#20110;&#33945;&#29305;&#21345;&#27931;&#24314;&#31435;BayNN&#27169;&#22411;&#20197;&#39640;&#25928;&#22320;&#20272;&#35745;&#19981;&#30830;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.15816</link><description>&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#38543;&#26426;&#23610;&#24230;&#26469;&#20272;&#35745;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#19981;&#30830;&#23450;&#24615;&#30340;&#35268;&#27169;&#20002;&#24323;
&lt;/p&gt;
&lt;p&gt;
Scale-Dropout: Estimating Uncertainty in Deep Neural Networks Using Stochastic Scale. (arXiv:2311.15816v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.15816
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20108;&#20540;&#31070;&#32463;&#32593;&#32476;&#65288;BNNs&#65289;&#30340;&#35268;&#27169;&#20002;&#24323;&#30340;&#26032;&#22411;&#27491;&#21017;&#21270;&#25216;&#26415;&#65292;&#24182;&#22522;&#20110;&#33945;&#29305;&#21345;&#27931;&#24314;&#31435;BayNN&#27169;&#22411;&#20197;&#39640;&#25928;&#22320;&#20272;&#35745;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#23545;&#20110;&#25913;&#21892;&#22312;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#20013;&#30340;&#21487;&#38752;&#24615;&#21644;&#39044;&#27979;&#30340;&#20449;&#24515;&#33267;&#20851;&#37325;&#35201;&#12290;&#20855;&#26377;Dropout&#20316;&#20026;&#36817;&#20284;&#30340;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#65288;BayNNs&#65289;&#20026;&#37327;&#21270;&#19981;&#30830;&#23450;&#24615;&#25552;&#20379;&#20102;&#19968;&#31181;&#31995;&#32479;&#26041;&#27861;&#65292;&#20294;&#23427;&#20204;&#22312;&#21151;&#32791;&#12289;&#20869;&#23384;&#21644;&#35745;&#31639;&#26041;&#38754;&#20855;&#26377;&#39640;&#30828;&#20214;&#24320;&#38144;&#12290;&#22240;&#27492;&#65292;&#23558;BayNNs&#24212;&#29992;&#20110;&#36164;&#28304;&#26377;&#38480;&#30340;&#36793;&#32536;&#35774;&#22791;&#25110;&#39640;&#24615;&#33021;&#24212;&#29992;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#36890;&#36807;&#22312;&#20855;&#26377;&#33258;&#26059;&#30005;&#23376;&#23384;&#20648;&#22120;&#21644;&#21442;&#25968;&#20108;&#20540;&#21270;&#30340;&#35745;&#31639;&#20869;&#23384;&#65288;CIM&#65289;&#26550;&#26500;&#19978;&#21152;&#36895;&#23427;&#20204;&#21487;&#20197;&#20943;&#23569;BayNNs&#30340;&#19968;&#20123;&#22266;&#26377;&#25104;&#26412;&#12290;&#28982;&#32780;&#65292;&#23454;&#26045;&#24120;&#35268;&#22522;&#20110;dropout&#30340;BayNN&#38656;&#35201;&#22823;&#37327;&#30340;&#38543;&#26426;&#21333;&#20803;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20108;&#20540;&#31070;&#32463;&#32593;&#32476;&#65288;BNNs&#65289;&#30340;&#35268;&#27169;&#20002;&#24323;&#27491;&#21017;&#21270;&#25216;&#26415;&#65292;&#24182;&#22522;&#20110;&#33945;&#29305;&#21345;&#27931;-&#35268;&#27169;&#20002;&#24323;&#65288;MC-Scale Dropout&#65289;&#30340;BayNNs&#36827;&#34892;&#39640;&#25928;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
Uncertainty estimation in Neural Networks (NNs) is vital in improving reliability and confidence in predictions, particularly in safety-critical applications. Bayesian Neural Networks (BayNNs) with Dropout as an approximation offer a systematic approach to quantifying uncertainty, but they inherently suffer from high hardware overhead in terms of power, memory, and computation. Thus, the applicability of BayNNs to edge devices with limited resources or to high-performance applications is challenging. Some of the inherent costs of BayNNs can be reduced by accelerating them in hardware on a Computation-In-Memory (CIM) architecture with spintronic memories and binarizing their parameters. However, numerous stochastic units are required to implement conventional dropout-based BayNN. In this paper, we propose the Scale Dropout, a novel regularization technique for Binary Neural Networks (BNNs), and Monte Carlo-Scale Dropout (MC-Scale Dropout)-based BayNNs for efficient uncertainty estimatio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#22312;&#22797;&#26434;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#19978;&#25506;&#32034;&#35838;&#31243;&#23398;&#20064;&#21644;&#27169;&#20223;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#21457;&#29616;&#35838;&#31243;&#23398;&#20064;&#26159;&#25913;&#21892;&#22797;&#26434;&#26102;&#38388;&#24207;&#21015;&#25511;&#21046;&#20219;&#21153;&#24615;&#33021;&#30340;&#26032;&#36884;&#24452;&#65292;&#32780;&#27169;&#20223;&#23398;&#20064;&#20063;&#24212;&#35813;&#34987;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2311.13326</link><description>&lt;p&gt;
&#12298;&#22522;&#20110;&#35838;&#31243;&#23398;&#20064;&#21644;&#27169;&#20223;&#23398;&#20064;&#30340;&#37329;&#34701;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#26080;&#20851;&#25511;&#21046;&#12299;
&lt;/p&gt;
&lt;p&gt;
Curriculum Learning and Imitation Learning for Model-free Control on Financial Time-series. (arXiv:2311.13326v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.13326
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#22312;&#22797;&#26434;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#19978;&#25506;&#32034;&#35838;&#31243;&#23398;&#20064;&#21644;&#27169;&#20223;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#21457;&#29616;&#35838;&#31243;&#23398;&#20064;&#26159;&#25913;&#21892;&#22797;&#26434;&#26102;&#38388;&#24207;&#21015;&#25511;&#21046;&#20219;&#21153;&#24615;&#33021;&#30340;&#26032;&#36884;&#24452;&#65292;&#32780;&#27169;&#20223;&#23398;&#20064;&#20063;&#24212;&#35813;&#34987;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35838;&#31243;&#23398;&#20064;&#21644;&#27169;&#20223;&#23398;&#20064;&#22312;&#26426;&#22120;&#20154;&#39046;&#22495;&#24050;&#34987;&#24191;&#27867;&#36816;&#29992;&#12290;&#28982;&#32780;&#65292;&#22312;&#39640;&#24230;&#38543;&#26426;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#19978;&#21033;&#29992;&#36825;&#20123;&#24819;&#27861;&#36827;&#34892;&#25511;&#21046;&#20219;&#21153;&#30340;&#30740;&#31350;&#38750;&#24120;&#26377;&#38480;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20174;&#29702;&#35770;&#21644;&#23454;&#35777;&#20004;&#20010;&#26041;&#38754;&#25506;&#35752;&#20102;&#36825;&#20123;&#26041;&#27861;&#22312;&#22797;&#26434;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#19978;&#30340;&#20195;&#34920;&#24615;&#25511;&#21046;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#23454;&#29616;&#20102;&#35838;&#31243;&#23398;&#20064;&#30340;&#22522;&#26412;&#24605;&#24819;&#65292;&#32780;&#36890;&#36807;&#27169;&#20223;&#23398;&#20064;&#20174;&#19987;&#23478;&#20013;&#33976;&#39311;&#20986;&#31574;&#30053;&#26469;&#23454;&#29616;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#35838;&#31243;&#23398;&#20064;&#22312;&#25913;&#36827;&#22797;&#26434;&#26102;&#38388;&#24207;&#21015;&#25511;&#21046;&#30340;&#20219;&#21153;&#24615;&#33021;&#26041;&#38754;&#24212;&#34987;&#35270;&#20026;&#19968;&#31181;&#26032;&#30340;&#26041;&#21521;&#12290;&#25105;&#20204;&#30340;&#22823;&#37327;&#38543;&#26426;&#31181;&#23376;&#22806;&#26679;&#26412;&#23454;&#35777;&#21644;&#28040;&#34701;&#30740;&#31350;&#23545;&#20110;&#26102;&#38388;&#24207;&#21015;&#25511;&#21046;&#30340;&#35838;&#31243;&#23398;&#20064;&#38750;&#24120;&#40723;&#33310;&#20154;&#24515;&#12290;&#36825;&#20123;&#21457;&#29616;&#23588;&#20854;&#40723;&#33310;&#20154;&#24515;&#65292;&#22240;&#20026;&#25105;&#20204;&#22312;&#22522;&#32447;&#19978;&#35843;&#25972;&#20102;&#25152;&#26377;&#37325;&#21472;&#30340;&#36229;&#21442;&#25968;&#65292;&#32473;&#20986;&#20102;&#22522;&#32447;&#30340;&#20248;&#21183;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#25105;&#20204;&#21457;&#29616;&#27169;&#20223;&#23398;&#20064;&#24212;&#35813;&#34987;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Curriculum learning and imitation learning have been leveraged extensively in the robotics domain. However, minimal research has been done on leveraging these ideas on control tasks over highly stochastic time-series data. Here, we theoretically and empirically explore these approaches in a representative control task over complex time-series data. We implement the fundamental ideas of curriculum learning via data augmentation, while imitation learning is implemented via policy distillation from an oracle. Our findings reveal that curriculum learning should be considered a novel direction in improving control-task performance over complex time-series. Our ample random-seed out-sample empirics and ablation studies are highly encouraging for curriculum learning for time-series control. These findings are especially encouraging as we tune all overlapping hyperparameters on the baseline -- giving an advantage to the baseline. On the other hand, we find that imitation learning should be use
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;Alympics&#65292;&#19968;&#20010;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#20154;&#36827;&#34892;&#21338;&#24328;&#35770;&#30740;&#31350;&#30340;&#31995;&#32479;&#24615;&#27169;&#25311;&#26694;&#26550;&#12290;&#36890;&#36807;&#27169;&#25311;&#20154;&#31867;&#25112;&#30053;&#20114;&#21160;&#65292;&#26694;&#26550;&#33021;&#22815;&#23450;&#24615;&#21644;&#23450;&#37327;&#22320;&#20998;&#26512;&#28216;&#25103;&#20915;&#23450;&#22240;&#32032;&#12289;&#31574;&#30053;&#21644;&#32467;&#26524;&#65292;&#24182;&#23545;&#20195;&#29702;&#20154;&#22312;&#25112;&#30053;&#20915;&#31574;&#22330;&#26223;&#20013;&#30340;&#34920;&#29616;&#36827;&#34892;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2311.03220</link><description>&lt;p&gt;
ALYMPICS&#65306;&#35821;&#35328;&#20195;&#29702;&#20154;&#19982;&#21338;&#24328;&#35770;&#30456;&#36935;&#8212;&#8212;&#29992;AI&#20195;&#29702;&#20154;&#25506;&#32034;&#25112;&#30053;&#20915;&#31574;
&lt;/p&gt;
&lt;p&gt;
ALYMPICS: Language Agents Meet Game Theory -- Exploring Strategic Decision-Making with AI Agents. (arXiv:2311.03220v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.03220
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;Alympics&#65292;&#19968;&#20010;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#20154;&#36827;&#34892;&#21338;&#24328;&#35770;&#30740;&#31350;&#30340;&#31995;&#32479;&#24615;&#27169;&#25311;&#26694;&#26550;&#12290;&#36890;&#36807;&#27169;&#25311;&#20154;&#31867;&#25112;&#30053;&#20114;&#21160;&#65292;&#26694;&#26550;&#33021;&#22815;&#23450;&#24615;&#21644;&#23450;&#37327;&#22320;&#20998;&#26512;&#28216;&#25103;&#20915;&#23450;&#22240;&#32032;&#12289;&#31574;&#30053;&#21644;&#32467;&#26524;&#65292;&#24182;&#23545;&#20195;&#29702;&#20154;&#22312;&#25112;&#30053;&#20915;&#31574;&#22330;&#26223;&#20013;&#30340;&#34920;&#29616;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;Alympics&#65288;&#20195;&#29702;&#20154;&#30340;&#22885;&#36816;&#20250;&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20195;&#29702;&#20154;&#36827;&#34892;&#21338;&#24328;&#35770;&#30740;&#31350;&#30340;&#31995;&#32479;&#24615;&#27169;&#25311;&#26694;&#26550;&#12290;Alympics&#21019;&#24314;&#20102;&#19968;&#20010;&#22810;&#21151;&#33021;&#24179;&#21488;&#65292;&#29992;&#20110;&#30740;&#31350;&#22797;&#26434;&#30340;&#21338;&#24328;&#35770;&#38382;&#39064;&#65292;&#36890;&#36807;&#25552;&#20379;&#19968;&#20010;&#25511;&#21046;&#29615;&#22659;&#26469;&#27169;&#25311;&#19982;LLM&#20195;&#29702;&#20154;&#36827;&#34892;&#31867;&#20284;&#20154;&#31867;&#30340;&#25112;&#30053;&#20114;&#21160;&#65292;&#24357;&#21512;&#20102;&#29702;&#35770;&#21338;&#24328;&#35770;&#21644;&#23454;&#35777;&#30740;&#31350;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#22312;&#25105;&#20204;&#30340;&#35797;&#28857;&#26696;&#20363;&#30740;&#31350;&#20013;&#65292;&#8220;&#27700;&#36164;&#28304;&#20998;&#37197;&#25361;&#25112;&#8221;&#65292;&#25105;&#20204;&#36890;&#36807;&#19968;&#20010;&#20851;&#27880;&#31232;&#32570;&#29983;&#23384;&#36164;&#28304;&#22810;&#36718;&#25293;&#21334;&#30340;&#25361;&#25112;&#24615;&#25112;&#30053;&#28216;&#25103;&#26469;&#25506;&#32034;Alympics&#12290;&#36825;&#39033;&#30740;&#31350;&#23637;&#31034;&#20102;&#35813;&#26694;&#26550;&#22312;&#23450;&#24615;&#21644;&#23450;&#37327;&#20998;&#26512;&#28216;&#25103;&#20915;&#23450;&#22240;&#32032;&#12289;&#31574;&#30053;&#21644;&#32467;&#26524;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#20154;&#31867;&#35780;&#20272;&#21644;&#23545;LLM&#20195;&#29702;&#20154;&#22312;&#25112;&#30053;&#20915;&#31574;&#22330;&#26223;&#20013;&#30340;&#28145;&#20837;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#19981;&#20165;&#25193;&#23637;&#20102;&#23545;LLM&#20195;&#29702;&#20154;&#27169;&#25311;&#20154;&#31867;&#25112;&#30053;&#34892;&#20026;&#33021;&#21147;&#30340;&#29702;&#35299;&#65292;&#36824;
&lt;/p&gt;
&lt;p&gt;
This paper introduces Alympics (Olympics for Agents), a systematic simulation framework utilizing Large Language Model (LLM) agents for game theory research. Alympics creates a versatile platform for studying complex game theory problems, bridging the gap between theoretical game theory and empirical investigations by providing a controlled environment for simulating human-like strategic interactions with LLM agents. In our pilot case study, the "Water Allocation Challenge," we explore Alympics through a challenging strategic game focused on the multi-round auction on scarce survival resources. This study demonstrates the framework's ability to qualitatively and quantitatively analyze game determinants, strategies, and outcomes. Additionally, we conduct a comprehensive human assessment and an in-depth evaluation of LLM agents in strategic decision-making scenarios. Our findings not only expand the understanding of LLM agents' proficiency in emulating human strategic behavior but also h
&lt;/p&gt;</description></item><item><title>CausalCite&#26159;&#19968;&#31181;&#20197;&#22240;&#26524;&#25512;&#26029;&#20026;&#22522;&#30784;&#30340;&#35770;&#25991;&#24341;&#29992;&#20844;&#24335;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#25991;&#26412;&#36827;&#34892;&#23884;&#20837;&#21644;&#30456;&#20284;&#26679;&#26412;&#30340;&#25552;&#21462;&#26469;&#35780;&#20272;&#35770;&#25991;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#22312;&#21508;&#20010;&#26631;&#20934;&#19978;&#23637;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.02790</link><description>&lt;p&gt;
CausalCite&#65306;&#19968;&#31181;&#35770;&#25991;&#24341;&#29992;&#30340;&#22240;&#26524;&#20844;&#24335;&#21270;
&lt;/p&gt;
&lt;p&gt;
CausalCite: A Causal Formulation of Paper Citations. (arXiv:2311.02790v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.02790
&lt;/p&gt;
&lt;p&gt;
CausalCite&#26159;&#19968;&#31181;&#20197;&#22240;&#26524;&#25512;&#26029;&#20026;&#22522;&#30784;&#30340;&#35770;&#25991;&#24341;&#29992;&#20844;&#24335;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#25991;&#26412;&#36827;&#34892;&#23884;&#20837;&#21644;&#30456;&#20284;&#26679;&#26412;&#30340;&#25552;&#21462;&#26469;&#35780;&#20272;&#35770;&#25991;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#22312;&#21508;&#20010;&#26631;&#20934;&#19978;&#23637;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#31185;&#23398;&#30028;&#26469;&#35828;&#65292;&#35780;&#20272;&#19968;&#31687;&#35770;&#25991;&#30340;&#37325;&#35201;&#24615;&#33267;&#20851;&#37325;&#35201;&#20294;&#20063;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#23613;&#31649;&#24341;&#29992;&#27425;&#25968;&#26159;&#26368;&#24120;&#29992;&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;&#20294;&#23427;&#20204;&#34987;&#24191;&#27867;&#25209;&#35780;&#20026;&#26080;&#27861;&#20934;&#30830;&#21453;&#26144;&#19968;&#31687;&#35770;&#25991;&#30340;&#30495;&#27491;&#24433;&#21709;&#21147;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22240;&#26524;&#25512;&#26029;&#26041;&#27861;&#65292;&#31216;&#20026;TextMatch&#65292;&#23427;&#23558;&#20256;&#32479;&#30340;&#21305;&#37197;&#26694;&#26550;&#36866;&#24212;&#20110;&#39640;&#32500;&#25991;&#26412;&#23884;&#20837;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23545;&#27599;&#31687;&#35770;&#25991;&#36827;&#34892;&#25991;&#26412;&#23884;&#20837;&#65292;&#36890;&#36807;&#20313;&#24358;&#30456;&#20284;&#24615;&#25552;&#21462;&#30456;&#20284;&#26679;&#26412;&#65292;&#24182;&#26681;&#25454;&#30456;&#20284;&#24230;&#20540;&#30340;&#21152;&#26435;&#24179;&#22343;&#21512;&#25104;&#19968;&#20010;&#21453;&#20107;&#23454;&#26679;&#26412;&#12290;&#25105;&#20204;&#23558;&#24471;&#21040;&#30340;&#25351;&#26631;&#31216;&#20026;CausalCite&#65292;&#20316;&#20026;&#35770;&#25991;&#24341;&#29992;&#30340;&#22240;&#26524;&#20844;&#24335;&#21270;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#23427;&#22312;&#21508;&#31181;&#26631;&#20934;&#19978;&#30340;&#26377;&#25928;&#24615;&#65292;&#22914;&#19982;&#31185;&#23398;&#19987;&#23478;&#23545;1K&#31687;&#35770;&#25991;&#30340;&#25253;&#21578;&#30340;&#35770;&#25991;&#24433;&#21709;&#21147;&#30340;&#39640;&#30456;&#20851;&#24615;&#65292;&#36807;&#21435;&#35770;&#25991;&#30340;&#65288;&#32463;&#36807;&#26102;&#38388;&#32771;&#39564;&#30340;&#65289;&#22870;&#39033;&#65292;&#20197;&#21450;&#22312;&#21508;&#20010;&#23376;&#39046;&#22495;&#30340;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Evaluating the significance of a paper is pivotal yet challenging for the scientific community. While the citation count is the most commonly used proxy for this purpose, they are widely criticized for failing to accurately reflect a paper's true impact. In this work, we propose a causal inference method, TextMatch, which adapts the traditional matching framework to high-dimensional text embeddings. Specifically, we encode each paper using the text embeddings by large language models (LLMs), extract similar samples by cosine similarity, and synthesize a counterfactual sample by the weighted average of similar papers according to their similarity values. We apply the resulting metric, called CausalCite, as a causal formulation of paper citations. We show its effectiveness on various criteria, such as high correlation with paper impact as reported by scientific experts on a previous dataset of 1K papers, (test-of-time) awards for past papers, and its stability across various sub-fields o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#24212;&#24615;&#30693;&#35782;&#20445;&#30041;&#21098;&#26525;&#31574;&#30053;&#65292;&#26088;&#22312;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#23545;&#25239;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#22312;&#21098;&#26525;&#36807;&#31243;&#20013;&#20445;&#30041;&#26356;&#22810;&#30340;&#39044;&#35757;&#32451;&#30693;&#35782;&#12290;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#23637;&#29616;&#20102;&#26356;&#22909;&#30340;&#24179;&#34913;&#12290;</title><link>http://arxiv.org/abs/2310.13191</link><description>&lt;p&gt;
&#26397;&#30528;&#40065;&#26834;&#21098;&#26525;&#65306;&#19968;&#31181;&#38754;&#21521;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#36866;&#24212;&#30693;&#35782;&#20445;&#30041;&#21098;&#26525;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Towards Robust Pruning: An Adaptive Knowledge-Retention Pruning Strategy for Language Models. (arXiv:2310.13191v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13191
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#24212;&#24615;&#30693;&#35782;&#20445;&#30041;&#21098;&#26525;&#31574;&#30053;&#65292;&#26088;&#22312;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#23545;&#25239;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#22312;&#21098;&#26525;&#36807;&#31243;&#20013;&#20445;&#30041;&#26356;&#22810;&#30340;&#39044;&#35757;&#32451;&#30693;&#35782;&#12290;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#23637;&#29616;&#20102;&#26356;&#22909;&#30340;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21098;&#26525;&#30446;&#26631;&#36817;&#26399;&#19981;&#20165;&#20165;&#23616;&#38480;&#20110;&#20934;&#30830;&#24615;&#21644;&#31232;&#30095;&#24615;&#65292;&#36824;&#21253;&#25324;&#23545;&#35821;&#35328;&#27169;&#22411;&#40065;&#26834;&#24615;&#30340;&#25552;&#21319;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#22312;&#25345;&#32493;&#22686;&#21152;&#27169;&#22411;&#31232;&#30095;&#24615;&#26102;&#65292;&#23545;&#25239;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#25552;&#21319;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#24182;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#36807;&#31243;&#12290;&#38543;&#30528;&#20154;&#20204;&#27493;&#20837;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26102;&#20195;&#65292;&#36825;&#20123;&#38382;&#39064;&#21464;&#24471;&#36234;&#26469;&#36234;&#31361;&#20986;&#12290;&#26412;&#25991;&#25552;&#20986;&#65292;&#35821;&#35328;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#19982;&#20854;&#28085;&#30422;&#30340;&#39044;&#35757;&#32451;&#30693;&#35782;&#31243;&#24230;&#25104;&#27491;&#27604;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21518;&#35757;&#32451;&#30340;&#21098;&#26525;&#31574;&#30053;&#65292;&#26088;&#22312;&#22312;&#21098;&#26525;&#36807;&#31243;&#20013;&#20445;&#30041;&#26356;&#22810;&#39044;&#35757;&#32451;&#30693;&#35782;&#65292;&#20197;&#24544;&#23454;&#22320;&#22797;&#21046;&#23494;&#38598;&#35821;&#35328;&#27169;&#22411;&#30340;&#23884;&#20837;&#31354;&#38388;&#21644;&#29305;&#24449;&#31354;&#38388;&#12290;&#22312;&#36825;&#20010;&#35774;&#32622;&#20013;&#65292;&#27599;&#19968;&#23618;&#30340;&#37325;&#26500;&#35823;&#24046;&#19981;&#20165;&#28304;&#33258;&#33258;&#36523;&#65292;&#36824;&#21253;&#25324;&#21069;&#38754;&#23618;&#30340;&#32047;&#31215;&#35823;&#24046;&#65292;&#28982;&#21518;&#36827;&#34892;&#33258;&#36866;&#24212;&#30340;&#30699;&#27491;&#12290;&#19982;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#22522;&#32447;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23637;&#29616;&#20102;&#26356;&#22909;&#30340;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
The pruning objective has recently extended beyond accuracy and sparsity to robustness in language models. Despite this, existing methods struggle to enhance robustness against adversarial attacks when continually increasing model sparsity and require a retraining process. As humans step into the era of large language models, these issues become increasingly prominent. This paper proposes that the robustness of language models is proportional to the extent of pre-trained knowledge they encompass. Accordingly, we introduce a post-training pruning strategy designed to faithfully replicate the embedding space and feature space of dense language models, aiming to conserve more pre-trained knowledge during the pruning process. In this setup, each layer's reconstruction error not only originates from itself but also includes cumulative error from preceding layers, followed by an adaptive rectification. Compared to other state-of-art baselines, our approach demonstrates a superior balance bet
&lt;/p&gt;</description></item><item><title>CodeFuse-13B&#26159;&#19968;&#20010;&#39044;&#35757;&#32451;&#30340;&#22810;&#35821;&#35328;&#20195;&#30721;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#19987;&#20026;&#20195;&#30721;&#30456;&#20851;&#20219;&#21153;&#35774;&#35745;&#65292;&#25903;&#25345;&#36229;&#36807;40&#31181;&#32534;&#31243;&#35821;&#35328;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#39640;&#36136;&#37327;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#20197;&#21450;&#22823;&#37327;&#23454;&#39564;&#30340;&#39564;&#35777;&#65292;&#23637;&#29616;&#20102;&#20854;&#22312;&#22810;&#35821;&#35328;&#36755;&#20837;&#19979;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.06266</link><description>&lt;p&gt;
CodeFuse-13B: &#19968;&#20010;&#39044;&#35757;&#32451;&#30340;&#22810;&#35821;&#35328;&#20195;&#30721;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
CodeFuse-13B: A Pretrained Multi-lingual Code Large Language Model. (arXiv:2310.06266v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06266
&lt;/p&gt;
&lt;p&gt;
CodeFuse-13B&#26159;&#19968;&#20010;&#39044;&#35757;&#32451;&#30340;&#22810;&#35821;&#35328;&#20195;&#30721;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#19987;&#20026;&#20195;&#30721;&#30456;&#20851;&#20219;&#21153;&#35774;&#35745;&#65292;&#25903;&#25345;&#36229;&#36807;40&#31181;&#32534;&#31243;&#35821;&#35328;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#39640;&#36136;&#37327;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#20197;&#21450;&#22823;&#37327;&#23454;&#39564;&#30340;&#39564;&#35777;&#65292;&#23637;&#29616;&#20102;&#20854;&#22312;&#22810;&#35821;&#35328;&#36755;&#20837;&#19979;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20195;&#30721;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(Code LLMs)&#22240;&#20854;&#22312;&#36719;&#20214;&#24037;&#31243;&#20840;&#29983;&#21629;&#21608;&#26399;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#32780;&#21463;&#21040;&#24037;&#19994;&#30028;&#30340;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#27169;&#22411;&#22312;&#29702;&#35299;&#38750;&#33521;&#35821;&#36755;&#20837;&#30340;&#22810;&#35821;&#35328;&#20195;&#30721;&#30456;&#20851;&#20219;&#21153;&#26041;&#38754;&#30340;&#25928;&#26524;&#20173;&#28982;&#36828;&#26410;&#34987;&#20805;&#20998;&#30740;&#31350;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;CodeFuse-13B&#65292;&#19968;&#20010;&#24320;&#28304;&#30340;&#39044;&#35757;&#32451;&#20195;&#30721;LLM&#12290;&#23427;&#19987;&#20026;&#21253;&#21547;&#33521;&#25991;&#21644;&#20013;&#25991;&#25552;&#31034;&#30340;&#20195;&#30721;&#30456;&#20851;&#20219;&#21153;&#32780;&#35774;&#35745;&#65292;&#24182;&#25903;&#25345;&#36229;&#36807;40&#31181;&#32534;&#31243;&#35821;&#35328;&#12290;CodeFuse&#36890;&#36807;&#21033;&#29992;&#30001;&#31243;&#24207;&#20998;&#26512;&#22120;&#31934;&#24515;&#31579;&#36873;&#24182;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20248;&#21270;&#30340;&#39640;&#36136;&#37327;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#26469;&#23454;&#29616;&#20854;&#25928;&#26524;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#21253;&#25324;&#20351;&#29992;&#30495;&#23454;&#19990;&#30028;&#30340;&#20351;&#29992;&#22330;&#26223;&#12289;&#24037;&#19994;&#26631;&#20934;&#22522;&#20934;HumanEval-x&#65292;&#20197;&#21450;&#19987;&#20026;&#20013;&#25991;&#25552;&#31034;&#35774;&#35745;&#30340;CodeFuseEval&#12290;&#20026;&#20102;&#35780;&#20272;CodeFuse&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#31215;&#26497;&#25910;&#38598;&#20102;AntGroup&#36719;&#20214;&#24320;&#21457;&#22242;&#38431;&#30340;&#23453;&#36149;&#20154;&#24037;&#21453;&#39304;&#12290;
&lt;/p&gt;
&lt;p&gt;
Code Large Language Models (Code LLMs) have gained significant attention in the industry due to their wide applications in the full lifecycle of software engineering. However, the effectiveness of existing models in understanding non-English inputs for multi-lingual code-related tasks is still far from well studied. This paper introduces CodeFuse-13B, an open-sourced pre-trained code LLM. It is specifically designed for code-related tasks with both English and Chinese prompts and supports over 40 programming languages. CodeFuse achieves its effectiveness by utilizing a high quality pre-training dataset that is carefully filtered by program analyzers and optimized during the training process. Extensive experiments are conducted using real-world usage scenarios, the industry-standard benchmark HumanEval-x, and the specially designed CodeFuseEval for Chinese prompts. To assess the effectiveness of CodeFuse, we actively collected valuable human feedback from the AntGroup's software develop
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#36890;&#36807;&#26681;&#26412;&#24615;&#30340;&#25913;&#38761;&#65292;&#23558;&#30005;&#23376;&#36135;&#24065;&#32435;&#20837;&#32463;&#27982;&#19982;&#37329;&#34701;&#29702;&#35770;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#21253;&#25324;&#30005;&#23376;&#36135;&#24065;&#30340;&#20272;&#20540;&#22522;&#20110;&#23439;&#35266;&#32463;&#27982;&#29702;&#35770;&#21644;&#36135;&#24065;&#25919;&#31574;&#30340;&#22522;&#26412;&#26041;&#31243;&#65292;&#20197;&#21450;&#30005;&#23376;&#36135;&#24065;&#31649;&#29702;&#20844;&#21496;&#20316;&#20026;&#21327;&#35843;&#27425;&#32463;&#27982;&#20307;&#36135;&#24065;&#21644;&#36130;&#25919;&#25919;&#31574;&#30340;&#23454;&#20307;&#12290;&#35813;&#30740;&#31350;&#36991;&#20813;&#20351;&#29992;&#26222;&#36941;&#20294;&#19981;&#36866;&#24403;&#30340;&#25351;&#25968;&#39118;&#38505;&#27169;&#22411;&#65292;&#32780;&#26159;&#37319;&#29992;&#22810;&#26102;&#38388;&#23610;&#24230;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2310.04986</link><description>&lt;p&gt;
&#19968;&#31181;&#20851;&#20110;&#36135;&#24065;&#30340;&#26032;&#30340;&#32463;&#27982;&#19982;&#37329;&#34701;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
A new economic and financial theory of money. (arXiv:2310.04986v1 [econ.TH])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04986
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#36890;&#36807;&#26681;&#26412;&#24615;&#30340;&#25913;&#38761;&#65292;&#23558;&#30005;&#23376;&#36135;&#24065;&#32435;&#20837;&#32463;&#27982;&#19982;&#37329;&#34701;&#29702;&#35770;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#21253;&#25324;&#30005;&#23376;&#36135;&#24065;&#30340;&#20272;&#20540;&#22522;&#20110;&#23439;&#35266;&#32463;&#27982;&#29702;&#35770;&#21644;&#36135;&#24065;&#25919;&#31574;&#30340;&#22522;&#26412;&#26041;&#31243;&#65292;&#20197;&#21450;&#30005;&#23376;&#36135;&#24065;&#31649;&#29702;&#20844;&#21496;&#20316;&#20026;&#21327;&#35843;&#27425;&#32463;&#27982;&#20307;&#36135;&#24065;&#21644;&#36130;&#25919;&#25919;&#31574;&#30340;&#23454;&#20307;&#12290;&#35813;&#30740;&#31350;&#36991;&#20813;&#20351;&#29992;&#26222;&#36941;&#20294;&#19981;&#36866;&#24403;&#30340;&#25351;&#25968;&#39118;&#38505;&#27169;&#22411;&#65292;&#32780;&#26159;&#37319;&#29992;&#22810;&#26102;&#38388;&#23610;&#24230;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#32463;&#27982;&#19982;&#37329;&#34701;&#29702;&#35770;&#36827;&#34892;&#20102;&#26681;&#26412;&#24615;&#25913;&#38761;&#65292;&#21253;&#25324;&#30005;&#23376;&#36135;&#24065;&#22312;&#20869;&#12290;&#30005;&#23376;&#36135;&#24065;&#30340;&#20272;&#20540;&#23558;&#22522;&#20110;&#23439;&#35266;&#32463;&#27982;&#29702;&#35770;&#21644;&#36135;&#24065;&#25919;&#31574;&#30340;&#22522;&#26412;&#26041;&#31243;&#65292;&#32780;&#19981;&#26159;&#24494;&#35266;&#32463;&#27982;&#23398;&#20013;&#30340;&#36148;&#29616;&#29616;&#37329;&#27969;&#29702;&#35770;&#12290;&#19982;&#23558;&#32929;&#31080;&#35270;&#20026;&#19982;&#27425;&#32463;&#27982;&#20307;&#30340;&#26080;&#24418;&#36164;&#20135;&#20851;&#32852;&#30340;&#25152;&#26377;&#26435;&#19981;&#21516;&#65292;&#25105;&#20204;&#23558;&#21457;&#23637;&#30005;&#23376;&#36135;&#24065;&#20316;&#20026;&#19982;&#27425;&#32463;&#27982;&#20307;&#26377;&#24418;&#36164;&#20135;&#20851;&#32852;&#30340;&#20132;&#26131;&#26435;&#30410;&#30340;&#35266;&#28857;&#12290;&#25105;&#20204;&#36824;&#23558;&#21457;&#23637;&#30005;&#23376;&#36135;&#24065;&#31649;&#29702;&#20844;&#21496;&#20316;&#20026;&#19968;&#20010;&#36127;&#36131;&#21327;&#35843;&#27425;&#32463;&#27982;&#20307;&#30340;&#36135;&#24065;&#65288;&#30005;&#23376;&#36135;&#24065;&#20379;&#24212;&#21644;&#20215;&#20540;&#31283;&#23450;&#65289;&#21644;&#36130;&#25919;&#65288;&#25237;&#36164;&#21644;&#36816;&#33829;&#65289;&#25919;&#31574;&#30340;&#23454;&#20307;&#30340;&#35270;&#35282;&#65292;&#20197;&#23454;&#29616;&#30005;&#23376;&#36135;&#24065;&#30340;&#27969;&#21160;&#24615;&#12290;&#22312;&#20272;&#20540;&#21644;&#20915;&#31574;&#20013;&#20351;&#29992;&#30340;&#39118;&#38505;&#27169;&#22411;&#19981;&#20250;&#26159;&#26080;&#22788;&#19981;&#22312;&#20294;&#19981;&#21512;&#36866;&#30340;&#25351;&#25968;&#39118;&#38505;&#27169;&#22411;&#65292;&#23427;&#23558;&#23548;&#33268;&#36148;&#29616;&#29575;&#65292;&#32780;&#26159;&#22810;&#26102;&#38388;&#23610;&#24230;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper fundamentally reformulates economic and financial theory to include electronic currencies. The valuation of the electronic currencies will be based on macroeconomic theory and the fundamental equation of monetary policy, not the microeconomic theory of discounted cash flows. The view of electronic currency as a transactional equity associated with tangible assets of a sub-economy will be developed, in contrast to the view of stock as an equity associated mostly with intangible assets of a sub-economy. The view will be developed of the electronic currency management firm as an entity responsible for coordinated monetary (electronic currency supply and value stabilization) and fiscal (investment and operational) policies of a substantial (for liquidity of the electronic currency) sub-economy. The risk model used in the valuations and the decision-making will not be the ubiquitous, yet inappropriate, exponential risk model that leads to discount rates, but will be multi time sc
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#20284;&#28982;&#30340;&#20256;&#24863;&#22120;&#26657;&#20934;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#29289;&#32852;&#32593;&#31995;&#32479;&#20013;&#23454;&#29616;&#19987;&#23478;&#25903;&#25345;&#30340;&#20998;&#24067;&#24335;&#23398;&#20064;&#31639;&#27861;&#12290;&#36890;&#36807;&#23545;&#27169;&#25311;&#21644;&#23454;&#38469;&#27979;&#37327;&#25968;&#25454;&#30340;&#35780;&#20272;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#25913;&#36827;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.11526</link><description>&lt;p&gt;
&#22522;&#20110;&#20284;&#28982;&#30340;&#29289;&#32852;&#32593;&#31995;&#32479;&#20013;&#19987;&#23478;&#25903;&#25345;&#30340;&#20998;&#24067;&#24335;&#23398;&#20064;&#31639;&#27861;&#20013;&#20256;&#24863;&#22120;&#26657;&#20934;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Likelihood-based Sensor Calibration for Expert-Supported Distributed Learning Algorithms in IoT Systems. (arXiv:2309.11526v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11526
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#20284;&#28982;&#30340;&#20256;&#24863;&#22120;&#26657;&#20934;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#29289;&#32852;&#32593;&#31995;&#32479;&#20013;&#23454;&#29616;&#19987;&#23478;&#25903;&#25345;&#30340;&#20998;&#24067;&#24335;&#23398;&#20064;&#31639;&#27861;&#12290;&#36890;&#36807;&#23545;&#27169;&#25311;&#21644;&#23454;&#38469;&#27979;&#37327;&#25968;&#25454;&#30340;&#35780;&#20272;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#25913;&#36827;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#24863;&#22120;&#25216;&#26415;&#39046;&#22495;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#20219;&#21153;&#26159;&#23558;&#19968;&#20010;&#20256;&#24863;&#22120;&#30340;&#27979;&#37327;&#32467;&#26524;&#39640;&#25928;&#22320;&#36866;&#24212;&#21040;&#21478;&#19968;&#20010;&#20855;&#26377;&#30456;&#21516;&#35774;&#35745;&#30340;&#20256;&#24863;&#22120;&#12290;&#19968;&#31181;&#24819;&#27861;&#26159;&#20351;&#29992;&#19981;&#21516;&#31995;&#32479;&#20043;&#38388;&#30340;&#20223;&#23556;&#21464;&#25442;&#20272;&#35745;&#65292;&#36825;&#21487;&#20197;&#36890;&#36807;&#19987;&#23478;&#30340;&#30693;&#35782;&#36827;&#34892;&#25913;&#36827;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;Glacier Research&#22312;1973&#24180;&#21457;&#34920;&#30340;&#25913;&#36827;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#23637;&#31034;&#20102;&#35813;&#35299;&#20915;&#26041;&#26696;&#21487;&#20197;&#29992;&#20110;&#20256;&#24863;&#22120;&#30340;&#36719;&#20214;&#26657;&#20934;&#12289;&#22522;&#20110;&#19987;&#23478;&#30340;&#36866;&#24212;&#21644;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#27169;&#25311;&#21644;&#23454;&#38469;&#27979;&#37327;&#25968;&#25454;&#23545;&#25105;&#20204;&#30340;&#30740;&#31350;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#23454;&#39564;&#20013;&#20351;&#29992;&#20102;&#19968;&#20010;&#20855;&#26377;8&#20010;&#30456;&#21516;&#20256;&#24863;&#22120;&#30340;&#22810;&#20256;&#24863;&#22120;&#26495;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#26080;&#35770;&#26159;&#27169;&#25311;&#36824;&#26159;&#23454;&#39564;&#25968;&#25454;&#65292;&#37117;&#24471;&#21040;&#20102;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
An important task in the field of sensor technology is the efficient implementation of adaptation procedures of measurements from one sensor to another sensor of identical design. One idea is to use the estimation of an affine transformation between different systems, which can be improved by the knowledge of experts. This paper presents an improved solution from Glacier Research that was published back in 1973. It is shown that this solution can be adapted for software calibration of sensors, implementation of expert-based adaptation, and federated learning methods. We evaluate our research with simulations and also with real measured data of a multi-sensor board with 8 identical sensors. The results show an improvement for both the simulation and the experiments with real data.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#27169;&#24577;&#22810;&#20219;&#21153;&#23545;&#35805;&#34892;&#20026;&#20998;&#31867;&#20219;&#21153;&#65292;&#24182;&#20351;&#29992;&#38750;&#24179;&#31283;&#22810;&#33218;&#36172;&#21338;&#26426;&#21644;&#39640;&#26031;&#20808;&#39564;&#30340;&#25240;&#25187;&#27748;&#26222;&#26862;&#37319;&#26679;&#26041;&#27861;&#36827;&#34892;&#20219;&#21153;&#36873;&#25321;&#21644;&#20998;&#37197;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#35782;&#21035;&#20219;&#21153;&#25928;&#29992;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#36991;&#20813;&#26080;&#29992;&#25110;&#26377;&#23475;&#30340;&#20219;&#21153;&#65292;&#24182;&#22312;UAR&#26041;&#38754;&#20855;&#26377;&#26174;&#33879;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2309.09832</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#22810;&#20219;&#21153;&#23545;&#35805;&#34892;&#20026;&#20998;&#31867;&#20013;&#30340;&#20219;&#21153;&#36873;&#25321;&#21644;&#20998;&#37197;&#26041;&#27861;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Task Selection and Assignment for Multi-modal Multi-task Dialogue Act Classification with Non-stationary Multi-armed Bandits. (arXiv:2309.09832v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.09832
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#27169;&#24577;&#22810;&#20219;&#21153;&#23545;&#35805;&#34892;&#20026;&#20998;&#31867;&#20219;&#21153;&#65292;&#24182;&#20351;&#29992;&#38750;&#24179;&#31283;&#22810;&#33218;&#36172;&#21338;&#26426;&#21644;&#39640;&#26031;&#20808;&#39564;&#30340;&#25240;&#25187;&#27748;&#26222;&#26862;&#37319;&#26679;&#26041;&#27861;&#36827;&#34892;&#20219;&#21153;&#36873;&#25321;&#21644;&#20998;&#37197;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#35782;&#21035;&#20219;&#21153;&#25928;&#29992;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#36991;&#20813;&#26080;&#29992;&#25110;&#26377;&#23475;&#30340;&#20219;&#21153;&#65292;&#24182;&#22312;UAR&#26041;&#38754;&#20855;&#26377;&#26174;&#33879;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#20219;&#21153;&#23398;&#20064;&#26088;&#22312;&#36890;&#36807;&#19982;&#30456;&#20851;&#36741;&#21161;&#20219;&#21153;&#30340;&#32852;&#21512;&#23398;&#20064;&#26469;&#25552;&#39640;&#20027;&#35201;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#20256;&#32479;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#26041;&#27861;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#38543;&#26426;&#36873;&#25321;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#21644;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#38543;&#26426;&#36873;&#25321;&#20219;&#21153;&#30340;&#26041;&#27861;&#21487;&#33021;&#23545;&#24615;&#33021;&#27809;&#26377;&#24110;&#21161;&#65292;&#29978;&#33267;&#20250;&#26377;&#23475;&#12290;&#22240;&#27492;&#65292;&#38656;&#35201;&#25506;&#32034;&#22810;&#20219;&#21153;&#23398;&#20064;&#20013;&#20219;&#21153;&#36873;&#25321;&#21644;&#20998;&#37197;&#30340;&#26032;&#31574;&#30053;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#27169;&#24577;&#22810;&#20219;&#21153;&#23545;&#35805;&#34892;&#20026;&#20998;&#31867;&#20219;&#21153;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38750;&#24179;&#31283;&#22810;&#33218;&#36172;&#21338;&#26426;&#21644;&#39640;&#26031;&#20808;&#39564;&#30340;&#25240;&#25187;&#27748;&#26222;&#26862;&#37319;&#26679;&#26041;&#27861;&#26469;&#36873;&#25321;&#21644;&#20998;&#37197;&#20219;&#21153;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#19981;&#21516;&#30340;&#35757;&#32451;&#38454;&#27573;&#65292;&#19981;&#21516;&#30340;&#20219;&#21153;&#20855;&#26377;&#19981;&#21516;&#30340;&#25928;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#35782;&#21035;&#20219;&#21153;&#25928;&#29992;&#65292;&#20027;&#21160;&#36991;&#20813;&#26080;&#29992;&#25110;&#26377;&#23475;&#30340;&#20219;&#21153;&#65292;&#24182;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#23454;&#29616;&#20219;&#21153;&#20998;&#37197;&#12290;&#22312;UAR&#26041;&#38754;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#26174;&#30528;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-task learning (MTL) aims to improve the performance of a primary task by jointly learning with related auxiliary tasks. Traditional MTL methods select tasks randomly during training. However, both previous studies and our results suggest that such a random selection of tasks may not be helpful, and can even be harmful to performance. Therefore, new strategies for task selection and assignment in MTL need to be explored. This paper studies the multi-modal, multi-task dialogue act classification task, and proposes a method for selecting and assigning tasks based on non-stationary multi-armed bandits (MAB) with discounted Thompson Sampling (TS) using Gaussian priors. Our experimental results show that in different training stages, different tasks have different utility. Our proposed method can effectively identify the task utility, actively avoid useless or harmful tasks, and realise the task assignment during training. Our proposed method is significantly superior in terms of UAR a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36974;&#25513;&#22270;&#20687;&#24314;&#27169;&#30340;&#24322;&#26500;&#29983;&#25104;&#30693;&#35782;&#33976;&#39311;&#65288;H-GKD&#65289;&#26041;&#27861;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#20174;&#22823;&#22411;Transformer&#27169;&#22411;&#20256;&#36882;&#30693;&#35782;&#32473;&#23567;&#22411;CNN&#27169;&#22411;&#12290;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#24102;&#26377;&#31232;&#30095;&#21367;&#31215;&#30340;UNet&#39118;&#26684;&#23398;&#29983;&#32593;&#32476;&#65292;&#26725;&#25509;&#20102;Transformer&#27169;&#22411;&#21644;CNN&#27169;&#22411;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#23454;&#29616;&#20102;&#35270;&#35273;&#34920;&#31034;&#30340;&#26377;&#25928;&#27169;&#25311;&#12290;</title><link>http://arxiv.org/abs/2309.09571</link><description>&lt;p&gt;
&#20351;&#29992;&#36974;&#25513;&#22270;&#20687;&#24314;&#27169;&#30340;&#24322;&#26500;&#29983;&#25104;&#30693;&#35782;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Heterogeneous Generative Knowledge Distillation with Masked Image Modeling. (arXiv:2309.09571v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.09571
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36974;&#25513;&#22270;&#20687;&#24314;&#27169;&#30340;&#24322;&#26500;&#29983;&#25104;&#30693;&#35782;&#33976;&#39311;&#65288;H-GKD&#65289;&#26041;&#27861;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#20174;&#22823;&#22411;Transformer&#27169;&#22411;&#20256;&#36882;&#30693;&#35782;&#32473;&#23567;&#22411;CNN&#27169;&#22411;&#12290;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#24102;&#26377;&#31232;&#30095;&#21367;&#31215;&#30340;UNet&#39118;&#26684;&#23398;&#29983;&#32593;&#32476;&#65292;&#26725;&#25509;&#20102;Transformer&#27169;&#22411;&#21644;CNN&#27169;&#22411;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#23454;&#29616;&#20102;&#35270;&#35273;&#34920;&#31034;&#30340;&#26377;&#25928;&#27169;&#25311;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35745;&#31639;&#36164;&#28304;&#26377;&#38480;&#30340;&#36793;&#32536;&#35774;&#22791;&#19978;&#37096;&#32626;&#26102;&#65292;&#23567;&#22411;&#22522;&#20110;CNN&#30340;&#27169;&#22411;&#36890;&#24120;&#38656;&#35201;&#20174;&#22823;&#22411;&#27169;&#22411;&#20013;&#36716;&#31227;&#30693;&#35782;&#12290;&#36974;&#25513;&#22270;&#20687;&#24314;&#27169;&#65288;MIM&#65289;&#26041;&#27861;&#22312;&#21508;&#31181;&#35270;&#35273;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#24456;&#22823;&#30340;&#25104;&#21151;&#65292;&#20294;&#22312;&#24322;&#26500;&#28145;&#24230;&#27169;&#22411;&#30340;&#30693;&#35782;&#33976;&#39311;&#20013;&#20173;&#28982;&#24456;&#23569;&#34987;&#25506;&#32034;&#12290;&#20027;&#35201;&#21407;&#22240;&#26159;&#22522;&#20110;Transformer&#30340;&#22823;&#22411;&#27169;&#22411;&#19982;&#22522;&#20110;CNN&#30340;&#23567;&#22411;&#32593;&#32476;&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#22522;&#20110;MIM&#24320;&#21457;&#20102;&#31532;&#19968;&#20010;&#20351;&#29992;&#24322;&#26500;&#29983;&#25104;&#30693;&#35782;&#33976;&#39311;&#65288;H-GKD&#65289;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20197;&#29983;&#25104;&#30340;&#33258;&#30417;&#30563;&#26041;&#24335;&#39640;&#25928;&#22320;&#20174;&#22823;&#22411;Transformer&#27169;&#22411;&#20256;&#36882;&#30693;&#35782;&#32473;&#23567;&#22411;&#22522;&#20110;CNN&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#24102;&#26377;&#31232;&#30095;&#21367;&#31215;&#30340;UNet&#39118;&#26684;&#30340;&#23398;&#29983;&#32593;&#32476;&#26469;&#26500;&#24314;Transformer&#27169;&#22411;&#21644;CNN&#20043;&#38388;&#30340;&#26725;&#26753;&#65292;&#36825;&#20010;&#23398;&#29983;&#32593;&#32476;&#21487;&#20197;&#26377;&#25928;&#22320;&#27169;&#20223;&#25945;&#24072;&#32593;&#32476;&#22312;&#36974;&#25513;&#24314;&#27169;&#19979;&#25512;&#26029;&#30340;&#35270;&#35273;&#34920;&#31034;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#23398;&#20064;&#33539;&#24335;&#65292;&#29992;&#20110;&#23398;&#20064;&#35270;&#35273;&#25552;&#31034;&#21644;&#30693;&#35782;&#22312;&#24322;&#26500;&#27169;&#22411;&#20043;&#38388;&#30340;&#36716;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;
Small CNN-based models usually require transferring knowledge from a large model before they are deployed in computationally resource-limited edge devices. Masked image modeling (MIM) methods achieve great success in various visual tasks but remain largely unexplored in knowledge distillation for heterogeneous deep models. The reason is mainly due to the significant discrepancy between the Transformer-based large model and the CNN-based small network. In this paper, we develop the first Heterogeneous Generative Knowledge Distillation (H-GKD) based on MIM, which can efficiently transfer knowledge from large Transformer models to small CNN-based models in a generative self-supervised fashion. Our method builds a bridge between Transformer-based models and CNNs by training a UNet-style student with sparse convolution, which can effectively mimic the visual representation inferred by a teacher over masked modeling. Our method is a simple yet effective learning paradigm to learn the visual 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#20132;&#20114;&#24335;&#36229;&#21442;&#25968;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#24212;&#29992;&#20559;&#22909;&#23398;&#20064;&#26469;&#35299;&#20915;&#22810;&#30446;&#26631;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.03581</link><description>&lt;p&gt;
&#36890;&#36807;&#20559;&#22909;&#23398;&#20064;&#22312;&#22810;&#30446;&#26631;&#38382;&#39064;&#20013;&#36827;&#34892;&#20132;&#20114;&#24335;&#36229;&#21442;&#25968;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Interactive Hyperparameter Optimization in Multi-Objective Problems via Preference Learning. (arXiv:2309.03581v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03581
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#20132;&#20114;&#24335;&#36229;&#21442;&#25968;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#24212;&#29992;&#20559;&#22909;&#23398;&#20064;&#26469;&#35299;&#20915;&#22810;&#30446;&#26631;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36229;&#21442;&#25968;&#20248;&#21270;&#23545;&#20110;&#21457;&#25381;&#26426;&#22120;&#23398;&#20064;&#30340;&#28508;&#21147;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#29992;&#25143;&#36890;&#24120;&#23545;&#22810;&#30446;&#26631;&#38382;&#39064;&#24863;&#20852;&#36259;&#65292;&#21363;&#20248;&#21270;&#21487;&#33021;&#23384;&#22312;&#20914;&#31361;&#30340;&#30446;&#26631;&#65292;&#27604;&#22914;&#20934;&#30830;&#24615;&#21644;&#33021;&#32791;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#32477;&#22823;&#22810;&#25968;&#22810;&#30446;&#26631;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#23558;&#19968;&#32452;&#38750;&#25903;&#37197;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#24085;&#32047;&#25176;&#21069;&#27839;&#36820;&#22238;&#32473;&#29992;&#25143;&#12290;&#28982;&#32780;&#65292;&#20248;&#21270;&#36825;&#31181;&#31639;&#27861;&#30340;&#36229;&#21442;&#25968;&#24182;&#19981;&#23481;&#26131;&#65292;&#22240;&#20026;&#35780;&#20272;&#19968;&#20010;&#36229;&#21442;&#25968;&#37197;&#32622;&#28041;&#21450;&#35780;&#20272;&#24471;&#21040;&#30340;&#24085;&#32047;&#25176;&#21069;&#27839;&#30340;&#36136;&#37327;&#12290;&#22312;&#25991;&#29486;&#20013;&#65292;&#24050;&#26377;&#19968;&#20123;&#25351;&#26631;&#21487;&#20197;&#36890;&#36807;&#37327;&#21270;&#19981;&#21516;&#23646;&#24615;&#65288;&#22914;&#20307;&#31215;&#12289;&#19982;&#21442;&#32771;&#28857;&#30340;&#25509;&#36817;&#31243;&#24230;&#65289;&#26469;&#35780;&#20272;&#24085;&#32047;&#25176;&#21069;&#27839;&#30340;&#36136;&#37327;&#65288;&#20363;&#22914;&#36229;&#20307;&#31215;&#12289;R2&#65289;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#29992;&#25143;&#26469;&#35828;&#65292;&#36873;&#25321;&#23548;&#33268;&#26399;&#26395;&#30340;&#24085;&#32047;&#25176;&#21069;&#27839;&#30340;&#25351;&#26631;&#21487;&#33021;&#26159;&#19968;&#39033;&#22256;&#38590;&#30340;&#20219;&#21153;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#20132;&#20114;&#24335;&#36229;&#21442;&#25968;&#20248;&#21270;&#26041;&#27861;&#65292;&#38024;&#23545;&#22810;&#30446;&#26631;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20559;&#22909;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hyperparameter optimization (HPO) is important to leverage the full potential of machine learning (ML). In practice, users are often interested in multi-objective (MO) problems, i.e., optimizing potentially conflicting objectives, like accuracy and energy consumption. To tackle this, the vast majority of MO-ML algorithms return a Pareto front of non-dominated machine learning models to the user. Optimizing the hyperparameters of such algorithms is non-trivial as evaluating a hyperparameter configuration entails evaluating the quality of the resulting Pareto front. In literature, there are known indicators that assess the quality of a Pareto front (e.g., hypervolume, R2) by quantifying different properties (e.g., volume, proximity to a reference point). However, choosing the indicator that leads to the desired Pareto front might be a hard task for a user. In this paper, we propose a human-centered interactive HPO approach tailored towards multi-objective ML leveraging preference learnin
&lt;/p&gt;</description></item><item><title>ProAgent&#26159;&#19968;&#20010;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26500;&#24314;&#30340;&#20027;&#21160;&#21512;&#20316;&#30340;AI&#26694;&#26550;&#65292;&#33021;&#22815;&#39044;&#27979;&#38431;&#21451;&#30340;&#20915;&#31574;&#24182;&#20026;&#33258;&#24049;&#21046;&#23450;&#22686;&#24378;&#35745;&#21010;&#65292;&#20855;&#26377;&#39640;&#24230;&#30340;&#27169;&#22359;&#21270;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.11339</link><description>&lt;p&gt;
ProAgent&#65306;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26500;&#24314;&#20027;&#21160;&#21512;&#20316;&#30340;&#20154;&#24037;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
ProAgent: Building Proactive Cooperative AI with Large Language Models. (arXiv:2308.11339v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11339
&lt;/p&gt;
&lt;p&gt;
ProAgent&#26159;&#19968;&#20010;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26500;&#24314;&#30340;&#20027;&#21160;&#21512;&#20316;&#30340;AI&#26694;&#26550;&#65292;&#33021;&#22815;&#39044;&#27979;&#38431;&#21451;&#30340;&#20915;&#31574;&#24182;&#20026;&#33258;&#24049;&#21046;&#23450;&#22686;&#24378;&#35745;&#21010;&#65292;&#20855;&#26377;&#39640;&#24230;&#30340;&#27169;&#22359;&#21270;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;AGI&#30740;&#31350;&#20013;&#65292;&#26500;&#24314;&#20855;&#26377;&#33258;&#36866;&#24212;&#34892;&#20026;&#30340;&#20154;&#24037;&#26234;&#33021;&#20197;&#36827;&#34892;&#20154;&#24037;&#26234;&#33021;&#21644;&#20154;&#31867;&#30340;&#21512;&#20316;&#25104;&#20026;&#19968;&#20010;&#20851;&#38190;&#20851;&#27880;&#28857;&#12290;&#30446;&#21069;&#65292;&#24320;&#21457;&#21512;&#20316;&#20195;&#29702;&#20154;&#30340;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#25919;&#31574;&#27867;&#21270;&#20005;&#37325;&#20381;&#36182;&#20110;&#19982;&#29305;&#23450;&#38431;&#21451;&#30340;&#36807;&#21435;&#20114;&#21160;&#12290;&#36825;&#20123;&#26041;&#27861;&#38480;&#21046;&#20102;&#20195;&#29702;&#20154;&#22312;&#38754;&#23545;&#26032;&#30340;&#38431;&#21451;&#26102;&#37325;&#26032;&#26657;&#20934;&#31574;&#30053;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;ProAgent&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#21019;&#24314;&#19968;&#20010;&#20855;&#26377;&#39044;&#27979;&#38431;&#21451;&#26410;&#26469;&#20915;&#31574;&#33021;&#21147;&#21644;&#20026;&#33258;&#36523;&#21046;&#23450;&#22686;&#24378;&#35745;&#21010;&#33021;&#21147;&#30340;&#20027;&#21160;&#20195;&#29702;&#12290;ProAgent&#22312;&#21512;&#20316;&#25512;&#29702;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#33021;&#22815;&#21160;&#24577;&#35843;&#25972;&#34892;&#20026;&#20197;&#22686;&#24378;&#19982;&#38431;&#21451;&#30340;&#21327;&#20316;&#21162;&#21147;&#12290;&#27492;&#22806;&#65292;ProAgent&#26694;&#26550;&#20855;&#26377;&#39640;&#24230;&#30340;&#27169;&#22359;&#21270;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#20415;&#20110;&#26080;&#32541;&#38598;&#25104;&#65292;&#20197;&#24212;&#23545;&#21508;&#31181;&#21327;&#35843;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
Building AIs with adaptive behaviors in human-AI cooperation stands as a pivotal focus in AGI research. Current methods for developing cooperative agents predominantly rely on learning-based methods, where policy generalization heavily hinges on past interactions with specific teammates. These approaches constrain the agent's capacity to recalibrate its strategy when confronted with novel teammates. We propose \textbf{ProAgent}, a novel framework that harnesses large language models (LLMs) to fashion a \textit{pro}active \textit{agent} empowered with the ability to anticipate teammates' forthcoming decisions and formulate enhanced plans for itself. ProAgent excels at cooperative reasoning with the capacity to dynamically adapt its behavior to enhance collaborative efforts with teammates. Moreover, the ProAgent framework exhibits a high degree of modularity and interpretability, facilitating seamless integration to address a wide array of coordination scenarios. Experimental evaluations
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#35843;&#26597;&#20102;&#22312;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#20013;&#24212;&#29992;&#24378;&#21270;&#23398;&#20064;&#30340;&#29616;&#29366;&#12289;&#26426;&#20250;&#21644;&#24320;&#25918;&#30740;&#31350;&#38382;&#39064;&#12290;&#20316;&#32773;&#20027;&#35201;&#35752;&#35770;&#20102;&#19977;&#31181;&#24212;&#29992;&#31867;&#22411;&#65306;&#26080;&#29305;&#23450;&#30446;&#26631;&#30340;&#29983;&#25104;&#26041;&#24335;&#12289;&#21516;&#26102;&#26368;&#22823;&#21270;&#30446;&#26631;&#20989;&#25968;&#30340;&#36755;&#20986;&#29983;&#25104;&#26041;&#24335;&#20197;&#21450;&#23558;&#26080;&#27861;&#36890;&#36807;&#30446;&#26631;&#20989;&#25968;&#25429;&#25417;&#30340;&#26399;&#26395;&#29305;&#24449;&#23884;&#20837;&#29983;&#25104;&#36807;&#31243;&#30340;&#26041;&#24335;&#12290;&#36825;&#20010;&#26032;&#20852;&#39046;&#22495;&#20013;&#23384;&#22312;&#30528;&#20016;&#23500;&#30340;&#26426;&#20250;&#21644;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2308.00031</link><description>&lt;p&gt;
&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#30340;&#24378;&#21270;&#23398;&#20064;&#65306;&#29616;&#29366;&#12289;&#26426;&#20250;&#21644;&#24320;&#25918;&#30740;&#31350;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning for Generative AI: State of the Art, Opportunities and Open Research Challenges. (arXiv:2308.00031v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00031
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#35843;&#26597;&#20102;&#22312;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#20013;&#24212;&#29992;&#24378;&#21270;&#23398;&#20064;&#30340;&#29616;&#29366;&#12289;&#26426;&#20250;&#21644;&#24320;&#25918;&#30740;&#31350;&#38382;&#39064;&#12290;&#20316;&#32773;&#20027;&#35201;&#35752;&#35770;&#20102;&#19977;&#31181;&#24212;&#29992;&#31867;&#22411;&#65306;&#26080;&#29305;&#23450;&#30446;&#26631;&#30340;&#29983;&#25104;&#26041;&#24335;&#12289;&#21516;&#26102;&#26368;&#22823;&#21270;&#30446;&#26631;&#20989;&#25968;&#30340;&#36755;&#20986;&#29983;&#25104;&#26041;&#24335;&#20197;&#21450;&#23558;&#26080;&#27861;&#36890;&#36807;&#30446;&#26631;&#20989;&#25968;&#25429;&#25417;&#30340;&#26399;&#26395;&#29305;&#24449;&#23884;&#20837;&#29983;&#25104;&#36807;&#31243;&#30340;&#26041;&#24335;&#12290;&#36825;&#20010;&#26032;&#20852;&#39046;&#22495;&#20013;&#23384;&#22312;&#30528;&#20016;&#23500;&#30340;&#26426;&#20250;&#21644;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#26159;&#36817;&#21313;&#24180;&#26469;&#35745;&#31639;&#26426;&#31185;&#23398;&#39046;&#22495;&#26368;&#20196;&#20154;&#20852;&#22859;&#30340;&#21457;&#23637;&#20043;&#19968;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#22312;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#20013;&#24050;&#32463;&#25104;&#20026;&#38750;&#24120;&#25104;&#21151;&#30340;&#33539;&#24335;&#12290;&#22312;&#26412;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#23558;RL&#24212;&#29992;&#20110;&#29983;&#25104;AI&#20013;&#30340;&#29616;&#29366;&#12289;&#26426;&#20250;&#21644;&#24320;&#25918;&#30340;&#30740;&#31350;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#35752;&#35770;&#19977;&#31181;&#24212;&#29992;&#31867;&#22411;&#65292;&#21363;&#20316;&#20026;&#19968;&#31181;&#26080;&#29305;&#23450;&#30446;&#26631;&#30340;&#29983;&#25104;&#26041;&#24335;&#65292;&#20316;&#20026;&#19968;&#31181;&#21516;&#26102;&#26368;&#22823;&#21270;&#30446;&#26631;&#20989;&#25968;&#30340;&#36755;&#20986;&#29983;&#25104;&#26041;&#24335;&#65292;&#20197;&#21450;&#20316;&#20026;&#19968;&#31181;&#23558;&#26080;&#27861;&#36890;&#36807;&#30446;&#26631;&#20989;&#25968;&#36731;&#26494;&#25429;&#25417;&#30340;&#26399;&#26395;&#29305;&#24449;&#23884;&#20837;&#29983;&#25104;&#36807;&#31243;&#30340;&#26041;&#24335;&#12290;&#25105;&#20204;&#22312;&#35843;&#26597;&#32467;&#26524;&#20013;&#23545;&#36825;&#20010;&#36855;&#20154;&#30340;&#26032;&#20852;&#39046;&#22495;&#20013;&#30340;&#26426;&#20250;&#21644;&#25361;&#25112;&#36827;&#34892;&#20102;&#28145;&#20837;&#30340;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative Artificial Intelligence (AI) is one of the most exciting developments in Computer Science of the last decade. At the same time, Reinforcement Learning (RL) has emerged as a very successful paradigm for a variety of machine learning tasks. In this survey, we discuss the state of the art, opportunities and open research questions in applying RL to generative AI. In particular, we will discuss three types of applications, namely, RL as an alternative way for generation without specified objectives; as a way for generating outputs while concurrently maximizing an objective function; and, finally, as a way of embedding desired characteristics, which cannot be easily captured by means of an objective function, into the generative process. We conclude the survey with an in-depth discussion of the opportunities and challenges in this fascinating emerging area.
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#35843;&#26597;&#20102;&#28145;&#24230;&#36801;&#31227;&#23398;&#20064;&#22312;&#24037;&#19994;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#20351;&#29992;&#12290;&#28145;&#24230;&#36801;&#31227;&#23398;&#20064;&#36890;&#36807;&#21033;&#29992;&#30456;&#20851;&#20219;&#21153;&#30340;&#30693;&#35782;&#21644;&#32771;&#34385;&#25968;&#25454;&#20998;&#24067;&#30340;&#21464;&#21270;&#65292;&#35299;&#20915;&#20102;&#20165;&#26377;&#23569;&#37327;&#25110;&#27809;&#26377;&#38468;&#21152;&#26631;&#35760;&#25968;&#25454;&#24773;&#20917;&#19979;&#30340;&#26032;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2307.05638</link><description>&lt;p&gt;
&#28145;&#24230;&#36801;&#31227;&#23398;&#20064;&#22312;&#24037;&#19994;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#32508;&#21512;&#35843;&#26597;&#65306;&#26041;&#27861;&#12289;&#24212;&#29992;&#21644;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Survey of Deep Transfer Learning for Anomaly Detection in Industrial Time Series: Methods, Applications, and Directions. (arXiv:2307.05638v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05638
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#35843;&#26597;&#20102;&#28145;&#24230;&#36801;&#31227;&#23398;&#20064;&#22312;&#24037;&#19994;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#20351;&#29992;&#12290;&#28145;&#24230;&#36801;&#31227;&#23398;&#20064;&#36890;&#36807;&#21033;&#29992;&#30456;&#20851;&#20219;&#21153;&#30340;&#30693;&#35782;&#21644;&#32771;&#34385;&#25968;&#25454;&#20998;&#24067;&#30340;&#21464;&#21270;&#65292;&#35299;&#20915;&#20102;&#20165;&#26377;&#23569;&#37327;&#25110;&#27809;&#26377;&#38468;&#21152;&#26631;&#35760;&#25968;&#25454;&#24773;&#20917;&#19979;&#30340;&#26032;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#21270;&#30417;&#27979;&#24037;&#19994;&#36807;&#31243;&#26377;&#28508;&#21147;&#36890;&#36807;&#21450;&#26102;&#26816;&#27979;&#24322;&#24120;&#20107;&#20214;&#24182;&#20419;&#36827;&#21450;&#26102;&#24178;&#39044;&#26469;&#25552;&#39640;&#25928;&#29575;&#21644;&#20248;&#21270;&#36136;&#37327;&#12290;&#28145;&#24230;&#23398;&#20064;&#36890;&#36807;&#35782;&#21035;&#22823;&#25968;&#25454;&#38598;&#20013;&#30340;&#38750;&#24179;&#20961;&#27169;&#24335;&#65292;&#22312;&#36825;&#19968;&#36807;&#31243;&#20013;&#21457;&#25381;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#26631;&#20934;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#36866;&#29992;&#20110;&#35299;&#20915;&#29305;&#23450;&#31867;&#22411;&#30340;&#25968;&#25454;&#32473;&#23450;&#29305;&#23450;&#20219;&#21153;&#30340;&#38382;&#39064;&#12290;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#36825;&#20123;&#31639;&#27861;&#38656;&#35201;&#22823;&#37327;&#30340;&#26631;&#35760;&#35757;&#32451;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#24037;&#33402;&#21644;&#29615;&#22659;&#30340;&#21160;&#24577;&#24615;&#65292;&#20026;&#27599;&#20010;&#31245;&#26377;&#19981;&#21516;&#30340;&#24773;&#20917;&#37325;&#26032;&#33719;&#24471;&#25152;&#38656;&#25968;&#25454;&#36827;&#34892;&#26631;&#20934;&#28145;&#24230;&#23398;&#20064;&#35757;&#32451;&#26159;&#19981;&#29616;&#23454;&#30340;&#12290;&#28145;&#24230;&#36801;&#31227;&#23398;&#20064;&#25552;&#20379;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#21033;&#29992;&#30456;&#20851;&#20219;&#21153;&#30340;&#30693;&#35782;&#21644;&#32771;&#34385;&#25968;&#25454;&#20998;&#24067;&#30340;&#21464;&#21270;&#65292;&#36825;&#20010;&#23398;&#20064;&#26694;&#26550;&#21487;&#20197;&#35299;&#20915;&#26032;&#20219;&#21153;&#65292;&#21363;&#20351;&#27809;&#26377;&#25110;&#21482;&#26377;&#24456;&#23569;&#30340;&#38468;&#21152;&#26631;&#35760;&#25968;&#25454;&#12290;&#36825;&#31181;&#26041;&#27861;&#36991;&#20813;&#20102;&#20174;&#22836;&#24320;&#22987;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#30340;&#38656;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automating the monitoring of industrial processes has the potential to enhance efficiency and optimize quality by promptly detecting abnormal events and thus facilitating timely interventions. Deep learning, with its capacity to discern non-trivial patterns within large datasets, plays a pivotal role in this process. Standard deep learning methods are suitable to solve a specific task given a specific type of data. During training, the algorithms demand large volumes of labeled training data. However, due to the dynamic nature of processes and the environment, it is impractical to acquire the needed data for standard deep learning training for every slightly different case anew. Deep transfer learning offers a solution to this problem. By leveraging knowledge from related tasks and accounting for variations in data distributions, this learning framework solves new tasks even with little or no additional labeled data. The approach bypasses the need to retrain a model from scratch for ev
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#21644;&#22810;&#20219;&#21153;&#30340;&#33457;&#26679;&#28369;&#20912;&#25968;&#25454;&#38598;&#65288;MMFS&#65289;&#65292;&#21253;&#21547;&#20102;256&#20010;&#31867;&#21035;&#30340;&#21160;&#20316;&#24471;&#20998;&#21644;&#31354;&#38388;&#21644;&#26102;&#38388;&#26631;&#31614;&#12290;&#35813;&#25968;&#25454;&#38598;&#30340;&#20851;&#38190;&#36129;&#29486;&#26159;&#39318;&#27425;&#24341;&#20837;&#20102;&#29420;&#31435;&#30340;&#31354;&#38388;&#21644;&#26102;&#38388;&#20998;&#31867;&#65292;&#24182;&#39318;&#27425;&#20351;&#29992;&#39592;&#39612;&#27169;&#24577;&#36827;&#34892;&#31934;&#32454;&#21160;&#20316;&#36136;&#37327;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2307.02730</link><description>&lt;p&gt;
&#31934;&#32454;&#21160;&#20316;&#20998;&#26512;&#65306;&#19968;&#31181;&#22810;&#27169;&#24577;&#21644;&#22810;&#20219;&#21153;&#30340;&#33457;&#26679;&#28369;&#20912;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Fine-grained Action Analysis: A Multi-modality and Multi-task Dataset of Figure Skating. (arXiv:2307.02730v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02730
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#21644;&#22810;&#20219;&#21153;&#30340;&#33457;&#26679;&#28369;&#20912;&#25968;&#25454;&#38598;&#65288;MMFS&#65289;&#65292;&#21253;&#21547;&#20102;256&#20010;&#31867;&#21035;&#30340;&#21160;&#20316;&#24471;&#20998;&#21644;&#31354;&#38388;&#21644;&#26102;&#38388;&#26631;&#31614;&#12290;&#35813;&#25968;&#25454;&#38598;&#30340;&#20851;&#38190;&#36129;&#29486;&#26159;&#39318;&#27425;&#24341;&#20837;&#20102;&#29420;&#31435;&#30340;&#31354;&#38388;&#21644;&#26102;&#38388;&#20998;&#31867;&#65292;&#24182;&#39318;&#27425;&#20351;&#29992;&#39592;&#39612;&#27169;&#24577;&#36827;&#34892;&#31934;&#32454;&#21160;&#20316;&#36136;&#37327;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#21160;&#20316;&#25968;&#25454;&#38598;&#30340;&#31934;&#32454;&#21160;&#20316;&#20998;&#26512;&#38754;&#20020;&#30528;&#21160;&#20316;&#31867;&#21035;&#19981;&#36275;&#12289;&#32454;&#31890;&#24230;&#20302;&#12289;&#27169;&#24577;&#21644;&#20219;&#21153;&#26377;&#38480;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#22810;&#20219;&#21153;&#30340;&#33457;&#26679;&#28369;&#20912;&#25968;&#25454;&#38598;&#65288;MMFS&#65289;&#65292;&#35813;&#25968;&#25454;&#38598;&#26159;&#20174;&#19990;&#30028;&#33457;&#26679;&#28369;&#20912;&#38182;&#26631;&#36187;&#20013;&#25910;&#38598;&#32780;&#26469;&#30340;&#12290;MMFS&#21253;&#25324;&#21160;&#20316;&#35782;&#21035;&#21644;&#21160;&#20316;&#36136;&#37327;&#35780;&#20272;&#65292;&#20351;&#29992;RGB&#12289;&#39592;&#39612;&#21644;11671&#20010;&#35270;&#39057;&#29255;&#27573;&#37319;&#38598;&#20102;256&#20010;&#31867;&#21035;&#30340;&#21160;&#20316;&#24471;&#20998;&#65292;&#24182;&#21253;&#21547;&#20102;&#31354;&#38388;&#21644;&#26102;&#38388;&#26631;&#31614;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#30340;&#20851;&#38190;&#36129;&#29486;&#21253;&#25324;&#20197;&#19979;&#19977;&#20010;&#26041;&#38754;&#65306;&#65288;1&#65289;&#39318;&#27425;&#25552;&#20986;&#20102;&#29420;&#31435;&#30340;&#31354;&#38388;&#21644;&#26102;&#38388;&#20998;&#31867;&#65292;&#20197;&#36827;&#19968;&#27493;&#25506;&#32034;&#31934;&#32454;&#21160;&#20316;&#35782;&#21035;&#21644;&#36136;&#37327;&#35780;&#20272;&#12290;(2)MMFS&#39318;&#27425;&#24341;&#20837;&#20102;&#39592;&#39612;&#27169;&#24577;&#29992;&#20110;&#22797;&#26434;&#30340;&#31934;&#32454;&#21160;&#20316;&#36136;&#37327;&#35780;&#20272;&#12290;(3)&#25105;&#20204;&#30340;&#22810;&#27169;&#24577;&#21644;&#22810;&#20219;&#21153;&#25968;&#25454;&#38598;&#40723;&#21169;&#26356;&#22810;&#30340;&#21160;&#20316;&#20998;&#26512;&#27169;&#22411;&#12290;&#20026;&#20102;&#35780;&#20272;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#22522;&#20110;RGB&#21644;&#22522;&#20110;&#39592;&#39612;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The fine-grained action analysis of the existing action datasets is challenged by insufficient action categories, low fine granularities, limited modalities, and tasks. In this paper, we propose a Multi-modality and Multi-task dataset of Figure Skating (MMFS) which was collected from the World Figure Skating Championships. MMFS, which possesses action recognition and action quality assessment, captures RGB, skeleton, and is collected the score of actions from 11671 clips with 256 categories including spatial and temporal labels. The key contributions of our dataset fall into three aspects as follows. (1) Independently spatial and temporal categories are first proposed to further explore fine-grained action recognition and quality assessment. (2) MMFS first introduces the skeleton modality for complex fine-grained action quality assessment. (3) Our multi-modality and multi-task dataset encourage more action analysis models. To benchmark our dataset, we adopt RGB-based and skeleton-based
&lt;/p&gt;</description></item><item><title>TL-nvSRAM-CIM&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#23384;&#20648;&#22120;&#20869;&#35745;&#31639;&#26041;&#26696;&#65292;&#21033;&#29992;&#36229;&#39640;&#23494;&#24230;&#30340;&#19977;&#32423;ReRAM&#36741;&#21161;&#35745;&#31639;&#26469;&#35299;&#20915;&#22823;&#35268;&#27169;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20013;&#26435;&#37325;&#23481;&#37327;&#19981;&#36275;&#30340;&#38382;&#39064;&#65292;&#24182;&#37319;&#29992;&#20102;&#38646;&#30452;&#27969;&#21151;&#32791;&#24674;&#22797;&#21644;&#19977;&#24577;MAC&#25805;&#20316;&#26469;&#25552;&#39640;&#33021;&#25928;&#21644;&#20445;&#25345;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.02717</link><description>&lt;p&gt;
TL-nvSRAM-CIM&#65306;&#20855;&#26377;&#38646;&#30452;&#27969;&#21151;&#32791;&#24674;&#22797;&#21644;&#19977;&#24577;MAC&#25805;&#20316;&#30340;&#36229;&#39640;&#23494;&#24230;&#19977;&#32423;ReRAM&#36741;&#21161;&#35745;&#31639;&#23384;&#20648;&#22120;&#20869;&#35745;&#31639;
&lt;/p&gt;
&lt;p&gt;
TL-nvSRAM-CIM: Ultra-High-Density Three-Level ReRAM-Assisted Computing-in-nvSRAM with DC-Power Free Restore and Ternary MAC Operations. (arXiv:2307.02717v1 [cs.AR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02717
&lt;/p&gt;
&lt;p&gt;
TL-nvSRAM-CIM&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#23384;&#20648;&#22120;&#20869;&#35745;&#31639;&#26041;&#26696;&#65292;&#21033;&#29992;&#36229;&#39640;&#23494;&#24230;&#30340;&#19977;&#32423;ReRAM&#36741;&#21161;&#35745;&#31639;&#26469;&#35299;&#20915;&#22823;&#35268;&#27169;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20013;&#26435;&#37325;&#23481;&#37327;&#19981;&#36275;&#30340;&#38382;&#39064;&#65292;&#24182;&#37319;&#29992;&#20102;&#38646;&#30452;&#27969;&#21151;&#32791;&#24674;&#22797;&#21644;&#19977;&#24577;MAC&#25805;&#20316;&#26469;&#25552;&#39640;&#33021;&#25928;&#21644;&#20445;&#25345;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#35268;&#27169;&#24222;&#22823;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#23558;&#25152;&#26377;&#26435;&#37325;&#25918;&#22312;&#33455;&#29255;&#19978;&#20173;&#28982;&#26159;SRAM-CIM&#30340;&#19968;&#22823;&#25361;&#25112;&#65292;&#22240;&#20026;&#20854;&#23384;&#20648;&#23481;&#37327;&#26377;&#38480;&#12290;&#20043;&#21069;&#30340;&#38750;&#26131;&#22833;&#24615;SRAM-CIM&#65288;nvSRAM-CIM&#65289;&#36890;&#36807;&#22312;&#39640;&#25928;SRAM-CIM&#19978;&#38598;&#25104;&#39640;&#23494;&#24230;&#21333;&#32423;ReRAM&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#22312;&#33455;&#29255;&#19978;&#20648;&#23384;&#26435;&#37325;&#20197;&#28040;&#38500;&#33455;&#29255;&#22806;&#30340;&#20869;&#23384;&#35775;&#38382;&#12290;&#28982;&#32780;&#65292;&#20043;&#21069;&#30340;SL-nvSRAM-CIM&#22312;&#22686;&#21152;SL-ReRAM&#25968;&#37327;&#21644;&#35745;&#31639;&#25928;&#29575;&#21463;&#38480;&#26041;&#38754;&#23384;&#22312;&#38382;&#39064;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36229;&#39640;&#23494;&#24230;&#19977;&#32423;ReRAM&#36741;&#21161;&#30340;&#38750;&#26131;&#22833;&#24615;SRAM&#35745;&#31639;&#23384;&#20648;&#22120;&#20869;&#35745;&#31639;&#26041;&#26696;&#65288;TL-nvSRAM-CIM&#65289;&#29992;&#20110;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#12290;&#37319;&#29992;&#20102;&#38598;&#32676;&#24335;&#36873;&#25321;&#22120;-n-ReRAM&#65288;cluster-nSnRs&#65289;&#26469;&#21487;&#38752;&#22320;&#24674;&#22797;&#26435;&#37325;&#65292;&#24182;&#28040;&#38500;&#20102;&#30452;&#27969;&#21151;&#32791;&#12290;&#27492;&#22806;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#24046;&#20998;&#35745;&#31639;&#26041;&#26696;&#30340;&#19977;&#24577;SRAM-CIM&#26426;&#21046;&#65292;&#29992;&#20110;&#33021;&#37327;&#39640;&#25928;&#30340;&#19977;&#24577;MAC&#25805;&#20316;&#65292;&#24182;&#20445;&#25345;&#39640;NN&#20934;&#30830;&#24615;&#12290;&#25552;&#20986;&#30340;TL-nvSRAM-CIM&#23454;&#29616;&#20102;7.
&lt;/p&gt;
&lt;p&gt;
Accommodating all the weights on-chip for large-scale NNs remains a great challenge for SRAM based computing-in-memory (SRAM-CIM) with limited on-chip capacity. Previous non-volatile SRAM-CIM (nvSRAM-CIM) addresses this issue by integrating high-density single-level ReRAMs on the top of high-efficiency SRAM-CIM for weight storage to eliminate the off-chip memory access. However, previous SL-nvSRAM-CIM suffers from poor scalability for an increased number of SL-ReRAMs and limited computing efficiency. To overcome these challenges, this work proposes an ultra-high-density three-level ReRAMs-assisted computing-in-nonvolatile-SRAM (TL-nvSRAM-CIM) scheme for large NN models. The clustered n-selector-n-ReRAM (cluster-nSnRs) is employed for reliable weight-restore with eliminated DC power. Furthermore, a ternary SRAM-CIM mechanism with differential computing scheme is proposed for energy-efficient ternary MAC operations while preserving high NN accuracy. The proposed TL-nvSRAM-CIM achieves 7.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#21160;&#24577;&#39046;&#22495;&#30693;&#35782;&#30340;&#33647;&#29289;&#25512;&#33616;&#26694;&#26550;DKINet&#65292;&#23558;&#39046;&#22495;&#30693;&#35782;&#19982;&#24739;&#32773;&#20020;&#24202;&#34920;&#29616;&#30456;&#32467;&#21512;&#65292;&#27492;&#20026;&#39318;&#27425;&#23454;&#39564;&#12290;</title><link>http://arxiv.org/abs/2305.19604</link><description>&lt;p&gt;
&#36890;&#36807;&#39046;&#22495;&#30693;&#35782;&#21551;&#31034;&#30340;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#33647;&#29289;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Medication Recommendation via Domain Knowledge Informed Deep Learning. (arXiv:2305.19604v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19604
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#21160;&#24577;&#39046;&#22495;&#30693;&#35782;&#30340;&#33647;&#29289;&#25512;&#33616;&#26694;&#26550;DKINet&#65292;&#23558;&#39046;&#22495;&#30693;&#35782;&#19982;&#24739;&#32773;&#20020;&#24202;&#34920;&#29616;&#30456;&#32467;&#21512;&#65292;&#27492;&#20026;&#39318;&#27425;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33647;&#29289;&#25512;&#33616;&#26159;&#21307;&#30103;&#20445;&#20581;&#30340;&#22522;&#26412;&#20294;&#33267;&#20851;&#37325;&#35201;&#30340;&#20998;&#25903;&#65292;&#25552;&#20379;&#26426;&#20250;&#20026;&#22797;&#26434;&#20581;&#24247;&#29366;&#20917;&#30340;&#24739;&#32773;&#25903;&#25345;&#20020;&#24202;&#21307;&#29983;&#26356;&#31934;&#30830;&#30340;&#33647;&#29289;&#22788;&#26041;&#12290;&#20174;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#20013;&#23398;&#20064;&#25512;&#33616;&#33647;&#29289;&#26159;&#20808;&#21069;&#30740;&#31350;&#20013;&#26368;&#24120;&#35265;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#30740;&#31350;&#24573;&#35270;&#20102;&#26681;&#25454;&#24739;&#32773;&#30340;EHR&#20013;&#30340;&#20020;&#24202;&#34920;&#29616;&#32435;&#20837;&#39046;&#22495;&#30693;&#35782;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#21160;&#24577;&#39046;&#22495;&#30693;&#35782;&#30340;&#33647;&#29289;&#25512;&#33616;&#26694;&#26550;&#65292;&#21363;&#39046;&#22495;&#30693;&#35782;&#21551;&#31034;&#32593;&#32476;&#65288;DKINet&#65289;&#65292;&#29992;&#20110;&#23558;&#39046;&#22495;&#30693;&#35782;&#19982;&#21487;&#35266;&#23519;&#30340;&#24739;&#32773;&#20020;&#24202;&#34920;&#29616;&#30456;&#32467;&#21512;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#39318;&#20808;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20110;&#39046;&#22495;&#30693;&#35782;&#30340;&#32534;&#30721;&#22120;&#26469;&#25429;&#25417;&#39046;&#22495;&#20449;&#24687;&#65292;&#28982;&#21518;&#24320;&#21457;&#20102;&#19968;&#20010;&#25968;&#25454;&#39537;&#21160;&#30340;&#32534;&#30721;&#22120;&#23558;&#39046;&#22495;&#30693;&#35782;&#25972;&#21512;&#21040;&#21487;&#35266;&#23519;&#30340;EHR&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Medication recommendation is a fundamental yet crucial branch of healthcare, which provides opportunities to support clinical physicians with more accurate medication prescriptions for patients with complex health conditions. Learning from electronic health records (EHR) to recommend medications is the most common way in previous studies. However, most of them neglect incorporating domain knowledge according to the clinical manifestations in the EHR of the patient. To address these issues, we propose a novel \textbf{D}omain \textbf{K}nowledge \textbf{I}nformed \textbf{Net}work (DKINet) to integrate domain knowledge with observable clinical manifestations of the patient, which is the first dynamic domain knowledge informed framework toward medication recommendation. In particular, we first design a knowledge-driven encoder to capture the domain information and then develop a data-driven encoder to integrate domain knowledge into the observable EHR. To endow the model with the capability
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#23545;&#40784;&#35780;&#20272;&#26041;&#27861;A2EHV&#65292;&#37319;&#29992;&#24322;&#36136;&#20215;&#20540;&#31995;&#32479;&#65292;&#24182;&#22522;&#20110;&#20215;&#20540;&#21512;&#29702;&#24615;&#21644;&#31038;&#20250;&#20215;&#20540;&#23450;&#21521;&#26694;&#26550;&#35780;&#20272;&#20195;&#29702;&#20154;&#34892;&#20026;&#30340;&#31038;&#20250;&#20559;&#22909;&#65292;&#32467;&#26524;&#34920;&#26126;&#27604;&#20256;&#32479;&#23545;&#40784;&#26041;&#27861;&#26356;&#21512;&#29702;&#12290;</title><link>http://arxiv.org/abs/2305.17147</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24322;&#36136;&#20215;&#20540;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Heterogeneous Value Evaluation for Large Language Models. (arXiv:2305.17147v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17147
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#23545;&#40784;&#35780;&#20272;&#26041;&#27861;A2EHV&#65292;&#37319;&#29992;&#24322;&#36136;&#20215;&#20540;&#31995;&#32479;&#65292;&#24182;&#22522;&#20110;&#20215;&#20540;&#21512;&#29702;&#24615;&#21644;&#31038;&#20250;&#20215;&#20540;&#23450;&#21521;&#26694;&#26550;&#35780;&#20272;&#20195;&#29702;&#20154;&#34892;&#20026;&#30340;&#31038;&#20250;&#20559;&#22909;&#65292;&#32467;&#26524;&#34920;&#26126;&#27604;&#20256;&#32479;&#23545;&#40784;&#26041;&#27861;&#26356;&#21512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#20986;&#29616;&#20351;&#24471;&#23558;&#23427;&#20204;&#30340;&#20215;&#20540;&#19982;&#20154;&#31867;&#20215;&#20540;&#23545;&#40784;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#24403;&#21069;&#30340;&#26041;&#27861;&#36890;&#24120;&#23581;&#35797;&#23558;&#20854;&#19982;&#19968;&#31181;&#21516;&#36136;&#30340;&#20154;&#31867;&#20215;&#20540;&#23545;&#40784;&#65292;&#24182;&#38656;&#35201;&#20154;&#31867;&#39564;&#35777;&#65292;&#20294;&#32570;&#20047;&#23545;&#23545;&#40784;&#25152;&#38656;&#26041;&#38754;&#21644;&#28145;&#24230;&#30340;&#20849;&#35782;&#20197;&#21450;&#36896;&#25104;&#30340;&#20154;&#31867;&#20559;&#35265;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#23545;&#40784;&#35780;&#20272;&#26041;&#27861;A2EHV&#65292;&#35813;&#26041;&#27861;&#37319;&#29992;&#24322;&#36136;&#20215;&#20540;&#31995;&#32479;&#65292;&#65288;1&#65289;&#26159;&#33258;&#21160;&#21270;&#30340;&#65292;&#20197;&#26368;&#23567;&#21270;&#21333;&#20010;&#20154;&#31867;&#20559;&#35265;&#65292;&#24182;&#19988;&#65288;2&#65289;&#20801;&#35768;&#35780;&#20272;&#38024;&#23545;&#21508;&#31181;&#30446;&#26631;&#20540;&#30340;&#24322;&#36136;&#20195;&#29702;&#20154;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#20215;&#20540;&#21512;&#29702;&#24615;&#30340;&#27010;&#24565;&#65292;&#23427;&#20195;&#34920;&#20102;&#20195;&#29702;&#20154;&#25191;&#34892;&#26368;&#33021;&#28385;&#36275;&#30446;&#26631;&#20215;&#20540;&#34892;&#20026;&#30340;&#33021;&#21147;&#12290;&#20215;&#20540;&#21512;&#29702;&#24615;&#30340;&#37327;&#21270;&#26159;&#36890;&#36807;&#31038;&#20250;&#24515;&#29702;&#23398;&#20013;&#30340;&#31038;&#20250;&#20215;&#20540;&#23450;&#21521;&#26694;&#26550;&#36827;&#34892;&#30340;&#65292;&#35813;&#26694;&#26550;&#23558;&#20215;&#20540;&#31354;&#38388;&#20998;&#20026;&#22235;&#20010;&#31867;&#21035;&#65292;&#20197;&#35780;&#20272;&#20195;&#29702;&#20154;&#34892;&#20026;&#30340;&#31038;&#20250;&#20559;&#22909;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#19977;&#20010;&#27169;&#22411;&#30340;&#20215;&#20540;&#21512;&#29702;&#24615;&#65292;&#32467;&#26524;&#34920;&#26126;A2EHV&#26041;&#27861;&#27604;&#20256;&#32479;&#23545;&#40784;&#26041;&#27861;&#26356;&#21512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
The emergent capabilities of Large Language Models (LLMs) have made it crucial to align their values with those of humans. Current methodologies typically attempt alignment with a homogeneous human value and requires human verification, yet lack consensus on the desired aspect and depth of alignment and resulting human biases. In this paper, we propose A2EHV, an Automated Alignment Evaluation with a Heterogeneous Value system that (1) is automated to minimize individual human biases, and (2) allows assessments against various target values to foster heterogeneous agents. Our approach pivots on the concept of value rationality, which represents the ability for agents to execute behaviors that satisfy a target value the most. The quantification of value rationality is facilitated by the Social Value Orientation framework from social psychology, which partitions the value space into four categories to assess social preferences from agents' behaviors. We evaluate the value rationality of e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#31995;&#32479;&#20998;&#31867;&#20307;&#31995;&#65292;&#20998;&#31867;&#21644;&#27604;&#36739;&#20102;&#22522;&#30784;&#27169;&#22411;&#21644;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#31995;&#32479;&#30340;&#29305;&#28857;&#12290;&#23427;&#20026;&#35774;&#35745;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#31995;&#32479;&#26102;&#20570;&#20986;&#20027;&#35201;&#30340;&#35774;&#35745;&#20915;&#31574;&#25552;&#20379;&#20102;&#20855;&#20307;&#30340;&#25351;&#23548;&#65292;&#24182;&#31361;&#20986;&#20102;&#30456;&#20851;&#30340;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2305.05352</link><description>&lt;p&gt;
&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#31995;&#32479;&#35774;&#35745;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Framework for Designing Foundation Model based Systems. (arXiv:2305.05352v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05352
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#31995;&#32479;&#20998;&#31867;&#20307;&#31995;&#65292;&#20998;&#31867;&#21644;&#27604;&#36739;&#20102;&#22522;&#30784;&#27169;&#22411;&#21644;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#31995;&#32479;&#30340;&#29305;&#28857;&#12290;&#23427;&#20026;&#35774;&#35745;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#31995;&#32479;&#26102;&#20570;&#20986;&#20027;&#35201;&#30340;&#35774;&#35745;&#20915;&#31574;&#25552;&#20379;&#20102;&#20855;&#20307;&#30340;&#25351;&#23548;&#65292;&#24182;&#31361;&#20986;&#20102;&#30456;&#20851;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#25512;&#20986;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#22914;ChatGPT&#65292;&#36825;&#24341;&#36215;&#20102;&#20154;&#20204;&#23545;&#22522;&#30784;&#27169;&#22411;&#30340;&#24191;&#27867;&#20851;&#27880;&#12290;&#22522;&#30784;&#27169;&#22411;&#34987;&#24191;&#27867;&#35748;&#20026;&#23558;&#25104;&#20026;&#26410;&#26469;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#22522;&#30707;&#12290;&#30001;&#20110;&#22522;&#30784;&#27169;&#22411;&#22788;&#20110;&#26089;&#26399;&#38454;&#27573;&#65292;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#31995;&#32479;&#35774;&#35745;&#23578;&#26410;&#24471;&#21040;&#31995;&#32479;&#22320;&#25506;&#32034;&#12290;&#20154;&#20204;&#23545;&#22312;&#36719;&#20214;&#26550;&#26500;&#20013;&#24341;&#20837;&#22522;&#30784;&#27169;&#22411;&#30340;&#24433;&#21709;&#30693;&#20043;&#29978;&#23569;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#31995;&#32479;&#20998;&#31867;&#27861;&#65292;&#23545;&#22522;&#30784;&#27169;&#22411;&#21644;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#31995;&#32479;&#30340;&#29305;&#28857;&#36827;&#34892;&#20102;&#20998;&#31867;&#21644;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#20998;&#31867;&#27861;&#21253;&#25324;&#19977;&#20010;&#31867;&#21035;&#65306;&#22522;&#30784;&#27169;&#22411;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#12289;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#31995;&#32479;&#26550;&#26500;&#35774;&#35745;&#21644;&#36127;&#36131;&#20219;&#30340;AI&#35774;&#35745;&#12290;&#36825;&#20010;&#20998;&#31867;&#27861;&#20026;&#35774;&#35745;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#31995;&#32479;&#26102;&#20570;&#20986;&#20027;&#35201;&#30340;&#35774;&#35745;&#20915;&#31574;&#25552;&#20379;&#20102;&#20855;&#20307;&#30340;&#25351;&#23548;&#65292;&#24182;&#31361;&#20986;&#20102;&#30456;&#20851;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent release of large language model (LLM) based chatbots, such as ChatGPT, has attracted significant attention on foundations models. It is widely believed that foundation models will serve as the fundamental building blocks for future AI systems. As foundation models are in their early stages, the design of foundation model based systems has not yet been systematically explored. There is little understanding about the impact of introducing foundation models in software architecture. Therefore, in this paper, we propose a taxonomy of foundation model based systems, which classifies and compares the characteristics of foundation models and foundation model based systems. Our taxonomy comprises three categories: foundation model pretraining and fine-tuning, architecture design of foundation model based systems, and responsible-AI-by-design. This taxonomy provides concrete guidance for making major design decisions when designing foundation model based systems and highlights trade-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#32452;&#21512;&#32467;&#26500;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23884;&#20837;&#31354;&#38388;&#20013;&#36739;&#23567;&#38598;&#21512;&#30340;&#21521;&#37327;&#32452;&#21512;&#26469;&#36817;&#20284;&#34920;&#31034;&#26469;&#33258;&#32534;&#30721;&#22120;&#30340;&#34920;&#31034;&#24418;&#24335;&#30340;&#26041;&#27861;&#65292;&#23558;&#36825;&#20123;&#21521;&#37327;&#35270;&#20026;&#8220;&#29702;&#24819;&#21333;&#35789;&#8221;&#65292;&#24182;&#22312;CLIP&#30340;&#23884;&#20837;&#20013;&#20197;&#23454;&#39564;&#26041;&#24335;&#25506;&#32034;&#20102;&#36825;&#20123;&#32467;&#26500;&#30340;&#21487;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.14383</link><description>&lt;p&gt;
&#24847;&#20041;&#30340;&#32447;&#24615;&#31354;&#38388;&#65306;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#32452;&#21512;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
Linear Spaces of Meanings: Compositional Structures in Vision-Language Models. (arXiv:2302.14383v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.14383
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#32452;&#21512;&#32467;&#26500;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23884;&#20837;&#31354;&#38388;&#20013;&#36739;&#23567;&#38598;&#21512;&#30340;&#21521;&#37327;&#32452;&#21512;&#26469;&#36817;&#20284;&#34920;&#31034;&#26469;&#33258;&#32534;&#30721;&#22120;&#30340;&#34920;&#31034;&#24418;&#24335;&#30340;&#26041;&#27861;&#65292;&#23558;&#36825;&#20123;&#21521;&#37327;&#35270;&#20026;&#8220;&#29702;&#24819;&#21333;&#35789;&#8221;&#65292;&#24182;&#22312;CLIP&#30340;&#23884;&#20837;&#20013;&#20197;&#23454;&#39564;&#26041;&#24335;&#25506;&#32034;&#20102;&#36825;&#20123;&#32467;&#26500;&#30340;&#21487;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#39044;&#35757;&#32451;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLM&#65289;&#20013;&#30340;&#25968;&#25454;&#23884;&#20837;&#30340;&#32452;&#21512;&#32467;&#26500;&#12290;&#20256;&#32479;&#19978;&#65292;&#32452;&#21512;&#24615;&#19982;&#39044;&#20808;&#23384;&#22312;&#30340;&#35789;&#27719;&#34920;&#20013;&#30340;&#21333;&#35789;&#23884;&#20837;&#30340;&#20195;&#25968;&#36816;&#31639;&#26377;&#20851;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#35797;&#22270;&#20351;&#29992;&#23884;&#20837;&#31354;&#38388;&#20013;&#36739;&#23567;&#38598;&#21512;&#30340;&#21521;&#37327;&#32452;&#21512;&#26469;&#36817;&#20284;&#34920;&#31034;&#26469;&#33258;&#32534;&#30721;&#22120;&#30340;&#34920;&#31034;&#24418;&#24335;&#12290;&#36825;&#20123;&#21521;&#37327;&#21487;&#20197;&#34987;&#30475;&#20316;&#26159;&#22312;&#27169;&#22411;&#30340;&#23884;&#20837;&#31354;&#38388;&#20013;&#30452;&#25509;&#29983;&#25104;&#27010;&#24565;&#30340;&#8220;&#29702;&#24819;&#21333;&#35789;&#8221;&#12290;&#25105;&#20204;&#39318;&#20808;&#20174;&#20960;&#20309;&#23398;&#30340;&#35282;&#24230;&#25552;&#20986;&#20102;&#29702;&#35299;&#32452;&#21512;&#32467;&#26500;&#30340;&#26694;&#26550;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35299;&#37322;&#20102;VLM&#23884;&#20837;&#22312;&#27010;&#29575;&#19978;&#30340;&#36825;&#20123;&#32452;&#21512;&#32467;&#26500;&#30340;&#21547;&#20041;&#65292;&#24182;&#25552;&#20379;&#20102;&#23427;&#20204;&#22312;&#23454;&#36341;&#20013;&#20135;&#29983;&#30340;&#30452;&#35273;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#22312;CLIP&#30340;&#23884;&#20837;&#20013;&#20197;&#23454;&#39564;&#26041;&#24335;&#25506;&#32034;&#20102;&#36825;&#20123;&#32467;&#26500;&#65292;&#24182;&#35780;&#20272;&#20102;&#23427;&#20204;&#22312;&#35299;&#20915;&#20998;&#31867;&#12289;&#21435;&#20559;&#21644;&#26816;&#32034;&#31561;&#19981;&#21516;&#35270;&#35273;&#35821;&#35328;&#20219;&#21153;&#20013;&#30340;&#26377;&#29992;&#24615;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#23884;&#20837;&#31354;&#38388;&#20013;&#31616;&#21333;&#30340;&#32447;&#24615;&#20195;&#25968;&#36816;&#31639;&#21487;&#20197;&#23454;&#29616;&#19982;&#26356;&#22797;&#26434;&#30340;&#26041;&#27861;&#30456;&#23218;&#32654;&#29978;&#33267;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#24847;&#20041;&#30340;&#32447;&#24615;&#31354;&#38388;&#30340;&#26377;&#25928;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate compositional structures in data embeddings from pre-trained vision-language models (VLMs). Traditionally, compositionality has been associated with algebraic operations on embeddings of words from a pre-existing vocabulary. In contrast, we seek to approximate representations from an encoder as combinations of a smaller set of vectors in the embedding space. These vectors can be seen as "ideal words" for generating concepts directly within the embedding space of the model. We first present a framework for understanding compositional structures from a geometric perspective. We then explain what these compositional structures entail probabilistically in the case of VLM embeddings, providing intuitions for why they arise in practice. Finally, we empirically explore these structures in CLIP's embeddings and we evaluate their usefulness for solving different vision-language tasks such as classification, debiasing, and retrieval. Our results show that simple linear algebraic o
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25581;&#31034;&#20102;&#26426;&#22120;&#36951;&#24536;&#26041;&#27861;&#22312;&#20844;&#24179;&#24615;&#26041;&#38754;&#30340;&#24433;&#21709;&#65292;&#24110;&#21161;&#36719;&#20214;&#24037;&#31243;&#24072;&#20570;&#20986;&#36127;&#36131;&#20219;&#30340;&#20915;&#31574;&#12290;</title><link>http://arxiv.org/abs/2302.03350</link><description>&lt;p&gt;
&#34987;&#36951;&#24536;&#36824;&#26159;&#34987;&#20844;&#24179;&#23545;&#24453;&#65306;&#25581;&#31034;&#26426;&#22120;&#36951;&#24536;&#26041;&#27861;&#30340;&#20844;&#24179;&#24615;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
To Be Forgotten or To Be Fair: Unveiling Fairness Implications of Machine Unlearning Methods. (arXiv:2302.03350v2 [cs.SE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.03350
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25581;&#31034;&#20102;&#26426;&#22120;&#36951;&#24536;&#26041;&#27861;&#22312;&#20844;&#24179;&#24615;&#26041;&#38754;&#30340;&#24433;&#21709;&#65292;&#24110;&#21161;&#36719;&#20214;&#24037;&#31243;&#24072;&#20570;&#20986;&#36127;&#36131;&#20219;&#30340;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20010;&#20154;&#20449;&#24687;&#34987;&#36951;&#24536;&#30340;&#26435;&#21033;&#26159;&#30001;&#20154;&#20204;&#19981;&#24076;&#26395;&#33258;&#24049;&#30340;&#36807;&#21435;&#34892;&#20026;&#27704;&#20037;&#22320;&#32473;&#20104;&#19981;&#21033;&#24433;&#21709;&#30340;&#24895;&#26395;&#25152;&#25512;&#21160;&#30340;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25968;&#25454;&#21024;&#38500;&#38656;&#35201;&#24443;&#24213;&#21644;&#27704;&#20037;&#65292;&#36824;&#24517;&#39035;&#20174;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#21024;&#38500;&#12290;&#30740;&#31350;&#20154;&#21592;&#25552;&#20986;&#20102;&#26426;&#22120;&#36951;&#24536;&#31639;&#27861;&#65292;&#26088;&#22312;&#26356;&#39640;&#25928;&#22320;&#20174;&#35757;&#32451;&#27169;&#22411;&#20013;&#21024;&#38500;&#29305;&#23450;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#25913;&#21464;&#20102;&#25968;&#25454;&#36755;&#20837;&#27169;&#22411;&#21644;&#35757;&#32451;&#26041;&#24335;&#65292;&#21487;&#33021;&#20174;&#20844;&#24179;&#24615;&#30340;&#35282;&#24230;&#22949;&#21327;&#20102;&#20154;&#24037;&#26234;&#33021;&#20262;&#29702;&#12290;&#20026;&#20102;&#24110;&#21161;&#36719;&#20214;&#24037;&#31243;&#24072;&#22312;&#37319;&#29992;&#36825;&#20123;&#36951;&#24536;&#26041;&#27861;&#26102;&#20570;&#20986;&#36127;&#36131;&#20219;&#30340;&#20915;&#31574;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#39318;&#20010;&#26426;&#22120;&#36951;&#24536;&#26041;&#27861;&#30340;&#20844;&#24179;&#24615;&#24433;&#21709;&#30740;&#31350;&#12290;&#25105;&#20204;&#35774;&#35745;&#24182;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#20351;&#29992;&#19977;&#20010;&#20844;&#24179;&#24615;&#25968;&#25454;&#38598;&#21644;&#19977;&#31181;&#19981;&#21516;&#30340;&#21024;&#38500;&#31574;&#30053;&#65292;&#23545;&#20004;&#31181;&#20856;&#22411;&#30340;&#26426;&#22120;&#36951;&#24536;&#26041;&#27861;&#65288;SISA&#21644;AmnesiacML&#65289;&#20197;&#21450;&#37325;&#26032;&#35757;&#32451;&#26041;&#27861;&#65288;ORTR&#65289;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#38750;&#22343;&#21248;&#25968;&#25454;&#21024;&#38500;&#30340;&#24773;&#20917;&#19979;&#65292;&#26426;&#22120;&#36951;&#24536;&#26041;&#27861;&#21487;&#33021;&#23545;&#20844;&#24179;&#24615;&#20135;&#29983;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
The right to be forgotten (RTBF) is motivated by the desire of people not to be perpetually disadvantaged by their past deeds. For this, data deletion needs to be deep and permanent, and should be removed from machine learning models. Researchers have proposed machine unlearning algorithms which aim to erase specific data from trained models more efficiently. However, these methods modify how data is fed into the model and how training is done, which may subsequently compromise AI ethics from the fairness perspective. To help software engineers make responsible decisions when adopting these unlearning methods, we present the first study on machine unlearning methods to reveal their fairness implications. We designed and conducted experiments on two typical machine unlearning methods (SISA and AmnesiacML) along with a retraining method (ORTR) as baseline using three fairness datasets under three different deletion strategies. Experimental results show that under non-uniform data deletio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20851;&#20110;&#26410;&#26469;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30740;&#31350;&#21644;&#21457;&#23637;&#30340;&#24895;&#26223;&#65292;&#35813;&#24895;&#26223;&#26159;&#22522;&#20110;&#20027;&#21160;&#25512;&#29702;&#21644;&#33258;&#35777;&#26126;&#30340;&#26234;&#33021;&#29983;&#24577;&#31995;&#32479;&#65292;&#20854;&#20013;&#20154;&#31867;&#20316;&#20026;&#20849;&#20139;&#26234;&#33021;&#30340;&#19981;&#21487;&#25110;&#32570;&#30340;&#21442;&#19982;&#32773;&#12290;&#35813;&#24895;&#26223;&#26088;&#22312;&#26368;&#22823;&#21270;&#27169;&#22411;&#35777;&#25454;&#65292;&#36890;&#36807;&#19981;&#21516;&#23610;&#24230;&#19978;&#30340;&#20449;&#24565;&#26356;&#26032;&#26469;&#32047;&#31215;&#20851;&#20110;&#33258;&#36523;&#24863;&#30693;&#19990;&#30028;&#30340;&#35777;&#25454;&#12290;</title><link>http://arxiv.org/abs/2212.01354</link><description>&lt;p&gt;
&#20174;&#31532;&#19968;&#21407;&#29702;&#35774;&#35745;&#26234;&#33021;&#29983;&#24577;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Designing Ecosystems of Intelligence from First Principles. (arXiv:2212.01354v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.01354
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20851;&#20110;&#26410;&#26469;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30740;&#31350;&#21644;&#21457;&#23637;&#30340;&#24895;&#26223;&#65292;&#35813;&#24895;&#26223;&#26159;&#22522;&#20110;&#20027;&#21160;&#25512;&#29702;&#21644;&#33258;&#35777;&#26126;&#30340;&#26234;&#33021;&#29983;&#24577;&#31995;&#32479;&#65292;&#20854;&#20013;&#20154;&#31867;&#20316;&#20026;&#20849;&#20139;&#26234;&#33021;&#30340;&#19981;&#21487;&#25110;&#32570;&#30340;&#21442;&#19982;&#32773;&#12290;&#35813;&#24895;&#26223;&#26088;&#22312;&#26368;&#22823;&#21270;&#27169;&#22411;&#35777;&#25454;&#65292;&#36890;&#36807;&#19981;&#21516;&#23610;&#24230;&#19978;&#30340;&#20449;&#24565;&#26356;&#26032;&#26469;&#32047;&#31215;&#20851;&#20110;&#33258;&#36523;&#24863;&#30693;&#19990;&#30028;&#30340;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30333;&#30382;&#20070;&#25552;&#20986;&#20102;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#26410;&#26469;&#21313;&#24180;&#65288;&#29978;&#33267;&#26356;&#20037;&#65289;&#30340;&#30740;&#31350;&#21644;&#24320;&#21457;&#24895;&#26223;&#12290;&#20854;&#20013;&#65292;&#26680;&#24515;&#26159;&#26500;&#24314;&#19968;&#20010;&#33258;&#28982;&#19982;&#21512;&#25104;&#24863;&#30693;&#30340;&#21327;&#21516;&#26234;&#33021;&#29983;&#24577;&#31995;&#32479;&#65292;&#23558;&#20154;&#31867;&#20316;&#20026;&#19981;&#21487;&#25110;&#32570;&#30340;&#21442;&#19982;&#32773; - &#25105;&#20204;&#31216;&#20043;&#20026;&#8220;&#20849;&#20139;&#26234;&#33021;&#8221;&#12290;&#35813;&#24895;&#26223;&#22522;&#20110;&#20027;&#21160;&#25512;&#29702;&#65292;&#19968;&#31181;&#33258;&#36866;&#24212;&#34892;&#20026;&#30340;&#24418;&#24335;&#65292;&#21487;&#20197;&#34987;&#35270;&#20026;&#26234;&#33021;&#30340;&#29289;&#29702;&#23398;&#65292;&#24182;&#32487;&#25215;&#20102;&#33258;&#32452;&#32455;&#29289;&#29702;&#23398;&#30340;&#29305;&#24449;&#12290;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#23558;&#26234;&#33021;&#29702;&#35299;&#20026;&#32047;&#31215;&#20851;&#20110;&#33258;&#24049;&#24863;&#30693;&#19990;&#30028;&#30340;&#29983;&#25104;&#27169;&#22411;&#30340;&#35777;&#25454;&#30340;&#33021;&#21147; - &#20063;&#34987;&#31216;&#20026;&#33258;&#35777;&#26126;&#12290;&#22312;&#24418;&#24335;&#19978;&#65292;&#36825;&#23545;&#24212;&#20110;&#36890;&#36807;&#22312;&#22810;&#20010;&#23610;&#24230;&#19978;&#36827;&#34892;&#20449;&#24565;&#26356;&#26032;&#26469;&#26368;&#22823;&#21270;&#65288;&#36125;&#21494;&#26031;&#65289;&#27169;&#22411;&#35777;&#25454;&#65292;&#21253;&#25324;&#25512;&#29702;&#12289;&#23398;&#20064;&#21644;&#27169;&#22411;&#36873;&#25321;&#12290;&#22312;&#25805;&#20316;&#19978;&#65292;&#36825;&#31181;&#33258;&#35777;&#26126;&#21487;&#20197;&#36890;&#36807;&#23545;&#22240;&#23376;&#22270;&#36827;&#34892;&#65288;&#21464;&#20998;&#65289;&#28040;&#24687;&#20256;&#36882;&#25110;&#20449;&#24565;&#20256;&#25773;&#26469;&#23454;&#29616;&#12290;&#20027;&#21160;&#25512;&#29702;&#30340;&#20851;&#38190;&#26159;&#31361;&#20986;&#20102;&#23384;&#22312;&#30340;&#32039;&#36843;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This white paper lays out a vision of research and development in the field of artificial intelligence for the next decade (and beyond). Its denouement is a cyber-physical ecosystem of natural and synthetic sense-making, in which humans are integral participants -- what we call ''shared intelligence''. This vision is premised on active inference, a formulation of adaptive behavior that can be read as a physics of intelligence, and which inherits from the physics of self-organization. In this context, we understand intelligence as the capacity to accumulate evidence for a generative model of one's sensed world -also known as self-evidencing. Formally, this corresponds to maximizing (Bayesian) model evidence, via belief updating over several scales: i.e., inference, learning, and model selection. Operationally, this self-evidencing can be realized via (variational) message passing or belief propagation on a factor graph. Crucially, active inference foregrounds an existential imperative
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#20998;&#24067;&#24335;&#29615;&#22659;&#30340;&#36890;&#20449;&#39640;&#25928;&#30340;&#20998;&#23618;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#21487;&#25193;&#23637;&#30340;&#26080;&#32447;&#32858;&#21512;&#26041;&#26696;&#21644;&#24102;&#23485;&#26377;&#38480;&#30340;&#24191;&#25773;&#26041;&#26696;&#65292;&#35299;&#20915;&#20102;&#35774;&#22791;&#24178;&#25200;&#21644;&#36793;&#32536;&#26381;&#21153;&#22120;&#24178;&#25200;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2211.16162</link><description>&lt;p&gt;
&#21487;&#25193;&#23637;&#30340;&#20998;&#23618;&#26080;&#32447;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Scalable Hierarchical Over-the-Air Federated Learning. (arXiv:2211.16162v2 [cs.IT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.16162
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#20998;&#24067;&#24335;&#29615;&#22659;&#30340;&#36890;&#20449;&#39640;&#25928;&#30340;&#20998;&#23618;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#21487;&#25193;&#23637;&#30340;&#26080;&#32447;&#32858;&#21512;&#26041;&#26696;&#21644;&#24102;&#23485;&#26377;&#38480;&#30340;&#24191;&#25773;&#26041;&#26696;&#65292;&#35299;&#20915;&#20102;&#35774;&#22791;&#24178;&#25200;&#21644;&#36793;&#32536;&#26381;&#21153;&#22120;&#24178;&#25200;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#21253;&#21547;&#26680;&#24515;&#26381;&#21153;&#22120;&#21644;&#22810;&#20010;&#36793;&#32536;&#26381;&#21153;&#22120;&#21450;&#35774;&#22791;&#38598;&#32676;&#30340;&#20998;&#24067;&#24335;&#29615;&#22659;&#30340;&#36890;&#20449;&#39640;&#25928;&#30340;&#20998;&#23618;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#12290;&#20551;&#35774;&#19981;&#21516;&#30340;&#23398;&#20064;&#20219;&#21153;&#65292;&#20855;&#26377;&#30456;&#21516;&#20219;&#21153;&#30340;&#38598;&#32676;&#36827;&#34892;&#21327;&#20316;&#12290;&#20026;&#20102;&#22312;&#26080;&#32447;&#38142;&#36335;&#19978;&#23454;&#29616;&#31639;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#20998;&#31751;&#26080;&#32447;&#32858;&#21512;&#26041;&#26696;&#65292;&#29992;&#20110;&#19978;&#34892;&#38142;&#36335;&#65292;&#21516;&#26102;&#37319;&#29992;&#24102;&#23485;&#26377;&#38480;&#30340;&#24191;&#25773;&#26041;&#26696;&#29992;&#20110;&#19979;&#34892;&#38142;&#36335;&#65292;&#27599;&#20010;&#31639;&#27861;&#36845;&#20195;&#21482;&#38656;&#35201;&#19968;&#20010;&#36164;&#28304;&#22359;&#65292;&#19981;&#21463;&#36793;&#32536;&#26381;&#21153;&#22120;&#21644;&#35774;&#22791;&#25968;&#37327;&#30340;&#24433;&#21709;&#12290;&#36825;&#31181;&#35774;&#32622;&#38754;&#20020;&#30528;&#19978;&#34892;&#38142;&#36335;&#35774;&#22791;&#24178;&#25200;&#21644;&#19979;&#34892;&#38142;&#36335;&#36793;&#32536;&#26381;&#21153;&#22120;&#24178;&#25200;&#30340;&#38382;&#39064;&#65292;&#38656;&#35201;&#36827;&#34892;&#20005;&#26684;&#30340;&#24314;&#27169;&#12290;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#23558;&#35774;&#22791;&#24314;&#27169;&#20026;&#19968;&#20010;&#27850;&#26494;&#38598;&#32676;&#36807;&#31243;&#65292;&#22312;&#35774;&#32622;&#20013;&#24314;&#31435;&#20102;&#19968;&#20010;&#31354;&#38388;&#27169;&#22411;&#65292;&#24182;&#23545;&#30001;&#24178;&#25200;&#24341;&#36215;&#30340;&#19978;&#34892;&#38142;&#36335;&#21644;&#19979;&#34892;&#38142;&#36335;&#30340;&#35823;&#24046;&#36827;&#34892;&#37327;&#21270;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#38754;&#30340;&#25968;&#23398;&#26041;&#27861;&#26469;&#25512;&#23548;&#25910;&#25947;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we propose a communication-efficient hierarchical federated learning algorithm for distributed setups including core servers and multiple edge servers with clusters of devices. Assuming different learning tasks, clusters with a same task collaborate. To implement the algorithm over wireless links, we propose a scalable clustered over-the-air aggregation scheme for the uplink with a bandwidth-limited broadcast scheme for the downlink that requires only a single resource block for each algorithm iteration, independent of the number of edge servers and devices. This setup is faced with interference of devices in the uplink and interference of edge servers in the downlink that are to be modeled rigorously. We first develop a spatial model for the setup by modeling devices as a Poisson cluster process over the edge servers and quantify uplink and downlink error terms due to the interference. Accordingly, we present a comprehensive mathematical approach to derive the convergenc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;CP-PINNs&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;PINNs&#19982;&#24635;&#21464;&#24046;&#24809;&#32602;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#20934;&#30830;&#30340;&#21464;&#28857;&#26816;&#27979;&#21644;PDE&#30340;&#21457;&#29616;&#12290;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#20803;&#23398;&#20064;&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#25968;&#25454;&#30340;&#36830;&#32493;&#25209;&#27425;&#19978;&#21160;&#24577;&#25913;&#36827;&#20248;&#21270;&#30446;&#26631;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#23384;&#22312;&#21464;&#28857;&#30340;&#24773;&#20917;&#19979;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#20934;&#30830;&#20272;&#35745;&#21442;&#25968;&#21644;&#27169;&#22411;&#23545;&#40784;&#65292;&#22312;&#27809;&#26377;&#21464;&#28857;&#30340;&#24773;&#20917;&#19979;&#33021;&#22815;&#25968;&#20540;&#19978;&#25910;&#25947;&#21040;&#21407;&#22987;PINNs&#27169;&#22411;&#30340;&#35299;&#12290;</title><link>http://arxiv.org/abs/2208.08626</link><description>&lt;p&gt;
CP-PINNs: &#20351;&#29992;&#29289;&#29702;&#30693;&#35782;&#31070;&#32463;&#32593;&#32476;&#21644;&#24635;&#21464;&#24046;&#24809;&#32602;&#36827;&#34892;PDE&#20013;&#30340;&#21464;&#28857;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
CP-PINNs: Changepoints Detection in PDEs using Physics Informed Neural Networks with Total-Variation Penalty. (arXiv:2208.08626v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.08626
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;CP-PINNs&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;PINNs&#19982;&#24635;&#21464;&#24046;&#24809;&#32602;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#20934;&#30830;&#30340;&#21464;&#28857;&#26816;&#27979;&#21644;PDE&#30340;&#21457;&#29616;&#12290;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#20803;&#23398;&#20064;&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#25968;&#25454;&#30340;&#36830;&#32493;&#25209;&#27425;&#19978;&#21160;&#24577;&#25913;&#36827;&#20248;&#21270;&#30446;&#26631;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#23384;&#22312;&#21464;&#28857;&#30340;&#24773;&#20917;&#19979;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#20934;&#30830;&#20272;&#35745;&#21442;&#25968;&#21644;&#27169;&#22411;&#23545;&#40784;&#65292;&#22312;&#27809;&#26377;&#21464;&#28857;&#30340;&#24773;&#20917;&#19979;&#33021;&#22815;&#25968;&#20540;&#19978;&#25910;&#25947;&#21040;&#21407;&#22987;PINNs&#27169;&#22411;&#30340;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#20102;&#22312;&#21442;&#25968;&#20013;&#23384;&#22312;&#26410;&#30693;&#21464;&#28857;&#30340;&#24773;&#20917;&#19979;&#65292;&#29289;&#29702;&#30693;&#35782;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#21487;&#33021;&#26080;&#27861;&#27491;&#30830;&#20272;&#35745;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#30340;&#21160;&#24577;&#36807;&#31243;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;CP-PINNs&#27169;&#22411;&#65292;&#23558;PINNs&#19982;&#24635;&#21464;&#24046;&#24809;&#32602;&#30456;&#32467;&#21512;&#65292;&#29992;&#20110;&#20934;&#30830;&#30340;&#21464;&#28857;&#26816;&#27979;&#21644;PDE&#30340;&#21457;&#29616;&#12290;&#20026;&#20102;&#22312;&#27169;&#22411;&#25311;&#21512;&#12289;PDE&#21457;&#29616;&#21644;&#21464;&#28857;&#26816;&#27979;&#20219;&#21153;&#20043;&#38388;&#36827;&#34892;&#26368;&#20248;&#32452;&#21512;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#20803;&#23398;&#20064;&#31639;&#27861;&#65292;&#21033;&#29992;&#25209;&#37327;&#23398;&#20064;&#22312;&#25968;&#25454;&#30340;&#36830;&#32493;&#25209;&#27425;&#19978;&#21160;&#24577;&#25913;&#36827;&#20248;&#21270;&#30446;&#26631;&#12290;&#22312;&#23454;&#35777;&#26041;&#38754;&#65292;&#22312;&#21160;&#24577;&#36807;&#31243;&#20013;&#23384;&#22312;&#21464;&#28857;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#20934;&#30830;&#20272;&#35745;&#21442;&#25968;&#21644;&#27169;&#22411;&#23545;&#40784;&#65292;&#22312;&#25968;&#25454;&#20013;&#27809;&#26377;&#21464;&#28857;&#30340;&#24773;&#20917;&#19979;&#65292;&#25968;&#20540;&#19978;&#25910;&#25947;&#21040;&#21407;&#22987;PINNs&#27169;&#22411;&#30340;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
The paper shows that Physics-Informed Neural Networks (PINNs) can fail to estimate the correct Partial Differential Equations (PDEs) dynamics in cases of unknown changepoints in the parameters. To address this, we propose a new CP-PINNs model which integrates PINNs with Total-Variation penalty for accurate changepoints detection and PDEs discovery. In order to optimally combine the tasks of model fitting, PDEs discovery, and changepoints detection, we develop a new meta-learning algorithm that exploits batch learning to dynamically refines the optimization objective when moving over the consecutive batches of the data. Empirically, in case of changepoints in the dynamics, our approach demonstrates accurate parameter estimation and model alignment, and in case of no changepoints in the data, it converges numerically to the solution from the original PINNs model.
&lt;/p&gt;</description></item></channel></rss>